{"cell_type":{"29a67848":"code","bedaa49c":"code","6bf50c79":"code","c515c784":"code","1ce42761":"code","4dc19afd":"code","41fa12b4":"code","a86ab7b0":"code","03ec914b":"code","37fa065d":"code","d64cc080":"code","7a91ad91":"code","ce0fb759":"code","04621ed8":"code","a9b19a4d":"code","e0f83b76":"code","d7db1aa1":"code","98a80e33":"code","8ec52108":"code","5600d4ca":"code","881fc22c":"code","a5a42edd":"code","0a9f55ae":"code","eddb0a24":"code","80c0b66f":"code","54c7a9bd":"code","d344c51a":"code","e3bca6d0":"code","f89ee855":"code","d5193c64":"code","6bae4db8":"code","2377d652":"code","bb56819d":"code","943eb852":"code","3f2a3658":"code","0f902e9e":"code","de64cb2b":"code","b4c7f700":"code","48590628":"code","cf6cfacf":"code","a9a3b424":"code","ec1c4aa0":"code","f92e680f":"code","f6015d8f":"code","d13038ee":"code","6398521a":"code","2c25bb17":"code","c16e540e":"code","0b57003f":"code","705a04ec":"code","81b7d4b6":"code","460620d6":"code","e5c4a2e2":"code","bea1b50e":"code","0cff12aa":"code","a3ab47a4":"code","284eba11":"code","de25364f":"code","38a816b5":"code","f1599124":"code","5b8abf03":"code","7be7abb4":"code","6030591c":"code","99735164":"code","d43dbefa":"code","07728fc4":"code","ca4a4284":"code","98b58161":"code","f824f36b":"code","73dfd13b":"code","9d7dfb73":"code","3280f0bc":"code","cf35f20f":"code","45c5b208":"code","a6612951":"code","deffa269":"code","ae839796":"code","2a50f55d":"code","bdab8b25":"code","f2daf993":"code","40742683":"markdown","40949fb9":"markdown","8e383f36":"markdown","987d6bf2":"markdown","9d897602":"markdown","4ea758ab":"markdown","4b3c60f5":"markdown","11b4b176":"markdown","844bd34f":"markdown","ee576974":"markdown","a040fd72":"markdown","ca9590e2":"markdown","76b5d30b":"markdown","59a9d20a":"markdown","96096ff2":"markdown"},"source":{"29a67848":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bedaa49c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()","6bf50c79":"pwd","c515c784":"!pip install openpyxl","1ce42761":"!pip install xlrd==1.2.0","4dc19afd":"train_data = pd.read_excel('\/kaggle\/input\/flight-fare-prediction-mh\/Data_Train.xlsx',engine='openpyxl')","41fa12b4":"train_data.head(5)","a86ab7b0":"pd.set_option('display.max_columns', None) ##to display all the columns","03ec914b":"train_data.head(10)","37fa065d":"train_data.info() ##function is used to print a concise summary of a DataFrame. This method prints information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage","d64cc080":"train_data[\"Duration\"].value_counts() ## Return a Series containing counts of unique values","7a91ad91":"train_data.dropna(inplace = True)","ce0fb759":"train_data.isnull().sum()","04621ed8":"train_data[\"Journey_day\"] = pd.to_datetime(train_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day","a9b19a4d":"train_data[\"Journey_month\"] = pd.to_datetime(train_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month","e0f83b76":"train_data.head()","d7db1aa1":"# Since we have converted Date_of_Journey column into integers, Now we can drop as it as insignificant column\n\ntrain_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)","98a80e33":"# Departure time is when a plane leaves the gate. \n# Similar to Date_of_Journey we can extract values from Dep_Time\n\n# Extracting Hours\ntrain_data[\"Dep_hour\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Dep_min\"] = pd.to_datetime(train_data[\"Dep_Time\"]).dt.minute\n\n# Now we can drop Dep_Time as it is of no use\ntrain_data.drop([\"Dep_Time\"], axis = 1, inplace = True)","8ec52108":"train_data.head()","5600d4ca":"# Arrival time is when the plane pulls up to the gate.\n# Similar to Date_of_Journey we can extract values from Arrival_Time\n\n# Extracting Hours\ntrain_data[\"Arrival_hour\"] = pd.to_datetime(train_data.Arrival_Time).dt.hour\n\n# Extracting Minutes\ntrain_data[\"Arrival_min\"] = pd.to_datetime(train_data.Arrival_Time).dt.minute\n\n# Now we can drop Arrival_Time as it is of no use\ntrain_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)","881fc22c":"train_data.head()","a5a42edd":"# Time taken by plane to reach destination is called Duration\n# It is the differnce betwwen Departure Time and Arrival time\n\n\n# Assigning and converting Duration column into list\nduration = list(train_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration","0a9f55ae":"# Adding duration_hours and duration_mins list to train_data dataframe\n\ntrain_data[\"Duration_hours\"] = duration_hours\ntrain_data[\"Duration_mins\"] = duration_mins","eddb0a24":"train_data.drop([\"Duration\"], axis = 1, inplace = True) #drop insignificant columns","80c0b66f":"train_data.head(10)","54c7a9bd":"train_data[\"Airline\"].value_counts()","d344c51a":"# As Airline is Nominal Categorical data we will perform OneHotEncoding\n\nAirline = train_data[[\"Airline\"]]\n\nAirline = pd.get_dummies(Airline, drop_first= True)\n\nAirline.head()","e3bca6d0":"train_data[\"Source\"].value_counts()","f89ee855":"# Source vs Price\n\nsns.catplot(y = \"Price\", x = \"Source\", data = train_data.sort_values(\"Price\", ascending = False), kind=\"boxen\", height = 4, aspect = 3)\nplt.show()","d5193c64":"# As Source is Nominal Categorical data we will perform OneHotEncoding\n\nSource = train_data[[\"Source\"]]\n\nSource = pd.get_dummies(Source, drop_first= True)\n\nSource.head()","6bae4db8":"train_data[\"Destination\"].value_counts()","2377d652":"# As Destination is Nominal Categorical data we will perform OneHotEncoding\n\nDestination = train_data[[\"Destination\"]]\n\nDestination = pd.get_dummies(Destination, drop_first = True)\n\nDestination.head()","bb56819d":"train_data[\"Route\"]","943eb852":"# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\n\ntrain_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)","3f2a3658":"train_data[\"Total_Stops\"].value_counts()","0f902e9e":"# As this is case of Ordinal Categorical type we perform LabelEncoder\n# Here Values are assigned with corresponding keys\n\ntrain_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)","de64cb2b":"train_data.head(10)","b4c7f700":"# Concatenate dataframe --> train_data + Airline + Source + Destination\n\ndata_train = pd.concat([train_data, Airline, Source, Destination], axis = 1)","48590628":"data_train.head(10)","cf6cfacf":"data_train.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)","a9a3b424":"data_train.head()","ec1c4aa0":"data_train.shape","f92e680f":"test_data = pd.read_excel('\/kaggle\/input\/flight-fare-prediction-mh\/Test_set.xlsx',engine='openpyxl')","f6015d8f":"test_data.head()","d13038ee":"# Preprocessing\n\nprint(\"Test data Info\")\nprint(\"-\"*75)\nprint(test_data.info())\n\nprint()\nprint()\n\nprint(\"Null values :\")\nprint(\"-\"*75)\ntest_data.dropna(inplace = True)\nprint(test_data.isnull().sum())\n\n# EDA\n\n# Date_of_Journey\ntest_data[\"Journey_day\"] = pd.to_datetime(test_data.Date_of_Journey, format=\"%d\/%m\/%Y\").dt.day\ntest_data[\"Journey_month\"] = pd.to_datetime(test_data[\"Date_of_Journey\"], format = \"%d\/%m\/%Y\").dt.month\ntest_data.drop([\"Date_of_Journey\"], axis = 1, inplace = True)\n\n# Dep_Time\ntest_data[\"Dep_hour\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.hour\ntest_data[\"Dep_min\"] = pd.to_datetime(test_data[\"Dep_Time\"]).dt.minute\ntest_data.drop([\"Dep_Time\"], axis = 1, inplace = True)\n\n# Arrival_Time\ntest_data[\"Arrival_hour\"] = pd.to_datetime(test_data.Arrival_Time).dt.hour\ntest_data[\"Arrival_min\"] = pd.to_datetime(test_data.Arrival_Time).dt.minute\ntest_data.drop([\"Arrival_Time\"], axis = 1, inplace = True)\n\n# Duration\nduration = list(test_data[\"Duration\"])\n\nfor i in range(len(duration)):\n    if len(duration[i].split()) != 2:    # Check if duration contains only hour or mins\n        if \"h\" in duration[i]:\n            duration[i] = duration[i].strip() + \" 0m\"   # Adds 0 minute\n        else:\n            duration[i] = \"0h \" + duration[i]           # Adds 0 hour\n\nduration_hours = []\nduration_mins = []\nfor i in range(len(duration)):\n    duration_hours.append(int(duration[i].split(sep = \"h\")[0]))    # Extract hours from duration\n    duration_mins.append(int(duration[i].split(sep = \"m\")[0].split()[-1]))   # Extracts only minutes from duration\n\n# Adding Duration column to test set\ntest_data[\"Duration_hours\"] = duration_hours\ntest_data[\"Duration_mins\"] = duration_mins\ntest_data.drop([\"Duration\"], axis = 1, inplace = True)\n\n\n# Categorical data\n\nprint(\"Airline\")\nprint(\"-\"*75)\nprint(test_data[\"Airline\"].value_counts())\nAirline = pd.get_dummies(test_data[\"Airline\"], drop_first= True)\n\nprint()\n\nprint(\"Source\")\nprint(\"-\"*75)\nprint(test_data[\"Source\"].value_counts())\nSource = pd.get_dummies(test_data[\"Source\"], drop_first= True)\n\nprint()\n\nprint(\"Destination\")\nprint(\"-\"*75)\nprint(test_data[\"Destination\"].value_counts())\nDestination = pd.get_dummies(test_data[\"Destination\"], drop_first = True)\n\n# Additional_Info contains almost 80% no_info\n# Route and Total_Stops are related to each other\ntest_data.drop([\"Route\", \"Additional_Info\"], axis = 1, inplace = True)\n\n# Replacing Total_Stops\ntest_data.replace({\"non-stop\": 0, \"1 stop\": 1, \"2 stops\": 2, \"3 stops\": 3, \"4 stops\": 4}, inplace = True)\n\n# Concatenate dataframe --> test_data + Airline + Source + Destination\ndata_test = pd.concat([test_data, Airline, Source, Destination], axis = 1)\n\ndata_test.drop([\"Airline\", \"Source\", \"Destination\"], axis = 1, inplace = True)\n\nprint()\nprint()\n\nprint(\"Shape of test data : \", data_test.shape)","6398521a":"data_test.head(10)","2c25bb17":"data_train.shape","c16e540e":"data_train.columns","0b57003f":"X= data_train.loc[:,data_train.columns!='Price']\nX","705a04ec":"y=data_train.loc[:, \"Price\"]\ny","81b7d4b6":"# Finds correlation between Independent and dependent attributes\n\nplt.figure(figsize = (18,18))\nsns.heatmap(train_data.corr(), annot = True, cmap = \"RdYlGn\")\n\nplt.show()","460620d6":"# Important feature using ExtraTreesRegressor\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nselection = ExtraTreesRegressor()\nselection.fit(X, y)","e5c4a2e2":"print(selection.feature_importances_)","bea1b50e":"#plot graph of feature importances for better visualization\n\nplt.figure(figsize = (12,8))\nfeat_importances = pd.Series(selection.feature_importances_, index=X.columns)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","0cff12aa":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","a3ab47a4":"from sklearn.ensemble import RandomForestRegressor\nreg_rf = RandomForestRegressor()\nreg_rf.fit(X_train, y_train)","284eba11":"y_pred = reg_rf.predict(X_test)","de25364f":"reg_rf.score(X_train, y_train)","38a816b5":"reg_rf.score(X_test, y_test)","f1599124":"import warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')","5b8abf03":"sns.distplot(y_test-y_pred)\nplt.show()","7be7abb4":"plt.scatter(y_test, y_pred, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","6030591c":"from sklearn import metrics","99735164":"print('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","d43dbefa":"# RMSE\/(max(DV)-min(DV))\n\n2090.5509\/(max(y)-min(y))","07728fc4":"metrics.r2_score(y_test, y_pred)","ca4a4284":"from sklearn.model_selection import RandomizedSearchCV","98b58161":"#Randomized Search CV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1200, num = 12)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 30, num = 6)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 15, 100]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 5, 10]","f824f36b":"# Create the random grid\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","73dfd13b":"# Random search of parameters, using 5 fold cross validation, \n# search across 100 different combinations\nrf_random = RandomizedSearchCV(estimator = reg_rf, param_distributions = random_grid,scoring='neg_mean_squared_error', n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)","9d7dfb73":"rf_random.fit(X_train,y_train)","3280f0bc":"rf_random.best_params_","cf35f20f":"prediction = rf_random.predict(X_test)","45c5b208":"plt.figure(figsize = (8,8))\nsns.distplot(y_test-prediction)\nplt.show()","a6612951":"plt.figure(figsize = (8,8))\nplt.scatter(y_test, prediction, alpha = 0.5)\nplt.xlabel(\"y_test\")\nplt.ylabel(\"y_pred\")\nplt.show()","deffa269":"print('MAE:', metrics.mean_absolute_error(y_test, prediction))\nprint('MSE:', metrics.mean_squared_error(y_test, prediction))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))","ae839796":"import pickle\n# open a file, where you ant to store the data\nfile = open('flight_rf.pkl', 'wb')\n\n# dump information to that file\npickle.dump(rf_random, file)","2a50f55d":"model = open('flight_rf.pkl','rb')\nforest = pickle.load(model)","bdab8b25":"y_prediction = forest.predict(X_test)","f2daf993":"metrics.r2_score(y_test, y_prediction)","40742683":"1.Split dataset into train and test set in order to prediction w.r.t X_test\n\n2.If needed do scaling of data\n\nScaling is not done in Random forest\n\n3.Import model\n\n4.Fit the data\n\n5.Predict w.r.t X_test\n\n6.In regression check RSME Score\nPlot graph","40949fb9":"## Fitting model using Random Forest","8e383f36":"## Test Data","987d6bf2":"## Hyperparameter Tuning","9d897602":"Choose following method for hyperparameter tuning\n\nRandomizedSearchCV --> Fast\n\nGridSearchCV\n\nAssign hyperparameters in form of dictionery\n\nFit the model\n\nCheck best paramters and best score","4ea758ab":"## Flight Price Prediction","4b3c60f5":"## Feature Selection","11b4b176":"### Exploratory Data Analysis","844bd34f":"One can find many ways to handle categorical data. Some of them categorical data are,\n\n1.**Nominal data** --> data are not in any order --> **OneHotEncoder** is used in this case\n\n2.**Ordinal data** --> data are in order --> **LabelEncoder** is used in this case","ee576974":"Finding out the best feature which will contribute and have good relation with target variable. Following are some of the feature selection methods,\n\n1.**heatmap**\n\n2.**feature_importance_**\n\n3.**SelectKBest**","a040fd72":"### drop rows having null values","ca9590e2":"Project is done looking at the video live implementation of Flight price prediction by Krish Naik done by Amar Mandal\n\nCredits to Krish naik and Amar mandal","76b5d30b":"## Save the model to reuse it again","59a9d20a":"## Handling Categorical Data","96096ff2":"From description we can see that Date_of_Journey is a object data type,\\ Therefore, we have to convert this datatype into timestamp so as to use this column properly for prediction\n\nFor this we require pandas to_datetime to convert object data type to datetime dtype.\n\n**.dt.day method will extract only day of that date**\\ **.dt.month method will extract only month of that date**"}}