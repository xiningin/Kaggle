{"cell_type":{"22d007bf":"code","45f157b5":"code","2565f4b5":"code","e2295807":"code","bc4d211c":"code","88cc9e05":"code","4ec937c6":"code","77be9d70":"code","eba4d9bc":"code","8d84f04c":"code","0598f3a8":"code","816a89ac":"code","4ef35a4b":"code","25649b7d":"code","22c9b07a":"code","0cb309d7":"code","59321000":"code","60e2daaa":"code","a8f65e79":"code","c735c1c6":"code","ace9cf06":"code","4710d260":"markdown"},"source":{"22d007bf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","45f157b5":"# creating whl files: https:\/\/stackoverflow.com\/questions\/57942120\/how-to-package-all-my-library-dependencies-in-wheel-file\n!ls ..\/input\/pycaret-231-whl\/","2565f4b5":"!unzip ..\/input\/pycaret-231-whl\/pycaret_2.3.1.zip -d ..\/pycaret","e2295807":"!ls ..\/pycaret\/","bc4d211c":"# installing multiple whl files: https:\/\/stackoverflow.com\/questions\/65844775\/how-to-install-multiple-whl-files-in-the-right-order\n# this command installs all whl files in the wheelhouse folder\n!pip install --no-index --no-deps ..\/pycaret\/wheelhouse\/*.whl","88cc9e05":"from nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer","4ec937c6":"!pip show pycaret","77be9d70":"from pycaret.regression import * ","eba4d9bc":"from nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.stem.snowball import SnowballStemmer","8d84f04c":"train = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/sample_submission.csv')","0598f3a8":"vec = TfidfVectorizer(tokenizer=word_tokenize, stop_words=stopwords.words('english'), ngram_range=(1, 3), min_df=50)\nX_train = vec.fit_transform(train['excerpt'])\nX_test = vec.transform(test['excerpt'])\nprint(X_train.shape, X_test.shape)","816a89ac":"train = pd.concat([pd.DataFrame(X_train.todense()), train.target], axis=1)","4ef35a4b":"X_test = pd.DataFrame(X_test.todense())","25649b7d":"reg = setup(train, target='target', fold=5, fold_shuffle=True, session_id=42, silent=True)","22c9b07a":"top3 = compare_models(n_select=3)","0cb309d7":"blender_specific = blend_models(estimator_list=top3)","59321000":"pred_holdout = predict_model(blender_specific)","60e2daaa":"final = finalize_model(blender_specific)","a8f65e79":"predictions = predict_model(final, data=X_test)","c735c1c6":"submission['target'] = predictions['Label']\nsubmission","ace9cf06":"submission.to_csv('submission.csv', index=False)","4710d260":"Creating whl files for external packges is described on [HERE](https:\/\/www.kaggle.com\/jiny333\/creating-whl-files-to-install-external-libraries)."}}