{"cell_type":{"a64ce4fb":"code","73aec559":"code","a0dd8af9":"code","cafe3a30":"code","3a3b96bc":"code","95079185":"code","017459f6":"code","f6d98a33":"code","76c99abe":"code","8895727b":"code","0a49cb2c":"code","e3dc547b":"code","89cd1174":"code","f346d9d0":"code","880150b1":"code","4dec53a0":"code","e057f465":"code","f0c37ed0":"code","52127462":"code","472ebc5c":"code","98b5db25":"code","a478b346":"code","2648a090":"code","9441c9c3":"code","5669445f":"code","5584a16c":"code","ee0f19de":"code","a6dfaa33":"code","8c6f5c9b":"code","0017e636":"code","3390892f":"markdown","9c64e664":"markdown","1c018ce1":"markdown","745b983a":"markdown","c7c77e37":"markdown","24ec06c1":"markdown","23030806":"markdown","64323be5":"markdown","b15178b1":"markdown","5a1fbc3c":"markdown","8be8cb2a":"markdown","07fa3cd4":"markdown","d4ef397e":"markdown","d65fcbf6":"markdown","1f76bc4a":"markdown","7f753de8":"markdown","6707f339":"markdown","f5b5a25c":"markdown","3a82cc1d":"markdown","5c360928":"markdown","8a40858f":"markdown","3c2a6a1a":"markdown","21627851":"markdown","7fee9b28":"markdown","ece1ed52":"markdown","b2fb6015":"markdown","ab223689":"markdown","6532d725":"markdown","c9196440":"markdown","c6f12ca3":"markdown","f0e8d3f1":"markdown","9ec85ee7":"markdown","55c9c723":"markdown","8fe819b9":"markdown","6d336971":"markdown","0bcc4154":"markdown","9fb55a29":"markdown","35192ee6":"markdown"},"source":{"a64ce4fb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.offline import iplot, init_notebook_mode\n#import cufflinks\n#import cufflinks as cf\nimport plotly.figure_factory as ff\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","73aec559":"def resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=2),2) \n\n    return summary\n\ndef cross_heatmap(df, cols, normalize=False, values=None, aggfunc=None):\n    temp = cols\n    cm = sns.light_palette(\"green\", as_cmap=True)\n    return (round(pd.crosstab(df[temp[0]], df[temp[1]], \n                       normalize=normalize, values=values, aggfunc=aggfunc) * 100,2)).style.background_gradient(cmap = cm)\n\ndef date_time_col(df, col, form='%m\/%d\/%Y'):\n    df[col] = pd.to_datetime(df[col], format=form)\n    return df[col]","a0dd8af9":"kernel_tags = pd.read_csv('\/kaggle\/input\/meta-kaggle\/KernelTags.csv')\nkernels = pd.read_csv('\/kaggle\/input\/meta-kaggle\/Kernels.csv')\nkernel_versions = pd.read_csv('\/kaggle\/input\/meta-kaggle\/KernelVersions.csv')\nkernel_votes = pd.read_csv('\/kaggle\/input\/meta-kaggle\/KernelVotes.csv')\nkernel_langs = pd.read_csv('\/kaggle\/input\/meta-kaggle\/KernelLanguages.csv')\nfollow_users = pd.read_csv('\/kaggle\/input\/meta-kaggle\/UserFollowers.csv')\nusers_achi = pd.read_csv(\"\/kaggle\/input\/meta-kaggle\/UserAchievements.csv\")\nusers = pd.read_csv('\/kaggle\/input\/meta-kaggle\/Users.csv')\ntags = pd.read_csv('\/kaggle\/input\/meta-kaggle\/Tags.csv')\n\ncomp = pd.read_csv('\/kaggle\/input\/meta-kaggle\/Competitions.csv')","cafe3a30":"kernel_tags.rename(columns={'Id':'KernelId'}, inplace=True)\nkernels.rename(columns={'Id':'KernelId'}, inplace=True)\nkernel_versions.rename(columns={'Id':'KVersId', \n                                'TotalVotes':'TotalVotes_version'}, inplace=True)\nkernel_votes.rename(columns={'Id':'KernVotId'}, inplace=True)\nkernel_langs.rename(columns={'Id':'KernelLanguageId'}, inplace=True)\nfollow_users.rename(columns={'Id':'UserId'}, inplace=True)\nusers_achi.rename(columns={'Id':'AchId'}, inplace=True)\nusers.rename(columns={'Id':'UserId'}, inplace=True)\nusers_achi.rename(columns={'Id':'AchId'}, inplace=True)\n#dataset.rename(columns={'Id':'dsetId'}, inplace=True)\nkernel_versions.rename(columns={'TotalVotes':'TotalVotes_version'}, inplace=True)","3a3b96bc":"resumetable(users)","95079185":"resumetable(kernels)","017459f6":"resumetable(users_achi)","f6d98a33":"def date_time_col(df, col, form='%m\/%d\/%Y'):\n    df[col] = pd.to_datetime(df[col], format=form)\n    return df[col]\n\nusers['RegisterDate'] = date_time_col(users, 'RegisterDate')","76c99abe":"# Calling the function to transform the date column in datetime pandas object\n\n#seting some static color options\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\n\ndates_temp = users.groupby(users['RegisterDate'].dt.date)['UserId'].count().reset_index()\n# renaming the columns to apropriate names\n\n# creating the first trace with the necessary parameters\ntrace = go.Scatter(x=dates_temp['RegisterDate'], y=dates_temp.UserId,\n                    opacity = 0.8, line = dict(color = color_op[7]), name= 'Registrations')\n\ntmp_count = users.groupby(users['RegisterDate'])['UserId'].count().reset_index()\ntmp_count['cumsum'] = tmp_count['UserId'].cumsum()\ntmp_count['cumsum'] = tmp_count['cumsum'] \n\n# using the new dates_temp_sum we will create the second trace\ntrace1 = go.Scatter(x=tmp_count.RegisterDate, line = dict(color = color_op[1]), \n                    name=\"CumSum Registers\",\n                    y=tmp_count['cumsum'], opacity = 0.8, yaxis='y2')\n\ntmp_mean = users.groupby(users['RegisterDate'].dt.date)['UserId'].count().reset_index()\ntmp_mean['mean'] = round(tmp_mean['UserId'].rolling(90).mean())\n\n# using the new dates_temp_sum we will create the second trace\ntrace2 = go.Scatter(x=tmp_mean.RegisterDate, line = dict(color = color_op[3]), \n                    name=\"MovAverage 90 days\",\n                    y=tmp_mean['mean'], opacity = 0.8)\n\n\nlayout = dict(\n    title= \"Kaggle Users Registration by Dates\",\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=3, label='3m', step='month', stepmode='backward'),\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(count=12, label='12m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date' ),\n    yaxis=dict(title='Total Registrations by Day'),\n    yaxis2=dict(overlaying='y',\n                anchor='x', side='right',\n                zeroline=False, showgrid=False,\n                title='Cumulative total subscribers'),\n    annotations=[\n        go.layout.Annotation(\n            x='2017-06-11',\n            y=1001168,\n            xref=\"x\",\n            yref=\"y2\",\n            text=\"2017-06-11<br>1 Million day\",\n            showarrow=True,\n            arrowhead=3,\n            ax=-60,\n            ay=-60\n        ),\n        go.layout.Annotation(\n            x='2018-08-26',\n            y=2001571,\n            xref=\"x\",\n            yref=\"y2\",\n            text=\"2018-08-26<br>2 Million day\",\n            showarrow=True,\n            arrowhead=3,\n            ax=-80,\n            ay=-40),\n        go.layout.Annotation(\n            x='2019-05-19',\n            y=3002331,\n            xref=\"x\",\n            yref=\"y2\",\n            text=\"2019-05-19<br>3 Million day\",\n            showarrow=True,\n            arrowhead=3,\n            ax=-80,\n            ay=-30),\n                \n    ]\n)\n\n# creating figure with the both traces and layout\nfig = dict(data= [trace, trace1, trace2],\n           layout=layout)\n\n#rendering the graphs\niplot(fig) #it's an equivalent to plt.show()","8895727b":"users['month_reg'] = users['RegisterDate'].dt.month\nusers['dow_reg'] = users['RegisterDate'].dt.dayofweek\nusers['day_reg'] = users['RegisterDate'].dt.day","0a49cb2c":"#snsusers['dow_reg'].value_counts()\ntotal = len(users)\nplt.figure(figsize=(15,19))\n\nplt.subplot(311)\ng = sns.countplot(x=\"day_reg\", data=users, color='coral')\ng.set_title(\"User Registers by DAY OF MONTH\", fontsize=20)\ng.set_ylabel(\"Count\",fontsize= 17)\ng.set_xlabel(\"Day of Registrations\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(312)\ng2 = sns.countplot(x=\"dow_reg\", data=users, color='coral')\ng2.set_title(\"User Registers by DAY OF WEEK\", fontsize=20)\ng2.set_ylabel(\"Count\",fontsize= 17)\ng2.set_xlabel(\"Day of the Week of Registrations\", fontsize=17)\nsizes=[]\nfor p in g2.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g2.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=14) \ng2.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(313)\ng1 = sns.countplot(x=\"month_reg\", data=users, color='coral')\ng1.set_title(\"User Registers by MONTHS\", fontsize=20)\ng1.set_ylabel(\"Performance Tier Cats\",fontsize= 17)\ng1.set_xlabel(\"\", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g1.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=14) \ng1.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplots_adjust(hspace = 0.3)\n\nplt.show()","e3dc547b":"cross_heatmap(users, \n              ['dow_reg', 'month_reg'],\n              normalize='columns')","89cd1174":"tier_dict = {0:'Novice', 1:'Contributor', 2:'Expert',\n             3:'Master', 4:'GranMaster', 5:'KaggleTeam'}\n\nusers['PerformanceTier'] = users['PerformanceTier'].map(tier_dict)\n\nplt.figure(figsize=(15,6))\n\ng = sns.countplot(x=\"PerformanceTier\", data=users, color='coral',\n                  order=['KaggleTeam', 'Novice', 'Contributor', \n                         'Expert', 'Master', 'GranMaster'])\ng.set_title(\"Performance Tier Distribution\", fontsize=20)\ng.set_ylabel(\"Performance Tier Cats\",fontsize= 17)\ng.set_xlabel(\"\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.4f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=14) \ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.show()","f346d9d0":"users_comb = users_achi.merge(users, on='UserId')\nusers_comb['PerformanceTier'] = users_comb['PerformanceTier'].map(tier_dict)\nusers_comb['Tier'] = users_comb['Tier'].map(tier_dict)","880150b1":"users_comb['RegisterDate'] = date_time_col(users_comb, 'RegisterDate')\nusers_comb['TierAchievementDate'] = date_time_col(users_comb, 'TierAchievementDate')\n\nusers_comb['diffReg'] = (users_comb['TierAchievementDate'].dt.date - users_comb['RegisterDate'].dt.date).dt.days\n","4dec53a0":"\ntable_user_tier = users_comb[(users_comb['Tier'] != 'Novice') & \n                             (users_comb['RegisterDate'] >= '2011-03-01')].groupby(['Tier', \n                                                                                    'AchievementType'])['diffReg'].mean().reset_index()\ntier_count = users_comb[(users_comb['Tier'] != 'Novice') &\n                        (users_comb['RegisterDate'] >= '2011-03-01')].groupby(['Tier',\n                                                                               'AchievementType'])['UserId'].count().reset_index()\n\n\nplt.figure(figsize=(15,11))\n\nplt.subplot(211)\ng0 = sns.barplot(x='Tier', y='UserId', \n                hue='AchievementType', data=tier_count,\n                order = ['Expert', 'Master', 'GranMaster'])\ng0.set_title(\"Distribution of each Category\", fontsize=20)\ng0.set_xlabel(\"Kaggle Category's\", fontsize=17)\ng0.set_ylabel(\"Count total\", fontsize=17)\nsizes=[]\nfor p in g0.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g0.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.0f}'.format(height),\n            ha=\"center\", fontsize=14) \ng0.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplot(212)\ng = sns.barplot(x='Tier', y='diffReg', \n                hue='AchievementType', data=table_user_tier,\n                order = ['Expert', 'Master', 'GranMaster'])\ng.set_title(\"Time to take different Tiers (IN DAYS)\", fontsize=20)\ng.set_xlabel(\"Kaggle Category's\", fontsize=17)\ng.set_ylabel(\"Mean of Days\", fontsize=17)\n\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.0f}'.format(height),\n            ha=\"center\", fontsize=14) \ng.set_ylim(0, max(sizes) * 1.15)\n\nplt.subplots_adjust(hspace = 0.3)\n\nplt.show()\n","e057f465":"users_comb = pd.merge(kernels, users, left_on='AuthorUserId', right_on='UserId', how='left')","f0c37ed0":"users_comb['CreationDate'] = date_time_col(users_comb, 'CreationDate',\n                                           form='%m\/%d\/%Y %H:%M:%S')\n\nusers_comb['MadePublicDate'] = date_time_col(users_comb, 'MadePublicDate',\n                                             form='%m\/%d\/%Y')\nusers_comb['RegisterDate'] = users_comb['RegisterDate'].dt.date\nusers_comb['CreationDate'] = users_comb['CreationDate'].dt.date\nusers_comb['MadePublicDate'] = users_comb['MadePublicDate'].dt.date\n\n# To get votes by date\nkernel_versions['CreationDate'] = date_time_col(kernel_versions, 'CreationDate',\n                                                  form='%m\/%d\/%Y %H:%M:%S')\n\nkernel_versions['CreationDate'] = kernel_versions['CreationDate'].dt.date","52127462":"# Calling the function to transform the date column in datetime pandas object\n\n#seting some static color options\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\n\ntmp_kern = users_comb.groupby('CreationDate')['KernelId'].nunique().reset_index()\n# renaming the columns to apropriate names\n\n# creating the first trace with the necessary parameters\ntrace = go.Scatter(x=tmp_kern['CreationDate'], y=tmp_kern.KernelId,\n                    opacity = 0.8, line = dict(color = color_op[7]), \n                   name= 'Daily Creations')\n\n\ntmp_kern['cumsum'] = tmp_kern['KernelId'].cumsum()\n\n# using the new dates_temp_sum we will create the second trace\ntrace1 = go.Scatter(x=tmp_kern.CreationDate, line = dict(color = color_op[1]), \n                    name=\"Total Sum Kernels\",\n                    y=tmp_kern['cumsum'], opacity = 0.8, yaxis='y2')\n\ntmp_kern['mean'] = round(tmp_kern['KernelId'].rolling(30).mean())\n\n# using the new dates_temp_sum we will create the second trace\ntrace2 = go.Scatter(x=tmp_kern.CreationDate, line = dict(color = color_op[3]), \n                    name=\"Mov.Average 30 Days\", \n                    y=tmp_kern['mean'], opacity = 0.8)\n\n\n##tmp_votes = kernel_versions.groupby('CreationDate')['TotalVotes_version'].sum().reset_index()\n\n##trace3 = go.Scatter(x=tmp_votes.CreationDate, line = dict(color = color_op[6]), \n##                    name=\"Mov.Average 3fds0 Days\",\n##                    y=tmp_votes['TotalVotes_version'], opacity = 0.8)\n\n\nlayout = dict(\n    title= \"Kernel Informations by Date\",\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=3, label='3m', step='month', stepmode='backward'),\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(count=12, label='12m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date' ),\n    yaxis=dict(title='Kernel Creation by Day'),\n    yaxis2=dict(overlaying='y',\n                anchor='x', side='right',\n                zeroline=False, showgrid=False,\n                title='Cumulative Total Kernels'),\n\n    annotations=[\n            go.layout.Annotation(\n                x='2017-06-11',\n                y=43,\n                xref=\"x\",\n                yref=\"y\",\n                text=\"NOTE<br>It's 1 million day. <br>what happened?\",\n                showarrow=True,\n                arrowhead=3,\n                ax=51,\n                ay=-50\n            )\n\n        ]\n)\n\n# creating figure with the both traces and layout\nfig = dict(data= [trace, trace1, trace2, #trace3\n                 ],\n           layout=layout)\n\n#rendering the graphs\niplot(fig) ","472ebc5c":"comp['RewardType'].fillna('None', inplace=True)","98b5db25":"total = len(comp)\n\nprint(f'Total of registered competitions on Kaggle: {total}')\n\nplt.figure(figsize=(14,5))\n\ng = sns.countplot(x='RewardType', data=comp, color='coral')\ng.set_title(\"Competition Reward Types\", fontsize=22)\ng.set_ylabel(\"Count\", fontsize=17)\ng.set_xlabel(\"Reward Type Name\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.2f}%'.format(height\/total*100),\n            ha=\"center\", fontsize=14) \n    \ng.set_ylim(0, max(sizes) * 1.15)\n\n\nplt.show()","a478b346":"comp.loc[comp[\"RewardType\"].isin(['Knowledge', 'Swag',\n                                  'Jobs', 'Kudos']), 'MoneyComp'] = 0\ncomp['MoneyComp'].fillna(1, inplace=True)\n\ncomp['EnabledDate'] = date_time_col(comp, 'EnabledDate',\n                                           form='%m\/%d\/%Y %H:%M:%S')\ncomp['EnabledDate'] = comp['EnabledDate'].dt.date","2648a090":"tmp_comp = comp.groupby('EnabledDate')['Id'].count().reset_index()\nmoney_comp = comp.loc[comp[\"MoneyComp\"] == 1].groupby('EnabledDate')['Id'].count().reset_index()\nknowledge_comp = comp.loc[comp[\"MoneyComp\"] == 0].groupby('EnabledDate')['Id'].count().reset_index()","9441c9c3":"# Calling the function to transform the date column in datetime pandas object\n\n#seting some static color options\ncolor_op = ['#5527A0', '#BB93D7', '#834CF7', '#6C941E', '#93EAEA', '#7425FF', '#F2098A', '#7E87AC', \n            '#EBE36F', '#7FD394', '#49C35D', '#3058EE', '#44FDCF', '#A38F85', '#C4CEE0', '#B63A05', \n            '#4856BF', '#F0DB1B', '#9FDBD9', '#B123AC']\n\n\n#tmp_kern = comp.groupby('CreationDate')['KernelId'].nunique().reset_index()\n# renaming the columns to apropriate names\n\n# creating the first trace with the necessary parameters\ntrace = go.Scatter(x=money_comp['EnabledDate'], y=money_comp.Id,\n                   opacity = 0.8, line = dict(color = color_op[7]), \n                   name= 'Money Comps')\n\n\ntmp_comp['cumsum'] = tmp_comp['Id'].cumsum()\n\n# using the new dates_temp_sum we will create the second trace\ntrace1 = go.Scatter(x=tmp_comp.EnabledDate, line = dict(color = color_op[1]), \n                    name=\"Total cumulative\",\n                    y=tmp_comp['cumsum'], opacity = 0.8, yaxis='y2')\n\n# using the new dates_temp_sum we will create the second trace\ntrace2 = go.Scatter(x=knowledge_comp.EnabledDate, line = dict(color = color_op[3]), \n                    name=\"Knowledge_comps\", \n                    y=knowledge_comp['Id'], opacity = 0.8)\n\n\n##tmp_votes = kernel_versions.groupby('CreationDate')['TotalVotes_version'].sum().reset_index()\n\n##trace3 = go.Scatter(x=tmp_votes.CreationDate, line = dict(color = color_op[6]), \n##                    name=\"Mov.Average 3fds0 Days\",\n##                    y=tmp_votes['TotalVotes_version'], opacity = 0.8)\n\n\nlayout = dict(\n    title= \"Competitions Creation Distribution by Dates\",\n    xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=3, label='3m', step='month', stepmode='backward'),\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(count=12, label='12m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date' ),\n    yaxis=dict(title='Total by the days'),\n    yaxis2=dict(overlaying='y',\n                anchor='x', side='right',\n                zeroline=False, showgrid=False,\n                title='Cumulative Total Comps')\n)\n\n# creating figure with the both traces and layout\nfig = dict(data= [trace, trace1, trace2, #trace3\n                 ],\n           layout=layout)\n\n#rendering the graphs\niplot(fig) ","5669445f":"user_votes = pd.merge(kernel_votes, users[['UserName','DisplayName', \n                                           'RegisterDate','UserId',\n                                           'PerformanceTier' ]],\n                      left_on='UserId', right_on='UserId', how='left')\n\nkerneld_users_id = kernel_versions[['KVersId', 'KernelId', 'AuthorUserId']]\nuser_votes_2 = pd.merge(user_votes[['UserId', 'KernelVersionId',\n                                    'UserName','DisplayName', 'KernVotId',\n                                    'PerformanceTier']], \n                      kerneld_users_id,\n                      left_on='KernelVersionId', right_on='KVersId', how='left')\n\nuser_votes_23 = pd.merge(user_votes_2, users[['UserName','DisplayName', \n                                           'RegisterDate','UserId',\n                                           'PerformanceTier' ]], \n                         left_on='AuthorUserId', right_on='UserId', how='left')","5584a16c":"tmp = user_votes_23[user_votes_23['PerformanceTier_y'] != 'KaggleTeam'].groupby([\"DisplayName_y\",'PerformanceTier_y']).agg({'UserName_x': 'nunique',\n                                                                                                      'KernVotId': 'nunique',\n                                                                                                      'KernelId': 'nunique'}).reset_index()\n\n","ee0f19de":"tmp.rename(columns={'DisplayName_y': 'User_Name',\n                    'UserName_x':'Total_uniques',\n                    'KernVotId': 'Total_Votes',\n                    'PerformanceTier_y': 'Tier',\n                    'KernelId': 'Total_Kernels'}, inplace=True)","a6dfaa33":"tmp['vote_ratio'] = round(tmp['Total_Votes'] \/ tmp['Total_uniques'], 2)\ntmp_uniq_vot = tmp[#(tmp.vote_ratio != np.inf) &\n                     #(tmp.Total_uniques > 10) & \n                     (tmp.Tier.isin(['Expert', 'Master', 'GranMaster']))].sort_values('Total_uniques', ascending=False)[:30]\ntmp_kernl_tot = tmp[#(tmp.vote_ratio != np.inf) &\n                    # (tmp.Total_uniques > 10) & \n                     (tmp.Tier.isin(['Expert', 'Master', 'GranMaster']))].sort_values('Total_Votes', ascending=False)[:30]\ntmp_kernl = tmp[#(tmp.vote_ratio != np.inf) &\n                #     (tmp.Total_uniques > 10) & \n                     (tmp.Tier.isin(['Expert', 'Master', 'GranMaster']))].sort_values('Total_Kernels', ascending=False)[:30]\ntmp_vote_ratio = tmp[(tmp.vote_ratio != np.inf) &\n                     (tmp.Total_uniques > 10) & \n                     (tmp.Tier.isin(['Expert', 'Master', 'GranMaster']))].sort_values('vote_ratio',\n                                                                                       ascending=False).head(30)","8c6f5c9b":"#snsusers['dow_reg'].value_counts()\nplt.figure(figsize=(14,21))\n\nplt.subplot(311)\ng = sns.barplot(x=\"User_Name\", y='Total_uniques',  \n                data=tmp_uniq_vot,  color='sandybrown',\n                   order=list(tmp_uniq_vot['User_Name'].values))\ng.set_title(\"TOTAL UNIQUE VOTES \\nTOP 30 USERS WITH MORE VOTE FROM UNIQUE USERS\", fontsize=20)\ng.set_ylabel(\"Count \",fontsize= 17)\ng.set_xlabel(\"\", fontsize=17)\nsizes=[]\nfor p in g.patches:\n    height = p.get_height()\n    sizes.append(height)\ng.set_ylim(0, max(sizes) * 1.15)\ng.set_xticklabels(g.get_xticklabels(),\n                  rotation=90)\ngt = g.twinx()\ngt = sns.pointplot(x='User_Name', y='vote_ratio', data=tmp_uniq_vot, color='green',\n                   order=list(tmp_uniq_vot['User_Name'].values),\n                   # color='black',\n                   legend=False)\ngt.set_ylim(tmp_uniq_vot['vote_ratio'].min()-.3,tmp_uniq_vot['vote_ratio'].max()*1.1)\ngt.set_ylabel(\"Mean of votes by User\", fontsize=16)\n\nplt.subplot(312)\ng1 = sns.barplot(x=\"User_Name\", y='Total_Votes',  data=tmp_kernl_tot,  color='sandybrown')\ng1.set_title(\"TOTAL VOTES \\nTOP 30 USERS WITH MORE VOTES\", fontsize=20)\ng1.set_ylabel(\"Count \",fontsize= 17)\ng1.set_xlabel(\" \", fontsize=17)\nsizes=[]\nfor p in g1.patches:\n    height = p.get_height()\n    sizes.append(height)\n\ng1.set_ylim(0, max(sizes) * 1.15)\ng1.set_xticklabels(g1.get_xticklabels(),\n                  rotation=90)\ngt = g1.twinx()\ngt = sns.pointplot(x='User_Name', y='vote_ratio', data=tmp_kernl_tot,\n                   order=list(tmp_kernl_tot['User_Name'].values),\n                   color='green', legend=False)\ngt.set_ylim(tmp_kernl_tot['vote_ratio'].min()-.3,\n            tmp_kernl_tot['vote_ratio'].max()*1.1)\ngt.set_ylabel(\"Mean of votes by User\", fontsize=16)\n\n\nplt.subplot(313)\ng2 = sns.barplot(x=\"User_Name\", y='vote_ratio',  \n                 data=tmp_vote_ratio,  color='sandybrown')\ng2.set_title(\"UNIQUE USERS VOTE RATIO \\nTOP 30 Users with the highest mean vote from unique Users\", fontsize=20)\ng2.set_ylabel(\"Count \",fontsize= 17)\ng2.set_xlabel(\"TOP 30 Users\", fontsize=17)\nsizes=[]\nfor p in g2.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g2.text(p.get_x()+p.get_width()\/2.,\n            height + .05,\n            f'{height}',\n            ha=\"center\", fontsize=11)\n\ng2.set_ylim(0, max(sizes) * 1.15)\ng2.set_xticklabels(g2.get_xticklabels(),\n                  rotation=90)\n\nplt.subplots_adjust(hspace = 0.8)\n\nplt.show()","0017e636":"plt.figure(figsize=(14,6))\ng2 = sns.barplot(x=\"User_Name\", y='Total_Kernels',  \n                 data=tmp_kernl,  color='sandybrown',\n                   order=list(tmp_kernl['User_Name'].values))\ng2.set_title(\"KERNELS  <br>TOP 30 USERS WITH MORE KERNELS\", fontsize=20)\ng2.set_ylabel(\"Count \",fontsize= 17)\ng2.set_xlabel(\"TOP 30 Users\", fontsize=17)\nsizes=[]\nfor p in g2.patches:\n    height = p.get_height()\n    sizes.append(height)\n    g2.text(p.get_x()+p.get_width()\/2.,\n            height + 3,\n            '{:1.0f}'.format(height),\n            ha=\"center\", fontsize=11)\ng2.set_ylim(0, max(sizes) * 1.15)\ng2.set_xticklabels(g2.get_xticklabels(),\n                  rotation=90)\ngt = g2.twinx()\ngt = sns.pointplot(x='User_Name', y='vote_ratio', data=tmp_kernl, color='green',\n                   order=list(tmp_kernl['User_Name'].values),\n                   # color='black',\n                   legend=False)\ngt.set_ylim(tmp_kernl['vote_ratio'].min()-.3,tmp_kernl['vote_ratio'].max()*1.1)\ngt.set_ylabel(\"Mean of votes by User\", fontsize=16)\n\nplt.show()","3390892f":"# Explorating Registration Dates details\nAs we can see a Seasonality, let's see if the data shows is  ","9c64e664":"## Users Achievement","1c018ce1":"Cool! <br>At Users Achievement table we have the total of registrations multiplied by 3; <br>\nThis table shows the tier of each Kaggle category and the total medals for each one, date of achievement and points for each category type. <br>\nWe will explore it furthuer later. ","745b983a":"Here we can see informations about all Registrations on Kaggle. ","c7c77e37":"WTF! Kevin have a lot of Kernels... But we have a problem here. When I go to her profile, I can see Kernels(412) where are the other kernels of him? Maybe it's hidden... Someone knows about that?\n<br>\nWe can note that many beasts of DS have more than 100 kernels; \n- ","24ec06c1":"# Kaggle is a Trend?\n\nLet's explore the total of registrations for each day since the launch of the platform.\nWhat we will see:\n- Count\n- Mean\n- Cumulative total Registers\n\nThem try to analyze and understand the patterns, peaks, differences and so on","23030806":"## Crosstab Analysis - Day of the Week by Months","64323be5":"# I'm working at this Kernel, so upvote the kernel and stay tuned\n- Please, DON'T FORGET THE VOTE, IT'S IMPORTANT TO ME GET THE GRANDMASTER TIER!!!! ","b15178b1":"## Util Functions","5a1fbc3c":"# Competitions \nNow, we will explore the competitions:\n- How many competitions we have? \n- Which are the distribution of paid and not paid competitions by the time?\n- What is the distribution of Prizes in paid competitions? \n\nLet's investigate it further. ","8be8cb2a":"# Objective:\nI will explore this interesting that talk about us =D\n\n- How many users are registered on Kaggle?\n- How many of them are actively on the platform?\n- What's the distribution of the tiers?\n- When the plataform reached the mark of 1 million registrations?\n- Which are the mean average rate of registrations by day?\n- Which was the peak of registrations?\n- How long time in mean takes to get the Tiers?\n\nAnd many more interesting questions that probably will raise","07fa3cd4":"Cool! Now, we have:\n\n- Total votes by Users\n- Total Unique votes by Users\n- Total kernels by User\n\nUsing the cols of aggregation, we can also create new metrics to better understand the patterns:\n\nmean_unique_ratio = Unique_user_votes \/ total votes <br>\nkernel_votes_mean = Total_Votes \/ Total_Kernels <br>\nkernel_unique_votes_mean = Unique_user_votes \/ Total_Kernels <br>\n\n","d4ef397e":"Cool! <br>\nThe total of AuthorUserId is 100,631 that is the same number that are shown in the Kernels Rank profile. ","d65fcbf6":"Let's start with an overview of the following tables:\n- kernels \n- users\n- users achievements\n","1f76bc4a":"## Ploting Historical Kernels Creation\n- Let's see the patterns of Kernel Creations by Dates","7f753de8":"\nCool. We can see that only a small part of all users have at least contributor Tier. <br>\nCould us infer that 97.9% of Kaggle community is passive? We can't forget that have many people that visit the platform but don't have already registered.  ","6707f339":"## Performance Tier Distribution at Kaggle\n\nWe have total of 3.4M unique registers in Kaggle. <br>\nBefore we go further at the timeseries to see the registers by the time, Let's see the Performance Tier Distribution;\n","f5b5a25c":"## The Million Days\nOne interesting thing that I see is that:\n- Jun, 11 of 2017 was the day that Kaggle achieved 1 Million registers. \n- Aug, 26 of 2018 achieved 2 Million Registers.\n- May, 19 of 2019 achieved 3 Million Registers.\n\nCool! We are more than 3 million people all over the world. =D","3a82cc1d":"## Welcome to my kernel on Meta Kaggle Dataset\n\nFinally I understood that Meta-Kaggle is a dataset; <br>I ever saw amazing kernels that the source was <b>[Dataset no longer available]<\/b> and only now I became to here. ","5c360928":"## Users summary","8a40858f":"## Reward Type Distribution","3c2a6a1a":"Hey, we have a very important thing happening on the one Million day;  <br>\n\nIn my research to better understand this data I discovered that Kaggle only started the kernels in January, 2016; \n\nWe can see some interesting peaks:\n- Feb 26, 2016 was the first Peak. At this day was created 368 kernels;\n- On May 3, 2017 the second Peak, It had 646 kernels created;\n- The third and fourth peaks are very similar. These ocurred on Feb 13, 2018 and Mar 26, 2018 with 718 and 690 respectively;\n\nIt would be interesting if we can get informations about the One Million day. What happened this day?","21627851":"## <font color=\"red\"> I'm near of grandmaster tier, so, if you find this kernel useful or interesting, please don't forget to upvote the kernel =)<\/font>","7fee9b28":"This chart is amazing. It is enough to tell us a good history of Kaggle growth! <br>\nIt's interesting to note that:\n- Firts peak was on November, 4 of 2011 with 808 user registers. \n- Second peak was on December, 26 of 2013 with 1590 user registers.\n- Third peak on May, 23 of 2016 with 1728 registers isn't so evident because that epoch Kaggle already had almost 1000 registration by day;\n- After 2017 we can note a clear improvement and consolidation of more than 1k of registers each day. \n- The peak at March, 9 of 2017 with 3931 registers I find that is the day that Google announced the Kaggle acquirement.\n- After it we can see that the number of registrations are apparently cyclical, let's explore other informations of user registration. \n\nSummary: We can see a clearly tendency in Kaggle registers, the number of register is very consistent and achieves easy 3k people registers by day with the highest peak at Apr 2019 with 5764 new registers. <br>\nLooking this timeseries of registers I bet that Kaggle will keep receiving so much new data scientists and enthusiasts to the platform. \n\n<i> OFF: An interesting that I discovered is that I joined at Kaggle 11 days before it gets 1M users! =D <\/i>","ece1ed52":"# Kernels\nWhat do we will analyze here?\n- Total of Kernels created through the time\n- How many kernels are created by month? Does this quantity is growing through time?","b2fb6015":"## How many time in mean to get the tiers?","ab223689":"# Importing Libraries","6532d725":"We can't see a very clear pattern considering these date features, but something call my attention. <br>\n\nSome findings about the charts:\n - In days: It's apparently normal... All days of month are very similar less the day 31 that make all sense. \n - In days of Week: We can see that Saturday and Sunday have the smallest % of registration with 10.7 ~ 10.9%;\n - In Months: We can see that December and January are the months with less registrations. \n \nNow, Let's use the crosstab function to see if have any relation between months and week days","c9196440":"Nice!!! Some interesting things to note:<br>\n### Note: In this charts, I'm considering only Experts, Masters and Grandmasters. \nFirst Chart: \n- We can see that the \"Top unique votes\" are SRK with more than 7k votes from unique users. \n- The second and third (Anistropic and DATAI) are very close from each other.\n- We can see interestingly that Andrew (top 1) and Pedro Marcelino (The king of titanic) are almost in a draw;\n\nIn the top 30 users with more votes from unique users, we have:\n- 3 Contributors\n- 2 Experts\n- 14 master \n- 11 grandmaster\n\nWe can note that in this chart the highest ratio total_votes\/unique_votes:<Br>\n- 1.92 of Bojan Tunguz \n- 1.89 of DATAI \n- 1.76 Chris\n\nSo in mean, Bojan received 1.92 votes from each user. \n____________________\nSecond Chart:<br>\nWe have 4 guys with more than 8k votes\n- DATAI have almost 12k votes in total. \n- SRK few more than 10k \n- Andrew with that is the 1\u00ba rank are the third with more votes ~ 9.5k\n\nThis time, we can see that the highest values of the ratio of unique users are: Yury Kashnitsky and NowYSM with ~2.5 votes for each user\n___________________________\n\nThird Chart:<br>\n- We can see that many users have values more than 3 in mean of votes;\n- I noted a common pattern, many of them have a high number of votes but a small value of forks.\n- In general the ratio of Upvotes \/ Forks is higher than 2 ~ 3;\n\n\nSo, the top isn't necessary the guy with more unique votes or with highest diversity of votes.\n\nThe next object is to get the total kernels and divide by the total of unique user votes... I find that it will give us interesting insights. ","c6f12ca3":"Cool! <br>\nIt's a very interesting chart to understand Kaggle competitions profile. <br>\n\nWe can see that:\n- Only 13.69% of all competitions has money prizes. \n- The None values (that is the most common value with 65.10%) we can consider as \"Knowledge\" competition. It's very interesting because sometimes we consider Kaggle a plataform of paid competitions, but the data shows us the truth. \n\nNow, that we have clealy this differentiation of paid or not comps, what about we plot the comps through the time and try understand how it happened;\n","f0e8d3f1":"Interesting. <br>\nIt gives us a table showing the ratio of each day of the week for each month.\n- We can note that the weekends are consistently worst than other days but in May it is even worst.","9ec85ee7":"# Votes in Kernels - Total unique users votes \nThe idea of it is take all votes and combine the tables until we get the total of votes per unique users to Kagglers. \n\n- The objective is understand the distribution of unique users that have voted in a other user\n- We can get an index of purity in the kernel votes or detect posible frauds?\n- How is the result of total kernels and total unique votes received? \n","55c9c723":"# Total votes from unique Users","8fe819b9":"Cool! <br>\nAs I said I don't know exactly if it's really correct. but show us interesting and similar patterns. <br>\nOnly in Gran Master Tier we can see a clear difference in mean days.\n<br>\nI'm trying to do a correctly handling to avoid any type of leakage.<br>\nWe can see that when the time to a person do all things to become a Contributor is in median <br><br>\n\nHow can I be sure that it is correctly?! The Grandmaster of scripts seem to be wrong but I think it is because the outlier guys like Chris Deotte, MH Bahamani and nananshi that have rushed very fast. <br>I will investigate it further.  ","6d336971":"# TOP users with more Kernels\n- Let's explore the users with more kernels","0bcc4154":"## Importing the dataset that we will work at. ","9fb55a29":"Cool!!! \nIt's a good chart to understand the pattern of competitions on Kaggle and their distribution throught the time\n\nWe can note that something has changed after September, 2017.... Maybe new politician by Google or what? <br>\nTo keep at it, I will take a look at prizes distributions.","35192ee6":"## Kernels summary"}}