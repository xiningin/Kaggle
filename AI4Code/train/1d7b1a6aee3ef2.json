{"cell_type":{"495f2bd5":"code","b81c4617":"code","84eb4dc1":"code","074aeae1":"code","ac9729cd":"code","b3859504":"code","c6fc36ae":"code","1fc38858":"code","8ed7ea10":"code","e5da78a4":"code","d641c5a1":"code","bbe4106d":"code","39249b6e":"code","fdfd51c5":"code","18dda9b0":"code","a1b750d8":"code","8c16b43e":"code","11d00fd1":"code","30773963":"code","167308da":"code","eaa5d59e":"code","274df8a8":"code","73c0cadc":"code","3650a37a":"code","2451af3b":"code","c574af67":"code","c6627a7a":"code","5b2a5277":"code","b159f9b3":"code","da7e3d56":"code","35ff2546":"code","56e39d48":"code","145894bc":"code","a945a5be":"code","71135a2b":"markdown","44bd2021":"markdown","60b8cc0f":"markdown","730b0bc8":"markdown"},"source":{"495f2bd5":"!git clone https:\/\/github.com\/huggingface\/transformers # 2.8, kaggle kernel default is 2.7","b81c4617":"!pip install transformers\/","84eb4dc1":"!pip install -r transformers\/examples\/requirements.txt","074aeae1":"!ls ..\/input\/writingprompts-combine-one-line-data-for-gpt2","ac9729cd":"import numpy as np\nimport torch","b3859504":"DATA_PATH = '\/kaggle\/input\/writingprompts-combine-one-line-data-for-gpt2\/'","c6fc36ae":"!head -n 4 {DATA_PATH}train.wp_combined\n","1fc38858":"TRAIN_FILE=DATA_PATH+'valid.wp_combined' # Use valid as train to minimize training time first\nTEST_FILE=DATA_PATH+'test.wp_combined'\nprint(TRAIN_FILE)","8ed7ea10":"!mkdir output\n# !ls","e5da78a4":"import os\n!cp {TRAIN_FILE} . \n!cp {TEST_FILE} . \n!ls","d641c5a1":"\n!python transformers\/examples\/run_language_modeling.py --help","bbe4106d":"!python transformers\/examples\/run_generation.py --help","39249b6e":"# 5 minutes or a bit more\n!git clone https:\/\/github.com\/NVIDIA\/apex\n!pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\/apex\n","fdfd51c5":"# Note : failed option : \n# --line_by_line \\ <-- works in last commit, now have to create our own tokenizer with padding first.\n# fp16, fp16_opt_level=O1 <-- O2 works though\n\n'''\nNote ** TRAIN on Valid data (20 times smaller than the real train)\n** use block_size=300 & fp16 on P100-- if reduce batch_size, use gradient_accum appropriately\ndistilgpt2 (375MB), default batch_size=8 , 8.5min\/epoch ==> Test PPL around 25-27\ngpt2 (750MB), batchsize=8, around 15min\ngpt2-medium (1.5GB), batchsize=4, around 42min ==> Test PPL 16.95 [Eval time 11 mins]\ngpt2-large (3.25GB), batchsize=1, block_size=250 around xxx, we DONT have enough disk space in kaggle\n'''","18dda9b0":"!python transformers\/examples\/run_language_modeling.py \\\n    --model_type=gpt2 \\\n    --model_name_or_path=gpt2-medium \\\n    --per_gpu_train_batch_size=4 \\\n    --gradient_accumulation_steps=2 \\\n    --fp16 \\\n    --fp16_opt_level=O2 \\\n    --block_size=300 \\\n    --do_train \\\n    --train_data_file=\/kaggle\/working\/valid.wp_combined \\\n    --num_train_epochs=1 \\\n    --do_eval \\\n    --eval_data_file=\/kaggle\/working\/test.wp_combined \\\n    --output_dir=output \\\n    --save_total_limit=1 \\\n    --save_steps=5000 \\\n    --cache_dir=output \\\n    --overwrite_cache \\\n    --overwrite_output_dir\n","a1b750d8":"!du output\/* -sh\n!cat output\/eval_results_lm.txt","8c16b43e":"from transformers import GPT2Tokenizer, GPT2LMHeadModel\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2',pad_token='pad')\ntokenizer.save_pretrained('.')\n!ls -sh","11d00fd1":"!cp *.json output\n!ls -sh output\/*.json","30773963":"model = GPT2LMHeadModel.from_pretrained('output')","167308da":"%%time\nids = tokenizer.encode('[ WP ] Aliens have arrived , and ask for a single human to plead humanity case and save them from extinction <endprompts>')\nids = np.array(ids)[np.newaxis]\nprint(ids.shape)\n\nsample_outputs = model.generate(\n    torch.tensor(ids),\n    do_sample=True, \n    max_length=300, # \u0e2a\u0e23\u0e49\u0e32\u0e07 texts \u0e08\u0e33\u0e19\u0e27\u0e19 100 \u0e04\u0e33\n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5 # \u0e43\u0e2b\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07 5 \u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"\\n\"+\"===\"*10)\n  print(\"{}: {}\".format(i+1, tokenizer.decode(sample_output, skip_special_tokens=False)))\n    \n","eaa5d59e":"%%time\nids = tokenizer.encode('[ WP ] Magic exists , but the nature of it is unique to the user , like a fingerprint . <endprompts>',\n                      return_tensors='pt')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=300, # \u0e2a\u0e23\u0e49\u0e32\u0e07 texts \u0e08\u0e33\u0e19\u0e27\u0e19 100 \u0e04\u0e33\n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5 # \u0e43\u0e2b\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07 5 \u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"\\n\"+\"===\"*10)\n  print(\"{}: {}\".format(i+1, tokenizer.decode(sample_output, skip_special_tokens=False)))\n    \n","274df8a8":"%%time\nids = tokenizer.encode('[ WP ] Naruto meets Sasuke in the Jonin exam, and both of them have a serious fight with sand monster. <endprompts>',\n                      return_tensors='pt')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=300, # \u0e2a\u0e23\u0e49\u0e32\u0e07 texts \u0e08\u0e33\u0e19\u0e27\u0e19 100 \u0e04\u0e33\n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5 # \u0e43\u0e2b\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07 5 \u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"\\n\"+\"===\"*10)\n  print(\"{}: {}\".format(i+1, tokenizer.decode(sample_output, skip_special_tokens=False)))\n    \n","73c0cadc":"%%time\nids = tokenizer.encode('[ WP ] It is a romantic story between a guy from downtown and a beautiful girl who is a millionare <endprompts>',\n                      return_tensors='pt')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=300, # \u0e2a\u0e23\u0e49\u0e32\u0e07 texts \u0e08\u0e33\u0e19\u0e27\u0e19 100 \u0e04\u0e33\n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5 # \u0e43\u0e2b\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07 5 \u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"\\n\"+\"===\"*10)\n  print(\"{}: {}\".format(i+1, tokenizer.decode(sample_output, skip_special_tokens=False)))\n    \n","3650a37a":"%%time\nids = tokenizer.encode('[ WP ] Two god-like beings , disguised as old men , play a game of chess on a park bench to decide the final fate of humanity .<endprompts>',\n                      return_tensors='pt')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=300, # \u0e2a\u0e23\u0e49\u0e32\u0e07 texts \u0e08\u0e33\u0e19\u0e27\u0e19 100 \u0e04\u0e33\n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5 # \u0e43\u0e2b\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07 5 \u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"\\n\"+\"===\"*10)\n  print(\"{}: {}\".format(i+1, tokenizer.decode(sample_output, skip_special_tokens=False)))\n    \n","2451af3b":"%%time\nids = tokenizer.encode('[ WP ] Write a happy story about a dog . <endprompts>',\n                      return_tensors='pt')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=300, # \u0e2a\u0e23\u0e49\u0e32\u0e07 texts \u0e08\u0e33\u0e19\u0e27\u0e19 100 \u0e04\u0e33\n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5 # \u0e43\u0e2b\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07 5 \u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"\\n\"+\"===\"*10)\n  print(\"{}: {}\".format(i+1, tokenizer.decode(sample_output, skip_special_tokens=False)))\n    \n","c574af67":"%%time\nids = tokenizer.encode('[ WP ] Scientists found for the first time bacteria which can be alive even without Oxygen . <endprompts>',\n                      return_tensors='pt')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=300, # \u0e2a\u0e23\u0e49\u0e32\u0e07 texts \u0e08\u0e33\u0e19\u0e27\u0e19 100 \u0e04\u0e33\n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5 # \u0e43\u0e2b\u0e49\u0e2a\u0e23\u0e49\u0e32\u0e07 5 \u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\n)\n\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n  print(\"\\n\"+\"===\"*10)\n  print(\"{}: {}\".format(i+1, tokenizer.decode(sample_output, skip_special_tokens=False)))\n    \n","c6627a7a":"!pip install git+https:\/\/github.com\/ssut\/py-googletrans.git\n\nfrom googletrans import Translator\n\ntranslator = Translator()","5b2a5277":"th_stories = ['[ WP ] \u0e40\u0e23\u0e37\u0e48\u0e2d\u0e07\u0e02\u0e2d\u0e07\u0e04\u0e38\u0e13\u0e2b\u0e21\u0e2d\u0e40\u0e08\u0e21\u0e2a\u0e4c \u0e17\u0e35\u0e48\u0e25\u0e31\u0e1a\u0e2b\u0e25\u0e31\u0e07\u0e19\u0e31\u0e49\u0e19\u0e17\u0e33\u0e18\u0e38\u0e23\u0e01\u0e34\u0e08\u0e21\u0e37\u0e14 <endprompts>',\n             '[ WP ] \u0e17\u0e35\u0e21\u0e19\u0e31\u0e01\u0e27\u0e34\u0e17\u0e22\u0e32\u0e28\u0e32\u0e2a\u0e15\u0e23\u0e4c\u0e0a\u0e32\u0e27\u0e40\u0e22\u0e2d\u0e23\u0e21\u0e31\u0e19\u0e41\u0e25\u0e30\u0e0d\u0e35\u0e48\u0e1b\u0e38\u0e48\u0e19\u0e44\u0e14\u0e49\u0e04\u0e49\u0e19\u0e1e\u0e1a\u0e22\u0e32\u0e17\u0e35\u0e48\u0e08\u0e30\u0e21\u0e32\u0e08\u0e31\u0e14\u0e01\u0e32\u0e23 Corona Virus <endprompts>',\n             '[ WP ] \u0e23\u0e32\u0e04\u0e32\u0e19\u0e49\u0e33\u0e21\u0e31\u0e19\u0e15\u0e01\u0e15\u0e48\u0e33\u0e2b\u0e19\u0e31\u0e01\u0e40\u0e1b\u0e47\u0e19\u0e1b\u0e23\u0e30\u0e27\u0e31\u0e15\u0e34\u0e01\u0e32\u0e23\u0e13\u0e4c <endprompts>',\n              '[ WP ] \u0e19\u0e31\u0e01\u0e27\u0e34\u0e17\u0e22\u0e32\u0e28\u0e32\u0e2a\u0e15\u0e23\u0e4c\u0e44\u0e14\u0e49\u0e04\u0e49\u0e19\u0e1e\u0e1a\u0e41\u0e23\u0e48\u0e18\u0e32\u0e15\u0e38\u0e43\u0e2b\u0e21\u0e48 vibranium \u0e17\u0e35\u0e48\u0e21\u0e35\u0e04\u0e27\u0e32\u0e21\u0e17\u0e19\u0e17\u0e32\u0e19\u0e2a\u0e39\u0e07\u0e01\u0e27\u0e48\u0e32\u0e40\u0e1e\u0e0a\u0e23 <endprompts>',\n              '[ WP ] \u0e19\u0e31\u0e01\u0e27\u0e34\u0e17\u0e22\u0e32\u0e28\u0e32\u0e2a\u0e15\u0e23\u0e4c\u0e44\u0e14\u0e49\u0e04\u0e49\u0e19\u0e1e\u0e1a\u0e22\u0e39\u0e19\u0e34\u0e04\u0e2d\u0e23\u0e4c\u0e19\u0e41\u0e25\u0e30\u0e21\u0e31\u0e07\u0e01\u0e23\u0e40\u0e1b\u0e47\u0e19\u0e04\u0e23\u0e31\u0e49\u0e07\u0e41\u0e23\u0e01\u0e02\u0e2d\u0e07\u0e42\u0e25\u0e01\u0e43\u0e19\u0e1b\u0e48\u0e32\u0e25\u0e36\u0e01\u0e2d\u0e40\u0e21\u0e0b\u0e2d\u0e19!! <endprompts>',\n             ]","b159f9b3":"%%time\n\n'''\u0e08\u0e23\u0e34\u0e07\u0e46 \u0e1a\u0e31\u0e49\u0e01\u0e19\u0e34\u0e14\u0e2b\u0e19\u0e48\u0e2d\u0e22 \u0e15\u0e49\u0e2d\u0e07 concat [WP] <endprompt> \u0e44\u0e1b\u0e17\u0e35\u0e2b\u0e25\u0e31\u0e07\u0e01\u0e32\u0e23 translate'''\n\norig_input=th_stories[0]\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='pt')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=300, \n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e17\u0e35\u0e48{}: {}\\n\".format(i+1, new_out))\n    \n","da7e3d56":"%%time\norig_input=th_stories[1]\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='pt')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=300, \n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e17\u0e35\u0e48{}: {}\\n\".format(i+1, new_out))\n    \n","35ff2546":"%%time\norig_input=th_stories[2]\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='pt')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=300, \n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e17\u0e35\u0e48{}: {}\\n\".format(i+1, new_out))\n    \n","56e39d48":"%%time\norig_input=th_stories[3]\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='pt')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=300, \n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e17\u0e35\u0e48{}: {}\\n\".format(i+1, new_out))\n    \n","145894bc":"%%time\norig_input=th_stories[4]\nnew_input = translator.translate(orig_input, dest='en').text\n\nids = tokenizer.encode(new_input, return_tensors='pt')\n\nsample_outputs = model.generate(\n    ids,\n    do_sample=True, \n    max_length=300, \n    top_k=55, \n    top_p=0.925, \n    num_return_sequences=5\n)\nprint(\"Input: %s\\n\\n\" % orig_input)\nprint(\"Output:\\n\" + 100 * '-')\nfor i, sample_output in enumerate(sample_outputs):\n    out = tokenizer.decode(sample_output, skip_special_tokens=True)\n    new_out = translator.translate(out, dest='th').text\n    print(\"\\n\"+\"===\"*10)\n    print(\"\u0e15\u0e31\u0e27\u0e2d\u0e22\u0e48\u0e32\u0e07\u0e17\u0e35\u0e48{}: {}\\n\".format(i+1, new_out))\n    \n","a945a5be":"'''\nApparently, at the time of my coding, Huggingface is updating their repo, and run_generation.py cannot be used at the moment\nSo, I have to code by myself\n'''\n\n# !python transformers\/examples\/run_generation.py \\\n#     --model_type=gpt2 \\\n#     --model_name_or_path=output\/ \\\n#     --length=300\\\n#     --k=50 \\\n#     --repetition_penalty=1.2 \\\n#     --num_return_sequences=5 \\\n#     --prompt='[ WP ] Aliens have arrived , and ask for a single human to plead humanity case and save them from extinction <endprompts>'\n","71135a2b":"-  V6. fine tuned DistilGPT2 , line-by-line training\n- V8. fine tuned GPT2-Medium , NO-line-by-line training, fp16\n- V13 ==> V8+\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22","44bd2021":"# start training","60b8cc0f":"# \u0e17\u0e14\u0e2a\u0e2d\u0e1a\u0e20\u0e32\u0e29\u0e32\u0e44\u0e17\u0e22","730b0bc8":"# Introduction\n\nThis notebook show how can we improve GPT-2 to generate a \"better\" stories by finetuning with \"WritingPrompts\" dataset.\n\nWritingPrompts is a dataset published by Facebook AI Research intending to make machines able to compose a long and consistent story. For more details see : \n\nhttps:\/\/www.kaggle.com\/ratthachat\/writing-prompts\n\nI also preprocess WritingPrompts dataset to be easily used here : https:\/\/www.kaggle.com\/ratthachat\/writingprompts-combine-one-line-data-for-gpt2\n\nIn this notebook, I use gpt2-medium since gpt2-large will consume too much disk space more than Kaggle provides us.\nIt's amazing to be able to use gpt2-medium though. With default setting, we can use only distil-gpt2 which is 5 times smaller and much less capable (see V.6 of this notebook)\n\nCombining with HuggingFace's amazing ability to use top-K and top-P sampling easily,in this notebook, I think we got quite satisfactory results :)\n\n**Note**  \n- Here, I use \"valid data\" which is 16 times smaller than training data to finetune, with only 1 epoch. The total finetuning time is around 1 hour (including evaluating perplexity in test set). Perplexity on test set for 1 epoch is 16.95 (for Benchmark, distilGPT2 no finetune got around PPL 42, I expect (guess) gpt2-medium no finetune to have around PPL 30)\n\n- If we use full training data, it will exceed Kaggle's kernel running time (you are free to use it partially though to make much better results than shown in this kernel!)\n\n- the current version is the \"quick save\" version from interactive kernel so that we don't have to waste our precious GPU time twice :D\n\n- I work this mini-project for fun beside the fierce competitions we usually have!"}}