{"cell_type":{"16c8f74e":"code","1ba8a7f6":"code","f2d7c96a":"code","10624da3":"code","5edec4ac":"code","2fa8c366":"code","76f23704":"code","e26923ff":"code","09b76fe9":"code","5bf3401a":"code","f3dc83ba":"code","1d8be7f5":"code","3d2948ca":"code","8b837480":"code","1c5c1658":"code","fa0141d9":"code","d3b99d68":"code","aa40e94b":"code","50648bd9":"code","240614a2":"code","91d4008c":"code","8df4ad8c":"code","4545364c":"code","b3a93b98":"code","14941688":"code","2ed66473":"code","c22d4d12":"code","ccff5bac":"code","236dafa3":"code","f7db5d1a":"code","ce73df5c":"code","29b0c239":"code","aae28073":"code","0eb907c7":"code","a09f7026":"code","985adc8d":"code","622d8187":"code","84cc8fc4":"code","a6abf753":"code","2f51b984":"code","0050e50a":"code","1a22a452":"code","a2a63b75":"code","077478f7":"code","b538d123":"code","d43b61a2":"code","b3c28317":"code","2b1aa10b":"code","21783db2":"code","55c49130":"code","864887d4":"code","6459d322":"code","c7b6f1ee":"code","1d5aa604":"code","df6fd49c":"code","d04ba1cb":"code","400b0082":"markdown","d3823786":"markdown","a6c68c5d":"markdown","33c30773":"markdown","a8981d29":"markdown","72d46c60":"markdown","02512f1b":"markdown","5249979e":"markdown","b51ce235":"markdown","2da71ac0":"markdown","c2c62a7e":"markdown","7bb2feaa":"markdown","8939be77":"markdown","184785b4":"markdown","f97446d6":"markdown","1d8b942c":"markdown","b3acbbef":"markdown","a411eae4":"markdown","3f4c8841":"markdown","0cdc39a9":"markdown"},"source":{"16c8f74e":"! conda install -c conda-forge gdcm -y","1ba8a7f6":"import pandas as pd\nimport numpy as np\nimport re\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport cv2\nimport albumentations as A\nfrom tqdm.notebook import tqdm\n\nfrom fastai.vision.all import *\nfrom fastai.medical.imaging import *","f2d7c96a":"from matplotlib.pyplot import figure\nfigure(figsize=(12, 10), dpi=80)","10624da3":"path = Path('..\/input\/siim-covid19-detection')","5edec4ac":"path.ls()","2fa8c366":"path_train_image = path\/'train'\npath_test_image = path\/'test'\nss = pd.read_csv(path\/'sample_submission.csv')\ntrain_image = pd.read_csv(path\/'train_image_level.csv')\ntrain_study = pd.read_csv(path\/'train_study_level.csv')","76f23704":"train_study.head()","e26923ff":"#number of individual studies\nlen(train_study)","09b76fe9":"#sanity check to make sure no repeats for individual studies\nassert len(train_study) == train_study.id.nunique()","5bf3401a":"class_dist = train_study.iloc[:,1:].sum(axis=0); class_dist\/sum(class_dist)","f3dc83ba":"plt.bar(class_dist.index, class_dist.values)\n\nfor x, y in zip(class_dist.index, class_dist.values):\n    plt.text(x, y+50, f'{y\/len(train_study)*100:.3}%', fontsize=14)\n\nplt.tight_layout()\nplt.show()","1d8be7f5":"# are there studies with more than one label? No\n(train_study.iloc[:, 1:].sum(1) > 1).sum()","3d2948ca":"train_image.head(5)","8b837480":"# there are more images than study. This is because one study can have more than one image.\nlen(train_image)","1c5c1658":"# checks if the number of studies in the image level is same as the train_study. Yes, it is\nassert train_image.StudyInstanceUID.nunique() == len(train_study)","fa0141d9":"numimagesperstudy = train_image.groupby('StudyInstanceUID').count()['id'].sort_values(ascending=False)","d3b99d68":"numimagesperstudycounts = numimagesperstudy.value_counts(); numimagesperstudycounts","aa40e94b":"x = plt.bar(numimagesperstudycounts.index, numimagesperstudycounts.values)\n\nfor x, y in zip(numimagesperstudycounts.index, numimagesperstudycounts.values):\n    plt.text(x, y+50, f'{y}', fontsize=14)\n\nplt.tight_layout()\nplt.show()","50648bd9":"numimagesperstudy[numimagesperstudy >8].index[0]","240614a2":"train_image[train_image['StudyInstanceUID'] == numimagesperstudy[numimagesperstudy >8].index[0]]","91d4008c":"train_study[train_study['id'] == '0fd2db233deb_study']","8df4ad8c":"train_image['n_boxes'] = train_image['boxes'].apply(lambda x: sum(1 for _ in re.findall('width', str(x))))","4545364c":"n_boxes = train_image['n_boxes'].value_counts(); n_boxes\/sum(n_boxes)*100","b3a93b98":"x = plt.bar(n_boxes.index, n_boxes.values)\n\nfor x, y in zip(n_boxes.index, n_boxes.values):\n    plt.text(x, y+50, f'{y\/len(train_image)*100:.3}%', fontsize=14)\n\nplt.tight_layout()\nplt.show()","14941688":"train_image['label_only'] = train_image['label'].apply(lambda x: x.split()[0])","2ed66473":"labels = train_image['label_only'].value_counts()","c22d4d12":"x = plt.bar(labels.index, labels.values)\n\nfor x, y in zip(labels.index, labels.values):\n    plt.text(x, y+50, f'{y\/len(train_study)*100:.3}%', fontsize=14)\n\nplt.tight_layout()\nplt.show()","ccff5bac":"train_study.columns = ['StudyInstanceUID', 'Negative for Pneumonia', 'Typical Appearance',\n       'Indeterminate Appearance', 'Atypical Appearance']","236dafa3":"train_image['StudyInstanceUID'] = train_image['StudyInstanceUID'].apply(lambda x: f'{x}_study')","f7db5d1a":"df = train_image.merge(train_study, on='StudyInstanceUID')","ce73df5c":"df.head()","29b0c239":"dcm_fns = get_dicom_files(path_train_image)","aae28073":"samp = dcm_fns[0].dcmread()","0eb907c7":"samp","a09f7026":"#to generate dicom metadata for train images\n#df_dcm = pd.DataFrame.from_dicoms(dcm_fns, px_summ=True)\n#df_dcm.to_csv('COVID_dcm_metadata.csv')","985adc8d":"#to generate dicom metadata for test images\n#dcm_test_fns = get_dicom_files(path_test_image)\n#df_dcm_test = pd.DataFrame.from_dicoms(dcm_test_fns, px_summ=True)\n#df_dcm_test.to_csv('COVID_dcm_metadata_test.csv')","622d8187":"df_dcm = pd.read_csv('..\/input\/covid-dataframes\/COVID_dcm_metadata.csv')\ndf_dcm_test = pd.read_csv('..\/input\/covid-dataframes\/COVID_dcm_metadata_test.csv')","84cc8fc4":"df_dcm.columns","a6abf753":"# Let's look at the modality\nprint(f\"{df_dcm['Modality'].value_counts()}\\n\\n{df_dcm_test['Modality'].value_counts()}\")","2f51b984":"print(f\"{df_dcm['PatientSex'].value_counts()}\\n\\n{df_dcm_test['PatientSex'].value_counts()}\")","0050e50a":"print(f\"{df_dcm['PhotometricInterpretation'].value_counts()}\\n\\n{df_dcm_test['PhotometricInterpretation'].value_counts()}\")","1a22a452":"print(f\"{df_dcm['PixelRepresentation'].value_counts()}\\n\\n{df_dcm_test['PixelRepresentation'].value_counts()}\")","a2a63b75":"df_dcm['BitsAllocated'].value_counts()","077478f7":"#the following codes are from https:\/\/www.kaggle.com\/tanlikesmath\/siim-covid-19-detection-a-simple-eda\n\ndef dicom2array(path, voi_lut=True, fix_monochrome=True):\n    dicom = pydicom.read_file(path)\n    # VOI LUT (if available by DICOM device) is used to\n    # transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data\n        \n    \ndef plot_img(img, size=(7, 7), is_rgb=True, title=\"\", cmap='gray'):\n    plt.figure(figsize=size)\n    plt.imshow(img, cmap=cmap)\n    plt.suptitle(title)\n    plt.grid(False)\n    plt.axis('off')\n    plt.show()\n\n\ndef plot_imgs(imgs, cols=3, size=7, is_rgb=True, title=\"\", cmap='gray', img_size=(500,500), label=[]):\n    rows = len(imgs)\/\/cols + 1\n    fig = plt.figure(figsize=(cols*size, rows*size))\n    for i, img in enumerate(imgs):\n        if img_size is not None:\n            img = cv2.resize(img, img_size)\n        fig.add_subplot(rows, cols, i+1)\n        plt.imshow(img, cmap=cmap)\n        plt.title(label[i])\n        plt.grid(False)\n        plt.axis('off')\n\n    plt.suptitle(title)\n    plt.show()","b538d123":"plot_img(dicom2array(dcm_fns[0]))","d43b61a2":"imgs_path = get_dicom_files(path_train_image\/'0fd2db233deb')\nimgs_id = [f\"{str(img).split('\/')[-1].split('.dcm')[0]}_image\" for img in imgs_path]\nimgs_label = list(df[df['id'].isin(imgs_id)]['label'].apply(lambda x: x.split()[0]).values)","b3c28317":"plot_imgs([dicom2array(img) for img in imgs_path], label=imgs_label)","2b1aa10b":"df.head(2)","21783db2":"path_list = []\nimage_list = []\nsplits = []\n\nfor split in ['train']:   \n    for dirname, _, filenames in tqdm(os.walk(f'..\/input\/siim-covid19-detection\/{split}')):\n        for file in filenames:\n            fullpath = dirname + '\/' + file\n            path_list.append(fullpath)\n            image_list.append(file)\n            \ntemp_df = pd.DataFrame(image_list, columns =['image_id'])\ntemp_df['image_path'] = path_list","55c49130":"size=512\ntransform = A.Compose(\n    [\n        A.Resize(height = size , width = size, p=1),\n    ], \n    p=1.0,  bbox_params=A.BboxParams( format='pascal_voc', min_area=0,  min_visibility=0, label_fields=['labels']  ))        ","864887d4":"df['image_id']  = df['id'].apply(lambda s: s.replace('_image','') + '.dcm')\ndf = pd.merge(left = df, right = temp_df, on = 'image_id')","6459d322":"!mkdir train512","c7b6f1ee":"OUTPUT_DIRECTORY = Path('.\/train512')","1d5aa604":"img_list = []\nlabel_list = []\n\n# loop over files\nfor ii in tqdm(range(len(df)), total=len(df)):\n    # get the image\n    row = df.loc[ii]\n    img_path = row['image_path']\n    img = dicom2array(path=img_path)\n    newname = img_path.split('\/')[-1].replace('dcm', 'jpg')\n    img_list.append(newname)\n    \n    # get the bounding boxes\n    bboxes = []\n    bbox = []\n    labels = []\n    confidences = []\n\n    for i, l in enumerate(row['label'].split(' ')):\n        if (i % 6 == 0) :\n            labels.append(l)\n        if (i % 6 == 1):\n            confidences.append(l)\n        if (i % 6 > 1):\n            bbox.append(np.clip(float(l), a_min = 0, a_max = None ))\n        if i % 6 == 5:\n            bboxes.append(bbox)\n            bbox = []    \n\n    # transform both\n    result = transform(image = img, bboxes = bboxes, labels = np.ones(len(bboxes)))\n    new_image = result['image']\n    new_bboxes = np.array(result['bboxes']).tolist()\n\n    # format the output\n    # print('orig label: ' + row['label'])\n    newlabel = ''\n    if labels[0] == 'none':\n        newlabel = 'none 1 0 0 1 1'\n    else:\n        for j in range(len(labels)):\n            newlabel += labels[j] + ' ' + confidences[j] + ' ' +  ' '.join([str(np.round(f,5)) for f in new_bboxes[j]]) + ' '\n    #print('new label:' + newlabel)\n    label_list.append(newlabel)\n    \n    # store the new image\n    cv2.imwrite(str(OUTPUT_DIRECTORY\/newname), new_image)","df6fd49c":"# store the new boxes with image_ids\nxmeta = pd.DataFrame(img_list, columns =['image_id'])\nxmeta['label'] = label_list\nxmeta.to_csv('bounding_boxes512.csv', index = False)","d04ba1cb":"# wrap it up\n!zip -rm -qq rescaled_with_bb512.zip train512 bounding_boxes_512.csv","400b0082":"# Understanding train_study DF","d3823786":"MONOCHROME1\nPixel data represent a single monochrome image plane. The minimum sample value is intended to be displayed as white after any VOI gray scale transformations have been performed. \n\nMONOCHROME2\nPixel data represent a single monochrome image plane. The minimum sample value is intended to be displayed as black after any VOI gray scale transformations have been performed.","a6c68c5d":"Some are 16bits while others are 8bits","33c30773":"The train images are stored in this format\n```\n--root [..\/input\/siim-covid19-detection\/train]\n    |--StudyID01\n        |--ImageID01\n        |--ImageID02\n    |--StudyID02\n        |--ImageID01\n        |--ImageID02\n```","a8981d29":"At study level, the label is 'Intermediate Appearace'.\n\nNow, lets take a look at the distribution of number of bboxes per image.","72d46c60":"Most images have <= 2 bboxes. ","02512f1b":"We have to consider this when we are preparing the dataset.","5249979e":"Let's merge the dataframes. ","b51ce235":"70.9% of the images have label opacity.","2da71ac0":"# DICOM Metadata","c2c62a7e":"All dicom are `unsigned`. ","7bb2feaa":"# Understanding train_image DF","8939be77":"The following codes are from this wonderful [notebook](https:\/\/www.kaggle.com\/konradb\/diy-rescaled-images-with-bboxes\/output).","184785b4":"Let's create dicom metadata df. This takes a while hence I generated and uploaded the csv.","f97446d6":"# Resize and create Smaller images for prototyping","1d8b942c":"Most studies have only one image. About 230 studies have more than one image. There is one study with 9 images. Lets take a look at this.","b3acbbef":"Let's take a look at the 9 images from study - `0fd2db233deb`","a411eae4":"This is interesting. Out of the 9 images, only one has a bbox. Other images have label none. Let's take a look at the study level label for this.","3f4c8841":"Number of classes = 4","0cdc39a9":"They all look same to me :("}}