{"cell_type":{"925177c4":"code","00cb1545":"code","755bfc1f":"code","c5b1fe49":"code","a885115c":"code","8e805801":"code","b0c60147":"code","6d87c697":"code","316a6d10":"code","f41b1ba5":"code","a344c73c":"code","4b3dfe9f":"code","6f83081d":"code","620e1073":"code","8d3d515b":"code","58e275d4":"code","0a697c49":"code","8bb30450":"code","ba2cf473":"code","2a223095":"code","19a8c246":"code","39cd5b16":"code","2566503b":"code","d272c6af":"code","ef4905ce":"code","ddc2d323":"code","ec81be31":"code","876cdb79":"code","9c303147":"code","1454d2be":"code","0aa7c4fe":"code","2f0d4aa6":"code","feffa763":"code","551ff7a5":"code","e7d551c0":"code","6a7988bb":"code","71d3851f":"code","9d5a910a":"code","d0c7db27":"markdown","6d1b43dd":"markdown","b622f70f":"markdown","8de6414a":"markdown","58e03478":"markdown","5314e4fb":"markdown","b1d78117":"markdown","c0555cf8":"markdown","29b983e5":"markdown","bc3a6844":"markdown","aaf38260":"markdown","583dffff":"markdown","d1c011e6":"markdown","39486e43":"markdown","98706330":"markdown","f4b3c0ea":"markdown","6327ad95":"markdown","f0b9e42c":"markdown","5b511c59":"markdown","b2481822":"markdown","6b6c70b1":"markdown","cbe9e077":"markdown","eb5d5c6b":"markdown","b9757aeb":"markdown","3517201f":"markdown","0464a675":"markdown","5bfbce64":"markdown","4b834c8d":"markdown","60fd82bd":"markdown","ae7a577e":"markdown","b1685f53":"markdown","bd61b915":"markdown","f2124d85":"markdown","328ce65d":"markdown","cf7f7906":"markdown"},"source":{"925177c4":"# Anaonda installition\n#!wget https:\/\/repo.anaconda.com\/archive\/Anaconda3-5.2.0-Linux-x86_64.sh && bash Anaconda3-5.2.0-Linux-x86_64.sh -bfp \/usr\/local","00cb1545":"# MiniConda installition\n#!wget https:\/\/repo.continuum.io\/miniconda\/Miniconda3-4.5.4-Linux-x86_64.sh && bash Miniconda3-4.5.4-Linux-x86_64.sh -bfp \/usr\/localv","755bfc1f":"# Test installition\n#!conda info --all\n#!conda list","c5b1fe49":"# To install PyTorch you can use following command\n#!conda install pytorch cpuonly -c pytorch -y","a885115c":"# Import PyTorch\nimport torch","8e805801":"t_Float = torch.FloatTensor([6.])\nt_Double = torch.DoubleTensor([19.23])\nt_Int = torch.IntTensor([0.])\nt_Bool = torch.BoolTensor([True])\n\nt_Float, t_Double, t_Int, t_Bool","b0c60147":"t_Float = torch.tensor([6.], dtype=torch.float32)\nt_Double = torch.tensor([19.23], dtype=torch.float64)\nt_Int = torch.tensor([0.], dtype=torch.int32)\nt_Bool = torch.tensor([True], dtype=torch.bool)\n\nt_Float, t_Double, t_Int, t_Bool","6d87c697":"# Example 1 - Constant Tensor\nt0 = torch.tensor(6.)  # dtype=float32 by default\nt0","316a6d10":"# Example 2 - Vector (1 dimensional tensor) with 3 elements\nt1 = torch.tensor([1., 2., 3.], dtype=torch.float64)\nt1","f41b1ba5":"# Example 3 - 2 by 4 matrix (2 dimensional tensor) with 4 element in each dimension\nt2 = torch.tensor([[-1., -2., -3., -4.],\n                   [-5., -6., -7., -8.]], dtype=torch.float64)\nt2","a344c73c":"# Example 4 - 3 dimensional tensor\nt3 = torch.tensor([[[0., 2., 4.],\n                    [6., 8., 10.]],\n                   [[-10., -8., -6.],\n                    [-4., -2., 0.]],\n                   [[0., 2., 4.],\n                    [6., 8., 10.]]], dtype = torch.float64)\nt3","4b3dfe9f":"# Example 5 - 4 dimensional tensor\nt4 = torch.tensor([[[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n                   [[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n                   [[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n                   [[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]]], dtype = torch.float64)\nt4","6f83081d":"# Example 7 - 4 dimensional GPU tensor\nt4_GPU = torch.cuda.FloatTensor([[[[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n                                 [[[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n                                 [[[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n                                 [[[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]]])\nt4_GPU","620e1073":"cuda0 = torch.device('cuda:0')\ncuda0","8d3d515b":"# Example 6 - 4 dimensional GPU tensor\nt4_GPU = torch.tensor([[[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n                       [[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n                       [[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n                       [[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]]], dtype = torch.float64, device=cuda0)\n\nt4_GPU","58e275d4":"t4.device, t4_GPU.device","0a697c49":"import numpy as np\n\nmx = np.array([[23, 67, 43],  # Define a multi dimensional numpy array\n               [91, 88, 64], \n               [38, 35, 76], \n               [13, 43, 37], \n               [69, 56, 70]], dtype='float32')\n\nt = torch.from_numpy(mx)\ntype(mx), type(t)","8bb30450":"mx = t.numpy()\ntype(mx)","ba2cf473":"print(\"Shape of tensor t0: {}\\n\\\nShape of tensor t1: {}\\n\\\nShape of tensor t2: {}\\n\\\nShape of tensor t3: {}\\n\\\nShape of tensor t4: {}\\n\".format(t0.shape, t1.shape, t2.shape, t3.size(), t4.size()))","2a223095":"print(\"Dimension of tensor t0: {}\\n\\\nDimension of tensor t1: {}\\n\\\nDimension of tensor t2: {}\\n\\\nDimension of tensor t3: {}\\n\\\nDimension of tensor t4: {}\\n\".format(t0.ndim, t1.ndim, t2.ndim, t3.dim(), t4.dim()))","19a8c246":"t2_changed = t2.reshape(1, 8)\nt2, t2.shape, t2_changed, t2_changed.shape","39cd5b16":"t2_changed = t2.t()\nt2, t2.shape, t2_changed, t2_changed.shape","2566503b":"# Normal dimension order is 0th dimension, 1st dimension and 2nd dimension (0, 1, 2)\nt3_changed = t3.permute(0, 2, 1)  # Set dimension order as 0th dimension, 2nd dimension and 1st dimension (0, 2, 1)\nt3, t3.shape, t3_changed, t3_changed.shape","d272c6af":"t4.flatten(), t4.flatten().shape, t4.view(-1), t4.view(-1).shape, t4.reshape(-1), t4.reshape(-1).shape","ef4905ce":"zeros = torch.zeros([2, 3], dtype=torch.int32)\nones = torch.ones([2, 3], dtype=torch.int32)\nrandom = torch.randn([2, 3], dtype=torch.float32)\n\nzeros, ones, random","ddc2d323":"random = torch.randn([3, 5], dtype=torch.float32, requires_grad=True)\nresult = random.pow(3).sum()  # Take \"random\" tensor to the element wise power 3 and sum all of the elements\nprint(\"Randomly initialized tensor: {}\\n\\n\\Resultant tensor after pow by 3 and sum of all elements: {}\".format(random, result))","ec81be31":"result.backward()  # Perform back propagation\nrandom.grad  # Print gradients of \"result\" with respect to \"random\"","876cdb79":"random.grad.zero_()  # Reset the gradients at the memory of tensor","9c303147":"x = torch.randn(5)\ny = torch.randn(5)\nx, y","1454d2be":"tAdd = torch.add(x, y)  # x + y\ntSub = torch.sub(x, y)  # x- y\ntMul = torch.mul(x, 5)  # x * 5 (element wise multiplication)\ntDiv = torch.div(x, 5)  # x \/ 5\ntAdd, tSub, tMul, tDiv","0aa7c4fe":"mx_1 = torch.randn(3, 5)  # 1st multiplier\nmx_2 = torch.randn(5, 2)  # 2nd multiplier\nmul = mx_1@mx_2\nmul","2f0d4aa6":"torch.cos(x), torch.sin(x)","feffa763":"inp_1 = torch.tensor([True, True, False], dtype=torch.int16)\ninp_2 = torch.tensor([False, True, False], dtype=torch.int16)\ninp_1, inp_2","551ff7a5":"tNot = torch.bitwise_not(inp_1, out=None)\ntAnd = torch.bitwise_and(inp_1, inp_2, out=None)\ntOr = torch.bitwise_or(inp_1, inp_2, out=None)\ntXor = torch.bitwise_xor(inp_1, inp_2, out=None)\ntNot, tAnd, tOr, tXor","e7d551c0":"tens = torch.randn(1, 10)  # Create a random tensor\ntens","6a7988bb":"inx, value = torch.max(tens, dim=1)\nprint(\"Index of the maximum value: {}, maximum value: {}\".format(inx, value))","71d3851f":"# Bounded between 0 to 10\ntorch.clamp(tens, 0, 10, out=None)  # tens is randomly initialized tensor from previous cells","9d5a910a":"# Vector tensor started from 0 to 100 with step size of 5\ntorch.linspace(0,100,steps=5,dtype=torch.int32)","d0c7db27":"One other way to define a tensor is using `tensor` function. We can indicate data type of the tensor by simply adding the data type `dtype=X` flag. Default data type for the `tensor` function is float32. Examples of tensors can be seen below.\n\nNOTE: You can not use string data types in tensors!","6d1b43dd":"Some of the logical operations.","b622f70f":"Let's create two boolean tensors for logical operations.","8de6414a":" `max()` is widely used in deep neural network models, this makes it one of the most important function in this notebook. It is basically takes a tensor as an argument, and takes the highest number within the tensor and returns number's itself with that number's index. Follow the example for better understanding.","58e03478":"`max()` function is widely used with `cross_entropy()` and softmax activation function since the softmax returns probabilistic values, `max()` function is very usefull to grab the highest probability from sofmax's output.","5314e4fb":"Anaconda is not preinstalled in the default configuration. We should install Anaconda to install PyTorch. Follow Setup section to install prerequisties it. Follow Setup section to install prerequisties it.","b1d78117":"### Tensor Examples with Different Dimensions","c0555cf8":"## Environment\nIn this series of articles, I will use Kaggle as an environment, however you are free to choose another platforms such as Google Colab, Binder, Amazon or your local machine. Kaggle is an invaluable free platform for AI and Data Science applications, tutorials and competitions developed and maintained by Google while a Jupyter Notebook frontend running on Google\u2019s servers. You can get GPU\/TPU support for more powerful computing. It is completely free. Kaggle comes with some preinstalled configuration, probably with tools prefered by Google. Eventhough, some of the librariers and frameworks are not preinstalled in the default configuration so if your needs are not present on Kaggle you should install them by yourself.","29b983e5":"Taking transpose of the matrix tensor using `t()`:","bc3a6844":"Even though PyTorch performs all operations using tensors, sometimes datasets comprise of numpy arrays, in such situations we have to transform numpy arrays to PyTorch tensors. We can transform numpy arrays to the tensors via `from_numpy()` function.","aaf38260":"Some of the trigonometric operations.","583dffff":"Please note that, in our random tensor there are randomly initialized numbers and one of them is has the highest value. The number with the highest value is \"1.8813\" and the index of this number is 8. `max()` function return these two values. See below.","d1c011e6":"We mentioned GPU feature of Kaggle before. Also we talked about GPU tensors. We can store our tensors in GPU. We can use cuda function to use GPU tensors.\n\nNOTE: To be able to execete operations on GPU, you should set your accelarator as GPU.","39486e43":"## 4 - Max Function","98706330":"Tensors can be initialized with various numeric options like zeros, ones or random numbers. To initialize tensor with zeros, we can use `zeros()`, for ones we can use `ones()`, for random numbers we can use `randn()`. All of these functions takes dimension of tensor as arguments.","f4b3c0ea":"To see the shape or size of the tensors we can use two seperate function: `shape` and `size()`. These two are executing the same task, only difference is `size()` is a callable object.","6327ad95":"Alternatively we can use device flag. To do this run the command below. It is simply assign GPU device into a variable. After assigning the GPU device into a variable, we need to set `device=X` flag with our variable in tensor definitions.","f0b9e42c":"To see dimension of the tensors we can use two seperate function: `ndim` or `dim()` functions. These two are executing the same task, only difference is `dim()` is a callable object.","5b511c59":"Also we can transform tensors back to numpy arrays via `numpy()` function.","b2481822":"## Setup","6b6c70b1":"## 5 - Bounded and Stepped Tensors\n\n","cbe9e077":"\nTo create a stepped vector tensor into the range of [start, end] we can use `linspace` function.\n\nUsage: *torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)*\n","eb5d5c6b":"Changing the shape of the tensor using `reshape()`:","b9757aeb":"To bound all elements in an input tensor into the range of [min, max] we can use `clamp` function.\n\nUsage: *torch.clamp(input, min, max, out=None)*\n\n","3517201f":"Please note that `mul()` function performs element wise multiplication. To perform matrix multiplication, we can use `@` operator.\n\nA quick linear algebra trick: to be able to succes matrix multiplication, number of columns of 1st multiplier should be equal to number of rows of the 2nd multiplier.","0464a675":"### 1. Tensors: Data Types, Cuda Feature, Shape, Reshape, View, Ones, Zeros, Randn\n\nPyTorch performs all its operations using tensors. Tensor is general term for vector and matrix representation. You can represent multi dimensional matrices with tensors. Every tensor is a matrix, but vice versa is not valid.\n\nTensors are seperated into two categories: CPU tensors and GPU tensors. We will come back this later. \n\nThere are many tensor functions represented with the name of the datatypes. We can define tensors with these name-specific functions. Some of them are indicated below. Data types are not limited with the ones below. For further information, please check out the PyTorch documentation.","5bfbce64":"Some of the arithmetic operations.\n\n","4b834c8d":"## Reference Links\n* Official documentation for `torch.Tensor`: https:\/\/pytorch.org\/docs\/stable\/tensors.html\n* PyTorch Zero to GANs Course: http:\/\/zerotogans.com\/\n\nSpecial thanks to Jovian.ml team and freeCodeCamp.org!\n","60fd82bd":"To flat the tensor we can use two seperate functions which are `flatten()` and `view()`. While `flatten()` flatten tensor to row vector by default, `view()` can flatten tensor to both column vector or row vector, it depends on passed argument. `reshape()` function also can be used for the same purposes. \"-1\" can be passed as an argument to `view()` and `reshape()` to flatten tensor to row vector for easy manner. These do not change the original tensor. If you want to keep tensors changed, you should assign it to a new tensor or itself.","ae7a577e":"## 3 - Arithmetic and Logical Operators\n\nVarious arithmetic and logical operations are available for tensors. These are not limited with the operations shown below. Also functions such as sum, square, pow, sqrt, log, mean and many more are available. Check documentation for those ones.","b1685f53":"# Deep Learning with Python + PyTorch from Zero to GANs\n\n## Table of Contents\n\nA short introduction to PyTorch and the chosen functions with short explanations.\n1. Tensors: Data Types, Cuda Feature, Numpy Arrays, Shape, Flattening, View, Ones, Zeros, Random\n2. Backward Propagation and Gradient\n3. Arithmetic and Logical Operations\n4. max() Function\n5. Bounded and Stepped Tensors","bd61b915":"Permuting the tensor is also kind of reshaping operations with small difference; instead of indicating new shape we are indicating the order of dimensions. Please see the example usage:","f2124d85":"## 2 - Backward and Grad\n\nGrad or gradient is a fancier term for derivative used in deep learning to represent slope of a cost function. We often use gradient calculations in deep learning algorithms. Taking derivative from end to starting point through the deep neural network called back propagation. To automize the backward process and finding gradients, PyTorch offers `backward` and `grad` functions. While `backward` function applies back propagation, `grad` function stores variable's gradients in special flags. For automatic differentiation `requires_grad` flag should be stated as \"True\" in the tensor definition.","328ce65d":"To change and manipulate the shape of the tensor we can use several functions which are `reshape()`, `t()` and `permute(arg1, arg2, arg3)` functions. These functions are not write changes into memory, so if you want to keep shape changed, you should assign it to a new tensor or itself.","cf7f7906":"Using `zero_()` to clearing memory of `grad` flag that keeps gradient values:"}}