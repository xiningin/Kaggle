{"cell_type":{"2987f453":"code","d40d768d":"code","36130538":"code","7948ddfb":"code","0e05bca9":"code","995fdb61":"code","5349b3d1":"code","f215839d":"code","41347a42":"code","2c08e4dc":"code","b7c42837":"code","e9a074d4":"code","44830167":"code","49fac1a8":"code","9a071418":"code","5e788537":"code","631af863":"code","1983bbbe":"code","4e7d41a5":"code","119e1405":"code","b66d85ec":"code","c658c66d":"code","0c8ecece":"code","4336044c":"code","4c984276":"code","66501e1e":"code","a7f05eca":"code","990bbbd7":"code","08ba0113":"code","aaa16ab0":"code","9ea4c02b":"code","df8c6f8b":"code","487cb8b6":"code","e3b8b41e":"code","c283b6b9":"code","585a0132":"code","fd890a71":"code","edf4faf2":"code","5b804d68":"code","6b45908f":"markdown"},"source":{"2987f453":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d40d768d":"import numpy as np\nimport nltk\nimport tensorflow as tf\nimport keras\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nprint(stopwords.words('english'))","36130538":"df = pd.read_csv(\"..\/input\/language-identification-datasst\/dataset.csv\")\ndf.head()","7948ddfb":"df.info()","0e05bca9":"df[\"Text\"] = df[\"Text\"].str.lower()\ndf.head()","995fdb61":"#NLP Preprocessing : Tokenization and Embeddings","5349b3d1":"max_words = 50000\nmax_len = 100\n","f215839d":"tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words = max_words)\ntokenizer.fit_on_texts(list(df['Text']))\ntrain_df = tokenizer.texts_to_sequences(list(df['Text']))\n","41347a42":"train_df = tf.keras.preprocessing.sequence.pad_sequences(train_df,maxlen = max_len)","2c08e4dc":"sequences","b7c42837":"max_len","e9a074d4":"df.head()","44830167":"len(tokenizer.word_index)","49fac1a8":"Y = df['language']\nY","9a071418":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nle = preprocessing.LabelEncoder()","5e788537":"le.fit(Y)\n","631af863":"list(le.classes_)","1983bbbe":"Y2 = le.fit_transform(Y)","4e7d41a5":"total_languages = df['language'].nunique()","119e1405":"Y2 = keras.utils.to_categorical(Y2,num_classes=total_languages)","b66d85ec":"np.shape(Y2)","c658c66d":"X_train,X_test,Y_train,Y_test = train_test_split(train_df,Y2)","0c8ecece":"embedding_dims = 500\nvocab_size = len(tokenizer.word_index)+1\n","4336044c":"# Build the neural network model","4c984276":"model = tf.keras.Sequential([tf.keras.layers.Embedding(input_dim = vocab_size,output_dim = embedding_dims,input_length = max_len),\n                            tf.keras.layers.Flatten(),\n                            tf.keras.layers.Dense(total_languages,activation=tf.nn.softmax)\n                            ])","66501e1e":"model.summary()","a7f05eca":"model.compile(optimizer ='adam',loss = 'categorical_crossentropy',metrics=['accuracy'])","990bbbd7":"model.fit(np.array(X_train),np.array(Y_train),epochs=3)","08ba0113":"model.evaluate(np.array(X_test),np.array(Y_test))","aaa16ab0":"print(\"English \",le.transform(['English']))\nprint(\"French \",le.transform(['French']))\nprint(\"Dutch \",le.transform(['Dutch']))\nprint(\"Swedish \",le.transform(['Swedish']))","9ea4c02b":"#text = [\"Once you know all the elements, it's not difficult to pull together a sentence.\"]\ntext = [\"N\u00e4r du v\u00e4l k\u00e4nner till alla element \u00e4r det inte sv\u00e5rt att ta ihop en mening.\"] #swedish\n#text = [\"Als je eenmaal alle elementen kent, is het niet moeilijk om een zin samen te stellen.\"] # Dutch\n#text =[\"Une fois que vous connaissez tous les \u00e9l\u00e9ments, il n'est pas difficile de rassembler une phrase.\"] #French","df8c6f8b":"test_text = tokenizer.texts_to_sequences(text)\ntest_text = tf.keras.preprocessing.sequence.pad_sequences(test_text, maxlen=max_len)","487cb8b6":"predictions = model.predict(test_text)","e3b8b41e":"out = predictions.argmax()\nprint(le.inverse_transform([out]))\nprint(predictions)","c283b6b9":"#text = [\"Once you know all the elements, it's not difficult to pull together a sentence.\"]\n#text = [\"N\u00e4r du v\u00e4l k\u00e4nner till alla element \u00e4r det inte sv\u00e5rt att ta ihop en mening.\"] #swedish\ntext = [\"Als je eenmaal alle elementen kent, is het niet moeilijk om een zin samen te stellen.\"] # Dutch\n#text =[\"Une fois que vous connaissez tous les \u00e9l\u00e9ments, il n'est pas difficile de rassembler une phrase.\"] #French","585a0132":"test_text = tokenizer.texts_to_sequences(text)\ntest_text = tf.keras.preprocessing.sequence.pad_sequences(test_text, maxlen=max_len)\npredictions = model.predict(test_text)\nout = predictions.argmax()\nprint(le.inverse_transform([out]))\nprint(predictions)","fd890a71":"#text = [\"Once you know all the elements, it's not difficult to pull together a sentence.\"]\n#text = [\"N\u00e4r du v\u00e4l k\u00e4nner till alla element \u00e4r det inte sv\u00e5rt att ta ihop en mening.\"] #swedish\n#text = [\"Als je eenmaal alle elementen kent, is het niet moeilijk om een zin samen te stellen.\"] # Dutch\ntext =[\"Une fois que vous connaissez tous les \u00e9l\u00e9ments, il n'est pas difficile de rassembler une phrase.\"] #French","edf4faf2":"test_text = tokenizer.texts_to_sequences(text)\ntest_text = tf.keras.preprocessing.sequence.pad_sequences(test_text, maxlen=max_len)\npredictions = model.predict(test_text)\nout = predictions.argmax()\nprint(le.inverse_transform([out]))\nprint(predictions)","5b804d68":"text =[\"Una vez que conoces todos los elementos, no es dif\u00edcil armar una oraci\u00f3n.\"] #French\ntest_text = tokenizer.texts_to_sequences(text)\ntest_text = tf.keras.preprocessing.sequence.pad_sequences(test_text, maxlen=max_len)\npredictions = model.predict(test_text)\nout = predictions.argmax()\nprint(le.inverse_transform([out]))\nprint(predictions)","6b45908f":"# PAD SEQUENCES"}}