{"cell_type":{"4ad06d98":"code","11b5d7c7":"code","9143cdc0":"code","d4c240f9":"code","65803c66":"code","ac51c121":"code","ecc91862":"code","9daafbb3":"code","8795bd96":"code","306780b5":"code","7d0d24e9":"code","051c5de6":"code","1f49bb88":"code","4eb7cf87":"code","94f371c7":"code","3ee553b2":"code","1d80472f":"code","30f3f1bb":"code","385ce6ed":"code","f35afa74":"code","11b6354a":"code","5150fc9a":"code","4879916d":"code","2f96a202":"code","c42f14cc":"code","cdba5f67":"code","0e87610d":"code","50fbfa7c":"code","093bf2d9":"code","ffaad783":"code","2af76414":"code","840b137c":"code","86f1e0ae":"code","86a6c912":"code","4ccc1f0c":"code","4686adf6":"code","91dcf5a0":"code","9a108d76":"code","2bba3d0a":"code","7f7ecbcf":"code","f0ea8e55":"code","3efc5749":"code","47348748":"code","d84ec7e8":"code","e47efcf4":"code","101c63a5":"code","1c563a5e":"code","853371f3":"code","053881ce":"markdown","11b18875":"markdown","776be9db":"markdown","ddf5ce95":"markdown","efa2f5f4":"markdown","d456bfc2":"markdown","99c8e28a":"markdown","5edbf3d2":"markdown","6fa01330":"markdown","dd3e3bf7":"markdown","9c201009":"markdown","4ade8b7f":"markdown","723ade3e":"markdown","54126722":"markdown","269ffb46":"markdown","d17d3b25":"markdown","0923d352":"markdown","a605d533":"markdown","f5116884":"markdown","713f04e5":"markdown","b2cd7ce0":"markdown"},"source":{"4ad06d98":"# Making our tools ready\n\n# importing all the evaluating models\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\n# importing all the machine learning tools\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# importing all featuring tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_score, f1_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import roc_curve, plot_roc_curve\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV","11b5d7c7":"# loading the datesets\ndf = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ndf.head()","9143cdc0":"df.describe()","d4c240f9":"df.shape","65803c66":"df['target'].value_counts()","ac51c121":"# Sex vs Target\n\npd.crosstab(df.sex, df.target)","ecc91862":"pd.crosstab(df.sex, df.target).plot(\n    kind='bar',\n    rot=0,\n    ylabel='Frequency',\n    xlabel='Sex',\n    title='Frequency graph between the Sex and Target',\n    colormap='tab20c'\n)\nplt.legend(['No-Disease', 'Disease']);","9daafbb3":"pd.crosstab(df.cp, df.target)","8795bd96":"pd.crosstab(df.cp, df.target).plot(\n    kind='bar',\n    rot=0,\n    xlabel='Chest Pain',\n    ylabel='Frequency',\n    title='Frequency Graph between the Chest Pain and Target',\n    colormap='tab20c'\n)\nplt.legend(['No-Disease', 'Disease']);","306780b5":"pd.crosstab(df.age, df.trestbps)","7d0d24e9":"# Let's try to make this more understandable and visual\nplt.figure(figsize=(10, 6))\n# Relation between the resting blood pressure and age with positive target\nplt.scatter(df.trestbps[df.target == 1], df.age[df.target == 1])\n# Relation between the resting blood pressure and age with negative target\nplt.scatter(df.trestbps[df.target == 0], df.age[df.target == 0])\nplt.xlabel('Resting Blood Pressure (mm Hg)')\nplt.ylabel('Age (Years)')\nplt.title('Relation between the Resting Blood Pressure vs Age')\nplt.legend(['Disease', 'No-Disease']);","051c5de6":"df.corr()","1f49bb88":"# Let's visualise this correlation matrix using the seaborn heatmap\ncorr_matrix = df.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr_matrix,\n           fmt='.2f',\n            linewidth=0.5,\n           annot=True,\n           cmap='Blues');","4eb7cf87":"np.random.seed(42)\n# Create feature and label data\nX = df.drop(\"target\", axis=1)\ny = df[\"target\"]\n\n# Split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","94f371c7":"# Create a dictionary of different models\nmodel = { \"Logistic Model\": LogisticRegression(),\n         \"K Neighbors\": KNeighborsClassifier(),\n         \"Random Forest\": RandomForestClassifier()        \n}\n\ndef fit_and_score(model, X_train, X_test, y_train, y_test):\n    '''\n    Function to train the different model and find out the best score for classification problem\n    model : Different classification model\n    X_train : training data (no label)\n    X_test : testing data (no label)\n    y_train : training label\n    y_test : test label\n    '''\n    np.random.seed(42)\n    \n    model_score = {}\n    \n    for name, model in model.items():\n        model.fit(X_train, y_train)\n        model_score[name] = model.score(X_test, y_test)\n        \n    return model_score","3ee553b2":"model_score=fit_and_score(model=model,\n                         X_train=X_train,\n                         X_test=X_test,\n                         y_train=y_train,\n                         y_test=y_test)\nmodel_score","1d80472f":"pd.DataFrame(model_score.items(), columns=['Model', 'Score'])","30f3f1bb":"# Let's make our score more visual\nmodel_df = pd.DataFrame(model_score.items(), columns=['Model', 'Score'])\nmodel_df.plot(kind='bar', colormap='tab20c')\nplt.ylabel('Score')\nplt.title('Model Predicting Score Graph')\nplt.xticks([0, 1, 2], ['Logistic Regression', 'K Neighbors', 'Random Forest']);","385ce6ed":"# Create grid for RandomSearchCV\n\n# Random Search Grid for logisitic Regression\nrs_lr_grid = {'penalty': ['l2', 'l1'], \n              'C': [0.001,0.01,0.1,1,10,100],\n              'random_state': np.arange(0, 42, 5)}\n# Random search grid for K-Neighbors Classifier\nrs_knn_grid = {\n    'n_neighbors': np.arange(1, 150, 5)\n}\n# Random search grid for random forest classifier\nrs_rf_grid = {\n    'n_estimators': [10, 30, 50, 100],\n    'min_samples_leaf': [1, 5, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'n_jobs': [None, 1]\n}","f35afa74":"# Let's fit the model and score them\n\ndef rs_fit_score(grid, model):\n    '''\n    Used to return the object for different classfication model performing Randomized Search CV\n    grid : Random Search CV grid\n    model : Classification Model\n    '''\n    rs_model = RandomizedSearchCV(model,\n                                    param_distributions=grid,\n                                    cv=5,\n                                    n_iter=20,\n                                    verbose=True)\n    return rs_model","11b6354a":"# Fiting Logistic Model\nnp.random.seed(42)\n\nlr_model = rs_fit_score(rs_lr_grid, LogisticRegression())\nlr_model.fit(X_train, y_train)","5150fc9a":"lr_model.best_params_","4879916d":"lr_score = lr_model.score(X_test, y_test)","2f96a202":"# Fiting K-Neighbors Model\nnp.random.seed(42)\nknn_model = rs_fit_score(rs_knn_grid, KNeighborsClassifier())\nknn_model.fit(X_train, y_train)","c42f14cc":"knn_model.best_params_","cdba5f67":"knn_score = knn_model.score(X_test, y_test)","0e87610d":"# Fitting Random Forest Classifier\nrf_model = rs_fit_score(rs_rf_grid, RandomForestClassifier())\nrf_model.fit(X_train, y_train)","50fbfa7c":"rf_model.best_params_","093bf2d9":"rf_score = rf_model.score(X_test, y_test)","ffaad783":"new_model_score = {'Logistic Regression': lr_score,\n                   'K Neighbors': knn_score,\n                   'Random Forest': rf_score}\npd.DataFrame(new_model_score.items(), columns=['Model', 'Score'])","2af76414":"new_model_df = pd.DataFrame(new_model_score.items(), columns=['Model', 'Score'])\ncombine_model = pd.DataFrame(model_score.keys(), index=[0, 1, 2], columns=['Model'])\ncombine_model['Old Score'] = model_score.values()\ncombine_model['New Score'] = new_model_score.values()\ncombine_model","840b137c":"combine_model.plot(kind='bar', cmap='tab20c')\nplt.xticks([0, 1, 2], combine_model['Model']);\nplt.ylabel('Score')\nplt.title('Hyperparameter Tuning Graph');","86f1e0ae":"# Create grid for RandomSearchCV\n\n# Random Search Grid for logisitic Regression\ngs_lr_grid = {'C': np.logspace(-4, 4, 20),\n              'penalty': ['l1', 'l2', None]}\n# Random search grid for K-Neighbors Classifier\ngs_knn_grid = {\n    'n_neighbors': np.arange(1, 150, 5)\n}\n# Random search grid for random forest classifier\ngs_rf_grid = {\n    'n_estimators': np.arange(0, 101, 10),\n    'min_samples_leaf': [1, 5, 10, 20],\n    'min_samples_split': [2, 5, 10, 30],\n    'n_jobs': [None, 1, 2]\n}","86a6c912":"# Let's fit the model and score them\n\ndef gs_fit_score(grid, model):\n    '''\n    Used to return the object for different classfication model performing Randomized Search CV\n    grid : Random Search CV grid\n    model : Classification Model\n    '''\n    gs_model = GridSearchCV(model,\n                            param_grid=grid,\n                            cv=5,\n                            verbose=True)\n    return gs_model","4ccc1f0c":"gs_lg_model = gs_fit_score(gs_lr_grid, LogisticRegression())\ngs_lg_model.fit(X_train, y_train);","4686adf6":"gs_lg_model.best_params_","91dcf5a0":"gs_lg_model.score(X_test, y_test)","9a108d76":"knn_lg_model = gs_fit_score(gs_knn_grid, KNeighborsClassifier())\nknn_lg_model.fit(X_train, y_train);","2bba3d0a":"knn_lg_model.best_params_","7f7ecbcf":"knn_lg_model.score(X_test, y_test)","f0ea8e55":"rf_lf_model = rs_fit_score(gs_rf_grid, RandomForestClassifier())\nrf_lf_model.fit(X_train, y_train);","3efc5749":"rf_lf_model.best_params_","47348748":"rf_lf_model.score(X_test, y_test)","d84ec7e8":"y_pred = lr_model.predict(X_test)","e47efcf4":"plot_roc_curve(lr_model, X_test, y_test)\nplt.title(\"ROC and AUC Curve\");","101c63a5":"confusion_matrix(y_test, y_pred)","1c563a5e":"conf_mat = confusion_matrix(y_test, y_pred)\nsns.heatmap(conf_mat, annot=True, cbar=False, cmap='Blues', xticklabels=['FPR', 'TPR'], yticklabels=['FPR', 'TPR'])\nplt.title('Confusion Matrix');","853371f3":"print(classification_report(y_test, y_pred))","053881ce":"**What we know from this graph?**\n\nMost of the patient between the 40-60 years of age is facing the heart disease problem. Let's study the graph more deeper, Resting Blood Pressure from 100-140 mm Hg result into heart disease problem and the patient having the Resting Blood Pressure from 160-200 mm Hg don't facing the heart disease problem.","11b18875":"From the above graph we can predict that, Person suffering from Non-Aginal Chest Pain have more chances of getting heart disease problem and the person suffering from typical angina chest pain doesnot face heart disease problem. ","776be9db":"Let's create function to perform different machine learning model on our train and test data and find out the best score for our classification problem.","ddf5ce95":"### 1.3 Predicting the relation between the Independent Variables\n\n#### Resting Blood Pressure vs Age","efa2f5f4":"So, From the above dataframe we can predict that Logistic Regression Model is result into best score among the three different models.\n\nBut we try to make our others model accuracy more accurate but at this stage we can see that out Classifier problem scoring is around 89% which is given by Logisitic Model. So, first we try to make our models to achieve this score by performing some hyperparameter tuning.","d456bfc2":"So, from the above graph we can predict that the difference between the disease and no-disease for female is more than the male. Female with no-disease is less than men and also Female with disease is less than the male.","99c8e28a":"So, finally we perform some hyperparameter to tune our model. We use RandomSearch CV and GridSearch CV with Cross-Validation.\n\n**So, first we use RandomSearchCV**\n* Logistic Regression Parameter Tuning : https:\/\/www.kaggle.com\/joparga3\/2-tuning-parameters-for-logistic-regression","5edbf3d2":"## 1. Evaluate the Data\n\n### 1.1 Predicting the Data using the sex and target\n\nsex: sex (1 = male; 0 = female)","6fa01330":"So, from this auc and roc curve we see that we got 93% result where our model predict the true value (true positive rate, where 1 is come as 1 and false positive rate, where 0 is come as 0) which good enough in the first try. \n\nSo, we try to find out the ```confusion matrix``` now to check out how much value we are getting under false positive and true negative.","dd3e3bf7":"Enough EDA perform on the data to evaluate the dataset and gather the knowledge about the data. Let's perform some Machine Learning model and Experimentation to create a model that help us to acheive our goal we state in the problem defination.\n\n## 5. Modeling\nWe use different machine learning model to solve our classification problem:\n1. Logistic Regression\n2. K-Neighbor Classifier\n3. Random Forest Classifier\n\nAfter creating the Model, we perform some hypermeter tunning to make our model more mature and more accurate before deploying it into real enviornment and create report for classification problem. This report include:\n1. Accuracy\n2. Precision\n3. Recall\n4. F1 Score\n5. ROC Curve\n6. Area Under Curve (AUC)\n\nSo, Let's make our data ready for training and testing our machine learning model.","9c201009":"Under construction....","4ade8b7f":"So, from this we find out that 25 result are such that where our result were accurate in terms of negativity and 4 result were like where we need to be negative but got positive results means we want to get 0 but got 1. Similary in positive result we got 3 result where we want 1 but get 0 as our result and 29 times our model predict the correct positive results.\n\nTo make this more understandable let's plot this into seaborn heatmap...","723ade3e":"So, performing GridSearchCV we see that no model get more accuracy even after changing some parameter. Finally, we get Logistic Regression machine learing model with the accuracy of 89%. \n\nSo, Whats next? Now, we prepare classification report which consists of:\n* Accuracy\n* Precision\n* Recall\n* F1-Score\n* ROC or AUC Curve\n* Classification Report\n* Confussion Matrix\n\nand we use cross-validation where ever we found that usefull...\n\nLet's create roc and auc score for predicting wheather our Logistic Regression model working accurate by plotting the graph between the true positive rate and false positive rate","54126722":"### 1.2 Predicting the data using Chest Pain and Target\n\n* cp: chest pain type\n    - Value 1: typical angina\n    - Value 2: atypical angina\n    - Value 3: non-anginal pain\n    - Value 4: asymptomatic","269ffb46":"## Correlation Matrix\n\n> A correlation matrix is simply a table which displays the correlation. The measure is best used in variables that demonstrate a linear relationship between each other. The fit of the data can be visually represented in a scatterplot. coefficients for different variables. For more understandable follow: https:\/\/www.displayr.com\/what-is-a-correlation-matrix\/\n\nLet's perform the Coorelation matrix to understand the relation between the dependent variable and the independent variable and within the independent variable.","d17d3b25":"# Heart Disease Machine Learning Model\n\n> This is a analysis of the heart disease problem using the different machine learning models. It predicts wheather or not the patient have a heart disease. For Analysis such model we use different machine learning tools like pandas, matplotlib, sckit-learn, etc. \n\nFor attaining this goal we use specific steps:\n1. Problem Defination\n2. Data\n3. Evaluation\n4. Feature\n5. Modeling\n6. Experimentation\n\n## 1. Problem Defination\n> We are using the problem based on the heart disease which train the model using the parameters and analysis wheater or not the patient have a heart disease.\nWe use the binary data (data based upon 0 and 1) to predict the heart disease problem using the classification model convert the data more precise using different machine learning tools.\n\n## 2. Data\nWe collect the data from the Kaggle https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci <br>\nand from the UCI Machine Learning Repository https:\/\/archive.ics.uci.edu\/ml\/datasets\/heart+disease\n\n**Data Dictonary**<br>\nWe use only 14 attributes to predict wheater or not the patient is having heart disease.\n* age: age in years\n* sex: sex (1 = male; 0 = female)\n* cp: chest pain type\n    - Value 1: typical angina\n    - Value 2: atypical angina\n    - Value 3: non-anginal pain\n    - Value 4: asymptomatic\n* trestbps: resting blood pressure (in mm Hg on admission to the hospital)\n* chol: serum cholestoral in mg\/dl\n* fbs: (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* restecg: resting electrocardiographic results\n    - Value 0: normal\n    - Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n    - Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n* thalach: maximum heart rate achieved\n* exang: exercise induced angina (1 = yes; 0 = no)\n* oldpeak = ST depression induced by exercise relative to rest\n* slope: the slope of the peak exercise ST segment\n    - Value 1: upsloping\n    - Value 2: flat\n    - Value 3: downsloping\n* ca: number of major vessels (0-3) colored by flourosopy\n* thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n* num: diagnosis of heart disease (angiographic disease status)\n    - Value 0: < 50% diameter narrowing\n    - Value 1: > 50% diameter narrowing (in any major vessel: attributes 59 through 68 are vessels)","0923d352":"In above result, we are getting the value between the +1 value to -1 value. So, positive value indicate positive relationship and the negative value indicate the negative relationship.","a605d533":"## 3. Evaluating the Model\nEvaluation of the model consist of evaluating the dataset wheater it have any empty cells inside the table (in this case table refers to csv file), predicting the mean values, shape of the dataset, for what purpose we are actually creating this model, what we predicting, etc\n\nWhile Evaluating this dataset, We are Evaluating:\n- What parameters is responsible for heart disease problem?\n- What we are predicting while making the model train?\n- What is the average age of the people who are suffering from heart disease problem?\n- Is this dataset is enough to make the model train?\n- How much percentage we are successed in training our model?\n","f5116884":"So, from the above graph we can predict that after performing some hyperparameter tuning, Logistic Regression is still have a maximum accuracy among the three different classification model. Random Forest Classifier increase after tuning the parameter but still have a less acuracy than the Logistic Regression.\n\nLet's perform GridSearchCV.\n\n### GridSearchCV","713f04e5":"Let's use ```classification report``` to look more deeper in the confusion matrix","b2cd7ce0":"Let's compare both the dataframe i.e. dataframe before performing hypermeter tuning and the dataframe after performing hyperparameter tuning"}}