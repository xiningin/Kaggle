{"cell_type":{"5d5793b3":"code","c94392a6":"code","2ea8df25":"code","40cf7fe8":"code","1ca4fcf5":"code","0719e411":"code","ce3323c7":"code","0da2b6c0":"code","f4894208":"code","9ce06991":"code","1ee7f49d":"code","615ad332":"code","f6684aaf":"code","2c5f5f64":"code","c80f4562":"code","2012131d":"code","4b90cfdb":"code","9b0bd7cf":"code","ab1ada29":"code","438a0709":"code","7922f3ad":"code","038d03cc":"code","be2fdc40":"markdown","300408f4":"markdown","8a17f649":"markdown","a1b0013e":"markdown","f7f48e02":"markdown","51546f17":"markdown","9ab2c46c":"markdown","2630922b":"markdown","c882dfb2":"markdown","7dda8d56":"markdown","56fa6eae":"markdown"},"source":{"5d5793b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  #  matlotlib for plotting data , simple and effecient \nimport datetime as dt  # for datetime manipulation \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c94392a6":"## here we will read our base data \ndf = pd.read_csv('..\/input\/nifty50-index-daily-data\/Nifty50_Index_data.csv')\ndf.head()","2ea8df25":"## calculate multi day SMA\/EMA signal values for the data \n\n## simple moving average : 10 day,20,day , 50 day periods \ndf[\"sma10\"] =  df.Close.rolling(window=10,min_periods = 1).mean().round(2)\ndf[\"sma20\"] =  df.Close.rolling(window=20,min_periods = 1).mean().round(2)\ndf[\"sma50\"] =  df.Close.rolling(window=50,min_periods = 1).mean().round(2)\n\n## exp moving average calc : 10 day,20,day , 50 day \ndf['ema10'] = df.Close.ewm(span=10,adjust=False,min_periods=1).mean().round(2)\ndf['ema20'] = df.Close.ewm(span=20,adjust=False,min_periods=1).mean().round(2)\ndf['ema50'] = df.Close.ewm(span=50,adjust=False,min_periods=1).mean().round(2)\n\n","40cf7fe8":"### below we calculate the 10,20,50 day simple moving average buy sell signals  : \n### as our Technical Indicator this will be a binary variables\n\n## create dummy data variable\ndf['sma10sig'],df['sma20sig'],df['sma50sig'] = df.sma10,df.sma20,df.sma50\n\n## here we will populate buy sell binary for sma calls, if sma> close, buy , else sell \nfor i in range(len(df.Close)):\n    if df.sma10[i] < df.Close[i]:\n        df.sma10sig.loc[i] = 1\n    else:\n        df.sma10sig.loc[i] = 0\n   \n    if df.sma20[i] < df.Close[i]:\n        df.sma20sig.loc[i] = 1\n    else:\n        df.sma20sig.loc[i] = 0\n        \n    if df.sma50[i] < df.Close[i]:\n        df.sma50sig.loc[i] = 1\n    else:\n        df.sma50sig.loc[i] = 0\n        \n# to check values must be present \nprint(df.sma10sig.sum(),df.sma20sig.sum(),df.sma50sig.sum())   ## values should be non -zero sum on col   \n","1ca4fcf5":"### below we calculate the 10,20,50 day exponential moving average buy sell signals  : as our Technical Indicator\n\n## create dummy data variable\ndf['ema10sig'],df['ema20sig'],df['ema50sig'] = df.ema10,df.ema20,df.ema50\n\n## here we will populate buy sell sig for ema calls, if ema > close, buy , else sell \nfor i in range(len(df.Close)):\n    if df.ema10[i] < df.Close[i]:\n        df.ema10sig.loc[i] = 1\n    else:\n        df.ema10sig.loc[i] = 0\n   \n    if df.ema20[i] < df.Close[i]:\n        df.ema20sig.loc[i] = 1\n    else:\n        df.ema20sig.loc[i] = 0\n        \n    if df.ema50[i] < df.Close[i]:\n        df.ema50sig.loc[i] = 1\n    else:\n        df.ema50sig.loc[i] = 0\n        \n# to check values must be present \nprint(df.ema10sig.sum(),df.ema20sig.sum(),df.ema50sig.sum()) ## values should be non -zero sum on col   \n","0719e411":"## volatility defined as percentage change from last day closing price\ndf['Volatility'] = round(df.Close.pct_change(periods=1,fill_method='pad') * 100 , 2)\n\n## Momentum, which is 0 or 1 based on if last closing < current close or vice versa\ndf['Momentum'] = df.Volatility.apply(lambda x : 0 if (x<0) else 1)\n\n## OI_change represents change in Volume\ndf['OI_change'] = df.Volume.diff(periods =1)\n\n## OI_Volatility = percent change in OI\/Volume \ndf['OI_Volatility'] = round(df.Volume.pct_change(periods=1,fill_method='pad') * 100,2)\n\n## OI_Momentum which is 0 or 1 based on if last Volume < current Volume or vice versa\ndf['OI_Momentum'] = df.OI_Volatility.apply(lambda x : 0 if (x<0) else 1)\n\n","ce3323c7":"## lets cleanup the data by removing NA values, which are introduced from feature creation \n\ndf.dropna(axis=0,inplace=True)\ndf.info()","0da2b6c0":"## drop the sma,ema corr features set to reduce dimensinality snce we already genertaed signal values from them :\n\ndf.drop(['sma10','sma20','sma50','ema10','ema20','ema50'],axis=1,inplace=True)\ndf.info()","f4894208":"## convert date column to Index : we can do it anytime , i just remembered to do it now\ndf[\"Date\"]=pd.to_datetime(df.Date,format=\"%Y-%m-%d\")","9ce06991":"\n#Split the training and test set\ntraining_set = df.loc[0:3000,:]\ntest_set = df.loc[3000:,:]\nprint('Training Set Shape:' + str(training_set.shape) ,'Test Set Shape:' + str(test_set.shape))","1ee7f49d":"## we remove date column from in dataset and convert the numerical data to numpy array for processing \ntraining_set.set_index('Date',inplace=True)\ntest_set.set_index('Date',inplace=True)\n\n## convert dataframe to np arrary for reshaping \ntrain_data = training_set.to_numpy()\ntest_data = test_set.to_numpy()\nprint('Training Set Shape:' + str(train_data.shape) ,'Test Set Shape:' + str(test_data.shape))","615ad332":"from sklearn.preprocessing import MinMaxScaler  ## import scalar \nscaler=MinMaxScaler(feature_range=(0,1))\n\n## transfrm in dataframe for both training_set and test_set it will return final ndarray for reshape and input \nscaler1=MinMaxScaler(feature_range=(0,1))\nscaler2=MinMaxScaler(feature_range=(0,1))\nX_train  = scaler1.fit_transform(train_data)\nX_test   = scaler2.fit_transform(test_data)\nprint('Training Set Shape:' + str(X_train.shape) ,'Test Set Shape:' + str(X_test.shape))\n\n## Here we will extract y_train, as will be the close price, as we are predicting Close price  which is column 3 in array\ny_train = X_train[:,3]\ny_test = X_test[:,3]\nprint('y_train Set Shape:' + str(y_train.shape) ,'y_Test Set Shape:' + str(y_test.shape))\n","f6684aaf":"## correctly reshape data : sample,timesteps,features here our 3d tmestep is 1 day \nX_train = np.reshape(X_train,(X_train.shape[0],1,X_train.shape[1]))\nX_test = np.reshape(X_test,(X_test.shape[0],1,X_test.shape[1]))\nprint('X_train Set Shape:' + str(X_train.shape) ,'X_Test Set Shape:' + str(X_test.shape))","2c5f5f64":"from sklearn.metrics import r2_score\n\n# import the relevant Keras modules\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM \nimport keras ","c80f4562":"## define LSTM MODEL base network model here # initialise model architecture :: training of LSTM  model begins \n## the model will run for n runs, each run with e epochs the putput params wil be averaged out\n\nfrom sklearn.metrics import mean_absolute_error , r2_score\n## to be used for reverse scaling for predicted close price from the model vs the original data\nscale = MinMaxScaler() \n## here we copy class values used for scaling test set and use same values for scaling close price\n## scaler2 instance was contains scaled parameters of test set ,this is used since scaler2 instance has paremetrs of all 16 Features, \n    ## we will extract only th parameter for close price scaling, this can be done for any parameters in list \n    \nscale.min_,scale.scale_ = scaler2.min_[0],scaler2.scale_[0] \n\nepoch = 200 ## number of epochs\nvalidation_set = 0.2\nbatch_size = 10  ## batch size per epoch run \n\ntraining_loss = np.zeros(epoch)\ntraining_mae_loss = np.zeros(epoch)\nvalidation_Loss = np.zeros(epoch)\nvalidation_mae_loss =np.zeros(epoch)\ntraining_rmse_loss = np.zeros(epoch)\ntraining_msle_loss = np.zeros(epoch)\npredicted_average_price = np.zeros(len(y_test))\n\n\n## -- define model layer paraeters\n## processing loop starts \n\nmodel = Sequential()\nneurons = 128 \ndense_l1 = 64\n\noutput_size = 1\nactiv_func ='relu'\nactiv_out_func ='linear'\nloss = keras.losses.MeanSquaredError(reduction=\"auto\", name=\"mean_squared_error\")\ndropout = 0.2\nopt_adam = keras.optimizers.Adam(learning_rate=0.0001)\n \n\n## -- create the model layer parameters \n    \nmodel.add(LSTM(units=neurons,activation=activ_func,return_sequences=True,recurrent_activation=activ_func,dropout=dropout,input_shape=(X_train.shape[1],X_train.shape[2])))  # size is 2000 x 1 x 5\nmodel.add(LSTM(units=neurons,activation=activ_func,return_sequences=True,dropout=dropout)) \nmodel.add(LSTM(units=neurons,activation=activ_func,return_sequences=True,dropout=dropout)) \nmodel.add(LSTM(units=neurons,activation=activ_func,return_sequences=True,dropout=dropout)) \nmodel.add(LSTM(units=neurons,activation=activ_func,return_sequences=True,dropout=dropout)) \nmodel.add(LSTM(units=neurons,activation=activ_func,return_sequences=True,dropout=dropout)) \nmodel.add(LSTM(units=neurons,activation=activ_func,dropout=dropout)) \nmodel.add(Dense(units=dense_l1,activation=activ_func))\nmodel.add(Dense(units=output_size,activation=activ_out_func))\n  \nmodel.compile(loss=loss, optimizer=opt_adam,metrics=[keras.metrics.RootMeanSquaredError()])\n \n## -- run the model here for n runs, e epochs , set verbose = 0 to disable output \n    \nmodel_history = model.fit(X_train,y_train,epochs=epoch,validation_split= validation_set , batch_size=batch_size, verbose=2, shuffle=False)\n  \n## predictions and R2 score calculation on model data\n## ------------------------------------------------------------------------------------\n\nclosing_price=model.predict(X_test)\n    \n    \nclosing_price_scaled=scale.inverse_transform(closing_price)\nX2 = pd.DataFrame.copy(test_set)\nX2['Predictions']=closing_price_scaled\n    \n# -----------------------------------------------------------------------------------\n## here we will create a copy and for each run will add the output values final output\n\ntraining_loss = training_loss + model_history.history['loss']\nvalidation_Loss = validation_Loss + model_history.history['val_loss']\ntraining_rmse_loss= training_rmse_loss + model_history.history['root_mean_squared_error']\npredicted_average_price = predicted_average_price +  closing_price_scaled.reshape(len(y_test)) ## reshape the output array to scalar \n    \n# ------------------------------------------------------------------------\nprint('run =',runs ,'completed')\n    ## processing loop stops \n\n\nprint(\"LSTM Model execution finished\")","2012131d":"## create final dataset with all values as above and write to file\nNew_Labels=['training_loss','validation_Loss','training_rmse_loss']\ndf_out =  pd.DataFrame([training_loss,validation_Loss,training_rmse_loss],index=None)\ndf_out = df_out.transpose()\ndf_out.columns = New_Labels\ndf_out.head()","4b90cfdb":"## here we prepare the test predicted\/original close prices for output\ndf_pred = pd.DataFrame()\nNew_Labels=['Date','Original_Close','Predicted_close']\ndf_pred = pd.DataFrame.copy(test_set)\ndf_pred['Predicted_Close_price'] = predicted_average_price.round(2)\ndf_pred = df_pred.filter(items=['Close', 'Predicted_Close_price'])\ndf_pred.head()\n","9b0bd7cf":"## here we set jupyter  parameters for plotting data  \n\nplt.rc('font', size=10)          # controls default text sizes\nplt.rc('axes', titlesize=14)     # fontsize of the axes title\nplt.rc('axes', labelsize=12)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=10)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=10)    # fontsize of the tick labels\nplt.rc('legend', fontsize=12)    # legend fontsize\nplt.rc('figure', titlesize=14)  # fontsize of the figure title","ab1ada29":"# Plot the LSTM Real Close Price and the Predicted Prices against Date \nplt.figure(figsize=(16,8))\nplt.plot(df_pred.index,df_pred.Close, label='Real Close',color='royalblue',linewidth=2)\nplt.plot(df_pred.index,df_pred.Predicted_Close_price, label='Predicted Close',color='crimson',linewidth=2)\nplt.ylabel(\"Closing Price in INR\")\nplt.xlabel(\"Date Year-Month\")\nplt.title(\"Model Output:Real vs Predicted Prices\")\nplt.legend()\nplt.show()\n","438a0709":"## for plotting set the variables \ndf_out['epochs'] = np.arange(epoch)\n\nplt.figure(figsize=(16,8))\nplt.plot(df_out.epochs,df_out.training_loss, label='Training Loss:MSE',color='royalblue',linewidth=2)\nplt.ylabel(\"Loss Value\")\nplt.xlabel(\"Number of Epochs\")\nplt.title(\"Model Output:Training Loss \")\nplt.legend()\nplt.show()\n","7922f3ad":"plt.figure(figsize=(16,8))\nplt.plot(df_out.epochs,df_out.training_rmse_loss, label='Metric:RMSE',color='royalblue',linewidth=2)\nplt.ylabel(\"Loss Value\")\nplt.xlabel(\"Number of Epochs\")\nplt.title(\"Model Output:Metric Root Mean Square Error\")\nplt.legend()\nplt.show()","038d03cc":"from sklearn.metrics import r2_score\npred= r2_score(df_pred.Close,df_pred.Predicted_Close_price)*100 \nprint(\"R2 Score for Model : \" , pred.round(4))","be2fdc40":"### Plot the LSTM Training Metrics plots ,we will plot common loss and metrics","300408f4":"# Model exection starts here : we will do the following here\n\n1.   Import all packages \n2.   Define the LSTM Model Parameters\n3.   Define the LSTM Model Architecture\n4.   Compile and Run the LSTM Model\n5.   Track the outputs metrics for processing for n runs\n","8a17f649":"* we use min max scaler class on the train and test datasets for normalization of data the train & test datasets are finalized for processing\n* this scaling can be done using Standard Scaler or Differencing both popular approaches, for Time series normalization, we have chosen min-max scaling to remove the -ve values for certain features in our dataset  ","a1b0013e":"## Note this is a sample illustration of a LSTM based model for Stock Trade, IT only models the data, to predict or forecast n timesteps in future we will need to feed the predicted data back into network for next step forecasting . i will cover that in another notebook ","f7f48e02":"This Code is a sample illustration of stock modeling using the technical indicators for buy sell signals in market.\ncode can be extended using other common tecnical indicators \nthe forecasting canbe done using n-wndow output, and will be covered in another book. this one is for the modelling of data","51546f17":"*  we will create a few technical parameters for the base data \n*  the idea being the technical parameters are for buy\/sell signals to generate for buy\/sell decisions\n*  2 technical parameters will be created for buy sell signals the SMA and the EMA signals ","9ab2c46c":"# The LSTM layers expects input to be in a 3D matrix with the dimensions: [samples, time steps, features].  we will reshape our input data in the correct shape, the timesteps will always be 1 as we are predicting 1 day model \n\n## Samples: These are independent observations from the domain, typically rows of data. \n### Time steps: These are separate time steps of a given variable for a given observation.    \n### Features: These are separate measures observed at the time of observation. ","2630922b":"### Here we will start Data plots of our output loss and pred variables ","c882dfb2":"## Now we start final processing of data:  \n#### We will split the Data directly split on 90:10 for windows as below. 3000 records for Training will be selected, 20% of training data will be held for validation , remaining records will be test data ","7dda8d56":"### Now we calculate The dependent fields for volatility , Open Interest and Momentum as additional parameters as below","56fa6eae":"## we split data into x-train, y-train and convert data to numpy array for processing Date column will be removed from test,train set as it will not be an input of Model"}}