{"cell_type":{"a87bc4b0":"code","d32c2db4":"code","380f054a":"code","154c0975":"code","8b08e261":"code","e1190ca8":"code","59a42616":"code","687b9ce9":"code","2f756f20":"code","f5c19c05":"code","91c4a14d":"code","9976367f":"code","084c91d1":"code","49933b6b":"code","f8a46b8a":"code","50f19964":"code","b630179e":"code","fa54b94d":"code","2234d253":"code","9aad74c1":"code","989e2799":"code","d0ba2940":"code","c742e34b":"code","069cbaed":"code","9f3dd9e7":"code","258af335":"code","352b30e1":"code","aa8bb0df":"code","712985fd":"code","9b37ee0a":"code","bc761466":"code","81093434":"code","2d01014c":"code","5c6cddfb":"code","fe6ba386":"code","d4669880":"code","804abe83":"code","711ee18c":"code","f067b2d8":"code","dfc5f373":"code","3de405d6":"code","2fa40878":"code","02b8392c":"code","9e1cbc63":"code","baf03c8c":"code","1967cdf0":"code","f4a95924":"code","5cbf61a9":"code","471fbd66":"code","29ffce12":"code","06755712":"code","12c9044e":"code","0e97560b":"code","630d54e2":"code","60d518f1":"code","f402624f":"code","32d023d0":"code","57b1ffa3":"code","d3d3c679":"code","6a21f6da":"code","dc9c8347":"code","bf856d2b":"code","83b03d13":"code","c4201c00":"code","76c4b69d":"code","24412c71":"code","b42e7216":"code","4a3915f4":"code","8e6cbb28":"code","97bfbabb":"code","53f5cd13":"code","39de3aa2":"code","3e28d8bb":"code","46bd4899":"code","d4d0b548":"code","6404d714":"code","0fbd6cbd":"code","6adeab75":"code","4e59a02f":"code","6e36b55f":"code","5071e148":"code","34f95257":"code","65874cd4":"code","07e19647":"code","7545a6fe":"code","0ded0661":"code","92573385":"code","a50f14ab":"code","4ed91d3b":"code","09f6558e":"code","4c1f5e8d":"code","2a173b3b":"code","e8908d0d":"code","f52ca34d":"code","fc61ca90":"code","dc5052dd":"code","04a5ec1d":"code","bba39097":"code","3d4989ca":"code","eaf20b13":"code","c054963b":"code","908e382c":"code","cb29f50b":"code","125dceae":"code","d101331c":"code","5a8338d5":"code","08085307":"code","70945602":"code","70c940d9":"code","5dfd2881":"code","ec13c200":"code","495e4e80":"code","aa99124b":"code","e3d6e49a":"code","3ff938bc":"code","c9657e25":"code","377efd9c":"code","14e94c33":"code","3c596886":"code","2171aed7":"code","e5823b48":"code","1d1065ba":"code","121314f5":"code","293786ed":"code","80d870d5":"code","b94d5db5":"code","1d10432d":"code","8ec03cb7":"code","3233fa1b":"code","0e104468":"code","8276e5e0":"code","254397eb":"code","6ee01389":"code","94e20395":"code","91032a04":"markdown","bff7c53f":"markdown","1337ea78":"markdown","099f1f67":"markdown","f6a96551":"markdown","ba3bb979":"markdown","9ee24d75":"markdown","d0246327":"markdown","8bbc1d5e":"markdown","ab7090bc":"markdown","11c752a9":"markdown","15a200ad":"markdown","ec23a041":"markdown","7c01e582":"markdown","9f764598":"markdown","4387f2ba":"markdown","ed1d7ced":"markdown","17ce63b7":"markdown","97859a2f":"markdown","3ceee4ba":"markdown","ca0aaec2":"markdown","d135a8e7":"markdown","ca7cd854":"markdown","763de5e9":"markdown","e84a543e":"markdown","962fa75b":"markdown","4b2db9cd":"markdown","72b10657":"markdown","cf10ccc5":"markdown","2e5ca89f":"markdown","1a723bc8":"markdown","cb893eb5":"markdown","ff517ad7":"markdown","ff9a108f":"markdown","e74588a9":"markdown","33e26e09":"markdown","101ee330":"markdown","9504b44a":"markdown","f4814519":"markdown","d5fba644":"markdown","f0c9d110":"markdown"},"source":{"a87bc4b0":"# suppress warnings \nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n# 'Pandas' is used for data manipulation and analysis\nimport pandas as pd \n\n# 'Numpy' is used for mathematical operations on large, multi-dimensional arrays and matrices\nimport numpy as np\n\n# 'Matplotlib' is a data visualization library for 2D and 3D plots, built on numpy\nimport matplotlib.pyplot as plt\n\n# 'Seaborn' is based on matplotlib; used for plotting statistical graphics\nimport seaborn as sns\n\n# 'Scikit-learn' (sklearn) emphasizes various regression, classification and clustering algorithms\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\n# 'Statsmodels' is used to build and analyze various statistical models\nimport statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.tools.eval_measures import rmse\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# 'SciPy' is used to perform scientific computations\nfrom scipy.stats import shapiro\nfrom scipy import stats\n\n# import functions to perform feature selection\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\n\n#import functions for time series\nimport itertools\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tsa.stattools import adfuller","d32c2db4":"#reading the data\ndf_energy=pd.read_csv(\"..\/input\/energy-dataset\/energy_dataset.csv\")","380f054a":"#displaying the first five records\ndf_energy.head()","154c0975":"#\"df.shape\" gives the number of rows and columns in the dataset\ndf_energy.shape","8b08e261":"#understanding the data types and null values in each column\ndf_energy.info()","e1190ca8":"#We need to further understand that whether other columns are really of float type or other datatype\nfor i in df_energy.columns:\n    print(i,\"--->\",df_energy[i].nunique(),\"--->\",df_energy[i].dtypes)","59a42616":"#checking the values in the columns 'generation fossil coal-derived gas','generation fossil oil shale',\n#'generation fossil peat','generation geothermal','generation marine','generation wind offshore'\ncols=['generation fossil coal-derived gas','generation fossil oil shale','generation fossil peat',\n      'generation geothermal','generation marine','generation wind offshore']\nfor values in cols:\n    print(df_energy[values].unique())","687b9ce9":"#deleting the columns above\ndf_energy=df_energy.drop(['generation fossil coal-derived gas','generation fossil oil shale','generation fossil peat','generation geothermal','generation marine','generation wind offshore'],axis=1)","2f756f20":"#dropping the column 'generation hydro pumped storage aggregated' as there are no values in it\ndf_energy=df_energy.drop(['generation hydro pumped storage aggregated'],axis=1)","f5c19c05":"#conversion of datatypes of columns\ncols=['rain_1h','snow_3h','weather_description','weather_main']\ndf_energy[cols]=df_energy[cols].astype(object)","91c4a14d":"#Changing the datatype of time column\ndf_energy[['Date','Time']]=df_energy['time'].str.split(\" \",n=1,expand=True)\ndf_energy['Date']=pd.to_datetime(df_energy['Date'])\ndf_energy[['Time','Spare']]=df_energy['Time'].str.split(\"+\",n=1,expand=True)\n","9976367f":"df_energy=df_energy.drop([\"Spare\",\"time\"],axis=1)\ndf_energy['Time']=pd.to_datetime(df_energy['Time'],format='%H:%M:%S')\ndf_energy['Time']=df_energy['Time'].dt.time","084c91d1":"#Finally checking the columns and the datatype of all the columns \ndf_energy.info()","49933b6b":"#sns.distplot(df_energy['generation biomass'])\nsns.set_color_codes()\nsns.distplot(df_energy['generation biomass'], color=\"b\")\nplt.show()","f8a46b8a":"#creating a new variable fossil and adding up all the power generated from fossil\nfossil=df_energy['generation fossil brown coal\/lignite']+df_energy['generation fossil gas']+df_energy['generation fossil hard coal']+df_energy['generation fossil oil']\nsns.distplot(fossil, color=\"b\")\nplt.show()","50f19964":"#creating a variable renewable and storing adding up all the powers generated from renewable source of energy\nrenewable=df_energy['generation hydro run-of-river and poundage']+df_energy['generation hydro water reservoir']+df_energy['generation hydro pumped storage consumption']+df_energy['generation wind onshore']+df_energy['generation other renewable']+df_energy['generation solar']\nsns.distplot(renewable, color=\"b\")\nplt.show()","b630179e":"sns.distplot(df_energy['generation other'])\nplt.show()","fa54b94d":"sns.distplot(df_energy['total load actual'])\nplt.show()","2234d253":"df_energy['total load actual'].skew()","9aad74c1":"sns.distplot(df_energy['temp'])\nplt.show()","989e2799":"sns.distplot(df_energy['humidity'])\nplt.show()","d0ba2940":"sns.boxplot(df_energy['pressure'])\nplt.show()","c742e34b":"sns.boxplot(df_energy['temp'])","069cbaed":"sns.boxplot(df_energy['wind_speed'])","9f3dd9e7":"sns.boxplot(df_energy['wind_deg'])","258af335":"from scipy.stats.mstats import winsorize\ndf_energy['pressure']=winsorize(df_energy['pressure'],(0.1,0.1))\n","352b30e1":"df_energy['wind_speed']=winsorize(df_energy['wind_speed'],(0.01,0.1))","aa8bb0df":"sns.boxplot(df_energy['pressure'])\nplt.show()","712985fd":"sns.boxplot(df_energy['wind_speed'])\nplt.show()","9b37ee0a":"missing=df_energy.isnull().sum()\nmissing_percent=(df_energy.isna().mean())*100\npd.concat([missing,missing_percent],axis=1,keys=[\"missing\",\"missing_percent\"])","bc761466":"#making a dataframe of all missing values\ndf1=df_energy[df_energy.isnull().any(axis=1)]","81093434":"#plotting a swarmplot of all missing values with respect to date\nsns.swarmplot(x='Date', data=df1)\nplt.xticks(rotation=60)\nplt.title('Missing values with respect to time')\nplt.show()","2d01014c":"#interpolating the missing values\ndf_energy.interpolate(method='linear', limit_direction='forward', inplace=True, axis=0)","5c6cddfb":"#Checking the dataframe after handling the missing values\ndf_energy.isnull().sum()","fe6ba386":"#plotting to see if there is any other missing value in our dataset\nplt.figsize=(15,10)\nsns.heatmap(df_energy.isnull(),cbar=False)\nplt.show()","d4669880":"#Findig the correlation between variables\ndf_energy.corr()\n","804abe83":"#plotting the correlation between variables in which correlation is high\ndf5=df_energy.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(df5[(df5>0.5)|(df5<-0.5)],annot=True,cbar=False,linewidth=0.5,linecolor='blue')","711ee18c":"df_energy=df_energy.drop(['temp_min','temp_max'],axis=1)","f067b2d8":"#segregating the categorical and numeric variables into two variables\ndf_cat=df_energy.select_dtypes(include=object)\ndf_num=df_energy.select_dtypes(include=np.number)","dfc5f373":"#getting dummies for categorical variables\ndf_dummy=pd.get_dummies(df_cat,drop_first=True)","3de405d6":"#creating the final dataframe for model building\ndf_final=pd.concat([df_dummy,df_num],axis=1)","2fa40878":"X=df_final.drop('total load actual',axis=1)","02b8392c":"y=df_final['total load actual']","9e1cbc63":"# add the intercept column using 'add_constant()'\nX= sm.add_constant(X)\n\n\n\n# split data into train subset and test subset for predictor and target variables\n# 'test_size' returns the proportion of data to be included in the test set\n# set 'random_state' to generate the same dataset each time you run the code \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","baf03c8c":"# build a full model using OLS()\nlinreg_full_model = sm.OLS(y_train, X_train).fit()\n","1967cdf0":"# print the summary output\nlinreg_full_model.summary()","f4a95924":"# predict the 'log_Property_Sale_Price' using predict()\npredicted = linreg_full_model.predict(X_test)","5cbf61a9":"# calculate rmse using rmse()\nlinreg_full_model_rmse = rmse(y_test, predicted)\n\n# calculate R-squared using rsquared\nlinreg_full_model_rsquared = linreg_full_model.rsquared\n\n# calculate Adjusted R-Squared using rsquared_adj\nlinreg_full_model_rsquared_adj = linreg_full_model.rsquared_adj ","471fbd66":"# create a list of column names\ncols = ['Model', 'RMSE', 'R-Squared', 'Adj. R-Squared']\n\n# create a empty dataframe of the colums\nresult_tabulation = pd.DataFrame(columns = cols)\n\n# compile the required information\nlinreg_full_model_with_metrics = pd.Series({'Model': \"Linreg full model\",\n                     'RMSE':linreg_full_model_rmse,\n                     'R-Squared': linreg_full_model_rsquared,\n                     'Adj. R-Squared': linreg_full_model_rsquared_adj     \n                   })\n\n# append our result table using append()\n# ignore_index=True: does not use the index labels\n# python can only append a Series if ignore_index=True or if the Series has a name\nresult_tabulation = result_tabulation.append(linreg_full_model_with_metrics, ignore_index = True)\n\n# print the result table\nresult_tabulation","29ffce12":"# create a new variable 'TotalFossil' using the variables 'generation fossil brown coal\/lignite', 'generation fossil gas', 'generation fossil hard coal', and 'generation fossil oil'\n# add the new variable to the dataframe 'df_house'\ndf_energy['TotalFossil'] = df_energy['generation fossil brown coal\/lignite'] + df_energy['generation fossil gas'] + df_energy['generation fossil hard coal'] + df_energy['generation fossil oil']\n\n","06755712":"#segregating the variables into categorical and continuous\ndf_num=df_energy.select_dtypes(include=np.number)\ndf_cat=df_energy.select_dtypes(include=object)","12c9044e":"#dropping the redundant variables\ndf_num=df_num.drop(['generation fossil brown coal\/lignite',\n       'generation fossil gas', 'generation fossil hard coal',\n       'generation fossil oil'], axis=1)","0e97560b":"#getting dummies for categorical variables\ndf_dummy=pd.get_dummies(df_cat,drop_first=True)\n\n#creating the final dataframe for model building\ndf_final=pd.concat([df_dummy,df_num],axis=1)\n\nX=df_final.drop('total load actual',axis=1)\n\ny=df_final['total load actual']","630d54e2":"# add the intercept column using 'add_constant()'\nX= sm.add_constant(X)\n\n\n\n# split data into train subset and test subset for predictor and target variables\n# 'test_size' returns the proportion of data to be included in the test set\n# set 'random_state' to generate the same dataset each time you run the code \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","60d518f1":"linreg_full_model_fossil = sm.OLS(y_train, X_train).fit()\n","f402624f":"predicted = linreg_full_model_fossil.predict(X_test)","32d023d0":"linreg_full_model_fossil_rmse = rmse(y_test, predicted)\n\n# calculate R-squared using rsquared\nlinreg_full_model_fossil_rsquared = linreg_full_model_fossil.rsquared\n\n# calculate Adjusted R-Squared using rsquared_adj\nlinreg_full_model_fossil_rsquared_adj = linreg_full_model_fossil.rsquared_adj ","57b1ffa3":"# create the result table for all accuracy scores\n# accuracy measures considered for model comparision are RMSE, R-squared value and Adjusted R-squared value\n# create a list of column names\ncols = ['Model', 'RMSE', 'R-Squared', 'Adj. R-Squared']\n\n# create a empty dataframe of the colums\n# columns: specifies the columns to be selected\n\n\n# compile the required information\nlinreg_full_model_fossil = pd.Series({'Model': \"Linreg full model with new feature(Total generation by Fossil) \",\n                     'RMSE':linreg_full_model_fossil_rmse,\n                     'R-Squared': linreg_full_model_fossil_rsquared,\n                     'Adj. R-Squared': linreg_full_model_fossil_rsquared_adj     \n                   })\n\n# append our result table using append()\n# ignore_index=True: does not use the index labels\n# python can only append a Series if ignore_index=True or if the Series has a name\nresult_tabulation = result_tabulation.append(linreg_full_model_fossil, ignore_index = True)\n\n# print the result table\nresult_tabulation","d3d3c679":"#dropping the feature added\ndf_energy=df_energy.drop('TotalFossil',axis=1)","6a21f6da":"#creating a variable renewable in which total power generation by renewable energies are added\ndf_energy['renewable']=df_energy['generation other renewable']+df_energy['generation solar']+df_energy['generation wind onshore']+df_energy['generation hydro pumped storage consumption']+df_energy['generation hydro run-of-river and poundage']+df_energy['generation hydro water reservoir']","dc9c8347":"#segregating the categorical and numerical variables\ndf_num=df_energy.select_dtypes(include=np.number)\ndf_cat=df_energy.select_dtypes(include=object)","bf856d2b":"#dropping the redundant variables\ndf_num.drop(['generation hydro pumped storage consumption',\n       'generation hydro run-of-river and poundage',\n       'generation hydro water reservoir', 'generation other renewable', 'generation solar',\n       'generation wind onshore'],axis=1,inplace=True)","83b03d13":"#getting dummies for categorical variables\ndf_dummy=pd.get_dummies(df_cat,drop_first=True)\n\n#creating the final dataframe for model building\ndf_final=pd.concat([df_dummy,df_num],axis=1)\n\nX=df_final.drop('total load actual',axis=1)\n\ny=df_final['total load actual']","c4201c00":"# add the intercept column using 'add_constant()'\nX= sm.add_constant(X)\n\n\n\n# split data into train subset and test subset for predictor and target variables\n# 'test_size' returns the proportion of data to be included in the test set\n# set 'random_state' to generate the same dataset each time you run the code \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","76c4b69d":"linreg_full_model_renewable = sm.OLS(y_train, X_train).fit()\n","24412c71":"predicted = linreg_full_model_renewable.predict(X_test)","b42e7216":"linreg_full_model_renewable_rmse = rmse(y_test, predicted)\n\n# calculate R-squared using rsquared\nlinreg_full_model_renewable_rsquared = linreg_full_model_renewable.rsquared\n\n# calculate Adjusted R-Squared using rsquared_adj\nlinreg_full_model_renewable_rsquared_adj = linreg_full_model_renewable.rsquared_adj ","4a3915f4":"#create a list of column names\ncols = ['Model', 'RMSE', 'R-Squared', 'Adj. R-Squared']\n\n\n\n# compile the required information\nlinreg_full_model_renewable = pd.Series({'Model': \"Linreg full model with new feature(renewable) \",\n                     'RMSE':linreg_full_model_renewable_rmse,\n                     'R-Squared': linreg_full_model_renewable_rsquared,\n                     'Adj. R-Squared': linreg_full_model_renewable_rsquared_adj     \n                   })\n\n# append our result table using append()\nresult_tabulation = result_tabulation.append(linreg_full_model_renewable, ignore_index = True)\n\n# print the result table\nresult_tabulation","8e6cbb28":"#dropping the column added\ndf_energy=df_energy.drop(['renewable'],axis=1)","97bfbabb":"#dropping the dependent variable\ndf_features = df_energy.drop(['total load actual'], axis = 1)\n\n# filter the numerical features in the dataset\ndf_numeric_features_vif = df_features.select_dtypes(include=[np.number])","53f5cd13":"# for each numeric variable, calculate VIF and save it in a dataframe 'vif'\n\n# use for loop to iterate the VIF function \nfor ind in range(len(df_numeric_features_vif.columns)):\n    \n    # create an empty dataframe\n    vif = pd.DataFrame()\n\n    # calculate VIF using list comprehension\n    vif[\"VIF_Factor\"] = [variance_inflation_factor(df_numeric_features_vif.values, i) for i in range(df_numeric_features_vif.shape[1])]\n\n    # create a column of variable names\n    vif[\"Features\"] = df_numeric_features_vif.columns\n\n    # filter the variables with VIF greater than 10 and store it in a dataframe 'multi' \n    # one can choose the threshold other than 10 (it depends on the business requirements)\n    multi = vif[vif['VIF_Factor'] > 10]\n    \n    # if dataframe 'multi' is not empty, then sort the dataframe by VIF values\n    # if dataframe 'multi' is empty (i.e. all VIF <= 10), then print the dataframe 'vif' and break the for loop using 'break' \n    if(multi.empty == False):\n        df_sorted = multi.sort_values(by = 'VIF_Factor', ascending = False)\n    else:\n        print(vif)\n        break\n    \n    # use if-else to drop the variable with the highest VIF\n    #  else print the final dataframe 'vif' with all values after removal of variables with VIF less than 10  \n    if (df_sorted.empty == False):\n        df_numeric_features_vif = df_numeric_features_vif.drop(df_sorted.Features.iloc[0], axis=1)\n    else:\n        print(vif)","39de3aa2":"#creating the final dataframe for model building\ndf_final = pd.concat([df_numeric_features_vif, df_dummy], axis=1)\nX=df_final\ny=df_energy[['total load actual']]","3e28d8bb":"# add the intercept column using 'add_constant()'\nX= sm.add_constant(X)\n\n\n\n# split data into train subset and test subset for predictor and target variables\n# 'test_size' returns the proportion of data to be included in the test set\n# set 'random_state' to generate the same dataset each time you run the code \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","46bd4899":"# build a full model using OLS()\n# consider the log of sales price as the target variable\n# use fit() to fit the model on train data\nlinreg_full_model_vif = sm.OLS(y_train, X_train).fit()\n\n# print the summary output\nprint(linreg_full_model_vif.summary())","d4d0b548":"# predict the 'log_Property_Sale_Price' using predict()\npredicted = linreg_full_model_vif.predict(X_test)","6404d714":"# calculate rmse using rmse()\nlinreg_full_model_vif_rmse = rmse(y_test, predicted)\n\n# calculate R-squared using rsquared\nlinreg_full_model_vif_rsquared = linreg_full_model_vif.rsquared\n\n# calculate Adjusted R-Squared using rsquared_adj\nlinreg_full_model_vif_rsquared_adj = linreg_full_model_vif.rsquared_adj ","0fbd6cbd":"# append the accuracy scores to the table\n# compile the required information\nlinreg_full_model_vif_metrics = pd.Series({'Model': \"Linreg with VIF\",\n                                                'RMSE': rmse(y_test,predicted)[0],\n                                                'R-Squared': linreg_full_model_vif_rsquared,\n                                                'Adj. R-Squared': linreg_full_model_vif_rsquared_adj})\n\n# append our result table using append()\n# ignore_index=True: does not use the index labels\n# python can only append a Series if ignore_index=True or if the Series has a name\nresult_tabulation = result_tabulation.append(linreg_full_model_vif_metrics, ignore_index = True)\n\n# print the result table\nresult_tabulation","6adeab75":"# filter the numerical features in the dataset using select_dtypes()\ndf_numeric_features = df_energy.select_dtypes(include=np.number)\n\n# filter the categorical features in the dataset using select_dtypes()\ndf_categoric_features = df_energy.select_dtypes(include = object)","4e59a02f":"# use 'get_dummies()' from pandas to create dummy variables\ndf_dummy = pd.get_dummies(df_categoric_features, drop_first = True)","6e36b55f":"# concatenate the numerical and dummy encoded categorical variables using concat()\ndf_final = pd.concat([df_numeric_features, df_dummy], axis=1)\nX = df_final.drop(['total load actual'], axis = 1)\ny = df_final[['total load actual']]","5071e148":"# add the intercept column using 'add_constant()'\nX= sm.add_constant(X)\n\n\n\n# split data into train subset and test subset for predictor and target variables\n# 'test_size' returns the proportion of data to be included in the test set\n# set 'random_state' to generate the same dataset each time you run the code \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","34f95257":"# initiate linear regression model to use in feature selection\nlinreg = LinearRegression()\n\n# build step forward selection\nlinreg_forward = sfs(estimator = linreg, k_features = 'best', forward = True, verbose = 2, scoring = 'r2', n_jobs = -1)\n\nsfs_forward = linreg_forward.fit(X_train, y_train)","65874cd4":"# print the number of selected features\nprint('Number of features selected using forward selection method:', len(sfs_forward.k_feature_names_))\n\n# print a blank line\nprint('\\n')\n\n# print the selected feature names when k_features = 'best'\nprint('Features selected using forward selection method are: ')\nprint(sfs_forward.k_feature_names_)","07e19647":"# consider numeric features\ndf_numeric_features = df_energy.loc[:, ['generation biomass', 'generation fossil brown coal\/lignite', 'generation fossil gas', \n                                        'generation fossil hard coal', 'generation fossil oil', 'generation hydro pumped storage consumption', 'generation hydro run-of-river and poundage', \n                                        'generation hydro water reservoir', 'generation nuclear', 'generation other', 'generation other renewable', 'generation solar', 'generation waste', \n                                        'generation wind onshore', 'temp', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'clouds_all']]\n\n# consider categoric features\ndf_categoric_features = df_energy.loc[:, [\"rain_1h\",\"snow_3h\",\"weather_main\",\"weather_description\"]]","7545a6fe":"dummy_encoded_variables = pd.get_dummies(df_categoric_features, drop_first = True)","0ded0661":"df_dummy = pd.concat([df_numeric_features, dummy_encoded_variables], axis=1)\nX=df_dummy\ny = df_energy[['total load actual']]\n","92573385":"# add the intercept column using 'add_constant()'\nX= sm.add_constant(X)\n\n\n\n# split data into train subset and test subset for predictor and target variables\n# 'test_size' returns the proportion of data to be included in the test set\n# set 'random_state' to generate the same dataset each time you run the code \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","a50f14ab":"# build a full model using OLS()\nlinreg_full_model_forward = sm.OLS(y_train, X_train).fit()\n\n# print the summary output\nprint(linreg_full_model_forward.summary())","4ed91d3b":"linreg_full_model_forward_predictions = linreg_full_model_forward.predict(X_test)","09f6558e":"# calculate rmse using rmse()\nlinreg_full_model_forward_rmse = rmse(y_test, linreg_full_model_forward_predictions)\n\n# calculate R-squared using rsquared\nlinreg_full_model_forward_rsquared = linreg_full_model_forward.rsquared\n\n# calculate Adjusted R-Squared using rsquared_adj\nlinreg_full_model_forward_rsquared_adj = linreg_full_model_forward.rsquared_adj ","4c1f5e8d":"# append the accuracy scores to the table\n# compile the required information\nlinreg_full_model_forward_metrics = pd.Series({'Model': \"Linreg with Forward Selection\",\n                                                'RMSE': linreg_full_model_forward_rmse[0],\n                                                'R-Squared': linreg_full_model_forward_rsquared,\n                                                'Adj. R-Squared': linreg_full_model_forward_rsquared_adj})\n\n# append our result table using append()\n# ignore_index=True: does not use the index labels\n# python can only append a Series if ignore_index=True or if the Series has a name\nresult_tabulation = result_tabulation.append(linreg_full_model_forward_metrics, ignore_index = True)\n\n# print the result table\nresult_tabulation","2a173b3b":"# filter the numerical features in the dataset using select_dtypes()\ndf_numeric_features = df_energy.select_dtypes(include=np.number)\n\n# filter the categorical features in the dataset using select_dtypes()\ndf_categoric_features = df_energy.select_dtypes(include = object)","e8908d0d":"# use 'get_dummies()' from pandas to create dummy variables\ndummy_encoded_variables = pd.get_dummies(df_categoric_features, drop_first = True)","f52ca34d":"# concatenate the numerical and dummy encoded categorical variables using concat()\ndf_dummy = pd.concat([df_numeric_features, dummy_encoded_variables], axis=1)\nX = df_dummy.drop(['total load actual'], axis = 1)\n\n# extract the target variable from the data set\ny = df_dummy[['total load actual']]","fc61ca90":"# add the intercept column using 'add_constant()'\nX= sm.add_constant(X)\n\n\n\n# split data into train subset and test subset for predictor and target variables\n# 'test_size' returns the proportion of data to be included in the test set\n# set 'random_state' to generate the same dataset each time you run the code \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","dc5052dd":"# initiate linear regression model to use in feature selection\nlinreg = LinearRegression()\n\n# build step backward feature selection\nlinreg_backward = sfs(estimator = linreg, k_features = 'best', forward = False, verbose = 2, scoring = 'r2', n_jobs = -1)\n\n# fit the backward elimination on train data using fit()\nsfs_backward = linreg_backward.fit(X_train, y_train)","04a5ec1d":"# print the number of selected features\nprint('Number of features selected using backward elimination method:', len(sfs_backward.k_feature_names_))\n\n# print a blank line\nprint('\\n')\n\n# print the selected feature names when k_features = 'best'\nprint('Features selected using backward elimination method are: ')\nprint(sfs_backward.k_feature_names_)","bba39097":"# consider numeric features\ndf_numeric_features = df_energy.loc[:, ['generation biomass', 'generation fossil brown coal\/lignite', 'generation fossil gas', 'generation fossil hard coal', \n                                        'generation fossil oil', 'generation hydro pumped storage consumption', 'generation hydro run-of-river and poundage', \n                                        'generation hydro water reservoir', 'generation nuclear', 'generation other', 'generation other renewable', 'generation solar', 'generation waste', 'generation wind onshore',\n                                        'temp', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'rain_3h', 'clouds_all', 'weather_id']]\n\n# consider categoric features\ndf_categoric_features = df_energy.loc[:, [\"rain_1h\",\"snow_3h\",\"weather_main\",\"weather_description\",\"Time\"]]","3d4989ca":"# use 'get_dummies()' from pandas to create dummy variables\ndummy_encoded_variables = pd.get_dummies(df_categoric_features, drop_first = True)","eaf20b13":"# concatenate the numerical and dummy encoded categorical variables using concat()\ndf_dummy = pd.concat([df_numeric_features, dummy_encoded_variables], axis=1)\nX=df_dummy\ny=df_energy[['total load actual']]","c054963b":"# add the intercept column using 'add_constant()'\nX= sm.add_constant(X)\n\n\n\n# split data into train subset and test subset for predictor and target variables\n# 'test_size' returns the proportion of data to be included in the test set\n# set 'random_state' to generate the same dataset each time you run the code \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \n# print dimension of predictors train set\nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","908e382c":"linreg_full_model_backward = sm.OLS(y_train, X_train).fit()\n\n# print the summary output\nprint(linreg_full_model_backward.summary())","cb29f50b":"# predict the 'log_Property_Sale_Price' using predict()\nlinreg_full_model_backward_predictions = linreg_full_model_backward.predict(X_test)","125dceae":"# calculate rmse using rmse()\nlinreg_full_model_backward_rmse = rmse(y_test, linreg_full_model_backward_predictions)\n\n# calculate R-squared using rsquared\nlinreg_full_model_backward_rsquared = linreg_full_model_backward.rsquared\n\n# calculate Adjusted R-Squared using rsquared_adj\nlinreg_full_model_backward_rsquared_adj = linreg_full_model_backward.rsquared_adj ","d101331c":"# append the accuracy scores to the table\nlinreg_full_model_backward_metrics = pd.Series({'Model': \"Linreg with Backward Elimination\",\n                                                'RMSE': linreg_full_model_backward_rmse[0],\n                                                'R-Squared': linreg_full_model_backward_rsquared,\n                                                'Adj. R-Squared': linreg_full_model_backward_rsquared_adj})\n\n# append our result table using append()\nresult_tabulation = result_tabulation.append(linreg_full_model_backward_metrics, ignore_index = True)\n\n# print the result table\nresult_tabulation","5a8338d5":"#segregating the categorical and numerical variables\ndf_num=df_energy.select_dtypes(include=np.number)\ndf_cat=df_energy.select_dtypes(include=object)\n","08085307":"#getting dummies for categorical variables\ndf_dummy=pd.get_dummies(df_cat,drop_first=True)\n\n#creating the final dataframe for model building\ndf_final=pd.concat([df_dummy,df_num],axis=1)\n\nX=df_final.drop('total load actual',axis=1)\n\ny=df_final['total load actual']\n","70945602":"# split data into train subset and test subset for predictor and target variables\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)\n\n# check the dimensions of the train & test subset for \nprint(\"The shape of X_train is:\",X_train.shape)\n\n# print dimension of predictors test set\nprint(\"The shape of X_test is:\",X_test.shape)\n\n# print dimension of target train set\nprint(\"The shape of y_train is:\",y_train.shape)\n\n# print dimension of target test set\nprint(\"The shape of y_test is:\",y_test.shape)","70c940d9":"from sklearn.linear_model import LinearRegression\n# build the model\nOLS_model = LinearRegression()\n\n# fit the model\nOLS_model.fit(X_train, y_train)\n","5dfd2881":"# predict the values\ny_pred_OLS = OLS_model.predict(X_test)","ec13c200":"# compute the R-Squared\nr_squared_OLS = OLS_model.score(X_train,y_train)\n\n# Number of observation or sample size\nn = 24544 \n\n# No of independent variables\np = 85\n\n#Compute Adj-R-Squared\nAdj_r_squared_OLS = 1 - (1-r_squared_OLS)*(n-1)\/(n-p-1)\n\n# Compute RMSE\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrmse_OLS = sqrt(mean_squared_error(y_test, y_pred_OLS))\n\n","495e4e80":"# append the accuracy scores to the table\nlinreg_full_model_SGD = pd.Series({'Model': \"Linreg with SGD\",\n                                                'RMSE': rmse_OLS,\n                                                'R-Squared': r_squared_OLS,\n                                                'Adj. R-Squared':Adj_r_squared_OLS})\n\n# append our result table using append()\nresult_tabulation = result_tabulation.append(linreg_full_model_SGD, ignore_index = True)\n\n# print the result table\nresult_tabulation","aa99124b":"plt.rcParams['figure.figsize'] = [10,8]","e3d6e49a":"result=pd.DataFrame({'Model':[1,2,3,4,5,6,7],\n                     'RMSE':[1238.66,1280.40,1696.45,6584.66,6000.29,6552.28,1238.61]}                                                                                                                             \n                   )\nresult.plot(kind='bar',x='Model',y='RMSE')","3ff938bc":"#copying the dataframe in another dataframe\ndf=df_energy.copy(deep=True)","c9657e25":"#displaying the first five records\ndf.head()","377efd9c":"#Dropping all the columns except total actual load and date\n#As in time series forecasting we reuire the column to be forecasted and the date\ncols = ['generation biomass', 'generation fossil brown coal\/lignite',\n       'generation fossil gas', 'generation fossil hard coal',\n       'generation fossil oil', 'generation hydro pumped storage consumption',\n       'generation hydro run-of-river and poundage',\n       'generation hydro water reservoir', 'generation nuclear',\n       'generation other', 'generation other renewable', 'generation solar',\n       'generation waste', 'generation wind onshore',\n       'temp', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'rain_1h',\n       'rain_3h', 'snow_3h', 'clouds_all', 'weather_id', 'weather_main',\n       'weather_description','Time']\ndf=df.drop(cols,axis=1)\ndf = df.sort_values('Date')\n","14e94c33":"#grouping the data by date and taking the sum of all the load on that date\ndf = df.groupby('Date')['total load actual'].sum().reset_index()","3c596886":"#setting the index of the dataframe to date\ndf.set_index('Date', inplace=True)","2171aed7":"#displaying the final dataframe\ndf.head()","e5823b48":"#plotting the dataframe in time axis\ndf.plot(figsize=(15, 6))\nplt.show()","1d1065ba":"#resampling the data by month as working with the current data is difficult due to lots of data\ny = df['total load actual'].resample('MS').mean()","121314f5":"\ndecomposition = seasonal_decompose(y)\n\nplt.plot(y, label = 'Original')\nplt.legend(loc = 'best')\n\ntrend = decomposition.trend\nplt.show()\nplt.plot(trend, label = 'Trend')\nplt.legend(loc = 'best')\n\nseasonal = decomposition.seasonal\nplt.show()\nplt.plot(seasonal, label = 'Seasonal')\nplt.legend(loc = 'upper right')\n\nresidual = decomposition.resid\nplt.show()\nplt.plot(residual, label = 'Residual')\nplt.legend(loc='best')","293786ed":"from pandas import Series\nfrom statsmodels.tsa.stattools import adfuller\n#series = Series.from_csv('daily-total-female-births.csv', header=0)\nresult = adfuller(y)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))","80d870d5":"#Differencing to make the series stationary\ny = y - y.shift(1)","b94d5db5":"#plotting the series after differencing\ny.dropna(inplace=True)\ny.plot()","1d10432d":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(y)\n\nplt.plot(y, label = 'Original')\nplt.legend(loc = 'best')\n\ntrend = decomposition.trend\nplt.show()\nplt.plot(trend, label = 'Trend')\nplt.legend(loc = 'best')\n\nseasonal = decomposition.seasonal\nplt.show()\nplt.plot(seasonal, label = 'Seasonal')\nplt.legend(loc = 'best')\n\nresidual = decomposition.resid\nplt.show()\nplt.plot(residual, label = 'Residual')\nplt.legend(loc='best')","8ec03cb7":"#dividing the data into test and train\nsize = int(len(y) * 0.95)\ntrain, test = y[0:size], y[size:len(y)]","3233fa1b":"p = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","0e104468":"from pylab import rcParams\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = sm.tsa.statespace.SARIMAX(y, order=param,\nseasonal_order=param_seasonal,\nenforce_stationarity=False, \nenforce_invertibility=False)\n            results = mod.fit()\n            print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n        except:\n            continue","8276e5e0":"mod = sm.tsa.statespace.SARIMAX(y,\n                                order=(0, 1, 1),\n                                seasonal_order=(0, 1, 1, 12),\n                                enforce_invertibility=False)\nresults = mod.fit()\nprint(results.summary().tables[1])","254397eb":"results.plot_diagnostics(figsize=(16, 8))\nplt.show()","6ee01389":"#set forecasts to start at 2017\u201301\u201301 to the end of the data to forecast\npred = results.get_prediction(start=pd.to_datetime('2017-01-01'), dynamic=False)\npred_ci = pred.conf_int()\nax = y['2015':].plot(label='observed')\npred.predicted_mean.plot(ax=ax, label='One-step ahead Forecast', alpha=.7, figsize=(14, 7))\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.2)\nax.set_xlabel('Date')\nax.set_ylabel('Total Load Actual')\nplt.legend()\nplt.show()","94e20395":"y_forecasted = pred.predicted_mean\ny_truth = train['2016-01-01':]\nmse = ((y_forecasted - y_truth) ** 2).mean()\nprint('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n\nprint('The Root Mean Squared Error of our forecasts is {}'.format(round(np.sqrt(mse), 2)))","91032a04":"From the graphs plotted above we can see that maximum graphs are normally distributed.The new variable renewable is slightly right skewed and the feature humidity is slightly left skewed.","bff7c53f":"# Reading the dataframe","1337ea78":"# Fitting the ARIMA model","099f1f67":"# Time Series Forcasting using ARIMA","f6a96551":"In this case study we need to predict the total load consumption depending upon different parameters, such as generation from different sources and the weather condition.","ba3bb979":"# Model 7(Linear Regression using SGD)","9ee24d75":"# Prediction of Total Energy Consumption","d0246327":"# Extrapolatory Data Analysis ","8bbc1d5e":"The temperature minumim column and the temperature maximum column are having high correlation.So we will drop these columns to avoid multicollinearity.","ab7090bc":"# Checking for the missing values","11c752a9":"# Model 2(using feature engineering(Total generation from fossil))","15a200ad":"# Time Series Analysis","ec23a041":"# Decomposing\n\nDecomposing the time series into three distinct components: trend, seasonality, and noise.","7c01e582":"# Model 1(Ordinary least square)","9f764598":"# Model 6(Using Backward elimination) ","4387f2ba":"# Model 3(Using feature engineering(Total generation from renewable energy))","ed1d7ced":"Checking the columns after handling the outliers","17ce63b7":"We can see from the above output that these columns doesnot have any values other than 0.So we delete these columns.","97859a2f":"# Preparing the data","3ceee4ba":"We see that the columns \"generation fossil coal-derived gas\",\"generation fossil oil shale\",\"generation fossil peat\",\"generation geothermal\",\"generation marine\",\"generation wind offshore\" have just one value.So these should be of object type.Let us explore it further.","ca0aaec2":"# Data Preparation for model building","d135a8e7":"1)From the above display we can see that the datatype of time column is object whereas it should be in datetime format.\n\n2)The columns \"generation hydro pumped storage aggregated\" has null values.There are even missing values in other columns which needs to be handled. ","ca7cd854":"Now our dataset is ready for model building","763de5e9":"# Model 4(Using VIF selecting the important features)","e84a543e":"As the p-value is greater than 0.05, it means the series is not stationary.Even the statistics value is greater than the 1% critical value so we can conclude that the series is not stationary. ","962fa75b":"So we have handled the missing values.Now our dataset is ready to used for making model.","4b2db9cd":"\n# Importing Libraries","72b10657":"# Running Model Diagnostics","cf10ccc5":"# Model 5(Using forward elimination)","2e5ca89f":"# Conclusion:","1a723bc8":"## Understanding the data features:\n\n1)generation biomass-Power generated by biomass\n\n2)generation fossil brown coal\/lignite-Power generated by fossil brown coal\/lignite\n\n3)generation fossil gas-power generated by fossil gas\n\n4)generation fossil hard coal-power generated by fossil hard coal\n\n5)generation fossil oil-power generated by fossil oil\n\n6)generation hydro pumped storage consumption-power generated by pumped storage consumption(This is used as an emergency power                                               resource) \n\n7)generation hydro run-of-river and poundage-power generated by hydro run\n\n8)generation hydro water reservoir-power generated by water reservior\n\n9)generation nuclear-power generated by nuclear energy\n\n10)generation other-power generated by other sources\n\n11)generation other renewable-power generated by other renewable energies other than mentioned in the dataset\n\n12)generation solar-power generated by solar energy\n\n13)generation waste-power generated by waste\n\n14)generation wind onshore-power generated by wind onshore\n\n15)total load actual-__This is the dependent varibale.It tell us about the total load consumption.__\n\n16)temp- Temperature of the area when load consumption was recorded\n\n17)pressure-Pressure of the area when load consumption was recorded\n\n18)humidity-Humidity of the area when load consumption was recorded.\n\n19)wind_speed-Wind speed of the area when load consumption was recorded.\n\n20)wind_deg-Wind direction of the area when load consumption was recorded.\n\n21)rain_1h-It tells us about the intensity of rainfall.\n\n22)snow_3h-It is divided into 4 values and tells us about the intensity of snowfall.\n\n23)weather_id-It gives us 23 values of different weather condition. \n\n24)weather_main-Even this column tells us about the weather condition i.e whether it was clear or cloudy or it was raining when                 the load was recorded.\n\n25)weather_description-It tells us about the overcast,whether it was raining or sunny.\n\n26)time-It gives us the date and time when load was recorded","cb893eb5":"We can see there are many missing values in the starting of the dataframe.","ff517ad7":"We can see that there are many outliers in the pressure and temperature column.We will handle these outliers.","ff9a108f":"# Parameter Selection ","e74588a9":"There are 35064 rows and 37 columns","33e26e09":"Total 7 models have been built to predict the total load consumption depending upon various generation and weather factors.\nOut of all the models we select the 7th model that is linear regression using SGD to predict the total load consumption because \nit has got the best Adjusted R-squared value and least RMSE value.\n\nAs we know that Adjusted R-squared gives us information about the best features added and RMSE gives us information about \nthe least difference between actual and predicted value.Since RMSE value for linear regression with SGD is minimum so we select\nthis model to predict the power consumption.\n\nEven from the statistical summary if we see the AIC,BIC and log-likelihood values, then we can observe that the AIC and BIC values\nof linear regression with SGD is minimum.AIC and BIC is the penalty that is given to the model for losing information during model\nbuilding.So, as the values of AIC and BIC is minimum for the model we select this model.","101ee330":"# Checking Stationarity","9504b44a":"# Validating Forecasts","f4814519":"Now our dataset is ready for doing EDA","d5fba644":"We can see that the columns have been handled and the datatype of time column has been changed.","f0c9d110":"# Understanding the dataset"}}