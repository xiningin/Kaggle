{"cell_type":{"49f454d1":"code","3bc343ef":"code","b015c277":"code","361610d9":"code","f132f5ad":"code","4ef8e034":"code","75b134c6":"code","20d3e84f":"code","266a1a09":"code","2169836f":"code","8cc98cef":"code","169707a0":"code","460dccb2":"code","6f3ef82f":"code","113ca11e":"code","96117e41":"code","b04095ab":"code","80364020":"code","8d7670b1":"code","89e28c5d":"code","6a813945":"code","b8b72307":"code","c0917699":"code","f81c1aa8":"code","7e61614e":"code","e42b475e":"code","9868a3cd":"code","df4ea768":"code","0244824d":"code","d7e82f35":"markdown","cf80d2fb":"markdown","1fa3fe52":"markdown","d7ba6f09":"markdown","de1a6ab0":"markdown","b283acd7":"markdown","2912c41d":"markdown","3249bee3":"markdown","0531e887":"markdown","9a327f72":"markdown","68974b6a":"markdown","459f0794":"markdown","254a9bcb":"markdown","79a1782d":"markdown","966ec492":"markdown","89ff7849":"markdown","e0252b9d":"markdown","26948707":"markdown","32616ec5":"markdown","d6b885ea":"markdown","6934e975":"markdown","a8e9178c":"markdown","3d4252e6":"markdown","24185cef":"markdown","84fbe6d6":"markdown","5ba48369":"markdown","eb4fa179":"markdown","64858dea":"markdown","b2df0ef5":"markdown","5e9b8468":"markdown","40861a65":"markdown","bfa7624b":"markdown","54917ab5":"markdown","85f8b401":"markdown","5cae1772":"markdown","65e26d7b":"markdown","26e80def":"markdown","44287054":"markdown","76709e3f":"markdown","dfea64ca":"markdown","676eae7a":"markdown","398f1501":"markdown","9cffdd6e":"markdown","41666f88":"markdown","c87be80d":"markdown","4682cc1b":"markdown","73df5ef7":"markdown","1c7281d0":"markdown","b626f961":"markdown","6ee9c29a":"markdown","7b89fb8c":"markdown","67886cc4":"markdown"},"source":{"49f454d1":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","3bc343ef":"# Deleting index form dataset so we don't compromise the predictions\ntrain = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')\nx = train.iloc[:, 1:-1] # Training Dataset without dependient variable and index (pandas index = dataset index - 1)\ny = train.iloc[:, -1] # Training Dependient variable\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0) # Train and test set split\n\ntest_final = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv').iloc[:, 1:] # Test Dataset variable and index (pandas index = dataset index - 1)","b015c277":"display(train)","361610d9":"train.isnull().sum()","f132f5ad":"dtypes_train = train.dtypes\ndisplay(dtypes_train)","4ef8e034":"fig, ax = plt.subplots(2, 3, figsize = (30, 20))\n\n# Gender plot\ng_gender = sns.countplot(\n    data = train,\n    x = 'Gender',\n    palette = sns.color_palette('Set2'),\n    ax = ax[0, 0]\n)\n\n# Driving License plot\ng_driving_license = sns.countplot(\n    data = train,\n    x = 'Driving_License',\n    palette = sns.color_palette('Set2'),\n    ax = ax[1, 0]\n)\n\n# Previously insured plot\ng_previously_insured = sns.countplot(\n    data = train,\n    x = 'Previously_Insured',\n    palette = sns.color_palette('Set2'),\n    ax = ax[0, 1]\n)\n\n# Vehicle damage plot\ng_damage = sns.countplot(\n    data = train,\n    x = 'Vehicle_Damage',\n    palette = sns.color_palette('Set2'),\n    ax = ax[1, 1]\n)\n\n# Vehicle Age plot\ng_vehicle_age = sns.countplot(\n    data = train,\n    x = 'Vehicle_Age',\n    palette = sns.color_palette('Set2'),\n    ax = ax[0, 2]\n)\n\n# Response plot\ng_response = sns.countplot(\n    data = train,\n    x = 'Response',\n    palette = sns.color_palette('Set2'),\n    ax = ax[1, 2]\n)\n\n# Titles\nax[0, 0].set_title('Gender', fontsize=20)\nax[1, 0].set_title('Driving License', fontsize=20)\nax[0, 1].set_title('Previously Insured', fontsize=20)\nax[1, 1].set_title('Vehicle Damage', fontsize=20)\nax[0, 2].set_title('Vehicle Age', fontsize=20)\nax[1, 2].set_title('Response', fontsize=20)\n\n# Delete x and y labels\nfor ax in ax.reshape(-1): \n  ax.set_xlabel(None)\n  ax.set_ylabel(None)\n\n# Super title\nfig.suptitle('Categorical variables', size = '40', y = 1.0)\n\nplt.tight_layout() # Plots fit the fig area\nplt.show()\nplt.close()","75b134c6":"fig, ax = plt.subplots(5, 1, figsize = (30, 20))\n\n# Age plot\ng_edad = sns.histplot(\n    data = train,\n    x = 'Age',\n    kde=True,\n    ax = ax[0]\n)\n\n# Region Code plot\ng_region = sns.histplot(\n    data = train,\n    x = 'Region_Code',\n    kde=True,\n    ax = ax[1]\n)\n\n# Premium annual plot\ng_premium_anual = sns.histplot(\n    data = train,\n    x = 'Annual_Premium',\n    kde=True,\n    ax = ax[2]\n)\n\n# Policy sales channel plot\ng_policy_sales_channel = sns.histplot(\n    data = train,\n    x = 'Policy_Sales_Channel',\n    kde=True,\n    ax = ax[3]\n)\n\n# Vintage plot\ng_vintage = sns.histplot(\n    data = train,\n    x = 'Vintage',\n    kde=True,\n    ax = ax[4]\n)\n\n# Titles\nax[0].set_title('Age', fontsize=20)\nax[1].set_title('Region Code', fontsize=20)\nax[2].set_title('Annual Premium', fontsize=20)\nax[3].set_title('Policy Sales Channel', fontsize=20)\nax[4].set_title('Vintage', fontsize=20)\n\n# Set x ticks limit to premium annual due to its nonuniform distribution\nax[2].set_xlim(0, 90000)\n\n# Delete x and y label\nfor ax in ax.reshape(-1): \n  ax.set_xlabel(None)\n  ax.set_ylabel(None)\n\n# Super title\nfig.suptitle('Categorical variables', size = '40', y = 1.0)\n\nplt.tight_layout()  # Plots fit the fig area\nplt.show()\nplt.close()","20d3e84f":"# Vehicle age vs Gender vs Vehicle Damage vs Response plot\ng_vage_gender_vdama = sns.catplot(\n    data = train,\n    y = 'Vehicle_Age', hue = 'Response', col = 'Gender', row = 'Vehicle_Damage',    \n    kind = 'count', height=6, aspect=13\/6,    \n    legend = True,  \n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n    edgecolor = '.6',\n    margin_titles = True,\n    legend_out = False)\n\n# Title axis & super title  \ng_vage_gender_vdama.set_titles(size = '30')\ng_vage_gender_vdama.set_axis_labels(\"Total\", \"Vehicle Age\", size = '30')\ng_vage_gender_vdama.fig.suptitle('Vehicle Age vs Gender vs Vehicle Damage vs Response', size = '40', y = 1.1)\nplt.setp(g_vage_gender_vdama._legend.get_texts(), fontsize=25)\nplt.setp(g_vage_gender_vdama._legend.get_title(), fontsize=25)\n\nplt.tight_layout() # Plots fit the fig area\nplt.show()\nplt.close()","266a1a09":"# Previously Insured vs Vehicle Age vs Driving License Plot\ng_pins_vdama = sns.catplot(\n    data = train,\n    hue = 'Response', x = 'Vehicle_Age', col = 'Previously_Insured', row = 'Driving_License',    \n    kind = 'count', height=6, aspect=13\/6,\n    legend = True,  \n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n    edgecolor = '.6',\n    margin_titles = True,\n    legend_out = False\n)\n\n# Title axis & super title  \ng_pins_vdama.set_titles(size = '30')\ng_pins_vdama.set_axis_labels(\"Vehicle Age\", \"Total\", size = '30')\ng_pins_vdama.fig.suptitle('Previously Insured vs Vehicle Age vs Driving License vs Response', size = '40', y = 1.1)\nplt.setp(g_pins_vdama._legend.get_texts(), fontsize=25)\nplt.setp(g_pins_vdama._legend.get_title(), fontsize=25)\n\nplt.tight_layout()  # Plots fit the fig area\nplt.show()\nplt.close()","2169836f":"fig, ax = plt.subplots(4, 1, figsize = (20, 20))\n\n# Age vs Response Plot\ng_age_response = sns.histplot(\n    data = train,\n    x = 'Age', hue = 'Response',\n    multiple = 'fill',\n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n    edgecolor = '.6',\n    linewidth = .5,\n    ax = ax[0]\n)\n# x ticks\nax[0].set_xticks(np.arange(19, 85, 2))\n\n# Region vs Age vs Response Plot\ng_region_ = sns.violinplot(\n    data = train,     \n    x = 'Region_Code', y = 'Age', hue = 'Response', \n    split = True, inner = 'quart', linewidth = 1,\n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n    edgecolor = '.6',\n    ax = ax[1]\n)\n#x ticks\nax[1].set_xticks(np.arange(0, 55, 2))\n\n# Region vs Policy Sales Channel vs Response Plot\ng_region_policy_response = sns.violinplot(\n    data = train,     \n    x = 'Region_Code', y = 'Policy_Sales_Channel', hue = 'Response', \n    split = True, inner = 'quartile', linewidth = 1,\n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n    edgecolor = '.6',\n    ax = ax[2]\n)\n#x ticks\nax[2].set_xticks(np.arange(0, 55, 2))\n\n# Region vs Response\ng_region_response = sns.histplot(\n    data = train,\n    x = 'Region_Code', hue = 'Response',\n    multiple = 'fill',\n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n    edgecolor = '.6',\n    linewidth = .5,\n    ax = ax[3]\n)\n#x ticks\nax[3].set_xticks(np.arange(0, 55, 2))\n\n# Titles\nax[0].set_title('Age', fontsize=20)\nax[1].set_title('Region Code - Age', fontsize=20)\nax[2].set_title('Region Code - Policy Channel', fontsize=20)\nax[3].set_title('Region Code', fontsize=20)\n\nfig.tight_layout() # Plots fit the fig area\nplt.show()\nplt.close()","8cc98cef":"# Vintage vs Response Plot\ng_vintage = sns.displot(\n    data = train,\n    x = 'Vintage', hue = 'Response',\n    kind=  'kde', height=6, aspect=3,\n    multiple = 'fill',\n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n)\ng_vintage.fig.suptitle('Vintage', size = '30', y = 1.1)\n\n# Annual Premium vs Response\ng_annual_premium = sns.displot(\n    data=train,\n    x = 'Annual_Premium', hue = 'Response',\n    kind = 'kde', height=6, aspect=3,\n    multiple = 'fill',\n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n)\n# Title, xticks, xlimit (nonuniform distribution)\ng_annual_premium.fig.suptitle('Annual Premium', size = '30', y = 1.1)\ng_annual_premium.set(xticks=np.arange(0, 100000, 10000))\ng_annual_premium.set(xlim=(0,90000))\n\n# Scatter plots: Annual Premium vs (Vintage or Region or Policy Sales Channel or Age)\nfig, ax = plt.subplots(5, 1, figsize=(30, 40))\n\ng_vint_premium = sns.scatterplot(\n    data = train, x = 'Vintage', y = 'Annual_Premium', hue = 'Response',\n    ci = None,\n    linewidth=2.5,\n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n    ax=ax[0])\nax[0].set_title('Vintage - Annual Premium', fontsize = 30)\n\ng_region_premium = sns.scatterplot(\n    data = train, x = 'Region_Code', y = 'Annual_Premium', hue = 'Response',\n    ci = None,\n    linewidth=2.5,\n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n    ax=ax[1])\nax[1].set_title('Region Code - Annual Premium', fontsize = 30)\n\ng_pol_premium = sns.scatterplot(\n    data = train, x = 'Policy_Sales_Channel', y = 'Annual_Premium', hue = 'Response',\n    ci = None,\n    linewidth=2.5,\n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n    ax=ax[2])\nax[2].set_title('Policy Sales Channel - Annual Premium', fontsize = 30)\n\ng_age_premium = sns.scatterplot(\n    data = train, x = 'Age', y = 'Annual_Premium', hue = 'Response',\n    ci = None,\n    linewidth=2.5,\n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n    ax=ax[3])\nax[3].set_title('Age - Annual Premium', fontsize = 30)\n\ng_vehicle_age_premium = sns.histplot(\n    data = train, x = 'Vehicle_Age', y = 'Annual_Premium', hue = 'Response',\n    linewidth=2.5,\n    palette = sns.color_palette(['#E74C3C', '#58D68D']),\n    ax=ax[4])\nax[4].set_title('Vehicle Age - Annual Premium', fontsize = 30)\n\nfig.tight_layout() # Plots fit the fig area\nplt.show()\nplt.close()\n","169707a0":"# Generate boolean values for categorical columns (train set & test set)\nle = LabelEncoder()\nx_train = pd.get_dummies(x_train, columns=['Gender', 'Vehicle_Age', 'Policy_Sales_Channel', 'Region_Code'], prefix=['Gender', 'Vehicle_Age', 'Policy_Sales_Channel', 'Region_Code'])\nx_train['Vehicle_Damage'] = le.fit_transform(x_train['Vehicle_Damage']) # Yes -> 1 | No -> 0\n\nx_test = pd.get_dummies(x_test, columns=['Gender', 'Vehicle_Age', 'Policy_Sales_Channel', 'Region_Code'], prefix=['Gender', 'Vehicle_Age', 'Policy_Sales_Channel', 'Region_Code'])\nx_test['Vehicle_Damage'] = le.fit_transform(x_test['Vehicle_Damage']) # Yes -> 1 | No -> 0\n\ntest_final = pd.get_dummies(test_final, columns=['Gender', 'Vehicle_Age', 'Policy_Sales_Channel', 'Region_Code'], prefix=['Gender', 'Vehicle_Age', 'Policy_Sales_Channel', 'Region_Code'])\ntest_final['Vehicle_Damage'] = le.fit_transform(test_final['Vehicle_Damage']) # Yes -> 1 | No -> 0\n\n# Columns not ordered\ndisplay(x_train)\ndisplay(x_test)\n\n# Reordering columns so categorical data are the last columns\ncols = list(x_train.columns) # list of columns names\ncols = cols[0:1] + cols[4:6] + cols[1:4] + cols[4:] # Reordering\nx_train = x_train[cols] # Copy\n\n# Get missing columns in the training test\nmissing_cols_test = set(x_train.columns) - set(x_test.columns)\nmissing_cols_finaltest = set(x_train.columns) - set(test_final.columns)\n\n# Add a missing column in test set with default value equal to 0\nfor c in missing_cols_test:\n    x_test[c] = 0\n\nfor c in missing_cols_finaltest:\n    test_final[c] = 0\n\n# Ensure test-set columns follows same order than in train-set and copy\nx_test = x_test[x_train.columns]\ntest_final = test_final[x_train.columns]\n\n# Columns ordered\ndisplay(x_train)\ndisplay(x_test)\ndisplay(test_final)","460dccb2":"# Continous variables standarization\nsc = StandardScaler()\n\n# Continous variables only\nx_train.iloc[:, :3] = sc.fit_transform(x_train.iloc[:, :3])\nx_test.iloc[:, :3] = sc.fit_transform(x_test.iloc[:, :3])\ntest_final.iloc[:, :3] = sc.fit_transform(test_final.iloc[:, :3])\n\ndisplay(x_train)\ndisplay(x_test)\ndisplay(test_final)","6f3ef82f":"accuracy_1_scores = np.empty(5)\naccuracy_total = np.empty(5)","113ca11e":"logmodel = LogisticRegression(solver='lbfgs', max_iter=200) # Logistic Regression\nlogmodel.fit(x_train, y_train) # Training...\npredictions_logmodel = logmodel.predict(x_test) # Predictions over test set","96117e41":"# Reports: confussion matrix & accuracy score\ncm = confusion_matrix(y_test, predictions_logmodel)\nlog_model_acc = accuracy_score(predictions_logmodel, y_test)\n\nprint('Accuracy: ', log_model_acc)\nprint('\\nREPORT:\\n', classification_report(y_test, predictions_logmodel))\n\n# Confussion matrix table\nfig, ax =plt.subplots(1, figsize=(10,4))\n\nlabels_pred =['Pred 0', 'Pred 1']\nlabels_result =['Actual 0', 'Actual 1']\nax.axis('tight')\nax.axis('off')\ncolors = [['g', 'r'],[ 'r', 'g']]\n\nconf_matrix_table = ax.table(   cellText = cm, \n                                colLabels = labels_pred,\n                                rowLabels = labels_result,    \n                                loc = 'center',   \n                                cellColours = colors, \n                                cellLoc = 'center')\n\nax.set_title('Matriz de confusi\u00f3n', fontsize = 20)\nconf_matrix_table.auto_set_font_size(False)\nconf_matrix_table.set_fontsize(8)\nconf_matrix_table.set_fontsize(14)\nconf_matrix_table.scale(1.5, 1.5)\n\n# Pred 1 rate\npred_1_rate = cm[1, 1] \/ (cm[0, 1] + cm[1, 1])\n\naccuracy_1_scores[0] = pred_1_rate\naccuracy_total[0] = log_model_acc\n\nprint('Actual 1 prediction rate: ', pred_1_rate)\n","b04095ab":"knn_model = KNeighborsClassifier(metric='minkowski', p=2, n_jobs=-1) # kNN model\nknn_model.fit(x_train, y_train) # Training...\npredictions_knn_model = knn_model.predict(x_test) # Predictions over test set","80364020":"# Reports: confussion matrix & accuracy score\ncm = confusion_matrix(y_test, predictions_knn_model)\nknn_model_acc = accuracy_score(predictions_knn_model, y_test)\n\nprint('Accuracy: ', knn_model_acc)\nprint('\\nREPORT:\\n', classification_report(y_test, predictions_knn_model))\n\n# Confussion matrix table\nfig, ax =plt.subplots(1, figsize=(10,4))\n\nlabels_pred =['Pred 0', 'Pred 1']\nlabels_result =['Actual 0', 'Actual 1']\nax.axis('tight')\nax.axis('off')\ncolors = [['g', 'r'],[ 'r', 'g']]\n\nconf_matrix_table = ax.table(   cellText = cm, \n                                colLabels = labels_pred,\n                                rowLabels = labels_result,    \n                                loc = 'center',   \n                                cellColours = colors, \n                                cellLoc = 'center')\n\n\nax.set_title('Matriz de confusi\u00f3n', fontsize = 20)\nconf_matrix_table.auto_set_font_size(False)\nconf_matrix_table.set_fontsize(8)\nconf_matrix_table.set_fontsize(14)\nconf_matrix_table.scale(1.5, 1.5)\n\n# Pred 1 rate\npred_1_rate = cm[1, 1] \/ (cm[0, 1] + cm[1, 1])\n\naccuracy_1_scores[1] = pred_1_rate\naccuracy_total[1] = knn_model_acc\n\nprint('Actual 1 prediction rate: ', pred_1_rate)","8d7670b1":"nb_model = GaussianNB() # Naive Bayes model\nnb_model.fit(x_train, y_train) # Training...\npredictions_nb_model = nb_model.predict(x_test) # Predictions over test set","89e28c5d":"# Reports: confussion matrix & accuracy score\ncm = confusion_matrix(y_test, predictions_nb_model)\nnb_model_acc = accuracy_score(predictions_nb_model, y_test)\n\nprint('Accuracy: ', nb_model_acc)\nprint('\\nREPORT:\\n', classification_report(y_test, predictions_nb_model))\n\n# Confussion matrix table\nfig, ax =plt.subplots(1, figsize=(10,4))\n\nlabels_pred =['Pred 0', 'Pred 1']\nlabels_result =['Actual 0', 'Actual 1']\nax.axis('tight')\nax.axis('off')\ncolors = [['g', 'r'],[ 'r', 'g']]\n\nconf_matrix_table = ax.table(   cellText = cm, \n                                    colLabels = labels_pred,\n                                    rowLabels = labels_result,\n                                    loc = 'center',   \n                                    cellColours = colors, \n                                    cellLoc = 'center')\n\n\nax.set_title('Matriz de confusi\u00f3n', fontsize = 20)\nconf_matrix_table.auto_set_font_size(False)\nconf_matrix_table.set_fontsize(8)\nconf_matrix_table.set_fontsize(14)\nconf_matrix_table.scale(1.5, 1.5)\n\n# Pred 1 rate\npred_1_rate = cm[1, 1] \/ (cm[0, 1] + cm[1, 1])\n\naccuracy_1_scores[2] = pred_1_rate\naccuracy_total[2] = nb_model_acc\n\nprint('Actual 1 prediction rate: ', pred_1_rate)","6a813945":"dtc_model = DecisionTreeClassifier(criterion = 'entropy', random_state = 0) # Decision Tree Model\ndtc_model.fit(x_train, y_train) # Training...\npredictions_dtc_model = dtc_model.predict(x_test) # Predictions over test set","b8b72307":"# Reports: confussion matrix & accuracy score\ncm = confusion_matrix(y_test, predictions_dtc_model)\ndtc_model_acc = accuracy_score(predictions_dtc_model, y_test)\n\nprint('Accuracy: ', dtc_model_acc)\nprint('\\nREPORT:\\n', classification_report(y_test, predictions_dtc_model))\n\n# Confussion matrix table\nfig, ax =plt.subplots(1, figsize=(10,4))\n\nlabels_pred =['Pred 0', 'Pred 1']\nlabels_result =['Actual 0', 'Actual 1']\nax.axis('tight')\nax.axis('off')\ncolors = [['g', 'r'],[ 'r', 'g']]\n\nconf_matrix_table = ax.table(   cellText = cm, \n                                colLabels = labels_pred,\n                                rowLabels = labels_result,    \n                                loc = 'center',   \n                                cellColours = colors, \n                                cellLoc = 'center')\n\n\nax.set_title('Confussion Matrix', fontsize = 20)\nconf_matrix_table.auto_set_font_size(False)\nconf_matrix_table.set_fontsize(8)\nconf_matrix_table.set_fontsize(14)\nconf_matrix_table.scale(1.5, 1.5)\n\n# Pred 1 rate\npred_1_rate = cm[1, 1] \/ (cm[0, 1] + cm[1, 1])\n\naccuracy_1_scores[3] = pred_1_rate\naccuracy_total[3] = dtc_model_acc\n\nprint('Actual 1 prediction rate: ', pred_1_rate)","c0917699":"rfc_model = RandomForestClassifier(n_estimators = 10, criterion='entropy', random_state=0) # Random Forest Model\nrfc_model.fit(x_train, y_train) # Training...\npredictions_rfc_model = rfc_model.predict(x_test) # Predictions over test set","f81c1aa8":"# Reports: confussion matrix & accuracy score\ncm = confusion_matrix(y_test, predictions_rfc_model)\nrfc_model_acc = accuracy_score(predictions_rfc_model, y_test)\n\nprint('Accuracy: ', rfc_model_acc)\nprint('\\nREPORT:\\n', classification_report(y_test, predictions_rfc_model))\n\n# Confussion matrix table\nfig, ax =plt.subplots(1, figsize=(10,4))\n\nlabels_pred =['Pred 0', 'Pred 1']\nlabels_result =['Actual 0', 'Actual 1']\nax.axis('tight')\nax.axis('off')\ncolors = [['g', 'r'],[ 'r', 'g']]\n\nconf_matrix_table = ax.table(   cellText = cm, \n                                    colLabels = labels_pred,\n                                    rowLabels = labels_result,  \n                                    loc = 'center',   \n                                    cellColours = colors, \n                                    cellLoc = 'center')\n\n\nax.set_title('Confussion Matrix', fontsize = 20)\nconf_matrix_table.auto_set_font_size(False)\nconf_matrix_table.set_fontsize(8)\nconf_matrix_table.set_fontsize(14)\nconf_matrix_table.scale(1.5, 1.5)\n\n# Pred 1 rate\npred_1_rate = cm[1, 1] \/ (cm[0, 1] + cm[1, 1])\n\naccuracy_1_scores[4] = pred_1_rate\naccuracy_total[4] = rfc_model_acc\n\nprint('Actual 1 prediction rate: ', pred_1_rate)","7e61614e":"# Dataframe of accuracy values\nacc_data = pd.DataFrame({'Model': ['Log Model', 'kNN', 'Naive Bayes', 'Decission Tree', 'Random Forest'], 'Accuracy':accuracy_total, 'Actual 1 Accuracy':accuracy_1_scores})\ndisplay(acc_data)\n\n# Plots\nfig, ax =plt.subplots(2, figsize=(10,10))\n\nsns.barplot(\n    data=acc_data, x='Model', y='Accuracy', \n    estimator=sum, ci=None, \n    ax = ax[0])\n\nax[0].set_title('Accuracy', fontsize = '20')\n\nsns.barplot(\n    data=acc_data, x='Model', y='Actual 1 Accuracy', \n    estimator=sum, ci=None, \n    ax = ax[1])\n\n\nax[1].set_title('Actual 1 Accuracy', fontsize = '20')\n\nplt.tight_layout()\nplt.show()\nplt.close()\n","e42b475e":"final_pred = logmodel.predict(test_final) # using logmodel due it is the mos accurate even in real wordl random forest would be the best one","9868a3cd":"test_final = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')\ntest_final['Response'] = final_pred\ntest_final.drop(test_final.columns.difference(['id','Response']), 1, inplace=True)","df4ea768":"display(test_final)","0244824d":"test_final.to_csv('predictions_car_insurance.csv', index = False)","d7e82f35":"Complete dataset:","cf80d2fb":"Checking NaN values:","1fa3fe52":"## Dataset cleaning and processing","d7ba6f09":"### Continuous variables analysis","de1a6ab0":"A health insurance company wants to offer its old customers a new vehicle insurance. They need our service to create a model that predicts whether a customer would be interested in this new insurance.\n\nThe company has provided us a dataset with former costumers and their responses to the offer. Likewise, they ask us to analyze and return the possible response of another dataset with new costumers based on the created model.\n\nThe datasets are divided into the following variables:\n\n| Variable | Definition |\n| --- | --- |\n| id | Identifier |\n| Gender | Gender (M\/F) |\n| Age | Age |\n| Driving_License | They costumer has driving license (1\/0) |\n| Region_Code | costumer region code |\n| Previously_Insured | costumer already has vehicle insurance (1\/0) |\n| Vehicle_Age | Vehicle age |\n| Vehicle_Damage | Previous damage in the costumer's vehicle (1\/0) |\n| Annual_Premium | Amount to pay for the new car insurance |\n| Policy_Sales_Channel | Communication channel (e-mail, phone, person, etc) |\n| Vintage | costumer number of days with the company |\n| Response | Dependient variable: yes, no (1\/0) |\n\n### The problem will be solved by a classification machine learning model","b283acd7":"### Variables comparison","2912c41d":"#### Result","3249bee3":"We compare some continuous variables to see how the responses are distributed.\n\n#### We compare the following variables:\n\n- Age\n- Region code\n- Polici Sales Channel\n- Response","0531e887":"#### Vintage and Anual_Premium Analysis\n\nIn this last section we analyze the influence of two important values: the costumer's seniority and the price to pay for the insurance.\n\nWe also analyze these variables on the policy sales channels, the vehicle's age, the costumer's age and costumer's region","9a327f72":"### kNN analysis","68974b6a":"#### We'll save models accuracy in these arrays","459f0794":"We receive affirmative responses from both the newest and the oldest costumers.\n\nThe distribution of **annual premium** we could see that it was not uniform. Despite this, we can see a clear affirmative answer around 9000 points. More data would be necessary to give this graph an important value.\n\nWe note that **the insurer does not offer improvements in annual premium with respect to seniority, policy sales channel, region or the most senior costumers**. It would be interesting to offer some kind of offer, at least on older costumers.\n\nIt is interesting to see in the last graph where we find the limit where the costumer decide to pay the annual premium according their vehicle's age. We found a somewhat more permissive decisions on 1-2 year old vehicles. We could **lower the price to customers with newer cars** as their likelihood of needing assistance would be lower.","254a9bcb":"We can consider having previously received damage and the vehicle's age two of the most decisives variables. We are also going to divide these two variables according to costumer's gender.\n#### We compare the following variables:\n\n- Vehicle's age\n- Previous damage to the vehicle\n- Gender\n- Response","79a1782d":"- Less precision than kNN.\n- Much faster than kNN.\n- Higher ratio of pred 1 and therefore less effort on the part of the insurer in contacts with costumers.","966ec492":"In these plots we observe that:\n\n- The age group between 33-57 years are more willing to response yes to our offer\n- Although we have positive answers between 19 and 27 years old, the proportion of affirmative answers is lower than other ages\n- Policy sales channels close to 150 seems to receive a very bad proportion of affirmative responses\n- We did not find any relevant pattern regarding the region of the disguise","89ff7849":"### Model's analysis\n\nVisualizing models accuracy","e0252b9d":"Checking data types","26948707":"### Decision Tree analysis","32616ec5":"#### Model configuration","d6b885ea":"#### Model configuration","6934e975":"#### Result","a8e9178c":"- Gender: we observe almost an uniform distribution.\n- Previously insured: we observe almost an uniform distribution.\n- Vehicle years: uneven distribution. There is not too many data about customers with vehicles older than two years.\n- Driving license: there is almost no data on customers without a license. It would be interesting in order to offer insurance to future drivers.\n- Damage to the vehicle: we observe an uniform distribution.\n- Response: we observe many more negative responses than affirmative ones.","3d4252e6":"Let's see how customers who already have insurance and do not have a driving license behave:\n\n#### We compare the following variables:\n\n- Vehicle's age\n- Driving license\n- Previously insured\n- Response","24185cef":"## Result and conclusions\n\nFinally, we use the model to create a new column *Response* in the dataset provided by the insurer about possible future costumers.","84fbe6d6":"- We get a very good precision result.\n- Most failures are found in false negatives.\n- The drawback of this model is that it offers very few absolute actual 1 well predicted (future costumers). Although the ratio is good (less cost).","5ba48369":"#### Saving dataset","eb4fa179":"# Vehicle Insurance Predictive Analysis\n","64858dea":"### Naive Bayes analysis","b2df0ef5":"#### Importing dataset again to return the results with the input format","5e9b8468":"### Categorical variables analysis","40861a65":"### Standardization\n\nWe standardize continuous values \u200b\u200bso not confuse future analysis. Very large values \u200b\u200bof certain variables can carry much more weight than other variables with smaller values. This may be the case for *Annual_premium* over *Vintage*. Although they may have the same weight on the final decision, in a logistic regression model, the *Annual_premium* values \u200b\u200bwill have greater weight in the final decision if we do not standardize.","bfa7624b":"### Random Forest analysis","54917ab5":"#### Making predictions","85f8b401":"We can see how the best model we have created is the **Random Forest**. This model offers the second highest overall accuracy, but its accuracy on positives is much higher than the others.","5cae1772":"- Faster model but with less precision.\n- It offers many more absolute actual 1 correctly predicted and good ratio between real and predicted positives.","65e26d7b":"- Slight decrease in precision.\n- This model, although it has less precision (more effort) offers more absolute actual 1 well predicted (future costumers).","26e80def":"### Categorical values\n\nCategorical values to bool values","44287054":"#### Model configuration","76709e3f":"## Dataset analysis\n\nDataset structure analysis","dfea64ca":"- Precision practically the same as logistic regression.\n- The precision on the pred 1 is much higher than the other models.","676eae7a":"#### Result","398f1501":"## Predictive analytics: model creation\n\nWe proceed to obtain the best model to predict the response of the costumers.\n\nWe are going to pay special attention to what our costumer asks of us. The insurer wants to contact the costumers that our model predicts as interested costumer. Therefore, apart from measuring the general model's accuracy, **we must measure the accuracy of actual affirmative response well predicted**. This last measure will mean the cost that the company will have to make in contacting potential costumers. We bear in mind that the insurer will not proceed to contact the costumers who appear as *\"not interested\"* in our model.\n\nThe vector machine model is suppressed due to excess computing time.","9cffdd6e":"We can see how customers who have previously received damage to the vehicle are more likely to give a positive response to the insurer's offer.\n\nWe can also see how the affirmative response from customers with newer vehicles is somewhat lower than in the other two cases.","41666f88":"- Age: there is much more data on young customers.\n- Region: uneven distribution. Possibly due to the marketing target.\n- Annual Premium: very uneven distribution. It would be convenient to diversify the price to obtain a more valuable response from customers when analyzing future forecasts.\n- Communication channels: majority use of channels (25-30), (120-130), (150, 165)\n- Vintage: we observe an uniform distribution.","c87be80d":"## Dataset visualization\n\nIn this section we are going to visualize the dataset in order to be able to analyze in a simpler way how costumers behave under the different variable conditions. This section is divided into:\n\n- Categorical variables analysis\n- Continuous variables analysis\n- Variables comparison","4682cc1b":"#### Result","73df5ef7":"#### Model configuration","1c7281d0":"#### Model configuration","b626f961":"#### Result","6ee9c29a":"### Logistic regresion analysis","7b89fb8c":"We can see how having the vehicle previously insured makes customers all respond negatively to insurer's offer.\n\nWe don't have enough data on unlicensed customers to make the data meaningful.","67886cc4":"## Dataset import and split"}}