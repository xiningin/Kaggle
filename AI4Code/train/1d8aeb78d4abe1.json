{"cell_type":{"41a68d5e":"code","55b23bdb":"code","fc1d5f2e":"code","9445b82f":"code","29abfbb0":"code","feae6789":"code","d3cd3200":"code","0675a0b1":"code","3718b766":"code","70abe5f0":"code","2fc5fbc5":"code","dcf59dce":"code","151f6cc5":"code","8723f4d3":"code","b72f17f5":"code","a7e34bb3":"code","44c4f652":"code","bca4fce8":"code","2784fe75":"code","da86690a":"code","c498e41f":"markdown","2c18876a":"markdown","1c12f80e":"markdown","9344e95d":"markdown","9a7e36ee":"markdown","6978645c":"markdown","a8519f7b":"markdown","01428a11":"markdown","67f931e5":"markdown","c8005654":"markdown","7e929f15":"markdown","dee1dc68":"markdown","09769fcf":"markdown","17d0c8bc":"markdown","410cb886":"markdown","988dac82":"markdown","421a16e1":"markdown","4d560bc7":"markdown"},"source":{"41a68d5e":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt # for vizualization\nfrom matplotlib.pyplot import figure # for figuresize","55b23bdb":"voice_data = pd.read_csv(\"..\/input\/voice.csv\")","fc1d5f2e":"voice_data.head()","9445b82f":"voice_data.label = [1 if each == \"male\" else 0 for each in voice_data.label]\nvoice_data.head() # check if 1-0 conversion worked","29abfbb0":"Y_data = voice_data.label.values\nX_data = voice_data.drop([\"label\"], axis = 1)\nX_data = (X_data - X_data.min())\/(X_data.max() - X_data.min()) # normalization","feae6789":"print(voice_data.shape)\nprint(X_data.shape)\nprint(Y_data.shape)","d3cd3200":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X_data, Y_data, test_size = 0.3, random_state = 42)\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)","0675a0b1":"x_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T","3718b766":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","70abe5f0":"def initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    \n    parameters = {\"W1\": np.random.randn(2,x_train.shape[0]) * 0.1,\n                  \"b1\": np.zeros((2,1)),\n                  \"W2\": np.random.randn(2,2) * 0.1,\n                  \"b2\": np.zeros((2,1)),\n                  \"W3\": np.random.randn(1,2) * 0.1,\n                  \"b3\": np.zeros((1,1))}\n    \n    return parameters\n\nparameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)","2fc5fbc5":"print(parameters[\"W1\"].shape)\nprint(parameters[\"W2\"].shape)\nprint(parameters[\"W3\"].shape)","dcf59dce":"print(parameters[\"b1\"].shape)\nprint(parameters[\"b2\"].shape)\nprint(parameters[\"b3\"].shape)","151f6cc5":"def sigmoid(z):\n    return 1\/(1 + np.exp(-z))","8723f4d3":"def forward_propagation_NN(x_train, parameters):\n    \n    Z1 = np.dot(parameters[\"W1\"],x_train) + parameters[\"b1\"]\n    A1 = np.tanh(Z1) # tanh is used as activation function 1\n    Z2 = np.dot(parameters[\"W2\"],A1) + parameters[\"b2\"]\n    A2 = np.tanh(Z2) # tanh is used as activation function 2\n    Z3 = np.dot(parameters[\"W3\"],A2) + parameters[\"b3\"]\n    A3 = sigmoid(Z3)\n\n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"W1\": parameters[\"W1\"],\n             \"Z2\": Z2,\n             \"A2\": A2,\n             \"W2\": parameters[\"W2\"],\n             \"Z3\": Z3,\n             \"A3\": A3,\n             \"W3\": parameters[\"W3\"]}\n    \n    return A3, cache\n\nA3, cache = forward_propagation_NN(x_train, parameters)","b72f17f5":"def compute_cost_NN(A3, Y, parameters):\n    \n    logprobs = np.multiply(np.log(A3),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    \n    return cost\n\ncost = compute_cost_NN(A3, y_train, parameters)","a7e34bb3":"def backward_propagation_NN(parameters, cache, X, Y):\n    \n    dimension = X.shape[0] # it is 20 for our case\n    dZ3 = cache[\"A3\"] - Y # d(cost)\/d(Z3)\n    dW3 = 1\/dimension * np.dot(dZ3,cache[\"A2\"].T) # d(cost)\/d(W3)\n    db3 = 1\/dimension * np.sum(dZ3, axis=1, keepdims=True) # d(cost)\/d(b3)\n    dZ2 = np.multiply(np.dot(dZ3.T, cache[\"W3\"]).T , 1-np.power(cache[\"A2\"],2)) # d(cost)\/d(Z2)\n    dW2 = 1\/dimension * np.dot(cache[\"A1\"], dZ2.T) # d(cost)\/d(W2)\n    db2 = 1\/dimension * np.sum(dZ2, axis=1, keepdims=True) # d(cost)\/d(b2)\n    dZ1 = np.multiply(np.dot(dZ2.T, cache[\"W2\"].T).T,1-np.power(cache[\"A1\"],2)) # d(cost)\/d(Z1)\n    dW1 = 1\/dimension * np.dot(dZ1, X.T) # d(cost)\/d(W1)\n    db1 = 1\/dimension * np.sum(dZ1,axis=1, keepdims=True) # d(cost)\/d(b1)\n    grads = {'dW3':dW3, \n             'db3':db3,\n             'dW2':dW2,\n             'db2':db2,\n             'dW1':dW1,\n             'db1':db1}\n    \n    return grads\n\ngrads = backward_propagation_NN(parameters, cache, x_train, y_train)","44c4f652":"Learning_Rate = 0.001","bca4fce8":"def update_parameters_NN(parameters, grads, learning_rate = Learning_Rate):\n    parameters = {\"W1\": parameters[\"W1\"]-learning_rate*grads[\"dW1\"],\n                  \"b1\": parameters[\"b1\"]-learning_rate*grads[\"db1\"],\n                  \"W2\": parameters[\"W2\"]-learning_rate*grads[\"dW2\"],\n                  \"b2\": parameters[\"b2\"]-learning_rate*grads[\"db2\"],\n                  \"W3\": parameters[\"W3\"]-learning_rate*grads[\"dW3\"],\n                  \"b3\": parameters[\"b3\"]-learning_rate*grads[\"db3\"]}\n    \n    return parameters\n\nparameters = update_parameters_NN(parameters, grads, learning_rate = Learning_Rate)","2784fe75":"def predict_NN(parameters,x_test):\n    # x_test is the input for forward propagation\n    A3, cache = forward_propagation_NN(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n\n    for i in range(A3.shape[1]):\n        if A3[0,i]<= 0.5: # if smaller than 0.5, predict it as 0\n            Y_prediction[0,i] = 0\n        else: # if greater than 0.5, predict it as 1\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n\nY_prediction = predict_NN(parameters,x_test)","da86690a":"def three_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations):\n    cost_list = []\n    index_list = []\n    \n    #initialize parameters and layer sizes\n    parameters = initialize_parameters_and_layer_sizes_NN(x_train, y_train)\n\n    for i in range(0, num_iterations):\n        # forward propagation\n        A3, cache = forward_propagation_NN(x_train,parameters)\n        # compute cost\n        cost = compute_cost_NN(A3, y_train, parameters)\n        # backward propagation\n        grads = backward_propagation_NN(parameters, cache, x_train, y_train)\n        # update parameters\n        parameters = update_parameters_NN(parameters, grads)\n        \n        if i % 100 == 0: # to visualize data in each 100 iteration\n            cost_list.append(cost)\n            index_list.append(i)\n\n    figure(num=None, figsize=(8, 6), dpi=80, facecolor='w', edgecolor='k')\n    plt.plot(index_list,cost_list)\n    plt.xticks(index_list,rotation='vertical')\n    plt.xlabel(\"Number of Iterarions\", fontsize = 14)\n    plt.ylabel(\"Cost\", fontsize = 14)\n    plt.show()\n    \n    # predict\n    y_prediction_test = predict_NN(parameters,x_test)\n    y_prediction_train = predict_NN(parameters,x_train)\n\n    # Print train\/test Accuracies\n    print(\"train accuracy: %{}\".format(round(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100,3)))\n    print(\"test accuracy: %{}\".format(round(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100,3)))\n    return parameters\n\nparameters = three_layer_neural_network(x_train, y_train,x_test,y_test, num_iterations=5000)","c498e41f":"## Loss function and Cost function\n* Loss and cost functions are same with logistic regression\n* Cross entropy function\n<a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/image.ibb.co\/nyR9LU\/as.jpg\" alt=\"as\" border=\"0\"><\/a><br \/>","2c18876a":"* For more detailed explanations, you can check the following link: https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners.\n* In this link, Artificial Neural Network (ANN) is explained in very detail and a 2-Layer ANN model is coded explicitly.\n* In my kernel, I implemented a 3-layer ANN model explicitly.","1c12f80e":"## Compute Cost","9344e95d":"## Extract gender (Y_data) and feature (X_data) information","9a7e36ee":"## Forward Propagation","6978645c":"## Create 3-layer Neural Network Model","a8519f7b":"## Set the learning rate","01428a11":"## Vizualization of voice_data, X_data and Y_data\n<a href=\"https:\/\/ibb.co\/ePbiaA\"><img src=\"https:\/\/preview.ibb.co\/gwBGvA\/im1.png\" alt=\"im1\" border=\"0\"><\/a>","67f931e5":"## Predict for the test data with updated weight and bias (with updated parameters)","c8005654":"## Split data for train and test purposes","7e929f15":"## Import data","dee1dc68":"## Create weight and bias for 3-layer neural network ( 2 hidden layers )","09769fcf":"## Sigmoid function :  sigmoid(x) = 1 \/ ( 1 + exp(-x) )","17d0c8bc":"## Backward Propagation with Gradient Decent","410cb886":"## Update weight and bias","988dac82":"## Convert male to 1, female to 0","421a16e1":"## Preview data","4d560bc7":"## 3-Layer Artificial Neural Network (ANN) Construction\n<a href=\"https:\/\/ibb.co\/dGQUyV\"><img src=\"https:\/\/preview.ibb.co\/eRs9yV\/im2.png\" alt=\"im2\" border=\"0\"><\/a>"}}