{"cell_type":{"a0794ece":"code","37f60612":"code","aefadd6b":"code","f286fed1":"code","a4a73007":"code","0caced54":"code","763f57de":"code","6d397e39":"code","fd82f081":"code","fc350347":"code","c1e59ce9":"code","45f9ee96":"code","86f4c58e":"code","fd99d55a":"code","22f383b9":"markdown","92fc462c":"markdown","888c62f7":"markdown","962feb09":"markdown","e6337bf4":"markdown","c336d069":"markdown","94a744d6":"markdown","3093f879":"markdown"},"source":{"a0794ece":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom matplotlib import pyplot as plt # show graph\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.utils import resample\nfrom imblearn.over_sampling import RandomOverSampler,SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTETomek\n\nimport xgboost as xgb\n\nfrom scipy.stats import ks_2samp\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\npd.set_option('display.max_columns', None)\n# pd.set_option('display.max_rows', None)","37f60612":"# Read data\ninputDF = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ninputDF.head(5)","aefadd6b":"inputDF.shape","f286fed1":"# Remove any duplications\ninputDF = inputDF.drop_duplicates(subset=None)\ninputDF.shape","a4a73007":"inputDF.describe()","0caced54":"# Graph distribution\ninputDF.hist(bins=50, figsize=(20,15), color = 'deepskyblue')\nplt.show()","763f57de":"len(inputDF[inputDF['Class']==0]), len(inputDF[inputDF['Class']==1])","6d397e39":"Class 1 only covers 0.2% of the data. The dataset is very imbalance.","fd82f081":"#correlation matrix \nf, (ax1, ax2, ax3) = plt.subplots(1,3,figsize =( 30, 16))\n\nsns.heatmap(inputDF.query('Class==1').drop(['Class'],1).corr(), vmax = .8, square=True, ax = ax1)\nax1.set_title('Fraud')\n\nsns.heatmap(inputDF.query('Class==0').drop(['Class'],1).corr(), vmax = .8, square=True, ax = ax2);\nax2.set_title('Normal')\n\nsns.heatmap(inputDF.corr(), vmax = .8, square=True, ax = ax3);\nax3.set_title('All')\n\nplt.show()","fc350347":"y = inputDF.Class\nX = inputDF.drop('Class', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1, stratify=y, shuffle=True)\nX_train.shape, X_test.shape","c1e59ce9":"def checkTest(clf, X_test, y_test):\n    y_pred = clf.predict(X_test)\n    reportTest(y_pred, y_test)\n    \ndef reportTest(y_pred, y_test):\n    print(\"Classification report:\")\n    print(classification_report(y_test, y_pred))\n    print(\"Confusion matrix:\")\n    print(confusion_matrix(y_test, y_pred))\n\n    print(\"The accuracy is {}\".format(accuracy_score(y_test, y_pred))) \n    print(\"The precision is {}\".format(precision_score(y_test, y_pred))) \n    print(\"The recall is {}\".format(recall_score(y_test, y_pred))) \n    print(\"The F1-Score is {}\".format(f1_score(y_test, y_pred))) \n    print(\"The AUC is {} \".format(roc_auc_score(y_test, y_pred)))","45f9ee96":"rfc = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\nxgbc = xgb.XGBClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n\nfor clf, clf_name in [(rfc, 'rf'), (xgbc, 'xgb')]:\n    print(\"Training for \" + clf_name)\n    clf.fit(X_train, y_train)\n    checkTest(clf, X_test, y_test)\n    print(\"-------------\\n\")","86f4c58e":"# sampler = RandomOverSampler()\n# sampler = RandomUnderSampler()\n# sampler = SMOTETomek(sampling_strategy='auto', n_jobs=-1, random_state=42)\nsampler = SMOTE(sampling_strategy='minority', n_jobs=-1, random_state=42)\nX_train_resampled, y_train_resampled = sampler.fit_sample(X_train, y_train)\n\nrfc_SMOTE = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\nxgbc_SMOTE = xgb.XGBClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n\nfor clf, clf_name in [(rfc_SMOTE, 'rf_SMOTE'), (xgbc_SMOTE, 'xgb_SMOTE')]:\n    print(\"Training for \" + clf_name)\n    clf.fit(X_train_resampled, y_train_resampled)\n    checkTest(clf, X_test, y_test)\n    print(\"-------------\\n\")","fd99d55a":"# reinitialize rfc & xgbc\nrfc = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\nxgbc = xgb.XGBClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n\nestimators = [('rfc', rfc), ('xgb', xgbc)]\n\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\nprint(\"Training for StackingClassifier\")\nclf.fit(X_train, y_train)\ncheckTest(clf, X_test, y_test)\nprint(\"-------------\\n\")","22f383b9":"# Bagging & Boosting\n\nFirst, let's try with 2 different solution:\n- Random Forest which is a bagging of Decision Trees.\n- XGBoost is a Boosting of Decision Trees.","92fc462c":"# SMOTE\n\nThese low values of Recall & F1-Score probably come from the imbalance nature of the data. Let's try a technique called SMOTE (Synthetic Minority Oversampling Technique) which synthesizes more datat points for minority dataset and makes a new balance dataset. ","888c62f7":"The REMOTE generated data helps to slightly improve recall values (from 0.75 to 0.788); however, the precision value drops to only 0.84 which makes the F1-Score values also decrease.\n\n","962feb09":"# Preprocess","e6337bf4":"In this particular imbalance dataset of fauld credit cards, precision value is quite high (0.9468 for XGBoost & 0.9263 for Random Forest); recall, however, is quite small (only around 0.75). Because of low recall values, F1-score is also small (0.8396 for XGBoost & 0.8263 for Random Forest).","c336d069":"# Stacking\nLet's try to stack both Random Forest & XGBoost into 1 ensemble model to see whether they can improve overall solution.","94a744d6":"Let's check how imbalance our data is","3093f879":"The F1-Score slightly decreases if comparing to Random Forest. Probably, the reason is that Stacking splits training dataset again to have some data to train Logistic."}}