{"cell_type":{"c3e77921":"code","2495bd08":"code","9f5aec84":"code","1dbdf7d1":"code","0ba21a43":"code","f2b3b7d5":"code","6a27a490":"code","9c637c01":"code","6a29cf51":"code","43029c58":"code","6e1c1ada":"code","05149d6e":"code","7ee87cf8":"code","3e9f1808":"code","244214d5":"code","3ee5f4ed":"code","aa72ba09":"code","152f5396":"code","407c1a7a":"code","918c2bbc":"code","44c5e6ae":"code","14bdbf81":"code","127326e4":"code","ba2028dc":"code","9c7266fd":"code","2de63559":"code","01178acd":"code","b24d6a7a":"markdown","d5d0bc5b":"markdown","f9badf77":"markdown","cb281118":"markdown","78974fbe":"markdown","7ea9e71b":"markdown","474ac844":"markdown","ab113680":"markdown","041f9b07":"markdown","81be2021":"markdown","11d4810a":"markdown","b85bbb6f":"markdown"},"source":{"c3e77921":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","2495bd08":"df=pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\")\ndf.isna().sum()\ndf.head()","9f5aec84":"df.sex[df.target==1].value_counts().plot(kind='bar',figsize=(10,6),color=['green','blue'])\nplt.title(\"Count of the number of males and females with heart disease\")\n","1dbdf7d1":"pd.crosstab(df.target,df.sex)","0ba21a43":"pd.crosstab(df.target,df.sex).plot(kind='bar',figsize=(10,6),color=[\"lightblue\",\"pink\"])\nplt.title(\"Frequency of Heart Disease vs Sex\")\nplt.xlabel(\"0= Heart Disease, 1= No disease\")\nplt.ylabel(\"Number of people with heart disease\")\nplt.legend([\"Female\",\"Male\"])\nplt.xticks(rotation=0);","f2b3b7d5":"cor_mat=df.corr()\nfig,ax=plt.subplots(figsize=(15,10))\nsns.heatmap(cor_mat,annot=True,linewidths=0.5,fmt=\".3f\")\n","6a27a490":"from sklearn.preprocessing import MinMaxScaler\nscal=MinMaxScaler()\nfeat=['age', \t'sex', \t'cp', 'trestbps', 'chol', \t'fbs', \t'restecg', \t'thalach' ,\t'exang', \t'oldpeak' ,\t'slope', \t'ca', 'thal']\ndf[feat] = scal.fit_transform(df[feat])\ndf.head()","9c637c01":"X=df.drop(\"target\",axis=1)\nY=df.target","6a29cf51":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,random_state=0,test_size=0.2)","43029c58":"from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score,confusion_matrix,roc_auc_score\n\ndef evaluation(Y_test,Y_pred):\n  acc=accuracy_score(Y_test,Y_pred)\n  rcl=recall_score(Y_test,Y_pred)\n  f1=f1_score(Y_test,Y_pred)\n  auc_score=roc_auc_score(Y_test,Y_pred)\n  prec_score=precision_score(Y_test,Y_pred)\n \n\n  metric_dict={'accuracy': round(acc,3),\n               'recall': round(rcl,3),\n               'F1 score': round(f1,3),\n               'auc score': round(auc_score,3),\n               'precision': round(prec_score,3)\n               \n              }\n\n  return print(metric_dict)\n","6e1c1ada":"np.random.seed(42)\nfrom sklearn.neighbors import KNeighborsClassifier\nKnn_clf=  KNeighborsClassifier()\nKnn_clf.fit(X_train,Y_train)\nKnn_Y_pred=Knn_clf.predict(X_test)\nKnn_score=Knn_clf.score(X_test,Y_test)\n#print(Knn_score)\nevaluation(Y_test,Knn_Y_pred)","05149d6e":"np.random.seed(42)\nfrom sklearn.linear_model import LogisticRegression\nLR_clf=LogisticRegression()\nLR_clf.fit(X_train,Y_train)\nLR_Y_pred=LR_clf.predict(X_test)\nLR_score=LR_clf.score(X_test,Y_test)\n#print(LR_score)\nevaluation(Y_test,LR_Y_pred)","7ee87cf8":"np.random.seed(42)\nfrom sklearn.ensemble import RandomForestClassifier\nRF_clf=RandomForestClassifier(n_estimators=450)\nRF_clf.fit(X_train,Y_train)\nRF_score=RF_clf.score(X_test,Y_test)\nRF_Y_pred=RF_clf.predict(X_test)\n#print(RF_score)\nevaluation(Y_test,RF_Y_pred)","3e9f1808":"np.random.seed(42)\nfrom sklearn.svm import SVC\nSVC_clf=SVC()\nSVC_clf.fit(X_train,Y_train)\nSVC_score=SVC_clf.score(X_test,Y_test)\nSVC_Y_pred=SVC_clf.predict(X_test)\n#print(SVC_score)\nevaluation(Y_test,SVC_Y_pred)","244214d5":"from xgboost import XGBClassifier\nXGB_clf=XGBClassifier()\nXGB_clf.fit(X_train,Y_train)\nXGB_score=XGB_clf.score(X_test,Y_test)\nXGB_Y_pred=XGB_clf.predict(X_test)\n#print(SVC_score)\nevaluation(Y_test,XGB_Y_pred)","3ee5f4ed":"model_comp = pd.DataFrame({'Model': ['Logistic Regression','Random Forest',\n                    'K-Nearest Neighbour','Support Vector Machine',\"XGBoost\"], 'Accuracy': [LR_score*100,\n                    RF_score*100,Knn_score*100,SVC_score*100,XGB_score*100]})\nmodel_comp","aa72ba09":"neighbors = range(1, 21) # 1 to 20\n\n# Setup algorithm\nknn = KNeighborsClassifier()\n\n# Loop through different neighbors values\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n    \n    # Fit the algorithm\n    print(f\"Accuracy with {i} no. of neighbors: {knn.fit(X_train, Y_train).score(X_test,Y_test)}%\")","152f5396":"np.random.seed(42)\nfrom sklearn.neighbors import KNeighborsClassifier\nKnn_clf=  KNeighborsClassifier(n_neighbors=7)\nKnn_clf.fit(X_train,Y_train)\nKnn_Y_pred=Knn_clf.predict(X_test)\nKnn_score=Knn_clf.score(X_test,Y_test)\nevaluation(Y_test,Knn_Y_pred)","407c1a7a":"from sklearn.ensemble import RandomForestClassifier\nnp.random.seed(42)\nfor i in range(1,40,1):\n  print(f\"With {i*10} estimators:\")\n  clf2=RandomForestClassifier(n_estimators=i*10,max_depth=i,random_state=i).fit(X_train,Y_train)\n  print(f\"Accuracy: {clf2.score(X_test,Y_test)*100:2f}%\")","918c2bbc":"from sklearn.ensemble import RandomForestClassifier\nRF_clf2=RandomForestClassifier(n_estimators=30,max_depth=3,random_state=3)\nRF_clf2.fit(X_train,Y_train)\nRF2_acc_score=RF_clf2.score(X_test,Y_test)\nRF2_Y_pred=RF_clf2.predict(X_test)\nprint(RF2_acc_score)\nevaluation(Y_test,RF2_Y_pred)","44c5e6ae":"xgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\n\nxgb.fit(X_train,Y_train)\nxgb_score=XGB_clf.score(X_test,Y_test)\nxgb_Y_pred=XGB_clf.predict(X_test)\n#print(SVC_score)\nevaluation(Y_test,xgb_Y_pred)","14bdbf81":"\nfrom sklearn.model_selection import GridSearchCV \n  \n# defining parameter range \nparam_grid = {'C': [0.1, 1,2, 10, 100, 1000],  \n              'gamma': [1, 0.1, 0.01, 0.001, 0.0001], \n              'kernel': ['rbf','linear']}  \n  \ngs_clf = GridSearchCV(SVC(), param_grid,cv=5, refit = True, verbose = 3) \n  \n# fitting the model for grid search \ngs_clf.fit(X_train, Y_train)\n\nprint(gs_clf.best_params_)\n\nprint(f\"Accuracy score:{gs_clf.score(X_test,Y_test)}%\")\n\n","127326e4":"knn_grid={'n_neighbors': np.arange(1,20,1),\n          'leaf_size': np.arange(1,30,1)}\n\ngs_knn=GridSearchCV(KNeighborsClassifier(),param_grid=knn_grid,cv=5,verbose=True)\n\ngs_knn.fit(X_train, Y_train)\n\ngs_knn.best_params_\n\nprint(f\"Accuracy score:{gs_knn.score(X_test,Y_test)*100}%\")","ba2028dc":"model_comp = pd.DataFrame({'Model': ['Logistic Regression','Random Forest',\n                    'K-Nearest Neighbour','Support Vector Machine','Extreme Gradient Boost'], 'Accuracy': [LR_score*100,\n                    RF2_acc_score*100,Knn_score*100,SVC_score*100, XGB_score*100]})\nmodel_comp","9c7266fd":"print(\" Best evaluation parameters achieved with KNN:\") \nevaluation(Y_test,Knn_Y_pred)\n\n","2de63559":"KNN_final_metrics={'Accuracy': Knn_clf.score(X_test,Y_test),\n                   'Precision': precision_score(Y_test,Knn_Y_pred),\n                   'Recall': recall_score(Y_test,Knn_Y_pred),\n                   'F1': f1_score(Y_test,Knn_Y_pred),\n                   'AUC': roc_auc_score(Y_test,Knn_Y_pred)}\n\nKNN_metrics=pd.DataFrame(KNN_final_metrics,index=[0])\n\nKNN_metrics.T.plot.bar(title='KNN metric evaluation',legend=False);","01178acd":"from sklearn.metrics import confusion_matrix\n\nfig,ax=plt.subplots()\nax=sns.heatmap(confusion_matrix(Y_test,Knn_Y_pred),annot=True,cbar=True);","b24d6a7a":"## **Splitting the data into train and test sets**","d5d0bc5b":"## **Creating Features and Target variable**","f9badf77":" ### **Count of the number of males and females who have heart disease**","cb281118":"## **Create a function for evaluating metrics**","78974fbe":"# **Hyper parameter tuning KNN using GridSearchCV**","7ea9e71b":"# **Tuning XGBoost manually**","474ac844":"## **Building a Correlation Matrix**","ab113680":"# **Tuning Random Forest**","041f9b07":"## **Tuning KNN**","81be2021":"# **Hyper parameter tuning  SVC using GridSearchCV**","11d4810a":"# **Looking at the evaluation metrics for our best model**","b85bbb6f":"## **Fitting and Comparing different Models**"}}