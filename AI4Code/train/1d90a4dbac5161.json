{"cell_type":{"05a6d349":"code","522bdebc":"code","a52e2c61":"code","91444f02":"code","f3840807":"code","86fa9f3e":"code","b0f99ccd":"code","99d9c284":"code","655d331b":"code","c5135afa":"code","7d9ceb50":"code","1e9adc11":"code","05bccb5e":"code","04f1f3a5":"code","1716d58e":"code","fe5bd62d":"code","f846e643":"code","67562af0":"code","752df40d":"code","90f10262":"code","215af21b":"code","5e627a5a":"markdown","e40d3f01":"markdown","8a279efa":"markdown","20cfec4c":"markdown"},"source":{"05a6d349":"import tensorflow as tf\nfrom tensorflow.keras import layers \nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_addons as tfa\nfrom tensorflow.keras import datasets, layers, optimizers, Sequential, metrics\nfrom tensorflow.python.framework.convert_to_constants import convert_variables_to_constants_v2\nimport os\nimport pathlib\nimport random\nimport nibabel as nib\nimport imageio\n","522bdebc":"BUFFER_SIZE = 5000\nBATCH_SIZE = 1","a52e2c61":"def preprocess_image(image):\n    image= tf.image.decode_png(image, channels=1)\n    image = tf.image.resize(image, [256, 256])\n    image = (image-127.5)\/127.5\n    return image\n\ndef load_and_preprocess_image(path):\n    image = tf.io.read_file(path)\n    return preprocess_image(image)\n\ndef preprocess_image_T2(image):\n    image = tf.image.decode_png(image, channels=1)\n    image = tf.image.resize(image, [256, 256])\n    image = (image-127.5)\/127.5 \n    image = tf.transpose(image, [ 1, 0, 2])\n    image = tf.image.flip_up_down(image)\n    return image\n\ndef load_and_preprocess_image_T2(path):\n    image = tf.io.read_file(path)\n    return preprocess_image_T2(image)","91444f02":"data_root = pathlib.Path('..\/input\/ixi-t1\/image slice-T1')\nall_image_paths_T1 = list(data_root.glob('*\/*'))\nall_image_paths_T1.sort()\nall_image_paths_T1 = [str(path) for path in all_image_paths_T1[:20000]]\n#random.shuffle(all_image_paths)\nimage_count = len(all_image_paths_T1)\nds_T1 = tf.data.Dataset.from_tensor_slices((all_image_paths_T1))\ndataset_T1 = ds_T1.map(load_and_preprocess_image)","f3840807":"data_root = pathlib.Path('..\/input\/ixit2-slices\/image slice-T2')\nall_image_paths_T2 = list(data_root.glob('*\/*'))\nall_image_paths_T2.sort()\nall_image_paths_T2 = [str(path) for path in all_image_paths_T2[:20000]]\n#random.shuffle(all_image_paths)\nimage_count = len(all_image_paths_T2)\nds_T2 = tf.data.Dataset.from_tensor_slices((all_image_paths_T2))\ndataset_T2 = ds_T2.map(load_and_preprocess_image_T2)\ndataset = tf.data.Dataset.zip((dataset_T1, dataset_T2)) .shuffle(BUFFER_SIZE).batch(BATCH_SIZE)","86fa9f3e":"def mkdir(path):\n    import os\n    path=path.strip()\n    path=path.rstrip(\"\\\\\")\n    isExists=os.path.exists(path)\n    if not isExists:\n        \n        os.makedirs(path)\n        print (' create success')\n\n        return True\n\n    else:\n        print(' path has existed')\n        return False","b0f99ccd":"OUTPUT_CHANNELS = 1\n\ndef downsample(filters, size, apply_instancenorm=True):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n                             kernel_initializer=initializer, use_bias=False))\n\n    if apply_instancenorm:\n        result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    result.add(layers.LeakyReLU())\n\n    return result","99d9c284":"def upsample(filters, size, apply_dropout=False):\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    result = keras.Sequential()\n    result.add(layers.Conv2DTranspose(filters, size, strides=2,\n                                      padding='same',\n                                      kernel_initializer=initializer,\n                                      use_bias=False))\n\n    result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))\n\n    if apply_dropout:\n        result.add(layers.Dropout(0.5))\n\n    result.add(layers.ReLU())\n\n    return result","655d331b":"def Generator():\n    inputs = layers.Input(shape=[256,256,1])\n\n    # bs = batch size\n    down_stack = [\n        downsample(64, 4, apply_instancenorm=False), # (bs, 128, 128, 64)\n        downsample(128, 4), # (bs, 64, 64, 128)\n        downsample(256, 4), # (bs, 32, 32, 256)\n        downsample(512, 4), # (bs, 16, 16, 512)\n        downsample(512, 4), # (bs, 8, 8, 512)\n        downsample(512, 4), # (bs, 4, 4, 512)\n        downsample(512, 4), # (bs, 2, 2, 512)\n        downsample(512, 4), # (bs, 1, 1, 512)\n    ]\n\n    up_stack = [\n        upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n        upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n        upsample(512, 4), # (bs, 16, 16, 1024)\n        upsample(256, 4), # (bs, 32, 32, 512)\n        upsample(128, 4), # (bs, 64, 64, 256)\n        upsample(64, 4), # (bs, 128, 128, 128)\n    ]\n\n    initializer = tf.random_normal_initializer(0., 0.02)\n    last = layers.Conv2DTranspose(OUTPUT_CHANNELS, 4,\n                                  strides=2,\n                                  padding='same',\n                                  kernel_initializer=initializer,\n                                  activation='tanh') # (bs, 256, 256, 3)\n\n    x = inputs\n\n    # Downsampling through the model\n    skips = []\n    for down in down_stack:\n        x = down(x)\n        skips.append(x)\n\n    skips = reversed(skips[:-1])\n\n    # Upsampling and establishing the skip connections\n    for up, skip in zip(up_stack, skips):\n        x = up(x)\n        x = layers.Concatenate()([x, skip])\n\n    x = last(x)\n\n    return keras.Model(inputs=inputs, outputs=x)","c5135afa":"def Discriminator():\n    initializer = tf.random_normal_initializer(0., 0.02)\n    gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n\n    inp = layers.Input(shape=[256, 256, 1], name='input_image')\n\n    x = inp\n\n    down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n    down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n    down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n\n    zero_pad1 = layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n    conv = layers.Conv2D(512, 4, strides=1,\n                         kernel_initializer=initializer,\n                         use_bias=False)(zero_pad1) # (bs, 31, 31, 512)\n\n    norm1 = tfa.layers.InstanceNormalization(gamma_initializer=gamma_init)(conv)\n\n    leaky_relu = layers.LeakyReLU()(norm1)\n\n    zero_pad2 = layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n\n    last = layers.Conv2D(1, 4, strides=1,\n                         kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n\n    return tf.keras.Model(inputs=inp, outputs=last)","7d9ceb50":"\nT1_generator = Generator() # transforms T2 to T1\nT2_generator = Generator() # transforms T1 paintings to be T2\n\nT1_discriminator = Discriminator() # differentiates real T1 and generated T1\nT2_discriminator = Discriminator() # differentiates real T2 and generated T2","1e9adc11":"class CycleGan(keras.Model):\n    def __init__(\n        self,\n        T1_generator,\n        T2_generator,\n        T1_discriminator,\n        T2_discriminator,\n        lambda_cycle=10,\n    ):\n        super(CycleGan, self).__init__()\n        self.T1_gen = T1_generator\n        self.T2_gen = T2_generator\n        self.T1_disc = T1_discriminator\n        self.T2_disc = T2_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        T1_gen_optimizer,\n        T2_gen_optimizer,\n        T1_disc_optimizer,\n        T2_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn\n    ):\n        super(CycleGan, self).compile()\n        self.T1_gen_optimizer = T1_gen_optimizer\n        self.T2_gen_optimizer = T2_gen_optimizer\n        self.T1_disc_optimizer = T1_disc_optimizer\n        self.T2_disc_optimizer = T2_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        real_T1, real_T2 = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # photo to monet back to photo\n            fake_T1 = self.T1_gen(real_T2, training=True)\n            cycled_T2 = self.T2_gen(fake_T1, training=True)\n\n            # monet to photo back to monet\n            fake_T2 = self.T2_gen(real_T1, training=True)\n            cycled_T1 = self.T1_gen(fake_T2, training=True)\n\n            # generating itself\n            same_T1 = self.T1_gen(real_T1, training=True)\n            same_T2 = self.T2_gen(real_T2, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_T1 = self.T1_disc(real_T1, training=True)\n            disc_real_T2 = self.T2_disc(real_T2, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_T1 = self.T1_disc(fake_T1, training=True)\n            disc_fake_T2 = self.T2_disc(fake_T2, training=True)\n\n            # evaluates generator loss\n            T1_gen_loss = self.gen_loss_fn(disc_fake_T1)\n            T2_gen_loss = self.gen_loss_fn(disc_fake_T2)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_T1, cycled_T1, self.lambda_cycle) + self.cycle_loss_fn(real_T2, cycled_T2, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_T1_gen_loss = T1_gen_loss + total_cycle_loss + self.identity_loss_fn(real_T1, same_T1, self.lambda_cycle)\n            total_T2_gen_loss = T2_gen_loss + total_cycle_loss + self.identity_loss_fn(real_T2, same_T2, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            T1_disc_loss = self.disc_loss_fn(disc_real_T1, disc_fake_T1)\n            T2_disc_loss = self.disc_loss_fn(disc_real_T2, disc_fake_T2)\n\n        # Calculate the gradients for generator and discriminator\n        T1_generator_gradients = tape.gradient(total_T1_gen_loss,\n                                                  self.T1_gen.trainable_variables)\n        T2_generator_gradients = tape.gradient(total_T2_gen_loss,\n                                                  self.T2_gen.trainable_variables)\n\n        T1_discriminator_gradients = tape.gradient(T1_disc_loss,\n                                                      self.T1_disc.trainable_variables)\n        T2_discriminator_gradients = tape.gradient(T2_disc_loss,\n                                                      self.T2_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.T1_gen_optimizer.apply_gradients(zip(T1_generator_gradients,\n                                                 self.T1_gen.trainable_variables))\n\n        self.T2_gen_optimizer.apply_gradients(zip(T2_generator_gradients,\n                                                 self.T2_gen.trainable_variables))\n\n        self.T1_disc_optimizer.apply_gradients(zip(T1_discriminator_gradients,\n                                                  self.T1_disc.trainable_variables))\n\n        self.T2_disc_optimizer.apply_gradients(zip(T2_discriminator_gradients,\n                                                  self.T2_disc.trainable_variables))\n        \n        return {\n            \"T1_gen_loss\": total_T1_gen_loss,\n            \"T2_gen_loss\": total_T2_gen_loss,\n            \"T1_disc_loss\": T1_disc_loss,\n            \"T2_disc_loss\": T2_disc_loss\n        }","05bccb5e":"def discriminator_loss(real, generated):\n     real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n\n     generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n\n     total_disc_loss = real_loss + generated_loss\n\n     return total_disc_loss * 0.5","04f1f3a5":" def generator_loss(generated):\n    return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)","1716d58e":" def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n\n    return LAMBDA * loss1","fe5bd62d":"def identity_loss(real_image, same_image, LAMBDA):\n    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n    return LAMBDA * 0.5 * loss","f846e643":"T1_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\nT2_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\nT1_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\nT2_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","67562af0":" cycle_gan_model = CycleGan(\n    T1_generator, T2_generator, T1_discriminator, T2_discriminator\n    )\n\ncycle_gan_model.compile(\n    T1_gen_optimizer = T1_generator_optimizer,\n    T2_gen_optimizer = T2_generator_optimizer,\n    T1_disc_optimizer = T1_discriminator_optimizer,\n    T2_disc_optimizer = T2_discriminator_optimizer,\n    gen_loss_fn = generator_loss,\n    disc_loss_fn = discriminator_loss,\n    cycle_loss_fn = calc_cycle_loss,\n    identity_loss_fn = identity_loss\n    )","752df40d":"cycle_gan_model.fit(\n    dataset,\n    epochs=2\n)       ","90f10262":"_, ax = plt.subplots(5, 2, figsize=(12, 12)) \nfor i, img in enumerate(dataset.take(5)):            \n    image_A, image_B = img\n    prediction = T1_generator(image_B, training=False)[0].numpy()\n    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)\n    image_A = (image_A[0] * 127.5 + 127.5).numpy().astype(np.uint8)\n\n    ax[i, 0].imshow(image_A, cmap = 'gray')\n    ax[i, 1].imshow(prediction, cmap = 'gray')\n    ax[i, 0].set_title(\"Input T2\")\n    ax[i, 1].set_title(\"T1-esque\")\n    ax[i, 0].axis(\"off\")\n    ax[i, 1].axis(\"off\")\nplt.show()","215af21b":"path = '\/kaggle\/working\/model'\nT1_generator.save('\/kaggle\/working\/model\/T1_generator')\nT2_generator.save('\/kaggle\/working\/model\/T2_generator')\nT1_discriminator.save('\/kaggle\/working\/model\/T1_discriminator')\nT2_discriminator.save('\/kaggle\/working\/model\/T2_discriminator')","5e627a5a":"to_T2 = T2_generator(image_T1)\n\nplt.subplot(1, 2, 1)\nplt.title(\"Original T1\")\n#image_T1 = image_T1 * 127.5 + 127.5\n#image_T1 = tf.cast(image_T1, dtype=tf.int32)\nplt.imshow(image_T1[0])\n\nplt.subplot(1, 2, 2)\nplt.title(\"generated T2\")\nplt.imshow(to_T2[0] * 127.5 + 127.5)\nplt.show()","e40d3f01":"iteration = iter(dataset)\niteration_test = iter(dataset)\nimage_T1 , image_T2 = next(iteration)\nimage_T1 = image_T1  * 127.5 + 127.5\nimage_T1 = tf.cast(image_A ,dtype=tf.int32)\nimage_T2 = image_T2  * 127.5 + 127.5\nimage_T2 = tf.cast(image_B ,dtype=tf.int32)\n\nplt.subplot(1,2,1)\nplt.imshow(image_T1[0], cmap='gray')\nplt.axis('off')\nplt.subplot(1,2,2)\nplt.imshow(image_T2[0], cmap='gray')\nplt.axis('off')\nplt.show()","8a279efa":"# Build the discriminator","20cfec4c":"# Build the generator\n"}}