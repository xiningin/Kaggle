{"cell_type":{"8646b791":"code","2a91a8c6":"code","31d2fea2":"code","5a238ea2":"code","8587632f":"code","4978fd29":"code","aa4a5917":"code","4e7018e8":"code","6f2fa462":"code","db5af17c":"code","047b82bc":"code","c9ecd3cf":"code","3d99e2f6":"code","6713ea8c":"code","c1a21bbd":"code","784eaf4d":"code","3597caa6":"code","09e44ab9":"code","be4abf6f":"code","7cb93de3":"code","2e8926ad":"code","638f2daf":"code","22466be3":"code","688ea81f":"code","b2980cd0":"code","0f5454a9":"code","37010be0":"code","0291481e":"code","75f9b9c1":"code","3d907319":"code","195229f8":"code","652b04ca":"code","8b34ead5":"code","f8950712":"code","f3344ede":"code","df3633a5":"code","ac2e9482":"code","4a741c5c":"code","b8148254":"code","9c2723f8":"markdown","e333aecc":"markdown","f183d993":"markdown","43771b65":"markdown","844e74fe":"markdown","49c2196b":"markdown"},"source":{"8646b791":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2a91a8c6":"train = pd.read_csv(\"\/kaggle\/input\/gene-expression\/data_set_ALL_AML_train.csv\")\nactual = pd.read_csv(\"\/kaggle\/input\/gene-expression\/actual.csv\")\nindependent = pd.read_csv(\"\/kaggle\/input\/gene-expression\/data_set_ALL_AML_independent.csv\")\ntrain.head()","31d2fea2":"actual.head()","5a238ea2":"independent.head()","8587632f":"train.describe()","4978fd29":"train.columns","aa4a5917":"# we don't need call columns so let's remove them\n\ntrainreq = [col for col in train.columns if \"call\" not in col]\ntrain = train[trainreq]\ntrain.head()","4e7018e8":"train = train.T\ntrain.head()","6f2fa462":"train2 = train.drop(['Gene Description','Gene Accession Number'],axis=0)\ntrain2.index = pd.to_numeric(train2.index)\ntrain2.head()\n","db5af17c":"train2.sort_index(inplace=True)","047b82bc":"train2.head()","c9ecd3cf":"train2.shape","3d99e2f6":"actual['cancer'].value_counts()","6713ea8c":"patients = list(actual[:38]['cancer'])\ntrain2['Patient_Category'] = patients\ntrain2.head()","c1a21bbd":"train2['Patient_Category'] = train2['Patient_Category'].replace({\"ALL\":0,\"AML\":1})\ntrain2.head()","784eaf4d":"X = train2.drop(\"Patient_Category\",axis=1)\ny = train2.Patient_Category","3597caa6":"X.head()","09e44ab9":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.impute import SimpleImputer","be4abf6f":"sc = StandardScaler()\nX_scl = sc.fit_transform(X)","7cb93de3":"n_components=30\npca = PCA(n_components=n_components)\nX_pca = pca.fit_transform(X_scl)","2e8926ad":"Xpcadf = pd.DataFrame(X_pca)\nXpcadf.columns =['PC'+str(i) for i in range(1,n_components+1)]\n\nXpcadf['Patient Category'] = patients\nprint(f\"Percent of explained variance with {n_components} Components : \", round(pca.explained_variance_ratio_.sum()*100,2))\nXpcadf.head()","638f2daf":"import plotly.express as px\nimport seaborn as sns\nimport plotly.graph_objects as go","22466be3":"plt.figure(figsize=(16,10))\nsns.set_style(\"darkgrid\")\nax = sns.scatterplot(Xpcadf['PC1'],Xpcadf['PC2'],hue=Xpcadf['Patient Category'])\n\nplt.xlabel(f'PC1 ({round(pca.explained_variance_ratio_[0] * 100,1)} %)')\nplt.ylabel(f'PC2 ({round(pca.explained_variance_ratio_[1] * 100,1)} %)')\nplt.title('overall scatter plot of PC1 and PC2')\nplt.grid()\nplt.show()","688ea81f":"fig = px.scatter_3d(data_frame=Xpcadf,x='PC1',y='PC2',z='PC3',color='Patient Category',template='plotly_dark')\nfig.update_layout(title = \"3D Scatter plot for first 3 principal components\",title_x=0.5)\nfig.show()","b2980cd0":"variance = []\ncomps = []\nfor comp in range(2,31,1):\n    pca = PCA(n_components=comp)\n    imppca = pca.fit_transform(X_scl)\n    var = round(pca.explained_variance_ratio_.sum()*100,2)\n    variance.append(var)\n    comps.append(comp)\n\npcatable = pd.DataFrame({\"Number of components\":comps,\"% of variance explained\":variance})\npcatable.style.background_gradient(cmap=\"Reds\")","0f5454a9":"plt.figure(figsize=(12,8))\nsns.barplot(data=pcatable,x='Number of components',y='% of variance explained',palette='coolwarm')\nplt.show()","37010be0":"X.head(2)","0291481e":"def create_importance_dataframe(pca, original_num_df):\n    importance_df  = pd.DataFrame(pca.components_)\n    importance_df.columns  = original_num_df.columns\n    importance_df =importance_df.apply(np.abs)\n    importance_df=importance_df.transpose()\n    num_pcs = importance_df.shape[1]\n    new_columns = [f'PC{i}' for i in range(1, num_pcs + 1)]\n    importance_df.columns  =new_columns\n    return importance_df\n\nimportance_df  =create_importance_dataframe(pca,X)\ndisplay(importance_df.head())","75f9b9c1":"pc1_top_10_features = importance_df['PC1'].sort_values(ascending = False)[:10]\nprint(), print(f'PC1 top 10 features are \\n')\npd.DataFrame(pc1_top_10_features).reset_index().rename(columns={\"index\":\"Gene\",\"PC1\":\"Importance value for PC1\"})","3d907319":"pc2_top_10_features = importance_df['PC2'].sort_values(ascending = False)[:10]\nprint(), print(f'PC2 top 10 features are \\n')\npd.DataFrame(pc1_top_10_features).reset_index().rename(columns={\"index\":\"Gene\",\"PC2\":\"Importance value for PC2\"})","195229f8":"train2.head()","652b04ca":"from sklearn.cluster import KMeans\nsumofsq = {}\nfor k in range(1,15):\n    km = KMeans(n_clusters=k,init='k-means++',max_iter=1000)\n    km = km.fit(X_scl)\n    sumofsq[k] = km.inertia_","8b34ead5":"sns.set_style('whitegrid')\nplt.figure(figsize=(14,8))\nplt.xlabel('Number of Clusters(k)')\nplt.ylabel('Sum of Square Distances')\nplt.title('Elbow Method For Optimal number of Clusters')\nsns.pointplot(x=list(sumofsq.keys()),y=list(sumofsq.values()),color='red')\nplt.show()","f8950712":"kmeansmodel = KMeans(n_clusters=3, init= 'k-means++', max_iter= 1000)\nkmeansmodel.fit_transform(X_scl)\nprint()","f3344ede":"Kmeansdf = X.copy()\nKmeansdf['Cluster_by_KMeans'] = kmeansmodel.labels_","df3633a5":"Xpcadf['Kmeans_cluster'] = kmeansmodel.labels_","ac2e9482":"plt.figure(figsize=(12,7))\nsns.set_style(\"whitegrid\")\nax = sns.scatterplot(Xpcadf['PC1'],Xpcadf['PC2'],hue=Xpcadf['Kmeans_cluster'],palette=\"gist_rainbow\")\n\nplt.xlabel(f'PC1 ({round(pca.explained_variance_ratio_[0] * 100,1)} %)')\nplt.ylabel(f'PC2 ({round(pca.explained_variance_ratio_[1] * 100,1)} %)')\nplt.title('scatter plot of PC1 and PC2 with KMeans clusters')\nplt.grid()\nplt.show()","4a741c5c":"from sklearn.metrics import silhouette_samples, silhouette_score\nimport matplotlib.cm as cm","b8148254":"range_n_clusters = [2, 3, 4, 5, 6,7,8,9,10]\nX = X_scl.copy()\n\nfor n_clusters in range_n_clusters:\n    fig, (ax1, ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 7)\n    ax1.set_xlim([-0.1, 1])\n    ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X)\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\"The average silhouette_score is :\", silhouette_avg)\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        y_lower = y_upper + 10  \n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n    colors = cm.nipy_spectral(cluster_labels.astype(float) \/ n_clusters)\n    ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n                c=colors, edgecolor='k')\n\n    centers = clusterer.cluster_centers_\n    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n                c=\"white\", alpha=1, s=200, edgecolor='k')\n\n    for i, c in enumerate(centers):\n        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n                    s=50, edgecolor='k')\n\n    ax2.set_title(\"The visualization of the clustered data.\")\n    ax2.set_xlabel(\"Feature space for the 1st feature\")\n    ax2.set_ylabel(\"Feature space for the 2nd feature\")\n\n    plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n                  \"with n_clusters = %d\" % n_clusters),\n                 fontsize=14, fontweight='bold')\n\nplt.show()","9c2723f8":"# There are two datasets containing the initial (training, 38 samples) and independent (test, 34 samples) datasets used in the paper. These datasets contain measurements corresponding to ALL and AML samples from Bone Marrow and Peripheral Blood. Intensity values have been re-scaled such that overall intensities for each chip are equivalent.","e333aecc":"# let's try KMeans for some more intuition","f183d993":"# PCA","43771b65":"# clusters = 3 seems fine !","844e74fe":"# From both Elbow method and silhouette analysis n_clusters = 3 seems fine !","49c2196b":"# Lets find out which all features added the most to our PC1 and PC2"}}