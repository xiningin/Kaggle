{"cell_type":{"9f553859":"code","08200ac9":"code","4c3b7231":"code","a71d1699":"code","7a07c2c7":"code","5de94d29":"code","1b39b17d":"code","16097938":"code","a6492945":"code","9039d88d":"code","b9d32274":"code","6881b83a":"code","d2094f5e":"code","c381a1ed":"code","fe9b7618":"code","950585ad":"code","02107de4":"code","e5f80633":"code","121b7a97":"code","fc9c0ba8":"code","50560e2d":"code","6423e8d0":"code","e2a39de6":"code","b8823bc1":"code","7d8ac7eb":"code","3f514291":"code","ed710c0e":"code","16cff3f3":"code","fba3a035":"code","254cc485":"code","6fa46d15":"code","e20d507c":"code","d7d3447a":"code","10748835":"code","6bdeb611":"code","85968057":"code","26786a39":"code","ac025788":"code","4e5f3bb0":"code","85d2d578":"code","4a983e18":"code","c90f2612":"code","939bd58f":"code","b36dc66e":"code","3bc57e5f":"code","0afc1e9e":"code","c1603c74":"code","9bc94589":"code","a26d02b9":"code","3d75bebd":"code","3dff1e2b":"code","8776af54":"markdown","75c6a58d":"markdown"},"source":{"9f553859":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.tokenize import RegexpTokenizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import auc,classification_report,confusion_matrix , roc_auc_score ,accuracy_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.ensemble import GradientBoostingClassifier , RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm \nfrom sklearn.tree import DecisionTreeClassifier , ExtraTreeClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm.classes import SVC\nfrom sklearn.metrics.classification import cohen_kappa_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.linear_model import RidgeClassifier","08200ac9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4c3b7231":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","a71d1699":"train.head()","7a07c2c7":"train.info()","5de94d29":"df = train.copy()","1b39b17d":"def word_tokenze(x):\n    tokenizer = RegexpTokenizer(r'[A-Za-z]+[.]+')\n    return str(tokenizer.tokenize(x)[0])","16097938":"df['Title'] = train.Name.apply(word_tokenze)","a6492945":"df.head()","9039d88d":"df['Age'] = df.groupby('Title')['Age'].apply(lambda x: x.fillna(x.mean()))","b9d32274":"df.info()","6881b83a":"grid = sns.FacetGrid(df , row = 'Embarked')\ngrid.map(sns.pointplot , 'Pclass' ,'Survived' , 'Sex' , palette = 'deep')\ngrid.add_legend()","d2094f5e":"grid = sns.FacetGrid(df , row='Embarked' ,col='Survived' ,size=3 ,aspect=2)\ngrid.map(plt.hist , 'Age' ,alpha =0.7 , bins = 20)\ngrid.add_legend()","c381a1ed":"pd.crosstab(df.Sex,df.Survived)","fe9b7618":"pd.crosstab(df.Survived , df.Embarked)","950585ad":"pd.crosstab(df.Survived ,df.SibSp)","02107de4":"pd.crosstab(df.Survived ,df.Parch)","e5f80633":"pd.crosstab(df.Survived , df.Pclass)","121b7a97":"(df.groupby('Title')['Survived'].mean()*100).round(2).astype(str)+'%'","fc9c0ba8":"(df.groupby('Sex')['Survived'].mean()*100).round(2).astype(str)+'%'","50560e2d":"(df.groupby('Embarked')['Survived'].mean()*100).round(2).astype(str)+'%'","6423e8d0":"(df.groupby('SibSp')['Survived'].mean()*100).round(2).astype(str)+'%'","e2a39de6":"df.groupby('Pclass')['Fare'].mean()","b8823bc1":"(df.groupby('Parch')['Survived'].mean()*100).round(2).astype(str)+'%'","7d8ac7eb":"(df.groupby('Pclass')['Survived'].mean()*100).round(2).astype(str)+'%'","3f514291":"class FinalData:\n    def __init__(self,train , test):\n        self.train = train\n        self.test = test\n    \n    def title_extract(self , series):\n        tokenizer = RegexpTokenizer(r'[A-Za-z]+[.]+')\n        return str(tokenizer.tokenize(series)[0])\n    \n    def final_dataset(self):\n        combine = pd.concat([self.train , self.test] , ignore_index=True)\n        combine['Title'] = combine.Name.apply(self.title_extract)\n        combine = combine.drop('Name' ,axis = 1)\n        combine['Age'] = combine.groupby('Title')['Age'].apply(lambda x: x.fillna(x.mean()))\n        combine['Title'] = combine['Title'].replace(['Don.', 'Rev.', 'Dr.','Dona.',\n        'Major.', 'Lady.', 'Sir.', 'Col.', 'Capt.',\n       'Countess.', 'Jonkheer.'] , 'rare')\n        combine['Title'] = combine['Title'].replace('Mme.' , 'Mrs.')\n        combine['Title'] = combine['Title'].replace('Ms.' , 'Miss.')\n        combine['Title'] = combine['Title'].replace('Mlle.' , 'Miss.')\n        \n        combine['Pclass_sex'] = combine['Pclass'].astype(str) + combine['Sex'].astype(str)\n        pclass_sex = pd.get_dummies(combine.Pclass_sex)\n        combine = pd.concat([combine , pclass_sex] , axis = 1)\n        \n        \n        combine['Sex'] = combine['Sex'].map({'male':1 ,'female':0})\n        combine['AgeBand'] = pd.cut(combine['Age'] , 5)\n        combine.loc[ combine['Age'] <= 16, 'Age'] = 0\n        combine.loc[(combine['Age'] > 16) & (combine['Age'] <= 32), 'Age'] = 1\n        combine.loc[(combine['Age'] > 32) & (combine['Age'] <= 48), 'Age'] = 2\n        combine.loc[(combine['Age'] > 48) & (combine['Age'] <= 64), 'Age'] = 3\n        combine.loc[ combine['Age'] > 64, 'Age'] =4\n        \n        combine.loc[ combine['Fare'] <= 102, 'Fare'] = 0\n        combine.loc[(combine['Fare'] > 102) & (combine['Fare'] <= 204), 'Fare'] = 1\n        combine.loc[(combine['Fare'] > 204) & (combine['Fare'] <= 307), 'Fare'] = 2\n        combine.loc[(combine['Fare'] > 307) & (combine['Fare'] <= 410), 'Fare'] = 3\n        combine.loc[ combine['Fare'] > 410, 'Age'] =4\n        \n        combine['Embarked'] = pd.factorize(combine.Embarked)[0]\n        combine.Title = pd.factorize(combine.Title)[0]\n        combine = combine.drop(['PassengerId' , 'Ticket' , 'Cabin','AgeBand' , 'Pclass_sex'] , axis = 1)\n        combine['Members'] = combine['SibSp'] + combine['Parch'] + 1\n        \n        combine['IsAlone'] = 1\n        combine.loc[(combine['Members'] > 0), 'IsAlone'] = 0\n        \n        combine = combine.drop(['SibSp' , 'Parch'] , axis = 1)\n        pclass_enc = pd.get_dummies(combine.Pclass)\n        combine = pd.concat([combine , pclass_enc] , axis = 1)\n        train_df = combine[combine.index<self.train.shape[0]]\n        test_df = combine[combine.index>=self.train.shape[0]]\n        \n        return (train_df ,test_df)","ed710c0e":"train1, test1 =  FinalData(train , test).final_dataset()","16cff3f3":"test1.Fare.fillna(13.675550 , inplace = True)","fba3a035":"train1 =train1.drop('Pclass' , axis =1 )","254cc485":"y = train1.Survived","6fa46d15":"X = train1.drop('Survived' , axis= 1)","e20d507c":"X_train , X_test , y_train , y_test = train_test_split(X , y , random_state = 42 , test_size = 0.30)","d7d3447a":"X_test.shape","10748835":"train1.head()","6bdeb611":"test1 = test1.drop(['Survived' , 'Pclass'] , axis = 1)","85968057":"stacked_train = np.zeros((y_train.shape[0] ,8))","26786a39":"stacked_test = np.zeros((y_test.shape[0] , 8))","ac025788":"lr = LogisticRegression()\nsgd = SGDClassifier()\nperceptron = Perceptron()\ngbc = GradientBoostingClassifier()\nrfc = RandomForestClassifier()\nenet = RidgeClassifier()\ndtc = DecisionTreeClassifier()\nextc = ExtraTreeClassifier()","4e5f3bb0":"from sklearn.preprocessing import MinMaxScaler\nscaler1 = MinMaxScaler()\nX_scaled = scaler1.fit_transform(X)","85d2d578":"y_lr = cross_val_predict(lr , X_scaled , y , cv = 5)\ny_sgd = cross_val_predict(sgd , X_scaled , y , cv = 5)\ny_perceptron = cross_val_predict(perceptron , X_scaled , y , cv = 5)\ny_gbc = cross_val_predict(gbc , X_scaled , y , cv = 5)\ny_rfc = cross_val_predict(rfc , X_scaled , y, cv = 5)\ny_enet = cross_val_predict(enet , X_scaled , y , cv = 5)\ny_dtc = cross_val_predict(dtc , X_scaled , y, cv = 5)\ny_extc = cross_val_predict(extc ,X_scaled , y , cv = 5)","4a983e18":"stacked_train = np.c_[y_lr , y_sgd , y_perceptron ,y_gbc , y_rfc ,y_enet , y_dtc , y_extc]","c90f2612":"lr.fit(X_scaled ,y)\nsgd.fit(X_scaled ,y)\nperceptron.fit(X_scaled ,y)\ngbc.fit(X_scaled ,y)\nrfc.fit(X_scaled ,y)\nenet.fit(X_scaled ,y)\ndtc.fit(X_scaled ,y)\nextc.fit(X_scaled ,y)","939bd58f":"y_p1 = lr.predict(test1)\ny_p2 = sgd.predict(test1)\ny_p3 = perceptron.predict(test1)\ny_p4 = gbc.predict(test1)\ny_p5 = rfc.predict(test1)\ny_p6 = enet.predict(test1)\ny_p7 = dtc.predict(test1)\ny_p8 = extc.predict(test1)","b36dc66e":"stacked_test = np.c_[y_p1 , y_p2, y_p3, y_p4,y_p5,y_p6,y_p7,y_p8]","3bc57e5f":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ntrain11 = scaler.fit_transform(stacked_train)\ntest11 =  scaler.fit_transform(stacked_test)","0afc1e9e":"lr1 =DecisionTreeClassifier()\nlr1.fit(train11 , y)","c1603c74":"lr1.score(train11 , y)","9bc94589":"model = LogisticRegression()\nmodel.fit(X_train , y_train)\nmodel.score(X_train , y_train)\nmodel.score(X_test , y_test)\n\ntprs = []\naucs = []\nmean_fpr = np.linspace(0, 1, 100)\ncv = StratifiedKFold(n_splits=5)\n\nfig, ax = plt.subplots(figsize=(10,10))\nfor i, (tr, val) in enumerate(cv.split(X_train, y_train)):\n    model.fit(X_train.iloc[tr], y_train.iloc[tr])\n    viz = plot_roc_curve(model, X_train.iloc[val], y_train.iloc[val],\n                         name='ROC fold {}'.format(i),\n                         alpha=0.4, lw=2, ax=ax)\n    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n    interp_tpr[0] = 0.0\n    tprs.append(interp_tpr)\n    aucs.append(viz.roc_auc)\n    \nax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n        label='Chance', alpha=.8)\n\nmean_tpr = np.mean(tprs, axis=0)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nstd_auc = np.std(aucs)\nax.plot(mean_fpr, mean_tpr, color='b',\n        label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n        lw=2, alpha=.8)\n\nstd_tpr = np.std(tprs, axis=0)\ntprs_upper = np.minimum(mean_tpr + std_tpr, 1)\ntprs_lower = np.maximum(mean_tpr - std_tpr, 0)\nax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n                label=r'$\\pm$ 1 std. dev.')\n\nax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n       title=\"Receiver operating characteristic curve\")\nax.legend(loc=\"lower right\")\nplt.show()","a26d02b9":"y_predict = model.predict(X_test)\nprint(accuracy_score(y_test , y_predict))\nprint(roc_auc_score(y_test , y_predict))\nprint(classification_report(y_test , y_predict))","3d75bebd":"predt = lr1.predict(test11)","3dff1e2b":"pid = test.PassengerId\nsubmit = pd.DataFrame()\nsubmit['PassengerId'] = pid\nsubmit['Survived'] = predt.astype(int)\nsubmit.to_csv('submission.csv' , index=False)","8776af54":"# Hello Kagglers! This is my first notebook on kaggle. I hope you will like it. Please upvote.\n> This hackathon is only for learning purpose so it's better get 10k rank with your work rather 1st rank by cheating. Accuracy of 1 is not possible by any machine learning algorithm. But you can use others work and learn from them. It will help you in your future competitions. ","75c6a58d":"### FinalData Class"}}