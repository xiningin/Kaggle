{"cell_type":{"9a4a9929":"code","5aa8c47c":"code","2cb8fd36":"code","5ad86448":"code","dfe97a16":"code","6b49c0ea":"code","07f17287":"code","dc8b5968":"code","5e42c6e1":"code","81fd5255":"code","082fef0f":"code","5a878c04":"code","69816923":"code","084d603c":"code","4846b319":"code","c7caaffe":"code","b4ce4e8b":"code","24610013":"code","e186c238":"code","ce75e51f":"code","7134a7f4":"code","2aafc485":"code","8b45c694":"code","a5725570":"code","1ad1f9bd":"code","953417d3":"code","4a21c6f9":"code","4602ef08":"code","e03d6226":"code","947c5a09":"code","d7cfe90c":"code","1ffb0e17":"markdown","c0941627":"markdown","f76465bb":"markdown","71fbb157":"markdown","cd994c11":"markdown","b0705aca":"markdown","3e300283":"markdown","47535206":"markdown","b4990989":"markdown","4fd3d4db":"markdown","a4fe5514":"markdown"},"source":{"9a4a9929":"!pip install chart_studio","5aa8c47c":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pylab as plt\nimport chart_studio.plotly as py\nimport plotly.graph_objs as go\nfrom plotly.offline import iplot\n\nfrom sklearn import model_selection\nimport tensorflow as tf\nfrom keras import optimizers, regularizers\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, LSTM, Conv1D, Activation, MaxPooling1D, Flatten\nimport keras.backend as K\n\n%matplotlib inline","2cb8fd36":"bpath = '..\/input\/competitive-data-science-predict-future-sales\/'\ntrain = pd.read_csv(bpath + 'sales_train.csv', parse_dates=['date'], infer_datetime_format=True)\ntrain.head()","5ad86448":"train.info()","dfe97a16":"train['sales'] = train.item_price * train.item_cnt_day\ntrain.head()","6b49c0ea":"daily_sales = train.groupby('date', as_index=False)['sales'].sum()\ndaily_sales = daily_sales.sort_values('date', axis=0)\ndaily_sales.head()","07f17287":"daily_sales_sp = go.Scatter(x=daily_sales.date, y=daily_sales.sales)\nlayout = go.Layout(title='Daily Sales', xaxis=dict(title='Date'), yaxis=dict(title='Daily Sales'))\nfig = go.Figure(data=[daily_sales_sp], layout=layout)\niplot(fig)","dc8b5968":"daily_sales_by_store = train.groupby(['date', 'shop_id'], axis=0, as_index=False)['sales'].sum()\ndaily_sales_by_store_sp = []\nstores = np.sort(train.shop_id.unique())\nfor store in stores[26:36] :\n    dummy = daily_sales_by_store[daily_sales_by_store.shop_id == store]\n    daily_sales_by_store_sp.append(go.Scatter(x=dummy.date, y=dummy.sales, name='Store %s' % store))\n    \nlayout = go.Layout(title='Daily Sales by Store', xaxis=dict(title='Date'), yaxis=dict(title='Sales'))\nfig = go.Figure(data=daily_sales_by_store_sp, layout=layout)\niplot(fig)","5e42c6e1":"daily_sales_by_item = train.groupby(['date', 'item_id'], as_index=False, axis=0)['sales'].sum()\ndaily_sales_by_item = daily_sales_by_item.sort_values('date', axis=0)\n\nitems = train.item_id.unique()\ndaily_sales_by_item_sp = []\nfor item in items[450:550] :\n    dummy = daily_sales_by_item[daily_sales_by_item.item_id == item]\n    daily_sales_by_item_sp.append(go.Scatter(x=dummy.date, y=dummy.sales, name=('item %s' %item)))\n    \nlayout = go.Layout(title='Daily sales by item', xaxis=dict(title='Date'), yaxis=dict(title='sales'))\nfig = go.Figure(data=daily_sales_by_item_sp, layout=layout)\niplot(fig)","81fd5255":"test = pd.read_csv(bpath + 'test.csv')\nprint(test.shape)\ntest.head()","082fef0f":"df_train = train.groupby([train.date.apply(lambda x: x.strftime('%Y-%m')), 'item_id', 'shop_id']).sum().reset_index()\ndf_train = df_train[['date','item_id','shop_id','item_cnt_day']]\ndf_train = df_train.pivot_table(index=['item_id','shop_id'], columns='date',\n                                values='item_cnt_day',fill_value=0).reset_index()\ndf_train.head()","5a878c04":"df_train.info()","69816923":"df_test = pd.merge(test, df_train, on=['item_id','shop_id'], how='left')\ndf_test = df_test.fillna(0)\ndf_test = df_test.drop(labels=['ID', 'shop_id', 'item_id'], axis=1)\ndf_test.head()","084d603c":"last_month = '2015-12'\ny_train = df_test[last_month]\nx_train = df_test.drop(labels=[last_month], axis=1)\nx_train = x_train.to_numpy()\ny_train = y_train.to_numpy()\ntrain_x, valid_x, train_y, valid_y = model_selection.train_test_split(x_train, y_train, \n                                                                      train_size=0.8, shuffle=True)\nprint(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)","4846b319":"x_test = df_test.drop(labels=['2013-01'], axis=1)\nx_test = x_test.to_numpy()\nprint(x_test.shape)","c7caaffe":"def saleModel_mlp(input_shape) :\n    x_input = Input(input_shape)\n    x = Dense(64, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(0.01),\n              bias_regularizer=regularizers.l2(0.02))(x_input)\n    x = Dense(32, activation='relu', use_bias=True, kernel_regularizer=regularizers.l2(0.01),\n              bias_regularizer=regularizers.l2(0.02))(x)\n    x_out = Dense(1, activation=None)(x)\n    \n    model = Model(inputs=x_input, outputs=x_out, name='saleModel_mlp')\n    \n    return model","b4ce4e8b":"mlpModel = saleModel_mlp(np.shape(train_x[1,:]))\nmlpModel.summary()","24610013":"optim = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.99)\nmlpModel.compile(optimizer=optim, metrics=['accuracy'], loss='mean_squared_error')","e186c238":"nepochs = 200\nmlp_history = mlpModel.fit(x=train_x, y=train_y, validation_data=(valid_x, valid_y), epochs=nepochs, \n                           batch_size=512, verbose=1, shuffle=True, validation_split=0.0)","ce75e51f":"fig = plt.figure(figsize=(8,4))\nplt.plot(range(nepochs), mlp_history.history['loss'], 'r', label='train')\nplt.plot(range(nepochs), mlp_history.history['val_loss'], 'b', label='valid')\nplt.legend()\nplt.title('multi-layer perceptron')\nplt.xlabel('epochs')\nplt.ylabel('loss');","7134a7f4":"def saleModel_lstm(input_shape) :\n    x_input = Input(input_shape)\n    x = LSTM(units=32, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \n                        kernel_initializer='glorot_uniform', return_sequences=True)(x_input)\n    x = LSTM(units=16, activation='tanh', recurrent_activation='sigmoid', use_bias=True, \n                        kernel_initializer='glorot_uniform', return_sequences=False)(x)\n    x_out = Dense(1, activation=None)(x)\n    \n    model = Model(inputs=x_input, outputs=x_out, name='saleModel_lstm')\n    \n    return model","2aafc485":"train_xx = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\nvalid_xx = valid_x.reshape((valid_x.shape[0], valid_x.shape[1], 1))\ntest_xx = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))\nprint(train_xx.shape, valid_xx.shape, test_xx.shape)","8b45c694":"lstmModel = saleModel_lstm(np.shape(train_xx[1,:, :]))\nlstmModel.summary()","a5725570":"lstmModel.compile(optimizer=optim, loss='mean_squared_error', metrics=['accuracy'])\nlstm_history = lstmModel.fit(x=train_xx, y=train_y, validation_data=(valid_xx, valid_y), verbose=1,\n                            epochs=100, batch_size=1024, shuffle=True)","1ad1f9bd":"fig = plt.figure(figsize=(8,4))\nplt.plot(range(100), lstm_history.history['loss'], 'r', label='train')\nplt.plot(range(100), lstm_history.history['val_loss'], 'b', label='valid')\nplt.legend()\nplt.title('LSTM')\nplt.xlabel('epochs')\nplt.ylabel('loss');","953417d3":"def saleModel_cnn(input_shape) :\n    x_input = Input(input_shape)\n    x = Conv1D(filters=64, padding='valid', strides=1, kernel_size=3)(x_input)\n    x = Activation('relu')(x)\n    x = MaxPooling1D(pool_size=2, strides=None, padding='valid')(x)\n    \n    x = Conv1D(filters=32, padding='valid', strides=1, kernel_size=3)(x)\n    x = Activation('relu')(x)\n    x = MaxPooling1D(pool_size=2, strides=None, padding='valid')(x)\n    \n    x = Flatten()(x)\n    x_out = Dense(1, activation=None)(x)\n    \n    model = Model(inputs=x_input, outputs=x_out, name='saleModel_cnn')\n    \n    return model","4a21c6f9":"cnnModel = saleModel_cnn(np.shape(train_xx[1,:,:]))\ncnnModel.summary()","4602ef08":"cnnModel.compile(optimizer=optim, loss='mse', metrics=['accuracy'])\ncnn_history = cnnModel.fit(x=train_xx, y=train_y, validation_data=(valid_xx, valid_y), verbose=1,\n                          epochs=nepochs, batch_size=512, shuffle=True)","e03d6226":"fig = plt.figure(figsize=(5,4))\nplt.plot(range(nepochs), cnn_history.history['loss'], 'r', label='train')\nplt.plot(range(nepochs), cnn_history.history['val_loss'], 'b', label='valid')\nplt.legend()\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.title('1D CNN');","947c5a09":"mlp_pred = mlpModel.predict(x_test)\nlstm_pred = lstmModel.predict(test_xx)\ncnn_pred = cnnModel.predict(test_xx)\nprint(mlp_pred, lstm_pred, cnn_pred)","d7cfe90c":"submission = pd.read_csv('sample_submission.csv')\nsubmission.item_cnt_month = lstm_pred\nsubmission.to_csv ('submission.csv', index = None, header = True)","1ffb0e17":"### 2.4. Daily sales by item","c0941627":"## 4. Prepare time series","f76465bb":"### 2.2. Daily sales","71fbb157":"## 5. Multi-layer perceptron","cd994c11":"### 2.3. Daily sales by store","b0705aca":"# Deep Learning for Time Series Forecasting","3e300283":"## 6. LSTM","47535206":"## 7. Convolutional NN","b4990989":"## 3. Load test data","4fd3d4db":"## 2. Explore data \n\n### 2.1. Overall daily sales","a4fe5514":"## 1. Load data"}}