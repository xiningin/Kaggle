{"cell_type":{"0ecc6b97":"code","b15c124d":"code","d3c8c74d":"code","7c629f72":"code","8e5a29b5":"code","1c90ddff":"code","35ad3604":"code","172ade27":"code","c540ce2c":"code","c99c3041":"code","a58c4b46":"code","f733e575":"code","6af265df":"code","c44623df":"code","a575e5a2":"code","6ef0db3d":"code","3efff289":"code","f253cfb9":"code","09bc2987":"code","471487d7":"code","1b75fb59":"code","ffe1b7a7":"code","9c40da71":"code","e65cd60e":"code","d9ae2bc1":"code","a65d1324":"code","e29fe27c":"code","27e14071":"code","60763085":"code","06f81164":"code","4edfa42d":"code","34ed8701":"code","67ac0d9e":"code","448c0e32":"code","fe446cea":"code","123fd789":"code","2effd8a7":"code","ef5698d9":"code","4e2917fc":"code","21dfb012":"code","b96904cb":"code","3d461f52":"code","0bb3db45":"code","5353a2ba":"code","f8e808ef":"code","d1273f0c":"code","70649313":"code","133be668":"code","f3a5393d":"code","0105f9ae":"code","7059ce81":"code","65314ef0":"code","55f68ad7":"code","20be081b":"code","7d1c5b4d":"code","5ba73748":"markdown","7b91e48c":"markdown","ea47c710":"markdown","320c2a70":"markdown","ea0153d7":"markdown","a5a4b722":"markdown","4aa1c4bd":"markdown","5fae6499":"markdown","988cfa25":"markdown","f40a9244":"markdown","3a16c7d8":"markdown","528127f3":"markdown","76eb9ceb":"markdown","3d979ceb":"markdown","705a39c6":"markdown","d647d83a":"markdown","1d24e7cd":"markdown","dbc59dc2":"markdown","b139912e":"markdown","3bb9e318":"markdown","4ab369c8":"markdown","03c400c8":"markdown","51b848df":"markdown","3e206a6d":"markdown","8baf6460":"markdown","d84457ea":"markdown","b4bff40b":"markdown","c8f8de74":"markdown","5c21158c":"markdown","e0b0d47a":"markdown","f8a2aa9a":"markdown","bfa81765":"markdown","834bdbc3":"markdown","d34f180c":"markdown","23510943":"markdown","53e10f9c":"markdown","c951b1cd":"markdown","8658bc14":"markdown","2bd9bff7":"markdown","939b6f7c":"markdown","19e8838f":"markdown","307760ab":"markdown","9c5a33a0":"markdown","f8131393":"markdown","8852bd86":"markdown","1f1927c6":"markdown","fb67ac49":"markdown","61fb61b5":"markdown","c6fdb246":"markdown","d98d4171":"markdown","7e68cbd6":"markdown","033e850a":"markdown","fc1ab184":"markdown","67396e71":"markdown","a971d3b8":"markdown","697c73eb":"markdown","6a4b4829":"markdown","d68dd008":"markdown","3ea34680":"markdown","2f892028":"markdown","bd9df7ad":"markdown"},"source":{"0ecc6b97":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#VISUALIZATION\nfrom matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n%matplotlib inline\nimport seaborn as sns\nfrom skimage import io\nsns.set()\n\n#MACHINELEARNING\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.decomposition import PCA\n\nfrom glob import glob\nfrom os import listdir\nimport pickle\n\nfrom tqdm.notebook import tqdm\ntqdm().pandas();\n\nprint('import complete')","b15c124d":"files = listdir(\"..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\")\nprint(f'total number of subfolders inside our folder: {len(files)}')\nprint(f'preview of first 10 subfolders:')\nprint(files[0:10])","d3c8c74d":"patient_folder = listdir(\"..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/13689\")\nprint(f'number of subfolders inside patient number folder: {len(patient_folder)}')","7c629f72":"patient_folder","8e5a29b5":"from pprint import pprint\nclass_0 = listdir('..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/13689\/0')\nclass_1 = listdir('..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/13689\/1')\n\nprint(f'Contents of folder 0:')\npprint(class_0[0:5])\nprint()\nprint(f'Contents of folder 1:')\npprint(class_1[0:5])","1c90ddff":"base_path = '..\/input\/breast-histopathology-images\/IDC_regular_ps50_idx5\/'\n# We will save the base path to the patient folders, so that we can easily loop over them\npatient_ids = listdir(base_path)","35ad3604":"class_0_total = 0\nclass_1_total = 0\nfrom pprint import pprint\nfor patient_id in patient_ids:\n    class_0_files = listdir(base_path + patient_id + '\/0')\n    class_1_files = listdir(base_path + patient_id + '\/1')\n\n    class_0_total += len(class_0_files)\n    class_1_total += len(class_1_files) \n\ntotal_images = class_0_total + class_1_total\n    \nprint(f'Number of patches in Class 0: {class_0_total}')\nprint(f'Number of patches in Class 1: {class_1_total}')\nprint(f'Total number of patches: {total_images}')\n","172ade27":"columns = [\"patient_id\",'x','y',\"target\",\"path\"]\ndata_rows = []\ni = 0\niss = 0\nisss = 0\n\n# note that we loop through the classes after looping through the \n# patient ids so that we avoid splitting our data into [all class 0 then all class 1]\nfor patient_id in tqdm(patient_ids):\n    for c in [0,1]:\n        class_path = base_path + patient_id + '\/' + str(c) + '\/'\n        imgs = listdir(class_path)\n        \n        # Extracting Image Paths\n        img_paths = [class_path + img + '\/' for img in imgs]\n        \n        # Extracting Image Coordinates\n        img_coords = [img.split('_',4)[2:4] for img in imgs]\n        x_coords = [int(coords[0][1:]) for coords in img_coords]\n        y_coords = [int(coords[1][1:]) for coords in img_coords]\n\n        for (path,x,y) in zip(img_paths,x_coords,y_coords):\n            values = [patient_id,x,y,c,path]\n            data_rows.append({k:v for (k,v) in zip(columns,values)})\n# We create a new dataframe using the list of dicts that we generated above\ndata = pd.DataFrame(data_rows)\nprint(f'Shape of Dataframe: {data.shape}')\ndata.head()","c540ce2c":"cancer_perc = data.groupby(\"patient_id\").target.value_counts()\/ data.groupby(\"patient_id\").target.size()\ncancer_perc = cancer_perc.unstack()\n\nfig, ax = plt.subplots(1,3,figsize=(25,5))\n\n# Plotting Frequency of Patches per Patient\nsns.distplot(data.groupby(\"patient_id\").size(), ax=ax[0], color=\"Orange\", kde=False, bins=15)\nax[0].set_xlabel(\"Number of patches\")\nax[0].set_ylabel(\"Frequency\")\nax[0].set_title(\"How many patches do we have per patient?\")\n\n# Plotting Percentage of an image that is covered by Invasive Ductile Carcinoma\nsns.distplot(cancer_perc.loc[:, 1]*100, ax=ax[1], color=\"Tomato\", kde=False, bins=15)\nax[1].set_title(\"How much percentage of an image is covered by IDC?\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_xlabel(\"% of patches with IDC\")\n\n# Plotting number of patches that show IDC\nsns.countplot(data.target, palette='pastel', ax=ax[2]);\nax[2].set_ylabel(\"Count\")\nax[2].set_xlabel(\"no(0) versus yes(1)\")\nax[2].set_title(\"How many patches show IDC?\");","c99c3041":"# Return random sample indexes that are cancer positive and cancer negative of size 50\n# replace = False means that no duplication is allowed\npositive_tissue = np.random.choice(data[data.target==1].index.values, size=100, replace=False)\nnegative_tissue = np.random.choice(data[data.target==0].index.values, size=100, replace=False)\n\nn_rows = 10\nn_cols = 10","a58c4b46":"fig,ax = plt.subplots(n_rows,n_cols,figsize = (30,30))\n\nfor row in tqdm(range(n_rows)):\n    for col in range(n_cols):\n        # below is a counter to cycle through the image indexes\n        idx = positive_tissue[col + n_cols*row]\n        img = io.imread(data.loc[idx, \"path\"])\n        ax[row,col].imshow(img[:,:,:])\n        ax[row,col].grid(False)","f733e575":"fig,ax = plt.subplots(n_rows,n_cols,figsize = (30,30))\n\nfor row in tqdm(range(n_rows)):\n    for col in range(n_cols):\n        # below is a counter to cycle through the image indices\n        idx = negative_tissue[col + n_cols*row]\n        img = io.imread(data.loc[idx, \"path\"])\n        ax[row,col].imshow(img[:,:,:])\n        ax[row,col].grid(False)","6af265df":"def get_patient_df(patient_id):\n    return data.loc[data['patient_id']== patient_id,:]","c44623df":"n_rows = 5\nn_cols = 3\nn_imgs = n_rows*n_cols\ncolors = ['pink', 'purple']\n\nfig, ax = plt.subplots(n_rows,n_cols,figsize=(20, 27))\n\npatient_ids = np.random.choice( data.patient_id.unique(), size=n_imgs, replace=False)\n\nfor row in tqdm(range(n_rows)):\n    for col in range(n_cols):\n        patient_id = patient_ids[col + n_cols*row]\n        patient_df = get_patient_df(patient_id)\n        \n        ax[row,col].scatter(patient_df.x.values, \\\n                            patient_df.y.values, \\\n                            c=patient_df.target.values,\\\n                            cmap=ListedColormap(colors), s=20)\n        ax[row,col].set_title(\"patient \" + patient_id)","a575e5a2":"def visualise_breast_tissue(patient_id, df = data,pred = False, crop_dimension = [50,50]):\n    # Plotting Settings\n    plt.xticks([])\n    plt.yticks([])\n    # Get patient dataframe\n    p_df = get_patient_df(patient_id)\n    # Get the dimensions of the breast tissue image\n    max_coord = np.max((*p_df.x,*p_df.y))\n    # Allocate an array to fill image pixels in,use uint8 type as you don't need an int over 255\n    grid = 255*np.ones(shape = (max_coord + crop_dimension[0], max_coord + crop_dimension[1], 3)).astype(np.uint8)\n    mask = 255*np.ones(shape = (max_coord + crop_dimension[0], max_coord + crop_dimension[1], 3)).astype(np.uint8)\n    # Replace array values with values of the image\n    for x,y,target,path in zip(p_df['x'],p_df['y'],p_df['target'],p_df['path']):\n        try:\n            img = io.imread(path)\n            # Replace array values with cropped image values\n            grid[y:y+crop_dimension[1],x:x+crop_dimension[0]] = img\n            # Check if target is cancerous or not\n            if target != 0:\n                # If the target is cancerous then, replace array values with the color blue\n                mask[y:y+crop_dimension[1],x:x+crop_dimension[0]] = [0,0,255]\n        except: pass\n    # if prediction is not specifies then show the image normally\n    if pred == False:\n        io.imshow(grid)\n        img = grid\n    # if prediction is specified then apply a mask to the areas that contain predicted cancerous cells\n    else:\n        # Specify the desired alpha value\n        alpha = 0.78\n        # This is step is very important, adding 2 numpy arrays sets the values to float64, which is why convert them back to uint8\n        img = (mask * (1.0 - alpha) + grid * alpha).astype('uint8')\n        io.imshow(img)\n    return img","6ef0db3d":"n_rows = 5\nn_cols = 3\nn_imgs = n_rows*n_cols\n\nfig, ax = plt.subplots(n_rows,n_cols,figsize=(20, 27))\n\nfor row in tqdm(range(n_rows)):\n    for col in range(n_cols):\n        p_id = patient_ids[col + n_cols*row]\n        \n        img = visualise_breast_tissue(p_id, pred = True)\n        ax[row,col].grid(False)\n        ax[row,col].set_xticks([])\n        ax[row,col].set_yticks([])\n        ax[row,col].set_title(\"Breast tissue slice of patient: \" + p_id)        \n        ax[row,col].imshow(img)","3efff289":"def get_classes_split(series):\n    ratio = np.round(series.value_counts()\/series.count()*100,decimals = 1)\n    return ratio","f253cfb9":"import random\ndef shuffle_patients(frame):\n    groups = [df for _, df in frame.groupby('patient_id')]\n    random.shuffle(groups)\n    shuffled_df = pd.concat(groups).reset_index(drop=True)\n    return shuffled_df","09bc2987":"from sklearn.model_selection import train_test_split\nfor i in range(3):\n    data = shuffle_patients(data)\n    display(data.head())","471487d7":"(get_classes_split(data['target']))","1b75fb59":"from sklearn.model_selection import train_test_split\n\ndef split_data(data,train_test_percentage = 0.7,percentage_of_dataset = 0.2):\n    # get number of datapoints to split from the total dataset\n    n_points = round(data.shape[0]*0.2)\n    # retrieve the desired initial split of the data based on the specified percentage of the dataset\n    data = data[0:n_points]\n    # retrieve all the columns except the target columns and store them in X\n    X = data.loc[:,data.columns != 'target']\n    # retrieve the target column and store them in y \n    y = data.loc[:,['target']]\n    # Split the model to training and testing sets for both the features and the targets\n    X_train, X_test, y_train, y_test =  train_test_split(X, y, train_size=train_test_percentage, shuffle = False)\n    return [X_train, X_test, y_train, y_test]","ffe1b7a7":"X_train, X_test, y_train, y_test = split_data(data)","9c40da71":"print(f'Number of training samples: {X_train.shape[0]} ')\nprint(f'Number of testing samples: {X_test.shape[0]} ')","e65cd60e":"print('training data split:')\ndisplay(get_classes_split(y_train.target))\nprint('testing data split:')\ndisplay(get_classes_split(y_test.target))","d9ae2bc1":"# path to images\nimg_path = X_train.path[0]\n# reading the images\nrgb_img = io.imread(img_path)\ngray_img = io.imread(img_path, as_gray = True) # careful when spelling the word gray\n# plot images\nfig,ax = plt.subplots(1,3, figsize = (25,50))\nax[0].grid(False)\nax[1].grid(False)\nax[2].grid(False)\n# set titles\nax[0].set_title('RGB Image')\nax[1].set_title('Grayscale Image')\nax[2].set_title('Grayscale Image with cmap')\n# display images\nax[0].imshow(rgb_img)\nax[1].imshow(gray_img,cmap = 'gray')\nax[2].imshow(gray_img)","a65d1324":"print(f'rgb image dimension: {rgb_img.shape}')\nprint(f'grayscale image dimension: {gray_img.shape}')","e29fe27c":"def paths_to_array(paths,n_pixels = 2500):\n    imgs_array = np.zeros((paths.shape[0],n_pixels))\n    for i,path in tqdm(enumerate(paths),total = imgs_array.shape[0]):\n        # Read image\n        img = io.imread(path,as_gray = True)\n        # Flatten image\n        img = img.flatten()\n        # Store image in corresponding array row\n        imgs_array[i,0:img.shape[0]] = img\n    return imgs_array","27e14071":"# Store paths inside variable\npaths = X_train.path\n# Convert paths to array of images\nimgs_array = paths_to_array(paths)\nimgs_array.shape","60763085":"imgs_array_mean = imgs_array.mean(axis = 1)","06f81164":"X_train['img_mean'] = imgs_array_mean","4edfa42d":"import plotly_express as px\n\navg_fig = px.histogram(X_train, x = \"img_mean\", color = y_train.target,marginal = \"violin\")\navg_fig.update_layout(barmode = 'overlay') # choose stack, overlay, relative or group\navg_fig.update_traces(opacity = 0.75)\navg_fig.update_layout(title_text = 'mean image distribution')\navg_fig.show()","34ed8701":"def avg_quadrants(flat_img):\n    # get the shape of the image array \n    shape = np.sqrt(flat_img.shape[0]).astype(int)\n    shape_2 = (shape\/2).astype(int)\n    # reshape the flat image in order to extract the quadrants\n    img = flat_img.reshape(shape,shape)\n    # get the mean value for each quadrant\n    top_left = img[:shape_2,:shape_2].mean()\n    top_right = img[:shape_2,shape_2:].mean()\n    bottom_right = img[shape_2:,shape_2:].mean()\n    bottom_left = img[shape_2:,:shape_2].mean()\n    \n    return [top_left,top_right,bottom_left,bottom_right]","67ac0d9e":"def get_quadrant_array(imgs_array):\n    # initialize array of zeros\n    avg_quad_array = np.zeros((imgs_array.shape[0],4))\n    # loop through imgs_array and get the averages for each quadrant\n    for i in tqdm(range(imgs_array.shape[0])):\n        flat_img = imgs_array[i]\n        quad_avgs = avg_quadrants(flat_img)\n        avg_quad_array[i] = quad_avgs\n    return avg_quad_array","448c0e32":"avgs_array = get_quadrant_array(imgs_array)","fe446cea":"rand_n = np.random.randint(imgs_array.shape[0])\n# reading the images\ngray_img = imgs_array[rand_n].reshape(50,50)\nquad_img = avgs_array[rand_n].reshape(2,2)\n# plot images\nfig,ax = plt.subplots(1,2, figsize = (12.5,25))\nax[0].grid(False)\nax[1].grid(False)\n# set titles\nax[0].set_title('Grayscale Image')\nax[1].set_title('Grayscale Quadrant Average')\n# display images\nax[0].imshow(gray_img,cmap = 'gray')\nax[1].imshow(quad_img,cmap = 'gray')","123fd789":"# create a new pandas dataframe using the numpy average quadrant image data\navg_quad_df = pd.DataFrame(avgs_array, columns = ['top_left','top_right','bottom_left','bottom_right'])","2effd8a7":"avg_quad_df.head()","ef5698d9":"# Remember to store dataframes in a tuple, and axis = 1\nX_train = pd.concat((X_train,avg_quad_df), axis = 1) \nX_train.head()","4e2917fc":"# Top Left no cancer \nTL_no_c = X_train.loc[y_train.target == 0,'top_left']\n# Top Left with cancer\nTL_c = X_train.loc[y_train.target == 1,'top_left']\n# Top Right no cancer \nTR_no_c = X_train.loc[y_train.target == 0,'top_right']\n# Top Right with cancer\nTR_c = X_train.loc[y_train.target == 1,'top_right']\n# Bottom Left no cancer \nBL_no_c = X_train.loc[y_train.target == 0,'bottom_left']\n# Bottom Left with cancer\nBL_c = X_train.loc[y_train.target == 1,'bottom_left']\n# Bottom Right no cancer \nBR_no_c = X_train.loc[y_train.target == 0,'bottom_right']\n# Bottom Right with cancer\nBR_c = X_train.loc[y_train.target == 1,'bottom_right']","21dfb012":"from plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\n# Initialize figure with subplots\nfig = make_subplots(\n    rows=2, cols=2, subplot_titles=(\"Top Left Quadrant\", \"Top Right Quadrant\", \"Bottom Left Quadrant\", \"Bottom Right Quadrant\")\n)\n\n\n# Add traces\nfig.add_trace(go.Histogram(x=TL_no_c), row=1, col=1)\nfig.add_trace(go.Histogram(x=TL_c), row=1, col=1)\n\nfig.add_trace(go.Histogram(x=TR_no_c), row=1, col=2)\nfig.add_trace(go.Histogram(x=TR_c), row=1, col=2)\n\nfig.add_trace(go.Histogram(x=BL_no_c), row=2, col=1)\nfig.add_trace(go.Histogram(x=BL_c), row=2, col=1)\n\nfig.add_trace(go.Histogram(x=BR_no_c), row=2, col=2)\nfig.add_trace(go.Histogram(x=BR_c), row=2, col=2)\n\n# Update xaxis properties\nfig.update_xaxes(title_text=\"Average Top Left Quadrant Values\", showgrid=False, row=1, col=1)\nfig.update_xaxes(title_text=\"Average Top Right Quadrant Values\", showgrid=False, row=1, col=2)\nfig.update_xaxes(title_text=\"Average Bottom Left Quadrant Values\", showgrid=False, row=2, col=1)\nfig.update_xaxes(title_text=\"Average Bottom Right Quadrant Values\", showgrid=False, row=2, col=2)\n\n# Update yaxis properties\nfig.update_yaxes(title_text=\"Count\", showgrid=False, row=1, col=1)\nfig.update_yaxes(title_text=\"Count\", showgrid=False, row=1, col=2)\nfig.update_yaxes(title_text=\"Count\", showgrid=False, row=2, col=1)\nfig.update_yaxes(title_text=\"Count\", showgrid=False, row=2, col=2)\n\n# Update title and height\nfig.update_layout(title_text=\"Average Quadrant Data Split\", height=700)\n\nfig.show()","b96904cb":"X_train.head()","3d461f52":"from sklearn.svm import SVC\nsvc = SVC(kernel = 'rbf',gamma = 'auto' )\nsvc_n = SVC(gamma = 'auto')","0bb3db45":"X_train.head()","5353a2ba":"X_train = X_train.loc[:,['img_mean','top_left','top_right','bottom_left','bottom_right']]\nX_train.head()","f8e808ef":"svc.fit(X_train,y_train)","d1273f0c":"svc.score(X_train,y_train)","70649313":"from sklearn.metrics import confusion_matrix\n\ntn, fp, fn, tp = confusion_matrix(svc.predict(X_train),y_train).ravel()\n\nprint(f'training set: true negatives: {tn}')\nprint(f'training set: true positives: {tp}')\nprint(f'training set: false negatives: {fn}')\nprint(f'training set: false positives: {fp}')","133be668":"# tn, fp, fn, tp = confusion_matrix(X_data,y_data).ravel()\n\n# print(f'training set: true negatives: {tn}')\n# print(f'training set: true positives: {tp}')\n# print(f'training set: false negatives: {fn}')\n# print(f'training set: false positives: {fp}')","f3a5393d":"# def pickle_file(output_file_name, object_to_pickle):\n#     \"\"\"\n#     Accepts output file name and the object to pickle\n#     Returns a pickled file with the desired output file name\n#     \"\"\"\n#     with open(output_file_name, \"wb\") as out_file:\n#         pickle.dump(object_to_pickle, out_file)\n\n\n# def unpickle_file(file_name):\n#     \"\"\"\n#     Accepts the pickle file name \n#     Returns an object from the pickled file \n#     \"\"\"\n#     with open(file_name, \"rb\") as in_file:\n#         return pickle.load(in_file)","0105f9ae":"# pickle_file('svc_rbf',svc)","7059ce81":"# pickle_file('rgb_pca',rgb_pca)\n# pickle_file('X_train_df',X_train_df)\n# pickle_file('X_test_df',X_test_df)\n# pickle_file('y_train_df',y_train_df)\n# pickle_file('y_test_df',y_test_df)\n# pickle_file('OSX_df',OSX_df)\n# pickle_file('OSy_df',OSy_df)","65314ef0":"# img_paths = data['path']\n# # Read the data of the first path \n# img = io.imread(img_paths[0])\n# print(f'original image shape: {img.shape}')\n# # Flattening the image\n# img = img.flatten()\n# print(f'flattened image shape: {img.shape}')\n# # Reshaping the image in order to display it properly again\n# img = img.reshape(50,50,3)\n# io.imshow(img)","55f68ad7":"# print('we reached here!')","20be081b":"# def  paths_to_array(img_paths,flat_img_size = 7500):\n#     # Get the total number of images to be stored in the array\n#     number_of_images = img_paths.shape[0]\n#     # Initialize numpy array \n#     img_array = np.zeros((number_of_images,flat_img_size))\n#     # Use tqdm to keep track of progress\n#     n_errors = 0\n#     for i,img_path in tqdm(enumerate(img_paths),total = number_of_images):\n#         img = io.imread(img_path)\n#         img = img.flatten()\n#         try:\n#             img_array[i,:] = img                                     \n#         except: \n#             n_errors+=1\n#     print(n_errors)\n#     return img_array","7d1c5b4d":"import sys\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https:\/\/stackoverflow.com\/a\/1094933\/1870254, modified'''\n    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num \/= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\nfor name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n                         key= lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))\n","5ba73748":"# Preparation & peek at the data structure\n\n## Loading packages","7b91e48c":"# Checkpoint \n## Saving Outputs\nNow that we have stored our images in array, let's make a checkpoint by saving the array and dataframe. The reason we want to do that, is so that we don't rerun the whole notebook when we want to continue our analysis. This way we can directly start from here. Furthermore, we will be deleting all the  variables that we created in the process to free up some more memory.\n\nWe will do this by pickling the file, in other words serializing our data so that we can read it later. Below are 2 Functions that I created to make the process of saving and loading our variables easier.","ea47c710":"We have 2 subfolders, let's take a peak at what they contain.","320c2a70":"Let's have a look at how many samples we ended up with per class.","ea0153d7":"### Insights\u00b6\n* Sometimes we can find artifacts or incomplete patches, some images are also less than 50 x 50 pxs. \n* Patches with cancer look more violet and crowded than healthy ones. Is this really typical for cancer or is it more typical for ductal cells and tissue?\n* Though some of the healthy patches are very violet colored too!\n* Would be very interesting to hear what criteria are important for a [pathologist](https:\/\/en.wikipedia.org\/wiki\/Pathology).\n","a5a4b722":"As expected, the shape of our dataframe matches our expectations, containing a total of 5 features, and about 280,000 patches.","4aa1c4bd":"### Insights\n1. The number of image patches per patient varies a lot! This leads to the questions whether all images show the same resolution of tissue cells if this varies between patients.\n2. Some patients have more than 80 % patches that show IDC! Consequently the tissue is full of cancer or only a part of the breast was covered by the tissue slice that is focused on the IDC cancer. Does a tissue slice per patient cover the whole region of interest?\n3. The classes of IDC versus no IDC are imbalanced. We have to check this again after setting up a validation strategy and find a strategy to deal with class weights (in case we would like to rebalance the classes).","5fae6499":"This is a forked and simplified version of [my original kernel](https:\/\/www.kaggle.com\/amerii\/breast-cancer-classification-end-to-end). This kernel aims to be more beginner friendly than the previous kernel. \n\nAlso check out [Allunia's Kernel](https:\/\/www.kaggle.com\/allunia\/breastcancer), and [Joni Juvonen's](https:\/\/www.kaggle.com\/qitvision\/a-complete-ml-pipeline-fast-ai) Kernel as they are also great places to start as well.","988cfa25":"### How many patches do we have in total?\n\n#### Which of them are IDC patches and which are Non-IDC?\nIn order to train our model we need to feed our model each patch individually, therefore each patch will act as an input.\n\nThe snippet below loops through the entire file structure and extracts the total number of crops for each of the classes.","f40a9244":"It seems that we have reached the files containing the tissue images of each patient, \nthe structure seems to be as follows:\n\nPatient_id\/xcoordinate_of_patch\/ycoordinate_of_patch\/class_of_cancer","3a16c7d8":"Now that we have set up our data, let's create a visual summary to help us draw some insights from our data.","528127f3":"Each subfolder seems to be the ID of the corresponding patient","76eb9ceb":"## Visualizing the Breast Tissue\nEarlier we extracted the coordinates of the cropped tissue cells, we can use those coordinates to reconstruct the whole breast tissue of the patient. This way we can explore how the diseased tissue looks when compared to the healthy tissue. \n\nWe can also explore the most common places that the cancer tends to occur in. It would be interesting to plot a heatmap of the  most common areas where the cancer appears. \n\nIf position of the crop has significance then perhaps we can use it as an input feature for our model.","3d979ceb":"# Storing the Images\nWe know that each RGB image has a dimension of (50,50,3). One way of storing them is by flattening the image array into an array with a shape of (1,7500). This means that in order to store all the images we will need a numpy array with a total dimension of (280000,7500), this is because we have a total of 280,000 images. The problem with following this method comes to the limited amount of memory that we have when running a notebook. \n\n## So what can we do? \n\n* ### Reduce the number of features that we have by:\n  * #### Grayscaling\n  * #### Averaging the out the pixel values for each image\n  * #### Downscaling our image\n  \n  \n* ### Reduce the number of images that we use to train our model\n\n","705a39c6":"### Average Pixel Values\nWe managed to store the flattened images in the rows of the numpy array above. Next we decide what we want to feed our model as training data. \n\nThe simplest thing to do would be to take the average pixel values of each flattened image. So let's do that below and see if that will aid us in our analysis.","d647d83a":"Above we created a small for loop to shuffle our data randomly 3 times. As you can see our starting values have changed on all 3 cases verifying that our function is working as intended. Next we will determine the percentage split of our cancerous and non cancerous images, using the function we created earlier.","1d24e7cd":"### Model Evaluation","dbc59dc2":"### Binary target visualisation per tissue slice \nBefore we will take a look at the whole tissue let's keep it a bit simpler by looking at the target structure in the x-y-space for a handful of patients:","b139912e":"The function below, takes a list of paths to the desired images. Then reads and reshapes the arrays from (50,50,3) to (2500,3). After reshaping it adds the reshaped array to the total array. The final array will have a total dimension of (2500\\*120000,3) = (300000000,3). PCA is then applied to this array and the principle components and fitted to this object. ","3bb9e318":"### RGB Image -> Grayscale Image","4ab369c8":"Let's ensure that our classes split is maintained after the shuffling and splitting.","03c400c8":"### Splitting Function\nThe splitting function below automates how we split our data. It uses sklearn's train_test_split function, which splits the data to training and testing sets according to the specified percentage. The function below takes it a step further and allows the user to specify the percentage of the initial dataset that you want to use for the split. \n\nThis is useful, in our case, because we have such a large dataset. So when starting out we might want to a smaller part of our dataset and ensure that everything runs correctly first and then we can transition to the larger dataset. This will help us reduce the total computation time required to when manipulating our data, which in turn will also help us reduce the total time needed to test and iterate over our model.","51b848df":"The function below returns the percentage of unique items we have in a class so that we can gain a better understanding of our target variables. In our case, the data will be split between 0 and 1 values. These values represent the following:\n\n* 0 -> Tissue does not contain IDC \n* 1 -> Tissue contains IDC","3e206a6d":"## Healthy Tissue Patches Vs Cancerous Tissue Patches\nLet us now explore the visual differences between cancerous tissue cells, and healthy tissue cells. Usually partnering with a specialist is a good idea so that they can point the exact points of interest that differentiate the 2 from each other.","8baf6460":"### Cancerous Patches","d84457ea":"To simplify things we will create a function that slices our existing dataframe and retrieves the values associated with a patient id.","b4bff40b":"Now that we have coded a proof concept, let's apply this to all the image paths that we have using a loop. We will create a function that does that for us, the function accepts the paths, and the flattened image size and returns the reconstructed flattened array of images that we spoke of earlier.","c8f8de74":"### Insights\n* Sometimes we don't have the full tissue information. It seems that tissue patches have been discarded or lost during preparation.\n* Cancerous Tissue tends to appear in clusters rather than, being dispersed all over the place.\n","5c21158c":"For a quick refresher on the different libraries used to read images and how to use them check out this medium article by [TowardDataScience](https:\/\/towardsdatascience.com\/what-library-can-load-image-in-python-and-what-are-their-difference-d1628c6623ad) ","e0b0d47a":"### Visualizing the average quadrant values of an image","f8a2aa9a":"## What do we know about our data?\nNow that we have a good understanding of the file structure let's try and understand how much data we are about to process.\n\n### How many patients do we have?\nIt seems that we have a total number of 280 patients. This sample size is relatively small therefore we have to be careful not to overfit our model. We need to implement our model in such a way that it maximizes generalization.\n\nEach patient has a batch of patches that were extracted, therefore the total number of patches is likely much greater than 280.","bfa81765":"When observing the data above we notice, that it's probably impossible to split the data and achieve the desired accuracy. We do notice that there tends to be a range that the cancerous images tend to occur in, however this range also happens overlap with non cancerous images as well.\n\n***So what do we do?***\n\nOne thing we can do is introduce more features. \n\n***What features can we possibly introduce?***\n\nLet's carry on with this notebook's theme of simplicity and introduce the average of the quadrants of the image as new features that we can use.","834bdbc3":"### Fitting the model ","d34f180c":"## Data Augmentation\nThere are couple of ways we can use to avoid overfitting; more data, augmentation, regularization and less complex model architectures. Note that if we apply augmentation here, augmentations will also be applied when we are predicting (inference). This is called test time augmentation (TTA) and it can improve our results if we run inference multiple times for each image and average out the predictions. \n\n**The augmentations we can use for this type of data:**\n- random rotation\n- random crop\n- random flip (horizontal and vertical both)\n- random lighting\n- random zoom \n- Gaussian blur \n\n\nAlso possible to make a heat map that tells you the most likely region that Invasive Ductal Carcinoma is likely to occur, and to see whether the coordinates can be used as a feature or not.","23510943":"We add the average pixel values of each flattened image to our original training dataframe as a new feature below.","53e10f9c":"uint8 is used unsigned 8 bit integer. And that is the range of pixel. We can't have pixel value more than 2^8 -1. Therefore, for images uint8 type is used. Whereas double is used to handle very big numbers.","c951b1cd":"Now that we have a function that gets the average of each of the quadrants, let's create a function that determines the averages of all the images given an array of flattened images.","8658bc14":"Great, we managed to reduce the number of dimensions we have. Let's apply this to our training set X_train by creating a function that reads the images as grayscale, then flattens them and stores them in numpy arrays. \n\nWe start by initializing a numpy array filled with zeros as it is more computationally efficient. We then replace each row with the corresponding flattened image.","2bd9bff7":"Let's try and visualize the distribution of the average image pixel value and see whether we can use it to split our data. ","939b6f7c":"-\n--\n\n-\n\n\n-\n\n-\n\n\n-\n\n-\n\n-\n\n-\n\n--\n\n-\n\n-\n\n-\n\n-\n\n-\n\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n\n-\n\n-\n\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n-\n\n\n-","19e8838f":"Next let's concatinate our training dataframe with our newly engineered data. ","307760ab":"### Non-Cancerous Patches","9c5a33a0":"Let's check our image dimensions for both the grayscale and the RGB:","f8131393":"# Pre-processing\nNow that we have a function that allows us to vizualize the where the cancer occurs, we can finally begin pre-processing our data to get it ready to be fed into our classifier.  \n\nThe next steps that we are going to take are shown below:\n\n* Split Data to Training & Testing Data\n* Decrease the number of dimensions for our images\n\nNotice how we split our data into testing and training sets before decreasing our number of dimensions. This is because we want to fit the data to our training data and not our testing data as that would bias our results. One more thing to consider is that when we receive our data to be classified, we are always transforming our data onto our already fitted model. Therefore applying the fit to our testing data, would not reflect what happens when classifying in real life.","8852bd86":"## Storing the image_path, patient_id, target and x & y coordinates\nWe have about 278,000 images. To feed the algorithm with image patches we will store the path of each image and create a dataframe containing all the paths. **This way we can load batches of images one by one without storing the individual pixel values of all images**. \n\nIn addition to extracting the paths, we will also extract the x & y coordinates of each of the images. Then we can use the coordinates to reconstruct the whole breast tissue of a patient. This way we can also explore how diseased tissue looks like compared to healthy ones.\n\nWe know that the path to the images is as follows: <br\/>\n> ..\/input\/breast-histopathology-images\/[patient_id]\/[class]\/[image]\n\nBelow we start by constructing a list of dictionaries and then creating a dataframe based on that list. This is much more computationally efficient than using 'append' or 'loc' methods. For more info check out this [post](https:\/\/stackoverflow.com\/questions\/10715965\/add-one-row-to-pandas-dataframe\/17496530#17496530) on stackoverflow.","1f1927c6":"## Training & Testing Split\n\nThere are a few things that we must consider before splitting our data. The first being the ratio of cancerous to non cancerous images that we have. We determined earlier that our data is not split uniformly. We have many more, non-cancerous images that non-cancerous images, which may possibly bias our model. \n\nThis may also pose a problem when evaluating our model. Let's take an extreme example where we have a dataset consisting of 100 points, 90 of them are cancerous and 10 of them are non-cancerous, let's also assume that our model classified our entire dataset as cancerous, this would give an accuracy of 90%, however this is not really representitive of what is really happening. In reality it is always misclassifying our non-cancerous data.\n\nSo how do we go about solving this?\n\nWe can use the train-test split function offered in sklearn model_selection, this function has the option to split our data such that we get equally distributed training and testing sets when grouped by the classes.\n\nFor more information, check out this [article](https:\/\/towardsdatascience.com\/3-things-you-need-to-know-before-you-train-test-split-869dfabb7e50), that talks about the considerations that need to be taken before splitting your data.","fb67ac49":"### Repatching the Actual Breast Tissue Image\nNow it's time to go one step deeper with our EDA. Instead of plotting the target values using the x-y coordinates, we now plot the images themselves on their respective x-y coordinates. This will help us visualize how the cancerous tissue looks like from a macro perspective.","61fb61b5":"# Exploratory Data Analysis (EDA)\n\n## Let's create a visual summary of our data","c6fdb246":"## Exploring the data structure\nWe start by exploring the file structure to gain an understanding of how everything is organised","d98d4171":"### Shuffling Our Data\nBelow is a function that we will use to shuffle our data randomly while grouping by the patient ID.","7e68cbd6":"#### Adding the new average values to our dataframe","033e850a":"### Setting model parameters","fc1ab184":"# Training Model","67396e71":"Notice how the ratio of IDC patches to non-IDC patches is highly uneven. Here we approximately a 28\/70 split, which could be problematic down the road due to the overrepresentation of non-IDC samples relative to IDC samples. We need to account for it during the validation phase.","a971d3b8":"### Visualizing average quadrant data split","697c73eb":"Each patient directory has 2 sub-folders, labeled 1 and 0 <br\/>\nFolder 0: Non-Invasive Ductal Carcinoma (IDC) <br\/>\nFolder 1 : Invasive Ductal Carcinoma (IDC) <br\/>\n\nLet's check out the folder contents of each directory.","6a4b4829":"### Getting the average pixel values for each quadrant of the image\nBelow we create a simple function that accepts a flattened image and returns the average value for each quadrant within the image. The reason it accepts a flattened image is so that we can make looping through the array of images that we have easier, so that we can get the quadrant averages for all the images that we have.","d68dd008":"Let's now visualize what some of the reconstructed tissue cells look like by using the function above. \n\nNote: The function can be improved by having edges on the mask, rather than overlaying a color on top. This will help us preserve the true image of the tissue cells.","3ea34680":"# Motivation \n\n### My aunt has recently been diagnosed with breast cancer, this kernel is dedicated to her in the hopes that I can learn from this experience and one day build an improved version of this classifier.","2f892028":"## Splitting our Data\nNow that we know the initial, split of the classes, let's split our data into training and testing sets. We usually split our data as follows:\n\n* ***Training Data*** : Data that we will use to train our model\n* ***Testing Data*** : Data that we will use to test our model\n* ***Out of Sample Data*** : Data that we will use to further validate our testing. Usually this is taken before preprocessing the data.\n\nWe will be using sklearn's train_test_split to split our data. \n\nShuffle is set to False, this is because we already manually shuffled our data in the previous cell. \n\nFor simplicity's sake we will not be considering the Out of Sample data, and we will only be splitting our data to training and testing sets.","bd9df7ad":"#### Splitting our Data "}}