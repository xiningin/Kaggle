{"cell_type":{"8cc3be0b":"code","1180517e":"code","0ece99e4":"code","c9dbc4ca":"code","4b535982":"code","8aed310b":"code","296a8594":"code","5f1f878a":"code","615d0eeb":"code","2eb68fc1":"code","ccb46869":"markdown","d4443a72":"markdown","192c432c":"markdown","f55f94a2":"markdown","3279ffd9":"markdown","4a634b40":"markdown"},"source":{"8cc3be0b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport string\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n","1180517e":"# Read data\ndf = pd.read_csv(\n    '\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv',\n    encoding='latin-1',\n    usecols=['v1', 'v2']).rename(\n        columns={'v1':'target', 'v2':'content'})\ndf.head()","0ece99e4":"# Check null values\ndf.isna().sum()","c9dbc4ca":"# Check total spam and ham\ndf.target.value_counts()","4b535982":"# Check content length\ndf['len_content'] = df['content'].apply(lambda x:len(x))\ndf.head()","8aed310b":"df.groupby('target')['len_content'].mean()","296a8594":"plt.figure(figsize=(10, 5))\ndf[df.target=='spam']['len_content'].plot(kind='hist', color='r', label='Spam', alpha=0.6)\ndf[df.target=='ham']['len_content'].plot(kind='hist', bins=35, color='b', label='Ham', alpha=0.5)\nplt.legend(loc='upper right')\nplt.xlabel('Content length')\nplt.show()","5f1f878a":"steps = [\n    ('tfidf', TfidfVectorizer(min_df=4)),\n    ('model', MultinomialNB())]\npipeline = Pipeline(steps)\n\n# Clean content\nstopword = stopwords.words('english') + [\n    'u', '\u00fc', 'ur', '4', '2', 'im', 'dont',\n    'doin', 'ure', 'n', 'e', 'c', 'r', 'v',\n    'k', 'y', 'x']\n\ndef before_text_clean(dataframe):\n    # Train test split\n    y = dataframe['target']\n    X = dataframe['content']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    # Model fit\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    print('Accuracy: {}\\nConfusion matrix:\\n{}'.format(accuracy, cf_matrix))\n\n    \ndef cleaning_text(text):\n    # Remove punctuations\n    text  = \"\".join([char for char in text if char not in string.punctuation])\n        \n    # Tokenize\n    text = word_tokenize(text.lower())\n    \n    # Remove stopwords\n    return \" \".join([char for char in text if char not in stopword])                  \n    \n\ndef after_text_clean(dataframe):\n    # Text clean\n    dataframe['clean_text'] = dataframe['content'].apply(lambda x:cleaning_text(x))\n    \n    # Train test split\n    y = dataframe['target']\n    X = dataframe['clean_text']\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n    \n    # Model fit\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    accuracy = accuracy_score(y_test, y_pred)\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    print('Accuracy: {}\\nConfusion matrix:\\n{}'.format(accuracy, cf_matrix))","615d0eeb":"before_text_clean(df)","2eb68fc1":"after_text_clean(df)","ccb46869":"Text cleaning helped to increase the accuracy slightly. ","d4443a72":"### Part 2: EDA","192c432c":"### Part 1: Read dataset","f55f94a2":"Spam messages have longer content length than ham.\n","3279ffd9":"### Spam Classifier","4a634b40":"### Part 3: Classifier"}}