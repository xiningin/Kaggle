{"cell_type":{"86c904e2":"code","30330060":"code","335bcfd1":"code","e5f7918d":"code","bc6eac22":"code","f80162a3":"code","cc6a2121":"code","be5dc8da":"code","7a545ef8":"code","d48a62d2":"code","d4315110":"code","364dabb5":"code","3264a50e":"code","2ecf48d2":"code","5617a1e7":"markdown","3c0d0aac":"markdown","9acce090":"markdown","8c39a4c5":"markdown","25c62694":"markdown","ac51b789":"markdown","24f72631":"markdown"},"source":{"86c904e2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","30330060":"from pathlib import Path\nROOT_DIR = Path('\/kaggle')\nINPUT_DIR = ROOT_DIR \/ 'input'\nWORK_DIR = ROOT_DIR \/ 'working'\n\ntrain = pd.read_csv(INPUT_DIR \/ 'tensorflow-great-barrier-reef\/train.csv')","335bcfd1":"train['n_bbox'] = train['annotations'].apply(lambda x: len(eval(x)))","e5f7918d":"# number of bounding boxes\ntrain['n_bbox'].sum()","bc6eac22":"# number of annotated frames\nlen(train.query('n_bbox > 0'))","f80162a3":"image_path = INPUT_DIR \/ 'starfish-generate-psgan-dataset\/psgan_datasets\/images\/train\/10.png'","cc6a2121":"fig, ax = plt.subplots()\nimg = plt.imread(image_path)\nax.imshow(img)\nplt.axis('off');","be5dc8da":"import os\n\n\ndef plot_real_and_fake_image(image_id,\n                             root_path='\/kaggle\/input\/starfish-psgan-results\/results\/img_128_v7\/test_latest\/images'):\n    \n    image_paths = [\n        os.path.join(root_path, f'{image_id}_real_B.png'),\n        os.path.join(root_path, f'{image_id}_real_A.png'),\n        os.path.join(root_path, f'{image_id}_fake_B.png'),\n    ]\n    \n    fig, axs = plt.subplots(1, 3, figsize=(9, 3))\n    \n    labels = ['real', 'input', 'synthesized']\n    for ax, image_path, label in zip(axs, image_paths, labels):\n        img = plt.imread(image_path)\n        ax.imshow(img)\n        ax.set_title(label, fontsize=14)\n        ax.axis('off')\n        \n    plt.tight_layout()","7a545ef8":"plot_real_and_fake_image(91)","d48a62d2":"plot_real_and_fake_image(67)","d4315110":"plot_real_and_fake_image(22)","364dabb5":"plot_real_and_fake_image(63)","3264a50e":"plot_real_and_fake_image(92)","2ecf48d2":"plot_real_and_fake_image(15)","5617a1e7":"## Sample of Synthesized Images","3c0d0aac":"I trained the model for about 150 epochs, but the latest model's discriminator seems overfitting because almost perfectly discriminates fake and real. So I used weight of 125 epoch.\n\nThe model sometimes generates COTS-like object, sometimes not. Since I fed only 1200 input data, I think the model are overfitted. It sometimes outputs completely fake-looking images when fed new image which doesn't apper in train dataset.\n\nI doubt this model can generate the data useful for object detection.","9acce090":"## Sample of Poorly Sinthesized Images","8c39a4c5":"# What's the Notebook for?\n\nThis competition has 11,898 bounding boxes and 4,919 annotated. It looks sufficient to train the model, but considering they are sourced from almost identical successive frames, it is doubtful that they are sufficient.\n\nCan we inprove the detection model by adding new synthesized images in GAN? This is the first motivation of this approach.\n\nHere, I used the model Pedestrian-Syntheis-GAN [1], which was used to generate pseudo-images of pedestrians. The model can be trained if we prepare an image in which the BBox and the target are replaced by noise, so I trained it using a partial dataset of the competition (about 1,200 sampled clips of size 256x256).\n\n[1] https:\/\/arxiv.org\/abs\/1804.02047","25c62694":"# Reference\n\nThe original repository [2] has many bugs, so I forked and modified the repository [3]. \n(The modified repository might still contain many bugs.)\n\n* [2] https:\/\/github.com\/yueruchen\/Pedestrian-Synthesis-GAN\n* [3] https:\/\/github.com\/bilzard\/Pedestrian-Synthesis-GAN","ac51b789":"# Input Image for Pedestrian-Synthesis GAN (PS-GAN)","24f72631":"# Synthesized Results\n\nHere, I can show part of the synthesized results."}}