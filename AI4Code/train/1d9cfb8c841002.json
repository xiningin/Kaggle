{"cell_type":{"37cb5d0c":"code","ed8e27d3":"code","f73971e0":"code","f0eccc82":"markdown","8b9ddb07":"markdown"},"source":{"37cb5d0c":"from math import log\n\ndef entropy(*probs):\n  \"\"\"Calculate information entropy\"\"\"\n  try:\n    total = sum(probs)\n    return sum([-p \/ total * log(p \/ total, 2) for p in probs])\n  except:\n    return 0\n\nage = {\n    '<=30': entropy(2, 6),\n    '31-40': entropy(6, 0),\n    '>40': entropy(4, 2),\n}\n\nincome = {\n    'high': entropy(3, 2),\n    'medium': entropy(5, 3),\n    'low': entropy(4, 3),\n}\n\nstudent = {\n    'yes': entropy(8, 1),\n    'no': entropy(4, 7),\n}\n\ncredit_ranking = {\n    'excellent': entropy(5, 5),\n    'fair': entropy(7, 3),\n}\n\n\nentropy_root = entropy(12, 8)\nentropy_age = 8\/20 * age['<=30'] + 6\/20 * age['31-40'] + 6\/20 * age['>40']\nentropy_income = 5\/20 * income['high'] + 8\/20 * income['medium'] + 7\/20 * income['low']\nentropy_student = 9\/20 * student['yes'] + 11\/20 * student['no']\nentropy_credit_ranking = 10\/20 * credit_ranking['excellent'] + 10\/20 * credit_ranking['fair'] \n\nprint('The root entropy is H(T):')\nprint(entropy_root)\nprint('')\nprint('The resulting entropy is H(T, age)')\nprint(entropy_age)\nprint('Thus, information gain if the set is split according to age')\nprint(entropy_root - entropy_age)\nprint('')\nprint('The resulting entropy is H(T, income)')\nprint(entropy_income)\nprint('Thus, information gain if the set is split according to income')\nprint(entropy_root - entropy_income)\nprint('')\nprint('The resulting entropy is H(T, student)')\nprint(entropy_student)\nprint('Thus, information gain if the set is split according to student')\nprint(entropy_root - entropy_student)\nprint('')\nprint('The resulting entropy is H(T, credit_ranking)')\nprint(entropy_credit_ranking)\nprint('Thus, information gain if the set is split according to credit rating')\nprint(entropy_root - entropy_credit_ranking)","ed8e27d3":"from graphviz import Digraph\n\ntree = Digraph()\n\ntree.edge(\"Age\\nsamples = 20\", \"Student\\nsamples = 8\\nvalue = [6, 2]\", \"<=30\")\ntree.edge(\"Student\\nsamples = 8\\nvalue = [6, 2]\", \"Buying\\nsamples = 2\\nvalue = [0, 2]\", \"Yes\")\ntree.edge(\"Student\\nsamples = 8\\nvalue = [6, 2]\", \"Not Buying\\nsamples = 6\\nvalue = [6, 0]\", \"No\")\n\ntree.edge(\"Age\\nsamples = 20\", \"Buying\\nsamples = 6\\nvalue = [0, 6]\", \"31-40\")\n\ntree.edge(\"Age\\nsamples = 20\", \"Student\\nsamples = 6\", \">40\")\ntree.edge(\"Student\\nsamples = 6\", \"Credit Rating\\nsamples = 4\", \"Yes\")\ntree.edge(\"Credit Rating\\nsamples = 4\", \"Buying\\nsamples = 2\\nvalue = [0,2]\", \"Fair\")\ntree.edge(\"Credit Rating\\nsamples = 4\", \"Income\\nsamples = 2\\nvalue = [1, 1]\", \"Excellent\")\ntree.edge(\"Income\\nsamples = 2\\nvalue = [1, 1]\", \"Buying\\nsamples = 1\\nvalue = [0, 1]\", \"Low\")\ntree.edge(\"Income\\nsamples = 2\\nvalue = [1, 1]\", \"Not Buying\\nsamples = 1\\nvalue = [1, 0]\", \"Medium\")\n\ntree.edge(\"Student\\nsamples = 6\", \"Credit Rating\\nsamples = 2\", \"No\")\ntree.edge(\"Credit Rating\\nsamples = 2\", \"Buying\\nsamples = 1\", \"Fair\")\ntree.edge(\"Credit Rating\\nsamples = 2\", \"Not Buying\\nsamples = 1\", \"Excellent\")\n\ntree","f73971e0":"import numpy as np\nimport pandas as pd\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\n\n# translate values into unique integers to handle them\n\nx = {\n    'Age': [\n        0, 0, 1, 2, 2, \n        2, 1, 0, 0, 2,\n        0, 1, 1, 2, 0, \n        0, 0, 1, 2, 1 \n    ], \n    'Income': [\n        4, 4, 4, 5, 3, \n        3, 3, 5, 3, 5, \n        5, 5, 4, 5, 5,\n        3, 3, 3, 5, 4\n    ],\n    'Student': [\n        6, 6, 6, 6, 7, \n        7, 7, 6, 7, 7, \n        7, 6, 7, 6, 6, \n        6, 6, 7, 7, 6 \n    ],\n    'Credit_rating': [\n        9, 8, 9, 9, 9,\n        8, 8, 9, 9, 9,\n        8, 8, 9, 8, 8,\n        9, 8, 9, 8, 8\n    ],\n}\n\ny = {\n    'Buys Computer': [\n        'Buys Not', 'Buys Not', 'Buys', 'Buys', 'Buys', \n        'Buys Not', 'Buys', 'Buys Not', 'Buys', 'Buys', \n        'Buys', 'Buys', 'Buys', 'Buys Not', 'Buys Not', \n        'Buys Not', 'Buys Not', 'Buys', 'Buys', 'Buys' \n    ]\n}\n\nx_df = pd.DataFrame(data=x)\ny_df = pd.DataFrame(data=y)\n\nX = x_df.to_numpy()\ny = y_df.to_numpy()\n\ntree_clf = DecisionTreeClassifier()\ntree_clf.fit(X, y)\n\nfrom sklearn.tree import export_graphviz\n\nx_df = pd.DataFrame(data=x)\ny_df = pd.DataFrame(data=y)\n\nX = x_df.to_numpy()\ny = y_df.to_numpy().astype('U10')\n    \nexport_graphviz(\n     tree_clf,\n     feature_names=list(x_df.columns),\n     class_names=np.unique(y),\n     filled=True\n )","f0eccc82":"### 2. Check with Webgraphviz","8b9ddb07":"## Result DET\n\n### 1. Calculating\n\n| Rec | Age   | Income | Student | Credit_ratin | Buys_computer |\n|-----|-------|--------|---------|--------------|---------------|\n| r1  | <=30  | High   | No      | Fair         | No            |\n| r2  | <=30  | High   | No      | Excellent    | No            |\n| r3  | 31-40 | High   | No      | Fair         | Yes           |\n| r4  | >40   | Medium | No      | Fair         | Yes           |\n| r5  | >40   | Low    | Yes     | Fair         | Yes           |\n| r6  | >40   | Low    | Yes     | Excellent    | No            |\n| r7  | 31-40 | Low    | Yes     | Excellent    | Yes           |\n| r8  | <=30  | Medium | No      | Fair         | No            |\n| r9  | <=30  | Low    | Yes     | Fair         | Yes           |\n| r10 | >40   | Medium | Yes     | Fair         | Yes           |\n| r11 | <=30  | Medium | Yes     | Excellent    | Yes           |\n| r12 | 31-40 | Medium | No      | Excellent    | Yes           |\n| r13 | 31-40 | High   | Yes     | Fair         | Yes           |\n| r14 | >40   | Medium | No      | Excellent    | No            |\n| r15 | <=30  | Medium | No      | Excellent    | No            |\n| r16 | <=30  | Low    | No      | Fair         | No            |\n| r17 | <=30  | Low    | No      | Excellent    | No            |\n| r18 | 31-40 | Low    | Yes     | Fair         | Yes           |\n| r19 | >40   | Medium | Yes     | Excellent    | Yes           |\n| r20 | 31-40 | High   | No      | Excellent    | Yes           |\n\n#### Formula\nInformation entropy\nAlso called Shannon entropy (after the father of intromation theory)\n\nUsually information entropy is denoted as $H$\n\n$H$ is defined as the weighted average of the self-information of all possible outcomes\n\n\n$H(X) = \\sum\\limits_{i=1}^N p_i \\cdot I(p_i) = -\\sum\\limits_{i=1}^N p_i\\cdot\\log(p_i)$\n\nWe can also calulate the entropy after T was partitioned in Ti with respect to some feature X\n\n\n$H(T, X) = \\sum\\limits_{i=1}^N p_i\\cdot H(T_i)$\n\nAnd the information gain is defined as\n\n\n$G(X) = H(T) - H(T, X)$"}}