{"cell_type":{"0ef88986":"code","1ef6c7cf":"code","f60fd5d7":"code","03d1c537":"code","3173bfa7":"code","1a444144":"code","82962e0f":"code","e12371e7":"code","d0b35a42":"code","169875f8":"code","1b67df35":"code","dcc9e7da":"code","343ab061":"code","e8408c56":"code","0199b75f":"code","4c663de7":"code","ebbcbe4a":"code","d1d02b4a":"code","814616f7":"code","3d22f193":"code","9cb0968b":"code","5de6e987":"code","89e3d3fc":"code","8c0c43ef":"code","f7ee428e":"code","cbd6b22a":"code","0e9c7a25":"code","2d5d9e21":"code","0c23a72a":"code","6e15fb31":"code","a031bd89":"code","4bd15356":"code","f2e7b817":"code","ab8cb685":"code","ca133547":"code","8fb81877":"code","efcf4b48":"code","d1b3fc5b":"code","fe3feadd":"code","c3822255":"code","e4224967":"code","accef76f":"code","d8cb3a6e":"code","fbb4aa2c":"code","f261de54":"code","83c5b4dc":"code","854fa96a":"code","3a6b9768":"code","a65537ca":"code","4033f29a":"code","b85f8914":"code","9fd677eb":"code","dfce4d1b":"code","e817bea5":"code","3bc03750":"code","3158bba8":"code","9c704c3d":"code","fb8fe8c7":"code","c962651d":"code","ac5c651b":"code","dc811ec9":"code","51a96a07":"code","3e2ef2c4":"code","5a8fb8a1":"code","bfc23eef":"code","21bdc6df":"code","d68e9948":"code","690a3cf6":"code","a348155e":"code","b275c14a":"code","750ca4b6":"code","eb4aacef":"code","1678a1d2":"code","10a2d0c2":"markdown","f5883b1c":"markdown","d2da4f4f":"markdown","2a664504":"markdown","e20b81b1":"markdown","d2a69183":"markdown","93499256":"markdown","92b1e4fd":"markdown","6f6e71c0":"markdown","3f1b5b19":"markdown","9a92eea3":"markdown","77c48c14":"markdown","5426e738":"markdown","b70986d3":"markdown","ca87be88":"markdown","eaea7b86":"markdown","27098ddb":"markdown","e3619e4a":"markdown","93b5831c":"markdown","4e20c4b8":"markdown","8fc75d19":"markdown","f358fdc2":"markdown","dc6d2034":"markdown","ebe989b5":"markdown","1b3adfe7":"markdown","e81668eb":"markdown","d21eb485":"markdown","ca645712":"markdown","23d7b06f":"markdown","4c8d36e3":"markdown","f8965202":"markdown","b3b4e2af":"markdown","605ceb07":"markdown","04b42594":"markdown","40bb9248":"markdown","8849e4cf":"markdown","8ec5b429":"markdown","ebda8319":"markdown","9ef34b85":"markdown","591b870b":"markdown","6bf80794":"markdown"},"source":{"0ef88986":"import numpy as np\nimport pandas as pd\nfrom matplotlib import image\nimport matplotlib.pyplot as plt\nimport re\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom scipy import stats\nimport cv2\nfrom keras.utils import Sequence","1ef6c7cf":"#load and display the 1st 10 rows of the train data\ndf = pd.read_csv(\"..\/input\/training\/input.csv\", index_col=0)\ndf.head(5)","f60fd5d7":"# dataset dimension\nn_rows = df.shape[0]\nn_cols = df.shape[1]\n\nprint(\"\\nNumber of rows - \", n_rows, \"\\nNumber of columns - \", n_cols)\n\n# dataset datatypes - #int cols, #string cols\n# would be useful during one-hot encoding\nn_stringTypes = 0 \n\nfor i in df.iloc[0]:\n    if type(i)==str:\n        n_stringTypes+=1\nprint(\"Number of string type columns - \", n_stringTypes) ","03d1c537":"filepath = '..\/input\/petfinder-adoption-prediction\/PetFinder-BreedLabels.csv'\ndf_breedLabels = pd.read_csv(filepath)\ndf_breedLabels.head(5)","3173bfa7":"# dataset dim\nn_df_breedRows = df_breedLabels.shape[0]\nn_df_breedCols = df_breedLabels.shape[1]\nprint(\"Dataset shape - \", df_breedLabels.shape)\n# different number of breeds for dog [type=1] and cat [type=2]\n\ngrp1, grp2 = df_breedLabels.groupby('Type').apply(lambda ser: ser['BreedName'].unique())\nprint(\"\\nDog Breeds - \", grp1[:10], \"....\",\"\\nNumber of Dog Breeds - \", len(grp1))\nprint(\"\\nCat Breeds - \", grp2[:10], \"....\",\"\\nNumber of Cat Breeds - \", len(grp2))","1a444144":"filepath = '..\/input\/petfinder-adoption-prediction\/PetFinder-ColorLabels.csv'\ndf_ColorLabels = pd.read_csv(filepath)\nprint(df_ColorLabels)\nprint(\"Shape - \", df_ColorLabels.shape)\nprint(\"Number of unique color for pet dataset - \", len(df_ColorLabels))","82962e0f":"filepath = '..\/input\/petfinder-adoption-prediction\/PetFinder-StateLabels.csv'\ndf_StateLabels = pd.read_csv(filepath)\nprint(df_StateLabels)\nprint(\"Shape - \", df_StateLabels.shape)\nprint(\"Number of unique states for pet dataset - \", len(df_StateLabels))","e12371e7":"# Loading pet images corresponding to top 3 rows of the dataframe. This is just for visualization\n# Not loading all images at once, as it would take up a lot of memory\ndef plot_image(filename):\n    data = image.imread(filename)\n    print(\"\\nData type - \",data.dtype, \"\\nData shape - \", data.shape)\n    # display image\n    plt.imshow(data)\n    plt.show()\n    \nimg_path = \"..\/input\/petfinder-adoption-prediction\/train_images\/\"\ndff = df.head(2)\nfor i in range(0,len(dff)):\n    print(\"\\nName - \", dff.loc[i,'Name'], \"\\nPetID - \",dff.loc[i,'PetID'])\n    plot_image(img_path+dff.loc[i,'PetID']+'-'+'1.jpg')\n    ","d0b35a42":"df.loc[:,'PetID'].duplicated().any() #if False, no duplicate values present","169875f8":"def checkIfNull():\n    for i in df.columns:\n        if df[i].isnull().any():\n            print('Column','\"',i,'\"',' has missing values')\n        else:\n            continue\ncheckIfNull()","1b67df35":"#Let's print the column Names\nnames = df.loc[:,'Name']\nnames[:28]\n# The NaN values are visible","dcc9e7da":"df.loc[:,'Name'] = df.loc[:,'Name'].fillna('Unknown')\nfor i,name in enumerate(names):\n    if re.search(\"Name\", name):\n        df.loc[:,'Name'][i] = \"Unknown\"\nprint(df.loc[:,'Name'][:15])\nprint(df.loc[:,'Name'].isnull().any())","343ab061":"dtypes = {}\nfor i,k in enumerate(df.iloc[0]):\n    dtypes[df.columns[i]] = type(k)\nprint(dtypes)\n# from the output it can be stated that the data types of each col match their content","e8408c56":"#Set Membership \n\n# Gender\nprint('Gender - ', df.loc[:,'Gender'].isin([1,2,3]).all())\n#Fur length\nprint('Fur length - ', df.loc[:,'FurLength'].isin([0,1,2,3]).all())\n# MaturitySize\nprint('Maturity - ', df.loc[:,'MaturitySize'].isin([0,1,2,4,3]).all())\n# Vaccinated\nprint('Vaccinated - ', df.loc[:,'Vaccinated'].isin([1,2,3]).all())\n# Dewormed\nprint('Dewormed - ', df.loc[:,'Dewormed'].isin([1,2,3]).all())\n# Sterilized\nprint('Sterilized - ', df.loc[:,'Sterilized'].isin([1,2,3]).all())\n# Health\nprint('Health - ', df.loc[:,'Health'].isin([0,1,2,3]).all())","0199b75f":"# Foreign Key membership - Breed, color, state\n\n#breed\nprint('Breed 1 - ', df.loc[:,'Breed1'].isin(df_breedLabels.loc[:,'BreedID']).all())\nprint('Breed 2 - ', df.loc[:,'Breed2'].isin(df_breedLabels.loc[:,'BreedID']).all())\n\n#color\nprint('Color 1 - ', df.loc[:,'Color1'].isin(df_ColorLabels.loc[:,'ColorID']).all())\nprint('Color 2 - ', df.loc[:,'Color2'].isin(df_ColorLabels.loc[:,'ColorID']).all())\nprint('Color 3 - ', df.loc[:,'Color3'].isin(df_ColorLabels.loc[:,'ColorID']).all())\n\n#State\nprint('State - ', df.loc[:,'State'].isin(df_StateLabels.loc[:,'StateID']).all())","4c663de7":"#Cols - Breed1, Breed2, Color2, Color3 needs to be checked\ndef getIndices(col,colname):\n    indices = col[col==False].index[:]\n    ls = [df.loc[i,colname] for i in indices]\n    return ls, indices\n        \nbreed1 = df.loc[:,'Breed1'].isin(df_breedLabels.loc[:,'BreedID'])\nbreed2 = df.loc[:,'Breed2'].isin(df_breedLabels.loc[:,'BreedID'])\ncolor2 = df.loc[:,'Color2'].isin(df_ColorLabels.loc[:,'ColorID'])\ncolor3 = df.loc[:,'Color3'].isin(df_ColorLabels.loc[:,'ColorID'])\n\nb1,ind = getIndices(breed1, 'Breed1')\nb2,ind2 = getIndices(breed2, 'Breed2')\nc2,ind3 = getIndices(color2, 'Color2')\nc3,ind4 = getIndices(color3, 'Color3')\nprint('\\nb1 values (anomaly) -', b1,' at indices', ind,'\\nb2 no of anomalous values - ',len(ind2))\nprint('c2 no of anomalous values - ', len(ind3), '\\nc3 no of anomalous values - ', len(ind4))","ebbcbe4a":"fig, axes = plt.subplots(1,2, figsize=(14,5))\naxes[0].hist(df.loc[:,'Breed2'], color='blue')\naxes[0].set_xlabel('Breed2 values')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('No of pets per Breed2')\n\naxes[1].hist(df.loc[:,'Color3'], bins = df_ColorLabels.shape[0],histtype='barstacked')\naxes[1].set_xlabel('Color3 values')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('No of pets per Color3')\nplt.show()","d1d02b4a":"# Dropping the Breed2 and Color3 columns\ndf = df.drop(['Breed2', 'Color3'], axis=1)\ndf.head(5)","814616f7":"fig, axes = plt.subplots(1,3, figsize=(17,4))\n\naxes[0].hist(df.loc[:,'FurLength'], bins = range(0,6,1))\naxes[0].set_xlabel('FurLength values')\naxes[0].set_ylabel('Frequency')\naxes[0].set_title('# of Pets per Fur length')\n\naxes[1].hist(df.loc[:,'MaturitySize'], bins = range(0,6,1),color='g')\naxes[1].set_xlabel('MaturitySize values')\naxes[1].set_ylabel('Frequency')\naxes[1].set_title('# of Pets vs MaturitySize')\n\naxes[2].hist(df.loc[:,'Health'], bins = range(0,5,1),color='y')\naxes[2].set_xlabel('Health values')\naxes[2].set_ylabel('Frequency')\naxes[2].set_title('# of Pets vs Health Values')\n\nplt.show()","3d22f193":"# non-categorical attributes - Age, Quantity,Fee,VideoAmt,PhotoAmt. Calculating range of values\n\nage_min = df.loc[:,'Age'].min() #age is in months\nage_max = df.loc[:,'Age'].max()\nprint(\"Pet ages range - \", age_max-age_min, \"(max -\", age_max,\",min - \", age_min,\")\")\nprint(\"Average age of pets - \", df.loc[:,'Age'].mean())\n\nquantity_min = df.loc[:,'Quantity'].min()\nquantity_max = df.loc[:,'Quantity'].max()\nprint(\"\\nMin and Max no. pets in a profile - \", quantity_min,\",\",quantity_max)\n\nfee_min = df.loc[:,'Fee'].min()\nfee_max = df.loc[:,'Fee'].max()\nprint(\"\\nMin and Max fee - \", fee_min,\",\",fee_max)\nprint(\"Average adoption fee of pets - \", df.loc[:,'Fee'].mean())\n\nvideo_avg = df.loc[:,'VideoAmt'].mean()\nprint(\"\\nMean no of videos uploaded for each pet - \", video_avg)\n\nphoto_avg = df.loc[:,'PhotoAmt'].mean()\nprint(\"Mean no of photos uploaded for each pet - \", photo_avg)","9cb0968b":"#AdoptionSpeed vs Distribution of VideoAmt, PhotoAmt\ndef adoptionSpeedDistribution():\n    adoption0 = np.where(df.loc[:,'AdoptionSpeed']==0)\n    adoption1 = np.where(df.loc[:,'AdoptionSpeed']==1)\n    adoption2 = np.where(df.loc[:,'AdoptionSpeed']==2)\n    adoption3 = np.where(df.loc[:,'AdoptionSpeed']==3)\n    adoption4 = np.where(df.loc[:,'AdoptionSpeed']==4)\n    adoption_ = [adoption0, adoption1, adoption2, adoption3, adoption4]\n    return adoption_  ","5de6e987":"adoption_ = adoptionSpeedDistribution()\nn_pets = []\nvideo_amt = []\nfor i in range(5):\n    n_pets += [i]* len(adoption_[i][0])\n    video_amt += [i]* df.loc[adoption_[i][0],'VideoAmt'].sum()\n\nfig, axes = plt.subplots(1,2, figsize=(14,5))\n\n#Mean is not plotted for videoAmt, instead the sum is plotted, since the mean is 0 at every adoptionSpeed \n#target. Plotting the sum (which too is less as can be seen in the graph) clearly shows that VideoAmt has \n# 0 or very less impact on the target\n\naxes[0].hist([n_pets,video_amt],bins = range(0,6,1), color = ['r','y'], label=['pets', 'videos'])\naxes[1].plot(range(0,101),df.loc[:100, 'VideoAmt'], label=['VideoAmt'])\naxes[1].plot(range(0,101),df.loc[:100, 'AdoptionSpeed'], label=['AdoptionSpeed'])\n\naxes[0].set_xlabel('Adoption Speed - 0,1,2,3,4 days')\naxes[0].set_ylabel('Number of pets, videos')\n\naxes[1].set_xlabel('Pet# (Only till 100)')\naxes[1].set_ylabel('Number of videos')\naxes[1].legend()\naxes[0].legend()\naxes[0].set_title('AdoptionSpeed Distribution wrt VideoAmt_sum')\naxes[1].set_title('Distribution VideoAmt')\nplt.show()","89e3d3fc":"adoption_p = adoptionSpeedDistribution()\nn_pets_ = []\nphoto_amt = []\nfor i in range(5):\n    n_pets_ += [i]* int(len(adoption_p[i][0])\/100)\n    photo_amt += [i]* int(df.loc[adoption_p[i][0],'PhotoAmt'].astype('int64').mean())\n\nfig, axes = plt.subplots(1,2, figsize=(14,5))\n\naxes[0].hist([n_pets_,photo_amt],bins = range(0,6,1), color = ['r','y'], label=['pets', 'photos'])\naxes[1].plot(range(0,101),df.loc[:100, 'PhotoAmt'], label=['PhotoAmt'])\naxes[1].plot(range(0,101),df.loc[:100, 'AdoptionSpeed'], label=['AdoptionSpeed'])\n\naxes[0].set_xlabel('Adoption Speed - 0,1,2,3,4 days')\naxes[0].set_ylabel('Number of pets (1\/100), photos')\n\naxes[1].set_xlabel('Pet# (Only till 100)')\naxes[1].set_ylabel('Number of photos')\naxes[1].legend()\naxes[0].legend()\naxes[0].set_title('AdoptionSpeed Distribution wrt PhotoAmt_mean')\naxes[1].set_title('Distribution PhotoAmt')\nplt.show()","8c0c43ef":"df = df.drop(['VideoAmt'], axis=1)\ndf.shape","f7ee428e":"# box-plot for Age and adoption price - to check for outliers\nfig, axes = plt.subplots(2,2,figsize=(14,6), sharex=True)\n\nsns.set(style=\"whitegrid\")\nsns.boxplot(x=df.loc[:,'Age'], ax = axes[0,0])\naxes[0,0].set_title('Age Box-plot')\n\nsns.boxplot(x=df.loc[:,'Fee'], ax = axes[0,1])\naxes[0,1].set_title('Adoption Fee Box-plot')\n\naxes[1,0].scatter(range(0,14993), df.loc[:,'Age'])\naxes[1,0].set_xlabel('Sample#')\naxes[1,0].set_ylabel('Age')\naxes[1,0].set_title('Pet Ages vs sample#')\n\naxes[1,1].scatter(range(0,14993), df.loc[:,'Fee'])\naxes[1,1].set_xlabel('Sample#')\naxes[1,1].set_ylabel('Adoption Fee')\naxes[1,1].set_title('Adoption fee vs sample#')\nplt.setp(axes, yticks=[])\nplt.tight_layout()\n","cbd6b22a":"# Checking adoption speed of cats vs dogs\n\nadoption_ = adoptionSpeedDistribution()\ncats = []\ndogs = []\nfor i in range(len(adoption_)):\n    cats += [i]*(np.where(df.loc[adoption_[i][0],'Type']==2)[0].shape[0])\n    dogs += [i]*(np.where(df.loc[adoption_[i][0],'Type']==1)[0].shape[0])\n\nfig, axes = plt.subplots()\nplt.hist([cats,dogs],bins = range(0,6,1), color = ['b','g'], label=['cats', 'dogs'])\nplt.xlabel('Adoption Speed - 0,1,2,3,4 days')\nplt.ylabel('Number of pets')\nplt.legend()\nplt.title('Cats and Dogs Adoption Speed')\nplt.show()","0e9c7a25":"# Label Encoding 'Name', 'RescuerID', 'State'\ndef labelEncode(attr):\n    enc = LabelEncoder()\n    attr_ = list(attr)\n    enc.fit(attr_)\n    return enc.transform(attr_)\n\n#Not encoding PetID, since it represents the image, which would be seperated out as a different dataset\ndf.loc[:,'Name'] = labelEncode(df.loc[:,'Name'])\ndf.loc[:,'RescuerID'] = labelEncode(df.loc[:,'RescuerID'])\ndf.loc[:,'State'] = labelEncode(df.loc[:,'State'].astype(str))\ndf.head(5)","2d5d9e21":"df.loc[:,'State'].max() #number of unique state values","0c23a72a":"# Correlation matrix of the dataset\n\ncorr = df.corr(method='pearson')\nadoptionSpeed = corr['AdoptionSpeed'][:-1]\n#correlation of attributes with adoptionspeed\nadoptionSpeed","6e15fb31":"x = np.where(adoptionSpeed<0)\ncol = adoptionSpeed[x[0]].idxmax() #column to delete\ncol","a031bd89":"df = df.drop(['RescuerID'],axis=1)\ndf.shape","4bd15356":"#correlation matrix\ncorr","f2e7b817":"def combineFeatures(dff, col1, col2, col3):\n    x,y,z = df.loc[:,col1], df.loc[:,col2], df.loc[:,col3] \n    a = np.hstack((np.array(x).reshape(-1,1), np.array(y).reshape(-1,1)))\n    a = np.hstack((np.array(a), np.array(z).reshape(-1,1)))\n    col_new = [int(stats.mode(a[i])[0]) for i in range(a.shape[0])]\n    print('Original dataframe - ', dff.shape)\n    dff = df.drop([col1, col2, col3],axis=1)\n    # Let the new column name be Vaccinated\n    dff['Vaccinated'] = col_new\n    print('New Dataframe - ', dff.shape)\n    return dff\n\ndf = combineFeatures(df, 'Vaccinated', 'Dewormed', 'Sterilized')","ab8cb685":"plt.hist(df.loc[:,'Fee'])","ca133547":"df = df.drop(['Fee'], axis=1)\nprint('Current Dataframe - ', df.shape)","8fb81877":"from skimage.color import gray2rgb\nfrom skimage.transform import resize\n\nmean = [0.485,0.456,0.406] # standard values, based on ImageNet data\nstd = [0.229,0.224,0.225] # standard values, based on ImageNet data\n\ndef read_image(path):\n    \"\"\"\n    resizing image into size 224x224x3 to feed into ResNet50\n    \"\"\"\n    default_path = '..\/input\/petfinder-adoption-prediction\/train_images\/86e1089a3-1.jpg'\n    try:\n        img = image.imread(path)\n        img = img\/255.0\n        \n    except FileNotFoundError:\n        img = image.imread(default_path) #read this default img (randomly selected to fill missing data)\n        img = img\/255.0\n    \n    return gray2rgb(resize(img, (160,160)))\n\ndef normalize_image(img):\n    \n    img[:,:,0] -= mean[0]\n    img[:,:,0] \/= std[0]\n        \n    img[:,:,1] -= mean[1]\n    img[:,:,1] \/= std[1]\n        \n    img[:,:,2] -= mean[2]\n    img[:,:,2] \/= std[2]\n        \n    return img","efcf4b48":"from sklearn.model_selection import train_test_split\n\n# seperate the target and features\ntarget = df.loc[:, 'AdoptionSpeed'].to_numpy()\nfeatures = df.drop(['AdoptionSpeed'], axis=1).to_numpy()\n\ntrain_x, val_x, train_y, val_y = train_test_split(features, target, test_size=0.10, random_state=42)","d1b3fc5b":"# remove the image col from the numpy arrays (index 12 in a row)\ntrain_images = train_x[:, 12]\nval_images = val_x[:,12]\ntrain_x = np.delete(train_x, 12, 1)\nval_x = np.delete(val_x, 12, 1)","fe3feadd":"# One Hot Encoding the labels\nenc = OneHotEncoder(handle_unknown='ignore', sparse=False)\ny_train = enc.fit_transform(train_y.reshape(-1,1))\ny_val = enc.fit_transform(val_y.reshape(-1,1))","c3822255":"print('Train X - ', train_x.shape, '\\nVal X - ', val_x.shape, '\\nTrain Y - ', y_train.shape, '\\nVal Y - '\n     , y_val.shape, '\\nTrain images - ', train_images.shape, '\\nVal images - ', val_images.shape)","e4224967":"# get the output of a neural network layer. Would be useful for data generator\ndef get_layer_output(model, data, layer = 'dense_4'):\n    \n    network_output = model.get_layer(layer).output\n    feature_extraction_model = Model(model.input, network_output)\n    prediction = feature_extraction_model.predict(np.asarray(data).astype(np.float32))\n    #print(type(prediction))\n    #print(prediction.shape)\n    return np.asarray(prediction).astype(np.float32)","accef76f":"# Generator to read and preprocess data in batches for model training\nclass DataGenerator(Sequence) :\n     \n    def __init__(self, trainX, train_imgs, y_train, model, densenet, batch_size) :\n        self.trainx = trainX\n        self.imgs = train_imgs\n        self.labels = y_train\n        self.model_1 = model\n        self.densenet = densenet\n        self.batch_size = batch_size\n    \n    def __len__(self) :\n        return (np.ceil(len(self.imgs) \/ float(self.batch_size))).astype(np.int)\n  \n    def __getitem__(self, idx) :\n        batch_x_imgs = self.imgs[idx * self.batch_size : (idx+1) * self.batch_size]\n        batch_x = self.trainx[idx * self.batch_size : (idx+1) * self.batch_size]\n        batch_y = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n        \n        NN_1_data = get_layer_output(self.model_1, batch_x)\n        images = []\n        for file in batch_x_imgs:\n            images.append(normalize_image(read_image(img_path+file+'-1.jpg')))\n        \n        DenseNet_data = get_layer_output(self.densenet, np.array(images), layer='avg_pool')\n        \n        return np.concatenate((NN_1_data, DenseNet_data), axis=1), np.array(batch_y)","d8cb3a6e":"def plot_history(history):\n    # summarize history for accuracy\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()\n    # summarize history for loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'val'], loc='upper left')\n    plt.show()","fbb4aa2c":"from keras.models import Model, Sequential\nfrom keras.layers import Flatten, Dense, Activation, Input, Dropout, Activation, BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.optimizers import Adam, SGD\nfrom keras.applications import DenseNet169\nfrom keras.models import load_model","f261de54":"# NN-1 model\ninputs = Input(shape=(14,))\nx = Dense(32, activation='sigmoid')(inputs)\n#x = Dropout(0.2)(x)\nx = Dense(64, activation='sigmoid')(x)\n#x = Dense(128)(x) ## Deeper networks degrade the model. Doesn't fit the data well\n#x = Dense(256, activation='tanh')(x)\n#x = Dropout(0.25)(x)\n#x = Dense(512, activation='sigmoid')(x)\n#x = Dense(20, activation='tanh')(x)\n#x = Dropout(0.25)(x)\nout = Dense(5, activation='softmax')(x)\n\nmodel = Model(inputs=inputs, outputs=out)\nmodel.summary()","83c5b4dc":"# NN-1 model\nlr = 0.001\ncheckpt = ModelCheckpoint(filepath='..\/input\/output\/models\/best_model.h5',monitor='val_acc',save_best_only=True)\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=1, mode='min')\nmodel.compile(optimizer=Adam(lr=lr),loss='categorical_crossentropy',metrics=['accuracy'])","854fa96a":"# EXPERIMENTAL\n# scaling down the name column of training data\ndef scaleData(X):\n    X_std = (X - X.min(axis=0)) \/ (X.max(axis=0) - X.min(axis=0))\n    return X_std * (10)\ntrain_x[:,3] = scaleData(train_x[:, 3])\ntrain_x[:,1] = scaleData(train_x[:, 1])\n\n# After scaling, all values remain same, except training accuracy increases by 8%","3a6b9768":"# NN-1 model\nhistory = model.fit(np.asarray(train_x).astype(np.float32),np.asarray(y_train),epochs=200,batch_size=32,shuffle=True,\n          validation_data=(np.asarray(val_x).astype(np.float32),np.asarray(y_val)),callbacks=[checkpt])","a65537ca":"plot_history(history) #an averaged out curve might look less noisy","4033f29a":"model.evaluate(np.array(val_x).astype(np.float32), np.array(y_val), batch_size=32)","b85f8914":"model.save('best_model_latest.h5')\n#model = load_model('..\/input\/models-files\/best_model_latest.h5')","9fd677eb":"# NN-2 model\nmodel_densenet = DenseNet169(include_top=True, weights=\"imagenet\")\nmodel_densenet.summary()","dfce4d1b":"# NN-2 model\nbatch_size = 64\n\ntrain_gen = DataGenerator(train_x, train_images, y_train, model, model_densenet, batch_size)\nval_gen = DataGenerator(val_x, val_images, y_val, model, model_densenet,batch_size)","e817bea5":"# NN-2 model\n# Model architecture - input : (1728,)\ninputs = Input(shape=(1728,))\n\nx = Dense(1024, activation='sigmoid')(inputs)\nx = Dropout(0.2)(x)\n\nx = Dense(512, activation='sigmoid')(x)\n\nx = Dense(256, activation='tanh')(x)\nx = Dropout(0.25)(x)\n\nx = Dense(256, activation='sigmoid')(x)\n\nx = Dense(64, activation='tanh')(x)\nx = Dropout(0.25)(x)\n\nout = Dense(5, activation='softmax')(x)\n\nmodel_2 = Model(inputs=inputs, outputs=out)\nmodel_2.summary()","3bc03750":"# NN-2 model\nlr = 0.001\ncheckpt = ModelCheckpoint(filepath='..\/input\/output\/models\/best_model_Part2.h5',monitor='val_acc',save_best_only=True)\nearlystop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=10, verbose=1, mode='min')\nmodel_2.compile(optimizer=Adam(lr=lr),loss='categorical_crossentropy',metrics=['accuracy'])","3158bba8":"# NN-2 model\nhistory = model_2.fit_generator(generator=train_gen,steps_per_epoch = int(13493 \/\/ batch_size),epochs = 20,\n                   verbose = 1,validation_data = val_gen,validation_steps = int(1500 \/\/ batch_size))","9c704c3d":"model_2.save('final_model_latest.h5')","fb8fe8c7":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import multilabel_confusion_matrix","c962651d":"# load the trained model\nmodel_2 = load_model('final_model_latest.h5')","ac5c651b":"# evaluate model\nmodel_2.evaluate_generator(val_gen, 1500)","dc811ec9":"y_pred = model_2.predict_generator(val_gen, 1500)\ny_pred.shape","51a96a07":"# Converting predictions into suitable format to feed into sklearn libraries\ndef process_predictions():\n    prediction = []\n    for i in range(len(y_pred)):\n        t = np.zeros(5)\n        t[np.argmax(y_pred[i])] = 1\n        prediction.append(t)\n    return np.array(prediction)\n\nprediction = process_predictions()","3e2ef2c4":"# Classification Report\nprint(classification_report(y_val, prediction))","5a8fb8a1":"# Confusion Matrix\nprint(multilabel_confusion_matrix(y_val,prediction,labels=[0,1,2,3,4]))","bfc23eef":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier","21bdc6df":"clf1 = AdaBoostClassifier(n_estimators=100, random_state=0)\nclf2 = GradientBoostingClassifier(min_samples_split = 8, random_state=0)","d68e9948":"# transform One hot encoded data to labelled data (as in labelEncoded data)\ndef decode(y):\n    ys = []\n    for i in range(len(y)):\n        ys.append(np.argmax(y[i]))\n    return np.array(ys)\ny_train = decode(y_train)\ny_val = decode(y_val)\ny_pred = decode(prediction) # NN model predictions","690a3cf6":"clf1.fit(train_x, y_train) # fit the classifier","a348155e":"clf2.fit(train_x, y_train) # fit the classifier","b275c14a":"pred1 = clf1.predict(val_x) # AdaBoost predictions","750ca4b6":"pred2 = clf2.predict(val_x) # GB predictions","eb4aacef":"#combining both models' predictions\ndef combinePreds(pred, y_pred):\n    pred_combined = []\n    for i in range(len(pred)):\n        if pred[i]==y_pred[i]:\n            pred_combined.append(pred[i])\n        else:\n            pred_combined.append(int((pred[i]+y_pred[i])\/\/2))\n    \n    return pred_combined\n\npred_combined1 = combinePreds(pred1, y_pred)\npred_combined2 = combinePreds(pred2, y_pred)","1678a1d2":"# GradientBoosting Classifier classification report \nprint(classification_report(y_val, pred_combined2))","10a2d0c2":"The tabular data (training) is loaded in a dataframe above. Now, let's load some image data corresponding to the tabular data. Each row in the dataframe has a PetID value. The images are stored with PetID as the filename - format: <PetID>-<Integer> . The integer represent which #image is of that PetID. Default image of the pet is the one labelled 1. ","f5883b1c":"#### FEATURE SELECTION - IMPACT OF AN ATTRIBUTE IN DETERMINING THE TARGET","d2da4f4f":"#### As per the Classification Report, the trained model shows : <br>\n\n1. Accuracy : 67% <br>\n2. Recall : 67% <br>\n3. F1-Score : 67% <br>\n\nConsidering only the sample averages. Class wise, the model has the highest accuracy, recall and F1-score for class 'Adoption Speed 4' as can be seen from the report. \n\nGiven the complexity of the data, the performance of the model seems to be decent enough (especially with limited resources to experiment). The performance\/scores can be increased even more, with better architecture, preprocessing and tuning","2a664504":"### 1.4 State Labels [Part of Training Data]\nContains StateName for each StateID","e20b81b1":"#### CHECK FOR MISSING VALUES AND DUPLICATES\nFor duplicates, we don't need to search every column. Need to check only those columns which shouldn't semantically contain duplicate value, i.e. PetID","d2a69183":"#### NEURAL NETWORK MODEL 1 (PET ATTRIBUTES DATA)","93499256":"Scaling the values of 'Fee' to range [0-10]. But before that, let's visualize the distribution of 'Fee' values.","92b1e4fd":"From the correlation matrix above, it can be seen that 3 features - vaccinated, dewormed, sterilized are highly correlated (Through manual observation). Thus, instead of keeping all the 3 features, can simply keep 1 feature, that is a representative of all the 3. Although PCA is ideal for feature extraction tasks, here it is done manually, since number of features are very less (PCA would be more apt with higher dimensional data - say 500) ","6f6e71c0":"### 1.3 Color Labels [Part of training data] \n Contains ColorName for each ColorID","3f1b5b19":"The values corresponding to Name, RescuerID, Breed1 attributes are quite large. Feeding these values would result in an uneven model training. Need to check out their distribution, if some values can be removed. \n\nHowever, before scaling the values of these columns, let's first check whether these attributes are really important for training the model. ","9a92eea3":"From above plots, it is clear that most of the values in Breed2 and Color3 are 0, i.e. unknown, and as such do not provide any significant information in the prediction of the adoption speed. \nThus, manually these features can be removed","77c48c14":"#### NEURAL NETWORK MODEL 2 (COMBINES ATTRIBUTE DATA FEATURES AND IMAGE FEATURES)","5426e738":"Let's check how data is distributed in other categorical columns, especially those having more than 3 unique values (assumed - data would be approx uniformly distributed when no of bins are less)","b70986d3":"# 4. Data Preparation\n\nPreparing the data in the right format to feed into the neural network model. Utility functions to help during training, plotting,..","ca87be88":"#### TRYING OUT A DIFFERENT TECHNIQUE\n\nTraining different ML classifiers like Adaboost, GB classifiers and combining the predictions with the predictions of NN model. Only the ordered data has been fed into these classifier during training\n\nHowever performances are nearly same, in fact, combining worsens the performance. This is just an experimental technique, and isn't fully correct, as combining 2 different predictions (from 2 different models) just by averaging isn't semantically correct. A better and more robust model\/architecture would be to train only a ML classifier(like DT) on the ordered data, and a neural network on the image data. The labels in both the cases would be same. Although, combining the predictions,in this case, still seems to be an issue, but a suitable classifier, (i.e. according to the type of training data) would be trained for ordered and the unordered data","eaea7b86":"As can be seen from the plot above, >14k samples have a value of 0. As such this attribute won't contribute much to training, instead might make it worse. It is similar to having a sparse vector as training data. \nGiven, the distribution there is no point in scaling the values and keeping it for training (scaling to a smaller range, say 1-10, would only make most of the non-zero values close to 0). Hence, it is better to remove this attribute although semantically it might seem to be an important attribute","27098ddb":"### 1.5 Loading Pet Image Data","e3619e4a":"#### DATA DISTRIBUTION IN COLUMNS - MIN-MAX VALUES, OVERALL STATS\nHow are the values distributed in each data columns? Let's perform some statistical analysis","93b5831c":"From the plots, the video amount (sum) at every target is very less to cause any impact on the adoption speed. The video amount is plotted as a sum at every bin. The 2nd graph of VideoAmt shows an almost flat curve wrt AdoptionSpeed. However, it is different for PhotoAmt. Hence, VideoAmt can conclusively be dropped off","4e20c4b8":"### 1.1 Training Data \nTabular\/text data for the training set","8fc75d19":"# Predict the speed of Pet Adoption\n\n\nThe goal of the project is to build a product that would predict the speed at which a pet would be adopted, given the description of the pet. The most interesting thing about the project is the dataset - it contains both, structured and unstructred dataset, i.e. tabular as well image data. The dataset is intuitive with respect to the problem and at the same time quite challenging.[](http:\/\/) ","f358fdc2":"# An overview of the dataset\n\nThe dataset used is an open-source dataset of kaggle. The following gives an overview of the data fields representing a pet \n\nPetID - Unique hash ID of pet profile <br>\nAdoptionSpeed - Categorical speed of adoption. Lower is faster. This is the value to predict <br> \n**Type - Type of animal (1 = Dog, 2 = Cat)** <br>\nName - Name of pet (Empty if not named) <br>\nAge - Age of pet when listed, in months <br>\nBreed1 - Primary breed of pet (Refer to BreedLabels dictionary) <br>\nBreed2 - Secondary breed of pet, if pet is of mixed breed (Refer to BreedLabels dictionary) <br>\nGender - Gender of pet (1 = Male, 2 = Female, 3 = Mixed, if profile represents group of pets) <br>\nColor1 - Color 1 of pet (Refer to ColorLabels dictionary) <br>\nColor2 - Color 2 of pet (Refer to ColorLabels dictionary) <br>\nColor3 - Color 3 of pet (Refer to ColorLabels dictionary) <br>\nMaturitySize - Size at maturity (1 = Small, 2 = Medium, 3 = Large, 4 = Extra Large, 0 = Not Specified) <br>\nFurLength - Fur length (1 = Short, 2 = Medium, 3 = Long, 0 = Not Specified) <br>\nVaccinated - Pet has been vaccinated (1 = Yes, 2 = No, 3 = Not Sure) <br>\nDewormed - Pet has been dewormed (1 = Yes, 2 = No, 3 = Not Sure) <br>\nSterilized - Pet has been spayed \/ neutered (1 = Yes, 2 = No, 3 = Not Sure) <br>\nHealth - Health Condition (1 = Healthy, 2 = Minor Injury, 3 = Serious Injury, 0 = Not Specified) <br>\nQuantity - Number of pets represented in profile <br>\nFee - Adoption fee (0 = Free) <br>\nState - State location in Malaysia (Refer to StateLabels dictionary) <br>\nRescuerID - Unique hash ID of rescuer <br>\nVideoAmt - Total uploaded videos for this pet <br>\nPhotoAmt - Total uploaded photos for this pet <br>","dc6d2034":"# 2. Data Quality Assessment - Exploring the Data\nLet's conduct some quality assessment on the data obtained. Based on the assessments the data would be accordingly cleaned and transformed (if required)","ebe989b5":"### 1.2 Breed Labels [Part of training data]\nContains Type, and BreedName for each BreedID. Type 1 is dog, 2 is cat.","1b3adfe7":"From the above results, the average age (in months) of pets is very near to the min age and much farther than the max age of pets. \nSimilarly, the average adoption fee is nearer to the min adoption fee. \n\nThere is a possibility of outliers being present in both the cases. A few visualizations should help clear the air.\n\nMean no. of videos is ~ 0. As such, this value won't seem to contribute much to training. And hence can be dropped. ","e81668eb":"#### LABEL ENCODING CATEGORICAL (STRING TYPE) ATTRIBUTES, SCALING ATTRIBUTE VALUES","d21eb485":"# 5. Model Training\n\nThe proposed model is similar to the architecture shown in the figure below. The ordered data ('categorical data A' as per the figure) would be fed into the 1st neural network (NN-1) which would be trained on the actual targets. After NN-1 is trained, the output (Out-1) of a layer in NN-1 would be taken to prepare the input for 2nd neural network (NN-2). \nThe image data would be fed into a pretrained convolutional neural network (say, ResNet50), and the output of last hidden layer would be extracted (Out-2). This output combined with NN-1 output (Out-1 + Out-2) is then fed into NN-2 as input. NN-2 then outputs the class probabilities\n\n\nFig credits - [[StackExchange](https:\/\/datascience.stackexchange.com\/questions\/29634\/how-to-combine-categorical-and-continuous-input-features-for-neural-network-trai)]","ca645712":"The adoption speed for both cats and dogs are pretty much comparable. ","23d7b06f":"Since the no of samples having values different from the allowed values (wrt reference table) is quite large, removing these samples\/rows just based on these 3 attributes would reduce a significant chunk of data. And filling these col values would only make these samples spurious. So,let's keep it as it is - and treat these values as breed\/color unknown.\nMoreover, as the number of values having other than allowed values in b2 and c3 is near to the total no of samples, these columns won't contribute much to training. Hence, can be dropped","4c8d36e3":"Instead of having any threshold for correlation values (as most of the values are in similar range) to eliminate certain attribute, let's simply eliminate the least correlated attribute (1 +ve corr, -1 -ve corr, 0 no corr)","f8965202":"#### Note: Due to certain resource constraints (Memory error , Long training times + connectivity issues) the 1st model training was halted. The model was saved and trained seperately for several days for fewer number of epochs (8 or 10) each time, on Kaggle. With no provision for saving the cell output on a kaggle editable notebook, the model training output could not be present as part of this main notebook","b3b4e2af":"#### FEATURE ENGINEERING IMAGE DATA","605ceb07":"As can be seen from the plots, both 'Age' and 'Fee' have certain outliers.Let's keep the outliers for now (as no upper bound\/limits mentioned for these 2 categories)","04b42594":"#### **FIND THE UPDATED VERSION (WITH CELL OUTPUT), HERE - [GITHUB](https:\/\/gist.github.com\/abhilash97\/11945d1cdfe5658432d59932f1baeb88)**","40bb9248":"# 6. Model Evaluation\n \nThe model performance is measured using classification metrics like classification report, confusion matrix","8849e4cf":"#### SET AND FOREIGN KEY MEMBERSHIP\nSet Membership -> Check if only allowed values are chosen for categorical fields.<br>\nFK Membership -> Check if only allowed values (with respect to the reference table values) are present in a field","8ec5b429":"![model](https:\/\/i.stack.imgur.com\/QgQFq.png)","ebda8319":"Filling the missing values - The best way to fill the missing names is to name the pet - 'No Name' (or unknown). Some pets are assigned 'No Name yet' and some 'No Name'. It would be easier to one-hot encode the names if all these no name pets are given a single name (eg. unknown)","9ef34b85":"# 3. Feature Engineering\nFeature selection, feature extraction, Normalization, ...","591b870b":"#### DATA TYPE CHECK\nCheck if data types of col match their content","6bf80794":"# 1. Load Data\n\nThe ETL process is performed locally. The data is extracted from the csv and jpg files. The transformed tabular and image data are stored in a dataframe, numpy files respectively, which are then used for modelling"}}