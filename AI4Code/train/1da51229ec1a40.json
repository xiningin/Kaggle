{"cell_type":{"956bb9f6":"code","57eb98b2":"code","62b55e50":"code","8bdff7c3":"code","bdf46f58":"code","07258d26":"code","691b1e97":"code","35e20c83":"code","b5d68fb1":"code","d5a6e656":"code","f4a6fabc":"code","899756ac":"code","bd59bf7d":"code","a39578d7":"markdown","4cdbd509":"markdown","99b1b43a":"markdown","89fc1853":"markdown","8611399b":"markdown","a64b1a03":"markdown","bb5045e5":"markdown","8f6bd0df":"markdown","b70df7e6":"markdown"},"source":{"956bb9f6":"from IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#8F003C','#eb3446','Tourney','Smokum',45,10\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h4 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h4>\"\"\"%string))\n    \nfrom IPython.display import HTML\nHTML(\"\"\"\n<style>\nh1,h2,h3 {\n\tmargin: 1em 0 0.5em 0;\n\tfont-weight: 600;\n\tfont-family: 'Titillium Web', sans-serif;\n\tposition: relative;  \n\tfont-size: 36px;\n\tline-height: 40px;\n\tpadding: 15px 15px 15px 2.5%;\n\tcolor: #00018D;\n\tbox-shadow: \n\t\tinset 0 0 0 1px rgba(97,0,45, 1), \n\t\tinset 0 0 5px rgba(53,86,129, 1),\n\t\tinset -285px 0 35px #F2D8FF;\n\tborder-radius: 0 10px 0 15px;\n\tbackground: #FFD8B2\n    \n},\n\nh4 {\n\tmargin: 1em 0 0.5em 0;\n\tfont-weight: 600;\n\tfont-family: 'Titillium Web', sans-serif;\n\tposition: relative;  \n\tfont-size: 36px;\n\tline-height: 40px;\n\tpadding: 15px 15px 15px 2.5%;\n\tcolor: #00018D;\n\n\tborder-radius: 0 10px 0 15px;\n\tbackground: #FFD8B2\n    \n}\n<\/style>\n\"\"\")","57eb98b2":"import numpy as np\nimport pandas as pd\ntrain=pd.read_pickle(\"..\/input\/optiver006\/train.pkl\")\n# replace by order sum (tau)\ntrain['size_tau'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique'] )\n#test['size_tau'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique'] )\n#train['size_tau_450'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_450'] )\n#test['size_tau_450'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_450'] )\ntrain['size_tau_400'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_400'] )\n#test['size_tau_400'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_400'] )\ntrain['size_tau_300'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_300'] )\n#test['size_tau_300'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_300'] )\n#train['size_tau_150'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_150'] )\n#test['size_tau_150'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_150'] )\ntrain['size_tau_200'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_200'] )\n#test['size_tau_200'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_200'] )\ntrain['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )\n#test['size_tau2'] = np.sqrt( 1\/ test['trade_order_count_sum'] )\n#train['size_tau2_450'] = np.sqrt( 0.25\/ train['trade_order_count_sum'] )\n#test['size_tau2_450'] = np.sqrt( 0.25\/ test['trade_order_count_sum'] )\ntrain['size_tau2_400'] = np.sqrt( 0.33\/ train['trade_order_count_sum'] )\n#test['size_tau2_400'] = np.sqrt( 0.33\/ test['trade_order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5\/ train['trade_order_count_sum'] )\n#test['size_tau2_300'] = np.sqrt( 0.5\/ test['trade_order_count_sum'] )\n#train['size_tau2_150'] = np.sqrt( 0.75\/ train['trade_order_count_sum'] )\n#test['size_tau2_150'] = np.sqrt( 0.75\/ test['trade_order_count_sum'] )\ntrain['size_tau2_200'] = np.sqrt( 0.66\/ train['trade_order_count_sum'] )\n#test['size_tau2_200'] = np.sqrt( 0.66\/ test['trade_order_count_sum'] )\n\n# delta tau\ntrain['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\n#test['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']","62b55e50":"from sklearn.cluster import KMeans\n# making agg features\n\n\ntrain_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\n\n\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\n\nnnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     'total_volume_sum_6c1',\n     'trade_size_sum_0c1',\n     'trade_size_sum_1c1', \n     'trade_size_sum_3c1',\n     'trade_size_sum_4c1', \n     'trade_size_sum_6c1',\n     'trade_order_count_sum_0c1',\n     'trade_order_count_sum_1c1',\n     'trade_order_count_sum_3c1',\n     'trade_order_count_sum_4c1',\n     'trade_order_count_sum_6c1',      \n     'price_spread_sum_0c1',\n     'price_spread_sum_1c1',\n     'price_spread_sum_3c1',\n     'price_spread_sum_4c1',\n     'price_spread_sum_6c1',   \n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     'bid_spread_sum_6c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     'ask_spread_sum_6c1',   \n     'volume_imbalance_sum_0c1',\n     'volume_imbalance_sum_1c1',\n     'volume_imbalance_sum_3c1',\n     'volume_imbalance_sum_4c1',\n     'volume_imbalance_sum_6c1',       \n     'bid_ask_spread_sum_0c1',\n     'bid_ask_spread_sum_1c1',\n     'bid_ask_spread_sum_3c1',\n     'bid_ask_spread_sum_4c1',\n     'bid_ask_spread_sum_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] \ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')","8bdff7c3":"train.isnull().sum().sum()","bdf46f58":"import os\nimport glob\nfrom joblib import Parallel, delayed\n\nimport pandas as pd\nimport numpy as np\nimport scipy as sc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, KFold\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 300)\n\nimport optuna\n#N_TRIALS = 100\nTIME = 600\nN_SPLITS = 5\nRANDOM_STATE = 99\nkfold = KFold(N_SPLITS, random_state=RANDOM_STATE, shuffle=True)\n\nFIXED_PARAMS = {\n                'learning_rate':0.07,\n                'metric': 'rmse',\n                'verbosity': -1,\n                'n_jobs': -1,\n                #'max_bin': 127,\n                'seed': RANDOM_STATE}\n\ndef rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\n    \ndef objective(trial, cv=kfold):\n    \n    params = {\n        'n_estimators': trial.suggest_int('n_estimators',500,2000),\n        'num_leaves': trial.suggest_int('num_leaves', 400, 800),\n        'max_depth': trial.suggest_int('max_depth', -1, 3),\n        'max_bin':trial.suggest_int('max_bin', 50, 200),\n        'min_data_in_leaf':trial.suggest_int('min_data_in_leaf',400,700),\n        #'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 5),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 5),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1),\n        'subsample': trial.suggest_float('subsample', 0.4, 1),\n        'subsample_freq': trial.suggest_int('subsample_freq',1,10)\n        \n     \n        #'cat_smooth': trial.suggest_float('cat_smooth', 10, 100.0),  \n        #'feature_fraction': trial.suggest_float('feature_fraction',0.3,0.99),\n        \n       \n    }\n    \n    params.update(FIXED_PARAMS)\n\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"rmse\", valid_name='valid_1')\n    rmspe_list = []\n    \n    for kfold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        X_train = X.iloc[train_idx]\n        y_train = y.iloc[train_idx]\n        X_val = X.iloc[val_idx]\n        y_val = y.iloc[val_idx]\n\n        d_train = lgb.Dataset(X_train, label=y_train)\n        d_valid = lgb.Dataset(X_val, label=y_val)\n\n        model = lgb.train(params,\n                      train_set=d_train,\n                      valid_sets=[d_train, d_valid],\n                      verbose_eval=0,\n                      early_stopping_rounds=100,\n                      callbacks=[pruning_callback])\n\n        preds = model.predict(X_val)\n        score = rmspe(y_val, preds)\n        \n        rmspe_list.append(score)\n        \n    \n    return np.mean(rmspe_list)","07258d26":"X_display = train.drop(['row_id', 'time_id', 'target'], axis = 1)\nX = X_display\ny = train['target']","691b1e97":"X= X.interpolate(method='index')","35e20c83":"X.isnull().sum().sum()","b5d68fb1":"\nstudy = optuna.create_study(direction='minimize', pruner=optuna.pruners.MedianPruner(n_warmup_steps=25))\nstudy.optimize(objective, timeout=TIME)\n    \nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","d5a6e656":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\n\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\nfrom catboost import Pool, CatBoostRegressor\n\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}\n\n# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\ndef calc_wap3(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\ndef calc_wap4(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))\n\n# Function to read our base train and test set\ndef read_train_test():\n    #train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    #train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    #print(f'Our training set has {train.shape[0]} rows')\n    return test\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    df['wap3'] = calc_wap3(df)\n    df['wap4'] = calc_wap4(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    df['log_return3'] = df.groupby(['time_id'])['wap3'].apply(log_return)\n    df['log_return4'] = df.groupby(['time_id'])['wap4'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.std],\n        'wap2': [np.sum, np.std],\n        'wap3': [np.sum, np.std],\n        'wap4': [np.sum, np.std],\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n        'wap_balance': [np.sum, np.max],\n        'price_spread':[np.sum, np.max],\n        'price_spread2':[np.sum, np.max],\n        'bid_spread':[np.sum, np.max],\n        'ask_spread':[np.sum, np.max],\n        'total_volume':[np.sum, np.max],\n        'volume_imbalance':[np.sum, np.max],\n        \"bid_ask_spread\":[np.sum,  np.max],\n    }\n    create_feature_dict_time = {\n        'log_return1': [realized_volatility],\n        'log_return2': [realized_volatility],\n        'log_return3': [realized_volatility],\n        'log_return4': [realized_volatility],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id__100'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    df['amount']=df['price']*df['size']\n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, np.max, np.min],\n        'order_count':[np.sum,np.max],\n        'amount':[np.sum,np.max,np.min],\n    }\n    create_feature_dict_time = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.sum],\n    }\n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(fe_dict,seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(fe_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(create_feature_dict,seconds_in_bucket = 0, add_suffix = False)\n    df_feature_500 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 500, add_suffix = True)\n    df_feature_400 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 300, add_suffix = True)\n    df_feature_200 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 200, add_suffix = True)\n    df_feature_100 = get_stats_window(create_feature_dict_time,seconds_in_bucket = 100, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff\/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})\n    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_500, how = 'left', left_on = 'time_id_', right_on = 'time_id__500')\n    df_feature = df_feature.merge(df_feature_400, how = 'left', left_on = 'time_id_', right_on = 'time_id__400')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_200, how = 'left', left_on = 'time_id_', right_on = 'time_id__200')\n    df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__500','time_id__400', 'time_id__300', 'time_id__200','time_id','time_id__100'], axis = 1, inplace = True)\n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_400', 'log_return2_realized_volatility_400', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_200', 'log_return2_realized_volatility_200', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_400', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_200']\n\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n# Read train and test\ntrain =pd.read_pickle(\"..\/input\/optiver006\/train.pkl\")\ntest = read_train_test()\n\n# Get unique stock ids \n#train_stock_ids = train['stock_id'].unique()\n# Preprocess them using Parallel and our single stock id functions\n#train_ = preprocessor(train_stock_ids, is_train = True)\n#train = train.merge(train_, on = ['row_id'], how = 'left')\n\n# Get unique stock ids \ntest_stock_ids = test['stock_id'].unique()\n\n# Preprocess them using Parallel and our single stock id functions\ntest_ = preprocessor(test_stock_ids, is_train = False)\ntest = test.merge(test_, on = ['row_id'], how = 'left')\n\n# Get group stats of time_id and stock_id\n#train = get_time_stock(train)\ntest = get_time_stock(test)\n\n# replace by order sum (tau)\n#train['size_tau_500'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_500'] )\n#test['size_tau_500'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_500'])\ntrain['size_tau'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique'] )\ntest['size_tau'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique'] )\n#train['size_tau_450'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_450'] )\n#test['size_tau_450'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_450'] )\ntrain['size_tau_400'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_400'] )\ntest['size_tau_400'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_400'] )\ntrain['size_tau_300'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_300'] )\ntest['size_tau_300'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_300'] )\n#train['size_tau_150'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_150'] )\n#test['size_tau_150'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_150'] )\ntrain['size_tau_200'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_200'] )\ntest['size_tau_200'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_200'] )\n\ntrain['size_tau_100'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_100'] )\ntest['size_tau_100'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_100'] )\ntrain['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1\/ test['trade_order_count_sum'] )\n\n#train['size_tau2_500'] = np.sqrt( 0.15\/ train['trade_order_count_sum'] )\n#test['size_tau2_500'] = np.sqrt( 0.15\/ test['trade_order_count_sum'] )\n\ntrain['size_tau2_400'] = np.sqrt( 0.33\/ train['trade_order_count_sum'] )\ntest['size_tau2_400'] = np.sqrt( 0.33\/ test['trade_order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5\/ train['trade_order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5\/ test['trade_order_count_sum'] )\n#train['size_tau2_150'] = np.sqrt( 0.75\/ train['trade_order_count_sum'] )\n#test['size_tau2_150'] = np.sqrt( 0.75\/ test['trade_order_count_sum'] )\ntrain['size_tau2_200'] = np.sqrt( 0.66\/ train['trade_order_count_sum'] )\ntest['size_tau2_200'] = np.sqrt( 0.66\/ test['trade_order_count_sum'] )\n\ntrain['size_tau2_100'] = np.sqrt( 0.88\/ train['trade_order_count_sum'] )\ntest['size_tau2_100'] = np.sqrt( 0.88\/ test['trade_order_count_sum'] )\n\n\n# delta tau\ntrain['size_tau2_d'] = train['size_tau2_400'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_400'] - test['size_tau2']\n\ntrain['size_tau2_d1'] = train['size_tau2_300'] - train['size_tau2_400']\ntest['size_tau2_d1'] = test['size_tau2_300'] - test['size_tau2_400']\ntrain['size_tau2_d2'] = train['size_tau2_200'] - train['size_tau2_300']\ntest['size_tau2_d2'] = test['size_tau2_200'] - test['size_tau2_300']\ncolNames = [col for col in list(train.columns)\n            if col not in {\"stock_id\", \"time_id\", \"target\", \"row_id\"}]\nlen(colNames)\n\nfrom sklearn.cluster import KMeans\n# making agg features\n\ntrain_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\n\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n    \n\nmat = []\nmatTest = []\n\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    \n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    \n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\n\nmat2 = pd.concat(matTest).reset_index()\n\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)\n\nnnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_sum_0c1',\n     'total_volume_sum_1c1', \n     'total_volume_sum_3c1',\n     'total_volume_sum_4c1', \n     'total_volume_sum_6c1',\n     'trade_size_sum_0c1',\n     'trade_size_sum_1c1', \n     'trade_size_sum_3c1',\n     'trade_size_sum_4c1', \n     'trade_size_sum_6c1',\n     'trade_order_count_sum_0c1',\n     'trade_order_count_sum_1c1',\n     'trade_order_count_sum_3c1',\n     'trade_order_count_sum_4c1',\n     'trade_order_count_sum_6c1',      \n     'price_spread_sum_0c1',\n     'price_spread_sum_1c1',\n     'price_spread_sum_3c1',\n     'price_spread_sum_4c1',\n     'price_spread_sum_6c1',   \n     'bid_spread_sum_0c1',\n     'bid_spread_sum_1c1',\n     'bid_spread_sum_3c1',\n     'bid_spread_sum_4c1',\n     'bid_spread_sum_6c1',       \n     'ask_spread_sum_0c1',\n     'ask_spread_sum_1c1',\n     'ask_spread_sum_3c1',\n     'ask_spread_sum_4c1',\n     'ask_spread_sum_6c1',   \n     'volume_imbalance_sum_0c1',\n     'volume_imbalance_sum_1c1',\n     'volume_imbalance_sum_3c1',\n     'volume_imbalance_sum_4c1',\n     'volume_imbalance_sum_6c1',       \n     'bid_ask_spread_sum_0c1',\n     'bid_ask_spread_sum_1c1',\n     'bid_ask_spread_sum_3c1',\n     'bid_ask_spread_sum_4c1',\n     'bid_ask_spread_sum_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] \ntrain = pd.merge(train,mat1[nnn],how='left',on='time_id')\ntest = pd.merge(test,mat2[nnn],how='left',on='time_id')\n\nimport gc\ndel mat1,mat2\ngc.collect()\n\ntrain=train[~(train[\"stock_id\"]==31)]\ntrain.shape,test.shape","f4a6fabc":"train= train.interpolate(method='index')\ntest= test.interpolate(method='index')\ntrain.isnull().sum().sum()\n","899756ac":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\n\nseed0=31\nparams0 = {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':100,\n    'min_data_in_leaf':500,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 3,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':seed0,\n    'feature_fraction_seed': seed0,\n    'bagging_seed': seed0,\n    'drop_seed': seed0,\n    'data_random_seed': seed0,\n    'n_jobs':-1,\n    'verbose': -1}\nseed1=42\nparams1 ={\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'n_estimators': 813,\n    'learning_rate': 0.1,\n 'num_leaves': 511, \n 'max_depth': -1,\n 'max_bin': 70, \n 'min_data_in_leaf': 617, \n 'reg_alpha': 0.009140934268631783, \n 'reg_lambda': 0.03287295891732818, \n 'colsample_bytree': 0.5030558578525355,\n 'subsample': 0.8207921722604871, \n 'subsample_freq': 7,\n'verbose': -1}\n# Function to early stop with root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False\n\ndef train_and_evaluate_lgb(train, test, params):\n    # Hyperparammeters (just basic)\n    \n    features = [col for col in train.columns if col not in {\"time_id\", \"target\", \"row_id\"}]\n    y = train['target']\n    # Create out of folds array\n    oof_predictions = np.zeros(train.shape[0])\n    # Create test array to store predictions\n    test_predictions = np.zeros(test.shape[0])\n    # Create a KFold object\n    kfold = KFold(n_splits = 5, random_state = 31, shuffle = True)\n    # Iterate through each fold\n    for fold, (trn_ind, val_ind) in enumerate(kfold.split(train)):\n        print(f'Training fold {fold + 1}')\n        x_train, x_val = train.iloc[trn_ind], train.iloc[val_ind]\n        y_train, y_val = y.iloc[trn_ind], y.iloc[val_ind]\n        # Root mean squared percentage error weights\n        train_weights = 1 \/ np.square(y_train)\n        val_weights = 1 \/ np.square(y_val)\n        train_dataset = lgb.Dataset(x_train[features], y_train, weight = train_weights)\n        val_dataset = lgb.Dataset(x_val[features], y_val, weight = val_weights)\n        model = lgb.train(params = params,\n                          num_boost_round=600,\n                          train_set = train_dataset, \n                          valid_sets = [train_dataset, val_dataset], \n                          verbose_eval = 250,\n                          early_stopping_rounds=30,\n                          feval = feval_rmspe)\n        # Add predictions to the out of folds array\n        oof_predictions[val_ind] = model.predict(x_val[features])\n        # Predict the test set\n        test_predictions += model.predict(test[features]) \/ 5\n    rmspe_score = rmspe(y, oof_predictions)\n    print(f'Our out of folds RMSPE is {rmspe_score}')\n    lgb.plot_importance(model,max_num_features=20)\n    # Return test predictions\n    return test_predictions\n# Traing and evaluate\npredictions_lgb= train_and_evaluate_lgb(train, test,params0)\ntest['target'] = predictions_lgb\ntest[['row_id', 'target']].to_csv('submission.csv',index = False)","bd59bf7d":"test['target']","a39578d7":"**LGBM** \n\nthere was a Golden age of XGBoost and  was champion on kaggle, Now LGBM have taken over. LGBM outperform XGBoost.\nwhy LGBM is a great choice:\n1 LGBM was developed and maintained by Microsoft himself.\n2 easier to implement \n3 faster than typical gradient boosting algorithm \n\nhowever: But to XGBoost\u2019s credit, XGBoost has been around the block longer than either LightGBM and CatBoost, so it has better learning resources and a more active developer community. It also doesn\u2019t hurt that XGBoost is substantially faster and more accurate than its predecessors and other competitors such as Scikit-learn.\n\n","4cdbd509":"<h1 style=\"background-color:lightblue;font-size:20px;color:#00033E;font-weight : bold\"> \ud83d\udc4d   2. Gradient Boosting methods:<\/h1>","99b1b43a":"\n\nOn Kaggle, LightGBM is indeed the \"meta\" base learner of almost all of the competitions that have structured datasets right now. This is mostly because of LightGBM's implementation; it doesn't do exact searches for optimal splits like XGBoost does in it's default setting (XGBoost now has this functionality as well but it's still not as fast as LightGBM) but rather through histogram approximations. The result is a slight decrease in predictive performance for a much larger speed increase in training. This means more opportunity for feature engineering\/experimentation\/model tuning (all of which are key to winning Kaggle competitions) which inevitably yields larger increases in predictive performance (despite using histogram approximations).\n\nCatBoost is not used as much because on average, it it found to be much slower than LightGBM. That being said, CatBoost is different in its implementation of gradient boosting which at times can give slightly more accurate predictions, in particular if you have large amounts of categorical features. I have never used CatBoost and so I encourage you to read that paper. Regardless, because rapid experimentation is vital in Kaggle competitions, LightGBM tends to be the go to algorithm when first creating strong base learners.\n\nIn general, it is important to note that a large amount of approaches I've seen involve combining all three boosting algorithms in a model stack (i.e. ensembling). LightGBM, CatBoost, and XGBoost might be thrown together as three base learners and then combined via. a GLM or neural network. This is done to really squeeze out decimal places on the leaderboard and so I doubt there is any theoretical (or practical) justification for it besides competitions.","89fc1853":"<h4 style=\"background-color:white;font-size:16px;color:black;\">it's the first algorithm i try when solving a tabular problem. because of its handy nature.\n\nLightGBM is a light version of gradient boosting framework based on decision trees to increases the efficiency of the model and reduces memory usage.\n<\/h4>\n\n* [LightGBM Github Documentation](https:\/\/github.com\/microsoft\/LightGBM\/tree\/master\/python-package)\n    \n* [check out feature of LightGBM](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Features.html)\n\n<h4 style=\"background-color:white;font-size:16px;color:black;\">To get most out of your LightGBM model, its necessary to understand the dynamics of the algorithm<\/h4>\n\n* [Official Documentation](https:\/\/lightgbm.readthedocs.io\/en\/latest\/index.html)\n\n<h4 style=\"background-color:white;font-size:16px;color:black;\">\nThis are the question to be answered in this notebook\n1. Which Gradient Boosting methods are implemented in LightGBM.\n2. Which parameters are important in general?\n3. Which regularization parameters need to be tuned?\n\n<\/h4>\n\n","8611399b":"<h4 style=\"background-color:white;font-size:16px;color:black;\">\nXGBoost and LGBM are cousine and they both implement the same idea Gradient Boosting Decision Tree\n<\/h4>\n\nThere are different kinds of Gradient Boosting, methods available in LGBM are:GBDT, DART, and GOSS and they can be controlled by boosting parameter\n\n**gbdt (gradient boosted decision trees)** It is based on three important principles:\n\n * Weak learners (decision trees) \n \n * Gradient Optimization \n \n * Boosting Technique\n \n * So in the gbdt method we have a lot of decision trees(weak learners). Those trees are built sequentially:\n \n\n<h4 style=\"background-color:white;font-size:16px;color:black;\">\nfirst tree learns how to fit to the target variable\nsecond tree learns how to fit to the residual (difference) between the predictions of the first tree and the ground truth\nThe third tree learns how to fit the residuals of the second tree and so on.\nAll those trees are trained by propagating the gradients of errors throughout the system.\n   <\/h4>\n","a64b1a03":"<h1 style=\"background-color:lightblue;font-size:20px;color:#00033E;font-weight : bold\"> \ud83d\udc4d   3. Read this Instead Hit and Trail parameter search:<\/h1>\n\n<h4 style=\"background-color:white;font-size:16px;color:black;\">\n<b>Parameter that control decision trees in lgbm<\/b><\/h4>\n\n* num_leaves: controls the number of decision leaves in a single tree. there will be multiple trees in pool.\n* max_depth: this the height of a decision tree. if its more possibility of overfitting but too low may underfit.\n* min_data_in_leaf: the minimum number of data\/sample\/count per leaf (default is 20; lower min_data_in_leaf means less conservative\/control, potentially overfitting).\n**NOTE: max_depth have direct impact on:**\n1 The best value for the num_leaves parameter\n2 Model Performance\n3 Training Time\n\n\n<h4 style=\"background-color:white;font-size:16px;color:black;\">\n<b>For Better Accuracy<\/b><\/h4>\n\n* Use large max_bin (may be slower)\n\n* Use small learning_rate with large num_iterations\n\n* Use large num_leaves (may cause over-fitting)\n\n* Use bigger training data\n\n* Try dart\n\n<h4 style=\"background-color:white;font-size:16px;color:black;\">\nDeal with Over-fitting<\/h4>\n\n* Use small max_bin\n\n* Use small num_leaves\n\n* Use min_data_in_leaf and min_sum_hessian_in_leaf\n\n* Use bagging by set bagging_fraction and bagging_freq\n\n* Use feature sub-sampling by set feature_fraction\n\n* Use bigger training data\n\n* Try lambda_l1, lambda_l2 and min_gain_to_split for regularization\n\n* Try max_depth to avoid growing deep tree\n\n* Try extra_trees\n\n* Try increasing path_smooth\n\n    \n      \n","bb5045e5":"<h1 style=\"background-color:lightblue;font-size:20px;color:#00033E;font-weight : bold\"> IMPORTANT PARAMETERS OF LGBM:<\/h1>\n\n**objective**\n * When you change it affects other parameters\tSpecify the type of ML model\n * default- value regression\n * aliases- Objective_type\n\n\n**boosting**\n * If you set it RF, that would be a bagging approach\n * default- gbdt\n * Range- [gbdt, rf, dart, goss]\n * aliases- boosting_type\n\n**lambda_l1**\n * regularization parameter\n * default- 0.0\n * Range- [0, \u221e]\n * aliases- reg_alpha\n * constraints- lambda_l1 >= 0.0\n\n**bagging_fraction**\n * randomly select part of data without resampling\n * default-1.0\n * range- [0, 1]\n * aliases- Subsample\n * constarints- 0.0 < bagging_fraction <= 1.0\n\n**bagging_freq**\n * default- 0.0\n * range- [0, \u221e]\n * aliases- subsample_freq\n * bagging_fraction should be set to value smaller than 1.0 as well 0 means disable bagging\n\n**num_leaves**\n * max number of leaves in one tree\n * default- 31\n * Range- [1, \u221e]\n * Note- 1 < num_leaves <= 131072\n\n**feature_fraction**\n * if you set it to 0.8, LightGBM will select 80% of features\n * default- 1.0\n * Range- [0, 1]\n * aliases- sub_feature\n * constarint- 0.0 < feature_fraction <= 1.0\n\n**max_depth**\n * default- [-1]\n * range- [-1, \u221e]m\n * Larger is usually better, but overfitting speed increases.\n * limit the max depth Forr tree model\n\n**max_bin**\n * deal with over-fitting\n * default- 255\n * range- [2, \u221e]\n * aliases- Histogram Binning\n * max_bin > 1\n\n**num_iterations**\n * number of boosting iterations\n * default- 100\n * range- [1, \u221e]\n * AKA- Num_boost_round, n_iter\n * constarints- num_iterations >= 0\n\n**learning_rate**\n * default- 0.1\n * range- [0 1]\n * aliases- eta\n * general values- learning_rate > 0.0Typical: 0.05.\n\n**early_stopping_round**\n * will stop training if validation doesn\u2019t improve in last early_stopping_round\n * Model Performance, Number of Iterations, Training Time\n * default- 0\n * Range- [0, \u221e]\n\n**categorical_feature** \n * to sepecify or Handle categorical features\n * i.e LGBM automatically handels categorical variable we dont need to one hot encode them.\n\n**bagging_freq**\n * default-0.0\n * Range-[0, \u221e]\n * aliases- subsample_freq\n * note- 0 means disable bagging; k means perform bagging at every k iteration\n * enable    bagging, bagging_fraction should be set to value smaller than 1.0 as well\n\n**verbosity**\n * default- 0\n * range- [-\u221e, \u221e]\n * aliases- verbose\n * constraints- {< 0: Fatal, = 0: Error (Warning), = 1: Info, > 1}\n\n**min_data_in_leaf**\n * Can be used to deal with over-fitting:\n * default- 20\n * constarint-min_data_in_leaf >= 0","8f6bd0df":"<h1 style=\"background-color:lightblue;font-size:20px;color:#00033E;font-weight : bold\"> Parameter Optimization:<\/h1>","b70df7e6":"<h1 style=\"background-color:lightblue;font-size:20px;color:#00033E;font-weight : bold\"> \ud83d\udc4d   1. INTRODUCTION:<\/h1>"}}