{"cell_type":{"a5bf919c":"code","51fad0d5":"code","8714c739":"code","1225c038":"code","6b31a820":"code","b13ea05e":"code","ae10aa4d":"code","9535e06d":"code","39ff2596":"code","b3a3ec6c":"code","0a25008d":"code","1f7b89d9":"code","7dab00b2":"code","15bc13b4":"code","26182e29":"code","b5a413c8":"code","62924e83":"code","d4b54b91":"code","9d52476a":"code","a5935572":"code","7c753d55":"code","7c6548e9":"code","e3673e1e":"code","3218c594":"code","52284ecb":"code","5775a4a0":"code","98862f78":"code","f6775b71":"markdown","30baeacf":"markdown","b04378e0":"markdown","02aa701b":"markdown","3274fe2e":"markdown"},"source":{"a5bf919c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom functools import partial\nimport optuna\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.model_selection import StratifiedKFold\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport gc\nfrom scipy import stats","51fad0d5":"df_train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv',index_col = 'Id')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv',index_col='Id')","8714c739":"#Use this notebook to make my pseudolabels file https:\/\/www.kaggle.com\/remekkinas\/tps-12-pseudolabels-for-classification-tutorial\/notebook\n\npseudo_df = pd.read_csv('..\/input\/tbsdexxgbclassifierprediction\/tps12-pseudolabels.csv',index_col =\"Id\")\n\nnew_df_train = pd.concat([df_train,pseudo_df],axis =0)\nnew_df_train.reset_index(drop=True)","1225c038":"del df_train","6b31a820":"# reduce the data usage\n# from the discussion board (https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/291844)\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","b13ea05e":"new_df_train = reduce_mem_usage(new_df_train)\ndf_test = reduce_mem_usage(df_test)","ae10aa4d":"#Soil_Type7 and SoilType15 has only zero values. Need to delete those two columns.\n\nnew_df_train = new_df_train.drop(['Soil_Type7','Soil_Type15'],axis=1)\ndf_test = df_test.drop(['Soil_Type7','Soil_Type15'],axis=1)\n","9535e06d":"# Cover_Type (Target) distribution. \nnew_df_train.Cover_Type.value_counts()","39ff2596":"# Cover_Type 5 was only one sample in this data.  \nnew_df_train = new_df_train[new_df_train.Cover_Type != 5]","b3a3ec6c":"#separate targets and features\n\ntargets = new_df_train.Cover_Type\nfeatures = new_df_train.drop(['Cover_Type'],axis=1)\nfeatures = reduce_mem_usage(features)\n\n","0a25008d":"del new_df_train","1f7b89d9":"encoder = LabelEncoder()\ntargets[:] = encoder.fit_transform(targets[:])","7dab00b2":"#Make Aspect values from 0 to 359 degree\n#Extra feature engineering from the discussion board https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293373\n\nfeatures[\"Aspect\"][features[\"Aspect\"] <0] +=360\nfeatures[\"Aspect\"][features[\"Aspect\"] >359]-=360\n\ndf_test[\"Aspect\"][df_test[\"Aspect\"] <0] +=360\ndf_test[\"Aspect\"][df_test[\"Aspect\"] >359] -=360\n\n\nfeatures.loc[features[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\ndf_test.loc[df_test[\"Hillshade_9am\"] < 0, \"Hillshade_9am\"] = 0\n\nfeatures.loc[features[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\ndf_test.loc[df_test[\"Hillshade_Noon\"] < 0, \"Hillshade_Noon\"] = 0\n\nfeatures.loc[features[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\ndf_test.loc[df_test[\"Hillshade_3pm\"] < 0, \"Hillshade_3pm\"] = 0\n\nfeatures.loc[features[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\ndf_test.loc[df_test[\"Hillshade_9am\"] > 255, \"Hillshade_9am\"] = 255\n\nfeatures.loc[features[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\ndf_test.loc[df_test[\"Hillshade_Noon\"] > 255, \"Hillshade_Noon\"] = 255\n\nfeatures.loc[features[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\ndf_test.loc[df_test[\"Hillshade_3pm\"] > 255, \"Hillshade_3pm\"] = 255\n\n","15bc13b4":"#some more features engineering\n# from this discussion https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/293612\n\n\nfeatures['Euclidean_Distance_to_Hydrology'] =  ((features['Horizontal_Distance_To_Hydrology']).astype(np.int32)**2 + (features['Vertical_Distance_To_Hydrology']).astype(np.int32)**2)**0.5\n\n\nfeatures['Manhattan_Distance_to_Hydrology'] = np.abs(features['Horizontal_Distance_To_Hydrology']) + np.abs(features['Vertical_Distance_To_Hydrology'])\n\n\ndf_test['Euclidean_Distance_to_Hydrology'] =  ((df_test['Horizontal_Distance_To_Hydrology']).astype(np.int32)**2 + (df_test['Vertical_Distance_To_Hydrology']).astype(np.int32)**2)**0.5\n\ndf_test['Manhattan_Distance_to_Hydrology'] = np.abs(df_test['Horizontal_Distance_To_Hydrology']) + np.abs(df_test['Vertical_Distance_To_Hydrology'])","26182e29":"features","b5a413c8":"#extra feature engineering from the discussion board. \n#sum of soil_type and wilderness_ares https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/292823\n\n\nfeature_list = features.columns\nsoil_features = [x for x in feature_list if x.startswith(\"Soil_Type\")]\nfeatures['soil_type_count'] = features[soil_features].sum(axis=1)\ndf_test['soil_type_count'] =df_test[soil_features].sum(axis=1)\n\nwilderness_features= [x for x in feature_list if x.startswith('Wilderness')]\nfeatures['wilderness_area_count']=features[wilderness_features].sum(axis=1)\ndf_test['wilderness_area_count'] = df_test[wilderness_features].sum(axis=1)","62924e83":"#Scaling the values.\n\n\nfrom sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler()\n#scaler = preprocessing.StandardScaler()\n\nnumeric_features = features.columns[0:11].to_list() + features.columns[-4:].to_list()\n\nfeatures[numeric_features] = scaler.fit_transform(features[numeric_features])\ndf_test[numeric_features] = scaler.transform(df_test[numeric_features])","d4b54b91":"features","9d52476a":"#Separate data into train and validation data\n#NN model input should be array. \n#X_train,X_val,y_train,y_val = train_test_split(features.values,targets.values, random_state=15)","a5935572":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import plot_model","7c753d55":"def build_model():\n    model = tf.keras.models.Sequential([\n      tf.keras.layers.Flatten(input_shape=[features.shape[1]]),\n      tf.keras.layers.BatchNormalization(),\n      tf.keras.layers.Dense(256, kernel_initializer=\"lecun_normal\", activation='selu'),\n      tf.keras.layers.BatchNormalization(),\n      tf.keras.layers.Dense(128,kernel_initializer=\"lecun_normal\" ,activation='selu'),\n      tf.keras.layers.BatchNormalization(),\n      tf.keras.layers.Dense(64,kernel_initializer=\"lecun_normal\", activation='selu'),\n      tf.keras.layers.BatchNormalization(),\n      tf.keras.layers.Dense(units=6,activation='softmax')\n       ])\n      \n    model.compile(loss='sparse_categorical_crossentropy',\n             optimizer= 'adam',\n             metrics=['accuracy'])\n    \n    return model\n\n      ","7c6548e9":"# setting is from  this notebook https:\/\/www.kaggle.com\/balamurugan1603\/tps-dec-21-nn-feature-engg-tf\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n\nreduce_lr = ReduceLROnPlateau(\n    monitor=\"val_loss\",\n    factor=0.5,\n    patience=5\n)\n\nearly_stop = EarlyStopping(\n    monitor=\"val_accuracy\",\n    patience=20,\n    restore_best_weights=True\n)\n\ncallbacks = [reduce_lr, early_stop]","e3673e1e":"# referred to this notebook  https:\/\/www.kaggle.com\/hamzaghanmi\/tps-dec-step-by-step\/notebook\n\npreds = []\n\nkf = StratifiedKFold(n_splits=23, random_state=2,shuffle=True)\nacc = []\nn=0\n\nfor trn_idx, test_idx in kf.split(features,targets):\n    X_tr,X_val = features.iloc[trn_idx].values, features.iloc[test_idx].values\n    y_tr,y_val = targets.iloc[trn_idx].values, targets.iloc[test_idx].values\n    \n    model = build_model()\n    model.fit(X_tr,y_tr,\n                    epochs=10,\n                    batch_size=2021,\n                    verbose=False,\n                    callbacks=callbacks,\n                    validation_data=(X_val,y_val))\n    \n    preds.append(model.predict(df_test))\n    \n    pre = np.argmax(model.predict(X_val),axis=1)\n    \n    acc.append(accuracy_score(y_val,pre))\n                    \n    print(f\"fold: {n+1} , accuracy: {round(acc[n]*100,3)}\")\n    n+=1\n                                                    \n    del X_tr,X_val,y_tr, y_val\n    gc.collect()\n                                                    \n    \n    ","3218c594":"from scipy import stats\npredictions = stats.mode(preds)[0][0]\n","52284ecb":"preds = np.argmax(predictions,axis=1)\npreds = encoder.inverse_transform(preds)\n","5775a4a0":"index = pd.read_csv(\"..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv\")\nindex['Cover_Type'] = preds\nindex.to_csv('submission.csv',index=False)","98862f78":"index.head(10)","f6775b71":"# Background of this notebook\n\nAccording [to the discussiong board](https:\/\/www.kaggle.com\/c\/tabular-playground-series-dec-2021\/discussion\/295617), deep neural networks works well with this competition data. It will be a good practice to write a multi classification NN model.\n\nMy best score was 0.95516 with XGBClassifier on [this notebook](https:\/\/www.kaggle.com\/satoshiss\/tps-december-xgbclassifier)\n\nLet's see how it goes.\n****************************************************************","30baeacf":"# Import libraries and Load Data","b04378e0":"**************************************************************************\n# 1. Data Explanatory Analysis and Cleaning\n\nThis is my third time to join Tabular Play Ground Series. I did not put much time for data explanatory analysis in the last two competitions. I will put some more effort on it this time to do effective feature engineering later on. To do so, I referred to [Machine Learning Explainability](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability) course on Kaggle. \n","02aa701b":"# Making Model and Predict \n","3274fe2e":"# Data Description\n\nFor this competition, you will be predicting a categorical target based on a number of feature columns given in the data. The data is synthetically generated by a GAN that was trained on a the data from the Forest Cover Type Prediction. This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data.\n\nPlease refer to this data page for a detailed explanation of the features.\n\nFiles\n* train.csv - the training data with the target Cover_Type column\n* test.csv - the test set; you will be predicting the Cover_Type for each row in this file (the target integer class)\n* sample_submission.csv - a sample submission file in the correct format\n\n\nFrom the competition data page.\n***************************************************************************\n"}}