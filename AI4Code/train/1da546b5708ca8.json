{"cell_type":{"337b7c58":"code","6c5d2a91":"code","cd3f7b23":"code","697ad458":"code","149b75bb":"code","1b7634ee":"code","aa5e6da8":"code","2e4e1df4":"code","db6c25c7":"code","5cf3e772":"code","89f9891c":"code","9fd6efe5":"code","83bb2068":"code","1a016c2c":"code","e13030eb":"code","ade41b81":"code","b3babe2d":"code","81213115":"code","1caadef4":"markdown","10eee278":"markdown","e7f1b8fb":"markdown","1759c0f9":"markdown","08d7b87b":"markdown","f1e4a931":"markdown","5a0312c2":"markdown","4260c138":"markdown","cd406182":"markdown","36c844c1":"markdown","6684c0fd":"markdown","709b4cb9":"markdown","bff25b2b":"markdown","b48b900e":"markdown"},"source":{"337b7c58":"# import necessary libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.datasets import load_boston\nfrom random import seed\nfrom random import randrange\nfrom csv import reader\nfrom math import sqrt\nfrom sklearn import preprocessing\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom prettytable import PrettyTable\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom numpy import random\nfrom sklearn.model_selection import train_test_split","6c5d2a91":"boston_data=pd.DataFrame(load_boston().data,columns=load_boston().feature_names)\nY=load_boston().target\nX=load_boston().data\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3)","cd3f7b23":"# data overview\nboston_data.head(3)","697ad458":"# standardizing data\nscaler = preprocessing.StandardScaler().fit(x_train)\nx_train = scaler.transform(x_train)\nx_test=scaler.transform(x_test)","149b75bb":"train_data=pd.DataFrame(x_train)\ntrain_data['price']=y_train\ntrain_data.head(3)","1b7634ee":"x_test=np.array(x_test)\ny_test=np.array(y_test)","aa5e6da8":"# shape of test and train data matxis\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","2e4e1df4":"# SkLearn SGD classifier\nclf_ = SGDRegressor()\nclf_.fit(x_train, y_train)\nplt.scatter(y_test,clf_.predict(x_test))\nplt.grid()\nplt.xlabel('Actual y')\nplt.ylabel('Predicted y')\nplt.title('scatter plot between actual y and predicted y')\nplt.show()\nprint('Mean Squared Error :',mean_squared_error(y_test, clf_.predict(x_test)))\nprint('Mean Absolute Error :',mean_absolute_error(y_test, clf_.predict(x_test)))","db6c25c7":"# SkLearn SGD classifier predicted weight matrix\nsklearn_w=clf_.coef_\nsklearn_w","5cf3e772":"# implemented SGD Classifier\ndef CustomGradientDescentRegressor(train_data,learning_rate=0.001,n_itr=1000,k=10):\n    w_cur=np.zeros(shape=(1,train_data.shape[1]-1))\n    b_cur=0\n    cur_itr=1\n    while(cur_itr<=n_itr):\n        w_old=w_cur\n        b_old=b_cur\n        w_temp=np.zeros(shape=(1,train_data.shape[1]-1))\n        b_temp=0\n        temp=train_data.sample(k)\n        #print(temp.head(3))\n        y=np.array(temp['price'])\n        x=np.array(temp.drop('price',axis=1))\n        for i in range(k):\n            w_temp+=x[i]*(y[i]-(np.dot(w_old,x[i])+b_old))*(-2\/k)\n            b_temp+=(y[i]-(np.dot(w_old,x[i])+b_old))*(-2\/k)\n        w_cur=w_old-learning_rate*w_temp\n        b_cur=b_old-learning_rate*b_temp\n        if(w_old==w_cur).all():\n            break\n        cur_itr+=1\n    return w_cur,b_cur\ndef predict(x,w,b):\n    y_pred=[]\n    for i in range(len(x)):\n        y=np.asscalar(np.dot(w,x[i])+b)\n        y_pred.append(y)\n    return np.array(y_pred)\n\n\ndef plot_(test_data,y_pred):\n    #scatter plot\n    plt.scatter(test_data,y_pred)\n    plt.grid()\n    plt.title('scatter plot between actual y and predicted y')\n    plt.xlabel('actual y')\n    plt.ylabel('predicted y')\n    plt.show()        \n        ","89f9891c":"# Funtion to get optimal learning rate on the implemented SGD Classifier\nfrom math import log\nx1_train,x1_test,y1_train,y1_test=train_test_split(X,Y,test_size=0.3)\nx1_train,x1_cv,y1_train_,y1_cv_=train_test_split(x1_train,y1_train,test_size=0.3)\n\nx1_train = scaler.transform(x1_train)\nx1_cv=scaler.transform(x1_cv)\n\nx1_train_=np.array(x1_train)\nx1_train_data=pd.DataFrame(x1_train)\nx1_train_data['price']=y1_train_\n\nx1_cv_data=pd.DataFrame(x1_cv)\nx1_cv_data['price']=y1_cv_\n\ny1_train_=np.array(y1_train_)\ny1_cv_=np.array(y1_cv_)\n#print(y1_cv_.shape)\n\ndef tuneParams_learning_rate():\n    train_error=[]\n    cv_error=[]\n    r=[0.00001,0.0001,0.001,0.01,0.1]\n    for itr in r:\n        w,b=CustomGradientDescentRegressor(x1_train_data,learning_rate=itr,n_itr=1000)\n       # print(w.shape,b.shape,x1_train_.shape)\n        y1_pred_train=predict(x1_train_,w,b)\n        train_error.append(mean_squared_error(y1_train_,y1_pred_train))\n        w,b=CustomGradientDescentRegressor(x1_cv_data,learning_rate=itr,n_itr=1000)\n        y1_pred_cv=predict(x1_cv,w,b)\n        cv_error.append(mean_squared_error(y1_cv_,y1_pred_cv))\n    return train_error,cv_error \n\n    \n        ","9fd6efe5":"train_error,cv_error=tuneParams_learning_rate()","83bb2068":"  # plotting obtained values\nimport math\nr=[0.00001,0.0001,0.001,0.01,0.1]\nx1=[math.log10(i) for i in r]\nplt.plot(x1,train_error,label='train MSE')\nplt.plot(x1,cv_error,label='CV MSE')\nplt.scatter(x1,train_error)\nplt.scatter(x1,cv_error)\nplt.legend()\nplt.xlabel('log of learning rate')\nplt.ylabel('Mean Squared Error')\nplt.title('log(learning rate) vs MSE')\nplt.grid()\nplt.show()","1a016c2c":"# running implemented SGD Classifier with obtained optimal learning rate\nw,b=CustomGradientDescentRegressor(train_data,learning_rate=0.001,n_itr=1000)\ny_pred=predict(x_test,w,b)\nplot_(y_test,y_pred)","e13030eb":"# Errors in implemeted model\nprint(mean_squared_error(y_test,y_pred))\nprint(mean_absolute_error(y_test,y_pred))","ade41b81":"# weight vector obtained from impemented SGD Classifier\ncustom_w=w\ncustom_w","b3babe2d":"from prettytable import PrettyTable\n# MSE = mean squared error\n# MAE =mean absolute error\nx=PrettyTable()\nx.field_names=['Model','Weight Vector','MSE','MAE']\nx.add_row(['sklearn',sklearn_w,mean_squared_error(y_test, clf_.predict(x_test)),mean_absolute_error(y_test, clf_.predict(x_test))])\nx.add_row(['custom',custom_w,mean_squared_error(y_test,y_pred),(mean_absolute_error(y_test,y_pred))])\nprint(x)","81213115":"sklearn_pred=clf_.predict(x_test)\nimplemented_pred=y_pred\nx=PrettyTable()\nx.field_names=['SKLearn SGD predicted value','Implemented SGD predicted value']\nfor itr in range(15):\n    x.add_row([sklearn_pred[itr],implemented_pred[itr]])\nprint(x)   ","1caadef4":"# SGD implementation of Linear regression","10eee278":"[1]Initialize interation no.,intersept value and weight vector.                         \n[2]while current iteration is not total no. of iteration .                                   \n[3]     for all items in batch.                                                     \n[4]       calculate weighted vector and intercept value.                                                \n[5]update weighted vector and intercept values by reducing from old values .   \n[6]update iteration number.\n[7]stop when current iteration > total iteration or weight vectors of two sucessive iterations are same.                        \n","e7f1b8fb":"**Reference(s):**","1759c0f9":"# Custom Implementation","08d7b87b":"**Notes(s):**","f1e4a931":"# Data Preprocessing:","5a0312c2":"# SGD on Linear Regression : SKLearn Implementation","4260c138":"# Comparing Models","cd406182":"# SGD with optimal learning rate","36c844c1":"1. The predicted values between two implementations are almost similar.\n2. The SGD classifier is implmented with batch size of 20 and a learning rate of 0.001 without any regularization term.\n","6684c0fd":"**Comparison Between top 15 predicted value of both models:**","709b4cb9":"**Pseudocode:**","bff25b2b":"[1]https:\/\/machinelearningmastery.com\/implement-linear-regression-stochastic-gradient-descent-scratch-python\/\n[2]https:\/\/www.kaggle.com\/premvardhan\/stocasticgradientdescent-implementation-lr-python","b48b900e":"# Hyper Parameter tunning for optimal Learning rate"}}