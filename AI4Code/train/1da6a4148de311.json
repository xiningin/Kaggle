{"cell_type":{"90b7a719":"code","1bc508ae":"code","20ec05e3":"code","57d6cf5d":"code","9b0dab83":"code","fc9070a6":"code","512d43a8":"code","f968f822":"code","1aef868b":"code","c49ecca6":"code","8ef26aa0":"code","15604b78":"code","af48d543":"code","8d19bb2d":"code","06ba7b13":"code","e783b529":"code","a682d57d":"code","235e79f1":"code","32d21e9f":"code","41a6dd3d":"markdown","586332f2":"markdown","d841f584":"markdown","4f5a259b":"markdown","a0aeef9d":"markdown","70dffe45":"markdown","0ff9f4b4":"markdown","1383df9f":"markdown","15588522":"markdown","c1e31a71":"markdown","703522cc":"markdown","ceef785f":"markdown","b1daac07":"markdown","d1b0f5ca":"markdown","0d0bc98a":"markdown","f7899814":"markdown","81acf2dd":"markdown","6f299ce0":"markdown","a8e472f7":"markdown","77942ad6":"markdown","e501dd10":"markdown","08b901c3":"markdown","81ca6f4e":"markdown"},"source":{"90b7a719":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1bc508ae":"train = pd.read_feather('..\/input\/training-data-to-feather-python-r-low-mem\/train.feather')","20ec05e3":"obs = train.shape[0]\nprint(f\"number of observations: {obs}\")","57d6cf5d":"time_steps, assets = train.time_id.nunique(), train.investment_id.nunique()\nprint(f\"number of assets: {assets} \\t time steps: {time_steps}\")","9b0dab83":"print(f\"number of assets: {assets} (range from {train.investment_id.min()} to {train.investment_id.max()})\")","fc9070a6":"obs_by_asset = train.groupby(['investment_id'])['target'].count()\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nobs_by_asset.plot.hist(bins=60)\nplt.title(\"target by asset distribution\")\nplt.show()","512d43a8":"train[['investment_id', 'time_id']].plot.scatter('time_id', 'investment_id', figsize=(20, 30), s=0.5)\nplt.show()","f968f822":"mean_target = train.groupby(['investment_id'])['target'].mean()\nmean_mean_target = np.mean(mean_target)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nmean_target.plot.hist(bins=60)\nplt.title(\"mean target distribution\")\nplt.show()\n\nprint(f\"Mean of mean target: {mean_mean_target: 0.5f}\")","1aef868b":"sts_target = train.groupby(['investment_id'])['target'].std()\nmean_std_target = np.mean(sts_target)\n\nfig, ax = plt.subplots(1, 1, figsize=(12, 6))\nsts_target.plot.hist(bins=60)\nplt.title(\"standard deviation of target distribution\")\nplt.show()\n\nprint(f\"Mean of std target: {mean_std_target: 0.5f}\")","c49ecca6":"ax = sns.jointplot(x=obs_by_asset, y=mean_target, kind=\"reg\", \n                   height=8, joint_kws={'line_kws':{'color':'red'}})\nax.ax_joint.set_xlabel('observations')\nax.ax_joint.set_ylabel('mean target')\nplt.show()","8ef26aa0":"qx = sns.jointplot(x=obs_by_asset.values, y=sts_target, kind=\"reg\", \n                   height=8, joint_kws={'line_kws':{'color':'red'}})\nax.ax_joint.set_xlabel('observations')\nax.ax_joint.set_ylabel('std target')\nplt.show()","15604b78":"fig, ax = plt.subplots(1, 1, figsize=(12, 6))\ntrain.groupby('time_id')['investment_id'].nunique().plot()\nplt.title(\"number of unique assets by time\")\nplt.show()","af48d543":"plt.figure(figsize=(12, 6))\n\nplt.subplot(3, 1, 1,)\n(train.groupby('time_id')['investment_id'].nunique()).plot()\nplt.title(\"number of unique assets by time\")\n\nplt.subplot(3, 1, 2)\ntrain.groupby('time_id')['target'].mean().plot()\nplt.title(\"average target by time\")\nplt.axhline(y=mean_mean_target, color='r', linestyle='--', label=\"mean\")\nplt.legend(loc='lower left')\n\nplt.subplot(3, 1, 3)\ntrain.groupby('time_id')['target'].std().plot()\nplt.title(\"std of target by time\")\nplt.axhline(y=mean_std_target, color='r', linestyle='--', label=\"mean\")\nplt.legend(loc='lower left')\n\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=1.3, \n                    wspace=0.4, \n                    hspace=0.4)\n\nplt.show()","8d19bb2d":"r = np.corrcoef(train.groupby('time_id')['investment_id'].nunique(), train.groupby('time_id')['target'].mean())[0][1]\nprint(f\"Correlation of number of assets by target: {r:0.3f}\")","06ba7b13":"time2target_mean = train.groupby(['time_id'])['target'].mean()\ntime2target_std = train.groupby(['time_id'])['target'].std()\n\n_, axes = plt.subplots(1, 1, figsize=(24, 12))\nplt.fill_between(\n        time2target_mean.index,\n        time2target_mean - time2target_std,\n        time2target_mean + time2target_std,\n        alpha=0.1,\n        color=\"b\",\n    )\nplt.plot(\n        time2target_mean.index, time2target_mean, \"o-\", color=\"b\", label=\"Training score\"\n    )\nplt.axhline(y=mean_mean_target, color='r', linestyle='--', label=\"mean\")\naxes.set_ylabel(\"target\")\naxes.set_xlabel(\"time\")\nplt.show()","e783b529":"time2target_mean = train.groupby(['time_id'])['target'].mean()\ntime2target_std = train.groupby(['time_id'])['target'].std()\n\n_, axes = plt.subplots(1, 1, figsize=(24, 12))\nplt.fill_between(\n        time2target_mean.index,\n        time2target_mean - time2target_std,\n        time2target_mean + time2target_std,\n        alpha=0.1,\n        color=\"b\",\n    )\nplt.plot(\n        time2target_mean.index, time2target_mean, \"o-\", color=\"b\", label=\"Training score\"\n    )\nplt.axhline(y=mean_mean_target, color='r', linestyle='--', label=\"mean\")\n\nasset = 70\nplt.plot(train[train.investment_id==asset].time_id,\n               train[train.investment_id==asset].target, '.')\n\naxes.set_ylabel(\"target\")\naxes.set_xlabel(\"time\")\nplt.show()","a682d57d":"obs_by_asset = train.groupby(['investment_id'])['target'].count().to_dict()\ntarget = train.investment_id.copy().replace(obs_by_asset).astype(np.int16)\nfeatures = train.columns[4:]\n\ndel(obs_by_asset)","235e79f1":"corrs = list()\nfor col in features:\n    corr = np.corrcoef(target, train[col])[0][1]\n    corrs.append(corr)\n    \ndel(target)","32d21e9f":"feat_importances = pd.Series(corrs, index=features)\nfeat_importances.nlargest(20).plot(kind='barh', figsize=(12, 6)).invert_yaxis()\nplt.show()","41a6dd3d":"The new scatterplot reveals that the less the observations, imply a much more uncertainty in the mean target. Since the mechanism o ","586332f2":"As we have reasoned how the investments with less observations seem more risky, we notice how the number of the assets present at each time step is quite different and also highly oscillating. By the end of the avaliable time, the number of assets has grown by one third.","d841f584":"# Hypothesis: we can get a proxy of the count of obs by asset based on the features?","4f5a259b":"Assets are distributed in a different way, there are assets that are actually more frequently observed and others that are not. **A good cv and modelling strategy should keep this into account** (stratify if you are working with subsamples).","a0aeef9d":"By jointly plotting the distribution of observartions by asset and the mean target value by asset, we may notice that the target value slightly reduces proportionally to the number of observation. The dispersion of values tends to grow with less observations, hence we need to re-plot the scatterplot this time using the standard deviation.","70dffe45":"**Strategy**: in training you need to control this effect by expliciting the number of observations because this is predictive of the uncertainty of the predictions. In the test phase, instead, when you are working with an asset that you don't know about, you need to impute an average number of observations, thus expecting an average dispersion of predictions for that asset.","0ff9f4b4":"# Target analysis","1383df9f":"Given the large number of features and groups present in the data, gaining insight on how the data is structured will bring an advantage when modelling it (hint: don't rush to build massive models treating all the data in the same way).","15588522":"## If liked the notebook and you found it useful, please consider to upvote :-)","c1e31a71":"Clearly the target is detrended and forced to mean zero and unit standard deviation, though at times when the number of assets is reduced the average changes and consequently shifts also the confidence interval. Being able to figure out such times of mean shift in the test set could be quite advantageous.","703522cc":"Basically, this chart is the key. The task of the competition is to find out the position of an asset in a day. Is the asset near the average or how much is far away from it (you are predicting volatility, basically). In fact the evaluation is based on the mean of the Pearson correlation coefficient for each time ID.\n\nIn the following chart we are overimposing the target for asset 70 with the market average and the unit standard deviation band.\n\nClearly the position of asset 70 depends on its performance but also on the way the mean and standard deviation for that period_id is calculated (are we analyzing the volatility inside a basket of investment, maybe?).","ceef785f":"*Version history and highlights*\n\nvers. 01 : adding a first target analysis and making the notebook public\n\nvers. 02 : added more comparative plots\n\nvers. 03 : added a scatter of time and assets and a brief investigation on features\n\nvers. 04 : more on relationship between target and time","b1daac07":"Getting an idea of how many observations, assets and time steps","d1b0f5ca":"**Strategy**: now your cv strategy should be clear, you have to do groupkfold on the time_id, keeping all the assets realtive to a time_id or in train or in validation.","0d0bc98a":"The average of mean target by asset show a bell-shaped distribution, beware that there are outliers, anyway, because there are some assets with quite negative average target (-0.4 area) and some quite positive ones (+0.8 area). Overall the average mean target by asset is slightly negative (-0.0231)","f7899814":"Also the average of mean standard deviation (std) by asset presents some interesting patterns. First of all, it is skewed toward the right, with some assets having more std (up to 2.5). On the other side there are also some few assets with std almost at zero.","81acf2dd":"If you more carefully look at the fabric of assets by time, you will notice, quite a few discontinuitites and that the discontinuities are more present in the first section of the time.","6f299ce0":"The range of assets is more extended than the number fo assets themselves. In fact, assets will change in part in the test set, therefore you have to consider strategies for handling the ones in the training set in a generalizable way.\nSee: https:\/\/www.kaggle.com\/c\/ubiquant-market-prediction\/discussion\/301693#1656092","a8e472f7":"Actually based on a few features, the fact that an asset has less or more observations should be quite predictable based on features with high correlation. ","77942ad6":"# Introduction","e501dd10":"If we plot the number of assets by time alongside the average target by time, it becomes evident that when there are less assets, the target oscillates more with prevalently higher targets. The correlation of assets number and target is negative, in fact. I wonder if we are modelling the asset allocation strategies alongside the markets. ","08b901c3":"Let's now observe more closely the relationship between target and time:","81ca6f4e":"# A first look over the data"}}