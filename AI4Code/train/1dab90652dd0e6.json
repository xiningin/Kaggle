{"cell_type":{"1ecde984":"code","315c5834":"code","b884e58d":"code","2c865e25":"code","eec1e4ad":"code","d4578286":"code","d864a840":"code","60e70e6c":"code","83bec027":"code","03256514":"code","62d77bc7":"code","8f98def0":"code","86cb9e7a":"code","b15dcd6a":"code","71212f30":"code","0391b839":"code","ccc2761b":"code","ce548f60":"code","70d1e530":"code","08cb3ad4":"code","cc6cba7f":"code","7f64e2db":"code","a15f2555":"code","bbd78abf":"code","dd9010fc":"markdown","1cdf27bc":"markdown","dafe301d":"markdown","b8962f74":"markdown","5e78adf9":"markdown","95ec2f8f":"markdown","e4aa8d4f":"markdown","959af020":"markdown","aaececb6":"markdown","edcd3f54":"markdown","c99dd65d":"markdown","30e13e54":"markdown","7fda21e3":"markdown","357be0d5":"markdown","4d0d1b29":"markdown","e46f3bc7":"markdown","d539fed5":"markdown","7208d5d9":"markdown","082ff8e1":"markdown","26dd8643":"markdown","0586b8ff":"markdown","e1d6d2e2":"markdown","974e4d31":"markdown"},"source":{"1ecde984":"#Importing modules\nfrom __future__ import print_function, division\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport time\nimport copy\nfrom PIL import Image\nimport os\nimport matplotlib.pyplot as plt \nimport numpy as np \nimport pandas as pd ","315c5834":"#If you run this cell in  as a Kaggle's notebook:\nDATASET_PATH = \"\/kaggle\/input\/fashion-product-images-dataset\/fashion-dataset\/fashion-dataset\/\"\nprint(os.listdir(DATASET_PATH))","b884e58d":"# Reading the rows and dropping the ones with errors\ndf = pd.read_csv(DATASET_PATH + \"styles.csv\", nrows=44416, error_bad_lines=False)\ndf['image'] = df.apply(lambda row: str(row['id']) + \".jpg\", axis=1)\ndf = df.reset_index(drop=True)\ndf.head(5)","2c865e25":"plt.figure(figsize=(7,20))\ndf.articleType.value_counts().sort_values().plot(kind='barh')","eec1e4ad":"N_Pictures = 250\nN_Classes = np.sum(df.articleType.value_counts().to_numpy() > N_Pictures)\n#Number of classes with sufficient images to train on:\nN_Classes","d4578286":"#Inspecting the item classes that made it to our new dataset\ntemp = df.articleType.value_counts().sort_values(ascending=False)[:N_Classes]\ntemp[-5:]","d864a840":"#Saving item types(labels) with their counts\nitems_count = temp.values\nitems_label = temp.index.tolist()","60e70e6c":"#Creating new dataframes for training\/validation\ndf_train = pd.DataFrame(columns=['articleType','image'])\ndf_val   = pd.DataFrame(columns=['articleType','image'])\n\n\nfor ii in range(0,N_Classes):\n    \n    #print(items_label[ii])\n    \n    temp = df[df.articleType==items_label[ii]].sample(N_Pictures)\n\n    df_train = pd.concat([df_train, temp[ :int(N_Pictures*0.6) ][['articleType','image']] ]            , sort=False)\n    df_val   = pd.concat([df_val,   temp[  int(N_Pictures*0.6): N_Pictures ][['articleType','image']] ], sort=False)\n\ndf_train.reset_index(drop=True)\ndf_val.reset_index(drop=True)","83bec027":"#Create folders for new dataset\nos.chdir(r'\/kaggle\/working\/')\nos.mkdir('data')\nos.mkdir('data\/train')\nos.mkdir('data\/val')\nos.chdir(r'\/kaggle\/working\/')\n\ndata = {'train': df_train, 'val': df_val}\n\n# and save each individual image to the new directory\nfor x in ['train','val']:\n    \n    print(x)\n    \n    for label, file in data[x].values:\n        \n        try:\n            img = Image.open(DATASET_PATH+'images\/'+file)\n        except FileNotFoundError:\n            # If file does not exist continue\n            continue\n            \n        #Else save file to new directory  \n        try:\n            img.save('data\/'+x+'\/'+label+'\/'+file) \n\n        except FileNotFoundError:\n            #If folder does not exist, create one and save the image\n            os.mkdir('data\/'+x+'\/'+label)\n            img.save('data\/'+x+'\/'+label+'\/'+file)\n            print(label,end=' ')\n    ","03256514":"#Inspect if all the folders have been created\n%ls data\/train\/","62d77bc7":"# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        #transforms.RandomResizedCrop(224),\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\ndata_dir = 'data'\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","8f98def0":"def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])","86cb9e7a":"def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","b15dcd6a":"def visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images\/\/2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)","71212f30":"model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\n\n#Changing the number of outputs in the last layer to the number of different item types\nmodel_ft.fc = nn.Linear(num_ftrs, len(class_names))\n\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)","0391b839":"model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=5)","ccc2761b":"#Saving our model's weights: \n\n%mkdir model\ntorch.save(model_ft.state_dict(), 'model\/model_fine_tuned.pt')\n%ls \n\n#Download the model weights and save them locally\nfrom IPython.display import FileLink\nFileLink(r'model\/model_fine_tuned.pt')","ce548f60":"visualize_model(model_ft)","70d1e530":"model_conv = torchvision.models.resnet18(pretrained=True)\n# Freezing the weights\nfor param in model_conv.parameters():\n    param.requires_grad = False\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_ftrs, len(class_names))\n\nmodel_conv = model_conv.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that only parameters of final layer are being optimized as\n# opposed to before.\noptimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)","08cb3ad4":"model_conv = train_model(model_conv, criterion, optimizer_conv,\n                         exp_lr_scheduler, num_epochs=15)","cc6cba7f":"#Saving our model's weights: \n\n%mkdir model\ntorch.save(model_conv.state_dict(), 'model\/model_fixed_feature.pt')\n%ls model\/\n\nfrom IPython.display import FileLink\nFileLink(r'model\/model_fixed_feature.pt')","7f64e2db":"visualize_model(model_conv)\n\nplt.ioff()\nplt.show()","a15f2555":"import shutil\nimport requests\n\n%mkdir test_data\n\n#Download the images from internet:\nurls = [r'https:\/\/media.karousell.com\/media\/photos\/products\/2017\/02\/19\/bnib_ecco_formal_shoes_brown_1487494013_3659d7da.jpg',\n        r'https:\/\/17pprhpagc13i5210btdmqmf-wpengine.netdna-ssl.com\/wp-content\/uploads\/2020\/06\/wallets-vegancom.jpg',\n        r'https:\/\/www.tudorwatch.com\/-\/media\/model-assets\/wrist\/l\/tudor-m79030n-0001.jpg',\n        r'https:\/\/hips.hearstapps.com\/hmg-prod.s3.amazonaws.com\/images\/gettyimages-1256162242.jpg?crop=0.889xw:1.00xh;0,0&resize=640:*',\n        r'https:\/\/cdn.shopify.com\/s\/files\/1\/0285\/2916\/4420\/products\/P8501_20PORTER_20JEAN_20MODEL_20_D_1024x1024.jpg?v=1584339217',\n        r'https:\/\/images1.novica.net\/pictures\/26\/p350363_2.jpg',\n        r'https:\/\/cdn.shopify.com\/s\/files\/1\/0021\/6331\/0691\/products\/DP0423HS_43ddebc5-5e53-4dd6-af00-76ff2970bbd2_grande.jpg?v=1586232410',\n        r'https:\/\/goodhousekeeping.fetcha.co.za\/wp-content\/uploads\/2019\/03\/sweat.jpg',\n        r'https:\/\/sneakernews.com\/wp-content\/uploads\/2020\/08\/undefeated-kobe-5-protro-gold-purple-DA6809-700-3.jpg']\n\nfor ii, url in enumerate(urls):\n    response = requests.get(url, stream=True)\n    with open('test_data\/img_test_' + str(ii) + '.jpg', 'wb') as out_file:\n        shutil.copyfileobj(response.raw, out_file)\n    del response\n    \n%ls test_data","bbd78abf":"from PIL import Image\n\nmodel = model_ft\n\n\nmodel.eval()\nfig = plt.figure(figsize=(10,18))\n\nwith torch.no_grad():\n\n    for ii, file_name in enumerate(os.listdir( 'test_data' )):\n        img = Image.open( 'test_data' + '\/' + file_name)\n        img_t = data_transforms['val'](img).unsqueeze(0)\n        img_t = img_t.to(device)\n\n        outputs = model(img_t)\n        _, preds = torch.max(outputs, 1)\n        \n        ax = plt.subplot(len(os.listdir( 'test_data' )),3, ii+1)\n        ax.axis('off')\n        ax.set_title('predicted: {}'.format(class_names[int(preds.cpu().numpy())]))\n        plt.imshow( np.array(img) )\n        ","dd9010fc":"## Data collection <a name=\"data\"><\/a>","1cdf27bc":"**Train and evaluate**\n\nIt will take about 30-40 min to go through 6 epoch (using GPU).","dafe301d":"**Visualizing the image count over all the product types (or classes)**","b8962f74":"## Testing on different kind of images  <a name=\"introduction\"><\/a>\n\nLet's test our model on diferent kind of images (for example those with a colorfull background).","5e78adf9":"### Visualize a few images\n\n#### Let\u2019s visualize a few training images so as to understand the data augmentations.","95ec2f8f":"## Introduction: What is Transfer learning? <a name=\"introduction\"><\/a>\n\n\nTransfer learning is a method of training a new Neutal Network model by using pretrained model's weights and architecture (or slightly adjusting it to our needs). This allows our model to use already learned knowledge of the donor network on different dataset.\n\n<a href=\"https:\/\/www.researchgate.net\/figure\/Illustrations-of-transfer-learning-a-neural-network-is-pretrained-on-ImageNet-and_fig1_336874848\"><img src=\"https:\/\/www.researchgate.net\/publication\/336874848\/figure\/fig1\/AS:819325225144320@1572353764073\/Illustrations-of-transfer-learning-a-neural-network-is-pretrained-on-ImageNet-and.png\" alt=\"Illustrations of transfer learning: a neural network is pretrained on ImageNet and subsequently trained on retinal, OCT, X-ray images, B-scans for different disease classifications\"\/><\/a>\n\\*\\* picture taken from: *Xu, Jie & Xue, Kanmin & Zhang, Kang. (2019). Current status and future trends of clinical diagnoses via image-based deep learning. Theranostics. 9. 7556-7565. 10.7150\/thno.38065.*\n\n\nThere are two major transfer learning techniques:\n\n* **Finetuning the convnet:** Instead of random initializaion, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual.\n\n\n* **ConvNet as fixed feature extractor:** Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.\n\n\n**We will use both techniques and analyse wich one is better performing on our dataset.**","e4aa8d4f":"As we can see there is 37 item types with the sufficient number of images. So essentially we have 37x150=**5550** images for training and 37x100=**3700** images for validation tasks.","959af020":"\\*\\* As we can see, the model started to overfit after epoch number 3 ( the accuracy gain after each epoch on the validation dataset is miniscule compared to the training). The best validation accuracy is 0.893 (it could be improve by increasing the size of the training data).","aaececb6":"### Finetuning the convnet\n\n#### Load a pretrained model and reset final fully connected layer.","edcd3f54":"**As we can see our model correctly described 7 out of 9 images not included in the dataset (with other than white background). Not bad at all**","c99dd65d":"# Fashion items classification with Pytorch and transfer learning\n\n![Test Image 4](https:\/\/635.gtbank.com\/wp-content\/uploads\/2017\/09\/Must-have.jpg)\n\n\nIn this project we will use [Kaggle's fashion items dataset](www.kaggle.com) and retrain a ResNet18 architecture model using two transfer learning techniques and choosing the best performing one. Our classification model will be capable of classifying 37 different fashion item types: \n* Backpacks, Belts, Bra, Briefs, \n* Caps, Casual Shoes, Clutches, \n* Deodorant, Dresses, \n* Earrings, \n* Flats, Flip Flops, Formal Shoes, \n* Handbags, Heels, \n* Jackets, Jeans, \n* Kurtas, \n* Lipstick, \n* Nail Polish, \n* Perfume and Body Mist, \n* Sandals, Sarees, Shirts, Shorts, Socks, Sports Shoes, Sunglasses, Sweaters, Sweatshirts, \n* Ties, Tops, Track Pants, Trousers, Tshirts, \n* Wallets, Watches,\n\n\n\n## Table of contents\n* [Introduction](#introduction)\n* [Data collection](#data)\n* [Model training](#training)\n* [Testing on different kind of images](#testing)\n* [Useful links](#links)","30e13e54":"* [A Gentle Introduction to Transfer Learning for Deep Learning](https:\/\/machinelearningmastery.com\/transfer-learning-for-deep-learning\/)\n* [A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning](https:\/\/towardsdatascience.com\/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)\n* [Pytorch's official transfer learning tutorial](https:\/\/pytorch.org\/tutorials\/beginner\/transfer_learning_tutorial.html)\n","7fda21e3":"## Useful links <a name=\"links\"><\/a>\n","357be0d5":"### Loading the New Dataset\n\nWe will use **torch.utils.data** packages ( *Dasatest* and *Dataloaders*) to load the new subset. We also need to transform our images into 224x224 px resolution and normalize the chanel values (the colors) suitable for the ResNet architecture. It can be easily done by exploiting the **torchvision.transforms** module.","4d0d1b29":"### Visualizing the model predictions\n\n#### Generic function to display predictions for a few images","e46f3bc7":"\\*\\* Otherwise you will need to use your Kaggle's API keys to download and use this dataset (More info [here](https:\/\/www.kaggle.com\/general\/74235)).","d539fed5":"So let's create separate folders for every item type that has at least 250 images in the original dataset and store them there (a new subset). Separating them for **Trainig** and **Validation** tasks.\n\nIt will take around 10-15 min on the Kaggle's notebook.","7208d5d9":"**Creating a new dataset**\n\nFor ease of use in the later sections (loading images using pytorchs **Dataset** and **Dataloader** modules) we will create a our own sub dataset out of the original one.\n\nWe will choose items with at least 250 images (150 for training and 100 for validation tasks) in the original dataset.","082ff8e1":"**As mentione above we will use [Kaggle's Fashion Product Images dataset](https:\/\/www.kaggle.com\/paramaggarwal\/fashion-product-images-dataset)** to train our newly model.\n\nFirst of all lets inpect the dataset.","26dd8643":"## Training the model <a name=\"training\"><\/a>\n\nLets define our iterative learning process as a function with nested loops(epochs) inside:","0586b8ff":"\\*\\* We can see, that the accuracy of this method saturates in the range of 0.87 for the training and 0.84 for the validation datasets. This limitation is a consequence of freezing the feature extraction part of our model and just rellying on the optimization of the classifier part.\n\nIn such a case we will use the former model instead (trained by finetuning all the weights).","e1d6d2e2":"**Train and evaluate**\n\nIt will take about 40+ min to go through 15 epoch (using GPU).","974e4d31":"### ConvNet as fixed feature extractor:"}}