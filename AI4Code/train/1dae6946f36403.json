{"cell_type":{"704deb12":"code","927faf70":"code","910faf5e":"code","5a0046a0":"code","4fe362fa":"code","8c8d6ff7":"code","4889c2cc":"code","e746ada9":"code","3dec33e1":"code","0b744915":"code","b2581de9":"code","b1bcf2cc":"code","a18a5453":"code","c15e16f4":"code","4ec7abb1":"code","48f5b26c":"code","0a186dd1":"code","0e638ed2":"code","0fd80e46":"code","08fca7d7":"code","04176b75":"code","7813efdf":"code","ef4f4f1e":"code","bd67aca5":"code","610f9088":"code","57b815a1":"code","1ddb8fe6":"code","6a75542c":"code","ec2359e5":"code","67fd01e1":"code","1732208d":"code","724e37f3":"code","4dc520a6":"code","a09d4abd":"code","3ec94fbf":"code","f249a289":"code","f17afcdf":"code","7b98db0b":"code","275d37c3":"code","9a551923":"code","c4fa69bc":"code","76e56178":"code","31b23224":"code","8ae74aac":"code","b6a0772e":"code","f6ba410c":"code","5cd92af1":"code","0b6cb1e9":"code","a63cf096":"code","c972bd6a":"code","c1c22887":"code","c7098d9f":"code","5c6897c4":"code","aee6e295":"code","0ddd6a07":"code","6d38046c":"code","6353aa2d":"code","0635344d":"code","715280d8":"code","311de82e":"code","df05629c":"code","1506ed1f":"code","73bfbecc":"markdown","b683ea59":"markdown","46ba2b9a":"markdown","62953ddf":"markdown"},"source":{"704deb12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy             as np # linear algebra\nimport pandas            as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn           as sns \nimport statsmodels.api   as sm\nimport missingno         as msno\n%matplotlib inline\n\nfrom sklearn.preprocessing      import MinMaxScaler\nfrom sklearn.feature_selection  import RFE\nfrom sklearn.model_selection    import train_test_split\nfrom sklearn.linear_model       import LogisticRegression\nfrom sklearn.metrics            import confusion_matrix,accuracy_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","927faf70":"#importing test, train datasets in enviroment \ndf_train = pd.read_csv(r'..\/input\/titanic\/train.csv')\ndf_test  = pd.read_csv(r'..\/input\/titanic\/test.csv')","910faf5e":"df_train.head()","5a0046a0":"df_test.head()","4fe362fa":"#checking the null values \nmsno.matrix(df_train)","8c8d6ff7":"#Dropping unwanted columns \ncol_drop = ['Cabin','Name','Ticket']\ndf_train.drop(col_drop,axis=1,inplace=True)\ndf_test.drop(col_drop,axis=1,inplace=True)","4889c2cc":"#Checking the null values in the dataset\nprint('Missing age values in train df:',(df_train['Age'].isnull().sum()\/len(df_train))*100)\nprint('Missing age values in test df:' ,(df_test['Age'].isnull().sum()\/len(df_test))*100)","e746ada9":"#Function to visualise any column wrt quantile \ndef describing(df,col_name):\n    return df[col_name].quantile([0.25,0.50,0.75,0.90,0.95,0.96,0.97,0.98,0.99,1.00])","3dec33e1":"print(describing(df_train,'Age'))\nprint(describing(df_test,'Age'))","0b744915":"sns.boxplot(df_train['Age']);","b2581de9":"sns.boxplot(df_test['Age']);","b1bcf2cc":"#Checking how many values are outliers in both the dataframe\nprint('Count of outliers in train dataset for age col is :',df_train[df_train['Age']>62]['Age'].count())\nprint('Count of outliers in test  dataset for age col is :',df_test[df_test['Age']>62]['Age'].count())","a18a5453":"#Imputing the null values in the dataset \ngrouping = df_train.groupby(['Sex', 'Pclass'])['Age'].agg(['median']).round(1)\ngrouping","c15e16f4":"#Function to impute the null values \ndef imputing(sex,p,n):\n    df_train.loc[(df_train['Age'].isnull()) & (df_train['Pclass'] == p) & (df_train['Sex']==sex),'Age'] = grouping.iloc[n,0]","4ec7abb1":"#Imputing with median values\nimputing('female',1,0)\nimputing('female',2,1)\nimputing('female',3,2)\nimputing('male',1,3)\nimputing('male',2,4)\nimputing('male',3,5)","48f5b26c":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer(strategy='most_frequent')\nimputed_train_data = pd.DataFrame(my_imputer.fit_transform(df_train))\nimputed_train_data.columns = df_train.columns","0a186dd1":"df_train.head()","0e638ed2":"#Creating the dummy values for embarbed & sex columns \ndummy_col = ['Sex','Embarked']\ndummy1 = pd.get_dummies(imputed_train_data[dummy_col],drop_first=True)\ndummy2 = pd.get_dummies(df_test[dummy_col],drop_first=True)","0fd80e46":"imputed_train_data.drop(dummy_col,axis=1,inplace=True)\ndf_test.drop(dummy_col,axis=1,inplace=True)","08fca7d7":"#Concatenating the dataframe & dropping the orignal columns \nimputed_train_data = pd.concat([imputed_train_data,dummy1],axis=1)\ndf_test = pd.concat([df_test,dummy2],axis=1)","04176b75":"df_test.shape","7813efdf":"scaler = MinMaxScaler()\nscale_col = ['Age','Fare']\nimputed_train_data[scale_col] = scaler.fit_transform(imputed_train_data[scale_col])\ndf_test[scale_col] = scaler.transform(df_test[scale_col])","ef4f4f1e":"imputed_train_data.head()","bd67aca5":"#Dropping passenger ID \nimputed_train_data.drop('PassengerId',axis=1,inplace=True)\ndf_test.drop('PassengerId',axis=1,inplace=True)","610f9088":"imputed_train_data.info()","57b815a1":"df_test.head()","1ddb8fe6":"#Converting the columns into int\ncol_type = ['Survived','Pclass','SibSp','Parch']\nimputed_train_data[col_type] = imputed_train_data[col_type].astype('int')\n","6a75542c":"#Function to test logistic regression \n#function to test the logistic Regression model \ndef validating_lr(y_real,y_pred):\n    from sklearn.metrics import confusion_matrix, accuracy_score\n    print('Confusion Matrix')\n    confusion = confusion_matrix(y_pred,y_real)\n    print(confusion)\n    print('\\n')\n    print('Accuracy Score',(accuracy_score(y_pred,y_real)*100))\n    TP = confusion[1,1] # true positive \n    TN = confusion[0,0] # true negatives\n    FP = confusion[0,1] # false positives\n    FN = confusion[1,0] # false negatives\n    print('\\n')\n    print('Sensitivity:',(TP \/ float(TP+FN)*100))\n    print('\\n')\n    print('specificity:',(TN \/ float(TN+FP)*100))\n    print('\\n')\n    print('false postive rate - predicting 1 when its 0:',(FP\/ float(TN+FP)*100))\n    print('\\n')\n    print('Positive predictive value:',(TP \/ float(TP+FP)*100))\n    print('\\n')\n    print('Negative predictive value:',(TN \/ float(TN+ FN)*100))","ec2359e5":"#Function to create a table with pred values for logistic regression \ndef prediction(model_name,x_test,y_test,threshold):\n    y_pred                        = model_name.predict(x_test)\n    y_pred_final                  = pd.DataFrame({'op_train_Prob':y_pred})\n    y_pred_final['train_op']      = y_test\n    y_pred_final['op_train_pred'] = y_pred_final['op_train_Prob'].apply(lambda x:1 if x>threshold else 0)\n    return y_pred_final","67fd01e1":"#Function to check the VIF of the df\ndef vif_validation(X_train):\n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    # Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n    vif = pd.DataFrame()\n    vif['Features'] = X_train.columns\n    vif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif","1732208d":"def RFE_validation(X_train,y_train,parameters):\n    # Running RFE with the output number of the variable equal to 10\n    lm = LogisticRegression()\n    lm.fit(X_train, y_train)\n    rfe = RFE(lm,parameters)             \n    rfe = rfe.fit(X_train, y_train)\n    total_features = list(zip(X_train.columns,rfe.support_,rfe.ranking_))\n    col_selected   = X_train.columns[rfe.support_]\n    col_rejected   = X_train.columns[~rfe.support_]\n    print('List of columns selected = ',col_selected)\n    print('\\n')\n    print('List of columns rejected = ',col_rejected)\n    print('\\n')\n    print('List of total columns    = ',total_features)","724e37f3":"def impute(df):\n    from sklearn.impute import SimpleImputer\n    my_imputer           = SimpleImputer(strategy='most_frequent')\n    imputed_data         = pd.DataFrame(my_imputer.fit_transform(df))\n    imputed_data.columns = df.columns\n    return imputed_data","4dc520a6":"y_train = imputed_train_data['Survived']\nX_train = imputed_train_data.drop('Survived',axis=1).copy()","a09d4abd":"imputed_train_data['Survived'].value_counts(normalize=True)*100","3ec94fbf":"X_train_sm = sm.add_constant(X_train)\ny_train = y_train.values.reshape(-1,1)","f249a289":"#Training the dataset & fitting the model \nlr_model1 = sm.GLM(y_train,X_train_sm,family=sm.families.Binomial())\nlr_model  = lr_model1.fit()\nprint(lr_model.summary())","f17afcdf":"vif = vif_validation(X_train)\nvif.head()","7b98db0b":"#Testing the model on train dataset \ny_pred = prediction(lr_model,X_train_sm,y_train,0.5)\nvalidating_lr(y_pred['train_op'],y_pred['op_train_pred'])","275d37c3":"RFE_validation(X_train,y_train,5);","9a551923":"#Cleaning the test data set \ndf_test.isnull().sum()","c4fa69bc":"#Columns to be dropped \ndrop_col = ['Pclass','Embarked_S','Embarked_Q']\nX_train_sm.drop(drop_col,axis=1,inplace=True)\ndf_test.drop(drop_col,axis=1,inplace=True)","76e56178":"#Training the dataset & fitting the model\nlr_model1 = sm.GLM(y_train,X_train_sm,family=sm.families.Binomial())\nlr_model  = lr_model1.fit()\nprint(lr_model.summary())","31b23224":"#Testing the model on train dataset \ny_pred = prediction(lr_model,X_train_sm,y_train,0.65)\nvalidating_lr(y_pred['train_op'],y_pred['op_train_pred'])","8ae74aac":"y_pred.head()","b6a0772e":"#Function to get the probablities for all possible threshold\ndef optimum_threshold(y_pred):\n    numbers = [float(x)\/10 for x in range(10)]\n    for i in numbers:\n        y_pred[i]= y_pred.op_train_Prob.map(lambda x: 1 if x > i else 0)\n    return y_pred","f6ba410c":"y_pred = optimum_threshold(y_pred)\ny_pred.head()","5cd92af1":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ndef optimum_accuracy(df,op):\n    cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n    from sklearn.metrics import confusion_matrix\n\n    # TP = confusion[1,1] # true positive \n    # TN = confusion[0,0] # true negatives\n    # FP = confusion[0,1] # false positives\n    # FN = confusion[1,0] # false negatives\n\n    num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n    for i in num:\n        cm1              = confusion_matrix(df[op],df[i] )\n        total1           = sum(sum(cm1))\n        accuracy         = (cm1[0,0]+cm1[1,1])\/total1\n        speci            = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n        sensi            = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n        cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n    return cutoff_df","0b6cb1e9":"cutoff_df = optimum_accuracy(y_pred,'train_op')\ncutoff_df","a63cf096":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ndef Roc_plotting(df):\n    df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n    plt.show()","c972bd6a":"Roc_plotting(cutoff_df)","c1c22887":"#Testing the model on train dataset \ny_pred = prediction(lr_model,X_train_sm,y_train,0.30)\nvalidating_lr(y_pred['train_op'],y_pred['op_train_pred'])","c7098d9f":"#Fitting our final model using sklearn \nlr          = LogisticRegression()\nfinal_model = lr.fit(X_train_sm,y_train)","5c6897c4":"df_test = sm.add_constant(df_test)","aee6e295":"imputed_test_df = impute(df_test)","0ddd6a07":"lr_predict = final_model.predict(imputed_test_df)","6d38046c":"test = pd.read_csv(r'..\/input\/titanic\/test.csv')","6353aa2d":"final_sub = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':lr_predict})","0635344d":"final_sub.head()","715280d8":"final_sub.to_csv('submission.csv', index=False)","311de82e":"pip install kaggle --upgrade","df05629c":"mkdir .kaggle","1506ed1f":"kaggle competitions {list, files, download, submit, submissions, leaderboard}\nkaggle datasets {list, files, download, create, version, init}\nkaggle kernels {list, init, push, pull, output, status}\nkaggle config {view, set, unset}","73bfbecc":"1. Importing dataset & analysing it ","b683ea59":"3. Preprocessing the data ","46ba2b9a":"4. Training the model & evalution","62953ddf":"2. Data cleaning & preparation "}}