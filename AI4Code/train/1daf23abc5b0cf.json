{"cell_type":{"3576f9f2":"code","e05ee48b":"code","c4e4d458":"code","d86bfeb6":"code","c0883d6c":"code","b2f3255b":"code","c38963a6":"code","b9e9e3e0":"code","74bca4d3":"code","1c6136a4":"code","85011000":"code","38852a08":"code","8a567d0f":"code","30930626":"code","ed72e748":"code","85af90a1":"code","1be1ab1a":"code","ebd3a630":"code","e7f4f054":"code","ca3609f9":"code","37a7ad72":"code","cd73f5f0":"code","93cd6f8b":"code","9b6f87e0":"code","2b299749":"code","47b87c39":"code","85d71022":"code","0fb96346":"code","88bced89":"code","6e246409":"code","2ee1af49":"code","c9e52fe3":"code","0e860c2d":"code","82c90c52":"code","6202b1ff":"code","58857e22":"code","29c9cc82":"code","8b7c260a":"markdown","bc71101c":"markdown","1e4dd1be":"markdown","74153448":"markdown","bdd2050c":"markdown","1ccbf2fd":"markdown","78b59b31":"markdown","0ade83ff":"markdown","fdbe65ec":"markdown","ba3320fd":"markdown","fd6c995f":"markdown","f39d1923":"markdown","28139f5b":"markdown","46110a9a":"markdown","ce287484":"markdown","9cc71660":"markdown","e250f888":"markdown","c77b2ca7":"markdown","d5ed3caa":"markdown","74e4d778":"markdown","fd7ff9d7":"markdown","06e0346b":"markdown","8b688b17":"markdown","4fa1d3a5":"markdown","9d70630d":"markdown","4fb3e953":"markdown","54dead1b":"markdown","8b93bb03":"markdown","7c747e72":"markdown","f4c75fbf":"markdown","2c8faed9":"markdown","ea17e5c1":"markdown","3aff2581":"markdown","d7b4a092":"markdown"},"source":{"3576f9f2":"# Change this to True to replicate the result\nCOMPLETE_RUN = False\n#True\u4ee3\u8868\u8fd0\u884c\u5168\u90e8\u6570\u636e\u96c6\uff0cFalse\u4ee3\u8868\u8fd0\u884c\u90e8\u5206\u6570\u636e\u96c6","e05ee48b":"import numpy as np#\u77e9\u9635\u8fd0\u7b97\nnp.random.seed(1001)#\u968f\u673a\u79cd\u5b50\n\nimport os#\u7cfb\u7edf\u547d\u4ee4\nimport shutil#\u6587\u4ef6\u64cd\u4f5c\n\nimport IPython#\u4ea4\u4e92\u5f0f\u7f16\u7a0b\nimport matplotlib#\u7ed8\u56fe\nimport matplotlib.pyplot as plt\nimport pandas as pd#\u7279\u5f81\u5de5\u7a0b\nimport seaborn as sns#\u7ed8\u56fe\nfrom tqdm import tqdm_notebook#\u8fdb\u5ea6\u6761\nfrom sklearn.model_selection import StratifiedKFold#\u5206\u5c42\u91c7\u6837\u4ea4\u53c9\u5207\u5206\n\n%matplotlib inline\n#\u5c06\u56fe\u5185\u5d4c\u5230notebook\nmatplotlib.style.use('ggplot')#\u6837\u5f0f\u7f8e\u5316","c4e4d458":"train = pd.read_csv(\"..\/input\/freesound-audio-tagging\/train.csv\")#pandas \u8bfbcsv:\u8bfb\u8bad\u7ec3\u96c6\ntest = pd.read_csv(\"..\/input\/freesound-audio-tagging\/sample_submission.csv\")#pandas \u8bfbcsv:\u8bfb\u63d0\u4ea4\u6837\u672c","d86bfeb6":"train.head()#pandas head:\u8bfb\u524d\u9762\u51e0\u884c\n#\u63cf\u8ff0\u4e86\u6bcf\u4e2awav\u6587\u4ef6\u5bf9\u5e94\u7684ID\uff0c\u4ee5\u53ca\u5b83\u7684\u5206\u7c7b\uff0c\u8fd8\u6709\u8be5\u5206\u7c7b\u6807\u6ce8\u662f\u5426\u7ecf\u8fc7\u4eba\u5de5\u5ba1\u67e5\uff0c\u5982\u4e0b\uff1a","c0883d6c":"print(\"Number of training examples=\", train.shape[0], \"  Number of classes=\", len(train.label.unique()))\n#train.shape[0]\u6709\u591a\u5c11\u884c\n#len(train.label.unique())\u6709\u591a\u5c11\u4e2a\u72ec\u4e00\u65e0\u4e8c\u7684\u7c7b","b2f3255b":"print(train.label.unique())#\u6253\u5370\u51fa\u72ec\u4e00\u65e0\u4e8c\u7684\u7c7b\u540d","c38963a6":"category_group = train.groupby(['label', 'manually_verified']).count()\n#pandas groupby : \u6839\u636e\u6807\u7b7e\u548c\u662f\u5426\u4eba\u5de5\u9a8c\u8bc1\u5c06\u8bad\u7ec3\u96c6\u5206\u7ec4\uff0c\u7136\u540e\u8ba1\u7b97\u6bcf\u4e2a\u5206\u7ec4\u7684\u6570\u91cf\n\nplot = category_group.unstack().   reindex(category_group.unstack().sum(axis=1).sort_values().index)\\\n          .plot(kind='bar', stacked=True, title=\"Number of Audio Samples per Category\", figsize=(16,10))\n#pandas unstack : \u9ed8\u8ba4\u5c06category_group.index.names[-1] (\u5373manually_verified)\u4f5c\u4e3a\u5217\u540d\uff0c\u5bf9series\u4e0d\u5806\u79ef\u6210pandas\n#pandas sum(axis=1) : \u5bf9\u5217\u805a\u5408\u6c42\u548c(\u5373\u53d8\u4e3a1\u5217)\n#pandas sort_values : \u6392\u5e8f\uff0c\u9ed8\u8ba4\u5347\u5e8f\n#pandas index : \u5f97\u5230\u7d22\u5f15\u5217\u8868\n#pandas reindex : \u91cd\u5efa\u7d22\u5f15\u3002\n#category_group.unstack().reindex(category_group.unstack().sum(axis=1).sort_values().index)\uff1a\u542b\u4e49\u5982\u4e0b\n#\u5c06category_group\u4e0d\u5806\u79ef\uff0c\u7136\u540e\u6839\u636e\u6bcf\u4e2a\u7c7b\u522b\u7684\u6570\u76ee\u505a\u5347\u5e8f\u6392\u5e8f\n#\\\u662f\u6362\u884c\u7b26\n#pandas plot : \u753b\u67f1\u72b6\u56febar\uff0c stacked\u662f\u5c06\u4e24\u5217\u5806\u6210\u4e00\u5217 , title\u6807\u9898 \uff0c figsize\u56fe\u50cf\u5927\u5c0f\n\nplot.set_xlabel(\"Category\")#x\u8f74\u6807\u7b7e\nplot.set_ylabel(\"Number of Samples\");#y\u8f74\u6807\u7b7e","b9e9e3e0":"print('Minimum samples per category = ', min(train.label.value_counts()))\n# min(train.label.value_counts()) \uff1a train\u7684label\u5217\u4e2d\u6bcf\u4e2a\u7c7b\u7684\u6570\u76ee\u7684\u6700\u5c0f\u503c\nprint('Maximum samples per category = ', max(train.label.value_counts()))\n# max(train.label.value_counts()) \uff1a train\u7684label\u5217\u4e2d\u6bcf\u4e2a\u7c7b\u7684\u6570\u76ee\u7684\u6700\u5927\u503c","74bca4d3":"import IPython.display as ipd  # To play sound in the notebook \u5c55\u793a\nfname = '\/kaggle\/input\/freesound-audio-tagging\/audio_train\/audio_train\/' + '00044347.wav'   # Hi-hat \u9f13\nipd.Audio(fname)#\u64ad\u653e\u97f3\u9891","1c6136a4":"# Using wave library\nimport wave#\u89e3\u6790wav\u6587\u4ef6\nwav = wave.open(fname)#\u6253\u5f00wav\u6587\u4ef6\nprint(\"Sampling (frame) rate = \", wav.getframerate())#\u5e27\u7387\nprint(\"Total samples (frames) = \", wav.getnframes())#\u603b\u5e27\u6570\nprint(\"Duration = \", wav.getnframes()\/wav.getframerate())#\u65f6\u95f4","85011000":"# Using scipy\u79d1\u5b66\u8ba1\u7b97\nfrom scipy.io import wavfile#\u8bfb\u53d6wav\u6587\u4ef6\nrate, data = wavfile.read(fname)#\u8bfb\u53d6wav\u6587\u4ef6\uff0c\u8fd4\u56de\u91c7\u6837\u7387(\u5e27\u7387)\u548cnumpy\u5f62\u5f0f\u7684\u6570\u636e\nprint(\"Sampling (frame) rate = \", rate)\nprint(\"Total samples (frames) = \", data.shape)#\u6570\u636e\u7ef4\u5ea6\nprint(data)#\u6253\u5370\u6570\u636e","38852a08":"plt.plot(data, '-', );\n#matplotlib\u7ed8\u56fe\u3002\u9f13\u58f0\u4ece\u5927\u5230\u5c0f","8a567d0f":"plt.figure(figsize=(16, 4))#\u753b\u5e03\u5927\u5c0f\nplt.plot(data[:500], '.'); #matplotlib plot . :\u524d500\u4e2a\u6570\u636e\u7684\u6563\u70b9\u56fe\nplt.plot(data[:500], '-');#matplotlib plot - :\u524d500\u4e2a\u6570\u636e\u7684\u6298\u7ebf\u56fe","30930626":"train['nframes'] = train['fname'].apply(lambda f: wave.open('\/kaggle\/input\/freesound-audio-tagging\/audio_train\/audio_train\/' + f).getnframes())\n#pandas apply \uff1a \u5bf9\u8bad\u7ec3\u96c6\u7684fname\u5217\u4e0b\u6bcf\u4e2a\u5143\u7d20\u505a\u4e00\u4e2a\u8fd0\u7b97\uff1a lambda\u533f\u540d\u51fd\u6570\uff0c\u5c06\u6bcf\u4e2a\u5143\u7d20\u6240\u5bf9\u5e94\u7684wav\u6587\u4ef6\u6253\u5f00\uff0c\u8fd4\u56de\u6bcf\u4e2a\u97f3\u9891\u7684\u603b\u5e27\u6570\u3002\ntest['nframes'] = test['fname'].apply(lambda f: wave.open('\/kaggle\/input\/freesound-audio-tagging\/audio_test\/audio_test\/' + f).getnframes())\n#pandas apply \uff1a \u5bf9\u6d4b\u8bd5\u96c6\u7684fname\u5217\u4e0b\u6bcf\u4e2a\u5143\u7d20\u505a\u4e00\u4e2a\u8fd0\u7b97\uff1a lambda\u533f\u540d\u51fd\u6570\uff0c\u5c06\u6bcf\u4e2a\u5143\u7d20\u6240\u5bf9\u5e94\u7684wav\u6587\u4ef6\u6253\u5f00\uff0c\u8fd4\u56de\u6bcf\u4e2a\u97f3\u9891\u7684\u603b\u5e27\u6570\u3002\n\n_, ax = plt.subplots(figsize=(16, 4))\n#\u521b\u5efa\u5b50\u56fe\u7684\u516c\u5171\u5e03\u5c40\nsns.violinplot(ax=ax, x=\"label\", y=\"nframes\", data=train)\n#seaborn violinplot \u63d0\u7434\u56fe\nplt.xticks(rotation=90)\n#matplotlib xticks : x\u8f74\u523b\u5ea6\u65cb\u8f6c90\u5ea6\nplt.title('Distribution of audio frames, per label', fontsize=16)\n#matplotlib title \uff1a \u753b\u51fa\u6807\u9898\nplt.show()\n#\u5c55\u793a","ed72e748":"fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16,5))  \n#matpoltlib subplots :\u521b\u5efa\u4e00\u7cfb\u5217\u7684\u5b50\u56fe  nrows\u5b50\u56fe\u884c\u7684\u4e2a\u6570\uff0cncols\u5b50\u56fe\u5217\u7684\u4e2a\u6570 ,figsize\u753b\u5e03\u5c3a\u5bf8, axes\u5b50\u56fe\u7684\u8f74\uff0c fig\u56fe\ntrain.nframes.hist(bins=100, ax=axes[0])\n#pandas hist \uff1a \u8bad\u7ec3\u96c6\u7684\u603b\u5e27\u6570\u7684\u76f4\u65b9\u56fe\uff0cbins\u662f\u5206\u6876\u6570\uff0c ax\u662f\u753b\u5728\u67d0\u4e2a\u5b50\u56fe\u7684\u8f74\u4e0a0\u4ee3\u8868\u7b2c\u4e00\u4e2a\u5b50\u56fe\u7684\u8f74\ntest.nframes.hist(bins=100, ax=axes[1])\n#pandas hist \uff1a \u6d4b\u8bd5\u96c6\u7684\u603b\u5e27\u6570\u7684\u76f4\u65b9\u56fe\uff0cbins\u662f\u5206\u6876\u6570\uff0c ax\u662f\u753b\u5728\u67d0\u4e2a\u5b50\u56fe\u7684\u8f74\u4e0a1\u4ee3\u8868\u7b2c\u4e8c\u4e2a\u5b50\u56fe\u7684\u8f74\nplt.suptitle('Frame Length Distribution in Train and Test', ha='center', fontsize='large');\n#\u6807\u9898","85af90a1":"abnormal_length = [707364, 353682, 138474, 184338]\n\nfor length in abnormal_length:\n    abnormal_fnames = test.loc[test.nframes == length, 'fname'].values\n    #DataFrame.values : \u53ea\u8fd4\u56deDataFrame\u4e2d\u7684\u503c\uff0c\u4e0d\u8fd4\u56de\u8f74\u7684\u6807\u7b7e\n    #test.nframes == length \uff1a pandas boolean index\n    #pandas loc \uff1a \u5b9a\u4f4d\uff0c \u83b7\u53d6\u5f02\u5e38\u957f\u5ea6\u7684\u97f3\u9891\u7684\u6587\u4ef6\u540d\n    \n    print(\"Frame length = \", length, \" Number of files = \", abnormal_fnames.shape[0], end=\"   \")\n    #\u5f02\u5e38\u957f\u5ea6  \u6709\u8fd9\u4e2a\u5f02\u5e38\u957f\u5ea6\u7684\u6587\u4ef6\u6570\u91cf  \n    \n    fname = np.random.choice(abnormal_fnames)\n    #np random choice : \u968f\u673a\u53d6\u6837\u4e00\u6b21\n    \n    print(\"Playing \", fname)\n    #\u6253\u5370\u51fa\u8fd9\u4e2a\u968f\u673a\u6837\u672c\u7684\u540d\u5b57\n    IPython.display.display(ipd.Audio( '..\/input\/freesound-audio-tagging\/audio_test\/audio_test\/' + fname))\n    #\u5c55\u793a\u8fd9\u4e2a\u968f\u673a\u97f3\u9891\u6837\u672c","1be1ab1a":"import librosa#\u97f3\u9891\u5904\u7406\u5e93\nimport numpy as np#\u77e9\u9635\u8fd0\u7b97\nimport scipy#\u79d1\u5b66\u8ba1\u7b97\nfrom keras import losses, models, optimizers\n#keras Model:\u6cdb\u578b\u6a21\u578b\uff0c\u5373\u5e7f\u4e49\u7684\u62e5\u6709\u8f93\u5165\u548c\u8f93\u51fa\u7684\u6a21\u578b\n#keras losses:\u635f\u5931\u51fd\u6570\n#keras optimizers: \u4f18\u5316\u5668\nfrom keras.activations import relu, softmax\n#keras relu softmax:\u6fc0\u6d3b\u51fd\u6570\n\nfrom keras.callbacks import (EarlyStopping, LearningRateScheduler,\n                             ModelCheckpoint, TensorBoard, ReduceLROnPlateau)\n#\u56de\u8c03\u51fd\u6570\uff0c\u5728\u8bad\u7ec3\u65f6\u4f7f\u7528\uff0c\u67e5\u770b\u6a21\u578b\u72b6\u6001\u548c\u7edf\u8ba1\u3002\n#EarlyStopping\u5f53\u8bc4\u4ef7\u6307\u6807\u4e0d\u518d\u63d0\u5347\u65f6\uff0c\u51cf\u5c11\u5b66\u4e60\u7387\n#LearningRateScheduler\u5b66\u4e60\u901f\u7387\u5b9a\u65f6\u5668\n#ModelCheckpoint\u6a21\u578b\u68c0\u67e5\u70b9\n#TensorBoard\u53ef\u89c6\u5316\u5de5\u5177\n#ReduceLROnPlateau\u5f53\u6807\u51c6\u8bc4\u4f30\u505c\u6b62\u63d0\u5347\u65f6\uff0c\u964d\u4f4e\u5b66\u4e60\u901f\u7387\u3002\n\nfrom keras.layers import (Convolution1D, Dense, Dropout, GlobalAveragePooling1D, \n                          GlobalMaxPool1D, Input, MaxPool1D, concatenate)\n#keras layers: keras\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\n#keras Convolution1D:\u4e00\u7ef4\u5377\u79ef\u5c42\uff08\u5373\u65f6\u57df\u5377\u79ef\uff09\n#keras Dense\uff1a\u5168\u8fde\u63a5\u5c42\uff0cDropout\u968f\u673a\u5931\u6d3b\u5c42\n#keras Dropout:\u968f\u673a\u5931\u6d3b\u5c42\n#keras GlobalAveragePooling1D:\u4e3a\u65f6\u57df\u4fe1\u53f7\u65bd\u52a0\u5168\u5c40\u5e73\u5747\u503c\u6c60\u5316\n#keras Input:\u8f93\u5165\u5c42\n#keras MaxPool1D:\u4e3a\u65f6\u57df\u4fe1\u53f7\u65bd\u52a0\u6700\u5927\u6c60\u5316\n#keras concatenate:\u878d\u5408\u5c42\n\nfrom keras.utils import Sequence, to_categorical\n#keras Sequence :\u5e8f\u5217\u6570\u636e\u7684\u57fa\u7c7b\uff0c\u4f8b\u5982\u4e00\u4e2a\u6570\u636e\u96c6\n#keras to_categorical\uff1a\u5c06\u7c7b\u522b\u5411\u91cf(\u4ece0\u5230nb_classes\u7684\u6574\u6570\u5411\u91cf)\u6620\u5c04\u4e3a\u4e8c\u8fdb\u5236\u7c7b\u522b\u77e9\u9635","ebd3a630":"# \u914d\u7f6e\u7c7b\nclass Config(object):\n    def __init__(self,\n                 sampling_rate=16000, audio_duration=2, n_classes=41,\n                 use_mfcc=False, n_folds=10, learning_rate=0.0001, \n                 max_epochs=50, n_mfcc=20):\n        #\u97f3\u9891\u7684\u9891\u7387\n        self.sampling_rate = sampling_rate\n        #\u97f3\u9891\u7684\u65f6\u957f\n        self.audio_duration = audio_duration\n        #\u97f3\u9891\u7684\u7c7b\u522b\u6570\u76ee\n        self.n_classes = n_classes\n        #\u662f\u5426\u4f7f\u7528mfcc\u6a21\u578b\n        self.use_mfcc = use_mfcc\n        \n        self.n_mfcc = n_mfcc\n        #\u6298\u6570\n        self.n_folds = n_folds\n        #\u5b66\u4e60\u901f\u7387\n        self.learning_rate = learning_rate\n        #\u8f6e\u6570\n        self.max_epochs = max_epochs\n\n        #\u97f3\u9891\u957f\u5ea6\n        self.audio_length = self.sampling_rate * self.audio_duration\n\n        #self.dim\u7ef4\u5ea6\n        if self.use_mfcc:\n            #MFCC\u4e0d\u662f\u4e00\u4e2a\u91c7\u6837\u70b9\u5c31\u8ba1\u7b97\u4e00\u6b21\uff0c\u9700\u8981\/512\n            self.dim = (self.n_mfcc, 1 + int(np.floor(self.audio_length\/512)), 1)\n        else:\n            self.dim = (self.audio_length, 1)","e7f4f054":"# \u5728\u4f7f\u7528keras\u8bad\u7ec3model\u7684\u65f6\u5019\uff0c\u4e00\u822c\u4f1a\u5c06\u6240\u6709\u7684\u8bad\u7ec3\u6570\u636e\u52a0\u8f7d\u5230\u5185\u5b58\u4e2d\uff0c\u7136\u540e\u5582\u7ed9\u7f51\u7edc\uff0c\u4f46\u5f53\u5185\u5b58\u6709\u9650\uff0c\u4e14\u6570\u636e\u91cf\u8fc7\u5927\u65f6\uff0c\u6b64\u65b9\u6cd5\u5219\u4e0d\u518d\u53ef\u7528\u3002\n# \u56e0\u6b64\u6211\u4eec\u51c6\u5907\u6784\u5efa\u4e00\u4e2a\u6570\u636e\u8fed\u4ee3\u5668\u3002\n\nclass DataGenerator(Sequence):\n    #keras Sequence :\u5e8f\u5217\u6570\u636e\u7684\u57fa\u7c7b\uff0c\u4f8b\u5982\u4e00\u4e2a\u6570\u636e\u96c6\n    \n    def __init__(self, config, data_dir, list_IDs, labels=None, \n                 batch_size=64, preprocessing_fn=lambda x: x):\n        self.config = config\n        #\u6570\u636e\u5730\u5740\n        self.data_dir = data_dir\n        #\u6587\u4ef6\u540d\n        self.list_IDs = list_IDs\n        #\u7c7b\u522b\u6807\u7b7e\n        self.labels = labels\n        #\u6279\u6b21\u5927\u5c0f\n        self.batch_size = batch_size\n        #\u9884\u5904\u7406\u51fd\u6570\uff0c\u9ed8\u8ba4\u4e0d\u5904\u7406\n        self.preprocessing_fn = preprocessing_fn\n        #\u5728\u6bcf\u8f6e\u7ed3\u675f\u540e\u8c03\u7528\n        self.on_epoch_end()\n        #\u7ef4\u5ea6\n        self.dim = self.config.dim\n\n    # \u4ee5\u4e0b\u4e24\u4e2a\u51fd\u6570\u662f\u5fc5\u987b\u5728Sequence\u7c7b\u91cc\u5b9e\u73b0\u7684\u65b9\u6cd5\n    # \u8fd4\u56de\u4e00\u5171\u6709\u591a\u5c11\u4e2abatch\n    def __len__(self):\n        #\u8ba1\u7b97\u6570\u636e\/\u6279\u6b21=\u8f6e\u6570\n        return int(np.ceil(len(self.list_IDs) \/ self.batch_size))\n        #np.ceil\u5411\u4e0a\u53d6\u6574\n        \n    # \u8fd4\u56de\u7b2cindex\u4e2abatch\u5185\u7684\u5185\u5bb9\n    def __getitem__(self, index):\n        # \u8fd4\u56de\u67d0\u4e2a\u6279\u6b21\u5728\u5168\u90e8\u6570\u636e\u4e2d\u7d22\u5f15\u7684\u8d77\u6b62\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n        # \u8fd4\u56de\u67d0\u4e2a\u6279\u6b21\u7684\u6587\u4ef6\u540d\u5217\u8868\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        return self.__data_generation(list_IDs_temp)\n\n    def on_epoch_end(self):\n        #\u5728\u6bcf\u8f6e\u7ed3\u675f\u540e\uff0c\u5c06list_IDs\u6587\u4ef6\u540d\u5217\u8868\u4e0e[0,1,2,3,,,]\u5217\u8868\u5bf9\u5e94\n        self.indexes = np.arange(len(self.list_IDs))\n        #numpy arange : \u5728\u7ed9\u5b9a\u95f4\u9694\u5185\u8fd4\u56de\u5747\u5300\u95f4\u9694\u7684\u503c\n\n    def __data_generation(self, list_IDs_temp):\n        #\u6279\u6b21\u6570\u636e\u7684\u957f\u5ea6\n        cur_batch_size = len(list_IDs_temp)\n        \n        X = np.empty((cur_batch_size, *self.dim))\n        #x\u662f\u6a21\u578b\u7684\u8f93\u5165\n        #numpy empty : \u521b\u5efa\u7a7a\u6570\u7ec4\n        # \u8fd9\u91cc\u7684*\u662f\u56e0\u4e3aself.dim\u7c7b\u4f3cshape\uff0c\u662f\u4e00\u4e2a\u591a\u7ef4\u7684\u53c2\u6570\uff0c\u901a\u8fc7*(\u89e3\u5305)\u5f53\u6210\u53c2\u6570\u5217\u8868\u4f20\u9012\u8fdb\u53bb\n\n        input_length = self.config.audio_length\n        #\u8f93\u5165\u957f\u5ea6\n        \n        for i, ID in enumerate(list_IDs_temp):\n            #i\u662f\u5e8f\u53f7\uff0cID\u662f\u5185\u5bb9\n            file_path = self.data_dir + ID\n            #file_path\u662f\u97f3\u9891\u6587\u4ef6\u7684\u8def\u5f84\n            \n            data, _ = librosa.core.load(file_path, sr=self.config.sampling_rate,\n                                        res_type='kaiser_fast')\n            #librosa.core.load : \u8bfb\u53d6\u97f3\u9891\u6587\u4ef6\u5e76\u53d6\u6837\n            #\u8fd4\u56de\u503cdata\u662fnp.ndarray\u5f62\u5f0f\u7684\u97f3\u9891\u5e8f\u5217\u6570\u636e\uff0csr\u662f\u91c7\u6837\u9891\u7387\uff0cres_type\u662f\u5feb\u901f\u7c7b\u578b\u3002\n\n            # Random offset \/ Padding\n            #\u5982\u679c\u97f3\u9891\u957f\u5ea6\u5927\u4e8e\u6a21\u578b\u7684\u8f93\u5165\u957f\u5ea6    \u8fc7\u957f \u4ece\u4e2d\u53d6\u51fa\u6307\u5b9a\u957f\u5ea6\u7684\u97f3\u9891 \n            if len(data) > input_length:\n                max_offset = len(data) - input_length\n                # \u6570\u636e\u957f\u5ea6\u6bd4\u6a21\u578b\u8f93\u5165\u957f\u5ea6\u957f\u591a\u5c11\n                offset = np.random.randint(max_offset)\n                #\u5728(0,max_offset)\u4e4b\u95f4\u968f\u673a\u751f\u6210\u4e00\u4e2a\u6574\u6570\n                data = data[offset:(input_length+offset)]\n                #\u968f\u673a\u622a\u53d6\u4e00\u6bb5\u957f\u5ea6\u4e3ainput_length\u7684\u97f3\u9891\n            else:\n                # \u5982\u679c\u97f3\u9891\u8fc7\u77ed\uff0c\u8865\u6210\u6307\u5b9a\u957f\u5ea6\n                if input_length > len(data):\n                    max_offset = input_length - len(data)\n                    # \u6a21\u578b\u8f93\u5165\u957f\u5ea6\u6bd4\u6570\u636e\u957f\u5ea6\u957f\u591a\u5c11\n                    offset = np.random.randint(max_offset)\n                    #\u5728(0,max_offset)\u4e4b\u95f4\u968f\u673a\u751f\u6210\u4e00\u4e2a\u6574\u6570\n                else:\n                    offset = 0\n                    #\u5982\u679c\u76f8\u7b49\uff0c\u504f\u79fb\u91cf\u4e3a0\n                data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n                #nimpy pad : \u5728\u6570\u7ec4\u524d\u540e\u586b\u5145\u5e38\u6570\u3002\u6570\u7ec4\u539f\u672c\u957f\u5ea6\u662flen(data)\uff0c\u5728\u524d\u9762\u968f\u673a\u586b\u5145offset\u4e2a\u5e38\u6570\uff0c\u5728\u540e\u9762\u968f\u673a\u586b\u5145input_length - len(data) - offset\u4e2a\u5e38\u6570\uff0c\u603b\u957f\u5ea6\u53d8\u4e3ainput_length\n                \n            # Normalization + Other Preprocessing\n            #\u5982\u679c\u4f7f\u7528mfcc\u6a21\u578b\n            #\u6885\u5c14\u9891\u7387\u5012\u8c31\u7cfb\u6570  https:\/\/www.cnblogs.com\/BaroC\/p\/4283380.html\n            if self.config.use_mfcc:\n                data = librosa.feature.mfcc(data, sr=self.config.sampling_rate,\n                                                   n_mfcc=self.config.n_mfcc)\n                #\u4ee5numpy_ndarray\u7684\u5f62\u5f0f\uff0c\u8fd4\u56de\u6885\u5c14\u9891\u7387\u5012\u8c31\u7cfb\u6570\n                data = np.expand_dims(data, axis=-1)\n                #\u5728\u6570\u7ec4\u6700\u540e\u6dfb\u52a0\u4e00\u4e2a\u7ef4\u5ea6\n            else:\n                # \u8fd9\u91cc\u7684preprocessing_fn\u662f\u9884\u5904\u7406\u51fd\u6570\uff0c\u5bf9\u97f3\u9891\u8fdb\u884c\u5f52\u4e00\u5316\u5904\u7406\uff0c\u4ee3\u7801\u5728\u540e\u9762\n                data = self.preprocessing_fn(data)[:, np.newaxis]\n                # [:, np.newaxis] \uff1a \u589e\u6dfb\u4e00\u4e2a\u65b0\u7ef4\u5ea6\u4ee5\u7b26\u5408\u7f51\u7edcInput\u5f62\u72b6\n            X[i,] = data\n            #x\u662f\u6a21\u578b\u7684\u8f93\u5165\n            # \u7b2ci\u4e2aX \u4e3a \u7b2ci\u4e2a\u5904\u7406\u4e4b\u540e\u7684data.\n            \n        # \u5982\u679c\u5e26\u6709label(\u8bf4\u660e\u662f\u8bad\u7ec3\u96c6)\n        if self.labels is not None:\n            y = np.empty(cur_batch_size, dtype=int)\n            #y\u662f\u6bcf\u4e2a\u6279\u6b21\u7684\u6807\u7b7e\n            for i, ID in enumerate(list_IDs_temp):\n                y[i] = self.labels[ID]\n            return X, to_categorical(y, num_classes=self.config.n_classes)\n        #\u5982\u679c\u4e0d\u5e26label(\u8bf4\u660e\u662f\u6d4b\u8bd5\u96c6)\n        else:\n            return X","ca3609f9":"#\u516c\u5f0f\u5982\u4e0a\ndef audio_norm(data):\n    max_data = np.max(data)\n    min_data = np.min(data)\n    data = (data-min_data)\/(max_data-min_data+1e-6)\n    #+1e-6\u662f\u9632\u6b62\u5206\u6bcd\u4e3a0\n    return data-0.5","37a7ad72":"def get_1d_dummy_model(config):\n    # \u4ec5\u7528\u4e8e\u8c03\u8bd5\n    nclass = config.n_classes\n    #\u7c7b\u522b\u4e2a\u6570\n    input_length = config.audio_length\n    #\u6a21\u578b\u8f93\u5165\u957f\u5ea6\n    \n    inp = Input(shape=(input_length,1))\n    #keras Input \uff1a \u6784\u5efa\u6a21\u578b\u8f93\u5165\n    x = GlobalMaxPool1D()(inp)\n    #keras GlobalMaxPool1D : \u65f6\u5e8f\u5168\u5c40\u6700\u5927\u6c60\u5316\n    out = Dense(nclass, activation=softmax)(x)\n    #\u5168\u8fde\u63a5\u5c42\uff0c\u8f93\u51fa\u7ef4\u5ea6\u662fnclass\uff0c\u6fc0\u6d3b\u51fd\u6570\u662fsoftmax,\u8f93\u5165\u662fx\n\n    model = models.Model(inputs=inp, outputs=out)\n    #keras Model:\u5b9e\u4f8b\u5316\u4e00\u4e2a\u6a21\u578b\n    opt = optimizers.Adam(config.learning_rate)\n    #keras Adam :\u8bbe\u7f6e\u4f18\u5316\u7b97\u6cd5\n\n    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n    #keras model compile : \u672c\u51fd\u6570\u7f16\u8bd1\u6a21\u578b\u4ee5\u4f9b\u8bad\u7ec3\n    #opt\u662f\u4f18\u5316\u7b97\u6cd5\uff0closs\u662f\u635f\u5931\u51fd\u6570,\u591a\u7c7b\u7684\u5bf9\u6570\u635f\u5931,metrics\u6307\u6807\u662f\u51c6\u786e\u7387\n    \n    return model\n\ndef get_1d_conv_model(config):\n    \n    nclass = config.n_classes\n    #\u7c7b\u522b\u4e2a\u6570\n    input_length = config.audio_length\n    #\u6a21\u578b\u8f93\u5165\u957f\u5ea6\n    \n    inp = Input(shape=(input_length,1))\n    #\u8bbe\u7f6e\u6a21\u578b\u7684\u8f93\u5165\n    \n    x = Convolution1D(16, 9, activation=relu, padding=\"valid\")(inp)#\u4e00\u7ef4\u5377\u79ef\u5c42\uff08\u5373\u65f6\u57df\u5377\u79ef\uff09\n    x = Convolution1D(16, 9, activation=relu, padding=\"valid\")(x)#\u4e00\u7ef4\u5377\u79ef\u5c42\uff08\u5373\u65f6\u57df\u5377\u79ef\uff09\n    x = MaxPool1D(16)(x)#\u4e3a\u65f6\u57df\u4fe1\u53f7\u65bd\u52a0\u6700\u5927\u6c60\u5316\n    x = Dropout(rate=0.1)(x)#\u968f\u673a\u5931\u6d3b\u5c42\n    \n    x = Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n    x = Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n    x = MaxPool1D(4)(x)\n    x = Dropout(rate=0.1)(x)\n    \n    x = Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n    x = Convolution1D(32, 3, activation=relu, padding=\"valid\")(x)\n    x = MaxPool1D(4)(x)\n    x = Dropout(rate=0.1)(x)\n    \n    x = Convolution1D(256, 3, activation=relu, padding=\"valid\")(x)\n    x = Convolution1D(256, 3, activation=relu, padding=\"valid\")(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dropout(rate=0.2)(x)\n\n    x = Dense(64, activation=relu)(x)#\u5168\u8fde\u63a5\u5c42\n    x = Dense(1028, activation=relu)(x)\n    out = Dense(nclass, activation=softmax)(x)#\u8f93\u51fa\u5c42\n\n    model = models.Model(inputs=inp, outputs=out)\n    #keras Model:\u5b9e\u4f8b\u5316\u4e00\u4e2a\u6a21\u578b\n    opt = optimizers.Adam(config.learning_rate)\n    #keras Adam :\u8bbe\u7f6e\u4f18\u5316\u7b97\u6cd5\n    \n    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n    #keras model compile : \u672c\u51fd\u6570\u7f16\u8bd1\u6a21\u578b\u4ee5\u4f9b\u8bad\u7ec3\n    #opt\u662f\u4f18\u5316\u7b97\u6cd5\uff0closs\u662f\u635f\u5931\u51fd\u6570,\u591a\u7c7b\u7684\u5bf9\u6570\u635f\u5931,metrics\u6307\u6807\u662f\u51c6\u786e\u7387\n    return model","cd73f5f0":"LABELS = list(train.label.unique())\n# \u539f\u59cb\u6807\u7b7e\u5217\u8868\nlabel_idx = {label: i for i, label in enumerate(LABELS)}\n# \u5b57\u5178\u751f\u6210\u5f0f \uff1a \u6784\u5efa\u5b57\u5178  \u6807\u7b7e\uff1a\u5e8f\u53f7\ntrain.set_index(\"fname\", inplace=True)\n#pandas set_index : \u5c06\u8bad\u7ec3\u96c6train\u7684frame\u5217\u8bbe\u7f6e\u4e3a\u7d22\u5f15\u5217\uff0c\u5e76\u4fdd\u5b58\u4fee\u6539\ntest.set_index(\"fname\", inplace=True)\n#pandas set_index : \u5c06\u6d4b\u8bd5\u96c6test\u7684frame\u5217\u8bbe\u7f6e\u4e3a\u7d22\u5f15\u5217\uff0c\u5e76\u4fdd\u5b58\u4fee\u6539\ntrain[\"label_idx\"] = train.label.apply(lambda x: label_idx[x])\n#panda apply : \u5bf9\u6bcf\u4e2a\u5143\u7d20\u64cd\u4f5c\u3002\u7ed9train\u65b0\u5efa\u4e00\u5217label_idx\uff0c\u503c\u662f\u6807\u7b7e\u5bf9\u5e94\u7684\u5e8f\u53f7\u3002\nif not COMPLETE_RUN:#\u5982\u679c\u4e4b\u524d\u7684COMPLETE_RUN\u4e0d\u4e3a\u7a7a\n    train = train[:2000]#\u8bad\u7ec3\u96c6\u53d6\u524d2000\u4e2a\n    test = test[:2000]#\u6d4b\u8bd5\u96c6\u53d6\u524d2000\u4e2a","93cd6f8b":"config = Config(sampling_rate=16000, audio_duration=2, n_folds=10, learning_rate=0.001)\n#\u914d\u7f6e\nif not COMPLETE_RUN:#\u5982\u679cCOMPLETE_RUN\u4e0d\u4e3a\u7a7a\u3002\u8fd9\u91cc\u7f29\u5c0f\u4e86\u6570\u636e\u7684\u89c4\u6a21\u3002\n    config = Config(sampling_rate=100, audio_duration=1, n_folds=2, max_epochs=1)","9b6f87e0":"# ```python\n!export HDF5_USE_FILE_LOCKING=FALSE\n#\u6587\u4ef6\u9501\uff0c\u591a\u7ebf\u7a0b\u65f6\u6ca1\u6709\u8fd9\u53e5\u8bdd\u53ef\u80fd\u4f1a\u62a5\u9519\n\nPREDICTION_FOLDER = \"predictions_1d_conv\"\n#\u9884\u6d4b\u7ed3\u679c\u6587\u4ef6\u5939\n\nif not os.path.exists(PREDICTION_FOLDER): #\u5982\u679c\u4e0d\u5b58\u5728\u8fd9\u4e2a\u6587\u4ef6\u5939\n    os.mkdir(PREDICTION_FOLDER)#\u90a3\u5c31\u521b\u5efa\u8fd9\u4e2a\u6587\u4ef6\u5939\nif os.path.exists('logs\/' + PREDICTION_FOLDER):\n    shutil.rmtree('logs\/' + PREDICTION_FOLDER) #\u5220\u9664\u6587\u4ef6\u5939\n\nskf = StratifiedKFold(n_splits=config.n_folds).split(train, train.label_idx)\n#keras StratifiedKFold: \n# \u5c06\u5168\u90e8\u8bad\u7ec3\u96c6S\u5206\u6210k\u4e2a\u4e0d\u76f8\u4ea4\u7684\u5b50\u96c6\uff0c\u5047\u8bbeS\u4e2d\u7684\u8bad\u7ec3\u6837\u4f8b\u4e2a\u6570\u4e3am\uff0c\u90a3\u4e48\u6bcf\u4e00\u4e2a\u81ea\u5df1\u6709m\/k\u4e2a\u8bad\u7ec3\u6837\u4f8b\uff0c\u76f8\u5e94\u7684\u5b50\u96c6\u4e3a{s1\uff0cs2\uff0c...\uff0csk}\n# \u6bcf\u6b21\u4ece\u5206\u597d\u7684\u5b50\u96c6\u91cc\u9762\uff0c\u62ff\u51fa\u4e00\u4e2a\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\uff0c\u5176\u4ed6k-1\u4e2a\u4f5c\u4e3a\u8bad\u7ec3\u96c6\n# \u5728k-1\u4e2a\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u51fa\u5b66\u4e60\u5668\u6a21\u578b\uff0c\u628a\u8fd9\u4e2a\u6a21\u578b\u653e\u5230\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5f97\u5230\u5206\u7c7b\u7387\u7684\u5e73\u5747\u503c\uff0c\u4f5c\u4e3a\u8be5\u6a21\u578b\u6216\u8005\u5047\u8bbe\u51fd\u6570\u7684\u771f\u5b9e\u5206\u7c7b\u7387\n# skf = StratifiedKFold(n_splits=config.n_folds).split(train, train.label_idx)#\u8fd9\u4e2a\u662f\u65b0\u7248\u672c\u7684\n\nfor i, (train_split, val_split) in enumerate(skf):\n    #i\u662f\u7b2c\u51e0\u6b21\u62c6\u5206\uff0ctrain_split\u662f\u62c6\u5206\u540e\u8bad\u7ec3\u96c6\u7684\u5e8f\u53f7\uff0cval_split\u662f\u62c6\u5206\u540e\u6d4b\u8bd5\u96c6\u7684\u5e8f\u53f7\n    train_set = train.iloc[train_split]\n    #pandas iloc : \u7528\u6574\u6570\u7d22\u5f15\u6765 \u53d6\u503c\u3002\u8fd9\u91cc\u53ef\u4ee5\u5c06\u8bad\u7ec3\u96c6\u4e2d\u7684K\u6298\u62c6\u5206\u8bad\u7ec3\u96c6\u9009\u51fa\u6765\n    val_set = train.iloc[val_split]\n    #pandas iloc : \u7528\u6574\u6570\u7d22\u5f15\u6765 \u53d6\u503c\u3002\u8fd9\u91cc\u53ef\u4ee5\u5c06\u8bad\u7ec3\u96c6\u4e2d\u7684K\u6298\u62c6\u5206\u6d4b\u8bd5\u96c6\u9009\u51fa\u6765\n    checkpoint = ModelCheckpoint('best_%d.h5'%i, monitor='val_loss', verbose=1, save_best_only=True)\n    #keras :\u8be5\u56de\u8c03\u51fd\u6570\u5c06\u5728\u6bcf\u4e2aepoch\u8bad\u7ec3\u7ed3\u675f\u540e   \u81ea\u52a8  \u4fdd\u5b58\u6a21\u578b\u5230filepath\uff0cmonitor\uff1a\u9700\u8981\u76d1\u89c6\u7684\u503c\uff0cverbose\uff1a\u4fe1\u606f\u5c55\u793a\u6a21\u5f0f\uff0c0\u62161\uff0csave_best_only\uff1a\u5f53\u8bbe\u7f6e\u4e3aTrue\u65f6\uff0c\u5c06\u53ea\u4fdd\u5b58\u5728\u9a8c\u8bc1\u96c6\u4e0a\u6027\u80fd\u6700\u597d\u7684\u6a21\u578b\n    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n    #keras :\u5f53\u76d1\u6d4b\u503c\u4e0d\u518d\u6539\u5584\u65f6\uff0c\u8be5\u56de\u8c03\u51fd\u6570\u5c06\u4e2d\u6b62\u8bad\u7ec3.monitor\uff1a\u9700\u8981\u76d1\u89c6\u7684\u91cf.\u5728min\u6a21\u5f0f\u4e0b\uff0c\u5982\u679c\u68c0\u6d4b\u503c\u505c\u6b62\u4e0b\u964d\u5219\u4e2d\u6b62\u8bad\u7ec3.patience\uff1a\u5f53early stop\u88ab\u6fc0\u6d3b\uff08\u5982\u53d1\u73b0loss\u76f8\u6bd4\u4e0a\u4e00\u4e2aepoch\u8bad\u7ec3\u6ca1\u6709\u4e0b\u964d\uff09\uff0c\u5219\u7ecf\u8fc7patience\u4e2aepoch\u540e\u505c\u6b62\u8bad\u7ec3\u3002\n    tb = TensorBoard(log_dir='.\/logs\/' + PREDICTION_FOLDER + '\/fold_%d'%i, write_graph=True)\n    #keras TensorBoard : \u8be5\u56de\u8c03\u51fd\u6570\u662f\u4e00\u4e2a\u53ef\u89c6\u5316\u7684\u5c55\u793a\u5668\u3002log_dir\uff1a\u4fdd\u5b58\u65e5\u5fd7\u6587\u4ef6\u7684\u5730\u5740\uff0c\u8be5\u6587\u4ef6\u5c06\u88abTensorBoard\u89e3\u6790\u4ee5\u7528\u4e8e\u53ef\u89c6\u5316.write_graph:\u662f\u5426\u53ef\u89c6\u5316\u68af\u5ea6\u76f4\u65b9\u56fe\n\n    callbacks_list = [checkpoint, early, tb]\n    #\u56de\u8c03\u51fd\u6570\u53c2\u6570\u5217\u8868\uff0c\u7528\u4e8e\u4e4b\u540e\u4f20\u503c\n    \n    print(\"Fold: \", i)\n    print(\"#\"*50)\n    if COMPLETE_RUN:#\u5927\u8bad\u7ec3\u96c6\n        model = get_1d_conv_model(config)\n    else:#\u5c0f\u8bad\u7ec3\u96c6\n        model = get_1d_dummy_model(config)\n    #\u5b9e\u4f8b\u5316\u4e00\u4e2a\u6570\u636e\u751f\u6210\u7c7b\uff0c\u5f97\u5230K\u6298\u62c6\u5206\u8bad\u7ec3\u96c6\n    train_generator = DataGenerator(config, '..\/input\/freesound-audio-tagging\/audio_train\/audio_train\/', train_set.index, \n                                    train_set.label_idx, batch_size=64,\n                                    preprocessing_fn=audio_norm)\n    #\u5b9e\u4f8b\u5316\u4e00\u4e2a\u6570\u636e\u751f\u6210\u7c7b\uff0c\u5f97\u5230K\u6298\u62c6\u5206\u6d4b\u8bd5\u96c6\n    val_generator = DataGenerator(config, '..\/input\/freesound-audio-tagging\/audio_train\/audio_train\/', val_set.index, \n                                  val_set.label_idx, batch_size=64,\n                                  preprocessing_fn=audio_norm)\n    #\u8bad\u7ec3\n    history = model.fit_generator(train_generator, callbacks=callbacks_list, validation_data=val_generator,\n                                  epochs=config.max_epochs, use_multiprocessing=False, workers=0, max_queue_size=10)\n    #\u539f\u4f5c\u8005\u4f7f\u7528\u7684\u53c2\u6570\u662f\uff1ause_multiprocessing=True\uff0cworkers=6.\u4f46\u662f\u672c\u5730\u8fd0\u884c\u4f1a\u62a5\u9519\uff0c\u9700\u8981\u5bf9\u53c2\u6570\u505a\u8c03\u6574\uff0c\u8fd8\u8981\u628ah5py\u5378\u8f7d\u91cd\u88c52.7.1 \u7248\u672c\u624d\u4e0d\u4f1a\u62a5\u9519\u3002\n    model.load_weights('best_%d.h5'%i)\n    #keras load_weights : \u4ecehdf5\u6587\u4ef6\u4e2d\u52a0\u8f7d\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u5c42\u7684\u6743\u91cd(\u4e0a\u9762\u6709\u4e2a\u81ea\u52a8\u8c03\u7528\u7684\u56de\u8c03\u51fd\u6570\u7528\u4e8e\u4fdd\u5b58\u8bad\u7ec3\u7684\u6743\u91cd)\n    \n    \n    # Save train predictions\u6d4b\u8bd5\n    #\u5b9e\u4f8b\u5316\u4e00\u4e2a\u6570\u636e\u751f\u6210\u7c7b\uff0c\u5f97\u5230\u539f\u8bad\u7ec3\u96c6\n    train_generator = DataGenerator(config, '..\/input\/freesound-audio-tagging\/audio_train\/audio_train\/', train.index, batch_size=128,\n                                    preprocessing_fn=audio_norm)\n    #\u4f7f\u7528\u524d\u9762\u8bad\u7ec3\u7684model\u53bb\u9884\u6d4b\u8bad\u7ec3\u96c6\n    predictions = model.predict_generator(train_generator, use_multiprocessing=True, \n                                          workers=6, max_queue_size=20, verbose=1)\n    #\u7528numpy .npy\u6587\u4ef6\u4fdd\u7559\u9884\u6d4b\u7ed3\u679c\n    np.save(PREDICTION_FOLDER + \"\/train_predictions_%d.npy\"%i, predictions)\n    \n    #\u5b9e\u4f8b\u5316\u4e00\u4e2a\u6570\u636e\u751f\u6210\u7c7b\uff0c\u5f97\u5230\u539f\u6d4b\u8bd5\u96c6\n    test_generator = DataGenerator(config, '..\/input\/freesound-audio-tagging\/audio_test\/audio_test\/', test.index, batch_size=128,\n                                    preprocessing_fn=audio_norm)\n    #\u4f7f\u7528\u524d\u9762\u8bad\u7ec3\u7684model\u53bb\u9884\u6d4b\u6d4b\u8bd5\u96c6\n    predictions = model.predict_generator(test_generator, use_multiprocessing=True, \n                                          workers=6, max_queue_size=20, verbose=1)\n    #\u7528numpy .npy\u6587\u4ef6\u4fdd\u7559\u9884\u6d4b\u7ed3\u679c\n    np.save(PREDICTION_FOLDER + \"\/test_predictions_%d.npy\"%i, predictions)\n    \n    \n    # Make a submission file\u751f\u6210\u63d0\u4ea4\u6587\u4ef6\n    \n    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n    #LABELS -> top_3  :  (2000,41) -> (2000,3)\n    #numpy : \u5728\u6bcf\u884c\uff0c\u628apredictions\u4ece\u5927\u5230\u5c0f\u6392\u5e8f\uff0c\u53d6\u524d\u4e09\u4e2a\u7684\u7d22\u5f15\uff0c\u6839\u636e\u7d22\u5f15\uff0c\u9009\u51faLABELS\u6bcf\u884c\u4e2d\u5bf9\u5e94\u7684\u6570\u636e(\u7c7b\u522b)\u3002\u5373\u9009\u51fa\u6bcf\u4e2a\u6837\u672c\u9884\u6d4b\u6982\u7387\u6700\u5927\u7684\u4e09\u4e2a\u7c7b\u522b\u3002\n    predicted_labels = [' '.join(list(x)) for x in top_3]\n    #python join : \u628atop_3\u6bcf\u884c\u76843\u4e2a\u7c7b\u522b\uff0c\u4ee5\u7a7a\u683c\u9694\u5f00\uff0c\u7ec4\u6210\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002(2000,3) -> (2000,)\n    test['label'] = predicted_labels\n    #pandas : test\u65b0\u589e'label'\u4e00\u5217\uff0c\u6570\u636e\u662fpredicted_labels\n    test[['label']].to_csv(PREDICTION_FOLDER + \"\/predictions_%d.csv\"%i)\n    #\u5c06\u7ed3\u679c\u4fdd\u5b58\u5230.csv\u6587\u4ef6\n    # \u53cc\u5c42\u62ec\u53f7\u8fd4\u56de\u7684\u662fDataFrame\u7684\u5f62\u5f0f\uff08\u5e26label\u7684\u8868\u5934\u4e14\u542bindex\uff09\n    \n#     ```","2b299749":"pred_list = []\n\nfor i in range(2):\n    #\u62ec\u53f7\u4e2d\u7684\u6570\u5b57\u662f\u6570\u636e\u96c6\u62c6\u5206\u7684\u6b21\u6570\n    pred_list.append(np.load(\"..\/input\/freesound-prediction-file\/test_predictions_%d.npy\"%i))\n    #\u53d6\u7b2ci\u4e2a\u6d4b\u8bd5\u7ed3\u679c\uff0c\u8ffd\u52a0\u5230pred_list\u5217\u8868\u4e2d.\u6700\u540e\u7684\u7ef4\u5ea6\u7c7b\u4f3c\u4e8e(2, 2000, 41)\nprediction = np.ones_like(pred_list[0])\n#numpy ones_like : \u8fd4\u56de\u4e0e\u7ed9\u5b9a\u6570\u7ec4\u5177\u6709\u76f8\u540c\u5f62\u72b6(2000, 41)\u548c\u7c7b\u578b\u7684\u75281\u586b\u5145\u7684\u6570\u7ec4\u3002\n\n#\u6c42\u51e0\u4f55\u5e73\u5747\u503c\uff08\u96c6\u6210\u4e4b\u524d\u7684\u591a\u4e2a\u6d4b\u8bd5\u6587\u4ef6\uff09\nfor pred in pred_list:\n    #pred\u7684\u7ef4\u5ea6\u662f(2000, 41)\n    prediction = prediction*pred  #   pred[0]\u4e0epred[1]  \u5bf9\u5e94\u5143\u7d20\u76f8\u4e58\nprediction = prediction**(1.\/len(pred_list))\n# \u51e0\u4f55\u5e73\u5747 https:\/\/en.wikipedia.org\/wiki\/Geometric_mean\n\n# Make a submission file  \u5236\u4f5c\u6700\u540e\u7684\u63d0\u4ea4\u6587\u4ef6\ntop_3 = np.array(LABELS)[np.argsort(-prediction, axis=1)[:, :3]]\n#LABELS -> top_3  :  (2000,41) -> (2000,3)\n#numpy : \u5728\u6bcf\u884c\uff0c\u628apredictions\u4ece\u5927\u5230\u5c0f\u6392\u5e8f\uff0c\u53d6\u524d\u4e09\u4e2a\u7684\u7d22\u5f15\uff0c\u6839\u636e\u7d22\u5f15\uff0c\u9009\u51faLABELS\u6bcf\u884c\u4e2d\u5bf9\u5e94\u7684\u6570\u636e(\u7c7b\u522b)\u3002\n# \u5373\u9009\u51fa\u6bcf\u4e2a\u6837\u672c\u9884\u6d4b\u6982\u7387\u6700\u5927\u7684\u4e09\u4e2a\u7c7b\u522b\u3002\n\npredicted_labels = [' '.join(list(x)) for x in top_3]\n#python join : \u628atop_3\u6bcf\u884c\u76843\u4e2a\u7c7b\u522b\uff0c\u4ee5\u7a7a\u683c\u9694\u5f00\uff0c\u7ec4\u6210\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002(2000,3) -> (2000,)\uff0c \u8fd9\u662f\u6700\u540e\u7684\u63d0\u4ea4\u683c\u5f0f\u54e6\n\n#2000\ntest = pd.read_csv('..\/input\/freesound-audio-tagging\/sample_submission.csv')\n#pandas read_csv : \u8bfb\u53d6kaggle\u7ed9\u7684sample_submission.csv\n\n# test.set_index(\"fname\", inplace=True)\ntest['label'] = predicted_labels\n#test\u65b0\u589e\u4e00\u5217label\uff0c\u6bcf\u4e00\u884c\u7684\u503c\u4e0epredicted_labels\u5bf9\u5e94\n\ntest[['fname', 'label']].to_csv(\"1d_conv_ensembled_submission.csv\", index=False)\n#pandas to_cvs : \u5c06DataFrame\u4fdd\u5b58\u4e3a.csv\u6587\u4ef6","47b87c39":"import librosa\n#\u97f3\u9891\u5904\u7406\u5e93\nSAMPLE_RATE = 44100\n#\u91c7\u6837\u9891\u7387\nfname = '..\/input\/freesound-audio-tagging\/audio_train\/audio_train\/' + '00044347.wav'   # Hi-hat\u9f13\n#\u4e00\u4e2awav\u6587\u4ef6\u7684\u5730\u5740\n\nwav, _ = librosa.core.load(fname, sr=SAMPLE_RATE)\n#librosa.core.load : \u8bfb\u53d6\u97f3\u9891\u6587\u4ef6\u5e76\u53d6\u6837\n#\u8fd4\u56de\u503cdata\u662fnp.ndarray\u5f62\u5f0f\u7684\u97f3\u9891\u5e8f\u5217\u6570\u636e\uff0csr\u662f\u91c7\u6837\u9891\u7387\uff0cres_type\u662f\u5feb\u901f\u7c7b\u578b\u3002\n\nwav = wav[:2*44100]\n#\u53d6\u524d2*44100\u5e27","85d71022":"mfcc = librosa.feature.mfcc(wav, sr = SAMPLE_RATE, n_mfcc=40)\n#\u4ee5numpy_ndarray\u7684\u5f62\u5f0f\uff0c\u8fd4\u56de\u6885\u5c14\u9891\u7387\u5012\u8c31\u7cfb\u6570\nmfcc.shape","0fb96346":"plt.figure(figsize=(16, 4))\nplt.imshow(mfcc, cmap='hot', interpolation='nearest');","88bced89":"from keras.layers import (Convolution2D, GlobalAveragePooling2D, BatchNormalization, Flatten,\n                          GlobalMaxPool2D, MaxPool2D, concatenate, Activation)\n#keras layers: keras\u7684\u795e\u7ecf\u7f51\u7edc\u5c42\n#keras Convolution2D : \u4e8c\u7ef4\u5377\u79ef\u5c42\u5bf9\u4e8c\u7ef4\u8f93\u5165\u8fdb\u884c\u6ed1\u52a8\u7a97\u5377\u79ef\n#keras GlobalAveragePooling2D : \u4e3a\u7a7a\u57df\u4fe1\u53f7\u65bd\u52a0\u5168\u5c40\u5e73\u5747\u503c\u6c60\u5316\n#keras BatchNormalization : \u8be5\u5c42\u5728\u6bcf\u4e2abatch\u4e0a\u5c06\u524d\u4e00\u5c42\u7684\u6fc0\u6d3b\u503c\u91cd\u65b0\u89c4\u8303\u5316\uff0c\u5373\u4f7f\u5f97\u5176\u8f93\u51fa\u6570\u636e\u7684\u5747\u503c\u63a5\u8fd10\uff0c\u5176\u6807\u51c6\u5dee\u63a5\u8fd11\n#keras Flatten : Flatten\u5c42\u7528\u6765\u5c06\u8f93\u5165\u201c\u538b\u5e73\u201d\uff0c\u5373\u628a\u591a\u7ef4\u7684\u8f93\u5165\u4e00\u7ef4\u5316\uff0c\u5e38\u7528\u5728\u4ece\u5377\u79ef\u5c42\u5230\u5168\u8fde\u63a5\u5c42\u7684\u8fc7\u6e21\u3002\n#keras GlobalMaxPool2D : \u4e3a\u7a7a\u57df\u4fe1\u53f7\u65bd\u52a0\u5168\u5c40\u6700\u5927\u503c\u6c60\u5316\n#keras MaxPool2D \uff1a \u4e3a\u7a7a\u57df\u4fe1\u53f7\u65bd\u52a0\u6700\u5927\u503c\u6c60\u5316\n#keras concatenate : \u878d\u5408\u5c42\n#keeas Activation : \u6fc0\u6d3b\u5c42\n\nfrom keras.utils import Sequence, to_categorical\n#keras Sequence :\u5e8f\u5217\u6570\u636e\u7684\u57fa\u7c7b\uff0c\u4f8b\u5982\u4e00\u4e2a\u6570\u636e\u96c6\n#keras to_categorical\uff1a\u5c06\u7c7b\u522b\u5411\u91cf(\u4ece0\u5230nb_classes\u7684\u6574\u6570\u5411\u91cf)\u6620\u5c04\u4e3a\u4e8c\u8fdb\u5236\u7c7b\u522b\u77e9\u9635\n\nfrom keras import backend as K\n#Keras\u540e\u7aef","6e246409":"#\u6d4b\u8bd5\u6a21\u578b\ndef get_2d_dummy_model(config):\n    \n    nclass = config.n_classes\n    #\u7c7b\u522b\u6570\n    inp = Input(shape=(config.dim[0],config.dim[1],1))\n    #keras Input : \u6a21\u578b\u8f93\u5165\n    x = GlobalMaxPool2D()(inp)\n    #keras GlovalMaxPool2D : \u4e3a\u7a7a\u57df\u4fe1\u53f7\u65bd\u52a0\u5168\u5c40\u6700\u5927\u503c\u6c60\u5316\n    out = Dense(nclass, activation=softmax)(x)\n    #keras Dense : \u5168\u8fde\u63a5\u5c42\n    model = models.Model(inputs=inp, outputs=out)\n    #keras Model:\u5b9e\u4f8b\u5316\u4e00\u4e2a\u6a21\u578b\n    opt = optimizers.Adam(config.learning_rate)\n    #keras optimizers : \u4f18\u5316\u5668\n    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n    #keras model compile : \u672c\u51fd\u6570\u7f16\u8bd1\u6a21\u578b\u4ee5\u4f9b\u8bad\u7ec3\n    #opt\u662f\u4f18\u5316\u7b97\u6cd5\uff0closs\u662f\u635f\u5931\u51fd\u6570,\u591a\u7c7b\u7684\u5bf9\u6570\u635f\u5931,metrics\u6307\u6807\u662f\u51c6\u786e\u7387\n    return model\n\n#\u5b9e\u9645\u7528\u7684\u6a21\u578b\ndef get_2d_conv_model(config):\n    \n    nclass = config.n_classes\n    #\u7c7b\u522b\u6570\n    inp = Input(shape=(config.dim[0],config.dim[1],1))\n    #keras Input : \u6a21\u578b\u8f93\u5165\n    x = Convolution2D(32, (4,10), padding=\"same\")(inp)\n    #keras Convolution2D : \u4e8c\u7ef4\u5377\u79ef\u5c42\u5bf9\u4e8c\u7ef4\u8f93\u5165\u8fdb\u884c\u6ed1\u52a8\u7a97\u5377\u79ef\n    x = BatchNormalization()(x)\n    #keras BatchNormalization \uff1a \u8be5\u5c42\u5728\u6bcf\u4e2abatch\u4e0a\u5c06\u524d\u4e00\u5c42\u7684\u6fc0\u6d3b\u503c\u91cd\u65b0\u89c4\u8303\u5316\uff0c\u5373\u4f7f\u5f97\u5176\u8f93\u51fa\u6570\u636e\u7684\u5747\u503c\u63a5\u8fd10\uff0c\u5176\u6807\u51c6\u5dee\u63a5\u8fd11\n    x = Activation(\"relu\")(x)\n    #keras Activation : \u6fc0\u6d3b\u5c42\n    x = MaxPool2D()(x)\n    #keras MaxPool2D : \u4e3a\u7a7a\u57df\u4fe1\u53f7\u65bd\u52a0\u6700\u5927\u503c\u6c60\u5316\n    \n    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    \n    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n    \n    x = Convolution2D(32, (4,10), padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = MaxPool2D()(x)\n\n    x = Flatten()(x)\n    #keras Flatten : Flatten\u5c42\u7528\u6765\u5c06\u8f93\u5165\u201c\u538b\u5e73\u201d\uff0c\u5373\u628a\u591a\u7ef4\u7684\u8f93\u5165\u4e00\u7ef4\u5316\uff0c\u5e38\u7528\u5728\u4ece\u5377\u79ef\u5c42\u5230\u5168\u8fde\u63a5\u5c42\u7684\u8fc7\u6e21\u3002    \n    x = Dense(64)(x)\n    #keras Dense : \u5168\u8fde\u63a5\u5c42\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    out = Dense(nclass, activation=softmax)(x)\n\n    model = models.Model(inputs=inp, outputs=out)\n    opt = optimizers.Adam(config.learning_rate)\n\n    model.compile(optimizer=opt, loss=losses.categorical_crossentropy, metrics=['acc'])\n    return model","2ee1af49":"config = Config(sampling_rate=44100, audio_duration=2, n_folds=10, \n                learning_rate=0.001, use_mfcc=True, n_mfcc=40)\n#\u5b9e\u4f8b\u5316\u914d\u7f6e\u7c7b\nif not COMPLETE_RUN:\n    #\u5982\u679c\u4e0d\u662f\u5b8c\u5168\u8fd0\u884c\uff0c\u51cf\u5c0f\u8bad\u7ec3\u91cf\n    config = Config(sampling_rate=44100, audio_duration=2, n_folds=2, \n                    max_epochs=1, use_mfcc=True, n_mfcc=40)","c9e52fe3":"#\u4e00\u4e9b\u9884\u5904\u7406\u64cd\u4f5c\uff0c\u8fd4\u56de\u6885\u5c14\u9891\u7387\u5012\u8c31\u7cfb\u6570\ndef prepare_data(df, config, data_dir):\n    X = np.empty(shape=(df.shape[0], config.dim[0], config.dim[1], 1))\n    #numpy empty : \u7528\u96f6\u6784\u5efa\u6307\u5b9a\u5f62\u72b6\u7684\u6570\u7ec4\n    input_length = config.audio_length\n    #\u8f93\u5165\u7684\u957f\u5ea6\n    for i, fname in enumerate(df.index):\n        #pandas index : df.index\u662f\u6587\u4ef6\u540d\n        print(fname)\n        file_path = data_dir + fname\n        #\u6587\u4ef6\u7684\u8def\u5f84\n        data, _ = librosa.core.load(file_path, sr=config.sampling_rate, res_type=\"kaiser_fast\")\n        #\u8bfb\u53d6\u97f3\u9891\u6587\u4ef6\u5e76\u53d6\u6837\n        #\u8fd4\u56de\u503cdata\u662fnp.ndarray\u5f62\u5f0f\u7684\u97f3\u9891\u5e8f\u5217\u6570\u636e\uff0csr\u662f\u91c7\u6837\u9891\u7387\uff0cres_type\u662f\u5feb\u901f\u7c7b\u578b\u3002\n\n        # Random offset \/ Padding\n        #\u5982\u679c\u97f3\u9891\u957f\u5ea6\u5927\u4e8e\u6a21\u578b\u7684\u8f93\u5165\u957f\u5ea6    \u8fc7\u957f \u4ece\u4e2d\u53d6\u51fa\u6307\u5b9a\u957f\u5ea6\u7684\u97f3\u9891 \n        if len(data) > input_length:\n            max_offset = len(data) - input_length\n            # \u6570\u636e\u957f\u5ea6\u6bd4\u6a21\u578b\u8f93\u5165\u957f\u5ea6\u957f\u591a\u5c11\n            offset = np.random.randint(max_offset)\n            #\u5728(0,max_offset)\u4e4b\u95f4\u968f\u673a\u751f\u6210\u4e00\u4e2a\u6574\u6570\n            data = data[offset:(input_length+offset)]\n            #\u968f\u673a\u622a\u53d6\u4e00\u6bb5\u957f\u5ea6\u4e3ainput_length\u7684\u97f3\u9891\n        else:\n            # \u5982\u679c\u97f3\u9891\u8fc7\u77ed\uff0c\u8865\u6210\u6307\u5b9a\u957f\u5ea6\n            if input_length > len(data):\n                max_offset = input_length - len(data)\n                # \u6a21\u578b\u8f93\u5165\u957f\u5ea6\u6bd4\u6570\u636e\u957f\u5ea6\u957f\u591a\u5c11\n                offset = np.random.randint(max_offset)\n                #\u5728(0,max_offset)\u4e4b\u95f4\u968f\u673a\u751f\u6210\u4e00\u4e2a\u6574\u6570\n            else:\n                offset = 0\n                #\u5982\u679c\u76f8\u7b49\uff0c\u504f\u79fb\u91cf\u4e3a0\n            data = np.pad(data, (offset, input_length - len(data) - offset), \"constant\")\n            #nimpy pad : \u5728\u6570\u7ec4\u524d\u540e\u586b\u5145\u5e38\u6570\u3002\u6570\u7ec4\u539f\u672c\u957f\u5ea6\u662flen(data)\uff0c\u5728\u524d\u9762\u968f\u673a\u586b\u5145offset\u4e2a\u5e38\u6570\uff0c\u5728\u540e\u9762\u968f\u673a\u586b\u5145input_length - len(data) - offset\u4e2a\u5e38\u6570\uff0c\u603b\u957f\u5ea6\u53d8\u4e3ainput_length\n\n        data = librosa.feature.mfcc(data, sr=config.sampling_rate, n_mfcc=config.n_mfcc)\n        #\u4ee5numpy_ndarray\u7684\u5f62\u5f0f\uff0c\u8fd4\u56de\u6885\u5c14\u9891\u7387\u5012\u8c31\u7cfb\u6570\n        data = np.expand_dims(data, axis=-1)\n        #\u5728\u6570\u7ec4\u6700\u540e\u6dfb\u52a0\u4e00\u4e2a\u7ef4\u5ea6\n        X[i,] = data\n        #x\u662f\u6a21\u578b\u7684\u8f93\u5165\n        #X\u7b2ci\u4e2a\u5143\u7d20 \u4e3a \u7b2ci\u4e2a\u5904\u7406\u4e4b\u540e\u7684data.\n    return X","0e860c2d":"# ```python \u8fd9\u91cc\u4f1a\u751f\u6210\u5f88\u591a\u5e9f\u8bdd\uff0c\u6211\u4e0d\u77e5\u9053\u600e\u4e48\u5173\uff0c\u8010\u5fc3\u7ffb\u5427\u5c11\u5e74\nX_train = prepare_data(train, config, '..\/input\/freesound-audio-tagging\/audio_train\/audio_train\/')\n#\u5f97\u5230\u5904\u7406\u597d\u7684\u8bad\u7ec3\u96c6\ntest.set_index(\"fname\", inplace=True)\n#\u4e0d\u52a0\u4e0a\u9762\u8fd9\u53e5\u8bdd\u4f1a\u62a5\u9519\u3002\n#pandas set_index : \u5c06\u6d4b\u8bd5\u96c6test\u7684frame\u5217\u8bbe\u7f6e\u4e3a\u7d22\u5f15\u5217\uff0c\u5e76\u4fdd\u5b58\u4fee\u6539\nX_test = prepare_data(test, config, '..\/input\/freesound-audio-tagging\/audio_test\/audio_test\/')\n#\u5f97\u5230\u5904\u7406\u597d\u7684\u6d4b\u8bd5\u96c6\ny_train = to_categorical(train.label_idx, num_classes=config.n_classes)\n#\u5c06\u7c7b\u522b\u5411\u91cf(\u4ece0\u5230nb_classes\u7684\u6574\u6570\u5411\u91cf)\u6620\u5c04\u4e3a\u4e8c\u8fdb\u5236\u7c7b\u522b\u77e9\u9635\nfan='julyfan'\nfan\n# ```","82c90c52":"#\u8fd9\u91cc\u5173\u95ed\u6ce8\u91ca\uff0c\u5c31\u662f\u5f52\u4e00\u5316\n# mean = np.mean(X_train, axis=0)#\u5747\u503c\n# std = np.std(X_train, axis=0)#\u65b9\u5dee\n\n# X_train = (X_train - mean)\/std#\u8bad\u7ec3\u96c6\u5f52\u4e00\u5316\n# X_test = (X_test - mean)\/std#\u6d4b\u8bd5\u96c6\u5f52\u4e00\u5316\n","6202b1ff":"# ```python\nPREDICTION_FOLDER = \"predictions_2d_conv\"\n#\u6587\u4ef6\u5939\u7684\u540d\u5b57\nif not os.path.exists(PREDICTION_FOLDER):#\u5982\u679c\u4e0d\u5b58\u5728\u8fd9\u4e2a\u6587\u4ef6\u5939\n    os.mkdir(PREDICTION_FOLDER)#\u90a3\u5c31\u521b\u5efa\u8fd9\u4e2a\u6587\u4ef6\u5939\nif os.path.exists('logs\/' + PREDICTION_FOLDER):\n    shutil.rmtree('logs\/' + PREDICTION_FOLDER)#\u5220\u9664\u76ee\u5f55\n\n# skf = StratifiedKFold(train.label_idx, n_folds=config.n_folds) \u4f5c\u8005\u5199\u7684\u4ee3\u7801\u5df2\u7ecf\u88ab\u79fb\u9664\u4e86\nskf = StratifiedKFold(n_splits=config.n_folds).split(train, train.label_idx)\n# keras StratifiedKFold: \n# \u5c06\u5168\u90e8\u8bad\u7ec3\u96c6S\u5206\u6210k\u4e2a\u4e0d\u76f8\u4ea4\u7684\u5b50\u96c6\uff0c\u5047\u8bbeS\u4e2d\u7684\u8bad\u7ec3\u6837\u4f8b\u4e2a\u6570\u4e3am\uff0c\u90a3\u4e48\u6bcf\u4e00\u4e2a\u81ea\u5df1\u6709m\/k\u4e2a\u8bad\u7ec3\u6837\u4f8b\uff0c\u76f8\u5e94\u7684\u5b50\u96c6\u4e3a{s1\uff0cs2\uff0c...\uff0csk}\n# \u6bcf\u6b21\u4ece\u5206\u597d\u7684\u5b50\u96c6\u91cc\u9762\uff0c\u62ff\u51fa\u4e00\u4e2a\u4f5c\u4e3a\u6d4b\u8bd5\u96c6\uff0c\u5176\u4ed6k-1\u4e2a\u4f5c\u4e3a\u8bad\u7ec3\u96c6\n# \u5728k-1\u4e2a\u8bad\u7ec3\u96c6\u4e0a\u8bad\u7ec3\u51fa\u5b66\u4e60\u5668\u6a21\u578b\uff0c\u628a\u8fd9\u4e2a\u6a21\u578b\u653e\u5230\u6d4b\u8bd5\u96c6\u4e0a\uff0c\u5f97\u5230\u5206\u7c7b\u7387\u7684\u5e73\u5747\u503c\uff0c\u4f5c\u4e3a\u8be5\u6a21\u578b\u6216\u8005\u5047\u8bbe\u51fd\u6570\u7684\u771f\u5b9e\u5206\u7c7b\u7387\n# skf = StratifiedKFold(n_splits=config.n_folds).split(train, train.label_idx)#\u8fd9\u4e2a\u662f\u65b0\u7248\u672c\u7684\n\nfor i, (train_split, val_split) in enumerate(skf):\n    #i\u662f\u7b2c\u51e0\u6b21\u62c6\u5206\uff0ctrain_split\u662f\u62c6\u5206\u540e\u8bad\u7ec3\u96c6\u7684\u5e8f\u53f7\uff0cval_split\u662f\u62c6\u5206\u540e\u6d4b\u8bd5\u96c6\u7684\u5e8f\u53f7\n    K.clear_session()\n    #keras clear_session : \u7ed3\u675f\u5f53\u524d\u7684TF\u8ba1\u7b97\u56fe\uff0c\u5e76\u65b0\u5efa\u4e00\u4e2a\u3002\u6709\u6548\u7684\u907f\u514d\u6a21\u578b\/\u5c42\u7684\u6df7\u4e71\n    X, y, X_val, y_val = X_train[train_split], y_train[train_split], X_train[val_split], y_train[val_split]\n    #K\u6298\u5206\u5272\u8bad\u7ec3\u96c6\u3002\n    checkpoint = ModelCheckpoint('best_%d.h5'%i, monitor='val_loss', verbose=1, save_best_only=True)\n    #keras ModelCheckpoint : \u8be5\u56de\u8c03\u51fd\u6570\u5c06\u5728\u6bcf\u4e2aepoch\u8bad\u7ec3\u7ed3\u675f\u540e  \u81ea\u52a8   \u4fdd\u5b58\u6a21\u578b\u5230filepath\uff0cmonitor\uff1a\u9700\u8981\u76d1\u89c6\u7684\u503c\uff0cverbose\uff1a\u4fe1\u606f\u5c55\u793a\u6a21\u5f0f\uff0c0\u62161\uff0csave_best_only\uff1a\u5f53\u8bbe\u7f6e\u4e3aTrue\u65f6\uff0c\u5c06\u53ea\u4fdd\u5b58\u5728\u9a8c\u8bc1\u96c6\u4e0a\u6027\u80fd\u6700\u597d\u7684\u6a21\u578b\n    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5)\n    #keras EarlyStopping : \u5f53\u76d1\u6d4b\u503c\u4e0d\u518d\u6539\u5584\u65f6\uff0c\u8be5\u56de\u8c03\u51fd\u6570\u5c06\u4e2d\u6b62\u8bad\u7ec3.monitor\uff1a\u9700\u8981\u76d1\u89c6\u7684\u91cf.\u5728min\u6a21\u5f0f\u4e0b\uff0c\u5982\u679c\u68c0\u6d4b\u503c\u505c\u6b62\u4e0b\u964d\u5219\u4e2d\u6b62\u8bad\u7ec3.patience\uff1a\u5f53early stop\u88ab\u6fc0\u6d3b\uff08\u5982\u53d1\u73b0loss\u76f8\u6bd4\u4e0a\u4e00\u4e2aepoch\u8bad\u7ec3\u6ca1\u6709\u4e0b\u964d\uff09\uff0c\u5219\u7ecf\u8fc7patience\u4e2aepoch\u540e\u505c\u6b62\u8bad\u7ec3\u3002\n    tb = TensorBoard(log_dir='.\/logs\/' + PREDICTION_FOLDER + '\/fold_%i'%i, write_graph=True)\n    #keras TensorBoard : \u8be5\u56de\u8c03\u51fd\u6570\u662f\u4e00\u4e2a\u53ef\u89c6\u5316\u7684\u5c55\u793a\u5668\u3002log_dir\uff1a\u4fdd\u5b58\u65e5\u5fd7\u6587\u4ef6\u7684\u5730\u5740\uff0c\u8be5\u6587\u4ef6\u5c06\u88abTensorBoard\u89e3\u6790\u4ee5\u7528\u4e8e\u53ef\u89c6\u5316.write_graph:\u662f\u5426\u53ef\u89c6\u5316\u68af\u5ea6\u76f4\u65b9\u56fe\n    callbacks_list = [checkpoint, early, tb]\n    #\u56de\u8c03\u51fd\u6570\u53c2\u6570\u5217\u8868\uff0c\u7528\u4e8e\u4e4b\u540e\u4f20\u503c\n    print(\"#\"*50)\n    print(\"Fold: \", i)\n    model = get_2d_conv_model(config)\n    #\u8c03\u7528\u81ea\u5b9a\u4e49\u51fd\u6570\uff0c\u6784\u9020\u590d\u6742\u6a21\u578b\n    history = model.fit(X, y, validation_data=(X_val, y_val), callbacks=callbacks_list, \n                        batch_size=64, epochs=config.max_epochs)\n    #\u8bad\u7ec3\n    \n    model.load_weights('best_%d.h5'%i)\n    #keras load_weights : \u4ecehdf5\u6587\u4ef6\u4e2d\u52a0\u8f7d\u6240\u6709\u795e\u7ecf\u7f51\u7edc\u5c42\u7684\u6743\u91cd\n    \n    # Save train predictions\n    #\u4f7f\u7528\u524d\u9762\u8bad\u7ec3\u7684model\u53bb\u9884\u6d4b\u6d4b\u8bd5\u96c6\n    predictions = model.predict(X_train, batch_size=64, verbose=1)\n    #\u7528numpy .npy\u6587\u4ef6\u4fdd\u7559\u9884\u6d4b\u7ed3\u679c\n    np.save(PREDICTION_FOLDER + \"\/train_predictions_%d.npy\"%i, predictions)\n    #\u4f7f\u7528\u524d\u9762\u8bad\u7ec3\u7684model\u53bb\u9884\u6d4b\u6d4b\u8bd5\u96c6\n    # Save test predictions\n    predictions = model.predict(X_test, batch_size=64, verbose=1)\n    #\u7528numpy .npy\u6587\u4ef6\u4fdd\u7559\u9884\u6d4b\u7ed3\u679c\n    np.save(PREDICTION_FOLDER + \"\/test_predictions_%d.npy\"%i, predictions)\n\n    # Make a submission file\u5236\u4f5c\u63d0\u4ef7\u6587\u4ef6\n    top_3 = np.array(LABELS)[np.argsort(-predictions, axis=1)[:, :3]]\n    #LABELS -> top_3  :  (2000,41) -> (2000,3)\n    #numpy : \u5728\u6bcf\u884c\uff0c\u628apredictions\u4ece\u5927\u5230\u5c0f\u6392\u5e8f\uff0c\u53d6\u524d\u4e09\u4e2a\u7684\u7d22\u5f15\uff0c\u6839\u636e\u7d22\u5f15\uff0c\u9009\u51faLABELS\u6bcf\u884c\u4e2d\u5bf9\u5e94\u7684\u6570\u636e(\u7c7b\u522b)\u3002\n    #\u5373\u9009\u51fa\u6bcf\u4e2a\u6837\u672c\u9884\u6d4b\u6982\u7387\u6700\u5927\u7684\u4e09\u4e2a\u7c7b\u522b\u3002\n    \n    predicted_labels = [' '.join(list(x)) for x in top_3]\n    #python join : \u628atop_3\u6bcf\u884c\u76843\u4e2a\u7c7b\u522b\uff0c\u4ee5\u7a7a\u683c\u9694\u5f00\uff0c\u7ec4\u6210\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002(2000,3) -> (2000,)\n    \n    test['label'] = predicted_labels\n    #test\u65b0\u589e\u4e00\u5217label\uff0c\u503c\u4e00\u884c\u7684\u503c\u4e0epredicted_labels\u5bf9\u5e94\n    test[['label']].to_csv(PREDICTION_FOLDER + \"\/predictions_%d.csv\"%i)\n    #pandas to_cvs : \u5c06DataFrame\u4fdd\u5b58\u4e3a.csv\u6587\u4ef6\n# ```","58857e22":"pred_list = []\n\nfor i in range(2):\n    #\u62ec\u53f7\u4e2d\u7684\u6570\u5b57\u662f\u6570\u636e\u96c6\u62c6\u5206\u7684\u6b21\u6570\n    pred_list.append(np.load(\"..\/input\/freesound-prediction-file\/test_predictions_%d.npy\"%i))\n    #\u53d6\u7b2ci\u4e2a\u6d4b\u8bd5\u7ed3\u679c\uff0c\u8ffd\u52a0\u5230pred_list\u5217\u8868\u4e2d.\u6700\u540e\u7684\u7ef4\u5ea6\u7c7b\u4f3c\u4e8e(2, 9400, 41)\nprediction = np.ones_like(pred_list[0])\n#numpy ones_like : \u8fd4\u56de\u4e0e\u7ed9\u5b9a\u6570\u7ec4\u5177\u6709\u76f8\u540c\u5f62\u72b6(9400, 41)\u548c\u7c7b\u578b\u7684\u75281\u586b\u5145\u7684\u6570\u7ec4\u3002\n\n#\u6c42\u51e0\u4f55\u5e73\u5747\u503c\nfor pred in pred_list:\n    #pred\u7684\u7ef4\u5ea6\u662f(9400, 41)\n    prediction = prediction*pred #pred[0] pred[1] \u5bf9\u5e94\u5143\u7d20\u76f8\u4e58\nprediction = prediction**(1.\/len(pred_list))\n#\u6c42\u51e0\u4f55\u5e73\u5747\n\n# Make a submission file\u5236\u4f5c\u63d0\u4ef7\u6587\u4ef6\ntop_3 = np.array(LABELS)[np.argsort(-prediction, axis=1)[:, :3]]\n#LABELS -> top_3  :  (9400,41) -> (9400,3)\n#numpy : \u5728\u6bcf\u884c\uff0c\u628apredictions\u4ece\u5927\u5230\u5c0f\u6392\u5e8f\uff0c\u53d6\u524d\u4e09\u4e2a\u7684\u7d22\u5f15\uff0c\u6839\u636e\u7d22\u5f15\uff0c\u9009\u51faLABELS\u6bcf\u884c\u4e2d\u5bf9\u5e94\u7684\u6570\u636e(\u7c7b\u522b)\u3002\n# \u5373\u9009\u51fa\u6bcf\u4e2a\u6837\u672c\u9884\u6d4b\u6982\u7387\u6700\u5927\u7684\u4e09\u4e2a\u7c7b\u522b\u3002\n\npredicted_labels = [' '.join(list(x)) for x in top_3]\n#python join : \u628atop_3\u6bcf\u884c\u76843\u4e2a\u7c7b\u522b\uff0c\u4ee5\u7a7a\u683c\u9694\u5f00\uff0c\u7ec4\u6210\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002(9400,3) -> (9400,)\n\n#9400\ntest = pd.read_csv('..\/input\/freesound-audio-tagging\/sample_submission.csv')\n#pandas read_csv : \u8bfb\u53d6kaggle\u7ed9\u7684sample_submission.csv\n\n# test.set_index(\"fname\", inplace=True)\ntest['label'] = predicted_labels\n#test\u65b0\u589e\u4e00\u5217label\uff0c\u6bcf\u4e00\u884c\u7684\u503c\u4e0epredicted_labels\u5bf9\u5e94\n\ntest[['fname', 'label']].to_csv(\"1d_conv_ensembled_submission.csv\", index=False)\n#pandas to_cvs : \u5c06DataFrame\u4fdd\u5b58\u4e3a.csv\u6587\u4ef6","29c9cc82":"pred_list = []\n# for i in range(10):\n#     pred_list.append(np.load(\"..\/input\/freesound-prediction-data-2d-conv-reduced-lr\/test_predictions_%d.npy\"%i))\nfor i in range(2):\n    pred_list.append(np.load(\"..\/input\/freesound-prediction-data-2d-conv-reduced-lr\/test_predictions_%d.npy\"%i))\n    #\u53d6\u7b2ci\u4e2a1d_conv\u6d4b\u8bd5\u7ed3\u679c\uff0c\u8ffd\u52a0\u5230pred_list\u5217\u8868\u4e2d.\u6700\u540e\u7684\u7ef4\u5ea6\u7c7b\u4f3c\u4e8e(4, 2000, 41)    \nfor i in range(2):\n    pred_list.append(np.load(\"..\/input\/freesound-prediction-file\/test_predictions_%d.npy\"%i))\n    #\u53d6\u7b2ci\u4e2a2d_conv\u6d4b\u8bd5\u7ed3\u679c\uff0c\u8ffd\u52a0\u5230pred_list\u5217\u8868\u4e2d.\u6700\u540e\u7684\u7ef4\u5ea6\u7c7b\u4f3c\u4e8e(4, 2000, 41)    \nprediction = np.ones_like(pred_list[0])\n#numpy ones_like : \u8fd4\u56de\u4e0e\u7ed9\u5b9a\u6570\u7ec4\u5177\u6709\u76f8\u540c\u5f62\u72b6(2000, 41)\u548c\u7c7b\u578b\u7684\u75281\u586b\u5145\u7684\u6570\u7ec4\u3002\n\n#\u6c42\u51e0\u4f55\u5e73\u5747\u503c\nfor pred in pred_list:\n    #pred\u7684\u7ef4\u5ea6\u662f(2000, 41)\n    prediction = prediction*pred\n    #np * : \u5bf9\u5e94\u5143\u7d20\u76f8\u4e58\n    #n\u4e2a\u53d8\u91cf\u503c\u8fde\u4e58\u79ef\nprediction = prediction**(1.\/len(pred_list))\n#\u6c42\u5e73\u5747\n\n# Make a submission file\u5236\u4f5c\u63d0\u4ef7\u6587\u4ef6\ntop_3 = np.array(LABELS)[np.argsort(-prediction, axis=1)[:, :3]]\n#LABELS -> top_3  :  (2000,41) -> (2000,3)\n#numpy : \u5728\u6bcf\u884c\uff0c\u628apredictions\u4ece\u5927\u5230\u5c0f\u6392\u5e8f\uff0c\u53d6\u524d\u4e09\u4e2a\u7684\u7d22\u5f15\uff0c\u6839\u636e\u7d22\u5f15\uff0c\u9009\u51faLABELS\u6bcf\u884c\u4e2d\u5bf9\u5e94\u7684\u6570\u636e(\u7c7b\u522b)\u3002\n# \u5373\u9009\u51fa\u6bcf\u4e2a\u6837\u672c\u9884\u6d4b\u6982\u7387\u6700\u5927\u7684\u4e09\u4e2a\u7c7b\u522b\u3002\n\npredicted_labels = [' '.join(list(x)) for x in top_3]\n#python join : \u628atop_3\u6bcf\u884c\u76843\u4e2a\u7c7b\u522b\uff0c\u4ee5\u7a7a\u683c\u9694\u5f00\uff0c\u7ec4\u6210\u4e00\u4e2a\u5b57\u7b26\u4e32\u3002(2000,3) -> (2000,)\n\ntest = pd.read_csv('..\/input\/freesound-audio-tagging\/sample_submission.csv')\n#pandas read_csv : \u8bfb\u53d6kaggle\u7ed9\u7684sample_submission.csv\n\ntest['label'] = predicted_labels\n#test\u65b0\u589e\u4e00\u5217label\uff0c\u6bcf\u4e00\u884c\u7684\u503c\u4e0epredicted_labels\u5bf9\u5e94\n\ntest[['fname', 'label']].to_csv(\"1d_2d_ensembled_submission.csv\", index=False)\n#pandas to_cvs : \u5c06DataFrame\u4fdd\u5b58\u4e3a.csv\u6587\u4ef6","8b7c260a":"<a id=\"1d_2d_ensembling\"><\/a>\n## <center>5. Ensembling 1D Conv and 2D Conv Predictions<\/center>  \n\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd51\u7ef4\u5377\u79ef \u548c MFCC\u65b9\u6cd52 \u7684\u9884\u6d4b\u503c\u96c6\u6210","bc71101c":"We observe:  \n\u6211\u4eec\u89c2\u5bdf\u5230\uff1a  \n\n1. The distribution of audio length across labels is non-uniform and has high variance.  \n\u4e0d\u540c\u6807\u7b7e\u7684\u97f3\u9891\u7684\u957f\u5ea6\u7684\u5206\u5e03\u662f\u4e0d\u5747\u5300\u7684\uff0c\u5e76\u4e14\u5177\u6709\u9ad8\u7684\u65b9\u5dee\u3002  \nLet's now analyze the frame length distribution in Train and Test.  \n\u73b0\u5728\u8ba9\u6211\u4eec\u5206\u6790\u4e00\u4e0bTrain\u548cTest\u4e2d\u7684\u5e27\u957f\u5206\u5e03\u3002","1e4dd1be":"#### Some sssential imports  \n\u5bfc\u5165\u5fc5\u8981\u7684\u5e93","74153448":"Let's plot the audio frames","bdd2050c":"<a id=\"audio_length\"><\/a>\n### Audio Length  \n\u97f3\u9891\u7684\u957f\u5ea6  \n\nWe shall now analyze the lengths of the audio files in our dataset  \n\u6211\u4eec\u73b0\u5728\u5c06\u5206\u6790\u6570\u636e\u96c6\u4e2d\u97f3\u9891\u6587\u4ef6\u7684\u957f\u5ea6","1ccbf2fd":"# <center>Freesound General-Purpose Audio Tagging Challenge<\/center>  \n<center>Freesound\u901a\u7528\u97f3\u9891\u6807\u6ce8\u6311\u6218  <\/center>  \n\n\n![Logo](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/3c\/Freesound_project_website_logo.png)\n\nFreesound is a collaborative database of Creative Commons Licensed sounds. The aim of this competition is to classify audio files that cover real-world sounds from musical instruments, humans, animals, machines, etc. Few of the labels are: `Trumpet`, `Squeak`, `Meow`, `Applause` and `Finger_sapping`.  One of the challenges is that not all labels are manually verified. A creative solution should be able to partially rely on these *weak* annotations.  \nFreesound\u662fCreative Commons Licensed\u58f0\u97f3\u7684\u534f\u4f5c\u6570\u636e\u5e93\u3002 \u672c\u6b21\u6bd4\u8d5b\u7684\u76ee\u7684\u662f\u5bf9\u97f3\u9891\u6587\u4ef6\u8fdb\u884c\u5206\u7c7b\uff0c\u8fd9\u4e9b\u6587\u4ef6\u6db5\u76d6\u4e50\u5668\uff0c\u4eba\u7c7b\uff0c\u52a8\u7269\uff0c\u673a\u5668\u7b49\u771f\u5b9e\u4e16\u754c\u7684\u58f0\u97f3\u3002\u5f88\u5c11\u91cf\u7684\u6807\u7b7e\u662f\uff1a\u5c0f\u53f7\uff0c\u5431\u5431\u58f0\uff0c\u55b5\u55b5\u58f0\uff0c\u638c\u58f0\u548c\u6253\u54cd\u6307\u3002\u5176\u4e2d\u4e00\u4e2a\u6311\u6218\u662f\u5e76\u975e\u6240\u6709\u6807\u7b7e\u90fd\u7ecf\u8fc7\u4e86\u4eba\u5de5\u9a8c\u8bc1\u3002\u521b\u9020\u6027\u7684\u89e3\u51b3\u65b9\u6848\u5e94\u8be5\u80fd\u591f\u90e8\u5206\u4f9d\u8d56\u8fd9\u4e9b\u5f31\u6ce8\u91ca\u3002  \n\nLet's take a tour of the data visualization and model building through this kernel. If you like this work, please show your support by upvotes. Happy Kaggling!  \n\u8ba9\u6211\u4eec\u901a\u8fc7\u8fd9\u4e2a\u5185\u6838\u6d4f\u89c8\u6570\u636e\u53ef\u89c6\u5316\u548c\u6a21\u578b\u6784\u5efa\u3002 \u5982\u679c\u60a8\u559c\u6b22\u8fd9\u9879\u5de5\u4f5c\uff0c\u8bf7\u901a\u8fc7upvotes\u8868\u793a\u60a8\u7684\u652f\u6301\u3002 \u5feb\u4e50\u7684Kaggling\uff01  \n\u7ffb\u8bd1\u6ce8\u91ca\uff1a\u4e03\u6708\u5728\u7ebffan  \n\n\n### Contents\n1. [Exploratory Data Analysis](#eda)\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790\n    * [Loading data](#loading_data)\u52a0\u8f7d\u6570\u636e\n    * [Distribution of Categories](#distribution)\u7c7b\u7684\u5206\u5e03\n    * [Reading Audio Files](#audio_files)\u6d4f\u89c8\u97f3\u9891\u6587\u4ef6\n    * [Audio Length](#audio_length)\u97f3\u9891\u957f\u5ea6\n2. [Building a Model using Raw Wave](#1d_model_building)\u4f7f\u7528\u539f\u59cb\u6ce2\u5efa\u7acb\u6a21\u578b\n    * [Model Discription](#1d_discription)\u6a21\u578b\u63cf\u8ff0\n    * [Configuration](#configuration)\u7ed3\u6784\n    * [DataGenerator class](#data_generator)\u6570\u636e\u751f\u6210\u5668\n    * [Normalization](#1d_normalization)1d\u5f52\u4e00\u5316\n    * [Training 1D Conv](#1d_training) \u8bad\u7ec31\u7ef4\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\n    * [Ensembling 1D Conv Predictions](#1d_ensembling)\u96c6\u6210\u4e00\u7ef4\u5377\u79ef\u7684\u9884\u6d4b\u7ed3\u679c\n3. [Introduction to MFCC](#intro_mfcc)MFCC\u7b80\u4ecb\n    * [Generating MFCC using Librosa](#librosa_mfcc)\u4f7f\u7528Librosa\u751f\u6210MFCC\n4. [Building a Model using MFCC](#2d_model_building)\u4f7f\u7528MFCC\u6784\u5efa\u6a21\u578b\n    * [Preparing Data](#2d_data)\u51c6\u5907\u6570\u636e\n    * [Normalization](#2d_normalization)\u5f52\u4e00\u5316\n    * [Training 2D Conv on MFCC](#2d_training)\u8bad\u7ec3MFCC\u76842D Conv\n    * [Ensembling 2D Conv Predictions](#2d_ensembling)\u96c6\u62102D Conv\u7684\u9884\u6d4b\u7ed3\u679c\n5. [Ensembling 1D Conv and 2D Conv Predictions](#1d_2d_ensembling)\u96c6\u62101D Conv\u548c2D Conv\u7684\u9884\u6d4b\u7ed3\u679c\n6. [Results and Conclusion](#conclusion)\u7ed3\u679c\u548c\u7ed3\u8bba  \n\n\u53c2\u8003\uff1a  \nhttps:\/\/www.jianshu.com\/p\/880bd818fca0  \nhttps:\/\/www.kaggle.com\/fizzbuzz\/beginner-s-guide-to-audio-data  \n\n\n<a id=\"eda\"><\/a>\n## <center>1. Exploratory Data Analysis\u63a2\u7d22\u6027\u6570\u636e\u5206\u6790<\/center>","78b59b31":"## Coming Soon  \n\u63a5\u4e0b\u6765\u7684\u5de5\u4f5c  \n\n1. Data Augmentation\u6570\u636e\u589e\u5f3a  \n2. Training on Manually Verified Labels\u5728\u624b\u5de5\u9a8c\u8bc1\u96c6\u4e0a\u8bad\u7ec3","0ade83ff":"We observe that:  \n\u6211\u4eec\u89c2\u5bdf\u5230\uff1a  \n1. The number of audio samples per category is **non-nform**. The minimum number of audio samples in a category is `94` while the maximum is `300`  \n\u6bcf\u4e2a\u7c7b\u522b\u7684\u97f3\u9891\u6837\u672c\u6570\u91cf\u662f\u975e\u7a7a\u7684\u3002 \u4e00\u4e2a\u7c7b\u522b\u4e2d\u97f3\u9891\u6837\u672c\u7684\u6700\u5c0f\u6570\u91cf\u4e3a94\uff0c\u800c\u6700\u5927\u503c\u4e3a300  \n\n2. Also, the proportion of `maually_verified` labels per category is non-uniform.\n<a id=\"audio_files\"><\/a>  \n\u6b64\u5916\uff0c\u6bcf\u4e2a\u7c7b\u522b\u7684maually_verified(\u4eba\u5de5\u6807\u8bb0)\u6807\u7b7e\u7684\u6bd4\u4f8b\u662f\u4e0d\u5747\u5300\u7684(\u901a\u8fc7\u67f1\u72b6\u56fe\u53ef\u4ee5\u770b\u51fa\u6765)\u3002  \n\n\n### Reading Audio Files  \n\u9605\u8bfb\u97f3\u9891\u6587\u4ef6  \n\nThe audios are [Pulse-code modulated](https:\/\/en.wikipedia.org\/wiki\/Audio_bit_depth) with a [bit depth](https:\/\/en.wikipedia.org\/wiki\/Audio_bit_depth) of 16 and a [sampling rate](https:\/\/en.wikipedia.org\/wiki\/Sampling_%28signal_processing%29) of 44.1 kHz  \n\u97f3\u9891\u91c7\u7528\u8109\u51b2\u7f16\u7801\u8c03\u5236\uff0c\u6bd4\u7279\u957f\u5ea6\u4e3a16\u4f4d\uff0c\u91c7\u6837\u7387\u4e3a44.1 kHz  \n\n\n![16-bit PCM](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/bf\/Pcm.svg\/500px-Pcm.svg.png)\n\n* **Bit-depth = 16**: The amplitude of each sample in the audio is one of 2^16 (=65536) possible values.   \n\u6bd4\u7279\u957f\u5ea6= 16 \uff0c \u610f\u601d\u662f \uff1a \u97f3\u9891\u4e2d\u6bcf\u4e2a\u53d6\u6837\u7684\u5e45\u5ea6\u662f2 ^ 16\uff08= 65536\uff09\u4e2a\u53ef\u80fd\u503c\u4e4b\u4e00\u3002  \n* **Samplig rate = 44.1 kHz**: Each second in the audio consists of 44100 samples. So, if the duration of the audio file is 3.2 seconds, the audio will consist of 44100\\*3.2 = 141120 values.  \nSamplig rate = 44.1 kHz  \u610f\u601d\u662f\uff1a\u97f3\u9891\u4e2d\u6bcf\u79d2\u5305\u542b44100\u6b21\u53d6\u6837\u3002 \u56e0\u6b64\uff0c\u5982\u679c\u97f3\u9891\u6587\u4ef6\u7684\u6301\u7eed\u65f6\u95f4\u4e3a3.2\u79d2\uff0c\u5219\u97f3\u9891\u5c06\u5305\u542b44100 * 3.2 = 141120\u4e2a\u503c\u3002  \n\nLet's listen to an audio file in our dataset and load it to a numpy array  \n\u8ba9\u6211\u4eec\u6536\u542c\u6570\u636e\u96c6\u4e2d\u7684\u97f3\u9891\u6587\u4ef6\u5e76\u5c06\u5176\u52a0\u8f7d\u5230numpy\u6570\u7ec4\u4e2d  ","fdbe65ec":"![raw](https:\/\/raw.githubusercontent.com\/zaffnet\/images\/master\/images\/raw_model.jpg)","ba3320fd":"\u7ebf\u6027\u51fd\u6570\u5f52\u4e00\u5316\uff08\u6458\u81ea\u767e\u9762\u673a\u5668\u5b66\u4e60\uff09  \n![image.png](attachment:image.png)","fd6c995f":"<a id=\"2d_data\"><\/a>\n### Preparing data\u51c6\u5907\u51c6\u636e","f39d1923":"<a id=\"1d_ensembling\"><\/a>\n#### Ensembling 1D Conv Predictions  \n\u4e00\u7ef4Conv\u6a21\u578b\u9884\u6d4b\u503c\u96c6\u6210  \nNow that we have trained our model, it is time average the predictions of 10-folds. We will try Geometric Mean averaging and see what will be our Public LB score.  \n\u73b0\u5728\u6211\u4eec\u5df2\u7ecf\u8bad\u7ec3\u4e86\u6211\u4eec\u7684\u6a21\u578b\uff0c\u73b0\u5728\u9700\u8981\u5e73\u574710\u6298(\u8fd9\u91cc\u662f2\u6298)\u7684\u9884\u6d4b\u7ed3\u679c\u3002 \u6211\u4eec\u5c06\u5c1d\u8bd5\u51e0\u4f55\u5e73\u5747\u503c(n\u4e2a\u53d8\u91cf\u503c\u8fde\u4e58\u79ef\u540e\u5f00n\u6b21\u65b9)\uff0c\u5e76\u67e5\u770b\u6211\u4eec\u7684\u516c\u5171LB(kaggle LeaderBoard)\u5f97\u5206\u3002","28139f5b":"<a id=\"distribution\"><\/a>\n### Distribution of Categories  \n\u4e0d\u540c\u7c7b\u7684\u5206\u5e03","46110a9a":"<a id=\"data_generator\"><\/a>\n#### DataGenerator Class  \n\u6570\u636e\u751f\u6210\u7c7b","ce287484":"\u7ebf\u6027\u51fd\u6570\u5f52\u4e00\u5316\uff08\u6458\u81ea\u767e\u9762\u673a\u5668\u5b66\u4e60\uff09  \n![image.png](attachment:image.png)","9cc71660":"It is important to convert raw labels to integer indices  \n\u5c06\u539f\u59cb\u6807\u7b7e\u8f6c\u6362\u4e3a\u6574\u6570\u7d22\u5f15\u5f88\u91cd\u8981","e250f888":"The Configuration object stores those learning parameters that are shared between data generators, models, and training functions. Anything that is `global` as far as the training is concerned can become the part of Configuration object.  \nConfiguration\u5bf9\u8c61\u5b58\u50a8\u4e86 \u5728\u6570\u636e\u751f\u6210\u5668\uff0c\u6a21\u578b\u548c\u8bad\u7ec3\u51fd\u6570\u4e4b\u95f4\u5171\u4eab\u7684\u5b66\u4e60\u53c2\u6570\u3002 \u5c31\u8bad\u7ec3\u800c\u8a00\uff0c\u4efb\u4f55\u201c\u5168\u5c40\u201d\u7684\u4e1c\u897f\u90fd\u53ef\u4ee5\u6210\u4e3aConfiguration\u5bf9\u8c61\u7684\u4e00\u90e8\u5206\u3002","c77b2ca7":"<a id=\"intro_mfcc\"><\/a>\n## <center> 3. Introuction to MFCC \u4f7f\u7528MFCC\u7279\u5f81\u4ece\u9891\u57df\u5206\u7c7b\n\nAs we have seen in the previous section, our Deep Learning models are powerful enough to classify sounds from the raw audio. We do not require any complex feature engineering. But before the Deep Learning era, people developed techniques to extract features from audio signals. It turns out that these techniques are still useful. One such technique is computing the MFCC (Mel Frquency Cepstral Coefficients) from the raw audio. Before we jump to MFCC, let's talk about extracting features from the sound.  \n\u6b63\u5982\u6211\u4eec\u5728\u4e0a\u4e00\u8282\u4e2d\u770b\u5230\u7684\uff0c\u6211\u4eec\u7684\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8db3\u591f\u5f3a\u5927\uff0c\u53ef\u4ee5\u5bf9\u539f\u59cb\u97f3\u9891\u4e2d\u7684\u58f0\u97f3\u8fdb\u884c\u5206\u7c7b\u3002\u6211\u4eec\u4e0d\u9700\u8981\u4efb\u4f55\u590d\u6742\u7684\u7279\u5f81\u5de5\u7a0b\u3002\u4f46\u5728\u6df1\u5ea6\u5b66\u4e60\u65f6\u4ee3\u4e4b\u524d\uff0c\u4eba\u4eec\u5f00\u53d1\u4e86\u4ece\u97f3\u9891\u4fe1\u53f7\u4e2d\u63d0\u53d6\u7279\u5f81\u7684\u6280\u672f\u3002\u4e8b\u5b9e\u8bc1\u660e\uff0c\u8fd9\u4e9b\u6280\u672f\u4ecd\u7136\u6709\u7528\u3002\u4e00\u79cd\u8fd9\u6837\u7684\u6280\u672f\u662f\u4ece\u539f\u59cb\u97f3\u9891\u8ba1\u7b97MFCC\uff08Mel Frquency Cepstral Coefficients\u6885\u5c14\u9891\u7387\u5012\u8c31\u7cfb\u6570\uff09\u3002\u5728\u6211\u4eec\u8df3\u5230MFCC\u4e4b\u524d\uff0c\u8ba9\u6211\u4eec\u8c08\u8c08\u4ece\u58f0\u97f3\u4e2d\u63d0\u53d6\u7279\u5f81\u3002  \n\nIf we just want to classify some sound, we should build features that are **speaker independent**. Any feature that only gives information about the speaker (like the pitch of their voice) will not be helpful for classification. In other words, we should extract features that depend on the \"content\" of the audio rather than the nature of the speaker. Also, a good feature extraction technique should mimic the human speech perception. We don't hear loudness on a linear scale. If we want to double the perceived loudness of a sound, we have to put 8 times as much energy into it. Instead of a linear scale, our perception system uses a log scale.   \n\u5982\u679c\u6211\u4eec\u53ea\u60f3\u5bf9\u67d0\u4e9b\u58f0\u97f3\u8fdb\u884c\u5206\u7c7b\uff0c\u6211\u4eec\u5e94\u8be5\u6784\u5efa\u4e0e\u626c\u58f0\u5668\u65e0\u5173\u7684\u7279\u5f81\u3002\u4efb\u4f55\u53ea\u63d0\u4f9b\u6709\u5173\u626c\u58f0\u5668\u4fe1\u606f\u7684\u529f\u80fd\uff08\u5982\u58f0\u97f3\u7684\u97f3\u9ad8\uff09\u5bf9\u5206\u7c7b\u6ca1\u6709\u5e2e\u52a9\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u5e94\u8be5\u63d0\u53d6\u4f9d\u8d56\u4e8e\u97f3\u9891\u201c\u5185\u5bb9\u201d\u800c\u4e0d\u662f\u8bdd\u7b52\u7684\u7279\u5f81\u3002\u6b64\u5916\uff0c\u826f\u597d\u7684\u7279\u5f81\u63d0\u53d6\u6280\u672f\u5e94\u8be5\u6a21\u4eff\u4eba\u7c7b\u8bed\u97f3\u611f\u77e5\u3002\u6211\u4eec\u542c\u5230\u7684\u97f3\u9636\u7684\u54cd\u5ea6\u4e0d\u662f\u7ebf\u6027\u7684\u3002\u5982\u679c\u6211\u4eec\u60f3\u8981\u5c06\u58f0\u97f3\u7684\u611f\u77e5\u54cd\u5ea6\u52a0\u500d\uff0c\u6211\u4eec\u5fc5\u987b\u5c068\u500d\u7684\u80fd\u91cf\u6295\u5165\u5176\u4e2d\u3002\u6211\u4eec\u7684\u611f\u77e5\u7cfb\u7edf\u4f7f\u7528log\u51fd\u6570\u800c\u4e0d\u662f\u7ebf\u6027\u51fd\u6570\u3002  \n\nTaking these things into account, Davis and Mermelstein came up with MFCC in the 1980's. MFCC mimics the logarithmic perception of loudness and pitch of human auditory system and tries to eliminate speaker dependent characteristics by excluding the fundamental frequency and their harmonics. The underlying mathematics is quite complicated and we will skip that. For those interested, here is the [detailed explanation](http:\/\/practicalcryptography.com\/miscellaneous\/machine-learning\/guide-mel-frequency-cepstral-coefficients-mfccs\/).  \n\u8003\u8651\u5230\u8fd9\u4e9b\u56e0\u7d20\uff0c\u6234\u7ef4\u65af\u548c\u6885\u5c14\u65af\u5766\u572820\u4e16\u7eaa80\u5e74\u4ee3\u63d0\u51fa\u4e86MFCC\u3002 MFCC\u6a21\u4eff\u4eba\u7c7b\u542c\u89c9\u7cfb\u7edf\u7684\u54cd\u5ea6\u548c\u97f3\u9ad8\u7684\u5bf9\u6570\u611f\u77e5\uff0c\u5e76\u8bd5\u56fe\u901a\u8fc7\u6392\u9664\u57fa\u9891\u548c\u5b83\u4eec\u7684\u8c10\u6ce2\u6765\u6d88\u9664\u8bf4\u8bdd\u8005\u76f8\u5173\u7684\u7279\u5f81\u3002\u57fa\u7840\u6570\u5b66\u975e\u5e38\u590d\u6742\uff0c\u6211\u4eec\u5c06\u8df3\u8fc7\u8fd9\u4e00\u70b9\u3002\u5bf9\u4e8e\u90a3\u4e9b\u611f\u5174\u8da3\u7684\u4eba\uff0c\u8fd9\u91cc\u6709\u8be6\u7ec6\u7684\u89e3\u91ca\u3002\n\n![http:\/\/recognize-speech.com\/images\/FeatureExtraction\/MFCC\/MFCC_Flowchart.png](http:\/\/recognize-speech.com\/images\/FeatureExtraction\/MFCC\/MFCC_Flowchart.png)\n\n<a id=\"librosa_mfcc\"><\/a>\n#### Generating MFCC using Librosa\u4f7f\u7528Librosa\u5e93\u751f\u6210MFCC  \nThe library librosa has a function to calculate MFCC. Let's compute the MFCC of an audio file and visualize it.  \n\u5e93librosa\u5177\u6709\u8ba1\u7b97MFCC\u7684\u529f\u80fd\u3002\u8ba9\u6211\u4eec\u8ba1\u7b97\u97f3\u9891\u6587\u4ef6\u7684MFCC\u5e76\u5c06\u5176\u53ef\u89c6\u5316\u3002\n\nhttp:\/\/recognize-speech.com\/images\/FeatureExtraction\/MFCC\/MFCC_Flowchart.png","d5ed3caa":"Let's zoom in on first 1000 frames","74e4d778":"<a id=\"1d_model_building\"><\/a>  \n## <center>2. Building a Model using Raw Wave\u4f7f\u7528\u539f\u59cb\u6ce2\u5efa\u7acb\u6a21\u578b  <\/center>\nWe will build two models:  \n\u6211\u4eec\u5c06\u5efa\u7acb\u4e24\u4e2a\u6a21\u578b\uff1a  \n1. The first model will take the raw audio (1D array) as input and the primary operation will be Conv1D  \n\u7b2c\u4e00\u4e2a\u6a21\u578b\u5c06\u539f\u59cb\u97f3\u9891\uff081D\u9635\u5217\uff09\u4f5c\u4e3a\u8f93\u5165\uff0c\u4e3b\u8981\u64cd\u4f5c\u5c06\u662fConv1D  \n\n2. The second model will take the MFCCs as input. (We will explain MFCC later)  \n\u7b2c\u4e8c\u4e2a\u6a21\u578b\u5c06MFCC\u4f5c\u4e3a\u8f93\u5165\u3002 \uff08\u6211\u4eec\u7a0d\u540e\u4f1a\u5728\u7b2c\u56db\u8282\u89e3\u91caMFCC\uff09 \n\n\n<a id=\"1d_discription\"><\/a>\n### Keras Model using raw wave  \n\u4f7f\u7528\u539f\u59cb\u6ce2\u7684Keras\u6a21\u578b  \n\nOur model has the architecture as follows:  \n\u6211\u4eec\u7684\u6a21\u578b\u5177\u6709\u5982\u4e0b\u67b6\u6784\uff1a\n![raw](https:\/\/raw.githubusercontent.com\/zaffnet\/images\/master\/images\/raw_model.jpg)\n\n**Important\u91cd\u8981\u63d0\u793a\uff1a**  \nDue to the time limit on Kaggle Kernels, it is not possible to perform 10-fold training of a large model. I have trained the model locally and uploaded its output files as a dataset. If you wish to train the bigger model, change `COMPLETE_RUN = True` at the beginning of the kernel.  \n\u7531\u4e8eKaggle Kernels\u7684\u65f6\u95f4\u9650\u5236\uff0c\u65e0\u6cd5\u5bf9\u5927\u578b\u6a21\u578b\u8fdb\u884c10\u6b21\u8bad\u7ec3\u3002 \u6211\u5df2\u5728\u672c\u5730\u8bad\u7ec3\u6a21\u578b\u5e76\u5c06\u6a21\u578b\u4e0a\u4f20\u4e3a\u6570\u636e\u96c6\u3002 \u5982\u679c\u60a8\u5e0c\u671b\u8bad\u7ec3\u66f4\u5927\u7684\u6a21\u578b\uff0c\u8bf7\u5728\u5185\u6838\u5f00\u5934\u66f4\u6539COMPLETE_RUN = True\u3002","fd7ff9d7":"![](http:\/\/)<a id=\"configuration\"><\/a>\n#### Configuration\u914d\u7f6e","06e0346b":"<a id=\"1d_normalization\"><\/a>\n#### Normalization  \n\u5f52\u4e00\u5316  \nNormalization is a crucial preprocessing step. The simplest method is rescaling the range of features to scale the range in [0, 1].   \n\u5f52\u4e00\u5316\u662f\u4e00\u4e2a\u81f3\u5173\u91cd\u8981\u7684\u9884\u5904\u7406\u6b65\u9aa4\u3002 \u6700\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u7ebf\u6027\u8c03\u6574\u7279\u5f81\u8303\u56f4\u4ee5\u7f29\u653e\u5230[0,1]\u4e2d\u7684\u8303\u56f4\u3002","8b688b17":"<a id=\"2d_ensembling\"><\/a>\n#### Ensembling 2D Conv Predictions\n\u4e8c\u7ef4Convk\u6298\u9884\u6d4b\u503c\u96c6\u6210","4fa1d3a5":"The DataGenerator class inherits from **`keras.utils.Sequence`** . It is useful for preprocessing and feeding the data to a Keras model.   \nDataGenerator\u7c7b\u7ee7\u627f\u81eakeras.utils.Sequence\u3002 \u5b83\u5bf9\u4e8e\u9884\u5904\u7406\u548c\u5c06\u6570\u636e\u5582\u5230Keras\u6a21\u578b\u975e\u5e38\u6709\u7528\u3002  \n\n* Once initialized with a batch_size, it computes the number of batches in an epoch. The **`__len__`** method tells Keras how many batches to draw in each epoch.   \n\u4e00\u65e6\u4f7f\u7528batch_size\u521d\u59cb\u5316\uff0c\u5b83\u5c31\u4f1a\u8ba1\u7b97\u4e00\u8f6e\u4e2d\u7684\u6279\u6b21\u6570\u3002 __len__\u65b9\u6cd5\u544a\u8bc9Keras\u6bcf\u8f6e\u6709\u591a\u5c11\u6279\u6b21\u3002  \n* The **`__getitem__`** method takes an index (which is the batch number) and returns a batch of the data (both X and y) after calculating the offset. During test time, only `X` is returned.  \n__getitem__\u65b9\u6cd5\u83b7\u53d6\u7d22\u5f15\uff08\u8fd9\u662f\u6279\u53f7\uff09\u5e76\u5728\u8ba1\u7b97\u504f\u79fb\u91cf\u540e\u8fd4\u56de\u4e00\u6279\u6570\u636e\uff08X\u548cy\uff09\u3002 \u5728\u6d4b\u8bd5\u671f\u95f4\uff0c\u4ec5\u8fd4\u56deX.  \n* If we want to perform some action after each epoch (like shuffle the data, or increase the proportion of augmented data), we can use the **`on_epoch_end`** method.  \n\u5982\u679c\u6211\u4eec\u60f3\u5728\u6bcf\u8f6e\u4e4b\u540e\u6267\u884c\u4e00\u4e9b\u64cd\u4f5c\uff08\u6bd4\u5982\u6253\u4e71\u6570\u636e\uff0c\u6216\u8005\u589e\u52a0 \u589e\u5f3a\u6570\u636e\u7684\u6bd4\u4f8b\uff09\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528on_epoch_end\u65b9\u6cd5\u3002  \n\nNote:\n**`Sequence`** are a safer way to do multiprocessing. This structure guarantees that the network will only train once on each sample per epoch which is not the case with generators.  \n\u6ce8\u610f\uff1aSequence\u662f\u8fdb\u884c\u591a\u8fdb\u7a0b\u5904\u7406\u7684\u66f4\u5b89\u5168\u7684\u65b9\u6cd5\u3002\u8fd9\u79cd\u7ed3\u6784\u4fdd\u8bc1\u7f51\u7edc\u5728\u6bcf\u4e2a\u65f6\u671f\u6bcf\u4e2a\u6837\u672c\u53ea\u8bad\u7ec3\u4e00\u6b21\uff0c\u8fd9\u4e0e\u751f\u6210\u5668\u4e0d\u540c","9d70630d":"Here is the code for 10-fold training:  \n\u4ee5\u4e0b\u662f10\u6298\u8bad\u7ec3\u7684\u4ee3\u7801\uff1a  \n* We use **`from sklearn.cross_validation.StratifiedKFold`** for splitting the trainig data into 10 folds.  \n\u6211\u4eec\u4f7f\u7528sklearn.model_selection.StratifiedKFold\u5c06trainig\u6570\u636e\u5206\u621010\u4efd\u3002  \n* We use some Keras callbacks to monitor the training.  \n\u6211\u4eec\u4f7f\u7528\u4e00\u4e9bKeras\u56de\u8c03\u51fd\u6570\u6765\u76d1\u63a7\u8bad\u7ec3\u8fc7\u7a0b\u3002  \n    * **`ModelCheckpoint`** saves the best weight of our model (using validation data). We use this weight to make test predictions.  \n    ModelCheckpoint\u4fdd\u5b58\u4e86\u6211\u4eec\u6a21\u578b\u7684\u6700\u4f73\u6743\u91cd\uff08\u4f7f\u7528\u9a8c\u8bc1\u6570\u636e\uff09\u3002 \u6211\u4eec\u4f7f\u7528\u6b64\u6743\u91cd\u8fdb\u884c\u6d4b\u8bd5\u9884\u6d4b\u3002  \n    * **`EarlyStopping`** stops the training once validation loss ceases to decrease  \n    \u5f53\u9a8c\u8bc1\u635f\u5931\u51fd\u6570\u4e0d\u518d\u4e0b\u964d\uff0cEarlyStopping\u5c06\u505c\u6b62\u8bad\u7ec3  \n    * **`TensorBoard`** helps us visualize training and validation loss and accuracy.  \n    TensorBoard\u5e2e\u52a9\u6211\u4eec\u53ef\u89c6\u5316\u57f9\u8bad\u548c\u67e5\u770b\u635f\u5931\u548c\u51c6\u786e\u6027\u3002  \n* We fit the model using **`DataGenerator`** for training and validation splits.   \n\u6211\u4eec\u4f7f\u7528DataGenerator\u751f\u6210\u6570\u636e\u6765\u8fdb\u884c\u6a21\u578b\u62df\u5408\uff0c\u4ee5\u8fdb\u884c\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u5206\u5272\u3002  \n* We get both training and test predictions and save them as .npy format. We also generate a submission file. For 10-fold CV, the number of prediction files should be 10. We will ensemble these predictions later.  \n\u6211\u4eec\u5f97\u5230\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9884\u6d4b\u7ed3\u679c\uff0c\u5e76\u5c06\u5b83\u4eec\u4fdd\u5b58\u4e3a.npy\u683c\u5f0f\u3002 \u6211\u4eec\u8fd8\u751f\u6210\u4e00\u4e2a\u63d0\u4ea4\u6587\u4ef6\u3002 \u5bf9\u4e8e10\u6298\u4ea4\u53c9\u9a8c\u8bc1\uff0c\u9884\u6d4b\u6587\u4ef6\u7684\u6570\u91cf\u5e94\u4e3a10.\u6211\u4eec\u5c06\u5728\u7a0d\u540e\u96c6\u6210\u8fd9\u4e9b\u9884\u6d4b\u3002","4fb3e953":"<a id=\"conclusion\"><\/a>\n## <center>Results and Conclusion<\/center>  \n\u7ed3\u679c\u4e0e\u5c55\u671b","54dead1b":"<a id=\"2d_model_building\"><\/a>\n## <center>4. Building a Model using MFCC\n\nWe will build now build a 2D Convolutional model using MFCC.   \n\u6211\u4eec\u73b0\u5728\u5c06\u4f7f\u7528MFCC\u6784\u5efa2D\u5377\u79ef\u6a21\u578b\u3002","8b93bb03":"So far, we have trained two models. Let's analyze their relative complexity and strength.  \n\u5230\u76ee\u524d\u4e3a\u6b62\uff0c\u6211\u4eec\u5df2\u7ecf\u8bad\u7ec3\u4e86\u4e24\u4e2a\u6a21\u578b\u3002 \u8ba9\u6211\u4eec\u5206\u6790\u5b83\u4eec\u7684\u76f8\u5bf9\u590d\u6742\u6027\u548c\u597d\u574f\u3002   \n\n\n\n| Model        | Number of Trainable parameters           | Public LB score  |\n| ------------- |:-------------:| -----:|\n| 1D Conv on Raw wave(\u65b9\u6cd5\u4e00)      | 360,513 | 0.809 |\n| 2D Conv on MFCC (verified labels only)    | 168,361  |   0.785 |\n| 2D Conv on MFCC(\u65b9\u6cd5\u4e8c)     | 168,361  |   0.844 |\n| 1D Conv + 2D Conv Ensemble(\u6cd51+\u6cd5\u4e8c)     | N\/A  |   0.895 |\n\n**As we can see, 2D Convolution on MFCC performs better than 1D Convolution on Raw waves.**  \n\u539f\u4f5c\u8005\u6700\u7ec8\u8dd1\u51fa\u6765\u7684\u7ed3\u679c\u53d1\u73b0\u6cd5\u4e8cMFCC\u6bd4\u6cd5\u4e00\u6df1\u5ea6\u5b66\u4e60\u4e00\u7ef4\u5377\u79ef\u6548\u679c\u66f4\u597d","7c747e72":"<a id=\"2d_training\"><\/a>\n#### Training 2D Conv on MFCC \u8bad\u7ec3\uff0c\u751f\u6210\u591a\u4e2a\u63d0\u4ea4\u6587\u4ef6(K\u6298)","f4c75fbf":"<a id=\"1d_training\"><\/a>\n#### Training 1D Conv\n\u8bad\u7ec31D\u5377\u79ef\u7f51\u7edc","2c8faed9":"<a id=\"2d_normalization\"><\/a>\n#### Normalization  \n\u5f52\u4e00\u5316","ea17e5c1":"* The dummy model is just for debugging purpose.  \n\u5047\u8bbe\u6a21\u578b\u4ec5\u7528\u4e8e\u8c03\u8bd5\u76ee\u7684\u3002  \n* Our 1D Conv model is fairly deep and is trained using Adam Optimizer with a learning rate of 0.0001  \n\u6211\u4eec\u7684\u4e00\u7ef4Conv\u6a21\u578b\u975e\u5e38\u6df1\uff0c\u4f7f\u7528Adam Optimizer\u8fdb\u884c\u8bad\u7ec3\uff0c\u5b66\u4e60\u7387\u4e3a0.0001","3aff2581":"<a id=\"loading_data\"><\/a>\n### Loading data  \n\u52a0\u8f7d\u6570\u636e","d7b4a092":"We observe:  \n\u6211\u4eec\u89c2\u5bdf\u5230\uff1a  \n1. Majority of the audio files are short.\n\u5927\u591a\u6570\u97f3\u9891\u6587\u4ef6\u5f88\u77ed\u3002  \n1. There are four `abnormal` length in the test histogram. Let's analyze them.  \n\u6d4b\u8bd5\u76f4\u65b9\u56fe(\u53f3\u56fe)\u4e2d\u6709\u56db\u4e2a\u5f02\u5e38\u957f\u5ea6\u3002 \u6211\u4eec\u6765\u5206\u6790\u4e00\u4e0b\u5427\u3002  "}}