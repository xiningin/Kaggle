{"cell_type":{"0dc8ee0e":"code","5500f66c":"code","3f8b7911":"code","1fc908bf":"code","572b83a8":"code","73b50521":"code","f197b8b1":"code","487adb58":"code","30600a27":"code","91ee7623":"code","f7a26eeb":"code","63556fde":"code","7a12e7f4":"code","7d4017b6":"code","8179dadf":"code","81d2ce58":"code","b6a17479":"code","fa25fa81":"markdown","b0de44f9":"markdown","3b6ec5c8":"markdown","7c39f31a":"markdown","50beec55":"markdown","307b6f44":"markdown","01dab4b6":"markdown","4fcf317f":"markdown","30766d91":"markdown","ef1a3763":"markdown","3cfbbbcf":"markdown","8bb85ea5":"markdown","283c1a38":"markdown","4a00b755":"markdown","8d553b6d":"markdown","21a90359":"markdown","36f1d6fc":"markdown"},"source":{"0dc8ee0e":"import os\nimport pandas as pd\nimport numpy as np\nimport re\nimport pickle\n\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model, Sequential, load_model\nfrom keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional, Dropout, LSTM\nfrom keras.layers.embeddings import Embedding\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.losses import sparse_categorical_crossentropy\nfrom keras.callbacks import EarlyStopping","5500f66c":"data_dir = \"..\/input\/gujarati-to-english-translation\/\"\n\nfiles = os.listdir(data_dir)\nprint(files)","3f8b7911":"# Let's load sentences.\n\nwith open(data_dir+os.sep+'train.gu', 'r', encoding='utf8') as file:\n    guj_sentences = file.read().splitlines()\n\nwith open(data_dir+os.sep+'train.en', 'r', encoding='utf8') as file:\n    eng_sentences = file.read().splitlines()\n    \nprint(f\"Number of Gujarati Sentences: {len(guj_sentences)}\")\nprint(f\"Number of English Sentences: {len(eng_sentences)}\")","1fc908bf":"# Visualize some sentences. Temporarily we will create a DataFramet to visualize it properly.\n\nsent_count = 10\n\ndf = pd.DataFrame(list(zip(guj_sentences, eng_sentences)), columns=['Gujarati', 'English'])\n\n# Enlarge widths for columsns, so we can visualize it properly.\npd.set_option('max_colwidth', 400)\n\ndf['Guj Tokens'] = df['Gujarati'].str.split().str.len()\ndf['Eng Tokens'] = df['English'].str.split().str.len()\n\ndf.head()","572b83a8":"valid_sentences = df[(df['Guj Tokens'] <= 15) & (df['Eng Tokens'] <= 15)]\nvalid_sentences","73b50521":"guj_sentences = valid_sentences['Gujarati']\neng_sentences = valid_sentences['English']","f197b8b1":"# let's see number of unique words and characters in both languages.\n\nwith open(data_dir+os.sep+'vocab.gu', 'r', encoding='utf8') as file:\n    guj_words = file.read().splitlines()\n\nwith open(data_dir+os.sep+'vocab.en', 'r', encoding='utf8') as file:\n    eng_words = file.read().splitlines()\n    \nprint(f\"Number of Gujarati Words: {len(guj_words)}\")\nprint(f\"Number of English Words: {len(eng_words)}\")\n\nguj_characters = set()\neng_characters = set()\n\nfor w in guj_words:\n    for c in w:\n        guj_characters.add(c)\n\nfor w in eng_words:\n    for c in w:\n        eng_characters.add(c)\n        \nprint(f\"Number of Gujarati Characters: {len(guj_characters)}\")\nprint(f\"Number of English Characters: {len(eng_characters)}\")","487adb58":"print(guj_characters)\nprint(\"=\"*125)\nprint(eng_characters)","30600a27":"# Replace unnecessary characters.\nnoise_char = ['$',\"'\",'.','\"','_','\\\\',')','!','\u0a06',']','\/','(','#',',','>','<',';','=','?',':','[','&','`']\n\nguj_noise_reg = re.compile(\"|\".join([re.escape(x) for x in noise_char])+\"|[a-zA-Z0-9]\")\neng_noise_reg = re.compile(\"|\".join([re.escape(x) for x in noise_char])+\"|[0-9]\")\n\nguj_sentences = [guj_noise_reg.sub(\"\", sent) for sent in guj_sentences]\neng_sentences = [eng_noise_reg.sub(\"\", sent) for sent in eng_sentences]\n\n# Visualize some sentences. Temporarily we will create a DataFramet to visualize it properly.\nsent_count = 10\n\ndf = pd.DataFrame(list(zip(guj_sentences[:sent_count], eng_sentences[:sent_count])), columns=['Gujarati', 'English'])\n\n# Enlarge widths for columsns, so we can visualize it properly.\npd.set_option('max_colwidth', 400)\n\ndf.head(sent_count)","91ee7623":"class TextPreprocessor:\n    \"\"\"\n    This class will become helpful when we want to perform tokenization and preprocessing on \n    training, testing and on our own data. It contains tokenizer and id_to_word mapping.\n    \"\"\"\n    \n    def __init__(self, name, sentences, noise_reg):\n        # Language name to identify processor.\n        self.name = name\n\n        self.tokenizer = Tokenizer()\n        self.id_to_word = dict()\n        \n        self.max_sent_length = None\n        self.vocab_size = None\n        \n        self.noise_reg = noise_reg\n        \n    def init(self, sentences):\n        # Remove noise\n        sentences = self.remove_noise(sentences)\n        \n        # Tokenize data.\n        self.tokenizer.fit_on_texts(sentences)\n        self.vocab_size = len(self.tokenizer.word_index) + 1\n        tokenized_data = self.tokenizer.texts_to_sequences(sentences)\n        \n        # Padding at the end.\n        processed_data = pad_sequences(tokenized_data, maxlen=None, padding='post')\n        \n        # Storing max sentence length for future use (prediction on our text).\n        self.max_sent_length = processed_data.shape[1]\n        \n        # Store id to word mapping. It will be used for inverting prediction.\n        for word, idx in self.tokenizer.word_index.items():\n            self.id_to_word[idx] = word\n        \n        return processed_data\n    \n    def remove_noise(self, sentences):\n        # Remove noise\n        return [self.noise_reg.sub(\"\", sent) for sent in sentences]\n    \n    def process(self, sentences):\n        # Remove noise\n        sentences = self.remove_noise(sentences)\n        \n        tokenized_data = self.tokenizer.texts_to_sequences(sentences)\n        processed_data = pad_sequences(tokenized_data, maxlen=self.max_sent_length, padding='post')\n        \n        return processed_data\n        \n    def idx_to_text(self, idx_sentences):\n        pred_text = []\n        \n        for sent in idx_sentences:\n            new_sent = []\n            last_word = None \n            \n            for idx in sent:\n                word = self.id_to_word.get(np.argmax(idx), \"_\")\n                \n                if word != last_word:\n                    new_sent.append(word)\n                    last_word = word\n                \n            pred_text.append(\" \".join(new_sent))\n\n        return pred_text","f7a26eeb":"guj_preprocessor = TextPreprocessor(\"Gujarati\", guj_sentences, guj_noise_reg)\nprocessed_guj_data = guj_preprocessor.init(guj_sentences)\n\neng_preprocessor = TextPreprocessor(\"English\", eng_sentences, eng_noise_reg)\nprocessed_eng_data = eng_preprocessor.init(eng_sentences)","63556fde":"# Split data for training and testing.\nX_train, X_test, y_train, y_test = train_test_split(processed_guj_data, processed_eng_data, test_size=0.20, random_state=1)\n\ny_train = y_train.reshape(y_train.shape[0], y_train.shape[1], 1)\ny_test = y_test.reshape(y_test.shape[0], y_test.shape[1], 1)","7a12e7f4":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","7d4017b6":"def define_model(in_vocab,out_vocab, in_timesteps,out_timesteps):\n    learning_rate = 0.003\n    \n    # Build the layers    \n    model = Sequential()\n    # Embedding\n    model.add(Embedding(in_vocab, 256, input_length=X_train.shape,\n                         input_shape=X_train.shape[1:]))\n    # Encoder\n    model.add(Bidirectional(LSTM(512)))\n    model.add(RepeatVector(out_timesteps))\n    # Decoder\n    model.add(Bidirectional(LSTM(512, return_sequences=True)))\n    model.add(TimeDistributed(Dense(1024, activation='relu')))\n    model.add(Dropout(0.5))\n    model.add(TimeDistributed(Dense(out_vocab, activation='softmax')))\n    \n    model.compile(loss=sparse_categorical_crossentropy,\n                  optimizer=Adam(learning_rate),\n                  metrics=['accuracy'])\n    \n    return model\n\n# model compilation\nmodel = define_model(guj_preprocessor.vocab_size, eng_preprocessor.vocab_size, \n                     guj_preprocessor.max_sent_length, eng_preprocessor.max_sent_length)","8179dadf":"# Let's add callbacks to stop training when our model stops learning.\ncallback = EarlyStopping(monitor='val_loss', patience=10)\n    \nmodel.fit(X_train, y_train, epochs=1000, batch_size=1024, validation_data=(X_test, y_test), \n          verbose=1, callbacks=[callback])","81d2ce58":"predictions = model.predict(X_test[0:10])\n\ntranslated_sentences = eng_preprocessor.idx_to_text(predictions)\n\ndef get_original_sentence(ids):\n    words = []\n    \n    for x in ids:\n        words.append(guj_preprocessor.id_to_word.get(x, '_'))\n        \n    return \" \".join(words)\n\n\ndef get_original_sentence_2(ids):\n    words = []\n    \n    for x in ids:\n        words.append(eng_preprocessor.id_to_word.get(x[0], '_'))\n        \n    return \" \".join(words)\n\nfor idx, sent in enumerate(translated_sentences):\n    print(\"Actual Sentence:\", get_original_sentence(X_test[idx]))\n    print(\"Actual English Sentence:\", get_original_sentence_2(y_test[idx]))\n    print(\"Predicted Sentence:\", sent)\n    print(\"=\"*125)","b6a17479":"model.save('guj_to_eng.h5')\n\nwith open(\"guj_preprocessor\", \"wb\", pickle.HIGHEST_PROTOCOL) as file:\n    pickle.dump(guj_preprocessor, file)\n    \nwith open(\"eng_preprocessor\", \"wb\", pickle.HIGHEST_PROTOCOL) as file:\n    pickle.dump(eng_preprocessor, file)","fa25fa81":"Let's replace inappropriate characters from both languages.","b0de44f9":"# Save Model","3b6ec5c8":"### Observation\nWe can see that there are some characters like ?, \\_, \", ;, (, ], >, etc. exists in both languages. So, we need to remove those characters in tokenization.","7c39f31a":"# Prepare Data\n","50beec55":"# Prediction\nWe will predict on test dataset which we didn't pass for training.","307b6f44":"# Load Libraries","01dab4b6":"Use TextPreprocessor and generate processed data","4fcf317f":"Split Data","30766d91":"Training model with callback.","ef1a3763":"Let's create a class which contains all the important details for analysis.","3cfbbbcf":"# EDA\n\nLet's see some example sentences and check number of unique characters in both the languages currently we have. We will also look at the unique characters, so we can perform noise cleaning.","8bb85ea5":"# Gujarati to English Translation\nIn this notebook, we will use deep learning to translate Gujarati language to English language. We will use dataset from the below link.\n\nhttps:\/\/github.com\/shahparth123\/eng_guj_parallel_corpus\n\nIt contains 65k sentences. Vocabularies for both languages too.","283c1a38":"# Conclusion\n\nAfter understanding that length of sentences matters. I have seen great amount of improvement in prediction. Previous models with higher sentence length giving too poor output as compared to this one.\n\nWe can improve model more by adding more data, changing architecture.","4a00b755":"Below block will create pandas dataframe which contains token count too.","8d553b6d":"We will consider sentences with maximum length upto 15 only based on my previous model training experience.","21a90359":"Below code shows that we have built a model using Encoder-Decoder strategy. We have used embedding, Bi-directional LSTM and TimeDistributed layers to create encoder and decoder. ","36f1d6fc":"Our output looks good, it's not 100% perfect, but still it's good."}}