{"cell_type":{"61593d68":"code","12113982":"code","36a69a4a":"code","d68c460c":"code","5a800ed5":"code","107cea41":"code","73b36d7b":"code","5b1701e1":"code","7cc26f01":"code","7961a489":"code","4e967c2f":"code","7ed7c80e":"code","bf6f051f":"code","381bc963":"code","513a8f70":"code","d62b6a88":"code","62627b62":"code","cccb10d3":"code","9a30606d":"code","61e4cc8b":"code","b1365f36":"code","4c50a842":"code","c94327bf":"code","0e1c5ebc":"code","0cf8ff8f":"code","2622bacb":"code","4efc0a8d":"code","b7842b4b":"code","e5bd4663":"code","83a757d0":"code","11828259":"code","22e233b7":"code","562ace8c":"code","51e1e31e":"code","ae3abb5a":"code","a076472d":"code","5c304a64":"code","e4751307":"code","e42f542f":"code","8216c7d2":"code","22c7ba36":"code","5471e64f":"code","5aac8b60":"code","b87f994f":"markdown","9fa22344":"markdown","0814fc4d":"markdown","2a5552d5":"markdown","9d4cf8f1":"markdown","7ad51f5c":"markdown","fc04ae91":"markdown","0fcb8b14":"markdown","284c8f3b":"markdown","13b49985":"markdown","b3a1d9eb":"markdown","6212201d":"markdown","5f4c278b":"markdown","04b3ad61":"markdown","5a5703ad":"markdown","66894041":"markdown","1702e020":"markdown","b18fcbbb":"markdown","c4a2c8d0":"markdown","98ab2b17":"markdown","f7dc4695":"markdown"},"source":{"61593d68":"#_____________________________importing required libraries for dataframe(df) & Visuvalization________________________________#\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime as dt\n\n#_______________________________importing libraries that are reqiuired for clustering________________________________________#\nimport sklearn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\nfrom scipy.cluster.hierarchy import cut_tree\n\n#____________________________importing warnings to avoid unneccessary information___________________________________________#\nimport warnings\nwarnings.filterwarnings(\"ignore\")","12113982":"df=pd.read_csv('..\/input\/onlineretail\/OnlineRetail.csv', encoding= 'cp1252' , header=0)\nprint(df)","36a69a4a":"df.shape","d68c460c":"df.info()","5a800ed5":"df.describe()\n","107cea41":"# Calculating the Missing Values % contribution in DF\n\ndf_null = round(100*(df.isnull().sum())\/len(df), 2)\ndf_null","73b36d7b":"# Droping rows having missing values\n\ndf = df.dropna()\ndf.shape","5b1701e1":"# Changing the datatype of Customer Id as per Business understanding\n\ndf['CustomerID'] = df['CustomerID'].astype(str)","7cc26f01":"plt.figure(figsize=(12,5))\nsns.countplot(df['Country'],palette= 'Set3')\nplt.xticks(rotation=40,ha='right')\nplt.title(\"Country Distribution\")\nplt.xlabel('Country')\nplt.ylabel('Count');","7961a489":"plt.figure(figsize=(8,5))\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'], errors='coerce')\nsns.countplot(df['InvoiceDate'].dt.year,palette= 'Set1')\nplt.xticks(rotation=40,ha='right')\nplt.title(\"Year Distribution\")\nplt.xlabel('Year')\nplt.ylabel('Count');","4e967c2f":"plt.figure(figsize=(8,5))\nplt.xticks(rotation=40,ha='right')\nsns.countplot(df['InvoiceDate'].dt.month_name(),palette= 'Spectral')\nplt.title(\"Month Distribution\")\nplt.ylabel('Count')\nplt.xlabel('Month')","7ed7c80e":"plt.figure(figsize=(8,5))\nsns.countplot(df['InvoiceDate'].dt.day_name(),palette= 'Set1')\nplt.xticks(rotation=40)\nplt.title(\"Week Distribution\")\nplt.xlabel('Week')\nplt.ylabel('Count')","bf6f051f":"# New Attribute : Amount:\n\ndf['Amount'] = df['Quantity']*df['UnitPrice']\nrfm_m = df.groupby('CustomerID')['Amount'].sum()\nrfm_m = rfm_m.reset_index()\nrfm_m.head()","381bc963":"# New Attribute : Frequency\n\nrfm_f = df.groupby('CustomerID')['InvoiceNo'].count()\nrfm_f = rfm_f.reset_index()\nrfm_f.columns = ['CustomerID', 'Frequency']\nrfm_f.head()","513a8f70":"# Merging the two dfs\n\nrfm = pd.merge(rfm_m, rfm_f, on='CustomerID', how='inner')\nrfm.head()\n","d62b6a88":"# New Attribute : Recency\n\n# Convert to datetime to proper datatype\n\ndf['InvoiceDate'] = pd.to_datetime(df['InvoiceDate'],format='%d-%m-%Y %H:%M')","62627b62":"# Compute the maximum date to know the last transaction date\n\nmax_date = max(df['InvoiceDate'])\nmax_date","cccb10d3":"# Compute the difference between max date and transaction date\n\ndf['Diff'] = max_date - df['InvoiceDate']\ndf.head()\n","9a30606d":"# Compute last transaction date to get the recency of customers\n\nrfm_p = df.groupby('CustomerID')['Diff'].min()\nrfm_p = rfm_p.reset_index()\nrfm_p.head()\n","61e4cc8b":"# Extract number of days only\n\nrfm_p['Diff'] = rfm_p['Diff'].dt.days\nrfm_p.head()","b1365f36":"# Merge tha dataframes to get the final RFM dataframe\n\nrfm = pd.merge(rfm, rfm_p, on='CustomerID', how='inner')\nrfm.columns = ['CustomerID', 'Amount', 'Frequency', 'Recency']\nrfm.head()","4c50a842":"# Outlier Analysis of Amount Frequency and Recency\n\nattributes = ['Amount','Frequency','Recency']\nplt.rcParams['figure.figsize'] = [10,8]\nsns.boxplot(data = rfm[attributes], orient=\"v\", palette=\"Set2\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Range\", fontweight = 'bold')\nplt.xlabel(\"Attributes\", fontweight = 'bold')\n","c94327bf":"# Removing (statistical) outliers for Amount\nQ1 = rfm.Amount.quantile(0.05)\nQ3 = rfm.Amount.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Amount >= Q1 - 1.5*IQR) & (rfm.Amount <= Q3 + 1.5*IQR)]\n\n# Removing (statistical) outliers for Recency\nQ1 = rfm.Recency.quantile(0.05)\nQ3 = rfm.Recency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Recency >= Q1 - 1.5*IQR) & (rfm.Recency <= Q3 + 1.5*IQR)]\n\n# Removing (statistical) outliers for Frequency\nQ1 = rfm.Frequency.quantile(0.05)\nQ3 = rfm.Frequency.quantile(0.95)\nIQR = Q3 - Q1\nrfm = rfm[(rfm.Frequency >= Q1 - 1.5*IQR) & (rfm.Frequency <= Q3 + 1.5*IQR)]","0e1c5ebc":"# Rescaling the attributes\n\nrfm_df = rfm[['Amount', 'Frequency', 'Recency']]\n\n# Instantiate\nscaler = StandardScaler()\n\n# fit_transform\nrfm_df_scaled = scaler.fit_transform(rfm_df)\nrfm_df_scaled.shape","0cf8ff8f":"rfm_df_scaled = pd.DataFrame(rfm_df_scaled)\nrfm_df_scaled.columns = ['Amount', 'Frequency', 'Recency']\nrfm_df_scaled.head()","2622bacb":"# k-means with some arbitrary k\n\nkmeans = KMeans(n_clusters=4, max_iter=50)\nkmeans.fit(rfm_df_scaled)","4efc0a8d":"kmeans.labels_","b7842b4b":"# Elbow-curve\/SSD\n\nssd = []\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\nfor num_clusters in range_n_clusters:\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    ssd.append(kmeans.inertia_)\n    \n# plot the SSDs for each n_clusters\nplt.plot(ssd, marker='o')\nplt.title('Clusters Vs SSD')\nplt.xlabel('No of Clusters')\nplt.ylabel('Intertia')\n","e5bd4663":"# Silhouette analysis\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50)\n    kmeans.fit(rfm_df_scaled)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(rfm_df_scaled, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","83a757d0":"# Final model with k=3\nkmeans = KMeans(n_clusters=3, max_iter=50)\nkmeans.fit(rfm_df_scaled)\n","11828259":" kmeans.labels_","22e233b7":"# assign the label\nrfm['Cluster_Id'] = kmeans.labels_\nrfm.head()","562ace8c":"# Box plot to visualize Cluster Id vs Frequency\n\nsns.boxplot(x='Cluster_Id', y='Amount', data=rfm)","51e1e31e":"\nsns.boxplot(x='Cluster_Id', y='Frequency', data=rfm)","ae3abb5a":"# Box plot to visualize Cluster Id vs Recency\n\nsns.boxplot(x='Cluster_Id', y='Recency', data=rfm)","a076472d":"# Single linkage: \n\nmergings = linkage(rfm_df_scaled, method=\"single\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","5c304a64":"# Complete linkage\n\nmergings = linkage(rfm_df_scaled, method=\"complete\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","e4751307":"# Average linkage\n\nmergings = linkage(rfm_df_scaled, method=\"average\", metric='euclidean')\ndendrogram(mergings)\nplt.show()","e42f542f":"# 3 clusters\ncluster_labels = cut_tree(mergings, n_clusters=3).reshape(-1, )\ncluster_labels","8216c7d2":"# Assign cluster labels\n\nrfm['Cluster_Labels'] = cluster_labels\nrfm.head()","22c7ba36":"# Plot Cluster Id vs Amount\n\nsns.boxplot(x='Cluster_Labels', y='Amount', data=rfm)","5471e64f":"# Plot Cluster Id vs Frequency\n\nsns.boxplot(x='Cluster_Labels', y='Frequency', data=rfm)","5aac8b60":"# Plot Cluster Id vs Recency\n\nsns.boxplot(x='Cluster_Labels', y='Recency', data=rfm)","b87f994f":"In these we select several classes to initialize their respective centre points. The centre points are vector of same length as each data point vector and \u2018v\u2019 as graph It is like mean graph, but it is used arbitrary starting data point using distance epsilon to find parameter of each cluster. It uses top down or bottom approach.\nThe determination of the optimum number of clusters into which the data may be clustered is a fundamental step for any unsupervisedalgorithm. One of the most common methods for evaluating this optimum k value is the Elbow Method.","9fa22344":"During the Wednesday and the Thursday there are high amount of sales that can sold so servers might be busy on those couple of days, are need to maintained correctly.","0814fc4d":"# 6. K-Means","2a5552d5":"# 9. Results (Hierarchical Clustering)","9d4cf8f1":"# 5. Dealing with Outliers","7ad51f5c":"# 8. Hierarchical Clustering","fc04ae91":"**Complete Linkage:**\n\nThe distance between two clusters\u201d, is known as the longest distance between two \npoints in each cluster in complete linkage hierarchical clustering.\n\n![Clustering_complete.png](attachment:32be8080-b100-4a55-9b5c-f475daa58e20.png)","0fcb8b14":"**Average Linkage:**\n\nThe distance between two clusters in average linkage hierarchical clustering is defined as, \u201cthe average distance between each point in one cluster and each point in the other cluster.\n\n\n![Clustering_average.png](attachment:85822676-7c70-4a1d-81f7-2cd53f1f2ce2.png)","284c8f3b":"In the year ending there were highest amount of sales than the beginning of the year.","13b49985":"# 4. Attributes (Amount, Frequency and Recency)","b3a1d9eb":"The United Kingdom has the highest distribution than the other countries.","6212201d":"The equation of Silhouette is: \ud835\udc46\ud835\udc56\ud835\udc59\u210e\ud835\udc5c\ud835\udc62\ud835\udc52\ud835\udc61\ud835\udc61\ud835\udc52 \ud835\udc60\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52 = \ud835\udc5d\u2212\ud835\udc5e \/ \ud835\udc5a\ud835\udc4e\ud835\udc65(\ud835\udc5d,\ud835\udc5e)\n    Where, P is the mean distance of the closest cluster where it is not a member of the data point. \n            In relation to its own cluster, q is the mean intra cluster distance to all the points.\n\u2022 The value of this score varies from -1 to 1.\n\u2022 Which is closer to 1, is very similar in its own cluster to the other point.\n\u2022 It is not identical to the data points in its cluster when the score is -1\n\n","5f4c278b":"**Hierarchical Clustering:**\n\nThe algorithm works on the principal \u201chierarchy of clusters\u201d. In this algorithm two nearest data clusters are merged into same cluster. It terminates when it has only one cluster. This algorithm uses bottom-up approach. The decision of merged the clusters is formed by closeness.\n\n\n**Single Linkage:**\n\nThe distance between two clusters is defined in single linkage hierarchical clustering as \u201cthe shortest distance between two points in each cluster\u201d.\n\n![Clustering_single.png](attachment:94085ba9-fb13-4ba9-8cd1-0471f9f81a3b.png)\n","04b3ad61":"In the year 2011 there were high sales than the 2010.","5a5703ad":"# 7. Results (K-means Clustering)\n\n**Analysis of Results (K-Means clustering):**\n\n1. Customer with Cluster_id 1 has contributed the highest amount and least is the \n   customers with Cluster_id 2.\n2. The most frequent buyers are clients with Cluster-id 1.\n3. Customers are not recent buyers of Cluster-id 2.\n","66894041":"# 2. Loading Dataset","1702e020":"# 3. Basic Analysis","b18fcbbb":"Analysis of Results (Hierarchical Clustering):\n \n1. Customers with cluster_labels 1 are contributed highest amount where as the least \n   are with cluster_labels 0.\n2. Customers with cluster_label 2 are not recent buyers.\n3. Customer with cluster_labels 2 are frequent buyers and followed by customers with \n   cluster_lables 1.","c4a2c8d0":"**Online Retail Dataset: **\n\n* Online retail is a transactional data collection comprising all transactions for a UK-based and registered online retail non-store between 01\/12\/2010 and 09\/12\/2011. The business primarily offers distinctive all-occasion gifts. Many of the firm's clients are wholesalers.\n* The size of this dataset is about 541909 rows and 8 columns (invoice number, Stock code, Description, Quantity, Invoice date, Unit price, Customer ID)\n* We had performed Unsupervised learning algorithms on this dataset like K-Means and Hierarchical clustering ","98ab2b17":"# 1. Basic Imports","f7dc4695":"> ** Thank you...! For Visiting, Please upvote my page and encourage **\n\n![image.png](attachment:88f2392c-08bf-4dee-a953-8f8e22622149.png)"}}