{"cell_type":{"0e377c37":"code","64aaa1aa":"code","efeeaf23":"code","722a65e9":"code","a930151f":"code","91d7268e":"code","7ee2f9b8":"code","998f8d6a":"code","4d409a7d":"code","8de0076c":"code","94a38c20":"code","68fa7783":"code","11e4464d":"code","00485ca0":"code","e3a03559":"code","59810c01":"code","4f46b6ee":"code","bec76eee":"code","97234a9e":"code","13465f70":"code","b85dc399":"code","e7da1cc2":"code","0dddd755":"code","2a7bdd61":"code","2404e8e6":"code","eb4a3325":"markdown","b93a24a8":"markdown","5de2dbee":"markdown","8b36a35f":"markdown","9f595be2":"markdown","6d5b5940":"markdown","e14fea76":"markdown","39e399e6":"markdown","732a761c":"markdown","e0fb4fb6":"markdown","bb27df73":"markdown","3ae22ece":"markdown","91824b79":"markdown"},"source":{"0e377c37":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py","64aaa1aa":"!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev\n!export XLA_USE_BF16=1","efeeaf23":"!pip install -q colored\n!pip install -q transformers","722a65e9":"import os\nimport gc\nimport re\n\nimport time\nimport colored\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom colored import fg, bg, attr\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport torch\nimport torch.nn as nn\nfrom torch.optim import Adam\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\n\nfrom torch.multiprocessing import Pipe, Process\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.utils import shuffle\nfrom transformers import RobertaModel, RobertaTokenizer\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences as pad","a930151f":"EPOCHS = 20\nSPLIT = 0.8\nMAXLEN = 48\nDROP_RATE = 0.3\nnp.random.seed(42)\n\nOUTPUT_UNITS = 3\nBATCH_SIZE = 384\nLR = (4e-5, 1e-2)\nROBERTA_UNITS = 768\nVAL_BATCH_SIZE = 384\nMODEL_SAVE_PATH = 'sentiment_model.pt'","91d7268e":"test_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\ntrain_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')","7ee2f9b8":"test_df.head()","998f8d6a":"train_df.head()","4d409a7d":"class TweetDataset(Dataset):\n    def __init__(self, data, tokenizer):\n        self.data = data\n        self.text = data.text\n        self.tokenizer = tokenizer\n        self.sentiment = data.sentiment\n        self.sentiment_dict = {\"positive\": 0, \"neutral\": 1, \"negative\": 2}\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, i):\n        start, finish = 0, 2\n        pg, tg = 'post', 'post'\n        tweet = str(self.text[i]).strip()\n        tweet_ids = self.tokenizer.encode(tweet)\n\n        attention_mask_idx = len(tweet_ids) - 1\n        if start not in tweet_ids: tweet_ids = start + tweet_ids\n        tweet_ids = pad([tweet_ids], maxlen=MAXLEN, value=1, padding=pg, truncating=tg)\n\n        attention_mask = np.zeros(MAXLEN)\n        attention_mask[1:attention_mask_idx] = 1\n        attention_mask = attention_mask.reshape((1, -1))\n        if finish not in tweet_ids: tweet_ids[-1], attention_mask[-1] = finish, start\n            \n        sentiment = [self.sentiment_dict[self.sentiment[i]]]\n        sentiment = torch.FloatTensor(to_categorical(sentiment, num_classes=3))\n        return sentiment, torch.LongTensor(tweet_ids), torch.LongTensor(attention_mask)","8de0076c":"class Roberta(nn.Module):\n    def __init__(self):\n        super(Roberta, self).__init__()\n        self.softmax = nn.Softmax(dim=1)\n        self.drop = nn.Dropout(DROP_RATE)\n        self.roberta = RobertaModel.from_pretrained(model)\n        self.dense = nn.Linear(ROBERTA_UNITS, OUTPUT_UNITS)\n        \n    def forward(self, inp, att):\n        inp = inp.view(-1, MAXLEN)\n        _, self.feat = self.roberta(inp, att)\n        return self.softmax(self.dense(self.drop(self.feat)))","94a38c20":"model = 'roberta-base'\ntokenizer = RobertaTokenizer.from_pretrained(model)","68fa7783":"def cel(inp, target):\n    _, labels = target.max(dim=1)\n    return nn.CrossEntropyLoss()(inp, labels)*len(inp)\n\ndef accuracy(inp, target):\n    inp_ind = inp.max(axis=1).indices\n    target_ind = target.max(axis=1).indices\n    return (inp_ind == target_ind).float().sum(axis=0)","11e4464d":"m = Roberta(); print(m)","00485ca0":"del m; gc.collect()","e3a03559":"def print_metric(data, batch, epoch, start, end, metric, typ):\n    t = typ, metric, \"%s\", data, \"%s\"\n    if typ == \"Train\": pre = \"BATCH %s\" + str(batch-1) + \"%s  \"\n    if typ == \"Val\": pre = \"\\nEPOCH %s\" + str(epoch+1) + \"%s  \"\n    time = np.round(end - start, 1); time = \"Time: %s{}%s s\".format(time)\n    fonts = [(fg(211), attr('reset')), (fg(212), attr('reset')), (fg(213), attr('reset'))]\n    xm.master_print(pre % fonts[0] + \"{} {}: {}{}{}\".format(*t) % fonts[1] + \"  \" + time % fonts[2])","59810c01":"global val_losses; global train_losses\nglobal val_accuracies; global train_accuracies\n\ndef train_fn(train_df):\n    train_df = shuffle(train_df)\n    train_df = train_df.reset_index(drop=True)\n\n    split = np.int32(SPLIT*len(train_df))\n    val_df, train_df = train_df[split:], train_df[:split]\n\n    val_df = val_df.reset_index(drop=True)\n    val_dataset = TweetDataset(val_df, tokenizer)\n    val_sampler = DistributedSampler(val_dataset, num_replicas=8,\n                                     rank=xm.get_ordinal(), shuffle=True)\n    \n    val_loader = DataLoader(val_dataset, batch_size=VAL_BATCH_SIZE,\n                            sampler=val_sampler, num_workers=0, drop_last=True)\n\n    train_df = train_df.reset_index(drop=True)\n    train_dataset = TweetDataset(train_df, tokenizer)\n    train_sampler = DistributedSampler(train_dataset, num_replicas=8,\n                                       rank=xm.get_ordinal(), shuffle=True)\n\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE,\n                              sampler=train_sampler, num_workers=0, drop_last=True)\n\n    device = xm.xla_device()\n    network = Roberta().to(device)\n    optimizer = Adam([{'params': network.dense.parameters(), 'lr': LR[1]},\n                      {'params': network.roberta.parameters(), 'lr': LR[0]}])\n\n    val_losses, val_accuracies = [], []\n    train_losses, train_accuracies = [], []\n    \n    start = time.time()\n    xm.master_print(\"STARTING TRAINING ...\\n\")\n\n    for epoch in range(EPOCHS):\n\n        batch = 1\n        network.train()\n        fonts = (fg(48), attr('reset'))\n        xm.master_print((\"EPOCH %s\" + str(epoch+1) + \"%s\") % fonts)\n\n        val_parallel = pl.ParallelLoader(val_loader, [device]).per_device_loader(device)\n        train_parallel = pl.ParallelLoader(train_loader, [device]).per_device_loader(device)\n        \n        for train_batch in train_parallel:\n            train_targ, train_in, train_att = train_batch\n            \n            network = network.to(device)\n            train_in = train_in.to(device)\n            train_att = train_att.to(device)\n            train_targ = train_targ.to(device)\n\n            train_preds = network.forward(train_in, train_att)\n            train_loss = cel(train_preds, train_targ.squeeze(dim=1))\/len(train_in)\n            train_accuracy = accuracy(train_preds, train_targ.squeeze(dim=1))\/len(train_in)\n\n            optimizer.zero_grad()\n            train_loss.backward()\n            xm.optimizer_step(optimizer)\n            \n            end = time.time()\n            batch = batch + 1\n            acc = np.round(train_accuracy.item(), 3)\n            print_metric(acc, batch, None, start, end, metric=\"acc\", typ=\"Train\")\n\n        val_loss, val_accuracy, val_points = 0, 0, 0\n\n        network.eval()\n        with torch.no_grad():\n            for val_batch in val_parallel:\n                targ, val_in, val_att = val_batch\n\n                targ = targ.to(device)\n                val_in = val_in.to(device)\n                val_att = val_att.to(device)\n                network = network.to(device)\n            \n                val_points += len(targ)\n                pred = network.forward(val_in, val_att)\n                val_loss += cel(pred, targ.squeeze(dim=1)).item()\n                val_accuracy += accuracy(pred, targ.squeeze(dim=1)).item()\n        \n        end = time.time()\n        val_loss \/= val_points\n        val_accuracy \/= val_points\n        acc = xm.mesh_reduce('acc', val_accuracy, lambda x: sum(x)\/len(x))\n        print_metric(np.round(acc, 3), None, epoch, start, end, metric=\"acc\", typ=\"Val\")\n    \n        xm.master_print(\"\")\n        val_losses.append(val_loss); train_losses.append(train_loss.item())\n        val_accuracies.append(val_accuracy); train_accuracies.append(train_accuracy.item())\n\n    xm.master_print(\"ENDING TRAINING ...\")\n    xm.save(network.state_dict(), MODEL_SAVE_PATH); del network; gc.collect()\n\n    metric_names = ['val_loss_', 'train_loss_', 'val_acc_', 'train_acc_']\n    metric_lists = [val_losses, train_losses, val_accuracies, train_accuracies]\n    \n    for i, metric_list in enumerate(metric_lists):\n        for j, metric_value in enumerate(metric_list):\n            torch.save(metric_value, metric_names[i] + str(j) + '.pt')","4f46b6ee":"FLAGS = {}\ndef _mp_fn(rank, flags): train_fn(train_df)\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","bec76eee":"val_losses = [torch.load('val_loss_{}.pt'.format(i)) for i in range(EPOCHS)]\ntrain_losses = [torch.load('train_loss_{}.pt'.format(i)) for i in range(EPOCHS)]\nval_accuracies = [torch.load('val_acc_{}.pt'.format(i)) for i in range(EPOCHS)]\ntrain_accuracies = [torch.load('train_acc_{}.pt'.format(i)) for i in range(EPOCHS)]","97234a9e":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(val_losses)+1),\n                         y=val_losses, mode=\"lines+markers\", name=\"val\",\n                         marker=dict(color=\"hotpink\", line=dict(width=.5,\n                                                                color='rgb(0, 0, 0)'))))\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(train_losses)+1),\n                         y=train_losses, mode=\"lines+markers\", name=\"train\",\n                         marker=dict(color=\"mediumorchid\", line=dict(width=.5,\n                                                                     color='rgb(0, 0, 0)'))))\n\nfig.update_layout(xaxis_title=\"Epochs\", yaxis_title=\"Cross Entropy\",\n                  title_text=\"Cross Entropy vs. Epochs\", template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\n\nfig.show()","13465f70":"fig = go.Figure()\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(val_accuracies)+1),\n                         y=val_accuracies, mode=\"lines+markers\", name=\"val\",\n                         marker=dict(color=\"hotpink\", line=dict(width=.5,\n                                                                color='rgb(0, 0, 0)'))))\n\nfig.add_trace(go.Scatter(x=np.arange(1, len(train_accuracies)+1),\n                         y=train_accuracies, mode=\"lines+markers\", name=\"train\",\n                         marker=dict(color=\"mediumorchid\", line=dict(width=.5,\n                                                                     color='rgb(0, 0, 0)'))))\n\nfig.update_layout(xaxis_title=\"Epochs\", yaxis_title=\"Accuracy\",\n                  title_text=\"Accuracy vs. Epochs\", template=\"plotly_white\", paper_bgcolor=\"#f0f0f0\")\n\nfig.show()","b85dc399":"network = Roberta()\nnetwork.load_state_dict(torch.load('sentiment_model.pt'))","e7da1cc2":"device = xm.xla_device()\nnetwork = network.to(device)\n\ndef predict_sentiment(tweet):\n    pg, tg = 'post', 'post'\n    tweet_ids = tokenizer.encode(tweet.strip())\n    sent = {0: 'positive', 1: 'neutral', 2: 'negative'}\n\n    att_mask_idx = len(tweet_ids) - 1\n    if 0 not in tweet_ids: tweet_ids = 0 + tweet_ids\n    tweet_ids = pad([tweet_ids], maxlen=MAXLEN, value=1, padding=pg, truncating=tg)\n\n    att_mask = np.zeros(MAXLEN)\n    att_mask[1:att_mask_idx] = 1\n    att_mask = att_mask.reshape((1, -1))\n    if 2 not in tweet_ids: tweet_ids[-1], att_mask[-1] = 2, 0\n    tweet_ids, att_mask = torch.LongTensor(tweet_ids), torch.LongTensor(att_mask)\n    return sent[np.argmax(network.forward(tweet_ids.to(device), att_mask.to(device)).detach().cpu().numpy())]","0dddd755":"predict_sentiment(\"It does not look good now ...\")","2a7bdd61":"predict_sentiment(\"I want to know more about your product.\")","2404e8e6":"predict_sentiment(\"I have done something good today and so should you :D\")","eb4a3325":"<img src=\"https:\/\/i.imgur.com\/fOeJmu3.jpg\" width=\"600px\">","b93a24a8":"# Introduction\n\nHello everyone! In this project, I will try to finetune roBERTa base to predict the sentiment of a tweet. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. Building machine learning models to understand the sentiment behind a tweet can help drive decisions by many parties and leverage the large amount of information on Twitter. \n\nI will use **PyTorch XLA** (PyTorch for TPUs) and **huggingface transformers** for this project.","5de2dbee":"## Load model\n\n* We now load the model to evaluate its performance.","8b36a35f":"## Define PyTorch Dataset\n\n* Now we define a PyTorch Dataset which will help us feed data to the roBERTa model for training and inference.\n* We remove leading and trailing whitespaces using .strip(), tokenize the values using huggingface, and pad the tokens using keras.","9f595be2":"## Install and import libraries\n\n* We will import several different packages and libraries required for different parts of the project. For example, we import <code>numpy<\/code> and <code>pandas<\/code> for data manipulation, <code>torch<\/code> and <code>torch_xla<\/code> for modeling, and <code>plotly<\/code> for visualization.","6d5b5940":"## Define roBERTa-base model\n\n* Now, we get to the interesting part: modeling! roBERTa base is a pretrained language model by Facebook AI.\n* We will use roBERTa with pretrained weights and add a (Dropout + Dense) head to to use it as a text classifier.","e14fea76":"## Define tokenizer\n\n* Here we simply define the RobertaTokenizer from huggingface which we use to generate tokens from words.","39e399e6":"## Set up PyTorch-XLA\n\n* These few lines of code sets up PyTorch XLA for us.\n* We need PyTorch XLA to help us train PyTorch models on TPU.","732a761c":"## Train model on all 8 TPU cores\n\n* Now, we will train the roBERTa base model to classify tweet sentiments.\n* We define a simple training loop in PyTorch to train the model and validate it after each epoch.\n* We parallelize the training on all 8 TPU cores using <code>xmp.spawn<\/code> from PyTorch XLA (distributes training).\n* We aslo use <code>DistributedSampler<\/code> and <code>ParallelLoader<\/code> to parallelize data sampling and model training.","e0fb4fb6":"## Define cross entropy and accuracy\n\n* Here we implement categorical cross entropy and accuracy functions in PyTorch.\n* CEL is the loss function which is commonly used in classification tasks and helps us finetune roBERTa's weights.","bb27df73":"## Visualize loss and accuracy over time\n\n* We now visualize how the loss and accuracy of the model change over time.\n* We can see that the model eventually converges to around 80% accuracy towards the end.","3ae22ece":"## Define hyperparameters and load data\n\n* Here, we define the required hyperparameters such as the training batch size, learning rate, train\/val split, etc.\n* We also load the training and tessting data required for the project using the read_csv function (from <code>pandas<\/code>)","91824b79":"## Sample sentiment prediction\n\n* We will now see how the model performs on sample comments.\n* It appears to classify sentiment pretty accurately in these simple examples."}}