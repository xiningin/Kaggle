{"cell_type":{"9a550fe1":"code","893166a4":"code","06399d2e":"code","128eb99c":"code","79b09c62":"code","a108b7b8":"code","d1434572":"code","b8b0e857":"code","931a4ca8":"code","5995249f":"code","bc8a1f38":"markdown","f476482e":"markdown","c0805ccc":"markdown"},"source":{"9a550fe1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):#\u904d\u6b77\u8cc7\u6599\u593e\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","893166a4":"df = pd.read_csv('\/kaggle\/input\/patient\/dataset.csv')\n#df.info()\n\nprint('Negative : {}'.format(len(df[df['hospital_death']==0])))\nprint('Positive : {}'.format(len(df[df['hospital_death']==1])))","06399d2e":"# df.info()     # view overall information\n# df.describe() # view descriptive statistic\n\n# solve_missing {'drop' , 'mean', 'median'}\ndef clean_data(dataframe, \n               solve_missing : str ='drop', \n               solve_missing_cat : str = 'drop',\n               drop_list : list = [], \n               category_list : list = []): \n    df = dataframe.drop_duplicates()\n    df = df.drop(drop_list, axis=1)#\u628a\u6c7a\u5b9a\u4e0d\u9700\u8981\u7684\u6b04\u4f4d\u79fb\u9664\n    if solve_missing == 'drop' or solve_missing_cat == 'drop':\n        df = df.dropna()#\u6e05\u9664\u6b04\u4f4d\u6709NaN\u7684\u5217\n        object_dtype = list(df.select_dtypes(include='object').columns)\n        for col in df.columns:\n            if col in category_list or col in object_dtype:\n                df[col] = df[col].astype('category')\n    else:\n        object_dtype = list(df.select_dtypes(include='object').columns)\n        for col in df.columns:\n            if col in category_list or col in object_dtype:\n                df[col] = df[col].astype('category')\n                if solve_missing_cat == 'mode':#\u5c0d\u7f3a\u9677\u503c\u88dc\u96f6\n                    df[col] = df[col].fillna(df[col].mode()[0], inplace=False)\n            else:\n                if solve_missing == 'mean':#\u5c0d\u7f3a\u9677\u503c\u88dc\u5e73\u5747\u6578\n                    df[col] = df[col].fillna(df[col].mean(), inplace=False)\n                else:#\u5c0d\u7f3a\u9677\u503c\u88dc\u4e2d\u4f4d\u6578\n                    df[col] = df[col].fillna(df[col].median(), inplace=False)\n    return df\n\ndef split_xy(dataframe, label : str):\n    return dataframe.drop(labels=label, axis=1), dataframe[label]\n\ndef encode(df):\n    category_dtype = list(df.select_dtypes(include='category').columns)\n    cat = pd.get_dummies(df, columns = category_dtype, drop_first = True)\n    for col in cat.columns:\n        if cat[col].dtype == np.uint8:\n            cat[col] = cat[col].astype('category')\n    return cat\n\ndef scale(features : tuple):\n    trainFeatures, valFeatures, testFeatures = features # type pandas DataFrame\n    scaler = StandardScaler()\n    category_dtype = list(trainFeatures.select_dtypes(include='category').columns)\n    continuous_dtype = list(filter(lambda c: c not in category_dtype, trainFeatures.columns))\n    # SCALING\n    scaler.fit(trainFeatures[continuous_dtype])\n    cont_xtrain = scaler.transform(trainFeatures[continuous_dtype])#\u5c07\u67d0\u4e9b\u6b04\u4f4d\u7684\u503c\u6a19\u6e96\u5316\n    cont_xval = scaler.transform(valFeatures[continuous_dtype])\n    cont_xtest = scaler.transform(testFeatures[continuous_dtype])\n    # ENCODING\n    cat_xtrain = trainFeatures[category_dtype]\n    cat_xval = valFeatures[category_dtype]\n    cat_xtest = testFeatures[category_dtype]\n    print(cat_xtrain.shape, cat_xval.shape, cat_xtrain.shape)\n    xtrain = np.concatenate((cont_xtrain, cat_xtrain), axis=1)\n    xval = np.concatenate((cont_xval, cat_xval), axis=1)\n    xtest = np.concatenate((cont_xtest, cat_xtest), axis=1)\n    return scaler, xtrain, xval, xtest","128eb99c":"category_list = ['elective_surgery', 'ethnicity', 'icu_admit_source', 'icu_stay_type', 'icu_type', 'apache_3j_bodysystem', 'apache_2_bodysystem', \n                 'aids', 'cirrhosis', 'diabetes_mellitus', 'hepatic_failure', 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis', 'hospital_death']\ndrop_list = ['encounter_id', 'patient_id', 'hospital_id', 'Unnamed: 83']\nclean_df = clean_data(df, 'drop', 'drop', drop_list, category_list)#\u6e05\u9664\u4e0d\u9700\u8981\u7684\u6b04\u4f4d\nprint('Negative : {}'.format(len(clean_df[clean_df['hospital_death']==0])))\nprint('Positive : {}'.format(len(clean_df[clean_df['hospital_death']==1])))\n# clean_df.info()\nfeatures, labels = split_xy(clean_df, 'hospital_death')#\u5c07\u7279\u5fb5\u53ca\u6a19\u7c64\u5206\u958b\nprint(features.shape)\nprint(labels.shape)\n####\nencoded_features = encode(features)#one hot encoding\nprint('Encoded Features Shape : {}'.format(encoded_features.shape))\nX_train, X_test, ytrain, ytest = train_test_split(encoded_features, labels, test_size = 0.2, stratify = labels)#\u5c07\u5168\u90e8\u8cc7\u6599\u5206\u70ba\u8a13\u7df4\u8cc7\u6599\u3001\u6e2c\u8a66\u8cc7\u6599\u3001\u8a13\u7df4\u6a19\u7c64\u3001\u6e2c\u8a66\u6a19\u7c64\nX_test, X_val, ytest, yval = train_test_split(X_test, ytest, test_size = 0.5, stratify = ytest)#\u5c07\u6e2c\u8a66\u8cc7\u6599\u5206\u70ba\u6e2c\u8a66\u53ca\u9a57\u8b49\nscaler, xtrain, xval, xtest = scale((X_train, X_val, X_test))#\u8cc7\u6599\u6a19\u6e96\u5316\nprint(scaler.mean_)\nprint(scaler.var_)\nprint(xtrain.shape, ytrain.shape)\nprint(xval.shape, yval.shape)\nprint(xtest.shape, ytest.shape)\nxtrain = np.array(xtrain, dtype='float32')\nxval = np.array(xval, dtype='float32')\nxtest = np.array(xtest, dtype='float32')\nytrain = np.array(ytrain, dtype='float32')\nyval = np.array(yval, dtype='float32')\nytest = np.array(ytest, dtype='float32')","79b09c62":"import torch\nimport torch.nn.functional as F\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torchmetrics import Accuracy, MeanSquaredError, Precision, Recall\n\nclass Model(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim, 1024)\n        self.gnorm1 = nn.GroupNorm(32, 1024)\n        \n        self.layer2 = nn.Linear(1024, 512)\n        self.gnorm2 = nn.GroupNorm(16, 512)\n        \n        self.layer3 = nn.Linear(512, 128)\n        self.gnorm3 = nn.GroupNorm(4, 128)\n        self.layer4 = nn.Linear(128, output_dim)\n        \n    def forward(self, x):\n        x = F.relu(self.gnorm1(self.layer1(x)))\n        x = F.relu(self.gnorm2(self.layer2(x)))\n        x = F.relu(self.gnorm3(self.layer3(x)))\n        x = torch.sigmoid(self.layer4(x))\n        return x\n\ndef L2(params):\n    l2_lambda = 0.001\n    l2_reg = torch.tensor(0.)\n    for param in params:\n        l2_reg += torch.sum(torch.pow(param, 2))\n    return l2_lambda * l2_reg","a108b7b8":"from torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):#Map-style datasets\n    def __init__(self, x, y, device):\n        self._x = x\n        self._y = y\n        self._device = device\n\n    def __len__(self):\n        return len(self._x)\n\n    def __getitem__(self, idx):\n        X, Y = self._x[idx], self._y[idx].ravel()\n        return torch.as_tensor(X, dtype=torch.float32, device=self._device), torch.as_tensor(Y, dtype=torch.float32, device=self._device)\n    \ndef train_loop(dataloader, valloader, model, loss_fn, optimizer, max_iter, metrics : dict, device):\n    model.train()#\u555f\u7528dropout\u3001BatchNormalization\n    history = dict()\n    history['Loss'] = []\n    history['val_Loss'] = []\n    for m in metrics:\n        history[m['name']] = []\n        history['val_{}'.format(m['name'])] = []\n    size = len(dataloader.dataset)\n    for itr in range(max_iter):\n        real_time = dict()\n        real_time['Loss'] = []\n        for m in metrics:\n            real_time[m['name']] = []\n        for batch, (X, y) in enumerate(dataloader):\n            # Compute prediction and loss\n            X.to(device)\n            y.to(device)\n            pred = model(X)\n            loss = loss_fn(pred, y)#\u9810\u6e2c\u8207\u771f\u6b63\u7b54\u6848\u8a08\u7b97\u640d\u5931\n            loss += L2(model.parameters())\n            # Backpropagation\n            optimizer.zero_grad()#\u6e05\u7a7a\u524d\u4e00\u6b21\u7684\u68af\u5ea6\n            loss.backward()#\u6839\u64daloss \u8a08\u7b97\u68af\u5ea6\n            optimizer.step()#\u68af\u5ea6\u4e0b\u964d\n            # compute metrics\n            real_time['Loss'].append(loss.item())\n            for m in metrics:\n                if m['type'] == 'float':\n                    real_time[m['name']].append(m['fn'](pred, y).item())\n                else:\n                    real_time[m['name']].append(m['fn'](pred, y.type(torch.uint8)).item())\n        history['Loss'].append(np.mean(real_time['Loss']))\n        for m in metrics:\n            history[m['name']].append(np.mean(real_time[m['name']]))\n            print(f\"{m['name']}: {np.mean(real_time[m['name']]):>8f}\", end='\\t')\n        print('')\n        epoch_loss = np.mean(real_time['Loss'])\n        print(f\"Loss: {epoch_loss:>7f} [{itr:>5d}\/{max_iter:>5d}]\")\n        print('Val', end = '\\t')\n        tmp = dict()\n        tmp['Loss'] = []\n        for m in metrics:\n            tmp[m['name']] = []\n        model.eval()#\u4e0d\u555f\u7528dropout\u3001BatchNormalization\n        with torch.no_grad():\n            for X, y in valloader:\n                X.to(device)\n                y.to(device)\n                pred = model(X)\n                test_loss = loss_fn(pred, y)\n                tmp['Loss'].append(test_loss.item())\n                for m in metrics:\n                    if m['type'] == 'float':\n                        tmp[m['name']].append(m['fn'](pred, y).item())\n                    else:\n                        tmp[m['name']].append(m['fn'](pred, y.type(torch.uint8)).item())\n        for k, v in tmp.items():\n            print(f\"{k}: {np.mean(v):>8f}\", end='\\t')\n            history['val_{}'.format(k)].append(np.mean(v))\n        print('###END')\n        #torch.save(model, f'.\/model_E{itr:>5d}.pt')\n    return history","d1434572":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel     = Model(xtrain.shape[-1], 1)#\u547c\u53eb\u81ea\u5b9a\u7fa9\u6a21\u578b\nmodel.to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.000001)\n\nif torch.cuda.is_available():\n    loss_fn   = nn.BCELoss().cuda()#binary cross entropy\n    metrics = [\n        {'name': 'Accuracy', 'fn' : Accuracy().cuda(), 'type' : 'int'},\n        {'name': 'MeanSquaredError', 'fn': MeanSquaredError().cuda(), 'type' : 'float'},\n        {'name': 'Precision', 'fn': Precision().cuda(), 'type': 'int'},\n        {'name': 'Recall', 'fn': Recall().cuda(), 'type':'int'}\n    ]\nelse:\n    loss_fn = nn.BCELoss().cpu()\n    metrics = [\n        {'name': 'Accuracy', 'fn' : Accuracy().cpu(), 'type' : 'int'},\n        {'name': 'MeanSquaredError', 'fn': MeanSquaredError().cpu(), 'type' : 'float'},\n        {'name': 'Precision', 'fn': Precision().cpu(), 'type' : 'int'},\n        {'name': 'Recall', 'fn': Recall().cpu(), 'type' : 'int'}\n    ]\ntrain_dataset = CustomDataset(xtrain, ytrain, device)\nval_dataset = CustomDataset(xval, yval, device)\ntest_dataset = CustomDataset(xtest, ytest, device)\ntrain_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)","b8b0e857":"epochs = 5\nhistory = train_loop(train_loader, val_loader, model, loss_fn, optimizer, epochs, metrics, device)","931a4ca8":"import matplotlib.pyplot as plt","5995249f":"fig, ax = plt.subplots(6, 1)\nax[0].plot([i+1 for i in range(len(history['Loss']))], history['Loss'], color='black', linewidth=1)\nax[0].plot([i+1 for i in range(len(history['Loss']))], history['val_Loss'], color='blue', linewidth=1)\nax[0].set_ylabel('Loss')","bc8a1f38":"# **Charts**","f476482e":"# **Model**","c0805ccc":"# **Data Loading**"}}