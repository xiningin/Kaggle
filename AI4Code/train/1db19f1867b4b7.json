{"cell_type":{"6e54df55":"code","9dab58a7":"code","532322e4":"code","f97a3486":"code","e2efa4e5":"code","73390b7c":"code","309958db":"code","66cb15de":"code","c98d32b3":"code","c335b8e0":"code","87441409":"code","048a430b":"code","096f17a2":"code","d5102bfa":"code","2e30f678":"code","8b122ab6":"code","3c73f096":"code","585d654b":"code","4189209d":"code","f6874a45":"code","5d823e4e":"code","d3a4db00":"code","008e485e":"code","be89845d":"code","ed64041a":"code","3b3e000f":"code","17ad7d6a":"code","76bccaf7":"code","c691fc95":"markdown","b2ec1689":"markdown","30aaae77":"markdown","3888b276":"markdown","95c413b9":"markdown","8e0e5694":"markdown","d2b193cb":"markdown","36fd8a2e":"markdown","02966cb1":"markdown","373d7811":"markdown","e6d3de16":"markdown","c691cef7":"markdown","0dcee5b9":"markdown","e5e65054":"markdown","f857cc54":"markdown","332d0daf":"markdown","d8364403":"markdown","0ec5e2c8":"markdown","f1c96371":"markdown","6e0d3623":"markdown","e16e3f80":"markdown","cb20f8ff":"markdown"},"source":{"6e54df55":"#For ignoring warning\nimport warnings\nwarnings.filterwarnings('ignore', category = DeprecationWarning)","9dab58a7":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport re\nfrom keras.preprocessing.text import text_to_word_sequence\nimport nltk\nfrom nltk.stem import WordNetLemmatizer\nimport unidecode\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom sklearn.svm import SVC\nfrom sklearn.multiclass import OneVsRestClassifier\nimport json\n\n","532322e4":"train = pd.read_json(\"..\/input\/whats-cooking-kernels-only\/train.json\")\ntest = pd.read_json(\"..\/input\/whats-cooking-kernels-only\/test.json\")","f97a3486":"train.head()","e2efa4e5":"test.head()","73390b7c":"print(train.shape)                 \nprint(train.columns)","309958db":"print(test.shape)                 \nprint(test.columns)","66cb15de":"train.isnull().sum()","c98d32b3":"plt.figure(figsize=(16,5))\nplt.xticks(rotation=60)\nax= sns.countplot(x='cuisine', data= train, order = train['cuisine'].value_counts().index)\n#print(train.cuisine.value_counts())\n\n","c335b8e0":"print('Maximum Number of Ingredients in a recipe: ',train['ingredients'].str.len().max())\nprint('Minimum Number of Ingredients in a recipe: ',train['ingredients'].str.len().min())","87441409":"#no of Ingredients\ntrain['ing_count'] = train['ingredients'].str.len()","048a430b":"#distribution of number of ingredients\nplt.figure(figsize=(10,5))\nsns.kdeplot(data=train[\"ing_count\"], shade=True)\nplt.title('kdeplot of ingredient count',fontweight=\"bold\")","096f17a2":"plt.figure(figsize=(16,6))\nsns.countplot(x='ing_count', data= train)","d5102bfa":"train[train['ing_count'] >= 40]","2e30f678":"train[train['ing_count'] <= 1]","8b122ab6":"# Taking Out all the ingredients in the dataset and storing in a list\ningredients_list = [ing for ingredients in train['ingredients'] for ing in ingredients]","3c73f096":"from collections import Counter\ningredients_count = pd.Series(dict(Counter(','.join(ingredients_list).split(',')))).sort_values(ascending=False)\ntop20ingredients = ingredients_count.head(20)\n\nplt.figure(figsize=(15,5))\nsns.barplot(x= top20ingredients.index, y=top20ingredients)\nplt.xticks(rotation=60)\nplt.title('20 common ingredients', fontsize=15, fontweight='bold')\nplt.xlabel('ingredients')\nplt.show()","585d654b":"train = train[train['ing_count'] > 1]\ntrain = train[train['ing_count']<60]","4189209d":"lemmatizer = WordNetLemmatizer()\ndef preprocess(ingredients):\n    ingredients = ' '.join(ingredients)\n    ingredients = ingredients.lower() #Convert to lowercase\n    ingredients = re.sub('[,\\.!?:()\"]', '',ingredients) # remove punctuation marks \n    ingredients = re.sub('[^a-zA-Z\"]',' ',ingredients) # remove all strings that contain a non-letter\n    ingredients = ingredients.replace('-', ' ')\n    words = []\n    for word in ingredients.split():\n        word = re.sub(\"[0-9]\",\" \",word) #removing numbers\n        word = re.sub((r'\\b(oz|ounc|ounce|pound|lb|inch|inches|kg|to)\\b'), ' ', word) # Removing Units\n        if len(word) <= 2: continue\n        word = unidecode.unidecode(word)\n        word = lemmatizer.lemmatize(word)\n        if len(word) > 0: words.append(word)\n    return ' '.join(words)\n","f6874a45":"train['x'] = train['ingredients'].progress_apply(preprocess)\ntest['x'] = test['ingredients'].progress_apply(preprocess)\ntrain.head()","5d823e4e":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(sublinear_tf=True) \n# sublinear_tf scaling addresses the problem that 20 occurrences of a word is probably not 20 times more important than 1 occurrence","d3a4db00":"X_train = vectorizer.fit_transform(train['x'].values)\nX_train.sort_indices()\nX_test = vectorizer.transform(test['x'].values)","008e485e":"label_encoder = LabelEncoder()\nY_train = label_encoder.fit_transform(train['cuisine'].values)","be89845d":"classifier = SVC(C=100, # penalty parameter\n                 kernel='rbf', # kernel type, rbf working fine here\n                 degree=3, # default value\n                 gamma=1, # kernel coefficient\n                 coef0=1, # change to 1 from default value of 0.0\n                 shrinking=True, # using shrinking heuristics\n\t \t\t\t tol=0.001, # stopping criterion tolerance \n\t      \t\t probability=False, # no need to enable probability estimates\n\t      \t\t cache_size=200, # 200 MB cache size\n\t      \t\t class_weight=None, # all classes are treated equally \n\t      \t\t verbose=False, # print the logs \n\t      \t\t max_iter=-1, # no limit, let it run\n          \t\t #decision_function_shape=None, # will use one vs rest explicitly \n          \t\t random_state=None)","ed64041a":"model = OneVsRestClassifier(classifier, n_jobs=4)\nmodel.fit(X_train, Y_train)","3b3e000f":"Y_test = model.predict(X_test)\nY_pred = label_encoder.inverse_transform(Y_test)","17ad7d6a":"test_id = test['id']\nsubmission = pd.DataFrame({'id': test_id, 'cuisine': Y_pred}, columns=['id', 'cuisine'])\nsubmission.to_csv('submission.csv', index=False)","76bccaf7":"submission.head()","c691fc95":"**Remove outliers**","b2ec1689":"OneVsRest is a heuristic method for using binary classification algorithms for multi-class classification.","30aaae77":"# Great ingredients make great food","3888b276":"# Preprocessing","95c413b9":"# Dataset Overview","8e0e5694":"# Importing Libraries","d2b193cb":"**Lets look at the unbelievable ingredients**","36fd8a2e":"The 65 ingredient recipe is from italian cuisine.","02966cb1":"**Basic cleaning**","373d7811":"From the countplot we can observe that most recipes were from italian cuisine, followed by mexican, southern_us.","e6d3de16":"Before Predictive modelling,we need to convert words to numeric values. We can use TfidfVectorizer.","c691cef7":"**Lets create a feature that stores number of ingredients**","0dcee5b9":"ingredient count is right skewed","e5e65054":"# Target Feature- cuisine\n\nOur target feature is the cuisine","f857cc54":"**Most common ingredients**","332d0daf":"No missing values\n\n* We have 39774 unique recipes from different cuisines in the train set\n* We have 9944 unique recipes from different cuisines in the test set\n* We have ingredients list","d8364403":"There seems to be recipe that contains only one ingredient.","0ec5e2c8":"# **Exploratory data Analysis**","f1c96371":"We can see a japanese cuisine with just water as the ingredient. What can that be??","6e0d3623":"We can see recipes with 1 or 2 ingredients. Then there are recipes with more than 30 ingredients. These are outliers and get adversely affect our model. But the thing is that,there are recipes with this number of ingredients.","e16e3f80":"It is not suprising to find salt as the most common ingredients. We can also find olive oil, onions, water as common ingredients.","cb20f8ff":"# Final Model"}}