{"cell_type":{"4932395c":"code","a0f413a0":"code","08fe8943":"code","cce41a50":"code","b06d83d0":"code","38acf79a":"code","b6c968a4":"code","e60eceea":"code","049ebb72":"code","9caabd14":"code","5fbf222a":"code","8ace4550":"code","a0a6edc8":"code","6f8f13a8":"code","5ca172f2":"code","e1ea1465":"code","6853bd1f":"code","a330f403":"code","4ec36e74":"code","76ad0409":"code","31e11c1b":"code","ce0e81e4":"code","c2acf45e":"code","59f2da65":"code","f51edaf5":"code","85a76515":"code","2195242c":"code","a729130c":"code","96883dd5":"code","38043b36":"code","e9d2e1ad":"code","8f2d39ca":"code","fa04fc50":"code","b0d4643a":"code","515e4618":"markdown","e6e7e8b9":"markdown","5a5f4486":"markdown","dd2f6c0f":"markdown","f19ac7f1":"markdown","c7175c97":"markdown","8fdacfd5":"markdown","679f251e":"markdown","71754479":"markdown","c1e3fc33":"markdown","1b8d07f8":"markdown","1afe941e":"markdown","ebcdebdf":"markdown","4fe3e6f8":"markdown","6200d48f":"markdown","9f6f5adc":"markdown","f69d65e2":"markdown","15810958":"markdown","565c3253":"markdown","1d4faf74":"markdown","ff46eb29":"markdown","36cdb8b4":"markdown","9aaf7e76":"markdown","5f30eb39":"markdown","d9db6029":"markdown","0c42a2b5":"markdown","72652962":"markdown","21de315b":"markdown","3762e511":"markdown","4b649ca4":"markdown","269f1f06":"markdown","26224a74":"markdown","4d0d72d3":"markdown"},"source":{"4932395c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a0f413a0":"# We are reading our data\ndf = pd.read_csv(\"..\/input\/heart.csv\")","08fe8943":"# First 5 rows of our data\ndf.head()","cce41a50":"df.target.value_counts()","b06d83d0":"sns.countplot(x=\"target\", data=df, palette=\"bwr\")\nplt.show()","38acf79a":"countNoDisease = len(df[df.target == 0])\ncountHaveDisease = len(df[df.target == 1])\nprint(\"Percentage of Patients Haven't Heart Disease: {:.2f}%\".format((countNoDisease \/ (len(df.target))*100)))\nprint(\"Percentage of Patients Have Heart Disease: {:.2f}%\".format((countHaveDisease \/ (len(df.target))*100)))","b6c968a4":"sns.countplot(x='sex', data=df, palette=\"mako_r\")\nplt.xlabel(\"Sex (0 = female, 1= male)\")\nplt.show()","e60eceea":"countFemale = len(df[df.sex == 0])\ncountMale = len(df[df.sex == 1])\nprint(\"Percentage of Female Patients: {:.2f}%\".format((countFemale \/ (len(df.sex))*100)))\nprint(\"Percentage of Male Patients: {:.2f}%\".format((countMale \/ (len(df.sex))*100)))","049ebb72":"df.groupby('target').mean()","9caabd14":"pd.crosstab(df.age,df.target).plot(kind=\"bar\",figsize=(20,6))\nplt.title('Heart Disease Frequency for Ages')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.savefig('heartDiseaseAndAges.png')\nplt.show()","5fbf222a":"pd.crosstab(df.sex,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#1CA53B','#AA1111' ])\nplt.title('Heart Disease Frequency for Sex')\nplt.xlabel('Sex (0 = Female, 1 = Male)')\nplt.xticks(rotation=0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency')\nplt.show()","8ace4550":"plt.scatter(x=df.age[df.target==1], y=df.thalach[(df.target==1)], c=\"red\")\nplt.scatter(x=df.age[df.target==0], y=df.thalach[(df.target==0)])\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","a0a6edc8":"pd.crosstab(df.slope,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#DAF7A6','#FF5733' ])\nplt.title('Heart Disease Frequency for Slope')\nplt.xlabel('The Slope of The Peak Exercise ST Segment ')\nplt.xticks(rotation = 0)\nplt.ylabel('Frequency')\nplt.show()","6f8f13a8":"pd.crosstab(df.fbs,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#FFC300','#581845' ])\nplt.title('Heart Disease Frequency According To FBS')\nplt.xlabel('FBS - (Fasting Blood Sugar > 120 mg\/dl) (1 = true; 0 = false)')\nplt.xticks(rotation = 0)\nplt.legend([\"Haven't Disease\", \"Have Disease\"])\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","5ca172f2":"pd.crosstab(df.cp,df.target).plot(kind=\"bar\",figsize=(15,6),color=['#11A5AA','#AA1190' ])\nplt.title('Heart Disease Frequency According To Chest Pain Type')\nplt.xlabel('Chest Pain Type')\nplt.xticks(rotation = 0)\nplt.ylabel('Frequency of Disease or Not')\nplt.show()","e1ea1465":"a = pd.get_dummies(df['cp'], prefix = \"cp\")\nb = pd.get_dummies(df['thal'], prefix = \"thal\")\nc = pd.get_dummies(df['slope'], prefix = \"slope\")","6853bd1f":"frames = [df, a, b, c]\ndf = pd.concat(frames, axis = 1)\ndf.head()","a330f403":"df = df.drop(columns = ['cp', 'thal', 'slope'])\ndf.head()","4ec36e74":"y = df.target.values\nx_data = df.drop(['target'], axis = 1)","76ad0409":"# Normalize\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)).values","31e11c1b":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=0)","ce0e81e4":"#transpose matrices\nx_train = x_train.T\ny_train = y_train.T\nx_test = x_test.T\ny_test = y_test.T","c2acf45e":"accuracies = {}\n\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nacc = lr.score(x_test.T,y_test.T)*100\n\naccuracies['Logistic Regression'] = acc\nprint(\"Test Accuracy {:.2f}%\".format(acc))","59f2da65":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(x_train.T, y_train.T)\nprediction = knn.predict(x_test.T)\n\nprint(\"{} NN Score: {:.2f}%\".format(2, knn.score(x_test.T, y_test.T)*100))","f51edaf5":"# try ro find best k value\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(x_train.T, y_train.T)\n    scoreList.append(knn2.score(x_test.T, y_test.T))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(scoreList)*100\naccuracies['KNN'] = acc\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))","85a76515":"from sklearn.svm import SVC","2195242c":"svm = SVC(random_state = 1)\nsvm.fit(x_train.T, y_train.T)\n\nacc = svm.score(x_test.T,y_test.T)*100\naccuracies['SVM'] = acc\nprint(\"Test Accuracy of SVM Algorithm: {:.2f}%\".format(acc))","a729130c":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train.T, y_train.T)\n\nacc = nb.score(x_test.T,y_test.T)*100\naccuracies['Naive Bayes'] = acc\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))","96883dd5":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\ndtc.fit(x_train.T, y_train.T)\n\nacc = dtc.score(x_test.T, y_test.T)*100\naccuracies['Decision Tree'] = acc\nprint(\"Decision Tree Test Accuracy {:.2f}%\".format(acc))","38043b36":"# Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 1000, random_state = 1)\nrf.fit(x_train.T, y_train.T)\n\nacc = rf.score(x_test.T,y_test.T)*100\naccuracies['Random Forest'] = acc\nprint(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(acc))","e9d2e1ad":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","8f2d39ca":"# Predicted values\ny_head_lr = lr.predict(x_test.T)\nknn3 = KNeighborsClassifier(n_neighbors = 3)\nknn3.fit(x_train.T, y_train.T)\ny_head_knn = knn3.predict(x_test.T)\ny_head_svm = svm.predict(x_test.T)\ny_head_nb = nb.predict(x_test.T)\ny_head_dtc = dtc.predict(x_test.T)\ny_head_rf = rf.predict(x_test.T)","fa04fc50":"from sklearn.metrics import confusion_matrix\n\ncm_lr = confusion_matrix(y_test,y_head_lr)\ncm_knn = confusion_matrix(y_test,y_head_knn)\ncm_svm = confusion_matrix(y_test,y_head_svm)\ncm_nb = confusion_matrix(y_test,y_head_nb)\ncm_dtc = confusion_matrix(y_test,y_head_dtc)\ncm_rf = confusion_matrix(y_test,y_head_rf)\n","b0d4643a":"plt.figure(figsize=(24,12))\n\nplt.suptitle(\"Confusion Matrixes\",fontsize=24)\nplt.subplots_adjust(wspace = 0.4, hspace= 0.4)\n\nplt.subplot(2,3,1)\nplt.title(\"Logistic Regression Confusion Matrix\")\nsns.heatmap(cm_lr,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,2)\nplt.title(\"K Nearest Neighbors Confusion Matrix\")\nsns.heatmap(cm_knn,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,3)\nplt.title(\"Support Vector Machine Confusion Matrix\")\nsns.heatmap(cm_svm,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,4)\nplt.title(\"Naive Bayes Confusion Matrix\")\nsns.heatmap(cm_nb,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,5)\nplt.title(\"Decision Tree Classifier Confusion Matrix\")\nsns.heatmap(cm_dtc,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.subplot(2,3,6)\nplt.title(\"Random Forest Confusion Matrix\")\nsns.heatmap(cm_rf,annot=True,cmap=\"Blues\",fmt=\"d\",cbar=False, annot_kws={\"size\": 24})\n\nplt.show()","515e4618":"https:\/\/medium.com\/greyatom\/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb\n","e6e7e8b9":"### Creating Dummy Variables","5a5f4486":"In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.","dd2f6c0f":"## Naive Bayes Algorithm","f19ac7f1":"## <font color=\"magenta\">Accuracy of Naive Bayes: <font color=\"red\">86.89%<\/font><\/font>","c7175c97":"**Decision Tree Algorithm**\n<br>\n![image.png](attachment:image.png)","8fdacfd5":"## <font color=\"orange\">Test Accuracy of SVM Algorithm is <font color=\"red\"> 86.89%<\/font><\/font>","679f251e":"## Confusion Matrix","71754479":"![](http:\/\/)Since 'cp', 'thal' and 'slope' are categorical variables we'll turn them into dummy variables.","c1e3fc33":"### Sklearn Logistic Regression","1b8d07f8":"## K-Nearest Neighbour (KNN) Classification\n<br>\nLet's see what will be score if we use KNN algorithm.","1afe941e":"Our models work fine but best of them are KNN and Random Forest with 88.52% of accuracy. Let's look their confusion matrixes.","ebcdebdf":"1. ## <font color = \"purple\">Our model works with <font color=\"red\">**86.89%**<\/font> accuracy.<\/font>","4fe3e6f8":"## <font color=\"#CFC60E\">Test Accuracy of Decision Tree Algorithm: <font color=\"red\">78.69%<\/font><\/font>","6200d48f":"## Creating Model for Logistic Regression\n<br>\nWe can use sklearn library or we can write functions ourselves. Let's them both. Firstly we will write our functions after that we'll use sklearn library to calculate score.","9f6f5adc":"We will split our data. 80% of our data will be train data and 20% of it will be test data.","f69d65e2":"In statistics, the logistic model (or logit model) is used to model the probability of a certain class or event existing such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. This can be extended to model several classes of events such as determining whether an image contains a cat, dog, lion, etc... Each object being detected in the image would be assigned a probability between 0 and 1 and the sum adding to one.","15810958":"Data contains; <br>\n\n* age - age in years <br>\n* sex - (1 = male; 0 = female) <br>\n* cp - chest pain type <br>\n* trestbps - resting blood pressure (in mm Hg on admission to the hospital) <br>\n* chol - serum cholestoral in mg\/dl <br>\n* fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) <br>\n* restecg - resting electrocardiographic results <br>\n* thalach - maximum heart rate achieved <br>\n* exang - exercise induced angina (1 = yes; 0 = no) <br>\n* oldpeak - ST depression induced by exercise relative to rest <br>\n* slope - the slope of the peak exercise ST segment <br>\n* ca - number of major vessels (0-3) colored by flourosopy <br>\n* thal - 3 = normal; 6 = fixed defect; 7 = reversable defect <br>\n* target - have disease or not (1=yes, 0=no)","565c3253":"Let's say weight = 0.01 and bias = 0.0","1d4faf74":"## Data Exploration","ff46eb29":"**Naive Bayes Algorithm**\n<br>\n<img src=\"https:\/\/s3.ap-south-1.amazonaws.com\/techleer\/204.png\" width=\"500px\"\/>","36cdb8b4":"## Random Forest Classification","9aaf7e76":"## <font color=\"#0FBBAE\">Test Accuracy of Random Forest: <font color=\"red\">88.52%<\/font><\/font>","5f30eb39":"## Decision Tree Algorithm","d9db6029":"In statistics and econometrics, particularly in regression analysis, a dummy variable is one that takes only the value 0 or 1 to indicate the absence or presence of some categorical effect that may be expected to shift the outcome","0c42a2b5":"### Normalize Data\n<br>\n<br>\n<img src=\"https:\/\/beyondbacktesting.files.wordpress.com\/2017\/07\/normalization.png?w=863\" width=\"400px\"\/>","72652962":"**I am new with data science. Please comment me your feedbacks to help me improve myself. Thanks for your time.**","21de315b":"Veri i\u00e7erir; <br>\n\n* ya\u015f - y\u0131l cinsinden ya\u015f <br>\n* seks - (1 = erkek; 0 = kad\u0131n) <br>\n* cp - g\u00f6\u011f\u00fcs a\u011fr\u0131s\u0131 tipi <br>\n* trestbps - dinlenme kan bas\u0131nc\u0131 (hastaneye yat\u0131\u015fta mm Hg cinsinden) <br>\n* chol - serum kolestoral mg \/ dl cinsinden <br>\n* fbs - (a\u00e7l\u0131k kan \u015fekeri> 120 mg \/ dl) (1 = do\u011fru; 0 = yanl\u0131\u015f) <br>\n* restecg - elektrokardiyografik sonu\u00e7lar\u0131n dinlenmesi <br>\n* thalach - ula\u015f\u0131lan maksimum kalp at\u0131\u015f h\u0131z\u0131 <br>\n* exang - egzersize ba\u011fl\u0131 anjin (1 = evet; 0 = hay\u0131r) <br>\n* oldpeak - Dinlenmeye g\u00f6re egzersizle ind\u00fcklenen ST depresyonu <br>\n* e\u011fim - en y\u00fcksek egzersiz ST segmentinin e\u011fimi <br>\n* ca - floroskopi taraf\u0131ndan renklendirilen ana damarlar\u0131n (0-3) say\u0131s\u0131 <br>\n* thal - 3 = normal; 6 = sabit hata; 7 = tersinir kusur <br>\n* hedef - hastal\u0131\u011fa sahip olup olmad\u0131\u011f\u0131na (1 = evet, 0 = hay\u0131r)","3762e511":"# INTRODUCTION\n<br>\nWe have a data which classified if patients have heart disease or not according to features in it. We will try to use this data to create a model which tries predict if a patient has this disease or not. We will use logistic regression (classification) algorithm.","4b649ca4":"Linear SVC Machine learning SVM example with Python. ... The objective of a Linear SVC (Support Vector Classifier) is to fit to the data you provide, returning a \"best fit\" hyperplane that divides, or categorizes, your data.","269f1f06":"## Comparing Models","26224a74":"As you can see above if we define k as 3-7-8 we will reach maximum score. <br>\n## <font color=\"green\">KNN Model's Accuracy is <font color=\"red\">88.52%<\/font><\/font>","4d0d72d3":"## Read Data"}}