{"cell_type":{"a63be075":"code","2e9938bc":"code","9860e50f":"code","6684e290":"code","c30b0cf9":"code","0d4c86a6":"code","bf92a9b3":"code","87abc5c2":"code","bde94c97":"code","70bada75":"code","e48d96ec":"code","5f90e633":"code","66887440":"code","80f81bcf":"code","ac4ea114":"code","5453535c":"code","1c51f57a":"code","674481cb":"code","3749945c":"code","7530bc94":"code","22cbf99e":"code","a16d3b59":"code","70c7bb8e":"code","ee90f286":"code","29ccaab5":"code","c3c9a65d":"code","0d6de203":"code","d2a0c5ce":"code","7536f12c":"code","8b6eafd3":"code","d23601ce":"code","0a40f6a0":"code","43a80158":"code","49abacd4":"code","3af33e75":"code","874afaa4":"code","2ae3c0ed":"code","d928c756":"code","04883f52":"code","f9ea2812":"code","74cdf116":"code","0d261d29":"code","f4d0c792":"code","a9d90e8b":"code","94812f18":"code","5df2b121":"code","2f73e6cf":"code","cc6d8a29":"code","caa5a35a":"code","814c9b16":"code","a0bd45b0":"markdown","1ad1912a":"markdown","6a763dbf":"markdown","8f7fabc7":"markdown","454b0f5a":"markdown","c23bb8b4":"markdown","f40e3f47":"markdown","5c8196bc":"markdown"},"source":{"a63be075":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport torch\nimport category_encoders as ce\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold,StratifiedKFold","2e9938bc":"sample_submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv\")","9860e50f":"# for col in train.columns:\n#     print(col, \" Missing Data Count: \",train[col].isnull().sum())","6684e290":"train['target'].value_counts().plot(kind = 'barh',color=\"gray\")","c30b0cf9":"train.shape,test.shape","0d4c86a6":"#!pip install pycaret","bf92a9b3":"#!pip show pycaret","87abc5c2":"train.head()","bde94c97":"feature_cols = train.columns[train.columns.str.startswith('feature_')]","70bada75":"num_cols = 5\nplt.figure(figsize = (10, 5))\nf, axes = plt.subplots(nrows=10, ncols=5, figsize=(20, 30))\nfor index, col in enumerate(feature_cols):\n    i,j = (index \/\/ num_cols, index % num_cols)\n    sns.kdeplot(train.loc[train['target'] == 'Class_1', col] , color=\"gray\",ax=axes[i,j])\n    sns.kdeplot(train.loc[train['target'] == 'Class_2', col] , color=\"red\",ax=axes[i,j])\n    sns.kdeplot(train.loc[train['target'] == 'Class_3', col] , color=\"black\",ax=axes[i,j])\n    sns.kdeplot(train.loc[train['target'] == 'Class_4', col] , color=\"maroon\",ax=axes[i,j])\n    \nplt.title('Distribution of Target');\nplt.tight_layout()\nplt.show()","e48d96ec":"sns.heatmap(train.corr(),annot=False, vmin=0, vmax=0.05,linewidths=0.3) \nfig=plt.gcf()\nfig.set_size_inches(25,12)\nplt.show()","5f90e633":"# %%time\n# from sklearn.manifold import TSNE\n# tsne_model = TSNE(n_components=2)\n# transformed = tsne_model.fit_transform(train[feature_cols])","66887440":"# scatter = plt.scatter(transformed[:,0], transformed[:,1], c=train['target'],color=train.target,labels={'color': 'target'})\n# handles, _ = scatter.legend_elements(prop='colors')\n# #plt.legend(handles, labels)\n# plt.show","80f81bcf":"# # # initialize setup\n# from pycaret.classification import * \n# s = setup(train, target = 'target', log_experiment = True, experiment_name = 'ModelExperiment 1')","ac4ea114":"#Referenced from https:\/\/www.kaggle.com\/pranjalverma08\/tps-may-21-the-ensemble-approach-with-optuna\n\nparams = {}\nparams[\"objective\"] = \"multiclass\"\nparams[\"num_class\"] = \"4\"\n\nparams[\"boosting\"] = \"gbdt\"\nparams['metric']= \"multi_logloss\",\n\nparams[\"max_depth\"] = 30\n#params[\"min_data_in_leaf\"] = 1\nparams[\"min_child_samples\"] = 77\nparams[\"colsample_bytree\"] = 0.18\nparams[\"subsample\"] = 0.013\n\nparams[\"cat_l2\"] =  10\nparams[\"max_bin\"] =  8\nparams[\"min_data_per_group\"] =  90\n\nparams[\"lambda_l1\"] =  8.80654952977217\nparams[\"lambda_l2\"] = 3.918745336956548e-06\nparams[\"learning_rate\"] = 0.001\nparams[\"feature_fraction\"] = 0.40025260416683445\nparams[\"bagging_fraction\"] = 0.7543739297812108\nparams[\"bagging_freq\"] = 1\nparams[\"num_leaves\"] = 11   #50\nparams[\"n_estimators\"] = 1000\n#params[\"cat_smooth\"] = 60\nparams[\"nthread\"] =  4\nparams[\"verbosity\"] = -1\nparams['early_stopping_rounds'] = 500\nnum_rounds = 1000","5453535c":"newArray = []\nfor i in range(len(train)):\n    if train[\"target\"][i] == \"Class_1\":\n        newArray.append(0)\n    elif train[\"target\"][i] == \"Class_2\":\n        newArray.append(1)\n    elif train[\"target\"][i] == \"Class_3\":\n        newArray.append(2)\n    elif train[\"target\"][i] == \"Class_4\":\n        newArray.append(3)\ntrain[\"target\"] = newArray","1c51f57a":"# Reference https:\/\/www.kaggle.com\/udbhavpangotra\/tps-may-21-extensive-eda-catboost-shap#Model---CATBOOST\nprint(len(train))\ntrain = train[train['feature_5']!=10]\n\ntrain = train[train['feature_6']!=26]\ntrain = train[train['feature_6']!=27]\n\ntrain = train[train['feature_7']!=30]\ntrain = train[train['feature_7']!=31]\n\ntrain = train[train['feature_9']!=17]\n\ntrain = train[train['feature_10']!=16]\n\ntrain = train[train['feature_11']!=12]\n\ntrain = train[train['feature_15']!=20]\n\ntrain = train[train['feature_16']!=18]\n\ntrain = train[train['feature_23']!=18]\ntrain = train[train['feature_23']!=19]\n\ntrain = train[train['feature_27']!=29]\n\ntrain = train[train['feature_28']!=23]\n\ntrain = train[train['feature_29']!=13]\n\ntrain = train[train['feature_33']!=24]\n\ntrain = train[train['feature_32']!=26]\ntrain = train[train['feature_32']!=27]\n\ntrain = train[train['feature_35']!=43]\ntrain = train[train['feature_35']!=-2]\ntrain = train[train['feature_35']!=38]\ntrain = train[train['feature_35']!=39]\n\n\ntrain = train[train['feature_38']!=65]\ntrain = train[train['feature_38']!=55]\ntrain = train[train['feature_38']!=-8]\ntrain = train[train['feature_38']!=-3]\ntrain = train[train['feature_38']!=-2]\ntrain = train[train['feature_38']!=63]\n\ntrain = train[train['feature_39']!=65]\ntrain = train[train['feature_39']!=66]\ntrain = train[train['feature_39']!=-5]\ntrain = train[train['feature_39']!=-3]\ntrain = train[train['feature_39']!=-2]\ntrain = train[train['feature_39']!=63]\n\ntrain = train[train['feature_42']!=37]\ntrain = train[train['feature_42']!=-2]\ntrain = train[train['feature_42']!=-1]\n\ntrain = train[train['feature_43']!=33]\ntrain = train[train['feature_43']!=31]\n\nprint(len(train))","674481cb":"train_df = train[feature_cols]\ny = pd.DataFrame(train['target'])\ny = np.array(y)\ny = y.ravel()\ntest_df = test[feature_cols]","3749945c":"# Calculate the classes weights\n#from lightgbm import LGBMClassifier\nfrom sklearn.utils import class_weight\nclass_ratio = class_weight.compute_class_weight('balanced', np.unique(y), y)\nprint(class_ratio)","7530bc94":"#https:\/\/www.kaggle.com\/omkarchoulwar\/tps-may-21-eda-and-models\n\nl_neg = [19,30,31,32,35,38,39,42]\nlneg = []\nfor i in l_neg:\n    name = 'feature_'+str(i)\n    lneg.append(name)\n    \ncols = [x for x in feature_cols if x not in lneg]\nfor i in cols:\n    train_df[i] = np.sqrt(train_df[i])\n    test_df[i] = np.sqrt(test_df[i])","22cbf99e":"from sklearn.preprocessing import QuantileTransformer,RobustScaler\nrs = RobustScaler()\ntrain_df[feature_cols] = rs.fit_transform(train_df[feature_cols])\ntest_df[feature_cols] = rs.transform(test_df[feature_cols])","a16d3b59":"from sklearn.metrics import accuracy_score\nfrom lightgbm import Dataset, cv\nimport optuna\n\ndef objective(trial, model, train_df, y):\n    train_set = Dataset(train_df[feature_cols], label = y)\n    hparams = {\n        'objective': 'multiclass',\n        'metric': 'multi_logloss',\n        'num_classes': 4,\n        'verbosity': -1,\n        'boosting_type': 'gbdt',\n        'learning_rate': trial.suggest_float('lambda_l1', 0.001, 1),\n        'lambda_l1': trial.suggest_float('lambda_l1', 0.001, 12.0),\n        'lambda_l2': trial.suggest_float('lambda_l2', 0.001, 12.0),\n        'num_leaves': trial.suggest_int('num_leaves', 5, 25),\n        'feature_fraction': trial.suggest_float('feature_fraction', 0.1, 0.9),\n        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.1, 0.9),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 5),        \n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int('max_depth', 5, 12),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.5),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 10, 100),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 10),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 50, 100),\n        'device': 'gpu',\n        'gpu_platform_id': 0,\n        'gpu_device_id': 0,\n    }\n    \n    k = 5\n    cv_results = cv(\n            params = hparams,\n            train_set = train_set,\n            num_boost_round = 2500,\n            nfold = k,\n            stratified = True,\n            early_stopping_rounds = 100,\n            verbose_eval = False,\n        )\n        \n    trial.set_user_attr(\"n_estimators\", len(cv_results['multi_logloss-mean']))\n    # Extract the best score.\n    best_score = cv_results['multi_logloss-mean'][-1]\n    return best_score","70c7bb8e":"study_lgbm = optuna.create_study(direction = 'minimize')\nstudy_lgbm.optimize(lambda trial: objective(trial, 'lgbm', train_df[feature_cols], y), n_trials = 100, timeout = 600)","ee90f286":"print(\"Number of finished trials: \", len(study_lgbm.trials))\nprint(\"Best trial:\")\ntrial = study_lgbm.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\nprint(\"  Number of estimators: {}\".format(trial.user_attrs[\"n_estimators\"]))","29ccaab5":"#Referenced from https:\/\/www.kaggle.com\/pranjalverma08\/tps-may-21-the-ensemble-approach-with-optuna\n\nparams = {}\nparams[\"objective\"] = \"multiclass\"\nparams[\"num_class\"] = \"4\"\n\nparams[\"boosting\"] = \"gbdt\"\nparams['metric']= \"multi_logloss\",\n\nparams[\"max_depth\"] = 10\n#params[\"min_data_in_leaf\"] = 1\nparams[\"min_child_samples\"] = 99\nparams[\"colsample_bytree\"] = 0.18413248040220542\nparams[\"subsample\"] = 0.013\n\nparams[\"cat_l2\"] =  10\nparams[\"max_bin\"] =  8\nparams[\"min_data_per_group\"] =  50\n\nparams[\"lambda_l1\"] =  0.029717160559365505\nparams[\"lambda_l2\"] = 3.5230989314953916\nparams[\"learning_rate\"] = 0.001\nparams[\"feature_fraction\"] = 0.14093762554475536\nparams[\"bagging_fraction\"] = 0.8886889549576145\nparams[\"bagging_freq\"] = 1\nparams[\"num_leaves\"] = 25   #50\nparams[\"n_estimators\"] = 1000\nparams[\"cat_l2\"] = 10\nparams[\"cat_smooth\"] = 10\nparams[\"min_child_samples\"] = 99\nparams[\"cat_l2\"] = 4\nparams[\"nthread\"] =  4\nparams[\"verbosity\"] = -1\nparams['early_stopping_rounds'] = 500\nnum_rounds = 1000","c3c9a65d":"%%time\n\nooflgb = np.zeros((train_df.shape[0],4))\npredictionslgb= np.zeros((test_df.shape[0],4))\n\nfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=2021)\ni=1\n\nfor dev_index, val_index in fold.split(train_df[feature_cols],y):    \n\n    dev_X, val_X = train_df[feature_cols].loc[dev_index,:], train_df[feature_cols].loc[val_index,:]\n    dev_y, val_y = y[dev_index], y[val_index]\n    \n    lgtrain = lgb.Dataset(dev_X, label=dev_y)\n    lgtest = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, num_rounds,\n                          valid_sets=[lgtest], early_stopping_rounds=300, verbose_eval=50)\n    \n    pred_val  = model.predict(val_X, num_iteration=model.best_iteration)\n    pred_test = model.predict(test_df[feature_cols], num_iteration=model.best_iteration)\n      \n    ooflgb[val_index] = pred_val\n    predictionslgb += pred_test\n    \npredictionslgb \/= 5.","0d6de203":"sample_submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\nsample_submission['Class_1'] = np.clip(pd.DataFrame(predictionslgb)[0], 0.08,0.95)\nsample_submission['Class_2'] = np.clip(pd.DataFrame(predictionslgb)[1], 0.08,0.95)\nsample_submission['Class_3'] = np.clip(pd.DataFrame(predictionslgb)[2], 0.08,0.95)\nsample_submission['Class_4'] = np.clip(pd.DataFrame(predictionslgb)[3], 0.08,0.95)\nsample_submission.to_csv(path_or_buf='submission_lgb.csv', index=False)","d2a0c5ce":"!pip install -q \/kaggle\/input\/pytorchtabnet\/pytorch_tabnet-2.0.1-py3-none-any.whl","7536f12c":"from sklearn.model_selection import train_test_split, KFold\nfrom pytorch_tabnet.tab_model import TabNetClassifier","8b6eafd3":"# from sklearn.model_selection import train_test_split\n# X_train, X_valid, y_train, y_valid = train_test_split(train_df, y, test_size=0.33,shuffle=False,random_state=42)\n#X_train.shape, y_train.shape, X_valid.shape, y_valid.shape","d23601ce":"#train_df[feature_cols].nunique().sort_values()","0a40f6a0":"data = pd.concat([train,test],axis=0)\ndata.shape,train_df.shape,test_df.shape","43a80158":"# Features analysed using PyCaret\ndata[\"feature_0\"]  = data[\"feature_0\"].astype('category')\ndata[\"feature_2\"]  = data[\"feature_2\"].astype('category')\ndata[\"feature_5\"]  = data[\"feature_5\"].astype('category')\ndata[\"feature_9\"] = data[\"feature_9\"].astype('category')\ndata[\"feature_10\"] = data[\"feature_10\"].astype('category')\ndata[\"feature_11\"] = data[\"feature_11\"].astype('category')\ndata[\"feature_12\"] = data[\"feature_12\"].astype('category')\ndata[\"feature_13\"] = data[\"feature_13\"].astype('category')\ndata[\"feature_16\"] = data[\"feature_16\"].astype('category')\ndata[\"feature_17\"] = data[\"feature_17\"].astype('category')\ndata[\"feature_18\"] = data[\"feature_18\"].astype('category')\ndata[\"feature_20\"] = data[\"feature_20\"].astype('category')\ndata[\"feature_22\"] = data[\"feature_22\"].astype('category')\ndata[\"feature_23\"] = data[\"feature_23\"].astype('category')\ndata[\"feature_29\"] = data[\"feature_29\"].astype('category')\ndata[\"feature_36\"] = data[\"feature_36\"].astype('category')\ndata[\"feature_37\"] = data[\"feature_37\"].astype('category')\ndata[\"feature_44\"] = data[\"feature_44\"].astype('category')\ndata[\"feature_49\"] = data[\"feature_49\"].astype('category')","49abacd4":"# for col in data.columns[data.dtypes == 'category']:\n#     print(col, data[col].nunique())","3af33e75":"from sklearn.preprocessing import LabelEncoder\ncategorical_columns = []\ncategorical_dims =  {}\nfor col in data.columns[data.dtypes == 'category']:\n    print(col, data[col].nunique())\n    l_enc = LabelEncoder()\n    #train[col] = train[col].fillna(\"VV_likely\")\n    data[col] = l_enc.fit_transform(data[col].values)\n    \n    categorical_columns.append(col)\n    categorical_dims[col] = len(l_enc.classes_)","874afaa4":"test_df = data.loc[data.target.isnull()]\ntrain_df = data.loc[data.target.isnull() == False]","2ae3c0ed":"#features = feature_cols #[ col for col in train.columns if col not in unused_feat+[target]] \ncat_idxs = [ i for i, f in enumerate(feature_cols) if f in categorical_columns]\ncat_dims = [ categorical_dims[f] for i, f in enumerate(feature_cols) if f in categorical_columns]","d928c756":"y = pd.DataFrame(train['target'])\ny = np.array(y)\ny = y.ravel()","04883f52":"train_df.shape,test_df.shape,y.shape","f9ea2812":"train_df.reset_index(drop=True, inplace=True)","74cdf116":"%%time\nfrom sklearn.metrics import accuracy_score\nscores = []\nooftabnet = np.zeros((train_df.shape[0],4))\npredictionstabnet= np.zeros((test_df.shape[0],4))\n\nfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=2021)\ni=1\nn_d = n_a = 50\n\ntabnet_params = dict(verbose=40)\n\nfor dev_index, val_index in fold.split(train_df[feature_cols],y): \n    #print(dev_index)\n    dev_X, val_X = train_df[feature_cols].loc[dev_index,:], train_df[feature_cols].loc[val_index,:]\n    dev_y, val_y = y[dev_index], y[val_index]\n    \n    model = TabNetClassifier(\n    n_d=n_d,n_a=n_a, n_steps=5,\n    gamma=1.5, n_independent=2, n_shared=2,\n    cat_idxs=cat_idxs,\n    cat_dims=cat_dims,\n    cat_emb_dim=1,\n    lambda_sparse=1e-2, momentum=0.3, clip_value=2.,\n    optimizer_fn=torch.optim.Adam,\n    optimizer_params=dict(lr=2e-2),\n    scheduler_params = {\"gamma\": 0.95,\n                     \"step_size\": 10},\n    scheduler_fn=torch.optim.lr_scheduler.StepLR, epsilon=1e-15)\n    \n    model.fit(\n      dev_X.values, dev_y,\n      eval_set=[(val_X.values, val_y)],\n      patience=100,\n      max_epochs=300,\n    )\n    scores.append(accuracy_score(val_y, model.predict(val_X.values)))\n    \n    pred_val  = model.predict_proba(val_X.values)\n    pred_test = model.predict_proba(test_df[feature_cols].values)\n      \n    ooftabnet[val_index] = pred_val\n    predictionstabnet += pred_test\n    \npredictionstabnet \/= 5.","0d261d29":"sample_submission['Class_1'] = np.clip(pd.DataFrame(predictionstabnet)[0], 0.025,0.975)\nsample_submission['Class_2'] = np.clip(pd.DataFrame(predictionstabnet)[1], 0.025,0.975)\nsample_submission['Class_3'] = np.clip(pd.DataFrame(predictionstabnet)[2], 0.025,0.975)\nsample_submission['Class_4'] = np.clip(pd.DataFrame(predictionstabnet)[3], 0.025,0.975)\nsample_submission.to_csv(path_or_buf='submission_tab.csv', index=False)","f4d0c792":"# Reference the hyper parameters from https:\/\/www.kaggle.com\/antonellomartiello\/tps-may-catboost-optuna-clip-probabilities\nparam_cb ={\n    'depth': 3, \n    'l2_leaf_reg': 4.287566030099442, \n    'bagging_temperature': 27.174417642203863, \n    #'auto_class_weights': None, \n    'loss_function': 'MultiClassOneVsAll',\n    'eval_metric': 'AUC',    \n    'grow_policy': 'Lossguide',\n    'bootstrap_type': 'Poisson', \n    'learning_rate': 0.06389970558475937, \n    'max_bin': 484, \n    'min_data_in_leaf': 414,\n    'cat_features': cat_idxs,\n    'task_type':'GPU',\n    'iterations':10000,\n    'random_state':2021,\n    'subsample': 0.13534551086578891}","a9d90e8b":"%%time\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score\nfrom catboost import CatBoostClassifier\n\n\n# categorical_features_indices = np.where(train_df.dtypes =='category')[0]\n# categorical_features_indices\noofcat         = np.zeros((train.shape[0],4))\npredictionscat = np.zeros((test.shape[0],4))\n\n\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=2021)\ni=1\nfor dev_index, val_index in fold.split(train_df[feature_cols],y):    \n    dev_X, val_X = train_df[feature_cols].loc[dev_index,:], train_df[feature_cols].loc[val_index,:]\n    dev_y, val_y = y[dev_index], y[val_index]\n\n    m=CatBoostClassifier(**param_cb)\n    m.fit(dev_X,dev_y,eval_set=[(val_X, val_y)], early_stopping_rounds=100,verbose=100)\n  \n    oofcat[val_index]  = m.predict_proba(val_X.values)\n    pred_test = m.predict_proba(test_df[feature_cols].values)\n    predictionscat += pred_test\n\npredictionscat = predictionscat\/5 ","94812f18":"sample_submission['Class_1'] = pd.DataFrame(predictionscat)[0]\nsample_submission['Class_2'] = pd.DataFrame(predictionscat)[1]\nsample_submission['Class_3'] = pd.DataFrame(predictionscat)[2]\nsample_submission['Class_4'] = pd.DataFrame(predictionscat)[3]\nsample_submission.to_csv(path_or_buf='submission.csv', index=False)","5df2b121":"sample_submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\ncol1 = (pd.DataFrame(predictionscat)[0] * 0.50 + pd.DataFrame(predictionslgb)[0] * 0.50) \ncol2 = (pd.DataFrame(predictionscat)[1] * 0.50 + pd.DataFrame(predictionslgb)[1] * 0.50)\ncol3 = (pd.DataFrame(predictionscat)[2] * 0.50 + pd.DataFrame(predictionslgb)[2] * 0.50)\ncol4 = (pd.DataFrame(predictionscat)[3] * 0.50 + pd.DataFrame(predictionslgb)[3] * 0.50)\nsample_submission['Class_1'] =  col1\nsample_submission['Class_2'] =  col2\nsample_submission['Class_3'] =  col3\nsample_submission['Class_4'] =  col4\nsample_submission.to_csv(path_or_buf='submission_blend_lgb_cat.csv', index=False)","2f73e6cf":"sample_submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\ncol1 = (pd.DataFrame(predictionscat)[0] * 0.80 + pd.DataFrame(predictionstabnet)[0] * 0.20) \ncol2 = (pd.DataFrame(predictionscat)[1] * 0.80 + pd.DataFrame(predictionstabnet)[1] * 0.20)\ncol3 = (pd.DataFrame(predictionscat)[2] * 0.80 + pd.DataFrame(predictionstabnet)[2] * 0.20)\ncol4 = (pd.DataFrame(predictionscat)[3] * 0.80 + pd.DataFrame(predictionstabnet)[3] * 0.20)\nsample_submission['Class_1'] =  col1\nsample_submission['Class_2'] =  col2\nsample_submission['Class_3'] =  col3\nsample_submission['Class_4'] =  col4\nsample_submission.to_csv(path_or_buf='submission_blend_tab_cat.csv', index=False)","cc6d8a29":"sample_submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\ncol1 = (pd.DataFrame(predictionscat)[0] * 0.80 + pd.DataFrame(predictionslgb)[0] * 0.20) \ncol2 = (pd.DataFrame(predictionscat)[1] * 0.80 + pd.DataFrame(predictionslgb)[1] * 0.20)\ncol3 = (pd.DataFrame(predictionscat)[2] * 0.80 + pd.DataFrame(predictionslgb)[2] * 0.20)\ncol4 = (pd.DataFrame(predictionscat)[3] * 0.80 + pd.DataFrame(predictionslgb)[3] * 0.20)\nsample_submission['Class_1'] =  col1\nsample_submission['Class_2'] =  col2\nsample_submission['Class_3'] =  col3\nsample_submission['Class_4'] =  col4\nsample_submission.to_csv(path_or_buf='submission_blend_lgb_cat1.csv', index=False)","caa5a35a":"sample_submission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\ncol1 = (pd.DataFrame(predictionscat)[0] * 0.80 + pd.DataFrame(predictionslgb)[0] * 0.22) \ncol2 = (pd.DataFrame(predictionscat)[1] * 0.80 + pd.DataFrame(predictionslgb)[1] * 0.22)\ncol3 = (pd.DataFrame(predictionscat)[2] * 0.80 + pd.DataFrame(predictionslgb)[2] * 0.22)\ncol4 = (pd.DataFrame(predictionscat)[3] * 0.80 + pd.DataFrame(predictionslgb)[3] * 0.22)\nsample_submission['Class_1'] =  col1\nsample_submission['Class_2'] =  col2\nsample_submission['Class_3'] =  col3\nsample_submission['Class_4'] =  col4\nsample_submission.to_csv(path_or_buf='submission_blend_lgb_cat2.csv', index=False)","814c9b16":"# Finish","a0bd45b0":"### Cat Boost Classifier","1ad1912a":"## **Tabular Playground Series - May 2021**","6a763dbf":"1. Data Visualization\n*     a. Feature Analysis\n*     b. Feature Corelation Maps\n2. Feature Selection\n*     a. PyCaret - Category Features\n3. Model Building\n*     a. LGBM\n*     b. Tabnet\n*     c. CatBoost\n4. Ensemble\n*     a. Blend","8f7fabc7":"### Tabular Playgroud Series - May 2021","454b0f5a":"### Tabnet","c23bb8b4":"### Ensemble","f40e3f47":"### Data Visualization","5c8196bc":"### LGBM"}}