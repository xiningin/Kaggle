{"cell_type":{"11b777ae":"code","17cf8cb4":"code","96530c90":"code","4b0c31d6":"code","f147bfe6":"code","caa1139f":"code","5769aa35":"code","86eb079f":"code","cec65ded":"code","311c212a":"code","ec1685dd":"code","6d4ada65":"code","c6e30613":"code","038db4b9":"code","8ce08366":"code","4fbc7536":"code","71aa1bc0":"code","ff27ee07":"code","816baa89":"code","bd1301a8":"code","6020aec5":"code","dae511f7":"code","0d01e658":"code","837deb8b":"code","df9a7464":"code","da0a1d51":"code","a924a72a":"code","a4fd7348":"code","82919b51":"code","6cbdaae8":"code","7ff806fa":"code","5c584ad6":"code","c6bb70b0":"code","119afe0d":"code","7c63637f":"code","0b14c84e":"code","3a50434d":"code","50301af6":"code","9e26761e":"code","3583235c":"code","90434a17":"code","0cfc29f2":"code","c9a8555e":"code","f3ef0627":"code","02e2201a":"code","7379793a":"code","4299d531":"code","e39dbb73":"code","376ac7d7":"code","e526477b":"code","6133c718":"code","5933f290":"code","75f630bb":"code","e622c0b7":"code","4beccae1":"code","2513f22a":"code","81c2dc1e":"code","3d3a6a1f":"code","93da0878":"code","9b20e66f":"code","6a93315f":"code","5539708f":"code","70303e56":"code","424b0420":"code","3743e7d0":"code","9110dc1a":"code","c8c837da":"code","f36e1dc8":"code","cee19c4a":"code","4de6d756":"code","fc023b0a":"code","8b86e43a":"code","03da86fe":"code","365bd413":"code","dff6cb42":"markdown","9cacd41c":"markdown","d8901761":"markdown","f9a0e705":"markdown","2d761d10":"markdown","290108cf":"markdown","899e97af":"markdown","ac4b18bb":"markdown","b0b4e242":"markdown","931ec4c6":"markdown","32e1b374":"markdown","ad2a7477":"markdown","a335f024":"markdown","f42dc27a":"markdown","1b2ec05d":"markdown","6cf1ec07":"markdown","e76eef27":"markdown","4c3835d5":"markdown","fccdce56":"markdown","a5b89615":"markdown","0d41e256":"markdown","aad3e775":"markdown","4c5c1bc0":"markdown","3a04953f":"markdown","1a0d3581":"markdown","1e6f82e4":"markdown","367b5c32":"markdown","4e6e3de6":"markdown","c6b63d36":"markdown","b1f89b6f":"markdown","1e6a1ad6":"markdown","104bcf1d":"markdown","ab461d52":"markdown","84dce84e":"markdown","2ff98aca":"markdown","ad67ccf3":"markdown","5f0e8ab5":"markdown","5d11c310":"markdown","18514d19":"markdown","c303586f":"markdown","e90c94a5":"markdown","74e1021f":"markdown","c6652574":"markdown","2187a574":"markdown","69dea3bf":"markdown","3a359a17":"markdown","e5b6757e":"markdown","47322588":"markdown","7c0d756e":"markdown","e68eb3fc":"markdown","4dff1d61":"markdown","1aa7a9e8":"markdown","51aa629a":"markdown","103bfa23":"markdown","b7de6a3b":"markdown","3e1b1330":"markdown","811834a1":"markdown","5b580521":"markdown","e620c001":"markdown","36502b3b":"markdown","802a84db":"markdown","97844411":"markdown"},"source":{"11b777ae":"import datetime, warnings, scipy \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nfrom matplotlib.patches import ConnectionPatch\nfrom collections import OrderedDict\nfrom matplotlib.gridspec import GridSpec\nfrom mpl_toolkits.basemap import Basemap\nfrom sklearn import metrics, linear_model\nfrom sklearn.preprocessing import PolynomialFeatures, StandardScaler\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom scipy.optimize import curve_fit\nplt.rcParams[\"patch.force_edgecolor\"] = True\nplt.style.use('fivethirtyeight')\nmpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"last_expr\"\npd.options.display.max_columns = 50\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")","17cf8cb4":"df = pd.read_csv('..\/input\/flights.csv', low_memory=False)\nprint('Dataframe dimensions:', df.shape)\n#____________________________________________________________\n\n# gives some infos on columns types and number of null values\ntab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(df.isnull().sum()\/df.shape[0]*100)\n                         .T.rename(index={0:'null values (%)'}))\ntab_info","96530c90":"#Play a  little bit  with the  data\nflights=pd.read_csv('..\/input\/flights.csv')\nflights.head()","4b0c31d6":"flights.shape","f147bfe6":"airports = pd.read_csv(\"..\/input\/airports.csv\")\n#airports.head()\nairports.shape","caa1139f":"count_flights = df['ORIGIN_AIRPORT'].value_counts()\n#___________________________\nplt.figure(figsize=(11,11))\n#________________________________________\n# define properties of markers and labels\ncolors = ['yellow', 'red', 'lightblue', 'purple', 'green', 'orange']\nsize_limits = [1, 100, 1000, 10000, 100000, 1000000]\nlabels = []\nfor i in range(len(size_limits)-1):\n    labels.append(\"{} <.< {}\".format(size_limits[i], size_limits[i+1])) \n#____________________________________________________________\nmap = Basemap(resolution='i',llcrnrlon=-180, urcrnrlon=-50,\n              llcrnrlat=10, urcrnrlat=75, lat_0=0, lon_0=0,)\nmap.shadedrelief()\nmap.drawcoastlines()\nmap.drawcountries(linewidth = 3)\nmap.drawstates(color='0.3')\n#_____________________\n# put airports on map\nfor index, (code, y,x) in airports[['IATA_CODE', 'LATITUDE', 'LONGITUDE']].iterrows():\n    x, y = map(x, y)\n    isize = [i for i, val in enumerate(size_limits) if val < count_flights[code]]\n    ind = isize[-1]\n    map.plot(x, y, marker='o', markersize = ind+5, markeredgewidth = 1, color = colors[ind],\n             markeredgecolor='k', label = labels[ind])\n#_____________________________________________\n# remove duplicate labels and set their order\nhandles, labels = plt.gca().get_legend_handles_labels()\nby_label = OrderedDict(zip(labels, handles))\nkey_order = ('1 <.< 100', '100 <.< 1000', '1000 <.< 10000',\n             '10000 <.< 100000', '100000 <.< 1000000')\nnew_label = OrderedDict()\nfor key in key_order:\n    new_label[key] = by_label[key]\nplt.legend(new_label.values(), new_label.keys(), loc = 1, prop= {'size':11},\n           title='Number of flights per year', frameon = True, framealpha = 1)\nplt.show()","5769aa35":"print(\"Nb of airports: {}\".format(len(df['ORIGIN_AIRPORT'].unique())))","86eb079f":"origin_nb = dict()\nfor carrier in abbr_companies.keys():\n    liste_origin_airport = df[df['AIRLINE'] == carrier]['ORIGIN_AIRPORT'].unique()\n    origin_nb[carrier] = len(liste_origin_airport)","cec65ded":"test_df = pd.DataFrame.from_dict(origin_nb, orient='index')\ntest_df.rename(columns = {0:'count'}, inplace = True)\nax = test_df.plot(kind='bar', figsize = (8,3))\nlabels = [abbr_companies[item.get_text()] for item in ax.get_xticklabels()]\nax.set_xticklabels(labels)\nplt.ylabel('Number of airports visited', fontsize=14, weight = 'bold', labelpad=12)\nplt.setp(ax.get_xticklabels(), fontsize=11, ha = 'right', rotation = 80)\nax.legend().set_visible(False)\nplt.show()","311c212a":"temp = pd.read_csv('..\/input\/airports.csv')\nidentify_airport = temp.set_index('IATA_CODE')['CITY'].to_dict()\nlatitude_airport = temp.set_index('IATA_CODE')['LATITUDE'].to_dict()\nlongitude_airport = temp.set_index('IATA_CODE')['LONGITUDE'].to_dict()","ec1685dd":"def make_map(df, carrier, long_min, long_max, lat_min, lat_max):\n    fig=plt.figure(figsize=(7,3))\n    ax=fig.add_axes([0.,0.,1.,1.])\n    m = Basemap(resolution='i',llcrnrlon=long_min, urcrnrlon=long_max,\n                  llcrnrlat=lat_min, urcrnrlat=lat_max, lat_0=0, lon_0=0,)\n    df2 = df[df['AIRLINE'] == carrier]\n    count_trajectories = df2.groupby(['ORIGIN_AIRPORT', 'DESTINATION_AIRPORT']).size()\n    count_trajectories.sort_values(inplace = True)\n    \n    for (origin, dest), s in count_trajectories.iteritems():\n        nylat,   nylon = latitude_airport[origin], longitude_airport[origin]\n        m.plot(nylon, nylat, marker='o', markersize = 10, markeredgewidth = 1,\n                   color = 'seagreen', markeredgecolor='k')\n\n    for (origin, dest), s in count_trajectories.iteritems():\n        nylat,   nylon = latitude_airport[origin], longitude_airport[origin]\n        lonlat, lonlon = latitude_airport[dest], longitude_airport[dest]\n        if pd.isnull(nylat) or pd.isnull(nylon) or \\\n                pd.isnull(lonlat) or pd.isnull(lonlon): continue\n        if s < 100:\n            m.drawgreatcircle(nylon, nylat, lonlon, lonlat, linewidth=0.5, color='b',\n                             label = '< 100')\n        elif s < 200:\n            m.drawgreatcircle(nylon, nylat, lonlon, lonlat, linewidth=2, color='r',\n                             label = '100 <.< 200')\n        else:\n            m.drawgreatcircle(nylon, nylat, lonlon, lonlat, linewidth=2, color='gold',\n                              label = '> 200')    \n    #_____________________________________________\n    # remove duplicate labels and set their order\n    handles, labels = plt.gca().get_legend_handles_labels()\n    by_label = OrderedDict(zip(labels, handles))\n    key_order = ('< 100', '100 <.< 200', '> 200')                \n    new_label = OrderedDict()\n    for key in key_order:\n        if key not in by_label.keys(): continue\n        new_label[key] = by_label[key]\n    plt.legend(new_label.values(), new_label.keys(), loc = 'best', prop= {'size':8},\n               title='flights per month', facecolor = 'palegreen', \n               shadow = True, frameon = True, framealpha = 1)    \n    m.drawcoastlines()\n    m.fillcontinents()\n    ax.set_title('{} flights'.format(abbr_companies[carrier]))","6d4ada65":"coord = dict()\ncoord['AA'] = [-165, -60, 10, 55]\ncoord['AS'] = [-182, -63, 10, 75]\ncoord['HA'] = [-180, -65, 10, 52]\nfor carrier in ['AA', 'AS', 'HA']: \n    make_map(df, carrier, *coord[carrier])","c6e30613":"airport_mean_delays = pd.DataFrame(pd.Series(df['ORIGIN_AIRPORT'].unique()))\nairport_mean_delays.set_index(0, drop = True, inplace = True)\n\nfor carrier in abbr_companies.keys():\n    df1 = df[df['AIRLINE'] == carrier]\n    test = df1['DEPARTURE_DELAY'].groupby(df['ORIGIN_AIRPORT']).apply(get_stats).unstack()\n    airport_mean_delays[carrier] = test.loc[:, 'mean'] ","038db4b9":"sns.set(context=\"paper\")\nfig = plt.figure(1, figsize=(8,8))\n\nax = fig.add_subplot(1,2,1)\nsubset = airport_mean_delays.iloc[:50,:].rename(columns = abbr_companies)\nsubset = subset.rename(index = identify_airport)\nmask = subset.isnull()\nsns.heatmap(subset, linewidths=0.01, cmap=\"Accent\", mask=mask, vmin = 0, vmax = 35)\nplt.setp(ax.get_xticklabels(), fontsize=10, rotation = 85) ;\nax.yaxis.label.set_visible(False)\n\nax = fig.add_subplot(1,2,2)    \nsubset = airport_mean_delays.iloc[50:100,:].rename(columns = abbr_companies)\nsubset = subset.rename(index = identify_airport)\nfig.text(0.5, 1.02, \"Delays: impact of the origin airport\", ha='center', fontsize = 18)\nmask = subset.isnull()\nsns.heatmap(subset, linewidths=0.01, cmap=\"Accent\", mask=mask, vmin = 0, vmax = 35)\nplt.setp(ax.get_xticklabels(), fontsize=10, rotation = 85) ;\nax.yaxis.label.set_visible(False)\n\nplt.tight_layout()","8ce08366":"#_________________________________________________________________\n# We select the company and create a subset of the main dataframe\ncarrier = 'AA'\ndf1 = df[df['AIRLINE']==carrier][['ORIGIN_AIRPORT','DESTINATION_AIRPORT','DEPARTURE_DELAY']]\n#___________________________________________________________\n# I collect the routes and list the delays for each of them\ntrajet = dict()\nfor ind, col in df1.iterrows():\n    if pd.isnull(col['DEPARTURE_DELAY']): continue\n    route = str(col['ORIGIN_AIRPORT'])+'-'+str(col['DESTINATION_AIRPORT'])\n    if route in trajet.keys():\n        trajet[route].append(col['DEPARTURE_DELAY'])\n    else:\n        trajet[route] = [col['DEPARTURE_DELAY']]\n#____________________________________________________________________        \n# I transpose the dictionary in a list to sort the routes by origins        \nliste_trajet = []\nfor key, value in trajet.items():\n    liste_trajet.append([key, value])\nliste_trajet.sort()","4fbc7536":"mean_val = [] ; std_val = [] ; x_label = []\n\ni = 0\nfor route, liste_retards in liste_trajet:\n    #_____________________________________________\n    # I set the labels as the airport from origin\n    index = route.split('-')[0]\n    x_label.append(identify_airport[index])\n    #______________________________________________________________________________\n    # I put a threshold on delays to prevent that high values take too much weight\n    trajet2 = [min(90, s) for s in liste_retards]\n    #________________________________________\n    # I compute mean and standard deviations\n    mean_val.append(scipy.mean(trajet2))\n    std_val.append(scipy.std(trajet2))\n    i += 1\n#________________\n# Plot the graph\nfig, ax = plt.subplots(figsize=(10,4))\nstd_min = [ min(15 + mean_val[i], s) for i,s in enumerate(std_val)] \nax.errorbar(list(range(i)), mean_val, yerr = [std_min, std_val], fmt='o') \nax.set_title('Mean route delays for \"{}\"'.format(abbr_companies[carrier]),\n             fontsize=14, weight = 'bold')\nplt.ylabel('Mean delay at origin (minutes)', fontsize=14, weight = 'bold', labelpad=12)\n#___________________________________________________\n# I define the x,y range and positions of the ticks\nimin, imax = 145, 230\nplt.xlim(imin, imax) ; plt.ylim(-20, 45)\nliste_ticks = [imin]\nfor j in range(imin+1,imax):\n    if x_label[j] == x_label[j-1]: continue\n    liste_ticks.append(j)\n#_____________________________\n# and set the tick parameters  \nax.set_xticks(liste_ticks)\nax.set_xticklabels([x_label[int(x)] for x in ax.get_xticks()], rotation = 90, fontsize = 8)\nplt.setp(ax.get_yticklabels(), fontsize=12, rotation = 0)\nax.tick_params(axis='y', which='major', pad=15)\n\nplt.show()","71aa1bc0":"class Figure_style():\n    #_________________________________________________________________\n    def __init__(self, size_x = 11, size_y = 5, nrows = 1, ncols = 1):\n        sns.set_style(\"white\")\n        sns.set_context(\"notebook\", font_scale=1.2, rc={\"lines.linewidth\": 2.5})\n        self.fig, axs = plt.subplots(nrows = nrows, ncols = ncols, figsize=(size_x,size_y,))\n        #________________________________\n        # convert self.axs to 2D array\n        if nrows == 1 and ncols == 1:\n            self.axs = np.reshape(axs, (1, -1))\n        elif nrows == 1:\n            self.axs = np.reshape(axs, (1, -1))\n        elif ncols == 1:\n            self.axs = np.reshape(axs, (-1, 1))\n    #_____________________________\n    def pos_update(self, ix, iy):\n        self.ix, self.iy = ix, iy\n    #_______________\n    def style(self):\n        self.axs[self.ix, self.iy].spines['right'].set_visible(False)\n        self.axs[self.ix, self.iy].spines['top'].set_visible(False)\n        self.axs[self.ix, self.iy].yaxis.grid(color='lightgray', linestyle=':')\n        self.axs[self.ix, self.iy].xaxis.grid(color='lightgray', linestyle=':')\n        self.axs[self.ix, self.iy].tick_params(axis='both', which='major',\n                                               labelsize=10, size = 5)\n    #________________________________________\n    def draw_legend(self, location='upper right'):\n        legend = self.axs[self.ix, self.iy].legend(loc = location, shadow=True,\n                                        facecolor = 'g', frameon = True)\n        legend.get_frame().set_facecolor('whitesmoke')\n    #_________________________________________________________________________________\n    def cust_plot(self, x, y, color='b', linestyle='-', linewidth=1, marker=None, label=''):\n        if marker:\n            markerfacecolor, marker, markersize = marker[:]\n            self.axs[self.ix, self.iy].plot(x, y, color = color, linestyle = linestyle,\n                                linewidth = linewidth, marker = marker, label = label,\n                                markerfacecolor = markerfacecolor, markersize = markersize)\n        else:\n            self.axs[self.ix, self.iy].plot(x, y, color = color, linestyle = linestyle,\n                                        linewidth = linewidth, label=label)\n        self.fig.autofmt_xdate()\n    #________________________________________________________________________\n    def cust_plot_date(self, x, y, color='lightblue', linestyle='-',\n                       linewidth=1, markeredge=False, label=''):\n        markeredgewidth = 1 if markeredge else 0\n        self.axs[self.ix, self.iy].plot_date(x, y, color='lightblue', markeredgecolor='grey',\n                                  markeredgewidth = markeredgewidth, label=label)\n    #________________________________________________________________________\n    def cust_scatter(self, x, y, color = 'lightblue', markeredge = False, label=''):\n        markeredgewidth = 1 if markeredge else 0\n        self.axs[self.ix, self.iy].scatter(x, y, color=color,  edgecolor='grey',\n                                  linewidths = markeredgewidth, label=label)    \n    #___________________________________________\n    def set_xlabel(self, label, fontsize = 14):\n        self.axs[self.ix, self.iy].set_xlabel(label, fontsize = fontsize)\n    #___________________________________________\n    def set_ylabel(self, label, fontsize = 14):\n        self.axs[self.ix, self.iy].set_ylabel(label, fontsize = fontsize)\n    #____________________________________\n    def set_xlim(self, lim_inf, lim_sup):\n        self.axs[self.ix, self.iy].set_xlim([lim_inf, lim_sup])\n    #____________________________________\n    def set_ylim(self, lim_inf, lim_sup):\n        self.axs[self.ix, self.iy].set_ylim([lim_inf, lim_sup])           ","ff27ee07":"carrier = 'WN'\nid_airport = 4\nliste_origin_airport = df[df['AIRLINE'] == carrier]['ORIGIN_AIRPORT'].unique()\ndf2 = df[(df['AIRLINE'] == carrier) & (df['ARRIVAL_DELAY'] > 0)\n         & (df['ORIGIN_AIRPORT'] == liste_origin_airport[id_airport])]\ndf2.sort_values('SCHEDULED_DEPARTURE', inplace = True)","816baa89":"fig1 = Figure_style(11, 5, 1, 1)\nfig1.pos_update(0, 0)\nfig1.cust_plot(df2['SCHEDULED_DEPARTURE'], df2['DEPARTURE_DELAY'], linestyle='-')\nfig1.style() \nfig1.set_ylabel('Delay (minutes)', fontsize = 14)\nfig1.set_xlabel('Departure date', fontsize = 14)\ndate_1 = datetime.datetime(2015,1,1)\ndate_2 = datetime.datetime(2015,1,15)\nfig1.set_xlim(date_1, date_2)\nfig1.set_ylim(-15, 260)","bd1301a8":"#_______________________________\ndef func2(x, a, b, c):\n    return a * x**2 +  b*x + c\n#_______________________________\ndf2['heure_depart'] =  df2['SCHEDULED_DEPARTURE'].apply(lambda x:x.time())\ntest2 = df2['DEPARTURE_DELAY'].groupby(df2['heure_depart']).apply(get_stats).unstack()\nfct = lambda x:x.hour*3600+x.minute*60+x.second\nx_val = np.array([fct(s) for s in test2.index]) \ny_val = test2['mean']\npopt, pcov = curve_fit(func2, x_val, y_val, p0 = [1, 2, 3])\ntest2['fit'] = pd.Series(func2(x_val, *popt), index = test2.index)","6020aec5":"fig1 = Figure_style(8, 4, 1, 1)\nfig1.pos_update(0, 0)\nfig1.cust_plot_date(df2['heure_depart'], df2['DEPARTURE_DELAY'],\n                    markeredge=False, label='initial data points')\nfig1.cust_plot(test2.index, test2['mean'], linestyle='--', linewidth=2, label='mean')\nfig1.cust_plot(test2.index, test2['fit'], color='r', linestyle='-', linewidth=3, label='fit')\nfig1.style() ; fig1.draw_legend('upper left')\nfig1.set_ylabel('Delay (minutes)', fontsize = 14)\nfig1.set_xlabel('Departure time', fontsize = 14)\nfig1.set_ylim(-15, 210)","dae511f7":"df_train = df[df['SCHEDULED_DEPARTURE'].apply(lambda x:x.date()) < datetime.date(2015, 1, 23)]\ndf_test  = df[df['SCHEDULED_DEPARTURE'].apply(lambda x:x.date()) > datetime.date(2015, 1, 23)]\ndf = df_train","0d01e658":"carrier = 'AA'\ncheck_airports = df[(df['AIRLINE'] == carrier)]['DEPARTURE_DELAY'].groupby(\n                         df['ORIGIN_AIRPORT']).apply(get_stats).unstack()\ncheck_airports.sort_values('count', ascending = False, inplace = True)\ncheck_airports[-5:]","837deb8b":"def get_flight_delays(df, carrier, id_airport, extrem_values = False):\n    df2 = df[(df['AIRLINE'] == carrier) & (df['ORIGIN_AIRPORT'] == id_airport)]\n    #_______________________________________\n    # remove extreme values before fitting\n    if extrem_values:\n        df2['DEPARTURE_DELAY'] = df2['DEPARTURE_DELAY'].apply(lambda x:x if x < 60 else np.nan)\n        df2.dropna(how = 'any')\n    #__________________________________\n    # Conversion: date + heure -> heure\n    df2.sort_values('SCHEDULED_DEPARTURE', inplace = True)\n    df2['heure_depart'] =  df2['SCHEDULED_DEPARTURE'].apply(lambda x:x.time())\n    #___________________________________________________________________\n    # regroupement des vols par heure de d\u00e9part et calcul de la moyenne\n    test2 = df2['DEPARTURE_DELAY'].groupby(df2['heure_depart']).apply(get_stats).unstack()\n    test2.reset_index(inplace=True)\n    #___________________________________\n    # conversion de l'heure en secondes\n    fct = lambda x:x.hour*3600+x.minute*60+x.second\n    test2.reset_index(inplace=True)\n    test2['heure_depart_min'] = test2['heure_depart'].apply(fct)\n    return test2","df9a7464":"def linear_regression(test2):\n    test = test2[['mean', 'heure_depart_min']].dropna(how='any', axis = 0)\n    X = np.array(test['heure_depart_min'])\n    Y = np.array(test['mean'])\n    X = X.reshape(len(X),1)\n    Y = Y.reshape(len(Y),1)\n    regr = linear_model.LinearRegression()\n    regr.fit(X, Y)\n    result = regr.predict(X)\n    return X, Y, result","da0a1d51":"id_airport = 'PHL'\ndf2 = df[(df['AIRLINE'] == carrier) & (df['ORIGIN_AIRPORT'] == id_airport)]\ndf2['heure_depart'] =  df2['SCHEDULED_DEPARTURE'].apply(lambda x:x.time())\ndf2['heure_depart'] = df2['heure_depart'].apply(lambda x:x.hour*3600+x.minute*60+x.second)\n#___________________\n# first case\ntest2 = get_flight_delays(df, carrier, id_airport, False)\nX1, Y1, result2 = linear_regression(test2)\n#___________________\n# second case\ntest3 = get_flight_delays(df, carrier, id_airport, True)\nX2, Y2, result3 = linear_regression(test3)","a924a72a":"fig1 = Figure_style(8, 4, 1, 1)\nfig1.pos_update(0, 0)\nfig1.cust_scatter(df2['heure_depart'], df2['DEPARTURE_DELAY'], markeredge = True)\nfig1.cust_plot(X1, Y1, color = 'b', linestyle = ':', linewidth = 2, marker = ('b','s', 10))\nfig1.cust_plot(X2, Y2, color = 'g', linestyle = ':', linewidth = 2, marker = ('g','X', 12))\nfig1.cust_plot(X1, result2, color = 'b', linewidth = 3)\nfig1.cust_plot(X2, result3, color = 'g', linewidth = 3)\nfig1.style()\nfig1.set_ylabel('Delay (minutes)', fontsize = 14)\nfig1.set_xlabel('Departure time', fontsize = 14)\n#____________________________________\n# convert and set the x ticks labels\nfct_convert = lambda x: (int(x\/3600) , int(divmod(x,3600)[1]\/60))\nfig1.axs[fig1.ix, fig1.iy].set_xticklabels(['{:2.0f}h{:2.0f}m'.format(*fct_convert(x))\n                                            for x in fig1.axs[fig1.ix, fig1.iy].get_xticks()]);","a4fd7348":"class fit_polynome:\n\n    def __init__(self, data):\n        self.data = data[['mean', 'heure_depart_min']].dropna(how='any', axis = 0)\n\n    def split(self, method):        \n        self.method = method        \n        self.X = np.array(self.data['heure_depart_min'])\n        self.Y = np.array(self.data['mean'])\n        self.X = self.X.reshape(len(self.X),1)\n        self.Y = self.Y.reshape(len(self.Y),1)\n\n        if method == 'all':\n            self.X_train = self.X\n            self.Y_train = self.Y\n            self.X_test  = self.X\n            self.Y_test  = self.Y                        \n        elif method == 'split':            \n            self.X_train, self.X_test, self.Y_train, self.Y_test = \\\n                train_test_split(self.X, self.Y, test_size=0.3)\n    \n    def train(self, pol_order):\n        self.poly = PolynomialFeatures(degree = pol_order)\n        self.regr = linear_model.LinearRegression()\n        self.X_ = self.poly.fit_transform(self.X_train)\n        self.regr.fit(self.X_, self.Y_train)\n    \n    def predict(self, X):\n        self.X_ = self.poly.fit_transform(X)\n        self.result = self.regr.predict(self.X_)\n    \n    def calc_score(self):        \n        X_ = self.poly.fit_transform(self.X_test)\n        result = self.regr.predict(X_)\n        self.score = metrics.mean_squared_error(result, self.Y_test)","82919b51":"fig = plt.figure(1, figsize=(10,4))\n\nax = ['_' for _ in range(4)]\nax[1]=fig.add_subplot(131) \nax[2]=fig.add_subplot(132) \nax[3]=fig.add_subplot(133) \n\nid_airport = 'BNA'\ntest2 = get_flight_delays(df, carrier, id_airport, True)\n\nresult = ['_' for _ in range(4)]\nscore = [10000 for _ in range(4)]\nfound = [False for _ in range(4)]\nfit = fit_polynome(test2)\n\ncolor = '.rgbyc'\n\ninc = 0\nwhile True:\n    inc += 1\n    fit.split('split')\n    for i in range(1,4):\n        fit.train(pol_order = i)\n        fit.predict(fit.X)\n        result[i] = fit.result\n        fit.calc_score()\n        score[i]  = fit.score\n\n    [ind_min] = [j for j,val in enumerate(score) if min(score) == val]\n    print(\"mod\u00e8le n\u00ba{:<2}, min. pour n = {}, score = {:.1f}\".format(inc, ind_min,score[ind_min]))\n    \n    if not found[ind_min]:            \n        for i in range(1,4):\n            ax[ind_min].plot(fit.X, result[i], color[i], linewidth = 4 if i == ind_min else 1)\n        ax[ind_min].scatter(fit.X, fit.Y)                \n        ax[ind_min].text(0.05, 0.95, 'MSE = {:.1f}, {:.1f}, {:.1f}'.format(*score[1:4]),\n                         style='italic', transform=ax[ind_min].transAxes, fontsize = 8,\n                         bbox={'facecolor':'tomato', 'alpha':0.8, 'pad':5})                \n        found[ind_min] = True\n\n    shift = 0.5\n    plt.text(-1+shift, 1.05, \"polynomial order:\", color = 'k',\n                transform=ax[2].transAxes, fontsize = 16, family='fantasy')\n    plt.text(0+shift, 1.05, \"n = 1\", color = 'r', \n                transform=ax[2].transAxes, fontsize = 16, family='fantasy')\n    plt.text(0.4+shift, 1.05, \"n = 2\", color = 'g', \n                transform=ax[2].transAxes, fontsize = 16, family='fantasy')\n    plt.text(0.8+shift, 1.05, \"n = 3\", color = 'b',\n                transform=ax[2].transAxes, fontsize = 16, family='fantasy')\n   \n    if inc == 40 or all(found[1:4]): break","6cbdaae8":"class fit_polynome_cv:\n\n    def __init__(self, data):\n        self.data = data[['mean', 'heure_depart_min']].dropna(how='any', axis = 0)\n        self.X = np.array(self.data['heure_depart_min'])\n        self.Y = np.array(self.data['mean'])\n        self.X = self.X.reshape(len(self.X),1)\n        self.Y = self.Y.reshape(len(self.Y),1)\n\n    def train(self, pol_order, nb_folds):\n        self.poly = PolynomialFeatures(degree = pol_order)\n        self.regr = linear_model.LinearRegression()\n        self.X_ = self.poly.fit_transform(self.X)\n        self.result = cross_val_predict(self.regr, self.X_, self.Y, cv = nb_folds)\n    \n    def calc_score(self, pol_order, nb_folds):\n        self.poly = PolynomialFeatures(degree = pol_order)\n        self.regr = linear_model.LinearRegression()\n        self.X_ = self.poly.fit_transform(self.X)\n        self.score = np.mean(cross_val_score(self.regr, self.X_, self.Y,\n                                             cv = nb_folds, scoring = 'mean_squared_error'))","7ff806fa":"#id_airport = 1129804 \nnb_folds = 10\nprint('Max possible number of folds: {} \\n'.format(test2.shape[0]-1))\nfit2 = fit_polynome_cv(test2)\nfor i in range(1, 8):\n    fit2.calc_score(i, nb_folds)\n    print('n={} -> MSE = {}'.format(i, round(abs(fit2.score),3)))","5c584ad6":"fit = fit_polynome(test2)\nfit.split('all')\nfit.train(pol_order = 2)\nfit.predict(fit.X)","c6bb70b0":"fit2.train(pol_order = 2, nb_folds = nb_folds)","119afe0d":"fig1 = Figure_style(8, 4, 1, 1) ; fig1.pos_update(0, 0)\nfig1.cust_scatter(fit2.X, fit2.Y, markeredge = True, label = 'initial data points')\nfig1.cust_plot(fit.X,fit2.result,color=u'#1f77b4',linestyle='--',linewidth=2,label='CV output')\nfig1.cust_plot(fit.X,fit.result,color=u'#ff7f0e',linewidth = 3,label='final fit')\nfig1.style(); fig1.draw_legend('upper left')\nfig1.set_ylabel('Delay (minutes)') ; fig1.set_xlabel('Departure time')\n#____________________________________\n# convert and set the x ticks labels\nfct_convert = lambda x: (int(x\/3600) , int(divmod(x,3600)[1]\/60))\nfig1.axs[fig1.ix, fig1.iy].set_xticklabels(['{:2.0f}h{:2.0f}m'.format(*fct_convert(x))\n                                            for x in fig1.axs[fig1.ix, fig1.iy].get_xticks()]);","7c63637f":"score = metrics.mean_squared_error(fit.result, fit2.Y)\nscore","0b14c84e":"test_data = get_flight_delays(df_test, carrier, id_airport, True)\ntest_data = test_data[['mean', 'heure_depart_min']].dropna(how='any', axis = 0)\nX_test = np.array(test_data['heure_depart_min'])\nY_test = np.array(test_data['mean'])\nX_test = X_test.reshape(len(X_test),1)\nY_test = Y_test.reshape(len(Y_test),1)\nfit.predict(X_test)","3a50434d":"score = metrics.mean_squared_error(fit.result, Y_test)\nscore","50301af6":"'Ecart = {:.2f} min'.format(np.sqrt(score))","9e26761e":"def get_merged_delays(df, carrier):\n    liste_airports = df[df['AIRLINE'] == carrier]['ORIGIN_AIRPORT'].unique()\n    i = 0\n    liste_columns = ['AIRPORT_ID', 'heure_depart_min', 'mean']\n    for id_airport in liste_airports:\n        test2 = get_flight_delays(df, carrier, id_airport, True)\n        test2.loc[:, 'AIRPORT_ID'] = id_airport\n        test2 = test2[liste_columns]\n        test2.dropna(how = 'any', inplace = True)\n        if i == 0:\n            merged_df = test2.copy()\n        else:\n            merged_df = pd.concat([merged_df, test2], ignore_index = True)\n        i += 1    \n    return merged_df","3583235c":"carrier = 'AA'\nmerged_df = get_merged_delays(df, carrier)\nmerged_df.shape","90434a17":"label_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(merged_df['AIRPORT_ID'])\n#__________________________________________________________\n# correspondance between the codes and tags of the airports\nzipped = zip(integer_encoded, merged_df['AIRPORT_ID'])\nlabel_airports = list(set(list(zipped)))\nlabel_airports.sort(key = lambda x:x[0])\nlabel_airports[:5]","0cfc29f2":"onehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\nb = np.array(merged_df['heure_depart_min'])\nb = b.reshape(len(b),1)\nX = np.hstack((onehot_encoded, b))\nY = np.array(merged_df['mean'])\nY = Y.reshape(len(Y), 1)\nprint(X.shape, Y.shape)","c9a8555e":"lm = linear_model.LinearRegression()\nmodel = lm.fit(X,Y)\npredictions = lm.predict(X)\nprint(\"MSE =\", metrics.mean_squared_error(predictions, Y))","f3ef0627":"icount = 0\nfor i, val in enumerate(Y):\n    if abs(val-predictions[i]) > 15: icount += 1\n'{:.2f}%'.format(icount \/ len(predictions) * 100)","02e2201a":"tips = pd.DataFrame()\ntips[\"prediction\"] = pd.Series([float(s) for s in predictions]) \ntips[\"original_data\"] = pd.Series([float(s) for s in Y]) \nsns.jointplot(x=\"original_data\", y=\"prediction\", data=tips, size = 6, ratio = 7,\n              joint_kws={'line_kws':{'color':'limegreen'}}, kind='reg')\nplt.xlabel('Mean delays (min)', fontsize = 15)\nplt.ylabel('Predictions (min)', fontsize = 15)\nplt.plot(list(range(-10,25)), list(range(-10,25)), linestyle = ':', color = 'r')\nsns.plt.show()","7379793a":"poly = PolynomialFeatures(degree = 2)\nregr = linear_model.LinearRegression()\nX_ = poly.fit_transform(X)\nregr.fit(X_, Y)","4299d531":"result = regr.predict(X_)\nprint(\"MSE =\", metrics.mean_squared_error(result, Y))","e39dbb73":"icount = 0\nfor i, val in enumerate(Y):\n    if abs(val-result[i]) > 15: icount += 1\n'{:.2f}%'.format(icount \/ len(result) * 100)","376ac7d7":"tips = pd.DataFrame()\ntips[\"prediction\"] = pd.Series([float(s) for s in result]) \ntips[\"original_data\"] = pd.Series([float(s) for s in Y]) \nsns.jointplot(x=\"original_data\", y=\"prediction\", data=tips, size = 6, ratio = 7,\n              joint_kws={'line_kws':{'color':'limegreen'}}, kind='reg')\nplt.xlabel('Mean delays (min)', fontsize = 15)\nplt.ylabel('Predictions (min)', fontsize = 15)\nplt.plot(list(range(-10,25)), list(range(-10,25)), linestyle = ':', color = 'r')\nsns.plt.show()","e526477b":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)","6133c718":"X_train.shape","5933f290":"poly = PolynomialFeatures(degree = 2)\nregr = linear_model.LinearRegression()\nX_ = poly.fit_transform(X_train)\nregr.fit(X_, Y_train)\nresult = regr.predict(X_)\nscore = metrics.mean_squared_error(result, Y_train)\nprint(\"Mean squared error = \", score)","75f630bb":"X_ = poly.fit_transform(X_test)\nresult = regr.predict(X_)\nscore = metrics.mean_squared_error(result, Y_test)\nprint(\"Mean squared error = \", score)","e622c0b7":"somme = 0\nfor valeurs in zip(result, Y_test):\n    ajout = (float(valeurs[0]) - float(valeurs[1]))**2\n    somme += ajout\n    if ajout > 10**4:\n        print(\"{:<.1f} {:<.1f} {:<.1f}\".format(ajout, float(valeurs[0]), float(valeurs[1])))","4beccae1":"from sklearn.linear_model import Ridge\nridgereg = Ridge(alpha=0.3,normalize=True)\npoly = PolynomialFeatures(degree = 2)\nX_ = poly.fit_transform(X_train)\nridgereg.fit(X_, Y_train)","2513f22a":"X_ = poly.fit_transform(X_test)\nresult = ridgereg.predict(X_)\nscore = metrics.mean_squared_error(result, Y_test)\nprint(\"Mean squared error = \", score)","81c2dc1e":"score_min = 10000\nfor pol_order in range(1, 3):\n    for alpha in range(0, 20, 2):\n        ridgereg = Ridge(alpha = alpha\/10, normalize=True)\n        poly = PolynomialFeatures(degree = pol_order)\n        regr = linear_model.LinearRegression()\n        X_ = poly.fit_transform(X_train)\n        ridgereg.fit(X_, Y_train)        \n        X_ = poly.fit_transform(X_test)\n        result = ridgereg.predict(X_)\n        score = metrics.mean_squared_error(result, Y_test)        \n        if score < score_min:\n            score_min = score\n            parameters = [alpha\/10, pol_order]\n        print(\"n={}\u00a0alpha={} , MSE = {:<0.5}\".format(pol_order, alpha, score))","3d3a6a1f":"ridgereg = Ridge(alpha = parameters[0], normalize=True)\npoly = PolynomialFeatures(degree = parameters[1])\nX_ = poly.fit_transform(X)\nridgereg.fit(X_, Y)\nresult = ridgereg.predict(X_)\nscore = metrics.mean_squared_error(result, Y)        \nprint(score)","93da0878":"carrier = 'AA'\nmerged_df_test = get_merged_delays(df_test, carrier)","9b20e66f":"label_conversion = dict()\nfor s in label_airports:\n    label_conversion[s[1]] = s[0]\n\nmerged_df_test['AIRPORT_ID'].replace(label_conversion, inplace = True)\n\nfor index, label in label_airports:\n    temp = merged_df_test['AIRPORT_ID'] == index\n    temp = temp.apply(lambda x:1.0 if x else 0.0)\n    if index == 0:\n        matrix = np.array(temp)\n    else:\n        matrix = np.vstack((matrix, temp))\nmatrix = matrix.T\n\nb = np.array(merged_df_test['heure_depart_min'])\nb = b.reshape(len(b),1)\nX_test = np.hstack((matrix, b))\nY_test = np.array(merged_df_test['mean'])\nY_test = Y_test.reshape(len(Y_test), 1)","6a93315f":"X_ = poly.fit_transform(X_test)\nresult = ridgereg.predict(X_)\nscore = metrics.mean_squared_error(result, Y_test)\n'MSE = {:.2f}'.format(score)","5539708f":"'Ecart = {:.2f} min'.format(np.sqrt(score))","70303e56":"def create_df(df, carrier):\n    df2 = df[df['AIRLINE'] == carrier][['SCHEDULED_DEPARTURE','SCHEDULED_ARRIVAL',\n                                    'ORIGIN_AIRPORT','DESTINATION_AIRPORT','DEPARTURE_DELAY']]\n    df2.dropna(how = 'any', inplace = True)\n    df2['weekday'] = df2['SCHEDULED_DEPARTURE'].apply(lambda x:x.weekday())\n    #____________________\n    # delete delays > 1h\n    df2['DEPARTURE_DELAY'] = df2['DEPARTURE_DELAY'].apply(lambda x:x if x < 60 else np.nan)\n    df2.dropna(how = 'any', inplace = True)\n    #_________________\n    # formating times\n    fct = lambda x:x.hour*3600+x.minute*60+x.second\n    df2['heure_depart'] = df2['SCHEDULED_DEPARTURE'].apply(lambda x:x.time())\n    df2['heure_depart'] = df2['heure_depart'].apply(fct)\n    df2['heure_arrivee'] = df2['SCHEDULED_ARRIVAL'].apply(fct)\n    df3 = df2.groupby(['heure_depart', 'heure_arrivee', 'ORIGIN_AIRPORT'],\n                      as_index = False).mean()\n    return df3","424b0420":"df3 = create_df(df, carrier)    \ndf3[:5]","3743e7d0":"label_encoder = LabelEncoder()\ninteger_encoded = label_encoder.fit_transform(df3['ORIGIN_AIRPORT'])\n#_________________________________________________________\nzipped = zip(integer_encoded, df3['ORIGIN_AIRPORT'])\nlabel_airports = list(set(list(zipped)))\nlabel_airports.sort(key = lambda x:x[0])\n#_________________________________________________\nonehot_encoder = OneHotEncoder(sparse=False)\ninteger_encoded = integer_encoded.reshape(len(integer_encoded), 1)\nonehot_encoded = onehot_encoder.fit_transform(integer_encoded)\n#_________________________________________________\nb = np.array(df3[['heure_depart', 'heure_arrivee']])\nX = np.hstack((onehot_encoded, b))\nY = np.array(df3['DEPARTURE_DELAY'])\nY = Y.reshape(len(Y), 1)","9110dc1a":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3)","c8c837da":"score_min = 10000\nfor pol_order in range(1, 3):\n    for alpha in range(0, 20, 2):\n        ridgereg = Ridge(alpha = alpha\/10, normalize=True)\n        poly = PolynomialFeatures(degree = pol_order)\n        regr = linear_model.LinearRegression()\n        X_ = poly.fit_transform(X_train)\n        ridgereg.fit(X_, Y_train)\n        \n        X_ = poly.fit_transform(X_test)\n        result = ridgereg.predict(X_)\n        score = metrics.mean_squared_error(result, Y_test)\n        \n        if score < score_min:\n            score_min = score\n            parameters = [alpha, pol_order]\n\n        print(\"n={}\u00a0alpha={} , MSE = {:<0.5}\".format(pol_order, alpha\/10, score))","f36e1dc8":"ridgereg = Ridge(alpha = parameters[0], normalize=True)\npoly = PolynomialFeatures(degree = parameters[1])\nX_ = poly.fit_transform(X)\nridgereg.fit(X_, Y)\nresult = ridgereg.predict(X_)\nscore = metrics.mean_squared_error(result, Y)        \nprint(score)","cee19c4a":"df3 = create_df(df_test, carrier)    \ndf3[:5]","4de6d756":"label_conversion = dict()\nfor s in label_airports:\n    label_conversion[s[1]] = s[0]\n\ndf3['ORIGIN_AIRPORT'].replace(label_conversion, inplace = True)\n\nfor index, label in label_airports:\n    temp = df3['ORIGIN_AIRPORT'] == index\n    temp = temp.apply(lambda x:1.0 if x else 0.0)\n    if index == 0:\n        matrix = np.array(temp)\n    else:\n        matrix = np.vstack((matrix, temp))\nmatrix = matrix.T\n\nb = np.array(df3[['heure_depart', 'heure_arrivee']])\nX_test = np.hstack((matrix, b))\nY_test = np.array(df3['DEPARTURE_DELAY'])\nY_test = Y_test.reshape(len(Y_test), 1)","fc023b0a":"X_ = poly.fit_transform(X_test)\nresult = ridgereg.predict(X_)\nscore = metrics.mean_squared_error(result, Y_test)\nprint('MSE = {}'.format(round(score, 2)))","8b86e43a":"'Ecart = {:.2f} min'.format(np.sqrt(score))","03da86fe":"icount = 0\nfor i, val in enumerate(Y_test):\n    if abs(val-predictions[i]) > 15: icount += 1\nprint(\"ecarts > 15 minutes: {}%\".format(round((icount \/ len(predictions))*100,3)))","365bd413":"tips = pd.DataFrame()\ntips[\"prediction\"] = pd.Series([float(s) for s in predictions]) \ntips[\"original_data\"] = pd.Series([float(s) for s in Y_test]) \nsns.jointplot(x=\"original_data\", y=\"prediction\", data=tips, size = 6, ratio = 7,\n              joint_kws={'line_kws':{'color':'limegreen'}}, kind='reg')\nplt.xlabel('Mean delays (min)', fontsize = 15)\nplt.ylabel('Predictions (min)', fontsize = 15)\nplt.plot(list(range(-10,25)), list(range(-10,25)), linestyle = ':', color = 'r')\nsns.plt.show()","dff6cb42":"and then, I read the file that contains the details of all the flights that occured in 2015. I output some informations concerning the types of the variables in the dataframe and the quantity of null values for each variable:","9cacd41c":"** If you see any kind of improvement, or mistakes, thanks in advance for telling me !!** <br>\n**_If you liked this notebook, thanks for upvoting_ :)**","d8901761":"The current MSE score is calculated on all the airports served by _American Airlines_, whereas previously it was calculated on the data of a single airport. The current model is therefore more general. Moreover, considering the previous model, it is likely that predictions will be poor for airports with low statistics.\n____\n## 6.3 Model n\u00ba3: Accounting for destinations","f9a0e705":"___\n#### 5.2.3 Setting the free parameters\n\nAbove, the two models were fit and tested on the training set. In practice, as mentioned above, there is a risk of overfitting by proceeding that way and the free parameters of the model will be biased. Hence,  the model will not allow a good generalization. In what follows, I will therefore split the datas in order to train and then test the model. The purpose will be to determine the polynomial degree which allows the best generalization of the predictions:","2d761d10":"At this stage, the model was driven is tested on the training set which include the data of the first 3 weeks of January. We now look at the comparison of predictions and observations for the fourth week of January:","290108cf":"We see that some predictions show very large errors. In practice, this can be explained by the fact that during the separation in train and test sets, **data with no equivalent in the training set was put in the test data**. Thus, when calculating the prediction, the model has to **perform an extrapolation**. If the coefficients of the fit are large (which is often the case when overfitting), extrapolated values will show important values, as in the present case. In order to have a control over this phenomenon, we can use a **regularization method** which will put a penalty to the models whose coefficients are the most important:","899e97af":"As before, I fit the model on the training set:","ac4b18bb":"___\n### 4.2 How the origin airport impact delays\n\nIn this section, I will have a look at the variations of the delays with respect to the origin airport and for every airline. The first step thus consists in determining the mean delays per airport:","b0b4e242":"## Conclusion\n\nThese notebook was two-fold. The first part dealt with an exploration of the dataset, with the aim of understanding some properties of the delays registered by flights. This exploration gave me the occasion of using various vizualization tools offered by python. The second part of the notebook consisted in the elaboration of a model aimed at predicting flight delays. For that purpose, I used polynomial regressions and showed the importance of regularisation techniques. In fact, I only used ridge regression but it is important to keep in mind that other regularisations techniques could be more appropriate ( e.g Lasso or Elastic net). \n\n","931ec4c6":"Henceforth, regroupings are made on departure and arrival times, and the (specific) airports of origin and destination are implicitly taken into account. As before, I carry out the encoding of the airports:","32e1b374":"The * fit_polynome * class allows you to perform all operations related to a fit and to save the results. When calling the  **split()** method, the variable '*method*' defines how the initial data is separated:\n- *method = 'all' *: all input data is used to train and then test the model\n- *method = 'split' *: we use the * train_test_split() * method of sklearn to define test & training sets\n \nThen, the other methods of the class have the following functions:\n- ** train (n) **: drives the data on the training set and makes a polynomial of order n\n- ** predict (X) **: calculates the Y points associated with the X input and for the previously driven model\n- ** calc_score () **: calculates the model score in relation to the test set data\n\nIn order to illustrate the bias introduced by the selection of the test set, I proceed in the following way: I carry out several \"train \/ test\" separation of a data set and for each case, I fit polynomials of orders ** n = 1, 2 and 3 **, by calculating their respective scores. Then, I show that according to the choice of separation, the best score can be obtained with any of the values \u200b\u200bof ** n **. In practice, it is enough to carry out a dozen models to obtain this result. Moreover, this bias is introduced by the choice of the separation \"train \/ test\" and results from the small size of the dataset to be modeled. In fact, in the following, I take as an example the case of the airline * American Airlines * (the second biggest airline) and the airport of id 1129804, which is the airport with the most registered flights for that company. This is one of the least favorable scenarios for the emergence of this kind of bias, which, nevertheless, is present:","ad2a7477":"___\n#### 5.1.2 Polynomial degree: splitting the dataset\n\n\nIn practice, rather than performing a simple linear regression, we can improve the model doing a fit with a polynomial of order $N$. Doing so, it is necessary to define the degree $N$ which is optimal to represent the data. When increasing the polynomial order, it is important ** to prevent over-fitting** and we do this by splitting the dataset in **test and training sets**. A problem that may arise with this procedure is that the model ends by *indirectly* learning the contents of the test set and is thus biased. To avoid this, the data can be re-separated into 3 sets: *train*, *test* and *validation*. An alternative to this technique, which is often more robust, is the so-called cross-validation method. This method consists of performing a first separation of the data in *training* and *test* sets. As always, learning is done on the training set, but to avoid over-learning, it is split into several pieces that are used alternately for training and testing.\n\nNote that if the data set is small, the separation in test & training sets can introduce a bias in the estimation of the parameters. In practice, the *cross-validation* method avoids such bias. In fact, in the current model, we will encounter this type of problem and in what follows, I will highlight this. For example, we can consider an extreme case where, after separation, the training set would contain only hours $<$20h and the test set would have hours$>$ 20h. The model would then be unable to reproduce precisely this data, of which it would not have seen equivalent during the training. The cross-validation method avoids this bias because all the data are used successively to drive the model.\n\n** a) Bias introduced by the separation of the data set **\n\nIn order to test the impact of data separation on model determination, I first define the class * fit_polynome *:","a335f024":"#### 6.3.2 Test of the model: late January delays\n\nNow I test the quality of the predictions on the data of the last week of January. I first extract these data:","f42dc27a":"# FIA03-Groupe01\n# Ikbel benabdessamad &&  Fares  bahri && Chaouch safa &&  Ayedi Hajer\n\nFrom a **_technical point of view_**, the main aspects of python covered throughout the notebook are:\n- **visualization**: matplolib, seaborn, basemap\n- **data manipulation**: pandas, numpy\n- **modeling**: sklearn, scipy\n- **class definition**: regression, figures\n\nThis notebook is composed of three parts:      \n* Cleaning (section 1)\n* Exploration (section 2-5) \n* Modeling (section 6).\n\n** _Preamble_:** _overview of the dataset_ <br>\n\n** 1. Cleaning** \n- 1.1 Dates and times\n- 1.2 Filling factor\n\n**2. Comparing airlines**\n- 2.1 Basic statistical description of airlines\n- 2.2 Delays distribution: establishing the ranking of airlines \n\n** 3. Delays: take-off or landing ?** <br>\n** 4.  Relation between the origin airport and delays** <br>\n- 4.1 Geographical area covered by airlines  <br>\n- 4.2 How the origin airport impact delays <br>\n- 4.3 Flights with usual delays ? <br>\n\n** 5. Temporal variability of delays** <br>\n** 6. Predicting flight delays** <br>\n- 6.1 Model n\u00ba1: one airline, one airport \n  * 6.1.1 Pitfalls\n  * 6.1.2 Polynomial degree: splitting the dataset\n  * 6.1.3 Model test: prediction of end-January delays\n- 6.2 Model n\u00ba2: one airline, all airports\n  * 6.2.1 Linear regression  \n  * 6.2.2 Polynomial regression\n  * 6.2.3 Setting the free parameters\n  * 6.2.4 Model test: prediction of end-January delays\n- 6.3 Model n\u00ba3: Accounting for destinations\n   * 6.3.1 Choice of the free parameters\n   * 6.3.2 Model test: prediction of end-January delays \n   \n**Conclusion**\n","1b2ec05d":"Above, I have assigned a label to each airport. The correspondence between the label and the original identifier has been saved in the *label_airport* list. Now I proceed with the \"One Hot Encoding\" by creating a matrix where instead of the **ORIGIN_AIRPORT** variable that contained $M$ labels, we build a matrix with $M$ columns, filled of 0 and 1 depending on the correspondance with particular airports:","6cf1ec07":"This grid search allows to find the best set of $\\alpha$ and $n$ parameters. Let us note, however, that for this model, the estimates obtained with a linear regression or a polynomial of order 2 are quite close. Now I use these parameters to test this template over the test set:","e76eef27":"Now, if we calculate the score associated to the predictions made with a regularization technique, we have:","4c3835d5":"which corresponds to an average delay of:","fccdce56":"___\n#### 5.2.1 Linear regression\n\nThe matrices X and Y thus created can be used to perform a linear regression:","a5b89615":"This figure gives the average delays for *American Airlines*, according to the city of origin and the destination (note that on the abscissa axis, only the origin is indicated for the sake of clarity). The error bars associated with the different paths correspond to the standard deviations.\nIn this example, it can be seen that for a given airport of origin, delays will fluctuate depending on the destination. We see, for example, that here the greatest variations are obtained for New York or Miami where the initial average delays vary between 0 and $\\sim$20 minutes.\n\n___\n## 4. Temporal variability of delays\n\nIn this section, I look at the way delays vary with time. Considering the case of a specific airline and airport, delays can be easily represented by day and time (_aside_: before doing this, I define a class that I will use extensively in what follows to produce graphs):","0d41e256":"Looking at this list, we can see that the less visited aiports only have a few flights in a month.\nThus, in the least favorable case, it is impossible to perform a regression.\n\n**b) Extreme delays**\n\nAnother pitfall to avoid is that of \"accidental\" delays: a particular attention should be paid to extreme delays. Indeed, during the exploration, it was seen that occasionally, delays of several hours (even tens of hours) could be recorded. This type of delay is however marginal (a few %) and the cause of these delays is probably linked to unpredictable events (weather, breakdown, accident, ...). On the other hand, taking into account a delay of this type will likely introduce a bias in the analysis. Moreover, the weight taken by large values will be significant if we have a small statistics.\n\nIn order to illustrate this, I first define a function that calculates the mean flights delay per airline and per airport: ","aad3e775":"___\n### 5.1 Model n\u00ba1: one airline, one airport\n\nI first decide to model the delays by considering separately the different airlines and by splitting the data according to the different home airports. This first model can be seen as a *\"toy-model\"*  that enables to identify problems that may arise at the  production stage. When treating the whole dataset,  the number of fits will be large. Hence we have to be sure that the automation of the whole process is robust enough to insure the quality of the fits.\n\n\n#### 5.1.1 Pitfalls <br>\n\n\n**a) Unsufficient statistics**\n\nFirst of all, I consider the *American Airlines* flights and make a census of the number of flights that left each airport:","4c5c1bc0":"To have a global overview of the geographical area covered in this dataset, we can plot the airports location and indicate the number of flights recorded during year 2015 in each of them:","3a04953f":"Since the number of airports is quite large, a graph showing all the information at once would be a bit messy, since it would represent around 4400 values (i.e. 312 airports $\\times$ 14 airlines). Hence, I just represent a subset of the data:","1a0d3581":"Each entry of the `flights.csv` file corresponds to a flight and we see that more than 5'800'000 flights have been recorded in 2015. These flights are described according to 31 variables. A description of these variables can be found [here](https:\/\/www.transtats.bts.gov\/DL_SelectFields.asp?Table_ID=236&DB_Short_Name=On-Time) and I briefly recall the meaning of the variables that will be used in this notebook:\n\n- **YEAR, MONTH, DAY, DAY_OF_WEEK**: dates of the flight <br\/>\n- **AIRLINE**: An identification number assigned by US DOT to identify a unique airline <br\/>\n- **ORIGIN_AIRPORT** and **DESTINATION_AIRPORT**: code attributed by IATA to identify the airports <br\/>\n- **SCHEDULED_DEPARTURE** and **SCHEDULED_ARRIVAL** : scheduled times of take-off and landing <br\/> \n- **DEPARTURE_TIME** and **ARRIVAL_TIME**: real times at which take-off and landing took place <br\/> \n- **DEPARTURE_DELAY** and **ARRIVAL_DELAY**: difference (in minutes) between planned and real times <br\/> \n- **DISTANCE**: distance (in miles)  <br\/>\n\nAn additional file of this dataset, the `airports.csv` file, gives a more exhaustive description of the airports:","1e6f82e4":"I can then create the predictions","367b5c32":"This figure shows the existence of cycles, both in the frequency of the delays but also in their magnitude. In fact, intuitively, it seems quite logical to observe such cycles since they will be a consequence of the day-night alternation and the fact that the airport activity will be greatly reduced (if not inexistent) during the night. This suggests that a **important  variable** in the modeling of delays will be **take-off time**. To check this hypothesis, I look at the behavior of the mean delay as a function of departure time, aggregating the data of the current month:","4e6e3de6":"Now, by testing on the test set we get:","c6b63d36":"We can see that a polynomial fit improves slightly the MSE score. In practice, the percentage of values where the difference between predictions and real delays is greater than 15 minutes is:","b1f89b6f":"Here, I calculated the MSE score of the fit. In practice, we can have a feeling of the quality of the fit by considering the number of predictions where the differences with real values is greater than 15 minutes:","1e6a1ad6":"In the *merged_df* dataframe, airports are referenced by an identifier given in the ** ORIGIN_AIRPORT** variable.\nThe corresponding labels can't be used directly in a fit and I thus use the *one-hot-encoding* method:","104bcf1d":"___\n#### 6.3.1 Choice of model parameters\n\nAs before, I will perform a regression with regularization and I will have to define the value to attribute to the parameter $\\alpha$. I therefore separate the data to train and then test the model to select the best value for $\\alpha$:","ab461d52":"I then calculate the average delay on the various paths A $\\to$ B, as well as the standard deviation and once done, I create a graphical representation (for a sample of the flights):\n","84dce84e":"___\n### 4.3 Flights with usual delays ?\n\nIn the previous section, it has been seen that there is variability in delays when considering the different airlines and the different airports of origin. I'm now going to add a level of granularity by focusing not just on the original airports but on flights: origin $\\to$ destination. The objective here is to see if some flights are systematically delayed or if, on the contrary, there are flights that would always be on time.\n\nIn the following, I consider the case of a single airline. I list all the flights A $\\to$ B carried out by this company and for each of them, I create the list of delays that have been recorded:","2ff98aca":"And we can see that we obtain a reasonnable score. Hence, with the current procedure, to determine the best model, we have two free parameters to adjust: the polynomial order and the $\\alpha$ coefficient of the * 'Ridge Regression' *:","ad67ccf3":"___\n## 4. Relation between the origin airport and delays\n\nI will now try to define if there is a correlation between the delays registered and the airport of origin. I recall that in the dataset, the number of airports considered is: ","5f0e8ab5":"Here, we can see that the average delay tends to increase with the departure time of day: flights leave on time in the morning  and the delay grows almost monotonously up to 30 minutes at the end of the day. In fact, this behavior is quite general and looking at other aiports or companies, we would find similar trends.\n\n___\n## 6. Predicting flight delays \n\nThe previsous sections dealt with an exploration of the dataset. Here, I start with the modeling of flight delays.\nIn this section, my goal is to create a model that uses a window of 3 weeks to predict the delays of the following week.\nHence, I decide to work on the data of January with the aim of predicting the delays of the epoch $23^{th}-31^{th}$ of Januaray","5d11c310":"#### 5.1.3 Model test: prediction of end-January delays","18514d19":"Thus, in the following figure, the juxtaposition of the K = 50 polynomial fits corresponding to the cross validation calculation leads to the red curve. The polynomial fit corresponding to the final model corresponds to the blue curve.","c303586f":"Here, we see that the **fit is particularly bad with a MSE > 500** (the exact value depends on the run and on the splitting of the dataset), which means that the fit performs poorly when generalyzing to other data. Now let's examine in detail the reasons why we have such a bad score. Below, I examing all the terms of the MSE calculation and identify the largest terms:","e90c94a5":"First of all, in this figure, the points corresponding to the individual flights are represented by the points in gray.\nThe mean of these points gives the mean delays and the mean of the set of initial points corresponds to the blue squares. By removing extreme delays (> 1h), one obtains the average delays represented by the green crosses.\nThus, in the first case, the fit (solid blue curve) leads to a prediction which corresponds to an average delay of $\\sim$ 10 minutes larger than the predicton obtained in the second case (green curve), and this, at any hour of the day.\n\nIn conclusion, we see in this example that the way in which we manage the extreme delays will have an important impact on the modeling. Note, however, that the current example corresponds to a *chosen case* where the impact of extreme delays is magnified by the limited number of flights. Presumably, the impact of such delays will be less pronounced in the majority of cases.","74e1021f":"then I convert them into a format suitable to perform the fit. At this stage, I manually do one-hot-encoding by re-using the labeling that had been established on the training data:","c6652574":"___\n### 5.2 Model n\u00ba2: One airline, all airports\n\nIn the previous section, the model only considered one airport. This procedure is potentially inefficient because it is likely that some of the observations can be extrapolated from an ariport to another. Thus, it may be advantageous to make a single fit, which would take all the airports into account. In particular, this will allow to predict delays on airports for which the number of data is low with a better accuracy.","2187a574":"This class has two methods:\n- ** train (n, nb_folds) **: defined 'nb_folds' training sets from the initial dataset and drives a 'n' order polynomial on each of these sets. This method returns as a result the Y predictions obtained for the different test sets.\n- ** calc_score (n, nb_folds) **: performs the same procedure as a ** train ** method except that this method calculates the fit score and not the predicted values \u200b\u200bon the different test data.\n\nBy default, the *'K-fold'* method is used by sklearn * cross_val_predict () * and * cross_val_score () * methods. These methods are deterministic in the choice of the K folds, which implies that for a fixed K value, the results obtained using these methods will always be identical. As seen in the previous example, this was not the case when using the *train_test_split()* method. Thus, if we take the same dataset as in the previous example, the method of cross validation makes it possible to choose the best polynomial degree:","69dea3bf":"___\n#### 5.2.2 Polynomial regression\n\nI will now extend the previous fit by using a polynomial rather than a linear function:","3a359a17":"thus giving the difference in minutes between the predicted delay and the actual delay. In this case, the difference between the model and the observations is thus typically:","e5b6757e":"which visually gives:","47322588":"In practice, this model tends to underestimate the large delays, which can be seen in the following figure:","7c0d756e":"In the previous model, I grouped the flights per departure time. Thus, flights with different destinations were grouped as soon as they leave at the same time. Now I make a model that accounts for both departure and arrival times:","e68eb3fc":"To get an idea of the meaning of such a value for the MSE, we can assume a constant error on each point of the dataset. In which case, at each point $ i $, we have:\n\n\\begin{eqnarray}\ny_i - f(x_i) = cste = \\sqrt{MSE}\n\\end{eqnarray}","4dff1d61":"\n### 4.1 Geographical area covered by airlines \n\nHere, I have a quick look at the number of destination airports for each airline:","1aa7a9e8":"\nand then a function that performs a linear regression on these values:","51aa629a":"And as before, it can be seen that model tends to be worse in the case of large delays:","103bfa23":"and I call them to modify the dataframe variables:","b7de6a3b":"#### 6.2.4 Testing the model: delays of end-january\n\n\nAt this stage, model predictions are tested against end-January data. These data are first extracted:","3e1b1330":"I then consider two scenarios. In the first case, I take all the initial values and in the second case, I eliminate all delays greater than 1h before calculating the average delay. The comparison of the two cases is quite explicit:","811834a1":"This figure allows to draw some conclusions. First, by looking at the data associated with the different airlines, we find the behavior we previously observed: for example, if we consider the right panel, it will be seen that the column associated with  *American Eagle Airlines* mostly reports large delays, while the column associated with *Delta Airlines* is mainly associated  with delays of less than 5 minutes. If we now look at the airports of origin, we will see that some airports favor late departures: see e.g. Denver, Chicago or New York. Conversely, other airports will mainly know on time departures such as Portland or Oakland.\n\nFinally, we can deduce from these observations that there is a high variability in average delays, both between the different airports but also between the different airlines. This is important because it implies that in order to accurately model the delays, it will be necessary to adopt a model that is ** specific to the company and the home airport **. ","5b580521":"and the MSE score of the model is:","e620c001":"We can see that using this method gives us that the best model (ie the best generalized model) is obtained with a polynomial of order 2. At this stage of the procedure, the choice of the polynomial order a has been validated and we can now use all the data in order to perform the fit:","36502b3b":"\nAs before, assuming that the delay is independent of the point, this MSE score is equivalent to an average delay of:","802a84db":"In this figure, the panels from left to right correspond to 3 separations of the data in train and test sets, for which the best models are obtained respectively with polynomials of order 1, 2 and 3. On each of these panels the 3 fits polynomials have been represented and the best model corresponds to the thick curve.\n\n** b) Selection by cross-validation**\n\nOne of the advantages of the cross-validation method is that it avoids the bias that has just been put forward when choosing the polynomial degree. In order to use this method, I define a new class that I will use later to perform the fits:","97844411":"___\n## _Preamble_: overview of the dataset\n\nFirst, I load all the packages that will be needed during this project:"}}