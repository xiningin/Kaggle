{"cell_type":{"84ab53ea":"code","57f1ffa9":"code","4b13e520":"code","9a30613e":"code","124dea49":"code","a835e85e":"code","2bffe1aa":"code","f2052471":"code","c8299a1a":"code","c0d2838d":"code","88ff7cca":"code","5d9b4bfe":"code","2e9855a3":"code","8fc5fa53":"code","2df4c7c4":"code","67f6ba80":"code","18104618":"code","4ad86495":"code","94175b91":"code","f2c74eef":"code","2406c568":"code","ddfc654f":"code","e3547f0f":"code","e5a7849a":"code","ad0874df":"code","377eacd3":"code","d4052961":"code","c8b4b19a":"code","2777837f":"code","7697ed7c":"code","a4836ffe":"code","7c32ad33":"code","ae50d18a":"code","bdf322d6":"code","fac63351":"code","00cdcef6":"code","91d5f1af":"code","0cb08f70":"markdown","b6bef8cf":"markdown","3ca5802f":"markdown","09bc36d1":"markdown","7318401d":"markdown","cefbe91f":"markdown","693ddf90":"markdown","2be18296":"markdown","5d9a3619":"markdown","a7cc7321":"markdown","183e1425":"markdown","1efcb855":"markdown","0cfe947f":"markdown","6674ad8c":"markdown","e893b6c6":"markdown","843529bf":"markdown","f5346334":"markdown","91e9cdf8":"markdown"},"source":{"84ab53ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # plot the dataset\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","57f1ffa9":"df = pd.read_csv('\/kaggle\/input\/cern-electron-collision-data\/dielectron.csv')\ndf.head()","4b13e520":"df.info()","9a30613e":"df.describe().T","124dea49":"df.hist(bins = 50, figsize = (20,15))\nplt.show()","a835e85e":"df['E1'].hist(bins = 10, figsize = (5,5))\nplt.show()","2bffe1aa":"df.isnull().sum()","f2052471":"df2 = df.dropna(subset = ['M'])\ndf2.isnull().sum()","c8299a1a":"df2.info()","c0d2838d":"corr_matrix = df2.corr()\ncorr_matrix['M'].sort_values(ascending=False)","88ff7cca":"from pandas.plotting import scatter_matrix\nattributes = ['M','pt1','pt2','E1','E2']\nscatter_matrix(df2[attributes],figsize=(12,8))\nplt.show()","5d9b4bfe":"sns.set(rc={'figure.figsize':(15,15)})\ng = sns.heatmap(df2.corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","2e9855a3":"df2.drop(labels = [\"Run\",\"Event\"], axis = 1, inplace = True)\ndf2.head()","8fc5fa53":"train_set,test_set = train_test_split(df2,test_size = 0.2, random_state = 42)","2df4c7c4":"dataframe = train_set.drop('M',axis = 1)\ndataframe_labels = train_set['M'].copy()","67f6ba80":"scaler = StandardScaler()","18104618":"dataframe_scaled = scaler.fit_transform(dataframe)","4ad86495":"dataframe_scaled","94175b91":"lin_reg = LinearRegression()\nlin_reg.fit(dataframe_scaled, dataframe_labels)","f2c74eef":"dataframe_predictions = lin_reg.predict(dataframe_scaled)\nlin_mse = mean_squared_error(dataframe_labels, dataframe_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","2406c568":"\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(dataframe_scaled,dataframe_labels)","ddfc654f":"dataframe_predictions = tree_reg.predict(dataframe_scaled)\ntree_mse = mean_squared_error(dataframe_labels,dataframe_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","e3547f0f":"forest_reg = RandomForestRegressor()\nforest_reg.fit(dataframe_scaled,dataframe_labels)","e5a7849a":"dataframe_predictions = forest_reg.predict(dataframe_scaled)\nforest_mse = mean_squared_error(dataframe_labels,dataframe_predictions)\nforest_rmse = np.sqrt(forest_mse)\nforest_rmse","ad0874df":"def display_scores(scores):\n    print('Scores',scores)\n    print('Mean',scores.mean())\n    print('Standard Deviation',scores.std())","377eacd3":"lin_scores = cross_val_score(lin_reg,dataframe_scaled,dataframe_labels,\n                             scoring = 'neg_mean_squared_error',cv =10)\nlin_rmse_scores = np.sqrt(-lin_scores)\ndisplay_scores(lin_rmse_scores)","d4052961":"scores = cross_val_score(tree_reg,dataframe_scaled,dataframe_labels,\n                         scoring='neg_mean_squared_error',cv = 10)\ntree_rmse_scores = np.sqrt(-scores)","c8b4b19a":"display_scores(tree_rmse_scores)","2777837f":"forest_scores = cross_val_score(forest_reg,dataframe_scaled,dataframe_labels,\n                                scoring = 'neg_mean_squared_error',cv =10)","7697ed7c":"forest_rmse_scores = np.sqrt(-forest_scores)\ndisplay_scores(forest_rmse_scores)","a4836ffe":"param_grid = [\n    {'n_estimators':[3,10,30], 'max_features':[2,4,6,8]},\n    {'bootstrap':[False],'n_estimators':[3,10],'max_features':[2,3,4]},\n]\nforest_reg = RandomForestRegressor()\ngrid_search = GridSearchCV(forest_reg,param_grid, cv=5,scoring = 'neg_mean_squared_error')\ngrid_search.fit(dataframe_scaled,dataframe_labels)","7c32ad33":"grid_search.best_params_","ae50d18a":"grid_search.best_estimator_","bdf322d6":"cvres = grid_search.cv_results_\nfor mean_score,params in zip(cvres['mean_test_score'],cvres['params']):\n    print(np.sqrt(-mean_score),params)","fac63351":"feature_importances =  grid_search.best_estimator_.feature_importances_\nfeature_importances","00cdcef6":"final_model = grid_search.best_estimator_\nX_test = test_set.drop(\"M\", axis=1)\ny_test = test_set[\"M\"].copy()\nX_test_prepared = scaler.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","91d5f1af":"print(final_rmse)","0cb08f70":"### Loading the Dataset","b6bef8cf":"RMSE of Linear Regression is quite high","3ca5802f":"### Scaling\nWe can see the deviation among the min and max values of different columns, so what we thought of doing is Scaling and using StandardScaler \nStandardScaler first it subtracts the mean value (so standardized\nvalues always have a zero mean), and then it divides by the standard deviation so that\nthe resulting distribution has unit variance. In other words, it centralizes the data.","09bc36d1":"# **Models** ","7318401d":"### ***LINEAR REGRESSION***","cefbe91f":"### Using Cross Validation","693ddf90":"### ***DECISION TREE***","2be18296":"We can see that without any fine tuning Decision Tree is overfitting the data and Random Forest is near overfitting. Hence will do Cross Validation","5d9a3619":"Of the three we can see that Random Forest has lowest RMSE and is also quite stable, so will use GridSearchCV for hyperparameter tuning\n\n### GridSearchCV","a7cc7321":"First we tried rough implementation of different modeling techniques","183e1425":"Run and Event columns are there only to specify the run number and event number, so we find it safe just to drop those columns","1efcb855":"> Scikit-Learn\u2019s cross-validation features expect a utility function\n(greater is better) rather than a cost function (lower is better), so\nthe scoring function is actually the opposite of the MSE (i.e., a neg\u2010\native value), which is why the preceding code computes -scores\nbefore calculating the square root.","0cfe947f":"## Train Test Split","6674ad8c":"As there are only 85 missing values, that too the target values, we opted to drop those rows","e893b6c6":"Now we will do the split of Training Data and Testing Data with 80:20 ratio.","843529bf":"### ***Random Forest***","f5346334":"Looking at the minimum and maximum values of multiple column we are quite sure that this dataset needs to be scaled.","91e9cdf8":"Here we can see that 85 values from Target column is missing"}}