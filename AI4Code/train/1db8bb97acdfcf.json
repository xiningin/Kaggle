{"cell_type":{"4319977d":"code","70ed7f5a":"code","c25c64de":"code","0753e963":"code","dd5a1def":"code","c462ff51":"code","96d97a89":"code","8ea8191a":"markdown","765e3365":"markdown","e7e787fe":"markdown","fb62e9e4":"markdown","13c2a5b4":"markdown"},"source":{"4319977d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","70ed7f5a":"!pip install transformers==2.10.0\n!pip install simpletransformers","c25c64de":"from simpletransformers.classification import ClassificationModel\nimport pandas as pd\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n# Train and Evaluation data needs to be in a Pandas Dataframe containing at least two columns. If the Dataframe has a header, it should contain a 'text' and a 'labels' column. If no header is present, the Dataframe should contain at least two columns, with the first column is the text with type str, and the second column in the label with type int.\ntrain_data = [['Example sentence belonging to class 1', 1], ['Example sentence belonging to class 0', 0], ['Example eval senntence belonging to class 2', 2]]\ntrain_df = pd.DataFrame(train_data)\n\neval_data = [['Example eval sentence belonging to class 1', 1], ['Example eval sentence belonging to class 0', 0], ['Example eval senntence belonging to class 2', 2]]\neval_df = pd.DataFrame(eval_data)\n\n# Create a ClassificationModel\nmodel = ClassificationModel('bert', 'bert-base-cased', num_labels=3, args={'reprocess_input_data': True, 'overwrite_output_dir': True},use_cuda=False)\n# You can set class weights by using the optional weight argument\n\n# Train the model\nmodel.train_model(train_df)\n\n# Evaluate the model\nresult, model_outputs, wrong_predictions = model.eval_model(eval_df)\n\npredictions, raw_outputs = model.predict([\"Some arbitary sentence\"])\nprint(predictions)","0753e963":"from simpletransformers.ner import NERModel\nimport pandas as pd\nimport logging\n\n\nlogging.basicConfig(level=logging.INFO)\ntransformers_logger = logging.getLogger(\"transformers\")\ntransformers_logger.setLevel(logging.WARNING)\n\n# Creating train_df  and eval_df for demonstration\ntrain_data = [\n    [0, 'Simple', 'B-MISC'], [0, 'Transformers', 'I-MISC'], [0, 'started', 'O'], [1, 'with', 'O'], [0, 'text', 'O'], [0, 'classification', 'B-MISC'],\n    [1, 'Simple', 'B-MISC'], [1, 'Transformers', 'I-MISC'], [1, 'can', 'O'], [1, 'now', 'O'], [1, 'perform', 'O'], [1, 'NER', 'B-MISC']\n]\ntrain_df = pd.DataFrame(train_data, columns=['sentence_id', 'words', 'labels'])\n\neval_data = [\n    [0, 'Simple', 'B-MISC'], [0, 'Transformers', 'I-MISC'], [0, 'was', 'O'], [1, 'built', 'O'], [1, 'for', 'O'], [0, 'text', 'O'], [0, 'classification', 'B-MISC'],\n    [1, 'Simple', 'B-MISC'], [1, 'Transformers', 'I-MISC'], [1, 'then', 'O'], [1, 'expanded', 'O'], [1, 'to', 'O'], [1, 'perform', 'O'], [1, 'NER', 'B-MISC']\n]\neval_df = pd.DataFrame(eval_data, columns=['sentence_id', 'words', 'labels'])\n\n# Create a NERModel\nmodel = NERModel('bert', 'bert-base-cased', args={'overwrite_output_dir': True, 'reprocess_input_data': True},use_cuda=False)\n\n# Train the model\nmodel.train_model(train_df)\n\n# Evaluate the model\nresult, model_outputs, predictions = model.eval_model(eval_df)\n\n# Predictions on arbitary text strings\npredictions, raw_outputs = model.predict([\"Some arbitary sentence\"])\nprint(predictions)","dd5a1def":"!pip install sentence-transformers","c462ff51":"from sentence_transformers import SentenceTransformer\nmodel = SentenceTransformer('bert-base-nli-mean-tokens')","96d97a89":"sentences = ['This framework generates embeddings for each input sentence',\n    'Sentences are passed as a list of string.', \n    'The quick brown fox jumps over the lazy dog.']\nsentence_embeddings = model.encode(sentences)\nprint(sentence_embeddings[0].shape)","8ea8191a":"## Sentence transformers for getting context vectors using bert\nhttps:\/\/pypi.org\/project\/sentence-transformers\/","765e3365":"## simple classificationmodel\nCurrent versions that i was able to make work for training.\nas of now training works only on CPU.\n\nto get gpu training : takes >10mins\nimport os\nos.system('git clone https:\/\/github.com\/NVIDIA\/apex; cd apex; pip install -v --no-cache-dir' + \n              ' --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" .\/')\nos.system('rm -rf apex\/.git') # too many files, Kaggle fails\nfrom apex import amp","e7e787fe":"## Basic NER","fb62e9e4":"Change the model name to try different bert models.","13c2a5b4":"# Simple transformers- basic guide\n\nsimple transformers is a wrapper on top of huggingface transformers lib. Easy to use for starters\nhttps:\/\/github.com\/ThilinaRajapakse\/simpletransformers"}}