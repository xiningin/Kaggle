{"cell_type":{"bba8694f":"code","88d801d3":"code","8d183bac":"code","2be3a294":"code","df008f62":"code","9d5759db":"code","885026c2":"code","7a26e18f":"code","9bd229ad":"code","7e19184a":"code","6a23f6bd":"code","6ac76f5a":"code","3195eac9":"code","c5e73649":"code","efcbc6fb":"code","97d6acba":"code","051c9392":"code","231acb37":"code","d25cc529":"code","23b14f89":"code","d700615c":"code","ec4383a8":"markdown","2efd3a69":"markdown","45742aa6":"markdown","3bf40199":"markdown","41dfc5dc":"markdown","ac4dd0fd":"markdown","7d5d3475":"markdown","07974b36":"markdown","f425e721":"markdown","6041515e":"markdown"},"source":{"bba8694f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","88d801d3":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom pandas_profiling import ProfileReport\n\nfrom scipy import signal\nfrom scipy.signal import butter, deconvolve\nimport pywt","8d183bac":"df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ndf.head()","2be3a294":"dt = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\ndt.head()","df008f62":"dates = [d for d in df.columns if 'd_' in d]\nprint('dates in the training set: ', dates[:5], ' ... ', dates[-5:])","9d5759db":"ids = sorted(list(set(df['id'])))\n\nall_sales = df[dates].sum(axis=0).values\n\nx1_sales = (df['store_id'] == 'CA_2').values\nx1_sales = np.where(x1_sales == True)\nx1_sales = df.iloc[x1_sales[0]][dates].sum(axis=0).values\n\nx2_sales = df.loc[df['id'] == ids[2]].set_index('id')[dates].values[0]\n\nfig, ax = plt.subplots(3, figsize=(15,15))\nax[0].plot([x for x in range(len(all_sales))], all_sales, 'tab:green')\nax[0].set_title('All sales \/ day')\nax[0].set(xlabel='days', ylabel='sales')\n\nax[1].plot([x for x in range(len(x1_sales))], x1_sales, 'tab:red')\nax[1].set_title('Sales of a store \/ day')\nax[1].set(xlabel='days', ylabel='sales')\n\nax[2].plot([x for x in range(len(x2_sales))], x2_sales, 'tab:blue')\nax[2].set_title('Sales of an item \/ day')\nax[2].set(xlabel='days', ylabel='sales')\n\nfig.tight_layout()\nfig.show()","885026c2":"from scipy.fftpack import fft, ifft","7a26e18f":"def maxN(elements, n):\n    return sorted(elements, reverse=True)[:n]\ndef apply_fft(data, n):\n    fourier = fft(data)\n    power = [int((abs(x) ** (2 \/ fourier.shape[0])) * 1000) for x in fourier]\n    limit  = min(maxN(power, n))\n    for i in range(fourier.shape[0]):\n        if (power[i] < limit):\n            fourier[i] = 0\n    inv_fourier = ifft(fourier).real\n    return inv_fourier\n\nfft_sales_small = apply_fft(x2_sales, 5)\nfft_sales_medium = apply_fft(x2_sales, 10)\nfig, ax = plt.subplots(2, figsize=(15,10))\nax[0].plot([x for x in range(len(fft_sales_small))], fft_sales_small, 'coral')\nax[0].set_title('FFT with high noise reduction for the second graph above')\nax[0].set(xlabel='days', ylabel='sales')\n\nax[1].plot([x for x in range(len(fft_sales_medium))], fft_sales_medium, 'crimson')\nax[1].set_title('FFT with medium noise reduction for the second graph above')\nax[1].set(xlabel='days', ylabel='sales')\n\nfig.tight_layout()\nfig.show()","9bd229ad":"rolling_mean1 = pd.DataFrame({'y':x1_sales})\nrolling_mean1 = rolling_mean1.y.rolling(window=14).mean()\n\nrolling_mean2 = pd.DataFrame({'y':x2_sales})\nrolling_mean2 = rolling_mean2.y.rolling(window=20).mean()\n\nrolling_mean3 = pd.DataFrame({'y':all_sales})\nrolling_mean3 = rolling_mean3.y.rolling(window=20).mean()\n\nprint('The graphs below are the first 3 graphs with MA noise reduction')\n\nfig, ax = plt.subplots(3, figsize=(15,15))\nax[0].plot([x for x in range(len(rolling_mean3))], rolling_mean3, 'coral')\nax[0].set_title('Moving averages noise reduction')\nax[0].set(xlabel='days', ylabel='sales')\n\nax[1].plot([x for x in range(len(rolling_mean1))], rolling_mean1, 'crimson')\nax[1].set_title('Moving averages noise reduction')\nax[1].set(xlabel='days', ylabel='sales')\n\nax[2].plot([x for x in range(len(rolling_mean2))], rolling_mean2, 'tomato')\nax[2].set_title('Moving averages noise reduction')\nax[2].set(xlabel='days', ylabel='sales')\n\nfig.tight_layout()\nfig.show()","7e19184a":"def maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef denoise_signal(x, wavelet='db4', level=1):\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1\/0.45) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    return pywt.waverec(coeff, wavelet, mode='per')\n\ny_w1 = denoise_signal(all_sales)\ny_w2 = denoise_signal(x1_sales)\ny_w3 = denoise_signal(x2_sales)\n\nprint('The graphs below are the first 3 graphs with Wave denoising')\n\nfig, ax = plt.subplots(3, figsize=(15,15))\nax[0].plot([x for x in range(len(y_w1))], y_w1, 'coral')\nax[0].set_title('Wave denoising noise reduction')\nax[0].set(xlabel='days', ylabel='sales')\n\nax[1].plot([x for x in range(len(y_w2))], y_w2, 'crimson')\nax[1].set_title('Wave denoising noise reduction')\nax[1].set(xlabel='days', ylabel='sales')\n\nax[2].plot([x for x in range(len(y_w3))], y_w3, 'tomato')\nax[2].set_title('Wave denoising noise reduction')\nax[2].set(xlabel='days', ylabel='sales')\n\nfig.tight_layout()\nfig.show()","6a23f6bd":"item_1 = df.iloc[1].values[6:]\nitem_1_pred = dt.iloc[1].values[1:]\nfig, ax = plt.subplots(1, figsize=(15,5))\nax.plot(([x for x in range(len(item_1))]+[x for x in range(len(item_1_pred))]), np.hstack((item_1,item_1_pred)), 'blue')\nax.axvspan(len(item_1), (len(item_1)+len(item_1_pred)), color='red', alpha=0.4)\nax.set(xlabel='days', ylabel='sales')\nax.set_title('We need to predict the sales in the red line')\n","6ac76f5a":"!pip install pmdarima\nfrom pmdarima import auto_arima","3195eac9":"model = auto_arima(item_1[-300:-14], seasonal=True, m=14)","c5e73649":"fig, ax = plt.subplots(1, figsize=(15,5))\nax.plot([x for x in range(300)], item_1[-300:], 'blue', label='Item sales')\nax.plot([x for x in range(300+len(item_1_pred))], np.hstack((model.predict_in_sample(),model.predict(14), model.predict(14+len(item_1_pred))[14:])), 'orange', label='Sarima prediction')\nax.axvspan((300-14), 300, color='green', alpha=0.4, label='test set')\nax.axvspan(300, 300+len(item_1_pred), color='red', alpha=0.2, label='prediction')\nax.legend(loc=\"upper left\")\nax.set(xlabel='days', ylabel='sales')\nax.set_title('Iteam sales and sarima prediction')\nfig.show()","efcbc6fb":"from sklearn.metrics import mean_squared_error\ndef measure_mse(actual, predicted):\n    return mean_squared_error(actual, predicted)","97d6acba":"print('the score (mse)  of the model is: ', measure_mse(item_1[-14:],model.predict(14)))","051c9392":"from math import sqrt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D","231acb37":"def train_test_split(data, n_test):\n    return data[:-n_test], data[-n_test:]\n\ndef series_to_supervised(data, n_in, n_out=1):\n    df = pd.DataFrame(data)\n    cols = list()\n\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n\n    agg = pd.concat(cols, axis=1)\n\n    agg.dropna(inplace=True)\n    return agg.values\n\ndef prepare_dataset(train, n_input):\n    data = series_to_supervised(train, n_input)\n    train_x, train_y = data[:, :-1], data[:, -1]\n    train_x = train_x.reshape((train_x.shape[0], train_x.shape[1], 1))\n    return train_x, train_y\n    \ndef model_fit(train, config):\n    n_input, n_filters, n_kernel, n_epochs, n_batch = config\n    train_x, train_y = prepare_dataset(train, n_input)\n    \ndef model_predict(model, history, config):\n    \n    n_input, _, _, _, _ = config\n    \n    x_input = np.array(history[-n_input:]).reshape((1, n_input, 1))\n    \n    yhat = model.predict(x_input, verbose=0)\n    return yhat[0]\n\ndef predict_next_n(model, history, config, n):\n    n_input, _, _, _, _ = config\n    pred = []\n    for i in range(n):\n        x_input = np.array(history[-n_input:]).reshape((1, n_input, 1))\n\n        yhat = model.predict(x_input, verbose=0)\n        pred.append(yhat[0][0])\n        history.append(yhat[0][0])\n    return pred\n\ndef walk_forward_validation(data, n_test, cfg):\n    predictions = list()\n\n    train, test = train_test_split(data, n_test)\n\n    model = model_fit(train, cfg)\n\n    history = [x for x in train]\n\n    for i in range(len(test)):\n        yhat = model_predict(model, history, cfg)\n        predictions.append(yhat)\n        history.append(test[i])\n    error = measure_mse(test, predictions)\n    print(' > %.3f' % error)\n    return [error, model]\n\ndef repeat_evaluate(data, config, n_test, n_repeats=1):\n    for _ in range(n_repeats):\n        return_list = walk_forward_validation(data, n_test, config)\n        scores = return_list[0]\n    return scores, return_list[1]\n\n\ndef prepare_dataset_no_y(train, n_input):\n    data_prep = series_to_supervised(train, n_input)\n    data_prep = data_prep.reshape((data_prep.shape[0], data_prep.shape[1], 1))\n    return data_prep\n\ndef model_fit(train, config):\n    n_input, n_filters, n_kernel, n_epochs, n_batch = config\n    train_x, train_y = prepare_dataset(train, n_input)\n    \n    model = Sequential()\n    model.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu', input_shape=(n_input, 1), padding='same'))\n    model.add(LSTM(128, activation='relu'))\n    model.add(Dense(64, activation='relu'))\n#     model.add(Conv1D(filters=n_filters, kernel_size=n_kernel, activation='relu'))\n#     model.add(MaxPooling1D(pool_size=2))\n#     model.add(Flatten())\n    model.add(Dense(1))\n    model.compile(loss='mse', optimizer='adam')\n    model.fit(train_x, train_y, epochs=n_epochs, batch_size=n_batch, verbose=0)\n    return model","d25cc529":"data = item_1[-300:]\nn_input = 7\nn_test = 14\nconfig = [n_input, 256, 3, 400, 300]\nscores, model_cnn = repeat_evaluate(data, config, n_test)\n","23b14f89":"print('the score (mse)  of the model is: ',scores)","d700615c":"test_x = series_to_supervised(data, n_in=(n_input-1), n_out=1)\ntest_x = test_x.reshape((test_x.shape[0], test_x.shape[1], 1))\n\nx = np.arange(n_input, dtype=int)\n\npred = predict_next_n(model_cnn, data.tolist(), config, len(item_1_pred))\n\nfig, ax = plt.subplots(1, figsize=(15,5))\nax.plot(data, color = 'blue')\nax.plot(np.vstack((np.zeros_like(x).reshape(-1,1),model_cnn.predict(test_x).reshape(-1,1), np.array(pred).reshape(-1,1))), color='orange')\nax.axvspan((len(data)-14), len(data), color='green', alpha=0.4, label='test set')\nax.axvspan((len(data)), len(data)+len(item_1_pred), color='red', alpha=0.2, label='prediction')\nax.legend()","ec4383a8":"# Models","2efd3a69":"<hr>\n# Forecasting Methods\n1. SARIMA\n<hr>","45742aa6":"# Reducing noise\n1. Fast Fourier Transform","3bf40199":"**We need to predcit for every item id the next n dates.**","41dfc5dc":"Even though SARIMA may have better mse, the CNN + LSTM model seems to make better predictions and fit better on the data","ac4dd0fd":"2. Moving Averages","7d5d3475":"## **I only selected the last 300 time steps for the sarima model as it takes a pretty long time to fit the model on the entire data**","07974b36":"2. Simple Neural Network Model ( CNN + LSTM + Dense )","f425e721":"# NOTEBOOK CONTENTS\n1. Data visualization\n2. Noise reduction Tehniques\n3. Comparison between SARIMA and NN","6041515e":"3. Wave denoising <br>\n**from this awesome notebook -** https:\/\/www.kaggle.com\/tarunpaparaju\/m5-competition-eda-models"}}