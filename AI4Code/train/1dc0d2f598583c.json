{"cell_type":{"d9272d1d":"code","64c87cec":"code","7cd452b9":"code","7bfdbebf":"code","2d3c9a0c":"code","d0bd7c34":"code","7a413e20":"code","48efafcc":"code","1291278a":"code","cb810798":"code","e78a890d":"code","284d3ef4":"code","e18177a7":"code","72cdd44e":"code","77b772a3":"code","be2e7b5f":"code","c0cfb6bb":"code","a77f4cb1":"code","d2032573":"code","7b99fc66":"code","2578e234":"code","c51b85d3":"code","12777b1f":"code","dab7d7e9":"code","5051d0f0":"code","0aa74576":"code","042232b4":"code","71f8728a":"code","8ae22887":"code","d5b40fe4":"code","3b931a94":"code","1918fd5e":"code","fb5f6704":"code","54011b28":"code","5abcb804":"code","6b551eac":"code","190b41ff":"code","773227ca":"code","61bd3047":"code","5ce46beb":"code","4a280a3d":"code","ed4cfdef":"code","085ab2ad":"code","c01e72a9":"code","fb24e33e":"code","fdd590db":"code","26acfa12":"code","0c31781a":"code","d8b23f71":"code","88c77549":"code","8af734f1":"code","b79d1d46":"code","bee99181":"code","d19856e1":"code","1c02d2ff":"code","4115bb76":"code","be272daa":"code","f55d4e12":"code","5306bbe5":"code","2c9c7a13":"markdown","b798e9ed":"markdown","309097fb":"markdown","91ff3b46":"markdown","be79cb35":"markdown","f5f57eea":"markdown","6b6d9dce":"markdown","853062c1":"markdown","ed0afabc":"markdown","2e90fa4f":"markdown","61375aa0":"markdown","f197a14f":"markdown","3a873300":"markdown","20198293":"markdown","655c232b":"markdown","65dce915":"markdown","08e1a9e5":"markdown","f32e791b":"markdown","de7d7cb4":"markdown","93ecca8b":"markdown","90d1e7c4":"markdown"},"source":{"d9272d1d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nfrom mpl_toolkits.mplot3d import Axes3D\nimport requests\nimport numpy as np\nimport pandas as pd\nimport io\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom random import random\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport requests\nimport pandas as pd\nfrom datetime import date\nimport io\nimport torch\nimport os\nfrom tqdm import tqdm\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.preprocessing import MinMaxScaler\nfrom pandas.plotting import register_matplotlib_converters\nfrom torch import nn, optim\n","64c87cec":"filename = \"\/kaggle\/input\/ece657aw20asg4coronavirus\/time_series_covid19_confirmed_global.csv\"\ndf_confirmed = pd.read_csv(filename)\ndf_confirmed.head(100)","7cd452b9":"filename = \"\/kaggle\/input\/ece657aw20asg4coronavirus\/time_series_covid19_recovered_global.csv\"\ndf_recovered = pd.read_csv(filename)\ndf_recovered.head(100)","7bfdbebf":"filename = \"\/kaggle\/input\/ece657aw20asg4coronavirus\/time_series_covid19_deaths_global.csv\"\ndf_deaths = pd.read_csv(filename)\ndf_deaths.head(100)","2d3c9a0c":"BASE_URL = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/'\nCONFIRMED = 'time_series_covid19_confirmed_global.csv'\nDEATH = 'time_series_covid19_deaths_global.csv'\nRECOVERED = 'time_series_covid19_recovered_global.csv'\nCONFIRMED_US = 'time_series_covid19_confirmed_US.csv'\nDEATH_US = 'time_series_covid19_deaths_US.csv'\n\ndef get_covid_data(subset = 'CONFIRMED'):\n    \"\"\"This function returns the latest available data subset of COVID-19. \n        The returned value is in pandas DataFrame type.\n    Args:\n        subset (:obj:`str`, optional): Any value out of 5 subsets of 'CONFIRMED',\n        'DEATH', 'RECOVERED', 'CONFIRMED_US' and 'DEATH_US' is a valid input. If the value\n        is not chosen or typed wrongly, CONFIRMED subet will be returned.\n    \"\"\"    \n    switcher =  {\n                'CONFIRMED'     : BASE_URL + CONFIRMED,\n                'DEATH'         : BASE_URL + DEATH,\n                'RECOVERED'     : BASE_URL + RECOVERED,\n                'CONFIRMED_US'  : BASE_URL + CONFIRMED_US,\n                'DEATH_US'      : BASE_URL + DEATH_US,\n                }\n\n    CSV_URL = switcher.get(subset, BASE_URL + CONFIRMED)\n\n    with requests.Session() as s:\n        download        = s.get(CSV_URL)\n        decoded_content = download.content.decode('utf-8')\n        data            = pd.read_csv(io.StringIO(decoded_content))\n\n    return data","d0bd7c34":"death = get_covid_data(subset = 'DEATH')\nprint(death.head(10))\nconfirmed = get_covid_data(subset = 'CONFIRMED')\nprint(confirmed.head(10))\nCONFIR_US = get_covid_data(subset = 'CONFIRMED_US')\nDEATH_US = get_covid_data(subset = 'DEATH_US')","7a413e20":"df_1 = DEATH_US.iloc[:,12:]\ndf_1\ndf_1.isnull().sum().sum()","48efafcc":"%matplotlib inline\n%config InlineBackend.figure_format='retina'\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#93D30C\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 14, 10\nregister_matplotlib_converters()\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)","1291278a":"df = death.iloc[:, 4:]\ndf.head()\ndf.isnull().sum().sum()","cb810798":"daily_cases = df.sum(axis=0)\ndaily_cases.index = pd.to_datetime(daily_cases.index)\ndaily_cases.tail()\ndaily_cases_1 = daily_cases\nDEATH_US = df_1.sum(axis=0)\nDEATH_US.index = pd.to_datetime(DEATH_US.index)\nDEATH_US.tail()","e78a890d":"df_confir = confirmed.iloc[:, 4:]\ndf_confir.head()\ndf_confir.isnull().sum().sum()\nconfirm_cases = df_confir.sum(axis=0)\nconfirm_cases.index = pd.to_datetime(confirm_cases.index)\nconfirm_cases.head()","284d3ef4":"plt.plot(daily_cases)\nplt.title(\"Cumulative daily death cases\");","e18177a7":"dc = daily_cases.shape\ndc","72cdd44e":"#Subtracting the current value from previous value to know how many new cases were reported in a day\ndaily_cases_1 = daily_cases.diff().fillna(daily_cases[0]).astype(np.int64)\ndaily_cases_1.head()\nplt.plot(daily_cases_1)\nplt.title(\"Daily Death cases\");","77b772a3":"class CoronaVirusPredictor(nn.Module):\n\n  def __init__(self, n_features, n_hidden, seq_len, n_layers=2):\n    super(CoronaVirusPredictor, self).__init__()\n\n    self.n_hidden = n_hidden\n    self.seq_len = seq_len\n    self.n_layers = n_layers\n\n    self.lstm = nn.LSTM(\n      input_size=n_features,\n      hidden_size=n_hidden,\n      num_layers=n_layers,\n      dropout=0.5\n    )\n\n    self.linear = nn.Linear(in_features=n_hidden, out_features=1)\n\n  def reset_hidden_state(self):\n    self.hidden = (\n        torch.zeros(self.n_layers, self.seq_len, self.n_hidden),\n        torch.zeros(self.n_layers, self.seq_len, self.n_hidden)\n    )\n\n  def forward(self, sequences):\n    lstm_out, self.hidden = self.lstm(\n      sequences.view(len(sequences), self.seq_len, -1),\n      self.hidden\n    )\n    last_time_step = \\\n      lstm_out.view(self.seq_len, len(sequences), self.n_hidden)[-1]\n    y_pred = self.linear(last_time_step)\n    return y_pred\n#Training RNN model w\/ train and test data\ndef train_model(\n  model,\n  train_data,\n  train_labels,\n  test_data=None,\n  test_labels=None\n):\n  loss_fn = torch.nn.MSELoss(reduction='sum')\n\n  optimiser = torch.optim.Adam(model.parameters(), lr=1e-3)\n  num_epochs = 60\n\n  train_hist = np.zeros(num_epochs)\n  test_hist = np.zeros(num_epochs)\n\n  for t in range(num_epochs):\n    model.reset_hidden_state()\n\n    y_pred = model(X_train)\n\n    loss = loss_fn(y_pred.float(), y_train)\n\n    if test_data is not None:\n      with torch.no_grad():\n        y_test_pred = model(X_test)\n        test_loss = loss_fn(y_test_pred.float(), y_test)\n      test_hist[t] = test_loss.item()\n\n      if t % 10 == 0:\n        print(f'Epoch {t} train loss: {loss.item()} test loss: {test_loss.item()}')\n    elif t % 10 == 0:\n      print(f'Epoch {t} train loss: {loss.item()}')\n\n    train_hist[t] = loss.item()\n\n    optimiser.zero_grad()\n\n    loss.backward()\n\n    optimiser.step()\n\n  return model.eval(), train_hist, test_hist","be2e7b5f":"df = pd.read_csv('..\/input\/corona\/Corona_data.csv',encoding='ISO-8859-1') # load data from csv\ntoday = date.today()\nprint(\"Total corona virus cases in all countries on date:\", today)\ndf[[\"Total cases\"]] = df[[\"Total cases X 10^6\"]]*1000\ndf[[\"Total Population in Million\"]] = df[[\"Total Population in Million\"]]*10\ndf.head() # Gives first 5 rows","c0cfb6bb":"df_2 = df.iloc[:,0:13]\nplt.figure(figsize=(30,30))\nplt.matshow(df.corr(),fignum = 1)\nplt.xticks(range(len(df_2.columns)), df_2.columns)\nplt.yticks(range(len(df_2.columns)), df_2.columns)\nplt.colorbar()\nplt.show()","a77f4cb1":"df_1 = df.iloc[0:20]\ndf_1\ndf[[\"Total cases X 10^6\"]] = df[[\"Total cases X 10^6\"]]*100\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = [17, 17]\nfrom matplotlib import figure\nfrom matplotlib.figure import Figure\nGDP = df_1['Total Population in Million'].to_numpy()\nGDP\nTC = df_1['Total cases'].to_numpy()\nTC\nindex = df_1['Country'].to_numpy()\ndf = pd.DataFrame({'Total Population in Million': GDP,\n                    'Total cases': TC}, index=index)\nax = df.plot.bar(rot=0)","d2032573":"import matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = [17, 17]\nfrom matplotlib import figure\nfrom matplotlib.figure import Figure\nGDP = df_1['GDP per capita'].to_numpy()\nGDP\nTC = df_1['Total cases'].to_numpy()\nTC\nindex = df_1['Country'].to_numpy()\ndf = pd.DataFrame({'GDP per capita': GDP,\n                    'Total cases': TC}, index=index)\nax = df.plot.bar(rot=0)","7b99fc66":"#Splitting training and testing data into 80\/20 ratio. 80% for training and 20% for testing\ntest_data_size = 18\ntrain_data = daily_cases[:-test_data_size]\ntest_data = daily_cases[-test_data_size:]\ntrain_data.shape\nscaler = MinMaxScaler()\nscaler = scaler.fit(np.expand_dims(train_data, axis=1))\ntrain_data = scaler.transform(np.expand_dims(train_data, axis=1))\ntest_data = scaler.transform(np.expand_dims(test_data, axis=1))","2578e234":"def create_sequences(data, seq_length):\n    xs = []\n    ys = []\n\n    for i in range(len(data)-seq_length-1):\n        x = data[i:(i+seq_length)]\n        y = data[i+seq_length]\n        xs.append(x)\n        ys.append(y)\n\n    return np.array(xs), np.array(ys)\nseq_length = 5\nX_train, y_train = create_sequences(train_data, seq_length)\nX_test, y_test = create_sequences(test_data, seq_length)\nX_train = torch.from_numpy(X_train).float()\ny_train = torch.from_numpy(y_train).float()\nX_test = torch.from_numpy(X_test).float()\ny_test = torch.from_numpy(y_test).float()","c51b85d3":"X_train.shape\nX_train[:2]\ny_train.shape\ny_train[:2]\ntrain_data[:10]","12777b1f":"model = CoronaVirusPredictor(\n  n_features=1,\n  n_hidden=512,\n  seq_len=seq_length,\n  n_layers=2\n)\nmodel, train_hist, test_hist = train_model(\n  model,\n  X_train,\n  y_train,\n  X_test,\n  y_test\n)","dab7d7e9":"plt.plot(train_hist, label=\"Training loss for death data\")\nplt.plot(test_hist, label=\"Test loss for death data\")\nplt.ylim((0, 5))\nplt.legend();","5051d0f0":"with torch.no_grad():\n  test_seq = X_test[:1]\n  preds = []\n  for _ in range(len(X_test)):\n    y_test_pred = model(test_seq)\n    pred = torch.flatten(y_test_pred).item()\n    preds.append(pred)\n    new_seq = test_seq.numpy().flatten()\n    new_seq = np.append(new_seq, [pred])\n    new_seq = new_seq[1:]\n    test_seq = torch.as_tensor(new_seq).view(1, seq_length, 1).float()","0aa74576":"true_cases = scaler.inverse_transform(\n    np.expand_dims(y_test.flatten().numpy(), axis=0)\n).flatten()\n\npredicted_cases = scaler.inverse_transform(\n  np.expand_dims(preds, axis=0)\n).flatten()","042232b4":"plt.plot(\n  daily_cases.index[:len(train_data)],\n  scaler.inverse_transform(train_data).flatten(),\n  label='Historical Daily death Cases'\n)\n\nplt.plot(\n  daily_cases.index[len(train_data):len(train_data) + len(true_cases)],\n  true_cases,\n  label='Real Daily death Cases'\n)\n\nplt.plot(\n  daily_cases.index[len(train_data):len(train_data) + len(true_cases)],\n  predicted_cases,\n  label='Predicted Daily death Cases'\n)\n\nplt.legend();","71f8728a":"#Now using all data to predcit deaths using RNN\nscaler = MinMaxScaler()\nscaler = scaler.fit(np.expand_dims(daily_cases, axis=1))\nall_data = scaler.transform(np.expand_dims(daily_cases, axis=1))\nall_data.shape","8ae22887":"X_all, y_all = create_sequences(all_data, seq_length)\nX_all = torch.from_numpy(X_all).float()\ny_all = torch.from_numpy(y_all).float()\nmodel = CoronaVirusPredictor(\n  n_features=1,\n  n_hidden=512,\n  seq_len=seq_length,\n  n_layers=2\n)\nmodel, train_hist, _ = train_model(model, X_all, y_all)","d5b40fe4":"DAYS_TO_PREDICT = 12\nwith torch.no_grad():\n  test_seq = X_all[:1]\n  preds = []\n  for _ in range(DAYS_TO_PREDICT):\n    y_test_pred = model(test_seq)\n    pred = torch.flatten(y_test_pred).item()\n    preds.append(pred)\n    new_seq = test_seq.numpy().flatten()\n    new_seq = np.append(new_seq, [pred])\n    new_seq = new_seq[1:]\n    test_seq = torch.as_tensor(new_seq).view(1, seq_length, 1).float()","3b931a94":"predicted_cases = scaler.inverse_transform(\n  np.expand_dims(preds, axis=0)\n).flatten()","1918fd5e":"predicted_index = pd.date_range(\n  start=daily_cases.index[-1],\n  periods=DAYS_TO_PREDICT + 1,\n  closed='right'\n)\n\npredicted_cases = pd.Series(\n  data=predicted_cases,\n  index=predicted_index\n)\n\nplt.plot(predicted_cases, label='Predicted Daily Death Cases')\nplt.legend();","fb5f6704":"plt.plot(daily_cases, label='Historical Daily Death Cases')\nplt.plot(predicted_cases, label='Predicted Daily Death Cases')\nplt.legend();","54011b28":"#Now using all data to predcit cnfirmed cases using RNN\nscaler = MinMaxScaler()\nscaler = scaler.fit(np.expand_dims(confirm_cases, axis=1))\nall_data = scaler.transform(np.expand_dims(confirm_cases, axis=1))\nall_data.shape","5abcb804":"X_all, y_all = create_sequences(all_data, seq_length)\nX_all = torch.from_numpy(X_all).float()\ny_all = torch.from_numpy(y_all).float()\nseq_length = 5\nmodel = CoronaVirusPredictor(\n  n_features=1,\n  n_hidden=512,\n  seq_len=seq_length,\n  n_layers=2\n)\nmodel, train_hist, _ = train_model(model, X_all, y_all)","6b551eac":"DAYS_TO_PREDICT = 30\nwith torch.no_grad():\n  test_seq = X_all[:1]\n  preds = []\n  for _ in range(DAYS_TO_PREDICT):\n    y_test_pred = model(test_seq)\n    pred = torch.flatten(y_test_pred).item()\n    preds.append(pred)\n    new_seq = test_seq.numpy().flatten()\n    new_seq = np.append(new_seq, [pred])\n    new_seq = new_seq[1:]\n    test_seq = torch.as_tensor(new_seq).view(1, seq_length, 1).float()","190b41ff":"predicted_cases = scaler.inverse_transform(\n  np.expand_dims(preds, axis=0)\n).flatten()\ndaily_cases.index[-1]","773227ca":"predicted_index = pd.date_range(\n  start=confirm_cases.index[-1],\n  periods=DAYS_TO_PREDICT + 1,\n  closed='right'\n)\n\npredicted_cases = pd.Series(\n  data=predicted_cases,\n  index=predicted_index\n)\nplt.ticklabel_format(useOffset=False, style='plain')\nplt.plot(predicted_cases, label='Predicted Daily Confirmed Cases')\nplt.legend();","61bd3047":"plt.ticklabel_format(useOffset=False, style='plain')\nplt.plot(confirm_cases, label='Historical Daily Confirmed Cases')\nplt.plot(predicted_cases, label='Predicted Daily Confirmed Cases')\nplt.legend();","5ce46beb":"BASE_URL = 'https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/'\nCONFIRMED = 'time_series_covid19_confirmed_global.csv'\nDEATH = 'time_series_covid19_deaths_global.csv'\nRECOVERED = 'time_series_covid19_recovered_global.csv'\nCONFIRMED_US = 'time_series_covid19_confirmed_US.csv'\nDEATH_US = 'time_series_covid19_deaths_US.csv'\n\ndef get_covid_data(subset = 'CONFIRMED'):\n    \"\"\"This function returns the latest available data subset of COVID-19. \n        The returned value is in pandas DataFrame type.\n    Args:\n        subset (:obj:`str`, optional): Any value out of 5 subsets of 'CONFIRMED',\n        'DEATH', 'RECOVERED', 'CONFIRMED_US' and 'DEATH_US' is a valid input. If the value\n        is not chosen or typed wrongly, CONFIRMED subet will be returned.\n    \"\"\"    \n    switcher =  {\n                'CONFIRMED'     : BASE_URL + CONFIRMED,\n                'DEATH'         : BASE_URL + DEATH,\n                'RECOVERED'     : BASE_URL + RECOVERED,\n                'CONFIRMED_US'  : BASE_URL + CONFIRMED_US,\n                'DEATH_US'      : BASE_URL + DEATH_US,\n                }\n\n    CSV_URL = switcher.get(subset, BASE_URL + CONFIRMED)\n\n    with requests.Session() as s:\n        download        = s.get(CSV_URL)\n        decoded_content = download.content.decode('utf-8')\n        data            = pd.read_csv(io.StringIO(decoded_content))\n\n    return data","4a280a3d":"##COMPARISON\ntest_data_size = 18\ntrain_data = daily_cases_1[:-test_data_size]\ntest_data = daily_cases_1[-test_data_size:]\ntrain_data.shape","ed4cfdef":"test_data.shape","085ab2ad":"#Comparison of RNN with real daily cases\nscaler = MinMaxScaler()\n\nscaler = scaler.fit(np.expand_dims(train_data, axis=1))\n\nall_data = scaler.transform(np.expand_dims(train_data, axis=1))\n\nall_data.shape","c01e72a9":"X_all, y_all = create_sequences(all_data, seq_length)\n\nX_all = torch.from_numpy(X_all).float()\ny_all = torch.from_numpy(y_all).float()\n\nmodel = CoronaVirusPredictor(\n  n_features=1,\n  n_hidden=512,\n  seq_len=seq_length,\n  n_layers=2\n)\nmodel, train_hist, _ = train_model(model, X_all, y_all)","fb24e33e":"DAYS_TO_PREDICT = 18\n\nwith torch.no_grad():\n  test_seq = X_all[:1]\n  preds = []\n  for _ in range(DAYS_TO_PREDICT):\n    y_test_pred = model(test_seq)\n    pred = torch.flatten(y_test_pred).item()\n    preds.append(pred)\n    new_seq = test_seq.numpy().flatten()\n    new_seq = np.append(new_seq, [pred])\n    new_seq = new_seq[1:]\n    test_seq = torch.as_tensor(new_seq).view(1, seq_length, 1).float()","fdd590db":"predicted_cases = scaler.inverse_transform(\n  np.expand_dims(preds, axis=0)\n).flatten()","26acfa12":"train_data.index[-1]","0c31781a":"predicted_index = pd.date_range(\n  start=train_data.index[-1],\n  periods=DAYS_TO_PREDICT + 1,\n  closed='right'\n)\n\npredicted_cases = pd.Series(\n  data=predicted_cases,\n  index=predicted_index\n)\n\nplt.plot(predicted_cases, label='Predicted Daily Cases')\nplt.legend();","d8b23f71":"plt.plot(train_data, label='Historical Daily Cases')\nplt.plot(predicted_cases, label='Predicted Daily Cases')\nplt.plot(test_data,label='Real daily cases')\nplt.legend();","88c77549":"def covid_model(input_len):\n  covid_19_RNN=tf.keras.models.Sequential()\n  #input layer\n  covid_19_RNN.add(tf.keras.layers.LSTM(units = 250, return_sequences=True,input_shape=(input_len,1)))\n  covid_19_RNN.add(tf.keras.layers.Dropout(0.01))\n  #1st hidden layer\n  covid_19_RNN.add(tf.keras.layers.LSTM(units = 150, return_sequences=True))\n  covid_19_RNN.add(tf.keras.layers.Dropout(0.01))\n  # #2nd hidden layer\n  covid_19_RNN.add(tf.keras.layers.LSTM(units = 200, return_sequences=True))\n  covid_19_RNN.add(tf.keras.layers.Dropout(0.02))\n  #3rd hidden layer\n  covid_19_RNN.add(tf.keras.layers.LSTM(units = 150, return_sequences=True))\n  covid_19_RNN.add(tf.keras.layers.Dropout(0.02))\n  #4th hidden layer\n  covid_19_RNN.add(tf.keras.layers.LSTM(units = 125, return_sequences=True))\n  covid_19_RNN.add(tf.keras.layers.Dropout(0.04))\n  #output layer\n  covid_19_RNN.add(tf.keras.layers.Dense(units = 1))\n  #compile the model\n  covid_19_RNN.compile(optimizer='adam',loss='mae')\n  return covid_19_RNN\n# Initializing Parameters\nTEST_PORTION = 0\nTEST_SIZE = 14 \nOBSERVATION_WINDOW_SIZE = 14\nAUGMENTATION_LEVEL = 50\nAUGMENTATION_SCORE = 0.13\nINPUT_FILE = 'CONFIRMED'\n\"\"\"\nINPUT_FILE : 'CONFIRMED', 'DEATH', 'RECOVERED', 'CONFIRMED_US' and 'DEATH_US'\n\"\"\"","8af734f1":"# Reading the Input File\n\ndf = get_covid_data(subset = INPUT_FILE)\nprint(df)\n\nif 'US' in INPUT_FILE:\n  data_start = 10\nelse:\n  data_start = 3\n\nconfirmed_data=pd.DataFrame(df.sum(axis=0))[data_start:]\ndates=list(confirmed_data.index)\ntest_size = np.asarray(max( np.round( len(dates) * TEST_PORTION ) , TEST_SIZE )).astype(int)\ntime_steps = max( OBSERVATION_WINDOW_SIZE , 1)\naug_steps = max( AUGMENTATION_LEVEL , 1)\nscaler_threshold = min( AUGMENTATION_SCORE, 0.2 )\n\nday=[]\nfor i in range(0,len(dates)):\n  day.append(i+1)\nday=np.array(day)","b79d1d46":"# Augmentation and Preprocessing of the Train Data\nscaler = MinMaxScaler( feature_range=(random()*0.2,1-random()*0.2) )\ntrain_data = confirmed_data[:-test_size]\nx_data=[]\ny_data=[]\nfor j in range(0,aug_steps):\n    for i in range(time_steps,len(train_data)):\n        scaler=MinMaxScaler( feature_range=(random()*scaler_threshold, 1 - random()*scaler_threshold) )\n        augmentation = scaler.fit_transform(train_data[i-time_steps:i+1])\n        x_data.append(augmentation[:time_steps])\n        y_data.append(augmentation[time_steps])\nx_data = np.array(x_data)\nx_data = np.reshape(x_data,(x_data.shape[0],x_data.shape[1],1))\ny_data = np.array(y_data)","bee99181":"# Fitting the model to the augmented data\ncovid_19_RNN = covid_model(x_data.shape[1])\ncovid_19_RNN.fit(x_data ,y_data , batch_size=2048 , epochs=200 , validation_split=0.4 , verbose=2)","d19856e1":"# The test data sequence generation and preprocessing\ntest_data = confirmed_data.values\ntest_data = test_data[-(test_size + time_steps):]\ntest_data = np.array(test_data).reshape(-1,1)","1c02d2ff":"# Prediction using the trained covid_19_RNN model\nscaler=MinMaxScaler( feature_range = (scaler_threshold, 1 - scaler_threshold) )\nprediction_result = []\nfor i in range(time_steps ,len(test_data)):\n  x_test = []\n  x_test.append( scaler.fit_transform(test_data[ i - time_steps : i ]) )\n  x_test = np.reshape(x_test[0],(1 , x_test[0].shape[0], 1))\n  predicted_covid_19_spread = covid_19_RNN.predict(x_test)[0][-1]\n  predict_val = scaler.inverse_transform(predicted_covid_19_spread.reshape(1,1))\n  predict_val = predict_val.astype(int)\n  prediction_result.append(predict_val[0][0])\n\ny_test = test_data[time_steps : len(test_data)]\ny_test = y_test.reshape(len(y_test)).astype(int)","4115bb76":"# Test Data Results\nresult_error = prediction_result - y_test\nresult_error_percentage = 100*np.divide(result_error, y_test)\ntest_dates  = np.asarray(dates[-len(y_test):]).reshape(len(y_test))\nreal_value  = y_test.reshape(len(y_test)).astype(int)\npredition   = np.asarray(prediction_result)\nerror_p     = result_error_percentage.reshape(len(y_test))\nerror_p     = np.around(error_p, decimals=2)\nResults = {'Date': test_dates, 'Real Value [No.]': real_value, 'Predicted Value [No.]': predition, 'Error Percentage [%]':error_p}\ndf_results = pd.DataFrame(data=Results)\nprint(df_results)\nMAEP  = mean_absolute_error(predition, y_test)\nMAEP  = np.around(MAEP, decimals=0)\nprint('MAE of Prediction                 :' + str(MAEP))\nMAPEP = mean_absolute_error(error_p,np.zeros(len(error_p)))\nMAPEP = np.around(MAPEP, decimals=2)\nprint('Mean Absolute of Prediction Error Percentages  [%]:' + str(MAPEP))","be272daa":"plt.figure(2, figsize=(12,9))\nplt.plot(range(0,len(confirmed_data)),confirmed_data[0],label=\"Actual Number of corona-cases\", c='g')\nplt.plot(range(len(confirmed_data) - test_size ,len(confirmed_data)),prediction_result,label=\"Prediction of Number of corona-cases\",c='r')\nplt.xlabel('Days')\nplt.ylabel('COVID-19 Cases')\nplt.legend()\nplt.grid(which='both', axis='both')\nplt.title('Daily prediction of COVID-19 Cases - ' + INPUT_FILE.upper() + ' GLOBAL')\nplt.show()","f55d4e12":"# Prediction using the trained covid_19_RNN model\nscaler=MinMaxScaler( feature_range = (scaler_threshold, 1 - scaler_threshold) )\nprediction_result = []\n\nfor i in range(time_steps ,len(test_data)):\n  x_test = []\n  x_test.append( scaler.fit_transform(test_data[ i - time_steps : i ]) )\n  x_test = np.reshape(x_test[0],(1 , x_test[0].shape[0], 1))\n  predicted_covid_19_spread = covid_19_RNN.predict(x_test)[0][-1]\n  predict_val = scaler.inverse_transform(predicted_covid_19_spread.reshape(1,1))\n  predict_val = predict_val.astype(int)\n  test_data[i]= predict_val\n  prediction_result.append(predict_val[0][0])\n\ntest_data = confirmed_data.values\ntest_data = test_data[-(test_size + time_steps):]\ntest_data = np.array(test_data).reshape(-1,1)\n\ny_test = test_data[time_steps : len(test_data)]\ny_test = y_test.reshape(len(y_test)).astype(int)","5306bbe5":"# Test Data Results\nresult_error = prediction_result - y_test\nresult_error_percentage = 100*np.divide(result_error, y_test)\ntest_dates  = np.asarray(dates[-len(y_test):]).reshape(len(y_test))\nreal_value  = y_test.reshape(len(y_test)).astype(int)\npredition   = np.asarray(prediction_result)\nerror_p     = result_error_percentage.reshape(len(y_test))\nerror_p     = np.around(error_p, decimals=2)\nResults = {'Date': test_dates, 'Real Value [No.]': real_value, 'Predicted Value [No.]': predition, 'Error Percentage [%]':error_p}\ndf_results = pd.DataFrame(data=Results)\nprint(df_results)\nMAEP  = mean_absolute_error(predition, y_test)\nMAEP  = np.around(MAEP, decimals=0)\nprint('\\nMean Absolute Error of Prediction                 :' + str(MAEP))\nMAPEP = mean_absolute_error(error_p,np.zeros(len(error_p)))\nMAPEP = np.around(MAPEP, decimals=2)\nprint('Mean Absolute of Prediction Error Percentages  [%]:' + str(MAPEP))","2c9c7a13":"# READING IN THE DATASETS FOR ANALYSIS","b798e9ed":"# USING RNN WITH ALL DATA TO PREDICT TOTAL CONFIRMED CASES FROM TODAY TO NEXT 30 DAYS","309097fb":"# COMPARISON OF RNN PREDICTIONS W\/ REAL VALUES OF DAILY CORONAVIROUS CASES OR THE LAST 18 DAYS","91ff3b46":"**The LSTM module, which today can be seen as multiple switch gates, and a bit like ResNet it can bypass units and thus remember for longer time steps. LSTM thus have a way to remove some of the vanishing gradients problems.LSTM and GRU and derivatives are able to learn a lot of longer term information which help LSTM in time series analysis**.** Therefore we conclude that model 2 should be used for time series analysis as It takes data and use data augmentation in addition to different hidden layers of the network to have good time series results.**","be79cb35":"For the purpose of Assignment We used LSTM type RNN with two variations. Method 1 model uses default parameter values and those parameter values were fixed at  (n_features=1,n_hidden=512,seq_len=seq_length,n_layers=2). \nIn second approach i.e. Method 2 we used another method to train the Layer's of RNN and with Data Augmentation to check which model gives us the best results.","f5f57eea":"**Analysis: Using Method 2 for RNN with data augmentation the prediction is coming out to be very precise. The prediction follows the actual number of the cases very closely and it is clear that RNN with 4 input layers is performing very well to predict the number of cases of COVID-19. As given in the graph the data follows an exponential curve and trend seems to increase for now.**","6b6d9dce":"# Making use of all the data available to predict the deaths using RNN","853062c1":"**Analysis: After using whole dataset we found out that our model performs the best and but unfortunately as RNN suggest that death toll to keep rising and with prediction it shows that there will be in total 120,000 more deaths by the early May.There is no sign for the curve to slow down therefore it shows that this trend is going to continue for next few days. At the time of writing this report there were 5400 cases in a single day. Therefore the trend seems to be justified in the next few days. The whole graph is exponential as the values on Y axis are very small and to the degree of 10^6. I used this algorithm to predict the total deaths in the next 12 days.**","ed0afabc":"# FINDING CORRELATIONS OF CORONAVIROUS W\/ GDP AND POPULATION\n# Is there any relationship between COVID-19 cases, GDP per capita and Population of that particular Country?","2e90fa4f":"Analysis: A very interesting fact that we came across from this graph is that Countries with population range of 20 million to 350 million(USA) has suffered the most from the virous. The countries with high population such as India, China, Bangladesh has relatively less cases than per million people than other countries such as USA, Spain, Italy, Turkey. We plotted top 20 countries which consitutes about 80% of the total coronavirous cases in the World. Some countries with less population in relative terms are most affected such as Spain, Italy, France, UK and Germany while others countries such as Austria and Ireland suffered less.\nAnother interesting fact that we found out about the virous is that 15 out of the total 20 countries are in Europe or North America which means that due to easy border restrictions in Europe the virous spreaded quickly through the europe as opposed to the south east asia where there are far less cases of the virous. Therefore Europe right now acts as the centre for the virous.","61375aa0":"# Method 1 model: Using default parameter for RNN model\n## RNN(Recurrent Neural Network) : Recurrent neural networks (RNN) are a class of neural networks that is powerful for modeling sequence data such as time series or natural language. Schematically, a RNN layer uses a for loop to iterate over the timesteps of a sequence, while maintaining an internal state that encodes information about the timesteps it has seen so far\n","f197a14f":"**Analysis: After using the total dataset for confirmed cases using RNN the algorithm predicts that by May 15th, there will be total of 2 million more confirmed cases in the world. The algorithm suggests that the curve of total confirmed cases will almost become flat after 2 millions more cases and will increase very slowly. The trend seems reasonable as in few countries this virous is just at its early stages and we can expect more cases to unearth in the coming few days.**","3a873300":"# PREDICTING DEATHS USING RECURRENT NEURAL NETWORK BY SPLITTING DATA AS 80% FOR TRAINING FOR 20% FOR TESTING","20198293":"Analysis: After applying RNN with default parameters on the given data we found out that there is a error between the prediction of the RNN model and real daily corona virous cases. This can be contributed to the fact that there is still less data avialble for the analysis and lot of physical factors are involved in the spread of the virous therefore It is very hard for a algorithm to predict all the physical factors and daily corona cases. However the model suggests that the cases curve will flatten out few days after the mid of the month which is the case in the real scenario as well. In real scenario the count for daily cases jumped by 10,000 but the algothirm predicts them to jump to 60000 and then keep constant.","655c232b":"Analysis: This bar graph was included to support the bar graph given above. As from this it can be concluded that virous spreaded heavily in the Rich countries. Countries with GDP per capita more than 25000 has high cases of the coronavirous while the poor countries such as IRAN, INDIA, TURKEY AND CHINA has relatively less cases than the other countries. It can be stated as to the slow testing rate of the poor countries as not many resources are available but till date the RICh countries have high concentration of the virous.\nMany coutries with per capita GDP more than 25000$ such as USA, ITALY, GERMANY, UK, BELGIUM and CANADA has more cases in comparison to the poor countries.\nThe study and interpretation can be inculded in the future work about coronovirous and other factors such as epidenmic history and other diesaese data can be inluded in the analysis as well. Overall the graph shows that the poor countries have less cases than the coutries with GDP per capita more than 25K USD.\n","65dce915":"**ANALYSIS: After using Method 2 to predict confirmed cases of coronavirous using Recuurent neural network with LSTM method and by adding more layers and data augmentation to the model the accuracy was pretty acurate and the error percentage got reduced to 7.7%. The maximum individua error for a particular day was 17.76% for 20th April. \nThe graph generated that the prediction using RNN was quite accurate and the curve was pretty similar to the real data.\nAs RNN and LSTM use sequential processing over the time therefore the predictions were quite accurate with second model.\n![image.png](attachment:image.png)**","08e1a9e5":"**Analysis: As we can see that with each and every iteration the training and test loss is decreasing. As the algorithm learns data and the best training loss is acheived after 50 epoch's.**","f32e791b":"# USING second technique for RNN to predict using Data Augmentation","de7d7cb4":"**ANALYSIS:** Correlation plot was used to find the correlations between the different features of the data. Some features were added in the data to make sense and correlate those features with the data. Features like GDP per capita of the country and Population of the country can play a significant role while dealing with the virous. \nTherefore those features were added to the data and the relationships from the corrplot are described below:\n1. GDP per capita is positiviely correlated with Serious\/critical cases. The correlation is about 80% which strongly suggests that GDP per capita has a correlation with Serious\/Critical cases. A conclusion can be driven from here that if a country has higher GDP per capita the tedency for that country to have serious and critical cases is high.\n\n2.Total population in million has positive correlation with total deaths which is pretty obious as well. The correlation is as high as 80% and as the population of the country increases so as the total deaths.","93ecca8b":"# UNIVERSITY OF WATERLOO\n# ASSIGNMENT 4 - ECE657A \n# NAVKIRAN SINGH 20802009\n# SHRUTHI SRINIVASAN 20764126","90d1e7c4":"Analysis: After doing the 80 and 20% spit of training and testing data we saw that our model perfromance worsend as both the values of real and predicted value are far away from each other. This might be due to the reason that when we do 80\/20 split and there is very less training data and even less to test as well. There is not much training data therefore model is not able to perform that great."}}