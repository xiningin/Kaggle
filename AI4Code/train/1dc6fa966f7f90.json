{"cell_type":{"03a8b0c2":"code","5e513939":"code","e21717b9":"code","7eda5264":"code","799859dd":"code","75b1f468":"code","ca9356c2":"code","8c93ae4b":"code","f617237c":"code","161e87e8":"code","a50081b7":"code","6115bec0":"code","749c6e4c":"code","7a41291d":"code","adfa7926":"code","7e8de66a":"code","952a7933":"code","08bff9ec":"code","486adbc0":"code","b967a089":"code","52698f29":"code","09b90e97":"code","dca6d648":"code","7f691d78":"code","de8b31f9":"code","94d1f01b":"code","18df20bc":"code","0628c2dd":"code","f723c580":"code","b835bffb":"markdown","abd3b4a2":"markdown","cf1af15f":"markdown","e6fef73c":"markdown","fe1d8083":"markdown","4db81d28":"markdown","25971b3e":"markdown","bf691d38":"markdown","f90868c1":"markdown","5a6e2a29":"markdown","3ee5d5ee":"markdown","59c977f1":"markdown","4b8d8c90":"markdown","ec92d213":"markdown","18fbf7a0":"markdown","57d82a87":"markdown","92a047e3":"markdown","5cde2f21":"markdown","e5812028":"markdown","c389f75b":"markdown","b75579e3":"markdown","44113583":"markdown","0dbeca85":"markdown","d2aad916":"markdown","51db752a":"markdown","584713a1":"markdown","805568c7":"markdown","75935ac0":"markdown","fb38de21":"markdown","8ad8c9a4":"markdown","05fdd67d":"markdown","3359f95c":"markdown","f8c8e36b":"markdown","1e6ea881":"markdown","3e858f9e":"markdown","136d0ad3":"markdown","0dc60812":"markdown","477afd07":"markdown","b6c34cdf":"markdown","acd1fa79":"markdown"},"source":{"03a8b0c2":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nsns.set()\nimport matplotlib.pyplot as plt \nfrom numpy import cov","5e513939":"person=pd.read_csv('..\/input\/gender-classification-dataset\/gender_classification_v7.csv')\nperson\n","e21717b9":"person.info()","7eda5264":"person.gender.value_counts()","799859dd":"gender1=person.groupby(['gender']).mean()\ngender1","75b1f468":"from matplotlib.ticker import FuncFormatter\n\nplt.figure(figsize=(20,20))\ngender2=gender1.plot(kind='bar',figsize=(10,6))\n\nfor item in gender2.get_xticklabels():\n    item.set_rotation(0)\n    \nplt.ylim(0.0, 14.0)\nplt.legend(loc='center')\nplt.title('avarage parameters per gender')\nplt.show()","ca9356c2":"person.groupby(['lips_thin','gender']).size().unstack().plot(kind='bar',figsize=(10,6))\nplt.title('lips thin per gender')\nperson.groupby(['distance_nose_to_lip_long','gender']).size().unstack().plot(kind='bar',figsize=(10,6))\nplt.title('distance from nose to lip  per gender')\nplt.show()\n# print the size:\nperson.groupby(['lips_thin','gender']).size()\n","8c93ae4b":"person.groupby(['distance_nose_to_lip_long','gender']).size()","f617237c":"sns.displot(data=person, x=\"forehead_width_cm\", col=\"gender\",kde=True, color='b')","161e87e8":"sns.displot(data=person, x=\"forehead_height_cm\", col=\"gender\",kde=True, color='r' )\n","a50081b7":"person.groupby(['long_hair','gender']).size().unstack().plot(kind='bar',figsize=(10,6))\n\nplt.title('long hair per gender')\nplt.show()\nperson.groupby(['long_hair','gender']).size()","6115bec0":"person.groupby(['nose_wide','gender']).size().unstack().plot(kind='bar',figsize=(10,6))\nplt.title('nose wide per gender')\nperson.groupby(['nose_long','gender']).size().unstack().plot(kind='bar',figsize=(10,6))\nplt.title('nose long per gender')\nplt.show()\n##printing the size\nperson.groupby(['nose_wide','gender']).size()","749c6e4c":"person.groupby(['nose_long','gender']).size()","7a41291d":" person['gender_code']=pd.factorize(person.gender)[0]\nperson.head()","adfa7926":"person.drop(['gender'],axis=1 ,inplace = True)","7e8de66a":"mask = np.zeros_like(person.corr())\nmask[np.tril_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(10, 8))\ncorr = person.corr()\nsns.heatmap(corr, vmax=1,annot=True,cmap='twilight', mask=mask.T,square=True)\n\nplt.title('Correlation between facial features')","952a7933":"from sklearn.model_selection import train_test_split\n\ny = person[\"gender_code\"]\nx = person.drop([\"gender_code\"], axis=1)\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state = 42)\n\nprint(\"x_train : \",x_train)\nprint ('x train size is:' ,x_train.shape)\nprint ()\n\nprint(\"x_test : \",x_test)\nprint ('x test size is:' ,x_test.shape)\nprint ()\n\nprint(\"y_train : \",y_train)\nprint ('y train size is:' ,y_train.shape)\nprint ()\n\nprint(\"y_test : \",y_test)\nprint ('y test size is:' ,y_test.shape)\nprint ()\n\nprint(f'Size of Training set: {len(x_train), len(y_train)}')\nprint(f'Size of Test set {len(x_test), len(y_test)}')","08bff9ec":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\nk_range = list(range(1, 100))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train, y_train)\n    y_pred = knn.predict(x_test)\n    # calculate accuracy\n    mas=metrics.accuracy_score(y_test, y_pred)\n    if mas>=0.97:\n        print('for k=',k,'the accuracy is:', mas)","486adbc0":"k_range = list(range(1, 100))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(x_train, y_train)\n    y_pred_knn = knn.predict(x_test)\n    mas=metrics.accuracy_score(y_test, y_pred_knn)\n    if mas>=0.972:\n        print('for k=',k,'the accuracy is:', mas)","b967a089":"confusion = metrics.confusion_matrix(y_test, y_pred)\nTP = confusion[1, 1]\nTN = confusion[0, 0]\nFP = confusion[0, 1]\nFN = confusion[1, 0]\n\nf, ax = plt.subplots(figsize = (5,5))\n\nsns.heatmap(confusion,annot = True, linewidth = 0.5, fmt = \".0f\", ax = ax,cmap=\"crest\")\nplt.show()","52698f29":"print('Out of 2001 samples the model was right in',TP+TN,'of the samples and wrong in', FP+FN, 'sampels')\n# how often is the classifier correct?\nprint('Classification Accuracy:',(TP + TN) \/ float(TP + TN + FP + FN))\n#how often is the classifier incorrect?\nprint('Classification Error:',(FP + FN) \/ float(TP + TN + FP + FN))","09b90e97":"from sklearn.metrics import  roc_curve\nfrom sklearn.metrics import roc_auc_score\nfalse_positive_rate, true_positiv_rate, _= roc_curve(y_test, y_pred)\nauc= roc_auc_score(y_test, y_pred)\nplt.plot(false_positive_rate, true_positiv_rate, label=\"auc=\"+str(auc))\nplt.box(False)\nplt.title('ROC CURVE')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)\nplt.show()\n \nprint(f\"The score ROC Curve is: {round(auc,3)*100}%\")\n\n","dca6d648":"from sklearn.model_selection import cross_val_score\n# 10-fold cross-validation\nknn = KNeighborsClassifier(n_neighbors=43)\nprint(cross_val_score(knn, x, y, cv=10, scoring='accuracy'))\nprint(cross_val_score(knn, x, y, cv=10, scoring='accuracy').mean())","7f691d78":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nmodels = [LogisticRegression(),RandomForestClassifier(), DecisionTreeClassifier()]\nmodel_names=['LogisticRegression','RandomForestClassifier','DecisionTree']\nacc=[]\ncolors1=[\"mako\",\"rocket\",\"magma\"]\ncolors2=[\"y\",\"b\",\"r\"]\n\nfor model in range(3):\n    clf=models[model]\n    clf.fit(x_train,y_train)\n    pred=clf.predict(x_test)\n    \n    #accuracy of each model\n    acc.append(accuracy_score(pred,y_test))\n    \n    # confusion matrix\n    cm = confusion_matrix(y_test, pred)\n    f ,ax = plt.subplots(figsize = (5,5))\n    sns.heatmap(cm, annot = True, linewidth = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax, cmap=colors1[model])\n    titels=['Logistic Regression','Random Forest','Decision Tree']\n    plt.title(\"Confusion Metrix of \"+titels[model])\n    plt.show()\n    TP = cm[1, 1]\n    TN = cm[0, 0]\n    FP = cm[0, 1]\n    FN = cm[1, 0]\n    print('Out of',TP + TN + FP + FN,'samples the model was right in',TP+TN,'of the samples and wrong in', FP+FN, 'sampels')\n    print('Classification Accuracy:',(TP + TN) \/ float(TP + TN + FP + FN))\n    print('Classification Error:',(FP + FN) \/ float(TP + TN + FP + FN))\n   \n    #ROC\n    false_positive_rate, true_positiv_rate, _= roc_curve(y_test, pred)\n    auc= roc_auc_score(y_test, pred)\n    plt.plot(false_positive_rate, true_positiv_rate, label=\"auc=\"+str(auc), color=colors2[model])\n    plt.box(False)\n    plt.title('ROC Curve of '+titels[model])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.grid(True)\n    plt.show()\n    print(f\"The score ROC Curve is: {round(auc,3)*100}%\")\n\n","de8b31f9":"d={'Modelling':model_names,'Accuracy':acc}    \nacc_frame=pd.DataFrame(d)\nacc_frame\n\n","94d1f01b":"sns.catplot(x='Modelling',y='Accuracy',data=acc_frame,kind='point',height=4,aspect=3.5)","18df20bc":"from sklearn.dummy import DummyClassifier\n  \ndummy_clf = DummyClassifier(strategy=\"uniform\")\ndummy_clf.fit(x_train, y_train)\n\ny_pred_dummy=dummy_clf.predict(x_test)\ndummy_clf.score(y_test, y_pred_dummy)\n","0628c2dd":"cm = confusion_matrix(y_test, y_pred_dummy)\nf ,ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm, annot = True, linewidth = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax, cmap=colors1[model])\nplt.title(\"Confusion Metrix of Dummy Model \")\nplt.show()\nTP = cm[1, 1]\nTN = cm[0, 0]\nFP = cm[0, 1]\nFN = cm[1, 0]\nprint('Out of',TP + TN + FP + FN,'samples the model was right in',TP+TN,'of the samples and wrong in', FP+FN, 'sampels')\nprint('Classification Accuracy:',(TP + TN) \/ float(TP + TN + FP + FN))\nprint('Classification Error:',(FP + FN) \/ float(TP + TN + FP + FN))","f723c580":"false_positive_rate, true_positiv_rate, _= roc_curve(y_test, y_pred_dummy)\nauc_dum= roc_auc_score(y_test, y_pred_dummy)\nplt.plot(false_positive_rate, true_positiv_rate, label=\"auc=\"+str(auc_dum))\nplt.box(False)\nplt.title('ROC CURVE For Dummy')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)\nplt.show()\n \nprint(f\"The score ROC Curve is: {round(auc,3)*100}%\")","b835bffb":"### Confution Metrics","abd3b4a2":" ### Roc Curve","cf1af15f":"* We can see that the heigest accuracy is in the model  Random Forest Classifier with 96%.","e6fef73c":"* <B> we can see that the model 95% accurate. ","fe1d8083":"## Logistic Regassion, Random Forest and Decision Tree","4db81d28":"<B>We can see that the average of all the parameters (accept of hair) in males is higher than the females. lets see it more closely:","25971b3e":"## Preparing The Data:\n#### Changing Gender To Numeric\nmales=0 females=1:","bf691d38":"<B> We will check in range of 100  which accuracy we get with specific k, and get just the accuracy above 0.97","f90868c1":"## Cross Validation","5a6e2a29":"### Hair Length By Gender :","3ee5d5ee":"<b>long hair:<\/b> Boolean features. If the hair is short=0, if it's long=1.\\\n<b>forehead_width_cm:<\/b> The width of the forehead in cm.\\\n<b>forehead_height_cm:<\/b> The hught of the forehead in cm.\\\n<b>nose_wide:<\/b> Boolean features. If the nose is wide=1, if it's thin=0.\\\n<b>nose_long:<\/b> Boolean features. If the nose is short=0, if it's long=1.\\\n<b>lips_thin:<\/b> Boolean features. If the lips are thin =1, if they width=0.\\\n<b>distance_nose_to_lip_long:<\/b> boolean features. If the distance between the nose and the lip is short=0, if it's long=1.\\\n<b>gender:<\/b> Male or female\n\n<B>Please note <\/b>- I did not find according to what the ratio is defined as long and short, and therefore please assume that it was defined after a general average calculation of the relevant parameters in the population. \n\n    \n    ","59c977f1":"## Length And Width Nose By Gender","4b8d8c90":"# Classification Between Males And Females\n<b>  In this dataset, I have chosen to deal with the gender classification of human beings based on facial structure such as nose width, Forehead length, etc.","ec92d213":"### Accuracy:\n\n","18fbf7a0":"* We can see that there is a negative correlation between gender and nose width, nose length, lips width and  distance  from nose to lips.\n* There is a low positive correlation between the parameters of the nose and lips. ","57d82a87":"### The Average Parameters Of Each Gender:","92a047e3":"# Thank you for watching! \ud83d\ude43","5cde2f21":"<B>We can see that there is 2501 sumples of males and 2500 sumples of females.\n\n","e5812028":"* <B> We can see that the best model is KNN  of all the modeles that I checked, with 97%","c389f75b":"## Modeling\n### KNN:","b75579e3":"* <B>we can see that males has more wide and high forehead than females.","44113583":"## Information","0dbeca85":"* <B> we can see that the final accuracy is 97%\n","d2aad916":"### Lip Thickness By Gender","51db752a":"* <B> We can see that there is very small diferences in hair length between males and females","584713a1":"#### Erase The Original Column Of Gender","805568c7":"## Dummy Model:","75935ac0":"* <b>We can see that  most female have wider lips and shorter distance between the nose and lips than the males.","fb38de21":"* <B> We can see that females have thiner and shorter noses than males.","8ad8c9a4":"## Explanation Of Features","05fdd67d":"###  Foreheade Wide And Height By Gender:","3359f95c":"## Gender\nGender is the target that I want to predict. Let's see the relationship between gender and other parameters\n\n* Note that \"gender\" is categorical feature, so we  change it to numerical (after we show the data):\n","f8c8e36b":"* <B> It can be seen that the maximum accuracy is about 0.97 so I checked wich k gives that:\n","1e6ea881":"### Confution Metrics","3e858f9e":"### Roc Curve","136d0ad3":"## Conclusion:\nIn this project I worked on a dataset of facial features and tried to predict gender according to those characteristics.\nDuring the presentation of the data it could be seen that there is a correlation between the forehead and nose fillers and the gender type. No correlation was found between hair length and gender.\nIn the model training it can be seen that the percentage of accuracy in all of them is relatively high - over 95% for all models, and out of them the model KNN showed the highest percentage of accuracy-97%.\n","0dc60812":"<b> There is 5000 raws and 8 columns. \nThere is no null values in this dataFrame<b>\n    \n","477afd07":"* <B>We can see that all this k gives the same accuracy, **so the final accuraccy is 0.9720139930034982, for k=43\/49\/95\/97**","b6c34cdf":"## Correlation:","acd1fa79":"## Train Test Split"}}