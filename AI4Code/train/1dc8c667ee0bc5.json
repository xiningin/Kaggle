{"cell_type":{"78343af6":"code","9a8914f2":"code","d2ed8e89":"code","4c46e5a2":"code","d8ff93e4":"code","0ea00835":"code","654abb7e":"code","ed769537":"code","da451a67":"code","14a68a87":"code","a2c7811c":"code","50272bd9":"code","6ce7ad37":"code","b99e25b0":"code","3dfca6b0":"code","2aae390b":"code","7ad385c9":"code","c892dbd5":"code","491d6ac4":"code","a06b356d":"code","7e7018a7":"code","b5911976":"code","5507487b":"code","bc0fbb01":"code","633a8ff4":"code","808b2473":"code","23c02a44":"code","746450a7":"code","4009dcb4":"code","00a5d552":"code","9f16906d":"code","a76cbbdb":"code","f6f8aee3":"code","9e209458":"code","582c1b9a":"code","b91e9eff":"code","e34242c6":"code","8f894bf3":"code","b6d74bb7":"code","52f100cf":"code","9dc25eee":"code","01b7d168":"code","092511ec":"code","7b6fe771":"code","e61fb3ea":"code","77517050":"code","b0899b2e":"code","6004ff14":"code","71dfc4ea":"code","2e7e2d75":"code","9c98a70f":"code","eedf8c4f":"code","d9646210":"code","e6718f1e":"code","c2a43f90":"code","1a99fe60":"code","354c8a4e":"code","b21cd5d8":"code","70026535":"code","9c30ac54":"code","0774a3d9":"code","f2573576":"code","1426f666":"code","8c48374b":"code","aca84067":"code","1719c7e6":"code","4aa381cd":"code","54e385c1":"code","1c2ff055":"code","010b4e40":"code","4d447cb2":"code","182fdbb6":"code","f3b45e1c":"code","9a596fc9":"code","cbb291e8":"code","f108cd59":"code","719147a8":"code","4e523d43":"code","0add3ea2":"code","7b204c8e":"code","9553f307":"code","4ef85c1a":"code","60e8ab8f":"code","d368380b":"code","2dd318cf":"code","0560be61":"code","b01f2173":"code","606940b6":"code","854fa6d8":"code","c12788f5":"code","25eb2c65":"code","345e3312":"code","60acd27f":"code","29ef6822":"code","ff23c8e7":"code","d20add69":"code","88f16b69":"code","4fd89c3a":"code","0056ab4b":"code","f44fb40d":"code","7bab1978":"code","59a610c6":"code","490ed298":"code","725970a0":"code","29325b3c":"code","80fbb426":"code","dd89ba7c":"code","9fa04bf5":"code","4838ee34":"code","7e81442a":"code","344f177e":"code","bfc9a6e8":"markdown","1ac7ea6b":"markdown","6c4499a8":"markdown","ebfef9d9":"markdown","cab7da41":"markdown","5e078abc":"markdown","69735927":"markdown","a2f368c7":"markdown","b0063c9e":"markdown","1864091a":"markdown","aacfa694":"markdown","9b06c8de":"markdown","c164ab9f":"markdown","44ea960c":"markdown","29cd9e7d":"markdown","373cce77":"markdown","90de894c":"markdown","81191a2d":"markdown","1c8840c7":"markdown","48bf0b4d":"markdown","dd568ef4":"markdown","5ed77dac":"markdown","f78e69f1":"markdown","752df542":"markdown","cc2a2309":"markdown","d5426008":"markdown","23afebb7":"markdown","6bd2d44f":"markdown","24305355":"markdown","9977edb5":"markdown","035cc1af":"markdown"},"source":{"78343af6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom functools import partial\nfrom math import sqrt\nfrom sklearn.metrics import cohen_kappa_score as kappa_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix as sk_cmatrix\nfrom sklearn.metrics import mean_squared_error\nimport scipy as sp\nfrom skimage import feature\nfrom collections import defaultdict\nfrom collections import Counter\nimport operator\nimport cv2\nfrom scipy.stats import itemfreq\nfrom joblib import Parallel, delayed\nimport json\nimport string\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nfrom PIL import Image as IMG\nfrom PIL import Image\nimport glob\nkappa_scorer = None\n\nimport os\nos.environ['USER'] = 'root'\nos.system('pip install ..\/input\/xlearn\/xlearn\/xlearn-0.40a1\/')\n\nimport xlearn as xl\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/petfinder-adoption-prediction\/\"))\nprint(os.listdir(\"..\/input\/petfinder-adoption-prediction\/train\"))\nprint(os.listdir(\"..\/input\/petfinder-adoption-prediction\/test\"))","9a8914f2":"train_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/train\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/test\/test.csv\")","d2ed8e89":"y = train_df[\"AdoptionSpeed\"]","4c46e5a2":"labels_breed = pd.read_csv('..\/input\/petfinder-adoption-prediction\/breed_labels.csv')","d8ff93e4":"# add features from ratings \nwith open('..\/input\/cat-and-dog-breeds-parameters\/rating.json', 'r') as f:\n    ratings = json.load(f)\ncat_ratings = ratings['cat_breeds']\ndog_ratings = ratings['dog_breeds']\nbreed_id = {}","0ea00835":"pet_attributes = set(list(cat_ratings[\"Abyssinian\"].keys()) + list(dog_ratings[\"German Shepherd Dog\"].keys()))","654abb7e":"for i in pet_attributes:\n    train_df[i] = 0\n    test_df[i] = 0","ed769537":"cat_ratings = {k.lower(): v for k, v in cat_ratings.items()}\ndog_ratings = {k.lower(): v for k, v in dog_ratings.items()}","da451a67":"for id,name in zip(labels_breed.BreedID,labels_breed.BreedName):\n    breed_id[id] = str(name).lower()\nbreed_names_1 = [i for i in cat_ratings.keys()]\nbreed_names_2 = [i for i in dog_ratings.keys()]\nfor i, id in enumerate(train_df['Breed1']):\n    if id in breed_id.keys(): \n        name = str(breed_id[id]).lower() \n        if name in breed_names_1:\n            #print(cat_ratings[name])\n            for key in cat_ratings[name].keys():\n                #print(key)\n                train_df.loc[i, key] = cat_ratings[name][key]\n        if name in breed_names_2:\n            #print(dog_ratings[name])\n            for key in dog_ratings[name].keys():\n                \n                train_df.loc[i, key] = dog_ratings[name][key]\n                \n                \n                \nfor i, id in enumerate(test_df['Breed1']):\n    if id in breed_id.keys(): \n        str(breed_id[id]).lower() \n        if name in breed_names_1:\n            #print(cat_ratings[name])\n            for key in cat_ratings[name].keys():\n                #print(key)\n                test_df.loc[i, key] = cat_ratings[name][key]\n        if name in breed_names_2:\n            #print(dog_ratings[name])\n            for key in dog_ratings[name].keys():\n                \n                test_df.loc[i, key] = dog_ratings[name][key]","14a68a87":"train_image_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_images\/*.jpg'))\ntrain_metadata_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_metadata\/*.json'))\ntrain_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/train_sentiment\/*.json'))\n\nprint('num of train images files: {}'.format(len(train_image_files)))\nprint('num of train metadata files: {}'.format(len(train_metadata_files)))\nprint('num of train sentiment files: {}'.format(len(train_sentiment_files)))\n\n\ntest_image_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_images\/*.jpg'))\ntest_metadata_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_metadata\/*.json'))\ntest_sentiment_files = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/test_sentiment\/*.json'))\n\nprint('num of test images files: {}'.format(len(test_image_files)))\nprint('num of test metadata files: {}'.format(len(test_metadata_files)))\nprint('num of test sentiment files: {}'.format(len(test_sentiment_files)))","a2c7811c":"# Images:\ntrain_df_ids = train_df[['PetID']]\nprint(train_df_ids.shape)\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split('\/')[-1].split('-')[0])\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\nprint(len(train_imgs_pets.unique()))\n\npets_with_images = len(np.intersect1d(train_imgs_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images \/ train_df_ids.shape[0]))\n\n# Metadata:\ntrain_df_ids = train_df[['PetID']]\ntrain_df_metadata = pd.DataFrame(train_metadata_files)\ntrain_df_metadata.columns = ['metadata_filename']\ntrain_metadata_pets = train_df_metadata['metadata_filename'].apply(lambda x: x.split('\/')[-1].split('-')[0])\ntrain_df_metadata = train_df_metadata.assign(PetID=train_metadata_pets)\nprint(len(train_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(train_metadata_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas \/ train_df_ids.shape[0]))\n\n# Sentiment:\ntrain_df_ids = train_df[['PetID']]\ntrain_df_sentiment = pd.DataFrame(train_sentiment_files)\ntrain_df_sentiment.columns = ['sentiment_filename']\ntrain_sentiment_pets = train_df_sentiment['sentiment_filename'].apply(lambda x: x.split('\/')[-1].split('.')[0])\ntrain_df_sentiment = train_df_sentiment.assign(PetID=train_sentiment_pets)\nprint(len(train_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(train_sentiment_pets.unique(), train_df_ids['PetID'].unique()))\nprint('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments \/ train_df_ids.shape[0]))","50272bd9":"# Images:\ntest_df_ids = test_df[['PetID']]\nprint(test_df_ids.shape)\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split('\/')[-1].split('-')[0])\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\nprint(len(test_imgs_pets.unique()))\n\npets_with_images = len(np.intersect1d(test_imgs_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with images: {:.3f}'.format(pets_with_images \/ test_df_ids.shape[0]))\n\n\n# Metadata:\ntest_df_ids = test_df[['PetID']]\ntest_df_metadata = pd.DataFrame(test_metadata_files)\ntest_df_metadata.columns = ['metadata_filename']\ntest_metadata_pets = test_df_metadata['metadata_filename'].apply(lambda x: x.split('\/')[-1].split('-')[0])\ntest_df_metadata = test_df_metadata.assign(PetID=test_metadata_pets)\nprint(len(test_metadata_pets.unique()))\n\npets_with_metadatas = len(np.intersect1d(test_metadata_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with metadata: {:.3f}'.format(pets_with_metadatas \/ test_df_ids.shape[0]))\n\n\n\n# Sentiment:\ntest_df_ids = test_df[['PetID']]\ntest_df_sentiment = pd.DataFrame(test_sentiment_files)\ntest_df_sentiment.columns = ['sentiment_filename']\ntest_sentiment_pets = test_df_sentiment['sentiment_filename'].apply(lambda x: x.split('\/')[-1].split('.')[0])\ntest_df_sentiment = test_df_sentiment.assign(PetID=test_sentiment_pets)\nprint(len(test_sentiment_pets.unique()))\n\npets_with_sentiments = len(np.intersect1d(test_sentiment_pets.unique(), test_df_ids['PetID'].unique()))\nprint('fraction of pets with sentiment: {:.3f}'.format(pets_with_sentiments \/ test_df_ids.shape[0]))\n\n\n# are distributions the same?\nprint('images and metadata distributions the same? {}'.format(\n    np.all(test_metadata_pets == test_imgs_pets)))","6ce7ad37":"class PetFinderParser(object):\n    \n    def __init__(self, debug=False):\n        \n        self.debug = debug\n        self.sentence_sep = ' '\n        \n        # Does not have to be extracted because main DF already contains description\n        self.extract_sentiment_text = False\n        \n        \n    def open_metadata_file(self, filename):\n        \"\"\"\n        Load metadata file.\n        \"\"\"\n        with open(filename, 'r', encoding='utf-8') as f:\n            metadata_file = json.load(f)\n        return metadata_file\n            \n    def open_sentiment_file(self, filename):\n        \"\"\"\n        Load sentiment file.\n        \"\"\"\n        with open(filename, 'r', encoding='utf-8') as f:\n            sentiment_file = json.load(f)\n        return sentiment_file\n            \n    def open_image_file(self, filename):\n        \"\"\"\n        Load image file.\n        \"\"\"\n        image = np.asarray(Image.open(filename))\n        return image\n        \n    def parse_sentiment_file(self, file):\n        \"\"\"\n        Parse sentiment file. Output DF with sentiment features.\n        \"\"\"\n        \n        file_sentiment = file['documentSentiment']\n        file_entities = [x['name'] for x in file['entities']]\n        file_entities = self.sentence_sep.join(file_entities)\n\n        if self.extract_sentiment_text:\n            file_sentences_text = [x['text']['content'] for x in file['sentences']]\n            file_sentences_text = self.sentence_sep.join(file_sentences_text)\n        file_sentences_sentiment = [x['sentiment'] for x in file['sentences']]\n        \n        file_sentences_sentiment = pd.DataFrame.from_dict(\n            file_sentences_sentiment, orient='columns').sum()\n        file_sentences_sentiment = file_sentences_sentiment.add_prefix('document_').to_dict()\n        \n        file_sentiment.update(file_sentences_sentiment)\n        \n        df_sentiment = pd.DataFrame.from_dict(file_sentiment, orient='index').T\n        if self.extract_sentiment_text:\n            df_sentiment['text'] = file_sentences_text\n            \n        df_sentiment['entities'] = file_entities\n        df_sentiment = df_sentiment.add_prefix('sentiment_')\n        \n        return df_sentiment\n    \n    def parse_metadata_file(self, file):\n        \"\"\"\n        Parse metadata file. Output DF with metadata features.\n        \"\"\"\n        \n        file_keys = list(file.keys())\n        \n        if 'labelAnnotations' in file_keys:\n            file_annots = file['labelAnnotations'][:int(len(file['labelAnnotations']) * 0.3)]\n            file_top_score = np.asarray([x['score'] for x in file_annots]).mean()\n            file_top_desc = [x['description'] for x in file_annots]\n        else:\n            file_top_score = np.nan\n            file_top_desc = ['']\n        \n        file_colors = file['imagePropertiesAnnotation']['dominantColors']['colors']\n        file_crops = file['cropHintsAnnotation']['cropHints']\n\n        file_color_score = np.asarray([x['score'] for x in file_colors]).mean()\n        file_color_pixelfrac = np.asarray([x['pixelFraction'] for x in file_colors]).mean()\n\n        file_crop_conf = np.asarray([x['confidence'] for x in file_crops]).mean()\n        \n        if 'importanceFraction' in file_crops[0].keys():\n            file_crop_importance = np.asarray([x['importanceFraction'] for x in file_crops]).mean()\n        else:\n            file_crop_importance = np.nan\n\n        df_metadata = {\n            'annots_score': file_top_score,\n            'color_score': file_color_score,\n            'color_pixelfrac': file_color_pixelfrac,\n            'crop_conf': file_crop_conf,\n            'crop_importance': file_crop_importance,\n            'annots_top_desc': self.sentence_sep.join(file_top_desc)\n        }\n        \n        df_metadata = pd.DataFrame.from_dict(df_metadata, orient='index').T\n        df_metadata = df_metadata.add_prefix('metadata_')\n        \n        return df_metadata\n    \n\n# Helper function for parallel data processing:\ndef extract_additional_features(pet_id, mode='train'):\n    \n    sentiment_filename = '..\/input\/petfinder-adoption-prediction\/{}_sentiment\/{}.json'.format(mode, pet_id)\n    try:\n        sentiment_file = pet_parser.open_sentiment_file(sentiment_filename)\n        df_sentiment = pet_parser.parse_sentiment_file(sentiment_file)\n        df_sentiment['PetID'] = pet_id\n    except:\n        df_sentiment = []\n\n    dfs_metadata = []\n    metadata_filenames = sorted(glob.glob('..\/input\/petfinder-adoption-prediction\/{}_metadata\/{}*.json'.format(mode, pet_id)))\n    if len(metadata_filenames) > 0:\n        for f in metadata_filenames:\n            metadata_file = pet_parser.open_metadata_file(f)\n            df_metadata = pet_parser.parse_metadata_file(metadata_file)\n            df_metadata['PetID'] = pet_id\n            dfs_metadata.append(df_metadata)\n        dfs_metadata = pd.concat(dfs_metadata, ignore_index=True, sort=False)\n    dfs = [df_sentiment, dfs_metadata]\n    \n    return dfs\n\n\npet_parser = PetFinderParser()","b99e25b0":"# Unique IDs from train and test:\ndebug = False\ntrain_pet_ids = train_df.PetID.unique()\ntest_pet_ids = test_df.PetID.unique()\n\nif debug:\n    train_pet_ids = train_pet_ids[:1000]\n    test_pet_ids = test_pet_ids[:500]\n\n\n# Train set:\n# Parallel processing of data:\ndfs_train = Parallel(n_jobs=6, verbose=1)(\n    delayed(extract_additional_features)(i, mode='train') for i in train_pet_ids)\n\n# Extract processed data and format them as DFs:\ntrain_dfs_sentiment = [x[0] for x in dfs_train if isinstance(x[0], pd.DataFrame)]\ntrain_dfs_metadata = [x[1] for x in dfs_train if isinstance(x[1], pd.DataFrame)]\n\ntrain_dfs_sentiment = pd.concat(train_dfs_sentiment, ignore_index=True, sort=False)\ntrain_dfs_metadata = pd.concat(train_dfs_metadata, ignore_index=True, sort=False)\n\nprint(train_dfs_sentiment.shape, train_dfs_metadata.shape)\n\n\n# Test set:\n# Parallel processing of data:\ndfs_test = Parallel(n_jobs=6, verbose=1)(\n    delayed(extract_additional_features)(i, mode='test') for i in test_pet_ids)\n\n# Extract processed data and format them as DFs:\ntest_dfs_sentiment = [x[0] for x in dfs_test if isinstance(x[0], pd.DataFrame)]\ntest_dfs_metadata = [x[1] for x in dfs_test if isinstance(x[1], pd.DataFrame)]\n\ntest_dfs_sentiment = pd.concat(test_dfs_sentiment, ignore_index=True, sort=False)\ntest_dfs_metadata = pd.concat(test_dfs_metadata, ignore_index=True, sort=False)\n\nprint(test_dfs_sentiment.shape, test_dfs_metadata.shape)","3dfca6b0":"like most other people we did a bunch of aggregates on various features.","2aae390b":"# Extend aggregates and improve column naming\naggregates = ['mean', 'sum', 'var', 'max', 'min']\n\n# Train\ntrain_metadata_desc = train_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntrain_metadata_desc = train_metadata_desc.reset_index()\ntrain_metadata_desc[\n    'metadata_annots_top_desc'] = train_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntrain_metadata_gr = train_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in train_metadata_gr.columns:\n    if 'PetID' not in i:\n        train_metadata_gr[i] = train_metadata_gr[i].astype(float)\ntrain_metadata_gr = train_metadata_gr.groupby(['PetID']).agg(aggregates)\ntrain_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in train_metadata_gr.columns.tolist()])\ntrain_metadata_gr = train_metadata_gr.reset_index()\n\n\ntrain_sentiment_desc = train_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntrain_sentiment_desc = train_sentiment_desc.reset_index()\ntrain_sentiment_desc[\n    'sentiment_entities'] = train_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntrain_sentiment_gr = train_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in train_sentiment_gr.columns:\n    if 'PetID' not in i:\n        train_sentiment_gr[i] = train_sentiment_gr[i].astype(float)\ntrain_sentiment_gr = train_sentiment_gr.groupby(['PetID']).agg(aggregates)\ntrain_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in train_sentiment_gr.columns.tolist()])\ntrain_sentiment_gr = train_sentiment_gr.reset_index()\n\n\n# Test\ntest_metadata_desc = test_dfs_metadata.groupby(['PetID'])['metadata_annots_top_desc'].unique()\ntest_metadata_desc = test_metadata_desc.reset_index()\ntest_metadata_desc[\n    'metadata_annots_top_desc'] = test_metadata_desc[\n    'metadata_annots_top_desc'].apply(lambda x: ' '.join(x))\n\nprefix = 'metadata'\ntest_metadata_gr = test_dfs_metadata.drop(['metadata_annots_top_desc'], axis=1)\nfor i in test_metadata_gr.columns:\n    if 'PetID' not in i:\n        test_metadata_gr[i] = test_metadata_gr[i].astype(float)\ntest_metadata_gr = test_metadata_gr.groupby(['PetID']).agg(aggregates)\ntest_metadata_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in test_metadata_gr.columns.tolist()])\ntest_metadata_gr = test_metadata_gr.reset_index()\n\n\ntest_sentiment_desc = test_dfs_sentiment.groupby(['PetID'])['sentiment_entities'].unique()\ntest_sentiment_desc = test_sentiment_desc.reset_index()\ntest_sentiment_desc[\n    'sentiment_entities'] = test_sentiment_desc[\n    'sentiment_entities'].apply(lambda x: ' '.join(x))\n\nprefix = 'sentiment'\ntest_sentiment_gr = test_dfs_sentiment.drop(['sentiment_entities'], axis=1)\nfor i in test_sentiment_gr.columns:\n    if 'PetID' not in i:\n        test_sentiment_gr[i] = test_sentiment_gr[i].astype(float)\ntest_sentiment_gr = test_sentiment_gr.groupby(['PetID']).agg(aggregates)\ntest_sentiment_gr.columns = pd.Index(['{}_{}_{}'.format(\n            prefix, c[0], c[1].upper()) for c in test_sentiment_gr.columns.tolist()])\ntest_sentiment_gr = test_sentiment_gr.reset_index()","7ad385c9":"# Train merges:\ntrain_proc = train_df.copy()\ntrain_proc = train_proc.merge(\n    train_sentiment_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_gr, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_metadata_desc, how='left', on='PetID')\ntrain_proc = train_proc.merge(\n    train_sentiment_desc, how='left', on='PetID')\n\n# Test merges:\ntest_proc = test_df.copy()\ntest_proc = test_proc.merge(\n    test_sentiment_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_gr, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_metadata_desc, how='left', on='PetID')\ntest_proc = test_proc.merge(\n    test_sentiment_desc, how='left', on='PetID')\n\n\nprint(train_proc.shape, test_proc.shape)\n# assert train_proc.shape[0] == train_df.shape[0]\n# assert test_proc.shape[0] == test_df.shape[0]","c892dbd5":"X = pd.concat([train_proc, test_proc], ignore_index=True, sort=False, axis = 0)\nprint('NaN structure:\\n{}'.format(np.sum(pd.isnull(X))))","491d6ac4":"X_temp = X.copy()\ntext_columns = ['Description', 'metadata_annots_top_desc', 'sentiment_entities']","a06b356d":"# Count RescuerID occurrences:\nrescuer_count = X.groupby(['RescuerID'])['PetID'].count().reset_index()\nrescuer_count.columns = ['RescuerID', 'RescuerID_COUNT']\n\n# Merge as another feature onto main DF:\nX_temp = X_temp.merge(rescuer_count, how='left', on='RescuerID')\n\n# Count Unique breed occurrences:\nunique_count = X.groupby(['RescuerID'])['Breed1'].nunique().reset_index()\nunique_count.columns = ['RescuerID', 'RescuerID_UNIQUE']\n\n# Merge as another feature onto main DF:\nX_temp = X_temp.merge(unique_count, how='left', on='RescuerID')\n\n\n# Subset text features:\nX_text = X_temp[text_columns]\n\nfor i in X_text.columns:\n    X_text.loc[:, i] = X_text.loc[:, i].fillna('<MISSING>')","7e7018a7":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import SparsePCA, TruncatedSVD, LatentDirichletAllocation, NMF\n\nn_components = 16\ntext_features = []\n\n\n# Generate text features:\nfor i in X_text.columns:\n    \n    # Initialize decomposition methods:\n    print('generating features from: {}'.format(i))\n    svd_ = TruncatedSVD(\n        n_components=n_components, random_state=1337)\n    \n    tfidf_col = TfidfVectorizer().fit_transform(X_text.loc[:, i].values)\n    svd_col = svd_.fit_transform(tfidf_col)\n    svd_col = pd.DataFrame(svd_col)\n    svd_col = svd_col.add_prefix('SVD_{}_'.format(i))\n    text_features.append(svd_col)\n\n    \n# Combine all extracted features:\ntext_features = pd.concat(text_features, axis=1)\n\n# Concatenate with main DF:\nX_temp = pd.concat([X_temp.reset_index(drop = True), text_features], axis=1)\ntext_columns = ['metadata_annots_top_desc', 'sentiment_entities']\n# Remove raw text columns:\nfor i in text_columns:\n    X_temp = X_temp.drop(i, axis=1)","b5911976":"train_df = X_temp.loc[np.isfinite(X_temp.AdoptionSpeed), :]\ntest_df = X_temp.loc[~np.isfinite(X_temp.AdoptionSpeed), :]\n\n# Remove missing target column from test:\ntest_df = test_df.drop(['AdoptionSpeed'], axis=1)","5507487b":"cat_cols = ['Breed1','Breed2', 'Gender', 'Color1', 'Color2', 'Color3', 'State', 'label_description']","bc0fbb01":"num_cols = ['Fee', 'Age', 'MaturitySize', 'FurLength', 'Vaccinated', 'Dewormed',\n 'Sterilized', 'Health', 'Quantity', 'VideoAmt', 'PhotoAmt',\n 'Type', 'doc_sent_mag', 'doc_sent_score', 'vertex_x', 'vertex_y',\n 'bounding_confidence', 'bounding_importance', 'dominant_blue', 'dominant_green',\n 'dominant_red', 'label_score', 'state_gdp', 'state_population', \n 'sentiment_sentiment_magnitude_MEAN', 'sentiment_sentiment_magnitude_SUM',\n 'sentiment_sentiment_score_MEAN', 'sentiment_sentiment_score_SUM', \n 'sentiment_sentiment_document_magnitude_MEAN', 'sentiment_sentiment_document_magnitude_SUM',\n 'sentiment_sentiment_document_score_MEAN', 'sentiment_sentiment_document_score_SUM',\n 'metadata_metadata_annots_score_MEAN', 'metadata_metadata_annots_score_SUM', 'metadata_metadata_color_score_MEAN',\n 'metadata_metadata_color_score_SUM', 'metadata_metadata_color_pixelfrac_MEAN', 'metadata_metadata_color_pixelfrac_SUM',\n 'metadata_metadata_crop_conf_MEAN', 'metadata_metadata_crop_conf_SUM', 'metadata_metadata_crop_importance_MEAN',\n 'metadata_metadata_crop_importance_SUM', 'RescuerID_COUNT', \n 'agg_mean_Age', 'agg_mean_Gender', 'agg_mean_Color1',\n 'agg_mean_Color2', 'agg_mean_Color3', 'agg_mean_FurLength', 'agg_mean_Vaccinated', 'agg_mean_Dewormed',\n 'agg_mean_Sterilized', 'agg_mean_Health', 'agg_mean_Quantity', 'agg_mean_Fee', 'agg_mean_PhotoAmt',\n 'agg_mean_VideoAmt', 'agg_std_Age', 'agg_std_Gender', 'agg_std_Color1', 'agg_std_Color2', \n 'agg_std_Color3', 'agg_std_FurLength', 'agg_std_Vaccinated', 'agg_std_Dewormed', 'agg_std_Sterilized',\n 'agg_std_Health', 'agg_std_Quantity', 'agg_std_Fee', 'agg_std_PhotoAmt', 'agg_std_VideoAmt', 'name_len', \n'average_word_length', 'num_words', 'desc_char_length', 'RescuerID_UNIQUE'] ","633a8ff4":"text_cols = ['Description']","808b2473":"import json\ntrain_id = train_df['PetID']\ntest_id = test_df['PetID']\ndoc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in train_id:\n    try:\n        with open('..\/input\/petfinder-adoption-prediction\/train_sentiment\/' + pet + '.json', 'r', encoding = \"utf-8\") as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntrain_df.loc[:, 'doc_sent_mag'] = doc_sent_mag\ntrain_df.loc[:, 'doc_sent_score'] = doc_sent_score\n\ndoc_sent_mag = []\ndoc_sent_score = []\nnf_count = 0\nfor pet in test_id:\n    try:\n        with open('..\/input\/petfinder-adoption-prediction\/test_sentiment\/' + pet + '.json', 'r', encoding = \"utf-8\") as f:\n            sentiment = json.load(f)\n        doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n        doc_sent_score.append(sentiment['documentSentiment']['score'])\n    except:\n        nf_count += 1\n        doc_sent_mag.append(-1)\n        doc_sent_score.append(-1)\n\ntest_df.loc[:, 'doc_sent_mag'] = doc_sent_mag\ntest_df.loc[:, 'doc_sent_score'] = doc_sent_score\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in train_id:\n    try:\n        with open('..\/input\/petfinder-adoption-prediction\/train_metadata\/' + pet + '-1.json', 'r', encoding = \"utf-8\") as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\nprint(nl_count)\ntrain_df.loc[:, 'vertex_x'] = vertex_xs\ntrain_df.loc[:, 'vertex_y'] = vertex_ys\ntrain_df.loc[:, 'bounding_confidence'] = bounding_confidences\ntrain_df.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntrain_df.loc[:, 'dominant_blue'] = dominant_blues\ntrain_df.loc[:, 'dominant_green'] = dominant_greens\ntrain_df.loc[:, 'dominant_red'] = dominant_reds\ntrain_df.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntrain_df.loc[:, 'dominant_score'] = dominant_scores\ntrain_df.loc[:, 'label_description'] = label_descriptions\ntrain_df.loc[:, 'label_score'] = label_scores\n\n\nvertex_xs = []\nvertex_ys = []\nbounding_confidences = []\nbounding_importance_fracs = []\ndominant_blues = []\ndominant_greens = []\ndominant_reds = []\ndominant_pixel_fracs = []\ndominant_scores = []\nlabel_descriptions = []\nlabel_scores = []\nnf_count = 0\nnl_count = 0\nfor pet in test_id:\n    try:\n        with open('..\/input\/petfinder-adoption-prediction\/test_metadata\/' + pet + '-1.json', 'r', encoding = \"utf-8\") as f:\n            data = json.load(f)\n        vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n        vertex_xs.append(vertex_x)\n        vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n        vertex_ys.append(vertex_y)\n        bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n        bounding_confidences.append(bounding_confidence)\n        bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n        bounding_importance_fracs.append(bounding_importance_frac)\n        dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n        dominant_blues.append(dominant_blue)\n        dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n        dominant_greens.append(dominant_green)\n        dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n        dominant_reds.append(dominant_red)\n        dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n        dominant_pixel_fracs.append(dominant_pixel_frac)\n        dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n        dominant_scores.append(dominant_score)\n        if data.get('labelAnnotations'):\n            label_description = data['labelAnnotations'][0]['description']\n            label_descriptions.append(label_description)\n            label_score = data['labelAnnotations'][0]['score']\n            label_scores.append(label_score)\n        else:\n            nl_count += 1\n            label_descriptions.append('nothing')\n            label_scores.append(-1)\n    except FileNotFoundError:\n        nf_count += 1\n        vertex_xs.append(-1)\n        vertex_ys.append(-1)\n        bounding_confidences.append(-1)\n        bounding_importance_fracs.append(-1)\n        dominant_blues.append(-1)\n        dominant_greens.append(-1)\n        dominant_reds.append(-1)\n        dominant_pixel_fracs.append(-1)\n        dominant_scores.append(-1)\n        label_descriptions.append('nothing')\n        label_scores.append(-1)\n\nprint(nf_count)\ntest_df.loc[:, 'vertex_x'] = vertex_xs\ntest_df.loc[:, 'vertex_y'] = vertex_ys\ntest_df.loc[:, 'bounding_confidence'] = bounding_confidences\ntest_df.loc[:, 'bounding_importance'] = bounding_importance_fracs\ntest_df.loc[:, 'dominant_blue'] = dominant_blues\ntest_df.loc[:, 'dominant_green'] = dominant_greens\ntest_df.loc[:, 'dominant_red'] = dominant_reds\ntest_df.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\ntest_df.loc[:, 'dominant_score'] = dominant_scores\ntest_df.loc[:, 'label_description'] = label_descriptions\ntest_df.loc[:, 'label_score'] = label_scores","23c02a44":"print(train_df.shape, test_df.shape)\nfrom sklearn.preprocessing import LabelEncoder","746450a7":"import tensorflow","4009dcb4":"from tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","00a5d552":"print('getting embeddings')\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in tqdm(open('..\/input\/fasttext-english-word-vectors-including-subwords\/wiki-news-300d-1M-subword.vec', encoding= \"utf-8\")))\n","9f16906d":"num_words = 200000\nmaxlen = 200\nembed_size = 300","a76cbbdb":"test_df = test_df.reset_index(drop = True)","f6f8aee3":"train_df.loc[:, \"Description\"] = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/train\/train.csv\")[\"Description\"]\ntest_df.loc[:, \"Description\"] = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/test\/test.csv\")[\"Description\"]","9e209458":"train_df['Description'] = train_df['Description'].astype(str).fillna('no text')\ntest_df['Description'] = test_df['Description'].astype(str).fillna('no text')\ntrain_df['Description1'] = train_df['Description'].astype(str).fillna('no text')\ntest_df['Description1'] = test_df['Description'].astype(str).fillna('no text')","582c1b9a":"train_df['Description1'] = train_df['Description1'].astype(str).fillna('no text')\ntest_df['Description1'] = test_df['Description1'].astype(str).fillna('no text')","b91e9eff":"print(\"Fitting tokenizer...\")\ntokenizer = Tokenizer(num_words=num_words)\ntokenizer.fit_on_texts(train_df['Description'].values.tolist() + test_df['Description'].values.tolist())","e34242c6":"train_df['Description'] = tokenizer.texts_to_sequences(train_df['Description'])\ntest_df['Description'] = tokenizer.texts_to_sequences(test_df['Description'])","8f894bf3":"\navg_word_length = []\ndesc_length = []\nnumber_words = []\nfor desc in train_df[\"Description1\"]:\n    desc_length.append(len(desc))\n    words = desc.split()\n    number_words.append(len(words))\n    word_len = 0\n    for word in words:\n        word_len += len(word)\n    avg_word_length.append(word_len \/ len(words))\ntrain_df[\"average_word_length\"] = avg_word_length\ntrain_df[\"num_words\"] = number_words\ntrain_df[\"desc_char_length\"] = desc_length","b6d74bb7":"avg_word_length = []\ndesc_length = []\nnumber_words = []\nfor desc in test_df[\"Description1\"]:\n    desc_length.append(len(desc))\n    words = desc.split()\n    number_words.append(len(words))\n    word_len = 0\n    for word in words:\n        word_len += len(word)\n    avg_word_length.append(word_len \/ len(words))\ntest_df[\"average_word_length\"] = avg_word_length\ntest_df[\"num_words\"] = number_words\ntest_df[\"desc_char_length\"] = desc_length","52f100cf":"name_len = []\nfor name in train_df[\"Name\"]:\n    try:\n        name_len.append(len(name))\n    except:\n        name_len.append(-1)\ntrain_df[\"name_len\"] = name_len","9dc25eee":"name_len = []\nfor name in test_df[\"Name\"]:\n    try:\n        name_len.append(len(name))\n    except:\n        name_len.append(-1)\ntest_df[\"name_len\"] = name_len","01b7d168":"state_gdp = {\n    41336: 116.679,\n    41325: 40.596,\n    41367: 23.02,\n    41401: 190.075,\n    41415: 5.984,\n    41324: 37.274,\n    41332: 42.389,\n    41335: 52.452,\n    41330: 67.629,\n    41380: 5.642,\n    41327: 81.284,\n    41345: 80.167,\n    41342: 121.414,\n    41326: 280.698,\n    41361: 32.270\n}\nstate_population = {\n    41336: 33.48283,\n    41325: 19.47651,\n    41367: 15.39601,\n    41401: 16.74621,\n    41415: 0.86908,\n    41324: 8.21110,\n    41332: 10.21064,\n    41335: 15.00817,\n    41330: 23.52743,\n    41380: 2.31541,\n    41327: 15.61383,\n    41345: 32.06742,\n    41342: 24.71140,\n    41326: 54.62141,\n    41361: 10.35977\n}\n\ntrain_df[\"state_gdp\"] = train_df.State.map(state_gdp)\ntrain_df[\"state_population\"] = train_df.State.map(state_population)\ntest_df[\"state_gdp\"] = test_df.State.map(state_gdp)\ntest_df[\"state_population\"] = test_df.State.map(state_population)","092511ec":"word_index = tokenizer.word_index\nnb_words = min(num_words, len(word_index))\nembedding_matrix = np.zeros((nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= nb_words: continue\n    try:\n        embedding_vector = embeddings_index[word]\n    except:\n        embedding_vector = None\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","7b6fe771":"for col in cat_cols:\n    le = LabelEncoder()\n    le.fit(train_df[col].tolist() + test_df[col].tolist())\n    train_df[col] = le.transform(train_df[col].tolist())\n    test_df[col] = le.transform(test_df[col].tolist())\nembed_sizes = [len(set(list(train_df[col].unique()) + list(test_df[col].unique()))) + 1 for col in cat_cols]","e61fb3ea":"agg_list = [\"Age\", \"Gender\", \"FurLength\", \"Vaccinated\", \"Dewormed\", \"Sterilized\",\n \"Health\", \"Quantity\", \"Fee\", \"PhotoAmt\", \"VideoAmt\", \"Color1\", \"Color2\", \"Color3\", \"vertex_y\", \"label_score\"]\ntrain_df[agg_list]","77517050":"for col in agg_list:\n    means = train_df.groupby(\"RescuerID\")[col].mean()\n    train_df[\"agg_mean_\" + col] = train_df[\"RescuerID\"].map(means)\n    means = test_df.groupby(\"RescuerID\")[col].mean()\n    test_df[\"agg_mean_\" + col] = test_df[\"RescuerID\"].map(means)\n    num_cols.append(\"agg_mean_\" + col)\nfor col in agg_list:\n    stds = train_df.groupby(\"RescuerID\")[col].std()\n    stds = stds.fillna(stds.mean())\n    train_df[\"agg_std_\" + col] = train_df[\"RescuerID\"].map(stds)\n    stds = test_df.groupby(\"RescuerID\")[col].std()\n    test_df[\"agg_std_\" + col] = test_df[\"RescuerID\"].map(stds)\n    num_cols.append(\"agg_std_\" + col)","b0899b2e":"from tqdm import tqdm\nfrom copy import deepcopy\ntrain_df_copy = deepcopy(train_df)\ntest_df_copy = deepcopy(test_df)","6004ff14":"# train_df = train_df_copy\n# test_df = test_df_copy","71dfc4ea":"img_size = 256\nbatch_size = 16\npet_ids = train_df['PetID'].values\nn_batches = len(pet_ids) \/\/ batch_size + (len(pet_ids) % batch_size != 0)\nfrom keras.applications.densenet import preprocess_input, DenseNet121\ndef resize_to_square(im):\n    old_size = im.shape[:2] # old_size is in (height, width) format\n    ratio = float(img_size)\/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h\/\/2, delta_h-(delta_h\/\/2)\n    left, right = delta_w\/\/2, delta_w-(delta_w\/\/2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image\n\nfrom keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\ninp = Input((256,256,3))\nbackbone = DenseNet121(input_tensor = inp, include_top = False, weights = None)\nbackbone.load_weights(\"..\/input\/densenet-keras\/DenseNet-BC-121-32-no-top.h5\")\nprint(\"weights loaded succesfully\")\nx = backbone.output\nx = GlobalAveragePooling2D()(x)\nx = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n# x = AveragePooling1D(4)(x)\nout = Lambda(lambda x: x[:,:,0])(x)\n\nm = Model(inp,out)\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/train_images\/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\ntrain_feats = pd.DataFrame.from_dict(features, orient='index').values\nn_components = 32\nsvd = TruncatedSVD(n_components=n_components)\nsvd.fit(train_feats)\ntrain_feats = svd.transform(train_feats)\ntrain_feats = pd.DataFrame(train_feats, columns=['img_svd1_{}'.format(i) for i in range(n_components)])\ntrain_feats.to_csv('train_img_features.csv')\ntrain_df = pd.concat([train_df, train_feats], axis = 1)\npet_ids = test_df['PetID'].values\nn_batches = len(pet_ids) \/\/ batch_size + (len(pet_ids) % batch_size != 0)\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/test_images\/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\ntest_feats = pd.DataFrame.from_dict(features, orient='index').values\ntest_feats = svd.transform(test_feats)\ntest_feats = pd.DataFrame(test_feats, columns=['img_svd1_{}'.format(i) for i in range(n_components)])\ntest_feats.to_csv('test_img_features.csv')\ntest_df = pd.concat((test_df.reset_index(drop = True), test_feats), axis=1)\nprint(train_df.shape, test_df.shape)","2e7e2d75":"print(test_df.shape, test_feats.shape)","9c98a70f":"from keras.models import load_model\ncatvsdog = load_model(\"..\/input\/kerascatvsdog\/best.hd5\")\nimg_size = 299\nbatch_size = 16\npet_ids = train_df['PetID'].values\nn_batches = len(pet_ids) \/\/ batch_size + (len(pet_ids) % batch_size != 0)\nfrom keras.applications.densenet import preprocess_input, DenseNet121\ndef resize_to_square(im):\n    old_size = im.shape[:2] # old_size is in (height, width) format\n    ratio = float(img_size)\/max(old_size)\n    new_size = tuple([int(x*ratio) for x in old_size])\n    # new_size should be in (width, height) format\n    im = cv2.resize(im, (new_size[1], new_size[0]))\n    delta_w = img_size - new_size[1]\n    delta_h = img_size - new_size[0]\n    top, bottom = delta_h\/\/2, delta_h-(delta_h\/\/2)\n    left, right = delta_w\/\/2, delta_w-(delta_w\/\/2)\n    color = [0, 0, 0]\n    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n    return new_im\n\ndef load_image(path, pet_id):\n    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n    new_image = resize_to_square(image)\n    new_image = preprocess_input(new_image)\n    return new_image\n\n","eedf8c4f":"from keras.models import Model\nfrom keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\nimport keras.backend as K\nbackbone = catvsdog\nout = backbone.layers[-2].output\n\n\nm = Model(backbone.input, out)\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/train_images\/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\ntrain_feats = pd.DataFrame.from_dict(features, orient='index').values\nn_components = 32\nsvd = TruncatedSVD(n_components=n_components)\nsvd.fit(train_feats)\ntrain_feats = svd.transform(train_feats)\ntrain_feats = pd.DataFrame(train_feats, columns=['img_svd2_{}'.format(i) for i in range(n_components)])\ntrain_feats.to_csv('train_img_features.csv')\ntrain_df = pd.concat([train_df, train_feats], axis = 1)\npet_ids = test_df['PetID'].values\nn_batches = len(pet_ids) \/\/ batch_size + 1\nfeatures = {}\nfor b in tqdm(range(n_batches)):\n    start = b*batch_size\n    end = (b+1)*batch_size\n    batch_pets = pet_ids[start:end]\n    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n    for i,pet_id in enumerate(batch_pets):\n        try:\n            batch_images[i] = load_image(\"..\/input\/petfinder-adoption-prediction\/test_images\/\", pet_id)\n        except:\n            pass\n    batch_preds = m.predict(batch_images)\n    for i,pet_id in enumerate(batch_pets):\n        features[pet_id] = batch_preds[i]\ntest_feats = pd.DataFrame.from_dict(features, orient='index').values\ntest_feats = svd.transform(test_feats)\ntest_feats = pd.DataFrame(test_feats, columns=['img_svd2_{}'.format(i) for i in range(n_components)])\ntest_feats.to_csv('test_img_features.csv')\ntest_df = pd.concat((test_df.reset_index(drop = True), test_feats), axis=1)\nprint(train_df.shape, test_df.shape)","d9646210":"n_components = 32\ntrain_desc = train_df.Description1.fillna(\"none\").values\ntest_desc = test_df.Description1.fillna(\"none\").values\nprint(test_desc.shape)\ntfv = TfidfVectorizer(min_df=50,  max_features=None,\n        strip_accents='unicode', analyzer='word', token_pattern=r'(?u)\\b\\w+\\b',\n        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n        )\n    \n# Fit TFIDF\ntfv.fit(list(train_desc))\nX =  tfv.transform(train_desc)\nX_test = tfv.transform(test_desc)\n\nsvd = TruncatedSVD(n_components=n_components)\nsvd.fit(X)\nX = svd.transform(X)\nX = pd.DataFrame(X, columns=['svd1_{}'.format(i) for i in range(n_components)])\ntrain_df = pd.concat((train_df, X), axis=1)\nX_test = svd.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=['svd1_{}'.format(i) for i in range(n_components)])\ntest_df = pd.concat((test_df.reset_index(drop = True), X_test), axis=1)\nprint(test_df.shape)","e6718f1e":"#helper for resetting\n# train_df = train_df.drop(['svd2_{}'.format(i) for i in range(n_components)], axis = 1)\n# test_df = test_df.drop(['svd2_{}'.format(i) for i in range(n_components)], axis = 1)","c2a43f90":"from PIL import Image\nsplit_char = '\/'\ntrain_df_ids = train_df[['PetID']]\ntest_df_ids = test_df[['PetID']]\n\ntrain_df_imgs = pd.DataFrame(train_image_files)\ntrain_df_imgs.columns = ['image_filename']\ntrain_imgs_pets = train_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntest_df_imgs = pd.DataFrame(test_image_files)\ntest_df_imgs.columns = ['image_filename']\ntest_imgs_pets = test_df_imgs['image_filename'].apply(lambda x: x.split(split_char)[-1].split('-')[0])\n\ntrain_df_imgs = train_df_imgs.assign(PetID=train_imgs_pets)\ntest_df_imgs = test_df_imgs.assign(PetID=test_imgs_pets)\n\ndef getSize(filename):\n    st = os.stat(filename)\n    return st.st_size\n\ndef getDimensions(filename):\n    img_size = Image.open(filename).size\n    return img_size \n\ntrain_df_imgs['image_size'] = train_df_imgs['image_filename'].apply(getSize)\ntrain_df_imgs['temp_size'] = train_df_imgs['image_filename'].apply(getDimensions)\ntrain_df_imgs['width'] = train_df_imgs['temp_size'].apply(lambda x : x[0])\ntrain_df_imgs['height'] = train_df_imgs['temp_size'].apply(lambda x : x[1])\ntrain_df_imgs = train_df_imgs.drop(['temp_size'], axis=1)\n\ntest_df_imgs['image_size'] = test_df_imgs['image_filename'].apply(getSize)\ntest_df_imgs['temp_size'] = test_df_imgs['image_filename'].apply(getDimensions)\ntest_df_imgs['width'] = test_df_imgs['temp_size'].apply(lambda x : x[0])\ntest_df_imgs['height'] = test_df_imgs['temp_size'].apply(lambda x : x[1])\ntest_df_imgs = test_df_imgs.drop(['temp_size'], axis=1)\n\naggs = {\n    'image_size': ['sum', 'mean', 'var'],\n    'width': ['sum', 'mean', 'var'],\n    'height': ['sum', 'mean', 'var'],\n}\n\nagg_train_imgs = train_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_train_imgs.columns = new_columns\nagg_train_imgs = agg_train_imgs.reset_index()\n\nagg_test_imgs = test_df_imgs.groupby('PetID').agg(aggs)\nnew_columns = [\n    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n]\nagg_test_imgs.columns = new_columns\nagg_test_imgs = agg_test_imgs.reset_index()\n\ntrain_df = train_df.merge(agg_train_imgs, how='left', on='PetID')\ntest_df = test_df.merge(agg_test_imgs, how='left', on='PetID')","1a99fe60":"for col in agg_train_imgs.columns[1:]:\n    num_cols.append(col)","354c8a4e":"#num_cols = [x for x in keep_features if x not in cat_cols]\nprint('scaling num_cols')\nfor col in num_cols:\n#     if col in no_scale_cols:\n#         continue\n    print('scaling {}'.format(col))\n    try:\n        col_mean = train_df[col].mean()\n        train_df[col].fillna(col_mean, inplace=True)\n        test_df[col].fillna(col_mean, inplace=True)\n        scaler = StandardScaler()\n        train_df[col] = scaler.fit_transform(train_df[col].values.reshape(-1, 1))\n        test_df[col] = scaler.transform(test_df[col].values.reshape(-1, 1))\n    except:\n        print(\"*********\")","b21cd5d8":"\n\nfrom keras import backend as K\ndef rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\ndef get_input_features(df):\n    nn_nums= [x for x in list(num_cols) #if x not in list(no_scale_cols)\n             ]\n    X = {'description':pad_sequences(df['Description'], maxlen=maxlen)}\n    X['numerical'] = np.array(df[nn_nums])\n    X['bow_inputs'] = np.array(df[list([\"svd1_\" + str(i) for i in range(n_components)] + \n                                       [\"SVD_metadata_annots_top_desc_\" + str(i) for i in range(16)] +\n                                       [\"SVD_sentiment_entities_\" + str(i) for i in range(16)]\n                                     )])\n    X['img_inputs'] = np.array(df[list([\"img_svd1_\" + str(i) for i in range(n_components)] + [\"img_svd2_\" + str(i) for i in range(n_components)]\n                                     )])\n    for cat in cat_cols:\n        X[cat] = np.array(df[cat])\n    return X","70026535":"import tensorflow as tf\n\ndef kappa_loss(y_pred, y_true, y_pow=2, eps=1e-10, N=5, bsize=256, name='kappa'):\n    \"\"\"A continuous differentiable approximation of discrete kappa loss.\n        Args:\n            y_pred: 2D tensor or array, [batch_size, num_classes]\n            y_true: 2D tensor or array,[batch_size, num_classes]\n            y_pow: int,  e.g. y_pow=2\n            N: typically num_classes of the model\n            bsize: batch_size of the training or validation ops\n            eps: a float, prevents divide by zero\n            name: Optional scope\/name for op_scope.\n        Returns:\n            A tensor with the kappa loss.\"\"\"\n\n    with tf.name_scope(name):\n        y_true = tf.to_float(y_true)\n        repeat_op = tf.to_float(tf.tile(tf.reshape(tf.range(0, N), [N, 1]), [1, N]))\n        repeat_op_sq = tf.square((repeat_op - tf.transpose(repeat_op)))\n        weights = repeat_op_sq \/ tf.to_float((N - 1) ** 2)\n    \n        pred_ = y_pred ** y_pow\n        try:\n            pred_norm = pred_ \/ (eps + tf.reshape(tf.reduce_sum(pred_, 1), [-1, 1]))\n        except Exception:\n            pred_norm = pred_ \/ (eps + tf.reshape(tf.reduce_sum(pred_, 1), [bsize, 1]))\n    \n        hist_rater_a = tf.reduce_sum(pred_norm, 0)\n        hist_rater_b = tf.reduce_sum(y_true, 0)\n    \n        conf_mat = tf.matmul(tf.transpose(pred_norm), y_true)\n    \n        nom = tf.reduce_sum(weights * conf_mat)\n        denom = tf.reduce_sum(weights * tf.matmul(\n            tf.reshape(hist_rater_a, [N, 1]), tf.reshape(hist_rater_b, [1, N])) \/\n                              tf.to_float(bsize))\n    \n        return nom \/ (denom + eps)","9c30ac54":"from keras.layers import Input, Embedding, Concatenate, Flatten, Dense, Dropout, BatchNormalization, CuDNNLSTM, SpatialDropout1D\nfrom keras.layers import Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D, Conv1D, MaxPool1D, concatenate\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import Model\nfrom keras.optimizers import  Adam\nimport keras.backend as k\ndef make_model(softmax = False):\n    k.clear_session()\n\n    categorical_inputs = []\n    for cat in cat_cols:\n        categorical_inputs.append(Input(shape=[1], name=cat))\n\n    categorical_embeddings = []\n    for i, cat in enumerate(cat_cols):\n        categorical_embeddings.append(\n            Embedding(embed_sizes[i], 10, name = cat + \"_embed\")(categorical_inputs[i]))\n\n    categorical_logits = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\n    categorical_logits = Dropout(.5)(categorical_logits)\n    categorical_logits = Dense(50, activation = 'relu')(categorical_logits)\n\n    numerical_inputs = Input(shape=[X_train[\"numerical\"].shape[1]], name = 'numerical')\n    numerical_logits = Dropout(.2)(numerical_inputs)\n    numerical_logits = Dense(50, activation = 'relu')(numerical_logits)\n    numerical_logits = Dense(50, activation = 'relu')(numerical_logits)\n    \n    img_inputs = Input(shape = [n_components * 2], name = \"img_inputs\")\n    img_logits = Dropout(.2)(img_inputs)\n    \n    bow_inputs = Input(shape = [n_components * 2], name = \"bow_inputs\")\n    bow_logits = Dropout(.2)(bow_inputs)\n    \n\n    text_inp = Input(shape=[maxlen], name='description')\n    text_embed = Embedding(nb_words, embed_size, weights=[embedding_matrix], trainable=False)(text_inp)\n    emb_desc = SpatialDropout1D(.6)(text_embed)\n    filter_sizes=[1,2,3,4]\n    convs = []\n    for filter_size in filter_sizes:\n        conv = Conv1D(8, kernel_size=(filter_size), \n                        kernel_initializer=\"normal\", activation=\"relu\")(emb_desc)\n        convs.append(MaxPool1D(pool_size=(maxlen-filter_size+1))(conv))\n    text_logits = concatenate(convs)\n    avg_pool = GlobalAveragePooling1D()(text_logits)\n    max_pool = GlobalMaxPooling1D()(text_logits)\n    text_logits = Concatenate()([avg_pool, max_pool])     \n        \n\n    x = Concatenate()([\n        bow_logits,\n        categorical_logits, \n        text_logits, \n        numerical_logits,\n        img_logits\n    ])\n    x = Dense(300, activation = 'relu')(x)\n    x = Dropout(.2)(x)\n    x = Dense(200, activation = 'relu')(x)\n    x = Dropout(.2)(x)\n    x = Dense(100, activation = 'relu')(x)\n    \n    if softmax == True:\n        out = Dense(5, activation = 'softmax')(x)\n    else:\n        out = Dense(1, activation = 'sigmoid')(x)\n    \n\n    model = Model(inputs=[text_inp] + categorical_inputs + [numerical_inputs] + [bow_inputs] + [img_inputs],outputs=out)\n    if softmax == True:\n        loss = kappa_loss\n    else:\n        loss = root_mean_squared_error\n    model.compile(optimizer=Adam(lr = 0.0005), loss = loss)\n    return model\n","0774a3d9":"# The following 3 functions have been taken from Ben Hamner's github repository\n# https:\/\/github.com\/benhamner\/Metrics\ndef confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the confusion matrix between rater's ratings\n    \"\"\"\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(rater_a + rater_b)\n    if max_rating is None:\n        max_rating = max(rater_a + rater_b)\n    num_ratings = int(max_rating - min_rating + 1)\n    conf_mat = [[0 for i in range(num_ratings)]\n                for j in range(num_ratings)]\n    for a, b in zip(rater_a, rater_b):\n        conf_mat[a - min_rating][b - min_rating] += 1\n    return conf_mat\n\n\ndef histogram(ratings, min_rating=None, max_rating=None):\n    \"\"\"\n    Returns the counts of each type of rating that a rater made\n    \"\"\"\n    if min_rating is None:\n        min_rating = min(ratings)\n    if max_rating is None:\n        max_rating = max(ratings)\n    num_ratings = int(max_rating - min_rating + 1)\n    hist_ratings = [0 for x in range(num_ratings)]\n    for r in ratings:\n        hist_ratings[r - min_rating] += 1\n    return hist_ratings\n\n\ndef quadratic_weighted_kappa(y, y_pred):\n    \"\"\"\n    Calculates the quadratic weighted kappa\n    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n    value, which is a measure of inter-rater agreement between two raters\n    that provide discrete numeric ratings.  Potential values range from -1\n    (representing complete disagreement) to 1 (representing complete\n    agreement).  A kappa value of 0 is expected if all agreement is due to\n    chance.\n    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n    each correspond to a list of integer ratings.  These lists must have the\n    same length.\n    The ratings should be integers, and it is assumed that they contain\n    the complete range of possible ratings.\n    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n    is the minimum possible rating, and max_rating is the maximum possible\n    rating\n    \"\"\"\n    rater_a = y\n    rater_b = y_pred\n    min_rating=None\n    max_rating=None\n    rater_a = np.array(rater_a, dtype=int)\n    rater_b = np.array(rater_b, dtype=int)\n    assert(len(rater_a) == len(rater_b))\n    if min_rating is None:\n        min_rating = min(min(rater_a), min(rater_b))\n    if max_rating is None:\n        max_rating = max(max(rater_a), max(rater_b))\n    conf_mat = confusion_matrix(rater_a, rater_b,\n                                min_rating, max_rating)\n    num_ratings = len(conf_mat)\n    num_scored_items = float(len(rater_a))\n\n    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n\n    numerator = 0.0\n    denominator = 0.0\n\n    for i in range(num_ratings):\n        for j in range(num_ratings):\n            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n                              \/ num_scored_items)\n            d = pow(i - j, 2.0) \/ pow(num_ratings - 1, 2.0)\n            numerator += d * conf_mat[i][j] \/ num_scored_items\n            denominator += d * expected_count \/ num_scored_items\n\n    return (1.0 - numerator \/ denominator)","f2573576":"# put numerical value to one of bins\ndef to_bins(x, borders):\n    for i in range(len(borders)):\n        if x <= borders[i]:\n            return i\n    return len(borders)\n\nclass OptimizedRounder(object):\n    def __init__(self):\n        self.coef_ = 0\n\n    def _loss(self, coef, X, y, idx):\n        X_p = np.array([to_bins(pred, coef) for pred in X])\n        ll = -quadratic_weighted_kappa(y, X_p)\n        return ll\n\n    def fit(self, X, y):\n        coef = [1.5, 2.0, 2.5, 3.0]\n        golden1 = 0.618\n        golden2 = 1 - golden1\n        ab_start = [(1, 2), (1.5, 2.5), (2, 3), (2.5, 3.5)]\n        for it1 in range(10):\n            for idx in range(4):\n                # golden section search\n                a, b = ab_start[idx]\n                # calc losses\n                coef[idx] = a\n                la = self._loss(coef, X, y, idx)\n                coef[idx] = b\n                lb = self._loss(coef, X, y, idx)\n                for it in range(20):\n                    # choose value\n                    if la > lb:\n                        a = b - (b - a) * golden1\n                        coef[idx] = a\n                        la = self._loss(coef, X, y, idx)\n                    else:\n                        b = b - (b - a) * golden2\n                        coef[idx] = b\n                        lb = self._loss(coef, X, y, idx)\n        self.coef_ = {'x': coef}\n\n    def predict(self, X, coef):\n        X_p = np.array([to_bins(pred, coef) for pred in X])\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","1426f666":"rescuerID = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/train\/train.csv\")[\"RescuerID\"]","8c48374b":"def root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))","aca84067":"num_folds = 10\nsoftmax = False\nkf = GroupKFold(n_splits=num_folds)\ny = train_df[\"AdoptionSpeed\"]\nfold_splits = kf.split(train_df, y, rescuerID)\noof = np.zeros((y.shape))\ntest_preds = np.zeros((test_df.shape[0]))\nif softmax == True:\n    test_preds = np.zeros((test_df.shape[0], 5))\n    oof = np.zeros(shape = (y.shape[0], 5))\nfor i, (dev_index, val_index) in enumerate(fold_splits):\n    tr_df = train_df.iloc[dev_index, :]\n    val_df = train_df.iloc[val_index, :]\n    X_train = get_input_features(tr_df)\n    X_valid = get_input_features(val_df)\n    X_test = get_input_features(test_df)\n    model = make_model(softmax = softmax)\n    ckpt = ModelCheckpoint(\"model\" + str(i) + \".h5\", monitor='val_loss', save_best_only = True, verbose = False)\n    rlr = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=5, mode='auto', verbose = False)\n    if softmax == True:\n        y_train = np.zeros(shape = (tr_df.shape[0], 5))\n        y_valid = np.zeros(shape = (val_df.shape[0], 5))\n        for j, l in enumerate(tr_df['AdoptionSpeed'].values):\n            j = int(j)\n            l = int(l)\n            y_train[j,l] = 1\n        for j, l in enumerate(val_df['AdoptionSpeed'].values):\n            j = int(j)\n            l = int(l)\n            y_valid[j,l] = 1\n    else:\n        y_train = tr_df['AdoptionSpeed'].values \/ 4\n        y_valid = val_df['AdoptionSpeed'].values \/ 4\n\n    \n\n    hist = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 500, epochs = 35, verbose = 2, callbacks = [ckpt, rlr])\n    model.load_weights(\"model\" + str(i) + \".h5\")\n    if softmax == True:\n        val_preds = model.predict(X_valid)\n        oof[val_index] = val_preds\n        test_pred = model.predict(X_test)\n        test_preds += (test_pred\/num_folds)\n    else:\n        val_preds = model.predict(X_valid)[:, 0] * 4\n        y_valid = y_valid * 4\n        optR = OptimizedRounder()\n        optR.fit(val_preds, y_valid)\n        coefficients = optR.coefficients()\n        val_pred_rounded = optR.predict(val_preds, coefficients)\n        oof[val_index] = val_preds\n        test_pred = model.predict(X_test)[:, 0] * 4\n        test_preds += (test_pred\/num_folds)\nif softmax == True:\n    oof_rounded = np.argmax(oof, axis = 1)\n    test_rounded = np.argmax(test_preds, axis = 1)\nelse:\n    optR = OptimizedRounder()\n    optR.fit(oof, y)\n    coefficients = optR.coefficients()\n    oof_rounded = optR.predict(oof, coefficients)\n    test_rounded = optR.predict(test_preds, coefficients)","1719c7e6":"print(num_cols)\nprint(quadratic_weighted_kappa(y, oof_rounded))\noof_nn = oof\ntest_preds_nn = test_preds\n","4aa381cd":"from copy import deepcopy\nX_temp = deepcopy(pd.concat([train_df, test_df], axis = 0))","54e385c1":"from sklearn.preprocessing import KBinsDiscretizer\nn_bins = 5\nfor c in num_cols:\n    if X_temp[c].nunique() < 5:\n        continue\n    vals = X_temp[c].values.reshape(-1, 1)\n    est = KBinsDiscretizer(n_bins=n_bins, encode='ordinal')\n    est.fit(vals)\n    vals = est.transform(vals).reshape(1, -1)[0]\n    X_temp[c] = vals","1c2ff055":"train_df_ffm = X_temp.iloc[:train_df.shape[0], :]\ntest_df_ffm = X_temp.iloc[train_df.shape[0]:, :]","010b4e40":"field_features = defaultdict()\nglobal max_val\nmax_val = 1\nglobal max_val\nline = []\nlabel = []\ndtypes = {}\nfor feature in num_cols:\n#     if \"agg_std\" in feature:\n#         #print(feature)\n#         continue\n#     if \"RescuerID_UNIQUE\" in feature:\n#         print(feature)\n#         continue\n    dtypes[feature] = \"float16\"\nfor feature in cat_cols:\n    dtypes[feature] = \"category\"\n    \nfrom collections import defaultdict\nimport math\n\ndont_use = ['RescuerID', 'AdoptionSpeed', 'Description', 'PetID']\ntoo_many_vals = [\"\"]\n\ncategories = [k for k, v in dtypes.items() if k not in dont_use]\ncategories_index = dict(zip(categories, range(len(categories))))\n\ndef generate_ffm_file(index, train = True):\n    global max_val\n    ffeatures = []\n    if train == True:\n        path = 'train.libffm'\n    else:\n        path = 'valid.libffm'\n    with open(path, 'w') as the_file:\n        for t, (index, row) in enumerate(train_df_ffm.iloc[index, :].iterrows()):\n            if t % 5000 == 0:\n                print(t, len(field_features), max_val)\n                #print(line)\n            label = [str(int(row['AdoptionSpeed']))]\n            ffeatures = []\n\n            for field in categories:\n                if field == 'AdoptionSpeed':\n                    continue\n                feature = row[field]\n                if feature == '':\n                    feature = \"unk\"\n                if field not in num_cols:\n                    ff = field + '_____' + str(feature)\n                else:\n                    if feature == \"unk\" or float(feature) == -1:\n                        ff = field + '_____' + str(0)\n                    else:\n                        if field in too_many_vals:\n                            ff = field + '_____' + str(int(round(math.log(1 + float(feature)))))\n                        else:\n                            ff = field + '_____' + str(feature)\n                if ff not in field_features:\n                    if len(field_features) == 0:\n                        field_features[ff] = 1\n                        max_val += 1\n                    else:\n                        field_features[ff] = max_val + 1\n                        max_val += 1\n\n                fnum = field_features[ff]\n                ffeatures.append('{}:{}:1'.format(categories_index[field], str(fnum)))\n\n            line = label + ffeatures\n            the_file.write('{}\\n'.format(' '.join(line)))","4d447cb2":"with open('test.libffm', 'w') as the_file:\n    global max_val\n    ffeatures = []\n    for t, (index, row) in tqdm(enumerate(test_df_ffm.iterrows())):\n        if t % 3000 == 0:\n            print(t, len(field_features), max_val)\n            #print(line)\n        label = [str(int(0))]\n        ffeatures = []\n\n        for field in categories:\n            if field == 'AdoptionSpeed':\n                continue\n            feature = row[field]\n            if feature == '':\n                feature = \"unk\"\n            if field not in num_cols:\n                ff = field + '_____' + str(feature)\n            else:\n                if feature == \"unk\" or float(feature) == -1:\n                    ff = field + '_____' + str(0)\n                else:\n                    if field in too_many_vals:\n                        ff = field + '_____' + str(int(round(math.log(1 + float(feature)))))\n                    else:\n                        ff = field + '_____' + str(feature)\n            if ff not in field_features:\n                if len(field_features) == 0:\n                    field_features[ff] = 1\n                    max_val += 1\n                else:\n                    field_features[ff] = max_val + 1\n                    max_val += 1\n\n            fnum = field_features[ff]\n            ffeatures.append('{}:{}:1'.format(categories_index[field], str(fnum)))\n\n        line = label + ffeatures\n        the_file.write('{}\\n'.format(' '.join(line)))","182fdbb6":"n_splits = 20\nkfold = GroupKFold(n_splits=n_splits)\nffm_oof_train = np.zeros((train_df_ffm.shape[0]))\nffm_oof_test = np.zeros((test_df_ffm.shape[0], n_splits))\n\ni = 0\nfor train_index, valid_index in kfold.split(train_df_ffm, y.values, rescuerID):\n    print('\\nsplit ', i)\n    generate_ffm_file(train_index)\n    generate_ffm_file(valid_index, train = False)\n    # create ffm model\n    ffm_model = xl.create_ffm()\n    # set training\n    ffm_model.setTrain(\"train.libffm\")\n    ffm_model.setValidate(\"valid.libffm\")\n    # define params\n    param = {'task':'reg', 'lr':.2,\n             'lambda':0.0002, 'metric':'rmse', 'epoch' : 100, 'k': 4}\n    # train the model\n    ffm_model.fit(param, \".\/model.out\")\n    # # set the valid data\n    ffm_model.setTest(\"valid.libffm\")\n\n    ffm_model.predict(\".\/model.out\", \"output_valid.txt\")\n    valid_ffm = pd.read_csv('output_valid.txt', header=None)[0].values\n    optR = OptimizedRounder()\n    optR.fit(valid_ffm, y[valid_index])\n    coefficients = optR.coefficients()\n    valid_ffm_rounded = optR.predict(valid_ffm, coefficients)\n    print(quadratic_weighted_kappa(valid_ffm_rounded, y[valid_index]))\n    ffm_oof_train[valid_index] = valid_ffm\n    \n    ffm_model.setTest(\"test.libffm\")\n    ffm_model.predict(\".\/model.out\", \"output_test.txt\")\n    test_ffm = pd.read_csv('output_test.txt', header=None)[0].values\n    ffm_oof_test[:, i] = test_ffm\n    print(test_ffm)\n    i += 1\n    \nffm_oof_test = ffm_oof_test.mean(axis=1)\n","f3b45e1c":"print(ffm_oof_test.mean())\nprint(ffm_oof_train.mean())","9a596fc9":"optR = OptimizedRounder()\noptR.fit(ffm_oof_train, y)\ncoefficients = optR.coefficients()\ntrain_ffm_rounded = optR.predict(ffm_oof_train, coefficients)\nprint(quadratic_weighted_kappa(train_ffm_rounded, y))","cbb291e8":"# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n# from sklearn.preprocessing import LabelEncoder\n\ndef rmse(actual, predicted):\n    return sqrt(mean_squared_error(actual, predicted))\n","f108cd59":"from copy import deepcopy\ntrain = deepcopy(train_df)\ntest = deepcopy(test_df)","719147a8":"train.loc[:, cat_cols] = train[cat_cols].astype('category')\ntest.loc[:, cat_cols] = test[cat_cols].astype('category')\nprint(train.shape)\nprint(test.shape)\ntrain.head()","4e523d43":"train.head()","0add3ea2":"# num_cols = num_cols1\n# bad_cols = [\n#            'Color3',\n#            'Vaccinated',\n#            'Dewormed',\n#            'num_words_upper',\n#            'Health',\n#            'bounding_importance',\n#            'Type',\n#            'VideoAmt',\n#            'bounding_confidence']\n# num_cols = [col for col in num_cols if col not in bad_cols]","7b204c8e":"train.drop(set(['Name', 'RescuerID', 'Description', 'Description1', 'PetID', 'AdoptionSpeed']), axis=1, inplace=True)\ntest.drop(set(['Name', 'RescuerID', 'Description', 'Description1', 'PetID']), axis=1, inplace=True)","9553f307":"# train.drop(list(bad_cols), axis=1, inplace=True)\n# test.drop(list(bad_cols), axis=1, inplace=True)","4ef85c1a":"# train.to_csv(\"train_lgb.csv\")\n# test.to_csv(\"test_lgb.csv\")","60e8ab8f":"# train[\"null_importance\"] = np.random.normal(size = train.shape[0])\n# test[\"null_importance\"] = np.random.normal(size = test.shape[0])","d368380b":"def run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model', num_iters = 1, num_folds = 19, ):\n    cv_scores = []\n    qwk_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0], num_folds))\n    all_coefficients = np.zeros((num_folds * num_iters, 4))\n    feature_importance_df = pd.DataFrame()\n    i = 1\n    for j in range(num_iters):\n        kf = GroupKFold(n_splits=num_folds)\n        fold_splits = kf.split(train, target, rescuerID)\n\n        for dev_index, val_index in fold_splits:\n            print('Started ' + label + ' fold ' + str(i) + '\/5')\n            if isinstance(train, pd.DataFrame):\n                dev_X, val_X = train.iloc[dev_index], train.iloc[val_index]\n                dev_y, val_y = target[dev_index], target[val_index]\n            else:\n                dev_X, val_X = train[dev_index], train[val_index]\n                dev_y, val_y = target[dev_index], target[val_index]\n\n            params2 = params.copy()\n            pred_val_y, pred_test_y, importances, coefficients, qwk = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n            pred_full_test += (pred_test_y \/ num_iters)\n            pred_train[val_index] += (pred_val_y\/num_iters)\n            all_coefficients[i-1, :] = coefficients\n            if eval_fn is not None:\n                cv_score = eval_fn(val_y, pred_val_y)\n                cv_scores.append(cv_score)\n                qwk_scores.append(qwk)\n                print(label + ' cv score {}: RMSE {} QWK {}'.format(i, cv_score, qwk))\n            fold_importance_df = pd.DataFrame()\n            fold_importance_df['feature'] = train.columns.values\n            fold_importance_df['importance'] = importances\n            fold_importance_df['fold'] = i\n            feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)        \n            i += 1\n        pred_full_test = (pred_full_test \/ num_folds)\n        print('{} cv RMSE scores : {}'.format(label, cv_scores))\n        print('{} cv mean RMSE score : {}'.format(label, np.mean(cv_scores)))\n        print('{} cv std RMSE score : {}'.format(label, np.mean(cv_scores)))\n        print('{} cv QWK scores : {}'.format(label, qwk_scores))\n        print('{} cv mean QWK score : {}'.format(label, np.mean(qwk_scores)))\n        print('{} cv std QWK score : {}'.format(label, np.std(qwk_scores)))\n        results = {'label': label,\n                   'train': pred_train, 'test': pred_full_test,\n                    'cv': cv_scores, 'qwk': qwk_scores,\n                   'importance': feature_importance_df,\n                   'coefficients': all_coefficients}\n    return results\n\n\nparams = {'application': 'regression',\n          'boosting': 'gbdt',\n          'metric': 'rmse',\n          'num_leaves': 70,\n          'max_depth': 7,\n          'learning_rate': 0.02,\n          'bagging_fraction': 0.85,\n          'feature_fraction': 0.8,\n          'min_split_gain': 0.02,\n          'min_child_samples': 150,\n          'min_child_weight': 0.2,\n          'lambda_l2': 0.05,\n          'verbosity': -1,\n          'data_random_seed': 24,\n          'early_stop': 100,\n          'verbose_eval': 100,\n          'num_rounds': 10000}\n\ndef runLGB(train_X, train_y, test_X, test_y, test_X2, params):\n    print('Prep LGB')\n    d_train = lgb.Dataset(train_X, label=train_y)\n    d_valid = lgb.Dataset(test_X, label=test_y)\n    watchlist = [d_train, d_valid]\n    print('Train LGB')\n    num_rounds = params.pop('num_rounds')\n    verbose_eval = params.pop('verbose_eval')\n    early_stop = None\n    if params.get('early_stop'):\n        early_stop = params.pop('early_stop')\n    model = lgb.train(params,\n                      train_set=d_train,\n                      num_boost_round=num_rounds,\n                      valid_sets=watchlist,\n                      verbose_eval=verbose_eval,\n                      early_stopping_rounds=early_stop)\n    print('Predict 1\/2')\n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    optR = OptimizedRounder()\n    optR.fit(pred_test_y, test_y)\n    coefficients = optR.coefficients()\n    pred_test_y_k = optR.predict(pred_test_y, coefficients)\n    print(\"Valid Counts = \", Counter(test_y))\n    print(\"Predicted Counts = \", Counter(pred_test_y_k))\n    print(\"Coefficients = \", coefficients)\n    qwk = quadratic_weighted_kappa(test_y, pred_test_y_k)\n    print(\"QWK = \", qwk)\n    print('Predict 2\/2')\n    pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n    return pred_test_y.reshape(-1, 1), pred_test_y2.reshape(-1, 1), model.feature_importance(\"gain\"), coefficients, qwk\n\n# results = run_cv_model(train, test, y, runLGB, params, rmse, 'lgb')","2dd318cf":"no_drop = ['Color1', 'Color2', 'Color3', 'Dewormed', 'Fee', 'FurLength',\n       'Gender', 'Health', 'MaturitySize','PhotoAmt', 'Quantity','Sterilized', 'Type', 'Vaccinated',\n           #'label_description',\n           'state_gdp', 'state_population',]","0560be61":"num_cols","b01f2173":"keep_features = (['Age', 'Breed1', 'Breed2', 'Color1', 'Color2', 'Color3', 'Fee', 'Quantity',\n        'MaturitySize','FurLength','Vaccinated','Dewormed', 'Sterilized','Health','Quantity','Fee',\n        'VideoAmt','PhotoAmt', 'state_gdp','state_population', 'doc_sent_mag', 'doc_sent_score', 'vertex_x', 'vertex_y', \n        'bounding_confidence', 'bounding_importance','dominant_blue', 'dominant_green', 'dominant_red', 'dominant_pixel_frac',\n        'dominant_score', 'label_description', 'label_score',\n       'RescuerID_COUNT','State', 'Sterilized', 'agg_mean_Age', 'agg_mean_Color1',\n       'agg_mean_Color2', 'agg_mean_Color3', 'agg_mean_Dewormed',\n       'agg_mean_Fee', 'agg_mean_FurLength', 'agg_mean_Gender',\n       'agg_mean_PhotoAmt', 'agg_mean_Quantity', 'agg_mean_Sterilized',\n       'agg_mean_Vaccinated', 'agg_mean_VideoAmt', 'agg_std_Age',\n       'agg_std_Color1', 'agg_std_Color2', 'agg_std_Color3',\n       'agg_std_Dewormed', 'agg_std_Fee', 'agg_std_FurLength',\n       'agg_std_Gender', 'agg_std_PhotoAmt', 'agg_std_Quantity',\n       'agg_std_Sterilized', 'agg_std_Vaccinated', 'label_score',\n       'metadata_metadata_annots_score_MAX',\n       'metadata_metadata_annots_score_MEAN',\n       'metadata_metadata_annots_score_MIN',\n       'metadata_metadata_annots_score_SUM',\n       'metadata_metadata_annots_score_VAR',\n       'metadata_metadata_color_pixelfrac_MIN',\n       'metadata_metadata_color_pixelfrac_SUM',\n       'metadata_metadata_color_pixelfrac_VAR',\n       'metadata_metadata_color_score_SUM',\n       'metadata_metadata_color_score_VAR', 'vertex_x', 'vertex_y','RescuerID_UNIQUE'] \n       +[\"svd1_\" + str(i) for i in range(n_components)] \n       +[\"img_svd1_\" + str(i) for i in range(n_components)] \n       +[\"img_svd2_\" + str(i) for i in range(n_components)]\n#        +[\"SVD_metadata_annots_top_desc_\" + str(i) for i in range(16)]\n#        +[\"SVD_sentiment_entities_\" + str(i) for i in range(16)]\n        )\n","606940b6":"results = run_cv_model(train[keep_features], test[keep_features], y, runLGB, params, rmse, 'lgb')","854fa6d8":"len(keep_features)","c12788f5":"imports = results['importance'].groupby('feature')['feature', 'importance'].mean().reset_index()\nimports.sort_values('importance', ascending=False)","25eb2c65":"import xgboost as xgb\nxgb_params = {\n    'eval_metric': 'rmse',\n    'seed': 1337,\n    'eta': 0.0123,\n    'subsample': 1.0,\n    'colsample_bytree': 0.6,\n    'min_child_weight': 1.0,\n    'gamma' : .5,\n    'max_depth' : 5,\n    'tree_method': 'gpu_hist',\n    'device': 'gpu',\n    'silent': 1,\n}","345e3312":"def run_xgb(params, X_train, X_test):\n    n_splits = 20\n    verbose_eval = 1000\n    num_rounds = 30000\n    early_stop = 500\n\n    kf = GroupKFold(n_splits=n_splits)\n\n    oof_train = np.zeros((X_train.shape[0]))\n    oof_test = np.zeros((X_test.shape[0], n_splits))\n\n    i = 0\n\n    for train_idx, valid_idx in kf.split(X_train, y,  rescuerID):\n\n        X_tr = X_train.iloc[train_idx, :]\n        X_val = X_train.iloc[valid_idx, :]\n\n        y_tr = y.iloc[train_idx]\n        y_val = y.iloc[valid_idx]\n        d_train = xgb.DMatrix(data=X_tr, label=y_tr, feature_names=X_tr.columns)\n        d_valid = xgb.DMatrix(data=X_val, label=y_val, feature_names=X_val.columns)\n\n        watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n        model = xgb.train(dtrain=d_train, num_boost_round=num_rounds, evals=watchlist,\n                         early_stopping_rounds=early_stop, verbose_eval=verbose_eval, params=params)\n\n        valid_pred = model.predict(xgb.DMatrix(X_val, feature_names=X_val.columns), ntree_limit=model.best_ntree_limit)\n        test_pred = model.predict(xgb.DMatrix(X_test, feature_names=X_test.columns), ntree_limit=model.best_ntree_limit)\n\n        oof_train[valid_idx] = valid_pred\n        oof_test[:, i] = test_pred\n\n        i += 1\n    return model, oof_train, oof_test","60acd27f":"final_feats = keep_features","29ef6822":"X_temp = pd.concat([train, test], ignore_index=True, sort=False)\nfor i in cat_cols:\n    X_temp = pd.concat([pd.get_dummies(X_temp.loc[:, i], prefix = str(i)), X_temp], axis = 1)\n    final_feats = final_feats + [i + \"_\"+ str(x) for x in range(X_temp.loc[:, i].nunique())]\ntrain = X_temp.loc[:train.shape[0]-1, :]\ntest = X_temp.loc[train.shape[0]:, :]","ff23c8e7":"train[cat_cols] = train[cat_cols].astype(int)\ntest[cat_cols] = test[cat_cols].astype(int)","d20add69":"model, oof_train, oof_test = run_xgb(xgb_params, train[train[final_feats].columns.unique()], test[test[final_feats].columns.unique()])","88f16b69":"optR = OptimizedRounder()\noptR.fit(oof_train, y)\ncoefficients = optR.coefficients()\noof_xgb = optR.predict(oof_train, coefficients)\nqwk = quadratic_weighted_kappa(y, oof_xgb)\nprint(\"QWK = \", qwk)\n","4fd89c3a":"test_preds_xgb = optR.predict(oof_test.mean(axis=1), coefficients).astype(np.int8)","0056ab4b":"optR = OptimizedRounder()\ncoefficients_ = np.mean(results['coefficients'], axis=0)\nprint(coefficients_)\ntrain_predictions = [r[0] for r in results['train']]\noof_lgb = np.array(train_predictions)\ntrain_predictions = optR.predict(train_predictions, coefficients_).astype(int)\nCounter(train_predictions)","f44fb40d":"test_predictions = [r[0] for r in results['test']]\ntest_preds_lgb = np.array(test_predictions)\ntest_predictions = optR.predict(test_predictions, coefficients_).astype(int)","7bab1978":"print(oof_nn.shape)\nprint(oof_lgb.shape)\nprint(oof_xgb.shape)\nprint(test_preds_nn.shape)\nprint(test_preds_lgb.shape)\nprint(test_preds_xgb.shape)\n","59a610c6":"print((test_preds_lgb).mean())\nprint(oof_lgb.mean())","490ed298":"#lgb only\noptR = OptimizedRounder()\noptR.fit(oof_lgb, y)\ncoefficients = optR.coefficients()\nprint(coefficients)\noof_rounded = optR.predict(oof_lgb, coefficients)\nprint(quadratic_weighted_kappa(y, oof_rounded))\ntest_rounded_lgb = optR.predict(test_preds_lgb, coefficients)","725970a0":"#nn only\nif softmax == False:\n    optR = OptimizedRounder()\n    optR.fit(oof_nn, y)\n    coefficients = optR.coefficients()\n    print(coefficients)\n    oof_rounded = optR.predict(oof_nn, coefficients)\n    print(quadratic_weighted_kappa(y, oof_rounded))\n    test_rounded_nn = optR.predict(test_preds_nn, coefficients)\nif softmax == True:\n    oof_rounded = np.argmax(oof_nn, axis = 1)\n    test_rounded_nn = np.argmax(test_preds_nn, axis = 1)\n    print(quadratic_weighted_kappa(y, oof_rounded))","29325b3c":"oof_models = [oof_nn, oof_lgb\n                 , oof_train, ffm_oof_train\n                ]\ntest_models = [test_preds_nn, test_preds_lgb\n                                     , oof_test.mean(axis = 1), ffm_oof_test\n                                    ]","80fbb426":"from sklearn.linear_model import LinearRegression, Ridge\nlr = Ridge(fit_intercept=False)\nlr.fit(np.array(oof_models).T, y)\nprint(lr.coef_)\nlr.coef_ = lr.coef_ * 1\/(sum(lr.coef_))\nprint(lr.coef_)\noof_lr = lr.predict(np.array(oof_models).T)\ntest_preds_lr = lr.predict(np.array(test_models).T)\n#lr of nn and lgb and xgb\noptR = OptimizedRounder()\noptR.fit(oof_lr, y)\ncoefficients = optR.coefficients()\nprint(coefficients)\noof_rounded = optR.predict(oof_lr, coefficients)\nprint(quadratic_weighted_kappa(y, oof_rounded))\ntest_rounded_lr = optR.predict(test_preds_lr, coefficients)","dd89ba7c":"submission_df = pd.read_csv(\"..\/input\/petfinder-adoption-prediction\/test\/sample_submission.csv\")","9fa04bf5":"submission_df[\"AdoptionSpeed\"] = test_rounded_lr.astype(int)\nsubmission_df.to_csv(\"submission.csv\", index=False)","4838ee34":"submission_df['AdoptionSpeed'].mean()","7e81442a":"submission_df['AdoptionSpeed'].value_counts(normalize = False)","344f177e":"print(test_preds_nn.mean(), test_preds_lgb.mean(), oof_test.mean(), ffm_oof_test.mean())\nprint(oof_nn.mean(), oof_lgb.mean(), oof_train.mean(), ffm_oof_train.mean())","bfc9a6e8":"Now we can start looking at the FFM setup. While FFM's are pretty good at dealing with categoricals, theyre not very useful with numericals on their own. So to deal with this we used the KBinsDiscretizer which put the numerical variables into their own groupings. This seemed to have a strong impact on CV. ","1ac7ea6b":"Had to remap categoricals to dummy variables since XGBoost cant handle these out of the box like LGBM does. ","6c4499a8":"Here we did a bunch of aggregations on the rescuerID. So here we knew that we couldn't use rescuerID as a feature to learn from as there was no overlap between the train and test set, but we also knew it was a very predictive feature so we tried to instead get proxy understandings about the rescuerID's. What kind of pets did they typically have, how old were they, do they do a good job of uploading a lot of photos. agg_mean_PhotoAmt was one of our most important features consistently. More important so than even PhotoAmt on it's own. ","ebfef9d9":"In order to ensemble our models we did Ridge Regression of the outputs of the various models. We turned fit intercept off and in this way the regression was simply finding the best set of weights to combine the the models. One thing to note is that this is optimizing for loss, rather than qwk. One thing we explored was doing some iterative hill climbing approach so we could more directly optimize for QWK, but we didnt pursue it. Ultimately I think that would have just led to overfitting as now we are directly training on the labels of the data without any validation structure in place. \n\nOne thing to note with the implementation we used is I renormalized the weights. Out of the box Ridge regression might choose to give a total weighting of 1.2 rather than 1.0. I renormalized these weights to sum to 1. This likely isnt truly valid because the regression is trying to move our distribution to match the train distribution more closely. By renormalizing it we are shifting it away from fitting the distribution better, but it seems we got away with it. Would be interesting to compare the results vs non-normalized. ","cab7da41":"Used these state gdp and state population inputs like most other people. Wasn't crucial but did seem to get some use in the LGBM feature importance charts","5e078abc":"Here's one of the main sections I think we got a bump over other people. For our NN embedding layer we need them to be the correct size for the number of possible categories. The way that Christof set things up in his original kernel a lot of people branched from he didnt fit on both the train and test options and if i'm not mistaken he didn't relabel encode the categoricals. This meant that in the NN it would look up a embedding for breed 274, but the embedding layer would only be size 125 because it didnt have all possible options. This caused major NN instability. The below code takes inventory of all possible categories and then condenses them down to indexes counting up 1, 2, 3, 4, etc. to match with the size of the embedding layer. This was kind of found by sheer luck. I increased the embedding size by plus 50 and it seemed to alleviate some, but not all of the stability issues. That shouldnt have made any performance difference so I inspected that area a bit more. Prior to that I was focused on maybe lowering the learning rate or clipping gradients because I couldnt figure out what was going wrong. ","69735927":"We used the same json parser as most other people. We found at the end that many people made some slight alterations that made this process way faster. Time wasn't really an issue for the submission, but it would have helped our prototyping phase if we didnt have to do the 20 minute wait for this everytime. ","a2f368c7":"We toyed around with the cats and dog breeds rating files but found the coverage was very poor. We tried to do some fuzzy matching and remapping, but didn't end up completing this to get enough coverage to really be useful. This was just leftover code. ","b0063c9e":"Here is the definition of our NN model. It isn't anything too fancy, just embedding layers for the categoricals, dense layers for the numericals, img and text SVD features and then CNN over the text. ","1864091a":"We used the same process of tfidf and then svd or other dimensionality reduction tricks, but we decided to do much smaller component count than most other people. This was informed by our feature selection process I will describe in more detail later. ","aacfa694":"## Handling text columns","9b06c8de":"## Handling categorical columns","c164ab9f":"We did RescuerID_Count like most other people and we also found an additional feature, number of unique breeds per rescuerID that gave us a small boost as well. The intuition behind this is that rescuer ID count tells you the quantity of pets, big corporation or individual and the number of unique breeds likely gives you a more clear signal of if they were breeders of one specific dog or not. \n\nI think we likely could have created a lot more features around this, but ultimately we just ran out of time and didn't create as many features as most other people. ","44ea960c":"Scaling is very important for the NN but not for any of the other methods.","29cd9e7d":"We utilized the \"Golden\" rounder which seemed to give us a very small fractional gain consistently. Only disadvantage was it was much slower. Likely would have been better to use the faster methods while prototyping but we werent constrained too tightly on this competition. ","373cce77":"Our lists of categorical features and numerical features. In an ideal world we would have had time to tune each model individually with the best features for each but again we didn't really have enough time to truly hone in on each one indivudally so this is basically our feature set with maybe one or two alterations between models. ","90de894c":"Like many other people discovered, because the rescuerID was unique between the train and test set we chose to use groupKFold which allowed us to have unique rescuerID's between our train and validation sets. We also chose higher K than most people in hopes this would give us more stable results. We used 10 for the NN and 20 for the others. This may or may not have been the way to go as it makes your validation set very small. At the same time when training so many models you have to assume across all of them it will find some useful combinations of the data. ","81191a2d":"One other interesting thing we did was use a sigmoid activation function. Some others used a linear function, but I wanted out predictions to only be bounded to the possible labels. In order to do this I divided the adoption speed by 4. This condensed the range down to between 0-1 and allowed for sigmoid acitvation function at the end. I don't think this was important to our solution, it seems many teams were able to get similar results with linear activations, but sigmoid seemed to converge more quickly for me. ","1c8840c7":"We utilized image features like many others but these werent a huge factor. ","48bf0b4d":"Honestly really not a fan of the tools that prefer you to  write these things to specific file formats. Have worked with this one and vowpal wabbit now. Similar boilerplate from the public kernels. Someone from our group tried the scikit-learn stype api and it seemed to function much slower so we conceded defeat and did the file writing process. ","dd568ef4":"This result was actually a big surprise. We were at ~260th in the public leaderboard and I was kind of resigned to us having done something critically wrong (we did). Conversely our \"A\" team from the meetup was sitting pretty at 42nd. Looks like the major trap they fell into was target encoding. ","5ed77dac":"Some simple text features we added in. Didnt seem to make any real difference but werent hurting anything so left them in. ","f78e69f1":"Had to reload things and reorganize some stuff. This was mostly because at the beginning we would use some preprocessed input data instead of recomputing things everytime. Because when we made our precomputed input file I forgot we had tokenized inplace I needed to load in the descriptions again. This section is more correcting errors than doing anything deliberate and intelligent. ","752df542":"Here we did the same image through pretrained network trick but again we did a slight twist on it. Instead of doing the averaging step down to 256 or 128 we used SVD again in order to more efficiently pack that same information down to 32 components. This was again informed by the feature selection process we used. ","cc2a2309":"## Define NN Model","d5426008":"The features we eneded up utilizing after many runs determining which of our features were successful and useful","23afebb7":"Our LGBM model really isn't anything too interesting but what was interesting was how we used it for feature selection and to inform our decisions. Most people have seen the feature importance plots. We built on top of these in order to drop features. We injected a random noise feature and found that many features ended up below this random variable. We dropped any features that were below this threshold (unless we were confident they would be useful\/harmless to leave in) This allowed us to drastically cut down our number of useless features, speed up our processing time and also increase our generalization capability. There was a point where we had ~600 features we would plug in and after null importance filtering we would only have ~100. \n\nThis informed our design of the component counts to use as well. We reduced the number of components until all of the components would consistently end up above the null feature importance. In this way we ensured that the information density from each was better than random noise. Not sure if this is definitively the best way to do that but it seemed to make logical sense at the time. ","6bd2d44f":"Name length feature. Also didnt seem to make a measurable difference but was left in","24305355":"The major structure of our code is very similar to other people's. As a general overview we did 4 models: NN, xLearn FFM, LGBM, XGBoost. We didn't have time to individually test the performance on leaderboard of each, but we have QWK CV numbers for each:\n* NN - .416\n* xLearn FFM - .418\n* LGBM - .443\n* XGBoost - ..445\n\nTotal ensemble(Ridge Regression) CV - .46\n\nMany of our features were the same as everyone else's, but we also had a couple unique ones and did some interesting things for feature selection. ","9977edb5":"We toyed with directly optimizing qwk loss for the NN, but this didn't seem to be better so we stuck with treating it as a regression problem and using rmse. ","035cc1af":"Xgboost. Again, nothing exceptionally interesting here. Did some minor hyperparameter tuning. One thing we considered though was that we are doing hyperparameter tuning on models individually. It would likely be more useful in the future to tune parameters dependent on performance in an ensemble. Ie. optimizing the NN. then we could optimize the parameters of xLearn when ensembled with the NN. In theory this might steer us away from strong individual model performance but diverse models that ensemble better. Just a thought though. Not necessarily something we have implemented here."}}