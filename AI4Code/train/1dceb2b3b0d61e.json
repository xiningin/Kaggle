{"cell_type":{"ffd645fa":"code","81d8fb07":"code","c9b33eef":"code","21755cf7":"code","4ae80c3f":"code","c53ef12b":"code","f437fbb0":"code","2591fae7":"code","058981fa":"code","9fd6016e":"code","774b2913":"code","f4377bcc":"code","c0f3ec98":"code","67085362":"code","525add38":"code","270234ab":"code","9f46bf71":"code","a5e1522e":"code","82648f7e":"code","a6d6cdf1":"code","ae9ab440":"code","8cd1a3be":"code","b8ffc830":"code","ee487c1f":"code","bd94dd33":"code","cda1b36b":"code","4483b08d":"code","d3d14397":"code","f3d6dff5":"code","ba1140cc":"code","984847a6":"code","ee9857ea":"code","2e30043c":"code","659ea6da":"code","efca3ea3":"code","0b8e5a53":"code","63617136":"code","6ddbf9e5":"code","27f15f3d":"code","92e39278":"code","602967aa":"code","d33c9870":"code","0fdd82b3":"code","99713edb":"code","46d38523":"code","03deac17":"code","89b8959f":"code","2f1840e2":"code","884f3ee6":"code","309fa655":"code","24b21746":"code","9033bc79":"code","e809fa5a":"code","bb0f343f":"code","389b742c":"code","302a1902":"code","323d562b":"code","556badec":"code","67a72745":"code","48ce76c6":"code","8e9dcc30":"code","fc02a019":"code","dd8423e3":"code","f9cd2bbc":"code","6e98b70b":"markdown","1e7d4d49":"markdown","efe4605f":"markdown","a00c648b":"markdown","7f3b8907":"markdown","9619c93a":"markdown","858d76e9":"markdown","e753d73a":"markdown","024f3bd6":"markdown","29abdbb7":"markdown","9d155eea":"markdown","9d6c6948":"markdown","29f84a08":"markdown","27a55881":"markdown","d7d6c7fa":"markdown","912c6142":"markdown","b41a38da":"markdown","41d5058c":"markdown","ba7cd418":"markdown","0ce2a4cd":"markdown","59e6032f":"markdown","d9685cea":"markdown","8f0bd429":"markdown","96962ff2":"markdown","3a655987":"markdown","ca1f45ac":"markdown","bdd1fd5b":"markdown","3a831885":"markdown","17c65427":"markdown","e727fb45":"markdown","10be45bf":"markdown","467c09db":"markdown","e006c07e":"markdown","277930e8":"markdown","aab5890f":"markdown","be98061e":"markdown","a452c8fc":"markdown","1d11c6a8":"markdown","82f0f437":"markdown","b24939e7":"markdown","43806175":"markdown","eb868143":"markdown","5db2c23d":"markdown","34eb6a24":"markdown","f89fa754":"markdown","e3e4aa6a":"markdown","cb4fbca4":"markdown","7ca1728c":"markdown","fb4d6d30":"markdown","8d83e0a8":"markdown","1ff6ff6f":"markdown"},"source":{"ffd645fa":"#Data\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport sys\n\n#Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\nimport plotly.figure_factory as ff\nimport plotly.graph_objs as go\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport colorlover as cl\nfrom IPython.display import HTML, SVG\nimport random\n\nrandom.seed(42)\ninit_notebook_mode(connected=True)\n#%matplotlib inline","81d8fb07":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_comp = pd.read_csv('..\/input\/test.csv')","c9b33eef":"iplot(ff.create_table(df_train.iloc[0:10,0:10]), filename='jupyter-table1')","21755cf7":"n_size = 10\nsel_int = 5\nnum_array = df_train[df_train.label == sel_int]\n\nfig = tools.make_subplots(rows=10, cols=10, print_grid=False)\nfor row in range(1,n_size+1):\n    for column in range(1,n_size+1):\n        trace = go.Heatmap(z=num_array.iloc[row*10-10+column-1, 1:].values.reshape((28,28))[::-1], colorscale=[[0,'rgb(0,0,0)'],[1,'rgb(255,255,255)']], showscale=False)\n        fig.append_trace(trace, row, column)\n        fig['layout']['xaxis'+str(((row-1)*10 + column))].update(showticklabels=False, ticks='')\n        fig['layout']['yaxis'+str(((row-1)*10 + column))].update(showticklabels=False, ticks='')\n        \nfig['layout'].update(height=500, width=500)\nfig['layout']['margin'].update(l=10, r=10, b=10, t=10)\niplot(fig, filename='number_plot')","4ae80c3f":"#df.label.value_counts().values\ntrace = go.Bar(x=df_train.label.value_counts().index,y=df_train.label.value_counts().values)\nlayout = go.Layout(xaxis=dict(title='Number', nticks=10),\n                  yaxis=dict(title='# Occurance'),\n                  width = 600,\n                  height = 400\n                  )\nfigure = go.Figure(data = [trace],\n                  layout = layout)\nfigure['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(figure)","c53ef12b":"df_train.describe()","f437fbb0":"df_train.isnull().sum().sum()","2591fae7":"from sklearn.model_selection import train_test_split","058981fa":"Y = df_train.label\nX = df_train.drop('label', axis=1)\n\nX = X \/ 255\nX_comp = df_comp \/ 255\n\nX_train, X_cross, Y_train, Y_cross = train_test_split(X, Y,test_size=0.1, random_state=42)\nX_valid, X_test, Y_valid, Y_test = train_test_split(X_cross, Y_cross, test_size=0.5, random_state=42)","9fd6016e":"trace1 = go.Bar(x=Y_train.value_counts().index,y=Y_train.value_counts().values\/Y_train.value_counts().values.sum(), name='Training set')\ntrace2 = go.Bar(x=Y_valid.value_counts().index,y=Y_valid.value_counts().values\/Y_valid.value_counts().values.sum(), name='Validation set')\ntrace3 = go.Bar(x=Y_test.value_counts().index,y=Y_test.value_counts().values\/Y_test.value_counts().values.sum(), name='Test set')\nfig = go.Figure(data=[trace1, trace2, trace3])\nfig['layout'].update(xaxis=dict(title='Number', nticks=10), yaxis=dict(title='# Occurance'), width = 600, height = 400)\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","774b2913":"from keras.models import Sequential, load_model\nfrom keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten\nfrom keras.utils import plot_model, to_categorical\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score","f4377bcc":"X_train = X_train.values.reshape(X_train.shape[0],28,28,1)\nX_valid = X_valid.values.reshape(X_valid.shape[0],28,28,1)\nX_test = X_test.values.reshape(X_test.shape[0],28,28,1)\nX_comp = X_comp.values.reshape(X_comp.shape[0],28,28,1)\n\nY_train = to_categorical(Y_train)\nY_valid = to_categorical(Y_valid)\nY_test = to_categorical(Y_test)","c0f3ec98":"datagen = ImageDataGenerator(height_shift_range=0.1,\n                             width_shift_range=0.1,\n                             #brightness_range=(0,0.1),\n                             rotation_range=10,\n                             zoom_range=0.1,\n                             fill_mode='constant',\n                             cval=0\n                            )\n\ndatagen.fit(X_train)","67085362":"model = Sequential()\ndroprate = 0.175\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel1 = model\nmodel1.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","525add38":"epochsN = 25\nbatch_sizeN = 63\nhistory1 = model1.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)\/batch_sizeN, epochs=epochsN, verbose=2)","270234ab":"model1.evaluate(X_test, Y_test, verbose=0)","9f46bf71":"model1.save('model_1.h5')","a5e1522e":"history = history1\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","82648f7e":"del model\nmodel = Sequential()\ndroprate = 0.15\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\n#model.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\n#model.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\n#model.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel2 = model\nmodel2.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","a6d6cdf1":"epochsN = 35\nbatch_sizeN = 63\nhistory2 = model2.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)\/batch_sizeN, epochs=epochsN, verbose=2)","ae9ab440":"model2.evaluate(X_test, Y_test, verbose=0)","8cd1a3be":"model2.save('model_2.h5')","b8ffc830":"history = history2\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","ee487c1f":"del model\nmodel = Sequential()\ndroprate = 0.2\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel3 = model\nmodel3.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","bd94dd33":"epochsN = 40\nbatch_sizeN = 63\nhistory3 = model3.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)\/batch_sizeN, epochs=epochsN, verbose=2)","cda1b36b":"model3.evaluate(X_test, Y_test, verbose=0)","4483b08d":"model3.save('model_3.h5')","d3d14397":"history = history3\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","f3d6dff5":"del model\nmodel = Sequential()\ndroprate = 0.20\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=16, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=16, strides=(1,1), padding='same',activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(droprate))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel4 = model\nmodel4.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","ba1140cc":"epochsN = 90\nbatch_sizeN = 63\nhistory4 = model4.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)\/batch_sizeN, epochs=epochsN, verbose=2)","984847a6":"model4.evaluate(X_test, Y_test, verbose=0)","ee9857ea":"model4.save('model_4.h5')","2e30043c":"history = history4\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","659ea6da":"del model\nmodel = Sequential()\ndroprate = 0.1\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=16, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(3,3), filters=16, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dropout(droprate))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel5 = model\nmodel5.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","efca3ea3":"epochsN = 90\nbatch_sizeN = 63\nhistory5 = model5.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)\/batch_sizeN, epochs=epochsN, verbose=2)","0b8e5a53":"model5.evaluate(X_test, Y_test, verbose=0)","63617136":"model5.save('model_5.h5')","6ddbf9e5":"history = history5\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","27f15f3d":"del model\nmodel = Sequential()\ndroprate = 0.15\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=32, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=16, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dropout(droprate))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel6 = model\nmodel6.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","92e39278":"epochsN = 45\nbatch_sizeN = 63\nhistory6 = model6.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)\/batch_sizeN, epochs=epochsN, verbose=2)","602967aa":"model6.evaluate(X_test, Y_test, verbose=0)","d33c9870":"model6.save('model_6.h5')","0fdd82b3":"history = history6\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","99713edb":"del model\nmodel = Sequential()\ndroprate = 0.35\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu', input_shape=(28,28,1)))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\n#model.add(Conv2D(kernel_size=(2,2), filters=64, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=64, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(3,3), filters=128, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(3,3), filters=128, strides=(1,1), padding='same',activation='relu'))\n#model.add(Conv2D(kernel_size=(3,3), filters=128, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(2,2), filters=128, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Conv2D(kernel_size=(3,3), filters=256, strides=(1,1), padding='valid',activation='relu'))\nmodel.add(Conv2D(kernel_size=(3,3), filters=256, strides=(1,1), padding='valid',activation='relu'))\n#model.add(Conv2D(kernel_size=(3,3), filters=256, strides=(1,1), padding='same',activation='relu'))\nmodel.add(Conv2D(kernel_size=(3,3), filters=256, strides=(2,2), padding='valid',activation='relu'))\nmodel.add(Dropout(droprate))\nmodel.add(Flatten())\nmodel.add(Dense(10, activation='softmax'))\n\nmodel7 = model\nmodel7.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","46d38523":"epochsN = 60\nbatch_sizeN = 63\nhistory7 = model7.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_sizeN), validation_data=(X_valid, Y_valid), steps_per_epoch=len(X_train)\/batch_sizeN, epochs=epochsN, verbose=2)","03deac17":"model7.evaluate(X_test, Y_test, verbose=0)","89b8959f":"model7.save('model_7.h5')","2f1840e2":"history = history7\nfig = tools.make_subplots(rows=1, cols=2, print_grid=False)\ntrace1 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['loss'], name='Training Loss')\ntrace2 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_loss'], name='Validation Loss')\ntrace3 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['acc'], name='Training Accuracy')\ntrace4 = go.Scatter(x=list(range(1,epochsN+1)), y=history.history['val_acc'], name='Validation Accuracy')\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 1)\nfig.append_trace(trace3, 1, 2)\nfig.append_trace(trace4, 1, 2)\nfig['layout'].update(xaxis1=dict(title='Epoch', nticks=10), yaxis1=dict(title='Loss', type='log'), width = 1200, height = 400)\nfig['layout'].update(xaxis2=dict(title='Epoch', nticks=10), yaxis2=dict(title='Accuracy', type = 'log'))\nfig['layout']['margin'].update(l=50, r=50, b=50, t=50)\n\n#fig['layout'].update(xaxis=dict(title='Epoch', nticks=10), yaxis=dict(title='Loss'), width = 600, height = 400)\n#fig['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(fig)","884f3ee6":"trained_models = [model1, model2, model3, model4, model5, model6, model7]","309fa655":"acc_scores = pd.Series()\nfor num, model in enumerate(trained_models):\n    acc_scores.loc['Model ' + str(num + 1)] = accuracy_score(np.argmax(Y_test, axis=1), np.argmax(model.predict(X_test), axis=1))","24b21746":"trace = go.Bar(x=acc_scores.values ,y=acc_scores.index, orientation='h')\nlayout = go.Layout(xaxis=dict(title='Accuracy', nticks=10, range=[0.985, 1]),\n                  #yaxis=dict(title='Model'),\n                  width = 600,\n                  height = 400\n                  )\nfigure = go.Figure(data = [trace],\n                  layout = layout)\nfigure['layout']['margin'].update(l=50, r=50, b=50, t=50)\niplot(figure)","9033bc79":"print(acc_scores.idxmax(), ': ', acc_scores[acc_scores.idxmax()])","e809fa5a":"ind_best_model = acc_scores.reset_index().loc[:, 0].idxmax(axis=0)\nY_test_pred = trained_models[ind_best_model].predict(X_test)\nconfM = confusion_matrix(np.argmax(Y_test, axis=1), np.argmax(Y_test_pred, axis=1))\n\nfig = ff.create_annotated_heatmap(x=list(map(str, range(0,10))), y=list(map(str, range(0,10))), z=np.log(confM+1), annotation_text=confM, colorscale='Jet')\nfig['layout'].update(xaxis=dict(title='Predictated Label'), yaxis=dict(title='Actual Label', autorange='reversed'), width = 600, height = 600)\n\niplot(fig)","bb0f343f":"def summing_classifier(data, model_list):\n    total_pred_prob = model_list[0].predict(data)\n    for model in model_list[1:]:\n        total_pred_prob += model.predict(data)\n        \n    return np.argmax(total_pred_prob, axis=1)","389b742c":"acc_scores.loc['Summing Classifier'] = accuracy_score(np.argmax(Y_test, axis=1), summing_classifier(X_test, trained_models))\nacc_scores.loc['Summing Classifier']","302a1902":"acc_scores.iloc[0:6].mean()","323d562b":"confM = confusion_matrix(np.argmax(Y_test, axis=1), summing_classifier(X_test, trained_models))\n\nfig = ff.create_annotated_heatmap(x=list(map(str, range(0,10))), y=list(map(str, range(0,10))), z=np.log(confM+1), annotation_text=confM, colorscale='Jet')\nfig['layout'].update(xaxis=dict(title='Predictated Label'), yaxis=dict(title='Actual Label', autorange='reversed'), width = 600, height = 600)\n\niplot(fig)","556badec":"def voting_classifier(data, model_list):\n    pred_list = np.argmax(model_list[0].predict(data), axis=1).reshape((1,len(data)))\n    for model in model_list[1:]:\n        pred_list = np.append(pred_list, [np.argmax(model.predict(data), axis=1)], axis=0)\n    return np.array(list(map(lambda x: np.bincount(x).argmax(), pred_list.T)))","67a72745":"acc_scores.loc['Voting Classifier'] = accuracy_score(np.argmax(Y_test, axis=1), voting_classifier(X_test, trained_models))\nacc_scores.loc['Voting Classifier']","48ce76c6":"confM = confusion_matrix(np.argmax(Y_test, axis=1), voting_classifier(X_test, trained_models))\n\nfig = ff.create_annotated_heatmap(x=list(map(str, range(0,10))), y=list(map(str, range(0,10))), z=np.log(confM+1), annotation_text=confM, colorscale='Jet')\nfig['layout'].update(xaxis=dict(title='Predictated Label'), yaxis=dict(title='Actual Label', autorange='reversed'), width = 600, height = 600)\n\niplot(fig)","8e9dcc30":"trace = go.Bar(x=acc_scores.sort_values(ascending=True).values ,y=acc_scores.sort_values(ascending=True).index, orientation='h')\nlayout = go.Layout(xaxis=dict(title='Accuracy', nticks=10, range=[0.985, 1]),\n                  #yaxis=dict(title='Model'),\n                  width = 600,\n                  height = 400\n                  )\nfigure = go.Figure(data = [trace],\n                  layout = layout)\nfigure['layout']['margin'].update(l=130, r=50, b=50, t=50)\niplot(figure)","fc02a019":"best_model_results = pd.DataFrame({'Label' : np.argmax(trained_models[ind_best_model].predict(X_comp), axis=1)})\nbest_model_results = best_model_results.reset_index().rename(columns={'index' : 'ImageId'})\nbest_model_results['ImageId'] = best_model_results['ImageId'] + 1\nbest_model_results.to_csv('best_model_result_kaggle.csv', index=False)","dd8423e3":"esmbl_sum_results = pd.DataFrame({'Label' : summing_classifier(X_comp, trained_models)})\nesmbl_sum_results = esmbl_sum_results.reset_index().rename(columns={'index' : 'ImageId'})\nesmbl_sum_results['ImageId'] = esmbl_sum_results['ImageId'] + 1\nesmbl_sum_results.to_csv('esmbl_sum_result_kaggle.csv', index=False)","f9cd2bbc":"esmbl_vote_results = pd.DataFrame({'Label' : voting_classifier(X_comp, trained_models)})\nesmbl_vote_results = esmbl_vote_results.reset_index().rename(columns={'index' : 'ImageId'})\nesmbl_vote_results['ImageId'] = esmbl_vote_results['ImageId'] + 1\nesmbl_vote_results.to_csv('esmbl_vote_result_kaggle.csv', index=False)","6e98b70b":"This gives a score of:","1e7d4d49":"# Create train, validation and test datasets","efe4605f":"Let's get the score of the best performing model:","a00c648b":"# Import of libraries and import of the data","7f3b8907":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))","9619c93a":"## Ensemble classifier based on majority vote:","858d76e9":"Let's check whether the labels are evenly distributed:","e753d73a":"### Plot images","024f3bd6":"No missing values - perfect.","29abdbb7":"Let's try if we get a different result by determining the label with a majority vote of the six different models:","9d155eea":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))","9d6c6948":"Hellouu peoples.\n\nThis notebook presents my results of playing around with CNNs in Keras. I train seven different architectures and use them two create an ensemble classifier in order to compare accuracies. Why seven? I am so glad you asked. The number is motivated by a mixture of random coincidence and [this epic movie](https:\/\/en.wikipedia.org\/wiki\/Seven_Samurai). \n\nBefore I start I want to give credit to [Yassine Ghouzam's notebook](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6), which I used as inspiration and which is probably the better notbook if you are working with Keras for the first time. You might also want to have a look at [this](http:\/\/cs231n.github.io\/convolutional-networks\/#architectures), which helped me a lot.\n\nThis is what you're going to find here:\n\n__1. Import of libraries and import of the data__\n\n__2. First overview of data__\n  1. Plot images\n  2. Check distribution\n  3. Check for missing values\n  \n__3. Create train, validation and test datasets__\n\n__4. Define and train the convolutional neural networks__\n  1. Model 1\n  2. Model 2\n  3. Model 3\n  4. Model 4\n  5. Model 5\n  6. Model 6\n  7. Model 7\n  \n__5. Ensemble classifiers and confusion matrices__\n  1. Overview of model performance so far:\n  2. Ensemble classifier based on summing the probabilities\n  3. Ensemble classifier based on a majority vote\n  \n__6. Conlusion__\n\n__7. Output routines__","29f84a08":"## Ensemble classifier based on summing the probabilities:","27a55881":"## Model 6","d7d6c7fa":"This results in a accuracy of:","912c6142":"# Define and train the convolutional neural networks","b41a38da":"# Ensemble classifiers and confusion matrices","41d5058c":"As a reference, let's plot the confusion matrix for this model:","ba7cd418":"# Output routines","0ce2a4cd":"Let's plot the accuracies again:","59e6032f":"### Competition predictions of the ensemble based on the sum:","d9685cea":"### Competition predictions of the ensemble based on the vote:","8f0bd429":"#Optionally load the models from disk:\nmodel1 = load_model('model_1.h5')\nmodel2 = load_model('model_2.h5')\nmodel3 = load_model('model_3.h5')\nmodel4 = load_model('model_4.h5')\nmodel5 = load_model('model_5.h5')\nmodel6 = load_model('model_6.h5')\nmodel7 = load_model('model_7.h5')","96962ff2":"## Model 7","3a655987":"Which we can compare to the average of the six models:","ca1f45ac":"Let's create an overview of the data so that we get a feeling of what we're dealing with. You can change the displayed number by changing *sel_int*.","bdd1fd5b":"## Model 5","3a831885":"Let's make sure there is nothing going wrong with the distribution of the labels in the three sets:","17c65427":"The confusion matrix for the summing classifier looks like this:","e727fb45":"As usual we need to split our data into a training, validation and test data set.","10be45bf":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))","467c09db":"### Check distribution","e006c07e":"## Model 1","277930e8":"### Competition predictions of the best single model:","aab5890f":"# Conclusion","be98061e":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))","a452c8fc":"Let's make a list and plot the performance of the created models:","1d11c6a8":"## Model 4","82f0f437":"# First overview of data","b24939e7":"So that's it. While I wouldn't trust the accuracies determined from a small test set of ~2000 images too much, it seems that the ensemble enhances the accurance beyond what the best single model can do.","43806175":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))","eb868143":"### Check for missing values","5db2c23d":"Here's where the meat comes on the table. In the next sections the six CNNs are defined and trained using a GPU and the ImageDataGenerator from Keras to make the networks more robust.","34eb6a24":"And here is the confusing matrix we see that we make different mistakes (but not more or less):","f89fa754":"## Model 2","e3e4aa6a":"## Overview of model performance so far:","cb4fbca4":"## Model 3","7ca1728c":"# Playing with ensemble classifiers based on seven different CNN models (~99.5 % accuracy)","fb4d6d30":"Now lets see whether we can get a better classification by summing up the probabilities of the six different models:","8d83e0a8":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))","1ff6ff6f":"plot_model(model1, show_shapes=True, to_file='Network_9.png')\nSVG(model_to_dot(model1, show_shapes=True).create(prog='dot', format='svg'))"}}