{"cell_type":{"afdad1cc":"code","58aaf523":"code","20f5bb2d":"code","bc6e8ba0":"code","9e7c11ff":"code","41b32317":"code","c66c49ee":"code","6b99a329":"code","660e62ca":"code","8d2f81e8":"code","ee27d667":"code","4d8d7ba5":"code","69e2b17c":"code","6531936f":"code","f3964473":"code","daeb7bbe":"code","de5dc870":"code","7211aa6f":"code","685c2424":"code","f692edad":"code","435053fd":"code","70bcddcd":"code","43d5a442":"code","49d27c14":"code","77da048b":"code","df6ecb9c":"code","48d97112":"code","513475cc":"code","658ba2c7":"code","afe76fdb":"code","cc31bd06":"code","97a009a9":"code","c2f0dbb9":"code","ac0b3347":"code","4d3ad3ef":"code","f9526d64":"code","685396cf":"code","47ffcd64":"markdown","5d75e89a":"markdown","588d6b9d":"markdown","136de73e":"markdown","e4df48a9":"markdown","1636e436":"markdown","f49d10da":"markdown","d55aac8b":"markdown","f18be04b":"markdown","63169b27":"markdown","82ba3a73":"markdown","875666ab":"markdown"},"source":{"afdad1cc":"import numpy as np\nimport pandas as pd\nimport spacy\nimport seaborn as sns\nimport matplotlib.pyplot as plt","58aaf523":"data = pd.read_csv(\"..\/input\/disaster-tweets\/tweets.csv\")\ndata.head()","20f5bb2d":"data.text[0]","bc6e8ba0":"data.shape","9e7c11ff":"data[\"numberOfcaractere\"] = data.text.str.len()","41b32317":"data.numberOfcaractere.hist(figsize=(14,8))","c66c49ee":"plt.figure(figsize=(14,8))\nsns.histplot(x=\"numberOfcaractere\", data=data, hue=\"target\", kde=True)","6b99a329":"data[\"numberOfWords\"] = data.text.str.split().map(lambda x: len(x))\n#data.text.str.split().map( lambda x : len(x)).hist(figsize=(14,8))","660e62ca":"plt.figure(figsize=(14,8))\nsns.histplot(x=\"numberOfWords\", data=data, hue=\"target\", kde=True)","8d2f81e8":"data.location.value_counts()","ee27d667":"data.location.isnull().value_counts()","4d8d7ba5":"plt.figure(figsize=(12,8))\nsns.countplot(x=\"target\", data=data)","69e2b17c":"#preprocessing\n\nimport string\nimport re\n\ndef remove_url(text):\n    url = re.compile(r\"https?:\/\/\\S+|www\\.\\S+\")\n    return url.sub(r\"\", text)\n\ndef remove_ablazeWord(text):\n    return text.replace(\"ablaze\", \"\")\n\ndef remove_punct(text):\n    translator = str.maketrans(\"\", \"\", string.punctuation)\n    return text.translate(translator)\n\ndef remove_tripleDot(text):\n    return text.replace('\\u2026', \"\")\n\ndef remove_emojis(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)","6531936f":"data[\"text\"] = data.text.map(remove_url)\ndata[\"text\"] = data.text.map(remove_punct)\ndata[\"text\"] = data.text.map(remove_tripleDot)\ndata[\"text\"] = data.text.map(remove_emojis)\ndata[\"text\"] = data.text.map(remove_ablazeWord)","f3964473":"#remove stop word\n\nfrom spacy.lang.en import STOP_WORDS\n\nstopwords = set(STOP_WORDS)\n\nnlp = spacy.load('en_core_web_sm')\n\ndef remove_stopword(text):\n    filleterd_text = [word.lemma_.lower() for word in nlp.tokenizer(text) if not word.text.lower() in stopwords]\n    return \" \".join(filleterd_text)","daeb7bbe":"data[\"text\"] = data.text.map(remove_stopword)","de5dc870":"from wordcloud import WordCloud\n\ncomment_words = ''\n  \n# iterate through the csv file\nfor val in data.text:\n      \n    # typecaste each val to string\n    val = str(val)\n  \n    # split the value\n    tokens = val.split()\n      \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n      \n    comment_words += \" \".join(tokens)+\" \"\n  \n\nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='black',\n                min_font_size = 10,\n                colormap='Set2', collocations=False).generate(comment_words)","7211aa6f":"def plot_word(wordcloud):\n    plt.figure(figsize = (20, 10), facecolor = None)\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.tight_layout(pad = 0)\n    plt.show()","685c2424":"plot_word(wordcloud=wordcloud)","f692edad":"from sklearn.model_selection import train_test_split, learning_curve\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.naive_bayes import MultinomialNB","435053fd":"X = data.text\ny = data[\"target\"]","70bcddcd":"tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_df=0.9, min_df=10)\ntfidf_matrix = tfidf_vectorizer.fit_transform(X)\ndense = tfidf_matrix.todense()\ndense.shape","43d5a442":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","49d27c14":"svm = make_pipeline(TfidfVectorizer(ngram_range=(1, 2)),TfidfTransformer(), SVC(kernel=\"linear\", gamma=\"auto\", C=2, random_state=0))","77da048b":"def evaluate(model):\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    print(confusion_matrix(y_true=y_test, y_pred=y_pred))\n    print(classification_report(y_true=y_test, y_pred=y_pred))\n\n    # N, train_score, val_score = learning_curve(model, X_train, y_train, cv=4)\n    # plt.figure(figsize=(12,8))\n    # plt.plot(N, train_score.mean(axis=1), label='train score')\n    # plt.plot(N, val_score.mean(axis=1), label='validation score')\n    # plt.legend()","df6ecb9c":"evaluate(svm)","48d97112":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.layers import Dense, Dropout, concatenate, Input, Embedding, SpatialDropout1D, GlobalAvgPool1D, GlobalMaxPool1D\nfrom tensorflow.keras.models import Model","513475cc":"max_length = 50\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\n\ntokenizer = Tokenizer(oov_token=oov_tok)\ntokenizer.fit_on_texts(X_train)\n\nword_index = tokenizer.word_index\n\ntrain_sequences = tokenizer.texts_to_sequences(X_train)\ntrain_padded = pad_sequences(train_sequences, maxlen=max_length, padding= padding_type, truncating=trunc_type)\n\ntest_sequences = tokenizer.texts_to_sequences(X_test)\ntest_padded = pad_sequences(test_sequences, maxlen=max_length, padding= padding_type, truncating=trunc_type)","658ba2c7":"vocab_size= len(word_index) + 1\nembedding_dim = 16","afe76fdb":"vocab_size","cc31bd06":"train_padded = np.array(train_padded)\ntest_padded = np.array(test_padded)\ntrain_labels = np.array(y_train)\ntest_labels = np.array(y_test)","97a009a9":"print(train_padded.shape)\nprint(test_padded.shape)\nprint(train_labels.shape)","c2f0dbb9":"model = keras.models.Sequential([\n    keras.layers.Embedding(input_dim=vocab_size,output_dim= embedding_dim, input_length=max_length),\n    keras.layers.GlobalAvgPool1D(),\n    keras.layers.Dense(32, activation='relu'),\n    keras.layers.Dense(16, activation=\"relu\"),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\noptRm = keras.optimizers.RMSprop(lr=0.001, rho=0.9)\n\nmodel.compile(loss=\"binary_crossentropy\", optimizer=optRm, metrics=[\"accuracy\"])\n\nmodel.summary()\n","ac0b3347":"history = model.fit(train_padded, train_labels, validation_split=.1, epochs=20, batch_size=32)","4d3ad3ef":"import matplotlib.pyplot as plt\npd.DataFrame(history.history).plot(figsize=(14,8))\nplt.grid(True)","f9526d64":"y_pred = model.predict(test_padded)\ny_final = np.array([1 if pred > .5 else 0 for pred in y_pred])\ny_final","685396cf":"print(confusion_matrix(y_true=y_test, y_pred=y_final))\nprint(classification_report(y_true=y_test, y_pred=y_final))","47ffcd64":"## 3. Location","5d75e89a":"## 1. Number of caracteres in tweets","588d6b9d":"# TEXT PROCESSING","136de73e":"# EDA","e4df48a9":"## 1. SVM","1636e436":"# WORDCLOUD","f49d10da":"### Remarques:\n- le nombre de caracteres dans les phrases est entre 20 - 140\n- les tweets qui font reference a un desastre on souvant un nombre de caractere comprise entre 100 et 130","d55aac8b":"### Remarques:\n- le nombre de mots dans les tweets sont entres 3 et 30, mais generalement au tour de 15-25","f18be04b":"## 2. Number of words","63169b27":"# MODEL","82ba3a73":"## 3. Target","875666ab":"## 2.Build NN for text classification"}}