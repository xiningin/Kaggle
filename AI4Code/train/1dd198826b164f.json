{"cell_type":{"4c2651cb":"code","283701d1":"code","46980a49":"code","5c90f566":"code","aa577702":"code","01e99634":"code","1bc16805":"code","120d438b":"code","869fe504":"code","7caf7b57":"code","112cb1ff":"code","edc65247":"code","475abab1":"code","0fa60718":"code","e40c4939":"code","3925053d":"code","df7185cd":"code","8827c360":"code","5e878a21":"code","a3079512":"code","9d1c1ce5":"markdown","8550a5d9":"markdown","511c8b5a":"markdown","dd83bd16":"markdown","e9bff388":"markdown","3e345db1":"markdown","968f2733":"markdown","fad79094":"markdown"},"source":{"4c2651cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","283701d1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","46980a49":"df=pd.read_csv('..\/input\/texas-wind-turbine-dataset-simulated\/TexasTurbine.csv')\ndf.head()","5c90f566":"df.columns","aa577702":"df['Pressure | (atm)'].max()","01e99634":"df.isna().sum()","1bc16805":"from sklearn.preprocessing import StandardScaler\n# create an object of the StandardScaler\nscaler = StandardScaler()\nscaler.fit(np.array(df['Wind speed | (m\/s)']).reshape(-1,1))\n# transform the data\ndf['Wind speed | (m\/s)'] = scaler.transform(np.array(df['Wind speed | (m\/s)']).reshape(-1,1))\nscaler.fit(np.array(df['Wind direction | (deg)']).reshape(-1,1))\ndf['Wind direction | (deg)']=scaler.transform(np.array(df['Wind direction | (deg)']).reshape(-1,1))\nscaler.fit(np.array(df[\"Air temperature | ('C)\"]).reshape(-1,1))\ndf[\"Air temperature | ('C)\"]=scaler.transform(np.array(df[\"Air temperature | ('C)\"]).reshape(-1,1))\nscaler.fit(np.array(df[\"System power generated | (kW)\"]).reshape(-1,1))\ndf[\"System power generated | (kW)\"]=scaler.transform(np.array(df[\"System power generated | (kW)\"]).reshape(-1,1))\ndf.head()","120d438b":"from sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import  RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error","869fe504":"train_X = df.drop(columns=['Time stamp','System power generated | (kW)'])\ntrain_Y = df['System power generated | (kW)']\n\n# randomly split the data\ntrain_x, test_x, train_y, test_y = train_test_split(train_X, train_Y,test_size=0.2,random_state=0)\n\n# shape of train and test splits\ntrain_x.shape, test_x.shape, train_y.shape, test_y.shape","7caf7b57":"model_LR = LinearRegression()\n\n# fit the model with the training data\nmodel_LR.fit(train_x, train_y)\n# predict the target on train and test data \npredict_train = model_LR.predict(train_x)\npredict_test  = model_LR.predict(test_x)\nfrom sklearn.metrics import mean_squared_error, r2_score\n# Root Mean Squared Error on train and test date\nprint('RMSE on train data: ', mean_squared_error(train_y, predict_train)**(0.5))\nprint('RMSE on test data: ',  mean_squared_error(test_y, predict_test)**(0.5))\nprint('Coefficient of determination: %.2f'\n      % r2_score(test_y, predict_test))","112cb1ff":"df=pd.read_csv('..\/input\/texas-wind-turbine-dataset-simulated\/TexasTurbine.csv')\ntrain_X = df.drop(columns=['Time stamp','System power generated | (kW)'])\ntrain_Y = df[['System power generated | (kW)']]\n\n# randomly split the data\ntrain_x, test_x, train_y, test_y = train_test_split(train_X, train_Y,test_size=0.2,random_state=0)\n\n# shape of train and test splits\ntrain_x.shape, test_x.shape, train_y.shape, test_y.shape","edc65247":"## Pipelines Creation\n## 1. Data Preprocessing by using Standard Scaler\n## 2. Reduce Dimension using PCA\n## 3. Apply  Classifier","475abab1":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import  ExtraTreesRegressor\nimport xgboost\nfrom xgboost import XGBRegressor\npipeline_lr=Pipeline([('scalar1',StandardScaler()),\n                     ('pca1',PCA()),\n                     ('lr_regressor',LinearRegression())])\npipeline_dt=Pipeline([('scalar2',StandardScaler()),\n                     ('pca2',PCA()),\n                     ('dt_regressor',DecisionTreeRegressor(min_samples_leaf=.0001))])\npipeline_randomforest=Pipeline([('scalar3',StandardScaler()),\n                     ('pca3',PCA()),\n                     ('rf_regressor',RandomForestRegressor(n_estimators=500,min_samples_leaf=.0001))])\npipeline_gradientbr=Pipeline([('scalar4',StandardScaler()),\n                     ('pca4',PCA()),\n                     ('gb_regressor',GradientBoostingRegressor(n_estimators=1000))])\npipeline_etr=Pipeline([('scalar5',StandardScaler()),\n                     ('pca5',PCA()),\n                     ('et_regressor',ExtraTreesRegressor(n_estimators=1000))])\npipeline_xgbr=Pipeline([('scalar6',StandardScaler()),\n                     ('pca6',PCA()),\n                     ('xgb_regressor',XGBRegressor(n_estimators=1000))])","0fa60718":"## Let's make the list of pipelines\npipelines = [pipeline_lr, pipeline_dt, pipeline_randomforest, pipeline_gradientbr,pipeline_etr, pipeline_xgbr]","e40c4939":"best_accuracy=0.0\nbest_classifier=0\nbest_pipeline=\"\"","3925053d":"# Dictionary of pipelines and classifier types for ease of reference\npipe_dict = {0: 'Linear Regression', 1: 'Decision Tree', 2: 'RandomForest', 3: 'Gradient Boosting', 4:'Extra Trees', 5:'XGBoost'}\n# Fit the pipelines\nfor pipe in pipelines:\n    pipe.fit(train_x,train_y)","df7185cd":"for i,model in enumerate(pipelines):\n    print(\"{} Test Accuracy: {}\".format(pipe_dict[i],model.score(test_x,test_y)))","8827c360":"for i,model in enumerate(pipelines):\n    if model.score(test_x,test_y)>best_accuracy:\n        best_accuracy=model.score(test_x,test_y)\n        best_pipeline=model\n        best_classifier=i\nprint('Classifier with best accuracy:{}'.format(pipe_dict[best_classifier]))","5e878a21":"import numpy as np\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = [7.50, 3.50]\nplt.rcParams[\"figure.autolayout\"] = True\n\nfig, axes = plt.subplots(3, 2)\nfor i,model in enumerate(pipelines):\n  plt.subplot(3,2,i+1)\n  plt.scatter(test_y,model.predict(test_x))\n  plt.title(pipe_dict[i])\nplt.show()","a3079512":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfor i,model in enumerate(pipelines):\n  print(pipe_dict[i]+\":\")\n  print(\"MSE: {}\".format(mean_squared_error(test_y,model.predict(test_x))))\n  print(\"RMSE: {}\".format(mean_squared_error(test_y,model.predict(test_x),squared=False)))\n  print(\"MAE: {}\".format(mean_absolute_error(test_y,model.predict(test_x))))\n  print(\"-------\")","9d1c1ce5":"# Performing Simple Linear Regression without pipeline creation.","8550a5d9":"# Reading in the dataset","511c8b5a":"# Using a pipeline","dd83bd16":"# Importing the required libraries","e9bff388":"The different techniques of supervised learning used in this case include: Linear Regression, Decision Tree Regression, Random Forest Regression, Gradient Boosting Regression, Extra Trees Regression and XGBoost.","3e345db1":"# Testing the models and viewing the accuracies and loss metrics","968f2733":"# So, our job here would be to predict the System power generated given the rest of these parameters.","fad79094":"# This is the end of the code. If you liked it, please feel free to upvote my work and provide any suggestions to improve the code. Thank you!"}}