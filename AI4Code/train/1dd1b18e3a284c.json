{"cell_type":{"8b2ede7e":"code","7c34f172":"code","2245c7c3":"code","7701d029":"code","ee89f04e":"code","1e32b3f3":"code","3bcf89d3":"code","156dc45d":"code","c536fc7b":"code","1601c06b":"code","08a4a149":"code","06ee27cd":"code","98c3532b":"code","12f9b122":"code","0ce582c2":"code","ac5b17e0":"code","ff41c0b7":"code","e88f6c68":"code","0b2a71f9":"code","0b10aa0a":"code","58b38966":"code","6f4b4cc2":"code","80c9bc29":"code","bea5b7ff":"code","ab4831cc":"code","08f8b023":"code","a3b270c7":"code","730396fd":"code","77642778":"code","bd90af17":"code","74a0c894":"code","47816146":"code","917d844d":"code","2a7e2158":"code","2622f15a":"code","9b045bf2":"code","f8968c70":"code","4d5f7c83":"markdown","e2ffc8e5":"markdown","0b4c039d":"markdown","d5c8351c":"markdown","b5cedb61":"markdown","f19fe8cc":"markdown","14bb9409":"markdown","6f3b253e":"markdown","a4b8e7d3":"markdown","c83e5b57":"markdown","46c10f3a":"markdown","cb12965d":"markdown"},"source":{"8b2ede7e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\n\nreview_dataset_path=\"\/kaggle\/input\/movie-review\/movie_reviews\/movie_reviews\"\nprint(os.listdir(review_dataset_path))","7c34f172":"#Positive and negative reviews folder paths \npos_review_folder_path=review_dataset_path+\"\/\"+\"pos\"\nneg_review_folder_path=review_dataset_path+\"\/\"+\"neg\"","2245c7c3":"#Positive and negative file names\npos_review_file_names=os.listdir(pos_review_folder_path)\nneg_review_file_names=os.listdir(neg_review_folder_path)","7701d029":"def load_text_from_textfile(path):\n    file=open(path,\"r\")\n    review=file.read()\n    file.close()\n    \n    return review\n\ndef load_review_from_textfile(path):\n    return load_text_from_textfile(path)","ee89f04e":"def get_data_target(folder_path, file_names, review_type):\n    data=list()\n    target =list()\n    for file_name in file_names:\n        full_path = folder_path + \"\/\" + file_name\n        review =load_review_from_textfile(path=full_path)\n        data.append(review)\n        target.append(review_type)\n    return data, target","1e32b3f3":"pos_data, pos_target=get_data_target(folder_path=pos_review_folder_path,\n               file_names=pos_review_file_names,\n               review_type=\"positive\")\nprint(\"Positive data ve target builded...\")\nprint(\"positive data length:\",len(pos_data))\nprint(\"positive target length:\",len(pos_target))","3bcf89d3":"neg_data, neg_target = get_data_target(folder_path = neg_review_folder_path,\n                                      file_names= neg_review_file_names,\n                                      review_type=\"negative\")\nprint(\"Negative data ve target builded..\")\nprint(\"negative data length :\",len(neg_data))\nprint(\"negative target length :\",len(neg_target))","156dc45d":"data = pos_data + neg_data\ntarget_ = pos_target + neg_target\nprint(\"Positive and Negative sets concatenated\")\nprint(\"data length :\",len(data))\nprint(\"target length :\",len(target_))","c536fc7b":"data1=pd.DataFrame(data)\n","1601c06b":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(target_)\ntarget = le.transform(target_)","08a4a149":"data2=pd.DataFrame(target)","06ee27cd":"#det = pd.concat([location, food], join = 'outer', axis = 1)","98c3532b":"new_data=pd.concat([data1,data2],join='outer',axis=1)","12f9b122":"new_data.head()","0ce582c2":"#df.rename(columns={\"A\": \"a\", \"B\": \"c\"})","ac5b17e0":"new_data.columns","ff41c0b7":"new_data.columns=['reviews','setiment']","e88f6c68":"print(le.inverse_transform([0,0,0,1,1,1]))","0b2a71f9":"new_data.head()","0b10aa0a":"new_data['reviews'][1]","58b38966":"print(new_data.setiment.value_counts())","6f4b4cc2":"new_data.loc[0,'reviews'][-50:]","80c9bc29":"import re\ndef preprocessor(text):\n    text =re.sub('<[^>]*>', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n    return text\npreprocessor('This is a :) test :-( !')\n","bea5b7ff":"def replacer(text):\n    text=text.replace('\\n','')\n    text=text.replace('(\\)','')\n    text=text.lower()\n    text=text.replace('-',' ')\n    text=text.replace('\\ ' ,'')\n    return text\n    ","ab4831cc":"new_data['reviews'][2]","08f8b023":"replacer(new_data['reviews'][2])","a3b270c7":"preprocessor (new_data['reviews'][2])","730396fd":"new_data['reviews']=new_data['reviews'].apply(preprocessor)","77642778":"from nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\nporter=PorterStemmer()","bd90af17":"def tokenizer(text):\n    return text.split()\ntokenizer('runners like running thus they run')","74a0c894":"def tokenizer_stemmer(text):\n    return[porter.stem(word) for word in text.split()]\ntokenizer_stemmer(\"runners like running thus they run\")","47816146":"import nltk\nfrom nltk.corpus import stopwords\n\nstop_w=stopwords.words(\"english\")\n[w for w in tokenizer_stemmer('ship and lion in is and sleep sleeps with sleeping')[-11:]if w not in stop_w]","917d844d":"new_data.columns","2a7e2158":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(strip_accents=None,\n                                lowercase=True,\n                                preprocessor=None, # defined preprocessor in Data Cleaning\n                                tokenizer=tokenizer_stemmer,\n                                use_idf=True,\n                                norm='l2',\n                                smooth_idf=True)\ny = new_data.setiment.values\nX = tfidf.fit_transform(new_data.reviews)","2622f15a":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.5, shuffle=True)\n","9b045bf2":"\nfrom sklearn.linear_model import LogisticRegressionCV\n\nclf = LogisticRegressionCV(cv=5,\n                            scoring='accuracy',\n                            random_state=1,\n                            n_jobs=-1,\n                            verbose=3,  \n                            max_iter=300).fit(X_train, y_train)","f8968c70":"clf.score(X_test,y_test)","4d5f7c83":"\"positive\"=1 ,\"negative\"=0","e2ffc8e5":"stopword\n\ncommomn words that always occur in a sentance like \"is\" \"a\" \"and\"","0b4c039d":"TfidfVectorizer method that carries out all the above steps in one single step.","d5c8351c":"# Tokanisation of documents","b5cedb61":"the below code is for the removal of html codes and emojis from the text document ","f19fe8cc":"IN this dataset we have to remove \/n and \/ charecters from the text for processing","14bb9409":"# Data preprocessing","6f3b253e":"# loading the dataset","a4b8e7d3":"# Document Classification using Logistic Regression\n\nwe have target vector  in y and tdidfvalues in x.lets split the data into train and text sets.then fit the training into logistic model\n\nNote that instead of manually hyperparamter tuning our model, we\u2019re using LogisticRegressionCV to specify the number of cross-validation folds we want to do to tune the hyperparameter \u2014 that is 5-fold cross-validation","c83e5b57":"In this section, we will represent our data as a collection of words or tokens; we will also be performing word-level preprocessing tasks such as stemming. To achieve this, we will utilize the natural language toolkit, or nltk.\n\n**Stemming** is a technic that reduces the inflectional forms, and sometimes derivationally related forms, of a common word to a base form. For example, the words \u2018organizer\u2019 and \u2018organizing\u2019 stems from the base word \u2018organize\u2019. So, stemming is conventionally referred to as a crude heuristic process that \u2018chops\u2019 off the ends of words, in the hope of achieving the goal correctly most of the time \u2014 this often includes the removal of derivational affixes.gives as the root of the word from derivation of that word","46c10f3a":"# converting the text data into TF-IDF","cb12965d":"# model evaluvation"}}