{"cell_type":{"d5f6f578":"code","7c8cd906":"code","0727ff01":"code","31480ff3":"code","9abb8b53":"code","784f30b0":"code","a268a0c9":"code","dc452778":"code","a3b9270a":"code","73ed8d23":"code","24e94b47":"code","c17a2e4f":"code","6dd45353":"code","ed4bc22a":"code","19f9f426":"code","eb58daeb":"code","399bb947":"code","871c1759":"code","437317c0":"code","d4724def":"code","9e20690a":"code","16484e8b":"code","fe6e640b":"code","e6c38d55":"code","0b22661e":"code","be7f7de1":"code","d91c74e3":"code","66145282":"code","6e595218":"code","903e083a":"code","6639d293":"code","08f63fb8":"code","e4fabbce":"code","12946d37":"code","9903e61c":"code","fd9d653e":"code","0814f9f4":"code","c370495f":"code","39efd89b":"code","5a2b2852":"code","8d1ce7a8":"code","82ea9333":"code","0ace344d":"code","1206144d":"code","4032689d":"code","43c7a0d9":"code","f7d75004":"markdown","030330c4":"markdown","c3341bb3":"markdown","a485d4b0":"markdown","52c17339":"markdown","80a911e4":"markdown","383e4f60":"markdown","c17508e2":"markdown","272c435c":"markdown","59f29193":"markdown","84e736af":"markdown","d8491b5e":"markdown","e0f6569e":"markdown","49908a68":"markdown","cbe45925":"markdown","5dca9e48":"markdown","e226f3ae":"markdown","bed1c8b1":"markdown","74782d70":"markdown","8ec7a239":"markdown","1e6e2e31":"markdown","65b21516":"markdown","5c580ace":"markdown","1715cbba":"markdown","3c5bad29":"markdown","55ccc28a":"markdown","69e96cb7":"markdown","d8100712":"markdown","8a8f3864":"markdown","887a430a":"markdown","354ec226":"markdown","2f9a494e":"markdown","a56356f4":"markdown","61d664f1":"markdown","ca07e230":"markdown","36c34864":"markdown","0aab4092":"markdown","d3a45424":"markdown","64483eb0":"markdown","fb9adba2":"markdown","7972c4d8":"markdown","436437f2":"markdown","a67c7c02":"markdown","484e27ba":"markdown","b1d6296c":"markdown","22cfcb3f":"markdown"},"source":{"d5f6f578":"!pip install gensim\n!pip install keras \n!pip install pandas ","7c8cd906":"# DataFrame\nimport pandas as pd\n\n# Matplot\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Scikit-learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.manifold import TSNE\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\nfrom keras import utils\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\n# nltk\nimport nltk\nfrom nltk.corpus import stopwords\nfrom  nltk.stem import SnowballStemmer\n\n# Word2vec\nimport gensim\n\n# Utility\nimport re\nimport numpy as np\nimport os\nfrom collections import Counter\nimport logging\nimport time\nimport pickle\nimport itertools\n\n# Set log\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","0727ff01":"nltk.download('stopwords')","31480ff3":"# DATASET\nDATASET_COLUMNS = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\nDATASET_ENCODING = \"ISO-8859-1\"\n\n# TEXT CLENAING\nTEXT_CLEANING_RE = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"","9abb8b53":"df = pd.read_csv('..\/input\/training.1600000.processed.noemoticon.csv', encoding =DATASET_ENCODING , names=DATASET_COLUMNS)","784f30b0":"print(\"Dataset size:\", len(df))","a268a0c9":"df.head(5)","dc452778":"decode_map = {0: \"NEGATIVE\", 2: \"NEUTRAL\", 4: \"POSITIVE\"}\ndef decode_sentiment(label):\n    return decode_map[int(label)]","a3b9270a":"%%time\ndf.target = df.target.apply(lambda x: decode_sentiment(x))","73ed8d23":"stop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")","24e94b47":"def preprocess(text, stem=False):\n    # Remove link,user and special characters\n    text = re.sub(TEXT_CLEANING_RE, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","c17a2e4f":"%%time\ndf.text = df.text.apply(lambda x: preprocess(x))","6dd45353":"df.head(5)","ed4bc22a":"target_cnt = Counter(df.target)\n\nplt.figure(figsize=(16,8))\nplt.bar(target_cnt.keys(), target_cnt.values())\nplt.title(\"Dataset labels distribuition\")","19f9f426":"# TRAINING\nTRAIN_SIZE = 0.8\n\n# WORD2VEC \nW2V_SIZE = 300\nW2V_WINDOW = 7\nW2V_EPOCH = 32\nW2V_MIN_COUNT = 10\n\n# KERAS\nSEQUENCE_LENGTH = 300\nEPOCHS = 8\nBATCH_SIZE = 1024\n\n# SENTIMENT\nPOSITIVE = \"POSITIVE\"\nNEGATIVE = \"NEGATIVE\"\nNEUTRAL = \"NEUTRAL\"\nSENTIMENT_THRESHOLDS = (0.4, 0.7)\n\n# EXPORT\nKERAS_MODEL = \"model.h5\"\nWORD2VEC_MODEL = \"model.w2v\"\nTOKENIZER_MODEL = \"tokenizer.pkl\"\nENCODER_MODEL = \"encoder.pkl\"","eb58daeb":"df_train, df_test = train_test_split(df, test_size=1-TRAIN_SIZE, random_state=42)\nprint(\"TRAIN size:\", len(df_train))\nprint(\"TEST size:\", len(df_test))","399bb947":"%%time\ndocuments = [_text.split() for _text in df_train.text] \n\nw2v_model = gensim.models.word2vec.Word2Vec(size=W2V_SIZE, \n                                            window=W2V_WINDOW, \n                                            min_count=W2V_MIN_COUNT, \n                                            workers=8)\n\nw2v_model.build_vocab(documents)","871c1759":"words = w2v_model.wv.vocab.keys()\nvocab_size = len(words)\nprint(\"Vocab size\", vocab_size)","437317c0":"%%time\nw2v_model.train(documents, total_examples=len(documents), epochs=W2V_EPOCH)","d4724def":"w2v_model.most_similar(\"hate\")","9e20690a":"w2v_model.most_similar(\"love\")","16484e8b":"%%time\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(df_train.text)\n\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Total words\", vocab_size)","fe6e640b":"%%time\nx_train = pad_sequences(tokenizer.texts_to_sequences(df_train.text), maxlen=SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(df_test.text), maxlen=SEQUENCE_LENGTH)","e6c38d55":"labels = df_train.target.unique().tolist()\nlabels.append(NEUTRAL)\nlabels","0b22661e":"encoder = LabelEncoder()\nencoder.fit(df_train.target.tolist())\n\ny_train = encoder.transform(df_train.target.tolist())\ny_test = encoder.transform(df_test.target.tolist())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train\",y_train.shape)\nprint(\"y_test\",y_test.shape)","be7f7de1":"print(\"x_train\", x_train.shape)\nprint(\"y_train\", y_train.shape)\nprint()\nprint(\"x_test\", x_test.shape)\nprint(\"y_test\", y_test.shape)","d91c74e3":"y_train[:10]","66145282":"embedding_matrix = np.zeros((vocab_size, W2V_SIZE))\nfor word, i in tokenizer.word_index.items():\n  if word in w2v_model.wv:\n    embedding_matrix[i] = w2v_model.wv[word]\nprint(embedding_matrix.shape)","6e595218":"embedding_layer = Embedding(vocab_size, W2V_SIZE, weights=[embedding_matrix], input_length=SEQUENCE_LENGTH, trainable=False)","903e083a":"model = Sequential()\nmodel.add(embedding_layer)\nmodel.add(Dropout(0.5))\nmodel.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","6639d293":"model.compile(loss='binary_crossentropy',\n              optimizer=\"adam\",\n              metrics=['accuracy'])","08f63fb8":"callbacks = [ ReduceLROnPlateau(monitor='val_loss', patience=5, cooldown=0),\n              EarlyStopping(monitor='val_acc', min_delta=1e-4, patience=5)]","e4fabbce":"%%time\nhistory = model.fit(x_train, y_train,\n                    batch_size=BATCH_SIZE,\n                    epochs=EPOCHS,\n                    validation_split=0.1,\n                    verbose=1,\n                    callbacks=callbacks)","12946d37":"%%time\nscore = model.evaluate(x_test, y_test, batch_size=BATCH_SIZE)\nprint()\nprint(\"ACCURACY:\",score[1])\nprint(\"LOSS:\",score[0])","9903e61c":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n \nepochs = range(len(acc))\n \nplt.plot(epochs, acc, 'b', label='Training acc')\nplt.plot(epochs, val_acc, 'r', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n \nplt.figure()\n \nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n \nplt.show()","fd9d653e":"def decode_sentiment(score, include_neutral=True):\n    if include_neutral:        \n        label = NEUTRAL\n        if score <= SENTIMENT_THRESHOLDS[0]:\n            label = NEGATIVE\n        elif score >= SENTIMENT_THRESHOLDS[1]:\n            label = POSITIVE\n\n        return label\n    else:\n        return NEGATIVE if score < 0.5 else POSITIVE","0814f9f4":"def predict(text, include_neutral=True):\n    start_at = time.time()\n    # Tokenize text\n    x_test = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=SEQUENCE_LENGTH)\n    # Predict\n    score = model.predict([x_test])[0]\n    # Decode sentiment\n    label = decode_sentiment(score, include_neutral=include_neutral)\n\n    return {\"label\": label, \"score\": float(score),\n       \"elapsed_time\": time.time()-start_at}  ","c370495f":"predict(\"I love the music\")","39efd89b":"predict(\"I like that\")","5a2b2852":"predict(\"I hate the sound\")","8d1ce7a8":"%%time\ny_pred_1d = []\ny_test_1d = list(df_test.target)\nscores = model.predict(x_test, verbose=1, batch_size=8000)\ny_pred_1d = [decode_sentiment(score, include_neutral=False) for score in scores]","82ea9333":"def plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=30)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90, fontsize=22)\n    plt.yticks(tick_marks, classes, fontsize=22)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=25)\n    plt.xlabel('Predicted label', fontsize=25)","0ace344d":"%%time\n\ncnf_matrix = confusion_matrix(y_test_1d, y_pred_1d)\nplt.figure(figsize=(12,12))\nplot_confusion_matrix(cnf_matrix, classes=df_train.target.unique(), title=\"Confusion matrix\")\nplt.show()","1206144d":"print(classification_report(y_test_1d, y_pred_1d))","4032689d":"accuracy_score(y_test_1d, y_pred_1d)","43c7a0d9":"model.save(KERAS_MODEL)\nw2v_model.save(WORD2VEC_MODEL)\npickle.dump(tokenizer, open(TOKENIZER_MODEL, \"wb\"), protocol=0)\npickle.dump(encoder, open(ENCODER_MODEL, \"wb\"), protocol=0)","f7d75004":"# 7.Build the Neural Network\n\nNext step is to build our prediction Neural network. We start with our embedding layer, followed by a dropout layer that is a trick to improve the quality of the learning, a Long short-Term memory that is a Recurrent neural network with a specific memory with 100 output, then a fully connected layer to a sigmoid function that take 0.4 for negative, 0.7 for neutral and the rest for Positive to decide the sentiment.","030330c4":"Finnaly, we train our model for **32** epochs (W2V_EPOCH).","c3341bb3":"Next, the model expects that each sequence(each training example) will be of the same length(same number of words\/tokens), so we applied the padding so each vector has the same length but with a maximum of sequence of **300** (SEQUENCE_LENGTH).\n\n![token](https:\/\/i.ibb.co\/L1wwKWt\/02.png)","a485d4b0":"# 1.Gather the dataset\n\n\nWe were searching tweets dataset that contains the text and the target sentiment. We found this dataset \"Sentiment140\". It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment, which is exactly what we need for a [deep learning][1] application.\n\nThe data contains the fields listed below :\n\n1. target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n2. ids: The id of the tweet ( 2087)\n3. date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n4. flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n5. user: the user that tweeted (robotickilldozr)\n6. text: the text of the tweet (Lyx is cool)\n\nWe are mostly interested on the target i.e. the sentiment and the text that we want to apply the sentiment for.\n\n![Sentiment Analysis](https:\/\/i.ibb.co\/z8c9qYB\/sentiment-fig-1-689.jpg)\n\n[1]: https:\/\/en.wikipedia.org\/wiki\/Deep_learning","52c17339":"# Introduction\n\nThis notebook aims to extract the sentiment out from users tweets. We label them at the end : Positive, Neutral or Negative. The final end is to create a set of applications such as statistical dashboard as the picture below. It is very important to set that we have worked on English words only.\n\n![Kibana](https:\/\/i.ibb.co\/xzf4FrK\/Capture.png)\n\nThe steps that we have applied are listed below :\n\n1. Gather the dataset\n\n2. Prepocess the data\n\n3. Apply the [EDA][3](Exploratory Data Analysis) - Visualization\n\n4. Word Vectorization\n\n5. Tekonization\n\n6. Create Embedding layer\n\n7. Build the neural network\n\n8. Evaluation\n\n9. Create the prediction web API\n\n10. Create Web application ( http:\/\/thefourtwofour.com )\n\n11. Create Real time Dashboard applications","80a911e4":"As mentionned before the data set contains 1.6 million raws.","383e4f60":"We got an accuracy around 80% which is satisfiable in this kind of applications.\n\nNext, we used the history to plot the learning and the loss curve.","c17508e2":"We then compiled our model, we used the binary cross entropy for the loss function and the Adam optimizer to reduce the cost function of the decent gradient by updating the weights more recurrently for the parameters.","272c435c":"We can see the data before the cleaning with the different columns.\n\nAfter that, we decode the target (Sentiment) that came with the data labeled as :\n\n**Neagaive: 0**\n\n**Neutral:  2**\n\n**Positive: 4**","59f29193":"It took us around 1 hour and 30 minutes to train the model, and we got an accuracy of around 80%. We tried also the same model with 16 epochs, we found that the accuracy and the learning curve didn't really improve comparing to the execution time that exceded 3 hours.","84e736af":"Next, we defined the data structure, columns and the encoding to read from the dataset as by the norm [ISO-8859-1][1].\n\nWe defined also the clearning regular expression that would delete any \"@\",\"https\",\"http\" and leave only the alphanumerical charachters because what we want from the tweet text is only the real comment.\n\n[1]: https:\/\/en.wikipedia.org\/wiki\/ISO\/IEC_8859-1","d8491b5e":"First, we split the data into 80% training and 20% test.","e0f6569e":"Now, let's talk about Word vectorization. We will use the Gensim implementation of Word2Vec. The first step was to prepare the text corpus for learning the embedding by creating word tokens, removing punctuation, removing stop words etc. The word2vec algorithm processes documents sentence by sentence.\n\nWord2Vec itself is not a deep neural network, but it turns input text into a numerical form that deep neural networks can process as inputs.\n\n![Word2Vec](https:\/\/i.ibb.co\/9pdNgxB\/2664.png)\n\nThe output of the Word2vec neural net is a vocabulary in which each item has a vector attached to it, which can be fed into a deep-learning net or simply queried to detect relationships between words. \n\n**Word2Vec Parameters**\n\n1. W2V_SIZE: The size means the dimensionality of word vectors. It defines the number of tokens used to represent each word. We choosed **300**\n\n2. W2V_WINDOW: The maximum distance between the target word and its neighboring word. For example,\"queen represents kingdom\" is the sentence if you have window=1 so \"represent\" is the neighbor here, if you have window=2, \"kingdom\" also is a  neighbor\". We choosed **7**\n\n3. W2V_MIN_COUNT: Ignores all words with total frequency lower than this count. We choosed **10**\n\n","49908a68":"Next, we download a known English stopwords from the library [nltk][1].\n\n[1]: https:\/\/www.nltk.org\/","cbe45925":"We found out that we did a good job, an accuracy of 80% for True positive and True negative.","5dca9e48":"Next, we applied the preprocessing function to the text column.","e226f3ae":"**Preprocess the tweet text**\n\nThe main target of this preprocessing is to clean the text and reduce the words that would compose our final vector for the word2Vec.\n\nFor this applied 3 steps :\n\n1. Get the stop words\n\nThe stop words are some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely.\n\nAn example of this list are : {\u2018ourselves\u2019, \u2018hers\u2019, \u2018between\u2019, \u2018yourself\u2019, \u2018but\u2019, \u2018again\u2019, \u2018there\u2019, \u2018about\u2019, \u2018once\u2019, \u2018during\u2019, \u2018out\u2019, \u2018very\u2019, \u2018having\u2019, \u2018with\u2019, \u2018they\u2019, \u2018own\u2019, \u2018an\u2019, \u2018be\u2019, \u2018some\u2019, \u2018for\u2019, \u2018do\u2019, \u2018its\u2019, \u2018yours\u2019, \u2018such\u2019, \u2018into\u2019, \u2018of\u2019, \u2018most\u2019, \u2018itself\u2019, \u2018other\u2019, \u2018off\u2019, \u2018is\u2019, \u2018s\u2019, \u2018am\u2019, \u2018or\u2019, \u2018who\u2019, \u2018as\u2019, \u2018from\u2019, \u2018him\u2019, \u2018each\u2019, \u2018the\u2019, \u2018them'}\n\n2. Get the stemmers\n\nThe goal is to reduce related words to a common stem. for example : ['frightening', 'frightened', 'frightens'] would be ['frighten']\n","bed1c8b1":"The process is to first split every text to a list of words, then create the model and build the vocabulary.","74782d70":"# 4.Word Vectorization\n\n**Expirement parameters**\n\nFirst of all, and for much clearence we define the expirement parameters.","8ec7a239":"We create a very important function that we will be using to predict the tweet text. \n\nFirst, we tokenize the text, we pad it, clean it and then pass it to the model to predict the sentiment.","1e6e2e31":"Finnaly, we export our models that we will need after for our API implementation.","65b21516":"**Import the libraries**","5c580ace":"We apply the decoding for plotting and visualization.","1715cbba":"# 3.Understand the data\n\nTo understand the dataset we applied the EDA. A bunch of plots and visualizations.\n\n**Negative & Positive count**\n\nWe just plot the target counts.","3c5bad29":"# 9.Create the prediction web API\n\nNext, we extract the model and created using Flask a web API to predict from a text the sentiment.\n\n# 10.Create Web application\n\nWe create based on the prediction API a web application that can be seen on http:\/\/thefourtwofour.com.\n\n![WebAPP](https:\/\/i.ibb.co\/54pZxXk\/Screenshot-25.png)\n\n# 11.Create Real time Dashboard applications\n\nWe want to apply our model to a real time data, we used Apache Nifi to gather the data, ELK Stack (ElastickSearch) to generate a real time dashboard, and a mobile application.\n\n![Dashboard](https:\/\/i.ibb.co\/xzf4FrK\/Capture.png)\n","55ccc28a":"We can see clearly, that we got pretty much good results, as love and like are positive whereas hate is negative by a good probability.","69e96cb7":"We noticed that the learning is smoothly going up by epochs with a gap that represents the variation. The learning curve tells us that we need more data, but since it took us 1h30m for 1.6 million tweets, what would be for more data :O :O. The loss curve tells us basically the same notice since it is going down on by epochs for both the training and the validation sets.","d8100712":"We encoded the labels of the targets.","8a8f3864":"**Words count by Sentiment**\n\nNext we plot the words count in the text according to the sentiment positive or negative.\n\nFist of all, we split the text to a separated array of words.\n\n![1](https:\/\/i.ibb.co\/rk0Yrhc\/1.png)\n![2](https:\/\/i.ibb.co\/8m0y0Y6\/2.png)\n\nThen, we count each word appearence uniquely on the Positive and the negative sentiment texts.\n\n![3](https:\/\/i.ibb.co\/s3PScNN\/3.png)\n![4](https:\/\/i.ibb.co\/VCyqswG\/4.png)\n![5](https:\/\/i.ibb.co\/FVnr58Y\/5.png)\n![6](https:\/\/i.ibb.co\/WpWTdgS\/6.png)\n\nWe noticed that the most common word in the Positive sentiment are : good, day, quot and love. A notice that we have applied this only on 20,000 samples not the whole data.\n\n\n![7](https:\/\/i.ibb.co\/qxCWLWD\/7.png)\n\nFor the negative sentiment, we noticed that the most common word in the Positive sentiment are : work ( :p which make sense nobody love to work :D ) , go and get.\n\nNext, we tried the world cloud on the same data to see which words are the most used according to the sentiment.\n\n![8](https:\/\/i.ibb.co\/QnywWGt\/8.png)\n\nAs before, the most common word in the positive section is Good.\n\n![9](https:\/\/i.ibb.co\/stGFyM1\/9.png)\n\nAlso, the most common word in the negative section is Work :D.\n\n**Text Words count by Sentiment**\n\nWe want to expirement if the number of words influence on the sentiment.\n\nFirst of all, we split the words into the array, then we count the number of words in this list, and then do the plots.\n\n![10](https:\/\/i.ibb.co\/8zXKf9G\/10.png)\n\n\n![11](https:\/\/i.ibb.co\/Wcrv1q7\/11.png)\n\nWe noticed that the more the number of the words the less tweets we have, which make sense because the tweets aren't that long. The pick is 4 words, of course we are dealing with meaningful words. We noticed that mainly the number of positive tweets doesn't get affected by the length, it is nearly constant comparing to the negative sentiment that goes down after 5 words.\n\n**Sentiment by hour of the day**\n\nWe want to know if the time of the day affects the tweets sentiment. Therefor we transform the timestamp to datetime object so we can extract the hour of the day, then count the positive and negative targets by hour of the day.\n\n![12](https:\/\/i.ibb.co\/b2XgM9p\/12.png)\n\n\n![13](https:\/\/i.ibb.co\/10FzDpW\/13.png)\n\nThe results were amazing, we noticed that the graphs are very close, the picks are mainly around 9am, which make sense, we tweet more in the morning. We noticed also a period of idle time in the afternoon.\n","887a430a":"We want to know the vocabulary size.","354ec226":"We got around 201K words with 300 dimensions for each one.\n\nWe created next the embedding layer based on the all parameters we have been talking about.","2f9a494e":"3. Clean the text\n\nWe create a function that applied the 2 operation of removing any stop or stemmer words. Adding to that, we will clean the text and letting only meaningful alphnumerical characters.","a56356f4":"We want to know the most similair words that are plotted into the highest dimensions.","61d664f1":"# 2.Preprocess the data\n\nFirst of all, we want to clean the data so it would be ready for both the deep learning steps and the EDA.\n\n**Install the libraries**\n\nFirst step, we need to install the libraries [Keras][1] for Deep learning on [Tensorflow][2]. We needed also [Gensim][3] an analyze plain-text documents for semantic structure that we have used for Word vectorization \"Word2Vec\" that we will talk about next on the kernel.\n\n[1]: https:\/\/keras.io\/\n[2]: https:\/\/www.tensorflow.org\/\n[3]: https:\/\/radimrehurek.com\/gensim\/","ca07e230":"We can see clearly that there is no more special characters or user tags, and also lowercase all the words.","36c34864":"We observe that the sentiments are equally divided, 50% each between positive and negative sentiments.\n\n","0aab4092":"We fit the model with a Batch size of 1024, 8 epochs, a validation split of 10%. ","d3a45424":"We confirmed that our results are good by watching the other scores such as : precision, recall and f1-score.","64483eb0":"# Refrences\n\nBig credit to some refrences that help us a lot :\n\n[Code_1][1]\n\n[Code_2][2]\n\n[1]: https:\/\/towardsdatascience.com\/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\n[2]: https:\/\/www.kaggle.com\/paoloripamonti\/twitter-sentiment-analysis","fb9adba2":"# 8.Evaluation\n\nFirst of all, we print the accuracy and the loss score.","7972c4d8":"Next, we want to predict the sentiment of the tweet text.","436437f2":"Next, we wamt to plot the confusion matrix.","a67c7c02":"We can see that it works pretty well.\n\n# 5.Tokenization\n\nThe tokenizer once fitted to the data also keeps an index of words(dictionary of words which we can use to assign a unique number to a word) which can be accessed by tokenizer.word_index.\n\n![Tokenization](https:\/\/i.ibb.co\/02NCvQq\/01.png)","484e27ba":"# 6.Create the Embedding layer\n\nThe general purpose is to create the neural network shown in the picture below that start with an embedding layer.\n\n![NeuralNet](https:\/\/i.ibb.co\/GRYGWbv\/100.png)\n\nFirst, we get the sentiments labels, notice that in our dataset there is no Neutral so we added it manually.","b1d6296c":"Now, let's create the embedding matrix from our w2v_model. We are advantagous since we are using the tokenizer indexes so the words are unique, we get for each word the vector of the neares 300 words.","22cfcb3f":"**Read the data**\n\nNext we read the data from the CSV file using the columns names and the encoding norm."}}