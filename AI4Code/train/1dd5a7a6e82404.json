{"cell_type":{"6fec4b2e":"code","66b418c3":"code","bb2fdbe6":"code","71446122":"code","25c6ae43":"code","d5c264f6":"code","ce52e6c3":"code","73a94303":"code","a178e2b2":"code","feb5c7e0":"code","b4509eed":"code","6656af41":"code","efe8be72":"code","041d6af1":"code","7bf1760e":"code","2cf2cdf8":"code","30251be2":"code","111bfe9d":"code","2eb7561c":"code","ff384f69":"code","eec63979":"code","c9c614f9":"code","7c93d420":"code","68e62ff4":"code","ce545383":"code","9003c5a0":"code","f0243613":"code","af445c72":"code","1ac34ec9":"code","9988d871":"code","4c8d5bcb":"code","1fea5af2":"code","0be9e4dd":"code","46c289f0":"code","9e1c93b8":"code","e9762779":"code","6fcdf33e":"code","72dae103":"code","2c55fcc8":"code","5ed528dd":"code","5275eb26":"code","fd4836c3":"code","f1c45960":"code","73136bc4":"code","0782be49":"code","1d8689f8":"code","b10a9368":"markdown","ca53b962":"markdown","4a536f4c":"markdown","f9fb7b89":"markdown","5d26397d":"markdown","d7f6823b":"markdown","737d259a":"markdown","b9994448":"markdown","38c32fa2":"markdown","82fbd9ca":"markdown","a811416d":"markdown","170e1507":"markdown","42b0e830":"markdown","2b2b8ad8":"markdown","8a9cef09":"markdown","00af78bd":"markdown","a2759eea":"markdown","85a5c37e":"markdown","205765dc":"markdown","a7972d6e":"markdown","41667769":"markdown","c570dc71":"markdown","dc782f6e":"markdown","cc942444":"markdown","cc45df58":"markdown","d51bc728":"markdown","307fdfe8":"markdown","31b0a8ed":"markdown","52b051fe":"markdown","96644fb9":"markdown","c2e0f74e":"markdown","7163793d":"markdown","b2b7eaa0":"markdown","cbbb89dd":"markdown","f57e2a66":"markdown","39384e3e":"markdown","36bc7535":"markdown","21846879":"markdown","61adb073":"markdown","50a75965":"markdown","41a1be90":"markdown","a48cd677":"markdown","bfd65193":"markdown","446350c8":"markdown","2d4ac7a5":"markdown","1bcc27be":"markdown","a34b90ba":"markdown","ce3e2d0f":"markdown","f64ba07a":"markdown","1908ff5c":"markdown","73746399":"markdown","eeab6db6":"markdown","7eed8321":"markdown","b516d93e":"markdown","1eddcf5a":"markdown","4d711d99":"markdown","8d2a9e4c":"markdown","47c562d6":"markdown","31f238b5":"markdown","a8540030":"markdown","90f677dd":"markdown","29d0fce1":"markdown","4320afa1":"markdown","a3272052":"markdown","f50c0c9b":"markdown","208cb8d7":"markdown","d70d9b60":"markdown","dc63e6b2":"markdown","fef9f5dc":"markdown","db54a5b3":"markdown","07de7792":"markdown","58f61e35":"markdown","4ea93633":"markdown","6654f687":"markdown","2eec23a5":"markdown","a78612fe":"markdown"},"source":{"6fec4b2e":"import pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.special import boxcox1p\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") # ignoring annoying warnings\n\nfrom pandasql import sqldf\npysqldf = lambda q: sqldf(q, globals())","66b418c3":"features = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip')\ntrain = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip')\nstores = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv')\ntest = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip')\nsample_submission = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip')","bb2fdbe6":"feat_sto = features.merge(stores, how='inner', on='Store')","71446122":"feat_sto.head(5)","25c6ae43":"pd.DataFrame(feat_sto.dtypes, columns=['Type'])","d5c264f6":"train.head(5)","ce52e6c3":"pd.DataFrame({'Type_Train': train.dtypes, 'Type_Test': test.dtypes})","73a94303":"feat_sto.Date = pd.to_datetime(feat_sto.Date)\ntrain.Date = pd.to_datetime(train.Date)\ntest.Date = pd.to_datetime(test.Date)","a178e2b2":"feat_sto['Week'] = feat_sto.Date.dt.week \nfeat_sto['Year'] = feat_sto.Date.dt.year","feb5c7e0":"train_detail = train.merge(feat_sto, \n                           how='inner',\n                           on=['Store','Date','IsHoliday']).sort_values(by=['Store',\n                                                                            'Dept',\n                                                                            'Date']).reset_index(drop=True)","b4509eed":"test_detail = test.merge(feat_sto, \n                           how='inner',\n                           on=['Store','Date','IsHoliday']).sort_values(by=['Store',\n                                                                            'Dept',\n                                                                            'Date']).reset_index(drop=True)","6656af41":"del features, train, stores, test","efe8be72":"null_columns = (train_detail.isnull().sum(axis = 0)\/len(train_detail)).sort_values(ascending=False).index\nnull_data = pd.concat([\n    train_detail.isnull().sum(axis = 0),\n    (train_detail.isnull().sum(axis = 0)\/len(train_detail)).sort_values(ascending=False),\n    train_detail.loc[:, train_detail.columns.isin(list(null_columns))].dtypes], axis=1)\nnull_data = null_data.rename(columns={0: '# null', \n                                      1: '% null', \n                                      2: 'type'}).sort_values(ascending=False, by = '% null')\nnull_data = null_data[null_data[\"# null\"]!=0]\nnull_data","041d6af1":"pysqldf(\"\"\"\nSELECT\n    T.*,\n    case\n        when ROW_NUMBER() OVER(partition by Year order by week) = 1 then 'Super Bowl'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 2 then 'Labor Day'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 3 then 'Thanksgiving'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 4 then 'Christmas'\n    end as Holyday,\n    case\n        when ROW_NUMBER() OVER(partition by Year order by week) = 1 then 'Sunday'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 2 then 'Monday'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 3 then 'Thursday'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2010 then 'Saturday'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2011 then 'Sunday'\n        when ROW_NUMBER() OVER(partition by Year order by week) = 4 and Year = 2012 then 'Tuesday'\n    end as Day\n    from(\n        SELECT DISTINCT\n            Year,\n            Week,\n            case \n                when Date <= '2012-11-01' then 'Train Data' else 'Test Data' \n            end as Data_type\n        FROM feat_sto\n        WHERE IsHoliday = True) as T\"\"\")","7bf1760e":"weekly_sales_2010 = train_detail[train_detail.Year==2010]['Weekly_Sales'].groupby(train_detail['Week']).mean()\nweekly_sales_2011 = train_detail[train_detail.Year==2011]['Weekly_Sales'].groupby(train_detail['Week']).mean()\nweekly_sales_2012 = train_detail[train_detail.Year==2012]['Weekly_Sales'].groupby(train_detail['Week']).mean()\nplt.figure(figsize=(20,8))\nsns.lineplot(weekly_sales_2010.index, weekly_sales_2010.values)\nsns.lineplot(weekly_sales_2011.index, weekly_sales_2011.values)\nsns.lineplot(weekly_sales_2012.index, weekly_sales_2012.values)\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=16)\nplt.title('Average Weekly Sales - Per Year', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\nplt.show()","2cf2cdf8":"train_detail.loc[(train_detail.Year==2010) & (train_detail.Week==13), 'IsHoliday'] = True\ntrain_detail.loc[(train_detail.Year==2011) & (train_detail.Week==16), 'IsHoliday'] = True\ntrain_detail.loc[(train_detail.Year==2012) & (train_detail.Week==14), 'IsHoliday'] = True\ntest_detail.loc[(test_detail.Year==2013) & (test_detail.Week==13), 'IsHoliday'] = True","30251be2":"weekly_sales_mean = train_detail['Weekly_Sales'].groupby(train_detail['Date']).mean()\nweekly_sales_median = train_detail['Weekly_Sales'].groupby(train_detail['Date']).median()\nplt.figure(figsize=(20,8))\nsns.lineplot(weekly_sales_mean.index, weekly_sales_mean.values)\nsns.lineplot(weekly_sales_median.index, weekly_sales_median.values)\nplt.grid()\nplt.legend(['Mean', 'Median'], loc='best', fontsize=16)\nplt.title('Weekly Sales - Mean and Median', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Date', fontsize=16)\nplt.show()","111bfe9d":"weekly_sales = train_detail['Weekly_Sales'].groupby(train_detail['Store']).mean()\nplt.figure(figsize=(20,8))\nsns.barplot(weekly_sales.index, weekly_sales.values, palette='dark')\nplt.grid()\nplt.title('Average Sales - per Store', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Store', fontsize=16)\nplt.show()","2eb7561c":"weekly_sales = train_detail['Weekly_Sales'].groupby(train_detail['Dept']).mean()\nplt.figure(figsize=(25,8))\nsns.barplot(weekly_sales.index, weekly_sales.values, palette='dark')\nplt.grid()\nplt.title('Average Sales - per Dept', fontsize=18)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Dept', fontsize=16)\nplt.show()","ff384f69":"sns.set(style=\"white\")\n\ncorr = train_detail.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nf, ax = plt.subplots(figsize=(20, 15))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.title('Correlation Matrix', fontsize=18)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True)\n\nplt.show()","eec63979":"train_detail = train_detail.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])\ntest_detail = test_detail.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])","c9c614f9":"def make_discrete_plot(feature):\n    fig = plt.figure(figsize=(20,8))\n    gs = GridSpec(1,2)\n    sns.boxplot(y=train_detail.Weekly_Sales, x=train_detail[feature], ax=fig.add_subplot(gs[0,0]))\n    plt.ylabel('Sales', fontsize=16)\n    plt.xlabel(feature, fontsize=16)\n    sns.stripplot(y=train_detail.Weekly_Sales, x=train_detail[feature], ax=fig.add_subplot(gs[0,1]))\n    plt.ylabel('Sales', fontsize=16)\n    plt.xlabel(feature, fontsize=16)\n    fig.show()","7c93d420":"def make_continuous_plot(feature):\n    \n    fig = plt.figure(figsize=(18,15))\n    gs = GridSpec(2,2)\n    \n    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n                        x=boxcox1p(train_detail[feature], 0.15), ax=fig.add_subplot(gs[0,1]), palette = 'blue')\n\n    plt.title('BoxCox 0.15\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(boxcox1p(train_detail[feature], 0.15)),2)) +\n              ', Skew: ' + str(np.round(stats.skew(boxcox1p(train_detail[feature], 0.15), nan_policy='omit'),2)))\n    \n    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n                        x=boxcox1p(train_detail[feature], 0.25), ax=fig.add_subplot(gs[1,0]), palette = 'blue')\n\n    plt.title('BoxCox 0.25\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(boxcox1p(train_detail[feature], 0.25)),2)) +\n              ', Skew: ' + str(np.round(stats.skew(boxcox1p(train_detail[feature], 0.25), nan_policy='omit'),2)))\n    \n    j = sns.distplot(train_detail[feature], ax=fig.add_subplot(gs[1,1]), color = 'green')\n\n    plt.title('Distribution\\n')\n    \n    j = sns.scatterplot(y=train_detail['Weekly_Sales'], \n                        x=train_detail[feature], ax=fig.add_subplot(gs[0,0]), color = 'red')\n\n    plt.title('Linear\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(train_detail[feature]),2)) + ', Skew: ' + \n               str(np.round(stats.skew(train_detail[feature], nan_policy='omit'),2)))\n    \n    fig.show()","68e62ff4":"make_discrete_plot('IsHoliday')","ce545383":"make_discrete_plot('Type')","9003c5a0":"train_detail.Type = train_detail.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))\ntest_detail.Type = test_detail.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))","f0243613":"make_continuous_plot('Temperature')","af445c72":"train_detail = train_detail.drop(columns=['Temperature'])\ntest_detail = test_detail.drop(columns=['Temperature'])","1ac34ec9":"make_continuous_plot('CPI')","9988d871":"train_detail = train_detail.drop(columns=['CPI'])\ntest_detail = test_detail.drop(columns=['CPI'])","4c8d5bcb":"make_continuous_plot('Unemployment')","1fea5af2":"train_detail = train_detail.drop(columns=['Unemployment'])\ntest_detail = test_detail.drop(columns=['Unemployment'])","0be9e4dd":"make_continuous_plot('Size')","46c289f0":"def WMAE(dataset, real, predicted):\n    weights = dataset.IsHoliday.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(real-predicted))\/(np.sum(weights)), 2)","9e1c93b8":"def random_forest(n_estimators, max_depth):\n    result = []\n    for estimator in n_estimators:\n        for depth in max_depth:\n            wmaes_cv = []\n            for i in range(1,5):\n                print('k:', i, ', n_estimators:', estimator, ', max_depth:', depth)\n                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n                RF = RandomForestRegressor(n_estimators=estimator, max_depth=depth)\n                RF.fit(x_train, y_train)\n                predicted = RF.predict(x_test)\n                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n            print('WMAE:', np.mean(wmaes_cv))\n            result.append({'Max_Depth': depth, 'Estimators': estimator, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)","e9762779":"def random_forest_II(n_estimators, max_depth, max_features):\n    result = []\n    for feature in max_features:\n        wmaes_cv = []\n        for i in range(1,5):\n            print('k:', i, ', max_features:', feature)\n            x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n            RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=feature)\n            RF.fit(x_train, y_train)\n            predicted = RF.predict(x_test)\n            wmaes_cv.append(WMAE(x_test, y_test, predicted))\n        print('WMAE:', np.mean(wmaes_cv))\n        result.append({'Max_Feature': feature, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)","6fcdf33e":"def random_forest_III(n_estimators, max_depth, max_features, min_samples_split, min_samples_leaf):\n    result = []\n    for split in min_samples_split:\n        for leaf in min_samples_leaf:\n            wmaes_cv = []\n            for i in range(1,5):\n                print('k:', i, ', min_samples_split:', split, ', min_samples_leaf:', leaf)\n                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n                RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, \n                                           min_samples_leaf=leaf, min_samples_split=split)\n                RF.fit(x_train, y_train)\n                predicted = RF.predict(x_test)\n                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n            print('WMAE:', np.mean(wmaes_cv))\n            result.append({'Min_Samples_Leaf': leaf, 'Min_Samples_Split': split, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)","72dae103":"X_train = train_detail[['Store','Dept','IsHoliday','Size','Week','Type','Year']]\nY_train = train_detail['Weekly_Sales']","2c55fcc8":"n_estimators = [56, 58, 60]\nmax_depth = [25, 27, 30]\n\nrandom_forest(n_estimators, max_depth)","5ed528dd":"max_features = [2, 3, 4, 5, 6, 7]\n\nrandom_forest_II(n_estimators=58, max_depth=27, max_features=max_features)","5275eb26":"min_samples_split = [2, 3, 4]\nmin_samples_leaf = [1, 2, 3]\n\nrandom_forest_III(n_estimators=58, max_depth=27, max_features=6, \n                  min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)","fd4836c3":"RF = RandomForestRegressor(n_estimators=58, max_depth=27, max_features=6, min_samples_split=3, min_samples_leaf=1)\nRF.fit(X_train, Y_train)","f1c45960":"X_test = test_detail[['Store', 'Dept', 'IsHoliday', 'Size', 'Week', 'Type', 'Year']]\npredict = RF.predict(X_test)","73136bc4":"Final = X_test[['Store', 'Dept', 'Week']]\nFinal['Weekly_Sales'] = predict","0782be49":"Final_adj = pysqldf(\"\"\"\n    SELECT\n        Store,\n        Dept,\n        Week,\n        Weekly_Sales,\n        case \n            when Week = 52 and last_sales > 2*Weekly_Sales then Weekly_Sales+(2.5\/7)*last_sales\n            else Weekly_Sales \n        end as Weekly_Sales_Adjusted\n    from(\n        SELECT\n            Store, \n            Dept, \n            Week, \n            Weekly_Sales,\n            case \n                when Week = 52 then lag(Weekly_Sales) over(partition by Store, Dept) \n            end as last_sales\n        from Final)\"\"\")","1d8689f8":"sample_submission['Weekly_Sales'] = Final_adj['Weekly_Sales_Adjusted']\nsample_submission.to_csv('submission.csv',index=False)","b10a9368":"> Same for 'CPI'.","ca53b962":"> Same fields for Test Data.","4a536f4c":"> Yeah, there are Sales difference between the Stores.","f9fb7b89":"> Some interesting notes about the result:\n    >* All Holidays fall on the same week\n    >* Test Data doesn't have Labor Day, so this Holiday is not very relevant\n    >* Christmas has 0 pre-holiday days in 2010, 1 in 2011 and 3 in 2012. The model will not consider more Sales in 2012 for Test Data, so we are going to adjust it at the end, with a formula and an explanation.","5d26397d":"## Project Phases:\n> \n* **1) Libraries and Data Loading**\n* **2) Exploratory Analysis and Data Cleaning**\n* **3) Machine Learning**\n* **4) Christmas Adjustment**\n* **5) Submission**","d7f6823b":"# 1-Libraries and Data Loading","737d259a":"> Let's see the correlation between variables, using Pearson Correlation.","b9994448":"# 4-Christmas Adjustment","38c32fa2":"> And the structure of a decision tree is as follows:","82fbd9ca":"![image.png](attachment:image.png)","a811416d":"> Correlation Metrics:\n    >* 0: no correlation at all\n    >* 0-0.3: weak correlation \n    >* 0.3-0.7: moderate correlaton\n    >* 0.7-1: strong correlation\n\n> Positive Correlation indicates that when one variable increase, the other also does. Negative is the opposite.","170e1507":"> First, two plot functions that will help us.\n\n> The discrete plot is for finite numbers. We will use boxplot, to see the medians and interquartile ranges, and the striplot, which is a better way of seeing the distribution, even more when lots of outliers are present.\n\n> The continuous plot, as the name says, is for continuous variables. We will see the distribution of probabilities and use BoxCox to understand if there is increase of correlation and decrease of skewness for each variable. In some cases the process of transforming a variable can help, depending on the model.","42b0e830":"# 3-Machine Learning","2b2b8ad8":"> The 'Date' field doesn't represent the day itself, but each week, ending every friday. So, it's interesting two create a 'Week' field and also we can create one for 'Year'. ","8a9cef09":"> Tuning 'max_features'.","00af78bd":"> Same for 'Unemployment' rate.","a2759eea":"> The result by the time I run it is 3 and 1.","85a5c37e":"> The functions for Random Forest, Parameters Tuning and Cross Validation are:","205765dc":"> We don't know what 'Type' is, but we can assume that A > B > C in terms of Sales Median. So, let's treat it as an ordinal variable and replace its values. \n\n> Ordinal variables are explained in the figure below.","a7972d6e":"### Training Model","41667769":"> Although skewness changes, correlation doesn't seem to change at all. We can decide to drop it.","c570dc71":"# 2-Exploratory Analysis and Data Cleaning","dc782f6e":"> Let's use another dataframe and SQL to solve it quickly.","cc942444":"### Weekly_Sales x Unemployment","cc45df58":"![image.png](attachment:image.png)","d51bc728":"> As we can see in the figure below, the evaluation is based on Weighted Mean Absolute Error (WMAE), with a weight of 5 for Holiday Weeks and 1 otherwise.","307fdfe8":"## Author: Caio Avelino\n* [LinkedIn](https:\/\/www.linkedin.com\/in\/caioavelino\/)\n* [Kaggle](https:\/\/www.kaggle.com\/avelinocaio)","31b0a8ed":"[Image Source](https:\/\/towardsdatascience.com\/random-forest-and-its-implementation-71824ced454f)","52b051fe":"> The final model:","96644fb9":"> It is important to tune these parameters to find the best predictor and to minimize overfitting.","c2e0f74e":"> Tuning 'n_estimators' and 'max_depth'.\n\n> Here, it is possible to test a lot of values. Those are the final ones, after a bit of testing.","7163793d":"> Let's take a look at the Average Weekly Sales per Year and find out if there are another holiday peak sales that were not considered by 'IsHoliday' field.","b2b7eaa0":"> The result by the time I run it is 58 and 27.","cbbb89dd":"### Weekly_Sales x IsHoliday","f57e2a66":"> We can use SQL, putting the week days for each Holiday in every year. Doing some research, the Super Bowl, Labor Day and Thanksgiving fall on the same day. In the other hand, Christmas is always on December 25th, so the week day can change.","39384e3e":"> Here, we will analyze the week days that the Holidays fall on each year. This is relevant to know how many pre-holiday days are inside each Week marked as 'True' inside 'IsHoliday' field.\n\n> If, for a certain Week, there are more pre-holiday days in one Year than another, then it is very possible that the Year with more pre-holiday days will have greater Sales for the same Week. So, the model will not take this consideration and we might need to adjust the predicted values at the end.\n\n> Another thing to take into account is that Holiday Weeks but with few or no pre-holiday days might have lower Sales than the Week before.","36bc7535":"> As we can see, there is one important Holiday not included in 'IsHoliday'. It's the Easter Day. It is always in a Sunday, but can fall on different weeks. \n    >* In 2010 is in Week 13\n    >* In 2011, Week 16\n    >* Week 14 in 2012\n    >* and, finally, Week 13 in 2013 for Test set\n\n> So, we can change to 'True' these Weeks in each Year.","21846879":"> So, turning the formula into a function, since we can't use GridSearchCV or RandomSearchCV:","61adb073":"![image.png](attachment:image.png)","50a75965":"> Let's start by merging data from two of the datasets: features and stores. They have a commom key 'Stores'. The data will be loaded into 'feat_sto'.","41a1be90":"> 'MarkDown' 1 to 5 are not strong correlated to 'Weekly_Sales' and they have a lot of null values, then we can drop them.\n\n> Also, 'Fuel_Price' is strong correlated to 'Year'. One of them must be dropped else they would carry similar information to the model. 'Year' will not be dropped, because it differentiate same Weeks for 'Store'+'Dept'.\n\n> Other variables that have weak correlation with 'Weekly_Sales' can be analyzed to see if they are useful.","a48cd677":"> The types of this dataframe are:","bfd65193":"![image.png](attachment:image.png)","446350c8":"### Model Functions","2d4ac7a5":"> And there are Sales difference between the Departments too. Also some Depts are not in the list, like number '15', for example.","1bcc27be":"### Average Sales per Store and Department","a34b90ba":"> The algorithm chooses a feature to be the Root Node and make a split of the samples. The function to measure the quality of a split we can choose and pass as a parameter. The splitting continues until the Internal Node has less samples than 'min_samples_split' to split and become a Leaf Node. And the 'min_samples_leaf' tells the minimum number of samples to be considered as a Leaf Node. There is also an important parameter called 'max_features' and it is the maximum number of features considered when the node is requiring the best split. The number of layers is the 'max_depth' parameter.","ce3e2d0f":"### Predictions","f64ba07a":"### Introduction","1908ff5c":"> This field is going to be important to differentiate Week Holidays. As we can see, Week Holidays have more high sales events than non-Holiday Weeks.","73746399":"### Holidays Analysis","eeab6db6":"### Variables Correlation","7eed8321":"> Just as an observation, the mean and the median are very different, suggesting that some stores\/departments might sell much more than others.","b516d93e":"> This is important, there are columns with more than 60% of null values. If the correlations of these features with the target 'WeeklySales' are ~0, then it is not a good idea to use them. Also, they are anonymized fields, it can be difficult to know what they mean.","1eddcf5a":"> In this section, we will explore the datasets provided, join information between some of them and make relevant transformations.","4d711d99":"[Image Source](http:\/\/survivestatistics.com\/variables\/) ","8d2a9e4c":"> Ok, now it's time to make the Christmas Adjustment.\n\n> We can remember that Christmas Week has 0 pre-holiday days in 2010, 1 in 2011 and 3 in 2012. So, it's a difference of 3 days from 2012 to 2010 and 2 days from 2012 to 2011. A 2.5 days average, in a week (7 days). So, this is the value that we are going to multiply to Week 51 and add to Week 52 to compensate what the model didn't take into account.\n\n> But we are going to use this formula just for 'Stores'+'Departments' that have a big difference between Week 51 and Week 52 Sales. Let's say Week51 > 2 * Week52.","47c562d6":"> That's it. Let's make the submission.\n\n> Last time I checked, the submission file returned 2688.84 (Private) and 2673.97 (Public).","31f238b5":"### Weekly_Sales x Temperature","a8540030":"### Analyzing Variables","90f677dd":"> And, finally, we will continue with this variable, since it has moderate correlation with 'WeeklySales'.","29d0fce1":"> As we can see, the 'Date' field has string type. We can convert them to datetime and see as well if 'train' and 'test' dataframes has 'Date' type to convert. ","4320afa1":"> The model chosen for this project is the Random Forest Regressor. It is an ensemble method and uses multiples decision trees ('n_estimators' parameter of the model) to determine final output, which is an average of the outputs of all trees.","a3272052":"> You will find an overview of what is done at the beginning of each part.","f50c0c9b":"# 5-Submission","208cb8d7":"Reference: THE USE OF FACIAL MICRO-EXPRESSION STATE AND TREE-FOREST MODEL FOR PREDICTING CONCEPTUAL-CONFLICT BASED CONCEPTUAL CHANGE - Scientific Figure on ResearchGate. Available from: https:\/\/www.researchgate.net\/figure\/Basic-structure-of-a-decision-tree-All-decision-trees-are-built-through-recursion_fig3_295860754 [accessed 2 Apr, 2020]","d70d9b60":"### **If you made it this so far, let me know if you have questions, suggestions or critiques to improve the model. Thanks a lot!**","dc63e6b2":"> At this point, we can search for null values for each column.","fef9f5dc":"> The same chart, but showing also the median of the Sales and not divided by Year:","db54a5b3":"### Weekly_Sales x Type","07de7792":"> The result by the time I run it is 6.","58f61e35":"> We can create two new dataframes 'train_detail' and 'test_detail' containing all the information we need, joining 'train' and 'test' with 'feat_sto'. The keys here can be 'Store', 'Dept' and 'IsHoliday'.","4ea93633":"> Preparing Train Set.","6654f687":"### Weekly_Sales x CPI","2eec23a5":"### Weekly_Sales x Size","a78612fe":"> Tuning 'min_samples_split' and 'min_samples_leaf'."}}