{"cell_type":{"a80b5eff":"code","f76e00ad":"code","83dc4949":"code","3e3976da":"code","57e124e6":"code","e69ab8a5":"code","242f6e9e":"markdown","a71cfd61":"markdown","b53848f8":"markdown","229d44d4":"markdown"},"source":{"a80b5eff":"#                           linear1,nn.BatchNorm1d(hiddenLayer1Size),relu,\n#                           linear2,dropout,relu,\n#                           linear3,dropout,relu,\n#                           linear4,dropout,relu,\n#                           linear5,dropout,relu,\n#                           linear6,dropout,relu,\n#                           sigmoid","f76e00ad":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nimport sklearn\nimport sklearn.model_selection\nfrom sklearn.metrics import f1_score,confusion_matrix,accuracy_score, log_loss\nimport seaborn as sns # data visualization library ","83dc4949":"news = pd.read_csv('..\/input\/OnlineNewsPopularityReduced.csv')\nX = news.iloc[:,2:-1]\ncolumn_names = list(X.columns.values)\nN_FEATURES = len(column_names)\ny = news.iloc[:,-1]\ny = (y > 1400) # a news article is considered popular if it is shared more than 1400 times.\nnews.head()","3e3976da":"x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, random_state=42, test_size=0.3)","57e124e6":"def xNumpyToTensor(array):\n    array = np.array(array, dtype=np.float32) \n    return Variable(torch.from_numpy(array)).type(torch.FloatTensor)\n\ndef yNumpyToTensor(array):\n    array = np.array(array.astype(int))\n    return Variable(torch.from_numpy(array)).type(torch.FloatTensor)\n\nx_tensor_train = xNumpyToTensor(x_train)\ny_tensor_train = yNumpyToTensor(y_train)\nx_tensor_test = xNumpyToTensor(x_test)\ny_tensor_test = yNumpyToTensor(y_test)","e69ab8a5":"# Neural Network parameters\nDROPOUT_PROB = 0.9\n\nLR = 0.001\nMOMENTUM= 0.99\ndropout = torch.nn.Dropout(p=1 - (DROPOUT_PROB))\n\nhiddenLayer1Size=512\nhiddenLayer2Size=int(hiddenLayer1Size\/4)\nhiddenLayer3Size=int(hiddenLayer1Size\/8)\nhiddenLayer4Size=int(hiddenLayer1Size\/16)\nhiddenLayer5Size=int(hiddenLayer1Size\/32)\n\n#Neural Network layers\nlinear1=torch.nn.Linear(N_FEATURES, hiddenLayer1Size, bias=True) \nlinear2=torch.nn.Linear(hiddenLayer1Size, hiddenLayer2Size)\nlinear3=torch.nn.Linear(hiddenLayer2Size, hiddenLayer3Size)\nlinear4=torch.nn.Linear(hiddenLayer3Size, hiddenLayer4Size)\nlinear5=torch.nn.Linear(hiddenLayer4Size, hiddenLayer5Size)\nlinear6=torch.nn.Linear(hiddenLayer5Size, 1)\nsigmoid = torch.nn.Sigmoid()\nthreshold = nn.Threshold(0.5, 0)\ntanh=torch.nn.Tanh()\nrelu=torch.nn.LeakyReLU()\n\n#Neural network architecture\nnet = torch.nn.Sequential(linear1,nn.BatchNorm1d(hiddenLayer1Size),relu,\n                          linear2,dropout,relu,\n                          linear3,dropout,relu,\n                          linear4,dropout,relu,\n                          linear5,dropout,relu,\n                          linear6,dropout,relu,\n                          sigmoid\n                          )\n\noptimizer = torch.optim.Adam(net.parameters(), lr=LR,weight_decay=5e-3)\nloss_func=torch.nn.BCELoss()\nepochs = 200\nall_losses = []\n\n#Training in batches\nfor step in range(epochs):    \n    out = net(x_tensor_train)                 # input x and predict based on x\n    cost = loss_func(out, y_tensor_train) \n    optimizer.zero_grad()   # clear gradients for next train\n    cost.backward()         # backpropagation, compute gradients\n    optimizer.step()        # apply gradients \n        \n    if step % 5 == 0:        \n        loss = cost.data\n        all_losses.append(loss)\n        print(step, cost.data.cpu().numpy())\n        # RuntimeError: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). \n        # Use .cpu() to move the tensor to host memory first.        \n        prediction = (net(x_tensor_test).data).float() # probabilities         \n#         prediction = (net(X_tensor).data > 0.5).float() # zero or one\n#         print (\"Pred:\" + str (prediction)) # Pred:Variable containing: 0 or 1\n#         pred_y = prediction.data.numpy().squeeze()            \n        pred_y = prediction.cpu().numpy().squeeze()\n        target_y = y_tensor_test.cpu().data.numpy()\n        print ('LOG_LOSS={} '.format(log_loss(target_y, pred_y))) \n\n#Evaluating the performance of the model\n%matplotlib inline\nplt.plot(all_losses)\nplt.show()\npred_y = pred_y > 0.5\nprint('f1 score', f1_score(target_y, pred_y))\nprint('accuracy',accuracy_score(target_y, pred_y))\ncm = confusion_matrix(target_y, pred_y)\nsns.heatmap(cm, annot=True)","242f6e9e":"This model achieved a F1 score of ~.567 and accuracy of ~.62\nNot bad! I'll spare the details of hyperparameter tuning and cross-validation for another notebook, but I hope this gives you a nice introduction to how to use PyTorch DNN on a dataset in a Kaggle Kernel! Hope you can apply this model to your own dataset! \ud83d\ude04","a71cfd61":"# Testing a PyTorch DNN Model on the Mashable Online News Popularity Dataset\n\n#### This notebook is a quick and gentle introduction to training a Deep Neural Network on the Mashable Online News Popularity Dataset in PyTorch. The DNN architecture used is as follows:\n                         \n","b53848f8":"transform the training and test data from numpy arrays to Tensors, so that PyTorch can use the data","229d44d4":"split the dataset into a training and test set"}}