{"cell_type":{"a53ffd95":"code","158f13df":"code","61f21c24":"code","1dd5a69d":"code","6fb63296":"code","29356ebd":"code","b037f076":"code","47dbc083":"code","eb9c79ec":"code","f2ce6efe":"code","4d6734e3":"code","4997fa65":"code","2905cf7d":"code","10bec1a4":"code","859930bd":"code","7088b0ae":"code","121dbd67":"code","6f4348a7":"code","f3f487de":"code","6178a832":"code","9462585c":"code","bff6db50":"code","41a69199":"code","44d458eb":"code","174ee1e5":"code","728c9e2a":"code","f7534bc6":"code","62947a36":"code","2defa46d":"code","51fe8a5d":"code","e1d9a37b":"code","1c41e8bc":"code","56af06ca":"markdown","50ae0e26":"markdown","cbf32f99":"markdown","fb131a8a":"markdown","33e969bd":"markdown","83db5f06":"markdown","e218de3a":"markdown","1b368961":"markdown","b54b6c6d":"markdown"},"source":{"a53ffd95":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","158f13df":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nimport warnings as w\nw.filterwarnings(\"ignore\")","61f21c24":"df = pd.read_csv('\/kaggle\/input\/breast-cancer-csv\/breastCancer.csv')\ndf","1dd5a69d":"df.head(10)","6fb63296":"df.shape","29356ebd":"df.describe()","b037f076":"df.info()","47dbc083":"df.columns","eb9c79ec":"print(df['class'].value_counts()\/6.99)\ndf['class'].value_counts()","f2ce6efe":"c = {col:df[df[col] == \"?\"].shape[0] for col in df.columns}\nc","4d6734e3":"import numpy as np\nfor i in range(df.shape[1]):\n    for j in range(df.shape[0]):\n        if(df.iloc[j,i]=='?'):\n            df.iloc[j,i]=np.NaN","4997fa65":"list(df['bare_nucleoli'].mode())","2905cf7d":"df[\"bare_nucleoli\"]=df[\"bare_nucleoli\"].apply(lambda x: 1.0 if pd.isnull(x) else x)","10bec1a4":"df.corr()","859930bd":"fig1 = plt.figure(figsize=(10,8))\nsns.heatmap(df.corr(),annot=True,cmap='YlGnBu',vmax=1.0,vmin=-1.0)","7088b0ae":"fig2 = plt.figure(figsize=(6,6))\nsns.pairplot(df.iloc[:,1:],hue='class',palette='Set2')","121dbd67":"X_train, X_test, y_train, y_test = train_test_split(df.iloc[:,1:-1],df.iloc[:,-1])\nprint(X_train,\"\\n\")\nprint(X_test,\"\\n\")\nprint(y_train,\"\\n\")\nprint(y_test,\"\\n\")\nprint(\"The dimension of X_train is : \",X_train.shape,\"\\n\")\nprint(\"The dimension of X_test is : \",X_test.shape,\"\\n\")\nprint(\"The dimension of y_train is : \",y_train.shape,\"\\n\")\nprint(\"The dimension of y_test is : \",y_test.shape,\"\\n\")","6f4348a7":"error_rate = []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred = knn.predict(X_test)\n    error_rate.append(np.mean(pred != y_test))","f3f487de":"plt.figure(figsize=(10,6))\nplt.plot(range(1,40), error_rate,'o--')\nplt.ylabel('Error Rate')\nplt.xlabel('K')","6178a832":"model1 = KNeighborsClassifier(n_neighbors=4).fit(X_train,y_train)","9462585c":"fig3, axs = plt.subplots(figsize=(5,5))\nplot_confusion_matrix(model1,X_test,y_test,ax=axs)","bff6db50":"print(classification_report(model1.predict(X_train),y_train))\nprint(classification_report(model1.predict(X_test),y_test))","41a69199":"print(accuracy_score(y_test,pred))","44d458eb":"gaussnb = GaussianNB()\ngaussnb.fit(X_train,y_train)\ngaussnbpred = gaussnb.predict(X_test)\ngaussnbresults = confusion_matrix(y_test,gaussnbpred)","174ee1e5":"gaussnbacc_score = accuracy_score(y_test,gaussnbpred)","728c9e2a":"print(\"The accuracy of NaiveBayes model is : %0.4f \", gaussnbacc_score)\nprint(\"The confusion matrix is :\\n\", gaussnbresults)","f7534bc6":"fig4, axs = plt.subplots(figsize=(5,5))\nplot_confusion_matrix(gaussnb,X_test,y_test,ax=axs)","62947a36":"print(classification_report(y_test,gaussnbpred))","2defa46d":"logreg = LogisticRegression()\nlogreg.fit(X_train,y_train)\nlogpred = logreg.predict(X_test)","51fe8a5d":"logacc_score = accuracy_score(y_test,logpred)\nlogresults = confusion_matrix(y_test,logpred)\nprint(\"The accuracy of Logistic Regression is : %0.4f\", logacc_score)\nprint(\"The confusion matrix is : \\n \", logresults )","e1d9a37b":"fig5, axs = plt.subplots(figsize=(5,5))\nplot_confusion_matrix(logreg,X_test,y_test,ax=axs)","1c41e8bc":"print(classification_report(y_test,logpred))","56af06ca":"**Applying the K Nearest Neighbour algorithm**","50ae0e26":"We can see here that our data is imbalanced.As the class 2 data is about 65% and class 4 is 34.5%.","cbf32f99":"1. After Applying all the Models like Knn,Logitic Regression & GaussianNB we have all the confusion matrix plot and the classification report of the models. \n2. From the above we choose the most accurate algorithm.","fb131a8a":"**Applying the Train and Test split for splitting the data for applying the models.**","33e969bd":"***As we can see in above Error rate vs k plot the optimal values for k is 4.***","83db5f06":"**Applying Logistic Regression Model.**","e218de3a":"Let's Check if there are any missing values present in the dataset.","1b368961":"**Applying the GaussianNB Algorithm.**","b54b6c6d":"Here we can see there are some missing values present in the 'bare_nucleoli' feature."}}