{"cell_type":{"9e573507":"code","b218754f":"code","5cf87952":"code","b81f1561":"code","4685c27d":"code","ae227f6f":"code","78b397e6":"code","16d70efd":"code","4d42a2b9":"code","a9944fd9":"code","70b98eb3":"code","dd6612b6":"code","b85f3760":"code","1e82246e":"code","feb7bf12":"code","0f307f7b":"code","3f4a4e71":"code","381b18d9":"code","56fc529f":"code","362241ca":"code","2eacc445":"code","e54fdf05":"code","d569dae1":"code","93bb1ae6":"code","4947d390":"code","72cc8dc6":"code","c0d5ca8d":"code","15d6fabd":"code","139b255f":"code","93ff8d35":"code","c4206b63":"code","c367e430":"code","ec52f945":"code","a70984ad":"code","568690c0":"code","73925b9a":"code","e1c0cc92":"code","9b00d157":"code","b50c6051":"code","133ef9b2":"code","23e8d5c9":"code","2e3c3b22":"code","93259f73":"code","34a262bc":"code","3cc043d0":"code","7b1bd027":"code","abb9ed96":"code","d5f1fa84":"code","3fce6c96":"code","40c64a74":"code","5648152c":"code","6fba31d6":"code","dda7b1d7":"code","a14618ea":"code","e379326a":"code","60610677":"code","ecba8b2e":"code","a7b86de8":"code","8f800759":"code","a54fc377":"code","56b51601":"code","faa552c0":"code","7554b7b7":"code","b207e9ef":"code","3771fcd1":"code","028985ac":"code","f73b0cdb":"code","f60aef20":"code","71c868ee":"code","5e64d557":"code","23439d34":"markdown","63379354":"markdown","508fa952":"markdown","b114fb2d":"markdown","8ae8ef16":"markdown","544bb42c":"markdown","0e67487f":"markdown","d5d24992":"markdown","889df1d2":"markdown"},"source":{"9e573507":"# Generic Libraries\nfrom PIL import Image\nimport os\nimport pandas as pd\nimport numpy as np\nimport re,string,unicodedata\n\n#Tesseract Library\nimport pytesseract\n\n#Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Garbage Collection\nimport gc\n\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport pytesseract","b218754f":"!pip install date-extractor","5cf87952":"img = cv2.imread(\"..\/input\/test-dataset\/resume1.jpg\") # image in BGR format\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nfig = plt.figure(figsize = [10,10])\nheight,width,channel = img.shape\nplt.imshow(img)\nprint(type(img))\nprint(height,width,channel)","b81f1561":"text = pytesseract.image_to_string(img)\nprint(text)","4685c27d":"extracted_text={}","ae227f6f":"match=re.search(r'[\\w\\.-]+@[\\w\\.-]+',text)\nmatch.group(0)#E-MAIL\nimport re\ndef get_email_addresses(string):\n    r = re.compile(r'[\\w\\.-]+@[\\w\\.-]+')\n    return r.findall(string)\n\nemail = get_email_addresses(text)\nprint(email)\nextracted_text['E-mail'] = email","78b397e6":"from date_extractor import extract_dates\n\ndates = extract_dates(text)\nprint(dates)\nextracted_text['DATE'] = dates","16d70efd":"def get_phone_numbers(string):\n    r = re.compile(r'(\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4})')\n    phone_numbers = r.findall(string)\n    return [re.sub(r'\\D', '', num) for num in phone_numbers]\n\nphone_number= get_phone_numbers(text)\nprint(phone_number)\nextracted_text['Phone number'] = phone_number","4d42a2b9":"!pip install nlp","a9944fd9":"# initialize matcher with a vocab\nfrom spacy.matcher import Matcher\nmatcher = Matcher(nlp.vocab)\n\ndef extract_name(resume_text):\n   nlp_text = nlp(resume_text)\n \n   # First name and Last name are always Proper Nouns\n   pattern = [{'POS': 'PROPN'}, {'POS': 'PROPN'}]\n \n   matcher.add('NAME', None, pattern)\n \n   matches = matcher(nlp_text)\n \n   for match_id, start, end in matches:\n       span = nlp_text[start:end]\n       return span.text\n\nname = extract_name(text)\nprint(name)\nextracted_text['NAME'] = name","70b98eb3":"print(extracted_text)","dd6612b6":"# the output of OCR can be saved in a file in necessary\nfile = open('output.txt','a') # file opened in append mode\nfile.write(str(extracted_text))\nfile.close()","b85f3760":"import spacy\nfrom spacy import displacy\nfrom collections import Counter\nimport en_core_web_sm\nnlp = en_core_web_sm.load()\n\ndoc = nlp(text)\nprint([(X.text, X.label_) for X in doc.ents])\n\n","1e82246e":"# visualize named entities\ndisplacy.render(doc, style='ent', jupyter=True)","feb7bf12":"named_entities = []\nfor sentence in doc:\n    temp_entity_name = ''\n    temp_named_entity = None\n    sentence = nlp(text)\n    for word in sentence:\n        term = word.text \n        tag = word.ent_type_\n        if tag:\n            temp_entity_name = ' '.join([temp_entity_name, term]).strip()\n            temp_named_entity = (temp_entity_name, tag)\n        else:\n            if temp_named_entity:\n                named_entities.append(temp_named_entity)\n                temp_entity_name = ''\n                temp_named_entity = None\n\nentity_frame = pd.DataFrame(named_entities, \n                            columns=['Entity Name', 'Entity Type'])","0f307f7b":"# get the top named entities\ntop_entities = (entity_frame.groupby(by=['Entity Name', 'Entity Type'])\n                           .size()\n                           .sort_values(ascending=False)\n                           .reset_index().rename(columns={0 : 'Frequency'}))\ntop_entities.T.iloc[:,:20]","3f4a4e71":"import re\n\ntest = text\n\nregex = re.compile(\n    r'^(\\d+) ?([A-Za-z](?= ))? (.*?) ([^ ]+?) ?((?<= )APT)? ?((?<= )\\d*)?$'\n)\n\nprint(sum([regex.findall(x) for x in test],[]))","381b18d9":"import re\nregexp = \"\/\\s+(\\d{2,5}\\s+)(?![a|p]m\\b)(([a-zA-Z|\\s+]{1,5}){1,2})?([\\s|,|.]+)?(([a-zA-Z|\\s+]{1,30}){1,4})(court|ct|street|st|drive|dr|lane|ln|road|rd|blvd)([\\s|,|.|;]+)?(([a-zA-Z|\\s+]{1,30}){1,2})([\\s|,|.]+)?\\b(AK|AL|AR|AZ|CA|CO|CT|DC|DE|FL|GA|GU|HI|IA|ID|IL|IN|KS|KY|LA|MA|MD|ME|MI|MN|MO|MS|MT|NC|ND|NE|NH|NJ|NM|NV|NY|OH|OK|OR|PA|RI|SC|SD|TN|TX|UT|VA|VI|VT|WA|WI|WV|WY)([\\s|,|.]+)?(\\s+\\d{5})?([\\s|,|.]+)\/i\"\naddress = re.findall(regexp, text)\nprint(address)","56fc529f":"!pip install address_parser\n!pip install phonetics","362241ca":"from address_parser import Parser\n\nparser=Parser()\nadr=parser.parse(text)\nprint(adr)","2eacc445":"pip install geotext","e54fdf05":"from geotext import GeoText\nplaces = GeoText(text)\nplaces.cities","d569dae1":"import re\n\nregexp = \"[0-9]{1,3} .+, .+, [A-Z]{2} [0-9]{5}\"\naddress = re.findall(regexp, text)\nprint(address)","93bb1ae6":"# USIN# USING NLTK\nimport nltk","4947d390":"# Tokenize the words\nwords=nltk.word_tokenize(text)","72cc8dc6":"pos_tags=nltk.pos_tag(words)","c0d5ca8d":"# either NE or not NE\nchunks=nltk.ne_chunk(pos_tags,binary=False)","15d6fabd":"entities=[]\nlabels=[]\nfor chunk in chunks:\n    if hasattr(chunk,'label'):\n        entities.append(''.join(c[0] for c in chunk))\n        labels.append(chunk.label())\n        \nentities_labels=list(set(zip(entities,labels)))\nentities_df=pd.DataFrame(entities_labels)\nentities_df.columns=[\"Entities\",\"Labels\"]\nentities_df","139b255f":"!pip install --no-deps seqeval[gpu]","93ff8d35":"%who","c4206b63":"pip install pytorch_pretrained_bert","c367e430":"import numpy as np\nimport pandas as pd\n\nimport spacy\nfrom spacy.gold import biluo_tags_from_offsets\nnlp = spacy.load(\"en_core_web_lg\")\n\nfrom tqdm import trange\nimport torch\nimport torch.nn.functional as F\nfrom torch.optim import Adam\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertConfig\nfrom pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n\nfrom seqeval.metrics import classification_report, accuracy_score, f1_score","ec52f945":"# Adding '\\n' to the default spacy tokenizer\n\nprefixes = ('\\\\n', ) + nlp.Defaults.prefixes\nprefix_regex = spacy.util.compile_prefix_regex(prefixes)\nnlp.tokenizer.prefix_search = prefix_regex.search","a70984ad":"# Personal Custom Tags Dictionary\nentity_dict = {\n    'Name': 'NAME', \n    'College Name': 'CLG',\n    'Degree': 'DEG',\n    'Graduation Year': 'GRADYEAR',\n    'Years of Experience': 'YOE',\n    'Companies worked at': 'COMPANY',\n    'Designation': 'DESIG',\n    'Skills': 'SKILLS',\n    'Location': 'LOC',\n    'Email Address': 'EMAIL'\n}","568690c0":"# loading the dataset\ndf = pd.read_json('\/kaggle\/input\/resume-entities-for-ner\/Entity Recognition in Resumes.json', lines=True)\ndf.head()","73925b9a":"# Checking for unique values present in 'extras' column\ndf['extras'].unique()","e1c0cc92":"# Since, 'extras' column contains no information we can drop the column\ndf = df.drop(['extras'], axis=1)\ndf.head()","9b00d157":"def mergeIntervals(intervals):\n    sorted_by_lower_bound = sorted(intervals, key=lambda tup: tup[0])\n    merged = []\n\n    for higher in sorted_by_lower_bound:\n        if not merged:\n            merged.append(higher)\n        else:\n            lower = merged[-1]\n            if higher[0] <= lower[1]:\n                if lower[2] is higher[2]:\n                    upper_bound = max(lower[1], higher[1])\n                    merged[-1] = (lower[0], upper_bound, lower[2])\n                else:\n                    if lower[1] > higher[1]:\n                        merged[-1] = lower\n                    else:\n                        merged[-1] = (lower[0], higher[1], higher[2])\n            else:\n                merged.append(higher)\n\n    return merged","b50c6051":"# From 'annotation' column, we are extracting the starting index, ending index, entity label\n# So that we can convert the content in BILOU format\n\ndef get_entities(df):\n    \n    entities = []\n    \n    for i in range(len(df)):\n        entity = []\n    \n        for annot in df['annotation'][i]:\n            try:\n                ent = entity_dict[annot['label'][0]]\n                start = annot['points'][0]['start']\n                end = annot['points'][0]['end'] + 1\n                entity.append((start, end, ent))\n            except:\n                pass\n    \n        entity = mergeIntervals(entity)\n        entities.append(entity)\n    \n    return entities","133ef9b2":"# Adding a new column 'entities'\ndf['entities'] = get_entities(df)\ndf.head()","23e8d5c9":"def get_train_data(df):\n    tags = []\n    sentences = []\n\n    for i in range(len(df)):\n        text = df['content'][i]\n        entities = df['entities'][i]\n    \n        doc = nlp(text)\n    \n        tag = biluo_tags_from_offsets(doc, entities)\n        tmp = pd.DataFrame([list(doc), tag]).T\n        loc = []\n        for i in range(len(tmp)):\n            if tmp[0][i].text is '.' and tmp[1][i] is 'O':\n                loc.append(i)\n        loc.append(len(doc))\n    \n        last = 0\n        data = []\n        for pos in loc:\n            data.append([list(doc)[last:pos], tag[last:pos]])\n            last = pos\n    \n        for d in data:\n            tag = ['O' if t is '-' else t for t in d[1]]\n            if len(set(tag)) > 1:\n                sentences.append(d[0])\n                tags.append(tag)\n    \n    return sentences, tags","2e3c3b22":"sentences, tags = get_train_data(df)\nlen(sentences), len(tags)","93259f73":"tag_vals = set(['X', '[CLS]', '[SEP]'])\nfor i in range(len(tags)):\n    tag_vals = tag_vals.union(tags[i])\ntag_vals","34a262bc":"tag2idx = {t: i for i, t in enumerate(tag_vals)}\n","3cc043d0":"idx2tag = {tag2idx[key] : key for key in tag2idx.keys()}\n","7b1bd027":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()","abb9ed96":"tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)","d5f1fa84":"def get_tokenized_train_data(sentences, tags):\n\n    tokenized_texts = []\n    word_piece_labels = []\n\n    for word_list, label in zip(sentences, tags):\n    \n        # Add [CLS] at the front\n        temp_lable = ['[CLS]']\n        temp_token = ['[CLS]']\n    \n        for word, lab in zip(word_list, label):\n            token_list = tokenizer.tokenize(word.text)\n            for m, token in enumerate(token_list):\n                temp_token.append(token)\n                if m == 0:\n                    temp_lable.append(lab)\n                else:\n                    temp_lable.append('X')  \n                \n        # Add [SEP] at the end\n        temp_lable.append('[SEP]')\n        temp_token.append('[SEP]')\n    \n        tokenized_texts.append(temp_token)\n        word_piece_labels.append(temp_lable)\n    \n    return tokenized_texts, word_piece_labels","3fce6c96":"tokenized_texts, word_piece_labels = get_tokenized_train_data(sentences, tags)","40c64a74":"print(tokenized_texts[0])\nprint(word_piece_labels[0])","5648152c":"MAX_LEN = 512\nbs = 4","6fba31d6":"input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\nprint(len(input_ids[0]))\nprint(input_ids[0])","dda7b1d7":"tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in word_piece_labels], maxlen=MAX_LEN, value=tag2idx[\"O\"], \n                     padding=\"post\", dtype=\"long\", truncating=\"post\")\nprint(len(tags[0]))\nprint(tags[0])","a14618ea":"attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\nprint(attention_masks[0])","e379326a":"tr_inputs, val_inputs, tr_tags, val_tags, tr_masks, val_masks = train_test_split(input_ids, tags, attention_masks, random_state=2020, \n                                                                                 test_size=0.3)","60610677":"tr_inputs = torch.tensor(tr_inputs)\nval_inputs = torch.tensor(val_inputs)\ntr_tags = torch.tensor(tr_tags)\nval_tags = torch.tensor(val_tags)\ntr_masks = torch.tensor(tr_masks)\nval_masks = torch.tensor(val_masks)","ecba8b2e":"train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n\nvalid_data = TensorDataset(val_inputs, val_masks, val_tags)\nvalid_sampler = SequentialSampler(valid_data)\nvalid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n","a7b86de8":"model= BertForTokenClassification.from_pretrained(\"bert-base-cased\", num_labels=len(tag2idx))","8f800759":"model.cuda();","a54fc377":"FULL_FINETUNING = True\nif FULL_FINETUNING:\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'gamma', 'beta']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.01},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n         'weight_decay_rate': 0.0}\n    ]\nelse:\n    param_optimizer = list(model.classifier.named_parameters()) \n    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\noptimizer = Adam(optimizer_grouped_parameters, lr=3e-5)","56b51601":"epochs = 10\nmax_grad_norm = 1.0\n\nfor _ in trange(epochs, desc=\"Epoch\"):\n    # TRAIN loop\n    model.train()\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    for step, batch in enumerate(train_dataloader):\n        # add batch to gpu\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_labels = batch\n        # forward pass\n        loss = model(b_input_ids, token_type_ids=None,\n                     attention_mask=b_input_mask, labels=b_labels)\n        # backward pass\n        loss.backward()\n        # track train loss\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n        # gradient clipping\n        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n        # update parameters\n        optimizer.step()\n        model.zero_grad()\n    # print train loss per epoch\n    print(\"Train loss: {}\".format(tr_loss\/nb_tr_steps))","faa552c0":"model.eval()\n\ny_true = []\ny_pred = []\neval_loss, eval_accuracy = 0, 0\nnb_eval_steps, nb_eval_examples = 0, 0\n\nfor batch in valid_dataloader:\n    batch = tuple(t.to(device) for t in batch)\n    input_ids, input_mask, label_ids = batch\n\n    with torch.no_grad():\n        logits = model(input_ids, token_type_ids=None, attention_mask=input_mask,)\n\n    logits = logits.detach().cpu().numpy()\n    logits = [list(p) for p in np.argmax(logits, axis=2)]\n    \n    label_ids = label_ids.to('cpu').numpy()\n    input_mask = input_mask.to('cpu').numpy()\n    \n    for i,mask in enumerate(input_mask):\n        temp_1 = [] # Real one\n        temp_2 = [] # Predict one\n        \n        for j, m in enumerate(mask):\n            # Mark=0, meaning its a pad word, dont compare\n            if m:\n                if idx2tag[label_ids[i][j]] != \"X\" and idx2tag[label_ids[i][j]] != \"[CLS]\" and idx2tag[label_ids[i][j]] != \"[SEP]\" : # Exclude the X label\n                    temp_1.append(idx2tag[label_ids[i][j]])\n                    temp_2.append(idx2tag[logits[i][j]])\n            else:\n                break\n        \n            \n        y_true.append(temp_1)\n        y_pred.append(temp_2)\n    \nprint(\"f1 socre: %f\"%(f1_score(y_true, y_pred)))\nprint(\"Accuracy score: %f\"%(accuracy_score(y_true, y_pred)))\n\nprint(classification_report(y_true, y_pred,digits=4))","7554b7b7":"test_sentence = \"\"\"MY NAME IS ADITYA MISHRA\n\"\"\"","b207e9ef":"from transformers import BertTokenizer, BertConfig","3771fcd1":"tz= BertTokenizer.from_pretrained(\"bert-base-cased\")","028985ac":"pip install config","f73b0cdb":"tokenized_sentence =tz.encode(text=test_sentence,add_special_tokens=True)\ninput_ids = torch.tensor([tokenized_sentence]).cuda()","f60aef20":"with torch.no_grad():\n    output = model(input_ids)\nlabel_indices = np.argmax(output[0].to('cpu').numpy(), axis=0)","71c868ee":"# join bpe split tokens\ntokens = tokenizer.convert_ids_to_tokens(input_ids.to('cpu').numpy()[0])\nnew_tokens, new_labels = [], []\nfor token, label_idx in zip(tokens, label_indices):\n    if token.startswith(\"##\"):\n        new_tokens[-1] = new_tokens[-1] + token[2:]\n    else:\n        new_labels.append(idx2tag[label_idx])\n        new_tokens.append(token)","5e64d557":"for token, label in zip(new_tokens, new_labels):\n    print(\"{}\\t{}\".format(token,label))","23439d34":"**USING SPACY LIB**","63379354":"**USING BERT**","508fa952":"**EXTRACTING PHONE NUMBER**","b114fb2d":"**EXTRACTING EMAIL**","8ae8ef16":"**USING NLTK**","544bb42c":"**EXTRACTING NAME**","0e67487f":"**ADDRESS EXTRACTION**","d5d24992":"**EXTRACTING DATE AND TIME**","889df1d2":"**REPRESENTING EXTRACTED INFO**"}}