{"cell_type":{"171b0898":"code","6e415a9e":"code","e5eb5a54":"code","a204912b":"code","674cec38":"code","3bb0658e":"code","c0f90253":"code","6a97dd9d":"code","5a88f286":"code","8b5ceeb4":"code","b8b0259e":"code","7528b55f":"code","6e8f86a9":"code","08c9ecc6":"code","7ba54eeb":"code","a48555ea":"code","a94d80b5":"code","eaf234b1":"code","6c086093":"markdown","ce6e7e4c":"markdown","ab0852fc":"markdown","e61b6bb8":"markdown","e28fd6d2":"markdown"},"source":{"171b0898":"import numpy as np\nimport pandas as pd\npd.options.display.max_rows = 1000\npd.options.display.max_columns = 1000\n\nfrom collections import Counter\n%matplotlib inline","6e415a9e":"# Load data\ntrain = pd.read_csv('..\/input\/armut_challenge_training.csv', index_col=0, parse_dates=['createdate'])\ntrain = train.sort_values(by=['userid', 'createdate'])\ntest = pd.read_csv('..\/input\/armut_challenge_test.csv', index_col=0)\ntest = test.sort_values(by='userid')\ndisplay(train.head(3))\ndisplay(test.head(3))","e5eb5a54":"# Get long sequences of services\ntrain_services = train.groupby('userid').apply(lambda x:list(x.serviceid.values))\ntrain_services.head()","a204912b":"# Get long sequences of dates\ntrain['createdate_day'] = ((pd.to_datetime('1985-05-06') - train.createdate) \/ pd.to_timedelta('1D')).values\ntrain_days = train.groupby('userid').apply(lambda x:list(x.createdate_day.values))\ntrain_days.head()","674cec38":"feat_len = train.groupby('userid').size().to_frame('feat_len').reset_index()\nfeat_len.head()","3bb0658e":"# Build sequences from train set. This shouldn't be confused with given test set.\n# We'll use it as an oof set.\nSEQ_SIZE = 5\ntrain_sequences = []\ntrain_sequence_ids = []\ntrain_sequence_dates = []\nfor uid, s in train_services.items():\n    train_sequences.extend([s[i:i+SEQ_SIZE] for i in range(len(s)-SEQ_SIZE+1)])\n    train_sequence_ids.extend( [uid] * (len(s)-SEQ_SIZE+1) )\n    td = train_days[uid]\n    train_sequence_dates.extend([td[i:i+SEQ_SIZE-1] for i in range(len(td)-SEQ_SIZE+1)]) #td[3:-1])\n\ntrain_sequences = pd.DataFrame(np.array(train_sequences),\n                                columns=['last_4', 'last_3', 'last_2', 'last_1', 'serviceid'])\ntrain_sequences['userid'] = train_sequence_ids\ntrain_sequences = train_sequences[train_sequences.columns[::-1]]\n\ntrain_sequence_dates = pd.DataFrame(np.array(train_sequence_dates),\n                                     columns=['date_4', 'date_3', 'date_2', 'date_1'])\ntrain_sequence_dates = train_sequence_dates[train_sequence_dates.columns[::-1]]\n\ntrain_sequences = pd.concat([train_sequences, train_sequence_dates], axis=1, sort=False)\n\ntrain_sequences = train_sequences.merge(feat_len, on='userid', how='inner')\n\ntrain_sequences['date_43'] = train_sequences['date_4'] - train_sequences['date_3']\ntrain_sequences['date_32'] = train_sequences['date_3'] - train_sequences['date_2']\ntrain_sequences['date_21'] = train_sequences['date_2'] - train_sequences['date_1']\n\nprint(train_sequences.shape)\ntrain_sequences.head()","c0f90253":"# Build sequences from train set. We'll use it on both oof test and prediction.\nSEQ_SIZE = 5\ntest_sequences = []\ntest_sequence_dates = []\nfor uid, s in train_services.items():\n    test_sequences.append(s[-SEQ_SIZE+1:]) # 4 adet\n    td = train_days[uid]\n    test_sequence_dates.append(td[-SEQ_SIZE+1:])\n\ntest_sequences = pd.DataFrame(np.array(test_sequences),\n                              columns=['last_4', 'last_3', 'last_2', 'last_1'])\ntest_sequences['userid'] = train_services.index\ntest_sequences = test_sequences[test_sequences.columns[::-1]]\n\ntest_sequence_dates = pd.DataFrame(np.array(test_sequence_dates),\n                                   columns=['date_4', 'date_3', 'date_2', 'date_1'])\ntest_sequence_dates = test_sequence_dates[test_sequence_dates.columns[::-1]]\ntest_sequences = pd.concat([test_sequences, test_sequence_dates], axis=1, sort=False)\ntest_sequences = test_sequences.merge(feat_len, on='userid', how='inner')\n\ntest_sequences['date_43'] = test_sequences['date_4'] - test_sequences['date_3']\ntest_sequences['date_32'] = test_sequences['date_3'] - test_sequences['date_2']\ntest_sequences['date_21'] = test_sequences['date_2'] - test_sequences['date_1']\n\ntest_sequences = test_sequences.merge(test, on='userid', how='left')\n\nprint(test_sequences.shape)\ntest_sequences.head()","6a97dd9d":"# Prepare train and test set for training\ntmp = train_sequences.copy()\n\n# Create targets\n#tmp['target'] = 1*(tmp.serviceid == target_service_id)\ntmp[f'target_last_1'] = 1*(tmp['last_1'] == tmp.serviceid)\ntmp[f'target_last_2'] = 1*(tmp['last_2'] == tmp.serviceid)\ntmp[f'target_last_3'] = 1*(tmp['last_3'] == tmp.serviceid)\ntmp[f'target_last_4'] = 1*(tmp['last_4'] == tmp.serviceid)\n    \n# Split test and train\ntmp = tmp.reset_index()\n#df_test = tmp.groupby('userid').last().reset_index()\n#df_train = tmp[~tmp['index'].isin(df_test['index'])].copy()\n\nfrom sklearn.model_selection import train_test_split\nuids = tmp.userid.unique()\nuid_train, uid_test = train_test_split(uids, test_size=0.2, random_state=84)\ndf_train = tmp[tmp.userid.isin(set(uid_train))].copy()\ndf_test  = tmp[tmp.userid.isin(set(uid_test))].copy()\n\nprint(f'df_train.shape: {df_train.shape}')\nprint(f'df_test.shape: {df_test.shape}')","5a88f286":"col_cat = [\n    'last_1', 'last_2', 'last_3', 'last_4',\n    'mc_1', 'mc_2', 'mc_3', 'mc_4'\n]\ncol_targets = [c for c in tmp.columns if 'target' in c]\ncol_preds = [c for c in tmp.columns if 'pred' in c]\ncol_not_use = ['userid', 'serviceid', 'winner', 'index'] + col_targets + col_preds\ncol_not_use += [\n    'mc_1', 'mc_2', 'mc_3', 'mc_4',\n    'date_1', 'date_2', 'date_3', 'date_4'\n]\ncol_use = [c for c in tmp.columns if c not in col_not_use]\ncol_cat = [c for c in col_cat if c in col_use]\n\nprint('col_use', col_use)\nprint('col_targets',col_targets)\nprint(f'df_train.shape: {df_train.shape}')\nprint(f'df_test.shape: {df_test.shape}')\n\nfrom lightgbm import LGBMClassifier\nmodels = dict()\nfor col_target in col_targets:\n    \n    print(f'Model for {col_target}')\n    \n    model = LGBMClassifier(objective='binary', random_state=42, learning_rate=0.1,\n                           n_estimators=2000,\n                           reg_alpha=5,\n                           reg_lambda=5,)\n    model.fit(df_train[df_train[col_target].notnull()][col_use],\n              df_train[df_train[col_target].notnull()][col_target],\n              #categorical_feature=col_cat,\n              early_stopping_rounds=100,\n              eval_set=(df_test[df_test[col_target].notnull()][col_use],\n                        df_test[df_test[col_target].notnull()][col_target]),\n              eval_metric=['binary_logloss'],\n              verbose=100)\n    col_pred = col_target.replace('target', 'pred')\n    models[col_pred] = model\n    preds = model.predict_proba(df_test[col_use])[:, 1]\n    print(pd.crosstab(preds>0.5, df_test[col_target]))\n    \n    print('OK')\nprint('Done')","8b5ceeb4":"tmp = df_test.copy()\n\n# Create targets\n#tmp['target'] = 1*(tmp.serviceid == target_service_id)\ntmp[f'target_last_1'] = 1*(tmp['last_1'] == tmp.serviceid)\ntmp[f'target_last_2'] = 1*(tmp['last_2'] == tmp.serviceid)\ntmp[f'target_last_3'] = 1*(tmp['last_3'] == tmp.serviceid)\ntmp[f'target_last_4'] = 1*(tmp['last_4'] == tmp.serviceid)\n\n# Make predictions\nfor col_pred, model in models.items():\n    tmp[col_pred] = model.predict_proba(tmp[col_use])[:, 1]\n","b8b0259e":"# Calibrate predictions for test set.\nfrom itertools import product\n\nr = np.round(np.arange(0.9, 1.1, 0.02), 2)\nks = []\nfor k2, k3, k4 in product(r, r, r):\n    k1 = 1\n    ttt = tmp.copy()\n    ttt['pred_last_1'] *= k1\n    ttt['pred_last_2'] *= k2\n    ttt['pred_last_3'] *= k3\n    ttt['pred_last_4'] *= k4\n    ix = tmp.index\n    kkk = ttt.loc[ix, ['pred_last_1','pred_last_2','pred_last_3','pred_last_4']].idxmax(axis=1)\n    ttt.loc[ix, 'winner'] = kkk\n    ix = (ttt['winner'] == 'pred_last_1')\n    ttt.loc[ix, 'pred'] = ttt.loc[ix, 'last_1']\n    ix = (ttt['winner'] == 'pred_last_2')\n    ttt.loc[ix, 'pred'] = ttt.loc[ix, 'last_2']\n    ix = (ttt['winner'] == 'pred_last_3')\n    ttt.loc[ix, 'pred'] = ttt.loc[ix, 'last_3']\n    ix = (ttt['winner'] == 'pred_last_4')\n    ttt.loc[ix, 'pred'] = ttt.loc[ix, 'last_4']\n    rr = (ttt['serviceid'] == ttt['pred']).mean()\n    print([k1, k2, k3, k4, rr], ' '*10, end='\\r')\n    ks.append([k1, k2, k3, k4, rr])\nprint('')\nprint('Done.')\n\nks = pd.DataFrame(ks)\nks.sort_values(by=4, ascending=False, inplace=True)\nks.head(10)","7528b55f":"# Get OOF set\ntmp = test_sequences[test_sequences['serviceid'].notnull()].copy()\n\n# Create targets\n#tmp['target'] = 1*(tmp.serviceid == target_service_id)\ntmp[f'target_last_1'] = 1*(tmp['last_1'] == tmp.serviceid)\ntmp[f'target_last_2'] = 1*(tmp['last_2'] == tmp.serviceid)\ntmp[f'target_last_3'] = 1*(tmp['last_3'] == tmp.serviceid)\ntmp[f'target_last_4'] = 1*(tmp['last_4'] == tmp.serviceid)\n\n# Make predictions\nfor col_pred, model in models.items():\n    tmp[col_pred] = model.predict_proba(tmp[col_use])[:, 1]\n    \ntmp.head()","6e8f86a9":"# Calibrate predictions for OOF set.\nfrom itertools import product\n\nr = np.round(np.arange(0.9, 1.1, 0.02), 2)\nks = []\nfor k2, k3, k4 in product(r, r, r):\n    k1 = 1\n    ttt = tmp.copy()\n    ttt['pred_last_1'] *= k1\n    ttt['pred_last_2'] *= k2\n    ttt['pred_last_3'] *= k3\n    ttt['pred_last_4'] *= k4\n    ix = tmp.index\n    kkk = ttt.loc[ix, ['pred_last_1','pred_last_2','pred_last_3','pred_last_4']].idxmax(axis=1)\n    ttt.loc[ix, 'winner'] = kkk\n    ix = (ttt['winner'] == 'pred_last_1')\n    ttt.loc[ix, 'pred'] = ttt.loc[ix, 'last_1']\n    ix = (ttt['winner'] == 'pred_last_2')\n    ttt.loc[ix, 'pred'] = ttt.loc[ix, 'last_2']\n    ix = (ttt['winner'] == 'pred_last_3')\n    ttt.loc[ix, 'pred'] = ttt.loc[ix, 'last_3']\n    ix = (ttt['winner'] == 'pred_last_4')\n    ttt.loc[ix, 'pred'] = ttt.loc[ix, 'last_4']\n    rr = (ttt['serviceid'] == ttt['pred']).mean()\n    print([k1, k2, k3, k4, rr], ' '*10, end='\\r')\n    ks.append([k1, k2, k3, k4, rr])\nprint('')\nprint('Done.')\n\nks = pd.DataFrame(ks)\nks.sort_values(by=4, ascending=False, inplace=True)\nks.head(10)","08c9ecc6":"tmp = test_sequences[test_sequences['serviceid'].isnull()].copy()\n\n# Create targets\n#tmp['target'] = 1*(tmp.serviceid == target_service_id)\ntmp[f'target_last_1'] = 1*(tmp['last_1'] == tmp.serviceid)\ntmp[f'target_last_2'] = 1*(tmp['last_2'] == tmp.serviceid)\ntmp[f'target_last_3'] = 1*(tmp['last_3'] == tmp.serviceid)\ntmp[f'target_last_4'] = 1*(tmp['last_4'] == tmp.serviceid)\n\ntmp.head()","7ba54eeb":"ttt = tmp.copy()\n\n#k1, k2, k3, k4 = 1.0, 1.06, 0.92, 1.06\n#k1, k2, k3, k4 = 1.0, 0.92, 1.04, 0.98\n\n#k1, k2, k3, k4 = 1.0, 1.04, 0.90, 0.94\nk1, k2, k3, k4 = 1.0, 1.08, 0.98, 0.98\n\nfor col_pred, model in models.items():\n    ttt[col_pred] = model.predict_proba(ttt[col_use])[:, 1]\nttt['pred_last_1'] *= k1\nttt['pred_last_2'] *= k2\nttt['pred_last_3'] *= k3\nttt['pred_last_4'] *= k4\nix = ttt.index\nttt['pred'] = 0\nkkk = ttt.loc[ix, ['pred_last_1','pred_last_2','pred_last_3','pred_last_4']].idxmax(axis=1)\nttt.loc[ix, 'winner'] = kkk\nix = (ttt['winner'] == 'pred_last_1')\nttt.loc[ix, 'pred'] = ttt.loc[ix, 'last_1']\nix = (ttt['winner'] == 'pred_last_2')\nttt.loc[ix, 'pred'] = ttt.loc[ix, 'last_2']\nix = (ttt['winner'] == 'pred_last_3')\nttt.loc[ix, 'pred'] = ttt.loc[ix, 'last_3']\nix = (ttt['winner'] == 'pred_last_4')\nttt.loc[ix, 'pred'] = ttt.loc[ix, 'last_4']\n#rr = (ttt['serviceid'] == ttt['pred']).mean()\nprint([k1, k2, k3, k4])\nttt['serviceid'] = ttt['pred']","a48555ea":"ttt.head()","a94d80b5":"filename = f'submission_7_84_{k1*100:03.0f}_{k2*100:03.0f}_{k3*100:03.0f}_{k4*100:03.0f}.csv'\nfilename","eaf234b1":"ttt[['userid', 'serviceid']].to_csv(filename, index=False)","6c086093":"### Train Model","ce6e7e4c":"Herkese selamlar.\n\n\u00d6ncelikle son dakika ata\u011f\u0131yla 1. olan Ostrich Team ekibini, @malietu ve @burakcanizmirli arkada\u015flar\u0131m\u0131z\u0131 kutlar\u0131m.\n\n@yvztpe ve  @safakacar '\u0131n forumdaki yorumlar\u0131 do\u011frultusunda, ben de \u00e7\u00f6z\u00fcm\u00fcm\u00fc payla\u015fmak isterim. \u00d6zetle 1 \u00f6nceki \u00fcr\u00fcn\u00fcn test k\u00fcmesinde %15 gibi bir frekans\u0131 var. 2 \u00f6nceki \u00fcr\u00fcn de %10 ... ve gidiyor. Ben de m\u00fc\u015fterinin 1, 2, 3 ya da 4 \u00f6nceki \u00fcr\u00fcn\u00fc alma olas\u0131l\u0131\u011f\u0131 \u00fczerine 4 model kurup, e\u015fik de\u011ferlerini optimize etmeye \u00e7al\u0131\u015f\u0131yorum. Modelleri e\u011fitirken train test k\u00fcmesini userid'ler \u00fczerinden kurdum. A\u015fa\u011f\u0131daki kodda \u00e7e\u015fitli e\u015fik de\u011ferleri denedi\u011fim k\u0131s\u0131m var. En son submission bir iki \u00e7\u0131kt\u0131n\u0131n birle\u015fimi oldu\u011fu i\u00e7in ayn\u0131 sonucu alamayabilirsiniz.\n\nLaf\u0131 uzatmadan, s\u00f6z\u00fc koda b\u0131rakay\u0131m :)\n","ab0852fc":"### Build Train and Test data","e61b6bb8":"### OOF Test","e28fd6d2":"## Predict"}}