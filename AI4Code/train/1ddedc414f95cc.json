{"cell_type":{"8ea45fa6":"code","34f9079c":"code","f741ff2f":"code","e286e4cc":"code","06433405":"code","7ba67d90":"code","237e5404":"code","a4b07e89":"code","f11e48e0":"code","2cb3e0c9":"code","2dc9065c":"code","1e72e021":"code","57d18e53":"code","cb3af0fd":"code","6da6445f":"code","8351215e":"code","3d7f8f79":"code","2aa63aa1":"code","43bfa677":"code","81a87b7b":"code","3f2dfd18":"code","baa57efe":"code","4a5d12ff":"code","d5c33dee":"markdown","82a53980":"markdown","e670986a":"markdown","5ed2f4df":"markdown","bf986823":"markdown","6e73495f":"markdown","05da9aa2":"markdown","c144c98e":"markdown","4629d6c2":"markdown","cf584351":"markdown","376473ee":"markdown","aa7222c4":"markdown","bad8b01a":"markdown","26ab311b":"markdown","54fbe1ad":"markdown","3605194f":"markdown","b5368414":"markdown","d1110adf":"markdown","504b545f":"markdown"},"source":{"8ea45fa6":"import numpy as np\nimport pandas as pd","34f9079c":"def numbify_data(df):\n    # Convert sex to numeric\n    df['Sex'].replace('female', 0, inplace=True)\n    df['Sex'].replace('male', 1, inplace=True)\n\n    # Convert embarked to numeric\n    df['Embarked'].replace('Q', 0, inplace=True)\n    df['Embarked'].replace('S', 1, inplace=True)\n    df['Embarked'].replace('C', 2, inplace=True)\n\n    # Replace nan values by mean\n    df.fillna(df.mean(), inplace=True)\n    \n    \ndef load_train_set():\n    df = pd.read_csv('..\/input\/train.csv', delimiter = ',')\n    df = df[['Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n    numbify_data(df)\n    train_set_x = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].values.T\n    train_set_y = df[['Survived']].values.T\n    return train_set_x, train_set_y\n\ndef load_test_set():\n    df = pd.read_csv('..\/input\/test.csv', delimiter = ',')\n    df = df[['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']]\n    numbify_data(df)\n    test_set_x = df[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']].values.T\n    test_set_id = df[['PassengerId']].values.T\n    return test_set_x, test_set_id\n\ndef sigmoid(z):\n    s = 1\/(1 + np.exp(-z))\n    return s\n\ndef relu(x):\n    return np.abs(x) * (x > 0)","f741ff2f":"train_set_x, train_set_y = load_train_set()\ntest_set_x, test_set_id = load_test_set()","e286e4cc":"m_train = train_set_x.shape[1]\nm_test = test_set_x.shape[1]\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of testing examples: m_test = \" + str(m_test))","06433405":"\ndef initialize_with_zeros(dim_features):\n    w = np.zeros((dim_features, 1))\n    b = 0\n    return w, b","7ba67d90":"dim_features = train_set_x.shape[0]\nw, b = initialize_with_zeros(dim_features)\nprint (\"w = \" + str(w))\nprint (\"b = \" + str(b))","237e5404":"def propagate(w, b, X, Y):\n    m = X.shape[1]\n    #forward propagation\n    A = sigmoid(np.dot(w.T, X) + b)\n    cost = (-1\/m) * np.sum(Y*np.log(A) + (1 - Y)*np.log(1-A))\n    \n    #backward propagation\n    dw = (1\/m) * np.dot(X, (A-Y).T)\n    db = (1\/m) * np.sum(A-Y)\n    \n    grads = {'dw': dw,\n            'db': db}\n    return grads, cost\n    ","a4b07e89":"def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost=False):\n    costs = []\n    for i in range(num_iterations):\n        grads, cost = propagate(w, b, X, Y)\n        w = w - learning_rate*grads['dw']\n        b = b - learning_rate*grads['db']\n        if print_cost and i % 1000 == 0:\n            print(\"cost after iteration\", i,\":\", cost)\n        costs.append(cost)\n    params = {'w': w,\n             'b': b}\n\n    grads = {'dw': grads['dw'],\n             'db': grads['db']}\n    \n    return params, grads, costs","f11e48e0":"X = train_set_x\nY = train_set_y\nw, b = initialize_with_zeros(X.shape[0])\nparams, grads, costs = optimize(w, b, X, Y, num_iterations= 10000, learning_rate = 0.004, print_cost = True)\n\nprint (\"w = \" + str(params[\"w\"]))\nprint (\"b = \" + str(params[\"b\"]))","2cb3e0c9":"def predict(w, b, X):\n    m = X.shape[1]\n    Y_prediction = np.zeros((1,m))\n    w = w.reshape(X.shape[0], 1)\n    \n    A = sigmoid(np.dot(w.T,X) + b)\n    \n    for i in range(A.shape[1]):\n        Y_prediction[0, i] = 1 if A[0, i] >= .5 else 0 \n        pass\n    return Y_prediction","2dc9065c":"X_train = train_set_x\nY_train = train_set_y\n\nnp.nan_to_num(X_train, copy=False)\nnp.nan_to_num(Y_train, copy=False)\n\nX_test = test_set_x\nnp.nan_to_num(X_test, copy=False)\n\nY_prediction_test = predict(w,b,X_test)\nY_prediction_train = predict(w,b,X_train)\n\nprint(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\nnp.savetxt('submission.csv', np.hstack([test_set_id.T, Y_prediction_test.T]), delimiter=',')","1e72e021":"def layer_sizes(X, Y):\n    \"\"\"\n    Arguments:\n    X -- input dataset of shape (input size, number of examples)\n    Y -- labels of shape (output size, number of examples)\n    \n    Returns:\n    n_x -- the size of the input layer\n    n_h -- the size of the hidden layer\n    n_y -- the size of the output layer\n    \"\"\"\n    n_x = X.shape[0] # size of input layer\n    n_h = 5\n    n_y = Y.shape[0] # size of output layer\n    return (n_x, n_h, n_y)","57d18e53":"(n_x, n_h, n_y) = layer_sizes(X_train, Y_train)\nprint(\"The size of the input layer is: n_x = \" + str(n_x))\nprint(\"The size of the hidden layer is: n_h = \" + str(n_h))\nprint(\"The size of the output layer is: n_y = \" + str(n_y))","cb3af0fd":"def initialize_parameters(n_x, n_h, n_y):\n    \"\"\"\n    Argument:\n    n_x -- size of the input layer\n    n_h -- size of the hidden layer\n    n_y -- size of the output layer\n    \n    Returns:\n    params -- python dictionary containing your parameters:\n                    W1 -- weight matrix of shape (n_h, n_x)\n                    b1 -- bias vector of shape (n_h, 1)\n                    W2 -- weight matrix of shape (n_y, n_h)\n                    b2 -- bias vector of shape (n_y, 1)\n    \"\"\"\n    W1 = (np.random.randn(n_h,n_x)*0.01).reshape((n_h, n_x))\n    b1 = np.zeros((n_h,1))\n    W2 = (np.random.rand(n_y,n_h)*0.01).reshape((n_y, n_h))\n    b2 = np.zeros((n_y, 1))\n    \n    assert (W1.shape == (n_h, n_x))\n    assert (b1.shape == (n_h, 1))\n    assert (W2.shape == (n_y, n_h))\n    assert (b2.shape == (n_y, 1))\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","6da6445f":"parameters = initialize_parameters(n_x, n_h, n_y)\nprint(\"W1 = \" + str(parameters[\"W1\"]))\nprint(\"b1 = \" + str(parameters[\"b1\"]))\nprint(\"W2 = \" + str(parameters[\"W2\"]))\nprint(\"b2 = \" + str(parameters[\"b2\"]))","8351215e":"def forward_propagation(X, parameters):\n    \"\"\"\n    Argument:\n    X -- input data of size (n_x, m)\n    parameters -- python dictionary containing your parameters (output of initialization function)\n    \n    Returns:\n    A2 -- The sigmoid output of the second activation\n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n    \"\"\"\n    # Retrieve each parameter from the dictionary \"parameters\"\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Implement Forward Propagation to calculate A2 (probabilities)\n    Z1 = np.dot(W1,X) + b1\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(W2,A1) + b2\n    A2 = sigmoid(Z2)\n    \n    cache = {\"Z1\": Z1,\n             \"A1\": A1,\n             \"Z2\": Z2,\n             \"A2\": A2}\n    \n    return A2, cache","3d7f8f79":"def compute_cost(A2, Y, parameters):\n    \"\"\"\n    Arguments:\n    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n    \n    Returns:\n    cost -- cross-entropy cost given equation (13)\n    \"\"\"\n    \n    m = Y.shape[1] # number of example\n\n    # Compute the cross-entropy cost\n    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1 - A2),1 - Y)\n    cost = -1\/m * np.sum(logprobs)\n    \n    cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n                                # E.g., turns [[17]] into 17 \n    \n    return cost","2aa63aa1":"def backward_propagation(parameters, cache, X, Y):\n    \"\"\"\n    Arguments:\n    parameters -- python dictionary containing our parameters \n    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n    X -- input data of shape (2, number of examples)\n    Y -- \"true\" labels vector of shape (1, number of examples)\n    \n    Returns:\n    grads -- python dictionary containing your gradients with respect to different parameters\n    \"\"\"\n    m = X.shape[1]\n    \n    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n    W1 = parameters[\"W1\"]\n    W2 = parameters[\"W2\"]\n        \n    # Retrieve also A1 and A2 from dictionary \"cache\".\n    A1 = cache[\"A1\"]\n    A2 = cache[\"A2\"]\n    \n    # Backward propagation: calculate dW1, db1, dW2, db2. \n    dZ2 = A2 - Y\n    dW2 = 1\/m * np.dot(dZ2,A1.T)\n    db2 = 1\/m * np.sum(dZ2, axis = 1, keepdims = True)\n    dZ1 = np.dot(W2.T,dZ2)*(1 - np.power(A1,2))\n    dW1 = 1\/m * np.dot(dZ1,X.T)\n    db1 = 1\/m * np.sum(dZ1, axis = 1, keepdims = True)\n    \n    grads = {\"dW1\": dW1,\n             \"db1\": db1,\n             \"dW2\": dW2,\n             \"db2\": db2}\n    \n    return grads","43bfa677":"def update_parameters(parameters, grads, learning_rate = 0.005):\n    \"\"\"\n    Arguments:\n    parameters -- python dictionary containing your parameters \n    grads -- python dictionary containing your gradients \n    \n    Returns:\n    parameters -- python dictionary containing your updated parameters \n    \"\"\"\n    # Retrieve each parameter from the dictionary \"parameters\"\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Retrieve each gradient from the dictionary \"grads\"\n    dW1 = grads[\"dW1\"]\n    db1 = grads[\"db1\"]\n    dW2 = grads[\"dW2\"]\n    db2 = grads[\"db2\"]\n    \n    # Update rule for each parameter\n    W1 = W1 - learning_rate*dW1\n    b1 = b1 - learning_rate*db1\n    W2 = W2 - learning_rate*dW2\n    b2 = b2 - learning_rate*db2\n    \n    parameters = {\"W1\": W1,\n                  \"b1\": b1,\n                  \"W2\": W2,\n                  \"b2\": b2}\n    \n    return parameters","81a87b7b":"def nn_model(X, Y, n_h, num_iterations = 10000, learning_rate = 0.004, print_cost=False):\n    \"\"\"\n    Arguments:\n    X -- dataset of shape (2, number of examples)\n    Y -- labels of shape (1, number of examples)\n    n_h -- size of the hidden layer\n    num_iterations -- Number of iterations in gradient descent loop\n    print_cost -- if True, print the cost every 1000 iterations\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    \"\"\"\n    \n    np.random.seed(3)\n    n_x = layer_sizes(X, Y)[0]\n    n_y = layer_sizes(X, Y)[2]\n    \n    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n    parameters = initialize_parameters(n_x, n_h, n_y)\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n        A2, cache = forward_propagation(X, parameters)\n        \n        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n        cost = compute_cost(A2,Y,parameters)\n \n        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n        grads = backward_propagation(parameters, cache, X, Y)\n \n        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n        # Print the cost every 10000 iterations\n        if print_cost and i % 10000 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n\n    return parameters","3f2dfd18":"parameters = nn_model(X_train, Y_train, 5, num_iterations=100000, learning_rate=0.004, print_cost=True)","baa57efe":"def predict(parameters, X):\n    \"\"\"\n    Using the learned parameters, predicts a class for each example in X\n    \n    Arguments:\n    parameters -- python dictionary containing your parameters \n    X -- input data of size (n_x, m)\n    \n    Returns\n    predictions -- vector of predictions of our model (red: 0 \/ blue: 1)\n    \"\"\"\n    \n    # Computes probabilities using forward propagation, and classifies to 0\/1 using 0.5 as the threshold.\n    A2, cache = forward_propagation(X, parameters)\n    predictions = A2 > .5\n    predictions = predictions.astype(int)\n    \n    return predictions","4a5d12ff":"predictions_train = predict(parameters, X_train)\nprint ('Accuracy on train set: %d' % float((np.dot(Y,predictions_train.T) + np.dot(1-Y,1-predictions_train.T))\/float(Y.size)*100) + '%')\npredictions_test = predict(parameters, X_test)\ndf = pd.DataFrame({'PassengerId':np.squeeze(test_set_id), 'Survived':np.squeeze(predictions_test)})\ndf.to_csv('submission.csv',index=False)","d5c33dee":"## 4.5 - Update Paramaters","82a53980":"## 4.6 - Integrate all functions into the neural network model","e670986a":"## 2.3 - Overview of the data set","5ed2f4df":"## 3.4 - Testing the model","bf986823":"## 3.5 - Conclusion : Simple logistic regression\nAt this point, we can see that, with a ridiculous train accuracy of 38%, we can safely assume that a simple logistic regression model is far from the best solution to this problem.\n\nLet's give it another shot with a more sophisticated model. i.e with more hidden layers.","6e73495f":"## 2.2 - Load training set and test set","05da9aa2":"## 4.1 - Defining the neural network structure","c144c98e":"## 3.3 - Optimization","4629d6c2":"## 4.3 Forward Propagation","cf584351":"## 3.1 - Initializing parameters","376473ee":"# 4 - Neural Network Model\n\nWe are now going to try with a 1 hidden layer neural network","aa7222c4":"## 4.4 - Backward Propagation","bad8b01a":"# 3 - Simple logistic regression","26ab311b":"## 3.2 - Forward and BackWard Propagation","54fbe1ad":"## 4.2 - Initialize the model's parameters","3605194f":"# 1 - Packages","b5368414":"# Titanic: Machine Learning from Disaster\n- **Goal**\nYou are asked to predict if a passenger survived the sinking of the Titanic or not. \nFor each in the test set, we must predict a 0 or 1 value for the variable.\n- **Metric**\nYour score is the percentage of passengers you correctly predict. This is known simply as \"accuracy\u201d.\n- **Submission File Format**\nYou should submit a csv file with exactly 418 entries plus a header row. Your submission will show an error if you have extra columns (beyond PassengerId and Survived) or rows.\n\nThe file should have exactly 2 columns:\n\nPassengerId (sorted in any order)\nSurvived (contains your binary predictions: 1 for survived, 0 for deceased)\n~~~\nPassengerId,Survived\n 892,0\n 893,1\n 894,0\n Etc.\n ~~~","d1110adf":"# 2 - Load Data\nFirst we have to select \"the most\" relevant features to use.\nAfter quick inspection of the train.csv file. I select `pclass, sex, age, sibsp, parch, fare, embarked`\n## 2.1 - Utility functions ","504b545f":"## 4.7 Conclusion - Shallow neural network\n\nNow we have an accuracy of `84%` which much better compare to the one we had using "}}