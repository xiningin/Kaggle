{"cell_type":{"a26a7ceb":"code","1738898a":"code","c93ae33c":"code","62ee643d":"code","8cb6571b":"code","7ecdbbb5":"code","51d448cd":"code","ea474922":"markdown","043d827e":"markdown"},"source":{"a26a7ceb":"import os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import f1_score, accuracy_score\n\nplt.style.use('dark_background')","1738898a":"train = pd.read_csv('..\/input\/ghost-drift-and-outliers\/train_clean_kalman.csv')\ntest= pd.read_csv('..\/input\/ghost-drift-and-outliers\/test_clean_kalman.csv')","c93ae33c":"class ViterbiClassifier:\n    def __init__(self, num_bins=1000):\n        self._n_bins = num_bins\n        self._p_trans = None\n        self._p_signal = None\n        self._signal_bins = None\n        self._p_in = None\n    \n    def fit(self, x, y):\n        self._p_trans = self.markov_p_trans(y)\n        self._p_signal, self._signal_bins = self.markov_p_signal(true_state, x, self._n_bins)\n        \n        self._p_in = np.ones(len(self._p_trans)) \/ len(self._p_trans)\n        return self\n        \n    def predict(self, x):\n        x_dig = self.digitize_signal(x, self._signal_bins)\n        return self.viterbi(self._p_trans, self._p_signal, self._p_in, x_dig)\n    \n    @classmethod\n    def digitize_signal(cls, signal, signal_bins):\n        # https:\/\/www.kaggle.com\/friedchips\/the-viterbi-algorithm-a-complete-solution\n        signal_dig = np.digitize(signal, bins=signal_bins) - 1 # these -1 and -2 are necessary because of the way...\n        signal_dig = np.minimum(signal_dig, len(signal_bins) - 2) # ... numpy.digitize works\n        return signal_dig\n    \n    @classmethod\n    def markov_p_signal(cls, state, signal, num_bins = 1000):\n        # https:\/\/www.kaggle.com\/friedchips\/the-viterbi-algorithm-a-complete-solution\n        states_range = np.arange(state.min(), state.max() + 1)\n        signal_bins = np.linspace(signal.min(), signal.max(), num_bins + 1)\n        p_signal = np.array([ np.histogram(signal[state == s], bins=signal_bins)[0] for s in states_range ])\n        p_signal = np.array([ p \/ np.sum(p) if np.sum(p) != 0 else p for p in p_signal ]) # normalize to 1\n        return p_signal, signal_bins\n    \n    @classmethod\n    def markov_p_trans(cls, states):\n        # https:\/\/www.kaggle.com\/friedchips\/the-viterbi-algorithm-a-complete-solution\n        max_state = np.max(states)\n        states_next = np.roll(states, -1)\n        matrix = []\n        for i in range(max_state + 1):\n            current_row = np.histogram(states_next[states == i], bins=np.arange(max_state + 2))[0]\n            if np.sum(current_row) == 0: # if a state doesn't appear in states...\n                current_row = np.ones(max_state + 1) \/ (max_state + 1) # ...use uniform probability\n            else:\n                current_row = current_row \/ np.sum(current_row) # normalize to 1\n            matrix.append(current_row)\n        return np.array(matrix)\n    \n    @classmethod\n    def viterbi(cls, p_trans, p_signal, p_in, signal):\n        # https:\/\/www.kaggle.com\/friedchips\/the-viterbi-algorithm-a-complete-solution\n        offset = 10**(-20) # added to values to avoid problems with log2(0)\n\n        p_trans_tlog  = np.transpose(np.log2(p_trans  + offset)) # p_trans, logarithm + transposed\n        p_signal_tlog = np.transpose(np.log2(p_signal + offset)) # p_signal, logarithm + transposed\n        p_in_log      =              np.log2(p_in     + offset)  # p_in, logarithm\n\n        p_state_log = [ p_in_log + p_signal_tlog[signal[0]] ] # initial state probabilities for signal element 0 \n\n        for s in signal[1:]:\n            p_state_log.append(np.max(p_state_log[-1] + p_trans_tlog, axis=1) + p_signal_tlog[s]) # the Viterbi algorithm\n\n        states = np.argmax(p_state_log, axis=1) # finding the most probable states\n    \n        return states","62ee643d":"true_state = train.open_channels.values\nsignal = train.signal.values","8cb6571b":"viterbi = ViterbiClassifier().fit(signal, true_state)\ntrain_prediction = viterbi.predict(signal)","7ecdbbb5":"print(\"Accuracy =\", accuracy_score(y_pred=train_prediction, y_true=true_state))\nprint(\"F1 macro =\", f1_score(y_pred=train_prediction, y_true=true_state, average='macro'))","51d448cd":"df_subm = pd.read_csv(\"..\/input\/liverpool-ion-switching\/sample_submission.csv\")\ndf_subm['open_channels'] = viterbi.predict(test.signal.values)\ndf_subm.to_csv(\"viterbi.csv\", float_format='%.4f', index=False)","ea474922":"## Intruduction\nThis research based on simple [Viterbi Algorith](https:\/\/www.kaggle.com\/friedchips\/the-viterbi-algorithm-a-complete-solution) by [Markus F](https:\/\/www.kaggle.com\/friedchips) and my [previous work with data cleaning](https:\/\/www.kaggle.com\/miklgr500\/ghost-drift-and-outliers). The main aim of this research is to understand usability cleaned data and the ability to avoid group workouts without losing quality metrics on this data.","043d827e":"## Conclusion\nThe Viterbi algorithm fitted on cleaned data without groups obtain the same result as the Viterbi algorithm, which trained on groups. So cleaned data help to avoid overfitting algorithms on constructed groups."}}