{"cell_type":{"6d949794":"code","5654ab99":"code","719643ad":"code","802543e9":"code","57050771":"code","21ddeeaa":"code","ce9745e3":"code","53154535":"code","a1a2f667":"code","14b2b783":"code","c9a2cc2e":"code","d67a1a74":"code","7ec57dbb":"code","12ee08e5":"code","2b0e417c":"code","7f9af443":"code","42196f7c":"code","882bd6b2":"code","cf87f398":"code","c64366c5":"code","890dcc29":"code","0d010edf":"code","4d4dedb5":"code","c096e9dc":"code","168a7bcc":"code","c0086157":"code","623297a7":"code","806ddab8":"code","bffa7308":"code","b176a28d":"code","d5b84811":"code","365b32c8":"code","232a98fe":"code","3578b2e6":"code","0bd8ba34":"code","abfe4bc9":"code","3fc4121b":"code","a3fba4c6":"code","12f3d8d8":"code","a92302a8":"code","f4d5784b":"code","577362a6":"code","d94d8c30":"code","f496e72a":"code","bd7b5302":"code","ba4e06b8":"code","39481b4e":"code","e2c7bc8f":"code","c8498bd6":"code","a201a2c8":"code","0dc1153a":"code","b55eaef2":"markdown","65c8e1eb":"markdown","0ce20e66":"markdown"},"source":{"6d949794":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5654ab99":"texto_input_classify = '\/kaggle\/input\/coronavirus-covid19-tweets\/'\ntexto_input_created = '\/kaggle\/input\/tweets-categories\/'","719643ad":"df_to_categorize1 = pd.read_csv(texto_input_classify+'2020-03-00 Coronavirus Tweets (pre 2020-03-12).CSV')","802543e9":"df_to_categorize1 = df_to_categorize1.loc[(df_to_categorize1['country_code'].isin(['US','IN','GB','ZA','CA','NG','AU','PH','CN','PK'])) & (df_to_categorize1['lang'] == 'en'),:]\nlen(df_to_categorize1)","57050771":"df_to_categorize1['tweet_id'] = df_to_categorize1.apply(lambda x: str(x['status_id'])+'_'+ str(x['user_id']), axis = 1)\ndf_to_categorize1 = df_to_categorize1.drop(['status_id', 'user_id', 'screen_name','source', 'reply_to_status_id', 'reply_to_user_id',\n       'reply_to_screen_name', 'is_quote', 'is_retweet',\n       'favourites_count', 'retweet_count','place_full_name', 'place_type', 'followers_count',\n       'friends_count', 'account_lang', 'account_created_at', 'verified',\n       'lang'], axis=1).reset_index(drop=True)\ndf_to_categorize1[:5]","21ddeeaa":"# df_to_categorize2 = pd.read_csv(texto_input_classify+'2020-03-19 Coronavirus Tweets.CSV')\n# df_to_categorize2 = df_to_categorize2.append(pd.read_csv(texto_input_classify+'2020-03-20 Coronavirus Tweets.CSV'), ignore_index= True)\n# df_to_categorize2 = df_to_categorize2.append(pd.read_csv(texto_input_classify+'2020-03-21 Coronavirus Tweets.CSV'), ignore_index= True)\n# df_to_categorize2 = df_to_categorize2.append(pd.read_csv(texto_input_classify+'2020-03-22 Coronavirus Tweets.CSV'), ignore_index= True)\n# df_to_categorize2 = df_to_categorize2.append(pd.read_csv(texto_input_classify+'2020-03-23 Coronavirus Tweets.CSV'), ignore_index= True)\n# len(df_to_categorize2)","ce9745e3":"# df_to_categorize2 = df_to_categorize2.loc[(df_to_categorize2['country_code'].isin(['US','IN','GB','ZA','CA','NG','AU','PH','CN','PK'])) & (df_to_categorize2['lang'] == 'en'),:]\n# len(df_to_categorize2)","53154535":"# df_to_categorize2['tweet_id'] = df_to_categorize2.apply(lambda x: str(x['status_id'])+'_'+ str(x['user_id']), axis = 1)\n# df_to_categorize2 = df_to_categorize2.drop(['status_id', 'user_id', 'screen_name','source', 'reply_to_status_id', 'reply_to_user_id',\n#        'reply_to_screen_name', 'is_quote', 'is_retweet',\n#        'favourites_count', 'retweet_count','place_full_name', 'place_type', 'followers_count',\n#        'friends_count', 'account_lang', 'account_created_at', 'verified',\n#        'lang'], axis=1).reset_index(drop=True)\n# df_to_categorize2[:5]","a1a2f667":"df_to_categorize3 = pd.read_csv(texto_input_classify+'2020-03-24 Coronavirus Tweets.CSV')\ndf_to_categorize3 = df_to_categorize3.append(pd.read_csv(texto_input_classify+'2020-03-25 Coronavirus Tweets.CSV'), ignore_index= True)\ndf_to_categorize3 = df_to_categorize3.append(pd.read_csv(texto_input_classify+'2020-03-26 Coronavirus Tweets.CSV'), ignore_index= True)\ndf_to_categorize3 = df_to_categorize3.append(pd.read_csv(texto_input_classify+'2020-03-27 Coronavirus Tweets.CSV'), ignore_index= True)\ndf_to_categorize3 = df_to_categorize3.append(pd.read_csv(texto_input_classify+'2020-03-28 Coronavirus Tweets.CSV'), ignore_index= True)\nlen(df_to_categorize3)","14b2b783":"df_to_categorize3 = df_to_categorize3.loc[(df_to_categorize3['country_code'].isin(['US','IN','GB','ZA','CA','NG','AU','PH','CN','PK'])) & (df_to_categorize3['lang'] == 'en'),:]\nlen(df_to_categorize3)","c9a2cc2e":"df_to_categorize3['tweet_id'] = df_to_categorize3.apply(lambda x: str(x['status_id'])+'_'+ str(x['user_id']), axis = 1)\ndf_to_categorize3 = df_to_categorize3.drop(['status_id', 'user_id', 'screen_name','source', 'reply_to_status_id', 'reply_to_user_id',\n       'reply_to_screen_name', 'is_quote', 'is_retweet',\n       'favourites_count', 'retweet_count','place_full_name', 'place_type', 'followers_count',\n       'friends_count', 'account_lang', 'account_created_at', 'verified',\n       'lang'], axis=1).reset_index(drop=True)\ndf_to_categorize3[:5]","d67a1a74":"df_categorized = pd.read_excel(texto_input_created+'Tweets.xlsx')\ndf_categorized[:5]","7ec57dbb":"df_us = df_to_categorize1.loc[df_to_categorize1['country_code']=='US',:]\nprint(len(df_us))\ndf_us = df_us.append(df_to_categorize3.loc[df_to_categorize3['country_code']=='US',:], ignore_index = True)\nprint(len(df_us))\ndf_us = df_us.sample(n=400)\ndf_us[:5]","12ee08e5":"import nltk\n#nltk.download('punkt')\nfrom nltk import wordpunct_tokenize\n#nltk.download('stopwords')\nfrom nltk.corpus import stopwords","2b0e417c":"stopwords_english = set(stopwords.words('english'))","7f9af443":"#!pip install -U spacy","42196f7c":"import spacy\nnlp = spacy.load(\"en_core_web_sm\")","882bd6b2":"def first_part_process(sentence):\n    sentence = sentence.lower()\n    \n    # stopword and remove puntuationcs\n    stopword_applied = [word for word in nltk.word_tokenize(sentence) if not word in stopwords_english and word.isalnum()] \n    stopword_applied = \" \".join(stopword_applied) # I change it back to full sentence\n\n    # lemma and pos\n    spacy_applied = nlp(stopword_applied)\n    spacy_applied = [word.lemma_ for word in spacy_applied if word.pos_ in ['ADJ','VERB']] # rember fix emojis\n\n    return \" \".join(spacy_applied)\n","cf87f398":"\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef countWords(tweets):\n    stop_words_included = frozenset(['14th','1st'])\n    cv = CountVectorizer(analyzer=\"word\",stop_words=stop_words_included)\n    word_count_vector=cv.fit_transform(tweets)\n    return word_count_vector, cv\n\ndef define_idfs(word_count_vector):\n    tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n    tfidf_transformer.fit(word_count_vector)\n    return tfidf_transformer\n\n","c64366c5":"df_categorized['text_processed'] = df_categorized.apply(lambda x: first_part_process(x['text']), axis=1)\nprint(len(df_categorized))\ndf_categorized[:5]","890dcc29":"df_us['text_processed'] = df_us.apply(lambda x: first_part_process(x['text']), axis=1)\nprint(len(df_us))\ndf_us[:5]","0d010edf":"# count frequency of terms\nword_count_vector, tf_v = countWords(df_categorized['text_processed'])\nword_count_vector.shape","4d4dedb5":"df_tf_v1 = pd.DataFrame(data=word_count_vector.toarray(),index=np.arange(1, 81),columns=tf_v.get_feature_names())\ndf_tf_v1['cat'] = df_categorized['cat']\nprint(len(df_tf_v1))\ndf_tf_v1[:1]","c096e9dc":"feature_names = tf_v.get_feature_names()\ndic_terms = {} \ndic_terms_pos = {} \ndic_terms_neg = {} \n\nfor f in feature_names:\n    dic_terms[f] = 0\n    dic_terms_pos[f] = 0\n    dic_terms_neg[f] = 0\n\ndef count_tf_terms(row):\n    for key in feature_names:\n        dic_terms[key] += row[key]\n    return 0\n\ndef count_tf_terms_pos(row):\n    if row['cat'] == 'positive ':\n        for key in feature_names:\n            dic_terms_pos[key] += row[key]\n    return 0\n\ndef count_tf_terms_neg(row):\n    if row['cat'] == 'negative':\n        for key in feature_names:\n            dic_terms_neg[key] += row[key]\n    return 0\n    \nb = df_tf_v1.apply(lambda x: count_tf_terms(x), axis=1)\nb = df_tf_v1.apply(lambda x: count_tf_terms_pos(x), axis=1)\nb = df_tf_v1.apply(lambda x: count_tf_terms_neg(x), axis=1)\ndf_terms_counts= pd.DataFrame(dic_terms.items(), columns=['term', 'tf'])\ndf_terms_counts['positive'] = df_terms_counts['term'].map(dic_terms_pos) \ndf_terms_counts['negative'] = df_terms_counts['term'].map(dic_terms_neg) \ndf_terms_counts[:10]","168a7bcc":"# top ten general\ndf_terms_counts.nlargest(10, 'tf')[:10]","c0086157":"# top ten negative\ndf_terms_counts.nlargest(10, 'negative')[:10]","623297a7":"# top ten negative\ndf_terms_counts.nlargest(10, 'positive')[:10]","806ddab8":"df_terms_counts.nlargest(10, 'positive').loc[:,['term','positive']][:10].positive","bffa7308":"df_terms_counts.positive.max()","b176a28d":"import matplotlib.pyplot as plt\n\ndata_upper = df_terms_counts.positive.max() + 5\nfig, ax = plt.subplots(figsize=(15,7))\nax.set_ylabel('Tf', fontsize=20)\n\ndf_terms_counts.nlargest(10, 'positive').loc[:,['term','positive']].positive.plot(kind = 'bar', \n                                           ylim = [0, data_upper], \n                                           rot = 0, fontsize = 16, ax=ax)\nax.set_xlabel('Terms', fontsize=20)\nax.set_title('Positive Term Frequency - 80 Tweets', fontsize=24)\nax.set_xticklabels(df_terms_counts.nlargest(10, 'positive').loc[:,['term','positive']].term)\nax.grid(True)","d5b84811":"tfidf_transformer = define_idfs(word_count_vector)","365b32c8":"# count matrix\ncount_vector = tf_v.transform(df_categorized['text_processed'])\n \n# tf-idf scores\ntf_idf_vector = tfidf_transformer.transform(count_vector)","232a98fe":"df_tf_idf_v1 = pd.DataFrame(data=tf_idf_vector.toarray(),index=np.arange(0, len(df_categorized)),columns=tf_v.get_feature_names())\nprint(tf_idf_vector.shape)\nprint(len(df_categorized))\ndf_tf_idf_v1['tweet_id'] = df_categorized['status_id']\ndf_tf_idf_v1['code'] = df_categorized['code']\ndf_tf_idf_v1['created_at'] = df_categorized['created_at']\ndf_tf_idf_v1['cat_original'] = df_categorized['cat']\ndf_tf_idf_v1[75:]","3578b2e6":"len(tf_v.get_feature_names())","0bd8ba34":"df_tf_idf_v1.cat_original.unique()","abfe4bc9":"df_tf_idf_v1['cat_ori_en'] = df_tf_idf_v1.apply(lambda x: 1 if str(x['cat_original']) == 'positive ' else 0, axis=1)\ndf_tf_idf_v1[40:50]","3fc4121b":"df_tf_idf_v1.cat_ori_en.unique()","a3fba4c6":"df_tf_idf_v1.iloc[:5,0:562]","12f3d8d8":"from sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import ensemble\n\nfrom sklearn import metrics\n\n# define 10-fold cross validation test harness\nseed = 7\nnp.random.seed(seed)\nkfold = StratifiedShuffleSplit(n_splits=10, test_size=0.35, random_state=7)","a92302a8":"history = {}\nhistory['acc'] = []\nhistory['precision_p'] = []\nhistory['recall_p'] = []\nhistory['precision_n'] = []\nhistory['recall_n'] = []\nhistory['true_values_x'] = []\nhistory['true_values_y'] = []\nhistory['pred_values_y'] = []\n\nmodels_list = []\n\ndef classifier_tree(train_x, valid_x, train_y, valid_y):\n    classifier_tree = DecisionTreeClassifier(random_state= 7)\n#     classifier_tree = ensemble.RandomForestClassifier(random_state= 1)\n    classifier_tree = classifier_tree.fit(train_x, train_y)\n\n    # predict the labels on validation dataset\n    valid_y_pred_tree = classifier_tree.predict(valid_x)\n    \n    accuracy_tree = metrics.accuracy_score(valid_y, valid_y_pred_tree)\n    \n    recall_tree_p = metrics.recall_score(valid_y, valid_y_pred_tree,pos_label=1,zero_division = 0 )\n    precision_tree_p = metrics.precision_score(valid_y, valid_y_pred_tree,pos_label=1,zero_division = 0)\n    \n    recall_tree_n = metrics.recall_score(valid_y, valid_y_pred_tree,pos_label=0, zero_division = 0)\n    precision_tree_n = metrics.precision_score(valid_y, valid_y_pred_tree,pos_label=0, zero_division = 0)\n    \n    history['acc'].append(accuracy_tree)\n    history['precision_p'].append(precision_tree_p)\n    history['recall_p'].append(recall_tree_p)\n    history['precision_n'].append(precision_tree_n)\n    history['recall_n'].append(recall_tree_n)\n    history['true_values_x'].append(valid_x)\n    history['true_values_y'].append(valid_y)\n    history['pred_values_y'].append(valid_y_pred_tree)\n    models_list.append(classifier_tree)","f4d5784b":"df_X = df_tf_idf_v1.iloc[:,0:294]\ndf_Y = df_tf_idf_v1['cat_ori_en']","577362a6":"for train, test in kfold.split(df_X, df_Y):\n    classifier_tree(df_X.iloc[train],df_X.iloc[test],df_Y[train],df_Y[test])\n\nprint(\"Accuracy kfold=10\", sum(history['acc'])\/len(history['acc']))","d94d8c30":"len(models_list)","f496e72a":"len(history['true_values_y'])","bd7b5302":"fig, ax = plt.subplots(figsize=(12, 12))\nfor i, mod in enumerate(models_list):\n    koko = metrics.plot_roc_curve(mod,history['true_values_x'][i],history['true_values_y'][i],name='ROC fold {}'.format(i+1), lw=3, ax=ax)\n\nplt.show()       ","ba4e06b8":"fig, ax = plt.subplots(figsize=(12, 12))\nfor i, mod in enumerate(models_list):\n    koko = metrics.plot_precision_recall_curve(mod,history['true_values_x'][i],history['true_values_y'][i],name='ROC fold {}'.format(i+1), lw=3, ax=ax)\n\nplt.show()  ","39481b4e":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_true=history['true_values_y'][9], y_pred=history['pred_values_y'][9]) ","e2c7bc8f":"import itertools\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix',\n                          cmap=sns.cubehelix_palette(as_cmap=True)):\n    \"\"\"\n    This function is modified from: \n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    classes.sort()\n    tick_marks = np.arange(len(classes))    \n    plt.figure(figsize=(4, 4),dpi=115)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","c8498bd6":"plot_confusion_matrix(cm, classes=[0,1], title='Confusion matrix')","a201a2c8":"count_vector = tf_v.transform(df_us['text_processed'])\ntf_idf_vector = tfidf_transformer.transform(count_vector)\ndf_us['cat_pre'] = models_list[7].predict(tf_idf_vector)\ndf_us.loc[:,['text','cat_pre']][:5]","0dc1153a":"df_us.to_csv('.\/df_us.csv', index=False)","b55eaef2":"# tf and tf-idf","65c8e1eb":"# other datasets","0ce20e66":"# Model"}}