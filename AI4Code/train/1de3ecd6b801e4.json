{"cell_type":{"f9292459":"code","43e4e132":"code","87831f59":"code","a86ba715":"code","94ff50c8":"code","620aca88":"code","a3580fb4":"code","7c98318e":"code","131fa3f1":"code","00df19c9":"code","53e0a626":"code","f490581b":"code","9987f154":"code","802f06ca":"code","987e69d4":"code","82e4ea2f":"code","0158b1a5":"code","df0cd88a":"code","00a30611":"code","8edfaafb":"code","e15da63d":"code","d7f4d013":"code","e060040c":"code","2f42e094":"code","56b350a0":"code","4a047378":"code","1279a874":"code","97b301f0":"code","85430c6c":"code","dbf4017b":"code","560ed003":"code","ab027e9b":"code","8c67d8bf":"code","09512553":"code","6bcac0ca":"code","58291463":"code","b70c79e2":"code","163f7fca":"code","de6a4c6d":"code","09544094":"code","0a6ae14e":"code","af059b13":"code","f135ae02":"code","a0bc86e8":"code","8de571fe":"code","909d5c46":"code","2e0b03d2":"code","ade80954":"code","2048d2f0":"code","579c42c7":"code","5d1c915d":"code","ce4d3a04":"code","8b1d707b":"code","8caba890":"code","7122b42a":"code","a3648f2c":"code","262288cf":"code","53b8cacc":"code","09fa6d4a":"code","5a5fd872":"code","188433b8":"code","89bfcd84":"code","2e8a0218":"code","b029037c":"code","ad0f97ed":"code","bba87bb4":"code","a9efd163":"code","f62a2d2c":"code","9dbf350d":"code","31d66c8a":"code","293affa8":"code","d41af0e4":"code","914df90a":"code","f8ae39d2":"code","85cdc99f":"code","d63cc726":"code","996b56fe":"code","85fa5f00":"code","acbe7a0c":"code","3ea96696":"code","a35f6aaa":"code","a3837fd8":"markdown","27c992c9":"markdown","007e7305":"markdown","de01b984":"markdown","9b091b9b":"markdown","5d035e40":"markdown","17e7be66":"markdown","60b0edd4":"markdown","fa8a3380":"markdown","39066d26":"markdown","1796dc1d":"markdown","50c97c85":"markdown","03b058b6":"markdown","5a120cab":"markdown","3c8e17ad":"markdown","feb0379f":"markdown","ba870150":"markdown","923a082f":"markdown","ba070f6f":"markdown","3e5f868b":"markdown","f6696c20":"markdown","92e5fcaa":"markdown","384e76d6":"markdown","04632dc5":"markdown","529b1b7b":"markdown","de43e221":"markdown","6212fe1b":"markdown","bee78b52":"markdown"},"source":{"f9292459":"%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom collections import Counter\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\n\nwarnings.filterwarnings(\"ignore\")","43e4e132":"df_titanic_train = pd.read_csv('..\/input\/train.csv')\ndf_titanic_test = pd.read_csv('..\/input\/test.csv')\nPassengerId = df_titanic_test[\"PassengerId\"]","87831f59":"# Outlier detection \n\ndef detect_outliers(df, n, features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp, Parch and Fare\noutliers_to_drop = detect_outliers(df_titanic_train, 2, [\"Age\", \"SibSp\", \"Parch\", \"Fare\"])\n\n# Outliers\n\ndf_titanic_train.loc[outliers_to_drop]","a86ba715":"# Drop outliers\n\ndf_titanic_train = df_titanic_train.drop(outliers_to_drop, axis = 0).reset_index(drop=True)","94ff50c8":"# Concatenate the two dataframes to minimize the bias and to have the same columns after feature engineering\n\ntrain_size = len(df_titanic_train)\ndf_titanic =  pd.concat(objs=[df_titanic_train, df_titanic_test], axis=0).reset_index(drop=True)\ndf_titanic.head()","620aca88":"# Filling empty values with NaN and checking the null values\n\ndf_titanic = df_titanic.fillna(np.nan)\n\n# Survived will not be considered because the empty values are from test dataset\n\ndf_titanic.isnull().sum()","a3580fb4":"def absolute_relative_freq(variable):\n    absolute_frequency = variable.value_counts()\n    relative_frequency = round(variable.value_counts(normalize = True)*100, 2) \n    df = pd.DataFrame({'Absolute Frequency':absolute_frequency, 'Relative Frequency(%)':relative_frequency})\n    print('Absolute and Relative Frequency of [',variable.name,']')\n    display(df)","7c98318e":"# Get some conclusion about the correlation among 'Survived' and SibSp, Parch, Age and Fare.\n\nfig, ax = plt.subplots(figsize=(12,8))\ng = sns.heatmap(\n    df_titanic[[\"Survived\", \"SibSp\", \"Age\", \"Parch\", \"Fare\"]].corr(),\n    annot=True, \n    fmt = \".3f\", \n    cmap = \"Greens\",\n    ax=ax)","131fa3f1":"# View the proportion between SibSp and Survived\n\ng = sns.factorplot(x=\"SibSp\", \n                   y=\"Survived\", \n                   data=df_titanic, \n                   kind=\"bar\", \n                   size=5, \n                   palette = \"Greens\")\n\ng = g.set_ylabels(\"Survived\")","00df19c9":"# View the distribution of Age\n\ng = sns.FacetGrid(df_titanic, \n                  col='Survived',\n                  aspect=2)\n\ng = g.map(sns.distplot, \"Age\", \n          bins=20, \n          color='g', \n          hist_kws=dict(edgecolor=\"w\", linewidth=1))","53e0a626":"# View the proportion between Parch and Survived\n\ng = sns.factorplot(x=\"Parch\", \n                   y=\"Survived\", \n                   data=df_titanic, \n                   kind=\"bar\", \n                   size=5, \n                   palette = \"Greens\")\n\ng = g.set_ylabels(\"Survived\")","f490581b":"# Filling with the median the only one Fare equals to NaN\n\ndf_titanic['Fare'] = df_titanic['Fare'].fillna(df_titanic['Fare'].median())","9987f154":"# Viewing the Fare distribution\n \nfig, ax = plt.subplots(figsize=(7,5))\ng = sns.distplot(df_titanic[\"Fare\"], \n                 color=\"g\", \n                 label=\"Skewness : %.3f\"%(df_titanic[\"Fare\"].skew()), \n                 hist_kws=dict(edgecolor=\"w\", linewidth=1),\n                 ax=ax)\n                 \ng = g.legend(loc=\"best\")","802f06ca":"df_titanic[\"Fare\"] = df_titanic[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","987e69d4":"# Viewing the Fare distribution after applying log function\n \nfig, ax = plt.subplots(figsize=(7,5))\ng = sns.distplot(df_titanic[\"Fare\"], \n                 color=\"g\", \n                 label=\"Skewness : %.3f\"%(df_titanic[\"Fare\"].skew()), \n                 hist_kws=dict(edgecolor=\"w\", linewidth=1),\n                 ax=ax)\n                 \ng = g.legend(loc=\"best\")","82e4ea2f":"# View the proportion between Sex and Survived\n\ng = sns.barplot(x=\"Sex\", y=\"Survived\",data=df_titanic, palette='cool')\ng = g.set_ylabel(\"Survived\")","0158b1a5":"absolute_relative_freq(df_titanic['Sex'])","df0cd88a":"# View the relationship between Sex and Age\n\ng = sns.factorplot(x=\"Sex\", \n                   y=\"Age\", \n                   data=df_titanic, \n                   kind=\"box\", \n                   size=5, \n                   palette = \"cool\")\n\ng = g.set_ylabels(\"Survived\")","00a30611":"# View the proportion between Pclass, Sex and Survived\n\ng = sns.factorplot(x=\"Pclass\", \n                   y=\"Survived\", \n                   data=df_titanic,\n                   hue='Sex',\n                   kind=\"bar\", \n                   size=5, \n                   palette = \"cool\")\n\ng = g.set_ylabels(\"Survived\")","8edfaafb":"absolute_relative_freq(df_titanic['Pclass'])","e15da63d":"df_titanic['Embarked'].value_counts()","d7f4d013":"# View the proportion between Embarked and Survived\n\n# Filling the NaN value with 'S' (more frequent city)\n\ndf_titanic['Embarked'].fillna('S', inplace=True)\n\ng = sns.barplot(x=\"Embarked\", y=\"Survived\",data=df_titanic, palette='Greens')\ng = g.set_ylabel(\"Survived\")","e060040c":"absolute_relative_freq(df_titanic['Embarked'])","2f42e094":"# View the proportion between Embarked, Sex and Survived\n\ng = sns.factorplot(x=\"Embarked\", \n                   y=\"Survived\", \n                   data=df_titanic,\n                   hue='Sex',\n                   kind=\"bar\", \n                   size=5, \n                   palette = \"cool\")\n\ng = g.set_ylabels(\"Survived\")","56b350a0":"# View the proportion between Embarked, Age less than 10 and Survived\n\ng = sns.factorplot(x=\"Embarked\", \n                   y=\"Survived\", \n                   data=df_titanic[df_titanic['Age'] < 10] ,\n                   kind=\"bar\", \n                   size=5, \n                   palette = \"Greens\")\n\ng = g.set_ylabels(\"Survived\")","4a047378":"# View the proportion between Pclass and Embarked\n\ng = sns.factorplot(\"Pclass\",\n                   col=\"Embarked\",\n                   data=df_titanic,\n                   size=5, \n                   kind=\"count\", \n                   palette=\"Greens\")\n\ng = g.set_ylabels(\"Count\")","1279a874":"labelEncoder = LabelEncoder()\ndf_titanic['Embarked'] = labelEncoder.fit_transform(df_titanic['Embarked'])","97b301f0":"df_titanic['Age'].isnull().sum()","85430c6c":"labelEncoder = LabelEncoder()\ndf_titanic['Sex'] = labelEncoder.fit_transform(df_titanic['Sex'])","dbf4017b":"fig, ax = plt.subplots(figsize=(12,8))\ng = sns.heatmap(\n    df_titanic[[\"Age\", \"SibSp\", \"Parch\", \"Pclass\", \"Sex\"]].corr(),\n    annot=True, \n    fmt = \".3f\", \n    cmap = \"Greens\",\n    ax=ax)","560ed003":"df_titanic['Age'].isnull().sum()","ab027e9b":"# Rows with Age equals to NaN\n\ncondition = df_titanic['Age'].isnull()\nage_NaN = df_titanic['Age'][condition].index\n\nfor age in age_NaN :\n    \n    # Conditions\n    \n    condition1 = df_titanic['SibSp'] == df_titanic.iloc[age][\"SibSp\"]\n    condition2 = df_titanic['Pclass'] == df_titanic.iloc[age][\"Pclass\"]\n    condition3 = df_titanic['Parch'] == df_titanic.iloc[age][\"Parch\"]\n    condition = condition1 & condition2 & condition3\n    \n    new_age = df_titanic['Age'][condition].median()\n    df_titanic['Age'].iloc[age] = new_age if not np.isnan(new_age) else df_titanic['Age'].median()","8c67d8bf":"df_titanic['Age'].isnull().sum()","09512553":"df_titanic['Age'] = (df_titanic['Age'] - df_titanic['Age'].mean()) \/ df_titanic['Age'].std()","6bcac0ca":"# Viewing the Age distribution\n \nfig, ax = plt.subplots(figsize=(7,5))\ng = sns.distplot(df_titanic[\"Age\"], \n                 color=\"g\", \n                 label=\"Skewness : %.3f\"%(df_titanic[\"Age\"].skew()), \n                 hist_kws=dict(edgecolor=\"w\", linewidth=1),\n                 ax=ax)\n                 \ng = g.legend(loc=\"best\")","58291463":"df_titanic['Name'].head()","b70c79e2":"# Getting the titles from Name feature\n\ndf_titanic['Title'] = df_titanic['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf_titanic['Title'].unique()","163f7fca":"# Replace the values to new categories and converting the new feature to numeric\n\ndf_titanic['Title'] = df_titanic['Title'].replace(['Don', 'Rev', 'Dr', 'Major', 'Lady', 'Sir', \n                                                   'Col', 'Capt', 'Countess', 'Jonkheer', 'Dona'], 'Rare')","de6a4c6d":"# View the proportion between Title and Survived\n\ng = sns.factorplot(x=\"Title\", \n                   y=\"Survived\", \n                   data=df_titanic,\n                   kind=\"bar\", \n                   size=5)\n\ng = g.set_ylabels(\"Survived\")","09544094":"absolute_relative_freq(df_titanic['Title'])","0a6ae14e":"df_titanic[\"Title\"] = df_titanic['Title'].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1, \"Mme\":1, \n                                               \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\n\ndf_titanic['Title'] = df_titanic[\"Title\"].astype(int)","af059b13":"# Creating a column with the Surname\n\ndf_titanic['Surname'] = df_titanic['Name'].map(lambda i: i.split(',')[0])","f135ae02":"# Deleting Name\n\ndel df_titanic['Name']","a0bc86e8":"df_titanic['Cabin'].isnull().sum()","8de571fe":"df_titanic['Cabin'].unique()","909d5c46":"df_titanic['Cabin'] = df_titanic['Cabin'].map(lambda i: i[0] if not pd.isnull(i) else 'Z')\ndf_titanic['Cabin'].unique()","2e0b03d2":"# View the proportion between Cabin and Survived\n\ng = sns.factorplot(x=\"Cabin\", \n                   y=\"Survived\", \n                   data=df_titanic,\n                   kind=\"bar\", \n                   size=5, \n                   order=['A','B','C','D','E','F','G','T','Z'])\n\ng = g.set_ylabels(\"Survived\")","ade80954":"absolute_relative_freq(df_titanic['Cabin'])","2048d2f0":"df_titanic['Ticket'].unique()","579c42c7":"# Getting the first information of the ticket\n\ndf_titanic['Ticket'] = df_titanic['Ticket'].map(\n    lambda i: i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0] if not i.isdigit() else \"TKT\")","5d1c915d":"df_titanic['Ticket'].unique()","ce4d3a04":"df_titanic['Family'] = df_titanic['SibSp'] + df_titanic['Parch'] + 1","8b1d707b":"df_titanic['Family'].unique()","8caba890":"# Creating new features: \n#   1: Alone\n#   2: Small family\n#   3 to 4: Medium family\n#   larger than 5: Large family\n\ndf_titanic['Alone'] = df_titanic['Family'].map(lambda i: 1 if i == 1 else 0)\ndf_titanic['Small'] = df_titanic['Family'].map(lambda i: 1 if i == 2 else 0)\ndf_titanic['Medium'] = df_titanic['Family'].map(lambda i: 1 if 3 <= i <= 4 else 0)\ndf_titanic['Large'] = df_titanic['Family'].map(lambda i: 1 if i >= 5 else 0)","7122b42a":"df_titanic.head()","a3648f2c":"# df_titanic['Title'] = df_titanic['Title'].astype(\"category\")\ndf_titanic['Pclass'] = df_titanic['Pclass'].astype(\"category\")\n\n# Creating dummies...\n\ncolumns = ['Title', 'Surname', 'Cabin', 'Ticket', 'Pclass']\nfor col in columns:\n    df_titanic = pd.get_dummies(df_titanic, columns=[col], prefix=col)","262288cf":"df_titanic.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","53b8cacc":"df_titanic.head()","09fa6d4a":"df_titanic_train = df_titanic[:train_size]\ndf_titanic_test = df_titanic[train_size:]\n\ndf_titanic_train['Survived'] = df_titanic_train['Survived'].astype(int)\ndel df_titanic_test['Survived']","5a5fd872":"X_train = df_titanic_train.drop(['Survived'], axis=1)\ny_train = df_titanic_train['Survived']\nX_test = df_titanic_test\n\nsc = StandardScaler()\nsc.fit(X_train)\nX_train = sc.transform(X_train)\nX_test = sc.transform(X_test)","188433b8":"print(X_train.shape)\nprint(X_test.shape)","89bfcd84":"# Validation of the model with Kfold stratified splitting the data into 10 parts\n\nkfold = StratifiedKFold(n_splits=10)\n\nseed = 20\n\n# List of classifiers to test\n\nclfs = []\nclfs.append(SVC(random_state=seed))\nclfs.append(DecisionTreeClassifier(random_state=seed))\nclfs.append(RandomForestClassifier(random_state=seed))\nclfs.append(ExtraTreesClassifier(random_state=seed))\nclfs.append(GradientBoostingClassifier(random_state=seed))\nclfs.append(MLPClassifier(random_state=seed))\nclfs.append(KNeighborsClassifier())\nclfs.append(LogisticRegression(random_state=seed))\nclfs.append(XGBClassifier(random_state = seed))","2e8a0218":"# Getting all results from 10 validations for each classifier\n\nclf_results = []\nfor clf in clfs :\n    clf_results.append(cross_val_score(clf, X_train, y=y_train, scoring = \"accuracy\", cv=kfold, n_jobs=1))","b029037c":"# Getting the mean and standard deviation from each classifier's result after 10 validations\n\nclf_means = []\nclf_std = []\nfor clf_result in clf_results:\n    clf_means.append(clf_result.mean())\n    clf_std.append(clf_result.std())","ad0f97ed":"# Let's see which are the best scores\n\ndf_result = pd.DataFrame({\"Means\":clf_means, \n                          \"Stds\": clf_std, \n                          \"Algorithm\":[\"SVC\", \n                                       \"DecisionTree\", \n                                       \"RandomForest\",\n                                       \"ExtraTrees\",\n                                       \"GradientBoosting\",\n                                       \"MLPClassifier\",\n                                       \"KNeighboors\",\n                                       \"LogisticRegression\", \n                                       \"XGBoost\"]})\n\ndf_result.sort_values(by=['Means'], ascending=False)","bba87bb4":"# Plotting learning curves of the algorithms\n#------------------------------------------------------------------------------------------------\n# Code from http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html\n#------------------------------------------------------------------------------------------------\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 20)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train\/test splits.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    \"\"\"\n    \n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","a9efd163":"# Logistic Regression, XGBoost, Gradient Boosting, Random Forest, Extra Trees\n\nextraTrees = ExtraTreesClassifier(random_state=seed)\ngBoosting = GradientBoostingClassifier(random_state=seed)\nrandomForest = RandomForestClassifier(random_state=seed)\nlogReg = LogisticRegression(random_state=seed)\nxgbc = XGBClassifier(random_state=seed)","f62a2d2c":"clfs = []\nclfs.append(extraTrees)\nclfs.append(gBoosting)\nclfs.append(randomForest)\nclfs.append(logReg)\nclfs.append(xgbc)\n\ntitles = ['Learning Curves (Extra Tree)', 'Learning Curves (Gradient Boosting)',\n          'Learning Curves (Random Forest)', 'Learning Curves (Logistic Regression)',\n          'Learning Curves (XGBoost)']\n\nfor clf, title in zip(clfs, titles):\n    plot_learning_curve(clf, title, X_train, y_train, ylim=(0.7, 1.01), cv=kfold, n_jobs=1);","9dbf350d":"## Search grid for optimal parameters (Extra Trees)\n\nparam_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngrid_result = GridSearchCV(extraTrees,\n                      param_grid = param_grid, \n                      cv=kfold, \n                      scoring=\"accuracy\", \n                      n_jobs= -1, \n                      verbose = 1)\n\ngrid_result.fit(X_train,y_train)\n\nextraTrees_best_result = grid_result.best_estimator_\n\n# Best score\nprint('Best score:', np.round(grid_result.best_score_*100, 2))\n\n# Best estimator\nprint('Best estimator:', extraTrees_best_result)","31d66c8a":"## Search grid for optimal parameters (Gradient Boosting)\n\nparam_grid = {'learning_rate': [0.01, 0.02],\n              'max_depth': [4, 5, 6],\n              'max_features': [0.2, 0.3, 0.4], \n              'min_samples_split': [2, 3, 4],\n              'random_state':[seed]}\n\ngrid_result = GridSearchCV(gBoosting, \n                           param_grid=param_grid, \n                           cv=kfold, \n                           scoring=\"accuracy\", \n                           n_jobs=-1,\n                           verbose=1)\n\ngrid_result.fit(X_train, y_train)\n\ngBoosting_best_result = grid_result.best_estimator_\n\n# Best score\nprint('Best score:', np.round(grid_result.best_score_*100, 2))\n\n# Best estimator\nprint('Best estimator:', gBoosting_best_result)","293affa8":"## Search grid for optimal parameters (Random Forest)\n\nparam_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 2],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\ngrid_result = GridSearchCV(randomForest, \n                           param_grid=param_grid, \n                           cv=kfold, \n                           scoring=\"accuracy\", \n                           n_jobs= -1,\n                           verbose = 1)\n\ngrid_result.fit(X_train, y_train)\n\nrandomForest_best_result = grid_result.best_estimator_\n\n# Best score\nprint('Best score:', np.round(grid_result.best_score_*100, 2))\n\n# Best estimator\nprint('Best estimator:', randomForest_best_result)","d41af0e4":"## Search grid for optimal parameters (Logistic Regression)\n\nparam_grid = {'penalty' : ['l1', 'l2'],\n              'C': np.logspace(0, 4, 10),\n              'solver' : ['liblinear', 'saga']\n              }\n\ngrid_result = GridSearchCV(logReg, \n                           param_grid=param_grid, \n                           cv=kfold, \n                           scoring=\"accuracy\", \n                           n_jobs= -1,\n                           verbose = 1)\n\ngrid_result.fit(X_train, y_train)\n\nlogReg_best_result = grid_result.best_estimator_\n\n# Best score\nprint('Best score:', np.round(grid_result.best_score_*100, 2))\n\n# Best estimator\nprint('Best estimator:', logReg_best_result)","914df90a":"## Search grid for optimal parameters (XGBoost)\n\nparam_grid = {'n_estimators': [275, 280],\n              'learning_rate': [0.01, 0.03],\n              'subsample': [0.9, 1],\n              'max_depth': [3, 4],\n              'colsample_bytree': [0.8, 0.9],\n              'min_child_weight': [2, 3],\n              'random_state':[seed]}\n\ngrid_result = GridSearchCV(xgbc, \n                           param_grid=param_grid, \n                           cv=kfold, \n                           scoring=\"accuracy\", \n                           n_jobs= -1,\n                           verbose = 1)\n\ngrid_result.fit(X_train, y_train)\n\nxgbc_best_result = grid_result.best_estimator_\n\n# Best score\nprint('Best score:', np.round(grid_result.best_score_*100, 2))\n\n# Best estimator\nprint('Best estimator:', xgbc_best_result)","f8ae39d2":"# List of classifiers to retrain\n\nclfs = []\nclfs.append(extraTrees_best_result)\nclfs.append(gBoosting_best_result)\nclfs.append(randomForest_best_result)\nclfs.append(logReg_best_result)\nclfs.append(xgbc_best_result)\n\n# Getting all results from 10 validations for each classifier\n\nclf_results = []\nfor clf in clfs :\n    clf_results.append(cross_val_score(clf, X_train, y=y_train, scoring = \"accuracy\", cv=kfold, n_jobs=1))\n\n# Getting the mean and standard deviation from each classifier's result after 10 validations\n\nclf_means = []\nclf_std = []\nfor clf_result in clf_results:\n    clf_means.append(clf_result.mean())\n    clf_std.append(clf_result.std())\n\n# Let's see which are the best scores\n\ndf_result = pd.DataFrame({\"Means\":clf_means, \n                          \"Stds\": clf_std, \n                          \"Algorithm\":[\"Extra Trees\",\n                                       \"GradientBoosting\",\n                                       \"Random Forest\",\n                                       \"LogisticRegression\", \n                                       \"XGBoost\"]})\n\ndf_result.sort_values(by=['Means'], ascending=False)","85cdc99f":"titles = ['Learning Curves (Extra Tree)', 'Learning Curves (Gradient Boosting)',\n          'Learning Curves (Random Forest)', 'Learning Curves (Logistic Regression)',\n          'Learning Curves (XGBoost)']\n\nfor clf, title in zip(clfs, titles):\n    plot_learning_curve(clf, title, X_train, y_train, ylim=(0.7, 1.01), cv=kfold, n_jobs=1);","d63cc726":"survived_ET = pd.Series(extraTrees_best_result.predict(X_test), name=\"ET\")\nsurvived_GB = pd.Series(gBoosting_best_result.predict(X_test), name=\"GB\")\nsurvived_RF = pd.Series(randomForest_best_result.predict(X_test), name=\"RF\")\nsurvived_LR = pd.Series(logReg_best_result.predict(X_test), name=\"LR\")\nsurvived_XB = pd.Series(xgbc_best_result.predict(X_test), name=\"XB\")\n\n# Concatenate all classifiers results\nensemble_results = pd.concat([survived_ET,\n                              survived_GB,\n                              survived_RF,\n                              survived_LR,\n                              survived_XB],\n                             axis=1)\n\nfig, ax = plt.subplots(figsize=(12,8))\ng= sns.heatmap(ensemble_results.corr(),\n               annot=True, \n               fmt = \".3f\", \n               cmap = \"Greens\",\n               ax=ax)","996b56fe":"# Using voting soft (XB, GB, RF, LR, and ET)\n\nvoting = VotingClassifier(estimators=[('XB', xgbc_best_result), \n                                      ('GB', gBoosting_best_result),\n                                      ('RF', randomForest_best_result),\n                                      ('LR', logReg_best_result),\n                                      ('ET', extraTrees_best_result)],\n                           voting='soft', n_jobs=-1)\nvoting.fit(X_train, y_train)","85fa5f00":"print(\"Score (Voting): \" + str(voting.score(X_train, y_train)))","acbe7a0c":"# Predicting survivors\n\ny_predict = voting.predict(X_test)","3ea96696":"solution = pd.DataFrame({\n                        \"PassengerId\": PassengerId,\n                        \"Survived\": y_predict.astype(int)\n                        })\n\nsolution.to_csv('solution_final_v1.csv', index=False)\ndf_solution = pd.read_csv('solution_final_v1.csv')","a35f6aaa":"absolute_relative_freq(df_solution['Survived'])","a3837fd8":"<font color=\"blue\" size=3><b>According to the first plot, women had more chances than men to survive. There is a very irrelevant difference between the median age of men and women (approximately the same value).<\/b><\/font>","27c992c9":"<font color=\"blue\" size=3><b>As we saw previously, younger passengers have more chances to survive. Those kids that embarked in Chesbourg (mainly) and Southampton had good chances to survive. Chesbourg had more women and kids, so we can explain many survivors from there. Let's see the proportion with Pclass.<\/b><\/font>","007e7305":"## Creating and testing the models","de01b984":"## Loading data and checking the values","9b091b9b":"## Analysing the features (columns) and filling up the NaN values","5d035e40":"## Creating and adjusting features","17e7be66":"<font color=\"blue\" size=3><b>Passengers with 1 or 2 parents\/children had more chances to survive. Passengers with 3 parents\/children had good chances to survive in this dataset, but we can see a large variance.<\/b><\/font>","60b0edd4":"<font color=\"blue\" size=3><b>We still have some rows with NaN Age.Let's try to figure out one correlation among Age and other columns.<\/b><\/font>","fa8a3380":"<font color=\"blue\" size=3><b>All cabins start with a letter, so let's simplyfing their values.<\/b><\/font>","39066d26":"<font color=\"blue\" size=3><b>Cabin<\/b><\/font>","1796dc1d":"<font color=\"blue\" size=3><b>Passengers with no cabin had less chances to survive.<\/b><\/font>","50c97c85":"<font color=\"blue\" size=3><b>Checking the outliers<\/b><\/font>","03b058b6":"<font color=\"blue\" size=3><b>Passengers with many siblings\/spouses have less chances to survive (more than 2). Passengers that are alone or with 1 or 2 siblings\/spouses have more chances to survive.<\/b><\/font>","5a120cab":"<font color=\"blue\" size=3><b>Passengers in the first class had more chances to survive, following by second class and third one. Again, women had more chances to survive in the 3 classes.<\/b><\/font>","3c8e17ad":"<font color=\"blue\" size=3><b>Tickets<\/b><\/font>","feb0379f":"<font color=\"blue\" size=3><b>Family (SibSp, Parch)<\/b><\/font>","ba870150":"<font color=\"blue\" size=3><b>Fare has a low correlation with Survived, but if we compare to the others features, this is more relevant.<\/b><\/font>","923a082f":"<font color=\"blue\" size=3><b>Again we can see that women had more chances to survive. Rare titles had more chances than Mister and less than Master.<\/b><\/font>","ba070f6f":"<font color=\"blue\" size=3><b>Fare distribution is very skewed to the right. Let's use the log function to minimize this skewness.<\/b><\/font>","3e5f868b":"<font color=\"blue\" size=3><b>As we got in the boxplot some cells behind, Age has a very small correlation with Sex (median age is practically the same). Pclass, SibSp and Parch have better correlations with Sex respectively. The rows with Age equals to NaN will be filled by the median value of similar rows (the same Pclass and SibSp) - two more correlated with Age.<\/b><\/font>","f6696c20":"## Retrain models and check precision, recall, specificity and F1 Score","92e5fcaa":"<font color=\"blue\" size=3><b>Titles and Surnames<\/b><\/font>","384e76d6":"<font color=\"blue\" size=3><b>Chesbourg had more passengers in first class compared to second and third one. Good explanation to have more survivors. And Southampton had a large number of passengers in third class explaining less survivors if we compared to the other cities.<\/b><\/font>","04632dc5":"<font color=\"blue\" size=3><b>Getting the best parameters for classifiers.<\/b><\/font>","529b1b7b":"<font color=\"blue\" size=3><b>Passengers that embarked in Cherbourg had more chances to survive than the other.<\/b><\/font>","de43e221":"<font color=\"blue\" size=3><b>Creating dummies features<\/b><\/font>","6212fe1b":"<font color=\"blue\" size=3><b>According to previous plots, Decision Tree, Random Forest, and Extra Trees algothims overfitted the training data during validation. Logistic Regression and Gradient Boosting showed better generalization because training and test curves are close together.<\/b><\/font>","bee78b52":"<font color=\"blue\" size=3><b>Younger passengers had more chance to survive and older ones had less chances to get saved. Passengers that are between 20 and 40 sometimes had chances to survive or not.<\/b><\/font>"}}