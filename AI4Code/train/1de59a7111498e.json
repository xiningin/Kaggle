{"cell_type":{"11082f6b":"code","ee6fb8a9":"code","cf32f760":"code","1dced596":"code","83fb04cd":"code","1515c6c7":"code","951fa28b":"code","b2ac5ec8":"code","91eecbea":"code","bbee7868":"code","a424eb03":"code","f5f38416":"code","1abe03e1":"code","770c076b":"code","8ce9c9ba":"code","dcfee851":"code","c5e9f206":"code","2e402145":"code","c3407f2e":"code","827f4db3":"code","a6eab8b4":"code","33a0dea8":"code","66ebf099":"code","8df9179a":"code","14120dc8":"code","2f5b3c6a":"markdown","ebd38f6f":"markdown","2ef13a7b":"markdown","89ee8ca8":"markdown","018a9298":"markdown","2f568482":"markdown"},"source":{"11082f6b":"import string\nimport re\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.metrics import Recall, Precision\nfrom keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom keras.optimizers import Adam\n\nfrom sklearn.model_selection import train_test_split","ee6fb8a9":"class Config:\n    seed = 44\n    n_epochs = 100\n    embedding_dim = 256\n    maxlen = 220\n        \n    validation_rate = 0.2    \n    dropout_rate = 0.1\n    \n    optimizer = Adam(lr=1e-3)\n    loss = 'binary_crossentropy'\n    metrics=['accuracy',  Precision(), Recall()]\n    \n    reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n                                  mode='auto',\n                                  factor=0.8,\n                                  patience=2,\n                                  epsilon=1e-4,\n                                  coldown=5,\n                                  min_lr=1e-5)\n    \n    checkpoint_best = ModelCheckpoint('best_model.h5',\n                                      monitor='val_loss',\n                                      mode='min',\n                                      verbose=1,\n                                      save_best_only=True,\n                                      save_weights_only=False)\n    \n    checkpoint_last = ModelCheckpoint('last_model.h5',\n                                      monitor='val_loss',\n                                      mode='min',\n                                      verbose=1,\n                                      save_best_only=False,\n                                      save_weights_only=False)\n    \n    early_stop = EarlyStopping(monitor='val_loss', mode='min', patience=15)\n    \n    callbacks = [reduce_lr, checkpoint_best, checkpoint_last, early_stop]\n    \n    paths = {'train': '..\/input\/nlp-getting-started\/train.csv',\n             'test': '..\/input\/nlp-getting-started\/test.csv'}\n    ","cf32f760":"config = Config()","1dced596":"train_df = pd.read_csv(config.paths['train'])\ntest_df = pd.read_csv(config.paths['test'])","83fb04cd":"train_df","1515c6c7":"test_df","951fa28b":"x_train_raw = train_df.text\ny_train = train_df.target","b2ac5ec8":"x_train_raw","91eecbea":"y_train","bbee7868":"y_train.value_counts()","a424eb03":"def clean_text(text):\n    stop_words = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once',\n               'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', \n                'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', \n                'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each',\n                'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', \n                'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', \n                'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'no', \n                'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', \n                'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', \n                'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', \n                'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', \n                'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further',\n                'was', 'here', 'than'}\n    \n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+|http:?\/\/\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    #we need to get rid of some special characters\n    text = re.sub(r\"\\x89\u00db_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d3\", \"\", text)\n    \n    sentence_list = text.split()\n    new_sentence = []\n\n\n    for word in sentence_list:\n        for stop_word in stop_words:\n            if (stop_word == word):\n                word = re.sub(stop_word, '', word)\n        new_sentence.append(word) \n    return (\" \".join(new_sentence))\n","f5f38416":"x_train_raw = x_train_raw.apply(lambda x: clean_text(x))\ntest_df.text = test_df.text.apply(lambda x: clean_text(x))\nx_train_raw","1abe03e1":"test_df.text","770c076b":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(x_train_raw)","8ce9c9ba":"x_train = tokenizer.texts_to_sequences(x_train_raw)\nx_test = tokenizer.texts_to_sequences(test_df.text)\nprint(x_train[0])\nprint(x_train_raw[0])\n","dcfee851":"x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=config.maxlen)\nx_train","c5e9f206":"x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=config.maxlen)\nx_test","2e402145":"import keras.backend as K\n\nfrom keras.models import Sequential\nfrom keras.layers import Layer\nfrom keras.layers import Embedding, Bidirectional, Dense, LSTM, merge\nfrom keras import initializers, regularizers, constraints, optimizers, layers","c3407f2e":"class Attention(Layer):\n    def __init__(self, step_dim,\n                 W_regularizer=None, b_regularizer=None,\n                 W_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        self.step_dim = step_dim\n        self.features_dim = 0\n        super(Attention, self).__init__(**kwargs)\n\n    def build(self, input_shape):\n        self.W = self.add_weight(shape=(input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        self.features_dim = input_shape[-1]\n\n        if self.bias:\n            self.b = self.add_weight(shape=(input_shape[1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n        else:\n            self.b = None\n\n        self.built = True\n\n    def compute_mask(self, input, input_mask=None):\n        return None\n\n    def call(self, x, mask=None):\n        features_dim = self.features_dim\n        step_dim = self.step_dim\n\n        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n\n        if self.bias:\n            eij += self.b\n\n        eij = K.tanh(eij)\n\n        a = K.exp(eij)\n\n        if mask is not None:\n            a *= K.cast(mask, K.floatx())\n\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0],  self.features_dim\n    \n    def get_config(self):\n        return super(Attention,self).get_config()","827f4db3":"model = Sequential([\n    Embedding(len(tokenizer.word_index) + 1, config.embedding_dim, input_length=config.maxlen),\n    Bidirectional(LSTM(64, return_sequences=True, dropout=config.dropout_rate)),\n    #Attention(config.maxlen),\n    Dense(64, activation=\"relu\"),\n    Dense(1, activation=\"sigmoid\")])\nmodel.summary()","a6eab8b4":"model.compile(loss=config.loss, optimizer=config.optimizer, metrics=config.metrics)\nmodel.summary()","33a0dea8":"model.fit(x_train, y_train, epochs=config.n_epochs, verbose=1, validation_split=config.validation_rate, shuffle=True, callbacks=config.callbacks)","66ebf099":"model.load_weights('.\/best_model.h5')","8df9179a":"result_dataframe = pd.DataFrame(columns=['id', 'target'])\nresult_dataframe['id'] = test_df['id']\nresult_dataframe['target'] = np.where(np.array(model.predict(x_test, verbose=1)) >= 0.5, 1, 0 )\nresult_dataframe.to_csv('submission.csv', index= False)","14120dc8":"result_dataframe","2f5b3c6a":"## Model create","ebd38f6f":"## Clean text","2ef13a7b":"## Import modules","89ee8ca8":"## Check and load data","018a9298":"## Tokenize text","2f568482":"## Configuration"}}