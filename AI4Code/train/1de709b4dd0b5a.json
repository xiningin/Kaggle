{"cell_type":{"59ec060f":"code","c87d39f9":"code","2976537f":"code","e8d2eea5":"code","e841d18e":"code","e1ab5758":"code","1b015c73":"code","130bf2eb":"code","6f919192":"code","d7c40c04":"code","7643ad29":"code","f327e475":"code","57098313":"code","f8166d62":"markdown","7596283e":"markdown","76d3172f":"markdown","e97009d6":"markdown","4c28aa38":"markdown","008376fd":"markdown","79c66835":"markdown","eb7f97c9":"markdown"},"source":{"59ec060f":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport nltk\nimport gensim\n\nfrom nltk.corpus import stopwords\nfrom nltk import download\ndownload('stopwords')\nstop_words = stopwords.words('english')\n\nfrom gensim.models import KeyedVectors\n\nfrom tqdm import tqdm_notebook","c87d39f9":"files = list(filter(lambda x : x.endswith('.txt'), os.listdir('..\/input\/donald-trumps-rallies')))\nfiles","2976537f":"speaches = [open(\"..\/input\/donald-trumps-rallies\/\" + file, \"r\", encoding=\"UTF-8\").read() for file in files]\nspeaches[0][:1000]","e8d2eea5":"documents = [gensim.utils.simple_preprocess(speach) for speach in speaches]\ndocuments[0][:10]","e841d18e":"len(documents)","e1ab5758":"word2vec = gensim.models.Word2Vec(\n    documents,\n    vector_size=300,\n    window=2,\n    min_count=1,\n    workers=1,\n    epochs=10)","1b015c73":"# testing\n\nword = 'president'\nword2vec.wv.most_similar(word, topn=7)","130bf2eb":"!wget -P \/root\/input\/ -c \"https:\/\/s3.amazonaws.com\/dl4j-distribution\/GoogleNews-vectors-negative300.bin.gz\"\nEMBEDDING_FILE = \"\/root\/input\/GoogleNews-vectors-negative300.bin.gz\"\npre_word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)","6f919192":"def cos(x1, x2):\n  return np.dot(x1, x2) \/ (np.linalg.norm(x1) * np.linalg.norm(x2))","d7c40c04":"compare_words = [('america', 'us'),\n                 ('america', 'china'),\n                 ('us', 'them'),\n                 ('man', 'woman'),\n                 ('car', 'bike'),\n                 ('donald', 'trump')\n                ]","7643ad29":"for word_pair in compare_words:\n    w0, w1 = word_pair\n    print(f'Comparing \"{w0}\" with \"{w1}\":')\n    print(f'\\tCosine for precomputed is {cos(pre_word2vec[w0], pre_word2vec[w1]):.4f}')\n    print(f'\\tCosine for my model is {cos(word2vec.wv[w0], word2vec.wv[w1]):.4f}')\n    print()","f327e475":"# list to get the top 10 closest words\nwords_to_compare = ['president', 'us', 'america', 'news', 'police', 'me']","57098313":"for word in words_to_compare:\n    print(f'Getting the top 10 most similar words to \"{word}\" in my model:')\n    [print(f'\\t{i+1} - \"{x[0]}\" with similarity {x[1]:.4f}.') for i, x in enumerate(word2vec.wv.most_similar(word, topn=10))]\n    print(f'Getting the top 10 most similar words to \"{word}\" in pre-treined model:')\n    [print(f'\\t{i+1} - \"{x[0]}\" with similarity {x[1]:.4f}.') for i, x in enumerate(pre_word2vec.most_similar(word, topn=10))]\n    print()\n    ","f8166d62":"_Esse notebook foi originalmente minha **Atividade 2 da mat\u00e9ria de Minera\u00e7\u00e3o de Dados N\u00e3o Estruturados** - SCC0287 - no ano de 2021, segundo semestre com o Professor Ricardo Marcondes Marcacini._\n\n_O objetivo dela \u00e9 estudar como funcionam word embeddings e como treinar seu pr\u00f3prio modelo para se adequar a um corpus textual de um determinado problema. Al\u00e9m disso, t\u00ednhamos que comparar similaridades entre palavras utilizando nossa embedding e uma embedding pr\u00e9-treinada._","7596283e":"# 2. Treinando a Word2Vec","76d3172f":"# 0. Imports","e97009d6":"Vemos que termos espec\u00edficos da pol\u00edtica possuem mais similaridade no nosso modelo do que no modelo pr\u00e9-treinado.  Enquanto que palavras gen\u00e9ricas possuem mais similaridade no modelo pr\u00e9-treinado. Em particular, as palavras `donald` e `trump` possuem muita similaridade no nosso modelo e praticamente nenhuma no pr\u00e9-treinado.","4c28aa38":"Dados tirados de:\n\nhttps:\/\/www.kaggle.com\/christianlillelund\/donald-trumps-rallies","008376fd":"# 3. Comparando com um modelo pr\u00e9-treinado","79c66835":"Este \u00e9 um dataset com v\u00e1rios discustos do ex-presidente dos Estados Unidos, Donald Trump.\n\nA ideia aqui \u00e9 que provavelmente os discursos dele possuem um padr\u00e3o de contexto e, portanto, algumas palavras v\u00e3o ser representadas de forma mais pr\u00f3xima do que em textos mais gerais (onde um W2V pr\u00e9-treinado foi treinado).","eb7f97c9":"# 1. Organizando os Conjuntos de Dados"}}