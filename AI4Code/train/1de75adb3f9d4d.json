{"cell_type":{"f4beade6":"code","efa0987d":"code","9289cb40":"code","28615adb":"code","9ea3162b":"code","6d9b1b43":"code","f8dd0f83":"code","9e566ead":"code","bab14b3f":"code","92372b51":"code","71b84103":"code","a0934f3e":"code","0cae289c":"code","5ed6b4ec":"code","0633b649":"code","f0dc7302":"code","12c05977":"code","8e63347b":"code","099e4b39":"markdown","283bcd8b":"markdown","002a1ee8":"markdown","6bc06a76":"markdown","67bf1c7c":"markdown","a90fb3f6":"markdown","187400cd":"markdown","600351db":"markdown","0c4df566":"markdown","3dbaa01b":"markdown","de387929":"markdown","566f04d2":"markdown","835dfa21":"markdown","24901c80":"markdown","84c4adf0":"markdown"},"source":{"f4beade6":"# importing common libraries\n\nimport re\nimport spacy as sp\nimport pandas as pd\nimport pickle as pk\n\nfrom scipy import sparse\nfrom nltk.stem import PorterStemmer\nfrom nltk import (corpus, word_tokenize, WordNetLemmatizer, pos_tag)\nfrom sklearn import (feature_extraction, datasets, linear_model, naive_bayes, ensemble, model_selection)","efa0987d":"# importing IMDB dataset into a pandas dataframe\n\nraw_df = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","9289cb40":"# getting an overview of the dataset\n\nraw_df.info()","28615adb":"# checking a sample of the data\n\nraw_df.sample(5)","9ea3162b":"# checking types of sentiments in dataset\n\nraw_df.sentiment.hist(bins=3)","6d9b1b43":"# Checking the existing list of stopwords\n\nstopWords = corpus.stopwords.words(\"english\")\nprint(\"NLTK's STOP WORDS LIST:\\n\\t\", stopWords)","f8dd0f83":"# checking difference between tokenization using regex vs NLTK's word_tokenize()\n\nsample_review = raw_df.review[7]\nregex_tk = re.compile(r\"\\b[A-Za-z0-9']+\\b\")\n\nprint(\"ORIGINAL SENTENCE:\\n\\t\", sample_review, \"\\n\")\nprint(\"REGEX TOKENIZED WORDS:\\n\\t\", re.findall(regex_tk, sample_review), \"\\n\")\nprint(\"WORD_TOKENIZED WORDS:\\n\\t\", word_tokenize(sample_review), \"\\n\")\n","9e566ead":"# this stemmer-tokenizer will not include stop words\n\nclass stemTokenizer:\n    def __init__(self):\n        self.stemmer = PorterStemmer()\n        self.token_pattern = re.compile(r\"\\b[A-Za-z0-9']{2,}\\b\")\n        \n    def __call__(self, sent):\n        sent = re.findall(self.token_pattern, sent)\n        return [self.stemmer.stem(word) for word in sent if word not in stopWords]\n    \n    \n# creating TFIDF matrix from raw data\n\ntfidf_vec = feature_extraction.text.TfidfVectorizer(tokenizer = stemTokenizer())","bab14b3f":"# this stemmer-tokenizer will include stop words\n# and with ngram range (1, 2)\n\nclass stemTokenizer_ngram:\n    def __init__(self):\n        self.stemmer = PorterStemmer()\n        self.token_pattern = re.compile(r\"[A-Za-z0-9']{2,}\")\n        \n    def __call__(self, sent):\n        sent = re.findall(self.token_pattern, sent)\n        return [self.stemmer.stem(word) for word in sent]\n\n    \n# creating TFIDF matrix with ngrams and stopwords\n\ntfidf_vec_ngram = feature_extraction.text.TfidfVectorizer(tokenizer = stemTokenizer_ngram(), \n                                                          ngram_range = (1, 2), \n                                                          max_features = 500000)","92372b51":"# creating features \n\nX = tfidf_vec.fit_transform(list(raw_df.review))","71b84103":"# creating n-grammed features \n\nX_ngram = tfidf_vec_ngram.fit_transform(list(raw_df.review))","a0934f3e":"# creating labels \n\ny = [1 if (i == \"positive\") else 0 for i in raw_df.sentiment]","0cae289c":"# getting an overview of the normal features\n\nprint(f\"There are total {len(tfidf_vec.get_feature_names())} features in the matrix\")\nprint(\"some of the features are: \", tfidf_vec.get_feature_names()[0:-1:10000])","5ed6b4ec":"# getting an overview of the n-grammed features\n\nprint(f\"There are total {len(tfidf_vec_ngram.get_feature_names())} features in the matrix\")\nprint(\"some of the features are: \", tfidf_vec_ngram.get_feature_names()[0:-1:50000])","0633b649":"# splitting features and labels into training and testing data\n\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.3)","f0dc7302":"# splitting ngrammed features and labels into training and testing data\n\nX_train_ngram, X_test_ngram, y_train_ngram, y_test_ngram = model_selection.train_test_split(X_ngram, y, test_size=0.3)","12c05977":"# testing normal features using MultinomialNB\n\nnb = naive_bayes.MultinomialNB()\nnb.fit(X_train, y_train)\nprint(\"Accuracy with normal features using Multinomial Naive Bayes:\\n\", nb.score(X_test, y_test))","8e63347b":"# testing ngrammed features using MultinomialNB\n\nnb_ngram = naive_bayes.MultinomialNB()\nnb_ngram.fit(X_train_ngram, y_train_ngram)\nprint(\"Accuracy with n-gram features using Multinomial Naive Bayes:\\n\", nb_ngram.score(X_test_ngram, y_test_ngram))","099e4b39":"We will be using Sklearn's `train_test_split()` function to split the processed data. \\\nThe train:test ratio is 70:30 to avoid overfitting.","283bcd8b":"We will try two types of feature processing models. \n1. Both will use Tfidf to vectorize the data.\n2. Both will use regex to filter out necessary words using regex regex pattern- `r\"\\b[A-Za-z0-9']{2,}\\b\"`\n3. Both will use NLTK's PorterStemmer for stemming words\n\nThe key differences are: \n1. Excluding and including stop words in the features\n2. Using different ngram ranges- (1, 1) and (1, 2)","002a1ee8":"### importing libraries and dataset","6bc06a76":"### getting training and testing data ready","67bf1c7c":"### processing features","a90fb3f6":"#### apparantly using stopwords and ngrams wins. ","187400cd":"# IMDB movie review sentiment analysis","600351db":"### sklearn's MultinomialNB","0c4df566":"### analyzing processed features","3dbaa01b":"We will use Naive Bayes classifier because it is useful for binary classification. \\\nSklearn' MultinomialNB is mostly used for term frequency data. ","de387929":"### calculating Tfidf of features","566f04d2":"### exploratary analysis","835dfa21":"This notebook will compare and use different NLP techniques and perform sentiment analysis on [imdb-dataset-of-50k-movie-reviews](http:\/\/https:\/\/www.kaggle.com\/lakshmi25npathi\/imdb-dataset-of-50k-movie-reviews) dataset. \n\n### Table of Contents:\n\n1.  Importing libraries and dataset\n2.  exploratory analysis\n    - overview of dataset\n    - sample of dataset\n    - graphical representation of different sentiments in the dataset\n    - listing NLTK's stopwords\n3. processing features\n    - custom stemmer-tokenizer functions\n        - normal features\n        - ngram features\n    - calculating Tfdf and features intentionally\n4. processing labels\n5. analyzing processed features\n    - overview of normal features\n    - overview of n-gram features\n6. getting training and testing data ready\n7. sklearn's MultinomialNB","24901c80":"### processing labels","84c4adf0":"Apparently regex performs tokenization in a meaningful way and is fast too. \nSo, we will be using regex for tokenizing the data."}}