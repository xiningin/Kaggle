{"cell_type":{"d8a82ba9":"code","e4b87e4f":"code","967de77d":"code","c77e8011":"code","e2f7a3d0":"code","fd3021ad":"code","457f8630":"code","afbe2457":"code","db12b4a5":"code","7a4151f6":"code","e198e1c5":"code","714a7aa5":"code","4372d57e":"code","d7209b94":"code","26b85940":"code","e7f61624":"code","d15b2195":"code","8e212181":"code","3a09db65":"code","6506a79f":"code","95d86fe0":"code","be96101f":"code","72c435b2":"code","f4abf124":"code","94ae8876":"code","fefd0604":"code","42de95e3":"code","43705f46":"code","c0337115":"code","04f0405a":"code","59077352":"code","96817bc4":"code","251b60d8":"code","a98586d9":"code","e7e38341":"code","3b0a8f7a":"code","219f99b3":"code","f58c477d":"code","c92ae5d5":"code","830ed7d9":"code","d7f8b607":"code","e0e4be2c":"code","d32973ac":"code","f51fd0d5":"code","cae29d79":"code","619aa79b":"code","48246aea":"code","29c6a822":"code","a277feae":"code","10068e24":"code","664023c9":"code","3288f2ef":"markdown","ac87b93f":"markdown","4d7ee2bc":"markdown","139786c5":"markdown","f17653ec":"markdown","b454222e":"markdown","dcf06d6a":"markdown","2e36a7e7":"markdown","c119d945":"markdown","a0727100":"markdown","0a26bf27":"markdown","41cd06bc":"markdown","c3a026a2":"markdown","998790ba":"markdown","eceb96b0":"markdown","d41d9bda":"markdown","da10cd20":"markdown","c462c730":"markdown","b60738ee":"markdown","a5e713ee":"markdown","fcb97833":"markdown","f5314277":"markdown","36002180":"markdown","24708f29":"markdown","4ca54486":"markdown","9e1200e3":"markdown","92bf185a":"markdown","49e5dcf6":"markdown","95368f1d":"markdown","42114d8f":"markdown","f7d9dc2d":"markdown"},"source":{"d8a82ba9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nimport matplotlib\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\n%matplotlib inline","e4b87e4f":"records = pd.read_csv('..\/input\/students-performance-in-exams\/StudentsPerformance.csv', sep=',')\nrecords.head()","967de77d":"print(records['math score'].describe())\nprint(records.gender.count())","c77e8011":"records.select_dtypes('object').nunique()","e2f7a3d0":"records.isnull().any()","fd3021ad":"scores = records.copy()\ndel scores['gender']\ndel scores['race\/ethnicity']\ndel scores['parental level of education']\ndel scores['lunch']\ndel scores['test preparation course']\n\n\n\ndef queueHistograms(df):\n    for col in df.columns:\n        drawHistogram(df[col], col)\n\n\ndef drawHistogram(records, col):\n    fig, ax = plt.subplots()\n    fig = plt.gcf()\n    fig.set_size_inches(10,7)\n    plt.hist(records, 10, density=False, color=(0.2, 0.4, 0.6, 0.6))\n    plt.xlabel('Scores')\n    plt.ylabel('# of students')\n    plt.title('Histogram of ' + col)\n    plt.grid(True)\n    plt.show()\n    \nqueueHistograms(scores)","457f8630":"def graphSpread(name, cols, vals):\n    y_pos = np.arange(len(cols))\n    fig, ax = plt.subplots()\n    fig = plt.gcf()\n    fig.set_size_inches(10,7)\n\n    rects1 = ax.bar(cols, vals, align='center', color=(0.2, 0.4, 0.6, 0.6))\n    plt.xticks(y_pos, cols)\n    ax.set_title(name + ' spread', fontsize=22)\n    plt.ylabel('Count')\n    autolabel(rects1, ax)\n    plt.show()\n    return\n\ndef getData(df, col):\n    cols = df[col].unique()\n    counts = df[col].value_counts()\n    return cols,counts\n\n\ndef runGraphs(df):\n    for col in df.columns:\n        c, v = getData(df, col)\n        graphSpread(col, c, v)\n\ndef autolabel(rects, ax):\n    \"\"\"\n    Attach a text label above each bar displaying its height\n    \"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        \n        ax.text(rect.get_x() + rect.get_width()\/2., 1.01*height,\n                '%d' % float(height),\n        ha='center', va='bottom')\n\n\nchartDf = records.copy()\ndel chartDf['math score']\ndel chartDf['reading score']\ndel chartDf['writing score']\nrunGraphs(chartDf)\n","afbe2457":"remove_scores = records[records['math score'] >= 80].copy()\ndel remove_scores['math score']\ndel remove_scores['reading score']\ndel remove_scores['writing score']\ntotal = remove_scores.gender.count()\nprint(total)","db12b4a5":"remove_scores.head(10)","7a4151f6":"def loopData(ds, subject, grade):\n    \"\"\"\n    Loop through all of the columns, format the data, and create a graph\n    \"\"\"\n    for col in ds.columns:\n        df = createData(ds, col, ds.gender.count())\n        createPie(df, col, subject, grade)\n\ndef createData(ds, header, total):\n    \"\"\"\n    Format the data how we want it for the chart\n    \"\"\"\n    df = ds.groupby(header, as_index=False).size().reset_index(name='count')\n    df['count'] = df['count'].astype(float)\n    df['count'] = (df['count'] \/ total * 100)\n    df['count'] = df['count'].round(decimals=0)\n    return df\n\ndef createPie(data, header, subject, grade):\n    \"\"\"\n    Create pie chart.  I am not using this anymore, but it may be helpful for others.\n    \"\"\"\n    labels = data[header].unique()\n    sizes = data['count'].unique()\n    plt.figure(figsize=(15,10))\n    fig1, ax1 = plt.subplots()\n    ax1.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90, radius=1900)\n    ax1.axis('equal')\n    ax1.set_title(grade + '%+ ' + subject + ' grade by ' + header, bbox={'facecolor':'0.8', 'pad':3}, fontsize=22)\n    fig = plt.gcf()\n    fig.set_size_inches(10,10)\n    plt.show()","e198e1c5":"def createDfs(df, col):\n    \"\"\"\n    Create data frames so that we can create the graphs\n    \"\"\"\n    ds = df[df['math score'] >= 80].copy()\n    df1 = createData(ds, col, ds.gender.count())\n\n    ds2 = df[df['reading score'] >= 80].copy()\n    df2 = createData(ds2, col, ds2.gender.count())\n\n    ds3 = df[df['writing score'] >= 80].copy()\n    df3 = createData(ds3, col, ds3.gender.count())\n\n    ds4 = df[(df['writing score'] >= 80) & (df['reading score'] >= 80) & (df['math score'] >= 80)].copy()\n    df4 = createData(ds4, col, ds4.gender.count())\n    return df1, df2, df3, df4\n\n\n\ndef runGraphs(df, col):\n    \"\"\"\n    Create the graphs to show how the spread was for the top of the classes for each subject.\n    \"\"\"\n    \n    df1, df2, df3, df4 = createDfs(df, col)\n    category_names = list(df1[col])\n\n    results = {\n        'Math Score 80%+': list(df1['count']),\n        'Writing Score 80%+' : list(df2['count']),\n        'Reading Score 80%+' : list(df3['count']),\n        'All Scores 80%+' : list(df4['count']),\n    }\n    \n    survey(results, category_names)\n    fig = plt.gcf()\n    plt.suptitle(col + ' impact on Exam Scores',x=.5)\n    fig.set_size_inches(15,7.5) \n\n    plt.show()\n\n\n\ndef survey(results, category_names):\n    \"\"\"\n    Parameters\n    ----------\n    results : dict\n        A mapping from question labels to a list of answers per category.\n        It is assumed all lists contain the same number of entries and that\n        it matches the length of *category_names*.\n    category_names : list of str\n        The category labels.\n    \"\"\"\n    labels = list(results.keys())\n    data = np.array(list(results.values()))\n    data_cum = data.cumsum(axis=1)\n    category_colors = plt.get_cmap('twilight')(\n        np.linspace(0.15, 0.85, data.shape[1]))\n\n    fig, ax = plt.subplots(figsize=(9.2, 5))\n    ax.invert_yaxis()\n    ax.xaxis.set_visible(False)\n    ax.set_xlim(0, np.sum(data, axis=1).max())\n\n    for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n        widths = data[:, i]\n        starts = data_cum[:, i] - widths\n        ax.barh(labels, widths, left=starts, height=0.5,\n                label=colname, color=color)\n        xcenters = starts + widths \/ 2\n\n        r, g, b, _ = color\n        text_color = 'white' if r * g * b < 0.5 else 'darkgrey'\n        for y, (x, c) in enumerate(zip(xcenters, widths)):\n            ax.text(x, y, str(int(c)), ha='center', va='center',\n                    color=text_color)\n    ax.legend(ncol=len(category_names), bbox_to_anchor=(0, 1),\n              loc='lower left', fontsize='small')\n\n    return fig, ax","714a7aa5":"# Create all of the graphs\nfor col in remove_scores.columns:\n    runGraphs(records, col)","4372d57e":"# Get a count of everything\nrecords['math high mark'] = records['math score'] >= 80\nrecords['reading high mark'] = records['reading score'] >= 80\nrecords['writing high mark'] = records['writing score'] >= 80\nrecords['all high mark'] = (records['math score'] >= 80) &(records['reading score'] >= 80) &(records['writing score'] >= 80)\n\nprint(records['math high mark'].value_counts())\nprint(records['reading high mark'].value_counts())\nprint(records['writing high mark'].value_counts())\nprint(records['all high mark'].value_counts())","d7209b94":"df = records.groupby('gender', as_index=False).size().reset_index(name='count')","26b85940":"print(df)","e7f61624":"def createDBData(df, label):\n    \"\"\"\n    Create a new data frame for the bars to say how many got high marks, \n    and how many did not.\n    \"\"\"\n    newDF = pd.DataFrame(data={label: [], 'high mark': [], 'not high mark': []})\n    for col in df[label].unique():\n        tds = records[df[label] == col]\n        total = len(tds)\n        tdf = tds.groupby('math high mark', as_index=False).size().reset_index(name='count')\n        high = tdf.iloc[1]['count']\n        low = tdf.iloc[0]['count']\n\n        newDF = newDF.append({label: col, 'high mark': high, 'not high mark': low}, ignore_index=True)\n    return newDF","d15b2195":"def autolabel2(rects, ax):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(0, 1),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom', color='black')\n\n\ndef drawBars(df, col):\n    labels = df[col].unique()\n    high_mark = df['high mark']\n    low_mark = df['not high mark']\n\n    x = np.arange(len(labels))  # the label locations\n    width = 0.35  # the width of the bars\n\n    fig, ax = plt.subplots()\n    rects1 = ax.bar(x - width\/2, high_mark, width, label='Score >= 80', align='center', color=(0.2, 0.4, 0.6, 0.6))\n    rects2 = ax.bar(x + width\/2, low_mark, width, label='Score < 80', align='center', color = (0.6, 0.4, 0.2, 0.6))\n\n    # Add some text for labels, title and custom x-axis tick labels, etc.\n    ax.set_ylabel('Count')\n    ax.set_title('Math Count by score and ' + col)\n    ax.set_xticks(x)\n    ax.set_xticklabels(labels)\n    ax.legend()\n    autolabel2(rects1, ax)\n    autolabel2(rects2, ax)\n\n    fig.tight_layout()\n\n    fig.set_size_inches(15,10) # or (4,4) or (5,5) or whatever\n\n    plt.show()","8e212181":"for col in remove_scores.columns:\n    newDF = createDBData(records, col)\n    drawBars(newDF, col)","3a09db65":"def convertToGrade(percent):\n    \"\"\"\n    Convert the % grade to a letter grade.\n    \"\"\"\n    if(percent >= 90):\n        return 'A'\n    if(percent >=80):\n        return 'B'\n    if(percent >=70):\n        return 'C'\n    if(percent >=60):\n        return 'D'\n    return 'F'","6506a79f":"records['math grade'] = records.apply(lambda x: convertToGrade(x['math score']), axis = 1)\nrecords['reading grade'] = records.apply(lambda x: convertToGrade(x['reading score']), axis = 1)\nrecords['writing grade'] = records.apply(lambda x: convertToGrade(x['writing score']), axis = 1)\n\n\nprint(records['math grade'].value_counts())\nprint(records['reading grade'].value_counts())\nprint(records['writing grade'].value_counts())\n","95d86fe0":"histDF = records.copy()\nhistDF.head()","be96101f":"# Delete everything but the grades for charting\ndel histDF['gender']\ndel histDF['race\/ethnicity']\ndel histDF['parental level of education']\ndel histDF['test preparation course']\ndel histDF['math score']\ndel histDF['reading score']\ndel histDF['writing score']\ndel histDF['math high mark']\ndel histDF['reading high mark']\ndel histDF['writing high mark']\ndel histDF['all high mark']\ndel histDF['lunch']\n\nhistDF.head()","72c435b2":"histDFSorted = histDF.sort_values(by=['math grade'], ascending=False)\nhistDFSorted.head()\nqueueHistograms(histDF)","f4abf124":"histDFSorted['math grade'].value_counts(sort=False)","94ae8876":"records.isnull().any()","fefd0604":"modelDF = records.copy()","42de95e3":"def passOrFail(percent):\n    \"\"\"\n    Create a pass\/fail column\n    \"\"\"\n    if(percent >= 60):\n        return 1\n    return 0","43705f46":"modelDF['average score'] = (modelDF['math score'] + modelDF['reading score'] + modelDF['writing score'])  \/ 3\nmodelDF['average score'] = modelDF['average score'].round(decimals=0)\nmodelDF['average grade'] = modelDF.apply(lambda x : convertToGrade(x['average score']), axis=1)\nrecords['average grade'] = modelDF['average grade']\n\nmodelDF['passed'] = modelDF.apply(lambda x: passOrFail(x['average score']), axis = 1)\nmodelDF.head()","c0337115":"def convertColumn(df, col):\n    \"\"\"\n    We can't use Strings on some of these columns, so let's do a simple conversion to numeric\n    \"\"\"\n    i = 0\n    newDF = df.copy()\n    for val in df[col].unique():\n        newDF[col] = newDF[col].replace(val, i)\n        i = i + 1\n    return newDF[col]\n\n\nmodelDF['gender'] = convertColumn(modelDF,'gender')\nmodelDF['race\/ethnicity'] = convertColumn(modelDF,'race\/ethnicity')\nmodelDF['parental level of education'] = convertColumn(modelDF,'parental level of education')\nmodelDF['lunch'] = convertColumn(modelDF,'lunch')\nmodelDF['test preparation course'] = convertColumn(modelDF,'test preparation course')\n\n\n\nmodelDF.head()\n\n","04f0405a":"y = modelDF[['average grade']].copy()\ny.head()","59077352":"#features = ['gender', 'race\/ethnicity', 'lunch', 'test preparation course', 'math score','reading score', 'writing score', 'math high mark', 'reading high mark', 'writing high mark', 'all high mark', 'math grade', 'reading grade', 'writing grade', 'average score']\n#features = ['gender', 'race\/ethnicity', 'lunch', 'test preparation course', 'math score','reading score', 'writing score', 'math high mark', 'reading high mark', 'writing high mark', 'all high mark']\n#features = ['gender', 'race\/ethnicity', 'lunch', 'test preparation course', 'math score','reading score', 'writing score']\n#features = ['gender', 'race\/ethnicity', 'lunch', 'test preparation course', 'math score','reading score', 'writing score', 'math high mark', 'reading high mark', 'writing high mark', 'all high mark', 'average score']\n\n\nfeatures = ['gender', 'race\/ethnicity', 'lunch', 'test preparation course']\n\n\nX = modelDF[features].copy()","96817bc4":"X.columns","251b60d8":"y.columns","a98586d9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)","e7e38341":"math_classifier = DecisionTreeClassifier(max_leaf_nodes=35, random_state=337)\nmath_classifier.fit(X_train, y_train)","3b0a8f7a":"predictions = math_classifier.predict(X_test)","219f99b3":"accuracy_score(y_true = y_test, y_pred = predictions)","f58c477d":"y2 = modelDF['passed']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.25, random_state=12)\n\nmath_classifier = DecisionTreeClassifier(max_leaf_nodes=3, random_state=337)\nmath_classifier.fit(X_train, y_train)\n\npredictions = math_classifier.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)","c92ae5d5":"features = ['gender', 'race\/ethnicity', 'lunch', 'test preparation course', 'reading score', 'writing score']\n\n\ny3 = modelDF['math grade']\nX = modelDF[features].copy()\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y3, test_size=0.25, random_state=12)\n\nmath_classifier = DecisionTreeClassifier(max_leaf_nodes=40, random_state=337)\nmath_classifier.fit(X_train, y_train)\n\npredictions = math_classifier.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)","830ed7d9":"y4 = modelDF.apply(lambda x: passOrFail(x['math score']), axis = 1)\n\nfeatures = ['gender', 'race\/ethnicity', 'lunch', 'test preparation course', 'reading score', 'writing score']\nX = modelDF[features].copy()\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y4, test_size=0.25, random_state=12)\n\nmath_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=337)\nmath_classifier.fit(X_train, y_train)\n\npredictions = math_classifier.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)\n","d7f8b607":"secondDF = records.copy()\nsecondDF.head()","e0e4be2c":"secondDF = pd.get_dummies(secondDF)","d32973ac":"secondDF.head()","f51fd0d5":"secondDF.columns","cae29d79":"using = secondDF.copy()","619aa79b":"features = ['gender_female', 'gender_male', 'race\/ethnicity_group A',\n       'race\/ethnicity_group B', 'race\/ethnicity_group C',\n       'race\/ethnicity_group D', 'race\/ethnicity_group E',\n       \"parental level of education_associate's degree\",\n       \"parental level of education_bachelor's degree\",\n       'parental level of education_high school',\n       \"parental level of education_master's degree\",\n       'parental level of education_some college',\n       'parental level of education_some high school', 'lunch_free\/reduced',\n       'lunch_standard', 'test preparation course_completed',\n       'test preparation course_none']\n\nX = using[features]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)\nmath_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=337)\nmath_classifier.fit(X_train, y_train)\n\npredictions = math_classifier.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)","48246aea":"X_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.25, random_state=12)\nmath_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=337)\nmath_classifier.fit(X_train, y_train)\n\npredictions = math_classifier.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)","29c6a822":"features = ['gender_female', 'gender_male', 'race\/ethnicity_group A',\n       'race\/ethnicity_group B', 'race\/ethnicity_group C',\n       'race\/ethnicity_group D', 'race\/ethnicity_group E',\n       \"parental level of education_associate's degree\",\n       \"parental level of education_bachelor's degree\",\n       'parental level of education_high school',\n       \"parental level of education_master's degree\",\n       'parental level of education_some college',\n       'parental level of education_some high school', 'lunch_free\/reduced',\n       'lunch_standard', 'test preparation course_completed',\n       'test preparation course_none', 'reading grade_A',\n       'reading grade_B', 'reading grade_C', 'reading grade_D',\n       'reading grade_F', 'writing grade_A', 'writing grade_B',\n       'writing grade_C', 'writing grade_D', 'writing grade_F','math grade_A', 'math grade_B',\n       'math grade_C', 'math grade_D', 'math grade_F' ]\n\nX = using[features]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)\nmath_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=337)\nmath_classifier.fit(X_train, y_train)\n\npredictions = math_classifier.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)","a277feae":"features = ['gender', 'race\/ethnicity', 'lunch', 'test preparation course', 'math score','reading score', 'writing score', 'math high mark', 'reading high mark', 'writing high mark', 'all high mark']\n\nX = modelDF[features].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=12)\nmath_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=337)\nmath_classifier.fit(X_train, y_train)\n\npredictions = math_classifier.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)\n","10068e24":"features = ['gender_female', 'gender_male', 'race\/ethnicity_group A',\n       'race\/ethnicity_group B', 'race\/ethnicity_group C',\n       'race\/ethnicity_group D', 'race\/ethnicity_group E',\n       \"parental level of education_associate's degree\",\n       \"parental level of education_bachelor's degree\",\n       'parental level of education_high school',\n       \"parental level of education_master's degree\",\n       'parental level of education_some college',\n       'parental level of education_some high school', 'lunch_free\/reduced',\n       'lunch_standard', 'test preparation course_completed',\n       'test preparation course_none', 'reading grade_A',\n       'reading grade_B', 'reading grade_C', 'reading grade_D',\n       'reading grade_F', 'writing grade_A', 'writing grade_B',\n       'writing grade_C', 'writing grade_D', 'writing grade_F','math grade_A', 'math grade_B',\n       'math grade_C', 'math grade_D', 'math grade_F' ]\n\nX = using[features]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.25, random_state=12)\nmath_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=337)\nmath_classifier.fit(X_train, y_train)\n\npredictions = math_classifier.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)","664023c9":"features = ['gender', 'race\/ethnicity', 'lunch', 'test preparation course', 'math score','reading score', 'writing score', 'math high mark', 'reading high mark', 'writing high mark', 'all high mark']\n\nX = modelDF[features].copy()\nX_train, X_test, y_train, y_test = train_test_split(X, y2, test_size=0.25, random_state=12)\nmath_classifier = DecisionTreeClassifier(max_leaf_nodes=10, random_state=337)\nmath_classifier.fit(X_train, y_train)\n\npredictions = math_classifier.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)\n","3288f2ef":"### Are we missing any data?","ac87b93f":"### Less accurate\nIt's interesting that this made it worse.  Maybe guessing a pass\/fail will work better with this method?","4d7ee2bc":"# Thank you\nThank you to the following:\n\n- [Kaggle](https:\/\/kaggle.com) for such a great platform.\n\n- [Jakki](https:\/\/www.kaggle.com\/spscientist\/students-performance-in-exams?select=StudentsPerformance.csv) for the data he created.\n\n- [Jack Vial](http:\/\/jackvial.com) for looking through my notebook and helping me look at it from other angles.\n\n- [Dan Beck](https:\/\/www.kaggle.com\/dansbecker\/using-categorical-data-with-one-hot-encoding) on his tutorial on One-Hot Encoding.\n\n- [UC San Diego](https:\/\/www.edx.org\/micromasters\/uc-san-diegox-data-science) for their Micro Masters program on Data Science","139786c5":"### Get basic stats","f17653ec":"### Another approach\nLet's try and do One-hot encoding.","b454222e":"### Import Libraries","dcf06d6a":"### Better\nThis is better than guessing their grade, but not great.  Before I stated that I didn't want to know the grades, as I felt it was cheating.  But what if we knew the reading and writing grades, and wanted to know what their Math grade would be?","2e36a7e7":"### Also less accurate\nBut at least we know it didn't work as well.  What if we want to know their average grade, but we already had all the other data, this should be very high, right?","c119d945":"### Features\nWe are going to go with their social and economic value.  We will not be using scores from their other classes, as I feel that is way too easy.  You can uncomment trying some other combinations though.","a0727100":"### Get total students that scored higher than 80%","0a26bf27":"### Standard","41cd06bc":"### Make our predictions\nHow well did the model guess on the average grade someone got just based on social and economic factors?","c3a026a2":"### Draw histograms of scores\nLet's see the spread of scores for Math, Reading, and Writing","998790ba":"### Train the model","eceb96b0":"### Better\nThis is a lot more accurate than previously, but much less accurate than the passing grade.  What about just guessing if they will pass math?","d41d9bda":"### Initial impressions\nInitially, this looks like there is a huge impact for Group A and free\/reduced students on getting at least an 80% on their tests.  However, this does not show the entire picture.  The groups that had the lowest numbers could also potentially have the least amount of data.\n\nLet create a new type of graph","da10cd20":"### Graph spread of the different columns\nHow even is the spread across the different categories?","c462c730":"### Open file","b60738ee":"### The model could not figure out what grade a student.  \nMaybe it can figure out a simple pass\/fail based on these factors?","a5e713ee":"# Machine Learning task\nWe are going to use the most basic machine learning task and do a DecisionTreeClassifier.  This is a classification task.  I am worried that there isn't enough data.","fcb97833":"# Student's Performance in Exams\nWhat type of impact does someone's background have on their performance in Exams?\n\nGrabbed data from Kaggle\n[https:\/\/www.kaggle.com\/spscientist\/students-performance-in-exams?select=StudentsPerformance.csv](https:\/\/www.kaggle.com\/spscientist\/students-performance-in-exams?select=StudentsPerformance.csv)\n\n**Note**: This is fictitious data.","f5314277":"### Training and test data split\nWe are going to split our data since we do not have a separate set of data to test things on.","36002180":"### Create reusable functions\nFormat the data as needed, as well as creating a pie chart.","24708f29":"This is interesting.  More than half of the people go less than 50% on their math score!","4ca54486":"# Final Notes\nWe were semi reasonably able to determine if a student would pass or fail based on the social and economic factors, but not remotely reasonable to determine their grade.  If we knew two of their grades, we could reasonably guess what their overall grade would be.  This could potentially be useful for teachers to give extra attention to certain students.\n\nSomeone's economic background may have a big impact on the grades.  What can we do as a community to help those in need?","9e1200e3":"### Lastly\nLet's guess if they passed overall if we already knew the grades.","92bf185a":"### How many unique records in each column do we have","49e5dcf6":"### One-Hot Encoding","95368f1d":"### What we are trying to guess\nWe are going to try and guess what the average grade will be.  This seems very difficult.","42114d8f":"# Results\nThere is a big impact on lunch with regards to grades.  36% of the students who had standard lunch got an 80%+ on their math. 6.6% of free\/reduced lunch students got an 80%+ on their math!  What a big difference!\n\nI also thought test preparation would have a bigger impact on the scores.  19.5% of students with no prep course got 80%+ on Math.  For those that did complete the course, 32.6% of them got an 80%+.  Definitely better, but not by as much as I would have thought.\n\nGender played a small role on performance.  16% of Female's got high marks, while 23% of Male's did.","f7d9dc2d":"### Get graphs for impact for Math, Writing, Reading, and All\nThe student must get at least an 80% in math, writing, reading, or in all of them."}}