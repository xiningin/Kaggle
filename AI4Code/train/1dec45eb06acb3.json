{"cell_type":{"9a704e1b":"code","735d32d3":"code","753cc9cc":"code","cc9c6bc5":"code","112d8364":"code","59861494":"code","76d003cf":"code","d79b2d55":"code","23e87e3a":"code","0928e60d":"code","c722d6d5":"code","fce3e8d2":"code","f4dfa855":"code","ec751d2d":"code","d4ed514f":"code","b46d5f73":"code","42b62b5d":"code","4d42c39e":"code","38590ec7":"code","15ab6f32":"code","9de0fa01":"code","500373c8":"code","7de73f05":"code","ff86a968":"code","bb7e157f":"code","a58761d3":"code","d96ba149":"code","38ec5d8e":"code","6907dfea":"code","86c2151c":"code","35165a15":"code","52c6e9eb":"code","3c1830ce":"code","e2b5e85c":"code","2facdd22":"markdown","3080aad9":"markdown"},"source":{"9a704e1b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","735d32d3":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import plot_roc_curve","753cc9cc":"heart = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","cc9c6bc5":"df=heart.copy()\ndf.head()\n#columns names\ndf.columns","112d8364":"print('SHAPE'.center(80,\"*\"))\nprint('rows:{} columns:{} '.format(df.shape[0],df.shape[1]))\nprint('datatypes'.center(80,\"*\"))\nprint(df.dtypes)\nprint('descriptions'.center(80,\"*\"))\nprint(df.describe(include=\"all\"))\nprint('Info'.center(80,\"*\"))\nprint(df.info())\n\n","59861494":"#AGE\nprint(df['age'].isnull().sum())\n\nax=sns.distplot(df['age'],\n                  bins=100,\n                  kde=True,\n                  color='skyblue',\n                  hist_kws={\"linewidth\": 15,'alpha':1})\n\nbx = sns.boxplot(data=df['age'], orient=\"h\", palette=\"Set2\")\n\ndf['age'].value_counts(dropna=False)\n#zscore\nfrom scipy import stats\nz_score=np.abs(stats.zscore(df['age']))\nprint(z_score>3)","76d003cf":"\nprint(df['sex'].isnull().sum())\nax=sns.countplot(df['sex']\n              )\nprint(df['age'].isnull().sum())","d79b2d55":"for colmns in df.columns:\n    print(colmns)\n    print(set(df[colmns]))\n    print(df[colmns].isnull().sum())","23e87e3a":"\nfor cols in df:\n    if df[cols].nunique() < 10:\n        plt.show()\n        sns.countplot(df[cols])\n        \n   \n    \n    ","0928e60d":"for cols in df:\n    if df[cols].nunique() > 10:\n        plt.show()\n        sns.distplot(df[cols])\n        \n   ","c722d6d5":"for cols in df:\n    if df[cols].nunique() > 10:\n        plt.show()\n        sns.boxplot(df[cols])\n        ","fce3e8d2":"\nfor cols in df:\n    if df[cols].nunique() > 10:\n        \n        z_score=np.abs(stats.zscore(df[cols]))\n        print(z_score>3)","f4dfa855":"sns.pairplot(df,palette='Accent',hue='output')","ec751d2d":"#pearson ion using \ndf.corr()\nplt.show()\nf,ax=plt.subplots(figsize = (14,14))\nsns.heatmap(df.corr(),annot= True,fmt = \".2f\",annot_kws={\"size\": 12} ,vmin = -1,ax=ax, vmax = 1, linewidth = 0.4,)","d4ed514f":"#want to find distributions of each variabele using shapiro test\nfrom scipy.stats import shapiro  \nstats=[]\nfor cols in df.columns:\n    \n    stat,p=shapiro(np.array(df[cols]))\n    stats.append(stat)\n    print(stat)\nfig = plt.figure(figsize =(10, 7))\nplt.bar(df.columns,stats,width=.4)","b46d5f73":"#Pre processing\n#missing values correction\ndf.isnull().sum()","42b62b5d":"#duplicate rows in tthe data'\ndf.duplicated().sum()\ndf[df.duplicated()]\ndf.drop_duplicates(keep='first',inplace=True)\ndf.duplicated().sum()","4d42c39e":"\ndef outliers_detect(col):\n    ascv=[]\n    df[col]\n    for v in df[col]:\n        ascv.append(v)\n    ascv.sort()\n\n    Q1,Q3=np.percentile(ascv,[25,75])\n    IQR=Q3-Q1\n    low_range=Q1-(1.5*IQR)\n    high_range=Q3+(1.5*IQR)\n   \n    filter1=df[col]>high_range\n    filter2=df[col]<low_range  \n    outliers=df.where(filter1 | filter2 ,axis=0)\n    return df[outliers[cols].notnull()].index.tolist()","38590ec7":"outliers={}\noutliers_list=[]\nfor cols in df.columns:\n    if df[cols].nunique() > 10 :\n        outliers[cols]=outliers_detect(cols)\n        outliers_list.append(outliers_detect(cols))\n","15ab6f32":"from itertools import chain\noutliers_list=set(list(chain(*outliers_list)))","9de0fa01":"outliers","500373c8":"\ndf=df.drop(labels=outliers_list,axis=0)","7de73f05":"#scaling the values","ff86a968":"test_size = 0.3\nrandom_state = 42\ny=df['output']\nX=df.drop(\"output\",axis=1)\nX_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=.3)","bb7e157f":"X_train","a58761d3":"scaler = StandardScaler() #StandardScaler - RobustScaler\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","d96ba149":"X_train\n","38ec5d8e":"\nr=GaussianNB()\nr.fit(X_train,Y_train)\nresult=r.predict(X_test)\ny_pred=result\nprint(y_pred)\nprint(classification_report(Y_test, y_pred, target_names=['0','1']))\nprint(confusion_matrix(Y_test,y_pred))    \n    ","6907dfea":"sgd = SGDClassifier(loss='hinge',random_state=random_state)\nsgd.fit(X_train,Y_train)\nresult=r.predict(X_test)\ny_pred=result\nprint(y_pred)\nprint(classification_report(Y_test, y_pred, target_names=['0','1']))\nprint(confusion_matrix(Y_test,y_pred))\n\n    ","86c2151c":"lda = LinearDiscriminantAnalysis(solver='svd',n_components = 1)\nsgd = SGDClassifier(loss='hinge',random_state=random_state)\nsgd.fit(lda.fit_transform(X_train,Y_train),Y_train)\nresult=r.predict(lda.transform(X_test))\ny_pred=result\nprint(y_pred)\nprint(classification_report(Y_test, y_pred, target_names=['0','1']))\nprint(confusion_matrix(Y_test,y_pred))\n\n    \n","35165a15":"\nr=GaussianNB()\nr.fit(lda.fit_transform(X_train,Y_train),Y_train)\nresult=r.predict(lda.transform(X_test))\ny_pred=result\nprint(y_pred)\nprint(classification_report(Y_test, y_pred, target_names=['0','1']))\nprint(confusion_matrix(Y_test,y_pred))    \n    ","52c6e9eb":"#models using grid search cv\naccuracy=[]\nroc_auc_scores = []\nrecall_scores = []\nprecision_scores = []\nf1_scores = []\n\nmodels=[RandomForestClassifier(random_state = random_state), LogisticRegression(random_state = random_state),\n              KNeighborsClassifier()]\n\npara_rf={\"max_features\": [1,3,10],\n                \"min_samples_split\":[2,3,10],\n                \"min_samples_leaf\":[1,3,10],\n                \"bootstrap\":[False],\n                \"n_estimators\":[100,300],\n                \"criterion\":[\"gini\"]}\npara_lr={\"C\":np.logspace(-4, 4, 20),\n                    \"penalty\": [\"l1\",\"l2\",\"none\"]}\npara_knn={\"n_neighbors\": np.linspace(2,20,12, dtype = int).tolist(),\n                 \"weights\": [\"uniform\",\"distance\"],\n                 \"metric\":[\"euclidean\",\"manhattan\",\"minkowski\"],\n                 \"leaf_size\": [1,3,5,12,30]}\n\nmodel_para=[para_rf,para_lr,para_knn]\n\nfor model in range(len(models)):\n    \n    clf = GridSearchCV(models[model],param_grid=model_para[model],cv = StratifiedKFold(n_splits = 10),\n                        scoring = \"accuracy\",n_jobs = -1,verbose = 2)\n    clf.fit(X_train,Y_train)\n    print(clf.best_estimator_)\n    accuracy.append(accuracy_score(clf.predict(X_test),Y_test))\n    roc_auc_scores.append(roc_auc_score(Y_test, clf.predict_proba(X_test)[:,1]))\n    recall_scores.append(recall_score(Y_test, clf.predict(X_test)))\n    precision_scores.append(precision_score(Y_test, clf.predict(X_test), average='weighted'))\n    f1_scores.append(f1_score(Y_test, clf.predict(X_test), average='weighted'))\n  \n    \n   ","3c1830ce":"cv_results = pd.DataFrame({\"Accuracy\":accuracy,\n                           \n                           \"ROC AUC\":roc_auc_scores,\n                           \"Recall\": recall_scores,\n                           \"Precision\": precision_scores,\n                           \"F1-Score\":f1_scores,\n                           \"Models\": models})\n","e2b5e85c":"cv_results.index = cv_results[\"Models\"]\ncv_results  = cv_results.drop([\"Models\"], axis = 1)\n\nf,ax = plt.subplots(figsize=(14,10))\nsns.heatmap(cv_results, annot=True,cmap = \"Purples\",\n            fmt= '.3f',ax=ax,linewidths = 5,\n            cbar = False,annot_kws={\"size\": 18})\n\nplt.xticks(size = 16)\nplt.yticks(size = 16, rotation = 0)\nplt.title(\"Grid Search Results\", size = 16)\nplt.show()  ","2facdd22":"Analysing varibales individually(Univariate analysis)","3080aad9":"\n\n    Age : Age of the patient\n\n    Sex : Sex of the patient\n\n    exang: exercise induced angina (1 = yes; 0 = no)\n\n    ca: number of major vessels (0-3)\n\n    cp : Chest Pain type chest pain type\n        Value 1: typical angina\n        Value 2: atypical angina\n        Value 3: non-anginal pain\n        Value 4: asymptomatic\n\n    trtbps : resting blood pressure (in mm Hg)\n\n    chol : cholestoral in mg\/dl fetched via BMI sensor\n\n    fbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n    rest_ecg : resting electrocardiographic results\n        Value 0: normal\n        Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n        Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\n    thalach : maximum heart rate achieved\n\n    target : 0= less chance of heart attack 1= more chance of heart attack\n\nn "}}