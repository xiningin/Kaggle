{"cell_type":{"956a5829":"code","03422991":"code","684bec1d":"code","5a948958":"code","8c7dbce9":"code","2c964ed0":"code","cb4196f5":"code","dadb734d":"code","5e97cfa9":"code","8455234b":"code","ba8641a4":"code","1586079b":"code","046767c3":"code","a78ca87f":"code","a8381a06":"code","436b1349":"code","a7082c35":"code","6178e186":"code","a8b1cc91":"code","ac680a2c":"code","7507655d":"code","5d7d59aa":"code","0558305a":"code","690c445d":"code","ff8b6871":"code","7c9129c3":"markdown","63465799":"markdown","8fbd8df5":"markdown","64f655b6":"markdown","8778bc51":"markdown","3c2d836a":"markdown","293e3640":"markdown","2ce7ac56":"markdown","f6832199":"markdown","b4f6e2ff":"markdown","d59c2987":"markdown","f17e3beb":"markdown","eab5aa80":"markdown","33024e2a":"markdown","8b0c0cf6":"markdown","1229c4ea":"markdown","09e86b46":"markdown","bdfd30f0":"markdown","8c5d3bf1":"markdown","5ff82bae":"markdown","853e997c":"markdown","2554a0c8":"markdown","ec53d07a":"markdown","ba390727":"markdown","1814b705":"markdown"},"source":{"956a5829":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.graph_objects as go\n\n# nlp lib\nimport sys\nimport nltk\nfrom nltk import wordpunct_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize   \nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport string, collections, unicodedata\nfrom gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\nfrom gensim.summarization import summarize\nfrom gensim.summarization import keywords\n\n# map\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom geopandas.tools import geocode\n!pip install geopy==1.22.0\nimport pandas as pd\nimport geopandas as gpd\nimport numpy as np\nimport folium\nfrom folium import Marker\nimport folium\nfrom folium import Choropleth, Circle, Marker\nfrom folium.plugins import HeatMap, MarkerCluster\nimport math\nimport re\nfrom pprint import pprint as print\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03422991":"donald_trump = pd.read_csv('\/kaggle\/input\/us-election-2020-tweets\/hashtag_donaldtrump.csv', lineterminator='\\n')\ndonald_trump.loc[:,'who'] = 'donald Trump'\njoe_biden = pd.read_csv('\/kaggle\/input\/us-election-2020-tweets\/hashtag_joebiden.csv', lineterminator='\\n')\njoe_biden.loc[:,'who'] = 'joe Biden'\ndata = pd.concat([joe_biden,donald_trump])","684bec1d":"data.sort_values(by='created_at').info()","5a948958":"data","8c7dbce9":"y = data.query('(who == \"joe Biden\") ').dropna(subset=['country']).groupby(by='country').count().tweet.sort_values(ascending=False)\nx = data.query('(who == \"joe Biden\") ').dropna(subset=['country']).groupby(by='country').count().tweet.sort_values(ascending=False).index\ny2 = data.query('(who == \"donald Trump\") ').dropna(subset=['country']).groupby(by='country').count().tweet.sort_values(ascending=False)\nx2 = data.query('(who == \"donald Trump\")').dropna(subset=['country']).groupby(by='country').count().tweet.sort_values(ascending=False).index\nfig = go.Figure([go.Bar(x=x, y=y, name='joe Biden'),\n                 go.Bar(x=x2, y=y2, name='donald Trump')])\n\n# Customize aspect\nfig.update_layout(title_text='tweets count for each countries')\nfig.update_xaxes(title='users')\nfig.update_yaxes(title='tweets count')\nfig.show()","2c964ed0":"y = data.query('(who == \"joe Biden\") & (country == \"United States of America\")').dropna(subset=['state','country']).groupby(by='state').count().tweet.sort_values(ascending=False)\nx = data.query('(who == \"joe Biden\") & (country == \"United States of America\")').dropna(subset=['state','country']).groupby(by='state').count().tweet.sort_values(ascending=False).index\ny2 = data.query('(who == \"donald Trump\") & (country == \"United States of America\")').dropna(subset=['state','country']).groupby(by='state').count().tweet.sort_values(ascending=False)\nx2 = data.query('(who == \"donald Trump\") & (country == \"United States of America\")').dropna(subset=['state','country']).groupby(by='state').count().tweet.sort_values(ascending=False).index\nfig = go.Figure([go.Bar(x=x, y=y, name='joe Biden'),\n                 go.Bar(x=x2, y=y2, name='donald Trump')])\n\n# Customize aspect\nfig.update_layout(title_text='Tweets count for each states of America')\nfig.update_xaxes(title='users')\nfig.update_yaxes(title='tweets count')\nfig.show()","cb4196f5":"y = data.query('who == \"joe Biden\"').groupby(by='user_name').count().tweet.sort_values(ascending=False)[:10]\nx = data.query('who == \"joe Biden\"').groupby(by='user_name').count().tweet.sort_values(ascending=False)[:10].index\ny2 = data.query('who == \"donald Trump\"').groupby(by='user_name').count().tweet.sort_values(ascending=False)[:10]\nx2 = data.query('who == \"donald Trump\"').groupby(by='user_name').count().tweet.sort_values(ascending=False)[:10].index\nfig = go.Figure([go.Bar(x=x, y=y, name='joe Biden'),\n                 go.Bar(x=x2, y=y2, name='donald Trump')])\n\n# Customize aspect\nfig.update_layout(title_text='top 10 most active users')\nfig.update_xaxes(title='users')\nfig.update_yaxes(title='tweets count')\nfig.show()","dadb734d":"y = data.query('who == \"joe Biden\"').groupby(by='source').count().tweet.sort_values(ascending=False)[:10]\nx = data.query('who == \"joe Biden\"').groupby(by='source').count().tweet.sort_values(ascending=False)[:10].index\ny2 = data.query('who == \"donald Trump\"').groupby(by='source').count().tweet.sort_values(ascending=False)[:10]\nx2 = data.query('who == \"donald Trump\"').groupby(by='source').count().tweet.sort_values(ascending=False)[:10].index\nfig = go.Figure([go.Bar(x=x, y=y, name='joe Biden'),\n                 go.Bar(x=x2, y=y2, name='donald Trump')])\n\n# Customize aspect\nfig.update_layout(title_text='top 10 sources')\nfig.update_xaxes(title='sources')\nfig.update_yaxes(title='tweets count')\nfig.show()","5e97cfa9":"y = data.query('who == \"joe Biden\"').sort_values('user_followers_count',ascending=False).drop_duplicates(['user_name']).user_followers_count[:10]\nx = data.query('who == \"joe Biden\"').sort_values('user_followers_count',ascending=False).drop_duplicates(['user_name']).user_name[:10]\ny2 = data.query('who == \"donald Trump\"').sort_values('user_followers_count',ascending=False).drop_duplicates(['user_name']).user_followers_count[:10]\nx2 = data.query('who == \"donald Trump\"').sort_values('user_followers_count',ascending=False).drop_duplicates(['user_name']).user_name[:10]\nfig = go.Figure([go.Bar(x=x, y=y, name='joe Biden'),\n                 go.Bar(x=x2, y=y2, name='donald Trump')])\n\n# Customize aspect\nfig.update_layout(title_text='top 10 famous users')\nfig.update_xaxes(title='users')\nfig.update_yaxes(title='followers count')\nfig.show()","8455234b":"def _calculate_languages_ratios(text):\n    \"\"\"\n    Calculate probability of given text to be written in several languages and\n    return a dictionary that looks like {'french': 2, 'spanish': 4, 'english': 0}\n    \n    @param text: Text whose language want to be detected\n    @type text: str\n    \n    @return: Dictionary with languages and unique stopwords seen in analyzed text\n    @rtype: dict\n    \"\"\"\n\n    languages_ratios = {}\n\n    '''\n    nltk.wordpunct_tokenize() splits all punctuations into separate tokens\n    \n    >>> wordpunct_tokenize(\"That's thirty minutes away. I'll be there in ten.\")\n    ['That', \"'\", 's', 'thirty', 'minutes', 'away', '.', 'I', \"'\", 'll', 'be', 'there', 'in', 'ten', '.']\n    '''\n\n    tokens = wordpunct_tokenize(text)\n    words = [word.lower() for word in tokens]\n\n    # Compute per language included in nltk number of unique stopwords appearing in analyzed text\n    for language in stopwords.fileids():\n        stopwords_set = set(stopwords.words(language))\n        words_set = set(words)\n        common_elements = words_set.intersection(stopwords_set)\n\n        languages_ratios[language] = len(common_elements) # language \"score\"\n\n    return languages_ratios\n\ndef detect_language(text):\n    \"\"\"\n    Calculate probability of given text to be written in several languages and\n    return the highest scored.\n    \n    It uses a stopwords based approach, counting how many unique stopwords\n    are seen in analyzed text.\n    \n    @param text: Text whose language want to be detected\n    @type text: str\n    \n    @return: Most scored language guessed\n    @rtype: str\n    \"\"\"\n\n    ratios = _calculate_languages_ratios(text)\n\n    most_rated_language = max(ratios, key=ratios.get)\n\n    return most_rated_language","ba8641a4":"def usa_tweet(who, nb):\n    \"\"\"return the tweets related to who. nb is the number of tweets used\"\"\"\n    \n    tweets = data.query('who == \"'+who+'\"').sort_values('user_followers_count',ascending=False).drop_duplicates(['user_name'])[['tweet','long','lat','country']]\n    tweets = tweets.dropna().loc[tweets.country=='United States of America'][:nb]\n    languages = [detect_language(val) for val in tweets.tweet]\n    text = tweets.loc[np.array(languages) == 'english']\n    return text\n\ndef map_tweets(df):\n    \"\"\"return a map of the USA tweets\"\"\"\n    \n    m_3 = folium.Map(location=[35.32,-91.0589], tiles='cartodbpositron', zoom_start=4)\n\n    # Add points to the map\n    df = df.dropna(subset=['long','lat'])\n    mc = HeatMap(data=df[['lat','long']])\n    m_3.add_child(mc)\n\n    # Display the map\n    return m_3\n\ndef clean_text(df):\n    \"\"\"Remove HTTPS... and concatenate sentences in one string\"\"\"\n    \n    string = ''\n    for t in df.tweet.replace('\\n', ' ').str.lower():\n        string = string+' '+t\n    return string\n\ndef sum_up(string, ratio):\n    \"\"\"return sum up, the ratio determine the lenght of the summary\"\"\"\n    \n    print('-----sum up-----\/n')\n    print(summarize(string,ratio=ratio, split=True))\n    print('\/n-----Key words-----')\n    print(keywords(string, ratio=ratio))\n    ","1586079b":"df_j = usa_tweet('joe Biden', 1000)\ntxt_j = clean_text(df_j)\nsum_up(txt_j, 0.005)","046767c3":"map_tweets(df_j)","a78ca87f":"df_d = usa_tweet('donald Trump', 1000)\ntxt_d = clean_text(df_d)\nsum_up(txt_d, 0.005)","a8381a06":"map_tweets(df_d)","436b1349":"# We have to clean the text and split each word\nstopWords = set(stopwords.words('english'))\n\ndef stop_words(sent):\n    \n    sent = (unicodedata.normalize('NFKD', sent)\n            .encode('ascii', 'ignore')\n            .decode('utf-8', 'ignore')\n            .lower())\n    sent = re.sub(r'[^\\w\\s]', '', sent)\n    list_words = nltk.word_tokenize(sent)\n    list_words = [w for w in list_words if w not in stopWords]\n    return list_words\n\n\ndf_d = usa_tweet('donald Trump', 2000).tweet.str.replace('\\n', ' ').str.lower()\n# Split the text\nall_words = [stop_words(sent) for sent in df_d]\n# from gensim (It creates a neural network to trasform word into vectors)\nmodel = Word2Vec(all_words, size=10, window=1, min_count=1, workers=4)","a7082c35":"data_sim = model.wv.most_similar('joe',topn=25)\nx = [i for i,j in data_sim]\ny = [j for i,j in data_sim]\n\nfig, ax = plt.subplots(figsize=(10,10))\nsns.barplot(x=x, y=y, color=\"salmon\", saturation=.5, ax=ax)\nax.set_title('Closest words to joe')\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\nax.set_ylim(np.min(y),1)\nax.set_xlabel('Words')\nax.set_ylabel('Proportion of similarities')","6178e186":"data_sim = model.wv.most_similar('donald',topn=25)\n\nx = [i for i,j in data_sim]\ny = [j for i,j in data_sim]\n\nfig, ax = plt.subplots(figsize=(10,10))\nsns.barplot(x=x, y=y, color=\"salmon\", saturation=.5, ax=ax)\nax.set_title('Closest words to joe')\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n         rotation_mode=\"anchor\")\n\nax.set_ylim(np.min(y),1)\nax.set_xlabel('Words')\nax.set_ylabel('Proportion of similarities')","a8b1cc91":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\n\ndef analyse(who):\n    sid = SentimentIntensityAnalyzer()\n    neg = []\n    neu = []\n    pos = []\n    for sentence in usa_tweet(who, 7500).tweet.str.replace('\\n', ' '):\n        ss = sid.polarity_scores(sentence)\n        neg.append(ss[sorted(ss)[1]])\n        neu.append(ss[sorted(ss)[2]])\n        pos.append(ss[sorted(ss)[3]])\n\n    result = pd.DataFrame({'pos':pos,'neg':neg,'neu':neu})\n\n    a = (result.pos>result.neg).sum()\n    b = (result.pos<result.neg).sum()\n    c = (result.pos==result.neg).sum()\n\n    \n    colors = ['gold', 'mediumturquoise', 'darkorange', 'lightgreen']\n    fig = go.Figure(data=[go.Pie(labels=[\"positivity\",\"negativity\",\"neutrality\"], values=[a,b,c]),])\n    fig.update_traces(hoverinfo='label+percent', textinfo='value', textfont_size=20,\n                      marker=dict(colors=colors, line=dict(color='#000000', width=2)))\n    fig.update(layout_title_text=f'Proportions of subjectivity on {who} tweets')\n\n    fig.show()\n\n    confidence_interval(a,b+c)\n\ndef usa_tweet(who, nb):\n    \"\"\"return the tweets related to who. nb is the number of tweets used\"\"\"\n    \n    tweets = data.query('who == \"'+who+'\"').sample(frac = 1).drop_duplicates(['user_name'])[['tweet','state','long','lat','country']]\n    tweets = tweets.dropna().loc[tweets.country=='United States of America'][:nb]\n    languages = [detect_language(val) for val in tweets.tweet]\n    text = tweets.loc[np.array(languages) == 'english']\n    return text\n\ndef confidence_interval(N,N2):\n    \"\"\"1.96 means that we are sure up to 95% that p is under low and up bounds\"\"\"\n    \n    p = round(N\/(N+N2),3)\n    low = round(p-1.96*math.sqrt(p*(1-p)\/(N+N2)),3)\n    up = round(p+1.96*math.sqrt(p*(1-p)\/(N+N2)),3)\n    print(\"The confidence interval for positivity tweets is : Low:{0} mean:{1} up:{2}\".format(low,p,up))","ac680a2c":"analyse('donald Trump')","7507655d":"analyse('joe Biden')","5d7d59aa":"poly = gpd.read_file('\/kaggle\/input\/usapolygones4\/tl_2014_us_state.shp', SHAPE_RESTORE_SHX='YES')\n\n# preprocessing\nstring = ''\nfor x in poly.NAME:\n    if x not in [\"Guam\", \"Puerto Rico\", \"United States Virgin Islands\", \"Commonwealth of the Northern Mariana Islands\", \"American Samoa\", \"District of Columbia\"]:\n        string = string + x + '|'\n    \nusa_states = data[['tweet','state','who']].dropna()\nusa_states_filt = usa_states.loc[usa_states.state.str.contains(string[:-1], case=False)]\nusa_states_filt = usa_states_filt.loc[~usa_states_filt.state.isin(['Baja California','Baja California Sur'])]","0558305a":"def kfold_stratified(X, y, nb):\n    \"\"\"\n    X: dataframe\n    y: string\n    nb_split: number of exemples per classes\n    \n    return a sample of the dataframe, same number of data for each classes\n    \"\"\"\n    \n    X_tf = pd.DataFrame(columns=X.columns)\n    for name in X['who'].unique():\n        for classe in X[str(y)].unique():\n            X_ech = X.loc[(X[str(y)]==classe)&(X['who']==name)].sample(frac=1).iloc[:int(nb\/2)]\n            X_tf = pd.concat([X_tf, X_ech], axis=0, ignore_index=True)\n    return X_tf\n\n# samples\nusa_states_tf = kfold_stratified(usa_states_filt, 'state',10000)\n\nsid = SentimentIntensityAnalyzer()\n    \ndef clean2(text):\n    text = (unicodedata.normalize('NFKD', text)\n            .encode('ascii', 'ignore')\n            .decode('utf-8', 'ignore')\n            .lower())\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text\n\ndef sent_tweets(t):\n    \"\"\"\n    return the polarity of the sentence t\n    \n    1: neg\n    2: neu\n    3: pos\n    \"\"\"\n    \n    t = t.replace('\\n', ' ')\n    ss = sid.polarity_scores(clean2(t))\n    if ss[sorted(ss)[1]] > ss[sorted(ss)[3]]:\n        return 1\n    elif ss[sorted(ss)[3]] > ss[sorted(ss)[1]]:\n        return 3\n    else:\n        return 2\n    \n\ndef sentiment(x):\n    \"\"\"return the % of positivity for each state\"\"\"\n    df = x.value_counts()\n    try:\n        return (df[3]*100)\/ df.sum()\n    except:\n        return np.nan\n    \n# Sentiments   \nusa_states_tf['sentiment'] = usa_states_tf.tweet.apply(sent_tweets)\n\n# Count proportions\nusa_states_tf_gb = usa_states_tf.groupby(['state','who']).agg({'sentiment':sentiment})\n\nresults = pd.DataFrame(index=usa_states_tf.state.unique(), columns=['NAME'])\nfor data in results.index:\n    if usa_states_tf_gb.loc[[(data,'donald Trump')]].values > usa_states_tf_gb.loc[[(data,'joe Biden')]].values:\n        results.loc[data,'NAME'] = 1 #donald Trump\n    else:\n        results.loc[data,'NAME'] = 2 #joe Biden","690c445d":"m_6 = folium.Map(location=[35.32,-81.0589], tiles='cartodbpositron', zoom_start=3)\n\n# Add a choropleth map to the base map\nChoropleth(geo_data=poly[['NAME','geometry']].set_index('NAME').__geo_interface__, \n           data=results['NAME'],\n           bins=3,\n           key_on=\"feature.id\", \n           fill_color='YlGnBu', \n           legend_name='(yellow)donald Trump vs (blue)joe Biden'\n          ).add_to(m_6)\n\n# Display the map\nm_6","ff8b6871":"\nx = usa_states_tf_gb.reset_index()\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x=x.state.unique(),\n    y=x.loc[x.who=='donald Trump'].sentiment,\n    name='donald Trump',\n    marker_color='indianred'\n))\nfig.add_trace(go.Bar(\n    x=x.state.unique(),\n    y=x.loc[x.who=='joe Biden'].sentiment,\n    name='joe Biden',\n    marker_color='lightsalmon'\n))\n\n# Here we modify the tickangle of the xaxis, resulting in rotated labels.\nfig.update_layout(barmode='group', xaxis_tickangle=-45, title='Donald Trump VS Joe Biden: Positivity proportion per states')\nfig.show()","7c9129c3":"For this inspiration i took some assumptions. I only took into account the american top famous users and study their tweets. \n\nI tried to give an overview of the top famous tweets and show the similarites to Joe Biden and Donald Trump names: \n\nThanks to this approach, we can show if a candidate is more criticized.","63465799":"You can find and downloads the usa shapefiles dataset to create choroplet map. Be carefull 6 states have to be removed: Guam, Puerto Rico, United States Virgin Islands, Commonwealth of the Northern Mariana Islands, American Samoa, District of Columbia. \n\nThis map show the candidate who have a better global appreciation for each state.","8fbd8df5":"### Content\n\nThe 2020 US election is happening on the 3rd November 2020 and the resulting impact to the world will no doubt be large, irrespective of which candidate is elected! After reading the two papers, here and here, I was inspired to attempt a similar sentiment analysis myself!\n\nColumns are as follows:\n\n- created_at: Date and time of tweet creation\n- tweet_id: Unique ID of the tweet\n- tweet: Full tweet text\n- likes: Number of likes\n- retweet_count: Number of retweets\n- source: Utility used to post tweet\n- user_id: User ID of tweet creator\n- user_name: Username of tweet creator\n- user_screen_name: Screen name of tweet creator\n- user_description: Description of self by tweet creator\n- user_join_date: Join date of tweet creator\n- user_followers_count: Followers count on tweet creator\n- user_location: Location given on tweet creator's profile\n- lat: Latitude parsed from user_location\n- long: Longitude parsed from user_location\n- city: City parsed from user_location\n- country: Country parsed from user_location\n- state: State parsed from user_location\n- state_code: State code parsed from user_location\n- collected_at: Date and time tweet data was mined from twitter*","64f655b6":"<h2 style=\"text-align: center;\">USA map<\/h2> <a id=f><a\/>\n<hr>","8778bc51":"### Donald Trump","3c2d836a":"## 3 - Joe Biden tweets summary and Heat Map<a id=c><a\/>","293e3640":"First, import the data:","2ce7ac56":"You will find in this section multiple graphs which give you informations about the data.","f6832199":"## 2 - We apply 4 functions to get the tweets we need, clean it, display on a map the location and to give a sum up and important words on the bench of tweets.","b4f6e2ff":"<img style=\"float:left;\" src=\"https:\/\/img.icons8.com\/carbon-copy\/100\/000000\/futurama-bender.png\"\/>\n<h1 style=\"text-align: center; font-size:50px;\"> US election tweets <\/h1> \n<hr>\n<hr>","d59c2987":"Unhide cells to see the code :) ","f17e3beb":"For this part, i used the sentimental analyser from nltk which give us a proportion of neutrality, positivity and negativity for each sentence. It can be used only on english tweets.\n\n>Hutto, C.J. & Gilbert, E.E. (2014). VADER: A Parsimonious Rule-based Model for Sentiment Analysis of Social Media Text. Eighth International Conference on Weblogs and Social Media (ICWSM-14). Ann Arbor, MI, June 2014.\n\nLet's first take the case of Donald Trump: Over 5000 most famous tweets.\n\nAssumptions: \n- A tweet is counted as positive if its value pos > neg\n- A tweet is counted as negative if its value neg > pos\n- A tweet is counted as neutral if its value neu == 1","eab5aa80":"### Joe Biden","33024e2a":"We could use these previous assumptions to estimate the final result:\n","8b0c0cf6":"## 4 - Donald Trump tweets summary and Heat Map<a id=d><a\/>","1229c4ea":"#### Similarities with 'donald'","09e86b46":"## Let's see what are the closest words for each candidate name","bdfd30f0":"<h2 style=\"text-align: center;\">Exploring the data<\/h2> <a id=a><a\/>\n<hr>","8c5d3bf1":"<h2 style=\"text-align: center;\">Can we detect if there are or were any attempts at manipulating the election. <\/h2><a id=b><a\/>\n<hr>","5ff82bae":"#### Similarities with 'joe'","853e997c":"# Thanks for your time and hope this notebook will help you!!","2554a0c8":"## Conclusion:\n\nIt's difficult to say if someone tries to manipulate the election. the previous summaries give us an overview of the users feeling. The two candidates has their own defects. We can add an analysis to complete the previous work.","ec53d07a":"## 1 - These two functions are used to detect if the language used in the tweet is english or not. If not, we remove them.","ba390727":"<h2 style=\"text-align: center;\">Sentimental Analysis<\/h2> <a id=e><a\/>\n<hr>","1814b705":"## Table of contents\n\n- [Exploring the data](#a)\n- [Can we detect if there are or were any attempts at manipulating the election.](#b)\n- [Summary of Joe Biden tweets](#c)\n- [Summary of Donald Trump tweets](#d)\n- [Sentimental analysis](#e)\n- [USA map](#f)"}}