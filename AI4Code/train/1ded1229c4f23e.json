{"cell_type":{"6e7c0b75":"code","e06d7689":"code","d3cc8cbe":"code","aa46ebce":"code","6e6ebd8f":"code","f333cee1":"code","bc46699c":"code","6102aab1":"code","d2b308e3":"markdown","c65c37bd":"markdown","c0ae4a90":"markdown","08e9c26a":"markdown","cb9f0cd3":"markdown"},"source":{"6e7c0b75":"import matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom PIL import Image, ImageOps\nfrom scipy.spatial import cKDTree\nfrom skimage.feature import plot_matches\nfrom skimage.measure import ransac\nfrom skimage.transform import AffineTransform\nfrom six import BytesIO\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom six.moves.urllib.request import urlopen","e06d7689":"MAGE_1_URL = 'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/2\/28\/Bridge_of_Sighs%2C_Oxford.jpg'\nIMAGE_2_URL = 'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/c\/c3\/The_Bridge_of_Sighs_and_Sheldonian_Theatre%2C_Oxford.jpg'\n\nIMAGE_1_URL = 'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/1\/1e\/Golden_gate2.jpg'\nIMAGE_2_URL = 'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/3\/3e\/GoldenGateBridge.jpg'\n\nIMAGE_1_URL = 'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/c\/ce\/2006_01_21_Ath%C3%A8nes_Parth%C3%A9non.JPG'\nIMAGE_2_URL = 'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/5c\/ACROPOLIS_1969_-_panoramio_-_jean_melis.jpg'\n\nIMAGE_1_JPG = 'image_1.jpg'\nIMAGE_2_JPG = 'image_2.jpg'","d3cc8cbe":"def download_and_resize_image(url, filename, new_width=256, new_height=256):\n    response = urlopen(url)\n    image_data = response.read()\n    pil_image = Image.open(BytesIO(image_data))\n    pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n    pil_image_rgb = pil_image.convert('RGB')\n    pil_image_rgb.save(filename, format='JPEG', quality=90)\n\ndownload_and_resize_image(IMAGE_1_URL, IMAGE_1_JPG)\ndownload_and_resize_image(IMAGE_2_URL, IMAGE_2_JPG)\n\ndef show_images(image_path_list):\n    plt.figure()\n    for i, image_path in enumerate(image_path_list):\n        plt.subplot(1, len(image_path_list), i+1)\n        plt.imshow(np.asarray(Image.open(image_path)))\n        plt.title(image_path)\n        plt.grid(False)\n        plt.yticks([])\n        plt.xticks([])\n    plt.show()\n\nshow_images([IMAGE_1_JPG, IMAGE_2_JPG])","aa46ebce":"def image_input_fn():\n    filename_queue = tf.train.string_input_producer(\n      [IMAGE_1_JPG, IMAGE_2_JPG], shuffle=False)\n    reader = tf.WholeFileReader()\n    _, value = reader.read(filename_queue)\n    image_tf = tf.image.decode_jpeg(value, channels=3)\n    return tf.image.convert_image_dtype(image_tf, tf.float32)","6e6ebd8f":"tf.reset_default_graph()\ntf.logging.set_verbosity(tf.logging.FATAL)\n\nm = hub.Module('https:\/\/tfhub.dev\/google\/delf\/1')\n\n# The module operates on a single image at a time, so define a placeholder to\n# feed an arbitrary image in.\nimage_placeholder = tf.placeholder(\n    tf.float32, shape=(None, None, 3), name='input_image')\n\nmodule_inputs = {\n    'image': image_placeholder,\n    'score_threshold': 100.0,\n    'image_scales': [0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0],\n    'max_feature_num': 1000,\n}\n\nmodule_outputs = m(module_inputs, as_dict=True)\n\nimage_tf = image_input_fn()\n\nwith tf.train.MonitoredSession() as sess:\n    results_dict = {}  # Stores the locations and their descriptors for each image\n    for image_path in [IMAGE_1_JPG, IMAGE_2_JPG]:\n        image = sess.run(image_tf)\n        print('Extracting locations and descriptors from %s' % image_path)\n        results_dict[image_path] = sess.run(\n            [module_outputs['locations'], module_outputs['descriptors']],\n            feed_dict={image_placeholder: image})","f333cee1":"def match_images(results_dict, image_1_path, image_2_path):\n    distance_threshold = 0.8\n\n    # Read features.\n    locations_1, descriptors_1 = results_dict[image_1_path]\n    num_features_1 = locations_1.shape[0]\n    print(\"Loaded image 1's %d features\" % num_features_1)\n    locations_2, descriptors_2 = results_dict[image_2_path]\n    num_features_2 = locations_2.shape[0]\n    print(\"Loaded image 2's %d features\" % num_features_2)\n\n    # Find nearest-neighbor matches using a KD tree.\n    d1_tree = cKDTree(descriptors_1)\n    _, indices = d1_tree.query(\n      descriptors_2, distance_upper_bound=distance_threshold)\n\n    # Select feature locations for putative matches.\n    locations_2_to_use = np.array([\n      locations_2[i,]\n      for i in range(num_features_2)\n      if indices[i] != num_features_1\n    ])\n    locations_1_to_use = np.array([\n      locations_1[indices[i],]\n      for i in range(num_features_2)\n      if indices[i] != num_features_1\n    ])\n\n    # Perform geometric verification using RANSAC.\n    _, inliers = ransac(\n      (locations_1_to_use, locations_2_to_use),\n      AffineTransform,\n      min_samples=3,\n      residual_threshold=20,\n      max_trials=1000)\n\n    print('Found %d inliers' % sum(inliers))\n\n    # Visualize correspondences.\n    _, ax = plt.subplots()\n    img_1 = mpimg.imread(image_1_path)\n    img_2 = mpimg.imread(image_2_path)\n    inlier_idxs = np.nonzero(inliers)[0]\n    plot_matches(\n      ax,\n      img_1,\n      img_2,\n      locations_1_to_use,\n      locations_2_to_use,\n      np.column_stack((inlier_idxs, inlier_idxs)),\n      matches_color='b')\n    ax.axis('off')\n    ax.set_title('DELF correspondences')\n\nmatch_images(results_dict, IMAGE_1_JPG, IMAGE_2_JPG)","bc46699c":"import pandas as pd\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.sample(5)","6102aab1":"for _, (c_id, c_df) in zip(range(3), train_df.groupby('landmark_id')):\n    print(c_id)\n    # take the first 2 urls\n    url_1, url_2, *_ = c_df['url'].values\n    download_and_resize_image(url_1, IMAGE_1_JPG)\n    download_and_resize_image(url_2, IMAGE_2_JPG)\n    with tf.train.MonitoredSession() as sess:\n        results_dict = {}  # Stores the locations and their descriptors for each image\n        for image_path in [IMAGE_1_JPG, IMAGE_2_JPG]:\n            image = sess.run(image_tf)\n            print('Extracting locations and descriptors from %s' % image_path)\n            results_dict[image_path] = sess.run(\n                [module_outputs['locations'], module_outputs['descriptors']],\n                feed_dict={image_placeholder: image})\n    match_images(results_dict, IMAGE_1_JPG, IMAGE_2_JPG)","d2b308e3":"## Post-processing and visualization","c65c37bd":"# Overview\nThe kaggle kernel-based version of the tf-hub notebook [here](https:\/\/github.com\/tensorflow\/hub\/blob\/master\/examples\/colab\/tf_hub_delf_module.ipynb). The example uses the pretrained [DELF](https:\/\/github.com\/tensorflow\/models\/tree\/master\/research\/delf) feature detector for the recognition\n    ","c0ae4a90":"## Apply to Contest Data\nHere we can try to apply the model to some contest data","08e9c26a":"# Apply the DELF module to the data\nThe DELF module takes an image as input and will describe noteworthy points with vectors. ","cb9f0cd3":"# Demo Images"}}