{"cell_type":{"3a930b7a":"code","e4092c1c":"code","80755429":"code","02c988fe":"code","0637383d":"code","055efd23":"code","5e25653a":"code","077b659d":"code","3d295cf2":"code","ee2a7280":"code","6fe49861":"code","b685ff67":"code","7582ed70":"code","1f766a67":"code","739f65e0":"code","0904fe03":"code","741fb866":"code","c8515e50":"code","30cae66d":"code","6dab21a2":"markdown","2ebd64a6":"markdown","75b00053":"markdown","ecf6df8d":"markdown","aa669ad7":"markdown","1e900395":"markdown","df9f7311":"markdown","cf216f95":"markdown","7635d732":"markdown","8d99b85b":"markdown","769038ff":"markdown","d2a25564":"markdown","2f61532d":"markdown","e51e96a9":"markdown","2e5b89bf":"markdown","96ca4dcd":"markdown","b7613a1b":"markdown","cc092e27":"markdown","eedf5838":"markdown","8d219683":"markdown","83c3aaee":"markdown","c8906e1f":"markdown"},"source":{"3a930b7a":"import matplotlib.pyplot as plt\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input,Dense\nfrom tensorflow.keras import Model","e4092c1c":"(x_train,y_train), (x_test, y_test) = mnist.load_data()","80755429":"print('X_train: ', (x_train.shape))\nprint('Y_train: ', (y_train.shape))\nprint('X_test:  ' , (x_test.shape))\nprint('Y_test:  ' , (y_test.shape))","02c988fe":"# We need to reshape the data to be only a list of 1D vectors of size (28*28) = 784\nx_train = x_train.reshape(-1,28*28)\nx_test = x_test.reshape(-1,28*28)","0637383d":"# Scaling the data between 0 and 1\nx_train = x_train\/255\nx_test = x_test\/255","055efd23":"autoencoder = Sequential([Input(x_train[0].shape),\n                      Dense(512,activation='elu',name = 'encoder_layer_1'),\n                      Dense(128,activation='elu',name = 'encoder_layer_2'),\n                      Dense(8,activation = 'linear',name = 'bottleneck'),\n                      Dense(128,activation='elu',name = 'decoder_layer_1'),\n                      Dense(512,activation='elu',name = 'decoder_layer_2'),\n                      Dense(28*28,activation = 'sigmoid',name = 'decoder_output'),\n                      ])","5e25653a":"autoencoder.compile(loss='mean_squared_error', optimizer = 'Adam')","077b659d":"autoencoder.summary()","3d295cf2":"training = autoencoder.fit(x_train,x_train,validation_data=(x_test,x_test),epochs = 10)","ee2a7280":"plt.plot(training.history['loss'])\nplt.plot(training.history['val_loss'])\nplt.legend(['train', 'test'], loc='upper right')\nplt.ylabel('loss')\nplt.xlabel('epoch')","6fe49861":"# Checking what the autoencoder is doing\nplt.title('Encoder Input')\nplt.imshow(x_test[6].reshape(28,28))","b685ff67":"plt.title('Decoder Output')\nplt.imshow(autoencoder.predict(x_test[6].reshape(-1,784)).reshape(-1,28))","7582ed70":"intermediate_model = Model(inputs=autoencoder.input,outputs=autoencoder.get_layer('bottleneck').output)\n\n\ndef get_embeddings(img):\n  \"\"\"\n  Returns 8 dimension embeddings for a 28*28 image\n  \n  Args:\n  img[np.array of shape = (1,784)]\n\n  Returns:\n  embeddings[Tensor of shape = (1, 8)]\n\n  \"\"\"\n  return intermediate_model(img.reshape(1,784))\n \n","1f766a67":"get_embeddings(x_test[77])","739f65e0":"# Encoder model {Input -> ... -> Bottleneck}\nencoder = Model(autoencoder.input, autoencoder.get_layer('bottleneck').output)","0904fe03":"encoder.predict(x_test[1].reshape(-1,784))","741fb866":"# Decoder model {Input -> Bottleneck -> ... -> Output}\n\nencoded_input = Input(shape=(8,))\ndecoder = autoencoder.layers[-3](encoded_input)\ndecoder = autoencoder.layers[-2](decoder)\ndecoder = autoencoder.layers[-1](decoder)\n\ndecoder = Model(encoded_input, decoder)","c8515e50":"# Manually plotting encoder input and decoder output\nplt.figure(figsize=(8,6), dpi=100)\nplt.subplot(1,2,1)\nplt.title('Encoder Input')\nplt.imshow(x_test[50].reshape(28,28))\nplt.subplot(1,2,2)\nplt.title('Decoder Output')\n\nencoded_input = encoder.predict(x_test[50].reshape(-1,784))\noutput = decoder.predict(encoded_input).reshape(28,28)\n\nplt.imshow(output)","30cae66d":"# Checking multiple test samples\nplt.figure(figsize=(10,10), dpi=100)\nfor i in range(1,6):\n  plt.subplot(2,5,i)\n  plt.title('Encoder')\n  plt.imshow(x_test[i].reshape(28,28))\n\n\nplt.figure(figsize=(10,10), dpi=100)\nfor i in range(1,6):\n  plt.subplot(2,5,i)\n  plt.title('Decoder')\n  encoded_input = encoder.predict(x_test[i].reshape(-1,784))\n  output = decoder.predict(encoded_input).reshape(28,28)\n\n  plt.imshow(output)","6dab21a2":"An autoencoder is a type of artificial neural network used to learn efficient codings of unlabeled data (unsupervised learning).\nThe autoencoder learns a representation (encoding) for a set of data, typically for dimensionality reduction, by training the network to ignore insignificant data (\u201cnoise\u201d).","2ebd64a6":"Although I used keras sequential API, I suggest you use the functional API to reproduce this code. This would strengthen your concepts and gives more control over the layers.","75b00053":"# Loading the data","ecf6df8d":"<p> This notebook is a beginner friendly guide to Auto-encoders, containing intuitively readable and easy to understand code.\n\nThe sections of this notebook would induce an intuition in you about how simple and effective auto-encoders are and how easy it is to code them\n<\/p>","aa669ad7":"Evidently, the autoencoder words pretty well, retaining appropriate amount of information which is representative enough. This brings us one step closer to understand how Auto-Encoders are very powerful.\nIn this notebook, we essentially reduced 784 dimensional data to just 8 dimensions with the results being very robust and the model performing near perfect on the train and test set.","1e900395":"# Plotting the loss","df9f7311":"To make this tutorial easy, we are gonna use the MNIST dataset, every data scientist's first choice while experimenting with new architectures.","cf216f95":"# Validating ","7635d732":"# Diving into the code","8d99b85b":"# Definition","769038ff":"If this notebook does the trick for you and you now understand how an auto-encoder works, do drop a like and be sure to comment what you like and what could have I done better, I'll be mindful to keep them in mind the next time.\n\nThanks for reading","d2a25564":"# Conclusion\n","2f61532d":"# Training the model","e51e96a9":"# Obtaining separate encoder and decoder models","2e5b89bf":"<img src='https:\/\/miro.medium.com\/max\/875\/1*44eDEuZBEsmG_TCAKRI3Kw@2x.png'>","96ca4dcd":"# Checking model output","b7613a1b":"# Defining the model","cc092e27":"# The end","eedf5838":"## Importing the necessary libraries","8d219683":"# A visual representation","83c3aaee":"# Auto-Encoders : A beginner friendly guide","c8906e1f":"# Obtaining embeddings"}}