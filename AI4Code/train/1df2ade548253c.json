{"cell_type":{"6407615a":"code","42eadab2":"code","c90e4170":"code","55b329b8":"code","09ac9867":"code","be08c94f":"code","2cc3bf88":"code","f7210f51":"code","fc6bcb1f":"code","0b1fff4f":"code","527e5e7c":"code","f5b2a5e0":"code","e94a9d51":"code","b26a3266":"code","5cf0a3a7":"code","b9f05ea8":"code","6f3a6c45":"code","6e00b788":"code","87839a6e":"markdown"},"source":{"6407615a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","42eadab2":"import torch\nfrom transformers import T5ForConditionalGeneration,T5Tokenizer\n\n\ndef set_seed(seed):\n  torch.manual_seed(seed)\n  if torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\nset_seed(42)\n\nmodel = T5ForConditionalGeneration.from_pretrained('..\/input\/t5-tuning-for-paraphrasing-questions\/result\/')\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# print (\"device \",device)\nmodel = model.to(device)","c90e4170":"text = '''Tim is a skilled Software Testing professional. Tim's experience includes test team management, client and vendor \nmanagement, automation test engineering, test infrastructure creation and maintenance. \nTim is an expert in building testing teams from scratch. Tim puts together processes, strategies, tools and frameworks, \nskilled resources for efficient and seamless delivery. \nExpertise in designing and developing test strategy, test plan, test cases and generating test reports and defect reports.\nExtensive experience in coordinating testing effort, responsible for test deliverables,\nstatus reporting to management, issue escalations etc. Have Extensive experience in automating and \ntesting enterprise web applications using Selenium 1.0 and Selenium 2.0 (WebDriver) tool using both Microsoft and JAVA technology stack.\nTim is very good at JAVA, C# and python language programming. Tim has strong experience in C# programming using MS Visual Studio and\nother MS technology stack in particular MSTest unit testing framework. Tim has very good experience in setting up Continuous Integration\nenvironment using Team Foundation Server, creating Build definitions and management etc. Domain experience includes Superannuation, \nTelco, Insurance, Banking, Finance and healthcare domains. Tim's linkedin profile is https:\/\/www.linkedin.com\/in\/timothy-r-alex-ai\/.\nTim's contact phone number is 0470139767. Tim's email address is timothyrajan@gmail.com. \nTim has good exposure to gitlab and github hosting platforms.'''","55b329b8":"truefalse= 'True'","09ac9867":"answer= ['Tim','Java','Testing professional','linkedin']","be08c94f":"con1 = \"context: %s answer: %s<\/s>\" %(text, answer)","2cc3bf88":"con = \"truefalse: %s passage: %s <\/s>\" % (text, truefalse)","f7210f51":"con2= 'ParaphraseQuestion: %s <\/s>' % ('Can you tell me the right way please?')","fc6bcb1f":"encoding = tokenizer.encode_plus(con, return_tensors=\"pt\")\ninput_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)","0b1fff4f":"def greedy_decoding (inp_ids,attn_mask):\n    greedy_output = model.generate(input_ids=inp_ids, attention_mask=attn_mask, max_length=40)\n    Question =  tokenizer.decode(greedy_output[0], skip_special_tokens=True,clean_up_tokenization_spaces=True)\n    return Question.strip().capitalize()","527e5e7c":"print (con)\n# print (\"\\nGenerated Question: \",truefalse)\n\noutput = greedy_decoding(input_ids,attention_masks)\nprint (output)\n","f5b2a5e0":"def beam_search_decoding (inp_ids,attn_mask):\n      beam_output = model.generate(input_ids=inp_ids,\n                                     attention_mask=attn_mask,\n                                     max_length=256,\n                                   num_beams=30,\n                                   num_return_sequences=6,\n                                   no_repeat_ngram_size=2,\n                                   early_stopping=True\n                                   )\n      Questions = [tokenizer.decode(out, skip_special_tokens=True, clean_up_tokenization_spaces=True) for out in\n                   beam_output]\n      return [Question.strip().capitalize() for Question in Questions]\n      #return  Questions\n\n\n\ndef topkp_decoding (inp_ids,attn_mask):\n      topkp_output = model.generate(input_ids=inp_ids,\n                                     attention_mask=attn_mask,\n                                     max_length=256,\n                                   do_sample=True,\n                                   top_k=40,\n                                   top_p=0.80,\n                                   num_return_sequences=3,\n                                    no_repeat_ngram_size=2,\n                                    early_stopping=True\n                                   )\n      Questions = [tokenizer.decode(out, skip_special_tokens=True,clean_up_tokenization_spaces=True) for out in topkp_output]\n      return list(Question.strip().capitalize() for Question in Questions)","e94a9d51":"output = beam_search_decoding(input_ids,attention_masks)\nprint (\"\\nBeam decoding [Most accurate questions] ::\\n\")\nfor out in output:\n    print(out)","b26a3266":"output = topkp_decoding(input_ids,attention_masks)\nprint (\"\\nTopKP decoding [Not very accurate but more variety in questions] ::\\n\")\nfor out in output:\n    print (out)","5cf0a3a7":"import ast\nimport gc\ngc.collect()","b9f05ea8":"def t5_answer(t,a):\n    con = \"context: %s answer: %s<\/s>\" % (t,a)\n    encoding = tokenizer.encode_plus(con, return_tensors=\"pt\")\n    input_ids, attention_masks = encoding[\"input_ids\"].to(device), encoding[\"attention_mask\"].to(device)\n    output = beam_search_decoding (input_ids, attention_masks)\n    return str(output)","6f3a6c45":"t5_answer(text, answer)","6e00b788":"for ans in answer:\n    output = t5_answer(text,ans)\n    res = ast.literal_eval(output) \n    longest_string = max(res, key=len)\n\n    print(ans)\n    print(longest_string)","87839a6e":"In [this](https:\/\/www.kaggle.com\/bunnyyy\/t5-tuning-for-paraphrasing-questions) notebook, I trained T5 model for paraphrasing questions and Q\/A. Now in current nb, I've used the parameters and weights of pretrained model."}}