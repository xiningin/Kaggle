{"cell_type":{"181e7ad4":"code","e2ee45eb":"code","878123e3":"code","71891ad8":"code","fe765d96":"code","5263a68b":"code","6a1c570b":"code","7298f0e6":"code","83a0344f":"code","867a0c10":"code","47553482":"code","32071ca9":"markdown","b348400a":"markdown","334ef5ca":"markdown","0176c23e":"markdown","a8626c2d":"markdown","f3d39e38":"markdown","d97141c8":"markdown","d91ffba2":"markdown","50267470":"markdown"},"source":{"181e7ad4":"!pip install tf-agents","e2ee45eb":"from abc import ABC\nfrom random import choice\nimport tensorflow as tf\nimport numpy as np\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import q_network\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\n\nfrom kaggle_environments import make\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import *\n\nimport base64\nimport imageio\nimport IPython\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\n","878123e3":"tf.compat.v1.enable_v2_behavior()\n\n\ndef get_board(env):\n    config = env.configuration\n    columns = config.columns\n    rows = config.rows\n\n    numeric_board = np.full([columns * rows], 10, dtype=int)\n\n    food_number = 5\n\n    for pos in env.state[0].observation.food:\n        numeric_board[pos] = food_number\n\n    for index, goose in enumerate(env.state[0].observation.geese):\n        for position in goose:\n            numeric_board[position] = index\n\n    #numeric_board = numeric_board.reshape((columns, rows))\n\n    return numeric_board\n\n\nclass GeeseEnv(py_environment.PyEnvironment):\n\n    def __init__(self):\n\n        self._env = make(\"hungry_geese\")\n        # The number of agents\n        self._NUM_AGENTS = 2\n\n        # Reset environment\n        observations = self._env.reset(num_agents=self._NUM_AGENTS)\n\n        self._state = get_board(self._env)\n\n        self._action_spec = array_spec.BoundedArraySpec(\n            shape=(), dtype=np.int32, minimum=0, maximum=3, name='action')\n        self._observation_spec = array_spec.BoundedArraySpec(\n            shape=(1, 1, self._state.shape[0]), dtype=np.int32, minimum=0, maximum=10,\n            name='observation')\n        self._episode_ended = False\n\n    def action_spec(self):\n        return self._action_spec\n\n    def observation_spec(self):\n        return self._observation_spec\n\n    def _reset(self):\n        observations = self._env.reset(num_agents=self._NUM_AGENTS)\n        self._state = [[get_board(self._env)]]\n        self._episode_ended = False\n        return ts.restart(np.array(self._state, dtype=np.int32))\n\n    def _step(self, action):\n\n        if self._episode_ended:\n            # The last action ended the episode. Ignore the current action and start\n            # a new episode.\n            return self.reset()\n\n        self._state = get_board(self._env)\n\n        choices = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n\n        actions = [choices[action], choice(choices)]\n\n        self._env.step(actions)\n\n        reward = self._env.steps[len(self._env.steps) - 1][0].reward\n\n        if self._env.done:\n            self._episode_ended = True\n\n        if self._episode_ended:\n            return ts.termination(np.array([[self._state]], dtype=np.int32), reward)\n        else:\n            return ts.transition(\n                np.array([[self._state]], dtype=np.int32), reward=reward, discount=1.0)\n","71891ad8":"env = GeeseEnv()\nprint('action_spec:', env.action_spec())\nprint('time_step_spec.observation:', env.time_step_spec().observation)\nprint('time_step_spec.step_type:', env.time_step_spec().step_type)\nprint('time_step_spec.discount:', env.time_step_spec().discount)\nprint('time_step_spec.reward:', env.time_step_spec().reward)","fe765d96":"num_iterations = 10000  # @param {type:\"integer\"}\n\ninitial_collect_steps = 100  # @param {type:\"integer\"}\ncollect_steps_per_iteration = 1  # @param {type:\"integer\"}\nreplay_buffer_max_length = 100000  # @param {type:\"integer\"}\n\nbatch_size = 64  # @param {type:\"integer\"}\nlearning_rate = 1e-3  # @param {type:\"number\"}\nlog_interval = 200  # @param {type:\"integer\"}\n\nnum_eval_episodes = 10  # @param {type:\"integer\"}\neval_interval = 1000  # @param {type:\"integer\"}","5263a68b":"\ntf.compat.v1.enable_v2_behavior()\n\ntrain_py_env = GeeseEnv()\neval_py_env = GeeseEnv()\n\ntrain_env = tf_py_environment.TFPyEnvironment(train_py_env)\neval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n\nfc_layer_params = (1000,)\n\nq_net = q_network.QNetwork(\n    train_env.observation_spec(),\n    train_env.action_spec(),\n    fc_layer_params=fc_layer_params)\n\noptimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n\ntrain_step_counter = tf.Variable(0)\n\nagent = dqn_agent.DqnAgent(\n    train_env.time_step_spec(),\n    train_env.action_spec(),\n    q_network=q_net,\n    optimizer=optimizer,\n    td_errors_loss_fn=common.element_wise_squared_loss,\n    train_step_counter=train_step_counter)\n\nagent.initialize()\n\neval_policy = agent.policy\ncollect_policy = agent.collect_policy\n\nrandom_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n                                                train_env.action_spec())\n\n\ndef compute_avg_return(environment, policy, num_episodes=10):\n    total_return = 0.0\n    for _ in range(num_episodes):\n\n        time_step = environment.reset()\n        episode_return = 0.0\n\n        while not time_step.is_last():\n            action_step = policy.action(time_step)\n            time_step = environment.step(action_step.action)\n            episode_return += time_step.reward\n        total_return += episode_return\n\n    avg_return = total_return \/ num_episodes\n    return avg_return.numpy()[0]\n\n\ncompute_avg_return(eval_env, random_policy, num_eval_episodes)\n\nreplay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n    data_spec=agent.collect_data_spec,\n    batch_size=train_env.batch_size,\n    max_length=replay_buffer_max_length)\n\n\ndef collect_step(environment, policy, buffer):\n    time_step = environment.current_time_step()\n    action_step = policy.action(time_step)\n    next_time_step = environment.step(action_step.action)\n    traj = trajectory.from_transition(time_step, action_step, next_time_step)\n\n    # Add trajectory to the replay buffer\n    buffer.add_batch(traj)\n\n\ndef collect_data(env, policy, buffer, steps):\n    for _ in range(steps):\n        collect_step(env, policy, buffer)\n\n\ncollect_data(train_env, random_policy, replay_buffer, initial_collect_steps)\n\n# This loop is so common in RL, that we provide standard implementations.\n# For more details see the drivers module.\n# https:\/\/www.tensorflow.org\/agents\/api_docs\/python\/tf_agents\/drivers\n\n\ndataset = replay_buffer.as_dataset(\n    num_parallel_calls=3,\n    sample_batch_size=batch_size,\n    num_steps=2).prefetch(3)\n\niterator = iter(dataset)\n\nprint(iterator)\n\n# (Optional) Optimize by wrapping some of the code in a graph using TF function.\nagent.train = common.function(agent.train)\n\n# Reset the train step\nagent.train_step_counter.assign(0)\n\n# Evaluate the agent's policy once before training.\navg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\nreturns = [avg_return]\n\ntf_policy_saver = policy_saver.PolicySaver(agent.policy)\n\nfor _ in range(num_iterations):\n\n    # Collect a few steps using collect_policy and save to the replay buffer.\n    collect_data(train_env, agent.collect_policy, replay_buffer, collect_steps_per_iteration)\n\n    # Sample a batch of data from the buffer and update the agent's network.\n    experience, unused_info = next(iterator)\n    train_loss = agent.train(experience).loss\n\n    step = agent.train_step_counter.numpy()\n\n    if step % log_interval == 0:\n        print('step = {0}: loss = {1}'.format(step, train_loss))\n\n    if step % eval_interval == 0:\n        avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n        print('step = {0}: Average Return = {1}'.format(step, avg_return))\n        returns.append(avg_return)\n        tf_policy_saver.save('sub\/current_policy')\n","6a1c570b":"saved_policy = tf.compat.v2.saved_model.load('sub\/current_policy')","7298f0e6":"from tf_agents.trajectories import time_step as ts\n\nblank_board = np.zeros([1,1,77], dtype=np.int32)\nprint(blank_board.shape)\nstep_type = tf.convert_to_tensor(\n    [0], dtype=tf.int32, name='step_type')\nreward = tf.convert_to_tensor(\n    [0], dtype=tf.float32, name='reward')\ndiscount = tf.convert_to_tensor(\n    [1], dtype=tf.float32, name='discount')\nobservations = tf.convert_to_tensor(\n    [blank_board], dtype=tf.int32, name='observations')\ntimestep = ts.TimeStep(step_type, reward, discount, observations)\n\ntime_step = None\naction_step = saved_policy.action(timestep)\nprint(action_step)\n","83a0344f":"%%writefile sub\/main.py\n\n!pip install tf-agents\n\nfrom abc import ABC\nfrom random import choice\nimport tensorflow as tf\nimport numpy as np\n\nfrom tf_agents.environments import py_environment\nfrom tf_agents.specs import array_spec\nfrom tf_agents.trajectories import time_step as ts\nfrom tf_agents.agents.dqn import dqn_agent\nfrom tf_agents.drivers import dynamic_step_driver\nfrom tf_agents.environments import suite_gym\nfrom tf_agents.environments import tf_py_environment\nfrom tf_agents.eval import metric_utils\nfrom tf_agents.metrics import tf_metrics\nfrom tf_agents.networks import q_network\nfrom tf_agents.policies import random_tf_policy\nfrom tf_agents.policies import policy_saver\nfrom tf_agents.replay_buffers import tf_uniform_replay_buffer\nfrom tf_agents.trajectories import trajectory\nfrom tf_agents.utils import common\n\nfrom kaggle_environments import make\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import *\n\nimport base64\nimport imageio\nimport IPython\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL.Image\n\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n\nsaved_policy = tf.compat.v2.saved_model.load('sub\/current_policy')\n\ndef get_board(ob, co):\n    config = co\n    columns = config.columns\n    rows = config.rows\n\n    numeric_board = np.full([columns * rows], 10, dtype=int)\n\n    food_number = 5\n\n    for pos in ob.food:\n        numeric_board[pos] = food_number\n\n    for index, goose in enumerate(ob.geese):\n        for position in goose:\n            numeric_board[position] = index\n\n    return numeric_board\n\ndef agent(obs_dict, config_dict):\n    \"\"\"This agent always moves toward observation.food[0] but does not take advantage of board wrapping\"\"\"\n    this_board = np.array([[get_board(obs_dict, config_dict)]])\n    \n    step_type = tf.convert_to_tensor(\n        [0], dtype=tf.int32, name='step_type')\n    reward = tf.convert_to_tensor(\n        [0], dtype=tf.float32, name='reward')\n    discount = tf.convert_to_tensor(\n        [1], dtype=tf.float32, name='discount')\n    observations = tf.convert_to_tensor(\n        [this_board], dtype=tf.int32, name='observations')\n    timestep = ts.TimeStep(step_type, reward, discount, observations)\n\n    action = saved_policy.action(timestep)\n    \n    choices = ['NORTH', 'SOUTH', 'WEST', 'EAST']\n    \n    choice = choices[int(action.action)]\n    \n    print(f\"choice:{choice}\")\n    return choice","867a0c10":"from kaggle_environments import evaluate, make, utils\n\n# Setup a hungry_geese environment.\nenv = make(\"hungry_geese\", debug = True)\nenv.run([agent, \"random\"])\nenv.render(mode=\"ipython\", width=600, height=650)","47553482":"import tarfile\nimport os.path\n\ndef make_tarfile(output_filename, source_dir):\n    with tarfile.open(output_filename, \"w:gz\") as tar:\n        tar.add(source_dir, arcname=os.path.basename(source_dir))\n\nmake_tarfile('submission.tar.gz', '.\/sub\/')","32071ca9":"# Compress to submit","b348400a":"# Test the Policy with an empty state","334ef5ca":"# Build the training env","0176c23e":"# Save the Policy ","a8626c2d":"# Execute the Submission","f3d39e38":"# Build the Bot and Train\nFeel free to play with the hyper parms. All this code came from [here](https:\/\/www.tensorflow.org\/agents\/tutorials\/1_dqn_tutorial)","d97141c8":"# Smash that upvote button in you learned something new! ","d91ffba2":"# Final File","50267470":"# Imports"}}