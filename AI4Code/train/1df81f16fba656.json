{"cell_type":{"9c40ba16":"code","a3a3cffe":"code","c3ebcef7":"code","79bf2d2c":"code","accc281a":"code","3f7157d7":"code","f935c08c":"code","e2895ec9":"code","e686de28":"code","f881d477":"code","a35e131c":"code","26ac5e8a":"code","c1cf028a":"code","8e5d9d56":"code","2fa9bb28":"code","4ea54899":"code","7a6318e7":"code","3f27cdbb":"code","e484bcc3":"code","1f6431a3":"code","2d4b631e":"code","d2334611":"code","bd3d66c1":"code","aec52c3e":"code","fd23f67c":"code","ed9b312f":"code","e75e55bd":"markdown","861daad1":"markdown","7f767545":"markdown","d13ab90f":"markdown","19ef75b3":"markdown","75ce7012":"markdown","ddb0456b":"markdown","7e975800":"markdown","ee74c0f4":"markdown","37d828e6":"markdown","55582ad8":"markdown","6a854765":"markdown","81898775":"markdown","1fbfe82f":"markdown","240f39cf":"markdown"},"source":{"9c40ba16":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport nltk\nimport re\n\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\nstoplist = stopwords.words('english')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","a3a3cffe":"import requests\nimport json\n\nurl = \"http:\/\/paraphrase.org\/api\/en\/search\/\"\n\n\ndef get_synonyms(word):\n    results = []\n    querystring = {\"batchNumber\":\"0\",\"needsPOSList\":\"true\",\"q\":word}\n    headers = {\n        'cache-control': \"no-cache\",\n        'postman-token': \"2d3d31e7-b571-f4ae-d69b-8116ff64d752\"\n    }\n\n    response = requests.request(\"GET\", url, headers=headers, params=querystring)\n    response_js = response.json()\n    \n    res_count = response_js['hits']['found']\n    if res_count > 0:\n        res_count = min(3, res_count )\n        hits = response_js['hits']['hit'][:res_count]\n        results = [ hit['target'] for hit in hits]\n    return results","c3ebcef7":"# get_synonyms('so sick')  \n# get_synonyms('gutted') \n# get_synonyms('lovely')  ","79bf2d2c":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').dropna().reset_index(drop=True)\ntrain = train[train.sentiment != 'neutral'].dropna().reset_index(drop=True).head(100)","accc281a":"vader = SentimentIntensityAnalyzer()\n\ndef sentiment_scores(word):\n    score = vader.polarity_scores(word)\n    return score\n\ndef sort_by_len(lst): \n    lst.sort(key=len) \n    return lst ","3f7157d7":"def find_synonym( text, selected_text, sentiment):\n    \n    selected_texts = []\n    texts = []\n    \n    orig_words = selected_text.split()\n    words = [ word_tokenize(str(word)) for word in orig_words]\n    words = [ word[0] for word in words if len(word) > 0]\n\n    polar_words = []\n    if sentiment == 'positive':\n        polar_words = [ word for word in words if sentiment_scores(word)['pos'] > 0]\n    elif sentiment == 'negative':\n        polar_words = [ word for word in words if sentiment_scores(word)['neg'] > 0]\n\n    if len(polar_words) == 0:\n        b=orig_words[0]\n        b=re.sub(r'\\W+','',b).lower()\n        for word in orig_words:\n            if(len(word)>len(b)):\n                b=word\n        polar_words = [b]\n\n    polar_word = sort_by_len(polar_words)[-1]\n\n    try:        \n        similar_words = get_synonyms(polar_word) \n        \n        for similar in similar_words:\n            selected_texts.append(re.sub(polar_word,similar,selected_text))\n            texts.append(re.sub(polar_word,similar,text))\n        \n    except Exception as e:\n        print(e)\n        if texts == [] and selected_texts == []:\n            return ('','')\n            \n    return (texts,selected_texts)","f935c08c":"# generated = train.progress_apply(lambda x : find_synonym(x.text,x.selected_text, x.sentiment),axis=1)\n# x,y = list(map(list,zip(*generated.values.tolist())))\n\n# new_df=pd.DataFrame({\"textID\": train.textID.values,\"text\":x,\"sentiment\":train.sentiment.values,'selected_text':y})\n# new_df.to_csv('twitter_augmented.csv',index=False)\n# new_df.head()","e2895ec9":"aug_df = pd.read_csv(\"..\/input\/tse-augmented\/twitter_augmented.csv\")\naug_df.head(10)","e686de28":"import os\nimport pandas as pd\nfrom sklearn import model_selection\nimport tokenizers\nimport albumentations\nimport random\nimport transformers\nimport torch.nn as nn\nimport torch\nfrom ast import literal_eval\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\nfrom albumentations.core.transforms_interface import DualTransform, BasicTransform","f881d477":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","a35e131c":"seed_torch(seed=42)","26ac5e8a":"class config:\n    MAX_LEN = 168\n    TRAIN_BATCH_SIZE = 48\n    VALID_BATCH_SIZE = 16\n    EPOCHS = 4\n    BASE_PATH = \"..\/input\/tweet-sentiment-extraction\"\n    ROBERTA_PATH = '..\/input\/roberta-base'\n    TRAINING_FILE = 'train_folds.csv'\n    AUG_FILE = '..\/input\/tse-augmented\/twitter_augmented.csv'\n    device = torch.device('cuda')\n    \n    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n        vocab_file=f\"{ROBERTA_PATH}\/vocab.json\", \n        merges_file=f\"{ROBERTA_PATH}\/merges.txt\", \n        lowercase=True,\n        add_prefix_space=True\n    )","c1cf028a":"df = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/train.csv\")\ndf = df.dropna().reset_index(drop=True)\n\ndf[\"kfold\"] = -1\n\nkf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\nfor fold, (trn_, val_) in enumerate(kf.split(X=df, y=df.sentiment.values)):\n    print(len(trn_), len(val_))\n    df.loc[val_, 'kfold'] = fold\n\ndf.to_csv(\"train_folds.csv\", index=False)","8e5d9d56":"class NLPTransform(BasicTransform):\n    \"\"\" Transform for nlp task.\"\"\"\n\n    @property\n    def targets(self):\n        return {\"data\": self.apply}\n    \n    def update_params(self, params, **kwargs):\n        if hasattr(self, \"interpolation\"):\n            params[\"interpolation\"] = self.interpolation\n        if hasattr(self, \"fill_value\"):\n            params[\"fill_value\"] = self.fill_value\n        return params\n\n    def get_sentences(self, text, lang='en'):\n        return sent_tokenize(text, self.LANGS.get(lang, 'english'))\n\n\nclass AddSynonymTransform(NLPTransform):\n    \"\"\" Add random non toxic statement \"\"\"\n    def __init__(self, always_apply=False, p=0.5):\n        super(AddSynonymTransform, self).__init__(always_apply, p)\n\n\n    def apply(self, data, **params):\n        # text, lang = data\n        text, selected_text = data\n\n        rand_ind = random.randint(0,2)\n        text =  text[rand_ind] if len(text) > rand_ind else text[0]\n        selected_text =  selected_text[rand_ind] if len(selected_text) > rand_ind else selected_text[0]\n        return text, selected_text\n\n\ndef get_train_transforms():\n    return albumentations.Compose([\n        AddSynonymTransform(p=0.5),\n        # you can add more transforms here\n    ])","2fa9bb28":"\ndef process_data(tweet, selected_text, tweet_aug, selected_text_aug, sentiment, tokenizer, max_len, train_transforms):\n\n    if train_transforms and type(tweet_aug) != float and len(tweet_aug) > 0:\n        tweet_aug, selected_text_aug = train_transforms(data=(tweet_aug, selected_text_aug))['data']\n        if type(tweet_aug) != list:\n            tweet = tweet_aug\n            selected_text = selected_text_aug\n\n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n\n\n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n\n    input_ids = [0] + [sentiment_id[sentiment]]  + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4  +  tweet_offsets + [(0, 0)]\n    targets_start += 4\n    targets_end += 4\n\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([1] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        tweet_offsets = tweet_offsets + ([(0, 0)] * padding_length)\n    \n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }\n\n\nclass TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text, tweet_aug, selected_text_aug, train_transforms=None):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tweet_aug = tweet_aug\n        self.selected_text_aug = selected_text_aug\n        self.tokenizer = config.TOKENIZER\n        self.train_transforms = train_transforms\n        self.max_len = config.MAX_LEN\n\n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n\n        tweet_aug = self.tweet_aug[item] if len(self.tweet_aug) > item else []\n        selected_text_aug = self.selected_text_aug[item] if len(self.selected_text_aug) > item else []\n        \n\n        data = process_data(\n            self.tweet[item],\n            self.selected_text[item],\n            tweet_aug,\n            selected_text_aug,\n            self.sentiment[item],\n            self.tokenizer,\n            self.max_len,\n            self.train_transforms\n        )\n\n        return {\n            'ids': torch.tensor(data['ids'], dtype=torch.long),\n            'mask': torch.tensor(data['mask'], dtype=torch.long),\n            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long),\n            'targets_start': torch.tensor(data['targets_start'], dtype=torch.long),\n            'targets_end': torch.tensor(data['targets_end'], dtype=torch.long),\n            'orig_tweet': data['orig_tweet'],\n            'orig_selected': data['orig_selected'],\n            'sentiment': data['sentiment'],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n        }","4ea54899":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n\n    tk0 = tqdm(data_loader, total=len(data_loader))\n\n    for bi, d in enumerate(tk0):\n        ids = d['ids']\n        token_type_ids = d['token_type_ids']\n        mask = d['mask']\n        targets_start = d['targets_start']\n        targets_end = d['targets_end']\n        sentiment = d['sentiment']\n        orig_selected = d['orig_selected']\n        orig_tweet = d['orig_tweet']\n        targets_start = d['targets_start']\n        targets_end = d['targets_end']\n        offsets = d['offsets']\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        loss = utils.loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n        loss.backward()\n        optimizer.step()\n        if scheduler is not None:\n            scheduler.step()\n\n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n\n        jaccard_scores = []\n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            jaccard_score, _ = utils.calculate_jaccard_score(\n                original_tweet = tweet,\n                target_string=selected_tweet,\n                sentiment_val=tweet_sentiment,\n                idx_start = np.argmax(outputs_start[px,:]),\n                idx_end=np.argmax(outputs_end[px,:]),\n                offsets=offsets[px]\n            )\n            jaccard_scores.append(jaccard_score)\n        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n\n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d['ids']\n            token_type_ids = d['token_type_ids']\n            mask = d['mask']\n            targets_start = d['targets_start']\n            targets_end = d['targets_end']\n            sentiment = d['sentiment']\n            orig_selected = d['orig_selected']\n            orig_tweet = d['orig_tweet']\n            targets_start = d['targets_start']\n            targets_end = d['targets_end']\n            offsets = d['offsets'].numpy()\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n\n            outputs_start, outputs_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n            loss = utils.loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n\n            jaccard_scores = []\n            for px, tweet in enumerate(orig_tweet):\n                selected_tweet = orig_selected[px]\n                tweet_sentiment = sentiment[px]\n                jaccard_score, _ = utils.calculate_jaccard_score(\n                    original_tweet = tweet,\n                    target_string=selected_tweet,\n                    sentiment_val=tweet_sentiment,\n                    idx_start = np.argmax(outputs_start[px,:]),\n                    idx_end=np.argmax(outputs_end[px,:]),\n                    offsets=offsets[px]\n                )\n                jaccard_scores.append(jaccard_score)\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n    return jaccards.avg, losses.avg","7a6318e7":"class TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, model_config):\n        super(TweetModel, self).__init__(model_config)\n        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH, config=model_config)\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(768 , 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        seq, pooled = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )\n\n        out = self.drop_out(seq)\n        logits = self.l0(out)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","3f27cdbb":"class utils:\n    \n    def loss_fn(start_logits, end_logits, start_positions, end_positions):\n        loss_func = nn.CrossEntropyLoss()\n        start_loss = loss_func(start_logits, start_positions)\n        end_loss = loss_func(end_logits, end_positions)\n        total_loss = (start_loss + end_loss)\n        return total_loss\n\n\n\n    class AverageMeter():\n        \"\"\"Computes and stores the average and current value\"\"\"\n        def __init__(self):\n            self.reset()\n\n        def reset(self):\n            self.val = 0\n            self.avg = 0\n            self.sum = 0\n            self.count = 0\n\n        def update(self, val, n=1):\n            self.val = val\n            self.sum += val * n\n            self.count += n\n            self.avg = self.sum \/ self.count\n\n\n    def jaccard(str1, str2): \n        a = set(str1.lower().split()) \n        b = set(str2.lower().split())\n        c = a.intersection(b)\n        return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\n    def calculate_jaccard_score(\n        original_tweet, \n        target_string, \n        sentiment_val, \n        idx_start, \n        idx_end, \n        offsets,\n        verbose=False):\n\n        if idx_end < idx_start:\n            idx_end = idx_start\n\n        filtered_output  = \"\"\n        for ix in range(idx_start, idx_end + 1):\n            filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n            if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n                filtered_output += \" \"\n\n        if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n            filtered_output = original_tweet\n\n        jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n        return jac, filtered_output\n\n\n    class EarlyStopping:\n        def __init__(self, patience=7, mode=\"max\", delta=0.0003):\n            self.patience = patience\n            self.counter = 0\n            self.mode = mode\n            self.best_score = None\n            self.early_stop = False\n            self.delta = delta\n            if self.mode == \"min\":\n                self.val_score = np.Inf\n            else:\n                self.val_score = -np.Inf\n\n        def __call__(self, epoch_score, model, model_path):\n\n            if self.mode == \"min\":\n                score = -1.0 * epoch_score\n            else:\n                score = np.copy(epoch_score)\n\n            if self.best_score is None:\n                self.best_score = score\n                self.save_checkpoint(epoch_score, model, model_path)\n            elif score < self.best_score + self.delta:\n                self.counter += 1\n                print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n                if self.counter >= self.patience:\n                    self.early_stop = True\n            else:\n                self.best_score = score\n                self.save_checkpoint(epoch_score, model, model_path)\n                self.counter = 0\n            print(f'Best score till now {self.best_score}')\n\n        def save_checkpoint(self, epoch_score, model, model_path):\n            if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n                print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n                # print('')\n                torch.save(model.state_dict(), model_path)\n            self.val_score = epoch_score","e484bcc3":"def run(fold):\n\n    dfx = pd.read_csv(config.TRAINING_FILE)\n\n    aug_df = pd.read_csv(\n        config.AUG_FILE,\n        converters={\"text\": literal_eval, \"selected_text\": literal_eval}\n    )\n    aug_df.columns = ['textID', 'text_aug', 'sentiment', 'selected_text_aug']\n    dfx = dfx.merge(aug_df, how='left')\n\n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n    df_train = df_train.sample(frac=1).reset_index(drop=True)\n\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n\n\n    train_dataset = TweetDataset(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values,\n        tweet_aug=df_train.text_aug.values,\n        selected_text_aug=df_train.selected_text_aug.values,\n        train_transforms= get_train_transforms()\n    )\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        shuffle=True,\n        num_workers=0\n    )\n\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values,\n        tweet_aug=[],\n        selected_text_aug=[],\n        train_transforms = None\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        num_workers=0\n    )\n\n    device = config.device\n\n    model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)\n    model = TweetModel(model_config=model_config)\n\n    model.to(device)\n\n    num_train_steps = int(len(df_train) \/ config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n\n    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=1, \n        num_training_steps=num_train_steps\n    )\n\n    es = utils.EarlyStopping(patience=2, mode='max')\n    print('*'* 100)\n    print(f\"Training is Starting for fold={fold}\")\n    best_jac = 0\n\n    for epoch in range(config.EPOCHS):\n        train_fn(train_data_loader, model, optimizer, device, scheduler)\n        jaccard, loss = eval_fn(valid_data_loader, model, device)\n        print(f'Jaccard Score = {jaccard}')\n        print(f'Loss = {loss}')\n        es(jaccard, model, model_path=f'model_{fold}.bin')\n        if es.early_stop:\n            print('Early Stopping')\n            break\n        if jaccard > best_jac:\n            best_jac = jaccard\n","1f6431a3":"for i in range(0, 5):\n    run(i)","2d4b631e":"df_test = pd.read_csv(f\"{config.BASE_PATH}\/test.csv\")\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","d2334611":"df_test.head()","bd3d66c1":"model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH)\n\nENSEMBLES = [\n    {'model': TweetModel(model_config=model_config), 'state_dict': f\"model_0.bin\", 'weight': 1 }, # CV: ??\n    {'model': TweetModel(model_config=model_config), 'state_dict': f\"model_1.bin\", 'weight': 1}, # CV: ??\n    {'model': TweetModel(model_config=model_config), 'state_dict': f\"model_2.bin\", 'weight': 1}, # CV: ??\n    {'model': TweetModel(model_config=model_config), 'state_dict': f\"model_3.bin\", 'weight': 1}, # CV: ??\n    {'model': TweetModel(model_config=model_config), 'state_dict': f\"model_4.bin\", 'weight': 1}, # CV: ??\n]","aec52c3e":"models = []\nweights = []\n\nfor val in ENSEMBLES:\n    model = val['model']\n    model.to(config.device)\n    model.load_state_dict(torch.load(val['state_dict']))\n    model.eval()\n    models.append(model)\n    weights.append(val['weight'])","fd23f67c":"final_output = []\n\n\ntest_dataset = TweetDataset(\n    tweet=df_test.text.values,\n    sentiment=df_test.sentiment.values,\n    selected_text=df_test.selected_text.values, \n    tweet_aug=[],\n    selected_text_aug=[]\n)\n\ntest_data_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=config.VALID_BATCH_SIZE,\n    num_workers=0\n)\n\ndevice = config.device\n\n\nwith torch.no_grad():\n    tk0 = tqdm(test_data_loader, total=len(test_data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"].numpy()\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        outputs_start = []\n        outputs_end = []\n        \n        for index, model in enumerate(models):\n            output_start, output_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            outputs_start.append(output_start * weights[index]) \n            outputs_end.append(output_end * weights[index]) \n            \n        outputs_start = sum(outputs_start) \/ len(outputs_start) \n        outputs_end = sum(outputs_end) \/ len(outputs_end)\n    \n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        jaccard_scores = []\n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            _, output_sentence = utils.calculate_jaccard_score(\n                original_tweet=tweet,\n                target_string=selected_tweet,\n                sentiment_val=tweet_sentiment,\n                idx_start=np.argmax(outputs_start[px, :]),\n                idx_end=np.argmax(outputs_end[px, :]),\n                offsets=offsets[px]\n            )\n            \n            final_output.append(output_sentence)","ed9b312f":"sample = pd.read_csv(f\"{config.BASE_PATH}\/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = final_output\n\nsample.to_csv(\"submission.csv\", index=False)\nsample.head(10)","e75e55bd":"### 4.2 Creating NLP Augmentation pipeline similar to Albumentations in Deep Learning","861daad1":"### 4.1 create folds","7f767545":"# END NOTES\n\n\n**<span style=\"color:Red\">Please upvote this kernel if you like it . It motivates me to produce more quality content :)**  ","d13ab90f":"## 1. About this kernel\n\n* In this kernel i have shown how we can create more data by replacing polar words in selected_text with their synonyms.I have used SentimentIntensityAnalyzer from nltk to extract polar words and i have used http:\/\/paraphrase.org for phrase matching. For each polar word in selected_text i have created upto 3 variants.\n\n* The dataset can be found at https:\/\/www.kaggle.com\/rohitsingh9990\/tse-augmented\n\n\n## Whats New\n\n* In this version i am creating a training pipeline to show how we can use this augmented data for training.","19ef75b3":"## 2. Few samples:","75ce7012":"### 4.3 creating dataset","ddb0456b":"## 4. Creating Training Pipeline","7e975800":"### 4.3 train and eval functions","ee74c0f4":"> Note: **<span style=\"color:Red\">If you want to recreate the aug dataset from scratch, simply uncomment the below code and enable internet connection**  \n","37d828e6":"### 4.6 Training Function","55582ad8":"### 4.7 Inference","6a854765":"## 3. Reading Data","81898775":"### 4.4 Model","1fbfe82f":"## References:\n\n* https:\/\/www.kaggle.com\/abhishek\/bert-base-uncased-using-pytorch\n* https:\/\/www.kaggle.com\/shonenkov\/nlp-albumentations","240f39cf":"### 4.5 utils"}}