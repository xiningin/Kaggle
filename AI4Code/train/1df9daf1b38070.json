{"cell_type":{"17071eea":"code","8496e546":"code","78de99aa":"code","aa3de8b7":"code","8350ce2c":"code","9332b7a3":"code","e3200145":"code","d060df75":"code","ccc40a00":"code","a3e12231":"code","67612d5a":"code","6e48e62a":"code","23976c5b":"code","f82646cd":"code","a27589a1":"code","4d722ff4":"code","34c725d9":"code","fac7f7c2":"code","ede5f893":"code","b6417163":"code","f1ff95a4":"code","a8d4b96f":"code","ac21a745":"code","9446755a":"code","95fa936d":"code","3ed092c4":"code","e3f98c12":"code","18154361":"code","da838f99":"code","f1f1e9a1":"code","7792472f":"code","1ef4cef8":"code","291eec36":"code","31e66844":"code","dc4b05dd":"code","d55d2bf6":"code","f9de9563":"code","459f27bc":"code","ac7efcf7":"code","6d950e63":"code","9c2da665":"code","48867179":"code","4a2eb78f":"code","6cae6600":"code","0636e72f":"code","87c0d24e":"code","ae4478c7":"code","4e7ef006":"code","e8a79ae7":"code","775e459f":"code","88fe832f":"code","cf11f05a":"code","8a6e5533":"code","6131537b":"code","26ab8ecd":"code","a97d1ea4":"code","35977c1a":"code","dcfb13c9":"code","4087971e":"code","fdf0a7a6":"code","aa45823a":"code","f3840c40":"code","5cd581e3":"code","3ca74b12":"code","9fe4dfc9":"code","8362bbed":"code","8d1b85cc":"code","c9818291":"code","be1b2e42":"code","0ec57bc3":"code","6b4418c7":"code","b8a1359a":"code","fbf9cf92":"code","5c87d587":"code","eb3ea8b4":"code","8d74a789":"code","023838a8":"code","af0fcf98":"code","131b401d":"code","479d5d6e":"code","843bf284":"code","7de35dd5":"code","7a789650":"code","f95404d7":"code","95facf74":"code","cac586a7":"code","723d35ae":"code","93e12ee5":"code","c76fede1":"code","fc1445d5":"code","82a6b581":"code","06a559bf":"code","5cb4d245":"code","0f42c4db":"code","32347f17":"markdown","9e16f5fc":"markdown","df8c4e9f":"markdown","1159a6ba":"markdown","c722ca3c":"markdown","b2a37039":"markdown","f2545060":"markdown","76e0e6f7":"markdown","287d9cf1":"markdown","56c6c6a0":"markdown","fdccced6":"markdown","4ee41218":"markdown","916de88e":"markdown","4aa628a8":"markdown","58cf68c4":"markdown","c5d01d54":"markdown","fbd2a66c":"markdown","aa09e31e":"markdown","cc07922b":"markdown","d76dbb12":"markdown","91ce4077":"markdown","563dde38":"markdown","b77edbae":"markdown","0afd2379":"markdown","b4261b40":"markdown","e1052833":"markdown","d7127ce3":"markdown","da7caf2b":"markdown"},"source":{"17071eea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","8496e546":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","78de99aa":"test_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","aa3de8b7":"test_df.shape","8350ce2c":"train_df.head()","9332b7a3":"train_df.info()","e3200145":"import datetime\ncrt_yr = datetime.datetime.now().year","d060df75":"train_df['Building_Age'] =train_df['YearBuilt'].apply(lambda x:crt_yr-x)\ntrain_df['Garage_Age'] =train_df['GarageYrBlt'].apply(lambda x:crt_yr-x)\ntrain_df['Last_Remodelled']=train_df['YearRemodAdd'].apply(lambda x:crt_yr-x)\ntrain_df['Last_Sold']=train_df['YrSold'].apply(lambda x:crt_yr-x)\n","ccc40a00":"test_df['Building_Age'] =test_df['YearBuilt'].apply(lambda x:crt_yr-x)\ntest_df['Garage_Age'] =test_df['GarageYrBlt'].apply(lambda x:crt_yr-x)\ntest_df['Last_Remodelled']=test_df['YearRemodAdd'].apply(lambda x:crt_yr-x)\ntest_df['Last_Sold']=test_df['YrSold'].apply(lambda x:crt_yr-x)\n","a3e12231":"train_df.drop(['YearBuilt','GarageYrBlt','YearRemodAdd','YrSold'],axis=1, inplace=True)","67612d5a":"test_df.drop(['YearBuilt','GarageYrBlt','YearRemodAdd','YrSold'],axis=1, inplace=True)","6e48e62a":"train_df.info()","23976c5b":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\n\n\nrcParams['figure.figsize'] = 10,5\nplt.rcParams[\"patch.force_edgecolor\"] = True\nsns.set_style('darkgrid')","f82646cd":"plt.figure(figsize=(14,5))\nsns.heatmap(train_df.isnull(),cmap='magma', cbar=False, yticklabels=False)","a27589a1":"plt.figure(figsize=(14,5))\nsns.heatmap(test_df.isnull(),cmap='magma', cbar=False, yticklabels=False)","4d722ff4":"#Find columns having nan's more than certain % of data\n\ntrain_df_columns = train_df.columns\ntrain_df_columns_40_percent_plus_null = [col for col in train_df_columns if train_df[col].isna().sum() > round(0.4 * len(train_df)) ]\n","34c725d9":"train_df_columns_40_percent_plus_null","fac7f7c2":"test_df_columns = test_df.columns\ntest_data_columns_40_percent_plus_null = [col for col in test_df_columns if test_df[col].isna().sum() > round(0.4 * len(test_df)) ]\n","ede5f893":"test_data_columns_40_percent_plus_null","b6417163":"train_df.drop(train_df_columns_40_percent_plus_null, inplace=True, axis=1)\ntest_df.drop(test_data_columns_40_percent_plus_null, inplace=True, axis=1)","f1ff95a4":"plt.figure(figsize=(14,5))\nsns.heatmap(train_df.isnull(),cmap='magma', cbar=False, yticklabels=False)","a8d4b96f":"plt.figure(figsize=(14,5))\nsns.heatmap(test_df.isnull(),cmap='magma', cbar=False, yticklabels=False)","ac21a745":"'''Lot frontage is the front side of the area\nLot depth is the distance from the front to the rear boundary of a plot\nLot area is the product of lot depth and frontage\n\nLot frontage calculation depends on the shape of area, its the average of the frontages. Same is the case for lot depth.\n\nLot frontage = (Total frontages)\/ Total number of frontage\n'''\n","9446755a":"#Calculate average frontage value by Lot Shape, train data\nround(train_df.groupby('LotShape').mean()['LotFrontage'],2)","95fa936d":"#Calculate average frontage value by Lot Shape ,test data\nround(test_df.groupby('LotShape').mean()['LotFrontage'],2)","3ed092c4":"#Lets Create LotFrontage FIller function by lotshape and its average values\nimport math\n#For train Data\ndef lotFrontageFillerTrainDf(cols):\n    frontage = cols[0]\n    shape =cols[1]\n    \n    if math.isnan(frontage):\n        if shape == 'IR1':\n            return 76.09\n        elif shape == 'IR2':\n            return 76.5\n        elif shape == 'IR3':\n            return 138.43\n        else:\n            return 67.04\n    else:\n        return frontage\n    \n# Test Data\ndef lotFrontageFillerTestDf(cols):\n    frontage = cols[0]\n    shape =cols[1]\n    \n    if math.isnan(frontage):\n        if shape == 'IR1':\n            return 74.30\n        elif shape == 'IR2':\n            return 56.73\n        elif shape == 'IR3':\n            return 81.25\n        else:\n            return 66.66\n    else:\n        return frontage","e3f98c12":"#Apply lotFrontagefiller function to Both Train and Test\ntrain_df['LotFrontage']=train_df[['LotFrontage','LotShape']].apply(lotFrontageFillerTrainDf,axis=1)\ntest_df['LotFrontage']=test_df[['LotFrontage','LotShape']].apply(lotFrontageFillerTestDf,axis=1)","18154361":"def null_value_counter(df):\n    empty_cols =list(df.columns[df.isna().any()])\n    col_null_value_counts={}\n    for i in range(len(empty_cols)):\n        \n        #THe index of 1 is always True for this case\n        #Because 40%+ null value columns are removed already\n        #value_counts always provides return in descending order\n        #So, null = True have index 1\n        null_value_count = df[empty_cols[i]].isnull().value_counts().values[1]\n        col_null_value_counts[empty_cols[i]] = null_value_count\n       \n    print(col_null_value_counts)\n    return col_null_value_counts","da838f99":"test_df_col_with_null_values=null_value_counter(test_df)\n#train_df_col_with_null_values = null_value_counter(train_df)","f1f1e9a1":"train_df['GarageArea'].mean()","7792472f":"def null_value_filler(df,dict_of_null_columns):\n   \n    empty_columns =list(dict_of_null_columns.keys())\n    df_g = df[empty_columns].columns.to_series().groupby(df[empty_columns].dtypes).groups\n    df_columns_dtype_dict = {k.name: v for k, v in df_g.items()}\n    df_columns_dtype_dict_obj = list(df_columns_dtype_dict['object'])\n    df_columns_dtype_dict_float = list(df_columns_dtype_dict['float64'])\n\n    #print(test_df_columns_dtype_dict['float64'])\n    #print(test_df_columns_dtype_dict['object'])\n    \n    #Fill Object Values\n    for i in range(len(df_columns_dtype_dict_obj)):\n        df[df_columns_dtype_dict_obj[i]].fillna(df[df_columns_dtype_dict_obj[i]].mode()[0],inplace=True)\n        \n        \n    #Fill Float Values\n    for i in range(len(df_columns_dtype_dict_float)):\n        df[df_columns_dtype_dict_float[i]].fillna(df[df_columns_dtype_dict_float[i]].mean(),inplace=True)\n\n","1ef4cef8":"#Call the function for test_Df\nnull_value_filler(test_df,test_df_col_with_null_values)\n#null_value_filler(train_df,train_df_col_with_null_values)\n\n           \n        ","291eec36":"train_df.dropna(axis=0, inplace= True)","31e66844":"train_df_colByDtypes = dict(train_df.dtypes)\ntest_df_colByDtypes = dict(test_df.dtypes)","dc4b05dd":"train_df_g = train_df.columns.to_series().groupby(train_df.dtypes).groups\ntest_df_g = test_df.columns.to_series().groupby(test_df.dtypes).groups","d55d2bf6":"\ntrain_df_columns_dtype_dict = {k.name: v for k, v in train_df_g.items()}\ntest_df_columns_dtype_dict = {k.name: v for k, v in test_df_g.items()}","f9de9563":"train_df_columns_dtype_dict","459f27bc":"test_df_columns_dtype_dict","ac7efcf7":"test_df_columns_dtype_dict['object']","6d950e63":"train_df_obj_to_dummies = pd.get_dummies(data=train_df, columns=train_df_columns_dtype_dict['object'], drop_first=True)\ntest_df_obj_to_dummies = pd.get_dummies(data=test_df, columns=test_df_columns_dtype_dict['object'], drop_first=True)","9c2da665":"train_df_columns_dtype_dict['int64'][1:]","48867179":"train_df_columns_dtype_dict['int64'][1:]","4a2eb78f":"msSubClass_dict =  {20: 'Twenty', 30:'Thirty' ,40:'Forty' ,45: 'Forty-Five',50:'Fifty',60:'Sixty' ,70 :'Seventy' ,\n               75:'Sevety-Five' ,80 :'Eighty' ,85:'Eighty-Five' ,90:'Ninety' ,120:'One-Hundred-Twenty',150:'One-Hundred-Fifty',\n               160:'One-Hundred-Sixty', 180:'One-Hundred-Eighty',190:'One-Hundred-Ninety'}","6cae6600":"train_df['MSSubClass']= train_df['MSSubClass'].replace(msSubClass_dict)","0636e72f":"test_df['MSSubClass']= test_df['MSSubClass'].replace(msSubClass_dict)","87c0d24e":"overallQual_dict = {\n        10:'Very Excellent',\n       9:'Excellent',\n       8:'Very Good',\n       7:'Good',\n       6:'Above Average',\n       5:'Average',\n       4:'Below Average',\n       3:'Fair',\n       2:'Poor',\n       1:'Very Poor'}","ae4478c7":"train_df['OverallQual']= train_df['OverallQual'].replace(overallQual_dict)\ntrain_df['OverallCond']= train_df['OverallCond'].replace(overallQual_dict)","4e7ef006":"test_df['OverallQual']= test_df['OverallQual'].replace(overallQual_dict)\ntest_df['OverallCond']= test_df['OverallCond'].replace(overallQual_dict)","e8a79ae7":"train_df_ints_to_dummies = pd.get_dummies(data=train_df_obj_to_dummies, columns=['MSSubClass','OverallQual','OverallCond'], drop_first=True)","775e459f":"test_df_ints_to_dummies = pd.get_dummies(data=test_df_obj_to_dummies, columns=['MSSubClass','OverallQual','OverallCond'], drop_first=True)","88fe832f":"train_df_ints_to_dummies.columns[1:35]","cf11f05a":"test_df_ints_to_dummies.columns[1:34]","8a6e5533":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n#train_df[columns_dtype_dict['int64'][1:]] = scaler.fit_transform(train_df[columns_dtype_dict['int64'][1:]])\ntrain_df_ints_to_dummies.iloc[:,1:35] = scaler.fit_transform(train_df_ints_to_dummies.iloc[:,1:35])\ntest_df_ints_to_dummies.iloc[:,1:34] = scaler.fit_transform(test_df_ints_to_dummies.iloc[:,1:34] )\n","6131537b":"plt.figure(figsize=(10,10))\nsns.clustermap(train_df.corr(),cmap=\"magma\", lw=5, linecolor='black' )","26ab8ecd":"plt.figure(figsize=(10,10))\nsns.clustermap(test_df.corr(),cmap=\"magma\", lw=5, linecolor='black' )","a97d1ea4":"print(train_df_ints_to_dummies.shape,test_df_ints_to_dummies.shape )","35977c1a":"\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n","dcfb13c9":"# Add one extra column to train df  act as constant since stats model demands one\ntrain_df_ints_to_dummies_with_ones= np.append(arr=np.ones((1338,1)).astype(int), values=train_df_ints_to_dummies.iloc[:,1:],axis=1)\n\n# Add one extra column to test df act as constant since stats model demands one\ntest_df_ints_to_dummies_with_ones= np.append(arr=np.ones((1459,1)).astype(int), values=test_df_ints_to_dummies.iloc[:,1:],axis=1)","4087971e":"# Calculate Variance Infation Factor Train DF\ntrain_df_vif = [variance_inflation_factor(train_df_ints_to_dummies_with_ones, i) for i in range(train_df_ints_to_dummies_with_ones.shape[1])]\n# Create DF \ntrain_df_vif_to_df =pd.DataFrame(index=train_df_ints_to_dummies.iloc[:,1:].columns,data=train_df_vif[1:], columns=['Variance_inflation_factors'])\n","fdf0a7a6":"# Calculate Variance Infation Factor Test Df\ntest_df_vif = [variance_inflation_factor(test_df_ints_to_dummies_with_ones, i) for i in range(test_df_ints_to_dummies_with_ones.shape[1])]\n# Create DF \ntest_df_vif_to_df =pd.DataFrame(index=test_df_ints_to_dummies.iloc[:,1:].columns,data=test_df_vif[1:], columns=['Variance_inflation_factors'])\n","aa45823a":"#train_df_vif_grt_thn_five = list(train_df_vif_to_df[train_df_vif_to_df['Variance_inflation_factors']>5].index)\ntrain_df_vif_grt_eql_ten = list(train_df_vif_to_df[train_df_vif_to_df['Variance_inflation_factors'] >= 10].index)\n#Get Numeric columns from list of columns with value >=10\ntrain_df_vif_grt_eql_ten_nmrc_dtype = train_df_vif_grt_eql_ten[:10]","f3840c40":"train_df_vif_grt_eql_ten[:10]","5cd581e3":"train_df_vif_grt_eql_ten_nmrc_dtype[8]","3ca74b12":"#Remove the dependent calss from the list\ntrain_df_vif_grt_eql_ten_nmrc_dtype.pop(8)","9fe4dfc9":"\ntrain_df_ints_to_dummies[train_df_vif_grt_eql_ten_nmrc_dtype].head(2)","8362bbed":"#test_df_vif_grt_thn_five = list(test_df_vif_to_df[test_df_vif_to_df['Variance_inflation_factors']>5].index)\ntest_df_vif_grt_eql_ten = list(test_df_vif_to_df[test_df_vif_to_df['Variance_inflation_factors'] >= 10].index)\n#Get Numeric columns from list of columns with value >=10\ntest_df_vif_grt_eql_ten_nmrc_dtype = test_df_vif_grt_eql_ten[:9]","8d1b85cc":"test_df_vif_grt_eql_ten_nmrc_dtype","c9818291":"test_df_ints_to_dummies[test_df_vif_grt_eql_ten_nmrc_dtype].head(2)","be1b2e42":"#Drop all the numeric columns with vif >=10\ntrain_df_ints_to_dummies.drop(train_df_vif_grt_eql_ten_nmrc_dtype, axis=1,inplace=True)","0ec57bc3":"#Test df has no sale price columns so no nned to drop salepricr like in train df\ntest_df_ints_to_dummies.drop(test_df_vif_grt_eql_ten_nmrc_dtype, axis=1,inplace=True)","6b4418c7":"X=train_df_ints_to_dummies.loc[:, train_df_ints_to_dummies.columns != 'SalePrice']\nX = train_df_ints_to_dummies.iloc[:,1:]\ny=train_df_ints_to_dummies['SalePrice']","b8a1359a":"#info_highest_p_val_col = regr.pvalues[regr.pvalues == sorted(regr.pvalues,reverse=True)[0]] \n#col_index_with_highest_p_val= info_highest_p_val_col.index[0]\n#p_val =info_highest_p_val_col.values[0]\n\n#print(info_highest_p_val_col,'\\n', col_index_with_highest_p_val,'\\n', p_val)","fbf9cf92":"# Get  Initial Array\n# Fit Stats model\n# Compare highest P-value to Stats Model to significant level\n#If greater,  drop Col with highest p_value\n\n#This function gives uses np.array. So, it does not return Column Names\n\n'''def getOptFeatures(X_opt,y):\n    X_opt = X_opt\n    y = y\n    p_val = 1.0\n    \n    print(\"Init \",p_val)\n    while p_val > 0.05:\n        sm_regressor = sm.OLS(y,X_opt).fit()\n        print(\"X_opt shape = \",X_opt.shape)\n        #Get Info on col with highest p-val\n        info_highest_p_val_col = sm_regressor.pvalues[sm_regressor.pvalues == sorted(sm_regressor.pvalues,reverse=True)[0]] \n        print('Info on col with highest p_val: ', info_highest_p_val_col)\n        #Exclude const columns\n        if info_highest_p_val_col.index[0] == 'const':\n            print('Its a const')\n            X_opt = np.delete(X_opt,0, axis=1 )   \n            \n        else:\n            # COl with highest p-val\n            col_index_with_highest_p_val= int(info_highest_p_val_col.index[0][1:])\n            #Highest P-val\n            p_val =info_highest_p_val_col.values[0]         \n            X_opt = np.delete(X_opt,col_index_with_highest_p_val-1, axis=1 )      \n            print(\"Latest P-value :\",p_val)\n    return X_opt\n\n    '''","5c87d587":"import statsmodels.api as sm","eb3ea8b4":"# Add constant for stats model api\nX_opt =sm.add_constant(X)","8d74a789":"# Get  Initial Array\n# Fit Stats model\n# Compare highest P-value to Stats Model to significant level\n#If greater,  drop Col with highest p_value\n\n\ndef getOptFeatures(X_opt,y):\n    X_opt = X_opt\n    y = y\n    highest_p_val = 1.0\n    print(\"Init \",highest_p_val)\n    while highest_p_val > 0.05:\n        sm_regressor = sm.OLS(y,X_opt).fit()\n        print(\"X_opt shape = \",X_opt.shape)\n        \n        #Get Info column and p_val with highest p_val \n        info_highest_p_val_col = sm_regressor.pvalues[sm_regressor.pvalues == sorted(sm_regressor.pvalues,reverse=True)[0]] \n        col_with_highest_p_val= info_highest_p_val_col.index[0]\n        highest_p_val =info_highest_p_val_col.values[0]\n        if highest_p_val > 0.05:\n            print(col_with_highest_p_val, highest_p_val )\n            #Drop col with p_value higher than 0.05\n            X_opt.drop(col_with_highest_p_val,axis=1,inplace=True)\n       \n    return X_opt\n","023838a8":"final_optimized_X = getOptFeatures(X_opt,y)","af0fcf98":"final_optimized_X","131b401d":"final_X=final_optimized_X.loc[:,final_optimized_X.columns != 'SalePrice']\nfinal_train_df_y=final_optimized_X['SalePrice']","479d5d6e":"final_X","843bf284":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(final_X, final_train_df_y, test_size=0.3, random_state=42)","7de35dd5":"from sklearn.linear_model import LinearRegression","7a789650":"regressor = LinearRegression()","f95404d7":"regressor.fit(X_train,y_train)","95facf74":"y_pred = regressor.predict(X_test)","cac586a7":"y_pred.shape","723d35ae":"from sklearn.metrics import mean_squared_error","93e12ee5":"root_mean_sqr_err = np.sqrt(mean_squared_error(y_test,y_pred))","c76fede1":"print(root_mean_sqr_err)","fc1445d5":"final_cols_test_set = list(final_X.columns)\nfinal_cols_test_set","82a6b581":"final_test_df = test_df_ints_to_dummies[final_cols_test_set]","06a559bf":"test_data_y_pred = regressor.predict(final_test_df)","5cb4d245":"submission_df = pd.DataFrame({'Id':test_df_ints_to_dummies.iloc[:,0], 'SalePrice':test_data_y_pred})","0f42c4db":"submission_df.to_csv('My_Housing_Price_Comp_Submission.csv',index=False)","32347f17":"## Train  Data","9e16f5fc":" # Categories -> Dummies ","df8c4e9f":"Null Value Counter For  REmaining DF","1159a6ba":"## Test Data","c722ca3c":"# Integers -> Categories -> Dummies","b2a37039":"# Function to Replace Null Values By Dtypes\n## Replace all the null values with mode for 'obj' and mean with 'int' dtypes","f2545060":"The following columns are categorical values :['MSSubClass','OverallQual','OverallCond']","76e0e6f7":"# VIF Factors and Elimination","287d9cf1":"## OverAllQual, OverallCond -> Categories ","56c6c6a0":"# Splitting Optimized Dataset Data\n","fdccced6":"# Missing Data Visulalization ","4ee41218":"col_highest_p_val.index provides index <br \/>\ncol_highest_p_val.index[0] provides col in string for eg 'x1'<br \/>\ncol_highest_p_val.index[0][1:] provides integer part of string<br \/>\nint(col_highest_p_val.index[0][1:]) converts integer part to string<br \/>\n\n","916de88e":"Find get Name of columns with repsect to data types","4aa628a8":"From data_description.txt, columns \"OverallCond\" and \"OverallCond\" have same values. ","58cf68c4":"## For Test DF","c5d01d54":"# Create Extra Rows SMOTE","fbd2a66c":"## For Train DF","aa09e31e":"## LotFrontage By LotShape AVG","cc07922b":"# Scaling  Data","d76dbb12":"# Dealing With Missig Data\n","91ce4077":"Function to IterOver the p-values and keep dropping column until the highest p-vlaue is below sl","563dde38":"## MSSubClass -> Categories","b77edbae":"## Categories Ints ->Dummies","0afd2379":"# Backward Elimination\n","b4261b40":"## Remove the columns with vif >= 10","e1052833":" From the data_decription file the values of MSSubClass Column can be [ 20,  30\t,40\t,45,50,60\t,70\t,75\t,80\t,85\t,90\t,120,150,160, 180,190 ]","d7127ce3":"# Convert Time Features to Age","da7caf2b":"# Testing On Test Data"}}