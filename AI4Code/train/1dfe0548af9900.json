{"cell_type":{"a7bbb03f":"code","0fc81a4c":"code","acaeeabd":"code","c6616270":"code","b23b597a":"code","0b52d251":"code","4c309571":"code","7075b913":"code","c7237086":"code","c65ae0fb":"code","51799a2b":"code","1e37104c":"code","d59f05fc":"code","d8254e34":"code","91e30d90":"code","82fe5415":"code","3437f35b":"code","de2b0135":"code","ebffaf9c":"code","be653ee4":"code","2df7efd3":"code","200bb357":"code","974ef01b":"code","2a1c5005":"code","fde9391e":"code","2d3bb237":"code","ae5e3587":"code","54190af4":"code","4e5eec90":"code","8b8048d8":"code","b9787c13":"code","95bf824d":"code","bfd092f8":"code","c6e6aac7":"code","0e20fc19":"code","836825fa":"code","b2538d3d":"code","a98feb0d":"code","abebce1f":"code","acc64456":"code","9433ec86":"code","85160d9b":"code","9154774a":"code","2a626f6b":"code","0f67708e":"code","70d776b3":"code","c406adb4":"code","d07f5ed8":"code","0869e175":"code","a0e94f38":"code","aebcfdcb":"code","3ae4d9aa":"markdown","d5f85842":"markdown","e8e325f9":"markdown","dc6559a6":"markdown","231f6c95":"markdown","3d798944":"markdown","269f4fc1":"markdown","d6ac431c":"markdown","a9292aca":"markdown","1ce7b6f0":"markdown","2c4481ad":"markdown","f082c2f4":"markdown","cddb125c":"markdown","daca9c15":"markdown","0dfa0dc1":"markdown","da621009":"markdown","05f78a9b":"markdown","b53c01f0":"markdown","11d4bc49":"markdown","15892ec5":"markdown","9fa3b9a1":"markdown","08d94f44":"markdown","7cee86ea":"markdown","336d0d6c":"markdown","e1dfb26d":"markdown","0361b99d":"markdown","d2b0b2b8":"markdown","77b3c30f":"markdown","b6339994":"markdown","92602dab":"markdown","9a810c7d":"markdown","0c973f7a":"markdown","2514cf45":"markdown","04006e4b":"markdown"},"source":{"a7bbb03f":"from plotly.offline import init_notebook_mode, iplot_mpl, download_plotlyjs, plot, iplot\nimport plotly_express as px\nimport plotly.figure_factory as ff\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport warnings\nwarnings.filterwarnings('ignore')\ninit_notebook_mode(connected=True)\nimport pandas_profiling\nimport statsmodels.formula.api as sm\nimport missingno as msno\nfrom sklearn.preprocessing import LabelEncoder\nfrom statsmodels.compat import lzip\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\nfrom sklearn.metrics import silhouette_score, silhouette_samples","0fc81a4c":"#cargamos los datos\ndata=pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","acaeeabd":"df=data.copy()","c6616270":"df.info()","b23b597a":"pandas_profiling.ProfileReport(df)","0b52d251":"df.describe()","4c309571":"df.quality.value_counts(normalize=True)","7075b913":"fig = px.imshow(df.corr(),x=list(df.corr().columns),y=list(df.corr().columns),width=900, \n                height=700,title='Correlation Matrix', color_continuous_scale=px.colors.diverging.Tropic).update_layout( paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\nfig.show()","c7237086":"#We generate the atribute matriz without the target\nX=df.iloc[:,:-1]","c65ae0fb":"X.head()","51799a2b":"X = StandardScaler().fit_transform(X)","1e37104c":"#We iterate the inertia until k = 14\ninertia = []\nK = range(1,15)\nfor k in K:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(X)\n    inertia.append(kmeanModel.inertia_)","d59f05fc":"#create new df \ninertia = pd.DataFrame({'inertia':inertia})\ninertia.index=pd.RangeIndex(start=1, stop=15, step=1, dtype=None, copy=False, name=None)\ninertia['K']=inertia.index","d8254e34":"fig = px.line(inertia, x=\"K\", y=\"inertia\", title='Elbow plot K-clusters vs Inertia').update_layout( paper_bgcolor='rgb(243, 243, 243)',                                                                                             \n    plot_bgcolor='rgb(243, 243, 243)')\nfig.show()","91e30d90":"# # A list will collect the silhouette coefficients for each k\nsilhouette_coefficients = []\n\nfor k in range(2, 15):\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(X)\n    silhouette_coefficients.append(silhouette_score(X, labels=kmeanModel.labels_))","82fe5415":"#create new df \nsilhouette_coefficients = pd.DataFrame({'silhouette_coefficients':silhouette_coefficients})","3437f35b":"silhouette_coefficients.index=pd.RangeIndex(start=2, stop=15, step=1, dtype=None, copy=False, name=None)","de2b0135":"silhouette_coefficients['K']=silhouette_coefficients.index","ebffaf9c":"fig = px.line(silhouette_coefficients, x=\"K\", y=\"silhouette_coefficients\",\n              title='Silhouette Score per n=k clusters').update_layout( paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\nfig.show()","be653ee4":"k = 4\nkmeans = KMeans(n_clusters=k,random_state=30)\ny_pred = kmeans.fit_predict(X)","2df7efd3":"df['silhouette_samples']=silhouette_samples(X, labels=kmeans.labels_, metric='euclidean')\ndf['clusters']=y_pred.astype(str)","200bb357":"df['clusters']=y_pred.astype(str)\ndf1=df.groupby('clusters').aggregate(np.mean)","974ef01b":"#We transpose the data to have a better visualization of it.\ndf1.T","2a1c5005":"#standardize variables\ngc = StandardScaler().fit_transform(df1)","fde9391e":"gc=pd.DataFrame(data=gc, index=df1.index,columns=df1.columns)","2d3bb237":"gc.T.style.background_gradient(cmap='Reds')","ae5e3587":"gc=gc.T\ngc['atributes']=gc.index\ngc.columns=['cluster_0','cluster_1','cluster_2','cluster_3','atributes']","54190af4":"fig = px.scatter(gc, x='atributes',y=['cluster_0', 'cluster_1', 'cluster_2', 'cluster_3'],\n        title=\"Standarized mean values per atribute and cluster\",\n                 color_discrete_sequence=px.colors.qualitative.Pastel).update_traces(dict(marker_line_width=1,\n        marker_size=16,marker_line_color=\"black\",mode='markers+lines')).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)').add_shape(type='line', x0=-0.5,y0=0, x1=12.5,y1=0,line=dict(color='Black',\n    dash=\"dot\"),xref='x',yref='y')\n\nfig.show()","4e5eec90":"fig = px.bar(gc, x=\"atributes\", y=['cluster_0', 'cluster_1', 'cluster_2', 'cluster_3'],\n              title=\"Bar Plot - Standarized mean values per atribute and cluster\",\n              color_discrete_sequence=px.colors.qualitative.Pastel).update_layout( \n    paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)').add_shape(type='line', x0=-0.5, y0=0,x1=12.5,y1=0,\n                                                 line=dict(color='Black', dash=\"dot\"),xref='x',yref='y')\nfig.show()","8b8048d8":"df['index']=df.index.astype(str)","b9787c13":"fig = px.scatter(df, x='silhouette_samples',y='index', title=\"Silhouette samples per cluster\",\n                 color='clusters',color_discrete_sequence=px.colors.qualitative.Pastel).update_traces(dict(marker_line_width=1,marker_line_color=\"black\")).update_layout( paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.add_shape(type='line',\n                x0=0,\n                y0=0,\n                x1=0,\n                y1=1600,\n                line=dict(color='Red', dash=\"dot\"),\n                xref='x',\n                yref='y')\n\nfig.show()","95bf824d":"fig = px.box(df, x='silhouette_samples', title=\"Silouhette samples Box Plots per cluster\",\n                 color='clusters',orientation='h',\n             color_discrete_sequence=px.colors.qualitative.Pastel).update_traces(dict(marker_line_width=1,\n          marker_line_color=\"black\")).update_layout( paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","bfd092f8":"# We scale our Matrix (DataSet) First we transform our DF in a scaled Matrix (scaled by the \u03bc and \u03c3 of each columm) \ndf_=scale(X)","c6e6aac7":"# from sklearn we import PCA module and we fit our DataSet, we specify the number of PC and call the fit() method \nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\npca.fit(df_)","0e20fc19":"# Now we can transform our DataSet\ndf_2d = pca.transform(df_)","836825fa":"# We generate a new Dataframe with our two components as variables\ndf_2d = pd.DataFrame(df_2d)\ndf_2d.columns = ['PC1','PC2']","b2538d3d":"df_2d['clusters']=y_pred.astype('str')\ndf_2d['silhouette_samples']=df['silhouette_samples']","a98feb0d":"fig=px.scatter(df_2d, x='PC1',y='PC2',title=\"PC1 vs PC2 per cluster\",\n                    color='clusters',color_discrete_sequence=px.colors.qualitative.Pastel).update_traces(dict(marker_line_width=1, marker_line_color=\"black\")).update_layout( paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","abebce1f":"fig=px.scatter_3d(df_2d, x='PC1',y='PC2',z='silhouette_samples',title=\"PC1 vs PC2 vs Silohuette Score per cluster\",\n                    color='clusters',color_discrete_sequence=px.colors.qualitative.Pastel).update_traces(dict(marker_line_width=1, marker_line_color=\"black\")).update_layout( paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\nfig.show()","acc64456":"from sklearn.cluster import AgglomerativeClustering \n\nagglom = AgglomerativeClustering(n_clusters=5, linkage='average').fit(X)","9433ec86":"# A list will collect the silhouette coefficients for each k holds the silhouette coefficients for each k\nsilhouette_coefficients_ac = []\n\n# Notice you start at 2 clusters for silhouette coefficient\nfor k in range(2, 10):\n    agglom = AgglomerativeClustering(n_clusters=k, linkage='average').fit(X)\n    silhouette_coefficients_ac.append(silhouette_score(X, labels=agglom.labels_))","85160d9b":"#create new df \nsilhouette_coefficients_ac = pd.DataFrame({'silhouette_coefficients':silhouette_coefficients_ac})","9154774a":"silhouette_coefficients_ac.index=pd.RangeIndex(start=2, stop=10, step=1, dtype=None, copy=False, name=None)","2a626f6b":"silhouette_coefficients_ac['K']=silhouette_coefficients_ac.index","0f67708e":"fig = px.line(silhouette_coefficients_ac, x=\"K\", y=\"silhouette_coefficients\",\n              title='Silhouette Score per n=k clusters in  Agglomerative clustering ').update_layout( paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\n\nfig.show()","70d776b3":"agglom = AgglomerativeClustering(n_clusters=2, linkage='average').fit(X)\n\ndf_2d['clusters_ac'] = agglom.labels_.astype('str')","c406adb4":"fig = px.scatter(df_2d, x='PC1',y='PC2', title=\"PC1 vs PC2 scatter plot Agglomerative Clustering 2K\",\n                 color='clusters_ac',color_discrete_sequence=px.colors.qualitative.Pastel).update_traces(dict(marker_line_width=1,marker_line_color=\"black\")).update_layout(paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\nfig.show()","d07f5ed8":"from sklearn.cluster import MeanShift, estimate_bandwidth\n\n# The following bandwidth can be automatically detected using\nbandwidth = estimate_bandwidth(X, quantile=0.1)\nms = MeanShift(bandwidth).fit(X)\n\ndf_2d['clusters_ms'] = ms.labels_.astype('str')","0869e175":"fig = px.scatter(df_2d, x='PC1',y='PC2', title=\"PC1 vs PC2 scatter plot\",\n                 color='clusters_ms',color_discrete_sequence=px.colors.qualitative.Pastel).update_traces(dict(marker_line_width=1,marker_line_color=\"black\")).update_layout(paper_bgcolor='rgb(243, 243, 243)',\n    plot_bgcolor='rgb(243, 243, 243)')\nfig.show()","a0e94f38":"silhouette_score(X, labels=ms.labels_)","aebcfdcb":"len(df_2d['clusters_ms'].unique())","3ae4d9aa":"### Silhouette_score","d5f85842":"The silhouette coefficient is a metric of the cohesion and separation of the groups. It measures how well an observation is in its cluster based on two factors:\n\n     How close it is to the other observations in the cluster.\n     How far it is from the rest of the observations of the other clusters.\n\nThe silhouette coefficient values vary between -1 and 1.\n\n- A coefficient close to +1 means that the instance is within its own cluster and away from other groups.\n- A coefficient close to 0 means that it is close to another cluster.\n- A coefficient close to -1 means that the instance may have been assigned to the wrong cluster.\n\nIn the scikit-learn implementation, the silhouette coefficient is the mean of all the observations in a score.\n\nThe silhouette coefficient of each observation = $$ a (b - a) \/ max (a, b) $$\nwhere a is the mean distance to the other observations of the same cluster (that is, the mean intra-group distribution) and b is the mean distance to the closest cluster.","e8e325f9":"We standardize the variables.","dc6559a6":"We observe that the instances with the highest silouhette score belong to cluster 2 (Chloride and high sulfates) and the vast majority of negative values (outliers) belong to cluster 3. We can therefore determine that this last cluster has very poor cohesion.","231f6c95":"The algorithm appears to perform very poorly on this data set. It has generated 26 clusters! and it has a silhouette score of only 0.12; so we can determine that the clusters generated are hardly significant and cohesive.","3d798944":"## 1. DataSet Loading and Examination","269f4fc1":"In 3 dimensions (with the values of silhouette scores as the height dimension, we observe that of the values with the best relative silhouette score, almost all belong to cluster 0 and 1","d6ac431c":"## 7. Agglomerative hierarchical clustering","a9292aca":"- **Cluster 0 (High pH-low density):** It has the lowest level of density, fixed_acidity and citric_acid and high Ph and volatile_acidity In quality it is the second best valued cluster, being very close to the average.\n\n\n- **Cluster 1 (Low Dioxides-High Density and Alcohol):** Has the notoriously lower values of free_sulfur_dioxide and total_sulfur_dioxide. It has the highest level of alcohol, fixed acidity and density. In quality it is the best valued cluster. Standing at 1.65 standard deviation of the quality mean.\n\n\n- **Cluster 2 (Chloride and high sulfates):** It has the highest average values of citric_acid, chlorides and sulphates and the lowest pH. In quality it is the second worst valued. It is the most cohesive cluster, given its silhouette score.\n\n\n- **Cluster 3 (Dioxides and Re.Sugar Altos):** It has the notoriously higher values of residual_sugar and of free_sulfur_dioxide and total_sulfur_dioxide. In quality it is the worst valued.","1ce7b6f0":"Visualization with Principal Components shows us a relatively good separation of the clusters.\n\n- Cluster 2 (Chloride, citric acid and high sulfates) seems to have observations with values very close to cluster 1 (Low Dioxides-best valued).\n\n- All the highest values of PC2 are found in cluster 3, the worst valued.","2c4481ad":"## 6. Visualizaci\u00f3n de los clusteres con PCA","f082c2f4":"We can visualize the average standardized values per attribute for each of the clusters.","cddb125c":"We note that the DataSet has neither missing nor null values. All variables are numeric. The quality variable is an integer.","daca9c15":"The correlation matrix offers us a summary of the correlations between the variables.\n\nQuality has a negative correlation with all attributes, except alcohol, sulphates and citric acid.\n","0dfa0dc1":"- As we can examine, this visualization gives us much more information than inertia. It seems that the best option is k = 2, since it has the highest score. k = 4 would also be a good option, given its silhouette score. We will select K = 4, since it will give us a more interesting option to examine than not having only two clusters.\n\n\n- We observed that the scores of all the models are relatively poor, which means that the data set seems to have little idiosyncrasy to generate highly cohesive clusters.","da621009":"We observe that we have higher values of the Silhouette score than with K-means.\n\nThis time we will choose the highest value of k, k = 2 with a silhouette score 0.58.","05f78a9b":"## 5. Silhouette scores of the instances and clusters\n\nAnother metric worth analyzing is the distribution of silhouette samples by cluster. Thus, the higher the value of the silhouette scores, the more cohesion the cluster has compared to the other clusters (or less dispersion).\n\nIn general terms\n\n- 0.76-1.0: Strong cohesion.\n\n- 0.50-0.75: Moderate cohesion.\n\n- 0.25-0.49: Weak cohesion.\n\n- <0.25: Low Coeshion","b53c01f0":"Although we observe some outlier values with PCA classified to cluster 0, we do not obtain a clear separation of the clusters with PCA.","11d4bc49":"- We observe that, indeed, only cluster 2 (Chloride and high sulfates) has moderate cohesion.\n\n- The cohesion of the rest of the clusters is poor or very poor (the median of cluster 3 is 0.097).\n\n- We could completely rule out working this data set with clusters (since these are not very significant).","15892ec5":"With Pandas Profiling we can examine the distributions of the variables.\n\nWe observe that the vast majority of wines have ratings of 5 and 6.","9fa3b9a1":"Inertia falls rapidly when Kmeans reaches k = 5, but from there, it begins to decrease more slowly and its decrease becomes less significant. Having 4 or 5 clusters seems like a good compromise between having interpretable clusters and a relatively low level of inertia.","08d94f44":"## 2. K-means Clustering\n\nTo perform the clustering activity we select the DataSet with the attributes without the quality objective variable. Our clustering objective will be to generate relatively cohesive groups and, at the same time, very different from each other. The aim is to obtain groupings with high intragroup similarity and low intergroup similarity.","7cee86ea":"We will look at a clustering technique, which is Aglomerative Hierarchical Clustering.\n\nThis algorithm uses a bottom-up approach, each observation starts in its own group, and the pairs of groups are shuffled as one moves up the hierarchy. In general, mixtures and divisions are determined with a voracious algorithm.\n\nThe Agglomerative Clustering class will require two inputs:\n\n     - n_clusters: the number of clusters that will be formed, as well as the number of centroids that will be generated.\n     \n     - link: which link criteria to use. The linkage criterion determines what distance to use between observation sets. The algorithm will merge the cluster pairs that minimize this criterion. We will use average.","336d0d6c":"**Python has been used for the study of the DataSet. The Pandas library has been used for the manipulation and processing of data. The Poltly Express library has been used for data visualization. The activity has studied the nature of the variables, as well as the correlations between them. The possibility of generating significant clusters of the wine instances with K-means has been explored by examining the Elbow method and the mean of the Silhouette score. A basic interpretation of the generated clusters and several visualizations with PCA has been carried out. Given the values of the Silhouette score with K-means (and the other 2 algorithms), we can conclude that the DataSet Wine Quality is not very suitable for the generation of cohesive and highly significant clusters.**\n\nThis dataset is also available in the UCI machine learning repository, https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality.\n\nFor more information see [Cortez et al., 2009].\n\n## Attributes (based on physicochemical tests):\n\n - 1 - fixed acidity\n - 2 - volatile acidity\n - 3 - citric acid\n - 4 - residual sugar\n - 5 - chlorides\n - 6 - free sulfur dioxide\n - 7 - total sulfur dioxide\n - 8 - density\n - 9 - pH\n - 10 - sulphates\n - 11 - alcohol\n\n## Target (based on sensorial date):\n- 12 - quality (score between 0 and 10)","e1dfb26d":"We will standardize the grouping since this way we can better compare the variables that have different units and are measured at different scales.","0361b99d":"## 3 Optimal number of clusters k selection","d2b0b2b8":"In this section, we will examine the two most common methods for choosing the optimal number of clusters in K-means.\n\n      1) Elbow method\n      2) Silhouette_score\n\n   To perform the Elbow method (elbow, we execute n = k iterations of k-means, increasing the value of K in each iteration and register the inertia (or sum of errors to the square or distance of each point to its centroid). In the figure we observe that as k increases, the inertia decreases, this is because as more centroids are included, the distance from each point to its closest centroid decreases.\n\nWe have to find the optimal point where the curve of inertia begins to bend. We have to choose a value k making a reasonable balance between inertia and a number of suitable clusters.","77b3c30f":"## 4. Interpretation of clusters\n\nWe will execute a grouping that gives us the average values of each variable for each of the clusters in order to interpret their characteristics.","b6339994":"The maximum value of quality is 8. The minimum is 3. The average of the scores is 5.636023.","92602dab":"### Elbow Method","9a810c7d":"******\n- Wine DataSet Clustering\n- author:\"Xavier Martinez Bartra\"\n- date: \"November 2020\"\n******","0c973f7a":"## 8. MeanShift\n\nMeanShift clustering aims to discover clusters at a uniform density of samples. It is a centroid-based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to remove nearly duplicates to form the final set of centroids.\n\nThe algorithm automatically sets the number of clusters, rather than relying on a parameter bandwidth, which dictates the size of the region to search. This parameter can be configured manually, but can be estimated using the provided estimated bandwidth function, which is called if the bandwidth is not set.","2514cf45":"## 9. Conclusions\nExamining the silouhette scores gives a fairly good idea of the relative quality of each cluster using a specific clustering algorithm. Thus, the comparison of the silhouette scores (which is the average value of all the grouped objects) obtained from various clustering algorithms or obtained from the same grouping method but with a changing number of clusters K is commonly used to help decide which grouping provides more relevant data.","04006e4b":"The vast majority of wines have scores of 5 (42.59%) and 6, (39.90%)"}}