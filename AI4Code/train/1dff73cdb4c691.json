{"cell_type":{"fc41e6c7":"code","92f11baf":"code","2eb2722f":"code","77ecbc94":"code","80f45463":"code","274eccca":"code","19dc10b8":"code","2935413c":"code","2403de7f":"code","cdc3f225":"code","9ae6ab25":"code","e93513e4":"code","0a691ac5":"code","411af153":"code","41cb35bf":"code","0578c7b7":"code","88bdc089":"code","45abb8c7":"code","9a95a3d8":"code","a898c2f5":"code","65b9f9ed":"code","2b7070d5":"code","762a322e":"code","4597b2a3":"code","7b5e2221":"code","efb6c540":"markdown","f8aa0966":"markdown","b5f4c652":"markdown","8e988683":"markdown","b8d68bb7":"markdown","3471d61b":"markdown","e69cb471":"markdown","06aab5c6":"markdown","ff4078e4":"markdown","4e805d1e":"markdown"},"source":{"fc41e6c7":"#!pip install efficientnet_pytorch","92f11baf":"!pwd","2eb2722f":"import torch\nfrom torch import nn\nimport torchvision\nfrom torchvision import transforms\nfrom torch.utils.data.dataset import Dataset\nimport torch.nn.functional as F\nimport albumentations as albu\n\nimport glob\nfrom tqdm.notebook import tqdm\n\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nimport sklearn\n#from efficientnet_pytorch import EfficientNet\nimport pandas as pd\n\nimport cv2\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\n\n#modelName = 'efficientnet-b4'\n#modelSize = EfficientNet.get_image_size(modelName)\ntorch.manual_seed(42)","77ecbc94":"import pandas as pd\nimport glob\n\ndata = glob.glob(\"..\/input\/deweight-huaweimetal\/de_weight\/de-weight-train\/*\/*.jpg\")\n\nname2class = {}\nname = 6.5\nfor i in range(14):\n    name2class[str(name)] = i\n    name += 0.5\n    \nfileName = []\nlabel = []\nclassName = []\nfor fn in data:\n    fileName.append(fn.split(\"\/\")[-1].strip(\".jpg\"))\n    className.append(fn.split(\"\/\")[-2])\n    label.append(name2class[fn.split(\"\/\")[-2]])\nd = {\"fileName\":fileName, \"Class\":label,\"ClassName\":className}\ndf = pd.DataFrame(d)\n\ndf.to_csv('classes_de.csv')","80f45463":"#!unzip -q \/home\/ma-user\/work\/data.zip","274eccca":"df = pd.read_csv(\".\/classes_de.csv\")\n#df = pd.read_csv(\"\/home\/ma-user\/work\/classes.csv\")\nfnames = df[\"fileName\"].tolist()\nclassNames = df[\"ClassName\"].tolist()\nclasses = df[\"Class\"].tolist()","19dc10b8":"class MetalDataset(Dataset):\n    def __init__(self, fileNames, labels, training = True, preproccess = False):\n        #self.img_path = \"\/home\/ma-user\/work\/train\"\n        self.training = training\n        crop_size = 1104\n        if training:\n            self.tfms = albu.Compose([\n#                                         albu.Resize(256, 256), \n#                                         albu.RandomCrop(224, 224,p=0.2),\n#                                         albu.Resize(256, 256), \n#                                         albu.RandomResizedCrop(224, 224,scale=(0.7,1.00),ratio=(0.85, 1.1),p=0.2),  #\u6709\u6da8\u70b9\n                                        albu.CenterCrop(1104,1104),\n                                        albu.Resize(224, 224), \n                                        #albu.Resize(modelSize, modelSize),\n                                        #albu.ToGray(p = 0.2),\n                                        albu.HorizontalFlip(p=0.5),\n                                        albu.VerticalFlip(p=0.5),\n                                        albu.Transpose(p=0.2),\n                                        albu.ElasticTransform(alpha=2000,sigma=100,alpha_affine=1,p=0.2), #\u4e00\u822c\u6765\u8bf4\uff0calpha\u8d8a\u5c0f\uff0csigma\u8d8a\u5927\uff0c\u4ea7\u751f\u7684\u504f\u5dee\u8d8a\u5c0f\uff0c\u548c\u539f\u56fe\u8d8a\u63a5\u8fd1\n                                        albu.MotionBlur(blur_limit=5,p=0.1),\n                                        albu.Rotate(limit=(-180,180),p = 0.1, border_mode=cv2.BORDER_WRAP)],\n                         additional_targets={\n                                                'image': 'image',\n                                                'gray': 'image',}\n                        )      \n        else:\n            self.tfms = albu.Compose([\n                                        albu.CenterCrop(1104,1104),\n                                        albu.Resize(224, 224), \n                                        #albu.Resize(modelSize, modelSize), \n                                                           ], \n                            additional_targets={\n                                        'image': 'image',\n                                        'gray': 'image',}\n            )        \n            \n        self.fileNames = fileNames\n        self.labels = labels\n        self.name2class = {}\n        name = 6.5\n        for i in range(14):\n            self.name2class[name] = torch.tensor(i)\n            name += 0.5\n            \n    def __getitem__(self, index):\n        img = cv2.imread(\"..\/input\/deweight-huaweimetal\/de_weight\/de-weight-train\/{:}\/{:}.jpg\".format(self.labels[index],self.fileNames[index]))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        gray = cv2.cvtColor(cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), cv2.COLOR_GRAY2RGB)\n        transformed = self.tfms(image=img, gray=gray)\n        #transformed = self.tfms(image = img)\n        img = transformed[\"image\"]\n        gray = transformed[\"gray\"]\n        \n        img = Image.fromarray(img)   \n        gray = Image.fromarray(gray) \n        img = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])])(img)\n        gray = transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                 std=[0.5, 0.5, 0.5])])(gray)\n        img = torch.cat((img,gray),0)\n        return img, self.name2class[self.labels[index]]\n    \n    def __len__(self):\n        return len(self.fileNames)","2935413c":"MetalDataset(fnames, classNames)[0][0].shape","2403de7f":"def trainOneEpoche(model, loader, criterion, optimizer,reg = False):\n    model.train()\n    runningLoss = 0\n    i = 0\n    y_all = []\n    outputs_all = [] # For Acc\n    for imgs, labels in tqdm(loader):\n        imgs = imgs.to(device)\n        labels= labels.to(device)\n        optimizer.zero_grad()\n        \n        output = model(imgs).squeeze()\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        runningLoss += loss\n        i+=1\n        outputs_all.extend(output.cpu().detach().numpy())\n        y_all.extend(labels.cpu().detach().numpy())\n\n    acc = computeAccuracy(outputs_all, y_all, reg = reg)\n    return runningLoss\/i, acc\n\ndef evalOneEpoche(model, loader, criterion, reg = False):\n    model.eval()\n    runningLoss = 0\n    i = 0\n    \n    y_all = []\n    outputs_all = [] # For AuC\n    \n    with torch.no_grad():\n        \n        for imgs, labels in loader:\n            imgs = imgs.to(device)\n            labels= labels.to(device)\n            output = model(imgs).squeeze()\n            \n            loss = criterion(output, labels)\n            runningLoss += loss\n            i+=1\n            outputs_all.extend(output.cpu().detach().numpy())\n            y_all.extend(labels.cpu().detach().numpy())\n\n        acc = computeAccuracy(outputs_all, y_all, reg = reg)\n        return runningLoss\/i, acc","cdc3f225":"class MultiFocalLoss(nn.Module):\n    \"\"\"\n    This is a implementation of Focal Loss with smooth label cross entropy supported which is proposed in\n    'Focal Loss for Dense Object Detection. (https:\/\/arxiv.org\/abs\/1708.02002)'\n        Focal_Loss= -1*alpha*(1-pt)^gamma*log(pt)\n    :param num_class:\n    :param alpha: (tensor) 3D or 4D the scalar factor for this criterion\n    :param gamma: (float,double) gamma > 0 reduces the relative loss for well-classified examples (p>0.5) putting more\n                    focus on hard misclassified example\n    :param smooth: (float,double) smooth value when cross entropy\n    :param balance_index: (int) balance class index, should be specific when alpha is float\n    :param size_average: (bool, optional) By default, the losses are averaged over each loss element in the batch.\n    \"\"\"\n\n    def __init__(self, num_class, alpha=None, gamma=2, balance_index=-1, smooth=None, size_average=True):\n        super(MultiFocalLoss, self).__init__()\n        self.num_class = num_class\n        self.alpha = alpha\n        self.gamma = gamma\n        self.smooth = smooth\n        self.size_average = size_average\n\n        if self.alpha is None:\n            self.alpha = torch.ones(self.num_class, 1)\n        elif isinstance(self.alpha, (list, np.ndarray)):\n            assert len(self.alpha) == self.num_class\n            self.alpha = torch.FloatTensor(alpha).view(self.num_class, 1)\n            self.alpha = self.alpha \/ self.alpha.sum()\n        elif isinstance(self.alpha, float):\n            alpha = torch.ones(self.num_class, 1)\n            alpha = alpha * (1 - self.alpha)\n            alpha[balance_index] = self.alpha\n            self.alpha = alpha\n        else:\n            raise TypeError('Not support alpha type')\n\n        if self.smooth is not None:\n            if self.smooth < 0 or self.smooth > 1.0:\n                raise ValueError('smooth value should be in [0,1]')\n\n    def forward(self, input, target):\n        logit = F.softmax(input, dim=1)\n\n        if logit.dim() > 2:\n            # N,C,d1,d2 -> N,C,m (m=d1*d2*...)\n            logit = logit.view(logit.size(0), logit.size(1), -1)\n            logit = logit.permute(0, 2, 1).contiguous()\n            logit = logit.view(-1, logit.size(-1))\n        target = target.view(-1, 1)\n\n        # N = input.size(0)\n        # alpha = torch.ones(N, self.num_class)\n        # alpha = alpha * (1 - self.alpha)\n        # alpha = alpha.scatter_(1, target.long(), self.alpha)\n        epsilon = 1e-10\n        alpha = self.alpha\n        if alpha.device != input.device:\n            alpha = alpha.to(input.device)\n\n        idx = target.cpu().long()\n        one_hot_key = torch.FloatTensor(target.size(0), self.num_class).zero_()\n        one_hot_key = one_hot_key.scatter_(1, idx, 1)\n        if one_hot_key.device != logit.device:\n            one_hot_key = one_hot_key.to(logit.device)\n\n        if self.smooth:\n            one_hot_key = torch.clamp(\n                one_hot_key, self.smooth, 1.0 - self.smooth)\n        pt = (one_hot_key * logit).sum(1) + epsilon\n        logpt = pt.log()\n\n        gamma = self.gamma\n\n        alpha = alpha[idx]\n        loss = -1 * alpha * torch.pow((1 - pt), gamma) * logpt\n\n        if self.size_average:\n            loss = loss.mean()\n        else:\n            loss = loss.sum()\n        return loss","9ae6ab25":"class betterLoss(nn.Module):\n    ## criterion for True label, CE loss for fuzzy label\n    def __init__(self, criterion =  nn.CrossEntropyLoss(), beta = 0.1):\n        super(betterLoss, self).__init__()\n        self.criterion = criterion\n        self.beta = beta\n    def forward(self, output, labels):\n        lossTrue = self.criterion(output,labels) * (1\/(1+self.beta))\n        labels_plus = labels.clone()\n        labels_minus = labels.clone()\n        for i in range(len(labels)):\n            if labels[i] > 0:\n                labels_minus[i]-=1\n            if labels[i]< 13:\n                labels_plus[i]+=1\n\n        lossFuzzy = (self.beta\/(1+self.beta))*(self.criterion(output,labels_plus) + self.criterion(output,labels_minus))\n        \n        return lossTrue + lossFuzzy","e93513e4":"def computeAccuracy(output,labels,reg = False):\n    alpha = 0.4\n    beta = 0.6\n    acc_0_classes = [1]*14\n    acc_0_5_classes = [1]*14\n    class_num = [1]*14\n    with torch.no_grad():\n        score_0 = 0\n        score_0_5 = 0\n        output = torch.tensor(output)\n        if reg:\n            pred = torch.round(output) #For regression task\n        else:\n            _, pred = output.topk(1)\n        score = 0\n        for o,l in zip(pred,labels):\n            l = int(l)\n            o = max(0,min(13,o))\n            class_num[l] += 1\n            if o == l:\n                score += 1\n                score_0 +=1\n                score_0_5 += 1\n                acc_0_classes[l] += 1\n            elif o == l+1 or o == l-1:\n                score += beta\n                score_0_5 += 1\n                acc_0_5_classes[l] += 1\n        acc_0_classes = [x\/y for x,y in zip(acc_0_classes,class_num)]\n        acc_0_5_classes = [x\/y for x,y in zip(acc_0_5_classes,class_num)]\n        \n        return score\/len(labels), score_0\/len(labels), score_0_5\/len(labels), acc_0_classes, acc_0_5_classes","0a691ac5":"class mergeModel(nn.Module):\n    def __init__(self, model1=None, model2=None):\n        super(mergeModel, self).__init__()\n        if model1:\n            self.model1 = model1\n        else:\n            model = torchvision.models.__dict__['resnet152'](pretrained  = False)\n            channel_in = model.fc.in_features\n            model.fc = nn.Linear(channel_in, 14) #Classificiation\n            self.model1 = model\n        if model2:\n            self.model2 = model2\n        else:\n            model = torchvision.models.__dict__['resnet152'](pretrained  = False)\n            channel_in = model.fc.in_features\n            model.fc = nn.Linear(channel_in, 14) #Classificiation\n            self.model2 = model\n            \n        self.softmax = nn.Softmax(dim = 1)\n        \n    def forward(self, img):\n        o1 = self.model1(img[:,:3,:,:])\n        o2 = self.model2(img[:,3:,:,:])\n        o1 = self.softmax(o1)\n        o2 = self.softmax(o2)\n        out = 0.5*o1+0.5*o2\n        \n        return out","411af153":"batch_size = 64\ninitial_LR = 2e-4\ngamma = 0.8 #Exp scheduler gamma\n\nepoches = 70\nT_0 = 10\nT_mult = 2\nn_splits = 10\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nclass_weights= sklearn.utils.class_weight.compute_class_weight('balanced',np.unique(classes),np.array(classes))\n#class_weights=torch.tensor(class_weights,dtype=torch.float).to(device)\n\n#criterion = nn.CrossEntropyLoss(class_weights)\ncriterion = MultiFocalLoss(14, alpha=class_weights, gamma=5)\nprint(class_weights)\ncriterion = betterLoss(criterion,beta = 0.1)\n#criterion = nn.MSELoss()\n#eval_crit = nn.MSELoss()\neval_crit = nn.CrossEntropyLoss()\nuse_cv = True\npreproc = True\nreg = False\n\nskf = StratifiedKFold(n_splits=n_splits, shuffle = True, random_state = 21)","41cb35bf":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodelRGB = torch.load(\"..\/input\/metalmodels\/0.8501278772378519_trained_resnet152.pt\", map_location=device)\nmodelGRAY = torch.load(\"..\/input\/metalmodels\/0.8629156010230179_trained_effnet.pt\", map_location=device)\nmodel = mergeModel(modelRGB, modelGRAY)\n\ntorch.save(model,\"mergedModel.pt\")","0578c7b7":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = torch.load(\"..\/input\/metaltestmodels\/mergedModel.pt\", map_location=device)\ntorch.save(model.state_dict(),\"mergedModel.pt\")","88bdc089":"model = mergeModel()\nmodel.load_state_dict(torch.load(\"mergedModel.pt\"))","45abb8c7":"for i_train, i_val in skf.split(fnames, classes):\n    break","9a95a3d8":"    max_acc = 0.84\n    trainDataset = MetalDataset([fnames[i] for i in i_train],[classNames[i] for i in i_train],training = True)\n    valDataset = MetalDataset([fnames[i] for i in i_val],[classNames[i] for i in i_val],training = False)\n    trainLoader = torch.utils.data.DataLoader(trainDataset, batch_size = batch_size, shuffle=True, num_workers=4, pin_memory=True)\n    valLoader = torch.utils.data.DataLoader(valDataset, batch_size = batch_size, shuffle=False, num_workers=4, pin_memory=True)\n    \n    #\u6b64\u5904\u4e3a\u4e0d\u540c\u6a21\u578b\n    #model = torchvision.models.__dict__['resnet152'](pretrained  = True)\n    #channel_in = model.fc.in_features\n    #model.fc = nn.Linear(channel_in, 14) #Classificiation\n    #model.fc = nn.Linear(channel_in, 1) #Regression\n    modelRGB = torch.load(\"..\/input\/metalmodels\/0.8501278772378519_trained_resnet152.pt\")\n    modelGRAY = torch.load(\"..\/input\/metalmodels\/0.8629156010230179_trained_effnet.pt\")\n    model = mergeModel(modelRGB, modelGRAY)\n    #model = EfficientNet.from_pretrained(modelName)\n    #channel_in = model._fc.in_features\n    #model._fc = nn.Linear(channel_in, 14)\n    model.to(device)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=initial_LR)\n    #scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=gamma) # Learning rate scheduler\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = T_0, T_mult=T_mult, eta_min=initial_LR*0.1)\n    \n    TrainingLoss = []\n    ValLoss = []\n    ValAcc = []\n    ValAcc_0 = []\n    ValAcc_0_5 = []\n\n    TrainingLoss.append([])\n    ValLoss.append([])\n    ValAcc.append([])\n    ValAcc_0.append([])\n    ValAcc_0_5.append([])\n\n    for e in range(epoches):\n            #runningtrainLoss,(train_acc,train_acc_0,train_acc_0_5,train_acc_0_classes, train_acc_0_5_classes) = trainOneEpoche(model, trainLoader, criterion, optimizer,reg=reg)\n            runningvalLoss, (acc,acc_0,acc_0_5,acc_0_classes, acc_0_5_classes) = evalOneEpoche(model, valLoader, eval_crit,reg=reg)\n            print(\"Current LR: {:}\".format(scheduler.get_last_lr()))\n            #print(\"epoche {:} TrainingLoss: {:}, Accuracy: {:}, AccuracyTrue: {:}, Accuracy+-0.5: {:}\".format(e,round(float(runningtrainLoss),4), round(float(train_acc),4), round(float(train_acc_0),4), round(float(train_acc_0_5),4)))\n            print(\"--------- EvalLoss: {:}, Accuracy: {:}, AccuracyTrue: {:}, Accuracy+-0.5: {:}\".format(round(float(runningvalLoss),4),round(float(acc),4),round(float(acc_0),4),round(float(acc_0_5),4)))\n            print(acc_0_classes)\n            TrainingLoss[-1].append(runningtrainLoss)\n            ValLoss[-1].append(runningvalLoss)\n            ValAcc[-1].append(acc)\n            ValAcc_0[-1].append(acc_0)\n            ValAcc_0_5[-1].append(acc_0_5)\n            if acc > max_acc:\n                max_acc = acc\n                torch.save(model, \"{:}_trained_effnet.pt\".format(acc))\n            scheduler.step()\n","a898c2f5":"models = glob.glob(\".\/*.pt\")\n","65b9f9ed":"for i,(x,y,z,a,b) in enumerate(zip(TrainingLoss[0], ValLoss[0], ValAcc[0],ValAcc_0[0],ValAcc_0_5[0])):\n    print(\"epoches: {:}, TrainingLoss:{:}, EvalLoss: {:}, Accuracy: {:}, AccuracyTrue: {:}, Accuracy+-0.5: {:}\".format(i,round(float(x),6),round(float(y),4),round(float(z),4),round(float(a),4),round(float(b),4)))","2b7070d5":"t = 0\nfor trainingLoss in TrainingLoss[-1]:\n    plt.plot(list(range(epoches)), trainingLoss)\n    t+= trainingLoss[-1]\nprint(t\/len(TrainingLoss))\nplt.show()","762a322e":"t = 0\nfor valLoss in ValLoss:\n    plt.plot(list(range(epoches)), valLoss)\n    t+= valLoss[-1]\nprint(t\/len(TrainingLoss))\nplt.show()","4597b2a3":"#t = 0\n#for aucs in ValAuc:\n#    plt.plot(list(range(epoches)), aucs)\n#    t+= aucs[-1]\n#print(t\/len(ValAuc))\n#plt.show()","7b5e2221":"t = 0\nfor accs in ValAcc:\n    plt.plot(list(range(epoches)), accs)\n    t+= accs[-1]\nprint(t\/len(ValAcc))\nplt.show()","efb6c540":"***Train & Eval one epoche***","f8aa0966":"BetterLoss \u6a21\u62dfregression\uff0c\u628a\u5404\u4e2a\u76f8\u4f3c\u7c7b\u7684\u4fe1\u606f\u4e92\u901a \uff08\u5355\u7eaf\u8bad\u7ec3regression task\u7ed3\u679c\u4e0d\u5982\u5206\u7c7b\u597d ~-2.0% acc\uff09","b5f4c652":"***Do Cross Validation***","8e988683":"***Evaluate models***","b8d68bb7":"***Dataset***","3471d61b":"\u5c06\u4e24\u4e2a\u6a21\u578b\u7684\u7ed3\u679c\u5f52\u4e00\u540e 50% 50%\u76f8\u52a0 \u6211\u8bad\u7ec3\u4e86\u9ed1\u767d+\u5f69\u8272\u6a21\u578b","e69cb471":"***Custom Loss function***","06aab5c6":"***Params***","ff4078e4":"\u9884\u5904\u7406\uff1a - crop\u6b63\u65b9\u5f62\u4e4b\u540eresize (1376,1104) - > (1104,1104) - > (224,224)\n        - \u9ed1\u767d\u56fe\u7247normalize mean 0.5 \u6807\u51c6\u5dee 0.5(?)\n\nAug:   \u89c1dataset.tfms \n\n\u8bad\u7ec3\u9ed1\u767d\u548c\u5f69\u8272\u6a21\u578b\u65f6\u9700\u8981\u6539dataset\uff0c\u5efa\u8bae\u91cd\u65b0\u5199\u3002\n\u73b0\u5728dataset\u4e3a\u52a0\u8f7d\u5f69\u8272\u4e0e\u9ed1\u767d\u56fe\u7247\u540e cat\u5230\u4e00\u8d77 \uff08batch_size\uff0cchannel = 6\uff0c224,224\uff09","4e805d1e":"***Accuracy***"}}