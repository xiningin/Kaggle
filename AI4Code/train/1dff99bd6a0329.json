{"cell_type":{"f1d9e30d":"code","0286a5df":"code","bea354a5":"code","111369e7":"code","ae3f559f":"code","b2b8d562":"code","c69ec63a":"code","fdb8f617":"code","d832aac3":"code","557c62a8":"code","1b8d9e86":"code","b4471de4":"code","a089b6c3":"code","32223911":"code","9fb46934":"code","efc6106a":"code","49a45c32":"code","952fe9f1":"code","502bb466":"code","5b947992":"code","8ba4523a":"code","4d3ed0c6":"code","658b9ad9":"code","7b264394":"code","e7de6394":"code","bf0c9007":"code","9cb5d096":"code","79cb4240":"code","f13c92e0":"code","ed2a0f60":"code","11ea3c88":"code","d841773c":"code","5a61ccca":"code","c56fdd9e":"code","b08144d8":"code","37a8fe1c":"code","2150d1f7":"code","d52e4723":"code","4a731e2c":"code","0c7ef25f":"code","45906be1":"code","f386eeff":"code","de65960d":"code","e64b1a25":"code","f50cc6eb":"code","f21ca1c9":"code","507052ee":"code","a0a91dd7":"markdown","46803dbc":"markdown","e0858fdf":"markdown","e4435e25":"markdown","71fb2b16":"markdown","fa343685":"markdown","bda4dc51":"markdown","105de853":"markdown","6d3f5739":"markdown","01887b68":"markdown","99570c97":"markdown","eeb68f9e":"markdown","b03985df":"markdown","29a201dc":"markdown","3745636e":"markdown","231ac77d":"markdown","b6a9e2ec":"markdown","146e884d":"markdown","e1418d40":"markdown","731956fb":"markdown","ef71b5fd":"markdown","1a037064":"markdown","fcdcd378":"markdown","53e1bfc5":"markdown","127fe3a0":"markdown","c22bc049":"markdown","54df685f":"markdown","433d80d5":"markdown","bfce7f4e":"markdown","4dfac570":"markdown","3653d7c2":"markdown","33adb66a":"markdown","e96f12fe":"markdown","ab712826":"markdown","76fe862b":"markdown","d7a6fe2e":"markdown","901d5cbb":"markdown","12be815d":"markdown","b7710ed3":"markdown","9907a4f1":"markdown","32ab3bdb":"markdown","7f609e5d":"markdown","d07568ed":"markdown","9516d532":"markdown","61e7a52c":"markdown","8461d617":"markdown","919e442a":"markdown","d7ed360b":"markdown","941d7d7e":"markdown","9c06abf1":"markdown","d2a3328e":"markdown","257d6eb8":"markdown","4cc02a40":"markdown","e6cea2f9":"markdown","c37609f3":"markdown","aac6d0d0":"markdown","46ba95b2":"markdown","5138d8b5":"markdown","df110af0":"markdown","1507c51c":"markdown","58287fe4":"markdown","f6904732":"markdown","42b515d8":"markdown","e380fc67":"markdown","0faca147":"markdown","668bef1d":"markdown","ed52bba6":"markdown","ed5e2f9f":"markdown","52fd97e2":"markdown","ef9d5146":"markdown","97a11750":"markdown","ef82454d":"markdown","141cce14":"markdown","de83ee5f":"markdown","e563aff5":"markdown"},"source":{"f1d9e30d":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport pandas as pd\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport warnings\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import SGDClassifier\nimport plotly.offline as py\nimport plotly.graph_objs as go\nimport re\nfrom plotly.offline import init_notebook_mode, iplot\nfrom sklearn.model_selection import train_test_split\ninit_notebook_mode(connected=True)\nimport cufflinks as cf\ncf.go_offline()\nimport pickle\nimport gc\nimport lightgbm as lgb\nwarnings.filterwarnings('ignore')\n%matplotlib inline","0286a5df":"print('Reading the data....', end='')\napplication = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\nprint('done!!!')\nprint('The shape of data:',application.shape)\nprint('First 5 rows of data:')\napplication.head()","bea354a5":"count = application.isnull().sum().sort_values(ascending=False)\npercentage = ((application.isnull().sum()\/len(application)*100)).sort_values(ascending=False)\nmissing_application = pd.concat([count, percentage], axis=1, keys=['Count','Percentage'])\nprint('Count and percentage of missing values for top 20 columns:')\nmissing_application.head(20)","111369e7":"columns_without_id = [col for col in application.columns if col!='SK_ID_CURR']\n#Checking for duplicates in the data.\napplication[application.duplicated(subset = columns_without_id, keep=False)]\nprint('The no of duplicates in the data:',application[application.duplicated(subset = columns_without_id, keep=False)]\n      .shape[0])","ae3f559f":"cf.set_config_file(theme='polar')\ncontract_val = application['TARGET'].value_counts()\ncontract_df = pd.DataFrame({'labels': contract_val.index,\n                   'values': contract_val.values\n                  })\ncontract_df.iplot(kind='pie',labels='labels',values='values', title='Loan Repayment Metric', hole = 0.6)","b2b8d562":"application[application['AMT_INCOME_TOTAL'] < 2000000]['AMT_INCOME_TOTAL'].iplot(kind='histogram', bins=100,\n   xTitle = 'Total Income', yTitle ='Count of applicants',\n             title='Distribution of AMT_INCOME_TOTAL')","c69ec63a":"(application[application['AMT_INCOME_TOTAL'] > 1000000]['TARGET'].value_counts())\/len(application[application['AMT_INCOME_TOTAL'] > 1000000])*100","fdb8f617":"cf.set_config_file(theme='polar')\ncontract_val = application['NAME_CONTRACT_TYPE'].value_counts()\ncontract_df = pd.DataFrame({'labels': contract_val.index,\n                   'values': contract_val.values\n                  })\ncontract_df.iplot(kind='pie',labels='labels',values='values', title='Types of Loan', hole = 0.6)","d832aac3":"application['AMT_CREDIT'].iplot(kind='histogram', bins=100,\n            xTitle = 'Credit Amount',yTitle ='Count of applicants',\n            title='Distribution of AMT_CREDIT')","557c62a8":"np.log(application['AMT_CREDIT']).iplot(kind='histogram', bins=100,\n        xTitle = 'log(Credit Amount)',yTitle ='Count of applicants',\n        title='Distribution of log(AMT_CREDIT)')","1b8d9e86":"cf.set_config_file(theme='polar')\nsuite_val = (application['NAME_TYPE_SUITE'].value_counts()\/len(application))*100\nsuite_val.iplot(kind='bar', xTitle = 'Name of type of the Suite',\n             yTitle='Count of applicants in %',\n             title='Who accompanied client when applying for the  application in % ')","b4471de4":"suite_val = application['NAME_TYPE_SUITE'].value_counts()\nsuite_val_y0 = []\nsuite_val_y1 = []\nfor val in suite_val.index:\n    suite_val_y1.append(np.sum(application['TARGET'][application['NAME_TYPE_SUITE']==val] == 1))\n    suite_val_y0.append(np.sum(application['TARGET'][application['NAME_TYPE_SUITE']==val] == 0))\ndata = [go.Bar(x = suite_val.index, y = ((suite_val_y1 \/ suite_val.sum()) * 100), name='Yes' ),\n        go.Bar(x = suite_val.index, y = ((suite_val_y0 \/ suite_val.sum()) * 100), name='No' )]\nlayout = go.Layout(\n    title = \"Who accompanied client when applying for the  application in terms of loan is repayed or not in %\",\n    xaxis=dict(\n        title='Name of type of the Suite',\n       ),\n    yaxis=dict(\n        title='Count of applicants in %',\n        )\n)\nfig = go.Figure(data = data, layout=layout) \nfig.layout.template = 'plotly_dark'\npy.iplot(fig)","a089b6c3":"income_val = application['NAME_INCOME_TYPE'].value_counts()\nincome_val_y0 = []\nincome_val_y1 = []\nfor val in income_val.index:\n    income_val_y1.append(np.sum(application['TARGET'][application['NAME_INCOME_TYPE']==val] == 1))\n    income_val_y0.append(np.sum(application['TARGET'][application['NAME_INCOME_TYPE']==val] == 0))\ndata = [go.Bar(x = income_val.index, y = ((income_val_y1 \/ income_val.sum()) * 100), name='Yes' ),\n        go.Bar(x = income_val.index, y = ((income_val_y0 \/ income_val.sum()) * 100), name='No' )]\nlayout = go.Layout(\n    title = \"Income sources of Applicants in terms of loan is repayed or not  in %\",\n    xaxis=dict(\n        title='Income source',\n       ),\n    yaxis=dict(\n        title='Count of applicants in %',\n        )\n)\nfig = go.Figure(data = data, layout=layout) \nfig.layout.template = 'plotly_dark'\npy.iplot(fig)","32223911":"education_val = application['NAME_EDUCATION_TYPE'].value_counts()\neducation_val_y0 = []\neducation_val_y1 = []\nfor val in education_val.index:\n    education_val_y1.append(np.sum(application['TARGET'][application['NAME_EDUCATION_TYPE']==val] == 1))\n    education_val_y0.append(np.sum(application['TARGET'][application['NAME_EDUCATION_TYPE']==val] == 0))\ndata = [go.Bar(x = education_val.index, y = ((education_val_y1 \/ education_val.sum()) * 100), name='Yes' ),\n        go.Bar(x = education_val.index, y = ((education_val_y0 \/ education_val.sum()) * 100), name='No' )]\nlayout = go.Layout(\n    title = \"Education sources of Applicants in terms of loan is repayed or not  in %\",\n    xaxis=dict(\n        title='Education of Applicants',\n       ),\n    yaxis=dict(\n        title='Count of applicants in %',\n        )\n)\nfig = go.Figure(data = data, layout=layout) \nfig.layout.template = 'plotly_dark'\npy.iplot(fig)","9fb46934":"cf.set_config_file(theme='pearl')\n(application['DAYS_BIRTH']\/(-365)).iplot(kind='histogram', \n             xTitle = 'Age', bins=50,\n             yTitle='Count of type of applicants in %',\n             title='Distribution of Clients Age')","efc6106a":"cf.set_config_file(theme='pearl')\n(application['DAYS_EMPLOYED']).iplot(kind='histogram', \n             xTitle = 'Days',bins=50,\n             yTitle='Count of applicants in %',\n             title='Days before the application the person started current employment')","49a45c32":"error = application[application['DAYS_EMPLOYED'] == 365243]\nprint('The no of errors are :', len(error))\n(error['TARGET'].value_counts()\/len(error))*100","952fe9f1":"# Create an error flag column\napplication['DAYS_EMPLOYED_ERROR'] = application[\"DAYS_EMPLOYED\"] == 365243\n# Replace the error values with nan\napplication['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)","502bb466":"cf.set_config_file(theme='pearl')\n(application['DAYS_EMPLOYED']\/(-365)).iplot(kind='histogram', xTitle = 'Years of Employment',bins=50,\n             yTitle='Count of applicants in %',\n             title='Years before the application the person started current employment')","5b947992":"application[application['DAYS_EMPLOYED']>(-365*2)]['TARGET'].value_counts()\/sum(application['DAYS_EMPLOYED']>(-365*2))","8ba4523a":"# Flag to represent when Total income is greater than Credit\napplication['INCOME_GT_CREDIT_FLAG'] = application['AMT_INCOME_TOTAL'] > application['AMT_CREDIT']\n# Column to represent Credit Income Percent\napplication['CREDIT_INCOME_PERCENT'] = application['AMT_CREDIT'] \/ application['AMT_INCOME_TOTAL']\n# Column to represent Annuity Income percent\napplication['ANNUITY_INCOME_PERCENT'] = application['AMT_ANNUITY'] \/ application['AMT_INCOME_TOTAL']\n# Column to represent Credit Term\napplication['CREDIT_TERM'] = application['AMT_CREDIT'] \/ application['AMT_ANNUITY'] \n# Column to represent Days Employed percent in his life\napplication['DAYS_EMPLOYED_PERCENT'] = application['DAYS_EMPLOYED'] \/ application['DAYS_BIRTH']\nprint(application.shape)","4d3ed0c6":"print('Reading the data....', end='')\nbureau = pd.read_csv('..\/input\/home-credit-default-risk\/bureau.csv')\nprint('done!!!')\nprint('The shape of data:',bureau.shape)\nprint('First 5 rows of data:')\nbureau.head()","658b9ad9":"# Combining numerical features\ngrp = bureau.drop(['SK_ID_BUREAU'], axis = 1).groupby(by=['SK_ID_CURR']).mean().reset_index()\ngrp.columns = ['BUREAU_'+column if column !='SK_ID_CURR' else column for column in grp.columns]\napplication_bureau = application.merge(grp, on='SK_ID_CURR', how='left')\napplication_bureau.update(application_bureau[grp.columns].fillna(0))\n# Combining categorical features\nbureau_categorical = pd.get_dummies(bureau.select_dtypes('object'))\nbureau_categorical['SK_ID_CURR'] = bureau['SK_ID_CURR']\ngrp = bureau_categorical.groupby(by = ['SK_ID_CURR']).mean().reset_index()\ngrp.columns = ['BUREAU_'+column if column !='SK_ID_CURR' else column for column in grp.columns]\napplication_bureau = application_bureau.merge(grp, on='SK_ID_CURR', how='left')\napplication_bureau.update(application_bureau[grp.columns].fillna(0))\n# Shape of application and bureau data combined\nprint('The shape application and bureau data combined:',application_bureau.shape)","7b264394":"# Number of past loans per customer\ngrp = bureau.groupby(by = ['SK_ID_CURR'])['SK_ID_BUREAU'].count().reset_index().rename(columns = {'SK_ID_BUREAU': 'BUREAU_LOAN_COUNT'})\napplication_bureau = application_bureau.merge(grp, on='SK_ID_CURR', how='left')\napplication_bureau['BUREAU_LOAN_COUNT'] = application_bureau['BUREAU_LOAN_COUNT'].fillna(0)\n# Number of types of past loans per customer \ngrp = bureau[['SK_ID_CURR', 'CREDIT_TYPE']].groupby(by = ['SK_ID_CURR'])['CREDIT_TYPE'].nunique().reset_index().rename(columns={'CREDIT_TYPE': 'BUREAU_LOAN_TYPES'})\napplication_bureau = application_bureau.merge(grp, on='SK_ID_CURR', how='left')\napplication_bureau['BUREAU_LOAN_TYPES'] = application_bureau['BUREAU_LOAN_TYPES'].fillna(0)\n# Debt over credit ratio \nbureau['AMT_CREDIT_SUM'] = bureau['AMT_CREDIT_SUM'].fillna(0)\nbureau['AMT_CREDIT_SUM_DEBT'] = bureau['AMT_CREDIT_SUM_DEBT'].fillna(0)\ngrp1 = bureau[['SK_ID_CURR','AMT_CREDIT_SUM']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM': 'TOTAL_CREDIT_SUM'})\ngrp2 = bureau[['SK_ID_CURR','AMT_CREDIT_SUM_DEBT']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM_DEBT':'TOTAL_CREDIT_SUM_DEBT'})\ngrp1['DEBT_CREDIT_RATIO'] = grp2['TOTAL_CREDIT_SUM_DEBT']\/grp1['TOTAL_CREDIT_SUM']\ndel grp1['TOTAL_CREDIT_SUM']\napplication_bureau = application_bureau.merge(grp1, on='SK_ID_CURR', how='left')\napplication_bureau['DEBT_CREDIT_RATIO'] = application_bureau['DEBT_CREDIT_RATIO'].fillna(0)\napplication_bureau['DEBT_CREDIT_RATIO'].replace([np.inf, -np.inf], 0,inplace=True)\napplication_bureau['DEBT_CREDIT_RATIO'] = pd.to_numeric(application_bureau['DEBT_CREDIT_RATIO'], downcast='float')\n# Overdue over debt ratio\nbureau['AMT_CREDIT_SUM_OVERDUE'] = bureau['AMT_CREDIT_SUM_OVERDUE'].fillna(0)\nbureau['AMT_CREDIT_SUM_DEBT'] = bureau['AMT_CREDIT_SUM_DEBT'].fillna(0)\ngrp1 = bureau[['SK_ID_CURR','AMT_CREDIT_SUM_OVERDUE']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM_OVERDUE'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM_OVERDUE': 'TOTAL_CUSTOMER_OVERDUE'})\ngrp2 = bureau[['SK_ID_CURR','AMT_CREDIT_SUM_DEBT']].groupby(by=['SK_ID_CURR'])['AMT_CREDIT_SUM_DEBT'].sum().reset_index().rename(columns={'AMT_CREDIT_SUM_DEBT':'TOTAL_CUSTOMER_DEBT'})\ngrp1['OVERDUE_DEBT_RATIO'] = grp1['TOTAL_CUSTOMER_OVERDUE']\/grp2['TOTAL_CUSTOMER_DEBT']\ndel grp1['TOTAL_CUSTOMER_OVERDUE']\napplication_bureau = application_bureau.merge(grp1, on='SK_ID_CURR', how='left')\napplication_bureau['OVERDUE_DEBT_RATIO'] = application_bureau['OVERDUE_DEBT_RATIO'].fillna(0)\napplication_bureau['OVERDUE_DEBT_RATIO'].replace([np.inf, -np.inf], 0,inplace=True)\napplication_bureau['OVERDUE_DEBT_RATIO'] = pd.to_numeric(application_bureau['OVERDUE_DEBT_RATIO'], downcast='float')","e7de6394":"print('Reading the data....', end='')\nprevious_applicaton = pd.read_csv('..\/input\/home-credit-default-risk\/previous_application.csv')\nprint('done!!!')\nprint('The shape of data:',previous_applicaton.shape)\nprint('First 5 rows of data:')\nprevious_applicaton.head()","bf0c9007":"# Number of previous applications per customer\ngrp = previous_applicaton[['SK_ID_CURR','SK_ID_PREV']].groupby(by=['SK_ID_CURR'])['SK_ID_PREV'].count().reset_index().rename(columns={'SK_ID_PREV':'PREV_APP_COUNT'})\napplication_bureau_prev = application_bureau.merge(grp, on =['SK_ID_CURR'], how = 'left')\napplication_bureau_prev['PREV_APP_COUNT'] = application_bureau_prev['PREV_APP_COUNT'].fillna(0)\n# Combining numerical features\ngrp = previous_applicaton.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\nprev_columns = ['PREV_'+column if column != 'SK_ID_CURR' else column for column in grp.columns ]\ngrp.columns = prev_columns\napplication_bureau_prev = application_bureau_prev.merge(grp, on =['SK_ID_CURR'], how = 'left')\napplication_bureau_prev.update(application_bureau_prev[grp.columns].fillna(0))\n# Combining categorical features\nprev_categorical = pd.get_dummies(previous_applicaton.select_dtypes('object'))\nprev_categorical['SK_ID_CURR'] = previous_applicaton['SK_ID_CURR']\nprev_categorical.head()\ngrp = prev_categorical.groupby('SK_ID_CURR').mean().reset_index()\ngrp.columns = ['PREV_'+column if column != 'SK_ID_CURR' else column for column in grp.columns]\napplication_bureau_prev = application_bureau_prev.merge(grp, on=['SK_ID_CURR'], how='left')\napplication_bureau_prev.update(application_bureau_prev[grp.columns].fillna(0))","9cb5d096":"print('Reading the data....', end='')\npos_cash = pd.read_csv('..\/input\/home-credit-default-risk\/POS_CASH_balance.csv')\nprint('done!!!')\nprint('The shape of data:',pos_cash.shape)\nprint('First 5 rows of data:')\npos_cash.head()","79cb4240":"# Combining numerical features\ngrp = pos_cash.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\nprev_columns = ['POS_'+column if column != 'SK_ID_CURR' else column for column in grp.columns ]\ngrp.columns = prev_columns\napplication_bureau_prev = application_bureau_prev.merge(grp, on =['SK_ID_CURR'], how = 'left')\napplication_bureau_prev.update(application_bureau_prev[grp.columns].fillna(0))\n# Combining categorical features\npos_cash_categorical = pd.get_dummies(pos_cash.select_dtypes('object'))\npos_cash_categorical['SK_ID_CURR'] = pos_cash['SK_ID_CURR']\ngrp = pos_cash_categorical.groupby('SK_ID_CURR').mean().reset_index()\ngrp.columns = ['POS_'+column if column != 'SK_ID_CURR' else column for column in grp.columns]\napplication_bureau_prev = application_bureau_prev.merge(grp, on=['SK_ID_CURR'], how='left')\napplication_bureau_prev.update(application_bureau_prev[grp.columns].fillna(0))","f13c92e0":"print('Reading the data....', end='')\ninsta_payments = pd.read_csv('..\/input\/home-credit-default-risk\/installments_payments.csv')\nprint('done!!!')\nprint('The shape of data:',insta_payments.shape)\nprint('First 5 rows of data:')\ninsta_payments.head()","ed2a0f60":"# Combining numerical features and there are no categorical features in this dataset\ngrp = insta_payments.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\nprev_columns = ['INSTA_'+column if column != 'SK_ID_CURR' else column for column in grp.columns ]\ngrp.columns = prev_columns\napplication_bureau_prev = application_bureau_prev.merge(grp, on =['SK_ID_CURR'], how = 'left')\napplication_bureau_prev.update(application_bureau_prev[grp.columns].fillna(0))","11ea3c88":"print('Reading the data....', end='')\ncredit_card = pd.read_csv('..\/input\/home-credit-default-risk\/credit_card_balance.csv')\nprint('done!!!')\nprint('The shape of data:',credit_card.shape)\nprint('First 5 rows of data:')\ncredit_card.head()","d841773c":"# Combining numerical features\ngrp = credit_card.drop('SK_ID_PREV', axis =1).groupby(by=['SK_ID_CURR']).mean().reset_index()\nprev_columns = ['CREDIT_'+column if column != 'SK_ID_CURR' else column for column in grp.columns ]\ngrp.columns = prev_columns\napplication_bureau_prev = application_bureau_prev.merge(grp, on =['SK_ID_CURR'], how = 'left')\napplication_bureau_prev.update(application_bureau_prev[grp.columns].fillna(0))\n# Combining categorical features\ncredit_categorical = pd.get_dummies(credit_card.select_dtypes('object'))\ncredit_categorical['SK_ID_CURR'] = credit_card['SK_ID_CURR']\ngrp = credit_categorical.groupby('SK_ID_CURR').mean().reset_index()\ngrp.columns = ['CREDIT_'+column if column != 'SK_ID_CURR' else column for column in grp.columns]\napplication_bureau_prev = application_bureau_prev.merge(grp, on=['SK_ID_CURR'], how='left')\napplication_bureau_prev.update(application_bureau_prev[grp.columns].fillna(0))","5a61ccca":"application_bureau_prev = application_bureau_prev.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\ny = application_bureau_prev.pop('TARGET').values\nX_train, X_temp, y_train, y_temp = train_test_split(application_bureau_prev.drop(['SK_ID_CURR'],axis=1), y, stratify = y, test_size=0.3, random_state=42)\nX_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, stratify = y_temp, test_size=0.5, random_state=42)\nprint('Shape of X_train:',X_train.shape)\nprint('Shape of X_val:',X_val.shape)\nprint('Shape of X_test:',X_test.shape)","c56fdd9e":"# Seperation of columns into numeric and categorical columns\ntypes = np.array([dt for dt in X_train.dtypes])\nall_columns = X_train.columns.values\nis_num = types != 'object'\nnum_cols = all_columns[is_num]\ncat_cols = all_columns[~is_num]\n# Featurization of numeric data\nimputer_num = SimpleImputer(strategy='median')\nX_train_num = imputer_num.fit_transform(X_train[num_cols])\nX_val_num = imputer_num.transform(X_val[num_cols])\nX_test_num = imputer_num.transform(X_test[num_cols])\nscaler_num = StandardScaler()\nX_train_num1 = scaler_num.fit_transform(X_train_num)\nX_val_num1 = scaler_num.transform(X_val_num)\nX_test_num1 = scaler_num.transform(X_test_num)\nX_train_num_final = pd.DataFrame(X_train_num1, columns=num_cols)\nX_val_num_final = pd.DataFrame(X_val_num1, columns=num_cols)\nX_test_num_final = pd.DataFrame(X_test_num1, columns=num_cols)\n# Featurization of categorical data\nimputer_cat = SimpleImputer(strategy='constant', fill_value='MISSING')\nX_train_cat = imputer_cat.fit_transform(X_train[cat_cols])\nX_val_cat = imputer_cat.transform(X_val[cat_cols])\nX_test_cat = imputer_cat.transform(X_test[cat_cols])\nX_train_cat1= pd.DataFrame(X_train_cat, columns=cat_cols)\nX_val_cat1= pd.DataFrame(X_val_cat, columns=cat_cols)\nX_test_cat1= pd.DataFrame(X_test_cat, columns=cat_cols)\nohe = OneHotEncoder(sparse=False,handle_unknown='ignore')\nX_train_cat2 = ohe.fit_transform(X_train_cat1)\nX_val_cat2 = ohe.transform(X_val_cat1)\nX_test_cat2 = ohe.transform(X_test_cat1)\ncat_cols_ohe = list(ohe.get_feature_names(input_features=cat_cols))\nX_train_cat_final = pd.DataFrame(X_train_cat2, columns = cat_cols_ohe)\nX_val_cat_final = pd.DataFrame(X_val_cat2, columns = cat_cols_ohe)\nX_test_cat_final = pd.DataFrame(X_test_cat2, columns = cat_cols_ohe)\n# Final complete data\nX_train_final = pd.concat([X_train_num_final,X_train_cat_final], axis = 1)\nX_val_final = pd.concat([X_val_num_final,X_val_cat_final], axis = 1)\nX_test_final = pd.concat([X_test_num_final,X_test_cat_final], axis = 1)\nprint(X_train_final.shape)\nprint(X_val_final.shape)\nprint(X_test_final.shape)","b08144d8":"# Saving the Dataframes into CSV files for future use\nX_train_final.to_csv('X_train_final.csv',index=False)\nX_val_final.to_csv('X_val_final.csv',index=False)\nX_test_final.to_csv('X_test_final.csv',index=False)","37a8fe1c":"model_sk = lgb.LGBMClassifier(boosting_type='gbdt', max_depth=7, learning_rate=0.01, n_estimators= 2000, \n                 class_weight='balanced', subsample=0.9, colsample_bytree= 0.8, n_jobs=-1)\nX_train_final=X_train_final.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\ntrain_features, valid_features, train_y, valid_y = train_test_split(X_train_final, y_train, test_size = 0.15, random_state = 42)\nmodel_sk.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], eval_metric = 'auc', verbose = 200)","2150d1f7":"feature_imp = pd.DataFrame(sorted(zip(model_sk.feature_importances_, X_train_final.columns)), columns=['Value','Feature'])\nfeatures_df = feature_imp.sort_values(by=\"Value\", ascending=False)\nselected_features = list(features_df[features_df['Value']>=50]['Feature'])\n# Saving the selected features into pickle file\nwith open('select_features.txt','wb') as fp:\n    pickle.dump(selected_features, fp)\nprint('The no. of features selected:',len(selected_features))","d52e4723":"# Feature importance Plot\ndata1 = features_df.head(20)\ndata = [go.Bar(x =data1.sort_values(by='Value')['Value'] , y = data1.sort_values(by='Value')['Feature'], orientation = 'h',\n              marker = dict(\n        color = 'rgba(43, 13, 150, 0.6)',\n        line = dict(\n            color = 'rgba(43, 13, 150, 1.0)',\n            width = 1.5)\n    )) ]\nlayout = go.Layout(\n    autosize=False,\n    width=1300,\n    height=700,\n    title = \"Top 20 important features\",\n    xaxis=dict(\n        title='Importance value'\n        ),\n    yaxis=dict(\n        automargin=True\n        ),\n    bargap=0.4\n    )\nfig = go.Figure(data = data, layout=layout)\nfig.layout.template = 'seaborn'\npy.iplot(fig)","4a731e2c":"def plot_confusion_matrix(test_y, predicted_y):\n    # Confusion matrix\n    C = confusion_matrix(test_y, predicted_y)\n    \n    # Recall matrix\n    A = (((C.T)\/(C.sum(axis=1))).T)\n    \n    # Precision matrix\n    B = (C\/C.sum(axis=0))\n    \n    plt.figure(figsize=(20,4))\n    \n    labels = ['Re-paid(0)','Not Re-paid(1)']\n    cmap=sns.light_palette(\"purple\")\n    plt.subplot(1,3,1)\n    sns.heatmap(C, annot=True, cmap=cmap,fmt=\"d\", xticklabels = labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Orignal Class')\n    plt.title('Confusion matrix')\n    \n    plt.subplot(1,3,2)\n    sns.heatmap(A, annot=True, cmap=cmap, xticklabels = labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Orignal Class')\n    plt.title('Recall matrix')\n    \n    plt.subplot(1,3,3)\n    sns.heatmap(B, annot=True, cmap=cmap, xticklabels = labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Orignal Class')\n    plt.title('Precision matrix')\n    \n    plt.show()\ndef cv_plot(alpha, cv_auc):\n    \n    fig, ax = plt.subplots()\n    ax.plot(np.log10(alpha), cv_auc,c='g')\n    for i, txt in enumerate(np.round(cv_auc,3)):\n        ax.annotate((alpha[i],str(txt)), (np.log10(alpha[i]),cv_auc[i]))\n    plt.grid()\n    plt.xticks(np.log10(alpha))\n    plt.title(\"Cross Validation Error for each alpha\")\n    plt.xlabel(\"Alpha i's\")\n    plt.ylabel(\"Error measure\")\n    plt.show()","0c7ef25f":"missing_features_from_X_val= ['NAME_EDUCATION_TYPE_Highereducation', 'NAME_CONTRACT_TYPE_Cashloans', 'NAME_EDUCATION_TYPE_Secondarysecondaryspecial', 'ORGANIZATION_TYPE_Selfemployed', 'OCCUPATION_TYPE_Corestaff', 'NAME_EDUCATION_TYPE_Academicdegree', 'NAME_INCOME_TYPE_Stateservant']\nselected_features_diff = [feature for feature in selected_features if feature not in missing_features_from_X_val ]\nalpha = np.logspace(-4,4,9)\ncv_auc_score = []\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l1',class_weight = 'balanced', loss='log', random_state=28)\n    clf.fit(X_train_final[selected_features_diff], y_train)\n    sig_clf = CalibratedClassifierCV(clf, method='sigmoid')\n    sig_clf.fit(X_train_final[selected_features_diff], y_train)\n    y_pred_prob = sig_clf.predict_proba(X_val_final[selected_features_diff])[:,1]\n    cv_auc_score.append(roc_auc_score(y_val,y_pred_prob))\n    print('For alpha {0}, cross validation AUC score {1}'.format(i,roc_auc_score(y_val,y_pred_prob)))\ncv_plot(alpha, cv_auc_score)\nprint('The Optimal C value is:', alpha[np.argmax(cv_auc_score)])","45906be1":"best_alpha = alpha[np.argmax(cv_auc_score)]\nlogreg = SGDClassifier(alpha = best_alpha, class_weight = 'balanced', penalty = 'l1', loss='log', random_state = 28)\nlogreg.fit(X_train_final[selected_features_diff], y_train)\nlogreg_sig_clf = CalibratedClassifierCV(logreg, method='sigmoid')\nlogreg_sig_clf.fit(X_train_final[selected_features_diff], y_train)\ny_pred_prob = logreg_sig_clf.predict_proba(X_train_final[selected_features_diff])[:,1]\nprint('For best alpha {0}, The Train AUC score is {1}'.format(best_alpha, roc_auc_score(y_train,y_pred_prob) ))    \ny_pred_prob = logreg_sig_clf.predict_proba(X_val_final[selected_features_diff])[:,1]\nprint('For best alpha {0}, The Cross validated AUC score is {1}'.format(best_alpha, roc_auc_score(y_val,y_pred_prob) ))  \ny_pred_prob = logreg_sig_clf.predict_proba(X_test_final[selected_features_diff])[:,1]\nprint('For best alpha {0}, The Test AUC score is {1}'.format(best_alpha, roc_auc_score(y_test,y_pred_prob) ))\ny_pred = logreg.predict(X_test_final[selected_features_diff])\nprint('The test AUC score is :', roc_auc_score(y_test,y_pred_prob))\nprint('The percentage of misclassified points {:05.2f}% :'.format((1-accuracy_score(y_test, y_pred))*100))\nplot_confusion_matrix(y_test, y_pred)","f386eeff":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nauc = roc_auc_score(y_test,y_pred_prob)\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, marker='.')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.title('ROC curve', fontsize = 20)\nplt.xlabel('FPR', fontsize=15)\nplt.ylabel('TPR', fontsize=15)\nplt.grid()\nplt.legend([\"AUC=%.3f\"%auc])\nplt.show()","de65960d":"alpha = [200,500,1000,2000]\nmax_depth = [7, 10]\ncv_auc_score = []\nfor i in alpha:\n    for j in max_depth:\n        clf = RandomForestClassifier(n_estimators=i, criterion='gini', max_depth=j,class_weight='balanced',\n                                     random_state=42, n_jobs=-1)\n        clf.fit(X_train_final[selected_features_diff], y_train)\n        sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n        sig_clf.fit(X_train_final[selected_features_diff], y_train)\n        y_pred_prob = sig_clf.predict_proba(X_val_final[selected_features_diff])[:,1]\n        cv_auc_score.append(roc_auc_score(y_val,y_pred_prob))\n        print('For n_estimators {0}, max_depth {1} cross validation AUC score {2}'.\n              format(i,j,roc_auc_score(y_val,y_pred_prob)))","e64b1a25":"best_alpha = np.argmax(cv_auc_score)\nprint('The optimal values are: n_estimators {0}, max_depth {1} '.format(alpha[int(best_alpha\/2)],\n                                                                        max_depth[int(best_alpha%2)]))\nrf = RandomForestClassifier(n_estimators=alpha[int(best_alpha\/2)], criterion='gini', max_depth=max_depth[int(best_alpha%2)],\n                            class_weight='balanced', random_state=42, n_jobs=-1)\nrf.fit(X_train_final[selected_features_diff], y_train)\nrf_sig_clf = CalibratedClassifierCV(rf, method=\"sigmoid\")\nrf_sig_clf.fit(X_train_final[selected_features_diff], y_train)\ny_pred_prob = rf_sig_clf.predict_proba(X_train_final[selected_features_diff])[:,1]\nprint('For best n_estimators {0} best max_depth {1}, The Train AUC score is {2}'.format(alpha[int(best_alpha\/2)], \n                                                    max_depth[int(best_alpha%2)],roc_auc_score(y_train,y_pred_prob)))\ny_pred_prob = rf_sig_clf.predict_proba(X_val_final[selected_features_diff])[:,1]\nprint('For best n_estimators {0} best max_depth {1}, The Validation AUC score is {2}'.format(alpha[int(best_alpha\/2)],\n                                                            max_depth[int(best_alpha%2)],roc_auc_score(y_val,y_pred_prob)))\ny_pred_prob = rf_sig_clf.predict_proba(X_test_final[selected_features_diff])[:,1]\nprint('For best n_estimators {0} best max_depth {1}, The Test AUC score is {2}'.format(alpha[int(best_alpha\/2)],\n                                                        max_depth[int(best_alpha%2)],roc_auc_score(y_test,y_pred_prob)))\ny_pred = rf_sig_clf.predict(X_test_final[selected_features_diff])\nprint('The test AUC score is :', roc_auc_score(y_test,y_pred_prob))\nprint('The percentage of misclassified points {:05.2f}% :'.format((1-accuracy_score(y_test, y_pred))*100))\nplot_confusion_matrix(y_test, y_pred)","f50cc6eb":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nauc = roc_auc_score(y_test,y_pred_prob)\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, marker='.')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.title('ROC curve', fontsize = 20)\nplt.xlabel('FPR', fontsize=15)\nplt.ylabel('TPR', fontsize=15)\nplt.grid()\nplt.legend([\"AUC=%.3f\"%auc])\nplt.show()","f21ca1c9":"weight = np.ones((len(X_train_final),), dtype=int)\nfor i in range(len(X_train_final)):\n    if y_train[i]== 0:\n        weight[i]=1\n    else:\n        weight[i]=11\n\ntrain_data=lgb.Dataset(X_train_final[selected_features_diff], label = y_train, weight= weight )\nvalid_data=lgb.Dataset(X_val_final[selected_features_diff], label = y_val)\ncv_auc_score = []\nmax_depth = [3, 5, 7, 10]\nfor i in max_depth:\n    \n    params = {'boosting_type': 'gbdt',\n          'max_depth' : i,\n          'objective': 'binary',\n          'nthread': 5,\n          'num_leaves': 32,\n          'learning_rate': 0.05,\n          'max_bin': 512,\n          'subsample_for_bin': 200,\n          'subsample': 0.7,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.8,\n          'reg_alpha': 20,\n          'reg_lambda': 20,\n          'min_split_gain': 0.5,\n          'min_child_weight': 1,\n          'min_child_samples': 10,\n          'scale_pos_weight': 1,\n          'num_class' : 1,\n          'metric' : 'auc'\n          }\nlgbm = lgb.train(params,\n                 train_data,\n                 2500,\n                 valid_sets=valid_data,\n                 early_stopping_rounds= 100,\n                 verbose_eval= 10\n                 )\ny_pred_prob = lgbm.predict(X_val_final[selected_features_diff])\ncv_auc_score.append(roc_auc_score(y_val,y_pred_prob))\nprint('For  max_depth {0} and some other parameters, cross validation AUC score {1}'.format(i,roc_auc_score(y_val,y_pred_prob)))\nprint('The optimal  max_depth: ', max_depth[np.argmax(cv_auc_score)])\nparams = {'boosting_type': 'gbdt',\n          'max_depth' : max_depth[np.argmax(cv_auc_score)],\n          'objective': 'binary',\n          'nthread': 5,\n          'num_leaves': 32,\n          'learning_rate': 0.05,\n          'max_bin': 512,\n          'subsample_for_bin': 200,\n          'subsample': 0.7,\n          'subsample_freq': 1,\n          'colsample_bytree': 0.8,\n          'reg_alpha': 20,\n          'reg_lambda': 20,\n          'min_split_gain': 0.5,\n          'min_child_weight': 1,\n          'min_child_samples': 10,\n          'scale_pos_weight': 1,\n          'num_class' : 1,\n          'metric' : 'auc'\n          }\nlgbm = lgb.train(params,\n                 train_data,\n                 2500,\n                 valid_sets=valid_data,\n                 early_stopping_rounds= 100,\n                 verbose_eval= 10\n                 )\ny_pred_prob = lgbm.predict(X_train_final[selected_features_diff])\nprint('For best max_depth {0}, The Train AUC score is {1}'.format(max_depth[np.argmax(cv_auc_score)], \n                                                                  roc_auc_score(y_train,y_pred_prob) ))    \ny_pred_prob = lgbm.predict(X_val_final[selected_features_diff])\nprint('For best max_depth {0}, The Cross validated AUC score is {1}'.format(max_depth[np.argmax(cv_auc_score)], \n                                                                            roc_auc_score(y_val,y_pred_prob) ))  \ny_pred_prob = lgbm.predict(X_test_final[selected_features_diff])\nprint('For best max_depth {0}, The Test AUC score is {1}'.format(max_depth[np.argmax(cv_auc_score)], \n                                                                 roc_auc_score(y_test,y_pred_prob) ))\ny_pred = np.ones((len(X_test_final),), dtype=int)\nfor i in range(len(y_pred_prob)):\n    if y_pred_prob[i]<=0.5:\n        y_pred[i]=0\n    else:\n        y_pred[i]=1\nprint('The test AUC score is :', roc_auc_score(y_test,y_pred_prob))\nprint('The percentage of misclassified points {:05.2f}% :'.format((1-accuracy_score(y_test, y_pred))*100))\nplot_confusion_matrix(y_test, y_pred)","507052ee":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\nauc = roc_auc_score(y_test,y_pred_prob)\nplt.figure(figsize=(8,6))\nplt.plot(fpr, tpr, marker='.')\nplt.plot([0, 1], [0, 1], linestyle='--')\nplt.title('ROC curve', fontsize = 20)\nplt.xlabel('FPR', fontsize=15)\nplt.ylabel('TPR', fontsize=15)\nplt.grid()\nplt.legend([\"AUC=%.3f\"%auc])\nplt.show()","a0a91dd7":"*Observations:*\n\nMajority of people are willing to take cash loan than revolving loan\n\n* [What is revolving loan facility](https:\/\/www.investopedia.com\/terms\/r\/revolving-loan-facility.asp).\n\n* [What is cash basis loan](https:\/\/www.investopedia.com\/terms\/c\/cash-basis-loan.asp)","46803dbc":"Random Forest model results","e0858fdf":"<div id=\"feature\">\n     <h3>Data Preperation and Feature Engineering<\/h3>\n<\/div> ","e4435e25":"Cross validation results for Random Forest model.","71fb2b16":"Observations:\n1. The distribution is right skewed and there are extreme values, we can apply log distribution.\n2. People with high income(>1000000) are likely to repay the loan.","fa343685":"* Types of loan available","bda4dc51":"Cross validation results and plot for Logistic Regression model","105de853":"* Feature Engineering of Application data:","6d3f5739":"<div id=\"data\"> <\/div>\n<h3> Data Overview <\/h3>\nFor detailed information refer to: \n\n[Home Credit Data](https:\/\/www.kaggle.com\/c\/home-credit-default-risk\/data)\n\n![image.png](attachment:9e33d070-85f9-4088-9fe6-b988d6232ea7.png)\n","01887b68":"**Distribution of years before the application the person started current employment**","99570c97":"**Distribution of Income sources of Applicants in terms of loan is repayed or not**","eeb68f9e":"1. This dataset consists of 307511 rows and 122 columns.\n2. Each row has unique id \u2018SK_ID_CURR\u2019 and the output label is in the \u2018TARGET\u2019 column.\n3. TARGET indicating 0: the loan was repaid or 1: the loan was not repaid.\n4. The description of each column can be found in the file \u2018HomeCredit_columns_description.csv\u2019","b03985df":"**Using Credit card balance data:**","29a201dc":"* Checking distribution of data points across output class","3745636e":"ROC Curve for Logistic Regression with **AUC=0.759**","231ac77d":"LightGBM gives the best performance.","b6a9e2ec":"LightGBM gives the best performance and it is also faster to train when compared to Xgboost.","146e884d":"**Selection of features using LightGBM Classifier:**\n\nRefer: \n\n[LightGBM Documentation](https:\/\/lightgbm.readthedocs.io\/en\/latest\/)\n\n[Feature Selection in Python](https:\/\/towardsdatascience.com\/feature-selection-in-machine-learning-d5af31f7276)","e1418d40":"**Random Forest with selected features**","731956fb":"*Observations:*\n1. Among the applicant pool students and businessmen show 100% positive result in repaying loans","ef71b5fd":"# **Loan Repayment Prediction**","1a037064":"In this problem, the data is imbalanced. So we can\u2019t use accuracy as a error metric. When data is imbalanced we can use Log loss, F1-score and AUC. Here we are sticking to AUC which can handle imbalanced datasets.\n\n* **Area Under Curve (AUC)**\n\nAn ROC curve is the most commonly used way to visualize the performance of a binary classifier, and AUC is (arguably) the best way to summarize its performance in a single number.\n\n* **Confusion Matrix (To get an overview of complete predictions)**\n    \nFor more information, you can check out this video: [ROC and AUC Explained](https:\/\/youtu.be\/OAl6eAyP-yo)","fcdcd378":"### Dataset used in this kernel:\n\n[Home Credit](https:\/\/www.kaggle.com\/c\/home-credit-default-risk\/overview)\n \n In order to make sure this underserved population has a positive loan experience, Home Credit makes use of a variety of alternative data--including telco and transactional information--to predict their clients' repayment abilities.","53e1bfc5":"### Final Results:\n<table>\n  <tr>\n    <th>Model<\/th>\n    <th>Train AUC<\/th>\n    <th>Validation AUC<\/th>\n     <th>Test AUC<\/th>\n  <\/tr>\n  <tr>\n    <td>Logistic Regression with Selected Features<\/td>\n    <td>0.762<\/td>\n    <td>0.755<\/td>\n    <td>0.759<\/td>\n  <\/tr>\n  <tr>\n    <td>Random Forest with Selected Features<\/td>\n    <td>0.844<\/td>\n    <td>0.750<\/td>\n    <td>0.749<\/td>\n  <\/tr>\n  <tr>\n    <td>LightGBM with Selected Features<\/td>\n    <td>0.819<\/td>\n    <td>0.780<\/td>\n    <td>0.783<\/td>\n  <\/tr>\n    \n<\/table>","127fe3a0":"* Joining Credit card balance data to application_bureau_prev data:","c22bc049":"*Observations:*\n1. The data looks strange(we have -1000.66 years(-365243 days) of employment which is impossible), most likely a data entry error","54df685f":"**A kernel to explore loan repayment prediction using machine learning**","433d80d5":"**Using POS_CASH_balance data:**","bfce7f4e":"*Observations*\n\nThe data looks skewed and imbalanced 91.9%(Loan repayed-0) and 8.07%(Loan not repayed-1)) and we need to handle this problem.","4dfac570":"The Random Forest is a model made up of many decision trees. Rather than just simply averaging the prediction of trees (which we could call a \u201cforest\u201d), this model uses two key concepts that gives it the name random:\n\n1. Random sampling of training data points when building trees\n2. Random subsets of features considered when splitting nodes\n\nRefer: [Random Forest Simple Explaination](https:\/\/medium.com\/@williamkoehrsen\/random-forest-simple-explanation-377895a60d2d)","3653d7c2":"**Distribution of Clients Age**","33adb66a":"LightGBM model Results","e96f12fe":"**Logistic regression with selected features**","ab712826":"<div id =\"ml\">\n<h3>Machine Learning Models<\/h3>\n<\/div>","76fe862b":"ROC curve for LightGBM model with **AUC=0.783**","d7a6fe2e":"**Distribution of AMT_CREDIT**","901d5cbb":"*Komal Bakshi, 31st October 2021*","12be815d":"Logistic Regression Model Results","b7710ed3":"<div id=\"results\">\n    <h3> Results and Summary <\/h3>\n <\/div>","9907a4f1":"**Using Bureau Data:**","32ab3bdb":"**Using Previous Application Data:**","7f609e5d":"Observations:\n1. People who are taking credit for large amount are very likely to repay loans\n2. Originally the distribution is rightly skewed, implemented log transformation to make it normally distributed.","d07568ed":"**Distribution of years before the application the person started current employment after removal of error values**","9516d532":"**Distribution of Education of Applicants in terms of loan is repayed or not**","61e7a52c":"<div id=\"metric\">\n<h3>Performance Metric<\/h3>\n<\/div>","8461d617":"<div id=\"imp\">\n    Feature Importance Plot\n<\/div>","919e442a":"#### **Contents:**\n1. [Introduction](#introduction)\n2. [Real World Considerations](#realworld)\n3. [Performance Metric](#metric)\n4. [Data Overview](#data)\n5. [Exploratory Data Analysis](#explore)\n6. [Data Preperation and Feature Engineering](#feature)\n7. [Machine Learning models](#ml)\n8. [Results and Summary](#results)\n9. [References](#refer)","d7ed360b":"<div id=realworld>\n<h3>Real World Considerations<\/h3>\n<\/div>","941d7d7e":"*Observations*:\n1. The applicants with less than 2 years of employment are less likely to repay loans.","9c06abf1":"**Dividing final data into train, valid and test datasets:**","d2a3328e":"* Saving files to output for future reference:","257d6eb8":"**Using installments_payments data:**","4cc02a40":"<div id=\"introduction\">\n    <h3>Introduction<\/h3>\n\nAn existential problem for loan providers today is to identify loan applicants who are more likely to repay the loan. This ensures lending institutions minimise losses and incur profits.\n\n Home Credit offers easy, simple and fast loans for a range of Home Appliances, Mobile Phones, Laptops, Two Wheeler's , and various personal needs. Home Credit has come up with a dataset to enable evaluation of the loan applicants who are capable of repaying a loan, given the applicant data, credit data from Credit Bureau, historical application data from Home Credit and other such attributes.\n<\/div>","e6cea2f9":"*  Predicting the likelihood of an applicant repaying loan\n\nConsider two applicants A, B whose probability of repaying loan are 0.6 and 0.9 respectively. In case the accepted threshold in machine learning models is considered to be 0.5; both the applicants are labeled as a positive outcome (\u2018Repaying loan\u2019). But in reality applicant B is more likely to repay the loan as compared to applicant A. If we predict the probabilities we can set the threshold to higher values such as 0.8, 0.9 as per business requirements.\n    \n    \n*  Mis-classification can prove costly \n\nThis is binary classification problem (0- loan repayed, 1- defaulter), hence the cost of mis-classification can be high.\n\n*  Interpretability is partially significant\n\nAs long as our model performs well on the test data, we don\u2019t need to worry much about interpretability. This is more so because are problem is not set in a  medical context (where interpretability is significant- cancer detection). We can still give importance to some form of interpretability like feature significance.","c37609f3":"*Observation:*\n\nMajority of columns have 50%+ missing values, these will need to be handled","aac6d0d0":"* Reusable functions for plotting Confusion matrix and CV plot","46ba95b2":"* Joining Previous Application data to Application Bureau data:","5138d8b5":"**LightGBM with selected features:**\n\nLight GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks.\n\nSince it is based on decision tree algorithms, it splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word \u2018Light\u2019.","df110af0":"* Identifying missing values for each column","1507c51c":"*Observations:*\n1. People with academic degrees are more likely to repay loans(Out of 164, only 3 applicants were not able to repay)","58287fe4":"<div id=\"explore\">\n    <h3>Exploratory Data Analysis<\/h3>\n<\/div>","f6904732":"**Distribution of AMT_INCOME_TOTAL**","42b515d8":"* Joining POS_CASH_balance data to application_bureau_prev_data:","e380fc67":"**Distribution of Name of type of the Suite in terms of loan is repayed or not.**","0faca147":"**Featurizing the data:**","668bef1d":"<div id=\"refer\">\n    <h3> References <\/h3>\n <\/div>","ed52bba6":"* Joining Installments Payments data to application_bureau_prev_data:","ed5e2f9f":"* Joining Bureau data to Application data:","52fd97e2":"ROC curve for Random Forest Model with **AUC 0.749**","ef9d5146":"1. [Home Credit Data](https:\/\/www.kaggle.com\/c\/home-credit-default-risk)\n    \n2.  [ROC and AUC Explained](https:\/\/www.youtube.com\/watch?v=OAl6eAyP-yo)\n    \n3.  [Random Forest Explained](https:\/\/medium.com\/@williamkoehrsen\/random-forest-simple-explanation-377895a60d2d)\n   \n4. [LightGBM vs Xgboost](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/which-algorithm-takes-the-crown-light-gbm-vs-xgboost\/)\n    \n5. [LightGBM and Xgboost Explained](http:\/\/mlexplained.com\/2018\/01\/05\/lightgbm-and-xgboost-explained\/)\n    ","97a11750":"### Summary of Feature Engineering\nThe [feature importance plot](#imp) at the end of feature engineering displays the top 20 features having most impact on the target variable","ef82454d":"* Checking for duplicate data","141cce14":"**Feature Engineering of Bureau Data:**","de83ee5f":"* Importing relevant libraries","e563aff5":"### Summary of EDA\nThe initial exploratory data analysis helped in identifying a lot of interprative information as summerized below:\n\n1. The initial dataset had 122 features\/attributes\n2. For the top 20 columns the missing data was more than 50% that was handled by imputation.\n3. The TARGET variable which indicated if the applicant repayed loan or not was highly skewed in the train data with 91.9% of the data of repayed applicants. (This imbalance was handled in the data preparation phase)\n4. Distribution charts pointed towards below:\n    * Applicants with high income(>1000000) are likely to repay the loan.\n    * Majority of the applicants (90.5%) preferred taking cash loans.\n    * Applicants who are taking credit for large amount are more likely to repay loan\n    * Amongst the pool of applicants 100% of students and businessmen repayed loan\n    * People with academic degrees are more likely to repay loan\n    * Applicants with less than 2 years of employment are less likely to repay loan "}}