{"cell_type":{"f7dd1d8e":"code","a2cc4b6e":"code","6958344d":"code","0d15b8ab":"code","2788a174":"code","95265761":"code","c4ee18ad":"code","3e0a4573":"code","e0367a20":"code","5bb8f3a1":"code","08076ea0":"code","953ea2ce":"code","8958c08c":"code","4b86489b":"code","dc345f8a":"code","f20186b5":"markdown","f2dde3ad":"markdown","44a83dec":"markdown","9ac5d0c2":"markdown","656cc5cc":"markdown","b8b04dd7":"markdown","966c86b1":"markdown","b6021df8":"markdown","99a20bed":"markdown"},"source":{"f7dd1d8e":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras import layers, Sequential, layers\nfrom keras import models\nimport keras.backend as K\nfrom keras.preprocessing import image\nimport shap\n\nnp.random.seed(1)\n## Batch parameters\nBATCH_SIZE = 128\nIMG_HEIGHT = 224\nIMG_WIDTH = 224\n\nlist_files = []\nlist_category = []\ntrain_dir = '\/kaggle\/input\/new-plant-diseases-dataset\/New Plant Diseases Dataset(Augmented)\/New Plant Diseases Dataset(Augmented)\/train\/'\nfor dirname, _, filenames in os.walk(train_dir):\n    for filename in filenames:\n        list_files.append(os.path.join(dirname, filename))\n        list_category.append(dirname.split('\/')[7])\n\ntraining_set = tf.keras.preprocessing.image_dataset_from_directory(\n    '\/kaggle\/input\/new-plant-diseases-dataset\/new plant diseases dataset(augmented)\/New Plant Diseases Dataset(Augmented)\/train',\n    seed=42,\n    image_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE\n\n)\n\nvalid_set = tf.keras.preprocessing.image_dataset_from_directory(\n    '\/kaggle\/input\/new-plant-diseases-dataset\/new plant diseases dataset(augmented)\/New Plant Diseases Dataset(Augmented)\/valid',\n    seed=42,\n    image_size=(IMG_HEIGHT, IMG_WIDTH),\n    batch_size=BATCH_SIZE)\n\nclasses = training_set.class_names\n","a2cc4b6e":"fig,ax=plt.subplots(4,3)\nfig.set_size_inches(15,15)\nfor next_element in training_set:\n    x_batch, y_batch = next_element\n    for i in range (0,4):\n        for j in range(3):\n            random_example = np.random.randint(0, BATCH_SIZE)\n            ax[i,j].imshow(x_batch[random_example]\/250)\n            ax[i,j].set_title('Status: '+ classes[y_batch[random_example].numpy()])\n    break","6958344d":"plt.figure(figsize=(14,7))\nfig = sns.countplot(x=list_category)\nplt.xticks(rotation=85)\nplt.title(\"Class distribution\")\nplt.xlabel('State')\nplt.show()","0d15b8ab":"red_values = [np.mean(x_batch[idx][:, :, 0]) for idx in range(BATCH_SIZE)]\ngreen_values = [np.mean(x_batch[idx][:, :, 1]) for idx in range(BATCH_SIZE)]\nblue_values = [np.mean(x_batch[idx][:, :, 2]) for idx in range(BATCH_SIZE)]\n\nchannels = [blue_values,\n            red_values,\n            green_values\n           ]\nsns.boxplot(data=channels)\nplt.title(\"Distribution of channels: bleu, red, green\")","2788a174":"cnn_model = tf.keras.models.Sequential([\n    layers.Conv2D(32, 3, activation='relu', input_shape=(224, 224, 3)),\n    layers.Dropout(0.1),\n    layers.MaxPooling2D(),\n    layers.Conv2D(64, 3, activation='relu'),\n    layers.Dropout(0.15),\n    layers.MaxPooling2D(),\n    layers.Conv2D(128, 3, activation='relu'),\n    layers.Dropout(0.2),\n    layers.MaxPooling2D(),\n    layers.Flatten(),\n    layers.Dense(256, activation='relu'),\n    layers.Dense(len(classes), activation='softmax')\n])\n\ncnn_model.compile(optimizer='adam',loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',\n                                              mode='max',\n                                              patience=2,\n                                              restore_best_weights=True)","95265761":"## This patch has been run on GPU not in this framework. I loaded the .h5 directly later rather than running this.\n#get_train_kpi = cnn_model.fit(training_set\n#                              validation_data=valid_set,\n#                              epochs=15,\n#                              callbacks=early_stop)\n#\n#cnn_model.save('cnn_nobatchnorm_15epoch_v2_model.h5')\n#np.save('history_cnn_nobatchnorm_15epoch_v2_model.npy', get_train_kpi.history)\n","c4ee18ad":"path = '..\/input\/cnn-nobatchnorm-v2\/'\nget_train_kpi = np.load(path + 'history_cnn_nobatchnorm_15epoch_v2_model.npy', allow_pickle='TRUE').item()\nmodel = load_model(path + \"cnn_nobatchnorm_15epoch_v2_model.h5\")","3e0a4573":"list_files = []\nlist_category = []\ntest_dir = '..\/input\/new-plant-diseases-dataset\/test\/test\/'\nfor dirname, _, filenames in os.walk(test_dir):\n    for filename in filenames:\n        list_files.append(os.path.join(dirname, filename))\n        list_category.append(filename.split('.JPG')[0])","e0367a20":"pred = pd.DataFrame()\npred['y_label'] = list_category\npred['y_pred'] = np.nan\ni = 0\nfor image_path in list_files:\n    new_img = image.load_img(image_path, target_size=(224, 224))\n    img = image.img_to_array(new_img)\n    img = np.expand_dims(img, axis=0)\n    prediction = np.argmax(model.predict(img))\n    class_pred = classes[prediction].replace('_', '')\\\n    .replace('(maize)', '').lower()\\\n    .replace('leaf', '')\\\n    .replace('tomatotomato', 'tomato')\\\n    .replace('appleapple', 'apple')\\\n    .replace('applecedarapplerust', 'applecedarrust')\\\n    .lower()\\\n\n    pred.loc[i, 'y_pred'] = class_pred\n    pred.loc[i, 'acc'] = class_pred in list_category[i].lower()\n    i = i + 1\n    if i==1:# getting a second example\n        img2 = img\n    if i==4:# getting a second example\n        img3 = img\nprint(pred.acc.mean())\nprint(pred.head(5))","5bb8f3a1":"layer_outputs = [layer.output for layer in model.layers[:9]] # Extracts the outputs of the layers\nactivation_model = models.Model(inputs=model.input, outputs=layer_outputs) # Creates a model that will return these outputs, given the model input","08076ea0":"def visuals_prediction(img, activation_model=activation_model):\n    \"\"\" visuals first layer activation and activation along side the modeling\n    \"\"\"\n\n    # run model in predict mode\n    activations = activation_model.predict(img) # Returns a list of five Numpy arrays: one array per layer activation\n    first_layer_activation = activations[0]\n    plt.matshow(first_layer_activation[0, :, :, 4], cmap='viridis')\n    layer_names = []\n    for layer in model.layers[:9]:\n        layer_names.append(layer.name)\n\n    images_per_row = 24\n\n    for layer_name, layer_activation in zip(layer_names, activations): # Displays the feature maps\n        n_features = layer_activation.shape[-1] # Number of features in the feature map\n        size = layer_activation.shape[1] #The feature map has shape (1, size, size, n_features).\n        n_cols = n_features \/\/ images_per_row # Tiles the activation channels in this matrix\n        display_grid = np.zeros((size * n_cols, images_per_row * size))\n        for col in range(n_cols): # Tiles each filter into a big horizontal grid\n            for row in range(images_per_row):\n                channel_image = layer_activation[0,\n                                                 :, :,\n                                                 col * images_per_row + row]\n                channel_image -= channel_image.mean() # Post-processes the feature to make it visually palatable\n                channel_image \/= channel_image.std()\n                channel_image *= 64\n                channel_image += 128\n                channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n                display_grid[col * size : (col + 1) * size, # Displays the grid\n                             row * size : (row + 1) * size] = channel_image\n        scale = 1. \/ size\n        plt.figure(figsize=(scale * display_grid.shape[1],\n                            scale * display_grid.shape[0]))\n        plt.title(layer_name, fontsize=20)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='viridis')\n    return","953ea2ce":"visuals_prediction(img)","8958c08c":"visuals_prediction(img2)","4b86489b":"#shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough\nexplainer = shap.DeepExplainer(model, x_batch.numpy())\nshap_values = explainer.shap_values(x_batch.numpy()[0].reshape(1, 224, 224, 3), ranked_outputs=1, check_additivity=False)\nshap.image_plot(shap_values[0], x_batch.numpy()[0].reshape(1, 224, 224, 3))","dc345f8a":"shap_values = explainer.shap_values(img, ranked_outputs=2, check_additivity=False)\nshap.image_plot(shap_values[0], img)","f20186b5":"The classes are uniformly distributed. Let's notice that there are what could be called subclasses: tomatoes disease representation might be closer to each other than apples to tomatoes. For this study, we will ignore it. \n\n## Distribution of channels RGB","f2dde3ad":"## Loading saved model and history","44a83dec":"## What can be say from the SHAP values?\nFirst of all, I had quite some memory problem using this library with this dataset in this framework. I couldn't fit it with the number of sample adviced by the documentation (1000).\nFor the training set, the contours and spot on the leaves seem to be nicely extracted. Good to remember here that the overfitting of the model is not solved. When computing the SHAP values for the missclassification of the testing set, we can see the coutours\/spots on the leaves are poorly extracted. Also, the quality of the picture are not good: oversaturted and with shadow. This is probably why it's a misclassification, the quality of the image is not good enough. For the correct classifications in the test set, the shape values seem to be comparable to the training set.\n\n# Conclusion\n\nDeep Learning for the computer vision task is very relevant. I was fairly grateful to find a build-in dataset generator. Also, DL models extract the features directly from a raw input (also grateful to not have to handcraft the feature extraction steps). It always amazes me how easy it is for someone outside of a certain field (here computer vision) to sort out a fairly good classifier and an explanation of the features with the literature available.\nSeeing the activation layers helped to understand how the model learned the representation and come up with some predictions. Even though in this study we didn't implement a large-scale test set to assess performance, it's safe to assume that the proposed model can find relevant relationships between targets and pictures. The quality of the picture has been shown to be important.\n\n\nFinally, I got some problem making SHAP working, I would need to spend another couple of hours to get my hands on it and extract the concret value discribe in the paper.\n\nTo be continued...\n\nNext steps:\n- layers representation VS class predicted\n- memory problel SHAP\n- other architectures\n- LIME ?\n- debug the batch normalizer and SHAP\n\n\nSources: \n* https:\/\/shap-lrjball.readthedocs.io\/en\/latest\/generated\/shap.DeepExplainer.html\n* https:\/\/github.com\/gabrielpierobon\/cnnshapes\/blob\/master\/README.md","9ac5d0c2":"# Exploratory Data Analysis <a class=\"anchor\" id=\"eda\"><\/a>\n\nThe goal here is to get familiar with our data with visuals and statistics. Compare to a regression problem, there are only a few insights to extract. From the literature, computer vision EDA comes after the modeling and consists of visualizing the layers and activation.\n\n## How does it look like?","656cc5cc":"Deep learning models have shown higher performance than machine learning in many fields. Yet, I found it difficult to visualize and understand what is happening behind the layers and activations. It becomes quicly a problem when these models are taken away from the benchmark datasets and need to answer a business case. In my work as a data scientist, I am being challenged with explainability, transparency, and reliability. And even if I have many data points, I often prefer a statistical model to deep learning. This work is a way of challenging my perspective on deep learning and get out of my comfort zone with computer vision. The main objective will be to corroborate that the model is capturing relevant relationship, break down the BlackBox and get my hands on the state of the art model explainability library SHAP.\n\n# Table of Contents\n1. [Import and reading](#Importandreading)\n2. [Exploratory Data Analysis](#eda)\n3. [Modeling](#cnn)\n4. [Vizualizing the layers](#vizu)\n4. [SHAP](#XAI)\n\n\n# Import and reading <a class=\"anchor\" id=\"Importandreading\"><\/a>","b8b04dd7":"We note that the data is already augmented. This is relevant to have a predictive model that generalizes well: the predictions will not be dependent on the quality of the image, or the rotation. Data augmentation in image processing is mainly the following operation on the original image: rotating\/flipping, blurring.\n\n## Class distribution\n","966c86b1":"The test set is too small to be reliable and asses overfitting of the validation set. Though, we do see a disappointing drop in accuracy.\n\n# Vizualizing the layers and activations <a class=\"anchor\" id=\"vizu\"><\/a>\n\nAn advantage dealing with images is that it's very interpretable for the human eye. This section is about visualizing the learned representations. To do so, we will plot the hidden representation for each layer. This should give us an idea of how the input image is decomposed into different representations along with the model. \nTo create the visuals, the literature (http:\/\/faculty.neu.edu.cn\/yury\/AAI\/Textbook\/Deep%20Learning%20with%20Python.pdf) suggests using a Keras model taking as input the images, and as output the activations of all convolution and pooling layers. The results will be a model mapping the input (image) to each of the output (each of the layers). In other words, from an image, the model will return the values of the layer activations of the original model.\n\n","b6021df8":"Not being used to the computer vision task, it's a bit hard for me to draw any conclusion from the channel distribution. Statistically speaking, the three channels have very comparable distributions.\n\n\n# Modeling CNN <a class=\"anchor\" id=\"cnn\"><\/a>\n\nIn this section, we will implement a simple model. Being my first CNN model, I almost fell into a rabbit hole here. It's remarkable to see how many architectures are possible, all solving a specific problem and trying to extract the most out of the images.\nI should mention that the model that offered me the best accuracy was with a batch normalization layer. Though, I couldn't make SHAP running with this model. It's a known bug (https:\/\/github.com\/slundberg\/shap\/issues\/1761) without a fix yet.\n","99a20bed":"## What can be said about these visuals?\n* By construction the representation is being 'average' (with the polling layer), allowing the model to extract relevant pixel value\n* The contrast increase over time, also allowing the model to extract relevant variations in the level of each channel\n* The biggest step happens at the 3rd convolution, where the image disappears, only the relevant contrasts are kept.\n* Very interesting to see that some neurons are representative of the initial picture. I suppose some of these yellow dotes positions are highly correlated with the final predicted class.\n\n## SHAP : SHapley Additive exPlanations <a class=\"anchor\" id=\"XAI\"><\/a>\nThe original paper (https:\/\/arxiv.org\/pdf\/1705.07874.pdf) describe this technics as a reverse ingeneering of predictive model. It aims to quantify the contribution of each feature to individual predictions in the fashon of game theory. As the technic explored in the previous section, SHAP is also about local explainability."}}