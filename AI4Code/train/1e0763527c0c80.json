{"cell_type":{"fbc43738":"code","ebcafbd3":"code","b685e719":"code","c6acb063":"code","e081ea2d":"code","471ce5e2":"code","736f60cd":"code","7f6147c5":"code","39a5010f":"code","6f035e94":"code","b68c1119":"code","2606eb3c":"code","fa7bff86":"code","e005f1bc":"code","0fbffa27":"markdown","4191aa5f":"markdown","6b018a7e":"markdown","5566041f":"markdown","1ba431cd":"markdown","0271e829":"markdown","bb739d49":"markdown","f338556e":"markdown","81c2f737":"markdown","b575a9d5":"markdown","54aed0fd":"markdown","d424d450":"markdown","acd7c178":"markdown","42d51ec9":"markdown","61d8fb79":"markdown","cb8779d8":"markdown","8f0c1ba5":"markdown","df2d2c4c":"markdown"},"source":{"fbc43738":"import numpy as np # linear algebra\nimport math\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom tensorflow.python import keras\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,Flatten","ebcafbd3":"!ls ..\/input\/digit-recognizer\/","b685e719":"img_rows = 28\nimg_cols = 28\nnum_classes = 26\n\ndef extract_images(flattened_image_vectors,rows,cols):\n    num_images = flattened_image_vectors.shape[0]\n    return np.reshape(flattened_image_vectors,(num_images,rows,cols,1))\n\ndef prep_data(raw_data):\n    x = extract_images(raw_data[:,1:],img_rows,img_cols)\n    out_x = x \/ 255\n    \n    y = raw_data[:,0]\n    out_y = keras.utils.to_categorical(y, num_classes)\n    \n    return out_x,out_y","c6acb063":"mnist_test_file = \"..\/input\/digit-recognizer\/test.csv\"\nraw_test_data = np.loadtxt(mnist_test_file,skiprows=1,delimiter=',')","e081ea2d":"# raw_test_data.shape\nx_test = extract_images(raw_test_data,img_rows,img_cols)","471ce5e2":"mnist_train_file = \"..\/input\/digit-recognizer\/train.csv\"\nraw_data = np.loadtxt(mnist_train_file,skiprows=1,delimiter=',')\n","736f60cd":"x,y = prep_data(raw_data)","7f6147c5":"mnist_model = Sequential()\nmnist_model.add(Conv2D(32,kernel_size=5,input_shape=(img_rows,img_cols,1),activation='relu'))\nmnist_model.add(Conv2D(32,kernel_size=5,activation='relu'))\nmnist_model.add(Flatten())\nmnist_model.add(Dense(100,activation='relu'))\nmnist_model.add(Dense(num_classes,activation='softmax'))\nmnist_model.summary()\n\nmnist_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\nmnist_model.fit(x,y,batch_size=100,epochs=30,validation_split=0.2)","39a5010f":"print(\"\\t\\t|\\t\".join(mnist_model.metrics_names))\nprint(\"-\"*35)\nprint(\"\\t|\\t\".join(map(lambda x:str(x),mnist_model.test_on_batch(x,y))))","6f035e94":"predictions =  mnist_model.predict_classes(x_test)\ntrain_predictions = mnist_model.predict_classes(x)\n\nfig=plt.figure(figsize=(12,12))\nfor i in range(8):\n    fig.add_subplot(4,4,i+1)\n    plt.imshow(x_test[i].reshape(img_rows,img_cols),cmap='gray')\n    plt.title(f'model classified as {predictions[i]}')\nplt.show()","b68c1119":"predicted_classes = mnist_model.predict_classes(x)\ninvalid_classifications = predicted_classes!=np.argmax(y,axis=1)\nwrongly_classified_image_indices = np.argwhere(invalid_classifications==True).flatten()","2606eb3c":"fig=plt.figure(figsize=(12,7))\nwrongly_classified_image_indices = np.random.choice(np.argwhere(invalid_classifications==True).flatten(),size=8)\ninvalid_count = len(wrongly_classified_image_indices)\nfor index,image_index in enumerate(wrongly_classified_image_indices):\n    fig.add_subplot(math.ceil(invalid_count\/4),4,index+1)\n    plt.imshow(x[image_index].reshape(img_rows,img_cols),cmap='gray')\n    plt.title(f'marked  {predicted_classes[image_index]} | actually  {np.argmax(y,axis=1)[image_index]}')\nplt.show()\n    ","fa7bff86":"np.savetxt('submit1.csv',[(i+1,j) for i,j in enumerate(predictions)],fmt=\"%d,%d\",delimiter=',',header='ImageId,Label',comments='')","e005f1bc":"!head submit1.csv","0fbffa27":"### Import the required Libraries","4191aa5f":"## Find out the wrongly classified ones in the training set","6b018a7e":"## Analysis","5566041f":"#### Take a gander at the CSV file to make sure things are dandy","1ba431cd":"### Take a look at the available files","0271e829":"### Read the test data","bb739d49":"### Read the training data","f338556e":"## Using the Keras API with the tensorflow backend to take a stab at the age-old MNIST Dataset.\n**Check out the wrongly classified images section to see why it's so difficult to get above 99% accuracy for even the best models**","81c2f737":"## Output\n### Save to CSV File","b575a9d5":"### Check out a random sampling of them; Some of them were wrongly classified even by me!\nRe-run this cell to get a different random sampling of wrongly classified digits","54aed0fd":"### Parse the test data","d424d450":"### Retrieve Classification data and check them out","acd7c178":"### Final Training Loss and Accuracy","42d51ec9":"# Model Definition and Training","61d8fb79":"### Parse and prep the training data","cb8779d8":"## Preparation","8f0c1ba5":"## Parsing","df2d2c4c":"### Read and prepare the files"}}