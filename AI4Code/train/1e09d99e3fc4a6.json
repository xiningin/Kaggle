{"cell_type":{"6f18e1f5":"code","68afbb17":"code","9639cb65":"code","a5694e11":"code","1f1e4c9d":"code","87aa99bc":"code","08b2512f":"code","da56762c":"code","902749c4":"code","5daea330":"code","7625bbe2":"code","08de2442":"code","87fef4ef":"code","46cefae9":"code","ca5906ac":"code","a13c0ae2":"code","0763a4cc":"code","fb48656f":"code","6e050645":"code","d3fdb471":"code","6b75ab91":"markdown","90a00a2b":"markdown","e46e9769":"markdown","d9556619":"markdown","0aef4041":"markdown"},"source":{"6f18e1f5":"get_ipython().run_line_magic('matplotlib', 'inline') \nimport joblib\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Tensorflow\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Input,Dropout\nfrom tensorflow.keras.optimizers import SGD, Adam\nfrom tensorflow.keras.layers import BatchNormalization\n# SKLearn\nfrom sklearn import preprocessing,feature_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n# ImbLearn\nfrom imblearn.over_sampling import SMOTE\n# Scipy\nfrom scipy.special import boxcox1p\n\n# Frobidden using alll GPUS\ngpus = tf.config.experimental.list_physical_devices(device_type='GPU')\nfor gpu in gpus:\n    tf.config.experimental.set_memory_growth( device=gpu, enable=True)\n    \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","68afbb17":"cd {dirname}\n","9639cb65":"def setup_seed(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)","a5694e11":"## Categorize features\n@np.vectorize\ndef is_bool(x):\n    return x in [True,False]\n'''Load data'''\ndf_train=pd.read_csv(\"train.csv\")\ndf_test=pd.read_csv(\"test.csv\")\ndf_train['terrainType']=df_train['terrainType'].astype(\"str\")\ndf_test['terrainType']=df_test['terrainType'].astype(\"str\")\ndf_mix=df_test.merge(df_train,how=\"outer\")\ndf_tmp=df_mix.iloc[:2].drop(columns=[\"class\",'id'])\n'''Categorize features by dtype'''\nall_cols=df_tmp.columns.values\nnum_cols=np.array([col for col in df_tmp[all_cols]._get_numeric_data().columns.values if col!=\"terrainType\"])\ntmp_cols=np.setdiff1d(all_cols,num_cols)\ncat_cols=tmp_cols[np.logical_or(~is_bool(df_tmp[tmp_cols].values[0]),~is_bool(df_tmp[tmp_cols].values[1]))]\nbinary_cols=np.setdiff1d(tmp_cols,cat_cols)\ncooc_cols=np.array([cont for cont in binary_cols if \"cooc\" in cont ])\nbinary_cols=np.setdiff1d(binary_cols,cooc_cols)","1f1e4c9d":"## CYclic Encoding and Drop\ndf_train['appearTime_sin']=np.sin((df_train['appearedMinute']\/60+df_train['appearedHour'])\/12*np.pi)\ndf_train['appearTime_cos']=np.cos((df_train['appearedMinute']\/60+df_train['appearedHour'])\/12*np.pi)\ndf_test['appearTime_sin']=np.sin((df_test['appearedMinute']\/60+df_test['appearedHour'])\/12*np.pi)\ndf_test['appearTime_cos']=np.cos((df_test['appearedMinute']\/60+df_test['appearedHour'])\/12*np.pi)\nnum_cols=[col for col in num_cols if col not in [\"appearedMinute\",\"appearedHour\",\"gymDistanceKm\",\"pokestopDistanceKm\"]]+[\"appearTime_sin\",\"appearTime_cos\"]","87aa99bc":"# deal with skewneww and normalize\nnorm_param=joblib.load(\"norm_param.joblib\")\n\ndf_train[[\"windSpeed\",\"population_density\"]]=df_train[[\"windSpeed\",\"population_density\"]].apply(lambda x:boxcox1p(x+np.finfo(float).eps,.15))\ndf_test[[\"windSpeed\",\"population_density\"]]=df_test[[\"windSpeed\",\"population_density\"]].apply(lambda x:boxcox1p(x+np.finfo(float).eps,.15))\n\ndf_train[num_cols]=(df_train[num_cols]-norm_param[\"train_m\"])\/norm_param[\"train_s\"]\ndf_test[num_cols]=(df_test[num_cols]-norm_param[\"test_m\"])\/norm_param[\"test_s\"]\n\nx_num_train=df_train[num_cols].values\nx_num_test=df_test[num_cols].values","08b2512f":"# Encoding categorical data\nenc=joblib.load(\"cat_enc.joblib\")\nx_cat_train=enc.transform(df_train[cat_cols].values).toarray()\nx_cat_test=enc.transform(df_test[cat_cols].values).toarray()","da56762c":"x_train=np.concatenate([df_train[num_cols+list(binary_cols)+list(cooc_cols)].values,x_cat_train],axis=1).astype(\"float32\")\nx_test=np.concatenate([df_test[num_cols+list(binary_cols)+list(cooc_cols)].values,x_cat_test],axis=1).astype(\"float32\")\n\ny_train=df_train[[\"class\"]].values","902749c4":"## For firsttimers\ny_enc =preprocessing.OneHotEncoder()\ny_enc.fit(y_train)\n# joblib.dump(y_enc,\"y_enc.joblib\")\n\n## For secondtimers\n# y_enc=joblib.load(\"y_enc.joblib\")","5daea330":"\nsetup_seed(2021)\n\nx_train_1,x_val,y_train_1,y_val=train_test_split(x_train,y_train,test_size=0.1)\nx_train_1, y_train_1 = SMOTE(k_neighbors=10).fit_resample(x_train_1, y_train_1)\ny_train_1=y_train_1[...,np.newaxis]","7625bbe2":"y_train=y_enc.transform(y_train).toarray().astype(\"float32\")\ny_train_1=y_enc.transform(y_train_1).toarray().astype(\"float32\")\ny_val=y_enc.transform(y_val).toarray().astype(\"float32\")","08de2442":"print(len(x_train),len(x_train_1),len(x_val))","87fef4ef":"model = Sequential()\ndrp_rate=0.5\n# \u52a0\u5165\u7b2c\u4e00\u5c64 hidden layer (128 neurons) \u8207\u6307\u5b9a input \u7684\u7dad\u5ea6\nmodel.add(Dense(512, input_dim=x_train.shape[-1],activation=tf.nn.relu)) # \u6307\u5b9a activation function\nmodel.add(BatchNormalization())\nmodel.add(Dropout(drp_rate))\n\nmodel.add(Dense(256,activation=tf.nn.relu)) # \u6307\u5b9a activation function\nmodel.add(BatchNormalization())\nmodel.add(Dropout(drp_rate))\n\nmodel.add(Dense(128,activation=tf.nn.relu)) # \u6307\u5b9a activation function\nmodel.add(BatchNormalization())\nmodel.add(Dropout(drp_rate))\n\n# \u52a0\u5165 output layer (6 neurons)\nmodel.add(Dense(y_train.shape[1:][0],activation='softmax'))\n# \u89c0\u5bdf model summary\n# model.summary()","46cefae9":"N_PATIENCE = 10 # \u8a13\u7df4\u904e\u7a0b\u7d93\u904e20\u6b21\u6c92\u6709\u9032\u6b65\u4e4b\u5f8c\u505c\u6b62\n\n# \u5efaEarlyStopping\uff0cmonitor\u4ee5val_loss\u70ba\u4e3b\uff0c\u7d93\u904e\u4e94\u6b21patience\uff0cverbose=1\uff1a\u4fe1\u606f\u5c55\u793a\nearly_stopping = keras.callbacks.EarlyStopping(monitor = 'val_loss', \n                               patience = N_PATIENCE, \n                               verbose = 1)\nlr_scheduler=keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss',\n                               patience = 3,\n                               factor=0.1,\n                               verbose=1)\n\noptim = Adam(lr=0.00001,)\nloss_fun = 'categorical_crossentropy'\nmodel.compile(loss=loss_fun,\n              optimizer=optim,\n              metrics=['accuracy'])","ca5906ac":"setup_seed(2021)\n# \u8a2d\u5b9a batch_size\uff0c\u8a13\u7df4\u8fed\u4ee3\u8f2a\u6b21(\u56de\u5408\u6578) epochs\nBATCH_SIZE = 32 # 1\u500bmini-batch\u670932\u7b46\u8cc7\u6599\nEPOCHS = 400\n\n# \u958b\u59cb\u8a13\u7df4\u6a21\u578b\n# https:\/\/keras-cn.readthedocs.io\/en\/latest\/models\/model\/\nfitting_history = model.fit(x_train_1, y_train_1,      # X\u70ba\u8f38\u5165\u6578\u64da\u3001y\u70ba\u6a19\u7c64\n                batch_size=BATCH_SIZE, # \u4e00\u500bstep\u7684\u8a13\u7df4\u6a23\u672c\u6578\n                epochs=EPOCHS,         # \u5c07\u5168\u90e8\u6578\u64da\u96c6\u770b\u904e EPOCHS\u6b21\n                verbose=1,             # verbose = 1,\u5370\u51fa\u6bcf\u6b21\u8a13\u7df4\u904e\u7a0b\n                shuffle=True,          # \u5728\u6bcf\u8f2a\u8fed\u4ee3\u958b\u59cb\u524d\u6253\u6df7\u6578\u64da - (\u589e\u52a0\u4e82\u6578\u6027\uff0c\u8b93\u6a21\u578b\u8a13\u7df4\u66f4\u597d)\n                validation_split=0.2, callbacks=[early_stopping,lr_scheduler])  ","a13c0ae2":"def plot_fithist(fitting_history):\n    loss = fitting_history.history.get('loss')         # \u53d6\u51fa\u8a13\u7df4\u5b8c\u6210\u5f8closs\u7684\u8a13\u7df4\u6578\u64da\n    acc = fitting_history.history.get('accuracy')           # \u53d6\u51fa\u8a13\u7df4\u5b8c\u6210\u5f8caccuracy\u7684\u8a13\u7df4\u6578\u64da\n    val_loss = fitting_history.history.get('val_loss') # \u53d6\u51fa\u8a13\u7df4\u5b8c\u6210\u5f8cval_loss\u7684\u8a13\u7df4\u6578\u64da\n    val_acc = fitting_history.history.get('val_accuracy')   # \u53d6\u51fa\u8a13\u7df4\u5b8c\u6210\u5f8cval_accuracy\u7684\u8a13\u7df4\u6578\u64da\n    \n    ''' Visualize the loss and accuracy of both models'''\n    plt.figure(figsize=[10,7])\n    plt.subplot(121)\n    plt.plot(range(len(loss)), loss, label='Training loss')\n    plt.plot(range(len(val_loss)), val_loss, label='validation loss')\n    plt.title('Loss');plt.legend(loc='upper right')\n\n    plt.subplot(122)\n    plt.plot(range(len(acc)), acc, label='Training accuracy')\n    plt.plot(range(len(val_acc)), val_acc, label='validation accuracy')\n    plt.title('Accuracy');plt.legend(loc='lower right');plt.show()\n    \nplot_fithist(fitting_history)","0763a4cc":"y_pred=model.predict(x_val)\nprint(classification_report(y_pred.argmax(-1),y_val.argmax(-1)))","fb48656f":"y_pred=model.predict(x_train)\nprint(classification_report(y_pred.argmax(-1),y_train.argmax(-1)))","6e050645":"fitting_history = model.fit(x_train, y_train,      # X\u70ba\u8f38\u5165\u6578\u64da\u3001y\u70ba\u6a19\u7c64\n                batch_size=BATCH_SIZE, # \u4e00\u500bstep\u7684\u8a13\u7df4\u6a23\u672c\u6578\n                epochs=EPOCHS,         # \u5c07\u5168\u90e8\u6578\u64da\u96c6\u770b\u904e EPOCHS\u6b21\n                verbose=1,             # verbose = 1,\u5370\u51fa\u6bcf\u6b21\u8a13\u7df4\u904e\u7a0b\n                shuffle=True,          # \u5728\u6bcf\u8f2a\u8fed\u4ee3\u958b\u59cb\u524d\u6253\u6df7\u6578\u64da - (\u589e\u52a0\u4e82\u6578\u6027\uff0c\u8b93\u6a21\u578b\u8a13\u7df4\u66f4\u597d)\n                validation_split=0.2, callbacks=[early_stopping,lr_scheduler])  ","d3fdb471":"predict=model.predict_classes(x_test)\noutput=pd.DataFrame()\noutput[\"ID\"]=df_test[\"id\"]\noutput[\"class\"]=predict\noutput.set_index(\"ID\",inplace=True)\noutput.to_csv(\"\/kaggle\/working\/AT091094_pokemongo_dnn.csv\")","6b75ab91":"# Preprocess Inputs","90a00a2b":"# Fine tuning","e46e9769":"# Data splitting","d9556619":"# Build Model","0aef4041":"# Training+validation"}}