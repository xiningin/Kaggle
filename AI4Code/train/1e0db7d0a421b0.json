{"cell_type":{"39408ec9":"code","43feda86":"code","7839ac61":"code","3a57debc":"code","0616437a":"code","91c429ce":"code","4281f709":"code","88aa775b":"code","74e8344b":"code","0655158c":"code","4b59febb":"code","a3aa8b57":"code","22a2d683":"code","8c1cc804":"code","41818831":"code","1a413a9d":"code","b46afad7":"code","61c2b628":"code","19a84560":"code","24e2eb72":"markdown","fb98749e":"markdown","ce4f09c8":"markdown","0968f12c":"markdown","9940b9d7":"markdown","96e12df8":"markdown","86ebf600":"markdown","44585ee3":"markdown","138eb10c":"markdown","9c53f54b":"markdown","b4c35ca8":"markdown"},"source":{"39408ec9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","43feda86":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport matplotlib.pyplot as plt","7839ac61":"df_news = pd.read_json(\"\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines = \"True\")\nprint(df_news.head)","3a57debc":"df_news = df_news.drop(\"article_link\", axis=1)","0616437a":"df_news['is_sarcastic'].value_counts().plot.bar() #equally divided","91c429ce":"df_news.isna().sum() # check for any null entry","4281f709":"import string\npunct = string.punctuation\ndef remove_punctuation(text_sentence):\n    text = \"\".join([word for word in text_sentence if word not in punct])\n    return text\ndf_news['headline_nopunct'] = df_news['headline'].apply(lambda x: remove_punctuation(x))\ndf_news.head()\n    ","88aa775b":"# keras text preprocessing tokenizer is used after the pre processing but here i have used my own tokenizer using regular expression\n# in preprocessing \nimport re\ndef tokenize(text_sentence): \n    token = re.split('\\W+', text_sentence)\n    return token\ndf_news['headline_tokenize'] = df_news['headline_nopunct'].apply(lambda x: tokenize(x))\ndf_news.head()","74e8344b":"import nltk\nstopwords = nltk.corpus.stopwords.words('english')\ndef remove_stopword(text_sentence): \n    text = [word for word in text_sentence if word not in stopwords]\n    return text\ndf_news['headline_nostopword'] = df_news['headline_tokenize'].apply(lambda x: remove_stopword(x))\ndf_news.head()","0655158c":"ps = nltk.PorterStemmer() # i have used portstemmer but other stemmers can be used\ndef stemming(text_sentence): \n    text = [ps.stem(word) for word in text_sentence]\n    return text\ndf_news['headline_stem'] = df_news['headline_nostopword'].apply(lambda x : stemming(x))\ndf_news.head()\n\n# check the word veggies -> veggi , different->differ ( different and differ have very separate meaning and it wont help the neural network t\n# training if it sees different and differ are same.","4b59febb":"wm = nltk.WordNetLemmatizer()\ndef lemmatize(text_sentence):\n    text = [wm.lemmatize(word) for word in text_sentence]\n    return text\ndf_news['headline_lemmatize'] = df_news['headline_nostopword'].apply(lambda x : lemmatize(x))\ndf_news.head()\n\n# notice that different => different is not changed.","a3aa8b57":"vocab_size = 10000\nembedding_dim = 16\nmax_len = 250\ntrunc_type = \"post\"\noov_tok = \"<OOV>\"\ntraining_size = 20000","22a2d683":"sentences = []\nlabel = []\n\nfor i in df_news.index:\n    sentences.append(df_news['headline_lemmatize'][i])\n    label.append(df_news['is_sarcastic'][i])\n\ntrain_sentence = sentences[0:training_size]\nprint(\"Training dataset len \" , len(train_sentence))\ntest_sentence = sentences[training_size:]\nprint(\"Testing dataset len\" , len(test_sentence))\n\ntrain_label = np.array(label[0:training_size])\ntest_label = np.array(label[training_size:])\n\n","8c1cc804":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(train_sentence)\nword_index = tokenizer.word_index\n\ntrain_word_sequence = tokenizer.texts_to_sequences(train_sentence)\ntrain_padd_sequence = pad_sequences(train_word_sequence, maxlen=max_len, truncating=trunc_type)\n\ntest_word_sequence = tokenizer.texts_to_sequences(test_sentence)\ntest_padd_sequence = pad_sequences(test_word_sequence, maxlen=max_len, truncating=trunc_type)","41818831":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_len),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(64, activation = \"relu\"),\n    tf.keras.layers.Dense(1, activation = \"sigmoid\")\n])","1a413a9d":"model.summary()","b46afad7":"model.compile(optimizer=\"adam\", loss = tf.keras.losses.binary_crossentropy, metrics = [\"accuracy\"])","61c2b628":"history = model.fit(train_padd_sequence, train_label, validation_data = (test_padd_sequence, test_label), epochs = 10)","19a84560":"def plot_graphs(history, attr):\n  plt.plot(history.history[attr])\n  plt.plot(history.history['val_'+attr])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(attr)\n  plt.legend([attr, 'val_'+attr])\n  plt.show()\nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","24e2eb72":"Bi Directional LSTM Model","fb98749e":"Lemmatization:\n\nI prefer lemmatization than stemming because its context aware and helps more in a NLP task\n","ce4f09c8":"This notebook contains the basic building blocks for any NLP problem stement\n\na. Data Pre processing\nb. vectorization\nc. Feature engineering\nd. Model selection\n\nText data can be highly unorganized and pre processing is very importatnt for text data\nBelow pre processing steps are used for every text related problem statement\n\n    1 . Removing punctuation:\n    2 . Tokenizing\n    3 . Remove stop word\n    4 . stemming \/ lemmatization\n\nafter the preprocessing is done the feature feature is created and feed to the neural network\n       ","0968f12c":"Stemming \/ lemmatization\n\nBoth stemming and lemmtization are process of reducing inflected or derived words to its root\nbut there is certain difference in both where the stemming is the context aware and just remove the postfix where \nas the lemmatization is context aware, thus it computational expensive","9940b9d7":"Input preprocessing","96e12df8":"Tokenize the sentence","86ebf600":"> Hyper parameter","44585ee3":"Text pre processing:\n1. Remove punctuation\n2. Tokenize\n3. Remove stop words\n4. Stemming \/ Lemmatizatio","138eb10c":"Tokenization and padding","9c53f54b":"Remove stopword","b4c35ca8":"Remove the punctuation"}}