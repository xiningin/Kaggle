{"cell_type":{"38d57b64":"code","832e78d3":"code","faaa2557":"code","64b73029":"code","85d02321":"code","eaa1fac3":"code","8dfa0c6a":"code","fb76401b":"code","c9f55eb3":"code","99d9689f":"code","51a4fd0b":"code","51346ebb":"code","9c82a113":"markdown","243c226a":"markdown","418da1f5":"markdown","91155b3e":"markdown","c8bc4aac":"markdown"},"source":{"38d57b64":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn import svm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons, make_circles, make_classification\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","832e78d3":"training = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntraining['train_test'] = 1\ntest['train_test'] = 0\ntest['Survived'] = np.NaN\nall_data = pd.concat([training,test])\ntraining_prepared = training.copy()\ntraining_prepared = training_prepared.replace({'Sex': { 'male': 0, 'female': 1 }})\ntest_prepared = test.copy().replace({'Sex': { 'male': 0, 'female': 1 }})\n\n%matplotlib inline\nall_data.columns","faaa2557":"training_prepared","64b73029":"training.describe()","85d02321":"training_prepared.describe()","eaa1fac3":"# Separate numerical and categorical values\ndf_num = training_prepared[['Age','SibSp','Parch','Fare','Sex','Pclass']]\ndf_numsv = training_prepared[['Age','SibSp','Parch','Fare','Sex','Pclass','Survived']]\nnumsv = df_numsv.to_numpy()\n# remove all rows that contain NaN values\nnumsv = numsv[~np.isnan(numsv).any(axis=1)]\n              \nfdf = test_prepared[['Fare', 'Sex', 'Pclass']].to_numpy()\n# use 0s instead of nan (for testing data)\nfdf = np.nan_to_num(fdf, copy=False)","8dfa0c6a":"for i in df_numsv.columns:\n    plt.hist(df_numsv[i])\n    plt.title(i)\n    plt.show()","fb76401b":"print(df_numsv.corr())\nsns.heatmap(df_numsv.corr())","c9f55eb3":"# Survival rates across Age, SibSp, Parch, Fare, Sex and Pclass \npd.pivot_table(training_prepared, index = 'Survived', values = ['Age','SibSp','Parch','Fare','Sex','Pclass'])","99d9689f":"print(numsv)","51a4fd0b":"h = .02  # step size in the mesh\n\nnames = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\"]\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"linear\", C=0.025),\n    SVC(gamma=2, C=1),\n    GaussianProcessClassifier(1.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=3),\n    MLPClassifier(alpha=1, max_iter=10000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis()]\n\n# Select Fare, Sex, and Pclass as features\n#   and Survived as target\nX, y = numsv[:,-4:-1], numsv[:,-1:].T[0]\n\nfigure = plt.figure(figsize=(27, 9))\ni = 1\nX = StandardScaler().fit_transform(X)\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=.5)\n\nmms = []\nfor r in range(X.shape[1]):\n    mms.append(np.array([X[:, r].min() - .5, X[:, r].max() + .5]))\nmg = np.meshgrid(*[np.arange(*m, h) for m in mms])\n\n# just plot the dataset first\ncm = plt.cm.RdBu\ncm_bright = ListedColormap(['#FF0000', '#0000FF'])\nax = plt.subplot(2, (len(classifiers) + 1 + 1) \/\/ 2, i, projection='3d')\nax.set_title(\"Input data\")\n# Plot the training points\nax.scatter(X_train[:, 0], X_train[:, 1], X_train[:, 2], c=y_train, cmap=cm_bright,\n           edgecolors='k')\n# Plot the testing points\nax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=y_test, cmap=cm_bright, alpha=0.6,\n           edgecolors='k')\nax.set_xlim(mms[0].min(), mms[0].max())\nax.set_ylim(mms[1].min(), mms[1].max())\nax.set_zlim(mms[2].min(), mms[2].max())\nax.set_xticks(())\nax.set_yticks(())\nax.set_zticks(())\ni += 1\n\n# iterate over classifiers\nfor name, clf in zip(names, classifiers):\n    ax = plt.subplot(2, (len(classifiers) + 1 + 1) \/\/ 2, i, projection='3d')\n    clf.fit(X_train, y_train)\n    score = clf.score(X_test, y_test)\n    print(f'Classifier: {name};  score: {score}')\n    matches = (clf.predict(X_test) == y_test)\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    if hasattr(clf, \"decision_function\"):\n        Z = clf.decision_function(np.c_[mms[0].ravel(), mms[1].ravel(), mms[2].ravel()])\n    else:\n        Z = clf.predict_proba(np.c_[mms[0].ravel(), mms[1].ravel(), mms[2].ravel()])[:, 1]\n\n    # Plot the training points\n#     ax.scatter(X_train[:, 0], X_train[:, 1], X_train[:, 2], c=y_train, cmap=cm_bright,\n#                edgecolors='k')\n    # Plot the testing points\n    ax.scatter(X_test[:, 0], X_test[:, 1], X_test[:, 2], c=matches, cmap='PiYG',\n               edgecolors='k', alpha=0.5)\n\n    ax.set_xlim(mms[0].min(), mms[0].max())\n    ax.set_ylim(mms[1].min(), mms[1].max())\n    ax.set_zlim(mms[2].min(), mms[2].max())\n    ax.set_xticks(())\n    ax.set_yticks(())\n    ax.set_zticks(())\n    ax.set_title(f'{name} ({i - 2})')\n    ax.text(mms[0].max() - .3, mms[1].min() + .3, mms[2].min() + .3, ('%.3f' % score).lstrip('0'),\n            size=15, horizontalalignment='right')\n    i += 1\n\nplt.tight_layout()\nplt.show()","51346ebb":"# Select \"Gaussian Process\" classifier\ngcp = classifiers[3]\nX = StandardScaler().fit_transform(fdf)\nfinal_prediction = gcp.predict(X)\nff = np.int32(np.dstack((test['PassengerId'].to_numpy(), final_prediction))[0])\nfinal_df = pd.DataFrame(data=ff, columns=['PassengerId', 'Survived'])\nprint(final_df)\nwith open('result.csv', 'w') as f:\n    f.write(final_df.to_csv(index=False))\n","9c82a113":"# Data analysis","243c226a":"# Classifiers fitting and comparison\n\nCode is mostly taken from: https:\/\/scikit-learn.org\/stable\/auto_examples\/classification\/plot_classifier_comparison.html","418da1f5":"As can be seen Fare & Sex have a positive correlation with survivability, whilst Pclass is having a negative one","91155b3e":"# Data loading and preparation","c8bc4aac":"# Survivors prediction"}}