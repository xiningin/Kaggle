{"cell_type":{"f0447874":"code","98eabbb2":"code","084de84d":"code","2845508f":"code","7e96164b":"code","6a1265f7":"code","d2ef2d80":"code","0e200bff":"code","94cc39ff":"code","5a49e510":"code","029f0d8f":"code","d26d19d9":"code","543ed8bf":"code","ebd418db":"code","56a1b46b":"markdown","e25f1a92":"markdown","364f87ef":"markdown"},"source":{"f0447874":"!pip install keras-rl\n!pip install tensorflow==1.14","98eabbb2":"from kaggle_environments import evaluate, make, utils\n\nenv = make(\"connectx\", debug=True)\n#env.render()","084de84d":"import numpy as np\n\nfrom rl.core import Processor\nclass CustomProcessor(Processor):\n    def process_state_batch(self, obs_batch):\n        batch = np.array(obs_batch)\n        result = batch.reshape(-1, 6, 7)\n        def group(asd):\n            a, b = asd.copy(), asd.copy()\n            a[a == 1] = 0\n            a[a == 2] = 1\n            b[b == 2] = 0\n            return np.stack((a,b), axis=2)\n        def get_mark(asd):\n            return 1 if np.count_nonzero(asd==2) > np.count_nonzero(asd==1) else 2\n        res1, res2 = [group(a) for a in result], [get_mark(a) for a in result]\n        return [res1, res2]\n    def process_action(self, action):\n        return int(action)","2845508f":"from sklearn.utils import shuffle\n\ndef make_idiot():\n    asd = shuffle(list(range(7)))\n    def idiot(observation, configuration):\n        board = observation.board\n        columns = configuration.columns\n        options= [c for c in range(columns) if board[c] == 0]\n        return [a for a in asd if a in options][0]\n    return idiot","7e96164b":"import random\nclass ConnectTrainer():\n    \"\"\"Connect Trainer\n    \"\"\"\n    def __init__(self, env, configuration):\n        self.env =  env\n        self.configuration = configuration\n        self.get_players = lambda self: [None, 'random']\n        \n    def step(self, action):\n        observation, reward, done, info = self.trainer.step(action)\n        observation = observation.board\n        if reward is None:\n            # invalid action\n            reward = -100\n        else:\n            if reward == 0:\n                # lose\n                reward = -10\n            if not done:\n                # game still in course\n                reward = 0\n            # long game equals less penalty and less reward\n            reward *= 1+5\/np.count_nonzero(observation)\n        return observation, reward, done, info\n\n    def reset(self):\n        self.trainer = self.env.train(self.get_players(self))\n        observation = self.trainer.reset()\n        observation = observation.board\n        return observation\n    \n    def set_difficulty(self, difficulty):\n        if difficulty < 1:\n            self.get_players = lambda self: shuffle([None, random.choice([\"random\", make_idiot()])])\n        if difficulty >= 1:\n            self.get_players = lambda self: shuffle([None, random.choice([\"negamax\", make_idiot()])])\n        if 'my_agent' in globals():\n            if difficulty >= 2:\n                self.get_players = lambda self: shuffle([None, random.choice([\"negamax\", my_agent])])\n            if difficulty >= 3:\n                self.get_players = lambda self: shuffle([None, my_agent])\n        else:\n            print('Self play not available')\n        print(f'Difficulty set to: {difficulty}.')","6a1265f7":"from rl.callbacks import Callback\n\nclass DifficultyChangeCallback(Callback):\n    def __init__(self, initial_difficulty, difficulty_steepness=150):\n        self.difficulty_steepness = difficulty_steepness\n        self.difficulty = initial_difficulty\n        \n    def on_episode_begin(self, episode, logs):\n        if episode and episode % self.difficulty_steepness == 0:\n            self.difficulty += 1\n            self.env.set_difficulty(self.difficulty)","d2ef2d80":"import tensorflow as tf\n\nfrom rl.policy import BoltzmannQPolicy\nfrom rl.memory import SequentialMemory\nfrom rl.agents import DQNAgent\n\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Flatten,Dense,Activation\nfrom keras.optimizers import Adam\n\n\n# Environment setup\nnb_actions = 7\nenv_wrapper = ConnectTrainer(env, env.configuration)\n\n# Model setup\ninput_shape_1 = keras.Input(shape=(6, 7, 2))\ninput_shape_2 = keras.Input(shape=(1,)) #mark\ntower_1 = keras.layers.Conv2D(80, kernel_size=(2, 2), padding='same', activation='relu')(input_shape_1)\ntower_1 = keras.layers.Dropout(0.02)(tower_1)\ntower_2 = keras.layers.Conv2D(80, kernel_size=(3, 3), padding='same', activation='relu')(input_shape_1)\ntower_2 = keras.layers.Dropout(0.02)(tower_2)\nmerged = keras.layers.Concatenate(axis=1)([tower_1, tower_2])\nmerged = keras.layers.Flatten()(merged)\n# cropped = keras.layers.Cropping2D(cropping=((0, 0), (0, 6)))(input_shape_1)\n# cropped = keras.layers.Flatten()(cropped)\nflattened = keras.layers.Flatten()(input_shape_1)\nmerged = keras.layers.Concatenate(axis=1)([merged, flattened, input_shape_2])\nout = keras.layers.Dense(150, activation='relu', kernel_initializer='random_normal')(merged)\nout = keras.layers.Dropout(0.2)(out)\nout = keras.layers.Dense(80, activation='relu', kernel_initializer='random_normal')(out)\nout = keras.layers.Dropout(0.5)(out)\nout = keras.layers.Dense(7, activation='linear', kernel_initializer='random_normal')(out)\n\nmodel = keras.models.Model([input_shape_1, input_shape_2], out)\n\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n\n# Finally, we configure and compile our agent. You can use every built-in Keras optimizer and\n# even the metrics!\nmemory = SequentialMemory(limit=50000, window_length=1)\npolicy = BoltzmannQPolicy(tau=0.5)    \ndqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=100,\n               target_model_update=1e-2, policy=policy, processor=CustomProcessor())\ndqn.compile(Adam(lr=1e-3), metrics=['mae'])","0e200bff":"session = tf.Session()\n\nkeras.backend.set_session(session)\ninit = tf.global_variables_initializer()\nsession.run(init)\n\ndef my_agent(obs, conf):\n    import tensorflow as tf\n    import numpy as np\n    import json\n    \n    if 'dqn' in globals():\n        with session.as_default():\n            with session.graph.as_default():\n                q_values = dqn.compute_q_values(obs['board'])\n\n    else:\n        q_net = tf.keras.models.model_from_json('#MODEL')\n        q_net.set_weights(np.array([np.array(a) for a in json.loads('#WEIGHTS')]))\n\n        def process_state_batch(obs_batch):\n            batch = np.array(obs_batch)\n            result = batch.reshape(-1, 6, 7)\n            def group(asd):\n                a, b = asd.copy(), asd.copy()\n                a[a == 1] = 0\n                a[a == 2] = 1\n                b[b == 2] = 0\n                return np.stack((a,b), axis=2)\n            def get_mark(asd):\n                return 1 if np.count_nonzero(asd==2) > np.count_nonzero(asd==1) else 2\n            res1, res2 = np.array([group(a) for a in result]), np.array([get_mark(a) for a in result])\n            return [res1, res2]\n    \n        q_values = q_net.predict(process_state_batch(obs['board']))[0]\n\n    def prune_invalid_actions(board, q_values):\n        for i in range(7):\n            if board[i]:\n                q_values[i] = -1e7\n        return q_values\n\n    def boltzmannize(q_values, tau, clip=(-500., 500.)):\n        nb_actions = q_values.shape[0]\n        exp_values = np.exp(np.clip(q_values \/ tau, clip[0], clip[1]))\n        probs = exp_values \/ np.sum(exp_values)\n        return np.random.choice(range(nb_actions), p=probs)\n\n    return int(np.argmax(prune_invalid_actions(obs['board'], q_values)))\n    # return int(boltzmannize(prune_invalid_actions(obs['board'], q_values), 0.3))\n\nmy_agent({'board': [0]*42, 'mark': 1}, {})","94cc39ff":"# Okay, now it's time to learn something! We visualize the training here for show, but this\n# slows down training quite a lot. You can always safely abort the training prematurely using\n# Ctrl + C.\ndqn.fit(env_wrapper, nb_steps=100000, visualize=False, verbose=1, callbacks=[DifficultyChangeCallback(-1, 2000)])","5a49e510":"import json\nweights_json = json.dumps([w.tolist() for w in dqn.model.get_weights()])\nmodel_json = dqn.model.to_json()\n\nimport inspect\nimport os\nimport base64\n\ndef write_agent_to_file(function, file):\n    with open(file, \"w\") as f:\n        process = CustomProcessor().process_state_batch\n        source_code = inspect.getsource(function)\n        source_code = source_code.replace('#MODEL', model_json)\n        source_code = source_code.replace('#WEIGHTS', weights_json)\n\n        f.write(source_code)\n        print(function, \"written to\", file)\n        \n    \nwrite_agent_to_file(my_agent, \"submission.py\")","029f0d8f":"# Note: Stdout replacement is a temporary workaround.\nimport sys\nfrom submission import my_agent as agent\n\nenv = make(\"connectx\", debug=True)\nenv.run([agent, agent])\nprint(\"Success!\" if env.state[0].status == env.state[1].status == \"DONE\" else \"Failed...\")","d26d19d9":"env_wrapper.set_difficulty(0)\ndqn.test(env_wrapper,nb_episodes=10,visualize=False)","543ed8bf":"def mean_reward(rewards):\n    mr = sum(r[0] for r in rewards) \/ float(len(rewards))\n    return f'{((mr+1)\/2)*100}%'\n    \n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [agent, \"negamax\"], num_episodes=10)))","ebd418db":"env.reset()\nenv.run(['negamax', agent])\nenv.render(mode=\"ipython\", width=500, height=450)","56a1b46b":"# Setup","e25f1a92":"# Model definition","364f87ef":"# Custom classes"}}