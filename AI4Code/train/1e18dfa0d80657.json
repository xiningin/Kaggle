{"cell_type":{"a8de3a10":"code","a8193890":"code","20a9f9b3":"code","f817a004":"code","f2aaaf35":"code","cbb8319f":"code","68386884":"code","5ed281c0":"code","466bf3f0":"code","826f72eb":"code","a389755d":"code","bc168da2":"code","a51d885c":"code","7f0a3be0":"code","994c57fd":"code","b6b154f6":"code","29b944c4":"code","7056a2b8":"code","ea5df12d":"code","6c67bc7d":"code","330fece1":"code","6b6b8d79":"code","2456a43d":"code","ab8b2c97":"code","c57d39dc":"code","7464c51c":"code","6ff30047":"code","46b998e2":"code","27a2d5e5":"code","ac038a7a":"code","19e24964":"code","19a6dcfb":"code","68228a27":"code","8fa61039":"code","0450d25f":"code","98c3ff55":"code","65ec5f1a":"code","50884b53":"code","3ce829bb":"code","82fac900":"code","db2a679f":"code","bf0ba6cb":"code","a2cf0b0e":"code","1b800531":"code","e8364ba3":"markdown","899c3c62":"markdown","3ef83f5f":"markdown","20cd0d62":"markdown","83be0ec4":"markdown","30b7bfb0":"markdown","5da00708":"markdown","a49a4165":"markdown","71b953b5":"markdown","96b40d1a":"markdown","99de57d9":"markdown","95e837d0":"markdown","fe13ee70":"markdown","e036a791":"markdown","9a3e810c":"markdown","a39ad32b":"markdown","ab7ac3e8":"markdown","ee1a6eaf":"markdown","dc51c8d8":"markdown","a079d1a0":"markdown","411e68f3":"markdown","3df16e73":"markdown","27906003":"markdown","a2e34d03":"markdown","39eb6aab":"markdown"},"source":{"a8de3a10":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n\nfrom multiprocessing import cpu_count\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.stats import pearsonr\nfrom PIL import Image\n\nimport glob\nimport sys\nimport cv2\nimport imageio\nimport joblib\nimport math\nimport warnings\nimport os\nimport torch\nimport imagehash\n\n# Ignore Warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Activate pandas progress apply bar\ntqdm.pandas()\n\nprint(f'tensorflow version: {tf.__version__}')\nprint(f'tensorflow keras version: {tf.keras.__version__}')\nprint(f'python version: P{sys.version}')","a8193890":"# Efficient Data Types\ndtype = {\n    'Id': 'string',\n    'Subject Focus': np.uint8, 'Eyes': np.uint8, 'Face': np.uint8, 'Near': np.uint8,\n    'Action': np.uint8, 'Accessory': np.uint8, 'Group': np.uint8, 'Collage': np.uint8,\n    'Human': np.uint8, 'Occlusion': np.uint8, 'Info': np.uint8, 'Blur': np.uint8,\n    'Pawpularity': np.uint8,\n}\n\ntrain = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/train.csv', dtype=dtype)\ntest = pd.read_csv('\/kaggle\/input\/petfinder-pawpularity-score\/test.csv', dtype=dtype)","20a9f9b3":"# Add File path to Train\ndef get_image_file_path(image_id):\n    return f'\/kaggle\/input\/petfinder-pawpularity-score\/train\/{image_id}.jpg'\n\ntrain['file_path'] = train['Id'].apply(get_image_file_path)","f817a004":"display(train.head())","f2aaaf35":"display(train.info())","cbb8319f":"display(test.head())","68386884":"display(test.info())","5ed281c0":"# Example of cryptographic hashes\nprint(f'hash of 42: {hash(\"aaaaa\")}, hash 37: {hash(\"aaaab\")}')","466bf3f0":"# Return the perceptual hash\ndef get_hash(file_path):\n    img = Image.open(file_path)\n    img_hash = imagehash.phash(img)\n    \n    return img_hash.hash.reshape(-1).astype(np.uint8)\n    \ntrain['phash'] = train['file_path'].progress_apply(get_hash)","826f72eb":"def find_similar_images(threshold=0.90):\n    # Number of Duplicate Images Found\n    duplicate_counter = 1\n    # Indices of Duplicate Images\n    duplicate_idxs = set()\n    # For each image in the train dataset\n    for idx, phash in enumerate(tqdm(train['phash'])):\n        # Compute the similarity to all other images\n        for idx_other, phash_other in enumerate(train['phash']):\n            # Similarity score is imply the percentage of equal bits\n            similarity = (phash ==phash_other).mean()\n            # Prevent self comparison, threshold similarity and ignore repetetive duplicate detection\n            if idx != idx_other and similarity > threshold and not(duplicate_idxs.intersection([idx, idx_other])):\n                # Update Duplicate Indices\n                duplicate_idxs.update([idx, idx_other])\n                # Get DataFrame rows\n                row = train.loc[idx]\n                row_other = train.loc[idx_other]\n                # Plot Duplicate Images\n                fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(8,5))\n                ax[0].imshow(imageio.imread(row['file_path']))\n                ax[0].set_title(f'Idx: {idx}, Pawpularity: {row[\"Pawpularity\"]}')\n                ax[1].imshow(imageio.imread(row_other['file_path']))\n                ax[1].set_title(f'Idx: {idx_other}, Pawpularity: {row_other[\"Pawpularity\"]}')\n                plt.suptitle(f'{duplicate_counter} | PHASH Similarity: {similarity:.3f}')\n                plt.show()\n                # Increase Duplicate Counter\n                duplicate_counter += 1\n                \n    # Return Indices of Duplicates\n    return duplicate_idxs\n    \nduplicate_idxs = find_similar_images()","a389755d":"print(f'Found {len(duplicate_idxs)} Duplicate Images')\n# Removing Duplicate Images\ntrain = train.drop(duplicate_idxs).reset_index(drop=True)","bc168da2":"# Check if images are correctly removed\n# DataFrame size reduced by 27*2=54 from 9912 -> 9858\ndisplay(train.info())","a51d885c":"widths = []\nheights = []\nratios = []\nfor file_path in tqdm(train['file_path']):\n    image = imageio.imread(file_path)\n    h, w, _ = image.shape\n    heights.append(h)\n    widths.append(w)\n    ratios.append(w \/ h)","7f0a3be0":"# Images Heigt and Width Distribution\nprint('Width Statistics')\ndisplay(pd.Series(widths).describe())\nprint()\nprint('Height Statistics')\ndisplay(pd.Series(heights).describe())\n\nplt.figure(figsize=(15,8))\nplt.title(f'Images Height and Width Distribution', size=24)\nplt.hist(heights, bins=32, label='Image Heights')\nplt.hist(widths, bins=32, label='Image Widths')\nplt.legend(prop={'size': 16})\nplt.show()","994c57fd":"# Images Ratio Distribution\nprint('Ratio Statistics')\ndisplay(pd.Series(ratios).describe())\nplt.figure(figsize=(15,8))\nplt.title(f'Images Ratio Distribution', size=24)\nplt.hist(ratios, bins=16, label='Image Heights')\nplt.legend(prop={'size': 16})\nplt.show()","b6b154f6":"# Pawpularity Score Distribution\nprint('Pawpularity Statistics')\ndisplay(train['Pawpularity'].describe())\nplt.figure(figsize=(15,8))\nplt.title('Train Data Pawpularity Score Distribution', size=24)\nplt.hist(train['Pawpularity'], bins=32)\nplt.show()","29b944c4":"def box_plot(feature):\n    value_counts = train[feature].value_counts().sort_index()\n    value_counts_str = \"\"\n    if len(value_counts) == 2: # Binary Feature\n        for idx, (k, v) in enumerate(value_counts.to_dict().items()):\n            if idx > 0:\n                value_counts_str += ', '\n            value_counts_str += f'{k} count: {v}'\n    else: # Non-Binary Feature\n        display(value_counts.to_frame())\n        \n    fig, ax = plt.subplots(figsize=(12,8))\n    ax.set_title(f'{f.upper()}, {value_counts_str}', size=18)\n    ax.boxplot(train.groupby(feature)['Pawpularity'].apply(list))\n    plt.xticks(np.arange(1, len(value_counts) + 1), value_counts.keys(), size=16)\n    plt.yticks(size=16)\n    plt.grid()\n    plt.show()","7056a2b8":"def scatter_plot_with_correlation_line(df, feature):\n    x = df['Pawpularity']\n    y = df[feature]\n    \n    # Create scatter plot\n    plt.figure(figsize=(12,5))\n    plt.scatter(x, y, s=10)\n\n    # Add correlation line\n    axes = plt.gca()\n    m, b = np.polyfit(x, y, 1)\n    X_plot = np.linspace(axes.get_xlim()[0],axes.get_xlim()[1],100)\n    \n    # Pearson Correlation\n    cor, _ = pearsonr(x, y)\n    plt.title(f'{feature}, Pearson Correlation: {cor:.3f}', size=18)\n    plt.plot(X_plot, m*X_plot + b, '-', color='r', linewidth=3)","ea5df12d":"features = [\n    'Subject Focus',\n    'Eyes',\n    'Face',\n    'Near',\n    'Action',\n    'Accessory',\n    'Group',\n    'Collage',\n    'Human',\n    'Occlusion',\n    'Info',\n    'Blur',\n]\n\nfor f in features:\n    box_plot(f)","6c67bc7d":"# Shows a batch of images\ndef show_batch_df(df, rows=8, cols=4):\n    df = df.copy().reset_index()\n    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(cols*4, rows*4))\n    for r in range(rows):\n        for c in range(cols):\n            idx = r * cols + c\n            img = imageio.imread(df.loc[idx, 'file_path'])\n            axes[r, c].imshow(img)\n            axes[r, c].set_title(f'{idx}, label: {df.loc[idx, \"Pawpularity\"]}')","330fece1":"show_batch_df(train.sort_values('Pawpularity'))","6b6b8d79":"show_batch_df(train.sort_values('Pawpularity', ascending=False))","2456a43d":"# Download YOLOV5 GitHub Repo\n!git clone https:\/\/github.com\/ultralytics\/yolov5","ab8b2c97":"# Load Best Performing YOLOV5X Model\nyolov5x6_model = torch.hub.load('ultralytics\/yolov5', 'yolov5x6')","c57d39dc":"# Get Image Info\ndef get_image_info(file_path, plot=False):\n    # Read Image\n    image = imageio.imread(file_path)\n    h, w, c = image.shape\n    \n    if plot: # Debug Plots\n        fig, ax = plt.subplots(1, 2, figsize=(8,8))\n        ax[0].set_title('Pets detected in Image', size=16)\n        ax[0].imshow(image)\n        \n    # Get YOLOV5 results using Test Time Augmentation for better result\n    results = yolov5x6_model(image, augment=True)\n    \n    # Mask for pixels containing pets, initially all set to zero\n    pet_pixels = np.zeros(shape=[h, w], dtype=np.uint8)\n    \n    # Dictionary to Save Image Info\n    h, w, _ = image.shape\n    image_info = { \n        'n_pets': 0, # Number of pets in the image\n        'labels': [], # Label assigned to found objects\n        'thresholds': [], # confidence score\n        'coords': [], # coordinates of bounding boxes\n        'x_min': 0, # minimum x coordinate of pet bounding box\n        'x_max': w - 1, # maximum x coordinate of pet bounding box\n        'y_min': 0, # minimum y coordinate of pet bounding box\n        'y_max': h - 1, # maximum x coordinate of pet bounding box\n    }\n    \n    # Save found pets to draw bounding boxes\n    pets_found = []\n    \n    # Save info for each pet\n    for x1, y1, x2, y2, treshold, label in results.xyxy[0].cpu().detach().numpy():\n        label = results.names[int(label)]\n        if label in ['cat', 'dog']:\n            image_info['n_pets'] += 1\n            image_info['labels'].append(label)\n            image_info['thresholds'].append(treshold)\n            image_info['coords'].append(tuple([x1, y1, x2, y2]))\n            image_info['x_min'] = max(x1, image_info['x_min'])\n            image_info['x_max'] = min(x2, image_info['x_max'])\n            image_info['y_min'] = max(y1, image_info['y_min'])\n            image_info['y_max'] = min(y2, image_info['y_max'])\n            \n            # Set pixels containing pets to 1\n            pet_pixels[int(y1):int(y2), int(x1):int(x2)] = 1\n            \n            # Add found pet\n            pets_found.append([x1, x2, y1, y2, label])\n\n    if plot:\n        for x1, x2, y1, y2, label in pets_found:\n            c = 'red' if label == 'dog' else 'blue'\n            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor=c, facecolor='none')\n            # Add the patch to the Axes\n            ax[0].add_patch(rect)\n            ax[0].text(max(25, (x2+x1)\/2), max(25, y1-h*0.02), label, c=c, ha='center', size=14)\n                \n    # Add Pet Ratio in Image\n    image_info['pet_ratio'] = pet_pixels.sum() \/ (h*w)\n\n    if plot:\n        # Show pet pixels\n        ax[1].set_title('Pixels Containing Pets', size=16)\n        ax[1].imshow(pet_pixels)\n        plt.show()\n        \n    return image_info","7464c51c":"for file_path in train['file_path'].head(5):\n    get_image_info(file_path, plot=True)","6ff30047":"# Image Info\nIMAGES_INFO = {\n    'n_pets': [],\n    'label': [],\n    'coords': [],\n    'x_min': [],\n    'x_max': [],\n    'y_min': [],\n    'y_max': [],\n    'pet_ratio': [],\n}\n\n\nfor idx, file_path in enumerate(tqdm(train['file_path'])):\n    image_info = get_image_info(file_path, plot=False)\n    \n    IMAGES_INFO['n_pets'].append(image_info['n_pets'])\n    IMAGES_INFO['coords'].append(image_info['coords'])\n    IMAGES_INFO['x_min'].append(image_info['x_min'])\n    IMAGES_INFO['x_max'].append(image_info['x_max'])\n    IMAGES_INFO['y_min'].append(image_info['y_min'])\n    IMAGES_INFO['y_max'].append(image_info['y_max'])\n    IMAGES_INFO['pet_ratio'].append(image_info['pet_ratio'])\n    \n    # Not Every Image can be Correctly Classified\n    labels = image_info['labels']\n    if len(set(labels)) == 1: # unanimous label\n        IMAGES_INFO['label'].append(labels[0])\n    elif len(set(labels)) > 1: # Get label with highest confidence\n        IMAGES_INFO['label'].append(labels[0])\n    else: # unknown label, yolo could not find pet\n        IMAGES_INFO['label'].append('unknown')","46b998e2":"# Add Image Info to Train\nfor k, v in IMAGES_INFO.items():\n    train[k] = v","27a2d5e5":"# Image with 14 pets in it\nget_image_info(train.loc[train['n_pets'] == 14, 'file_path'].squeeze(), plot=True)\npass","ac038a7a":"box_plot('n_pets')","19e24964":"box_plot('label')","19a6dcfb":"# Show images where no pet could be detected\nshow_batch_df(train.loc[train['label'] == 'unknown'], rows=6)","68228a27":"# Pearson Correlation between per_ratio and Pawpularity\nscatter_plot_with_correlation_line(train, 'pet_ratio')","8fa61039":"pd.options.display.max_columns = 99\ndisplay(train.head())","0450d25f":"# Save Train\ntrain.to_pickle('train.pkl')","98c3ff55":"N_CHANNELS = 3\nVERSION = '1A'","65ec5f1a":"def process_image(file_path):\n    # Read Image\n    img = imageio.imread(file_path)\n    h, w, _ = img.shape\n\n    with open(file_path, 'rb') as f:\n        img_jpeg = f.read()\n    return img_jpeg, h, w","50884b53":"# Makes the actual TFRecords\ndef to_tf_records(data_split, subset, fold):\n    try:\n        os.makedirs(f'fold_{fold}\/{subset}')\n    except:\n        pass\n    \n    for idx, df_t in enumerate(tqdm(data_split)):\n        df = df_t.T\n        # Create image processing jobs and execute them in parallel\n        jobs = [joblib.delayed(process_image)(fp) for fp in df['file_path']]\n        imgs_resized = joblib.Parallel(\n            n_jobs=cpu_count(),\n            verbose=0,\n            batch_size=64,\n            pre_dispatch=64*cpu_count(),\n            require='sharedmem'\n        )(jobs)\n        tfrecord_name = f'{VERSION}_{subset}_fold_{fold}_batch_{idx}.tfrecords'\n        \n        # Create the actual TFRecords\n        with tf.io.TFRecordWriter(f'fold_{fold}\/{subset}\/{tfrecord_name}') as file_writer:\n            for (idx, row), (img, h, w) in zip(df.iterrows(), imgs_resized):\n                record_bytes = tf.train.Example(features=tf.train.Features(feature={\n                    # Image as JPEG bytes\n                    'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img])),\n                    # Label of image\n                    'label': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(row['Pawpularity'])])),\n                    # Height of image\n                    'height': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(h)])),\n                    # Width of image\n                    'width': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(w)])),\n                    # Minimum x value of pets\n                    'x_min': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(np.floor(row['x_min']))])),\n                    # Maximum x value of pets\n                    'x_max': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(np.floor(row['x_max']))])),\n                    # Minimum y value of pets\n                    'y_min': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(np.floor(row['y_min']))])),\n                    # Maximum y value of pets\n                    'y_max': tf.train.Feature(int64_list=tf.train.Int64List(value=[int(np.floor(row['y_max']))])),\n                })).SerializeToString()\n                file_writer.write(record_bytes)","3ce829bb":"N_KFOLDS = 4\nskf = StratifiedKFold(n_splits=N_KFOLDS, shuffle=True, random_state=42)\n\n# Make Fold Indices\nfold_idxs = skf.split(train['file_path'], train['Pawpularity'])\n\n# Create TFRecords for each Fold\nfor fold, (train_idxs, val_idxs) in enumerate(tqdm(fold_idxs, total=N_KFOLDS)):\n    print(f'Making TFRecords for Fold {fold}')\n    \n    # Train Data\n    train_fold_data = train.loc[train_idxs].T\n    train_fold_chunks = np.array_split(train_fold_data, 8 * 10, axis=1)\n    \n    # Val Data\n    val_fold_data = train.loc[val_idxs].T\n    val_fold_chunks = np.array_split(val_fold_data, 8 * 10, axis=1)\n\n    print(f'train_fold_chunks: {len(train_fold_chunks)}, val_fold_chunks: {len(val_fold_chunks)}')\n    \n    # Create TFRecords\n    to_tf_records(train_fold_chunks, 'train', fold)\n    to_tf_records(val_fold_chunks, 'val', fold)","82fac900":"IMG_SIZE = 512\n\n# Imagenet mean and standard deviation per channel\nIMAGENET_MEAN = tf.constant([0.485, 0.456, 0.406], dtype=tf.float32)\nIMAGENET_STD = tf.constant([0.229, 0.224, 0.225], dtype=tf.float32)\n\n# Number of channels, 3 for RGB images\nN_CHANNELS = tf.constant(3, dtype=tf.int64)","db2a679f":"# Function to decode the TFRecords\ndef decode_tfrecord(record_bytes):\n    features = tf.io.parse_single_example(record_bytes, {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'label': tf.io.FixedLenFeature([], tf.int64),\n        'width': tf.io.FixedLenFeature([], tf.int64),\n        'height': tf.io.FixedLenFeature([], tf.int64),\n    })\n\n    image = tf.io.decode_jpeg(features['image'])\n    label = features['label']\n    height = features['height']\n    width = features['width']\n    \n    # Cutout Random Square if image is not square\n    if height != width:\n        if height > width:\n            offset = (height - width) \/\/ 2\n            image = tf.slice(image, [offset, 0, 0], [width, width, N_CHANNELS])\n        else:\n            offset = (width - height) \/\/ 2\n            image = tf.slice(image, [0, offset, 0], [height, height, N_CHANNELS])\n    \n    # Reshape and Normalize\n    size = tf.math.reduce_min([height, width])\n    # Explicit reshape needed for TPU, tell cimpiler dimensions of image\n    image = tf.reshape(image, [size, size, N_CHANNELS])\n    # Some images are smaller than 384x384 and need to be upscaled\n    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n    # Convert to float32 and normalize to range 0-1\n    image = tf.cast(image, tf.float32)  \/ 255.0\n    # Normalize according to ImageNet mean and standard deviation\n    image = (image - IMAGENET_MEAN) \/ IMAGENET_STD\n    \n    return image, label","bf0ba6cb":"# Shows a batch of images\ndef show_batch(dataset, rows=8, cols=4):\n    imgs, lbls = next(iter(dataset))\n    fig, axes = plt.subplots(nrows=rows, ncols=cols, figsize=(cols*4, rows*4))\n    for r in range(rows):\n        for c in range(cols):\n            img = imgs[r*cols+c].numpy().astype(np.float32)\n            img += abs(img.min())\n            img \/= img.max()\n            axes[r, c].imshow(img)\n            axes[r, c].set_title(f'Label: {lbls[r*cols+c]}')","a2cf0b0e":"# Makes a TFRecordDataser iterator\ndef get_train_dataset():\n    FNAMES_TRAIN_TFRECORDS = tf.io.gfile.glob('.\/*\/*\/*.tfrecords')\n    train_dataset = tf.data.TFRecordDataset(FNAMES_TRAIN_TFRECORDS, num_parallel_reads=1)\n    train_dataset = train_dataset.map(decode_tfrecord, num_parallel_calls=1)\n    train_dataset = train_dataset.batch(32)\n    \n    return train_dataset","1b800531":"# Sanity Check, plot some images from the freshly created TFRecords\ntrain_dataset = get_train_dataset()\nshow_batch(train_dataset)","e8364ba3":"# Feature Correlation","899c3c62":"Images tend to be large for an image classification task, the most common image size is 960*720.","3ef83f5f":"[YOLOV5](https:\/\/github.com\/ultralytics\/yolov5) is the fifth iteration of the Yo Only Look Once object detection famility, which is quite controversial as no official paper has been published. It is however freely available, easy to use and scores fairly high in benchmarks.\n\nUsing object detection the images can be classified as either cat or dog, the contours of the pets can be determined and the amount of pets in the images can be counted.\n\nThis object detection can be a source of features and a fundamental tool for preprocessing.|","20cd0d62":"# Pets with Highest Pawpularity Scores","83be0ec4":"# YOLOV5 Object Detection","30b7bfb0":"# Check TFRecords","5da00708":"# TFRecords","a49a4165":"# Stratified KFold","71b953b5":"Hello Kagglers,\n\nThis notebook gives a brief Exploratory Data Analysis, followed by object detection using [YOLOV5](https:\/\/github.com\/ultralytics\/yolov5) and lastly creating [TFRecords](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/TFRecordDataset).\n\nObject detection is applied to classify the image as cat or dog and to find the contours of the pets inside the image. This could be used for cropping or to extract features. The TFRecords function for fast and easy data processing, especially when using TPU's thousands of images per second can be read.\n\n**V2 Updates**\n\n* Added Test Time Augmentations during YOLOV5 inference for better performance. Reduced the amount of images where no pet could be detected from 50 -> 26.\n\n* Added pet ratio feature. This denotes the percentage in pixels of an image containing a pet.\n\n**V3 Updates**\n\n* Added duplicate removal based on the amazing [post](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/278497) and [notebook](https:\/\/www.kaggle.com\/schulta\/petfinder-identify-duplicates-and-share-findings) from [schuta](https:\/\/www.kaggle.com\/schulta)","96b40d1a":"# YOLOV5 Feature Analysis","99de57d9":"# Remove Duplicates\n\nBased on the amazing [post](https:\/\/www.kaggle.com\/c\/petfinder-pawpularity-score\/discussion\/278497) and [notebook](https:\/\/www.kaggle.com\/schulta\/petfinder-identify-duplicates-and-share-findings) from [schuta](https:\/\/www.kaggle.com\/schulta). More info on image hashes can be found in the [ImageHash documentation](http:\/\/www.hackerfactor.com\/blog\/index.php?\/archives\/432-Looks-Like-It.html). A cryptographic hash maps a file of arbitrary length to a unique vector, where changing a single bit in the input file results in a new arbitrary vector as shown below. Image hashes on the other hand aim to generate similar vectors for similarly looking images. The image hash generates a hash of 256 bits, where similarity can be simply computed as the percentage of identical bits in the vectors of two images.\n\nThe perceptual hash is used for generating the fingerprint of images. The [pHash website](http:\/\/www.phash.org\/) explains the difference between cryptographic and image hashes once more:\n\n```\n\"A perceptual hash is a fingerprint of a multimedia file derived from various features from its content. Unlike cryptographic hash functions which rely on the avalanche effect of small changes in input leading to drastic changes in the output, perceptual hashes are \"close\" to one another if the features are similar.\"\n```","95e837d0":"# Read Train Test","fe13ee70":"[TFRecords](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/TFRecordDataset) allow for packing multiple samples with corresponding label and features inside one file. This speeds up data loading, as only a single file needs to be read from disk which contains multiple images, features and labels.","e036a791":"# Make TFRecords","9a3e810c":"# Image EDA","a39ad32b":"The relation between the given binary features and the Pawpularity score is visualised using box plots. As can be observed, the feature have no visual correlation with the Pawpularity score.","ab7ac3e8":"The following two plots show 32 examples of the lowest and highest scoring pet images. Personally, there is no clear difference between the low and high scoring images. For example, low scoring images 10, 13 and 19 seem quite cute, whereas high scoring image 2 and 13 do not seem like good pictures.","ee1a6eaf":"The image width to height ratio have a mean below zero and a peak on 0.75, pictures thus tend to be taken vertically, not horizontally.","dc51c8d8":"# Some YOLOV5 Examples","a079d1a0":"# Pets with Lowest Pawpularity Scores","411e68f3":"The training data is split into 4 folds, stratified on the Pawpularity score. Each fold the train and validation data will have the same Pawpularity distribution, to not make biased models.","3df16e73":"The number of pets identified in an image ranges from 0, when the pet can not be detected, to 14! For 50 out of 9912 images the pet could not be detected, about 0.5%.","27906003":"The pawpularity score is centered around 40 and has a peak on 0 and 100.","a2e34d03":"# Save Train","39eb6aab":"# Show Extreme Cases"}}