{"cell_type":{"5decdfde":"code","400d05b2":"code","585bb1ac":"code","2b20b70a":"code","8655c0ef":"code","1795b4dc":"code","90d86c8a":"code","dc614464":"code","856bc88d":"code","4eb27913":"code","194faadf":"code","d1f1514d":"code","77951b9f":"code","33fb6654":"code","675449e3":"code","12346a43":"code","23db3b21":"code","3edb5a78":"code","9c150bcd":"code","77200fbc":"markdown","ad0abe79":"markdown","eb51c1a0":"markdown","673bc9ee":"markdown","905c63b8":"markdown","1a0d5d4a":"markdown","a54636b5":"markdown","a403eec8":"markdown","973b9c55":"markdown","63dabe33":"markdown","1725f4c2":"markdown","7fed23b1":"markdown","c1c01cc8":"markdown","94d51d35":"markdown","95749f94":"markdown","49578a33":"markdown","6b7316f6":"markdown","0a9d904e":"markdown","09cb8aa3":"markdown","72fa2d48":"markdown","4aebfe0e":"markdown","adabdca7":"markdown","925e7e80":"markdown","75a0180a":"markdown","e53f9086":"markdown","485103ba":"markdown","f9c120a1":"markdown","275f6d6c":"markdown"},"source":{"5decdfde":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","400d05b2":"df=pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","585bb1ac":"#print(df.head(5))\n","2b20b70a":"df.info()","8655c0ef":"#df['TotalCharges'] = df['TotalCharges'].astype(float)\n\n\ndf['TotalCharges'] = pd.to_numeric(df['TotalCharges'],errors='coerce')\n\n#Checking Data Type after making changes\nprint(df.info())","1795b4dc":"#print(df[df['TotalCharges'].isnull()])\n\nsns.distplot(df['TotalCharges'])\nplt.show()\nprint(df['TotalCharges'].skew())","90d86c8a":"#sns.pairplot(df)\n#plt.show()\nfrom scipy.stats import boxcox\n\n#sns.distplot(df['TotalCharges'])\n#df['TotalCharges']=df['TotalCharges'].transform('sqrt')\ndf['TotalCharges'] = boxcox(df['TotalCharges'],0.5)\nprint(df['TotalCharges'].skew())\n\nsns.distplot(df['TotalCharges'])\nplt.show()\n\n\n","dc614464":"sns.distplot(df['MonthlyCharges'])\nplt.show()\nprint(\"Checking Skewness :- \",df['MonthlyCharges'].skew())\n","856bc88d":"# Creating a function to take DF and identify Categorical Varibles and create a crosstab \n#and plot the same.\ndef category_rel_y(df):\n    X=df.columns\n    #print(df[X[1]])\n \n    #print(len(X))\n \n    for i in range(1,len(X)-2):\n        \n    \n        if (df[X[i]].dtype=='object'):\n                fig, axs = plt.subplots(1, 2,figsize=(13,3))\n                tab_values=pd.crosstab(df[X[i]],df.iloc[:,20])\n                #Creating  % of total values cross tab by using 'normalize=True' in normal tab\n                tab_percentage=pd.crosstab(df[X[i]],df.iloc[:,20],normalize=True)\n                #print(tab_percentage)\n                tab_values.plot(kind='bar', stacked=False ,ax=axs[0] )\n                sns.heatmap(tab_percentage,annot=True,cmap='YlGnBu',ax=axs[1])\n                plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=1, hspace=None)\n                #sns.barplot(tab)\n                axs[0].set_title('Actual counts for CHURN output versus '+X[i].upper())\n              \n                plt.title(\"% of total distribution for CHURN and \"+(X[i]).upper()) \n                plt.show()\n                 \n            \n        \n        \n #plt.show()     \ncategory_rel_y(df)   ","4eb27913":"sns.distplot(df['tenure'])\ndf['tenure'].skew()\n\ndef bins(x):\n    if x>=0 and x<=15:\n        return '0-15'\n    elif x>=16 and x<=30:\n        return '16-30'\n    elif x>=31 and x<=45:\n        return '31-45'\n    elif x>=46 and x<=60:\n        return '46-60'\n    elif x>=61 and x<=75:\n        return '61-75'\n    else:\n        return 'Above 80'\ndf['tenure_bins']=df['tenure'].map(bins)\n#print(df[['tenure_bins','tenure']])\n\nx=(pd.crosstab(df['tenure_bins'],df['Churn']))\n\nx.plot(kind='bar', stacked=False)\n\nplt.show()","194faadf":"#sns.heatmap(df.corr(),annot=True,cmap='viridis')\n#plt.show()\n\n\n#Convert Churn into numerical values of 0 and 1 to get correlation with other numerical variables\nchurn=pd.get_dummies(df[\"Churn\"],drop_first=True)\nchurn.rename(columns = {'Yes':'Churn_continuous'}, inplace = True) \ndf=pd.concat([df,churn],axis=1)\n\n\n\ndf_corr=df[['MonthlyCharges','TotalCharges','tenure','Churn_continuous']].corr()\n#create a mask to avoid duplicates in correlation matrix\nmask = np.zeros_like(df_corr, dtype=np.bool)\n#print(mask.shape)\nmask[np.triu_indices_from(mask)] = True\nfig=plt.subplots(figsize=(8,4))\nsns.heatmap(df_corr,annot=True,cmap='viridis',mask=mask)\nplt.show()","d1f1514d":"list=[\"Contract\",\n\"OnlineSecurity\",\n\"InternetService\",\n\"PaymentMethod\",\n\"PaperlessBilling\",\"TechSupport\",\"StreamingMovies\"]\n\nfor i in range(len(list)):\n    fig, ax = plt.subplots(figsize =(8, 4)) \n    sns.violinplot(ax = ax,data=df, x = list[i],  \n                  y = \"MonthlyCharges\", hue=\"Churn\", kind='violin',split=True) \n\n","77951b9f":"X=df['Churn'].value_counts()\nprint(X)\ny=X\/len(df)\nsns.barplot(x=X,y=(X\/len(df))*100)\n\n\n\n\nplt.show()\n","33fb6654":"#print(df.columns)\n# drop 13 rows with null in \"Total charges\"\ndf = df.dropna()\n\n#one-hot encoding\n\nx=df[['gender', 'SeniorCitizen', 'Partner', 'Dependents','tenure',\n        'PhoneService', 'MultipleLines', 'InternetService',\n       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling',\n       'PaymentMethod', 'MonthlyCharges']]\nX=pd.get_dummies(x, drop_first=True )\n#print(X.columns)\nc=X.columns\n\n\nc=X.columns        \n#for i in range(0,len(c)):\n#    if (X[c[i]].dtype=='object'):\n#        X.drop(c[i],axis=1,inplace=True)\n    \n\ny=pd.get_dummies(df['Churn'], drop_first=True)\n#print(y)\n\n\n#print(\"After \",X)\n#print(\"After \",y)\n\n#Applyting SMOTE after doing one-hot encoding and removing null values\n\nimport sklearn.model_selection as model_selection\nX_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, train_size=0.70,test_size=0.30, random_state=101)\n\n        \n\nfrom imblearn.over_sampling import SMOTE\nsmt = SMOTE(random_state = 2)\nX_train, y_train = smt.fit_sample(X_train, y_train)\n\n\nsmote_class=y_train['Yes'].value_counts()\nprint(smote_class)\n#print(X_train.info())\n\n\n","675449e3":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix  \nfrom sklearn.metrics import classification_report  \n\n#from sklearn.preprocessing import StandardScaler\n#SC = StandardScaler()\n#X_train_MC = SC.fit_transform(X_train['MonthlyCharges'])\n#X_train.drop['MonthyCharges']\n#X_test = SC.transform(X_test)\n\n\nclf=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_test)\n\n# we can use predict_proba() as well\n#y_pred=clf.predict_proba(X_test)\n\n\n","12346a43":"\n\n\n\nprint(\"Accuracy:******************\",metrics.accuracy_score(y_test, y_pred))\n\n\n#visualizing confusion matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(clf, X_test, y_test,cmap='viridis')  # doctest: +SKIP\nplt.title(\"*Confusion Matrix*\")\nplt.show()\n\nprint(\"***************Classification report*************\")\nprint(classification_report(y_test, y_pred))\n\n#sns.heatmap(classification_report(y_test, y_pred))\n#plt.show()\n\nfeature_imp = pd.Series(clf.feature_importances_,index=X_train.columns).sort_values(ascending=False)\n#sns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\n#print(feature_imp)\nvisual=feature_imp.reset_index()\n   \n   \nvisual.rename(columns = {'index':'features', 0:'Importance_score'}, inplace = True) \nprint(visual)\nfig=plt.subplots(figsize=(10,15))\nsns.barplot(x = 'Importance_score', y = 'features', data = visual)\nplt.xlabel('Importance_score')\nplt.ylabel('Features')\n#plt.title(\"Visualizing Important Features\")\n\nplt.legend()\nplt.show()","23db3b21":"\n\n\nfrom lightgbm import LGBMClassifier\nmodel = LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n               importance_type='split', learning_rate=0.1, max_depth=-1,\n               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n               n_estimators=500, n_jobs=-1, num_leaves=31, objective=None,\n               random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","3edb5a78":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# define dataset\n\nmodel = XGBClassifier(silent=False, \n                      scale_pos_weight=1,\n                      learning_rate=0.1,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=1000, \n                      reg_alpha = 0.3,\n                      max_depth=4, \n                      gamma=10)\nmodel.fit(X_train, y_train)\ny_pred=model.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n\n","9c150bcd":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dropout,Dense,Flatten\n\nmodel=Sequential()\n\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(.4))\nmodel.add(Dense(64,activation='relu'))\n\n\n\nmodel.add(Dense(1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\n\nfrom tensorflow.keras.callbacks import EarlyStopping\nearly_stop=EarlyStopping(monitor='val_loss',patience=2)\nresults=model.fit(X_train,y_train,epochs=50,validation_data=(X_test,y_test),callbacks=[early_stop])\n","77200fbc":"# Using **Artificial neural network (ANN)** to check if there is **any improvement in accuracy**","ad0abe79":"# Checking how Good is our model performance!","eb51c1a0":"**we can see from above figure and skewness score that distribution is little better if we take square root of the column, instead of actual values!!**","673bc9ee":"# **Check the target variable for Class Imbalance**","905c63b8":"**After Observing above charts we can see some variables like** \n* \"Contract\"\n* \"OnlineSecurity\"\n* \"InternetService\" \n* \"Payment Method\"\n* \"Paperless Billing\"\n\n\n\n**Can strongly differentiate between Churn and No Churn**","1a0d5d4a":"#  **Checking distribution & skewness of continous variables and trying to improve skewness**","a54636b5":"# **Applying models and evaluting performance**","a403eec8":"True Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes. \n\nTrue Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no. \nFalse positives and false negatives, these values occur when your actual class contradicts with the predicted class.\n\nFalse Positives (FP) \u2013 When actual class is no and predicted class is yes. \nFalse Negatives (FN) \u2013 When actual class is yes but predicted class in no. \n\nOnce you understand these four parameters then we can calculate Accuracy, Precision, Recall and F1 score.\n\nAccuracy - Accuracy is the most intuitive performance measure and it is simply a ratio of correctly predicted observation to the total observations. One may think that, if we have high accuracy then our model is best. Yes, accuracy is a great measure but only when you have symmetric datasets where values of false positive and false negatives are almost same. Therefore, you have to look at other parameters to evaluate the performance of your model. \nAccuracy = TP+TN\/TP+FP+FN+TN\n\nPrecision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answer is of all passengers that labeled as survived, how many actually survived? High precision relates to the low false positive rate. \n\nPrecision = TP\/TP+FP\n\nRecall (Sensitivity) - Recall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. \nRecall = TP\/TP+FN\n\nF1 score - F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it\u2019s better to look at both Precision and Recall.\n\nF1 Score = 2*(Recall * Precision) \/ (Recall + Precision)\n\n\noriginal source Link:-\nStudy for more details with example\n\nhttps:\/\/blog.exsilio.com\/all\/accuracy-precision-recall-f1-score-interpretation-of-performance-measures\/#:~:text=F1%20score%20%2D%20F1%20Score%20is,have%20an%20uneven%20class%20distribution.\n\n","973b9c55":"# Lets check Relation of other important categorical variables with Monthly charges, since its seems to have strong negative relation with Churn","63dabe33":"# Checking **Tenure** relationship with **Churn** by creating bins","1725f4c2":"****1. Random Forest ********","7fed23b1":"**Above figure shows Actual distribution of column \"Total Charges\" **","c1c01cc8":"# **Analyzing all Categorical Varibles with respect to Churn Variable**","94d51d35":"Both Boxplot and violin chart show following Information:-\n*   median (a white dot on the violin plot)\n*   interquartile range (the black bar in the center of violin)\n*   Outliers\n\n![](https:\/\/miro.medium.com\/max\/780\/1*TTMOaNG1o4PgQd-e8LurMg.png)\nSource for image and info:-https:\/\/towardsdatascience.com\/violin-plots-explained-fb1d115e023d\n  \n\n\n**We will use Violin Chart instead of box plot** because of following reasons:-\nAdvantage of the violin plot over the box plot is that aside from showing the \nstatistics like it also shows the entire distribution of the data.\nThis is of interest, especially when dealing with multimodal data, i.e.,\na distribution with more than one peak. \n\n","95749f94":"# Lets Try to improve distribution of this column","49578a33":"# We can from above results that there is no significant improvement even after using ANN.","6b7316f6":"We have 5 different methods for dealing with imbalanced datasets:\n1. Change the performance metric\n1. Change the algorithm\n1. Oversample minority class\n1. Undersample majority class\n1. Generate synthetic samples (SMOTE)\n\nWe will use SMOTE for now\n\n1. Randomly pick a point from the minority class.\n1. Compute the k-nearest neighbors (for some pre-specified k) for this point.\n1. Add k new points somewhere between the chosen point and each of its neighbors","0a9d904e":"# Reading Data and Checking Basic Information","09cb8aa3":"It is visible here that people with less tenure are likely to quit more!","72fa2d48":"****2.Light Gradient Boosting ****","4aebfe0e":"**Since Skewness is already very low we will not take any action on this column!!**","adabdca7":"# * *We can clearly See from above Charts that if Monthly charges are high, there is tendancy for High Churn*\n","925e7e80":"# #Converting \"Total Charges\" to Numeric Column ","75a0180a":"We can see that there is class imbalance!! and we need to fix this.\nMost machine learning algorithms work best when the number of samples in each class are about equal.","e53f9086":"# **We will now check Correlation among numeric variables.**","485103ba":"**We can see above that SMOTE algo has provided eqaul samples for Churn and non Churn! in Training Data**","f9c120a1":"1. **We can see churn has high negative relation with \"Total Charges\" and \"Tenure\", which makes sense because if you are a long tenure customer or Charges are high then you are less like to quit the membership!!**\n \n1. **Also, We should remove \"tenure\" Or \"Total Charges\" from our train model since they seem to have very high Correlation among themselves.**\n1. **But we observe that \"Total Charges\" also have high correaltion with \"Monthly Charges\", So we will remove \"Total Charges\" from our Dataset**","275f6d6c":" **3. Gradient Boosting**"}}