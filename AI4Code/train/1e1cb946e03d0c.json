{"cell_type":{"de483fd8":"code","cd330200":"code","66607086":"code","4d799e98":"code","af00ab14":"code","16541818":"code","5521d8ed":"code","07232913":"code","28524f7d":"code","636e5517":"code","5c0abe74":"code","1ebbb45e":"code","496f0104":"code","3dbdcaa6":"code","856782b7":"code","ad4414d3":"code","3cb2f818":"code","96d15ae5":"code","2cd144f8":"markdown","ad3b0163":"markdown","33bd07a6":"markdown","8bb64377":"markdown","002ad2ee":"markdown","6b12daa5":"markdown","ab170ea1":"markdown"},"source":{"de483fd8":"import os\nimport gc\nimport sys\nimport time\nimport tqdm\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport datatable as dt\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\n\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor, Pool, CatBoost\n\nfrom colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nsr_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport lightgbm as lgb\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\nfrom sklearn.preprocessing import RobustScaler, QuantileTransformer, StandardScaler,PowerTransformer\nfrom sklearn.decomposition import PCA","cd330200":"path = '..\/input\/tabular-playground-series-jan-2021\/'\ntrain_data = pd.read_csv(path + 'train.csv')\ntest_data = pd.read_csv(path + 'test.csv')\nsample = pd.read_csv(path + 'sample_submission.csv')","66607086":"train_data.head()","4d799e98":"train_data['cont13_cont4_mul'] = train_data['cont13']*train_data['cont4']\ntrain_data['cont13_cont11_mul'] = train_data['cont13']*train_data['cont11']\ntrain_data['cont13_cont7_mul'] = train_data['cont13']*train_data['cont7']\ntrain_data['cont13_cont2_mul'] = train_data['cont13']*train_data['cont2']\ntrain_data['cont13_cont10_mul'] = train_data['cont13']*train_data['cont10']\n\ntest_data['cont13_cont4_mul'] = test_data['cont13']*test_data['cont4']\ntest_data['cont13_cont11_mul'] = test_data['cont13']*test_data['cont11']\ntest_data['cont13_cont7_mul'] = test_data['cont13']*test_data['cont7']\ntest_data['cont13_cont2_mul'] = test_data['cont13']*test_data['cont2']\ntest_data['cont13_cont10_mul'] = test_data['cont13']*test_data['cont10']","af00ab14":"## stratified k-fold for regression data\nnum_bins = int(1 + np.log2(len(train_data)))\ntrain_data.loc[:,'bins'] = pd.cut(train_data['target'].to_numpy(),bins=num_bins,labels=False)\n\nfeatures = [f'cont{x}' for x in range(1,15)]\nfeatures += [\n    'cont13_cont4_mul',\n    'cont13_cont11_mul',\n    'cont13_cont7_mul',\n    'cont13_cont2_mul',\n    'cont13_cont10_mul',\n]\n\ntarget_feature = 'target'\n\ntrain_data = train_data.query('target >=5')\nbins = train_data['bins'].to_numpy()\n\ntarget = train_data[target_feature].to_numpy()\ntrain_data = train_data[features].to_numpy()\ntest_data = test_data[features].to_numpy()\n\nscaler = PowerTransformer()\ntrain_data = scaler.fit_transform(train_data)\ntest_data = scaler.transform(test_data)","16541818":"def rmse_score(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","5521d8ed":"seed = 2021\nnfolds = 5\n\nparams={    \n 'objective':'regression',\n 'metrics':'rmse',\n 'boosting':'gbdt',\n 'min_data_per_group': 5,\n 'num_leaves': 256,\n 'max_depth': -1,\n 'learning_rate': 0.005,\n 'subsample_for_bin': 200000,\n 'lambda_l1': 1.074622455507616e-05,\n 'lambda_l2': 2.0521330798729704e-06,\n 'n_jobs': -1,\n 'cat_smooth': 1.0,\n 'silent': True,\n 'importance_type': 'gain',\n 'feature_pre_filter': False,\n 'bagging_fraction': 0.8206341150202605,\n 'min_data_in_leaf': 100,\n 'min_sum_hessian_in_leaf': 0.001,\n 'bagging_freq': 6,\n 'feature_fraction': 0.5,\n 'min_gain_to_split': 0.0,\n 'min_child_samples': 20}","07232913":"lgbm_preds = np.zeros(test_data.shape[0])\n\nkfold = StratifiedKFold(n_splits=nfolds,random_state=seed)\nlgbm_scores = list()\nfor train_idx, valid_idx in kfold.split(X=train_data,y=bins):\n    lgb_train = lgb.Dataset(train_data[train_idx],target[train_idx])\n    lgb_valid = lgb.Dataset(train_data[valid_idx],target[valid_idx],reference=lgb_train)\n    lgb_model = lgb.train(params,\n                      lgb_train, \n                      valid_sets=[lgb_train,lgb_valid],\n                      num_boost_round=10000,\n                      verbose_eval=200,\n                      early_stopping_rounds=100,\n                      )\n    lgbm_scores.append(rmse_score(target[valid_idx],lgb_model.predict(train_data[valid_idx])))\n    lgbm_preds += lgb_model.predict(test_data)\/nfolds\n\nprint(\"mean rmse score\",np.mean(lgbm_scores))","28524f7d":"def plot_feature_importance(model):\n    feature_importance = pd.DataFrame({\"feature\":features,\"importance\":model.feature_importance(importance_type='gain')})\n    feature_importance = feature_importance.sort_values(by='importance',ascending=False)\n    \n    plt.figure(figsize=(10,10))\n    plt.subplot(211)\n    sns.barplot(data=feature_importance,x='importance',y='feature')\n    \n    for idx, v in enumerate(feature_importance.importance):\n            plt.text(v, idx, \"  {:.2e}\".format(v))\n    \n    feature_importance = pd.DataFrame({\"feature\":features,\"importance\":model.feature_importance(importance_type='split')})\n    feature_importance = feature_importance.sort_values(by='importance',ascending=False)\n    \n    plt.subplot(212)\n    sns.barplot(data=feature_importance,x='importance',y='feature')\n    \n    for idx, v in enumerate(feature_importance.importance):\n            plt.text(v, idx, \"  {:.2e}\".format(v))","636e5517":"plot_feature_importance(lgb_model)","5c0abe74":"params = {'lambda': 0.0030282073258141168, \n         'alpha': 0.01563845128469084,\n         'colsample_bytree': 0.55,\n         'subsample': 0.7,\n         'learning_rate': 0.01,\n         'max_depth': 15,\n         'random_state': 2020, \n         'min_child_weight': 257,\n         }","1ebbb45e":"xgb_preds = np.zeros(test_data.shape[0])\n\nkfold = StratifiedKFold(n_splits=nfolds,random_state=seed)\nparams['random_state'] = seed\nxgb_scores = list()\nfor train_idx, valid_idx in kfold.split(X=train_data,y=bins):\n    xgb_train = xgb.DMatrix(train_data[train_idx],label=target[train_idx])\n    xgb_valid = xgb.DMatrix(train_data[valid_idx],label=target[valid_idx])\n\n    xgb_model = xgb.train(params,\n                    xgb_train,\n                    10000,\n                    verbose_eval=200,\n                    evals=[(xgb_train,'train'),(xgb_valid,'valid')],\n                    early_stopping_rounds=100)\n    xgb_scores.append(rmse_score(target[valid_idx],xgb_model.predict(xgb.DMatrix(train_data[valid_idx]))))\n    xgb_preds += xgb_model.predict(xgb.DMatrix(test_data))\/nfolds\n\nprint(\"mean rmse score\",np.mean(xgb_scores))","496f0104":"params = {'l2_leaf_reg': 0.02247766515106271, \n          'max_bin': 364,\n          'subsample': 0.6708650091202213,\n             'learning_rate': 0.010290546311954876,\n          'max_depth': 10,\n           'verbose':200,\n          'random_state': seed, \n          'min_data_in_leaf': 300,\n            'loss_function': 'RMSE',\n          'n_estimators':  25000,\n          'rsm':0.5,\n         'early_stopping_rounds':100}","3dbdcaa6":"cat_preds = np.zeros(test_data.shape[0])\nkfold = StratifiedKFold(n_splits=nfolds,random_state=seed)\nparams['random_state'] = seed\ncat_scores = list()\nfor train_idx, valid_idx in kfold.split(X=train_data,y=bins):\n    cat_train = Pool(train_data[train_idx],target[train_idx])\n    cat_valid = Pool(train_data[valid_idx],target[valid_idx])\n\n    cat_model = CatBoost(params)\n    cat_model.fit(cat_train,eval_set=cat_valid)\n    cat_scores.append(rmse_score(target[valid_idx],cat_model.predict(train_data[valid_idx])))\n    cat_preds += cat_model.predict(test_data)\/nfolds\n    \nprint('mean rmse score:',np.mean(cat_scores))","856782b7":"stacking_preds = pd.read_csv('..\/input\/tps-simple-stacking\/submission.csv')\nstacking_preds = stacking_preds.target.to_numpy()","ad4414d3":"predictions = pd.DataFrame({\"lgbm\":lgbm_preds,\"xgboost\":xgb_preds,'catboost':cat_preds,'stacking':stacking_preds})\nplt.figure(figsize=(7,7))\nsns.heatmap(predictions.corr(),annot=True);","3cb2f818":"sample.target = (0.6 * lgbm_preds.ravel() +  0.2 * xgb_preds.ravel() + 0.1 * cat_preds.ravel() + 0.1 * stacking_preds)\nsample.to_csv(\"submission.csv\",index=False)\nsample.head()","96d15ae5":"plt.figure(figsize=(15,7))\nplt.subplot(131)\nsns.distplot(sample.target)\nplt.title(\"test-target distribution\")\nplt.subplot(132)\nsns.distplot(target)\nplt.title(\"train-target distribution\")\nplt.subplot(133)\nsns.distplot(sample.target.to_numpy(),label='test')\nsns.distplot(target,label='target')\nplt.legend()\nplt.title(\"train and test target distribution\");","2cd144f8":"## XGB","ad3b0163":"## Stacking \n\n[Notebook](https:\/\/www.kaggle.com\/maunish\/tps-simple-stacking)","33bd07a6":"## correlation matrix","8bb64377":"## lgbm model","002ad2ee":"## submission","6b12daa5":"## Catboost","ab170ea1":"## Preprocessing"}}