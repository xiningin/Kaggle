{"cell_type":{"b0a3f6ab":"code","aa3e4e44":"code","3ceab81f":"code","27382e57":"code","d2ce913d":"code","6c5e4220":"code","1d9117ad":"code","065a4a3d":"markdown","9194c7d8":"markdown","e222732c":"markdown","a1287ab8":"markdown","aa137fe4":"markdown","be93f6e8":"markdown","7c53c30e":"markdown","751a8cf0":"markdown"},"source":{"b0a3f6ab":"import gc\nimport os\nimport joblib\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\ntf.random.set_seed(42)\ntf.keras.mixed_precision.experimental.set_policy('mixed_float16')\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n%matplotlib inline\n\npd.set_option('display.max_rows', 10)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nmpl.rcParams['figure.dpi'] = 600\n\nwarnings.filterwarnings('ignore')\ntf.get_logger().setLevel('WARNING')\n\n# ------------------------------------------------------------------------\n\ntrain_df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nsubmission = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv', index_col = 'ImageId')\n\nx = train_df.drop(columns = 'label').values.reshape(-1, 28, 28).astype(np.float32)\ny = train_df['label'].values.astype(np.float32)\nx_test = test_df.values.reshape(-1, 28, 28).astype(np.float32)\n\nn_labels = len(np.unique(y))\n\ndel train_df, test_df\n\ngc.collect()","aa3e4e44":"i = np.random.randint(x.shape[0], size = 100)\n\nfig, axs = plt.subplots(10, 10, sharex = True, sharey = True, figsize = (15, 20))\nplt.subplots_adjust(wspace = .05, hspace = .3)\n\nx_select = x[i]\ny_select = y[i]\n\nfor i in range(10):\n    for j in range(10):\n        k = i * 10 + j\n        axs[i][j].imshow(255 - x_select[k], cmap = 'gray', vmin = 0, vmax = 255)\n        axs[i][j].axes.get_xaxis().set_visible(False)\n        axs[i][j].axes.get_yaxis().set_visible(False)\n        axs[i][j].set_title('%d' % y_select[k])","3ceab81f":"from tensorflow.keras.models import Model\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\n\ndef define_model(input_shape, n_classes, n_conv_branches = 1, dropout = 0.2, reg_alpha = 1e-2):\n    inputs = layers.Input(shape = input_shape)\n    \n    b_in = layers.experimental.preprocessing.Rescaling(1. \/ 255)(inputs)\n    \n    branches = [b_in] * n_conv_branches\n    \n    for i in range(n_conv_branches):\n        for filter_size in [32, 64, 128, 128]:\n            branches[i] = layers.Conv2D(\n                filters = filter_size,\n                kernel_size = 3,\n                padding = 'same',\n                kernel_regularizer = regularizers.L2(reg_alpha),\n                bias_regularizer = regularizers.L2(reg_alpha)\n            )(branches[i])\n            branches[i] = layers.MaxPool2D(pool_size = (2, 2))(branches[i])\n            branches[i] = layers.ReLU()(branches[i])\n            branches[i] = layers.Dropout(dropout)(branches[i])\n    \n    if n_conv_branches > 1:\n        b_out = layers.concatenate(branches)\n        b_out = layers.Flatten()(b_out)\n    else:\n        b_out = layers.Flatten()(branches[0])\n    \n    b_out = layers.Dense(\n        units = 128,\n        kernel_regularizer = regularizers.L2(reg_alpha),\n        bias_regularizer = regularizers.L2(reg_alpha)\n    )(b_out)\n    b_out = layers.BatchNormalization()(b_out)\n    b_out = layers.ReLU()(b_out)\n    b_out = layers.Dropout(dropout)(b_out)\n    \n    outputs = layers.Dense(units = n_classes)(b_out)\n    \n    return Model(inputs, outputs)","27382e57":"model = define_model((x.shape[1], x.shape[2], 1), n_labels, 1, 0.2)\n\ntf.keras.utils.plot_model(model, show_shapes = True, show_layer_names = False)","d2ce913d":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom sklearn.model_selection import StratifiedKFold\n\nN_SPLITS = 10\nCHECKPOINT_DIR = '.\/checkpoint'\n\ncv = StratifiedKFold(n_splits = N_SPLITS, random_state = 42, shuffle = True)\n\noof_pred = np.zeros((x_test.shape[0], n_labels))\ncv_val_scores = np.zeros(N_SPLITS)\nhistories = []\n\nk = 0\n\nx = x.astype(np.float16)\ny = y.astype(np.float16)\n\nfor train_i, val_i in cv.split(x, y):\n    x_train = x[train_i, :]\n    x_valid = x[val_i, :]\n    \n    y_train = y[train_i]\n    y_valid = y[val_i]\n    \n    model = define_model(\n        (x.shape[1], x.shape[2], 1),\n        n_labels,\n        1,\n        0.2,\n        2e-4\n    )\n    \n    gc.collect()\n    \n    optimizer = Adam(\n        learning_rate = 5e-4,\n    )\n    \n    model.compile(\n        optimizer = optimizer,\n        loss = SparseCategoricalCrossentropy(from_logits = True),\n        metrics = ['accuracy']\n    )\n    \n    checkpoint_call = ModelCheckpoint(\n        filepath = CHECKPOINT_DIR,\n        save_weights_only = True,\n        monitor = 'val_accuracy',\n        mode = 'max',\n        save_best_only = True\n    )\n    \n    stopping_call = EarlyStopping(\n        monitor = 'val_accuracy',\n        patience = 50,\n        mode = 'max'\n    )\n    \n    history = model.fit(\n        x_train, y_train,\n        validation_data = (x_valid, y_valid),\n        epochs = 200,\n        callbacks = [checkpoint_call, stopping_call],\n        batch_size = 1024,\n    )\n    \n    histories += [history]\n    \n    model.load_weights(CHECKPOINT_DIR)\n    predictor_model = tf.keras.Sequential([model, layers.Softmax()])\n    \n    cv_val_scores[k] = model.evaluate(x_valid, y_valid)[1]\n    oof_pred += predictor_model.predict(x_test) \/ N_SPLITS\n    \n    k += 1","6c5e4220":"print('Validation AUC: {:.6} \u00b1 {:.4}'.format(cv_val_scores.mean(), cv_val_scores.std()))","1d9117ad":"submission.loc[:, 'Label'] = np.argmax(oof_pred, axis = 1)\nsubmission.to_csv('submission.csv')","065a4a3d":"# Model\n\nThis is an image recognition problem, therefore, CNNs are likely to perform well.","9194c7d8":"## Cross-validation\n\nTo train our model and make predictions, we will use cross validation folds with out-of-fold predictions.","e222732c":"# Preamble\n\nIn the preamble we must import the libraries that we will need, and the data. The training dataset will be imported in the variable ```train_df```, the test dataset in the variable ```test_df```, and the sample submission in the variable ```submission```, that we will use to build the submission file.","a1287ab8":"## 100 random images from the train dataset","aa137fe4":"# Submission","be93f6e8":"Let's plot an instance of our model:","7c53c30e":"## Architecture\n\nIn the next hidden cell a function to create our model is defined.","751a8cf0":"# Explore datasets\n\nThe MNIST dataset consists of images of handwritten digits. A detailed description can be found [here](https:\/\/www.kaggle.com\/c\/digit-recognizer\/data)."}}