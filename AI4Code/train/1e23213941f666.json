{"cell_type":{"75be56c6":"code","21238372":"code","5602ef97":"code","0f951312":"code","e7fa9714":"code","9ff30c70":"code","54244077":"code","d6063e19":"code","bdf52113":"code","9ed4d11b":"code","5ae9c56a":"code","22997f62":"code","cb405b62":"code","ad325d41":"code","43bbff86":"code","97451244":"code","1b9c5f17":"code","dc81fe12":"code","fdf7d1b5":"markdown","56193974":"markdown","6208b182":"markdown","2a4645de":"markdown","9b99787f":"markdown","72895e8f":"markdown","d48864b8":"markdown","4343f6ad":"markdown","f585023e":"markdown","c16f7b22":"markdown","7e039109":"markdown","2dc3538b":"markdown"},"source":{"75be56c6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21238372":"data = pd.read_csv(\"\/kaggle\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\ndata.head()","5602ef97":"Abnormal = data[data[\"class\"]==\"Abnormal\"]\nNormal = data[data[\"class\"]==\"Normal\"]","0f951312":"Abnormal.shape","e7fa9714":"Normal.shape","9ff30c70":"#Veri g\u00f6rselle\u015ftirme\n\nplt.scatter(Abnormal.pelvic_incidence, Abnormal.pelvic_radius,color=\"red\", label=\"AN\")\nplt.scatter(Normal.pelvic_incidence, Normal.pelvic_radius, color=\"green\", label=\"N\")\nplt.legend()\nplt.xlabel(\"Anormal\")\nplt.ylabel(\"Normal\")\nplt.show()","54244077":"data[\"class\"] = [1 if each==\"Abnormal\" else 0 for each in data[\"class\"]]\ndata.head()","d6063e19":"y = data[\"class\"].values\ny[205:220]","bdf52113":"x_data = data.drop([\"class\"], axis=1)\nx_data.head(3)","9ed4d11b":"x = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data))\nx.head()","5ae9c56a":"from sklearn.model_selection import train_test_split","22997f62":"x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.2, random_state=42)","cb405b62":"from sklearn.neighbors import KNeighborsClassifier","ad325d41":"knn = KNeighborsClassifier(n_neighbors=3)","43bbff86":"knn.fit(x_train,y_train)","97451244":"prediction = knn.predict(x_test)","1b9c5f17":"print(\"{} nn score: {}\".format(3, knn.score(x_test,y_test)))","dc81fe12":"score_list=[]\n\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n\nplt.plot(range(1,15), score_list, color=\"green\")\nplt.xlabel(\"k values\")\nplt.ylabel(\"accurarcy\")\nplt.show()","fdf7d1b5":"<a id=\"7\"><\/a>\n## 7. KNN Modeli Olu\u015fturma","56193974":"<a id=\"1\"><\/a>\n## 1. K-Nearest Neighbor (KNN) Intro","6208b182":"<a id=\"5\"><\/a>\n## 5. Normalization","2a4645de":"### KNN Algoritmas\u0131n\u0131n Ad\u0131mlar\u0131","9b99787f":"<a id=\"3\"><\/a>\n## 3.Data Visualization","72895e8f":"* Supervised learning algoritmalar\u0131ndan biri olan KNN algoritmas\u0131, hem classification hem de regression problemlerin \u00e7\u00f6z\u00fcmlerinde kullan\u0131l\u0131r.\n\n* S\u0131n\u0131flar\u0131 belli olan veri k\u00fcmesindeki verilerden faydanarak, k\u00fcmeye kat\u0131lacak yeni de\u011ferin class\u0131n\u0131 bulmakt\u0131r.\n\n* Mevcut verilere g\u00f6re uzakl\u0131\u011f\u0131 *euclidean distance* ile hesaplan\u0131r. \n> Manhattan Distance ve Minkowski Distance fonksiyonlar\u0131 da uzakl\u0131\u011f\u0131 hesaplamada kullan\u0131l\u0131r.\n\n* Euclidean Distance Formul:\n\n![euclidean.png](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/5\/55\/Euclidean_distance_2d.svg\/440px-Euclidean_distance_2d.svg.png)\n\n* Avantaj\u0131 : Basit ve g\u00fcr\u00fclt\u00fcl\u00fc e\u011fitim verilerine kar\u015f\u0131 diren\u00e7li olmas\u0131\n\n* Dezavantaj\u0131 : Uzakl\u0131k hesab\u0131 yaparken b\u00fct\u00fcn durumlar\u0131 saklad\u0131\u011f\u0131ndan, b\u00fcy\u00fck veriler i\u00e7in kullan\u0131ld\u0131\u011f\u0131nda \u00e7ok say\u0131da bellek alan\u0131na gereksinim duymaktad\u0131r.","d48864b8":"<a id=\"4\"><\/a>\n## 4. Datay\u0131 Kullanabilir Hale Getirme","4343f6ad":"#### 1. K de\u011ferini belirle\n#### 2. K'a en yak\u0131n data noktalar\u0131n\u0131 bul\n#### 3. K'a en yak\u0131n datalar\u0131n hangi classtan oldu\u011funu hesapla\n#### 4. Test edilen nokta hangi classtan tespit et","f585023e":"<a id=\"8\"><\/a>\n## 8. En Uygun K De\u011ferini Bulma","c16f7b22":"## Contents\n\n>### [1. K-Nearest Neighbors (KNN)](#1)\n>### [2. Load and Read Data](#2)\n>### [3. Data Visualization](#3)\n>### [4. Datay\u0131 Kullanabilir Hale Getirme](#4)\n>### [5. Normalization](#5)\n>### [6. Train Test Split](#6)\n>### [7. KNN Modeli Olu\u015fturma](#7)\n>### [8. En Uygun K De\u011ferini Bulma](#8)","7e039109":"<a id=\"2\"><\/a>\n## 2. Load and Read Data","2dc3538b":"<a id=\"6\"><\/a>\n## 6. Train Test Split"}}