{"cell_type":{"88d5c4cc":"code","a19c4c1d":"code","321f9508":"code","9417cf6f":"code","0e3ee629":"code","ef4d8157":"code","b9544e3a":"code","d6721ef8":"code","8d41b3cd":"code","4687a57d":"code","effe2d58":"code","ae067249":"code","612c2057":"code","a3e3b804":"code","3651a5da":"code","b2236f48":"code","c63b8a1b":"code","8eb1ccc3":"code","0fdeca4f":"code","02fb01af":"code","269034d6":"code","bf993855":"code","4be5d991":"code","64450e48":"code","3727258e":"code","e10f921a":"code","912f9e8c":"code","633c5bd4":"code","7113a0b8":"code","74102802":"code","839eae23":"code","ba8d5055":"code","344e2f8e":"code","9afa2964":"code","accccf88":"code","f440381a":"code","587a0247":"code","0dbf7411":"code","d9999c8f":"code","75218fe9":"code","eb698551":"code","161d252d":"code","f0360804":"code","b814e11a":"code","93c7c459":"code","283e6ee9":"code","1c2e4a96":"code","2d7c36c7":"code","5bf8ab78":"code","689f4c42":"code","79cec4a2":"code","85d096e0":"code","cd7baaf4":"code","a4d1b3b3":"code","32ef6d7d":"code","24e7d76d":"code","e9930fc6":"code","60464511":"code","8f87c866":"code","0ad686d1":"code","64c50646":"code","6da6fc83":"code","9d8e68ee":"code","d5e11b7c":"code","32b5ab68":"code","2f8cf7e1":"code","6dc96d82":"code","d1ba4e74":"code","60410485":"markdown","8215aa42":"markdown","bf208390":"markdown","040ba98f":"markdown","0dda6195":"markdown","37c0c19d":"markdown","26126e5a":"markdown","00f4b91e":"markdown","372d3e85":"markdown","f562866d":"markdown","10773192":"markdown","987ef0ff":"markdown","7f455791":"markdown","7bc56b28":"markdown","baef75a4":"markdown","85a9d921":"markdown","5a6faadc":"markdown","89edbd09":"markdown","1b811d32":"markdown","a1fd300c":"markdown","5a316146":"markdown","e10f0300":"markdown","d903ce9c":"markdown","4e6e767d":"markdown","7197fcce":"markdown","acdbeb70":"markdown","7511552c":"markdown","9a3d6566":"markdown","b4c5b87b":"markdown","28f7ac72":"markdown","f764d42e":"markdown","578b5f2d":"markdown","ec97d314":"markdown","2d886c99":"markdown","bd415b7f":"markdown","b413cf0c":"markdown","a166565a":"markdown","2dbc8960":"markdown","cd831b77":"markdown","ed19483f":"markdown","2dc0522c":"markdown","b1e58c08":"markdown","b2a14a7a":"markdown","5cbc62c8":"markdown"},"source":{"88d5c4cc":"# General tools\nimport pandas as pd\nimport numpy as np\nimport os, math\nfrom collections import Counter\n\n# Plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nmorancolor=sns.color_palette(['#6a2202', '#bc7201', '#e5ab09', '#22180d', '#0f1a26','#241c24', '#745656', '#c7b44f', '#977f48', '#392c23'])\nplt.style.use(\"fivethirtyeight\")\nsns.set_palette(morancolor)\n\nplt.rcParams['font.family']='serif'\nplt.rcParams['figure.dpi'] =100 # high resolution\n\n# Manage warnings\nimport warnings\nwarnings.filterwarnings('ignore')","a19c4c1d":"df=pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")\ndf=df.drop(columns='Unnamed: 0')\ndf.drop_duplicates(inplace=True)\ndf.dropna(inplace=True,how='all')\ndf.columns=df.columns.str.replace('.','_').str.lower()\ndf.shape","321f9508":"df.sample(3)","9417cf6f":"df.info()","0e3ee629":"df.nunique().sort_values(ascending=False)","ef4d8157":"df.describe()","b9544e3a":"df.isnull().sum().sort_values(ascending=False)","d6721ef8":"def despine():\n    sns.despine(top=1,bottom=1,right=1,left=1)\ndef title(title,fontsize=13):\n    plt.title(title,fontweight='bold',fontsize=fontsize)\n    \ndef countall(df,lst,h=4,w=10,cut=3,hspace=.5,wspace=.25,annotsize=10):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        sns.countplot(df[col])\n        for p in ax.patches:\n            ax.annotate(f\"{p.get_height()\/df[col].shape[0]*100:.2f}%\",xy=[p.get_x(),p.get_height()],fontsize=annotsize)\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold')\n\nfrom scipy.stats import skew\ndef kdeall(df,lst,h=4,w=10,cut=3,hspace=.5,wspace=.25,meanskew=True,kdecut=0,legendsize=10,xlabelsize=13,loc='best'):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        if meanskew==True:\n            sns.kdeplot(df.dropna(subset=[col])[col],cut=kdecut,label=f'Skewness: {skew(df.dropna(subset=[col])[col]):.2f}',lw=3)\n            plt.axvline(df.dropna(subset=[col])[col].mean(),label='mean',color='#22180d',lw=1.5)\n            plt.axvline(df.dropna(subset=[col])[col].median(),label='median',ls='--',color='#22180d',lw=1.5)\n            plt.legend(fontsize=legendsize,loc=loc)\n        else: sns.kdeplot(df.dropna(subset=[col])[col],cut=kdecut,lw=3)\n        sns.rugplot(df.dropna(subset=[col])[col])\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold',fontsize=xlabelsize)\n        \ndef boxall(df,lst,h=4,w=10,cut=3,hspace=.5,wspace=.25,target=None,choose=1,xlabelsize=14,xticksize=12,yticksize=12):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        if target==None: sns.boxplot(df[col])\n        else: \n            if choose==1: sns.boxplot(df[target],df[col])\n            else: sns.boxplot(df[col],df[target])\n        despine()\n        plt.ylabel('')\n        plt.yticks(fontsize=yticksize)\n        plt.xticks(fontsize=xticksize)\n        plt.xlabel(col,fontweight='bold',fontsize=xlabelsize)\n        \ndef pointall(df,lst,target,h=4,w=10,cut=3,hspace=.5,wspace=.25,annotsize=10,choose=1,titlesize=14):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        if choose==1: sns.pointplot(x=df[target],y=df[col],lw=3)\n        else: sns.pointplot(x=df[col],y=df[target],lw=3)\n        despine()\n        plt.ylabel('')\n        plt.xlabel('')\n        plt.title(col,fontweight='bold',fontsize=titlesize)\n\ndef kde2(df,lst,target,h=4,w=10,cut=3,hspace=.5,wspace=.25,legendsize=10,legendlabelsize=12,xlabelsize=13):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        g=sns.kdeplot(df[col],hue=df[target],cut=0,lw=3)\n        plt.setp(g.get_legend().get_texts(), fontsize=legendsize) # for legend text\n        plt.setp(g.get_legend().get_title(), fontsize=legendlabelsize, fontweight='bold') # for legend title\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold',fontsize=xlabelsize)            \n        \ndef barall(df,lst,target,h=4,w=10,cut=3,hspace=.5,wspace=.25,annotsize=10,choose=1,titlesize=13):\n    f=plt.figure(figsize=(w,h))\n    plt.subplots_adjust(hspace=hspace,wspace=wspace)\n    for i,col in enumerate(lst):\n        ax=f.add_subplot(math.ceil(len(lst)\/cut),cut,i+1)\n        if choose==1: sns.barplot(x=df[target],y=df[col],lw=3)\n        else: sns.barplot(x=df[col],y=df[target],lw=3)\n        despine()\n        plt.ylabel('')\n        plt.xlabel(col,fontweight='bold',fontsize=titlesize)        ","8d41b3cd":"countall(df,['quality'],cut=1,h=4,w=7)\ntitle('Distribution of target variable',fontsize=17)","4687a57d":"df['group_quality']=df.quality.apply(lambda x: 0 if x<5 else 1 if x<7 else 3).astype(int)","effe2d58":"countall(df,[\"group_quality\"],cut=1,w=7,h=4)","ae067249":"# let's save our independent features\nfeature=df.drop(\"group_quality\",1).corr().quality.abs().sort_values(ascending=False)[1:].index.tolist()\nfeature","612c2057":"df.drop(\"group_quality\",1).corr().quality.abs().sort_values(ascending=False)[1:][::-1].plot(kind='barh',figsize=(7,5))\nplt.show()","a3e3b804":"kdeall(df,feature,h=14)","3651a5da":"df[feature].boxplot(vert=0,figsize=(7,5))\nplt.xscale('log')","b2236f48":"df.drop(\"quality\",1).corr().group_quality.abs().sort_values(ascending=False)[1:][::-1].plot(kind='barh',figsize=(7,5))\nplt.show()","c63b8a1b":"f=plt.figure(figsize=(8,5))\nmask = np.triu(np.ones_like(df.corr()))\nplt.xticks(fontsize=11)\nplt.yticks(fontsize=11)\nsns.heatmap(df.corr(),cmap='RdBu',vmax=1,vmin=-1,annot=True,fmt='.2f',annot_kws={\"size\":8.2},cbar=False,mask=mask)","8eb1ccc3":"boxall(df,feature,target=\"group_quality\",h=10,cut=4)","0fdeca4f":"barall(df,feature,\"group_quality\",cut=4,h=8)","02fb01af":"kde2(df,feature,\"group_quality\",cut=4,h=9)","269034d6":"sns.pairplot(df.drop('quality',1),hue=\"group_quality\",corner=True,palette=sns.color_palette(morancolor[:3]))","bf993855":"boxall(df,feature,cut=6,xlabelsize=10)","4be5d991":"def outliers(x,df=df,fence=1.5):\n    q1,q3=np.percentile(df[x],25), np.percentile(df[x],75)\n    iqr=q3-q1\n    print(f\"{x}: {len(df[(df[x]<q1-fence*iqr)|(df[x]>q3+1.5*iqr)])} outliers\")\n    return df[(df[x]<q1-fence*iqr)|(df[x]>q3+1.5*iqr)].index.tolist()","64450e48":"outind=list(set([j for i in feature[:2] for j in outliers(i)]))\nprint(f\"\\nWe are going to drop {len(outind)} outliers\")\nprint(f\"Before: {df.shape}\")\ndf.drop(index=outind,inplace=True)\nprint(f\"After: {df.shape}\")","3727258e":"boxall(df,feature[:2],cut=4,h=2)","e10f921a":"kdeall(df,feature,cut=4,h=8)\nplt.suptitle('Before scaling',fontweight='bold')\nplt.show()","912f9e8c":"from sklearn.preprocessing import StandardScaler\ndf[[\"density\",\"ph\"]]=StandardScaler().fit_transform(df[[\"density\",\"ph\"]])","633c5bd4":"sub=list(set(feature)-set(['density','ph']))\nfrom sklearn.preprocessing import normalize\ndf[sub]=normalize(df[sub])","7113a0b8":"kdeall(df,feature,cut=4,h=8)\nplt.suptitle('After scaling',fontweight='bold')\nplt.show()","74102802":"df=df.drop('quality',1)\nX=df.iloc[:,:-1]\ny=df.group_quality\nX.shape,y.shape","839eae23":"from sklearn.decomposition import PCA\ndef reduct_pca(X,df=df):\n    pca=PCA(n_components=X.shape[1],random_state=0)\n    Xpca=pca.fit_transform(X)\n    f=plt.figure(figsize=(10,3))\n    plt.bar(range(1,df.shape[1]),pca.explained_variance_ratio_,label='individual explained variance')\n    plt.step(range(1,df.shape[1]), np.cumsum(pca.explained_variance_ratio_),where='mid',label='cumulative explained variance')\n    plt.xticks(range(1,df.shape[1]))\n    plt.xlabel('# of principal components')\n    plt.ylabel('explained variance ratio',fontsize=13)\n    plt.yticks(fontsize=12)\n    plt.legend(fontsize=13)\n    despine()\n    return Xpca","ba8d5055":"Xpca=pd.DataFrame(reduct_pca(X)[:,:4])\nXpca.shape","344e2f8e":"import statsmodels.api as sm\ndef select_by_pvalue(target,df,fence=.05,w=7,h=5,textw=.5,texth=.2):\n    mod=sm.OLS(df[target],df.drop(target,axis=1))\n    fii=mod.fit()\n    sub=fii.summary2().tables[1][\"P>|t|\"].sort_values()\n    sub.plot(kind='barh',figsize=(w,h))\n    pvalue_set=sub[sub<=fence].index.tolist()\n    plt.axvline(x=fence,c='black')\n    plt.figtext(textw, texth, 'dropped',fontsize=15, fontweight='bold')\n    return pvalue_set","9afa2964":"pvalset=select_by_pvalue(\"group_quality\",df)\npvalset","accccf88":"from statsmodels.tools.tools import add_constant\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\ndef vif(x):\n    X=add_constant(x)\n    return pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])],index=X.columns).sort_values(ascending=False)","f440381a":"vif(X)","587a0247":"vif(X[pvalset])","0dbf7411":"vif(X[pvalset].drop(['total_sulfur_dioxide','residual_sugar'],1))","d9999c8f":"feature","75218fe9":"edaset=feature[:4]\nedaset","eb698551":"vif(X[edaset])","161d252d":"from sklearn.model_selection import StratifiedKFold\ndef check(clf,X,y=y,n_splits=10):\n    kf=StratifiedKFold(n_splits=n_splits,random_state=0,shuffle=True)\n    k_tracc,k_teacc=[],[]\n    for (tr,te) in kf.split(X,y):\n        clf.fit(X.iloc[tr],y.iloc[tr])\n        k_tracc.append(clf.score(X.iloc[tr],y.iloc[tr]))\n        k_teacc.append(clf.score(X.iloc[te],y.iloc[te]))\n    print(f\"Train score: {np.mean(k_tracc)}\")\n    print(f\"Test score: {np.mean(k_teacc)}\")\n    return clf\n\n\ndef selectmoran(modellst,X=X,y=y,n_splits=10,random_state=0):\n    kf=StratifiedKFold(n_splits=n_splits,random_state=random_state,shuffle=True)\n    namelst,imp,tr_acc,te_acc=[],[],[],[]\n    for clf in modellst:\n        namelst.append(type(clf).__name__)\n        k_tracc, k_teacc, k_f1, k_imp=[],[],[],[]\n        for (tr,te) in kf.split(X,y): # train, test index\n            clf.fit(X.iloc[tr],y.iloc[tr])\n            k_tracc.append(clf.score(X.iloc[tr],y.iloc[tr]))\n            k_teacc.append(clf.score(X.iloc[te],y.iloc[te]))\n            if hasattr(clf,\"feature_importances_\"): k_imp.append(clf.feature_importances_)\n        tr_acc.append(np.mean(k_tracc))\n        te_acc.append(np.mean(k_teacc))\n        if len(k_imp)==0: imp.append(False)\n        else: imp.append(np.mean(k_imp,axis=0))\n    score=pd.DataFrame({'Model':namelst,'Train_accuracy':tr_acc,'Test_accuracy':te_acc}).sort_values('Test_accuracy',ascending=False)\n    return score,imp\n\ndef plotscoring(score,title,w=7,h=5,alpha=.97,axvline=.9,yticksize=12):\n    f,ax=plt.subplots(figsize=(w,h))\n    print(f\"Mean accuracy for all models: {np.mean(score.Test_accuracy)}\\n\")\n    print(score)\n    sns.barplot(x=score.Test_accuracy,y=score.Model,alpha=alpha,color='#bc7201')\n    sns.barplot(x=-score.Train_accuracy,y=score.Model,alpha=alpha,color='#6a2202')\n    ax.set_xlim(-1,1)\n    plt.axvline(x=0,color='black')\n    plt.xlabel('Train\/ Test accuracy')\n    plt.ylabel('')\n    plt.title(f\"Accuracy score for {title}\",fontweight='bold')\n    plt.axvline(x=axvline,ls=':')\n    plt.yticks(fontsize=yticksize)\n    despine()\n    \nfrom sklearn.model_selection import GridSearchCV\ndef grid(clf,params,X,y=y,cv=5):\n    grid=GridSearchCV(clf,params,cv=cv)\n    grid.fit(X,y)\n    print(f\"Best score: {grid.best_score_}\")\n    print(f\"Best params: {grid.best_params_}\")\n    return grid.best_estimator_\n\ndef plotting_importances(score,imp,X,w=8,h=3,rotation=90,xsize=10):\n    for (a,b) in zip(score.Model,imp):\n        if b is not False:\n            ind=np.argsort(b)[::-1]\n            cols=X.columns\n            plt.figure(figsize=(w,h))\n            plt.title(f\"Feature importances via {a}\",fontweight='bold',fontsize=13)\n            plt.bar(range(X.shape[1]),b[ind])\n            plt.xticks(range(X.shape[1]),cols[ind],rotation=rotation,fontsize=xsize)\n            plt.xlim([-1,X.shape[1]])\n            plt.tight_layout()\n            \n\ndef plot_meanscore(meanscore_set,title,axvline=.9):\n    print(meanscore_set)\n    f=plt.figure(figsize=(8,3))\n    meanscore_set.Test_accuracy[::-1].plot(kind='barh',color='#0f1a26',alpha=.95)\n    (-1*meanscore_set.Train_accuracy[::-1]).plot(kind='barh',alpha=.95)\n    plt.xlim([-1,1])\n    plt.axvline(x=0,c='black')\n    plt.axvline(x=axvline,ls=':')\n    despine()\n    plt.title(f\"{title} accuracy\",fontsize=17,fontweight='bold')            ","f0360804":"# for our modeling stage\nfrom sklearn.ensemble import RandomForestClassifier,BaggingClassifier,GradientBoostingClassifier,AdaBoostClassifier,ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression,Perceptron,SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nmodellst=[RandomForestClassifier(),\n          BaggingClassifier(),\n          GradientBoostingClassifier(),\n          AdaBoostClassifier(),\n          ExtraTreesClassifier(),\n          LogisticRegression(solver='liblinear'),\n          Perceptron(),\n          SGDClassifier(),         \n          LGBMClassifier(),\n          CatBoostClassifier(),\n          XGBClassifier(),\n          SVC(),\n          LinearDiscriminantAnalysis()]","b814e11a":"score,imp=selectmoran(modellst,X)","93c7c459":"plotscoring(score,'All features')","283e6ee9":"pvalset","1c2e4a96":"scorepv,imppv=selectmoran(modellst,X[pvalset])","2d7c36c7":"plotscoring(scorepv,'P-value subset')","5bf8ab78":"scorepca,imppca=selectmoran(modellst,Xpca)","689f4c42":"plotscoring(scorepca,'PCA susbet')","79cec4a2":"scoreeda,impeda=selectmoran(modellst,X[edaset])","85d096e0":"plotscoring(scoreeda,'EDA subset')","cd7baaf4":"meanscore=pd.DataFrame({'Test_accuracy':[np.mean(i.Test_accuracy) for i in [score,scoreeda,scorepca,scorepv]],\n             'Train_accuracy':[np.mean(i.Train_accuracy) for i in [score,scoreeda,scorepca,scorepv]]},\n             index=['All features','EDA subset','PCA subset','P-value subset']).sort_values('Test_accuracy',ascending=False)\n\nmxscore=pd.DataFrame({'Test_accuracy':[np.max(i.Test_accuracy) for i in [score,scoreeda,scorepca,scorepv]],\n             'Train_accuracy':[np.max(i.Train_accuracy) for i in [score,scoreeda,scorepca,scorepv]]},\n             index=['All features','EDA subset','PCA subset','P-value subset']).sort_values('Test_accuracy',ascending=False)","a4d1b3b3":"plot_meanscore(meanscore,'Mean')","32ef6d7d":"plot_meanscore(mxscore,'Max')","24e7d76d":"plotscoring(pd.concat([score,scoreeda,scorepca,scorepv]).groupby('Model').mean().reset_index().sort_values('Test_accuracy',ascending=False),'Classifiers')","e9930fc6":"plotscoring(score,'All features')","60464511":"tuningall=[RandomForestClassifier(max_depth=11, max_features='log2', n_estimators=121, random_state=1),\n          BaggingClassifier(n_estimators=20,random_state=1,oob_score=True,base_estimator=ExtraTreesClassifier()),\n          SGDClassifier(loss='huber',n_jobs=4,random_state=1),\n          GradientBoostingClassifier(min_samples_split=3, n_estimators=30, random_state=1),\n          LGBMClassifier(boosting_type='dart', max_depth=5, n_estimators=40, n_jobs=4, subsample=0.8),\n          ExtraTreesClassifier(criterion='entropy', max_features='log2', n_jobs=3, random_state=0),\n          AdaBoostClassifier(learning_rate=0.6, n_estimators=30, random_state=0),\n          XGBClassifier(),\n          XGBRFClassifier(booster='dart',n_jobs=4,learning_rate=.2,n_estimators=40,max_depth=7,random_state=0)]\nscore_tuall,_=selectmoran(tuningall,X)","8f87c866":"plotscoring(score_tuall,'Hypertuning - All features')","0ad686d1":"plotscoring(scorepca,'PCA subset')","64c50646":"tuningpca=[SVC(C=4, random_state=1),\n          BaggingClassifier(base_estimator=LinearDiscriminantAnalysis(solver='lsqr'),n_estimators=20, random_state=1),\n          GradientBoostingClassifier(min_samples_split=3, n_estimators=30, random_state=1,max_features='log2'),\n          LGBMClassifier(boosting_type='dart',max_depth=3,learning_rate=.03,n_estimators=40,n_jobs=4,subsample=.9),\n          AdaBoostClassifier(learning_rate=.04,n_estimators=30,random_state=1),\n          ExtraTreesClassifier(bootstrap=True, criterion='entropy', max_features='log2', n_jobs=4, oob_score=True),\n          SGDClassifier(loss='huber', n_jobs=4, penalty='l1')]","6da6fc83":"score_tupca,_=selectmoran(tuningpca,Xpca)","9d8e68ee":"plotscoring(score_tupca,'Hypertuning PCA subset')","d5e11b7c":"plotscoring(scoreeda,'EDA subset')","32b5ab68":"tuningeda=[LGBMClassifier(boosting_type='dart', learning_rate=0.09, max_depth=2, n_estimators=70, n_jobs=4, subsample=0.8),\n          BaggingClassifier(base_estimator=ExtraTreesClassifier(), n_jobs=4, oob_score=True, random_state=1),\n          ExtraTreesClassifier(bootstrap=True, max_features='log2', n_estimators=50, n_jobs=4, oob_score=True, random_state=0),\n          AdaBoostClassifier(base_estimator=RandomForestClassifier(), learning_rate=0.04, n_estimators=10, random_state=1),\n          GradientBoostingClassifier(learning_rate=0.02, max_features='log2', min_samples_leaf=2, min_samples_split=3, n_estimators=20, random_state=1, warm_start=True),\n          RandomForestClassifier(max_features='log2', n_jobs=4, oob_score=True, random_state=0),\n          CatBoostClassifier(**{'depth': 5, 'iterations': 60, 'learning_rate': 0.02})]\nscore_tueda,_=selectmoran(tuningeda,X[edaset])","2f8cf7e1":"plotscoring(score_tueda,'EDA subset')","6dc96d82":"mxscoretu=pd.DataFrame({'Test_accuracy':[np.mean(i.Test_accuracy) for i in [score_tuall,score_tueda,score_tupca]],\n             'Train_accuracy':[np.mean(i.Train_accuracy) for i in [score_tuall,score_tueda,score_tupca]]},\n             index=['All features','EDA subset','PCA subset']).sort_values('Test_accuracy',ascending=False)\nplot_meanscore(mxscoretu,'Hypetuning')\ntitle('Test score after hypertuning parameters',fontsize=17)","d1ba4e74":"print(f\"The best model: {score_tuall.iloc[0].Model} using all features with a test accuracy of {score_tuall.iloc[0].Test_accuracy*100:.2f}%\")","60410485":"## 1.1 Importing python libraries and our dataset","8215aa42":"From the pairplot, we see that there are some linear trends between fixed acidity and ph\/ density\/ citric acid, alcohol and density","bf208390":"- It seems that density and ph are not good predictors becauses we don't see a significant between groups of quality divided by these features","040ba98f":"## 2.1 Univariate analysis","0dda6195":"# 5. Feature selection","37c0c19d":"- Overall, quality distribution has a normal shape with very few exceptional high or low quality ratings\n- The minimum rating is 3 and the maximum rating is 8 for quality, with a very less number of wines rated to extremes. \n\n**New feature:**\n- We will create a new feature based on wine rating (less than 5, 5 and 6, greater than 6)","26126e5a":"# 6. Modeling","00f4b91e":"- The test accuracy for models using all features are around 82.78%","372d3e85":">*Red Wines are well attributed to positive health benefits. It lowers your chances of having a stroke compared to nondrinkers. For men and women who drink moderately, it lessens their chance of developing Type 2 diabetes by 30%*\n\nReference: \n- https:\/\/datauab.github.io\/red_wine_quality\/\n- https:\/\/www.kaggle.com\/sisharaneranjana\/what-makes-great-wine-great\n- https:\/\/winefolly.com\/deep-dive\/understanding-acidity-in-wine\/\n- https:\/\/waterhouse.ucdavis.edu\/whats-in-wine\/fixed-acidity","f562866d":"# Content\n1. Overview\n    - 1.1 Importing python libraries and our dataset\n    - 1.2 Loading our dataset\n    - 1.3 Data completeness\n2. Exploratory data analysis\n    - 2.1 Univariate analysis\n    - 2.2 Multivariate analysis\n3. Outliers detection\n4. Feature scaling\n5. Feature selection\n    - 5.1 Dimensionality reduction via PCA\n    - 5.2 Using P-value\n    - 5.3 EDA subset\n6. Modeling\n    - 6.1 All features\n    - 6.2 P-value subset\n    - 6.3 PCA subset\n    - 6.4 EDA subset\n    - 6.5 Conclusion\n7. Hypertuning parameters\n    - 7.1 All features\n    - 7.2 PCA subset\n    - 7.3 EDA subset\n    - 7.4 Conclusion","10773192":"- We have no missing values for all features, so no need to process","987ef0ff":"# 4. Feature scaling","7f455791":"- There is not much difference between test accuracy among our subsets, the mean value for test accuracy is around 82%\n- The highest test accuracy belongs to our initial features set","7bc56b28":"## 6.3 PCA subset","baef75a4":"### 2.1.1 Target variable: Quality","85a9d921":"- So, all of our features are continuous, except for target variable - quality","5a6faadc":"## 7.1 All features","89edbd09":"## 6.4 EDA susbet","1b811d32":"## 1.3 Data completeness","a1fd300c":"- We also create our EDA subset, including our most 4 important features","5a316146":"- Features with a large skewness: residual sugar, chlorides (right-skewed)\n- Other features have smaller skewnesses    \n- Features with a nearly normal distribution: density, ph","e10f0300":"## 5.1 Dimensionality reduction via PCA","d903ce9c":"## Basic Wine Characteristics\nSource: https:\/\/winefolly.com\/review\/wine-characteristics.\/\n- **Sweetness**: If you find a wine you like has **residual sugar**, you may enjoy a hint (or a lot!) of sweetness in your wine\n- **Acidity**: Acidity in food and drink tastes **tart and zesty**. Tasting acidity is also sometimes confused with alcohol.\n- **Tannin**: Tannin in wine is the presence of phenolic compounds that add **bitterness** to a wine.\n- **Alcohol**: The average glass of wine contains around **11\u201313% alcohol**. That said, wine ranges from as little as 5.5% alcohol by volume (ABV) to as much as around 20% ABV. Alcohol Characteristics: Wines with **higher alcohol tend to taste bolder and more oily**, wines with lower alcohol tend to taste lighter-bodied, most wines range between 11\u201313% ABV\n- **Body**: Body is the result of many factors \u2013 from wine variety, where it\u2019s from, vintage, alcohol level, and how it\u2019s made. Body is a snapshot of the overall impression of a wine. You can improve your skills by paying attention to where and when it\u2019s present.","4e6e767d":"# 1. Overview","7197fcce":"### 2.2.1 Correlation matrix","acdbeb70":"- All of our features are numeric\n- Looks like we don't have the missing values\n- residual.sugar has 75% of its value below 2.6, while the maximum is 15.5, there may be some outliers to this feature. Similar cases can be found at free.sulfur.dioxide and total.sulfur.dioxide\n- The average quality in our dataset is 5.6 and the maximum quality is 8.0\n- There might be some outliers in total sulfur dioxide and free sulfur dioxide","7511552c":"## 7.2 PCA subset","9a3d6566":"### 2.2.2 Target feature and other features","b4c5b87b":"## 2.2 Multivariate analysis","28f7ac72":"# 2. Exploratory data analysis","f764d42e":"### 2.1.2 Distribution of independent features","578b5f2d":"## 7.3 EDA subset","ec97d314":"## 5.3 EDA subset","2d886c99":"## 6.1 All features","bd415b7f":"- There is a small percentage of low quality and high quality wine while medium quality makes up the majority","b413cf0c":"## 7.4 Conclusion","a166565a":"## 1.2 Loading our dataset","2dbc8960":"## 6.5 Conclusion","cd831b77":"- We need only 4 features to explain about 100% variance, so we will create a new PCA subset, including these 4 principal components","ed19483f":"- Features having a clear impact on group quality (in order) are alcohol, volatile acidity, citric acid, and sulphates\n- The positive correlation between alcohol and group quality shows that the higher the alcohol content, the better the quality\n- The negative correlation between volatile acidity and group quality indicates that the lower the volatile acid content, the higher the wine quality\n- There are some positive correlations between citric acid\/ sulphates\/ fixed acidity and group quality\n- we also notice that some features have significant correlations, such as ph and acidity, density and acidity, sulphates and chlorides","2dc0522c":"## 6.2 P-value subset","b1e58c08":"## 5.2 Using P-value","b2a14a7a":"# 3. Outliers detection","5cbc62c8":"# 7. Hypertuning parameters"}}