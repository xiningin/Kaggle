{"cell_type":{"e094f795":"code","24cd3053":"code","505f9b5e":"code","206400b8":"code","25c3b8db":"code","f4109b64":"code","d1b47c5e":"code","2a8f2b7b":"code","85f327cf":"code","1d8083fb":"code","629a90a5":"code","ca437c9f":"code","55a35025":"code","01436453":"code","471707c5":"code","fdd255df":"code","b32e3c99":"code","509b8cfe":"code","db7675e2":"code","1af267c6":"code","1e9ac247":"code","ad42ab14":"markdown","3256b4d1":"markdown","e3d50fef":"markdown","c00b4c6c":"markdown","caad74f8":"markdown","92ccf473":"markdown","31fb951a":"markdown","749f6d00":"markdown","5f2191ea":"markdown","d13d308d":"markdown","343c9f9f":"markdown","478454a3":"markdown","e2a10e1f":"markdown","0fa7613b":"markdown","d6d00697":"markdown","77e8d08b":"markdown"},"source":{"e094f795":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","24cd3053":"!pip install keras-tuner\n\n# Plotting Libraries\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cufflinks as cf\n%matplotlib inline\n\n# Metrics for Classification technique\n\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\n# Keras\n\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.optimizers import RMSprop\nfrom keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPool2D\nfrom keras.layers import LeakyReLU, PReLU, ELU\nfrom keras.activations import relu, sigmoid\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint \nfrom kerastuner import RandomSearch\nfrom kerastuner.engine.hyperparameters import HyperParameters\nfrom keras.utils import np_utils\nfrom keras.layers import BatchNormalization\n\n# Others\n\nfrom sklearn.model_selection import train_test_split","505f9b5e":"# Load the data\n\ntrain = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ntest = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/digit-recognizer\/sample_submission.csv\")","206400b8":"y_train = train['label']\nX_train = train.drop(\"label\", axis = 1)","25c3b8db":"X_train = X_train \/ 255.0\ntest = test \/ 255.0","f4109b64":"y_train = np_utils.to_categorical(y_train)","d1b47c5e":"m = X_train.shape[0]\nn = test.shape[0]\n\nX_train = X_train.values.reshape(m,28,28,1)\ntest = test.values.reshape(n,28,28,1)","2a8f2b7b":"X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.1, random_state = 42)","85f327cf":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(20,7))\nfor i in range(30):  \n    plt.subplot(3, 10, i+1)\n    plt.imshow(X_train[i].reshape((28,28)),cmap=plt.cm.binary)\n    plt.axis('off')\nplt.show()","1d8083fb":"model = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', kernel_initializer = 'he_uniform', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', kernel_initializer = 'he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu',kernel_initializer = 'he_uniform'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu',kernel_initializer = 'he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation = \"relu\",kernel_initializer = 'he_uniform'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(10, activation = \"softmax\",kernel_initializer = 'he_uniform'))","629a90a5":"model.summary()","ca437c9f":"# Compile the model\n\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\nmodel.compile(optimizer = optimizer , loss = 'categorical_crossentropy', metrics =['accuracy'])","55a35025":"# ModelCheckpoint\n\nfrom keras.callbacks import ModelCheckpoint\n\ncheckpoint = ModelCheckpoint('..\/working\/mod.best.hdf5',monitor = 'val_loss', mode = \"min\", verbose = 1, save_best_model = True)\n\n# EarlyStopping\n\nfrom keras.callbacks import EarlyStopping\n\nearlystop = EarlyStopping(monitor = 'val_loss', patience = 3, min_delta = 0,verbose =1, restore_best_weights = True)\n\n# ReduceLROnPlateau\n\nfrom keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.5, min_lr=0.00001)\n\ncallbacks = [reduce_lr,checkpoint]","01436453":"# With data augmentation to prevent overfitting \n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","471707c5":"# Best model\n\nmodel = load_model(\"..\/working\/mod.best.hdf5\")","fdd255df":"# Training the model\n\nbatch_size = 86\nhistory = model.fit_generator(datagen.flow(X_train,y_train, batch_size = 86),\n                              epochs = 35, validation_data = (X_val,y_val),\n                              verbose = 2, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n                              , callbacks = callbacks)","b32e3c99":"y_pred = model.predict(X_val)\ny_pred = np.argmax(y_pred,axis=1)\ny_true = np.argmax(y_val,axis=1)\nCM = confusion_matrix(y_true,y_pred)\nplt.figure(figsize=(15,10))\nsns.heatmap(CM,annot=True,cmap=\"viridis\")\nplt.tight_layout()","509b8cfe":"# Evaluation\n\nscore = model.evaluate(X_val, y_val, verbose=0)\nprint(\"Val_Loss :\",score[0])\nprint(\"Val_Acc :\",score[1])","db7675e2":"# Plotting the model\n\nfrom keras.utils.vis_utils import plot_model\n\nplot_model(model,to_file=\"..\/working\/plot.png\",show_shapes=True,show_layer_names=True)\n","1af267c6":"result = model.predict(test)\nresult = np.argmax(result,axis=1)","1e9ac247":"submission[\"Label\"] = result\nsubmission.to_csv(\"CNN_Result\",index=False)","ad42ab14":"## Data Augmentation","3256b4d1":"# Modelling \n\n* Model architecture - [ [Conv2D * 2] -> BatchNormalization -> MaxPooling -> Dropout ] * 2 -> Flatten -> Dense -> Output\n* We have taken default value for strides in convolution layer.\n* 'he_uniform' method is used for weight initialization as it is preferrable method for relu activation function.\n* 'Same' padding method is used as image size remains same.\n* Batch Normalization layer is used to normalize the activation of the layers output.\n* Dropout layer is used to reduce overfitting by randomly selecting the nodes of layer according to input termed as rate and deactivates them.","e3d50fef":"## Thank You!!\n\n![CNN.png](attachment:CNN.png)","c00b4c6c":"## Adding Callbacks","caad74f8":"## Evaluation","92ccf473":"# Data Preparation","31fb951a":"### Reshaping\n\n* Gray Scale Images\n* Image Size - (28,28)","749f6d00":"# Digit Recognizer Using Keras\n\n* I have achieved the best accuracy of 99.625%.\n* Rank on the leaderboard : 204\n* If you find my work interesting, do upvote it.\n\n\n## Problem Statement\n\nIn this competition, our goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. The MNIST database contains 42,000 training images and 28,000 testing images. The dimension of images are 28 x 28 pixel.\n\n## Introduction\n\nIn this particular notebook, We will be dealing with three portion which are :-\n\n1. Data Preparation\n2. Modelling \n3. Model Plotting","5f2191ea":"### Splitting the Train Set","d13d308d":"## Importing Necessary Libraries\n\n\n","343c9f9f":"# Model Plotting","478454a3":"## Submission","e2a10e1f":"**This is default first cell in any kaggle kernel. They import NumPy and Pandas libraries and it also lists the available Kernel files. NumPy is the fundamental package for scientific computing with Python. Pandas is the most popular python library that is used for data analysis.**","0fa7613b":"### One-Hot Encoding\n\n![image.png](attachment:image.png)","d6d00697":"### Normalization\n\nData normalization is an important step which ensures that each input parameter (pixel, in this case) has a similar data distribution.","77e8d08b":"### Data Loading"}}