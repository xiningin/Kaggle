{"cell_type":{"912608da":"code","f033de7b":"code","34ba088f":"code","784fd09f":"code","ac40f276":"code","900c74cb":"code","7e9d5ec0":"code","c3fd59ae":"code","16b179a1":"code","5f9b5a4a":"code","e71df741":"code","ec534dd0":"code","b5c10120":"markdown","3ffcdbb6":"markdown","db828b91":"markdown","e2ac7a80":"markdown","3786dbc9":"markdown"},"source":{"912608da":"import tensorflow as tf\nfrom tensorflow import keras\n \nfrom tensorflow.nn import depth_to_space\nfrom tensorflow.nn import space_to_depth\nfrom tensorflow.nn import softmax\nfrom tensorflow.linalg import matmul\nfrom tensorflow.math import divide\nfrom tensorflow.math import sqrt\nfrom tensorflow.math import add\nfrom tensorflow.math import subtract\nfrom tensorflow.image import extract_patches\n \nfrom tensorflow.keras.layers import Conv2D \nfrom tensorflow.keras.layers import Conv2DTranspose\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n \nfrom tensorflow_addons.image import median_filter2d\n    \nimport os\nimport cv2\nfrom skimage.util import random_noise","f033de7b":"batch_size = 1\nimage_size = 512\npatch_size = 32\ntrain_len = len(os.listdir('..\/input\/div2k-dataset\/DIV2K_train_HR\/DIV2K_train_HR'))\ntest_len = len(os.listdir('..\/input\/div2k-dataset\/DIV2K_valid_HR\/DIV2K_valid_HR'))\n\ndatagen = ImageDataGenerator()\n\ntrain_dataset = datagen.flow_from_directory('..\/input\/div2k-dataset\/DIV2K_train_HR', target_size=(image_size, image_size), batch_size=batch_size, class_mode=None, classes=['DIV2K_train_HR'], color_mode='grayscale')\ntest_dataset = datagen.flow_from_directory('..\/input\/div2k-dataset\/DIV2K_valid_HR', target_size=(image_size, image_size), batch_size=batch_size, class_mode=None, classes=['DIV2K_valid_HR'], color_mode='grayscale')","34ba088f":"# TODO: Needs Rework\n# Patch Extraction Layer\nclass Patch(keras.layers.Layer):\n  def __init__(self, patch_size=32):\n    super(Patch, self).__init__()\n    self.patch_size = patch_size\n \n  def call(self, images):\n    channels = tf.shape(images)[-1]\n    patches = tf.image.extract_patches(\n        images=images, \n        sizes=[1, self.patch_size, self.patch_size, 1], \n        strides=[1, self.patch_size, self.patch_size, 1], \n        rates=[1, 1, 1, 1], padding=\"VALID\"\n        )\n    patches = tf.reshape(patches, [-1, self.patch_size, self.patch_size, channels])\n    return patches\n \n \n# TODO: Needs Rework\n# Noise Addition\nclass Noise(keras.layers.Layer):\n  def __init__(self, ratio=0.5):\n    super(Noise, self).__init__()\n    self.ratio = ratio\n\n  def call(self, inputs, training=None):\n    size = inputs.shape\n    mask_select = tf.keras.backend.random_bernoulli(shape=size, p=self.ratio, dtype=tf.float32)\n    mask_noise = tf.keras.backend.random_bernoulli(shape=size, p=0.5, dtype=tf.float32)\n    output = inputs * (1-mask_select) + (mask_noise * mask_select * 255)\n    output = tf.clip_by_value(output, 0.0, 255.0)\n    return output\n\n\n# TODO: -\n# Residual Layer\nclass Residual(keras.layers.Layer):\n    def __init__(self, filter=32, kernel=(3,3), stride=(1,1), n_conv=2):\n        super(Residual, self).__init__()\n        self.filter = filter\n        self.kernel = kernel\n        self.stride = stride\n        self.n_conv = n_conv\n        \n    def build(self, input_shape):\n        self.convs = [Conv2D(input_shape[-1], self.kernel, self.stride, padding='same', activation='relu') for _ in range(self.n_conv)]\n        \n    def call(self, inputs):\n        out = inputs\n        for conv in self.convs:\n            out = conv(out)\n        out = add(inputs, out)\n        return out","784fd09f":"class EncoderBlock(keras.layers.Layer):\n    def __init__(self, filter):\n        super(EncoderBlock, self).__init__()\n        self.filter = filter\n        \n    def build(self, input_shape):\n        self.residual = Residual(filter=self.filter)\n        self.downscale = Conv2D(self.filter, (3,3), (2,2), padding='same', activation='relu')\n        \n    def call(self, inputs):\n        out = self.residual(inputs)\n        out = median_filter2d(out)\n        out = self.downscale(out)\n        return out\n\nclass Encoder(keras.Model):\n    def __init__(self, init_filter=16, n_layers=4):\n        super(Encoder, self).__init__()\n        self.blocks = [EncoderBlock(init_filter*(i+1)) for i in range(n_layers)]\n\n    def call(self, inputs):\n        out = inputs\n        for block in self.blocks:\n            out = block(out)\n        return out","ac40f276":"class DecoderBlock(keras.layers.Layer):\n    def __init__(self, filter):\n        super(DecoderBlock, self).__init__()\n        self.filter = filter\n        \n    def build(self, input_shape):\n        self.residual = Residual(filter=self.filter)\n        self.upscale = Conv2DTranspose(self.filter, (3,3), (2,2), padding='same', activation='relu')\n        \n    def call(self, inputs):\n        out = self.residual(inputs)\n        out = self.upscale(out)\n        return out\n\nclass Decoder(keras.Model):\n    def __init__(self, init_filter=16, n_layers=4):\n        super(Decoder, self).__init__()\n        self.blocks = [DecoderBlock(init_filter*n_layers*(i\/n_layers)) for i in range(n_layers, 0, -1)]\n\n    def call(self, inputs):\n        out = inputs\n        for block in self.blocks:\n            out = block(out)\n        return out","900c74cb":"class AutoEncoder(keras.Model):\n    def __init__(self, init_filter=16, n_layers=4):\n        super(AutoEncoder, self).__init__()\n        self.encoder = Encoder(init_filter, n_layers)\n        self.decoder = Decoder(init_filter, n_layers)\n        self.projection = Conv2D(1, (3,3), (1,1), padding='same')\n        \n    def call(self, inputs):\n        out = median_filter2d(inputs)\n        out = self.encoder(out)\n        out = self.decoder(out)\n        out = self.projection(out)\n        return out","7e9d5ec0":"# TODO: Loss function is kinda fucked\n# Training loop\ndef train_step(batch, model):\n  image_clean = Patch(patch_size=patch_size)(batch) \/ 255.0\n  image_noise = Patch(patch_size=patch_size)(Noise(ratio=0.5)(batch)) \/ 255.0\n \n  with tf.GradientTape() as tape:\n    logits = model(image_noise)\n    loss = tf.keras.losses.MSE(image_clean, logits)\n    \n  grads = tape.gradient(loss, model.trainable_weights)\n  optimizer.apply_gradients(zip(grads, model.trainable_weights))\n  return loss\n \ndef train(epochs, dataset, model):\n  for epoch in range(epochs):\n    print('EPOCH:', epoch+1)\n    for step, batch in enumerate(dataset):\n      if step % 10 == 0: print('STEP:', step)\n      loss = train_step(batch, model)\n      if step >= train_len\/batch_size:\n        break","c3fd59ae":"model = AutoEncoder(init_filter=16, n_layers=3)\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\ntrain(5, test_dataset, model)","16b179a1":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n \ndef imshow(img):\n    import cv2\n    import IPython\n    import numpy as np\n    _,ret = cv2.imencode('.jpg', np.array(img)) \n    i = IPython.display.Image(data=ret)\n    IPython.display.display(i)\n    \nimage_count = 1\nimage_size = 1024\nimage_index = 10\namount = 0.5\n\nimage = tf.expand_dims(tf.keras.preprocessing.image.img_to_array(\n    tf.keras.preprocessing.image.load_img('..\/input\/div2k-dataset\/DIV2K_train_HR\/DIV2K_train_HR\/' + os.listdir('..\/input\/div2k-dataset\/DIV2K_train_HR\/DIV2K_train_HR')[image_index], target_size=(image_size, image_size), color_mode='grayscale')\n), axis=0)","5f9b5a4a":"imshow(image[0])","e71df741":"patches = Noise(ratio=amount)(image)\nimshow(patches[0])","ec534dd0":"patches = Noise(ratio=amount)(image) \/ 255.0\npatches = model(patches) * 255.0\npatches = tf.clip_by_value(patches, 0.0, 255.0)\nimshow(patches[0])","b5c10120":"# Neural Net","3ffcdbb6":"# Dependencies","db828b91":"# Evaluation","e2ac7a80":"# Training Loop","3786dbc9":"# Dataset Preparation"}}