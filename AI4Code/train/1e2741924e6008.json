{"cell_type":{"b3af40f1":"code","22814053":"code","4dfe7d82":"code","64767c0d":"code","4a492008":"code","55ad8916":"code","5bb7f5fc":"code","d06150ad":"code","65c617dc":"code","7bd46f77":"code","aff27a83":"code","9167ac4a":"code","de5a54ea":"code","f5fadf93":"code","80087ae5":"markdown","d223e3ee":"markdown","d3f2f946":"markdown","88962775":"markdown","bd6ef833":"markdown","2b44c5d4":"markdown","99335e31":"markdown"},"source":{"b3af40f1":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Flatten, Embedding, Input, Dropout, SpatialDropout1D, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.python.keras.layers.wrappers import TimeDistributed\nfrom tensorflow.python.keras.layers.recurrent import LSTM\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n\nfrom keras.initializers import Constant\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom sklearn.model_selection import train_test_split\nfrom os import path","22814053":"# Read the data\ndf_train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ndf_sample = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")","4dfe7d82":"df_train['excerpt'][0]","64767c0d":"# Remove unused columns\ndf_train = df_train.drop(columns=['url_legal', 'license'])\ndf_train.head()","4a492008":"# Remove unused columns\ndf_test = df_test.drop(columns=['url_legal', 'license'])\ndf_test.head()","55ad8916":"# Get the maximum number of words used in each text\n\nmax_length_training = max(df_train.apply(lambda x : len(x[\"excerpt\"].split(' ')), axis=1))\nmax_length_testing = max(df_test.apply(lambda x : len(x[\"excerpt\"].split(' ')), axis=1))\n\nprint(\"Maximum length of text in training set : \", max_length_training, \" | in the testing set : \", max_length_testing)","5bb7f5fc":"from nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nstop_words = set(stopwords.words(\"english\"))\nporter = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef preprocess_text(text):\n    \n    # Extract all the words in the phrase : get a list \n    tokens = word_tokenize(text)\n    \n    # Lowercase the words\n    tokens = [word.lower() for word in tokens]\n    \n    # Remove all tokens that are not alphabetic\n    words = [word for word in tokens if word.isalpha()]\n    \n    # Remove word in the stop word\n    #\u00a0words = [word for word in words if not word in stop_words]\n\n    # Get the root of the word \n    #\u00a0stemmed = [porter.stem(word) for word in words]\n    \n    # Lematize the word\n    #\u00a0lematized = [lemmatizer.lemmatize(word) for word in words]\n\n    return \" \".join(words)","d06150ad":"# Apply the preprocessing on the text\ndf_train['preprocess_text'] = df_train.excerpt.apply(preprocess_text)\ndf_test['preprocess_text'] = df_test.excerpt.apply(preprocess_text)","65c617dc":"#\u00a0Get the list of unique word\nunique_words = list(df_train.preprocess_text.str.split(' ', expand=True).stack().unique())\nprint(\"Number of unique words : \", len(unique_words))","7bd46f77":"import tokenizers\nfrom transformers import RobertaConfig, TFRobertaModel\nfrom transformers import RobertaTokenizer, TFRobertaForSequenceClassification\n\nroberta_path = '..\/input\/tf-roberta\/'\n\n# Get the max size from the analysis we have made at the beginning\nMAX_INPUT_LENGTH = max(max_length_training, max_length_testing)\n\n# Load our pretrained model\ntok = RobertaTokenizer.from_pretrained('..\/input\/roberta-base')\nprint(\"Vocabulary size : \", tok.vocab_size)","aff27a83":"# Encoder for our input data\ndef roberta_encode(texts, tokenizer, max_len=MAX_INPUT_LENGTH):\n    all_tokens = np.ones((len(texts), max_len), dtype='int32')\n    all_masks = np.zeros((len(texts), max_len), dtype='int32')\n    \n    for k, text in enumerate(texts):\n        encoded = tok.encode_plus(\n            text,                \n            add_special_tokens=True,\n            max_length=max_len,\n            truncation=True,\n            padding='max_length',\n            return_attention_mask=True,\n        )\n        \n        all_tokens[k, :max_len] = encoded['input_ids']\n        all_masks[k, :max_len] = encoded['attention_mask']\n        \n    return all_tokens, all_masks\n\n# Create our model\ndef build_roberta(max_len=MAX_INPUT_LENGTH):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    \n    # Get the pretrained roberta model\n    config = RobertaConfig.from_pretrained(roberta_path + 'config-roberta-base.json')\n    roberta_model = TFRobertaModel.from_pretrained(roberta_path + 'pretrained-roberta-base.h5', config=config)\n    \n    x = roberta_model([input_word_ids, input_mask])[0]\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(1, activation='linear')(x)\n    \n    model = Model(inputs = [input_word_ids, input_mask], outputs=out)\n    model.compile(Adam(lr = 1e-5), \n                  loss=\"mean_squared_error\", \n                  metrics=['mse', 'mae', RootMeanSquaredError()])\n    \n    return model","9167ac4a":"# Create our training and valdiation set\nX = df_train['preprocess_text']\nY = df_train['target'].values\n\nX_final = df_test['preprocess_text']\n\nX_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.1, random_state=42)","de5a54ea":"# Encode our data\nX_train_encode = roberta_encode(X_train, tok, MAX_INPUT_LENGTH)\nX_val_encode = roberta_encode(X_val, tok, MAX_INPUT_LENGTH)\n\n# Change if you want to use your pretrained model\nPRETRAINED = False\n\n# Build our model\nK.clear_session()\nmodel = build_roberta(MAX_INPUT_LENGTH)\n\nif not PRETRAINED:\n\n    # Save the best model\n    check = ModelCheckpoint(f'roberta_model.h5', \n                            monitor='val_loss', \n                            verbose=1, \n                            save_best_only=True,\n                            save_weights_only=True, \n                            mode='auto', \n                            save_freq='epoch')\n\n    # Train our model\n    history = model.fit(X_train_encode, \n                        y_train, \n                        validation_data=(X_val_encode, y_val),\n                        epochs=4, \n                        batch_size=8, \n                        verbose=1, \n                        callbacks=[check])\n    \n    # Load the best model\n    model.load_weights(f'roberta_model.h5')\n\nelse :\n    # Load pretrained model\n    model.load_weights(f'..\/input\/robertapretrained\/roberta_model.h5')","f5fadf93":"# Encode our final data\nX_final_encoding = roberta_encode(X_final, tok, max_len=MAX_INPUT_LENGTH)\n\n# Make the prediction\ny_pred = model.predict(X_final_encoding)\n\n# Do the mean of the output\ny_mean = np.mean(y_pred, axis=1)\n\n# Save to our submission file\ndf_sample['target'] = y_mean\ndf_sample.to_csv(\"submission.csv\", index=False)","80087ae5":"# Visualize some data","d223e3ee":"# Roberta model for NLP\n\nIn this kernel, we want to rate the complexity of literary passages. This will allow student to choose a text according to their level.\n\n[In the dataset](https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize), we will mainly focus on the text and the score we want to predict. Here we will faced a regression problem\n\nIn order to accomplish that, we will preprocessing the text and pass it to a pretrained [RoBERTa model](https:\/\/arxiv.org\/abs\/1907.11692). \n\nDon't hesitate if have questions or if you see some improvements that can be made.","d3f2f946":"# Preprocess the data\n\nIn order to preprocess the data, we are going to :\n\n- Word tokenize : we want to break down the sentence to get the words that compose it.\n- To lower case : normalize each word.\n- Remove punctuations\/digits.\n- (optional) Remove stopwords : remove non significative words.\n- (optional) Stemming : get the word stem, the root form of the word. (Example : fishing, fished, fisher => fish)\n- (optional) Lemmatized : Get the lemma of the word.\n\nIn this approach, I wanted to keep each words, because I think the connection between words is relevent. But, you can with the processing function uncomment the code and add the stopword list.  \nAlso, by using RoBERTa, the usage of stemming and lemmatization could be avoid.\n","88962775":"# References\n\n- https:\/\/www.kaggle.com\/msafi04\/tensorflow-roberta-commonlit-readability\n","bd6ef833":"# Improvement\n\n- For improvement, one possibility is to use Kfolding and create, let say 5 RoBERTa model. Then, you use the combination of the five models to generate a prediction.\n","2b44c5d4":"## Build our model","99335e31":"# Make the prediction"}}