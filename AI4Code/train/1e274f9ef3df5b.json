{"cell_type":{"4044c3c6":"code","5ff92cab":"code","167eeb10":"code","f7ee7512":"code","d09caa71":"code","28c40c68":"code","ed34bd75":"code","178bfebb":"code","034739b7":"markdown","1e13c435":"markdown","087599e6":"markdown","737e8213":"markdown","488a8bc7":"markdown","056ba633":"markdown"},"source":{"4044c3c6":"%load_ext autoreload\n%autoreload 2\n!pip install python-chess  # Python-Chess is the Python Chess Package that handles the chess environment\n!pip install --upgrade git+https:\/\/github.com\/arjangroen\/RLC.git  # RLC is the Reinforcement Learning package\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)import os\nimport inspect","5ff92cab":"import chess\nfrom chess.pgn import Game\nboard = chess.Board()\nimport RLC\nfrom RLC.capture_chess.environment import Board\nfrom RLC.capture_chess.learn import Reinforce, ActorCritic\nfrom RLC.capture_chess.agent import Agent, policy_gradient_loss\nboard = Board()\nagent = Agent(network='conv_pg',lr=0.3)\n\nR = Reinforce(agent,board)","167eeb10":"print(inspect.getsource(policy_gradient_loss))","f7ee7512":"pgn = R.learn(iters=2000)","d09caa71":"with open(\"final_game.pgn\",\"w\") as log:\n    log.write(str(pgn))\n    ","28c40c68":"board = Board()\n\ncritic = Agent(network='conv',lr=0.1)\ncritic.fix_model()\nactor = Agent(network='conv_pg',lr=0.3)\n\nR = ActorCritic(actor, critic,board)","ed34bd75":"pgn = R.learn(iters=1000)","178bfebb":"reward_smooth = pd.DataFrame(R.reward_trace)\nreward_smooth.rolling(window=100,min_periods=0).mean().plot(figsize=(16,9))","034739b7":"**Implementation**\n\nThe loss function is modified so that the gradient is proportional to the rewards gained. The implementation is described here:\nhttps:\/\/stackoverflow.com\/questions\/45961428\/make-a-custom-loss-function-in-keras\n","1e13c435":"# References\n#### 1. Reinforcement Learning: An Introduction\n> Richard S. Sutton and Andrew G. Barto\n1st Edition\nMIT Press, march 1998\n\n#### 2. RL Course by David Silver: Lecture playlist\n> https:\/\/www.youtube.com\/watch?v=2pWv7GOvuf0&list=PLqYmG7hTraZDM-OYHWgPebj2MfCFzFObQ\n\n#### 3, Notes on Policy Gradients in autodiff frameworks\n>  https:\/\/aleksispi.github.io\/assets\/pg_autodiff.pdf \n","087599e6":"### Actor Critic\nThe actor critic model is a combination of a policy-based approach such as REINFORCE and a value based approach such as Q learning (notebook 3). The central idea is to swap this Monte Carlo sample of Rewards by an estimate of the action value (Q). This Q value is approximated with a Q learning agent (notebook 3). \nThe motivation for this is that Monte Carlo samples have very high variance. Using our Q network reduces this variance and gives a better value approximation than a single Monte Carlo sample. Additionally, we don't have to wait untill the end of the episode to evaluate an action. We always have our Q-network that can evaluate a state-action.\n\nFor this notebook, this does not give a noticable improvement in performance. The reason is that we do not have high variance to begin with because of our large discount factor and our simple neural network. The reason for this implementation is purely educational. ","737e8213":"**Import and install**","488a8bc7":"# Reinforcement Learning Chess\nReinforcement Learning Chess is a series of notebooks where I implement Reinforcement Learning algorithms to develop a chess AI. I start of with simpler versions (environments) that can be tackled with simple methods and gradually expand on those concepts untill I have a full-flegded chess AI.  \n\n[Notebook 1: Policy Iteration  ](https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-1-policy-iteration)  \n[Notebook 2: Model-free learning  ](http:\/\/https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-2-model-free-methods)  \n[Notebook 3: Q-networks ](https:\/\/www.kaggle.com\/arjanso\/reinforcement-learning-chess-3-q-networks)  \n\n# Notebook 4: Policy Gradients\n\n**Environment:** Just like in notebook 3, I use a simplified version of chess named capture chess. In this environment the agent (playing white) is rewarded for capturing pieces (not for checkmate). After running this notebook, you end up with an agent that can capture pieces against a random oponnent as demonstrated in the gif below.  \n**Policy Gradients:** In this notebook I introduce a different approach to reinforcement learning called policy gradients. The intuition behind policy gradients is that we want to increase the probability of actions with a high reward, and reduce the probability of actions with a low reward. This is different from the approach the previous notebooks, where focus on estimating the value of a state-action and then behaving (epsilon-) greedily w.r.t. the estimated value. The former case, which is applied here, is a policy-based method, whereas the latter case are value-based methods.\n![](https:\/\/images.chesscomfiles.com\/uploads\/game-gifs\/90px\/green\/neo\/0\/cc\/0\/0\/anJXR21DNU9mTzJNT1hZSVg0M05kTj9WTjE4MTRYMVNYNlZ4NlpTWmd4N1l4TTkya0EyQm52WXdwd0JnaGdaUWNxR3lxSSFUSTB5cTBxUVpic1o4ZW04N2dlVEpzSjc2SnM2WnNIWjc,.gif)","056ba633":"**Theory**\n- In policy gradient algorithms, we directly update the policy in the directly to optimize the expected rewards. This is different from value function approximation where we try to estimate the value of an action in a state.\n- Policy Gradient is known to work well in continuous action spaces and large discrete action spaces (like chess). \n- The Policy Gradient algorithm demonstrated here is called REINFORCE\n- The Keras implementation is designed like a supervised learning algorithm with a customized loss function (3).\n   - We train a neural network to predict which action was taken under its own policy.\n   - The loss is multiplied by  the future reward (G). This modification results in a loss identical to the policy gradient. "}}