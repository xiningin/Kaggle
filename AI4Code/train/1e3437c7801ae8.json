{"cell_type":{"89bb30b2":"code","cc69523f":"code","a223c586":"code","a310ac8c":"code","b37659fd":"code","e4d89dda":"code","0f4324e4":"code","2c276ee0":"code","012c761b":"code","454bb24f":"code","9ab5496a":"code","23e30b56":"code","2013975f":"code","3d29ab5d":"code","d3c1350f":"code","976f6e52":"code","a9e1184f":"code","94558e42":"code","78111552":"code","0f57e7fa":"code","84aa82cf":"code","c164bc69":"code","a5bd8933":"code","13612c27":"code","af6dd6a2":"code","cc83853d":"code","df13900c":"code","f6d80bb2":"code","0b56f8b8":"code","11a0800f":"code","00127299":"code","49d460fd":"code","04bf8ff6":"code","e2f47b11":"code","70433339":"code","d5deeb9f":"code","7f22c4fb":"code","32a4421f":"code","c1bb7e46":"code","fe851bae":"markdown","a0b0899e":"markdown","dc36c63d":"markdown","cfe8f187":"markdown","e8f374c8":"markdown","da52b44c":"markdown","92d18c05":"markdown","75c1774e":"markdown","2f6a6f21":"markdown","5d7ed9f5":"markdown","0cdbd0ca":"markdown","f7195651":"markdown","50f89170":"markdown","0b80bac0":"markdown","b6f48034":"markdown","6b01d5be":"markdown","721326bd":"markdown","e085b78a":"markdown","faf1254d":"markdown","9ef3a8df":"markdown","1afd6ebc":"markdown","0e76addc":"markdown"},"source":{"89bb30b2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc69523f":"import plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nfrom colorama import Fore\n\nfrom pandas_profiling import ProfileReport\nimport seaborn as sns\nfrom sklearn import metrics\nfrom scipy import stats\nimport math\n\nfrom tqdm.notebook import tqdm\nfrom copy import deepcopy\n\n# Installed libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport torch","a223c586":"# Defining all our palette colours.\nprimary_blue = \"#496595\"\nprimary_blue2 = \"#85a1c1\"\nprimary_blue3 = \"#3f4d63\"\nprimary_grey = \"#c6ccd8\"\nprimary_black = \"#202022\"\nprimary_bgcolor = \"#f4f0ea\"\n\nprimary_green = px.colors.qualitative.Plotly[2]\n\nplt.rcParams['axes.facecolor'] = primary_bgcolor","a310ac8c":"colors = [primary_blue, primary_blue2, primary_blue3, primary_grey, primary_black, primary_bgcolor, primary_green]\nsns.palplot(sns.color_palette(colors))","b37659fd":"plt.rcParams['figure.dpi'] = 120\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['font.family'] = 'serif'","e4d89dda":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv')\ntrain_df.columns = [column.lower() for column in train_df.columns]\n# train_df = train_df.drop(columns=['passengerid'])\n\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv')\ntest_df.columns = [column.lower() for column in test_df.columns]\n\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\nsubmission.head()\n\ntrain_df.head()","0f4324e4":"feature_cols = train_df.drop(['survived', 'passengerid'], axis=1).columns\ntarget_column = 'survived'\n\n## Getting all the data that are not of \"object\" type. \nnumerical_columns = ['age', 'fare']\ncategorical_columns = train_df[feature_cols].drop(columns=numerical_columns).columns\n\npure_num_cols = train_df[feature_cols].select_dtypes(include=['int64','float64']).columns\npure_cat_cols = train_df[feature_cols].select_dtypes(exclude=['int64','float64']).columns\n\nprint(len(numerical_columns), len(categorical_columns))","2c276ee0":"train_df.info()","012c761b":"fig = px.histogram(\n    train_df, \n    x=target_column, \n    color=target_column,\n    color_discrete_sequence=[primary_blue, primary_grey],\n)\nfig.update_layout(\n    title_text='Target distribution', # title of plot\n    xaxis_title_text='Value', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    paper_bgcolor=primary_bgcolor,\n    plot_bgcolor=primary_bgcolor,\n)\nfig.show()","454bb24f":"nan_data = (train_df.isna().sum().sort_values(ascending=False) \/ len(train_df) * 100)[:6]\nfig, ax = plt.subplots(1,1,figsize=(7, 5))\n\nax.bar(nan_data.index, 100, color=primary_grey, width=0.6)\n\nbar = ax.bar(\n    nan_data.index, \n    nan_data, \n    color=primary_blue, \n    width=0.6\n)\nax.bar_label(bar, fmt='%.01f %%')\nax.spines.left.set_visible(False)\nax.set_yticks([])\nax.set_title('Null Data Ratio', fontweight='bold')\n\nplt.show()","9ab5496a":"num_rows, num_cols = 2,1\nf, axes = plt.subplots(nrows=num_rows, ncols=num_cols, figsize=(16, 16), facecolor=primary_bgcolor)\nf.suptitle('Distribution of Features', fontsize=20, fontweight='bold', fontfamily='serif', x=0.13)\n\n\nfor index, column in enumerate(train_df[numerical_columns].columns):\n    i,j = (index \/\/ num_cols, index % num_cols)\n    sns.kdeplot(train_df.loc[train_df[target_column] == 0, column], color=primary_grey, shade=True, ax=axes[i])\n    sns.kdeplot(train_df.loc[train_df[target_column] == 1, column], color=primary_blue, shade=True, ax=axes[i])\n\n# f.delaxes(axes[-1, -1])\nplt.tight_layout()\nplt.show()","23e30b56":"corr = train_df[pure_num_cols].corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nfig, ax = plt.subplots(figsize=(12, 12), facecolor=primary_bgcolor)\nax.text(-1.1, 0.16, 'Correlation between the Continuous Features', fontsize=20, fontweight='bold', fontfamily='serif')\nax.text(-1.1, 0.3, 'There is no features that pass 0.4 correlation within each other', fontsize=13, fontweight='light', fontfamily='serif')\n\n\n# plot heatmap\nsns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\", cmap='coolwarm',\n            cbar_kws={\"shrink\": .8}, vmin=0, vmax=1)\n# yticks\nplt.yticks(rotation=0)\nplt.show()","2013975f":"fig = plt.figure(figsize=(12, 8), facecolor=primary_bgcolor)\ngs = fig.add_gridspec(1, 1)\nax0 = fig.add_subplot(gs[0, 0])\n\nax0.set_facecolor(primary_bgcolor)\nax0.text(-1.1, 0.26, 'Correlation of Continuous Features with Target', fontsize=20, fontweight='bold', fontfamily='serif')\nax0.text(-1.1, 0.24, 'There is no features that pass 0.3 correlation with target', fontsize=13, fontweight='light', fontfamily='serif')\n\nchart_df = pd.DataFrame(train_df[pure_num_cols].corrwith(train_df[target_column]))\nchart_df.columns = ['corr']\nsns.barplot(x=chart_df.index, y=chart_df['corr'], ax=ax0, color=primary_blue, zorder=3, edgecolor='black', linewidth=1.5)\nax0.grid(which='major', axis='y', zorder=0, color='gray', linestyle=':', dashes=(1,5))\nax0.set_ylabel('')\n\nfor s in [\"top\",\"right\", 'left']:\n    ax0.spines[s].set_visible(False)\n\nplt.show()","3d29ab5d":"train_0_df = train_df.loc[train_df[target_column] == 0]\ntrain_1_df = train_df.loc[train_df[target_column] == 1]\n\nnum_rows, num_cols = 4,2\nfig = make_subplots(rows=num_rows, cols=num_cols)\n\nfor index, column in enumerate(train_df[categorical_columns].columns):\n    i,j = ((index \/\/ num_cols)+1, (index % num_cols)+1)\n    data = train_0_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 0',\n        marker_color=primary_grey,\n        marker_line_color=primary_black,\n        marker_line_width=1.5, \n        opacity=0.8,\n    ), row=i, col=j)\n\n    data = train_1_df.groupby(column)[column].count().sort_values(ascending=False)\n    data = data if len(data) < 10 else data[:10]\n    fig.add_trace(go.Bar(\n        x = data.index,\n        y = data.values,\n        name='Label: 1',\n        marker_color=primary_blue,\n        marker_line_color=primary_black,\n        marker_line_width=1.5, \n        opacity=0.8,\n    ), row=i, col=j)\n    \n    fig.update_xaxes(\n        title=column, \n        type='category', \n        row=i, \n        col=j\n    )\n    fig.update_layout(barmode='stack')\n    \nfig.update_layout(\n    autosize=False,\n    width=800,\n    height=1300,\n    showlegend=False,\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Categorical features<\/span>',\n    paper_bgcolor=primary_bgcolor,\n    plot_bgcolor=primary_bgcolor,\n)\nfig.show()","d3c1350f":"# label encoding\nle = LabelEncoder()\nle_data = train_df.copy().drop(columns=['passengerid'])\n\nfor col in pure_cat_cols:\n    le_data[col] = le.fit_transform(le_data[col])  \n\ncorrdata = le_data\n\n## correlation \ncorr = corrdata.corr().abs()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\ncorr1 = corr.mask(mask)\n\nfig = ff.create_annotated_heatmap(\n    z=corr1.to_numpy().round(2),\n    x=list(corr1.index.values),\n    y=list(corr1.columns.values),       \n    xgap=3, ygap=3,\n    zmin=0, zmax=1,\n    colorscale='blugrn',\n    colorbar_thickness=30,\n    colorbar_ticklen=3,\n)\n\nfig.update_layout(\n    title_text='<span style=\"font-size:32px; font-family:Times New Roman\">Features Correlation Matrix<\/span>', \n    font_family=\"Serif\",\n    titlefont={'size': 24},\n    width=800, height=700,\n    xaxis_showgrid=False,\n    yaxis_showgrid=False,\n    yaxis_autorange='reversed', \n    paper_bgcolor=primary_bgcolor,\n    plot_bgcolor=primary_bgcolor,\n    margin=dict(l=70, r=70, t=70, b=70, pad=1),\n)\n\nfig.show()","976f6e52":"def plot_train_test_bar(df, column, title='Title not set'):\n    fig = px.bar(\n        df, \n        x='index',\n        y=column,\n        color='source',\n        color_discrete_sequence=[primary_blue, primary_grey],\n    )\n    fig.update_xaxes(type='category')\n    fig.update_traces(\n        marker_line_color=primary_black,\n        marker_line_width=1.5, \n        opacity=0.8,\n    )\n    fig.update_layout(\n        title_text=title, # title of plot\n        xaxis_title_text='Value', # xaxis label\n        yaxis_title_text='Count', # yaxis label\n        bargap=0.2, # gap between bars of adjacent location coordinates\n        barmode='group',\n        paper_bgcolor=primary_bgcolor,\n        plot_bgcolor=primary_bgcolor,\n    )\n    fig.show()\n    \ndef plot_train_test_line(df, column, title='Title not set'):\n    fig = go.Figure()\n    \n    for uvalue in df[column].unique():\n        print(uvalue)\n        tseries = df.query(f'{column} == \"{uvalue}\"')[column].value_counts()\n        print(tseries)\n        fig.add_trace(go.Scatter(\n            x = tseries.index,\n            y = tseries.values,\n        ))\n    fig.update_traces(\n        opacity=0.8,\n    )\n    fig.update_layout(\n        title_text=title, # title of plot\n        xaxis_title_text='Value', # xaxis label\n        yaxis_title_text='Count', # yaxis label\n        paper_bgcolor=primary_bgcolor,\n        plot_bgcolor=primary_bgcolor,\n    )\n    fig.show()","a9e1184f":"num_rows, num_cols = 3,2\nfig = make_subplots(\n    rows=num_rows, \n    cols=num_cols,\n    specs=[[{}, {}],\n           [{}, {}],\n           [{\"colspan\": 2}, None]],\n)\n\ntmp_categorical_columns = list(set(categorical_columns) - set(['name', 'ticket', 'cabin']))\n\nfor index, column in enumerate(tmp_categorical_columns):\n    i,j = ((index \/\/ num_cols)+1, (index % num_cols)+1)\n    \n    temp_train = pd.DataFrame(train_df[column].value_counts()).reset_index(drop=False)\n    temp_train['source'] = 'train'\n    \n    temp_test = pd.DataFrame(test_df[column].value_counts()).reset_index(drop=False)\n    temp_test['source'] = 'test'\n    \n    temp_combine = pd.concat([temp_train, temp_test], axis=0)\n    \n    fig.add_trace(go.Bar(\n        x = temp_train['index'],\n        y = temp_train[column],\n        name='Train',\n        marker_color=primary_blue,\n        marker_line_color=primary_black,\n        marker_line_width=1.5, \n        opacity=0.8,\n    ), row=i, col=j)\n\n    fig.add_trace(go.Bar(\n        x = temp_test['index'],\n        y = temp_test[column],\n        name='Test',\n        marker_color=primary_grey,\n        marker_line_color=primary_black,\n        marker_line_width=1.5, \n        opacity=0.8,\n    ), row=i, col=j)\n    \n    fig.update_xaxes(\n        title=column, \n        type='category', \n        row=i, \n        col=j\n    )\n    fig.update_layout(barmode='group')\n    \nfig.update_layout(\n    autosize=False,\n    width=800,\n    height=1000,\n    showlegend=False,\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Categorical features<\/span>',\n    paper_bgcolor=primary_bgcolor,\n    plot_bgcolor=primary_bgcolor,\n)\nfig.show()","94558e42":"column = 'pclass'\n\ntemp_train = pd.DataFrame(train_df[column].value_counts()).reset_index(drop=False)\ntemp_train['source'] = 'train'\ntemp_test = pd.DataFrame(test_df[column].value_counts()).reset_index(drop=False)\ntemp_test['source'] = 'test'\ntemp_combine = pd.concat([temp_train, temp_test], axis=0)\n\nplot_train_test_bar(\n    temp_combine, \n    column,\n    title = '<span style=\"font-size:36px; font-family:Times New Roman\">Pclass Distribution over Sets<\/span>',\n)","78111552":"column = 'embarked'\ntemp_train = pd.DataFrame(train_df[column].value_counts()).reset_index(drop=False)\ntemp_train['source'] = 'train'\ntemp_test = pd.DataFrame(test_df[column].value_counts()).reset_index(drop=False)\ntemp_test['source'] = 'test'\ntemp_combine = pd.concat([temp_train, temp_test], axis=0)\n\nplot_train_test_bar(\n    temp_combine, \n    column,\n    title = '<span style=\"font-size:36px; font-family:Times New Roman\">Embarked Distribution over Sets<\/span>',\n)","0f57e7fa":"column = 'sex'\ntemp_train = pd.DataFrame(train_df[column].value_counts()).reset_index(drop=False)\ntemp_train['source'] = 'train'\ntemp_test = pd.DataFrame(test_df[column].value_counts()).reset_index(drop=False)\ntemp_test['source'] = 'test'\ntemp_combine = pd.concat([temp_train, temp_test], axis=0)\n\nplot_train_test_bar(\n    temp_combine, \n    column,\n    title = '<span style=\"font-size:36px; font-family:Times New Roman\">Sex Distribution over Sets<\/span>',\n)","84aa82cf":"column = 'sibsp'\ntemp_train = pd.DataFrame(train_df[column].value_counts()).reset_index(drop=False)\ntemp_train['source'] = 'train'\ntemp_test = pd.DataFrame(test_df[column].value_counts()).reset_index(drop=False)\ntemp_test['source'] = 'test'\ntemp_combine = pd.concat([temp_train, temp_test], axis=0)\n\nplot_train_test_bar(\n    temp_combine, \n    column,\n    title = '<span style=\"font-size:36px; font-family:Times New Roman\">Sibsp Distribution over Sets<\/span>',\n)","c164bc69":"column = 'parch'\ntemp_train = pd.DataFrame(train_df[column].value_counts()).reset_index(drop=False)\ntemp_train['source'] = 'train'\ntemp_test = pd.DataFrame(test_df[column].value_counts()).reset_index(drop=False)\ntemp_test['source'] = 'test'\ntemp_combine = pd.concat([temp_train, temp_test], axis=0)\n\nplot_train_test_bar(\n    temp_combine, \n    column,\n    title = '<span style=\"font-size:36px; font-family:Times New Roman\">Parch Distribution over Sets<\/span>',\n)","a5bd8933":"temp_combine = pd.concat([train_df, test_df], axis=0)","13612c27":"sns.set_palette(sns.color_palette([primary_blue, primary_grey]))\n\nplt.rcParams['figure.dpi'] = 300\nfig = plt.figure(figsize=(5, 2), facecolor=primary_bgcolor)\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(primary_bgcolor)\n\nax0.tick_params(axis = \"y\", which = \"both\", left = False)\n\nax0.text(-10, 0.056, 'Age', color='black', fontsize=7, ha='left', va='bottom', weight='bold')\nax0.text(-10, 0.055, 'Age distribution showing a bimodal data in train and test dataset',\n         color='#292929', fontsize=5, ha='left', va='top')\n\n# KDE plots\nax0_sns = sns.kdeplot(ax=ax0, x=train_df['age'], zorder=2, shade=True)\nax0_sns = sns.kdeplot(ax=ax0, x=test_df['age'], zorder=2, shade=True)\n\n# Axis and grid customization\nax0_sns.set_xlabel(\"Age\",fontsize=5, weight='bold')\nax0_sns.set_ylabel('')\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE')\n\n# Legend params\nax0.legend(['train', 'test'], prop={'size': 5})\nax0_sns.tick_params(labelsize=5)\n\nplt.show()","af6dd6a2":"plt.rcParams['figure.dpi'] = 300\nfig = plt.figure(figsize=(5, 2), facecolor=primary_bgcolor)\ngs = fig.add_gridspec(1, 1)\ngs.update(wspace=0, hspace=0)\nax0 = fig.add_subplot(gs[0, 0])\nax0.set_facecolor(primary_bgcolor)\n\nax0.tick_params(axis = \"y\", which = \"both\", left = False)\n\nax0.text(-60, 0.0355, 'Fare', color='black', fontsize=7, ha='left', va='bottom', weight='bold')\nax0.text(-60, 0.035, 'Fare distribution between train and test sets are similar', \n         color='#292929', fontsize=5, ha='left', va='top')\n\n# KDE plots\nax0_sns = sns.kdeplot(ax=ax0, x=train_df['fare'], zorder=2, shade=True)\nax0_sns = sns.kdeplot(ax=ax0, x=test_df['fare'], zorder=2, shade=True)\n\n# Axis and grid customization\nax0_sns.set_xlabel(\"Age\",fontsize=5, weight='bold')\nax0_sns.set_ylabel('')\nax0.grid(which='major', axis='x', zorder=0, color='#EEEEEE')\nax0.grid(which='major', axis='y', zorder=0, color='#EEEEEE')\n\n# Legend params\nax0.legend(['train', 'test'], prop={'size': 5})\nax0_sns.tick_params(labelsize=5)\n\nplt.show()","cc83853d":"column = 'cabin_code'\ntrain_df[\"cabin\"] = train_df[\"cabin\"].fillna(\"Nan\")\ntrain_df[\"cabin_code\"] = train_df[\"cabin\"].str[0]\ntest_df[\"cabin\"] = test_df[\"cabin\"].fillna(\"Nan\")\ntest_df[\"cabin_code\"] = test_df[\"cabin\"].str[0]\n\ntemp_train = pd.DataFrame(train_df[column].value_counts()).reset_index(drop=False)\ntemp_train['source'] = 'train'\ntemp_test = pd.DataFrame(test_df[column].value_counts()).reset_index(drop=False)\ntemp_test['source'] = 'test'\ntemp_combine = pd.concat([temp_train, temp_test], axis=0)\n\nplot_train_test_bar(\n    temp_combine, \n    column,\n    title = '<span style=\"font-size:36px; font-family:Times New Roman\">Cabin Distribution over Sets<\/span>',\n)","df13900c":"train_df.query('ticket == \"10867\"')","f6d80bb2":"train_df.query('cabin == \"C11139\"')","0b56f8b8":"train_df.head()","11a0800f":"def fix_features(all_df):\n    # Fillna on cabin feature\n    all_df[\"cabin\"] = all_df[\"cabin\"].fillna(\"Nan\")\n    all_df[\"cabin\"] = all_df[\"cabin\"].str[0]\n\n    # Fillna Age based on pclass\n    map_age_pclass = all_df[['age', 'pclass']].dropna().groupby('pclass').mean().to_dict()\n    all_df['age'] = all_df['age'].mask(all_df['age'].isna(), all_df['pclass'].map(map_age_pclass['age']))\n\n    print(all_df['age'].isna().sum())\n\n    # Fillna Age based on pclass\n    map_age_pclass = train_df[['fare', 'pclass']].dropna().groupby('pclass').mean().to_dict()\n    all_df['fare'] = all_df['fare'].mask(all_df['fare'].isna(), all_df['pclass'].map(map_age_pclass['fare']))\n\n    print(all_df['fare'].isna().sum())\n\n    # Ticket, fillna with 'X', split string and take first split \n    all_df['ticket'] = all_df['ticket'].map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n    \n    # Name, take only surnames\n    all_df['name'] = all_df['name'].apply(lambda x: str(x).split(',')[0])\n    \n    return all_df","00127299":"all_df = pd.concat([train_df, test_df]).reset_index(drop = True)\n\nall_df = fix_features(all_df)\n\ntrain_df, test_df = all_df[:len(train_df)], all_df[len(train_df):]\nprint(train_df.shape, test_df.shape)","49d460fd":"!pip install -U lightautoml","04bf8ff6":"# Imports from our package\nfrom lightautoml.automl.presets.tabular_presets import TabularAutoML, TabularUtilizedAutoML\nfrom lightautoml.dataset.roles import DatetimeRole\nfrom lightautoml.tasks import Task\nfrom lightautoml.utils.profiler import Profiler","e2f47b11":"N_THREADS = 4 # threads cnt for lgbm and linear models\nN_FOLDS = 5 # folds cnt for AutoML\nRANDOM_STATE = 42 # fixed random state for various reasons\nTEST_SIZE = 0.2 # Test size for metric check\nTIMEOUT = 300 # Time in seconds for automl run\n\nnp.random.seed(RANDOM_STATE)\ntorch.set_num_threads(N_THREADS)","70433339":"def acc_score(y_true, y_pred, **kwargs):\n    return accuracy_score(y_true, (y_pred > 0.5).astype(int), **kwargs)\n\ndef f1_metric(y_true, y_pred, **kwargs):\n    return f1_score(y_true, (y_pred > 0.5).astype(int), **kwargs)\n\ntask = Task('binary', metric = f1_metric)","d5deeb9f":"roles = {\n    'target': 'survived',\n    'drop': ['passengerid', 'name','ticket'],\n}","7f22c4fb":"%%time \n\nautoml = TabularUtilizedAutoML(task = task, \n                       timeout = TIMEOUT,\n                       cpu_limit = N_THREADS,\n                       general_params = {'use_algos': [['linear_l2', 'lgb', 'lgb_tuned']]},\n                       reader_params = {'n_jobs': N_THREADS})\noof_pred = automl.fit_predict(train_df, roles = roles)\nprint('oof_pred:\\n{}\\nShape = {}'.format(oof_pred[:10], oof_pred.shape))","32a4421f":"%%time\n\ntest_pred = automl.predict(test_df)\nprint('Prediction for test data:\\n{}\\nShape = {}'.format(test_pred[:10], test_pred.shape))\n\nprint('Check scores...')\nprint('OOF score: {}'.format(acc_score(train_df['survived'].values, oof_pred.data[:, 0])))","c1bb7e46":"submission['Survived'] = (test_pred.data[:, 0] > 0.5).astype(int)\nsubmission.to_csv('lightautoml_utilized_300s_f1_metric.csv', index = False)","fe851bae":"we can see that in the data there are 8 people with the very same Ticket number embarked at Cherbourg and Southampton, and the passenger class (`Pclass`) was variously 1st, 2nd or 3rd, and indeed many of them were seemingly traveling alone.","a0b0899e":"<a id='1.4'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.4 Categorical features<\/p>\n\nIn this case we are going to check in general terms, the distribution of categories values based on survival chances:\n- In the case of `name` and `cabin` there seems to be no possible information to stract, but will take a dive dive into those ones.\n- In the case of `sex` and `embarked` we can see that `females` and those in the high class had more chance to survive\n\n### Individual survival rate\n\n* **Sex**\n    * `Female` has higher chance to survived compared to `male`, this may also be the result of lifeboat priority for  `female` than `male`.\n    \n* **SibSp**\n    * Most of the passengers in Synthanic are travel alone, this make the survival rate for passengers without `siblings \/ spouses` higher than passengers with siblings \/ spouses.\n    \n* **Parch**\n    * As stated earlier, that most of the passengers in Synthanic are travel alone, this also make the survival rate for passenger that travel without `parents \/ children` are higher.\n    * Survival rate for passengers that travel without `parents \/ children` is almost the same with the survival rate for passenger that travel without `siblings \/ spouses`.\n    \n* **Embarked**\n    * Passengers that embarked from `Southampton` have the highest chance to survived.\n    * The second highest survival rate are passengers that embarked from `Cherbourg`.","dc36c63d":"# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:250%; text-align:center; border-radius: 15px 50px;\">Tabular Playground Series \ud83d\udcda - April 2021 \ud83d\udcc8<\/p>\n\n![kaggle-python.png](attachment:kaggle-python.png)","cfe8f187":"### Correlation\n\nAbout the correlation, we are going to check 2 points of view:\n- The correlation between the continuos variables\n- The correlation between this continuos features and the target\n\nAs we can see, the variables are not high correlation and also, no high correlation with the class, so we are not going to delete any variable.","e8f374c8":"<a id='3'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">3. Feature Engineering \ud83d\udee0<\/p>\n\nIn this section we are going to tune the features and customize them the improve the results. We will also try to continue with original Titanic feature engineering.\n\nThe first this we are going to check is [Original Titanic Dataset and Feature Engineering](https:\/\/www.kaggle.com\/andreshg\/titanic-dicaprio-s-safety-guide-h2o-automl) to see which were the techniques that we have used.\n\nRef: https:\/\/www.kaggle.com\/carlmcbrideellis\/synthanic-feature-engineering-beware\n\nAfter taking a look of the past feature engineering techniques we can see that:\n> One of the keys to success, besides overfitting and underfitting the Titanic, is feature selection and feature engineering.\n\nBut in this case, it's synthetic data, so we are not able to do the same. As Carl said:\n\n> When creating synthetic data it is very tricky to maintain the relationships between columns. These are known as constraints. For example, imagine creating two columns of synthetic data, `Country` and `City`. When creating synthetic data it is easy to populate the `Country` column by randomly selecting countries from a list of all countries, and the same goes for `City`. However, in real world data one would expect that there are certain cities that belong in certain countries. If one does not apply such a constraint, this relationship between the two columns is lost.","da52b44c":"<a id='3.1'><\/a>\n\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">4.1 Add new Features<\/p>\n\nIn this case, we will apply distinct methods for each features to fill NaNs and add new ones\n- As we dont have any information regardin cabian because of the synthetic origin of the data, we are going to just fill it\n- As `age` is correlated the most with Pclass (0.26) we will fill `age` nans feature based on `pclass mean age`\n- As `fare` is correlated the most with Pclass (0.41) we will fill `fare` nans feature based on `pclass mean fare`\n- In the case of `name`, we will only get the surname\n\nRef: https:\/\/www.kaggle.com\/alexryzhkov\/n3-tps-april-21-lightautoml-starter\nRef: https:\/\/www.kaggle.com\/tunguz\/tps-april-2021-feature-eng-only","92d18c05":"<a id='table-of-contents'><\/a>\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">Table of Content<\/p>\n\n* [1. Data visualization: Survival Analysis \ud83d\udcca](#1)\n    * [1.1 Target](#1.1)\n    * [1.2 Data missings](#1.2)\n    * [1.3 Numerical Columns](#1.3)\n    * [1.4 Categorical Columns](#1.4)\n    * [1.5 Global correlation matrix](#1.5)\n* [2. Data visualization: Compare Train and Test \ud83d\udcca](#2)\n    * [2.1 Categorical Features](#2.1)\n    * [2.2 Age](#2.2)\n    * [2.3 Fare](#2.3)\n    * [2.4 Cabin](#2.4)\n* [3. Feature Engineering \ud83d\udee0](#3)\n    * [3.1 Add new Features](#3.1)\n* [4. LightAutoML](#4)\n","75c1774e":"### Conclusiones\n\nDirectly citting from Carl...\n\n> We can see here in this small example that the 'story' that the women from the 1st and 2nd class had better survival that including women form all classes has changed between the Titanic dataset and the Synthanic dataset. We can also see that some of the feature engineering that works for the Titanic does not work for the Synthanic. So beware when it comes to Synthanic feature engineering!","2f6a6f21":"<a id='2.4'><\/a>\n\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">2.4 Cabin<\/p>\n\n`Cabin` is the cabin number. In the original Titanic Disaster datset, this feature could be converted to a categorical variable by taking the first letter in the feature. \nAlso, due to high missing value from this feature, a new `cabin` category `N` was created to address passengers without cabin number.\n\nAnycase, it was the case of the past dataset so we will need to inspect the data latter.\n\n**Observations:**\n* Be aware that this feature has the highest number of missing value of $67.866$ in `train` dataset and $70.831$ in `test` dataset, meaaning it's almost $70%$ of the information are missing. \n* As we explained, in previos dataset, cabin could be categorize into `9 categories`, with `N` filling the missing values.\n* There is quite a distinct imbalance data between `train` and `test` dataset in cabin `C`. It's near the double.","5d7ed9f5":"<a id='2.3'><\/a>\n\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">2.3 Fare<\/p>\n\n`Fare` is the paassenger fare.\n\n**Observations:**\n* `Fare` distribution between `train` and `test` seems to be very similar with little diferences.\n* The `highest` and teh `lowest` values of fare in the `train` and `test` dataset are more or less close`.\n* The `average` fare in `train` dataset and is very close with the one in `test` dataset.\n* There are $133 - 134$ missing value in `train` and `test` datasets respectively.","0cdbd0ca":"<a id='1'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">1. Data visualization: First Overview \ud83d\udcca<\/p>","f7195651":"### <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">Pclass<\/p>\n\nPclass is a proxy for socio-economic status (SES) where 1st = Upper, 2nd = Middle and 3rd = Lower. \n\n* Though `Pclass` is dominated by `class 3` both in `train` and `test` dataset, but the proportions between each classes are different in the `train` and `test` dataset:\n\n    * `Class 3` in `train` dataset contributes `40%` while in `test` dataset, it has a contribution above `60%`.\n    * `Class 2` is at a very low of `10%` in `test` dataset while in `train` dataset, it contributes around `30%`.\n    * `Class 1` in the `train` and `test` dataset are quite the same but it is higher in the train dataset.\n    \n### <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">Embarked<\/p>\n\n* `Emarked` is the port of embarkation:\n    * `C` is Cherbourg\n    * `Q` is Queenstown \n    * `S` is Southampton\n\n**Observations:**\n* There are 3 port of embarktion, they are `Cherbourg`, `Queenstown` and `Southampton`.\n* Most of Titanic passengers are embarked from `Southampton` which contributes almost $70%$ of the passengers.\n* There are `205` missing values in `train` dataset and `277` missing values in the `test` dataset.\n\n### <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">Parch<\/p>\n\n* `Parch` is the number (#) of parents \/ children aboard the Titantic syntetic dataset.\n* `Parents` are mother and father.\n* `Child` are daughter, son, stepdaughter and stepson.\n* Some children travelled only with a nanny, therefore `Parch = 0` for them.\n\n**Observations:**\n* Number of parents \/ children can be categorize into `8 categories`, this feature quite resemble the `SibSp` feature so we will apply the same ideas here.\n* The highest number of parents \/ children is `8 people` and the lowest is 0 meaning the passengers is traveling without their parents \/ childrens.\n* It seems that `train` and `test` are equally distributed.\n    \n### <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">Sex<\/p>\n\n* `Sex` consists of `male` and `female`.\n* `Male` is dominating the `sex` feature which contributed more than `50%` of `train` and `test` dataset. \n* Same as `Pclass`, there is a proportion differences beetween `train` and `test` dataset, in `train` dataset `female` contributed more than `40%` while in `test` dataset `female` only contributed `30%` of total dataset.\n\n### <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">SibSp<\/p>\n\n* `SibSp` is number of siblings \/ spouses aboard the Titanic syntetic dataset. \n* `Siblings` are brother, sister, stepbrother and stepsister. \n* `Spouses` are husband and wife (mistresses and fiances were ignored)\n\n**Observations:**.\n\n* Number of siblings \/ spouses can be categorize into `7 categories`. For me, it should be categorical but maybe with `LabelEncoder` insted of `OneHotEncoder` but we will see in the experimentation.\n* The highest \/ maximum number of sibblings \/ spouses that is going aboard with the passengers are `8 people` and the lowest is traveling without any sibblings \/ spouses. Most of Synthanic passenger don't travel with their family.\n* A notable thing in this feature, there is arround $10k$ differences on passenger that travel with $0$ or $1$ sibbling \/ spose between `train` and `test` dataset.","50f89170":"we can see that 5 people from two ports chose to share this spacious first class cabin. Walter Yancy and Tony Lower paid \u00a3221.85 and \u00a3264.83 each. However, Erika COnley had a great deal, paying only \u00a333.69 (but probably had to sleep on the floor...)","0b80bac0":"<a id='1.5'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.5 Global correlation matrix<\/p>\n\nWe can see that name and ticket have very low correlation to survival. We can easily drop them or leave them and engineer some features from them later. Sex, Embarked and Pclass have the highest correlation with survival. The highest feature-to-feature correlation is Cabin-Pclass at 0.625.","b6f48034":"<a id='1.3'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.3 Numerical features<\/p>\n\nThe first view over numerical variables is about show the diference distributions based on the class. As we can see, the `distplot` are very similar but there are some diferences:\n- In `age` feature it seems that people between $18-42$ tends to die with higher probability than those between $40-70$ years\n- In `fare` feature, those with lower values tend to die more than those with higher values","6b01d5be":"<a id='2'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">2. Data visualization: Compare Train and Test \ud83d\udcca<\/p>\n\nFirst of all, thanks a lot to [SharIto Cope](https:\/\/www.kaggle.com\/dwin183287) for the cool charts and ideas.\n\nThis section will try to explore and compare features in the `train` and `test` dataset. It should be noted that some features are not the same between `train` and `test` dataset as can be seen more detail on each sub-sections.","721326bd":"<a id='2.1'><\/a>\n\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">2.1 Categorical Features<\/p>","e085b78a":"<a id='4'><\/a>\n[back to top](#table-of-contents)\n# <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:150%; text-align:center; border-radius: 15px 50px;\">4. LightAutoML<\/p>\n\nIn this section I will implement [Alexander's LightAutoML](https:\/\/www.kaggle.com\/alexryzhkov) library as a base model for the Syntanic problem. The main idea is to show the power of this library an get a base score to compare with the future models.\n\nRef: https:\/\/www.kaggle.com\/alexryzhkov\/tps-april-21-lightautoml-starter","faf1254d":"The first thing that we can see is that we have some missing that we will have to handle later and that, as we knew, there are categorical and numerical features. \n\nAlso, we know that the class is binary.","9ef3a8df":"<a id='1.1'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.1 Target Variable<\/p>\n\nThe first thing we are going to check is the distribution of the target feature. It's important to know if the class is balanced or not. If so, we would probably have to handle it.","1afd6ebc":"<a id='1.2'><\/a>\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">1.2 Data missings<\/p>\n\nIn this section we are going to take a fast look about the null values and their distribution.\nWe will handle those missing later,m but is important to have a first reference.\n\nRef: https:\/\/www.kaggle.com\/subinium\/tps-apr-highlighting-the-data","0e76addc":"<a id='2.2'><\/a>\n\n[back to top](#table-of-contents)\n## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:140%; text-align:center; border-radius: 15px 50px;\">2.2 Age<\/p>\n\n`Age` is fractional if it less than 1. Age estimation in the form of xx.5.\n\n**Observations:**\n* `Age` distribution between `train` and `test` dataset are different.\n* There is a big differences in `Age` distribution between `train` and `test` dataset at range of $18-40$. \n* There are missing value in the `train` and `test` dataset, they are $3.292$ and $3.487$, respectively.\n\nRef: https:\/\/www.kaggle.com\/dwin183287\/tps-apr-2021-eda\/"}}