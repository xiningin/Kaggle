{"cell_type":{"fc80fd4d":"code","6fb0bd9a":"code","1417bfc4":"code","d4a53c5e":"code","e83c4dbe":"code","83b028bd":"code","4f338d80":"code","bd7bf7bf":"code","4e4f6577":"code","da789eed":"code","f37d142d":"code","93b45c55":"code","e0d8dec9":"code","ec5b63cd":"code","cb43aaa8":"code","2977a9a8":"code","c8abaf7c":"code","a4fb061d":"code","5a053c7f":"code","80975205":"code","59e1ff5c":"code","3e7ebfd5":"code","f903356f":"code","cb5ec97d":"code","50fc1438":"code","a638b1a1":"code","d6484c17":"code","4c1468d8":"code","41c268fa":"code","c4c34b7c":"code","c8bfe610":"code","c843f88d":"code","3277d646":"code","8deff0bf":"code","c6a6ccf5":"code","6f999957":"markdown","4ac54cf9":"markdown","ef7b7b3b":"markdown","34ca63fc":"markdown","6514bf87":"markdown","3e037582":"markdown","ae9ff36f":"markdown","3503bac5":"markdown","c0c79fc0":"markdown","030d82bc":"markdown","4d670aea":"markdown","c8a34e23":"markdown","9d4db353":"markdown","c546e5c2":"markdown","622b5a95":"markdown","e32aa8f9":"markdown","e9efea54":"markdown","1605b427":"markdown","6dd29c82":"markdown","5c9d80aa":"markdown"},"source":{"fc80fd4d":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn import preprocessing\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom sklearn import metrics","6fb0bd9a":"df=pd.read_csv('..\/input\/prostate-cancer\/Prostate_Cancer.csv')","1417bfc4":"df.T","d4a53c5e":"columns=df.columns\ncolumns_new=[]\nfor i in columns:\n    columns_new.append(any(df[i].isnull()|df[i].isnull()))\ndf=df.drop(columns[columns_new],axis=1)","e83c4dbe":"{'unique patients':len(df.id.unique()), 'records':len(df.id)}","83b028bd":"ax = sns.countplot(df.diagnosis_result,label=\"Count\")       # M = 212, B = 357\ndf.diagnosis_result.value_counts()","4f338d80":"ax = sns.boxplot( palette=\"Set2\", orient=\"h\",data=df[df.diagnosis_result=='B'])","bd7bf7bf":"ax = sns.boxplot( palette=\"Set2\", orient=\"h\",data=df[df.diagnosis_result=='M'])","4e4f6577":"X_train, X_test, y_train, y_test=train_test_split(\n    df.drop(['id','diagnosis_result'], axis=1),\n    df[['diagnosis_result']],\n    test_size=0.3,\n    random_state=41)","da789eed":"X_train.shape","f37d142d":"for column in X_train.columns:\n    \n    df_train1 = X_train[(y_train.diagnosis_result=='B') & (X_train[column]<np.mean(X_train.loc[y_train.diagnosis_result=='B',column])+3*np.std(X_train.loc[y_train.diagnosis_result=='B',column]))]\n    df_test1 = X_test[(y_test.diagnosis_result=='B') & (X_test[column]<np.mean(X_train.loc[y_train.diagnosis_result=='B',column])+3*np.std(X_train.loc[y_train.diagnosis_result=='B',column]))]\n    \n    label_train1 = y_train[(y_train.diagnosis_result=='B') & (X_train[column]<np.mean(X_train.loc[y_train.diagnosis_result=='B',column])+3*np.std(X_train.loc[y_train.diagnosis_result=='B',column]))]\n    label_test1 = y_test[(y_test.diagnosis_result=='B') & (X_test[column]<np.mean(X_train.loc[y_train.diagnosis_result=='B',column])+3*np.std(X_train.loc[y_train.diagnosis_result=='B',column]))]\n    \n    df_train2 = X_train[(y_train.diagnosis_result=='M') & (X_train[column]<np.mean(X_train.loc[y_train.diagnosis_result=='M',column])+3*np.std(X_train.loc[y_train.diagnosis_result=='M',column]))]\n    df_test2 = X_test[(y_test.diagnosis_result=='M') & (X_test[column]<np.mean(X_train.loc[y_train.diagnosis_result=='M',column])+3*np.std(X_train.loc[y_train.diagnosis_result=='M',column]))]\n    \n    label_train2 = y_train[(y_train.diagnosis_result=='M') & (X_train[column]<np.mean(X_train.loc[y_train.diagnosis_result=='M',column])+3*np.std(X_train.loc[y_train.diagnosis_result=='M',column]))]\n    label_test2 = y_test[(y_test.diagnosis_result=='M') & (X_test[column]<np.mean(X_train.loc[y_train.diagnosis_result=='M',column])+3*np.std(X_train.loc[y_train.diagnosis_result=='M',column]))]    ","93b45c55":"X_train=pd.concat([df_train1,df_train2])\ny_train=pd.concat([label_train1,label_train2])\n\nX_test=pd.concat([df_test1,df_test2])\ny_test=pd.concat([label_test1,label_test2])\n\nX_train.shape","e0d8dec9":"corrMatrix = X_train.corr()\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(corrMatrix, annot=True,ax=ax)\nplt.show()","ec5b63cd":"\ncorrelated_features = set()\nfor i in range(len(corrMatrix .columns)):\n    for j in range(i):\n        if abs(corrMatrix.iloc[i, j]) > 0.7:\n            colname = corrMatrix.columns[i]\n            correlated_features.add(colname)\nprint(correlated_features)","cb43aaa8":"X_train.drop(labels=correlated_features, axis=1, inplace=True)\nX_test.drop(labels=correlated_features, axis=1, inplace=True)","2977a9a8":"corrMatrix = X_train.corr()\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(corrMatrix, annot=True,ax=ax)\nplt.show()","c8abaf7c":"constant_filter = VarianceThreshold(threshold=0.0)\nconstant_filter.fit(X_train)\nX_train = constant_filter.transform(X_train)\nX_test = constant_filter.transform(X_test)\n\nX_train.shape, X_test.shape","a4fb061d":"mm_scaler = preprocessing.StandardScaler()\nX_train = pd.DataFrame(mm_scaler.fit_transform(X_train))\nX_test = pd.DataFrame(mm_scaler.transform(X_test))","5a053c7f":"def conf_matrix(matrix,pred):\n    class_names= [0,1]# name  of classes\n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    # create heatmap\n    sns.heatmap(pd.DataFrame(matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n    ax.xaxis.set_label_position(\"top\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()","80975205":"# Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(class_weight=\"balanced\",n_estimators=200,random_state = 1)\nrf.fit(X_train, y_train.values.ravel())\ny_pred=rf.predict(X_test)\nacc = metrics.accuracy_score(y_pred,y_test.values.ravel())*100\nprint(\"Random Forest Algorithm Accuracy Score : {:.2f}%\".format(acc))","59e1ff5c":"# make class predictions with the model\ny_pred = rf.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test)\nconf_matrix(cnf_matrix,y_test)\n# calculate prediction\nreport = classification_report(y_pred,y_test)\nprint(report)","3e7ebfd5":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(X_train, y_train.values.ravel())\n\ny_pred=nb.predict(X_test)\nacc = metrics.accuracy_score(y_pred,y_test.values.ravel())*100\n\nprint(\"Accuracy of Naive Bayes: {:.2f}%\".format(acc))","f903356f":"# make class predictions with the model\ny_pred = nb.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test)\nconf_matrix(cnf_matrix,y_test)\n# calculate prediction\nreport = classification_report(y_pred,y_test)\nprint(report)","cb5ec97d":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(X_train, y_train.values.ravel())\n\ny_pred=svm.predict(X_test)\nacc = metrics.accuracy_score(y_pred,y_test.values.ravel())*100\n\nprint(\"Test Accuracy of SVM Algorithm: {:.2f}%\".format(acc))","50fc1438":"# make class predictions with the model\ny_pred = svm.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test)\nconf_matrix(cnf_matrix,y_test)\n# calculate prediction\nreport = classification_report(y_pred,y_test)\nprint(report)","a638b1a1":"# KNN Model\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# try ro find best k value\nscore = []\n\nfor i in range(1,20):\n    knn = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn.fit(X_train, y_train.values.ravel())\n    score.append(knn.score(X_test, y_test.values.ravel()))\n    \nplt.plot(range(1,20), score)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K neighbors\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(score)*100\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))","d6484c17":"knn = KNeighborsClassifier(n_neighbors =11)  # n_neighbors means k\nknn.fit(X_train, y_train.values.ravel())   ","4c1468d8":"# make class predictions with the model\ny_pred = knn.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test)\nconf_matrix(cnf_matrix,y_test)\n# calculate prediction\nreport = classification_report(y_pred,y_test)\nprint(report)","41c268fa":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(max_iter=50)\nlogreg.fit(X_train, y_train.values.ravel())\ny_pred=logreg.predict(X_test)\nacc = metrics.accuracy_score(y_pred,y_test.values.ravel())*100\nprint(\"Test Accuracy of Logistic Regression Algorithm: {:.2f}%\".format(acc))","c4c34b7c":"# make class predictions with the model\ny_pred = logreg.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test)\nconf_matrix(cnf_matrix,y_test)\n# calculate prediction\nreport = classification_report(y_pred,y_test)\nprint(report)","c8bfe610":"# define the keras model\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=X_train.shape[1], activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the keras model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit the keras model on the dataset\nmodel.fit(X_train, y_train.replace({'B':0,'M':1}), epochs=100, batch_size=8)\n# evaluate the keras model\n_, accuracy = model.evaluate(X_test, y_test.replace({'B':0,'M':1}))","c843f88d":"# make class predictions with the model\ny_pred = model.predict_classes(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test.replace({'B':0,'M':1}))\nconf_matrix(cnf_matrix,y_test)\n# calculate prediction\nreport = classification_report(y_pred,y_test.replace({'B':0,'M':1}))\nprint(report)","3277d646":"from sklearn.ensemble import VotingClassifier\n\neclf1 = VotingClassifier(estimators=[('lr', logreg), ('rf', knn)],\n                         voting='hard')\neclf1 = eclf1.fit(X_train, y_train.values.ravel())\nprint(eclf1.predict(X_test))\neclf2 = VotingClassifier(estimators=[('lr', logreg), ('rf', knn)],voting='soft')\neclf2 = eclf2.fit(X_train, y_train.values.ravel())\nprint(eclf2.predict(X_test))","8deff0bf":"# make class predictions with the model\ny_pred = eclf1.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test)\nconf_matrix(cnf_matrix,y_test)\n# calculate prediction\nreport = classification_report(y_pred,y_test)\nprint(report)","c6a6ccf5":"# make class predictions with the model\ny_pred = eclf2.predict(X_test)\ncnf_matrix = metrics.confusion_matrix(y_pred,y_test)\nconf_matrix(cnf_matrix,y_test)\n# calculate prediction\nreport = classification_report(y_pred,y_test)\nprint(report)","6f999957":"# Removing features with 0 variance","4ac54cf9":"# Data reading","ef7b7b3b":"# Recommendation","34ca63fc":"# Libraries","6514bf87":"# Esemble models","3e037582":"# Data Exploration","ae9ff36f":"# Naive Bayes","3503bac5":"# Removing outliers","c0c79fc0":"# Random Forest","030d82bc":"# Neural Network","4d670aea":"# Logistic Regression","c8a34e23":"## First esemble","9d4db353":"# KNN ","c546e5c2":"# Suport Vector Machine","622b5a95":"# Removing correlated features","e32aa8f9":"# Training the model","e9efea54":"# Train and Test spliting","1605b427":"## Second esemble","6dd29c82":"100 data points I consider is a low amount of data to work, anyway it gave a decent result for experimentation, It can have better performance with larger amounts of data.","5c9d80aa":"# Scaling the data"}}