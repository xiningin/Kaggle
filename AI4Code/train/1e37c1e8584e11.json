{"cell_type":{"fdd5400b":"code","f737c1a4":"code","d41f42db":"code","c7a65c63":"code","c07e1a9f":"code","5ea75e4e":"code","1c7c0373":"code","c4350eb1":"code","a2526f15":"code","beb8cf21":"code","56adf69a":"code","b39d7455":"code","502bc64c":"code","9555ccbe":"code","09b32105":"code","bef61925":"code","035aeee9":"code","346be1a1":"code","86cf6cee":"code","39666b09":"code","3f44ad05":"code","801a9c70":"code","9b2da210":"code","3f3b280c":"code","ae174c1e":"markdown","13c5f287":"markdown","523214e1":"markdown","5bf74c98":"markdown","cb6b5848":"markdown","16739080":"markdown","9a8e0ffd":"markdown","6af3d01c":"markdown","3d259b9d":"markdown"},"source":{"fdd5400b":"!pip install ..\/input\/pytorch-tabnet\/pytorch_tabnet-1.2.0-py3-none-any.whl","f737c1a4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom itertools import cycle\nimport torch\nfrom pytorch_tabnet.tab_model import TabNetClassifier,TabNetRegressor\npd.set_option('max_columns', 50)\nplt.style.use('seaborn-dark')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\ncolor_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])","d41f42db":"!ls -GFlash --color ..\/input\/lish-moa\/","c7a65c63":"ss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\ntrain_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')","c07e1a9f":"GENE_COLS = [c for c in train_features.columns if c[:2] == 'g-']\nCELL_COLS = [c for c in train_features.columns if c[:2] == 'c-']\nprint('Number of gene columns:', len(GENE_COLS))\nprint('Number of cell columns:', len(CELL_COLS))","5ea75e4e":"from sklearn.metrics import log_loss\ndef kaggle_metric_np(targets, preds):\n    \"\"\"\n    Kaggle metric for MoA competition targets and preds\n    in numpy format.\n    \"\"\"\n    assert targets.shape[1] == 206\n    assert preds.shape[1] == 206\n    metrics = []\n    for t in range(206):\n        metrics.append(log_loss(targets[:, t], preds[:, t], labels=[0, 1]))\n    return np.mean(metrics)","1c7c0373":"from sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n# from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.svm import LinearSVC \nfrom sklearn.metrics import log_loss\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\n\nLABEL_ENCODE_COLS = ['cp_type','cp_time','cp_dose']\nfor l in LABEL_ENCODE_COLS:\n    le = LabelEncoder()\n    train_features[f'{l}_le'] = le.fit_transform(train_features[l])\n    test_features[f'{l}_le'] = le.transform(test_features[l])\n\nFEATURES = GENE_COLS + CELL_COLS + ['cp_type_le','cp_time_le','cp_dose_le']\nTARGETS = [t for t in train_targets_scored.columns if t != 'sig_id']\n\ndf = train_features[FEATURES]\ntest_df = test_features[FEATURES]\ny = train_targets_scored[TARGETS]\n\n\n\n# X = train_features[FEATURES].values\n# X_test = test_features[FEATURES].values\n# y = train_targets_scored[TARGETS].values\n\n# Needed\n# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1)\n\n\n\n# Not needed\n# X_full = np.concatenate([X, X_test])\n\n# Standard Scale\n# scale = StandardScaler()\n# scale.fit(X_full)\n# X_train = scale.transform(X_train)\n# X_val = scale.transform(X_val)\n# X_test = scale.transform(X_test)\n\n# # Apply PCA\n# pca = PCA(n_components=100, svd_solver='full')\n# pca.fit(X_full)\n# X_train = pca.transform(X_train)\n# X_val = pca.transform(X_val)\n# X_test = pca.transform(X_test)\n# print(X_train.shape, X_val.shape, X_test.shape)","c4350eb1":"from sklearn.model_selection import KFold\nNUM_FOLDS=5\n\ndf = df.dropna().reset_index(drop=True)\ndf[\"kfold\"] = -1\ny = y.dropna().reset_index(drop=True)\ny[\"kfold\"] = -1\n\ndf = df.sample(frac=1,random_state=2020).reset_index(drop=True)\ny = y.sample(frac=1,random_state=2020).reset_index(drop=True)\n\nkf = KFold(n_splits=NUM_FOLDS)\n\nfor fold, (trn_, val_) in enumerate(kf.split(X=df, y=y)):\n    df.loc[val_, 'kfold'] = fold\n    y.loc[val_,'kfold'] = fold","a2526f15":"y.loc[val_,'kfold']","beb8cf21":"y_test = np.zeros((test_df.shape[0],len(TARGETS), NUM_FOLDS))","56adf69a":"features=FEATURES\ntarget_features = TARGETS\ndef run(fold):\n    df_train = df[df.kfold != fold]\n    df_valid = df[df.kfold == fold]\n    \n    X_train = df_train[features].values\n    Y_train = y[y.kfold!=fold][TARGETS].values\n#     Y_train = df_train[target_features].values\n    \n    X_valid = df_valid[features].values\n    Y_valid = y[y.kfold==fold][TARGETS].values\n#     Y_valid = df_valid[target_features].values\n    \n    y_oof = np.zeros((df_valid.shape[0],len(target_features)))   # Out of folds validation\n    \n    print(\"--------Training Begining for fold {}-------------\".format(fold+1))\n     \n    model.fit(X_train = X_train,\n             y_train = Y_train,\n             X_valid = X_valid,\n             y_valid = Y_valid,\n             max_epochs = 1000,\n             patience =70)\n              \n    \n    print(\"--------Validating For fold {}------------\".format(fold+1))\n    \n    y_oof = model.predict(X_valid)\n    y_test[:,:,fold] = model.predict(test_df.values)\n    \n    val_score = kaggle_metric_np(Y_valid,y_oof)\n    \n    print(\"Validation score: {:<8.5f}\".format(val_score))\n    \n    # VISUALIZTION\n    plt.figure(figsize=(12,6))\n    plt.plot(model.history['train']['loss'])\n    plt.plot(model.history['valid']['loss'])","b39d7455":"import warnings\nwarnings.simplefilter(\"ignore\")\n\n# clf = OneVsRestClassifier(SVC(probability=True))\nmodel = TabNetRegressor(n_d=64,\n                       n_a=64,\n                       n_steps=8,\n                       gamma=1.9,\n                       n_independent=4,\n                       n_shared=5,\n                       seed=2020,\n                       optimizer_fn = torch.optim.Adam,\n                       scheduler_params = {\"milestones\": [150,250,300,350,400,450],'gamma':0.2},\n                       scheduler_fn=torch.optim.lr_scheduler.MultiStepLR)\n# clf.fit(X_train = X_train,\n#              y_train = y_train,\n#              X_valid = X_val,\n#              y_valid = y_val,\n#              max_epochs = 1000,\n#              patience =70)\n# clf.fit(X_train,y_train)\n# pred_train = clf.predict_proba(X_train)\n# pred_val = clf.predict_proba(X_val)\n# pred_test = clf.predict_proba(X_test)","502bc64c":"run(fold=0)","9555ccbe":"run(fold=1)","09b32105":"run(fold=2)","bef61925":"run(fold=3)","035aeee9":"run(fold=4)","346be1a1":"# run(fold=5)","86cf6cee":"# run(fold=6)","39666b09":"y_test = y_test.mean(axis=-1)","3f44ad05":"sub = pd.DataFrame(y_test, columns=TARGETS)\nsub['sig_id'] = test_features['sig_id'].values","801a9c70":"sub.shape, ss.shape","9b2da210":"sub.to_csv('submission.csv', index=False)","3f3b280c":"sub","ae174c1e":"For every `sig_id` you will be predicting the probability that the sample had a positive response for each <MoA> target. For N sig_id rows and M <MoA> targets, you will be making N\u00d7M predictions. Submissions are scored by the log loss:\n\n\n$$ \\text{score} = - \\frac{1}{M}\\sum_{m=1}^{M} \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_{i,m} \\log(\\hat{y}_{i,m}) + (1 - y_{i,m}) \\log(1 - \\hat{y}_{i,m})\\right] $$\n\n- \\(N\\) is the number of sig_id observations in the test data (\\(i=1,\u2026,N\\))\n- \\(M\\) is the number of scored MoA targets (\\(m=1,\u2026,M\\))\n- \\( \\hat{y}_{i,m} \\) is the predicted probability of a positive MoA response for a sig_id\n- \\( y_{i,m} \\) is the ground truth, 1 for a positive response, 0 otherwise\n- \\( log() \\) is the natural (base e) logarithm\n    \nNote: the actual submitted predicted probabilities are replaced with max(min(p,1\u221210\u221215),10\u221215). A smaller log loss is better.\n\n\n    ","13c5f287":"#### Hello Kagglers. It's been a while since I have been active here on Kaggle. \n\nThis notebook is my TabNet based submission to MoA competition.\nThe code is a MoA kinda version of [Tanul](https:\/\/www.kaggle.com\/tanulsingh077)'s TabNet Notebook [here](https:\/\/www.kaggle.com\/tanulsingh077\/achieving-sota-results-with-tabnet).\n\nIt scores **0.02252** on the current public leaderboard\n\nThe data preprocessing and loading code is borrowed from[ Rob](https:\/\/www.kaggle.com\/robikscube)'s notebook [here](https:\/\/www.kaggle.com\/robikscube\/mechanisms-of-action-moa-prediction-starter)","523214e1":"First we can look at the data format. Everything is stored as a CSV, and the largest file is only 150MB.","5bf74c98":"# Features\nThe start of the column:\n- `g-` signify gene expression data\n- `c-` signify cell viability data.\n- `cp_type` indicates samples treated with a compound (cp_vehicle) or with a control perturbation (ctrl_vehicle); control perturbations have no MoAs; \n- `cp_time` and `cp_dose` indicate treatment duration (24, 48, 72 hours) and dose (high or low).","cb6b5848":"# Make Submission","16739080":"# Multiclass Model","9a8e0ffd":"### *Drop an upvote if this looks good. I believe there are lots of tuning that can be done and will update you on the same :)*","6af3d01c":"# Evaluation Criteria","3d259b9d":"# Introduction and Credits"}}