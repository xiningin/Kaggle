{"cell_type":{"712b96fb":"code","c7d977fa":"code","7a655576":"code","acd9f749":"code","157d6030":"code","3d0690a8":"code","751e9123":"code","7f9b49d4":"code","60d691fc":"code","689b9850":"code","561969de":"code","b7537099":"code","6083f471":"code","a9cdbcd9":"code","3cb9d118":"code","27b0f70f":"code","cc182354":"code","b66ece83":"code","028b6403":"code","bee512c9":"code","37f0fd7b":"code","e942f9e9":"markdown","0b2737ff":"markdown","feeb3f2c":"markdown","72e5f937":"markdown","77088533":"markdown","c0a6f558":"markdown","04a5de17":"markdown","0cdc7d5a":"markdown","77415f5b":"markdown","fed6551d":"markdown","0d331d18":"markdown","db9b80fb":"markdown","eb65c557":"markdown","fbee6d88":"markdown","a5e8be8f":"markdown","23127a3e":"markdown"},"source":{"712b96fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# ref: #https:\/\/www.kaggle.com\/jaimebecerraguerrero\/simple-yet-powerful-bankrupt-prediction-model\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c7d977fa":"# runtime\nimport timeit\n\n# Data manipulation\nimport pandas as pd\nimport numpy as np\n\n# Data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n# preprocessing\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, Normalizer\n\n# Ml model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nfrom collections import Counter\nfrom sklearn.datasets import make_classification\nfrom imblearn.over_sampling import SMOTE\nfrom matplotlib import pyplot\nfrom numpy import where\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\n\nnp.warnings.filterwarnings('ignore')","7a655576":"data_df=pd.read_csv(\"\/kaggle\/input\/company-bankruptcy-prediction\/data.csv\")","acd9f749":"data_df.head()","157d6030":"display(data_df.isnull().sum())\nprint(\"====\/\/\/====\")\ndisplay(data_df.duplicated().sum())\nprint(\"====\/\/\/====\")\ndisplay(data_df.info())","3d0690a8":"# training set\nX = data_df.iloc[:,1:].values\ny = data_df.iloc[:,0].values.reshape(-1, 1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)","751e9123":"# determining optimal number of features\nn_features = [5, 10, 15, 20, 25, 30, 35, 40]\nfor i in n_features:\n    # Building the model based feature selection\n    select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=i)\n\n    select.fit(X_train, y_train)\n\n    mask = select.get_support()\n\n    X_train_rfe = select.transform(X_train)\n    X_test_rfe = select.transform(X_test)\n\n    score = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n    \n    print(\"Test score: {:.3f}\".format(score), \" number of features: {}\".format(i))","7f9b49d4":"select = RFE(RandomForestClassifier(n_estimators=100, random_state=42), n_features_to_select=15)\n\nselect.fit(X_train, y_train)\n\nmask = select.get_support()\n\nmask","60d691fc":"X_train_rfe = select.transform(X_train)\nX_test_rfe = select.transform(X_test)\n\nscore = RandomForestClassifier().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\n\nprint(\"Test score: {:.3f}\".format(score), \" number of features: {}\".format(15))\n\nfeatures = pd.DataFrame({'features':list(data_df.iloc[:,1:].keys()), 'select':list(mask)})\nfeatures = list(features[features['select']==True]['features'])\nfeatures.append('Bankrupt?')","689b9850":"data_df = data_df[features]\n\n\ndata_df.info()","561969de":"import matplotlib.pyplot as plt","b7537099":"# Split feature as categorical and continuous features\n\ncat_features=data_df.select_dtypes(exclude=np.number).columns\ncont_features=data_df.select_dtypes(include=[np.number,'float64','int64']).columns\n\nfeatures=print(data_df.columns)","6083f471":"# Exploring categorical data\n\nplt.figure(figsize=(20,8))\nfor index, col in enumerate(cat_features, start=1):\n    plt.subplot(4,4,index)\n    plt.title(col)\n    plt.pie(data_df[col].value_counts(), labels=data_df[col].value_counts().index)","a9cdbcd9":"# Exploring continuous data\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,12))\nfor index, col in enumerate(cont_features, start=1):\n    plt.subplot(4,5,index)\n    plt.title(col)\n    sns.distplot(data_df[col], kde_kws={'bw':0.5}) # bw is bandwith","3cb9d118":"# Using Boxplot\n\nimport numpy as np\nimport seaborn as sns\ncont_dummy=cont_features#.drop('store_and_fwd_flag')\nplt.figure(figsize=(20,16))\nfor index, col in enumerate(cont_dummy, start=1):\n    plt.subplot(4,5,index)\n    plt.title(col)\n    ax = sns.boxplot(data_df[col])\n","27b0f70f":"corrmat = data_df.corr(method='pearson')\nf, ax = plt.subplots(figsize=(12, 10))\nsns.heatmap(corrmat, ax=ax, cmap=\"YlGnBu\", linewidths=0.1, annot=True)","cc182354":"def cap_outliers(series, zscore_threshold=3, iqr_threshold=1.5, verbose=False, method=\"IQR\"):\n    if method==\"IQR\":\n        #https:\/\/stackoverflow.com\/questions\/59489073\/how-to-not-remove-but-handle-outliers-by-transforming-using-pandas\n        '''Caps outliers to closest existing value within threshold (IQR).'''\n        Q1 = series.quantile(0.25)\n        Q3 = series.quantile(0.75)\n        IQR = Q3 - Q1\n\n        lbound = Q1 - iqr_threshold * IQR\n        ubound = Q3 + iqr_threshold * IQR\n\n        outliers = (series < lbound) | (series > ubound)\n\n        series = series.copy()\n        series.loc[series < lbound] = series.loc[~outliers].min()\n        series.loc[series > ubound] = series.loc[~outliers].max()\n\n        # For comparison purposes.\n        if verbose:\n                print('\\n'.join(\n                    ['Capping outliers by the IQR method:',\n                     f'   IQR threshold: {iqr_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"mean\":\n        '''Caps outliers to closest existing value within threshold (Modified Z-score).'''\n        median_val = series.median()\n        mad_val = series.mad() # Median absolute deviation\n\n        z_score = (series - median_val) \/ mad_val\n        outliers = abs(z_score) > zscore_threshold\n\n        series = series.copy()\n        series.loc[z_score > zscore_threshold] = series.loc[~outliers].max()\n        series.loc[z_score < -zscore_threshold] = series.loc[~outliers].min() \n\n        # For comparison purposes.\n        if verbose:\n                lbound = median_val - zscore_threshold * mad_val\n                ubound = median_val + zscore_threshold * mad_val\n                print('\\n'.join(\n                    ['Capping outliers by the Modified Z-score method:',\n                     f'   Z-score threshold: {zscore_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"zscore\":\n        '''Caps outliers to closest existing value within threshold (Modified Z-score).'''\n        median_val = series.median()\n        mad_val = series.mad() # Median absolute deviation\n\n        z_score = (series - median_val) \/ mad_val\n        outliers = abs(z_score) > zscore_threshold\n\n        series = series.copy()\n        series.loc[z_score > zscore_threshold] = series.loc[~outliers].mean()\n        series.loc[z_score < -zscore_threshold] = series.loc[~outliers].mean() \n\n        # For comparison purposes.\n        if verbose:\n                lbound = median_val - zscore_threshold * mad_val\n                ubound = median_val + zscore_threshold * mad_val\n                print('\\n'.join(\n                    ['Capping outliers by the Modified Z-score method:',\n                     f'   Z-score threshold: {zscore_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"median\":\n        '''Caps outliers to closest existing value within threshold (Modified Z-score).'''\n        median_val = series.median()\n        mad_val = series.mad() # Median absolute deviation\n\n        z_score = (series - median_val) \/ mad_val\n        outliers = abs(z_score) > zscore_threshold\n\n        series = series.copy()\n        series.loc[z_score > zscore_threshold] = series.loc[~outliers].max()\n        series.loc[z_score < -zscore_threshold] = series.loc[~outliers].min() \n\n        # For comparison purposes.\n        if verbose:\n                lbound = median_val - zscore_threshold * mad_val\n                ubound = median_val + zscore_threshold * mad_val\n                print('\\n'.join(\n                    ['Capping outliers by the Modified Z-score method:',\n                     f'   Z-score threshold: {zscore_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"zscore\":\n        '''Caps outliers to closest existing value within threshold (Modified Z-score).'''\n        median_val = series.median()\n        mad_val = series.mad() # Median absolute deviation\n\n        z_score = (series - median_val) \/ mad_val\n        outliers = abs(z_score) > zscore_threshold\n\n        series = series.copy()\n        series.loc[z_score > zscore_threshold] = series.loc[~outliers].median()\n        series.loc[z_score < -zscore_threshold] = series.loc[~outliers].median() \n\n        # For comparison purposes.\n        if verbose:\n                lbound = median_val - zscore_threshold * mad_val\n                ubound = median_val + zscore_threshold * mad_val\n                print('\\n'.join(\n                    ['Capping outliers by the Modified Z-score method:',\n                     f'   Z-score threshold: {zscore_threshold}',\n                     f'   Lower bound: {lbound}',\n                     f'   Upper bound: {ubound}\\n']))\n    elif method==\"log\":\n        #https:\/\/stackoverflow.com\/questions\/37890849\/pandas-series-log-normalize\n        series=series.map(lambda x: np.log(x))\n        \n\n    return series\n\n\nremove_list = [' Net Value Growth Rate', 'Bankrupt?']   \nfeatures_treating = list(data_df.columns.values)\n\nres = [i for i in features_treating if i not in remove_list]\n\nfeatures_treating = res\nfeatures_treating\nfor i, col in enumerate(features_treating):\n    data_df[col]=cap_outliers(data_df[col],method=\"IQR\", verbose=True)","b66ece83":"# Verification of Deleting Outliers\n\ncont_dummy=cont_features#.drop('store_and_fwd_flag')\nplt.figure(figsize=(20,16))\nfor index, col in enumerate(cont_dummy, start=1):\n    plt.subplot(4,5,index)\n    plt.title(col)\n    ax = sns.boxplot(data_df[col])","028b6403":"X=data_df.drop('Bankrupt?',axis=1) #axis=1 drop the bulk of column\ny=data_df['Bankrupt?']\n\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)\n\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.25,random_state=0)\n\n#StSc = StandardScaler()\n#X_train  = StSc.fit_transform(X_train)\n#X_test  = StSc.fit_transform(X_test)","bee512c9":"for i, col in enumerate([XGBClassifier(eval_metric=\"logloss\"),RandomForestClassifier(),LogisticRegression()]):\n    print(col)\n    pipeModel=Pipeline([('scaler', StandardScaler()), ('model', col)])\n    \n    # the benefit of using K-Fold is that we could calculate the cross validation value using some of the methods of scoring \n    kfold = KFold(n_splits=10, shuffle=True, random_state=0)\n    cv_results = cross_val_score(pipeModel, X_train, y_train, cv=kfold)\n\n    pipeModel.fit(X_train, y_train)\n    # this is the scaled LR\n    print('Score for',col,'method:', pipeModel.score(X_test, y_test))\n    # the mean result (10 data) of negative mean squared error\n    print('Score for',col,'method using cross_val_score:', cv_results.mean())\n    y_pred=pipeModel.predict(X_test)\n    #print(mean_squared_error(y_test, y_pred))\n    print(classification_report(y_test,y_pred))\n    roc_auc_score(y_test,y_pred)","37f0fd7b":"from sklearn.metrics import confusion_matrix\ngbrt = GradientBoostingClassifier(n_estimators=100, random_state=42, max_depth=1).fit(X_train, y_train)\ny_pred=gbrt.predict(X_test)\nprint(\"training set score : {:.2f}\".format(gbrt.score(X_train, y_train)))\nprint(\"test set score: {:.2f}\".format(gbrt.score(X_test, y_test)))\n\nconfusion_matrix(y_test, y_pred)\n","e942f9e9":"<a id=\"8\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>8. Conclusion<\/b><\/font><br>","0b2737ff":"<a id=\"3\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3. Find Null and Duplicate Values<\/b><\/font><br>\n","feeb3f2c":"We have a problem regarding the large number of features to choose from. Jaime in his [notebook](https:\/\/www.kaggle.com\/jaimebecerraguerrero\/simple-yet-powerful-bankrupt-prediction-model) describes the feature selection using the random forest classifier which is great for reducing computational load.\n\nRecursive feature elimination (RFE) is a feature selection process that suits a model and eliminates the weakest feature (or features) before the required number of features is achieved. Features are rated by the model's coef_ or feature importances_ attributes, and RFE aims to remove dependencies and collinearity by recursively deleting a small number of features per loop.","72e5f937":"> We have trained a model with training set score 90% and test set score 90% using Gradient Boosting Classifier. Also, we have removed outliers using Interquartiles Caping Outliers Methods. ","77088533":"<a id=\"7\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>7. Modelling<\/b><\/font><br>","c0a6f558":"<a id=\"1\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>1. Load Packages<\/b><\/font><br>","04a5de17":"> No null values detected","0cdc7d5a":"<a id=\"6\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>6. Handling Outliers<\/b><\/font><br>","77415f5b":"<a id=\"4\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>4. Feature Selection<\/b><\/font><br>","fed6551d":"> We need to overcome the outliers problem","0d331d18":"## STEP 1: UNIVARIATE ANALYSIS","db9b80fb":"<center> <h3> Please Upvote if you like the work \ud83d\ude0a <\/h3><\/center>\n<center>Don't hesitate to give your comment on this notebook. It gives me motivation to improve the analysis in the future<\/center>","eb65c557":"<center>\n    <h1>   \n        ==#== Bankruptcy Predictive Model ==#==\n    <\/h1>\n<\/center>\n \n<center><img src=\"https:\/\/cdn.corporatefinanceinstitute.com\/assets\/bankruptcy-1024x627.jpeg\" alt=\"Bankruptcy\" width=\"600\" ><\/center>\n\n<center> <h3> Please Upvote if you like the work \ud83d\ude0a <\/h3><\/center>\n\n<center>Don't hesitate to give your comment on this notebook. It gives me motivation to improve the analysis in the future<\/center>\n\n# Introduction\nThis program is used to find the right model for the bankruptcy model. \n\n# Contents\nContents:\n\n* [1. Load Packages](#1)\n* [2. Load Dataset](#2)\n* [3. Drop NaN and duplicates values](#3)\n* [4. Do Explanatory Data Analysis](#4)\n* [5. Mapping Sentiment](#5) \n* [6. Modelling](#6)","fbee6d88":"# STEP 2: BIVARIATE\/MULTIVARIATE ANALYSIS","a5e8be8f":"<a id=\"2\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>2. Load Dataset<\/b><\/font><br>\n","23127a3e":"<a id=\"5\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>5. Exploratory Data Analysis<\/b><\/font><br>"}}