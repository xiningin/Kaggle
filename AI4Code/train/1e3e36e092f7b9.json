{"cell_type":{"57ac9078":"code","ae4343bb":"code","2922ba8e":"code","ebd82dd1":"code","c5dae7f2":"code","6f5375cb":"code","f0b0346e":"code","2ee878c5":"code","bdb2c668":"code","dd7a5ef3":"code","b3c8b989":"code","e055f08b":"code","102a7339":"code","70b7b138":"code","18769990":"code","c83556c8":"code","a3dfb5c0":"code","b00a3dbd":"code","71412482":"code","299f3245":"code","3470477b":"code","bccb4249":"code","a0e9ace6":"markdown","b5d08307":"markdown","e4208155":"markdown","1eb17195":"markdown","800beb80":"markdown","d1a5f4a5":"markdown","3e7b1f10":"markdown","66679257":"markdown","b0c2198a":"markdown","e0746378":"markdown","d0130d40":"markdown","f95a4a4b":"markdown","51655afa":"markdown","350b5464":"markdown","0565bb69":"markdown","f050c4a7":"markdown"},"source":{"57ac9078":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae4343bb":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.utils import to_categorical\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score\n\n%matplotlib inline","2922ba8e":"tf.__version__","ebd82dd1":"pd.set_option('display.max_columns', None)\n\ntrain_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')","c5dae7f2":"print(\"Shape of Training data: \", train_df.shape)\nprint(\"Shape of Test data: \", test_df.shape)","6f5375cb":"train_df.head(10)","f0b0346e":"test_df.head(10)","2ee878c5":"X_train = train_df.drop(columns='label', axis=1).to_numpy()\ny_train = train_df['label'].to_numpy()\nX_test = test_df.to_numpy()\n\nprint(\"Shape of X_train: \", X_train.shape)\nprint(\"Shape of y_train: \", y_train.shape)\nprint(\"Shape of X_test: \", X_test.shape)","bdb2c668":"X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n\ninput_shape = (28,28,1)\n\nprint(\"Shape of X_train : \", X_train.shape)\nprint(\"Shape of X_test : \", X_test.shape)","dd7a5ef3":"y_train[0:11]","b3c8b989":"y_cat_train = to_categorical(y_train, 10)\ny_cat_train[0:11]","e055f08b":"X_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\nX_train \/= 255\nX_test \/= 255","102a7339":"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='relu',input_shape=input_shape))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])","70b7b138":"model.summary()","18769990":"model.fit(X_train, y_cat_train, epochs=20)","c83556c8":"training_metrics = pd.DataFrame(model.history.history)\ntraining_metrics.columns","a3dfb5c0":"training_metrics.head()","b00a3dbd":"training_metrics['loss'].plot()","71412482":"training_metrics['accuracy'].plot()","299f3245":"predictions = np.argmax(model.predict(X_test), axis=-1)","3470477b":"submission = pd.DataFrame({'ImageID': range(1, len(test_df)+1), 'Label': predictions})\nsubmission.head(10)","bccb4249":"submission.to_csv('Predictions.csv', index=False)","a0e9ace6":"### Data Preprocessing:","b5d08307":"### One Hot Encoding of target labels:","e4208155":"### Model Training Performance:","1eb17195":"In this project, we will try to identify hand written digits by using the power of Deep learning. We will be working on the MNIST dataset to create a deep learning classification model & see how our model performs in acurately predicting images with the correct digit notation.\n\nMNIST is a database of handwritten digits made up of a training set of 60,000 examples, and a test set of 10,000 examples. The training examples are annotated by humans with the correct answer. For instance, if the handwritten digit is the number \"3\", then 3 is simply the label associated with that example.\n\nWe will train our model with the samples available in the training set, and then use the test set to evaluate how well our neural network has learned to recognize digits.\n\nLet's create a Convolutional Neural Network to solve this problem.","800beb80":"### Model Training:","d1a5f4a5":"We are going to use OHE as a simple tool to encode information used inside neural networks. \nIn many applications it is convenient to transform categorical (non-numerical) features into numerical variables. For instance, the categorical feature \"digit\" with value d in [0 \u2013 9] can be encoded into a binary vector with 10 positions, which always has 0 value except the d - th position where a 1 is present.\n\nFor example, the digit 3 can be encoded as [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]. This type of representation is called One-hot encoding, or sometimes simply one-hot, and is very common in data mining when the learning algorithm is specialized in dealing with numerical functions.","3e7b1f10":"### Model Creation:","66679257":"### Importing necessary libraries:","b0c2198a":"### Model Predictions:","e0746378":"### Model Summary:","d0130d40":"### Handwritten Digit Recognizer using Deep Learning:","f95a4a4b":"### Visualizing Dataset:","51655afa":"Now we will create our CNN model. A CNN model generally consists of convolutional and pooling layers. It works better for data that are represented as grid structures, this is the reason why CNN works well for image classification problems.","350b5464":"### Scaling feature data:","0565bb69":"The image data cannot be fed directly into the model. So we need to perform some operations and process the data to make it ready for our neural network. The dimension of the training data is (60000,28,28). The CNN model will require one more dimension so we reshape the matrix to shape (60000,28,28,1). This extra dimension is for the color channel, for grayscale images like MNIST, its value is 1. For color images, the channel value is 3 corresponding to Red, Green & Blue (RGB).","f050c4a7":"### Load Train & Test data:"}}