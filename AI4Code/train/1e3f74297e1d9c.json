{"cell_type":{"b723357f":"code","a8bcb0a7":"code","09cb21f7":"code","e5381a64":"code","0b4908df":"code","7676326b":"code","6693505b":"code","17757feb":"code","54809f24":"code","b8fe7498":"code","f397a978":"code","59f56d92":"code","89279b32":"code","0aac8714":"code","e566f4f9":"code","c4b05237":"code","f5541901":"code","6da3394f":"code","fe029b9a":"code","5e2541ff":"code","c36eec12":"code","f37c18a6":"code","442d58b0":"code","cef5c980":"code","d7547d14":"code","4b6dbade":"code","6d17516a":"code","4c274809":"code","3638e7ee":"code","55bd0068":"code","998f755f":"code","76470ee9":"code","e6ed66d9":"code","c7aaeb77":"code","24215e54":"code","f7bba7b1":"code","d8ad4162":"code","4c6138de":"code","6119c9ae":"code","d2b4c05e":"code","7cc8b23d":"code","02741c0d":"code","8fc375eb":"code","75c50466":"code","50b85194":"code","cf1c32c8":"code","12dac744":"code","021cf55f":"markdown","048d627f":"markdown","64ba6416":"markdown","3a7715cd":"markdown","03e4caff":"markdown","06a906a5":"markdown","cdf4e4ac":"markdown","27cda9df":"markdown","acbfd44b":"markdown","14c13db3":"markdown","d683db03":"markdown","0a10a720":"markdown","196a7707":"markdown","62a26b9c":"markdown","06fb8a46":"markdown","accb6bb7":"markdown","2e450819":"markdown","3e0aa324":"markdown","f7b7e6f6":"markdown","b4503913":"markdown","db4aac77":"markdown","32bebf0a":"markdown","2ba1c6a7":"markdown","cfe3c020":"markdown","94d54409":"markdown","1b820367":"markdown"},"source":{"b723357f":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","a8bcb0a7":"train_df=pd.read_csv(\"..\/input\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/test.csv\")","09cb21f7":"train_df.head()","e5381a64":"\nprint('__Test_DataSet_')\ntest_df.head()","0b4908df":"def missingdata(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    ms=pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    ms= ms[ms[\"Percent\"] > 0]\n    f,ax =plt.subplots(figsize=(8,6))\n    plt.xticks(rotation='90')\n    fig=sns.barplot(ms.index, ms[\"Percent\"],color=\"green\",alpha=0.8)\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15)\n    return ms","7676326b":"missingdata(train_df)","6693505b":"missingdata(test_df)","17757feb":"test_df['Age'].mean()","54809f24":"train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)","b8fe7498":"test_df['Fare'].fillna(test_df['Fare'].median(), inplace = True)","f397a978":"drop_column = ['Cabin']\ntrain_df.drop(drop_column, axis=1, inplace = True)\ntest_df.drop(drop_column,axis=1,inplace=True)","59f56d92":"test_df['Age'].fillna(test_df['Age'].median(), inplace = True)\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace = True)","89279b32":"print('check the nan value in train data')\nprint(train_df.isnull().sum())\nprint('___'*30)\nprint('check the nan value in test data')\nprint(test_df.isnull().sum())","0aac8714":"## combine test and train as single to apply some function\nall_data=[train_df,test_df]\n","e566f4f9":"# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in all_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","c4b05237":"import re\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in all_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in all_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', \n                                                 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n","f5541901":"## create bin for age features\nfor dataset in all_data:\n    dataset['Age_bin'] = pd.cut(dataset['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])\n","6da3394f":"## create bin for fare features\nfor dataset in all_data:\n    dataset['Fare_bin'] = pd.cut(dataset['Fare'], bins=[0,7.91,14.45,31,120], labels=['Low_fare','median_fare',\n                                                                                      'Average_fare','high_fare'])","fe029b9a":"### for our reference making a copy of both DataSet start working for copy of dataset\ntraindf=train_df\ntestdf=test_df","5e2541ff":"all_dat=[traindf,testdf]","c36eec12":"for dataset in all_dat:\n    drop_column = ['Age','Fare','Name','Ticket']\n    dataset.drop(drop_column, axis=1, inplace = True)","f37c18a6":"drop_column = ['PassengerId']\ntraindf.drop(drop_column, axis=1, inplace = True)","442d58b0":"testdf.head(2)","cef5c980":"\ntraindf = pd.get_dummies(traindf, columns = [\"Sex\",\"Title\",\"Age_bin\",\"Embarked\",\"Fare_bin\"],\n                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])","d7547d14":"testdf = pd.get_dummies(testdf, columns = [\"Sex\",\"Title\",\"Age_bin\",\"Embarked\",\"Fare_bin\"],\n                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])","4b6dbade":"testdf.head()","6d17516a":"sns.heatmap(traindf.corr(),annot=True,cmap='RdYlGn',linewidths=0.2) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(20,12)\nplt.show()","4c274809":"g = sns.pairplot(data=train_df, hue='Survived', palette = 'seismic',\n                 size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","3638e7ee":"\nfrom sklearn.model_selection import train_test_split #for split the data\nfrom sklearn.metrics import accuracy_score  #for accuracy_score\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nall_features = traindf.drop(\"Survived\",axis=1)\nTargeted_feature = traindf[\"Survived\"]\nX_train,X_test,y_train,y_test = train_test_split(all_features,Targeted_feature,test_size=0.3,random_state=42)\nX_train.shape,X_test.shape,y_train.shape,y_test.shape","55bd0068":"# machine learning\nfrom sklearn.linear_model import LogisticRegression # Logistic Regression\n\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\nprediction_lr=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Logistic Regression is',round(accuracy_score(prediction_lr,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_lr=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Logistic REgression is:',round(result_lr.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1.05, size=15)\n\n\n","998f755f":"# Random Forests\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(criterion='gini', n_estimators=700,\n                             min_samples_split=10,min_samples_leaf=1,\n                             max_features='auto',oob_score=True,\n                             random_state=1,n_jobs=-1)\nmodel.fit(X_train,y_train)\nprediction_rm=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Random Forest Classifier is',round(accuracy_score(prediction_rm,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_rm=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Random Forest Classifier is:',round(result_rm.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1.05, size=15)\n\n","76470ee9":"# Support Vector Machines\nfrom sklearn.svm import SVC, LinearSVC\n\nmodel = SVC()\nmodel.fit(X_train,y_train)\nprediction_svm=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Support Vector Machines Classifier is',round(accuracy_score(prediction_svm,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_svm=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Support Vector Machines Classifier is:',round(result_svm.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1.05, size=15)","e6ed66d9":"##knn\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nmodel = KNeighborsClassifier(n_neighbors = 4)\nmodel.fit(X_train,y_train)\nprediction_knn=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the K Nearst Neighbors Classifier is',round(accuracy_score(prediction_knn,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_knn=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for K Nearest Neighbors Classifier is:',round(result_knn.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1.05, size=15)","c7aaeb77":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nmodel= GaussianNB()\nmodel.fit(X_train,y_train)\nprediction_gnb=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Gaussian Naive Bayes Classifier is',round(accuracy_score(prediction_gnb,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_gnb=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Gaussian Naive Bayes classifier is:',round(result_gnb.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1.05, size=15)","24215e54":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nmodel= DecisionTreeClassifier(criterion='gini', \n                             min_samples_split=10,min_samples_leaf=1,\n                             max_features='auto')\nmodel.fit(X_train,y_train)\nprediction_tree=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the DecisionTree Classifier is',round(accuracy_score(prediction_tree,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_tree=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Decision Tree classifier is:',round(result_tree.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1.05, size=15)\n","f7bba7b1":"from sklearn.ensemble import AdaBoostClassifier\nmodel= AdaBoostClassifier()\nmodel.fit(X_train,y_train)\nprediction_adb=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the AdaBoostClassifier is',round(accuracy_score(prediction_adb,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_adb=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoostClassifier is:',round(result_adb.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1.05, size=15)\n","d8ad4162":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nmodel= LinearDiscriminantAnalysis()\nmodel.fit(X_train,y_train)\nprediction_lda=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the LinearDiscriminantAnalysis is',round(accuracy_score(prediction_lda,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_lda=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoostClassifier is:',round(result_lda.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1.05, size=15)\n","4c6138de":"from sklearn.ensemble import GradientBoostingClassifier\nmodel= GradientBoostingClassifier()\nmodel.fit(X_train,y_train)\nprediction_gbc=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Gradient Boosting Classifier is',round(accuracy_score(prediction_gbc,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_gbc=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoostClassifier is:',round(result_gbc.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)\nsns.heatmap(confusion_matrix(Targeted_feature,y_pred),annot=True,fmt='3.0f',cmap=\"summer\")\nplt.title('Confusion_matrix', y=1.05, size=15)\n","6119c9ae":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'AdaBoostClassifier', \n              'Gradient Decent', 'Linear Discriminant Analysis', \n              'Decision Tree'],\n    'Score': [result_svm.mean(), result_knn.mean(), result_lr.mean(), \n              result_rm.mean(), result_gnb.mean(), result_adb.mean(), \n              result_gbc.mean(), result_lda.mean(), result_tree.mean()]})\nmodels.sort_values(by='Score',ascending=False)","d2b4c05e":"train_X = traindf.drop(\"Survived\", axis=1)\ntrain_Y=traindf[\"Survived\"]\ntest_X  = testdf.drop(\"PassengerId\", axis=1).copy()\ntrain_X.shape, train_Y.shape, test_X.shape","7cc8b23d":"# Gradient boosting tunning\nimport xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel = GradientBoostingClassifier()\nparam_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300,400],\n              'learning_rate': [0.1, 0.05, 0.01,0.001],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.2,0.1] \n              }\n\nmodelf = GridSearchCV(model,param_grid = param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodelf.fit(train_X,train_Y)\n\n# Best score\nmodelf.best_score_\n\n# Best Estimator\nmodelf.best_estimator_","02741c0d":"modelf.best_score_","8fc375eb":"# Random Forest Classifier Parameters tunning \nmodel = RandomForestClassifier()\nn_estim=range(100,1000,100)\n\n## Search grid for optimal parameters\nparam_grid = {\"n_estimators\" :n_estim}\n\n\nmodel_rf = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodel_rf.fit(train_X,train_Y)\n\n\n\n# Best score\nprint(model_rf.best_score_)\n\n#best estimator\nmodel_rf.best_estimator_","75c50466":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nmodel =LinearDiscriminantAnalysis()\nparam_grid = {'tol':[0.001,0.01,.1,.2]}\n\nmodell = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodell.fit(train_X,train_Y)\n\n# Best score\nprint(modell.best_score_)\n\n# Best Estimator\nmodell.best_estimator_\n","50b85194":"model= SVC()\nparam_grid = {'kernel': ['rbf','linear'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\nmodelsvm = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodelsvm.fit(train_X,train_Y)\n\nprint(modelsvm.best_estimator_)\n\n# Best score\nprint(\"best score\",modelsvm.best_score_)","cf1c32c8":"# Random Forests\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nrandom_forest.fit(train_X, train_Y)\nY_pred_rf = random_forest.predict(test_X)\nrandom_forest.score(train_X,train_Y)\nacc_random_forest = round(random_forest.score(train_X, train_Y) * 100, 2)\n\npd.Series(random_forest.feature_importances_,train_X.columns).sort_values(ascending=True).plot.barh(width=0.8)\nprint('__'*30)\nprint(\" Best Score: \", acc_random_forest)\nprint(\" THANK YOU, Submitted By Jugal Budhlani(A-30)\")","12dac744":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred_rf})","021cf55f":"### Pairplots\nFinally let us generate some pairplots to observe the distribution of data from one feature to the other. Once again we use Seaborn to help us.","048d627f":"### Apply the Estimator which got from parameter tuning of Random Forest ","64ba6416":"## Support Vector Machines","3a7715cd":"By looking at all the matrices, we can say that Random Forest & SVM  classifier has a higher chance in correctly predicting dead passengers.","03e4caff":"###  Model\nNow we are ready to train a model and predict the required solution. There are lot of predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\nLogistic Regression\n\nKNN\n\nSupport Vector Machines\n\nNaive Bayes classifier\n\nDecision Tree\n\nRandom Forrest\n\nLinear Discriminant Analysis\n\nAda Boost Classifier \n\nGradient Boosting Classifier\n\nAnd also compared above given classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure and plot accuracy based confusion matrix","06a906a5":"## Random Forest Classifier ","cdf4e4ac":"### Correlation Between The Features","27cda9df":"### Filling missing Values","acbfd44b":"### Identifying Missing Value ","14c13db3":"## AdaBoost","d683db03":"### Cabin Featueres has more than 75% of missing data in both Test and train data so we are remove the Cabin ","0a10a720":"## Hyper-Parameters Tuning\n\nThe machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning\n\nSo based on the above given  acuracy result i will performance Grid search and random search for the \nSVM \n\nLDA\n\nLogistic Regression \n\nGradient Decent Classifier\n\nRandom Forest Classifier\n\nParameters\n\nJust a quick summary of the parameters that we will be listing here for completeness,\n\nn_jobs : Number of cores used for the training process. If set to -1, all cores are used.\n\nn_estimators : Number of classification trees in your learning model ( set to 10 per default)\n\nmax_depth : Maximum depth of tree, or how much a node should be expanded. Beware if set to too high a number would run the risk of overfitting as one would be growing the tree too deep\n\nverbose : Controls whether you want to output any text during the learning process. A value of 0 suppresses all text while a value of 3 outputs the tree learning process at every iteration.\n\nPlease check out the full description via the official Sklearn website. There you will find that there are a whole host of other useful parameters that you can play around with.","196a7707":"### Load the DataSet","62a26b9c":"### now every thing almost ready only one step we converted the catergical features in numerical by using dummy variable","06fb8a46":"### Feature engineering\n\nFeature engineering is the art of converting raw data into useful features. There are several feature engineering techniques that you can apply to be an artist. A comprehensive list of them is presented by Heaton (2016). We will use just two techniques:\n\nBox-Cox transformations (Box & Cox 1964)\n\nPolynomials generation through non-linear expansions.\nBefore the application of these techniques, we will just make some adjustments to the data, in order to prepare it for the modelling process.","accb6bb7":"The data has been split into two groups:\n\ntraining set (train.csv)\ntest set (test.csv)\nThe training set should be used to build your machine learning models. For the training set, we provide the outcome (also known as the \u201cground truth\u201d) for each passenger. Your model will be based on \u201cfeatures\u201d like passengers\u2019 gender and class. You can also use feature engineering to create new features.\n\nThe test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. For each passenger in the test set, use the model you trained to predict whether or not they survived the sinking of the Titanic.\n\n\n### Data Dictionary\nVariable\tDefinition\tKey\n\nsurvival\tSurvival\t0 = No, 1 = Yes\n\npclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex\tMale or Female\n\nAge\tAge in years\t\n\nsibsp\t# of siblings \/ spouses aboard the Titanic\t\n\nparch\t# of parents \/ children aboard the Titanic\t\n\nticket\tTicket number\t\n\nfare\tPassenger fare\t\n\ncabin\tCabin number\t\n\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n\nVariable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson\n\nSome children travelled only with a nanny, therefore parch=0 for them","2e450819":"## Model evaluation\nWe can now rank our evaluation of all the models to choose the best one for our problem.","3e0aa324":"## KNN Classifier","f7b7e6f6":"### Both the test and train Age features contains more the 15% of missing Data so we are fill with the median","b4503913":"## Gaussian Naive Bayes","db4aac77":"### Topic will be convered in build the Machine Learning Model\n\nA) Introduction\n\nB) Load the data\n\nC) Filling missing Values\n\nD) Feature engineering\n\nE) Modeling\n\nF) Prediction","32bebf0a":"Interpreting The Heatmap\nThe first thing to note is that only the numeric features are compared as it is obvious that we cannot correlate between alphabets or strings. Before understanding the plot, let us see what exactly correlation is.\n\nPOSITIVE CORRELATION: If an increase in feature A leads to increase in feature B, then they are positively correlated. A value 1 means perfect positive correlation.\n\nNEGATIVE CORRELATION: If an increase in feature A leads to decrease in feature B, then they are negatively correlated. A value -1 means perfect negative correlation.\n\nNow lets say that two features are highly or perfectly correlated, so the increase in one leads to increase in the other. This means that both the features are containing highly similar information and there is very little or no variance in information. This is known as MultiColinearity as both of them contains almost the same information.\n\nSo do you think we should use both of them as one of them is redundant. While making or training models, we should try to eliminate redundant features as it reduces training time and many such advantages.\n\nNow from the above heatmap,we can see that the features are not much correlated. The highest correlation is between SibSp and Parch i.e 0.41. So we can carry on with all features.","2ba1c6a7":"## Linear Discriminant Analysis","cfe3c020":"## LogisticRegression","94d54409":"## Gradient Boosting Classifier","1b820367":"#### Introduction\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. That's why the name DieTanic. This is a very unforgetable disaster that no one in the world can forget.\n\nIt took about $7.5 million to build the Titanic and it sunk under the ocean due to collision. The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle.\n\nThe Objective of this notebook is to give an idea how is the workflow in any predictive modeling problem. How do we check features, how do we add new features and some Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it.\n\nIf You Like the notebook and think that it helped you **PLEASE UPVOTE**. It will keep me motivated."}}