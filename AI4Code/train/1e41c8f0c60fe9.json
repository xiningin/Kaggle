{"cell_type":{"61e4757a":"code","2b93a3f3":"code","b959706b":"code","04967549":"code","00dddd3c":"code","4559b289":"code","adf81851":"code","78ec8271":"code","e73fce4d":"code","4629006f":"code","55e251c3":"code","e2b044af":"code","fdefc5b3":"code","0a8e3125":"code","0e000447":"code","d885c07a":"code","346c5cac":"code","e121d118":"markdown","578dc1a5":"markdown","53a2cac2":"markdown","097e848e":"markdown","58b8c9aa":"markdown","50ee3d02":"markdown","43dcaa88":"markdown","40717cdc":"markdown","5b0fb8e0":"markdown","73a79255":"markdown","ae2e0561":"markdown","dc694dd3":"markdown","5d6675c8":"markdown","a7a1bb46":"markdown","6ce44154":"markdown","1518a734":"markdown","ba301c4b":"markdown","6321138c":"markdown"},"source":{"61e4757a":"import pandas as pd \nimport numpy as np \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")","2b93a3f3":"train = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv\")\ntest = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv\")\nss = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/sample_submission.csv\")","b959706b":"train.head(2)","04967549":"test.head()","00dddd3c":"ss.head()","4559b289":"print(train.columns.tolist(), end = \"\\n\\n\")\n\nprint(test.columns.tolist() , end = \"\\n\\n\" )\n\nprint(ss.columns.tolist())","adf81851":"print(train.language.value_counts(), end = \"\\n\\n\")\nsns.countplot(\"language\" , data = train)","78ec8271":"print(test.language.value_counts(), end = \"\\n\\n\")\nsns.countplot(\"language\" , data = test)","e73fce4d":"train_h = train [ train[\"language\"] == \"hindi\" ]\ntrain_t = train [ train[\"language\"] == \"tamil\" ]","4629006f":"print(\"Hindi: \\n\\n\")\ndisplay(sorted(train_h[\"question\"].tolist() , key = lambda x : len(x.split(sep = \" \")) )[:3])\nprint(\"\\n\\nTamil: \\n\\n\")\ndisplay(sorted(train_t[\"question\"].tolist() , key = lambda x : len(x.split(sep = \" \")) )[:3])","55e251c3":"print(\"Hindi: \\n\\n\")\ndisplay(sorted(train_h[\"question\"].tolist() , key = lambda x : len(x.split(sep = \" \")) )[-3:])\nprint(\"\\n\\nTamil: \\n\\n\")\ndisplay(sorted(train_t[\"question\"].tolist() , key = lambda x : len(x.split(sep = \" \")) )[-3:])","e2b044af":"plt.figure(figsize = ( 15, 8))\nsns.displot( data =  pd.DataFrame({ \"Hindi\" : train_h[\"question\"].map(lambda x : len(x.split(sep = \" \"))) , \n                           \"Tamil\" : train_t[\"question\"].map(lambda x : len(x.split(sep = \" \")) ) }) ) ","fdefc5b3":"plt.figure(figsize = ( 15, 8))\nsns.displot( data =  pd.DataFrame({ \"Hindi\" : train_h[\"context\"].map(lambda x : len(x.split(sep = \" \"))) , \n                           \"Tamil\" : train_t[\"context\"].map(lambda x : len(x.split(sep = \" \")) ) }) ) ","0a8e3125":"sns.distplot(train_h[\"answer_start\"])","0e000447":"sns.distplot(train_h[\"answer_start\"])","d885c07a":"plt.figure(figsize = ( 15, 8))\nsns.displot( data =  pd.DataFrame({ \"Hindi\" : train_h[\"answer_text\"].map(lambda x : len(x.split(sep = \" \"))) , \n                           \"Tamil\" : train_t[\"answer_text\"].map(lambda x : len(x.split(sep = \" \")) ) }) ) ","346c5cac":"example = pd.DataFrame()\n\nexample[\"input_text\"] = [ a + k for a,k in zip(train_h[\"context\"].tolist() , train_h[\"question\"].tolist())]\nexample[\"answer_start\"] = train_h[\"answer_start\"].tolist()\nexample[\"answer_end\"] = [ a + len(k) for a,k in zip(train_h[\"answer_start\"].tolist() , train_h[\"answer_text\"].tolist())]\n\n\nexample.head()","e121d118":"#### **Observation** : The hindi text data points are almost twice of the tamil text data points ","578dc1a5":"#### **Observations:** the starting position of an answer can be very high. \n\n> NOTE: while developing a simple baseline using machine learning techniques we can exclude shuch points ","53a2cac2":"## lets have a look at outputs ","097e848e":"## Let's analyze the language distributions ","58b8c9aa":"### Happy to view your thoughts on different approches :) ","50ee3d02":"### Shortest questions( based on number of words )","43dcaa88":"# Simple EDA \n\nIn this notebook,\n\n1. We will explore and understand the data.\n2. Visualize the distributions of input and output data, understand the outliers. \n3. Disciss possible ML approchs to solve the problem. \n","40717cdc":"### Distribution of hindi and tamil answers length ","5b0fb8e0":"# Question ..... \n\n## With this high level understanding of the data can we think of a simple machine learning approch to solve this problem ?","73a79255":"## Lets analyze Hindi and Tamil text differently  ","ae2e0561":"### For example, \n\n#### for a single language \n#### One text input ---> Two outputs ( start and end position of answer in the contex) ","dc694dd3":"### Longest questions( based on number of words )","5d6675c8":"####  **Observations:** Most of the Hindi questions are longer than tamil questions \n\n> NOTE: this EDA is to look at the data from  high level perspective and not going deep into language understanding","a7a1bb46":"### Distribution of questions length ","6ce44154":"#### **Observations:**\n* Both the distributions are almost similler.\n* Most of the answers contains less than 10 words. ","1518a734":"### answer_start","ba301c4b":"## Summery \n\n1. We have observed what is input and what is output. \n2. We have discussed input and output distributions. \n ","6321138c":"#### **Observation:** the distribution of hindi and tamil context lengths is similler "}}