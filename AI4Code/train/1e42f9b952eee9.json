{"cell_type":{"ae6a86c2":"code","5ee6352d":"code","087bcb95":"code","88b67db8":"code","0ee84f30":"code","7bf4f018":"code","852cda44":"code","dd910a95":"code","87912eba":"code","810911d2":"code","8328d12a":"code","c4f16495":"code","ec0eadbf":"code","bfdc6a59":"code","c63a1961":"code","d7885436":"code","93363841":"code","f789661d":"code","10e360af":"code","5ac64439":"markdown","354e526b":"markdown","4802ff87":"markdown","eec1ed99":"markdown","922a324a":"markdown"},"source":{"ae6a86c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5ee6352d":"import matplotlib.pyplot as plt \nimport plotly.express as px \nimport seaborn as sns \nimport os \nimport requests, json \nimport cv2 \nfrom PIL import Image \nimport math \nfrom tqdm import tqdm \nfrom typing import Dict \nimport wandb \nfrom wandb.keras import WandbCallback\nfrom kaggle_secrets import UserSecretsClient \nimport time \nfrom statistics import mean\nimport random \nfrom wordcloud import WordCloud\nimport gc \n\nimport re \nimport nltk \nfrom nltk.corpus import stopwords\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.model_selection import StratifiedKFold \n\nimport tensorflow as tf \nfrom tensorflow import keras \nfrom tensorflow.keras.layers import Input, Dense, Embedding, GRU, Flatten, Dropout, concatenate, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ReduceLROnPlateau","087bcb95":"!apt-get install p7zip\n!p7zip -d -f -k \/kaggle\/input\/mercari-price-suggestion-challenge\/train.tsv.7z\n!unzip -o \/kaggle\/input\/mercari-price-suggestion-challenge\/sample_submission_stg2.csv.zip\n!unzip -o \/kaggle\/input\/mercari-price-suggestion-challenge\/test_stg2.tsv.zip","88b67db8":"config = dict(\n    competition = \"mercari\", \n    train = True, \n    type = \"train\", \n    inferece = True, \n    debug = False,\n    model_name = \"rnn\", \n    device = \"cpu\", \n    \n    n_fold = 4, \n    seed = 42, \n    batch_size = 20000, \n    epoch = 10, \n    \n)","0ee84f30":"# user_secrets = UserSecretsClient()\n# url = user_secrets.get_secret(\"WEB_HOOK_URL\") \n\n# user_secrets = UserSecretsClient()\n# api = user_secrets.get_secret(\"wandb_api\")\n\n\n# def setup_db(fold):\n#     if fold == 0:\n#         wandb.login(key=api)\n#     run = wandb.init(\n#         project = config[\"competition\"], \n#         name = config[\"model_name\"], \n#         config = config, \n#         group = config[\"model_name\"], \n#         job_type = config[\"type\"]\n#     )\n\n# def slack(txt):\n#     requests.post(url, data=json.dumps({\n#         \"username\": \"kaggle\", \n#         \"text\": txt \n#     }))","7bf4f018":"def reduce_mem_usage(train_data):\n    start_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    for col in train_data.columns:\n        col_type = train_data[col].dtype\n\n        if col_type != object:\n            c_min = train_data[col].min()\n            c_max = train_data[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    train_data[col] = train_data[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    train_data[col] = train_data[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    train_data[col] = train_data[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    train_data[col] = train_data[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    train_data[col] = train_data[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    train_data[col] = train_data[col].astype(np.float32)\n                else:\n                    train_data[col] = train_data[col].astype(np.float64)\n        else:\n            train_data[col] = train_data[col].astype('category')\n\n    end_mem = train_data.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return train_data","852cda44":"%%time \n\nif config[\"debug\"]:\n    train = pd.read_csv(\"train.tsv\", sep=\"\\t\", nrows=8000)    \n    test = pd.read_csv(\"test_stg2.tsv\", sep=\"\\t\", nrows=8000)\n    \nelse:\n    train = pd.read_csv(\"train.tsv\", sep=\"\\t\")    \n    test = pd.read_csv(\"test_stg2.tsv\", sep=\"\\t\")\n    \n\ntrain = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","dd910a95":"train.head()","87912eba":"def get_top(x):\n    if type(x) != list:\n        return \"\"\n    else:\n        return x[0]\ndef get_sub(x):\n    if type(x) != list:\n        return \"\"\n    else:\n        return x[1]\ndef get_item(x):\n    if type(x) != list:\n        return \"\"\n    else:\n        return x[2]\n    \n    \nnltk.download('stopwords')\nstop_words = stopwords.words('english')\nnon_alphanums = re.compile(u'[^A-Za-z0-9]+')\n\ndef cln_txt(x):\n    new = []\n    for t in x.split():\n        if t in stop_words: continue\n        t = t.lower()\n        t = non_alphanums.sub(\"\", t)\n        if t == \"\": continue \n        new.append(t)\n    return \" \".join(new)\n\n    \ndef add_feature(train, test):\n    last_train_shpae = train.shape[0]\n    df = pd.concat([train.drop([\"price\", \"train_id\"], axis=1), test.drop(\"test_id\", axis=1)])\n    y = train[\"price\"]\n    del train, test \n    gc.collect()\n    \n    df[\"name\"] = df.name.astype(\"category\")\n    df[\"category_name\"] = df.category_name.astype(\"category\")\n    df[\"brand_name\"] = df.brand_name.astype(\"category\")\n    df[\"item_description\"] = df.item_description.astype(\"category\")\n    \n    # category name \n    df[\"category_isnan\"] = df.category_name.isnull()\n    df[\"category_isnan\"] = df.category_isnan.apply(lambda x: 1 if x is True else 0)\n    df[\"category_len\"] =  df.category_name.apply(lambda x: x.split(\"\/\")) \n    \n    df[\"top\"] = df.category_len.apply(get_top)\n    df[\"sub\"] = df.category_len.apply(get_sub)\n    df[\"item\"] = df.category_len.apply(get_item)\n    df[\"category_len\"] = df.category_len.apply(lambda x: 0 if type(x) != list else len(x))\n    \n    df.drop([\"category_name\"], axis=1, inplace=True)\n    \n    # brand_name \n    df[\"no_brand\"] = df.brand_name.isnull()\n    df[\"no_brand\"] = df.no_brand.apply(lambda x: 1 if x is True else 0)\n    \n    # item_discription \n    df[\"not_discription\"] = df.item_description.apply(lambda x: 1 if x == \"No description yet\" else 0)\n    df[\"item_description_dummy\"] = df.item_description.apply(lambda x: \"\" if x == \"No description yet\" else x)\n    df[\"discription_len\"] = df.item_description_dummy.apply(lambda x: len(x.split()))\n    \n    df.drop(\"item_description_dummy\", axis=1, inplace=True)\n    \n    # name \n    df[\"name_len\"] = df.name.apply(lambda x: len(x.split()))\n    \n    #shipping_confident \n    df[\"condition_shipping\"] = df[\"item_condition_id\"].astype(str) + \"_\" + df[\"shipping\"].astype(str)\n    \n    # label encoder \n    cate_col = [ \"top\", \"sub\", \"item\", \"condition_shipping\"]\n    for col in cate_col:\n        la = LabelEncoder()\n        df[col] = la.fit_transform(df[col])\n        \n    # nlp \n    df[\"name\"] = df.name.apply(cln_txt)\n    df[\"brand_name\"] = df.brand_name.apply(cln_txt)\n    df[\"item_description\"] = df.item_description.apply(cln_txt)\n    \n#     df[\"name\"] = df.name.astype(\"category\")\n    df[\"brand_name\"] = df.brand_name.astype(\"category\")\n#     df[\"item_description\"] = df.item_description.astype(\"category\")\n    \n#     df[\"name\"] = df.name.cat.codes \n    df[\"brand_name\"] = df.brand_name.cat.codes \n#     df[\"item_description\"] = df.item_description.cat.codes \n    \n    train = df.iloc[:last_train_shpae]\n    train[\"price\"] = y \n    test = df.iloc[last_train_shpae:]\n    \n    del df, y \n    gc.collect()\n        \n    return train, test ","810911d2":"%%time\n\ntrain, test = add_feature(train, test)","8328d12a":"\n'''\ncreate vocabulary class Lang.\nname and discription transform tokenizer. \n\n'''\n\nclass Lang(object):\n    def __init__(self, name):\n        self.name = name \n        self.word2index = {\"<CLS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3, \"<PAD>\": 0}\n        self.index2word = {}\n        self.word2count = {}\n        \n        self._create_vocab()\n        \n    def _create_vocab(self):\n        for name in self.name:\n            if type(name) != str: continue \n            for txt in name.split():\n                if txt not in self.word2index:\n                    self.word2index[txt] = len(self.word2index)\n                    self.word2count[txt] = 1 \n                else:\n                    self.word2count[txt] += 1 \n        self.index2word = {v: k for k, v in self.word2index.items()}\n    \n    def fit_transform(self, doc):\n        token_list = []\n        token_list.append(self.word2index[\"<CLS>\"])\n        if type(doc) != str:\n            return token_list.append(self.word2index[\"<EOS>\"])\n        for txt in doc.split():\n            if txt in self.word2index:\n                token = self.word2index[txt]\n                token_list.append(token)\n            else:\n                token_list.append(self.word2index[\"<UNK>\"])\n        token_list.append(self.word2index[\"<EOS>\"])\n        return token_list \n    \n    \ndef transform_nlp(train, test):\n    last_train_shape = train.shape[0]\n    # name \n    df_name = pd.concat([train[[\"name\"]], test[[\"name\"]]])\n    lang_name = Lang(df_name[\"name\"].astype(str).to_list())\n    \n    df_name[\"name\"] = df_name.name.apply(lang_name.fit_transform)\n    \n    train[\"name\"] = df_name.iloc[:last_train_shape][\"name\"]\n    test[\"name\"] = df_name.iloc[last_train_shape:][\"name\"]\n    vocab_name = len(lang_name.word2index)\n    cnt_name = lang_name.word2count\n    del df_name, lang_name \n    gc.collect()\n    print(1)\n    \n    # discription \n    df_dis = pd.concat([train[[\"item_description\"]], test[[\"item_description\"]]])\n    lang_dis = Lang(df_dis[\"item_description\"].astype(str).to_list())\n    \n    df_dis[\"item_description\"] = df_dis.item_description.apply(lang_dis.fit_transform)\n    \n    train[\"item_description\"] = df_dis.iloc[:last_train_shape][\"item_description\"]\n    test[\"item_description\"] = df_dis.iloc[last_train_shape:][\"item_description\"]\n    vocab_dis = len(lang_dis.word2index)\n    cnt_dis = lang_dis.word2count\n    del df_dis, lang_dis\n    gc.collect()\n    print(2)\n    \n    return train, test, vocab_name, vocab_dis, cnt_name, cnt_dis\n","c4f16495":"%%time \n\ntrain, test, vocab_name, vocab_dis, cnt_name, cnt_dis = transform_nlp(train, test)","ec0eadbf":"train.head()","bfdc6a59":"train.isnull().sum().to_frame()","c63a1961":"def show_cloud(name, desc):\n    name = WordCloud(width=1500, height=1100, background_color=\"white\", max_words=15).generate_from_frequencies(name)\n    desc = WordCloud(width=1500, height=1100, background_color=\"white\", max_words=15).generate_from_frequencies(desc)\n    \n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    ax = axes.ravel()\n    \n    ax[0].imshow(name)\n    ax[0].set_title(\"name vocab\")\n    \n    ax[1].imshow(desc)\n    ax[1].set_title(\"discription vocab\")\n    \n    plt.tight_layout()\n    \nshow_cloud(cnt_name, cnt_dis)\n\ndel cnt_name, cnt_dis \ngc.collect()","d7885436":"def get_keras_data(dataset: pd.DataFrame) -> Dict[str, np.ndarray]:\n    feature_col = dataset.drop([\"name\", \"item_description\"], axis=1).columns\n    x = {\n        \"name\": pad_sequences(dataset.name, maxlen=10),\n        \"item_desc\": pad_sequences(dataset.item_description, maxlen=128),\n        \"feature\": np.array(dataset[feature_col].values)\n    }\n    return x \n\nx = get_keras_data(train.drop(\"price\", axis=1))\nprint(x)","93363841":"\n'''\n\u6587\u7ae0\u306f\u30c8\u30fc\u30af\u30f3\u5358\u4f4d\u3067\u57cb\u3081\u8fbc\u307f\u7a7a\u9593\u306b\u62e1\u5f35\u3057\u3066RNN\u306b\u3088\u308a\u6642\u7cfb\u5217\u306e\u6700\u7d42\u96a0\u308c\u5c64\u3092\u53d6\u5f97\u3059\u308b\u3002\n\u305d\u306e\u4ed6\u306f\u7dda\u5f62\u4ee3\u6570\u306b\u3088\u308b\u901a\u5e38\u5909\u63db\u3002\n\u306e\u3061\u306b\uff12\u3064\u306e\u51fa\u529b\u3092\u7d50\u5408\u3057\u3066\u6700\u7d42\u51fa\u529b\u3068\u3059\u308b\u30e2\u30c7\u30eb\u3002\n\n\u53c2\u7167: https:\/\/www.kaggle.com\/knowledgegrappler\/a-simple-nn-solution-with-keras-0-48611-pl\n'''\n\n\nparams = {\n    \"hidden_dim\": 64,\n    \"name_shape\": x[\"name\"].shape[1], \n    \"desc_shape\": x[\"item_desc\"].shape[1],\n    \"feature_shape\": x[\"feature\"].shape[1]\n}\n\n\n# tensorflow\ndef rmsle_cust(y_true, y_pred):\n    first_log = K.log(K.clip(y_pred, K.epsilon(), None) + 1.)\n    second_log = K.log(K.clip(y_true, K.epsilon(), None) + 1.)\n    return K.sqrt(K.mean(K.square(first_log - second_log), axis=-1))\n\n# ndarray \ndef rmsle(y, y_pred):\n    assert len(y) == len(y_pred)\n    to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n    return (sum(to_sum) * (1.0\/len(y))) ** 0.5\n\n\ndef build_model(\n    name_shape=params[\"name_shape\"],\n    desc_shape=params[\"desc_shape\"],\n    feature_shape=params[\"feature_shape\"]\n):\n    name = Input(shape=[name_shape], name=\"name\")\n    desc = Input(shape=[desc_shape], name=\"item_desc\")\n    feature = Input(shape=[feature_shape], name=\"feature\")\n    \n    emb_name = Embedding(vocab_name, params[\"hidden_dim\"])(name)\n    emb_desc = Embedding(vocab_dis, params[\"hidden_dim\"])(desc)\n    \n    hs_name = GRU(8)(emb_name)\n    hs_desc = GRU(16)(emb_desc)\n    hs_feature = Dense(32, activation=\"relu\")(feature)\n    \n    x = concatenate([\n        Flatten()(hs_name),\n        Flatten()(hs_desc),\n        hs_feature,\n    ])\n    \n    x = Dropout(0.1)(Dense(128, activation=\"relu\")(x))\n    x = Dropout(0.1)(Dense(64, activation=\"relu\")(x))\n    out = Dense(1)(x)\n    model = Model([name, desc, feature], out)\n    model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\"mae\", rmsle_cust])\n    return model \n\nmodel = build_model()\nmodel.summary()","f789661d":"if config[\"debug\"] is not True and config[\"device\"] == \"tpu\":\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n\ndef get_callbacks(fold):\n    lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=10, verbose=1)\n    es = EarlyStopping(monitor=\"val_loss\", patience=60, verbose=1, \n                           mode=\"auto\", restore_best_weights=True)\n    os.makedirs(\"models\", exist_ok=True)\n    checkpoint_filepath = f\"models\/rnn_{str(fold)}.hdf5\"\n    sv = keras.callbacks.ModelCheckpoint(\n            checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n            save_weights_only=False, mode='auto', save_freq='epoch',\n            options=None\n    )\n#     wb = WandbCallback(log_weights=True)\n    return lr, es, sv\n\ndef submit(pred, name):\n    sub = pd.read_csv(\".\/sample_submission_stg2.csv\")\n    sub = sub[[\"test_id\"]]\n    sub[\"price\"] = pred \n    sub.to_csv(\"submission.csv\", index=False)\n    del sub\n    gc.collect()\n    \n\ndef main(train, test):\n#     with tpu_strategy.scope():\n        predict_val, val_idx, predict_test = [], [], []\n        train[\"rank\"] = pd.cut(train.price, bins=10, labels=False)\n        kf = StratifiedKFold(n_splits=2 if config[\"debug\"] else config[\"n_fold\"], random_state=config[\"seed\"], shuffle=True)\n\n        for fold, (tr, va) in enumerate(kf.split(train, train[\"rank\"])):\n            x_train, x_val = get_keras_data(train.iloc[tr].drop([\"rank\", \"price\"], axis=1)), get_keras_data(train.iloc[va].drop([\"rank\", \"price\"], axis=1))\n            y_train, y_val = train.iloc[tr][\"price\"], train.iloc[va][\"price\"]\n\n            x_test = get_keras_data(test)\n\n            model = build_model()\n#             run = setup_db(fold)\n            lr, es, sv = get_callbacks(fold+1)\n\n            model.fit(x_train, \n                     y_train,\n                     validation_data=(x_val, y_val), \n                     epochs=1 if config[\"debug\"] else config[\"epoch\"], \n                     batch_size=12 if config[\"debug\"] else config[\"batch_size\"],\n                     callbacks=[lr, es, sv])\n\n            predv = model.predict(x_val).flatten()\n            predt = model.predict(x_test).flatten()\n\n            print(f\"fold: {fold+1} | rmse: {rmsle(y_val.values.ravel(), predv)}\")\n            predict_val.append(predv)\n            predict_test.append(predt)\n            val_idx.append(va)\n\n            del x_train, x_val, x_test, model \n            gc.collect()\n\n        predict_val = np.concatenate(predict_val)\n        val_idx = np.concatenate(val_idx)\n        val_idx = np.argsort(val_idx)\n        predict_val = predict_val[val_idx]\n\n        print(\"===========================================================\")\n        print(f\"CV SCORE: {rmsle(train.price.values.ravel(), predict_val)}\")\n        print(\"===========================================================\")\n\n        predict_test_mean = np.mean(predict_test, 0)\n#         predict_test_median = np.median(predict_test, 0)\n\n        if config[\"debug\"] is not True:\n            submit(predict_test_mean, \"mean\")\n#             submit(predict_test_median, \"median\")\n\n        del predict_test_mean\n        gc.collect()\n#         slack(\"Mercari RNN model Train done.\")\n        return predict_val ","10e360af":"if __name__ == \"__main__\":\n    main(train, test)","5ac64439":"# clensing NLP ","354e526b":"# Model ","4802ff87":"# Train phase ","eec1ed99":"# feature engineering. ","922a324a":"# Dataset"}}