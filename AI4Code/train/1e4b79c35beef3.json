{"cell_type":{"d0a14a79":"code","962ad3f0":"code","bf2689bb":"code","e58e8ebf":"code","6a680a39":"code","41657819":"code","848579b3":"code","3d0dd68d":"code","38c27564":"code","155d75b0":"markdown"},"source":{"d0a14a79":"\"\"\"\nCreated on Fri Oct  5 15:03:37 2018\n\n@author: alex\n\"\"\"\nfrom keras import losses, models, optimizers\nfrom keras.models import Sequential\nfrom keras.layers import (Dense, Dropout, Activation, Flatten) \nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.datasets import load_boston \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge\nfrom matplotlib import pyplot as plt\nimport numpy as np\n% matplotlib inline","962ad3f0":"boston = load_boston()\nprint(boston.DESCR)\nprint('data')\nprint(boston.data)\nprint('target')\nprint(boston.target)\n","bf2689bb":"np.shape(boston.data)[1]","e58e8ebf":"boston.data[:,1]","6a680a39":"# \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u043e\u0431'\u0454\u043a\u0442\u0430 figure\n  \nfor i in range(np.shape(boston.data)[1]):\n    # \u0421\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u043f\u0456\u0434\u0433\u0440\u0430\u0444\u0456\u043a\u0456\u0432\n    fig=plt.figure()\n    ax1=fig.add_subplot(1,1,1)\n    ax1.scatter(boston.data[:,i], boston.target)\n    #\u041d\u0430\u0437\u0432\u0430 \u0434\u0456\u0430\u0433\u0440\u0430\u043c\u0438\n    ax1.set_title(boston.feature_names[i])\n    #\u041f\u0456\u0434\u043f\u0438\u0441\u0438 \u0432\u0456\u0441\u0435\u0439\n    ax1.set_xlabel(boston.feature_names[i])\n    ax1.set_ylabel('House price')\n  ","41657819":"# \u041c\u043d\u043e\u0436\u0438\u043d\u043d\u0430 \u043b\u0456\u043d\u0456\u0439\u043d\u0430 \u0440\u0435\u0433\u0440\u0435\u0441\u0456\u044f\n\nx_mul = boston.data\ny = boston.target\nprint(type(x_mul))\n# Spliting\n\nX_train, X_test, y_train, y_test = train_test_split(x_mul, y, \n                                                    test_size=0.4, random_state=0)    \n\n","848579b3":"print(np.shape(X_train))\nprint(np.shape(X_test))\nprint(np.shape(y_train))\nprint(np.shape(y_test))","3d0dd68d":"\n\n### Deep Neural Net with Keras\n\nkernel_initializer='lecun_uniform'\nbias_initializer='zeros'\nkernel_regularizer=None\nactivation = \"tanh\"\nnb_epoch = 1000 # \u041a\u0456\u043b\u044c\u043a\u0456\u0441\u0442\u044c \u0435\u043f\u043e\u0445 \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f\nalpha_zero = 0.001 # \u041a\u043e\u0435\u0444\u0456\u0446\u0456\u0454\u043d\u0442 \u0448\u0432\u0438\u0434\u043a\u043e\u0441\u0442\u0456 \u043d\u0430\u0432\u0447\u0430\u043d\u043d\u044f\nbatch_size = 64\n\n\n\nmodel = Sequential()\n############ \u0414\u043e\u0434\u0430\u0432\u0430\u043d\u043d\u044f \u043f\u043e\u0432\u043d\u043e\u0437\u0432'\u044f\u0437\u043d\u043e\u0433\u043e \u0448\u0430\u0440\u0443 \n#model.add(Flatten())\nmodel.add(Dense(20, input_dim = 13 , activation = activation))\nmodel.add(Dense(15, activation = activation))\nmodel.add(Dense(10, activation = activation))\nmodel.add(Dense(5, activation = activation))\nmodel.add(Dense(1,kernel_initializer=kernel_initializer,\n                bias_initializer=bias_initializer, activation = activation))\nmodel.summary()\n############ \u041a\u043e\u043c\u043f\u0456\u043b\u044f\u0446\u0456\u044f \u043c\u043e\u0434\u0435\u043b\u0456\noptimizer = optimizers.Nadam(lr=alpha_zero, beta_1=0.9, beta_2=0.999, \n                             epsilon=None, schedule_decay=0.004)\nmodel.compile(loss = \"mean_squared_error\", optimizer = optimizer, \n              metrics = [\"accuracy\"])\n############ \u041d\u0430\u0432\u0447\u0430\u043d\u043d\u044f \u043c\u043e\u0434\u0435\u043b\u0456\nhistory = model.fit(X_train, y_train, batch_size = batch_size, \n                    epochs = nb_epoch, verbose=2, validation_data = (X_test, y_test))\n############ \u0412\u0438\u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044f \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u0456 \u043c\u043e\u0434\u0435\u043b\u0456\nscore = model.evaluate(X_test, y_test,verbose = 0)\n\nprint(\"test score: %f\" % score[0])\nprint(\"test accuracy: %f\" % score[1])\n############ \u041f\u043e\u0431\u0443\u0434\u043e\u0432\u0430 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0443\ny_pred = model.predict(X_test)\nr2_dnn = r2_score(y_pred, y_test) \n\nprint('r2_dnn',r2_dnn)\nprint('mse_dnn', mse_dnn)\n\n","38c27564":"# kf = KFold(n_splits=5, random_state=None, shuffle=False)\n# mse_mlp_kf = []\n# r2_mlp_kf = []  \n# for train_index, test_index in kf.split(x_mul):\n#     mlpReg.fit(x_mul[train_index],y[train_index])\n#     mse_mlp_kf.append(mean_squared_error(y[test_index], mlpReg.predict(x_mul[test_index])))\n#     r2_mlp_kf.append(r2_score(y[test_index], mlpReg.predict(x_mul[test_index])))\n# print(\"Accuracy: %0.2f (+\/- %0.2f)\" % (np.mean(mse_mlp_kf), np.std(mse_mlp_kf) * 2))\n# print(\"Mean R^2: %0.2f\" % (np.mean(r2_mlp_kf)))\n","155d75b0":"\"\"\"\nCreated on Fri Oct  5 15:03:37 2018\n\n@author: alex\n\"\"\"\nfrom keras import losses, models, optimizers\nfrom keras.models import Sequential\nfrom keras.layers import (Dense, Dropout, Activation, Flatten) \nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.datasets import load_boston \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import ElasticNet, Lasso, Ridge\nfrom matplotlib import pyplot as plt\nimport numpy as np\n% matplotlib inline\n"}}