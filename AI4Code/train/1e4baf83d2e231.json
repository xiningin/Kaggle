{"cell_type":{"9e6a8a1d":"code","67cf3b7a":"code","1e743661":"code","0ab7a293":"code","588d2e18":"markdown"},"source":{"9e6a8a1d":"!pip install kaggle-environments --upgrade -q\n\nimport numpy as np\nfrom kaggle_environments import make\nenv = make(\"mab\", debug=True)","67cf3b7a":"def best_of(n, file1, file2):\n    env = make(\"mab\", debug=True)\n    \n    wins = list()\n    for i in range(n):\n        env.run([file1, file2])\n        p1_score = env.steps[-1][0]['reward']\n        p2_score = env.steps[-1][1]['reward']\n        \n        if p1_score > p2_score:\n            wins.append(1)\n        elif p1_score < p2_score:\n            wins.append(2)\n        else:\n            pass\n        env.reset()\n        print(f\"Round {i+1}: {p1_score} - {p2_score}\")\n        \n    # print wins\n    print()\n    p1_wins = sum(np.array(wins) == 1)\n    p2_wins = sum(np.array(wins) == 2)\n    if p1_wins > p2_wins:\n        print(f'P1 wins {p1_wins} out of {n}')\n    elif p1_wins < p2_wins:\n        print(f'P2 wins {p2_wins} out of {n}')\n    else: \n        print(f'P1 and P2 ties with {p1_wins} wins out of {n}')","1e743661":"%%writefile fewa.py\n# fewa\nimport random\nimport numpy as np\n\n# seed\nrandom.seed(2020)\nnp.random.seed(2020)\n\n# global vars\ndecay = .97\nn_ag = None\nhistory = None\nlast_a_ag = None\nlast_a_op = None\nrewards = None\ntotal_reward = 0\nalpha = 0.1\ndelta0 = 1\n\n# filter\ndef filter_step(k, h, t, delta_t, rewards, sigma2=1):\n    # determine c\n    c = np.sqrt((2 * sigma2 \/ (h + 1)) * np.log(1 \/ delta_t))\n    # estimates\n    mu = np.mean(rewards[(t - h - 1):t, k], axis=0)\n    mu_max = np.max(mu)\n#     print('c', c, 'mu_max', mu_max, 'len(k)', len(k))\n    # filter\n    delta_i = mu_max - mu.reshape(-1,)\n    k_next = [i for i, di in zip(k, delta_i) if di <= 2 * c]\n    return k_next\n\n# agent\ndef agent(obs, conf):\n    global n_ag, last_a_ag, last_a_op, rewards, total_reward, history\n\n    # init\n    t = obs.step\n    if t == 0:\n        # init\n        n_ag = np.zeros(conf.banditCount, dtype=np.int)\n        history = np.zeros(conf.banditCount)\n        rewards = np.zeros(conf.banditCount)\n        # take action\n        action = int(obs.step)\n        # update history\n        hist_vector = 1 * (np.arange(conf.banditCount) == action)\n        history = np.vstack((history, hist_vector))\n    else:\n        # get opps last action\n        op_ix = (obs.agentIndex + 1) % len(obs.lastActions)\n        last_a_op = obs.lastActions[op_ix]\n        # update counts\n        n_ag[last_a_ag] += int(1)\n        #n_ag[last_a_op] += int(1)\n        #print(n_ag)\n        # update history\n        hist_vector = 1 * (np.arange(conf.banditCount) == last_a_op)\n        history = np.vstack((history, hist_vector))\n        # reward\n        r = (obs.reward - total_reward)\n        r_vector =  r * (np.arange(conf.banditCount) == last_a_op)\n        rewards = np.vstack((rewards, r_vector))\n        total_reward = obs.reward\n\n        # warmup\n        if t < conf.banditCount:\n            # take action\n            action = int(obs.step)\n        else:\n            # FEWA algorithm\n            # update delta\n            delta_t = delta0 \/ ((t+1) ** alpha)\n            # init\n            h = int(0)\n            k = list(range(conf.banditCount))\n            it = None\n            # loop\n            while it is None:\n                # filter\n                k_next = filter_step(k, h, t, delta_t, rewards)\n                k = k_next\n                # increment\n                h += int(1)\n                # there exists any bandit that number of selected times is h?\n                if any(n_ag[k] == h):\n                    if (n_ag[k] == h).sum() > 1:\n                        # breaks tie randomly\n                        it = int(np.random.choice(np.array(k)[n_ag[k] == h]))\n                    else:\n                        ix = np.argmin(n_ag[k])\n                        it = int(k[ix])\n#             print('potential bandits', len(k))\n#             print('action = ', it)\n            action = it\n    # update last action\n    last_a_ag = action\n    return action","0ab7a293":"best_of(5, \"..\/input\/santa-2020\/submission.py\", \"fewa.py\")","588d2e18":"Implementation of the FEWA (Filtering on expanding window average) algorithm described in [Seznec *et. al*, 2019](https:\/\/hal.inria.fr\/hal-01936894v2\/document).\n"}}