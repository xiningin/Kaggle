{"cell_type":{"62f8820f":"code","cb6b8f5d":"code","b6c8c725":"code","6f0cf0d1":"code","a1946eea":"code","74b8c580":"code","0a9f18bb":"code","dff89502":"code","2fdd9e38":"code","eab3b218":"code","c69ef451":"code","123aa8b6":"code","6b1a2420":"code","3a009de9":"code","5d4cb769":"code","65cd35c9":"code","66115250":"code","e584710a":"code","0f5abacf":"code","ae3a54cb":"code","a279bb03":"code","974ab344":"code","e8c4e4fb":"code","87730b6f":"code","8d4aa657":"code","90304edb":"code","e01eb229":"code","92c03c5a":"code","c17c7cc4":"code","38c0c86e":"code","4a1d017e":"code","026399d9":"code","b7a27a69":"code","4080e043":"code","f6ba0671":"code","5942f984":"code","919308e4":"code","24899cc5":"code","e8e2b105":"code","1e2bb196":"code","1f6e0874":"code","3bffae02":"code","c67d1778":"code","ae740f3d":"code","fee0e6c0":"markdown","80c66d1d":"markdown","8c90e40d":"markdown","bbadb7cd":"markdown","97b2a31c":"markdown","18cb21e1":"markdown","8c3a5010":"markdown","6ecb3ce0":"markdown","975510a6":"markdown","17bb3927":"markdown","ee7ed8e0":"markdown","450c2a7c":"markdown","848de615":"markdown","ecd5b049":"markdown","8056a835":"markdown","38ba13fe":"markdown","1b55612d":"markdown","a79d2bfb":"markdown","ea830932":"markdown","04db03fe":"markdown","48ad1337":"markdown","6f9e93c1":"markdown","005b581c":"markdown","ab86190e":"markdown","e30abaec":"markdown","d8cfa0f0":"markdown","17b5b992":"markdown","278236cc":"markdown","abd63ca4":"markdown","4594d9cb":"markdown","d20bd839":"markdown","2341c90d":"markdown","1c261442":"markdown","3b6aa56f":"markdown"},"source":{"62f8820f":"import numpy as np\nfrom scipy import sparse\nimport pandas as pd\nimport xgboost as xgb\nimport re\nimport string\nimport time\nimport seaborn as sns\nimport itertools\n\nfrom sklearn import preprocessing, pipeline, metrics, model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nimport lightgbm as lgb\n\nimport matplotlib.pyplot as plt\nimport pickle\n\n%matplotlib inline \n","cb6b8f5d":"!pip install -U category_encoders","b6c8c725":"train_data = pd.read_json('..\/input\/two-sigma-connect-rental-listing-inquiries\/train.json.zip', convert_dates=['created'])\ntest_data = pd.read_json('..\/input\/two-sigma-connect-rental-listing-inquiries\/test.json.zip', convert_dates=['created'])","6f0cf0d1":"train_size = train_data.shape[0]","a1946eea":"train_data['target'] = train_data['interest_level'].apply(lambda x: 0 if x=='low' else 1 if x=='medium' else 2)\ntrain_data['low'] = train_data['interest_level'].apply(lambda x: 1 if x=='low' else 0)\ntrain_data['medium'] = train_data['interest_level'].apply(lambda x: 1 if x=='medium' else 0)\ntrain_data['high'] = train_data['interest_level'].apply(lambda x: 1 if x=='high' else 0)","74b8c580":"full_data=pd.concat([train_data,test_data])","0a9f18bb":"num_vars = ['bathrooms','bedrooms','latitude','longitude','price']\ncat_vars = ['building_id','manager_id','display_address','street_address']\ntext_vars = ['description','features']\ndate_var = 'created'\nimage_var = 'photos'\nid_var = 'listing_id'","dff89502":"full_data['created_datetime'] = pd.to_datetime(full_data['created'], format=\"%Y-%m-%d %H:%M:%S\")\nfull_data['created_year']=full_data['created_datetime'].apply(lambda x:x.year) ## low variant\nfull_data['created_datetime'] = pd.to_datetime(full_data['created'], format=\"%Y-%m-%d %H:%M:%S\")\nfull_data['created_month']=full_data['created_datetime'].apply(lambda x:x.month)\nfull_data['created_day']=full_data['created_datetime'].apply(lambda x:x.day)\nfull_data['created_dayofweek']=full_data['created_datetime'].apply(lambda x:x.dayofweek)\nfull_data['created_dayofyear']=full_data['created_datetime'].apply(lambda x:x.dayofyear)\nfull_data['created_weekofyear']=full_data['created_datetime'].apply(lambda x:x.weekofyear)\nfull_data['created_hour']=full_data['created_datetime'].apply(lambda x:x.hour)\nfull_data['created_epoch']=full_data['created_datetime'].apply(lambda x:x.value\/\/10**9)\n\ndate_num_vars = ['created_month','created_dayofweek','created_dayofyear'\n                 ,'created_weekofyear','created_hour','created_epoch']","2fdd9e38":"full_data[\"geo_area_50\"] = \\\n    full_data[['latitude', 'longitude']]\\\n    .apply(lambda x:(int(x[0]*50)%50)*50+(int(-x[1]*50)%50),axis=1)                                         \n                         \n\nfull_data[\"geo_area_100\"] = \\\n    full_data[['latitude', 'longitude']]\\\n    .apply(lambda x:(int(x[0]*100)%100)*100+(int(-x[1]*100)%100),axis=1)                                         \n  \n\nfull_data[\"geo_area_200\"] = \\\n    full_data[['latitude', 'longitude']]\\\n    .apply(lambda x:(int(x[0]*200)%200)*200+(int(-x[1]*200)%200),axis=1)                                         \n\nimport math\n\n# Financial district\nlat=40.705628\nlon=-74.010278\nfull_data['distance_to_fi'] = full_data[['latitude', 'longitude']].apply(lambda x:math.sqrt((x[0]-lat)**2+(x[1]-lon)**2), axis=1)\n\n# Central park\nlat = 40.785091\nlon = -73.968285\nfull_data['distance_to_cp'] = full_data[['latitude', 'longitude']].apply(lambda x:math.sqrt((x[0]-lat)**2+(x[1]-lon)**2), axis=1)\n\n\ngeo_cat_vars = ['geo_area_50', 'geo_area_100', 'geo_area_200']\n\ngeo_num_vars = ['distance_to_fi', 'distance_to_cp']","eab3b218":"full_data['rooms'] = full_data['bedrooms'] + full_data['bathrooms'] \nfull_data['num_of_photos'] = full_data['photos'].apply(lambda x:len(x))\nfull_data['num_of_features'] = full_data['features'].apply(lambda x:len(x))\nfull_data['len_of_desc'] = full_data['description'].apply(lambda x:len(x))\nfull_data['words_of_desc'] = full_data['description'].apply(lambda x:len(re.sub('['+string.punctuation+']', '', x).split()))\n\n\nfull_data['nums_of_desc'] = full_data['description']\\\n        .apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n        .apply(lambda x: len([s for s in x if s.isdigit()]))\n        \nfull_data['has_phone'] = full_data['description'].apply(lambda x:re.sub('['+string.punctuation+']', '', x).split())\\\n        .apply(lambda x: [s for s in x if s.isdigit()])\\\n        .apply(lambda x: len([s for s in x if len(str(s))==10]))\\\n        .apply(lambda x: 1 if x>0 else 0)\nfull_data['has_email'] = full_data['description'].apply(lambda x: 1 if '@renthop.com' in x else 0)\n\nfull_data['building_id_is_zero'] = full_data['building_id'].apply(lambda x:1 if x=='0' else 0)\n\nadditional_num_vars = ['rooms','num_of_photos','num_of_features','len_of_desc',\n                    'words_of_desc','has_phone','has_email','building_id_is_zero']","c69ef451":"full_data['avg_word_len'] = full_data[['len_of_desc','words_of_desc']]\\\n                                    .apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\n    \nfull_data['price_per_room'] = full_data[['price','rooms']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['price_per_bedroom'] = full_data[['price','bedrooms']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['price_per_bathroom'] = full_data[['price','bathrooms']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['price_per_feature'] = full_data[['price','num_of_features']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['price_per_photo'] = full_data[['price','num_of_photos']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['price_per_word'] = full_data[['price','words_of_desc']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['price_by_desc_len'] = full_data[['price','len_of_desc']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\n\n\nfull_data['photos_per_room'] = full_data[['num_of_photos','rooms']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['photos_per_bedroom'] = full_data[['num_of_photos','bedrooms']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['photos_per_bathroom'] = full_data[['num_of_photos','bathrooms']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\n\nfull_data['desc_len_per_room'] = full_data[['len_of_desc','rooms']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['desc_len_per_bedroom'] = full_data[['len_of_desc','bedrooms']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['desc_len_per_bathroom'] = full_data[['len_of_desc','bathrooms']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['desc_len_per_word'] = full_data[['len_of_desc','words_of_desc']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['desc_len_per_numeric'] = full_data[['len_of_desc','nums_of_desc']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\n\nfull_data['features_per_room'] = full_data[['num_of_features','rooms']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['features_per_bedroom'] = full_data[['num_of_features','bedrooms']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['features_per_bathroom'] = full_data[['num_of_features','bathrooms']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['features_per_photo'] = full_data[['num_of_features','num_of_photos']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['features_per_word'] = full_data[['num_of_features','words_of_desc']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\nfull_data['features_by_desc_len'] = full_data[['num_of_features','len_of_desc']].apply(lambda x: x[0]\/x[1] if x[1]!=0 else 0, axis=1)\n\n\ninteractive_num_vars = ['avg_word_len','price_per_room','price_per_bedroom','price_per_bathroom',\n                        'price_per_feature','price_per_photo','price_per_word','price_by_desc_len',\n                        'photos_per_room','photos_per_bedroom','photos_per_bathroom',\n                        'desc_len_per_room','desc_len_per_bedroom','desc_len_per_bathroom','desc_len_per_word',\n                        'desc_len_per_numeric','features_per_room','features_per_bedroom','features_per_bathroom',\n                        'features_per_photo','features_per_word','features_by_desc_len']","123aa8b6":"%%time\ndisplay=full_data[\"display_address\"].value_counts()\nmanager_id=full_data[\"manager_id\"].value_counts()\nbuilding_id=full_data[\"building_id\"].value_counts()\nstreet=full_data[\"street_address\"].value_counts()\nbedrooms=full_data[\"bedrooms\"].value_counts()\nbathrooms=full_data[\"bathrooms\"].value_counts()\ncreated_dayofyear=full_data[\"created_dayofyear\"].value_counts()\ncreated_weekofyear=full_data[\"created_weekofyear\"].value_counts()\n\nfull_data[\"display_count\"]=full_data[\"display_address\"].apply(lambda x:display[x])\nfull_data[\"manager_count\"]=full_data[\"manager_id\"].apply(lambda x:manager_id[x])  \nfull_data[\"building_count\"]=full_data[\"building_id\"].apply(lambda x:building_id[x])\nfull_data[\"street_count\"]=full_data[\"street_address\"].apply(lambda x:street[x])\nfull_data[\"bedrooms_count\"]=full_data[\"bedrooms\"].apply(lambda x:bedrooms[x])\nfull_data[\"bathrooms_count\"]=full_data[\"bathrooms\"].apply(lambda x:bathrooms[x])\nfull_data[\"created_dayofyear_count\"]=full_data[\"created_dayofyear\"].\\\n    apply(lambda x:created_dayofyear[x])\nfull_data[\"created_weekofyear_count\"]=full_data[\"created_weekofyear\"].\\\n    apply(lambda x:created_weekofyear[x])\n\ncount_vars = ['manager_count', 'building_count', 'street_count', 'bedrooms_count',\n       'bathrooms_count', 'created_dayofyear_count', 'created_weekofyear_count']","6b1a2420":"num_cat_vars =[]\nprice_by_manager = full_data.groupby('manager_id')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\nprice_by_manager.columns = ['manager_id','min_price_by_manager',\n                            'max_price_by_manager','median_price_by_manager','mean_price_by_manager']\nfull_data = pd.merge(full_data,price_by_manager, how='left',on='manager_id')\n\nprice_by_building = full_data.groupby('building_id')['price'].agg([np.min,np.max,np.median,np.mean]).reset_index()\nprice_by_building.columns = ['building_id','min_price_by_building',\n                            'max_price_by_building','median_price_by_building','mean_price_by_building']\nfull_data = pd.merge(full_data,price_by_building, how='left',on='building_id')\n\n\nfull_data['price_percentile_by_manager']=\\\n            full_data[['price','min_price_by_manager','max_price_by_manager']]\\\n            .apply(lambda x:(x[0]-x[1])\/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n                  axis=1)\nfull_data['price_percentile_by_building']=\\\n            full_data[['price','min_price_by_building','max_price_by_building']]\\\n            .apply(lambda x:(x[0]-x[1])\/(x[2]-x[1]) if (x[2]-x[1])!=0 else 0.5,\n                  axis=1)\n\n\nnum_cat_vars.append('price_percentile_by_manager')\nnum_cat_vars.append('price_percentile_by_building')\n\nprint (num_cat_vars)","3a009de9":"for comb in itertools.combinations(cat_vars, 2):\n    comb_var_name = comb[0] +'-'+ comb[1]\n    full_data [comb_var_name] = full_data [ comb[0]].astype(str) +'_' + full_data [ comb[1]].astype(str)\n    cat_vars.append(comb_var_name)\n\ncat_vars    ","5d4cb769":"full_data[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\ncntvec = CountVectorizer(stop_words='english', max_features=200)\nfeature_sparse =cntvec.fit_transform(full_data[\"features\"]\\\n                                     .apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x])))\n\nfeature_vars = ['feature_' + v for v in cntvec.vocabulary_]\n\ncntvec = CountVectorizer(stop_words='english', max_features=100)\ndesc_sparse = cntvec.fit_transform(full_data[\"description\"])\ndesc_vars = ['desc_' + v for v in cntvec.vocabulary_]\n\n\ncntvec = CountVectorizer(stop_words='english', max_features=10)\nst_addr_sparse = cntvec.fit_transform(full_data[\"street_address\"])\nst_addr_vars = ['desc_' + v for v in cntvec.vocabulary_]","65cd35c9":"LBL = preprocessing.LabelEncoder()\n\nLE_vars=[]\nLE_map=dict()\nfor cat_var in cat_vars:\n    print (\"Label Encoding %s\" % (cat_var))\n    LE_var=cat_var+'_le'\n    full_data[LE_var]=LBL.fit_transform(full_data[cat_var])\n    LE_vars.append(LE_var)\n    LE_map[cat_var]=LBL.classes_\n    \nprint (\"Label-encoded feaures: %s\" % (LE_vars))","66115250":"OHE = preprocessing.OneHotEncoder(sparse=True)\nstart=time.time()\nOHE.fit(full_data[LE_vars])\nOHE_sparse=OHE.transform(full_data[LE_vars])\n                                   \nprint ('One-hot-encoding finished in %f seconds' % (time.time()-start))\n\n\nOHE_vars = [var[:-3] + '_' + str(level).replace(' ','_')\\\n                for var in cat_vars for level in LE_map[var] ]\n\nprint (\"OHE_sparse size :\" ,OHE_sparse.shape)\nprint (\"One-hot encoded catgorical feature samples : %s\" % (OHE_vars[:100]))","e584710a":"full_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars +geo_num_vars+ count_vars + LE_vars\ntrain_x = sparse.hstack([full_data[full_vars], \n                         feature_sparse, \n                         desc_sparse, \n                         st_addr_sparse]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars], \n                        feature_sparse, \n                        desc_sparse, \n                        st_addr_sparse]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\nfull_vars = full_vars + feature_vars + desc_vars + st_addr_vars    \nprint (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)","0f5abacf":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nfrom itertools import product\n\nclass MeanEncoder:\n    def __init__(self, categorical_features, n_splits=5, target_type='classification', prior_weight_func=None):\n        \"\"\"\n        :param categorical_features: list of str, the name of the categorical columns to encode\n\n        :param n_splits: the number of splits used in mean encoding\n\n        :param target_type: str, 'regression' or 'classification'\n\n        :param prior_weight_func:\n        a function that takes in the number of observations, and outputs prior weight\n        when a dict is passed, the default exponential decay function will be used:\n        k: the number of observations needed for the posterior to be weighted equally as the prior\n        f: larger f --> smaller slope\n        \"\"\"\n\n        self.categorical_features = categorical_features\n        self.n_splits = n_splits\n        self.learned_stats = {}\n\n        if target_type == 'classification':\n            self.target_type = target_type\n            self.target_values = []\n        else:\n            self.target_type = 'regression'\n            self.target_values = None\n\n        if isinstance(prior_weight_func, dict):\n            self.prior_weight_func = eval('lambda x: 1 \/ (1 + np.exp((x - k) \/ f))', dict(prior_weight_func, np=np))\n        elif callable(prior_weight_func):\n            self.prior_weight_func = prior_weight_func\n        else:\n            self.prior_weight_func = lambda x: 1 \/ (1 + np.exp((x - 2) \/ 1))\n\n    @staticmethod\n    def mean_encode_subroutine(X_train, y_train, X_test, variable, target, prior_weight_func):\n        X_train = X_train[[variable]].copy()\n        X_test = X_test[[variable]].copy()\n\n        if target is not None:\n            nf_name = '{}_pred_{}'.format(variable, target)\n            X_train['pred_temp'] = (y_train == target).astype(int)  # classification\n        else:\n            nf_name = '{}_pred'.format(variable)\n            X_train['pred_temp'] = y_train  # regression\n        prior = X_train['pred_temp'].mean()\n\n        col_avg_y = X_train.groupby(by=variable, axis=0)['pred_temp'].agg({'mean': 'mean', 'beta': 'size'})\n        col_avg_y['beta'] = prior_weight_func(col_avg_y['beta'])\n        col_avg_y[nf_name] = col_avg_y['beta'] * prior + (1 - col_avg_y['beta']) * col_avg_y['mean']\n        col_avg_y.drop(['beta', 'mean'], axis=1, inplace=True)\n\n        nf_train = X_train.join(col_avg_y, on=variable)[nf_name].values\n        nf_test = X_test.join(col_avg_y, on=variable).fillna(prior, inplace=False)[nf_name].values\n\n        return nf_train, nf_test, prior, col_avg_y\n\n    def fit_transform(self, X, y):\n        \"\"\"\n        :param X: pandas DataFrame, n_samples * n_features\n        :param y: pandas Series or numpy array, n_samples\n        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n        \"\"\"\n        X_new = X.copy()\n        if self.target_type == 'classification':\n            skf = StratifiedKFold(self.n_splits)\n        else:\n            skf = KFold(self.n_splits)\n\n        if self.target_type == 'classification':\n            self.target_values = sorted(set(y))\n            self.learned_stats = {'{}_pred_{}'.format(variable, target): [] for variable, target in\n                                  product(self.categorical_features, self.target_values)}\n            for variable, target in product(self.categorical_features, self.target_values):\n                nf_name = '{}_pred_{}'.format(variable, target)\n                X_new.loc[:, nf_name] = np.nan\n                for large_ind, small_ind in skf.split(y, y):\n                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, target, self.prior_weight_func)\n                    X_new.iloc[small_ind, -1] = nf_small\n                    self.learned_stats[nf_name].append((prior, col_avg_y))\n        else:\n            self.learned_stats = {'{}_pred'.format(variable): [] for variable in self.categorical_features}\n            for variable in self.categorical_features:\n                nf_name = '{}_pred'.format(variable)\n                X_new.loc[:, nf_name] = np.nan\n                for large_ind, small_ind in skf.split(y, y):\n                    nf_large, nf_small, prior, col_avg_y = MeanEncoder.mean_encode_subroutine(\n                        X_new.iloc[large_ind], y.iloc[large_ind], X_new.iloc[small_ind], variable, None, self.prior_weight_func)\n                    X_new.iloc[small_ind, -1] = nf_small\n                    self.learned_stats[nf_name].append((prior, col_avg_y))\n        return X_new\n\n    def transform(self, X):\n        \"\"\"\n        :param X: pandas DataFrame, n_samples * n_features\n        :return X_new: the transformed pandas DataFrame containing mean-encoded categorical features\n        \"\"\"\n        X_new = X.copy()\n\n        if self.target_type == 'classification':\n            for variable, target in product(self.categorical_features, self.target_values):\n                nf_name = '{}_pred_{}'.format(variable, target)\n                X_new[nf_name] = 0\n                for prior, col_avg_y in self.learned_stats[nf_name]:\n                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n                        nf_name]\n                X_new[nf_name] \/= self.n_splits\n        else:\n            for variable in self.categorical_features:\n                nf_name = '{}_pred'.format(variable)\n                X_new[nf_name] = 0\n                for prior, col_avg_y in self.learned_stats[nf_name]:\n                    X_new[nf_name] += X_new[[variable]].join(col_avg_y, on=variable).fillna(prior, inplace=False)[\n                        nf_name]\n                X_new[nf_name] \/= self.n_splits\n\n        return X_new","ae3a54cb":"mean_encoder = MeanEncoder(categorical_features=['manager_id','building_id'])\nmean_encoded_train = mean_encoder.fit_transform(train_data, train_data['target'])\nmean_encoded_test = mean_encoder.transform(test_data)\n\nmean_coded_vars = list(set(mean_encoded_train.columns) - set(train_data.columns))\nmean_coded_vars.append('listing_id')\nfull_data = pd.merge(full_data, \n                     pd.concat([mean_encoded_train[mean_coded_vars], mean_encoded_test[mean_coded_vars]]),\n                     how='left',\n                     on='listing_id'\n                    )","a279bb03":"from category_encoders.cat_boost import CatBoostEncoder\nfrom category_encoders.wrapper import PolynomialWrapper\n\nCB_encoder = PolynomialWrapper(CatBoostEncoder())\ntrain_cb = CB_encoder.fit_transform(full_data[:train_size][cat_vars], full_data[:train_size]['target'])\ntest_cb = CB_encoder.transform(full_data[train_size:][cat_vars])\n\n# noise = 0.05\n# train_te * np.random.uniform(1-noise,1+noise, train_te.shape)\n\nCB_vars = [f'cb_{c}' for c in train_cb.columns]\n\ntrain_cb.columns = CB_vars\ntest_cb.columns = CB_vars\n\nfull_data = pd.concat([full_data, pd.concat([train_cb, test_cb])], axis=1)","974ab344":"full_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars +geo_num_vars+ count_vars + LE_vars + mean_coded_vars + CB_vars\ntrain_x = sparse.hstack([full_data[full_vars], \n                         feature_sparse, \n                         desc_sparse, \n                         st_addr_sparse]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars], \n                        feature_sparse, \n                        desc_sparse, \n                        st_addr_sparse]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\nfull_vars = full_vars + feature_vars + desc_vars + st_addr_vars    \nprint (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)","e8c4e4fb":"sns.jointplot('listing_id', 'created_epoch', full_data)","87730b6f":"min_listing_id = full_data['listing_id'].min()\nmax_listing_id = full_data['listing_id'].max()\nfull_data['norm_listing_id']=full_data['listing_id'].apply(lambda x:np.float64((x-min_listing_id+1))\/(max_listing_id-min_listing_id+1))\nlisting_vars = [ 'norm_listing_id']","8d4aa657":"full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars +geo_num_vars + count_vars \\\n     + listing_vars\nfull_cat_vars = LE_vars + mean_coded_vars + CB_vars\nfull_vars = full_num_vars + full_cat_vars\ntrain_x = sparse.hstack([full_data[full_vars], \n                         feature_sparse, \n                         desc_sparse, \n                         st_addr_sparse]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars], \n                        feature_sparse, \n                        desc_sparse, \n                        st_addr_sparse]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\nfull_vars = full_vars + feature_vars + desc_vars + st_addr_vars    \nprint (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)","90304edb":"mkt_price = full_data.groupby(['building_id', 'display_address', 'bedrooms', 'bathrooms']).price.mean().reset_index()\nmkt_price = pd.merge(full_data[['building_id', 'display_address', 'bedrooms', 'bathrooms']],\n                     mkt_price, how='left', on=['building_id', 'display_address', 'bedrooms', 'bathrooms']).price\nfull_data['mkt_price'] = mkt_price.values\nfull_data['diff_to_mkt_price'] = full_data['price'] - full_data['mkt_price']\nfull_data['ratio_to_mkt_price'] = full_data['price'] \/ full_data['mkt_price']\n\nprice_vars = ['diff_to_mkt_price', 'ratio_to_mkt_price']","e01eb229":"import hashlib\n\n# unique identifer for listings - photo links uniquely identify a listing\nfull_data['photos_str'] = full_data['photos'].astype(str)\nfull_data['listing_uid'] = full_data[['manager_id', 'building_id','photos_str']].apply(lambda x: hashlib.md5((x[0] + x[1] + x[2]).encode()).hexdigest(), axis=1 )\nfull_data['posted_times'] = full_data.groupby('listing_uid').created_datetime.rank(method='first', na_option='top',pct=True)\n\n# Using html tag may improve listing quality\nfull_data['num_of_html_tag']=full_data.description.apply(lambda x:x.count('<'))\n\n# Studies have shown that titles with excessive all caps and special characters give renters the impression \n# that the listing is fraudulent \u2013 i.e. BEAUTIFUL***APARTMENT***CHELSEA.\nfull_data['num_of_#']=full_data.description.apply(lambda x:x.count('#'))\nfull_data['num_of_!']=full_data.description.apply(lambda x:x.count('!'))\nfull_data['num_of_$']=full_data.description.apply(lambda x:x.count('$'))\nfull_data['num_of_*']=full_data.description.apply(lambda x:x.count('*'))\nfull_data['num_of_>']=full_data.description.apply(lambda x:x.count('>'))\nfull_data['num_of_puncs']=full_data['num_of_#'] + full_data['num_of_!'] + full_data['num_of_$'] + full_data['num_of_*'] + full_data['num_of_>']\nfull_data['puncs_ratio'] = full_data['num_of_puncs']\/full_data['len_of_desc']\nfull_data['upper_char_ratio'] = full_data['description'].apply(lambda x: 0 if sum([s.isalpha() for s in x])==0 else sum([s.isalpha()&s.isupper() for s in x])\/ sum([s.isalpha() for s in x]))\n\n# Accuracy of location\/ address\nfull_data['disp_is_street'] = (full_data['display_address'] == full_data['street_address'])*1\nfull_data['disp_st_addr_word_ratio'] = full_data.apply(lambda x:len(x['display_address'].split(' '))\/len(x['street_address'].split(' ')), axis=1)\n\nlisting_quality_vars = ['disp_is_street', 'num_of_html_tag','num_of_#','num_of_!','num_of_$', 'num_of_*',\n                        'posted_times', 'disp_st_addr_word_ratio','upper_char_ratio']","92c03c5a":"full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars +geo_num_vars + count_vars \\\n    + listing_vars + listing_quality_vars\nfull_cat_vars = LE_vars + mean_coded_vars + CB_vars\nfull_vars = full_num_vars + full_cat_vars\ntrain_x = sparse.hstack([full_data[full_vars], \n                         feature_sparse, \n                         desc_sparse, \n                         st_addr_sparse]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars], \n                        feature_sparse, \n                        desc_sparse, \n                        st_addr_sparse]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\nfull_vars = full_vars + feature_vars + desc_vars + st_addr_vars    \nprint (\"training data size: \", train_x.shape,\"testing data size: \", test_x.shape)","c17c7cc4":"from scipy.stats import skew, kurtosis\n\ndef p25(x):\n    return np.percentile(x, 25)\ndef p50(x):\n    return np.percentile(x, 50)\ndef p75(x):\n    return np.percentile(x, 75)\ndef nunique(x):\n    return np.size(np.unique(x))\ndef max_min(x):\n    return np.max(x)-np.min(x)\ndef p75_p25(x):\n    return np.percentile(x, 75)-np.percentile(x, 25)\n\n\n\ndef get_group_stats(df, stat_funcs, target_column, group_column, ranking=False, ranking_pct=True):\n    aggr = df.groupby(group_column)[target_column].agg([v for v in stat_funcs.values()]).reset_index()\n    aggr.columns = [group_column] + [  target_column + '_' + k + '_by_' + group_column for k in stat_funcs.keys()]\n    aggr = df[[group_column]].merge(aggr, how='left', on=group_column)\n    \n    #rank\n    if ranking:\n        aggr[target_column + '_rank_by_' + group_column] = df.groupby(group_column)[target_column].rank(method='dense', \n                                                                                                    na_option='top',\n                                                                                                    pct=ranking_pct)\n    return aggr.drop(group_column, axis=1)","38c0c86e":"stat_funcs = {\n#     'count_unique': nunique,\n    'mean': np.mean,\n    'min': np.min,\n    'max': np.max,\n    'std': np.std,\n    'p25': p25,\n    'p50': p50,\n    'p75': p75,\n    'skew': skew,\n    'kurtosis': kurtosis,\n    'max_min': max_min,\n    'p75_p25': p75_p25\n}\n\n\nmgr_aggr = pd.DataFrame()\nfor num_var in num_vars + additional_num_vars + listing_quality_vars:\n    mgr_aggr = pd.concat([mgr_aggr,\n                          get_group_stats(full_data, stat_funcs,\n                                          target_column=num_var, group_column='manager_id', ranking=False)\n                          ],\n                         axis=1\n                         )\n    \n## manager activeness\nmgr_aggr = pd.concat([mgr_aggr,\n                      get_group_stats(full_data, {'max_min': max_min, 'p75_p25': p75_p25},\n                                      target_column='created_epoch', group_column='manager_id', ranking=False)\n                      ],\n                     axis=1\n                     )\n\nmgr_aggr = pd.concat([mgr_aggr,\n                      get_group_stats(full_data, {'nunique': nunique},\n                                      target_column='created_dayofyear', group_column='manager_id', ranking=False)\n                      ],\n                     axis=1\n                     )\n\n## Buildings managed by the manager\nmgr_aggr = pd.concat([mgr_aggr,\n                      get_group_stats(full_data, {'nunique': nunique},\n                                      target_column='building_id', group_column='manager_id', ranking=False)\n                      ],\n                     axis=1\n                     )\n\n## Areas \nfor aggr_col in ['geo_area_50', 'geo_area_100', 'geo_area_200']:\n    mgr_aggr = pd.concat([mgr_aggr,\n                          get_group_stats(full_data, {'nunique': nunique},\n                                          target_column=aggr_col, group_column='manager_id', ranking=False)\n                          ],\n                         axis=1\n                         )\n\n## Price fairness    \nfor aggr_col in ['diff_to_mkt_price', 'ratio_to_mkt_price']:\n    mgr_aggr = pd.concat([mgr_aggr,\n                          get_group_stats(full_data, {'mean': np.mean},\n                                          target_column=aggr_col, group_column='manager_id', ranking=False)\n                          ],\n                         axis=1\n                         )","4a1d017e":"full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+ geo_cat_vars + count_vars \\\n    + listing_vars + listing_quality_vars\nfull_cat_vars = LE_vars + mean_coded_vars + CB_vars\nfull_vars = full_num_vars + full_cat_vars\ntrain_x = sparse.hstack([full_data[full_vars],\n                         feature_sparse,\n                         desc_sparse,\n                         st_addr_sparse,\n                         mgr_aggr]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars],\n                        feature_sparse,\n                        desc_sparse,\n                        st_addr_sparse,\n                        mgr_aggr]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\nfull_vars = full_vars + feature_vars + desc_vars + st_addr_vars\nprint(\"training data size: \", train_x.shape,\n      \"testing data size: \", test_x.shape)","026399d9":"stat_funcs = {\n#     'count_unique': nunique,\n    'mean': np.mean,\n    'min': np.min,\n    'max': np.max,\n    'std': np.std,\n    'p25': p25,\n    'p50': p50,\n    'p75': p75,\n    'skew': skew,\n    'kurtosis': kurtosis,\n    'max_min': max_min,\n    'p75_p25': p75_p25\n}\n\n\nbuilding_aggr = pd.DataFrame()\n\nbuilding_aggr = pd.concat([building_aggr,\n                      get_group_stats(full_data, stat_funcs,\n                                      target_column='price', group_column='building_id', ranking=False)\n                      ],\n                     axis=1\n                     )\n    \n\n## Buildings managed by the manager\nbuilding_aggr = pd.concat([building_aggr,\n                      get_group_stats(full_data, {'nunique': nunique},\n                                      target_column='manager_id', group_column='building_id', ranking=False)\n                      ],\n                     axis=1\n                     )","b7a27a69":"from geopy.distance import vincenty\n\nfrom sklearn.cluster import KMeans\npark_listings = full_data[full_data[['description', 'features']].apply(lambda x: 'park' in x[0] or 'park' in x[1], axis=1)][['latitude', 'longitude']]\n\npark_n_clusters = 25\nkms = KMeans(n_clusters=park_n_clusters)\nkms.fit(park_listings)\n\npark_dist_data = pd.DataFrame(kms.transform(full_data[['latitude', 'longitude']]),\n                              columns = ['dist_to_park_' + str(i) for i in range(park_n_clusters)]\n                             )","4080e043":"subway_listings = full_data[full_data[['description', 'features']].apply(lambda x: 'subway' in x[0] or 'subway' in x[1], axis=1)][['latitude', 'longitude']]\n\nsubway_n_clusters = 400\nkms = KMeans(n_clusters=subway_n_clusters)\nkms.fit(subway_listings)\n\nsubway_dist_data = pd.DataFrame(kms.transform(full_data[['latitude', 'longitude']]),\n                              columns = ['dist_to_subway_' + str(i) for i in range(subway_n_clusters)]\n                             )","f6ba0671":"import spacy ## Spacy is the de-facto NLP tool used by industry\nfrom gensim.models import FastText  \nnlp = spacy.load(\"en_core_web_sm\")\n\n## Tokenize a sentence\ndef seq_to_token(seq, nlp=nlp):\n    doc = nlp(str(seq).lower())\n    tokens = [token.text for token in doc if not ( token.is_space | token.is_stop|token.like_num)]\n    return tokens\n\n## Convert tokens to vector\ndef tokens_to_vec(tokens, model, vec_size=10):\n    if len(tokens)==0:\n        return np.zeors(vec_size)\n    else:\n        return np.array([emb_model[token] for token in tokens]).mean(axis=1)","5942f984":"## Step 1: Generate \"sentences\"\nbuilding_by_mgr = full_data.groupby('manager_id')['building_id'].apply(list)\nbuilding_by_mgr.head()","919308e4":"## Step 2: Train a fasttext model\nbuilding_model = FastText(size=10, window=3, min_count=1, workers=16)  # instantiate\nbuilding_model.build_vocab(sentences=building_by_mgr)\nbuilding_model.train(sentences=building_by_mgr.values, total_examples=len(building_by_mgr.values), epochs=5)\n\n## Take a look at the embedding for building_id 8a8b08e08888819a3e745005a8cd0408\nbuilding_model['8a8b08e08888819a3e745005a8cd0408']","24899cc5":"## Step 3: Embed building ids\nbuilding_emb = full_data['building_id'].apply(lambda x:building_model[x]).values\nbuilding_emb = np.array([e.reshape(1,-1) for e in building_emb]).reshape(-1,10)\nbuilding_emb","e8e2b105":"manager_by_building = full_data.groupby('building_id')['manager_id'].apply(list)  \nmanager_model = FastText(size=10, window=3, min_count=1, workers=16)\nmanager_model.build_vocab(sentences=manager_by_building)\nmanager_model.train(sentences=manager_by_building.values, \n                    total_examples=len(manager_by_building.values), epochs=5)\nmanager_emb = full_data['manager_id'].apply(lambda x:manager_model[x]).values\nmanager_emb = np.array([e.reshape(1,-1) for e in manager_emb]).reshape(-1,10)\nmanager_emb","1e2bb196":"manager_by_building = full_data.groupby('building_id')['manager_id'].apply(list)  \nmanager_model = FastText(size=10, window=3, min_count=1, workers=16)\nmanager_model.build_vocab(sentences=manager_by_building)\nmanager_model.train(sentences=manager_by_building.values, \n                    total_examples=len(manager_by_building.values), epochs=5)\nmanager_emb = full_data['manager_id'].apply(lambda x:manager_model[x]).values\nmanager_emb = np.array([e.reshape(1,-1) for e in manager_emb]).reshape(-1,10)\nmanager_emb","1f6e0874":"desc_vec = np.vstack((pickle.load(open('\/kaggle\/input\/text-featurizer\/train_desc_vec_lg.pkl', 'rb')),\n                     pickle.load(open('\/kaggle\/input\/text-featurizer\/test_desc_vec_lg.pkl', 'rb'))))\n","3bffae02":"image_date = pd.read_csv(\"..\/input\/twosigma-magic-feature\/listing_image_time.csv\")\n\nimage_date.columns = [\"listing_id\", \"image_time_stamp\"]\nfull_data = pd.merge(full_data, image_date, on=\"listing_id\", how=\"left\")\nmagic_vars = ['image_time_stamp']","c67d1778":"full_num_vars = num_vars + date_num_vars + additional_num_vars + interactive_num_vars+geo_num_vars+ geo_cat_vars + count_vars \\\n    + listing_vars + listing_quality_vars + magic_vars + price_vars\nfull_cat_vars = LE_vars + mean_coded_vars + CB_vars\nfull_vars = full_num_vars + full_cat_vars\ntrain_x = sparse.hstack([full_data[full_vars],\n                         feature_sparse,\n                         desc_sparse,\n                         st_addr_sparse,\n                         mgr_aggr,\n                        building_aggr,\n                        park_dist_data,\n                        subway_dist_data,\n                        manager_emb,\n                        building_emb,\n                        desc_vec]).tocsr()[:train_size]\ntrain_y = full_data['target'][:train_size].values\ntest_x = sparse.hstack([full_data[full_vars],\n                        feature_sparse,\n                        desc_sparse,\n                        st_addr_sparse,\n                        mgr_aggr,\n                       building_aggr,\n                       park_dist_data,\n                       subway_dist_data,\n                       manager_emb,\n                    building_emb,\n                       desc_vec]).tocsr()[train_size:]\ntest_y = full_data['target'][train_size:].values\n\n\n\nprint(\"training data size: \", train_x.shape,\n      \"testing data size: \", test_x.shape)","ae740f3d":"sparse.save_npz('train_x.npz', train_x)\nsparse.save_npz('test_x.npz', test_x)\n\n","fee0e6c0":"## Merge training and testing data\nSo we don't have to perform transformations twice","80c66d1d":"## Create target variables\n\nWe need to convert the raw target variable into numeric","8c90e40d":"## Numeric-categorical interactions","bbadb7cd":"### Subways","97b2a31c":"## Numeric-Numeric interactions","18cb21e1":"## Baseline","8c3a5010":"## Embed manager id","6ecb3ce0":"## Two-way categorical features interactions","975510a6":"## Categorical features - CatBoost encoding\n\nCatboost is a recently created target-based categorical encoder. It is intended to overcome target leakage problems inherent in target encoding (mean encoding).","17bb3927":"### LightGBM \n\nTypically, GBDT model converges faster with a larger learning rate (e.g 0.1) than smaller learning rate however the accuracy may not be as promising. We will be using 0.1 as the learning rate for the rest of this notebook.","ee7ed8e0":"## Categorical features - label encoding","450c2a7c":"### Listing freshness and listing quality","848de615":"## Group variables","ecd5b049":"For your reference, we can use the following snippet to fine tune the optimal number clusters\n\n```python\n\nscores = []\nfor park_n_clusters in (10, 15, 20, 25, 30):\n    kms = KMeans(n_clusters=park_n_clusters)\n    kms.fit(park_listings)\n\n    park_dist_data = pd.DataFrame(kms.transform(full_data[['latitude', 'longitude']]),\n                                  columns=['dist_to_park_' +\n                                      str(i) for i in range(park_n_clusters)]\n                                 )\n\n    full_num_vars = num_vars + date_num_vars + additional_num_vars + \\\n        interactive_num_vars + listing_vars + listing_quality_vars + magic_vars + \\\n        num_cat_vars + mean_coded_vars + distance_vars\n    full_cat_vars = LE_vars\n    full_vars = full_num_vars + full_cat_vars\n    train_x = sparse.hstack([full_data[full_vars],\n                             feature_sparse,\n                             desc_sparse,\n                             st_addr_sparse,\n                             mgr_aggr,\n                            park_dist_data]).tocsr()[:train_size]\n    train_y = full_data['target'][:train_size].values\n    test_x = sparse.hstack([full_data[full_vars],\n                            feature_sparse,\n                            desc_sparse,\n                            st_addr_sparse,\n                            mgr_aggr,\n                            park_dist_data]).tocsr()[train_size:]\n    test_y = full_data['target'][train_size:].values\n\n    full_vars = full_vars + feature_vars + desc_vars + st_addr_vars\n    print(\"training data size: \", train_x.shape,\n          \"testing data size: \", test_x.shape)\n\n    lgb_params = dict()\n    lgb_params['objective'] = 'multiclass'\n    lgb_params['num_class'] = 3\n    lgb_params['learning_rate'] = 0.05\n    lgb_params['num_leaves'] = 63\n    lgb_params['max_depth'] = 15\n    lgb_params['min_gain_to_split '] = 1\n    lgb_params['subsample'] = 0.7\n    lgb_params['colsample_bytree'] = 0.7\n    lgb_params['min_sum_hessian_in_leaf'] = 0.001\n    lgb_params['seed'] = 42\n\n    lgb_cv = lgb.cv(lgb_params,\n                    lgb.Dataset(train_x,\n                                label=train_y\n                                ),\n                    num_boost_round=100000,\n                    nfold=5,\n                    stratified=True,\n                    shuffle=True,\n                    early_stopping_rounds=50,\n                    seed=42,\n                    verbose_eval=100)\n\n    best_score = min(lgb_cv['multi_logloss-mean'])\n    best_iteration = len(lgb_cv['multi_logloss-mean'])\n    print('Best iteration: %d, best score: %f' % (best_iteration, best_score))\n    scores.append([park_n_clusters, best_score])\nscores = np.array(scores)\nbest_park_n_clusters = scores[:, 0][(np.argmin(scores[:, 1]))]\nprint('best number of clusters: %d, best score: %f' % (best_park_n_clusters, np.min(scores[:, 1])))\n\n\n```","8056a835":"## item2vec\n\nWe can use the same idea for word2vec to embed any items","38ba13fe":"## listing ID\nTheoretically ID variable is not supposed to be included in training a model. However, for some reason listing_id appears to be correlated to the created date time and therefore might be a good candidate as a feature:","1b55612d":"## Hack \"HopScore\"\n\nThough it may not be 100% correlated it turns out that Renthop uses a system called [\"**HopScore**\"](https:\/\/www.renthop.com\/agent-guide\/the-hopscore) to rank listings. According to the official instruction there are three things to consider to improve HopScore:\n\n* Listing freshness\n* Listing quality\n* Manager performance\n\nThis finding is a breakthrough when I worked on feature engineering for this competition and resulted quite a few fruitful ideas.\n","a79d2bfb":"## Price","ea830932":"## Date\/time features","04db03fe":"## Location Location Location!!!\n\nNot all listings were created equally. Location is one of the most dominant factors when seeking a place to live. When we think of location we are not only talking about the absolute location but the relative location, e.g. proximity to facilities such as school, transportations and supermarkets. Unfortunately, these information are not provided by the dataset naively but thanks to Kaggler [Farron](https:\/\/www.kaggle.com\/mmueller) who graciously shared his secret sauce which brilliantly hacked the proximity information and helped him win the second place in this competition. Here's what he did: \n> It consists of kmeans cluster of (latitude, longitude) followed by computing statistics like the ones above and cluster center distances. In order to get some proxies for PoI's in the neighborhood, I created clusters after filtering the dataset based on certain words in the descriptions. That way, I estimated coordinates for things like \"supermarket\", \"shopping\", \"subway\", \"bus\", \"health\", \"fitness\", \"park\" etc. Afterwards I created minimal distances to those locations as well as counts based on different distances cut-offs.\n","48ad1337":"## Categorical features - one hot encoding\n\nThe output is a sparse matrix","6f9e93c1":"#### small learning rate","005b581c":"## Count features","ab86190e":"### Parks\n\nThe biggest challenge for replicating Faron's great idea is to figure out the appropriate number of clusters for each category. How can we do that?","e30abaec":"## Categorical features - mean encoding","d8cfa0f0":"### Similar for building","17b5b992":"## Pretrained word embedding for description\n\nDetails of how to use pretrained word embedding for encode text features can be found from https:\/\/www.kaggle.com\/chriscc\/text-featurizer\/","278236cc":"#### Large learning rate","abd63ca4":"## Numeric features: basic engineering","4594d9cb":"## The magic feature\n\nFirstly mentioned by Grand Master Silogram\nhttps:\/\/www.kaggle.com\/c\/two-sigma-connect-rental-listing-inquiries\/discussion\/31765\n\nDiscovered and made available to public by another Grand Master KazAnova\nhttps:\/\/www.kaggle.com\/c\/two-sigma-connect-rental-listing-inquiries\/discussion\/31870\n\nIt may contain the information when the listing was actually created.","d20bd839":"## Geolocation features","2341c90d":"### Embed building id","1c261442":"## Text features\n\n* Here we are using CountVectorizer but you are encouraged to give TfidfVectorizer a try.\n\n* The parameter of max_features to be tuned\n\n* The outputs are sparse matrices which can be merged with numpy arrays using scipy.stats.sparse.hstack function\n","3b6aa56f":"### Manager performance"}}