{"cell_type":{"fbe6bf3f":"code","4c1aaa06":"code","e196a04e":"code","1c1b9a95":"code","2c40776c":"code","6eb1f6b7":"code","c22fd321":"code","726f769b":"code","46dfda69":"code","42127b25":"code","e6cde9fd":"code","c48d7205":"code","81b044f0":"code","7913e3a2":"code","0a492594":"code","160c20ce":"code","d580ffa4":"code","c52ae271":"code","2003bdf9":"code","56df9294":"code","65f609ba":"code","d4f8d6ec":"code","43155337":"code","1774189e":"code","dc832f46":"code","e106e1ba":"code","ea510655":"code","a8948424":"code","c4f081df":"code","aee85dae":"code","72cb00f6":"code","3919761d":"code","c7c107d8":"code","2b77db7b":"code","9973d40a":"code","a9445693":"code","b7ca19e4":"code","11f33eac":"code","a055b417":"code","075a1616":"code","f76fe795":"code","eedbfb35":"code","8d031066":"code","c8823720":"code","265a3900":"code","b9cd7d11":"code","12b9e4f1":"code","89c5184e":"code","314baa4b":"code","26bd3f5c":"code","cb020f7a":"code","c9ddacb1":"markdown","a3ce761c":"markdown","3959e8da":"markdown","911de2f0":"markdown","fa25171a":"markdown","c526c42f":"markdown","b4fdb840":"markdown","9f418e5c":"markdown","a9ca7549":"markdown","4f6f1e6e":"markdown","f6fb3332":"markdown","924ef223":"markdown","8013cc8a":"markdown","587ce3ba":"markdown","07db091c":"markdown","dc596a31":"markdown","ad9e20f2":"markdown","ca4c781c":"markdown","4ae79d06":"markdown","15d031d6":"markdown","0ed6b768":"markdown","9a482186":"markdown","826cb513":"markdown","f5020f79":"markdown","63741b7c":"markdown","4b2f3690":"markdown","63c01cc8":"markdown","135e1af2":"markdown","6756b522":"markdown","44146777":"markdown","5607a086":"markdown","4cd3c41b":"markdown","80a2309a":"markdown","0c36ae50":"markdown","791c25a8":"markdown","f4140687":"markdown","f56ce324":"markdown","c994c4ce":"markdown"},"source":{"fbe6bf3f":"import os\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","4c1aaa06":"!ls -lha ..\/input\/satellite-images-of-hurricane-damage","e196a04e":"input_path = '..\/input\/satellite-images-of-hurricane-damage\/'\n\ndef print_file_sizes(input_path, subset):\n    print('{}:'.format(subset))\n    print('')\n    path = input_path + subset + '\/'\n    for f in os.listdir(path):\n        if not os.path.isdir(path + f):\n            print(f.ljust(30) + str(round(os.path.getsize(path + f) \/ 1000000, 2)) + 'MB')\n        else:\n            sizes = [os.path.getsize(path+f+'\/'+x)\/1000000 for x in os.listdir(path + f)]\n            print(f.ljust(30) + str(round(sum(sizes), 2)) + 'MB' + ' ({} files)'.format(len(sizes)))\n    print('')\n    \nprint_file_sizes(input_path, 'train_another')\nprint_file_sizes(input_path, 'validation_another')\nprint_file_sizes(input_path, 'test_another')\nprint_file_sizes(input_path, 'test')","1c1b9a95":"image_df = pd.DataFrame({'path': list(Path(input_path).glob('**\/*.jp*g'))})\n\nimage_df['damage'] = image_df['path'].map(lambda x: x.parent.stem)\nimage_df['data_split'] = image_df['path'].map(lambda x: x.parent.parent.stem)\nimage_df['location'] = image_df['path'].map(lambda x: x.stem)\nimage_df['lon'] = image_df['location'].map(lambda x: float(x.split('_')[0]))\nimage_df['lat'] = image_df['location'].map(lambda x: float(x.split('_')[-1]))\nimage_df['path'] = image_df['path'].map(lambda x: str(x)) # convert the path back to a string\n\nimage_df.head()","2c40776c":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n\ns = 3\nalpha = 0.25\n\n# get the train-validation-test splits\nimage_df_train = image_df[image_df['data_split']=='train_another'].copy()\nimage_df_val = image_df[image_df['data_split']=='validation_another'].copy()\nimage_df_test = image_df[image_df['data_split']=='test_another'].copy()\n\n# sort to ensure reproducible behaviour\nimage_df_train.sort_values('lat', inplace=True)\nimage_df_val.sort_values('lat', inplace=True)\nimage_df_test.sort_values('lat', inplace=True)\nimage_df_train.reset_index(drop=True,inplace=True)\nimage_df_val.reset_index(drop=True,inplace=True)\nimage_df_test.reset_index(drop=True,inplace=True)\n\nax[0].scatter(image_df_train['lon'], image_df_train['lat'], color='C0', s=s, alpha=alpha, label='train')\nax[0].scatter(image_df_val['lon'], image_df_val['lat'], color='C1', s=s, alpha=alpha, label='validation')\n\nax[0].set_title('split', fontsize=14, fontweight='bold')\nax[0].legend()\nax[0].set_xlabel('longitude')\nax[0].set_ylabel('latitude')\n\nimage_df_dmg = image_df[image_df['damage']=='damage'].copy()\nimage_df_nodmg = image_df[image_df['damage']=='no_damage'].copy()\n\nimage_df_dmg.reset_index(drop=True,inplace=True)\nimage_df_nodmg.reset_index(drop=True,inplace=True)\n\nax[1].scatter(image_df_dmg['lon'], image_df_dmg['lat'], color='C0', s=s, alpha=alpha, label='damage')\nax[1].scatter(image_df_nodmg['lon'], image_df_nodmg['lat'], color='C1', s=s, alpha=alpha, label='no damage')\n\nax[1].set_title('label', fontsize=14, fontweight='bold')\nax[1].legend()\nax[1].set_xlabel('longitude')\nax[1].set_ylabel('latitude')\n\nplt.show(fig)","6eb1f6b7":"fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20,6), sharex=True, sharey=True)\n\ns = 20\nalpha = 0.25\n\nax[0].scatter(image_df_dmg['lon'], image_df_dmg['lat'], color='C0', s=s, alpha=alpha, label='damage')\nax[0].set_title('damage', fontsize=14, fontweight='bold')\n\nax[1].scatter(image_df_dmg['lon'], image_df_dmg['lat'], color='k', s=s, alpha=alpha, label='damage')\nax[1].scatter(image_df_nodmg['lon'], image_df_nodmg['lat'], color='k', s=s, alpha=alpha, label='no damage')\nax[1].set_title('all images', fontsize=14, fontweight='bold')\n\nax[2].scatter(image_df_nodmg['lon'], image_df_nodmg['lat'], color='C1', s=s, alpha=alpha, label='no damage')\nax[2].set_title('no damage', fontsize=14, fontweight='bold')\n\nax[0].set_ylabel('latitude')\nax[0].set_xlabel('longitude')\nax[1].set_xlabel('longitude')\nax[2].set_xlabel('longitude')\n\nax[0].set_xlim(-95.3,-95)\nax[0].set_ylim(29.7,30.2)\n\nplt.show(fig)","c22fd321":"import cv2\n\n# read it in unchanged, to make sure we aren't losing any information\nimg = cv2.imread(image_df['path'][0], cv2.IMREAD_UNCHANGED)\nnp.shape(img)","726f769b":"type(img[0,0,0])","46dfda69":"np.min(img[:,:,:])","42127b25":"np.max(img[:,:,:])","e6cde9fd":"fig, ax = plt.subplots(nrows=4, ncols=10, sharex=True, sharey=True, figsize=(20,10))\n\nax = ax.flatten()\n\nfor i in range(20):\n    img = cv2.imread(image_df_dmg['path'][i], cv2.IMREAD_UNCHANGED)\n    ax[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    ax[i].set_title('damage')\n    \nfor i in range(20,40):\n    img = cv2.imread(image_df_nodmg['path'][i], cv2.IMREAD_UNCHANGED)\n    ax[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    ax[i].set_title('no damage')\n    \nplt.show()","c48d7205":"fig, ax = plt.subplots(nrows=4, ncols=3, sharex=True, sharey=True, figsize=(15,20))\n\nax = ax.flatten()\n\nselected_dmg = np.array([0,1,2,3,4,5])*100\nselected_nodmg = np.array([0,1,2,3,4,5])*99\n\nfor i in range(6):\n    img = cv2.imread(image_df_dmg['path'][selected_dmg[i]], cv2.IMREAD_UNCHANGED)\n    ax[i].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    #ax[i].set_title('damage')\n    \nfor i in range(6):\n    img = cv2.imread(image_df_nodmg['path'][selected_nodmg[i]], cv2.IMREAD_UNCHANGED)\n    ax[i+6].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    #ax[i+6].set_title('no damage')\n    \nfig.tight_layout()\nplt.show()","81b044f0":"jpg_channels = ['blue','green','red']\njpg_channel_colors = ['b','g','r']\n\nfig, ax = plt.subplots(figsize=(15,6))\n\nfor i in range(len(jpg_channels)):\n    ax.hist(img[:,:,i].flatten(), bins=np.arange(256),\n            label=jpg_channels[i], color=jpg_channel_colors[i], alpha=0.5)\n    ax.legend()\n    \nax.set_xlim(0,255)\n    \nplt.show(fig)","7913e3a2":"import tensorflow as tf\ntf.__version__","0a492594":"# paths\ntrain_path = image_df_train['path'].copy().values\nval_path = image_df_val['path'].copy().values\ntest_path = image_df_test['path'].copy().values\n\n# labels\ntrain_labels = np.zeros(len(image_df_train), dtype=np.int8)\ntrain_labels[image_df_train['damage'].values=='damage'] = 1\n\nval_labels = np.zeros(len(image_df_val), dtype=np.int8)\nval_labels[image_df_val['damage'].values=='damage'] = 1\n\ntest_labels = np.zeros(len(image_df_test), dtype=np.int8)\ntest_labels[image_df_test['damage'].values=='damage'] = 1","160c20ce":"train_ds = tf.data.Dataset.from_tensor_slices((train_path, train_labels))\nval_ds = tf.data.Dataset.from_tensor_slices((val_path, val_labels))\ntest_ds = tf.data.Dataset.from_tensor_slices((test_path, test_labels))\n\n# note that the `numpy()` function is required to grab the actual values from the Dataset\nfor path, label in train_ds.take(5):\n    print(\"path  : \", path.numpy().decode('utf-8'))\n    print(\"label : \", label.numpy())","d580ffa4":"# this function wraps `cv2.imread` - we treat it as a 'standalone' function, and therefore can use\n# eager execution (i.e. the use of `numpy()`) to get a string of the path.\n# note that no tensorflow functions are used here\ndef cv2_imread(path, label):\n    # read in the image, getting the string of the path via eager execution\n    img = cv2.imread(path.numpy().decode('utf-8'), cv2.IMREAD_UNCHANGED)\n    # change from BGR to RGB\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img, label\n\n# this function assumes that the image has been read in, and does some transformations on it\n# note that only tensorflow functions are used here\ndef tf_cleanup(img, label):\n    # convert to Tensor\n    img = tf.convert_to_tensor(img)\n    # unclear why, but the jpeg is read in as uint16 - convert to uint8\n    img = tf.dtypes.cast(img, tf.uint8)\n    # set the shape of the Tensor\n    img.set_shape((128, 128, 3))\n    # convert to float32, scaling from uint8 (0-255) to float32 (0-1)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    # resize the image\n    img = tf.image.resize(img, [128, 128])\n    # convert the labels into a Tensor and set the shape\n    label = tf.convert_to_tensor(label)\n    label.set_shape(())\n    return img, label\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\n# map the cv2 wrapper function using `tf.py_function`\ntrain_ds = train_ds.map(lambda path, label: tuple(tf.py_function(cv2_imread, [path, label], [tf.uint16, label.dtype])),\n                        num_parallel_calls=AUTOTUNE)\nval_ds = val_ds.map(lambda path, label: tuple(tf.py_function(cv2_imread, [path, label], [tf.uint16, label.dtype])),\n                    num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.map(lambda path, label: tuple(tf.py_function(cv2_imread, [path, label], [tf.uint16, label.dtype])),\n                      num_parallel_calls=AUTOTUNE)\n\n# map the TensorFlow transformation function - no need to wrap\ntrain_ds = train_ds.map(tf_cleanup, num_parallel_calls=AUTOTUNE)\nval_ds = val_ds.map(tf_cleanup, num_parallel_calls=AUTOTUNE)\ntest_ds = test_ds.map(tf_cleanup, num_parallel_calls=AUTOTUNE)","c52ae271":"def rotate_augmentation(img, label):\n    # rotate 0, 90, 180, or 270 degrees with 25% probability for each\n    img = tf.image.rot90(img, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32, seed=1111))\n    return img, label\n\ndef flip_augmentation(img, label):\n    # flip with 50% probability for left-right and up-down\n    img = tf.image.random_flip_left_right(img, seed=2222)\n    img = tf.image.random_flip_up_down(img, seed=3333)\n    return img, label\n\n# map the augmentations, creating a new Dataset\naugmented_train_ds = train_ds.map(rotate_augmentation, num_parallel_calls=AUTOTUNE)\naugmented_train_ds = augmented_train_ds.map(flip_augmentation, num_parallel_calls=AUTOTUNE)\n\naugmented_val_ds = val_ds.map(rotate_augmentation, num_parallel_calls=AUTOTUNE)\naugmented_val_ds = augmented_val_ds.map(flip_augmentation, num_parallel_calls=AUTOTUNE)\n\n# concatenate the augmented and original datasets\ntrain_ds = train_ds.concatenate(augmented_train_ds)\nval_ds = val_ds.concatenate(augmented_val_ds)","2003bdf9":"# double the number of samples in the training and validation splits, due to our augmentation procedure\nn_train = len(train_labels)*2\nn_val = len(val_labels)*2\nn_test = len(test_labels)\n\n# shuffle over the entire dataset, seeding the shuffling for reproducible results\ntrain_ds = train_ds.shuffle(n_train, seed=208, reshuffle_each_iteration=True)\nval_ds = val_ds.shuffle(n_val, seed=208, reshuffle_each_iteration=True)\ntest_ds = test_ds.shuffle(n_test, seed=208, reshuffle_each_iteration=True)","56df9294":"n_train_check = 0\nfor element in train_ds:\n    n_train_check = n_train_check + 1\nprint(n_train_check)","65f609ba":"n_val_check = 0\nfor element in val_ds:\n    n_val_check = n_val_check + 1\nprint(n_val_check)","d4f8d6ec":"n_test_check = 0\nfor element in test_ds:\n    n_test_check = n_test_check + 1\nprint(n_test_check)","43155337":"# check that the image was read in correctly\nfor image, label in train_ds.take(1):\n    print(\"image shape : \", image.numpy().shape)\n    print(\"label       : \", label.numpy())","1774189e":"fig, ax = plt.subplots(nrows=4, ncols=10, sharex=True, sharey=True, figsize=(20,10))\n\ni = 0\n\nfor image, label in train_ds.take(10):\n    ax[0,i].imshow(image[:,:,0])\n    ax[0,i].set_title('{} - {}'.format(label.numpy(), 'R'))\n    ax[1,i].imshow(image[:,:,1])\n    ax[1,i].set_title('{} - {}'.format(label.numpy(), 'G'))\n    ax[2,i].imshow(image[:,:,2])\n    ax[2,i].set_title('{} - {}'.format(label.numpy(), 'B'))\n    ax[3,i].imshow(image)\n    ax[3,i].set_title('{} - {}'.format(label.numpy(), 'RGB'))\n    \n    i = i+1","dc832f46":"fig, ax = plt.subplots(nrows=3, ncols=3, sharex=True, sharey=True, figsize=(20,15))\n\nax[0,0].set_xlim(0,1)\n\ni = 0\n\nfor image, label in train_ds.take(3):\n    ax[i,0].hist(image[:,:,0].numpy().flatten())\n    ax[i,0].set_title('{} - {}'.format(label.numpy(), 'R'))\n    ax[i,1].hist(image[:,:,1].numpy().flatten())\n    ax[i,1].set_title('{} - {}'.format(label.numpy(), 'G'))\n    ax[i,2].hist(image[:,:,2].numpy().flatten())\n    ax[i,2].set_title('{} - {}'.format(label.numpy(), 'B'))\n    \n    i = i+1","e106e1ba":"BATCH_SIZE = 32\n\ntrain_batches_ds = train_ds.batch(BATCH_SIZE)\nval_batches_ds = val_ds.batch(BATCH_SIZE)\ntest_batches_ds = test_ds.batch(BATCH_SIZE)","ea510655":"for image_batch, label_batch in train_batches_ds.take(1):\n    print(image_batch.shape)\n    print(label_batch.numpy())","a8948424":"IMG_SHAPE = (128, 128, 3)\n\n# create the base model from the pre-trained model VGG16\n# note that, if using a Kaggle server, internet has to be turned on\npretrained_model = tf.keras.applications.vgg16.VGG16(input_shape=IMG_SHAPE,\n                                                     include_top=False,\n                                                     weights='imagenet')\n\n# freeze the convolutional base\npretrained_model.trainable = False","c4f081df":"feature_batch = pretrained_model(image_batch)\nprint(feature_batch.shape)","aee85dae":"pretrained_model.summary()","72cb00f6":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","3919761d":"# set the initializers with a seed for reproducible behaviour\nprediction_layer = tf.keras.layers.Dense(1,\n                                         kernel_initializer=tf.keras.initializers.GlorotUniform(seed=1992),\n                                         bias_initializer=tf.keras.initializers.GlorotUniform(seed=1992))\n\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","c7c107d8":"model = tf.keras.Sequential([pretrained_model,\n                             global_average_layer,\n                             prediction_layer])","2b77db7b":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","9973d40a":"model.summary()","a9445693":"initial_epochs = 15\nsteps_per_epoch = n_train\/\/BATCH_SIZE\nvalidation_steps = 20\n\nloss0, accuracy0 = model.evaluate(val_batches_ds, steps=validation_steps)","b7ca19e4":"history = model.fit(train_batches_ds,\n                    epochs=initial_epochs,\n                    validation_data=val_batches_ds,\n                    validation_steps=validation_steps)","11f33eac":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10,8), sharex=True)\n\nx_plot = np.arange(1, initial_epochs+1)\n\nax[0].plot(x_plot, acc, '+-', label='training')\nax[0].plot(x_plot, val_acc, '+-', label='validation')\nax[0].legend()\nax[0].set_ylabel('accuracy')\nax[0].set_ylim(0.5, 1)\nax[0].grid(ls='--', c='C7')\nax[0].set_title('accuracy')\n\nax[1].plot(x_plot, loss, '+-', label='training')\nax[1].plot(x_plot, val_loss, '+-', label='validation')\nax[1].legend()\nax[1].set_ylabel('cross entropy')\nax[1].set_ylim(0, 1)\nax[1].grid(ls='--', c='C7')\nax[1].set_title('loss')\nax[1].set_xlabel('epoch')\n\nplt.show()","a055b417":"# unfreeze the layers\npretrained_model.trainable = True\n\n# let's take a look to see how many layers are in the base model\nprint(\"Number of layers in the pre-trained model: \", len(pretrained_model.layers))","075a1616":"# fine-tune from this layer onwards\nfine_tune_at = 15\n\n# freeze all the layers before the `fine_tune_at` layer\nfor layer in pretrained_model.layers[:fine_tune_at]:\n  layer.trainable =  False","f76fe795":"model.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate\/75),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel.summary()","eedbfb35":"len(model.trainable_variables)","8d031066":"fine_tune_epochs = 50\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nhistory_fine = model.fit(train_batches_ds,\n                         epochs=total_epochs,\n                         initial_epoch=history.epoch[-1]+1,\n                         validation_data=val_batches_ds,\n                         validation_steps=validation_steps)","c8823720":"acc += history_fine.history['accuracy']\nval_acc += history_fine.history['val_accuracy']\n\nloss += history_fine.history['loss']\nval_loss += history_fine.history['val_loss']\n\nfig, ax = plt.subplots(nrows=2, ncols=1, figsize=(10,8), sharex=True)\n\nx_plot = np.arange(1, total_epochs+1)\n\nax[0].plot(x_plot, acc, '+-', label='training')\nax[0].plot(x_plot, val_acc, '+-', label='validation')\nax[0].legend()\nax[0].set_ylabel('accuracy')\nax[0].set_ylim(0.5, 1)\nax[0].grid(ls='--', c='C7')\nax[0].set_title('accuracy')\nax[0].axvline(initial_epochs, c='C7', ls='--')\n\nax[1].plot(x_plot, loss, '+-', label='training')\nax[1].plot(x_plot, val_loss, '+-', label='validation')\nax[1].legend()\nax[1].set_ylabel('cross entropy')\nax[1].set_ylim(0, 1)\nax[1].grid(ls='--', c='C7')\nax[1].set_title('loss')\nax[1].set_xlabel('epoch')\nax[1].axvline(initial_epochs, c='C7', ls='--')\n\nplt.show()","265a3900":"val_loss, val_accuracy = model.evaluate(val_batches_ds)","b9cd7d11":"test_loss, test_accuracy = model.evaluate(test_batches_ds)","12b9e4f1":"# extract images and labels from the batches, and store predictions\neval_labels = np.array([])\neval_predictions = np.array([])\nfirst = True\nfor images, labels in test_batches_ds.take(-1): #take all the batches\n    if first:\n        eval_images = images.numpy()\n        first = False\n    else:\n        eval_images = np.concatenate((eval_images, images.numpy()), axis=0)\n    eval_labels = np.append(eval_labels, labels.numpy())\n    eval_predictions = np.append(eval_predictions, model.predict_on_batch(images).numpy())\n    \n# convert predictions from logit to binary\neval_predictions[eval_predictions>=0] = 1\neval_predictions[eval_predictions<0] = 0\n\n# change dtype to int\neval_predictions = eval_predictions.astype(int)\neval_labels = eval_labels.astype(int)\n\n# check that we extracted the images and the labels correctly\nprint(\"eval_images      : \", eval_images.shape)\nprint(\"eval_labels      : \", eval_labels.shape)\nprint(\"eval_predictions : \", eval_predictions.shape)","89c5184e":"from sklearn.metrics import confusion_matrix\nconfusion_mtx = confusion_matrix(eval_labels, eval_predictions)\nconfusion_mtx","314baa4b":"from sklearn.metrics import f1_score\n\nTN = confusion_mtx[0,0]\nFN = confusion_mtx[1,0]\nTP = confusion_mtx[1,1]\nFP = confusion_mtx[0,1]\nF1 = f1_score(eval_labels, eval_predictions)\n\nprint('accuracy = {:.4f}'.format((TP+TN)\/np.sum(confusion_mtx)))\nprint('positive recall = {:.4f}'.format(TP\/(TP+FN)))\nprint('negative recall = {:.4f}'.format(TN\/(TN+FP)))\nprint('positive precision = {:.4f}'.format(TP\/(TP+FP)))\nprint('negative precision = {:.4f}'.format(TN\/(TN+FN)))","26bd3f5c":"b_TP = TP\/8\nb_FN = FN\/8\n\nprint('accuracy = {:.4f}'.format((b_TP+TN)\/np.sum(2000)))\nprint('positive recall = {:.4f}'.format(b_TP\/(b_TP+b_FN)))\nprint('negative recall = {:.4f}'.format(TN\/(TN+FP)))\nprint('positive precision = {:.4f}'.format(b_TP\/(b_TP+FP)))\nprint('negative precision = {:.4f}'.format(TN\/(TN+b_FN)))","cb020f7a":"FP_eval_images = eval_images[(eval_labels==0)&(eval_predictions==1)]\nFN_eval_images = eval_images[(eval_labels==1)&(eval_predictions==0)]\n\nselected_FP = np.arange(10, dtype=int)*2\nselected_FN = np.arange(10, dtype=int)*2\n\nfig, ax = plt.subplots(nrows=4, ncols=5, sharex=True, sharey=True, figsize=(20,16))\n\nax = ax.flatten()\n\nfor i in range(10):\n    ax[i].imshow(FP_eval_images[selected_FP[i]])\n    \nfor i in range(10):\n    ax[i+10].imshow(FN_eval_images[selected_FN[i]])\n    \nfig.tight_layout()\nplt.show()","c9ddacb1":"`pretrained_model` converts each 128x128x3 image to a 4x4x512 block of features. See what it does to the example batch of images:","a3ce761c":"The input directory structure:","3959e8da":"# Satellite Images of Hurricane Damage\n\nContributors: Yuem Park\n\n**Overview**\n\nThe data used in this notebook are satellite images from the Greater Houston area after Hurricane Harvey in 2017. Each image has been labelled as either \"Flooded\/Damaged\" or \"Undamaged\".\n\nThe data is available for download [here](https:\/\/www.kaggle.com\/kmader\/satellite-images-of-hurricane-damage), and the study associated with the dataset is available [here](https:\/\/arxiv.org\/abs\/1807.01688).\n\n**Motivation**\n\nAs stated in the abstract of the associated study:\n\n> After a hurricane, damage assessment is critical to emergency managers for efficient response and resource allocation. One way to gauge the damage extent is to quantify the number of flooded\/damaged buildings, which is traditionally done by ground survey. This process can be labor-intensive and time-consuming.\n\nOne way to improve the efficiency of building damage assessment is to identify flooded\/damaged buildings from satellite remote sensing data alone.\n\n**Method**\n\nIn this notebook, I train a convolutional neural network to identify flooded\/damaged buildings.\n\nIn particular, I use transfer learning with fine-tuning to achieve high accuracy while minimizing the amount of compute required to train the classifier.\n\nNote that the notebook was executed using a Kaggle GPU kernel, so if you are viewing this notebook on GitHub, the images themselves have not been downloaded into the GitHub repository. However, you can still execute this notebook by either downloading the data using the links above, or forking this notebook on Kaggle [here](https:\/\/www.kaggle.com\/yuempark\/satellite-images-of-hurricane-damage).\n\nAlso, as this was my first experience using TensorFlow and Keras, I found the following websites to be particularly helpful in getting the model up and running:\n\n* https:\/\/www.tensorflow.org\/tutorials\/load_data\/images\n* https:\/\/www.tensorflow.org\/tutorials\/images\/transfer_learning\n* https:\/\/www.tensorflow.org\/guide\/data","911de2f0":"Recompile the model using a lower training rate:","fa25171a":"Now we can evaluate the full validation and test sets:","c526c42f":"Define functions to get the actual images from the paths and do simple transformations. Then map these onto the Dataset:\n\nNote that this is where the most issues were experienced. The code here, which seems simple enough, will fail with an opaque error message even if the slightest thing is done incorrectly. Take note of the comments carefully...","b4fdb840":"Get the confusion matrix, as well as other useful metrics:","9f418e5c":"Now stack all the components using a `tf.keras.Sequential` model:","a9ca7549":"Updated training curves:","4f6f1e6e":"A histogram of values:","f6fb3332":"Expected metrics if the test set was balanced:","924ef223":"File sizes:","8013cc8a":"Some notes from the TensorFlow tutorial on transfer learning:\n\n> Now fine-tune the weights of the top layers of the pre-trained model alongside the training of the classifier you added. The training process will force the weights to be tuned from generic features maps to features associated specifically to our dataset. This should only be attempted after you have trained the top-level classifier with the pre-trained model set to non-trainable. If you add a randomly initialized classifier on top of a pre-trained model and attempt to train all layers jointly, the magnitude of the gradient updates will be too large (due to the random weights from the classifier) and your pre-trained model will forget what it has learned. Only a small number of top layers of the pre-trained model should be fine-tuned. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features which generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning.","587ce3ba":"# Training Labels\n\nAll labels for the images are encoded into the directory structure of this dataset. Just extract it into a pandas DataFrame:","07db091c":"# Transfer Learning\n\nIn general, there are two types of transfer learning in the context of deep learning:\n\n* via feature extraction\n    * treating the network as an arbitrary feature extractor\n    * essentially 'chop off' the network at our pre-specified layer (typically prior to the fully-connected layers when actual classification predictions are made), then propagate some input through this 'shortened' network, get the output array, flatten it, and use that as the feature vector for the original input in another classification algorithm\n    * the two most common machine learning models for transfer learning via feature extraction are logistic regression and linear SVM:\n        * CNN's are non-linear models capable of learning non-linear features \u2014 we are assuming that the features learned by the CNN are already robust and discriminative\n        * feature vectors tend to be very large and have high dimensionality - we therefore need a fast model that can be trained on top of the features\n        * linear models tend to be very fast to train\n* via fine-tuning\n    * removing the fully-connected layers of an existing network, placing a new set of fully-connected layers on top of the network, and then fine-tuning these weights (and optionally previous layers) to recognize the new object classes\n    * this technique is usually recommended when the training dataset is large and very similar to the original dataset that the pre-trained model was trained on\n\nIn this notebook, I will use the fine-tuning method, but an example of using the feature extraction method can be found [here](https:\/\/www.kaggle.com\/kmader\/damage-classification-with-resnet-features).","dc596a31":"Inspect a batch of data:","ad9e20f2":"Image augmentation:\n\nAugment the training and validation data by applying random flips and rotations.","ca4c781c":"# The Model\n\nLet's use VGG16 for the base of the model:","4ae79d06":"Learning curves:","15d031d6":"Zoom in on a few images:","0ed6b768":"Apply a `tf.keras.layers.Dense` layer to convert these features into a single prediction per image. You don't need an activation function here because this prediction will be treated as a logit, or a raw prediction value. Positive numbers predict class 1, negative numbers predict class 0.","9a482186":"Shuffle:\n\nNote that the buffer size for shuffling defines how random the Dataset becomes - a buffer size that's equal to the number of instances will result in a uniform shuffling over the entire Dataset, and a buffer size equal to 1 will result in no shuffling. Since the data is currently ordered by label, we need make sure we do a nice full shuffle over the entire dataset:","826cb513":"Train the model:","f5020f79":"Check to make sure everything was read in correctly:","63741b7c":"Look at the model architecture:","4b2f3690":"# Image Preprocessing\n\nWe could use `tf.keras.preprocessing` to load the images, but it has 3 downsides:\n\n1. it's slow\n2. it lacks fine-grained control\n3. it's not well integrated with the rest of TensorFlow\n\nAlternatively, we could use `tf.data.Dataset`. From the documentation:\n\n> The `tf.data` API enables you to build complex input pipelines from simple, reusable pieces. For example, the pipeline for an image model might aggregate data from files in a distributed file system, apply random perturbations to each image, and merge randomly selected images into a batch for training.\n\nThis API allows the user to only call the data (and apply transformations, etc.) when the data is actually needed, instead of keeping all the images in RAM indefinitely.\n\nI'll use the `tf.data.Dataset` API.","63c01cc8":"Note that it is unclear why the accuracy computed manually from the predictions is slightly different from that calculated using `model.evaluate()`... it might have to do with how batching is implemented in `model.evaluate()`.\n\nNote that in the study associated with this dataset, the best performance achieved was:\n\n* convolutional neural network + data augmentation + 50% dropout in the fully connected layer + Adam optimizer:\n    * validation accuracy: 98.06%\n    * test accuracy (balanced): 97.29%\n\nHave a look at the images that were misclassified:","135e1af2":"Average over the 4x4 spatial locations to convert the block of features into a single 512-element vector per image:","6756b522":"Compile the model. Since there are two classes, use a binary cross-entropy loss:","44146777":"Merge into a Dataset:","5607a086":"Get the paths and labels:","4cd3c41b":"Let's plot a few (note that OpenCV reads the image in as BGR):","80a2309a":"Continue training:","0c36ae50":"Make a quick plot of the spatial distribution of the images:","791c25a8":"# Training Images\n\nFirst, inspect the images:","f4140687":"Zoom in on a specific geographic location to see more detail:","f56ce324":"Batch the data. A batch size of 32 seems like a common starting point, and using powers of 2 is preferred when using a GPU.","c994c4ce":"Get the predictions for the test set:"}}