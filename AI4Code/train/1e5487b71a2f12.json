{"cell_type":{"d9309e2b":"code","987b213b":"code","ff904f82":"code","b29650ff":"code","f767e4a5":"code","1e9c682d":"code","bb83cf5d":"code","adb62df1":"code","4527340b":"code","b62a0f9e":"code","deb8c9b7":"code","1ce5346d":"code","a71d1b04":"code","81339b39":"code","813d69ac":"code","26b9aee4":"code","e2d220bb":"code","cc4a80f7":"code","a20be7db":"code","f9178288":"code","6c21d29d":"code","b624d552":"code","c607be35":"code","44c4d03a":"code","d88fbbcc":"code","f2e9d3a0":"code","6d9293b2":"code","036b0ee7":"code","562ab8ba":"code","ab2d0821":"code","f822b25a":"code","91a6149b":"code","d17899ce":"code","777fd67e":"code","ca2a42f8":"code","6a3e550d":"code","9ea91272":"code","1037b3c6":"code","0f969de5":"code","d799a1b5":"code","46698d66":"code","58993965":"code","17c94919":"code","6b952971":"code","cbcfd96b":"code","b87efdbe":"markdown","e87dcc1b":"markdown","f8a48cbc":"markdown","e9aca76d":"markdown","9be8a5fa":"markdown","df649da4":"markdown","25c12f1b":"markdown","129128b0":"markdown","f1d720e6":"markdown","f7545193":"markdown","e94cda95":"markdown","ba03accb":"markdown","6d67fc4a":"markdown","20b7b986":"markdown","02ab60b2":"markdown","f5748e63":"markdown","dcc2a493":"markdown","6a2c3b78":"markdown","bf603564":"markdown","d458467a":"markdown","eb8edf57":"markdown","5114d246":"markdown","8e31d513":"markdown","6cbd6c54":"markdown","b9799610":"markdown"},"source":{"d9309e2b":"# import all necessary libraries\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.svm import SVC\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV","987b213b":"#import file and reading few lines\nnumbers = pd.read_csv('..\/input\/train.csv')\nnumbers.head(10)","ff904f82":"numbers.shape","b29650ff":"#checking the datatype\nnumbers.info()","f767e4a5":"numbers.describe(percentiles = [0.05,0.10,0.25,0.50,0.75,0.90,0.99])","1e9c682d":"round(100*(numbers.isnull().sum()\/(len(numbers.index))),2).sort_values(ascending = False)","bb83cf5d":"# let us check unique entries of label column\nnp.unique(numbers['label'])","adb62df1":"numbers['label'].value_counts()","4527340b":"#visualising the column - label\nsns.countplot(numbers['label'],palette = 'icefire')","b62a0f9e":"y = pd.value_counts(numbers.values.ravel()).sort_index()\nN = len(y)\nx = range(N)\nwidth =0.9\nplt.figure(figsize=[8,8])\nplt.bar(x, y, width, color=\"blue\")\nplt.title('Pixel Value Frequency (Log Scale)')\nplt.yscale('log')\nplt.xlabel('Pixel Value (0-255)')\nplt.ylabel('Frequency')","deb8c9b7":"plt.figure(figsize=(5,5))\nsns.distplot(numbers['pixel656'])\nplt.show()","1ce5346d":"plt.figure(figsize=(5,5))\nsns.distplot(numbers['pixel684'])","a71d1b04":"sns.barplot(x='label', y='pixel683', data=numbers)","81339b39":"sns.barplot(x='label', y='pixel572', data=numbers)","813d69ac":"one = numbers.iloc[2, 1:]\none = one.values.reshape(28,28)\nplt.imshow(one)\nplt.title(\"Digit 1\")","26b9aee4":"nine = numbers.iloc[11, 1:]\nnine = nine.values.reshape(28,28)\nplt.imshow(nine)\nplt.title(\"Digit 9\")","e2d220bb":"zero = numbers.iloc[1, 1:]\nzero = zero.values.reshape(28,28)\nplt.imshow(zero)\nplt.title(\"Digit 0\")","cc4a80f7":"plt.figure(figsize=(15,15))\nsns.heatmap(data=numbers.corr(),annot=False)","a20be7db":"# average feature values\npd.set_option('display.max_rows', 999)\nround(numbers.drop('label', axis=1).mean(), 2).sort_values(ascending = False)","f9178288":"# splitting into X and y\nX = numbers.drop(\"label\", axis = 1)\ny = numbers['label']","6c21d29d":"# scaling the features\nX_scaled = scale(X)\n\n# train test split\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, train_size=0.2,test_size = 0.8, random_state = 101)","b624d552":"print('X_train shape:',X_train.shape)\nprint('y_train shape:',y_train.shape)\nprint('X_test shape:',X_test.shape)\nprint('y_test shape:',y_test.shape)","c607be35":"# linear model\nmodel_linear = SVC(kernel='linear')\nmodel_linear.fit(X_train, y_train)\n\n# predict\ny_pred = model_linear.predict(X_test)","44c4d03a":"# confusion matrix and accuracy, precision, recall\n\n# accuracy\nprint(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n\n# cm\nprint(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))","d88fbbcc":"#precision, recall and f1-score\nscores=metrics.classification_report(y_test, y_pred, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nprint(scores)","f2e9d3a0":"# non-linear model\n# using poly kernel, C=1, default value of gamma\n\n# model\nnon_linear_model_poly = SVC(kernel='poly')\n\n# fit\nnon_linear_model_poly.fit(X_train, y_train)\n\n# predict\ny_pred = non_linear_model_poly.predict(X_test)","6d9293b2":"# confusion matrix and accuracy, precision, recall\n\n# accuracy\nprint(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n\n# cm\nprint(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))","036b0ee7":"# non-linear model\n# using rbf kernel, C=1, default value of gamma\n\n# model\nnon_linear_model = SVC(kernel='rbf')\n\n# fit\nnon_linear_model.fit(X_train, y_train)\n\n# predict\ny_pred = non_linear_model.predict(X_test)","562ab8ba":"# confusion matrix and accuracy, precision, recall\n\n# accuracy\nprint(\"accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), \"\\n\")\n\n# cm\nprint(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))","ab2d0821":"#precision, recall and f1-score\nscores=metrics.classification_report(y_test, y_pred, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nprint(scores)","f822b25a":"# creating a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 101)\n\n# specify range of hyperparameters\n# Set the parameters by cross-validation\nhyper_params = [ {'gamma': [0.01, 0.001,0.0001],\n                     'C': [1, 10, 100]}]\n\n\n# specify model\nmodel = SVC(kernel=\"rbf\")\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = model, \n                        param_grid = hyper_params, \n                        scoring= 'accuracy', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True,n_jobs = -1)      \n\n# fit the model\nmodel_cv.fit(X_train, y_train)  ","91a6149b":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","d17899ce":"# converting C to numeric type for plotting on x-axis\ncv_results['param_C'] = cv_results['param_C'].astype('int')\n\n# # plotting\nplt.figure(figsize=(20,7))\n\n# subplot 1\/3\nplt.subplot(131)\ngamma_01 = cv_results[cv_results['param_gamma']==0.01]\n\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.01\")\nplt.ylim([0.50, 1.1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n# subplot 2\/3\nplt.subplot(132)\ngamma_001 = cv_results[cv_results['param_gamma']==0.001]\n\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.001\")\nplt.ylim([0.50, 1.1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n\n# subplot 3\/3\nplt.subplot(133)\ngamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.0001\")\nplt.ylim([0.50, 1.1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')","777fd67e":"# printing the optimal accuracy score and hyperparameters\nbest_score = model_cv.best_score_\nbest_hyperparams = model_cv.best_params_\n\nprint(\"The best test score is {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))","ca2a42f8":"# model with optimal hyperparameters\n\n# model\nmodel = SVC(C=10, gamma=0.001, kernel=\"rbf\")\n\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n\n# metrics\nprint(\"accuracy\", metrics.accuracy_score(y_test, y_pred), \"\\n\")\nprint(metrics.confusion_matrix(y_test, y_pred), \"\\n\")","6a3e550d":"# different class-wise accuracy - #precision, recall and f1-score\nscores=metrics.classification_report(y_test, y_pred, labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nprint(scores)","9ea91272":"# Let us visualize our final model on unseen training dataset\n\ndf = np.random.randint(1,y_pred.shape[0]+1,5)\n\nplt.figure(figsize=(16,4))\nfor i,j in enumerate(df):\n    plt.subplot(150+i+1)\n    d = X_test[j].reshape(28,28)\n    plt.title(f'Predicted Label: {y_pred[j]}')\n    plt.imshow(d)\nplt.show()","1037b3c6":"#import file and reading few lines\ntest_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head(10)","0f969de5":"test_df.shape","d799a1b5":"test_df.info()","46698d66":"# scaling the features\ntest_scaled = scale(test_df)","58993965":"#model.predict\ntest_predict = model.predict(test_scaled)","17c94919":"# Plotting the distribution of prediction\na = {'ImageId': np.arange(1,test_predict.shape[0]+1), 'Label': test_predict}\ndata_to_export = pd.DataFrame(a)\nsns.countplot(data_to_export['Label'], palette = 'icefire')","6b952971":"# Let us visualize few of predicted test numbers\n\ndf = np.random.randint(1,test_predict.shape[0]+1,5)\n\nplt.figure(figsize=(16,4))\nfor i,j in enumerate(df):\n    plt.subplot(150+i+1)\n    d = test_scaled[j].reshape(28,28)\n    plt.title(f'Predicted Label: {test_predict[j]}')\n    plt.imshow(d)\nplt.show()","cbcfd96b":"# Exporting the predicted values \ndata_to_export.to_csv(path_or_buf='submission.csv', index=False)","b87efdbe":"#### Let us use our final model on test data (test.csv)","e87dcc1b":"#### Data Import","f8a48cbc":"#### Grid Search: Hyperparameter Tuning:","e9aca76d":"There are no null values.","9be8a5fa":"As we clearly see that the non-linear rbf model gives approx. 94% accuracy. And most of the precision is above 90%.Thus, going forward, let's choose hyperparameters corresponding to non-linear rbf models.\n","df649da4":"#### Let us visualise few numbers:","25c12f1b":"`Label` vs `pixel`","129128b0":"#### Data understanding and exploration","f1d720e6":"Column - `Label`","f7545193":"1. Let us first try - `Linear` model:","e94cda95":"The accuracy dropped to 87.12%, so obviously no point in going with polynomial. Let us try 'rbf'.","ba03accb":"All the data type are of type -int64.","6d67fc4a":"#### Building and Evaluating the Final Model","20b7b986":"We see that with hyperparameter - C = 10 and gamma = 0.001, we see overall accuracy of the model is 95% and also precision for each label is above 94%.","02ab60b2":"We see that average varies between 140 to 0. It is better to scale them.","f5748e63":"Almost all nearby pixel values are correlated, which is expected as well.","dcc2a493":"3. `rbf` kernel","6a2c3b78":"#### Let us check heatmap","bf603564":"2. `Poly` kernel","d458467a":"#### Model Building:","eb8edf57":"#### Data Preparation:","5114d246":"Let us try `Non- linear` models:","8e31d513":"Let us go with the best value ({'C': 10, 'gamma': 0.001}) as suggested by the sklearn.","6cbd6c54":"Let us examine few `pixels`","b9799610":"The linear model gives approx. 91.31% accuracy. Let's look at a non-linear model with randomly chosen hyperparameters."}}