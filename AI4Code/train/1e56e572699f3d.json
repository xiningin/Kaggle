{"cell_type":{"58a9370d":"code","a07ebfa9":"code","885e2efc":"code","24782421":"code","22ccccd3":"code","471ff687":"code","c98ccb9e":"code","9cf7e0a0":"code","bda3aaa8":"code","2e5fbefa":"code","6e0edfe8":"code","28291e77":"code","865e4583":"code","08bed820":"code","a62cb87b":"code","54e8b6f8":"code","fb5a0847":"code","41dedab8":"code","12de3220":"markdown","b654d017":"markdown","9a40163e":"markdown","64326702":"markdown","d34344b2":"markdown","87ebc8bf":"markdown","a855c99f":"markdown","1cdcd495":"markdown","61c36d29":"markdown","2bae39a5":"markdown","0cde9d4f":"markdown","f6c8952c":"markdown","a13a072c":"markdown","5b58b8b5":"markdown","63da08fd":"markdown","3da59945":"markdown","863060ad":"markdown","cc95af83":"markdown","870c8abf":"markdown","b1bbfb3e":"markdown"},"source":{"58a9370d":"# Importing the necessary packages\nimport pandas as pd\nimport numpy as np\nimport keras\nfrom sklearn.preprocessing import StandardScaler","a07ebfa9":"# Reading the file\ndf = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")","885e2efc":"df.shape # Shape of \u2018df\u2019","24782421":"df.columns # Prints columns of \u2018df\u2019\n","22ccccd3":"df.describe() # Displays properties of each column","471ff687":"dataset = df.values","c98ccb9e":"X = dataset[:,0:8]\ny = dataset[:,8].astype(\"int\")","9cf7e0a0":"# Standardization\na = StandardScaler()\na.fit(X)\nX_standardized = a.transform(X)","bda3aaa8":"pd.DataFrame(X_standardized).describe()","2e5fbefa":"# Importing the necessary packages\nfrom sklearn.model_selection import GridSearchCV, KFold\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.optimizers import Adam","6e0edfe8":"def create_model():\n    model = Sequential()\n    model.add(Dense(8, input_dim=8, kernel_initializer='normal', activation=\"relu\"))\n    model.add(Dense(4, input_dim=8, kernel_initializer='normal', activation=\"relu\"))\n    model.add(Dense(1, activation=\"sigmoid\"))\n    \n    adam = Adam(lr=0.01)\n    model.compile(loss=\"binary_crossentropy\", optimizer=adam, metrics=[\"accuracy\"])\n    return model","28291e77":"# Create the model\nmodel = KerasClassifier(build_fn = create_model,verbose = 0)\n# Define the grid search parameters\nbatch_size = [10,20,40]\nepochs = [10,50,100]\n# Make a dictionary of the grid search parameters\nparam_grid = dict(batch_size = batch_size,epochs = epochs)\n# Build and fit the GridSearchCV\ngrid = GridSearchCV(estimator = model,param_grid = param_grid,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(X_standardized,y)","865e4583":"# Summarize the results\nprint(\"Best : {}, using {}\".format(grid_result.best_score_,grid_result.best_params_))\nmeans = grid_result.cv_results_[\"mean_test_score\"]\nstds = grid_result.cv_results_[\"std_test_score\"]\nparams = grid_result.cv_results_[\"params\"]\nfor mean, stdev, param in zip(means, stds, params):\n  print(\"{},{} with: {}\".format(mean, stdev, param))","08bed820":"from keras.layers import Dropout\n\n# Defining the model\n\ndef create_model(learning_rate,dropout_rate):\n    model = Sequential()\n    model.add(Dense(8,input_dim = 8,kernel_initializer = 'normal',activation = 'relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(4,input_dim = 8,kernel_initializer = 'normal',activation = 'relu'))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = learning_rate)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Define the grid search parameters\n\nlearning_rate = [0.001,0.01,0.1]\ndropout_rate = [0.0,0.1,0.2]\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(learning_rate = learning_rate,dropout_rate = dropout_rate)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(X_standardized,y)\n\n# Summarize the results\n\nprint('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{},{} with: {}'.format(mean, stdev, param))","a62cb87b":"# Defining the model\n\ndef create_model(activation_function,init):\n    model = Sequential()\n    model.add(Dense(8,input_dim = 8,kernel_initializer = init,activation = activation_function))\n    model.add(Dropout(0.1))\n    model.add(Dense(4,input_dim = 8,kernel_initializer = init,activation = activation_function))\n    model.add(Dropout(0.1))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Define the grid search parameters\n\nactivation_function = ['softmax','relu','tanh','linear']\ninit = ['uniform','normal','zero']\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(activation_function = activation_function,init = init)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(X_standardized,y)\n\n# Summarize the results\n\nprint('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{},{} with: {}'.format(mean, stdev, param))","54e8b6f8":"# Defining the model\n\ndef create_model(neuron1,neuron2):\n    model = Sequential()\n    model.add(Dense(neuron1,input_dim = 8,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Define the grid search parameters\n\nneuron1 = [4,8,16]\nneuron2 = [2,4,8]\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(neuron1 = neuron1,neuron2 = neuron2)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(X_standardized,y)\n\n# Summarize the results\n\nprint('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{},{} with: {}'.format(mean, stdev, param))","fb5a0847":"from sklearn.metrics import classification_report, accuracy_score\n\n# Defining the model\n\ndef create_model():\n    model = Sequential()\n    model.add(Dense(16,input_dim = 8,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(4,input_dim = 16,kernel_initializer = 'uniform',activation = 'tanh'))\n    model.add(Dropout(0.1))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = 0.001)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0,batch_size = 40,epochs = 10)\n\n# Fitting the model\n\nmodel.fit(X_standardized,y)\n\n# Predicting using trained model\n\ny_predict = model.predict(X_standardized)\n\n# Printing the metrics\n\nprint(accuracy_score(y,y_predict))\nprint(classification_report(y,y_predict))\n","41dedab8":"def create_model(learning_rate,dropout_rate,activation_function,init,neuron1,neuron2):\n    model = Sequential()\n    model.add(Dense(neuron1,input_dim = 8,kernel_initializer = init,activation = activation_function))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = init,activation = activation_function))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(1,activation = 'sigmoid'))\n    \n    adam = Adam(lr = learning_rate)\n    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n    return model\n\n# Create the model\n\nmodel = KerasClassifier(build_fn = create_model,verbose = 0)\n\n# Define the grid search parameters\n\nbatch_size = [10,20,40]\nepochs = [10,50,100]\nlearning_rate = [0.001,0.01,0.1]\ndropout_rate = [0.0,0.1,0.2]\nactivation_function = ['softmax','relu','tanh','linear']\ninit = ['uniform','normal','zero']\nneuron1 = [4,8,16]\nneuron2 = [2,4,8]\n\n# Make a dictionary of the grid search parameters\n\nparam_grids = dict(batch_size = batch_size,epochs = epochs,learning_rate = learning_rate,dropout_rate = dropout_rate,\n                   activation_function = activation_function,init = init,neuron1 = neuron1,neuron2 = neuron2)\n\n# Build and fit the GridSearchCV\n\ngrid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\ngrid_result = grid.fit(X_standardized,y)\n\n# Summarize the results\n\nprint('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print('{},{} with: {}'.format(mean, stdev, param))","12de3220":"The best accuracy score is 0.7591 for \u2018activation_function\u2019 = tanh and \u2018kernel_initializer\u2019 = uniform. So we choose \u2018activation_function\u2019 = tanh and \u2018kernel_initializer\u2019 = uniform while tuning other hyperparameters.","b654d017":"## Tuning of Hyperparameters:- Activation Function and Kernel Initializer\nActivation functions introduce non-linear properties to the neural network such that non-linear complex functional mappings between input and output can be established. If we do not apply the activation function, then the output would be a simple linear function of the input.\n\nThe neural network needs to start with some weights and then iteratively update them to better values. Kernel initializer decides the statistical distribution or function to be used for initializing the weights.","9a40163e":"The dataset is read into \u2018df\u2019 dataframe.","64326702":"The best accuracy score is 0.7604 for \u2018batch_size\u2019 = 40 and \u2018epochs\u2019 = 10. So we choose \u2018batch_size\u2019 = 40 and \u2018epochs\u2019 = 10 while tuning other hyperparameters.\n\n## Tuning of Hyperparameters:- Learning rate and Drop out rate\nThe learning rate plays an important role in optimization algorithm. If the learning rate is too large, the algorithm may diverge and thus can\u2019t find the local optima. If the learning rate is too small, the algorithm may take many iterations to converge which results in high computational power and time. Thus we need an optimum value of learning rate which is small enough for the algorithm to converge and large enough to fasten the converging process. The learning rate helps with \u2018Early Stopping\u2019 which is a regularization method where the training set is trained as long as test set accuracy is increasing.\n\nDrop out is a regularization method that reduces the complexity of the model and thus prevents overfitting the training data. By dropping an activation unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections. Dropout rate can take values between 0 and 1. 0 implies no activation units are knocked out and 1 implies all the activation units are knocked out.","d34344b2":"We get an accuracy of 77.6% and F1 scores of 0.84 and 0.65.\nThe hyperparameter optimization was carried out by taking 2 hyperparameters at once. We may have missed the best values. The performance can be further improved by finding the optimum values of hyperparameters all at once given by the code snippet below. Note:- This process is computationally expensive.","87ebc8bf":"Mean of all columns is around 0 and Standard deviation of all columns is around 1. The data has been standardized.\n\n## Tuning of Hyperparameters :- Batch Size and Epochs\n","a855c99f":"The best accuracy score is 0.7695 for \u2018dropout_rate\u2019 = 0.1 and \u2018learning_rate\u2019 = 0.001. So we choose \u2018dropout_rate\u2019 = 0.1 and \u2018learning_rate\u2019 = 0.001 while tuning other hyperparameters.","1cdcd495":"The best accuracy score is 0.7591 for number of neurons in first layer = 16 and number of neurons in second layer = 4.","61c36d29":"The columns are [\u2018Pregnancies\u2019, \u2018Glucose\u2019, \u2018BloodPressure\u2019, \u2018SkinThickness\u2019, \u2018Insulin\u2019, \u2018BMI\u2019, \u2018DiabetesPedigreeFunction\u2019, \u2018Age\u2019, \u2018Outcome\u2019]\n","2bae39a5":"Now let us look aat mean and standard deviation of \u2018X_standardized\u2019.\n","0cde9d4f":"All the columns have count = 768 which suggests there are no missing values. The mean of \u2018Outcome\u2019 is 0.35 which suggests there are more \u2018Outcome\u2019 = 0 than \u2018Outcome\u2019 = 1 cases in the given dataset.\nThe dataframe \u2018df\u2019 is converted into numpy array \u2018dataset\u2019","f6c8952c":"## Tuning of Hyperparameter :-Number of Neurons in activation layer\nThe complexity of the data has to be matched with the complexity of the model. The number of neurons in activation layer decides the complexity of the model. Higher the number of neurons in activation layer, higher is the degree of non-linear complex functional mappings between input and output.\n","a13a072c":"Let us understand the dataframe \u2018df\u2019.","5b58b8b5":"## Training model with optimum values of Hyperparameters\nThe model is trained using optimum values of hyperparameters found in previous section.","63da08fd":"The neural architecture and optimization algorithm are defined. The neural network consists of 1 input layer, 2 hidden layers with rectified linear unit activation function and 1 output layer with sigmoid activation function. Adams optimization is chosen as the optimization algorithm for the neural network model.","3da59945":"We run the grid search for 2 hyperparameters :- \u2018batch_size\u2019 and \u2018epochs\u2019. The cross validation technique used is K-Fold with the default value k = 3. The accuracy score is calculated.","863060ad":"The results are summarized. The best accuracy score and the best values of hyperparameters are printed.","cc95af83":"## Standardization\nIt can be observed that mean value of columns are very different. Hence the dataset is to be standardized so that no inappropriate weightage is given to any feature.","870c8abf":"The \u2018dataset\u2019 is split into input X and output y\n","b1bbfb3e":"The necessary packages are imported."}}