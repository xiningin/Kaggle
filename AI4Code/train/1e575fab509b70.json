{"cell_type":{"231a63a7":"code","48c3d661":"code","f18f23f8":"code","bb6a403b":"code","643ecf03":"code","7d8b1167":"code","139baf66":"code","fda40c37":"code","cf469bd2":"code","f9afd0cf":"code","49eafffb":"code","94ce5a94":"code","0b2c52fc":"code","9f8aca53":"code","1924835d":"code","11618110":"code","6acae26f":"code","e318089c":"code","e985eacd":"code","6c5340b7":"code","1a5550ef":"code","9bb6f2ea":"code","21dd63b1":"code","4ec95c12":"code","9e88a885":"code","6279c979":"code","fd76d04e":"code","ab791628":"code","5b4301a0":"code","1532f798":"code","f131e183":"code","16b1d94c":"code","35d5434b":"code","1a29463c":"code","2582e981":"code","58469fba":"code","9b20873c":"code","bb60bf73":"code","6d4b1e4b":"code","efec341f":"code","637262d5":"code","d7f8097f":"code","65a4b5a4":"code","0f45c498":"code","5c0229c7":"code","d1b44ed1":"code","d2a466d8":"code","5fc922cf":"code","8ab19401":"code","b505b2cd":"code","9658952f":"code","9893a3a4":"code","26251041":"code","8fd1813d":"code","b33bcb6e":"code","c020fe0e":"code","1e9f9cdd":"code","411513a4":"markdown","171ad344":"markdown","3106e472":"markdown","b76c9400":"markdown","7b645e76":"markdown","3509f72c":"markdown","1aa20e0b":"markdown","3302b860":"markdown","d80d088c":"markdown","cb019072":"markdown","20431a25":"markdown","7efa9b26":"markdown","b174b5e2":"markdown","f3e7cd9b":"markdown","76243b94":"markdown","f0951051":"markdown","e29db879":"markdown","92c0a983":"markdown","5df63c79":"markdown","6d084717":"markdown","074176e3":"markdown","15ee7591":"markdown","808c3b38":"markdown","dd85334e":"markdown","c791de0c":"markdown","b934b9d3":"markdown","0e5666a8":"markdown","cfff85d3":"markdown","3e4bf357":"markdown","e699b902":"markdown","ee25a6ef":"markdown","8e07cbe6":"markdown","bc2f1166":"markdown","3d713464":"markdown","2e9a9bab":"markdown","df471b56":"markdown","ecfeda6b":"markdown"},"source":{"231a63a7":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport renders as rs\n%matplotlib inline\nsns.set()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","48c3d661":"data = pd.read_csv(\"..\/input\/creating-customer-segments\/customers.csv\")\ndf = data.copy()","f18f23f8":"df.drop(['Channel','Region'], axis=1, inplace=True)","bb6a403b":"df.head()","643ecf03":"df.tail()","7d8b1167":"df.shape","139baf66":"df.dtypes","fda40c37":"df.isnull().sum()","cf469bd2":"stats = df.describe()\nstats","f9afd0cf":"indices = [100,200,300]","49eafffb":"samples = df.loc[indices]","94ce5a94":"samples","0b2c52fc":"percentiles = df.rank(pct=True)\npercentiles = 100*percentiles.round(decimals=3)\npercentiles = percentiles.iloc[indices]\npercentiles","9f8aca53":"sns.heatmap(percentiles)\nplt.show()","1924835d":"cormat = df.corr()\nround(cormat,2)","11618110":"sns.pairplot(df)","6acae26f":"#Write code here\nlog_data = np.log(df)","e318089c":"outliers=[]\n# For each feature find the data points with extreme high or low values\nfor feature in log_data.keys():\n    \n    # TODO: Calculate Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(log_data, 25)\n    \n    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(log_data, 75)\n    \n    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3 - Q1) * 1.5\n\n    \n    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n    out=log_data[~((log_data[feature] >= Q1 - step) & (log_data[feature] <= Q3 + step))]\n    display(out)\n    outliers=outliers+list(out.index.values)\n    \noutliers = list(set([x for x in outliers if outliers.count(x) > 1]))    \nprint (\"Outliers: {}\".format(outliers))\n\ngood_data = log_data.drop(log_data.index[outliers]).reset_index(drop = True)","e985eacd":"sns.pairplot(good_data)","6c5340b7":"from sklearn.decomposition import PCA","1a5550ef":"pca_ = PCA(random_state=42)","9bb6f2ea":"pca_.fit(good_data)","21dd63b1":"pca_results = rs.pca_results(good_data, pca_)\npca_results","4ec95c12":"cumsum_pca_results= np.cumsum(pca_results)","9e88a885":"cumsum_pca_results","6279c979":"pca = PCA(n_components = 2)","fd76d04e":"reduced_data = pca.fit(good_data)\n","ab791628":"reduced_data = pca.transform(good_data)","5b4301a0":"reduced_data = pd.DataFrame(reduced_data,columns = ['Dimension 1', 'Dimension 2'])","1532f798":"reduced_data","f131e183":"from sklearn.metrics import silhouette_score\n","16b1d94c":"from sklearn.cluster import KMeans\n","35d5434b":"s_score = []\nfor k in range(2, 6):\n    kmeans = KMeans(n_clusters=k, random_state=0).fit(reduced_data)\n    s_score.append([k, silhouette_score(reduced_data, kmeans.labels_)])","1a29463c":"plt.figure(figsize=(15,6))\nsns.set_context('poster')\nplt.plot( pd.DataFrame(s_score)[0], pd.DataFrame(s_score)[1])\nplt.xlabel('clusters')\nplt.ylabel('score')\nplt.title('Silhouette Score for K-Means') \nplt.show()","2582e981":"\nfrom sklearn.cluster import AgglomerativeClustering","58469fba":"hs_score = []\nfor k in range(2, 6):\n    hc = AgglomerativeClustering(n_clusters=k).fit(reduced_data)\n    hs_score.append([k, silhouette_score(reduced_data, hc.labels_)])","9b20873c":"plt.figure(figsize=(15,6))\nsns.set_context('poster')\nplt.plot( pd.DataFrame(hs_score)[0], pd.DataFrame(s_score)[1])\nplt.xlabel('clusters')\nplt.ylabel('score')\nplt.title('Silhouette Score for Hierarchical Clustering') \nplt.show()","bb60bf73":"kmean = KMeans(n_clusters=2, random_state=0)\nkmean.fit(reduced_data)\n","6d4b1e4b":"preds = kmean.labels_\npreds","efec341f":"a_cluster= AgglomerativeClustering(n_clusters=2).fit(reduced_data)","637262d5":"preds_agg = a_cluster.labels_\npreds_agg","d7f8097f":"true_labels = data['Channel'].drop(data['Channel'].index[outliers]).reset_index(drop = True)","65a4b5a4":"# Import necessary libraries\nfrom sklearn.metrics.cluster import adjusted_rand_score","0f45c498":"kmeans_score = adjusted_rand_score(true_labels, preds)\nprint ('The score for Kmeans is ',kmeans_score)\n","5c0229c7":"aggl_score = adjusted_rand_score(true_labels, preds_agg)\nprint ('The score for Agglomerative Clustering is ', aggl_score)","d1b44ed1":"centers = kmean.cluster_centers_\ncenters","d2a466d8":"rs.cluster_results(reduced_data, preds, centers)","5fc922cf":"df_pred = df.drop(df.index[outliers]).reset_index(drop = True)\ndf_pred['pred'] = preds","8ab19401":"df_pred","b505b2cd":"clustered_avg = df_pred.groupby('pred')[\"Fresh\", \"Milk\",\"Grocery\",\"Frozen\",\"Detergents_Paper\", \"Delicatessen\"].median().reset_index()\nclustered_avg","9658952f":"!pip install chart_studio","9893a3a4":"\nimport chart_studio\nchart_studio.tools.set_credentials_file(username='hafsaniaz', api_key='lblY8Q6DCyF0C8DK7WB4')","26251041":"import chart_studio.plotly as py\nimport plotly.graph_objs as go","8fd1813d":"\nradar_data = [\n    go.Scatterpolar(\n      r = list(clustered_avg.loc[0,[\"Fresh\", \"Milk\",\"Grocery\",\"Frozen\",\"Detergents_Paper\", \"Delicatessen\"]]),\n      theta = [\"Fresh\", \"Milk\",\"Grocery\",\"Frozen\",\"Detergents_Paper\", \"Delicatessen\"],\n      fill = None,\n      fillcolor=None,\n      name = 'Cluster 0'\n    ),\n    go.Scatterpolar(\n      r = list(clustered_avg.loc[1,[\"Fresh\", \"Milk\",\"Grocery\",\"Frozen\",\"Detergents_Paper\", \"Delicatessen\"]]),\n      theta = [\"Fresh\", \"Milk\",\"Grocery\",\"Frozen\",\"Detergents_Paper\", \"Delicatessen\"],\n      fill = None,\n      fillcolor=None,\n      name = 'Cluster 1'\n    )\n]","b33bcb6e":"\nradar_layout = go.Layout(polar = dict(radialaxis = dict(visible = True,range = [0, 20000])), showlegend = True)","c020fe0e":"\nfig = go.Figure(data=radar_data, layout=radar_layout)\npy.iplot(fig, filename = \"radar\")","1e9f9cdd":"rs.channel_results(reduced_data, outliers)","411513a4":"**Some of the outliers are being repeated in more than one category, I believe they should'nt be removed. Might get some information from it**","171ad344":"**Logic in selecting the 3 samples: Quartiles**\n- As you can previously (in the object \"stats\"), we've the data showing the first and third quartiles.\n- We can filter samples that are starkly different based on the quartiles.\n    - This way we've two establishments that belong in the first and third quartiles respectively in, for example, the Frozen category.","3106e472":"** Grocery and Detergents_Paper have a strong positive correlation, as the points are spread along a diagonal. \nMilk has a moderate positive correlation with Grocery and Detergents_Paper.\nAnd majority of the distributions are positively skewed.\n**","b76c9400":"### Implementation: Selecting Samples\nTo get a better understanding of the customers and how their data will transform through the analysis, it would be best to select a few sample data points and explore them in more detail. In the code block below, add **three** indices of your choice to the `indices` list which will represent the customers to track. It is suggested to try different sets of samples until you obtain customers that vary significantly from one another.","7b645e76":"## Data Preprocessing\nWill preprocess the data to create a better representation of customers by normalizing it by **removing skewness** and **detecting (and optionally removing) outliers**. ","3509f72c":"### Pair Plot","1aa20e0b":"## Feature Transformation\nWill use principal component analysis (PCA) to draw conclusions about the underlying structure of the wholesale customer data. Since using PCA on a dataset calculates the dimensions which best maximize variance, we will find which compound combinations of features best describe customers.","3302b860":"These samples will be separated into another dataframe for finding out the details the type of customer each of the selected respresents","d80d088c":"- Silhouette Score","cb019072":"### Implementation: Dimensionality Reduction\nIn the code block below, you will need to implement the following:\n - Assign the results of fitting PCA in two dimensions with `good_data` to `pca`.\n - Apply a PCA transformation of `good_data` using `pca.transform`, and assign the results to `reduced_data`.\n - Apply a PCA transformation of the sample log-data `log_samples` using `pca.transform`, and assign the results to `pca_samples`.","20431a25":"** \nAs I have mentioned before, one cluster shows a retail shop and the other shows a restaurant. So my pedictions before were somewhat true, only confusion was due to outliers that were repeated in each category, so there must be some other way to handle such data points as they have\/may disturb some prediction in the future. And also it was an unsupervised learning method, so based on the results I'm satisfied, but the chances of improvement still exist.\n.\n**","7efa9b26":"**Choose values till 6 clusters in both cases (hierarchial and kmeans), the best score achieved is with 2 clusters**","b174b5e2":"## Implementation Hierarchical Clustering","f3e7cd9b":"### Implementation: Feature Scaling\nIf data is not normally distributed, especially if the mean and median vary significantly (indicating a large skew), it is most [often appropriate](http:\/\/econbrowser.com\/archives\/2014\/02\/use-of-logarithms-in-economics) to apply a non-linear scaling \u2014 particularly for financial data.","76243b94":"**  \n\ncustomer 100: mostly buys Detergents_Paper and delicatessen, often buys Milk and Grocery items, but usually avoids buying Fresh and Frozen food items. He\/she seems to run a cafe or snack bars. As delicatessen items are used in sandwiches or spring rolls types of food.\n\ncustomer 200: mostly buys Detergents_Paper, Milk and Grocery items, but usually avoids buying Fresh and Delicatessen items. Maybe he runs a Grocery Store or a bakery shop.\n\ncustomer 300: Mostly buys Delicatessen and Fresh products, probably runs a pizza parlor. **","f0951051":"** \n\ncluster 0: shows that customer runs a grocery store with mostly Milk and Grocery Items, and some of Detergents_Papers and Fresh items\n\ncluster 1: shows that a customer buys more Fresh items, along with some Frozen, so its basically a pizza parlor or a restaurant** \n","e29db879":"# Creating Customer Segments\n\n### Unsupervised Learning","92c0a983":"**the cumulative sum of dimension 1 and 2 values provide us with more variance, while the other remaining dimenions don't have much variance, so the we will make 2 clusters and remove the outliers **","5df63c79":"## Best Clustering Algorithm?","6d084717":"## Visualizing the clusters","074176e3":"Will choose to use either a K-Means clustering algorithm  and hierarchical clustering to identify the various customer segments hidden in the data. You will then recover specific data points from the clusters to understand their significance by transforming them back into their original dimension and scale. ","15ee7591":"### Visualizing Underlying Distributions\n\nAt the beginning of this project, it was discussed that the `'Channel'` and `'Region'` features would be excluded from the dataset so that the customer product categories were emphasized in the analysis. By reintroducing the `'Channel'` feature to the dataset, an interesting structure emerges when considering the same PCA dimensionality reduction applied earlier to the original dataset.\n","808c3b38":"**Will be using** `adjusted rand index` **to select the best clustering algorithm by comparing each of the calculated labels with actual labels found in** `data['Channel]` . Before calculating the score, we need to make sure that the shape of true labels is consistent with the resultant labels.","dd85334e":"## Implementation of K-Means","c791de0c":"### Implementation: PCA\n\nNow that the data has been scaled to a more normal distribution and has had any necessary outliers removed, we can now apply PCA to the `good_data` to discover which dimensions about the data best maximize the variance of features involved. In addition to finding these dimensions, PCA will also report the *explained variance ratio* of each dimension \u2014 how much variance within the data is explained by that dimension alone. Note that a component (dimension) from PCA can be considered a new \"feature\" of the space, however it is a composition of the original features present in the data.\n","b934b9d3":"Pairplot is a plot which is used to give and over view of the data in a graphical grid form. The result it shows gives us a picture of variables themselves in a graphical way as well as a relationship of one variable with all the others. For more details you can [click here](https:\/\/seaborn.pydata.org\/generated\/seaborn.pairplot.html)","0e5666a8":"### Silhouette Score for Hierarchical Clustering","cfff85d3":"### Implementation: Outlier Detection\nDetecting outliers in the data is extremely important in the data preprocessing step of any analysis. The presence of outliers can often skew results which take into consideration these data points. There are many \"rules of thumb\" for what constitutes an outlier in a dataset. Here, we will use [Tukey's Method for identfying outliers](http:\/\/datapigtechnologies.com\/blog\/index.php\/highlighting-outliers-in-your-data-with-the-tukey-method\/): An *outlier step* is calculated as 1.5 times the interquartile range (IQR). A data point with a feature that is beyond an outlier step outside of the IQR for that feature is considered abnormal.\n","3e4bf357":"The selected sample values should be ranked amongst the whole of the data values to check their ranks and get a better understanding of spending of each sample\/customer in each category","e699b902":"## Data Exploration","ee25a6ef":"# Profiling","8e07cbe6":"## Implementation: Creating Clusters","bc2f1166":"## Choosing K","3d713464":"**All the categories have standard deviation value greater than their mean value, which shows that the distribution or the spread of data is larger, meaning less of the data is clustered about the mean** ","2e9a9bab":"## Getting Started\n\nIn this project, you will analyze a dataset containing data on various customers' annual spending amounts (reported in *monetary units*) of diverse product categories for internal structure. One goal of this project is to best describe the variation in the different types of customers that a wholesale distributor interacts with. Doing so would equip the distributor with insight into how to best structure their delivery service to meet the needs of each customer.\n\nThe dataset for this project can be found on the [UCI Machine Learning Repository](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Wholesale+customers). For the purposes of this project, the features `'Channel'` and `'Region'` will be excluded in the analysis \u2014 with focus instead on the six product categories recorded for customers.\n\n**Description of Categories**\n- FRESH: annual spending (m.u.) on fresh products (Continuous)\n- MILK: annual spending (m.u.) on milk products (Continuous)\n- GROCERY: annual spending (m.u.) on grocery products (Continuous)\n- FROZEN: annual spending (m.u.)on frozen products (Continuous) \n- DETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous) \n- DELICATESSEN: annual spending (m.u.) on and delicatessen products (Continuous)\n    - \"A store selling cold cuts, cheeses, and a variety of salads, as well as a selection of unusual or foreign prepared foods.\"\n","df471b56":"**Before Implementing KMeans and hierarchical clustering, choose the optimal K using the following method**","ecfeda6b":"### Silhouette Score for K-Means"}}