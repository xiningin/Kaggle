{"cell_type":{"1850aa02":"code","cc6cb8d3":"code","6c309746":"code","33ae8451":"code","d06b5fd2":"code","16112cf4":"code","a2759feb":"code","81590647":"code","8ce33247":"code","3216bf62":"code","f83a53fb":"code","0b5d6788":"code","26d988ff":"code","0be6064a":"code","b4c0f076":"markdown","3f1d400b":"markdown","6a92b5c6":"markdown","249a1961":"markdown","aa9bbcf7":"markdown","7efa21e2":"markdown","6dec9803":"markdown","5f465bce":"markdown","c574925c":"markdown","fe80006a":"markdown","1b2cd284":"markdown","a86a7ae1":"markdown","bfb951ff":"markdown","d761a49b":"markdown","6cb8a82e":"markdown","2221db9a":"markdown","fa53f674":"markdown","f8e4e2ce":"markdown","9673614d":"markdown","e5647a3e":"markdown","83ab2faa":"markdown"},"source":{"1850aa02":"import numpy as np\n\ndef predict(X, w, b):\n    \"\"\"Make a prediction.\n    \n    Args:\n        X: a features matrix.\n        w: weights (a column vector).\n        b: a bias.\n      \n    Returns:\n        vector: a prediction with the same dimensions as a target column vector (n by 1).\n    \"\"\"\n    \n    # .dot() is a matrix multiplication in Numpy.\n    # We can ommit all-ones vector because Numpy can add a scalar to a vector directly.\n    return X.dot(w) + b\n\ndef J(y_hat, y):\n    \"\"\"Calculate a cost of this solution.\n    \n    Args:\n        y_hat: a prediction vector.\n        y: a target vector.\n    \n    Returns:\n        scalar: a cost of this solution.\n    \"\"\"\n    # **2 - raise to the power of two.\n    # .mean() - calculate a mean value of vector elements.\n    return ((y_hat - y)**2).mean()\n\ndef dw(X, y_hat, y):\n    \"\"\"Calculate a partial derivative of J with respect to w.\n    \n    Args:\n        X: a features matrix.\n        y_hat: a prediction vector.\n        y: a target vector.\n      \n    Returns:\n        vector: a partial derivative of J with respect to w.\n    \"\"\"\n    # .transpose() - transpose matrix.\n    return 2 * X.transpose().dot(y_hat - y) \/ len(y)\n\ndef db(y_hat, y):\n    \"\"\"Calculate a partial derivative of J with respect to b.\n    \n    Args:\n        y_hat: a prediction vector.\n        y: a target vector.\n    \n    Returns:\n        vector: a partial derivative of J with respect to b.\n    \"\"\"\n    return 2 * (y_hat - y).mean()","cc6cb8d3":"# A features matrix.\nX = np.array([\n                 [4, 7],\n                 [1, 8],\n                 [-5, -6],\n                 [3, -1],\n                 [0, 9]\n             ])\n\n# A target column vector.\ny = np.array([\n                 [37],\n                 [24],\n                 [-34], \n                 [16],\n                 [21]\n             ])\n\n# Initialize weights and a bias with some random values.\nw = np.array([ [2], [-17] ])\nb = 6","6c309746":"y_hat = predict(X, w, b)\ny_hat","33ae8451":"dw(X, y_hat, y)","d06b5fd2":"db(y_hat, y)","16112cf4":"!pip install jax jaxlib","a2759feb":"%env JAX_PLATFORM_NAME=cpu","81590647":"import jax.numpy as np\nfrom jax import grad","8ce33247":"# A features matrix.\nX = np.array([\n                 [4., 7.],\n                 [1., 8.],\n                 [-5., -6.],\n                 [3., -1.],\n                 [0., 9.]\n             ])\n\n# A target column vector.\ny = np.array([\n                 [37.],\n                 [24.],\n                 [-34.], \n                 [16.],\n                 [21.]\n             ])\n\n# Initialize weights and a bias with some random values.\nw = np.array([ [2.], [-17.] ])\nb = 6.","3216bf62":"y_hat = predict(X, w, b)\ny_hat","f83a53fb":"def predict(X, w, b):\n    \"\"\"Make a prediction.\n    \n    Args:\n        X: a features matrix.\n        w: weights (a column vector).\n        b: a bias.\n      \n    Returns:\n        vector: a prediction with the same dimensions as a target column vector (n by 1).\n    \"\"\"\n\n    # .dot() is a matrix multiplication in Numpy.\n    # We can ommit all-ones vector because Numpy can add a scalar to a vector directly.\n    return X.dot(w) + b\n\ndef J(y_hat, y):\n    \"\"\"Calculate a cost of this solution.\n    \n    Args:\n        y_hat: a prediction vector.\n        y: a target vector.\n    \n    Returns:\n        scalar: a cost of this solution.\n    \"\"\"\n    # **2 - raise to the power of two.\n    # .mean() - calculate a mean value of vector elements.\n    return ((y_hat - y)**2).mean()","0b5d6788":"def J_combined(X, w, b, y):\n    \"\"\"Cost function combining predict() and J() functions.\n\n    Args:\n        X: a features matrix.\n        w: weights (a column vector).\n        b: a bias.\n        y: a target vector.\n\n    Returns:\n        scalar: a cost of this solution.    \n    \"\"\"\n    y_hat = predict(X, w, b)\n    return J(y_hat, y)","26d988ff":"grad(J_combined, argnums=1)(X, w, b, y)","0be6064a":"grad(J_combined, argnums=2)(X, w, b, y)","b4c0f076":"NumPy with autograd capabilities will be provided by JAX from now on.","3f1d400b":"Partial derivative of cost with respect to bias.","6a92b5c6":"We need one function with weights and a bias as arguments for autograd to work. We will combine `predict()` and `J()` functions into `J_combined()`.","249a1961":"You can see that `y_hat` is the same as before.","aa9bbcf7":"These four functions are copypasted from [\"Math of a linear regression\"](https:\/\/www.kaggle.com\/grez911\/math-of-a-linear-regression) notebook.","7efa21e2":"Install JAX.","6dec9803":"Calculate derivatives with respect to variable `w` (it's selected by `argnums` value, counting starts from zero).","5f465bce":"## Automatic derivatives","c574925c":"These two functions are the same as in \"Manual derivatives\" section above.","fe80006a":"Make prediction with our linear regression.","1b2cd284":"Yeap, exactly the same as in the manual calculation.","a86a7ae1":"It's the same as in a manual calculation. Now get derivative with respect to `b`.","bfb951ff":"I will show how to take automatic derivatives using [JAX](https:\/\/github.com\/google\/jax). As example I will use a linear regression. I will reuse all functions from my [\"Math of a linear regression\"](https:\/\/www.kaggle.com\/grez911\/math-of-a-linear-regression) notebook and compare manually calculated derivatives with automatic ones.","d761a49b":"Make prediction with our linear regression.","6cb8a82e":"Let's create an example.","2221db9a":"## Manual derivatives","fa53f674":"Set an environment variable to use CPU as JAX's backend.","f8e4e2ce":"Remember these values.","9673614d":"Run the same example. I added dots to each value because JAX can work only with float numbers.","e5647a3e":"# Tutorial: Automatic derivatives with JAX","83ab2faa":"Partial derivative of cost with respect to weights."}}