{"cell_type":{"24c6ecf7":"code","17653532":"code","6ffa9564":"code","e2343a3c":"code","e48f628d":"code","5d5d8f19":"code","4c7ff97e":"code","12895153":"code","ac416aae":"code","b6cf92ef":"code","d4dad2b8":"code","6d780f85":"code","d56eadcd":"code","ffd1639f":"code","1752e130":"code","cf0513ea":"code","3b9602dc":"code","cedadbaf":"code","5cdd2d3e":"code","1e82acf2":"code","a58673e1":"code","203d8601":"code","a5d1ec68":"code","6bdf7a82":"code","f5fec73a":"code","72972f69":"code","8c37dbed":"code","5170b2a6":"code","3155e625":"code","162b47fc":"code","7ceebbfa":"code","751f80d6":"code","d2cf5a25":"code","0d09d594":"code","0150e1f2":"code","22affa13":"code","6cdd6019":"code","824686d2":"code","1dd93f6a":"code","60dd6fb0":"code","06d29251":"code","13141511":"code","70ff3b5c":"code","60e43142":"code","3b6fb5fb":"code","5f85556f":"code","1669a35f":"code","a7a82fb4":"code","ab2765d7":"code","0198beb5":"markdown","a892a450":"markdown","02a7d67a":"markdown","e149b0b8":"markdown","adfb3cdd":"markdown","6205fcbc":"markdown","4c543357":"markdown","6f559d8c":"markdown","5e2712ff":"markdown","c3d68cc4":"markdown","566a9329":"markdown","abf764f3":"markdown","6a353a52":"markdown","7b728f81":"markdown","860850a9":"markdown","53abc4f7":"markdown","61e51542":"markdown","dd7ecb4d":"markdown","bdc28a74":"markdown","9d8eddd2":"markdown","4c09b2e2":"markdown","daffa75c":"markdown","d0f45e84":"markdown","52925c0d":"markdown","7ecc49ee":"markdown","b05e9f70":"markdown","831f2c6b":"markdown","da61f5d1":"markdown","6218f6c0":"markdown","f795b1cf":"markdown","8686919e":"markdown","dadaad06":"markdown","a9a92d0a":"markdown","49c4b1af":"markdown","4a8af401":"markdown","d7e886e1":"markdown","7dcc1c74":"markdown","8d1d535e":"markdown","c90ce7f5":"markdown","b7cef730":"markdown","80ebe652":"markdown"},"source":{"24c6ecf7":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") ","17653532":"df = pd.read_csv(\"..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")","6ffa9564":"df.shape","e2343a3c":"df.sample(10)","e48f628d":"df.describe(include='all')","5d5d8f19":"dict = {}\nfor i in list(df.columns):\n    dict[i] = df[i].value_counts().shape[0]\n\npd.DataFrame(dict,index=[\"unique count\"]).transpose()","4c7ff97e":"df.isnull().sum()","12895153":"output_pos = df[df.output == 1]\noutput_neg = df[df.output == 0]\n\noutput_pos_percentage = round(float((len(output_pos) \/ len(df) * 100)), 2)\noutput_neg_percentage = round(float((len(output_neg) \/ len(df) * 100)),2)\n\nprint(f'People who had heart attack: {len(output_pos)} ({output_pos_percentage} %) ')\nprint(f'People who had not heart attack: {len(output_neg)} ({output_neg_percentage} %)')","ac416aae":"plt.figure(figsize=(15, 5))\n\nsns.kdeplot(data=df, x=\"age\", hue=\"output\")","b6cf92ef":"df.sex.value_counts()","d4dad2b8":"pd.crosstab(df['output'], df['sex'])","6d780f85":"plt.title('Distribution of cases (heart attacks) between Sex 0 and 1')\nsns.barplot(x='sex', y='output', data=df)","d56eadcd":"pd.crosstab(df['output'], df['sex']).plot(kind='bar')","ffd1639f":"sex_output_pos_average = df[['sex', 'output']].groupby(['sex'], as_index=False).mean()\nsex_output_pos_average","1752e130":"df.cp.value_counts()","cf0513ea":"pd.crosstab(df['output'], df['cp'])","3b9602dc":"cp_output_pos_average = df[['cp', 'output']].groupby(['cp'], as_index=False).mean()\ncp_output_pos_average","cedadbaf":"sns.barplot(x='cp', y='output', ci=None, data=df)","5cdd2d3e":"fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n\nsns.kdeplot(data=df, x=\"trtbps\", hue=\"output\", ax=axs[0]).set_title('Blood presure')\nsns.kdeplot(data=df, x=\"chol\", hue=\"output\", ax=axs[1]).set_title('Cholesterol')\nfig.show()","1e82acf2":"plt.figure(figsize=(15, 5))\n\nsns.kdeplot(data=df, x=\"thalachh\", hue=\"output\")","a58673e1":"df.exng.value_counts()","203d8601":"pd.crosstab(df['output'], df['exng'])","a5d1ec68":"sns.barplot(x='exng', y='output', data=df)","6bdf7a82":"exng_output_pos_average = df[['exng', 'output']].groupby(['exng'], as_index=False).mean()\nexng_output_pos_average","f5fec73a":"plt.figure(figsize=(15, 5))\n\nsns.kdeplot(data=df, x=\"oldpeak\", hue=\"output\")","72972f69":"pd.crosstab(df['output'], df['slp'])","8c37dbed":"slp_output_pos_average = df[['slp', 'output']].groupby(['slp'], as_index=False).mean()\nslp_output_pos_average","5170b2a6":"# We can verify the statement above on this chart\n# Chances to have the stroke depending on slp\n\nslp_output = df.groupby('slp').output.value_counts()\nslp_output.unstack(level=0).plot(kind='bar', subplots=False)","3155e625":"pd.crosstab(df['output'], df['caa'])","162b47fc":"caa_output_pos_average = df[['caa', 'output']].groupby(['caa'], as_index=False).mean()\ncaa_output_pos_average","7ceebbfa":"# We can verify the statement above on this chart\n# Chances to have the stroke depending on caa\n\ncaa_output = df.groupby('caa').output.value_counts()\ncaa_output.unstack(level=0).plot(kind='bar', subplots=False)","751f80d6":"pd.crosstab(df['output'], df['thall'])","d2cf5a25":"thall_output_pos_average = df[['thall', 'output']].groupby(['thall'], as_index=False).mean()\nthall_output_pos_average","0d09d594":"# We can verify the statement above on this chart\n# Chances to have the stroke depending on thall\n\nthall_output = df.groupby('thall').output.value_counts()\nthall_output.unstack(level=0).plot(kind='bar', subplots=False)","0150e1f2":"plt.figure(figsize=(15, 5))\n\nsns.kdeplot(data=df, x=\"trtbps\", hue=\"output\")","22affa13":"fbs_output_pos_average = df[['fbs', 'output']].groupby(['fbs'], as_index=False).mean()\nfbs_output_pos_average","6cdd6019":"pd.crosstab(df['output'], df['restecg'])","824686d2":"restecg_output_pos_average = df[['restecg', 'output']].groupby(['restecg'], as_index=False).mean()\nrestecg_output_pos_average","1dd93f6a":"plt.figure(figsize=(15,12))\nsns.heatmap(df.corr(), vmax=0.6, square=True, annot=True)","60dd6fb0":"df.hist(figsize=(15,10));","06d29251":"# Train Test Split\nfrom sklearn.model_selection import train_test_split\n\n# Since the [outliers] aren't an issue, we can skip RobustScaler\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Importing Classifier Modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","13141511":"# creating a copy of df\ndf1 = df\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df1, columns = cat_cols, drop_first = True)\n\n# defining the features and target\nX = df1.drop('output', axis=1)\ny = df1['output']\n\n# instantiating the scaler\nscaler = StandardScaler()\n\n# scaling the continuous featuree\nX[con_cols] = scaler.fit_transform(X[con_cols])\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\n","70ff3b5c":"clf = LogisticRegression()\nclf.fit(X_train, y_train)\ny_pred_log_reg = clf.predict(X_test)\nacc_log_reg = round( clf.score(X_train, y_train) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_log_reg) + '%')","60e43142":"clf = SVC()\nclf.fit(X_train, y_train)\ny_pred_svc = clf.predict(X_test)\nacc_svc = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_svc) + '%')","3b6fb5fb":"clf = LinearSVC()\nclf.fit(X_train, y_train)\ny_pred_linear_svc = clf.predict(X_test)\nacc_linear_svc = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_linear_svc) + '%')","5f85556f":"clf = KNeighborsClassifier(n_neighbors = 3)\nclf.fit(X_train, y_train)\ny_pred_knn = clf.predict(X_test)\nacc_knn = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_knn) + '%')","1669a35f":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred_decision_tree = clf.predict(X_test)\nacc_decision_tree = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_decision_tree) + '%')","a7a82fb4":"clf = RandomForestClassifier(n_estimators=100)\nclf.fit(X_train, y_train)\ny_pred_random_forest = clf.predict(X_test)\nacc_random_forest = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Train Accuracy: \" + str(acc_random_forest) + '%')","ab2765d7":"models = pd.DataFrame({\n    'Model': ['LR', 'SVM', 'L-SVC', \n              'KNN', 'DTree', 'RF',],\n    \n    'Score': [acc_log_reg, acc_svc, acc_linear_svc, \n              acc_knn,  acc_decision_tree, acc_random_forest]\n    })\n\nmodels = models.sort_values(by='Score', ascending=False)\nmodels","0198beb5":"#### 6) Random Forest\n\n[Random forests](https:\/\/en.wikipedia.org\/wiki\/Random_forest) or **random decision forests** are an **ensemble learning method** for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Random decision forests correct for *decision trees' habit of overfitting to their training set*.\n\n[Ensemble methods](https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning) use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone.","a892a450":"NOTE:<br>Heatmap of Correlation between different features:\n\n>Positive numbers = Positive correlation, i.e. increase in one feature will increase the other feature & vice-versa.<br>\n>Negative numbers = Negative correlation, i.e. increase in one feature will decrease the other feature & vice-versa.\n\nIn our case, we focus on which features have strong positive or negative correlation with the *Output* feature.","02a7d67a":"#### NOTE:<br>\n- For poeple in the group **cp = 0**, chances to having heart attack are much lower. \n- The rest 3 groups are peaking around 75% in average.","e149b0b8":"#### NOTE:<br>\n- People with higher heart rate (150 - 180) are very much probable to suffer from heart attack.","adfb3cdd":"#### Below is a brief information about each columns of the dataset:\n`age` - Age of the patient\n\n`sex` - Sex of the patient\n\n`cp` - Chest pain type ~ 0 = Typical Angina, 1 = Atypical Angina, 2 = Non-anginal Pain, 3 = Asymptomatic\n\n`trtbps` - Resting blood pressure (in mm Hg)\n\n`chol` - Cholestoral in mg\/dl fetched via BMI sensor\n\n`fbs` - (fasting blood sugar > 120 mg\/dl) ~ 1 = True, 0 = False\n\n`restecg` - Resting electrocardiographic results ~ 0 = Normal, 1 = ST-T wave normality, 2 = Left ventricular hypertrophy\n\n`thalachh`  - Maximum heart rate achieved\n\n`oldpeak` - Previous peak\n\n`slp` - Slope\n\n`caa` - Number of major vessels \n\n`thall` - Thalium Stress Test result ~ (0,3)\n\n`exng` - Exercise induced angina ~ 1 = Yes, 0 = No\n\n`output` - Target variable","6205fcbc":"\n#### In this section, we analyze relationship between different features with respect to **Output** (heart attack). We see how different feature values show different heart attack chance. We also plot different kinds of diagrams to visualize our data and findings.\n\n### We will refer to **heart attack** with the keyword **Output**","4c543357":"#### 2) Support Vector Machine (SVM)\n\n[Support Vector Machine (SVM)](https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine) model is a Supervised Learning model used for classification and regression analysis. It is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a $p$-dimensional vector (a list of $p$ numbers), and we want to know whether we can separate such points with a $(p-1)$-dimensional hyperplane.\n\nWhen data are not labeled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called **support vector clustering** and is often used in industrial applications either when data are not labeled or when only some data are labeled as a preprocessing for a classification pass.\n\nIn the below code, [SVC](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html) stands for Support Vector Classification.","6f559d8c":"### Age vs Output","5e2712ff":"#### Nothing that could affect the process","c3d68cc4":"#### NOTE:\n\n>People between age 40-55 are more likely to have heart disease \n\n>whereas people between age 55-60 are less likely to have it. ","566a9329":"### SLP vs Output","abf764f3":"#### NOTE:<br>\n- There is a lackage of data for people in group **caa = 4, 3 and 2**\n- Yet, it's clear that for the group 0, the chances are about 75%","6a353a52":"### CP vs Output","7b728f81":"#### NOTE:<br>\n- Due to the lackage of the data for the groups **thall = 0 and 1**\n- Only the groups 2 and 3 should be taken in count\n- And chances of heart attack are much higher for the group 2","860850a9":"### Features that don't affect to the Output","53abc4f7":"#### 4) $k$-Nearest Neighbors\n\n[$k$-nearest neighbors algorithm (k-NN)](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm) is one of the simplest machine learning algorithms and is used for classification and regression. In both cases, the input consists of the $k$ closest training examples in the feature space. The output depends on whether $k$-NN is used for classification or regression:\n\n- In *$k$-NN classification*, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its $k$ nearest neighbors ($k$ is a positive integer, typically small). If $k = 1$, then the object is simply assigned to the class of that single nearest neighbor.\n\n\n- In *$k$-NN regression*, the output is the property value for the object. This value is the average of the values of its $k$ nearest neighbors.","61e51542":"### Checking for Outliers","dd7ecb4d":"#### Task \nTo perform EDA and predict if a person is prone to a heart attack or not.","bdc28a74":"### THALL vs Output","9d8eddd2":"### Predictions","4c09b2e2":"### Feature Selection","daffa75c":"### CHOL vs Output & TRTBPS vs Output","d0f45e84":"#### NOTE:\n\n>People in category **sex = 1** are less likely to have heart attack\n\n>Around 45% of chanses for people in that category\n\n>Around 75% of chanses to have heart attack for people in the category **sex = 0**","52925c0d":"#### 1) Logistic Regression\n\n[Logistic regression](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression), or logit regression, or logit model is a regression model where the dependent variable (DV) is categorical. This article covers the case of a binary dependent variable\u2014that is, where it can take only two values, \"0\" and \"1\", which represent outcomes such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. Cases where the dependent variable has more than two outcome categories may be analysed in multinomial logistic regression, or, if the multiple categories are ordered, in ordinal logistic regression.","7ecc49ee":"#### NOTE:<br>\n- People without exercise induced angina are more probable to suffer from heart disease.\n- Chances are around 70%\n- ans only 30% respectively, for those with exercise induced angina","b05e9f70":"### Conclusions from the EDA\n\n1. There are no NaN values in the data.\n2. All the data is numeric, yet some columns need to be **scaled**\n3. The data consists of more than twice the number of people with `sex` = 1 than `sex` = 0. So the accuracy of the models for people coresponding to the group **sex = 0** might be slighly different \/ lower.\n4. It is intuitive that elder people might have higher chances of heart attack but according to the distribution plot of `age` vs `output`, it is evident that this isn't the case.\n\n","831f2c6b":"### THALACHH vs Output","da61f5d1":"#### Check proportions between people who had heart attack and those who did not","6218f6c0":"### Correlation","f795b1cf":"#### NOTE:<br>\n- People having cholestrol 150-250 and blood presure between 110 to 140 are more likely to have heart attack.","8686919e":"#### 3) Linear SVM\n\nLinear SVM is a SVM model with linear kernel.<br>\nIn the below code, [LinearSVC](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.LinearSVC.html) stands for Linear Support Vector Classification.","dadaad06":"#### NOTE:<br>\n- People with lower pevious peak achieved have higher chances of heart attack.","a9a92d0a":"#### 5) Decision Tree\n\nA [decision tree](https:\/\/en.wikipedia.org\/wiki\/Decision_tree) is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.","49c4b1af":"### EXNG vs Output","4a8af401":"### CAA vs Output","d7e886e1":"#### Check whether ther is missing data or not","7dcc1c74":"### Sex vs Output","8d1d535e":"##### Checking the number of unique values in each column","c90ce7f5":"### OLDPEACK vs Output","b7cef730":"#### NOTE <br>\n- In case of **trtbps** positive and negative result of **Output** are similar\n- In case of **fbs**, the same - 55% vs 51% \n- In case of **restecg** - there are lackage of the data for the option **2**. The rest of the options **0 and 1** are 46% vs 63%. Not so far from 50%...  ","80ebe652":"#### NOTE:<br>\n- Chances to have heart attack for people in the group **spl == 2** are much higher than for the groups 0 and 1."}}