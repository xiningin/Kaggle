{"cell_type":{"e19df2b8":"code","434b748b":"code","d592c138":"code","40de2efd":"code","7e547d1a":"code","12f77182":"code","1bcd8e6d":"code","a830b790":"code","1bb9c0ce":"code","610a2e2f":"code","4c318af2":"code","16920dc0":"code","7443a9e2":"code","f4958871":"code","0280785d":"code","03d21e96":"code","26bc8e91":"code","ba40b8eb":"code","8932778d":"code","55306976":"code","e51254ec":"code","1cd95be9":"code","3487fc12":"code","78ca287a":"code","79c1fde6":"code","611aa917":"code","73740474":"code","c0ecc4e7":"code","6c46468c":"code","e573005c":"code","36807d14":"code","db5abac6":"code","a76ddf04":"code","2a516693":"code","c9bcc8d3":"code","4efff83b":"code","17d012dc":"code","066330fc":"code","f931aaff":"code","6b11d08a":"code","5632d347":"code","940be10d":"code","62e10afd":"code","1bc7bd10":"code","0f5bc238":"code","84e617f6":"code","766fa276":"code","b79a68f1":"code","9ec872b6":"code","460d91c8":"code","70e33762":"code","d823abe8":"code","ac3d47b2":"code","3e1b5045":"code","5679a942":"code","3bf1b07b":"code","f5e1b595":"code","98449632":"markdown","6c19a1af":"markdown","8e4658c4":"markdown","54dc3310":"markdown","e16d368a":"markdown","b3500ccc":"markdown","6a8bec1d":"markdown","07cd9f13":"markdown","b07a5893":"markdown","cfa59ee8":"markdown","5bf78fc0":"markdown","be98dd4e":"markdown","d2d1ca70":"markdown","a2a1eafc":"markdown","fa5cf9d4":"markdown","104433b9":"markdown","325490ca":"markdown","278a6333":"markdown","666cc67e":"markdown","73e12d61":"markdown","b5bbf13a":"markdown","1715201a":"markdown","32a0b5a9":"markdown","79484778":"markdown","95906e3f":"markdown","e8130fc2":"markdown","1e22ab60":"markdown","065dd866":"markdown","2b554268":"markdown","820503f0":"markdown","7ac649fe":"markdown"},"source":{"e19df2b8":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","434b748b":"import numpy as np\nimport pandas as pd\nimport os\nimport pickle\nimport time\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\n\n# missing value compensation\nfrom sklearn.impute import SimpleImputer\n\n# Feature Decompression\nfrom sklearn.feature_selection import RFE\n\n# Train Algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\n\n # select model\nfrom ipywidgets import interact,interactive,fixed,interact_manual\nfrom IPython.display import display\nimport ipywidgets as widgets","d592c138":"# setting display option\npd.options.display.max_columns = 50","40de2efd":"# setting of path to dataset \ntrain_file = '\/kaggle\/input\/titanic\/train.csv'\ntest_file = '\/kaggle\/input\/titanic\/test.csv'\n\n# setting of submission file\nos.makedirs('\/kaggle\/working\/submit\/', exist_ok=True)\nsubmit_file_dir = '\/kaggle\/working\/'\nsubmit_file_name = 'submission.csv'","7e547d1a":"# ID column in submission file\nID_column = 'PassengerId'\n\n# target data column of submission file\ntarget_value = 'Survived'","12f77182":"# one-hot-encoding columns (categorical data)\nohe_columns = ['Pclass', 'Sex', 'SibSp','Parch', 'Embarked', 'Title_Group']\n\n# create dic to set data type for each column\nmy_dtype = {k: object  for k in ohe_columns}\nprint(my_dtype)","1bcd8e6d":"# Reject column list\nscore_reject_items =[ID_column,'Ticket', 'Cabin']\ntrain_reject_items = score_reject_items + [target_value]","a830b790":"# Read csv file\ndataset = pd.read_csv(train_file,\n                     header=0,    # 1st line is header, date[0]\n                     dtype=my_dtype)\n\n# Reject 1st row : ID and target row\nX = pd.DataFrame(dataset).drop(columns=train_reject_items, axis=1)\n\n# pandas.core.series.Series\ny = pd.Series(dataset[target_value])\n\n# check the shape\nprint('----------------------------------------------------------------')\nprint('X shape: (%i,%i)' %X.shape)\nprint('----------------------------------------------------------------')\nprint('y shape: (%i,)' %y.shape)\nprint('----------------------------------------------------------------')\nprint(y.value_counts())\nprint('Survived\uff081\uff1ayes\u30010\uff1ano\uff09')\nprint('----------------------------------------------------------------')\nprint()\nprint('dataset:raw data')\ndisplay(dataset.head())\nprint('X:train dataset')\nX.join(y).head()","1bb9c0ce":"# Read csv file\ndataset_s = pd.read_csv(test_file,\n                          header=0,     # 1st line is header, date[0]\n                          dtype=my_dtype)\n\n# Reject 1st row : ID and target row\nID_s = dataset_s.iloc[:,[0]]    # row 0 : ID \nX_s = dataset_s.drop(score_reject_items, axis=1)\n\n# Shape confirmation\nprint('-----------------------------------')\nprint('Raw Shape: (%i, %i)' %dataset_s.shape)\nprint('X_s Shape: (%i, %i)' %X_s.shape)\nprint('-----------------------------------')\nprint(X_s.dtypes)\nID_s.join(X_s).head()","610a2e2f":"# Missing values in each columns\ndataset.isnull().sum()","4c318af2":"# data type in each column\ndataset.dtypes","16920dc0":"# To extract title from name\nX['Title'] = X.Name.str.extract(r',\\s*([^\\.]*)\\s*\\.', expand = False)\nprint(X.Title.unique())","7443a9e2":"# Binning Title into 'Title Group'\nMr = ['Mr']\nCrew1 = ['Don', 'Rev', 'Capt']\nCrew2 = ['Major', 'Col', 'Dr']\nWomen_Masters = ['Mrs', 'Miss', 'Master']\nAffluence = ['Mme', 'Ms', 'Lady', 'Sir', 'Mile', 'the Countess', 'Jonkheer']\n\nX['Title_Group'] = np.where(X['Title'] == Mr[0], 'Mr', 'Affluence')\nX['Title_Group'] = np.where(X['Title'].isin(Crew1),  \"Crew1\", X['Title_Group'])\nX['Title_Group'] = np.where(X['Title'].isin(Crew2),  \"Crew2\", X['Title_Group'])\nX['Title_Group'] = np.where(X['Title'].isin(Women_Masters),  \"Women_Masters\", X['Title_Group'])\nX['Title_Group'] = np.where(X['Title'].isin(Affluence),  \"Affluence\", X['Title_Group'])\nX = X.drop(columns=['Name', 'Title'])","f4958871":"# Average for each Title Group\nTitleplot = X.join(y)['Survived'].groupby(X['Title_Group']).mean()\nTitleplot","0280785d":"# column list for age prediction\ncols_age_prediction = ['Age', 'Pclass', 'Sex', 'Parch', 'SibSp']\n\n# One-Hot-Encoding\nage_X_df = X[cols_age_prediction]\nage_X_df = pd.get_dummies(age_X_df)\n\n# Diveide into 2 groups , train data and test data for age prediction\nknown_age_df = age_X_df[age_X_df.Age.notnull()]\nunknown_age_df = age_X_df[age_X_df.Age.isnull()]","03d21e96":"Age_X, Age_y = known_age_df.drop(columns=['Age'],axis=1) , known_age_df['Age']\n# Holdout(Split data)\nAge_X_train, Age_X_test, Age_y_train, Age_y_test=train_test_split(Age_X,\n                                               Age_y,\n                                               test_size=0.3,\n                                               random_state=1)\n# Training\nAge_lgb = lgb.LGBMRegressor()\nAge_lgb.fit(Age_X_train, Age_y_train)\n\n# Prediction\nAge_y_pred = Age_lgb.predict(unknown_age_df.drop(columns = ['Age'], axis=1))\n\n# Compensate missing value with predicted value\nX.loc[(X.Age.isnull()), 'Age'] = Age_y_pred","26bc8e91":"# Comfirmation of missing value\nX.isnull().sum()","ba40b8eb":"def one_hot_encoding(data, ohe_columns):\n    X_ohe = pd.get_dummies(data,\n                       dummy_na=True,    # True:including missing value\n                       columns=ohe_columns)\n    print('X_ohe shape:(%i,%i)' % X_ohe.shape)\n    display(X_ohe.head())\n    return X_ohe\n    \nX_ohe = one_hot_encoding(X, ohe_columns)","8932778d":"X_ohe_columns = X_ohe.columns.values\nX_ohe_columns","55306976":"def imputing_nan(X_ohe_for_training, X_ohe_apply_to):\n    \n    imp = SimpleImputer()    # default setting(median value)\n    imp.fit(X_ohe_for_training)\n    \n    X_ohe_columns =  X_ohe_for_training.columns.values\n    X_ohe = pd.DataFrame(imp.transform(X_ohe_apply_to), columns=X_ohe_columns)\n    display(X_ohe.head())\n    return X_ohe,  X_ohe_columns, imp\n\nX_ohe, X_ohe_columns, imp = imputing_nan(X_ohe, X_ohe)","e51254ec":"X_ohe.shape","1cd95be9":"def dimension_compression(X_ohe, y):\n    start = time.time()\n    selector = RFE(RandomForestClassifier(n_estimators=100, random_state=1),\n               n_features_to_select=35, # number of column number after compression\n               step=.05)\n    selector.fit(X_ohe,y)\n    X_ohe_columns =  X_ohe.columns.values\n    \n    # selector.support_ list of True or False\n    X_fin = X_ohe.loc[:, X_ohe_columns[selector.support_]]\n    \n    # Duration time\n    duration = time.time() - start\n    print(duration,'s')\n    \n    print('Duration Time:', 'X_fin shape:(%i,%i)' % X_fin.shape)\n    display(X_fin.head())\n    return X_fin, selector\n    \nX_fin, selector = dimension_compression(X_ohe, y)","3487fc12":"print('-----------------------------------')\nprint('X_fin shape: (%i,%i)' %X_fin.shape)\nprint('-----------------------------------')\nprint(y.value_counts())\nprint('--------------------------------------------------')\nprint('Survived\uff081\uff1ayes\u30010\uff1ano\uff09:')\nprint('--------------------------------------------------')\nprint('y shape: (%i,)' %y.shape)\nprint('--------------------------------------------------')","78ca287a":"# set pipelines for different algorithms\npipelines = {\n    'knn':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',KNeighborsClassifier())]),\n    'logistic':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',LogisticRegression(random_state=1))]),\n    'rsvc':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',SVC(C=1.0, kernel='rbf', class_weight='balanced', random_state=1))]),\n    'lsvc':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',LinearSVC(C=1.0, class_weight='balanced',random_state=1))]),\n    'tree':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',DecisionTreeClassifier(random_state=1))]),\n    'rf':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',RandomForestClassifier(random_state=1))]),\n    'gb':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',GradientBoostingClassifier(random_state=1))]),\n    'mlp':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',MLPClassifier(hidden_layer_sizes=(3,3), max_iter=1000, random_state=1))]),\n    'xgb':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',xgb.XGBClassifier(verbosity = 0))]),\n    'lgbm':\n        Pipeline([('scl',StandardScaler()),\n                    ('est',lgb.LGBMClassifier())]),\n}","79c1fde6":"params = {\n    'knn' : {'est__n_neighbors':[5,7,10], 'est__weights':['uniform','distance'],},\n    \n    'logistic': {'est__C':[1, 100],},\n    \n    'rsvc': {'est__C':[1, 100],},\n    \n    'lsvc': {'est__C':[1, 100],},\n    \n    'tree': {'est__max_depth': list(range(10, 20)),\n             'est__criterion': ['gini', 'entropy'],},\n    \n    'rf': {'est__n_estimators':[320, 340],\n            'est__max_depth': [8, 10,16],\n            'est__random_state': [0],},\n    \n    'gb': {'est__loss':['deviance'],\n            'est__learning_rate': [0.01, 0.1],\n            'est__min_samples_split': np.linspace(0.1, 0.5, 2),\n            'est__min_samples_leaf': np.linspace(0.1, 0.5, 2),\n            'est__max_depth':[3,5],\n            'est__max_features':['log2','sqrt'],\n            'est__criterion': ['friedman_mse',  'mae'],\n            'est__subsample':[0.5, 1.0],\n            'est__n_estimators':[10],},\n    \n    'mlp': {'est__solver': ['lbfgs'],\n            'est__max_iter': [10000],\n            'est__alpha': 10.0 ** -np.arange(1, 3),\n            'est__hidden_layer_sizes':np.arange(10, 12),},\n    \n    'xgb': {'est__n_estimators':[100,500,],\n            'est__max_depth':[6, 8,10,],\n            'est__learning_rate':[0.001, 0.01, 0.1, 1,],\n            'est__min_child_weight': [1,6],},\n    \n    'lgbm': {'est__max_depth':[20, 50, 60,100],\n            'est__learning_rate':[0.001, 0.01, 0.1,1],\n            'est__num_leaves':[15, 31,64, 128],\n            'est__n_estimators':[100, 500, 700],},\n}","611aa917":"# Holdout\nX_train, X_test, y_train, y_test=train_test_split(X_fin,\n                                               y,\n                                               test_size=0.3,\n                                               random_state=1)","73740474":"import warnings\nwarnings.simplefilter('ignore')\n\n#  Metrics\nevaluation_scoring = 'f1'\n\n# Dict instance Initialization\nscores = {}\nbest_params ={}\nbest_scores ={}\n\n# algorithm pipeline and gridsearch\nfor pipe_name, pipeline in pipelines.items():\n    print(pipe_name)\n    print(params[pipe_name])\n    start = time.time()\n    gs = GridSearchCV(estimator=pipeline,\n                     param_grid = params[pipe_name],\n                     scoring=evaluation_scoring,\n                     cv=5,\n                     return_train_score=False)\n    # training\n    gs.fit(X_train, y_train)\n    \n    print('time', time.time()-start)\n    scores[(pipe_name,'train')] = accuracy_score(y_train, gs.predict(X_train))\n    scores[(pipe_name,'test')] = accuracy_score(y_test, gs.predict(X_test))\n    best_params[pipe_name] = gs.best_params_\n    best_scores[pipe_name] = gs.best_score_\n    \n    # Create directories to save each model data\n    os.makedirs('\/kaggle\/working\/models\/pipeline_models', exist_ok=True)\n    # Save model\n    file_name = '\/kaggle\/working\/models\/pipeline_models\/'+pipe_name+str(time.time())+'.pkl'\n    pickle.dump(pipeline, open(file_name, 'wb'))\n\nprint('---accuracy---')\npd.Series(scores).unstack()","c0ecc4e7":"print(scores)\nprint()\nprint(best_scores)\nprint()\nprint(best_params)","6c46468c":"def get_answer(x):\n    return x","e573005c":"model_selection = get_answer(widgets.RadioButtons(options=pipelines.keys()))\ndisplay(model_selection)","36807d14":"selected_model_name = model_selection.value\nprint(selected_model_name)","db5abac6":"selected_model_name = 'lgbm'","a76ddf04":"best_params[selected_model_name]","2a516693":"# To parset best_params[seleced_model_name] value from value to list \ndef params_parser(best_params, selected_model_name):\n    return {k:[v] for k,v in best_params[selected_model_name].items()}","c9bcc8d3":"best_param_parsed = params_parser(best_params, selected_model_name)\nprint(best_param_parsed)","4efff83b":"def train_on_selected_model(pipe_name, pipelines, params, evaluation_scoring, tag):\n    start = time.time()\n    gs = GridSearchCV(estimator=pipelines[pipe_name],\n                     param_grid = params,\n                     scoring=evaluation_scoring,\n                     cv=5,\n                     return_train_score=False)\n    # fit\n    model = gs.fit(X_fin, y)\n    \n    print('time', time.time()-start)\n    scores[(pipe_name,'train')] = accuracy_score(y_train, gs.predict(X_train))\n    scores[(pipe_name,'test')] = accuracy_score(y_test, gs.predict(X_test))\n    \n    # Create directories\n    os.makedirs('\/kaggle\/working\/models\/', exist_ok=True)\n     # To save model\n    file_name = '\/kaggle\/working\/models\/' + pipe_name + '_' + tag + str(time.time())+'.pkl'\n    pickle.dump(model, open(file_name, 'wb'))\n    \n    return model","17d012dc":"selected_model = train_on_selected_model(selected_model_name, pipelines, best_param_parsed, evaluation_scoring, 'selected')","066330fc":"from sklearn import metrics\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ny_pred_m = selected_model.predict(X_fin)\n\n# FPR, TPR(\nfpr, tpr, thresholds = metrics.roc_curve(y, y_pred_m)\n\n# AUC\nauc = metrics.auc(fpr, tpr)\n\n# Plot ROC curve\nplt.plot(fpr, tpr, label='ROC curve (area = %.2f)'%auc)\nplt.legend()\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)","f931aaff":"from sklearn.metrics import f1_score\nprint(f1_score(y, y_pred_m))","6b11d08a":"X_s.isnull().sum()","5632d347":"# Name\u304b\u3089\u656c\u79f0(Title)\u306e\u62bd\u51fa\nX_s['Title'] = X_s.Name.str.extract(r',\\s*([^\\.]*)\\s*\\.', expand = False)\n\nX_s['Title_Group'] = np.where(X_s['Title'] == Mr[0], 'Mr', 'Affluence')\nX_s['Title_Group'] = np.where(X_s['Title'].isin(Crew1),  \"Crew1\", X_s['Title_Group'])\nX_s['Title_Group'] = np.where(X_s['Title'].isin(Crew2),  \"Crew2\", X_s['Title_Group'])\nX_s['Title_Group'] = np.where(X_s['Title'].isin(Women_Masters),  \"Women_Masters\", X_s['Title_Group'])\nX_s['Title_Group'] = np.where(X_s['Title'].isin(Affluence),  \"Affluence\", X_s['Title_Group'])\nX_s = X_s.drop(columns=['Name', 'Title'])","940be10d":"X_s.head()","62e10afd":"# One-Hot-Encoding\nage_df_s = X_s[cols_age_prediction]\nage_df_s = pd.get_dummies(age_df_s)\n\n# Select data by Age value condition\nunknown_age_s = age_df_s[age_df_s.Age.isnull()].values\nunknown_age_s_df = age_df_s[age_df_s.Age.isnull()]\n\ncols_train_Age= set(unknown_age_df.columns.values)\ncols_test_Age = set(unknown_age_s_df.columns.values)\n\ndiff1 = cols_train_Age - cols_test_Age\nprint('Columns of existing in training data: %s' %diff1)\n\ndiff2 = cols_test_Age - cols_train_Age\nprint('Columns of existing in test data for submission: %s' %diff2)\n\n# Predict age for test data \nAge_s_y_pred = Age_lgb.predict(unknown_age_s_df.drop(columns = ['Age']+list(diff2), axis=1))\nX_s.loc[(X_s.Age.isnull()), 'Age'] = Age_s_y_pred","1bc7bd10":"X_s.isnull().sum()","0f5bc238":"X_ohe_s = pd.get_dummies(X_s,\n                         dummy_na=True,\n                         columns=ohe_columns)\nprint('X_ohe_s shape:(%i,%i)' % X_ohe_s.shape)\nX_ohe_s.head(3)","84e617f6":"cols_model= set(X_ohe.columns.values)\ncols_score = set(X_ohe_s.columns.values)\n\ndiff1 = cols_model - cols_score\nprint('Columns of existing in training data:: %s' %diff1)\n\ndiff2 = cols_score - cols_model\nprint('Columns of existing in test data for submission: %s' %diff2)","766fa276":"dataset_cols_m = pd.DataFrame(None,\n                         columns=X_ohe_columns,\n                         dtype=float)\ndisplay(dataset_cols_m)","b79a68f1":"X_ohe_s = pd.concat([dataset_cols_m, X_ohe_s])\nprint(X_ohe_s.shape)\ndisplay(X_ohe_s.head(3))","9ec872b6":"set_Xm = set(X_ohe.columns.values)\nset_Xs = set(X_ohe_s.columns.values)\nprint(set_Xs-set_Xm)\nX_ohe_s = X_ohe_s.drop(list(set_Xs-set_Xm),axis=1)\n\nprint(X_ohe_s.shape)\ndisplay(X_ohe_s.head(3))","460d91c8":"print(set_Xm-set_Xs)\nX_ohe_s.loc[:,list(set_Xm-set_Xs)] = X_ohe_s.loc[:,list(set_Xm-set_Xs)].fillna(0,axis=1)\nX_ohe_s.head(3)","70e33762":"X_ohe_s = X_ohe_s.reindex(X_ohe_columns, axis=1)\nX_ohe_s.head(3)\nprint(X_ohe_s.shape)","d823abe8":"X_ohe_s.isnull().sum()","ac3d47b2":"print('Missing Value before processing',X_ohe_s.isnull().sum().sum())\n\n# (important)\"imp\" instance made with train data shall be used for test data transform\n# If re-calculation with test data , median value will be replace test data median value\nX_ohe_s = pd.DataFrame(imp.transform(X_ohe_s),columns=X_ohe_columns)\n\nprint('Missing Value after processing',X_ohe_s.isnull().sum().sum())","3e1b5045":"X_fin_s = X_ohe_s.loc[:, X_ohe_columns[selector.support_]]\nprint(X_fin_s.shape)\nX_fin_s.head(3)\nX_fin_s = X_ohe_s","5679a942":"print('-----------------------------------')\nprint('X_fin_s shape: (%i,%i)' %X_fin_s.shape)\nprint('-----------------------------------')","3bf1b07b":"y_pred = selected_model.predict(X_fin_s)\nprint(len(y_pred))","f5e1b595":"result_for_submit = pd.DataFrame({ID_column:dataset_s[ID_column], target_value:y_pred})\n\nos.makedirs(submit_file_dir, exist_ok=True)\nresult_for_submit.to_csv(submit_file_name, index=False)","98449632":"Here is parameter setting.Many parameter makes long train time","6c19a1af":"## 1. Preparation","8e4658c4":"### 9-7. Missing value compensation\nTo use imp which is calcurated with train data, not test data","54dc3310":"### 9-3. Test data comparison with train data","e16d368a":"Path setting for dataset","b3500ccc":"### 4-4 .Missiong value compensation with Simple Imputer\nfit with train data","6a8bec1d":"### 3-2. test data","07cd9f13":"### 9-6. Define column order with reindex()","b07a5893":"### 5-2. Parameters for GridSearch","cfa59ee8":"## 5. Pipeline Definition \n### 5-1. Pipeline  \nTo provide pipeline in dict type","5bf78fc0":"## 7. Algorithm selection","be98dd4e":"## 9-8. Demension compression and feature column selection","d2d1ca70":"### 4-5. Dimension compression with RFE","a2a1eafc":"## 4. Data Preprocessing\n### 4-1. Name title extraction","fa5cf9d4":"### 9-2. Extract name titles","104433b9":"# Simple Auto ML for titanic data\nThis notebook shows a flow of data preprocessing and train with several algorithm and gridsearch on pipeline  \n\n1. Preparaion\n2. Dataset Preparation\n3. Read Dataset  \n4. Dataset preprocessing  \n  4-1. Title Extraction  \n  4-2. Age column missing value compensation  \n  4-3. One Hot Encoding  \n  4-4. Missing value compensation with Simple Imputer  \n  4-5. Dimension compression with RFE  \n5. pipeline definition  \n  5-1. Define algorithm in pipeline  \n  5-2. Parameter setting for GridSearch  \n6. Train on several algorithms in pipeline with Grid Search  \n7. Algorithm selection  \n8. Re-training with all  data  \n9. Test Data prediction  \n  9-1. Check columns of test data  \n  9-2. Title Extraction  \n  9-3. Test data comparison with train data  \n  9-4. One-hot-encoding Process  \n  9-5. Column selection  \n  9-6. Define column order with reindex()  \n  9-7. Missing value compensation  \n  9-8. Demension compression and feature column selection  \n10. Predition  \n11. Creat submission file    \n\nThis notebook shows a flow of data preprocessing and train with seleral algorithm and gridsearch on pipeline","325490ca":"## 11. Creating submission file","278a6333":"### 4-3. One Hot Encoding  \nOne hot encoding process for tha data of categorical data along with ohe_columns list","666cc67e":"## 6. Train on several algorithms in pipeline with Grid Search","73e12d61":"### 8. Re-Training with all data  ","b5bbf13a":"## 2. setting for data","1715201a":"## 9. Test data Scoring for submission\n### 9-1. Check columns of test data","32a0b5a9":"Select alogorithm with radio button","79484778":"This list is used for column selection.","95906e3f":"Option setting for display all columns","e8130fc2":"### 9-5. Column selection","1e22ab60":"### 9-4. One-hot-encoding Process","065dd866":"### 4-2. Missing value compensatin by prediction \n#### 'Age'","2b554268":"This One Hot columns list is used for process in 4-3.","820503f0":"## 3. Read dataset\n### 3-1. train data","7ac649fe":"## 10. Predicion"}}