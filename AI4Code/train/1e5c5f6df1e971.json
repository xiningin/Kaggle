{"cell_type":{"ab1f410a":"code","bb8855bd":"code","cbec3d46":"code","e8d68f59":"code","eeddc3fa":"code","6c804f84":"code","bfe5a1d9":"code","67d623ed":"code","a012e312":"code","c1ac2534":"code","2346dcef":"code","24b05b89":"code","a44f7218":"code","752a1def":"code","4581a480":"code","0e4b4f3a":"code","bae23a7a":"code","c6c9f8f3":"code","110be2ad":"code","0737797f":"code","8d6a016f":"code","e5aa3e18":"code","ac8a8535":"code","f79a8657":"code","acdeca1e":"code","231add6c":"code","8c9d0728":"code","2ff52563":"code","03f041de":"markdown","654fc889":"markdown","aa5fa067":"markdown","17dfc96d":"markdown","c6b87001":"markdown","53e7765c":"markdown","22222ca2":"markdown","ed25c2c4":"markdown","f3db64f4":"markdown","da7eb18a":"markdown"},"source":{"ab1f410a":"import numpy as np # linear algebra\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib.patches as patches\nimport os\nfrom PIL import Image\nimport cv2\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","bb8855bd":"!pip install xmltodict #installing the library to read XMl files\nimport xmltodict","cbec3d46":"imagenames=[] #list of imagefile names\nxmlnames=[] #list of xmlfile names\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if os.path.join(dirname, filename)[-3:]!=\"xml\":\n            imagenames.append(filename)\n        else:\n            xmlnames.append(filename)","e8d68f59":"path_im=\"\/kaggle\/input\/face-mask-detection\/images\/\"  #path for an image folder \npath_an=\"\/kaggle\/input\/face-mask-detection\/annotations\/\" #path for xmlfiles folder","eeddc3fa":"with open(path_an+xmlnames[0]) as f:\n    print(xmltodict.parse(f.read()))","6c804f84":"## Code for finding the total no of labels in our dataset\n\nlisting=[]\nfor i in imagenames[:]:\n    with open(path_an+i[:-4]+\".xml\") as fd:\n        doc=xmltodict.parse(fd.read())\n    temp=doc[\"annotation\"][\"object\"]\n    if type(temp)==list:\n        for i in range(len(temp)):\n            listing.append(temp[i][\"name\"])\n    else:\n        listing.append(temp[\"name\"])\n\nfor i in  set(listing):\n    print(i)","bfe5a1d9":"options={\"with_mask\":0,\"without_mask\":1,\"mask_weared_incorrect\":2} # mapping for predictions and analysis purpose","67d623ed":"def draw_bounding_box(input_image): #function to visualize images\n    with open(path_an+input_image[:-4]+\".xml\") as fd:\n        doc=xmltodict.parse(fd.read())\n    image=plt.imread(os.path.join(path_im+input_image))\n    fig,ax=plt.subplots(1)\n    ax.axis(\"off\")\n    fig.set_size_inches(10,5)\n    temp=doc[\"annotation\"][\"object\"]\n    if type(temp)==list:\n        for i in range(len(temp)):\n            if temp[i][\"name\"]==\"with_mask\":\n                a,b,c,d=list(map(int,temp[i][\"bndbox\"].values()))\n                patch=patches.Rectangle((a,b),c-a,d-b,linewidth=1, edgecolor='g',facecolor=\"none\",)\n                ax.add_patch(patch)\n            if temp[i][\"name\"]==\"without_mask\":\n                a,b,c,d=list(map(int,temp[i][\"bndbox\"].values()))     \n                patch=patches.Rectangle((a,b),c-a,d-b,linewidth=1, edgecolor='r',facecolor=\"none\",)\n                ax.add_patch(patch)\n            if temp[i][\"name\"]==\"mask_weared_incorrect\":\n                a,b,c,d=list(map(int,temp[i][\"bndbox\"].values()))\n                patch=patches.Rectangle((a,b),c-a,d-b,linewidth=1, edgecolor='y',facecolor=\"none\",)\n                ax.add_patch(patch)\n    else:\n        a,b,c,d=list(map(int,temp[\"bndbox\"].values()))\n        edgecolor={\"with_mask\":\"g\",\"without_mask\":\"g\",\"mask_weared_incorrect\":\"y\"}\n        patch=patches.Rectangle((a,b),d-b,c-a,linewidth=1, edgecolor=edgecolor[temp[\"name\"]],facecolor=\"none\",)\n    ax.imshow(image)\n    ax.add_patch(patch)","a012e312":"for i in range(0,5):\n    draw_bounding_box(imagenames[i])","c1ac2534":"def make_dataset(no_of_images): #function to make dataset\n    image_tensor=[]\n    label_tensor=[]\n    for i,j in enumerate(no_of_images):\n        with open(path_an+j[:-4]+\".xml\") as fd:\n            doc=xmltodict.parse(fd.read())\n        if type(doc[\"annotation\"][\"object\"])!=list:\n            temp=doc[\"annotation\"][\"object\"]\n            a,b,c,d=list(map(int,temp[\"bndbox\"].values()))\n            label=options[temp[\"name\"]]\n            image=transforms.functional.crop(Image.open(path_im+j).convert(\"RGB\"), b,a,d-b,c-a)\n            image_tensor.append(my_transform(image))\n            label_tensor.append(torch.tensor(label))\n        else:\n            temp=doc[\"annotation\"][\"object\"]\n            for k in range(len(temp)):\n                a,b,c,d=list(map(int,temp[k][\"bndbox\"].values()))\n                label=options[temp[k][\"name\"]]\n                image=transforms.functional.crop(Image.open(path_im+j).convert(\"RGB\"), b,a,d-b,c-a)\n                image_tensor.append(my_transform(image))\n                label_tensor.append(torch.tensor(label))\n                \n    final_dataset=[[k,l] for k,l in zip(image_tensor,label_tensor)]\n    return tuple(final_dataset)","2346dcef":"#importing neccessary libraries for deeplearning task..\nimport torch\nfrom torchvision import datasets,transforms,models\nfrom torch.utils.data import Dataset,DataLoader\n\nmy_transform=transforms.Compose([transforms.Resize((226,226)),\n                                 transforms.ToTensor()])\n\ndataset=make_dataset(imagenames) #making a datset\ntrain_size=int(len(dataset)*0.8)\ntest_size=len(dataset)-train_size\nbatch_size=32\ntrainset,testset=torch.utils.data.random_split(dataset,[train_size,test_size])\ntrain_loader =DataLoader(dataset=trainset,batch_size=batch_size,shuffle=True)\ntest_loader =DataLoader(dataset=testset,batch_size=batch_size,shuffle=True)","24b05b89":"dataiter=iter(train_loader) \nimages,labels=dataiter.next()\nimages=images.numpy()\n\nfig=plt.figure(figsize=(25,4))\nfor idx in np.arange(20):\n    ax=fig.add_subplot(2,20\/2,idx+1,xticks=[],yticks=[])\n    plt.imshow(np.transpose(images[idx],(1,2,0)))","a44f7218":"# We Will use pretrained resnet34 layer model.\nresnet=models.resnet34(pretrained=True)","752a1def":"for param in resnet.parameters():\n    param.requires_grad=False","4581a480":"import torch.nn as nn\nn_inputs=resnet.fc.in_features\nlast_layer=nn.Linear(n_inputs,3)\nresnet.fc.out_features=last_layer\n\nif torch.cuda.is_available():\n    resnet.cuda()\n    \nprint(resnet.fc.out_features)","0e4b4f3a":"if torch.cuda.is_available(): #checking for GPU availability\n    device = torch.device(\"cuda\")\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","bae23a7a":"for paramet in resnet.parameters():\n    paramet.requires_grad=True","c6c9f8f3":"import torch.optim as optim\n\ncriterion=nn.CrossEntropyLoss()\n\noptimizer=optim.SGD(resnet.parameters(),lr=0.001)","110be2ad":"n_epochs=3\n\nfor epoch in range(1,n_epochs+1):\n    train_loss = 0.0\n\n\n  ########################  TRAIN THE MODEL #################\n    for batch,(data,target) in enumerate(train_loader):\n    \n    \n        if torch.cuda.is_available():\n            data , target = data.cuda(), target.cuda()\n    \n        optimizer.zero_grad()\n        output=resnet(data)\n        loss=criterion(output,target)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n        if batch%20==19:\n            print(\"Epoch {}, batch {}, training loss {}\".format(epoch, batch+1,train_loss\/20))\n        train_loss = 0.0","0737797f":"#########Testing##########\ntest_loss=0.0\nacc=0\nresnet.eval()\n\nfor data,target in test_loader:\n    if torch.cuda.is_available():\n        data,target=data.cuda(),target.cuda()\n    output=resnet(data)\n    loss=criterion(output,target)\n    test_loss+=loss.item()\n    _,pred=torch.max(output,1)\n    predicted=pred.numpy()[:,np.newaxis] if not torch.cuda.is_available() else pred.cpu().numpy()[:,np.newaxis]\n    actual=target.numpy()[:,np.newaxis] if not torch.cuda.is_available() else target.cpu().numpy()[:,np.newaxis]\n    acc+=np.sum(predicted==actual)\/len(target.cpu().numpy())\n\nAverage_loss=test_loss\/len(test_loader)\nAverage_acc=acc\/len(test_loader)\n\nprint(\"Avg total loss is {:.6f}\".format(Average_loss))\nprint(\"Avg accuracy is {:.6f}\".format(Average_acc))\n","8d6a016f":"torch.save(resnet,open(\"resnet_model_face_mask\",\"wb\")) # saving the trained model.","e5aa3e18":"device = torch.device(\"cuda\")\nmodel=torch.load(open(\"\/kaggle\/working\/resnet_model_face_mask\",\"rb\"),map_location=device) #loading the model","ac8a8535":"!pip install mtcnn #installing library for predicting faces","f79a8657":"from mtcnn import MTCNN\ndetect=MTCNN()","acdeca1e":"def trans(bndbox,newimage):\n    a,b,c,d=bndbox[\"box\"]\n    image_crop=transforms.functional.crop(newimage, b,a,d-b,c-a)\n    my_transform=transforms.Compose([transforms.Resize((226,226)),\n                                     transforms.RandomCrop((224,224)),\n                                     transforms.ToTensor()])(image_crop)\n    return my_transform","231add6c":"def tag_plot(bndbox,filepath,predicted):\n    configut=[\"with_mask\",\"without_mask\",\"mask_weared_incorrect\"]\n    x=plt.imread(filepath)\n    fig,ax=plt.subplots(1)\n    ax.axis(\"off\")\n    fig.set_size_inches(15,10)\n    for i,j in zip(bndbox,predicted):\n        a,b,c,d=i[\"box\"]\n        patch=patches.Rectangle((a,b),c,d,linewidth=1, edgecolor='r',facecolor=\"none\",)\n        ax.imshow(x)\n        ax.text(a, b, configut[predicted[0]], size=10,\n                style='italic',verticalalignment=\"bottom\", horizontalalignment=\"left\",color=\"blue\")\n        ax.add_patch(patch)","8c9d0728":"model=model.eval()\ndef testing(filepath):\n    configut=[\"with_mask\",\"without_mask\",\"mask_weared_incorrect\"]\n    img = cv2.cvtColor(cv2.imread(filepath), cv2.COLOR_BGR2RGB)\n    newimage=Image.open(filepath).convert(\"RGB\")\n    bndbox=detect.detect_faces(img)\n    if len(bndbox)==1:\n        image_pred=trans(bndbox[0],newimage).unsqueeze(0)\n        _, pred=torch.max(model(image_pred.to(device)),1)\n        tag_plot(bndbox,filepath,predicted=pred)\n    else:\n        predicted=[]\n        for i in bndbox:\n            image_pred=trans(i,newimage).unsqueeze(0)\n            _, pred=torch.max(model(image_pred.to(device)),1)\n            predicted.append(pred)\n        tag_plot(bndbox,filepath,predicted)","2ff52563":"testing(path_im+imagenames[118]) # if you have images you can test them using this function..","03f041de":"The dataset contains image files and XML files.\nWe will check what's actually there in the image and XML file has.","654fc889":"### Thank you","aa5fa067":"## Testing","17dfc96d":"Now let's visualize some of the images.","c6b87001":"Now lets see how the images in the dataset looks like..","53e7765c":"We will start by importing necessary libraries","22222ca2":"Many Kagglers did very good job in this dataset,Since this is my first project in Kaggle so there are at many places codes need optimization.\nAny suugestions\/guidances in the code are most welcome.Feel free to comment.","ed25c2c4":"here you can see Xml file has properties of an image file including face locations and class it belong to..","f3db64f4":"## Now we will load the saved model and will see how it works.","da7eb18a":"## Training the model"}}