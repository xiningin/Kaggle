{"cell_type":{"fb9ebcf7":"code","94e55402":"code","6d1351a9":"code","5974b083":"code","e9cb0ed6":"code","3f240d32":"code","56224e1e":"code","1256796f":"code","f77a5797":"code","80cc086f":"code","3a40d7c5":"code","181087c6":"code","5e617632":"code","10b2a52a":"code","63ef3fce":"code","32981a09":"code","1c01e810":"code","a620fd51":"code","3a1aa2e5":"markdown","f885b89f":"markdown","92d01e30":"markdown","3d5dafb6":"markdown","5a6a36bf":"markdown","96eee754":"markdown","5ea8a9f7":"markdown","31cff717":"markdown","82a8bac0":"markdown","73ef4489":"markdown","1c5278e4":"markdown","5c2b7059":"markdown","cf39b3dc":"markdown","e9df9944":"markdown","77b33586":"markdown","dd222285":"markdown"},"source":{"fb9ebcf7":"#### Import\n\nimport re\nimport sys\nimport collections\n","94e55402":"### Function required to implement BPE\ndef get_stats(vocab):\n    pairs = collections.defaultdict(int)\n    for word, freq in vocab.items():\n        symbols = word.split()\n        for i in range(len(symbols)-1):\n            pairs[symbols[i],symbols[i+1]] += freq\n    return pairs\n\ndef merge_vocab(pair, v_in):\n    v_out = {}\n    bigram_pattern = re.escape(' '.join(pair))\n    p = re.compile(r'(?<!\\S)' + bigram_pattern + r'(?!\\S)')\n    for word in v_in:\n        w_out = p.sub(''.join(pair), word)\n        v_out[w_out] = v_in[word]\n    return v_out","6d1351a9":"def bpe_iter(data):\n    # calculate frq\n    pairs = collections.defaultdict(int)\n    data_arr = data.split()\n    for i in range(len(data_arr)-1):\n        pair = (data_arr[i], data_arr[i+1])\n        pairs[pair] += 1\n        \n    best = max(pairs, key=pairs.get)\n    if pairs[best] <= 1 :\n        return data\n    \n    data_new = data.replace(best[0] + ' ' + best[1], best[0] + best[1])\n    return data_new\n\ndef bpe(data):\n    while True:\n        data_new = bpe_iter(data)\n\n        if data_new == data:\n            break\n        else:\n            data = data_new\n    return data_new","5974b083":"data = \"a a a b d a a a b a c\"\ndata_new = bpe(data)\ndata_new","e9cb0ed6":"\ndata = \"l o w . l o w e s t .. n e w ... n e w e r .... w i d e r ..... l o w e r ...... w i d e s t\"\ndata = bpe(data)\ndata","3f240d32":"### importing Proter Stemmer\nfrom nltk.stem import PorterStemmer","56224e1e":"porter = PorterStemmer() ## Creating the object of porter stemmer\n#proide a word to be stemmed\nprint(\"Porter Stemmer\")\nprint(\"cats\",'->',porter.stem(\"cats\"))\nprint('trouble','->',porter.stem(\"trouble\"))\nprint('troubling','->',porter.stem(\"troubling\"))\nprint('troubled','->',porter.stem(\"troubled\"))","1256796f":"import nltk\nfrom nltk.stem import WordNetLemmatizer ## Lemmatizer Import\nwordnet_lemmatizer = WordNetLemmatizer() ## Lemmatizer Object Creation\n\nsentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\npunctuations=\"?:!.,;\"\nsentence_words = nltk.word_tokenize(sentence)\nfor word in sentence_words:\n    if word in punctuations:\n        sentence_words.remove(word)\n\nsentence_words\nprint(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\nfor word in sentence_words:\n    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))","f77a5797":"from nltk.corpus import stopwords\nprint(stopwords.words('english'))","80cc086f":"from collections import Counter\ndef count_term(sent : str)-> dict:\n    a = sent.lower().split()\n    return Counter(a)\n\nsent = 'Data Science is the sexiest job of the 21st century machine learning is the key for data science.'\nprint(count_term(sent))\n    ","3a40d7c5":"# import required module\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# assign documents\nd0 = 'Data Galcier for data geeks'\nd1 = 'data geeks'\nd2 = 'Artificial Intelligence'\n  \n# merge documents into a single corpus\nstring = [d0, d1, d2]","181087c6":"# create object\ntfidf = TfidfVectorizer()\n  \n# get tf-df values\nresult = tfidf.fit_transform(string)","5e617632":"# get idf values\nprint('\\nidf values:')\nfor ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n    print(ele1, ':', ele2)","10b2a52a":"\n# get indexing\nprint('\\nWord indexes:')\nprint(tfidf.vocabulary_)\n  \n# display tf-idf values\nprint('\\ntf-idf value:')\nprint(result)\n  \n# in matrix form\nprint('\\ntf-idf values in matrix form:')\nprint(result.toarray())","63ef3fce":"import matplotlib.pyplot as plt\nfrom nltk.corpus import brown ## Corpus import for data\nfrom wordcloud import WordCloud ## package import to generate word cloud\n \nwc = WordCloud().generate(' '.join(brown.words()))\n \nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()\n ","32981a09":"word2count = {}\nfor word in brown.words():\n    if word not in word2count.keys():\n        word2count[word] = 1\n    else:\n        word2count[word] += 1","1c01e810":"import heapq\nfreq_words = heapq.nlargest(100, word2count, key=word2count.get) ## Get most frquent 100 word","a620fd51":"import numpy as np\nX = []\nfor data in brown.sents(categories=['news', 'editorial', 'reviews']):\n    vector = []\n    for word in freq_words:\n        if word in data:\n            vector.append(1)\n        else:\n            vector.append(0)\n    X.append(vector)\nX = np.asarray(X)\nX","3a1aa2e5":"### Stopwrods","f885b89f":"### Bag of Words","92d01e30":"### Character Tokenization\nCharacter Tokenization splits apiece of text into a set of characters. It overcomes the drawbacks we saw above about Word Tokenization.\n\nCharacter Tokenizers handles OOV words coherently by preserving the information of the word. It breaks down the OOV word into characters and represents the word in terms of these characters\nIt also limits the size of the vocabulary. Want to talk a guess on the size of the vocabulary? 26 since the vocabulary contains a unique set of characters\n\nEx : **smarter**\n\ns-m-a-r-t-e-r\n","3d5dafb6":">  **Creating Vocabulary is the ultimate goal of Tokenization.**","5a6a36bf":"Here X is reprensentation can be use for Bag of Words.","96eee754":"### Stemming and Lemmatization","5ea8a9f7":"*Above are the stopwords present in the English language and part of nltk package.*","31cff717":"For more information please log on to : [Neural Machine Translation of Rare Words with Subword Units](https:\/\/www.aclweb.org\/anthology\/P16-1162.pdf)","82a8bac0":"### Word Tokenization\n\nWord Tokenization is the most commonly used tokenization algorithm. It splits a piece of text into individual words based on a certain delimiter. Depending upon delimiters, different word-level tokens are formed. Pretrained Word Embeddings such as Word2Vec and GloVe comes under word tokenization.\n\nEx : **I am a good boy**\n\n['I','am','a','good','boy']","73ef4489":"### Term Frequency","1c5278e4":"### Term Frequency & Inverse Document Frequency","5c2b7059":"## Introduction\nLanguage is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you\u2019ve ever picked up a language that wasn\u2019t your mother tongue, you\u2019ll relate to this! There are so many layers to peel off and syntaxes to consider \u2013 it\u2019s quite a challenge.\n\nAnd that\u2019s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That\u2019s where the concept of tokenization in Natural Language Processing (NLP) comes in.\n\nSimply put, we can\u2019t work with text data if we don\u2019t perform tokenization. Yes, it\u2019s really that important!","cf39b3dc":"> Tokenization is a common task in Natural Language Processing (NLP). It\u2019s a fundamental step in both traditional NLP methods like Count Vectorizer and Advanced Deep Learning-based architectures like Transformers","e9df9944":"### Subword Tokenization \nSubword Tokenization splits the piece of text into subwords (or n-gram characters). For example, words like lower can be segmented as low-er, smartest as smart-est, and so on.\n\nTransformed based models \u2013 the SOTA in NLP \u2013 rely on Subword Tokenization algorithms for preparing vocabulary. Now, I will discuss one of the most popular Subword Tokenization algorithm known as Byte Pair Encoding (BPE).\n\n\n![Subword Tokenization](https:\/\/www.thoughtvector.io\/blog\/subword-tokenization\/Subword%20Units.svg)","77b33586":"### Byte Pair Encoding (BPE)\nByte Pair Encoding (BPE) is a widely used tokenization method among transformer-based models. BPE addresses the issues of Word and Character Tokenizers:\n\n* BPE tackles OOV effectively. It segments OOV as subwords and represents the word in terms of these subwords\n* The length of input and output sentences after BPE are shorter compared to character tokenization\nBPE is a word segmentation algorithm that merges the most frequently occurring character or character sequences iteratively. Here is a step by step guide to learn BPE.\n\n","dd222285":"### Word Clouds"}}