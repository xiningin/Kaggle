{"cell_type":{"84f20495":"code","c85a0bff":"code","dba79dde":"code","80c2399a":"code","6dfdee89":"code","bbb92428":"code","f803f313":"code","f1c76385":"code","88be7838":"code","5d1856cc":"code","90fdbb5c":"code","57bd8947":"code","3a4cef38":"code","132a2906":"code","43e75833":"code","65191e14":"code","324656ac":"code","d5d73bfa":"code","7bca0ede":"code","0e2a537f":"code","cb2f7fe3":"code","0fb8e9c3":"code","ae4f8ad6":"code","2d47adb8":"code","1e0418e1":"code","6956848b":"code","adc346e1":"code","7cce5a45":"code","3376404b":"markdown","2a2b76db":"markdown","ec98cb63":"markdown","855a4e4f":"markdown","c353958c":"markdown","2f9d6cc7":"markdown","5ca354c4":"markdown","47ca0488":"markdown","6ab6c479":"markdown","93757e33":"markdown","23758d04":"markdown","d8eb63e6":"markdown"},"source":{"84f20495":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c85a0bff":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","dba79dde":"card_data = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\") ## load the data","80c2399a":"card_data.head() ## check the head","6dfdee89":"card_data.shape","bbb92428":"card_data.info()","f803f313":"## null value check\n\nnull_count = 0\nfor i in list(card_data.columns):\n    null_count = null_count+card_data[i].isnull().sum()\nif (null_count==0):\n    print(\"There is no null value in the data\")\nelse:\n    print(\"Total null value in the data is \",null_count)","f1c76385":"## let's check class imbalance\n\nfraud_trans = card_data[card_data['Class']==1]\nnorm_trans = card_data[card_data['Class']==0]\n\nprint (\"total percentage of fradulant transaction is : \",round(fraud_trans.shape[0]\/card_data.shape[0],3))","88be7838":"plt.figure(figsize = (10,8))\nax = sns.countplot(x='Class',data=card_data)\nax.set_yscale('log')\nplt.show()\n## variation in fradulant and non fradulant data","5d1856cc":"plt.figure(figsize=(20,10))\nay = sns.distplot(card_data['Amount'])\nay.set_yscale('log')\nplt.show()\n## distribution of transaction amount","90fdbb5c":"plt.figure(figsize = (20,10))\nplt.subplot(1,2,1)\nsns.scatterplot(data=fraud_trans,x=\"Time\",y=\"Amount\",hue=\"Class\")\nplt.subplot(1,2,2)\nsns.scatterplot(data=norm_trans,x=\"Time\",y=\"Amount\",hue=\"Class\")\nplt.show()","57bd8947":"cols = card_data.columns.tolist()\nplt.figure(figsize=(40,10))\nsns.heatmap(card_data.corr(),annot=True)\nplt.show()","3a4cef38":"import sklearn \nfrom sklearn.model_selection import train_test_split\n\ntrain,test = train_test_split(card_data,train_size=0.7,random_state=42)","132a2906":"train.shape\n\n","43e75833":"test.shape","65191e14":"y_train = train.pop(\"Class\")\nX_train = train\ny_test = test.pop(\"Class\")\nX_test = test","324656ac":"contamin = len(fraud_trans)\/float(len(norm_trans))\n\nfrom sklearn.ensemble import IsolationForest\n\nmodel = IsolationForest(max_samples=len(X_train),contamination=contamin,random_state=10,verbose=0)\nmodel.fit(X_train)\ny_pred = model.predict(X_train)\n\ny_pred[y_pred==1]=0\ny_pred[y_pred==-1]=1\n","d5d73bfa":"## lets check accuracy score \n\nfrom sklearn.metrics import accuracy_score\n\ntrain_score = accuracy_score(y_train,y_pred)","7bca0ede":"train_score ## final score ","0e2a537f":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_train,y_pred))","cb2f7fe3":"mis_classify = (y_pred!=y_train).sum()\n\nprint(\"mis classify in train set : \",mis_classify)","0fb8e9c3":"print(\"mis classify percentage is \",round(mis_classify\/X_train.shape[0],4))","ae4f8ad6":"## let's predict test data \n\ny_test_pred = model.predict(X_test)\n\ny_test_pred[y_test_pred==1]=0\ny_test_pred[y_test_pred==-1]=1","2d47adb8":"test_score = accuracy_score(y_test,y_test_pred)","1e0418e1":"test_score","6956848b":"print(classification_report(y_test,y_test_pred))","adc346e1":"mis_classify_test = (y_test_pred!=y_test).sum()\n\nprint(\"mis classify in train set : \",mis_classify_test)","7cce5a45":"print(\"mis classify percentage is \",round(mis_classify_test\/X_test.shape[0],4))","3376404b":"# Isolation Forest","2a2b76db":"* From the above two plots it is clear that fradunat transaction is varies between 0 to 2000 so there is not any high value fradulant transaction.\n* Normal transaction varies from 0 to 25000.","ec98cb63":"we will use the isolation forest algorithm to detect the anomaly on the data set. isolation forest generally works like a decision tree but in this case, we will not use any kind of measure like information gain.\n\n* First we will select randomly one feature and then select a split value of that feature which is in between min value and max value of that feature.\n* This process will iterate over time and isolate each point.\n* Generally on a dataset normal points are surrounded by all the other points but an anomaly is generally alone and situated a bit away from all the other points. so the path length between the root node and an anomaly is less than the path length between the root node and a normal point. It requires less splitting to isolate an anomoly.\n* Isolation forest is an ensemble method so it creates a forest of isolation trees.\n* Each point is classified by an anomaly score which is generated by the average path distance between a point (let's say x) and a root node of all the trees of the forest.\n\nFor more information check the following link [https:\/\/arpitbhayani.me\/blogs\/isolation-forest](http:\/\/)","855a4e4f":"# Context\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\n# Content\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders. This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, ... V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n# Inspiration\nIdentify fraudulent credit card transactions.\n\nGiven the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n\n# Acknowledgements\nThe dataset has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http:\/\/mlg.ulb.ac.be) of ULB (Universit\u00e9 Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on https:\/\/www.researchgate.net\/project\/Fraud-detection-5 and the page of the DefeatFraud project","c353958c":"* Classification report of train data.","2f9d6cc7":"# Model Building and Evalution","5ca354c4":"* Classification report of test data","47ca0488":"* Divide train and test set and split their independent and dependent values.","6ab6c479":"# Import Libraries and Inspect Data","93757e33":"**Contamination**- Contamination is the assumption about the fraction of anomalies in the dataset. This number is set by the intuition of the domain experts- generally the domain experts will have a rough idea about the fraction of the anomalies in the dataset. For most real-life cases, a value of 0.1 works.\n**max_samples**-The actual number of samples.","23758d04":"* As original data is already transformed into pca hence mainly variables are un corelated.","d8eb63e6":"# EDA"}}