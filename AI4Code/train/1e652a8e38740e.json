{"cell_type":{"ab3fba1c":"code","9f6e1924":"code","1214511f":"code","bf71fbe8":"code","1d35e98b":"code","b09eb4bd":"code","efe7ebec":"code","0ff57bd9":"code","fe84dbd2":"code","a4d1c9d4":"code","3bb9c402":"code","d5256a3c":"code","c502bc7c":"code","b80fee55":"code","c999172d":"code","4e2173ab":"code","5b340b72":"code","2731d7de":"code","c44f54ab":"code","adc4a63d":"code","2970a784":"code","01fbcd79":"code","485af44c":"code","9a571656":"code","2a37c88b":"code","728c3267":"code","59464f42":"code","f45d46f6":"code","e7d53083":"code","61ff1cd0":"code","a9fd7421":"code","a59e6997":"code","9674984d":"code","79f06839":"code","353dacd1":"code","72f069fe":"code","00014668":"code","3f13e521":"code","5ad6601e":"code","03593bbd":"code","6bae29df":"code","12cd2c83":"code","a7160462":"code","492256b2":"code","bb170e70":"code","8952a222":"code","f8f74c3e":"code","d1b196c5":"code","354980da":"code","de5a0d4a":"code","dc54da54":"code","4b7d29f1":"code","a3046d5c":"code","d30404d2":"code","1b2d28af":"code","00bdc4bb":"markdown","968100d2":"markdown","e45ab87c":"markdown","8b520b70":"markdown","eab655dc":"markdown","7bf87765":"markdown","d2693861":"markdown","7c130844":"markdown","cc266161":"markdown","ca8715d0":"markdown","19d0f76a":"markdown","16551a6d":"markdown","41e8064b":"markdown","61a54818":"markdown","cf38051c":"markdown","a09c041c":"markdown","426f0479":"markdown","cc40b22a":"markdown","52184b86":"markdown","01ffb565":"markdown","f1d6426d":"markdown","8eebd1e8":"markdown","d9685c8b":"markdown","05c88416":"markdown","12838163":"markdown","233d518c":"markdown","d5114fa2":"markdown","7156adcf":"markdown","9b799505":"markdown","c2f58f2d":"markdown","b5261401":"markdown","b2e36626":"markdown"},"source":{"ab3fba1c":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom urllib.request import urlopen\nfrom pylab import rcParams\n%matplotlib inline\ncolors = [\"#BF8069\", \"#73172F\", \"#F2BEA0\", \"#A63247\"]\ncustom_palette = sns.set_palette(sns.color_palette(colors))","9f6e1924":"import xgboost as xgb\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor","1214511f":"test_df = pd.read_csv('\/kaggle\/input\/house-price-prediction-challenge\/test.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/house-price-prediction-challenge\/train.csv')","bf71fbe8":"train_df.head()","1d35e98b":"train_df.shape","b09eb4bd":"train_df.isnull().sum()","efe7ebec":"train_df.info()","0ff57bd9":"train_df['TARGET(PRICE_IN_LACS)'].describe()","fe84dbd2":"rcParams['figure.figsize'] = 10, 6\nsns.distplot(train_df['TARGET(PRICE_IN_LACS)'])","a4d1c9d4":"train_df['LOG_TARGET'] = np.log(train_df['TARGET(PRICE_IN_LACS)'])\nsns.distplot(train_df['LOG_TARGET'])\nprint(train_df['LOG_TARGET'].skew(), train_df['LOG_TARGET'].kurt())","3bb9c402":"sns.boxplot(train_df['LOG_TARGET'])","d5256a3c":"sns.heatmap(train_df.corr(), annot=True)","c502bc7c":"train_df.drop('UNDER_CONSTRUCTION', axis=1, inplace=True)","b80fee55":"sns.distplot(train_df['SQUARE_FT'], color=custom_palette)","c999172d":"train_df['LOG_SQUARE'] = np.log(train_df['SQUARE_FT'])\nsns.distplot(train_df['LOG_SQUARE'], color=custom_palette)\nprint(train_df['LOG_SQUARE'].skew(), train_df['LOG_SQUARE'].kurt())","4e2173ab":"sns.boxplot(train_df['LOG_SQUARE'], color=custom_palette)","5b340b72":"sns.heatmap(train_df.corr(), annot=True)","2731d7de":"sns.jointplot('LOG_TARGET', 'LOG_SQUARE', data=train_df,\n                  kind=\"reg\", truncate=False,height=7)","c44f54ab":"rcParams['figure.figsize'] = 10, 6\nsns.relplot(\n    data=train_df,\n    x=\"LOG_TARGET\", y=\"LOG_SQUARE\",\n    size=\"BHK_NO.\", hue=\"RERA\", sizes=(10, 200),\n)","adc4a63d":"train_df.duplicated().sum()","2970a784":"print(train_df.shape)\ntrain_df.drop_duplicates(inplace=True)\nprint(train_df.shape)","01fbcd79":"pd.DataFrame({i:[len(train_df[i].unique()),train_df[i].unique()] for i in train_df.columns}, \n             index=['len_of_unique_values', 'values']).T.sort_values(by='len_of_unique_values')","485af44c":"rcParams['figure.figsize'] = 14, 8\ncategorical_columns = ['RERA', 'BHK_OR_RK', 'READY_TO_MOVE', 'RESALE', 'POSTED_BY', 'BHK_NO.']\nfig, axes = plt.subplots(2,3)\nfor i, ax in enumerate(axes.flatten()):\n   ax.hist(train_df[categorical_columns[i]], color = custom_palette)\n   ax.set_title([categorical_columns[i]])","9a571656":"train_df['PRICE_PER_SQUARE'] = train_df['SQUARE_FT'] \/ train_df['TARGET(PRICE_IN_LACS)']\ntrain_df.head()","2a37c88b":"train_df['ADDRESS1'] = train_df['ADDRESS'].apply(lambda x: x.split(',')[0].strip())\ntrain_df['ADDRESS2'] = train_df['ADDRESS'].apply(lambda x: x.split(',')[1].strip())\ntrain_df.head()","728c3267":"len(train_df['ADDRESS2'].unique())","59464f42":"state_df = train_df.groupby('ADDRESS2')['PRICE_PER_SQUARE'].agg(['min', 'max', 'mean', np.median, np.std, 'count']).reset_index()\nstate_df['diff_mean_median'] = np.abs(state_df['mean'] - state_df['median'])\nstate_df[state_df['count']>1000].nlargest(10, 'std')","f45d46f6":"bang_df = train_df[train_df['ADDRESS2'] == 'Bangalore']\nsns.heatmap(bang_df.corr(), annot=True)","e7d53083":"sns.boxplot(bang_df['LOG_SQUARE'])","61ff1cd0":"train_df = pd.concat([pd.get_dummies(train_df[categorical_columns], prefix=categorical_columns, \n                                     columns=categorical_columns), train_df], axis=1)\ntrain_df.drop(categorical_columns, axis=1, inplace=True)","a9fd7421":"le = LabelEncoder()\ntrain_df['LE_ADDRESS2'] = le.fit_transform(train_df['ADDRESS2'])\ntrain_df.head()","a59e6997":"X = train_df.drop(['LOG_TARGET', 'ADDRESS', 'ADDRESS1', 'ADDRESS2', \n                   'TARGET(PRICE_IN_LACS)', 'SQUARE_FT', 'PRICE_PER_SQUARE'], axis=1)\ny = train_df['LOG_TARGET'].values\nX_columns = X.columns # We'll use this values below\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nprint(len(X), len(y))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=17)\nprint(len(X_train), len(X_test))","9674984d":"clf_rf = RandomForestRegressor(n_estimators=250, max_depth=25)\nclf_rf.fit(X_train, y_train)\npred_rf = clf_rf.predict(X_test)","79f06839":"print(\"r2_score: {}\\nRMSE: {}\".format(r2_score(pred_rf, y_test), \n                                      np.sqrt(mean_squared_error(pred_rf, y_test))))","353dacd1":"#param = {\n#    'max_depth': range(8, 20, 4),\n#    'n_estimators': range(150, 300, 50)\n#}\n#clf_gs = GridSearchCV(clf_gbr, param)\n#clf_gs.fit(X_test, y_test)\n#print(clf_gs.best_params_, clf_gs.best_score_)","72f069fe":"clf_gbr = GradientBoostingRegressor(max_depth=8, n_estimators=250)\nclf_gbr.fit(X_train, y_train)\npred_gbr = clf_gbr.predict(X_test)","00014668":"print(\"r2_score: {}\\nRMSE: {}\".format(r2_score(pred_gbr, y_test), \n                                      np.sqrt(mean_squared_error(pred_gbr, y_test))))","3f13e521":"pd.DataFrame(clf_gbr.feature_importances_,X_columns, \n             columns=['coef']).sort_values(by='coef', ascending=False)","5ad6601e":"dmatrix = xgb.DMatrix(X, y)\nparams = {'max_depth': 20}\n\ncv_results = xgb.cv(dtrain=dmatrix, params=params, nfold=3,\n                    num_boost_round=100, early_stopping_rounds=10, metrics=\"rmse\", as_pandas=True, seed=17)","03593bbd":"cv_results.tail()","6bae29df":"xg_reg = xgb.XGBRegressor()\nxg_reg.fit(X_train, y_train)\npred_xb = xg_reg.predict(X_test)","12cd2c83":"print(\"r2_score: {}\\nRMSE: {}\".format(r2_score(pred_xb, y_test), \n                                      np.sqrt(mean_squared_error(pred_xb, y_test))))","a7160462":"rcParams['figure.figsize'] = 10, 6\nsns.boxplot(x='LOG_SQUARE', data=train_df)","492256b2":"def get_outliers(df, column_name):\n    \n    IQR = df[column_name].quantile(0.75) - df[column_name].quantile(0.25)\n    lower_sq_limit = df[column_name].quantile(0.25) - (IQR * 1.5)\n    upper_sq_limit = df[column_name].quantile(0.75) + (IQR * 1.5)\n    outliers = np.where(df[column_name] > upper_sq_limit, True,\n    np.where(df[column_name] < lower_sq_limit, True, False))\n    return outliers","bb170e70":"log_square_outliers = get_outliers(train_df, 'LOG_SQUARE')\n\ntrain_without_outliers = train_df.loc[~log_square_outliers,]\nprint(train_df.shape, \n      train_without_outliers.shape,\n      train_df.loc[log_square_outliers,].shape)\nsns.boxplot(x='LOG_SQUARE', data=train_without_outliers)","8952a222":"sns.distplot(train_without_outliers['LOG_SQUARE'])\nprint(train_without_outliers['LOG_SQUARE'].skew(), \n      train_without_outliers['LOG_SQUARE'].kurt())","f8f74c3e":"sns.boxplot(x='LOG_TARGET', data=train_without_outliers)","d1b196c5":"log_target_outliers = get_outliers(train_without_outliers, 'LOG_TARGET')\n\ntrain_without_outliers = train_without_outliers.loc[~log_target_outliers,]\nprint(train_df.shape, \n      train_without_outliers.shape)\nsns.boxplot(x='LOG_TARGET', data=train_without_outliers)","354980da":"sns.distplot(train_without_outliers['LOG_TARGET'])\nprint(train_without_outliers['LOG_TARGET'].skew(), \n      train_without_outliers['LOG_TARGET'].kurt())","de5a0d4a":"X = train_without_outliers.drop(['LOG_TARGET', 'ADDRESS', 'ADDRESS1', 'ADDRESS2', \n                   'TARGET(PRICE_IN_LACS)', 'SQUARE_FT', 'PRICE_PER_SQUARE'], axis=1)\ny = train_without_outliers['LOG_TARGET'].values\nX_columns = X.columns # We'll use this values below\n\nscaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nprint(X.shape, len(y))\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=17)\nprint(len(X_train), len(X_test))","dc54da54":"clf_gbr = GradientBoostingRegressor(max_depth=8, n_estimators=250)\nclf_gbr.fit(X_train, y_train)\npred_gbr = clf_gbr.predict(X_test)","4b7d29f1":"print(\"r2_score: {}\\nRMSE: {}\".format(r2_score(pred_gbr, y_test), \n                                      np.sqrt(mean_squared_error(pred_gbr, y_test))))","a3046d5c":"print(\"RMSE: {}\".format(np.exp(np.sqrt(mean_squared_error(pred_gbr, y_test)))))","d30404d2":"sns.scatterplot(pred_gbr, y_test, alpha=0.5, size=1)","1b2d28af":"fig, ax = plt.subplots(figsize=(30, 10))\nax.plot(y_test[:150], \n        label='y_test', color='red', linewidth=2)\nax.plot(clf_gbr.predict(X_test)[:150], \n        label='predicted', \n        linestyle='dashed', linewidth=2)\nax.legend(prop={\"size\":20})","00bdc4bb":"### <p style=\"background-color:#011526; color:#F2BEA0; text-align:center; padding:10px\">EDA<\/p>","968100d2":"<p style=\"font-size:110%; margin:0\"> Group by state and show more statistics about PRICE_PER_SQUARE. For example Top 10 standart deviation values ","e45ab87c":"<p style=\"font-size:110%; margin:0\">The same situation as target value. So, let's add to new column logarithm(SQUARE_FT)","8b520b70":"### <p style=\"background-color:#011526; color:#F2BEA0; text-align:center; padding:10px\"> Load necessary libraries <\/p>","eab655dc":"<p style=\"font-size:110%; margin:0\"> How to improve existing model? Delete outliers. This value can distort statistic data","7bf87765":"<p style=\"font-size:110%; margin:0\">Columns LOG_TARGET and LOG_SQARE have outliers. I'm not to delete outliers now, because I'm gonna to test model with outliers and without outliers.","d2693861":"<p style=\"font-size:110%; margin:0\">What is the distribution of the target column? ","7c130844":"<p style=\"font-size:110%; margin:0\">There are a few outliers in the form of black dots. Let's remove the outliers from the dataset. One of the possible ways to do this - using Inter Quartile Range (IQR) <a href=\"https:\/\/pypi.org\/project\/remove-outliers\/#:~:text=Multiply%20the%20interquartile%20range%20(IQR,IQR)%20from%20the%20first%20quartile\"> More details <\/a> <br>\nI'm going to create function for this","cc266161":"<p style=\"font-size:110%; margin:0\">What size of data? ","ca8715d0":"<p style=\"font-size:110%; margin:0\">How many unique states in the dataset?","19d0f76a":"<p style=\"font-size:110%; margin:0\"> May be dataset contains duplicate rows?<\/p>","16551a6d":"<p style=\"font-size:110%; margin:0\"> I'm gonna to  add a new column PRICE PER SQUARE = Logarithm(SQUARE \/ TARGET(PRICE)). <p style=\"font-weight:bold\">IMPORTANT: We can not use this feature in the model, because the test data frame does not contain column PRICE :). We can use this feature for descriptive statistics and getting more information about price. ","41e8064b":"<p style=\"font-size:110%; margin:0\">We know more about target now. Let's check correlation between columns.","61a54818":"\n<p style=\"font-size:110%; margin:0\">Now look at the ADDRESS. This column is contains 2 value separated by comma. If separate this column it's will be add two new feature<\/p>","cf38051c":"<p style=\"font-size:110%; margin:0\">What type of each variables?","a09c041c":"### Thank you. If you are interestest this notebook, please vote :)","426f0479":"<p style=\"font-size:110%; margin:0\">There is correlation between SQUARE_FT and LOG_TARGET 0.4. What we could know about SQUARE_FT? ","cc40b22a":"<p style=\"font-size:110%; margin:0\"> Label encoding column ADDRESS2","52184b86":"<p style=\"font-size:110%; margin:0\">How much missing values in dataset?","01ffb565":"<p style=\"font-size:110%; margin:0\">A dataset can contain variables of different types depending upon the data they store. It is important to know what's type of data contains dataset","f1d6426d":"<p style=\"font-size:110%; margin:0\"> One hot encoding for categorial columns","8eebd1e8":"### <p style=\"background-color:#011526; color:#F2BEA0; text-align:center; padding:10px\"> Model selection <\/p>","d9685c8b":"<p style=\"font-size:110%; margin:0;\"> RERA, BHK_OR_RK, READY_TO_MOVE, RESALE, POSTED_BY, BHK_NO. - Categorical data <\/p><p style=\"font-size:110%; margin:0\"> LOG_TARGET, LOG_SQUARE - Numeric continuous data <\/p><p style=\"font-size:110%; margin:0\"> LONGTIUDE, LANGITUDE - Coordinates, numeric continuous data <\/p><p style=\"font-size:110%; margin:0\"> ADDRESS - Text data <\/p>","05c88416":"# <p style=\"background-color:#011526; color:#F2BEA0; text-align:center; padding:10px\"> Exploratory Data Analysis, Feature extraction, Model selection<\/p>\n\n\n<p style=\"font-size:110%; margin:0\">In this notebook we perform the Exploratory Data Analytics(EDA), some technics of feauture extraction and model selection.<br>\nOur work is divided on the following steps<br>\n- Load necessary libraries<br>\n- Load data<br>\n- First look on dataset<br>\n- Feauture extraction\n- Model selection","12838163":"### <p style=\"background-color:#011526; color:#F2BEA0; text-align:center; padding:10px\"> Load data <\/p>","233d518c":"<p style=\"font-size:110%; margin:0\">Hm... Let's logarithm target column. I'm gonna to add  new column on data set","d5114fa2":"<p style=\"font-size:110%; margin:0\">Correlation between LOG_TARGET and LOG_SQUARE is equal 0.63. It's greather than correlation between SQUARE_FT and TARGET(PRICE_IN_LACS)","7156adcf":"<p style=\"font-size:110%; margin:0\">Ok. Let's look at target column - TARGET(PRICE_IN_LACS)","9b799505":"<p style=\"font-size:110%; margin:0\"> Get most important features","c2f58f2d":"<p style=\"font-size:110%; margin:0\">Correlation between UNDER_CONSTRUCTION and READY_TO_MOVE is -1. I'm gonna to delete one of those columns from dataset. ","b5261401":"### <p style=\"background-color:#011526; color:#F2BEA0; text-align:center; padding:10px\"> Feauture extraction <\/p>","b2e36626":"<p style=\"font-size:110%; margin:0\">Skewness is equal 1.134, it's greater than 1, the data are highly skewed. High kurtosis in a data set is an indicator that data has heavy outliers. Look at the boxplot"}}