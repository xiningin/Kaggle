{"cell_type":{"e954336c":"code","2faf3d58":"code","fd12ae8d":"code","d0ce1418":"code","bdb3a401":"code","09a95b16":"code","13c62960":"code","fa301072":"code","33e91a31":"code","39ebf38e":"code","44590830":"code","07a594f8":"code","c1eb24cc":"code","763af57a":"code","5127a8cd":"markdown","c2b32e0e":"markdown","c13bc4d4":"markdown","a21309ce":"markdown","162f0817":"markdown","17212eb1":"markdown","bdb5724b":"markdown","37c50c6d":"markdown","bb3d43b7":"markdown","412345c1":"markdown","832b671d":"markdown","f1835ca5":"markdown","7134ec01":"markdown"},"source":{"e954336c":"from keras.layers import Input\nfrom keras.layers.core import Activation, Dense, Dropout, Permute\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.merge import add, concatenate, dot\nfrom keras.layers.recurrent import LSTM\nfrom keras.models import Model, Sequential\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import np_utils\nfrom keras.utils.data_utils import get_file\n\nimport collections\nimport itertools\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport re\nimport tarfile\nfrom functools import reduce\n\n%matplotlib inline","2faf3d58":"path = get_file('babi-tasks-v1-2.tar.gz', origin='https:\/\/s3.amazonaws.com\/text-datasets\/babi_tasks_1-20_v1-2.tar.gz')\n\ntar = tarfile.open(path)","fd12ae8d":"def tokenize(sent):\n    '''\n    argument: a sentence string\n    returns a list of tokens(words)\n    '''\n    return word_tokenize(sent)\n \ndef parse_stories(lines):\n    '''\n    - Parse stories provided in the bAbI tasks format\n    - A story starts from line 1 to line 15. Every 3rd line,\n      there is a question &amp;amp;amp;amp;amp; answer.\n    - Function extracts sub-stories within a story and\n      creates tuples\n    '''\n    data = []\n    story = []\n    for line in lines:\n        line = line.decode('utf-8').strip()\n        nid, line = line.split(' ', 1)\n        nid = int(nid)\n        if nid == 1:\n            # reset story when line ID=1 (start of new story)\n            story = []\n        if '\\t' in line:\n            # this line is tab separated Q, A &amp;amp;amp;amp;amp; support fact ID\n            q, a, supporting = line.split('\\t')\n            # tokenize the words of question\n            q = tokenize(q)\n            # Provide all the sub-stories till this question\n            substory = [x for x in story if x]\n            # A story ends and is appended to global story data-set\n            data.append((substory, q, a))\n            story.append('')\n        else:\n            # this line is a sentence of story\n            sent = tokenize(line)\n            story.append(sent)\n    return data\n \ndef get_stories(f):\n    '''\n    argument: filename\n    returns list of all stories in the argument data-set file\n    '''\n    # read the data file and parse 10k stories\n    data = parse_stories(f.readlines())\n    # lambda func to flatten the list of sentences into one list\n    flatten = lambda data: reduce(lambda x, y: x + y, data)\n    # creating list of tuples for each story\n    data = [(flatten(story), q, answer) for story, q, answer in data]\n    return data","d0ce1418":"challenge = 'tasks_1-20_v1-2\/en-10k\/qa1_single-supporting-fact_{}.txt'\nprint('Extracting stories for the challenge: single_supporting_fact_10k')\n# Extracting train stories\ntrain_stories = get_stories(tar.extractfile(challenge.format('train')))\n# Extracting test stories\ntest_stories = get_stories(tar.extractfile(challenge.format('test')))","bdb3a401":"print('Number of training stories:', len(train_stories))\nprint('Number of test stories:', len(test_stories))\ntrain_stories[0]","09a95b16":"def vectorize_stories(data, word_idx, story_maxlen, query_maxlen):\n    # story vector initialization\n    X = []\n    # query vector initialization\n    Xq = []\n    # answer vector intialization\n    Y = []\n    for story, query, answer in data:\n        # creating list of story word indices\n        x = [word_idx[w] for w in story]\n        # creating list of query word indices\n        xq = [word_idx[w] for w in query]\n        # let's not forget that index 0 is reserved\n        y = np.zeros(len(word_idx) + 1)\n        # creating label 1 for the answer word index\n        y[word_idx[answer]] = 1\n        X.append(x)\n        Xq.append(xq)\n        Y.append(y)\n    return (pad_sequences(X, maxlen=story_maxlen),\n            pad_sequences(Xq, maxlen=query_maxlen), np.array(Y))","13c62960":"# creating vocabulary of words in train and test set\nvocab = set()\nfor story, q, answer in train_stories + test_stories:\n    vocab |= set(story + q + [answer])\n\n# sorting the vocabulary\nvocab = sorted(vocab)\n \n# Reserve 0 for masking via pad_sequences\nvocab_size = len(vocab) + 1\n \n# calculate maximum length of story\nstory_maxlen = max(map(len, (x for x, _, _ in train_stories + test_stories)))\n \n# calculate maximum length of question\/query\nquery_maxlen = max(map(len, (x for _, x, _ in train_stories + test_stories)))\n \n# creating word to index dictionary\nword_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n \n# creating index to word dictionary\nidx_word = dict((i+1, c) for i,c in enumerate(vocab))\n \n# vectorize train story, query and answer sentences\/word using vocab\ninputs_train, queries_train, answers_train = vectorize_stories(train_stories,\n                                                               word_idx,\n                                                               story_maxlen,\n                                                               query_maxlen)\n# vectorize test story, query and answer sentences\/word using vocab\ninputs_test, queries_test, answers_test = vectorize_stories(test_stories,\n                                                            word_idx,\n                                                            story_maxlen,\n                                                            query_maxlen)","fa301072":"print('-------------------------')\nprint('Vocabulary:\\n',vocab,\"\\n\")\nprint('Vocab size:', vocab_size, 'unique words')\nprint('Story max length:', story_maxlen, 'words')\nprint('Query max length:', query_maxlen, 'words')\nprint('Number of training stories:', len(train_stories))\nprint('Number of test stories:', len(test_stories))\nprint('-------------------------')","33e91a31":"# number of epochs to run\ntrain_epochs = 100\n# Training batch size\nbatch_size = 32\n# Hidden embedding size\nembed_size = 50\n# number of nodes in LSTM layer\nlstm_size = 64\n# dropout rate\ndropout_rate = 0.30","39ebf38e":"# placeholders\ninput_sequence = Input((story_maxlen,))\nquestion = Input((query_maxlen,))\n \nprint('Input sequence:', input_sequence)\nprint('Question:', question)\n \n# encoders\n# embed the input sequence into a sequence of vectors\ninput_encoder_m = Sequential()\ninput_encoder_m.add(Embedding(input_dim=vocab_size,\n                              output_dim=embed_size))\ninput_encoder_m.add(Dropout(dropout_rate))\n# output: (samples, story_maxlen, embedding_dim)\n \n# embed the input into a sequence of vectors of size query_maxlen\ninput_encoder_c = Sequential()\ninput_encoder_c.add(Embedding(input_dim=vocab_size,\n                              output_dim=query_maxlen))\ninput_encoder_c.add(Dropout(dropout_rate))\n# output: (samples, story_maxlen, query_maxlen)\n \n# embed the question into a sequence of vectors\nquestion_encoder = Sequential()\nquestion_encoder.add(Embedding(input_dim=vocab_size,\n                               output_dim=embed_size,\n                               input_length=query_maxlen))\nquestion_encoder.add(Dropout(dropout_rate))\n# output: (samples, query_maxlen, embedding_dim)\n \n# encode input sequence and questions (which are indices)\n# to sequences of dense vectors\ninput_encoded_m = input_encoder_m(input_sequence)\nprint('Input encoded m', input_encoded_m)\ninput_encoded_c = input_encoder_c(input_sequence)\nprint('Input encoded c', input_encoded_c)\nquestion_encoded = question_encoder(question)\nprint('Question encoded', question_encoded)\n \n# compute a 'match' between the first input vector sequence\n# and the question vector sequence\n# shape: `(samples, story_maxlen, query_maxlen)\nmatch = dot([input_encoded_m, question_encoded], axes=-1, normalize=False)\nprint(match.shape)\nmatch = Activation('softmax')(match)\nprint('Match shape', match)\n \n# add the match matrix with the second input vector sequence\nresponse = add([match, input_encoded_c])  # (samples, story_maxlen, query_maxlen)\nresponse = Permute((2, 1))(response)  # (samples, query_maxlen, story_maxlen)\nprint('Response shape', response)\n \n# concatenate the response vector with the question vector sequence\nanswer = concatenate([response, question_encoded])\nprint('Answer shape', answer)\n \nanswer = LSTM(lstm_size)(answer)  # Generate tensors of shape 32\nanswer = Dropout(dropout_rate)(answer)\nanswer = Dense(vocab_size)(answer)  # (samples, vocab_size)\n# we output a probability distribution over the vocabulary\nanswer = Activation('softmax')(answer)","44590830":"# build the final model\nmodel = Model([input_sequence, question], answer)\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n              metrics=['accuracy'])\n \nprint(model.summary())","07a594f8":"# start training the model\nhistory = model.fit([inputs_train, queries_train], answers_train, \n                    batch_size=batch_size, \n                    epochs=train_epochs,\n                    validation_data=([inputs_test, queries_test], answers_test))\n \n# save model\nmodel.save('model.h5')","c1eb24cc":"def plotmodelhistory(history): \n    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n    # summarize history for accuracy\n    axs[0].plot(history.history['accuracy']) \n    axs[0].plot(history.history['val_accuracy']) \n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy') \n    axs[0].set_xlabel('Epoch')\n    axs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\n    axs[1].plot(history.history['loss']) \n    axs[1].plot(history.history['val_loss']) \n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss') \n    axs[1].set_xlabel('Epoch')\n    axs[1].legend(['train', 'validate'], loc='upper left')\n    plt.show()\n\n# list all data in history\nprint(history.history.keys())\n\nplotmodelhistory(history)","763af57a":"for i in range(0,10):\n    current_inp = test_stories[i]\n    current_story, current_query, current_answer = vectorize_stories([current_inp], word_idx, story_maxlen, query_maxlen)\n    current_prediction = model.predict([current_story, current_query])\n    current_prediction = idx_word[np.argmax(current_prediction)]\n    print(' '.join(current_inp[0]), ' '.join(current_inp[1]), '| Prediction:', current_prediction, '| Ground Truth:', current_inp[2])\n    print(\"--------------------------------------------------------------\")","5127a8cd":"### 2.2. Vectorization of stories.\nAs previously, the input to our RNNs is a sequence of word IDs. So we need to use our vocabulary dictionary to convert the (story, question, and answer) triplet into a sequence of integer word IDs. The next block of code does this and zero pads the resulting sequences of story and answer to the maximum sequence lengths we computed previously. At this point, we have lists of padded word ID sequences for each triplet in the training and test sets:","c2b32e0e":"## 2. Pre-processing\n### 2.1 Getting User Stories : \n\nThis is a crucial step where we need to ingest the train and test file and extract stories out of it. The raw format in which text stories, questions and answers are kept is aforementioned. Initially, we will write some helper functions.","c13bc4d4":"### 2.3. Build a vocab.\nOur next step is to run through the texts in the generated lists and build our vocabulary. This should be quite familiar to us by now, since we have used a similar idiom a few times already. Unlike the previous time, our vocabulary is quite small, only 22 unique words, so we will not have any out of vocabulary words:","a21309ce":"## 3. Define and Train the model\n### 3.1. Define the model\nThe definition is longer than we have seen previously, so it may be convenient to refer to the diagram as you look through the definition:\n\n![model.PNG](attachment:model.PNG)\n\nThe model is trained using the RMSprop optimizer and categorical cross-entropy as the loss function:\n\nNow let's review the code of our model building.","162f0817":"## 1. Get the data.\nThe bAbI data for the first question answering task consists of 10,000 short sentences each for the training and the test sets. A story consists of two to three sentences, followed by a question. The last sentence in each story has the question and the answer appended to it at the end. The following block of code parses each of the training and test files into a list of triplets of story, question and answer:","17212eb1":"### 2.4.Find out the maximum length of the sequence.\nThe memory network is based on RNNs, where each sentence in the story and question is treated as a sequence of words, so we need to find out the maximum length of the sequence for our story and question. The following block of code does this. We find that the maximum length of a story is 14 words and the maximum length of a question is just 4 words:","bdb5724b":"**In this example, we will build a memory network for question answering.**\n\nThis exemple was simply taken in the book **Deep Learning with Keras,** *by Antonio Gulli and Sujit Pal, 2017.* \n\n* Memory networks are a specialized architecture that consist of a memory unit in addition to other learnable units, usually RNNs. Each input updates the memory state and the final output is computed by using the memory along with the output from the learnable unit. \n* This architecture was suggested in **2014** via the paper ***(Memory Networks, by J. Weston, S. Chopra, and A. Bordes, arXiv:1410.3916, 2014)***. A year later, another paper ***(Towards AIComplete Question Answering: A Set of Prerequisite Toy Tasks, by J. Weston, arXiv:1502.05698, 2015)*** put forward the idea of a synthetic dataset and a standard set of 20 question answering tasks, each with a higher degree of difficulty than the previous one, and applied various deep learning networks to solve these tasks. Of these, the memory network achieved the best results across all the tasks. \n* This dataset was later made available to the general public through **Facebook's bAbI** project (https:\/\/research.fb.com\/projects\/babi\/). The implementation of our memory network resembles most closely the one described in this paper (***End-To-End Memory Networks, by S. Sukhbaatar, J. Weston, and R. Fergus, Advances in Neural Information Processing Systems, 2015)***, in that all the training happens jointly in a single network. It uses the bAbI dataset to solve the first question answering task.\n\n### Table of interest:\n* 1. Get the data.\n* 2. Pre-processing.\n* 3. Model Building and Training.\n* 4. Model Evaluation and tests.\n\n\nLet's get started.","37c50c6d":"We train this network for 100 epochs with a batch size of 32 and achieve an accuracy of over 96% on the validation set:\n\n### 3.2. Training the Model","bb3d43b7":"**Hope that you find this notebook helpful. More to come.**\n\n**Please upvote this, to keep me motivate for doing better.**\n\n**Thanks.**\n","412345c1":"# Memory network for question answering with Keras on Babi Tasks(~97% accuracy)","832b671d":"## 4. Model evaluation.\n\n### 4.2. Visualization of history.","f1835ca5":"### 4.2. Test of model.\nWe ran the model against the first 10 stories from our test set to verify how good the predictions were:","7134ec01":"### References:\n* **Deep Learning with Keras**, *by Antonio Gulli and Sujit Pal, 2017.*\n* https:\/\/becominghuman.ai\/q-a-system-deep-learning-2-2-c0ad60800e3\n* https:\/\/appliedmachinelearning.blog\/2019\/05\/01\/developing-factoid-question-answering-system-on-babi-facebook-data-set-python-keras-part-1\/\n* https:\/\/appliedmachinelearning.blog\/2019\/05\/02\/building-end-to-end-memory-network-for-question-answering-system-on-babi-facebook-data-set-python-keras-part-2\/\n"}}