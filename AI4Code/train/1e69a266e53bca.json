{"cell_type":{"622bb898":"code","5b6df540":"code","4731e577":"code","face94e3":"code","d853952a":"code","8790ba37":"code","29f68757":"code","99419021":"code","e02d7138":"code","b8317a68":"code","a318d24d":"code","a2b644a5":"code","4a0d53ac":"code","f08e20e9":"code","ec874ff5":"code","80f2f8f4":"code","15ea01b6":"code","c862fb19":"code","cba7c6a4":"code","a51e5c30":"code","4facff71":"code","54b63f29":"code","3fbc5061":"code","357f2509":"code","0884efa4":"markdown","548c3461":"markdown","26f975e1":"markdown","89495458":"markdown","3de3752c":"markdown","079672b6":"markdown","7e59cb8a":"markdown","64d36b1c":"markdown","aca6dcde":"markdown","744b30ae":"markdown","e6877780":"markdown","42d5771a":"markdown","d34941f7":"markdown","42578673":"markdown","407ae937":"markdown","9801ef00":"markdown","0dbf1bc2":"markdown","724165f3":"markdown","16d6aa23":"markdown","044c0acf":"markdown"},"source":{"622bb898":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nfrom matplotlib import pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport glob\nimport pathlib\nimport os\nfrom os.path import isfile, join\nmypath = \"..\/input\/\"\nprint(os.listdir(mypath))\n\n\n# Any results you write to the current directory are saved as output.","5b6df540":"def getCSV(path, ending):\n    csvfiles = [os.path.join(root, name)\n             for root, dirs, files in os.walk(path)\n             for name in files\n             if name.endswith((ending))]\n    return(csvfiles)","4731e577":"# exctract the endings to get the projektnames\n# this will be used in dictionary\ndef get_dict(input_csv_files):\n    ending = {s.rstrip('0123456789').lower() : files for files in input_csv_files for c in files.columns for s in [c[c.rfind('_')+1:]] if len(s.rstrip('0123456789'))>3 }\n    return ending","face94e3":"missing_values = [\"nan\", np.nan] \ndf_data = [pd.read_csv(f, sep=';', decimal=\",\", na_values = missing_values) for f in getCSV(mypath,\".csv\")]","d853952a":"# erzeugt ein Dictionary f\u00fcr die Projekte und die Daten\ndata_dict = get_dict(df_data)\nprint(data_dict.keys())\n\nprojekt = 'IMAGEN' \n#projekt = 'FRANCES'\npredict_var = 'C.audit_total_IMAGEN12'\ndata = data_dict[projekt.lower()]\n\ndataval = data.values\nlabels = data.columns","8790ba37":"df=data\n# Get the number of NaN's for each column, discarding those with zero NaN's\nranking = df.loc[:,df.isnull().any()].isnull().sum().sort_values()\n# Turn into %\nx = ranking.values\/len(df)\n\n# Plot bar chart\nindex = np.arange(len(ranking))\nplt.bar(index, x)\nplt.xlabel('Features')\nplt.ylabel('% NaN observations')\nplt.title('% of null data points for each feature of project '+projekt)\nplt.show()\n\n\nprint('\\nFeatures:',ranking.index.tolist())\n\ndata.info()\n\nprint('\\nData types:',df.dtypes.unique())\nprint('\\nNumber of columns which have any NaN:',df.isnull().any().sum(),'\/',len(df.columns))\nprint('\\nNumber of rows which have any NaN:',df.isnull().any(axis=1).sum(),'\/',len(df))","29f68757":"n_nans = 5\nminfreq = 0.5\n\n# only numeric_data\ndata_numeric = data.select_dtypes(exclude=['object'])\nfreq = (data_numeric.isnull().sum())\/data_numeric.shape[0]\n\n#reduced_data = data_numeric.loc[:,data_numeric.isnull().sum() < n_nans]\nreduced_data = data_numeric.loc[:,freq < minfreq]","99419021":"df=reduced_data\ndf.info()\n\nprint('\\nNumber of columns which have any NaN:',df.isnull().any().sum(),'\/',len(df.columns))\nprint('\\nNumber of rows which have any NaN:',df.isnull().any(axis=1).sum(),'\/',len(df))","e02d7138":"from sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer(strategy = 'mean')\nimpute_data = reduced_data\ndata_with_imputed_values_df = pd.DataFrame(my_imputer.fit_transform(impute_data))\ndata_with_imputed_values_df.columns = impute_data.columns","b8317a68":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\ntarget_data = data_with_imputed_values_df\n\n\n# Labels are the values we want to predict\nlabels = np.array(target_data[predict_var])\n# Remove the labels from the features\n# axis 1 refers to the columns\nfeatures= target_data.drop([predict_var, \"ID\",'C.audit_total_IMAGEN10',\"C.audit_freq_IMAGEN10\",\"C.audit_prob_IMAGEN10\",\"C.audit_abuse_flag_IMAGEN10\",'C.audit_total_IMAGEN11','C.audit_freq_IMAGEN11',\n 'C.audit_abuse_flag_IMAGEN11','C.audit_prob_IMAGEN11','C.audit_symp_IMAGEN11','C.audit_abuse_flag_IMAGEN12',\n 'C.audit_freq_IMAGEN12','C.audit_prob_IMAGEN12','C.audit_symp_IMAGEN12'], axis = 1)\n# Saving feature names for later use\nfeature_df = features\nfeature_list = list(features.columns)\n# Convert to numpy array\nfeatures = np.array(features)\n\n\n\n# z-standardize the features\nnp.set_printoptions(precision=3)\nfeatures = StandardScaler().fit_transform(features)\n#labels=(labels - labels.mean(axis=0))\/labels.std(dtype = np.float64)\n#labels = StandardScaler().fit_transform(labels)\n#scaled_X_df = pd.DataFrame(scaled_X, columns=X_labels)","a318d24d":"train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.2)\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)","a2b644a5":"# Instantiate model with 300 decision trees\nrf = RandomForestRegressor(n_estimators = 300, random_state = 0)\n# Train the model on training data\nrf.fit(train_features, train_labels);","4a0d53ac":"# Get numerical feature importances\nimportances = list(rf.feature_importances_)\nimportances2 = rf.feature_importances_\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 7)) for feature, importance in zip(feature_list, importances)]\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\nindices = np.argsort(rf.feature_importances_)[::-1]\n\n# Print out the feature and importances \n[print('Variable: {:5} - Importance: {:5}'.format(*pair)) for pair in feature_importances]\n\n","f08e20e9":"for a in range(len(feature_importances)):\n    columns = [x[0] for x in feature_importances[:a] if x[1]>= 0.01]\nprint(columns)\nprint(len(columns))","ec874ff5":"import seaborn as sns\nsns.set(style=\"ticks\")\nsns.pairplot(target_data, vars=columns[:5])","80f2f8f4":"from sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import svm\n\n# random forest feature selction \nk_list_rf = []\nfor a in range(5,len(feature_importances)):\n    #columns = [x[0] for x in feature_importances[:a]]\n    k_list_rf.append([feature_df.columns.get_loc(c) for c in columns if c in feature_df])\nk_list_rf \n\n# Instantiate model with 300 decision trees\nrf = RandomForestRegressor(n_estimators = 300, random_state = 0)\nsvr = svm.SVR(kernel='linear') \n# Train the model on training data\n#svr = svm.SVR(kernel='linear') \nfeatures = np.array(feature_df.iloc[:,k_list_rf[0]])\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n\nrf.fit(train_features, train_labels) \nsvr.fit(train_features, train_labels) \n\ny_pred_rf = rf.predict(test_features)\ny_pred_svr = svr.predict(test_features)\n\nMSE_forest = mean_squared_error(test_labels, y_pred_rf)\nMSE_svr = mean_squared_error(test_labels, y_pred_svr)\nprint(\"The mean squared Error for Random Forest on the training set is: %.2f\" % MSE_forest)\nprint(\"The mean squared Error on Vector Regression the training set is: %.2f\" % MSE_svr)\n\n#Let every single estimator in the tree predict total audit\ny_pred_trees = [tree.predict(test_features) for tree in rf.estimators_]\n\n\n#calculate the standard deviation of the audit\nstd_audit = np.std(y_pred_trees,axis=0)\n","15ea01b6":"# Plot predicted vs real audit on the training set\n\nplt.figure()\nplt.title(\"Predicted vs. Real Audit on Sample \\n (standard deviation in 300 trees)\")\nplt.xlabel(\"Actual Audit\")\nplt.ylabel(\"mean predicted Audit\")\nplt.errorbar(x=test_labels,y=y_pred_rf,yerr=std_audit, ecolor='y', fmt='b.')\nplt.plot(range(0, 20), range(0, 20))\nplt.show()","c862fb19":"'''from sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import svm\n\n\n# random forest feature selction \nk_list_rf = []\nfor a in range(5,len(feature_importances)):\n    columns = [x[0] for x in feature_importances[:a]]\n    k_list_rf.append([feature_df.columns.get_loc(c) for c in columns if c in feature_df])\n\n\n# random feature selction  \nk_list_random = []\nwhile len(k_list_random) < len(k_list_rf[-1]):\n    number = np.random.randint(1, len(reduced_data.columns))\n    try:\n        feature_df.iloc[:,[number]]\n        k_list_random.append(number)\n        s = set(k_list_random)\n        k_list_random=[*s]\n    except:\n        pass\n\n##########################################################################################################################\n##########################################################################################################################\n\nsvr = svm.SVR(kernel='linear') \ndic_rf=[]\nfor a in range(25):\n    features = np.array(feature_df.iloc[:,k_list_rf[a]])\n    train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\n    svr.fit(train_features, train_labels) \n    y_pred_svr = svr.predict(test_features)\n    err = mean_squared_error(test_labels, y_pred_svr)\n    dic_rf.append(err)\n\ndic_random=[]\nfor b in range(5,30):\n    features2 = np.array(feature_df.iloc[:,k_list_random[:b]])\n    train_features, test_features, train_labels, test_labels = train_test_split(features2, labels, test_size = 0.25, random_state = 42)\n    svr.fit(train_features, train_labels) \n    y_pred_svr = svr.predict(test_features)\n    err = mean_squared_error(test_labels, y_pred_svr)\n    dic_random.append(err)\n    \n##########################################################################################################################\n##########################################################################################################################\n\n\nprint ('Random Forest:', dic_rf)    \nprint ('\\nRandom:', dic_random)  \n    \nindex_rf = np.arange(len(dic_rf))\nindex_random = np.arange(len(dic_random))\nplt.plot(index_rf, dic_rf, color='g', label='random Forest')\nplt.plot(index_random, dic_random, color='b',label='random Selection')\nplt.xlabel('k')\nplt.legend(loc='upper right')\nplt.ylabel('mean_squared_error')\nplt.title('RandomForrest vs. radom feature selection')\nplt.show()\n\nnsample = 40\nnpreds = len(reduced_data.columns)\n#ind = sorted(np.random.randint(2, npreds, size=(1, nsample)).tolist()[0])\nind=[ 3,9, 41, 44,  45,  51,  61,  76, 78,  138]'''\n\n\n'''X_df = reduced_data.loc[:,k_list]\nX = X_df.values\nX_labels = X_df.columns'''","cba7c6a4":"from sklearn import datasets, linear_model\nfrom sklearn.model_selection import train_test_split\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import SVR\n#y_pred_svr = svr.predict(X_test)\n\nfrom sklearn.metrics import mean_absolute_error\n#print(\"The Mean Absolute Error on the training sample: %f\" % mean_absolute_error(y_test, y_pred_svr))\n\n\nX=features\ny=labels\nscores = []\nscores2 = []\nbest_svr = SVR(kernel='rbf')\ncv = KFold(n_splits=10, random_state=42, shuffle=False)\nfor train_index, test_index in cv.split(X):\n    print(\"Train Index: \", train_index, \"\\n\")\n    print(\"Test Index: \", test_index)\n\n    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n    best_svr.fit(X_train, y_train)\n    scores.append(best_svr.score(X_test, y_test))\n    scores2.append(round(mean_absolute_error(y_test, best_svr.predict(X_test)),2))\n\nprint(scores)\nprint(scores2)\n\n\n\n#X = scaled_X_df\n#y = data_target_df.values\n\n\n# create training and testing vars\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.6)\n#print( X_train.shape, y_train.shape)\n#print( X_test.shape, y_test.shape)","a51e5c30":"train_features =  X_train\ntrain_labels = y_train\ntest_features =  X_test\ntest_labels = y_test\n\nprint('Training Features Shape:', train_features.shape)\nprint('Training Labels Shape:', train_labels.shape)\nprint('Testing Features Shape:', test_features.shape)\nprint('Testing Labels Shape:', test_labels.shape)","4facff71":"from sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import svm\n\nsvr = svm.SVR(kernel='linear')\nsvr.fit(X_train, y_train) \ny_pred_svr = svr.predict(X_test)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"The Mean Absolute Error on the training sample: %f\" % mean_absolute_error(y_test, y_pred_svr))\n\nregr = linear_model.LinearRegression()\nregr.fit(X_train, y_train)\ny_pred_regr = regr.predict(X_test)\n\nprint(\"The Mean Absolute Error on the training sample: %f\" % mean_absolute_error(y_test, y_pred_regr))\n\n# Make predictions using the testing set\n\n\n'''# The coefficients\nprint('Coefficients: \\n', regr.coef_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\"\n      % mean_squared_error(y_test, y_pred))\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % r2_score(y_test, y_pred))'''\n\n\npredictions = rf.predict(test_features)\n# Calculate the absolute errors\nerrors = abs(y_pred - y_test)\n\nprint(errors)\n# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2))\n\n\n# Plot outputs\nplt.scatter(X_test[:,0], y_test,  color='black')\n#plt.plot(X_test, y_pred_svr, color='blue', linewidth=1)\nplt.xlabel(\"Values\")\nplt.ylabel(\"Predictions\")\n\nplt.show()\n","54b63f29":"# fit a model\nlm = linear_model.LinearRegression()\nmodel = lm.fit(X_train, y_train)\npredictions = lm.predict(X_test)\n\n#print(cross_val_score(lm, X_train, y_train, scoring='r2', cv = 10))","3fbc5061":"predictions","357f2509":"## The line \/ model\nplt.scatter(y_test, predictions)\nplt.xlabel(\"Values\")\nplt.ylabel(\"Predictions\")","0884efa4":"<span style=\"color:red\">**MATLAB - Code_Step1_update**<\/span>\n```\n%4) actual regression\n%--------------------------------------------------------------------------\n%GLM\nn=nvps-nout;\nnpredictors=size(Xtraining,2);\nXX(:,1)=ones(n,1);\nXX(:,2:npredictors+1)=Xtraining;\n%XX(:,2:npredictors+1)=Xtraining.^2;\n\n\nb=(XX'*XX)^(-1)*XX'*ytrain;\n\n%in-sample prediction\nyfit_in=XX*b;\n\n%out-of-sample prediction\nclear XX \nn=nout;\nXX(:,1)=ones(n,1);\nXX(:,2:npredictors+1)=Xtest;\n%XX(:,2:npredictors+1)=Xtest.^2; %quadratische Effekte\n\n\nyfit_out=XX*b; %use regression weights gained by training set for prediction in test set!!!\n```","548c3461":"2) dimensionality reduction?\n\n```Matlab\n%2) dimensionality reduction?\n%--------------------------------------------------------------------------\n%z.B. PCA, t-SNE\n%random forest regression\n%variable selection by GLM (see example)\n```","26f975e1":"<span style=\"color:red\">**MATLAB - Code_Step1_update**<\/span>\n\n```Metlab\n%3) split training and test set (repeat)\n%--------------------------------------------------------------------------\nK=10; %Anzahl von cross-validierungs sets\nnout=ceil(nvps\/K);  %# von VPs die ich rausnehm (f\u00fcr test-set)\nindizes=1:nout;     %test set indizes\n\nXtest=X(indizes,:); %VPs zum testen\nXtraining=X(indizes(end)+1:end,:); %VPs zum trainieren\n\nytest=y(indizes);\nytrain=y(indizes(end)+1:end);\n```\n\n___","89495458":"<span style=\"color:red\">**MATLAB - Code_Step1_update**<\/span>\n\n```\n%plot results\n%--------------------------------------------------------------------------\nh1=figure; hold on; box on; lw=2;\nsubplot(1,2,1); hold on; box on\nl1=plot(ytrain,'r','LineWidth',lw);\nl2=plot(yfit_in,'-.b','LineWidth',lw);\ntitle('training')\nlegend([l1 l2],{'y-train','y-train-predicted'});\nylabel(ylab)\n\nsubplot(1,2,2); hold on; box on\nl1=plot(ytest,'r','LineWidth',lw);\nl2=plot(yfit_out,'-.b','LineWidth',lw);\ntitle('test')\nlegend([l1 l2],{'y-test','y-test-predicted'});\nylabel(ylab)\n\nset(h1,'Position',[400 400 900 300])\n\nkeyboard\n```","3de3752c":"# ML f\u00fcr Imagen","079672b6":"5) sort betas by T-Values","7e59cb8a":"## Load data file","64d36b1c":"4) actual regression","aca6dcde":"<span style=\"color:green\">**MATLAB - Code_Step1_update**<\/span>\n```Matlab\n%find to be predicted outcome y\n%--------------------------------------------------------------------------\nylab='audittotal';\nind=strfind(x.labels,ylab);\nfor i=1:length(ind)\n   if ~isempty(ind{i})\n       yind=i;\n   end\nend\ny=data(:,yind);      %vorherzusagender Outcome\nlabels{yind}=[];     %entferne aus den labels\ndata(:,yind)=[];     %entferne aus Daten\n````\n","744b30ae":"## plot results","e6877780":"3) split training and test set (repeat)","42d5771a":"## Exclude VPs with many nan entries ","d34941f7":"<span style=\"color:green\">**MATLAB - Code_Step1_update**<\/span>\n```Matlab\nnpreds=size(data,2); nsample=40;\nind=randsample(2:npreds,nsample);\n\n%ind=[ 3   9   41 44  45  51  61  76 78   138]; \n\n%@Karl: diese Pr\u00e4diktoren funktionieren beispielsweise sehr gut, \n%wohl aber trivial weil da 41=Cauditprob dabei ist, wo ich annehm das ist quasi das gleiche wie y\n%d\u00fcrfte man wohl nicht zu Vorhersage gebrauchen, aber so als Beispiel\n\nvars_used_for_prediction=ind;%size(data,2);\nX=data(:,vars_used_for_prediction);\nlabels=labels(vars_used_for_prediction);    %nimm nur die, die auch genutzt werden\n\n```","42578673":"***1) interpolate?***","407ae937":"## Find to be predicted outcome y","9801ef00":"<span style=\"color:green\">**MATLAB - Code_Step1_update**<\/span>\n```Matlab\nn_nans=50;\ntmp=isnan(data);\noutlier= find(sum(tmp')>n_nans);\ndata(outlier,:)=[];\n\ndata(isnan(data))=-1; %set other nans to  -1\nnvps=size(data,1);\n```","0dbf1bc2":"## Random Forest Feature Selection","724165f3":"## specify which predictors to use","16d6aa23":"2a) standardize\n\n```Matlab\n%2.a) standardize\n%--------------------------------------------------------------------------\n%z-transfo\nX=(X-mean(X)).\/std(X);\ny=(y-mean(y)).\/std(y);\n```","044c0acf":"<span style=\"color:red\">**MATLAB - Code_Step1_update**<\/span>\n```\n%sort betas by T-Values\n%--------------------------------------------------------------------------\n[n,p]=size(XX); \nv=(XX'*XX)^-1;\nsigma=((ytest-XX*b)'*(ytest-XX*b))\/(n-p-1);\n\nfor ibeta=2:length(b) %start at 2, and drop b_0\n    beta=b(ibeta);\n    vii=v(ibeta,ibeta);\n    \n    t=beta\/(sigma*sqrt(vii));\n    T(ibeta-1)=t;\n    %significance test\n    %dof=n-length(b)-1;\n    %p=2*(1-tcdf(abs(t),dof));\n    %txt=sprintf('b_%d=%2.3f : T(%d)=%2.3f, p=%2.3f',ibeta,beta,dof,t,p);\n    %disp(txt);\nend\n[val,ind]=sort(abs(T),'descend');\n\nfigure('color','white'); hold on; box on; lw=2;\nx=1:length(T);\nplot(x,val,'LineWidth',lw)\nset(gca,'XTick',x,'XTickLabel',labels(ind))\nxtickangle(45)\nylabel('T-value of \\beta')\nkeyboard\n```"}}