{"cell_type":{"7e13685c":"code","4098b58f":"code","a1aeac0a":"code","14b0a875":"code","eb8ffce1":"code","295911b1":"code","3492ed7d":"code","6afe9601":"code","1a81a078":"code","e48d6c39":"code","11719710":"code","bfbf1c73":"code","ce5a8246":"code","ff0e8d74":"code","5231ca5b":"code","16e2eccb":"code","788ae14b":"code","671ef1dc":"code","17be4efc":"code","7db4850d":"code","f4672c5d":"code","3154cb47":"code","6f5c7e75":"code","5dcbd490":"code","5307d969":"code","5ccf4669":"code","6323b17f":"code","d39beeb2":"code","b56a994d":"code","95664924":"code","52ca35d2":"code","1baf5342":"code","f11358bb":"code","37a2aa02":"code","0eda1e49":"code","74cc9e4e":"code","a961e3c2":"code","559945bd":"code","b51a90d3":"code","cef8227e":"code","3b45efcc":"code","81a6061b":"code","4a32f7af":"code","0dfc7d65":"code","98be7e59":"code","63ff267c":"code","4926d72d":"code","ccf1aedc":"code","43d3d2b7":"code","3fbdf791":"code","c451a4cf":"code","60207058":"code","2b365c89":"code","5afd39f9":"code","ef6b4ea9":"code","9989d0ff":"code","ce252cfd":"code","64ecc891":"code","00fb0344":"code","a24943b6":"code","eb1059ce":"code","55baf5fc":"code","b6065d39":"code","f7038425":"code","9cf5dc5b":"code","5bcb114a":"code","5d2476d3":"code","0b0b28eb":"code","a981a7b1":"code","a0a4e72c":"code","dad344ed":"code","bdb36f78":"code","5fbac331":"code","56abb712":"code","9560f58a":"code","6225cd06":"code","3b7dbaca":"code","d0f6cf17":"code","ae72a7c9":"code","da1c600c":"code","718dcba8":"code","ec45062b":"code","946350b5":"code","760d3539":"code","d86620a3":"markdown","53dc4078":"markdown","b164a1f8":"markdown","87548fa1":"markdown","71442bf9":"markdown","e9056c57":"markdown","fdaf0924":"markdown","388fd330":"markdown"},"source":{"7e13685c":"#import the necessary modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as ss\nimport folium as fl\nfrom folium.plugins import HeatMap\nfrom folium.plugins import PolyLineTextPath\n\n%matplotlib inline\nsns.set(rc = {'figure.figsize': (20,8)}, style = 'whitegrid', palette = 'Set2')","4098b58f":"# load airbnb dataset\ndf = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\ndf.head()","a1aeac0a":"df.shape","14b0a875":"# explore the 'calculated_host_listings_count' column\ndf.loc[(df['host_name'] == 'John') & (df['host_id'] == 2787)]\n\n## indicates that how many different housings\/options do each host have available","eb8ffce1":"# explore the categories for 'neighbourhood_group'\ndf.neighbourhood_group.value_counts()","295911b1":"plt.figure(figsize=(10,10))\nax = sns.countplot(df[\"neighbourhood_group\"])","3492ed7d":"# explore the categories for 'neighbourhood'\ndf.neighbourhood.value_counts()","6afe9601":"# explore the categories for 'room_type'\n#2.Which type of room is the most\/least common in NYC?\ndf.room_type.value_counts()","1a81a078":"plt.figure(figsize=(10,10))\nax = sns.countplot(df['neighbourhood_group'],hue=df['room_type'])","e48d6c39":"# remove the first two columns for now as it is not needed\ndf.drop(columns = ['id', 'name'], axis = 1, inplace = True)\ndf.rename(columns = {\n    'calculated_host_listings_count': 'host_listings_count',\n    'number_of_reviews': 'reviews_count'\n}, inplace = True)\ndf.head()","11719710":"# check for duplicate rows\ndf.duplicated().value_counts()\n\n## no duplicate rows","bfbf1c73":"df.info()","ce5a8246":"# convert the date field\ndf['last_review'] = pd.to_datetime(df['last_review'], format = '%Y-%m-%d')\ndf.info()","ff0e8d74":"#inspect the summary of numerical columns\ndf.describe().round(2)\n\n##need to look further into max of 'minimum_nights' and min of 'availability_365'","5231ca5b":"#check for null values\ndf.isna().sum()","16e2eccb":"#\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nplt.subplots(figsize=(25,15))\nwordcloud = WordCloud(\n                          background_color='white',\n                          width=1920,\n                          height=1080\n                         ).generate(\" \".join(df.neighbourhood))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.savefig('neighbourhood.png')\nplt.show()","788ae14b":"#price\nnum_nyc_airbnb=df._get_numeric_data()\n#Brooklyn\nsub_1=df.loc[(df['neighbourhood_group'] == 'Brooklyn') & (df['room_type']=='Shared room')]\nprice_sub1=num_nyc_airbnb['price'].iloc[sub_1.index]# prices for Neighbourhood group having Brooklyn\n\n\n#Bronx\nsub_2=df.loc[(df['neighbourhood_group'] == 'Bronx') & (df['room_type']=='Shared room')]\nprice_sub2=num_nyc_airbnb['price'].iloc[sub_2.index]# prices for Neighbourhood group having Bronx\n\n#Staten Island\nsub_3=df.loc[(df['neighbourhood_group'] == 'Staten Island') & (df['room_type']=='Shared room')]\nprice_sub3=num_nyc_airbnb['price'].iloc[sub_3.index]# prices for Neighbourhood group having Staten Island\n\n#Queens\nsub_4=df.loc[(df['neighbourhood_group'] == 'Queens') & (df['room_type']=='Shared room')]\nprice_sub4=num_nyc_airbnb['price'].iloc[sub_4.index]# prices for Neighbourhood group having Queens\n\n\n#Manhattan\nsub_5=df.loc[(df['neighbourhood_group'] == 'Manhattan') & (df['room_type']=='Shared room')]\nprice_sub5=num_nyc_airbnb['price'].iloc[sub_5.index]# prices for Neighbourhood group having Manhattan\n\n\npercentile_price_brooklyn=[]#percentile of prices for Private rooms near brooklyn\n\npercentile_price_Bronx=[]#percentile of prices for Private rooms near Bronx\n\npercentile_price_Staten_Island=[] #percentile of prices for Private rooms near Staten Island\n\npercentile_price_Queens=[] #percentile of prices for accomodations near Queens\n\npercentile_price_Manhattan=[] #percentile of prices for accomodations near Manhattan\n\n\npercentiles=[]# percentiles\nfor i in range(19,91):\n    percentile_price_brooklyn.append(int(price_sub1.quantile(i\/100)))\n    percentile_price_Bronx.append(int(price_sub2.quantile(i\/100)))\n    percentile_price_Staten_Island.append(int(price_sub3.quantile(i\/100)))\n    percentile_price_Queens.append(int(price_sub4.quantile(i\/100)))\n    percentile_price_Manhattan.append(int(price_sub5.quantile(i\/100)))\n    percentiles.append(i)\n\nplt.figure(figsize=(10,10))\nplt.title('Prices of 90% Shared Rooms',fontsize=10,color='Red')\n\n\n\n# for i in range(2):\n\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_brooklyn),label='Brooklyn')\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_Bronx),label='Bronx')\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_Staten_Island),label='Staten Island')\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_Queens),label='Queens')\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_Manhattan),label='Manhattan')\n\nsd.set(xlabel='Percentiles', ylabel='Percentile Prices in U.S $')","671ef1dc":"#price\nnum_nyc_airbnb=df._get_numeric_data()\n#Brooklyn\nsub_1=df.loc[(df['neighbourhood_group'] == 'Brooklyn') & (df['room_type']=='Private room')]\nprice_sub1=num_nyc_airbnb['price'].iloc[sub_1.index]# prices for Neighbourhood group having Brooklyn\n\n\n#Bronx\nsub_2=df.loc[(df['neighbourhood_group'] == 'Bronx') & (df['room_type']=='Private room')]\nprice_sub2=num_nyc_airbnb['price'].iloc[sub_2.index]# prices for Neighbourhood group having Bronx\n\n#Staten Island\nsub_3=df.loc[(df['neighbourhood_group'] == 'Staten Island') & (df['room_type']=='Private room')]\nprice_sub3=num_nyc_airbnb['price'].iloc[sub_3.index]# prices for Neighbourhood group having Staten Island\n\n#Queens\nsub_4=df.loc[(df['neighbourhood_group'] == 'Queens') & (df['room_type']=='Private room')]\nprice_sub4=num_nyc_airbnb['price'].iloc[sub_4.index]# prices for Neighbourhood group having Queens\n\n\n#Manhattan\nsub_5=df.loc[(df['neighbourhood_group'] == 'Manhattan') & (df['room_type']=='Private room')]\nprice_sub5=num_nyc_airbnb['price'].iloc[sub_5.index]# prices for Neighbourhood group having Manhattan\n\n\npercentile_price_brooklyn=[]#percentile of prices for Private rooms near brooklyn\n\npercentile_price_Bronx=[]#percentile of prices for Private rooms near Bronx\n\npercentile_price_Staten_Island=[] #percentile of prices for Private rooms near Staten Island\n\npercentile_price_Queens=[] #percentile of prices for accomodations near Queens\n\npercentile_price_Manhattan=[] #percentile of prices for accomodations near Manhattan\n\n\npercentiles=[]# percentiles\nfor i in range(19,91):\n    percentile_price_brooklyn.append(int(price_sub1.quantile(i\/100)))\n    percentile_price_Bronx.append(int(price_sub2.quantile(i\/100)))\n    percentile_price_Staten_Island.append(int(price_sub3.quantile(i\/100)))\n    percentile_price_Queens.append(int(price_sub4.quantile(i\/100)))\n    percentile_price_Manhattan.append(int(price_sub5.quantile(i\/100)))\n    percentiles.append(i)\n\nplt.figure(figsize=(10,10))\nplt.title('Prices of 90% Private Rooms',fontsize=10,color='Red')\n\n\n\n# for i in range(2):\n\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_brooklyn),label='Brooklyn')\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_Bronx),label='Bronx')\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_Staten_Island),label='Staten Island')\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_Queens),label='Queens')\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_Manhattan),label='Manhattan')\n\nsd.set(xlabel='Percentiles', ylabel='Percentile Prices in U.S $')","17be4efc":"#price\nnum_nyc_airbnb=df._get_numeric_data()\n#Brooklyn\nsub_1=df.loc[(df['neighbourhood_group'] == 'Brooklyn') & (df['room_type']=='Entire home\/apt')]\nprice_sub1=num_nyc_airbnb['price'].iloc[sub_1.index]# prices for Neighbourhood group having Brooklyn\n\n\n#Bronx\nsub_2=df.loc[(df['neighbourhood_group'] == 'Bronx') & (df['room_type']=='Entire home\/apt')]\nprice_sub2=num_nyc_airbnb['price'].iloc[sub_2.index]# prices for Neighbourhood group having Bronx\n\n#Staten Island\nsub_3=df.loc[(df['neighbourhood_group'] == 'Staten Island') & (df['room_type']=='Entire home\/apt')]\nprice_sub3=num_nyc_airbnb['price'].iloc[sub_3.index]# prices for Neighbourhood group having Staten Island\n\n#Queens\nsub_4=df.loc[(df['neighbourhood_group'] == 'Queens') & (df['room_type']=='Entire home\/apt')]\nprice_sub4=num_nyc_airbnb['price'].iloc[sub_4.index]# prices for Neighbourhood group having Queens\n\n\n#Manhattan\nsub_5=df.loc[(df['neighbourhood_group'] == 'Manhattan') & (df['room_type']=='Entire home\/apt')]\nprice_sub5=num_nyc_airbnb['price'].iloc[sub_5.index]# prices for Neighbourhood group having Manhattan\n\n\npercentile_price_brooklyn=[]#percentile of prices for Private rooms near brooklyn\n\npercentile_price_Bronx=[]#percentile of prices for Private rooms near Bronx\n\npercentile_price_Staten_Island=[] #percentile of prices for Private rooms near Staten Island\n\npercentile_price_Queens=[] #percentile of prices for accomodations near Queens\n\npercentile_price_Manhattan=[] #percentile of prices for accomodations near Manhattan\n\n\npercentiles=[]# percentiles\nfor i in range(19,91):\n    percentile_price_brooklyn.append(int(price_sub1.quantile(i\/100)))\n    percentile_price_Bronx.append(int(price_sub2.quantile(i\/100)))\n    percentile_price_Staten_Island.append(int(price_sub3.quantile(i\/100)))\n    percentile_price_Queens.append(int(price_sub4.quantile(i\/100)))\n    percentile_price_Manhattan.append(int(price_sub5.quantile(i\/100)))\n    percentiles.append(i)\n\nplt.figure(figsize=(10,10))\nplt.title('Prices of 90% Entire home\/apt',fontsize=10,color='Red')\n\n\n\n# for i in range(2):\n\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_brooklyn),label='Brooklyn')\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_Bronx),label='Bronx')\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_Staten_Island),label='Staten Island')\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_Queens),label='Queens')\nsd=sns.lineplot(x=pd.Series(percentiles),y=pd.Series(percentile_price_Manhattan),label='Manhattan')\n\nsd.set(xlabel='Percentiles', ylabel='Percentile Prices in U.S $')","7db4850d":"#VIS\u2b06","f4672c5d":"# check if we can impute the missing host_name values from their other listings\ndf.loc[df['host_id'].isin(df[df['host_name'].isna()].host_id)]\n\n## seems like those who have multiple listings, still have the host_name missing for those too. We assume its for privacy. 'host_id' column can be used instead for identification","3154cb47":"# drop 'host_name' column as we can use 'host_id' for identification\/uniqueness\n\ndf.drop(columns = 'host_name', axis = 1, inplace = True)","6f5c7e75":"# investigate on null values for 'last_review' & 'reviews_per_month'\ndf[(df['last_review'].isna()) | (df['reviews_per_month'].isna())]\n\n## we can see that the rows that have null for 'last_review', also has null for 'reviews_per_month'","5dcbd490":"# check if all these entries have 0 reviews\ndf[(df['last_review'].isna()) | (df['reviews_per_month'].isna())].reviews_count.value_counts()\n\n## reason for having NA, is because these listings never got a single review yet","5307d969":"# for NA 'reviews_per_month', we can impute it with '0' as these listings didnt get any reviews yet\ndf.reviews_per_month.fillna(value = 0, inplace = True)","5ccf4669":"df.last_review.min()","6323b17f":"df.last_review.max()","d39beeb2":"# to deal with NA in 'last_review', we change the column to a categorical variable (by year), and those that have NA will be categorised as 'Never', as they did not get any reviews yet\ndf.last_review = df.last_review.dt.year.astype('object')\ndf.last_review.fillna(value = 'Never', inplace = True)","b56a994d":"# check 'last_review' categories\ndf.last_review.value_counts()","95664924":"# confirm all null values are dealt with\ndf.isna().sum()","52ca35d2":"df.head()","1baf5342":"df.describe()\n## need to look further into max of 'minimum_nights' and min of 'availability_365'","f11358bb":"df.availability_365.value_counts()\n## 0 days availability can possibly refer to the listing not being available at the moment, so we can keep this data","37a2aa02":"#check values for 'minimum_nights'\ndf.minimum_nights.value_counts()","0eda1e49":"plt.figure(figsize = (25, 8))\nsns.boxplot(df['minimum_nights'])\n\n## can see some extreme outliers","74cc9e4e":"#check how many listings have minimum nights set to more than a year\ndf.loc[df['minimum_nights'] > 365]","a961e3c2":"# Given that having minimum nights as more than 365 days is extremely rare and could be invalid at times as well, we can remove them. Furthermore, its only a few entries, so it should not affect our data.\n# Therefore, we will only consider listings that have minimum nights set to a year or less.\ndf = df.loc[df['minimum_nights'] <= 365]\n\n# resetting the count for listings, as a few rows were removed\ndf['host_listings_count'] = df.groupby('host_id')['host_id'].transform('count')\n# rest index as well\ndf.reset_index(inplace = True)","559945bd":"# extreme outliers have been removed, while the moderate outliers are kept, as they are valid but uncommon cases\nplt.figure(figsize = (30, 8))\nsns.boxplot(df['minimum_nights'])","b51a90d3":"# now things look better\ndf.describe().round(2)","cef8227e":"#vis \u2b06","3b45efcc":"#model\u2b07","81a6061b":"df.minimum_nights.value_counts().sort_values(ascending = False).head(10)","4a32f7af":"# get correlation matrix for numerical columns\ndf_corr = df[['price', 'minimum_nights', 'reviews_count', 'reviews_per_month', 'host_listings_count', 'availability_365']].corr()\n\n# draw heatmap\nsns.heatmap(df_corr, annot = True, annot_kws = {'size': 15}, square = True, cmap = 'YlGnBu')\n\n## from this observation, we can conclude that most of the numerical features do not really have strong correlations with each other directly, apart from review count & rate.","0dfc7d65":"df.head()","98be7e59":"# create function so that it can be reused as a base for map\n# visualize the amount of listing clusters in NYC\n\ndef draw_map():\n    nyc = [df.latitude.mean(), df.longitude.mean()]\n    boroughs = df.groupby('neighbourhood_group').mean().reset_index()\n    boroughs_name = boroughs['neighbourhood_group'].values.tolist()\n    boroughs_loc = boroughs[['latitude', 'longitude']].values.tolist()\n\n    base_map = fl.Map(location = nyc, control_scale = True, zoom_start = 11, tiles = 'OpenStreetMap')\n\n    for i in range(len(boroughs_name)):\n        attr = {'fill': 'midnightblue', 'font-weight': 'bold', 'font-size': '20'}\n        pl = fl.PolyLine([[boroughs_loc[i][0], boroughs_loc[i][1]-.1], [boroughs_loc[i][0], boroughs_loc[i][1]+.1]], weight = 15, color = 'rgb(255,255,255, 0)')\n        base_map.add_child(pl)\n        base_map.add_child(PolyLineTextPath(pl, text = boroughs_name[i], attributes = attr, center = True))\n    \n    return base_map\n\nbase_map = draw_map()    \nbase_map.add_child(HeatMap(data = df[['latitude', 'longitude']], min_opacity = 0.2, radius = 15, blur = 40))\nbase_map.add_child(fl.ClickForMarker(popup='High amount of listings'))\n\n## Manhattan and Brooklyn have the most amount of listing options available","63ff267c":"# view the cluster of room types available\n# folium visual map is not used as the cluster colours blend, preventing from differentiating the room types\n\nplt.figure(figsize = (20, 15))\nsns.scatterplot(x = 'longitude', y = 'latitude', data = df, hue = 'room_type', style = 'room_type')\n\n## There are very few shared rooms available compared to private rooms & entire homes","4926d72d":"# check if 'review_counts' have many outliers\nsns.boxplot(x = 'neighbourhood_group', y = 'reviews_count', data = df)\n\n## has many outliers","ccf1aedc":"# since 'review_counts' have many outliers, we should use median instead of mean as the measure of central tendency.\n\ntop_listings = df.groupby('neighbourhood_group').median().sort_values(by = 'reviews_count', ascending = False).reset_index()\ntop10_listings = df.groupby(['neighbourhood_group', 'neighbourhood']).median().sort_values(by = 'reviews_count', ascending = False).reset_index().head(10)\n\nfig, axis = plt.subplots(1, 2, figsize = (25, 8), squeeze = True)\nfig.tight_layout(pad = 5)\nsns.barplot(x = 'reviews_count', y = 'neighbourhood', data = top10_listings, orient = 'h', ci = False, hue = 'neighbourhood_group', dodge = False, ax = axis[0], estimator = np.median).set(title = 'Top rented listings according to neighbourhood')\nsns.barplot(x = 'neighbourhood_group', y = 'reviews_count', data = top_listings, ci = False, dodge = False, ax = axis[1]).set(title = 'Average number of customers that hosts get in each borough')\nplt.show()\n\n## From the below observations, we can conclude that hosts in Staten Island are the most busy, in terms of getting customers\n## Silver Lake, Eltingville and Richmondtown seems to be the top 3 hotspots for hosts to get many customers","43d3d2b7":"# Investigate which hosts in Manhattan & Brooklyn have the most customers as they have the highest amount of listing options\n\ntop_listings = df.loc[(df.neighbourhood_group == 'Manhattan') | (df.neighbourhood_group == 'Brooklyn')].groupby('neighbourhood_group').median().sort_values(by = 'reviews_count', ascending = False).reset_index()\ntop10_listings = df.loc[(df.neighbourhood_group == 'Manhattan') | (df.neighbourhood_group == 'Brooklyn')].groupby(['neighbourhood_group', 'neighbourhood']).median().sort_values(by = 'reviews_count', ascending = False).reset_index().head(10)\n\nfig, axis = plt.subplots(1, 2, figsize = (25, 8), squeeze = True)\nfig.tight_layout(pad = 5)\nsns.barplot(x = 'reviews_count', y = 'neighbourhood', data = top10_listings, orient = 'h', ci = False, hue = 'neighbourhood_group', dodge = False, ax = axis[0], estimator = np.median).set(title = 'Top rented listings according to neighbourhood')\nsns.barplot(x = 'neighbourhood_group', y = 'reviews_count', data = top_listings, ci = False, dodge = False, ax = axis[1]).set(title = 'Average number of customers that hosts get in each borough')\nplt.show()\n\n# Brooklyn hosts tops this one, with Manhattan Beach, Dyker Heights & Bergen Beach being the common locations","3fbdf791":"plt.figure(figsize = (30, 8))\nsns.boxplot(df['price'])","c451a4cf":"# inspect the price differences for each borough\navg_roomtype_cost = df.groupby('room_type').price.median()\ntop_price = df.groupby(['neighbourhood_group', 'room_type']).median().sort_values(by = 'price', ascending = False).reset_index()\n\ng = sns.catplot(x = 'neighbourhood_group', y = 'price', data = top_price, ci = False, estimator = np.median, kind = 'point', col = 'room_type')\n\nfor i in range(len(avg_roomtype_cost)):\n    g.axes[0][i].axhline(avg_roomtype_cost[i], color = 'midnightblue')\n\nplt.show()\n\n# as we can see, Manhattan has the most expensive listing offers for all room_types\n# on the other hand, Staten Island has the cheapest listings. This further reinforces the previous observations we made as to why each Staten Island hosts get alot of customers.","60207058":"# aggregate to view and compare the listings count with reviews count. View rental rate as well.\nhost_count = df.groupby('neighbourhood_group').count().sort_values('index', ascending = False).reset_index()[['neighbourhood_group', 'index']].rename(columns = {'index': 'listings_count'})\ncustomer_count = df.groupby('neighbourhood_group').sum().sort_values('reviews_count', ascending = False).reset_index()[['neighbourhood_group', 'reviews_count']]\nborough_data = host_count.merge(customer_count, left_on = 'neighbourhood_group', right_on = 'neighbourhood_group', how = 'left')\nborough_data['rental_rate'] = (borough_data.reviews_count \/ borough_data.listings_count).round(2)\nborough_data","2b365c89":"# visualise the comparison for listings and reviews\nfig, axis = plt.subplots(1, 3, figsize = (25, 8), squeeze = True)\nfig.tight_layout(pad = 10)\n\nsns.barplot(x = 'listings_count', y = 'neighbourhood_group', data = borough_data, orient = 'h', ci = False, estimator = np.median, ax = axis[0]).set(title = 'Amount of listings')\nsns.barplot(x = 'reviews_count', y = 'neighbourhood_group', data = borough_data, orient = 'h', ci = False, estimator = np.median, ax = axis[1]).set(title = 'Amount of rentals')\nsns.barplot(x = 'rental_rate', y = 'neighbourhood_group', data = borough_data, orient = 'h', ci = False, estimator = np.median, ax = axis[2]).set(title = 'Rental Rate')\nplt.show()\n\n## As seen earlier, Manhattan & Brooklyn has more listings, but as we look at the rental rate. In average, Staten Island & Queens hosts are able to secure customers at a better rate compared to the other boroughs.","5afd39f9":"# inspect the availability of listings according to each borough and room type\n\nsns.catplot(x = 'neighbourhood_group', y = 'availability_365', data = df.loc[df.availability_365 > 0], ci = False, estimator = np.median, kind = 'boxen', col = 'room_type')\n\n## For private rooms, most Staten Island listings have them available throughout the whole year compared to the other boroughs.\n## Entire homes in NYC are generally not available for the whole year, and more towards the 6 months mark.\n## For shared rooms, Brooklyn & Queens have them available through out most of the year, while Staten Island and Bronx have them for only a couple months in a year.","ef6b4ea9":"# inspect the minimum nights for listings in terms of room type and borough\n# consider those that are 30 days or less, since higher is more uncommon in general\nsns.catplot(x = 'neighbourhood_group', y = 'minimum_nights', data = df.loc[df.minimum_nights <= 30], ci = False, estimator = np.median, kind = 'strip', col = 'room_type')\n\n## Staten Island listings tend to have very low minimum night requirements, which means that many vistors do not have to stay for long.","9989d0ff":"# view the trend for reviews per month against price. Ignore rates above 30 as they could potentially be invalid.\n\nsns.scatterplot(x = 'price', y = 'reviews_per_month', data = df.loc[(df.reviews_count > 0) & (df.reviews_per_month <= 30)].sort_values('reviews_count', ascending = False))\n\n\n# exponential trend where extreme high prices have a very low review rate while lower prices have a variety of rates.","ce252cfd":"sns.scatterplot(x = 'price', y = 'reviews_per_month', data = df.loc[(df.reviews_count > 0) & (df.reviews_per_month <= 30)& (df.price<2000)].sort_values('reviews_count', ascending = False))","64ecc891":"# get the list for each group and using sampling for private rooms, to ensure equal sample size\nplt.figure(figsize = (30, 15))\n\nshared_rooms = df.loc[df.room_type == 'Shared room']['reviews_per_month'].values.tolist()\nprivate_rooms = df.loc[df.room_type == 'Private room'].sample(len(shared_rooms), replace = False, random_state = 1)['reviews_per_month'].values.tolist()\n\n# view the distribution for each group\nsns.distplot(shared_rooms)\nsns.distplot(private_rooms)\nplt.xlabel('Reviews per Month')\nplt.legend(['Shared Room', 'Private Room'])\n\nplt.show()\n\n## we can see that both groups have a right skewed distribution, so we can proceed with the test","00fb0344":"# get the list for each group and using sampling for private rooms, to ensure equal sample size\nplt.figure(figsize = (30, 15))\n\nshared_rooms = df.loc[df.room_type == 'Entire home\/apt']['reviews_per_month'].values.tolist()\nprivate_rooms = df.loc[df.room_type == 'Private room'].sample(len(shared_rooms), replace = True, random_state = 1)['reviews_per_month'].values.tolist()\n\n# view the distribution for each group\nsns.distplot(shared_rooms)\nsns.distplot(private_rooms)\nplt.xlabel('Reviews per Month')\nplt.legend(['Entire home\/apt', 'Private Room'])\n\nplt.show()\n\n## we can see that both groups have a right skewed distribution, so we can proceed with the test","a24943b6":"# create general function to get the result for Mann Whitney U Test\ndef mann_whitney_u_test(d1, d2):\n    u_stat, p_val = ss.mannwhitneyu(d1, d2)\n    return u_stat, f'{p_val:.20f}'\n\nmann_whitney_u_test(shared_rooms, private_rooms)\n\n## we can see that the p-value is around 0.108, which is higher than our significance level. So we FAIL to reject the null hypothesis","eb1059ce":"#Onehot encode\nData1 = df.drop(columns=['index','host_id', 'last_review','price'])\nData1.fillna(Data1.mean(), inplace=True)\nData1=pd.get_dummies(Data1)\nData1['price_log'] = np.log(df.price+1)\nData1.info()\nData1.isnull().sum()\n","55baf5fc":"nyc_model_x=Data1.iloc[:,:-1]\nnyc_model_y =Data1.iloc[:,-1]","b6065d39":"import statsmodels\nimport statsmodels.api as sm\nimport scipy.stats as stats\nfrom scipy.stats import norm\nfig, axes = plt.subplots(1,3, figsize=(21,6))\nsns.distplot(df['price'], ax=axes[0])\nsns.distplot(np.log1p(df['price']), ax=axes[1])\naxes[1].set_xlabel('log(1+price)')\nsm.qqplot(np.log1p(df['price']), stats.norm, fit=True, line='45', ax=axes[2]);","f7038425":"print(nyc_model_y)","9cf5dc5b":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nnyc_model_x = scaler.fit_transform(nyc_model_x)","5bcb114a":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(nyc_model_x, nyc_model_y, test_size=0.3,random_state=42)","5d2476d3":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\n\n#feature_model = ExtraTreesClassifier(n_estimators=50)\n#feature_model.fit(X_train,Y_train)\n\n#plt.figure(figsize=(7,7))\n#feat_importances = pd.Series(feature_model.feature_importances_, index=Data1.iloc[:,:-1].columns)\n#feat_importances.nlargest(10).plot(kind='barh')\n#plt.show()\n","0b0b28eb":"def linear_reg(input_x, input_y, cv=5):\n    ## Defining parameters\n    model_LR= LinearRegression()\n\n    parameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\n\n    ## Building Grid Search algorithm with cross-validation and Mean Squared Error score.\n\n    grid_search_LR = GridSearchCV(estimator=model_LR,  \n                         param_grid=parameters,\n                         scoring='neg_mean_squared_error',\n                         cv=cv,\n                         n_jobs=-1)\n\n    ## Lastly, finding the best parameters.\n\n    grid_search_LR.fit(input_x, input_y)\n    best_parameters_LR = grid_search_LR.best_params_  \n    best_score_LR = grid_search_LR.best_score_ \n    print(best_parameters_LR)\n    print(best_score_LR)\n","a981a7b1":"def ridge_reg(input_x, input_y, cv=5):\n    ## Defining parameters\n    model_Ridge= Ridge()\n\n    # prepare a range of alpha values to test\n    alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n    normalizes= ([True,False])\n\n    ## Building Grid Search algorithm with cross-validation and Mean Squared Error score.\n\n    grid_search_Ridge = GridSearchCV(estimator=model_Ridge,  \n                         param_grid=(dict(alpha=alphas, normalize= normalizes)),\n                         scoring='neg_mean_squared_error',\n                         cv=cv,\n                         n_jobs=-1)\n\n    ## Lastly, finding the best parameters.\n\n    grid_search_Ridge.fit(input_x, input_y)\n    best_parameters_Ridge = grid_search_Ridge.best_params_  \n    best_score_Ridge = grid_search_Ridge.best_score_ \n    print(best_parameters_Ridge)\n    print(best_score_Ridge)","a0a4e72c":"def lasso_reg(input_x, input_y, cv=5):\n    ## Defining parameters\n    model_Lasso= Lasso()\n\n    # prepare a range of alpha values to test\n    alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n    normalizes= ([True,False])\n\n    ## Building Grid Search algorithm with cross-validation and Mean Squared Error score.\n\n    grid_search_lasso = GridSearchCV(estimator=model_Lasso,  \n                         param_grid=(dict(alpha=alphas, normalize= normalizes)),\n                         scoring='neg_mean_squared_error',\n                         cv=cv,\n                         n_jobs=-1)\n\n    ## Lastly, finding the best parameters.\n\n    grid_search_lasso.fit(input_x, input_y)\n    best_parameters_lasso = grid_search_lasso.best_params_  \n    best_score_lasso = grid_search_lasso.best_score_ \n    print(best_parameters_lasso)\n    print(best_score_lasso)","dad344ed":"def elastic_reg(input_x, input_y,cv=5):\n    ## Defining parameters\n    model_grid_Elastic= ElasticNet()\n\n    # prepare a range of alpha values to test\n    alphas = np.array([1,0.1,0.01,0.001,0.0001,0])\n    normalizes= ([True,False])\n\n    ## Building Grid Search algorithm with cross-validation and Mean Squared Error score.\n\n    grid_search_elastic = GridSearchCV(estimator=model_grid_Elastic,  \n                         param_grid=(dict(alpha=alphas, normalize= normalizes)),\n                         scoring='neg_mean_squared_error',\n                         cv=cv,\n                         n_jobs=-1)\n\n    ## Lastly, finding the best parameters.\n\n    grid_search_elastic.fit(input_x, input_y)\n    best_parameters_elastic = grid_search_elastic.best_params_  \n    best_score_elastic = grid_search_elastic.best_score_ \n    print(best_parameters_elastic)\n    print(best_score_elastic)","bdb36f78":"kfold_cv=KFold(n_splits=5, random_state=42, shuffle=False)\nfor train_index, test_index in kfold_cv.split(nyc_model_x,nyc_model_y):\n    X_train, X_test = nyc_model_x[train_index], nyc_model_x[test_index]\n    Y_train, Y_test = nyc_model_y[train_index], nyc_model_y[test_index]","5fbac331":"lr = LinearRegression(copy_X= True, fit_intercept = True, normalize = True)\nlr.fit(X_train, Y_train)\nlr_pred= lr.predict(X_test)\n\n#Ridge Model\nridge_model = Ridge(alpha = 0.01, normalize = True)\nridge_model.fit(X_train, Y_train)             \npred_ridge = ridge_model.predict(X_test) \n\n#Lasso Model\nLasso_model = Lasso(alpha = 0.001, normalize =False)\nLasso_model.fit(X_train, Y_train)\npred_Lasso = Lasso_model.predict(X_test) \n\n#ElasticNet Model\nmodel_enet = ElasticNet(alpha = 0.01, normalize=False)\nmodel_enet.fit(X_train, Y_train) \npred_test_enet= model_enet.predict(X_test)","56abb712":"from sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom math import sqrt\nfrom sklearn.metrics import r2_score\nprint('-------------Lineer Regression-----------')\n\nprint('--Phase-1--')\nprint('MAE: %f'% mean_absolute_error(Y_test, lr_pred))\nprint('RMSE: %f'% np.sqrt(mean_squared_error(Y_test, lr_pred)))   \nprint('R2 %f' % r2_score(Y_test, lr_pred))\n\n#print('--Phase-2--')\n#print('MAE: %f'% mean_absolute_error(y_test_x, lr_pred_x))\n#print('RMSE: %f'% np.sqrt(mean_squared_error(y_test_x, lr_pred_x)))   \n#print('R2 %f' % r2_score(y_test_x, lr_pred_x))\n\nprint('---------------Ridge ---------------------')\n\nprint('--Phase-1--')\nprint('MAE: %f'% mean_absolute_error(Y_test, pred_ridge))\nprint('RMSE: %f'% np.sqrt(mean_squared_error(Y_test, pred_ridge)))   \nprint('R2 %f' % r2_score(Y_test, pred_ridge))\n\n#print('--Phase-2--')\n#print('MAE: %f'% mean_absolute_error(y_test_x, pred_ridge_x))\n#print('RMSE: %f'% np.sqrt(mean_squared_error(y_test_x, pred_ridge_x)))   \n#print('R2 %f' % r2_score(y_test_x, pred_ridge_x))\n\nprint('---------------Lasso-----------------------')\n\nprint('--Phase-1--')\nprint('MAE: %f' % mean_absolute_error(Y_test, pred_Lasso))\nprint('RMSE: %f' % np.sqrt(mean_squared_error(Y_test, pred_Lasso)))\nprint('R2 %f' % r2_score(Y_test, pred_Lasso))\n\n#print('--Phase-2--')\n#print('MAE: %f' % mean_absolute_error(y_test_x, pred_Lasso_x))\n#print('RMSE: %f' % np.sqrt(mean_squared_error(y_test_x, pred_Lasso_x)))\n#print('R2 %f' % r2_score(y_test_x, pred_Lasso_x))\n\nprint('---------------ElasticNet-------------------')\n\nprint('--Phase-1 --')\nprint('MAE: %f' % mean_absolute_error(Y_test,pred_test_enet)) #RMSE\nprint('RMSE: %f' % np.sqrt(mean_squared_error(Y_test,pred_test_enet))) #RMSE\nprint('R2 %f' % r2_score(Y_test, pred_test_enet))\n\n#print('--Phase-2--')\n#print('MAE: %f' % mean_absolute_error(y_test_x,pred_test_enet_x)) #RMSE\n#print('RMSE: %f' % np.sqrt(mean_squared_error(y_test_x,pred_test_enet_x))) #RMSE\n#print('R2 %f' % r2_score(y_test_x, pred_test_enet_x))","9560f58a":"coef = pd.Series(Lasso_model.coef_, index = Data1.iloc[:,:-1].columns)\ncoef","6225cd06":"import matplotlib\nimport matplotlib.pyplot as plt\nimp_coef = pd.concat([coef.sort_values().iloc[:10],\n                     coef.sort_values().iloc[-10:]])\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","3b7dbaca":"coef = pd.Series(model_enet.coef_, index = Data1.iloc[:,:-1].columns)\ncoef","d0f6cf17":"import matplotlib\nimport matplotlib.pyplot as plt\nimp_coef = pd.concat([coef.sort_values().iloc[:10],\n                     coef.sort_values().iloc[-10:]])\nmatplotlib.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the ElasticNet Model\")","ae72a7c9":"import numpy as np\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 5)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 6)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n# Create the random grid\nrm_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrf2 = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \nrf2_random = RandomizedSearchCV(estimator = rf2, param_distributions = rm_grid, n_iter = 5, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n# Fit the random search model\nrf2_random.fit(X_train, Y_train)\n","da1c600c":"rf2_random.best_params_\ny_predL1= rf2_random.predict(X_test)\nprint('MAE: %f' % mean_absolute_error(Y_test,y_predL1)) #RMSE\nprint('RMSE: %f' % np.sqrt(mean_squared_error(Y_test,y_predL1))) #RMSE\nprint('R2 %f' % r2_score(Y_test, y_predL1))\n","718dcba8":"from sklearn.neural_network import MLPRegressor\nNN = MLPRegressor(solver='adam', alpha=1e-5, random_state=1, max_iter=2000,hidden_layer_sizes=(256))\nNN.fit(X_train, Y_train)\nY_pred = NN.predict(X_test)","ec45062b":"print('MAE: %f' % mean_absolute_error(Y_test,Y_pred)) #RMSE\nprint('RMSE: %f' % np.sqrt(mean_squared_error(Y_test,Y_pred))) #RMSE\nprint('R2 %f' % r2_score(Y_test, Y_pred))\n","946350b5":"NN.get_params()","760d3539":"y_predL1= NN.predict(X_test)\nprint('MAE: %f' % mean_absolute_error(Y_test,y_predL1)) #RMSE\nprint('RMSE: %f' % np.sqrt(mean_squared_error(Y_test,y_predL1))) #RMSE\nprint('R2 %f' % r2_score(Y_test, y_predL1))","d86620a3":"### 1. Data Load & Exploration","53dc4078":"### 3. Data Engineering & Exploratory Data Analysis (EDA)","b164a1f8":"### Hypothesis Test for Significance (Mann Whitney U Test)\n\nSince the reviews_per_month does not have a gaussian distribution, we will use the 'Mann Whitney U Test' which is the nonparametric equivalent of the 'Two Sample t-test', and we can therefore focus on the medians of the two groups rather than the mean.\n\n<b>Is there a significant difference between reviews_per_month for hosts of 'shared rooms' and 'private rooms'?<\/b>\n> H<sub>0<\/sub>: There is no significant difference between the medians of the two groups\n\n> H<sub>a<\/sub>: There is a significant difference between the medians of the two groups\n\n> Significance Level, \u03b1 = 0.05","87548fa1":"# <font color = 'teal'>Airbnb New York Analysis (2011 - 2019)<\/font>\n\nThis dataset contains information about Airbnb host listings, neighbourhoods, price, minimum nights, review counts\/rate and the availability throughout the year.\n\nLimitations: No data on customer stay duration, timeline, review score for each listing and no. of tourist attractions nearby.\n\nAnalysis by: Ahmad Abu Saiid\n\n---\n\n## Acknowledgements\n> Dataset taken from [Kaggle](https:\/\/www.kaggle.com\/dgomonov\/new-york-city-airbnb-open-data).\n> This is a <b>public dataset of Airbnb<\/b>, and the original source can be found in their website.\n\n\n## Main Insights\n<b>1. Which areas have the most listings and customers?<\/b>\n> Manhattan and Brooklyn\n\n<b>2. Which type of room is the most\/least common in NYC?<\/b>\n> Private rooms and Entire homes are the most common listings while Shared rooms are much less.\n\n<b>3. What is the average cost for each room type and which area is the most expensive\/cheap?<\/b>\n> a) Entire homes normally cost around 160 dollars\n\n> b) Private rooms cost around 70 dollars\n\n> c) Shared rooms cost around 45 dollars\n\n> Manhattan has the most expensive listings while Staten Island has the cheapest listings.\n\n<b>4. Which areas have the busiest hosts and why?<\/b>\n> Although Manhattan and Brooklyn have the most listings & customers, Staten Island has the busiest hosts, particularly in Silver Lake, Eltingville and Richmondtown. This is due to less competition, very low costs compared to other boroughs and less minimum night requirements which drives more customers. \n\n<b>5. Which area has the best\/worst rental rate?<\/b>\n> Lowest rental rate is in Manhattan (21), while best rental rate is at Staten Island (31)\n\n<b>6. Between Manhattan and Brooklyn, which area is doing better?<\/b>\n> Brooklyn hosts receive more customers, most notably in Manhattan Beach, Dyker Heights & Bergen Beach.\n\n<b>7. How is the availability of each room type and borough in general?<\/b>\n> a) For private rooms, most Staten Island listings have them available throughout the whole year compared to the other boroughs.\n\n> b) Entire homes in NYC are generally not available for the whole year, and more towards the 6 months mark.\n\n> c) For shared rooms, Brooklyn & Queens have them available through out most of the year, while Staten Island and Bronx have them for only a couple months in a year.\n\n<b>8. Is there any particular difference in minimum nights for each borough?<\/b>\n> Staten Island listings tend to have very low minimum night requirements, which means that many vistors do not have to stay for long.\n\n<b>9. How is the review rate for hosts for each room type and area?<\/b>\n> a) Althoough Manhattan has alot of private rooms & entire homes available, many people tend to opt for the shared rooms, possibly due to lower cost and travelling alone.\n\n> b) Entire homes seem to be the go to option for most visitors in Staten Island and Bronx.\n\n<b>10. Is there a significant difference between reviews_per_month for hosts of 'shared rooms' and 'private rooms'?<\/b>\n> There is no significant difference for reviews_per_month between hosts of 'shared rooms' and 'private rooms' (with a 95% confidence)\n\n\n---","71442bf9":"### 2. Data Cleaning & Processing","e9056c57":"**Q-Q plot**\uff1a","fdaf0924":"### 4. Model","388fd330":"### End of Analysis"}}