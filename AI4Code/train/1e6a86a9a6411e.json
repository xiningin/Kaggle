{"cell_type":{"b30cb8d9":"code","8425f7ef":"code","30528ae3":"code","f7b5952b":"code","7d49d7fc":"code","f31c6ca9":"code","91a89ffb":"code","378a794d":"code","b3f4f6d2":"code","65a8a57b":"code","21ce1e14":"code","834bf45d":"code","a5112b2b":"code","5854acb5":"code","184e2bf2":"code","fa684abb":"code","a139ce74":"code","bdc7e8bc":"code","48c33ef2":"code","c7c9bc1e":"code","6ecd6045":"code","03c6a3ee":"code","27484b95":"code","81528a39":"code","6a847f30":"code","ad59ce5c":"code","b2fabc5c":"code","9c6b4902":"code","c79ab0ba":"code","df50d549":"code","11b3ba83":"code","8dbe5603":"code","cf6adad7":"code","616527bb":"code","7c474023":"code","28da3d9c":"code","32461c13":"code","e1574a44":"code","d734c701":"code","7162c1bf":"code","6698c84b":"code","d3906021":"code","06bb3371":"code","1252f65a":"code","f086b64d":"code","0f860f5d":"code","419b74d1":"code","62537a48":"code","bc6ce81f":"code","9c65775d":"code","68880533":"code","430a4fc1":"code","c1b965d1":"code","3d0f82a3":"code","63bbf00a":"code","886da2dc":"code","12614ee7":"code","b679867a":"code","129b65f2":"code","3c60ba7e":"code","0e9bf8de":"code","a5898757":"code","929e3280":"code","88b2615c":"code","50fe91e2":"code","5f72091f":"code","82b6284d":"markdown","ef6dad51":"markdown","ebf8e116":"markdown","b8550309":"markdown","c82c9457":"markdown","41bd3dd3":"markdown","ab40bef2":"markdown","49bd68fc":"markdown","a4088e14":"markdown","adcc263b":"markdown"},"source":{"b30cb8d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8425f7ef":"import pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport scipy as sp\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nfrom sklearn.model_selection import KFold, GridSearchCV\n\nimport lightgbm as lgbm\nfrom xgboost import XGBRegressor","30528ae3":"sample_sub = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2021\/sample_submission.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-feb-2021\/test.csv\")","f7b5952b":"# save id column for submission\nid_col = test['id']\ntrain.drop(\"id\", axis=1, inplace=True)\ntest.drop(\"id\", axis=1, inplace=True)","7d49d7fc":"train.head()","f31c6ca9":"num_feat = [x for x in train.columns if x.startswith('co')]\ncat_feat = [x for x in train.columns if x.startswith('ca')]","91a89ffb":"def create_num_subplots(df, feat):\n    fig = plt.figure(figsize=(14, 20))\n\n    for i, col in enumerate(feat):\n        plt.subplot(12, 3, i+1)\n        sns.histplot(x=col, data=df)\n        plt.tight_layout()\n    plt.show()","378a794d":"create_num_subplots(train, num_feat)","b3f4f6d2":"#x = np.cumsum(train['cont0'])\nx = train['cont0']\nplt.plot(x);","65a8a57b":"x1 = (sp.stats.rankdata(x) \/ (len(x)+1)) *2 - 1\n#print(np.min(x1), np.max(x1))\nx1 = np.arctanh(x1)","21ce1e14":"fig, ax = plt.subplots(2,2, figsize=(12,8))\n\nax[0,0].plot(train['cont0'])\nax[0,0].set_title(\"Original Data\")\n\n\nax[0,1].plot(x1)\nax[0,1].set_title(\"Transformed Data\")\n\nax[1,0].hist(train['cont0'], bins=40)\nax[1,0].set_title(\"Original Data\")\n\nax[1,1].hist(x1, bins=40)\nax[1,1].set_title(\"Transformed Data\")\n\n\nplt.show()","834bf45d":"# Non monotonic relation\nplt.plot(x, x1, 's')\nplt.xlabel(\"orginal\")\nplt.ylabel(\"transformed\");","a5112b2b":"def gaussian_transformer(df):\n    \n    new_df = pd.DataFrame()\n    \n    for col in df.columns:\n        z = (sp.stats.rankdata(df[col]) \/ (len(x)+1)) *2 - 1\n        z = np.arctanh(z)\n        new_df[col] = z\n    return new_df","5854acb5":"gaussian_num_df = gaussian_transformer(train[num_feat])","184e2bf2":"gaussian_num_df","fa684abb":"create_num_subplots(gaussian_num_df, num_feat)","a139ce74":"from sklearn.preprocessing import MinMaxScaler","bdc7e8bc":"sc = MinMaxScaler(feature_range=(-1,1))\ngaussian_num_df[num_feat] = sc.fit_transform(gaussian_num_df[num_feat])","48c33ef2":"create_num_subplots(gaussian_num_df, num_feat)","c7c9bc1e":"gausian_test = gaussian_transformer(test[num_feat])\ngausian_test[num_feat] = sc.transform(gausian_test[num_feat])","6ecd6045":"for col in cat_feat:\n    diff = set(train[col]) - set(test[col])\n    print(f\"Differents between train and test set is: {diff}\")","03c6a3ee":"def create_cat_subplots(df, feat):\n    \n    fig = plt.figure(figsize=(14, 20))\n    \n    for i, col in enumerate(feat):\n        plt.subplot(10, 3, i+1)\n        sns.countplot(x=df[col])\n        plt.tight_layout()\n    plt.show()","27484b95":"create_cat_subplots(train, cat_feat)","81528a39":"cord_df = pd.DataFrame(train[cat_feat].nunique().values,\n                       index=cat_feat, columns=['cartinality'])","6a847f30":"cord_df","ad59ce5c":"train_cat_df = train[cat_feat].copy()\ntest_cat = test[cat_feat].copy()","b2fabc5c":"from sklearn.preprocessing import LabelBinarizer, LabelEncoder","9c6b4902":"lb = LabelBinarizer()","c79ab0ba":"for col in cat_feat:\n    train_cat_df[col] = lb.fit_transform(train_cat_df[col])\n    test_cat[col] = lb.transform(test_cat[col])","df50d549":"train_cat_df","11b3ba83":"create_cat_subplots(train_cat_df, cat_feat)","8dbe5603":"# First create new dataframe with transformed data\nnew_train = pd.concat([train_cat_df, gaussian_num_df], axis=1)\nnew_train.head()","cf6adad7":"X = new_train\ny = train['target']","616527bb":"new_X = pd.concat([X, y], axis=1)\n\ncorr_map = new_X.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr_map, dtype=bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr_map, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","7c474023":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=101)","28da3d9c":"rmse_score = []\n\n\n \nparams ={\"objective\": \"regression\",\n         \"metric\": \"rmse\",\n         \"verbosity\": -1,\n         \"boosting_type\": \"gbdt\",\n         \"feature_fraction\": 0.5,\n         \"max_depth\": 10,\n         \"num_leaves\": 60,\n         \"lambda_l1\": 2,\n         \"lambda_l2\": 2,\n         \"learning_rate\": 0.01,\n         \"min_child_samples\":50,\n         \"bagging_fraction\": 0.7,\n         \"bagging_freq\": 1, \n         \"max_bin\": 80,}\n          #\"is_unbalance\":True,\n          #\"subsample\":0.3}\n    \n    \nlgb_train = lgbm.Dataset(X_train, y_train)\nlgb_val = lgbm.Dataset(X_val, y_val)\ngbm = lgbm.train(params,\n                 lgb_train,\n                 valid_sets=[lgb_train, lgb_val],\n                 num_boost_round=10000,\n                 verbose_eval=100,\n                 early_stopping_rounds=100,\n                 )\n    \n# Extra Boosting\nlgb_train = lgbm.Dataset(X_train, y_train)\nlgb_val = lgbm.Dataset(X_val, y_val)\nparams = {\"objective\": \"regression\",\n          \"metrics\": \"rmse\",\n          \"verbosity\": -1,\n          \"boosting_type\": \"gbdt\",\n          \"feature_fraction\": 0.5,\n          \"max_depth\": 10,\n          \"num_leaves\":200,\n          \"lambda_l1\": 2,\n          \"labmda_l2\": 2,\n          \"learning_rate\": 0.003,\n          \"min_child_samples\": 50,\n          \"max_bin\": 80,\n          #\"is_unbalance\":True,\n          #\"subsample\":0.3\n          \"bagging_fraction\": 0.7,\n          \"bagging_freq\": 1,}\n    \ngbm = lgbm.train(params,\n                 lgb_train,\n                 valid_sets = [lgb_train, lgb_val],\n                 verbose_eval = 100,\n                 num_boost_round = 10000,\n                 early_stopping_rounds=100,\n                 init_model = gbm)\n    \ny_pred = gbm.predict(X_val)\nrmse_score.append(np.sqrt(mean_squared_error(y_val, y_pred)))","32461c13":"rmse_score","e1574a44":"kf = KFold(n_splits=5, shuffle=True, random_state=1)\noof = np.zeros(len(X))\nscore_list = []\nfold = 1\n\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n    \n\n    #X_train = X_train.abs()\n\n    \n    y_pred_list = []\n    for seed in [1]:\n        dtrain = lgbm.Dataset(X_train, y_train)\n        dvalid = lgbm.Dataset(X_val, y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"gbdt\",\n                  \"feature_fraction\": 0.5,\n                  \"max_depth\": 10,\n                  \"num_leaves\": 120,\n                  \"lambda_l1\": 2,\n                  \"lambda_l2\": 2,\n                  \"learning_rate\": 0.01,\n                  \"min_child_samples\":50,\n                  #\"bagging_fraction\": 0.7,\n                  #\"bagging_freq\": 1, \n                  \"max_bin\": 80,\n                  \"is_unbalance\":True,}\n                  #\"subsample\":0.3}\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                        dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=100000,\n                        early_stopping_rounds=100\n                    )\n        \n        # Extra boosting.\n        dtrain = lgbm.Dataset(X_train, y_train)\n        dvalid = lgbm.Dataset(X_val, y_val)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"gbdt\",\n                  \"feature_fraction\": 0.5,\n                  \"max_depth\": 10,\n                  \"num_leaves\": 120,\n                  \"lambda_l1\": 2,\n                  \"lambda_l2\": 2,\n                  \"learning_rate\": 0.03,\n                  \"min_child_samples\":50,\n                  #\"bagging_fraction\": 0.7,\n                  #\"bagging_freq\": 1, \n                  \"max_bin\": 80,\n                  \"is_unbalance\":True,}\n                  #\"subsample\":0.3}\n\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                            dtrain,\n                            valid_sets=[dtrain, dvalid],\n                            verbose_eval=100,\n                            num_boost_round=1000,\n                            early_stopping_rounds=100,\n                            init_model = model\n                            )\n\n    \n    \n        y_pred_list.append(model.predict(X_val))\n    \n   \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)","d734c701":"print(score_list)\nprint(np.mean(score_list))","7162c1bf":"import optuna","6698c84b":"def objective(trial):\n    \n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=45)\n    dtrain = lgbm.Dataset(X_train, label=y_train)\n    \n    param = {\n        'objective': 'regression',\n        'metric': 'rmse',\n        'boosting_type': 'gbdt',\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-8, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-8, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 2, 256),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.4, 1.0),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.4, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 100),\n        'max_depth': trial.suggest_int(\"max_depth\", 1, 100),\n        'max_bin': trial.suggest_int('max_bin', 1, 255)\n    }\n    \n    gbm = lgbm.train(param, dtrain)\n    preds = gbm.predict(X_val)\n    pred_labels = np.rint(preds)\n    rmse = np.sqrt(mean_squared_error(y_val, pred_labels))\n    return rmse\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=100)\n\nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial: \", study.best_trial.params)","d3906021":"study.best_trial.params","06bb3371":"kf = KFold(n_splits=5, shuffle=True, random_state=45)\noof = np.zeros(len(X))\nscore_list = []\nfold = 1\ntest_preds = []\n\n\nfor train_index, test_index in kf.split(X):\n    X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n\n\n    y_pred_list = []\n    for seed in [1]:\n        dtrain = lgbm.Dataset(X_train, y_train)\n        dvalid = lgbm.Dataset(X_val, y_val)\n        print(seed)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"gbdt\",\n                  \"feature_fraction\": 0.44071200607037225,\n                  \"max_depth\": 90,\n                  \"num_leaves\": 192,\n                  \"lambda_l1\": 1.264462581934323,\n                  \"lambda_l2\": 0.024398254447942604,\n                  \"learning_rate\": 0.01,\n                  \"min_child_samples\":91,\n                  \"bagging_fraction\": 0.9326330031125017,\n                  \"bagging_freq\": 3,\n                  \"max_bin\": 145,\n                  \"is_unbalance\":True}\n                  #\"subsample\":0.3}\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                           dtrain,\n                        valid_sets=[dtrain, dvalid],\n                        verbose_eval=100,\n                        num_boost_round=2500,\n                        early_stopping_rounds=100\n                    )\n        \n        # Extra boosting.\n        dtrain = lgbm.Dataset(X_train, y_train)\n        dvalid = lgbm.Dataset(X_val, y_val)\n        params = {\"objective\": \"regression\",\n                  \"metric\": \"rmse\",\n                  \"verbosity\": -1,\n                  \"boosting_type\": \"gbdt\",\n                  \"feature_fraction\": 0.44071200607037225,\n                  \"max_depth\": 90,\n                  \"num_leaves\": 192,\n                  \"lambda_l1\": 1.264462581934323,\n                  \"lambda_l2\": 0.024398254447942604,\n                  \"learning_rate\": 0.0001,\n                  \"min_child_samples\":91,\n                  \"bagging_fraction\": 0.9326330031125017,\n                  \"bagging_freq\": 3,\n                  \"max_bin\": 145,\n                  \"is_unbalance\":True,}\n                 # \"subsample\":0.3}\n\n        params[\"seed\"] = seed\n        model = lgbm.train(params,\n                            dtrain,\n                            valid_sets=[dtrain, dvalid],\n                            verbose_eval=100,\n                            num_boost_round=1500,\n                            early_stopping_rounds=100,\n                            init_model = model\n                            )\n\n    \n    \n        y_pred_list.append(model.predict(X_val))\n        #test_preds.append(model.predict(new_test))\n    \n   \n    \n    oof[test_index] = np.mean(y_pred_list,axis=0)    \n    score = np.sqrt(mean_squared_error(y_val, oof[test_index]))\n    score_list.append(score)\n    print(f\"RMSE Fold-{fold} : {score}\")\n    fold+=1\n\nnp.mean(score_list)\nprint(score_list)\nprint(np.mean(score_list))","1252f65a":"print(score_list)\nprint(np.mean(score_list))","f086b64d":"new_test = pd.concat([test_cat, gausian_test], axis=1)","0f860f5d":"preds = model.predict(new_test)\n\nsub = pd.DataFrame({\"id\":id_col,\n                    \"target\": preds})\n\nsub.to_csv(\"sub3_with_optuna.csv\", index=False)","419b74d1":"import joblib","62537a48":"joblib.dump(model, \"lgbm_optuna_model.joblib\")","bc6ce81f":"loaded_model = joblib.load(\".\/lgbm_optuna_model.joblib\")","9c65775d":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=101)","68880533":"from sklearn.model_selection import GridSearchCV\n\ndef model_gridsearchCV(algo,param,name):\n    \"\"\"\n    Function will perform gridsearchCV for given algorithm\n    and parameter grid. Returns grid model, y_pred. Prints out \n    mean absolute error, root mean squared error, R-square score\n    \"\"\"\n    # Instatiate base model\n    model = algo()\n    \n    # Instantiate grid for a model\n    model_grid = GridSearchCV(model, \n                             param,\n                             scoring=\"r2\",\n                             verbose=2,\n                             n_jobs=-1,\n                             cv=3)\n    # Fit the grid model\n    model_grid.fit(X_train, y_train)\n    \n    # Make prediction\n    y_pred = model_grid.predict(X_val)\n    \n    # Evaluate model\n    mae = mean_absolute_error(y_val, y_pred)\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    r2score = r2_score(y_val, y_pred)\n    \n    # Print \n    print(f\"**{name} with GridSearchCV**\")\n    print(f\"MAE: {mae:}\")\n    print(f\"RMSE: {rmse:}\")\n    print(f\"R-squared: {r2score:.2f}%\")\n    \n    return mae, rmse, r2score, y_pred, model_grid","430a4fc1":"param_grid = {\"loss\":[\"ls\",\"huber\",\"quantile\"],\n              \"learning_rate\": [ 0.01],\n              \"subsample\": [0.5, 0.2, 0.1],\n              \"max_depth\": [3,6,8]}\n\ngbr_grid_mae, gbr_grid_rmse, gbr_grid_r2, _ , gbr_grid = model_gridsearchCV(GradientBoostingRegressor, \n                                                                            param_grid,\n                                                                            \"GradientBoostingRegressor\")","c1b965d1":"joblib.dump(gbr_grid, \"GradientBoostingRegressor_model.joblib\")","3d0f82a3":"from sklearn.svm import SVR","63bbf00a":"param_grid = {\"kernel\":[\"linear\",\"rbf\",],\n              \"gamma\": [\"scale\",\"auto\"],\n              \"C\": [0.1, 0.5, 10],\n              \"epsilon\": [0.1, 0.01]}\n\nsvr_grid_mae, svr_grid_rmse, svr_grid_r2, svr_grid_y_pred, svr_grid_model = model_gridsearchCV(SVR,\n                                                                                param_grid,\n                                                                               \"SVR\")","886da2dc":"joblib.dump(gbr_grid, \"SVR_model.joblib\")","12614ee7":"param_grid = {\"learning_rate\":[0.01],\n              \"max_depth\":[3,4,8],\n              \"min_child_weight\":[3,5,7],\n              \"colsample_bytree\":[0.3, 0.5, 0.7]}\n\nxboost_gr_mae, xboost_gr_rmse, xboost_gr_r2, _ , xboost_gr_model = model_gridsearchCV(XGBRegressor,\n                                                                                      param_grid,\n                                                                                      \"XGBoost\")","b679867a":"xboost_gr_model.best_params_","129b65f2":"joblib.dump(xboost_gr_model, \"XGBoost_model.joblib\")","3c60ba7e":"X = new_train.values\ny = train['target'].values","0e9bf8de":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom keras.callbacks import EarlyStopping, TensorBoard\nfrom keras.layers import LeakyReLU\nfrom keras.optimizers import Adam, RMSprop, SGD","a5898757":"X.shape, y.shape","929e3280":"def evaluate_model(model):\n    y_pred = model.predict(X_val)\n    rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n    print(f\"RMSE: {rmse}\")","88b2615c":"def create_ann():\n    # Instantiate a model\n    model = Sequential()\n    # Add hidden layer \n    model.add(Dense(14, activation='relu'))\n    # Add hidden layer\n    model.add(Dense(24, activation='relu'))\n    model.add(Dropout(0.1))\n    # Add hidden layer\n    model.add(Dense(14, activation='relu'))\n    # Add output layer\n    model.add(Dense(1))\n    \n    # Compile the model\n    model.compile(optimizer=Adam(lr=0.001), loss=tensorflow.keras.losses.MeanSquaredError(),\n                  metrics=['mse'])\n    \n    return model\n","50fe91e2":"kf = KFold(n_splits=5, shuffle=True, random_state=45)\noof = np.zeros(len(X))\nscore_list = []\nfold = 1\n\ny_pred_list = []\nfor train_idx, test_idx in kf.split(X):\n    X_train, X_val = X[train_idx], X[test_idx]\n    y_train, y_val = y[train_idx], y[test_idx]\n    \n    early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n    \n    ann_model = create_ann()\n    ann_model.fit(X_train, \n                  y_train, \n                  validation_data=(X_val, y_val), \n                  batch_size=32, epochs=30, \n                  callbacks=[early_stop])\n    \n    \n    y_pred_list.append(ann_model.predict(X_val))\n    \n    \n    oof[test_idx] = np.mean(y_pred_list, axis=0).reshape(len(X_val),)\n    score = np.sqrt(mean_squared_error(y_val, oof[test_idx]))\n    score_list.append(score)\n    print(f\"RMSE fold -{fold} : {score}\")\n    fold +=1\n    \nprint(f\"RMSE mean 5 folds: {np.mean(score_list)}\")","5f72091f":"ann_model.save(\"ANN_model.h5\")","82b6284d":"## Categorical features","ef6dad51":"## Numerical Features","ebf8e116":"## Modeling","b8550309":"## Keras","c82c9457":"## GridSearchCV","41bd3dd3":"As we can see distribution of our numeric features is not Gaussian.","ab40bef2":"{'colsample_bytree': 0.7,\n 'learning_rate': 0.01,\n 'max_depth': 8,\n 'min_child_weight': 7}","49bd68fc":"## Submission","a4088e14":"**Test set**","adcc263b":"## Optuna to the rescue"}}