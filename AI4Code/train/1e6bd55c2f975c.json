{"cell_type":{"13ef7e43":"code","e6f362c3":"code","ea324a9e":"code","780aa87b":"code","c7335961":"code","be769934":"code","1ec8811b":"code","b99e700e":"code","373fddc6":"code","288f4fe5":"code","10818de9":"code","c8bb2d31":"code","aa424d75":"code","dea471c0":"code","d5e8f63c":"markdown","f6d5b8dd":"markdown","e5439e3e":"markdown","0487e843":"markdown","ee14436e":"markdown","824bb954":"markdown","54718241":"markdown","534ac618":"markdown","d7368afc":"markdown","e1225e67":"markdown"},"source":{"13ef7e43":"import numpy as np \nimport pandas as pd\nimport os\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import mean_squared_error, accuracy_score, log_loss\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\nimport optuna","e6f362c3":"train_data = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")\nsample_solution = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\n\n# Getting labels from \"target\" column\ntrain_labels = train_data['target']\nX_test = test_data.copy()\nX_test = X_test.drop(['id'], axis=1)","ea324a9e":"train_data.head()","780aa87b":"#Creating KFold training to to avoid overfitting.\n# A good resource for understanding k-fold and stratified k-fold is written here:\n# https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/\ntrain_data['kfold'] = -1\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=None)\nfor fold, (train_index, valid_index) in enumerate(kf.split(train_data, train_labels)):\n    print(\"TRAIN:\", train_index, \"TEST:\", valid_index)\n    train_data.loc[valid_index, 'kfold'] = fold","c7335961":"exclude_cols = [\"id\", \"kfold\", \"target\"]\nuseful_cols = [i for i in train_data.columns if i not in exclude_cols]\nfeature_cols = [col for col in train_data.columns if col.startswith('f')]","be769934":"num_cols_with_missing = sum(train_data.isnull().sum() > 0)\nnum_cols_with_missing","1ec8811b":"final_predictions = []\ndef run(trial):\n    fold = 0\n    param_grid = {'objective': 'binary:logistic',\n                  'use_label_encoder': False,\n                  'n_estimators': trial.suggest_int('n_estimators', 500, 5000),\n                  'learning_rate': trial.suggest_discrete_uniform('learning_rate',0.01,0.1,0.01),\n                  'subsample': trial.suggest_discrete_uniform('subsample', 0.3, 1.0, 0.1),\n                  'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1.0, 0.1),\n                  'max_depth': trial.suggest_int('max_depth', 2, 20),\n                  'booster': 'gbtree',\n                  'gamma': trial.suggest_uniform('gamma',1.0,10.0),\n                  'reg_alpha': trial.suggest_int('reg_alpha',50,100),\n                  'reg_lambda': trial.suggest_int('reg_lambda',50,100),\n                  'random_state': 42,\n                 }\n    \n    \n    \n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    \n    y_train = X_train['target']\n    X_train = X_train.drop(exclude_cols, axis=1)\n    y_valid = X_valid['target']\n    X_valid = X_valid.drop(exclude_cols, axis=1)\n    \n    model = XGBClassifier(**param_grid, tree_method='gpu_hist', predictor='gpu_predictor', eval_metric=['logloss'])\n    \n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             early_stopping_rounds = 200)\n    preds_valid_prob = model.predict_proba(X_valid)[:, 1]\n    roc_auc = roc_auc_score(y_valid, preds_valid_prob)\n    return roc_auc","b99e700e":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(run, n_trials=10)","373fddc6":"param_best = study.best_params\nstudy.best_params","288f4fe5":"final_predictions = []\nfor fold in range(5):\n\n    X_train = train_data[train_data.kfold != fold].reset_index(drop=True)\n    X_valid = train_data[train_data.kfold == fold].reset_index(drop=True)\n    \n    y_train = X_train['target']\n    X_train = X_train.drop(exclude_cols, axis=1)\n    y_valid = X_valid['target']\n    X_valid = X_valid.drop(exclude_cols, axis=1)\n    \n    model = XGBClassifier(**param_best, tree_method='gpu_hist', predictor='gpu_predictor', eval_metric=['logloss'])\n    \n    model.fit(X_train, y_train,\n             verbose = False,\n             eval_set = [(X_train, y_train), (X_valid, y_valid)],\n             early_stopping_rounds = 200)\n    preds_valid_prob = model.predict_proba(X_valid)[:, 1]\n    roc_auc = roc_auc_score(y_valid, preds_valid_prob)\n    test_pred_prob = model.predict_proba(X_test)[:, 1]\n    final_predictions.append(test_pred_prob)\n    print(fold, roc_auc)","10818de9":"final_predictions","c8bb2d31":"sample_solution['target'] = final_predictions[0]","aa424d75":"sample_solution.to_csv(\"submission_default_xgboost_1.csv\", index=False)","dea471c0":"sample_solution.head()","d5e8f63c":"## Using Final Predictions for Submission","f6d5b8dd":"## Using Optuna to tune hyperparameters for XGBoost","e5439e3e":"# Loading training and testing data","0487e843":"Writing the final submission file","ee14436e":"## Viewing training data","824bb954":"## Viewing final predictions","54718241":"## Creating 5-fold data","534ac618":"Using the k-fold training data to train the data by applying the best hyperparameters defined from Optuna","d7368afc":"Optuna is a hyperparameter optimization framework. It is framework agnostic and can be used with any ML or DL Framework.\n\nIt takes in an objective function and tries to optimize it based on the metric defined.\n\nMore on Optune: 1. https:\/\/optuna.org\/\n                2. https:\/\/www.analyticsvidhya.com\/blog\/2020\/11\/hyperparameter-tuning-using-optuna\/","e1225e67":"## Training XGBoost using best parameters"}}