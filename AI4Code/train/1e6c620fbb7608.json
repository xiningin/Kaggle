{"cell_type":{"381d6d0c":"code","6aa83110":"code","66772675":"code","b7550952":"code","1b40fb49":"code","3716f5da":"code","2944fe16":"code","b772e3fc":"code","c6e3e5dd":"code","fbe975ce":"code","9b993ec8":"code","5ea6cb53":"code","75d95f96":"code","96c5120c":"code","c4dbf33b":"code","683e76a7":"code","5f22063c":"code","1fa5db52":"code","96a22501":"code","dbbd1b57":"code","60f39a80":"code","6cf0840f":"code","2a3bb74f":"code","58a82c7a":"code","21f8da25":"markdown","48392650":"markdown","ebb6fef1":"markdown","4d0e69df":"markdown","f0871c3c":"markdown","3b179ff1":"markdown","caab4840":"markdown","f8a03ab2":"markdown","5722570c":"markdown","92fe396a":"markdown","0e8b0e31":"markdown","d3cf8f6e":"markdown","83bbaa1a":"markdown","1066203b":"markdown","a54e58d5":"markdown","a98ee0f4":"markdown","3dd7516c":"markdown","92c8a401":"markdown","2302af0f":"markdown","1ee9848a":"markdown","984cdb0f":"markdown","733777f5":"markdown","6f245891":"markdown","e903b557":"markdown","e4852456":"markdown","2d870b83":"markdown","3d83d334":"markdown","252a36cf":"markdown"},"source":{"381d6d0c":"from IPython.display import HTML\n\nHTML('<center><iframe width=\"800\" height=\"450\" src=\"https:\/\/www.youtube.com\/embed\/YhZXU5zUnO0\" frameborder=\"0\" allowfullscreen><\/iframe><\/center>')","6aa83110":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy\nimport numpy as np\nimport os\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nplt.style.use('seaborn')","66772675":"#change directory to 01_Input\nos.chdir('..\/input\/titanic')","b7550952":"df = pd.read_csv('train.csv')\n\ndf.head()","1b40fb49":"#version 1 with def and .apply\ndef survived_to_words(row):\n    if row.Survived == 1:\n        value = \"yes\"\n    elif row.Survived == 0:\n        value = \"no\"\n    else:\n        value = \"unkown\"\n    \n    return value\n\ndf[\"Survived_word\"] = df.apply(survived_to_words, axis=1)","3716f5da":"#version 2 with lambda\ndf[\"Survived_word\"] = df.Survived.apply(lambda row: \"yes\" if row == 1 else \"no\")","2944fe16":"#version 3 with np.where\ndf[\"Survived_word\"] = np.where(df.Survived == 1, \"yes\", \"no\")\n\ndf.head()","b772e3fc":"print('shape is',df.shape, '\\n')\nprint(df.count())","c6e3e5dd":"fig=plt.figure(figsize=(8, 5), dpi= 80, facecolor='w', edgecolor='k')\n\ndf.Survived.value_counts(normalize=True).plot(kind=\"bar\")\nplt.title(\"Survived\")\n\nplt.show()","fbe975ce":"fig=plt.figure(figsize=(8, 5), dpi= 80, facecolor='w', edgecolor='k')\n\nplt.scatter(df.Survived, df.Age, alpha=0.1)\nplt.title(\"Age wrt Survived\")\n\nplt.show()","9b993ec8":"fig=plt.figure(figsize=(22, 13), dpi= 80, facecolor='w', edgecolor='k')\n\nplt.subplot2grid((2,3), (0,0))\ndf.Pclass.value_counts(normalize=True).plot(kind=\"pie\")\nplt.title(\"Distribution of class\")\n\nplt.subplot2grid((2,3), (0,1))\ndf.Pclass.value_counts(normalize=True).plot(kind=\"bar\")\nplt.title(\"Distribution of class\")\n\nplt.show()","5ea6cb53":"fig=plt.figure(figsize=(10, 5), dpi= 80, facecolor='w', edgecolor='k')\n\nfor x in [1,2,3]:\n    df.Age[df.Pclass == x].plot(kind=\"kde\")\nplt.title(\"Class wrt Age\")\nplt.legend((\"Class 1\", \"Class 2\", \"Class 3\"))\n\nplt.show()","75d95f96":"fig=plt.figure(figsize=(8, 5), dpi= 80, facecolor='w', edgecolor='k')\n\ndf.Embarked.value_counts(normalize=True).plot(kind=\"bar\")\nplt.title(\"Embarked location\")\n\nplt.show()","96c5120c":"fig = plt.figure(figsize=(22,15))\n\nplt.subplot2grid((3,4), (0,0))\ndf.Survived_word[df.Sex == \"male\"].value_counts(normalize=True).plot(kind=\"bar\", color=\"#fc8320\")\nplt.title(\"Men survived\")\n\nplt.subplot2grid((3,4), (0,1))\ndf.Survived_word[df.Sex == \"female\"].value_counts(normalize=True).plot(kind=\"bar\", color=\"#006418\")\nplt.title(\"Women survived\")\n\nplt.subplot2grid((3,4), (0,2))\ndf.Sex[df.Survived_word == \"yes\"].value_counts(normalize=True).plot(kind=\"bar\", color=\"#065183\")\nplt.title(\"Sex of survivors\")\n\nplt.show()","c4dbf33b":"fig=plt.figure(figsize=(10, 5), dpi= 80, facecolor='w', edgecolor='k')\n\nfor x in [1,2,3]:\n    df.Survived[df.Pclass == x].plot(kind=\"kde\")\nplt.title(\"Survived wrt Class\")\nplt.legend((\"Class 1\", \"Class 2\", \"Class 3\"))\n\nplt.show()","683e76a7":"fig = plt.figure(figsize=(22,15))\n\nplt.subplot2grid((3,4), (0,0))\ndf.Survived_word[(df.Sex == \"male\")  &  (df.Pclass == 1)].value_counts(normalize=True).plot(kind=\"bar\", color=\"#fc8320\")\nplt.title(\"Rich men survived\")\n\nplt.subplot2grid((3,4), (0,1))\ndf.Survived_word[(df.Sex == \"male\")  &  (df.Pclass == 3)].value_counts(normalize=True).plot(kind=\"bar\", color=\"#006418\")\nplt.title(\"Poor men survived\")\n\nplt.subplot2grid((3,4), (1,0))\ndf.Survived_word[(df.Sex == \"female\")  &  (df.Pclass == 1)].value_counts(normalize=True).plot(kind=\"bar\", color=\"#065183\")\nplt.title(\"Rich women survived\")\n\nplt.subplot2grid((3,4), (1,1))\ndf.Survived_word[(df.Sex == \"female\")  &  (df.Pclass == 3)].value_counts(normalize=True).plot(kind=\"bar\", color=\"#b30000\", alpha=0.75)\nplt.title(\"Poor women survived\")\nplt.show()","5f22063c":"train = df.copy()\n\ntrain[\"Hypothesis\"] = np.where(train.Sex == 'female', 1, 0)\ntrain['Result'] = np.where(train.Hypothesis == train.Survived, 1, 0)\n\ntrain.Result.value_counts(normalize=True)","1fa5db52":"def clean_data(data):\n    # Fill the empty rows of Fare and Age with their median\n    data['Fare'] = data.Fare.fillna(data.Fare.dropna().median())\n    data['Age'] = data.Age.fillna(data.Age.dropna().median())\n    \n    # Convert Sex column from words \"Female\" and \"Male\" to 0 and 1\n    data[\"Sex\"] = np.where(data.Sex == \"male\", 0, 1)\n    \n    # Fill NaNs of Embarked column and convert to integers\n    data[\"Embarked\"] = data[\"Embarked\"].fillna(\"S\")\n    conditions = [data['Embarked'] == \"S\", \n                  data['Embarked'] == \"C\",\n                  data['Embarked'] == \"Q\"]\n    choices = [0, 1, 2]\n    data[\"Embarked\"] = np.select(conditions, choices, default = 0)\n\nclean_data(train)\n\ntrain.head()","96a22501":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score","dbbd1b57":"feature_columns = ['Pclass', 'Age', 'Sex', 'SibSp', 'Parch']\n\ntarget = train.Survived.values\nfeatures = train[feature_columns].values\n\nclassifier = LogisticRegression(solver='lbfgs')\nclassifier_ = classifier.fit(features, target)\n\nprint(\"logistic regression accuracy =\", classifier_.score(features, target))\n\nprint(\"logistic regression accuracy with cross validation = \", np.mean(cross_val_score(classifier, \n                                                                                        features, \n                                                                                        target, \n                                                                                        cv=10)))","60f39a80":"from sklearn.preprocessing import PolynomialFeatures","6cf0840f":"poly = PolynomialFeatures(degree=2)\npoly_features = poly.fit_transform(features)\n\nclassifier = LogisticRegression()\nclassifier_ = classifier.fit(poly_features, target)\nprint(\"accuracy with polynomial features =\", classifier_.score(poly_features, target))\n\nprint(\"accuracy with polynomial features combined with cross validation =\", \n      np.mean(cross_val_score(classifier, \n                               poly_features, \n                               target, \n                               cv=10)))","2a3bb74f":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(n_estimators=100)\nrf_ = rf.fit(features, target)\n\nprint(\"accuracy with random forrest regressor =\", rf_.score(features, target))\n\nprint(\"accuracy with random forrest regressor and cross validation =\",\n      np.mean(cross_val_score(rf, \n                               features, \n                               target, \n                               cv=10)))","58a82c7a":"from sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier(random_state=1,\n                                      max_depth=7,\n                                      min_samples_split=2)\n\ndecision_tree_ = decision_tree.fit(features, target)\n\nprint(\"accuracy with decision tree =\", decision_tree_.score(features, target))\nprint(\"accuracy with decision tree and cross validation =\",\n     np.mean(cross_val_score(decision_tree, features, target, cv=50)))","21f8da25":"### Lets plot how age is distributed regarding to  class with kernel density estimator plot\nFor more information on KDE, see [link](https:\/\/en.wikipedia.org\/wiki\/Kernel_density_estimation)","48392650":"<center><h1> Predicting the titanic survivors <\/h1><\/center>","ebb6fef1":"### In the movie we saw _'women and children first'_, lets see if women actually survived significantly more  ","4d0e69df":"# 5. ML Models\n<a id=\"ml\"><\/a>\n## Lets start to use some ML models on this data","f0871c3c":"## 5.3 Polynomial\n<a id=\"poly\"><\/a>\n\nWith logistic regression we assume that our data is linear, but it could be the case that some relations are second degree (polynomial). So lets try to fit second degree relations between the features and the target variable.b\n","3b179ff1":"### Also we can imagine that passengers with higher class had a bigger chance of surviving compated to lower classes, lets plot this","caab4840":"<center><h3>In this project I will gradually introduce machine learning with python by analyzing the titanic dataset.\n    <br>\n    <br>\n    the goal is to predict which passenger will survive and which will not, based on the attributes of these passengers <h3><\/center>","f8a03ab2":"# 4. Visualization pt 2\n<a id=\"visz2\"><\/a>\n## Now we understand our data by visualizing it, lets analyze it in more depth","5722570c":"**Three ways to make a calculated field in Python**\n\nWe will make a new column called `Survived_word`, so that our visualizations make more sens\n    1. define function and call it with .apply method\n    2. with lambdas \n    3. np.where function from numpy","92fe396a":"### Here we make a scatter plot with `Survived` (binary) on the x-axis and `Age` (continuous) on the y-axis to check of more younger poeple survived","0e8b0e31":"## 5.4 Random forrest classifier\n<a id=\"rf\"><\/a>\n\nWe can also try to attempt to fit our data with decision trees, not just one, but 100 of them.\nBasically we will use the mean of the accuracy of these 100 decision trees. \nThe advantage of RF compared to a single decision tree is that we can anticipate on overfitting for a part.","d3cf8f6e":"**Read the csv file into a dataframe**","83bbaa1a":"<center> This kernel is inspired on the video of Ju Liu at the No Slides Conf 03-12-2016 <\/center>","1066203b":"Conclusion: if we only use **gender** as a predictor, we already have a **79%**  accuracty, thats high!\n\nBut lets try to predict with actuall ML models like:\n1. Logistic Regression\n2. Polynomial\n3. Random forrest\n4. XGBoost\n5. Decision tree with cross validation","a54e58d5":"### First we do a basic check the on the distribution of `Survived`","a98ee0f4":"### Lets visualize how the passengers were distributed regarding to their class","3dd7516c":"## 5.5 Decision tree with cross validation\n<a id=\"dt\"><\/a>","92c8a401":"### Apparently not all passengers embarked from the same location, lets see how many embarked locations we have and what the distribution is","2302af0f":"# 3. Visualization pt 1\n<a id=\"visz1\"><\/a>\n## Lets make some visualizations get a better understanding of our data ","1ee9848a":"**First we start with some hypothesis which we found by visualizing our data (EDA)**\n1. Gender is a good predictor for suvival, women will more likely survive\n2. The higher the class (1 is high, 3 is low) the more likeley the person has survived\n3. Logically we can derive from the points above that a rich women has almost always survived (97%)","984cdb0f":"### read in the test data to predict after the modelling","733777f5":"**We print the shape of our dataframe (rows, columns) and after that the amount of rows that have a value in each column**","6f245891":"### Now we know that `Sex` and `Class` are a good predictor or `Survived`, lets analyze these in more depth\n### For example, did more higher class male\/females, survive more compared to lower class male\/females","e903b557":"<center> Author: Erfan Nariman <br>\n    Date: 01-12-2018 <br>\n<a href=\"https:\/\/github.com\/ErfPy\">Github<\/a><br><\/center>","e4852456":"# Table of contents\n\n[1. Import modules](#import)<br>\n[2. Import data](#data)<br>\n[3. Visualization pt 1](#visz1)<br>\n[4. Visualization pt 2](#visz2)<br>\n[5. ML models](#ml)<br>\n>[5.1 Feature engineering](#fe)<br>\n>[5.2 Logistic Regression Classifier](#lr)<br>\n>[5.3 Polynomial](#poly)<br>\n>[5.4 Random forrest regressor](#rf)<br>\n>[5.5 Decision tree](#dt)<br>\n\n","2d870b83":"# 2. Import data\n<a id=\"data\"><\/a>\n\n**Since our data is in another map called 01_Input, we need to change our working directory to that map**","3d83d334":"# 1. Import modules\n<a id=\"import\"><\/a>\n\nIn case you dont have any of these modules, please install before running this project.\nEither install through anaconda or pip ","252a36cf":"## 5.2 Logistic Regression Classifier\n<a id=\"lr\"><\/a>\n\nSince our dependent variable (Survived) is binary (0 or 1) we have to classify our data to these to outcomes. \nLogistic regression is one of the models we can use here to classify our data. "}}