{"cell_type":{"a2df3d01":"code","4a443876":"code","b79b9dff":"code","e7ebe08a":"code","6d8b2f79":"code","82ea94e8":"code","ce930b16":"code","0cbaa657":"code","fd097358":"code","5c4fa982":"code","4544e123":"code","cf104197":"code","6c90541c":"code","1584becd":"code","ede41b49":"code","e60af6fe":"code","a164dcfa":"code","6b8ed5d4":"code","f7081db0":"code","226ead71":"code","c07b137e":"code","114de117":"code","ecbbfb75":"code","7b3dc01c":"code","9a2fd8c8":"code","dc5fb0fb":"code","17bf2bd9":"code","872e7dff":"code","3fc81e48":"code","78121da2":"code","255a8952":"markdown","09cfc023":"markdown","6944a2e3":"markdown","bb6d1901":"markdown","fe625ae8":"markdown","dd02a65f":"markdown","841cf204":"markdown","b4f48ebf":"markdown","15797c5b":"markdown","2144c3b3":"markdown","7f83654d":"markdown","1cc0db90":"markdown","f0ccae5b":"markdown","f9713ef2":"markdown","b8a1598c":"markdown","1691fbb4":"markdown","84601acb":"markdown","e8926dc3":"markdown","e210ca56":"markdown"},"source":{"a2df3d01":"# Standard library\nimport random\nimport copy\nimport json\nimport glob\n\n# Specific import from standard library\nfrom collections import defaultdict, Counter\nfrom pathlib import Path\n\n# Basic data libraries\nimport numpy as np\nimport pandas as pd\n\n# Kaggle\nimport kaggle_environments\nfrom kaggle_environments import evaluate, make, utils\n\n# Interactivity\nimport ipywidgets as widgets\nfrom ipywidgets import interactive, interact\nfrom ipywidgets import IntSlider, interact_manual\n\n# Graphs\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom matplotlib.animation import FuncAnimation, PillowWriter\nfrom PIL import Image\n\n# Progress bar\nfrom tqdm.auto import tqdm","4a443876":"# Default graph settings\n\n# Seaborn advanced                                                                                                                                                           \nsns.set(style='ticks',          # 'ticks', 'darkgrid'                                                                                                                        \n        palette='colorblind',   # 'colorblind', 'pastel', 'muted', 'bright'                                                                                                  \n        #palette=sns.color_palette('Accent'),   # 'Set1', 'Set2', 'Dark2', 'Accent'                                                                                          \n        rc = {                                                                                                                                                               \n           'figure.autolayout': False,   # Automaticall set the figure size to fit in canvas                                                                       \n           'figure.figsize': (16, 10),   # Figure size - width, height (in inches)    \n           'figure.max_open_warning': False,\n           'figure.titlesize': 32,      # Whole figure title size (plt.suptitle)\n           'legend.frameon': True,      # Frame around the legend                                                                                                              \n           'patch.linewidth': 2.0,      # Width of frame around the legend                                                                                                        \n           'lines.markersize': 6,       # Size of marker points                                                                                                                      \n           'lines.linewidth': 2.0,      # Width of lines                                                                                                                      \n           'font.size': 14,             # Size of font on axes values                                                                                                           \n           'legend.fontsize': 18,       # Font size in the legend                                                                                                           \n           'axes.labelsize': 22,        # Font size of axes names                                                                                                                  \n           'axes.titlesize': 26,        # Font size of subplot titles (plt.title)                                                                                                                 \n           'axes.grid': True,           # Set grid on\/off                                                                                                                             \n           'grid.color': '0.9',         # Color of grid lines - 1 = white, 0 = black                                                                                          \n           'grid.linestyle': '-',       # Style of grid lines                                                                                                              \n           'grid.linewidth': 1.0,       # Width of grid lines                                                                                                                \n           'xtick.labelsize': 22,       # Font size of values on X-axis                                                                                                  \n           'ytick.labelsize': 22,       # Font size of values on Y-axis                                                                                                       \n           'xtick.major.size': 8,       # Size of ticks on X-axis                                                                                                    \n           'ytick.major.size': 8,       # Size of ticks on Y-axis                                                                                                 \n           'xtick.major.pad': 10.0,     # Distance of axis values from X-axis                                                                                               \n           'ytick.major.pad': 10.0,     # Distance of axis values from Y-axis   \n           'image.cmap': 'viridis'      # Default colormap\n           }                                                                                                                                                                 \n       )","b79b9dff":"class VendingMachine(object):\n    \n    def __init__(self, number=0, igm=0.58):\n        self.number = number\n        self.my_pulls = 0\n        self.opponent_pulls = 0\n        self.candies = 0\n        self.history = []\n        self.information_gain_multiplier = igm\n\n    @property\n    def pulls(self):\n        return self.my_pulls + self.opponent_pulls\n    \n    def opponent_pulled(self):\n        self.opponent_pulls += 1\n        self.history.append(\"O\")\n        \n    def me_pulled(self, success=False):\n        self.my_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"MS\")\n        else:\n            self.history.append(\"MF\")\n            \n    def both_pulled(self, success=False):\n        self.my_pulls += 1\n        self.opponent_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"BS\")\n        else:\n            self.history.append(\"BF\")\n\n    @staticmethod\n    def assume_probability_from_history(history):\n        \"\"\"\n        Assumes original probability of reward.\n        \n        As we have a history on a vending machine, we can compute\n        how likely is the current score. We basically go through\n        all possible starting probabilities (for cycle hidden\n        in linspace) and the probability that is most likely to \n        give the current history of pulls is probably the \n        original probability.\n        \n        I think we could also estimate an opponent's score using\n        this. I don't know for what it may be good, but \n        the possibility is interesting.\n        \"\"\"\n        #if history:\n        base = np.linspace(0, 1, 101)\n        decay = 0.97\n        probabilities = np.ones(101)\n        current_decay = 0.97\n        for event in history:\n            if event == \"O\":\n                current_decay *= decay\n            elif event == \"BS\":\n                probabilities *= (base*current_decay)\n                current_decay *= decay**2\n            elif event == \"BF\":\n                probabilities *= (1-(base*current_decay))\n                current_decay *= decay**2\n            elif event == \"MS\":\n                probabilities *= (base*current_decay)\n                current_decay *= decay\n            elif event == \"MF\":\n                probabilities *= (1-(base*current_decay))\n                current_decay *= decay\n        return np.argmax(probabilities)\n        #else:\n        #    return 50\n            \n    @property\n    def assumed_original_probability(self):\n        return self.assume_probability_from_history(self.history)\n    \n    @property\n    def assumed_current_probability(self):\n        return self.assumed_original_probability*0.97**self.pulls\n    \n    @staticmethod\n    def compute_information_gain(history):\n        # base_probability = self.assume_probability_from_history(history)\n        lose_probability = VendingMachine.assume_probability_from_history(history + [\"MF\"])\n        win_probability = VendingMachine.assume_probability_from_history(history + [\"MS\"])\n        return win_probability - lose_probability\n    \n    @property\n    def information_gain(self):\n        \"\"\"\n        I would like to somehow measure how much more precise\n        the assumed original probability would get if we pulled \n        the machine. \n        Opponent make moves too, is it really a useful information?\n        \"\"\"\n        return self.compute_information_gain(self.history)\n    \n    @property\n    def score(self):\n        return self.information_gain * self.information_gain_multiplier + self.assumed_current_probability","e7ebe08a":"class TrainingMachine(VendingMachine):\n    \n    def __init__(self, probability):\n        super().__init__()\n        self.probability = probability\n        self.ig_history = [self.information_gain]\n        self.aop_history = [self.assumed_original_probability]\n        self.acp_history = [self.assumed_current_probability]\n        self.markers = [\"s\"]\n    \n    def pull(self, me=True):\n        candy = random.random() < self.probability\n        self.probability *= 0.97\n        if me:\n            self.me_pulled(success=candy)\n            if candy:\n                self.markers.append(\"^\")\n            else:\n                self.markers.append(\"v\")\n        else:\n            self.opponent_pulled()\n            self.markers.append(\"s\")\n        self.ig_history.append(self.information_gain)\n        self.aop_history.append(self.assumed_original_probability)\n        self.acp_history.append(self.assumed_current_probability)\n        return candy\n    \n    def plot_sum(self, ax, label=\"\"):\n        s = np.array(self.ig_history)*0.61+np.array(self.acp_history)\n        x = np.array(list(range(len(s))))\n        m = np.array(self.markers)\n        ax.plot(x, s, \"-\", label=label)\n        for marker_type, color in zip([\"^\", \"v\", \"s\"], [\"green\", \"red\", \"blue\"]):\n            ax.scatter(x[m==marker_type], s[m==marker_type], marker=marker_type, color=color)","6d8b2f79":"tr2 = TrainingMachine(0.2)\ntr5 = TrainingMachine(0.5)\ntr8 = TrainingMachine(0.8)","82ea94e8":"for i in range(2):\n    tr2.pull(me=False)\n    tr5.pull(me=False)\n    tr8.pull(me=False)\nfor i in range(100):\n    tr2.pull()\n    tr5.pull()\n    tr8.pull()","ce930b16":"fig, ax = plt.subplots()\ntr2.plot_sum(ax, label=\"20\")\ntr5.plot_sum(ax, label=\"50\")\ntr8.plot_sum(ax, label=\"80\")\nplt.legend()\nplt.show()","0cbaa657":"class AlarmClock(object):\n    \n    def __init__(self, number, time):\n        self.number = number\n        self.time = time\n        \n    def tick(self):\n        self.time = np.max([self.time-1, 0])\n        \n    @property\n    def ready(self):\n        if self.time == 0:\n            return True\n        else:\n            return False","fd097358":"class Plan(object):\n    \n    def __init__(self):\n        self.list = list()\n        \n    def exists(self):\n        if self.list:\n            return True\n        else:\n            return False\n    \n    def tick(self):\n        for clock in self.list:\n            clock.tick()\n    \n    def add(self, number, time):\n        self.list.append(AlarmClock(number, time))\n    \n    def get_readied_machine(self):\n        clock_statuses = [clock.ready for clock in self.list]\n        selected_index = random.choice(np.argwhere(clock_statuses == np.amax(clock_statuses)).flatten())\n        selected_clock = self.list.pop(selected_index)\n        return selected_clock.number","5c4fa982":"class BreakRoomPlan(object):\n\n    def __init__(self, N_machines):\n        self.machines = [VendingMachine(i) for i in range(N_machines)]\n        self.plan = Plan()\n        \n    def get_indices_of_best_scores(self):\n        scores = [machine.score for machine in self.machines]\n        return list(np.argwhere(scores == np.amax(scores)).flatten())\n    \n    def get_machine(self):\n        if self.plan.exists():\n            self.plan.get_readied_machine()\n        best_machines = self.get_indices_of_best_scores()\n        selected = random.choice(best_machines)\n        if self.machines[selected].my_pulls == 0:\n            self.plan.add(selected, 3)\n        self.plan.tick()\n        return selected","4544e123":"%%writefile random_elf.py\nimport random\n\ndef random_elf(observation, configuration):\n    return random.randint(0, 99)","cf104197":"%%writefile gabriel.py\nimport random\nimport copy\n\ngood = list()\nreward = 0\n\ndef gabriel(observation, configuration):\n    global good, reward\n    step = observation[\"step\"]\n    if step > 0 and step < 100:\n        my_index = observation[\"agentIndex\"]\n        opponent_index = 0 if my_index == 1 else 1\n        my_action = observation[\"lastActions\"][my_index]\n        opponent_action = observation[\"lastActions\"][opponent_index]\n        current_total = observation[\"reward\"]\n        candy_gained = False if reward == current_total else True\n        reward = copy.deepcopy(current_total) \n        if candy_gained:\n            good.append(my_action)\n        return step\n    elif step==0:\n        return 0\n    else:\n        if good:\n            return good.pop(0)\n        else:\n            return random.randint(0, 99)","6c90541c":"def first_elf(observation, configuration):\n    global brp, reward\n    step = observation[\"step\"]\n    if step > 0:\n        my_index = observation[\"agentIndex\"]\n        opponent_index = 0 if my_index == 1 else 1\n        my_action = observation[\"lastActions\"][my_index]\n        opponent_action = observation[\"lastActions\"][opponent_index]\n        current_total = observation[\"reward\"]\n        candy_gained = False if reward == current_total else True\n        reward = copy.deepcopy(current_total)\n        if my_action != opponent_action:\n            brp.machines[my_action].me_pulled(success=candy_gained)\n            brp.machines[opponent_action].opponent_pulled()\n        elif my_action == opponent_action:\n            brp.machines[my_action].both_pulled(success=candy_gained)\n        brp.opponent_history.append(opponent_action)\n        brp.evaluate_opponent(opponent_action)\n        action = brp.get_machine()\n        return int(action)\n    else:\n        action = brp.get_machine()\n        return int(action)","1584becd":"%%writefile first_elf.py\nimport random\nimport copy\n\nimport numpy as np\n\n\nclass VendingMachine(object):\n    \n    def __init__(self, number=0, igm=0.58):\n        self.number = number\n        self.my_pulls = 0\n        self.opponent_pulls = 0\n        self.candies = 0\n        self.history = []\n        self.information_gain_multiplier = igm\n\n    @property\n    def pulls(self):\n        return self.my_pulls + self.opponent_pulls\n    \n    def opponent_pulled(self):\n        self.opponent_pulls += 1\n        self.history.append(\"O\")\n        \n    def me_pulled(self, success=False):\n        self.my_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"MS\")\n        else:\n            self.history.append(\"MF\")\n            \n    def both_pulled(self, success=False):\n        self.my_pulls += 1\n        self.opponent_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"BS\")\n        else:\n            self.history.append(\"BF\")\n\n    @staticmethod\n    def assume_probability_from_history(history):\n        \"\"\"\n        Assumes original probability of reward.\n        \n        As we have a history on a vending machine, we can compute\n        how likely is the current score. We basically go through\n        all possible starting probabilities (for cycle hidden\n        in linspace) and the probability that is most likely to \n        give the current history of pulls is probably the \n        original probability.\n        \n        I think we could also estimate an opponent's score using\n        this. I don't know for what it may be good, but \n        the possibility is interesting.\n        \"\"\"\n        base = np.linspace(0, 1, 101)\n        decay = 0.97\n        probabilities = np.ones(101)\n        current_decay = 0.97\n        for event in history:\n            if event == \"O\":\n                current_decay *= decay\n            elif event == \"BS\":\n                probabilities *= (base*current_decay)\n                current_decay *= decay**2\n            elif event == \"BF\":\n                probabilities *= (1-(base*current_decay))\n                current_decay *= decay**2\n            elif event == \"MS\":\n                probabilities *= (base*current_decay)\n                current_decay *= decay\n            elif event == \"MF\":\n                probabilities *= (1-(base*current_decay))\n                current_decay *= decay\n        return np.argmax(probabilities)\n            \n    @property\n    def assumed_original_probability(self):\n        return self.assume_probability_from_history(self.history)\n    \n    @property\n    def assumed_current_probability(self):\n        return self.assumed_original_probability*0.97**self.pulls\n    \n    @staticmethod\n    def compute_information_gain(history):\n        lose_probability = VendingMachine.assume_probability_from_history(history + [\"MF\"])\n        win_probability = VendingMachine.assume_probability_from_history(history + [\"MS\"])\n        return win_probability - lose_probability\n    \n    @property\n    def information_gain(self):\n        \"\"\"\n        I would like to somehow measure how much more precise\n        the assumed original probability would get if we pulled \n        the machine. \n        Opponent make moves too, is it really a useful information?\n        \"\"\"\n        return self.compute_information_gain(self.history)\n    \n    @property\n    def score(self):\n        return self.information_gain * self.information_gain_multiplier + self.assumed_current_probability\n    \nclass AlarmClock(object):\n    \n    def __init__(self, number, time):\n        self.number = number\n        self.time = time\n        \n    def tick(self):\n        self.time = np.max([self.time-1, 0])\n        \n    @property\n    def ready(self):\n        if self.time == 0:\n            return True\n        else:\n            return False\n    \n    def __repr__(self):\n        return f\"Clock - number {self.number} - time {self.time}\"\n        \nclass Plan(object):\n    \n    def __init__(self):\n        self.list = list()\n        \n    def exists(self):\n        if np.array([clock.ready for clock in self.list]).any():\n            return True\n        else:\n            return False\n    \n    def tick(self):\n        for clock in self.list:\n            clock.tick()\n    \n    def add(self, number, time):\n        clock_already_in_plan = False\n        for clock in self.list:\n            if clock.number == number:\n                clock_already_in_plan = True\n                clock.number = number\n        if not clock_already_in_plan:\n            self.list.append(AlarmClock(number, time))\n    \n    def get_random_ready(self):\n        clock_statuses = [clock.ready for clock in self.list]\n        selected_index = random.choice(np.argwhere(clock_statuses == np.amax(clock_statuses)).flatten())\n        selected_clock = self.list.pop(selected_index)\n        return selected_clock.number\n    \n    def get_all_ready(self):\n        readied_clocks = []\n        new_list = []\n        for i in range(len(self.list)):\n            clock = self.list.pop(0)\n            if clock.ready:\n                readied_clocks.append(clock.number)\n            else:\n                new_list.append(clock)\n        self.list = copy.deepcopy(new_list)\n        return readied_clocks\n        \nclass BreakRoomPlan(object):\n\n    def __init__(self, N_machines):\n        self.machines = [VendingMachine(i) for i in range(N_machines)]\n        self.plan = Plan()\n        self.forbidden_plan = Plan()\n        self.forbidden = []\n        self.opponent_history = []\n        \n    @property\n    def history(self):\n        return len(self.opponent_history)\n        \n    def get_indices_sorted_by_scores(self):\n        \"\"\"\n        Return indices of machines sorted by scores.\n        \n        This is a pretty general function - sort list of\n        values \n        \"\"\"\n        scores = list()\n        indices = list()\n        for machine in self.machines:\n            scores.append(machine.score)\n            indices.append(machine.number)\n        zipped = list(zip(indices, scores))\n        random.shuffle(zipped)\n        indices, scores = zip(*zipped)\n        sorted_indices = [index for index, _ in sorted(zip(indices,scores), key=lambda pair: pair[1])]\n        return sorted_indices\n    \n    def get_machine(self):\n        if self.plan.exists():\n            selected = self.plan.get_random_ready()\n        else:\n            best_machines = self.get_indices_sorted_by_scores()\n            for index in best_machines:\n                if index not in self.forbidden:\n                    selected = index\n        self.plan.tick()\n        self.forbidden_plan.tick()\n        for machine in self.forbidden_plan.get_all_ready():\n            self.forbidden.remove(machine)\n        if self.machines[selected].my_pulls == 0:\n            self.plan.add(selected, 3)\n        return selected\n    \n    def evaluate_opponent(self, action):\n        machine = self.machines[action]\n        # In the beginning I would like to try a machine if opponent pulls it a lot\n        if machine.opponent_pulls == 4 and machine.my_pulls == 0:\n            self.plan.add(action, 1)\n        \n\nreward = 0\nbrp = BreakRoomPlan(100)\n\ndef first_elf(observation, configuration):\n    global brp, reward\n    step = observation[\"step\"]\n    if step > 0:\n        my_index = observation[\"agentIndex\"]\n        opponent_index = 0 if my_index == 1 else 1\n        my_action = observation[\"lastActions\"][my_index]\n        opponent_action = observation[\"lastActions\"][opponent_index]\n        current_total = observation[\"reward\"]\n        candy_gained = False if reward == current_total else True\n        reward = copy.deepcopy(current_total)\n        if my_action != opponent_action:\n            brp.machines[my_action].me_pulled(success=candy_gained)\n            brp.machines[opponent_action].opponent_pulled()\n        elif my_action == opponent_action:\n            brp.machines[my_action].both_pulled(success=candy_gained)\n        brp.opponent_history.append(opponent_action)\n        brp.evaluate_opponent(opponent_action)\n        action = brp.get_machine()\n        return int(action)\n    else:\n        action = brp.get_machine()\n        return int(action)","ede41b49":"%%writefile second_elf.py\nimport random\nimport copy\n\nimport numpy as np\n\n\nclass VendingMachine(object):\n    \n    def __init__(self, number=0, igm=0.58):\n        self.number = number\n        self.my_pulls = 0\n        self.opponent_pulls = 0\n        self.candies = 0\n        self.history = []\n        self.information_gain_multiplier = igm\n\n    @property\n    def pulls(self):\n        return self.my_pulls + self.opponent_pulls\n    \n    def opponent_pulled(self):\n        self.opponent_pulls += 1\n        self.history.append(\"O\")\n        \n    def me_pulled(self, success=False):\n        self.my_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"MS\")\n        else:\n            self.history.append(\"MF\")\n            \n    def both_pulled(self, success=False):\n        self.my_pulls += 1\n        self.opponent_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"BS\")\n        else:\n            self.history.append(\"BF\")\n\n    @staticmethod\n    def assume_probability_from_history(history):\n        \"\"\"\n        Assumes original probability of reward.\n        \n        As we have a history on a vending machine, we can compute\n        how likely is the current score. We basically go through\n        all possible starting probabilities (for cycle hidden\n        in linspace) and the probability that is most likely to \n        give the current history of pulls is probably the \n        original probability.\n        \n        I think we could also estimate an opponent's score using\n        this. I don't know for what it may be good, but \n        the possibility is interesting.\n        \"\"\"\n        base = np.linspace(0, 1, 101)\n        decay = 0.97\n        probabilities = np.ones(101)\n        current_decay = 0.97\n        for event in history:\n            if event == \"O\":\n                current_decay *= decay\n            elif event == \"BS\":\n                probabilities *= (base*current_decay)\n                current_decay *= decay**2\n            elif event == \"BF\":\n                probabilities *= (1-(base*current_decay))\n                current_decay *= decay**2\n            elif event == \"MS\":\n                probabilities *= (base*current_decay)\n                current_decay *= decay\n            elif event == \"MF\":\n                probabilities *= (1-(base*current_decay))\n                current_decay *= decay\n        return np.argmax(probabilities)\n            \n    @property\n    def assumed_original_probability(self):\n        return self.assume_probability_from_history(self.history)\n    \n    @property\n    def assumed_current_probability(self):\n        return self.assumed_original_probability*0.97**self.pulls\n    \n    @staticmethod\n    def compute_information_gain(history):\n        lose_probability = VendingMachine.assume_probability_from_history(history + [\"MF\"])\n        win_probability = VendingMachine.assume_probability_from_history(history + [\"MS\"])\n        return win_probability - lose_probability\n    \n    @property\n    def information_gain(self):\n        \"\"\"\n        I would like to somehow measure how much more precise\n        the assumed original probability would get if we pulled \n        the machine. \n        Opponent make moves too, is it really a useful information?\n        \"\"\"\n        return self.compute_information_gain(self.history)\n    \n    @property\n    def score(self):\n        return self.information_gain * self.information_gain_multiplier + self.assumed_current_probability\n    \nclass AlarmClock(object):\n    \n    def __init__(self, number, time):\n        self.number = number\n        self.time = time\n        \n    def tick(self):\n        self.time = np.max([self.time-1, 0])\n        \n    @property\n    def ready(self):\n        if self.time == 0:\n            return True\n        else:\n            return False\n    \n    def __repr__(self):\n        return f\"Clock - number {self.number} - time {self.time}\"\n        \nclass Plan(object):\n    \n    def __init__(self):\n        self.list = list()\n        \n    def exists(self):\n        if np.array([clock.ready for clock in self.list]).any():\n            return True\n        else:\n            return False\n    \n    def tick(self):\n        for clock in self.list:\n            clock.tick()\n    \n    def add(self, number, time):\n        clock_already_in_plan = False\n        for clock in self.list:\n            if clock.number == number:\n                clock_already_in_plan = True\n                clock.number = number\n        if not clock_already_in_plan:\n            self.list.append(AlarmClock(number, time))\n    \n    def get_random_ready(self):\n        clock_statuses = [clock.ready for clock in self.list]\n        selected_index = random.choice(np.argwhere(clock_statuses == np.amax(clock_statuses)).flatten())\n        selected_clock = self.list.pop(selected_index)\n        return selected_clock.number\n    \n    def get_all_ready(self):\n        readied_clocks = []\n        new_list = []\n        for i in range(len(self.list)):\n            clock = self.list.pop(0)\n            if clock.ready:\n                readied_clocks.append(clock.number)\n            else:\n                new_list.append(clock)\n        self.list = copy.deepcopy(new_list)\n        return readied_clocks\n        \nclass BreakRoomPlan(object):\n\n    def __init__(self, N_machines):\n        self.machines = [VendingMachine(i) for i in range(N_machines)]\n        self.plan = Plan()\n        self.forbidden_plan = Plan()\n        self.forbidden = []\n        self.opponent_history = []\n        \n    @property\n    def history(self):\n        return len(self.opponent_history)\n        \n    def get_indices_sorted_by_scores(self):\n        \"\"\"\n        Return indices of machines sorted by scores.\n        \n        This is a pretty general function - sort list of\n        values \n        \"\"\"\n        scores = list()\n        indices = list()\n        for machine in self.machines:\n            scores.append(machine.score)\n            indices.append(machine.number)\n        zipped = list(zip(indices, scores))\n        random.shuffle(zipped)\n        indices, scores = zip(*zipped)\n        sorted_indices = [index for index, _ in sorted(zip(indices,scores), key=lambda pair: pair[1])]\n        return sorted_indices\n    \n    def get_machine(self):\n        if self.plan.exists():\n            selected = self.plan.get_random_ready()\n        else:\n            best_machines = self.get_indices_sorted_by_scores()\n            for index in best_machines:\n                if index not in self.forbidden:\n                    selected = index\n            self.forbidden.append(selected)\n            self.forbidden_plan.add(selected, 2)\n        self.plan.tick()\n        self.forbidden_plan.tick()\n        for machine in self.forbidden_plan.get_all_ready():\n            self.forbidden.remove(machine)\n        if self.machines[selected].my_pulls == 0:\n            self.plan.add(selected, 3)\n        return selected\n    \n    def evaluate_opponent(self, action):\n        machine = self.machines[action]\n        # In the beginning I would like to try a machine if opponent pulls it a lot\n        if machine.opponent_pulls == 4 and machine.my_pulls == 0:\n            self.plan.add(action, 1)\n        \n\nreward = 0\nbrp = BreakRoomPlan(100)\n\ndef second_elf(observation, configuration):\n    global brp, reward\n    step = observation[\"step\"]\n    if step > 0:\n        my_index = observation[\"agentIndex\"]\n        opponent_index = 0 if my_index == 1 else 1\n        my_action = observation[\"lastActions\"][my_index]\n        opponent_action = observation[\"lastActions\"][opponent_index]\n        current_total = observation[\"reward\"]\n        candy_gained = False if reward == current_total else True\n        reward = copy.deepcopy(current_total)\n        if my_action != opponent_action:\n            brp.machines[my_action].me_pulled(success=candy_gained)\n            brp.machines[opponent_action].opponent_pulled()\n        elif my_action == opponent_action:\n            brp.machines[my_action].both_pulled(success=candy_gained)\n        brp.opponent_history.append(opponent_action)\n        brp.evaluate_opponent(opponent_action)\n        action = brp.get_machine()\n        return int(action)\n    else:\n        action = brp.get_machine()\n        return int(action)","e60af6fe":"%%writefile third_elf.py\nimport random\nimport copy\n\nfrom collections import defaultdict\n\nimport numpy as np\n\nclass VendingMachine(object):\n    \n    def __init__(self, number=0, igm=0.58):\n        self.number = number\n        self.my_pulls = 0\n        self.opponent_pulls = 0\n        self.candies = 0\n        self.history = []\n        self.information_gain_multiplier = igm\n\n    @property\n    def pulls(self):\n        return self.my_pulls + self.opponent_pulls\n    \n    @property\n    def known_pulls(self):\n        return None\n    \n    def opponent_pulled(self):\n        self.opponent_pulls += 1\n        self.history.append(\"O?\")\n        \n    def me_pulled(self, success=False):\n        self.my_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"MS\")\n        else:\n            self.history.append(\"MF\")\n            \n    def both_pulled(self, success=False):\n        self.my_pulls += 1\n        self.opponent_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"BS\")\n        else:\n            self.history.append(\"BF\")\n\n    @staticmethod\n    def assume_probability_from_history(history):\n        \"\"\"\n        Assumes original probability of reward.\n        \n        As we have a history on a vending machine, we can compute\n        how likely is the current score. We basically go through\n        all possible starting probabilities (for cycle hidden\n        in linspace) and the probability that is most likely to \n        give the current history of pulls is probably the \n        original probability.\n        \n        I think we could also estimate an opponent's score using\n        this. I don't know for what it may be good, but \n        the possibility is interesting.\n        \"\"\"\n        base = np.linspace(0, 1, 101)\n        decay = 0.97\n        probabilities = np.ones(101)\n        current_decay = 0.97\n        for event in history:\n            if event == \"O?\":\n                current_decay *= decay\n            elif event == \"MS\" or event == \"OS\":\n                probabilities *= (base*current_decay)\n                current_decay *= decay\n            elif event == \"MF\" or event == \"OF\":\n                probabilities *= (1-(base*current_decay))\n                current_decay *= decay\n            elif event[0] == \"B\":\n                for letter in event[1:]:\n                    if letter == \"?\":\n                        pass\n                    elif letter == \"S\":\n                        probabilities *= (base*current_decay)\n                    elif letter == \"F\":\n                        probabilities *= (1-(base*current_decay))\n                current_decay *= decay**2\n        return np.argmax(probabilities)\n            \n    @property\n    def assumed_original_probability(self):\n        return self.assume_probability_from_history(self.history)\n    \n    @property\n    def assumed_current_probability(self):\n        return self.assumed_original_probability*0.97**self.pulls\n    \n    @staticmethod\n    def compute_information_gain(history):\n        lose_probability = VendingMachine.assume_probability_from_history(history + [\"MF\"])\n        win_probability = VendingMachine.assume_probability_from_history(history + [\"MS\"])\n        return win_probability - lose_probability\n    \n    @property\n    def information_gain(self):\n        \"\"\"\n        I would like to somehow measure how much more precise\n        the assumed original probability would get if we pulled \n        the machine. \n        Opponent make moves too, is it really a useful information?\n        \"\"\"\n        return self.compute_information_gain(self.history)\n    \n    @property\n    def score(self):\n        return self.information_gain * self.information_gain_multiplier + self.assumed_current_probability\n    \n    def reset_opponent_history(self):\n        for i, event in enumerate(self.history):\n            if event[0] == \"O\":\n                self.history[i] = \"O?\"\n    \nclass AlarmClock(object):\n    \n    def __init__(self, number, time):\n        self.number = number\n        self.time = time\n        \n    def tick(self):\n        self.time = np.max([self.time-1, 0])\n        \n    @property\n    def ready(self):\n        if self.time == 0:\n            return True\n        else:\n            return False\n    \n    def __repr__(self):\n        return f\"Clock - number {self.number} - time {self.time}\"\n        \nclass Plan(object):\n    \n    def __init__(self):\n        self.list = list()\n        self.number = None\n        \n    def exists(self):\n        return True if self.number else False\n        \n    def readied_exists(self):\n        if np.array([clock.ready for clock in self.list]).any():\n            return True\n        else:\n            return False\n        \n    def force(self, number):\n        self.number = number\n    \n    def tick(self):\n        for clock in self.list:\n            clock.tick()\n            \n    def tick_clock(self, number):\n        for clock in self.list:\n            if clock.number == number:\n                clock.tick()\n    \n    def add(self, number, time):\n        clock_already_in_plan = False\n        for clock in self.list:\n            if clock.number == number:\n                clock_already_in_plan = True\n                clock.number = number\n        if not clock_already_in_plan:\n            self.list.append(AlarmClock(number, time))\n    \n    def get_machine_number(self):\n        number = self.number\n        self.number = None\n        return number\n    \n    def get_random_ready(self):\n        clock_statuses = [clock.ready for clock in self.list]\n        selected_index = random.choice(np.argwhere(clock_statuses == np.amax(clock_statuses)).flatten())\n        selected_clock = self.list.pop(selected_index)\n        return selected_clock.number\n    \n    def get_all_ready(self):\n        readied_clocks = []\n        new_list = []\n        for i in range(len(self.list)):\n            clock = self.list.pop(0)\n            if clock.ready:\n                readied_clocks.append(clock.number)\n            else:\n                new_list.append(clock)\n        self.list = copy.deepcopy(new_list)\n        return readied_clocks\n\ndef get_index_of_nth_repeating(list_of_values, nth_repeating=1, repeating_for_time=2):\n    counter = defaultdict(lambda: 0)\n    numbers_repeated = 0\n    for i, value in enumerate(list_of_values):\n        counter[value] += 1\n        if counter[value] == repeating_for_time:\n            numbers_repeated += 1\n            if numbers_repeated == nth_repeating:\n                return i\n    return None   \n\nclass Strategy(object):\n        \n    def is_applicable(self):\n        raise NotImplementedError(\"\")\n        \n    def apply(self):\n        raise NotImplementedError(\"\")    \n\nclass Gabriel(Strategy):\n    \n    def __init__(self):\n        self.min_steps = 85\n        self.good = []\n        self.bad = []\n    \n    def is_applicable(self, history):\n        numbers = copy.deepcopy(history)\n        if len(numbers) > self.min_steps:\n            for i in range(1, 5):\n                second = get_index_of_nth_repeating(numbers, 1, 2)\n                if second is None:\n                    break\n                elif second > self.min_steps:\n                    third = get_index_of_nth_repeating(numbers, 1, 3)\n                    third = third if third is not None else -1\n                    self.good = numbers[second:third]\n                    indices = [numbers.index(x) for x in self.good]\n                    if indices == sorted(indices) and len(indices) > 1:\n                        self.bad = [x for x in numbers[:indices[-1]] if x not in self.good]\n                    return True\n                else:\n                    value = numbers[second]\n                    numbers = [x for x in numbers if x != value]\n        return False\n                    \n    def apply(self, machines):\n        for number in self.good:\n            for position, result in machines[number].history:\n                if result[0] == \"O\" or result[0] == \"B\":\n                    machines[number].history[position] = result[:-1] + \"S\"\n        for number in self.bad:\n            for position, result in machines[number].history:\n                if result[0] == \"O\" or result[0] == \"B\":\n                    machines[number].history[position] = result[:-1] + \"F\"\n    \nclass FourInSix(Strategy):\n        \n    def __init__(self):\n        self.good = []\n        \n    def is_applicable(self, history):\n        already_counted_numbers = set()\n        self.good = []\n        for i, number in enumerate(history):\n            if number not in already_counted_numbers:\n                if history[i:i+6].count(number) >= 4:\n                    self.good.append(number)\n                already_counted_numbers.add(number)        \n        return True\n    \n    def apply(self, machines):\n        for number in self.good:\n            counted = 0\n            for position, result in machines[number].history:\n                if result[0] == \"O\" or result[0] == \"B\":\n                    if counted < 3:\n                        machines[number].history[position] = result[:-1] + \"S\"\n                        counted += 1\n    \nclass VegasGambler(Strategy):\n    \n    def __init__(self):\n        self.min_groups = 90\n        self.good = {}\n        \n    def is_applicable(self, history):\n        grouped = []\n        counted = []\n        previous_number = history[0]\n        s = 1\n        for number in history[1:]:\n            if number == previous_number:\n                s += 1\n            else:\n                grouped.append(previous_number)\n                counted.append(s)\n                s = 1\n                previous_number = number\n        grouped.append(previous_number)\n        counted.append(s)\n        until = get_index_of_nth_repeating(grouped)\n        #TODO: Compute statistical criteria here\n        if until is not None and until > self.min_groups and max(counted) > 3:\n            self.good = dict(zip(grouped[:until], counted[:until]))\n            return True\n        else:\n            return False\n\n    def apply(self, machines):\n        for number, good_count in self.good.items():\n            counted = 0\n            for position, result in machines[number].history:\n                if result[0] == \"O\" or result[0] == \"B\":\n                    if counted < good_count:\n                        machines[number].history[position] = result[:-1] + \"S\"\n                        counted += 1\n                    elif counted == good_count:\n                        machines[number].history[position] = result[:-1] + \"F\"\n                        counted += 1\n    \nclass Opponent(object):\n    \n    def __init__(self):\n        self.history = []\n        self.forced_number = None\n        self.applied_strategy = None\n        self.known_strategies = [Gabriel(), FourInSix(), VegasGambler()]\n        \n    def evaluate_history(self, machines):\n        if len(self.history) >= 2:\n            if self.history[-1] == self.history[-2]:\n                if machines[self.history[-1]].score > 7:\n                    self.forced_number = self.history[-1]\n        applied_strategies = [strategy for strategy in self.known_strategies if strategy.is_applicable(self.history)]\n        for machine in machines:\n            machine.reset_opponent_history()\n        # Change probabilities according to strategies\n        for strategy in applied_strategies:\n            strategy.apply(machines)\n        \n    def get_forced_number(self):\n        forced_number = self.forced_number\n        self.forced_number = None\n        return forced_number\n    \n    def is_forcing_number(self):\n        return True if self.forced_number else False\n        \nclass BreakRoomPlan(object):\n\n    def __init__(self, N_machines):\n        self.machines = [VendingMachine(i) for i in range(N_machines)]\n        self.plan = Plan()\n        self.forbidden_plan = Plan()\n        self.forbidden = []\n        self.opponent = Opponent()\n        \n    @property\n    def history(self):\n        return len(self.opponent.history)\n        \n    def get_indices_sorted_by_scores(self):\n        \"\"\"\n        Return indices of machines sorted by scores.\n        \n        This is a pretty general function - sort list of\n        values \n        \"\"\"\n        scores = list()\n        indices = list()\n        for machine in self.machines:\n            scores.append(machine.score)\n            indices.append(machine.number)\n        zipped = list(zip(indices, scores))\n        random.shuffle(zipped)\n        indices, scores = zip(*zipped)\n        sorted_indices = [index for index, _ in sorted(zip(indices,scores), key=lambda pair: pair[1])]\n        return sorted_indices\n    \n    def get_machine(self):\n        if self.plan.exists():\n            selected = self.plan.get_machine_number()\n        else:\n            if self.plan.readied_exists():\n                selected = self.plan.get_random_ready()\n            else:\n                best_machines = self.get_indices_sorted_by_scores()\n                for index in best_machines:\n                    if index not in self.forbidden:\n                        selected = index\n                self.forbidden.append(selected)\n                self.forbidden_plan.add(selected, 2)\n        self.plan.tick()\n        self.forbidden_plan.tick()\n        if self.history:\n            self.forbidden_plan.tick_clock(self.opponent.history[-1])\n        for machine in self.forbidden_plan.get_all_ready():\n            self.forbidden.remove(machine)\n        if self.machines[selected].my_pulls == 0:\n            self.plan.add(selected, 3)\n        return selected\n    \n    def evaluate_opponent(self, action):\n        machine = self.machines[action]\n        # In the beginning I would like to try a machine if opponent pulls it a lot\n        self.opponent.evaluate_history(self.machines)\n        if machine.opponent_pulls == 4 and machine.my_pulls == 0:\n            self.plan.add(action, 1)\n        if self.opponent.is_forcing_number():\n            self.plan.number = self.opponent.get_forced_number()\n\nreward = 0\nbrp = BreakRoomPlan(100)\n\ndef third_elf(observation, configuration):\n    global brp, reward\n    step = observation[\"step\"]\n    if step > 0:\n        my_index = observation[\"agentIndex\"]\n        opponent_index = 0 if my_index == 1 else 1\n        my_action = observation[\"lastActions\"][my_index]\n        opponent_action = observation[\"lastActions\"][opponent_index]\n        current_total = observation[\"reward\"]\n        candy_gained = False if reward == current_total else True\n        reward = copy.deepcopy(current_total)\n        if my_action != opponent_action:\n            brp.machines[my_action].me_pulled(success=candy_gained)\n            brp.machines[opponent_action].opponent_pulled()\n        elif my_action == opponent_action:\n            brp.machines[my_action].both_pulled(success=candy_gained)\n        brp.opponent.history.append(opponent_action)\n        brp.evaluate_opponent(opponent_action)\n        action = brp.get_machine()\n        return int(action)\n    else:\n        action = brp.get_machine()\n        return int(action)","a164dcfa":"%%writefile third_elf.py\nimport random\nimport copy\n\nfrom collections import defaultdict\n\nimport numpy as np\n\nclass VendingMachine(object):\n    \n    def __init__(self, number=0, igm=0.58):\n        self.number = number\n        self.my_pulls = 0\n        self.opponent_pulls = 0\n        self.candies = 0\n        self.history = []\n        self.information_gain_multiplier = igm\n\n    @property\n    def pulls(self):\n        return self.my_pulls + self.opponent_pulls\n    \n    @property\n    def known_pulls(self):\n        return None\n    \n    def opponent_pulled(self):\n        self.opponent_pulls += 1\n        self.history.append(\"O?\")\n        \n    def me_pulled(self, success=False):\n        self.my_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"MS\")\n        else:\n            self.history.append(\"MF\")\n            \n    def both_pulled(self, success=False):\n        self.my_pulls += 1\n        self.opponent_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"BS\")\n        else:\n            self.history.append(\"BF\")\n\n    @staticmethod\n    def assume_probability_from_history(history):\n        \"\"\"\n        Assumes original probability of reward.\n        \n        As we have a history on a vending machine, we can compute\n        how likely is the current score. We basically go through\n        all possible starting probabilities (for cycle hidden\n        in linspace) and the probability that is most likely to \n        give the current history of pulls is probably the \n        original probability.\n        \n        I think we could also estimate an opponent's score using\n        this. I don't know for what it may be good, but \n        the possibility is interesting.\n        \"\"\"\n        base = np.linspace(0, 1, 101)\n        decay = 0.97\n        probabilities = np.ones(101)\n        current_decay = 0.97\n        for event in history:\n            if event == \"O?\":\n                current_decay *= decay\n            elif event == \"MS\" or event == \"OS\":\n                probabilities *= (base*current_decay)\n                current_decay *= decay\n            elif event == \"MF\" or event == \"OF\":\n                probabilities *= (1-(base*current_decay))\n                current_decay *= decay\n            elif event[0] == \"B\":\n                for letter in event[1:]:\n                    if letter == \"?\":\n                        pass\n                    elif letter == \"S\":\n                        probabilities *= (base*current_decay)\n                    elif letter == \"F\":\n                        probabilities *= (1-(base*current_decay))\n                current_decay *= decay**2\n        return np.argmax(probabilities)\n            \n    @property\n    def assumed_original_probability(self):\n        return self.assume_probability_from_history(self.history)\n    \n    @property\n    def assumed_current_probability(self):\n        return self.assumed_original_probability*0.97**self.pulls\n    \n    @staticmethod\n    def compute_information_gain(history):\n        lose_probability = VendingMachine.assume_probability_from_history(history + [\"MF\"])\n        win_probability = VendingMachine.assume_probability_from_history(history + [\"MS\"])\n        return win_probability - lose_probability\n    \n    @property\n    def information_gain(self):\n        \"\"\"\n        I would like to somehow measure how much more precise\n        the assumed original probability would get if we pulled \n        the machine. \n        Opponent make moves too, is it really a useful information?\n        \"\"\"\n        return self.compute_information_gain(self.history)\n    \n    @property\n    def score(self):\n        return self.information_gain * self.information_gain_multiplier + self.assumed_current_probability\n    \n    def reset_opponent_history(self):\n        for i, event in enumerate(self.history):\n            if event[0] == \"O\":\n                self.history[i] = \"O?\"\n    \nclass AlarmClock(object):\n    \n    def __init__(self, number, time):\n        self.number = number\n        self.time = time\n        \n    def tick(self):\n        self.time = np.max([self.time-1, 0])\n        \n    @property\n    def ready(self):\n        if self.time == 0:\n            return True\n        else:\n            return False\n    \n    def __repr__(self):\n        return f\"Clock - number {self.number} - time {self.time}\"\n        \nclass Plan(object):\n    \n    def __init__(self):\n        self.list = list()\n        self.number = None\n        \n    def exists(self):\n        return True if self.number else False\n        \n    def readied_exists(self):\n        if np.array([clock.ready for clock in self.list]).any():\n            return True\n        else:\n            return False\n        \n    def force(self, number):\n        self.number = number\n    \n    def tick(self):\n        for clock in self.list:\n            clock.tick()\n            \n    def tick_clock(self, number):\n        for clock in self.list:\n            if clock.number == number:\n                clock.tick()\n    \n    def add(self, number, time):\n        clock_already_in_plan = False\n        for clock in self.list:\n            if clock.number == number:\n                clock_already_in_plan = True\n                clock.number = number\n        if not clock_already_in_plan:\n            self.list.append(AlarmClock(number, time))\n    \n    def get_machine_number(self):\n        number = self.number\n        self.number = None\n        return number\n    \n    def get_random_ready(self):\n        clock_statuses = [clock.ready for clock in self.list]\n        selected_index = random.choice(np.argwhere(clock_statuses == np.amax(clock_statuses)).flatten())\n        selected_clock = self.list.pop(selected_index)\n        return selected_clock.number\n    \n    def get_all_ready(self):\n        readied_clocks = []\n        new_list = []\n        for i in range(len(self.list)):\n            clock = self.list.pop(0)\n            if clock.ready:\n                readied_clocks.append(clock.number)\n            else:\n                new_list.append(clock)\n        self.list = copy.deepcopy(new_list)\n        return readied_clocks\n\ndef get_index_of_nth_repeating(list_of_values, nth_repeating=1, repeating_for_time=2):\n    counter = defaultdict(lambda: 0)\n    numbers_repeated = 0\n    for i, value in enumerate(list_of_values):\n        counter[value] += 1\n        if counter[value] == repeating_for_time:\n            numbers_repeated += 1\n            if numbers_repeated == nth_repeating:\n                return i\n    return None   \n\nclass Strategy(object):\n        \n    def is_applicable(self):\n        raise NotImplementedError(\"\")\n        \n    def apply(self):\n        raise NotImplementedError(\"\")    \n\nclass Gabriel(Strategy):\n    \n    def __init__(self):\n        self.min_steps = 85\n        self.good = []\n        self.bad = []\n    \n    def is_applicable(self, history):\n        numbers = copy.deepcopy(history)\n        if len(numbers) > self.min_steps:\n            for i in range(1, 5):\n                second = get_index_of_nth_repeating(numbers, 1, 2)\n                if second is None:\n                    break\n                elif second > self.min_steps:\n                    third = get_index_of_nth_repeating(numbers, 1, 3)\n                    third = third if third is not None else -1\n                    self.good = numbers[second:third]\n                    indices = [numbers.index(x) for x in self.good]\n                    if indices == sorted(indices) and len(indices) > 1:\n                        self.bad = [x for x in numbers[:indices[-1]] if x not in self.good]\n                    return True\n                else:\n                    value = numbers[second]\n                    numbers = [x for x in numbers if x != value]\n        return False\n                    \n    def apply(self, machines):\n        for number in self.good:\n            for position, result in machines[number].history:\n                if result[0] == \"O\" or result[0] == \"B\":\n                    machines[number].history[position] = result[:-1] + \"S\"\n        for number in self.bad:\n            for position, result in machines[number].history:\n                if result[0] == \"O\" or result[0] == \"B\":\n                    machines[number].history[position] = result[:-1] + \"F\"\n    \nclass FourInSix(Strategy):\n        \n    def __init__(self):\n        self.good = []\n        \n    def is_applicable(self, history):\n        already_counted_numbers = set()\n        self.good = []\n        for i, number in enumerate(history):\n            if number not in already_counted_numbers:\n                if history[i:i+6].count(number) >= 4:\n                    self.good.append(number)\n                already_counted_numbers.add(number)        \n        return True\n    \n    def apply(self, machines):\n        for number in self.good:\n            counted = 0\n            for position, result in machines[number].history:\n                if result[0] == \"O\" or result[0] == \"B\":\n                    if counted < 3:\n                        machines[number].history[position] = result[:-1] + \"S\"\n                        counted += 1\n    \nclass VegasGambler(Strategy):\n    \n    def __init__(self):\n        self.min_groups = 90\n        self.good = {}\n        \n    def is_applicable(self, history):\n        grouped = []\n        counted = []\n        previous_number = history[0]\n        s = 1\n        for number in history[1:]:\n            if number == previous_number:\n                s += 1\n            else:\n                grouped.append(previous_number)\n                counted.append(s)\n                s = 1\n                previous_number = number\n        grouped.append(previous_number)\n        counted.append(s)\n        until = get_index_of_nth_repeating(grouped)\n        #TODO: Compute statistical criteria here\n        if until is not None and until > self.min_groups and max(counted) > 3:\n            self.good = dict(zip(grouped[:until], counted[:until]))\n            return True\n        else:\n            return False\n\n    def apply(self, machines):\n        for number, good_count in self.good.items():\n            counted = 0\n            for position, result in machines[number].history:\n                if result[0] == \"O\" or result[0] == \"B\":\n                    if counted < good_count:\n                        machines[number].history[position] = result[:-1] + \"S\"\n                        counted += 1\n                    elif counted == good_count:\n                        machines[number].history[position] = result[:-1] + \"F\"\n                        counted += 1\n    \nclass Opponent(object):\n    \n    def __init__(self):\n        self.history = []\n        self.forced_number = None\n        self.applied_strategy = None\n        self.known_strategies = [Gabriel(), FourInSix(), VegasGambler()]\n        \n    def evaluate_history(self, machines):\n        if len(self.history) >= 2:\n            if self.history[-1] == self.history[-2]:\n                if machines[self.history[-1]].score > 7:\n                    self.forced_number = self.history[-1]\n        applied_strategies = [strategy for strategy in self.known_strategies if strategy.is_applicable(self.history)]\n        for machine in machines:\n            machine.reset_opponent_history()\n        # Change probabilities according to strategies\n        for strategy in applied_strategies:\n            strategy.apply(machines)\n        \n    def get_forced_number(self):\n        forced_number = self.forced_number\n        self.forced_number = None\n        return forced_number\n    \n    def is_forcing_number(self):\n        return True if self.forced_number else False\n        \nclass BreakRoomPlan(object):\n\n    def __init__(self, N_machines):\n        self.machines = [VendingMachine(i) for i in range(N_machines)]\n        self.plan = Plan()\n        self.forbidden_plan = Plan()\n        self.forbidden = []\n        self.opponent = Opponent()\n        \n    @property\n    def history(self):\n        return len(self.opponent.history)\n        \n    def get_indices_sorted_by_scores(self):\n        \"\"\"\n        Return indices of machines sorted by scores.\n        \n        This is a pretty general function - sort list of\n        values \n        \"\"\"\n        scores = list()\n        indices = list()\n        for machine in self.machines:\n            scores.append(machine.score)\n            indices.append(machine.number)\n        zipped = list(zip(indices, scores))\n        random.shuffle(zipped)\n        indices, scores = zip(*zipped)\n        sorted_indices = [index for index, _ in sorted(zip(indices,scores), key=lambda pair: pair[1])]\n        return sorted_indices\n    \n    def get_machine(self):\n        if self.plan.exists():\n            selected = self.plan.get_machine_number()\n        else:\n            if self.plan.readied_exists():\n                selected = self.plan.get_random_ready()\n            else:\n                best_machines = self.get_indices_sorted_by_scores()\n                for index in best_machines:\n                    if index not in self.forbidden:\n                        selected = index\n                self.forbidden.append(selected)\n                self.forbidden_plan.add(selected, 2)\n        self.plan.tick()\n        self.forbidden_plan.tick()\n        if self.history:\n            self.forbidden_plan.tick_clock(self.opponent.history[-1])\n        for machine in self.forbidden_plan.get_all_ready():\n            self.forbidden.remove(machine)\n        if self.machines[selected].my_pulls == 0:\n            self.plan.add(selected, 3)\n        return selected\n    \n    def evaluate_opponent(self, action):\n        machine = self.machines[action]\n        # In the beginning I would like to try a machine if opponent pulls it a lot\n        self.opponent.evaluate_history(self.machines)\n        if machine.opponent_pulls == 4 and machine.my_pulls == 0:\n            self.plan.add(action, 1)\n        if self.opponent.is_forcing_number():\n            self.plan.number = self.opponent.get_forced_number()\n\nreward = 0\nbrp = BreakRoomPlan(100)\n\ndef third_elf(observation, configuration):\n    global brp, reward\n    step = observation[\"step\"]\n    if step > 0:\n        my_index = observation[\"agentIndex\"]\n        opponent_index = 0 if my_index == 1 else 1\n        my_action = observation[\"lastActions\"][my_index]\n        opponent_action = observation[\"lastActions\"][opponent_index]\n        current_total = observation[\"reward\"]\n        candy_gained = False if reward == current_total else True\n        reward = copy.deepcopy(current_total)\n        if my_action != opponent_action:\n            brp.machines[my_action].me_pulled(success=candy_gained)\n            brp.machines[opponent_action].opponent_pulled()\n        elif my_action == opponent_action:\n            brp.machines[my_action].both_pulled(success=candy_gained)\n        brp.opponent.history.append(opponent_action)\n        brp.evaluate_opponent(opponent_action)\n        action = brp.get_machine()\n        return int(action)\n    else:\n        action = brp.get_machine()\n        return int(action)","6b8ed5d4":"%%writefile fourth_elf.py\nimport random\nimport copy\n\nfrom collections import defaultdict\n\nimport numpy as np\n\n\n\nclass VendingMachine(object):\n    \n    def __init__(self, number=0, igm=0.58):\n        self.number = number\n        self.my_pulls = 0\n        self.opponent_pulls = 0\n        self.candies = 0\n        self.history = []\n        self.information_gain_multiplier = igm\n\n    @property\n    def pulls(self):\n        return self.my_pulls + self.opponent_pulls\n    \n    @property\n    def known_pulls(self):\n        return None\n    \n    def opponent_pulled(self):\n        self.opponent_pulls += 1\n        self.history.append(\"O?\")\n        \n    def me_pulled(self, success=False):\n        self.my_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"MS\")\n        else:\n            self.history.append(\"MF\")\n            \n    def both_pulled(self, success=False):\n        self.my_pulls += 1\n        self.opponent_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"BS\")\n        else:\n            self.history.append(\"BF\")\n\n    @staticmethod\n    def assume_probability_from_history(history):\n        \"\"\"\n        Assumes original probability of reward.\n        \n        As we have a history on a vending machine, we can compute\n        how likely is the current score. We basically go through\n        all possible starting probabilities (for cycle hidden\n        in linspace) and the probability that is most likely to \n        give the current history of pulls is probably the \n        original probability.\n        \n        I think we could also estimate an opponent's score using\n        this. I don't know for what it may be good, but \n        the possibility is interesting.\n        \"\"\"\n        base = np.linspace(0, 1, 101)\n        decay = 0.97\n        probabilities = np.ones(101)\n        current_decay = 0.97\n        for event in history:\n            if event == \"O?\":\n                current_decay *= decay\n            elif event == \"MS\" or event == \"OS\":\n                probabilities *= (base*current_decay)\n                current_decay *= decay\n            elif event == \"MF\" or event == \"OF\":\n                probabilities *= (1-(base*current_decay))\n                current_decay *= decay\n            elif event[0] == \"B\":\n                for letter in event[1:]:\n                    if letter == \"?\":\n                        pass\n                    elif letter == \"S\":\n                        probabilities *= (base*current_decay)\n                    elif letter == \"F\":\n                        probabilities *= (1-(base*current_decay))\n                current_decay *= decay**2\n        return np.argmax(probabilities)\n            \n    @property\n    def assumed_original_probability(self):\n        return self.assume_probability_from_history(self.history)\n    \n    @property\n    def assumed_current_probability(self):\n        return self.assumed_original_probability*0.97**self.pulls\n    \n    @staticmethod\n    def compute_information_gain(history):\n        lose_probability = VendingMachine.assume_probability_from_history(history + [\"MF\"])\n        win_probability = VendingMachine.assume_probability_from_history(history + [\"MS\"])\n        return win_probability - lose_probability\n    \n    @property\n    def information_gain(self):\n        \"\"\"\n        I would like to somehow measure how much more precise\n        the assumed original probability would get if we pulled \n        the machine. \n        Opponent make moves too, is it really a useful information?\n        \"\"\"\n        return self.compute_information_gain(self.history)\n    \n    @property\n    def score(self):\n        return self.information_gain * self.information_gain_multiplier + self.assumed_current_probability\n    \n    def reset_opponent_history(self):\n        for i, event in enumerate(self.history):\n            if event[0] == \"O\":\n                self.history[i] = \"O?\"\n            elif event[0] == \"B\":\n                self.history[i] = self.history[i][:-1] + \"?\"\n\n                \n                \nclass AlarmClock(object):\n    \n    def __init__(self, number, time):\n        self.number = number\n        self.time = time\n        \n    def tick(self):\n        self.time = np.max([self.time-1, 0])\n        \n    @property\n    def ready(self):\n        if self.time == 0:\n            return True\n        else:\n            return False\n    \n    def __repr__(self):\n        return f\"Clock - number {self.number} - time {self.time}\"\n        \nclass Plan(object):\n    \n    def __init__(self):\n        self.list = list()\n        self.number = None\n        \n    def exists(self):\n        return True if self.number else False\n        \n    def readied_exists(self):\n        if np.array([clock.ready for clock in self.list]).any():\n            return True\n        else:\n            return False\n        \n    def force(self, number):\n        self.number = number\n    \n    def tick(self):\n        for clock in self.list:\n            clock.tick()\n            \n    def tick_clock(self, number):\n        for clock in self.list:\n            if clock.number == number:\n                clock.tick()\n    \n    def add(self, number, time):\n        clock_already_in_plan = False\n        for clock in self.list:\n            if clock.number == number:\n                clock_already_in_plan = True\n                clock.number = number\n        if not clock_already_in_plan:\n            self.list.append(AlarmClock(number, time))\n    \n    def get_machine_number(self):\n        number = self.number\n        self.number = None\n        return number\n    \n    def get_random_ready(self):\n        clock_statuses = [clock.ready for clock in self.list]\n        selected_index = random.choice(np.argwhere(clock_statuses == np.amax(clock_statuses)).flatten())\n        selected_clock = self.list.pop(selected_index)\n        return selected_clock.number\n    \n    def get_all_ready(self):\n        readied_clocks = []\n        new_list = []\n        for i in range(len(self.list)):\n            clock = self.list.pop(0)\n            if clock.ready:\n                readied_clocks.append(clock.number)\n            else:\n                new_list.append(clock)\n        self.list = copy.deepcopy(new_list)\n        return readied_clocks\n\n    \n    \n# Helper functions for strategies\ndef get_index_of_nth_repeating(list_of_values, nth_repeating=1, repeating_for_time=2):\n    counter = defaultdict(lambda: 0)\n    numbers_repeated = 0\n    for i, value in enumerate(list_of_values):\n        counter[value] += 1\n        if counter[value] == repeating_for_time:\n            numbers_repeated += 1\n            if numbers_repeated == nth_repeating:\n                return i\n    return None   \n\n# Implement strategies used by opponents - we need to use as much info from them as possible\nclass Strategy(object):\n        \n    def is_applicable(self):\n        raise NotImplementedError(\"\")\n        \n    def apply(self):\n        raise NotImplementedError(\"\")    \n\nclass Gabriel(Strategy):\n    \"\"\"\n    Gabriel is a simple strategy - at first pull once every machine (sometimes numbers are\n    randomized, sometimes they are sorted) and then pull those machines that gave candy for\n    the first time. We mark all those pulled twice as success the first time they were pulled\n    and the rest as failure.\n    \"\"\"\n    \n    def __init__(self):\n        self.min_steps = 85\n        self.good = []\n        self.bad = []\n    \n    def is_applicable(self, history):\n        numbers = copy.deepcopy(history)\n        if len(numbers) > self.min_steps:\n            for i in range(1, 5):\n                second = get_index_of_nth_repeating(numbers, 1, 2)\n                if second is None:\n                    break\n                elif second > self.min_steps:\n                    third = get_index_of_nth_repeating(numbers, 1, 3)\n                    third = third if third is not None else -1\n                    self.good = numbers[second:third]\n                    indices = [numbers.index(x) for x in self.good]\n                    if indices == sorted(indices) and len(indices) > 1:\n                        self.bad = [x for x in numbers[:indices[-1]] if x not in self.good]\n                    return True\n                else:\n                    value = numbers[second]\n                    numbers = [x for x in numbers if x != value]\n        return False\n                    \n    def apply(self, machines):\n        for number in self.good:\n            for position, result in machines[number].history:\n                if result[0] == \"O\" or result[0] == \"B\":\n                    machines[number].history[position] = result[:-1] + \"S\"\n        for number in self.bad:\n            for position, result in machines[number].history:\n                if result[0] == \"O\" or result[0] == \"B\":\n                    machines[number].history[position] = result[:-1] + \"F\"\n    \nclass VegasGambler(Strategy):\n    \"\"\"\n    VegasGambler pulls the same machine until it gives no candy, then moves\n    to another machine. This way he tries all machines. When they start repeating\n    machines, we can mark all pulls except the last ones for all machines as success\n    and the last pull for each machine as failure. \n    \"\"\"\n    \n    def __init__(self):\n        self.min_groups = 90\n        self.good = {}\n        \n    def is_applicable(self, history):\n        grouped = []\n        counted = []\n        previous_number = history[0]\n        s = 1\n        for number in history[1:]:\n            if number == previous_number:\n                s += 1\n            else:\n                grouped.append(previous_number)\n                counted.append(s)\n                s = 1\n                previous_number = number\n        grouped.append(previous_number)\n        counted.append(s)\n        until = get_index_of_nth_repeating(grouped)\n        #TODO: Compute statistical criteria here\n        if until is not None and until > self.min_groups and max(counted) > 3:\n            self.good = dict(zip(grouped[:until], counted[:until]))\n            return True\n        else:\n            return False\n\n    def apply(self, machines):\n        for number, good_count in self.good.items():\n            counted = 0\n            for position, result in machines[number].history:\n                if result[0] == \"O\" or result[0] == \"B\":\n                    if counted < good_count:\n                        machines[number].history[position] = result[:-1] + \"S\"\n                        counted += 1\n                    elif counted == good_count:\n                        machines[number].history[position] = result[:-1] + \"F\"\n                        counted += 1\n    \nclass Consecutive(Strategy):\n    \"\"\"\n    Consecutive is not really a strategy, but it is default estimator of opponent\n    success for my elves. The purpose of previous basic strategies is to get through\n    a wall of VegasGamblers between 900-1000 points. The third elf got to ninth place,\n    but did not stay there, so the purpose of this strategy is to evaluate top players,\n    so that my elf can stay longer at the top.\n    By downloading statistics for top players, it should be possible to say,\n    that for example if opponent pulls a machine for the first time twice,\n    then in 70 % of cases the first pull is success and the second failure.\n    If we do this for e.g. numbers up to 8, we should be able to get info about opponent\n    success immediately. However, we need to keep in mind that some opponent do not pull\n    a machine two times in a row (this is a feature for my elves as well), so for those\n    we need to estimate the probabilities in a different way. It is also different if\n    my elves pull the machine as well.\n    \"\"\"\n        \n    def __init__(self):\n        self.good = []\n        set_rules = {\n            1: \"F\",\n            2: \"SF\",\n            3: \"SSF\",\n            4: \"SSSF\",\n            5: \"SSSF\",\n            6: \"SSSSFF\",\n            7: \"SSSSSFF\",\n            8: \"SSSSSFF\",\n        }\n        self.basic_rules = defaultdict(lambda: \"SSSSSFF\")\n        for key, value in set_rules.items():\n            self.basic_rules[key] = value\n        \n    def is_applicable(self, history):\n        if len(history) < 3:\n            return False\n        else:\n            already_counted_numbers = set()\n            self.good = {}\n            ones = 0\n            pluses = 0\n            for i, number in enumerate(history):\n                if number not in already_counted_numbers:\n                    counter = 1\n                    for j, next_number in enumerate(history[i:]):\n                        if number == next_number and j < len(history)-1:\n                            counter += 1\n                        else:\n                            self.good[number] = self.basic_rules[counter]\n                            if counter == 1:\n                                ones += 1\n                            else:\n                                pluses += 1\n                            counter = 1\n                            already_counted_numbers.add(number) \n                            break\n            if pluses \/ (ones + pluses) > 0.8:\n                return True\n            else:\n                return False\n    \n    def apply(self, machines):\n        for number, value in self.good.items():\n            counted = 0\n            for position, result in machines[number].history:\n                if result[0] == \"O\" or result[0] == \"B\":\n                    if counted < len(value):\n                        machines[number].history[position] = result[:-1] + value[counted]\n                        counted += 1\n                    else:\n                        break\n                        \n                        \n                        \nclass Opponent(object):\n    \n    def __init__(self):\n        self.history = []\n        self.forced_number = None\n        self.applied_strategy = None\n        self.known_strategies = [Gabriel(), VegasGambler(), Consecutive()]\n        \n    def evaluate_history(self, machines):\n        if len(self.history) >= 2:\n            if self.history[-1] == self.history[-2]:\n                if machines[self.history[-1]].score > 7:\n                    self.forced_number = self.history[-1]\n        applied_strategies = [strategy for strategy in self.known_strategies if strategy.is_applicable(self.history)]\n        for machine in machines:\n            machine.reset_opponent_history()\n        # Change probabilities according to strategies\n        for strategy in applied_strategies:\n            strategy.apply(machines)\n        \n    def get_forced_number(self):\n        forced_number = self.forced_number\n        self.forced_number = None\n        return forced_number\n    \n    def is_forcing_number(self):\n        return True if self.forced_number else False\n        \nclass BreakRoomPlan(object):\n\n    def __init__(self, N_machines):\n        self.machines = [VendingMachine(i) for i in range(N_machines)]\n        self.plan = Plan()\n        self.forbidden_plan = Plan()\n        self.forbidden = []\n        self.opponent = Opponent()\n        \n    @property\n    def history(self):\n        return len(self.opponent.history)\n        \n    def get_indices_sorted_by_scores(self):\n        \"\"\"\n        Return indices of machines sorted by scores.\n        \n        This is a pretty general function - sort list of\n        values \n        \"\"\"\n        scores = list()\n        indices = list()\n        for machine in self.machines:\n            scores.append(machine.score)\n            indices.append(machine.number)\n        zipped = list(zip(indices, scores))\n        random.shuffle(zipped)\n        indices, scores = zip(*zipped)\n        sorted_indices = [index for index, _ in sorted(zip(indices,scores), key=lambda pair: pair[1])]\n        return sorted_indices\n    \n    def get_machine(self):\n        if self.plan.exists():\n            selected = self.plan.get_machine_number()\n        else:\n            if self.plan.readied_exists():\n                selected = self.plan.get_random_ready()\n            else:\n                best_machines = self.get_indices_sorted_by_scores()\n                for index in best_machines:\n                    if index not in self.forbidden:\n                        selected = index\n                self.forbidden.append(selected)\n                self.forbidden_plan.add(selected, 2)\n        self.plan.tick()\n        self.forbidden_plan.tick()\n        if self.history:\n            self.forbidden_plan.tick_clock(self.opponent.history[-1])\n        for machine in self.forbidden_plan.get_all_ready():\n            self.forbidden.remove(machine)\n        if self.machines[selected].my_pulls == 0:\n            self.plan.add(selected, 3)\n        return selected\n    \n    def evaluate_opponent(self, action):\n        machine = self.machines[action]\n        # In the beginning I would like to try a machine if opponent pulls it a lot\n        self.opponent.evaluate_history(self.machines)\n        if machine.opponent_pulls == 4 and machine.my_pulls == 0:\n            self.plan.add(action, 1)\n        if self.opponent.is_forcing_number():\n            self.plan.number = self.opponent.get_forced_number()\n            \n            \n            \nreward = 0\nbrp = BreakRoomPlan(100)\n\ndef fourth_elf(observation, configuration):\n    global brp, reward\n    step = observation[\"step\"]\n    if step > 0:\n        my_index = observation[\"agentIndex\"]\n        opponent_index = 0 if my_index == 1 else 1\n        my_action = observation[\"lastActions\"][my_index]\n        opponent_action = observation[\"lastActions\"][opponent_index]\n        current_total = observation[\"reward\"]\n        candy_gained = False if reward == current_total else True\n        reward = copy.deepcopy(current_total)\n        if my_action != opponent_action:\n            brp.machines[my_action].me_pulled(success=candy_gained)\n            brp.machines[opponent_action].opponent_pulled()\n        elif my_action == opponent_action:\n            brp.machines[my_action].both_pulled(success=candy_gained)\n        brp.opponent.history.append(opponent_action)\n        brp.evaluate_opponent(opponent_action)\n        action = brp.get_machine()\n        return int(action)\n    else:\n        action = brp.get_machine()\n        return int(action)","f7081db0":"%%writefile clever_elf.py\nimport random\nimport copy\n\nfrom collections import defaultdict, Counter\n\nimport numpy as np\n\n\n\nclass VendingMachine(object):\n    \"\"\"\n    A class describing a single machine.\n    \n    Attributes\n    ==========\n    number: int\n        The number of a machine (in this competition up to 100)\n    my_pulls: int\n        How many times my agent pulled this machine?\n    opponent_pulls: int\n        How many times the opponent pulled this machine?\n    candies: int\n        How many candies did my agent get from this machine?\n    history: list of str\n        Every pull has a tag consisting of a few letters,\n        \"M\" means \"my agent\", \"O\" is opponent, \"B\" is for when\n        we pulled the machine both at the same time.\n        \"S\" is for success, \"F\" for failure, \"?\" for unknown result\n        (when the opponent pulls the machine).\n    information_gain_multiplier: float\n        Adjust the value of new information versus probability of reward,\n        if it is high, then exploration is more important than exploitation.\n    modifiers: set\n        Set of strategies applied to guess opponent's rewards. For example\n        if the opponent pulls a machine a single time and then they do no pull\n        it anymore it was very likely a failure. \n        To the modifiers set is added \"SinglePulls\" and the opponent\n        pull tag is changed from \"O?\" to \"OF\". This attribute serves mainly for\n        debugging and to check that multiply strategies do not make mess when\n        applied together.\n    \"\"\"\n    \n    def __init__(self, number=0, igm=0.58):\n        self.number = number\n        self.my_pulls = 0\n        self.opponent_pulls = 0\n        self.candies = 0\n        self.history = []\n        self.information_gain_multiplier = igm\n        self.modifiers = set()\n\n    @property\n    def pulls(self):\n        \"\"\"\n        Returns the total number of pulls of the machine.\n        \"\"\"\n        return self.my_pulls + self.opponent_pulls\n    \n    @property\n    def known_pulls(self):\n        \"\"\"\n        Returns the number of pull for which we know the outcome.\n        \"\"\"\n        s = 0\n        for record in self.history:\n            if record.count(\"S\") + record.count(\"F\") > 0:\n                s += 1\n        return s\n    \n    def opponent_pulled(self):\n        \"\"\"\n        Record an opponent pull.\n        \n        By default we add to the history \"O?\",\n        \"O\" for opponent and \"?\" for unknown result.\n        The question mark can be rewritten by applying \n        a strategy, i.e. if the opponent is predictable\n        we change the question mark to \"S\" or \"F\".\n        \"\"\"\n        self.opponent_pulls += 1\n        self.history.append(\"O?\")\n        \n    def me_pulled(self, success=False):\n        \"\"\"\n        Record my pull.\n        \"\"\"\n        self.my_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"MS\")\n        else:\n            self.history.append(\"MF\")\n            \n    def both_pulled(self, success=False):\n        \"\"\"\n        Record simultaneous pull by both elves.\n        \"\"\"\n        self.my_pulls += 1\n        self.opponent_pulls += 1\n        if success:\n            self.candies += 1\n            self.history.append(\"BS?\")\n        else:\n            self.history.append(\"BF?\")\n\n    @staticmethod\n    def assume_probability_from_history(history):\n        \"\"\"\n        Assumes original probability of reward.\n        \n        As we have a history on a vending machine, we can compute\n        how likely is the current probability. We basically go through\n        all possible starting probabilities (for cycle hidden\n        in linspace) and the probability that is most likely to \n        give the current history of pulls is the most certainly\n        the original probability.\n        \n        Note: I think we could also estimate an opponent's score using\n        this. I don't know for what it may be good, but \n        the possibility is interesting.\n        \"\"\"\n        base = np.linspace(0, 1, 101)\n        decay = 0.97\n        probabilities = np.ones(101)\n        current_decay = 0.97\n        for event in history:\n            if event == \"O?\":\n                current_decay *= decay\n            elif event == \"MS\" or event == \"OS\":\n                probabilities *= (base*current_decay)\n                current_decay *= decay\n            elif event == \"MF\" or event == \"OF\":\n                probabilities *= (1-(base*current_decay))\n                current_decay *= decay\n            elif event[0] == \"B\":\n                for letter in event[1:]:\n                    if letter == \"?\":\n                        pass\n                    elif letter == \"S\":\n                        probabilities *= (base*current_decay)\n                    elif letter == \"F\":\n                        probabilities *= (1-(base*current_decay))\n                current_decay *= decay**2\n        return np.argmax(probabilities)\n            \n    @property\n    def assumed_original_probability(self):\n        \"\"\"\n        Returns the assumed original probability.\n        \"\"\"\n        return self.assume_probability_from_history(self.history)\n    \n    @property\n    def assumed_current_probability(self):\n        \"\"\"\n        Return the assumed current probability of reward.\n        \n        As we can compute the original probability, we just apply\n        the decay and that is the current probability of a reward.\n        \"\"\"\n        return self.assumed_original_probability*0.97**self.pulls\n    \n    @staticmethod\n    def compute_information_gain(history):\n        \"\"\"\n        Return the information gain.\n        \n        I have defined the information gain as the difference between probability of reward\n        if I got a candy for the next pull and probability of reward if I didn't get a reward\n        for the next pull. This decays pretty quickly, because after a few pulls it is\n        estimated pretty well and another pull won't change the assumed probability much.\n        \"\"\"\n        lose_probability = VendingMachine.assume_probability_from_history(history + [\"MF\"])\n        win_probability = VendingMachine.assume_probability_from_history(history + [\"MS\"])\n        return win_probability - lose_probability\n    \n    @property\n    def information_gain(self):\n        \"\"\"\n        Returns the information gain.\n        \"\"\"\n        return self.compute_information_gain(self.history)\n    \n    @property\n    def score(self):\n        \"\"\"\n        Return the value (score) of the machine.\n        \"\"\"\n        return self.information_gain * self.information_gain_multiplier + self.assumed_current_probability\n    \n    def reset_opponent_history(self):\n        \"\"\"\n        Resets opponent history and modifiers.\n        \n        This function is run every round for every machine.\n        It basically sets every opponent tag from possible\n        \"S\" or \"F\" to \"?\" and resets the modifiers.\n        It has to be run every round, because all strategies\n        are evaluated every round, i.e. if an opponent is predictable\n        and fullfils a strategy, the strategy is applied.\n        On the other side, the opponent may e.g. just look like\n        he is applying Vegas strategy, but in the end, they are just\n        bluffing. So a strategy may be applied and opponent question\n        marks may be rewritten and then information from them is used,\n        but the strategy may be removed later and the information is\n        no longer used.\n        \"\"\"\n        self.modifiers = set()\n        for i, event in enumerate(self.history):\n            if event[0] == \"O\":\n                self.history[i] = \"O?\"\n            elif event[0] == \"B\":\n                self.history[i] = self.history[i][:-1] + \"?\"\n\n                \n                \nclass AlarmClock(object):\n    \"\"\"\n    A simple clock class.\n    \n    I have created a class Plan and it contains machines,\n    that the agent will pull in the future rounds. For example\n    a machine may be good, but I don't want the opponent\n    to notice me that I am pulling it a lot. So I set an\n    AlarmClock that reminds me which machine `number` I want\n    to pull in `time` rounds.\n    \n    Attributes\n    ==========\n    number: int\n        Number of machine for which this clock ticks.\n    time: int\n        Remaining time, before the machine is ready.\n    \"\"\"\n    \n    def __init__(self, number, time):\n        self.number = number\n        self.time = time\n        \n    def tick(self):\n        \"\"\"\n        Tick another round, lower the time by 1 to minimum of 0.\n        \"\"\"\n        self.time = np.max([self.time-1, 0])\n        \n    @property\n    def ready(self):\n        if self.time == 0:\n            return True\n        else:\n            return False\n    \n    def __repr__(self):\n        return f\"Clock - number {self.number} - time {self.time}\"\n        \nclass Plan(object):\n    \"\"\"\n    \n    Attributes\n    ==========\n    list: list\n        List of `AlarmClock`s, i.e. machines that I plan to pull\n        in the future.\n    number: int (None)\n        The number of selected machine.\n    \"\"\"\n    \n    def __init__(self):\n        self.list = list()\n        self.number = None\n        \n    def exists(self):\n        \"\"\"\n        Returns True if a number is selected.\n        \"\"\"\n        return True if self.number else False\n        \n    def readied_exists(self):\n        \"\"\"\n        Returns True if any machine is ready to pull.\n        \n        Goes through the list of clocks and returns True\n        if any of them has hands on zero.\n        \"\"\"\n        if np.array([clock.ready for clock in self.list]).any():\n            return True\n        else:\n            return False\n        \n    def force(self, number):\n        \"\"\"\n        Overwrite the selected number.\n        \"\"\"\n        self.number = number\n    \n    def tick(self):\n        \"\"\"\n        Tick all clocks in the list.\n        \n        This function is run every round, so that the remaining time\n        on all the clocks is updated.\n        \"\"\"\n        for clock in self.list:\n            clock.tick()\n            \n    def tick_clock(self, number):\n        \"\"\"\n        Tick selected clock.\n        \n        This is usually used when an opponent pulls a machine.\n        If I already plan to pull it, I may want to pull it earlier,\n        so I may tick the clock once more.\n        \"\"\"\n        for clock in self.list:\n            if clock.number == number:\n                clock.tick()\n    \n    def add(self, number, time):\n        \"\"\"\n        Plan to pull a machine with number `number`\n        in `time` rounds.\n        \"\"\"\n        clock_already_in_plan = False\n        for clock in self.list:\n            if clock.number == number:\n                clock_already_in_plan = True\n                clock.number = number\n        if not clock_already_in_plan:\n            self.list.append(AlarmClock(number, time))\n    \n    def get_machine_number(self):\n        \"\"\"\n        Return the number of machine that is selected according to the plan.\n        \n        It also removes clocks with the given number from the list.\n        \"\"\"\n        number = self.number\n        new_list = [clock for clock in self.list if clock.number != number]\n        self.list = copy.deepcopy(new_list)\n        self.number = None\n        return number\n    \n    def get_random_ready(self):\n        \"\"\"\n        Select a machine number from machines that are ready.\n        \"\"\"\n        clock_statuses = [clock.ready for clock in self.list]\n        selected_index = random.choice(np.argwhere(clock_statuses == np.amax(clock_statuses)).flatten())\n        selected_clock = self.list.pop(selected_index)\n        return selected_clock.number\n    \n    def get_all_ready(self):\n        \"\"\"\n        Return numbers of all machines that are ready.\n        \"\"\"\n        readied_clocks = []\n        new_list = []\n        for i in range(len(self.list)):\n            clock = self.list.pop(0)\n            if clock.ready:\n                readied_clocks.append(clock.number)\n            else:\n                new_list.append(clock)\n        self.list = copy.deepcopy(new_list)\n        return readied_clocks\n    \n    \n# Helper functions for strategies\ndef get_index_of_nth_repeating(list_of_values, nth_repeating=1, repeating_for_time=2):\n    \"\"\"\n    Get an index of a `nth_repeating` value that is repeated for `repeating_for_time`.\n    \n    For example if you want the index of the first number that is repeated twice,\n    you set `nth_repeating = 1` as the first number that is repeated and you set\n    `repeating_for_time = 2` for the repeating twice. It return the `repeating_for_time`-th\n    index.\n    \n    Examples\n    ========\n    l = [1,2,3,3,2,1,1,2,3]\n    get_index_of_nth_repeating(l, 1, 2)\n    > 3\n    get_index_of_nth_repeating(l, 2, 2)\n    > 2\n    get_index_of_nth_repeating(l, 1, 3)\n    > 1\n    \"\"\"\n    counter = defaultdict(lambda: 0)\n    numbers_repeated = 0\n    for i, value in enumerate(list_of_values):\n        counter[value] += 1\n        if counter[value] == repeating_for_time:\n            numbers_repeated += 1\n            if numbers_repeated == nth_repeating:\n                return i\n    return None   \n\n# Implement strategies used by opponents - we need to use as much info from them as possible\nclass Strategy(object):\n    \"\"\"\n    Strategies used by opponents.\n    \n    If an opponent is predictable, we can use the information from their pulls,\n    because from their behaviour we know, if they got a candy or not. \n    \"\"\"\n        \n    def is_applicable(self):\n        \"\"\"\n        Returns True if the strategy is applicable to the opponent behaviour.\n        \"\"\"\n        raise NotImplementedError(\"\")\n        \n    def apply(self):\n        \"\"\"\n        Apply the strategy to opponent's pulls.\n        \n        Changes opponent's question marks for machines to success (S) or failure (F),\n        so that information from these pulls may be used for more precise estimation\n        of probability of a reward.\n        \"\"\"\n        raise NotImplementedError(\"\")    \n\nclass SinglePulls(Strategy):\n    \"\"\"\n    If opponent pulls a machine only one time and then does not pull it another time\n    for a long time, it is likely that the first pull was failure.\n    \n    Attributes\n    ==========\n    name: str\n        The name of the strategy. Saves to `VendingMachine.modifiers` just to have\n        an option to check which strategies are applied to which machines.\n    min_history: int\n        Minimal history length that is needed for evaluation. \n    failed: list of ints\n        List of machine number to which this strategy will be applied, i.e. we\n        think that the opponent did not get a candy from them on the first pull.\n    \"\"\"\n    \n    def __init__(self):\n        self.name = \"SinglePulls\"\n        self.min_history = 130\n        self.failed = []\n        \n    def is_applicable(self, history):\n        \"\"\"\n        Checks whether SinglePulls strategy is applicable and on what\n        machines it is applicable.\n        \n        Mark each number that is not pulled again in `min_history` pulls after the\n        first pull as failed (only the first pull are marked as failed).\n        \n        I found out that 130 works pretty well, at first the number was different,\n        higher, but the agent did not work that well then. There were a few agents\n        that tried the machine again in 150 pulls even if that failed for the first\n        time and the information from the first unsuccessful pull wasn't used then.\n        \"\"\"\n        if len(history) > self.min_history:\n            self.failed = []\n            counted_numbers = set()\n            for i, number in enumerate(history[:-self.min_history]):\n                if number not in counted_numbers:\n                    counted_numbers.add(number)\n                    if history[i:i+self.min_history].count(number) == 1:\n                        self.failed.append(number)\n            return True\n        else:\n            return False\n        \n    def apply(self, machines):\n        \"\"\"\n        Mark all first opponent's pulls on machines with number in `failed`\n        as failed.\n        \"\"\"\n        for number in self.failed:\n            for position, result in enumerate(machines[number].history):\n                if result[0] == \"O\" or result[0] == \"B\":\n                    machines[number].history[position] = result[:-1] + \"F\"\n                    machines[number].modifiers.add(self.name)\n                    break        \n        \n        \nclass Gabriel(Strategy):\n    \"\"\"\n    Gabriel is a simple strategy - at first pull once every machine (sometimes numbers are\n    randomized, sometimes they are sorted) and then pull those machines that gave candy for\n    the first time. We mark all those pulled twice as success the first time they were pulled\n    and the rest as failure.\n    \n    Sometimes agents bluff, so the condition that Gabriel is not that the pulled every machine\n    exactly once, but that most of machines was pulled once. It is also possible to apply Gabriel\n    on multiple levels, for example some agents go through machines and pull them once. Then they\n    pull only machines where they get candy for the first time. Then they pull only machines where\n    they got candy for the first and second time, etc. So it is in the end possible to mark more\n    than just the first pulls as success. If someone is using this strategy and do not randomize pulls,\n    i.e. they pull machines in order, it is also possible to mark failures, for example if someone\n    pulls all machines from 1 to 100 and in the next round they pull 1, skip 2 and pull 3, we can mark\n    the first pull on the second machine as failure.\n    \n    Attributes\n    ==========\n    name: str\n        The name of the strategy. Saves to `VendingMachine.modifiers` just to have\n        an option to check which strategies are applied to which machines.\n    min_steps: int\n        Minimal history length that is needed for evaluation.     \n    \"\"\"\n    \n    def __init__(self):\n        self.name = \"Gabriel\"\n        self.min_steps = 80\n        self.results = defaultdict(lambda: \"\")\n    \n    def is_applicable(self, history):\n        self.results = defaultdict(lambda: \"\")\n        numbers = copy.deepcopy(history)\n        applicable = False\n        # Condition: At least 80 steps and at lest 60 pulled only once\n        if len(numbers) > self.min_steps and len(set(history[:self.min_steps])) > 0.75*self.min_steps:\n            for level in range(2, 5):\n                # e.g. level two means that opponent pulled every machine once\n                # and now started to pull them for the second time.\n                second = get_index_of_nth_repeating(numbers, 1, level)\n                if second is None:\n                    break\n                third = get_index_of_nth_repeating(numbers, 1, level+1)\n                third = third if third is not None else -1\n                for number in numbers[second:third]:\n                    if numbers[second:third].count(number) == 1 and numbers[:third].count(number) == level:\n                        self.results[number] += \"S\"\n                # Specifically, while marking level 1, if opponent pulls are sorted we can mark even failures\n                if level == 2:\n                    indices = [numbers.index(x) for x in numbers[second:third]]\n                    if indices == sorted(indices) and len(indices) > 1:\n                        for number in numbers[:indices[-1]]:\n                            if len(self.results[number]) == 0 and number not in indices:\n                                self.results[number] += \"F\"\n                    # We can't be sure that it is False if they are not sorted\n                applicable = True\n        return applicable\n                    \n    def apply(self, machines):\n        for number, letters in self.results.items():\n            max_value = len(letters)\n            counter = 0\n            if max_value != 0:\n                for position, result in enumerate(machines[number].history):\n                    if result[0] == \"O\" or result[0] == \"B\":\n                        machines[number].history[position] = result[:-1] + letters[counter]\n                        machines[number].modifiers.add(self.name)\n                        counter += 1\n                        if counter == max_value:\n                            break\n    \nclass VegasGambler(Strategy):\n    \"\"\"\n    VegasGambler pulls the same machine until it gives no candy, then moves\n    to another machine. This way he tries all machines. When they start repeating\n    machines, we can mark all pulls except the last ones for all machines as success\n    and the last pull for each machine as failure. \n    \n    Attributes\n    ==========\n    name: str\n        The name of the strategy. Saves to `VendingMachine.modifiers` just to have\n        an option to check which strategies are applied to which machines.\n    min_groups: int\n        Minimal number of machines that was pulled. This strategy is applied only\n        when the numbers are not identical, e.g. if opponent pulls 1,1,2,1,2,2,3,\n        he is obviously not a VegasGambler, because he returns to machines.\n    good: dict\n        Keys in the dictionary are machine numbers and values are integers as well,\n        meaning how many opponent's pull should be marked as successes.\n    \"\"\"\n    \n    def __init__(self):\n        self.name = \"VegasGambler\"\n        self.min_groups = 85\n        self.good = {}\n        \n    def is_applicable(self, history):\n        grouped = []\n        counted = []\n        previous_number = history[0]\n        s = 1\n        for number in history[1:]:\n            if number == previous_number:\n                s += 1\n            else:\n                grouped.append(previous_number)\n                counted.append(s)\n                s = 1\n                previous_number = number\n        grouped.append(previous_number)\n        counted.append(s)\n        until = get_index_of_nth_repeating(grouped)\n        # When it starts repeating and there is at least 85 groups and something was pulled at least 4 times:\n        if until is not None and until > self.min_groups and max(counted) > 3 and counted.count(1) > 10:\n            self.good = dict(zip(grouped[:until], counted[:until]))\n            return True\n        else:\n            return False\n\n    def apply(self, machines):\n        for number, good_count in self.good.items():\n            counted = 0\n            for position, result in enumerate(machines[number].history):\n                if result[0] == \"O\" or result[0] == \"B\":\n                    machines[number].modifiers.add(self.name)\n                    if counted < good_count and counted < 4:\n                        machines[number].history[position] = result[:-1] + \"S\"\n                        counted += 1\n                    elif counted == good_count or counted == 4:\n                        machines[number].history[position] = result[:-1] + \"F\"\n                        counted += 1\n    \nclass Consecutive(Strategy):\n    \"\"\"\n    Consecutive is not really a strategy, but it is default estimator of opponent's\n    success for my elves. The purpose of the previous basic strategies is to get through\n    a wall of VegasGamblers between 900-1000 points. The third elf got to ninth place,\n    but did not stay there, so the purpose of this strategy is to evaluate top players,\n    so that my elf can stay longer at the top.\n    By downloading statistics for top players, it should be possible to say,\n    that for example if opponent pulls a machine for the first time twice,\n    then in 70 % of cases the first pull is success and the second failure.\n    If we do this for e.g. numbers up to 8, we should be able to get info about opponent\n    success immediately. However, we need to keep in mind that some opponent do not pull\n    a machine two times in a row (this is a feature for my elves as well), so for those\n    we need to estimate the probabilities in a different way. It is also different if\n    my elves pull the machine as well.\n    This strategy did not work well in the end, but if information from just the first two\n    pulls is used, it works fine. I use it only in the beginning of the match.\n    \"\"\"\n        \n    def __init__(self):\n        self.name = \"Consecutive\"\n        self.good = []\n        #set_rules = {\n        #    1: \"F\",\n        #    2: \"SF\",\n        #    3: \"SSF\",\n        #    4: \"SSSF\",\n        #    5: \"SSSF\",\n        #    6: \"SSSSFF\",\n        #    7: \"SSSSSFF\",\n        #    8: \"SSSSSFF\",\n        #}\n        set_rules = {\n            1: \"F\",\n            2: \"SF\",\n            3: \"SS\",\n        }\n        self.basic_rules = defaultdict(lambda: \"SS\")\n        for key, value in set_rules.items():\n            self.basic_rules[key] = value\n        \n    def is_applicable(self, history):\n        if len(history) < 3 or len(history) > 400:\n            return False\n        else:\n            already_counted_numbers = set()\n            self.good = {}\n            ones = 0\n            pluses = 0\n            for i, number in enumerate(history):\n                if number not in already_counted_numbers:\n                    counter = 1\n                    for j, next_number in enumerate(history[i+1:]):\n                        if number == next_number and j < len(history)-1:\n                            counter += 1\n                        else:\n                            self.good[number] = self.basic_rules[counter]\n                            if counter == 1:\n                                ones += 1\n                            else:\n                                pluses += 1\n                            counter = 1\n                            already_counted_numbers.add(number) \n                            break\n            self.good[number] = self.basic_rules[counter]\n            if counter == 1:\n                ones += 1\n            else:\n                pluses += 1\n            if pluses \/ (ones + pluses) > 0.3:\n                return True\n            else:\n                return False\n    \n    def apply(self, machines):\n        for number, value in self.good.items():\n            counted = 0\n            for position, result in enumerate(machines[number].history):\n                if result[0] == \"O\" or result[0] == \"B\":\n                    if counted < len(value):\n                        machines[number].history[position] = result[:-1] + value[counted]\n                        machines[number].modifiers.add(self.name)\n                        counted += 1\n                    else:\n                        break\n                        \nclass Opponent(object):\n    \"\"\"\n    Class for storing opponent's history and evaluating their strategy.\n    \n    Attributes\n    ==========\n    history: list of ints\n        List of machine numbers the opponent pulled\n    forced_number: int (None)\n        Number which is needed to be pulled now as an immediate reaction to the opponent,\n        overrides planned number. Usually used when opponent pulls the same machine twice\n        in a row.\n    known_strategies: list of Strategy objects\n        List of known strategies that opponents are often using and if recognized we can\n        use information about successes and failures of pulls in order to better estimate\n        the current probability.\n    \"\"\"\n    \n    def __init__(self):\n        self.history = []\n        self.forced_number = None\n        self.known_strategies = [Gabriel(), Consecutive(), VegasGambler(), SinglePulls()]\n        \n    def evaluate_history(self, machines):\n        \"\"\"\n        Go through the opponent's history, check whether they are not pulling the same\n        machine, try to identify their strategy and if recognized, apply to machines.\n        \"\"\"\n        if len(self.history) >= 3:# and len(self.history) < 1800:\n            minimal_score = np.linspace(20, 10, 2000)[len(self.history)]\n            if self.history[-1] == self.history[-2]:# == self.history[-3]:\n                if machines[self.history[-1]].score > minimal_score:\n                    self.forced_number = self.history[-1]\n            else:\n                if len(self.history) >= 8:\n                    counter = Counter(self.history[-7:])\n                    mc = counter.most_common(1)[0]\n                    me_pulled = False\n                    for record in machines[mc[0]].history[-7:]:\n                        if record[0] == \"B\" or record[0] == \"M\":\n                            me_pulled = True\n                    if mc[1] >=3 and machines[mc[0]].score > minimal_score and not me_pulled:\n                        self.forced_number = mc[0]\n        applied_strategies = [strategy for strategy in self.known_strategies if strategy.is_applicable(self.history)]\n        for machine in machines:\n            machine.reset_opponent_history()\n        # Change probabilities according to strategies\n        for strategy in applied_strategies:\n            strategy.apply(machines)\n        \n    def get_forced_number(self):\n        \"\"\"\n        Returns the number of machine that needs to be pulled right now\n        as the reaction on opponent's behaviour.\n        \"\"\"\n        forced_number = self.forced_number\n        self.forced_number = None\n        return forced_number\n    \n    def is_forcing_number(self):\n        return True if self.forced_number is not None else False\n        \nclass BreakRoomPlan(object):\n    \"\"\"\n    Basically a manager for the environment.\n    \n    It contains list of machine classes, classes for plans and the opponent.\n    At Kaggle, variables stored between rounds have to be stored in\n    global variables or in classes, so this class also encapsulates\n    everything that we need to be saved.\n    \n    Attributes\n    ==========\n    machines: list of VendingMachine objects\n        List of all machines available in the game.\n    plan: Plan\n        Plan containing what the agent intends to pull in future rounds.\n    forbidden_plan: Plan\n        Plan containing machine numbers that are now in `forbidden` list,\n        and containing information when they will be available for pulling once again.\n    forbidden: list of ints\n        List of machine numbers that the agent can not pull now. For example\n        because he pulled it in the last round and don't want to be transparent.\n    opponent: Opponent\n        Class for storing information about opponent and analyzing their behaviour.\n    \"\"\"\n\n    def __init__(self, N_machines):\n        self.machines = [VendingMachine(i) for i in range(N_machines)]\n        self.plan = Plan()\n        self.forbidden_plan = Plan()\n        self.forbidden = []\n        self.opponent = Opponent()\n        \n    @property\n    def history(self):\n        return len(self.opponent.history)\n        \n    def get_indices_sorted_by_scores(self):\n        \"\"\"\n        Return indices of machines sorted by scores.\n        \n        This is a pretty general function - sort list of\n        values. Usually used for finding out what is the best machine\n        to pull, but we also need to return the indices to check\n        that a machine is not on `self.forbidden` list.\n        \"\"\"\n        scores = list()\n        indices = list()\n        for machine in self.machines:\n            scores.append(machine.score)\n            indices.append(machine.number)\n        zipped = list(zip(indices, scores))\n        # We want to shuffle, so that if the best scores are identical,\n        # a machine is selected randomly from the ones with the best scores.\n        # By default the one with lower index would be selected and that would\n        # make our strategy more transparent.\n        random.shuffle(zipped)\n        indices, scores = zip(*zipped)\n        sorted_indices = [index for index, _ in sorted(zip(indices,scores), key=lambda pair: pair[1])]\n        return sorted_indices\n    \n    def get_machine(self):\n        \"\"\"\n        Return a machine number to be pulled now.\n        \n        Goes through the plan and gets the forced or planned number.\n        If there is no plan, pull the machine with highest value.\n        Also tick all alarm clocks for plans and remove number from\n        `forbidden` list if there are numbers to be removed.\n        \"\"\"\n        if self.plan.exists():\n            selected = self.plan.get_machine_number()\n        else:\n            if self.plan.readied_exists():\n                selected = self.plan.get_random_ready()\n            else:\n                best_machines = self.get_indices_sorted_by_scores()\n                for index in best_machines:\n                    if index not in self.forbidden:\n                        selected = index\n                self.forbidden.append(selected)\n                self.forbidden_plan.add(selected, 2)\n        self.plan.tick()\n        self.forbidden_plan.tick()\n        if self.history:\n            self.forbidden_plan.tick_clock(self.opponent.history[-1])\n            self.plan.tick_clock(self.opponent.history[-1])\n        for machine in self.forbidden_plan.get_all_ready():\n            self.forbidden.remove(machine)\n        if self.machines[selected].known_pulls == 0:\n            self.plan.add(selected, 4)\n        return selected\n    \n    def evaluate_opponent(self, action):\n        \"\"\"\n        Evaluate the history and try to recognize opponent's strategy.\n\n        \"\"\"\n        machine = self.machines[action]\n        # In the beginning I would like to try a machine if opponent pulls it a lot\n        self.opponent.evaluate_history(self.machines)\n        if machine.opponent_pulls == 4 and machine.my_pulls == 0:\n            self.plan.add(action, 1)\n        # Even if a machine seems bad, it may be just a bad luck, so keep, so keep my\n        # pulls somehow balanced with the opponent. If they pull it a lot, have at least\n        # one third of the pulls on the machine.\n        # There is also a threshold that they have to pull it at least for times, because\n        # it is possible so that it is really a bad machine and they will never pull it again,\n        # so we can spare some pull there.\n        if machine.opponent_pulls > 4 and machine.opponent_pulls > machine.my_pulls*2:\n            self.plan.add(action, 1)\n        if self.opponent.is_forcing_number():\n            self.plan.force(self.opponent.get_forced_number())\n            \n    def show_machines(self):\n        \"\"\"\n        Function for debugging purposes.\n        \"\"\"\n        print(self.history, print(len(self.machines)))\n        for machine in self.machines:\n            print(machine.number, machine.modifiers, machine.history)\n        print(\"-\"*50)\n            \n    def apply_events(self):\n        \"\"\"\n        Apply changes to logic after a number of steps.\n        \n        Later in the game, we may decide to focus even more on\n        exploitation and we can for example lower the\n        `information_gain_multiplier` to achieve that.\n        This function was not used in the best solution though.\n        \"\"\"\n        lowering_igm = {1200:-0.1, 1800:-0.1}\n        for event_step, decrease in lowering_igm.items():\n            if self.history == event_step:\n                for machine in self.machines:\n                    machine.information_gain_multiplier += decrease\n            \nreward = 0\nbrp = BreakRoomPlan(100)\n\ndef clever_elf(observation, configuration):\n    global brp, reward\n    step = observation[\"step\"]\n    if step > 0:\n        my_index = observation[\"agentIndex\"]\n        opponent_index = 0 if my_index == 1 else 1\n        my_action = observation[\"lastActions\"][my_index]\n        opponent_action = observation[\"lastActions\"][opponent_index]\n        current_total = observation[\"reward\"]\n        candy_gained = False if reward == current_total else True\n        reward = copy.deepcopy(current_total)\n        if my_action != opponent_action:\n            brp.machines[my_action].me_pulled(success=candy_gained)\n            brp.machines[opponent_action].opponent_pulled()\n        elif my_action == opponent_action:\n            brp.machines[my_action].both_pulled(success=candy_gained)\n        brp.opponent.history.append(opponent_action)\n        brp.apply_events()\n        brp.evaluate_opponent(opponent_action)\n        action = brp.get_machine()\n        return int(action)\n    else:\n        action = brp.get_machine()\n        return int(action)","226ead71":"# Test that written version is working\nfrom kaggle_environments import make\n\nenv = make(\"mab\", debug=True)\n\nenv.run([None, \"clever_elf.py\"])","c07b137e":"def get_steps_from_json(json):\n    df = pd.DataFrame().from_dict(json[\"steps\"])\n    df_1 = df[0].apply(pd.Series)[[\"action\", \"reward\"]].rename({\"action\":\"action_1\", \"reward\":\"total_reward_1\"}, axis=1)\n    df_2 = df[1].apply(pd.Series)[[\"action\", \"reward\"]].rename({\"action\":\"action_2\", \"reward\":\"total_reward_2\"}, axis=1)\n    dfa = pd.concat((df_1, df_2), axis=1).iloc[1:, :]\n    dfa[\"reward_1\"] = dfa[\"total_reward_1\"] - dfa[\"total_reward_1\"].shift(1)\n    dfa[\"reward_2\"] = dfa[\"total_reward_2\"] - dfa[\"total_reward_2\"].shift(1)\n    dfa = dfa.fillna(0)\n    dfa[\"reward_1\"] = dfa[\"reward_1\"].astype(int)\n    dfa[\"reward_2\"] = dfa[\"reward_2\"].astype(int)\n    return dfa[[\"action_1\", \"reward_1\", \"total_reward_1\",\n                \"action_2\", \"reward_2\", \"total_reward_2\"]]\n\ndef get_steps_until(final_step, steps_df, player):\n    machines = defaultdict(lambda: [])\n    for row in steps_df.iloc[0:final_step, :].iterrows():\n        machines[row[1][f\"action_{player}\"]].append(row[1][f\"reward_{player}\"])\n    return machines","114de117":"game_record = env.toJSON()","ecbbfb75":"# with open(\".\/opponent.json\") as json_file:\n#     game_record = json.load(json_file)\n\n# env = kaggle_environments.make(\n#     \"mab\",\n#     configuration=game_record[\"configuration\"],\n#     steps=game_record[\"steps\"],\n#     info=game_record[\"info\"]\n# )","7b3dc01c":"df = pd.DataFrame().from_dict(game_record[\"steps\"])\nthresholds = df[0].apply(pd.Series)[\"observation\"].apply(pd.Series)[\"thresholds\"].apply(pd.Series)","9a2fd8c8":"steps_df = get_steps_from_json(game_record)","dc5fb0fb":"def set_board_machines(board, machines, move_by, good_color=(72, 219, 111), bad_color=(245, 70, 39)):\n    for m in range(100):\n        for height, value in enumerate(machines[m]):\n            if value == 0:\n                board[height, m*4+move_by] = bad_color\n            elif value == 1:\n                board[height, m*4+move_by] = good_color    \n    return board\n\ndef set_thresholds(board, step, thresholds, move_by, color=(0, 0, 120)):\n    values = thresholds.iloc[step-1, :].values\n    for position, height in enumerate(values):\n        h = int(height)\n        board[0:h, position*4+move_by] = color\n    return board","17bf2bd9":"def make_traces(env, data, step, steps_df):\n    \n    board = np.ones((100,400,3)) * 231\n    machines_1 = get_steps_until(step, steps_df, player=1)\n    machines_2 = get_steps_until(step, steps_df, player=2)\n    rewards_1 = [np.array(machines_1[i]).sum() for i in range(100)]\n    tries_1 = [len(machines_1[i]) for i in range(100)]\n    rewards_2 = [np.array(machines_2[i]).sum() for i in range(100)]\n    tries_2 = [len(machines_2[i]) for i in range(100)]\n    board = set_board_machines(board, machines_1, move_by=0)\n    board = set_board_machines(board, machines_2, move_by=2)\n\n    initial_thresholds = env.steps[0][0][\"observation\"][\"thresholds\"]\n    current_thresholds = env.steps[step][0][\"observation\"][\"thresholds\"]\n    actions = list(np.array([s[\"action\"] for s in env.steps[step]])*4+1)\n    rewards = list(np.array([s[\"reward\"] for s in env.steps[step]]))\n\n    if step > 0:\n        last_rewards = [s[\"reward\"] for s in env.steps[step - 1]]\n    else:\n        last_rewards = [0, 0]\n        \n    success = [1 if rewards[i] > last_rewards[i] else 0 for i in range(2)]\n    \n    text_offset = 20 if step < 300 else 5\n    red = \"red\"\n    blue = \"blue\"\n    \n    customdata = np.stack((np.around(np.array(current_thresholds), 2), np.array(initial_thresholds), \n                           np.array(rewards_1), np.array(tries_1), \n                           np.array(rewards_2), np.array(tries_2)), axis=-1)\n\n    traces = [\n        go.Image(name=\"reward_boxes\", z=board, hoverinfo=\"skip\"\n        ),\n        go.Scatter(\n            name=\"agent0_line\",\n            x=[actions[0]] * 2,\n            y=[0, 108],\n            mode=\"lines\",\n            line=dict(color=blue, dash=\"solid\" if success[0] else \"dot\", width=1),\n            hoverinfo=\"skip\",\n        ),\n        go.Scatter(\n            name=\"agent1_line\",\n            x=[actions[1]] * 2,\n            y=[0, 122],\n            mode=\"lines\",\n            line=dict(color=red, dash=\"solid\" if success[1] else \"dot\", width=1),\n            hoverinfo=\"skip\",\n        ),\n        go.Scatter(\n            name=\"agent_indicators\",\n            x=actions*4,\n            y=[108, 122],\n            text=[\"0\", \"1\"],\n            textposition=\"top left\",\n            textfont=dict(color=[blue, red]),\n            hoverinfo=\"skip\",\n            mode=\"markers+text\",\n            marker=dict(\n                color=[blue, red],\n                size=16,\n                symbol=[\"triangle-down\" if s else \"triangle-down-open\" for s in success],\n            ),\n        ),\n        go.Bar(\n            name=\"initial_thresholds\",\n            x=np.array(range(100))*4+1,\n            y=initial_thresholds,\n            marker_line_width=0,\n            marker_color=\"grey\",\n            width=1,\n            customdata=customdata,\n            hovertemplate=\"<br>Probability: %{customdata[0]} \/ %{customdata[1]}\\n\"+\\\n            \"<br>Agent 1: %{customdata[2]} \/ %{customdata[3]}\\n\"+\\\n            \"<br>Agent 2: %{customdata[4]} \/ %{customdata[5]}\\n<extra><\/extra>\"\n        ),\n        go.Scatter(\n            name=\"current_thresholds\",\n            x=np.array(range(100))*4+1,\n            y=current_thresholds,\n            mode=\"markers\",\n            marker_line_width=0,\n            marker_color=\"orange\",\n            customdata=customdata,\n            hovertemplate=\"<br>Probability: %{customdata[0]} \/ %{customdata[1]}\\n\"+\\\n            \"<br>Agent 1: %{customdata[2]} \/ %{customdata[3]}\\n\"+\\\n            \"<br>Agent 2: %{customdata[4]} \/ %{customdata[5]}\\n<extra><\/extra>\"\n        ),\n        go.Bar(\n            name=\"rewards\",\n            x=[r \/ 8 for r in rewards],\n            y=[180, 195],\n            text=rewards,\n            textposition=\"auto\",\n            marker_line_width=1.5,\n            marker_line_color=[blue, red],\n            marker_color=[blue, red],\n            orientation=\"h\",\n            hoverinfo=\"skip\",\n        ),\n        go.Scatter(\n            name=\"team_names\",\n            x=[r \/ 8 + text_offset for r in rewards],\n            y=[180, 195],\n            text=env.info.get(\"TeamNames\", [\"\"] * 2),\n            textposition=\"bottom right\",\n            textfont=dict(color=[blue, red]),\n            mode=\"text\",\n            hoverinfo=\"skip\"\n        ),\n        go.Bar(\n            name=\"expected_rewards\",\n            x=[r \/ 8 for r in data[\"expected_rewards\"][:, step]],\n            y=[150, 165],\n            text=[\"{:.2f}\".format(x) for x in data[\"expected_rewards\"][:, step]],\n            textposition=\"auto\",\n            textfont=dict(color=[blue, red]),\n            marker_line_width=1.5,\n            marker_line_color=[blue, red],\n            marker_color=\"silver\",\n            orientation=\"h\",\n            hoverinfo=\"skip\",\n        ),\n        go.Scatter(\n            name=\"expected_label\",\n            x=[r \/ 8 + text_offset for r in data[\"expected_rewards\"][:, step]],\n            y=[150, 165],\n            text=\"(expected)\",\n            textposition=\"bottom right\",\n            textfont=dict(color=[blue, red]),\n            mode=\"text\",\n            hoverinfo=\"skip\",\n        ),\n    ]\n    \n    return traces","872e7dff":"def make_figure_widget(env, steps_df, n_traces=30):\n\n    fig = go.FigureWidget()\n\n    fig.update_xaxes(\n        range=[-2, 402],\n        zeroline=False,\n        showgrid=False,\n    )\n    fig.update_yaxes(\n        range=[-2, 208],\n        zerolinecolor=\"white\",\n        showgrid=False,\n    )\n    fig.update_layout(\n        showlegend=False,\n        plot_bgcolor=\"rgb(231, 231, 231)\",\n        autosize=False,\n        width=1600,\n        height=700,\n        margin=dict(l=10, r=10, b=10, t=30, pad=1),\n    )\n    \n    # create a couple extra datasets\n    action_histogram = np.zeros((2, 2000, 100), dtype=int)\n    expected_rewards = np.zeros((2, 2000), dtype=float)\n    for step_idx, step in enumerate(env.steps):\n        if step_idx == 0:\n            continue\n        for agent_idx, agent in enumerate(step):\n            action = agent[\"action\"]\n            thresholds = env.steps[step_idx - 1][0][\"observation\"][\"thresholds\"]\n            expected_reward = np.ceil(thresholds[action]) \/ 101\n            action_histogram[agent_idx, step_idx:, action] += 1\n            expected_rewards[agent_idx, step_idx:] += expected_reward\n            \n    data = {\n        \"action_histogram\": action_histogram,\n        \"expected_rewards\": expected_rewards,\n    }\n\n    fig.add_traces(make_traces(env, data, 0, steps_df))\n\n    return fig, data","3fc81e48":"fig, data = make_figure_widget(env, steps_df)","78121da2":"@interact(step=(0, 1999))\ndef interactive_display(step=0):\n    fig.update(data=make_traces(env, data, step, steps_df))\nfig","255a8952":"## Plan ahead and don't be predictable\n\nI didn't want to be preditable and it is obvious that a machine is good when you pull it repetitively. So I implemented a class Plan. Basically Plan overrides selection of a machine with best value. If there is no plan, agent selects the best machine. If there is a plan, machine is selected according to the plan.\n\nThere were just a few simple rules in the beginning:\n- If opponent pulls machine four times and I didn't pull it yet, I pull it as well. It is likely that the machine is good and if I pull it, I will likely get a candy as well and it will have a good value, so I will pull it even more in the future.\n- If I am going to pull a machine and I have never pulled it before, I set a plan to select it in the next few rounds again. This should prevent the agent from marking it as unsuccessful - i.e. if I pull a machine and get no reward, the value of the machine will be low and I am not going to pull it for a long time. The machine may be good and if I pull it twice, I can estimate the probability much better.\n\nThere were actually two plans, one listed machines that I am going to pull and one listed machines that were forbidden to pull for some time. It should prevent my agents from being predictable - I am not going to pull machine twice in a row to mark it as good machine for opponent.","09cfc023":"# Visualizations\n\nAt first, I spent a day creating a visualization that would show for each machine a probability and how many times it was pulled by me and by the opponent. In the evening, another [notebook](https:\/\/www.kaggle.com\/lpkirwin\/santa-2020-replay-visualizer) was released that did what I wanted much better, but I missed a few things so I fiddled with it a bit and added a few things. I added the on-hover info and made spaces between machines and added information about number of pulls for each agent (agent 1 is on the left, agent 2 is on the right). This way, I have been going through records of matches and studying how opponents behave and adjusting my agent accordingly.\n\nI was using this locally, so I am not sure if it will work in Kaggle out of the box. As it uses plotly and ipywidgets as well, it is interactive and you can view any step you want or just go through step by step by clicking on the right arrow. In the end it shows:\n- Number of candies for each agent\n- Cumulative probability (e.g. pulling a machine with probability 0.2 (if it does not change) five times means that cumulative probability is 1, event though the number of candies can range between 0 and 5.) for each agent\n- Starting probability of a reward for each machine\n- Current probability of a reward for each machine\n- How many times each agent pulled a machine, including the number of succeses and failures and their order\n- The step number","6944a2e3":"## Transforming data","bb6d1901":"## Plotly","fe625ae8":"![image.png](attachment:image.png)","dd02a65f":"# The final elf","841cf204":"### The fourth elf\n\nInstead of FourInSix guess results of consecutive opponent pulls.","b4f48ebf":"### The second elf","15797c5b":"## Final rules\n\n- If opponent pulled the same machine in the previous two rounds, pull the machine as well.\n- If opponent pulled the same machine at least three times in the last seven rounds, pull the machine as well. \n- If you are going to pull a new machine (you have no info by yourself and no identified pulls from the opponent), pull it twice (but not twice in a row, make a gap between pulls).\n- Never pull the same machine twice in a row unless the opponent is pulling it as well.\n- If the opponent pulled a machine four times and you haven't ever pulled it, pull it.\n- If the total of opponent's pulls on a machine is more than four and more than twice of your pulls on the machine at the same time, pull the machine.\n- Pull the machine with the highest value.","2144c3b3":"## Notes - Finding out the value of information gain\n\nIs it worth to spend time (pulls) on determining the probability of a machine? If it gives out a candy on the first time, we may think \"Oh, it is a good vending machine\", but it is just possible that you hit a 1%-chance machine. \n\nThere is obviously gain on pulling better machines more often, but at the start we don't know which are good, so the reward is also obtaining an information about a machine. What is more important? I need to sum these two values and multiply one of them by a multiplier, probably the information gain. This will be a score of a machine and I will just pull a machine with the best score.\n\nI have created a training machine and just tried to run cells below multiple times. I think this works well when I have pulled the machine already a few times (e.g. 5 times), but it is bad for the first pull. If I don't get a reward on the first try, then I can just leave a good machine to an opponent. I think it would be better to pull machine two times if I didn't pull it at all.\n\nThis is too obvious! You can't pull a machine two times in a row, everyone would see that. Maybe create a plan, e.g. every time when you pull a new machine, you should pull it a second time, however, you can not do it immediately, so make a not to do it in e.g. three turns.","7f83654d":"## Adjusting law of large numbers - Assumed original probability\n\nIf every machine gave a reward with a random probability that does not change, it would be easy to find out the probability. However the probability does change, so it is a bit more difficult. I was thinking about it whole evening and googled, but just could not find anything. Anyway, the problem still seemed solvable and I did not go sleep until I solved it.\n\nIf the probability does not change, how do we estimate it? We pull the machine a couple of times, the more times the better estimate, and according to law of large numbers \"the sample average converges in probability towards the expected value\". \n\nIn our case the probability changes, so the law is not applicable, because after every measurement the machine is inherently different. So we need to generalize the law to include the case with changing probability, but it still needs to stay applicable for the default case.\n\nI figured out around the midnight that it can be generalized to: \n\n> Given a history of machine outputs, the probability of a machine is most likely the one which most likely outputs the history. \n\nThen the longer is the history, the better is the estimate, it is the same as in the default case and it still applies to the default case.\n\nI will give an example of what I mean. Let's say that we pull a machine three times. The first time we gain a reward, the second time not and the third time we gain the reward again. Now, consider starting probabilities. I am a physicist, so let me consider boundaries at first. If the starting probability was 1, then after the first pull, given the rules of this competition and 3% decay, probability of reward is 0.97 and for the third pull it will be 0.9409. The probability that machine outputs history \\[1, 0, 1\\] is:\n\n$$1 \\cdot\u00a0(1-0.97) \\cdot 0.9409 = 0.028227$$\n\nWith the starting probability 0, the machine can not output this history at all, so its probability is zero. Now, consider the middle, starting probability 0.5, the probability of outputting the history is:\n\n$$ 0.5 \\cdot (1 - 0.485) \\cdot 0.47045 = 0.121140875$$\n\nYou see now that it is more likely that the starting probability of machine was 0.5 than 1 and starting probability 0 is completely out of question. This Kaggle problem is more easier than a general case, because here we have only 100 possible starting probabilities to consider, plus considering all of them can be easily vectorized, so it is really a simple computation. For history \\[1, 0, 1\\] you can find out that the most likely starting probability is 0.71 and the probability of outputting the history is 0.1481462622648567.\n\nComputing current probability of the machine can then easily be done as we know the number of pulls and we know the estimate of the original probability.","1cc0db90":"# Strategy","f0ccae5b":"## Elves\n\n### Dummy elves\n\n#### Random elf","f9713ef2":"# Basic classes and elves\n\nYou can basically skip this part and go right to the \"The final elf\" section. This part is not documented well, it is just a kind of a history of classes and elves development. \n\n## Basic classes","b8a1598c":"## Evaluating an opponent\n\nNow, I had the first elf ready and I submited him and he did poorly. I didn't take into account opponent much (it was still the first version after all) and at that time [Vegas agent](https:\/\/www.kaggle.com\/sirishks\/pull-vegas-slot-machines) was published. I couldn't get through a wall of these agents (at that time between 900 and 1000). I did better in the end of matches, but in the beginning they got much more candy than me and I did not catch up on them. \n\nThe next step is obviously to better react on the opponent and I thought of two ways how to do it - to introduce better rules that react on the opponent or use information from opponent pulls. I have introduced a rule to pull a machine if opponent pulls it twice in a row, so if someone uses for example the Vegas strategy and finds a good machine, my agent starts to pull it as well. More intricate problem is how to use information from opponent pulls. The problem with publishing solutions is that a lot of people will submit the same agent and they will in turn create a wall and if you want your agent to succeed, they need to beat the published agent always. That is not a huge problem, because if you exactly know the rules how an agent behaves, you also know whether opponent got a reward or not at least for portion of their pulls. For example in the Vegas strategy, you can mark all pulls on a machine, before opponent moves to another machine, as successful, except the last pull which you mark as unsuccessful. Even though you don't get a candy, the opponent is basically playing for you. You get a lot of information and don't have to try machines yourselves.","1691fbb4":"## Information gain\n\nWith the adjusted law of big numbers, we pull good machines if we already have information about them. However, this is the case only later in the game, in the beginning we have to explore, not just exploit. I decided to make my agents simple and let them pull machines based on a single value and always pull the machine with the highest value. The value is obviously the probability of reward, but I need to incorporate value of obtaining information somehow. I have decided to base information gain on assumed probability and compute it as difference between assumed probability if the next pull is successful and assumed probability if the next pull is not successful.\n\nI have checked how this behaves and it did almost exactly as I wanted. For the few first pulls the value is pretty high, but it also decays pretty quickly. The only problem was that it was too significant and agents tended to pull new machines even if they found good machines, so I introduced a multiplier and the final value of each machine was:\n\n$$ \\text{value} = \\text{assumed probability} + 0.58 \\cdot \\text {information gain} $$\n\nI fiddled with values of a multiplier a bit and then selected 0.58.","84601acb":"## Submitted elves\n\n### The first elf\n\nBasically just pulls everytime the best machine he knows. If opponent pulls a machine four times and he haven't pulled it yet, then pull it two times to get a basic estimation, then continue with the best machine.","e8926dc3":"### The third elf","e210ca56":"#### Elf Gabriel"}}