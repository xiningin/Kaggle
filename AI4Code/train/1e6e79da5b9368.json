{"cell_type":{"314e8412":"code","628e3853":"code","db08240d":"code","c8d25670":"code","5c91c124":"code","a0a72091":"code","d869c9ff":"code","47fdf519":"code","2cbf2818":"code","f69e229b":"code","e6d39bc9":"code","eb8aa599":"code","6fc6016c":"code","21c49c71":"code","9b5683ba":"code","cfb60c53":"code","d6698ae9":"code","1827935b":"code","5912d3fc":"code","05b181a3":"code","5521609c":"code","646383b1":"code","9434af16":"code","fa4c0414":"code","07c23a07":"code","c54a3242":"code","ab113559":"code","083083a3":"code","a04e589a":"code","f548213b":"code","eaaf16e0":"code","4950b46e":"code","bbe5894b":"code","6c9905cb":"code","bf22918d":"code","c2a5e9f0":"code","5de13d22":"code","545306b4":"code","8e25b690":"code","3cb70167":"code","7df23bfc":"code","fb4c7ff9":"code","45ad2f33":"code","72bffcd3":"code","b54fe44e":"code","d3f64a8c":"code","3c94a490":"code","bcfce4c1":"code","cb6289c3":"code","047c9270":"code","986a021e":"code","3f009560":"code","d62cf2a8":"code","f4fba740":"code","1f2ed3e3":"code","9918d64d":"code","83769b2e":"code","59fe9860":"code","67d4b936":"code","8de9f7a8":"code","a690cfd9":"code","db83d5fc":"markdown","d8764b75":"markdown","c60e1019":"markdown","aa42ddee":"markdown","e15e23fb":"markdown","2a018016":"markdown","e2ab9276":"markdown","8c2a3ee9":"markdown","22717100":"markdown","d610048b":"markdown","aa6caf7c":"markdown","7d34b11e":"markdown","fa16d2ca":"markdown"},"source":{"314e8412":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","628e3853":"# read heart as pandas dataframe\ndfheart = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')\n# read the o2Saturation\ndfo2 = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/o2Saturation.csv')","db08240d":"#check for missing values\ndfheart.info()","c8d25670":"dfo2.info()","5c91c124":"import matplotlib as plt","a0a72091":"# different scaling\nfig = plt.figure(figsize = (20,10))\nplt.xticks(rotation='vertical')\ndfheart.boxplot()\n","d869c9ff":"# look at distribution\ndfheart.hist()","47fdf519":"numer_var = ['age','trtbps','chol','thalachh','oldpeak']","2cbf2818":"categ_var = ['sex','cp','fbs','restecg','exng','slp','caa','thall']","f69e229b":"import seaborn as sns\nsns.pairplot(dfheart, hue = 'output')","e6d39bc9":"#age and thalachh, thalachh and oldpeak slightly negatively correlated\nsns.heatmap(dfheart[numer_var].corr(),annot=True)","eb8aa599":"# reduce the number of features\nnumer_var = ['age','thalachh','oldpeak']\ncateg_var = ['sex','cp','restecg','exng','slp','caa','thall']","6fc6016c":"df = dfheart[numer_var].copy()","21c49c71":"df[categ_var] = dfheart[categ_var].copy()","9b5683ba":"df['output'] = dfheart['output']","cfb60c53":"# loop to change each column to category type\nfor col in categ_var:\n    df[col] = df[col].astype('category',copy=False)","d6698ae9":"df.info()","1827935b":"#set categorical to dummies\ndf = pd.get_dummies(df)","5912d3fc":"df.info()","05b181a3":"#take logarithm of oldpeak\ndf['oldpeak']=np.log(1+df['oldpeak'])","5521609c":"#scaling numerical variables\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(df[numer_var])","646383b1":"df[numer_var] = scaler.fit_transform(df[numer_var])","9434af16":"df[numer_var].boxplot()","fa4c0414":"# split train and test\ny = df['output']\nX = df.drop('output', axis = 1)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.25,random_state=1234)","07c23a07":"print(len(X_train),',',len(X_test))","c54a3242":"# look at distribution\ny_train_dist=y_train.groupby(y_train.iloc[:]).size()\/y_train.size\ny_test_dist=y_test.groupby(y_test.iloc[:]).size()\/y_test.size\n\ntrain_test_dist = pd.DataFrame({'train': y_train_dist, 'test': y_test_dist})\nax = train_test_dist.plot.bar(rot=0) # rotation of the labels","ab113559":"# first try with simple models\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\n\nparameter = {\"C\":[0.01,0.1,1,5,10,20], \n             \"penalty\":[\"l1\",\"l2\"],\n            \"n_jobs\" : [-1],\n            \"random_state\" : [1234]}\nlogreg = LogisticRegression()\nlogreg_cv = GridSearchCV(logreg,parameter,cv=10,scoring='f1')\nlogreg_cv.fit(X_train,y_train)\n","083083a3":"logreg_cv.best_estimator_","a04e589a":"best_logreg = logreg_cv.best_estimator_\nbest_logreg.fit(X_train,y_train)","f548213b":"y_pred = best_logreg.predict(X_test)","eaaf16e0":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","4950b46e":"def print_performance(gs_model, model_name, X_train,X_test,y_train,y_test):\n   print('##################### \\n')\n   print(\"Best score: %f using %s\" % (gs_model.best_score_, gs_model.best_params_))\n   print('##################### \\n')\n   best_model = gs_model.best_estimator_\n   y_pred_test = best_model.predict(X_test)\n   y_pred_train = best_model.predict(X_train)\n   print('#####################')\n   print('f1 Score on the train: ',metrics.f1_score(y_train,y_pred_train))\n   print('f1 Score on the test: ',metrics.f1_score(y_test,y_pred_test))\n   print('##################### \\n')","bbe5894b":"from sklearn import metrics\nmodel_name = 'Logistic Regression'\nprint_performance(logreg_cv, model_name, X_train,X_test,y_train,y_test)","6c9905cb":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n\ndef plot_ROC_curve(model, y_test, y_pred, name):\n   logit_roc_auc = roc_auc_score(y_test, y_pred)\n   fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n   plt.figure()\n   plt.plot(fpr, tpr, label='Area = %0.2f' % logit_roc_auc)\n   plt.plot([0, 1], [0, 1],'r--')\n   plt.xlim([0.0, 1.0])\n   plt.ylim([0.0, 1.05])\n   plt.xlabel('False Positive Rate')\n   plt.ylabel('True Positive Rate')\n   plt.title('Receiver operating characteristic')\n   plt.legend(loc=\"lower right\")\n   plt.savefig(name)\n   plt.show()\nname = 'Log_ROC'\nplot_ROC_curve(best_logreg,y_test, y_pred, name)","bf22918d":"from sklearn.tree import DecisionTreeClassifier\nparameter = { 'criterion':['gini','entropy'],\n             'max_depth': np.arange(2, 4),#np.arange(3, 15)\n             'random_state' : [1234]\n            }\ntree = DecisionTreeClassifier()\ntree_cv = GridSearchCV(tree,parameter,cv=10,scoring='f1')\ntree_cv.fit(X_train,y_train)\n\n","c2a5e9f0":"best_tree = tree_cv.best_estimator_\nbest_tree.fit(X_train,y_train)\ny_pred = best_tree.predict(X_test)","5de13d22":"best_tree","545306b4":"print(classification_report(y_test, y_pred))","8e25b690":"model_name = 'Classification Tree'\nprint_performance(tree_cv, model_name, X_train,X_test,y_train,y_test)","3cb70167":"plot_ROC_curve(best_tree,y_test, y_pred, name)","7df23bfc":"# XGBoost is slow on this machine\nRUN_XGBOOST = False # set to true to fit XGBoost","fb4c7ff9":"\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n#classification tree obtained better results with depth 6\nparameters = {\n        'learning_rate' : [0.01,0.1,0.8], \n        'min_child_weight': [1, 5, 10],\n        'gamma': [0.5, 1, 1.5, 2, 5],\n        'subsample': [0.6, 0.8],\n        'colsample_bytree': [0.6, 0.8],\n        'max_depth': [3, 4, 5, 6, 9],\n        'lambda' : [0.1,0.5],\n        'alpha' : [0.1,0.5],\n        'random_state' : [1234]\n        }\nxgb_model = XGBClassifier()\ngs_xbg = GridSearchCV(xgb_model, parameters, cv=10, scoring = 'f1', n_jobs=-1)\nif RUN_XGBOOST == True:\n   gs_xbg = gs_xbg.fit(X_train,y_train)\n   best_xgboost = gs_xbg.best_estimator_\n   best_xgboost.fit(X_train,y_train)\n   y_pred = best_xgboost.predict(X_test)\n   print(classification_report(y_test, y_pred))","45ad2f33":"from sklearn.svm import SVC","72bffcd3":"SuppVectC = SVC()\nparameters = {'kernel':['linear','rbf','polinomial'], \n                 'C': [20,50,100],#[8,9,10],#[5,10,100],#[0.1,1,10,100], # tried different regularisations\n                 'gamma':[1], \n                 'degree':[2,3,4,5], # degree of polynomial kernel\n                 'random_state' : [1234]\n                 }\ngs_svc = GridSearchCV(SuppVectC, parameters, cv=10, scoring = 'f1', n_jobs=-1)\ngs_svc = gs_svc.fit(X_train, y_train)","b54fe44e":"gs_svc.best_estimator_","d3f64a8c":"best_svc = gs_svc.best_estimator_\nbest_svc.fit(X_train,y_train)\ny_pred = best_svc.predict(X_test)\n","3c94a490":"print(classification_report(y_test, y_pred))","bcfce4c1":"model_name = 'Support Vector Classifier'\nprint_performance(gs_svc, model_name, X_train,X_test,y_train,y_test)","cb6289c3":"from sklearn.neighbors import KNeighborsClassifier","047c9270":"KN_model = KNeighborsClassifier()\nparameters = {'n_neighbors':np.arange(3,30)}\ngs_KN = GridSearchCV(KN_model, parameters, cv=10, scoring = 'f1', verbose=10, n_jobs=-1)\ngs_KN = gs_KN.fit(X_train,y_train)","986a021e":"gs_KN.best_estimator_","3f009560":"best_knn = gs_KN.best_estimator_\nbest_knn.fit(X_train,y_train)\ny_pred = best_svc.predict(X_test)","d62cf2a8":"print(classification_report(y_test, y_pred))","f4fba740":"model_name = 'KNN Classifier'\nprint_performance(gs_KN, model_name, X_train,X_test,y_train,y_test)","1f2ed3e3":"plot_ROC_curve(gs_KN,y_test, y_pred, name)","9918d64d":"from sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier()\nparameters = {\"hidden_layer_sizes\": [(5, 2),(2,2,2,2,2,2),(3,3,3,3),(10,8,5)],#[(5, 2),(2,2,2,2,2,2),(3,3,3,3),(10,8,5),(100)], \n              \"max_iter\": [100,200], \n              \"alpha\": [1,2,5],#[0.00001,0.1,0.5,1], # #L2 penalty\n              #\"activation\" : ['logistic', 'tanh', 'relu'] <- with these overfitting\n              \"random_state\" : [1234]}\ngs_mlp = GridSearchCV(mlp, parameters, cv=10, scoring = 'f1', n_jobs=-1)\ngs_mlp = gs_mlp.fit(X_train, y_train)","83769b2e":"gs_mlp.best_estimator_","59fe9860":"best_mlp = gs_mlp.best_estimator_\nbest_mlp.fit(X_train,y_train)\ny_pred = best_mlp.predict(X_test)","67d4b936":"print(classification_report(y_test, y_pred))","8de9f7a8":"model_name = 'MLP Classifier'\nprint_performance(gs_mlp, model_name, X_train,X_test,y_train,y_test)","a690cfd9":"plot_ROC_curve(gs_mlp,y_test, y_pred, name)","db83d5fc":"# Logistic Regression <a id='logistic_regression'><\/a>\n\n[Back to Index](#index)","d8764b75":"# Classification Tree <a id='classification_tree'><\/a> <br>\n\nEven with small tree we have overfitting. <br>\n\n[Back to Index](#index)","c60e1019":"### Analyze the data <a id='analyze_data'><\/a>\n\n[Back to Index](#index)","aa42ddee":"# Feature Engeneering & Feature selection <a id='feature_engeneering'><\/a>\n\n[Back to Index](#index)","e15e23fb":"# XGBoost <a id='xgboost'><\/a>\n\n[Back to Index](#index)","2a018016":"# Multy Layer Perceptron <a id='mlp'><\/a>\n\n\n[Back to Index](#index)","e2ab9276":"oldpeak has exponential decay -> Take logarithm <br>","8c2a3ee9":"# Split train test <a id='split_train_test'><\/a>\n\n[Back to Index](#index)","22717100":"# Support Vector Classifier <a id='svc'><\/a>\n\nThis classifier is overfitting.\n\n[Back to Index](#index)","d610048b":"### Read Data <a id='read_data'><\/a>\n\n[Back to Index](#index)","aa6caf7c":"# KNN <a id='knn'><\/a>\n\nLess overfitting. Good candidate\n\n[Back to Index](#index)","7d34b11e":"Variables separating output <br>\nsex,\ncp,\nrest_ecg,\nthalachh,\nexng,\noldpeak,\nslp,\ncaa,\nthall\n","fa16d2ca":"## Index \n<a id='index'><\/a>\n[Read Data](#read_data) <br>\n[Analyze Data](#analyze_data) <br>\n[Feature engeneering and feature selection](#feature_engeneering) <br>\n[Split train test](#split_train_test) <br>\n[Logistic Regression](#logistic_regression) <br>\n[Classification tree](#classification_tree) <br>\n[XGBoost](#xgboost) <br>\n[Support Vector Classifier](#svc) <br>\n[KNN](#knn) <br>\n[MLP](#mlp) <br>\n\n "}}