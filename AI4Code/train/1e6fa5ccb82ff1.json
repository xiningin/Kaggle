{"cell_type":{"b441f5d4":"code","88a9359d":"code","fe8e498c":"code","662dee85":"code","55a4d9b8":"code","765d6722":"code","281a2bd5":"code","e371f03c":"code","819323a8":"code","91f441eb":"code","7a534988":"code","fbdf40b8":"code","0e282963":"code","f02a4cac":"code","9fa39cf2":"code","ced733ff":"code","458373a7":"code","62f17d77":"code","329353c5":"code","9a1a5eed":"code","02db9223":"code","5d5b06d3":"code","45cbff63":"code","d7573b2b":"code","9a473fec":"code","a4757c4a":"code","5f67f465":"code","0eb51618":"code","5732ee1b":"code","15b55024":"code","7ba93c3d":"code","7b2e1221":"code","5265e286":"code","72de7110":"code","1905d827":"code","fb172a25":"markdown","72ae378b":"markdown","404023b6":"markdown","82e3e1f2":"markdown","0721a50b":"markdown","3acad1bb":"markdown","b6fa33d3":"markdown","21e2cbc1":"markdown","7274b440":"markdown","1c217fd6":"markdown","27443cea":"markdown","ed44eaf2":"markdown","48762757":"markdown"},"source":{"b441f5d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport datetime\nfrom pathlib import Path\n\nfrom sklearn.metrics import roc_auc_score\n\nimport numpy as np\nimport os\nimport cv2\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# IMPORT KERAS LIBRARY\nfrom keras.applications.inception_v3 import InceptionV3\nfrom keras.applications.densenet import DenseNet121\nfrom keras.preprocessing import image\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, GlobalAveragePooling2D, MaxPooling2D, Flatten, BatchNormalization, Dropout\nfrom keras.callbacks import ModelCheckpoint\nfrom keras import backend as K\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\n%load_ext autoreload\n%autoreload","88a9359d":"model_path='.'\npath='..\/input\/chexpert'\ntrain_folder=f'{path}train'\ntest_folder=f'{path}test'\ntrain_lbl=f'{path}train_labels.csv'","fe8e498c":"chestxrays_root = Path(path)\ndata_path = chestxrays_root\/'chexp'","662dee85":"!ls '..\/input'","55a4d9b8":"full_train_df = pd.read_csv(data_path\/'CheXpert-v1.0-small\/train.csv')\nfull_valid_df = pd.read_csv(data_path\/'CheXpert-v1.0-small\/valid.csv')","765d6722":"chexnet_targets = ['No Finding',\n       'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity',\n       'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis',\n       'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture',\n       'Support Devices']\n\nchexpert_targets = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Pleural Effusion']","281a2bd5":"full_train_df.head()","e371f03c":"u_one_features = ['Atelectasis', 'Edema']\nu_zero_features = ['Cardiomegaly', 'Consolidation', 'Pleural Effusion']","819323a8":"def feature_string(row):\n    feature_list = []\n    for feature in u_one_features:\n        if row[feature] in [-1,1]:\n            feature_list.append(feature)\n            \n    for feature in u_zero_features:\n        if row[feature] == 1:\n            feature_list.append(feature)\n            \n    return ';'.join(feature_list)\n            \n     ","91f441eb":"full_train_df['train_valid'] = False\nfull_valid_df['train_valid'] = True","7a534988":"full_train_df['patient'] = full_train_df.Path.str.split('\/',3,True)[2]\nfull_train_df  ['study'] = full_train_df.Path.str.split('\/',4,True)[3]\n\nfull_valid_df['patient'] = full_valid_df.Path.str.split('\/',3,True)[2]\nfull_valid_df  ['study'] = full_valid_df.Path.str.split('\/',4,True)[3]","fbdf40b8":"full_df = pd.concat([full_train_df, full_valid_df])\nfull_df.head()","0e282963":"full_df['feature_string'] = full_df.apply(feature_string,axis = 1).fillna('')\nfull_df['feature_string'] =full_df['feature_string'] .apply(lambda x:x.split(\";\"))\nfull_df.head()","f02a4cac":"#get the first 5 whale images\npaths =  full_df.Path[:5]\nlabels = full_df.feature_string[:5]\n\nfig, m_axs = plt.subplots(1, len(labels), figsize = (20, 10))\n#show the images and label them\nfor ii, c_ax in enumerate(m_axs):\n    c_ax.imshow(cv2.imread(os.path.join(data_path,paths[ii])))\n    c_ax.set_title(labels[ii])","9fa39cf2":"from collections import Counter\n\nlabels_count = Counter(label for chexpert_targets in full_df['feature_string'] for label in chexpert_targets)\n#plt.bar(chexpert_targets, labels_count.values(), align='center', alpha=0.5)\n#plt.show\nx_pos = np.arange(len(labels_count.values()))\n#Plot the data:\nmy_colors = 'rgbkymc'\nlbls = list.copy(chexpert_targets)\nlbls.insert(0,'')\nplt.bar(x_pos, labels_count.values(), align='center', alpha=0.5 , color=my_colors)\nplt.xticks(x_pos, lbls, rotation='vertical')\n","ced733ff":"sample_perc = 0.00\ntrain_only_df = full_df[~full_df.train_valid]\nvalid_only_df = full_df[full_df.train_valid]\nunique_patients = train_only_df.patient.unique()\nmask = np.random.rand(len(unique_patients)) <= sample_perc\nsample_patients = unique_patients[mask]\n\ndev_df = train_only_df[full_train_df.patient.isin(sample_patients)]\ntrain_df = train_only_df[~full_train_df.patient.isin(sample_patients)]\n\nprint(valid_only_df.Path.size)\nprint(train_df.Path.size)","458373a7":"datagen=image.ImageDataGenerator(rescale=1.\/255, \n                                 featurewise_center=True,\n                                 featurewise_std_normalization=True,\n                                 rotation_range=5,\n                                 width_shift_range=0.2,\n                                 height_shift_range=0.2,\n                                 horizontal_flip=True,\n                                 validation_split = 0.1)\ntest_datagen=image.ImageDataGenerator(rescale=1.\/255)","62f17d77":"def generate_datasets(image_size = 224):\n\n    train_generator=datagen.flow_from_dataframe(dataframe=train_df, directory=data_path, \n                                                x_col=\"Path\", y_col=\"feature_string\", has_ext=True, seed = 42, #classes = chexpert_targets,\n                                                class_mode=\"categorical\", target_size=(image_size,image_size), batch_size=32, subset = \"training\")\n\n    validation_generator = datagen.flow_from_dataframe(dataframe=train_df, directory=data_path, \n                                                       x_col=\"Path\", y_col=\"feature_string\", has_ext=True, seed = 42, #classes = chexpert_targets,\n                                                       class_mode=\"categorical\", target_size=(image_size,image_size), batch_size=32, subset = \"validation\")\n\n    test_generator = test_datagen.flow_from_dataframe(dataframe=valid_only_df, directory=data_path, \n                                                      target_size=(image_size,image_size),class_mode='categorical',\n                                                      batch_size=1, shuffle=False, #classes = chexpert_targets,\n                                                      x_col=\"Path\", y_col=\"feature_string\")\n    \n    return [train_generator,validation_generator,test_generator]","329353c5":"def auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc","9a1a5eed":"def build_model(image_size = 224, load_previous_weights = True, freeze_cnn = False):\n    base_model = DenseNet121(include_top= False, input_shape=(image_size,image_size,3), weights='imagenet')\n\n    # add a global spatial average pooling layer\n    x = base_model.output\n    x = GlobalAveragePooling2D(input_shape=(1024,1,1))(x)\n    # Add a flattern layer \n    x = Dense(2048, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    # Add a fully-connected layer\n    x = Dense(512, activation='relu')(x)\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(x)\n    # and a logistic layer --  we have 5 classes\n    predictions = Dense(6, activation='sigmoid')(x)\n    \n    # this is the model we will train\n    model = Model(inputs=base_model.input, outputs=predictions)\n\n    # Recover previously trained weights\n    if load_previous_weights:\n        try:\n            model.load_weights('..\/input\/chexpert-keras-base\/weights.hdf5')\n            print('Weights successfuly loaded')\n        except:\n            print('Weights not loaded')\n\n    # first: train only the top layers (which were randomly initialized)\n    # i.e. freeze all convolutional InceptionV3 layers\n    if freeze_cnn:\n        for layer in base_model.layers:\n            layer.trainable = False\n        \n    # compile the model (should be done *after* setting layers to non-trainable)\n    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', auc])\n        \n    return model","02db9223":"def train_model(model , datasets, epochs=1, image_size = 224):\n    \n    checkpointer = ModelCheckpoint(filepath='weights.hdf5', \n                                   verbose=1, save_best_only=True)\n    \n    train_generator,validation_generator,test_generator = datasets\n    \n    STEP_SIZE_TRAIN=train_generator.n\/\/train_generator.batch_size\n    STEP_SIZE_VALID=validation_generator.n\/\/validation_generator.batch_size\n    print(STEP_SIZE_TRAIN)\n    print(STEP_SIZE_VALID)\n\n    history = model.fit_generator(generator=train_generator,\n                        steps_per_epoch=STEP_SIZE_TRAIN,\n                        validation_data=validation_generator,\n                        validation_steps=STEP_SIZE_VALID,\n                        epochs=epochs, callbacks = [checkpointer])\n    return history","5d5b06d3":"image_size_input = 224\nmodel = build_model(image_size = image_size_input)","45cbff63":"datasets = generate_datasets(image_size = image_size_input)\ntrain_generator,validation_generator,test_generator = datasets","d7573b2b":"history = train_model(model , datasets, epochs=3, image_size = image_size_input)\nhistory.history","9a473fec":"plt.figure(1)    \n# summarize history for accuracy     \nplt.subplot(211)  \nplt.plot(history.history['acc'])  \nplt.plot(history.history['val_acc'])  \nplt.title('model accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.legend(['train', 'test'], loc='upper left')  \n\n# summarize history for loss    \nplt.subplot(212)  \nplt.plot(history.history['loss'])  \nplt.plot(history.history['val_loss'])  \nplt.title('model loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['train', 'test'], loc='upper left')  \nplt.show()  ","a4757c4a":"import pickle\nwith open('\/trainHistoryDict', 'wb') as file_pi:\n    pickle.dump(history.history, file_pi)","5f67f465":"from sklearn.preprocessing import MultiLabelBinarizer\ntest = pd.Series(test_generator.labels)\nmlb = MultiLabelBinarizer()\ny_labels = mlb.fit_transform(test)","0eb51618":"test_generator.reset()\ny_pred_keras = model.predict_generator(test_generator,verbose = 1,steps=test_generator.n)","5732ee1b":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\n\nplt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\n\nfor ii in range(1, y_pred_keras.shape[1]):\n    fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_labels[:,ii], y_pred_keras[:,ii])\n    auc_keras = auc(fpr_keras, tpr_keras)\n    plt.plot(fpr_keras, tpr_keras, label=chexpert_targets[ii-1] + '(area = {:.3f})'.format(auc_keras))\n    \nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()\n    \n\n","15b55024":"# get the first image of the testing dataset\nx = test_generator[0][0]\npreds = model.predict(x)","7ba93c3d":"preds = y_pred_keras[1,:]\nclass_idx = np.argmax(preds)\nclass_output = model.output[:, class_idx]\n#import the last convolutional layer of the model, this depends on the model\nlast_conv_layer = model.get_layer(\"conv5_block16_concat\")","7b2e1221":"grads = K.gradients(class_output, last_conv_layer.output)[0]\npooled_grads = K.mean(grads, axis=(0, 1, 2))\niterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\npooled_grads_value, conv_layer_output_value = iterate([x])\nfor i in range(1024):\n    conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n","5265e286":"# Generate the heatmap\nheatmap = np.mean(conv_layer_output_value, axis=-1)\nheatmap = np.maximum(heatmap, 0)\nheatmap \/= np.max(heatmap)","72de7110":"img = x[0] \nheatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\nheatmap = np.uint8(255 * heatmap)\nimg = np.uint8(255 * img)\nheatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)","1905d827":"superimposed_img = cv2.addWeighted(img, 0.6, heatmap, 0.4, 0)\nplt.subplot(1,2,1)\nplt.imshow(img)\nplt.title(\"Original\")\nplt.subplot(1,2,2)\nplt.imshow(superimposed_img)\nplt.title(\"Heat map\")","fb172a25":"### Uncertainty Approaches\n\nThe CheXpert paper outlines several different approaches to mapping using the uncertainty labels in the data:\n\n- Ignoring - essentially removing from the calculation in the loss function\n- Binary mapping - sending uncertain values to either 0 or 1\n- Prevalence mapping - use the rate of prevelance of the feature as it's target value\n- Self-training - consider the uncertain values as unlabeled\n- 3-Class Classification - retain a separate value for uncertain and try to predict it as a class in its own right\n\n\nThe paper gives the results of different experiments with the above approaches and indicates the most accurate approach for each feature.\n    \n|Approach\/Feature|Atelectasis|Cardiomegaly|Consolidation|Edema|PleuralEffusion|\n|-----------|-----------|-----------|-----------|-----------|-----------|\n|`U-Ignore`|0.818(0.759,0.877)|0.828(0.769,0.888)|0.938(0.905,0.970)|0.934(0.893,0.975)|0.928(0.894,0.962)|\n|`U-Zeros`|0.811(0.751,0.872)|0.840(0.783,0.897)|0.932(0.898,0.966)|0.929(0.888,0.970)|0.931(0.897,0.965)|\n|`U-Ones`|**0.858(0.806,0.910)**|0.832(0.773,0.890)|0.899(0.854,0.944)|0.941(0.903,0.980)|0.934(0.901,0.967)|\n|`U-Mean`|0.821(0.762,0.879)|0.832(0.771,0.892)|0.937(0.905,0.969)|0.939(0.902,0.975)|0.930(0.896,0.965)|\n|`U-SelfTrained`|0.833(0.776,0.890)|0.831(0.770,0.891)|0.939(0.908,0.971)|0.935(0.896,0.974)|0.932(0.899,0.966)|\n|`U-MultiClass`|0.821(0.763,0.879)|**0.854(0.800,0.909)**|0.937(0.905,0.969)|0.928(0.887,0.968)|0.936(0.904,0.967)|\n\nThe binary mapping approaches (U-Ones and U-Zeros) are easiest to implement and so to begin with we take the best option between U-Ones and U-Zeros for each feature\n\n- Atelectasis `U-Ones`\n- Cardiomegaly `U-Zeros`\n- Consolidation `U-Zeros`\n- Edema `U-Ones`\n- Pleural Effusion `U-Zeros`","72ae378b":"### Add target features string","404023b6":"### Create patient and study columns","82e3e1f2":"# Creating a Heatmap to visualize the classification output","0721a50b":"Create a feature string containing all the classes and omiting the unknowns","3acad1bb":"This is to get the number of study and the number of patient for each entry of the dataset","b6fa33d3":"# Set up data generation","21e2cbc1":"Define the metrics","7274b440":"## Set up the dataframe for training: \n* Separate the validation and the training samples\n* Take a unique sample per patient\n* Get a samle of 5% of the dataset","1c217fd6":"### Load Data and the targets ","27443cea":"# Show some images","ed44eaf2":"# Build the model","48762757":"### Load configuration with local path and url for dataset"}}