{"cell_type":{"f0d309e3":"code","187a4098":"code","a4ff5045":"code","2abf90b7":"code","cda17ff0":"code","bf54d34b":"code","4b2acd9d":"code","aaab75a8":"code","0c66a024":"code","453bba3d":"code","30019099":"code","1c1ba497":"code","058e356a":"code","e61241d2":"code","d93642d9":"code","18f72a76":"code","e2214205":"code","e4e01640":"code","758eea19":"code","9bd4302d":"code","db04ccfd":"code","aa986303":"code","2b9f299e":"code","25af1def":"code","e1e2aa26":"code","3712d432":"code","4774aadf":"code","b3dd2418":"code","b0ffcd11":"code","b850008a":"markdown","8334ac42":"markdown","c9c72ce7":"markdown","50991d6d":"markdown","caeac6d2":"markdown","04ca0133":"markdown","07ff8eea":"markdown","2b955aa1":"markdown","baca7b62":"markdown","6ddf7d93":"markdown","b85f76a7":"markdown","b244895e":"markdown"},"source":{"f0d309e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","187a4098":"df = pd.read_csv('\/kaggle\/input\/spam-filter\/emails.csv')","a4ff5045":"df.head()","2abf90b7":"df.shape","cda17ff0":"df.info()","bf54d34b":"df.isna().sum()","4b2acd9d":"df['spam'].value_counts()","aaab75a8":"sns.countplot(df['spam'])","0c66a024":"from nltk import word_tokenize","453bba3d":"def count_words(text):\n    words = word_tokenize(text)\n    return len(words)","30019099":"df['count']=df['text'].apply(count_words)","1c1ba497":"df['count']","058e356a":"df.groupby('spam')['count'].mean()","e61241d2":"import string\nfrom nltk.corpus import stopwords\n","d93642d9":"def process_text(text):\n    no_punc = [char for char in text if char not in string.punctuation]\n    no_punc = ''.join(no_punc)\n    \n    \n    return ' '.join([word for word in no_punc.split() if word.lower() not in stopwords.words('english')])","18f72a76":"df['text']=df['text'].apply(process_text)","e2214205":"df['text']","e4e01640":"from nltk.stem import PorterStemmer\nstemmer = PorterStemmer()","758eea19":"def stemming (text):\n    return ''.join([stemmer.stem(word) for word in text])","9bd4302d":"df['text']=df['text'].apply(stemming)","db04ccfd":"df.head()","aa986303":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer= CountVectorizer()\nmessage_bow = vectorizer.fit_transform(df['text'])","2b9f299e":"#print(vectorizer.get_feature_names())\n#print(message_bow.toarray())","25af1def":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(message_bow,df['spam'],test_size=0.20)","e1e2aa26":"from sklearn.naive_bayes import MultinomialNB\nnb= MultinomialNB()\nnb.fit(X_train,y_train)\ny_pred = nb.predict(X_test)","3712d432":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","4774aadf":"from sklearn.metrics import plot_roc_curve\nplot_roc_curve(nb,X_test,y_test)","b3dd2418":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(nb,X_test,y_test)","b0ffcd11":"from sklearn.model_selection import KFold, cross_val_score\nkfold = KFold(n_splits=5,shuffle=True)\nprint(\"Accuracy using Cross Validation is :\",np.mean(cross_val_score(nb,message_bow,df['spam'],cv=kfold,scoring=\"accuracy\"))*100,\" %\")","b850008a":"# Creating the Model and it's Evaluation","8334ac42":"**After cleaning the text. We will now carry out the process of Stemming to reduce infected words to their root**","c9c72ce7":"**Splitting the Data[](http:\/\/)**","50991d6d":"Loading and Observing the Dataset","caeac6d2":"Applying the function to df['text'] and storing the count in another column","04ca0133":"**Exploratory Data Analysis**","07ff8eea":"**Function to Process the text data and 1. Remove Punctuation 2.Stop Words 3.Stemming**","2b955aa1":"# Text Prepreocessing","baca7b62":"Function that tokenizes each and every email into words and returns it's length","6ddf7d93":"**Now we will use Count Vectorizer to convert string data into Bag of Words ie Known Vocabulary**","b85f76a7":"Checking the Length of email and it's relation","b244895e":"No Missing Values\n"}}