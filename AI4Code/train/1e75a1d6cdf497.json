{"cell_type":{"7b7c0e56":"code","9782230e":"code","8d5ea497":"code","f64781f1":"code","fc2de0ad":"code","3f8190f2":"code","51aff9f8":"code","eb9103e8":"code","fe9f7d1b":"code","8d651ce8":"code","cb2c4873":"code","969882e9":"code","21e4fe17":"code","92b6bc4f":"code","66f8ae09":"code","b344b940":"code","68f5ff62":"code","346069a9":"code","a00adcda":"code","274c2f04":"code","b56e53f6":"code","1f0349f5":"code","a0de629f":"code","a0faf701":"code","1670a199":"code","128151d2":"code","2cc908ee":"code","70f2fd39":"code","8566b9c3":"code","583ac3a9":"code","4f50269a":"code","53d86533":"code","100a7ad7":"code","baa81d7d":"code","4ad4ca1b":"code","56b7e43b":"code","f93417e6":"code","b8f1a023":"code","a0f6f722":"code","fe14d6b1":"code","242c3b1a":"code","db875769":"code","87340885":"code","7619d7e9":"code","c361bbf2":"code","f287d5c9":"code","4dfe9cc1":"code","c47261e9":"code","9c864262":"code","16a38ef4":"code","8bebcf61":"code","23b25a69":"code","7663c5f8":"code","a2a0a633":"code","a243cf10":"code","2b43e2dd":"code","ee631a8c":"code","a5f2b315":"code","6ee49443":"code","c3ee9465":"code","f865adaf":"code","00025181":"code","d7debd84":"code","2fd3df4b":"markdown","115e5764":"markdown","50f0bb67":"markdown","08c35db3":"markdown","ad337e10":"markdown","4b24ee65":"markdown","f40526a1":"markdown","a973be55":"markdown","ff79d41b":"markdown","516f04c9":"markdown","31b660ea":"markdown","521836dd":"markdown","6be3ad2d":"markdown"},"source":{"7b7c0e56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport scipy\nfrom scipy.stats import norm \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.offline as offline\noffline.init_notebook_mode()","9782230e":"application_train=pd.read_csv(r\"..\/input\/application_train.csv\")\napplication_test=pd.read_csv(r\"..\/input\/application_test.csv\")\nbureau_balance=pd.read_csv(r\"..\/input\/bureau_balance.csv\")\nbureau=pd.read_csv(r\"..\/input\/bureau.csv\")\ncredit_card_balance=pd.read_csv(r\"..\/input\/credit_card_balance.csv\")\nPOS_cash=pd.read_csv(r\"..\/input\/POS_CASH_balance.csv\")\nbureau=pd.read_csv(r\"..\/input\/bureau.csv\")\nprevious_application=pd.read_csv(r\"..\/input\/previous_application.csv\")\ninstall_payment=pd.read_csv(r\"..\/input\/installments_payments.csv\")","8d5ea497":"application_train.head()","f64781f1":"application_train.columns.values","fc2de0ad":"application_test.head()","3f8190f2":"application_test.columns.values","51aff9f8":"bureau.head()","eb9103e8":"bureau.info()","fe9f7d1b":"credit_card_balance.head()","8d651ce8":"application_train.shape","cb2c4873":"    application_train.describe()","969882e9":"#finding missing value of train dataset\ntotal_null=application_train.isnull().sum().sort_values(ascending=False)\npercentage=(application_train.isnull().sum()\/application_train.isnull().count() *100).sort_values(ascending=False)\nmissing_train_data=pd.concat([total_null,percentage],axis=1,keys=[\"Total_null\",\"Percentage\"])","21e4fe17":"missing_train_data.head()","92b6bc4f":"POS_cash.head()","66f8ae09":"POS_cash.shape","b344b940":"#finding missing value of POS_cash dataset\ntotal_null=POS_cash.isnull().sum().sort_values(ascending=False)\npercentage=(POS_cash.isnull().sum()\/POS_cash.isnull().count() *100).sort_values(ascending=False)\nmissing_POS_data=pd.concat([total_null,percentage],axis=1,keys=[\"Total_null\",\"Percentage\"])","68f5ff62":"missing_POS_data","346069a9":"#finding missing value of bureau dataset\ntotal_null=bureau.isnull().sum().sort_values(ascending=False)\npercentage=(bureau.isnull().sum()\/bureau.isnull().count() *100).sort_values(ascending=False)\nmissing_bureau_data=pd.concat([total_null,percentage],axis=1,keys=[\"Total_null\",\"Percentage\"])\nmissing_bureau_data.head(15)","a00adcda":"#missing value of bureau_balance dataset\ntotal_null=bureau_balance.isnull().sum().sort_values(ascending=False)\npercentage=(bureau_balance.isnull().sum()\/bureau_balance.isnull().count() *100).sort_values(ascending=False)\nmissing_bureau_balance=pd.concat([total_null,percentage],axis=1,keys=[\"Total_null\",\"Percentage\"])\nmissing_bureau_balance.head(15)","274c2f04":"#missing value of previous_application dataset\ntotal_null=previous_application.isnull().sum().sort_values(ascending=False)\npercentage=(previous_application.isnull().sum()\/previous_application.isnull().count() *100).sort_values(ascending=False)\nmissing_previous_application=pd.concat([total_null,percentage],axis=1,keys=[\"Total_null\",\"Percentage\"])\nmissing_previous_application.head(15)","b56e53f6":"#missing value of install_payment dataset\ntotal_null=install_payment.isnull().sum().sort_values(ascending=False)\npercentage=(install_payment.isnull().sum()\/install_payment.isnull().count() *100).sort_values(ascending=False)\nmissing_installment=pd.concat([total_null,percentage],axis=1,keys=[\"Total_null\",\"Percentage\"])\nmissing_installment.head()","1f0349f5":"#missing value of application_test dataset\ntotal_null=application_test.isnull().sum().sort_values(ascending=False)\npercentage=(application_test.isnull().sum()\/application_test.isnull().count() *100).sort_values(ascending=False)\nmissing_app_test=pd.concat([total_null,percentage],axis=1,keys=[\"Total_null\",\"Percentage\"])\nmissing_app_test.head(15)","a0de629f":"#lets checkout the discriptive plot of the amt_credit present in training dataset \nplt.figure(figsize=(18,8),dpi=100)\nsns.distplot(application_train['AMT_CREDIT'],fit=norm,color=\"red\");\nplt.title(\"Amt_credit distribution\");","a0faf701":"plt.figure(figsize=(10,5),dpi=80)\nplt.hist(application_train['TARGET'],bins=50);","1670a199":"target_count = application_train.TARGET.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ncount_class_0, count_class_1 = application_train.TARGET.value_counts()\n\n# Divide by class\ndf_class_0 = application_train[application_train['TARGET'] == 0]\ndf_class_1 = application_train[application_train['TARGET'] == 1]\ndf_class_0_under = df_class_0.sample(count_class_1)\ndf_test_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Random under-sampling:')\nprint(df_test_under.TARGET.value_counts())\n\ndf_test_under.TARGET.value_counts().plot(kind='bar', title='Count (TARGET)');","128151d2":"temp=application_train['NAME_CONTRACT_TYPE'].value_counts()\nx=temp.index\ny=temp.values\nplt.pie(x=temp.values,explode=(0.1,0),labels=temp.index,startangle=90,autopct='%1.1f%%',\n        colors=['#F1BF1B','#B1F11B'],frame=False,radius=1.5)","2cc908ee":"#purpose of loan\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10,8),dpi=100)\ncar_loan=application_train['FLAG_OWN_CAR'].value_counts()\nx=car_loan.index\ny=car_loan.values\nplt.subplot(2,2,1)\nplt.pie(x=car_loan.values,explode=(0.1,0),labels=car_loan.index,\n        startangle=90,autopct='%1.1f%%');\nplt.subplot(2,2,2)\nrealty_loan=application_train['FLAG_OWN_REALTY'].value_counts()\nX=realty_loan.index\nY=realty_loan.values\nplt.pie(x=realty_loan.values,explode=(0.1,0),labels=realty_loan.index,\n        startangle=90,autopct='%1.1f%%');","70f2fd39":"plt.figure(figsize=(13,5),dpi=200)\nloan_applicant=application_train['NAME_INCOME_TYPE'].value_counts()\nvalue=(loan_applicant\/application_train['NAME_INCOME_TYPE'].count()*100)\nsns.barplot(y=value,x=loan_applicant.index,hue=loan_applicant.index,ci=100)\nplt.ylabel(\"pecentage of loan applicant\",size=15)","8566b9c3":"plt.figure(figsize=(10,5),dpi=100)\napplicant_status=application_train['NAME_FAMILY_STATUS'].value_counts()\nsns.barplot(y=applicant_status.values,x=applicant_status.index,hue=applicant_status.index,ci=100)\nplt.title(\"Family Status of Applicant\",size=20,color=\"red\")","583ac3a9":"plt.figure(figsize=(10,6),dpi=100)\ntemp=application_train['NAME_HOUSING_TYPE'].value_counts()\nsns.barplot(y=temp.values,x=temp.index,hue=temp.index);\nplt.xlabel(\"Housing Type\",size=14,color=\"red\")\nplt.ylabel(\"frequency\",size=14,color=\"red\")\nplt.title(\"Housing Type Name\",size=20,color=\"blue\")","4f50269a":"#Occupation of Apllicant \nplt.figure(figsize=(15,10),dpi=100)\nocc=application_train['OCCUPATION_TYPE'].value_counts()\nsns.barplot(y=occ.values,x=occ.index)\nplt.xlabel(\"Occupation\",size=15,color=\"blue\")\nplt.ylabel(\"Frequency\",size=15,color=\"blue\")\nplt.title(\"Occupation Loan Applicant\",size=20,color=\"red\")\nplt.xticks(rotation=45);","53d86533":"#Education Of Applicant\nplt.figure(figsize=(10,5),dpi=100)\nedu=application_train[\"NAME_EDUCATION_TYPE\"].value_counts()\nsns.barplot(x=edu.index,y=edu.values)\nplt.xlabel(\"Education\",size=15,color=\"blue\")\nplt.ylabel(\"Frequency\",size=15,color=\"blue\")\nplt.title(\"Education of  Loan Applicant\",size=20,color=\"red\")\nplt.xticks(rotation=45,size=10);","100a7ad7":"temp = application_train[\"NAME_INCOME_TYPE\"].value_counts()\n#print(temp.values)\ntemp_y0 = []\ntemp_y1 = []\nfor val in temp.index:\n    temp_y1.append(np.sum(application_train[\"TARGET\"][application_train[\"NAME_INCOME_TYPE\"]==val] == 1))\n    temp_y0.append(np.sum(application_train[\"TARGET\"][application_train[\"NAME_INCOME_TYPE\"]==val] == 0))    \ntrace1 = go.Bar(\n    x = temp.index,\n    y = (temp_y1 \/ temp.sum()) * 100,\n    name='YES'\n)\ntrace2 = go.Bar(\n    x = temp.index,\n    y = (temp_y0 \/ temp.sum()) * 100, \n    name='NO'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Income sources of Applicant's in terms of loan is repayed or not  in %\",\n    #barmode='stack',\n    width = 1000,\n    xaxis=dict(\n        title='Income source',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","baa81d7d":"temp = application_train[\"OCCUPATION_TYPE\"].value_counts()\n#print(temp.values)\ntemp_y0 = []\ntemp_y1 = []\nfor val in temp.index:\n    temp_y1.append(np.sum(application_train[\"TARGET\"][application_train[\"OCCUPATION_TYPE\"]==val] == 1))\n    temp_y0.append(np.sum(application_train[\"TARGET\"][application_train[\"OCCUPATION_TYPE\"]==val] == 0))    \ntrace1 = go.Bar(\n    x = temp.index,\n    y = (temp_y1 \/ temp.sum()) * 100,\n    name='YES'\n)\ntrace2 = go.Bar(\n    x = temp.index,\n    y = (temp_y0 \/ temp.sum()) * 100, \n    name='NO'\n)\n\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title = \"Occupation of Applicant's in terms of loan is repayed or not in %\",\n    #barmode='stack',\n    width = 1000,\n    xaxis=dict(\n        title='Occupation of Applicant\\'s',\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='Count in %',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n)\n)\n\nfig = go.Figure(data=data, layout=layout)\niplot(fig)","4ad4ca1b":"prev=previous_application['NAME_CLIENT_TYPE'].value_counts()\ntrace = go.Pie(labels=prev.index, values=prev.values)\npy.iplot([trace], filename='basic_pie_chart')","56b7e43b":"prev1=previous_application[\"NAME_CONTRACT_TYPE\"].value_counts()\ntrace = go.Pie(labels=prev1.index, values=prev1.values)\npy.iplot([trace], filename='basic_pie_chart')","f93417e6":"prev2=previous_application[\"CHANNEL_TYPE\"].value_counts()\ntrace = go.Pie(labels=prev2.index, values=prev2.values)\npy.iplot([trace], filename='basic_pie_chart')","b8f1a023":"suite = previous_application['NAME_TYPE_SUITE'].value_counts()\ntrace = go.Pie(labels=suite.index, values=suite.values)\npy.iplot([trace], filename='basic_pie_chart')","a0f6f722":"application_train.select_dtypes('object').info()","fe14d6b1":"application_train.select_dtypes('int').info()","242c3b1a":"application_train.select_dtypes(\"object\").nunique()","db875769":"from sklearn.preprocessing import LabelEncoder\nlbl=LabelEncoder()\nlbl_count=0\nfor i in application_train:\n    if application_train[i].dtype=='object':\n        if len(list(application_train[i].unique())) <= 2:\n            lbl.fit(application_train[i])\n            #through this code we encode only those column who have less \n            #than or equal to 2 categorical variable\n            application_train[i]=lbl.transform(application_train[i])\n            application_test[i]=lbl.transform(application_test[i])\n            lbl_count +=1\nprint('%d column were encoded.'%lbl_count)\n  \n            \n","87340885":"application_train.select_dtypes(\"object\").nunique()","7619d7e9":"#for one hot enocder we use pd.get_dummies\napplication_train=pd.get_dummies(application_train)\napplication_test=pd.get_dummies(application_test)\nprint(\"application_train feature shape:\",application_train.shape)\nprint(\"application_test feature shape:\",application_test.shape)","c361bbf2":"train_target=application_train['TARGET']\napplication_train,application_test=application_train.align(application_test,axis=1,join='inner')","f287d5c9":"application_train['TARGET']=train_target","4dfe9cc1":"application_train.head()","c47261e9":"#as DAYS_BIRTH given negative in the dataset we hav to make positive to analyse in years\n(application_train['DAYS_BIRTH']\/(-365)).describe()","9c864262":"application_train.isnull().sum()","16a38ef4":"application_train.isnull().sum()","8bebcf61":"prev_category = pd.get_dummies(previous_application)\nbureau_category = pd.get_dummies(bureau)\npos_category = pd.get_dummies(POS_cash)\ncredit_category= pd.get_dummies(credit_card_balance)\n","23b25a69":"application_train=application_train.fillna(0)\napplication_test=application_test.fillna(0)","7663c5f8":"from sklearn.model_selection import train_test_split \nimport lightgbm as lgb\n\napplication_test['is_test'] = 1 \napplication_test['is_train'] = 0\napplication_train['is_test'] = 0\napplication_train['is_train'] = 1\n\n# target variable\nY = application_train['TARGET']\ntrain_X = application_train.drop(['TARGET'], axis = 1)\n\n# test ID\ntest_id = application_train['SK_ID_CURR']\ntest_X = application_test\n\n# merge train and test datasets for preprocessing\ndata = pd.concat([train_X, test_X], axis=0)","a2a0a633":"prev_apps = previous_application[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nprevious_application['SK_ID_PREV'] = previous_application['SK_ID_CURR'].map(prev_apps['SK_ID_PREV'])\n\n## Average values for all other features in previous applications\nprev_apps_avg = previous_application.groupby('SK_ID_CURR').mean()\nprev_apps_avg.columns = ['p_' + col for col in prev_apps_avg.columns]\ndata = data.merge(right=prev_apps_avg.reset_index(), how='left', on='SK_ID_CURR')","a243cf10":"bureau_avg = bureau.groupby('SK_ID_CURR').mean()\nbureau_avg['buro_count'] = bureau[['SK_ID_BUREAU','SK_ID_CURR']].groupby('SK_ID_CURR').count()['SK_ID_BUREAU']\nbureau_avg.columns = ['b_' + f_ for f_ in bureau_avg.columns]\ndata = data.merge(right=bureau_avg.reset_index(), how='left', on='SK_ID_CURR')","2b43e2dd":"install_pay= install_payment[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ninstall_payment['SK_ID_PREV'] = install_payment['SK_ID_CURR'].map(install_pay['SK_ID_PREV'])\n\n## Average values for all other variables in installments payments\navg_inst = install_payment.groupby('SK_ID_CURR').mean()\navg_inst.columns = ['i_' + f_ for f_ in avg_inst.columns]\ndata = data.merge(right=avg_inst.reset_index(), how='left', on='SK_ID_CURR')","ee631a8c":"pos_cash = POS_cash[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\nPOS_cash['SK_ID_PREV'] = POS_cash['SK_ID_CURR'].map(pos_cash['SK_ID_PREV'])\n\n## Average Values for all other variables in pos cash\nPOS_avg = POS_cash.groupby('SK_ID_CURR').mean()\ndata = data.merge(right=POS_avg.reset_index(), how='left', on='SK_ID_CURR')","a5f2b315":"credit_balns= credit_card_balance[['SK_ID_CURR', 'SK_ID_PREV']].groupby('SK_ID_CURR').count()\ncredit_card_balance['SK_ID_PREV'] = credit_card_balance['SK_ID_CURR'].map(credit_balns['SK_ID_PREV'])\n\n### average of all other columns \navg_credit_bal = credit_card_balance.groupby('SK_ID_CURR').mean()\navg_credit_bal.columns = ['credit_bal_' + f_ for f_ in avg_credit_bal.columns]\ndata = data.merge(right=avg_credit_bal.reset_index(), how='left', on='SK_ID_CURR')","6ee49443":"#final training and testing data\nignore_features = ['SK_ID_CURR', 'is_train', 'is_test']\nrelevant_features = [col for col in data.columns if col not in ignore_features]\ntrainX = data[data['is_train'] == 1][relevant_features]\ntestX = data[data['is_test'] == 1][relevant_features]","c3ee9465":"x_train, x_val, y_train, y_val = train_test_split(trainX, Y, test_size=0.2, random_state=18)\nlgb_train = lgb.Dataset(data=x_train, label=y_train)\nlgb_eval = lgb.Dataset(data=x_val, label=y_val)","f865adaf":"import lightgbm as lgb\nparams = {'task': 'train', 'boosting_type': 'gbdt', 'objective': 'binary', 'metric': 'auc', \n          'learning_rate': 0.01, 'num_leaves': 48, 'num_iteration': 5000, 'verbose': 0 ,\n          'colsample_bytree':.8, 'subsample':.9, 'max_depth':7, 'reg_alpha':.1, 'reg_lambda':.1, \n          'min_split_gain':.01, 'min_child_weight':1}\nmodel = lgb.train(params, lgb_train, valid_sets=lgb_eval, early_stopping_rounds=170, verbose_eval=200)","00025181":"lgb.plot_importance(model, max_num_features=100, figsize=(15, 30),color=\"red\")\n","d7debd84":"preds = model.predict(testX)\nsub = application_test[['SK_ID_CURR']].copy()\nsub['TARGET'] = preds\nsub.to_csv('sub.csv', index= False)\nsub.head(10)","2fd3df4b":"|||||housing type","115e5764":"|||||**Now we to have to check whether is there any missing value or not \nif there is  missing  value then we can find it by how much total and what is the percentage of missing value in that particular columns**","50f0bb67":"most of the catogrical value having small number of unique entity.","08c35db3":"||||||||Checking the head of each dataset ","ad337e10":"|||||**we can see that the data is highly imbalanced**","4b24ee65":"|||||Here we can see  that the top applicant who were applied for loan :\n\nLaborers:55-57k\n\nSales-Staff:32-33k \n\ncore_staff:28-29k\n\nmanagers:approx 22k \n","f40526a1":"||||Most of the loans are Cash loans which were taken by applicants. 90.5 % loans are Cash loans.\n","a973be55":"|||||Family Status of Applicant","ff79d41b":"thats looks good there is no outlier in the age field","516f04c9":"As we seen that in [dytpes=object] we have few unique entries so we have to deal with these categorical variable.\nfor this we have to use label encoder and pandas dummy\nwe use label encoder to deal  to with categorical variable who have only two unique entries so that it can asign only 0 and 1 and more than two entites we use one hot encoder to dea with","31b660ea":"|||||**Income Source of Applicant who applied for loan**","521836dd":"**Loan Is Repayed Or Not**","6be3ad2d":"|||||**We have seen through the info that only categorical  value have null value** "}}