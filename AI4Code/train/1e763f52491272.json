{"cell_type":{"5b5d2ba0":"code","6aae04c3":"code","a1dc5d3d":"code","41355493":"code","114fe2e1":"code","0bc79354":"code","58de82c6":"code","f6c29e32":"code","8208a300":"code","41417bdf":"code","f0d9c3bf":"code","373ed7a4":"code","a8ccdeba":"code","7110caa9":"code","01b7c144":"code","efc976c6":"code","2c66f3d0":"code","6fee0150":"code","3f6cd24f":"code","f2683ebf":"code","b5aab5b1":"code","2152ef59":"code","434eb3c6":"code","fba0199d":"code","9678a2e3":"code","e488b3dc":"code","3ea89e54":"code","9d31f887":"code","2bfef606":"code","f5b46c8a":"code","133791c4":"code","6978c784":"code","80cb599e":"code","78965e02":"code","c6174e72":"code","3bfbcd10":"code","eff1fc8a":"code","4f4600d5":"code","7f73f65a":"code","9d0fee84":"code","1b050927":"code","d569d9bf":"code","35c647fb":"code","054d315a":"code","536156f7":"code","6b9ee1dd":"code","f7362793":"markdown","0aff9c17":"markdown","83975878":"markdown","15d3adb1":"markdown","8b630196":"markdown","906f0435":"markdown","ad4c8274":"markdown","5cd41d25":"markdown","36235416":"markdown","77f22715":"markdown","9396c452":"markdown","500d9381":"markdown","f5cb3add":"markdown","65e7211d":"markdown","0363b886":"markdown","f0329304":"markdown","2eb1429e":"markdown","1f343adf":"markdown","d9206d68":"markdown","22137aab":"markdown","115ebbf2":"markdown","4ff9b3ce":"markdown","c162084a":"markdown","1317ddd9":"markdown","41cc3037":"markdown","80f89618":"markdown","0352f589":"markdown","6f354f82":"markdown","6ed6705f":"markdown","f8e9336d":"markdown"},"source":{"5b5d2ba0":"from numpy.random import seed\nseed(101)\nfrom tensorflow import set_random_seed\nset_random_seed(101)\n\nimport pandas as pd\nimport numpy as np\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten, Activation\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.optimizers import Adam\n\nimport os\nimport cv2\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline","6aae04c3":"\nIMAGE_SIZE = 96\nIMAGE_CHANNELS = 3\n\nSAMPLE_SIZE = 80000 # the number of images we use from each of the two classes\n","a1dc5d3d":"os.listdir('..\/input\/histopathologic-cancer-detection')","41355493":"\nprint(len(os.listdir('..\/input\/histopathologic-cancer-detection\/train')))\n","114fe2e1":"df_data = pd.read_csv('..\/input\/histopathologic-cancer-detection\/train_labels.csv')\n\n# removing this image because it caused a training error previously\ndf_data = df_data[df_data['id'] != 'dd6dfed324f9fcb6f93f46f32fc800f2ec196be2']\n\n# removing this image because it's black\ndf_data = df_data[df_data['id'] != '9369c7278ec8bcc6c880d99194de09fc2bd4efbe']\n\n\nprint(df_data.shape)","0bc79354":"df_data['label'].value_counts()","58de82c6":"# source: https:\/\/www.kaggle.com\/gpreda\/honey-bee-subspecies-classification\n\ndef draw_category_images(col_name,figure_cols, df, IMAGE_PATH):\n    \n    \"\"\"\n    Give a column in a dataframe,\n    this function takes a sample of each class and displays that\n    sample on one row. The sample size is the same as figure_cols which\n    is the number of columns in the figure.\n    Because this function takes a random sample, each time the function is run it\n    displays different images.\n    \"\"\"\n    \n\n    categories = (df.groupby([col_name])[col_name].nunique()).index\n    f, ax = plt.subplots(nrows=len(categories),ncols=figure_cols, \n                         figsize=(4*figure_cols,4*len(categories))) # adjust size here\n    # draw a number of images for each location\n    for i, cat in enumerate(categories):\n        sample = df[df[col_name]==cat].sample(figure_cols) # figure_cols is also the sample size\n        for j in range(0,figure_cols):\n            file=IMAGE_PATH + sample.iloc[j]['id'] + '.tif'\n            im=cv2.imread(file)\n            ax[i, j].imshow(im, resample=True, cmap='gray')\n            ax[i, j].set_title(cat, fontsize=16)  \n    plt.tight_layout()\n    plt.show()\n    ","f6c29e32":"IMAGE_PATH = '..\/input\/histopathologic-cancer-detection\/train\/' \n\ndraw_category_images('label',4, df_data, IMAGE_PATH)","8208a300":"df_data.head()","41417bdf":"# take a random sample of class 0 with size equal to num samples in class 1\ndf_0 = df_data[df_data['label'] == 0].sample(SAMPLE_SIZE, random_state = 101)\n# filter out class 1\ndf_1 = df_data[df_data['label'] == 1].sample(SAMPLE_SIZE, random_state = 101)\n\n# concat the dataframes\ndf_data = pd.concat([df_0, df_1], axis=0).reset_index(drop=True)\n# shuffle\ndf_data = shuffle(df_data)\n\ndf_data['label'].value_counts()","f0d9c3bf":"df_data.head()","373ed7a4":"# train_test_split\n\n# stratify=y creates a balanced validation set.\ny = df_data['label']\n\ndf_train, df_val = train_test_split(df_data, test_size=0.10, random_state=101, stratify=y)\n\nprint(df_train.shape)\nprint(df_val.shape)","a8ccdeba":"df_train['label'].value_counts()","7110caa9":"df_val['label'].value_counts()","01b7c144":"# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 2 folders inside 'base_dir':\n\n# train_dir\n    # a_no_met_tissue\n    # b_has_met_tissue\n\n# val_dir\n    # a_no_met_tissue\n    # b_has_met_tissue\n\n\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN AND VALIDATION FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nno_met_tissue = os.path.join(train_dir, 'a_no_met_tissue')\nos.mkdir(no_met_tissue)\nhas_met_tissue = os.path.join(train_dir, 'b_has_met_tissue')\nos.mkdir(has_met_tissue)\n\n\n# create new folders inside val_dir\nno_met_tissue = os.path.join(val_dir, 'a_no_met_tissue')\nos.mkdir(no_met_tissue)\nhas_met_tissue = os.path.join(val_dir, 'b_has_met_tissue')\nos.mkdir(has_met_tissue)\n\n","efc976c6":"# check that the folders have been created\nos.listdir('base_dir\/train_dir')","2c66f3d0":"# Set the id as the index in df_data\ndf_data.set_index('id', inplace=True)","6fee0150":"\n\n# Get a list of train and val images\ntrain_list = list(df_train['id'])\nval_list = list(df_val['id'])\n\n\n\n# Transfer the train images\n\nfor image in train_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname_tif = image + '.tif'\n    # get the label for a certain image\n    target = df_data.loc[image,'label']\n    \n    # these must match the folder names\n    if target == 0:\n        label = 'a_no_met_tissue'\n    if target == 1:\n        label = 'b_has_met_tissue'\n    \n    # source path to image\n    src = os.path.join('..\/input\/histopathologic-cancer-detection\/train', fname_tif)\n    # change the new file name to png\n    fname_png = image + '.png'\n    # destination path to image\n    dst = os.path.join(train_dir, label, fname_png)\n\n    \n    # read the file as an array\n    cv2_image = cv2.imread(src)\n    # save the image at the destination as a png file\n    cv2.imwrite(dst, cv2_image)\n \n\n\n# Transfer the val images\n\nfor image in val_list:\n    \n    # the id in the csv file does not have the .tif extension therefore we add it here\n    fname_tif = image + '.tif'\n    # get the label for a certain image\n    target = df_data.loc[image,'label']\n    \n    # these must match the folder names\n    if target == 0:\n        label = 'a_no_met_tissue'\n    if target == 1:\n        label = 'b_has_met_tissue'\n    \n\n    # source path to image\n    src = os.path.join('..\/input\/histopathologic-cancer-detection\/train', fname_tif)\n    # change the new file name to png\n    fname_png = image + '.png'\n    # destination path to image\n    dst = os.path.join(val_dir, label, fname_png)\n\n    \n    # read the file as an array\n    cv2_image = cv2.imread(src)\n    # save the image at the destination as a png file\n    cv2.imwrite(dst, cv2_image)\n\n\n   ","3f6cd24f":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir\/train_dir\/a_no_met_tissue')))\nprint(len(os.listdir('base_dir\/train_dir\/b_has_met_tissue')))\n","f2683ebf":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir\/val_dir\/a_no_met_tissue')))\nprint(len(os.listdir('base_dir\/val_dir\/b_has_met_tissue')))\n","b5aab5b1":"# End of Data Preparation\n### ================================================================================== ###\n# Start of Model Building\n","2152ef59":"train_path = 'base_dir\/train_dir'\nvalid_path = 'base_dir\/val_dir'\ntest_path = '..\/input\/test'\n\nnum_train_samples = len(df_train)\nnum_val_samples = len(df_val)\ntrain_batch_size = 10\nval_batch_size = 10\n\n\ntrain_steps = np.ceil(num_train_samples \/ train_batch_size)\nval_steps = np.ceil(num_val_samples \/ val_batch_size)","434eb3c6":"datagen = ImageDataGenerator(rescale=1.0\/255)\n\ntrain_gen = datagen.flow_from_directory(train_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=train_batch_size,\n                                        class_mode='categorical')\n\nval_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=val_batch_size,\n                                        class_mode='categorical')\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_gen = datagen.flow_from_directory(valid_path,\n                                        target_size=(IMAGE_SIZE,IMAGE_SIZE),\n                                        batch_size=1,\n                                        class_mode='categorical',\n                                        shuffle=False)","fba0199d":"# Source: https:\/\/www.kaggle.com\/fmarazzi\/baseline-keras-cnn-roc-fast-5min-0-8253-lb\n\nkernel_size = (3,3)\npool_size= (2,2)\nfirst_filters = 32\nsecond_filters = 64\nthird_filters = 128\n\ndropout_conv = 0.3\ndropout_dense = 0.3\n\n\nmodel = Sequential()\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu', input_shape = (96, 96, 3)))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(Conv2D(first_filters, kernel_size, activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size)) \nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(second_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(Conv2D(third_filters, kernel_size, activation ='relu'))\nmodel.add(MaxPooling2D(pool_size = pool_size))\nmodel.add(Dropout(dropout_conv))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(dropout_dense))\nmodel.add(Dense(2, activation = \"softmax\"))\n\nmodel.summary()\n","9678a2e3":"model.compile(Adam(lr=0.0001), loss='binary_crossentropy', \n              metrics=['accuracy'])","e488b3dc":"# Get the labels that are associated with each index\nprint(val_gen.class_indices)","3ea89e54":"filepath = \"model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n                              \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_gen, steps_per_epoch=train_steps, \n                    validation_data=val_gen,\n                    validation_steps=val_steps,\n                    epochs=20, verbose=1,\n                   callbacks=callbacks_list)","9d31f887":"# get the metric names so we can use evaulate_generator\nmodel.metrics_names","2bfef606":"# Here the best epoch will be used.\n\nmodel.load_weights('model.h5')\n\nval_loss, val_acc = \\\nmodel.evaluate_generator(test_gen, \n                        steps=len(df_val))\n\nprint('val_loss:', val_loss)\nprint('val_acc:', val_acc)","f5b46c8a":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()","133791c4":"# make a prediction\npredictions = model.predict_generator(test_gen, steps=len(df_val), verbose=1)","6978c784":"predictions.shape","80cb599e":"# This is how to check what index keras has internally assigned to each class. \ntest_gen.class_indices","78965e02":"# Put the predictions into a dataframe.\n# The columns need to be oredered to match the output of the previous cell\n\ndf_preds = pd.DataFrame(predictions, columns=['no_met_tissue', 'has_met_tissue'])\n\ndf_preds.head()","c6174e72":"# Get the true labels\ny_true = test_gen.classes\n\n# Get the predicted labels as probabilities\ny_pred = df_preds['has_met_tissue']\n","3bfbcd10":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_true, y_pred)","eff1fc8a":"# Source: Scikit Learn website\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/\n# model_selection\/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","4f4600d5":"# Get the labels of the test images.\n\ntest_labels = test_gen.classes","7f73f65a":"test_labels.shape","9d0fee84":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","1b050927":"# Print the label associated with each class\ntest_gen.class_indices","d569d9bf":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['no_met_tissue', 'has_met_tissue']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","35c647fb":"from sklearn.metrics import classification_report\n\n# Generate a classification report\n\n# For this to work we need y_pred as binary labels not as probabilities\ny_pred_binary = predictions.argmax(axis=1)\n\nreport = classification_report(y_true, y_pred_binary, target_names=cm_plot_labels)\n\nprint(report)\n","054d315a":"# Delete the base_dir directory we created to free up disk space to download tensorflowjs\n# and to prevent a Kaggle error.\n# Kaggle allows a max of 500 files to be saved.\n\nshutil.rmtree('base_dir')","536156f7":"!pip install tensorflowjs","6b9ee1dd":"# Use the command line conversion tool to convert the model\n\n!tensorflowjs_converter --input_format keras model.h5 tfjs_model_2\/model","f7362793":"### Conclusion","0aff9c17":"### Create a Classification Report","83975878":"### How many training images are in each folder?","15d3adb1":"### Train the Model","8b630196":"All the html, css, and javascript code used to build the web app is available on Github. \n\nLive web app:<br>\nhttp:\/\/histo.test.woza.work\/<br>\nGithub:<br>\nhttps:\/\/github.com\/vbookshelf\/Breast-Cancer-Analyzer\n\nThank you for reading.","906f0435":"\n### Resources\n\nI found these resources very helpful:\n\n1. Gabriel Preda, Honey Bee Subspecies Classification <br>\nhttps:\/\/www.kaggle.com\/gpreda\/honey-bee-subspecies-classification<br>\n\n2. Beluga, Black and White CNN<br>\nhttps:\/\/www.kaggle.com\/gaborfodor\/black-white-cnn-lb-0-77\n\n3.  Francesco Marazzi, Baseline Keras CNN<br>\nhttps:\/\/www.kaggle.com\/fmarazzi\/baseline-keras-cnn-roc-fast-5min-0-8253-lb\n\n4. YouTube tutorial by deeplizard on how to use Tensorflowjs<br>\nhttps:\/\/www.youtube.com\/watch?v=HEQDRWMK6yY\n\n5. Tutorial by jenkov.com explaining the HTML5 File API<br>\nhttp:\/\/tutorials.jenkov.com\/html5\/file-api.html\n\n6. Blog post by Anton Lavrenov on how to handle async\/await<br>\nhttps:\/\/blog.lavrton.com\/javascript-loops-how-to-handle-async-await-6252dd3c795\n\n7. jQuery tutorial by W3Schools<br>\nhttps:\/\/www.w3schools.com\/jquery\/default.asp\n","ad4c8274":"### Part 2 - Metastatic Model","5cd41d25":"### Create a Directory Structure","36235416":"### Transfer the images into the folders","77f22715":"**Recall **= Given a class, will the classifier be able to detect it?<br>\n**Precision **= Given a class prediction from a classifier, how likely is it to be correct?<br>\n**F1 Score** = The harmonic mean of the recall and precision. Essentially, it punishes extreme values.\n\nFrom the confusion matrix and classification report we see that our model is equally good at detecting both classes.","9396c452":"### Set Up the Generators","500d9381":"### Convert the model to from Keras to Tensorflowjs\nThis conversion needs to be done so that the model can be loaded into the web app.","f5cb3add":"### Check the class distribution","65e7211d":"*Link to Part 1:*<br>\nhttps:\/\/www.kaggle.com\/vbookshelf\/part-1-breast-cancer-analyzer-web-app","0363b886":"### Create the Train and Val Sets","f0329304":"**1.**<br>\nA large part of the web, including the web app for this project uses the Javascript language. I used Javascript to feed the images to the model. The challenge lies in the fact that Javascript is very fast whereas the model is not. This difference in speed can lead to incorrect predictions.\n\nFor example, say we are using a loop to feed ten images to the model for prediction. Because of the realtively slower prediction speed, the model may end up making all of it's ten predictions using only image 1 or image 10. To solve this problem, on each iteration of the loop, we must make the code wait for the model to finish a prediction before the next loop begins i.e. before feeding it the next image.\n\nIn the app I used async\/await to achieve this. Because my Javascript knowledge is very basic, you should check the Javascript code thoroughly before using it in one of your own projects.\n\n**2.**<br>\nI think there is one other potential source of prediction errors. In order to read an image Tensorflowjs needs the image to first be made part of an html image element. It then reads this image element and converts it into a tensor. When using multiple images, delays relating to putting an image into an html element (setting the src attribute) could also cause prediction errors. The programmer needs to keep this in mind especially if the image sizes are large.\n\n**3.**<br>\nAs mentioned above, most web browsers don't support the tiff image format. This needs to be kept in mind when pre-processing training data if the intention is to build a web app.\n\n**4.**<br>\nBecause Tensorflowjs is a new technology, web apps bulit using it may not work in some browsers. The user will see a message saying the \"Ai is loading...\" but that message will never go away because the app is actually frozen. It's better to advise users to use the latest version of Chrome.\n\n","2eb1429e":"### Create the Model Architecture\u00b6","1f343adf":"### Create a Dataframe containing all images","d9206d68":"#### Balance the target distribution\nWe will reduce the number of samples in class 0.","22137aab":"### What files are available?","115ebbf2":"### Labels as per csv file\n\n0 = no met tissue<br>\n1 =   has met tissue. <br>\n","4ff9b3ce":"Welcome back. In this section we will create the Metastatic_model. This model will predict whether or not Metastatic cancer tissue is present in histopathologic image  patches.\n\n**Dataset **\n\nWe will use Kaggle's version of the PCam (PatchCamelyon) dataset. It's part of the [Histopathologic Cancer Detection competition](https:\/\/www.kaggle.com\/c\/histopathologic-cancer-detection) where the challenge is to identify metastatic tissue in histopathologic scans of lymph node sections. \n\nThe dataset consists of 220,025 image patches of size 96x96 (130,908 Metastatic negative and 89,117 Metastatic positive). \n\nThe images are in tiff  format. Many web browsers, including Chrome, don't support the tiff format. Thus the web app wil not be able to accept tiff images. Before training, we will convert these images to png format. This will ensure that the model will be trained on images of similar quality to what we expect a user to submit.\n\n\n**Results**\n\nOur cnn model will achieve an accuracy and F1 score of approximately 0.94.\n\n","c162084a":"### What is the AUC Score?","1317ddd9":"### Display a random sample of train images  by class","41cc3037":"**A note on Keras class index values**\n\nKeras assigns it's own index value (here 0 and 1) to the classes. It infers the classes based on the folder structure.\nImportant: These index values may not match the index values we were given in the train_labels.csv file.\n\nI've used 'a' and 'b' folder name pre-fixes to get keras to assign index values to match what was in the train_labels.csv file - I guessed that keras is assigning the index value based on folder name alphabetical order.","80f89618":"### Create a Confusion Matrix","0352f589":"### Evaluate the model using the val set","6f354f82":"### Plot the Training Curves","6ed6705f":"### Lessons learned while building the web app","f8e9336d":"### Make a prediction on the val set\nWe need these predictions to calculate the AUC score, print the Confusion Matrix and calculate the F1 score."}}