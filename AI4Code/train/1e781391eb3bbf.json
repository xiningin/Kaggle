{"cell_type":{"38d08e4b":"code","9678edc6":"code","dd4c47a9":"code","20a35672":"code","e9fa9487":"code","8d15fadb":"code","3a36d954":"code","03a297f6":"code","b2220a3b":"code","50b469c9":"code","e9f9810f":"code","e74adf13":"code","1d5b3fca":"code","c0cf4fe1":"code","235df538":"code","d060c9be":"code","99e4e8e8":"code","1457ce5a":"code","1880b190":"code","f418d97e":"code","744ee4b6":"code","e58e982b":"markdown","b66f41f1":"markdown","751b070f":"markdown","91dc05fe":"markdown","3f6ea373":"markdown","89a89163":"markdown","5f1596fe":"markdown","09b7c44c":"markdown","d03d3f45":"markdown","c13ffa92":"markdown","ec70885f":"markdown","fdcd94d9":"markdown","5a40c432":"markdown","c9576c80":"markdown","d2df6a7b":"markdown","ccce67e4":"markdown","bd4db4e9":"markdown","b75a9faf":"markdown","8d73919d":"markdown","4049a53d":"markdown","c11e8987":"markdown","404ae18e":"markdown","5f124da9":"markdown","cf587693":"markdown","8624748f":"markdown"},"source":{"38d08e4b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebraa\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9678edc6":"df = pd.read_csv(\"\/kaggle\/input\/machine-learning-for-diabetes-with-python\/diabetes_data.csv\")\ndf.head()","dd4c47a9":"df.info()\n#data seti 768 sat\u0131r veriden  olu\u015fmaktad\u0131r","20a35672":"# y ve x_data'y\u0131 haz\u0131rlama\ny = df.Outcome.values\nx_data = df.drop([\"Outcome\"],axis=1)","e9fa9487":"x_data.head()","8d15fadb":"# Normalization; de\u011ferleri 0-1 aras\u0131nda yapmay\u0131 sa\u011flar\nx = (x_data - np.min(x_data)) \/ (np.max(x_data) - np.min(x_data)).values\nx.head()","3a36d954":"from sklearn.model_selection import train_test_split","03a297f6":"x_train,x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42)","b2220a3b":"# x = datasetimizde outcome d\u0131\u015f\u0131ndaki verilerimiz \nx_train.shape\n\n# 614 tane sample\n# 8 tane feature var","50b469c9":"x_test.shape","e9f9810f":"#Transposeunu alma\nx_train = x_train.T\nx_test = x_test.T\ny_train= y_train.T\ny_test = y_test.T","e74adf13":"x_train.shape","1d5b3fca":"# initialize weights and bias\n\ndef initialize_weights_and_bias(dimension):\n    \n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\n#w,b = initialize_weights_and_bias(4096)","c0cf4fe1":"#z de\u011ferinin bulunmas\u0131\n#z = np.dot(w.T,x_train)+b\n\n# z'nin Sigmoid funksiyona sokulmas\u0131\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","235df538":"# Forward Propagation & Backward Propagation Methods\n\ndef forward_backward_propagation(w,b,x_train,y_head):\n    \n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head) - (1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1]\n    \n    #backward propogation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","d060c9be":"# Updating(learning) parameters\n\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost)) #if section defined to print our cost values in every 10 iteration. We do not need to do that. It's optional.\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","99e4e8e8":"# prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is one means has diabete (y_head=1),\n    # if z is smaller than 0.5, our prediction is zero means does not have diabete (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n\n#predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","1457ce5a":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]\n    w,b = initialize_weights_and_bias(dimension)\n    \n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    \n\n    # Print train\/test Errors\n    \n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","1880b190":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.01, num_iterations = 200)","f418d97e":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 4, num_iterations = 120)","744ee4b6":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"test accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_test.T, y_test.T)))\nprint(\"train accuracy: {} \".format(logreg.fit(x_train.T, y_train.T).score(x_train.T, y_train.T)))","e58e982b":"<a id=\"s-11\"><\/a>\n> ## 4.7. Defining Logistic Regression Function","b66f41f1":"## Logistic Regression Intro\n\n* \u0130statistiklerde, lojistik model, ba\u015far\u0131l\u0131 \/ ba\u015far\u0131s\u0131z, kazan \/ kaybet, canl\u0131 \/ \u00f6l\u00fc veya sa\u011fl\u0131kl\u0131 \/ hasta gibi belirli bir s\u0131n\u0131f veya olay\u0131n olas\u0131l\u0131\u011f\u0131n\u0131 modellemek i\u00e7in kullan\u0131l\u0131r. Bu, bir g\u00f6r\u00fcnt\u00fcn\u00fcn kedi, k\u00f6pek, aslan vb. \u0130\u00e7erip i\u00e7ermedi\u011fini belirleme gibi \u00e7e\u015fitli olay s\u0131n\u0131flar\u0131n\u0131 modellemek i\u00e7in geni\u015fletilebilir. G\u00f6r\u00fcnt\u00fcde alg\u0131lanan her nesneye 0 ile 1 aras\u0131nda bir olas\u0131l\u0131k ve bir toplam\u0131 ekleme olas\u0131l\u0131\u011f\u0131 atan\u0131r.\n* Binary s\u0131n\u0131fland\u0131rma i\u00e7in en iyi modeldir.\n* Ayr\u0131ca Lojistik Regresyon derin \u00f6\u011frenmenin \u00e7ok temel bir \u015feklidir.","751b070f":"![LogReg.png](https:\/\/i.ibb.co\/9Hvb2Yf\/5.jpg)","91dc05fe":"<a id=\"s-10\"><\/a>\n > ## 4.5. Updating (Learning) Parameters","3f6ea373":"<a id=\"s-2\"><\/a>\n## 1. Load and Read Data","89a89163":"Yukar\u0131daki grafik Logistic Regression'un Computational Graps'\u0131d\u0131r.\nBu grafikte;\n\n* x_train'de 4096 tane piksel oldu\u011funu g\u00f6r\u00fcyoruz. Her pikselin kendine ait weightleri var. Pikselleri kendi weightleri ile \u00e7arp\u0131p, t\u00fcm \u00e7arp\u0131mlar\u0131 toplayal\u0131m ve bias\u0131 ekleyelim. Bu bize z de\u011ferini verecektir.\n\n### z = b + px1w1 + px2w2 + ... + px4096*w4096\n\n* z de\u011ferini Sigmoid fonksiyona verip hesaplama yap\u0131yoruz. \n> *Peki Sigmoid fonksiyon nedir?*\n> Sigmoid Function : Verdi\u011fimiz z de\u011ferini 0 ile 1 aras\u0131nda bir de\u011fere e\u015fitler. Bu de\u011fer probabilistik bir de\u011ferdir. Sigmoid func. t\u00fcrevi al\u0131nabilen bir fonksiyondur. T\u00fcrevi al\u0131nabilir olmas\u0131 sayesinde weight ile bias '\u0131 g\u00fcncelleyebiliyoruz. \n\n> y_head = Sigmoid(z)\n    \n* Sigmoid fonksiyonunun \u00e7\u0131kt\u0131s\u0131 olan y_head de\u011feri ile modelimizin loss de\u011ferini hesaplar\u0131z.\n\n* T\u00fcm loss de\u011ferlerinin toplam\u0131 Cost de\u011ferini verir. Cost de\u011feri y\u00fcksek olmas\u0131 modelin k\u00f6t\u00fc oldu\u011funu g\u00f6sterir\n\n\n\n","5f1596fe":"* Learning rate ve iterations say\u0131lar\u0131n\u0131 ayarlayarak do\u011fruluk oran\u0131 artt\u0131r\u0131labilinir. (accurarcy) \n* Ancak belirli bir noktadan sonra do\u011frulu\u011funuz de\u011fi\u015fmez. Cost graphda bunu g\u00f6zlemleyebilirsiniz.\n* Grafi\u011fin t\u00fcrevi (e\u011fim), artan iteration say\u0131s\u0131 ile azalmaktad\u0131r (e\u011fimin pozitif oldu\u011funu varsayal\u0131m). Bu nedenle, belirli miktarda yinelemeden sonra maliyet fonksiyonu azalmayacakt\u0131r.\n","09b7c44c":" ### Forward propagation steps:\n * find z = w.T*x+b\n * y_head = sigmoid(z)\n * loss(error) = loss(y,y_head)\n * cost = sum(loss)\n \n#### 4.4. b\u00f6l\u00fcm\u00fcnde fonksiyon g\u00fcncellencektir.","d03d3f45":"Datam\u0131zda Logistic Regression modelini kullanarak matematik bir denklem olan modelimizi elde ettik. \nModelimizi hem train hem de test edecek datam\u0131z olmal\u0131.\n\n> Traine test split","c13ffa92":"Gradient descent algortimas\u0131 ile geriye d\u00f6nerek weight ve bias\u0131 g\u00fcncellemeye Backward Propagation denir.\nBackward Propgtion metodunuda ekleyerek Forward Propagation metodunu g\u00fcncelleyelim.","ec70885f":"<a id=\"s-11\"><\/a>\n> ## 4.6. Prediction","fdcd94d9":"# Contents\n\n> ### [Logistic Regression Intro ](#s-1)\n> ### 1. [Load and Read Data](#s-2)\n> ### 2. [Normalization](#s-3)\n> ### 3. [Traine and Test Split](#s-4)\n> ### 4. [Functions](#s-5)\n   >> ### 4.1. [Initializing Parameters](#s-6)\n   >> ### 4.2. [Forward Propagation](#s-7)\n   >> ### 4.3. [Optimization Algorithm with Gradient Descent](#s-8)\n   >> ### 4.4. [Backward Propagation](#s-9)\n   >> ### 4.5. [Updating (Learning) Parameters](#s-10)\n   >> ### 4.6. [Prediction](#s-11)\n   >> ### 4.6. [Defining Logistic Regression Function ](#s-12)\n> ### 5. [With Sklearn](#s-13)\n","5a40c432":"### Datam\u0131z trainde 614 ,testte 154 olmak \u00fczere ayr\u0131lm\u0131\u015ft\u0131r.","c9576c80":"Traine test split ile datam\u0131z\u0131 belirledi\u011fimiz oranda train data ve test data olarak ay\u0131raca\u011f\u0131z.","d2df6a7b":"<a id=\"s-4\"><\/a>\n## 3. Train Test Split","ccce67e4":"#### Bu noktaya kadar \u015funlar\u0131 \u00f6\u011frendik;\n* Initializing parameters\n* Finding Cost with Forward Propagation\n* Updating (learning) parameters (weights - bias)\n\n\u015eimdi g\u00fcncellenen weight ve bias\u0131 implement edelim.","bd4db4e9":"<a id=\"s-8\"><\/a>\n> ## 4.3. Optimization Algorithm with Gradient Descent\n\n\u015euan cost de\u011ferini biliyoruz.Cost de\u011ferimiz y\u00fcksekse bunu d\u00fc\u015f\u00fcrmemiz laz\u0131m. (Y\u00fcksek Cost de\u011feri modelin k\u00f6t\u00fc oldu\u011funu g\u00f6sterir.)\nCostu azaltmak i\u00e7in weights ve bias'\u0131 g\u00fcncellemek gerekiyor.\n> Modelimiz cost func en aza indiren weight ve bias parametlerini \u00f6\u011frenmesi gerekir.\n\n> Bu tekni\u011fin ad\u0131 Gradient Descent Algoritmas\u0131d\u0131r.\n","b75a9faf":"# ** Machine Learning for Diabetes with Python **","8d73919d":"<a id=\"s-9\"><\/a>\n > ## 4.4. Backward Propagation ","4049a53d":"<a id=\"s-3\"><\/a>\n## 2. Normalization","c11e8987":"<a id=\"s-5\"><\/a>\n## 4. Functions","404ae18e":"Loss Function: \n![loss.func](https:\/\/image.ibb.co\/eC0JCK\/duzeltme.jpg)","5f124da9":"<a id=\"s-6\"><\/a>\n> ## 4.1. Initializing Parameters","cf587693":"<a id=\"s-7\"><\/a>\n> ## 4.2. Forward Propagation\n\n### Forward Propagation;\n* Z de\u011ferinin bulunmas\u0131n\u0131,\n    > z = (p1.w1)+(p2.w2)+(p3.w3)+..+(p4096.w4096)+ bias\n* Z de\u011ferini sigmoid funtiona sokarak de\u011fer bulunmas\u0131n\u0131,\n* Loss func hesaplamas\u0131n\u0131,\n* Cost func hesaplamas\u0131n\u0131 (sum(all loss)) i\u00e7erir.","8624748f":"<a id=\"s-13\"><\/a>\n## 5. With Sklearn"}}