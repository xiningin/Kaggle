{"cell_type":{"9017040d":"code","b5a76f30":"code","313781db":"code","05f99a11":"code","2b0cf921":"code","a70192a5":"code","f6a1e71c":"code","4a2a8e64":"code","5eb344eb":"code","a95284a0":"code","5b848eb0":"code","f3685917":"code","2da78d4d":"code","22933b5e":"markdown","9f952a1e":"markdown","8f71ad56":"markdown","34592314":"markdown","0c1d452e":"markdown","93ba7c6d":"markdown","f7c75cdd":"markdown","81859523":"markdown","cef9e7f6":"markdown","2e44d3f0":"markdown","f2ca1677":"markdown","fe434d5d":"markdown","843810d2":"markdown"},"source":{"9017040d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b5a76f30":"import numpy as np\n\nimport torch\nimport torch.nn as nn\ntorch.manual_seed(0)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy import stats\nimport pandas as pd\n\nfrom sklearn.metrics import accuracy_score\n\nimport wandb\nwandb.init(project='titanic_kaggle', save_code=True)","313781db":"titanic_training_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntitanic_test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntitanic_training_data.head()","05f99a11":"def clean_titanic(df, train=True):\n    df[\"Cabin\"] = df[\"Cabin\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df[\"Embarked\"] = df[\"Embarked\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df[\"AgeNan\"] = df[\"Age\"].apply(lambda x: pd.isna(x)).astype(bool)\n    df = pd.concat([df, pd.get_dummies(df['Sex'], dtype='bool', prefix='sex_'), pd.get_dummies(df['Pclass'], dtype='bool', prefix='pclass_')], axis=1)\n    df = df.drop(['PassengerId', 'Name','Ticket','Sex','Pclass'], axis=1)\n    if train:\n        df = df.drop(['Survived'], axis=1)\n    numeric_features = df.dtypes[(df.dtypes != 'object') & (df.dtypes != 'bool')].index\n    df[numeric_features] = df[numeric_features].apply(lambda x: (x - x.mean()) \/ (x.std()))\n    df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n    df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].mean())\n    return df\n\nlabels = torch.tensor(titanic_training_data[\"Survived\"].values, dtype=torch.float32)\ntitanic_training_data = clean_titanic(titanic_training_data)\ntitanic_training_data.head()","2b0cf921":"titanic_data_tensor = torch.tensor(titanic_training_data.astype('float').values, dtype=torch.float32)\ntitanic_data_tensor.shape","a70192a5":"dataset = torch.utils.data.TensorDataset(titanic_data_tensor, labels)","f6a1e71c":"training_size = int(0.7 * len(dataset))\nvalidation_size = len(dataset) - training_size\ntrain, val = torch.utils.data.random_split(dataset, [training_size, validation_size], generator=torch.Generator().manual_seed(0))\ndata_loader_train = torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\ndata_loader_val = torch.utils.data.DataLoader(val, batch_size=32, shuffle=True)","4a2a8e64":"#TODO Xavier Uniform to the weight and set the bias to 0\ndef init_my_layer(m):\n    # NOTE: no difference in accuracy in case of Normal distribution.\n    torch.nn.init.xavier_uniform_(m.weight)\n    torch.nn.init.constant_(m.bias, 0)\n    return m","5eb344eb":"class LinearModel(nn.Module):\n    #TODO One linear layer and Sigmoid applied to the ouput\n    def __init__(self, input_size, output_size):\n        super(LinearModel, self).__init__()\n        self.linear = init_my_layer(nn.Linear(input_size, output_size))\n        # Sigmoid makes this linear regression model into a linear regression model.\n        self.sigmoid = nn.Sigmoid() # NOTE: with ReLU the performance decreases\n    # The forward pass refers to the calculation process of the output data from the input.\n    # The function below takes x as its input and outputs the predicted value in 'output'.\n    def forward(self, x):\n        output = self.linear(x)\n        return self.sigmoid(output) # NOTE: with ReLU the performance decreases","a95284a0":"# rows x columns (number of features)\ntitanic_data_tensor.shape","5b848eb0":"num_epochs = 400 # NOTE: with 800 epochs the accuracy decreased\nbatch_size = 150\n# the input size of a model is = to the number of columns of the dataset = 12\ninput_size = titanic_data_tensor.shape[1]\n# the output size of a model is = to the number of classification classes\noutput_size = 1\n# defines the learning rates for the parameter updates\nlr_rate = 3e-3 # e.q to 0.003, you can change it if needed\n\nnet = LinearModel(input_size, output_size)\ncriterion = nn.BCEWithLogitsLoss()\n# To update the hyperparameters of the model.\noptimizer = torch.optim.Adam(net.parameters(), lr=lr_rate) # NOTE: SGD optimizer was outperformed by Adam\n\nfor epoch in range(num_epochs):\n    train_loss = 0\n    # TRAINING LOOP\n    training_loss = 0\n    for images, labels in data_loader_train:\n        # Clear the gradients as we don't want any gradient from previous epoch\n        # to carry forward: don't want to cummulate gradients.\n        optimizer.zero_grad()\n        # Forward Pass\n        output = net(images)\n        # Find the Loss\n        loss = criterion(output, labels.unsqueeze(1))\n        training_loss += loss.item()\n        # Calculate gradients\n        loss.backward()\n        # Update Weights\n        optimizer.step()\n    \n    # VALIDATION LOOP\n    with torch.no_grad(): # we don't need gradients in the validation phase\n#         net.eval()\n        validation_loss = 0\n        correct_classified = 0\n        for images, labels in data_loader_val:\n            # Forward Pass\n            output = net(images)\n            # Find the Loss\n            loss = criterion(output, labels.unsqueeze(1))\n            # Calculate Loss\n            validation_loss += loss.item()\n            # torch.ge: a boolean tensor that is True where input is greater than or equal\n            # to other and False elsewhere.\n            # This is needed because we are doing a binary classification and this way\n            # the predicted value is converted into either a 0 (dead) or 1 (alive).\n            # It is an array of array - tensor([[ True], [ True], [ True], ...]) - and we need\n            # to match this structure when comparing it with labels later.\n            predicted = torch.ge(output, 0.5)\n            # x = tensor([[ 1,  2,  3,  4]])\n            # torch.unsqueeze(x, 1)\n            # tensor([[ 1],\n            #        [ 2],\n            #        [ 3],\n            #        [ 4]])\n            # torch.eq: computes element-wise equality\n            correct_classified += int(predicted.eq(labels.unsqueeze(1)).sum().item())\n\n\n#         print('Epoch {}'.format(epoch+1),\n#               \"training_loss: \", training_loss,\n#               'validation_loss: ', validation_loss)\n    acc = correct_classified \/ validation_size\n    wandb.log({'training_loss': training_loss, 'validation_loss': validation_loss, 'accuracy': acc})","f3685917":"# SAVE THE MODEL\n# torch.save(net.state_dict(), 'my_titanic_regression_model_' + wandb.run.id +'.pth')\n# print(\"Done!\")","2da78d4d":"titanic_test_data_cleaned = clean_titanic(titanic_test_data, train=False)\ntitanic_data_tensor = torch.tensor(titanic_test_data_cleaned.astype('float').values, dtype=torch.float32)\n\nwith torch.no_grad():\n    net.eval()\n    test_pred = torch.LongTensor()\n    for i, data in enumerate(titanic_data_tensor):\n        output = net(data)\n        predicted = torch.ge(output, 0.5)\n        test_pred = torch.cat((test_pred, predicted), dim=0)\n    out_df = pd.DataFrame(np.c_[titanic_test_data['PassengerId'].values, test_pred.numpy()], columns=['PassengerId', 'Survived'])\n    out_df.to_csv('submission.csv', index=False)","22933b5e":"### Why is it important to initialize model weights?","9f952a1e":"\"The goal of training any deep learning model is finding the optimum set of weights for the model that gives us the desired results. The training methods used in Deep Learning are generally iterative in nature and require us to provide an initial set of weights that needs to be updated over time\" [source](https:\/\/www.askpython.com\/python-modules\/initialize-model-weights-pytorch).\n\n\"The initial weights play a huge role in deciding the final outcome of the training. Wrong initialization of weights can lead to vanishing or exploding gradients, which is obviously unwanted\" [source](https:\/\/www.askpython.com\/python-modules\/initialize-model-weights-pytorch). \n\nLayer initialization using Xavier Uniform on the weights and a constant 0 value on the bias.\n\n**NOTE**: Biases can generally be initialized to zero but weights need to be initialized carefully","8f71ad56":"Dataframe needs to be cleaned. Knowing if some information are unknown can be very important to determine if someone survived","34592314":"Create the LinearModel with one Linear layer and Sigmoid applied to the output","0c1d452e":"We first need to read the datasets","93ba7c6d":"# Titanic - Machine Learning from Disaster\n\n\nKaggle link: https:\/\/www.kaggle.com\/c\/titanic","f7c75cdd":"Create a `TensorDataset` to get tuple of data and label","81859523":"Import all the needed library and init Weights and Biases","cef9e7f6":"We then split between the training and validation set","2e44d3f0":"In linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. In logistic regression, the outcome (dependent variable) has only a limited number of possible values.\n\nLogistic regressions are meant to resolve binary classification problems where given an element you have to classify the same in 2 categories. It outputs the 'probability' of the input belonging to a label (0 or 1)","f2ca1677":"Initialize the network (call it `net`, it would make things easier later), the loss, the optimizer, and write the training loop\n\nDon't forget to check the validation loss and save your model at the end of each epoch!","fe434d5d":"This loop computes the prediction on the test dataset and creates a submission file\n\nYou then just have to click the submit button to get your score. Lucky you!","843810d2":"We then transform the data from numpy (pandas representation) into torch's `Tensor`"}}