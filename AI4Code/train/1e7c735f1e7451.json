{"cell_type":{"3f195485":"code","29aebc33":"code","f708726d":"code","43dedb76":"code","c763dbaf":"code","49f398cb":"code","3ca0ead5":"code","5d8f5af4":"code","008f2c6a":"code","70881158":"code","37f04c7b":"code","6bbea7a4":"code","a377de0b":"code","40dbac57":"code","7d5f73c1":"code","b1809167":"code","a3e43b90":"code","399ea6b6":"code","00ab7468":"code","d7cd1fa9":"code","33c2223f":"code","ba1644fe":"code","e045d941":"code","d33fa4f9":"code","22887033":"code","c83d320f":"code","9d4df361":"code","57a8f445":"code","a670add8":"markdown","b3f96498":"markdown","86b09a5c":"markdown","e4bd3b32":"markdown","c31785e6":"markdown","210b29d0":"markdown","5b88622e":"markdown","50ad8ec0":"markdown","7d33fa2a":"markdown","66728678":"markdown","905ae8ed":"markdown","4d71f8e0":"markdown","77adbee5":"markdown","d47f8390":"markdown","bf66e79a":"markdown","100e8a11":"markdown","dd36148e":"markdown","044e28db":"markdown","6a80c471":"markdown","18929ef3":"markdown"},"source":{"3f195485":"import numpy as np \nimport pandas as pd \npd.options.display.max_columns = 20\nimport os\nfrom sklearn.metrics import mean_squared_error\nfrom catboost import CatBoostRegressor\nfrom colorama import Fore, Back, Style\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib\nfrom matplotlib.patches import Patch\nfrom matplotlib import pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 0})\nplt.style.use('fivethirtyeight')\ncmap_data = plt.cm.Paired\ncmap_cv = plt.cm.coolwarm\nimport warnings\nwarnings.filterwarnings('ignore')","29aebc33":"train = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\nnp.unique(train['country']),np.unique(train['store'])","f708726d":"target=pd.DataFrame()\n\n#Mug Fin Mart : Mug sold in Finland by KaggleMart\n#Mug hat Mart : Hat sold in Finland by KaggleMart\n#Mug sti Mart : Sitcker sold in Finland by KaggleMart...\n\n# FINLAND :\ntarget['Mug Fin Mart'] = train[((((train['product'] == 'Kaggle Mug') & (train['country']=='Finland'))==True) & (train['store']=='KaggleMart')==True)].groupby('date').sum()['num_sold']\ntarget['Hat Fin Mart'] = train[((((train['product'] == 'Kaggle Hat') & (train['country']=='Finland'))==True) & (train['store']=='KaggleMart')==True)].groupby('date').sum()['num_sold']\ntarget['Sti Fin Mart'] = train[((((train['product'] == 'Kaggle Sticker') & (train['country']=='Finland'))==True) & (train['store']=='KaggleMart')==True)].groupby('date').sum()['num_sold']\ntarget['Mug Fin Rama'] = train[((((train['product'] == 'Kaggle Mug') & (train['country']=='Finland'))==True) & (train['store']=='KaggleRama')==True)].groupby('date').sum()['num_sold']\ntarget['Hat Fin Rama'] = train[((((train['product'] == 'Kaggle Hat') & (train['country']=='Finland'))==True) & (train['store']=='KaggleRama')==True)].groupby('date').sum()['num_sold']\ntarget['Sti Fin Rama'] = train[((((train['product'] == 'Kaggle Sticker') & (train['country']=='Finland'))==True) & (train['store']=='KaggleRama')==True)].groupby('date').sum()['num_sold']\n\n# NORWAY:\ntarget['Mug Nor Mart'] = train[((((train['product'] == 'Kaggle Mug') & (train['country']=='Norway'))==True) & (train['store']=='KaggleMart')==True)].groupby('date').sum()['num_sold']\ntarget['Hat Nor Mart'] = train[((((train['product'] == 'Kaggle Hat') & (train['country']=='Norway'))==True) & (train['store']=='KaggleMart')==True)].groupby('date').sum()['num_sold']\ntarget['Sti Nor Mart'] = train[((((train['product'] == 'Kaggle Sticker') & (train['country']=='Norway'))==True) & (train['store']=='KaggleMart')==True)].groupby('date').sum()['num_sold']\ntarget['Mug Nor Rama'] = train[((((train['product'] == 'Kaggle Mug') & (train['country']=='Norway'))==True) & (train['store']=='KaggleRama')==True)].groupby('date').sum()['num_sold']\ntarget['Hat Nor Rama'] = train[((((train['product'] == 'Kaggle Hat') & (train['country']=='Norway'))==True) & (train['store']=='KaggleRama')==True)].groupby('date').sum()['num_sold']\ntarget['Sti Nor Rama'] = train[((((train['product'] == 'Kaggle Sticker') & (train['country']=='Norway'))==True) & (train['store']=='KaggleRama')==True)].groupby('date').sum()['num_sold']\n\n# SWEDEN:\ntarget['Mug Swe Mart'] = train[((((train['product'] == 'Kaggle Mug') & (train['country']=='Sweden'))==True) & (train['store']=='KaggleMart')==True)].groupby('date').sum()['num_sold']\ntarget['Hat Swe Mart'] = train[((((train['product'] == 'Kaggle Hat') & (train['country']=='Sweden'))==True) & (train['store']=='KaggleMart')==True)].groupby('date').sum()['num_sold']\ntarget['Sti Swe Mart'] = train[((((train['product'] == 'Kaggle Sticker') & (train['country']=='Sweden'))==True) & (train['store']=='KaggleMart')==True)].groupby('date').sum()['num_sold']\ntarget['Mug Swe Rama'] = train[((((train['product'] == 'Kaggle Mug') & (train['country']=='Sweden'))==True) & (train['store']=='KaggleRama')==True)].groupby('date').sum()['num_sold']\ntarget['Hat Swe Rama'] = train[((((train['product'] == 'Kaggle Hat') & (train['country']=='Sweden'))==True) & (train['store']=='KaggleRama')==True)].groupby('date').sum()['num_sold']\ntarget['Sti Swe Rama'] = train[((((train['product'] == 'Kaggle Sticker') & (train['country']=='Sweden'))==True) & (train['store']=='KaggleRama')==True)].groupby('date').sum()['num_sold']\n\ntarget.index = np.arange(0,target.shape[0],1).tolist()\ntarget.head(3)","43dedb76":"Rama = [col for col in target.columns if 'Rama' in col]\nRama_swe = [col for col in target.columns if ('Rama' in col) & ('Swe' in col)]\nRama_fin = [col for col in target.columns if ('Rama' in col) & ('Fin' in col)]\nRama_nor = [col for col in target.columns if ('Rama' in col) & ('Nor' in col)]\n\nMart = [col for col in target.columns if 'Mart' in col]\nMart_swe = [col for col in target.columns if ('Mart' in col) & ('Swe' in col)]\nMart_fin = [col for col in target.columns if ('Mart' in col) & ('Fin' in col)]\nMart_nor = [col for col in target.columns if ('Mart' in col) & ('Nor' in col)]\n\ndef show_me(data) :\n    fig_dims = (20,10)\n    fig, ax = plt.subplots(figsize=fig_dims)\n    sns.set_theme(style=\"whitegrid\")\n    dates = pd.date_range(\"1 1 2015\", periods=365, freq=\"D\")\n    dates = pd.date_range(start='1\/1\/2015', end='31\/12\/2018',  freq=\"D\")\n    data.index = dates\n    sns.lineplot(data=data, palette=\"tab10\", linewidth=1)","c763dbaf":"show_me(target[Rama]) # Sales for all countries","49f398cb":"show_me(target[Rama_fin]) # sales for KaggleRAmma in Finland","3ca0ead5":"show_me(target[Mart]) # Sales for KaggleMart in all countries","5d8f5af4":"show_me(target[Mart_fin]) # sales for KaggleMarte in Finland","008f2c6a":"train_data = pd.DataFrame()\ntrain_data['date'] = np.unique(train['date']).tolist()\ntrain_data['date'] = pd.to_datetime(train_data['date'])\ntrain_data['year'] = train_data['date'].dt.year\ntrain_data['month'] = train_data['date'].dt.month\ntrain_data['day'] = train_data['date'].dt.day\ntrain_data['dayofweek'] = train_data['date'].dt.dayofweek\ntrain_data['dayofmonth'] = train_data['date'].dt.days_in_month\ntrain_data['dayofyear'] = train_data['date'].dt.dayofyear\ntrain_data['weekday'] = train_data['date'].dt.weekday\n\ntest_data = pd.DataFrame()\ntest_data['date'] = np.unique(test['date']).tolist()\ntest_data['date'] = pd.to_datetime(test_data['date'])\ntest_data['year'] = test_data['date'].dt.year\ntest_data['month'] = test_data['date'].dt.month\ntest_data['day'] = test_data['date'].dt.day\ntest_data['dayofweek'] = test_data['date'].dt.dayofweek\ntest_data['dayofmonth'] = test_data['date'].dt.days_in_month\ntest_data['dayofyear'] = test_data['date'].dt.dayofyear\ntest_data['weekday'] = test_data['date'].dt.weekday\n\ntrain_data.drop('date', axis = 1, inplace = True)\ntest_data.drop('date', axis = 1, inplace = True)\ntrain_data.shape,test_data.shape","70881158":"train_data.head(3)","37f04c7b":"y=pd.DataFrame()\ny['num_sold'] = target['Mug Fin Mart']\ny.index = pd.to_datetime(target['Mug Fin Mart'].index.tolist())","6bbea7a4":"from statsmodels.tsa.stattools import adfuller\ndftest = adfuller(y, autolag = 'AIC')\nprint(\"1. ADF : \",dftest[0])\nprint(\"2. P-Value : \", dftest[1])","a377de0b":"# We expect a yearly seasonality : 12 (months)\nfig_dims = (20,7)\nrolling_mean = y.rolling(window = 12).mean()\ny['rolling_mean_diff'] = rolling_mean - rolling_mean.shift()\nax1 = plt.subplot()\ny['rolling_mean_diff'].plot(title='after rolling mean & differencing',figsize=fig_dims);\nax2 = plt.subplot()\ny.plot(title='original',figsize=fig_dims);","40dbac57":"dftest = adfuller(y['rolling_mean_diff'].dropna(), autolag = 'AIC')\nprint(\"1. ADF : \",dftest[0])\nprint(\"2. P-Value : \", dftest[1])","7d5f73c1":"y=pd.DataFrame()\ny['num_sold'] = target['Mug Fin Mart'] # as example\ny.index = target.index","b1809167":"# Green = Training\n# Blue = validation set\n\ni=0\nindex_train_start = i*365\nindex_train_end = (1+i)*365\nindex_valid_start = (1+i)*365\nindex_valid_end = (2+i)*365\ny['num_sold'].iloc[index_train_start:index_valid_start].plot(figsize=(10,3),linewidth=1, color='green')\ny['num_sold'].iloc[index_valid_start:index_valid_end].plot(figsize=(10,3),linewidth=1,color='blue')","a3e43b90":"i=1\nindex_train_start = i*365\nindex_train_end = (1+i)*365\nindex_valid_start = (1+i)*365\nindex_valid_end = (2+i)*365\ny['num_sold'].iloc[index_train_start:index_valid_start].plot(figsize=(10,3),linewidth=1, color='green')\ny['num_sold'].iloc[index_valid_start:index_valid_end].plot(figsize=(10,3),linewidth=1,color='blue')","399ea6b6":"i=2\nindex_train_start = i*365\nindex_train_end = (1+i)*365\nindex_valid_start = (1+i)*365\nindex_valid_end = (2+i)*365\ny['num_sold'].iloc[index_train_start:index_valid_start].plot(figsize=(10,3),linewidth=1, color='green')\ny['num_sold'].iloc[index_valid_start:index_valid_end].plot(figsize=(10,3),linewidth=1,color='blue')","00ab7468":"# Green = Training\n# Blue = validation set\nINDEX =train_data.index.tolist()\ni=0\nindex_train_start = 0\nindex_train_end = (1+i)*365\nindex_valid_start = (1+i)*365\nindex_valid_end = (2+i)*365\ny['num_sold'].iloc[index_train_start:index_valid_start].plot(figsize=(10,3),linewidth=1, color='green')\ny['num_sold'].iloc[index_valid_start:index_valid_end].plot(figsize=(10,3),linewidth=1,color='blue')","d7cd1fa9":"i=1\nindex_train_start = 0\nindex_train_end = (1+i)*365\nindex_valid_start = (1+i)*365\nindex_valid_end = (2+i)*365\ny['num_sold'].iloc[index_train_start:index_valid_start].plot(figsize=(10,3),linewidth=1, color='green')\ny['num_sold'].iloc[index_valid_start:index_valid_end].plot(figsize=(10,3),linewidth=1,color='blue')","33c2223f":"i=2\nindex_train_start = 0\nindex_train_end = (1+i)*365\nindex_valid_start = (1+i)*365\nindex_valid_end = (2+i)*365\ny['num_sold'].iloc[index_train_start:index_valid_start].plot(figsize=(10,3),linewidth=1, color='green')\ny['num_sold'].iloc[index_valid_start:index_valid_end].plot(figsize=(10,3),linewidth=1,color='blue')","ba1644fe":"score_pred =[]\n\ntrain_pred = np.zeros((3*365,18))\ntest_pred_by_year = np.zeros((test_data.shape[0],18))\n\nfor i in range (3):\n    \n    index_train_start = i*365\n    index_train_end = (1+i)*365\n    index_valid_start = (1+i)*365\n    index_valid_end = (2+i)*365\n\n    X_train, y_train = train_data.iloc[index_train_start:index_valid_start], target.iloc[index_train_start:index_valid_start]\n    X_valid, y_valid = train_data.iloc[index_valid_start:index_valid_end], target.iloc[index_valid_start:index_valid_end]\n    \n    param1 = {   \n        'learning_rate': 0.004280047845210125, \n        'depth': 5, \n        'l2_leaf_reg': 0.0010555278350981901, \n        'loss_function': 'MultiRMSE', \n        'eval_metric': 'MultiRMSE', \n        'task_type': 'CPU', \n        'iterations': 16962\n        }\n    \n    clf = CatBoostRegressor(**param1)\n    clf.fit(\n                X_train, y_train,\n                eval_set=[(X_valid,y_valid)],\n                early_stopping_rounds = 1000,\n                verbose=0)\n    pred=clf.predict(X_valid)\n    train_pred[i*365:(1+i)*365]=pred\n    \n    score = np.round(mean_squared_error(y_valid,pred))\n    score_pred.append(score)\n    print(\"fold\",i+1,\"score MSE =\",score,\"RMSE =\",np.round(np.sqrt(score)))\n    \n    pred_test = clf.predict(test_data)\n    test_pred_by_year += pred_test\/3\n\nscore_total_mse = np.round((mean_squared_error(train_pred,target.iloc[366:,:])))\n    \nprint(70*'*')\nprint('Score oof MSE   =',score_total_mse)\nprint('Score oof RMSE  =',np.round(np.sqrt(score_total_mse)))\nprint('Score mean MSE  =',np.round(np.mean(score_pred)))\nprint('Score mean RMSE =',np.round(np.sqrt(np.mean(score_pred))))\nprint(70*'*')  ","e045d941":"score_pred =[]\n\ntrain_pred = np.zeros((3*365,18))\ntest_pred_cumulative = np.zeros((test_data.shape[0],18))\n\nfor i in range (3):\n    \n    index_train_start = 0\n    index_train_end = (1+i)*365\n    index_valid_start = (1+i)*365\n    index_valid_end = (2+i)*365\n\n    X_train, y_train = train_data.iloc[index_train_start:index_valid_start], target.iloc[index_train_start:index_valid_start]\n    X_valid, y_valid = train_data.iloc[index_valid_start:index_valid_end], target.iloc[index_valid_start:index_valid_end]\n    \n    param1 = {   \n        'learning_rate': 0.004280047845210125, \n        'depth': 5, \n        'l2_leaf_reg': 0.0010555278350981901, \n        'loss_function': 'MultiRMSE', \n        'eval_metric': 'MultiRMSE', \n        'task_type': 'CPU', \n        'iterations': 16962\n        }\n    \n    clf = CatBoostRegressor(**param1)\n    clf.fit(\n                X_train, y_train,\n                eval_set=[(X_valid,y_valid)],\n                early_stopping_rounds = 1000,\n                verbose=0)\n    pred=clf.predict(X_valid)\n    train_pred[i*365:(1+i)*365]=pred\n    \n    score = np.round(mean_squared_error(y_valid,pred))\n    score_pred.append(score)\n    print(\"fold\",i+1,\"score MSE =\",score,\"RMSE =\",np.round(np.sqrt(score)))\n    \n    pred_test = clf.predict(test_data)\n    test_pred_cumulative += pred_test\/3\n\nscore_total_mse = np.round((mean_squared_error(train_pred,target.iloc[366:,:])))\n    \nprint(70*'*')\nprint('Score oof MSE   =',score_total_mse)\nprint('Score oof RMSE  =',np.round(np.sqrt(score_total_mse)))\nprint('Score mean MSE  =',np.round(np.mean(score_pred)))\nprint('Score mean RMSE =',np.round(np.sqrt(np.mean(score_pred))))\nprint(70*'*')  ","d33fa4f9":"pred_test_df_by_year = pd.DataFrame(test_pred_by_year,columns=target.columns.tolist())\npred_test_df_cumulative = pd.DataFrame(test_pred_cumulative,columns=target.columns.tolist())\ndisplay(pred_test_df_by_year.head(3))\ndisplay(pred_test_df_cumulative.head(3))","22887033":"sub = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')\n\ndef make_submission(df):\n    submission = pd.DataFrame(data=np.zeros((sub.shape[0],2)),index = sub.index.tolist(),columns=['row_id','num_sold'])\n    INDEX = -1\n    for i in range(365):\n        for j in range (18) :\n            INDEX +=1\n            submission['num_sold'].loc[INDEX,1]=df.iloc[i,j]\n    submission['row_id'] = sub['row_id']\n    return submission\n\nsubmission_by_year = make_submission(pred_test_df_by_year)\nsubmission_cumulative = make_submission(pred_test_df_cumulative)\ndisplay(submission_by_year.head(3))\ndisplay(submission_cumulative.head(3))","c83d320f":"submission_mean=sub.copy()\nsubmission_mean['num_sold'] = (submission_by_year['num_sold']+submission_cumulative['num_sold'])\/2\nsubmission_mean.head()","9d4df361":"public_submission = pd.read_csv('..\/input\/tps-2022-01\/public_submission.csv')\nsubmission_bonus=sub.copy()\nsubmission_bonus['num_sold'] = (submission_mean['num_sold']+public_submission['num_sold'])\/2\nsubmission_bonus.head()","57a8f445":"submission_by_year.to_csv('submission_by_year.csv',index=False)\nsubmission_cumulative.to_csv('submission_cumulative.csv',index=False)\nsubmission_mean.to_csv('submission_mean.csv',index=False)\nsubmission_bonus.to_csv('submission_bonus.csv',index=False)","a670add8":"<h2> Transform data to timeserie with unique dates","b3f96498":"It seems clear that the seasonality is annual, but let's confirm it\nWe are going to transform our data to get a 'stationary serie'","86b09a5c":"<h2> Seasonality","e4bd3b32":"<h2> Let's split our dataset with seasonality for the training by year","c31785e6":"We can see that the p-value is less than 0.05\nSo our time series is stationary.","210b29d0":"<h2> Let's do a Training by cumulative years","5b88622e":"We can see that cumulative years seems a little better than by year","50ad8ec0":"<h2> Let's do a Training by year","7d33fa2a":"<h2> Data Engineering","66728678":"<h2> Let's prepare the submission files format","905ae8ed":"<h2> <h2> Let's split our dataset by cumulative years","4d71f8e0":"<h3> Split with training with 2017 and validation 2018","77adbee5":"<h3> Split with training with 2016 and validation : 2017","d47f8390":"We can see that every product for every country and shop has a seasonality and a small trend","bf66e79a":"<h3> Sales by KaggleMart","100e8a11":"We train with the previous year and validate with the next year","dd36148e":"<h3> Split with training with and 2015 validation : 2016","044e28db":"<h3> Sales by KaggleRama","6a80c471":"<h2> Some lists to visualize our data","18929ef3":"The P_value is much higher than 0.05 so it is not stationary, let's improve it"}}