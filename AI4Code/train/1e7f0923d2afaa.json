{"cell_type":{"36191a8e":"code","3f5c0b6a":"code","46ef55ec":"code","308cfc9d":"code","351a6052":"code","d622e044":"code","17c3f55b":"code","2526a520":"code","3ff707d6":"code","d55a0af0":"code","d99b8d0a":"code","557f4cef":"code","0ce07178":"code","446800a4":"code","ba0b412a":"code","3cadaff9":"markdown","6d352146":"markdown","3c8a6219":"markdown","98d16a4b":"markdown","e39ae0b0":"markdown","a03b45b8":"markdown","82cda221":"markdown","69d7f680":"markdown","864ddc1f":"markdown","7c269848":"markdown","ba8c8d12":"markdown","e2591138":"markdown","f5189f6c":"markdown","ebe9b208":"markdown","0030b2d9":"markdown","8ea4ac3c":"markdown","3d495097":"markdown"},"source":{"36191a8e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3f5c0b6a":"import numpy\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.externals.six import StringIO\nimport numpy as np\nimport graphviz\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz\n","46ef55ec":"#Build a simple data set with y = x + random\nnPoints = 100\n#x values for plotting\nxPlot = [(float(i)\/float(nPoints) - 0.5) for i in range(nPoints + 1)]\n#x needs to be list of lists.\nx = [[s] for s in xPlot]\n#y (labels) has random noise added to x-value\n#set seed\nnumpy.random.seed(1)\ny = [s + numpy.random.normal(scale=0.1) for s in xPlot]","308cfc9d":"plt.rcParams.update({'font.size': 14})\nplt.rcParams['figure.figsize'] = (12.0, 7.0)\nplt.plot(xPlot,y)\nplt.axis('tight')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Figure 1: Simple dataset')\nplt.show()","351a6052":"simpleTree = DecisionTreeRegressor(max_depth=1)\nsimpleTree.fit(x, y)","d622e044":"#draw the decision tree result with graphviz\ndot_data = export_graphviz(simpleTree,out_file = None,rounded = True,filled = True)\ngraph = graphviz.Source(dot_data)\ngraph.render() \ngraph","17c3f55b":"#compare prediction from tree with true values\nyHat = simpleTree.predict(x)\nplt.figure()\nplt.plot(xPlot, y, label='True y')\nplt.plot(xPlot, yHat, label='Tree Prediction ', linestyle='--');\nplt.legend(bbox_to_anchor=(1,0.2))\nplt.axis('tight')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Figure 2: Decision Tree Split Depth=1')\nplt.show()","2526a520":"simpleTree2 = DecisionTreeRegressor(max_depth=2)\nsimpleTree2.fit(x, y);","3ff707d6":"#draw the tree\ndot_data = export_graphviz(simpleTree2,out_file = None,rounded = True,filled = True)\ngraph = graphviz.Source(dot_data)\ngraph.render() \ngraph","d55a0af0":"#compare prediction from tree with true values\nyHat = simpleTree2.predict(x)\nplt.figure()\nplt.plot(xPlot, y, label='True y')\nplt.plot(xPlot, yHat, label='Tree Prediction ', linestyle='--')\nplt.legend(bbox_to_anchor=(1,0.2))\nplt.axis('tight')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Figure 3: Decision Tree Split Depth=2')\nplt.show()","d99b8d0a":"#split point calculations - try every possible split point to\n#find the best one\nsse = []\nxMin = []\nfor i in range(1, len(xPlot)):\n     #divide list into points on left and right of split point\n     lhList = list(xPlot[0:i])\n     rhList = list(xPlot[i:len(xPlot)])\n     #calculate averages on each side\n     lhAvg = sum(lhList) \/ len(lhList)\n     rhAvg = sum(rhList) \/ len(rhList)\n     #calculate sum square error on left, right and total\n     lhSse = sum([(s - lhAvg) * (s - lhAvg) for s in lhList])\n     rhSse = sum([(s - rhAvg) * (s - rhAvg) for s in rhList])\n     #add sum of left and right to list of errors\n     sse.append(lhSse + rhSse)\n     xMin.append(max(lhList))","557f4cef":"#SSE is sum of squared error\nplt.plot(range(1, len(xPlot)), sse)\nplt.xlabel('Split Point Index')\nplt.ylabel('Sum Squared Error')\nplt.title('Figure 4: SSE resulting from every possible split point location')\nplt.show()","0ce07178":"#minSse = min(sse)\n#idxMin = sse.index(minSse)\n#print(xMin[idxMin])","446800a4":"#increase the depth to 6\nsimpleTree6 = DecisionTreeRegressor(max_depth=6)\nsimpleTree6.fit(x, y);","ba0b412a":"#compare prediction from tree with true values\nyHat = simpleTree6.predict(x)\nplt.figure()\nplt.plot(xPlot, y, label='True y')\nplt.plot(xPlot, yHat, label='Tree Prediction ', linestyle='--')\nplt.legend(bbox_to_anchor=(1,0.2))\nplt.axis('tight')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title('Figure 5: Decision Tree Split Depth=6')\nplt.show()","3cadaff9":"## Decision Trees\nIn this competion and indeed any kaggle competiton you will most likely use a decision tree algorithm.  The most succesful and widely spread non-deep learning machine learning algorithms are based on decision trees.  Random Forests, Gradient Boosting, LightGBM are all at heart decision trees.  To be a success in data science and machine learning you should have a fundamental understanding of the basics of decision trees.  This short notebook will give you an intuitive understanding of how they work under the hood.  It focuses on two key issues; how they make their decisions and overfitting.","6d352146":"Because the simple synthetic problem has a single attribute only, the plot of the prediction generated by the trained tree alongside the actual values begins to give an idea about how the training of this simple tree was accomplished. The predicted values shown in Figure 2 below.  The prediction is a step function of the attribute. ","3c8a6219":" The curve in Figure 3 approximates the actual curve with a finer stairstep function. **More tree depth results in finer steps and higher fidelity to the training data.**","98d16a4b":"## Multivariable Tree Training\nWhat if the problem has more than one attribute? Then the algorithm checks all possible split points for all of the attributes to see which split point gives the best sum squared error for each attribute and then which attributes gives the overall minimum.  This split point calculation is where all the computation cycles go in training a decision tree.","e39ae0b0":"## Summary\nThis notebook has discussed the basics of the most important non-deep learning algorithms - decision trees.  It has demonstarted in basic terms how split points are detemined and shown one the drawbacks of decsion trees - overfitting.\n\nIf you found this interesting please upvote and I will produce further tutorials\n\n","a03b45b8":"## References\nMachine Learning in Python - Michael Bowles\n","82cda221":"Another way to look at overfitting with a binary tree is to consider the number of terminal nodes in the tree compared to the amount of data available. The tree that generated the prediction shown in Figure 5 was depth 6. That means that it has 64 terminal nodes (2^6). There are 100 points in the dataset. That means a lot of the points are the sole occupants of a terminal node, so their predicted value exactly matches their observed value. **Therefore the graph of the prediction is matching the wiggles due to noise.**","69d7f680":"A block diagram of the resulting tree is shown below.  The single decision at the root node is to compare the attribute value with -0.075. This number is called the `split` point because it splits the data into two groups. The two boxes that emanate from the decision indicate that 43 of the 101 input examples go down the left leg of the tree, and the remaining 58 examples go down the right leg. If the attribute is less than the split point, the prediction from the tree is what\u2019s indicated as value in the block diagram\u2014roughly \u20130.303. ","864ddc1f":"## Binary Decision Trees\nBinary decision trees operate by subjecting attributes to a series of binary (yes\/no) decisions. Each decision leads to one of two possibilities. Each decision leads to another decision or it leads to prediction.\nAny block diagram of a trained tree shows a number of boxes, which are called `nodes`. There are two types of nodes: Nodes can either pose a yes\/no question of the data, or they can be terminal nodes that assign a prediction to examples that end up in them. Terminal nodes are often referred to as `leaf` nodes. \n\nWhen an observation or row is passed to a nonterminal node, the row answers the node\u2019s question. If it answers yes, the row of attributes is passed to the leaf node below and to the left of the current node. If the row answers no, the row of attributes is passed to the leaf node below and to the right of the current node.  The process continues recursively until the row arrives at a terminal (that is, leaf) node where a prediction value is assigned to the row. The value assigned by the leaf node is the mean of the outcomes of the all the training observations that wound up in the leaf node.","7c269848":"## Overfitting with Decision Trees\nFigure 5 shows what happens when the tree depth is increased to 6. In Figure 5, it\u2019s hard to see the difference between the true value and the\nprediction. **The prediction follows almost every zig and zag. That begins to suggest that the model is overfitting the data**. The way the data were generated indicates that the best possible prediction would be for the prediction to equal the attribute value. The noise that was added to the attribute is unpredictable, and yet the prediction is following the noise-driven deviations of the label from the attribute. ","ba8c8d12":"From Figure 1 we can see that that y follows x with some random noise added.","e2591138":"## Selecting Split Points\n**How are the split points selected?**\n\nThe tree is trained to minimize the squared error of its predictions. Suppose first that the split point is given. Once the split point is given, the values assigned to the two groups are also determined. The\naverage of each group is the single quantity that minimizes the mean squared error. That only leaves the question of how the split point is determined. the listing below small section of code that goes through the process of determining the split. **The process is to try every possible split point. This is accomplished by dividing the data into two groups, approximating each group by its average, and then\ncalculating the resulting sum squared error.**\n","f5189f6c":"## Increasing the Depth of the Tree\nWe can increase the depth of the tree to 2 and repeat the above figures.  It shows what happens to the prediction curve as the tree depth increases from 1 to 2 by again using scikit learn.  ","ebe9b208":"## Generate a Tree\nUsing scikit learn regression tree package we can generate a simple tree with depth of 1 with the following code.\n\n\n\n","0030b2d9":"## How to Train a Binary Decision Tree\nLets look at a simple example.  We can create a dataset of 101 points `x` of values between -0.5 and 0.5, the labels y are equal to x with some random noise added.  This dataset is shown as Figure 1 below. ","8ea4ac3c":"Figure 4 below shows how the sum squared error varies as a function of the split point location. As you can see, there\u2019s a well-defined minimum at roughly the midpoint of the data set. **Training a decision tree entails exhaustively searching all possible split points to determine which one minimizes the sum squared error.** ","3d495097":"The resulting prediction curves are shown in Figure 3, and the block diagram for the tree is shown below.   Instead of having a single step, the prediction curve now has three steps. The second set of split points is determined in the same manner as the first one. Each node in the tree deals with the subset of points determined by the splits above it. The split point for each node is determined to minimize the sum squared error in the two nodes below"}}