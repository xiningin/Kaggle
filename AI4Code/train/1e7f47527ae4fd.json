{"cell_type":{"bf4c370a":"code","f3118814":"code","b984a148":"code","fb03b1bb":"code","b084e182":"code","bfb38efc":"code","f9b9967a":"code","cc6524c7":"code","73705909":"code","fea492c7":"code","5acf3e92":"code","64b2dd22":"code","8cf9f707":"code","6c724e62":"code","057f4136":"code","7f1bd35d":"code","fb7eb22a":"code","9b506461":"code","0b70a1f6":"code","18789a46":"code","e4ece19d":"code","ab54ef42":"code","c1298b43":"code","91370e26":"code","c7938855":"code","02926247":"code","b2492dcc":"code","7d99f68c":"code","c7cc8566":"code","806afe22":"code","d79110e3":"markdown","936ae0d4":"markdown","6e192446":"markdown","026b785d":"markdown","bb6051a6":"markdown","0b6f6cf7":"markdown","1d9981b6":"markdown","d57208bc":"markdown","9d2e52d2":"markdown","887b55c9":"markdown","a0a80ac0":"markdown","d3c035cb":"markdown","6feebb0c":"markdown","a33de886":"markdown","58e62cfc":"markdown"},"source":{"bf4c370a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f3118814":"# Imports\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\n\nimport numpy as np\n\nimport pandas as pd","b984a148":"# Data Loading\ndf = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')","fb03b1bb":"df[df['SARS-Cov-2 exam result']=='positive'].head()","b084e182":"df.dtypes.head(10)","bfb38efc":"df.index","f9b9967a":"df.columns","cc6524c7":"df.count()","73705909":"df = df.drop(columns=['Patient ID'])","fea492c7":"df2 = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')\ndf2.corr()","5acf3e92":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(100, 100))\nsns.heatmap(df.corr(),\n            annot = True,\n            fmt = '.2f',\n            cmap='Blues')\nplt.title('Heatmap - Correlation coefficients between variables')\nplt.show()","64b2dd22":"y = pd.get_dummies(df['SARS-Cov-2 exam result'])","8cf9f707":"#Test Positive covid-19 correlation matrix\nplt.figure(figsize=(100, 20))\ndf.reset_index(drop=True)\nk = 10 #number of variables for heatmap\ndf3 = pd.get_dummies(df.fillna(0))\ncorrmat = df3.corr()\ncols = corrmat.nlargest(k, 'SARS-Cov-2 exam result_positive')\ncm = np.corrcoef(df3[cols.index].values.T)\nhm = sns.heatmap(cm, cmap='Blues',cbar=True, annot=True, square=True, fmt='.2f', yticklabels=cols.index, xticklabels=cols.index)\nplt.show()","6c724e62":"#Test Negative covid-19 correlation matrix\nplt.figure(figsize=(100, 20))\ndf.reset_index(drop=True)\nk = 10 #number of variables for heatmap\ndf3 = pd.get_dummies(df.fillna(0))\ncorrmat = df3.corr()\ncols = corrmat.nlargest(k, 'SARS-Cov-2 exam result_negative')\ncm = np.corrcoef(df3[cols.index].values.T)\nhm = sns.heatmap(cm, cmap='Blues',cbar=True, annot=True, square=True, fmt='.2f', yticklabels=cols.index, xticklabels=cols.index)\nplt.show()","057f4136":"full_null_series = (df.isnull().sum() == df.shape[0])\nfull_null_columns = full_null_series[full_null_series == True].index\n# columns with all values equal null\nprint(full_null_columns.tolist())","7f1bd35d":"df.drop(full_null_columns, axis=1, inplace=True)","fb7eb22a":"df2 = pd.get_dummies(df.fillna(0))\ndf2","9b506461":"from sklearn.datasets import make_regression, load_boston\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import RFE, RFECV, f_regression\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nx = df2.loc[:, (df2.columns != 'SARS-Cov-2 exam result_negative') & (df2.columns != 'SARS-Cov-2 exam result_positive') ]\nnames = df2.columns\n\n#using linear regression as models\nlr = LinearRegression()\nrfe = RFE(estimator=lr, n_features_to_select=1)\nrfe.fit(x, y['positive'])","0b70a1f6":"print(\"Attributes sorted by rank hair:\")\nprint(sorted(zip(rfe.ranking_, names)))","18789a46":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport pandas as pd\nimport seaborn as sns\nX = df2.loc[:, (df2.columns != 'SARS-Cov-2 exam result_negative') & (df2.columns != 'SARS-Cov-2 exam result_positive') ]\nY = y['positive']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42)# training model\nmodel  = RandomForestClassifier()\nmodel.fit(X_train, y_train)# show the importance from each feature\nmodel.feature_importances_","e4ece19d":"plt.figure(figsize=(15,100))\nimportances = pd.Series(data=model.feature_importances_, index=X.columns)\nsns.barplot(x=importances, y=importances.index, orient='h').set_title('Importances from each feature')","ab54ef42":"from sklearn.preprocessing import MinMaxScaler, StandardScaler, Normalizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, mutual_info_classif, mutual_info_regression","c1298b43":"X = df2.loc[:, (df2.columns != 'SARS-Cov-2 exam result_negative') & (df2.columns != 'SARS-Cov-2 exam result_positive') ]\nY = y['positive']\nX_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=42)\nmi = mutual_info_classif(X_train, y_train)","91370e26":"mi = pd.Series(mi)\nmi.index = X_train.columns\nmi = mi.sort_values(ascending=True)","c7938855":"mi.plot.barh(figsize=(10,50))","02926247":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\ndef models(X_train, Y_train, X_test, Y_test):\n    from sklearn.linear_model import LogisticRegression\n    log = LogisticRegression(random_state = 0)\n    log.fit(X_train, Y_train)\n    log_pred = log.predict(X_test)\n    \n    from sklearn.neighbors import KNeighborsClassifier\n    knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n    knn.fit(X_train, Y_train)\n    knn_pred = knn.predict(X_test)\n    \n    from sklearn.svm import SVC\n    svc_lin = SVC(kernel='linear', random_state = 0, probability=True)\n    svc_lin.fit(X_train, Y_train)\n    svc_lin_pred = svc_lin.predict(X_test)\n    \n    from sklearn.svm import SVC\n    svc_rbf = SVC(kernel='rbf', random_state = 0)\n    svc_rbf.fit(X_train, Y_train)\n    svc_rbf_pred = svc_rbf.predict(X_test)\n    \n    from sklearn.naive_bayes import GaussianNB\n    gauss = GaussianNB()\n    gauss.fit(X_train, Y_train)\n    gauss_pred = gauss.predict(X_test)\n    \n    from sklearn.tree import DecisionTreeClassifier\n    tree = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n    tree.fit (X_train, Y_train)\n    tree_pred = tree.predict(X_test)\n    \n    from sklearn.ensemble import RandomForestClassifier\n    forest = RandomForestClassifier(n_estimators=10, criterion = 'entropy', random_state = 0)\n    forest.fit(X_train, Y_train)\n    forest_pred = forest.predict(X_test)\n    \n    from sklearn.linear_model import Perceptron\n    pcp = Perceptron(random_state = 0)\n    pcp.fit(X_train, Y_train)\n    pcp_pred = pcp.predict(X_test)\n    \n    print('[0]Logistic Regression Training Accuracy: ', log.score(X_train, Y_train))\n    print('[0]Logistic Regression Testing Accuracy: ', accuracy_score(Y_test, log_pred))\n    print(classification_report(Y_test, log_pred))\n    \n    print('[1]KNeighborns Training Accuracy: ', knn.score(X_train, Y_train))\n    print('[1]KNeighborns Testing Accuracy: ', accuracy_score(Y_test, knn_pred))\n    print(classification_report(Y_test, knn_pred))\n    \n    print('[2]SVC Linear Training Accuracy: ', svc_lin.score(X_train, Y_train))\n    print('[2]SVC Linear Testing Accuracy: ', accuracy_score(Y_test, svc_lin_pred))\n    print(classification_report(Y_test, svc_lin_pred))\n    \n    print('[3]SVC RBF Training Accuracy: ', svc_rbf.score(X_train, Y_train))\n    print('[3]SVC RBF Testing Accuracy: ', accuracy_score(Y_test, svc_rbf_pred))\n    print(classification_report(Y_test, svc_rbf_pred))\n    \n    print('[4]Gaussian NB Training Accuracy: ', gauss.score(X_train, Y_train))\n    print('[4]Gaussian NB Testing Accuracy: ', accuracy_score(Y_test, gauss_pred))\n    print(classification_report(Y_test, gauss_pred))\n    \n    print('[5]Decision Tree Training Accuracy: ', tree.score(X_train, Y_train))\n    print('[5]Decision Tree Testing Accuracy: ', accuracy_score(Y_test, tree_pred))\n    print(classification_report(Y_test, tree_pred))\n    \n    print('[6]Random Forest Training Accuracy: ', forest.score(X_train, Y_train))\n    print('[6]Random Forest Testing Accuracy: ', accuracy_score(Y_test, forest_pred))\n    print(classification_report(Y_test, forest_pred))\n    \n    print('[7] Perceptron Training Accuracy: ', pcp.score(X_train, Y_train))\n    print(\"[7] Perceptron Testing accuracy: \", accuracy_score(Y_test, pcp_pred))\n    print(classification_report(Y_test, pcp_pred))\n    \n    return log, knn, svc_lin, svc_rbf, gauss, tree, forest, pcp","b2492dcc":"mod = models(X_train,y_train, X_test, y_test)","7d99f68c":"df[df.index == 3462]","c7cc8566":"predict_fn = lambda x: mod[0].predict_proba(x).astype(float)\npredict_fn1 = lambda x: mod[1].predict_proba(x).astype(float)\npredict_fn2 = lambda x: mod[5].predict_proba(x).astype(float)\npredict_fn3 = lambda x: mod[6].predict_proba(x).astype(float)\npredict_fn4 = lambda x: mod[2].predict_proba(x).astype(float) # SVC","806afe22":"import lime\nimport lime.lime_tabular\n\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,                                            \n                 feature_names=X_train.columns.values.tolist(),                                        \n                 class_names=y_train.unique())\n\nnp.random.seed(42)\nexp = explainer.explain_instance(X_test.values[465], predict_fn, num_features = 8)\nexp.show_in_notebook(show_all=True) #only the features used in the explanation are displayed\n\nexp = explainer.explain_instance(X_test.values[465], predict_fn1, num_features = 8)\nexp.show_in_notebook(show_all=True)\n\nexp = explainer.explain_instance(X_test.values[465], predict_fn2, num_features = 8)\nexp.show_in_notebook(show_all=True)\n\nexp = explainer.explain_instance(X_test.values[465], predict_fn3, num_features = 8)\nexp.show_in_notebook(show_all=True)\n\n# SVC\nexp = explainer.explain_instance(X_test.values[465], predict_fn4, num_features = 8)\nexp.show_in_notebook(show_all=True)","d79110e3":"# Data Exploration","936ae0d4":"# Import Librarys","6e192446":"### Correlation coefficients between variables","026b785d":"#### SVC Linear have the best value precision to positive class","bb6051a6":"Split train and test set","0b6f6cf7":"# All correlations","1d9981b6":"# Features importance using mutual info classifier in ascending form","d57208bc":"## Convert Series to dummy codes","9d2e52d2":"### LIME (local interpretable model-agnostic explanations) is a package for explaining the predictions made by machine learning algorithms. Lime supports explanations for individual predictions from a wide range of classifiers, and support for scikit-learn is built in.","887b55c9":"# Top 10 variables with more correlation in positive covid-19 case","a0a80ac0":"# Machine Learning models","d3c035cb":"# Remove Invalid colums","6feebb0c":"# Top 10 variables with more correlation in negative covid-19 case","a33de886":"Exclude Patient ID.","58e62cfc":"# Import Librarys to manipulate and seletec features"}}