{"cell_type":{"dc199a84":"code","018734d4":"code","dcb9aed1":"code","40333222":"code","caf652eb":"code","1809206f":"code","85870b5a":"code","b020364c":"code","1ce688eb":"code","2084781e":"code","8e5a69cf":"code","b5381ec8":"code","4b76dfd7":"code","2159e842":"code","9a581fa4":"code","52313bdc":"code","405a6fd8":"code","6cc0c479":"code","8511c45d":"code","4f6368e5":"code","10938e2e":"code","cf37e227":"code","a1725dfa":"code","767c8a42":"code","f756c9a3":"code","68aac6c0":"code","a1da3288":"code","9792f02d":"code","18d03ed8":"code","f835259e":"code","7f21771e":"code","b33e3a0b":"code","88092c28":"code","f14b29a6":"code","46e6d4fe":"code","b67194c2":"code","3c54bd91":"code","37f9e3a2":"code","bd5111d1":"code","8f9c0301":"code","f523807b":"code","eefd9f56":"code","6980675c":"code","f61378ce":"code","77eee841":"code","e931defe":"code","aa6d9e96":"code","93d91b5a":"code","b785557b":"code","d51c69df":"code","867d4abd":"code","4df868e1":"code","e39c8ce0":"code","8b6bb318":"code","80b7ff65":"code","78bb77c2":"code","5a0096ed":"code","ae061042":"code","1517deda":"code","d2360a4a":"code","725fcd4c":"code","f89b3fad":"code","1fab422b":"code","eaf9ca5f":"code","bced3b42":"code","5fdc0b14":"code","c44ef3a0":"code","3ad83640":"code","66f77401":"code","f22da157":"markdown","0e81adae":"markdown","1286fe39":"markdown","bea53823":"markdown","db995e11":"markdown","ee4c95cf":"markdown","427cebb8":"markdown","109e3b22":"markdown","94c5c1fc":"markdown","5135a058":"markdown","f91634ce":"markdown"},"source":{"dc199a84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression,Ridge, Lasso\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.stats import norm\nfrom scipy import stats\nfrom math import sqrt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\npd.set_option('display.max_columns', None)\npd.options.display.float_format = '{:.2f}'.format ","018734d4":"trdf = pd.read_csv('..\/input\/train.csv')","dcb9aed1":"plt.figure(figsize = (10,10))\nsns.heatmap(trdf.isnull(),yticklabels = False, cbar = False, cmap='coolwarm')","40333222":"#drop missing data\n#columns = alley, fireplacequ, poolqc, fence, miscfeatre - Done\ntrdf = trdf.drop (columns = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'])","caf652eb":"#Fill missing rows - done\ntrdf['LotFrontage'].fillna(0, inplace = True)\n\nGlist = ['GarageCond', 'GarageType', 'GarageFinish', 'GarageQual','GarageYrBlt']\nfor item in Glist:\n    trdf[item].fillna(0, inplace = True)\n    \n\nBlist = ['BsmtExposure','BsmtQual','BsmtExposure','BsmtCond','BsmtFinType1','BsmtFinType2']\n\nfor item in Blist:\n    trdf[item].fillna(0, inplace = True)\n    \ntrdf['MasVnrType'].fillna('None', inplace = True)\n\ntrdf['MasVnrArea'].fillna(0, inplace = True)\n\ntrdf['Electrical'].fillna('SBrkr',inplace = True)","1809206f":"#Remove Outliers - ID:441\ntrdf[trdf['TotalBsmtSF']>trdf['GrLivArea']]","85870b5a":"#Remove Outliers - ID:1299\ntrdf[trdf['TotalBsmtSF']>5900]","b020364c":"#Remove Outliers - ID:524 and 1299\ntrdf[trdf['GrLivArea']>4000]","1ce688eb":"#confirmed those were it\nsns.pairplot(trdf.drop(index = [523,1298,440])[['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', \n       'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt',\n       'YearRemodAdd', 'MasVnrArea', 'Fireplaces']])","2084781e":"trdf.drop(index = [523,1298], inplace = True)","8e5a69cf":"#recoding numerical variables as cat\ntrdf = trdf.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                                   7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}})","b5381ec8":"#recoding categorical data as ordinal\ntrdf = trdf.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                     )","4b76dfd7":"#run a heatmap on all variables to identify collinear variables\nplt.figure(figsize = (10,10))\nsns.heatmap(trdf.corr())","2159e842":"#drop collinear variables (after checking against corr heatmap)\ntrdf.drop(columns = ['GarageArea','1stFlrSF', 'GarageYrBlt', 'TotRmsAbvGrd','YearRemodAdd', 'BsmtCond', \n                   'BsmtFinType1','BsmtFinType2', 'GarageCond' ], inplace = True)","9a581fa4":"trdf.info()","52313bdc":"testdf = pd.read_csv('..\/input\/test.csv')","405a6fd8":"testdf.head()","6cc0c479":"testdf.describe()","8511c45d":"(testdf.isna().sum()\/(testdf.count()+testdf.isna().sum())).sort_values(ascending = False)","4f6368e5":"#drop missing data\n#columns = alley, fireplacequ, poolqc, fence, miscfeatre - Done\ndf = testdf.drop (columns = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'])","10938e2e":"#Fill missing rows - done\ndf['LotFrontage'].fillna(0, inplace = True)\n\nGlist = ['GarageCond', 'GarageType', 'GarageFinish', 'GarageQual','GarageYrBlt']\nfor item in Glist:\n    df[item].fillna(0, inplace = True)\n    \n\nBlist = ['BsmtExposure','BsmtQual','BsmtExposure','BsmtCond','BsmtFinType1','BsmtFinType2']\n\nfor item in Blist:\n    df[item].fillna(0, inplace = True)\n    \ndf['MasVnrType'].fillna('None', inplace = True)\n\ndf['MasVnrArea'].fillna(0, inplace = True)\n\ndf['Electrical'].fillna('SBrkr',inplace = True)","cf37e227":"#recoding numerical variables as cat\ndf = df.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                                   7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}})","a1725dfa":"#recoding categorical data as ordinal\ndf = df.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"No\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                     )","767c8a42":"#drop collinear variables (after checking against corr heatmap)\ndf.drop(columns = ['GarageArea','1stFlrSF', 'GarageYrBlt', 'TotRmsAbvGrd','YearRemodAdd', 'BsmtCond', \n                   'BsmtFinType1','BsmtFinType2', 'GarageCond' ], inplace = True)","f756c9a3":"df.info()","68aac6c0":"dn = (df.isna().sum()>0)","a1da3288":"#input mode for remaining features\nfor feature in dn[dn].index:\n    df[feature].fillna(df[feature].mode()[0], inplace = True)","9792f02d":"df.isna().sum()>0","18d03ed8":"#Need to align and ensure the train and test set are the same size AFTER cleaning, but before log transform\n\n#store test ID for submission purposes\nTestId=df['Id']\n#align data set shapes and get dummies\ntotal_features=pd.concat((trdf.drop(['Id','SalePrice'], axis=1), df.drop(['Id'], axis=1)))\ntotal_features=pd.get_dummies(total_features, drop_first=True)\ntrain_features=total_features[0:trdf.shape[0]]\n\n#making sure the test set matches the train set\ntest_features=total_features[trdf.shape[0]:] ","f835259e":"#total features - concat train and test data sets on top of each other without ID and SalePrice to make sure they are the same\ntotal_features.shape","7f21771e":"train_features.shape","b33e3a0b":"test_features.shape","88092c28":"#Not normally distributed and needs a transformation. Fails fat pencil test.\nsns.distplot(trdf['GrLivArea'],bins = 50, fit=norm)\nfig = plt.figure()\nstats.probplot(trdf['GrLivArea'], plot=plt)\n#skewness and kurtosis\nprint(\"Skewness: %f\" % trdf['GrLivArea'].skew())\nprint(\"Kurtosis: %f\" % trdf['GrLivArea'].kurt())","f14b29a6":"#Not normally distributed and needs a transformation. Fails fat pencil test.\nsns.distplot(trdf['SalePrice'],bins = 50, fit=norm)\nfig = plt.figure()\nstats.probplot(trdf['SalePrice'], plot=plt)\n#skewness and kurtosis\nprint(\"Skewness: %f\" % trdf['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % trdf['SalePrice'].kurt())","46e6d4fe":"#Perform a Log transformation on GrLivArea\ntrain_features['GrLivArea'] = np.log(train_features['GrLivArea'])\ntest_features['GrLivArea'] = np.log(test_features['GrLivArea'])","b67194c2":"#Perform log transform on SalePrice as it is skewed as well\nX = train_features\ny = np.log(trdf['SalePrice'])","3c54bd91":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)","37f9e3a2":"#iterate through possible alpha values to optimize \nrmse=[]\n# check the below alpha values for Ridge Regression\nalpha=[0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30]\n\nfor alph in alpha:\n    ridge=Ridge(alpha=alph, copy_X=True, fit_intercept=True)\n    ridge.fit(X_train, y_train)\n    predict=ridge.predict(X_test)\n    rmse.append(np.sqrt(mean_squared_error(y_test, predict)))\nprint(rmse)\nplt.scatter(alpha, rmse)\nrmse = pd.Series(rmse, index = alpha)\nprint('Best alpha:', rmse.idxmin())","bd5111d1":"# Adjust alpha based on previous result - can iterate on this to dial it in as much as you need\nalpha=np.arange(.5,3, 0.25)\n\nrmse=[]\nfor alph in alpha:\n    ridge=Ridge(alpha=alph, copy_X=True, fit_intercept=True)\n    ridge.fit(X_train, y_train)\n    predict=ridge.predict(X_test)\n    rmse.append(np.sqrt(mean_squared_error(y_test, predict)))\nprint(rmse)\nplt.scatter(alpha, rmse)\nrmse = pd.Series(rmse, index = alpha)\nprint('Best alpha:', rmse.idxmin())","8f9c0301":"besta = rmse.idxmin()","f523807b":"#Run your ridge with the highest alpha \nridge=Ridge(alpha=besta, copy_X=True, fit_intercept=True)\nridge.fit(X_train, y_train)\npredictions=ridge.predict(X_test)","eefd9f56":"# Plot important coefficients \ncoefs = pd.Series(ridge.coef_, index = X_train.columns)\nprint(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Model\")\nplt.show()","6980675c":"#plot predictions against actuals\nplt.scatter(y_test,predictions)","f61378ce":"#plot residuals\nsns.distplot((y_test-predictions),bins=500)","77eee841":"#Ridge Explained variance \nexplained_variance_score(y_test,predictions)","e931defe":"print('RMSE',np.sqrt(mean_squared_error(y_test, predictions)))","aa6d9e96":"#iterate through possible alpha values to optimize \nrmse=[]\n# check the below alpha values for Ridge Regression\nalpha=[0.00001, 0.0002, 0.0003, 0.0004, 0.0005, .001, .01, .03, .10, .30, 1]\n\nfor alph in alpha:\n    lasso=Lasso(alpha=alph, copy_X=True, fit_intercept=True)\n    lasso.fit(X_train, y_train)\n    predict=lasso.predict(X_test)\n    rmse.append(np.sqrt(mean_squared_error(y_test, predict)))\nprint(rmse)\nplt.scatter(alpha, rmse)\nrmse = pd.Series(rmse, index = alpha)\nprint('Best alpha:', rmse.idxmin())","93d91b5a":"# Adjust alpha based on previous result - can iterate on this to dial it in as much as you need\nalpha=np.arange(.0001,.0004, 0.00005)\n\nrmse=[]\nfor alph in alpha:\n    lasso=Lasso(alpha=alph, copy_X=True, fit_intercept=True, max_iter=10000)\n    lasso.fit(X_train, y_train)\n    predict=lasso.predict(X_test)\n    rmse.append(np.sqrt(mean_squared_error(y_test, predict)))\nprint(rmse)\nplt.scatter(alpha, rmse)\n\nrmse=pd.Series(rmse, index=alpha)\nprint('Best alpha:', rmse.idxmin())","b785557b":"besta = pd.Series(rmse, index = alpha).idxmin()","d51c69df":"#Run your lasso with the highest alpha \nlasso=Lasso(alpha=besta, copy_X=True, fit_intercept=True)\nlasso.fit(X_train, y_train)\npredictions=lasso.predict(X_test)","867d4abd":"# Plot important coefficients \ncoefs = pd.Series(lasso.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")\nplt.show()","4df868e1":"#plot predictions against actuals\nplt.scatter(y_test,predictions)","e39c8ce0":"#plot residuals\nsns.distplot((y_test-predictions),bins=500)","8b6bb318":"#Ridge Explained variance - need to beat 90%... beat it with 92%!\nexplained_variance_score(y_test,predictions)","80b7ff65":"#Lasso is better on the train dataset than Ridge\nprint('RMSE',np.sqrt(mean_squared_error(y_test, predictions)))","78bb77c2":"#Find best alpha and l1 value\nfrom sklearn.linear_model import ElasticNet\nrmse=[]\n# check the below alpha values for Ridge Regression\nalpha = [0.0001, 0.0005, 0.001, 0.01, 0.03, 0.05, 0.1]\nl1_ratios = [1.5, 1.1, 1, 0.9, 0.8, 0.7, 0.5]\ncombined = [(alph, l1) for alph in alpha for l1 in l1_ratios]\n\nfor c in combined:\n    enet=ElasticNet(alpha=c[0], l1_ratio=c[1], normalize=False)\n    enet.fit(X_train, y_train)\n    predict=enet.predict(X_test)\n    rmse.append(np.sqrt(mean_squared_error(y_test, predict)))\nprint(rmse)\nrmse = pd.DataFrame(rmse, index = combined)\nrmse.plot(figsize = (10,10))\nplt.xticks(np.arange(len(rmse.index)), rmse.index, rotation = 70)\nplt.show()\nprint('Best alpha, l1:', rmse.idxmin())","5a0096ed":"#zoom in to better see alpha l1 combo\nrmse=[]\n# check the below alpha values for Ridge Regression\nalpha = [0.0001, 0.0005, 0.001, 0.01, 0.03, 0.05, 0.1]\nl1_ratios = [1.5, 1.1, 1, 0.9, 0.8, 0.7, 0.5]\ncombined = [(alph, l1) for alph in alpha for l1 in l1_ratios]\n\nfor c in combined[:10]:\n    enet=ElasticNet(alpha=c[0], l1_ratio=c[1], normalize=False)\n    enet.fit(X_train, y_train)\n    predict=enet.predict(X_test)\n    rmse.append(np.sqrt(mean_squared_error(y_test, predict)))\nprint(rmse)\nrmse = pd.DataFrame(rmse, index = combined[:10])\nrmse.plot(figsize = (10,10))\nplt.xticks(np.arange(len(rmse.index)), rmse.index, rotation = 70)\nplt.show()\nprint('Best alpha, l1:', rmse.idxmin())","ae061042":"besta = rmse.idxmin()","1517deda":"#Run your ElasticNet with the highest alpha \nenet=ElasticNet(alpha=besta[0][0], l1_ratio=besta[0][1], normalize=False)\nenet.fit(X_train, y_train)\npredictions=enet.predict(X_test)","d2360a4a":"# Plot important coefficients - THIS I LIKE!\ncoefs = pd.Series(enet.coef_, index = X_train.columns)\nprint(\"ENet picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the ENet Model\")\nplt.show()","725fcd4c":"#plot predictions against actuals\nplt.scatter(y_test,predictions)","f89b3fad":"#plot residuals\nsns.distplot((y_test-predictions),bins=500)","1fab422b":"#ENet Explained variance \nexplained_variance_score(y_test,predictions)","eaf9ca5f":"#ENet is better than Lasso and Ridge on the train dataset\nprint('RMSE',np.sqrt(mean_squared_error(y_test, predictions)))","bced3b42":"#create predictions for test set using trained model\n#lasso RMSE -  0.12117 \ntestpredictions=lasso.predict(test_features)\ntestpredictions","5fdc0b14":"#Ridge RMSE - 0.12144\n#testpredictions=ridge.predict(test_features)\n#testpredictions","c44ef3a0":"#ENet RMSE - 0.12149\n#testpredictions=enet.predict(test_features)\n#testpredictions","3ad83640":"#transform log sales price back to regular for submission\nTest_price=np.exp(list(testpredictions))\nTest_price","66f77401":"#create submission\nsubmission=pd.DataFrame()\nsubmission['Id']=TestId\nsubmission['SalePrice']=Test_price\nsubmission.to_csv('submission.csv', index=False)","f22da157":"# **Model Training**","0e81adae":"# **Predictions and Submission**","1286fe39":"### ** Elastic Net**","bea53823":"# **Data Cleaning**","db995e11":"## **Test Data Set**","ee4c95cf":"## **Split**","427cebb8":"# **Regression Training**\n\nThis is my attempt at the [Housing Prices](http:\/\/https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) Competition. My main goal was to become more familiar with how to participate in a competition. I was also looking to learn best practices from the Kaggle community on Regression Machine Learning algorithms. Specifically, I wanted to use Linear (baseline), Lasso, Ridge, and Elastic Net for regularization. \n\nKernels I found especially helpful:\n- [Comprehensive Data Exploration](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n- [Ridge Regression ](https:\/\/www.kaggle.com\/junyingzhang2018\/ridge-regression-score-0-119)\n- [Feature Selection and Elastic Net](https:\/\/www.kaggle.com\/cast42\/feature-selection-and-elastic-net)\n\nAs of 3\/12\/19, my best RMSE was 0.12117 (top 28%).","109e3b22":"# **Shaping for ML**","94c5c1fc":"## **Ridge**","5135a058":"### **Lasso**","f91634ce":"## **Train Data Set**"}}