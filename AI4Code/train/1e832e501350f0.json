{"cell_type":{"5771eac0":"code","e39c968d":"code","66635d40":"code","10d56603":"code","833ebcb1":"code","0d9b3f51":"code","5290c4fa":"code","c55b09ea":"code","874fd2b3":"code","9364052e":"code","5f417ffa":"code","313274db":"code","d20b7080":"code","63c99bb8":"code","df7b0c2d":"code","92abedc1":"code","495aa11b":"code","088fb17e":"code","4c29b2f4":"code","3989b916":"code","65df7830":"code","cfb991b4":"code","febb96e2":"code","3032c73e":"markdown","118674a7":"markdown","f458693b":"markdown","0d4647a9":"markdown","b81007c0":"markdown","2e0c8a1f":"markdown","0f63d6ed":"markdown","7b35bd51":"markdown","7ad2bbba":"markdown","360b0ce4":"markdown","f3bc8107":"markdown","b4dfa43a":"markdown","837f2a01":"markdown","afc59925":"markdown","07a8fb2b":"markdown","476e6bd4":"markdown","cb53d456":"markdown","7d4cd316":"markdown","e9eaf304":"markdown","20a520ac":"markdown","0705ec4c":"markdown","69c7f616":"markdown","b9e3d3b6":"markdown"},"source":{"5771eac0":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline\n\n# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import model_selection\n\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import datasets","e39c968d":"import tensorflow as tf\n\nfrom tensorflow.keras.models import Sequential, load_model\n\nfrom tensorflow.keras.layers import InputLayer, Dense, Dropout, Flatten\n\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, MaxPool2D\n\nfrom tensorflow.keras.utils import to_categorical\n\nfrom tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator","66635d40":"from tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.layers.experimental.preprocessing import Rescaling, RandomFlip, RandomRotation, RandomZoom, RandomContrast, RandomTranslation","10d56603":"def plot_scores(train) :\n    accuracy = train.history['accuracy']\n    val_accuracy = train.history['val_accuracy']\n    epochs = range(len(accuracy))\n    plt.plot(epochs, accuracy, 'b', label='Score apprentissage')\n    plt.plot(epochs, val_accuracy, 'r', label='Score validation')\n    plt.title('Scores')\n    plt.legend()\n    plt.show()","833ebcb1":"img = load_img('..\/input\/cat-and-dog\/training_set\/training_set\/cats\/cat.1.jpg')","0d9b3f51":"plt.imshow(img)","5290c4fa":"data_augmentation = Sequential([\n    RandomFlip(\"horizontal\"),\n    RandomRotation(1.\/16),\n    RandomZoom((-0.1,0.1)),\n    RandomContrast(0.2),  \n    RandomTranslation(0.1,0.1)\n])","c55b09ea":"batch = np.expand_dims(img,0)","874fd2b3":"for i in range(9):\n  augmented_image = data_augmentation(batch)\n  plt.imshow(augmented_image[0])\n  plt.axis(\"off\")\n  plt.show()","9364052e":"train_data_dir = \"..\/input\/cat-and-dog\/training_set\/training_set\"\nimage_size = (299, 299)\n\ndataset = image_dataset_from_directory(\n    train_data_dir,\n    image_size=image_size,\n)","5f417ffa":"plt.figure(figsize=(15, 25))\nclass_names = dataset.class_names\nfor images, labels in dataset.take(1):\n    for i in range(32):\n        plt.subplot(7, 5, i + 1)\n        plt.imshow(np.array(images[i]).astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","313274db":"train_data_dir = \"..\/input\/cat-and-dog\/training_set\/training_set\"\nimage_size = (299, 299)\n\ntrain_dataset = image_dataset_from_directory(\n    train_data_dir,\n    validation_split=0.2,\n    seed=1,\n    subset=\"training\",\n    label_mode=\"categorical\",\n    image_size=image_size\n)\n\nvalidation_dataset = image_dataset_from_directory(\n    train_data_dir,\n    validation_split=0.2,\n    seed=1,\n    subset=\"validation\",\n    label_mode=\"categorical\",\n    image_size=image_size\n)","d20b7080":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = train_dataset.cache().prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)","63c99bb8":"data_augmentation = Sequential([\n    RandomFlip(\"horizontal\"),\n    RandomRotation(0.1)\n    RandomZoom((-0.1,0.1)),\n    RandomContrast(0.05),  \n    RandomTranslation(0.1,0.1)\n])","df7b0c2d":"# Mod\u00e8le CNN \nmodel = Sequential()\nmodel.add(InputLayer(input_shape=(299, 299, 3)))\nmodel.add(data_augmentation)\nmodel.add(Rescaling(scale=1.\/255))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(Conv2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Conv2D(20, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(2, activation='softmax', kernel_initializer=tf.keras.initializers.Constant(0.01)))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-3), metrics=['accuracy'])","92abedc1":"history = model.fit(\n    train_dataset, \n    validation_data=validation_dataset, \n    epochs=50,\n    verbose=1)","495aa11b":"plot_scores(history)","088fb17e":"model = Sequential()\nmodel.add(InputLayer(input_shape=(299, 299, 3)))\nmodel.add(data_augmentation)\nmodel.add(Rescaling(scale=1.\/255))\nmodel.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\nmodel.add(Flatten())\nmodel.add(Dense(2, activation='softmax'), kernel_initializer=tf.keras.initializers.Constant(0.01)))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(1e-3), metrics=['accuracy'])","4c29b2f4":"history = model.fit(\n    train_dataset, \n    validation_data=validation_dataset, \n    epochs=50,\n    verbose=1)","3989b916":"from tensorflow.keras.applications import Xception\nxception = Xception(weights='imagenet', include_top=False, input_shape=(299,299,3))\nxception.trainable = False","65df7830":"model = Sequential()\nmodel.add(data_augmentation)\nmodel.add(Rescaling(scale=1.\/255))\nmodel.add(xception)\nmodel.add(Flatten())\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dense(2, activation='softmax'))\n\n# Compilation du mod\u00e8le\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])","cfb991b4":"history = model.fit(\n    train_dataset, \n    validation_data=validation_dataset, \n    epochs=10,\n    verbose=1)","febb96e2":"plot_scores(history)","3032c73e":"On peut cr\u00e9er des datasets d'apprentissage et de validation \u00e0 partir du m\u00eame r\u00e9pertoire avec l'option **validation_split=0.2** (20% des donn\u00e9es pour la validation)  \nDans ce cas, il faut donner la m\u00eame valeur **seed** (initialisation du g\u00e9n\u00e9rateur al\u00e9atoire) pour les deux datasets (sinon, il ne seront pas disjoints)  \n\n**label_mode** d\u00e9termine la forme des labels :\n- **categorical** pour le multiclasses (utiliser une *loss* de type *categorical_crossentropy*)\n- **int** pour des \u00e9tiquettes au format entier (utiliser une *loss* de type *sparse_categorical_crossentropy*)\n- **binary** pour une classification binaire (utiliser une *loss* de type *binary_crossentropy*)\n","118674a7":"### Fonctions utiles","f458693b":"## Lecture des images depuis un r\u00e9pertoire","0d4647a9":"Avec un mod\u00e8le de type VGG16 :","b81007c0":"## Transfer learning","2e0c8a1f":"## Transformations d'images","0f63d6ed":"On cr\u00e9e une couche d'augmentation (attention : il n'est pas forc\u00e9ment souhaitable de r\u00e9aliser trop de transformations, au risque de compromettre la convergence de l'entra\u00eenement) :","7b35bd51":"On va utiliser un mod\u00e8le Xception   \nhttps:\/\/towardsdatascience.com\/review-xception-with-depthwise-separable-convolution-better-than-inception-v3-image-dc967dd42568","7ad2bbba":"Pour des datasets de grande taille, il est difficile de les charger en m\u00e9moire sans risquer de saturation de la m\u00e9moire  \nC'est pourquoi on charge g\u00e9n\u00e9ralement les datasets par *batch* depuis le disque pour les traiter (avec des g\u00e9n\u00e9rateurs comme **ImageDataGenerator**). N\u00e9anmoins, la performance peut en souffrir, notamment avec l'utilisation de GPU, l'apprentissage \u00e9tant principalement ralenti par les temps d'acc\u00e8s au disque.  \nAvec **image_dataset_from_directory**, on peut r\u00e9gler la *pr\u00e9lecture* (*prefetch*) de mani\u00e8re \u00e0 optimiser le s\u00e9quen\u00e7age des op\u00e9rations entre lecture des donn\u00e9es et apprentissage  \nLe param\u00e8tre **AUTOTUNE** permet de r\u00e9gler dynamiquement la taille du buffer  \n<img src=\"https:\/\/www.tensorflow.org\/guide\/images\/data_performance\/naive.svg\">  \n\nhttps:\/\/www.tensorflow.org\/guide\/data_performance   \nhttp:\/\/restanalytics.com\/2020-08-17-Transfer_Learning_and_Fine_tuning_withTensorFlow__Pneumonia_Classification_on_X_rays4\/","360b0ce4":"### Keras","f3bc8107":"On transforme l'image en un *batch* d'une image en ajoutant une dimension :","b4dfa43a":"## Cr\u00e9ation des datasets ","837f2a01":"## Am\u00e9lioration des performances (optionnel)","afc59925":"## Mod\u00e8le et entra\u00eenement","07a8fb2b":"## Initialisations","476e6bd4":"On lance l'entra\u00eenement   \n**Remarque :** l'augmentation **ne g\u00e9n\u00e8re pas un dataset plus important en taille** ; \u00e0 chaque *epoch*, on utilise un \"nouveau\" dataset, avec des images (l\u00e9g\u00e8rement) transform\u00e9es","cb53d456":"Les *couches d'augmentation* (*augmentation layers*) de Keras sont des couches particuli\u00e8res d'un mod\u00e8le Keras qui permettents de transformer les images d'un *batch* d'images (ou d'un dataset) \n- **RandomFlip(*mode*)** : retourne une image ; *mode* peut \u00eatre \"horizontal\", \"vertical\", ou \"horizontal_and_vertical\"\n- **RandomRotation(*factor*)** : effectue une rotation de l'image de *factor**2*Pi\n- **RandomZoom(*height_factor*)** : effectue un zoom ; par exemple RandomZoom((-0.1,0.1)) effectue un zoom al\u00e9atoire entre -10% et +10%\n- **RandomContrast(*factor*)** : ajuste le contraste\n- **RandomTranslation(*height_factor*,*width_factor*)** : d\u00e9cale l'image de *width_factor* horizontalement et *height_factor* verticalement \n \n https:\/\/keras.io\/guides\/preprocessing_layers\/","7d4cd316":"# Cat and dog avec augmentation","e9eaf304":"On int\u00e8gre la couche d'augmentation dans le mod\u00e8le   \nOn ajoute \u00e9galement une couche **Rescaling**, puisque le dataset est g\u00e9n\u00e9r\u00e9 \"\u00e0 la vol\u00e9e\" (pour rappel, les algorithmes de descente du gradient convergent mieux pour des valeurs entre 0 et 1) :","20a520ac":"On peut g\u00e9n\u00e9rer autant d'images transform\u00e9es qu'on le souhaite :","0705ec4c":"### Divers","69c7f616":"**dataset.take(1)** g\u00e9n\u00e8re un premier batch (32 images par d\u00e9faut) :","b9e3d3b6":"**image_dataset_from_directory** cr\u00e9e un *dataset Tensorflow* \u00e0 partir des images d'un r\u00e9pertoire donn\u00e9.  \n\nOn suppose que les images sont rang\u00e9es dans des sous-r\u00e9pertoires, dont les noms donnent les noms de classes (attribut **class_names**)  \n\nLes images sont lues en m\u00e9moire par *batch* (32 iamges par d\u00e9faut)"}}