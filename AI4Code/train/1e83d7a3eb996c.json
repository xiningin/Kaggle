{"cell_type":{"5344cc5f":"code","d38f10bf":"code","74555cd8":"code","4206e398":"code","a6bfed27":"code","cb9a70e0":"code","dc75722e":"code","80434b94":"code","e561a1e3":"code","6496c0ed":"code","ac80ea32":"code","6415b99e":"code","4c0320f8":"code","a5f0f587":"code","f7149866":"code","5a0f97d4":"code","f3964144":"code","1c2354e2":"code","9de3d15e":"code","e3ab47c4":"code","236c4933":"code","d3a0ddba":"code","9a3a13e2":"code","c229a011":"markdown","6fe7f20e":"markdown","56578b19":"markdown","feea1c2c":"markdown","967fa4e0":"markdown","de0d8e33":"markdown","3c18fb99":"markdown","0e1816a7":"markdown","bc4ac060":"markdown","6d52359f":"markdown","f65d9414":"markdown","85aaad91":"markdown","cafa806a":"markdown"},"source":{"5344cc5f":"from __future__ import absolute_import, division, print_function, unicode_literals\n\ntry:\n    import tensorflow.compat.v2 as tf\n\nexcept Exception:\n    pass\n\ntf.enable_v2_behavior()\n\nprint(f\"Tensorflow Version: {tf.__version__}\")","d38f10bf":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport os\n%matplotlib inline\nimport matplotlib.pyplot as plt","74555cd8":"import warnings as warnings\nwarnings.filterwarnings('ignore', category = DeprecationWarning) \nwarnings.filterwarnings('ignore', category = FutureWarning) \nwarnings.filterwarnings('ignore', category = UserWarning)","4206e398":"os.chdir('..')","a6bfed27":"train_df = pd.read_csv('input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('input\/house-prices-advanced-regression-techniques\/test.csv')","cb9a70e0":"train_df.head()","dc75722e":"test_df.head()","80434b94":"plt.rcParams['figure.figsize'] = (12.0, 6.0)\nprices = pd.DataFrame({\"price\":train_df[\"SalePrice\"], \"log(price + 1)\":np.log1p(train_df[\"SalePrice\"])});\nprices.hist();","e561a1e3":"#log transform the target:\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])","6496c0ed":"numericColumns = []\ncategoricalColumns = []\n\nfor column in train_df.columns:\n    if train_df[column].dtypes == int or train_df[column].dtypes==float:\n        numericColumns.append(column)\n    else:\n        categoricalColumns.append(column)\n\nnumericColumns.remove('Id')\nnumericColumns.remove('SalePrice')\n\nprint( f\"{len(numericColumns)} Numeric columns: {numericColumns} \\n\")\nprint( f\"{len(categoricalColumns)} Categorical columns: {categoricalColumns} \\n\")\nprint( 'ID and SalePrice are seperated')","ac80ea32":"# categorical columns fillna\n# firstly, to avoid loss nan during NN training\n# secondly, to be able to pass to labelencoder\n\nfor column in categoricalColumns:\n    train_df[column].fillna('missing', inplace = True)\n    test_df[column].fillna('missing', inplace = True)","6415b99e":"from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n\n# labelencode string categorical column to integer categorical column\n# tf.dataset cannot take in mixed data type, and thus need to change to numeric\n# take care of df_test labelencoder transform, there are unique labels not in df_train\n\nimport bisect\n\nfor column in categoricalColumns:\n    le = LabelEncoder()\n    le.fit(train_df[column])\n    train_df[column] = le.transform(train_df[column])\n    le_classes = le.classes_.tolist()\n    \n    # to handle categorical feature only in testing data\n    # handle int and string categorical columns differently\n    if type(le_classes[0]) is str:\n        test_df[column] = test_df[column].map(lambda s: 'other' if s not in le.classes_ else s)\n        bisect.insort_left(le_classes, 'other')\n        le.classes_ = le_classes\n        test_df[column] = le.transform(test_df[column])\n    else:\n        test_df[column] = test_df[column].map(lambda s: -1 if s not in le.classes_ else s)\n        bisect.insort_left(le_classes, -1)\n        le.classes_ = le_classes\n        test_df[column] = le.transform(test_df[column])","4c0320f8":"from sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(train_df, test_size=0.2)\nprint(f\"{len(train_df)} train examples\")\nprint(f\"{len(val_df)} validation examples\")\nprint(f\"{len(test_df)} test examples\")","a5f0f587":"#numeric columns fillna, to avoid loss nan during NN training\n\nfor column in numericColumns:\n    train_df[column].fillna(train_df[column].median(), inplace = True)\n    val_df[column].fillna(val_df[column].median(), inplace = True)\n    test_df[column].fillna(test_df[column].median(), inplace = True)","f7149866":"# standardscale the input data\n\nscaler = StandardScaler()\ntrain_df[numericColumns] = scaler.fit_transform(train_df[numericColumns])\nval_df[numericColumns] = scaler.transform(val_df[numericColumns])\ntest_df[numericColumns] = scaler.transform(test_df[numericColumns])","5a0f97d4":"def data_pipeline(dataframe, Shuffle=True, Batchsize=32):\n    \"\"\"This function removes the SalePrice column\n    and build the data pipeline using tf.Data.Dataset.\n    \n    Taken from article: \n    https:\/\/towardsdatascience.com\/a-succinct-tensorflow-2-0-solution-for-kaggle-house-prices-prediction-challenge-99310ad03ad0\n    \n    \n    \n    Parameters:\n    -----------------------------------------------------------------------------------------------\n    df(pd.DataFrame): Pandas Dataframe\n    Shuffle(Boolean): If you want to shuffle data(Default=True)\n    Batchsize(int): Batch Size (Default=32)\n    \n    \"\"\"\n    \n    dataframe = dataframe.copy()\n    labels = dataframe.pop('SalePrice')\n    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n    if Shuffle:\n        ds = ds.shuffle(buffer_size=len(dataframe))\n    ds = ds.batch(Batchsize)\n    return ds","f3964144":"batch_size = 32\ntrain_ds = data_pipeline(train_df, Batchsize=batch_size)\nval_ds = data_pipeline(val_df, Shuffle=False, Batchsize=batch_size)\ntest_df['SalePrice'] = 0\ntest_ds = data_pipeline(test_df, Shuffle=False, Batchsize=batch_size)","1c2354e2":"from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, LeakyReLU, Dropout, Concatenate\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow import feature_column\n\n\n\n\ndef NN(dense_units, numericColumns, categoricalColumns, use_batch_norm=False,use_dropout=False):\n        \"\"\"Defines a neural network with dense units, the number of \n        layers is given by the lenght of dense_units.\n        \n        Parameters\n        ------------------------------------------------------------\n        input_dim(tuple): Dimension of the input.\n        dense_units(list): Number of units used on each dense layer.\n        use_batch_norm(Boolean): True if =False\n        use_dropout=False\n        \"\"\"            \n        \n        feature_columns = []\n        \n        for col in numericColumns:\n            col = feature_column.numeric_column(col)\n            feature_columns.append(col)\n        \n        for col in categoricalColumns:\n            col = feature_column.indicator_column(feature_column.categorical_column_with_vocabulary_list(col,train_df[col].unique()))\n            feature_columns.append(col)\n        \n        feature_layer = tf.keras.layers.DenseFeatures(feature_columns)        \n        inputs = {colname : tf.keras.layers.Input(name=colname, shape=(), dtype='float32') for colname in numericColumns}\n        inputs.update({colname : tf.keras.layers.Input(name=colname, shape=(), dtype='int64') for colname in categoricalColumns})\n        \n        x = feature_layer(inputs)        \n        \n        _n_layers = len(dense_units)\n        \n        for i in range(_n_layers):\n            \n            dense_layer = Dense( \n                dense_units[i]\n                , name = 'Dense_' + str(i)\n                )\n            \n            x = dense_layer(x)\n\n            if use_batch_norm:\n                x = BatchNormalization()(x)\n\n            x = LeakyReLU()(x)\n\n            if use_dropout:\n                x = Dropout(rate = 0.25)(x)\n        \n        output_model = Dense(1, name='Output')(x)\n        \n        model = Model(inputs, output_model)\n        \n        return model","9de3d15e":"initial_learning_rate = 0.001\n\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate,\n    decay_steps=10000,\n    decay_rate=0.96,\n    staircase=True)\n\noptimizer = tf.optimizers.RMSprop(learning_rate=lr_schedule)\n\n#optimizer = tf.optimizers.Adam(learning_rate=initial_learning_rate)\n\nloss_function = tf.keras.losses.mean_squared_logarithmic_error\n\nlayers = [256, 512]\nlearning_rate = 0.0005\nmodel = NN(\n           dense_units=layers\n           ,numericColumns=numericColumns\n           ,categoricalColumns=categoricalColumns\n           ,use_batch_norm=True\n           ,use_dropout=True\n          )\n\nmodel.compile(loss= loss_function,\n              optimizer=optimizer,\n              )\n\n#tf.keras.utils.plot_model(model, 'Regression.png', show_shapes=False, rankdir='LR')","e3ab47c4":"# train the model\nhistory = model.fit(train_ds,\n                    validation_data=val_ds,\n                    epochs=10,\n                    verbose=2\n                   )","236c4933":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\n\ndef plot_history(history):\n    hist = pd.DataFrame(history.history)\n    hist['epoch'] = history.epoch\n    \n    plt.figure()\n    plt.xlabel('Epoch')\n    plt.ylabel('Mean Squared Error')\n    plt.plot(hist['epoch'], hist['loss'],\n           label='Train Error')\n    plt.plot(hist['epoch'], hist['val_loss'],\n           label = 'Val Error')\n    plt.legend()\n    plt.show()\n\nplot_history(history)","d3a0ddba":"!ls","9a3a13e2":"submission = np.expm1(model.predict(test_ds).flatten())\n\ndf = pd.DataFrame(columns = ['Id', 'SalePrice'])\ndf['Id'] = test_df['Id']\ndf['SalePrice'] = submission\n\ndf.to_csv(\"working\/submission.csv\", index=False)","c229a011":"# Creating Data Pipeline","6fe7f20e":"# Data","56578b19":"# Regression using Keras\n\nIn this notebook we will learn how to use Tensorflow 2 and keras to preprocess categorical variables.","feea1c2c":"# Creating Model","967fa4e0":"This code is based on the following [article](https:\/\/towardsdatascience.com\/a-succinct-tensorflow-2-0-solution-for-kaggle-house-prices-prediction-challenge-99310ad03ad0) and [source code](https:\/\/github.com\/jhwang1992\/KaggleHousePricesPrediction\/blob\/master\/kagglepriceprediction_part3_kerasSequantialModel.ipynb)","de0d8e33":"## Reading Data","3c18fb99":"### Separating Categorical and Numeric Columns","0e1816a7":"### Creating Training and validation split","bc4ac060":"### Data Analysis","6d52359f":"### Imputing Data ","f65d9414":"## Data Preprocessing","85aaad91":"So if we use the log(Price + 1) we have a more localized histogram.","cafa806a":"# Submission"}}