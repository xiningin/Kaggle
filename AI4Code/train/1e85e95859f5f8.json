{"cell_type":{"1098ad33":"code","734ada68":"code","43e16bd7":"code","163bbe27":"code","791f5e57":"code","d050b1c4":"code","dc977e34":"code","8c9c0cca":"code","ffe997e4":"code","28155311":"code","c0d9b80d":"code","69fda8d1":"code","bea96a57":"code","17b71d72":"code","ff43fe73":"code","e0118550":"code","4733a74f":"code","bac8b5ea":"code","5a20ee66":"code","3899ea62":"code","40221b3e":"code","2505427e":"code","7112e663":"code","24c1335d":"code","59c9464d":"code","f2cf234c":"code","83a7b036":"code","f72ba700":"code","06a65bb5":"code","0e0aeca4":"code","ef451f8a":"code","4d70fc64":"code","c5959ffa":"code","7dbbe2c2":"code","ecad85a2":"code","fe3b5178":"code","3bf1b917":"code","32ab3fc8":"code","c0fc016f":"code","9c0fd6ae":"markdown","68bb2d56":"markdown","ec2ac213":"markdown","46565019":"markdown","54b60d33":"markdown","d91b8ff4":"markdown","7efd3ba0":"markdown","4dd6eab5":"markdown","c468eb7e":"markdown","9f0e18af":"markdown","2957e3e0":"markdown","f720670f":"markdown","2ad0ff96":"markdown","06dd4519":"markdown","32579694":"markdown","5759bf10":"markdown","f2144296":"markdown","5213377c":"markdown","27175d79":"markdown","ac9d5ab8":"markdown","2cefb99f":"markdown","ad649b34":"markdown","8b71e33c":"markdown","66f2b75e":"markdown","f845f65c":"markdown","e64fc798":"markdown","6bbcc579":"markdown","838ca451":"markdown","de5947fd":"markdown","0b0109ad":"markdown","44c1b112":"markdown","9c9afa5d":"markdown"},"source":{"1098ad33":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","734ada68":"#reading the data\n\ndf = pd.read_csv('..\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')\n\ndf.head()","43e16bd7":"df.shape","163bbe27":"df.info()","791f5e57":"# changing null values in the salary column to zero\n\ndf['salary'].fillna(0,inplace=True)\n\ndf.info()","d050b1c4":"#Dropping the variable sl_no since it has no impact on the dependent variable\n\ndf.drop('sl_no',axis=1,inplace=True)","dc977e34":"#Checking the distribution of the data\n\ndf.describe()","8c9c0cca":"#Checking for duplicate rows\n\ndf.loc[df.duplicated()]","ffe997e4":"#plotting the distribution plot\n\ndf_num = df.select_dtypes(include=[np.number])\n\ncol_num = list(df_num.columns)\n\nc = len(col_num)\nm = 1\nn = 0\n\nplt.figure(figsize=(20,30))\n\nfor i in col_num:\n  if m in range(1,c+1):\n    plt.subplot(8,4,m)\n    sns.distplot(df_num[df_num.columns[n]])\n    m=m+1\n    n=n+1\n\nplt.show()","28155311":"#Plotting the pairplot\n\nsns.heatmap(df.corr(),linewidth=0.5,cmap='YlGnBu',annot=True)\nplt.show()","c0d9b80d":"df.info()","69fda8d1":"df.ssc_b.replace('Others','sscb_other',inplace=True)\ndf.hsc_b.replace('Others','hscb_other',inplace=True)\ndf.degree_t.replace('Others','deg_other',inplace=True)","bea96a57":"df.head()","17b71d72":"# Function for creating dummy variables for categorical variables\n\ndef dummy(x,df):\n    temp = pd.get_dummies(df[x],drop_first = True)\n    df =pd.concat([df,temp],axis=1)\n    df.drop(x,axis=1,inplace=True)\n    return df\n\n#Getting dummy variables for the categorical variables in df\ndf = dummy('status',df)\ndf = dummy('specialisation',df)\ndf = dummy('workex',df)\ndf = dummy('degree_t',df)\ndf = dummy('hsc_s',df)\ndf = dummy('hsc_b',df)\ndf = dummy('ssc_b',df)\ndf = dummy('gender',df)","ff43fe73":"df.head()","e0118550":"df.rename(columns={'Yes':'Workex','M':'Male'},inplace=True)\ndf.head()","4733a74f":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\nnp.random.seed(1001)\n\ndf_train,df_test = train_test_split(df,test_size=0.2,random_state=100)","bac8b5ea":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ndf_train[col_num] = scaler.fit_transform(df_train[col_num])\n\n","5a20ee66":"y_train = df_train['mba_p']\nX_train = df_train[['ssc_p','hsc_p']]","3899ea62":"import statsmodels.api as sm\n\n#Adding constant to X_train since by default statsmodel fits a regression line passing through the origin.\nX_train = sm.add_constant(X_train)\n\n#Fitting linear model\n\nlm = sm.OLS(y_train,X_train).fit()\n\n","40221b3e":"#printing paramaters\n\nprint(lm.params)\n\n#Printing the summary\n\nprint(lm.summary())","2505427e":"y_train2 = df_train['mba_p']\nX_train2 = df_train[['ssc_p','degree_p']]","7112e663":"#Adding constant\nX_train2 = sm.add_constant(X_train2)\n\n#fitting linear model\n\nlm2 = sm.OLS(y_train2,X_train2).fit()","24c1335d":"#Printing model parameters\n\nprint(lm2.params)\n\n#printing model summary\nprint(lm2.summary())","59c9464d":"y_train3 = df_train['mba_p']\nX_train3 = df_train[['hsc_p','degree_p']]","f2cf234c":"#Adding constant to X_tarin3\n\nX_train3 = sm.add_constant(X_train3)\n\n#fitting the linear model\n\nlm3 = sm.OLS(y_train3,X_train3).fit()","83a7b036":"print(lm3.params)\nprint(lm3.summary())","f72ba700":"y_train4 = df_train['mba_p']\nX_train4 = df_train[['ssc_p','hsc_p','degree_p']]","06a65bb5":"#Adding constant to X_train4\n\nX_train4= sm.add_constant(X_train4)\n\n#Fitting the linear model\n\nlm4 = sm.OLS(y_train4,X_train4).fit()","0e0aeca4":"#Printing coefficients and statistical summary\nprint(lm4.params)\n\nprint(lm4.summary())","ef451f8a":"#Predicting on the train data\n\ny_train_pred = lm4.predict(X_train4)","4d70fc64":"# Plotting the histogram of the error terms\n\nfig = plt.figure()\nsns.distplot((y_train4 - y_train_pred), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                \nplt.xlabel('Errors', fontsize = 18)   \nplt.show()","c5959ffa":"df_test.head()","7dbbe2c2":"#Transforming the numerical varianles of test data\ndf_test[col_num] = scaler.transform(df_test[col_num])","ecad85a2":"#Extracting X_test and y_test from the df_test  \n\nX_test = df_test[['ssc_p','hsc_p','degree_p']]\ny_test = df_test['mba_p']\n","fe3b5178":"#Adding constant\n\nX_test = sm.add_constant(X_test)\n\n#Predicting on th emodel\n\ny_pred = lm4.predict(X_test)","3bf1b917":"#Evaluating R2 score on the predictions\n\nfrom sklearn.metrics import r2_score\n\nprint(r2_score(y_test,y_pred))","32ab3fc8":"# Plotting y_test and y_pred to understand the spread.\n\nfig = plt.figure()\nsns.scatterplot(y_test,y_pred)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)   \nplt.show()","c0fc016f":"#Putting y_test and y_pred to a dataframe.\n\ncompare_pred = pd.DataFrame(columns=['y_test','y_pred'])\ncompare_pred['y_test'] = y_test\ncompare_pred['y_pred'] = y_pred\n\ncompare_pred.head(10)","9c0fd6ae":"* The R-squared value and F-statistics have improved slightly.\n* The Independent variables have very low p-value which means ssc_p and degree_p are important features.","68bb2d56":"* Getting a R2 score which is slightly less than the training R2 score.","ec2ac213":"### Model Building","46565019":"* There are no duplicate rows.","54b60d33":"### Q3. Show the functional form of a multiple regression model. Build a regression model with mbap as dependent variable and sscp, hscp and degree_p as three independent variables.","d91b8ff4":"* Lets rename the column 'Yes' to 'Workex' for better understanding.\n","7efd3ba0":"#### Model 2","4dd6eab5":"### Q2. Estimate a multiple regression equation for each of the below scenarios and based on the model\u2019s R-square comment which model is better. \n\n#### (i) Use mbap as outcome variable and sscp & degreep as the two predictor variables. (Model 2)\n#### (ii) Use mbap as outcome variable and hscp & degreep as the two predictor variables. (Model 3)","c468eb7e":"* It is quite intresting to find out that salary does not depend on the MBA percentage or the employbility test percentage but there is a fair chance of getting a good salary if the student scores well in Secondary education.","9f0e18af":"#### Model 1","2957e3e0":"#### Feature Scaling","f720670f":"### Prediction and evaluation","2ad0ff96":"* The residuals are normally distributed with mean zero, which satisfies our assumptions of Linear regression.\n* Lets predict on the test data now.","06dd4519":"#### Applying scaling on test set.","32579694":"* All the columns except 'salary' have no null values. \n* Salary for the students who are not placed has been mentioned as null. We can impute those values as zero.","5759bf10":"* The value 'Others' have been used in three categorical columns, ssc_b,hsc_b & degree_t\n* It will create problem while converting the values to dummy variable and might result in same column name 'Others' for all these three columns.\n* We must make that value Unique.","f2144296":"* The R-squared value is 0.193 which means only 19.3% variance in mba_p is explained by ssc_p and hsc_p.\n* Coefficients of both the independent variables has a very low p-value which means these are statistically significant.\n* But the F-statistics is very low which explains overall fit of the model is not statistically significant. We can do better by adding more variables.","5213377c":"### Conclusion:\n\n* As we can see above the R2 score of test data is very low and the predicted values on test data is far from the actual values.\n* Hence Secondary school percentage and Higher secondary school percentage are not valid factors of deciding MBA percentage of a student.","27175d79":"# Campus Recruitment\n\n## Multiple Linear Regression","ac9d5ab8":"#### Data Preparation","2cefb99f":"#### Model 4","ad649b34":"### Q1.  Develop an estimated multiple linear regression equation with mbap as response variable and sscp & hscp as the two predictor variables. Interpret the regression coefficients and check whether they are significant based on the summary","8b71e33c":"#### Model 3","66f2b75e":"### Reading and Understanding the data","f845f65c":"### Data Visualization","e64fc798":"#### Dividing the data into train and test sets","6bbcc579":"### Residual Analysis","838ca451":"* All the columns except salary & etest_p are normally distributed. Distribution looks good.","de5947fd":"#### Dividing the train and test set into X&y variables.","0b0109ad":"* Now there is no null value in the dataframe.","44c1b112":"* There isn't much improvement in the R2 score with ssc_p,hsc_p,degree_p as independent variable.\n* The F-statistics has dropped to 16.56 which means overall fit of the model with three variables is worse than the previous model.\n* The best model among these four models is Model 3. Lets do the residual analysis of the model.","9c9afa5d":"* The results are quite similar to Model 2. Lets take all three independent variables and see if we get any improvement."}}