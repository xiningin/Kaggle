{"cell_type":{"0e910868":"code","1952aa8c":"code","e4e7b0fd":"code","32dd4c98":"code","a8ebd7d4":"code","6d6a8786":"code","6b0b171f":"code","4c668be5":"code","49e29f5a":"code","2c2ee66c":"code","6c14df7f":"code","cdda40ee":"code","c47cf8da":"code","8715b9cb":"code","cbaf1389":"code","62af791e":"code","ff442012":"code","d3cdfd9d":"code","165c3a08":"code","9bbbdeae":"code","b5b7e8c4":"code","79af8436":"code","dad3b186":"code","5514fe80":"code","1fa9ddc3":"code","75129cbf":"code","42632eae":"code","dca89f1c":"code","f25df2da":"code","741b5dc2":"code","3d1d35a4":"code","bf20a895":"code","15566a9e":"code","b76f3375":"code","166f62dd":"code","70a66725":"code","f2289f89":"code","b30f621d":"code","875095d2":"code","8ff5843e":"code","70dfef71":"code","d937e5d3":"code","975c5b62":"code","0520a107":"code","b692d2e5":"code","6648eb9c":"code","2b4b0d8e":"code","bf88c0a9":"code","90423ed0":"code","e4766a79":"code","cfd99506":"code","f7c28236":"code","04618b7a":"code","29f57a27":"code","5142e355":"code","e34d0735":"code","a3f8c7a6":"code","d7952a5f":"code","58b5fe63":"code","5b922d0d":"markdown","7def25b7":"markdown","c06ebb29":"markdown","8c9ebd64":"markdown","dd2265a1":"markdown","c5e2227c":"markdown","824bddc9":"markdown","24faf463":"markdown","42d5b3f5":"markdown","09cc46f9":"markdown","c05c3c60":"markdown","17dc1bc6":"markdown","2e8d0d77":"markdown","cb4c70db":"markdown","5cbfb569":"markdown","f164fc68":"markdown","4131844a":"markdown","c0e44e59":"markdown","705ce1e8":"markdown","40c6382d":"markdown","0b975c16":"markdown","ca24d465":"markdown","8d068038":"markdown","0f882564":"markdown","b484e968":"markdown","e8113b65":"markdown","068115f2":"markdown","dc211d09":"markdown","04cb77b6":"markdown","ac84bf27":"markdown","4918780c":"markdown","7ede1a11":"markdown","44f759d5":"markdown","f23cafab":"markdown","e5b5f30f":"markdown","e3e62850":"markdown","67c7bdea":"markdown"},"source":{"0e910868":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nmpl.rc('figure', max_open_warning = 0)\nimport seaborn as sns\nfrom pandas import DataFrame\n%matplotlib inline","1952aa8c":"train_data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest_data  = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","e4e7b0fd":"train_data.head()","32dd4c98":"train_data.shape","a8ebd7d4":"train_data.info()","6d6a8786":"test_data.shape","6b0b171f":"test_data.info()","4c668be5":"fig = plt.figure(figsize=(10,5))\nsns.distplot(train_data['SalePrice'])\nplt.tight_layout()\nplt.show()","49e29f5a":"from scipy import stats\nstats.probplot(train_data['SalePrice'], plot=plt)","2c2ee66c":"fig = plt.figure(figsize=(10,5))\nsns.distplot(np.log1p(train_data['SalePrice']))\nplt.tight_layout()\nplt.show()","6c14df7f":"stats.probplot(np.log1p(train_data['SalePrice']), plot=plt)","cdda40ee":"cat_data = train_data.select_dtypes(include='object')\ncat_cols = cat_data.columns\n\nnum_data = train_data.select_dtypes(exclude='object')\nnum_cols = num_data.columns\n\nnum_to_cat_cols = ['MSSubClass', 'MoSold', 'YrSold', 'OverallQual', 'OverallCond']\n\nnum_cols = [i for i in num_cols if not i in num_to_cat_cols]\nnum_cols = [i for i in num_cols if not i in ['Id']]\n\nnum_data = num_data.drop(['Id', 'MSSubClass', 'MoSold', 'YrSold', 'OverallQual', 'OverallCond'], axis=1)\n\nprint(\"There are %d Num , %d Cat, %d Num-Cat columns.\" % (len(num_cols), len(cat_cols), len(num_to_cat_cols)))","c47cf8da":"for i in range(len(cat_data.columns)):\n    f, ax = plt.subplots(figsize=(7, 4))\n    fig = sns.countplot(cat_data.iloc[:,i].dropna())\n    plt.xlabel(cat_data.columns[i])\n    plt.xticks(rotation=60)","8715b9cb":"for i in range(len(num_data.columns)):\n    f, ax = plt.subplots(figsize=(7, 4))\n    fig = sns.distplot(num_data.iloc[:,i].dropna(), rug=False, hist=False, kde_kws={'bw':0.1})\n    plt.xlabel(num_data.columns[i])","cbaf1389":"skew_dict = {}\nfor cols in num_cols:\n    skew_dict[cols] = {'Skewness': train_data[cols].skew()}\n    \nskew_df = pd.DataFrame(skew_dict).transpose()\nskew_df.columns = ['Skewness']\nskew_df.sort_values(by=['Skewness'], ascending=False)","62af791e":"fig = plt.figure(figsize=(12,18))\nfor i in range(len(num_data.columns)):\n    fig.add_subplot(9, 4, i+1)\n    sns.scatterplot(num_data.iloc[:, i], num_data['SalePrice'])\nplt.tight_layout()\nplt.show()","ff442012":"corr_matrix = train_data[num_cols].corr()\ncorr_matrix['SalePrice'].sort_values(ascending=False)","d3cdfd9d":"sns.heatmap(corr_matrix)","165c3a08":"sns.lmplot('GrLivArea', 'SalePrice', data=train_data, height=8)\nplt.title(\"GrLivArea vs SalePrice\")\nplt.show()","9bbbdeae":"train_data[(train_data['GrLivArea'] > 4000) & (train_data['SalePrice'] < 300000)]","b5b7e8c4":"train_data = train_data.drop(train_data[train_data['Id'] == 524].index)\ntrain_data = train_data.drop(train_data[train_data['Id'] == 1299].index)","79af8436":"from sklearn.impute import SimpleImputer\n\nmean_imputer    = SimpleImputer(strategy='mean')\nmedian_imputer = SimpleImputer(strategy='median')\nfreq_imputer_MSZ    = SimpleImputer(strategy='most_frequent')\nfreq_imputer_UTI    = SimpleImputer(strategy='most_frequent')\nfreq_imputer_EXT1    = SimpleImputer(strategy='most_frequent')\nfreq_imputer_EXT2    = SimpleImputer(strategy='most_frequent')\nfreq_imputer_ELE    = SimpleImputer(strategy='most_frequent')\nfreq_imputer_KIT    = SimpleImputer(strategy='most_frequent')\nfreq_imputer_FUN    = SimpleImputer(strategy='most_frequent')\nfreq_imputer_SAL    = SimpleImputer(strategy='most_frequent')\n\ntrain_data['MSZoning'] = freq_imputer_MSZ.fit_transform(train_data[['MSZoning']])\ntrain_data['Alley']    = train_data['Alley'].fillna('No alley access')\ntrain_data['Utilities']    = freq_imputer_UTI.fit_transform(train_data[['Utilities']])\ntrain_data['Exterior1st']    = freq_imputer_EXT1.fit_transform(train_data[['Exterior1st']])\ntrain_data['Exterior2nd']    = freq_imputer_EXT2.fit_transform(train_data[['Exterior2nd']])\ntrain_data['MasVnrType']    = train_data['MasVnrType'].fillna('None')\ntrain_data['BsmtQual']    = train_data['BsmtQual'].fillna('No Basement')\ntrain_data['BsmtCond']    = train_data['BsmtCond'].fillna('No Basement')\ntrain_data['BsmtExposure']    = train_data['BsmtExposure'].fillna('No Basement')\ntrain_data['BsmtFinType1']    = train_data['BsmtFinType1'].fillna('No Basement')\ntrain_data['BsmtFinType2']    = train_data['BsmtFinType2'].fillna('No Basement')\ntrain_data['Electrical']    = freq_imputer_ELE.fit_transform(train_data[['Electrical']])\ntrain_data['KitchenQual']    = freq_imputer_KIT.fit_transform(train_data[['KitchenQual']])\ntrain_data['Functional']    = freq_imputer_FUN.fit_transform(train_data[['Functional']])\ntrain_data['FireplaceQu']    = train_data['FireplaceQu'].fillna('No Fireplace')\ntrain_data['GarageType']    = train_data['GarageType'].fillna('No Garage')\ntrain_data['GarageYrBlt']    = train_data['GarageYrBlt'].fillna(train_data['YearBuilt'])\ntrain_data['GarageFinish']    = train_data['GarageFinish'].fillna('No Garage')\ntrain_data['GarageQual']    = train_data['GarageQual'].fillna('No Garage')\ntrain_data['GarageCond']    = train_data['GarageCond'].fillna('No Garage')\ntrain_data['PoolQC']    = train_data['PoolQC'].fillna('No Pool')\ntrain_data['Fence']    = train_data['Fence'].fillna('No Fence')\ntrain_data['MiscFeature']    = train_data['MiscFeature'].fillna('None')\ntrain_data['SaleType']    = freq_imputer_SAL.fit_transform(train_data[['SaleType']])\n\ntrain_data['LotFrontage']    = median_imputer.fit_transform(train_data[['LotFrontage']])\ntrain_data['MasVnrArea']    = train_data['MasVnrArea'].fillna(0)\ntrain_data['BsmtFinSF1']    = train_data['BsmtFinSF1'].fillna(0)\ntrain_data['BsmtFinSF2']    = train_data['BsmtFinSF2'].fillna(0)\ntrain_data['BsmtUnfSF']    = train_data['BsmtUnfSF'].fillna(0)\ntrain_data['TotalBsmtSF']    = train_data['TotalBsmtSF'].fillna(0)\ntrain_data['BsmtFullBath']    = train_data['BsmtFullBath'].fillna(0)\ntrain_data['BsmtHalfBath']    = train_data['BsmtHalfBath'].fillna(0)\ntrain_data['GarageCars']    = train_data['GarageCars'].fillna(0)\ntrain_data['GarageArea']    = train_data['GarageArea'].fillna(0)\n\ntrain_data[num_to_cat_cols] = train_data[num_to_cat_cols].astype(str)","dad3b186":"#Missing value imputation (test_data)\ntest_data['MSZoning'] = freq_imputer_MSZ.transform(test_data[['MSZoning']])\ntest_data['Alley']    = test_data['Alley'].fillna('No alley access')\ntest_data['Utilities']    = freq_imputer_UTI.transform(test_data[['Utilities']])\ntest_data['Exterior1st']    = freq_imputer_EXT1.transform(test_data[['Exterior1st']])\ntest_data['Exterior2nd']    = freq_imputer_EXT2.transform(test_data[['Exterior2nd']])\ntest_data['MasVnrType']    = test_data['MasVnrType'].fillna('None')\ntest_data['BsmtQual']    = test_data['BsmtQual'].fillna('No Basement')\ntest_data['BsmtCond']    = test_data['BsmtCond'].fillna('No Basement')\ntest_data['BsmtExposure']    = test_data['BsmtExposure'].fillna('No Basement')\ntest_data['BsmtFinType1']    = test_data['BsmtFinType1'].fillna('No Basement')\ntest_data['BsmtFinType2']    = test_data['BsmtFinType2'].fillna('No Basement')\ntest_data['Electrical']    = freq_imputer_ELE.transform(test_data[['Electrical']])\ntest_data['KitchenQual']    = freq_imputer_KIT.transform(test_data[['KitchenQual']])\ntest_data['Functional']    = freq_imputer_FUN.transform(test_data[['Functional']])\ntest_data['FireplaceQu']    = test_data['FireplaceQu'].fillna('No Fireplace')\ntest_data['GarageType']    = test_data['GarageType'].fillna('No Garage')\ntest_data['GarageYrBlt']    = test_data['GarageYrBlt'].fillna(test_data['YearBuilt'])\ntest_data['GarageFinish']    = test_data['GarageFinish'].fillna('No Garage')\ntest_data['GarageQual']    = test_data['GarageQual'].fillna('No Garage')\ntest_data['GarageCond']    = test_data['GarageCond'].fillna('No Garage')\ntest_data['PoolQC']    = test_data['PoolQC'].fillna('No Pool')\ntest_data['Fence']    = test_data['Fence'].fillna('No Fence')\ntest_data['MiscFeature']    = test_data['MiscFeature'].fillna('None')\ntest_data['SaleType']    = freq_imputer_SAL.transform(test_data[['SaleType']])\n\ntest_data['LotFrontage']    = median_imputer.transform(test_data[['LotFrontage']])\ntest_data['MasVnrArea']    = test_data['MasVnrArea'].fillna(0)\ntest_data['BsmtFinSF1']    = test_data['BsmtFinSF1'].fillna(0)\ntest_data['BsmtFinSF2']    = test_data['BsmtFinSF2'].fillna(0)\ntest_data['BsmtUnfSF']    = test_data['BsmtUnfSF'].fillna(0)\ntest_data['TotalBsmtSF']    = test_data['TotalBsmtSF'].fillna(0)\ntest_data['BsmtFullBath']    = test_data['BsmtFullBath'].fillna(0)\ntest_data['BsmtHalfBath']    = test_data['BsmtHalfBath'].fillna(0)\ntest_data['GarageCars']    = test_data['GarageCars'].fillna(0)\ntest_data['GarageArea']    = test_data['GarageArea'].fillna(0)\n\ntest_data[num_to_cat_cols] = test_data[num_to_cat_cols].astype(str)","5514fe80":"train_data['Total_Bathroom'] = train_data['BsmtFullBath'] + 0.5 * train_data['BsmtHalfBath'] + \\\n                               train_data['FullBath'] + 0.5 * train_data['HalfBath']\n    \ntrain_data['Total_SF'] = train_data['TotalBsmtSF'] + train_data['1stFlrSF'] + \\\n                         train_data['2ndFlrSF'] + train_data['GrLivArea']\n    \ntrain_data['Total_Porch'] = train_data['OpenPorchSF'] + train_data['EnclosedPorch'] + \\\n                            train_data['3SsnPorch'] + train_data['ScreenPorch']","1fa9ddc3":"test_data['SalePrice'] = 0\ntest_data['Total_Bathroom'] = test_data['BsmtFullBath'] + 0.5 * test_data['BsmtHalfBath'] + \\\n                              test_data['FullBath'] + 0.5 * test_data['HalfBath']\n    \ntest_data['Total_SF'] = test_data['TotalBsmtSF'] + test_data['1stFlrSF'] + \\\n                        test_data['2ndFlrSF'] + test_data['GrLivArea']\n    \ntest_data['Total_Porch'] = test_data['OpenPorchSF'] + test_data['EnclosedPorch'] + \\\n                           test_data['3SsnPorch'] + test_data['ScreenPorch']","75129cbf":"add_num_cols = ['Total_Bathroom', 'Total_SF', 'Total_Porch']\nnum_cols.extend(add_num_cols)","42632eae":"model_data = pd.concat([train_data, test_data], axis=0, sort=None, ignore_index=True)","dca89f1c":"model_data.tail()","f25df2da":"model_data['Age'] = model_data['YearRemodAdd'] - model_data['YearBuilt']\nnum_cols.extend(['Age'])","741b5dc2":"sns.lmplot('Age', 'SalePrice', data=model_data, height=8)\nplt.title(\"Age vs SalePrice\")\nplt.show()","3d1d35a4":"model_data['HasPorch'] = model_data['Total_Porch'].apply(lambda x: 1 if x > 0 else 0)\nmodel_data['HasGarage'] = model_data['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\nmodel_data['HasPool'] = model_data['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nmodel_data['WasRemodeled'] = (model_data['YearRemodAdd'] != model_data['YearBuilt']).astype(np.int64)\nmodel_data['IsNew'] = (model_data['YearBuilt'] > 2000).astype(np.int64)\nmodel_data['IsComplete'] = (model_data['SaleCondition'] != 'Partial').astype(np.int64)","bf20a895":"model_data.shape","15566a9e":"from sklearn.preprocessing import LabelEncoder\n\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(model_data[c].values)) \n    model_data[c] = lbl.transform(list(model_data[c].values))","b76f3375":"numeric_feats = model_data.dtypes[model_data.dtypes != \"object\"].index\nnumeric_feats = [elem for elem in numeric_feats if elem not in ('Id', 'SalePrice')]\n\nfrom scipy.special import boxcox1p\n\nlam = 0.15\nfor feat in numeric_feats:\n    model_data[feat] = boxcox1p(model_data[feat], lam)","166f62dd":"model_data.head()","70a66725":"model_data_new = model_data.copy()\nmodel_data_new = model_data_new.drop(['Id'], axis=1)\n\nmodel_data_new = pd.get_dummies(model_data_new)\nmodel_data_updated = pd.concat([model_data['Id'], model_data_new], axis=1)\n\nmodel_data_updated.head()","f2289f89":"model_train_data = model_data_updated[model_data_updated.Id < 1461]\nmodel_test_data  = model_data_updated[model_data_updated.Id > 1460]\nprint(\"Train Data Shape: \", model_train_data.shape)\nprint(\"Test Data Shape : \", model_test_data.shape)","b30f621d":"model_label_data = pd.DataFrame(model_train_data['SalePrice'])\nmodel_label_data.columns=['SalePrice']\nmodel_train_data = model_train_data.drop(['Id', 'SalePrice'], axis=1)\nmodel_test_data  = model_test_data.drop(['Id', 'SalePrice'], axis=1)\nprint(\"Train Data Shape: \", model_train_data.shape)\nprint(\"Test Data Shape : \", model_test_data.shape)","875095d2":"from sklearn.preprocessing import RobustScaler\n\nstd_scaler = RobustScaler()\n\nmodel_train_data[numeric_feats] = std_scaler.fit_transform(model_train_data[numeric_feats])\nmodel_test_data[numeric_feats]  = std_scaler.transform(model_test_data[numeric_feats])","8ff5843e":"model_label_data['SalePrice'] = np.log(model_label_data['SalePrice'])","70dfef71":"from sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nlasso_model = Lasso(alpha=0.0005, max_iter=30000, random_state=42)\nscore = cross_val_score(lasso_model, model_train_data, model_label_data, scoring='neg_mean_squared_error', cv=10)\nscores_lasso = np.sqrt(-score).mean()\nprint(\"LASSO RMSE: \", scores_lasso)\nprint(\"LASSO STD : \", np.sqrt(-score).std())","d937e5d3":"lasso_model.fit(model_train_data, model_label_data)","975c5b62":"from sklearn.linear_model import Ridge\n\nridge_model = Ridge(alpha=0.0005, max_iter=30000, random_state=42)\nscore = cross_val_score(ridge_model, model_train_data, model_label_data, scoring='neg_mean_squared_error', cv=10)\nscores_ridge = np.sqrt(-score).mean()\nprint(\"Ridge RMSE: \", scores_ridge)\nprint(\"Ridge STD : \", np.sqrt(-score).std())","0520a107":"ridge_model.fit(model_train_data, model_label_data)","b692d2e5":"from sklearn.linear_model import ElasticNet\n\nenet_model = ElasticNet(alpha=0.0001, max_iter=30000, l1_ratio=0.6, random_state=42)\nscore = cross_val_score(enet_model, model_train_data, model_label_data, scoring='neg_mean_squared_error', cv=10)\nscores_enet = np.sqrt(-score).mean()\nprint(\"ElasticNet RMSE: \", scores_enet)\nprint(\"ElasticNet STD : \", np.sqrt(-score).std())","6648eb9c":"enet_model.fit(model_train_data, model_label_data)","2b4b0d8e":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor(n_estimators=1000, random_state=42)\nscore = cross_val_score(rf_model, model_train_data, model_label_data.values.ravel(), scoring='neg_mean_squared_error', cv=10)\nscores_rf = np.sqrt(-score).mean()\nprint(\"RandomForestRegressor RMSE: \", scores_rf)\nprint(\"RandomForestRegressor STD : \", np.sqrt(-score).std())","bf88c0a9":"rf_model.fit(model_train_data, model_label_data.values.ravel())","90423ed0":"from sklearn.ensemble import GradientBoostingRegressor\n\ngb_model = GradientBoostingRegressor(learning_rate=0.05, n_estimators=1000,max_depth=3, random_state=42)\nscore = cross_val_score(gb_model, model_train_data, model_label_data.values.ravel(), scoring='neg_mean_squared_error', cv=10)\nscores_gb = np.sqrt(-score).mean()\nprint(\"GradientBoostingRegressor RMSE: \", scores_gb)\nprint(\"GradientBoostingRegressor STD : \", np.sqrt(-score).std())","e4766a79":"gb_model.fit(model_train_data, model_label_data.values.ravel())","cfd99506":"from xgboost import XGBRegressor\n\nxgb_model = XGBRegressor(learning_rate=0.05, n_estimators=2500, max_depth=3, random_state=42)\nscore = cross_val_score(xgb_model, model_train_data, model_label_data.values.ravel(), scoring='neg_mean_squared_error', cv=10)\nscores_xgb = np.sqrt(-score).mean()\nprint(\"XGBRegressor RMSE: \", scores_xgb)\nprint(\"XGBRegressor STD : \", np.sqrt(-score).std())","f7c28236":"xgb_model.fit(model_train_data, model_label_data.values.ravel())","04618b7a":"from sklearn.ensemble import StackingRegressor\n\nestimators = [('Lasso', lasso_model),\n              ('Ridge', ridge_model),\n              ('Elastic Net', enet_model),\n              ('Random Forest', rf_model),\n              ('Gradient Boosting', gb_model),\n              ('XGBoosting', xgb_model)]\n\nstack_reg = StackingRegressor(estimators=estimators, final_estimator=xgb_model)\nscore = cross_val_score(stack_reg, model_train_data, model_label_data.values.ravel(), scoring='neg_mean_squared_error', cv=10)\nscores_stack = np.sqrt(-score).mean()\nprint(\"Stacking Regressor RMSE: \", scores_stack)\nprint(\"Stacking Regressor STD : \", np.sqrt(-score).std())","29f57a27":"stack_reg.fit(model_train_data, model_label_data.values.ravel())","5142e355":"results = pd.DataFrame({\n    'Model':['Lasso',\n            'Ridge',\n            'ElasticNet',\n            'Random Forest',\n            'Gradient Boosting',\n            'XGBoost',\n            'Stack'],\n    'RMSE':[scores_lasso,\n            scores_ridge,\n            scores_enet,\n            scores_rf,\n            scores_gb,\n            scores_xgb,\n            scores_stack\n            ]})\n\nsorted_result = results.sort_values(by='RMSE', ascending=True).reset_index(drop=True)\nsorted_result","e34d0735":"f, ax = plt.subplots(figsize=(14,8))\nplt.xticks(rotation='45')\nsns.barplot(x=sorted_result['Model'], y=sorted_result['RMSE'])\nplt.xlabel('Model', fontsize=15)\nplt.ylabel('RMSE', fontsize=15)\nplt.ylim(0.10, 0.14)\nplt.title('Model Performance', fontsize=15)","a3f8c7a6":"test_data_id = test_data['Id']\nlasso_predict = np.exp(lasso_model.predict(model_test_data))\nridge_predict = np.exp(ridge_model.predict(model_test_data))\nenet_predict  = np.exp(enet_model.predict(model_test_data))\nrf_predict    = np.exp(rf_model.predict(model_test_data))\ngb_predict    = np.exp(gb_model.predict(model_test_data))\nxgb_predict   = np.exp(xgb_model.predict(model_test_data))\nstack_predict = np.exp(stack_reg.predict(model_test_data))\n\nmixed_predict = np.exp((0.30 * np.log(lasso_predict)) + (0.30 * np.log(enet_predict)) + (0.05 * np.log(xgb_predict)) + (0.05 * np.log(gb_predict)) + (0.20 * np.log(stack_predict)) + (0.05 * np.log(ridge_predict.ravel())) + (0.05 * np.log(rf_predict)))","d7952a5f":"res_lasso = DataFrame(lasso_predict)\nres_lasso.columns = ['SalePrice']\nresult_lasso = pd.concat([test_data_id, res_lasso], axis=1)\nresult_lasso.to_csv(\"Submission_Lasso.csv\", index=False)\n\nres_ridge = DataFrame(ridge_predict)\nres_ridge.columns = ['SalePrice']\nresult_ridge = pd.concat([test_data_id, res_ridge], axis=1)\nresult_ridge.to_csv(\"Submission_Ridge.csv\", index=False)\n\nres_enet = DataFrame(enet_predict)\nres_enet.columns = ['SalePrice']\nresult_enet = pd.concat([test_data_id, res_enet], axis=1)\nresult_enet.to_csv(\"Submission_ElasticNet.csv\", index=False)\n\nres_rf = DataFrame(rf_predict)\nres_rf.columns = ['SalePrice']\nresult_rf = pd.concat([test_data_id, res_rf], axis=1)\nresult_rf.to_csv(\"Submission_RandomForestRegressor.csv\", index=False)\n\nres_gb = DataFrame(gb_predict)\nres_gb.columns = ['SalePrice']\nresult_gb = pd.concat([test_data_id, res_gb], axis=1)\nresult_gb.to_csv(\"Submission_GradientBoostingRegressor.csv\", index=False)\n\nres_xgb = DataFrame(xgb_predict)\nres_xgb.columns = ['SalePrice']\nresult_xgb = pd.concat([test_data_id, res_xgb], axis=1)\nresult_xgb.to_csv(\"Submission_XGBoostRegressor.csv\", index=False)\n\nres_stack = DataFrame(stack_predict)\nres_stack.columns = ['SalePrice']\nresult_stack = pd.concat([test_data_id, res_stack], axis=1)\nresult_stack.to_csv(\"Submission_StackingRegressor.csv\", index=False)","58b5fe63":"res_mixed = DataFrame(mixed_predict)\nres_mixed.columns = ['SalePrice']\nresult_mixed = pd.concat([test_data_id, res_mixed], axis=1)\nresult_mixed.to_csv(\"Submission_Mixed.csv\", index=False)","5b922d0d":"#### RandomForest Regressor.","7def25b7":"#### Log transform 'SalePrice'.","c06ebb29":"#### Create some boolean variable.","8c9ebd64":"#### We are creating dummy variables; so we are removing 'Id' otherwise we will get dummy varibles created for 'Id' which we don't want.","dd2265a1":"#### Read the datasets.","c5e2227c":"#### From this visualizations we can infer the log transformation makes the data symmetrical.","824bddc9":"#### Visualise the performance of different models.","24faf463":"#### We can see the co-relation co-efficients as well.","42d5b3f5":"#### Another important observation is 2 outliers at the top-right corner which we need to take care.","09cc46f9":"#### Feature engineering - we will create:\n\n1. Total Bathroom\n2. Total SF\n3. Total Porch","c05c3c60":"#### Univariate analysis of numeric columns which will give us a hint about their distribution.","17dc1bc6":"#### To minimise the skewness we will apply log transformation.","2e8d0d77":"#### Split train-test data as per their 'Id' in the original dataset.","cb4c70db":"#### GradientBoosting Regressor.\n#### Boosting is an ensemble method that can combine several weak learners into a strong learner. The idea is to train predictors sequentially, eah trying to correct its predecessor. Most common types of Boosting algorithms are -\n\n1. *Adaptive Boosting*: First base predictor is trained to make predictions on the training set. The relative weight of is then increased based on the model performance metrics. A second predictor is trained using updated weights and it makes predictions on training data.The weights are updated again and this process continues.\n\n2. *Gradient Boosting*: Gradient Bossting works in the same sequential way of Adaptive Boosting but instead of tweaking the instance weights at every iteration it tries to fit new predictor to the residual errors of the previous predictor.","5cbfb569":"#### This is another Kaggle competition to test our skill of Regression techniques.\n\n#### I am sharing my way of solving this problem :)\n\n#### Import the libraries.","f164fc68":"#### So we can see there are 81 columns with different data types and some of them contain missing values which we need to impute and the variable to predict is SalePrice.\n\n#### Let's analyse the 'SalePrice' variable from train dataset.","4131844a":"#### ElasticNet.","c0e44e59":"#### Analyse the datasets.","705ce1e8":"#### Create Age of the house variable.","40c6382d":"#### LASSO.","0b975c16":"#### Ridge.","ca24d465":"#### We can see the distribution is not symmetric so we need to transform the data to smooth.","8d068038":"#### Let's see the frequency of values of each categorical variable.","0f882564":"#### Apply LabelEncoder to categorical features.","b484e968":"#### Stacking - This is another ensemble method called *Stacked Generalization*. Each predictor performs a regression task on a new instance and then a meta learner\/blender takes this predictions as inputs and makes the final prediction.","e8113b65":"#### Let's see the correlation of numerical columns with SalePrice (Bi-variate Analysis).","068115f2":"#### Outlier identification.","dc211d09":"#### So we have prepared the data for the model and we will start applying it.","04cb77b6":"#### Prediction","ac84bf27":"#### Scale numerical data.","4918780c":"#### We can see the data is skewed to the right i .e positively skewed which is eveident from the Quantile-Quantile plot as displayed below.","7ede1a11":"#### So there are 2 rows where GrLivArea is more than 4000 but prices are less than 200,000.\n#### Let's see their Id.","44f759d5":"#### Missing value imputation.","f23cafab":"#### Since in each categorical variable the value of majority class is high compared to other class we will impute the missing values with most frequent class in each variablebut for some cases we will impute them as per the default value in field description.","e5b5f30f":"#### Since the datasets have mixed data we need to separate them out into multiple groups so that we can apply different imputation and transformation strategies.","e3e62850":"#### For categorical variables we will create dummy variables so we need to merge train and test data to ensure that none of the category is left out; otherwise the model will fail.","67c7bdea":"#### Smooth the numerical data - apply BoxCox transformation."}}