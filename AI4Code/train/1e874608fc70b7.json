{"cell_type":{"4ec6d50b":"code","c95a846c":"code","9e85c324":"code","9dcf5645":"code","3d85183a":"code","d74d0e57":"code","9760c149":"code","3f581862":"code","f768b7b5":"code","c424308f":"code","05a8ebb8":"code","dcc3f847":"code","ac8f031d":"code","1847dc91":"code","47bc1df9":"code","4f8228a7":"code","5a7ee0f6":"code","114a784a":"code","35861423":"code","96adf3d9":"code","eccf38d5":"markdown","492ae6cb":"markdown","42bc0dc7":"markdown","461e6438":"markdown"},"source":{"4ec6d50b":"import json\nimport pandas as pd\nimport numpy as np","c95a846c":"with open('..\/input\/pakistans-largest-pakwheels-automobiles-listings\/usedCars.json') as f:\n    DICT = json.load(f)\n\n\nDF = pd.json_normalize(DICT['usedCars'])\nDF.head()","9e85c324":"DF.isnull().sum()\n\n# That's quite a lot of missing values","9dcf5645":"DF[DF['extraFeatures.EngineCapacity'].isnull()][['vehicleEngine.engineDisplacement', 'extraFeatures.EngineCapacity']]\n\n# Let us assume engineDisplacement is more reliable from these two redundant features","3d85183a":"extrafeatures_to_drop = [\n    # Too much missing, not worth looking into\n    'AuctionGrade', 'ChassisNo.', 'ImportDate', 'BatteryCapacity', 'Warranty',\n    \n    # Redundant\n    'BodyType', 'LastUpdated:', 'EngineCapacity',\n    \n    # Looks like ID (near-unique values without know-able meaning)\n    'AdRef#'\n]\ncols_to_drop = [f'extraFeatures.{c}' for c in extrafeatures_to_drop] + ['extraFeatures']\n\n# I'm not going to use the image links and keywords\ncols_to_drop.extend(['image', 'keywords'])\n\n# Brand name is redundant against manufacturer\ncols_to_drop.extend(['brand.@type', 'brand.name'])\n\n# Useless cols\ncols_to_drop.extend(['vehicleEngine.@type', '@type', 'priceCurrency'])\n\ndata_df = DF.drop(cols_to_drop, axis = 1)\n\n# Parse the dates\nfrom dateutil import parser\ndef tryparse(s):\n    try:\n        result = parser.parse(s)\n    except ValueError:\n        print(s)\n        result = np.nan\n    return result\n\ndata_df['adLastUpdated'] = data_df['adLastUpdated'].apply(tryparse)\ndata_df.head()","d74d0e57":"data_df.isnull().sum()\n\n# Looks slightly better now.","9760c149":"# Handle missing body types using nearest neighbor classifier\n# Body type should be inferrable from the make and model of the car.\n# i.e. car of the same make and model usually have similar body type.\n\n# List of known relationship (manufacturer, model) -> bodyType\nD = data_df[['manufacturer', 'model', 'bodyType']] \\\n    .sort_values(['manufacturer', 'model', 'bodyType']).drop_duplicates()\nimpute_donors = D.dropna()[['manufacturer', 'model']]\nimpute_targets = D.dropna()['bodyType']\n\nfrom sklearn.neighbors import RadiusNeighborsClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\n\n# Get the category of *only the exact same car*, hence radius = 0. No distance allowed.\n# If there is no same car in this dataset with nonmissing body type,\n# just admit it as 'unknown'.\nbodytype_imputer = make_pipeline(OneHotEncoder(handle_unknown = 'ignore'),\n                                 RadiusNeighborsClassifier(radius = 0, outlier_label = 'Unknown'))\n\nbodytype_imputer.fit(impute_donors, impute_targets)\n(bodytype_imputer.predict(data_df[['manufacturer', 'model']]) == 'Unknown').sum()\n# 970 missing remains\n# Very significant missing value handling (~6000, about 80% missingness handled)","3f581862":"# Build a fresh new dataframe\n\n# Unhandled variables\nnew_tidy_df = data_df[[\n    'model', 'itemCondition', 'modelDate', 'manufacturer', 'fuelType',\n    'vehicleTransmission', 'price'\n]].copy()\n\n# Handle body type\nnew_tidy_df['bodyType'] = bodytype_imputer.predict(data_df[['manufacturer', 'model']])\n\n# Simple imputation:\n# RegisteredIn, Assembly\nfrom sklearn.impute import SimpleImputer\nmost_frequent_imputer = SimpleImputer(strategy = 'most_frequent')\n\nnew_tidy_df['registeredIn'] = most_frequent_imputer.fit_transform(\n    data_df['extraFeatures.RegisteredIn'].values.reshape(-1, 1)\n)\nnew_tidy_df['assembly'] = most_frequent_imputer.fit_transform(\n    data_df['extraFeatures.Assembly'].values.reshape(-1, 1)\n)\n\n# Convert to numeric and handle the missing value simply\nmedian_imputer = SimpleImputer(strategy = 'median')\nnew_tidy_df['engineCapacity'] = median_imputer.fit_transform(\n    data_df['vehicleEngine.engineDisplacement'].str.rstrip(' c') \\\n        .apply(pd.to_numeric).values.reshape(-1, 1)\n)\nnew_tidy_df['odometerMileage'] = median_imputer.fit_transform(\n    data_df['mileageFromOdometer'].str.rstrip(' km').str.replace(',', '') \\\n        .apply(pd.to_numeric).values.reshape(-1, 1)\n)\n\n# Handle missing value by forward-filling because the original dataset looks mostly sequential by time\nnew_tidy_df['adLastUpdated'] = data_df['adLastUpdated'].fillna(method = 'ffill')","f768b7b5":"new_tidy_df.isnull().sum()","c424308f":"new_tidy_df","05a8ebb8":"new_tidy_df.to_csv('used_cars_tidy.csv', index = False)","dcc3f847":"# Using CountVectorizer to transform list of features into more\n# machine-readable format.\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer(tokenizer = lambda x: x, preprocessor = lambda x: x, binary = True)\nvec.fit_transform(data_df['features'])","ac8f031d":"car_features_df = pd.DataFrame(\n    vec.fit_transform(data_df['features']).toarray(),\n    columns = vec.get_feature_names()\n)\n\ncar_features_df","1847dc91":"car_features_df.to_csv('car_features.csv', index = False)","47bc1df9":"# Using Python package \"colour\" to extract more \"structured\" [i.e. valid CSS] color name\n# from the arbitrary descriptions given in the \"color\" column\n\n# 544 unique colors? What?!\n# To show what I mean:\nunique_colors = data_df['color'].str.lower().unique()\nprint(len(unique_colors))\nunique_colors[0:50]","4f8228a7":"!pip install colour rapidfuzz","5a7ee0f6":"# Define helper function\nfrom colour import Color\nimport re\n\ndef is_color(s):\n    try:\n        Color(s)\n        return True\n    except ValueError:\n        return False\n\npossible_delimiters = re.compile('[-., ]')\nw3c_helped_colors = []\n\n# Extract color from word by checking if\n# that word is recognized by W3c as valid CSS color\nfor color in unique_colors:\n    if is_color(color):\n        w3c_helped_colors.append(\n            (Color(color).get_web().lower(),)\n        )\n    else:\n        words = possible_delimiters.split(color)\n        w3c_helped_colors.append(\n            tuple(Color(word).get_web().lower() for word in words if word and is_color(word))\n        )","114a784a":"w3c_helped_colors[0:50]","35861423":"# Rectify the colors\n\n# Let us encode the unknown colors \"other\",\n# and if one car has multiple color, let us assume\n# the first mentioned color is primary\/base color\n\ncolor_dict = {}\nfor i, color in enumerate(w3c_helped_colors):\n    if color:\n        color_dict[unique_colors[i]] = color[0]\n\noriginal_color = data_df['color'].str.lower()\nrectified_color = original_color.apply(lambda x: color_dict.get(x, 'other'))\ncar_colors_df = pd.DataFrame({\n    'w3c_color': rectified_color\n})\ncar_colors_df","96adf3d9":"# Because I oversimplify and make several implicit assumptions\n# as stated in above comments, this extracted color\n# may be accurate but also can be erroneous.\ncar_colors_df.to_csv('car_colors.csv', index = False)","eccf38d5":"# Hacks on Car Colors","492ae6cb":"# Handling missing values","42bc0dc7":"# Hacks on Car Accessories\n\nMay be useful for future analyses.","461e6438":"**<font size = 5>Tidied data is available as this notebook's output. Enjoy.<\/font>**\n\n# What is this? (prologue)\n\nThis notebook is just my messy process of \"rectifying\" this dataset.\n\nMotivation:\n\n1. There are redundant variables.\n2. There are missing values that can be patched by inferring from other column.\n3. There are variables with overwhelming missing ratio.\n4. There are variables whose type is wrong; type is string while should be float.\n5. There are variables that contain valuable information, but is poorly formatted\/need extensive processing.\n\nUnfortunately, there is no library that provides what I want to do (especially 2, 4, 5) in a simple way, so I created this preprocessing notebook as my isolated manual legwork instead. I write the resulting tidy data into csv so that I can use it in another notebook."}}