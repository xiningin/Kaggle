{"cell_type":{"62d68f2a":"code","09974ee0":"code","8c676c97":"code","9e4edc37":"code","632bf2f9":"code","776d84ab":"code","8fb4f028":"code","1f7ab99a":"code","8cc7a1e8":"code","7e92b179":"code","8baf4e21":"code","95babe6f":"code","7e032f08":"code","863907df":"code","b1a7e299":"code","518d0495":"code","c1d5be47":"code","c11a6f66":"code","caa9b1ea":"code","e1507f31":"code","b3a053ba":"code","3b8a1c7f":"code","6596732e":"code","84dd0e45":"code","2342062b":"code","7143178c":"code","dfe302ba":"code","ba78f933":"code","d3fdd16f":"code","6b16bed7":"code","20cc1bbb":"code","b32a0a23":"code","191f23d4":"code","24137ae5":"code","5cdb5cd1":"code","03bdec15":"code","e7b94705":"code","52635054":"code","6c063c17":"code","59df03e7":"code","17506428":"code","fb95bf10":"code","328167ef":"code","9100b207":"code","3e5a529f":"code","234c200d":"code","ea6b3de8":"code","78ba276b":"code","3486c63c":"code","ff309d15":"code","5e872407":"code","05571971":"code","29eb7378":"code","845d68ec":"code","bcad6a01":"code","83a8bfbe":"code","69bed265":"code","04373b68":"code","0b000d10":"code","8b238cdd":"code","1e7ac9d8":"markdown","cb821cf3":"markdown","f8c2275d":"markdown","e3e526a1":"markdown","35d93187":"markdown","93b762f7":"markdown","6122325d":"markdown","e68c5ca0":"markdown","5615979e":"markdown","7e12855f":"markdown","7fcd2b88":"markdown","8ad62b0c":"markdown","f327ec56":"markdown","8950f275":"markdown","681b9386":"markdown","1b07d747":"markdown","72f57f1a":"markdown","623183c4":"markdown","902eca17":"markdown","f430b4f5":"markdown","9aa56525":"markdown","2e850f09":"markdown","9171db5a":"markdown","c250ba4b":"markdown"},"source":{"62d68f2a":"import pandas as pd\nimport numpy as np","09974ee0":"from warnings import filterwarnings\nfilterwarnings('ignore')","8c676c97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9e4edc37":"os.chdir(\"..\/input\")\nos.listdir()","632bf2f9":"df=pd.read_csv(\"..\/input\/seo-sample-data\/SEO_data.csv\")","776d84ab":"# uppercase-lowercase conversion\ndf['words'] = df['words'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\ndf['title'] = df['title'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\ndf['h1'] = df['h1'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\ndf['snippet'] = df['snippet'].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))","8fb4f028":"# Random value assignment \ndf[\"title_include_words\"] = 0\ndf[\"h1_include_words\"] = 0\ndf[\"snippet_include_words\"] = 0\ndf[\"title_include_h1\"] = 0\ndf['tripped_links'] = 'www.google.com'","1f7ab99a":"df.head()","8cc7a1e8":"df['snippet'] = df['snippet'].replace('nan', 'Not Found')","7e92b179":"# Mapping 0 if not include words 1 if include\nfor i in range(len(df)):\n    df['tripped_links'][i] = df['links'][i].split('\/')[2]\n    if df['words'][i] in df['title'][i]:\n        df[\"title_include_words\"][i]=1\n    else:\n        df[\"title_include_words\"][i]=0\n    if df['words'][i] in df['h1'][i]:\n        df[\"h1_include_words\"][i]=1\n    else:\n        df[\"h1_include_words\"][i]=0\n    if df['words'][i] in df['snippet'][i]:\n        df[\"snippet_include_words\"][i]=1\n    else:\n        df[\"snippet_include_words\"][i]=0\n    if df['h1'][i] in df['title'][i]:\n        df[\"title_include_h1\"][i]=1\n    else:\n        df[\"title_include_h1\"][i]=0\n        ","8baf4e21":"df.head(10)","95babe6f":"df['title_length'] = 0","7e032f08":"for i in range(len(df)):\n    \n    df['title_length'][i] = len(df['title'][i]) ","863907df":"df.head()","b1a7e299":"df['tripped_links'].value_counts()","518d0495":"df['tripped_links'].unique()","c1d5be47":"df['tripped_links'].value_counts(normalize = True) * 100","c11a6f66":"df.head()","caa9b1ea":"dx = (df['tripped_links']\n.value_counts()\n.to_frame()\n.assign(percentage=lambda df: df['tripped_links'].div(df['tripped_links'].sum()))\n.style.format(dict(tripped_links='{:,}', percentage='{:.1%}')))","e1507f31":"dxx = dx.data","b3a053ba":"dxx.head()","3b8a1c7f":"dxx.rename(columns={'tripped_links' : 'URL_Count'}, inplace=True)","6596732e":"indexs=dxx.index","84dd0e45":"dxx['Url'] = indexs","2342062b":"dxx.head()","7143178c":"dxx = dxx.reset_index()","dfe302ba":"dxx = dxx.drop(['index'],axis=1)","ba78f933":"dxx.head()","d3fdd16f":"df.head()","6b16bed7":"df.shape","20cc1bbb":"dxx.shape","b32a0a23":"df['Url_count'] = 0","191f23d4":"df['Url_percentage'] = 3.444","24137ae5":"df['Url_percentage'].astype(float)","5cdb5cd1":"df.head()","03bdec15":"for i in range(len(df)):\n    for j in range(len(dxx)):\n        if df['tripped_links'][i] == dxx['Url'][j]:\n            df['Url_count'][i] = dxx['URL_Count'][j]\n            df['Url_percentage'][i] = dxx['percentage'][j]*100\n","e7b94705":"df.head()","52635054":"df.head()","6c063c17":"from sklearn.feature_extraction.text import TfidfVectorizer","59df03e7":"text_data = ['words','title','h1','snippet','links','tripped_links']","17506428":"for text in text_data:\n    #noktalama i\u015faretleri\n    df[text] = df[text].str.replace('[^\\w\\s]','')\n    #say\u0131lar\n    df[text] = df[text].str.replace('\\d','')\n    #stopwords\n    import nltk\n    #nltk.download('stopwords')\n    from nltk.corpus import stopwords\n    sw = stopwords.words('english')\n    df[text] = df[text].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n    #lemmi\n    from textblob import Word\n    #nltk.download('wordnet')\n    df[text] = df[text].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) ","fb95bf10":"df.head()","328167ef":"attributes = []","9100b207":"for text1 in text_data:\n    tf_idf_word_vectorizer = TfidfVectorizer()\n    tf_idf_word_vectorizer.fit(df[text1])\n    x_tf_idf_word = tf_idf_word_vectorizer.transform(df[text1])\n    x_tf_idf_word.toarray()\n    attributes.append(x_tf_idf_word)\n\n    \n    ","3e5a529f":"attributes[0].shape","234c200d":"attributes[1].shape","ea6b3de8":"from scipy.sparse import hstack","78ba276b":"a1 = hstack((attributes[0], attributes[1],attributes[2], attributes[3],attributes[4], attributes[5]))","3486c63c":"X=df.select_dtypes(exclude=['object'])","ff309d15":"X.head()","5e872407":"X = X.values","05571971":"X = hstack((a1,X))","29eb7378":"y= df['rank']","845d68ec":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state = 42)","bcad6a01":"!pip install xgboost","83a8bfbe":"from xgboost import XGBRegressor","69bed265":"xgb = XGBRegressor()\nxgb_model = xgb.fit(X_train,y_train)\n","04373b68":"y_pred = xgb.predict(X_test) ","0b000d10":"# RMSE \nfrom sklearn import metrics\nnp.sqrt(metrics.mean_squared_error(y_test, y_pred)) ","8b238cdd":"# R2 Score\n\nmetrics.r2_score(y_test,y_pred)","1e7ac9d8":"df.head(10)","cb821cf3":"# Alternative Analysis\ndf.pivot_table('rank', 'tripped_links', aggfunc=['mean', 'count']).sort_values([('count', 'rank')], ascending=False).assign(cumsum=lambda df: df[('count', 'rank')].cumsum(),\n         cum_perc=lambda df: df['cumsum'].div(df[('count', 'rank')].sum())).head(10).style.format({('cum_perc',''): '{:.2%}', ('mean', 'rank'): '{:.1f}'})","f8c2275d":"d['snippet'][5]","e3e526a1":"import pandas as pd\ndf =pd.DataFrame.from_dict(d)","35d93187":"resultlist = []","93b762f7":"!pip install requests_html","6122325d":"# READ DATA FROM CSV FILE","e68c5ca0":"## TFIDF VECTORIZER","5615979e":"df.tail(10)","7e12855f":"for query in query_list:\n    resultlist.append(google_results(query))","7fcd2b88":"# d['title'].append(link.text)","8ad62b0c":"# Save data to CSV file\ndf.to_csv('SEO_data.csv', index=False)","f327ec56":"resultlist","8950f275":"X=X.drop(['Url_percentage'],axis=1)","681b9386":"import numpy as np","1b07d747":"#Then install google package\n!pip install google","72f57f1a":"import requests\nfrom bs4 import BeautifulSoup\ntry: \n    from googlesearch import search \nexcept ImportError: \n    print(\"No module named 'google' found\") \n\n       \n# to search \nquery = \"Artificial intelligence\"\nquery_list = [\"Artificial intelligence\", \"Classification\", \"Clustering\", \"Convolutional neural network\", \"Data science\",\n              \"Deep learning\",\"Machine learning\", \"Database\", \"Recurrent neural network\", \"Supervised learning\",\n              \"Unsupervised learning\", \"Chatbots\", \"Artificial neural network\", \"Multilayer perceptron\", \"Reinforcement learning\" ]\nheaders = {'User-Agent': Enter Your User Agent'}\nfor query in query_list:\n\n    count=0\n    for j in search(query, tld=\"com\", num=25, stop=25, pause=3): \n        count=count+1\n        d['rank'].append(count)\n        d['words'].append(query)\n        url=j\n        d['links'].append(j)\n        # Make a GET request to fetch the raw HTML content\n        html_content = requests.get(url,headers = headers).text\n\n        # Parse the html content\n        soup = BeautifulSoup(html_content, \"lxml\")\n        \n       \n        try:\n            #print(soup.p.text)\n            d['snippet'].append(soup.find_all('p')[0].text)\n            \n         \n        except:\n            #print(\"Snippet not found\")\n            d['snippet'].append(\"Snippet not found\")\n           \n        try:\n            #print(soup.title.get_text())\n            d['title'].append(soup.title.get_text())\n            \n         \n        except:\n            #print(\"Title not found\")\n            d['title'].append(\"Title not found\")\n           \n        try:\n\n            \n            d['h1'].append(soup.find(\"h1\").text)\n            #print(soup.find(\"h1\").text)\n        except:\n            #print(\"h1 not found\")\n            d['h1'].append(\"h1 not found\")\n\n    \n        \n    \n    \n\n\n\n    \n","623183c4":"def google_results(query):\n    headers = {\n        'User-Agent': 'Enter Your User Agent'}\n    r = requests.get('https:\/\/www.google.com\/search?q=' + query ,headers=headers)\n    soup = BeautifulSoup(r.text, 'html.parser')\n    total_results_text = soup.find(\"div\", {\"id\": \"result-stats\"}).find(text=True, recursive=False) # this will give you the outer text which is like 'About 1,410,000,000 results'\n    results_num = ''.join([num for num in total_results_text if num.isdigit()]) # now will clean it up and remove all the characters that are not a number .\n    return results_num","902eca17":"for i in range(15):\n    for j in range(25):\n        d['total_result'].append(resultlist[i])\n","f430b4f5":"# DATA COLLECTION AND FEATURE EXTRACTION","9aa56525":"d['links'][1]","2e850f09":"from warnings import filterwarnings\nfilterwarnings('ignore')","9171db5a":"#initalizing an empty dictionary that would be written as Pandas Dataframe and then CSV\nd = {'words':[],'rank':[],'title':[],'h1':[],'snippet':[],'links':[],'total_result':[]}","c250ba4b":"# google package has one dependency on beautifulsoup which need to be installed first.\n!pip install beautifulsoup4"}}