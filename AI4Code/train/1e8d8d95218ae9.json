{"cell_type":{"df3853f5":"code","9a7e414b":"code","097902ff":"code","7663565f":"code","d10f7340":"code","99e0f407":"code","7df1f0b8":"code","88bae201":"code","d3e06c79":"code","97a6770b":"code","ab5b11bc":"code","86fb1414":"code","2babf3cc":"code","b683a568":"markdown","55b3bcbe":"markdown","7c910956":"markdown","a4408a53":"markdown","954528c7":"markdown","62983af4":"markdown","347bdb49":"markdown","00dbe132":"markdown"},"source":{"df3853f5":"# Importing required modules\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","9a7e414b":"# Loading data\nfruits = pd.read_csv('..\/input\/fruits\/fruits.csv')","097902ff":"# Visualizing data\nfruits.head()","7663565f":"# Creating dictionary with fruit labels as keys and names as values\nfruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique()))\nfruit_name","d10f7340":"# lists with features and labels\nX = fruits[['height', 'width', 'mass', 'color_score']]\ny = fruits['fruit_label']\n\n# Splitting in train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","99e0f407":"# plotting a scatter matrix\nfrom matplotlib import cm\nfrom pandas.plotting import scatter_matrix\n\ncmap = cm.get_cmap('gnuplot')\nscatter = scatter_matrix(X_train, c= y_train, marker = 'o', s=40, hist_kwds={'bins':15}, figsize=(7,7), cmap=cmap)","7df1f0b8":"# plotting a 3D scatter plot\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection = '3d')\nax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c = y_train, marker = 'o', s=100)\nax.set_xlabel('width')\nax.set_ylabel('height')\nax.set_zlabel('color_score')\nplt.show()","88bae201":"# Choosing features\nX = fruits[['mass', 'width', 'height']]\ny = fruits['fruit_label']\n\n# Tran test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","d3e06c79":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 5)\n\n# Training\nknn.fit(X_train, y_train)\n\n# Estimating accuracy using test data\nknn.score(X_test, y_test)","97a6770b":"# first example: a small fruit with mass 20g, width 4.3 cm, height 5.5 cm\nfruit_prediction = knn.predict([[20, 4.3, 5.5]])\nfruit_name[fruit_prediction[0]]","ab5b11bc":"## Code by Shahul Es stack overflow\n\nimport matplotlib.cm as cm\nfrom matplotlib.colors import ListedColormap, BoundaryNorm\nimport matplotlib.patches as mpatches\nimport matplotlib.patches as mpatches\n\n\ndef plot_fruit_knn(X, y, n_neighbors, weights):\n    X_mat = X[['height', 'width']].values\n    y_mat = y.values\n# Create color maps\n    cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#FCBF56','#FFFD6B'])\n    cmap_bold  = ListedColormap(['#FF0000', '#00FF00', '#FFA100','#E8E500'])\n    clf = KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X_mat, y_mat)\n# Plot the decision boundary by assigning a color in the color map\n    # to each mesh point.\n\n    mesh_step_size = .01  # step size in the mesh\n    plot_symbol_size = 50\n\n    x_min, x_max = X_mat[:, 0].min() - 1, X_mat[:, 0].max() + 1\n    y_min, y_max = X_mat[:, 1].min() - 1, X_mat[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, mesh_step_size),\n                         np.arange(y_min, y_max, mesh_step_size))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n# Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light, shading='auto')\n# Plot training points\n    plt.scatter(X_mat[:, 0], X_mat[:, 1], s=plot_symbol_size, c=y, \n                cmap=cmap_bold, edgecolor = 'black')\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    patch0 = mpatches.Patch(color='#FF0000', label='apple')\n    patch1 = mpatches.Patch(color='#00FF00', label='mandarin')\n    patch2 = mpatches.Patch(color='#FFA100', label='orange')\n    patch3 = mpatches.Patch(color='#E8E500', label='lemon')\n    plt.legend(handles=[patch0, patch1, patch2, patch3])\n\n#plt.title(\"4-Class classification (k = %i, weights = '%s')\" % (n_neighbors, weights))    \n\nplot_fruit_knn(X_train, y_train, 5, 'uniform')\nplt.xlabel('height (cm)')\nplt.ylabel('width (cm)')\nplt.show()","86fb1414":"k_range = range(1,20)\nscores = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    scores.append(knn.score(X_test, y_test))\n\nplt.figure()\nplt.xlabel('k')\nplt.ylabel('accuracy')\nplt.scatter(k_range, scores)\nplt.xticks([0,5,10,15,20]);","2babf3cc":"t = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n\nknn = KNeighborsClassifier(n_neighbors = 5)\n\nplt.figure()\n\nfor s in t:\n\n    scores = []\n    for i in range(1,1000):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s)\n        knn.fit(X_train, y_train)\n        scores.append(knn.score(X_test, y_test))\n    plt.plot(s, np.mean(scores), 'bo')\n\nplt.xlabel('Training set proportion (%)')\nplt.ylabel('accuracy');","b683a568":"The features available for each entry are the mass, height, and width of mandarins, oranges, lemons and apples. Heights are measured along the core and widths are the widest measurment perpendicular to the core.","55b3bcbe":"### Creating a K-nearest neighbors classifier object","7c910956":"## Applied Machine Learning in Python - Notebook 1\n\n### Module 1: Simple classification task\n","a4408a53":"### Classifying unseen data","954528c7":"### Plotting k-NN classifier decision boundaries","62983af4":"### Examing the data","347bdb49":"### Checking k-NN accuracy sensitivity to the k parameter","00dbe132":"### Checking k-NN accuracy sensitivity to the train\/test split proportion"}}