{"cell_type":{"c3104e33":"code","e6479940":"code","eb482ca7":"code","59cced80":"code","d317d6e3":"code","70860f45":"code","82511c30":"code","0a031131":"code","4b849a16":"code","613c4c86":"code","6f0e835a":"code","2a974b7c":"code","312d42af":"code","b04cc9ce":"code","232796bc":"code","a1e6ad95":"code","90bd9a8e":"code","30ed9021":"code","bd5ab9b2":"code","76373fd4":"code","233dd403":"code","a4e3fcfc":"code","b7c5f0de":"code","0bdd0044":"code","084b849f":"code","77f18420":"code","d7af8d2d":"code","e592da0b":"code","083a159e":"code","dec402ba":"code","11dcb854":"code","469b4081":"code","4a30d4e8":"code","dffa5b6a":"code","0c6786ff":"code","e64607fc":"code","c1a0b8f3":"code","f58c903d":"code","c397ef44":"code","2a23c3dc":"code","ff8132ed":"code","a2540da0":"code","91c387ff":"code","124e310a":"code","0b859658":"code","05f246c2":"code","75b42421":"code","8f44452f":"code","ab3c2284":"code","ff5d39a1":"code","8b1c4707":"code","cb958ff8":"code","6557b96f":"code","a47024f9":"code","84ebed92":"code","95b92158":"code","5ad731bf":"code","56d704d0":"code","edcf410c":"markdown","96fba260":"markdown","04d90840":"markdown","1fafd2f2":"markdown","da82b683":"markdown","969ec8b1":"markdown","5066746c":"markdown","915c8f75":"markdown","872af814":"markdown","739a86b9":"markdown","dfd02f23":"markdown","47996c08":"markdown","38dd1457":"markdown","785c2b0b":"markdown","b4127cf8":"markdown","0a4de3ef":"markdown","fb5e5939":"markdown","b399b7b0":"markdown","966fb2ab":"markdown","4875dfb2":"markdown","e30a005b":"markdown","9917c3cf":"markdown","f515d305":"markdown","7270c7c5":"markdown","9af4e6fd":"markdown","5aa928c9":"markdown"},"source":{"c3104e33":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport shap\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, IsolationForest\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import export_graphviz\nimport pydot\n\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nimport imblearn.pipeline\n\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nfrom xgboost import plot_tree\n\nfrom hyperopt import hp\nfrom hyperopt import fmin, tpe\n\n%matplotlib inline\nplt.style.use('ggplot')","e6479940":"shap.initjs()","eb482ca7":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","59cced80":"df.shape","d317d6e3":"df.head()","70860f45":"# Convert all columns to lower case\ndf.columns = [col.lower() for col in df.columns]\n\ndf.head()","82511c30":"# Check for nulls\ndf.info()","0a031131":"df.describe()","4b849a16":"# Define all features\norig_feats = [col for col in df.columns if 'class' not in col]\n\nlen(orig_feats)","613c4c86":"sns.pairplot(df[orig_feats[:10]+['class']].sample(10000, random_state=1), hue='class')","6f0e835a":"sns.pairplot(df[orig_feats[10:20]+['class']].sample(10000, random_state=1), hue='class')","2a974b7c":"sns.pairplot(df[orig_feats[20:]+['class']].sample(10000, random_state=1), hue='class')","312d42af":"target = 'class'","b04cc9ce":"# Only 0.17% of the dataset is labelled as fraud\ndf[target].value_counts(normalize=True)","232796bc":"df[target].hist()","a1e6ad95":"df_corr = df.corr()","90bd9a8e":"df_corr.style.background_gradient().set_precision(2)","30ed9021":"df.corr()['class'].sort_values()","bd5ab9b2":"X = df[orig_feats]\ny = df[target]","76373fd4":"# Use stratify to ensure samples of fraud label are in the test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, random_state=1)\n\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","233dd403":"y_train.value_counts(normalize=True)","a4e3fcfc":"y_test.value_counts(normalize=True)","b7c5f0de":"naive_preds = np.zeros(y_test.shape[0])\nlen(naive_preds)","0bdd0044":"roc_auc_score(y_test, naive_preds)","084b849f":"print(classification_report(y_test, naive_preds))","77f18420":"num_boost_rounds = 1000\nearly_stopping_rounds = 10\n\ninitial_params = {'objective': 'binary:logistic', 'eval_metric': 'auc'}","d7af8d2d":"logit_cv = cross_val_score(estimator=LogisticRegression(), \n                            X=X_train, \n                            y=y_train, \n                            scoring='roc_auc',\n                            cv=StratifiedKFold(n_splits=5, random_state=1))","e592da0b":"rf_cv = cross_val_score(estimator=RandomForestClassifier(),\n                         X=X_train, \n                         y=y_train,\n                         scoring='roc_auc',\n                         cv=StratifiedKFold(n_splits=5, random_state=1))","083a159e":"xgb_cv = cross_val_score(estimator=XGBClassifier(num_boost_rounds=num_boost_rounds, \n                                                 early_stopping_rounds=early_stopping_rounds,\n                                                 **initial_params),\n                          X=X_train,\n                          y=y_train,\n                          scoring='roc_auc',\n                          cv=StratifiedKFold(n_splits=5, random_state=1))","dec402ba":"print(f'Logistic Regression CV Mean AUC score: {logit_cv.mean()}')\nprint(f'Random Forest CV Mean AUC score: {rf_cv.mean()}')\nprint(f'XGBoost CV Mean AUC score: {xgb_cv.mean()}')\n","11dcb854":"model_cv_results = {'logit': logit_cv, 'random_forest': rf_cv, 'xgb': xgb_cv}","469b4081":"fig, ax = plt.subplots(figsize=(10,8))\nplt.boxplot(model_cv_results.values())\nax.set_xticklabels(model_cv_results.keys())\nplt.title('AUC scores using Different Classification Algorithms')","4a30d4e8":"oversamp_pipeline = imblearn.pipeline.Pipeline([('oversample', RandomOverSampler(random_state=42)),\n                                                ('xgb', XGBClassifier(num_boost_rounds=num_boost_rounds, \n                                                                      early_stopping_rounds=early_stopping_rounds,\n                                                                      **initial_params))])","dffa5b6a":"undersamp_pipeline = imblearn.pipeline.Pipeline([('undersample', RandomUnderSampler(random_state=42)),\n                                                 ('xgb', XGBClassifier(num_boost_rounds=num_boost_rounds, \n                                                                       early_stopping_rounds=early_stopping_rounds,\n                                                                       **initial_params))])","0c6786ff":"smote_pipeline = imblearn.pipeline.Pipeline([('smote', SMOTE(random_state=42)),\n                                             ('xgb', XGBClassifier(num_boost_rounds=num_boost_rounds, \n                                                                   early_stopping_rounds=early_stopping_rounds,\n                                                                   **initial_params))])","e64607fc":"sampling_methods = {'random_oversampling': oversamp_pipeline,\n                    'random_undersampling': undersamp_pipeline,\n                    'smote': smote_pipeline}","c1a0b8f3":"sampling_cv_results = {}\n\nfor method, pipeline in sampling_methods.items():\n    cv_results = cross_val_score(pipeline, X_train, y_train, cv=StratifiedKFold(n_splits=5, random_state=1), scoring='roc_auc')\n    sampling_cv_results[method] = cv_results\n    print(method, cv_results.mean())","f58c903d":"fig, ax = plt.subplots(figsize=(10,8))\nplt.boxplot(sampling_cv_results.values())\nax.set_xticklabels(sampling_cv_results.keys())\nplt.title('AUC scores of XGB Classifier using Different Sampling Methods')","c397ef44":"# Set up grid for hyperopt\nspace = {\n    'max_depth': hp.quniform('max_depth', 4, 10, 2),\n    'min_child_weight': hp.quniform('min_child_weight', 1, 20, 2),\n    'gamma': hp.quniform('gamma', 0, 5, 0.5),\n    'subsample': hp.uniform('subsample', 0.5, 0.9),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.9),\n    'eta': hp.uniform('eta', 0.01, 0.3),\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc'\n}","2a23c3dc":"# Objective function\ndef objective(params):\n    params = {'max_depth': int(params['max_depth']),\n              'min_child_weight': int(params['min_child_weight']),\n              'gamma': params['gamma'],\n              'subsample': params['subsample'],\n              'colsample_bytree': params['colsample_bytree'],\n              'eta': params['eta'],\n              'objective': params['objective'],\n              'eval_metric': params['eval_metric']}\n    \n    xgb_clf = XGBClassifier(num_boost_rounds=num_boost_rounds, \n                            early_stopping_rounds=early_stopping_rounds,\n                            **params)\n    \n    best_score = cross_val_score(xgb_clf, X_train, y_train, scoring='roc_auc', cv=5, n_jobs=3).mean()\n    \n    loss = 1 - best_score \n    \n    return loss","ff8132ed":"best_result = fmin(fn=objective, space=space, max_evals=20, \n                   rstate=np.random.RandomState(42), algo=tpe.suggest)","a2540da0":"best_result","91c387ff":"best_params = best_result\nbest_params['max_depth'] = int(best_params['max_depth'])\nbest_params['min_child_weight'] = int(best_params['min_child_weight'])\nbest_params['gamma'] = best_params['gamma']\nbest_params['colsample_bytree'] = round(best_params['colsample_bytree'], 1)\nbest_params['eta'] = round(best_params['eta'], 1)\nbest_params['subsample'] = round(best_params['subsample'], 1)\nbest_params","124e310a":"final_model = imblearn.pipeline.Pipeline([('smote', SMOTE(random_state=1)),\n                                          ('xgb', XGBClassifier(num_boost_rounds=num_boost_rounds,\n                                                                early_stopping_rounds=early_stopping_rounds, \n                                                                **best_params))])","0b859658":"final_model.fit(X_train, y_train)","05f246c2":"final_preds = final_model.predict_proba(X_test)[:,1]","75b42421":"test = pd.merge(X_test, y_test, left_index=True, right_index=True)\ntest.head()","8f44452f":"test.shape","ab3c2284":"df_preds = test.copy()\ndf_preds['fraud_score'] = final_preds","ff5d39a1":"df_preds.head()","8b1c4707":"df_preds['fraud_score'].describe()","cb958ff8":"df_preds.to_csv('.\/final_model_preds.csv', index=False)","6557b96f":"roc_auc_score(y_test, final_preds)","a47024f9":"print(classification_report(y_test, final_preds.round()))","84ebed92":"explainer = shap.TreeExplainer(final_model[1])","95b92158":"shap_values = explainer.shap_values(X_test)","5ad731bf":"shap.summary_plot(shap_values, X_test)","56d704d0":"shap.summary_plot(shap_values, X_test, plot_type='bar')","edcf410c":"## Naive Models ","96fba260":"## Correlation Matrix","04d90840":"# Modeling","1fafd2f2":"Random Oversampling will sample the minority class with replacement until a defined threshold. The default sampling strategy will oversample the minority class until our new dataset has a 50\/50 split between 0 and 1 labels.","da82b683":"### Oversampling - SMOTE","969ec8b1":"Use cross validation and train naive models to see AUC scores and select model","5066746c":"### Random Oversampling","915c8f75":"## Compare Sampling Methods","872af814":"### Random Undersampling","739a86b9":"## Final Model Performance","dfd02f23":"# Train, Test, Split","47996c08":"Random Undersampling will remove instances of the majority class until a defined threshold. The default sampling strategy will undersample the majority class (without replacement by default) until our new dataset has a 50\/50 split between 0 and 1 labels.","38dd1457":"## Final Model Feature Importances","785c2b0b":"## Pairplots of Features and Target Variable","b4127cf8":"# EDA","0a4de3ef":"### Cross Validation to Compare Sampling Methods","fb5e5939":"# Bayesian Hyperparameter Tuning using Hyperopt","b399b7b0":"## Basic Checks for Nulls & Descriptive Statistics","966fb2ab":"## Train Final Model with Optimal Hyperparameters","4875dfb2":"## Predicting All Zeros","e30a005b":"## Target Variable","9917c3cf":"## Save Final Model Predictions to .csv","f515d305":"If we predict all 0's, what would be the AUC?","7270c7c5":"XGBoost performed the best, so select XGBoost as our model of choice going forward.","9af4e6fd":"# Evaluating Final Model Performance","5aa928c9":"## Import Data"}}