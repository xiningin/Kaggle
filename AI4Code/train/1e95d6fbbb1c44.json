{"cell_type":{"2feafcb8":"code","498a2e17":"code","c30f0c6c":"code","8f3149b2":"code","1f86b040":"code","d2b4f1f9":"code","77c81302":"code","8cd1a0e9":"code","55eece60":"code","239115d6":"code","11198f9f":"code","64b4af77":"code","a215c7ac":"code","f54ad35a":"code","fe3fc261":"code","02b07390":"code","ff5be58e":"code","0d8607cf":"code","fce0f34a":"code","c0041300":"code","1961ad8a":"code","5e9ff403":"code","03b4e193":"code","edd1bcfe":"code","18c49200":"code","8b666340":"code","14c27ed7":"code","86703b8b":"code","edfa04c1":"code","3c5c7f3c":"markdown"},"source":{"2feafcb8":"import os\nimport csv\nimport math\nimport random\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport torch.nn.functional as F\n\n\nimport torch\n\nimport torch.nn as nn\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom transformers import AdamW\nfrom transformers import AutoTokenizer\nfrom transformers import AutoModel\nfrom transformers import AutoConfig\nfrom transformers import get_cosine_schedule_with_warmup\nfrom sklearn import metrics\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection\n\nimport gc\ngc.enable()","498a2e17":"df = pd.read_csv(\"..\/input\/trec-train\/trec_train - trec_train (1).csv\", error_bad_lines = False, escapechar = \"\\\\\" , quoting = csv.QUOTE_NONE)","c30f0c6c":"train_df, test_set = model_selection.train_test_split(df, train_size = 0.1)\ntrain_df.reset_index(drop = True)","8f3149b2":"map_values=dict()\ntrain  = list(train_df['BROWSE_NODE_ID'])\ncounter= 0\nfor i in train:\n  if map_values.get(i)==None:\n    map_values[i]=counter\n    counter+=1\ntrain_df['BROWSE_NODE_ID'] = [map_values[x] for x in train]","1f86b040":"train_df.reset_index(drop = True)","d2b4f1f9":"len(train_df[\"BROWSE_NODE_ID\"].value_counts().unique())","77c81302":"train_df = train_set.reset_index(drop = True)","8cd1a0e9":"len(df[\"BROWSE_NODE_ID\"].value_counts())","55eece60":"map_values=dict()\ntrain  = list(df['BROWSE_NODE_ID'])\ncounter= 0\nfor i in train:\n  if map_values.get(i)==None:\n    map_values[i]=counter\n    counter+=1\ndf['BROWSE_NODE_ID'] = [map_values[x] for x in train]","239115d6":"df = df.dropna()\ndf = df.reset_index(drop = True)\ndf = df.drop_duplicates()\n","11198f9f":"len(df)","64b4af77":"NUM_FOLDS = 5\nNUM_EPOCHS = 3\nBATCH_SIZE = 16\nMAX_LEN = 64\nROBERTA_PATH = \"..\/input\/roberta-base\"\nTOKENIZER_PATH = \"..\/input\/roberta-base\"\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"","a215c7ac":"tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)\nlen(df[\"BROWSE_NODE_ID\"].unique())","f54ad35a":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.TITLE.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.BROWSE_NODE_ID.values, dtype=torch.long)        \n        self.MAX_LEN = 64\n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            max_length = self.MAX_LEN,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        padding_len = \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","fe3fc261":"class LitDataset(Dataset):\n    def __init__(self, df, inference_only=False):\n        super().__init__()\n\n        self.df = df        \n        self.inference_only = inference_only\n        self.text = df.TITLE.tolist()\n        #self.text = [text.replace(\"\\n\", \" \") for text in self.text]\n        \n        if not self.inference_only:\n            self.target = torch.tensor(df.BROWSE_NODE_ID.values, dtype=torch.float32)        \n    \n        self.encoded = tokenizer.batch_encode_plus(\n            self.text,\n            padding = 'max_length',            \n            max_length = MAX_LEN,\n            truncation = True,\n            return_attention_mask=True\n        )        \n \n\n    def __len__(self):\n        return len(self.df)\n\n    \n    def __getitem__(self, index):        \n        input_ids = torch.tensor(self.encoded['input_ids'][index])\n        attention_mask = torch.tensor(self.encoded['attention_mask'][index])\n        \n        if self.inference_only:\n            return (input_ids, attention_mask)            \n        else:\n            target = self.target[index]\n            return (input_ids, attention_mask, target)","02b07390":"a = LitDataset(df)\ntemp = a.__getitem__(900)\ntemp","ff5be58e":"\"\"\"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        config.update({\"output_hidden_states\":True, \n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})                       \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config=config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 1024),            \n            nn.Tanh(),                       \n            nn.Linear(1024, 2048),\n            nn.Tanh(),\n            nn.Linear(2048, 4096),            \n            nn.Tanh(),\n            nn.Linear(4096, 3094),            \n            nn.Softmax(dim=1)\n        )        \n\n        self.classifier = nn.Sequential(                        \n            nn.Linear(768, 1024),            \n            nn.ReLU(),            \n            nn.Dropout(0.2),\n            nn.Linear(1024, 2048),\n            nn.ReLU(),\n            nn.Dropout(0.3),\n            nn.Linear(2048, 4096),            \n            nn.ReLU(),\n            nn.Linear(4096, 3094),              \n        )\n        \n\n    def forward(self, input_ids, attention_mask):\n        roberta_output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)        \n\n        # There are a total of 13 layers of hidden states.\n        # 1 for the embedding layer, and 12 for the 12 Roberta layers.\n        # We take the hidden states from the last Roberta layer.\n        last_layer_hidden_states = roberta_output.hidden_states[-1]\n\n        # The number of cells is MAX_LEN.\n        # The size of the hidden state of each cell is 768 (for roberta-base).\n        # In order to condense hidden states of all cells to a context vector,\n        # we compute a weighted average of the hidden states of all cells.\n        # We compute the weight of each cell, using the attention neural network.\n        weights = self.attention(last_layer_hidden_states)\n                \n        # weights.shape is BATCH_SIZE x MAX_LEN x 1\n        # last_layer_hidden_states.shape is BATCH_SIZE x MAX_LEN x 768        \n        # Now we compute context_vector as the weighted average.\n        # context_vector.shape is BATCH_SIZE x 768\n        context_vector = torch.sum(weights * last_layer_hidden_states, dim=1)        \n        \n        # Now we reduce the context vector to the prediction score.\n        return self.classifier(context_vector)\"\"\"","0d8607cf":"class WeightedLayerPooling(nn.Module):\n    def __init__(self, num_hidden_layers, layer_start = 8, layer_weights = None):\n        super(WeightedLayerPooling, self).__init__()\n        self.layer_start = layer_start\n        self.num_hidden_layers = num_hidden_layers\n        self.layer_weights = layer_weights if layer_weights is not None \\\n            else nn.Parameter(\n                torch.tensor([1] * (num_hidden_layers+1 - layer_start), dtype=torch.float)\n            )\n\n    def forward(self, all_hidden_states):\n        all_layer_embedding = all_hidden_states[self.layer_start:, :, :, :]\n        print(all_layer_embedding.shape)\n        print(self.layer_weights.shape)\n        weight_factor = self.layer_weights.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand(all_layer_embedding.size())\n        #print(weight_factor.shape)\n        weighted_average = (weight_factor*all_layer_embedding).sum(dim=0) \/ self.layer_weights.sum()\n        print(\"Weighted average\", weighted_average.shape)\n        return weighted_average\n    \nlayer_start = 9\npooler = WeightedLayerPooling(\n    config.num_hidden_layers, \n    layer_start=layer_start, layer_weights=None\n)\nweighted_pooling_embeddings = pooler(all_hidden_states)\nprint(weighted_pooling_embeddings.shape)\nweighted_pooling_embeddings = weighted_pooling_embeddings[:, 0]\nprint(weighted_pooling_embeddings.shape)\nlogits = nn.Linear(config.hidden_size, 1)(weighted_pooling_embeddings)\n\nprint(f'Hidden States Output Shape: {all_hidden_states.detach().numpy().shape}')\nprint(f'Weighted Pooling Output Shape: {weighted_pooling_embeddings.detach().numpy().shape}')\nprint(f'Logits Shape: {logits.detach().numpy().shape}')\n\ndel config, model, tokenizer, features\ngc.collect();","fce0f34a":"class LitModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        config = AutoConfig.from_pretrained(ROBERTA_PATH)\n        '''config.update({\"output_hidden_states\": False,\n                       \"hidden_dropout_prob\": 0.0,\n                       \"layer_norm_eps\": 1e-7})     '''                  \n        \n        self.roberta = AutoModel.from_pretrained(ROBERTA_PATH, config = config)  \n            \n        self.attention = nn.Sequential(            \n            nn.Linear(768, 512),            \n            nn.ReLU(),                       \n            nn.Linear(512, 311),\n        )        \n\n  \n        \n\n    def forward(self, input_ids, attention_mask):\n        output = self.roberta(input_ids=input_ids,\n                                      attention_mask=attention_mask)     \n        pooled = output[1]\n        print(pooled.shape)\n        #print(roberta_output[0].shape)\n        out = self.attention(pooled)\n        print(out.shape)\n        return out","c0041300":"def ce_loss(\n    pred, truth, trg_pad_idx=-1, eps=0.1\n):\n    '''pred = np.argmax(pred, axis = 1)\n    print(pred.shape)'''\n    \n    truth = truth.contiguous().view(-1)\n\n    one_hot = torch.zeros_like(pred).scatter(1, truth.view(-1, 1), 1)\n\n    if smoothing:\n        n_class = pred.size(1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps \/ (n_class - 1)\n\n    loss = -one_hot * F.log_softmax(pred, dim=1)\n\n    if trg_pad_idx >= 0:\n        loss = loss.sum(dim=1)\n        non_pad_mask = truth.ne(trg_pad_idx)\n        loss = loss.masked_select(non_pad_mask)\n\n    return loss.sum()\n\n\ndef loss_fn(output, target):\n    \n    \n    #bs = output.size(0)\n\n    loss = ce_loss(\n        output,\n        target,\n        eps=0.1,\n    )\n\n\n    return loss ","1961ad8a":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n\n    #tk0 = tqdm(data_loader, total=len(data_loader))\n    \n    for i, (ids, attention, target) in enumerate(data_loader):\n\n        ids =ids\n        attention = attention\n        target = target\n\n        ids = ids.to(device, dtype=torch.long)\n        attention = attention.to(device, dtype=torch.long)\n        target = target.to(device, dtype=torch.long)\n\n        #print(ids.shape, token_type_ids.shape, mask.shape)\n        model.zero_grad()\n        outputs  = model(\n            input_ids=ids,\n            attention_mask=attention,\n        )\n        print(outputs,  target)\n        loss = loss_fn(outputs ,target)\n        print(loss)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        \n","5e9ff403":"def eval_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.eval()\n\n    final_targets = []\n    final_outputs = []\n    #tk0 = tqdm(data_loader, total=len(data_loader))\n    with torch.no_grad():\n        for i, (ids, attention, target) in enumerate(data_loader):\n\n            ids = ids\n            attention = attention\n            target = target\n\n            ids = ids.to(device, dtype=torch.long)\n            attention = attention.to(device, dtype=torch.long)\n            target = target.to(device, dtype=torch.long)\n\n            #print(ids.shape, token_type_ids.shape, mask.shape)\n            outputs  = model(\n                input_ids=ids,\n                attention_mask=attention,\n            )\n            \n            target = target.detach().cpu().numpy().to_list()\n            output = output.detach().cpu().numpy().to_list()\n            \n            final_targets.extend(target)\n            final_outputs.extend(output)\n            \n    return final_outputs, final_targets\n        \n","03b4e193":"train_df = train_df.reset_index(drop = True)\n","edd1bcfe":"train_df","18c49200":"#create fold\nkf = model_selection.GroupKFold(n_splits = 5)\ndf['kfold'] = -1\ndf = df.sample(frac = 1).reset_index(drop = True)\ny = df.BROWSE_NODE_ID.values\nfor f, (t_, v_) in enumerate(kf.split(X = df, y = y, groups = df.TITLE.values)):\n    df.loc[v_,'kfold'] = f","8b666340":"def create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:197]    \n    attention_parameters = named_parameters[199:203]\n    regressor_parameters = named_parameters[203:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = 2e-5\n\n        if layer_num >= 69:        \n            lr = 5e-5\n\n        if layer_num >= 133:\n            lr = 1e-4\n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return AdamW(parameters)","14c27ed7":"df.to_csv(\"new.csv\", index = False)","86703b8b":"def run(fold):\n    dfx = pd.read_csv(\".\/new.csv\")\n\n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = LitDataset(df_train)\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size= 16,\n        num_workers=4,\n        shuffle = True\n    )\n\n    valid_dataset = LitDataset(df_valid)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size= 1,\n        num_workers = 2\n    )\n\n    device = torch.device(\"cuda\")\n    model = LitModel()\n    model.to(device)\n\n    num_train_steps = int(len(df_train) \/ 16 * 3)\n \n    optimizer = create_optimizer(model)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps= 50, \n        num_training_steps=num_train_steps\n    )\n\n    print(f\"Training is Starting for fold={fold}\")\n    \n    # I'm training only for 3 epochs even though I specified 5!!!\n    for epoch in range(3):\n        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n        pred, target = eval_fn(valid_data_loader, model, device)\n        aucc = metrics.accuracy(pred, target)\n        print(f\"epochs {epoch} , validation accuracy is {aucc}\")\n        ","edfa04c1":"run(fold = 0)","3c5c7f3c":"# len(train_set[\"BROWSE_NODE_ID\"].unique())"}}