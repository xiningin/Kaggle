{"cell_type":{"259266bc":"code","e0a6cbcd":"code","4d1e8942":"code","6edc3ccc":"code","e477f621":"code","2b66dfd2":"code","13327a79":"code","a5e6a284":"code","f1d422a5":"code","af6187c7":"code","ba5bde16":"code","1dde5e5e":"code","5360cce0":"code","430bc656":"code","f67cd684":"code","24d37927":"code","2889176a":"code","7b518005":"code","bc28e218":"code","6c7b6181":"code","18d49f9d":"code","4871ce47":"code","7c50732a":"code","d576d430":"code","cba4ad65":"code","f6463a8a":"code","02ce1b1a":"code","c795e9f7":"code","a9e32a63":"code","07796000":"code","b86360c6":"code","b0c29510":"code","29fc6e4a":"code","5934f168":"code","bbcd0653":"code","103f934c":"code","221cc435":"code","54ce3229":"code","18b1a7c1":"code","071ed58a":"code","2d925cc8":"code","ff0cc2dd":"code","dde978db":"code","2b9b3231":"code","174e5249":"code","2506854f":"code","76deea99":"code","8e6e31be":"code","298ea9b7":"code","9f543abe":"code","f676ce98":"code","dcf2bffb":"code","279b1a78":"code","0ece0b57":"code","d532918f":"code","e72535bd":"code","27372d52":"code","40018641":"code","d5142f98":"code","dd9e6a58":"code","7c04de97":"code","6cc21ba0":"code","77742c4d":"code","39eb6980":"code","22109ec6":"code","06f4ca41":"code","e45757a5":"code","5b27194e":"code","c1e73c7c":"code","7062ee52":"code","961ab852":"code","824d24c4":"code","76f246ed":"code","5bd0aed8":"code","00c0648f":"code","300e9ee9":"code","91c62269":"code","eda0355e":"code","6c037855":"code","92518c71":"code","4d830c0e":"code","decafa28":"code","7445ddc3":"markdown","0daaf981":"markdown","eb69c7bf":"markdown","3e95a33e":"markdown","f74625f6":"markdown","c422640e":"markdown","44342663":"markdown","71950525":"markdown","737accc7":"markdown","8e0fb6c7":"markdown","b4c2100d":"markdown","65bdd36e":"markdown","6722b7ff":"markdown","82ae933f":"markdown","07637d5c":"markdown","df06158a":"markdown","cd722686":"markdown","f6ff0500":"markdown","c824eb5a":"markdown","865c1f39":"markdown","c4fadc1a":"markdown","6b54e7d4":"markdown","f4f27f0d":"markdown","2ebb06fa":"markdown","149a3e41":"markdown","d2977783":"markdown","b9520927":"markdown","e1de5007":"markdown","2f77ba54":"markdown","2fb8b5a8":"markdown","a805349f":"markdown","398d721e":"markdown","104637ad":"markdown","f443ccf3":"markdown","59b1ef17":"markdown","a55f13d6":"markdown","fb1f975c":"markdown","947ab0c3":"markdown","0894cb10":"markdown","cf9b0576":"markdown","953e7032":"markdown","aacf41bd":"markdown","825ee3cd":"markdown","f1e96820":"markdown","d69d0458":"markdown","451d2017":"markdown","29c8d876":"markdown","f12eddbe":"markdown","6e171ad4":"markdown","004902ea":"markdown","c9577cea":"markdown","24b14e5b":"markdown","6754d412":"markdown","4e90a672":"markdown","424f1c3e":"markdown","2199dacc":"markdown","125f7876":"markdown","a51bccc0":"markdown","33b7887c":"markdown","b415701d":"markdown","0f6e3c3e":"markdown","d4bc01c2":"markdown","7fa9f6a2":"markdown","a0dacfc6":"markdown","e7298088":"markdown","5efdbe0d":"markdown","dd1b7a91":"markdown","c1381358":"markdown","29d0f6e5":"markdown","c40a1f8e":"markdown"},"source":{"259266bc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","e0a6cbcd":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","4d1e8942":"train_data.head()","6edc3ccc":"test_data.head()","e477f621":"train_data.describe()","2b66dfd2":"train_data.columns","13327a79":"train_data.dtypes","a5e6a284":"column_names = train_data.columns\nfor column in column_names:\n    print(column + ' --> ' + str(train_data[column].isnull().sum()))","f1d422a5":"train_data.Survived.value_counts()","af6187c7":"plt = train_data.Survived.value_counts().plot('bar')\nplt.set_xlabel('Survived or Not')\nplt.set_ylabel('Passenger Count')","ba5bde16":"plt = train_data.Pclass.value_counts().sort_index().plot('bar')\nplt.set_xlabel('Pclass')\nplt.set_ylabel('Passenger Count')","1dde5e5e":"train_data[['Pclass', 'Survived']].groupby('Pclass').count()","5360cce0":"train_data[['Pclass', 'Survived']].groupby('Pclass').sum()","430bc656":"plt = train_data[['Pclass', 'Survived']].groupby('Pclass').mean().Survived.plot('bar')\nplt.set_xlabel('Pclass')\nplt.set_ylabel('Survival Probability')","f67cd684":"plt = train_data.Sex.value_counts().sort_index().plot('bar')\nplt.set_xlabel('Sex')\nplt.set_ylabel('Passenger Count')","24d37927":"plt = train_data[['Sex', 'Survived']].groupby('Sex').mean().Survived.plot('bar')\nplt.set_xlabel('Sex')\nplt.set_ylabel('Survival Probability')","2889176a":"plt = train_data.Embarked.value_counts().sort_index().plot('bar')\nplt.set_xlabel('Embarked')\nplt.set_ylabel('Passenger Count')","7b518005":"plt = train_data[['Embarked', 'Survived']].groupby('Embarked').mean().Survived.plot('bar')\nplt.set_xlabel('Embarked')\nplt.set_ylabel('Survival Probability')","bc28e218":"plt = train_data.SibSp.value_counts().sort_index().plot('bar')\nplt.set_xlabel('Sibling\/Spouse')\nplt.set_ylabel('Passenger Count')","6c7b6181":"plt = train_data[['SibSp', 'Survived']].groupby('SibSp').mean().Survived.plot('bar')\nplt.set_xlabel('Sibling\/Spouse')\nplt.set_ylabel('Survival Probability')","18d49f9d":"plt = train_data.Parch.value_counts().sort_index().plot('bar')\nplt.set_xlabel('Parent\/Children')\nplt.set_ylabel('Passenger Count')","4871ce47":"plt = train_data[['Parch', 'Survived']].groupby('Parch').mean().Survived.plot('bar')\nplt.set_xlabel('Parent\/Children')\nplt.set_ylabel('Survival Probability')","7c50732a":"sns.catplot('Pclass', col = 'Embarked', data = train_data, kind = 'count')","d576d430":"sns.catplot('Sex', col = 'Pclass', data = train_data, kind = 'count')","cba4ad65":"sns.catplot('Sex', col = 'Embarked', data = train_data, kind = 'count')","f6463a8a":"train_data.head()","02ce1b1a":"train_data['Family_Size'] = train_data['SibSp'] + train_data['Parch'] + 1\ntrain_data.head()","c795e9f7":"train_data = train_data.drop(columns = ['PassengerId', 'Ticket', 'Cabin'])\ntrain_data.head()","a9e32a63":"train_data['Sex'] = train_data['Sex'].map({'male':0, 'female':1})\ntrain_data['Embarked'] = train_data['Embarked'].map({'C':0, 'Q':1, 'S':2})\ntrain_data.head()","07796000":"train_data['Title'] = train_data.Name.str.extract('([A-Za-z]+)\\.', expand = False)\ntrain_data = train_data.drop(columns = 'Name')\ntrain_data.Title.value_counts().plot('bar')","b86360c6":"train_data['Title'] = train_data['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Countess', 'Sir', 'Jonkheer',\n                                                   'Capt', 'Lady', 'Don'], 'Others')\ntrain_data['Title'] = train_data['Title'].replace(['Ms', 'Mlle'], 'Miss')\ntrain_data['Title'] = train_data['Title'].replace('Mme', 'Mrs')\n\n\nplt = train_data.Title.value_counts().sort_index().plot('bar')\nplt.set_xlabel('Title')\nplt.set_ylabel('Passenger Count')","b0c29510":"plt = train_data[['Title', 'Survived']].groupby('Title').mean().Survived.plot('bar')\nplt.set_xlabel('Title')\nplt.set_ylabel('Survival Probability')","29fc6e4a":"train_data['Title'] = train_data['Title'].map({'Master':0, 'Miss':1, 'Mr':2, 'Mrs':3, 'Others':4})\ntrain_data.head()","5934f168":"corr_matrix = train_data.corr()\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (9, 8))\nsns.heatmap(data = corr_matrix, cmap='BrBG', annot = True, linewidths = 0.2)","bbcd0653":"train_data.isnull().sum()","103f934c":"train_data['Embarked'].isnull().sum()","221cc435":"train_data['Embarked'] = train_data['Embarked'].fillna(2)","54ce3229":"train_data['Embarked'].isnull().sum()","18b1a7c1":"corr_matrix = train_data[['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']].corr()\n\nplt.figure(figsize=(7, 6))\nsns.heatmap(data = corr_matrix,cmap='BrBG', annot=True, linewidths=0.2)","071ed58a":"Nan_indexes = train_data['Age'][train_data['Age'].isnull()].index\n\nfor i in Nan_indexes:\n    pred_age = train_data['Age'][((train_data.SibSp == train_data.iloc[i][\"SibSp\"])\n                                  & (train_data.Parch == train_data.iloc[i][\"Parch\"])\n                                  & (train_data.Pclass == train_data.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(pred_age):\n        train_data['Age'].iloc[i] = pred_age\n    else:\n        train_data['Age'].iloc[i] = train_data['Age'].median()","2d925cc8":"train_data.isnull().sum()","ff0cc2dd":"test_data.head()","dde978db":"test_data.isnull().sum()","2b9b3231":"test_data = test_data.drop(columns = ['Ticket', 'PassengerId', 'Cabin'])\ntest_data.head()","174e5249":"test_data['Sex'] = test_data['Sex'].map({'male':0, 'female':1})\ntest_data['Embarked'] = test_data['Embarked'].map({'C':0, 'Q':1, 'S':2})\ntest_data.head()","2506854f":"test_data['Title'] = test_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest_data = test_data.drop(columns='Name')\n\ntest_data['Title'] = test_data['Title'].replace(['Dr', 'Rev', 'Col', 'Major', 'Countess', 'Sir', 'Jonkheer', 'Lady', 'Capt', 'Don'], 'Others')\ntest_data['Title'] = test_data['Title'].replace('Ms', 'Miss')\ntest_data['Title'] = test_data['Title'].replace('Mme', 'Mrs')\ntest_data['Title'] = test_data['Title'].replace('Mlle', 'Miss')\n\ntest_data['Title'] = test_data['Title'].map({'Master':0, 'Miss':1, 'Mr':2, 'Mrs':3, 'Others':4})\ntest_data.head()","76deea99":"test_data.isnull().sum()","8e6e31be":"NaN_indexes = test_data['Age'][test_data['Age'].isnull()].index\n\nfor i in NaN_indexes:\n    pred_age = train_data['Age'][((train_data.SibSp == test_data.iloc[i][\"SibSp\"]) & (train_data.Parch == test_data.iloc[i][\"Parch\"]) & (test_data.Pclass == train_data.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(pred_age):\n        test_data['Age'].iloc[i] = pred_age\n    else:\n        test_data['Age'].iloc[i] = train_data['Age'].median()","298ea9b7":"title_mode = train_data.Title.mode()[0]\ntest_data.Title = test_data.Title.fillna(title_mode)","9f543abe":"fare_mean = train_data.Fare.mean()\ntest_data.Fare = test_data.Fare.fillna(fare_mean)","f676ce98":"test_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\ntest_data.head()","dcf2bffb":"test_data.isnull().sum()","279b1a78":"train_data.head()","0ece0b57":"X_train = train_data.drop(columns = 'Survived')\ny_train = train_data.Survived\ny_train = pd.DataFrame({'Survived': y_train.values})\nX_test = test_data","d532918f":"X_train.head()","e72535bd":"y_train.head()","27372d52":"X_train.shape","40018641":"y_train.shape","d5142f98":"X_test.head()","dd9e6a58":"X_test.shape","7c04de97":"from sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\ntest_data = pd.read_csv('..\/input\/test.csv')","6cc21ba0":"clf = LogisticRegression()\nclf.fit(X_train, np.ravel(y_train))\n\ny_pred_log_reg = clf.predict(X_test)\ny_pred_log_reg = pd.DataFrame(y_pred_log_reg)\ny_pred_log_reg.columns = ['Survived']\nlog_reg_pred = pd.DataFrame()\nlog_reg_pred['PassengerId'] = test_data.PassengerId\nlog_reg_pred['Survived'] = y_pred_log_reg.Survived\nlog_reg_pred.head()","77742c4d":"log_reg_pred.to_csv('Logistic_regression_prediction.csv', index = False)\n\nacc_log_reg = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy on train data: %i %% \\n\"%acc_log_reg)\n\nprint(\"Accuracy on test data: 72.727 %\")","39eb6980":"clf = SVC()\nclf.fit(X_train, np.ravel(y_train))\n\ny_pred_svc = clf.predict(X_test)\ny_pred_svc = pd.DataFrame(y_pred_svc)\ny_pred_svc.columns = ['Survived']\nsvm_pred = pd.DataFrame()\nsvm_pred['PassengerId'] = test_data.PassengerId\nsvm_pred['Survived'] = y_pred_svc.Survived\nsvm_pred.head()","22109ec6":"svm_pred.to_csv('svm_prediction.csv', index = False)\n\nacc_svm = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy on train data: %i %% \\n\"%acc_svm)\n\nprint('Accuracy on test data: 62.2 %')","06f4ca41":"clf = LinearSVC()\nclf.fit(X_train, np.ravel(y_train))\ny_pred_linear_svc = clf.predict(X_test)\ny_pred_linear_svc = pd.DataFrame(y_pred_linear_svc)\ny_pred_linear_svc.columns = ['Survived']\nlinear_svm_pred = pd.DataFrame()\nlinear_svm_pred['PassengerId'] = test_data.PassengerId\nlinear_svm_pred['Survived'] = y_pred_linear_svc.Survived\nlinear_svm_pred.head()","e45757a5":"linear_svm_pred.to_csv('linear_svm_prediction.csv', index = False)\n\nacc_linear_svm = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy on train data: %i %% \\n\"%acc_linear_svm)\n\nprint('Accuracy on test data: 45.454%')","5b27194e":"sgd = linear_model.SGDClassifier()\nsgd.fit(X_train, np.ravel(y_train))\n\ny_pred = sgd.predict(X_test)\ny_pred = pd.DataFrame(y_pred)\ny_pred.columns = ['Survived']\nsgd_pred = pd.DataFrame()\nsgd_pred['PassengerId'] = test_data.PassengerId\nsgd_pred['Survived'] = y_pred.Survived\n\nsgd_pred.head()","c1e73c7c":"sgd_pred.to_csv('sgd_prediction.csv', index = False)\n\nacc_sgd = round(sgd.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy on train data: %i %% \\n\"%acc_sgd)\n\nprint('Accuracy on test data: 65.55%')","7062ee52":"clf = RandomForestClassifier()\nclf.fit(X_train, np.ravel(y_train))\n\ny_pred_randomforest = clf.predict(X_test)\ny_pred_randomforest = pd.DataFrame(y_pred_randomforest)\ny_pred_randomforest.columns = ['Survived']\nrandomforest_pred = pd.DataFrame()\nrandomforest_pred['PassengerId'] = test_data.PassengerId\nrandomforest_pred['Survived'] = y_pred_randomforest.Survived\n\nrandomforest_pred.head()","961ab852":"randomforest_pred.to_csv('RandomForest_prediction.csv', index = False)\n\nacc_randomforest = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy on train data: %i %% \\n\"%acc_randomforest)\n\nprint('Accuracy on test data: 53.11%')","824d24c4":"clf = KNeighborsClassifier()\nclf.fit(X_train, np.ravel(y_train))\n\ny_pred_knn = clf.predict(X_test)\ny_pred_knn = pd.DataFrame(y_pred_knn)\ny_pred_knn.columns = ['Survived']\nknn_pred = pd.DataFrame()\nknn_pred['PassengerId'] = test_data.PassengerId\nknn_pred['Survived'] = y_pred_knn.Survived\nknn_pred.head()","76f246ed":"knn_pred.to_csv('knn_prediction.csv', index = False)\n\nacc_knn = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy on train data: %i %% \\n\"%acc_knn)\n\nprint('Accuracy on test data: 60.765%')","5bd0aed8":"clf = DecisionTreeClassifier()\nclf.fit(X_train, np.ravel(y_train))\n\ny_pred_decision_tree = clf.predict(X_test)\ny_pred_decision_tree = pd.DataFrame(y_pred_decision_tree)\ny_pred_decision_tree.columns = ['Survived']\ndecision_tree_pred = pd.DataFrame()\ndecision_tree_pred['PassengerId'] = test_data.PassengerId\ndecision_tree_pred['Survived'] = y_pred_decision_tree.Survived\n\ndecision_tree_pred.head()","00c0648f":"decision_tree_pred.to_csv('decision_tree_prediction.csv', index = False)\n\nacc_decision_tree = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy on train data: %i %% \\n\"%acc_decision_tree)\n\nprint('Accuracy on test data: 44.497 %')","300e9ee9":"clf = GaussianNB()\nclf.fit(X_train, np.ravel(y_train))\n\ny_pred_gnb = clf.predict(X_test)\ny_pred_gnb = pd.DataFrame(y_pred_gnb)\ny_pred_gnb.columns = ['Survived']\ngnb_pred = pd.DataFrame()\ngnb_pred['PassengerId'] = test_data.PassengerId\ngnb_pred['Survived'] = y_pred_gnb.Survived\ngnb_pred.head()","91c62269":"gnb_pred.to_csv('GaussianNB_prediction.csv', index = False)\n\nacc_gnb = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy on train data: %i %% \\n\"%acc_gnb)\n\nprint('Accuracy on test data: 71.291%')","eda0355e":"clf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\ny_pred_decision_tree_training_set = clf.predict(X_train)\nacc_decision_tree = round(clf.score(X_train, y_train) * 100, 2)\nprint (\"Accuracy on train data: %i %% \\n\"%acc_decision_tree)\n\nclass_names = ['Survived', 'Not Survived']\n\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_train, y_pred_decision_tree_training_set)\nnp.set_printoptions(precision=2)\n\nprint ('Confusion Matrix in Numbers')\nprint (cnf_matrix)\nprint ('')\n\ncnf_matrix_percent = cnf_matrix.astype('float') \/ cnf_matrix.sum(axis=1)[:, np.newaxis]\n\nprint ('Confusion Matrix in Percentage')\nprint (cnf_matrix_percent)\nprint ('')\n\ntrue_class_names = ['True Survived', 'True Not Survived']\npredicted_class_names = ['Predicted Survived', 'Predicted Not Survived']\n\ndf_cnf_matrix = pd.DataFrame(cnf_matrix, \n                             index = true_class_names,\n                             columns = predicted_class_names)\n\ndf_cnf_matrix_percent = pd.DataFrame(cnf_matrix_percent, \n                                     index = true_class_names,\n                                     columns = predicted_class_names)\n\nplt.figure(figsize = (15,5))\n\nplt.subplot(121)\nsns.heatmap(df_cnf_matrix, annot=True, fmt='d')\n\nplt.subplot(122)\nsns.heatmap(df_cnf_matrix_percent, annot=True)\nplt.xkcd()","6c037855":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines', 'Linear SVC', \n              'KNN', 'Decision Tree', 'Random Forest', 'Naive Bayes', 'Stochastic Gradient Decent'],\n    \n    'Train_Score': [acc_log_reg, acc_svm, acc_linear_svm, \n              acc_knn,  acc_decision_tree, acc_randomforest, acc_gnb, acc_sgd],\n    \n    'Test_Score': [ 72.727, 62.2, 45.454, \n              60.765,  44.497, 53.11, 71.291, 65.55]\n    })","92518c71":"models.sort_values(by='Train_Score', ascending=False)","4d830c0e":"models.sort_values(by='Test_Score', ascending=False)","decafa28":"plt.plot(models['Model'], models['Train_Score'], color='g', label = 'Train Accuracy Score')\nplt.plot(models['Model'], models['Test_Score'], color='orange', label = 'Test Accuracy Score')\nplt.xlabel('Models')\nplt.ylabel('Accuracy Score')\nplt.axis()\nplt.title('Titanic ML Model Performance')\nplt.legend(loc = 'center left', bbox_to_anchor=(1, 0.5))\nplt.xticks(rotation=90)\nplt.show()","7445ddc3":"## Preprocessing Data","0daaf981":"#### Sex","eb69c7bf":"Impute 'Embarked' with its majority class","3e95a33e":"## Now we will train several Machine Learning models and compare their results.","f74625f6":"There are no very highly correlated columns.\n\n#### Handling Missing Values","c422640e":"## Preprocessed Data","44342663":"###  Confusion Matrix\nA confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm. Each row of the matrix represents the instances in a predicted class while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabelling one as another).\n\nIn predictive analytics, a table of confusion (sometimes also called a confusion matrix), is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. This allows more detailed analysis than mere proportion of correct classifications (accuracy). Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the numbers of observations in different classes vary greatly). For example, if there were 95 cats and only 5 dogs in the data set, a particular classifier might classify all the observations as cats. The overall accuracy would be 95%, but in more detail the classifier would have a 100% recognition rate for the cat class but a 0% recognition rate for the dog class.","71950525":"As we can see, majority of them have no Parent\/Children","737accc7":"There are two null values in the column 'Embarked'. Let's impute them using majority class. The majority class is 'S'. Impute the unkonown values (NaN) using 'S'","8e0fb6c7":"#### Load Dataset","b4c2100d":"Create a new feature 'FamilySize' from 'SibSp' and 'Parch'","65bdd36e":"#### Pclass vs Sex","6722b7ff":"## 6. K-Nearest Neighbours\nk -nearest neighbors algorithm (k-NN) is one of the simplest machine learning algorithms and is used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k -NN is used for classification or regression:\n\nIn k -NN classification, the output is a class membership. An object is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors ( k is a positive integer, typically small). If k=1 , then the object is simply assigned to the class of that single nearest neighbor.\n\nIn k -NN regression, the output is the property value for the object. This value is the average of the values of its k nearest neighbors.","82ae933f":"## 5. Random Forest","07637d5c":"## Preprocess Test Data","df06158a":"### Create a new feature 'Family Size' from the features 'SibSp' and 'Parch'","cd722686":"*  Age is not correlated with 'Sex' and 'Fare'. So, we don't consider these two columns while imputing 'Sex'.\n*  'Pclass', 'SibSp' and 'Parch' are negatively correlated with 'Sex'.\n\nLet's fill Age with the median age of similar rows from 'Pclass', 'SibSp' and 'Parch'. If there are no similar rows, fill the age with the median age of total dataset.","f6ff0500":"#### Import Libraries","c824eb5a":"It's time to use this preprocessed data and apply different modelling algorithms.","865c1f39":"Most of them are from 3rd class","c4fadc1a":"#### Goal:\nIt is your job to predict if a passenger survived the sinking of the Titanic or not. For each PassengerId in the test set, you must predict a 0 or 1 value for the Survived variable.","6b54e7d4":"Majority of them are Male\n\n#### Sex- Survival Probability","f4f27f0d":"#### Parch - Survival Probability","2ebb06fa":"There are no missing values in the data.","149a3e41":"The passenger having 3 Parent\/Children have more Survival Probability\n\n3 > 1 > 2 > 0 > 5\n\n#### Embarked vs Pclass","d2977783":"As we can see, most of them have no sibling\/spouse\n\n#### SibSp - Survival Probability","b9520927":"#### Embarked - Survival Probability","e1de5007":"The survival probabilty of 'Mrs' and 'Miss' are more than other classes.\n\n#### Map Title to Numerical Values","2f77ba54":"### Visualization of Survived (Target Column)\nAs we know, majority of passenger couldn't survive.\n\nData is imbalance","2fb8b5a8":"As we see, the survival probabilty of female is more. They might have given more priority to female than male.\n\n#### Embarked","a805349f":"## 7. Decision Tree\nA decision tree is a flowchart-like structure in which each internal node represents a \"test\" on an attribute (e.g. whether a coin flip comes up heads or tails), each branch represents the outcome of the test, and each leaf node represents a class label (decision taken after computing all attributes). The paths from root to leaf represent classification rules.","398d721e":"#### Pclass\n","104637ad":"Impute 'Fare' with it's mean","f443ccf3":"## 3. Linear SVM\nLinear SVM is a SVM model with linear kernel.\n\nIn the below code, LinearSVC stands for Linear Support Vector Classification.","59b1ef17":"Remove unnecessary columns\n*  We can remove 'Ticket' and 'PassengerId', as they don't contribute to target class.\n*  Remove 'Cabin' as it has a lot of missing values in both train and test data","a55f13d6":"Drop 'Ticket', 'PassengerId' and 'Cabin' columns","fb1f975c":"## Model Accuracy Visualization","947ab0c3":"Impute 'Age' using median of columns 'SibSp', 'Parch' and 'Pclass'","0894cb10":"#### Train Data","cf9b0576":"## Ranking based on performance on training Data","953e7032":"#### Title - Survival Probability","aacf41bd":"### Correlation between Columns","825ee3cd":"#### Number of missing values","f1e96820":"From the above results, we can say that, 1st class has higher chance of surviving than other two classes.","d69d0458":"#### Pclass - Survival Probability","451d2017":"Majority of passenger is male in every class. But the survival probability for Female is High.\n\n#### Embarked vs Sex","29c8d876":"## 8. Gaussian Naive Bayes\nNaive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\n\nBayes' theorem (alternatively Bayes' law or Bayes' rule) describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using Bayes' theorem, a person's age can be used to more accurately assess the probability that they have cancer, compared to the assessment of the probability of cancer made without knowledge of the person's age.\n\nNaive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. It is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 10 cm in diameter. A naive Bayes classifier considers each of these features to contribute independently to the probability that this fruit is an apple, regardless of any possible correlations between the color, roundness, and diameter features.","f12eddbe":"#### Preprocess Name\n*  Extarct title from name of the passenger and categorize them.\n*  Drop the column 'Name'","6e171ad4":"Impute 'Title' with it's mode","004902ea":"# Titanic: Machine Learning from Disaster","c9577cea":"Most of them are from Southampton(S).","24b14e5b":"## 1. Logistic Regression\nLogistic regression, or logit regression, or logit model is a regression model where the dependent variable (DV) is categorical. This article covers the case of a binary dependent variable\u2014that is, where it can take only two values, \"0\" and \"1\", which represent outcomes such as pass\/fail, win\/lose, alive\/dead or healthy\/sick. Cases where the dependent variable has more than two outcome categories may be analysed in multinomial logistic regression, or, if the multiple categories are ordered, in ordinal logistic regression.","6754d412":"Number of missing values","4e90a672":"#### Test Data","424f1c3e":"## Visualize and Preprocess Train Data","2199dacc":"## 4. Stochastic Gradient Descent (SGD)\nStochastic gradient descent (often shortened to SGD), also known as incremental gradient descent, is an iterative method for optimizing a differentiable objective function, a stochastic approximation of gradient descent optimization. A recent article implicitly credits Herbert Robbins and Sutton Monro for developing SGD in their 1951 article titled \"A Stochastic Approximation Method\";\n\nsee Stochastic approximation for more information. It is called stochastic because samples are selected randomly (or shuffled) instead of as a single group (as in standard gradient descent) or in the order they appear in the training set.","125f7876":"The passenger having 1 sibling\/spouse has more survival probability\n\n1 > 2 > 0 > 3 > 4\n\n#### Parch - Parent\/Children","a51bccc0":"The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\n![](https:\/\/www.google.com\/url?sa=i&source=images&cd=&cad=rja&uact=8&ved=2ahUKEwjA_Niay7rfAhUROSsKHWEtDL4QjRx6BAgBEAU&url=http%3A%2F%2Fwww.usclimateplan.org%2F11741%2Ftitanic-wallpapers-11-11-2018%2Ftitanic-ship-hd-wallpaper-attractive-wallpapers-staggering-0%2F&psig=AOvVaw2NXBuAD4wafLBShgGH348q&ust=1545813776636577)","33b7887c":"## 2. SVM\nSupport Vector Machine (SVM) model is a Supervised Learning model used for classification and regression analysis. It is a representation of the examples as points in space, mapped so that the examples of the separate categories are divided by a clear gap that is as wide as possible. New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.\n\nIn addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the kernel trick, implicitly mapping their inputs into high-dimensional feature spaces. Suppose some given data points each belong to one of two classes, and the goal is to decide which class a new data point will be in. In the case of support vector machines, a data point is viewed as a p -dimensional vector (a list of p numbers), and we want to know whether we can separate such points with a (p\u22121) -dimensional hyperplane.\n\nWhen data are not labeled, supervised learning is not possible, and an unsupervised learning approach is required, which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. The clustering algorithm which provides an improvement to the support vector machines is called support vector clustering and is often used in industrial applications either when data are not labeled or when only some data are labeled as a preprocessing for a classification pass.\n\nIn the below code, SVC stands for Support Vector Classification.","b415701d":"#### Missing values - Age\nLet's find the columns that are useful to predict the value of Age.","0f6e3c3e":"Combine some of the classes and group all the rare classes into 'Others'.","d4bc01c2":"Extract 'Title' from 'Name' and convert to Numerical values.","7fa9f6a2":"Survival Probability --> C > Q > S\n\n#### SibSp - Sibling\/Spouse","a0dacfc6":"## Ranking based on performance on test Data\nEach file has been submitted to Kaggle and then the data has been recorded.","e7298088":"The columns 'Age' and 'Cabin' contains more NULL values\n\n### Insights\n*  'Survived' is the target column\n*  'PassengerId', 'Name' and 'Ticket' doesn't contribute to the target value 'Survived'. So, we can remove it from the data.\n*  'Age' and 'Embarked' has less number of missing value. We have to impute them using different techniques. \n*  As there is lot of missing value in 'Cabin', we can remove it from the data.\n*  'Pclass', 'Sex', 'SibSp', 'Parch', 'Fare' doesn't have any missing values.\n*  We can also create new variable like 'total size of the family' from the columns 'SibSp' and 'Parch'.\n","5efdbe0d":"#### Map 'Sex' and 'Embarked' to Numerical Values","dd1b7a91":"Convert 'Sex' and 'Embarked' to Numerical values","c1381358":"#### Import Libraries","29d0f6e5":"# Compare Models\n\nLet compare the accuracy score of all the classifier models used above.","c40a1f8e":"Practice Skills:\n*      Binary classification\n*      Preprocess Data\n*     \nPython basics"}}