{"cell_type":{"6d21a429":"code","ec908ffe":"code","79cf35dc":"code","ae798c03":"code","76973b9e":"code","abb228e6":"code","0fdd851c":"code","966e4d56":"code","7438e235":"code","32212d72":"code","08be42b2":"code","c0940b62":"code","40f10caf":"code","f568603a":"code","732649e4":"code","ae3082ce":"code","0366eabd":"code","da281501":"code","240b2dcf":"code","f9a65b3a":"code","9d57484d":"code","ada6f287":"code","eb50624f":"code","50fe8e14":"code","49227db4":"code","f919a28a":"code","768478a5":"code","c424516b":"code","0d9cdb6b":"code","1050066b":"code","80cebf24":"code","74359640":"code","01e4d6e1":"markdown","04c98466":"markdown","54a73614":"markdown","da85b8a8":"markdown","e7c61bd4":"markdown","1beedaca":"markdown","e932dcef":"markdown","a7a196f3":"markdown","6bd8f290":"markdown","d9d03514":"markdown","4b26fd8f":"markdown","6a833471":"markdown","c9bfad3b":"markdown","ca65dace":"markdown","9570c42c":"markdown","ae15d9b6":"markdown","7fab0d3e":"markdown","44b71bb6":"markdown","c368fdb4":"markdown","f98aad17":"markdown","d3e2a5af":"markdown","99efce3a":"markdown","9cd34d10":"markdown","c518f521":"markdown","88a9f1d1":"markdown","1729fba5":"markdown","830ea559":"markdown","b54c4595":"markdown","6ad68715":"markdown","3e88b611":"markdown","25bc49f6":"markdown","07c10b56":"markdown","be627bc6":"markdown","307918b2":"markdown","1806b03d":"markdown","e1bc2c0b":"markdown","a86d2880":"markdown","fafa9379":"markdown","e751ea07":"markdown","20905fac":"markdown","2b4c460d":"markdown","ef374b37":"markdown","26ae24f3":"markdown","bc73a543":"markdown","2972b749":"markdown","9c5a21ca":"markdown","56b08af0":"markdown","298c3596":"markdown","091a600d":"markdown","6bda5c73":"markdown","e3acdb27":"markdown"},"source":{"6d21a429":"import pandas as pd","ec908ffe":"df = pd.read_csv(\"..\/input\/train.csv\")","79cf35dc":"df.head()","ae798c03":"df.head(10)","76973b9e":"df.tail(10)","abb228e6":"df1 = pd.read_csv(\"..\/input\/train.csv\", usecols= [\"PassengerId\", \"Survived\", \"Pclass\"])\ndf1.head() #and add this to see if our code worked","0fdd851c":"df.describe()","966e4d56":"df.info()","7438e235":"df.dtypes","32212d72":"df.sort_values(\"Fare\").head(10)","08be42b2":"df.sort_values(\"Fare\", ascending = False).head(10)","c0940b62":"df = df.sort_values(\"Fare\", ascending = False)","40f10caf":"df.sort_values(\"Fare\", ascending = False, inplace = True)","f568603a":"df.head() #lets check the new version of the data","732649e4":"df.sort_values(\"Cabin\", ascending = True, na_position ='last').head(10)","ae3082ce":"df.tail(10) #lets check if the nan values are at the bottom of our dataset","0366eabd":"df.Sex.head()\ndf[\"Sex\"].head()","da281501":"df.Sex.value_counts()","240b2dcf":"df.nunique()","f9a65b3a":"df[\"Embarked\"].nunique() #we can also specify one or more column name too","9d57484d":"df[[\"Embarked\" , \"Sex\"]].nunique() #by putting a comma between two different columns, we can see number of the unique records in both columns","ada6f287":"df[\"Embarked\"] = df[\"Embarked\"].astype(\"category\") #we are changing the data type of the Embarked column to category here\ndf[\"Embarked\"].dtype #we are checking to see if our code worked here","eb50624f":"df[\"Embarked\"] == \"C\"","50fe8e14":"df[df[\"Embarked\"] == \"C\"]","49227db4":"embarked_c_mask = df[\"Embarked\"] == \"C\"\ndf[embarked_c_mask]","f919a28a":"df_fare_mask = df[\"Fare\"] < 100\ndf_sex_mask = df[\"Sex\"] == \"female\"\ndf[df_fare_mask & df_sex_mask]","768478a5":"df_fare_mask2 = df[\"Fare\"] > 500\ndf_age_mask = df[\"Age\"] > 70\ndf[df_fare_mask2 | df_age_mask]","c424516b":"null_mask = df[\"Cabin\"].isnull() #With this code, we are saying that \u201cShow me the passengers whose cabin is unknown\u201d\ndf[null_mask]","0d9cdb6b":"df.isnull().sum()","1050066b":"df.drop(labels = [\"Cabin\"], axis=1).head()","80cebf24":"df.drop(labels = [\"Cabin\", \"Name\"], axis=1).head()","74359640":"df[\"Age\"].fillna(0, inplace = True) #with inplace argument, we don't have to write it as\ndf[\"Age\"] = df[\"Age\"].fillna(0, inplace = True) #fill the missing values with zero\ndf['Age'] = df['Age'].fillna((df['Age'].median())) #fill the missing values with the median of Age column","01e4d6e1":"**Sorting columns with strings**","04c98466":"* We can see that count of **Age** column is 714, **mean** is 29.6, **standard deviation** is 14.52 and so on. Thanks to count, we can understand that there are some missing values in this column. We will deal with them later.","54a73614":"To count the occurence of a variable, we have to select the column first. You can select a column with two different ways:","da85b8a8":"* This method is used to get a **summary** of **numeric values** in your dataset. It calculates the **mean, standard deviation, minimum value, maximum value, 1st percentile, 2nd percentile, 3rd percentile** of the columns with numeric values. It also counts the number of variables in the dataset. So, we will be able to see if there are missing values in columns.","e7c61bd4":"**OR operator**\n* Let\u2019s do another example with OR operator. We are going to use | sign to do that. Let\u2019s see the passengers whose fare is more than 500 or older than 70.","1beedaca":"> We used .drop method to drop Cabin column. There is a 2 argument above. In the labels argument, we have to specify the column names that we want to drop, in the axis argument, we specified that we drop it column-wise.\n\n* If you want to drop more than one column, all you have to do is add it in the square brackets. For example:","e932dcef":"Let\u2019s assume that you don\u2019t want all of the columns in your CSV file. You just want 3 of them and you want to get rid of them at the begining. How to do that? Good news! There is a really easy way to do that. We are going to use usecols argument to specify the column names that we want to work with. Let\u2019s work with just **PassengerId, Survived, Pclass** columns.","a7a196f3":"**Filling missing values with .fillna()**\n\n* To fill missing values in a dataframe, there is a method called **.fillna()**.\n* Let\u2019s assume that we have lots of missing values in a column and we want to fill them with 0. All we have to do is write the code below","6bd8f290":"**.info method**","d9d03514":"**Filtering under two or more condition**\n* AND operator\nWe are going to use AND and OR operator to filter with more than one condition. Let\u2019s assume that we want to see the passengers whose Fare is smaller than 100 and who are female. We are going to create 2 new masks to complete that.","4b26fd8f":"To sum up, these methods return the **top** and **bottom** of the dataframe. The default number of rows is set to 5. But, you can change it by writing number of rows that you want to see inside the parentheses.","6a833471":"**Changing the data type**","c9bfad3b":"* Knowing how many unique variables are there in a column, or the occurence of each item in a column might be very useful in some cases. Let\u2019s count the number of male and female passengers with **.value_counts()**","ca65dace":"* Since .value_counts() is a method, all we have to do is appending this method to the code above. It will look like this:","9570c42c":"**Checking first elements of the DataFrame with .head() method**","ae15d9b6":"**Counting the occurences of variables**","7fab0d3e":"* We are going to sort the \u201c**Cabin**\u201d column. There are lots of missing (NaN) values in this column. How can we deal with them? Thankfully, there is an argument which is called **na_position** which helps us to set a position for the NaN values in the dataset.","44b71bb6":"> Instead of using this method on a column, it can be used on whole dataset too. If we want to count of the null values of all columns in a dataframe, we just have to write code below","c368fdb4":"> Getting some information about dataset with **.describe()** and **.info()**","f98aad17":"* After we load our dataset with **read_csv**, we would like to get some information about the columns. To do that, we are going to use .describe() and .info()","d3e2a5af":"We are dropping cabin column and name column at the same time. As I mentioned before, If we knew that we won\u2019t use these columns, we would have usecols argument of .read_csv method to get rid of that columns at the beginning.","99efce3a":"Another way to do that might be:","9cd34d10":"Inside the parentheses, we can write the number of elements that we want to see. If we leave it blank, it will show the first five elements. If we write 10 inside of the parentheses, it will show the first 10 elements of the dataframe.","c518f521":"Let's start with importing Pandas","88a9f1d1":"**Getting started with Exploratory Data Analysis with Python Pandas**","1729fba5":"* What if we dont want to see just Trues and Falses? What if we want to see all information of those whose Embarked is C? To do that:","830ea559":"**Dealing with missing values**\n\n* There are lots of dealing ways with missing values but in this article, we are going to use \u201c**ignore the tuple**\u201d and \u201c**fill it with median**\u201d. We are going to ignore the \u201c**Cabin**\u201d column since %70 of that column is **missing**. And we are going to fill the missing Ages with **median** value of that column.\n\n**Dropping a column**\n* To drop the \u201cCabin\u201d column, we have to execute the code below.","b54c4595":"Importing a dataset with **read_csv**","6ad68715":"**Finding the null values with .isnull()**\n* One of the most common problems in data science is missing values. To detect them, there is a beautiful method which is called .isnull(). With this method, we can get a boolean series (True or False). As we did before, by masking the condition, we can extract the values which are null. For example","3e88b611":"We used **.head()** method to see only first 10 of the results after sorting. We can see that the lowest \u201cFare\u201d values are 0. What if we want to see the highest fare? All we have to do is use the **.tail()** method. No, just kidding. We have to set the ascending argument as False. But using .tail() is an alternative :P.","25bc49f6":"**Checking last elements of the DataFrame with .tail() method**","07c10b56":"The common shortcut of Pandas is **pd**. Instead of writing \u201cpandas.\u201d we can write \u201cpd.\u201d now. So, there is a dot after \u201cpd\u201d which is used to call a method from Pandas library.","be627bc6":"Thanks to that, we can see the highest fares paid by passengers. If we want to save the sorted version of the dataframe, there are two alternatives. One is the old way, which is","307918b2":"**Reading a CSV file with specific columns**","1806b03d":"* If we want to see number of unique records in a dataset or in a column, we have to use .nunique() method.","e1bc2c0b":"There is also a method to see the see last n number of elements. The method is called **.tail()**.\nThe same rule is also applied here. If we leave the parentheses blank, it will be set as 5, if we write 10 inside of the parentheses, it will show the last 10 elements of the dataframe.","a86d2880":"**Filtering**\n* **Filtering under one condition**\nThe comparison sign in Python is == (double equal sign). So you should double check whether you used 2 equal signs. If you use just one equal sign, you might ruin your data. Let\u2019s assume that I want to see if the \u201cEmbarked\u201d column is equal to \u201cC\u201d. The true version of the comparison is:","fafa9379":"* In this kernel, I will focus on **importing datasets**, dealing with **missing values**, **changing data types**, **filtering, sorting, selecting specific column**(s), dealing with **duplicate values**, **dropping and adding rows and columns**, **counting values**, **counting unique values**.**","e751ea07":"You can find this kernel on towardsdatascience too.\nhttps:\/\/towardsdatascience.com\/getting-started-to-data-analysis-with-python-pandas-with-titanic-dataset-a195ab043c77\n   \nThanks for your time. I am open to criticism. All comments are much appreciated. If you have any questions, do not hesitate to ask.","20905fac":"**SORTING THE DATA**","2b4c460d":"After you run the code above, nothing [](http:\/\/)will appear. So you have to write df to see your data. But instead of seeing all the data, we are going to use the \u201c.head()\u201d method to see the first five elements of the data. Before you run the read_csv code, you can write df.head() below. ","ef374b37":"**Summary of *.head()* and *.tail()***","26ae24f3":">   \n* To sum up, we learned to read csv files with **.read_csv** method(with and without selecting specific columns), used **.head()** and **.tail()** to see elements in the top and at the bottom, got information about dataset with **.describe()** and **.info()**, sorted columns which include string or numeric values (with and without NaN values)","bc73a543":"**.describe() method**","2972b749":"The other way is using the **inplace** argument. If we set this argument as True, it will write over it.","9c5a21ca":"**Using *.nunique()* to count number of unique values that occur in dataset or in a column**","56b08af0":"**Sorting Numeric Data**\n\nThere is a column in our dataframe which represents the price of the ticket that passengers bought. Let\u2019s assume that we want to see lowest ticket price. To do that, we have to use **.sort_values()** method. It doesn\u2019t matter if the column that you want to sort is string or numeric. If it includes letters, it will sort in alphabetical order.","298c3596":"This method (.info) prints information about a DataFrame including the **index dtype** and **column dtypes**, **non-null values** and **memory usage**. ","091a600d":"We can also see the data types of columns with **.dtypes**","6bda5c73":"* We checked the data types of the columns in Titanic dataset. We saw that the type of **Embarked** column is object. After counting the unique values in Embarked column with **.unique()**, we can see that there are 3 unique values in that column. So we can consider that the data type should be categorical. To change the datatype of that column the code below must be executed:","e3acdb27":"During my graduation project, I have dealt with a lot of **Data Pre-processing** stuff without using any libraries. I have just used if statements and for loops. It literally killed me. After I started to learn Pandas, I decided to write this kernel to help students and beginners to get start with it. I will do my best to introduce you with Pandas\u2019 some of the most useful capabilities in the stage of Exploratory Data Analysis. "}}