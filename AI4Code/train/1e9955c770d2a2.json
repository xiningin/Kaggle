{"cell_type":{"d85d3fc5":"code","6ad0b88d":"code","e962e7f4":"code","229e3c1f":"code","d7d3895c":"code","97afcc02":"code","83b4fc18":"code","557fe5cc":"code","f784add6":"code","6758530d":"code","8d9bc573":"code","565a8077":"code","38f24d87":"code","0d13c7bb":"code","35ba8457":"code","af936b55":"code","e5e0be04":"code","d45ae05b":"markdown","53e7bcb1":"markdown","31612e53":"markdown","084e5c7f":"markdown","64986dfc":"markdown","7c95b965":"markdown","4ca8f5d7":"markdown","e394806b":"markdown","ef702579":"markdown","eaed4c6e":"markdown","f3acbc26":"markdown","267db3a3":"markdown","d19ad1ab":"markdown","32653d01":"markdown","336402c2":"markdown","ea356080":"markdown","9e04b72c":"markdown","c421110b":"markdown","8c3c4637":"markdown"},"source":{"d85d3fc5":"%%time\n!python ..\/input\/roberta-large-code\/infer.py","6ad0b88d":"%%time\n!python ..\/input\/xlnet-base\/infer.py","e962e7f4":"%%time\n!python ..\/input\/roberta-base\/infer.py","229e3c1f":"%%time\n!python ..\/input\/distil-roberta\/infer.py","d7d3895c":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_distilbert.py","97afcc02":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_bert_base.py","83b4fc18":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_bert_wwm.py","557fe5cc":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_albert.py","f784add6":"%%time\n\n!pip install \/kaggle\/input\/bertweet-libs\/sacrebleu-1.4.10-py3-none-any.whl\n!cp -R \/kaggle\/input\/bertweet-libs\/fairseq-0.9.0\/fairseq-0.9.0 \/kaggle\/working\n!cp -R \/kaggle\/input\/bertweet-libs\/fastBPE-0.1.0\/fastBPE-0.1.0\/ \/kaggle\/working\n\n!pip install \/kaggle\/working\/fairseq-0.9.0\/\n!pip install \/kaggle\/working\/fastBPE-0.1.0\/\n\n!python ..\/input\/tweet-inference-scripts\/inference_bertweet.py","6758530d":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_roberta_anton.py","8d9bc573":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_roberta_large_hiki.py","565a8077":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_roberta_hiki.py","38f24d87":"!python ..\/input\/tweet-inference-scripts-lvl-2\/inference_cnn_2_2.py","0d13c7bb":"!python ..\/input\/tweet-inference-scripts-lvl-2\/inference_wavenet_4_3.py","35ba8457":"!python ..\/input\/tweet-inference-scripts-lvl-2\/inference_rnn_5_3.py","af936b55":"!python ..\/input\/tweet-inference-scripts-lvl-2\/inference_wavenet_0_3.py","e5e0be04":"import pickle\n\nimport numpy as np\nimport pandas as pd\n\n\ndef string_from_preds_char_level(texts, preds):\n    selected_texts = []\n    n_models = len(preds)\n\n    for idx in range(len(texts)):\n        data = texts[idx]\n\n        start_probas = np.mean(\n            [preds[i][0][idx] for i in range(n_models)], 0)\n        end_probas = np.mean(\n            [preds[i][1][idx] for i in range(n_models)], 0)\n\n        start_idx = np.argmax(start_probas)\n        end_idx = np.argmax(end_probas)\n\n        if end_idx < start_idx:\n            selected_text = data\n        else:\n            selected_text = data[start_idx: end_idx]\n\n        selected_texts.append(selected_text.strip())\n\n    return selected_texts\n\n\ndf_test = pd.read_csv(\n    '..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')\ndf_test['selected_text'] = ''\nsub = pd.read_csv(\n    '..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n\n\npreds_1 = np.load('preds_char_test_cnn_2_2.npy')\npreds_2 = np.load('preds_char_test_wavenet_4_3.npy')\npreds_3 = np.load('preds_char_test_rnn_5_3.npy')\npreds_4 = np.load('preds_char_test_wavenet_0_3.npy')\n\ntest_preds = (preds_1 + preds_2 + preds_3 + preds_4) \/ 4\n\nselected_texts = string_from_preds_char_level(\n    df_test['text'].values, test_preds)\n\nsub['selected_text'] = selected_texts\ndf_test['selected_text'] = selected_texts\nsub.to_csv('submission.csv', index=False)","d45ae05b":"### Bertweet","53e7bcb1":"### Distilroberta","31612e53":"# Level 2 models","084e5c7f":"### Albert large","64986dfc":"### Roberta","7c95b965":"### Bert large wwm","4ca8f5d7":"### Xlnet","e394806b":"### Roberta large","ef702579":"## Hearkilla","eaed4c6e":"### Bert base","f3acbc26":"### Roberta","267db3a3":"### Roberta large","d19ad1ab":"## Anton","32653d01":"### Roberta","336402c2":"# Ensemble","ea356080":"## Theo","9e04b72c":"### Distilbert","c421110b":"## Hiki","8c3c4637":"# Stage 1 Models"}}