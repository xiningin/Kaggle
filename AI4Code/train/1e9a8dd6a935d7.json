{"cell_type":{"79190558":"code","f68e0e8d":"code","205eb5de":"code","1845e3f2":"code","24c8e0dc":"code","8f5bbbee":"code","196089f0":"code","0c1ec130":"code","1920c22c":"code","fbd9bfe2":"code","21b7900c":"code","ebffaa40":"code","c4496799":"markdown","979997be":"markdown","41612857":"markdown","7f4688f7":"markdown","a2670f7e":"markdown","f9863851":"markdown","67f09a18":"markdown","1a196023":"markdown","77476d5a":"markdown","a3d1a54a":"markdown","90098352":"markdown"},"source":{"79190558":"import os\n\nimport numpy as np\nimport pandas as pd\n\nimport librosa\nimport librosa.display\nimport soundfile as sf # librosa fails when reading files on Kaggle.\n\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix","f68e0e8d":"audio_path = '..\/input\/train\/Train\/2.wav'\nipd.Audio(audio_path)","205eb5de":"# Extract the audio data (x) and the sample rate (sr).\nx, sr = librosa.load(audio_path)\n\n# Plot the sample.\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(x, sr=sr)\nplt.show()","1845e3f2":"plt.figure(figsize=(12, 5))\nplt.plot(x[1000:1100]) # Zoom-in for seeing the example.\nplt.grid()\n\nn_crossings = librosa.zero_crossings(x[1000:1100], pad=False)\nprint(f'Number of crosses: {sum(n_crossings)}')","24c8e0dc":"centroids = librosa.feature.spectral_centroid(x, sr=sr)[0]\n\nprint(f'Centroids Shape: {centroids.shape}')\nprint(f'First 3 centroids: {centroids[:3]}')","8f5bbbee":"mfccs = librosa.feature.mfcc(x, sr=sr)\nprint(f'MFFCs shape: {mfccs.shape}')\nprint(f'First mffcs: {mfccs[0, :5]}')\n\n# We can even display an spectogram of the mfccs.\nlibrosa.display.specshow(mfccs, sr=sr, x_axis='time')\nplt.show()","196089f0":"def mean_mfccs(x):\n    return [np.mean(feature) for feature in librosa.feature.mfcc(x)]\n\ndef parse_audio(x):\n    return x.flatten('F')[:x.shape[0]] \n\ndef get_audios():\n    train_path = \"..\/input\/train\/Train\/\"\n    train_file_names = os.listdir(train_path)\n    train_file_names.sort(key=lambda x: int(x.partition('.')[0]))\n    \n    samples = []\n    for file_name in train_file_names:\n        x, sr = sf.read(train_path + file_name, always_2d=True)\n        x = parse_audio(x)\n        samples.append(mean_mfccs(x))\n        \n    return np.array(samples)\n\ndef get_samples():\n    df = pd.read_csv('..\/input\/train.csv')\n    return get_audios(), df['Class'].values","0c1ec130":"X, Y = get_samples()\n\n# Since the data manufacturer doesn't provide the labels for the test audios,\n# we will have do the split for the labeled data.\nx_train, x_test, y_train, y_test = train_test_split(X, Y)","1920c22c":"print(f'Shape: {x_train.shape}')\nprint(f'Observation: \\n{x_train[0]}')\nprint(f'Labels: {y_train[:5]}')","fbd9bfe2":"scaler = StandardScaler()\nscaler.fit(x_train)\nx_train_scaled = scaler.transform(x_train)\nx_test_scaled = scaler.transform(x_test)\n\npca = PCA().fit(x_train_scaled)\n\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)')\nplt.show()","21b7900c":"grid_params = {\n    'n_neighbors': [3, 5, 7, 9, 11, 15],\n    'weights': ['uniform', 'distance'],\n    'metric': ['euclidean', 'manhattan']\n}\n\nmodel = GridSearchCV(KNeighborsClassifier(), grid_params, cv=5, n_jobs=-1)\nmodel.fit(x_train_scaled, y_train)","ebffaa40":"print(f'Model Score: {model.score(x_test_scaled, y_test)}')\n\ny_predict = model.predict(x_test_scaled)\nprint(f'Confusion Matrix: \\n{confusion_matrix(y_predict, y_test)}')","c4496799":"As we can see most of the variance is explained using all the features of the MFCC. This is expected since each feature gives information about the wave shape. Finally we can fit our model.","979997be":"### Spectral Centroids\nA weighted mean of audio frequencies.","41612857":"Thanks to the library we are using for audio processing named *librosa*, we can display the wave plot of our sample.","7f4688f7":"### MFCC\nThis is the feature we will use for our analysis. It's one amongst the most popular features, because it provides data about the overall shape of the audio frequencies. ","a2670f7e":"As [this article](https:\/\/towardsdatascience.com\/extract-features-of-music-75a3f9bc265d) exceptionally explains, we can extract different features from our audios. I recommend you to check out the article because it gives a more in depth explanation about where these features come from.\n\n### Crossing Rate\nHow many times the audio wave crosses the zero line.","f9863851":"# Audio Processing\nFirst, as good data scientists, let's inspect our data by hearing some of those audios files.","67f09a18":"Let's see how our data looks like.","1a196023":"# Urban Sound Classification - Feature Extraction & KNN\n## Context\nThe automatic classification of environmental sound is a growing research field with multiple applications to largescale, content-based multimedia indexing and retrieval. In particular, the sonic analysis of urban environments is the subject of increased interest, partly enabled by multimedia sensor networks, as well as by large quantities of online multimedia content depicting urban scenes.\n\n## Content\nThe dataset is called UrbanSound and contains 8732 labeled sound excerpts (<=4s) of urban sounds from 10 classes: Air Conditioner, Car Horn, Children Playing, Dog bark, Drilling, Engine Idling, Gun Shot, Jackhammer, Siren Street, Music. The attributes of data are as follows: ID \u2013 Unique ID of sound excerpt Class \u2013 type of sound\n\n## Goals\nIn this notebook we will build a model which classifies each sound into one of the unique categories. To do so, we will use the algorithm *k-nearest neighbors* as our model and the [MFFC](https:\/\/en.wikipedia.org\/wiki\/Mel-frequency_cepstrum) as the extracted feature of each audio.","77476d5a":"Before building our model we should check if we would benefit of some noise reduction using PCA.","a3d1a54a":"# Model Creation\nNow that we've learnt how to extract features from our audios, let's create our model.\n\nTo start things off, we will compute the mean mfcc of each feature inside the mfcc. This is because the data that the mfcc returns is not always the same length, and since our model expects the data to be the same shape, we have transform it. Then we will parse each audio file and compute the mean mfcc for each one. Once that's done, we can feed our model with the data.","90098352":"It scored quite good taking in consideration how simple our approach was. Even so, in other circumstances we would focus more on improving our model. Perhaps we could choose a better algorithm, or maybe we could preprocess our data in a smarter way.\n \nSince this is not the goal of the notebook, I'd like to keep things simple. But I invite you to fork this notebook and try to push it beyond belief.\n \n<br>\nI hope you have learnt something new. See you in the next one!"}}