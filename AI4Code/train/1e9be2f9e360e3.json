{"cell_type":{"6cfa735a":"code","8abdee5c":"code","d8d4f593":"code","e09e9158":"code","c921a3ac":"code","cdc03545":"code","0ae812f5":"code","c10e78ad":"code","90ce3e31":"code","8c110617":"code","3cc7cdda":"code","ff6df9bf":"code","7e0c57aa":"code","398ca760":"code","f76b1848":"code","00bc13d2":"code","b42f6947":"code","1d027fa2":"code","f596d5ba":"code","e7ae94bf":"code","3df4d234":"code","a22f1f76":"code","4cc64f05":"code","cf47b392":"code","5dddf2c6":"code","349ed1c2":"code","231a6bd3":"code","e23b20c3":"code","cd547884":"code","36e15247":"code","dd781610":"code","f76016ab":"code","a7d0c875":"code","c3534e8d":"code","ed922472":"code","b6f0918e":"code","d504a8a3":"code","ec15d08b":"code","2f3e4820":"code","210b5ca4":"code","18d29f48":"code","e7c0d2d6":"code","4fa79118":"code","2cb61454":"code","52a3bf09":"code","6ecae54b":"code","7add35c1":"code","adc3276b":"code","82719008":"code","46a1e290":"code","9d1a2a16":"code","e81549c2":"code","9d4cf273":"markdown","c358e47d":"markdown","6c64694e":"markdown","7c1a798b":"markdown","fbe8aecc":"markdown","ef9c4c52":"markdown","f7e72a3f":"markdown","4a59b71c":"markdown","ed766fe3":"markdown","869501a5":"markdown","b7d7e37c":"markdown","a8e64bf9":"markdown","7da9c127":"markdown","919e0d7e":"markdown","b62a095e":"markdown","082d2bc9":"markdown","65101b88":"markdown"},"source":{"6cfa735a":"import os\nfrom IPython.display import Image\nImage(filename=\"..\/input\/images\/CNNs.png\")","8abdee5c":"Image(filename=\"..\/input\/images\/LSTM_CNN_image.png\")","d8d4f593":"Image(filename=\"..\/input\/images\/CNN-LSTM_image.png\")","e09e9158":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c921a3ac":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom wordcloud import WordCloud,STOPWORDS\nfrom bs4 import BeautifulSoup\nimport re,string,unicodedata\nimport os\nfrom IPython.display import Image\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, accuracy_score, confusion_matrix, plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve\n#from xgboost.sklearn import XGBClassifier\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.layers import Dense,Input, Embedding,LSTM,Dropout,Conv1D, MaxPooling1D, GlobalMaxPooling1D,Dropout,Bidirectional,Flatten,BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.utils import plot_model\n#import transformers\n#import tokenizers","cdc03545":"data = pd.read_csv('\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf= data.copy()","0ae812f5":"data.head()","c10e78ad":"data.shape","90ce3e31":"data.info()","8c110617":"data.describe()","3cc7cdda":"data['sentiment'].value_counts()","ff6df9bf":"data.isnull().sum()","7e0c57aa":" data.duplicated().sum()","398ca760":"data.drop_duplicates(inplace = True)","f76b1848":"data.shape","00bc13d2":"stop = stopwords.words('english')\nwl = WordNetLemmatizer()","b42f6947":"mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n           \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \n           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n           \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \n           \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \n           \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n           \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \n           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\n           \"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\n           \"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n           \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n           \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n           \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \n           \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n           \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n           \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n           \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\n           \"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \n           \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \n           \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \n           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \n           \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n           \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n           \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n           \"what'll\": \"what will\", \"what'll've\": \"what will have\",\"what're\": \"what are\",  \n           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \n           \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \n           \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n           \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n           \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n           \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n           \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n           \"you're\": \"you are\", \"you've\": \"you have\" }","1d027fa2":"#function to clean data\n\nimport nltk\nnltk.download('wordnet')\ndef clean_text(text,lemmatize = True):\n    soup = BeautifulSoup(text, \"html.parser\") #remove html tags\n    text = soup.get_text()\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")]) #expanding chatwords and contracts clearing contractions\n    emoji_clean= re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_clean.sub(r'',text)\n    text = re.sub(r'\\.(?=\\S)', '. ',text) #add space after full stop\n    text = re.sub(r'http\\S+', '', text) #remove urls\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation]) #remove punctuation\n    #tokens = re.split('\\W+', text) #create tokens\n    if lemmatize:\n        text = \" \".join([wl.lemmatize(word) for word in text.split() if word not in stop and word.isalpha()]) #lemmatize\n    else:\n        text = \" \".join([word for word in text.split() if word not in stop and word.isalpha()]) \n    return text","f596d5ba":"data['review']=data['review'].apply(clean_text,lemmatize = True)","e7ae94bf":"#converting target variable to numeric labels\ndata.sentiment = [ 1 if each == \"positive\" else 0 for each in data.sentiment]\ndata.head()","3df4d234":"#splitting into train and test\ntrain, test= train_test_split(data, test_size=0.2, random_state=42)\n\n#train dataset\nXtrain, ytrain = train['review'], train['sentiment']\n\n#test dataset\nXtest, ytest = test['review'], test['sentiment']\n\nprint(Xtrain.shape,ytrain.shape)\nprint(Xtest.shape,ytest.shape)","a22f1f76":"vect = TfidfVectorizer()\nXtrain_vect= vect.fit_transform(Xtrain)\nXtest_vect = vect.transform(Xtest)\n\n\ncount_vect = CountVectorizer() \nXtrain_count = count_vect.fit_transform(Xtrain)\nXtest_count = count_vect.transform(Xtest)","4cc64f05":"MAX_VOCAB_SIZE = 10000\ntokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE,oov_token=\"<oov>\")\ntokenizer.fit_on_texts(Xtrain)\nword_index = tokenizer.word_index\n#print(word_index)\nV = len(word_index)\nprint(\"Vocabulary of the dataset is : \",V)","cf47b392":"##create sequences of reviews\nseq_train = tokenizer.texts_to_sequences(Xtrain)\nseq_test =  tokenizer.texts_to_sequences(Xtest)","5dddf2c6":"#choice of maximum length of sequences\nseq_len_list = [len(i) for i in seq_train + seq_test]\n\n#if we take the direct maximum then\nmax_len=max(seq_len_list)\nprint('Maximum length of sequence in the list: {}'.format(max_len))","349ed1c2":"# when setting the maximum length of sequence, variability around the average is used.\nmax_seq_len = np.mean(seq_len_list) + 2 * np.std(seq_len_list)\nmax_seq_len = int(max_seq_len)\nprint('Maximum length of the sequence when considering data only two standard deviations from average: {}'.format(max_seq_len))","231a6bd3":"perc_covered = np.sum(np.array(seq_len_list) < max_seq_len) \/ len(seq_len_list)*100\nprint('The above calculated number coveres approximately {} % of data'.format(np.round(perc_covered,2)))","e23b20c3":"#create padded sequences\npad_train=pad_sequences(seq_train,truncating = 'post', padding = 'pre',maxlen=max_seq_len)\npad_test=pad_sequences(seq_test,truncating = 'post', padding = 'pre',maxlen=max_seq_len)","cd547884":"#Splitting training set for validation purposes\nXtrain,Xval,ytrain,yval=train_test_split(pad_train,ytrain,\n                                             test_size=0.2,random_state=10)","36e15247":"def lstm_model(Xtrain,Xval,ytrain,yval,V,D,maxlen,epochs):\n\n    print(\"----Building the model----\")\n    i = Input(shape=(maxlen,))\n    x = Embedding(V + 1, D,input_length = maxlen)(i)\n    x = BatchNormalization()(x)\n    x = Dropout(0.3)(x)\n    x = Conv1D(32,5,activation = 'relu')(x)\n    x = Dropout(0.3)(x)\n    x = MaxPooling1D(2)(x)\n    x = Bidirectional(LSTM(128,return_sequences=True))(x)\n    x = LSTM(64)(x)\n    x = Dropout(0.5)(x)\n    x = Dense(1, activation='sigmoid')(x)\n    model = Model(i, x)\n    model.summary()\n\n    #Training the LSTM\n    print(\"----Training the network----\")\n    model.compile(optimizer= Adam(0.0005),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n    r = model.fit(Xtrain,ytrain, \n                  validation_data = (Xval,yval), \n                  epochs = epochs, \n                  verbose = 2,\n                  batch_size = 32)\n                  #callbacks = callbacks\n    print(\"Train score:\", model.evaluate(Xtrain,ytrain))\n    print(\"Validation score:\", model.evaluate(Xval,yval))\n    n_epochs = len(r.history['loss'])\n    \n    return r,model,n_epochs ","dd781610":"D = 64 #embedding dims\nepochs = 5\nr,model,n_epochs = lstm_model(Xtrain,Xval,ytrain,yval,V,D,max_seq_len,epochs)","f76016ab":"def plotLearningCurve(history,epochs):\n    \n    epochRange = range(1,epochs+1)\n    fig , ax = plt.subplots(1,2,figsize = (10,5))\n  \n    ax[0].plot(epochRange,history.history['accuracy'],label = 'Training Accuracy')\n    ax[0].plot(epochRange,history.history['val_accuracy'],label = 'Validation Accuracy')\n    ax[0].set_title('Training and Validation accuracy')\n    ax[0].set_xlabel('Epoch')\n    ax[0].set_ylabel('Accuracy')\n    ax[0].legend()\n    ax[1].plot(epochRange,history.history['loss'],label = 'Training Loss')\n    ax[1].plot(epochRange,history.history['val_loss'],label = 'Validation Loss')\n    ax[1].set_title('Training and Validation loss')\n    ax[1].set_xlabel('Epoch')\n    ax[1].set_ylabel('Loss')\n    ax[1].legend()\n    fig.tight_layout()\n    plt.show()","a7d0c875":"plotLearningCurve(r,n_epochs)","c3534e8d":"print(\"Evaluate Model Performance on Test set\")\nresult = model.evaluate(pad_test,ytest)\nprint(dict(zip(model.metrics_names, result)))","ed922472":"#Generate predictions for the test dataset\nypred = model.predict(pad_test)\nypred = ypred>0.5\n#Get the confusion matrix\ncf_matrix = confusion_matrix(ytest, ypred)\nsns.heatmap(cf_matrix,annot = True,fmt ='g', cmap='Blues')\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.show()","b6f0918e":"from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score,auc, accuracy_score\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import datasets\n\nfrom tensorflow.keras.preprocessing import sequence\nfrom sklearn.datasets import fetch_20newsgroups\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, MaxPool1D, Dropout, SimpleRNN, LSTM\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.preprocessing import sequence\nimport numpy as np\nimport string\nimport re","d504a8a3":"df.sentiment = df.sentiment.map({ 'negative': 0, 'positive': 1 })\n\ntext = df.review.tolist()\nlabel = df.sentiment.tolist()","ec15d08b":"X_train, X_test, y_train, y_test = train_test_split(text, label, test_size=0.2, random_state=1)","2f3e4820":"translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space","210b5ca4":"X_train_clean = []\nX_test_clean = []\nclean = re.compile(r'<[^>]+>')\nfor i, test in enumerate(X_train):\n    tmp_text = test.lower()\n    tmp_text = tmp_text.replace('\\n', '')\n    tmp_text = clean.sub('', tmp_text)\n    tmp_text = tmp_text.translate(translator)\n    X_train_clean.append(tmp_text)\n\nfor i, test in enumerate(X_test):\n    tmp_text = test.lower()\n    tmp_text = tmp_text.replace('\\n', '')\n    tmp_text = clean.sub('', tmp_text)\n    tmp_text = tmp_text.translate(translator)\n    X_test_clean.append(tmp_text)\n\nX_train_clean = np.array(X_train_clean)\nX_test_clean = np.array(X_test_clean)\n\nX_train = X_train_clean\nX_test = X_test_clean","18d29f48":"top_words = 40000\ntokenizer = Tokenizer(num_words=top_words)\ntokenizer.fit_on_texts(X_train)\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","e7c0d2d6":"max_words = 100\nX_train = sequence.pad_sequences(X_train, maxlen=max_words, padding='post')\nX_test = sequence.pad_sequences(X_test, maxlen=max_words, padding='post')","4fa79118":"y_train = np.array(y_train)\ny_test = np.array(y_test)","2cb61454":"X_train.shape\n","52a3bf09":"model = Sequential()\nmodel.add(Embedding(20000,32, input_length=100))\nmodel.add(Conv1D(256, 3, activation='relu', padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv1D(128, 3, activation='relu', padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.2))\nmodel.add(Flatten())\nmodel.add(Dense(250, activation='relu'))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","6ecae54b":"model.summary()\n","7add35c1":"model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=20, batch_size=128, verbose=2)","adc3276b":"print(\"Evaluate Model Performance on Test set\")\nresult = model.evaluate(X_test,y_test)\nprint(dict(zip(model.metrics_names, result)))","82719008":"model = Sequential()\nmodel.add(Embedding(20000,32, input_length=100))\nmodel.add(Conv1D(256, 3, activation='relu', input_shape=(178, 1), padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.2))\nmodel.add(Conv1D(128, 3, activation='relu', padding='same'))\nmodel.add(MaxPool1D(2))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(64, return_sequences=True))\nmodel.add(LSTM(32))\nmodel.add(Flatten())\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])","46a1e290":"model.summary()\n","9d1a2a16":"model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=20, batch_size=128, verbose=2)","e81549c2":"print(\"Evaluate Model Performance on Test set\")\nresult = model.evaluate(X_test,y_test)\nprint(dict(zip(model.metrics_names, result)))","9d4cf273":"### Old work related to the paper ' Convolutional Neural Networks for Sentence Classification ' \n- https:\/\/arxiv.org\/pdf\/1408.5882.pdf\n\nIt train a simple (CNN) with one layer of convolution on top of word vectors obtained from an unsupervised neural language model. These vectors were trained by Mikolov etal. (2013) on 100 billion words of Google News, and are publicly available.1 We initially keep the word vectors static and learn only the other parameters of the model. Despite little tuning of hyperparameters, this simple model achieves excellent results on multiple benchmarks.","c358e47d":"\n\n\n# LSTMs\n\nLong-Term Short Term Memory (LSTMs) are a type of network that has a memory that \"remembers\" previous data from the input and makes decisions based on that knowledge. These networks are more directly suited for written data inputs, since each word in a sentence has meaning based on the surrounding words (previous and upcoming words).\n\nIn our particular case, it is possible that an LSTM could allow us to capture changing sentiment in a tweet. For example, a sentence such as: At first I loved it, but then I ended up hating it. has words with conflicting sentiments that would end-up confusing a simple Feed-Forward network. The LSTM, on the other hand, could learn that sentiments expressed towards the end of a sentence mean more than those expressed at the start.\n\n","6c64694e":"# CNN-LSTM Model\n\nThe first model I tried was the CNN-LSTM Model. Our CNN-LSTM model combination consists of an initial convolution layer which will receive word embeddings as input. Its output will then be pooled to a smaller dimension which is then fed into an LSTM layer. The intuition behind this model is that the convolution layer will extract local features and the LSTM layer will then be able to use the ordering of said features to learn about the input\u2019s text ordering. In practice, this model is not as powerful as our other LSTM-CNN model proposed.","7c1a798b":"### Data preprocessing","fbe8aecc":"# 1- LSTM model","ef9c4c52":"# Sources\n\n- http:\/\/konukoii.com\/blog\/2018\/02\/19\/twitter-sentiment-analysis-using-combined-lstm-cnn-models\/\n- https:\/\/arxiv.org\/pdf\/1408.5882.pdf\n- https:\/\/github.com\/pytorch\/ignite\/blob\/master\/examples\/notebooks\/TextCNN.ipynb\n- https:\/\/www.kaggle.com\/raghav2002sharma\/sentiment-classifier-with-cnn-bi-lstm\n- https:\/\/www.kaggle.com\/ashrafkhan94\/imdb-review-comparison-using-cnn-lstm-bert\n- https:\/\/www.kaggle.com\/parth05rohilla\/bi-lstm-and-cnn-model-top-10\/notebook\n- https:\/\/www.kaggle.com\/c\/movie-review-sentiment-analysis-kernels-only\/code?competitionId=10025&searchQuery=cnn\n- https:\/\/colab.research.google.com\/github\/d2l-ai\/d2l-en-colab\/blob\/master\/chapter_natural-language-processing-applications\/sentiment-analysis-cnn.ipynb\n- https:\/\/www.kaggle.com\/nafisur\/keras-models-lstm-cnn-gru-bidirectional-glove\n- https:\/\/www.kaggle.com\/derrelldsouza\/imdb-sentiment-analysis-eda-ml-lstm-bert#5.-Predictive-Modelling-using-Deep-Learning\n- https:\/\/www.kaggle.com\/clementbrehard\/imdb-conv1d-lstm","f7e72a3f":"### Data Cleaning","4a59b71c":"# 3- LSTM-CNN Model","ed766fe3":"#### Spliting the training dataset\n","869501a5":"# CNNs\n\nConvolutional Neural Networks (CNNs) are networks initially created for image-related tasks that can learn to capture specific features regardless of locality.\n\nFor a more concrete example of that, imagine we use CNNs to distinguish pictures of Cars vs. pictures of Dogs. Since CNNs learn to capture features regardless of where these might be, the CNN will learn that cars have wheels, and every time it sees a wheel, regardless of where it is on the picture, that feature will activate.\n\nIn our particular case, it could capture a negative phrase such as \"don't like\" regardless of where it happens in the tweet.\n\n*     I don't like watching those types of films\n*     That's the one thing I really don't like.\n*     I saw the movie, and I don't like how it ended.\n","b7d7e37c":"# 2-CNN model","a8e64bf9":"# What is new work we add on the old one and our contributions?\n\nInstead of unsing only the CNN to make the Sentence Classification we will use CNN in as well as LSTM to generate a combination model of them( CNN-LSTM ) and ( LSTM-CNN )","7da9c127":"### load needed libraries","919e0d7e":"### load our data","b62a095e":"#### Vectorizing data\n\n\n","082d2bc9":"# LSTM-CNN Model\n\nOur CNN-LSTM model consists of an initial LSTM layer which will receive word embeddings for each token in the tweet as inputs. The intuition is that its output tokens will store information not only of the initial token, but also any previous tokens; In other words, the LSTM layer is generating a new encoding for the original input. The output of the LSTM layer is then fed into a convolution layer which we expect will extract local features. Finally the convolution layer\u2019s output will be pooled to a smaller dimension and ultimately outputted as either a positive or negative label.\n\n","65101b88":"# Sentiment Analysis"}}