{"cell_type":{"31e6f871":"code","bf58afd3":"code","da7b8bb6":"code","7c27429f":"code","a724d59b":"code","a7fb8688":"code","716b404a":"code","bfa71e07":"code","b91ed28e":"code","55b7e727":"code","4ba96cf1":"code","0b5f07a8":"code","ead67225":"code","56e6963b":"code","6b71f051":"code","a2ef49fa":"code","e165a583":"code","323cc319":"code","c75fb617":"code","0b9902e1":"code","98a6e8fd":"code","65c5c785":"code","5fda7ba1":"code","e637597e":"code","706320cf":"code","d4ff9057":"code","7bef28f0":"code","d562f41b":"code","cacae116":"code","c82323f7":"code","35abc7ac":"code","4eedbd1f":"code","c49ed0e9":"code","5c53812c":"code","0df79ea2":"code","d3e38073":"code","e0dcfa30":"code","205cc43f":"code","f741e2d5":"code","ea11d71f":"code","bf8545e0":"code","69cb3dfa":"code","b047c17c":"code","89777f17":"code","90dacb40":"code","a0e86679":"code","5186cc4d":"code","734260b0":"code","6e739cae":"code","973c45c2":"code","ff330e1c":"code","b7f10083":"code","aee55907":"code","09dc0cc6":"code","a642c6d8":"code","011e982d":"code","2e307a48":"code","da83ca44":"code","99d2faac":"code","f61da352":"code","0cd3ce25":"code","5708d5d2":"code","016be0bb":"code","e43b308a":"code","cfe120c5":"code","221a8d10":"code","daf3d28c":"code","cffb7d27":"code","1b2ce485":"code","48297935":"code","b2a1a753":"code","0827d6d7":"code","9bb5d46a":"code","1334f389":"code","88ab5fee":"code","b95ad810":"code","b06c74be":"code","cf4319c1":"code","0bde40fd":"code","9003017b":"code","d21e4b8a":"code","42e42f45":"code","e2268da0":"code","b44fbf04":"code","641b2238":"code","4895ad45":"code","049f24d2":"code","3c32156d":"code","fc40f59a":"code","a78c8688":"code","868af9e6":"code","3737a69a":"code","ca0312d2":"code","6c28de41":"code","fcd05271":"code","f9b9018d":"code","bfbebee3":"code","8d60a19f":"code","3bfd5d28":"code","9b8e97ed":"code","7fb7542f":"code","554f9369":"code","5f7931a8":"code","7508279c":"code","ff19727f":"code","8ac88b5b":"code","3b9b48f3":"code","f5a9710a":"code","49782d91":"code","db36b7fe":"code","5e0c0704":"code","99d09961":"code","ad396a7e":"code","1903476e":"code","182cc0c5":"code","9c378810":"code","834976d5":"code","896c54e3":"code","819864cf":"code","22aadd83":"code","39cd8ef0":"code","d6e00df2":"code","dba7c6f6":"code","fe1ca383":"code","40e06ce9":"code","aa30263d":"code","f6d23523":"code","92663f29":"code","d4b8c4d7":"code","06cb5237":"code","9bb08f8e":"code","e49bf504":"code","68918852":"code","4d8c0a02":"code","dd20a975":"code","198192dd":"code","2f6d5c41":"code","7e47e22c":"code","52f09528":"code","ecf6047f":"code","5f61cbea":"code","baa02b58":"code","d4ccedf7":"code","c0adbe8e":"code","278b7162":"code","331b9437":"code","1b6ecfb6":"code","0016346b":"code","a1a78e61":"code","0d04e213":"code","4c9a123a":"code","1e6fd566":"markdown","c2eb3a83":"markdown","31514ba7":"markdown","50c12ddd":"markdown","87e4ad6f":"markdown","83c11c32":"markdown","a31c008f":"markdown","ecd395bd":"markdown","800e208a":"markdown","f3297587":"markdown","8e5f2a6a":"markdown","d8ee3165":"markdown","7553e561":"markdown","38b9ea64":"markdown","6275a458":"markdown","7e6a3d92":"markdown","1259e16f":"markdown","0cf35748":"markdown","e5f75ccb":"markdown","3816941e":"markdown","c03cef42":"markdown","5f496a38":"markdown","cac9ce81":"markdown","a0d30417":"markdown","5a99acb9":"markdown","94b6b118":"markdown","08423978":"markdown","5c59190e":"markdown","0231eccb":"markdown","9b9176db":"markdown","bb39b650":"markdown","f9ed51cc":"markdown","39f89771":"markdown","3ef243cb":"markdown","0d43a644":"markdown","d4eaf022":"markdown","79d4858e":"markdown","bf90e897":"markdown","6255c763":"markdown","ab7dd37b":"markdown","01ea33d2":"markdown","3bc8a377":"markdown","d69b0e0c":"markdown","b3343fd4":"markdown","05e23f99":"markdown","dabc7a1f":"markdown","ab842bab":"markdown","bc2c449f":"markdown","9a349cb9":"markdown","bee0faf7":"markdown","45fa60d4":"markdown","e8e409f5":"markdown","033016b1":"markdown","a6d0ab6f":"markdown","afe5d946":"markdown","81a44cb4":"markdown","add6bf1c":"markdown","f0924062":"markdown","d933f1b2":"markdown","566fcdcd":"markdown","3b113fa9":"markdown","e6ac0ee4":"markdown","f0c18721":"markdown","9a44f136":"markdown","345a243e":"markdown","6528ba5a":"markdown","cec73747":"markdown","655bec37":"markdown","dee321ca":"markdown","fd3cf1bc":"markdown","359305c4":"markdown","7b5b54c8":"markdown","934d8dc1":"markdown","82ce1cb3":"markdown","acff98ab":"markdown","d301ba63":"markdown","fb4d9c7d":"markdown","aafb75c3":"markdown","ab7a032c":"markdown","29fc6e1b":"markdown","5234c2fc":"markdown","8f188014":"markdown","46ef03f1":"markdown","6b2c8e12":"markdown","9bc1e4a2":"markdown","4ac10298":"markdown","95e0c8c2":"markdown","a2ebdbf2":"markdown","dfb82a85":"markdown","215431f6":"markdown","dcbec021":"markdown","87942bc9":"markdown","1929604d":"markdown","c135cf7a":"markdown","cde4f986":"markdown","4361b69e":"markdown","b09f850b":"markdown","01d2296c":"markdown","13867a92":"markdown","917f0d2f":"markdown","8fadc64a":"markdown","27096ee1":"markdown","9c31afeb":"markdown","1557f354":"markdown","d999f3dc":"markdown","8771d80b":"markdown","acfaf6f5":"markdown","0d61b40b":"markdown","dbabb0b6":"markdown","998ea5e6":"markdown","a7771e7d":"markdown","f348c96b":"markdown","0456434c":"markdown","335249ba":"markdown","e93f5c7f":"markdown","c3b60673":"markdown","22a739e6":"markdown","a7705fce":"markdown","c407caa7":"markdown","ffc20aac":"markdown","002cc47a":"markdown","c19890cb":"markdown","205e046c":"markdown","f6db8727":"markdown","92f067bb":"markdown","458bcaa2":"markdown","2949e79f":"markdown","11de024e":"markdown","b8dc3cee":"markdown","2aeb20e2":"markdown","d93b48c9":"markdown","195fb08e":"markdown","98b3f2a3":"markdown","9b2fcbb4":"markdown","e1090360":"markdown","3cd784ab":"markdown","6e321c24":"markdown","b0d2844f":"markdown","03c7b0d6":"markdown","162a0cab":"markdown","dc5b6156":"markdown","20bdd9ae":"markdown","348cab68":"markdown","08851cdc":"markdown","56a9e824":"markdown","36c3463e":"markdown","cba1ee69":"markdown"},"source":{"31e6f871":"import numpy as np \nimport pandas as pd \n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nimport wordcloud\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \n\nimport os\nprint(os.listdir(\"..\/input\"))","bf58afd3":"def print_validation_report(y_true, y_pred):\n    print(\"Classification Report\")\n    print(classification_report(y_true, y_pred))\n    acc_sc = accuracy_score(y_true, y_pred)\n    print(\"Accuracy : \"+ str(acc_sc))\n    \n    return acc_sc","da7b8bb6":"def plot_confusion_matrix(y_true, y_pred):\n    mtx = confusion_matrix(y_true, y_pred)\n    #fig, ax = plt.subplots(figsize=(4,4))\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n                cmap=\"Blues\", cbar=False, ax=ax)\n    #  square=True,\n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","7c27429f":"data = pd.read_csv(\"..\/input\/spam.csv\",encoding='latin-1')\ndata.head()","a724d59b":"data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\ndata = data.rename(columns={\"v1\":\"label\", \"v2\":\"text\"})","a7fb8688":"data.describe()","716b404a":"data.groupby(\"label\").describe()","bfa71e07":"data.label.value_counts()","b91ed28e":"data.label.value_counts().plot.bar();","55b7e727":"data['spam'] = data['label'].map( {'spam': 1, 'ham': 0} ).astype(int)\ndata.head(15)","4ba96cf1":"data['length'] = data['text'].apply(len)","0b5f07a8":"data.head(10)","ead67225":"data.hist(column='length',by='label',bins=60,figsize=(12,4));\nplt.xlim(-40,950);","56e6963b":"data_ham  = data[data['spam'] == 0].copy()\ndata_spam = data[data['spam'] == 1].copy()","6b71f051":"def show_wordcloud(data_spam_or_ham, title):\n    text = ' '.join(data_spam_or_ham['text'].astype(str).tolist())\n    stopwords = set(wordcloud.STOPWORDS)\n    \n    fig_wordcloud = wordcloud.WordCloud(stopwords=stopwords,background_color='lightgrey',\n                    colormap='viridis', width=800, height=600).generate(text)\n    \n    plt.figure(figsize=(10,7), frameon=True)\n    plt.imshow(fig_wordcloud)  \n    plt.axis('off')\n    plt.title(title, fontsize=20 )\n    plt.show()","a2ef49fa":"show_wordcloud(data_ham, \"Ham messages\")","e165a583":"show_wordcloud(data_spam, \"Spam messages\")","323cc319":"import string\nstring.punctuation","c75fb617":"from nltk.corpus import stopwords\nstopwords.words(\"english\")[100:110]","0b9902e1":"def remove_punctuation_and_stopwords(sms):\n    \n    sms_no_punctuation = [ch for ch in sms if ch not in string.punctuation]\n    sms_no_punctuation = \"\".join(sms_no_punctuation).split()\n    \n    sms_no_punctuation_no_stopwords = \\\n        [word.lower() for word in sms_no_punctuation if word.lower() not in stopwords.words(\"english\")]\n        \n    return sms_no_punctuation_no_stopwords","98a6e8fd":"data['text'].apply(remove_punctuation_and_stopwords).head()","65c5c785":"from collections import Counter","5fda7ba1":"data_ham.loc[:, 'text'] = data_ham['text'].apply(remove_punctuation_and_stopwords)\nwords_data_ham = data_ham['text'].tolist()\ndata_spam.loc[:, 'text'] = data_spam['text'].apply(remove_punctuation_and_stopwords)\nwords_data_spam = data_spam['text'].tolist()","e637597e":"list_ham_words = []\nfor sublist in words_data_ham:\n    for item in sublist:\n        list_ham_words.append(item)","706320cf":"list_spam_words = []\nfor sublist in words_data_spam:\n    for item in sublist:\n        list_spam_words.append(item)","d4ff9057":"c_ham  = Counter(list_ham_words)\nc_spam = Counter(list_spam_words)\ndf_hamwords_top30  = pd.DataFrame(c_ham.most_common(30),  columns=['word', 'count'])\ndf_spamwords_top30 = pd.DataFrame(c_spam.most_common(30), columns=['word', 'count'])","7bef28f0":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=df_hamwords_top30, ax=ax)\nplt.title(\"Top 30 Ham words\")\nplt.xticks(rotation='vertical');","d562f41b":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=df_spamwords_top30, ax=ax)\nplt.title(\"Top 30 Spam words\")\nplt.xticks(rotation='vertical');","cacae116":"fdist_ham  = nltk.FreqDist(list_ham_words)\nfdist_spam = nltk.FreqDist(list_spam_words)","c82323f7":"df_hamwords_top30_nltk  = pd.DataFrame(fdist_ham.most_common(30),  columns=['word', 'count'])\ndf_spamwords_top30_nltk = pd.DataFrame(fdist_spam.most_common(30), columns=['word', 'count'])","35abc7ac":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=df_hamwords_top30_nltk, ax=ax)\nplt.title(\"Top 30 Ham words\")\nplt.xticks(rotation='vertical');","4eedbd1f":"fig, ax = plt.subplots(figsize=(10, 6))\nsns.barplot(x='word', y='count', \n            data=df_spamwords_top30_nltk, ax=ax)\nplt.title(\"Top 30 Spam words\")\nplt.xticks(rotation='vertical');","c49ed0e9":"from sklearn.feature_extraction.text import CountVectorizer\nbow_transformer = CountVectorizer(analyzer = remove_punctuation_and_stopwords).fit(data['text'])","5c53812c":"print(len(bow_transformer.vocabulary_))","0df79ea2":"sample_spam = data['text'][8]\nbow_sample_spam = bow_transformer.transform([sample_spam])\nprint(sample_spam)\nprint(bow_sample_spam)","d3e38073":"rows, cols = bow_sample_spam.nonzero()\nfor col in cols: \n    print(bow_transformer.get_feature_names()[col])","e0dcfa30":"print(np.shape(bow_sample_spam))","205cc43f":"sample_ham = data['text'][4]\nbow_sample_ham = bow_transformer.transform([sample_ham])\nprint(sample_ham)\nprint(bow_sample_ham)","f741e2d5":"rows, cols = bow_sample_ham.nonzero()\nfor col in cols: \n    print(bow_transformer.get_feature_names()[col])","ea11d71f":"bow_data = bow_transformer.transform(data['text'])","bf8545e0":"bow_data.shape","69cb3dfa":"bow_data.nnz","b047c17c":"bow_data","89777f17":"bow_data.shape[0]","90dacb40":"bow_data.shape[1]","a0e86679":"bow_data.nnz","5186cc4d":"print( bow_data.nnz \/ (bow_data.shape[0] * bow_data.shape[1]) *100 )","734260b0":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer().fit(bow_data)","6e739cae":"tfidf_sample_ham = tfidf_transformer.transform(bow_sample_ham)\nprint(tfidf_sample_ham)","973c45c2":"tfidf_sample_spam = tfidf_transformer.transform(bow_sample_spam)\nprint(tfidf_sample_spam)","ff330e1c":"data_tfidf = tfidf_transformer.transform(bow_data)","b7f10083":"data_tfidf","aee55907":"np.shape(data_tfidf)","09dc0cc6":"from sklearn.model_selection import train_test_split\n\ndata_tfidf_train, data_tfidf_test, label_train, label_test = \\\n    train_test_split(data_tfidf, data[\"spam\"], test_size=0.3, random_state=5)","a642c6d8":"data_tfidf_train","011e982d":"data_tfidf_test","2e307a48":"from scipy.sparse import  hstack\nX2 = hstack((data_tfidf ,np.array(data['length'])[:,None])).A","da83ca44":"X2_train, X2_test, y2_train, y2_test = \\\n    train_test_split(X2, data[\"spam\"], test_size=0.3, random_state=5)","99d2faac":"data_tfidf_train = data_tfidf_train.A\ndata_tfidf_test = data_tfidf_test.A","f61da352":"spam_detect_model = MultinomialNB().fit(data_tfidf_train, label_train)\npred_test_MNB = spam_detect_model.predict(data_tfidf_test)\nacc_MNB = accuracy_score(label_test, pred_test_MNB)\nprint(acc_MNB)","0cd3ce25":"scaler = MinMaxScaler()\ndata_tfidf_train_sc = scaler.fit_transform(data_tfidf_train)\ndata_tfidf_test_sc  = scaler.transform(data_tfidf_test)","5708d5d2":"spam_detect_model_minmax = MultinomialNB().fit(data_tfidf_train_sc, label_train)\npred_test_MNB = spam_detect_model_minmax.predict(data_tfidf_test_sc)\nacc_MNB = accuracy_score(label_test, pred_test_MNB)\nprint(acc_MNB)","016be0bb":"spam_detect_model_2 = MultinomialNB().fit(X2_train, y2_train)\npred_test_MNB_2 = spam_detect_model_2.predict(X2_test)\nacc_MNB_2 = accuracy_score(y2_test, pred_test_MNB_2)\nprint(acc_MNB_2)","e43b308a":"X2_tfidf_train = X2_train[:,0:9431]\nX2_tfidf_test  = X2_test[:,0:9431]\nX2_length_train = X2_train[:,9431]\nX2_length_test  = X2_test[:,9431]","cfe120c5":"scaler = MinMaxScaler()\nX2_tfidf_train = scaler.fit_transform(X2_tfidf_train)\nX2_tfidf_test  = scaler.transform(X2_tfidf_test)","221a8d10":"scaler = MinMaxScaler()\nX2_length_train = scaler.fit_transform(X2_length_train.reshape(-1, 1))\nX2_length_test  = scaler.transform(X2_length_test.reshape(-1, 1))","daf3d28c":"X2_train = np.hstack((X2_tfidf_train, X2_length_train))\nX2_test  = np.hstack((X2_tfidf_test,  X2_length_test))","cffb7d27":"spam_detect_model_3 = MultinomialNB().fit(X2_train, y2_train)\npred_test_MNB_3 = spam_detect_model_3.predict(X2_test)\nacc_MNB_3 = accuracy_score(y2_test, pred_test_MNB_3)\nprint(acc_MNB_3)","1b2ce485":"parameters_KNN = {'n_neighbors': (10,15,17), }\n\ngrid_KNN = GridSearchCV( KNeighborsClassifier(), parameters_KNN, cv=5,\n                        n_jobs=-1, verbose=1)\n\ngrid_KNN.fit(data_tfidf_train, label_train)","48297935":"print(grid_KNN.best_params_)\nprint(grid_KNN.best_score_)","b2a1a753":"parameters_KNN = {'n_neighbors': (6,8,10), }\ngrid_KNN = GridSearchCV( KNeighborsClassifier(), parameters_KNN, cv=5,\n                        n_jobs=-1, verbose=1)\ngrid_KNN.fit(data_tfidf_train_sc, label_train)","0827d6d7":"print(grid_KNN.best_params_)\nprint(grid_KNN.best_score_)","9bb5d46a":"from sklearn.model_selection import train_test_split\n\nsms_train, sms_test, label_train, label_test = \\\n    train_test_split(data[\"text\"], data[\"spam\"], test_size=0.3, random_state=5)","1334f389":"sms_train.head()","88ab5fee":"pipe_MNB = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n                   ('tfidf'   , TfidfTransformer()),\n                   ('clf_MNB' , MultinomialNB()),\n                    ])","b95ad810":"pipe_MNB.fit(X=sms_train, y=label_train)\npred_test_MNB = pipe_MNB.predict(sms_test)\nacc_MNB = accuracy_score(label_test, pred_test_MNB)\nprint(acc_MNB)\nprint(pipe_MNB.score(sms_test, label_test))","b06c74be":"from sklearn.feature_extraction.text import TfidfVectorizer","cf4319c1":"pipe_MNB_tfidfvec = Pipeline([ ('tfidf_vec' , TfidfVectorizer(analyzer = remove_punctuation_and_stopwords)),\n                               ('clf_MNB'   , MultinomialNB()),\n                            ])","0bde40fd":"pipe_MNB_tfidfvec.fit(X=sms_train, y=label_train)\npred_test_MNB_tfidfvec = pipe_MNB_tfidfvec.predict(sms_test)\nacc_MNB_tfidfvec = accuracy_score(label_test, pred_test_MNB_tfidfvec)\nprint(acc_MNB_tfidfvec)\nprint(pipe_MNB_tfidfvec.score(sms_test, label_test))","9003017b":"pipe_KNN = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n                   ('tfidf'   , TfidfTransformer()),\n                   ('clf_KNN' , KNeighborsClassifier() )\n                    ])\n\nparameters_KNN = {'clf_KNN__n_neighbors': (8,15,20), }\n\ngrid_KNN = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,\n                        n_jobs=-1, verbose=1)\n\ngrid_KNN.fit(X=sms_train, y=label_train)","d21e4b8a":"grid_KNN.best_params_","42e42f45":"grid_KNN.best_score_","e2268da0":"pred_test_grid_KNN = grid_KNN.predict(sms_test)\nacc_KNN = accuracy_score(label_test, pred_test_grid_KNN)\nprint(acc_KNN)\nprint(grid_KNN.score(sms_test, label_test))","b44fbf04":"pipe_SVC = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n                   ('tfidf'   , TfidfTransformer()),\n                   ('clf_SVC' , SVC(gamma='auto', C=1000)),\n                    ])\n\n\nparameters_SVC = dict(tfidf=[None, TfidfTransformer()],\n                      clf_SVC__C=[500, 1000,1500]\n                      )\n#parameters = {'tfidf__use_idf': (True, False),    }\n\ngrid_SVC = GridSearchCV(pipe_SVC, parameters_SVC, \n                        cv=5, n_jobs=-1, verbose=1)\n\ngrid_SVC.fit(X=sms_train, y=label_train)","641b2238":"grid_SVC.best_params_","4895ad45":"grid_SVC.best_score_","049f24d2":"pred_test_grid_SVC = grid_SVC.predict(sms_test)\nacc_SVC = accuracy_score(label_test, pred_test_grid_SVC)\nprint(acc_SVC)\nprint(grid_SVC.score(sms_test, label_test))","3c32156d":"pipe_SGD = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n                   ('tfidf'   , TfidfTransformer()),\n                   ('clf_SGD' , SGDClassifier(random_state=5)),\n                    ])\n\nparameters_SGD = {\n    #'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n    'tfidf__use_idf': (True, False),\n    #'tfidf__norm': ('l1', 'l2'),\n    #'clf_SGD__max_iter': (5,10),\n    'clf_SGD__alpha': (1e-05, 1e-04),\n}\n\ngrid_SGD = GridSearchCV(pipe_SGD, parameters_SGD, cv=5,\n                               n_jobs=-1, verbose=1)\n\ngrid_SGD.fit(X=sms_train, y=label_train)","fc40f59a":"grid_SGD.best_params_","a78c8688":"grid_SGD.best_score_","868af9e6":"pred_test_grid_SGD = grid_SGD.predict(sms_test)\nacc_SGD = accuracy_score(label_test, pred_test_grid_SGD)\nprint(acc_SGD)\nprint(grid_SGD.score(sms_test, label_test))","3737a69a":"pipe_GBC = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n                      ('tfidf'   , TfidfTransformer() ),\n                      ('clf_GBC' , GradientBoostingClassifier(random_state=5) ),\n                    ])\n\nparameters_GBC = { 'tfidf__use_idf': (True, False), \n                   'clf_GBC__learning_rate': (0.1, 0.2),\n                   #'clf_GBC__min_samples_split': (3,5), \n                 }\n\ngrid_GBC = GridSearchCV(pipe_GBC, parameters_GBC, \n                        cv=5, n_jobs=-1, verbose=1)\n\ngrid_GBC.fit(X=sms_train, y=label_train)","ca0312d2":"grid_GBC.best_params_","6c28de41":"grid_GBC.best_score_","fcd05271":"pred_test_grid_GBC = grid_GBC.predict(sms_test)\nacc_GBC = accuracy_score(label_test, pred_test_grid_GBC)\nprint(acc_GBC)\nprint(grid_GBC.score(sms_test, label_test))","f9b9018d":"import xgboost as xgb\n\n# Set params['eval_metric'] = ...","bfbebee3":"pipe_XGB = Pipeline([ ('bow'  , CountVectorizer(analyzer = remove_punctuation_and_stopwords) ),\n                      ('tfidf'   , TfidfTransformer() ),\n                      ('clf_XGB' , xgb.XGBClassifier(random_state=5) ),\n                    ])\n\nparameters_XGB = { 'tfidf__use_idf': (True, False), \n                   'clf_XGB__eta': (0.01, 0.02),\n                   'clf_XGB__max_depth': (5,6), \n                 }\n\ngrid_XGB = GridSearchCV(pipe_XGB, parameters_XGB, \n                        cv=5, n_jobs=-1, verbose=1)\n\ngrid_XGB.fit(X=sms_train, y=label_train)","8d60a19f":"grid_XGB.best_params_","3bfd5d28":"grid_XGB.best_score_","9b8e97ed":"pred_test_grid_XGB = grid_XGB.predict(sms_test)\nacc_XGB = accuracy_score(label_test, pred_test_grid_XGB)\nprint(acc_XGB)\nprint(grid_XGB.score(sms_test, label_test))","7fb7542f":"from sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import roc_auc_score","554f9369":"def plot_confusion_matrix(y_true, y_pred):\n    mtx = confusion_matrix(y_true, y_pred)\n    #fig, ax = plt.subplots(figsize=(4,4))\n    sns.heatmap(mtx, annot=True, fmt='d', linewidths=.5,  \n                cmap=\"Blues\", square=True, cbar=False)\n    #  \n    plt.ylabel('true label')\n    plt.xlabel('predicted label')","5f7931a8":"list_clf = [\"MNB\", \"KNN\", \"SVC\", \"SGD\", \"GBC\", \"XGB\"]\n\nlist_pred = [pred_test_MNB, pred_test_grid_KNN, \n             pred_test_grid_SVC, pred_test_grid_SGD,\n             pred_test_grid_GBC, pred_test_grid_XGB]\n\ndict_pred = dict(zip(list_clf, list_pred))","7508279c":"def plot_all_confusion_matrices(y_true, dict_all_pred, str_title):\n    \n    list_classifiers = list(dict_all_pred.keys())\n    plt.figure(figsize=(10,7.5))\n    plt.suptitle(str_title, fontsize=20, fontweight='bold')\n    n=231\n\n    for clf in list_classifiers : \n        plt.subplot(n)\n        plot_confusion_matrix(y_true, dict_all_pred[clf])\n        plt.title(clf, fontweight='bold')\n        n+=1\n\n    plt.tight_layout()\n    plt.subplots_adjust(top=0.9)\n    ","ff19727f":"plot_all_confusion_matrices(label_test, dict_pred, \"Pipelines v1, scoring=accuracy\")","8ac88b5b":"dict_acc = {}\nfor clf in list_clf :\n    dict_acc[clf] = accuracy_score(label_test, dict_pred[clf])","3b9b48f3":"for clf in list_clf :\n    print(clf, \" \" , dict_acc[clf])","f5a9710a":"for clf in list_clf :\n    print(clf, \" \", precision_score(label_test, dict_pred[clf]))","49782d91":"for clf in list_clf :\n    print(clf, \" \", precision_score(label_test, dict_pred[clf], average=None, labels=[0,1]))","db36b7fe":"for clf in list_clf :\n    print(clf, \" \", recall_score(label_test, dict_pred[clf]))","5e0c0704":"for clf in list_clf :\n    print(clf, \" \", recall_score(label_test, dict_pred[clf], average=None, labels=[0,1] ))","99d09961":"for clf in list_clf :\n    print(clf, \" \", f1_score(label_test, dict_pred[clf]))","ad396a7e":"for clf in list_clf :\n    print(clf, \" \", f1_score(label_test, dict_pred[clf], average=None, labels=[0,1] ))","1903476e":"print(classification_report(label_test, pred_test_MNB))","182cc0c5":"for clf in list_clf :\n    print(clf, \" \", precision_recall_fscore_support(label_test, dict_pred[clf], average=None, labels=[0,1] ))","9c378810":"for clf in list_clf :\n    print(clf, \" \", roc_auc_score(label_test, dict_pred[clf] ))","834976d5":"import sklearn.metrics\nsklearn.metrics.SCORERS.keys()","896c54e3":"scoring = 'precision'","819864cf":"grid_KNN_2 = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_KNN_2.fit(X=sms_train, y=label_train)\npred_test_grid_KNN_2 = grid_KNN_2.predict(sms_test)","22aadd83":"grid_KNN_2.best_params_","39cd8ef0":"grid_SVC_2 = GridSearchCV(pipe_SVC, parameters_SVC, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_SVC_2.fit(X=sms_train, y=label_train)\npred_test_grid_SVC_2 = grid_SVC_2.predict(sms_test)","d6e00df2":"grid_SGD_2 = GridSearchCV(pipe_SGD, parameters_SGD, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_SGD_2.fit(X=sms_train, y=label_train)\npred_test_grid_SGD_2 = grid_SGD_2.predict(sms_test)","dba7c6f6":"grid_GBC_2 = GridSearchCV(pipe_GBC, parameters_GBC, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_GBC_2.fit(X=sms_train, y=label_train)\npred_test_grid_GBC_2 = grid_GBC_2.predict(sms_test)","fe1ca383":"grid_XGB_2 = GridSearchCV(pipe_XGB, parameters_XGB, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_XGB_2.fit(X=sms_train, y=label_train)\npred_test_grid_XGB_2 = grid_XGB_2.predict(sms_test)","40e06ce9":"list_clf = [\"MNB\", \"KNN_2\", \"SVC_2\", \"SGD_2\", \"GBC_2\", \"XGB_2\"]\n\nlist_pred = [pred_test_MNB, pred_test_grid_KNN_2, \n             pred_test_grid_SVC_2, pred_test_grid_SGD_2,\n             pred_test_grid_GBC_2, pred_test_grid_XGB_2]\n\ndict_pred_2 = dict(zip(list_clf, list_pred))","aa30263d":"plot_all_confusion_matrices(label_test, dict_pred_2, \"Pipelines v2, scoring=precision\")","f6d23523":"scoring = 'recall'","92663f29":"grid_KNN_3 = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_KNN_3.fit(X=sms_train, y=label_train)\npred_test_grid_KNN_3 = grid_KNN_3.predict(sms_test)","d4b8c4d7":"grid_KNN_3.best_params_","06cb5237":"grid_SVC_3 = GridSearchCV(pipe_SVC, parameters_SVC, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_SVC_3.fit(X=sms_train, y=label_train)\npred_test_grid_SVC_3 = grid_SVC_3.predict(sms_test)","9bb08f8e":"grid_SGD_3 = GridSearchCV(pipe_SGD, parameters_SGD, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_SGD_3.fit(X=sms_train, y=label_train)\npred_test_grid_SGD_3 = grid_SGD_3.predict(sms_test)","e49bf504":"grid_GBC_3 = GridSearchCV(pipe_GBC, parameters_GBC, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_GBC_3.fit(X=sms_train, y=label_train)\npred_test_grid_GBC_3 = grid_GBC_3.predict(sms_test)","68918852":"grid_XGB_3 = GridSearchCV(pipe_XGB, parameters_XGB, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_XGB_3.fit(X=sms_train, y=label_train)\npred_test_grid_XGB_3 = grid_XGB_3.predict(sms_test)","4d8c0a02":"list_clf = [\"MNB\", \"KNN_3\", \"SVC_3\", \"SGD_3\", \"GBC_3\", \"XGB_3\"]\n\nlist_pred = [pred_test_MNB, pred_test_grid_KNN_3, \n             pred_test_grid_SVC_3, pred_test_grid_SGD_3,\n             pred_test_grid_GBC_3, pred_test_grid_XGB_3]\n\ndict_pred_3 = dict(zip(list_clf, list_pred))","dd20a975":"plot_all_confusion_matrices(label_test, dict_pred_3, \"Pipelines v3, scoring=recall\")","198192dd":"scoring = 'roc_auc'","2f6d5c41":"grid_KNN_4 = GridSearchCV(pipe_KNN, parameters_KNN, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_KNN_4.fit(X=sms_train, y=label_train)\npred_test_grid_KNN_4 = grid_KNN_4.predict(sms_test)","7e47e22c":"grid_KNN_4.best_params_","52f09528":"from sklearn.metrics import roc_curve, auc\n\nfpr, tpr, thr = roc_curve(label_test, grid_KNN_4.predict_proba(sms_test)[:,1])\nplt.figure(figsize=(5, 5))\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Plot')\nauc_knn4 = auc(fpr, tpr) * 100\nplt.legend([\"AUC {0:.3f}\".format(auc_knn4)]);","ecf6047f":"grid_SVC_4 = GridSearchCV(pipe_SVC, parameters_SVC, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_SVC_4.fit(X=sms_train, y=label_train)\npred_test_grid_SVC_4 = grid_SVC_4.predict(sms_test)","5f61cbea":"grid_SGD_4 = GridSearchCV(pipe_SGD, parameters_SGD, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_SGD_4.fit(X=sms_train, y=label_train)\npred_test_grid_SGD_4 = grid_SGD_4.predict(sms_test)","baa02b58":"grid_GBC_4 = GridSearchCV(pipe_GBC, parameters_GBC, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_GBC_4.fit(X=sms_train, y=label_train)\npred_test_grid_GBC_4 = grid_GBC_4.predict(sms_test)","d4ccedf7":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thr = roc_curve(label_test, grid_GBC_4.predict_proba(sms_test)[:,1])\nplt.figure(figsize=(5, 5))\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Plot')\nauc_gbc4 = auc(fpr, tpr) * 100\nplt.legend([\"AUC {0:.3f}\".format(auc_gbc4)]);","c0adbe8e":"grid_XGB_4 = GridSearchCV(pipe_XGB, parameters_XGB, cv=5,\n                          scoring=scoring, n_jobs=-1, verbose=1)\n\ngrid_XGB_4.fit(X=sms_train, y=label_train)\npred_test_grid_XGB_4 = grid_XGB_4.predict(sms_test)","278b7162":"list_clf = [\"MNB\", \"KNN_4\", \"SVC_4\", \"SGD_4\", \"GBC_4\", \"XGB_4\"]\n\nlist_pred = [pred_test_MNB, pred_test_grid_KNN_4, \n             pred_test_grid_SVC_4, pred_test_grid_SGD_4,\n             pred_test_grid_GBC_4, pred_test_grid_XGB_4]\n\ndict_pred_4 = dict(zip(list_clf, list_pred))","331b9437":"plot_all_confusion_matrices(label_test, dict_pred_4, \"Pipelines v4, scoring=roc auc\")","1b6ecfb6":"from nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords","0016346b":"print(data['text'][7])","a1a78e61":"print(sent_tokenize(data['text'][7]))","0d04e213":"print(word_tokenize(data['text'][7]))","4c9a123a":"stopWords = set(stopwords.words('english'))\nwords = word_tokenize(data['text'][7])\nwordsFiltered = []\n\nfor w in words:\n    if w not in stopWords:\n        wordsFiltered.append(w)\n\nprint(wordsFiltered)","1e6fd566":"**best_params_**","c2eb3a83":"**Basic preprocessing for common NLP tasks includes converting text to lowercase and removing punctuation and stopwords.**  \n**Further steps, especially for text classification tasks, are:**  \n* Tokenization\n* Vectorization and \n* TF-IDF weighting  \n\n**Lets apply these approaches on the SMS messages.**","31514ba7":"number of none zero entries divided by matrix size  ","50c12ddd":"KNN","87e4ad6f":"### Punctuation\n**We use the punctuation list from the string library:**","83c11c32":"Lets look at some vectorization examples for spam and ham messages","a31c008f":"The support is the number of occurrences of each class in y_true.","ecd395bd":"### Add numerical label for spam   \nTarget must be numerical for ML classification models","800e208a":"**test score**","f3297587":"### The Bag of Words representation","8e5f2a6a":"**With the above lists for punctuation characters and stop words, we define a function to remove these from the text**  \n**This function also converts all text to lowercase**","d8ee3165":"### 3.5 Optimize classifiers with scoring by precision","7553e561":"### 2.2.1 Collections: Counter","38b9ea64":"**simple Pipeline. no optimization**","6275a458":"### 2.2.2 NLTK: FreqDist","7e6a3d92":"**Sparsity: percentage of none zero entries**  \nhttps:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction  \n**Sparsity**  \nAs most documents will typically use a very small subset of the words used in the corpus,  \nthe resulting matrix will have many feature values that are zeros (typically more than 99% of them).  \nFor instance a collection of 10,000 short text documents (such as emails) will use a vocabulary  \nwith a size in the order of 100,000 unique words in total while each document will use 100 to   \n1000 unique words individually.  \nIn order to be able to store such a matrix in memory but also to speed up algebraic operations matrix \/ vector,  \nimplementations will typically use a sparse representation such as available in the scipy.sparse package.\n","1259e16f":"for text data","0cf35748":"For spam detection optimizing tbe classifiers by precision seems most reasonable.  \nBut for other tasks it may be advantageous to have a classifier with maximum recall.  \nFor example, in Credit Card Fraud detections, you want to find all fraud samples.  \nFor all classifier pipelines, we perform GridSearchCV again, using the same parameter grids  \nand only changing the scoring method to \"recall\".","e5f75ccb":"The dataset contains 4825 ham and 747 spam messages.  \nFor both classes, some messages appear more than once (common phrases, etc.).","3816941e":"For a binary classification task, there are 4 possible results:\n\n\nTN: True negatives  (ham mails labeled as ham)  \nFP: False positives (ham mails labeled as spam)  \nFN: False negatives (spam mails labeled as ham)  \nTP: True positives  (spam mails labeled as spam)  ","c03cef42":"GBC","5f496a38":"\n[**Part 3: Classifiers**](#Part-3:-Classifiers)  \n[**3.1 First test for Classification**](#3.1-First-test-for-Classification) with Naive Bayes Classifier  \n[**3.2 train test split**](#3.2-train-test-split)  \n**3.3 Pipelines for Classification of unknown messages**  \n[Multinomial Naive Bayes](#3.3.1-MultinomialNB)  (simple: Preprocessing and Classification)  \n[KNN Classifier](#3.3.2-KNN)  (GridSearchCV for model parameter)   \n[Support Vector Classifier](#3.3.3-SVC)  (GridSearchCV for Preprocessing)  \n[SGD Classifier](#3.3.4-SGD)  (GridSearchCV for Preprocessing and model parameter)  \n[GradientBoostingClassifier](#3.3.5-GradientBoostingClassifier)    (GridSearchCV for Preprocessing and model parameter)  \n[XGBoost Classifier](#3.3.6-XGBoost-Classifier)    (GridSearchCV for Preprocessing and model parameter)  \n**3.4 Comparison of results**  \n[confusion_matrix](#confusion_matrix) +++ [accuracy_score](#accuracy_score)       \n[precision_score](#precision_score) +++ [recall_score](#recall_score)  \n[f1_score](#f1_score) +++  [classification_report](#classification_report)    \n[roc_auc_score](#roc_auc_score)  \n**3.5 Optimize classifiers with scoring by precision**  \n3.5.1 [GridSearchCV pipelines version 2](#3.5.1-GridSearchCV-pipelines-version-2)  \n3.5.2 [Confusion matrices for scoring by precision](#3.5.2-Confusion-matrices-for-scoring-by-precision)  \n**3.6 Optimize classifiers with scoring by recall**  \n3.6.1 [GridSearchCV pipelines version 3](#3.6.1-GridSearchCV-pipelines-version-3)  \n3.6.2 [Confusion matrices for scoring by recall](#3.6.2-Confusion-matrices-for-scoring-by-recall)  \n**3.7 Optimize classifiers with scoring by roc_auc**  \n3.7.1 [GridSearchCV pipelines version 4](#3.7.1-GridSearchCV-pipelines-version-4)    \n3.7.2 [Confusion matrices for scoring by roc auc](#3.7.2-Confusion-matrices-for-scoring-by-roc-auc)  \n\n[**Part 4: NLTK**](#Part-4:-NLTK)\n","cac9ce81":"SVC","a0d30417":"KNN","5a99acb9":"### 1.2 length of message","94b6b118":"### 2.2 Top 30 words in ham and spam messages","08423978":"# Part 4: NLTK","5c59190e":"**test score**","0231eccb":"MNB Model using only TFIDF matrix, scaled","9b9176db":"MNB model with TFIDF matrix and feature \"length\", scaled","bb39b650":"## 3.2 train test split","f9ed51cc":"## Functions","39f89771":"Columns 2,3,4 contain no important data and can be deleted.  \nAlso, we rename column v1 as \"label\" and v2 as \"text\"","3ef243cb":"### 3.3.2 KNN  \nPipeline with GridSearchCV  \noptimize best model parameter: n_neighbors","0d43a644":"**best_params_**","d4eaf022":"For futher details and example implementations see:  \nhttps:\/\/en.wikipedia.org\/wiki\/Bag-of-words_model  \nhttps:\/\/en.wikipedia.org\/wiki\/Document-term_matrix  \n\nAn Introduction to Bag-of-Words in NLP  \nhttps:\/\/medium.com\/greyatom\/an-introduction-to-bag-of-words-in-nlp-ac967d43b428","79d4858e":"XGB","bf90e897":"## 2.2 Bag of words with CountVectorizer","6255c763":"SVC","ab7dd37b":"GBC","01ea33d2":"MNB Model using only TFIDF matrix","3bc8a377":"### 3.7.2 Confusion matrices for scoring by roc auc","d69b0e0c":"**Dictionary of predictions**","b3343fd4":"### WordCloud: Ham messages","05e23f99":"### 3.3.4 SGD  \nPipeline with GridSearch  \nsearch best preprocessing: use_idf (yes\/no)  \nand best model parameters (alpha, penalty)","dabc7a1f":"### 3.5.2 Confusion matrices for scoring by precision","ab842bab":"**Part 0: Imports, define functions**  \n[import libraries](#Imports)  \ndefine [functions](#Functions) that are used often  \n\n[**Part 1: Exploratory Data Analysis**](#Part-1:-EDA)  \n**1.1 Get an overview of the dataset**  \nhead, describe and value counts  \n[Distribution of the target variable](#Distribution-of-the-target-variable)  \n[Add numerical label for spam](#Add-numerical-label-for-spam)  \n**1.2 length of message**  \n[Add feature: length of message](#Add-feature:-length-of-message)  \n**1.3 WordClouds**  \n[WordCloud: Ham messages](#WordCloud:-Ham-messages)  \n[WordCloud: Spam messages](#WordCloud:-Spam-messages)  \n\n[**Part 2: Preprocessing**](#Part-2:-Preprocessing)  \n[**2.1 Remove punctuation and stopwords**](#2.1-Remove-Punctuation-and-Stopwords)   \n[**2.2 Top 30 words in ham and spam messages**](#2.2-Top-30-words-in-ham-and-spam-messages)  \n**2.3 Bag of words with CountVectorizer**  \n[The Bag of Words representation](#The-Bag-of-Words-representation)  \n[Examples for spam and ham messages](#Examples-for-spam-and-ham-messages)  \n[Applying bow_transformer on all messages](#Applying-bow_transformer-on-all-messages)  \n**2.4 Term frequency inverse document frequency (TFIDF)**  \n[From occurrences to frequencies](#From-occurrences-to-frequencies)  \n[TfidfTransformer from sklearn](#TfidfTransformer-from-sklearn)  ","bc2c449f":"### precision_recall_fscore_support","9a349cb9":"### Distribution of the target variable","bee0faf7":"### Applying bow_transformer on all messages","45fa60d4":"### f1_score","e8e409f5":"# **Part 0: Imports, define functions** ","033016b1":"Comparing the accuracy_score with the confusion matrices, one finds that  \naccuracy score may not be the best parameter to choose the best classifier.  \nSGD, a model with high accuracy_score, incorrectly classifies 6 ham mails as spam,  \nwhich is usually not wanted for a spam classifier (important mails might get lost).  \nMNB has less accuracy than SGD, but it classifies all ham mails correctly.  \nSVC also classifies all ham mails correctly but compared to MNB it classifies   \nmuch more spam mails correctly.  \nApart from accuracy there are further scoring methods to evaluate a classifier.  \nLets look at the other classifier scores in more detail:  \nprecision, recall, fscore, support, roc_auc","a6d0ab6f":"MNB model with TFIDF matrix and feature \"length\", unscaled","afe5d946":"KNN","81a44cb4":"**best_params_**","add6bf1c":"**cross validation score: best_score_**","f0924062":"After splitting the data into a train and test set we now use a pipeline to apply the   \n**CountVectorizer** and the **TfidfTransformer** on both sets.  \nWe also add a classifier to the pipeline, so we can combine all necessary steps in one object:  \n* Preprecocessing  \n* Crossvalidation (GridsearchCV)\n* Fitting  \n* Predicting\n* Evaluating (test score)","d933f1b2":"### classification_report","566fcdcd":"XGB","3b113fa9":"Looks like spam messages are generally longer than ham messages:  \nBulk of ham has length below 100, for spam it is above 100.  \nWe will check if this feature is useful for the classification task in Part 3.  ","e6ac0ee4":"### roc_auc_score","f0c18721":"The precision is the ratio TP \/ (TP + FP) where TP is the number of true positives and TP the number of false positives.  \nThe precision is intuitively the ability of the classifier not to label as positive a sample that is negative.  \nPrecision = 1 for FP = 0 and precision goes up when FP goes down.","9a44f136":"Precision for classifying ham mails is 1.0 for the MNB and SVC classifier.  \nSGD has the best precision for classifying ham mails.","345a243e":"**References:**  \nParts of the EDA and preprocessing are based on the Capstone Project in Jose Portilla's Udemy course.  \nI can recommend this course for beginners in Python ML.  \n","6528ba5a":"https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_precision_recall.html","cec73747":"## 3.3 Classification Pipelines","655bec37":"# **Part 2: Preprocessing**","dee321ca":"### 3.6 Optimize classifiers with scoring by recall","fd3cf1bc":"## 3.4 Comparison of results","359305c4":"SGD","7b5b54c8":"## train test split","934d8dc1":"### WordCloud: Spam messages","82ce1cb3":"### Examples for spam and ham messages","acff98ab":"**cross validation score: best_score_**","d301ba63":"### Naive Bayes Classifier","fb4d9c7d":"### 3.5.1 GridSearchCV pipelines version 2","aafb75c3":"print Classification Report and Accuracy","ab7a032c":"Around 10% of the matrix are non zeros (=ones)","29fc6e1b":"### 3.3.5 GradientBoostingClassifier","5234c2fc":"### 3.6.2 Confusion matrices for scoring by recall","8f188014":"The confusion matrix gives an overview of the classification results:  \nThe diagonal elements represent the number of points for which the predicted label is equal to the true label,  \nwhile off-diagonal elements are those that are mislabeled by the classifier.  \nThe higher the diagonal values of the confusion matrix the better, indicating many correct predictions.  \nThe rows of a confusion matrix correspond to the true (actual) classes and the columns correspond to the predicted classes.  \nSo, all together the confusion matrix for a binary classifier consists of 4 values:","46ef03f1":"**Yes, results are identical**","6b2c8e12":"**test score**","9bc1e4a2":"# **SMS: Spam or Ham (Beginner)**","4ac10298":"**NLTK**","95e0c8c2":"### 3.6.1 GridSearchCV pipelines version 3","a2ebdbf2":"## Imports","dfb82a85":"sparse matrix to matrix","215431f6":"### 1.3 WordClouds","dcbec021":"TODO:  \nparamgrid for MNB","87942bc9":"For my first kernel on Natural Language Processing (NLP), I chose the SMS Spam Collection Dataset.  \nIt contains  the text of 5572 SMS messages and a label, classifying the message as \"spam\" or \"ham\".\n\nIn this kernel I explore some common techniques of NLP like:\n\n* **Removing Punctuation and Stopwords**\n* **Tokenizer, Bag of words**  \n* **Term frequency inverse document frequency (TFIDF)**\n\nBased on these preprocessing, I train 6 different models that classify **unknown** messages as spam or ham. \n\n* **Naive Bayes Classifier**\n* **SVM Classifier**  \n* **KNN Classifier**\n* **SGD Classifier**\n* **Gradient Boosting Classifier**\n* **XGBoost Classifier**\n\nFor easier handling of the preprocessing steps (for train and test data) and the optimization of different  \nmodels for the same conditions, the classification is done with **Pipelines** including GridSearchCV.  \nFinally, for the model evaluation different **metrics** are examined:  \naccuracy, precision, recall, fscore, roc_auc","1929604d":"We studied the same classifier, Multinomial Naive Bayes, with different set of features and found that the results vary regarding the accuracy of the predictions.  \nIn the following we study a different classifier, again with different set of features.\nAlso we study what this accuracy actually means and also if this metric is the optimal one we should apply for this task.","c135cf7a":"Recall is defined regarding the positive class (label=1, spam mails).  \nAgain, if we call the recall score method with the labels parameter, we get  \nthe recall for ham and spam messages:","cde4f986":"## 2.1 Remove Punctuation and Stopwords","4361b69e":"**cross validation score: best_score_**","b09f850b":"https:\/\/www.kaggle.com\/adamschroeder\/countvectorizer-tfidfvectorizer-predict-comments","01d2296c":"The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall,  \nwhere an F-beta score reaches its best value at 1 and worst score at 0.  \nThe F-beta score weights recall more than precision by a factor of beta.  \nbeta == 1.0 means recall and precision are equally important.","13867a92":"MNB","917f0d2f":"**Using seaborn heat map for nice plot of confusion matrix**","8fadc64a":"### 3.3.1 MultinomialNB","27096ee1":"### 3.7 Optimize classifiers with scoring by roc_auc_score","9c31afeb":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQUXi3mkDlIZMmaGJzZVQnEEC535eNtp3WbO5HzZMxhCcUwucLo)","1557f354":"KNN","d999f3dc":"In this kernel we apply the CountVectorizer from sklearn as BOW model.  \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html    \nAs tokenizer we use the remove_punctuation_and_stopwords function defined above","8771d80b":"### TfidfTransformer from sklearn\nBoth tf and tf\u2013idf can be computed as follows using TfidfTransformer:   \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfTransformer.html","acfaf6f5":"SGD","0d61b40b":"The two steps  \n**CountVectorizer** and **TfidfTransformer**  \ncan also be performed in one step with  \n**TfidfVectorizer**  \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html  \nConvert a collection of raw documents to a matrix of TF-IDF features  \nEquivalent to CountVectorizer followed by TfidfTransformer.","dbabb0b6":"### 3.3.6 XGBoost Classifier","998ea5e6":"https:\/\/towardsdatascience.com\/understanding-data-science-classification-metrics-in-scikit-learn-in-python-3bc336865019  \n\nhttps:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html","a7771e7d":"MNB","f348c96b":"In all sms messages bow_transformer counted 9431 different words.","0456434c":"TN FP  \nFN TP\n","335249ba":"Our first classifier seems to work well, it has an accuracy of 96.5 % for the test set.  ","e93f5c7f":"# Part 3: Classifiers","c3b60673":"### 3.7.1 GridSearchCV pipelines version 4","22a739e6":"Fitting MNB with the unscaled features TFIDF + length of message decreases performance.  \nLets now check the fit with the scaled features.","a7705fce":"### From occurrences to frequencies  \nhttps:\/\/scikit-learn.org\/stable\/tutorial\/text_analytics\/working_with_text_data.html#from-occurrences-to-frequencies\n\nOccurrence count is a good start but there is an issue: longer documents will have higher average count values  \nthan shorter documents, even though they might talk about the same topics.  \nTo avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document  \nby the total number of words in the document: these new features are called **tf for Term Frequencies**.  \nAnother refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are  \ntherefore less informative than those that occur only in a smaller portion of the corpus.  \nThis downscaling is called **tf\u2013idf for \u201cTerm Frequency times Inverse Document Frequency\u201d**.","c407caa7":"# **Part 1: EDA**","ffc20aac":"We perform GridSearchCV again, using the same parameter grids and pipelines like before.  \nFor all classifier pipelines, we only change the scoring method from \"accuracy\" to \"precision\".","002cc47a":"for TFIDF matrix and feature \"length\"","c19890cb":"Applying the min max scaler on the TFIDF matrix improves the performance of the MNB classifier:  \nIt now has an accuracy of 98.2 % for the test set. ","205e046c":"## 2.3 Term frequency inverse document frequency - TFIDF","f6db8727":"SVC","92f067bb":"### accuracy_score","458bcaa2":"**The Notebook follows this outline:**  ","2949e79f":"https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#text-feature-extraction  \n\nText Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.  \nIn order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n\n**Tokenization**  \ntokenizing strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.  \n**Vectorization**  \ncounting the occurrences of tokens in each document.  \n**TF-IDF**  \nnormalizing and weighting with diminishing importance tokens that occur in the majority of samples \/ documents.  \n\n\n**Bag of Words**  \nIn this scheme, features and samples are defined as follows:\neach individual token occurrence frequency (normalized or not) is treated as a feature.  \nthe vector of all the token frequencies for a given document is considered a multivariate sample.  \nA corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus.  \nWe call vectorization the general process of turning a collection of text documents into numerical feature vectors.   \nThis specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \u201cBag of n-grams\u201d representation.  \nDocuments are described by word occurrences while completely ignoring the relative position information of the words in the document.","11de024e":"**TODO : **  \n \n\n**include feature text length in model**   \n\n**NLTK**","b8dc3cee":"### recall_score","2aeb20e2":"### Stopwords  \nfrom sklearn documentation:  https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#stop-words  \nStop words are words like \u201cand\u201d, \u201cthe\u201d, \u201chim\u201d, which are presumed to be uninformative in representing the content of a text,   \nand which may be removed to avoid them being construed as signal for prediction.  \nSometimes, however, similar words are useful for prediction, such as in classifying writing style or personality.  \n\nDue to the known issues in the \u2019english\u2019 stop word list of sklearn, we use the stopwords from NLTK:","d93b48c9":"classification accuracy = correct predictions \/ total predictions = (TP + TN) \/ (TP + TN + FP + FN)\n\n","195fb08e":"The recall is the ratio TP \/ (TP + FN) where TP is the number of true positives and FN the number of false negatives.  \nThe recall is intuitively the ability of the classifier to find all the positive samples.","98b3f2a3":"SGD","9b2fcbb4":"By definition the precision is calculated for the negative class (label = 0, ham mails).  \nThis is also the default when calling precision score without any further parameters.  \nBut we can also examine the precision for the individual labels (ham,spam = 0,1)","e1090360":"## 3.1 First test for Classification  ","3cd784ab":"### 3.3.3 SVC  \nPipeline with GridSearchCV  \nsearch best preprocessing: apply TfidfTransformer (yes\/no)","6e321c24":"**test score**","b0d2844f":"plot_confusion_matrix","03c7b0d6":"XGB","162a0cab":"### confusion_matrix","dc5b6156":"### Add feature: length of message","20bdd9ae":"### precision_score","348cab68":"For futher details and example implementations see:  \nhttps:\/\/en.wikipedia.org\/wiki\/Tf%E2%80%93idf","08851cdc":"MNB","56a9e824":"for TFIDF matrix only","36c3463e":"The precision for MNB was already 1.0 so it can not be improved.","cba1ee69":"GBC"}}