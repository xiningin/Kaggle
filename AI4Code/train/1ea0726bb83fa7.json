{"cell_type":{"8d82aa10":"code","9fe8661f":"code","ebf2b311":"code","64bfffca":"code","045b1baa":"code","25414ba0":"code","943b81c6":"code","eab776d5":"code","275cf3fc":"code","89034287":"code","0a66d822":"code","3f453268":"code","68296ef3":"code","b8a8c216":"code","776e54b0":"code","a0360c8e":"code","4c98eac5":"code","ff2d1182":"code","3e6a0d53":"code","d3b608d2":"markdown","2e4d8b45":"markdown","d56aeed8":"markdown","38372f58":"markdown","b6e81194":"markdown","1b07776a":"markdown","11b1e10a":"markdown","db3f85e1":"markdown","0cb65a73":"markdown","50871a53":"markdown","8626a518":"markdown","94df872c":"markdown"},"source":{"8d82aa10":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport itertools\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split\nimport warnings\nwarnings.simplefilter('ignore')\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom mlxtend.classifier import StackingCVClassifier","9fe8661f":"SEED = 1992\nPROBAS = True\nFOLDS = 5\n\nTARGET = 'Survived'","ebf2b311":"train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\npseudo_labels = pd.read_csv(\"..\/input\/tps04preds\/dae.csv\")\ntest[TARGET] = pseudo_labels[TARGET]\n\nall_df = pd.concat([train, test]).reset_index(drop=True)\ntest['Survived'] = [x for x in pseudo_labels.Survived]\nall_df = pd.concat([train, test]).reset_index(drop=True)","64bfffca":"#Checking the null Data\nnull_data = (train.isna().sum().sort_values(ascending=False) \/ len(train) * 100)[:6] \nfig, ax = plt.subplots(1,1,figsize=(10, 7)) \nax.bar(null_data.index, 100, color='#dadada', width=0.6) \nbar = ax.bar(null_data.index,null_data, width=0.6) \nax.bar_label(bar, fmt='%.01f %%') \nax.spines.left.set_visible(False) \nax.set_yticks([]) \nax.set_title('Null Data Ratio', fontweight='bold') \nplt.show()","045b1baa":"#Age fillna with mean age for each class\nall_df['Age'] = all_df['Age'].fillna(all_df['Age'].mean())\n\n# Cabin, fillna with 'X' and take first letter\nall_df['Cabin'] = all_df['Cabin'].fillna('X').map(lambda x: x[0].strip())\n\n# Ticket, fillna with 'X', split string and take first split \nall_df['Ticket'] = all_df['Ticket'].fillna('X').map(lambda x:str(x).split()[0] if len(str(x).split()) > 1 else 'X')\n\n# Fare, fillna with mean value\nfare_map = all_df[['Fare', 'Pclass']].dropna().groupby('Pclass').median().to_dict()\nall_df['Fare'] = all_df['Fare'].fillna(all_df['Pclass'].map(fare_map['Fare']))\nall_df['Fare'] = np.log1p(all_df['Fare'])\n\n# Embarked, fillna with 'X' value\nall_df['Embarked'] = all_df['Embarked'].fillna('X')\n\n# Name, take only surnames\nall_df['Name'] = all_df['Name'].map(lambda x: x.split(',')[0])","25414ba0":"#The Feature Engineering Results\nall_df.shape","943b81c6":"label_cols = ['Name', 'Ticket', 'Sex']\nonehot_cols = ['Cabin', 'Embarked']\nnumerical_cols = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n\ndef label_encoder(c):\n    le = LabelEncoder()\n    return le.fit_transform(c)\n\nscaler = StandardScaler()\n\nonehot_encoded_df = pd.get_dummies(all_df[onehot_cols])\nlabel_encoded_df = all_df[label_cols].apply(label_encoder)\nnumerical_df = pd.DataFrame(scaler.fit_transform(all_df[numerical_cols]), columns=numerical_cols)\ntarget_df = all_df[TARGET]\n\n#Remove Duplicates\nall_df = all_df.loc[~all_df.index.duplicated(keep='first')]\n\n#Concat all dataframes\nall_df = pd.concat([numerical_df, label_encoded_df, onehot_encoded_df, target_df], axis=1)","eab776d5":"X = all_df.drop([TARGET], axis = 1)\ny = all_df[TARGET]\n\nprint (f'X:{X.shape} y: {y.shape} \\n')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state = SEED)\nprint (f'X_train:{X_train.shape} y_train: {y_train.shape}')\nprint (f'X_test:{X_test.shape} y_test: {y_test.shape}')","275cf3fc":"test = all_df[len(train):].drop([TARGET], axis = 1)\nprint (f'test:{test.shape}')","89034287":"lgbm_params = {\n    'metric': 'binary_logloss',\n    'n_estimators': 9000,\n    'objective': 'binary',\n    'random_state': SEED,\n    'learning_rate': 0.02,\n    'min_child_samples': 150,\n    'reg_alpha': 3e-5,\n    'reg_lambda': 9e-2,\n    'num_leaves': 20,\n    'max_depth': 16,#16\n    'colsample_bytree': 0.8,\n    'subsample': 0.7,\n    'subsample_freq': 2,\n    'max_bin': 240,\n    'device':'gpu'\n}\n\ncat_params = {#'iterations': 5000,\n          'eval_metric': 'AUC',\n          'loss_function':'Logloss',\n          'od_type':'Iter',\n          'num_trees':50000,\n          'max_depth': 6, \n          'l2_leaf_reg': 3,\n          'bootstrap_type': 'Bayesian',\n          'max_bin': 254,\n          'grow_policy': \"Lossguide\",\n          'random_seed': 314,\n          'min_data_in_leaf': 64,\n          'verbose': None,\n          'logging_level': 'Silent',\n          'task_type': 'GPU'\n}\n\nETC_params = {\n    'bootstrap':True,\n    'criterion': 'entropy',\n    'max_features': 0.55,\n    'min_samples_leaf': 8,\n    'min_samples_split': 4,\n    'n_estimators': 100\n}\n\nrf_params = {\n    'max_depth': 15,\n    'min_samples_leaf': 8,\n    'random_state': SEED\n}","0a66d822":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout,Reshape \nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier #To teat ANN as classifier\n\ndef ann_network():\n    i = Input(shape=(None, 32, 6))\n    x = Dense(60, activation='relu')(i)  \n    x = Dense(1, activation='sigmoid')(x) \n    model = Model(i, x)\n    \n    opt = tf.keras.optimizers.SGD(lr=1e-4, decay=1e-6, momentum=0.8, nesterov=True)\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=opt,\n        metrics=[\"accuracy\"]\n    )\n    \n    return model\n\n","3f453268":"from mlxtend.classifier import StackingCVClassifier,EnsembleVoteClassifier\nfrom sklearn.linear_model import LogisticRegression\n\ncl1 = CatBoostClassifier(**cat_params)\ncl2 = LGBMClassifier(**lgbm_params)\ncl3 = ExtraTreesClassifier(**ETC_params)\n\nmlr = LogisticRegression()\n\nann_clf = KerasClassifier(lambda: ann_network(), epochs=4)\n\n# ANN Ensembling\nclf = StackingCVClassifier(classifiers= [cl1,cl2,cl3], \n                            meta_classifier = ann_clf, \n                            use_probas = True, \n                            random_state = SEED) \n\n# Hard Voting Ensemble\nS_eclf = EnsembleVoteClassifier(clfs=[cl1, cl2, cl3],\n                              weights=[1, 1, 2], voting='soft')\n\n#Soft Voting Ensemble\nH_eclf = EnsembleVoteClassifier(clfs=[cl1, cl2, cl3],\n                              weights=[1, 1, 3], voting='hard')\n\n#PseudoMeta classifier\nAnnStakced_clf =  StackingCVClassifier(classifiers= [cl1,cl2,cl3],\n                            meta_classifier = S_eclf, \n                            use_probas = True,    \n                            random_state = SEED) \n\nclassifiers = [clf,H_eclf,S_eclf,AnnStakced_clf]\n\n# Fit the classifier variations\nclf.fit(X_train, y_train) \nH_eclf.fit(X_train, y_train) \nS_eclf.fit(X_train, y_train)\nAnnStakced_clf.fit(X_train, y_train) ","68296ef3":"preds = pd.DataFrame()\nclassifiers = {\"stacked CLF\": clf,\n              'Soft voted CLF':S_eclf,\n               'Hard voted CLF':H_eclf,\n              'Ann Clf':AnnStakced_clf} \nNUM_CLAS = 4\nfor key in classifiers:\n    try:\n        y_pred = classifiers[key].predict_meta_features(X_test)[:,1]\n    except:\n        y_pred = classifiers[key].predict_proba(X_test)[:,1]\n    preds[f\"{key}\"] = y_pred\n    auc = metrics.roc_auc_score(y_test, y_pred)\n    print(f\"{key} -> AUC: {auc:.3f}\")\n\npreds[TARGET] = pd.DataFrame(y_test).reset_index(drop=True)\n\nprint(preds.sample(10))","b8a8c216":"sns.set(font_scale = 1)\nsns.set_style({\"axes.facecolor\": \"1.0\", \"axes.edgecolor\": \"0.85\", \"grid.color\": \"0.85\",\n               \"grid.linestyle\": \"-\", 'axes.labelcolor': '0.4', \"xtick.color\": \"0.4\",\n               'ytick.color': '0.4'})\n\nf, ax = plt.subplots(figsize=(13, 4), nrows=1, ncols = NUM_CLAS)\n\nfor key, counter in zip(classifiers, range(NUM_CLAS)):\n    \n    y_pred = preds[key]\n   \n    auc = metrics.roc_auc_score(y_test, y_pred)\n    textstr = f\"AUC: {auc:.3f}\"\n\n\n    false_pred = preds[preds[TARGET] == 0]\n    sns.distplot(false_pred[key], hist=True, kde=True, \n                 bins=int(50), color = 'red', \n                 hist_kws={'edgecolor':'black'}, ax = ax[counter])\n    \n\n    true_pred = preds[preds[TARGET] == 1]\n    sns.distplot(true_pred[key], hist=True, kde=True, \n                 bins=int(50), color = 'green', \n                 hist_kws={'edgecolor':'black'}, ax = ax[counter])\n    \n    \n    props = dict(boxstyle='round', facecolor='white', alpha=0.5)\n    \n    ax[counter].text(0.05, 0.95, textstr, transform=ax[counter].transAxes, fontsize=14,\n                    verticalalignment = \"top\", bbox=props)\n    \n    ax[counter].set_title(f\"{key}\")\n    ax[counter].set_xlim(0,1)\n    ax[counter].set_xlabel(\"Probability\")\n\nplt.tight_layout()","776e54b0":"# For ANN Classifier we can not use predict_proba or .predict. Instead of this .predict_meta_features\n\ntest_preds1 = clf.predict_meta_features(test)[:,1]\n#test_preds2 = H_eclf.predict_meta_features(test)[:,1]\n","a0360c8e":"# tip -> Alexander Ryzhkov\n\nthreshold = pd.Series(test_preds1).sort_values(ascending = False).head(34911).values[-1]\nthreshold=threshold\nprint(f\"Current threshold is: {threshold}\")\n\n# Creation of Multiple Submission for Voting\nsubmission['submit_1'] = (test_preds1 > threshold).astype(int)\nsubmission['submit_2'] = pseudo_labels[TARGET]\n","4c98eac5":"\nsubmission[[col for col in submission.columns if col.startswith('submit_')]].sum(axis = 1).value_counts()\n","ff2d1182":"submission[TARGET] = (submission[[col for col in submission.columns if col.startswith('submit_')]].sum(axis=1) >= 2).astype(int)\nsubmission[TARGET].mean()","3e6a0d53":"# Final File preparation\nsubmission[['PassengerId', TARGET]].to_csv(\"Mstasko_final.csv\", index = False)\nsubmission[\"Survived\"].hist()","d3b608d2":"# 3. Load TPS-04 competition data","2e4d8b45":"# 9. Plot Results","d56aeed8":"# 8. Predict and Validate (AUC)","38372f58":"# 10. Final Prediction","b6e81194":"# 5. Label Encoding","1b07776a":"# 1. Description","11b1e10a":"# 6. Create Train and Test Datasets","db3f85e1":"The ensemble technique works best when the base models are not correlated.\nWe have 3 basics concept of ensembling techniques <br> \n***Max Voting***<br>\nThe prediction from each model is a vote. In max voting the final prediction come from the most votes\n- classifier 1 \u2013 class A\n- classifier 2 \u2013 class B\n- classifier 3 \u2013 class B\n- Output:\u00a0 \u00a0  **Class B**\n\n***Averaging***<br>\nThe final output is an average of all predictons (regression problems)\n- regressor 1 \u2013 200\n- regressor 2 \u2013 300\u00a0\n- regressor 3 \u2013 400\n- Output:\u00a0 \u00a0 **300** \u00a0 \n\n***Weighted Averaging***<br>\nThe base model with higher predictive power is more important.\n- Output:\u00a0 \u00a0\u00a0 **Weighted Average*\n\n","0cb65a73":"# 4. Preprocess data\nPreprocessing Logic: [BIZEN](https:\/\/www.kaggle.com\/hiro5299834\/tps-apr-2021-pseudo-labeling-voting-ensemble) notebook (as a benchmark) to compare results.","50871a53":"# 7. Create Meta Classifier","8626a518":"# 2. Set up script parameters","94df872c":"## According to the ensemble inspiration from [Remek Kinas](https:\/\/www.kaggle.com\/remekkinas) I have explored the Ensembing but with the ANN instad of Logistic Regression. If you like the code or have some ideas to improve please upvote and comment :) "}}