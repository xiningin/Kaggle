{"cell_type":{"53b4f79b":"code","b90268de":"code","33fe72bd":"code","ee2e1ae1":"code","82ae4f60":"code","adfd7805":"code","b5f376fe":"code","64cd3878":"code","a8ac7b2d":"code","ed2a5031":"code","2dbf069f":"code","6a9be1c9":"code","71a418ba":"code","9478ab12":"code","d3f2f0fb":"code","5e498da1":"code","628a56da":"code","6d686b0e":"code","2e62221e":"code","e4c582ed":"code","76e69a9c":"code","b22aeace":"code","5adcfe48":"code","53131771":"code","5b32a4e4":"code","ced904e7":"code","5836940a":"code","7438a646":"code","d0c0bdd5":"code","3038257a":"code","928a3685":"code","aaa59855":"code","9e270449":"code","7e68f4d3":"code","b0590ea0":"code","566d5f48":"code","1703522c":"code","e40153ea":"code","fa9783e2":"code","13b1207d":"code","7ab8e9b2":"code","33d63d8a":"code","6ff5e8f1":"code","2bb7f070":"code","e5194038":"code","854fe3d5":"code","7ecc6874":"code","0bb89b20":"code","41e5c63c":"code","b4149cba":"code","3f17bc9f":"code","cd0ed9d3":"code","97513ae1":"code","d4e661a9":"code","5eb20e55":"code","6092898a":"code","48eeb929":"code","f99ae58e":"code","4928e0f7":"code","e4409201":"markdown","870a35a0":"markdown","740a3ca5":"markdown","0391035d":"markdown","c3cca73c":"markdown","819309f8":"markdown","07e3d418":"markdown","13e07a9b":"markdown","628b0dfe":"markdown","e711b604":"markdown","eac3a04d":"markdown","1c7c6179":"markdown","8e5c68f1":"markdown","6eb8b984":"markdown","ed10c8a9":"markdown","0825d3d5":"markdown","449b2738":"markdown","0ce35afc":"markdown","cbd3264b":"markdown","d00d1b71":"markdown","9ad95657":"markdown","318b8a90":"markdown","55584203":"markdown","8e0574e4":"markdown","8532966c":"markdown","b29c1bc6":"markdown","5d0ea871":"markdown","e79da483":"markdown","4c7415e9":"markdown"},"source":{"53b4f79b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b90268de":"import matplotlib.pyplot as plt\nimport seaborn as sns","33fe72bd":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline","ee2e1ae1":"from sklearn.cluster import KMeans","82ae4f60":"!ls","adfd7805":"df = pd.read_csv('\/kaggle\/input\/pump-sensor-data\/sensor.csv')\ndf.info()","b5f376fe":"del df['sensor_15']","64cd3878":"df.head()","a8ac7b2d":"df.info()","ed2a5031":"### one more unnecessary column\ndel df['Unnamed: 0']","2dbf069f":"\ndf.nunique() ## many repeating value here","6a9be1c9":"##df.hist(df.columns, bins=25, layout=(8,7), figsize=(20, 18))\n##plt.show() \n## not much info here ","71a418ba":"df.shape","9478ab12":"df = df.drop_duplicates()\ndf.shape","d3f2f0fb":"def calc_percent_NAs(df):\n    nans = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)\/len(df), columns=['percent']) \n    idx = nans['percent'] > 0\n    return nans[idx]\n\ncalc_percent_NAs(df).head(10)","5e498da1":"#better code is required\ndf['sensor_50'].fillna((df['sensor_50'].mean()), inplace=True)\ndf['sensor_51'].fillna((df['sensor_51'].mean()), inplace=True)\ndf['sensor_00'].fillna((df['sensor_00'].mean()), inplace=True)\ndf['sensor_08'].fillna((df['sensor_08'].mean()), inplace=True)\ndf['sensor_07'].fillna((df['sensor_07'].mean()), inplace=True)\ndf['sensor_06'].fillna((df['sensor_06'].mean()), inplace=True)\ndf['sensor_09'].fillna((df['sensor_09'].mean()), inplace=True)","628a56da":"calc_percent_NAs(df).head(10)","6d686b0e":"df = df.dropna()","2e62221e":"df.info()","e4c582ed":"df['timestamp'] = pd.to_datetime(df['timestamp'])\n","76e69a9c":"df = df.set_index('timestamp')","b22aeace":"df.head() ## our time series is ready!! congrats","5adcfe48":"df['machine_status'].unique()","53131771":"\ndfBroken = df[df['machine_status']=='BROKEN']\ndfSensors = df.drop(['machine_status'], axis=1)\nsensorNames=dfSensors.columns\nfor sensor in sensorNames:\n    sns.set_context('talk')\n    _ = plt.figure(figsize=(18,3))\n    _ = plt.plot(dfBroken[sensor], linestyle='none', marker='X', color='red', markersize=12)\n    _ = plt.plot(df[sensor], color='grey')\n    _ = plt.title(sensor)\n    plt.show()","5b32a4e4":"x = df[sensorNames]\nscaler = StandardScaler()\npca = PCA()\npipeline = make_pipeline(scaler, pca)\npipeline.fit(x)","ced904e7":"features = range(pca.n_components_)\n_ = plt.figure(figsize=(22, 5))\n_ = plt.bar(features, pca.explained_variance_)\n_ = plt.xlabel('PCA')\n_ = plt.ylabel('Var')\n_ = plt.xticks(features)\n_ = plt.title(\"Important PCs\")\nplt.show()","5836940a":"pca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform(x)\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['PC0', 'PC1','PC2'])","7438a646":"df['PC0']=pd.Series(principalDf['PC0'].values, index=df.index)\ndf['PC1']=pd.Series(principalDf['PC1'].values, index=df.index)\ndf['PC2']=pd.Series(principalDf['PC2'].values, index=df.index)","d0c0bdd5":"#  for PC0\nq1_pc1, q3_pc1 = df['PC0'].quantile([0.25, 0.75])\niqr_pc1 = q3_pc1 - q1_pc1\nlower_pc1 = q1_pc1 - (1.5*iqr_pc1)\nupper_pc1 = q3_pc1 + (1.5*iqr_pc1)\n\n#PC1\nq1_pc2, q3_pc2 = df['PC1'].quantile([0.25, 0.75])\niqr_pc2 = q3_pc2 - q1_pc2\nlower_pc2 = q1_pc2 - (1.5*iqr_pc2)\nupper_pc2 = q3_pc2 + (1.5*iqr_pc2)\n#PC2\nq1_pc3, q3_pc3 = df['PC2'].quantile([0.25, 0.75])\niqr_pc3 = q3_pc3 - q1_pc3\nlower_pc3 = q1_pc3 - (1.5*iqr_pc3)\nupper_pc3 = q3_pc3 + (1.5*iqr_pc3)","3038257a":"print(lower_pc1, upper_pc1)\nprint(lower_pc2, upper_pc2)\nprint(lower_pc3, upper_pc3)","928a3685":"df['anomaly_pc1'] = ((df['PC0']>upper_pc1) | (df['PC0']<lower_pc1)).astype('int')\ndf['anomaly_pc2'] = ((df['PC1']>upper_pc2) | (df['PC1']<lower_pc2)).astype('int')\ndf['anomaly_pc3'] = ((df['PC2']>upper_pc3) | (df['PC2']<lower_pc3)).astype('int')","aaa59855":"print(df['anomaly_pc1'].value_counts())\nprint(df['anomaly_pc2'].value_counts())\nprint(df['anomaly_pc3'].value_counts())","9e270449":"outliers_pc1 = df.loc[(df['PC0']>upper_pc1) | (df['PC0']<lower_pc1), 'PC0']\noutliers_pc2 = df.loc[(df['PC1']>upper_pc2) | (df['PC1']<lower_pc2), 'PC1']\noutliers_pc3 = df.loc[(df['PC2']>upper_pc3) | (df['PC2']<lower_pc3), 'PC2']","7e68f4d3":"print(len(outliers_pc1)\/len(df))\nprint(len(outliers_pc2)\/len(df))\nprint(len(outliers_pc3)\/len(df))","b0590ea0":"# Apply SelectKBest class to extract the best 3 features - Univariate feature selection \nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import MinMaxScaler\nx = df.drop(['machine_status', 'PC0','PC1' ,'PC2', 'anomaly_pc1', 'anomaly_pc2', 'anomaly_pc3'], axis=1)\ny = df['machine_status']\nscaler = MinMaxScaler()\nx_scaled = scaler.fit_transform(x)\nbestfeatures = SelectKBest(score_func=chi2, k=3)\nfit = bestfeatures.fit(x_scaled, y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(x.columns)\nfeatureScores = pd.concat([dfcolumns, dfscores], axis=1)\nfeatureScores.columns = ['Feature', 'Score']\nprint(featureScores.nlargest(3, 'Score'))","566d5f48":"\n#PC3 ANOMALY on sensor_11 AND WHEN ACTUAL BREAK HAPPENED\na = df[df['anomaly_pc1'] == 1] #anomaly\nb = df[df['anomaly_pc2'] == 1]\nc = df[df['anomaly_pc3'] == 1]\n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(df['sensor_11'], color='grey', label='Normal')\n_ = plt.plot(a['sensor_11'], linestyle='none', marker='X', color='blue', markersize=12, label='pc1 Anomaly', alpha = 0.2)\n_ = plt.plot(b['sensor_11'], linestyle='none', marker='X', color='green', markersize=12, label='pc2 Anomaly')\n_ = plt.plot(c['sensor_11'], linestyle='none', marker='X', color='yellow', markersize=12, label='pc3 Anomaly', alpha=0.2)\n_ = plt.plot(dfBroken['sensor_11'], linestyle='none', marker='X', color='red', markersize=12, label='Broken')\n_ = plt.xlabel('Date and Time')\n_ = plt.ylabel('Sensor Reading')\n_ = plt.title('Sensor_11 Anomalies')\n_ = plt.legend(loc='best')\nplt.show();","1703522c":"kmeans = KMeans(n_clusters=3, random_state=42)\nkmeans.fit(principalDf.values)\nlabels = kmeans.predict(principalDf.values)\nunique_elements, counts_elements = np.unique(labels, return_counts=True)\nclusters = np.asarray((unique_elements, counts_elements))","e40153ea":"\n_ = plt.figure(figsize = (9, 7))\n_ = plt.bar(clusters[0], clusters[1], tick_label=clusters[0])\n_ = plt.xlabel('Clusters ON PCA')\n_ = plt.ylabel('Number of RECORDS')\n_ = plt.title('Number of RECORDS in cluster')\nplt.show()","fa9783e2":"principalDf.info()","13b1207d":"_ = plt.figure(figsize=(9,7))\n_ = plt.scatter(principalDf['PC0'], principalDf['PC1'], c=labels)\n_ = plt.xlabel('PC0')\n_ = plt.ylabel('PC1')\n_ = plt.title('K-means of clustering')\nplt.show()","7ab8e9b2":"_ = plt.figure(figsize=(9,7))\n_ = plt.scatter(principalDf['PC1'], principalDf['PC2'], c=labels)\n_ = plt.xlabel('PC1')\n_ = plt.ylabel('PC2')\n_ = plt.title('K-means of clustering')\nplt.show()","33d63d8a":"_ = plt.figure(figsize=(9,7))\n_ = plt.scatter(principalDf['PC0'], principalDf['PC2'], c=labels)\n_ = plt.xlabel('PC0')\n_ = plt.ylabel('PC2')\n_ = plt.title('K-means of clustering')\nplt.show()","6ff5e8f1":"\nfrom mpl_toolkits import mplot3d","2bb7f070":"# Creating figure\nfig = plt.figure(figsize = (10, 7))\nax = plt.axes(projection =\"3d\")\n \n# Creating plot\nax.scatter3D(principalDf['PC0'], principalDf['PC1'], principalDf['PC2'], c=labels)\nplt.title(\"Anomaly Clusters ON PCA\")\n \n# show plot\nplt.show()","e5194038":"\ndef getDistanceByPoint(data, model):\n    distance = []\n    for i in range(0,len(data)):\n        Xa = np.array(data.loc[i])\n        Xb = model.cluster_centers_[model.labels_[i]]\n        distance.append(np.linalg.norm(Xa-Xb))\n    return pd.Series(distance, index=data.index)","854fe3d5":"distance = getDistanceByPoint(principalDf, kmeans)","7ecc6874":"outliers_fraction = 0.11\nnumber_of_outliers = int(outliers_fraction*len(distance))","0bb89b20":"\nthreshold = distance.nlargest(number_of_outliers).min()","41e5c63c":"#(0:normal, 1:anomaly) \nprincipalDf['anomalyDist'] = (distance >= threshold).astype(int)","b4149cba":"principalDf['anomalyDist'].value_counts()","3f17bc9f":"df['anomalyDist'] = pd.Series(principalDf['anomalyDist'].values, index=df.index)","cd0ed9d3":"a = df[df['anomalyDist'] == 1] #anomaly\n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(df['sensor_04'], color='grey', label='Normal')\n_ = plt.plot(a['sensor_04'], linestyle='none', marker='X', color='blue', markersize=12, label='Dist Anomaly', alpha= 0.2)\n_ = plt.plot(dfBroken['sensor_04'], linestyle='none', marker='X', color='red', markersize=12, label='Broken')\n_ = plt.xlabel('Date and Time')\n_ = plt.ylabel('Sensor Reading')\n_ = plt.title('Centroid Anomalies on Sensor_04 ')\n_ = plt.legend(loc='best')\nplt.show();","97513ae1":"df[df['anomalyDist']==1]['machine_status'].value_counts()","d4e661a9":"df['machine_status'].value_counts()","5eb20e55":"from sklearn.ensemble import IsolationForest\noutliers_fraction = 0.11 ## why\nmodel =  IsolationForest(contamination=outliers_fraction)\nmodel.fit(principalDf.values) \nprincipalDf['anomalyForest'] = pd.Series(model.predict(principalDf.values)) ### here -1 is anomaly","6092898a":"df['anomalyForest'] = pd.Series(principalDf['anomalyForest'].values, index=df.index)\na = df.loc[df['anomalyForest'] == -1] #anomaly\n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(df['sensor_04'], color='grey', label='Normal')\n_ = plt.plot(a['sensor_04'], linestyle='none', marker='X', color='blue', markersize=12, label='Forest Anomaly', alpha=0.3)\n_ = plt.plot(dfBroken['sensor_04'], linestyle='none', marker='X', color='red', markersize=12, label='Broken')\n_ = plt.xlabel('Date and Time')\n_ = plt.ylabel('Sensor Reading')\n_ = plt.title('Sensor_04 FOREST ISOLATION Anomalies')\n_ = plt.legend(loc='best')\nplt.show();","48eeb929":"df['anomalyForest'].value_counts()","f99ae58e":"df[df['anomalyForest']==1]['machine_status'].value_counts()","4928e0f7":"df['machine_status'].value_counts()","e4409201":"3D plots looks ok!! we can create k=2 clusters as well!! for broken or not-broken ","870a35a0":"let's see what appears to be best three features if machine status is our variable of intrest","740a3ca5":"guess we are ready.\nmake df a time seires first. set date as index.","0391035d":"of course PC2 and PC3 DOMINATES on sensor 11","c3cca73c":"COMPARE THE HITS AND MISSES OF LAST TWO GRAPHS!! SCI-KIT FOREST ISOLATION WINS....","819309f8":"we are following very simple imputation strategy over here (there are many shopisticated ones)\nmean imputation for all columns that have more than 1% missing value \n(also one can remove the entire column if it has let's say more than 60% missing value. The upper limit is more of heuristic and experience based.)","07e3d418":"LET'S FIND A THRESHOLD DISTANCE","13e07a9b":"Asset Management can exploit Sensor Data and do some kool anomaly detection using machine learning techniques!!","628b0dfe":"INTERM OF NUMBERS ","e711b604":"2 quick observation \n1. sensor 15 is use less\n2. we have to do something for timestamp data type","eac3a04d":"this is common practice in missing value analyses -- top n column with highest missing values ","1c7c6179":"many redundant columns","8e5c68f1":"let's drop duplicate rows first if any","6eb8b984":"LET'S TAKE HELP OF SCI-KIT","ed10c8a9":"I WPN'T CHECK FOR AUTO-CORRELATION IN PCs (WHY? THEY ARE ORTHOGONAL)\nWHAT ABOUT STATIONARITY AGAIN ADF WILL SAY THEY STATIONAY (CHECK IT IF YOU LIKE) ","0825d3d5":"LET'S TRY FIGURE HOW PCs PERFORMED ","449b2738":"LET'S HAVE A LOOK AT MACHINE STATUS FIRST","0ce35afc":"6 of 7 TIMES| NOT BAD FOREST ISOLATION ||","cbd3264b":"NOW SOME DIMENSION REDUCTION -- WILL GO AHEAD WITH PCA BUT SOON WRITE ANOTHER NOTEBOOK ON TICK (CLUSTERING). HAVE SEEN IT'S USE GETTING POPULAR IN INDUSTRY.","d00d1b71":"LET'S TRY SOME BASIC ANOMALY DETECTION ON PUMP-SENSOR-DATA","9ad95657":"Now ~ next problem is gonna be missing value!! got to do something ","318b8a90":"I WILL NOT CHECK THE STATIONARITY OF THIS LONG SIRIES. \nAGAIN FROM MY MULTIPLE YEARS IN TIME SERIES ALMOST ALL HIGH FREQUENCY LONG TIME SERIES WILL PASS THE STATIONARY TEST. YOU CAN CHECK THIS. FOR LONG SERIES OF HIGH FREQUENCY DATA ECONOMERTICIAN SHOULD BETTER COME WITH NEW TEST RATHER THAN RELYING UPON TRADITIONAL ONES WHICH WORKS PERFECTLY ON LOW FREQUENCY DATA WITH LESS RECORDS.","55584203":"MANY SUCH COMBOS MIGHT BRING OUT BETTER INSIGHTS","8e0574e4":"ABOVE PICS SHOW DISTANCE BASED ANOMALIES MISSED A LOT OF BROKEN MACHINE STATUS ","8532966c":"DON'T FORGET THE OUTLIERS!! THEY ARE THE ONES WHO MIGHT HAVE PLAYED ROLE IN ANOMALY....\nSIMPLY REMOVING THEM MIGHT NOT BE A VERY GOOD IDEA","b29c1bc6":"COULD CAPTURE 1 OF 7","5d0ea871":"IQR BEFORE ANYTHING","e79da483":"NOW LET'S TRY K-MEANS","4c7415e9":"LETS CHECK DISTANCE FROM NEAREST CENTROID"}}