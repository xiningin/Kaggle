{"cell_type":{"78f8f1c3":"code","a76c5a78":"code","69e95761":"code","6cc8f003":"code","fe15661b":"code","f728b7ba":"code","f643ed67":"code","bc85319f":"code","ae21f0c6":"code","bf419b03":"code","b1b32118":"code","553f136a":"code","52bb4a91":"code","ce7becd4":"code","5c4052c5":"code","e44c1ea3":"code","253ac7f0":"code","601c27a4":"code","16ee2788":"code","e95e08bb":"code","d5112b42":"code","b0ff9289":"code","c4a23930":"code","6ea9ffc6":"code","ffd2bf59":"code","7b28e050":"code","0f674531":"code","de66055c":"code","2a764383":"markdown","c2a6d975":"markdown","500b1b8f":"markdown","ecf2d84f":"markdown","5fe85e7b":"markdown","20e9e085":"markdown","bb475232":"markdown","6f0fb9da":"markdown","ee9378df":"markdown"},"source":{"78f8f1c3":"# !pip install PySastrawi","a76c5a78":"!ls '..\/input\/'","69e95761":"import pandas as pd\nimport numpy as np","6cc8f003":"data = pd.read_csv('..\/input\/review-lapak-sentiment\/train.csv')","fe15661b":"data.head(10)","f728b7ba":"data['label'].value_counts()","f643ed67":"# from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n \n# factory = StopWordRemoverFactory()\n# stopword = factory.create_stop_word_remover()\n\n# def stop_words_removal(text):\n#     return stopword.remove(text)\n\n# print(stop_words_removal(\"Dengan Menggunakan Python dan Library Sastrawi saya dapat melakukan proses Stopword Removal\"))","bc85319f":"# from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n\n# factory = StemmerFactory()\n# stemmer = factory.create_stemmer()\n\n# def stemming(text):\n#     return stemmer.stem(text)\n\n# print(stemming(\"Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan\"))","ae21f0c6":"import itertools\ndef remove_repeating_character(text):\n    return ''.join(''.join(s)[:1] for _, s in itertools.groupby(text))\n\nremove_repeating_character(\"Halooo, duniaa!!\")","bf419b03":"import re\ndef remove_nonaplhanumeric(text):\n    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n    return text\n\ndef lowercase(text):\n    return text.lower()\n\nprint(remove_nonaplhanumeric(\"Halooo,,,,, duniaa!!\"))\nprint(lowercase(\"Halooo, duniaa!\"))","b1b32118":"def preprocess(text):\n    text = lowercase(text)\n    text = remove_nonaplhanumeric(text)\n#     text = stop_words_removal(text)\n#     text = stemming(text)\n    text = remove_repeating_character(text)\n    return text","553f136a":"data['review_sangat_singkat'] = data['review_sangat_singkat'].apply(lambda x: preprocess(x))\ndata.head()","52bb4a91":"unique_string = set()\nfor x in data['review_sangat_singkat']:\n    for y in x.split():\n        unique_string.add(y)","ce7becd4":"len(unique_string)","5c4052c5":"len_data = [len(x.split()) for x in data['review_sangat_singkat']]\nprint(np.mean(len_data))\nprint(np.median(len_data))\nprint(np.std(len_data))\nprint(np.min(len_data))\nprint(np.max(len_data))\nprint(np.percentile(len_data, 98))","e44c1ea3":"embed_size = 100 # how big is each word vector\nmax_features = 23000 # how many unique words to use\nmaxlen = 20 # max number of words in a comment to use","253ac7f0":"# Example\nfrom keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=4)\ntokenizer.fit_on_texts([\"ini sebuah kalimat hehe\"])\nexample = tokenizer.texts_to_sequences([\"ini contoh kalimat juga\"])\nprint(example[0])","601c27a4":"tokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(data['review_sangat_singkat'])\nlist_tokenized_train = tokenizer.texts_to_sequences(data['review_sangat_singkat'].values)","16ee2788":"list_tokenized_train[0]","e95e08bb":"# Example\nfrom keras.preprocessing.sequence import pad_sequences\npad_sequences(example, maxlen=maxlen)","d5112b42":"X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)","b0ff9289":"X_t[0]","c4a23930":"import gensim\nDIR_DATA_MISC = \"..\/input\/word2vec-100-indonesian\"\npath = '{}\/idwiki_word2vec_100.model'.format(DIR_DATA_MISC)\nid_w2v = gensim.models.word2vec.Word2Vec.load(path)\nprint(id_w2v.most_similar('itb'))","6ea9ffc6":"index2word_set = set(id_w2v.wv.index2word)","ffd2bf59":"word_index = tokenizer.word_index\nnb_words = max_features\nembedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\nunknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1.\nfor word, i in word_index.items():\n    cur = word\n    if cur in index2word_set:\n        embedding_matrix[i] = id_w2v[cur]\n        continue\n        \n    embedding_matrix[i] = unknown_vector\n","7b28e050":"from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, SpatialDropout1D, GlobalMaxPooling1D, Concatenate\nfrom keras.models import Model\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras import callbacks\n\nfrom keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\ndef get_model():\n    inp = Input(shape=(maxlen,))\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    x = SpatialDropout1D(0.3)(x)\n    x1 = Bidirectional(LSTM(32, return_sequences=True))(x)\n    x2 = Bidirectional(GRU(32, return_sequences=True))(x1)\n    max_pool1 = GlobalMaxPooling1D()(x1)\n    max_pool2 = GlobalMaxPooling1D()(x2)\n    conc = Concatenate()([max_pool1, max_pool2])\n    x = Dense(1, activation=\"sigmoid\")(conc)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[f1])\n    return model","0f674531":"from sklearn.model_selection import KFold\ndef get_kfold():\n    return KFold(n_splits=5, shuffle=True, random_state=1)","de66055c":"X = X_t\ny = data[\"label\"].values\n\npred_cv = np.zeros(len(y))\ncount = 0\n\nfor train_index, test_index in get_kfold().split(X, y):\n    count += 1\n    print(count, end='')\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    es = callbacks.EarlyStopping(monitor='val_f1', min_delta=0.0001, patience=8,\n                                             verbose=1, mode='max', baseline=None, restore_best_weights=True)\n\n    rlr = callbacks.ReduceLROnPlateau(monitor='val_f1', factor=0.5,\n                                      patience=3, min_lr=1e-6, mode='max', verbose=1)\n    \n    model = get_model()\n    model.fit(X_train, \n             y_train, batch_size=16, epochs=4,\n             validation_data=(X_test, y_test),\n             callbacks=[es, rlr],\n             verbose=1)\n    \n    pred_cv[[test_index]] += model.predict(X_test)[:,0]","2a764383":"## Import Data","c2a6d975":"# Introduction to Text","500b1b8f":"Pad sequences: https:\/\/keras.io\/preprocessing\/sequence\/","ecf2d84f":"## Extra Preprocessing","5fe85e7b":"## Feature Engineering","20e9e085":"Word Embedding: https:\/\/www.kaggle.com\/ilhamfp31\/word2vec-100-indonesian","bb475232":"## Preprocess","6f0fb9da":"Tokenizer: https:\/\/keras.io\/preprocessing\/text\/","ee9378df":"## Model"}}