{"cell_type":{"f3985d5a":"code","2cae18c0":"code","3024a714":"code","1332c6a9":"code","dde41e58":"code","46562c22":"code","deb11b33":"code","bffd53c7":"code","275d70d0":"code","611eddf5":"code","2d5c9cee":"code","76fb4109":"code","35dd100c":"code","0b002610":"code","98716764":"code","38d85e1d":"code","15e4c7ba":"code","64b45488":"markdown","d826fbfe":"markdown","97568294":"markdown","b37755bb":"markdown","ac7730a2":"markdown","d7a96953":"markdown","1c9acba8":"markdown"},"source":{"f3985d5a":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.preprocessing import StandardScaler , Binarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport time\nimport os, sys, gc, warnings, random, datetime\nfrom sklearn.model_selection import StratifiedKFold , KFold\nimport math\nimport shap\nimport joblib\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nwarnings.filterwarnings('ignore')\n","2cae18c0":"!pip install --upgrade git+https:\/\/github.com\/stanfordmlgroup\/ngboost.git\nfrom ngboost import NGBRegressor, NGBClassifier\nfrom ngboost.ngboost import NGBoost\nfrom ngboost.learners import default_tree_learner\nfrom ngboost.scores import CRPS, MLE , LogScore\nfrom ngboost.distns import LogNormal, Normal\nfrom ngboost.distns import k_categorical, Bernoulli\n\n","3024a714":"df = pd.read_pickle(\"..\/input\/handling-imbalanced-data-eda-small-fe\/df_for_use.pkl\")\ndf_fe = pd.read_pickle(\"..\/input\/handling-imbalanced-data-eda-small-fe\/df_fe.pkl\")","1332c6a9":"lgbm_clf = joblib.load('..\/input\/handling-imbalanced-data-supervised-learning\/lgbm_clf.pkl')\nrf_clf = joblib.load('..\/input\/handling-imbalanced-data-supervised-learning\/rf_clf.pkl')\nxgb_clf = joblib.load('..\/input\/handling-imbalanced-data-supervised-learning\/xgb_clf.pkl')\nngb_clf = joblib.load('..\/input\/handling-imbalanced-data-supervised-learning\/ngb_clf.pkl')\n","dde41e58":"X = df.drop('loan_condition_cat', axis=1)\ny = df['loan_condition_cat']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)\n\n","46562c22":"lgbm_cpu_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average = 'macro')\nrf_roc_score = roc_auc_score(y_test, rf_clf.predict_proba(X_test)[:,1], average = 'macro')\nxgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1], average = 'macro')\nNGb_roc_score = roc_auc_score(y_test, ngb_clf.predict_proba(X_test)[:,1], average = 'macro')\n\nprint( 'RandomForest_ROC_AUC : {0:.4f}'.format(rf_roc_score))\nprint( 'Lightgbm_ROC_AUC : {0:.4f}'.format(lgbm_cpu_roc_score))\nprint( 'Xgboost_ROC_AUC : {0:.4f}'.format(xgb_roc_score))\nprint( 'Ngboost_ROC_AUC : {0:.4f}'.format(NGb_roc_score))\n","deb11b33":"def get_clf_eval(y_test, pred):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test,pred)\n    f1 = f1_score(y_test, pred)\n    print('Confusion Matrix')\n    print(confusion)\n    print('Auccuracy : {0:.4f}, Precision : {1:.4f} , Recall : {2:.4f} , F1_Score : {3:.4f}'.format(accuracy , precision, recall, f1))\n    print('------------------------------------------------------------------------------')\n    \n\nthresholds = {0.1,0.15, 0.2,0.25, 0.3,0.35, 0.4 , 0.45 , 0.5}\n\ndef get_eval_by_threshold(y_test, pred_proba_c1, thresholds):\n    for custom_threshold in thresholds:\n        binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_c1)\n        custom_predict = binarizer.transform(pred_proba_c1)\n        print('threshold:', custom_threshold)\n        get_clf_eval(y_test, custom_predict)\n\n## get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)    ","bffd53c7":"### Random Forest\n\nftr_ims_values = rf_clf.feature_importances_\nftr_ims = pd.Series(ftr_ims_values, index = X_train.columns)\nftr_top20 = ftr_ims.sort_values(ascending = False)[:20]\n\nplt.figure(figsize = (10,8))\nplt.title('Feature importance')\nsns.barplot(x = ftr_top20, y = ftr_top20.index)\nplt.show()","275d70d0":"### XGboost\n\nfrom xgboost import plot_importance\nfig, ax = plt.subplots(1,1, figsize= (10,8))\nplot_importance(xgb_clf, ax= ax, max_num_features = 20 , height = 0.4)","611eddf5":"### LightGBM\n\nfrom lightgbm import plot_importance\nfig, ax = plt.subplots(1,1, figsize= (10,8))\nplot_importance(lgbm_clf, ax= ax, max_num_features = 20 , height = 0.4)","2d5c9cee":"### NGboost\n\n\n## Feature importance for loc trees\nfeature_importance_loc = ngb_clf.feature_importances_[0]\n\n# ## Feature importance for scale trees\n# feature_importance_scale = ngb_clf.feature_importances_[1]\n\ndf_loc = pd.DataFrame({'feature': X_train.columns, \n                       'importance':feature_importance_loc})\\\n    .sort_values('importance',ascending=False)\n# df_scale = pd.DataFrame({'feature':X_train.columns, \n#                        'importance':feature_importance_scale})\\\n#     .sort_values('importance',ascending=False)\n\nfig, ax1 = plt.subplots(1,1, figsize=(10,8))\nfig.suptitle(\"Feature importance plot for distribution parameters\", fontsize=17)\nsns.barplot(x='importance',y='feature',ax=ax1,data=df_loc, color=\"skyblue\").set_title('loc param')\n# sns.barplot(x='importance',y='feature',ax=ax2,data=df_scale, color=\"skyblue\").set_title('scale param')\n","76fb4109":"perm_lgbm = PermutationImportance(lgbm_clf, random_state=2020).fit(X_test, y_test)\neli5.show_weights(perm_lgbm, feature_names = X_test.columns.tolist())","35dd100c":"pi_features = eli5.explain_weights_df(perm_lgbm, feature_names = X_train.columns.tolist())\npi_features = pi_features.loc[pi_features['weight'] >= 0.005]['feature'].tolist()","0b002610":"X_pi = X[pi_features]\nX_train, X_test, y_train, y_test = train_test_split(X_pi, y, test_size = 0.2 , random_state = 2020, stratify = y)","98716764":"start = time.time()\n\nlgbm_clf = LGBMClassifier(n_estimators = 3000, random_state = 2020)\nevals = [(X_test, y_test)]\nlgbm_clf.fit(X_train, y_train, early_stopping_rounds = 100, eval_metric = 'auc' , eval_set = evals, verbose = 50)\n\n\nlgbm_cpu_runtime = time.time() - start\n\nget_eval_by_threshold(y_test, lgbm_clf.predict_proba(X_test)[:,1].reshape(-1,1), thresholds)\nlgbm_cpu_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average = 'macro')\n\nprint( 'LightGBM_cpu_ROC_AUC : {0:.4f} , Runtime : {1:.4f}'.format(lgbm_cpu_roc_score ,lgbm_cpu_runtime ))","38d85e1d":"from lightgbm import LGBMClassifier\n\nfrom time import time\nparams_lgb={'boosting_type':'gbdt',\n           'objective': 'binary',\n           'random_state':2020,\n           'metric':'auc'\n           }\n\nk_fold=5\nkf=StratifiedKFold(n_splits=k_fold,shuffle=True, random_state=2020)\ntraining_start_time = time()\naucs=[]\ny_preds = np.zeros(X_test.shape[0])\n\nfor fold, (trn_idx,val_idx) in enumerate(kf.split(X_train,y_train)):\n    start_time = time()\n    print('Training on fold {}'.format(fold + 1))\n    trn_data = lgb.Dataset(X_train.iloc[trn_idx], label=y_train.iloc[trn_idx])\n    val_data = lgb.Dataset(X_train.iloc[val_idx], label=y_train.iloc[val_idx])\n    clf = lgb.train(params_lgb, trn_data, num_boost_round=10000, valid_sets = [trn_data, val_data], \n                    verbose_eval=200, early_stopping_rounds=200)\n    aucs.append(clf.best_score['valid_1']['auc'])\n    print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n    y_preds += clf.predict(X_test) \/ 5\n    \n    \n    \nprint('-' * 30)\nprint('Training is completed!.')\nprint(\"\\n## Mean CV_AUC_Score : \", np.mean(aucs))\nprint('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n# print(clf.best_params_)\nprint('-' * 30)\n\n\n# pred_rf = clf.predict(X_test)\nauc = roc_auc_score(y_test,y_preds)\nprint(' ROC_AUC_Score : {0:.4f}'.format (auc))","15e4c7ba":"#### AUC got improved after applying permutation importance\n#### Also more improved with CV stratified 5 folds","64b45488":"### Apply to LightGBM","d826fbfe":"## Feature Importance","97568294":"## Libraries","b37755bb":"## Permutation Importance","ac7730a2":"### LightGBM with Stratified 5Fold","d7a96953":"## import & functions","1c9acba8":"#### Data fork from previous EDA kernel \nhttps:\/\/www.kaggle.com\/possiblemanjr\/handling-imbalanced-data-eda-small-fe\n\n#### trained model from following kernels\n\nhttps:\/\/www.kaggle.com\/possiblemanjr\/handling-imbalanced-data-supervised-learning"}}