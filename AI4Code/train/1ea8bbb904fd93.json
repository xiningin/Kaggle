{"cell_type":{"969e4be1":"code","c695cdf9":"code","f33f451a":"code","88ded9ae":"code","961d6f8e":"code","c6530da0":"code","e40baccb":"code","ce43c180":"code","c8670fa2":"code","b7356355":"code","2bbff9a1":"code","e513ec3a":"code","041c3322":"code","c531ebc3":"code","98823cd8":"code","959e64a2":"code","625df0de":"code","30a606fc":"code","5c64b9c2":"code","848bdc2a":"code","27460e5f":"code","7789f368":"code","962eb5c7":"code","78c8338c":"code","10817171":"code","0de9b575":"code","8c77974a":"code","c86c61fe":"code","b0a69039":"code","bc8129a4":"code","52d40f41":"code","9c84fff4":"code","0ed8f577":"code","1cf9df36":"code","a6645f70":"code","b9853290":"code","8b8e10b3":"code","448d9c60":"code","ebdf17aa":"code","fbc0a199":"code","23ebe40c":"code","82c783cf":"code","36e374cd":"code","da954cca":"code","03c9bea4":"code","62c8b7ec":"code","d9df6645":"code","dcdd5f09":"code","0a0351b0":"code","c4ff96e7":"code","6b78a539":"code","11d06781":"code","5896ea8f":"code","b15ca9b4":"code","5a45f87a":"code","76a7319c":"code","0aa1f638":"code","fa40df67":"code","e3b8beba":"code","e1b363dc":"code","59a1f04b":"code","6c2a823f":"markdown","428ac838":"markdown","18b7b4ac":"markdown","47f91373":"markdown","d860ab4b":"markdown","03081992":"markdown","a8fb46d0":"markdown","e98c04b0":"markdown","bbbf8399":"markdown","8fd50744":"markdown","f9f5ee86":"markdown","9784a79b":"markdown","7cda02d1":"markdown","63629f5e":"markdown","0955d911":"markdown","3ff20052":"markdown","b9eb274b":"markdown","9cb9f930":"markdown","e6ba0306":"markdown","51875995":"markdown"},"source":{"969e4be1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb","c695cdf9":"train = pd.read_csv('..\/input\/tmdb-box-office-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/tmdb-box-office-prediction\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tmdb-box-office-prediction\/sample_submission.csv')","f33f451a":"train.head()","88ded9ae":"test.head()","961d6f8e":"print(\"Number of rows(train): \"+str(len(train)))\nprint(\"Number of rows(test): \"+str(len(test)))","c6530da0":"train.info()","e40baccb":"train['has_collection'] = train['belongs_to_collection'].apply(lambda x: 1 if str(x) != 'nan' else 0)\ntrain['collection_id'] = train['belongs_to_collection'].apply(lambda x: eval(x)[0]['id'] if str(x) != 'nan' else 0)\n\ntest['has_collection'] = test['belongs_to_collection'].apply(lambda x: 1 if str(x) != 'nan' else 0)\ntest['collection_id'] = test['belongs_to_collection'].apply(lambda x: eval(x)[0]['id'] if str(x) != 'nan' else 0)","ce43c180":"train = train.drop(['belongs_to_collection'], axis=1)\ntest = test.drop(['belongs_to_collection'], axis=1)","c8670fa2":"list_of_genres = list(train['genres'].apply(lambda x: [i['name'] for i in eval(x)] if str(x) != 'nan' else []).values)","b7356355":"Counter([i for j in list_of_genres for i in j]).most_common(15)","2bbff9a1":"top_genres = [m[0] for m in Counter([i for j in list_of_genres for i in j]).most_common(15)]\n\ntrain['all_genres'] = train['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in eval(x)])) if (isinstance(x,int) or isinstance(x,str)) == True else '')\nfor gen in top_genres:\n    train['genre_' + gen] = train['all_genres'].apply(lambda x: 1 if gen in x else 0)\n    \ntest['all_genres'] = test['genres'].apply(lambda x: ' '.join(sorted([i['name'] for i in eval(x)])) if (isinstance(x,int) or isinstance(x,str)) == True else '')\nfor gen in top_genres:\n    test['genre_' + gen] = test['all_genres'].apply(lambda x: 1 if gen in x else 0)\n    \ntrain = train.drop(['genres'], axis=1)\ntest = test.drop(['genres'], axis=1)","e513ec3a":"lang_encoder = LabelEncoder()\ntrain['all_genres'] = lang_encoder.fit_transform(train['all_genres'])\ntest['all_genres'] = lang_encoder.fit_transform(test['all_genres'])","041c3322":"train['original_language'].unique()","c531ebc3":"lang_encoder = LabelEncoder()\ntrain['original_language'] = lang_encoder.fit_transform(train['original_language'])\ntest['original_language'] = lang_encoder.fit_transform(test['original_language'])","98823cd8":"prod_companies = list(train['production_companies'].apply(lambda x: [i['name'] for i in eval(x)] if str(x) != 'nan' else '').values)","959e64a2":"train['prod_companies_count'] = train['production_companies'].apply(lambda x: len([i for i in eval(x)]) if str(x) != 'nan' else 0)\ntest['prod_companies_count'] = test['production_companies'].apply(lambda x: len([i for i in eval(x)]) if str(x) != 'nan' else 0)","625df0de":"pop_production = Counter([i for j in prod_companies for i in j])","30a606fc":"train['production_score'] = train['production_companies'].apply(lambda x: np.tanh(max([pop_production[i['name']] for i in eval(x)])) if str(x) != 'nan' else 0)\ntest['production_score'] = test['production_companies'].apply(lambda x: np.tanh(max([pop_production[i['name']] for i in eval(x)])) if str(x) != 'nan' else 0)","5c64b9c2":"train = train.drop(['production_companies'], axis=1)\ntest = test.drop(['production_companies'], axis=1)","848bdc2a":"train['production_countries'] = train['production_countries'].apply(lambda x: [i['name'] for i in eval(x)][0] if str(x) != 'nan' else '')\ntest['production_countries'] = test['production_countries'].apply(lambda x: [i['name'] for i in eval(x)][0] if str(x) != 'nan' else '')","27460e5f":"prod_country_encoder = LabelEncoder()\ntrain['production_countries'] = prod_country_encoder.fit_transform(train['production_countries'])\ntest['production_countries'] = prod_country_encoder.fit_transform(test['production_countries'])","7789f368":"train['production_countries'].head()","962eb5c7":"train['release_date'] = train['release_date'].apply(lambda x: pd.to_datetime(x))\ntest['release_date'] = test['release_date'].apply(lambda x: pd.to_datetime(x))","78c8338c":"train['year'] = train['release_date'].apply(lambda x: x.year)\ntrain['month'] = train['release_date'].apply(lambda x: x.month)\ntrain['day_of_week'] = train['release_date'].apply(lambda x: x.weekday())\n\ntest['year'] = test['release_date'].apply(lambda x: x.year)\ntest['month'] = test['release_date'].apply(lambda x: x.month)\ntest['day_of_week'] = test['release_date'].apply(lambda x: x.weekday())","10817171":"train = train.drop(['release_date'], axis=1)\ntest = test.drop(['release_date'], axis=1)","0de9b575":"_ = sns.lineplot(x=train['year'], y=train['revenue'], color='r')","8c77974a":"train['year'] = train['year'].apply(lambda x: x-100 if x>2020 else x)\ntest['year'] = test['year'].apply(lambda x: x-100 if x>2020 else x)","c86c61fe":"_ = sns.lineplot(x=train['year'], y=train['revenue'], color='g')","b0a69039":"avg_runtime_train = train['runtime'].mean()\ntrain['runtime'] = train['runtime'].apply(lambda x: x if str(x) != 'nan' else avg_runtime_train)\n\navg_runtime_test = test['runtime'].mean()\ntest['runtime'] = test['runtime'].apply(lambda x: x if str(x) != 'nan' else avg_runtime_test)","bc8129a4":"_ = sns.distplot(train['runtime'])","52d40f41":"train[train['budget']==0].head() ","9c84fff4":"train['spoken_languages_count'] = train['spoken_languages'].apply(lambda x: len(eval(x)) if str(x) != 'nan' else 0)\ntest['spoken_languages_count'] = test['spoken_languages'].apply(lambda x: len(eval(x)) if str(x) != 'nan' else 0)","0ed8f577":"_ = sns.countplot(x=train['spoken_languages_count'])","1cf9df36":"train = train.drop(['spoken_languages'], axis=1)\ntest = test.drop(['spoken_languages'], axis=1)","a6645f70":"train['Keywords']","b9853290":"list_of_keywords = list(train['Keywords'].apply(lambda x: [i['name'] for i in eval(x)] if str(x) != 'nan' else []).values)","8b8e10b3":"train['num_Keywords'] = train['Keywords'].apply(lambda x: len(eval(x)) if str(x) != 'nan' else 0)\ntrain['all_Keywords'] = train['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in eval(x)])) if str(x) != 'nan' else '')\n\ntop_keywords = [m[0] for m in Counter([i for j in list_of_keywords for i in j]).most_common(30)]\n\nfor g in top_keywords:\n    train['keyword_'+g] = train['all_Keywords'].apply(lambda x: 1 if g in x else 0)\n    \n    \n    \ntest['num_Keywords'] = test['Keywords'].apply(lambda x: len(eval(x)) if str(x) != 'nan' else 0)\ntest['all_Keywords'] = test['Keywords'].apply(lambda x: ' '.join(sorted([i['name'] for i in eval(x)])) if str(x) != 'nan' else '')\n\ntop_keywords = [m[0] for m in Counter([i for j in list_of_keywords for i in j]).most_common(30)]\n\nfor g in top_keywords:\n    test['keyword_'+g] = test['all_Keywords'].apply(lambda x: 1 if g in x else 0)","448d9c60":"keywords_encoder = LabelEncoder()\ntrain['all_Keywords'] = keywords_encoder.fit_transform(train['all_Keywords'])\ntest['all_Keywords'] = keywords_encoder.fit_transform(test['all_Keywords'])","ebdf17aa":"train = train.drop(['Keywords'], axis=1)\ntest = test.drop(['Keywords'], axis=1)","fbc0a199":"train['cast'] = train['cast'].apply(lambda x: eval(x) if str(x) != 'nan' else [])\ntest['cast'] = test['cast'].apply(lambda x: eval(x) if str(x) != 'nan' else [])","23ebe40c":"train['gender_0_count'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntrain['gender_1_count'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntrain['gender_2_count'] = train['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))\n\ntest['gender_0_count'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 0]))\ntest['gender_1_count'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 1]))\ntest['gender_2_count'] = test['cast'].apply(lambda x: sum([1 for i in x if i['gender'] == 2]))","82c783cf":"list_of_cast_members = list(train['cast'].apply(lambda x: [i['name'] for i in x] if str(x) != 'nan' else []).values)","36e374cd":"top_cast_members = [m[0] for m in Counter([i for j in list_of_cast_members for i in j]).most_common(50)]","da954cca":"for g in top_cast_members:\n    train['cast_member_'+g] = train['cast'].apply(lambda x: 1 if g in str(x) else 0)\n    \nfor g in top_cast_members:\n    test['cast_member_'+g] = test['cast'].apply(lambda x: 1 if g in str(x) else 0)","03c9bea4":"train = train.drop(['cast'], axis=1)\ntest = test.drop(['cast'], axis=1)","62c8b7ec":"train['crew'] = train['crew'].apply(lambda x: eval(x) if str(x) != 'nan' else [])\ntest['crew'] = test['crew'].apply(lambda x: eval(x) if str(x) != 'nan' else [])","d9df6645":"list_of_crew_members = list(train['crew'].apply(lambda x: [i['name'] for i in x] if str(x) != 'nan' else []).values)","dcdd5f09":"top_crew_members = [m[0] for m in Counter([i for j in list_of_crew_members for i in j]).most_common(50)]","0a0351b0":"for g in top_crew_members:\n    train['crew_member_'+g] = train['crew'].apply(lambda x: 1 if g in str(x) else 0)\n    \nfor g in top_crew_members:\n    test['crew_member_'+g] = test['crew'].apply(lambda x: 1 if g in str(x) else 0)","c4ff96e7":"train = train.drop(['crew'], axis=1)\ntest = test.drop(['crew'], axis=1)","6b78a539":"train['has_homepage'] = train['homepage'].apply(lambda x: 1 if str(x) != 'nan' else 0)\ntest['has_homepage'] = test['homepage'].apply(lambda x: 1 if str(x) != 'nan' else 0)","11d06781":"train = train.drop(['homepage'], axis=1)\ntest = test.drop(['homepage'], axis=1)","5896ea8f":"train = train.drop(['poster_path'], axis=1)\ntest = test.drop(['poster_path'], axis=1)","b15ca9b4":"train = train.drop(['status'], axis=1)\ntest = test.drop(['status'], axis=1)","5a45f87a":"for col in ['title', 'tagline', 'overview', 'original_title']:\n    train['len_' + col] = train[col].fillna('').apply(lambda x: len(str(x)))\n    train['words_' + col] = train[col].fillna('').apply(lambda x: len(str(x.split(' '))))\n    \n    test['len_' + col] = test[col].fillna('').apply(lambda x: len(str(x)))\n    test['words_' + col] = test[col].fillna('').apply(lambda x: len(str(x.split(' '))))","76a7319c":"train = train.drop([\"imdb_id\", \"original_title\", \"overview\", \"tagline\", \"title\"], axis=1)\ntest = test.drop([\"imdb_id\", \"original_title\", \"overview\", \"tagline\", \"title\"], axis=1)","0aa1f638":"X = train.drop(['id', 'revenue'], axis=1)\nY = np.log1p(train['revenue'])\nX_test = test.drop(['id'], axis=1)","fa40df67":"X_train, X_valid, Y_train, Y_valid = train_test_split(X, Y, test_size=0.1)","e3b8beba":"params = {'num_leaves': 30,\n         'min_data_in_leaf': 20,\n         'objective': 'regression',\n         'max_depth': 5,\n         'learning_rate': 0.01,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9,\n         \"bagging_seed\": 11,\n         \"metric\": 'rmse',\n         \"lambda_l1\": 0.2,\n         \"verbosity\": -1}\nmodel = lgb.LGBMRegressor(**params, n_estimators = 20000, nthread = 4, n_jobs = -1)\nmodel.fit(X_train, Y_train, \n        eval_set=[(X_train, Y_train), (X_valid, Y_valid)], eval_metric='rmse',\n        verbose=1000, early_stopping_rounds=200)","e1b363dc":"y_pred_valid = model.predict(X_valid)\ny_pred = model.predict(X_test, num_iteration=model.best_iteration_)","59a1f04b":"sample_submission['revenue'] = np.expm1(y_pred)\nsample_submission.to_csv(\"submission.csv\", index=False)","6c2a823f":"#### Poster Path","428ac838":"#### Production Countries","18b7b4ac":"#### Keywords","47f91373":"#### Collection","d860ab4b":"#### Release Date","03081992":"Let's try to make a new feature called \"production score\". Score will be calculated on the popularity of production company( that is given by the number of times it has appeared in the dataset). If the movie have multiple productions then the score will be assigned by the most popular production.","a8fb46d0":"#### Title, Tagline, Overview, Original Title","e98c04b0":"#### Status","bbbf8399":"#### Budget","8fd50744":"#### Creating a model","f9f5ee86":"From the plot below we can observe a very weird trend that there are a lot of movies that have a release date like 2067, 2024 and so on. It turns out that this is just a typo. For example, the movie *Major Dundee* was released in 1965 whereas in the dataset it's release year is 2065. So we can just replace it with 1965.\n\n![Major Dundee](https:\/\/image.tmdb.org\/t\/p\/w600_and_h900_bestv2\/skv6Jsw6YyPKV4oQhs88zvwDCAL.jpg)","9784a79b":"#### Homepage","7cda02d1":"#### Genres","63629f5e":"#### Spoken Languages","0955d911":"#### Cast","3ff20052":"#### Runtime","b9eb274b":"#### Original Language","9cb9f930":"#### Crew","e6ba0306":"#### Production Companies","51875995":"#### Anomalies in release date"}}