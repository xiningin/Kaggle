{"cell_type":{"e2d6cc59":"code","7f4f6f25":"code","37e75c95":"code","0d84e22c":"code","ccf92afe":"code","d62ac454":"code","5febdda3":"code","fd2b11ce":"code","8a40bb26":"code","1b6aea7c":"code","12d6c006":"code","4b398dda":"code","44217b00":"code","7a685c29":"code","3c895ea3":"code","dc302fed":"code","431f5a3b":"code","847f972f":"code","625c17ac":"code","2e9f30d8":"code","b487a70c":"code","e649381a":"code","0cd45c85":"code","443b87a0":"code","51447988":"code","76876ee7":"code","30198fff":"code","14ce0de8":"code","3e5fd58e":"code","c07547ee":"code","fa30efbf":"code","180875b7":"code","0d469f43":"code","499fbfb0":"markdown","c078d0e6":"markdown","11fa6f26":"markdown","38c7bd5c":"markdown","7b3c719f":"markdown","2cfd9ea7":"markdown","3db21b25":"markdown","60152c09":"markdown","e8eb9c95":"markdown","8ab1cbd0":"markdown","a929d389":"markdown","75c2ca93":"markdown","3da9f359":"markdown","05309cac":"markdown","4f50e117":"markdown","6c16ed88":"markdown","db1380a6":"markdown","005a484b":"markdown","3a6b6c56":"markdown","b6010c43":"markdown","49bdc445":"markdown","45bdcea8":"markdown","4bd16181":"markdown","ec0d3c56":"markdown","af0ce1f8":"markdown","3b0cf32e":"markdown","96278461":"markdown","c7fdae50":"markdown","67c836a1":"markdown","786db350":"markdown","19cfe456":"markdown","ad04258b":"markdown","edabe4a7":"markdown","f9ee8540":"markdown","e4898455":"markdown","23b4f9a8":"markdown","fd4905c3":"markdown","7c21bae3":"markdown","a9bde073":"markdown","df9fee92":"markdown"},"source":{"e2d6cc59":"# BigMart Sales Prediction\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom scipy.stats import norm\nfrom scipy import stats\n\n# Load the data \na = pd.read_csv(r'..\/input\/big-mart-sales-prediction\/Train.csv')\nb = pd.read_csv(r'..\/input\/big-mart-sales-prediction\/Test.csv')\n# I store the ID for later use and delete it from the data.\nc = b.iloc[:, 0]\nd = b.iloc[:, 6]\n\na.drop(['Item_Identifier', 'Outlet_Identifier'], axis = 1, inplace = True)\nb.drop(['Item_Identifier', 'Outlet_Identifier'], axis = 1, inplace = True)","7f4f6f25":"categorical = ['Item_Fat_Content', 'Outlet_Size','Outlet_Location_Type','Outlet_Type', 'Item_Type']\ncontinuous = ['Item_Weight','Item_Visibility', 'Item_MRP','Item_Outlet_Sales']","37e75c95":"a.info()\n","0d84e22c":"b.info()","ccf92afe":"sns.distplot(a['Item_Outlet_Sales'])\nplt.show()","d62ac454":"a.head()","5febdda3":"a.isnull().sum()","fd2b11ce":"b.isnull().sum()","8a40bb26":"a.eq(0).sum()","1b6aea7c":"b.eq(0).sum()","12d6c006":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values  = np.nan, strategy = 'mean')\na.iloc[:, [0]] = imp.fit_transform(a.iloc[:,[0]])\nb.iloc[:, [0]] = imp.transform(b.iloc[:,[0]])\nimp1 = SimpleImputer(missing_values  = 0, strategy = 'mean')\na.iloc[:, [2]] = imp1.fit_transform(a.iloc[:,[2]])\nb.iloc[:, [2]] = imp1.transform(b.iloc[:,[2]])","4b398dda":"a['Outlet_Size'].fillna(a['Outlet_Size'].mode()[0], inplace = True)\nb['Outlet_Size'].fillna(b['Outlet_Size'].mode()[0], inplace = True)","44217b00":"sns.heatmap(a.corr(), annot = True)\nplt.show()","7a685c29":"sns.scatterplot(x = a['Item_MRP'], y = a['Item_Outlet_Sales'], data = a)\nplt.show()","3c895ea3":"sns.pairplot(a)\nplt.show()","dc302fed":"a['Outlet_Establishment_Year'].unique()","431f5a3b":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize = (15,8))\nfig.subplots_adjust(right=1)\nfig.suptitle('Relationshp with Categorical Features')\nfor ax, feature in zip(axes.flatten(),  categorical[0:3]):\n    sns.stripplot(x = feature,  y = 'Item_Outlet_Sales', data = a, ax = ax)\nplt.show()","847f972f":"a.Item_Fat_Content = a.Item_Fat_Content.replace({'low fat' : 'Low Fat', 'LF' : 'Low Fat', 'reg' : 'Regular'})\nb.Item_Fat_Content = b.Item_Fat_Content.replace({'low fat' : 'Low Fat', 'LF' : 'Low Fat', 'reg' : 'Regular'})\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize = (15,8))\nfig.subplots_adjust(right=1)\nfig.suptitle('Relationshp with Categorical Data')\nfor ax, feature in zip(axes.flatten(),  categorical[0:3]):\n    sns.stripplot(x = feature,  y = 'Item_Outlet_Sales', data = a, ax = ax)\nplt.show()","625c17ac":"fig, axes = plt.subplots(nrows = 2, ncols=1, figsize = (20,25))\nfig.subplots_adjust(hspace=0.5)\nfig.suptitle('Relationshp with Categorical Data')\nfor ax, feature in zip(axes.flatten(),  categorical[3:]):\n    sns.stripplot(x = 'Item_Outlet_Sales', y = feature, data = a, ax = ax)\nplt.show()","2e9f30d8":"fig,axes = plt.subplots(figsize = (10,10))\nsns.boxplot(x = a['Outlet_Establishment_Year'], y = a['Item_Outlet_Sales'], hue = a['Outlet_Type'], ax = axes )\nplt.plot","b487a70c":"# Encoding Categorical\na = pd.get_dummies(a, drop_first = True)\nb = pd.get_dummies(b, drop_first = True)","e649381a":"a.info()","0cd45c85":"# Splitting\nY = a['Item_Outlet_Sales']\nX = a.drop('Item_Outlet_Sales', axis = 1)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 0, test_size = 0.25)\n# Model\n# Linear Regression\nfrom sklearn.linear_model import LinearRegression\nlg = LinearRegression()\nlg.fit(X_train, Y_train)\nY_pred = lg.predict(X_test)\nresidue = Y_test - Y_pred\nsns.regplot(residue, Y_pred, lowess = True, line_kws={'color': 'red'})\nplt.show()\n","443b87a0":"Y_train = np.log(Y_train)\nY_test = np.log(Y_test)","51447988":"from sklearn.linear_model import LinearRegression\nlg = LinearRegression()\nlg.fit(X_train, Y_train)\nY_pred = lg.predict(X_test)\nresidue = Y_test - Y_pred\nsns.regplot(residue, Y_pred, lowess = True, line_kws={'color': 'red'})\nplt.show()\n","76876ee7":"# SVM\nfrom sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(X_train, Y_train)\nY_pred2 = regressor.predict(X_test)\n","30198fff":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(Y_test, Y_pred2))\nfrom sklearn.metrics import r2_score\nr2 = r2_score(Y_test, Y_pred2)\nprint('RMSE = ',rms, ' R2 score = ',r2)","14ce0de8":"from sklearn.linear_model import LassoCV\nmodel_lasso = LassoCV(alphas = [1, 0.1, 0.001, 0.0005])\nmodel_lasso.fit(X_train, Y_train)\ncoef = pd.Series(model_lasso.coef_, index = X_train.columns)\nimp_features = coef.index[coef!=0].tolist()\n\nimp_features\n","3e5fd58e":"X_train = X_train[imp_features]\nX_test = X_test[imp_features]\n\n","c07547ee":"from sklearn.preprocessing import KBinsDiscretizer\ndisc = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')\nX_train['Outlet_Establishment_Year'] = disc.fit_transform(X_train[['Outlet_Establishment_Year']])\nX_test['Outlet_Establishment_Year'] = disc.fit_transform(X_test[['Outlet_Establishment_Year']])\n","fa30efbf":"from sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(X_train, Y_train)\nY_pred3 = regressor.predict(X_test)\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(Y_test, Y_pred3))\nfrom sklearn.metrics import r2_score\nr2 = r2_score(Y_test, Y_pred3)\nprint('RMSE = ',rms, ' R2 score = ',r2)","180875b7":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train.iloc[:,0:2] = sc_X.fit_transform(X_train.iloc[:,0:2])\nX_test.iloc[:,0:2] = sc_X.transform(X_test.iloc[:,0:2])\n","0d469f43":"from sklearn.svm import SVR\nregressor = SVR(kernel = 'rbf')\nregressor.fit(X_train, Y_train)\nY_pred3 = regressor.predict(X_test)\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nrms = sqrt(mean_squared_error(Y_test, Y_pred3))\nfrom sklearn.metrics import r2_score\nr2 = r2_score(Y_test, Y_pred3)\nprint('RMSE = ',rms, ' R2 score = ',r2)","499fbfb0":"There is 0 value only in Item_Visibilty feature. We can approch this in 2 ways:\n* From the data description, we know that Item_Visibility is calculated in %. So, 0% can mean that the item was not on display. We can then,treat it as a normal data and move ahead with the analysis.\n* Or we can see it as missing value. Then, it belongs to the continuous feature and can be treated in the same way as Item Weight.\n\nHere, I see it as a missing data.","c078d0e6":"Eventhough, it looks like Sales increase with MRP, there are less number of sales for high MRP value. This trend seems natural, with less price, items tends to be sold out quickly.","11fa6f26":"In our data, values of Item Visibility, Outlet size and Item weight. \nWe can safely say that these are missing completely at random.\n\nI will impute them.","38c7bd5c":"Relationship of  target variable with Categorical Features.","7b3c719f":"Turns out that the VIP is skewed positively.\n","2cfd9ea7":"# Regression\n\nI will use SVM","3db21b25":"Oh oh, look at the first graph. Item Fat Content has error. Low Fat, LF and low fat all belongs to one category. We will need to correct this error.","60152c09":"Let's have a closer look at our VIP, the target variable.","e8eb9c95":"For categorical missing values, I will replace them with mode.","8ab1cbd0":"Calculating RMSE","a929d389":"Feature Selection Using Lasso","75c2ca93":"Looks like our VIP is not liked that much. Sad!\nOnly Item_MRP and the target variable has correltion coefficient above 0.5.\nBut Multicollinearity won't be a problem. ;)\n","3da9f359":"Treating our Categorical friends.","05309cac":"* Dropfirst is used to avoid dummy variale trap.\n* Since Item_Type has many categories, we can use hashing technique on it.\n","4f50e117":"I stored the numerical and categorical feature names in a variable for future use.","6c16ed88":"Perfect. \n* Low fat Items were sold little more than regular. Who can stay away from the good stuff right?\n* Small outlet size means congesting, maybe that's why sales went down. \n","db1380a6":"Some more insights","005a484b":"* Has nan values, ","3a6b6c56":"Checking for 0 valus.","b6010c43":"Let's have a look at the structure of our training data...","49bdc445":"Let's have a peek into the data.","45bdcea8":"* We can see that sales in grocery stores are rare even in previous years. \n* Many outliers are present. These are establishments having very huge sales for that year.","4bd16181":"Checking for missing data. \n\n\nSometimes missing data can be written as nan or just replaced as 0. \nIn some features 0 does not make sense, so that time it is safe to assume that this might be a missing data.","ec0d3c56":"Let's peek into the big picture.","af0ce1f8":"Wow, not much of a linear data. \n\n\nOutlet_Establishment year graph looks odd, lets look at the values. ","3b0cf32e":"Since, there are only 9 values here, we can treat this feature as categorical data and encode or bin it.","96278461":"After feature scaling, RMSE decreased but R square increased.","c7fdae50":"Relationship with Continuous Features.","67c836a1":"Correlation\n\nNow to check the relationship between our VIP and it's followers, I will use heatmap.","786db350":"Feature Engineering\n\nI will bin the Outlet_Establishment_Year feature. ","19cfe456":"# EDA\n\nExploratory data analysis is a crucial step of Data Analysis that helps in understanding the data. \nEDA gives insight and knowledge to the data which later helps us to build a suitable model. \n\nFor this notebook, I chose BigMart Sales data and the task is to build a regression model to prdict the sales of the items.\n\nThe data has both numerical and categorical features with missing values.\nLooks like I can apply all basic EDA techniques here!","ad04258b":"Model after feature scaling","edabe4a7":"* There are 9 features among which 5 are categorical and 4 are continuous.\n* Target variable is Item_Outlet_Sales and it is a continuous variable.\n* Both training and testing data have the same continuous and categorical features with the exception of target variable.\n* Eventhogh number of features are same, the testing data has less number of observations. Having too many observations will affect the working of the model since the model tends to fit nicely with the increase of the observations. Hence this difference in the number might help us anyway.\n\nBy looking at the features we can guess existence of relationship between MRP, FatContent, Visibility and the target variable but we won't arrive at a conclusion without looking at the correlations.","f9ee8540":"Normality of Errors\n\nFor checking normality of errors, we will need a model and fit the data to it.","e4898455":"Checking for assumptions:\n","23b4f9a8":"* The training data has missing values in Item_Weight and Outlet_Size which is continuous and categorical feature respectively. Hence, the treatment will be different.\n* The testing data also have null values in the same features.","fd4905c3":"Naturally, supermarkets are large establishments with increased number of choice, quality and quantity which increases salse. Afterall, they are SUPERmarkets ;)","7c21bae3":"Missing data Treatment\n\nThere are 3 types of missing data:\n* Missing completely at random - The data is missing be error and does not depend on any other feature or itself.\n* Missing at Random - Here, the data is missing because of other feature and not itself. For eg., women not disclosing age. Women here is another feature.\n* Not missing at random - The data is missing because of it's nature. For eg., salary, sex etc. \n\nTreatment for missing values varies on the category.","a9bde073":"... and the testing data.","df9fee92":"The graph is funnel shaped. Hence, it is heteroscedasticity.\n\nTo correct this, I will log transform Y variable and try."}}