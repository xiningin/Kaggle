{"cell_type":{"7a35548f":"code","d6a8f803":"code","491280ff":"code","60b0e37e":"code","252a8d5b":"code","3e0574e7":"code","1d612c78":"code","23037982":"code","6751f267":"code","00758487":"code","774a7b60":"code","643d38c3":"code","9cd48bb0":"code","fffcf1c6":"code","cd84a34b":"code","960ef2c6":"code","2e80a1c1":"code","3b90ff8e":"code","2228eaaf":"code","d4c691a6":"code","83e6c00b":"code","c37710be":"code","0644cb82":"code","fcceb52c":"code","2d7149bf":"code","411ac572":"code","986065c4":"code","98d93c5b":"code","6f5c4575":"code","862f8f1f":"code","304950c5":"code","12b138ad":"code","c89220c3":"code","0dfd2718":"code","2d7f768c":"code","6ff57e40":"code","e1d9b95e":"code","9e72a733":"code","b4f95816":"code","68ab05f3":"code","4c7ca3aa":"code","4f9c8b7c":"code","7031478a":"code","f1299d54":"code","220b98a2":"code","4c073aea":"code","fb93f6a7":"code","ebd07dc7":"code","51f7a41c":"code","c08c745d":"code","ce12a5df":"code","08038c71":"code","6a2665df":"code","297ebffa":"code","3e42bdcd":"code","d7622715":"code","da7ac179":"code","bc86b608":"code","9ed93fdf":"code","f0ffd6a4":"code","d6eb748b":"code","93a3d882":"code","f9bd016f":"code","8057e9d0":"code","7ede3657":"code","d87142ad":"code","dca338fb":"code","4bf63893":"code","5de19196":"code","aee05ac5":"code","43831334":"code","dbe865fb":"code","351348c8":"code","ad4f32db":"code","0bfffcba":"code","b922c738":"code","df8d62bb":"code","138fc628":"code","f8e8884a":"code","54a57ae3":"code","1d711cbe":"code","ca834c36":"code","8ffe7cec":"code","beb647fa":"markdown","f7731be2":"markdown","dae0e0eb":"markdown","cfd0ae32":"markdown","a5317cb3":"markdown","17d56e62":"markdown","d4532d3a":"markdown","2bbb6b63":"markdown","b9b006ae":"markdown","7d71d391":"markdown","8fe09464":"markdown"},"source":{"7a35548f":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nfrom pandas import DataFrame\nimport numpy as np\nimport math\nimport os\nimport random\nfrom datetime import datetime, timedelta\nimport time\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nfrom IPython.display import Image\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.utils import shuffle\nimport seaborn as sns   \nfrom scipy.stats import boxcox\nimport warnings\nimport xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics, preprocessing\n%matplotlib inline","d6a8f803":"# Lendo o dataset de treino e teste\n\ntrain = pd.read_csv(\"..\/input\/dataset_treino.csv\", \n                    parse_dates = True, low_memory = False)\n\ntest = pd.read_csv(\"..\/input\/dataset_teste.csv\", \n                    parse_dates = True, low_memory = False)\n\n\n# Lendo o dataset com informacoes das lojas\n\n\nlojas = pd.read_csv(\"..\/input\/lojas.csv\", \n                    low_memory = False)\n","491280ff":"print('O Dataframe de treino possui ' + str(test.shape[0]) + ' linhas e ' + str(test.shape[1]) + ' colunas')","60b0e37e":"print('O Dataframe de treino possui ' + str(train.shape[0]) + ' linhas e ' + str(train.shape[1]) + ' colunas')","252a8d5b":"print('O Dataframe de lojas possui ' + str(lojas.shape[0]) + ' linhas e ' + str(lojas.shape[1]) + ' colunas')","3e0574e7":"test.head(10)","1d612c78":"##### Visualizando as primeiras 10 linhas do dataframe de lojas\nlojas.head(10)","23037982":"# examinar os tipos de dados e estat\u00edsticas descritivas \nprint (train.info ()) \nprint (train.describe ())","6751f267":"print (lojas.info ()) \nprint (lojas.describe ())","00758487":"# Verificando a existencia de dados missing no dataset de treino (dados faltantes)\ntrain.isnull().sum()","774a7b60":"train['Open'].value_counts().plot(kind='bar', figsize=(6,6))\nplt.title('situa\u00e7\u00e3o da loja')\nplt.xlabel('Open')\nplt.ylabel('Frequency')\nplt.show()","643d38c3":"test['Open'].value_counts().plot(kind='bar', figsize=(6,6))\nplt.title('situa\u00e7\u00e3o da loja')\nplt.xlabel('Open')\nplt.ylabel('Frequency')\nplt.show()","9cd48bb0":"##### Visualizando as primeiras 10 linhas do dataframe de traino\ntrain.head(10)","fffcf1c6":"plt.figure(figsize=(14,3))\nInsulin_plt = train.groupby(train['Sales']).Open.count().reset_index()\nsns.distplot(train[train.Open == 0]['Sales'], color='red', kde=False, label='Loja fechada')\nsns.distplot(train[train.Open == 1]['Sales'], color='green', kde=False, label='Loja aberta')\nplt.legend()\nplt.title('Histograma dos valores das vendas, dependendo da situa\u00e7\u00e3o da loja')\nplt.show()","cd84a34b":"# Verifica\u00e7\u00e3o de lojas fechadas\nlojas_fechadas = train[(train.Open == 0) & ((train.Sales == 0))]","960ef2c6":"lojas_fechadas.head()","2e80a1c1":"print('Existem ' + str(lojas_fechadas.shape[0]) + ' lojas fechadas')","3b90ff8e":"# Verificando se existem lojas fechadas com vendas\ntrain[(train.Open == 0) & (train.Sales != 0)].count()","2228eaaf":"lojas_aberta_sem_venda = train[(train.Open != 0) & (train.Sales == 0)]","d4c691a6":"lojas_aberta_sem_venda.head()","83e6c00b":"print('Existem ' + str(lojas_aberta_sem_venda.shape[0]) + ' lojas abertas sem vendas')","c37710be":"train.shape","0644cb82":"print(train[(train[\"Open\"] == 0)].head(10))","fcceb52c":"# Criando a coluna TicketMedio para verificar a rela\u00e7\u00e3o entre vendas e consumidores\ntrain['TicketMedio'] = train['Sales']\/train['Customers']\ntrain['TicketMedio2'] = train.groupby(\"Store\")[\"TicketMedio\"].mean()\ntrain['TicketMedio2'] = (train[\"TicketMedio2\"].fillna(train.groupby(\"Store\")[\"TicketMedio\"].transform(\"mean\")))\ntrain['TicketMedio'] = train['TicketMedio2']\n# Criando a coluna com a m\u00e9dia geral de Clientes de cada loja\ntrain['MediaClientes'] = train.groupby(\"Store\")[\"Customers\"].mean()\ntrain['MediaClientes'] = (train[\"MediaClientes\"].fillna(train.groupby(\"Store\")[\"Customers\"].transform(\"mean\")))\n#\ntrain.drop(['TicketMedio2'], axis = 1, inplace = True)\ntrain['TicketMedio'].describe()","2d7149bf":"train.head(10).sort_values(by=['Store'], ascending=False)","411ac572":"# Verificando se existe algum valor missing\ntrain.isnull().sum()","986065c4":"# Criando as Colunas mes e ano \ntrain['Year'] = pd.DatetimeIndex(train['Date']).year\ntrain['Month'] = pd.DatetimeIndex(train['Date']).month","98d93c5b":"train[\"StateHoliday\"].value_counts()","6f5c4575":"train[\"SchoolHoliday\"].value_counts()","862f8f1f":"cleanup_nums = {\"StateHoliday\": {\"a\": 1, \"b\": 2, \"c\": 3}}","304950c5":"# Convertendo a coluna StateHoliday para n\u00fameros\ntrain.replace(cleanup_nums, inplace=True)","12b138ad":"train.info()","c89220c3":"# Verificando a rela\u00e7\u00e3o entre Vendas e Clientes\nN = 2\ncolors = np.random.rand(N)\nplt.scatter(train['Sales'], train['Customers'],  color='olive') \nplt.xlabel ('Sales')\nplt.ylabel ('Customers')\nplt.show() ","0dfd2718":"# Verificando a rela\u00e7\u00e3o entre Ticket M\u00e9dio e Clientes \nplt.scatter(train['Customers'], train['TicketMedio']) \nplt.xlabel ('Customers')\nplt.ylabel ('TicketMedio')\nplt.show() ","2d7f768c":"##### Visualizando as primeiras 10 linhas do dataframe de lojas\nlojas.head(10)","6ff57e40":"print (lojas.info ()) \nprint (lojas.describe ())","e1d9b95e":"lojas[\"PromoInterval\"].value_counts()","9e72a733":"lojas[\"StoreType\"].value_counts()","b4f95816":"lojas[\"Assortment\"].value_counts()","68ab05f3":"cleanup_nums = {\"PromoInterval\": {\"Jan,Apr,Jul,Oct\": 1, \"Feb,May,Aug,Nov\": 2, \"Mar,Jun,Sept,Dec\": 3},\n                \"StoreType\": {\"a\": 1, \"b\": 2, \"c\": 3, \"d\": 4},\n                \"Assortment\": {\"a\": 2, \"b\": 2, \"c\":3 }}","4c7ca3aa":"# Convertendo as colunas PromoInterval, StoreType e Assortment para n\u00fameros\nlojas.replace(cleanup_nums, inplace=True)","4f9c8b7c":"lojas.head(10)","7031478a":"# Verificando a existencia de dados missing no dataset das lojas\nlojas.isnull().sum()","f1299d54":"lojas.groupby('Promo2')[\"Promo2SinceWeek\",\"Promo2SinceYear\",\"PromoInterval\"].count()","220b98a2":"lojas['Promo2SinceWeek'] = lojas.apply(lambda row: 0 if (pd.isna(row['Promo2SinceWeek'])) else row['Promo2SinceWeek'], axis=1)\nlojas['Promo2SinceYear'] = lojas.apply(lambda row: 0 if (pd.isna(row['Promo2SinceYear'])) else row['Promo2SinceYear'], axis=1)\nlojas['PromoInterval'] = lojas.apply(lambda row: 0 if (pd.isna(row['PromoInterval'])) else row['PromoInterval'], axis=1)","4c073aea":"from sklearn.preprocessing import Imputer\nimputer = Imputer().fit(lojas)\nlojas_imputed = imputer.transform(lojas)","fb93f6a7":"lojas_tratadas = pd.DataFrame(lojas_imputed, columns=lojas.columns.values)","ebd07dc7":"lojas_tratadas.head(10)","51f7a41c":"# Verificando a existencia de dados missing no dataset das lojas_tratadas\nlojas_tratadas.isnull().sum()","c08c745d":"# Agora que j\u00e1 realizei a transforma\u00e7\u00f5es necess\u00e1rias nos dados, farei a jun\u00e7\u00e3o dos datasets train e lojas\ntrain_model = pd.merge(train, lojas_tratadas, how = 'left', on='Store')","ce12a5df":"train_model.head(10).sort_values(by=['Customers'], ascending=False)","08038c71":"# Eliminando as colunas Customers e Date\ntrain_model = train_model.drop('Customers', axis=1)\ntrain_model = train_model.drop('Date', axis=1)","6a2665df":"# Analisando as correla\u00e7\u00f5es\ncorr = train_model.corr()\n_ , ax = plt.subplots( figsize =( 16 , 14 ) )\ncmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n_ = sns.heatmap(corr, cmap = cmap, square=True, cbar_kws={ 'shrink' : .9 }, ax=ax, annot = True, annot_kws = {'fontsize' : 10 })","297ebffa":"Vendas = train_model['Sales']\ntrain_model = train_model.drop(['Sales'], axis=1)\ntrain_model['Sales'] = Vendas","3e42bdcd":"train_model.shape","d7622715":"train_model.head(10)","da7ac179":"test.head(10)","bc86b608":"test.isnull().sum()","9ed93fdf":"test[\"StateHoliday\"].value_counts()","f0ffd6a4":"cleanup_nums = {\"StateHoliday\": {\"a\": 1, \"b\": 2, \"c\": 3}}","d6eb748b":"# Convertendo a coluna StateHoliday para n\u00fameros\ntest.replace(cleanup_nums, inplace=True)","93a3d882":"test[\"Open\"].value_counts()","f9bd016f":"test_nan = test[(test.Open.isnull())]","8057e9d0":"test_nan.head()","7ede3657":"test_nan.isnull().sum()","d87142ad":"test['Open'] = test.apply(lambda row: 1 if (pd.isna(row['Open'])) else row['Open'], axis=1)","dca338fb":"test['Year'] = pd.DatetimeIndex(test['Date']).year\ntest['Month'] = pd.DatetimeIndex(test['Date']).month","4bf63893":"test.head()","5de19196":"merge_test = train_model[['Store','TicketMedio','MediaClientes']]","aee05ac5":"test_merge = merge_test.groupby('Store')['TicketMedio','MediaClientes'].max()","43831334":"test_merge = test_merge.reset_index()","dbe865fb":"test_merge.head()","351348c8":"test_merge2 = test.merge(test_merge, left_on='Store', right_on='Store')","ad4f32db":"test_merge3 = pd.merge(test_merge2, lojas_tratadas, how = 'left', on='Store')","0bfffcba":"test_model = test_merge3","b922c738":"test_model = test_model.drop('Date', axis=1)","df8d62bb":"test_model.head(10)","138fc628":"# Fun\u00e7\u00e3o para efetuar a Normaliza\u00e7\u00e3o\ndef normalize(df,columns,tipo):\n    df = df.convert_objects(convert_numeric=True)\n    result = df.copy()\n    for feature_name in df.columns:\n        if feature_name not in columns: \n            if tipo == 'min-max':\n                max_value = df[feature_name].max()\n                min_value = df[feature_name].min()\n                if (max_value - min_value) == 0:\n                    result[feature_name] = (df[feature_name] - min_value) \/ 1\n                else:\n                    result[feature_name] = (df[feature_name] - min_value) \/ (max_value - min_value)\n            else: \n                if tipo == 'media':  \n                    std_value = df[feature_name].std()\n                    mean_value = df[feature_name].mean()\n                    if std_value == 0:\n                        result[feature_name] = ((df[feature_name] - mean_value) \/ 1)\n                    else:\n                        result[feature_name] = ((df[feature_name] - mean_value) \/ std_value)\n    return result","f8e8884a":"#train_model.sort_index(axis=1,inplace=True)","54a57ae3":"# Creiando os datasets de treino e teste normalizados \n#train_model_N = normalize(train_model,[\"Store\",\"Sales\"],'min-max') \n#test_model_N  = normalize(test_model,[\"Store\",\"Id\"],'min-max') \ntrain_model_N = normalize(train_model,[\"Store\",\"Sales\"],'media') \ntest_model_N  = normalize(test_model,[\"Store\",\"Id\"],'media') \ntrain_stores_model = dict(list(train_model_N.groupby('Store')))\ntest_stores_model = dict(list(test_model_N.groupby('Store')))","1d711cbe":"# Aplicando o algoritmo XGBoost com valida\u00e7\u00e3o cruzada. \n\n# Ap\u00f3s varias tentativas de melhorar o desenpenho do modelo, percebi que utilizando todo o conjunto de dados, \n# tanto de treino como de teste, meu melhor rmspe foi de 0,82723, ent\u00e3o resolvi aplicar o modelo, considerando o\n# conjunto de dados de cada loja individualemnte, o que fez melhorar significativamente a precis\u00e3o do modelo.\n\nimport scipy.stats as st\n\none_to_left = st.beta(10, 1)  \nfrom_zero_positive = st.expon(0, 50)\n\nparams = {  \n    \"n_estimators\": st.randint(3, 40),\n    \"max_depth\": st.randint(3, 40),\n    \"learning_rate\": st.uniform(0.01, 0.4),\n    \"colsample_bytree\": one_to_left,\n    \"subsample\": one_to_left,\n    \"gamma\": st.uniform(0, 10),\n    'reg_alpha': from_zero_positive,\n    'seed': [42],\n    \"min_child_weight\": from_zero_positive,\n}\n\nfit_params = {'early_stopping_rounds' : [10]}\n\npreds = pd.Series()\ny_preds = list()\nfor i in test_stores_model:  \n    store = train_stores_model[i]\n    X_train = store.drop([\"Sales\", \"Store\"],axis=1)\n    y_train = store[\"Sales\"]\n    X_test  = test_stores_model[i].copy()   \n    store_ind = X_test[\"Id\"]\n    X_test.drop([\"Id\",\"Store\"], axis=1,inplace=True)\n    X_train.sort_index(axis=1,inplace=True)\n    X_test.sort_index(axis=1,inplace=True)\n    xg_train = xgb.DMatrix(X_train, label=y_train)\n    xg_test  = xgb.DMatrix(X_test)\n    bst = xgb.XGBRegressor()\n    print('Treinando a loja ' + str(i))\n    gs = RandomizedSearchCV(bst, params, n_jobs=1)  \n    gs.fit(X_train, y_train)\n    y_pred = gs.predict(X_test)\n    y_pred = y_pred.tolist()\n    preds = preds.append(pd.Series(y_pred, index=store_ind))    ","ca834c36":"preds = pd.DataFrame({ \"Id\": preds.index, \"Sales\": preds.values})","8ffe7cec":"# Salvando \npreds.to_csv(\"sample_submission.csv\", sep=',', index=False)","beb647fa":"Existem 11 valores missing na coluna Open, irei preenche-los com o valor 1, levando em considera\u00e7\u00e3o que na maioria dos dias a loja 622 esteve aberta","f7731be2":"No Dataset lojas temos tr\u00eas colunas categ\u00f3ricas, ent\u00e3o primeiramente irei transforma-las para valores num\u00e9ricos","dae0e0eb":"### Carga de dados","cfd0ae32":"### An\u00e1lise Explorat\u00f3ria","a5317cb3":"Para as colunas CompetitionDistance, CompetitionOpenSinceMonth e CompetitionOpenSinceYear preencherei os valores faltantes com a m\u00e9dia dos valores destas colunas.","17d56e62":"Agora realizarei o merge entre o dataset de teste e o dataset lojas","d4532d3a":"### Descri\u00e7\u00e3o dos Campos\nID - um ID que representa uma tupla (Store, Date) dentro do conjunto de dados\n\nStore - um ID \u00fanico para cada loja\n\nSales - o volume de neg\u00f3cios de um determinado dia (\u00e9 isso que voc\u00ea est\u00e1 prevendo)\n\nCustomers - o n\u00famero de clientes em um determinado dia\n\nOpen - um indicador para saber se a loja estava aberta: 0 = fechada, 1 = aberta\n\nStateHoliday - indica um feriado estadual. Normalmente, todas as lojas, com poucas exce\u00e7\u00f5es, est\u00e3o fechadas nos feriados\nestaduais. Note que todas as escolas est\u00e3o fechadas nos feriados e fins de semana. a = feriado p\u00fablico, b = feriado de P\u00e1scoa, c = Natal, 0 = Nenhum\n\nSchoolHoliday - indica se a (Store, Date) foi afetada pelo fechamento de escolas p\u00fablicas\n\nStoreType - diferencia entre 4 modelos de lojas diferentes: a, b, c, d\n\nAssortment - descreve um n\u00edvel: a = b\u00e1sico, b = extra, c = estendido\n\nCompetitionDistance - dist\u00e2ncia em metros at\u00e9 a loja concorrente mais pr\u00f3xima\n\nCompetitionOpenSince[Month\/Year] - indica o ano e m\u00eas aproximado da hora em que o concorrente mais pr\u00f3ximo foi aberto\n\nPromo - indica se uma loja est\u00e1 executando uma promo\u00e7\u00e3o nesse dia\n\nPromo2 - promo\u00e7\u00e3o cont\u00ednua e consecutiva para algumas lojas: 0 = loja n\u00e3o est\u00e1 participando, 1 = loja est\u00e1 participando\n\nPromo2Since[Year\/Week] - descreve o ano e a semana do calend\u00e1rio em que a loja come\u00e7ou a participar do Promo2\n\nPromoInterval - descreve os intervalos consecutivos que o Promo2 \u00e9 iniciado, indicando os meses em que a promo\u00e7\u00e3o \u00e9 iniciada novamente. Por exemplo. \"Feb, May, Aug, Nov\" significa que cada rodada come\u00e7a em fevereiro, maio, agosto, novembro de qualquer ano para aquela loja.","2bbb6b63":"## Preparando os dados para o Modelo\n\nUtilizarei o mesmo processo de transforma\u00e7\u00e3o aplicado no dataset de treino tamb\u00e9m no dataset de teste, transformando as colunas categoricas, preenchendo os valores missing, crindo novas colunas derivadas e fazendo o merge entre o dataset de teste e o dataset lojas.","b9b006ae":"# Solu\u00e7\u00e3o Proposta para Competi\u00e7\u00e3o DSA de Machine Learning\n# Edi\u00e7\u00e3o Mar\u00e7o-2019","7d71d391":"Analisando as colunas Promo2SinceWeek, Promo2SinceYear e PromoInterval posso perceber que sempre Promo2 \u00e9 zero elas n\u00e3o possuem valores, ent\u00e3o irei atribuir a estas colunas o valor zero.","8fe09464":"## Analisando e transformando os dados de teste"}}