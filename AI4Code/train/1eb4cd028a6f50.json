{"cell_type":{"ebe5af5d":"code","07fe3b37":"code","9da3048c":"code","bb97a6f3":"code","72029620":"code","a8fe54a2":"code","a9e8b46a":"code","8b83a41d":"code","963bccb6":"code","a0d9c66b":"code","e756aac7":"code","98dab4dc":"code","9d8fbd66":"markdown","ee2d429b":"markdown","4ea191ce":"markdown"},"source":{"ebe5af5d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","07fe3b37":"df = pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')","9da3048c":"# Converting Total Charges to a numerical data type.\ndf.TotalCharges = pd.to_numeric(df.TotalCharges, errors='coerce')\n#Removing missing values \ndf.dropna(inplace = True)\n\ndf.isnull().sum()","bb97a6f3":"#Remove customer IDs from the data set\ndf2 = df.iloc[:,1:]\n#Convertin the predictor variable in a binary numeric variable\ndf2['Churn'].replace(to_replace='Yes', value=1, inplace=True)\ndf2['Churn'].replace(to_replace='No',  value=0, inplace=True)\n\n#Let's convert all the categorical variables into dummy variables\ndf_dummies = pd.get_dummies(df2)\ndf_dummies.head()","72029620":"# We will use the data frame where we had created dummy variables\ny = df_dummies['Churn'].values\nX = df_dummies.drop(columns = ['Churn'])\n\n# Scaling all the variables to a range of 0 to 1\nfrom sklearn.preprocessing import MinMaxScaler\nfeatures = X.columns.values\nscaler = MinMaxScaler(feature_range = (0,1))\nscaler.fit(X)\nX = pd.DataFrame(scaler.transform(X))\nX.columns = features","a8fe54a2":"# Create Train & Test Data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","a9e8b46a":"# Create the Confusion matrix\nfrom sklearn.metrics import classification_report, confusion_matrix  ","8b83a41d":"# Running logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nresult = model.fit(X_train, y_train)\n\nfrom sklearn import metrics\npreds = model.predict(X_test)\n# Print the prediction accuracy\nprint (metrics.accuracy_score(y_test, preds))\nprint(confusion_matrix(y_test,preds))  ","963bccb6":"from sklearn.ensemble import RandomForestClassifier\nmodel_rf = RandomForestClassifier(n_estimators=1000 , oob_score = True, n_jobs = -1,\n                                  random_state =50, max_features = \"auto\",\n                                  max_leaf_nodes = 30)\nmodel_rf.fit(X_train, y_train)\n\n# Make predictions\npreds = model_rf.predict(X_test)\nprint (metrics.accuracy_score(y_test, preds))\nprint(confusion_matrix(y_test,preds))  ","a0d9c66b":"importances = model_rf.feature_importances_\nweights = pd.Series(importances,\n                 index=X.columns.values)\nweights.sort_values()[-10:].plot(kind = 'barh')","e756aac7":"from sklearn.svm import SVC\n\nmodel.svm = SVC(kernel='linear') \nmodel.svm.fit(X_train,y_train)\npreds = model.svm.predict(X_test)\nprint (metrics.accuracy_score(y_test, preds))\nprint(confusion_matrix(y_test,preds))  ","98dab4dc":"from xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\nprint (metrics.accuracy_score(y_test, preds))\nprint(confusion_matrix(y_test,preds))","9d8fbd66":"Hello, welcome to the part 2 of the notebook where we'll be focusing on the machine learning model.","ee2d429b":"### Tuning the model right for the business?! You can't have it both ways..\n\nA way to measure performance in a model is not just 1 value to predict it's accuracy, but also the precission and recall rate which can be used to tune the model to meet the business objectives\n\nTuning the model? What does this mean? Before we go there let's think about the possible outcome of the predictions. \n\nWhen the model predicts the customer has churn correctly, it is called **true positive** and when it predicts the customer has churn incorrectly it is called **false positive**. Similarly when the model predicts will not churn correctly is called **true negative** and incorrectly predicting customers that will not churn as churned this is called **false negative.** \n\nnote: note true means the model predicted correctly and false means the model predicted incorrectly.\n\nNot let's have a think here....\n\nAs a business you don't want to think the customer will not churn and be totatlly wrong! This means false neagtives!\n\nYou think the customer won't leave you in the circumstances so you won't be providing them with any offers. Instead you rather have predict the customer may leave next month be wrong in fact they will be staying with you (false positive). But you'll be providing an offers\/discount anyway incase you are wrong and they do leave. In data science term and this desired situation is called **high recall.**\n\n\nThe business wants to focus more on retaining customers and rather provide offers to the ones that they think will churn next month in order to retain them (i.e. end of mobile contract, it is very likely the customer may look to a new provider for a better deal). The business rather pay a small fee to captivate the customer to staying than losing 18-36 months of the mobile contract.\n\n\nHowever a special case is if the business considers a re-engagement email campaign which says something like \u201cWe noticed you may be leaving us. Please don\u2019t!\u201d. You are more focus on reducing the number of predictions the customer has churn incorrectly (false positives). In other words, you would want to minimise the number of happy users who see this email, and instead have this email almost exclusively hit users in danger of churning. This situtation desire in the model is **high precison.**\n\nThere is a trade off for precision and recall, you can't have 100% accuracy rate so you would have to compromised between the two.\n\nTL;TR\n\nIt is important what the business strategy is to reduce churn rate, are they going to provide offers to majority of their customers when their contract ends, or would you rather not provide offers to that many customers as they will be staying anyway so you can provide more savings from the offers. This ask the question what would they rather do in the two circumstances. \n\nThis decision determines how the model is tune to yield results better for the business - **recision or recall or both equally important?**\n\nReference:\n\n[Tuning Precision & Recall To Minimize Churn](https:\/\/www.vidora.com\/ml-in-business\/reducing-user-churn-with-machine-learning-precision-and-recall\/#:~:text=What%20are%20%E2%80%9Cprecision%E2%80%9D%20and%20%E2%80%9C,the%20churn%20prediction%20algorithm%20is.)\n\n[Quora: When is precision more important over recall](https:\/\/www.quora.com\/When-is-precision-more-important-over-recall)","4ea191ce":"### Training a ML model pipeline (my approah):\n\nThe notebook is a good experiemental environment to try different things out.\n\nThere is a general approach to training a machine learning model which I aim to follow and this is my approach for this dataset.\n\n[Part 1 - EDA of the dataset](https:\/\/www.kaggle.com\/richieone13\/1-churn-notebook-eda)\n* import the files\n* EDA on the dataset\n* treat missing data\n\nPart 2 - Machine Learning Model\n* selectively choose the features that want to include (select minimum features and scale horizontally - based on Exploratory Data Analysis (EDA)) this will help with overfitting and peformance in training the model)\n* OPTIONAL - remove the outliers if there are any so the model outliers does not skew the model - maybe using 3 standard deviation, data that is < 1%, data that is > 99%\n* OPTIONAL - shuffling the data to avoid skewness\n* import one hot encoder\n* OPTIONAL - scaling from 0 to 1 (not required rescaling as only concern of the output whether churn or not in the previous month)\n* measure the result via a confusion matrix\n* experiement with different models"}}