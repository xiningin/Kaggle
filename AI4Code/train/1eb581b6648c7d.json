{"cell_type":{"d895589b":"code","60394e6d":"code","d7a0836e":"code","16f4e2a2":"code","4e694967":"code","f4691e2a":"code","7c1f548c":"code","6da6007b":"code","39daa35b":"code","7d782628":"code","dae9edf2":"code","7ab2038b":"code","e63036d0":"code","606c6caa":"code","dc334bd9":"code","75d1c45c":"code","25a50d76":"code","c152d6c6":"code","f867f560":"code","9a114ab8":"code","12ea1ee0":"code","870365c9":"code","20cb05ac":"code","aecccd86":"code","538a0d0a":"code","64e4ee4b":"code","da1b5ab8":"code","bc75d966":"code","a9d789dd":"code","be7207d0":"code","b3883517":"code","20ad5de6":"code","91b37ca3":"code","7360bf73":"code","4f05d827":"code","06bdbf93":"code","732ecd6f":"code","43da3b0e":"code","ff861b79":"code","9bcd683b":"code","eb2eae39":"code","c0f405e6":"code","96396033":"code","7cde2b61":"code","c53582e0":"code","490c7680":"code","a41b115a":"markdown","a766654e":"markdown","f66e86be":"markdown","d23626bb":"markdown","7b921022":"markdown","6197e70e":"markdown","0dd5cbf0":"markdown","4108a844":"markdown","80aeedac":"markdown","f1e37cfb":"markdown","782cf23d":"markdown","037ae7a0":"markdown","5f0c4b07":"markdown","1dc3fa31":"markdown","a91be3c4":"markdown","d061435b":"markdown","65cbe5a6":"markdown","7cfd6a90":"markdown","587e51cc":"markdown","401f7b10":"markdown","04c20c48":"markdown","2e602f46":"markdown","4dcf15a0":"markdown","ef03bc6b":"markdown","03991de2":"markdown","37a4ea53":"markdown","34a71de3":"markdown","1a029839":"markdown","07b9fa68":"markdown","2681ca32":"markdown","680bde06":"markdown","1f371194":"markdown","1496a50a":"markdown","6a11498b":"markdown","329d4f65":"markdown","8ce40cfb":"markdown","09134ad5":"markdown","d9f3bd0a":"markdown","f1641f54":"markdown","18e12ad0":"markdown","58600c7b":"markdown","695f9223":"markdown","8150b584":"markdown","3bcb9ea0":"markdown","0afbf1ba":"markdown","5b267c7a":"markdown","c7be7217":"markdown"},"source":{"d895589b":"import numpy as np # linear algebra\n\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas.api.types import CategoricalDtype\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nfrom xgboost import XGBRegressor, plot_importance\n\nimport optuna\nimport shap\nimport pickle","60394e6d":"# load data\ndata_dir = \"..\/input\/house-prices-advanced-regression-techniques\/\"\ndf_train = pd.read_csv(data_dir + \"train.csv\", index_col=\"Id\")\ndf_test = pd.read_csv(data_dir + \"test.csv\", index_col=\"Id\")","d7a0836e":"# # df_train info, describe and head\n# print('df_train.info()\\n')\n# print(df_train.info())\n\n# print('\\ndf_train.describe()\\n') \n# print(df_train.describe())\n\n# print('\\ndf_train.head()\\n') \n# print(df_train.head())\n\n# # df_test info, describe and head\n# print('\\ndf_test.info()\\n')\n# print(df_test.info())\n\n# print('\\ndf_test.describe()\\n') \n# print(df_test.describe())\n\n# print('\\ndf_test.head()\\n') \n# print(df_test.head())","16f4e2a2":"df_combined = pd.concat([df_train, df_test])","4e694967":"# examine GarageYrBlt column\nsns.histplot(df_combined['GarageYrBlt'])\ndf_combined[df_combined['GarageYrBlt']>2009]['GarageYrBlt'].value_counts()","f4691e2a":"\ndef clean(df_combined):\n    # 1. change None to NA in MasVnrType\n    df_combined['MasVnrType'] = df_combined['MasVnrType'].where(df_combined.MasVnrType != 'None', 'NA')\n\n    assert (df_combined['MasVnrType'].str.contains('None').sum()==0)\n\n    # 2. if GarageYrBlt > 2010, replace with YearBuilt\n    df_combined['GarageYrBlt'] = df_combined['GarageYrBlt'].where(df_combined.GarageYrBlt <= 2010, df_combined.YearBuilt)\\\n\n    assert (df_combined['GarageYrBlt']<2010).any()\n    \n    return df_combined","7c1f548c":"# nominal features (unordered)\nnominal_features = [\"MSSubClass\", \"MSZoning\", \"Street\", \"Alley\", \"LandContour\", \"Utilities\", \"LotConfig\", \n                    \"Neighborhood\", \"Condition1\", \"Condition2\", \"BldgType\", \"HouseStyle\", \"RoofStyle\", \n                    \"RoofMatl\", \"Exterior1st\", \"Exterior2nd\", \"MasVnrType\", \"Foundation\", \"Heating\", \n                    \"CentralAir\", \"GarageType\", \"MiscFeature\", \"SaleType\", \"SaleCondition\"]\n\n# ordinal features (ordered)\n# common levels\nfive_levels = [\"Po\", \"Fa\", \"TA\", \"Gd\", \"Ex\"]\nten_levels = list(range(1,11))\n\nordinal_features = {\n    \"LotShape\": [\"Reg\", \"IR1\", \"IR2\", \"IR3\"],\n    \"LandSlope\": [\"Sev\", \"Mod\", \"Gtl\"],\n    \"OverallQual\": ten_levels,\n    \"OverallCond\": ten_levels,\n    \"ExterQual\": five_levels,\n    \"ExterCond\": five_levels,\n    \"BsmtQual\": five_levels,\n    \"BsmtCond\": five_levels,\n    \"BsmtExposure\": [\"No\", \"Mn\", \"Av\", \"Gd\"],\n    \"BsmtFinType1\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"BsmtFinType2\": [\"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n    \"HeatingQC\": five_levels,\n    \"Electrical\": [\"Mix\", \"FuseP\", \"FuseF\", \"FuseA\", \"SBrkr\"],\n    \"KitchenQual\": five_levels,\n    \"Functional\": [\"Sal\", \"Sev\", \"Maj1\", \"Maj2\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n    \"FireplaceQu\": five_levels,\n    \"GarageFinish\": [\"Unf\", \"RFn\", \"Fin\"],\n    \"GarageQual\": five_levels,\n    \"GarageCond\": five_levels,\n    \"PavedDrive\": [\"N\", \"P\", \"Y\"],\n    \"PoolQC\": five_levels,\n    \"Fence\": [\"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"]\n}\n\n# Add a NA level for missing values\nordinal_features = {key: [\"NA\"] + value for key, value in\n                  ordinal_features.items()}\n\ndef encode(df):\n    # Nominal categories\n    for name in nominal_features:\n        df[name] = df[name].astype(\"category\")\n        # Add a None category for missing values\n        if \"NA\" not in df[name].cat.categories:\n            df[name].cat.add_categories(\"NA\", inplace=True)\n    # Ordinal categories\n    for name, levels in ordinal_features.items():\n        df[name] = df[name].astype(CategoricalDtype(levels,\n                                                    ordered=True))\n    return df\n\n# encode(df_combined)\n\n# df_combined.info()","6da6007b":"\n\ndef impute(df):\n    \n    df['Electrical'] = df['Electrical'].fillna(df['Electrical'].mode())\n    df['KitchenQual'] = df['KitchenQual'].fillna(\"TA\")\n    df['Functional'] = df['Functional'].fillna(\"Typ\")\n    df['SaleType'] = df['SaleType'].fillna(\"Oth\")\n    df['GarageYrBlt'] = df['GarageYrBlt'].fillna(df['YearBuilt'])\n    \n#     # Get names of columns with missing values\n#     cols_with_missing = [col for col in df.columns if col != 'SalePrice' and df[col].isnull().any()]\n#     # Make new columns indicating imputed features (`SalePrice` column exluded)\n#     for col in cols_with_missing:\n#         df[col + '_was_missing'] = df[col].isnull()\n#         df[col + '_was_missing'] = (df[col + '_was_missing']) * 1\n    # Impute 0 for missing numeric values\n    for name in df.select_dtypes(\"number\"):\n        df[name] = df[name].fillna(0)\n    # Impute \"None\" for missing categorical values\n    for name in df.select_dtypes(\"category\"):\n        df[name] = df[name].fillna(\"NA\")\n    return df","39daa35b":"def preprocess(df, df_train, df_test):\n    # Preprocessing steps\n    df = clean(df)\n    df = encode(df)\n    df = impute(df)\n    # Reform splits\n    df_train = df.loc[df_train.index, :]\n    df_test = df.loc[df_test.index, :]\n    return df_train, df_test\n\ndf_train, df_test = preprocess(df_combined, df_train, df_test)\n\nprint(df_train.info())\nprint(df_test.info())","7d782628":"# explore dependent variable SalePrice\ndf_train[\"SalePrice\"].describe()","dae9edf2":"# histogram\nsns.histplot(df_train['SalePrice'])\nplt.show()\n\n# boxplot\nsns.boxplot(df_train['SalePrice'])\nplt.show()\n\n# swarm plot\nsns.swarmplot(df_train['SalePrice'], size=2.4)\nplt.show()","7ab2038b":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","e63036d0":"# My default XGB parameters\n\nxgb_params = dict(\n    max_depth=3,                           # maximum depth of each tree - try 2 to 10\n    learning_rate=0.1,                     # effect of each tree - try 0.0001 to 0.1\n    n_estimators=100,                      # number of trees (that is, boosting rounds) - try 1000 to 8000\n    min_child_weight=1,                    # minimum number of houses in a leaf - try 1 to 10\n    colsample_bytree=1,                    # fraction of features (columns) per tree - try 0.2 to 1.0\n    subsample=1,                           # fraction of instances (rows) per tree - try 0.2 to 1.0\n    reg_alpha=0,                           # L1 regularization (like LASSO) - try 0.0 to 10.0\n    reg_lambda=1,                          # L2 regularization (like Ridge) - try 0.0 to 10.0\n    num_parallel_tree=1,                   # set > 1 for boosted random forests\n)","606c6caa":"def score_dataset(X, y, model=XGBRegressor(**xgb_params)):\n    # Label encoding for categoricals\n    for colname in X.select_dtypes([\"category\"]):\n        X[colname] = X[colname].cat.codes\n    # Use RMSLE (Root Mean Squared Log Error) instead of MSE (Mean Squared Error)as evaluation metric\n    # (So, we need to log-transform y to train and exp-transform the predictions)\n    log_y = np.log(y)\n    score = cross_val_score(\n        model, X, log_y, cv=5, scoring='neg_mean_squared_error'\n    )\n    score = np.sqrt(-1 * score.mean())\n\n    return score\n\nX = df_train.copy()\ny = X.pop(\"SalePrice\")\n\nbaseline_score = score_dataset(X, y)\nprint(f\"Baseline score: {baseline_score:.5f} RMSE\")","dc334bd9":"def make_mi_scores(X, y):\n    X = X.copy()\n    \n    # encode all categorical variables as integer dtypes\n    for colname in X.select_dtypes([\"object\", \"category\"]):\n        X[colname], _ = X[colname].factorize()\n    \n    # get mask for discrete variables (variables with integer dtype) to let the mutual_information_regression function know which variables are discrete\n    discrete_features = [pd.api.types.is_integer_dtype(t) for t in X.dtypes]\n    mi_scores = mutual_info_regression(X, y, discrete_features=discrete_features, random_state=321)\n    mi_scores = pd.Series(mi_scores, name='MI Scores', index=X.columns)  # Make the mi scores a pandas series with the original column names as indexes\n    mi_scores = mi_scores.sort_values(ascending=True)\n    return mi_scores\n\ndef plot_mi_scores(scores):\n    plt.barh(list(scores.index), scores)\n#     plt.yticks(list(scores.index))\n    plt.title(\"Mutual Information Scores\")\n    \nmi_scores = make_mi_scores(X, y)\nplt.figure(figsize=(8,20))\nplot_mi_scores(mi_scores)","75d1c45c":"def drop_uninformative(df, mi_scores):\n    return df.loc[:, mi_scores > 0] , list(df.loc[:, mi_scores == 0].columns)\n\nX, dropped_columns = drop_uninformative(X, mi_scores)\n\nscore_dataset(X, y)","25a50d76":"def plot_and_save(df, x, y):\n    \n    if df[x].dtypes.name in ['object', 'category']:\n        sns.stripplot(data=df, x=x, y=y)\n    else:\n        sns.scatterplot(data=df, x=x, y=y)\n    filename = \"{} vs {}\".format(x, y)\n#     plt.savefig(filename)\n#     plt.close()\n    plt.show()\n\n# uncomment below for EDA\n# for column in df_train.columns:\n#     plot_and_save(df_train, column, \"SalePrice\")","c152d6c6":"def add_binary_variables(df):\n    X = add_binary_variable(df, 'hasLotFrontage', 'LotFrontage')\n    X = X.join(add_binary_variable(df, 'has2ndFlr', '2ndFlrSF'))\n    X = X.join(add_binary_variable(df, 'hasBsmt', 'TotalBsmtSF'))\n    X = X.join(add_binary_variable(df, 'hasGarage', 'GarageArea'))\n    X = X.join(add_binary_variable(df, 'hasPool', 'PoolArea'))\n\n    return X\n    \ndef add_binary_variable(df, new_column_name, base_column_name):\n    X = pd.DataFrame()\n    X[new_column_name] = df.apply(lambda row: 0 if row[base_column_name]==0 else 1, axis=1)\n    X[new_column_name] = X[new_column_name].astype(\"bool\")\n    return X\n\n# X = X.join(add_binary_variables(X))  # TODO: add to pipeline instead\n# X.info()\n\n# score_dataset(X, y)","f867f560":"def label_encode(df):\n    X = df.copy()\n    for colname in X.select_dtypes(['category']):\n        X[colname] = X[colname].cat.codes\n    return X\n\ndef one_hot_encode(df, columns): \n    X = df.copy()\n    \n    return pd.get_dummies(df, columns=columns, drop_first=True, dtype=bool)\n\nnominal_features = set(nominal_features) - set(dropped_columns)\n\n# X = label_encode(X)                      # TODO: add to pipeline instead\n# X = one_hot_encode(X, nominal_features)  # TODO: add to pipeline instead\n# X.info(verbose=True)\n\n# score_dataset(X, y)","9a114ab8":"def mathematical_transforms(df):\n    X = pd.DataFrame() # Empty dataframe to hold new features\n    X['LivLotRatio'] = df.GrLivArea \/ df.LotArea\n    X['Spaciousness'] = (df['1stFlrSF'] + df['2ndFlrSF']) \/ df.TotRmsAbvGrd\n    return X\n\n# X = X.join(mathematical_transforms(X))  # TODO: add to pipeline instead\n# X.info()\n\n# score_dataset(X, y)","12ea1ee0":"def counts(df):\n    X = pd.DataFrame()\n    X['PorchTypes'] = df[['WoodDeckSF',\n                        'OpenPorchSF',\n                        'EnclosedPorch',\n                        '3SsnPorch',\n                        'ScreenPorch'\n                        ]].gt(0.0).sum(axis=1)\n\n    X['TotalRoom'] = df.TotRmsAbvGrd + df.FullBath + df.HalfBath\n    return X\n\n# X = X.join(counts(X))  # TODO: add to pipeline instead\n# X.info(verbose=True)\n\n# score_dataset(X, y)","870365c9":"def group_transforms(df):\n    X = pd.DataFrame()\n    X['MedNhbdArea'] = df.groupby('Neighborhood')['GrLivArea'].transform('median')\n    return X\n\n# X = X.join(group_transforms(X))  # TODO: add to pipeline instead (must be before one-hot encoding)\n# X.info(verbose=True)\n\n# score_dataset(X, y)","20cb05ac":"cluster_features = [\n    \"LotArea\",\n    \"TotalBsmtSF\",\n    \"1stFlrSF\",\n    \"2ndFlrSF\",\n    \"GrLivArea\",\n    \"GarageArea\",\n    \"LotFrontage\"\n]","aecccd86":"def cluster_labels_and_distances(df, features, n_clusters=20):\n    X = df.copy()\n    X_unscaled = X.loc[:, features]\n    X_scaled = (X_unscaled - X_unscaled.mean(axis=0)) \/ X_unscaled.std(axis=0)\n    kmeans = KMeans(n_clusters=n_clusters, n_init=50, random_state=321)\n    \n    # cluster labels\n    X_new = pd.DataFrame()\n    X_new[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    \n    # cluster distances \n    X_cd = kmeans.transform(X_scaled)\n    X_cd = pd.DataFrame(\n        X_cd, columns=[f\"Centroid_{i}\" for i in range(X_cd.shape[1])]\n    )\n    \n    # distortion\n    distortion = kmeans.inertia_\n    \n    return X_new, X_cd, distortion\n\n# X_cl, X_cd, _ = cluster_labels_and_distances(X, cluster_features, n_clusters=20)\n# X = X.join(X_cl)  # TODO: add to pipeline instead \n# X = X.join(X_cd)  # TODO: add to pipeline instead \n\n# X.info(verbose=True)\n\n# score_dataset(X, y)\n\n","538a0d0a":"# elbow plot\n\ndistortions = []\nn_range = range(1,30)\n\n# for n in n_range:\n#     X_cl, X_cd, distortion = cluster_labels_and_distances(X, cluster_features, n_clusters=n)\n#     print(\"Distortion:\", distortion, \n#          \"\\n Value used for n_clusters (number of clusters) for labeling: n=\", n)\n#     distortions.append(distortion)\n    \n# plt.plot(list(n_range), distortions)","64e4ee4b":"# # Optimize number of clusters (best was n=3 (when use both labels and distances))\n# # by comparing cross validation scores\n# # (Will take some time--like HOURS)\n\n# scores = []\n# n_range = range(1,40)\n# for n in list(n_range):\n#     X_orig = df_train.copy().drop(\"SalePrice\", axis=1)\n#     X_cl, X_cd, _ = cluster_labels_and_distances(X, cluster_features, n_clusters=n)\n#     X = X_orig.join(X_cl)\n# #     X = X_orig.join(X_cd)\n#     score = score_dataset(X, y)\n#     scores.append(score)\n#     print(\"Cross-validation score:\", score, \n#         \"\\n Value used for n_clusters (number of clusters) for labeling:n=\", n)\n\n# plt.plot(list(n_range), scores)\n# plt.xlabel(num_clusters)\n# plt.ylabel(cv_score)\n# plt.show()","da1b5ab8":"print(\"Correlation with SalePrice:\\n\")\n\ncorr_with_SalePrice = df_train.drop(\"SalePrice\", axis=1).corrwith(df_train.SalePrice)\npca_features = list(corr_with_SalePrice\n                    .where(lambda x: abs(x)>0.5)\n                    .sort_values(ascending=False)\n                    .dropna()\n                    .index)\n\nprint(corr_with_SalePrice[pca_features])","bc75d966":"def apply_pca(X, standardize=True):\n    # Standardize\n    if standardize:\n        X = (X - X.mean(axis=0)) \/ X.std(axis=0)\n    # Create principal components\n    pca = PCA()\n    X_pca = pca.fit_transform(X)\n    # Convert to dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca, columns=component_names)\n    # Create loadings\n    loadings = pd.DataFrame(\n        pca.components_.T,  # transpose the matrix of loadings\n        columns=component_names,  # so the columns are the principal components\n        index=X.columns,  # and the rows are the original features\n    )\n    return pca, X_pca, loadings\n\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs","a9d789dd":"pca, X_pca, loadings = apply_pca(X[pca_features])\nprint(loadings)","be7207d0":"# Plot explained variance based on components from PCA\nplot_variance(pca)","b3883517":"def pca_inspired(df):\n    X = pd.DataFrame()\n    X[\"GrLivAreaPlusBsmtSF\"] = df.GrLivArea + df.TotalBsmtSF\n    X[\"RecentRemodLargeBsmt\"] = df.YearRemodAdd * df.TotalBsmtSF\n    return X\n\ndef pca_components(df, features):\n    X = df.loc[:, features]\n    _, X_pca, _ = apply_pca(X)\n    return X_pca\n\n# X = X.join(pca_inspired(X))  # TODO: add to pipeline instead \n# X = X.join(pca_components(X, pca_features))  # TODO: add to pipeline instead \n\n# X.info(verbose=True)\n\n# score_dataset(X, y)","20ad5de6":"def corrplot(df, method=\"pearson\", annot=True, figsize=(10,10), **kwargs):\n    sns.clustermap(\n        df.corr(method),\n        vmin=-1.0,\n        vmax=1.0,\n        cmap=\"icefire\",\n        method=\"complete\",\n        annot=annot,\n        figsize=figsize,\n        **kwargs,\n    )\n\ncorrelation_matrix = df_train.corr()\n\ncorrplot(df_train, annot=None)\nplt.show()","91b37ca3":"print(X_pca.melt())","7360bf73":"sns.catplot(\n    y=\"value\",\n    col=\"variable\",\n    data=X_pca.melt(),\n    kind='boxen',\n    sharey=False,\n    col_wrap=3,\n);","4f05d827":"# Can change PC1 to PC10\ncomponent = \"PC1\"\n\nidx = X_pca[component].sort_values(ascending=False).index\n\ndf_train[[\"SalePrice\", \"Neighborhood\", \"SaleCondition\"] + pca_features].iloc[idx]","06bdbf93":"def indicate_outliers(df):\n    X_new = pd.DataFrame()\n    X_new[\"Outlier\"] = (df.Neighborhood == \"Edwards\") & (df.SaleCondition == \"Partial\")\n    return X_new\n\n# print(indicate_outliers(df_train).sum())\n\n# X = X.join(indicate_outliers(df_train))  # TODO: add to pipeline instead \n\n# X.info(verbose=True)\n\n# score_dataset(X, y)","732ecd6f":"def create_features(df, df_test=None):\n    X = df.copy()\n    y = X.pop('SalePrice')\n    mi_scores = make_mi_scores(X, y)\n    # Combine splits if test data is given\n    #\n    # If we're creating features for test set predictions, we should\n    # use all the data we have available. After creating our features,\n    # we'll recreate the splits.\n    if df_test is not None:\n        X_test = df_test.copy()\n        X_test.pop(\"SalePrice\")\n        X = pd.concat([X, X_test])\n\n    # Step 1: Drop features with low Mutual Information scores\n    X, dropped_columns = drop_uninformative(X, mi_scores)\n    \n    # Step 2: Add binary variables\n    X = X.join(add_binary_variables(X))\n\n    # Step 3: Add features from mathematical transforms \n    ######## (`LivLotRatio`, `Spaciousness`)\n    X = X.join(mathematical_transforms(X))\n\n\n    # Step 4: Add new feature from counts \n    ######## (`PorchTypes`, `TotalHalfBath`, `TotalRoom`)\n    X = X.join(counts(X))\n\n    # Step 5: Add new feature from group transform\n    ######## (median `GrLivArea` by `neighborhood`)\n    X = X.join(group_transforms(X))\n\n    # Step 6: Add features from k-means clustering \n    ######## (cluster labels, cluster distance)\n    #X = X.join(cluster_labels(X, cluster_features, n_clusters=20))\n    #X = X.join(cluster_distance(X, cluster_features, n_clusters=20))\n\n    # Step 7: Add features from PCA\n    ######## (loadings-inspired features , PCA components, & outlier indicators)\n    X = X.join(pca_inspired(X))\n    #X = X.join(pca_components(X, pca_features))\n    #X = X.join(indicate_outliers(X))\n  \n    # Label encoding for the categorical features\n    X = label_encode(X)\n\n    # One-hot encoding\n#     X = one_hot_encode(X, nominal_features)\n    \n    # Reform splits\n    if df_test is not None:\n        X_test = X.loc[df_test.index, :]\n        X.drop(df_test.index, inplace=True)\n\n    if df_test is not None:\n        return X, X_test\n    else:\n        return X","43da3b0e":"X_train = create_features(df_train)\ny_train = df_train.loc[:, 'SalePrice']\n\nscore_dataset(X_train, y_train)","ff861b79":"def objective(trial):\n    xgb_params = dict(\n        max_depth=trial.suggest_int(\"max_depth\", 2, 10),\n        learning_rate=trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        n_estimators=trial.suggest_int(\"n_estimators\", 1000, 8000),\n        min_child_weight=trial.suggest_int(\"min_child_weight\", 1, 10),\n        colsample_bytree=trial.suggest_float(\"colsample_bytree\", 0.2, 1.0),\n        subsample=trial.suggest_float(\"subsample\", 0.2, 1.0),\n        reg_alpha=trial.suggest_float(\"reg_alpha\", 1e-4, 1e2, log=True),\n        reg_lambda=trial.suggest_float(\"reg_lambda\", 1e-4, 1e2, log=True),\n    )\n    xgb = XGBRegressor(**xgb_params)\n    return score_dataset(X_train, y_train, xgb)\n\n# study = optuna.create_study(direction=\"minimize\")\n# study.optimize(objective, n_trials=40)\n# xgb_params = study.best_params","9bcd683b":"# Tuned XGB parameters\n\ntuned_xgb_params = dict(\n    max_depth=4,                           \n    learning_rate=0.0053001261458812485,  \n    n_estimators=2694,                    \n    min_child_weight=1,                   \n    colsample_bytree=0.3413079567846533,                  \n    subsample=0.3010031420269477,                          \n    reg_alpha=0.8587198558684747,                          \n    reg_lambda=0.0002627429153954156,                          \n    num_parallel_tree=1,                   # set > 1 for boosted random forests\n)","eb2eae39":"X_train, X_test = create_features(df_train, df_test)\ny_train = df_train.loc[:, \"SalePrice\"]\n\nxgb = XGBRegressor(**tuned_xgb_params)\n\nxgb.fit(X_train, np.log(y_train))\npredictions = np.exp(xgb.predict(X_test))\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice': predictions})\n\n# Save Submission\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your predictions are successfully saved!\")\n\n# Save the XGB model\nfilename = 'ames_house_xgb_model.pkl'\npickle.dump(xgb, open(filename, 'wb'))\n\n# Save processed test data\nX_test.to_csv('df_test_processed.csv', index=False)","c0f405e6":"figsize = (15, 25)\nfig, ax = plt.subplots(1,1,figsize=figsize)\n\nplot_importance(xgb, ax=ax)","96396033":"# Pick an arbitrary row (first row starts at 0)\nrow_to_show = 42\ndata_for_prediction = X_test.iloc[[row_to_show]]\n\n# Generate prediction\ny_sample = np.exp(xgb.predict(data_for_prediction))\n\n# Create object that can calculate Shap values\nexplainer = shap.TreeExplainer(xgb)\n\n# Calculate Shap values from prediction\nshap_values = explainer.shap_values(data_for_prediction)\n\nplt.title('Feature importance based on SHAP values?')\nshap.summary_plot(shap_values, data_for_prediction, plot_type=\"bar\")","7cde2b61":"plt.title('Feature impact on model output (feature impact in details below)')\nshap.summary_plot(shap_values, data_for_prediction)\n\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction)","c53582e0":"# Use test set to get predictions\ndata_for_prediction = X_test\n\n# Generate predictions\ny_sample = np.exp(xgb.predict(data_for_prediction))\n\n# Create object that can calculate Shap values\nexplainer = shap.TreeExplainer(xgb)\n\n# Calculate Shap values from predictions\nshap_values = explainer.shap_values(data_for_prediction)","490c7680":"plt.title('Feature impact on overall model output (feature impact in details below)')\nshap.summary_plot(shap_values, data_for_prediction)\n\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction)","a41b115a":"### Histogram and box plot","a766654e":"### K-Means Clustering\n\nWe can use the results of clustering as features also. Let's cluster the data with k-means clustering using a few continuous variables:","f66e86be":"0.12863654701338248","d23626bb":"# 2. Data preprocessing\n\nFor the purpose of preprocessing, the df_test and df_train will be concatenated together temporarily.","7b921022":"Below is the code used to apply PCA and plot the variances of the components","6197e70e":"For further analysis, one could examine the clustermap of the correlation matrix for the inspiration of more features. But this was not used in the making of the model.","0dd5cbf0":"# 3. EDA - Analyzing the dependent variable (SalePrice)\n\nThis part was not directly used in the building of the model, but just to get a feel for the data.\n\n### Some statistics from .describe()","4108a844":"# 6. Hyperparameter Tuning\n\nLet's use Optuna to tune the hyperparameters of the XGBoost model. Optuna by default uses a Tree-based Parzen Estimator (TPE) by default, which is a type of Bayesian Optimization. ","80aeedac":"### Encoding\n\nSome claim that one-hot encoding is not necessary for tree-based models. But I will include a function that applies one-hot encoding on nominal features for experimentation. \n\n(from initial test, it seems that adding one-hot encoding made performance worse)","f1e37cfb":"### PCA\n\nThe principle components of PCA can be used to bring insights and inspire new features, or the components themselves can be used as new features.\n\nLet's choose some features that are the most highly correlated to the target feature, `SalePrice` to perform the PCA.","782cf23d":"# 8. Model Interpretation\n\n## Feature importances\n\nOne way we can get some insight into the model is with `xgboost.plot_importance()`. ","037ae7a0":"# Introduction\n\nThis notebook covers the process of building an XGBoost model to predict house prices from the Ames Housing Dataset. It covers data preprocessing, feature engineering, hyperparameter tuning, training the model, and model interpretation.\n\n\n## Credits\/Disclaimer\n\nThe work in this notebook was mainly for the purpose of learning, that is why large portions of code and text are nearly identical to the code in [this notebook](https:\/\/www.kaggle.com\/ruthgn\/house-prices-top-8-featengineering-xgb-optuna), which is based off [this one](https:\/\/www.kaggle.com\/ryanholbrook\/feature-engineering-for-house-prices).\n \nI was also influenced by [this notebook for EDA](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python).\n\nPlease check out the mentioned notebooks to get more detailed explanations of what's going on.\n","5f0c4b07":"#### Mathematical Transforms (ratios)\n\nNow to create new features that are ratios between features. This is to capture any potential interactions between features.\n\nLivLotRatio: $\\frac{GrLivArea}{LotArea} $\n\nSpaciousness: $\\frac{\\left ( 1stFlrSF + 2ndFlrSF \\right )}{TotRmsAbvGrd} $\n\n\n(initial test worsens performance)","1dc3fa31":"## Get scatter plots for all columns\n\nTODO:\n* plot just 'object' columns first and remove the columns that aren't useful\n* find some kind of correlation statistic for the categorical variables (chi-square, ANOVA, etc)\n* use sns.heatmap(df_train.corr()) for the numerical variables\n    * remove variables with low correlation\n    * deal with multicolinearity","a91be3c4":"# 5. Feature Engineering","d061435b":"### Grouped Transform\n\nHouse prices might be affected by the size of the house relative to the houses in the same neighbourhood. So we will create a feature MedNhbdArea -- the median of GrLivArea grouped on Neighborhood.\n\n(initial test - slightly improves performance)","65cbe5a6":"#### How does each feature affect the model's predictions in a big-picture sense? In other words, what is its typical effect when considered over a large number of possible predictions?\n\nThe plots below is the same as the plot for a single prediction, but it includes every sample.","7cfd6a90":"#### Optimizing n_clusters with the score_dataset function\n\nInstead of using an elbow plot, we can get the cross validation scores for various values of n. (This can take a while). \n\nThe plot below is from using both labels and distances as features. Performance might be dropping due to the larger number of features added when num_clusters is larger.\n\n![image.png](attachment:00f6c3c4-3d2e-4d31-9670-e5fae4bfd371.png)\n\nBut there doesn't seem to be a clear pattern when using just the cluster labels (plot below).\n\n![image.png](attachment:d68d2a7b-87ad-497b-ac02-cc4446f88051.png)\n\nFrom the above 2 graphs, it seems like it may be better not to include cluster labels and distances as features. (may experiment with this again after hyperparameter tuning)","587e51cc":"Uncomment as desired to view information on the train and test DataFrames.","401f7b10":"## Creating new features\n\nSome features have a lot of zeroes or NAs due to there being no such feature (eg. no Basement\/Garage). So it may benefit to add new binary variables to capture whether those features exists (eg hasBsmt). (Although, it might not be necessary for tree-based models)\n\n(from initial test, the performance remained unchanged before and after adding these binary variables)","04c20c48":"And we'll make new features using the cluster labels as well as the cluster distances (distance of each observation to the cluster) \n\n(initial test for cluster_labels - slightly improves performance)\n\n(initial test for cluster_distances - slightly worsens performance)","2e602f46":"### Skewness and kurtosis","4dcf15a0":"### Encoding\nThe dtype of some columns should be changed from object (and in one case int64) to categor, with some being nominal and some ordinal.","ef03bc6b":"* Definitely not normally distributed. \n* Positively skewed\n","03991de2":"* Mean and median different - not normally distributed\n\n* Min SalePrice is not zero or less. That's a good sign\n* outliers on the right","37a4ea53":"Investigation would show that a few houses with `Neighbourhood = Edwards` and `SaleCondition = Partial` appear as outliers for multiple components. This is possibly due to the fact that a \"partial\" sale.\n\nA partial sale is what occurs when there are multiple owners of a property and one or more of them sell their \"partial\" ownership of the property. These kinds of sales are often happen during the settlement of a family estate or the dissolution of a business and aren't advertised publicly.\n\nSo these samples are true outliers. But some models can benefit from the labeling of outliers, so below is some code to create a feature that indicates if a sample is an outlier.\n\n(initial test - no change in performance)","34a71de3":"Now that we have all the desired features, and we have the tuned hyperparameters, we can fit the model and get our predictions.","1a029839":"#### Elbow plot\n\nThe code below was me attempting to draw an elbow plot to decide on a value for n_clusters (number of clusters) for the k-means clustering algorithm. However, there is no clear elbow, so this method is not very helpful. ","07b9fa68":"#### PCA - outlier detection\n\nPCA can also be used for outlier detection. This can be done by plotting the distributions of the components.","2681ca32":"# 1. Preliminaries","680bde06":"# 4. Establish baseline\n\nBefore doing feature engineering, we establish a baseline score to judge changes to the feature set.\n\nSince evaluation of this competition is on Root-Mean-Squared-Error (RMSE) of the log of the target feature (because \"Taking logs means that errors in predicting expensive houses and cheap houses will affect the result equally\"). We need to log-transform the target feature in the training and test sets. \n\nFurthermore, according to [House Prices (Top 8%) - FeatEngineering,XGB+Optuna](https:\/\/www.kaggle.com\/ruthgn\/house-prices-top-8-featengineering-xgb-optuna\/notebook#Part-2---Feature-Engineering), this also makes sense mathematically because the target feature has a multiplicative relationship with some of the features. ","1f371194":"Let's check if removing the features with zero mutual information with the target feature improves the model's score.","1496a50a":"#### How did each feature in the data affect that particular prediction?\n\nThe plots below show us whether a feature contributed to a prediction positively or negatively and by how much.","6a11498b":"### Counts\n\nNew features that involve the counts of other features\n\nPorchTypes: number of different types of outdoor spaces (count if the following features are non-zero: WoodDeckSF, OpenPorchSF, EnclosedPorch, 3SsnPorch, ScreenPorch)\n\nTotalRooms: total number of rooms above ground (including bathrooms)\n\n(initial test worsens performance)","329d4f65":"## Create Final Feature Set\n\nPutting all the feature engineering steps in separate functions makes it easier for experimentation. We can comment and uncomment the steps we want to see which ones work best.","8ce40cfb":"### Cleaning\n","09134ad5":"It seems that there is a slight improvement, so we will be using this function in the pipeline.","d9f3bd0a":"Let's have a look at the loadings of the components. And let's also plot the variance of the components","f1641f54":"# 7. Train Model and Create Submissions","18e12ad0":"Below is the final output from Optuna:\n\n`[I 2021-12-07 09:13:46,321] Trial 39 finished with value: 0.12013877421064362 and parameters: {'max_depth': 4, 'learning_rate': 0.0053001261458812485, 'n_estimators': 2694, 'min_child_weight': 1, 'colsample_bytree': 0.3413079567846533, 'subsample': 0.3010031420269477, 'reg_alpha': 0.8587198558684747, 'reg_lambda': 0.0002627429153954156}. Best is trial 37 with value: 0.11646304724778636.`","58600c7b":"### Put it all together","695f9223":"Two things to clean:\n1. Change \"None\" values in the MasVnrType column to \"NA\" to match other columns.\n2. There's a row where the value of GarageYrBlt is 2207 (see plot below). Since the data set was compiled from 2006 to 2010 ([link to the paper](http:\/\/www.amstat.org\/publications\/jse\/v19n3\/decock.pdf)), all values greater than 2010 were set to the value of YearBuilt.","8150b584":"### Missing values\nMost missing values to be replaced with 0 for numerical variables, and NA for categorical variables. A few exceptions would be:\n1. Electrical, which has one missing value that will be replaced with the mode,\n2. KitchenQual, which has one missing value that will be replaced with TA (Typical\/Average),\n3. Functional, which has three missing values that will be replaced with Typ (Typical Functionality),\n4. SaleType, which has one missing value that will be replaced with Oth (Other), and\n4. GarageYrBlt, which has missing values due to there not being a garage. These will be replaced with YearBuilt.","3bcb9ea0":"### Model-level SHAP values\n\nThis time we use all the rows for `data_for_prediction` instead of a single row","0afbf1ba":"## Feature Selection - Mutual Information Scores\nMutual information is like a measure of dependency between variables, but is not limited to linear correlation and also works with categorical variables. We'll calculate the mutual information scores between all the features and the target feature to see which features give us the most information about the target features, and drop the features that have a mutual information score of zero.","5b267c7a":"As mentioned previously, PCA can inspire new features, and the components themselves can be used as features. The functions below will do just that.\n\n(initial test of pca_inspired - improved performance)\n\n(initial test of pca_components - worsens performance)","c7be7217":"## SHAP values\n\nAnd another way to interpret the model is with SHAP values. This can help answer the following questions\n\n* What features in the data did the model think are most important?\n* For any single prediction from a model, how did each feature in the data affect that particular prediction?\n* How does each feature affect the model's predictions in a big-picture sense (what is its typical effect when considered over a large number of possible predictions)?\n\n### Single Prediction\n\n#### For a single prediction, what features in the data did the model think are most important?\n\nThe plot below shows the absolute values. So it does not tell us whether a feature contributes positively or negatively to the prediction."}}