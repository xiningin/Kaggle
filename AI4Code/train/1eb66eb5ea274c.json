{"cell_type":{"64586ed4":"code","0754ea0c":"code","835b5cd1":"code","4eb4ff24":"code","a98d2bdc":"code","a46f6e49":"code","43768680":"code","368f7da0":"code","66b388bc":"code","54931524":"code","c397b8b4":"code","76ad2fa7":"code","f69b9171":"code","2bc5f6e9":"code","4f97ca74":"code","3b4369fb":"code","49e0753a":"code","26770806":"code","3a21cf75":"code","fde6c568":"code","06c02893":"code","9812e6b8":"code","4b8fd7ac":"code","d5050668":"code","fb22996a":"code","8dc136e9":"code","c54c12cc":"code","64ef1089":"code","b0b6a8d9":"markdown","5cc9b090":"markdown","4cbd75f9":"markdown","8e2a7052":"markdown"},"source":{"64586ed4":"import re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom nltk.stem import WordNetLemmatizer\n\n# Tensorflow libraries\n# Tensorflow libraries\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing import text, sequence\n# from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import regularizers\n\nimport tensorflow_hub as hub\n\n\n# sklearn libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\nfrom sklearn.metrics import roc_auc_score\n\nfrom gensim.models import Word2Vec # Word2Vec module\nfrom gensim.parsing.preprocessing import preprocess_string, strip_tags, strip_punctuation, remove_stopwords, strip_numeric, stem_text\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0754ea0c":"train_data = pd.read_csv(\"\/kaggle\/input\/fake-news-content-detection\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/fake-news-content-detection\/test.csv\")\nsubmission_data = pd.read_csv(\"\/kaggle\/input\/fake-news-content-detection\/sample submission.csv\")","835b5cd1":"# Sample data from training data\ntrain_data.sample(3)\n","4eb4ff24":"# Dataset information\ntrain_data.info()","a98d2bdc":"train_data[train_data.duplicated(['Text'])]","a46f6e49":"train_data = train_data.drop_duplicates(['Text'])","43768680":"train_data.sample(3)","368f7da0":"train_data['NewsText'] = train_data['Text_Tag'].astype(str) +\" \"+ train_data['Text']\ntest_data['NewsText'] = test_data['Text_Tag'].astype(str) +\" \"+ test_data['Text']","66b388bc":"# Stemmer object\nwnl = WordNetLemmatizer()\n\nclass DataPreprocess:\n    \n    def __init__(self):\n        self.filters = [strip_tags,\n                       strip_numeric,\n                       strip_punctuation,\n                       lambda x: x.lower(),\n                       lambda x: re.sub(r'\\s+\\w{1}\\s+', '', x),\n                       remove_stopwords]\n    def __call__(self, doc):\n        clean_words = self.__apply_filter(doc)\n        return clean_words\n    \n    def __apply_filter(self, doc):\n        try:\n            cleanse_words = set(preprocess_string(doc, self.filters))\n            filtered_words = set(wnl.lemmatize(w, 'v') for w in cleanse_words)\n            return ' '.join(cleanse_words)\n        except TypeError as te:\n            raise(TypeError(\"Not a valid data {}\".format(te)))","54931524":"train_data['Processed'] = train_data['NewsText'].apply(DataPreprocess())\ntest_data['Processed'] = test_data['NewsText'].apply(DataPreprocess())\n\n# train_data['Processed'] = train_data['Text'].apply(DataPreprocess())\n# test_data['Processed'] = test_data['Text'].apply(DataPreprocess())","c397b8b4":"test_data['Processed']","76ad2fa7":"X = train_data['Processed']\ny = train_data['Labels']\n\ny_category = keras.utils.to_categorical(y, 6)\n\n# Split data into Train and Holdout as 80:20 ratio\nX_train, X_valid, y_train, y_valid = train_test_split(X, y_category, shuffle=True, test_size=0.33, random_state=111)\n\nprint(\"Train shape : {}, Holdout shape: {}\".format(X_train.shape, X_valid.shape))","f69b9171":"def word_embedding(train, test, max_features, max_len=200):\n    try:\n        # Keras Tokenizer class object\n        tokenizer = text.Tokenizer(num_words=max_features)\n        tokenizer.fit_on_texts(train)\n        \n        train_data = tokenizer.texts_to_sequences(train)\n        test_data = tokenizer.texts_to_sequences(test)\n        \n        # Get the max_len\n        vocab_size = len(tokenizer.word_index) + 1\n        \n        # Padd the sequence based on the max-length\n        x_train = sequence.pad_sequences(train_data, maxlen=max_len, padding='post')\n        x_test = sequence.pad_sequences(test_data, maxlen=max_len, padding='post')\n        # Return train, test and vocab size\n        return tokenizer, x_train, x_test, vocab_size\n    except ValueError as ve:\n        raise(ValueError(\"Error in word embedding {}\".format(ve)))\n","2bc5f6e9":"max_features = 5000\nmax_len = 128\noutput_dim = len(np.unique(y))\n\n# Test data\nX_test = test_data['Processed']\n\ntokenizer, x_pad_train, x_pad_valid, vocab_size = word_embedding(X_train, X_valid, max_features)","4f97ca74":"# Test data\nX_test = test_data['Processed']\n\ntokenizer.fit_on_sequences(X_test)\n\nX_test_seq = tokenizer.texts_to_sequences(X_test)\nx_pad_test = sequence.pad_sequences(X_test_seq, maxlen=max_len, padding='post')","3b4369fb":"def compute_classweights(target):\n    \"\"\"\n    Computes the weights of the target values based on the samples\n    :param target: Y-target variable\n    :return: dictionary object\n    \"\"\"\n    # compute class weights\n    class_weights = class_weight.compute_class_weight('balanced',\n                                                     np.unique(target),\n                                                     target)\n    \n    # make the class weight list into dictionary\n    weights = {}\n    \n    # enumerate the list\n    for index, weight in enumerate(class_weights):\n        weights[index] = weight\n        \n    return weights\n\n# Get the class weights for the target variable\nweights = compute_classweights(y)","49e0753a":"weights","26770806":"X_train.sample(3)","3a21cf75":"def build_rnn(vocab_size, output_dim, max_len):\n    # Building RNN model\n    model = Sequential([\n        keras.layers.Embedding(vocab_size,128,\n                              input_length=max_len),\n        keras.layers.BatchNormalization(),\n#         keras.layers.Bidirectional(keras.layers.LSTM(128,return_sequences=True)),\n        keras.layers.Dense(128, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.002)),\n        keras.layers.GlobalMaxPool1D(), # Remove flatten layer\n        keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.002)),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.002)),\n        keras.layers.Dropout(0.3),\n        keras.layers.Dense(output_dim, activation='softmax')\n    ])\n\n    return model","fde6c568":"rnn_model = build_rnn(vocab_size, output_dim, max_len)\n\n# Summary of the model\nrnn_model.summary()","06c02893":"# Compile the model\nrnn_model.compile(optimizer=tf.keras.optimizers.Adam(1e-3), \n                  loss=keras.losses.CategoricalCrossentropy(from_logits=True), \n                  metrics=[tf.metrics.AUC()])","9812e6b8":"history = rnn_model.fit(x_pad_train, \n                        y_train,\n                        batch_size=512,\n                        epochs=20,\n                        verbose=1,\n                        validation_data=(x_pad_valid, y_valid),\n                       class_weight=weights)","4b8fd7ac":"results = rnn_model.evaluate(x_pad_valid, y_valid)","d5050668":"y_preds = rnn_model.predict_proba(x_pad_test, batch_size=256)","fb22996a":"y_preds[:,0]","8dc136e9":"final_df = pd.DataFrame({'0': y_preds[:,0],\n                        '1': y_preds[:,1],\n                        '2': y_preds[:,2],\n                        '3': y_preds[:,3],\n                        '4': y_preds[:,4],\n                        '5': y_preds[:,5]}, index=test_data.index)","c54c12cc":"final_df","64ef1089":"final_df.to_csv(\"fake_news_ann_08.csv\", index=False)","b0b6a8d9":"# Fake News Detection Model - NLP Transfer Learning\n\n\n\n|Version| Changes| Score|\n|--------|-------|------|\n|Version 1| Pretrained model NNML-128DIM| 2.26|\n|Version 2| Embedding using Pad Sequence| 1.8022|\n\n","5cc9b090":"## Compute class weights","4cbd75f9":"## Load Dataset","8e2a7052":"## Split data into Train and Holdout"}}