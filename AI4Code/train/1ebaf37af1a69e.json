{"cell_type":{"3e0a42cc":"code","2e1d1705":"code","5ad6deeb":"code","92cd2e8f":"code","608c8707":"code","14944908":"code","cbd568ce":"code","772d1b69":"code","59fdbab0":"code","5e7e07f6":"code","9467c63a":"code","1c6a26be":"code","0918ce84":"code","75674c87":"code","412b7e3c":"code","4596451c":"code","8d2a9d6a":"code","eb8d4f9f":"code","58476a5c":"code","6c5ff61c":"code","d2934fee":"code","3de0f1eb":"code","cb90437d":"code","348980e7":"code","de5b0ed7":"code","c7254b93":"code","b0c3d279":"code","ea1b1b38":"code","661aeec9":"code","1746e491":"code","e7f205d2":"code","f5a815e6":"markdown","4159a91a":"markdown","0b480d24":"markdown","5e431e00":"markdown","d5e78b0d":"markdown","5bdddc29":"markdown","55ebe484":"markdown","519d4ad8":"markdown","84faa7b3":"markdown","eba207aa":"markdown","4cff911f":"markdown","457ee660":"markdown","79e72093":"markdown","9e68d191":"markdown","f725ade0":"markdown","a322fec1":"markdown","17d16f70":"markdown","a113a492":"markdown","c0ed1da6":"markdown","dbdd6a82":"markdown","be9434ae":"markdown","0644d50f":"markdown","e9afd049":"markdown","a26dc0af":"markdown"},"source":{"3e0a42cc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.core.display import display, HTML\nimport plotly.graph_objects as go\nimport warnings\nimport datetime\nimport math\nfrom scipy.optimize import minimize\n","2e1d1705":"# Configure Jupyter Notebook\npd.set_option('display.max_columns', None) \npd.set_option('display.max_rows', 500) \npd.set_option('display.expand_frame_repr', False)\n# pd.set_option('max_colwidth', -1)\ndisplay(HTML(\"<style>div.output_scroll { height: 35em; }<\/style>\"))\n\n%matplotlib inline\n%config InlineBackend.figure_format ='retina'\n\nwarnings.filterwarnings('ignore')","5ad6deeb":"# the number of days into the future for the forecast\ndays_forecast = 40","92cd2e8f":"# download the latest data sets\nconf_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_19-covid-Confirmed.csv')\ndeaths_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_19-covid-Deaths.csv')\nrecv_df = pd.read_csv('https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_19-covid-Recovered.csv')","608c8707":"# create full table\ndates = conf_df.columns[4:]\n\nconf_df_long = conf_df.melt(id_vars=['Province\/State', 'Country\/Region', 'Lat', 'Long'], \n                            value_vars=dates, var_name='Date', value_name='Confirmed')\n\ndeaths_df_long = deaths_df.melt(id_vars=['Province\/State', 'Country\/Region', 'Lat', 'Long'], \n                            value_vars=dates, var_name='Date', value_name='Deaths')\n\nrecv_df_long = recv_df.melt(id_vars=['Province\/State', 'Country\/Region', 'Lat', 'Long'], \n                            value_vars=dates, var_name='Date', value_name='Recovered')\n\nfull_table = pd.concat([conf_df_long, deaths_df_long['Deaths'], recv_df_long['Recovered']], \n                       axis=1, sort=False)\n","14944908":"# avoid double counting\nfull_table = full_table[full_table['Province\/State'].str.contains(',')!=True]","cbd568ce":"# cases \ncases = ['Confirmed', 'Deaths', 'Recovered', 'Active']\n\n# Active Case = confirmed - deaths - recovered\nfull_table['Active'] = full_table['Confirmed'] - full_table['Deaths'] - full_table['Recovered']\n\n# replacing Mainland china with just China\nfull_table['Country\/Region'] = full_table['Country\/Region'].replace('Mainland China', 'China')\n\n# filling missing values \nfull_table[['Province\/State']] = full_table[['Province\/State']].fillna('')","772d1b69":"# Display the number cases globally\ndf = full_table.groupby(['Country\/Region', 'Province\/State'])['Confirmed', 'Deaths', 'Recovered', 'Active'].max()\ndf = full_table.groupby('Date')['Confirmed', 'Deaths', 'Recovered', 'Active'].sum().reset_index()\ndf =  df[df['Date']==max(df['Date'])].reset_index(drop=True)\ndf","59fdbab0":"# start a dataframe with the unique countries and provinces. \n# This will be used to keep track of the model performance for each one\ndf_model_select = full_table.groupby(['Country\/Region','Province\/State']).last().reset_index()\ndf_model_select = df_model_select[['Country\/Region','Province\/State','Confirmed']] \ndf_model_select['Offset error'] = 100\ndf_model_select['No Offset error'] = 100","5e7e07f6":"countries = list(set(full_table['Country\/Region']))\ncountries.sort()\n\nfor country in countries:\n    clusters = list(set(full_table['Province\/State'][(full_table['Country\/Region'] == country)]))\n    clusters.sort()\n    \n    for cluster in clusters:\n        print(' ')\n        print('-----------------')\n        print(country + ' - ' + cluster)\n        \n        df = full_table[(full_table['Country\/Region'] == country)&(full_table['Province\/State'] == cluster)]\n        df = df.groupby(['Date','Country\/Region']).sum().reset_index()\n        df['Date'] = pd.to_datetime(df['Date'])\n        df = df.sort_values(by=['Date'])\n        df = df.set_index('Date')[['Confirmed']]\n        df.drop(df.tail(3).index,inplace=True)\n        df_result = df.copy()\n        # df_result = df_result[['Date','Confirmed']]\n\n        # ensure that the model starts from when the first case is detected\n        # NOTE: its better not to truncate the dataset like this \n        # df = df[df[df.columns[0]]>0]\n\n        # define the models to forecast the growth of cases\n        def model(N, a, alpha, t0, t):\n            return N * (1 - math.e ** (-a * (t-t0))) ** alpha\n\n        def model_loss(params):\n            N, a, alpha, t0 = params\n            global df\n            r = 0\n            for t in range(len(df)):\n                r += (model(N, a, alpha, t0, t) - df.iloc[t, 0]) ** 2\n            return r \n        try:\n            N = df['Confirmed'][-1]\n            T = -df['Confirmed'][0]\n        except:\n            N = 10000\n            T = 0\n\n        opt = minimize(model_loss, x0=np.array([N, 0.1, 5, T]), method='Nelder-Mead', tol=1e-7).x\n        print('Offset' + str(opt))\n\n        # create series to be plotted \n        x_actual = pd.to_datetime(df.reset_index().iloc[:,0])\n        x_actual =list(x_actual)\n        y_actual = list(df.reset_index().iloc[:,1])\n\n        start_date = pd.to_datetime(df.index[0])\n\n        x_model = []\n        y_model = []\n\n        # get the model values for the same time series as the actuals\n        for t in range(len(df) + days_forecast):\n            x_model.append(start_date + datetime.timedelta(days=t))\n            y_model.append(round(model(*opt,t)))\n\n        # now add the results of the model to the dataframe\n        df2 = pd.DataFrame(y_model,index=x_model,columns=['Offset'])\n        df2.index.name = 'Date'\n        df_result = pd.merge(df_result,\n                             df2,\n                             how='outer',\n                             left_on=['Date'],\n                             right_on=['Date'])\n\n        # define the models to forecast the growth of cases\n        def model(N, a, alpha, t):\n            return N * (1 - math.e ** (-a * (t))) ** alpha\n\n        def model_loss(params):\n            N, a, alpha = params\n            global df\n            r = 0\n            # error minimization should prefer (weigh more on) the larger population in this case, therefore no normalization.\n            for t in range(len(df)):\n                r += (model(N, a, alpha, t) - df.iloc[t, 0]) ** 2\n            return r \n\n        try:\n            N = df['Confirmed'][-1]\n        except:\n            N = 10000\n\n        opt = minimize(model_loss, x0=np.array([N, 0.1, 5]), method='Nelder-Mead', tol=1e-7).x\n        print('No offset' + str(opt))\n\n        try:\n            start_date = pd.to_datetime(df.index[0])\n\n            x_model = []\n            y_model = []\n\n            # get the model values for the same time series as the actuals\n            for t in range(len(df) + days_forecast):\n                x_model.append(start_date + datetime.timedelta(days=t))\n                y_model.append(round(model(*opt,t)))\n\n            # now add the results of the model to the dataframe\n            df2 = pd.DataFrame(y_model,index=x_model,columns=['No Offset'])\n            df2.index.name = 'Date'\n            df_result = pd.merge(df_result,\n                                 df2,\n                                 how='outer',\n                                 left_on=['Date'],\n                                 right_on=['Date'])\n            \n            df_result = df_result[df_result['Confirmed'].notnull()]\n            err_offset = 0\n            err_no_offset = 0\n            for t in range(len(df_result)):\n                err_offset += (math.log(df_result['Offset'].iloc[t]+1)-math.log(df_result['Confirmed'].iloc[t]+1))**2\n                err_no_offset += (math.log(df_result['No Offset'].iloc[t]+1)-math.log(df_result['Confirmed'].iloc[t]+1))**2\n\n            err_offset = math.sqrt(err_offset\/len(df_result))\n            err_no_offset = math.sqrt(err_no_offset\/len(df_result))\n            \n            df_model_select['Offset error'][(df_model_select['Country\/Region']==country)&(df_model_select['Province\/State']==cluster)] = err_offset\n            df_model_select['No Offset error'][(df_model_select['Country\/Region']==country)&(df_model_select['Province\/State']==cluster)] = err_no_offset\n        except:\n            pass","9467c63a":"def highlight_max(s):\n    '''\n    highlight the absolute maximum value in a Series with red font.\n    '''\n    is_min = abs(s) == abs(s).max()\n    return ['color: red' if v else '' for v in is_min]\n\ndf_model_select.style.apply(highlight_max,axis=1,subset=['Offset error', 'No Offset error'])","1c6a26be":"# plot the errors\nx_model = list(df_model_select['Country\/Region']+' - '+ df_model_select['Province\/State'])\ny_model_off = list(df_model_select['Offset error'])\ny_model_noff = list(df_model_select['No Offset error'])\n\n# instantiate the figure and add the two series - actual vs modelled    \nfig = go.Figure()\n\nfig.update_layout(title='Error comparison',\n                  xaxis_title='Date',\n                  yaxis_title=\"error\",\n                  autosize=False,\n                  width=750,\n                  height=800,\n                  #yaxis_type='log'\n                 )\n\nfig.add_trace(go.Line(x=x_model,\n                      y=y_model_off,\n                      mode='lines',\n                      name='Offset error',\n                      line=dict(color='blue', \n                                width=1.5\n                               )\n                     ) \n             )\n\nfig.add_trace(go.Line(x=x_model,\n                      y=y_model_noff,\n                      mode='lines',\n                      name='No Offset error',\n                      line=dict(color='red', \n                                width=1.0,\n                                dash='dot'\n                               )\n                     ) \n             )\n\nfig.show()","0918ce84":"# start a dataframe with the unique countries and provinces. \n# This will be used to keep track of the model performance for each one\ndf_model_select = full_table.groupby(['Country\/Region','Province\/State']).last().reset_index()\ndf_model_select = df_model_select[['Country\/Region','Province\/State','Deaths']] \ndf_model_select['Offset error'] = 100\ndf_model_select['No Offset error'] = 100","75674c87":"countries = list(set(full_table['Country\/Region']))\ncountries.sort()\n\nfor country in countries:\n    clusters = list(set(full_table['Province\/State'][(full_table['Country\/Region'] == country)]))\n    clusters.sort()\n    \n    for cluster in clusters:\n        print(' ')\n        print('-----------------')\n        print(country + ' - ' + cluster)\n        \n        df = full_table[(full_table['Country\/Region'] == country)&(full_table['Province\/State'] == cluster)]\n        df = df.groupby(['Date','Country\/Region']).sum().reset_index()\n        df['Date'] = pd.to_datetime(df['Date'])\n        df = df.sort_values(by=['Date'])\n        df = df.set_index('Date')[['Deaths']]\n        df.drop(df.tail(3).index,inplace=True)\n        df_result = df.copy()\n        # df_result = df_result[['Date','Deaths']]\n\n        # ensure that the model starts from when the first case is detected\n        # NOTE: its better not to truncate the dataset like this \n        # df = df[df[df.columns[0]]>0]\n\n        # define the models to forecast the growth of cases\n        def model(N, a, alpha, t0, t):\n            return N * (1 - math.e ** (-a * (t-t0))) ** alpha\n\n        def model_loss(params):\n            N, a, alpha, t0 = params\n            global df\n            r = 0\n            for t in range(len(df)):\n                r += (model(N, a, alpha, t0, t) - df.iloc[t, 0]) ** 2\n            return r \n        try:\n            N = df['Deaths'][-1]\n            T = -df['Deaths'][0]\n        except:\n            N = 10000\n            T = 0\n\n        opt = minimize(model_loss, x0=np.array([N, 0.1, 5, T]), method='Nelder-Mead', tol=1e-7).x\n        print('Offset' + str(opt))\n\n        # create series to be plotted \n        x_actual = pd.to_datetime(df.reset_index().iloc[:,0])\n        x_actual =list(x_actual)\n        y_actual = list(df.reset_index().iloc[:,1])\n\n        start_date = pd.to_datetime(df.index[0])\n\n        x_model = []\n        y_model = []\n\n        # get the model values for the same time series as the actuals\n        for t in range(len(df) + days_forecast):\n            x_model.append(start_date + datetime.timedelta(days=t))\n            y_model.append(round(model(*opt,t)))\n\n        # now add the results of the model to the dataframe\n        df2 = pd.DataFrame(y_model,index=x_model,columns=['Offset'])\n        df2.index.name = 'Date'\n        df_result = pd.merge(df_result,\n                             df2,\n                             how='outer',\n                             left_on=['Date'],\n                             right_on=['Date'])\n\n        # define the models to forecast the growth of cases\n        def model(N, a, alpha, t):\n            return N * (1 - math.e ** (-a * (t))) ** alpha\n\n        def model_loss(params):\n            N, a, alpha = params\n            global df\n            r = 0\n            # error minimization should prefer (weigh more on) the larger population in this case, therefore no normalization.\n            for t in range(len(df)):\n                r += (model(N, a, alpha, t) - df.iloc[t, 0]) ** 2\n            return r \n\n        try:\n            N = df['Deaths'][-1]\n        except:\n            N = 10000\n\n        opt = minimize(model_loss, x0=np.array([N, 0.1, 5]), method='Nelder-Mead', tol=1e-7).x\n        print('No offset' + str(opt))\n\n        try:\n            start_date = pd.to_datetime(df.index[0])\n\n            x_model = []\n            y_model = []\n\n            # get the model values for the same time series as the actuals\n            for t in range(len(df) + days_forecast):\n                x_model.append(start_date + datetime.timedelta(days=t))\n                y_model.append(round(model(*opt,t)))\n\n            # now add the results of the model to the dataframe\n            df2 = pd.DataFrame(y_model,index=x_model,columns=['No Offset'])\n            df2.index.name = 'Date'\n            df_result = pd.merge(df_result,\n                                 df2,\n                                 how='outer',\n                                 left_on=['Date'],\n                                 right_on=['Date'])\n            \n            df_result = df_result[df_result['Deaths'].notnull()]\n            err_offset = 0\n            err_no_offset = 0\n            for t in range(len(df_result)):\n                err_offset += (math.log(df_result['Offset'].iloc[t]+1)-math.log(df_result['Deaths'].iloc[t]+1))**2\n                err_no_offset += (math.log(df_result['No Offset'].iloc[t]+1)-math.log(df_result['Deaths'].iloc[t]+1))**2\n\n            err_offset = math.sqrt(err_offset\/len(df_result))\n            err_no_offset = math.sqrt(err_no_offset\/len(df_result))\n            \n            df_model_select['Offset error'][(df_model_select['Country\/Region']==country)&(df_model_select['Province\/State']==cluster)] = err_offset\n            df_model_select['No Offset error'][(df_model_select['Country\/Region']==country)&(df_model_select['Province\/State']==cluster)] = err_no_offset\n        except:\n            pass","412b7e3c":"def highlight_max(s):\n    '''\n    highlight the absolute maximum value in a Series with red font.\n    '''\n    is_min = abs(s) == abs(s).max()\n    return ['color: red' if v else '' for v in is_min]\n\ndf_model_select.style.apply(highlight_max,axis=1,subset=['Offset error', 'No Offset error'])","4596451c":"# plot the errors\nx_model = list(df_model_select['Country\/Region']+' - '+ df_model_select['Province\/State'])\ny_model_off = list(df_model_select['Offset error'])\ny_model_noff = list(df_model_select['No Offset error'])\n\n# instantiate the figure and add the two series - actual vs modelled    \nfig = go.Figure()\n\nfig.update_layout(title='Error comparison',\n                  xaxis_title='Date',\n                  yaxis_title=\"% error\",\n                  autosize=False,\n                  width=750,\n                  height=800,\n                  #yaxis_type='log'\n                 )\n\nfig.add_trace(go.Line(x=x_model,\n                      y=y_model_off,\n                      mode='lines',\n                      name='Offset error',\n                      line=dict(color='blue', \n                                width=1.5\n                               )\n                     ) \n             )\n\nfig.add_trace(go.Line(x=x_model,\n                      y=y_model_noff,\n                      mode='lines',\n                      name='No Offset error',\n                      line=dict(color='red', \n                                width=1.0,\n                                dash='dot'\n                               )\n                     ) \n             )\n\nfig.show()","8d2a9d6a":"df_ca_train = pd.read_csv('..\/input\/covid19-global-forecasting-week-1\/train.csv')\ndf_ca_test = pd.read_csv('..\/input\/covid19-global-forecasting-week-1\/test.csv')\ndf_ca_submission = pd.read_csv('..\/input\/covid19-global-forecasting-week-1\/submission.csv')","eb8d4f9f":"df_ca_train.tail(10)","58476a5c":"full_table = df_ca_train\nfull_table[['Province\/State']] = full_table[['Province\/State']].fillna('')\ndf_comp = df_ca_test\ndf_comp[['Province\/State']] = df_comp[['Province\/State']].fillna('')\ndf_comp['Date'] = pd.to_datetime(df_comp['Date'])\ndf_comp['ConfirmedCases']=0\ndf_comp['Fatalities']=0","6c5ff61c":"countries = list(set(full_table['Country\/Region']))\ncountries.sort()\n\nfor country in countries:\n    clusters = list(set(full_table['Province\/State'][(full_table['Country\/Region'] == country)]))\n    clusters.sort()\n    \n    for cluster in clusters:\n        print(' ')\n        print('-----------------')\n        print(str(country) + ' - ' + str(cluster))\n        \n        df = full_table[(full_table['Country\/Region'] == country)&(full_table['Province\/State'] == cluster)]\n        df = df.groupby(['Date','Country\/Region']).sum().reset_index()\n        df['Date'] = pd.to_datetime(df['Date'])\n        df = df.sort_values(by=['Date'])\n        df = df.set_index('Date')[['ConfirmedCases']]\n\n        df_result = df.copy()\n        # df_result = df_result[['Date','Confirmed']]\n\n        # define the models to forecast the growth of cases\n        def model(N, a, alpha, t):\n            return N * (1 - math.e ** (-a * (t))) ** alpha\n\n        def model_loss(params):\n            N, a, alpha = params\n            global df\n            r = 0\n            for t in range(len(df)):\n                r += (model(N, a, alpha, t) - df.iloc[t, 0]) ** 2\n            return r \n        try:\n            N = df['ConfirmedCases'][-1]\n        except:\n            N = 10000\n\n        opt = minimize(model_loss, x0=np.array([N, 0.1, 5]), method='Nelder-Mead', tol=1e-7).x\n        print(opt)\n        \n        x_actual = pd.to_datetime(df.reset_index().iloc[:,0])\n        x_actual = list(x_actual)\n        y_actual = list(df.reset_index().iloc[:,1])\n        \n        start_date = pd.to_datetime(df.index[0])\n        days_forecast = len(df)+len(df_ca_test)-7\n\n        x_model = []\n        y_model = []\n\n        for t in range(days_forecast):\n            x_model.append(start_date + datetime.timedelta(days=t))\n            y_model.append(round(model(*opt,t)))\n        \n        # now add the results of the model to the competition dataframe\n        df2 = pd.DataFrame(y_model,index=x_model,columns=['ConfirmedCases'])\n        df2.index.name = 'Date'\n        df2['Country\/Region']=country\n        df2['Province\/State']=cluster\n        df_comp = pd.merge(df_comp,\n                             df2,\n                             how='left',\n                             on=['Date','Country\/Region','Province\/State']\n                          )\n        \n        df_comp = df_comp.rename(columns={'ConfirmedCases_y': 'ConfirmedCases'})\n        df_comp['ConfirmedCases'] = df_comp['ConfirmedCases'].fillna(df_comp['ConfirmedCases_x'])\n        df_comp = df_comp[['ForecastId','Province\/State','Country\/Region','Date','ConfirmedCases','Fatalities']]\n","d2934fee":"df_comp.head()","3de0f1eb":"countries = list(set(full_table['Country\/Region']))\ncountries.sort()\n\nfor country in countries:\n    clusters = list(set(full_table['Province\/State'][(full_table['Country\/Region'] == country)]))\n    clusters.sort()\n    \n    for cluster in clusters:\n        print(' ')\n        print('-----------------')\n        print(str(country) + ' - ' + str(cluster))\n        \n        df = full_table[(full_table['Country\/Region'] == country)&(full_table['Province\/State'] == cluster)]\n        df = df.groupby(['Date','Country\/Region']).sum().reset_index()\n        df['Date'] = pd.to_datetime(df['Date'])\n        df = df.sort_values(by=['Date'])\n        df = df.set_index('Date')[['Fatalities']]\n\n        df_result = df.copy()\n\n        # define the models to forecast the growth of cases\n        def model(N, a, alpha, t):\n            return N * (1 - math.e ** (-a * (t))) ** alpha\n\n        def model_loss(params):\n            N, a, alpha = params\n            global df\n            r = 0\n            for t in range(len(df)):\n                r += (model(N, a, alpha, t) - df.iloc[t, 0]) ** 2\n            return r \n        try:\n            N = df['Fatalities'][-1]\n        except:\n            N = 10000\n\n        opt = minimize(model_loss, x0=np.array([N, 0.1, 5]), method='Nelder-Mead', tol=1e-7).x\n        print(opt)\n        \n        x_actual = pd.to_datetime(df.reset_index().iloc[:,0])\n        x_actual = list(x_actual)\n        y_actual = list(df.reset_index().iloc[:,1])\n        \n        start_date = pd.to_datetime(df.index[0])\n        days_forecast = len(df)+len(df_ca_test)-7\n\n        x_model = []\n        y_model = []\n\n        for t in range(days_forecast):\n            x_model.append(start_date + datetime.timedelta(days=t))\n            y_model.append(round(model(*opt,t)))\n        \n        # now add the results of the model to the competition dataframe\n        df2 = pd.DataFrame(y_model,index=x_model,columns=['Fatalities'])\n        df2.index.name = 'Date'\n        df2['Country\/Region']=country\n        df2['Province\/State']=cluster\n        df_comp = pd.merge(df_comp,\n                             df2,\n                             how='left',\n                             on=['Date','Country\/Region','Province\/State']\n                          )\n        \n        df_comp = df_comp.rename(columns={'Fatalities_y': 'Fatalities'})\n        df_comp['Fatalities'] = df_comp['Fatalities'].fillna(df_comp['Fatalities_x'])\n        df_comp = df_comp[['ForecastId','Province\/State','Country\/Region','Date','ConfirmedCases','Fatalities']]\n","cb90437d":"df_comp[df_comp['Fatalities']>0]","348980e7":"df_comp.head()","de5b0ed7":"df_ca_test.head()","c7254b93":"df_ca_test['Date'] = pd.to_datetime(df_ca_test['Date'])","b0c3d279":"df_ca_submission.info()","ea1b1b38":"df_sub = df_comp[['ForecastId','ConfirmedCases','Fatalities']]","661aeec9":"df_sub.head()","1746e491":"#read in test file \ntest=pd.read_csv(\"\/kaggle\/input\/covid19-global-forecasting-week-1\/test.csv\")\n#join the submission file info to the test data set \ncomplete_test= pd.merge(test, df_sub, how=\"left\", on=\"ForecastId\")","e7f205d2":"df_sub.to_csv('submission.csv',index=False)","f5a815e6":"## Fatalities","4159a91a":"In this section we start by building and displaying a model for a country (ignoring the clusters). The model is simple to compare with the previous day's results. In the cases where the COVID-19 spread is recent and the number of confirmed cases are few the model is not accurate. In these low-number cases the predictions from yesterday to today may fluctate significantly. With the countries that have cases in the 100s or 1000s however, the model is fairly stable.  ","0b480d24":"# Import","5e431e00":"Hope the results was useful, enjoy!","d5e78b0d":"This section does a brief exploration of the latest data set.\n\nThe first table shows a global summary with the latest data. ","5bdddc29":"# Social diffusion Model","55ebe484":"# Explore","519d4ad8":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><\/li><li><span><a href=\"#Import\" data-toc-modified-id=\"Import-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Import<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Python-Libraries\" data-toc-modified-id=\"Python-Libraries-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;<\/span>Python Libraries<\/a><\/span><\/li><li><span><a href=\"#Study-Settings\" data-toc-modified-id=\"Study-Settings-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;<\/span>Study Settings<\/a><\/span><\/li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;<\/span>Data<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Explore\" data-toc-modified-id=\"Explore-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Explore<\/a><\/span><\/li><li><span><a href=\"#Social-diffusion-Model\" data-toc-modified-id=\"Social-diffusion-Model-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Social diffusion Model<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Confirmed-Cases\" data-toc-modified-id=\"Confirmed-Cases-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Confirmed Cases<\/a><\/span><\/li><li><span><a href=\"#Deaths\" data-toc-modified-id=\"Deaths-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Deaths<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Competition\" data-toc-modified-id=\"Competition-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Competition<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#ConfirmedCases\" data-toc-modified-id=\"ConfirmedCases-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>ConfirmedCases<\/a><\/span><\/li><li><span><a href=\"#Fatalities\" data-toc-modified-id=\"Fatalities-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Fatalities<\/a><\/span><\/li><li><span><a href=\"#Submission-File\" data-toc-modified-id=\"Submission-File-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Submission File<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Conclusion\" data-toc-modified-id=\"Conclusion-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Conclusion<\/a><\/span><\/li><\/ul><\/div>","84faa7b3":"## Confirmed Cases","eba207aa":"# Competition","4cff911f":"## ConfirmedCases","457ee660":"The model is the same one as before (except with an 'offset'). The model is from a marketing paper by Emmanuelle Le Nagard and Alexandre Steyer, that attempts to reflect the social structure of a diffusion process. The paper is available (in French) [here](https:\/\/www.jstor.org\/stable\/40588987)\n\nThe model is also sensitive to when we define the origin of time for the epidemic process. The model has an offset parameter included and better starting conditions for the optimization algorithm. The shape of the difusion can then be expressed in the following equation:\n\n$$N(1 - e^{-a(t-t_0)})^{\\alpha}$$\n","79e72093":"## Deaths","9e68d191":"Configure the notebook (see https:\/\/jupyter-notebook.readthedocs.io\/en\/stable\/config.html)","f725ade0":"## Data","a322fec1":"# Conclusion","17d16f70":"# Introduction","a113a492":"## Study Settings","c0ed1da6":"## Python Libraries","dbdd6a82":"What i've learnt in the following sections is that the inclusion of an offset is fickle. It can outperform the non-offset model, but when it fails, it has a much larger error than the non-offset model. For the competition then, the non-offset model is used to calculate the results for all countries. ","be9434ae":"## Submission File","0644d50f":"The figure above compares the errors of the offset model vs the no-offset model. Where the error is zero, the actuals did not have a value>0 yet.","e9afd049":"Well now, this is the same dataset that we've used in the previous chapter. Just up to 18 March. So the model can be used with just the last couple of days dropped. ","a26dc0af":"This study uses the work from the following source: https:\/\/www.kaggle.com\/alixmartin\/covid-19-predictions. The previous study applied a model at a country level to predict the growth of confirmed cases. \n\nThe previous approach could not adequately explain the odd behaviours of growth for countries like China. In the China data, the curve seems to grow exponentially, then tapers off, then picks up exponentially again, and then tapers off.  \n\nThis author reckons that this behaviour exists, because the growth is cluster-based. Each cluster should be treated as a newly infected 'country' and therefore modelled seperately with  their results rolled up to predict the growth at a country or global level.\n\nThe data does not identify the clusters per country explicitly (which is probably at a town or suburb level). Therefore the study will examine it by province\/state to see whether a significant improvement in accuracy can be obtained.\n\nIn future work, the study could estimate the number of clusters, and provide parameters in the model that could assist in identifying clusters that are managing the COVID-19 contagion well or poorly.  "}}