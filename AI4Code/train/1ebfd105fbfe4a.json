{"cell_type":{"73cdca8a":"code","74a1b77c":"code","107abf89":"code","19e55e77":"code","bedbec2f":"code","96607b6f":"code","838d59a9":"code","62073dca":"code","4508b9bb":"code","43fe2311":"code","0c80ab34":"code","d402511a":"code","a13d57dd":"code","677cb7b7":"code","0adfe7f4":"code","76c18da4":"code","9716eeb5":"code","5f159609":"code","d20e7f1d":"code","60de588d":"code","d18c2231":"code","5cdcf68a":"code","50ac0180":"code","c4556b91":"code","0acb3e4a":"code","a0ff5dfa":"code","3293e7c1":"code","cd60a2be":"markdown","86355787":"markdown"},"source":{"73cdca8a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\n\nfrom sklearn.preprocessing import OrdinalEncoder","74a1b77c":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","107abf89":"train","19e55e77":"train.info()","bedbec2f":"train.shape, test.shape","96607b6f":"# Train columns that do not exist in Test columns\ncols_missing = [col for col in train.columns if col not in test.columns]\nprint(\"Train columns that do not exist in Test columns:\", cols_missing)\n\n# Test columns that do not exist in Train columns\ncols_missing = [col for col in test.columns if col not in train.columns]\nprint(\"Test columns that do not exist in Train columns:\", cols_missing)","838d59a9":"# how many total missing values do we have?\nmissing_values_count = test.isnull().sum()\ntotal_cells = np.product(test.shape)\ntotal_missing = missing_values_count.sum()\n\n# percent of data that is missing\npercent_missing = (total_missing\/total_cells) * 100\nprint(percent_missing)","62073dca":"# Number of missing values in each column of training data\nmissing_val_count_by_column = train.isnull().sum()\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])\n\n# Number of missing values in each column of test data\nmissing_val_count_by_column = test.isnull().sum()\nprint(missing_val_count_by_column[missing_val_count_by_column > 0])","4508b9bb":"plt.figure(figsize=(8,5))\nsns.catplot(x='Embarked', hue='Survived', data=train, kind=\"count\")","43fe2311":"train.loc[train.Embarked.isnull()]","0c80ab34":"train.Embarked.fillna('S', inplace=True)","d402511a":"train.iloc[829, -1]","a13d57dd":"test.Fare.median()","677cb7b7":"test.loc[test.Fare.isnull()]","0adfe7f4":"test.Fare.fillna(test.Fare.median(), inplace=True)","76c18da4":"test.iloc[152, -3]","9716eeb5":"# Categorial features\ncat_features = ['Sex', 'Embarked']\n\n# List of numerical features\nnum_features = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']","5f159609":"# Categorical columns - unique values\nfrom collections import Counter\n\nfor col in cat_features:\n    train_unique = train[col].unique()\n    test_unique = test[col].unique()\n    if Counter(train_unique) != Counter(test_unique):\n        print(col, train_unique, test_unique)\n    \nprint(\"Done.\")","d20e7f1d":"# Columns that can be safely ordinal encoded\ngood_label_cols = [col for col in cat_features if \n                   set(test[col]).issubset(set(train[col]))]\n\n# Problematic columns that will be dropped from the dataset\nbad_label_cols = list(set(cat_features)-set(good_label_cols))\n\nprint(good_label_cols)\nprint(bad_label_cols)","60de588d":"oe_train = train.drop(bad_label_cols, axis=1)\noe_test = test.drop(bad_label_cols, axis=1)\n\n# Apply ordinal encoder \nordinal_encoder = OrdinalEncoder()\noe_train[good_label_cols] = ordinal_encoder.fit_transform(oe_train[good_label_cols])\noe_test[good_label_cols] = ordinal_encoder.transform(oe_test[good_label_cols])","d18c2231":"oe_test","5cdcf68a":"combined = cat_features + num_features\ncombined","50ac0180":"# Combined dataframe containing numerical features only\ndf = pd.concat([oe_train[combined], oe_test[combined]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(20,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n            # Train data histogram\n            hist1 = axs[r, c].hist(oe_train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            # Test data histogram\n            hist2 = axs[r, c].hist(oe_test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n# plt.suptitle(\"Numerical feature values distribution in both datasets\", y=0.99)\nplt.show();","c4556b91":"# Useful features\nuseful = ['PassengerId', 'Survived', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\noe_train[useful].to_csv(\"train_useful.csv\", index=False)","0acb3e4a":"# Useful features\nuseful = ['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\noe_test[useful].to_csv(\"test_useful.csv\", index=False)","a0ff5dfa":"train = pd.read_csv(\".\/train_useful.csv\")\ntrain","3293e7c1":"number_of_folds = 5\nfrom sklearn.model_selection import StratifiedKFold\n\n# Load original training dataset\nX = train.copy()\ny = X['Survived']\n# Add a column for fold number\nX[\"kfold\"] = -1\n\n# Split the data into folds\nskf = StratifiedKFold(n_splits=number_of_folds, shuffle=True, random_state=42)\nfor fold, (train_indices, valid_indices) in enumerate(skf.split(X, y)):\n    X.loc[valid_indices, \"kfold\"] = fold\n\n# Save the new train dataset\nX.to_csv(\"titanic_5folds.csv\", index=False)\nprint(\"Saved train_folds.csv. Check output\")","cd60a2be":"## Create Stratified folds","86355787":"Age - Predict using other columns  \nCabin - drop  \nEmbarked - Fill in a dummy value (Eg: 'S')  \nFare - Mean of Fares"}}