{"cell_type":{"89b47477":"code","1e44e0a2":"code","d74edbdb":"code","759a0970":"code","69cf2411":"code","c2d25440":"code","6e53c858":"code","d3ef7139":"code","3cb43bee":"code","ee4c494b":"code","3a93a1f7":"code","0e5fc7da":"code","df934be0":"code","de38dcc0":"code","68205c26":"code","b31ec5cb":"code","7a68898d":"code","1897bce7":"code","27401e49":"code","1ec98bad":"code","acef2f55":"code","5dfb2e32":"code","cdadb64d":"code","4bdbd70f":"code","456b8e9f":"code","6f3872bd":"code","a309581e":"code","ca95c92a":"code","a8b40d41":"markdown","a7078da4":"markdown","8df508fb":"markdown","73dda6f5":"markdown","416ea603":"markdown","9acc59bb":"markdown","ce2cf011":"markdown","4e02832d":"markdown","7905ed1a":"markdown","42a43a24":"markdown","b87de1ef":"markdown","435173ab":"markdown","afe51770":"markdown","03a04193":"markdown","a27c4cf8":"markdown","8f80a3eb":"markdown","87bef121":"markdown","5e31ec94":"markdown","c461e8eb":"markdown","ed035cc2":"markdown","48258e43":"markdown","2b19256a":"markdown","683ace0f":"markdown","1e701694":"markdown","0e0ca9f4":"markdown","ef30cc60":"markdown","1b071372":"markdown","664e1580":"markdown","602a3ec4":"markdown"},"source":{"89b47477":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt","1e44e0a2":"def open_csv(file):\n    path = '\/kaggle\/input\/competitive-data-science-predict-future-sales'\n    return pd.read_csv(os.path.join(path, file))\n\n# Abre os arquivos\nitems = open_csv('items.csv')\nshops = open_csv('shops.csv')\nitem_categories = open_csv('item_categories.csv')\ntrain = open_csv('sales_train.csv')\nsubmission = open_csv('sample_submission.csv')\ntest = open_csv('test.csv')","d74edbdb":"print(f'N\u00famero de elementos no dataframe de treino: {train.shape[0]}')\nprint(f'Quantidade de lojas diferentes: {shops.shape[0]}')\nprint(f'Quantidade de itens diferentes: {items.shape[0]}')\nprint(f'Quantidade de categorias diferentes: {item_categories.shape[0]}')","759a0970":"print(train.columns)","69cf2411":"# Remove dados duplicados\nattr = ['date', 'date_block_num', 'shop_id', 'item_id', 'item_cnt_day']\nold_size = train.shape[0]\ntrain.drop_duplicates(attr, inplace=True)\nprint(f'Quantidade de elementos duplicados: {old_size - train.shape[0]}')","c2d25440":"# Retira dados sem relev\u00e2ncia para a an\u00e1lise\ntrain.drop(columns=['date'], inplace=True)\nitems.drop(columns=['item_name'], inplace=True)\ntest.drop(columns=['ID'], inplace=True)","6e53c858":"# Retira outliers\nold_size = train.shape[0]\nprint('Vendas:')\nprint(train['item_cnt_day'].describe())\ntrain.boxplot(column='item_cnt_day')\nplt.show()\nprint('\\nPre\u00e7o:')\nprint(train['item_price'].describe())\ntrain.boxplot(column='item_price')\nplt.show()","d3ef7139":"train = train[(train['item_cnt_day'] >= 0) &\n              (train['item_cnt_day'] <= 800) &\n              (train['item_price'] >= 0) &\n              (train['item_price'] <= 100000)]\n\nprint(f'Quantidade de elementos retirados: {old_size - train.shape[0]}')","3cb43bee":"# Adiciona a categoria dos itens ao dataframe\ntrain = pd.merge(train, items, on='item_id', how='left')\ntest = pd.merge(test, items, on='item_id', how='left')","ee4c494b":"def reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Mem\u00f3ria utilizada pelo dataframe: {:.2f} MB.'.format(start_mem))\n    for col in df.columns:\n        col_type = df[col].dtype\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)\n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Mem\u00f3ria utilizada ap\u00f3s a otimiza\u00e7\u00e3o: {:.2f} MB.'.format(end_mem))\n    print('Uso de mem\u00f3ria reduzido em {:.1f}%.'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","3a93a1f7":"# Reduz a mem\u00f3ria utilizada\ntrain = reduce_mem_usage(train)","0e5fc7da":"from itertools import product\n\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n# Cria uma combina\u00e7\u00e3o de cada loja\/item daquele m\u00eas.\ngrid = []\nfor block_num in train['date_block_num'].unique():\n    cur_shops = train.loc[train['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = train.loc[train['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])), dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns=index_cols, dtype=np.int32)\n\nprint(f'\\nQuantidade de elementos: {grid.shape[0]}\\n')\n\nprint(grid)","df934be0":"# Reduz a mem\u00f3ria utilizada\ngrid = reduce_mem_usage(grid)","de38dcc0":"sales_m = train.groupby(index_cols).agg({'item_cnt_day': 'sum',\n                                         'item_price': np.mean}).reset_index()\n\nprint(sales_m)","68205c26":"sales_m = pd.merge(grid, sales_m, on=index_cols, how='left').fillna(0)\n\nprint(f'Quantidade de elementos: {sales_m.shape[0]}\\n')\nprint(sales_m)","b31ec5cb":"sales_m = pd.merge(sales_m, items, on='item_id', how='left')","7a68898d":"for type_id in ['item_id', 'shop_id', 'item_category_id']:\n    for column_id, aggregator, aggtype in [('item_price', np.mean, 'avg'),\n                                           ('item_cnt_day', np.sum, 'sum'),\n                                           ('item_cnt_day', np.mean, 'avg')]:\n\n        # Gera os novos atributos e renomeia as colunas\n        mean_df = train.groupby([type_id, 'date_block_num']).aggregate(aggregator).reset_index()[[column_id, type_id, 'date_block_num']]\n        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id, type_id, 'date_block_num']\n        \n        # Une os novos atributos ao dataframe original\n        sales_m = pd.merge(sales_m, mean_df, on=['date_block_num', type_id], how='left')\n    \nprint(sales_m.columns)","1897bce7":"# Definimos quais vari\u00e1veis ser\u00e3o analisadas dos meses passados\nlag_variables = list(sales_m.columns[6:])+['item_cnt_day']\n# Definimos a janela temporal\nlags = [1, 2, 3, 4, 5, 6, 12]\n    \ndef lag_features(df, sales_m):\n    for lag in lags:\n        # Cria-se uma c\u00f3pia do sales_m\n        sales_new_df = sales_m.copy()\n        # Translada os meses adicionando o valor lag a eles\n        sales_new_df.date_block_num += lag\n        # Adiciona as novas colunas com as vari\u00e1veis temporais\n        sales_new_df = sales_new_df[index_cols+lag_variables]\n        sales_new_df.columns = index_cols + [f'{lag_feat}_lag_{lag}'\n                                             for lag_feat in lag_variables]\n        # Une o dataset original com o com os atributos temporais, usando de refer\u00eancia\n        # o identificador do item e da loja, e o m\u00eas de venda (index_cols).\n        df = pd.merge(df, sales_new_df, on=index_cols, how='left')\n\n    return df\n\n\nlag_train = sales_m.copy()\nlag_train = lag_features(lag_train, sales_m)\n\nprint(f'Quantidade de atributos no dataframe: {lag_train.shape[1]}\\n')\nprint(lag_train.columns)","27401e49":"def fillNaN(df):\n    for feat in df.columns:\n        if 'item_cnt' in feat:\n            df[feat] = df[feat].fillna(0)\n        elif 'item_price' in feat:\n            df[feat] = df[feat].fillna(df[feat].median())\n\nfillNaN(lag_train)","1ec98bad":"def date_attr(df):\n    df['month'] = df['date_block_num'] % 12\n    df['year'] = df['date_block_num'] \/\/ 12\n    df.drop(columns='date_block_num', inplace=True)\n    return df\n\n# Retira os meses do primeiro ano\nlag_train = lag_train[lag_train['date_block_num'] > 12]\n# Gera novos atributos de ano e m\u00eas\nlag_train = date_attr(lag_train)","acef2f55":"# Retira os atributos que n\u00e3o ser\u00e3o utilizados na an\u00e1lise,\n# como os atributos utilizados para gerar as lag features.\ncols_to_drop = lag_variables[:-1] + ['item_price']\nlag_train.drop(columns=cols_to_drop, inplace=True)\n\n# Reduz a mem\u00f3ria utilizada\nlag_train = reduce_mem_usage(lag_train)","5dfb2e32":"test['date_block_num'] = 34\n\ntest = lag_features(test, sales_m)\nfillNaN(test)\ntest = date_attr(test)","cdadb64d":"# Verificando se o treino e o teste possuem as mesmas colunas\n_test = set(test.columns)\n_train = set(lag_train.drop(columns='item_cnt_day').columns)\nassert _test==_train","4bdbd70f":"from sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor(n_estimators=10, n_jobs = -1)\n\nregressor.fit(lag_train.drop(columns=['item_cnt_day']),\n              lag_train['item_cnt_day'])\n\npred = regressor.predict(test)\n\npred = pred.clip(0, 20)\n\n\nsubmission['item_cnt_month'] = pred\n\nsubmission.to_csv('submission_rf.csv', index=False)","456b8e9f":"# Feature importance\nfeat_importances = pd.Series(regressor.feature_importances_, index=lag_train.drop(columns=['item_cnt_day']).columns)\nfeat_importances.nlargest(10).plot(kind='barh')","6f3872bd":"param = {'max_depth': 10,\n         'subsample': 1,\n         'min_child_weight': 1,\n         'eta': 0.3,\n         'num_round': 1000,\n         'eval_metric': 'rmse',\n         'verbosity': 0}","a309581e":"import xgboost as xgb\n\nxgbtrain = xgb.DMatrix(lag_train.drop(columns=['item_cnt_day']),\n                       lag_train['item_cnt_day'])\n\nbst = xgb.train(param, xgbtrain)\n\nxgbpredict = xgb.DMatrix(test)\npred = bst.predict(xgbpredict)\npred = pred.clip(0, 20)\n\nsubmission['item_cnt_month'] = pred\n\nsubmission.to_csv('submission_xgboost.csv', index=False)","ca95c92a":"# Feature importance\nx = xgb.plot_importance(bst)\nx.figure.set_size_inches(10, 30)","a8b40d41":"## Conclus\u00e3o\n\nA abordagem utilizando lag features foi eficaz e trouxe resultados consider\u00e1veis, alcan\u00e7amos um erro rmse de 0.93 utilizando xgboost, que demora por volta de 30 minutos para treinar e predizer as vendas, e um erro rmse de 1.10 utilizando a random forest, que n\u00e3o conseguiu resultados t\u00e3o bons quanto o xgboost e levava por volta de 1 hora para treinar e predizer as vendas. Ambas as abordagens alcan\u00e7aram resultados melhores que os valores iniciais do arquivo de submiss\u00e3o, que chuta todas as predi\u00e7\u00f5es como 0.5, que alcan\u00e7ou 1.23 de erro.","a7078da4":"Uma vari\u00e1vel interessante de ser utilizada na an\u00e1lise \u00e9 a **categoria** dos itens, pois agrupa os produtos em rela\u00e7\u00e3o a suas caracter\u00edsticas, que podem ter alguma rela\u00e7\u00e3o as vendas. Esse atributo foi adicionado utilizando a fun\u00e7\u00e3o [**merge**](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.merge.html) do pandas.","8df508fb":"Abaixo podemos visualizar a quantidade de registros e o tamanho dos dataframes:","73dda6f5":"Com o dataset de treino completo, precisamos inserir essas vari\u00e1veis temporais nos testes, j\u00e1 que apenas possu\u00edmos a informa\u00e7\u00e3o de qual item em qual loja desejamos prever. Para isso, inicializamos a data como o 35\u00ba m\u00eas (valor 34), e utilizamos das mesmas fun\u00e7\u00f5es implementadas ao gerar os casos de treino para gerar as lag_features, preencher os dados vazios gerados e adicionar os novos dois atributos de m\u00eas e ano.","416ea603":"### Lag features\n\nAs\u00a0**lag features**\u00a0s\u00e3o abordagens muito utilizadas em problemas de predi\u00e7\u00e3o temporal, pois os transforma em problemas supervisionados. A ideia \u00e9 tentar prever valores em um tempo\u00a0**t**\u00a0dadas as informa\u00e7\u00f5es dos tempos\u00a0**t-1**,\u00a0**t-2**, ... Essas informa\u00e7\u00f5es auxiliam o algoritmo a prever vendas em um certo m\u00eas utilizando informa\u00e7\u00f5es de vendas nos meses passados.\nPara gerar as lag features, criamos novos atributos temporais com informa\u00e7\u00f5es de meses passados para cada um dos elementos, a partir de uma janela temporal que define quais meses anteriores ser\u00e3o analisados. A janela analisa os 6 \u00faltimos meses antes da predi\u00e7\u00e3o, e este mesmo m\u00eas no ano passado.\nCaso queira mais informa\u00e7\u00f5es sobre lag features [clique aqui]( https:\/\/machinelearningmastery.com\/basic-feature-engineering-time-series-data-python\/).\nAbaixo temos a implementa\u00e7\u00e3o de uma fun\u00e7\u00e3o que gera as lag features no dataframe.","9acc59bb":"### XGBoost\n\nDepois de tratar os dados da predi\u00e7\u00e3o, agora sim podemos process\u00e1-la. Primeiro \u00e9 claro treinamos com os meses passados o xgboost com os par\u00e2metros abaixo. Depois fazemos a predi\u00e7\u00e3o de vendas do xgboost, e criamos o arquivo padr\u00e3o para envio.","ce2cf011":"Adicionamos tamb\u00e9m no novo dataframe as categorias dos itens.","4e02832d":"A abordagem utilizando Random Forest alcan\u00e7ou um erro de 1.10","7905ed1a":"Os par\u00e2metros foram escolhidos em sua maioria a partir da an\u00e1lise do dataset feito em outros notebooks da competi\u00e7\u00e3o principalmente [esse]( https:\/\/www.kaggle.com\/alessandrosolbiati\/using-xgboost-for-time-series-prediction-top-20\n), alguns foram testes que n\u00f3s mesmos fizemos, que envolve em sua maioria reduzir o uso de mem\u00f3ria. O par\u00e2metro\u00a0**max_depth**\u00a0foi definido em 10 para dar uma maior liberdade de crescimento do programa que o valor default de 6, foi decidido colocar a\u00a0**verbosity**\u00a0em 0 para deixar o output mais limpo, e\u00a0**eval_metric**\u00a0foi decidido como forma de auto avalia\u00e7\u00e3o do algoritmo para aprendizado ser igual \u00e0 da competi\u00e7\u00e3o, para os outros par\u00e2metros foi decidido manter o valor default do programa depois dos testes darem bons resultados.","42a43a24":"Por utilizar a abordagem das lag features, os primeiros doze meses do dataframe n\u00e3o possuem as informa\u00e7\u00f5es necess\u00e1rias para gerar todas as vari\u00e1veis temporais, por isso eles s\u00e3o retirados do dataframe. Depois, para lidar melhor com a [sazonalidade]( https:\/\/www.devmedia.com.br\/data-mining-na-pratica-time-series\/5414) anual de venda dos produtos, utilizamos a abordagem de gerar dois novos atributos, um de ano, com um valor entre 0 e 2, e outro de m\u00eas, com um valor entre 0 e 11, dessa forma \u00e9 poss\u00edvel isolar caracter\u00edsticas que dizem respeito a certos per\u00edodos do ano.","b87de1ef":"## An\u00e1lise dos dados\n\nAntes de tudo, \u00e9 interessante analisarmos com o que estamos trabalhando, a base e o objetivo dela. A base \u00e9 composta de vendas realizadas ao longo de 34 meses, cada elemento da base representa uma venda, com informa\u00e7\u00f5es como identificador do item, identificador da loja onde foi vendido, pre\u00e7o, quantidade de produtos, a categoria do produto, entre outras informa\u00e7\u00f5es. O prop\u00f3sito desta competi\u00e7\u00e3o \u00e9, a partir dos dados ao longo dos 34 meses, prever vendas do 35\u00ba m\u00eas. Com as informa\u00e7\u00f5es dispon\u00edveis \u00e9 poss\u00edvel tentar entender um poss\u00edvel padr\u00e3o de vendas que generalize bem o cen\u00e1rio e us\u00e1-lo para prever as vendas futuras. Os dados da competi\u00e7\u00e3o s\u00e3o distribu\u00eddos em 6 partes:\n\n* **shops**: cont\u00e9m o nome de cada loja e seu id;\n\n* **items**: cont\u00e9m o nome de cada item, seu id, e a qual categoria ele pertence;\n\n* **item_categories**: cont\u00e9m as categorias dos itens;\n\n* **sales_train**: cont\u00e9m os dados das vendas, possui o id do item, o id da loja na qual a venda foi realizada, o dia e o m\u00eas da venda, o pre\u00e7o do item e a quantidade de itens vendidos;\n\n* **sample_submission**: um exemplo de submiss\u00e3o;\n\n* **test**: cont\u00e9m o id da loja e do item no qual se deseja prever a venda no 35\u00ba m\u00eas.\n\nAlgumas informa\u00e7\u00f5es n\u00e3o fazem muita diferen\u00e7a na an\u00e1lise, como o nome do produto e da loja, j\u00e1 que eles possuem id's que servem para identific\u00e1-los.","435173ab":"A abordagem utilizando XGBoost alcan\u00e7ou um erro de 0.93, e garantindo a posi\u00e7\u00e3o 2603\u00ba no rank da competi\u00e7\u00e3o no dia 30\/06\/2020.","afe51770":"O dataframe possui 6 colunas, que s\u00e3o descritas abaixo:\n* **date**: data da venda no formato dd\/mm\/aa;\n* **date_block_num**: m\u00eas da venda representado em uma vari\u00e1vel de 0 at\u00e9 33;\n* **shop_id**: identificador da loja;\n* **item_id**: identificador do item;\n* **item_price**: pre\u00e7o do item;\n* **item_cnt_day**: quantidade de itens vendidos.","03a04193":"Manipular dataframes com grandes quantidades de informa\u00e7\u00e3o (quase 3 milh\u00f5es de elementos no dataframe de treino) pode ser um problema, pois torna o pr\u00e9-processamento e o processamento computacionalmente custosos, necessitando de uma grande quantidade de mem\u00f3ria para manipular os dados. Para amenizar esse problema, utilizaremos de uma fun\u00e7\u00e3o de redu\u00e7\u00e3o de mem\u00f3ria no dataframe principal, que procura armazenar os dados num\u00e9ricos em vari\u00e1veis com menos precis\u00e3o, mas que ocupam menos espa\u00e7o em mem\u00f3ria. A fun\u00e7\u00e3o pode ser vista abaixo e encontrada neste [link](https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage), de onde foi retirada.","a27c4cf8":"## Novos atributos\n\nAgora, criamos novos atributos de m\u00e9dia e soma da quantidade de itens e de m\u00e9dia do pre\u00e7o, isso para cada item, loja e categoria diferente. Essa abordagem permite que o algoritmo receba informa\u00e7\u00f5es mais gerais a respeito dos pre\u00e7os e do n\u00famero de vendas dos itens, das lojas e das categorias, fazendo que o programa consiga analisar a situa\u00e7\u00e3o das vendas recentes analisando se um produto ou loja ficou ou saiu de tend\u00eancia.","8f80a3eb":"## Processamento\n\nAp\u00f3s o pr\u00e9-processamento feito no dataframe, ele est\u00e1 pronto para ser utilizado pelos algoritmos regressores. Aqui testaremos com dois deles, o [Random Forest]( https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html) e o [XGBoost]( https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html?highlight=matrix#xgboost.DMatrix), para que seja poss\u00edvel analisarmos a diferen\u00e7a dos resultados obtidos utilizando dois algoritmos diferentes.\n\n### Random Forest\n\nAp\u00f3s todas as an\u00e1lises feitas, e todo tratamento com a base, estamos enfim prontos para processar e estabelecer nossas predi\u00e7\u00f5es. Abaixo temos a finaliza\u00e7\u00e3o do projeto, aqui inicialmente usamos uma regress\u00e3o com floresta aleat\u00f3ria criando 10 \u00e1rvores, com n_jobs = -1, para usar todas as unidades centrais de processamento, j\u00e1 que a base, mesmo com fun\u00e7\u00f5es para diminuir o custo de mem\u00f3ria ainda \u00e9 muito pesado foi necess\u00e1rio reduzir o n\u00famero de \u00e1rvores para dez o que pelos testes feitos n\u00e3o afetaram tanto na precis\u00e3o da predi\u00e7\u00e3o. Treinamos a floresta com a base trabalhada, e transformamos o exemplo de submiss\u00e3o da base na nossa predi\u00e7\u00e3o. Al\u00e9m disso devemos para a submiss\u00e3o devemos limitar as vendas da predi\u00e7\u00e3o de 0 a 20 para enfim transformar em um arquivo csv e submeter.","87bef121":"# Vendas Futuras\n\nO notebook a seguir descreve os estudos e os passos levados para realizar a competi\u00e7\u00e3o [Predict Future Sales no kaggle](https:\/\/www.kaggle.com\/c\/competitive-data-science-predict-future-sales\/overview) que procura predizer as futuras vendas de uma rede de lojas. O notebook a seguir explica os passos levados para uma aplica\u00e7\u00e3o dos regressores Random Forest e XGBoost na an\u00e1lise do problema.\n\nEste notebook foi construindo utilizando diversas refer\u00eancias que podem ser encontradas nos links a seguir:\n\n1. https:\/\/machinelearningmastery.com\/basic-feature-engineering-time-series-data-python\/\n2. https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n3. https:\/\/mlwhiz.com\/blog\/2017\/12\/26\/win_a_data_science_competition\/\n4. https:\/\/www.kaggle.com\/alessandrosolbiati\/using-xgboost-for-time-series-prediction-top-20\n5. https:\/\/www.kaggle.com\/josemarrugo\/predictionsales","5e31ec94":"## Pr\u00e9-processamento\n\nNos pr\u00f3ximos blocos, s\u00e3o realizados alguns tratamentos iniciais no dataset. Tiramos features que foram duplicadas na jun\u00e7\u00e3o de arquivos, dados desnecess\u00e1rios, como nome dos itens. Como o prop\u00f3sito \u00e9 a predi\u00e7\u00e3o de um m\u00eas inteiro de vendas, os dados ser\u00e3o agrupados por m\u00eas utilizando a informa\u00e7\u00e3o contida na vari\u00e1vel\u00a0**date_block_num**, logo, o dia da venda tamb\u00e9m n\u00e3o \u00e9 \u00fatil e ser\u00e1 retirado do dataframe.","c461e8eb":"Com os novos atributos temporais prontos, precisamos preencher os valores ausentes, abaixo a fun\u00e7\u00e3o\u00a0**fillNaN**\u00a0trocamos todos os valores NaN gerados acima por zero quando se trata da quantidade de itens vendidas j\u00e1 que quando n\u00e3o h\u00e1 informa\u00e7\u00e3o indica que nada foi vendido daquele item na loja, ou pela mediana quando se trata dos pre\u00e7os j\u00e1 que durante um \u00fanico m\u00eas pode existir a varia\u00e7\u00e3o dos pre\u00e7os dos itens, e foi decidido colocar a mediana para representar o valor durante o m\u00eas inteiro.","ed035cc2":"Depois, utilizamos a fun\u00e7\u00e3o **merge** para unir as novas combina\u00e7\u00f5es de item\/loja da vari\u00e1vel **grid** no novo dataframe **sales_m**, e preenchemos eles com zeros.","48258e43":"O dataframe utilizado descreve ocorr\u00eancias de vendas, mas n\u00e3o a aus\u00eancia delas, n\u00e3o se tem a informa\u00e7\u00e3o de quando ocorreram zero vendas para certos itens em certas lojas, e sem essa informa\u00e7\u00e3o o regressor ficar\u00e1 enviesado por apenas possuir informa\u00e7\u00f5es de quando as vendas ocorrem, assumindo que pelo menos um item ser\u00e1 vendido em todas as predi\u00e7\u00f5es, mesmo sendo poss\u00edvel que nenhum item seja vendido naquele dia.\n\nPara resolver esse problema, precisamos gerar esses dados de quando n\u00e3o existem vendas, que s\u00e3o muitos, pois precisamos inserir zeros para cada combina\u00e7\u00e3o de item e loja que n\u00e3o possui informa\u00e7\u00e3o de venda. Abaixo temos a cria\u00e7\u00e3o desses novos dados, primeiro iniciamos a vari\u00e1vel **grid** e nela geramos todas essas combina\u00e7\u00f5es de itens e lojas para um certo m\u00eas, para isso utilizamos o m\u00e9todo [**unique**](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.Series.unique.html) para identificar os valores \u00fanicos de item e loja, depois da utilizamos da fun\u00e7\u00e3o [**product**](https:\/\/docs.python.org\/2\/library\/itertools.html) para gerar as combina\u00e7\u00f5es. Por fim, utilizamos a fun\u00e7\u00e3o [**vstack**](https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.vstack.html) para gerar um dataframe novo, onde as combina\u00e7\u00f5es por m\u00eas est\u00e3o empilhadas.","2b19256a":"Aqui o algoritmo percebe uma import\u00e2ncia muito grande para qual loja estamos falando, seguida das vendas do m\u00eas passado, o item sendo analisado, o m\u00eas do ano em que estamos, e a m\u00e9dia de pre\u00e7os dos itens vendidos no m\u00eas passado. A enorme import\u00e2ncia das lojas \u00e9 intuitiva, j\u00e1 que existem lojas que nunca vendem nada. O item tamb\u00e9m faz sentido, j\u00e1 que produtos s\u00e3o naturalmente mais vendidos e necess\u00e1rios que outros.\n\nMas o interessante a se notar aqui \u00e9 talvez, a diferen\u00e7a entre as import\u00e2ncias de atributos dado por ambos os modelos. Que se difere em muitos pontos, mas outros s\u00e3o id\u00eanticos, talvez seja esse o motivo pelo qual o resultado foi diferente, e mostram tamb\u00e9m a maior precis\u00e3o do xgboost para o dataset final.","683ace0f":"Foram retirados alguns elementos **outliers** do dataframe, que s\u00e3o valores at\u00edpicos ou prov\u00e1veis erros na coleta dos dados. Os valores retirados foram em rela\u00e7\u00e3o ao pre\u00e7o dos itens, e a quantidade de itens retirados. A partir da an\u00e1lise feita abaixo, foi identificada a exist\u00eancia de valores negativos, que n\u00e3o fazem sentido para essas vari\u00e1veis, e de valores muito distantes das m\u00e9dias, que foram selecionados a partir de um limite definido olhando para os gr\u00e1ficos.","1e701694":"Come\u00e7amos importando algumas bibliotecas utilizadas na an\u00e1lise, como a [pandas](https:\/\/pandas.pydata.org\/docs\/reference\/index.html), o [numpy](https:\/\/numpy.org\/doc\/stable\/reference\/index.html) e o [matplotlib](https:\/\/matplotlib.org\/). Depois, abrimos os arquivos com os dados das vendas.","0e0ca9f4":"Como pode-se ver acima foi mostrado o gr\u00e1fico com a feature importances que indica quais atributos mais influenciaram na predi\u00e7\u00e3o, nesse caso o programa definiu que a informa\u00e7\u00f5es mais importantes s\u00e3o a venda do m\u00eas passado, o item que est\u00e1 sendo vendido e a loja onde ele est\u00e1 sendo vendido, o que s\u00e3o resultados bem interessantes ao pensar que a venda do m\u00eas passado seja a informa\u00e7\u00e3o mais importante por ser a mais recente.","ef30cc60":"Uma vez que o algoritmo possui o intuito de prever vendas de um m\u00eas inteiro, e os dados que s\u00e3o fornecidos s\u00e3o de vendas di\u00e1rias, optamos por agrupar esses dados por m\u00eas e n\u00e3o utilizar a informa\u00e7\u00e3o do dia exato da venda. Essa abordagem \u00e9 menos custosa computacionalmente, pois diminui a quantidade de dados ao agrup\u00e1-los, e \u00e9 uma abordagem que faz sentido pensando no objetivo final da competi\u00e7\u00e3o, que \u00e9 predizer vendas em todo um m\u00eas. Para isso utilizamos a fun\u00e7\u00e3o [**groupby**](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.groupby.html) para agrupar as vendas de meses iguais, somando as vendas, tirando a m\u00e9dia dos pre\u00e7os e armazenando essa nova informa\u00e7\u00e3o no dataframe\u00a0**sales_m**.","1b071372":"Ap\u00f3s as modifica\u00e7\u00f5es feitas, utilizamos a fun\u00e7\u00e3o de redu\u00e7\u00e3o de mem\u00f3ria no dataframe para otimizar os pr\u00f3ximos processos.","664e1580":"## Evolu\u00e7\u00e3o do programa\nDurante a cria\u00e7\u00e3o desse notebook foram criadas v\u00e1rias abordagens do problema, que ao longo do tempo foram naturalmente evoluindo. Abaixo descrevemos como essa evolu\u00e7\u00e3o se deu. \n\n### vers\u00e3o 0\n\nPrimeiramente n\u00f3s come\u00e7amos apenas colocando o data set no programa para ver os resultados, para assim entendermos melhor as notas da competi\u00e7\u00e3o. Nos deparamos ent\u00e3o com o primeiro problema do dataset, ele vinha com os dados de dias e o target era um m\u00eas inteiro, felizmente esse foi um problema que foi facilmente resolvido por juntar as informa\u00e7\u00f5es dos dias do m\u00eas apenas no m\u00eas.\n\n### vers\u00e3o 1\n\nO que nos levou ao segundo problema pelo formato do dataset, n\u00f3s s\u00f3 temos dados de quando houveram vendas. Isso foi um grande obst\u00e1culo pois isso indica para o algoritmo, que n\u00f3s sempre temos vendas o que \u00e9 muito ruim para an\u00e1lise, j\u00e1 que a maioria das vezes n\u00e3o existem vendas. Ent\u00e3o n\u00f3s pegamos uma solu\u00e7\u00e3o simples para esse problema, inserir os zeros manualmente, por\u00e9m isso  veio com um custo, um alto consumo de mem\u00f3ria que se n\u00e3o parasse o algoritmo em fase de manipular os dados, o programa morria no treino.\n\n### vers\u00e3o 2\n\nAp\u00f3s isso criamos outra abordagem a de utilizar as categorias, que dessa forma o programa poderia adicionar os zeros e ainda treinar de uma forma tranquila. Por\u00e9m apesar de ter um resultado melhor que o primeiro por uma grande margem e funcionar diferentemente da segunda abordagem, n\u00f3s n\u00e3o estamos satisfeitos com simplesmente analisar os itens por categoria e dar resultados iguais a todos os itens, de uma categoria, j\u00e1 que isso \u00e9 de certa forma contraproducente, j\u00e1 que \u00e9 mais uma super generaliza\u00e7\u00e3o do comportamento do que uma solu\u00e7\u00e3o de fato. Ent\u00e3o n\u00f3s fomos a terceira abordagem e o in\u00edcio da implementa\u00e7\u00e3o de xgboost.\n\n### vers\u00e3o 3\n\nDiferentemente das outras abordagens n\u00f3s tentamos ver o problema em uma vis\u00e3o temporal onde cada m\u00eas anterior se torna um atributo para o pr\u00f3ximo e esse se torna o target. Essa foi por muito tempo nosso melhor resultado em xgboost por\u00e9m ele n\u00e3o se mostrou t\u00e3o efetivo quanto o m\u00e9todo anterior para Random Forest levando o grupo a tentar diferente.\n\n### vers\u00e3o 4\n\nNesse momento n\u00f3s tentamos dar uma olhada de volta na abordagem 2 para tentarmos melhora-la e encontramos a [reduce_mem_usage]( https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage) uma fun\u00e7\u00e3o muito \u00fatil que utiliza da redu\u00e7\u00e3o de vari\u00e1vel para reduzir o consumo de memoria. Tal fun\u00e7\u00e3o resultou em uma redu\u00e7\u00e3o de quase 80% do uso de mem\u00f3ria deixando a abordagem 1 vi\u00e1vel novamente e com isso tivemos um resultado melhor.\n\n### vers\u00e3o 5\n\nAqui percebemos que ambas as abordagens que testamos se tornaram poss\u00edveis, e obtiveram bons resultados. Separadamente. Ent\u00e3o tentamos ver se ambos juntas poderiam melhorar o nosso resultado. A ideia era utilizar as categorias e a redu\u00e7\u00e3o de mem\u00f3ria para colocar todos os 0. Mas a suposi\u00e7\u00e3o se mostrou errada, o resultado se tornou pior que ambos separados. Provavelmente o algoritmo se enviesa nessa uni\u00e3o, al\u00e9m de ter se tornado mais custoso.\n\n### vers\u00e3o 6\n\nPor fim resolvemos tentar implementar novamente o vi\u00e9s temporal no algoritmo, essa vers\u00e3o \u00e9 vers\u00e3o do notebook acima, com os resultados muito melhores do que os anteriores, tanto para Random Forest, quanto para Xgboost. Nesta vers\u00e3o usamos tamb\u00e9m partes das vers\u00f5es anteriores para melhorar a predi\u00e7\u00e3o com aspectos chave de cada uma.","602a3ec4":"O dataframe **lag_train** possui as informa\u00e7\u00f5es para o treino do regressor, todos os atributos preditivos mais a coluna **item_cnt_day**, que representa o n\u00famero de vendas, informa\u00e7\u00e3o que se deseja prever com esses testes.\n\nO dataframe **test** possui os mesmos atributos preditivos que o dataframe de treino, com informa\u00e7\u00f5es do 35\u00ba m\u00eas, por\u00e9m sem a coluna **item_cnt_day** que \u00e9 o que se deseja prever.\n\nUm teste abaixo foi feito para confirmar que os dataframes possuem a mesma estrutura."}}