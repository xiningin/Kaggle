{"cell_type":{"62999428":"code","cab7e9bb":"code","6cdb284c":"code","fddd5352":"code","68bbf3ac":"code","a8dde1cb":"code","8ea82d3e":"code","ec6997d0":"code","e69e97f2":"code","91fa984d":"code","ef58d3fa":"code","bff5f1c6":"code","6c2a2de2":"code","3fb86d48":"code","f248cb9c":"markdown","f4d0460e":"markdown","4659a589":"markdown","3dad8e51":"markdown","f583b4ee":"markdown","2d99147a":"markdown","9bbf5728":"markdown","f41feac3":"markdown","5ff73d39":"markdown","5dfb0dd8":"markdown","3357ebba":"markdown","d3d4c548":"markdown","e644b2af":"markdown"},"source":{"62999428":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats # for stats \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","cab7e9bb":"data_df = pd.read_csv(\"..\/input\/heart.csv\")\ndata_df.sample(10)","6cdb284c":"def convert_to_objects(df , feature):\n    return df[feature].astype('category')\n\nfor feature in ['sex','cp','fbs','restecg','exang','slope','ca','thal','target']:\n    data_df[feature] = convert_to_objects(data_df, feature)\n    \ndata_df.info()","fddd5352":"sns.set_palette(\"Set1\")\nsns.set_style(\"whitegrid\")\ncat_features = ['sex','cp','fbs','restecg','exang','slope','ca','thal','target']\nfig,ax = plt.subplots(3,3, figsize=(18,12))\nfor row in range(3):\n    for col in range(3):\n        feature_location = row * 3 + col\n        # print(cat_features[feature_location])\n        sns.countplot(data= data_df, x = cat_features[feature_location], ax = ax[row][col])\n        ax[row][col].set_title(cat_features[feature_location].capitalize())\n        ax[row][col].set(xlabel='',ylabel='Counts')\nplt.suptitle('Categorical Features Distributions',color='b',fontsize = 20);","68bbf3ac":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nnum_features = ['trestbps','chol','thalach','oldpeak']\ndef continuous_variables_plot(num_features):  \n    fig,ax = plt.subplots(2,2, figsize=(18,12))\n    for row in range(2):\n        for col in range(2):\n            feature_location = row * 2 + col\n            # print(cat_features[feature_location])\n            sns.distplot(data_df[num_features[feature_location]], ax = ax[row][col])\n            ax[row][col].set_title(\n                '{} (skew = {} kurtosis = {})'.format(\n                    num_features[feature_location].capitalize(),\n                    data_df[num_features[feature_location]].skew(),\n                    data_df[num_features[feature_location]].kurtosis()\n                ))\n            ax[row][col].set(xlabel='')\n            #ax[row][col].annotate('actual group', xy=(100,200), xytext=(100, 300))\n    plt.suptitle('Continous Features Distributions',color='b',fontsize = 20);\n    \ncontinuous_variables_plot(num_features)","a8dde1cb":"data_df['chol'] = data_df.apply(lambda row: 420 if (row['chol'] > 420) \n                                else row['chol'],axis = 1)\ndata_df['oldpeak'] = data_df.apply(lambda row: 0.01 if (row['oldpeak'] <= 0) \n                                   else row['oldpeak'],axis = 1)\ndata_df['trestbps'] = data_df.apply(lambda row: 165 if (row['trestbps'] > 165) \n                                   else row['trestbps'],axis = 1)\n\ndata_df['xt_thalach'], t_thalach = stats.boxcox(data_df['thalach'])\ndata_df['xt_chol'], t_chol = stats.boxcox(data_df['chol'])\ndata_df['xt_oldpeak'], t_oldpeak = stats.boxcox(data_df['oldpeak'])\ndata_df['xt_trestbps'], t_trestbps = stats.boxcox(data_df['trestbps'])\n#data_df.drop(num_features,axis=1,inplace=True)","8ea82d3e":"def skew_test(feature):\n    stat, pvalue = stats.skewtest(data_df[feature])\n    if pvalue > 0.05:\n        print('{} - Not Skewed Feature  p-value -{:.4} skewness {:.4}'.format(feature,pvalue,stat))\n    else:\n        print('{} - Skewed feature : p-value{:.4} skewness - {:.4}'.format(feature,pvalue,stat))\n\nskew_test('xt_thalach')\nskew_test('xt_chol')\nskew_test('xt_trestbps')\nskew_test('xt_oldpeak')","ec6997d0":"data_df['age_bucket'] = pd.cut(data_df['age'],bins=5)\nsns.countplot(data_df['age_bucket']);\n# data_df['age'].hist();","e69e97f2":"num_features = ['xt_thalach','xt_chol','xt_trestbps','xt_oldpeak']\ndef continuous_variables_boxplot(num_features):  \n    fig,ax = plt.subplots(2,2, figsize=(18,12))\n    for row in range(2):\n        for col in range(2):\n            feature_location = row * 2 + col\n            # print(cat_features[feature_location])\n            sns.boxplot(data=data_df, y = data_df[num_features[feature_location]],x='target',\n                        ax = ax[row][col])\n            ax[row][col].set_title(num_features[feature_location].capitalize())\n            ax[row][col].set(xlabel='',ylabel='')\n            #ax[row][col].annotate('actual group', xy=(100,200), xytext=(100, 300))\n    plt.suptitle('Continous Features Boxplot',color='b',fontsize = 20);\n\ncontinuous_variables_boxplot(num_features)","91fa984d":"def feature_independent(feature):\n    chi2, p , dof, expected = stats.chi2_contingency(pd.crosstab(data_df['target'],data_df[feature]))\n    #print(\"feature : {}\".format(feature))\n    #print('Chi : {:.4}\\np-value :{:.4} \\nDOF :{}'.format(chi2,p,dof))\n    print(\n        '{} is not associated. p-value {}'.format(feature,p)) if p > 0.05 else print(\n        '{} is associated.  p-value {}'.format(feature,p))\n    print('_'*55)\n        \nfor feature in ['sex','cp','fbs','restecg','exang','slope','ca','thal']:\n    feature_independent(feature)","ef58d3fa":"age_dummies = pd.get_dummies(data_df['age_bucket'],drop_first=True,prefix=\"age\")\ncp_dummies = pd.get_dummies(data_df['cp'],drop_first=True,prefix=\"cp\")\nca_dummies = pd.get_dummies(data_df['ca'],drop_first=True,prefix=\"ca\")\nslope_dummies = pd.get_dummies(data_df.slope,drop_first=True,prefix=\"slope\")\nthal_dummies = pd.get_dummies(data_df.thal,drop_first=True,prefix=\"thal\")\nrestecg_dummies = pd.get_dummies(data_df.restecg,drop_first=True,prefix=\"restecg\")\ndata = pd.concat([age_dummies,cp_dummies,ca_dummies,slope_dummies,\n           thal_dummies,restecg_dummies,data_df[num_features],data_df['target']],axis = 1)","bff5f1c6":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nscale = preprocessing.StandardScaler()\ndata['xt_chol'] = scale.fit_transform(data['xt_chol'].values.reshape(-1, 1))\ndata['xt_thalach'] = scale.fit_transform(data['xt_thalach'].values.reshape(-1, 1))\ndata['xt_trestbps'] = scale.fit_transform(data['xt_trestbps'].values.reshape(-1, 1))\ndata['xt_oldpeak'] = scale.fit_transform(data['xt_oldpeak'].values.reshape(-1, 1))\n\nX = data.iloc[:,0:-1]\ny = data['target']\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state = 42)","6c2a2de2":"def plot_roc_curve(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([0, 1, 0, 1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    \ndef plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n    plt.xlabel(\"Threshold\")\n    plt.legend(loc=\"upper left\")\n    plt.ylim([0, 1])","3fb86d48":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix,classification_report,roc_auc_score,roc_curve,recall_score, precision_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import precision_recall_curve\n\nlr = LogisticRegression(solver='liblinear', random_state=42, C=25, max_iter=50)\nlr_cvs = cross_val_score(lr, X_train, y_train, cv=5)\nprint('Mean Score: {:.3}\\t Std Deviation: {:.3}'.format(lr_cvs.mean(), lr_cvs.std()))\nlr_predict_train = cross_val_predict(lr,X_train, y_train, cv=5)\nprint('Precision Score: {:.3} \\t Recall Score: {:.3f}'.format(precision_score(y_train,lr_predict_train),\n                                                         recall_score(y_train,lr_predict_train)))\n\n\n\ny_scores = cross_val_predict(lr,X= X_train,y = y_train, cv = 3,method='decision_function')\nprecisions, recalls, thresholds = precision_recall_curve(y_train, y_scores)\nplot_precision_recall_vs_threshold(precisions, recalls, thresholds)\nplt.show()\nfpr, tpr, thresholds = roc_curve(y_train, y_scores)\nplot_roc_curve(fpr, tpr)\nplt.show()\n# lr.fit(X_train, y_train)\n# lr_predict = lr.predict(X_test)\n# lr_cm = confusion_matrix(y_test,lr_predict )\n# print(classification_report(y_test,lr_predict))\n# print('ROC AUC Score :{:.3}'.format(roc_auc_score(y_test,lr_predict)))","f248cb9c":"Seems fps is not associated with the target.\n\n#### One Hot Coding","f4d0460e":">As per Wikipedia, \"In probability theory and statistics, skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. The skewness value can be positive or negative, or undefined\"\n\n> Continuous variables are skewed. Let them normalized first\n\n> **Left Skewed** - thalach\n\n> **Right Skewed** - trestbps,chol,oldpeak\n\n#### Fix Skewness of the data","4659a589":"> **Findings**:\n> * Dataset has almost twice male observations\n> * There are 4 chest pain type in dataset. Chest pain type 0 is most common which is followed by Chest pain type 2. The Chest Pain type 3 is least common type.\n> * The Fbs is fasting blood sugar. There is majority of Non diabetics patients. Which is almost 5 fold.\n> * The restecg is resting electrocardiographic results. There are almost equal number of category 0 and 1. and there are few observation for type 2\n> * The exang exercise induced angina (1 = yes; 0 = no). There are more observation with exang value 0\n> * The slope of the peak exercise ST segment shows there are three categories where 1 and 2 have almost same number of observation and type 0 has very low numbers.\n> * ca number of major vessels (0-3) colored by flourosopy. vessels type 0 is most common.\n> * thal 3 = normal; 6 = fixed defect; 7 = reversable defect. This looks bit confusing.As per data description there are 3 category 3, 6, and 7 but dataset has 0 to 3. The 2 is most common type.\n> * target1 or 0. There are almost equal number of observation for both the classes. it can be considered as balance dataset\n","3dad8e51":"## Read Dataset\nProvided dataset contains 13 featutes that will help to predict target value. Lets read the data and see the quality of provided data.","f583b4ee":"Good thing there is no missing value. It is a small dataset with 303 observation. After converting ordinal features to categorical features there are 8 categorical features and 5 countinous features. Age is showing as countinous features but it require more understanding before to consider it continous or ordinal features.\n### Univariable Analysis","2d99147a":"#### Bucketing Age feature","9bbf5728":"### Bi-variable Analysis","f41feac3":"Seems threstbps does not have much difference.This can be considered for removal in later stage\n\n#### Features Association Test","5ff73d39":"#### Standard Scale Preprocessing &  train and test dataset split","5dfb0dd8":"## Heart Disease Classification\n\nThis is a binary classification problem. Target Variable contains two values 1 and 0. Our target will be to achive high accuracy without loosing the recall rate. Detail of each provided features are as:\n* **age** in years\n* **sex**(1 = male; 0 = female)\n* **cp** chest pain type\n* **trestbps** resting blood pressure (in mm Hg on admission to the hospital)\n* **cholserum** cholestoral in mg\/dl\n* **fbs**(fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n* **restecg** resting electrocardiographic results\n* **thalach** maximum heart rate achieved\n* **exang** exercise induced angina (1 = yes; 0 = no)\n* **oldpeak** ST depression induced by exercise relative to rest\n* **slope** the slope of the peak exercise ST segment\n* **ca** number of major vessels (0-3) colored by flourosopy\n* **thal** 3 = normal; 6 = fixed defect; 7 = reversable defect\n* **target 1 or 0**","3357ebba":"Why the precision curve is not smooth? Why it suddenly dropping after threshold 3.5. On the other hand recall in smoothly descresing with threshold value increasing. Since our purpose to get the balance model which is good in precision and recall both.Threshold value 0 looks good which is giving approx 81% recall and 83% precision.","d3d4c548":"Lets fix the datatype for ordinal features. This will half the memory uses and will help in data analysis and building model.","e644b2af":"#### Post fix Skewness test"}}