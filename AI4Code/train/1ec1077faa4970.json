{"cell_type":{"14e2b6df":"code","9a8c8c0a":"code","48ba4fac":"code","5e154e53":"code","548932f1":"code","3f53bfb9":"code","073170c1":"code","7454d88f":"code","f0d3b9a4":"code","1a151a86":"code","ef3d9a8e":"code","44801ff0":"code","6db14a2f":"code","cb2d2e44":"code","35eaee08":"code","ffd78d2d":"code","9d853a1a":"code","8676b40b":"code","c9e05100":"markdown","e957ace7":"markdown","35be8834":"markdown","670f6f5f":"markdown","c6aab2b3":"markdown","b73b3f26":"markdown","7bd3a246":"markdown","582dff99":"markdown","7748c0ea":"markdown","a5d5efdd":"markdown","b8111139":"markdown","9410f6c2":"markdown","bd5797c6":"markdown"},"source":{"14e2b6df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Pretty display for notebooks\n%matplotlib inline\n\n# Load the Santander dataset\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\nsubmission_data = pd.read_csv('..\/input\/sample_submission.csv')\n","9a8c8c0a":"#Size of training data\ntrain_data.shape","48ba4fac":"train_data['target'].head(5)\n","5e154e53":"train_data.describe()","548932f1":"X_train, X_val, y_train, y_val = train_test_split(feature_train_data, train_data['target'], test_size = 0.20, random_state = 25)","3f53bfb9":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\n","073170c1":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","7454d88f":"random_state = 42\n\nlgb_params = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : -1,\n    \"num_leaves\" : 13,\n    \"learning_rate\" : 0.01,\n    \"bagging_freq\": 5,\n    \"bagging_fraction\" : 0.4,\n    \"feature_fraction\" : 0.05,\n    \"min_data_in_leaf\": 80,\n    \"min_sum_heassian_in_leaf\": 10,\n    \"tree_learner\": \"serial\",\n    \"boost_from_average\": \"false\",\n    #\"lambda_l1\" : 5,\n    #\"lambda_l2\" : 5,\n    \"bagging_seed\" : random_state,\n    \"verbosity\" : 1,\n    \"seed\": random_state\n}","f0d3b9a4":"df_train.head()","1a151a86":"skf = StratifiedKFold(n_splits = 5, shuffle=True, random_state=random_state)\nskf.get_n_splits(X_train, y_train)","ef3d9a8e":"features = [col for col in df_train.columns if col not in ['target', 'ID_code']]\nX_test = df_test[features].values\nfeature_importance_df = pd.DataFrame()\npredictions = df_test[['ID_code']]\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(df_train, df_train['target'])):\n    print(\"FOLD: \", fold, \"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, y_train = df_train.iloc[trn_idx][features], df_train.iloc[trn_idx]['target']\n    X_valid, y_valid = df_train.iloc[val_idx][features], df_train.iloc[val_idx]['target']\n    \n    N = 5\n    p_valid = 0\n    yp = 0\n    \n    for i in range(N):\n        \n        trn_data = lgb.Dataset(X_train, label = y_train)\n        val_data = lgb.Dataset(X_valid, label = y_valid)\n        \n        lgb_clf = lgb.train(lgb_params,\n                   trn_data,\n                   100000,\n                   valid_sets = [trn_data, val_data],\n                    verbose_eval = 5000,\n                    early_stopping_rounds = 3000)\n        \n        p_valid += lgb_clf.predict(X_valid)\n        yp += lgb_clf.predict(X_test)\n    \n    \n    #Get importance of the fold when predicting test set\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    predictions['fold{}'.format(fold+1)] = yp\/N\n","44801ff0":"X_train.head()","6db14a2f":"predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1)\npredictions.to_csv('lgb_all_predictions.csv', index=None)\nsub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\nsub_df[\"target\"] = predictions['target']\nsub_df.to_csv(\"lgb_submission.csv\", index=False)\noof.to_csv('lgb_oof.csv', index=False)","cb2d2e44":"dfa = pd.DataFrame(np.random.randn(5, 4),\n                    columns=list('ABCD'),\n                   index=pd.date_range('20130101', periods=5))","35eaee08":"dfa","ffd78d2d":"dfa.columns","9d853a1a":"dfa.iloc[0:2][[col for col in dfa.columns if col not in ['B', 'C']] ]","8676b40b":"col for col in df_train.columns if col not in ['target', 'ID_code']","c9e05100":"### Calculate Dataset Statistics ###","e957ace7":"The provided train.csv file contains 200,000 unique rows corresponding to customer data. Given the large dataset, and the need to complete binary classification, there are many solutions to this problem: mine will involve using a deep neural network, after preprocessing the inputs by normalizing and scaling features, to classify the two target variables. After the model is trained and validated on a subset of the data from the train.csv file, I will run my trained model on the provided test set from Santander and measure the accuracy of each prediction. ","35be8834":"## Solution Statement ##","670f6f5f":"## Background Information ##","c6aab2b3":"## Data Exploration & Visualizations ##","b73b3f26":"Financial institutions, like Santander, help people and businesses prosper by providing tools and services to assess their personal financial health and to identify additional ways to help customers reach their monetary goals. In the United States, it is estimated that 40% of Americans cannot cover a $400 emergency expense1. As a result, it is imperative that financial institutions learn consumer habits to adopt new technologies to better serve their financial needs. ","7bd3a246":"Stratified KFold is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class. Especially for this problem, where we have an unbalanced binary classification issue, we need to keep this in mind.","582dff99":"Immediate Key Takeaways: The mean, standard deviation, and maximum values of the features vary widely; if we choose to implement a black-box algorithm, like neural networks, a key step will be data preprocessing, which will involve feature scaling, potentially outlier-detection and removal, and definitely normalization of these variables.","7748c0ea":"## Problem Statement ##\nSantander, a financial institution, is trying to predict the next transaction a given customer is trying to complete based on historical banking information. This is a binary classification problem where the input data contains 299 unnamed normally-distributed feature variables. The solution to this problem will be evaluated on a provided test data set by Santander. ","a5d5efdd":"### Separate Training and Validation Datasets ###","b8111139":"## Implement LGBM Algorithm ##","9410f6c2":"## Load Data ##","bd5797c6":"From the dataset statistics, we can see that the mean and standard deviatation of each var feature significantly varies. If we apply supervised learning algorithms without any feature scaling or preprocessing, the algorithm will bias to certain features over others when learning the relationship between the input features and the target classification of the customer.\n"}}