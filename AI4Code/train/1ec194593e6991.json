{"cell_type":{"1390d5a0":"code","727df8c5":"code","d823d20f":"code","14814269":"code","ee7b3245":"code","d91f5312":"code","11848eb4":"code","5f899414":"code","63d081d6":"code","4f2bb2eb":"code","6f0c1650":"code","f66a61ee":"code","69102973":"code","281cc4f0":"code","b7da35af":"code","aead9114":"code","f2cb9522":"code","369e7d66":"code","38e02f3f":"code","41f8d7e0":"code","825a6786":"code","56c0f172":"code","8e1ac2e2":"code","55912035":"code","54ea929a":"markdown","30d978eb":"markdown","806acaf8":"markdown"},"source":{"1390d5a0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\nimport optuna\nimport optuna.integration.lightgbm as lgb\nprint(\"Optuna version:  {}\".format(optuna.__version__))\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","727df8c5":"# read input files\ndf_train = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")","d823d20f":"# see EDA notebooks for why these columns are chosen\nfeature_cols = [col for col in df_train.columns if col.startswith(\"f\")] # all features\nbin_feat = [\"f22\"] + [\"f43\"] + list(df_train.columns[243:-1])           # binary features\nnum_feat = [x for x in feature_cols if x not in bin_feat]               # numeric features","14814269":"ts = time.time()\n\nprint(\"Before downcast:\")\ndisplay(df_train.info())\n\n# reduce memory size of data frames\ndf_train[num_feat] = df_train[num_feat].astype('float32')\ndf_train[bin_feat] = df_train[bin_feat].astype('uint8')\ndf_train['target'] = df_train['target'].astype('int8')\n\ndf_test[num_feat] = df_test[num_feat].astype('float32')\ndf_test[bin_feat] = df_test[bin_feat].astype('uint8')\n\nprint(\"\\nAfter downcast:\")\ndisplay(df_train.info())\n\nexecution_time = time.time() - ts\nprint(\"\\nExecution time: \" + str(round(execution_time,3)) + \"s\")","ee7b3245":"df_train.head()","d91f5312":"# dividing X, y into train and test data\nX = df_train.drop(columns=[\"id\",\"target\"])\ny = df_train.target\nX_train, X_val, y_train, y_val = train_test_split(X, y, random_state = 29, stratify=y)\nX_test = df_test.drop(columns=[\"id\"])\ndisplay(X_train.shape)","11848eb4":"dtrain = lgb.Dataset(X_train, label=y_train)\ndval = lgb.Dataset(X_val, label=y_val)","5f899414":"params = {\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\"\n    }","63d081d6":"#model = lgb.train(\n#        params, \n#        dtrain, \n#        valid_sets=[dtrain, dval], \n#        verbose_eval=100, \n#        early_stopping_rounds=100\n#    )\n#best_params = model.params\n#print(\"Best params:\", best_params)","4f2bb2eb":"#print(\"  Params: \")\n#for key, value in best_params.items():\n#    print(\"    {}: {}\".format(key, value))","6f0c1650":"#pred_val = model.predict(X_val, num_iteration=model.best_iteration)\n#roc_auc_score(y_val, pred_val)","f66a61ee":"#predictions = model.predict(X_test, num_iteration=model.best_iteration)","69102973":"import lightgbm as lgb\nprint(\"LightGBM version:  {}\".format(lgb.__version__))","281cc4f0":"lgb_params = {'objective': 'binary', \n          'metric': 'auc', \n          'num_iterations' : 12000,\n          'learning_rate' : 0.01,\n          'verbosity': -1, \n          'boosting_type': 'gbdt',  \n          'lambda_l1': 8.533875942246594, \n          'lambda_l2': 2.0533270677941314e-06, \n          'num_leaves': 13, \n          'feature_fraction': 0.4, \n          'bagging_fraction': 1.0, \n          'bagging_freq': 0, \n          'min_child_samples': 50}","b7da35af":"#dict_eval = {}","aead9114":"#ts = time.time()\n\n#model = lgb.train(        \n#        lgb_params, \n#        dtrain, \n#        valid_sets=[dtrain, dval],\n#        valid_names=['train','val'],\n        #evals_result = dict_eval,  # use this to store the auc scores, can be handy in cv loop\n#        verbose_eval=200, \n#        early_stopping_rounds=300\n#    )\n\n#execution_time = time.time() - ts\n#print(\"\\nTraining time: \" + str(round(execution_time,3)) + \"s\")","f2cb9522":"# early stopping = 300\n# learning rate 0.1, num_iter 3000 -> val's auc 0.856436 [925]\n# learning rate 0.05, num_iter 3000 -> [2247]\ttrain's auc: 0.870716\tval's auc: 0.856854\n# learning rate 0.01, num_iter 4000 -> [4000]\ttrain's auc: 0.861236\tval's auc: 0.855615\n# learning rate 0.01, num_iter 8000 -> [8000]\ttrain's auc: 0.867171\tval's auc: 0.856897\n# learning rate 0.01, num_iter 15000 -> [10712]\ttrain's auc: 0.870415\tval's auc: 0.856981\n# learning rate 0.01, num_iter 12000, val set size = 10% -> [9721]\ttrain's auc: 0.867667\tval's auc: 0.855951","369e7d66":"#lgb.plot_importance(model, figsize=(16,40))","38e02f3f":"#predictions = model.predict(X_test)","41f8d7e0":"# generate submission file \n#submission_lgbm = pd.DataFrame(data={\"id\" : sample_submission.id,\n#                                     \"target\" : predictions})\n\n#submission_lgbm.to_csv('submission_lgbm.csv', index=False)\n#submission_lgbm.head()","825a6786":"# new in version 6, I want to see how much can be gained by increasing the trees to 25000\nlgb_params = {'objective': 'binary', \n          'metric': 'auc', \n          'num_iterations' : 25000,\n          'learning_rate' : 0.01,\n          'verbosity': -1, \n          'boosting_type': 'gbdt',  \n          'lambda_l1': 8.533875942246594, \n          'lambda_l2': 2.0533270677941314e-06, \n          'num_leaves': 13, \n          'feature_fraction': 0.4, \n          'bagging_fraction': 1.0, \n          'bagging_freq': 0, \n          'min_child_samples': 50}","56c0f172":"# retrain on whole data and make a new submission file\ndtrain = lgb.Dataset(X, label=y)\nmodel = lgb.train(\n        lgb_params, \n        dtrain\n    )","8e1ac2e2":"predictions = model.predict(X_test)","55912035":"# generate submission file \nsubmission_lgbm = pd.DataFrame(data={\"id\" : sample_submission.id,\n                                     \"target\" : predictions})\n\nsubmission_lgbm.to_csv('submission_lgbm_retrain.csv', index=False)\nsubmission_lgbm.head()","54ea929a":"I'm using LightGBM (without tuner) here to run again with higher number of iterations and a lower learning rate. ","30d978eb":"# Tabular Playground Series October 2021 using Optuna's Light GBM Tuner\n\nDuring last month's competition I discovered that Optuna has a [build in solution for tuning LightGBM hyperparameters](https:\/\/optuna.readthedocs.io\/en\/stable\/reference\/generated\/optuna.integration.lightgbm.LightGBMTuner.html) in a sequential manner. This is supposed to be even faster than using Optuna and LightGBM in what [Kaggle grandmaster Kohei Osaki calls a \"naive way\"](https:\/\/medium.com\/optuna\/lightgbm-tuner-new-optuna-integration-for-hyperparameter-optimization-8b7095e99258). Instead of using the product of all hyperparameters which results in a large search space, LightGBM Tuner follows a step-wise approach. It tunes lambda_l1, lambda_l2, num_leaves, feature_fraction, bagging_fraction, bagging_freq and min_child_samples, sequentially. \n\nSo let's see how well it works!","806acaf8":"The cells below use Optuna's LightGBM Tuner."}}