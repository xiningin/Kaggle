{"cell_type":{"f0909e69":"code","2763ab62":"code","0cda6e18":"code","91e0b8ab":"code","995cee2a":"code","174ecffd":"code","b715aeb2":"code","3d0d1218":"code","290ff7d8":"code","aa518ec8":"code","17e1297c":"code","18ae6193":"code","5e7ff581":"code","0544e855":"code","fbcdc9db":"code","6423cbeb":"code","3521c141":"code","e67fd7db":"code","61791597":"code","1afe51c7":"code","feec212d":"code","c0a81d29":"code","67e2b300":"code","047ce40a":"markdown","5278e393":"markdown"},"source":{"f0909e69":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport datetime\nimport gc\nimport numpy as np\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import KFold, RepeatedKFold, GroupKFold\nfrom sklearn.utils.class_weight import compute_sample_weight\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import ADASYN\nimport category_encoders as ce\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline \n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2763ab62":"def dprint(*args, **kwargs):\n    print(\"[{}] \".format(datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\")) + \\\n        \" \".join(map(str,args)), **kwargs)\n\nid_name = 'Id'\ntarget_name = 'Target'\n\n","0cda6e18":"# Load data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n","91e0b8ab":"train['is_test'] = 0\ntest['is_test'] = 1\ndf_all = pd.concat([train, test], axis=0)\n","995cee2a":"dprint('Clean features...')\ncols = ['dependency']\nfor c in tqdm(cols):\n    x = df_all[c].values\n    strs = []\n    for i, v in enumerate(x):\n        try:\n            val = float(v)\n        except:\n            strs.append(v)\n            val = np.nan\n        x[i] = val\n    strs = np.unique(strs)\n\n    for s in strs:\n        df_all[c + '_' + s] = df_all[c].apply(lambda x: 1 if x == s else 0)\n\n    df_all[c] = x\n    df_all[c] = df_all[c].astype(float)\ndprint(\"Done.\")","174ecffd":"dprint(\"Extracting features...\")\ndef extract_features(df):\n    df['bedrooms_to_rooms'] = df['bedrooms']\/df['rooms']\n    df['rent_to_rooms'] = df['v2a1']\/df['rooms']\n    df['rent_to_bedrooms'] = df['v2a1']\/df['bedrooms']\n    df['tamhog_to_rooms'] = df['tamhog']\/df['rooms'] # tamhog - size of the household\n    df['tamhog_to_bedrooms'] = df['tamhog']\/df['bedrooms']\n    df['r4t3_to_tamhog'] = df['r4t3']\/df['tamhog'] # r4t3 - Total persons in the household\n    df['r4t3_to_rooms'] = df['r4t3']\/df['rooms'] # r4t3 - Total persons in the household\n    df['r4t3_to_bedrooms'] = df['r4t3']\/df['bedrooms']\n    df['rent_to_r4t3'] = df['v2a1']\/df['r4t3']\n    df['v2a1_to_r4t3'] = df['v2a1']\/(df['r4t3'] - df['r4t1'])\n    df['hhsize_to_rooms'] = df['hhsize']\/df['rooms']\n    df['hhsize_to_bedrooms'] = df['hhsize']\/df['bedrooms']\n    df['rent_to_hhsize'] = df['v2a1']\/df['hhsize']\n    df['qmobilephone_to_r4t3'] = df['qmobilephone']\/df['r4t3']\n    df['qmobilephone_to_v18q1'] = df['qmobilephone']\/df['v18q1']\n    \n\nextract_features(train)\nextract_features(test)\ndprint(\"Done.\")         ","b715aeb2":"from sklearn.preprocessing import LabelEncoder\n\ndef encode_data(df):\n   \n    yes_no_map = {'no': 0, 'yes': 1}\n    \n    df['dependency'] = df['dependency'].replace(yes_no_map).astype(np.float32)\n    \n    df['edjefe'] = df['edjefe'].replace(yes_no_map).astype(np.float32)\n    df['edjefa'] = df['edjefa'].replace(yes_no_map).astype(np.float32)\n    \n    df['idhogar'] = LabelEncoder().fit_transform(df['idhogar'])\n","3d0d1218":"dprint(\"Encoding Data....\")\nencode_data(train)\nencode_data(test)\ndprint(\"Done...\")","290ff7d8":"def do_features(df):\n    feats_div = [('children_fraction', 'r4t1', 'r4t3'), \n                 ('working_man_fraction', 'r4h2', 'r4t3'),\n                 ('all_man_fraction', 'r4h3', 'r4t3'),\n                 ('human_density', 'tamviv', 'rooms'),\n                 ('human_bed_density', 'tamviv', 'bedrooms'),\n                 ('rent_per_person', 'v2a1', 'r4t3'),\n                 ('rent_per_room', 'v2a1', 'rooms'),\n                 ('mobile_density', 'qmobilephone', 'r4t3'),\n                 ('tablet_density', 'v18q1', 'r4t3'),\n                 ('mobile_adult_density', 'qmobilephone', 'r4t2'),\n                 ('tablet_adult_density', 'v18q1', 'r4t2'),\n                 #('', '', ''),\n                ]\n    \n    feats_sub = [('people_not_living', 'tamhog', 'tamviv'),\n                 ('people_weird_stat', 'tamhog', 'r4t3')]\n\n    for f_new, f1, f2 in feats_div:\n        df['fe_' + f_new] = (df[f1] \/ df[f2]).astype(np.float32)       \n    for f_new, f1, f2 in feats_sub:\n        df['fe_' + f_new] = (df[f1] - df[f2]).astype(np.float32)\n    \n    # aggregation rules over household\n    aggs_num = {'age': ['min', 'max', 'mean'],\n                'escolari': ['min', 'max', 'mean']\n               }\n    aggs_cat = {'dis': ['mean']}\n    for s_ in ['estadocivil', 'parentesco', 'instlevel']:\n        for f_ in [f_ for f_ in df.columns if f_.startswith(s_)]:\n            aggs_cat[f_] = ['mean', 'count']\n    # aggregation over household\n    for name_, df_ in [('18', df.query('age >= 18'))]:\n        df_agg = df_.groupby('idhogar').agg({**aggs_num, **aggs_cat}).astype(np.float32)\n        df_agg.columns = pd.Index(['agg' + name_ + '_' + e[0] + \"_\" + e[1].upper() for e in df_agg.columns.tolist()])\n        df = df.join(df_agg, how='left', on='idhogar')\n        del df_agg\n    # do something advanced above...\n    \n    # Drop SQB variables, as they are just squres of other vars \n    df.drop([f_ for f_ in df.columns if f_.startswith('SQB') or f_ == 'agesq'], axis=1, inplace=True)\n    # Drop id's\n    df.drop(['Id', 'idhogar'], axis=1, inplace=True)\n    # Drop repeated columns\n    df.drop(['hhsize', 'female', 'area2'], axis=1, inplace=True)\n    return df\n    ","aa518ec8":"dprint(\"Do_feature Engineering....\")\ntrain = do_features(train)\ntest = do_features(test)\ndprint(\"Done....\")","17e1297c":"dprint(\"Fill Na value....\")\ntrain = train.fillna(0)\ntest = test.fillna(0)\ndprint(\"Done....\")","18ae6193":"train.shape,test.shape","5e7ff581":"cols_to_drop = [\n    id_name, \n    target_name,\n]\nX = train.drop(cols_to_drop, axis=1, errors='ignore')\ny = train[target_name].values\n","0544e855":"y = pd.get_dummies(y).values","fbcdc9db":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\nX_train.shape,y_train.shape,X_test.shape,y_test.shape","6423cbeb":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.optimizers import Adam\n\nfrom keras import regularizers\ninput_dim = len(X_train.columns) \ninput_dim","3521c141":"input_dim = len(X_train.columns) \nbatch_size=32\nmodel = Sequential()\n\n# first input layer with first hidden layer in a single statement\nmodel.add( Dense(120, input_shape=(input_dim,), activation='tanh') )\n# 10 is the size(no. of neurons) of first hidden layer, 4 is the no. of features in the input layer\n# input_shape=(4,)  can also be written as   input_dim=4\n\n# second hiden layer\nmodel.add(Dense(64,activation='relu')) # 8 = no. of neurons in second hidden layer\n\n# third hiden layer\nmodel.add(Dense(32,activation='relu')) # 6 = no. of neurons in third hidden layer\n\n# ouput layer\nmodel.add(Dense(4,activation='softmax')) # 3 = no. of neurons in output layer as three categories of labels are there\n\n# compile method receives three arguments: \"an optimizer\", \"a loss function\" and \"a list of metrics\"\nmodel.compile(Adam(lr=0.02),'mse', ['accuracy'])\n# we use \"binary_crossentropy\" for binary classification problems and\n# \"categorical_crossentropy\" for multiclass classification problems\n# the compile statement can also be written as:-\n# model.compile(optimizer=Adam(lr=0.04), loss='categorical_crossentropy',metrics=['accuracy'])\n# we can give more than one metrics like ['accuracy', 'mae', 'mape']\n\nmodel.summary()\nmodel.fit(X_train, y_train, steps_per_epoch=850 ,epochs = 10)\n\n","e67fd7db":"scores = model.evaluate(X_test, y_test)\nprint(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))","61791597":"predictions = model.predict_classes(test)","1afe51c7":"list(set(predictions))","feec212d":"sub = pd.read_csv(\"..\/input\/sample_submission.csv\")","c0a81d29":"sub['Target'] = predictions","67e2b300":"sub.to_csv(\"keras.csv\", index= False)","047ce40a":"## Road Map of Clean Data \/ Feature Engineering \/ Encoding \/ Remove Null Value\n1.  Load Data.....\n1.  Clean features...\n1.  Extracting features...\n1.  Encoding Data....\n1.  Fill NA value.....\n1.  Prepared Model.....\n1.  Predict Value on test Dataset......\n1.  Submit your result .......","5278e393":"## Challenge\n* The Inter-American Development Bank is asking the Kaggle community for help with income qualification for some of the world's poorest families. Are you up for the challenge?\n\n* Here's the backstory: Many social programs have a hard time making sure the right people are given enough aid. It\u2019s especially tricky when a program focuses on the poorest segment of the population. The world\u2019s poorest typically can\u2019t provide the necessary income and expense records to prove that they qualify."}}