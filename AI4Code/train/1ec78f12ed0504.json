{"cell_type":{"cdf18c47":"code","397bb967":"code","781796b3":"code","0d4a4ed9":"code","1279dd7d":"code","89c10c72":"code","ebb73e08":"code","8dc5f66a":"code","d04bbb28":"code","3645e73e":"code","acc3bc3b":"code","ec109ffc":"code","f9a1f49d":"code","604d8f0e":"code","b9bfd59a":"code","c3a17af9":"code","9b1160b5":"code","0f73c65f":"code","f4d2e287":"code","e3e43a7c":"code","ce54973c":"code","0e2746c9":"code","658c7050":"code","6b9abe76":"code","12a022f3":"code","4ce26b9d":"code","91452e3d":"code","3e44fa61":"code","41aec3be":"code","09ef34b9":"code","931a9d7d":"code","c316a65c":"code","a58a20da":"code","80046af0":"code","ce0c949b":"code","78a95876":"code","7ac6d2ad":"code","d72611fe":"code","dfde24e7":"code","54d228f2":"code","e2499f64":"code","6dbc73d9":"code","6cc25841":"code","fcf26f47":"code","1ddeee8b":"code","27f1cc3b":"code","da122bb1":"code","cac6d5c5":"code","22b9a770":"code","8ce09cb6":"code","ab05a814":"code","401c8101":"code","6cde0bb0":"code","349223ea":"code","b214ef53":"code","454cb1ed":"code","6ffc69bc":"code","3a9b646e":"code","634b9759":"code","ff0ecbef":"code","1dfb290e":"code","e83ab408":"code","f19926e0":"code","24042805":"code","e69dbeb0":"code","3f4fd362":"code","4f36359d":"code","6e31b828":"code","c6fb7d66":"code","4b12682c":"code","4f991c0b":"code","43353760":"code","7501b083":"code","b533cfd7":"code","3ab19252":"code","f974e83d":"code","fef0c281":"code","342d1143":"code","e4a56c6f":"code","41176a87":"code","962dffcb":"code","a3be7958":"code","521a6fdd":"code","2751b3f0":"code","31b7955e":"code","44011dfc":"code","3014511b":"code","822d01bb":"code","e2991913":"code","b0278b71":"code","798a3df6":"code","0b81b6f5":"code","2ea3b017":"code","4e42dc3c":"code","14b2a656":"code","e68126b7":"code","587a0320":"code","cd066f36":"code","4ea1fa6f":"code","0eb948db":"code","485f58d1":"code","af42e193":"code","bd3bf4b6":"code","b1249292":"code","f8c63bc5":"code","9684180b":"code","9e439adf":"code","eb776d1e":"code","b3334e35":"code","829c11e5":"code","83aa0277":"code","760a1fbd":"code","f8747160":"code","b52bda1b":"code","effea39d":"code","3cbaac2c":"code","7a7eec09":"code","833ba771":"code","8464154c":"code","7b9e3b8c":"code","d189bc75":"code","d0fab92d":"code","838a8795":"code","0caadafc":"code","03a94528":"code","6f7040ea":"code","df88251d":"code","022c92ef":"code","4cc00735":"markdown","27ec5415":"markdown","9714455f":"markdown","55e86d95":"markdown","213bf080":"markdown","4e343b8e":"markdown","0b93219b":"markdown","29dcb14b":"markdown","15fbf30e":"markdown","185875d1":"markdown","24ecfb77":"markdown","95be8936":"markdown","ad20c9d1":"markdown","fb048375":"markdown","6d734181":"markdown","6342f247":"markdown","91f05d5a":"markdown","8af3fb5f":"markdown","00f18e46":"markdown","159b4d43":"markdown","13be86bd":"markdown","87957039":"markdown","396605ff":"markdown"},"source":{"cdf18c47":"# data analysis libraries:\nimport numpy as np\nimport pandas as pd\n\n# data visualization libraries:\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# to ignore warnings:\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all columns:\npd.set_option('display.max_columns', None)\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV","397bb967":"# Going to use these 5 base models for the stacking\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)","781796b3":"from sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\n","0d4a4ed9":"import seaborn as sns","1279dd7d":"import pandas as pd","89c10c72":"import numpy as np","ebb73e08":"## train = pd.read_csv(\"C:\/Users\/DELL\/Desktop\/payton klasor\/lab\/train.csv\")","8dc5f66a":"# Read train and test data with pd.read_csv():\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","d04bbb28":"# copy data in order to avoid any change in the original:\ntrain = train_data.copy()\ntest = test_data.copy()","3645e73e":"train.head()","acc3bc3b":"test.head()","ec109ffc":"def targetSummaryWithCat(Parch, train, number_of_classes = 10):\n   \n    import pandas as pd\n    target_name = train[Pclass].name\n\n    for var in df:\n        if var != Parch:\n            if len(list(df[var].unique())) <= number_of_classes:\n                    print(pd.DataFrame({\"Pclass\": df.groupby(var)[Pclass].mean()}), end = \"\\n\\n\\n\")","f9a1f49d":"train.groupby(\"Parch\").aggregate([min, np.median, max])","604d8f0e":"train.isnull().sum()","b9bfd59a":"test.isnull().sum()","c3a17af9":"g = sns.factorplot(y=\"Age\",x=\"Sex\",data=train,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=train,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Parch\", data=train,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"SibSp\", data=train,kind=\"box\")","9b1160b5":"train[\"Age\"].fillna(train[\"Age\"].median(), inplace = True)","0f73c65f":"test[\"Age\"].fillna(test[\"Age\"].median(), inplace = True)","f4d2e287":"full_data = [train, test]","e3e43a7c":"train['Name_length'] = train['Name'].apply(len)\ntest['Name_length'] = test['Name'].apply(len)\n# Create CabinBool variable which states if someone has a Cabin data or not:\n\ntrain[\"CabinBool\"] = (train[\"Cabin\"].notnull().astype('int'))\ntest[\"CabinBool\"] = (test[\"Cabin\"].notnull().astype('int'))\n\ntrain = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)","ce54973c":"# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","0e2746c9":"# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","658c7050":"for dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1","6b9abe76":"# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')","12a022f3":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"","4ce26b9d":"train.head().T","91452e3d":"train.dtypes","3e44fa61":"train.isnull().sum()","41aec3be":"test.isnull().sum()","09ef34b9":"train[\"Age\"].fillna(train[\"Age\"].median(), inplace = True)","931a9d7d":"train[\"Age\"].fillna(train[\"Age\"].median(), inplace = True)","c316a65c":"train.isnull().sum()","a58a20da":"train['Pclass'].value_counts()","80046af0":"train['Sex'].value_counts()","ce0c949b":"train['SibSp'].value_counts()","78a95876":"train['Parch'].value_counts()","7ac6d2ad":"train['Ticket'].value_counts()","d72611fe":"train['Embarked'].value_counts()","dfde24e7":"sns.catplot(x = \"Sex\", y = \"Age\", hue= \"Survived\",data = train);","54d228f2":"sns.barplot(x = 'Pclass', y = 'Survived', data = train);","e2499f64":"sns.barplot(x = 'SibSp', y = 'Survived', data = train);","6dbc73d9":"sns.barplot(x = 'Parch', y = 'Survived', data = train);","6cc25841":"sns.barplot(x = 'Sex', y = 'Survived', data = train);","fcf26f47":"sns.catplot(y = \"Age\", kind = \"violin\", data = train);","1ddeee8b":"sns.catplot(x= \"Survived\", y = \"Sex\", kind = \"violin\", data = train);","27f1cc3b":"sns.catplot(x= \"Survived\", y = \"Age\", kind = \"violin\", data = train);","da122bb1":"sns.lmplot(x = \"Survived\", y = \"Age\", data = train);","cac6d5c5":"sns.lmplot(x = \"Survived\", y = \"Age\", hue = \"Sex\", data = train);","22b9a770":"sns.lmplot(x = \"Survived\", y = \"Age\", hue = \"Sex\", col = \"Pclass\", data = train);","8ce09cb6":"sns.pairplot(train);","ab05a814":"train.head().T","401c8101":"import seaborn as sns\nsns.jointplot(x=\"Survived\", y = \"Fare\", data = train, kind = \"reg\");","6cde0bb0":"sns.distplot(train.Fare);","349223ea":"train[\"Survived\"].value_counts()","b214ef53":"train[\"Age\"].value_counts()","454cb1ed":"train[\"Fare\"].describe()","6ffc69bc":"train.groupby(\"Survived\").aggregate([min, np.median, max])","3a9b646e":"Q1 = train['Fare'].quantile(0.25)\nQ3 = train['Fare'].quantile(0.75)\nIQR = Q3 - Q1\n\nlower_limit = Q1- 1.5*IQR\nlower_limit\n\nupper_limit = Q3 + 1.5*IQR\nupper_limit","634b9759":"# In boxplot, there are too many data higher than upper limit; we can not change all. Just repress the highest value -512- \ntrain['Fare'] = train['Fare'].replace(512.3292, 200)\ntest['Fare'] = test['Fare'].replace(512.3292, 200)","ff0ecbef":"test[\"Fare\"].fillna(test[\"Fare\"].median(), inplace = True)\ntrain[\"Fare\"].fillna(train[\"Fare\"].median(), inplace = True)","1dfb290e":"# Map Fare values into groups of numerical values:\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])","e83ab408":"# Drop Fare values:\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","f19926e0":"train.sort_values(\"FareBand\", ascending=False).head()","24042805":"train.isnull().sum()","e69dbeb0":"train.groupby(\"Survived\").aggregate([min, np.median, max])","3f4fd362":"train[train.notnull().all(axis=1)].T","4f36359d":"sns.distplot(train.Age, kde = False);","6e31b828":"sns.distplot(train.Pclass);","c6fb7d66":"test.head().T","4b12682c":"test.isnull().sum()","4f991c0b":"train.isnull().sum()","43353760":"test.isnull().sum()","7501b083":"# Map each Embarked value to a numerical value:\n\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\n\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)","b533cfd7":"train.head().T","3ab19252":"test.head().T","f974e83d":"train.isnull().sum()","fef0c281":"test.isnull().sum()","342d1143":"test.head().T","e4a56c6f":"test.isnull().sum()","41176a87":"train.isnull().sum()","962dffcb":"# We can drop the Ticket feature since it is unlikely to have useful information\ntrain = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket'], axis = 1)\n","a3be7958":"train[\"Title\"] = train[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)\ntest[\"Title\"] = test[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)","521a6fdd":"# Convert Sex values into 1-0:\n\nfrom sklearn import preprocessing\n\nlbe = preprocessing.LabelEncoder()\ntrain[\"Sex\"] = lbe.fit_transform(train[\"Sex\"])\ntest[\"Sex\"] = lbe.fit_transform(test[\"Sex\"])","2751b3f0":"train['Title'] = train['Title'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ntrain['Title'] = train['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ntrain['Title'] = train['Title'].replace('Mlle', 'Miss')\ntrain['Title'] = train['Title'].replace('Ms', 'Miss')\ntrain['Title'] = train['Title'].replace('Mme', 'Mrs')","31b7955e":"test['Title'] = test['Title'].replace(['Lady', 'Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\ntest['Title'] = test['Title'].replace(['Countess', 'Lady', 'Sir'], 'Royal')\ntest['Title'] = test['Title'].replace('Mlle', 'Miss')\ntest['Title'] = test['Title'].replace('Ms', 'Miss')\ntest['Title'] = test['Title'].replace('Mme', 'Mrs')","44011dfc":"train = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","3014511b":"train.head().T","822d01bb":"train[[\"Title\",\"Survived\"]].groupby(\"Title\").count()","e2991913":"# Map each of the title groups to a numerical value\n\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 5}\n\ntrain['Title'] = train['Title'].map(title_mapping)","b0278b71":"test['Title'] = test['Title'].map(title_mapping)","798a3df6":"bins = [0, 5, 12, 18, 24, 35, 60, np.inf]\nmylabels = ['Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = mylabels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = mylabels)","0b81b6f5":"# Map each Age value to a numerical value:\nage_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)","2ea3b017":"#dropping the Age feature for now, might change:\ntrain = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","4e42dc3c":"train.head()","14b2a656":"test.head().T","e68126b7":"def targetSummaryWithCat(target_name, df, number_of_classes = 10):\n   \n    import pandas as pd\n    target_name = train[Pclass].name\n\n    for var in df:\n        if var != target_name:\n            if len(list(df[var].unique())) <= number_of_classes:\n                    print(pd.DataFrame({\"TARGET_MEAN\": df.groupby(var)[target_name].mean()}), end = \"\\n\\n\\n\")","587a0320":"import seaborn as sns\nsns.jointplot(x=\"Survived\", y = \"AgeGroup\", data = train, kind = \"reg\");","cd066f36":"# Convert Title and Embarked into dummy variables:\n\ntrain = pd.get_dummies(train, columns = [\"Title\"])\ntrain = pd.get_dummies(train, columns = [\"Embarked\"], prefix=\"Em\")","4ea1fa6f":"train.head()","0eb948db":"test = pd.get_dummies(test, columns = [\"Title\"])\ntest = pd.get_dummies(test, columns = [\"Embarked\"], prefix=\"Em\")","485f58d1":"# Create categorical values for Pclass:\ntrain[\"Pclass\"] = train[\"Pclass\"].astype(\"category\")\ntrain = pd.get_dummies(train, columns = [\"Pclass\"],prefix=\"Pc\")","af42e193":"test[\"Pclass\"] = test[\"Pclass\"].astype(\"category\")\ntest = pd.get_dummies(test, columns = [\"Pclass\"],prefix=\"Pc\")","bd3bf4b6":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","b1249292":"train.isnull().sum()","f8c63bc5":"test.isnull().sum()","9684180b":"train.head()","9e439adf":"from sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale \nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn import neighbors\nfrom sklearn.svm import SVR","eb776d1e":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_test, y_train, y_test = train_test_split(predictors, target, test_size = 0.20, random_state = 0)","b3334e35":"x_train.shape","829c11e5":"x_test.shape","83aa0277":"from sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_test)\nacc_randomforest = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_randomforest)","760a1fbd":"from sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_test)\nacc_randomforest = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_randomforest)","f8747160":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)","b52bda1b":"xgb_params = {\n        'n_estimators': [200, 500],\n        'subsample': [0.6, 1.0],\n        'max_depth': [2,5,8],\n        'learning_rate': [0.1,0.01,0.02],\n        \"min_samples_split\": [2,5,10]}","effea39d":"xgb = GradientBoostingClassifier()\n\nxgb_cv_model = GridSearchCV(xgb, xgb_params, cv = 10, n_jobs = -1, verbose = 2)","3cbaac2c":"xgb_cv_model.fit(x_train, y_train)","7a7eec09":"xgb_cv_model.best_params_","833ba771":"xgb = GradientBoostingClassifier(learning_rate = xgb_cv_model.best_params_[\"learning_rate\"], \n                    max_depth = xgb_cv_model.best_params_[\"max_depth\"],\n                    min_samples_split = xgb_cv_model.best_params_[\"min_samples_split\"],\n                    n_estimators = xgb_cv_model.best_params_[\"n_estimators\"],\n                    subsample = xgb_cv_model.best_params_[\"subsample\"])","8464154c":"xgb_tuned =  xgb.fit(x_train,y_train)","7b9e3b8c":"y_pred = xgb_tuned.predict(x_test)\nacc_gbk = round(accuracy_score(y_pred, y_test) * 100, 2)\nprint(acc_gbk)","d189bc75":"test.head()","d0fab92d":"test","838a8795":"train.isnull().sum()","0caadafc":"test.isnull().sum()","03a94528":"ids = test['PassengerId']","6f7040ea":"predictions = xgb_tuned.predict(test.drop('PassengerId', axis=1))","df88251d":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = xgb_tuned.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","022c92ef":"output.head()","4cc00735":"Analysis and Visualization of Numeric and Categorical Variables","27ec5415":"# Variables and Their Types:\n\nSurvival: Survival -> 0 = No, 1 = Yes\n\nPclass: Ticket class -> 1 = 1st, 2 = 2nd, 3 = 3rd\n\nSex: Sex\n\nAge: Age in years\n\nSibSp:  of siblings \/ spouses aboard the Titanic\n\nParch:  of parents \/ children aboard the Titanic\n\nTicket: Ticket number\n\nFare: Passenger fare\n\nCabin: Cabin number\n\nEmbarked: Port of Embarkation -> C = Cherbourg, Q = Queenstown, S = Southampton","9714455f":"## Outlier Treatment","55e86d95":"Classes of some categorical variables","213bf080":"## Spliting the train data","4e343b8e":"Parch vs survived:","0b93219b":"SibSp vs survived:","29dcb14b":"# Data Understanding (Exploratory Data Analysis)\u00b6\n","15fbf30e":"Basic summary statistics about the numerical data","185875d1":"## Deployment","24ecfb77":"## Modeling, Evaluation and Model Tuning","95be8936":"## Random Forest","ad20c9d1":"# Titanic Survival Prediction:\n\nUse machine learning to create a model that predicts which passengers survived the Titanic shipwreck.","fb048375":"## Gradient Boosting Classifier","6d734181":"### Embarked","6342f247":"## Variable Transformation","91f05d5a":"## Name - Title","8af3fb5f":"Data Preparation","00f18e46":"Hello and Welcome to Kaggle, the online Data Science Community to learn, share, and compete. Most beginners get lost in the field, because they fall into the black box approach, using libraries and algorithms they don't understand. This tutorial will give you a 1-2-year head start over your peers, by providing a framework that teaches you how-to think like a data scientist vs what to think\/code. Not only will you be able to submit your first competition, but you\u2019ll be able to solve any problem thrown your way. I provide clear explanations, clean code, and plenty of links to resources. Please Note: This Kernel is still being improved. So check the Change Logs below for updates. Also, please be sure to upvote, fork, and comment and I'll continue to develop. Thanks, and may you have \"statistically significant\" luck!","159b4d43":"## WHAT HAPPENED IN TITANIC? DO YOU WANT TO KNOW?","13be86bd":"Sex vs survived:","87957039":"Pclass vs survived:","396605ff":"Feature Exploration, Engineering and Cleaning"}}