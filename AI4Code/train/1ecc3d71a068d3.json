{"cell_type":{"901670f9":"code","1779c90b":"code","fcaaf264":"code","b9a9392b":"code","43423f7c":"code","b8304636":"code","20cef226":"code","2b230e91":"code","0840fe62":"code","7dbd5d24":"code","cb4197ce":"code","a934efc1":"code","fa1dbf66":"code","6ae97d5b":"code","4ea96bf9":"code","e19de742":"code","aa6ce830":"code","7f983fac":"code","67a0ad1a":"code","8dea7796":"code","c28d3f6e":"code","ce4b0a82":"code","85508632":"code","33e28f94":"code","e8c67b59":"code","466ac466":"code","5a1bf706":"code","a5f50193":"code","96bacfbc":"code","fe568a70":"code","18c90c3c":"code","0d78df2c":"code","7dac45bc":"code","bbafaaed":"code","d1e6c134":"code","d9117117":"code","3ab6dba6":"code","55b2283b":"code","bddd8de7":"code","dacf07c0":"code","70969340":"code","0062448a":"code","175e88c0":"code","11a61fcf":"code","5cbcc850":"code","9fdb9585":"code","e4547e56":"code","d9e7ad7e":"code","4fb62d8c":"code","73b465f5":"code","1fced6d5":"code","4e5a49b0":"code","b3a2111e":"code","7c30f880":"code","737ea1d2":"code","a86a9399":"code","90b050cd":"code","46410af4":"code","d6c5d1a4":"code","b34a7798":"code","45e46dda":"code","4661001b":"code","ff9c081b":"code","3dea8721":"code","ddb1680d":"code","39bf808a":"code","27907faf":"code","8bc09ab3":"code","763a9375":"code","9e39ceef":"code","83130a90":"code","2ce99f44":"code","ece39a10":"code","9eae2df1":"code","09a91e05":"code","7a30600a":"code","d59db2a9":"code","18cb0ae4":"markdown","7a1b42d8":"markdown","2ad4e3d3":"markdown","b230b212":"markdown","21b14826":"markdown","2b2a349d":"markdown"},"source":{"901670f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1779c90b":"from tensorflow.python.client import device_lib\ndevice_lib.list_local_devices()","fcaaf264":"import numpy as np # linear algebra\nimport pandas as pd\nimport cv2\nimport os\nimport math\nimport imageio\nimport seaborn as sns\nfrom glob import glob\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nfrom sklearn.model_selection import train_test_split","b9a9392b":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import models\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.applications.vgg16 import VGG16\nfrom tensorflow.keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.applications import VGG16,VGG19,inception_v3\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\nfrom tensorflow.keras.layers import Input, Lambda,Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                                    Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\n                                    LSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape,\\\n                                    Conv2DTranspose, LeakyReLU, Conv1D, AveragePooling1D, MaxPooling1D\n\n\n","43423f7c":"# try:\n#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n#     print('Device:', tpu.master())\n#     tf.config.experimental_connect_to_cluster(tpu)\n#     tf.tpu.experimental.initialize_tpu_system(tpu)\n#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# except:\n#     strategy = tf.distribute.get_strategy()\n# print('Number of replicas:', strategy.num_replicas_in_sync)\n    \nprint(tf.__version__)","b8304636":"# AUTOTUNE = tf.data.experimental.AUTOTUNE\n# GCS_PATH = KaggleDatasets().get_gcs_path()\n# BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n# IMAGE_SIZE = [64, 64]\n# EPOCHS = 25","20cef226":"# filenames = tf.io.gfile.glob(str(GCS_PATH + '..\/input\/real-life-violence-situations-dataset\/Real Life Violence Dataset\/*\/*'))\n# filenames.extend(tf.io.gfile.glob(str(GCS_PATH + '..\/input\/violencedetectionsystem\/*\/*')))\n\n# train_filenames, val_filenames = train_test_split(filenames, test_size=0.2)","2b230e91":"Main_Video_Path = Path(\"..\/input\/real-life-violence-situations-dataset\/Real Life Violence Dataset\")\nVideo_Path = list(Main_Video_Path.glob(r\"*\/*.mp4\"))\nVideo_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"MP4\").astype(str)\nVideo_Labels_Series = pd.Series(Video_Labels,name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","0840fe62":"Violence_Data = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"Violence\"]\nNonViolence_Data = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"NonViolence\"]\n\nViolence_Data = Violence_Data.reset_index()\nNonViolence_Data = NonViolence_Data.reset_index()","7dbd5d24":"Violence_Data","cb4197ce":"Main_Video_Path = Path(\"..\/input\/violencedetectionsystem\")\nVideo_Path = list(Main_Video_Path.glob(r\"*\/*.mp4\"))\nVideo_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Video_Path))\nVideo_Path_Series = pd.Series(Video_Path,name=\"MP4\").astype(str)\nVideo_Labels_Series = pd.Series(Video_Labels,name=\"CATEGORY\")\nMain_MP4_Data = pd.concat([Video_Path_Series,Video_Labels_Series],axis=1)","a934efc1":"Main_MP4_Data","fa1dbf66":"Main_MP4_Data[\"CATEGORY\"].replace({'fight':'Violence','noFight':'NonViolence'}, inplace=True)\nVD = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"Violence\"]\nNVD = Main_MP4_Data[Main_MP4_Data[\"CATEGORY\"] == \"NonViolence\"]\n\n","6ae97d5b":"# VD = Violence_Data.reset_index()\n# NVD = NonViolence_Data.reset_index()\n# NND = NVD.drop(['level_0'],axis=1)\n# VD = VD.drop(['level_0'],axis=1)\nNVD","4ea96bf9":"Violence_Data = Violence_Data.append(VD,ignore_index=True, sort=False)\nNonViolence_Data = NonViolence_Data.append(NVD,ignore_index=True, sort=False)","e19de742":"Violence_Data","aa6ce830":"IMG_SIZE = 224\nBATCH_SIZE = 64\nEPOCHS = 10\n\nMAX_SEQ_LENGTH = 20\nNUM_FEATURES = 2048","7f983fac":"FPS = 30\nDELAY = int(100\/FPS)","67a0ad1a":"\n# def crop_center_square(frame):\n#     y, x = frame.shape[0:2]\n#     min_dim = min(y, x)\n#     start_x = (x \/\/ 2) - (min_dim \/\/ 2)\n#     start_y = (y \/\/ 2) - (min_dim \/\/ 2)\n#     return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n\n# def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n#     cap = cv2.VideoCapture(path)\n#     frames = []\n#     try:\n#         while True:\n#             ret, frame = cap.read()\n#             if not ret:\n#                 break\n#             frame = crop_center_square(frame)\n#             frame = cv2.resize(frame, resize)\n#             frame = frame[:, :, [2, 1, 0]]\n#             frames.append(frame)\n\n#             if len(frames) == max_frames:\n#                 break\n#     finally:\n#         cap.release()\n#     return np.array(frames)","8dea7796":"violence_frame_list = []\n\nfor file_video in Violence_Data.MP4:\n    Video_File_Path = file_video\n    \n    Video_Caption = cv2.VideoCapture(Video_File_Path)\n    Frame_Rate = Video_Caption.get(5)\n    \n    while Video_Caption.isOpened():\n        \n        Current_Frame_ID = Video_Caption.get(1)\n        \n        ret,frame = Video_Caption.read()\n        \n        if ret != True:\n            break\n            \n        if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n            Frame_Resize = cv2.resize(frame,(IMG_SIZE,IMG_SIZE))\n            violence_frame_list.append(Frame_Resize)\n            \n            \n    Video_Caption.release()","c28d3f6e":"# violence_frame_list = []\n\n# for file_video in Violence_Data.MP4:\n#     Video_File_Path = file_video\n    \n#     Video_Caption = cv2.VideoCapture(Video_File_Path)\n#     Frame_Rate = Video_Caption.get(5)\n    \n#     while Video_Caption.isOpened():\n        \n#         Current_Frame_ID = Video_Caption.get(1)\n        \n#         ret,frame = Video_Caption.read()\n        \n#         if ret != True:\n#             break\n            \n#         if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n#             Frame_Resize = cv2.resize(frame,(64,64))\n#             violence_frame_list.append(Frame_Resize)\n            \n            \n#     Video_Caption.release()","ce4b0a82":"figure,axis = plt.subplots(10,10,figsize=(20,20))\n\nfor i,ax in enumerate(axis.flat):\n    \n    Img_Pick = violence_frame_list[i]\n    \n    ax.set_xlabel(Img_Pick.shape)\n    ax.imshow(Img_Pick)\n\nplt.tight_layout()\nplt.show()","85508632":"X_4D_Violence = np.asarray(violence_frame_list)","33e28f94":"print(np.shape(X_4D_Violence))","e8c67b59":"X_4D_Violence_Labels = np.ones((6247,1))","466ac466":"print(np.shape(X_4D_Violence_Labels))","5a1bf706":"X_4D_Violence_Labels = X_4D_Violence_Labels.flatten()","a5f50193":"X_4D_Violence_Labels = X_4D_Violence_Labels.astype(int)","96bacfbc":"print(X_4D_Violence_Labels)","fe568a70":"nonviolence_frame_list = []\n\nfor file_video in NonViolence_Data.MP4:\n    Video_File_Path = file_video\n    \n    Video_Caption = cv2.VideoCapture(Video_File_Path)\n    Frame_Rate = Video_Caption.get(5)\n    \n    while Video_Caption.isOpened():\n        \n        Current_Frame_ID = Video_Caption.get(1)\n        \n        ret,frame = Video_Caption.read()\n        \n        if ret != True:\n            break\n            \n        if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n            Frame_Resize = cv2.resize(frame,(IMG_SIZE,IMG_SIZE))\n            nonviolence_frame_list.append(Frame_Resize)\n            \n            \n    Video_Caption.release()","18c90c3c":"figure,axis = plt.subplots(5,5,figsize=(12,12))\n\nfor i,ax in enumerate(axis.flat):\n    \n    Img_Pick = nonviolence_frame_list[i]\n    \n    ax.set_xlabel(Img_Pick.shape)\n    ax.imshow(Img_Pick)\n\nplt.tight_layout()\nplt.show()","0d78df2c":"X_4D_NonViolence = np.asarray(nonviolence_frame_list)","7dac45bc":"print(np.shape(X_4D_NonViolence))","bbafaaed":"X_4D_NonViolence_Labels = np.zeros((5384,1))","d1e6c134":"X_4D_NonViolence_Labels = X_4D_NonViolence_Labels.flatten()","d9117117":"X_4D_NonViolence_Labels = X_4D_NonViolence_Labels.astype(int)","3ab6dba6":"print(np.shape(X_4D_NonViolence_Labels))","55b2283b":"print(X_4D_NonViolence_Labels)","bddd8de7":"X_Train = np.concatenate((X_4D_Violence,X_4D_NonViolence),axis=0)","dacf07c0":"print(np.shape(X_Train))\n","70969340":"Y_Train = np.concatenate((X_4D_Violence_Labels,X_4D_NonViolence_Labels),axis=0)","0062448a":"print(np.shape(Y_Train))","175e88c0":"Target_X = X_Train\nLabel_X = Y_Train","11a61fcf":"xTrain,xvalid,yTrain,yvalid = train_test_split(Target_X,Label_X,train_size=0.9,random_state=42,shuffle=True)","5cbcc850":"print(xTrain.shape)\nprint(yTrain.shape)\nprint(xvalid.shape)\nprint(yvalid.shape)","9fdb9585":"# def build_model():\n#     model =VGG16(\n#         weights=\"imagenet\",\n#         include_top=False,\n#         pooling=\"avg\",\n#         input_shape=(64, 64, 3),\n#     )\n    \n    \n#     model.add(keras.Input((64, 64, 3)))\n#     model.add(preprocess_input())\n# #     inputs = keras.Input((64, 64, 3))\n# #     preprocessed = preprocess_input(inputs)\n    \n","e4547e56":"# x =VGG16(\n#         weights=\"imagenet\",\n#         include_top=False,\n#         pooling=\"avg\",\n#         input_shape=(64, 64, 3),\n#     )","d9e7ad7e":"# x.input","4fb62d8c":"# x.summary()","73b465f5":"# data_dir = \"..\/input\/real-life-violence-situations-dataset\/Real Life Violence Dataset\"\n# img_height , img_width = 64, 64\n# seq_len = 70\n \n# classes = ['Violence','NonViolence']\n","1fced6d5":"# cnn_out","4e5a49b0":"# video = Input(shape=(224,224,3))\n# cnn_base = VGG16(input_shape=(224,224,3),\n#                  weights=\"imagenet\",\n#                  include_top=False)\n# cnn_out = GlobalAveragePooling2D()(cnn_base.output)\n# cnn = Model(cnn_base.input,cnn_out)\n# cnn.trainable = False\n# encoded_frames = TimeDistributed(cnn)(video)\n# encoded_sequence = Bidirectional(LSTM(256,return_sequences=True,\n#                                   dropout=0.5,\n#                                   recurrent_dropout=0.5))(encoded_frames)\n# encoded = LSTM(256)(encoded_sequence)\n# hidden_layer = Dense(128,\"relu\")(encoded)\n# outputs = Dense(2, activation=\"sigmoid\")(hidden_layer)\n# model = Model([video], outputs)","b3a2111e":"video = Input(shape=(224,224,3))\ncnn_base = VGG16(input_shape=(224,224,3),\n                 weights=\"imagenet\",\n                 include_top=False)\n# cnn_out = GlobalAveragePooling2D()(cnn_base.output)\ncnn_out = cnn_base.output\ncnn = Model(cnn_base.input,cnn_out)\ncnn.trainable = False\n\nmodel = keras.Sequential()\nmodel.add(keras.Input(shape=(224,224,3)))\nmodel.add(cnn)\nmodel.add(TimeDistributed(Flatten()))\nmodel.add(Bidirectional(LSTM(256,return_sequences=True,\n                                  dropout=0.5,\n                                  recurrent_dropout=0.5)))\nmodel.add(LSTM(256))\nmodel.add(Dense(128,\"relu\"))\nmodel.add(Dense(1, activation=\"sigmoid\"))\n\n","7c30f880":"# for layers in x.layers:\n#     layers.trainable = False\n# x.output","737ea1d2":"model.summary()","a86a9399":"Callback_Stop_Early = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode='auto')\n\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n\nCNN_model = model.fit(xTrain,yTrain,\n                      validation_data=(xvalid,yvalid),\n                      callbacks=[Callback_Stop_Early],\n                      epochs=50)","90b050cd":"model.save(filepath='mymodel_vgg16i.hdf5')","46410af4":"def Predict(vediopath):\n    frame_list = []\n    Video_Caption = cv2.VideoCapture(vediopath)\n    Frame_Rate = Video_Caption.get(5)\n    \n    while Video_Caption.isOpened():\n        \n        Current_Frame_ID = Video_Caption.get(1)\n        \n        ret,frame = Video_Caption.read()\n        \n        if ret != True:\n            break\n            \n        if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n            Frame_Resize = cv2.resize(frame,(224,224))\n            frame_list.append(Frame_Resize)\n            \n            \n    Video_Caption.release()\n    \n    X_4D = np.asarray(frame_list)\n    \n    print((model.predict(X_4D)))\n    ","d6c5d1a4":"Predict(\"..\/input\/tempfile\/production ID_4821746.mp4\")","b34a7798":"# cnn = Model(cnn_base.input,cnn_base.output)\n# cnn.trainable = False\n# encoded_frames = TimeDistributed(cnn)(video)\n# encoded_sequence = LSTM(256)(encoded_frames)\n# hidden_layer = Dense(output_dim=1024, activation=\"relu\")(encoded_sequence)\n# outputs = Dense(output_dim=classes, activation=\"softmax\")(hidden_layer)\n# model = Model([video], outputs)","45e46dda":"\n\n# Model = keras.Sequential()\n# # Model.add(keras.Input((64, 64, 3)))\n# # model.add(preprocess_input)\n# Model.add(x)\n# Model.add(TimeDistributed(Flatten()))\n# Model.add(Bidirectional(LSTM(64,\n#                                   return_sequences=True,\n#                                   dropout=0.5,\n#                                   recurrent_dropout=0.5)))\n# Model.add(Bidirectional(LSTM(64,\n#                                   return_sequences=True,\n#                                   dropout=0.5,\n#                                   recurrent_dropout=0.5)))\n\n# #\n# Model.add(Flatten())\n# Model.add(Dense(128,activation=\"relu\"))\n# Model.add(Dropout(0.5))\n# Model.add(Dense(1,activation=\"sigmoid\"))\n# model.summary()","4661001b":"# feature_extractor = build_feature_extractor()","ff9c081b":"# train_datagen = ImageDataGenerator(\n#     preprocessing_function=preprocess_input,\n#     rotation_range=40,\n#     width_shift_range=0.2,\n#     height_shift_range=0.2,\n#     shear_range=0.2,\n#     zoom_range=0.2,\n#     horizontal_flip=True,\n#     fill_mode='nearest')","3dea8721":"# test_datagen = ImageDataGenerator(\n#     preprocessing_function=preprocess_input,\n#     rotation_range=40,\n#     width_shift_range=0.2,\n#     height_shift_range=0.2,\n#     shear_range=0.2,\n#     zoom_range=0.2,\n#     horizontal_flip=True,\n#     fill_mode='nearest')","ddb1680d":"# Model = Sequential()\n\n# Model.add(SeparableConv2D(12,(3,3),activation=\"relu\",\n#                  input_shape=(64,64,3)))\n# Model.add(BatchNormalization())\n# Model.add(MaxPooling2D((2,2)))\n\n# #\n# Model.add(SeparableConv2D(24,(3,3),\n#                  activation=\"relu\",padding=\"same\"))\n# Model.add(Dropout(0.2))\n# Model.add(MaxPooling2D((2,2)))\n\n\n# #\n# Model.add(TimeDistributed(Flatten()))\n# Model.add(Bidirectional(LSTM(64,\n#                                   return_sequences=True,\n#                                   dropout=0.5,\n#                                   recurrent_dropout=0.5)))\n# Model.add(Bidirectional(LSTM(64,\n#                                   return_sequences=True,\n#                                   dropout=0.5,\n#                                   recurrent_dropout=0.5)))\n\n# #\n# Model.add(Flatten())\n# Model.add(Dense(128,activation=\"relu\"))\n# Model.add(Dropout(0.5))\n# Model.add(Dense(1,activation=\"sigmoid\"))","39bf808a":"# print(Model.summary())","27907faf":"# Callback_Stop_Early = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3)\n\n# Model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n\n# CNN_Model = Model.fit(xTrain,yTrain,\n#                       validation_data=(xvalid,yvalid),\n#                       callbacks=Callback_Stop_Early,\n#                       epochs=50)","8bc09ab3":"# Model.save('mymodel1.hdf5')","763a9375":"# def Predict(vediopath):\n#     frame_list = []\n#     Video_Caption = cv2.VideoCapture(vediopath)\n#     Frame_Rate = Video_Caption.get(5)\n    \n#     while Video_Caption.isOpened():\n        \n#         Current_Frame_ID = Video_Caption.get(1)\n        \n#         ret,frame = Video_Caption.read()\n        \n#         if ret != True:\n#             break\n            \n#         if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n#             Frame_Resize = cv2.resize(frame,(64,64))\n#             frame_list.append(Frame_Resize)\n            \n            \n#     Video_Caption.release()\n    \n#     X_4D = np.asarray(frame_list)\n    \n#     print(np.mean(Model.predict(X_4D)))\n    ","9e39ceef":"Predict(\"..\/input\/violencedetectionsystem\/noFight\/nofi002.mp4\")","83130a90":"def getp():\n    frame_list = []\n    \n    Frame_Rate = Video_Caption.get(5)\n    \n    while(True):\n        \n        \n        vid = cv2.VideoCapture(0)\n        ret,frame = Video_Caption.read()\n        \n        if ret != True:\n            break\n            \n        if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n            Frame_Resize = cv2.resize(frame,(64,64))\n            frame_list.append(Frame_Resize)\n            \n            \n    Video_Caption.release()\n    \n    X_4D = np.asarray(frame_list)\n    \n    print(np.mean(Model.predict(X_4D)))\n    ","2ce99f44":"# from tensorflow.keras.models import load_model","ece39a10":"# loaded_model = load_model('..\/input\/tempfile\/mymodel1.hdf5')","9eae2df1":"# Model = loaded_model","09a91e05":"def Predict(vediopath):\n    frame_list = []\n    Video_Caption = cv2.VideoCapture(vediopath)\n    Frame_Rate = Video_Caption.get(5)\n    \n    while Video_Caption.isOpened():\n        \n        Current_Frame_ID = Video_Caption.get(1)\n        \n        ret,frame = Video_Caption.read()\n        \n        if ret != True:\n            break\n            \n        if Current_Frame_ID % math.floor(Frame_Rate) == 0:\n            Frame_Resize = cv2.resize(frame,(224,224))\n            frame_list.append(Frame_Resize)\n            \n            \n    Video_Caption.release()\n    \n    X_4D = np.asarray(frame_list)\n    \n    res = np.mean(model.predict(X_4D))\n    \n    if res >= 0.5:\n        print(\"Vedio is violent with {} % Confidence\".format((res*100)))\n    else:\n        print(\"Vedio is Non-violent with {} % Confidence\".format(100-(res*100)))\n    ","7a30600a":"Predict(\"..\/input\/tempfile\/production ID_4821746.mp4\")","d59db2a9":"Predict(\"..\/input\/tempfile\/production ID_4824122.mp4\")","18cb0ae4":"## Framing","7a1b42d8":"# **Importing Libraries**","2ad4e3d3":"## Train validation split","b230b212":"# Vedio Preprocessing","21b14826":"# **Load Model**","2b2a349d":"# **Model Building**"}}