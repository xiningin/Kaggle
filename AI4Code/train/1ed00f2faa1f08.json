{"cell_type":{"0e84b97b":"code","3b8a38b9":"code","a3520125":"code","c246a144":"code","632ac57d":"code","f9e81f87":"code","f59e94e2":"code","98f4b465":"code","c5fa716b":"code","3e86d1eb":"code","bb07c8b3":"code","d6aa579d":"code","ab66a3e5":"code","df8726ff":"code","b9398089":"code","7b4eaaa7":"code","c99a21b7":"code","5db8541d":"code","284a1823":"code","7af5d30f":"code","bc5a84b2":"code","4d9bb18b":"code","0ff08b92":"code","ec5ffe41":"code","b4a71156":"code","f08b21c5":"markdown","db64d1a3":"markdown","ceb0fe59":"markdown"},"source":{"0e84b97b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b8a38b9":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder","a3520125":"data = pd.read_csv(\"..\/input\/cleaneddata\/Cleaned-Data.csv\",sep=',',header=0)\ndata.head()","c246a144":"data['severity']= data['Severity_Mild'].map(str)+data['Severity_Moderate'].map(str)+data['Severity_None'].map(str)+data['Severity_Severe'].map(str)\nle_severity = LabelEncoder()\ndata['l_severity'] = le_severity.fit_transform(data['severity']) ","632ac57d":"inputs = data.drop(['Severity_Mild','Severity_Moderate','Severity_None','Severity_Severe','Country','severity','l_severity'], axis='columns')\ntarget = data['l_severity']","f9e81f87":"target.head()","f59e94e2":"X_train, X_test, y_train, y_test = train_test_split(inputs, target, test_size = 0.2, random_state = 42)","98f4b465":"tree = DecisionTreeClassifier(criterion='entropy',random_state=42)\ntree.fit(X_train, y_train)","c5fa716b":"y_pred=tree.predict(X_test)\naccuracy_score(y_test, y_pred)","3e86d1eb":"tree.predict([[1,1,1,1,1,0,0,1,0,1,1,0,1,0,1,1,0,0,0,1,0,0]])","bb07c8b3":"tree.predict([[1,1,1,1,1,0,0,1,0,1,1,0,1,0,1,1,0,0,0,0,1,0]])","d6aa579d":"from sklearn.ensemble import RandomForestClassifier","ab66a3e5":"rfClassifier = RandomForestClassifier(n_estimators=100, criterion ='entropy', max_depth = 8, max_features = 6, min_samples_leaf = 1, random_state = 42, n_jobs=-1)\nrfClassifier.fit(X_train, y_train)","df8726ff":"y_pred=rfClassifier.predict(X_test)\naccuracy_score(y_test, y_pred)","b9398089":"rfClassifier.predict([[1,1,1,1,1,0,0,1,0,1,1,0,1,0,1,1,0,0,0,1,0,0]])","7b4eaaa7":"rfClassifier.predict([[1,1,1,1,1,0,0,1,0,1,1,0,1,0,1,1,0,0,0,0,1,0]])","c99a21b7":"from sklearn.model_selection import StratifiedKFold","5db8541d":"skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=17)","284a1823":"rfc_params = {'max_features': range(1,11), 'min_samples_leaf': range(1,3), 'max_depth': range(3,13), 'criterion':['gini','entropy']}\n\nrfc = RandomForestClassifier(n_estimators=100, random_state=17, n_jobs= -1)\n\ngcv = GridSearchCV(rfc, rfc_params, n_jobs=-1, cv=skf, scoring='recall')\n\ngcv.fit(X_train, y_train)","7af5d30f":"from sklearn.ensemble import BaggingClassifier","bc5a84b2":"dtc = DecisionTreeClassifier(random_state=123)","4d9bb18b":"bc = BaggingClassifier( base_estimator=dtc, n_estimators=100,random_state=123)\nbc.fit(X_train, y_train)","0ff08b92":"y_pred=bc.predict(X_test)\naccuracy_score(y_test, y_pred)","ec5ffe41":"bc.predict([[1,1,1,1,1,0,0,1,0,1,1,0,1,0,1,1,0,0,0,1,0,0]])","b4a71156":"bc.predict([[1,1,1,1,1,0,0,1,0,1,1,0,1,0,1,1,0,0,0,0,1,0]])","f08b21c5":"RandomForestClassifier\nici lorsque j'ai utilis\u00e9 les param\u00e8tres par d\u00e9faut le score est 0.10","db64d1a3":"ici j'ai chang\u00e9 les param\u00e8tres et et il n'y a pas une grande diff\u00e9rence\nj'ai utilis\u00e9 (criterion='gini', random_state=17 class_weight=None) de plus les param\u00e8tres par defaut","ceb0fe59":"DecisionTreeClassifier**"}}