{"cell_type":{"0eed0eec":"code","34668548":"code","1e7b0139":"code","e7fba8b3":"code","dd067ab2":"code","a9c93602":"code","b5bc38bd":"code","03d2bd0a":"code","b10ddcd1":"code","660d6d86":"code","4d600962":"code","0276fcb9":"code","887a49f6":"code","b7bfa908":"code","a3262343":"code","5b00798b":"code","6831eafa":"code","ed05796b":"code","ee8ba2ad":"code","217e7753":"code","25856255":"code","9fb8094a":"code","c9031f0d":"code","2697ff9a":"code","55a6992f":"code","6b7d4033":"code","1ccbff1c":"code","479c14db":"code","63937701":"code","fea90156":"code","a2ada774":"code","cd8c79c2":"code","4e161880":"code","0c35d287":"code","ed63de53":"code","199add54":"code","2866163d":"code","f9e1352e":"code","22f0c899":"code","edae19b2":"code","b190537f":"code","9bba8687":"code","3b4e395a":"code","2188657a":"code","b5542334":"code","09d41ac5":"code","51e15cda":"code","e8b2a34f":"code","9fe37c2e":"code","0b88a599":"code","6f8d9c4c":"code","c1c2e01e":"code","53f34ede":"code","94eebe56":"markdown","27d11855":"markdown","e3674324":"markdown","ddb10c53":"markdown","015402eb":"markdown","5c4752e1":"markdown","b64cacc1":"markdown","45919274":"markdown","b6d1ba70":"markdown","f9a84879":"markdown","97604059":"markdown","ad6b73b3":"markdown","a817e4b7":"markdown","8d8ea45c":"markdown","8108dd0d":"markdown","133f0532":"markdown","c1ecc00b":"markdown","d0fbad04":"markdown","b3687192":"markdown","8f735ae7":"markdown","4c8d4909":"markdown","c9d35aec":"markdown","14e55975":"markdown","51d6e7e3":"markdown","ef12ceb9":"markdown","b7c14e22":"markdown","6095ec48":"markdown"},"source":{"0eed0eec":"import os\nprint(os.listdir(\"..\/input\"))","34668548":"# Data Processing\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# Data Visualizing\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Data Modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC, SVC, NuSVC\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import ExtraTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# Data Evalutation\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve\n\n# Warning Removal\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)","1e7b0139":"df = pd.read_csv('..\/input\/diabetes.csv')","e7fba8b3":"df.head(5)","dd067ab2":"df.describe(include=\"all\")","a9c93602":"df.info(verbose=True)","b5bc38bd":"df.corr()['Outcome'].sort_values(ascending = False)","03d2bd0a":"zero_count = (df == 0).sum() # (df.isnull()).sum()\nzero_count_df = pd.DataFrame(zero_count)\nzero_count_df.drop('Outcome', axis=0, inplace=True)\nzero_count_df.columns = ['count_0']\n\n# https:\/\/stackoverflow.com\/questions\/31859285\/rotate-tick-labels-for-seaborn-barplot\/60530167#60530167\nsns.set(style='whitegrid')\nplt.figure(figsize=(13,8))\nsns.barplot(x=zero_count_df.index, y=zero_count_df['count_0'])\nplt.xticks(rotation=70)\nplt.show()","b10ddcd1":"plt.figure(figsize=(13,8))\nsns.boxplot(data=df.drop('Outcome', axis=1))\nplt.show()","660d6d86":"cols = ['Glucose','BloodPressure','Insulin','BMI','SkinThickness']\ndf[cols] = df[cols].replace(0, np.nan)\n# Check:  df[df['Glucose'].isnull()]","4d600962":"df.isnull().sum()","0276fcb9":"cols = ['Pregnancies','Glucose','BloodPressure','Insulin','BMI','SkinThickness','DiabetesPedigreeFunction', 'Age']\narr_median = []\ndef median_target(cols):\n    for col in cols:\n        temp = df[df[col].notnull()]\n        temp = temp[[col, 'Outcome']].groupby('Outcome').median()\n        arr_median.append(temp)\n    return arr_median\nmedian_target(cols)\na = pd.concat(arr_median, axis=1)","887a49f6":"arr_mean=[]\ndef mean_target(cols):\n    for col in cols:\n        temp = df[df[col].notnull()]\n        temp = temp[[col, 'Outcome']].groupby('Outcome').mean()\n        arr_mean.append(temp)\n    return arr_mean\nmean_target(cols)\nb = pd.concat(arr_mean, axis=1)","b7bfa908":"c = pd.concat([a,b], axis=0)\nc","a3262343":"c.loc[0].iloc[0]['Pregnancies']","5b00798b":"g = sns.FacetGrid(df, col=\"Outcome\")\ng = g.map(plt.hist, \"Pregnancies\")","6831eafa":"g = sns.FacetGrid(df, col=\"Outcome\")\ng = g.map(plt.hist, \"Glucose\")","ed05796b":"g = sns.FacetGrid(df, col=\"Outcome\")\ng = g.map(plt.hist, \"BloodPressure\")","ee8ba2ad":"g = sns.FacetGrid(df, col=\"Outcome\")\ng = g.map(plt.hist, \"Insulin\")","217e7753":"g = sns.FacetGrid(df, col=\"Outcome\")\ng = g.map(plt.hist, \"BMI\")","25856255":"g = sns.FacetGrid(df, col=\"Outcome\")\ng = g.map(plt.hist, \"SkinThickness\")","9fb8094a":"df.loc[(df['Outcome'] == 0 ) & (df['Glucose'].isnull()), 'Glucose'] = 107\ndf.loc[(df['Outcome'] == 1 ) & (df['Glucose'].isnull()), 'Glucose'] = 140\n\ndf.loc[(df['Outcome'] == 0 ) & (df['BloodPressure'].isnull()), 'BloodPressure'] = 70\ndf.loc[(df['Outcome'] == 1 ) & (df['BloodPressure'].isnull()), 'BloodPressure'] = 74.5\n\ndf.loc[(df['Outcome'] == 0 ) & (df['Insulin'].isnull()), 'Insulin'] = 102.5\ndf.loc[(df['Outcome'] == 1 ) & (df['Insulin'].isnull()), 'Insulin'] = 169.5\n\ndf.loc[(df['Outcome'] == 0 ) & (df['BMI'].isnull()), 'BMI'] = 30.1\ndf.loc[(df['Outcome'] == 1 ) & (df['BMI'].isnull()), 'BMI'] = 34.3\n\ndf.loc[(df['Outcome'] == 0 ) & (df['SkinThickness'].isnull()), 'SkinThickness'] = 27\ndf.loc[(df['Outcome'] == 1 ) & (df['SkinThickness'].isnull()), 'SkinThickness'] = 32","c9031f0d":"df.isnull().sum()","2697ff9a":"f, ax = plt.subplots(figsize=(11, 15))\nax.set(xlim=(-.05, 300))\nsns.boxplot(data=df.drop('Outcome', axis=1), orient = 'h')\nplt.show()","55a6992f":"sns.boxplot(df['Pregnancies'])\nplt.show()","6b7d4033":"df['Pregnancies'].value_counts()","1ccbff1c":"df.loc[(df['Outcome'] == 0 ) & (df['Pregnancies']>12), 'Pregnancies'] = 2\ndf.loc[(df['Outcome'] == 1 ) & (df['Pregnancies']>12), 'Pregnancies'] = 4","479c14db":"sns.boxplot(df['BloodPressure'])\nplt.show()","63937701":"df.loc[(df['Outcome'] == 0 ) & (df['BloodPressure']<40), 'BloodPressure'] = 70\ndf.loc[(df['Outcome'] == 1 ) & (df['BloodPressure']<40), 'BloodPressure'] = 74.5\n\ndf.loc[(df['Outcome'] == 0 ) & (df['BloodPressure']>110), 'BloodPressure'] = 70\ndf.loc[(df['Outcome'] == 1 ) & (df['BloodPressure']>110), 'BloodPressure'] = 74.5","fea90156":"sns.boxplot(df['Insulin'])\nplt.show()","a2ada774":"df.loc[(df['Outcome'] == 0 ) & (df['Insulin']>270), 'Insulin'] = 102.5\ndf.loc[(df['Outcome'] == 1 ) & (df['Insulin']>270), 'Insulin'] = 169.5","cd8c79c2":"sns.boxplot(df['BMI'])\nplt.show()","4e161880":"df.loc[(df['Outcome'] == 0 ) & (df['BMI']>50), 'BMI'] = 30.1\ndf.loc[(df['Outcome'] == 1 ) & (df['BMI']>50), 'BMI'] = 34.3","0c35d287":"sns.boxplot(df['SkinThickness'])\nplt.show()","ed63de53":"df.loc[(df['Outcome'] == 0 ) & (df['SkinThickness']>42), 'SkinThickness'] = 27\ndf.loc[(df['Outcome'] == 1 ) & (df['SkinThickness']>42), 'SkinThickness'] = 32\n\ndf.loc[(df['Outcome'] == 0 ) & (df['SkinThickness']<15), 'SkinThickness'] = 27\ndf.loc[(df['Outcome'] == 1 ) & (df['SkinThickness']<15), 'SkinThickness'] = 32","199add54":"sns.boxplot(df['DiabetesPedigreeFunction'])\nplt.show()","2866163d":"df.loc[(df['Outcome'] == 0 ) & (df['DiabetesPedigreeFunction']>1.1), 'DiabetesPedigreeFunction'] = 0.336\ndf.loc[(df['Outcome'] == 1 ) & (df['DiabetesPedigreeFunction']>1.1), 'DiabetesPedigreeFunction'] = 0.449","f9e1352e":"sns.boxplot(df['Age'])\nplt.show()","22f0c899":"df.loc[(df['Outcome'] == 0 ) & (df['Age']>62), 'Age'] = 27\ndf.loc[(df['Outcome'] == 1 ) & (df['Age']>62), 'Age'] = 36","edae19b2":"df.describe()","b190537f":"x = StandardScaler().fit_transform(df.drop('Outcome', axis=1))\ny = df['Outcome']","9bba8687":"x_train, x_test, y_train, y_test = train_test_split(x, y,\n                                                    test_size=0.3, random_state=0)","3b4e395a":"pwd","2188657a":"models_score = []\n\n# LOGISTICREGRESSION\nlogistic = LogisticRegression(random_state=0).fit(x_train, y_train)\nmodels_score.append([logistic.__class__.__name__, logistic.score(x_test, y_test)])\n\n# KNN\nmax = 0\nnum_neighbor = 0\n\ntrain_scores = []\ntest_scores = []\n\nfor i in range (1, 100):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train, y_train)\n    \n    train_scores.append(knn.score(x_train, y_train))\n    test_scores.append(knn.score(x_test, y_test))   \n    # source: https:\/\/stackoverflow.com\/questions\/455612\/limiting-floats-to-two-decimal-points\/28142318#28142318\nmodels_score.append([knn.__class__.__name__, np.max(test_scores)])\n    \n# NAIVE BAYES\nNB = GaussianNB().fit(x_train, y_train)\nmodels_score.append([NB.__class__.__name__, NB.score(x_test, y_test)])\n\n# LINEAR - NONLINEAR SVM\nlinearSVC = LinearSVC(random_state=0, penalty='l2', loss='hinge', dual=True, C=1, multi_class='ovr', max_iter=1000).fit(x_train, y_train)\nRbfSVC = SVC(C=1, kernel='rbf', gamma='scale', coef0=1, decision_function_shape='ovr').fit(x_train, y_train)\nmodels_score.append([linearSVC.__class__.__name__, linearSVC.score(x_test, y_test)])\nmodels_score.append([RbfSVC.__class__.__name__, RbfSVC.score(x_test, y_test)])\n\n\n# DEEP LEARNING\n# define the keras model\nmodel = Sequential()\nmodel.add(Dense(12, input_dim=8, activation='relu'))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n# compile the keras model\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# fit the keras model on the dataset\n    # choose the best model\nmc = ModelCheckpoint('.\/input\/best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n    # stop traning for not accuracy improvement \nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\nhistory = model.fit(\n    x_train,\n    y_train,\n    epochs=60,\n    batch_size=10,\n    #validation_split=0.1,\n    validation_data=(x_test, y_test),\n    verbose = 0,\n    shuffle=True,\n    callbacks = [mc,es]\n)\n# evaluate the keras model\n_, accuracy = model.evaluate(x_test, y_test)\nmodels_score.append(['Deep Learning', accuracy])\n    # load the best model for prediction\n# saved_model = load_model('best_model.h5')\n# test_loss, test_acc = saved_model.evaluate(x_test, y_test)\n# models_score.append(['Deep Learning', test_acc])\n\n\n# Unscaling Data\nx = df.drop('Outcome', axis=1)\ny = df['Outcome']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y,\n                                                    test_size=0.3, random_state=0)\n\n# DECISION TREE\ndt = DecisionTreeClassifier(random_state=0).fit(x_train,y_train)\nmodels_score.append([dt.__class__.__name__, dt.score(x_test, y_test)])\n\n# RANDOM FOREST\nrdf=RandomForestClassifier(random_state=0).fit(x_train,y_train)\nmodels_score.append([rdf.__class__.__name__, rdf.score(x_test, y_test)])\n\n# EXTRA TREE\netc = ExtraTreeClassifier(criterion='gini', max_depth=7, max_features=None, random_state=0).fit(x_train,y_train)\nmodels_score.append([etc.__class__.__name__, etc.score(x_test, y_test)])\n\n# ADABOOST\nadb = AdaBoostClassifier(base_estimator=None, n_estimators=90, algorithm='SAMME', random_state=0).fit(x_train,y_train)\nmodels_score.append([adb.__class__.__name__, adb.score(x_test, y_test)])\n\n# GRADIENTBOOSTING\ngbc = GradientBoostingClassifier(loss='deviance', n_estimators=100, subsample=1, random_state=0).fit(x_train,y_train)\nmodels_score.append([gbc.__class__.__name__, gbc.score(x_test, y_test)])\n\n# XGBOOST\nXGB = XGBClassifier(n_estimators=120, max_depth=5, learning_rate=0.05, \n                    subsample=0.96, objective='binary:logistic', \n                    booster='gbtree', tree_method='exact', random_state=0).fit(x_train,y_train)\nmodels_score.append([XGB.__class__.__name__, XGB.score(x_test, y_test)])\n\n# LIGHT GBM\nLGBM = LGBMClassifier(boosting='gbdt', n_estimators=100, \n                      colsample_bytree=1.0, max_depth=10, \n                      min_child_samples=3, objective='binary', \n                      reg_alpha=0.0, subsample=1.0).fit(x_train,y_train)\nmodels_score.append([LGBM.__class__.__name__, LGBM.score(x_test, y_test)])","b5542334":"model_evaluation = pd.DataFrame(models_score, columns=['Model','Score'])\nmodel_evaluation.sort_values(by=['Score'],\n                            ascending=False,\n                            inplace=True)\nmodel_evaluation","09d41ac5":"plt.figure(figsize=(12,5))\np = sns.lineplot(range(1,100), train_scores,marker='*', label='Train Score')\np = sns.lineplot(range(1,100), test_scores,marker='o', label='Test Score')","51e15cda":"y_pred = knn.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred)\n\nplt.plot([0,1],[0,1],'k--')\nplt.plot(fpr,tpr, label='Knn')\nplt.xlabel('false positive rate')\nplt.ylabel('true positive rate')\nplt.title('Knn(n_neighbors=13) ROC curve')\nplt.show()","e8b2a34f":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss) + 1)\n\nplt.figure(figsize=(18,5))\nplt.plot(epochs, loss, 'y', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\n\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","9fe37c2e":"# Source: https:\/\/stackoverflow.com\/questions\/27817994\/visualizing-decision-tree-in-scikit-learn\/54836424#54836424\nfrom sklearn import tree\nimport graphviz \nfrom graphviz import Source\ndot_data = tree.export_graphviz(dt, out_file=None, feature_names=x.columns)\ngraph = graphviz.Source(dot_data, format='png')\ngraph.render(\"decision tree\",view = True)","0b88a599":"# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'decision tree.png')","6f8d9c4c":"# Source: https:\/\/www.kaggle.com\/parulpandey\/intrepreting-machine-learning-models\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(dt, random_state=1).fit(x_train,y_train)\neli5.show_weights(perm, feature_names = x_test.columns.tolist())","c1c2e01e":"from pdpbox import pdp, get_dataset, info_plots\n\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=dt, dataset=x_test, model_features=x_test.columns, feature='Glucose')\n\n# plot it\npdp.pdp_plot(pdp_goals, 'Glucose')\nplt.show()","53f34ede":"row_to_show = 1\ndata_for_prediction = x_test.iloc[row_to_show]\n\nimport shap\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(dt)\n# Calculate Shap values\nshap_values = explainer.shap_values(data_for_prediction)\nshap.initjs()\nshap.force_plot(explainer.expected_value[1], shap_values[1], data_for_prediction)","94eebe56":"# FEATURE ENGINEERING","27d11855":"### VISUALIZATION\n- dist\n- box","e3674324":"### EXLORATORY\n- read_csv\n- head\n- describe\n- info\n- correlation","ddb10c53":"##### Partial Dependence Plots\n- The Y-axis represents the change in prediction from what it would be predicted at the baseline or leftmost value.\n- Blue area denotes the confidence interval\n- For the \u2018Glucose\u2019 graph, we observe that probability of a person having diabetes steeply increases as the glucose level goes beyond 140 and then the probability remains high.","015402eb":"##### Plot ROC\n- In general, an AUC of 0.5 suggests no discrimination (i.e., ability to diagnose patients with and without the disease or condition based on the test),\n- 0.7 to 0.8 is considered acceptable,\n- 0.8 to 0.9 is considered excellent,\n- and more than 0.9 is considered outstanding.","5c4752e1":"# LIBRARY","b64cacc1":"# Pima Diabetes Classification Prediction\n### Vu Duong\n#### May 14, 2020","45919274":"##### SHAP Values\n- Helps to break down a prediction to show the impact of each feature.\n- Let's pick a random patient, here is 1, from our dataset to see how our model predict based on individual feature\n- Glucose = 112 has the biggest impact on prediction of not being diabetes\n- While, BMI feature has the biggest effect in prediction of being diabetes","b6d1ba70":"### Box Plot to check for Outliers","f9a84879":"##### Median according to the Output","97604059":"##### Permutation Importance\n- The features at the top are most important and at the bottom, the least.\n- For this example, 'Glucose level' is the most important feature which decides whether a person will have diabetes","ad6b73b3":"# MODELING","a817e4b7":"##### Plot KNN with respect to K neighbors","8d8ea45c":"##### Training&Validation Loss in Deep Learning\n- The more training on dataset, the better it is till the epoch 38.\n- Afterward, validation loss started increasing for overfitting model","8108dd0d":"### Train-Test Split","133f0532":"##### Ploting Nan\/0","c1ecc00b":"### SCALING\n- standardization\n- normalize","d0fbad04":"### MODEL INTERPRETATION","b3687192":"### DATA IMPUTATION\n- mean\n- median\n- mode\n- correlation with other columns","8f735ae7":"### MODEL EVALUATION","4c8d4909":"###### Replacing 0 with NaN to handle easily","c9d35aec":"##### Plot a tree","14e55975":"##### Ploting Outliers","51d6e7e3":"### Models","ef12ceb9":"##### Correct Outliers with Median","b7c14e22":"# CREDIT\nThis work is inspired by multiple great sources done before:\n- https:\/\/www.kaggle.com\/parulpandey\/intrepreting-machine-learning-models\n- https:\/\/www.kaggle.com\/tariqmhmd5\/pima-india-diabetes-prediction-with-6-algorithms\/comments#749402","6095ec48":"##### Filling NaN with Median according to Outcome"}}