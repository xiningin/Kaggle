{"cell_type":{"8b4d378b":"code","7124ab8e":"code","c3cb94ee":"code","1d1e80f5":"code","78e5ee70":"code","bb9386ec":"code","3b5a3f28":"code","6af91c6e":"code","5a0bee2d":"code","c1812d01":"code","55dfec91":"code","d8f8503a":"code","ee635a3f":"code","3c3a2add":"code","03c407b4":"code","0b35991a":"code","f2b95ad5":"code","1519d95d":"code","0e121376":"code","14d90144":"code","d6f0772d":"code","6addb847":"code","6e84286d":"code","0b4c6d04":"code","73355f3e":"code","21651853":"code","52fa4473":"code","f76ad28f":"code","44c85add":"code","3ccd7e88":"code","7443fa33":"code","fdfc110f":"code","6d7dd041":"code","991c1c92":"markdown","64896d5b":"markdown","b6bba5b3":"markdown","5bdb3896":"markdown","a80584d6":"markdown","1506216c":"markdown","483de58b":"markdown","bb8da388":"markdown","d47a4deb":"markdown","5c139d9e":"markdown","8e0f10d2":"markdown","b07fc531":"markdown"},"source":{"8b4d378b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"white\")\nfrom scipy import stats\nfrom scipy.stats import norm, skew \n%matplotlib inline\n\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, RobustScaler\n\nfrom sklearn.decomposition import PCA","7124ab8e":"from sklearn.linear_model import LinearRegression, Lasso, ElasticNet, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor, GradientBoostingRegressor\n\nfrom sklearn.metrics import mean_squared_error","c3cb94ee":"import warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points","1d1e80f5":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","78e5ee70":"train_data.head(5)","bb9386ec":"#test_data.head(5)","3b5a3f28":"#Save the 'Id' column\ntrain_ID = train_data['Id']\ntest_ID = test_data['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain_data.drop(\"Id\", axis = 1, inplace = True)\ntest_data.drop(\"Id\", axis = 1, inplace = True)","6af91c6e":"#Correlation map to see how features are correlated with SalePrice\nsales_cor = train_data.corr()[['SalePrice']]\nsales_cor = sales_cor[sales_cor.SalePrice>0.5]\nbest_predictors = list(sales_cor.sort_values(['SalePrice'], ascending = False).index)\n\ncorrmat = train_data[best_predictors].corr()\nplt.subplots(figsize=(8,5))\nsns.heatmap(corrmat)","5a0bee2d":"#best_5_predictors = list(sales_cor.sort_values(['SalePrice'], ascending = False).head(5).index)\n#sns.pairplot(train_data[best_5_predictors]) \ncorrmat","c1812d01":"plt.subplots(figsize=(10,7))\nsns.boxplot(x=\"OverallQual\", y=\"SalePrice\", data=train_data)\nplt.ylabel('Sale Price ($)', fontsize=15)\nplt.title('Sale Price vs Overall Quality Rating', fontsize=15)\nplt.xlabel('OverallQual', fontsize=15)","55dfec91":"fig, ax = plt.subplots(figsize=(10,7))\nax.scatter(train_data['GrLivArea'], train_data['SalePrice'])\nplt.title('Sale Price vs Above Ground Living Area', fontsize=15)\nplt.ylabel('Sale Price ($)', fontsize=15)\nplt.xlabel('GrLivArea (sqft)', fontsize=15)\nplt.show()","d8f8503a":"#Deleting outliers\ntrain_data = train_data.drop(train_data[(train_data['GrLivArea']>4000) & (train_data['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots(figsize=(10,7))\nax.scatter(train_data['GrLivArea'], train_data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","ee635a3f":"plt.subplots(figsize=(10,7))\nsns.distplot(train_data['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_data['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency', fontsize=15)\nplt.title('Sale Price Distribution', fontsize=15)\nplt.xlabel('Sale Price ($)', fontsize=15)\n\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)\nplt.show()","3c3a2add":"# Log Transform the Target Variable\n\n#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain_data[\"SalePrice\"] = np.log1p(train_data[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train_data['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train_data['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train_data['SalePrice'], plot=plt)\nplt.show()","03c407b4":"# Combine features and make y_train\nntrain = train_data.shape[0]\nntest = test_data.shape[0]\ny = train_data.SalePrice\ny_train = train_data.SalePrice.values\nall_data = pd.concat((train_data, test_data)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","0b35991a":"# Check for na values\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(10)","f2b95ad5":"f, ax = plt.subplots(figsize=(10, 7))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent Missing Data by Feature', fontsize=15)","1519d95d":"# PoolQC : data description says NA means \"No Pool\". \n# That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.\nall_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n\n# MiscFeature : data description says NA means \"no misc feature\"\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n\n# Alley : data description says NA means \"no alley access\"\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n\n# Fence : data description says NA means \"no fence\"\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n\n# FireplaceQu : data description says NA means \"no fireplace\"\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n\n# LotFrontage : Since the area of each street connected to the house property most likely have a similar area to \n# other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.\n# Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\n# GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n\n# GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n\n# BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for\n# having no basement\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n\n# BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, \n# NaN means that there is no basement.\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n\n# MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and \n# None for the type.\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\n# MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing \n# values with 'RL'\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\n# Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.\nall_data = all_data.drop(['Utilities'], axis=1)\n\n# Functional : data description says NA means typical\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n\n# Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n\n# KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n\n# Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n\n# SaleType : Fill in again with most frequent which is \"WD\"\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\n# MSSubClass : Na most likely means No building class. We can replace missing values with None\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","0e121376":"#Check remaining missing values again if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","14d90144":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","d6f0772d":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","6addb847":"y_train","6e84286d":"num_folds = 5\nseed = 42","0b4c6d04":"#Validation function for metric: RMSE(log)\ndef rmsle_cv(model):\n    kf = KFold(n_splits=5, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","73355f3e":"# prepare models\nmodels = []\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('RR', Ridge()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('AB', AdaBoostRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('ET', ExtraTreesRegressor()))\nmodels.append(('GBM', GradientBoostingRegressor()))\n\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    score = rmsle_cv(model)\n    results.append(score)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, score.mean(), score.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = plt.figure(figsize=(8,4))\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.title('Inital Algorithm Comparison - RMSE(log)', fontsize=13)\nplt.ylabel('RMSE(log)', fontsize=13)\nplt.xlabel('Initial Models', fontsize=13)\nplt.show()","21651853":"# Comparing several starter models with r^2\nmodels = []\n#models.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('RR', Ridge()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\nmodels.append(('AB', AdaBoostRegressor()))\nmodels.append(('RF', RandomForestRegressor()))\nmodels.append(('ET', ExtraTreesRegressor()))\nmodels.append(('GBM', GradientBoostingRegressor()))\n\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, train, y_train, cv=kfold, scoring='r2')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# Compare Algorithms\nfig = plt.figure(figsize=(8, 4))\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.title('Inital Algorithm Comparison - R^2', fontsize=13)\nplt.ylabel('R^2', fontsize=13)\nplt.xlabel('Initial Models', fontsize=13)\nplt.show()","52fa4473":"# Lasso tuning for lambda\nalphas = np.array([1,0.1,0.01,0.001,0.0001,0])\nparam_grid = dict(alpha=alphas)\nmodel = Lasso()\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid.fit(train, y_train)\nprint(grid.best_score_)\nprint(grid.best_estimator_.alpha)","f76ad28f":"#Enet Tuning\nalphas = np.array([1,0.1,0.01,0.001,0.0001,0])\nparam_grid = dict(alpha=alphas)\nmodel = ElasticNet()\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid.fit(train, y_train)\nprint(grid.best_score_)\nprint(grid.best_estimator_.alpha)","44c85add":"# RR tuning for alpha\nalphas = np.array([1,0.1,0.01,0.001,0.0001,0])\nparam_grid = dict(alpha=alphas)\nmodel = Ridge()\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid.fit(train, y_train)\nprint(grid.best_score_)\nprint(grid.best_estimator_.alpha)","3ccd7e88":"## GBM tuning\nparam_grid = {'n_estimators':[100, 500],'max_depth':[2, 4],'min_samples_leaf':[2, 5]}\nmodel = GradientBoostingRegressor()\ngrid = GridSearchCV(estimator=model, param_grid=param_grid)\ngrid.fit(train, y_train)\nprint(grid.best_score_)\nprint(grid.best_estimator_.max_depth)\nprint(grid.best_estimator_.min_samples_leaf)\nprint(grid.best_estimator_.n_estimators)","7443fa33":"# tuned models\n\n# LASSO Regression : This model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline\nlasso_t = Lasso(alpha =0.001)\n\n#Elastic Net Regression:again made robust to outliers\nENet_t = ElasticNet(alpha=0.001)\n\n# Ridge Regression tuned\nRR_t = Ridge(alpha=1)\n\n# GBR tuned\nGBoost_t = GradientBoostingRegressor(n_estimators=500, max_depth=2,min_samples_leaf=2)\n","fdfc110f":"# prepare models\nmodels = []\nmodels.append(('Lasso', lasso_t))\nmodels.append(('Enet', ENet_t))\nmodels.append(('Ridge', RR_t))\nmodels.append(('GBoost', GBoost_t))\n\n# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    score = rmsle_cv(model)\n    results.append(score)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, score.mean(), score.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = plt.figure(figsize=(8, 4))\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.title('Tuned Algorithm Comparison - RMSE(log)', fontsize=13)\nplt.ylabel('RMSE(log)', fontsize=13)\nplt.xlabel('Tuned Models', fontsize=13)\nplt.show()","6d7dd041":"models = []\nmodels.append(('Lasso', lasso_t))\nmodels.append(('Enet', ENet_t))\nmodels.append(('Ridge', RR_t))\nmodels.append(('GBoost', GBoost_t))\n\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, train, y_train, cv=kfold, scoring='r2')\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# Compare Algorithms\nfig = plt.figure(figsize=(8, 4))\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.title('Tuned Algorithm Comparison - R^2', fontsize=13)\nplt.ylabel('R^2', fontsize=13)\nplt.xlabel('Tuned Models', fontsize=13)\nplt.show()","991c1c92":"# Advanced Regression Techniques - Machine Learning with Python\n## Housing Price Regression Project\n\n#### Tyler Watkins\n\n10-Aug-2018\n\t\t\t\n#### Introduction\n\nAccurately predicting housing prices can be a complex endeavor. Just knowing the number of bedrooms and the area zip code isn\u2019t enough. In this notebook I aim to model residential housing prices in Ames, Iowa in an attempt to build an accurate model for predicting housing prices.\n\nI use the Ames Housing dataset which can be found at https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques. This\ndataset and problem were previously launched as a knowledge competition on Kaggle.\n\nThe results of this notebook were submitted to the competition and as of 10-Aug-2018 scored in the top 18% of over 4600 entries.\n\n\n#### Data Exploration and Preparation Process:\n- Import data\n- Explore best numerical variables\n- Remove 2 obvious outliers\n- Log transform response variable\n- Fix NA issues\n- One-hot encode categorical variables\n\n\n#### Model Process:\n- Fit 9 initial models using default parameters\n- Select best models and parameter tune\n- Choose best model\n- Predict on test data and export csv","64896d5b":"## Modelling","b6bba5b3":"### Correlation Map","5bdb3896":"### Import Libraries and Data","a80584d6":"### Handle Missing  Values","1506216c":"### One-Hot Encode Data","483de58b":"### Final Models","bb8da388":"## Data Exploration and Preparation","d47a4deb":"### First Models","5c139d9e":"### Combine train and test data","8e0f10d2":"### Target Variable: Sale Price","b07fc531":"### Parameter Tuning"}}