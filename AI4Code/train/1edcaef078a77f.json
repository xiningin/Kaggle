{"cell_type":{"d5f846aa":"code","a90092bf":"code","de5478ec":"code","354c8f44":"code","3918d157":"code","58e507a8":"code","959a552e":"code","d19dfe8d":"code","ab4ed6bc":"code","4ce4bbfd":"code","f31a2dc2":"code","013240bf":"code","ed606568":"code","e2e96082":"code","1186565b":"code","6e7d444f":"code","306a6e42":"code","59eddba3":"code","eb83c428":"code","9bbadd39":"code","f874dffb":"code","e0cbbded":"code","03b23462":"code","63b13bdf":"code","91dc4147":"code","4f1b35bc":"code","37ea8c64":"code","fe215e1b":"code","e05c005c":"markdown","59401070":"markdown","5fd52f64":"markdown","c5d30329":"markdown","c3c322ea":"markdown","fd774050":"markdown","49778df4":"markdown","7cfbae97":"markdown","48c7aeaa":"markdown","dfb00096":"markdown","ae444596":"markdown","82c4a2cc":"markdown","baac928f":"markdown","185f7f99":"markdown","d986d465":"markdown","718ffe2f":"markdown","069b6d95":"markdown","cf2025d5":"markdown","715c54fd":"markdown","694ace8e":"markdown","e424f57f":"markdown","8d7bc55e":"markdown","1564e7e2":"markdown","c9464c03":"markdown","7b7ecaf6":"markdown","d1ceaf29":"markdown","22bfdba6":"markdown","ca40d445":"markdown","f6a26538":"markdown"},"source":{"d5f846aa":"#importing the libraries to use \nimport os\nimport re\nimport nltk\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans\nfrom sklearn import preprocessing as pr\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\n\nfrom keras import initializers\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential, Model\nfrom keras.callbacks import ReduceLROnPlateau\n\nglobalSeed=50\nfrom numpy.random import seed\nseed(globalSeed)\nfrom tensorflow import set_random_seed\nset_random_seed(globalSeed)","a90092bf":"#General plot style\ndef PlotStyle(Axes,Title,x_label,y_label):\n    \n    Axes.spines['top'].set_visible(False)\n    Axes.spines['right'].set_visible(False)\n    Axes.spines['bottom'].set_visible(True)\n    Axes.spines['left'].set_visible(True)\n    Axes.xaxis.set_tick_params(labelsize=12)\n    Axes.yaxis.set_tick_params(labelsize=12)\n    Axes.set_ylabel(y_label,fontsize=14)\n    Axes.set_xlabel(x_label,fontsize=14)\n    Axes.set_title(Title)\n    ","de5478ec":"AnswersData=pd.read_csv(r\"..\/input\/answers.csv\")\nQuestionsData=pd.read_csv(r\"..\/input\/questions.csv\")\n\nAnswersData=AnswersData.drop(AnswersData.index[48625]) #While exploring the data that answer body is equal to 'Nan'","354c8f44":"AnswersBody=np.array(AnswersData['answers_body'])\nQuestionsBody=np.array(QuestionsData['questions_body'])\nQuestionsTitle=np.array(QuestionsData['questions_title'])","3918d157":"def TextHTMLRemoval(TargetString):\n  #Removes HTML tags from the text'\n  characterList=['<p>','<\/p>','<br>','<ol>','<li>','<\/li>','h1&gt','&lt;','\\n','\\r','href','html','http','https']\n  cTarg=TargetString\n  \n  for val in characterList:\n    cTarg=re.sub(val,' ',cTarg)\n    \n  return cTarg\n  \ndef GetTextOnly(TargetString):\n  #Removes numbers and punctuation from the text\n  cTarg=TargetString\n  cTarg=re.sub(r'[^\\w\\s]',' ',cTarg)\n  cTarg=re.sub(r'[0-9]+', ' ', cTarg)\n  \n  return cTarg\n  \ndef MakeTextTransform(TargetString):\n  #Wrapper function \n  cTarg=TargetString\n  cTarg=TextHTMLRemoval(cTarg)\n  cTarg=GetTextOnly(cTarg)\n  \n  return cTarg.lower()","58e507a8":"FiltAnswers=[MakeTextTransform(val) for val in AnswersBody]\nFiltQuestionsB=[MakeTextTransform(val) for val in QuestionsBody]\n\ndel AnswersBody,QuestionsBody,QuestionsTitle","959a552e":"def UniqueDataSetTokens(TextData):\n  \n  cData=TextData\n  nData=len(cData)\n    \n  def SplitAndReduce(TargetString):\n    return list(set(TargetString.split()))#returns the unique text elements\n  \n  container=SplitAndReduce(cData[0])\n  \n  for k in range(1,nData):\n    container=container+SplitAndReduce(cData[k])\n    if k%100==0:  #only each 100 steps the container is transformed to a set to eliminate duplicates\n      container=list(set(container))\n  \n  return container\n","d19dfe8d":"def TokenProcessing(TokenData):\n  \n  cToken=TokenData\n  s=stopwords.words('english')\n  cToken=list(set(cToken)-set(s)) #Eliminates stop words\n   \n  PosTagging=nltk.pos_tag(cToken) #Selecting only nouns using part of speech tagging\n  lToken=[]\n  for val in  PosTagging:\n    if val[1]=='NN':\n      lToken.append(val[0])\n  \n  localps=PorterStemmer()\n  localToken=[]\n  for val in lToken:\n    localToken.append(localps.stem(val))#stem of each token \n  \n  stemS=[]\n  for val in s:\n    stemS.append(localps.stem(val))\n  \n  localToken=list(set(localToken))\n  localTok=[val for val in localToken if 16>len(val)>4]#eliminating words of small size and those greater than 16 characters\n  \n  return localTok","ab4ed6bc":"def TokenFrequencies(TargetString,Token):\n  \n  cST0=TargetString.split()\n  localPS=PorterStemmer()\n  cST=[]\n  for val in cST0:\n    cST.append(localPS.stem(val))#changes each word in the text to the stem word\n  \n  cTok=Token\n  nToken=len(cTok)\n  Container=[0 for k in range(nToken)]#frequencies of each token \n  cDict={}\n  \n  for k in range(nToken):\n    cDict[cTok[k]]=k #returns the index over the container \n    \n  for val in cST:\n    try: \n      cval=cDict[val]\n      Container[cval]=Container[cval]+1 \n    except KeyError: \n      pass\n  \n  return np.array(Container)*(1\/(sum(Container)+1))#adding one to avoid division by zero error \n","4ce4bbfd":"TargetDataSet=FiltQuestionsB #Data set used to generate the autoencoder can be changed to QuestionsTitle or AnswersBody \n \nTargetTokens=UniqueDataSetTokens(TargetDataSet)   \nstemToken=TokenProcessing(TargetTokens)\n\nTargetFreq=[TokenFrequencies(val,stemToken) for val in TargetDataSet]\nTargetFreq=np.reshape(TargetFreq,(len(TargetFreq),len(TargetFreq[0])))\nsumFreqs=TargetFreq.sum(axis=0)\n\nplt.figure(1)\nplt.hist(sumFreqs,bins=200)\nplt.yscale('log',nonposy='clip')\nax=plt.gca()\nPlotStyle(ax,'','Token Frequency','Frequency Counts')","f31a2dc2":"CropFactor=15\nSelectedIndex=[j for j in range(sumFreqs.size) if sumFreqs[j]>CropFactor*sumFreqs.min()]\nFinalToken=[stemToken[val] for val in SelectedIndex]","013240bf":"TargetFreq=[TokenFrequencies(val,FinalToken) for val in TargetDataSet]\nTargetSc=pr.MinMaxScaler()#MinMaxScaling of the target data\nTargetSc.fit(TargetFreq)  \nnormData=TargetSc.transform(TargetFreq)","ed606568":"fixedSeed=2017\n\ninputShape=normData[0].size\n\nae = Sequential()\nae.add(Dense(int(inputShape\/3),  activation='elu',\n            kernel_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            bias_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            input_shape=(inputShape,)))\n\nae.add(Dense(380,  activation='sigmoid',\n            kernel_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            bias_initializer=initializers.glorot_uniform(seed=fixedSeed)))\n\nae.add(Dense(190,  activation='sigmoid',\n            kernel_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            bias_initializer=initializers.glorot_uniform(seed=fixedSeed)))\n\nae.add(Dense(45,  activation='sigmoid',\n            kernel_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            bias_initializer=initializers.glorot_uniform(seed=fixedSeed)))\n\nae.add(Dense(12,  activation='elu',\n            kernel_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            bias_initializer=initializers.glorot_uniform(seed=fixedSeed)))\n\nae.add(Dense(3,    activation='linear', name=\"bottleneck\",\n            kernel_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            bias_initializer=initializers.glorot_uniform(seed=fixedSeed)))\n\nae.add(Dense(12,  activation='elu',\n            kernel_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            bias_initializer=initializers.glorot_uniform(seed=fixedSeed)))\n\nae.add(Dense(45,  activation='sigmoid',\n            kernel_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            bias_initializer=initializers.glorot_uniform(seed=fixedSeed)))\n\nae.add(Dense(190,  activation='sigmoid',\n            kernel_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            bias_initializer=initializers.glorot_uniform(seed=fixedSeed)))\n\nae.add(Dense(380,  activation='sigmoid',\n            kernel_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            bias_initializer=initializers.glorot_uniform(seed=fixedSeed)))\n\nae.add(Dense(int(inputShape\/3),  activation='elu',\n            kernel_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            bias_initializer=initializers.glorot_uniform(seed=fixedSeed)))\n\nae.add(Dense(inputShape,  activation='linear',\n            kernel_initializer=initializers.glorot_uniform(seed=fixedSeed),\n            bias_initializer=initializers.glorot_uniform(seed=fixedSeed)))\n\ndecay=0.001\nreduce_lr=ReduceLROnPlateau(monitor='loss',factor=0.1,patience=5,min_lr=decay)    \n\nae.compile(loss='mean_squared_error', optimizer = Adam(lr=0.01,decay=decay))\nae.fit(normData, normData, batch_size=3024, epochs=50, verbose=1,callbacks=[reduce_lr])\n\nencoder=Model(ae.input, ae.get_layer('bottleneck').output)\nZenc=encoder.predict(normData)\n","e2e96082":"EncScaler=pr.MinMaxScaler() #Min Max scaling of the encoded data\nEncScaler.fit(Zenc)\nEncoderScaled=EncScaler.transform(Zenc)\n\nfig = plt.figure(2,figsize=(8,8))\n\nax0=fig.add_subplot(2, 2, 1)\nax0.plot(EncoderScaled[:,0],EncoderScaled[:,1],'bo',alpha=0.0125)\nPlotStyle(ax0,'','Encoded Dimension 1','Encoded Dimension 2')\n\nax1=fig.add_subplot(2, 2, 2)\nax1.plot(EncoderScaled[:,0],EncoderScaled[:,2],'bo',alpha=0.0125)\nPlotStyle(ax1,'','Encoded Dimension 1','Encoded Dimension 3')\n\nax2=fig.add_subplot(2, 2, 3)\nax2.plot(EncoderScaled[:,1],EncoderScaled[:,2],'bo',alpha=0.0125)\nPlotStyle(ax2,'','Encoded Dimension 2','Encoded Dimension 3')\n\nax3=fig.add_subplot(2, 2, 4,projection='3d')\nax3.scatter(EncoderScaled[:,0],EncoderScaled[:,1],EncoderScaled[:,2],color='b',alpha=0.05)\n\nplt.tight_layout()","1186565b":"kmeansInertia=[]\nfitness=[]\nxv=[]\n\nfor k in range(5,60,3):\n  \n  cluster=KMeans(n_clusters=k)\n  cluster.fit(EncoderScaled)\n  labels =cluster.labels_\n  kmeansInertia.append(cluster.inertia_)\n  fitness.append(metrics.silhouette_score(EncoderScaled, labels) )\n  xv.append(k)\n\nplt.figure(3)\nplt.plot(xv,kmeansInertia\/max(kmeansInertia),label='SSE\/max(SSE)')\nplt.plot(xv,fitness\/max(fitness),label='Silhouette Score')\nplt.legend(loc=0)\nax=plt.gca()\nPlotStyle(ax,'','k-clusters','Normalized performance')\n\noptnClusters=xv[np.argmax(fitness[3:len(fitness)])]\nOptimalKMeans=KMeans(n_clusters=optnClusters)\nOptimalKMeans.fit(EncoderScaled)\nYVals=OptimalKMeans.labels_","6e7d444f":"RandomQuestTSample=np.random.randint(0,len(YVals),5)\n\nfor val in RandomQuestTSample:\n  \n  print('Cluster number = ' + str(YVals[val]))\n  print(FiltQuestionsB[val])","306a6e42":"Xtrain,Xtest,Ytrain,Ytest=train_test_split(EncoderScaled, YVals, train_size = 0.75,test_size=0.25, random_state = 23)\n\nRFC=RandomForestClassifier(n_estimators=100)\nRFC.fit(Xtrain,Ytrain)\n\nyPred=RFC.predict(Xtest)\nYTest=np.reshape(np.array(Ytest),(yPred.shape))\n\ncm=metrics.confusion_matrix(YTest,yPred)\nncm=cm\/cm.sum(axis=1)\n\ndel Xtrain, Xtest, Ytrain, Ytest, yPred\n\nplt.figure(4)\nax=plt.gca()\nim=ax.imshow(ncm,interpolation='nearest',cmap=plt.cm.Blues)\nax.figure.colorbar(im,ax=ax)","59eddba3":"AnswersTokFreq=[TokenFrequencies(val,FinalToken) for val in FiltAnswers]\nnormAnswerData=TargetSc.transform(AnswersTokFreq)\nEncAnswers=encoder.predict(normAnswerData)\nEncScaledAnswers=EncScaler.transform(EncAnswers)\nAnswersClasses=RFC.predict(EncScaledAnswers)","eb83c428":"RandomClusters=np.random.randint(0,optnClusters,2)\n\nfor val in RandomClusters:\n  \n  print('---------------------------------------------')\n  print('Cluster Number = ' + str(val))\n  for j in range(len(YVals)):\n    if YVals[j]==val:\n      print('----------------Question----------------')\n      print(FiltQuestionsB[j])\n      break\n    \n  for j in range(len(AnswersClasses)):\n    if AnswersClasses[j]==val:\n      print('----------------Answer----------------')\n      print(FiltAnswers[j])\n      break","9bbadd39":"def GetAuthorFrequency(Authors,UniqueAuthors):\n  \n  cAuth=Authors\n  uAuth=UniqueAuthors\n  nAuthors=len(uAuth)\n  \n  localFreq=[0 for k in range(nAuthors)]#frequencies of each author\n  localDict={}\n\n  for j in range(nAuthors):\n    localDict[uAuth[j]]=j#returns the index over localFreq\n  \n  for val in cAuth:\n    localFreq[localDict[val]]=localFreq[localDict[val]]+1\n  \n  return np.array(localFreq)\/len(cAuth)\n\ndef AuthorClusterCorrespondance(AnswerID,UniqueAuthors,AnswerClasses):\n  \n  cAnswID=AnswerID\n  uAuth=UniqueAuthors  \n  cClass=AnswerClasses\n  nAuthors=len(uAuth)\n  nData=len(cClass)\n  \n  localMatrix=np.zeros((nAuthors,optnClusters))#frequencies of each author over each cluster\n  localDict={}\n\n  for j in range(nAuthors):\n  \n    localDict[uAuth[j]]=j#returns the index over localMatrix\n    \n  for j in range(nData):\n    \n    AuthLoc=localDict[cAnswID[j]]\n    ClassLoc=cClass[j]\n    localMatrix[AuthLoc,ClassLoc]=localMatrix[AuthLoc,ClassLoc]+1\n    \n  return localMatrix\/len(YVals)\n\nAnswersAuthorID=np.array(AnswersData['answers_author_id'])\nUniqueAuthors=np.unique(AnswersAuthorID)\n\nAuthorFreq=GetAuthorFrequency(AnswersAuthorID,UniqueAuthors)\nAuthorMatrix=AuthorClusterCorrespondance(AnswersAuthorID,UniqueAuthors,AnswersClasses)\n\nplt.figure(6)\nfig,axes=plt.subplots(1,2,figsize=(10,4))\n\naxes[0].plot(AuthorFreq)\nPlotStyle(axes[0],'','Unique Authors','Answer Probability')\n\naxes[1].hist(AuthorFreq,bins=40)\naxes[1].set_yscale('log')\nPlotStyle(axes[1],'','Answer Probability','Probability Counts')\n","f874dffb":"def GetMostLikelyAuthors(ClusterNumber,UniqueAuthors,AuthorMatrix):\n  #Wrapper function for the first recomendation\n  cData=AuthorMatrix[:,ClusterNumber]\n  ind=np.argsort(cData) #sorting of the data\n  lAuthors=[UniqueAuthors[ind[-1]],UniqueAuthors[ind[-2]],UniqueAuthors[ind[-3]],UniqueAuthors[ind[-4]],UniqueAuthors[ind[-5]]]\n  \n  return lAuthors\n\ndef FindLocations(value,List):\n  \n  cval=value\n  clis=List\n  cont=[]\n    \n  for k in range(len(clis)):\n    if cval==clis[k]:\n      cont.append(k)\n      \n  return cont","e0cbbded":"QuestionID=np.array(QuestionsData['questions_id'])\nAnswerQuestionID=np.array(AnswersData['answers_question_id'])\nAnswProb=[]\n\nfor k in range(len(YVals)):\n  \n  cClust=YVals[k]\n  \n  cRec=GetMostLikelyAuthors(cClust,UniqueAuthors,AuthorMatrix)\n  locs=FindLocations(QuestionID[k],AnswerQuestionID)\n  kL=[0,0,0,0,0]\n  \n  for val in locs:\n    \n   cAid=AnswersAuthorID[val]\n   \n   if cAid==cRec[0]:\n     kL=[1,0,0,0,0]\n   elif cAid==cRec[1]:\n      kL=[0,1,0,0,0]\n   elif cAid==cRec[2]:\n      kL=[0,0,1,0,0]\n   elif cAid==cRec[3]:\n      kL=[0,0,0,1,0]\n   elif cAid==cRec[4]:\n      kL=[0,0,0,0,1]\n       \n  AnswProb.append(kL)\n\nAnswProb=np.array(AnswProb)\n\nfig,axes=plt.subplots(1,2,figsize=(10,4))\n\naxes[0].plot(AnswProb.sum(axis=0)\/len(YVals))\nPlotStyle(axes[0],'','Recomendations','Answer Probability')\n\naxes[1].plot(np.cumsum(AnswProb.sum(axis=0))\/len(YVals))\nPlotStyle(axes[1],'','Recomendations','Cumulative Answer Probability')","03b23462":"ProfesionalsData=pd.read_csv(r\"..\/input\/professionals.csv\")\nVolunteersData=ProfesionalsData.dropna()\nVolunteersData['variance'] = VolunteersData['professionals_industry'] +' ' +VolunteersData['professionals_headline']\n\nVolunteersID=np.array(VolunteersData['professionals_id'])\nVolunteersText=np.array(VolunteersData['variance'])","63b13bdf":"FiltVolunteers=[MakeTextTransform(val) for val in VolunteersText]","91dc4147":"VolunteersTokFreq=[TokenFrequencies(val,FinalToken) for val in FiltVolunteers]\nnormVolunteersData=TargetSc.transform(VolunteersTokFreq)\nEncVolunteers=encoder.predict(normVolunteersData)\nEncScaledVolunteers=EncScaler.transform(EncVolunteers)\nVolunteersClasses=RFC.predict(EncScaledVolunteers)","4f1b35bc":"def GetLikelyVolunteers(cluster,VolunteersID,VolunteersClasses):\n  \n  VolID=VolunteersID\n  VolClas=VolunteersClasses\n  cClust=cluster\n  \n  nData=len(VolID)\n  cont=[]\n  for j in range(nData):  \n\n    cVol=VolClas[j]\n    if cVol==cClust:\n      \n      cont.append(j)\n  \n  if len(cont)<=20:  \n    inxs=cont\n  else:  \n    inxs=np.random.choice(cont,20)\n    \n  vol=VolunteersID[tuple([inxs])]\n  \n  return list(vol)","37ea8c64":"def FinalRecomendation(cluster,UniqueAuthors,AuthorMatrix,VolunteersID,VolunteersClasses):\n  #Wrapper function for the final recomendation \n  cClus=cluster\n  UA=UniqueAuthors\n  AM=AuthorMatrix\n  VolID=VolunteersID\n  VolC=VolunteersClasses\n  \n  Rec0=GetMostLikelyAuthors(cClus,UA,AM)\n  Rec1=GetLikelyVolunteers(cClus,VolID,VolC)\n  \n  NonOverlapping=list(set(Rec1)-set(Rec0))\n  \n  for k in range(5):\n    Rec0.append(NonOverlapping[k])\n    \n  return Rec0","fe215e1b":"print(FinalRecomendation(3,UniqueAuthors,AuthorMatrix,VolunteersID,VolunteersClasses))","e05c005c":"The probability to get an answer for the first recommendation is around 0.06, almost twice the probability of the most prolific author, the combined probability of all the recommendations is around 0.2, almost twice the probability of all the most prolific authors combined. ","59401070":"Cropping the least frequent tokens ","5fd52f64":"Printing the then final recomendations ","c5d30329":"Recommendation of volunteers","c3c322ea":"Author characteristics","fd774050":"Wrapper for final recommendations","49778df4":"Some functions to remove some artifacts of the web scrapping ","7cfbae97":"Classification of the volunteers","48c7aeaa":"First loading the data","dfb00096":"Frequencies of the final token over the target data set and scaling of the data matrix","ae444596":"Probability to get an answer from the recommended authors ","82c4a2cc":"Filtering the merged text","baac928f":"Cleaning the text with the wrapper function","185f7f99":"**A deep autoencoder based recommendation system for CareerVillage.org**","d986d465":"Visualization of some of the clusters","718ffe2f":"Determination of the optimal number of clusters ","069b6d95":"Working with only the text.","cf2025d5":"Counting the frequencies of the token terms ","715c54fd":"*  Overview\n\nThe following kernel is an entry to the \"Data Science for Good: CareerVillage.org\" kaggle competition and describes a method to match questions to professionals motivated to answer them.\nDivided into five steps, tokenization, autoencoding, clustering, prediction and recommendation.\n\n* Method description\n\nThe proposed method is built upon the following general assumptions: questions and answers share similar terms, engagement by volunteers is proportional to the number of answers posted by them and volunteer information is enough to determinate their expertise. The following summarizes the main steps.\n\n* **Tokenization** \nFrom Standford NLP page \"Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens, often loosely referred to as terms or words\". To perform the tokenization unique words from questions_body data set are selected. Then only the nouns from that initial token are taken and each word stem is taken as the token first preprocessed token. And finally, the frequency of each token over each question is measured and only those words with a frequency over 15 (15 is referred in the code as CropFactor) times the global minimum frequency are taken as the final token. \n\n* **Autoencoding **\nUsing the final token, the frequencies over each token and each question is measured, leaving a (samples, tokens) size matrix. As the resulting data set is highly dimensional, to reduce the amount of data needed to perform the recommendation a dimensionality reduction technique known as autoencoder is used. From Wikipedia \"An autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction\". \nThe resulting high dimensional matrix is then auto-encoded with twelve layers under complete deep autoencoder, with a bottleneck size of 3, leaving a (samples,3) matrix. \n\n* **Clustering**\nWith a low dimensional data set searching for similar questions is an easier task, also known as clustering. Defined as \"the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups  (clusters). From the dimensionally reduced matrix, the number of clusters is calculated by a combination of the elbow method and the silhouette factor. That will ensure the minimum number of clusters with the highest accuracy.\n\n* **Prediction**\nBy determining the clusters in the dimensionally reduced data set, a number of labels are obtained and those labels can be used for classification. With the labels obtained from the clustering step and the encoded matrix, a random forest classifier is trained, to predict the cluster of each data point and to predict future datasets without the need of the clustering step.\n\n* **Recommendation**\nFinally, all the answers have a similar process. Token frequencies are calculated, encoded using the already pre-trained model and classified with the pre-trained random forest classifier. Then the frequency of each unique author answers a question of each predicted cluster is measured. The first recommendation function suggests five authors with the highest frequency to answer a question from a given cluster. \n\n* **Recommendation Extension**\nThe first batch of recommendations will represent the most likely volunteers that will answer a question. To extend the recommendations to those volunteers without answers, data provided by the volunteers from the professionals dataset is processed as described in the recommendation step. And five additional volunteers are suggested that based on the information provided by themselves. The final recommendation function will check for volunteer duplicity and in those cases will change it to a different volunteer.\n\n\n*  Method implementation \n\nThe intended use for the proposed method is as follows: once a question is posted over CareerVillage.org, such question is classified and the resulting label is then passed to the final recommendation function that returns ten possible volunteers to answer that question. All the trained models are generated by the use of well documented and popular machine learning libraries, additional data can be added to the models regularly.\n\n*  Method Extensibility\n\nIn terms of extensibility, the autoencoding of the questions can also be used for population segmentation for both students and volunteers. By segmenting the population new tags can be proposed, or a tag suggestion system can be implemented based on the respective population. Also, activity, time to response or some other metrics can be measured for each population to get a better awareness of the community.\n\n\n* Code\n\nFor simplicity over each code cell the markdown cell will contain a general description of the process performed by the code, and over the code cell a detailed description over each element if is needed.","694ace8e":"Deep autoencoding of the token frequencies ","e424f57f":"Merging the data over the professionals dataset","8d7bc55e":"Basic recommendation function ","1564e7e2":"Calculating the distribution of the tokens","c9464c03":"Visualization of some of the question in the generated clusters ","7b7ecaf6":"Processing of the answers data ","d1ceaf29":"Selecting only the unique words in the text","22bfdba6":"Training of a random forest classifier for further predictions ","ca40d445":"Selecting only the nouns from the text","f6a26538":"Visualization of the bottleneck representation "}}