{"cell_type":{"497af89c":"code","fefb3a53":"code","4a276725":"code","302f8f9a":"code","5ca1da20":"code","fa12938b":"code","1bee5135":"code","149239f9":"code","446cc8e0":"code","a84b7011":"code","c2a81f2b":"code","522b4a26":"code","8c6cd105":"code","60a3825f":"code","b6eecbf9":"code","f6e68491":"code","b74a3277":"code","61600aab":"code","619bc533":"markdown","4de991b7":"markdown","6a0489e0":"markdown","b1fb4e82":"markdown","9a205b50":"markdown"},"source":{"497af89c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fefb3a53":"# define dtypes on loading data  - will speed up and reduce memory use\ncategorical_dtypes = {\n    'event_type':'category', 'product_id':'category',\n    'category_id':'category',\n       'category_code':'category', 'brand':'category', \n    'user_id':'category', 'user_session':'category'\n}","4a276725":"df = pd.read_csv(\"\/kaggle\/input\/ecommerce-behavior-data-from-multi-category-store\/2019-Oct.csv\",dtype=categorical_dtypes) #,nrows=123456\nprint(df.shape)\nprint(df.columns)\nprint(df.dtypes)\ndf.tail()","302f8f9a":"## hierarchical code - we can split by digits into up to 3 levels\nprint(df[\"category_code\"].sample(frac=0.01).nunique())\ndf[\"category_code\"].value_counts()","5ca1da20":"df[\"category_code_level1\"] = df[\"category_code\"].str.split(\".\",expand=True)[0].astype('category')\n\nprint(df[\"category_code_level1\"].nunique())\ndf[\"category_code_level1\"].value_counts()","fa12938b":"df.head(9)[\"category_code\"].str.split(\".\",expand=True)[1]","1bee5135":"### setting the dtype on import to categorical doesn't work when concatenating..  + We have a lot of data in this scenario - start with just 1 month\n# df = pd.concat([pd.read_csv(\"\/ecommerce-behavior-data-from-multi-category-store\/2019-Oct.csv\",dtype=categorical_dtypes)\n#                 ,pd.read_csv(\"\/kaggle\/input\/ecommerce-behavior-data-from-multi-category-store\/2019-Nov.csv\",dtype=categorical_dtypes)])\n\ndf['event_time'] = pd.to_datetime(df['event_time'],infer_datetime_format=True)\n\n\n# add joint key for user + product  and\/or user + category\n# df[\"user_product\"] = (df['user_id'].astype(str)+df['product_id'].astype(str)).astype('category').cat.codes.astype('category')\ndf[\"user_category\"] = (df['user_id'].astype(str)+df['category_id'].astype(str)).astype('category').cat.codes.astype('category')\n\n\n## categorical\/label encoding of the IDs (instead of string - save memory\/file size):\n### we still need to define ats categoircal due to the concatenation of the input files\ndf['user_session'] = df['user_session'].astype('category').cat.codes.astype('category')\ndf['user_id'] = df['user_id'].astype('category').cat.codes.astype('category')\ndf['category_id'] = df['category_id'].astype('category').cat.codes.astype('category')\ndf['product_id'] = df['product_id'].astype('category').cat.codes.astype('category')\n\n\nprint(df.shape)\ndf.head()","149239f9":"## meory usage\ndf.info()","446cc8e0":"df[\"event_type\"].value_counts()","a84b7011":"df[\"product_id\"].value_counts().describe()","c2a81f2b":"df.sample(frac=0.02).drop([\"event_time\"],axis=1).nunique()","522b4a26":"## get all \"positive events\" , then later we'll add 0s\ndf_targets = df.loc[df[\"event_type\"].isin([\"cart\",\"purchase\"])].drop_duplicates(subset=['event_type', 'product_id',\n                                                                                        'price', 'user_id',\n                                                                                        'user_session'])\n\nprint(df_targets.shape)\ndf_targets.tail()","8c6cd105":"## not filtering by price (discount?) doesn't change much\ndf_targets.drop_duplicates(subset=['event_type', 'product_id', 'user_id', 'user_session']).shape[0]","60a3825f":"## ## could also do this with np.where  ;  or stack + inindex ; or with a join (on filtered rows containing purhcase and fillna(0))\n\n# # df2[\"purchase\"] = (df2[\"event_type\"]==\"purchase\").astype(int)\n\n# df2[\"purchase\"] = np.where(df2[\"event_type\"]==\"purchase\",1,0)\n\n## https:\/\/stackoverflow.com\/questions\/48175172\/assign-a-pandas-series-to-a-groupby-operation","b6eecbf9":"## laziest option - add a row where purchased, groupby(max), then keep rows at time of added to cart\n\n# df_targets[\"purchased\"] = (df_targets[\"event_type\"]==\"purchase\").astype(int) #np.where() ## only kept 1s - bug ..\n\ndf_targets[\"purchased\"] = np.where(df_targets[\"event_type\"]==\"purchase\",1,0)\nprint(df_targets.shape)\ndf_targets[\"purchased\"].describe()","f6e68491":"## set overseved = True to avoid memory error when using categoricals\ndf_targets[\"purchased\"] = df_targets.groupby([\"user_session\",\"product_id\"],observed=True,sort=False)[\"purchased\"].transform(\"max\")\ndf_targets[\"purchased\"].describe()","b74a3277":"# keep only rows with the time of addition to cart  = time of prediction\n## also, drop duplicates for cases of multiple purchases of same it or readding (only a small amount - a few hundred such cases)\ndf_targets = df_targets.loc[df_targets[\"event_type\"]==\"cart\"].drop_duplicates([\"user_session\",\"product_id\",\"purchased\"])\n\nprint(df_targets.shape)\ndf_targets[\"purchased\"].describe()","61600aab":"df.to_csv(\"ecom_store_timeSeries.csv.gz\",index=False,compression=\"gzip\")\n\n# output a sample of targets for modelling speed\ndf_targets.drop([\"event_type\"],axis=1).sample(frac=0.35).to_csv(\"ecom_store_cart_purchase_labels.csv.gz\",index=False,compression=\"gzip\")","619bc533":"# Export","4de991b7":"# Target:\n* For each product the user viewed  or (depending on problem definition) added to cart , get combinations and predict if there was purchase.\n* Additional possible proxy target - did user add item to cart.\n\n\n* Q: Do we want to consider multiple user purchases of the same item? If so then we would want to not drop duplicates across sessions. For now, we will assume that the defined session seperates the data ok\n\n\n    * https:\/\/stackoverflow.com\/questions\/42854801\/including-missing-combinations-of-values-in-a-pandas-groupby-aggregation\n    \n    \n* get all conbinations then fillna 0 :\n    * https:\/\/stackoverflow.com\/questions\/31786881\/adding-values-for-missing-data-combinations-in-pandas","6a0489e0":"## Target: For items added to basked: Predict if they will be purchased in that session\n\n* Different from predicting if they will be purchased in the future\n* Much less class imbalance.\n* Ignore possible target of items being deleted from session\n\n* Could also be modelled as recomender problem (+- explicit\/implicit (`lightFM`)) \/: 1 : added, -1: removed from shopping cart, 0: not purchased \"yet\" or not added to cart\n\n    * https:\/\/stackoverflow.com\/questions\/31786881\/adding-values-for-missing-data-combinations-in-pandas    * ","b1fb4e82":"## optional: Feature engineering","9a205b50":"## PREPARE Data for classification\n* Forked from version run on smaller cosmetics dataset https:\/\/www.kaggle.com\/danofer\/ecommerce-cosmetic-predict-purchases-data-prep\n* predict at time of addition to shopping cart if user will purchase a given product or not \n* this is about setting up the data & targets\n* Feature engineering will be done externally\n\n* another target possible - will an item be removed from cart ? "}}