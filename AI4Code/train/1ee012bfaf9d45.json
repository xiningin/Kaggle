{"cell_type":{"28103489":"code","c496ae13":"code","e2e4eac4":"code","c5147117":"code","c42b5084":"code","a224784a":"code","9665e060":"code","ca0bd7c5":"code","e98f27f9":"code","b611852b":"code","88a6ee04":"code","9e25922b":"code","909ec7d3":"code","32116467":"code","de9a2039":"code","6733a98e":"code","20302b82":"code","a47014f1":"code","c2713727":"code","d3237387":"code","af603698":"code","0af4170a":"code","d45c1f5f":"code","854df352":"code","5fbb2f86":"code","6e5dbf02":"code","5ce96c86":"code","c10e683d":"code","99cc89ea":"code","1c5606c4":"code","6a69bade":"code","fb6ac39a":"code","88dfde8b":"code","6f8b1165":"code","10d8263a":"code","dbfde2e3":"code","b1d054d5":"code","5beb1c58":"code","532855bf":"code","3e31da2f":"code","f6ed7e51":"code","3d014688":"code","87cf997a":"code","f8ef34ca":"code","0963178a":"code","fd01559a":"code","cfe618c9":"code","1fd24163":"code","90affb0d":"code","a0d92752":"code","659111b2":"code","c7cf43a0":"code","23896f8d":"code","c0cdb0c6":"code","9c50790d":"code","d7cd6dbc":"code","eab8cfc4":"code","b019376b":"code","878b23ef":"code","4a24f3d6":"code","86a1670d":"code","35f31889":"code","394503c2":"code","e00b8d1d":"code","5de6fdcf":"code","d3ffae07":"code","492b5e61":"code","cc3cdef2":"code","2bef0460":"code","552f6300":"code","335a5664":"code","367e5911":"code","03c0899f":"code","40fb8f95":"code","670b2ae2":"code","07495f02":"code","6024753f":"code","86f1ee09":"code","f20b6904":"code","f673c794":"code","7a5ff60b":"code","c69000d2":"code","a4c0ecf1":"code","dd966df8":"code","597d1608":"code","6334da0b":"code","65977c89":"code","e93a7826":"code","b1cf6afe":"code","fd26e543":"code","09649d12":"code","992d71c7":"code","d2960503":"code","002b57ed":"code","96a9e047":"markdown","94a9ae1c":"markdown","f1916915":"markdown","ba7a5bdc":"markdown","9dd6eaf7":"markdown","a07908f3":"markdown","51ea03e0":"markdown","5c79a00f":"markdown","05abb630":"markdown","184cbd72":"markdown","507684bd":"markdown","ab6c3764":"markdown","f754d87a":"markdown","73eb29e2":"markdown","e525f1da":"markdown","745b59ea":"markdown","5d87ac94":"markdown","8687b36d":"markdown","0ebdc5a7":"markdown"},"source":{"28103489":"import numpy as np\nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \n# Import ML Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api as sm\nimport statsmodels\nfrom sklearn.feature_selection import RFE\nfrom sklearn.preprocessing import RobustScaler\n\n# Import Data processing Libraries\nimport numpy as np\nimport pandas as pd \n\n# Import Data Visualization Libraries\nimport plotly.figure_factory as ff\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\npio.templates.default = \"plotly_dark\"\n%matplotlib inline\n\n#Ignore Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom IPython.display import display, HTML","c496ae13":"\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\ntrain.head()","e2e4eac4":"train.shape","c5147117":"test.shape","c42b5084":"train.info()","a224784a":"# Datatype count\nfig=px.bar(x=train.dtypes.value_counts().index.astype(str).to_list() ,  \n       y = train.dtypes.value_counts().to_list(),\n       color = train.dtypes.value_counts().to_list(),\n       text = train.dtypes.value_counts().to_list(),\n      )\nfig.update_traces(textposition='outside',\n                  #marker_coloraxis=None\n                 )\nfig.show()","9665e060":"train.describe()","ca0bd7c5":"train.isnull().mean()","e98f27f9":"# Dropping Cabin column (77% Missing values)\ntrain.drop('Cabin',axis =1 , inplace = True)\ntrain.head()","b611852b":"# Dropping Ticket & name as they are unique identifiers\ntrain.drop(['Name', 'Ticket'],axis =1 , inplace = True)\ntrain.head()","88a6ee04":"# Function to get all info related to categorical variables\ndef col_info(col_list):\n    d_type = []\n    unique = []\n    uni_vals = []\n    null_values = []\n    null_perc = []\n    freq_val = []\n    freq_val_perc = []\n    val_counts = []\n    for i in col_list:\n        d_type.append(str(train[i].dtype))\n        unique.append(', '.join([str(i) for i in train[i].unique().tolist()])) \n        uni_vals.append(train[i].nunique())\n        null_values.append(train[i].isnull().sum())\n        null_perc.append(round(train[i].isnull().mean()*100,2))\n        freq_val.append(train[i].mode().tolist()[0])\n        freq_val_perc.append(float(round(train[i].value_counts(normalize=True)*100,2)[train[i].mode()]))\n        val_counts.append('\\n'.join([str(i)+':- '+str(j) for i , j in zip(train[i].value_counts(dropna=False).index, train[i].value_counts(dropna=False).values)])) \n    df = pd.DataFrame({'Column Name':col_list, 'Data Type':d_type,'Unique Values':uni_vals,\n                       'NULL Values':null_values, 'NULL Perc':null_perc, 'Mode':freq_val, \n                       'Mode Perc':freq_val_perc,'Value Counts':val_counts,'Values':unique})\n    df.sort_values('NULL Values', ascending =False, inplace=True)\n    display(HTML(df.to_html(classes= ['table text-align:center table-bordered'],index=False).replace(\"\\\\n\",\"<br>\"))) ","9e25922b":"# Display info about Categorical columns\ncol_info(train.dtypes.index[train.dtypes.values == 'object'].to_list()) ","909ec7d3":"# Replace NULL values in Embarked column with Mode\n\ntrain.Embarked.fillna('S',inplace=True)\ncol_info(['Embarked'])","32116467":"# Replace NULL values in Age column with Median\n\ntrain.Age.fillna(train.Age.median(),inplace=True)\ntrain.isnull().sum()","de9a2039":"train.Pclass = train.Pclass.map({1:'Ist',2:'2nd' , 3 :'3rd'})\ntrain.Pclass.value_counts()","6733a98e":"#C = Cherbourg, Q = Queenstown, S = Southampton\ntrain.Embarked = train.Embarked.map({'C':'Cherbourg','Q':'Queenstown' , 'S' :'Southampton'})\ntrain.Embarked.value_counts()","20302b82":"# Function to generate Count subplots (Bar Plot)\ndef count_subplot(df,col_list,rows,cols,height):\n    fig = make_subplots(rows=rows, \n                        cols=cols,\n                        subplot_titles=col_list,\n                        vertical_spacing=0.13\n                       )\n    count=0\n    for i in range(1,rows+1):\n        for j in range(1,cols+1):\n            fig.add_trace(go.Bar(x=[str(i) for i in df[col_list[count]].value_counts(dropna=False).index], \n                                 y=df[col_list[count]].value_counts(dropna=False),\n                                 name=col_list[count],\n                                 textposition='auto',\n                                 text= [str(i) + '%' for i in (df[col_list[count]].value_counts(normalize=True,dropna=False)*100).round(1).tolist()],\n                            ),\n                      row=i,col=j)\n            count+=1\n    fig.update_layout(\n                    title=dict(text = \"Analyze Categorical variables (Frequency \/ Percentage)\",x=0.5,y=0.99),\n                    title_font_size=20,\n                    showlegend=False,\n                    height = height,\n                  )\n    fig.show()","a47014f1":"count_subplot(train,['Embarked', 'Sex','Pclass'],1,3,500)","c2713727":"train.head()","d3237387":"train.describe()","af603698":"cols = train.dtypes.index[train.dtypes.values != 'object'].to_list()[1:5]\n\n#Subplot initialization\nfig = make_subplots(\n                     rows=2,  \n                     cols=2,\n                     subplot_titles=cols,\n                     horizontal_spacing=0.1,\n                     vertical_spacing=0.15 \n                   )\n# Adding subplots\ncnt=0\nfor i in range(1,3):\n    for j in range(1,3):\n        fig.add_trace(go.Box( y=train[cols[cnt]], \n                            ),\n                      row=i,col=j)\n        cnt+=1 \nfig.update_layout(\n                    title=dict(text = \"Bi-Variate Analysis\",x=0.5,y=0.99),\n                    title_font_size=20,\n                    showlegend=False,\n                    height = 900,\n                  )\nfig.show()","0af4170a":"# Bivariate Analysis function\ndef bivariate(cols):\n    for col in cols:\n        count =0 \n        fig = [None]*len(cols)\n        df1= pd.DataFrame(train[col].value_counts()).reset_index()\n        df1.columns = [col,'Value Counts']\n        df2 =train.groupby([col]).sum()[['Survived']].reset_index()\n        df3 = train[train['Survived']==0].groupby([col]).count()[['Survived']].reset_index() \n        df3.columns = [col, 'Not Survived']\n        df = pd.merge(df1,df2,on=col)\n        df = pd.merge(df,df3,on=col)\n        fig[count] =go.Figure()\n        fig[count].add_traces(go.Bar(\n                 x= df[col],\n                 y= df.Survived,\n                 marker={'color' : '#00FF00'},\n                 name = 'Survived',\n                 text=(df.Survived\/df['Value Counts']*100).apply(lambda x : str(round(x))+'%'),\n                 textposition='outside'\n                ) \n              )\n        fig[count].add_traces( go.Bar(\n                 x= df[col],\n                 y= df['Value Counts'],\n                 marker= dict (color ='#9399FF' ),\n                 name = 'Value Counts',\n                 text=df['Value Counts'],\n                 textposition='outside'\n                )\n              )\n        fig[count].add_traces(go.Bar( \n                 x= df[col],\n                 y= df['Not Survived'],\n                 marker={'color' : '#FF3300'},\n                 name = 'Not Survived',\n                 text=(df['Not Survived']\/df['Value Counts']*100).apply(lambda x : str(round(x))+'%'),\n                 textposition='outside'\n                ) \n              )\n        fig[count].update_layout(\n                     title=dict(text = col + \" VS Survived\",x=0.5,y=0.95,font_size=20), \n                     barmode = 'group',\n                     height=550,\n                    # width = 1480,\n                     xaxis_tickangle=-45,\n                     xaxis_tickfont_size=12,\n                     xaxis=dict(\n                                 title=col,\n                                 titlefont_size=16,\n                                 tickfont_size=14,\n                               ), \n                     legend=dict(x=.9, y=.97,font_size=16),\n                    )\n        fig[count].show()\n        count+=1","d45c1f5f":"bivariate(train.select_dtypes(exclude =[\"number\"]).columns.values)","854df352":"train.Sex = train.Sex.map({'male':1,'female':0})\ntrain.head()","5fbb2f86":"dummy = pd.get_dummies(train[train.select_dtypes(exclude =[\"number\"]).columns.to_list()],drop_first=True)\ndummy.head()","6e5dbf02":"train = pd.concat([train,dummy],axis=1)\ntrain.head()","5ce96c86":"# Dropping Pclass , Embarked & Sex Columns\ntrain.drop(['Pclass', 'Embarked'],axis = 1, inplace = True)\ntrain.head()","c10e683d":"df_train=train.copy()\ndf_train","99cc89ea":"df_train.head()","1c5606c4":"scaler = StandardScaler() # Initialize a Standard Scaler object\nnum_cols = ['Age', 'Fare'] # Numerical Vaeriables \ndf_train[num_cols]\ndf_train[num_cols] = scaler.fit_transform(df_train[num_cols])\ndf_train[num_cols]","6a69bade":"PassengerId_train = df_train.pop('PassengerId')","fb6ac39a":"df_train.head()","88dfde8b":"# Heatmap\nfig=ff.create_annotated_heatmap(np.array(df_train.corr().round(2).fillna(0)),\n                            x=df_train.corr().columns.to_list(),\n                            y=df_train.corr().columns.to_list(), \n                            colorscale='bluyl',\n                            showscale=True) \n\nfig.update_layout(title=dict(text='Train Data Heatmap',x=0.01,y=0.97,font_color='red'),height=800,width=800)\nfig.show()","6f8b1165":"# Feature importance by Correlation Coefficient\nfig=px.bar(df_train.corr().loc['Survived',:].apply(lambda x: abs(x)).sort_values(ascending=False).head(21)[1:],\n       title='Feature importance by Correlation Coefficient',\n       color = df_train.corr().loc['Survived',:].apply(lambda x: abs(x)).sort_values(ascending=False).head(21)[1:],\n       text = df_train.corr().loc['Survived',:].apply(lambda x: abs(x)).sort_values(ascending=False).round(2).head(21)[1:] \n      ) \nfig.update_traces(textposition='outside',marker_coloraxis=None)\nfig.update_layout(height = 650)\nfig.show()","10d8263a":"# X Train & Y train\ny_train=df_train.pop('Survived')\nX_train=df_train","dbfde2e3":"y_train.head(5)","b1d054d5":"X_train.head(5)","5beb1c58":"logml = sm.GLM(y_train,sm.add_constant(X_train),family=sm.families.Binomial())\nlogml.fit().summary()","532855bf":"# VIF\nvif=pd.DataFrame()\nvif['Features']=X_train.columns\nvif['VIF']=[variance_inflation_factor(X_train.values,i) for i in range(X_train.shape[1])]\nvif['VIF'] = vif['VIF'].apply(lambda x: round(x,2))\nvif= vif.sort_values(by='VIF',ascending=False)\nvif","3e31da2f":"# Dropping Embarked_Queenstown due to high P-Value\nX_train.drop('Embarked_Queenstown',axis=1,inplace=True)","f6ed7e51":"logml = sm.GLM(y_train,sm.add_constant(X_train),family=sm.families.Binomial())\nlogml.fit().summary()","3d014688":"# VIF\nvif=pd.DataFrame()\nvif['Features']=X_train.columns\nvif['VIF']=[variance_inflation_factor(X_train.values,i) for i in range(X_train.shape[1])]\nvif['VIF'] = vif['VIF'].apply(lambda x: round(x,2))\nvif= vif.sort_values(by='VIF',ascending=False)\nvif","87cf997a":"# Dropping Parch due to high P-Value\nX_train.drop('Parch',axis=1,inplace=True)","f8ef34ca":"logml = sm.GLM(y_train,sm.add_constant(X_train),family=sm.families.Binomial())\nlogml.fit().summary()","0963178a":"# Dropping Fare due to high P-Value\nX_train.drop('Fare',axis=1,inplace=True)","fd01559a":"logml = sm.GLM(y_train,sm.add_constant(X_train),family=sm.families.Binomial())\nres = logml.fit()\nres.summary()","cfe618c9":"# Relative Feature importance\npred =  ((res.params[1:]\/res.params[1:].max())*100).sort_values(ascending=True)\nfig =px.bar(data_frame=pred,x=pred.values,y=pred.index,color=pred.values) \nfig.update_xaxes(title='Feature Importance') \nfig.update_yaxes(title='Features')\nfig.update_layout(\n                        title=dict(text = 'Relative Feature Importance',x=0.5,y=0.95),\n                        title_font_size=20,\n                        showlegend=False,\n                        height =600,\n                        width = 1200\n                      )","1fd24163":"X_train_sm= sm.add_constant(X_train)\ny_train_pred = res.predict(X_train_sm) \ny_train_pred.head(10)","90affb0d":"y_train_pred_final = pd.DataFrame({'PassengerId':PassengerId_train,'Survived':y_train,'Survived_Prob':y_train_pred})\ny_train_pred_final","a0d92752":"y_train_pred_final['predicted'] = y_train_pred_final['Survived_Prob'].apply(lambda x : 1 if x >0.5 else 0 )\ny_train_pred_final.head(10)","659111b2":"confusion = metrics.confusion_matrix(y_train_pred_final.Survived,y_train_pred_final.predicted)\nprint(confusion)","c7cf43a0":"print(metrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.predicted))","23896f8d":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","c0cdb0c6":"TP,TN,FP,FN ","9c50790d":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","d7cd6dbc":"# Let us calculate specificity\nTN \/ float(TN+FP)","eab8cfc4":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","b019376b":"# positive predictive value \nprint (TP \/ float(TP+FP))","878b23ef":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","4a24f3d6":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs, drop_intermediate = False )\n    fig=px.line(x=fpr,y=tpr)\n    auc_score =metrics.roc_auc_score(actual, probs)\n    fig.update_xaxes(title='False Positive Rate') \n    fig.update_yaxes(title='True Positive Rate')\n    fig.update_traces(line=dict( width=2.5,color='orange'))\n    fig.add_annotation(x=0.81,\n                   y=0.04,\n                   text=\"<b>ROC Curve (Area = %0.2f)<\/b>\"%auc_score,\n                   font=dict(size=16,color=\"black\"),\n                   bordercolor=\"#c7c7c7\",\n                   borderwidth=2,\n                   borderpad=4,\n                   bgcolor=\"#ff7f0e\",\n                   opacity=0.8,\n                   showarrow=False)\n    fig.update_layout(\n                    height = 700,\n                    width= 700,\n                    title=dict(text = \"ROC Curve\",x=0.5,y=0.95), \n                  )\n    fig.show()","86a1670d":"draw_roc(y_train_pred_final.Survived, y_train_pred_final.Survived_Prob)","35f31889":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","394503c2":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","e00b8d1d":"# Let's plot accuracy sensitivity and specificity for various probabilities.\nfig = px.line(cutoff_df,x='prob', y=['accuracy','sensi','speci'])\nfig.update_layout(\n                    height = 700,\n                    width= 700\n                  )\nfig.show()","5de6fdcf":"y_train_pred_final['final_predicted'] = y_train_pred_final.Survived_Prob.map( lambda x: 1 if x > 0.366 else 0)\n\ny_train_pred_final.head()","d3ffae07":"# Let's check the overall accuracy.\ntrain1 = pd.DataFrame(columns=['Data Set','Measure','Value'])\ntrain1.loc[len(train1.index)] = ['Train Data Set','Accuracy', metrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.final_predicted) ]\n\nconfusion1 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.final_predicted )\n\nTP = confusion1[1,1] # true positive \nTN = confusion1[0,0] # true negatives\nFP = confusion1[0,1] # false positives\nFN = confusion1[1,0] # false negatives\n\n\n# Let's see the sensitivity of our logistic regression model\ntrain1.loc[len(train1.index)] = ['Train Data Set','Sensitivity', TP \/ float(TP+FN)]\n\n\n\n# Let us calculate specificity\ntrain1.loc[len(train1.index)] = ['Train Data Set','Specificity', TN \/ float(TN+FP)] \n\n\n# Calculate false postive rate - predicting churn when customer does not have churned\ntrain1.loc[len(train1.index)] = ['Train Data Set','False Positive Rate', FP\/ float(TN+FP)] \n\n\n# Positive predictive value \ntrain1.loc[len(train1.index)] = ['Train Data Set','Positive predictive value', TP \/ float(TP+FP)] \n\n\n# Negative predictive value\ntrain1.loc[len(train1.index)] = ['Train Data Set','Negative predictive value', TN \/ float(TN+ FN)] \n\ntrain1","492b5e61":"# Precision & Recall\n\ntrain1.loc[len(train1.index)] = ['Train Data Set','Precision', TP \/ (TP + FP)]  \n \n\ntrain1.loc[len(train1.index)] = ['Train Data Set','Recall',TP \/ (TP + FN)] \ntrain1 ","cc3cdef2":"test","2bef0460":"test.isnull().sum()","552f6300":"test.Age.fillna(test.Age.median(),inplace=True)\ntest.isnull().sum()","335a5664":"train.head()","367e5911":"test.drop(['Cabin', 'Name','Ticket'],axis=1,inplace=True)\ntest.head()","03c0899f":"test.isnull().sum()","40fb8f95":"test.Fare.fillna(test.Fare.median(),inplace=True)\ntest.isnull().sum()","670b2ae2":"test.head()","07495f02":"train.head()","6024753f":"test.Sex = test.Sex.map({'male':1,'female':0})\ntest.head()","86f1ee09":"test.Pclass = test.Pclass.map({1:'Ist',2:'2nd' , 3 :'3rd'})\ntest.Pclass.value_counts()\n\n\n#C = Cherbourg, Q = Queenstown, S = Southampton\ntest.Embarked = test.Embarked.map({'C':'Cherbourg','Q':'Queenstown' , 'S' :'Southampton'})\ntest.Embarked.value_counts()\n\n\ndummy = pd.get_dummies(test[test.select_dtypes(exclude =[\"number\"]).columns.to_list()],drop_first=True)\ndummy.head()","f20b6904":"test = pd.concat([test,dummy],axis=1)\ntest.head()","f673c794":"test.drop(['Pclass', 'Embarked'],axis=1,inplace=True)\ntest.head()","7a5ff60b":"df_test = test.copy()","c69000d2":"num_cols = ['Age', 'Fare'] # Numerical Vaeriables \ndf_test[num_cols] = scaler.transform(df_test[num_cols])\ndf_test[num_cols]","a4c0ecf1":"df_test=df_test[df_train.columns]\ndf_test","dd966df8":"test_PassengerId = test.PassengerId\ntest_PassengerId","597d1608":"X_test_sm = sm.add_constant(df_test)\nX_test_sm","6334da0b":"y_test_pred = res.predict(X_test_sm)\ny_test_pred[:10]","65977c89":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","e93a7826":"# Let's see the head\ny_pred_1.head()","b1cf6afe":"# Putting CustID to index\ny_test_df = pd.DataFrame()\ny_test_df['PassengerId'] = test_PassengerId\ny_pred_final = pd.concat([y_test_df,y_pred_1],axis=1)\ny_pred_final.head()","fd26e543":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Survived_Prob'})\ny_pred_final","09649d12":"y_pred_final['Survived'] = y_pred_final.Survived_Prob.map(lambda x: 1 if x > 0.366 else 0)\ny_pred_final.head()","992d71c7":"submission = y_pred_final[['PassengerId','Survived']]\nsubmission","d2960503":"submission.to_csv('submission.csv',index = False)","002b57ed":"#End","96a9e047":"### Model Accuracy","94a9ae1c":"### Confusion Matrix","f1916915":"### Model-4","ba7a5bdc":"### Converted VS predicted probability","9dd6eaf7":"### Binary Encoding","a07908f3":"### One-Hot Encoding on Categorical Variables","51ea03e0":"### Model-3","5c79a00f":"### Exploring Data","05abb630":"### Model-2","184cbd72":"### Model -1","507684bd":"###  Plotting the ROC Curve","ab6c3764":"### Train Model","f754d87a":"### Metrics beyond simply accuracy","73eb29e2":"### Finding Optimal Cutoff Point","e525f1da":"##### From the curve above, 0.37 is the optimum point to take it as a cutoff probability.","745b59ea":"### Making Prediction on Test Data","5d87ac94":"### Data understanding and preparation","8687b36d":"### Load Data","0ebdc5a7":"### Bivariate Analysis"}}