{"cell_type":{"9721a08b":"code","ae4c5e9b":"code","6471d9ed":"code","b339b191":"code","22dc697d":"code","b4b99aa8":"code","95fe6959":"code","1ff0542b":"code","b73befdf":"code","fb949951":"code","65db5572":"code","2336e28f":"code","217b96ba":"code","2ec28a28":"code","42247da5":"code","b4d6c3e3":"code","d1eea887":"code","4f65359a":"code","e2b20191":"code","d733dfaa":"code","e427b846":"code","d97952da":"code","a0c53c30":"code","7ba67412":"code","1d03576b":"markdown","1571ff8e":"markdown","7314d316":"markdown","b11bd2d0":"markdown","236ae286":"markdown","1db98f6f":"markdown","69db8a1c":"markdown"},"source":{"9721a08b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae4c5e9b":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","6471d9ed":"%matplotlib inline","b339b191":"wine = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","22dc697d":"wine.head()","b4b99aa8":"wine.info()","95fe6959":"wine.describe()","1ff0542b":"from sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, classification_report, r2_score","b73befdf":"X = wine.drop('quality',axis=1)\ny = wine['quality']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","fb949951":"lr = LinearRegression()\nlr.fit(X_train,y_train)\npredict_lr = lr.predict(X_test)\n\nprint(r2_score(y_test,predict_lr))","65db5572":"log_r = LogisticRegression()\nlog_r.fit(X_train,y_train)\npredict_logr = log_r.predict(X_test)\n\nprint(confusion_matrix(y_test,predict_logr))\nprint(\"\\n\")\nprint(classification_report(y_test,predict_logr))","2336e28f":"dtc = DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\npredict_dtc = dtc.predict(X_test)\n\nprint(confusion_matrix(y_test,predict_dtc))\nprint(\"\\n\")\nprint(classification_report(y_test,predict_dtc))","217b96ba":"rfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(X_train,y_train)\npredict_rfc = rfc.predict(X_test)\n\nprint(confusion_matrix(y_test,predict_rfc))\nprint(\"\\n\")\nprint(classification_report(y_test,predict_rfc))","2ec28a28":"kn = KNeighborsClassifier(n_neighbors =1)\nkn.fit(X_train,y_train)\npredict_k = kn.predict(X_test)\n\nprint(confusion_matrix(y_test,predict_k))\nprint(\"\\n\")\nprint(classification_report(y_test,predict_k))","42247da5":"error_rate = []\nfor i in range(1,40):\n    k = KNeighborsClassifier(n_neighbors = i)\n    k.fit(X_train,y_train)\n    predict_i = k.predict(X_test)\n    error_rate.append(np.mean(predict_i != y_test))","b4d6c3e3":"plt.figure(figsize=(10,7))\nplt.plot(range(1,40),error_rate,color='blue',linestyle='dashed',marker='o',markerfacecolor='red',markersize=10)","d1eea887":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train,y_train)\npredict_knn = kn.predict(X_test)\n\nprint(confusion_matrix(y_test,predict_knn))\nprint(\"\\n\")\nprint(classification_report(y_test,predict_knn))","4f65359a":"svm = SVC()\nsvm.fit(X_train,y_train)\npredict_svm = svm.predict(X_test)\n\nprint(confusion_matrix(y_test,predict_svm))\nprint(\"\\n\")\nprint(classification_report(y_test,predict_svm))","e2b20191":"# grid Search","d733dfaa":"param_grid = {'C':[0.1,1,10,100,1000],'gamma':[1,0.1,0.01,0.001,0.0001]}","e427b846":"grid = GridSearchCV(SVC(),param_grid,verbose=3)\ngrid.fit(X_train,y_train)","d97952da":"grid.best_params_","a0c53c30":"grid.best_score_","7ba67412":"predict_grid = grid.predict(X_test)\n\nprint(confusion_matrix(y_test,predict_grid))\nprint(\"\\n\")\nprint(classification_report(y_test,predict_grid))","1d03576b":"# K Nearest Neighbors","1571ff8e":"# Linear Regression","7314d316":"# Logistic Regression","b11bd2d0":"# Accuracy of RandomForest is better compared to other models","236ae286":"# Random Forest","1db98f6f":"# Decision Tree","69db8a1c":"# Support Vector Machine"}}