{"cell_type":{"bd5d1b55":"code","f81081fc":"code","9236e0a7":"code","a5508ccc":"code","0a639692":"code","32af0817":"code","b71dce41":"code","7047100f":"code","8eb01a2d":"code","4a68bd20":"code","f8ea5653":"code","e3d8b67a":"code","14d3bbe8":"code","e8949b3b":"code","ee27fc8a":"code","401b9f04":"code","dcce8071":"code","576a26a0":"code","9abd3529":"code","aee354ac":"code","7ae90f1f":"code","1ff0a57e":"code","da08c3f4":"code","6c5b4925":"code","b214532e":"code","fb8d0831":"code","81625e14":"code","cf95f36f":"code","6b39fc4b":"code","963ec486":"code","45c66cf0":"code","271ddbcf":"code","391a4be7":"code","31ae1bd6":"code","f52b72f2":"code","6cc1a025":"code","2ab76acf":"code","fa874ac7":"code","dbbef62a":"code","10d41869":"code","db3cccb4":"code","3a217544":"code","7a27f775":"markdown","e9872836":"markdown","04057f31":"markdown","ff4ccca0":"markdown","7bd9c2e8":"markdown","b67e72fe":"markdown","89653861":"markdown","19c33c4b":"markdown","2f91b446":"markdown","988e9362":"markdown","13767ac8":"markdown","e0489f7c":"markdown","cd77c0f3":"markdown","5b95fcaf":"markdown","a5844d3a":"markdown","b0a59eac":"markdown","06cb5499":"markdown","6c2ac46a":"markdown","41a44091":"markdown","3268ec33":"markdown","9210133a":"markdown","fe1f6c95":"markdown","dd08aaf6":"markdown","1244ec09":"markdown","f206d8c1":"markdown","94e4754b":"markdown","ba946a43":"markdown","30a90f27":"markdown","4708353f":"markdown","c0adc71e":"markdown","7c117422":"markdown","23434cbb":"markdown","62b86ec5":"markdown","8da055ea":"markdown","4a5f7f89":"markdown","2588b15a":"markdown","6c5bfc09":"markdown","caaecdbb":"markdown","c6aa2629":"markdown","55ef7213":"markdown","fbae55d4":"markdown","5a28f290":"markdown","664e18ac":"markdown","8cc8b06f":"markdown"},"source":{"bd5d1b55":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# les arrays sont batis avec les dimensions suivantes :\n# pluie , arroseur , watson , holmes\n# et chaque dimension : faux , vrai\n\nprob_pluie = np.array ( [ 0.8,0.2] ).reshape( 2 , 1 , 1 , 1 )\nprint( \"Pr( Pluie ) ={}\\n\".format( np.squeeze ( prob_pluie ) ) )\nprob_arroseur = np.array ( [0.9,0.1] ).reshape( 1 , 2 , 1 , 1 )\nprint( \"Pr ( Arroseur ) ={}\\n\".format( np.squeeze(prob_arroseur) ) )\nwatson = np.array( [ [0.8,0.2] , [ 0 , 1 ] ] ) .reshape ( 2 , 1 , 2 , 1 )\nprint( \"Pr (Watson | Pluie ) ={}\\n\" . format ( np.squeeze( watson ) ) )\nholmes = np.array([[1,0],[0.1,0.9],[0,1],[0,1]]).reshape(2,2,1,2)\nprint( \"Pr ( Holmes | Pluie ,arroseur ) ={}\\n\" .format ( np.squeeze( holmes ) ) )\nwatson [ 0 , : , 1 , : ] # prob watson mouille \u2212 plui e\nholmes [ 0 , 1 , 0 , 1 ] # prob gazon holmes mouille si a r r o s e u r \u2212 p l u i e","f81081fc":"#### 1-a:\n'''P(W=1)'''\nP_W=(watson*prob_pluie).sum(0).squeeze( )[ 1 ] # prob gazon watson mouille\nprint( \"Pr(W = 1) = {}\\n\".format( P_W ) )","9236e0a7":"#### 1-b:\nprob_H=(holmes*prob_pluie*prob_arroseur).sum(0).sum(0).squeeze()[1]","a5508ccc":"'''P(W=1|H=1)'''\nP_WH=(watson * holmes * prob_pluie*prob_arroseur).sum(0).sum(0)[1,1]\/prob_H\nprint( \"Pr(W = 1|H = 1) = {}\\n\".format( P_WH ) )","0a639692":"#### 1-c:\n'''P(W=1,H=1,A=0)'''\nP1=(holmes*prob_pluie*prob_arroseur*watson).sum(0).squeeze()[0,1,1]\n'''P(H=1,A=0)'''\nP2=(holmes*prob_pluie*prob_arroseur*watson).sum(0).sum(1).squeeze()[0,1]\n'''P(W=1|H=1,A=0)'''\nprint( \"Pr(W = 1|H = 1, A = 0) = {}\\n\".format( np.squeeze(P1\/P2) ) )","32af0817":"#### 1-d:\n'''P(W=1|A=0)'''\n#independent donc p(W=1)\nP_W=(watson*prob_pluie).sum(0).squeeze( )[ 1 ] # prob gazon watson m o uill e\nprint( \"Pr(W = 1|H = 1, A = 0) = {}\\n\".format( np.squeeze(P_W) ) )","b71dce41":"P_W_A=(holmes*prob_pluie*prob_arroseur*watson).sum(0).sum(2).squeeze()[0,1]\nP_A=prob_arroseur.squeeze()[0]\nprint( \"Pr(W = 1|H = 1, A = 0) = {}\\n\".format( np.squeeze(P_W_A\/P_A) ) )","7047100f":"#### 1-e:\n'''P(W = 1|P = 1)'''\nprint( \"Pr(W = 1|P = 1) = {}\\n\".format( watson[1,:,1,:].squeeze() ) )","8eb01a2d":"\"\"\" Import Libraries \"\"\"\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split","4a68bd20":"\"\"\" Import Data \"\"\"\ndigits= datasets. load_digits( )\n\nX = digits.data\ny = digits.target","f8ea5653":"\"\"\" Shuffle Data \"\"\"\n#import Shuffle and Seed from Random Library \nfrom random import shuffle,seed\n#Set seed\nseed(1)\n#Shuffle Data\nind_list = [i for i in range(len(y))]\nshuffle(ind_list)\nX  = X[ind_list, :,]\ny = y[ind_list,]","e3d8b67a":"y_one_hot = np.zeros ( ( y.shape [ 0 ] , len ( np.unique ( y ) ) ) )\ny_one_hot [ np.arange ( y.shape [ 0 ] ) , y ] = 1 # one hot target or shape NxK","14d3bbe8":"\"\"\" Ajout de colonne remplie de 1 \u00e0 X et initialisation de W avec des valeurs al\u00e9atoires faibles \"\"\"\nprint(\"La dimension de X avant d'ajouter la colonne des biais est \",X.shape)\n#Add Biais column\nX = np.hstack((np.ones((X.shape[0],1)),X))    \nprint(\"La dimension de X apr\u00e8s l'ajout de la colonne des biais est \",X.shape)\n\n\n\"\"\" Initialiser les valeurs de W al\u00e9atoirement (loi Normale) \"\"\"\n#W c'est theta dans ce cas \nW = np.random.normal ( 0 , 0.01 , ( len ( np.unique ( y ) ) , X.shape [ 1 ] ) ) # weights of shape Kx(L+1)","e8949b3b":"X_train , X_test , y_train , y_test = train_test_split(X, y_one_hot , test_size =0.3 , random_state=42)\nX_test , X_validation , y_test , y_validation = train_test_split ( X_test , y_test , test_size =0.5 , random_state=42)","ee27fc8a":"def softmax ( x ) :\n    \"\"\"Sotmax function \"\"\"\n    # assurez vous que la fonction est numeriquement stable\n    # e.g . softmax (np.array( [ 1000 , 10000 , 100000] , ndmin=2) )\n    p = np.zeros(x.shape)\n    m=np.max(x,axis=1)\n    for i in range(x.shape[0]):\n        p[i,:]  = np.exp(x[i,:]-m[i])\n        p[i,:] \/= np.sum(p[i,:])\n    return(p)","401b9f04":"\"\"\" Test demand\u00e9 pour la stabilit\u00e9 \"\"\"\nsoftmax(np.array( [ 1000 , 10000 , 100000] , ndmin=2))","dcce8071":"def get_accuracy (X, y , W) :\n    \"\"\" Donne la pr\u00e9cision \"\"\"\n    y_proba=softmax(np.dot(X,np.transpose(W)))\n    y_test=np.zeros(y_proba.shape)\n    y_test[np.arange ( y_test.shape [ 0 ] ) , np.argmax(y_proba, axis=1) ] = 1 \n    return(float(np.sum(np.sum(y_test==y,axis=1)==W.shape[0]))\/X.shape[0])","576a26a0":"def get_grads ( y , y_pred , X) :\n    #y_pred est la prediction en probibilit\u00e9 \n    dJ=np.dot(np.transpose(X),(y_pred-y))\/X.shape[0]\n    return(dJ)","9abd3529":"#0*np.log(0)","aee354ac":"def get_loss ( y , y_pred ) :\n    #pour des raisons de stabilit\u00e9, on remplace les prob egale 0 et 1 par epsilon et 1-epsilon\n    y_pred[y_pred>1-1.0e-5]=1-1.0e-5\n    y_pred[y_pred<1.0e-5]=1.0e-5\n    J=-np.sum(np.log(y_pred)*y)\n    return(J)","7ae90f1f":"def train_model(nb_epochs, minibatch_size, lr,W):\n    losses = [ ]\n    val_losses=[]\n    accuracies = [ ]\n    best_W = None\n    best_accuracy = 0\n    for epoch in range ( nb_epochs ) :\n        loss = 0\n        accuracy = 0\n        for i in range ( 0 , X_train.shape [ 0 ] , minibatch_size ):\n            if i+minibatch_size>X_train.shape [ 0 ]:\n                end=X_train.shape [ 0 ]\n            else:\n                end=i+minibatch_size\n\n            logits = np.dot(X_train[i:end,:],np.transpose(W))\n            y_pred = softmax(logits)\n            loss+= get_loss(y_train[i:end,:],y_pred)  \n            W=W-lr*np.transpose(get_grads(y_train[i:end,:],y_pred, X_train[i:end,:]))\n   \n        losses.append ( loss\/y_train.shape[0] ) # compute the l o s s on the t r a i n s e t\n        val_losses.append(get_loss(y_validation,softmax(np.dot(X_validation, np.transpose(W))))\/y_validation.shape[0] )\n        accuracy = get_accuracy(X_validation,y_validation,W)\n        accuracies.append ( accuracy ) # compute the acc u r a c y on the v a l i d a t i o n s e t\n        if accuracy > best_accuracy :\n            best_accuracy=accuracy\n            best_W=W# s e l e c t the b e s t p a r ame te r s based on the v a l i d a t i o n ac cu r ac y\n    return(best_W,losses,val_losses)\n","1ff0a57e":"lr = 0.001\nnb_epochs = 50\nminibatch_size = len ( y ) \/\/ 20\nbest_W,losses,val_losses=train_model(nb_epochs, minibatch_size, lr,W)","da08c3f4":"accuracy_on_unseen_data = get_accuracy ( X_test , y_test , best_W )\nprint ( accuracy_on_unseen_data ) # 0.897506925208","6c5b4925":"plt.imshow (best_W [ 4 , 0: 64].reshape ( 8 , 8 ) )\nplt.show()","b214532e":"plt.plot ( losses ,color=\"blue\")\nplt.plot(val_losses,color=\"orange\")\nplt.legend(('train', 'validation'),loc='upper right')\nplt.title('Loss function')\nplt.xlabel(\"epochs\")\nplt.ylabel(\"negative log likelihood\")\nplt.show()","fb8d0831":"print(\"La moyenne des poids associ\u00e9s aux dimensions contenant des vraies donn\u00e9es: \" ,np.mean(best_W))\nprint(\"La variance des poids associ\u00e9s aux dimensions contenant des vraies donn\u00e9es: \",np.var(best_W))","81625e14":"lrs=[0.1, 0.01, 0.001]\nminibatch_sizes=[len(y), len(y)\/\/20, len(y)\/\/200, len(y)\/\/1000]\nfor lr in lrs:\n    for minibatch_size in minibatch_sizes:\n        W = np.random.normal ( 0 , 0.01 , ( len ( np.unique ( y ) ) , X.shape [ 1 ] ) ) #initialize weights\n        best_W,losses,val_losses=train_model(nb_epochs, minibatch_size, lr,W)\n        accuracy_on_unseen_data = get_accuracy ( X_test , y_test , best_W )\n        legende=\"lr:\"+str(lr)+\" & minibatch_size:\"+str(minibatch_size)\n        print(legende)\n        print (\"l'accaracy est: \", accuracy_on_unseen_data ) \n        plt.imshow (best_W [ 4 , 0: 64].reshape ( 8 , 8 ) )\n        plt.show()\n        plt.plot ( losses ,color=\"blue\")\n        plt.plot(val_losses,color=\"orange\")\n        plt.legend(('train', 'validation'),loc='upper right')\n        plt.title('Loss function')\n        plt.xlabel(\"epochs\")\n        plt.ylabel(\"negative log likelihood\")\n        plt.show()\n\n","cf95f36f":"\"\"\" Load Dataset \"\"\"\ndigits = datasets. load_digits( )\n\nX = digits.data\ny = digits.target\ny_one_hot = np.zeros ( ( y.shape [ 0 ] , len ( np.unique ( y ) ) ) )\ny_one_hot [ np.arange ( y.shape [ 0 ] ) , y ] = 1 # one hot t a r g e t o r shape NxK","6b39fc4b":"\"\"\" Ajout des 8 dimensions \"\"\"\ndimensions = np.random.uniform ( np.min(X),np.max(X), (X.shape[0],8 ) ) # add 8 random uniform dimensions  \nX=np.hstack((X,dimensions))","963ec486":"\"\"\" Ajout de la colonne du biais \"\"\"\nX = np.hstack((np.ones((X.shape[0],1)),X))","45c66cf0":"\"\"\" Split Train Validation Test \"\"\"\nX_train , X_test , y_train , y_test = train_test_split(X, y_one_hot , test_size =0.3 , random_state=42)\n\nX_test , X_validation , y_test , y_validation = train_test_split ( X_test , y_test , test_size =0.5 , random_state=42)","271ddbcf":"\"\"\" Initialiser les poids W \"\"\"\nW = np.random.normal ( 0 , 0.01 , ( len ( np.unique ( y ) ) , X.shape [ 1 ] ) )","391a4be7":"\"\"\" Choix des hyperparametres \"\"\"\nnb_examples=X.shape[0]\nlr = 0.001\nnb_epochs = 50\nminibatch_size = len ( y ) \/\/ 20\n\n\"\"\" La fonction Softmax est rest\u00e9e intacte \"\"\"\ndef softmax ( x ) :\n    p = np.zeros(x.shape)\n    m=np.max(x,axis=1)\n    for i in range(x.shape[0]):\n        p[i,:]  = np.exp(x[i,:]-m[i])\n        p[i,:] \/= np.sum(p[i,:])\n    return(p)\n         \n    \n\"\"\" La fonction Accuracy reste la m\u00eame \"\"\"\ndef get_accuracy (X, y , W) :\n    y_proba=softmax(np.dot(X,np.transpose(W)))\n    y_test=np.zeros(y_proba.shape)\n    y_test[np.arange ( y_test.shape [ 0 ] ) , np.argmax(y_proba, axis=1) ] = 1 \n    return(float(np.sum(np.sum(y_test==y,axis=1)==W.shape[0]))\/X.shape[0])","31ae1bd6":"\"\"\" Calcule le gradient de la fonction Loss \"\"\"\ndef get_grads ( y , y_pred , X,alpha, beta,W) :\n    # y_pred est la prediction en probibilit\u00e9 \n    # On met les poids du biais \u00e0 zero car on ne le prend pas en consid\u00e9ration \n    W=W.copy()\n    W[:,0]=0\n    dJ=np.dot(np.transpose(X),(y_pred-y))\/X.shape[0]\n    dJ+=(alpha*np.transpose(W)*2+beta*np.sign(np.transpose(W)))*X.shape[0]\/nb_examples\n    return(dJ)\n\"\"\" Calcul la fonction Loss \"\"\"\ndef get_loss ( y , y_pred ,alpha,beta,W) :\n    #pour des raisons de stabilit\u00e9, on remplace les prob egale 0 et 1 par des proba plus petite \n    y_pred[y_pred>1-1.0e-5]=1-1.0e-5\n    y_pred[y_pred<1.0e-5]=1.0e-5\n    J=-np.sum(np.log(y_pred)*y)\n    W=W.copy()\n    W[:,0]=0\n    # On rajoute le terme de la regularisation \n    J=J+alpha*np.sum(W**2)+beta*np.sum(np.absolute(W))\n    return(J)","f52b72f2":"def train_model(nb_epochs, minibatch_size, lr,W,alpha,beta):\n    losses = [ ]\n    val_losses=[]\n    accuracies = [ ]\n    best_W = None\n    best_accuracy = 0\n    for epoch in range ( nb_epochs ) :\n        loss = 0\n        accuracy = 0\n        for i in range ( 0 , X_train.shape [ 0 ] , minibatch_size ):\n            if i+minibatch_size>X_train.shape [ 0 ]:\n                end=X_train.shape [ 0 ]\n            else:\n                end=i+minibatch_size\n\n\n            logits = np.dot(X_train[i:end,:],np.transpose(W))\n            y_pred = softmax(logits)\n            loss+= get_loss(y_train[i:end,:],y_pred,alpha,beta,W)  \n            W=W-lr*np.transpose(get_grads(y_train[i:end,:],y_pred, X_train[i:end,:],alpha,beta,W))\n   \n        losses.append ( loss\/y_train.shape[0] ) # compute the l o s s on the t r a i n s e t\n        val_losses.append(get_loss(y_validation,softmax(np.dot(X_validation, np.transpose(W))),alpha,beta,W)\/y_validation.shape[0] )\n        accuracy = get_accuracy(X_validation,y_validation,W)\n        accuracies.append ( accuracy ) # compute the acc u r a c y on the v a l i d a t i o n s e t\n        if accuracy > best_accuracy :\n            best_accuracy=accuracy\n            best_W=W     # select the best parameters based on the validation accuracy\n    return(best_W,losses,val_losses,best_accuracy)\n\n#test avec un exemple de alpha et beta \nbest_W,losses,val_losses,accuracy=train_model(nb_epochs, minibatch_size, lr,W,alpha=0.1,beta=0.001)\naccuracy_on_unseen_data = get_accuracy ( X_test , y_test , best_W )\nprint ( \"L'accuracy pour le test est: \",accuracy_on_unseen_data ) \n\n#Dessin des poids pour le chiffre 4 \nplt.imshow (best_W [ 4 , 0: 64].reshape ( 8 , 8 ) )\nplt.show()\n","6cc1a025":"alphas=[0.00001,0.0001,0.001,0.01,0.1]\nbetas=[0.00001,0.0001,0.001,0.01,0.1]\n#liste pour tout enregistrer\nvaleur_alpha=[]\nvaleur_beta=[]\nmoyenne_sans=[]\nmoyenne_8=[]\nvariance_sans=[]\nvariance_8=[]\n\n\nbest_acc=0\nbest_alpha=None\nbest_beta=None\nbest_W=None\ni=0\nfor alpha in alphas: \n    for beta in betas: \n        i+=1\n        print(i,\"\/\",len(alphas)*len(betas))\n        W = np.random.normal ( 0 , 0.01 , ( len ( np.unique ( y ) ) , X.shape [ 1 ] ) ) # w ei g h t s o f shape KxL\n        W,losses,val_losses,accuracy = train_model(100, minibatch_size, lr,W,alpha,beta)\n        \n        #Remplir les donnes des moyennes et variances\n        valeur_alpha.append(alpha)\n        valeur_beta.append(beta)\n        moyenne_sans.append(np.mean(W[:,:65]))\n        moyenne_8.append(np.mean(W[:,-8:]))\n        variance_sans.append(np.var(W[:,:65]))\n        variance_8.append(np.var(W[:,-8:]))\n        \n        if accuracy>best_acc:\n            best_acc=accuracy\n            best_alpha=alpha\n            best_beta=beta\n            best_W=W","2ab76acf":"print(\"la meilleure combinaison trouv\u00e9e est pour (\u03bb1,\u03bb2) est (\", best_alpha ,\",\",best_beta,\")\")","fa874ac7":"plt.plot(np.log10(valeur_alpha),moyenne_8,'ro')\nplt.plot(np.log10(valeur_alpha),moyenne_sans,'bo')\nplt.plot(np.log10(valeur_alpha),variance_8,'go')\nplt.plot(np.log10(valeur_alpha),variance_sans,'co')\nplt.axis(ymax=0.005)\nplt.legend(('Moyenne poids des 8 dimensions', 'Moyennes des poids contenant les vraies valeurs','Variance poids des 8 dimensions', 'Variance des poids contenant les vraies valeurs'),loc='upper right')\nplt.title('Courbe en fonction de log10(alpha)')\nplt.xlabel(\"log10(alpha)\")\nplt.show()","dbbef62a":"plt.plot(np.log10(valeur_beta),moyenne_8,'ro')\nplt.plot(np.log10(valeur_beta),moyenne_sans,'bo')\nplt.plot(np.log10(valeur_beta),variance_8,'go')\nplt.plot(np.log10(valeur_beta),variance_sans,'co')\nplt.axis(ymax=0.005)\nplt.legend(('Moyenne poids des 8 dimensions', 'Moyennes des poids contenant les vraies valeurs','Variance poids des 8 dimensions', 'Variance des poids contenant les vraies valeurs'),loc='upper right')\nplt.title('Courbe en fonction de log10(beta)')\nplt.xlabel(\"log10(beta)\")\nplt.show()","10d41869":"# This import registers the 3D projection, but is otherwise unused.\nfrom mpl_toolkits.mplot3d import Axes3D  \n\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nn = 100\n\n\nfor c, m, zlow, zhigh in [('r', 'o', -50, -25)]:\n    xs = np.log10(valeur_alpha)\n    ys = np.log10(valeur_beta)\n    zs = np.matrix(np.log10(abs(np.array(moyenne_8))))\n    ax.scatter(xs, ys, zs, c=c, marker=m)\n\nax.set_xlabel('log(Alpha)')\nax.set_ylabel('log(Beta)')\nax.set_zlabel('log(|moyenne|) poids 8 dimensions')\n\nplt.show()","db3cccb4":"fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n\nn = 100\n\n\nfor c, m, zlow, zhigh in [('r', 'o', -50, -25)]:\n    xs = np.log10(valeur_alpha)\n    ys = np.log10(valeur_beta)\n    zs = np.matrix(np.log10(abs(np.array(variance_8))))\n    ax.scatter(xs, ys, zs, c=c, marker=m)\n\nax.set_xlabel('log(Alpha)')\nax.set_ylabel('log(Beta)')\nax.set_zlabel('log(|variance|) poids 8 dimensions')\n\nplt.show()","3a217544":"print(\"La moyenne des poids associ\u00e9s aux dimensions contenant des vraies donn\u00e9es: \" ,np.mean(best_W[:,:65]))\nprint(\"La variance des poids associ\u00e9s aux dimensions contenant des vraies donn\u00e9es: \",np.var(best_W[:,:65]))\nprint(\"#######################################\")\nprint(\"La moyenne des poids associ\u00e9s \u00e0 ces dimensions contenant des valeurs al\u00e9atoires est: \" ,np.mean(best_W[:,-8:]))\nprint(\"La variance des poids associ\u00e9s \u00e0 ces dimensions contenant des valeurs al\u00e9atoires est: \",np.var(best_W[:,-8:]))","7a27f775":"Dans cette partie, on va ajouter 8 dimensions suppl\u00e9mentaires \u00e0 la fin de chaque exemple dans l\u2019ensemble d\u2019entra\u00eenement, l\u2019ensemble de validation et l\u2019ensemble de test. \n\nOn va remplir ces dimensions avec des valeurs al\u00e9atoires en utilisant une **distribution al\u00e9atoire uniforme** entre la plus **petite valeur** et la plus **grande valeur** parmi toutes les dimensions de l\u2019ensemble de donn\u00e9es d\u2019origine. ","e9872836":"Montrez les r\u00e9sultats pour diff\u00e9rents **taux d\u2019apprentissage**, $e.g. 0.1, 0.01, 0.001$, et diff\u00e9rentes **tailles de minibatch**, $e.g. 1, 20, 200, 1000$.","04057f31":"On remarque que quand le **taux d'apprentissage** lr est **trop petit**, le mod\u00e8le apprend **lentement**. Il a besoin de plus d'epochs pour converger. Par contre quand ce dernier est assez **\u00e9lev\u00e9** le mod\u00e8le converge plus vite. Cepndant avec un **lr** grand le mod\u00e8le peut osciller autours du minimum sans jamais l'atteindre. \n\nD'o\u00f9 on utilise souvent un **taux d'apprentissage variable**, qui commence \u00e9lev\u00e9 aux premi\u00e8res it\u00e9rations et diminue au fur et \u00e0 mesure afin de converger plus rapidement au minimum. \n\n\nOn remarque aussi que le choix du **minibatch** affecte l'apprentissage: en effet un minibatch **tr\u00e8s petit** peut enra\u00eener un **surapprentissage** (Le loss de la validation est plus \u00e9lev\u00e9 que celui d'entra\u00eenement). \nAussi, depend-t-il de la carte graphique de l'ordinateur utilis\u00e9. \n","ff4ccca0":"##### Plot loss pour l'entrainement et la validation: ","7bd9c2e8":"Nous allons ajouter le biais \u00e0 notre matrice de poids $\\mathrm{W}$ pour devenir la matrice $\\Theta$ du cours: il faut rajouter une colonne \u00e0 la matrice $\\mathrm{W}$.\n\nDonc pour le prendre en consid\u00e9ration dans le calcul, il est d'usage d'ajouter une colonne suppl\u00e9mentaire \u00e0 X, cette colonne est remplie de 1. \n\n**PS**: nous allons garder la notation $\\mathrm{W}$ pour la matrice $\\Theta$\n","b67e72fe":"Nous allons it\u00e9rer sur plusieurs valeurs de $\\lambda_1$ et $\\lambda_2$ pour trouver **la meilleure combinaison** en utilisant les donn\u00e9es de validation","89653861":"##### Dessiner les poids pour le chiffre 4 : ","19c33c4b":"#### <span style=\"color:blue\">2-2: Description des t\u00e2ches","2f91b446":"##### Diviser la base de donn\u00e9es en Train\/Validation\/Test\nQuand on veut tester les performances de l'apprentissage d'un algorithme de machine learning, on **ne le teste pas sur les donn\u00e9es utilis\u00e9es pour l'apprentissage**.\n\nEn effet, ce qui nous int\u00e9resse c'est que notre algorithme soit **capable de g\u00e9n\u00e9raliser** ses pr\u00e9dictions \u00e0 des donn\u00e9es qu'il n'a **jamais vu**. \n\nPar cons\u00e9quent, avant de commencer n'importe quel entraitement, la premi\u00e8re chose \u00e0 faire et de le **diviser en trois parties**: un ensemble d'**entra\u00eenement** (**70** du dataset), un ensemble de **valiation**(**15%** du dataset) et un ensemble de **test**(**15%** du dataset).\n\n- Les donn\u00e9es d'entrainement permettent \u00e0 l'algorithme d'apprendre les poids de notre mod\u00e8le\n- Les donn\u00e9es de validation permettent le choix du meilleur mod\u00e8le\n- Les donn\u00e9es de test permettent de tester le mod\u00e8le final choisi\n\nLe **train_test_split** permet de diviser la base donn\u00e9es de mani\u00e8re al\u00e9atoire en deux parties:\n- La premi\u00e8re sera 0,7 pour le train et 0,3 pour le test \n- La deuxi\u00e8me est pour diviser le 0,3 du dataset en test et validation. \nCela garantit l'absence de connaissances pr\u00e9alables de l'ensemble de validation\/test lors de l'entrainement.\n","988e9362":"**Digits** est une base de donn\u00e9es de chiffres entre $[0..10]$. Chaque exemple est une image $8*8$ pixels.","13767ac8":"##### Calculer l'accuracy sur le test Dataset: ","e0489f7c":"##### Softmax:\nSoftmax est une **fonction d'activtion** qui \u00e0 partir de l'Input $\\mathrm{X}$ et la matrice des poids $\\mathrm{W}$ donne le **matrice de probabilit\u00e9** correspondante.\n\nPour cela on d\u00e9finit la **fonction softmax**:\n\n$$ \\hat{p_x}^k = softmax(z)_k = \\frac{exp(z_k)}{\\sum_{\\substack{1<j<K}} exp(z_j)} $$\n\nAvec z le **logit** qui est une ligne de $\\mathrm{Z}$ dont l'e\u00e9pression dans notre cas est \u00e9gale \u00e0:\n\n$$ \\mathrm{Z} = \\mathrm{X}*\\mathrm{W}^t $$\n\nIntuitivement, pour un logit de z, $z_k$, on prend l'exponentielle de cette valeur et on la divise par la somme des exponentielles de chaque logit du vecteur **z**. On obtient  $\\hat{p_x}^k$ la probabilit\u00e9 que l'exemple **x** appartienne \u00e0 la classe **k**.\n\nOn r\u00e9it\u00e8re l'op\u00e9ration pour chaque logit du vecteur **z**. \n\nOn obtient ainsi un vecteur de probabilit\u00e9s $\\hat{p_x}$ pour un exemple **x**. \n\nLa division permet de rendre la somme des termes du vecteur $\\hat{p_x}$ \u00e9gale \u00e0 1 ce qui est indispensable dans le cadre des probabilit\u00e9s.\n","cd77c0f3":"#### <span style=\"color:blue\">1-d:<\/span>","5b95fcaf":"##### Loss function:\nLa fonction de perte qu'on veut minimiser est la log-vraisemblance n\u00e9gative. \n\nDans le cas de classification minimiser la log-vraisemblance n\u00e9gative revient \u00e0 minimiser **l'entropie crois\u00e9e** (Cross_Entropy). \n\nEn effet, pour des raisons de simplification d'\u00e9criture, consid\u00e9rons la classification binaire: \n\n\nLe mod\u00e8le de **r\u00e9gression logistique** est un mod\u00e8le **binomial**, donc la vraisemblance s'\u00e9crit sous cette forme:\n$$ \\Pr(y|x;\\theta)= \\prod_{i=1}^n\\Pr_\\theta(y|x_i)^{y_i}(1-\\Pr_\\theta(y|x_i))^{1-y_i}$$\n\n\nAinsi la log-vraisemblance n\u00e9gative est \u00e9gale \u00e0: \n\n\\begin{equation*}\n\\begin{aligned}\nL(\\theta,x,y) & = -log(\\Pr(y|x;\\theta)) \\\\\n& =-log(\\prod_{i=1}^n\\Pr_\\theta(y|x_i)^{y_i}(1-\\Pr_\\theta(y|x_i))^{1-y_i}) \\\\\n& = -\\sum_{i=1}^n y_i log(\\Pr_\\theta(y|x_i))+(1-y_i) log(1-\\Pr_\\theta(y|x_i)) \n\\end{aligned}\n\\end{equation*}\n\nCeci est la formule pour l'entropie crois\u00e9e. \n\n**Resource:** https:\/\/www.quora.com\/What-are-the-differences-between-maximum-likelihood-and-cross-entropy-as-a-loss-function\n","a5844d3a":"On a: \n\\begin{equation*}\n\\Pr(\\mathrm{W}=1|\\mathrm{H}=1,\\mathrm{A}=0)=\\frac{\\Pr(\\mathrm{W}=1,\\mathrm{H}=1,\\mathrm{A}=0)}{\\Pr(\\mathrm{H}=1,\\mathrm{A}=0)}\n\\end{equation*}\n\nOn a d'apr\u00e8s la r\u00e8gle de marginalisation \n\\begin{equation*}\n\\Pr(\\mathrm{W}=1,\\mathrm{H}=1,\\mathrm{A}=0)=\\sum_p\n\\Pr(\\mathrm{W}=1,\\mathrm{H}=1,\\mathrm{P}=p,\\mathrm{A}=0)\n\\end{equation*}\n\nEn utilisant la r\u00e8gle de r\u00e9seau bay\u00e9sien: \n\\begin{equation*}\n\\Pr(\\mathrm{W}=1,\\mathrm{H}=1,\\mathrm{A}=0)=\\sum_p\n\\Pr(\\mathrm{W}=1|\\mathrm{P}=p)\\Pr(\\mathrm{H}=1|\\mathrm{P}=p,\\mathrm{A}=0)\\Pr(\\mathrm{P}=p)\\Pr(\\mathrm{A}=0)\n\\end{equation*}\n\nde m\u00eame on trouve : \n\\begin{equation*}\n\\Pr(\\mathrm{H}=1,\\mathrm{A}=0)=\\sum_w\\sum_p\n\\Pr(\\mathrm{W}=w|\\mathrm{P}=p)\\Pr(\\mathrm{H}=1|\\mathrm{P}=p,\\mathrm{A}=0)\\Pr(\\mathrm{P}=p)\\Pr(\\mathrm{A}=0)\n\\end{equation*}\n","b0a59eac":"#### <span style=\"color:blue\">1-a:<\/span>","06cb5499":"On va ajouter maintenant la r\u00e9gularisation du types **Elastic Net** \u00e0 notre mod\u00e8le. \n\nOn veut alors minimiser la fonction suivante: \n$$ F(\\theta, x, y, \\lambda_1, \\lambda_2)= L(\\theta,x,y)+\\lambda_1 ||\\theta||^2 +\\lambda_2||\\theta||_1$$\n\nLe gradient de cette fonction est \u00e9gale: \n\n$$ \\frac{\\partial}{\\partial \\theta}F(\\theta, x, y, \\lambda_1, \\lambda_2)=\\frac{\\partial}{\\partial \\theta}L(\\theta,x,y)+ \\lambda_1  2\\theta+ \\lambda_2 sign(\\theta) $$\n\nLe gradient de la fonction $L(\\theta,x,y)$ est d\u00e9j\u00e0 calcul\u00e9 dans la partie 2. \n\n**Ps:** La r\u00e9gularisation n'affecte pas les poids du biais\n","6c2ac46a":"On a la v.a W est ind\u00e9pendante de A\n\ndonc \n\\begin{equation*}\n\\begin{aligned}\n\\Pr(\\mathrm{W}=1|\\mathrm{A}=0)&=\\frac{\\Pr(\\mathrm{W}=1,\\mathrm{A}=0)}{\\Pr(\\mathrm{A}=0)}\\\\\n&=\\frac{\\Pr(\\mathrm{W}=1)\\Pr(\\mathrm{A}=0)}{\\Pr(\\mathrm{A}=0)}\\\\\n&=\\Pr(\\mathrm{W}=1)\n\\end{aligned}\n\\end{equation*}\nDonc d'apr\u00e8s 1-a on obtient","41a44091":"#### <span style=\"color:blue\">1-b:<\/span>","3268ec33":"On va maintenant **ajuster les coefficients de ces deux nouveaux termes** dans votre fonction de perte en utilisant l\u2019ensemble de validation.","9210133a":"### <span style=\"color:red\"> **Remarque** :<\/span>\nNous n'avons pas diviser par X.shape[0], car nous allons sommer sur tous les batchsize et diviser \u00e0 la fin de chaque **epoch**. \n\n**D\u00e9tail d'impl\u00e9mentation:** La fonction log n'est pas d\u00e9finie pour des valeurs de probabilit\u00e9 de 0. et nulle pour 1. , donc il faut s'assurer que \u00e9tant donn\u00e9 $\\epsilon$, les probabilit\u00e9s sont comprises dans [$\\epsilon$, 1. - $\\epsilon$].\n","fe1f6c95":"On a $\\Pr(\\mathrm{W}=1|\\mathrm{P}=1)$ dans le tenseur Watson ","dd08aaf6":"##### <span style=\"color:blue\">2-3-b:","1244ec09":"On a: \n\\begin{equation*}\n\\Pr(\\mathrm{W}=1|\\mathrm{H}=1)=\\frac{\\Pr(\\mathrm{W}=1,\\mathrm{H}=1)}{\\Pr(\\mathrm{H}=1)}\n\\end{equation*}\nCommen\u00e7ons par $Pr(H=1)$\nOn a \n\\begin{equation*}\n\\Pr(\\mathrm{H}=1)= \\sum_p \\sum_a \\Pr(\\mathrm{H}=1|\\mathrm{P}=p,\\mathrm{A}=a)\\Pr(\\mathrm{P}=p,\\mathrm{A}=a)\n\\end{equation*}\nComme A et P ont ind\u00e9pendantes sans evidence sur H, donc \n\\begin{equation*}\n\\Pr(\\mathrm{P},\\mathrm{A})=\\Pr(\\mathrm{P})*\\Pr(\\mathrm{A})\n\\end{equation*}\nDonc \n\\begin{equation*}\n\\Pr(\\mathrm{H}=1)= \\sum_p \\sum_a \\Pr(\\mathrm{H}=1|\\mathrm{P}=p,\\mathrm{A}=a)\\Pr(\\mathrm{P}=p)\\Pr(\\mathrm{A}=a)\n\\end{equation*}","f206d8c1":"**### <span style=\"color:blue\">Partie 2: R\u00e9gression Logistique","94e4754b":"Le one-hot encoding permet de repr\u00e9senter un vecteur de donn\u00e9es **cat\u00e9goriques** (10 classes).","ba946a43":"Comme les graphes 2D ne permettent pas une bonne int\u00e9rpretation des resultats, j'ai opt\u00e9 pour deux **graphes 3D** de la **moyenne** des valeurs de la matrice poids associ\u00e9es aux 8 dimensions ajout\u00e9es, ainsi que leur **variance** en fonction des valeurs de alpha et b\u00e9ta. ","30a90f27":"On peut aussi verifer notre r\u00e9sultat en utilisant les r\u00e8gles de marginalisation et de Bayes de la m\u00eame mani\u00e8re que 1-c","4708353f":"##### Cette partie n'est pas obilgatoire pour Digits: \nCette partie permet de m\u00e9langer la base de donn\u00e9es pour \u00e9liminer tout biais\n\n$i.e$: si la base de donn\u00e9es de donn\u00e9es poss\u00e8de un ordre bien d\u00e9fini par exemple. ","c0adc71e":"-  **Numpy** : La librairie fondamentale pour le calcul scientifique et le traitement matriciel.\n-  **Matplotlib** : Dessin de graphe 2D \n-  **Sklearn** : Librairie d'Apprentissage Machine","7c117422":"Passons maintenant au deuxi\u00e8me terme $\\Pr(\\mathrm{W}=1,\\mathrm{H}=1)$ \n\nOn a d'apr\u00e8s la r\u00e8gle de marginalisation \n\\begin{equation*}\n\\Pr(\\mathrm{W}=1,\\mathrm{H}=1)=\\sum_p\\sum_a\n\\Pr(\\mathrm{W}=1,\\mathrm{H}=1,\\mathrm{P}=p,\\mathrm{A}=a)\n\\end{equation*}\n\nEn utilisant la r\u00e8gle de r\u00e9seau bay\u00e9sien: \n\\begin{equation*}\n\\Pr(\\mathrm{W}=1,\\mathrm{H}=1)=\\sum_p\\sum_a\n\\Pr(\\mathrm{W}=1|\\mathrm{P}=p)\\Pr(\\mathrm{H}=1|\\mathrm{P}=p,\\mathrm{A}=a)\\Pr(\\mathrm{P}=p)\\Pr(\\mathrm{A}=a)\n\\end{equation*}\n","23434cbb":"##### Gardient:\n\nLa descente de gradient est un algorithme qui permet trouver la solution optimale d'un certains nombre de probl\u00e8mes. Le principe est le suivant: \n\non d\u00e9finit une **fonction de co\u00fbt J**  qui caract\u00e9rise le probl\u00e8me, qui est la log-vraisemblance n\u00e9gative.\nCette fonction d\u00e9pend d'un ensemble de **param\u00e8tres $\\theta$ **. La descente de gradient cherche \u00e0 **minimiser** la fonction de co\u00fbt en **modifiant it\u00e9rativement** les param\u00e8tres.\n\nD'apr\u00e8s les calculs fait dans la partie **2-1** on a: \n\n$$ \\frac{\\partial}{\\partial \\theta}log \\prod_{i=1}^N \\Pr(y_i|x_i;\\theta)=\\sum_{i=1}^N(y_i-\\hat{y}_i)x_i^T$$\n\navec $y_i, x_i$ les vraies donn\u00e9es et $\\hat{y}_i $les probas pr\u00e9dites \n\n**PS**: On divise par X.shape[0] car d'apr\u00e8s la formule du cours on divise sur la taille du minibatch\n\n\n","62b86ec5":"#### <span style=\"color:blue\">1-e:","8da055ea":"## <span style=\"color:orange\">R\u00e9seau Bayesien & R\u00e9gression Logistique From scratch <\/span>\n","4a5f7f89":"On remarque que meme si **la moyenne** est basse pour tous les poids, **la variance** est plus petite pour les poids ajout\u00e9s, ce qui confirme que **ces derniers varient tr\u00e8s peu aux alentours de z\u00e9ro**\n\n\nEn comparant la moyenne et la variance avec et sans r\u00e9gularisation on trouve que la moyenne et la variance des poids associ\u00e9s aux dimensions contenant des vraies donn\u00e9es sont tr\u00e8s comparable \u00e0 ceux sans r\u00e9gularisation. \n\n\n    Moyenne sans r\u00e9gularisation -0.00026008029224096406\n    Variance sans r\u00e9gularisation 0.0011035396464422107","2588b15a":"On remarque que **la moyenne** est minimis\u00e9e avec des valeurs petites pour alpha ($10^{-4}, 10^{-5}$) (correspondant \u00e0 la r\u00e9gularisation L2) et assez \u00e9lev\u00e9es pour b\u00e9ta ($10^{-1}$) (Regularisation L1). \n\nCeci confirme le fait que la **r\u00e9gression ridge (L2)** nous permet de **r\u00e9duire l'amplitude des coefficients** d'une r\u00e9gression lin\u00e9aire et d'\u00e9viter le sur-apprentissage. Cependant, pour **annuler certains coefficients \u00e0 z\u00e9ro** de mani\u00e8re que les variables associ\u00e9es ne feront plus partie du mod\u00e8le, **Lasso** est la meilleure r\u00e9gularisation. ","6c5bfc09":"#### <span style=\"color:blue\">1-c: <\/span>","caaecdbb":"On a:\n\\begin{equation*}\n\\Pr(\\mathrm{W}=1)=\\sum_p\\Pr(\\mathrm{W}=1|\\mathrm{P}=p)\\Pr(\\mathrm{P}=p)\n\\end{equation*}\nAinsi ","c6aa2629":"##### Accuracy:\nOn calcule la probabilit\u00e9 pour chaque classe en utilisant la fonction d\u00e9j\u00e0 d\u00e9finie **Softmax**.\n\nLa classe pr\u00e9dite est celle ayant la propabilit\u00e9 **la plus \u00e9lev\u00e9e** parmi les 10 classes. \n\nL'accuracy est alors d\u00e9finie comme: \n\n$$Accuracy=\\frac{N_{ \\; Exemples \\; bien \\; class\u00e9s}}{N_{ \\; Exemples \\; totaux}}$$\n\nL'accuracy est une valeur entre $[0,1]$, le plus proche de 1, le mieux c'est. \n","55ef7213":"##### Initialiser les param\u00eatres et entrainer le mod\u00e8le: ","fbae55d4":"##### <span style=\"color:blue\">2-3-a:","5a28f290":"### <span style=\"color:blue\">Partie 1: R\u00e9seau Bayesien <\/span> \n![image.png](attachment:image.png)","664e18ac":"### <span style=\"color:red\"> **Remarque** :<\/span>\n\nPour assurer la stabilit\u00e9 num\u00e9rique de la fonction Softmax, nous allons soustraire le maximum de chaque logit z: \n\nSoit la formule devient: \n\n$$ \\hat{p_x}^k = softmax(z)_k = \\frac{exp(z_k-max_{1<j<K}(z_j))}{\\sum_{\\substack{1<j<K}} exp(z_j-max_{1<j<K}(z_j))} $$\n","8cc8b06f":"##### Entrainement du mod\u00e8le:\n{I_1,..,I_K} K minibatches\n\n$\\eta$ le taux d'apprentissage \n\nfor n in epochs: \n\n        for k in K mini_batches:\n\n$$g=\\frac{1}{B_k}\\sum_{i in I_k}[\\frac{\\partial}{\\partial \\theta} L(x_i,\\theta,y_i) ]$$\n\n$$\\theta=\\theta-\\eta g$$\n"}}