{"cell_type":{"86870711":"code","0b7fb66f":"code","cca1b4a2":"code","956a68d8":"code","9e74d33b":"code","22c47d7e":"code","aefe6f00":"code","780f5c90":"code","90dd4452":"code","e5264432":"code","1e1e4ff5":"code","7c0a3a5b":"code","5f9282eb":"code","a42ede25":"code","7d1967da":"code","969d6e37":"code","a1c18fca":"code","803f6973":"code","9bcaa39f":"code","035b0282":"code","cdec95f8":"code","c3328fe4":"code","8e7ba215":"code","64266557":"code","363da380":"code","f744d486":"code","c36b79a5":"code","5047ad9e":"code","31626b46":"code","a0308258":"code","6d989548":"code","0cd0ad4b":"code","b925ab81":"code","a6648454":"code","0e1b3649":"code","8ab2b317":"markdown","fa574990":"markdown","2f5df683":"markdown","d801eb04":"markdown","026c7492":"markdown"},"source":{"86870711":"#importing the required libraries\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport re\nimport string\nimport operator\nimport collections\nimport itertools\nimport json\n\nfrom flashtext import KeywordProcessor\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split,cross_validate,ShuffleSplit\nfrom sklearn.metrics import accuracy_score, classification_report,confusion_matrix\n\nimport nltk\nen_stop = set(nltk.corpus.stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize\n\nimport re\nimport spacy \nnlp=spacy.load('en_core_web_sm')\nfrom spacy.tokenizer import _get_regex_pattern\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom wordcloud import WordCloud, STOPWORDS\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport fasttext\n\nfrom sklearn.model_selection import StratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.metrics import precision_score, recall_score, f1_score","0b7fb66f":"## Getting the file path\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","cca1b4a2":"df_train = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")\ndf_sample_submission = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/sample_submission.csv\")","956a68d8":"print(\"For training data:\")\nprint(df_train[\"sentiment\"].shape)\nprint(df_train[\"sentiment\"].value_counts())\n# print(df_train.describe)\nprint()\nprint(\"For testing data:\")\nprint(df_test[\"sentiment\"].shape)\nprint(df_test[\"sentiment\"].value_counts())","9e74d33b":"df_train[\"text\"]= df_train[\"text\"].str.strip().replace(np.nan, \"\").str.lower()\ndf_train[\"selected_text\"]= df_train[\"selected_text\"].str.strip().replace(np.nan, \"\").str.lower()\ndf_test[\"text\"]= df_test[\"text\"].str.strip().replace(np.nan, \"\").str.lower()","22c47d7e":"def jaccard(str1, str2): \n    try:\n        a = set(str1.split()) \n        b = set(str2.split())\n        c = a.intersection(b)\n        return float(len(c)) \/ (len(a) + len(b) - len(c))\n    except ZeroDivisionError:\n        return 0\n\ndf_train[\"jaccard_scr\"] = df_train.progress_apply(lambda x: jaccard(x[\"text\"], x[\"selected_text\"]), axis=1)","aefe6f00":"print(df_train[\"jaccard_scr\"].mean())\nprint(df_train[df_train[\"sentiment\"]==\"neutral\"][\"jaccard_scr\"].mean())\nprint(df_train[df_train[\"sentiment\"]==\"positive\"][\"jaccard_scr\"].mean())\nprint(df_train[df_train[\"sentiment\"]==\"negative\"][\"jaccard_scr\"].mean())","780f5c90":"## Text cleaning\ntext_clean_replace_list = [\n    # Special characters\n    ['\\x89\u00db_', ''],\n    ['\\x89\u00db\u00d2', ''],\n    ['\\x89\u00db\u00d3', ''],\n    ['\\x89\u00db\u00cfWhen', 'When'],\n    ['\\x89\u00db\u00cf', ''],\n    ['China\\x89\u00db\u00aas', \"China's\"],\n    ['let\\x89\u00db\u00aas', \"let's\"],\n    ['\\x89\u00db\u00f7', ''],\n    ['\\x89\u00db\u00aa', ''],\n    ['\\x89\u00db\\x9d', ''],\n    ['\u00e5_', ''],\n    ['\\x89\u00db\u00a2', ''],\n    ['\\x89\u00db\u00a2\u00e5\u00ca', ''],\n    ['from\u00e5\u00cawounds', 'from wounds'],\n    ['\u00e5\u00ca', ''],\n    ['\u00e5\u00c8', ''],\n    ['Jap\u00cc_n', 'Japan'],\n    ['\u00cc\u00a9', 'e'],\n    ['\u00e5\u00a8', ''],\n    ['Suru\u00cc\u00a4', 'Suruc'],\n    ['\u00e5\u00c7', ''],\n    ['\u00e5\u00a33million', '3 million'],\n    ['\u00e5\u00c0', ''],\n\n    # Contractions\n    \n    [\"i`m\", 'i am'],\n    [\"he's\", 'he is'],\n    [\"there's\", 'there is'],\n    [\"We're\", 'We are'],\n    [\"That's\", 'That is'],\n    [\"won't\", 'will not'],\n    [\"they're\", 'they are'],\n    [\"Can't\", 'Cannot'],\n    [\"wasn't\", 'was not'],\n    ['don\\x89\u00db\u00aat', 'do not'],\n    [\"aren't\", 'are not'],\n    [\"isn't\", 'is not'],\n    [\"What's\", 'What is'],\n    [\"haven't\", 'have not'],\n    [\"hasn't\", 'has not'],\n    [\"There's\", 'There is'],\n    [\"He's\", 'He is'],\n    [\"It's\", 'It is'],\n    [\"You're\", 'You are'],\n    [\"I'M\", 'I am'],\n    [\"shouldn't\", 'should not'],\n    [\"wouldn't\", 'would not'],\n    [\"i'm\", 'I am'],\n    ['I\\x89\u00db\u00aam', 'I am'],\n    [\"I'm\", 'I am'],\n    [\"Isn't\", 'is not'],\n    [\"Here's\", 'Here is'],\n    [\"you've\", 'you have'],\n    ['you\\x89\u00db\u00aave', 'you have'],\n    [\"we're\", 'we are'],\n    [\"what's\", 'what is'],\n    [\"couldn't\", 'could not'],\n    [\"we've\", 'we have'],\n    ['it\\x89\u00db\u00aas', 'it is'],\n    ['doesn\\x89\u00db\u00aat', 'does not'],\n    ['It\\x89\u00db\u00aas', 'It is'],\n    ['Here\\x89\u00db\u00aas', 'Here is'],\n    [\"who's\", 'who is'],\n    ['I\\x89\u00db\u00aave', 'I have'],\n    [\"y'all\", 'you all'],\n    ['can\\x89\u00db\u00aat', 'cannot'],\n    [\"would've\", 'would have'],\n    [\"it'll\", 'it will'],\n    [\"we'll\", 'we will'],\n    ['wouldn\\x89\u00db\u00aat', 'would not'],\n    [\"We've\", 'We have'],\n    [\"he'll\", 'he will'],\n    [\"Y'all\", 'You all'],\n    [\"Weren't\", 'Were not'],\n    [\"Didn't\", 'Did not'],\n    [\"they'll\", 'they will'],\n    [\"they'd\", 'they would'],\n    [\"DON'T\", 'DO NOT'],\n    ['That\\x89\u00db\u00aas', 'That is'],\n    [\"they've\", 'they have'],\n    [\"i'd\", 'I would'],\n    [\"should've\", 'should have'],\n    ['You\\x89\u00db\u00aare', 'You are'],\n    [\"where's\", 'where is'],\n    ['Don\\x89\u00db\u00aat', 'Do not'],\n    [\"we'd\", 'we would'],\n    [\"i'll\", 'I will'],\n    [\"weren't\", 'were not'],\n    [\"They're\", 'They are'],\n    ['Can\\x89\u00db\u00aat', 'Cannot'],\n    ['you\\x89\u00db\u00aall', 'you will'],\n    ['I\\x89\u00db\u00aad', 'I would'],\n    [\"let's\", 'let us'],\n    [\"it's\", 'it is'],\n    [\"can't\", 'cannot'],\n    [\"don't\", 'do not'],\n    [\"you're\", 'you are'],\n    [\"i've\", 'I have'],\n    [\"that's\", 'that is'],\n    [\"i'll\", 'I will'],\n    [\"doesn't\", 'does not'],\n    [\"i'd\", 'I would'],\n    [\"didn't\", 'did not'],\n    [\"ain't\", 'am not'],\n    [\"you'll\", 'you will'],\n    [\"I've\", 'I have'],\n    [\"Don't\", 'do not'],\n    [\"I'll\", 'I will'],\n    [\"I'd\", 'I would'],\n    [\"Let's\", 'Let us'],\n    [\"you'd\", 'You would'],\n    [\"It's\", 'It is'],\n    [\"Ain't\", 'am not'],\n    [\"Haven't\", 'Have not'],\n    [\"Could've\", 'Could have'],\n    ['youve', 'you have'],\n    ['don\u00e5\u00abt', 'do not'],\n\n    # Character entity references\n    ['&gt;', '>'],\n    ['&lt;', '<'],\n    ['&amp;', '&'],\n\n    # Typos', 'slang and informal abbreviations\n    ['w\/e', 'whatever'],\n    ['w\/', 'with'],\n    ['USAgov', 'USA government'],\n    ['recentlu', 'recently'],\n    ['Ph0tos', 'Photos'],\n    ['amirite', 'am I right'],\n    ['exp0sed', 'exposed'],\n    ['<3', 'love'],\n    ['lmao', 'laughing my ass off'],\n    ['thankU', 'thank you'],\n\n    # Acronyms\n    ['MH370', 'Malaysia Airlines Flight 370'],\n    ['m\u00cc\u00bcsica', 'music'],\n    ['okwx', 'Oklahoma City Weather'],\n    ['arwx', 'Arkansas Weather'],\n    ['gawx', 'Georgia Weather'],\n    ['scwx', 'South Carolina Weather'],\n    ['cawx', 'California Weather'],\n    ['tnwx', 'Tennessee Weather'],\n    ['azwx', 'Arizona Weather'],\n    ['alwx', 'Alabama Weather'],\n    ['wordpressdotcom', 'wordpress'],\n    ['usNWSgov', 'United States National Weather Service'],\n    ['Suruc', 'Sanliurfa'],\n\n    # Grouping same words without embeddings\n    ['Bestnaijamade', 'bestnaijamade'],\n    ['SOUDELOR', 'Soudelor'],\n]\n\n## Replace using flashtext instead of regex as it is many folds faster in text search and text replace\n\nkp_replace_clean = KeywordProcessor()\nfor elem in text_clean_replace_list:\n    kp_replace_clean.add_keyword(elem[0].strip(), elem[1].strip())\ntest__ =\"Hi this is don't testing on http:\/\/www.kaggle.com\/rtatman\/import-functions-from-kaggle-script and not this\"\n\ndef clean(string):\n    kp_replace_clean.replace_keywords(string)\n    return re.sub(r\"https?:[^\\s]*\", \"website\", string, re.I)\n    \nclean(test__)","90dd4452":"## Cleaning the text columns\ndf_train[\"text\"]=df_train[\"text\"].progress_apply(lambda x: clean(x))\ndf_train[\"selected_text\"]=df_train[\"selected_text\"].progress_apply(lambda x: clean(x))\ndf_test[\"text\"]=df_test[\"text\"].progress_apply(lambda x: clean(x))","e5264432":"df_train[\"text_tok\"]=df_train[\"text\"].progress_apply(lambda x: [elem.orth_ for elem in nlp(x)])\ndf_train[\"len_text_tok\"]=df_train[\"text_tok\"].progress_apply(lambda x: len(x))\n\ndf_train[\"selected_text_tok\"]=df_train[\"selected_text\"].progress_apply(lambda x: [elem.orth_ for elem in nlp(x)])\ndf_train[\"len_selected_text_tok\"]=df_train[\"selected_text_tok\"].progress_apply(lambda x: len(x))\n\ndf_test[\"text_tok\"]=df_test[\"text\"].progress_apply(lambda x: [elem.orth_ for elem in nlp(x)])\ndf_test[\"len_text_tok\"]=df_test[\"text_tok\"].progress_apply(lambda x: len(x))","1e1e4ff5":"df_train = df_train[df_train[\"len_text_tok\"]>0]\ndf_train.reset_index(drop=True, inplace=True)","7c0a3a5b":"df_train.groupby(\"sentiment\").mean()[\"jaccard_scr\"]","5f9282eb":"## Remove stopwords just for EDA \nen_stop_new=list(string.punctuation)\nen_stop = list(en_stop) + en_stop_new\n\ndef freq_analysis_df(df, col, top_freq=False):\n    temp_text = ' '.join(df[col])\n    wordcloud2 = WordCloud().generate(temp_text)\n    plt.imshow(wordcloud2)\n    plt.axis(\"off\")\n    plt.show()\n    sns.distplot(df[\"len_\"+col+\"_tok\"])\n        \n    temp_flat = [item.lower().strip() for sublist in df[col+\"_tok\"] for item in sublist if item.lower() not in en_stop]\n    if top_freq!=False:\n        temp = pd.DataFrame(collections.Counter(temp_flat).most_common(top_freq), columns = ['Common_words','count'])\n        display(temp.style.background_gradient(cmap='Blues'))","a42ede25":"freq_analysis_df(df_train, \"text\", 30)","7d1967da":"freq_analysis_df(df_train, \"selected_text\", 50)","969d6e37":"freq_analysis_df(df_test, \"text\", 50)","a1c18fca":"freq_analysis_df(df_train[df_train[\"sentiment\"]==\"neutral\"], \"text\", 50)","803f6973":"freq_analysis_df(df_train[df_train[\"sentiment\"]==\"negative\"], \"text\", 50)","9bcaa39f":"freq_analysis_df(df_train[df_train[\"sentiment\"]==\"positive\"], \"text\", 50)","035b0282":"## Getting unique\/non-overlapping words in each of the category\n\npositive_text_list = df_train[df_train[\"sentiment\"]==\"positive\"][\"text_tok\"]\npositive_text_list = [item for sublist in positive_text_list for item in sublist]\nnegative_text_list = df_train[df_train[\"sentiment\"]==\"negative\"][\"text_tok\"]\nnegative_text_list = [item for sublist in negative_text_list for item in sublist]\nneutral_text_list = df_train[df_train[\"sentiment\"]==\"neutral\"][\"text_tok\"]\nneutral_text_list = [item for sublist in neutral_text_list for item in sublist]\n\n\npositive_sel_text_list = df_train[df_train[\"sentiment\"]==\"positive\"][\"selected_text_tok\"]\npositive_sel_text_list = [item for sublist in positive_sel_text_list for item in sublist]\nnegative_sel_text_list = df_train[df_train[\"sentiment\"]==\"negative\"][\"selected_text_tok\"]\nnegative_sel_text_list = [item for sublist in negative_sel_text_list for item in sublist]\nneutral_sel_text_list = df_train[df_train[\"sentiment\"]==\"neutral\"][\"selected_text_tok\"]\nneutral_sel_text_list = [item for sublist in neutral_sel_text_list for item in sublist]\n\n\ntemp_pos_neg_text = list(set(positive_text_list + negative_text_list))\ntemp_neut_neg_text = list(set(neutral_text_list + negative_text_list))\ntemp_neut_pos_text = list(set(neutral_text_list + positive_text_list))\n\ntemp_pos_neg_sel_text = list(set(positive_sel_text_list + negative_sel_text_list))\ntemp_neut_neg_sel_text = list(set(neutral_sel_text_list + negative_sel_text_list))\ntemp_neut_pos_sel_text = list(set(neutral_sel_text_list + positive_sel_text_list))\n\n### Unique words in each of the categories in the \"text\" column\nunique_positive_text_list = [elem for elem in positive_text_list if elem not in (temp_neut_neg_text)]\nunique_negative_text_list = [elem for elem in negative_text_list if elem not in (temp_neut_pos_text)]\nunique_neutral_text_list = [elem for elem in neutral_text_list if elem not in (temp_pos_neg_text)]\n\n### Unique words in each of the categories in the \"selected_text\" column\nunique_positive_text_sel_text_list = [elem for elem in positive_sel_text_list if elem not in (temp_neut_neg_sel_text)]\nunique_negative_text_sel_text_list = [elem for elem in negative_sel_text_list if elem not in (temp_neut_pos_sel_text)]\nunique_neutral_text_sel_text_list = [elem for elem in neutral_sel_text_list if elem not in (temp_pos_neg_sel_text)]","cdec95f8":"## Remove stopwords just for EDA \n# en_stop_new=list(string.punctuation)\n# en_stop = list(en_stop) + en_stop_new\n\ndef freq_analysis_list(list_, top_freq=False):\n    temp_text = ' '.join(list_)\n    wordcloud2 = WordCloud().generate(temp_text)\n    plt.imshow(wordcloud2)\n    plt.axis(\"off\")\n    plt.show()\n\n    if top_freq!=False:\n        temp = pd.DataFrame(collections.Counter(list_).most_common(top_freq), columns = ['Common_words','count'])\n        display(temp.style.background_gradient(cmap='Blues'))","c3328fe4":"freq_analysis_list(unique_neutral_text_sel_text_list, 30)","8e7ba215":"freq_analysis_list(unique_positive_text_sel_text_list, 30)","64266557":"freq_analysis_list(unique_negative_text_sel_text_list, 30)","363da380":"# Adpated from https:\/\/www.kaggle.com\/jonathanbesomi\/question-answering-starter-pack","f744d486":"# # Prepare data in QA format\n# # Example-format: Training Data\n\n# one_train_data = {\n#     'context': \"This tweet sentiment extraction challenge is great\",\n#     'qas': [\n#         {\n#             'id': \"00001\",\n#             'question': \"positive\",\n#             'answers': [\n#                 {\n#                     'text': \"is great\",\n#                     'answer_start': 43\n#                 }\n#             ]\n#         }\n#     ]\n# }\n\n\n# # Example-format: Testing Data\n\n# one_test_data = {\n#     'context': 'last session of the day  website',\n#     'qas': [\n#         {'question': 'neutral',\n#          'id': 'f87dea47db',\n#          'is_impossible': False,\n#          'answers': [\n#              {\n#                  'answer_start': 1000000,\n#                  'text': '__None__'\n#              }\n#          ]\n#         }\n#     ]\n# }","c36b79a5":"def data_in_qa_format(df_row, data_type):\n    if data_type==\"train\":\n        answer_start = df_row[\"text\"].find(df_row[\"selected_text\"])\n        selected_text = df_row[\"selected_text\"]\n    else:\n        answer_start=1000000\n        selected_text = \"__None__\"\n\n    if type(df_row[\"text\"])!=str:\n        df_row[\"text\"]=\"\"\n    if type(df_row[\"sentiment\"])!=str:\n        df_row[\"sentiment\"]=\"\"\n\n    return {\n        'context': df_row[\"text\"],\n        'qas': [{'id': df_row[\"textID\"],\n                 'question': df_row[\"sentiment\"],\n                 'answers': [{'text': selected_text,\n                              'answer_start': answer_start\n                             }\n                            ]\n                }\n               ]\n    }\nnu=2\ndisplay(data_in_qa_format(df_train.iloc[nu], \"train\"))\ndisplay(data_in_qa_format(df_train.iloc[nu], \"test\"))","5047ad9e":"df_train[\"train_dic\"] = df_train.progress_apply(lambda x: data_in_qa_format(x, \"train\"), axis=1)\ndf_test[\"test_dic\"] = df_test.progress_apply(lambda x: data_in_qa_format(x, \"test\"), axis=1)","31626b46":"df_train[\"train_dic\"].tolist()[2:4]","a0308258":"df_test[\"test_dic\"].tolist()[2:4]","6d989548":"with open('train.json', 'w') as outfile:\n    json.dump(df_train[\"train_dic\"].tolist(), outfile)\n\nwith open('test.json', 'w') as outfile:\n    json.dump(df_test[\"test_dic\"].tolist(), outfile)    ","0cd0ad4b":"## Install simple-transformers, a tool to train and test transformers model easily.\n\n!pip install simpletransformers","b925ab81":"import gc\ngc.collect()","a6648454":"# Train the distilbert-base-uncased-distilled-squad model\nuse_cuda = True # whether to use GPU or not\n\nfrom simpletransformers.question_answering import QuestionAnsweringModel\n\nMODEL_PATH = '\/kaggle\/input\/transformers-pretrained-distilbert\/distilbert-base-uncased-distilled-squad\/'\n\n# Create the QuestionAnsweringModel\nmodel = QuestionAnsweringModel('distilbert', \n                               MODEL_PATH, \n                               args={'reprocess_input_data': True,\n                                     'overwrite_output_dir': True,\n                                     'learning_rate': 5e-5,\n                                     'num_train_epochs': 3,\n                                     'max_seq_length': 192,\n                                     'doc_stride': 64,\n                                     'fp16': False,\n                                    },\n                              use_cuda=use_cuda)\n\nmodel.train_model('train.json')","0e1b3649":"predictions = model.predict(df_test[\"test_dic\"].tolist())\npredictions_df = pd.DataFrame.from_dict(predictions[0])\ndf_test['selected_text'] = predictions_df['answer'].progress_apply(lambda x:x[0])\ndf_test[[\"textID\", \"selected_text\"]].to_csv('submission.csv', index=False)\ndf_test.head()","8ab2b317":"## **Solving it as a Question and Answer problem:**","fa574990":"Following is to be done:\n- Improve\/Finetune this model\n- Try different methods and models\n\nThanks for your time<br \/>\n## **\u30c3**","2f5df683":"### **EDA and Data Cleaning**","d801eb04":"### **EDA**","026c7492":"### **Missing value imputation**"}}