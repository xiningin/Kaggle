{"cell_type":{"f9ea5020":"code","005e8061":"code","3a1f96e6":"code","d367b7e5":"code","e6159b6c":"code","dc56210e":"code","224ccc9f":"code","7e29c1e8":"code","27749223":"code","3b2a9ce8":"code","81ea4073":"code","d6a070d0":"code","1c8110e0":"code","4f3473b7":"code","cfad4756":"code","6cb8024c":"code","ec01662f":"code","d40b7453":"code","76c7e491":"code","bd9b8f81":"code","5b4bf291":"code","93a25de7":"code","4295b595":"code","e0d114eb":"code","18b61dd6":"code","79cfa951":"code","4a889a05":"code","dae13764":"code","0cce09d3":"code","a93c5437":"code","7f201bd5":"code","54d9f0d5":"code","c0b5853a":"code","11559e1d":"code","846859d5":"code","8b1a419f":"code","1a3b2426":"code","17e56e82":"code","589b3327":"code","a1770cd6":"code","11093d29":"code","d2de30d1":"code","fcc52a92":"code","5175c4b7":"code","0c8176d9":"code","34b6babf":"code","9cec2267":"code","b7d95ada":"code","77b9371a":"code","ff00feea":"code","4b9c2b96":"code","d287c9e6":"code","08592752":"code","42509e5c":"code","9ef58704":"code","47aa6167":"code","088cf38a":"code","216a368a":"code","d0eed84b":"code","4ff2ff70":"code","172c69ae":"code","c3b1ecdc":"code","b0fa1dc8":"code","03015b33":"code","8c2d9454":"code","e5060c6d":"code","6cab1feb":"markdown","ba87593f":"markdown","fc752a9f":"markdown","723edb9a":"markdown","fd320581":"markdown","a7e00ef4":"markdown","6c8c2bd9":"markdown","7c88323c":"markdown","bb3c5604":"markdown","e6b13732":"markdown","261ffe51":"markdown","8f9a1745":"markdown","c3ee03b6":"markdown","f476d43d":"markdown","f4e9cfd9":"markdown","3a9a0dc4":"markdown","d2c18a5f":"markdown","3331b1ec":"markdown","9c7e7817":"markdown","ec8070c2":"markdown","c5f4d30e":"markdown","e9e78cfb":"markdown","1cc80b44":"markdown","32aaa5a5":"markdown","4c07f14d":"markdown","02fbb4a5":"markdown","b38b8c21":"markdown","bc04578f":"markdown","db5b408e":"markdown","f7efca89":"markdown","bf58f05c":"markdown","8b095430":"markdown","add4dbcb":"markdown","be540231":"markdown","79720b24":"markdown","b72afdb1":"markdown","3d31ef58":"markdown"},"source":{"f9ea5020":"import os\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # visualize data\n\nfrom sklearn.pipeline import Pipeline","005e8061":"## Spoiler Alert!\n## File Configurations\ndata_reduction = True\nif data_reduction:\n    num_of_personalities = 16 #16 max\n\nskip_early_classification = False\n\nskip_first_cleaning = True\nskip_first_clean_posts_classification = False\n\nskip_second_cleaning = True\nskip_second_clean_posts_classification = False\n\nskip_lemmatazation = True\nskip_lemma_posts_classification = False\n\nsave_clean_data_to_files = False\nlemmatization_and_save = False\n\n# TPOT configurations\ngenerations = 10 #100 default\npopulation_size = 8 #100 default\nn_jobs=-1\nmax_time_mins = 130 # 9*60\/4 = 135 mins\nrandom_state= 42\nconfig_dict='TPOT sparse'\nmemory='auto'\nearly_stop = 3\nverbosity = 3","3a1f96e6":"dataset=pd.read_csv(\"\/kaggle\/input\/mbti-type\/mbti_1.csv\")\ndataset.head(2)","d367b7e5":"dataset.describe()","e6159b6c":"from pandas_profiling import ProfileReport","dc56210e":"profile = ProfileReport(dataset,progress_bar=False )","224ccc9f":"# profile","7e29c1e8":"if data_reduction:\n    data = pd.DataFrame(columns=dataset.columns)\n    data_len =50\n    personalities = dataset[\"type\"].unique()\n    \n    for personality in personalities[:num_of_personalities] :\n        data=pd.concat(  [ data,dataset.loc[dataset['type']==personality][0 : data_len]])\nelse:\n    data=dataset","27749223":"profile = ProfileReport(data,progress_bar=False)","3b2a9ce8":"# profile","81ea4073":"from sklearn.feature_extraction.text import TfidfVectorizer","d6a070d0":"tfidf = TfidfVectorizer(stop_words='english')\nX = tfidf.fit_transform(data['posts'])\nprint(\"The ducument-term matrix has {} rows (documents) and {} columns (total words).\".format(data.shape[0],len(tfidf.get_feature_names())))","1c8110e0":"pd.DataFrame(X.toarray(),index=data.index,columns=tfidf.get_feature_names())","4f3473b7":"tfidf2 = TfidfVectorizer(stop_words='english',min_df=2)\ntfidf2.fit(data['posts'])\n\nprint(\"The ducument-term matrix has {} rows (documents) and {} columns (total words).\".format(data.shape[0],len(tfidf2.get_feature_names())))","cfad4756":"X = tfidf2.transform(data['posts'])","6cb8024c":"pd.DataFrame(X.toarray(),index=data.index,columns=tfidf2.get_feature_names())","ec01662f":"from sklearn.preprocessing import LabelEncoder","d40b7453":"type_le = LabelEncoder()\ndata['num_type']=type_le.fit_transform(data['type'])\ny=data['num_type']","76c7e491":"from tpot import TPOTClassifier\n\nfrom tpot.config.classifier_sparse import  classifier_config_sparse\n[estimator for estimator in classifier_config_sparse ]","bd9b8f81":"from sklearn.model_selection import train_test_split\n\nfrom sklearn.feature_selection import SelectFwe\nfrom sklearn.naive_bayes import MultinomialNB","5b4bf291":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1, random_state=42,shuffle=True)\n\n  \nif skip_early_classification :\n    tpot = Pipeline([('selectfwe', SelectFwe(alpha=0.006)), ('multinomialnb', MultinomialNB(alpha=0.01, fit_prior=False))])\n    tpot.fit(X_train ,y_train)\n    steps = tpot.steps\nelse:\n    tpot = TPOTClassifier(generations=generations, population_size=population_size, n_jobs=n_jobs,\n                          max_time_mins=max_time_mins,   random_state= random_state,\n                          config_dict=config_dict,\n                          memory=memory,\n                          early_stop = early_stop,\n                          verbosity = verbosity)\n    \n    tpot.fit(X_train ,y_train)\n    steps = tpot.fitted_pipeline_.steps\n\npure_dataset_score  = tpot.score(X_test,y_test)*100\nprint(\"First classification's accuracy is {:.2f} % and model's steps are: {}\".format(pure_dataset_score,steps))","93a25de7":"import plotly.express as px \nimport plotly.graph_objects as go\nimport re\n\ndef smooth(text):\n    estimator = text.split(\"(\")[0]\n    text = estimator+text.replace(estimator,\"\",)\n    text = text.replace(\"input_matrix\",\"data\").replace(\"estimator\",\"est\").replace(\"verbosity=0\",\"\").replace(\"__\",\"\").replace('weights','w').replace(\"n_jobs=1,\",\"\")\n    text = re.sub(r'(?<=\\()(?P<preprocessor_set>\\w*?\\(.*?\\))(?P<estimator_tail>.*)',r'pre_data\\g<estimator_tail><br>\\g<preprocessor_set>',text) \n    return text\n\ndef make_icicle_figure(pipelines_dict):\n    names_list = []\n    parent_list=[]\n    acc_list=[]\n    labels = []\n    for d in pipelines_dict:\n        \n        pipeline = smooth(d)\n        names_list.append(pipeline )\n        labels.append(pipeline)\n        internal_cv_score = pipelines_dict[d]['internal_cv_score']\n        acc_list.append(internal_cv_score)\n\n        predecessor = pipelines_dict[d]['predecessor'][0]\n        parent_list.append(smooth(predecessor) )\n        \n    fig = px.icicle(\n        names = names_list,\n        parents = parent_list,\n        values= acc_list,\n        color=acc_list,\n        title = 'Tpot Results',\n    )\n    return fig","4295b595":"if not skip_early_classification :\n    fig = make_icicle_figure(tpot.evaluated_individuals_)\n    fig.update_layout( \n        autosize=False,\n        width=1500,\n        height=800\n    )\n    fig.show()","e0d114eb":"data['#_posts'] = data['posts'].apply(lambda x : len(x.split(\"|||\")))\ndisplay(data['#_posts'].describe())","18b61dd6":"print(\"There are {} rows with less than 50 posts. This is the {:.2f} % of the dataset.\\n\".format( len(data[data['#_posts']<50]),len(data[data['#_posts']<50])\/data.shape[0]*100 ) )\nprint(\"There are {} rows with less than 40 posts. This is the {:.2f} % of the dataset.\\n\".format( len(data[data['#_posts']<40]),len(data[data['#_posts']<40])\/data.shape[0]*100 ) )\ndata[data['#_posts']<50]['#_posts'].sort_values().plot(kind='bar',title=\"Less than 50\",figsize=(15,4)) ; plt.show()","79cfa951":"print(\"\\nThere are {} rows with more than 50 posts. This is the {:.2f} % of the dataset.\\n\".format( len(data[data['#_posts']>50]),len(data[data['#_posts']>50])\/data.shape[0]*100 ) )","4a889a05":"data.groupby('#_posts').sum().plot(kind='bar',title=\"#_posts variation\",figsize=(15,4)) ; plt.show()","dae13764":"tmp = data[data['#_posts']<50].groupby('type').count()[['#_posts']].join( data.groupby('type').count()[['#_posts']],lsuffix=\"<50\",rsuffix='_total' )\ntmp['Contain (%)'] = round(tmp['#_posts<50'] \/ tmp['#_posts_total'] *100)\ntmp.sort_values('Contain (%)',ascending=False)","0cce09d3":"data.groupby('type').median()[['#_posts']].join(data.groupby('type').mean()[['#_posts']],lsuffix='_median',rsuffix='_mean')","a93c5437":"np.corrcoef(data['#_posts'],data['num_type'])[0][1]","7f201bd5":"data","54d9f0d5":"voc = tfidf.vocabulary_\nprint(\"Basic post's vocabulary contains {} words.\".format(len(voc)))","c0b5853a":"import string\npunctuation = string.punctuation","11559e1d":"import spacy\nnlp = spacy.load(\"en_core_web_sm\",disable=['tagger','ner'])\n\nstopwords = spacy.lang.en.stop_words.STOP_WORDS ","846859d5":"text = \"Hello, what's up? ||| What a nice day!\"\nprint(\"From spaCy parser: \",[token for token in nlp(text)])\nprint(\"From split method: \",text.split())","8b1a419f":"givenpatterns =[(r'(\\|\\|\\|)',r' ||| '),\n                (r'http\\S+', r''),\n                (r\"(.[^\\|])\\1{2,}\",r\"\\1\\1\"),\n                (r'[0-9]\\S+',r''),\n                (r\"'\",r''),\n                (r' {2,}',r' ')] #delete any queue of spaces\n\ndef cleaning(text):\n    text = text.lower()\n    \n    text = re.sub(r'(\\|\\|\\|)',r' ||| ',text)\n    text = re.sub(r'http\\S+', r' URL ',text)\n    \n    doc = nlp(text)\n    text = \" \".join([word.text for word in doc if not word.text in stopwords])\n    \n    for token in punctuation.replace(\"'\",'').replace(\"|\",''):\n        text = text.replace(token,' ')\n        \n    for (raw,rep) in givenpatterns:\n        regex = re.compile(raw)\n        text = regex.sub(rep,text)\n        \n    #This is a try to remove non-Latin words.    \n    text =re.sub(r'[^a-z\\| ]','',text)\n    \n    return text","1a3b2426":"text = data['posts'][0]\ntext","17e56e82":"cleaning(text)","589b3327":"if skip_first_cleaning:\n    first_clean_posts = pd.read_csv(\"..\/input\/only-df-clean-posts\/first_clean_posts.csv\",index_col=0)['first_clean_posts']\n    data=data.join(first_clean_posts)\nelse:\n    # This is a time consuming process...\n    data['first_clean_posts']=data['posts'].apply(cleaning)\n    ","a1770cd6":"data.head(5)","11093d29":"from sklearn.feature_extraction.text import CountVectorizer","d2de30d1":"first_clean_cv = CountVectorizer().fit(data['first_clean_posts'])\nfirst_clean_voc = first_clean_cv.vocabulary_\nprint(\"After cleaning, dataset contains {} words. This is {:.2f}% of the initial dataset's total words.\".format(len(first_clean_voc),len(first_clean_voc)\/len(voc)*100))","fcc52a92":"X=TfidfVectorizer( stop_words='english').fit_transform(data['first_clean_posts'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42,shuffle=True)\n\nif skip_first_clean_posts_classification :\n\n    tpot = Pipeline([('selectfwe', SelectFwe(alpha=0.006)), ('multinomialnb', MultinomialNB(alpha=0.01, fit_prior=False))])\n    tpot.fit(X_train ,y_train)\n    steps = tpot.steps\nelse:\n    tpot = TPOTClassifier(generations=generations, population_size=population_size, n_jobs=n_jobs,\n                          max_time_mins=max_time_mins,   random_state= random_state,\n                          config_dict=config_dict,\n                          memory=memory,\n                          early_stop = early_stop,\n                          verbosity = verbosity)\n\n    tpot.fit(X_train ,y_train)\n    steps = tpot.fitted_pipeline_.steps\n    \nfirst_clean_dataset_score = tpot.score(X_test,y_test)*100\nprint(\"With clean data we have {:.2f} % accuracy. And the best pipeline is: {}\".format(\n                    first_clean_dataset_score ,steps)  )","5175c4b7":"if not skip_first_clean_posts_classification :\n    fig = make_icicle_figure(tpot.evaluated_individuals_)\n    fig.update_layout( \n        autosize=False,\n        width=1500,\n        height=800\n    )\n    fig.show()","0c8176d9":"print(\"Score without cleaning: \\t{:.2f} %\".format(pure_dataset_score))\nprint(\"Score after first cleaning: \\t{:.2f} %\".format(first_clean_dataset_score))","34b6babf":"DTM = pd.DataFrame(first_clean_cv.transform(data['first_clean_posts']).toarray(),index=data.index,columns=first_clean_cv.get_feature_names()).join(data['type'],rsuffix=\"_pers\")\nDTM","9cec2267":"# DTM=DTM.join(data['type'],rsuffix=\"_pers\")\nfr= DTM.groupby('type_pers').sum().transpose()\nfr","b7d95ada":"import wordcloud as wc","77b9371a":"fig, ax = plt.subplots(len(data['type'].unique()), sharex=True, figsize=(20,15))\nfig.patch.set_facecolor('xkcd:tan')\n\nwordcloud = wc.WordCloud(stopwords=None,background_color='white',relative_scaling=1,max_font_size=100 ,normalize_plurals=False)\n\nfor i,pers in enumerate(data['type'].unique(),start=1):\n    plt.subplot(4,4,i)\n    scores = fr[pers].sort_values(ascending=False)[:10]\n    wordcloud.fit_words(scores) \n    plt.imshow(wordcloud,interpolation='bilinear'); plt.title(pers); plt.axis('off')\nplt.show()","ff00feea":"for pers in DTM['type_pers'].unique():\n    scores = fr[pers].sort_values(ascending=False)[:10]\n    print(\"###\",pers,\": \",scores.index.values,\"\\n\")","4b9c2b96":"personalities = [personality.lower() for personality in data['type'].unique()]\npersonalities_expression = r'|'.join([personality+\"s*\" for personality in personalities])\n\ndef second_cleaning(text):\n    text = re.sub(personalities_expression,\"\",text)\n    return text","d287c9e6":"if skip_second_cleaning:\n    clean_posts = pd.read_csv(\"..\/input\/only-df-clean-posts\/clean_posts.csv\",index_col=0)['clean_posts']\n    data=data.join(clean_posts)\nelse:\n    # This is not a time consuming process...\n    data['clean_posts']=data['first_clean_posts'].apply(second_cleaning)\n    ","08592752":"data.head()","42509e5c":"clean_cv = CountVectorizer().fit(data['clean_posts'])\nclean_voc = clean_cv.vocabulary_\nprint(\"After second cleaning, dataset contains {} words.This is {:.2f}% of the initial dataset's total words.\".format(len(clean_voc),len(clean_voc)\/len(voc)*100))","9ef58704":"X=TfidfVectorizer(stop_words='english').fit_transform(data['clean_posts'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42,shuffle=True)\nif skip_second_clean_posts_classification:\n    \n    tpot = Pipeline([('selectfwe', SelectFwe(alpha=0.006)), ('multinomialnb', MultinomialNB(alpha=0.01, fit_prior=False))])\n    tpot.fit(X_train ,y_train)\n    steps = tpot.steps\nelse:\n    \n    tpot = TPOTClassifier(generations=generations, population_size=population_size, n_jobs=n_jobs,\n                          max_time_mins=max_time_mins,   random_state= random_state,\n                          config_dict=config_dict,\n                          memory=memory,\n                          early_stop = early_stop,\n                          verbosity = verbosity)\n    \n    tpot.fit(X_train ,y_train)\n    steps = tpot.fitted_pipeline_.steps\n                          \n\nsecond_clean_dataset_score = tpot.score(X_test,y_test)*100\nprint(\"With twice clean data we have {:.2f} % accuracy. And the best pipeline is: {}\".format(second_clean_dataset_score,steps))","47aa6167":"if not skip_second_clean_posts_classification :\n    fig = make_icicle_figure(tpot.evaluated_individuals_)\n    fig.update_layout( \n        autosize=False,\n        width=1500,\n        height=800\n    )\n    fig.show()","088cf38a":"print(\"Score without cleaning: \\t{:.2f} %\".format(pure_dataset_score))\nprint(\"Score after first cleaning: \\t{:.2f} %\".format(first_clean_dataset_score))\nprint(\"Score after second cleaning: \\t{:.2f} %\".format(second_clean_dataset_score))","216a368a":"if save_clean_data_to_files:\n    dataset['first_clean_posts'] = dataset['posts'].apply(cleaning)\n    dataset[['type','first_clean_posts']].to_csv(\"first_clean_posts.csv\")\n    dataset['clean_posts'] = dataset['first_clean_posts'].apply(second_cleaning)\n    dataset[['type','clean_posts']].to_csv(\"clean_posts.csv\")\nelse:\n    print(\"Not Saved!\")","d0eed84b":"data","4ff2ff70":"nlp = spacy.load(\"en_core_web_sm\",disable=['parser','ner'])\n\ndef spacy_lemma(text):\n    doc = nlp(text)\n    return ' '.join([word.lemma_ for word in doc])","172c69ae":"if skip_lemmatazation :\n    data = data.join(pd.read_csv(\"\/kaggle\/input\/only-df-clean-posts\/lemmatized_posts.csv\",index_col=0) )\n    print(\"Data loaded.\")\nelse:\n    data['lemma_posts'] = data['clean_posts'].map(spacy_lemma)\n    print(\"Posts have been lemmatized\")","c3b1ecdc":"data.head()","b0fa1dc8":"X=TfidfVectorizer( stop_words='english').fit_transform(data['lemma_posts'])\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42,shuffle=True)\nif skip_lemma_posts_classification:\n    \n    tpot = Pipeline([('selectfwe', SelectFwe(alpha=0.006)), ('multinomialnb', MultinomialNB(alpha=0.01, fit_prior=False))])\n    tpot.fit(X_train ,y_train)\n    steps = tpot.steps\nelse:\n    \n    tpot = TPOTClassifier(generations=generations, population_size=population_size, n_jobs=n_jobs,\n                          max_time_mins=max_time_mins,   random_state= random_state,\n                          config_dict=config_dict,\n                          memory=memory,\n                          early_stop = early_stop,\n                          verbosity = verbosity)\n    \n    tpot.fit(X_train ,y_train)\n    steps = tpot.fitted_pipeline_.steps\n                          \nlemma_post_dataset_score = tpot.score(X_test,y_test)*100\nprint(\"With lemmatised data we have {:.2f} % accuracy. And the best pipeline is: {}\".format(lemma_post_dataset_score,steps))","03015b33":"if not skip_lemma_posts_classification :\n    fig = make_icicle_figure(tpot.evaluated_individuals_)\n    fig.update_layout( \n        autosize=False,\n        width=1500,\n        height=800\n    )\n    fig.show()","8c2d9454":"print(\"Score without cleaning: \\t{:.2f} %\".format(pure_dataset_score))\nprint(\"Score after first cleaning: \\t{:.2f} %\".format(first_clean_dataset_score))\nprint(\"Score after second cleaning: \\t{:.2f} %\".format(second_clean_dataset_score))\nprint(\"Score after lemmatization: \\t{:.2f} %\".format(lemma_post_dataset_score))","e5060c6d":"if lemmatization_and_save:\n    clean_posts = pd.read_csv(\"..\/input\/only-df-clean-posts\/clean_posts.csv\",index_col=0)['clean_posts']\n    dataset=dataset.join(clean_posts)\n    \n    dataset['lemma_posts'] = dataset['clean_posts'].map(spacy_lemma)\n    dataset['lemma_posts'].to_csv(\"lemmatized_posts.csv\")","6cab1feb":"Again, we check how many words are included in our dataset.","ba87593f":"Let's see if there is any **correlation** beetwen number of posts and personality post.","fc752a9f":"\nIn order to split the dataset we will use <code>train_test_split<\/code> and we will keep 10% for test.","723edb9a":"From the above we see that lines with less than 50 posts compose more than 10% of most classes. So we can not assume them as noise and we can not remove them.\n<br>So, most of the lines contains 50 posts.<br>","fd320581":"<h3>We can keep cleaning the dataset, but we are stopping here.<\/h3>","a7e00ef4":"Lemmatisation is the process which we transform different inflected forms of a word into the basic word. In order to do this, I use <a href=\"https:\/\/spacy.io\/\" >spaCy<\/a> library. <br>\n**spaCy** is a python library for natural language process (nlp). It contains a variety of models, which are trained to extract information from text data in a lot of different languages.<br>\nIn detail, I use the spaCy's model for english language <code>en_core_web_sm<\/code>.","6c8c2bd9":"Consider TPOT your Data Science Assistant. TPOT is a Python Automated Machine Learning tool that optimizes machine learning pipelines using genetic programming. More info http:\/\/epistasislab.github.io\/tpot\/ <br>\nTPOTClassifier is what we need. At the moment we do not have to figure out the best model, just a simple accuracy number.\nIn addition to this, dataset is very large and limited RAM is an issue.\nSo, <code>TPOTClassifier<\/code> with TPOT_sparce configuration is the best for us. <code>TPOT_sparce<\/code> is a list of preproccesors and estimators that run fast on sparce matrices.\n","7c88323c":"That's a lot of columns. Let's try to remove words with less than 2 instances throughout all the dataset.","bb3c5604":"By the dataset's description we know the following:\n<ul>\n    <li>This data was collected through the <a href=\"http:\/\/personalitycafe.com\/\">PersonalityCafe<\/a> forum, as it provides a large selection of people and their MBTI personality type, as well as what they have written.<\/li>\n    <li>The dataset is consist of : \n        <ul>\n        <li> 2 categorical columns (<code>type<\/code> and <code>posts<\/code>) and<\/li>  \n        <li> 8675 rows, each row represents a user.<\/li>\n        <\/ul>\n\nSo, each user has a personality <code>type<\/code> and has posted some <code>posts<\/code>.<br>","e6b13732":"What did <code>TfidfVectorizer<\/code> return is the following.","261ffe51":"<h2 name= \"Text Cleaning\">Text cleaning<\/h2> (round 2)","8f9a1745":"Let's use an module called <code>pandas-profiling<\/code> to extract some basic insights.","c3ee03b6":"<h3>Text column's transformation.<\/h3>\nThere are many ways to do this, I choose TfidfVectorizer from scikit-learn, because it is fast and it tends to improve models' accuracy.\nTfidfVectorizer will transform words to numbers. For each person\/row a new row will be showing how frequently each word is used. Finaly a document-word_frequency matrix will be yield.","f476d43d":"Due to imbalance problem and cpu runtime limit, I remove some instances of majority classes. By doing this, we will have no more than 100 instances per class.","f4e9cfd9":"These are the classifiers and transformers TPOTClassifier is able to use (on 'sparce' mode).","3a9a0dc4":"Tpot Tree","d2c18a5f":"<h3>Now we can take a closer look at our dataset.<\/h3>","3331b1ec":"<code>posts<\/code> column contains posts of each person. Each cell is made up of posts separated by <code>|||<\/code>. How many posts does each line contains?","9c7e7817":"We import two libraries for text processing. <br>\n<code>re<\/code> stands for Regular Expresions, it will help in finding spesific character patterns in texts and replace with new ones. For more info <a href=\"https:\/\/docs.python.org\/3\/library\/re.html\">here<\/a> .<br>\n<code>nltk<\/code> stands for Natural Language Toolkit, NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries. For more info <a href=https:\/\/www.nltk.org\/>here<\/a>.","ec8070c2":"We will try to messure the accuracy of some models' predictions. In order to do this, we will use TPOT module. ","c5f4d30e":"In the end, we have to solve a classification problem. So, I want to know what the accuracy will be if we train a model with the initial texts. Next, we will do some text-cleanning and then we will be able to compare new models' accuracy with the initial one.<br>\nIt may be too early for this, but let's see what the accuracy of a simple model is.<br>\nObviously, the classification's features are the <code>posts<\/code>'s text, and target column is the <code>type<\/code> column.<br>\nSad to say, models can train only with numeric values, so we have to convert both <code>posts<\/code> and <code>type<\/code> columns into numbers. We will transform them with diferrent ways.","e9e78cfb":"We can not consider that there is any correlation.","1cc80b44":"<h3>Target column transformation<\/h3>\nWe will transform personality type column into a new one num_type.","32aaa5a5":"<h3>Now we have cleaned up our dataset, let's take a look at vocabulary.<br>\n    Also, we can check each personality type's top words.","4c07f14d":"<h1>Lemmatisation<\/h1>","02fbb4a5":"<h1>First classification try<\/h1>","b38b8c21":"<h2><a id = Test><\/a>Data<\/h2>","bc04578f":"Tpot tree","db5b408e":"So, we see that people like discusing personality types, this is expected because dataset comes from a forum for personalities. Moreover, people like discusing their own personality more frequent than the other ones. This is logical but it is wrong to train a model with this dataset. We have to remove all personalities' references.","f7efca89":"I want to remind you that the data which has been used to train and test the models above is reduced. There is still the issue with imbalanced dataset.","bf58f05c":"What is beneath of 50 ?","8b095430":"<h1>Introduction<\/h1>\nThis notebook is presenting an approach to solve a text-classification problem with machine learning techniques. The solution is including three steps - data understanding, text pre-processing and classification.<br>\n\nThe purpose is to train a model with our clean text dataset, that will be able to make predictions.","add4dbcb":"Now that dataset is more balanced we can continue.","be540231":"How tpot tree has been shaped...","79720b24":"<h1> Text Cleaning<\/h1>","b72afdb1":"If we want to do some classification, we should remove content like urls, contractions and numbers. Also, capitals letters will be tranformed into lower and characters that are repeated more than two times in a row, we will keep only the first two of them (e.x. \"aaaaaand\" will be \"aand\").","3d31ef58":"We have the following insights:\n<ul>\n    <li> <code>type<\/code> column is our target column and it has 16 unique values\/classes.<\/li>\n    <li> there are minority classes and majority classes, so the dataset is imbalanced.<\/li>\n    <li> There are no missing values<\/li>\n    <li> There are links that we have to process<\/li>\n    <li> There are <code>|||<\/code> that seperate posts in each line.<\/li>\n    <li> There may be lines with no latin words.<\/li>\n    \n<\/ul>"}}