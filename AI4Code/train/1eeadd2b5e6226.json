{"cell_type":{"bff33de5":"code","cef9e0ee":"code","60ba1240":"code","31f8aae5":"code","23f72a3b":"code","63bc328f":"code","d8f4518e":"code","10b1cba5":"code","925ab0ca":"code","1728e001":"code","d5b65df3":"code","cd7bfe75":"code","d32ee3b2":"code","acb3cf73":"code","a77d5922":"code","bdfa0973":"code","3ccbd9b4":"code","9e8c6c7d":"code","f7b447bd":"code","9132b8a7":"code","40a59eef":"code","cfbe8800":"code","2a700764":"code","babadc2a":"code","45a14118":"code","64664309":"code","2e6d5289":"code","c7ae70dc":"code","27c0a854":"code","db2b0ec4":"code","38b9cd95":"code","f67f8705":"code","72939a02":"code","eb819982":"code","3733e0e6":"code","bf81e14c":"code","2d01e946":"code","ff030cd2":"code","04cfd147":"code","82cda206":"code","c33c5395":"markdown","187803aa":"markdown","81f59138":"markdown","13dbae3a":"markdown","75877e58":"markdown","e9538dbc":"markdown","5e4c2ee6":"markdown","caeb3485":"markdown","2cc58073":"markdown","b71f1094":"markdown","77567d8a":"markdown","54bed73e":"markdown","995a6cb7":"markdown","42b1b136":"markdown","022e5ad7":"markdown","c99e95c9":"markdown"},"source":{"bff33de5":"!pip install category_encoders","cef9e0ee":"# Importing Libraries\n\n## For Data Operations and Visualizations\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom category_encoders import TargetEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report, roc_auc_score\n\n## For Classifiers\nfrom sklearn.ensemble import RandomForestClassifier as rfc\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier","60ba1240":"import warnings\nwarnings.filterwarnings('ignore')","31f8aae5":"# Getting cwd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","23f72a3b":"# Importing Dataset\ndf = pd.read_csv('\/kaggle\/input\/churn-modelling\/Churn_Modelling.csv')","63bc328f":"df.columns","d8f4518e":"# Dropping off redundant columns\ndf.drop(['RowNumber', 'CustomerId', 'Surname'], inplace = True, axis = 1)  ","10b1cba5":"df.info()","925ab0ca":"# Check for Imbalance\ndf.groupby('Exited')['Geography'].count()","1728e001":"l = LabelEncoder()\ndf['Gender'] = l.fit_transform(df['Gender'])","d5b65df3":"encoder = TargetEncoder()\ndf['country'] = encoder.fit_transform(df['Geography'], df['Exited'])","cd7bfe75":"df.drop(['Geography'], inplace = True, axis = 1)","d32ee3b2":"df","acb3cf73":"# Spliting into dependent and independent vectors\nx = df.drop(['Exited'], axis = 1)\ny = df.Exited","a77d5922":"# Standard Scaling\nS = StandardScaler()\nx = S.fit_transform(x)","bdfa0973":"x_train, x_test, y_train, y_test = train_test_split(x, y ,test_size = 0.25, \n                                                    random_state = 0)","3ccbd9b4":"# fitting my model\nclassifier = rfc(n_estimators = 100, random_state = 0, criterion = 'entropy')\nclassifier.fit(x_train, y_train)","9e8c6c7d":"# predicting the test set results\ny_pred = classifier.predict(x_test)","f7b447bd":"# Checking Accuracy\nprint(classification_report(y_test, y_pred))","9132b8a7":"# fitting my model\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(x_train, y_train)","40a59eef":"# predicting the test set results\ny_pred = classifier.predict(x_test)","cfbe8800":"# Checking Accuracy\nprint(classification_report(y_test, y_pred))","2a700764":"# fitting my model\nclassifier = GaussianNB()\nclassifier.fit(x_train, y_train)","babadc2a":"# predicting the test set results\ny_pred = classifier.predict(x_test)","45a14118":"# Checking Accuracy\nprint(classification_report(y_test, y_pred))","64664309":"# fitting my model\nclassifier = MLPClassifier(activation = \"relu\", alpha = 0.05, random_state = 0)\nclassifier.fit(x_train, y_train)","2e6d5289":"# predicting the test set results\ny_pred = classifier.predict(x_test)","c7ae70dc":"# Checking Accuracy\nprint(classification_report(y_test, y_pred))","27c0a854":"# Importing Necessary Libraries\nfrom sklearn.ensemble import StackingClassifier","db2b0ec4":"# Initialising the Stacking Algorithms\nestimators = [\n        ('naive-bayes', GaussianNB()),\n        ('random-forest', rfc(n_estimators = 100, random_state = 0)),\n        ('mlp', MLPClassifier(activation = \"relu\", alpha = 0.05, random_state = 0))\n        ]","38b9cd95":"# Setting up the Meta-Classifier\nclf = StackingClassifier(\n        estimators = estimators, \n        final_estimator = LogisticRegression(random_state = 0)\n        )","f67f8705":"# fitting my model\nclf.fit(x_train, y_train)","72939a02":"# getting info about the hyperparameters \nclf.get_params()","eb819982":"# predicting the test set results\ny_pred = clf.predict(x_test)","3733e0e6":"# Checking Accuracy\nprint(classification_report(y_test, y_pred))","bf81e14c":"# Defining Parameter Grid\nparams = {'final_estimator__C': [1.0,1.1,1.5],\n          'final_estimator__max_iter': [50,100,150,200],\n          'final_estimator__n_jobs': [1,-1,5],\n          'final_estimator__penalty': ['l1','l2'],\n          'final_estimator__random_state': [0],\n          }","2d01e946":"# Initialize GridSearchCV\ngrid = GridSearchCV(estimator = clf, \n                    param_grid = params, \n                    cv = 5,\n                    scoring = \"roc_auc\",\n                    verbose = 10,\n                    n_jobs = -1)","ff030cd2":"# Fit GridSearchCV\ngrid.fit(x_train, y_train)","04cfd147":"# predicting the test set results\ny_pred = grid.predict(x_test)","82cda206":"# Checking Accuracy\nprint(classification_report(y_test, y_pred))","c33c5395":"# 8. Multi-Layer Perceptron Classifier ","187803aa":"# 10. Tuning the Meta-Classifier","81f59138":"Note : You can setup any classifier as stacking classifiers or meta-classifiers according to your choice or performance.","13dbae3a":"# End","75877e58":"# Stacking Classifier\n\nDocumentations -\n\n1.   [Sklearn Documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingClassifier.html)\n2.   [Mlxtend](http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingClassifier\/)\n\n","e9538dbc":"# 7. Naive Bayes Classifier ","5e4c2ee6":"# 2. Feature Engineering and Selection ","caeb3485":"# 4. Splitting the dataset into training set and test set  ","2cc58073":"# 5. Random Forest Classifier","b71f1094":"Encoding Categorical Variables","77567d8a":"# 6. Logistic Regression Classifier","54bed73e":"On a Naive Note, Regardless of the improvement\/demotion we got after hyperparameter tuning, the basic idea was to demonstrate how it's done.","995a6cb7":"\n# 1. Importing Libraries and Dataset\n","42b1b136":"# 3. Data Preprocessing  ","022e5ad7":"The below was for Simple Random Forest Classifier,\n```\n precision    recall  f1-score   support\n\n           0       0.87      0.96      0.91      1991\n           1       0.72      0.45      0.56       509\n\n    accuracy                           0.85      2500\n   macro avg       0.80      0.70      0.73      2500\nweighted avg       0.84      0.85      0.84      2500\n```\nSo, by comparison on a naive basis, we can say that we can obtain better results by merging two or more algorithms together forming ensemble based learning and obtain better results without hyperparameter tuning as compared to 1-Base Algo Ensembled Based Learning.\n","c99e95c9":"# 9. Stacking Classifier"}}