{"cell_type":{"e7aa212d":"code","858d8de2":"code","f3e2ba2a":"code","6ce6d954":"code","2cae3215":"code","bcb7e8f1":"code","3b415e8d":"code","090d7ba8":"code","29700cc6":"code","311b5c34":"code","05687bb8":"code","52307b58":"code","699aa572":"code","a3bd9053":"code","84fd3ac2":"code","f333fcc4":"code","5b5301a7":"code","b7e2c7c3":"code","1430d50c":"code","d945c653":"code","1cc7ac94":"code","7d04bac7":"code","66118314":"code","933b3d2a":"markdown","2414cf99":"markdown","fc5fe1e7":"markdown","bd36e9df":"markdown","5cf39b96":"markdown","b5331ab3":"markdown","e2507032":"markdown","41e64e9f":"markdown","8c30b2e4":"markdown","40e03b0a":"markdown","e07b5bf8":"markdown","f6df730a":"markdown","3bcdcb33":"markdown","a4060c76":"markdown","7ad23f89":"markdown","933da5ff":"markdown","9cc40484":"markdown","77b824c2":"markdown","eb6f67c7":"markdown","6c1b2746":"markdown","63c6c7a2":"markdown","70b9bf5e":"markdown","976a5cc6":"markdown","93354c5f":"markdown"},"source":{"e7aa212d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","858d8de2":"#importing the dataset\ndata = pd.read_csv('..\/input\/BlackFriday.csv')","f3e2ba2a":"data.head()","6ce6d954":"missing_values = data.isnull().sum().sort_values(ascending = False)\nmissing_values = missing_values[missing_values > 0]\/data.shape[0]\nprint(f'{missing_values *100} %')","2cae3215":"data = data.fillna(0)","bcb7e8f1":"missing_values = data.isnull().sum().sort_values(ascending = False)\nmissing_values = missing_values[missing_values > 0]\/data.shape[0]\nprint(f'{missing_values *100} %')","3b415e8d":"data.dtypes","090d7ba8":"#unique values in Gender parameter\ngender = np.unique(data['Gender'])\ngender","29700cc6":"def map_gender(gender):\n    if gender == 'M':\n        return 1\n    else:\n        return 0\ndata['Gender'] = data['Gender'].apply(map_gender)","311b5c34":"age = np.unique(data['Age'])\nage","05687bb8":"def map_age(age):\n    if age == '0-17':\n        return 0\n    elif age == '18-25':\n        return 1\n    elif age == '26-35':\n        return 2\n    elif age == '36-45':\n        return 3\n    elif age == '46-50':\n        return 4\n    elif age == '51-55':\n        return 5\n    else:\n        return 6\ndata['Age'] = data['Age'].apply(map_age)","52307b58":"city_category = np.unique(data['City_Category'])\ncity_category","699aa572":"def map_city_categories(city_category):\n    if city_category == 'A':\n        return 2\n    elif city_category == 'B':\n        return 1\n    else:\n        return 0\ndata['City_Category'] = data['City_Category'].apply(map_city_categories)","a3bd9053":"city_stay = np.unique(data['Stay_In_Current_City_Years'])\ncity_stay","84fd3ac2":"def map_stay(stay):\n        if stay == '4+':\n            return 4\n        else:\n            return int(stay)\n#             current_years = stay\n#             current_years = current_years.astype(int)\n#             return current_years\ndata['Stay_In_Current_City_Years'] = data['Stay_In_Current_City_Years'].apply(map_stay)    ","f333fcc4":"cols = ['User_ID','Product_ID']\ndata.drop(cols, inplace = True, axis =1)","5b5301a7":"data.head()","b7e2c7c3":"data[['Gender','Purchase']].groupby('Gender').mean().plot.bar()\nsns.barplot('Gender', 'Purchase', data = data)\nplt.show()","1430d50c":"data[['Age','Purchase']].groupby('Age').mean().plot.bar()\nsns.barplot('Age', 'Purchase', data = data)\nplt.show()","d945c653":"sns.boxplot('Age','Purchase', data = data)\nplt.show()","1cc7ac94":"data[['City_Category','Purchase']].groupby('City_Category').mean().plot.bar()\nsns.barplot('City_Category', 'Purchase', data = data)\nplt.show()","7d04bac7":"corrmat = data.corr()\nfig,ax = plt.subplots(figsize = (12,9))\nsns.heatmap(corrmat, vmax=.8, square=True)","66118314":"mean_cat_1 = data['Product_Category_1'].mean()\nmean_cat_2 = data['Product_Category_2'].mean()\nmean_cat_3= data['Product_Category_3'].mean()\nprint(f\"PC1: {mean_cat_1} \\n PC2: {mean_cat_2} \\n PC3 : {mean_cat_3}\")","933b3d2a":"It looks like that men tend to spend more on Black Friday although women are not far behind.","2414cf99":"# **EDA**","fc5fe1e7":"Lets see how city category affects the purchase.","bd36e9df":"So, we have taken care of the missing values. Let's move on and see what all data types are avialable to us inour dataset.","5cf39b96":"So, the available datatypes are : int64, float64 and objects.\nWe will leave the numeric datatypes alone and focus on object datatypes as the cannot be directly fen into a Machine Learning Model","b5331ab3":"**Beautiful!**","e2507032":"It can be seen that nothing is highly correlated with the Purchase variable.\nAlthough a few conclusions can be drawn:\n1. Product_Category_1 has a negative correlation with Purchase.\n2. Maritial_Status and Age are strongly correlated. As Expected.\n3. Product_Category_3 has a strong correlation with Purchase. Maybe the products in this category were cheap. Let's chrun out some number related to this.","41e64e9f":"I believe that the NaN values for **Product_Category_2** and **Product_Categrory_3** would mean that the concerned person did not buy the products from these categories.\n\nHence, I believe that it would be safe to replace them with 0.","8c30b2e4":"So, we do not have any 'Other' gender type. I will create a fuction and map M=1 and F=0. No sexism intended.","40e03b0a":"Let's see how Age affects the Purchase. Of the top of my head I can say that people of higher age will tend to spen more as they would have more income. Let's see where this gets us.","e07b5bf8":"We can drop  **User_ID** and **Product_ID** parameters. Let's get going","f6df730a":"Lets see how are final dataframe looks like!","3bcdcb33":"Not much of a deciation there. We can say that no matter what age group you belong to, you are gonna make full use of your purchasing power on a Black Friday. Maybe, because everything is so damn cheap (That's what I have heard! :P)","a4060c76":"Let's see what's cooking in the **Age** parameter","7ad23f89":"I will also plot some boxplots to study the deviation in  Age vs Purchase","933da5ff":"Let's Start mapping","9cc40484":"Well, yes! My intial hypothesis was correct. It can be seen that product category 3 is much lesser in price tham 1 or 2. Hence, is the strong correlation.","77b824c2":"Okay so, the people belonging to category 0 tend to spend a little more. These may be the more developed cities that we are talking about here. ","eb6f67c7":"# **DATA CLEANING **","6c1b2746":"Let's now draw a heatmap to clearly see what are the correlations here.","63c6c7a2":"Well, that's taken care of. \n\nLets tend to the needs of **City_Category**.","70b9bf5e":"Let's do the final mapping : **Stay_In_Current_City_Years**","976a5cc6":"Let's get **Gender** first.","93354c5f":"So, we are having bins. Lets make these bins into numeric values"}}