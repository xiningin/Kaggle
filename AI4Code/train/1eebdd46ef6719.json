{"cell_type":{"97bdab4b":"code","7a8a0e39":"code","cc26a8d8":"code","3e7fb968":"code","a3a727c0":"code","60750bc9":"code","a9957d57":"code","febb08c8":"code","34c6bc23":"code","4bc14a9a":"code","6f28c4ca":"code","239fc2db":"code","1572c254":"code","d4781b76":"code","6a972741":"code","cdce1525":"code","aeabf1b6":"code","a8f805aa":"code","dd80acd0":"code","fa02e26e":"code","2f6373e5":"code","9ae2dfbb":"code","9171b88d":"code","2ff79099":"code","a1e3edbb":"code","15e52209":"code","9eadfe43":"code","3420d8f7":"code","aa3283bc":"code","f40a8895":"code","cdf89f4a":"code","48d66106":"code","37850b7a":"code","48225715":"markdown","ef71dc1d":"markdown","4211810f":"markdown","e236e640":"markdown","62903e91":"markdown","865697c7":"markdown","66169871":"markdown","2790e92c":"markdown","fb7d0bf0":"markdown"},"source":{"97bdab4b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.experimental import enable_iterative_imputer  \nfrom sklearn.impute import IterativeImputer\n\nimport lightgbm as lgbm\nimport xgboost as xgb\nimport catboost as cat\n\nimport optuna\n\nimport missingno as msno\n\n%matplotlib inline","7a8a0e39":"train = pd.read_csv(\"..\/input\/song-popularity-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/song-popularity-prediction\/test.csv\")","cc26a8d8":"train.head(5)","3e7fb968":"train.info()","a3a727c0":"train[\"song_popularity\"].describe()","60750bc9":"sns.histplot(data=train, x=\"song_popularity\", bins=2);","a9957d57":"# Skewness\ntrain[\"song_popularity\"].value_counts(normalize=True)* 100","febb08c8":"# unique values ( <= 15 -> Categorical Features)\ntrain.nunique() > 15","34c6bc23":"numerical_cols = ['song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'liveness', 'loudness', 'speechiness', 'tempo', 'audio_valence']","4bc14a9a":"train.isnull().sum()","6f28c4ca":"msno.bar(train, color=(.6, .4, .4));","239fc2db":"msno.matrix(train, color=(0.55, 0.25, 0.75));","1572c254":"msno.heatmap(train);","d4781b76":"def corr_map(df):\n    corr = df[numerical_cols].corr()\n    mask = np.zeros_like(corr, dtype='bool')\n    mask[np.triu_indices_from(mask)] = True\n    plt.subplots(figsize=(11, 9))\n    sns.heatmap(corr, mask=mask, annot=True);    ","6a972741":"corr_map(train[numerical_cols])","cdce1525":"def feature_plot(df, feature, color, ax):\n    sns.kdeplot(data=df, x=feature, color=color, fill=True, ax = ax)\n    ax.set_title(feature)","aeabf1b6":"fig, ax = plt.subplots(5, 2, figsize=(12,16))\ncounter = 0\nfor i in range(5):\n    for j in range(2):\n        feature_plot(train, numerical_cols[counter], \"red\", ax[i,j])\n        counter += 1\nplt.subplots_adjust(wspace=0.4, hspace=0.8)\nplt.show()","a8f805aa":"fig, ax = plt.subplots(1, 3, figsize=(16, 6))\nsns.countplot(x=train['time_signature'], ax = ax[0])\nsns.countplot(x=train['audio_mode'], ax = ax[1])\nsns.countplot(x=train['key'], ax = ax[2])\nplt.show()","dd80acd0":"columns = [c for c in train.columns if c not in (\"id\", \"song_popularity\")]\nX = train[columns]\ny = train[\"song_popularity\"]","fa02e26e":"def run(trial, data=X,target=y):\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=42)\n    params = {\n        'metric': 'auc', \n        'random_state': 22,\n        \"n_estimators\": trial.suggest_categorical(\"n_estimators\", [9000, 10000, 11000, 12000, 13000, 14000]),\n        'boosting_type': trial.suggest_categorical(\"boosting_type\", [\"gbdt\"]),\n        'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-3, 10.0),\n        'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-3, 10.0),\n        'bagging_fraction': trial.suggest_categorical('bagging_fraction', [.4, .5, .45, 0.6, 0.7]),\n        'feature_fraction': trial.suggest_categorical('feature_fraction', [.3, .4, .5, 0.6, 0.7, 0.80]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [.018, .0186, .0192, 0.0196, .0193, .0199]),\n        'max_depth': trial.suggest_int('max_depth', 5, 20, step=1),\n        'num_leaves' : trial.suggest_int('num_leaves', 650, 850, step=5),\n        'min_child_samples': trial.suggest_int('min_child_samples', 40, 80, step=5),      \n        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 300, 600, step=20),\n        \"max_bin\": trial.suggest_int(\"max_bin\", 250, 350),\n        'min_gain_to_split': trial.suggest_categorical('min_gain_to_split', [0.02084372652774491, 0.01084372652774491, 0.03084372652774491]),\n    }\n    \n    clf = lgbm.LGBMClassifier(**params)  \n    clf.fit(\n            X_train, y_train,\n            eval_set=[(X_valid, y_valid), (X_train, y_train)],\n            callbacks=[lgbm.log_evaluation(period=100), lgbm.early_stopping(stopping_rounds=100)],           \n    )\n    \n    y_proba = clf.predict_proba(X_valid)[:, 1]\n    auc = roc_auc_score(y_valid, y_proba)\n    return auc","2f6373e5":"# best_params = optuna.create_study(direction='maximize')\n# best_params.optimize(run, n_trials=100)\n# best_params.best_params\n# best_params.best_value","9ae2dfbb":"imputer = IterativeImputer()\ntrain = pd.DataFrame(imputer.fit_transform(train), columns=train.columns)\ntest = pd.DataFrame(imputer.fit_transform(test), columns=test.columns)","9171b88d":"X = train[columns]\ny = train['song_popularity']\nX_test = test[columns]","2ff79099":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\n\nlgbm_params = {\n    'boosting_type': 'gbdt',\n    'n_estimators': 9000,\n    'learning_rate': 0.0196,\n    'num_leaves': 740,\n    'min_child_samples': 60,\n    'max_depth': 12,\n    'min_data_in_leaf': 500,\n    'max_bin': 280,\n    'lambda_l1': 0.49149217034909334,\n    'lambda_l2': 0.017202393346011927,\n    'min_gain_to_split': 0.02084372652774491,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 1,\n    'feature_fraction': 0.4,\n}\n\npredictions = []\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = lgbm.LGBMClassifier(**lgbm_params)\n    model.fit(\n        X_train, \n        y_train,\n        eval_set=[(X_valid, y_valid), (X_train, y_train)],\n        callbacks=[lgbm.log_evaluation(period=100), lgbm.early_stopping(stopping_rounds=100)],\n\n    )\n    \n    valid_preds = model.predict_proba(X_valid)[:,1]    \n    score = roc_auc_score(y_valid, valid_preds)\n    scores.append(score)\n    print(f\"Fold: {fold}, AUC: {score}\")\n    test_pred = model.predict_proba(X_test)[:,1]\n    predictions.append(test_pred)\n    \nprint(np.mean(scores))    \n    \n    ","a1e3edbb":"preds = np.mean(np.column_stack(predictions), axis=1)\nsubmission = pd.DataFrame(columns = ['id', 'song_popularity'])\nsubmission['id'] = test.index\nsubmission['song_popularity'] = preds\nsubmission.to_csv(\"lgb.csv\", index=False)\nsubmission","15e52209":"def run(trial, data=X,target=y):\n    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=42)\n    \n    param = {\n        'tree_method':'gpu_hist',\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': 4000,\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n    }    \n    \n    clf = xgb.XGBClassifier(**param)\n    clf.fit(X_train, y_train, eval_set=[(X_valid, y_valid), (X_train, y_train)], verbose=20)\n    \n    y_proba = clf.predict_proba(X_valid)[:, 1]\n    auc = roc_auc_score(y_valid, y_proba)\n    return auc\n","9eadfe43":"# best_params = optuna.create_study(direction='maximize')\n# best_params.optimize(run, n_trials=50)","3420d8f7":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\n\nxgb_params= {\n    'tree_method':'gpu_hist', \n    'gpu_id': 0,\n    'max_depth': 12,\n    'n_estimators': 5600,\n    'learning_rate': 0.07074059946646541,\n    'subsample': 0.8,\n    'colsample_bytree': 0.6000000000000001,\n    'colsample_bylevel': 0.5,\n    'min_child_weight': 0.00390933891195369,\n    'reg_lambda': 2176.9882633091584,\n    'reg_alpha': 0.00027465681042320085,\n    'gamma': 0.45590270702576924\n}\n\n\npredictions = []\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = xgb.XGBRegressor(**xgb_params)\n    model.fit(X_train,y_train,verbose=20)\n    \n    valid_preds = model.predict(X_valid)\n    score = roc_auc_score(y_valid, valid_preds)\n    scores.append(score)\n    print(f\"Fold: {fold} - AUC: {score}\")\n    test_pred = model.predict(X_test[columns])\n    predictions.append(test_pred)\n    \nprint(f\"Mean: {np.mean(scores)}\")    ","aa3283bc":"preds = np.mean(np.column_stack(predictions), axis=1)\nsubmission = pd.DataFrame(columns = ['id', 'song_popularity'])\nsubmission['id'] = test.index\nsubmission['song_popularity'] = preds\nsubmission.to_csv(\"xgb.csv\", index=False)\nsubmission","f40a8895":"def run(trial, data=X,target=y):    \n    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.10, random_state=42)\n    \n    param = {\n        'learning_rate': trial.suggest_discrete_uniform(\"learning_rate\", 0.001, 0.02, 0.001),\n        'bootstrap_type': trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS']),\n        'depth': trial.suggest_int('depth', 9, 15),\n        'min_child_samples': trial.suggest_categorical('min_child_samples', [1, 4, 8, 16, 32]),\n        'iterations': trial.suggest_categorical('iterations', [5, 10, 15, 25, 50]),        \n    }    \n    clf = cat.CatBoostRegressor(**param)\n    clf.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=20, early_stopping_rounds=100)\n    \n    y_proba = model.predict(X_valid)    \n    auc = roc_auc_score(y_valid, y_proba)\n    \n    return auc    ","cdf89f4a":"# best_params = optuna.create_study(direction='maximize')\n# best_params.optimize(run, n_trials=50)","48d66106":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = []\npredictions = []\n\nparams = {\n    'learning_rate': 0.007,\n    'depth': 10,\n    'l2_leaf_reg': 1.5,\n    'min_child_samples': 4\n}\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n    \n    X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n    y_train , y_valid = y.iloc[train_idx], y.iloc[val_idx]\n    \n    model = cat.CatBoostRegressor(**params)\n    model.fit(X_train, y_train,verbose=False)\n    \n    valid_preds = model.predict(X_valid)    \n    score = roc_auc_score(y_valid, valid_preds)    \n    scores.append(score)    \n    print(f\"Fold: {fold} - AUC: {score}\")\n    test_pred = model.predict(X_test[columns])\n    predictions.append(test_pred)\n    \nprint(f\"Mean: {np.mean(scores)}\")","37850b7a":"preds = np.mean(np.column_stack(predictions), axis=1)\nsubmission = pd.DataFrame(columns = ['id', 'song_popularity'])\nsubmission['id'] = test.index\nsubmission['song_popularity'] = preds\nsubmission.to_csv(\"cat.csv\", index=False)\nsubmission","48225715":"# CatBoost","ef71dc1d":"# EDA \n### Feature Distribution","4211810f":"# XGBoost","e236e640":"# Target Analysis","62903e91":"### Numeric Features","865697c7":"### Categorical Features","66169871":"# Features Analysis","2790e92c":"# Missing Values","fb7d0bf0":"# LGBM"}}