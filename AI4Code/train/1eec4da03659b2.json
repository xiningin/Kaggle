{"cell_type":{"fbceb6bf":"code","82f389e9":"code","f8697c3e":"code","f819a59a":"code","47bc84a3":"code","b043f09e":"code","b0d6fcc3":"code","b08b2e73":"code","a50ac7a7":"code","0367e52c":"code","974c7d3a":"code","dbc4c9ec":"code","ee94f5a2":"code","2c0f75ab":"code","012fbf61":"code","e4535fa0":"code","fbd98ee8":"code","cd45a408":"code","c4ce7997":"code","26842289":"code","fe2c9c2e":"code","a0768ae0":"code","18804007":"code","8d3358de":"code","e063608c":"code","08e27fc3":"code","4fc61820":"code","806226f5":"code","aa437190":"code","a923d378":"code","3da5b5fa":"code","32a65d2d":"code","1c82dc84":"code","821cd2af":"code","414eb14d":"code","f9aaf2b1":"code","c96c8bfe":"code","417832ee":"code","bafa4591":"code","f55ace81":"code","8a6152bb":"code","423be6b2":"code","b27274db":"code","75d8826a":"code","dd9eb80e":"code","93b37caa":"code","5f28086b":"code","2cfd26ff":"code","e681d949":"code","218e9ef0":"code","57e66353":"code","f21737ae":"code","9226e97c":"code","d2dbdbed":"code","e8839a5d":"code","4ebc6da1":"code","99fc4fcd":"code","86d613ac":"code","444893cf":"code","5e152dac":"code","88c9efaf":"code","5298b09e":"code","85189aa3":"code","9722e3b3":"code","ad0444c4":"markdown","0274fe48":"markdown","ef5e8177":"markdown","3e76f530":"markdown","ab707826":"markdown","c489880f":"markdown","2f865cff":"markdown","192f8e0c":"markdown","a1812d73":"markdown","5d008ada":"markdown"},"source":{"fbceb6bf":"# First lets import the libraries\nimport pandas as pd\nimport datetime as dt\nimport re\nimport numpy as np\nimport calendar\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mpl_toolkits.basemap import Basemap\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import accuracy_score\nimport xgboost as xgb \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nimport os\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n# Setting pandas display options and column names for the dataset\npd.set_option('display.max_columns', 500)\ncols = ['datetime','city','state','country','shape','seconds',\n              'minutes','comments','date added','lat','long']\n\n# loading the uncleaned dataset and assign columns names\ndf = pd.read_csv('\/kaggle\/input\/complete.csv', sep=',',names =cols)\ndf = df.drop(0, axis=0)\ndf.head(3)","82f389e9":"df.info()","f8697c3e":"df.isna().sum()","f819a59a":"#Using Fillna To Impute 'unspecified' in place of NaN values, will deal with numeric cols classed as 'object' later\ndf = df.fillna('unspecified')","47bc84a3":"# Removing The 500+ Rows Containing A Typo 5 Digit Sequences (All Other Rows Are Dates)\ndf['mask'] = df['datetime'].str.contains('^\\d{5}') \ndf = df[df['mask'] != True]","b043f09e":"df.shape","b0d6fcc3":"# Extracting Date and Time from datetime Col into sighting_date(Datetime64) and time(int) Columns\ndf['sighting_date'] = df['datetime'].str.findall(r'\\d{1,2}.\\d{1,2}.\\d{1,4}').str.join('')\ndf['time'] = df['datetime'].str.findall(r'\\s\\d{1,2}.\\d{1,2}').str.join('').str.replace(':', '')\ndf['sighting_date'] = pd.to_datetime(df['sighting_date'])\ndf['time'] = df['time'].astype(int)\ndf.shape","b08b2e73":"# Removing Latitiude Cell Typo 'q' and Converting Lat Long Cols To Floats \ndf['lat'] = df['lat'].str.replace('q','')\ndf[['lat', 'long']] = df[['lat', 'long']].astype(float)","a50ac7a7":"# Removing Row Typos And The String Fillna From Earlier\ndf['seconds'] = df['seconds'].str.replace('8\/16\/2002 01:30', '0.0')\ndf['seconds'] = df['seconds'].str.replace('`','')\ndf['seconds'] = df['seconds'].str.replace('unspecified', '0.0')","0367e52c":"# Updating All 0 Second Sightings to Median(120 secs) \ndf['seconds'] = df['seconds'].str.replace('0.0', '120')\ndf['seconds'] = df['seconds'].str.replace(r'^0', '120', regex=True)\n\n# Converting Seconds To Float\ndf['seconds'] = df['seconds'].astype(float)\n\n# Updating Minutes Column With Seconds Coversion (Easier To Interperet\/acceptable foprmat for modelling)\ndf['minutes'] = df['seconds'] \/ 60","974c7d3a":"# Check the variance of the minutes column\nprint('Minutes Variance =',df[ 'minutes'].var())\n\n# Log normalize the minutes column to reduce Variance\ndf[\"minutes_log\"] = np.log(df['minutes'])\n\n# Print out the variance of just the seconds_log column\nprint('Log Transformed Minutes Variance =',df['minutes_log'].var())","dbc4c9ec":"# Maybe We Can Drop The Outliers\nplt.hist(df['minutes_log'], bins=10)\nplt.xticks(np.arange(-5, 17.5, 2.5))\nplt.show()\nprint('Max Minutes Log:', df['minutes_log'].max() )","ee94f5a2":"# Extracting month and month name from the month column\ndf[\"month\"] = df[\"sighting_date\"].apply(lambda x: x.month)\ndf['month_name'] = df['month'].apply(lambda x: calendar.month_name[x])\n\n# Extracting the year from the date column\ndf[\"year\"] = df[\"sighting_date\"].apply(lambda x: x.year)\n\n# All three columns\nprint(df[['sighting_date', 'month', 'month_name', 'year']].head(3))\nprint(df.shape)","2c0f75ab":"# Missing A Lot Of Countries Filled With 'Unspecified'\ndf['country'].value_counts()","012fbf61":"# Less Missing For States \ndf['state'].value_counts()","e4535fa0":"# Setting Region Boundries With Google Maps Coordinates. \n# These Are Broad Continent\/Regions and will be added to the 'Countries' Column\n\nconditions = [ \n#aus\n(df['lat'] > -48.0) & (df['lat'] <-7.0) & (df['long'] > -171.0) & (df['lat'] < 104.0)\n    & (df['lat'] != -0) & (df['long'] != -0), \n#africa                                                                                        \n(df['lat'] > -37.0) & (df['lat'] <37.0) & (df['long'] > -26.0) & (df['lat'] < 48.0)\n    & (df['lat'] != -0) & (df['long'] != -0),\n#mideast\n(df['lat'] >10.0) & (df['lat'] <47.0) & (df['long'] > 35.0) & (df['lat'] < 90.0)\n    & (df['lat'] != 0) & (df['long'] != 0),\n#latam\n(df['lat'] >-59.0) & (df['lat'] <30.0) & (df['long'] < -29.0) & (df['lat'] >-126.0)\n    & (df['lat'] != -0) & (df['long'] != -0)]\n\n\n                                                                                       \n\ncategories = ['aus-nz', 'africa', 'mideast', 'latam']\n\ndf['added_countries'] = np.select(conditions, categories)\ndf['added_countries'] = df['added_countries'].str.replace('0','')\ndf['country'] = df['country'] + df['added_countries']\ndf['added_countries'].value_counts()","fbd98ee8":"df = df.drop('added_countries', axis=1)","cd45a408":"# Mapping new regions and combining with existing regions\ndf['country'] = df['country'].map({'unspecifiedafrica':'africa', 'unspecifiedlatam ': 'latam',\n                                  'unspecifiedaus-nz':'aus-nz', 'auaus-nz': 'aus-nz', \"au\":'aus-nz',\n                                  'unspecifiedmideast':'mideast', 'uslatam': 'latam', 'gb':'uk-irl',\n                                  'ca':'ca', 'de':'de', 'us':'us', 'unspecified':'unspecified'})\n\ndf['country'] = df['country'].fillna('unspecified')\ndf['country'].value_counts()","c4ce7997":"# Label Encoding The Country\/Regions\nle = LabelEncoder()\n\nx = le.fit_transform(df['country'])\n\ndf['country_enc'] = x","26842289":"df['state']","fe2c9c2e":"# Label Encoding The States\nle = LabelEncoder()\n\ny = le.fit_transform(df['state'])\n\ndf['state_enc'] = y","a0768ae0":"# Extracting Cities From A String Copied That I Copied From Google\nwith open('\/kaggle\/input\/country_list.txt') as f:\n    lines = f.readlines()\nlines = [x.strip() for x in lines]\nlines = [x.lower() for x in lines]\nlines[:2]","18804007":"# Extracting The List and Filling NaNs' with 'Unspecified'\ndf['added_region'] = df['city'].str.findall(f'({\"|\".join(lines)})')\ndf['added_region'] = df['city'].str.extract(f'({\"|\".join(lines)})')\ndf['added_region'] = df['added_region'].fillna('unspecified')\ndf['added_region'].value_counts()","8d3358de":"# Label Encoding The Added Cities\nle = LabelEncoder()\n\nx = le.fit_transform(df['added_region'])\n\ndf['city_enc'] = x","e063608c":"# Creating Time Of Day Column\ndf.loc[(df['time'] >=600) & (df['time'] <= 1200), 'sighting_time'] = 'morning'+'\\n'+'(6am-12pm)'\ndf.loc[(df['time'] >=1200) & (df['time'] <=  1600), 'sighting_time'] = 'afternoon'+'\\n'+'(12pm-4pm)'\ndf.loc[(df['time'] >=1600) & (df['time'] <=  2100), 'sighting_time'] = 'evening'+'\\n'+'(4pm-9pm)'\ndf.loc[(df['time'] >=2100) & (df['time'] <=  2400), 'sighting_time'] = 'nighttime'+'\\n'+'(9pm-12am)'\ndf.loc[(df['time'] >=0) & (df['time'] <=  600), 'sighting_time'] = 'latenight'+'\\n'+'(12am-6am)'","08e27fc3":"# Encoding sighting_time \ndef sighting(val):\n    if val == 'morning'+'\\n'+'(6am-12pm)':\n        return 1\n    elif val == 'afternoon'+'\\n'+'(12pm-4pm)':\n        return 2\n    elif val =='evening'+'\\n'+'(4pm-9pm)':\n        return 3\n    elif val== 'nighttime'+'\\n'+'(9pm-12am)':\n        return 4\n    else:\n        return 5\n\ndf['sighting_time_enc'] = df['sighting_time'].apply(sighting)","4fc61820":"# Unique shape sighting count\nprint(df['shape'].value_counts())\n# Number Of Shapes\nprint('\\n'+'Number Of Shapes =', len(df['shape'].unique()))","806226f5":"# OHE the ufo shapes\nshape_set = pd.get_dummies(df['shape'])\n\n# Concatenating OHE Values to the df \ndf = pd.concat([df, shape_set], axis=1)\ndf.shape","aa437190":"# Smoother More Legible Plotting Style\nsns.set(style=\"white\", context=\"talk\")","a923d378":"# Plotting Sighting Over Time (Since 1940)\nplt.figure(figsize=(20,10))\n\nsns.lineplot(data=df['year'].value_counts())\n\nplt.xlim(1940, 2018)\nplt.xticks(range(1940,2020, 5), rotation=0, size=14)\nplt.yticks(range(0,10000, 1000), rotation=0, size=14)\n\nplt.xlabel('Year', fontweight='semibold')\nplt.ylabel('Sighting Count', fontweight='semibold')\nplt.title('Number of Sightings Per Year', fontweight='semibold')\nplt.show()","3da5b5fa":"# Number Of Sightings By Month\nplt.figure(figsize=(20,10))\nsns.countplot(data=df.sort_values(by='month'),x='month_name')\nplt.xlabel('Month Of Sighting', fontweight='semibold')\nplt.ylabel('Sighting Count', fontweight='semibold')\nplt.title('Number Of Sightings By Month', fontweight='semibold')\nplt.show()","32a65d2d":"# Number Of Sightings By Time Of Day\nplt.figure(figsize=(20,10))\nsns.countplot(data=df,x='sighting_time', \n              order=df['sighting_time'].value_counts().index)\nplt.xlabel('Time Of Day (Sighting)', fontweight='semibold')\nplt.ylabel('Sighting Count', fontweight='semibold')\nplt.title('Number Of Sightings By Time Of Day', fontweight='semibold')\nplt.show()","1c82dc84":"# Make The Upcoming Labels Easier To Read, Some I will Set To Upper After Grouping\ndf['state'] = df['state'].apply(lambda x: x.upper())\ndf['city'] = df['city'].apply(lambda x: x.upper())\ndf['country'] = df['country'].apply(lambda x: x.upper())","821cd2af":"# Sightings By Country\/Region\nplt.figure(figsize=(20,10))\nsns.countplot(data=df, x='country')\nplt.xlabel('Worldwide Region', fontweight='semibold')\nplt.ylabel('Sighting Count', fontweight='semibold')\nplt.title('Sightings By Country\/Region', fontweight='semibold')\nplt.show()","414eb14d":"# Group By States With Over 1000 Sighting AKA Hotspots\nby_state = (df\n    .groupby('state')\n    .filter(lambda x: len(x) > 1000))","f9aaf2b1":"# Sighting By State Hotspots\nplt.figure(figsize=(20,10))\nsns.countplot(x = 'state',\n              data = by_state,\n              order = by_state['state'].value_counts().index)\n\nplt.xticks(size=16, rotation=90)\nplt.yticks(range(0,12000,1000),size=16, rotation=0)\nplt.xlabel('State', fontweight='semibold')\nplt.ylabel('Sighting Count', fontweight='semibold')\nplt.title('US Sighting Hotspots', fontweight='semibold')\nplt.show()","c96c8bfe":"# Grouping Cities With Over 200 Sighting\nby_city = (df\n    .groupby('city')\n    .filter(lambda x: len(x) > 200))","417832ee":"# Sighting By Global City Hotspot\nplt.figure(figsize=(25,10))\nsns.countplot(x = 'city',\n              data = by_city,\n              order = by_city['city'].value_counts().index)\nplt.xlabel('City', fontweight='semibold')\nplt.ylabel('Sighting Count', fontweight='semibold')\nplt.title('Global City Hotspots', fontweight='semibold')\nplt.xticks(rotation=90)\nplt.show()","bafa4591":"# Groupby Count Transforming Data To Heatmap Compatable Count Of Shapes Per Cities\nshape_by_city = (df\n    .groupby('city')\n    .filter(lambda x: len(x) > 100) \n    .groupby(['shape', 'city'])\n    .size()\n    .unstack())\n\n# Make The Shapes More Legible\nshape_by_city.index = shape_by_city.index.str.upper()","f55ace81":"# Shape By City Hotspot\nplt.figure(figsize=(30,8))\nheat = sns.heatmap(\n        shape_by_city,\n        square=True,\n        cbar_kws= {'fraction': 0.01},\n        cmap='Oranges',\n        linewidth=1\n)\nheat.set_xticklabels(heat.get_xticklabels(), fontsize=14, rotation=90,horizontalalignment='center')\nheat.set_yticklabels(heat.get_yticklabels(), fontsize=14, rotation=0)\nplt.xlabel('Global City', fontweight='semibold')\nplt.ylabel('Sighting Count', fontweight='semibold')\nplt.title('Sighting Count Per Shape', fontweight='semibold')\nplt.show()","8a6152bb":"shape_by_time = (df\n    .groupby('shape')\n    .filter(lambda x: len(x) > 1) # Filters Out European Sightings and Lower Activity US Cities\n    .groupby(['sighting_time', 'shape'])\n    .size()\n    .unstack())\nshape_by_time.index = shape_by_time.index.str.upper()\nshape_by_time.columns = shape_by_time.columns.str.upper()","423be6b2":"# Shape By Time Of Day \nplt.figure(figsize=(20,10))\nheat = sns.heatmap(\n        shape_by_time,\n        square=True,\n        cbar_kws= {'fraction': 0.01},\n        cmap='Oranges',\n        linewidth=1\n)\nheat.set_xticklabels(heat.get_xticklabels(), fontsize=14, rotation=45,horizontalalignment='right')\nheat.set_yticklabels(heat.get_yticklabels(), fontsize=14, rotation=0,horizontalalignment='right')\nplt.xlabel('Shape', fontweight='semibold')\nplt.ylabel('Time Of Day', fontweight='semibold')\nplt.title('UFO Shape By Time Of Day', fontweight='semibold')\nplt.show()","b27274db":"shape_by_year = (df\n    .groupby('shape')\n    .filter(lambda x: len(x) > 1) # Filters Out European Sightings and Lower Activity US Cities\n    .groupby(['shape', 'year'])\n    .size()\n    .unstack())\nshape_by_year.index = shape_by_year.index.str.upper()","75d8826a":"# Shape By Year \nplt.figure(figsize=(20,10))\nheat = sns.heatmap(\n        shape_by_year,\n        square=True,\n        cbar_kws= {'fraction': 0.01},\n        cmap='Oranges',\n        linewidth=1\n)\nheat.set_xticklabels(heat.get_xticklabels(), fontsize=14, rotation=45,horizontalalignment='right')\nheat.set_yticklabels(heat.get_yticklabels(), fontsize=12, rotation=0)\nplt.xlabel('Year', fontweight='semibold')\nplt.ylabel('UFO Shape', fontweight='semibold')\nplt.title('UFO Shape By Year', fontweight='semibold')\nplt.show()","dd9eb80e":"# Global Map Of UFO Sightings Using Lat Long\nplt.figure(figsize=(30,15))\nm = Basemap(projection='mill',\n           llcrnrlat = -50,\n           urcrnrlat = 90,\n           llcrnrlon = -180,\n           urcrnrlon = 180,\n           resolution = 'c')\n\nm.drawstates()\nm.drawcountries()\nm.drawcoastlines()\n\nlat, long = df['lat'].tolist(), df['long'].tolist()\n\nt = np.arange(88178)\n\nm.scatter(long, lat, marker = 'o', c=t, cmap='YlOrRd', s=1, zorder=10, latlon=True)\nm.fillcontinents(color='g', alpha =0.3)\n\nplt.title(\"Global UFO Sightings\", fontsize=26, fontweight='semibold')\n\nplt.show()","93b37caa":"df.head()","5f28086b":"df.dtypes","2cfd26ff":"# Dropping Columns That Have Converted\/Encoded Copies\nno_nan_df = df.drop(['datetime', 'seconds', 'state', 'city', 'minutes','country', 'shape','comments',\n              'date added','month_name', 'added_region','sighting_time','sighting_date', 'mask'],axis=1)","e681d949":"# Check For High Variance \nno_nan_df.var()","218e9ef0":"# Check For Remaining NaNs\ndf.isna().sum()","57e66353":"# Dropping NaN LAtitudes before Creating Dtype Masks (Will Attempt To Model Missing Lats Later)\nno_nan_df = no_nan_df.dropna()","f21737ae":"X, y = no_nan_df.drop('lat', axis=1), no_nan_df['lat']\n\n# Split the X and y sets using train_test_split, setting stratify=y\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n\nprint(X_train.shape)\nprint(X_test.shape)","9226e97c":"clf_r = xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.01, learning_rate = 1,\n                max_depth = 50, alpha = 0.1, n_estimators = 100)","d2dbdbed":"# Basic Tuned XGBClassifier (No SearchCV Performed)\nclf_r = xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 1, learning_rate = 1,\n                max_depth = 50, alpha = 0.1, n_estimators = 100)\n\n\nclf_r.fit(X_train,y_train)\n\npreds = clf_r.predict(X_test)\n\nrmse = np.sqrt(mean_squared_error(y_test, preds))\n\nprint(\"Test Set RMSE: %f\" % (rmse))","e8839a5d":"minutes_agg = df['minutes_log'].agg([min, np.median, np.mean, max, np.std])\nminutes_agg","4ebc6da1":"# Creating Numeric X, y Vairables\nX, y = no_nan_df.drop(\n    ['minutes_log', 'lat', 'long'],axis=1),no_nan_df['minutes_log']\n\n\n# Split the X and y sets using train_test_split, setting stratify=y\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n\nprint(X_train.shape)\nprint(X_test.shape)","99fc4fcd":"# Basic Tuned XGBRegressor (No SearchCV Performed)\nclf_r = xg_reg = xgb.XGBRegressor(objective ='reg:squarederror', colsample_bytree = 0.01, learning_rate = 1,\n                max_depth = 50, alpha = 0.1, n_estimators = 100)\n\n\nclf_r.fit(X_train,y_train)\n\npreds = clf_r.predict(X_test)\n\nrmse = np.sqrt(mean_squared_error(y_test, preds))\n\nprint(\"Test Set RMSE: %f\" % (rmse))","86d613ac":"# Cross Validating The Results\ndata_dmatrix = xgb.DMatrix(data=X,label=y)\n\nparams = {\"objective\":'reg:squarederror','colsample_bytree': 0.9,'learning_rate': 0.7,\n                'max_depth': 5, 'alpha': 0.1}\n\ncv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=3,\n                    num_boost_round=50,early_stopping_rounds=10,metrics=\"rmse\", as_pandas=True, seed=123)\n\ncv_results.head()","444893cf":"no_nan_df.head(2)","5e152dac":"# Creating Numeric X, y Vairables\nX, y = no_nan_df.drop(\n    ['country_enc','lat','long'],\n    axis=1),no_nan_df['country_enc']\n\n# Split the X and y sets using train_test_split, setting stratify=y\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)\n\nprint(X_train.shape)\nprint(X_test.shape)","88c9efaf":"# Tuned XGboost Classifier (Tuned With Random Search Below)\nclf_c = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.15, max_delta_step=0, max_depth=4,\n              min_child_weight=1, n_estimators=50, n_jobs=0, num_parallel_tree=2,\n              objective='multi:softprob', random_state=123, reg_alpha=0.1,\n              reg_lambda=1, scale_pos_weight=None, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=1) \n\n\nclf_c.fit(X_train, y_train)\n\n# Test and Training Set Acurracy Score\naccuracy_train = accuracy_score(y_train, clf_c.predict(X_train))\naccuracy_test = accuracy_score(y_test, clf_c.predict(X_test))\nprint(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))","5298b09e":"# Further Assessing Accuracy Across Classes\ny_pred = clf_c.predict(X_test)\nprint(classification_report(y_test, y_pred))","85189aa3":"kfold = KFold(n_splits=10)\nresults = cross_val_score(clf_c, X, y, cv=kfold)\nprint(\"Mean Accuracy: %.2f%%, Standard Deviation (%.2f%%)\" % (results.mean()*100, results.std()*100))","9722e3b3":"# # # Running A GridSearchCV To Find More Optimal Parameters\n# xgb_param_grid = {\n#     'learning_rate': np.arange(0.05, 1, 0.05),\n#     'max_depth': np.arange(5, 12, 2),\n#     'n_estimators': np.arange(0, 100, 50),\n#     'gamma': np.arange(0, 1, 0.1),\n#     'colsample_bylevel': np.arange(0, 1, 0.1)\n# }\n    \n# rndm_xgb = RandomizedSearchCV(estimator=clf_c, param_distributions=xgb_param_grid, verbose=2, \n#                              scoring='precision_weighted', cv=3, n_jobs=3)\n\n# rndm_xgb.fit(X, y)\n\n# # Computing Param metrics\n# print('Best precision_weighted: %.2f'% rndm_xgb.best_score_)\n# print('Best Params:',rndm_xgb.best_estimator_)","ad0444c4":"# Visual EDA and Modelling of UFO Sightings Dataset\nIm going to perform a visual EDA of the UFO sightings dataset and subsequently create two models one which will attempt to attempt to predict the sighting length and the second the sighting location (exclusing latitude\/longitudue\/and countries data). \n\nI will do all of the preproccessing first and as mentioned some of which will be for nicer EDA visualzations and some will be for modelling","0274fe48":"# Sighting Time of Day and Shapes","ef5e8177":"# -------------------------Final Checks For ML Modelling----------------------------","3e76f530":"# Visual EDA Of Cleaned Dataframe","ab707826":"# ----------------------------Predict Latitiude To Insert To DF--------------------------","c489880f":"# Countries and City Columns","2f865cff":"# -----------------Classifying Region Of Sighting (Region, Lat, Long)----------------","192f8e0c":"# Time and Dates Columns","a1812d73":"# Cleaning The Dataframe\n## Feature Extraction & Feature Engineering","5d008ada":"# ----------------------Regression Model Sighting Time -----------------------------"}}