{"cell_type":{"100690c7":"code","5c727e98":"code","5c613632":"code","a4cef3f1":"code","4161223f":"code","ce90d0d3":"code","4321457e":"code","06a444bf":"code","7f1e84a1":"code","b547e7f0":"code","b94e75fd":"code","81f62bd0":"code","0934240e":"code","b715c03c":"code","f8f2afa5":"code","84f6720d":"code","662ce439":"code","777f4d9d":"code","5ab24f22":"code","dfdfdaae":"code","de8b31b1":"code","3c8f2da8":"code","337d384b":"code","0d66500d":"code","78584483":"code","5023c561":"markdown","af08177e":"markdown","bc766faa":"markdown","5658558f":"markdown","12215dcd":"markdown","dee6ffa7":"markdown","66533090":"markdown"},"source":{"100690c7":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nimport os\nimport cv2\nfrom PIL import Image\nimport matplotlib.pyplot as plt","5c727e98":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","5c613632":"def get_coordinates(dir_roi, height=2048, width=2048):\n    '''\n    Read the ROI files and store the relative coordinates\n    '''\n    img_id = []\n    cx_pxl = []\n    cy_pxl = []\n    \n    for dirpath, dirnames, files in os.walk(dir_roi):\n        sample_id = os.path.split(dirpath)[1].split('.')[0]\n        for file in files:\n            img_id.append(sample_id + file[:4])\n            cx_pxl.append(int(file[10:14]))\n            cy_pxl.append(int(file[5:9]))\n    \n    df_roi = pd.DataFrame({'img_id': img_id,\n                           'cx_pxl': cx_pxl,\n                           'cy_pxl': cy_pxl},\n                           columns = ['img_id', 'cx_pxl', 'cy_pxl'])\n    \n    df_roi['cx'] = df_roi.cx_pxl \/ float(width)\n    df_roi['cy'] = df_roi.cy_pxl \/ float(height)\n    \n    return df_roi","a4cef3f1":"df_centers_original = get_coordinates(\"..\/input\/bone-lab\/roi\/\").sort_values(by='img_id').reset_index(drop=True)\n\nprint(df_centers_original.head())","4161223f":"def transform_coordinates(df_roi, shift=(359,359), crop=(13330,1330)):\n    df_roi_cp = df_roi.copy()\n    shift_x, shift_y = shift\n    height, width = crop\n    \n    # apply shift\n    df_roi_cp.cx_pxl = df_roi_cp.cx_pxl - shift_x\n    df_roi_cp.cy_pxl = df_roi_cp.cy_pxl - shift_y\n    \n    # apply the new resolution\n    df_roi_cp.cx = df_roi_cp.cx_pxl \/ float(width)\n    df_roi_cp.cy = df_roi_cp.cy_pxl \/ float(height)\n    \n    return df_roi_cp","ce90d0d3":"df_centers = transform_coordinates(df_centers_original)\n\nprint(df_centers.head())","4321457e":"def get_and_verify_roi_images(path, df_centers):\n    images = pd.Series(sorted(os.listdir(path)))\n    img_ids = images.str.split('.').str[0]\n    assert df_centers.img_id.equals(img_ids), \"Image lists do not match\"\n    return images","06a444bf":"images = get_and_verify_roi_images('..\/input\/bone-lab\/trainset\/', df_centers)","7f1e84a1":"def get_training_images(path, images, df_centers, img_size=(32, 32), train_size=4000):\n    mat_images = np.zeros((train_size, img_size[0] * img_size[1]))\n    \n    train_images = images.sample(train_size, random_state=10)\n    train_ids = train_images.str.split('.').str[0]\n    \n    for i, file in enumerate(train_images):\n        img = Image.open(path + file)\n        img = img.resize(img_size, Image.LANCZOS)\n        mat_images[i] = np.asarray(img).flatten()\n        \n    df_train = pd.DataFrame(mat_images, columns=['px1' + str(i) for i in range(img_size[0]*img_size[1])])\n    \n    df_train.insert(0, 'img_id', train_ids.values)\n    df_train = pd.merge(df_train, df_centers[['img_id', 'cx', 'cy']], on='img_id', validate=\"1:1\")\n    \n    return df_train","b547e7f0":"df_train = get_training_images('..\/input\/bone-lab\/trainset\/', images, df_centers)\n\nprint(df_train.head())\nprint(df_train.shape)","b94e75fd":"def mirror(df_imgs, flip_axis, height=32, width=32):\n    assert 'h' in flip_axis or 'v' in flip_axis, \"Must be either 'v' - vertical or 'h' - horizontal\"\n    \n    pxl_cols = [col for col in df_imgs.columns if 'px' in col]\n    num_imgs = df_imgs.shape[0]\n    img_size = height * width\n    \n    mat_images = df_imgs[pxl_cols].values\n    mat_images = mat_images.reshape((num_imgs, height, width))\n    df_imgs_flip = df_imgs.copy()\n    \n    if 'h' in flip_axis:\n        flipping = 1\n        df_imgs_flip.img_id = df_imgs.img_id + 'hf'\n        df_imgs_flip.cy = 1.0 - df_imgs.cy  \n    elif 'v' in flip_axis:\n        flipping = 2\n        df_imgs_flip.img_id = df_imgs.img_id + 'vf'\n        df_imgs_flip.cx = 1.0 - df_imgs.cx   \n        \n    mat_images_flip = np.flip(mat_images, flipping).reshape((num_imgs, img_size))\n    df_imgs_flip[pxl_cols] = mat_images_flip\n    \n    return df_imgs_flip","81f62bd0":"# horizontal flip\ndf_train_hflip = mirror(df_train, 'h')\n\n# vertical flip\ndf_train_vflip = mirror(df_train, 'v')","0934240e":"# Merge original + augmented\ndf_train = pd.concat([df_train, df_train_hflip, df_train_vflip],ignore_index=True)\n\nprint(\"New dataframe's shape: {}\".format(df_train.shape))","b715c03c":"df_centers = df_train[['img_id', 'cx', 'cy']]","f8f2afa5":"# Get the features, labels and Ids\nX = df_train.drop(columns=['img_id', 'cx', 'cy']).values.reshape((-1, 32, 32, 1))\nY = df_train[['cx', 'cy']].values\nIds = df_train.img_id.values","84f6720d":"# Normalization\nX \/= 255.","662ce439":"# Convert to Pytorch Tensors\ncuda = torch.device('cuda')\n\nX = torch.tensor(X, dtype=torch.float32).cuda()\nY = torch.tensor(Y, dtype=torch.float32).cuda()","777f4d9d":"# Switch the channel placement for pytorch\nX = X.permute(0, 3, 1, 2)","5ab24f22":"# Split training, testing data\nX_train, X_test, y_train, y_test, I_train, I_test = train_test_split(X, Y, Ids, test_size=0.3, random_state=1)","dfdfdaae":"class Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 32, 5)\n        self.maxpool = nn.MaxPool2d(2)\n        self.conv2 = nn.Conv2d(32, 64, 3)\n        self.flat = nn.Flatten()\n        self.tanh = nn.Tanh()\n        self.fc1 = nn.Linear(64 * 6 * 6, 1024)\n        self.fc2 = nn.Linear(1024, 512)\n        self.fc3 = nn.Linear(512, 256)\n        self.fc4 = nn.Linear(256, 128)\n        self.fc5 = nn.Linear(128, 64)\n        self.fc6 = nn.Linear(64, 32)\n        self.fc7 = nn.Linear(32, 2)\n        self.softmax = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.maxpool(x)\n        x = F.relu(self.conv2(x))\n        x = self.maxpool(x)\n        x = self.flat(x)\n        x = self.tanh(self.fc1(x))\n        x = self.tanh(self.fc2(x))\n        x = self.tanh(self.fc3(x))\n        x = self.tanh(self.fc4(x))\n        x = self.tanh(self.fc5(x))\n        x = self.tanh(self.fc6(x))\n        x = self.softmax(self.fc7(x))\n        \n        return x","de8b31b1":"model = Model().cuda()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)","3c8f2da8":"nr_batches = 128\nnr_epochs = 401\n\n# Split X and Y in batches\nX_split = torch.split(X_train, nr_batches)\ny_split = torch.split(y_train, nr_batches)","337d384b":"def get_accuracy(model, x, y):\n    out = model(x)\n    x = torch.argmax(out, dim=1)\n    y = torch.argmax(y, dim=1)\n    equal = len(torch.where(x == y)[0])\n    return equal \/ len(x)","0d66500d":"losses = []\ntrain_accuracies = []\ntest_accuracies = []\n\nfor epoch in range(nr_epochs):\n    \n    epoch_losses = []\n    for i, data in enumerate(X_split):\n        optimizer.zero_grad()\n        \n        out = model(data)\n        \n        y = torch.argmax(y_split[i], dim=1)\n        loss = criterion(out, y)\n        epoch_losses.append(loss.item())\n        \n        loss.backward()\n        \n        optimizer.step()\n   \n    epoch_losses = np.array(epoch_losses)\n    \n    if(epoch % 50 == 0):\n        losses.append(epoch_losses)\n        \n        train_accuracy = get_accuracy(model, X_train, y_train)\n        train_accuracies.append(train_accuracy)\n        \n        test_accuracy = get_accuracy(model, X_test, y_test)\n        test_accuracies.append(test_accuracy)\n        \n        print(\"Epoch:{0:4d}, Loss_mean:{1:1.3f}, Train_accuracy:{2:1.3f}, Test_accuracy:{3:1.3f}\"\n              .format(epoch, epoch_losses.mean(), train_accuracy, test_accuracy))","78584483":"# Pick the last element for each epoch\nplt_losses = [np.array(losses)[i][-1] for i in range(len(losses))]\n\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111)\nax.plot(plt_losses, label='Mean Losses')\nax.plot(train_accuracies, label='Train Accuracies')\nax.plot(test_accuracies, label='Test Accuracies')\nax.legend()\nplt.show()","5023c561":"# CT Scan Pytorch CNN","af08177e":"# Prepare Data","bc766faa":"# Merge augmented data and split training set","5658558f":"# Analyze Trained Data ","12215dcd":"# Training","dee6ffa7":"# Data Augmentation","66533090":"# PyTorch Model"}}