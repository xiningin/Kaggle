{"cell_type":{"d8c5cb1b":"code","281d2eca":"code","544de60e":"code","37b6b0f4":"code","73f3b2ff":"code","da24a010":"code","6db695f4":"code","9c9fce5f":"code","0c2f4fc6":"code","bb4c97d9":"code","5792f55c":"code","93ea9d2e":"code","f51f803a":"code","a362ecc6":"code","35a544a7":"code","5bdf0fe7":"code","7f93158c":"code","e61911b3":"code","457f7750":"code","79806e01":"code","52835e79":"code","be18159d":"code","69a05a89":"code","dec622c4":"code","ca487837":"code","2f8e5470":"code","823df7bc":"code","a34da09a":"code","446abc03":"code","16c58d02":"code","4b4d01ca":"code","daca7dad":"code","d21634f5":"code","ba9d0f26":"code","5b6598a8":"code","845d8bb8":"code","a4dd0e54":"code","978bd693":"code","91775461":"code","11dd6151":"code","da271aa0":"code","60ba4467":"code","63a51931":"code","7eea0db4":"code","22579f96":"code","e2aa77bf":"code","b2ce17ce":"code","4fe4271a":"code","d530960e":"code","593f65a0":"code","65793e65":"markdown","2a071904":"markdown","b34a338b":"markdown","d1da8e45":"markdown","77af5949":"markdown","b4e4cc69":"markdown","8f7fa7dd":"markdown","ea499421":"markdown","48b71f4a":"markdown","de3fc32d":"markdown","820445aa":"markdown","0e8c8f6e":"markdown","fb42fa84":"markdown","8b7f25f3":"markdown","d4c8017e":"markdown","d8de0208":"markdown","3b6c8c2c":"markdown"},"source":{"d8c5cb1b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","281d2eca":"%matplotlib inline\nsns.set_style('darkgrid')","544de60e":"# import data to dataframes\nlist_df = pd.read_csv('..\/input\/boston\/listings.csv')","37b6b0f4":"# Copy the dataframe\nlist_clean = list_df.copy()","73f3b2ff":"# First, let's drop some columns that are not useful for analysis\ncols = ['thumbnail_url', 'medium_url', 'picture_url', 'xl_picture_url', 'listing_url', 'host_url',\n       'host_thumbnail_url', 'host_picture_url', 'country', 'country_code', 'neighbourhood',\n       'smart_location', 'street', 'market', 'first_review', 'last_review', 'state', 'calendar_last_scraped',\n       'calendar_updated', 'city', 'scrape_id', 'last_scraped', 'space', 'host_neighbourhood', \n        'neighborhood_overview', 'host_listings_count', 'zipcode', 'is_location_exact', 'host_location',\n       'host_total_listings_count']\nlist_clean.drop(cols, axis=1, inplace=True)","da24a010":"# drop the colunms with more than half of the missing value\ncols = list_clean.columns[list_clean.isnull().sum()\/list_clean.shape[0] > 0.5]\nlist_clean.drop(cols, axis=1, inplace=True)","6db695f4":"# Next, let's fix some datatype errors, extract numbers and change to int type\ncols = ['host_response_rate', 'host_acceptance_rate', 'price', 'cleaning_fee', 'extra_people']\nfor col in cols:\n    list_clean[col] = list_clean[col].str.extract(r'(\\d+)')\n    list_clean[col] = list_clean[col].astype('float128').astype('Int64')\nlist_clean[cols].dtypes","9c9fce5f":"# Change datatype for host_since\nlist_clean['host_since'] = pd.to_datetime(list_clean.host_since)\ntemp = pd.to_datetime('12\/31\/2019')","0c2f4fc6":"list_clean['host_len'] = list_clean.host_since.apply(lambda x: pd.Timedelta(temp-x).days)\nlist_clean = list_clean.drop('host_since', axis=1)","bb4c97d9":"# extract the number of amenities \nlist_clean['n_amenities'] = list_clean['amenities'].apply(lambda x: len(x.replace('{', '').\\\n                        replace('{', '').replace('\"', '').split(',')))\nlist_clean.drop('amenities', axis=1, inplace=True)","5792f55c":"df_num = list_clean.select_dtypes(include=['int', 'float'])","93ea9d2e":"# fill na for the columns\nint_fillmean = lambda x: x.fillna(round(x.mean()))\ndf_num = df_num.apply(int_fillmean, axis=0)\ndf_num = df_num.drop(['id', 'host_id', 'latitude', 'longitude'], axis=1).astype(float)","f51f803a":"# visualizae the price\nplt.figure(figsize=(8, 6))\nsns.distplot(df_num['price'], bins=50, kde=True)\nplt.ylabel('Percentage', fontsize=12)\nplt.xlabel('Price (dollar)', fontsize=12)\nplt.title('Listed Price Distribution', fontsize=14);","a362ecc6":"# we can see that some listed price are 1 dollar, which is not intuitive\ndf_num = df_num[df_num['price'] != 1]\n# we will also exlude some outliers of the price > 1.5 IQR\ndf_num = df_num[df_num['price'] < 423]","35a544a7":"# visualize the correlation matrix\ncorr = df_num.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(18, 16))\n    ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True, annot=True, fmt='.2f', cmap='coolwarm')","5bdf0fe7":"# visualizae the price\nplt.figure(figsize=(8, 6))\nsns.heatmap(df_num.groupby(['bathrooms', 'bedrooms']).mean()['price'].reset_index()\\\n                .pivot('bathrooms', 'bedrooms', 'price').sort_index(ascending=False),\n            cmap=\"coolwarm\", fmt='.0f', annot=True, linewidths=0.5);","7f93158c":"# Next let's look at the categorical values\ndf_cat = list_clean.select_dtypes(include=['object'])\n# we will drop few description lines for future analysis\ndf_cat = df_cat.drop(['name', 'summary', 'description', 'experiences_offered', 'host_about', \n                      'host_verifications', 'host_name'], axis=1)","e61911b3":"# find the not na rows with True\ndf_cat['with_inter'] = df_cat.interaction.notna().astype('object')\ndf_cat['with_access']= df_cat.access.notna().astype('object')\ndf_cat['with_rules'] = df_cat.house_rules.notna().astype('object')\ndf_cat['with_transit'] = df_cat.transit.notna().astype('object')\ndf_cat = df_cat.drop(['transit', 'access', 'interaction', 'house_rules'], axis=1)","457f7750":"df_cat = pd.concat([df_cat, list_clean.price], axis=1)\ndf_cat = df_cat[df_cat['price'] != 1]\ndf_cat = df_cat[df_cat['price'] < 423]","79806e01":"# get dummy values for categorical features\ndf_cat_dummies = pd.get_dummies(df_cat.iloc[:,:-1], dummy_na=False)","52835e79":"# concatenate the model df\ndf_mod = pd.concat([df_num, df_cat_dummies], axis=1)","be18159d":"# We will first extract the dataframe for word analysis\ndf_word = list_clean.loc[:,['description', 'price']]\ndf_word = df_word[df_word.price!=1] \ndf_word = df_word[df_word.price<423] \ndf_word.sample(5)","69a05a89":"# segment price into two groups \nbin_edges = [0, 84, 145, 205, 417]\nbin_name = ['25%', '50%', '75%', '100%']\ndf_word['price_bin'] = pd.cut(df_word['price'], bins=bin_edges, labels=bin_name)","dec622c4":"# get the post content for each price group\np_words = {};\nfor i in range(len(bin_name)):\n    p_words[i] = ''\n    df_i = df_word[df_word['price_bin'] == bin_name[i]]['description'].astype(str)\n    for j in range(len(df_i)):\n        words = df_i.iloc[j].split(' ')\n        for word in words:\n            p_words[i] += word+' '","ca487837":"# Get the most popular 20 words for each price group \nfrom collections import Counter\nimport string\ncounter = {}\noccur = {}\nstop_words = ['a', 'the', 'and', 'is', 'of', 'with', '', 'in', 'i', 'you', 'for', 'on', 'at', 'this', 'there', \n              'that', 'to', 'from', 'have', 'has', 'we', 'your', 'my', 'are', 'be', 'or', 'will', 'our', 'it',\n             'located', 'all', 'as']\nfor i in range(len(bin_name)):\n    words = p_words[i].lower().translate(str.maketrans('', '', string.punctuation)).split(' ')\n    counter[i] = Counter([word for word in words if word not in stop_words])\n    occur[i] = counter[i].most_common(20)\n    \ndf1 = pd.DataFrame.from_dict(occur[0]).rename(columns={0:'word', 1:'count'})\ndf2 = pd.DataFrame.from_dict(occur[1]).rename(columns={0:'word', 1:'count'})\ndf3 = pd.DataFrame.from_dict(occur[2]).rename(columns={0:'word', 1:'count'})\ndf4 = pd.DataFrame.from_dict(occur[3]).rename(columns={0:'word', 1:'count'})","2f8e5470":"pd.DataFrame.from_dict(occur).rename(columns={0:'0%-25%', 1:'25%-50%', 2:'50%-75%', 3:'75%-100%'})","823df7bc":"# visualization \nplt.figure(figsize=(20, 14))\nplt.subplot(2, 2, 1)\nax1 = sns.barplot(data=df1, x='word', y='count', palette = 'RdBu')\nplt.xticks(rotation=60)\nplt.xlabel('')\nplt.title('0%-25%', fontsize=12);\n\nplt.subplot(2, 2, 2)\nax2 = sns.barplot(data=df2, x='word', y='count', palette = 'RdBu')\nplt.xticks(rotation=60)\nplt.xlabel('')\nplt.title('25%-50%', fontsize=12)\n\nplt.subplot(2, 2, 3)\nax2 = sns.barplot(data=df3, x='word', y='count', palette = 'RdBu')\nplt.xticks(rotation=60)\nplt.title('50%-75%', fontsize=12)\n\nplt.subplot(2, 2, 4)\nax2 = sns.barplot(data=df4, x='word', y='count', palette = 'RdBu')\nplt.xticks(rotation=60)\nplt.title('75%-100%', fontsize=12)\n\nplt.suptitle('Popular Words in Different Price Group');","a34da09a":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nstopwords = set(STOPWORDS)\nstopwords.update(stop_words)","446abc03":"# generate word cloud for high price group\nwordcloud = WordCloud(background_color='white', max_words=1000, contour_width=3,contour_color='firebrick', \n                      stopwords = stopwords)\n\nwordcloud.generate(p_words[3])\nplt.figure(figsize=(12,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","16c58d02":"# generate word cloud for low price group\nwordcloud = WordCloud(background_color='white', max_words=1000, contour_width=3,contour_color='firebrick', \n                      stopwords = stopwords)\n\nwordcloud.generate(p_words[0])\nplt.figure(figsize=(12,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","4b4d01ca":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import PolynomialFeatures","daca7dad":"# get the feature and label\nX = df_mod.drop('price', axis=1)\ny = df_mod['price']","d21634f5":"# There are two many features, let's select top 30 features with REF model\nn = np.arange(5, 100, 5)\nmodel = LinearRegression()\nR2 = []\n#Initializing RFE model\nfor num in n:\n    rfe = RFE(model, num)\n    #Transforming data using RFE\n    X_rfe = rfe.fit_transform(X,y)  \n    #Fitting the data to model\n    model.fit(X_rfe,y)\n    # generate new feature matrix\n    X_new = X.iloc[:,rfe.support_]\n    # split training and testing model\n    X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.2, random_state=42)\n    # fit the data\n    lm = LinearRegression()\n    lm.fit(X_train, y_train)\n    pred = lm.predict(X_test)\n    R2.append(metrics.r2_score(y_test, pred))","ba9d0f26":"ind = np.array(R2).argmax()\nf_n = n[ind]\nrfe = RFE(model, f_n)\nX_rfe = rfe.fit_transform(X,y) \nmodel.fit(X_rfe,y)\nX_new = X.iloc[:,rfe.support_]\nprint('{} features are selected'.format(f_n))","5b6598a8":"# split training and testing model\nX_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.3, random_state=42)","845d8bb8":"# fit the data\nlm = LinearRegression()\nlm.fit(X_train, y_train)\npred = lm.predict(X_test)","a4dd0e54":"# evaluate\nmse = metrics.mean_squared_error(y_test, pred)\nr_square = metrics.r2_score(y_test, pred)\n\nprint('Mean absolute error is {}'.format(mse))\nprint('R^2 is {}'.format(r_square))","978bd693":"# create a function to visualize the distribution of the prediction and the test\ndef DistributionPlot(RedFunction, BlueFunction, RedName, BlueName, Title):\n    plt.figure(figsize=(10, 8))\n\n    ax1 = sns.distplot(RedFunction, hist=False, color=\"r\", label=RedName)\n    ax2 = sns.distplot(BlueFunction, hist=False, color=\"b\", label=BlueName, ax=ax1)\n    plt.title(Title)\n    plt.xlabel('Price (dollars)')\n    plt.show()\n    plt.close()","91775461":"# Visualize the results\nDistributionPlot(y_test, pred, 'Actual Values (Train)', 'Predicted Values (Train)', \n                 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution')","11dd6151":"Rcross = cross_val_score(lm, X_new, y, cv=10)\nprint(\"The mean of the folds are\", Rcross.mean(), \"and the standard deviation is\" , Rcross.std())","da271aa0":"# screen ridge model\nRR_square = []\nRR_train = []\nalpha = [0.0001, 0.001, 0.1, 1, 10, 20]\nfor a in alpha:\n    RigeModel = Ridge(alpha=a) \n    RigeModel.fit(X_train, y_train)\n    RR_square.append(RigeModel.score(X_test, y_test))\n    RR_train.append(RigeModel.score(X_train, y_train))","60ba4467":"# visualize\nplt.figure(figsize=(8, 5))\nplt.plot(alpha,RR_square, label='validation data')\nplt.plot(alpha,RR_train, 'r', label='training Data')\nplt.xlabel('alpha')\nplt.ylabel('R^2')\nplt.ylim(0.5, 0.8)\nplt.legend();","63a51931":"# Choose a ridge\nRigeModel = Ridge(alpha=5) \nRigeModel.fit(X_train, y_train)\nrr_pred = RigeModel.predict(X_test)\nprint('r2 score is: {}'.format(RigeModel.score(X_test, y_test)))","7eea0db4":"# Visualize the results\nDistributionPlot(y_test, rr_pred, 'Actual Values (Train)', 'Predicted Values (Train)', \n                 'Distribution  Plot of  Predicted Value Using Training Data vs Training Data Distribution')","22579f96":"coef_df = pd.DataFrame(data=list(lm.coef_), index=X_new.columns).reset_index().rename(columns={'index': 'Var', 0: 'Coef'})","e2aa77bf":"# Predicted prive VS. actural price \nplt.figure(figsize=(10,5))\nsns.regplot(x=y_test, y=rr_pred, color=sns.color_palette()[0])\nplt.xlim(0, 430)\nplt.title('Predict Model', fontsize=14)\nplt.xlabel('Test Data', fontsize=12)\nplt.ylabel('Predictions', fontsize=12);","b2ce17ce":"# get the neighboorhood\nneighborhoods = coef_df[coef_df.Var.str.contains('neighbourhood')].sort_values(by='Coef', ascending=False)\nneighborhoods['Var'] = neighborhoods['Var'].apply(lambda x: x.split('_')[2])","4fe4271a":"# Visualize neighborhood effect on price\nplt.figure(figsize=(12, 8))\nsns.barplot(data=neighborhoods, x='Coef', y='Var', palette='Blues_d')\nplt.xlabel('Relative Price (dollars)', fontsize=12)\nplt.ylabel('Neighborhood', fontsize=12)\nplt.title(\"Neighborhoods' Effects on Predicted Price\", fontsize=14);","d530960e":"# get the neighboorhood\nproperty_type = coef_df[coef_df.Var.str.contains('property_type')].sort_values(by='Coef', ascending=False)\nproperty_type['Var'] = property_type['Var'].apply(lambda x: x.split('_')[2])","593f65a0":"# Visualize property_type effect on price\nplt.figure(figsize=(12, 8))\nsns.barplot(data=property_type, x='Coef', y='Var', palette='Blues_d')\nplt.xlabel('Relative Price (dollars)', fontsize=12)\nplt.ylabel('Property Type', fontsize=12)\nplt.title(\"Property Type's Effects on Predicted Price\", fontsize=14);","65793e65":"## Word Cloud","2a071904":"# 4. Model Build","b34a338b":"Airbnb is an online marketplace for arranging or offering lodging, primarily homestays, or tourism experiences. Since 2008, guests and hosts have used Airbnb to expand on traveling possibilities. It renders an unique and personalized way of experiencing the world and socializing with new people. This dataset describes the listing activity and metrics in Boston, MA for 2019. This data file includes all needed information to find out more about hosts, geographical availability, necessary metrics to make predictions and draw conclusions. What can we learn about AirBnB rentals if we try to model their price? \nThe analysis could provide some insights on:\n1. What features affect the price? By how much?\n2. The popular description words in different price groups. \n\nI hope you like this kernel and your UPVOTES would be appreciate :)","d1da8e45":"# 3. Popular Word and Word Cloud","77af5949":"We can see that ridge only slightly improve the model...","b4e4cc69":"# 1. Introduction","8f7fa7dd":"## Feature selection and Linear model","ea499421":"## Cross-Validation","48b71f4a":"# 2. EDA","de3fc32d":"1. According to our model, every additional bedroom will cost extra 28.5 dollars, while each additional bathroom will cost extra 2.3 dollars. Each additional guests will cost 6 dollars more.\n\n2. Real bed costs more than alternative options.\n\n3. Neighbourhood areas strongly bias the price, which will be plot next.\n\n4. Superhosts' listings are 6 dollars more expensive on average. \n\n5. Property types strongly affect the price. For example: a camp\/RV will cost 150 dollars less on average but a boat will cost 39.7 dollars more on average.\n\n6. Entire room on average will cost 76.6 more than a shared-room.\n\n7. In general, the more strict the cacellation policy is, the more expensive the listing will be. Maybe listings have strict cancellation policy are tend to be more expensive and popular. ","820445aa":"Looking at the word cloud, it is interesting that the more expensive listings contains more information about the amenities, such as dryer and washer and fully-equipped, and they also emphasize the location (south end, back bay) more frequentyly.","0e8c8f6e":"![](https:\/\/image.cnbcfm.com\/api\/v1\/image\/105611441-1544204687419bostonmassachusetts.jpg?v=1544204740&w=1910)","fb42fa84":"The interaction, house_rules, access can substantial a listing post. Maybe provide the information would attract more people and pontentially inrease the value of the listing. Since there are a lot of missing values in these columns, I would categorize them with t (with info) and f (without info). ","8b7f25f3":"# 5. Conclusion","d4c8017e":"## Ridge Regression","d8de0208":"There a a lot of overlapping in different price groups, one thing that is interesting is that the higher priced listings contains 'restuarants' more frequently, while lower price listings emphasize transportation using words such as 'bus' and 'line' (metro). ","3b6c8c2c":"## Popular word"}}