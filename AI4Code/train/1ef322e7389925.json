{"cell_type":{"97c4fd61":"code","651a0ebb":"code","493b8289":"code","880f7766":"code","09f706c5":"code","568a1a58":"code","0c300f6e":"code","d43402d2":"code","cb7cf154":"code","70c7d468":"code","bcbbf400":"code","7944bf14":"code","ba7cc64b":"code","6a98d7f6":"code","d6fda2c1":"code","55ada6b3":"code","95b2e42a":"code","1e83c6c7":"code","97cda35e":"code","cf64f5a0":"code","c6b750e5":"code","d4d2b29e":"code","81418ec1":"code","4880f423":"code","be028869":"code","4521fbdd":"markdown","c501af12":"markdown","78399986":"markdown","8d55dd84":"markdown","b4aae50e":"markdown","5e6831be":"markdown","6900f31c":"markdown","660fc717":"markdown","ed25b2c4":"markdown","f146b706":"markdown","44acb80e":"markdown","2e8b3bbb":"markdown","d084132c":"markdown"},"source":{"97c4fd61":"!pip install fastai2\n!pip install fast_tabnet","651a0ebb":"from fastai2.basics import *\nfrom fastai2.tabular.all import *\nfrom fast_tabnet.core import *\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","493b8289":"PATH = '\/kaggle\/input\/covid19-global-forecasting-week-4\/'\ntrain_df = pd.read_csv(PATH + 'train.csv', parse_dates=['Date'])\ntest_df = pd.read_csv(PATH + 'test.csv', parse_dates=['Date'])\n\nadd_datepart(train_df, 'Date', drop=False)\nadd_datepart(test_df, 'Date', drop=False)","880f7766":"PATH1 = '\/kaggle\/input\/covid19-country-data-wk3-release\/'\n\nmeta_convert_fun = lambda x: np.float32(x) if x not in ['N.A.', '#N\/A', '#NULL!'] else np.nan\n\nmeta_df = pd.read_csv(PATH1 + 'Data Join - RELEASE.csv', thousands=\",\",\n                     converters={\n                         ' TFR ': meta_convert_fun,\n                         'Personality_uai': meta_convert_fun,\n                     }).rename(columns=lambda x: x.strip())\n\nPATH2 = '\/kaggle\/input\/countryinfo\/'\ncountryinfo = pd.read_csv(PATH2 + 'covid19countryinfo.csv', thousands=\",\", parse_dates=['quarantine', 'schools', 'publicplace', 'gathering', 'nonessential'])\ntestinfo = pd.read_csv(PATH2 + 'covid19tests.csv', thousands=\",\")\n\ncountryinfo.rename(columns={'region': 'Province_State', 'country': 'Country_Region'}, inplace=True)\ntestinfo.rename(columns={'region': 'Province_State', 'country': 'Country_Region'}, inplace=True)\ntestinfo = testinfo.drop(['alpha3code', 'alpha2code', 'date'], axis=1)\n\nPATH3 = '\/kaggle\/input\/covid19-forecasting-metadata\/'\ncontinent_meta = pd.read_csv(PATH3 + 'region_metadata.csv')\ncontinent_meta = continent_meta[['Country_Region' ,'Province_State', 'continent']]\n\nrecoveries_meta = pd.read_csv(PATH3 + 'region_date_metadata.csv', parse_dates=['Date'])\n\ndef fill_unknown_state(df):\n    df.fillna({'Province_State': 'Unknown'}, inplace=True)\n    \nfor d in [train_df, test_df, meta_df, countryinfo, testinfo, continent_meta, recoveries_meta]:\n    fill_unknown_state(d)","09f706c5":"idx_group = ['Country_Region', 'Province_State']\n\ndef day_reached_cases(df, name, no_cases=1):\n    \"\"\"For each country\/province get first day of year with at least given number of cases.\"\"\"\n    gb = df[df['ConfirmedCases'] >= no_cases].groupby(idx_group)\n    return gb.Dayofyear.first().reset_index().rename(columns={'Dayofyear': name})\n\ndef area_fatality_rate(df):\n    \"\"\"Get average fatality rate for last known entry, for each country\/province.\"\"\"\n    gb = df[df['Fatalities'] >= 22].groupby(idx_group)\n    res_df = (gb.Fatalities.last() \/ gb.ConfirmedCases.last()).reset_index()\n    return res_df.rename(columns={0 : 'FatalityRate'})","568a1a58":"def joined_data(df):\n    res = df.copy()\n    \n    fatality = area_fatality_rate(train_df)\n    first_nonzero = day_reached_cases(train_df, 'FirstCaseDay', 1)\n    first_fifty = day_reached_cases(train_df, 'First50CasesDay', 50)\n    \n    # Add external features\n    res = pd.merge(res, continent_meta, how='left')\n    res = pd.merge(res, recoveries_meta, how='left')\n    res = pd.merge(res, meta_df, how='left')\n    res = pd.merge(res, countryinfo, how='left')\n    res = pd.merge(res, testinfo, how='left', left_on=idx_group, right_on=idx_group)\n    \n    # Add calculated features\n    res = pd.merge(res, fatality, how='left')\n    res = pd.merge(res, first_nonzero, how='left')\n    res = pd.merge(res, first_fifty, how='left')\n    return res\n\ntrain_df = joined_data(train_df)\ntest_df = joined_data(test_df)","0c300f6e":"# It turns out any country in train dataset has at least one case.\ntrain_df.FirstCaseDay.isna().sum()","d43402d2":"def with_new_features(df, train=True):\n    res = df.copy()\n    add_datepart(res, 'quarantine', prefix='qua')\n    add_datepart(res, 'schools', prefix='sql')\n    \n    res['DaysSinceFirst'] = res['Dayofyear'] - res['FirstCaseDay']\n    res['DaysSince50'] = res['Dayofyear'] - res['First50CasesDay']\n    res['DaysQua'] = res['Dayofyear'] - res['quaDayofyear']\n    res['DaysSql'] = res['Dayofyear'] - res['sqlDayofyear']\n    \n    # Since we will take log of dependent variable, we won't make it nonzero.\n    if train:\n        res['ConfirmedCases'] += 1\n    return res\n    \ntrain_df = with_new_features(train_df)\ntest_df = with_new_features(test_df, train=False)","cb7cf154":"train_df.shape","70c7d468":"# Categorical variables - only basic identifiers, some features like continent will be worth adding.\ncat_vars = ['Country_Region', 'Province_State',\n            'continent'\n#             'publicplace', 'gathering', 'nonessential'\n           ]\n\n# Continuous variables - just ones directly connected with time.\ncont_vars = ['DaysSinceFirst', 'DaysSince50', 'Dayofyear',\n            'DaysQua', 'DaysSql',\n            'latitude', 'longitude',\n            'TRUE POPULATION', 'TFR', 'Personality_uai',\n            'testper1m', 'positiveper1m',\n            'casediv1m', 'deathdiv1m', \n            'FatalityRate',\n#             'density', 'urbanpop', 'medianage', 'hospibed','healthperpop', 'fertility',\n            'smokers', 'lung', \n#             'continent_gdp_pc', 'continent_happiness', 'continent_Life_expectancy','GDP_region', \n#            'abs_latitude', 'temperature', 'humidity',\n            ]\n\n# We will predict only confirmed cases. \n# For fatalities, one could train another model but we won't do it - multiplying by average fatality in each area is enough for a sample submission.\ndep_var = ['ConfirmedCases', 'Fatalities']\n\ndf = train_df[cont_vars + cat_vars + dep_var +['Date']].copy().sort_values('Date')","bcbbf400":"print(test_df.Date.min())\nMAX_TRAIN_IDX = df[df['Date'] < test_df.Date.min()].shape[0]","7944bf14":"df1 = df.copy()\ndf1['ConfirmedCases'] = np.log(df1['ConfirmedCases'])\ndf1['Fatalities'] = np.log(df1['Fatalities'] + 1)","ba7cc64b":"path = '\/kaggle\/working\/'\n\nprocs=[FillMissing, Categorify, Normalize]\n\nsplits = list(range(MAX_TRAIN_IDX)), (list(range(MAX_TRAIN_IDX, len(df))))\n\n%time to = TabularPandas(df1, procs, cat_vars.copy(), cont_vars.copy(), dep_var, y_block=TransformBlock(), splits=splits)","6a98d7f6":"dls = to.dataloaders(bs=512, path=path)\ndls.show_batch()","d6fda2c1":"to_tst = to.new(test_df)\nto_tst.process()\nto_tst.all_cols.head()","55ada6b3":"emb_szs = get_emb_sz(to); print(emb_szs)","95b2e42a":"dls.c = 2 # Number of outputs we expect from our network - in this case 2.\nmodel = TabNetModel(emb_szs, len(to.cont_names), dls.c, n_d=16, n_a=8, n_steps=3)\nopt_func = partial(Adam, wd=0.01, eps=1e-5)\nlearn = Learner(dls, model, MSELossFlat(), opt_func=opt_func, lr=3e-2, metrics=[rmse])","1e83c6c7":"learn.lr_find()","97cda35e":"cb = SaveModelCallback()\nlearn.fit_one_cycle(200, cbs=cb)","cf64f5a0":"# Load the best model so far (based on validation score)\nlearn.load('model')","c6b750e5":"tst_dl = dls.valid.new(to_tst)\ntst_dl.show_batch()","d4d2b29e":"learn.metrics = []\ntst_preds,_ = learn.get_preds(dl=tst_dl)","81418ec1":"res1 = np.expm1(tst_preds)\nres2 = list(map(lambda x: x[0], res1.numpy()))\nres3 = list(map(lambda x: x[1], res1.numpy()))\nsubmit = pd.DataFrame({'ConfirmedCases': res2, 'Fatalities': res3})\nsubmit.index = test_df.ForecastId","4880f423":"submit.to_csv('submission.csv')","be028869":"import seaborn as sns\n\nmin_date = test_df.Date.min()\nmax_date = train_df.Date.max()\n\nf, axes = plt.subplots(10, 1, figsize=(16, 60))\n\ndef plot_preds(country, ax):\n    targets = train_df[(train_df['Country_Region'] == country) & (train_df['Date'] >= min_date)].ConfirmedCases\n    subset = test_df[(test_df['Country_Region'] == country) & (test_df['Date'] <= max_date)]\n    \n    idx = subset.index\n    dates = subset.Date\n    predicted = submit.iloc[idx].ConfirmedCases\n    \n    targets.index = dates\n    predicted.index = dates\n    \n    combined = pd.DataFrame({'real' : targets, 'pred': predicted})\n    \n    sns.lineplot(data=combined, ax=axes[ax]).set_title(country)\n\nplot_preds('Italy', 0)\nplot_preds('Spain', 1)\nplot_preds('Germany', 2)\nplot_preds('Poland', 3)\nplot_preds('Czechia', 4)\nplot_preds('Russia', 5)\nplot_preds('Iran', 6)\nplot_preds('Sweden', 7)\nplot_preds('Japan', 8)\nplot_preds('Belgium', 9)","4521fbdd":"# Model\n\nThe model provided here is a simple baseline from [fast_tabnet](https:\/\/github.com\/mgrankin\/fast_tabnet) documentation, without any fine tuning.\n\n[Here](https:\/\/github.com\/mgrankin\/fast_tabnet#how-to-use) you can find **how to tune hyperparameters**.\n\nIn contrary to fast.ai tabular learner, this network accepts **multivariable output** like here - (Cases, Fatalities)\nis what we are going to predict.","c501af12":"# Add external features to input dataframes\nFor each country, we want to extract the first day when it reached at least 1 and 50 cases.\n\nAverage fatality rate will be calculated from last data available, simply taking deaths \/ cases.","78399986":"# Conclusion\n\nAs neural networks are thought not to perform well on tabular data, which is true especially for some trivial architectures like MLP used in fast.ai tabular v1.\n\nThere is quite a lot to explore in this TabNet, in terms of hyperparameters and feature selection, so I would expect a much better performance with tuning.\n\n# [#masks4all](https:\/\/masks4all.co\/why-we-need-mandatory-mask-laws-masks4all\/)","8d55dd84":"# Add temporal features\n\nSome basic features like number of days since the first case in each country\/province with analogous feature for 50 days may be worth adding,\nincluding containment measures such as closed schools and quarantine.","b4aae50e":"# [#masks4all](https:\/\/masks4all.co\/why-we-need-mandatory-mask-laws-masks4all\/)\n\n# Introduction\n\nThe goal of this notebook is to provide some basic [fast.ai](https:\/\/www.fast.ai\/), [TabNet (paper here)](https:\/\/arxiv.org\/abs\/1908.07442) model for COVID-19 dataset.\n\nThe solution utilizes mostly [fast.ai](https:\/\/www.fast.ai\/) library with [fast_tabnet](https:\/\/github.com\/mgrankin\/fast_tabnet) and stuff included in [this course](https:\/\/course.fast.ai\/)\n\nAlthough it is not the best approach here (doesn't have an RNN part, for instance), it requires reasonably small amount of code and obviously no feature engineering.","5e6831be":"# Example predictions on our validation set.\n\nDisplay some interesting countries to see if our model performs any good.","6900f31c":"# Preparing submission","660fc717":"# Processed test set","ed25b2c4":"# Feature selection\nIn fast.ai we can easily select categorical and continuous variables for training.\n\nI decided not to choose any external data in baseline model. Adding numerical values from country data provided in this notebook doesn't seem to improve the validation score much.","f146b706":"# Metadata (continent, population, tests per million etc.)","44acb80e":"# Preparing training set\n\nIn fast.ai v2, we have to manually take log of dependent variables. Preprocessing setup is vanilla fast.ai stuff,\nwe utilize our training index to make a unleaky train\/valid split. \n\nAn important thing is also picking the right batch size in our DataLoader, I've chosen value 512 which is pretty big but doesn't seem to affect the training negatively.","2e8b3bbb":"# Avoid leakage - take only non-overlapping values for training\n\nFor now, the only available data to validate our model is in training set. \n\nAs our test set starts on **02.04.2020**, we should take only rows before that date for training to avoid leakage.","d084132c":"# Load input data"}}