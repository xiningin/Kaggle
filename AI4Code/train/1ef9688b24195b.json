{"cell_type":{"3c270886":"code","55b53aa7":"code","451f54a0":"code","896bf5f7":"code","cea17fae":"code","d2751b8a":"code","fc07a29b":"code","ee20256e":"code","5f46d696":"code","4d757cc5":"code","a0b02462":"code","ec9b9112":"code","eca23830":"code","7465b495":"code","75d4a694":"code","2df58eaa":"code","65204399":"code","f629e987":"code","5a8ef16c":"code","d35e55f7":"code","e8c72c5e":"code","2d0c0718":"code","5d639dcc":"code","a31a9c38":"code","d3fbd159":"code","ce1e1a9f":"code","d3050763":"code","3f2ec77c":"markdown","ac5f89a2":"markdown","bd26dde1":"markdown","5d7d87b3":"markdown","ecd54abc":"markdown","4c70e7de":"markdown","f0f1b771":"markdown","84faf14f":"markdown","34bd2a53":"markdown","5ba94fdb":"markdown","3536e9e1":"markdown","e2ee5d90":"markdown","a75bfe24":"markdown","0ddd7a69":"markdown","2832753b":"markdown","01930084":"markdown","3c76146f":"markdown","1ceed25d":"markdown","109e8766":"markdown","3ab8d928":"markdown","c39018ab":"markdown","bdec9a0d":"markdown","a68d01d0":"markdown","d200eb01":"markdown","5021818c":"markdown","9832befb":"markdown","36b3902b":"markdown","4808b2c2":"markdown","5da1856c":"markdown","ec0ec879":"markdown","26fca740":"markdown","967eb7e6":"markdown","1091c521":"markdown","81e35c25":"markdown","baa5091e":"markdown","be679098":"markdown","1f7b1368":"markdown"},"source":{"3c270886":"import os\n\nimport numpy as np\nimport pandas as pd\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import image\nimport seaborn as sns\n\nfrom sklearn.metrics import confusion_matrix\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.optimizers import Adam","55b53aa7":"os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\" # Block Numa warning","451f54a0":"input_folder = \"\/kaggle\/input\/\"\ndataset_folder = input_folder + \"eurosat-dataset\/\" + \"EuroSAT\/\"","896bf5f7":"column_names =  [\"id\", \"path\", \"class_id\", \"class_name\"]\n\ntraining_df = pd.read_csv(dataset_folder + \"train.csv\", names=column_names)\nvalidation_df = pd.read_csv(dataset_folder + \"validation.csv\", names=column_names)\ntest_df = pd.read_csv(dataset_folder + \"test.csv\", names=column_names)","cea17fae":"training_df_size = len(training_df)\nvalidation_df_size = len(validation_df)\ntest_df_size = len(test_df)\n\ntotal_dataset_size = training_df_size + validation_df_size + test_df_size\n\nprint(f\"Training set size: {training_df_size} ({round(training_df_size \/ total_dataset_size * 100)}%)\")\nprint(f\"Validation set size: {validation_df_size} ({round(validation_df_size \/ total_dataset_size * 100)}%)\")\nprint(f\"Test set size: {test_df_size} ({round(test_df_size \/ total_dataset_size * 100)}%)\")","d2751b8a":"rescaling_factor = 1. \/ 255","fc07a29b":"training_generator = ImageDataGenerator(rescale=rescaling_factor)\ntest_generator = ImageDataGenerator(rescale=rescaling_factor)","ee20256e":"image_width = 64\nimage_height = 64","5f46d696":"batch_size = 128","4d757cc5":"training_set = training_generator.flow_from_dataframe(\n    dataframe=training_df,\n    x_col=\"path\",\n    y_col=\"class_name\",\n    directory=dataset_folder,\n    target_size=(image_height, image_width),\n    batch_size=batch_size,\n    class_mode=\"categorical\",\n    shuffle=\"true\",\n)\n\nvalidation_set = test_generator.flow_from_dataframe(\n    dataframe=validation_df,\n    x_col=\"path\",\n    y_col=\"class_name\",\n    directory=dataset_folder,\n    target_size=(image_height, image_width),\n    batch_size=batch_size,\n    class_mode=\"categorical\",\n    shuffle=\"true\",\n)\n\ntest_set = test_generator.flow_from_dataframe(\n    dataframe=test_df,\n    x_col=\"path\",\n    y_col=\"class_name\",\n    directory=dataset_folder,\n    target_size=(image_height, image_width),\n    batch_size=batch_size,\n    class_mode=\"categorical\",\n    shuffle=False\n)","a0b02462":"class_name_list = np.array(list(training_set.class_indices.keys()))\nlabels, count = np.unique(training_set.classes, return_counts=True)\n\nprint(pd.Series(count, index=class_name_list))","ec9b9112":"figure_height = 3\nfigure_width = 3\nfigure_size = (14, 14)","eca23830":"images, class_ids = next(training_set)\nplt.figure(figsize=figure_size)\nfor index in range(figure_height * figure_width):\n    plt.subplot(figure_height, figure_width, index + 1)\n    plt.imshow(images[index])\n    plt.title(class_name_list[class_ids[index].astype(bool)][0])\n    plt.axis(\"off\")","7465b495":"base_model = VGG16(include_top=False, input_shape=(64, 64, 3))\nbase_model.summary()","75d4a694":"class_count = len(class_name_list)\nmodel = Sequential()\nmodel.add(base_model)\nmodel.add(Flatten())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(units=512, activation=\"relu\", kernel_initializer=\"he_normal\"))\nmodel.add(Dense(class_count, activation=\"softmax\"))\n\nmodel.summary()","2df58eaa":"model_path = \"\/kaggle\/working\/models\/eurosat_rgb_model.h5\"\n\ncheckpoint = ModelCheckpoint(filepath=model_path, monitor=\"val_loss\", save_best_only=True)\nreduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5) \nearly_stopping = EarlyStopping(monitor=\"val_loss\", patience=15, restore_best_weights=True, verbose=True)\n\ncallback_list = [checkpoint, early_stopping, reduce_lr]","65204399":"model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=1e-4), metrics=[\"accuracy\"])","f629e987":"history = model.fit(\n    training_set,\n    validation_data=validation_set,\n    callbacks=callback_list,\n    epochs=100,\n    verbose=1,\n)","5a8ef16c":"plt.figure(figsize=(18, 5))\n\n# Loss\nplt.subplot(1, 2, 1)\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"Loss\")\nplt.ylabel(\"loss\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\n\n# Accuracy\nplt.subplot(1, 2, 2)\nplt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])\nplt.title(\"Accuracy\")\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"epoch\")\nplt.legend([\"train\", \"test\"], loc=\"upper left\")\n\nplt.show()","d35e55f7":"loss, accuracy = model.evaluate(test_set, verbose=1)","e8c72c5e":"test_set.reset()","2d0c0718":"predicted_probabilities = model.predict(test_set, steps=test_set.n, verbose=1)\npredicted_class_ids = np.argmax(predicted_probabilities, axis=1)","5d639dcc":"confusion_matrix_array = confusion_matrix(y_true=test_set.labels, y_pred=predicted_class_ids)\nconfusion_matrix_df = pd.DataFrame(\n    data=confusion_matrix_array, \n    index=class_name_list, \n    columns=class_name_list\n)\n\nplt.figure(figsize=(10, 7))\nsns.set(font_scale=1)\nsns.heatmap(confusion_matrix_df, annot=True, cmap=\"Blues\", annot_kws={\"size\": 9}, fmt=\"g\")\nplt.ylabel(\"Label\")\nplt.xlabel(\"Prediction\")\nplt.show()","a31a9c38":"results_df = pd.DataFrame(\n    {\n        \"label\": test_set.labels, \n        \"prediction\": predicted_class_ids, \n        \"path\": test_set.filepaths\n    }\n)","d3fbd159":"mistakes = results_df[\"label\"] != results_df[\"prediction\"]\nmistakes_df = results_df[mistakes]\n\nprint(f\"{len(mistakes_df)} wrong predictions out of {len(results_df)}\")","ce1e1a9f":"mistaken_predictions = (class_name_list[pred] for pred in mistakes_df[\"prediction\"])\nmistaken_labels = (class_name_list[label] for label in mistakes_df[\"label\"])\nmistaken_images = (image.imread(path) for path in mistakes_df[\"path\"])","d3050763":"figure_size = (14, 14)\nplt.figure(figsize=figure_size)\nfor index in range(figure_height * figure_width):\n    \n    label = next(mistaken_predictions)\n    prediction = next(mistaken_labels)\n    picture = next(mistaken_images)\n    \n    plt.subplot(figure_height, figure_width, index + 1)\n    plt.imshow(picture)\n    plt.title(f\"Label: {label} \\nPrediction: {prediction}\")\n    plt.axis(\"off\")","3f2ec77c":"The [EuroSat Dataset](https:\/\/github.com\/phelber\/EuroSAT) contains images extracted from [Sentinel-2](https:\/\/sentinel.esa.int\/web\/sentinel\/missions\/sentinel-2) scenes that have been resampled to 10m resolution for all bands. The images are provided in two formats: *jpg* with three channels (RGB) and *tif* with the full set of Sentinel-2 bands. In this notebook I will work with the RGB dataset, so the first step will be to create the path to the data folder.","ac5f89a2":"After some experiments, I have chosen the [VGG16](https:\/\/keras.io\/api\/applications\/vgg\/#vgg16-function) architecture for this notebook. This architecture was originally trained on the [ImageNet](https:\/\/image-net.org\/) dataset. Keras provides a [function to preprocess](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/applications\/vgg16\/preprocess_input) our dataset as the ImageNet in case we want to take advantage of the pretrained weights, but the EuroSat dataset is large enough to train the whole architecture without fear of overfitting it, so I am not going to use that function. However, I am going to initialize the weights as they come pre-trained. This has shown to work better than initializing them randomly or following a more orthodox transfer learning approach using the preprocessing function and gradually adapting the model to the dataset. ","bd26dde1":"## Results dataframe\n\nLet's have a look into the images and see if we can get a better understanding of the model mistakes. We can do that creating a dataframe with the results and retrieving those we are interested in.","5d7d87b3":"## Compile model\n\nFinally, I will compile the model using a [categorical crossentropy](https:\/\/keras.io\/api\/losses\/probabilistic_losses\/#categoricalcrossentropy-class) loss function. This is because I configured the generators to provide the labels as one hot encoded (*class_mode* parameter). This loss function will be optimized by the [Adam](https:\/\/keras.io\/api\/optimizers\/adam\/) algorithm.","ecd54abc":"## Set paths","4c70e7de":"Since data augmentation will not be used, we could actually use the same generator for all the sets, but I prefer to work with two generators (in theory the first one for the training set and the second one for the validation and test sets) since otherwise it would be easy to forget this detail if a different strategy is chosen.","f0f1b771":"## Plot scores","84faf14f":"The result is even better and gets close to the one achieved by the [dataset autors (0.9857)](https:\/\/github.com\/phelber\/EuroSAT) using a [ResNet-50](https:\/\/keras.io\/api\/applications\/resnet\/#resnet50-function).","34bd2a53":"## Load VGG16 architecture","5ba94fdb":"It would be interesting to know the size of each set:","3536e9e1":"We could consider it a more or less balanced dataset: there are a good number of examples of each class and the differences between them are small. Only *pasture* has a little less, but still enough. \n\nIt would be also a good idea to take a look at the images the model will be trained on. Let's define a 3x3 grid:","e2ee5d90":"# Conclusions\n\n- Standard Data augmentation techniques did not add significant value to the models.\n- The VGG16 architecture performs really well on this dataset.\n- The result achieved is similar to the [one achieved by the authors](https:\/\/www.researchgate.net\/publication\/319463676_EuroSAT_A_Novel_Dataset_and_Deep_Learning_Benchmark_for_Land_Use_and_Land_Cover_Classification) using a bigger architecture.\n- The value of the non-visible light bands is minor in these data.\n","a75bfe24":"## Training\n\nLet's see how far it goes.","0ddd7a69":"The following cell extracts the images from the generator and can be run as many times as desired, producing a new set of random images on each run. ","2832753b":"The good thing about satellite images is that, as long as we use the same sensor, the images are all taken from the same distance, which reduces the range of possibilities the model has to take into account. Between the image of a cat photographed from far away and one photographed very close up there is a huge difference. However, two images of crop fields taken using the sentinel-2 will always have a similar size.\n\nIt is also noticeable that images taken from above do not have a clear reference of what is up and what is down. We can find all kinds of orientations in the training set without a clear trend. \n\nOn the other hand, the images [have been taken from a very small area of the world](https:\/\/www.researchgate.net\/publication\/319463676_EuroSAT_A_Novel_Dataset_and_Deep_Learning_Benchmark_for_Land_Use_and_Land_Cover_Classification) (Europe). All this added to the fact that there is a good number of examples of each class is what makes the use of traditional data augmentation techniques unnecessary in my opinion. However, the resulting model will only be generalizable to similar conditions. This might be addressed using more specific remote sensing data augmentation techniques.","01930084":"## Set dataframes","3c76146f":"## Configure generators","1ceed25d":"In general herbaceous vegetation seems to be a bit unclear class to our eyes, its details are difficult to appreciate with a spatial resolution of 10m. Probably this class might be better identified using the infrared bands of the Sentinel-2. \n\nBut that is material for a another notebook.","109e8766":"# Keras basic toolbox for image recognition\n\n[Roberto Nogueras Zondag](https:\/\/www.linkedin.com\/in\/roberto-nogueras\/) December 2021\n\n<div style=\"text-align:center\">\n\t<figure class=\"image\">\n\t  <img src=\"https:\/\/www.heise.de\/imgs\/18\/2\/5\/0\/3\/0\/6\/8\/transformation-31d49ea5fedf3443.jpeg\" alt=\"{{ include.description }}\" style=\"width: 500px\">\n\t  <figcaption> B. de Smit and H.W. Lenstra Jr: Artful Mathematics. The\nHeritage of M. C. Escher <\/figcaption>\n\t<\/figure>\n<\/div>\n\n## Introduction\n\nThe goal of this notebook is to develop a basic data pipeline that can be easily applied to new image recognition projects. Some concepts developed here:\n\n- Use of generators for data loading using *flow_from_dataframe*.\n- Triple splitting of the dataset (training, validation and test sets) for model training and evaluation.\n- Usage of predefined CNN architectures.\n- Evaluation of the model performance.\n\n## Imports","3ab8d928":"Once this is done, we can classify the test set examples using an argmax function, which is used to translate the outputs of the softmax function into labels.","c39018ab":"It is to be expected that the model will have some difficulty in distinguishing annual crops from permanent crops. This is difficult enough for a person with no knowledge in this area. However, it has even more problems to distinguish herbaceous vegetation from forest and permanent crops.","bdec9a0d":"## Callbacks\n\nI will also define some callbacks: [Checkpoint](https:\/\/keras.io\/api\/callbacks\/model_checkpoint\/) will save the weights of the best model, [ReduceLROnPleateau](https:\/\/keras.io\/api\/callbacks\/reduce_lr_on_plateau\/) will accelerate the transit of the gradient through the plains of the cost function and [EarlyStopping](https:\/\/keras.io\/api\/callbacks\/early_stopping\/) will end the training if the loss function does not improve in 15 epochs.","a68d01d0":"## Predict test set\n\nIt is necessary to reset the generator before using the predict method. ","d200eb01":"## Evaluate model\n\nLet's see now how the model generalizes over unseen data.","5021818c":"## Plot wrong predictions","9832befb":"## Add custom layers\n\nNext I will define the final layers of the architecture. I will use a [dropout layer](https:\/\/keras.io\/api\/layers\/regularization_layers\/dropout\/) to prevent overfitting of the convolutional base, a [He normal](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/initializers\/HeNormal) initializer for the weights of the [dense layer](https:\/\/keras.io\/api\/layers\/core_layers\/dense\/) and a [Softmax function](https:\/\/keras.io\/api\/layers\/activation_layers\/softmax\/) to produce the multiclass output. ","36b3902b":"Then we can use generators to visualize the images in a similar way as we did with the training set generator. ","4808b2c2":"## Take a look at the data\n\nNow that the generators have been set up, it is a good moment to have a look on the data. Let's start by the balance between classes.","5da1856c":"The size of the input data is 64 x 64 pixels.","ec0ec879":"## Confusion matrix\n\nHaving the predictions, we can carry out a more detailed analysis of the model errors. For this purpose, it never hurts to have a confusion matrix at hand.","26fca740":"The dataset provides 3 dataframes linked to each of the sets that will be used to fit and evaluate the model: training, validation and test. Each of them contains the path and the label of each of the examples that conform the dataset. Let's load them into the memory.","967eb7e6":"Not bad at all. Let's see the metrics.","1091c521":"Once you have the dataframe it is easy to get rid of the successful results, which we are not going to analyze here (although it is not superfluous and can provide important information).","81e35c25":"We can see that the model has an overfit of about 2% from the point of view of the accuracy. It would be interesting to try [regularizers](https:\/\/keras.io\/api\/layers\/regularizers\/) and see if we can reduce the distance between the sets. I leave that to you, feel free to try and improve those curves.","baa5091e":"In order to link each generator to the data, I will use the [flow_from_dataframe](https:\/\/vijayabhaskar96.medium.com\/tutorial-on-keras-flow-from-dataframe-1fd4493d237c) method. ","be679098":"A batch size of 128 has proven to work quite well.","1f7b1368":"Although the images are not very large, the amount of them makes it reasonable to consider using a Tensorflow [Image Data Generator](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/preprocessing\/image\/ImageDataGenerator) in order not to overload the memory. However, due to the size of the dataset and the experiments performed, I have come to the conclusion that using data augmentation is of little use, so I am going to dispense with that technique and simply rescale the pixels to the range 0, 1. The data has been previously normalized to the range 0, 255, so we can rescale it dividing by the maximum."}}