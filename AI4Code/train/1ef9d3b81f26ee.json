{"cell_type":{"a16e05e4":"code","bd4c2044":"code","34d40db9":"code","dbc5b79b":"code","0707ecef":"code","300ad7cb":"code","7c4acf0b":"code","f6060a21":"code","8846ecaf":"code","4e7726a2":"code","90289bd4":"code","519dadd5":"code","7b369e76":"code","3857bfde":"code","ac8af732":"code","f85ce58d":"code","9d1b8749":"code","11616928":"code","e76b9552":"code","6ca4b42f":"code","76d19c69":"code","a85454db":"code","2c8377cd":"code","a8c6cc8f":"code","3bd1ccf1":"code","91d7d7f1":"code","0a3447fe":"code","c944532c":"code","99d396b1":"code","08d0be49":"code","b5af0831":"markdown","ee6c1ae0":"markdown","c0f62198":"markdown","d79e7f8e":"markdown"},"source":{"a16e05e4":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline","bd4c2044":"from fastai.vision import *\nfrom fastai.callbacks.hooks import *","34d40db9":"path = untar_data(URLs.CAMVID)\npath.ls()","dbc5b79b":"path_lbl = path\/'labels'\npath_img = path\/'images'","0707ecef":"fnames = get_image_files(path_img)\nfnames[:3]","300ad7cb":"lbl_names = get_image_files(path_lbl)\nlbl_names[:3]","7c4acf0b":"img_f = fnames[0]\nimg = open_image(img_f)\nimg.show(figsize=(5,5))","f6060a21":"get_y_fn = lambda x: path_lbl\/f'{x.stem}_P{x.suffix}'","8846ecaf":"mask = open_mask(get_y_fn(img_f))\nmask.show(figsize=(5,5), alpha=1)","4e7726a2":"src_size = np.array(mask.shape[1:])\nsrc_size,mask.data","90289bd4":"codes = np.loadtxt(path\/'codes.txt', dtype=str); codes","519dadd5":"size = src_size\/\/2\nbs=8","7b369e76":"src = (SegmentationItemList.from_folder(path_img)\n       .split_by_fname_file('..\/valid.txt')\n       .label_from_func(get_y_fn, classes=codes))","3857bfde":"data = (src.transform(get_transforms(), size=size, tfm_y=True)\n        .databunch(bs=bs, num_workers=0)\n        .normalize(imagenet_stats))","ac8af732":"data.show_batch(2, figsize=(10,7))","f85ce58d":"data.show_batch(2, figsize=(10,7), ds_type=DatasetType.Valid)","9d1b8749":"name2id = {v:k for k,v in enumerate(codes)}\nvoid_code = name2id['Void']\n\ndef acc_camvid(input, target):\n    target = target.squeeze(1)\n    mask = target != void_code\n    return (input.argmax(dim=1)[mask]==target[mask]).float().mean()","11616928":"metrics=acc_camvid\n# metrics=accuracy","e76b9552":"wd=1e-2","6ca4b42f":"learn = unet_learner(data, models.resnet34, metrics=metrics, wd=wd)","76d19c69":"lr_find(learn)\nlearn.recorder.plot()","a85454db":"lr=3e-3","2c8377cd":"learn.fit_one_cycle(10, slice(lr), pct_start=0.9)","a8c6cc8f":"learn.save('stage-1')","3bd1ccf1":"learn.load('stage-1');","91d7d7f1":"learn.show_results(rows=3, figsize=(8,9))","0a3447fe":"learn.unfreeze()","c944532c":"lrs = slice(lr\/400,lr\/4)","99d396b1":"learn.fit_one_cycle(12, lrs, pct_start=0.8)","08d0be49":"learn.save('stage-2');","b5af0831":"## Model","ee6c1ae0":"## Data","c0f62198":"## Datasets","d79e7f8e":"## Image segmentation with CamVid"}}