{"cell_type":{"379a8697":"code","044db0f7":"code","e246256a":"code","41b7e15d":"code","e42994f9":"code","ab8aff6e":"code","6ab201e2":"code","a1458d5c":"code","64809973":"code","daf1d1c4":"code","4a736f81":"markdown","6ec6e27a":"markdown","8f57b225":"markdown","2ead8bc2":"markdown","2cf71cfc":"markdown","7561de49":"markdown","f6548b2a":"markdown","194a289a":"markdown"},"source":{"379a8697":"#Importing the useful methods from the RNN library keras (sublibrary of tensorflow)\nfrom keras.layers import SimpleRNN, Embedding, Dense, LSTM, GRU\nfrom keras.models import Sequential\n\n#Importing th basic math,plotting and data manipulation libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns; sns.set()","044db0f7":"#Reading the data from the csv file and taking a look at it\ndata = pd.read_csv(\"..\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv\")\ndata.head(5)","e246256a":"#Generating the training\/test data for the model and assigning numerical labels 0(nonspam), 1(spam).\n\nemails = []\nlabels = []\nfor i, label in enumerate(data['Category']):\n    emails.append(data['Message'][i])\n    if label == 'ham':\n        labels.append(0)\n    else:\n        labels.append(1)\n\nemails = np.asarray(emails)\nlabels = np.asarray(labels)\n\n\nprint(\"number of emails :\" , len(emails))\nprint(\"number of labels: \", len(labels))","41b7e15d":"#from keras.layers import SimpleRNN, Embedding, Dense, LSTM\n#from keras.models import Sequential\n\n#Import libraries for preprocessing text\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# number of words used as features\nmax_features = 10000\n# cut off the words after seeing 500 words in each document(email)\nmaxlen = 500\n\n\n# we will use 80% of data as training, 20% as validation data\ntraining_samples = int(len(emails) * .8)\ntest_samples = int(len(emails) - training_samples)\n\nprint(\"The number of training {0}, validation {1} \".format(training_samples, test_samples))\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(emails)\nsequences = tokenizer.texts_to_sequences(emails)\n\n\nword_index = tokenizer.word_index\nprint(\"Found {0} unique words: \".format(len(word_index)))\n\nfirst_email = []\nfor i in sequences[0]:\n    first_email.append([word for word, index in word_index.items() if index == i][0]);\nprint(first_email)\n\ndata = pad_sequences(sequences, maxlen=maxlen)\n\nprint(\"data shape: \", data.shape)\n\nnp.random.seed(23)\n# shuffle data\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\n\n\nemails_train = data[:training_samples]\ny_train = labels[:training_samples]\nemails_test = data[training_samples:]\ny_test = labels[training_samples:]","e42994f9":"#Define the RNN model\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory_rnn = model.fit(emails_train, y_train, epochs=10, batch_size=60, validation_split=0.2)","ab8aff6e":"acc = history_rnn.history['acc']\nval_acc = history_rnn.history['val_acc']\nloss = history_rnn.history['loss']\nval_loss = history_rnn.history['val_loss']\nepochs = range(len(acc))\nplt.plot(epochs, acc, '-', color='orange', label='training acc')\nplt.plot(epochs, val_acc, '-', color='blue', label='validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, '-', color='orange', label='training loss')\nplt.plot(epochs, val_loss,  '-', color='blue', label='validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","6ab201e2":"pred = model.predict_classes(emails_test)\nacc = model.evaluate(emails_test, y_test)\nproba_rnn = model.predict_proba(emails_test)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Test loss is {0:.2f} accuracy is {1:.2f}  \".format(acc[0],acc[1]))\nprint(confusion_matrix(pred, y_test))","a1458d5c":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\nhistory_ltsm = model.fit(emails_train, y_train, epochs=10, batch_size=30, validation_split=0.2)","64809973":"acc = history_ltsm.history['acc']\nval_acc = history_ltsm.history['val_acc']\nloss = history_ltsm.history['loss']\nval_loss = history_ltsm.history['val_loss']\nepochs = range(len(acc))\nplt.plot(epochs, acc, '-', color='orange', label='training acc')\nplt.plot(epochs, val_acc, '-', color='blue', label='validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(epochs, loss, '-', color='orange', label='training loss')\nplt.plot(epochs, val_loss,  '-', color='blue', label='validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","daf1d1c4":"pred = model.predict_classes(emails_test)\nacc = model.evaluate(emails_test, y_test)\nproba_ltsm = model.predict_proba(emails_test)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Test loss is {0:.2f} accuracy is {1:.2f}  \".format(acc[0],acc[1]))\nprint(confusion_matrix(pred, y_test))","4a736f81":"# <a name='practical-applications'><\/a>2 Practical Applications\n\n<img src=\"https:\/\/miro.medium.com\/max\/1223\/1*XosBFfduA1cZB340SSL1hg.png\">\n\nSource: [Medium](https:\/\/medium.com\/explore-artificial-intelligence\/an-introduction-to-recurrent-neural-networks-72c97bf0912)\n\n1) Vanilla mode of processing without RNN, from fixed-sized input to fixed-sized output (e.g. image classification)\n\n2) Sequence output (e.g. image captioning takes an image and outputs a sentence of words)\n\n3) Sequence input (e.g. sentiment analysis where a given sentence is classified as expressing positive or negative sentiment)\n\n4) Sequence input and sequence output (e.g. Machine Translation: an RNN reads a sentence in English and then outputs a sentence in French)\n\n5) Synced sequence input and output (e.g. video classification where we wish to label each frame of the video)\n\nSource: [Medium](https:\/\/medium.com\/explore-artificial-intelligence\/an-introduction-to-recurrent-neural-networks-72c97bf0912)\n\n#### Other example: Time Series Data e.g. predicting stock prices\n#### Other example: Anticipating car routes in autonomous driving\n#### Other example: Generating new art [MAGENTA](https:\/\/magenta.tensorflow.org\/) Link to Video: [Making music with RNN](https:\/\/www.youtube.com\/watch?time_continue=82&v=iTXU9Z0NYoU&feature=emb_title)\n\n","6ec6e27a":"(1) [NLP from basics to using RNN and LSTM, vibhor nigam](https:\/\/towardsdatascience.com\/natural-language-processing-from-basics-to-using-rnn-and-lstm-ef6779e4ae66)\n\n(2) [Recurrent Neural Networks(RNNs), Javaid Nabi](https:\/\/towardsdatascience.com\/recurrent-neural-networks-rnns-3f06d7653a85)\n\n(3) [Sequence Modeling: Recurrentand Recursive Nets](http:\/\/www.deeplearningbook.org\/contents\/rnn.html)\n\n(4) [An Introduction to Recurrent Neural Networks, Suvro Banerjee](https:\/\/medium.com\/explore-artificial-intelligence\/an-introduction-to-recurrent-neural-networks-72c97bf0912)\n\n(4) [Illustrated Guide to Recurrent Neural Networks, Michael Nguyen](https:\/\/towardsdatascience.com\/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n\n(5) [How Recurrent Neural Networks work, Simeon Kostadinov](https:\/\/towardsdatascience.com\/learn-how-recurrent-neural-networks-work-84e975feaaf7)","8f57b225":"# <a name='references'><\/a> 4 References","2ead8bc2":"# <a name='theoretical-background'><\/a> 1 Theoretical Background ","2cf71cfc":"## What are RNNs?\n<img src=\"https:\/\/www.researchgate.net\/profile\/Weijiang_Feng\/publication\/318332317\/figure\/fig1\/AS:614309562437664@1523474221928\/The-standard-RNN-and-unfolded-RNN.png\"\/>\n\n\nSource: [ResearchGate](https:\/\/www.researchgate.net\/figure\/The-standard-RNN-and-unfolded-RNN_fig1_318332317)\n\n## Unfolded RNN\n\nRecurrent neural networks involve loops where the output of the networks is fed back as input for further processing. RNNs can use their internal state (memory) to process sequences of inputs.\nIn a recurrent neural network we store the output activations from one or more of the layers of the network. Often these are hidden later activations. Then, the next time we feed an input example to the network, we include the previously-stored outputs as additional inputs.\n\n$x^{(t)}$ - current input at time $t$ \n\n$x^{(t-1)}$ - input at time $(t-1)$\n\n$x^{(t+1)}$  - input at time $(t+1)$\n\n$o^{(t)}$  - output at time $t$ \n\n$h^{(t)}$  - hidden state at time $t$\n\n$U, V, W$ - weight matrices (hyperparameters) \n\n\n## Forward Pass\n\n<img src=\"https:\/\/miro.medium.com\/max\/419\/1*55c3opV_tqm3wUwcj0m-jg.png\"\/>\n\nSource: [Deep Learning, Goodfellow](http:\/\/www.deeplearningbook.org\/contents\/rnn.html)\n\n$a^{(t)}$ - bias vector $b$ + weight matrix $W$ * previous hidden state (at time $(t-1)$ ) + weight matrix $U$ * current input $x$ (at time $t$)\n\n$h^{(t)}$ - hyperbolic tangent activation function for hidden state at time $t$ \n\n$o^{(t)}$ - output: bias vector $c$ + weight matrix $V$ * hidden state at time $t$ \n\n$\\hat{y}^{(t)}$ - predicted probability using softmax ","7561de49":"## LTSM","f6548b2a":"# <a name='coding-example'><\/a>3 Coding Example - RNN Implementation in Python using Keras","194a289a":"# 0 Introduction to the Notebook\n\n## How to use this notebook \n\nIn order to use this notebook and run the code, follow the next steps: \n1. Sign up to kaggle.com and log in \n2. Click on the following [link]() and click Copy and Edit in the upper right corner (a new tab opens with an editable version of this notebook) \n3. Run each cell with Shift + Enter\n\n## Motivation and Disclaimer\n\nThis is a notebook introducing Recurrent Neural Networks(RNN) written by Sheraz Ali, Diana Alexandra Gaiu and Jonas Kowalick for the seminar project \"Social Media and Business Analytics\" in WS19\/20 at Potsdam University. \nThe practical example in section 3 is inspired by the notebook 'RNN for Spam Detection' which can be found under the following [link](https:\/\/www.kaggle.com\/kentata\/rnn-for-spam-detection) and makes use of the Kaggle database found at [link](https:\/\/www.kaggle.com\/team-ai\/spam-text-message-classification)\n\n## Outline \n\n1. [Theoretical Background](#theoretical-background)\n2. [Practical Applications](#practical-applications)\n3. [Coding Example - RNN Implementation in Python using Keras](#coding-example)\n4. [References](#references)"}}