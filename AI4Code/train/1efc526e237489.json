{"cell_type":{"3243b88a":"code","d5693163":"code","4f275221":"code","755086f9":"code","e12b98b4":"code","9554dcfa":"code","3a5c2400":"code","273965eb":"code","c4bc63c5":"code","f1ea4ea3":"code","1d76f373":"code","bd7b16d7":"code","3219b0d7":"code","bc1081b4":"code","8e91272e":"code","35bebfd9":"code","fa6e1d5b":"code","531f3c21":"code","8e50468b":"markdown","21c15ea5":"markdown","421d7cb9":"markdown","8a212d11":"markdown","f9b681a2":"markdown","af826001":"markdown","3a837770":"markdown","b5a657ae":"markdown","5cacdd18":"markdown","8153207b":"markdown","fe00db26":"markdown","0046768b":"markdown","b30530f6":"markdown","c3fd2fd4":"markdown","03f1055b":"markdown"},"source":{"3243b88a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\npd.set_option('max_colwidth', None)","d5693163":"path = '\/kaggle\/input\/bcg-manually-reviewed-cleaned'\nfile = f'{path}\/manually_reviewed_cleaned.csv'\ndf = pd.read_csv(file, encoding = \"ISO-8859-1\")","4f275221":"df.head(1)","755086f9":"f\"Using {df.shape[0]} entries\"","e12b98b4":"import spacy\nnlp = spacy.load('en_core_web_sm')\n\ndef get_snippets(text):\n    '''\n        Returns sentences in the text which contain more than 5 tokens and at least one verb.\n    '''\n    return [sent.text.strip() for sent in nlp(text).sents \n                 if len(sent.text.strip().split()) > 5 and any([token.pos_ == 'VERB' for token in sent])]","9554dcfa":"!pip3 install tensorflow_text>=2.0.0rc0","3a5c2400":"import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text","273965eb":"module = hub.load('https:\/\/tfhub.dev\/google\/universal-sentence-encoder-qa\/3')","c4bc63c5":"questions = [\"Which is the First Year of the BCG Policy?\",\n             \"Which is the last year of the BCG Policy?\",\n             \"Is BCG vaccination mandatory for all children?\",\n             \"What is the timing for the BCG vaccination (age)?\",\n             \"Which BCG Strain has been used?\",\n             \"Are revaccinations (boosters) recommended for BCG?\",\n             \"What is the timing of BCG revaccination?\",\n             \"Which in the body (arm) is the BCG Vaccine administered?\"]\n\nquestion_embeddings = module.signatures['question_encoder'](\n            tf.constant(questions))","f1ea4ea3":"def read_text(row):\n    code = row['alpha_2_code']\n    filename=row['filename'].replace('.txt', '')\n    filename = f'\/kaggle\/input\/hackathon\/task_1-google_search_txt_files_v2\/{code}\/{filename}.txt'\n    \n    with open(filename, 'r') as file:\n        data = file.read().replace('\\n', ' ')\n    return data\n\nquestion_names = ['first_year','last_year','is_mandatory','timing','strain','has_revaccinations','revaccination_timing','location']\n\ndef apply_USE_model(row):\n    data = read_text(row)\n    \n    snippets = get_snippets(data)\n    \n    response_embeddings = module.signatures['response_encoder'](\n        input=tf.constant(snippets),\n        context=tf.constant(snippets))\n    scores = np.inner(question_embeddings['outputs'], response_embeddings['outputs'])\n\n    result = pd.DataFrame(scores.T, columns=question_names)\n    result['sentence'] = snippets\n    result['len'] = result['sentence'].apply(len)\n    result['country'] = row['country']\n    \n    return result","1d76f373":"dfs = []\nimport tqdm\nfor _, row in tqdm.tqdm(df.iterrows()):\n    result = apply_USE_model(row)\n    dfs.append(result)","bd7b16d7":"final_eval = pd.concat(dfs, ignore_index=True)","3219b0d7":"f\"The evaluation is performed on {final_eval.shape[0]} snippets\"","bc1081b4":"final_eval.iloc[final_eval[question_names].idxmax()]","8e91272e":"for k in question_names:\n    print(k)\n    display(final_eval.sort_values(k, ascending=False)[[k, 'sentence','country','len']].head(3))","35bebfd9":"final_eval.drop('len', axis=1).plot.box(figsize=(15,5))","fa6e1d5b":"df.columns = ['alpha_2_code', 'country', 'url', 'filename', 'is_pdf','Comments',\n              'Snippet'] + question_names + ['snippet_len', 'text_len']","531f3c21":"for _, row in df.head(10).iterrows():\n    print('----' * 10)\n    print('ACTUAL:')\n    print('\\n'.join([f\"<{i}>: {v}\" for i, v in row[question_names].dropna().iteritems()]))\n    \n    cols = [i for i, v in row[question_names].dropna().iteritems()]\n    \n    result = apply_USE_model(row)\n    print(f\"Total snippets: {result.shape[0]}\")\n    \n    for k in cols:\n        display(result.sort_values(k, ascending=False)[[k, 'sentence','country','len']].head(3))\n    ","8e50468b":"To split the texts into snippets we'll use spacy and its functionality to return all sentences in a document. After that we apply additional fitering - we want only sentences which contain more than 5 tokens and at least one verb.","21c15ea5":"The evaluation of the approach will be performed in the following steps:\n* Splitting the original texts from the 35 files into snippets\n* Calculating a score between each snippet and each question using the pretrained model\n* Comparing the highest scoring snippets with the manually extracted ones","421d7cb9":"Below are displayed top 3 answers for each question.","8a212d11":"Comparison of predicted snippets with the extracted ones (only for the first 10 entries):","f9b681a2":"The dataset we'll use is based on the *task_1-google_search_manually_reviewed_metadata.csv* file provided in the original data. The following processing steps have been applied to it:\n* Cleaning of the data using code from this notebook: [Task 1: EDA + cleanup on manually reviewed](https:\/\/www.kaggle.com\/didizlatkova\/task-1-eda-cleanup-on-manually-reviewed)\n* Manually going through the snippets and extracting sub-snippets which answer one or more of the 8 questions, defined in the task.\n* Adding 8 columns for the respective questions containing the extracted sub-snippets or NaN value, if no sub-snippet answers the question.\n\nThe resulting data is exported to a new dataset called **bcg-manually-reviewed-cleaned**.","af826001":"### Split texts into snippets","3a837770":"By going through the results one can see that they are not too great. In most cases the predicted sentences do not match the actual snippets from the reviewed dataset.","b5a657ae":"## Evaluation","5cacdd18":"## Data","8153207b":"Below is listed the top 1 highest scoring answer for each of the 8 questions.","fe00db26":"#### Model Details\n* Developed by researchers at Google, 2019, v2 [1].\n* Transformer.\n* Strong performance on question answer retrieval for English.\n* Use the **question_encoder** signature to encode variable length questions in any of the aforementioned languages and the output is a 512 dimensional vector. The default signature is identical with the question_encoder signature.\n* Use the **response_encoder** signature to encode the answer and the output is a 512 dimensional vector.\n* The response_encoder signature acceptes two input fields:\n    * **text**: the answer text.\n    * **context**: usually the text around the answer text, for example it could be 2 sentneces before plus 2 sentences after, it could also be the paragraph containing the answer text. If you don't have context to include, you can duplicate of answer into this field.\n* All input text can have arbitrary length! However, it is recommended question and response inputs to be approximately one sentence in length.","0046768b":"### Calculate scores","b30530f6":"# Motivation\nThis notebook applies a Universal Sentence Encoder for Question Answering on the manually reviewed dataset.\n\nUSE-QA is a greater-than-word length text encoder for question answer retrieval. It is trained on a variety of data sources and tasks, with the goal of learning text representations that are useful out-of-the-box to retrieve an answer given a question, as well as question and answers across different languages. [Source](https:\/\/tfhub.dev\/google\/universal-sentence-encoder-qa\/3)\n","c3fd2fd4":"We can see that the model works best for answering questions about location.","03f1055b":"A distribution of the overall scores for each question is shown below."}}