{"cell_type":{"593645d1":"code","83ff29d5":"code","3a461e98":"code","b5911dbb":"code","2af44542":"code","4bce9d66":"code","86229d3e":"code","ee7fb770":"code","160c753e":"code","e89d7739":"code","ff0d1988":"code","e27eec50":"code","44ad83df":"code","4ccbacd7":"code","c30de501":"code","1d99bab3":"code","c109b183":"code","a54a460b":"code","ac393c91":"code","645d9804":"code","7ab746e6":"code","8f0ee3f4":"code","00b4b8df":"code","4a7c17b8":"code","9411ec4a":"code","a1f37afb":"code","7071cf76":"code","cef1c480":"code","e3d7f834":"code","48e10633":"code","164ba69e":"code","4733f16d":"code","3299a65a":"code","3a3585b9":"code","9b300923":"code","9dba3416":"code","33c3c308":"code","1b7cb197":"code","16cae9a7":"code","2d306df4":"code","ba38619d":"code","8cd6ed2f":"code","729fd8ea":"code","e76dd55a":"code","a15a713d":"markdown","8d38f057":"markdown","323c75c8":"markdown","de5b4a01":"markdown","5089a989":"markdown","69aee44f":"markdown","67111424":"markdown","12120b66":"markdown","41e31c25":"markdown","b0a77a71":"markdown","12415c2c":"markdown","8812fefc":"markdown","47d25fc8":"markdown","ed8fc17b":"markdown","c532203f":"markdown","3736d3c6":"markdown","6f3d9445":"markdown"},"source":{"593645d1":"import numpy as np\nimport pandas as pd\nimport cv2\nfrom tqdm import tqdm\n\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, InputLayer, Flatten, Dropout, Activation, Input, UpSampling2D\nfrom keras.models import Sequential\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom keras.activations import softmax\nfrom keras.utils import to_categorical\nfrom keras.losses import categorical_crossentropy\nfrom keras.callbacks import EarlyStopping\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import animation\n\nfrom IPython.display import HTML","83ff29d5":"train_df = pd.read_csv(\"..\/input\/fashion-mnist_train.csv\", index_col=0, header=0)","3a461e98":"train_df.head()","b5911dbb":"def aug_top(image, f=False):\n    s = image.shape[1] \/\/ 2\n    new_image = np.zeros_like(image)\n    part_of_image = image[int(s*np.random.uniform(1, 1.7)):, :]\n    new_image[0:part_of_image.shape[0], 0:part_of_image.shape[1]] = part_of_image\n    if f:\n        new_image = cv2.flip(new_image, 0)\n    return new_image\n\ndef aug_left(image, f=False):\n    s = image.shape[0] \/\/ 2\n    new_image = np.zeros_like(image)\n    part_of_image = image[:, int(s*np.random.uniform(1, 1.7)):]\n    new_image[0:part_of_image.shape[0], 0:part_of_image.shape[1]] = part_of_image\n    if f:\n        new_image = cv2.flip(new_image, 1)\n    return new_image\n\ndef aug(image):\n    flip = np.random.choice([False, True])\n    aug_type = np.random.choice([0, 1, 2])\n    if aug_type == 0:\n        aug_image = aug_top(image, flip)\n    elif aug_type == 1:\n        aug_image = aug_left(image, flip)\n    elif aug_type == 2:\n        aug_image_1 = aug_top(image, False)\n        aug_image_2 = aug_top(image, True)\n        half_height = image.shape[0] \/\/ 2\n        aug_image = np.zeros_like(image)\n        aug_image[0:half_height, :] = aug_image_1[0:half_height, :]\n        aug_image[half_height:, :] = aug_image_2[half_height:, :]\n    return aug_image","2af44542":"test_image = x_train[12]\n\nfig, axs = plt.subplots(1, 4)\naxs[0].imshow(aug(test_image))\naxs[1].imshow(aug(test_image))\naxs[2].imshow(aug(test_image))\naxs[3].imshow(aug(test_image))","4bce9d66":"x_train = train_df.values\nx_train = x_train.reshape(x_train.shape[0], 28, 28)\nx_train = x_train.astype(np.float32) \/ 255.0","86229d3e":"y_train = train_df.index.values","ee7fb770":"nb_of_images_for_aug = 1000\n\naugmentation_image_indexes = np.arange(len(x_train), dtype=int)\nnp.random.shuffle(augmentation_image_indexes)\naugmentation_image_indexes = augmentation_image_indexes[:nb_of_images_for_aug]","160c753e":"for i in tqdm(augmentation_image_indexes):\n    aug_image = aug(x_train[i])\n    x_train = np.vstack((x_train, np.expand_dims(aug_image, 0)))\n    y_train = np.hstack((y_train, [10]))","e89d7739":"nb_of_classes = len(np.unique(y_train))","ff0d1988":"x_train.shape","e27eec50":"y_train = to_categorical(y_train, nb_of_classes)","44ad83df":"model = Sequential()\n# (rows, cols, channels)\nmodel.add(InputLayer((None, None, 1)))\n\nmodel.add(Conv2D(32, (5, 5), activation=\"relu\"))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(32, (3, 3), activation=\"relu\"))\nmodel.add(MaxPooling2D((2, 2)))\n\nmodel.add(Conv2D(800, (5, 5), activation=\"relu\"))\nmodel.add(Conv2D(800, (1, 1), activation=\"relu\"))\n\nmodel.add(Conv2D(nb_of_classes, (1, 1), activation=\"softmax\"))","4ccbacd7":"model.summary()","c30de501":"model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])","1d99bab3":"hist = model.fit(np.expand_dims(x_train, 3),\n                 np.expand_dims(np.expand_dims(y_train, 1), 1),\n                 epochs=32,\n                 batch_size=32,\n                 validation_split=0.15,\n                 callbacks=[EarlyStopping(monitor=\"val_loss\", patience=5)],\n                 verbose=1)","c109b183":"# Only above this threshold we count somethig as \"detected\"\nCONFIDENCE_LEVEL = 0.95","a54a460b":"pred = model.predict(np.expand_dims(x_train[:4], 3))","ac393c91":"pred_labels = np.argmax(pred, 3).flatten()\npred_conf = np.max(pred, 3).flatten()","645d9804":"fig = plt.figure(figsize=(10, 4))\n\nfor i, (label, conf, img) in enumerate(zip(pred_labels, pred_conf, x_train[:4])):\n    ax = fig.add_subplot(1, len(pred_labels), i+1)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.set_title(\"Pred: {0} | Conf: {1:.2f}\".format(label, conf))\n    ax.imshow(img)","7ab746e6":"black_image = np.zeros_like(x_train[0])","8f0ee3f4":"tmp_1 = np.concatenate((x_train[1], black_image), axis=1)\ntmp_2 = np.concatenate((black_image, x_train[3]), axis=1)\ntest_image = np.concatenate((tmp_1, tmp_2), axis=0)","00b4b8df":"test_image_display = (test_image*255).astype(np.uint8)\ntest_image_h, test_image_w = test_image_display.shape[:2]","4a7c17b8":"plt.imshow(test_image_display);","9411ec4a":"test_image.shape","a1f37afb":"pred = model.predict(np.expand_dims(np.expand_dims(test_image, 2), 0))[0]","7071cf76":"pred.shape","cef1c480":"predictions_per_location = np.argmax(pred, 2)\npredictions_confidences_per_location = np.max(pred, 2)","e3d7f834":"fig = plt.figure()\n\nax = fig.add_subplot(1, 1, 1)\nax.set_title(\"Predictions w\/ confidence heatmap for each grid cell location\", y=1.1)\nax.set_xticks(np.arange(0, 8))\nax.set_yticks(np.arange(0, 8))\nims_conf = ax.imshow(predictions_confidences_per_location)\nfor i in range(0, predictions_confidences_per_location.shape[0]):\n    for j in range(0, predictions_confidences_per_location.shape[0]):\n        text_color = \"black\"\n        ax.text(j, i,\n                 predictions_per_location[i, j],\n                 horizontalalignment=\"center\",\n                 verticalalignment=\"center\",\n                 color=text_color)\nfig.colorbar(ims_conf, ax=ax);","48e10633":"def extract_locations(image, window_shape=(28, 28), step=4):\n    w_h, w_w = window_shape\n    i_h, i_w = image.shape[:2]\n    \n    rects = []\n    sub_images = []\n    \n    for i, y in enumerate(range(0, i_h+2-w_h, step)):\n        for j, x in enumerate(range(0, i_w+2-w_w, step)):\n            rects.append((x, y, x+w_h, y+w_w))\n            sub_image = image[y:y+w_h, x:x+w_w]\n            sub_images.append(sub_image)\n    \n    return np.array(rects), np.array(sub_images)","164ba69e":"location_rects, sub_images = extract_locations(test_image_display, step=4)","4733f16d":"len(location_rects)","3299a65a":"fig = plt.figure(figsize=(10, 4))\n\nax1 = fig.add_subplot(1, 3, 1)\nax1.set_title(\"Sliding Window\")\nims_sliding_window = ax1.imshow(test_image_display)\n\nax2 = fig.add_subplot(1, 3, 2)\nax2.set_title(\"Sub Image\")\nims_sub_image = ax2.imshow(sub_images[0])\n\nax3 = fig.add_subplot(1, 3, 3)\nax3.set_title(\"Prediction w\/ confidence\")\nax3.imshow(np.zeros((test_image_h, test_image_w), dtype=np.uint8))\ntext_prediction = ax3.text(test_image_w\/\/2, 20, \"0\", horizontalalignment='center', verticalalignment='center', size=25, color=\"white\")\ntest_confidence = ax3.text(test_image_w\/\/2, 40, \"0\", horizontalalignment='center', verticalalignment='center', size=20, color=\"white\")\n\nplt.close()\n\ndef animate(i):\n    new_sliding_widndow_image = cv2.rectangle(test_image_display.copy(), tuple(location_rects[i][:2]),\n                                              tuple(location_rects[i][2:]), 255, 1)\n    ims_sliding_window.set_data(new_sliding_widndow_image)\n    \n    ims_sub_image.set_data(sub_images[i])\n    \n    pred_conf = np.round(np.max(pred, 2).flatten()[i], 3)\n    text_prediction.set_text(np.argmax(pred, 2).flatten()[i])\n    test_confidence.set_text(pred_conf)\n    \n    if pred_conf < CONFIDENCE_LEVEL:\n        text_prediction.set_color(\"red\")\n    else:\n        text_prediction.set_color(\"green\")\n    \n    return ims_sliding_window, ims_sub_image, text_prediction, test_confidence","3a3585b9":"anim = animation.FuncAnimation(fig, animate, frames=len(location_rects), interval=300, blit=True)","9b300923":"def show_animation_kaggle(animation):\n    import tempfile\n    import io\n    import base64\n    import os\n    \n    with tempfile.NamedTemporaryFile(suffix=\"_anim.gif\") as tmp:\n        file_name = tmp.name\n    anim.save(file_name, writer='imagemagick')\n    video = io.open(file_name, 'r+b').read()\n    encoded = base64.b64encode(video)\n    data = '''<img src=\"data:image\/gif;base64,{0}\" type=\"gif\" \/>'''.format(encoded.decode('ascii'))\n    os.remove(file_name)\n    return HTML(data=data)","9dba3416":"show_animation_kaggle(anim)","33c3c308":"indexes_to_keep = np.where(predictions_confidences_per_location.flatten() > CONFIDENCE_LEVEL)\n# These are the confidences and bounding boxes we want to keep for the detection\nconfident_confidences = predictions_confidences_per_location.flatten()[indexes_to_keep]\nconfident_locations = location_rects[indexes_to_keep]\nconfident_predictions_labels = predictions_per_location.flatten()[indexes_to_keep]","1b7cb197":"def draw_predictions(image, locations, color=(255, 0, 0)):\n    tmp_image = image.copy()\n    for loc in locations:\n        p1 = tuple(loc[:2])\n        p2 = tuple(loc[2:])\n        tmp_image = cv2.rectangle(tmp_image, p1, p2, color, 1)\n    return tmp_image","16cae9a7":"detection_conf_loc_image = cv2.cvtColor(test_image_display.copy(), cv2.COLOR_GRAY2BGR)\ndetection_conf_loc_image = draw_predictions(detection_conf_loc_image, confident_locations)","2d306df4":"plt.imshow(detection_conf_loc_image);","ba38619d":"def box_area(box):\n    x1, y1, x2, y2 = box\n    return (x2 - x1) * (y2 - y1)\n\ndef box_intersection_area(a, b):\n    a_xmin, a_ymin, a_xmax, a_ymax = a\n    b_xmin, b_ymin, b_xmax, b_ymax = b\n    \n    dx = min(a_xmax, b_xmax) - max(a_xmin, b_xmin)\n    dy = min(a_ymax, b_ymax) - max(a_ymin, b_ymin)\n\n    if (dx>=0) and (dy>=0):\n        return dx*dy\n    \n    raise ValueError(\"Boxes have no intersection\")\n\ndef calulate_iou(box_1, other_boxes):\n    ious = []\n    for box_2 in other_boxes:\n        try:\n            intersection_area = box_intersection_area(box_1, box_2)\n        except ValueError:\n            # No intersection\n            ious.append(0)\n            continue\n        iou = intersection_area \/ (box_area(box_1) + box_area(box_2) - intersection_area)\n        ious.append(iou)\n    return np.array(ious)\n    \ndef nms(boxes, confidences, iou_threshold=0.5):\n    if len(boxes) < 2:\n        return boxes\n\n    sort_index = np.argsort(confidences)[::-1]\n    boxes = boxes[sort_index]\n    confidences = confidences[sort_index]\n\n    final_boxes = [boxes[0]]\n    \n    for box in boxes:\n        iou_array = calulate_iou(box, final_boxes)\n        overlap_idxs = np.where(iou_array > iou_threshold)[0]\n        if len(overlap_idxs) == 0:\n            final_boxes.append(box)\n    return final_boxes\n\ndef nms_multiclass(prediction_labels, boxes, confidences, iou_threshold=0.5):\n    unique_labels = np.unique(prediction_labels)\n    pred_dict_after_nms = {}\n    for label in unique_labels:\n        class_specific_indexes = np.where(prediction_labels == label)\n        class_boxes = boxes[class_specific_indexes]\n        class_confidences = confidences[class_specific_indexes]\n        \n        kept_boxes = nms(class_boxes, class_confidences, iou_threshold)\n        \n        pred_dict_after_nms[label] = kept_boxes\n    return pred_dict_after_nms\n        ","8cd6ed2f":"locations_after_nms_dict = nms_multiclass(confident_predictions_labels, confident_locations, confident_confidences, 0.1)","729fd8ea":"detection_after_nms_image = cv2.cvtColor(test_image_display.copy(), cv2.COLOR_GRAY2BGR)\ncolors = [(255, 0, 0),\n          (0, 255, 0),\n          (0, 0, 255),\n          (255, 255, 0),\n          (255, 0, 255),\n          (0, 255, 255),\n          (127, 0, 0),\n          (0, 127, 0),\n          (0, 0, 127),\n          (127, 0, 127)]\nfor i, (label, bboxes) in enumerate(locations_after_nms_dict.items()):\n    detection_after_nms_image = draw_predictions(detection_after_nms_image, bboxes, colors[i])","e76dd55a":"plt.imshow(detection_after_nms_image);","a15a713d":"### Test on single images","8d38f057":"## Creating the model","323c75c8":"(I've hidden a few longer functions which only serve as utils, to keep you focused on the algorithm and not those little util functions)","de5b4a01":"Now, I'll make a bigger image by combining a few together and test the network on it","5089a989":"### Apply NMS","69aee44f":"## Prepare data","67111424":"At the image below we can see all the cell's predictions and the confidences. As I said, every cell here defines a `(x, y, x+28, y+28)` location on the original image.\n\nFirst one is `(0, 0, 28, 28)` then the second is `(4, 0, 32, 28)`","12120b66":"At first I try it on single images (first 4 image), so our output should be `(4, 1, 1, 10)` as we have 4 predictions with shape `(1, 1, 10)`","41e31c25":"### Test on combined multiple images","b0a77a71":"This is just an toy  model I've put together. The only thing I cared about here is that for inputs like `(28x28) grayscale` images  the output should be `(1x1x10)`. This is how I \"converted\" the last fully connected layers to convolutional.\n\nThen the real input of the model as you can see is `(None, None, 1)`. This is because that way it accepts any dimensional images (should be greater than 28x28).\n\nYou can learn more about this conversion from [here](https:\/\/www.coursera.org\/lecture\/convolutional-neural-networks\/convolutional-implementation-of-sliding-windows-6UnU4)","12415c2c":"Using a \"classical\" sliding window is not effective, because we have to pass every sub-image to our CNN. With that approach we would compute things over and over again, as it is highly duplicative. Instead we can share the computation and get rid of the redundant calculations. This is called *convolutional sliding window*.\n\nThe idea is used in state of the art *Single Shot Detectors* like *Yolo, SSD*. These are called single shot, because it does the computation in 1 step not like region proposal methods which first proposes locations and only then classify it.\n\nThis is just a toy example of how it works, so take everything with a grain of salt. :)","8812fefc":"# Convolutional Fast Sliding Window","47d25fc8":"As you can see the problem here, is that after the confidence level thresholding and NMS, we still have a lot of detected locations. One way of to get rid of these is, to create a new class like background, and create training data with from the original, like cropped tshirts, etc... They those would be detected too and we could ignore them. Right now, our classifier is too certain about things that does not exists.","ed8fc17b":"\\begin{equation}\n    \\text{pred} = \n    \\begin{bmatrix}\n        p_{1,1} & p_{1,2} & \\dots  & p_{1,8} \\\\\n        p_{2,1} & p_{2,2} & \\dots  & p_{2, 8} \\\\\n        \\vdots & \\vdots & \\ddots & \\vdots \\\\\n        p_{8,1} & p_{8,2} & \\dots  & p_{8, 8}\n    \\end{bmatrix}\n    , \\qquad \\text{where} \\quad p_{i,j} = \\left( prob_{class_1}, prob_{class_2}, \\ldots, prob_{class_{10}} \\right)\n\\end{equation}","c532203f":"## Test the model & Playground","3736d3c6":"As we have `56x56` image,  and we slide the `28x28` size window with stride `(4, 4)` we should get an output `(8x8x10)`. This means we have a `8x8` grid from the image and for every cell's depth the probabilities for all the classes.","6f3d9445":"## Visualize Confident Predictions"}}