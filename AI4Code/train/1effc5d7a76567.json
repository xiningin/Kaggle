{"cell_type":{"a04839c5":"code","78da7996":"code","b806b0ac":"code","37ea322d":"code","e5334f5a":"code","95adc1c7":"code","056b7496":"code","c35e50c3":"code","eff98c69":"code","c398a612":"code","91a264d5":"code","5b7477c6":"code","9b5d1c50":"code","ce5d2742":"code","bf59c907":"code","31bc91ad":"code","8ad2649a":"code","4707f377":"code","158f77d4":"code","2eecf925":"code","06ad8578":"code","fa7ee418":"code","a93b16cb":"code","1f088f6d":"code","2f5691f8":"code","8ad9ea90":"code","b1e2a0fc":"code","d76694d7":"code","ce822925":"code","ca2e12a4":"code","03b61429":"code","8186fda7":"code","c7c653a3":"code","9d024544":"code","f8dca31f":"code","0f15529a":"code","88dcf3eb":"code","18fcc636":"code","8904d062":"code","af06fbde":"code","81c5aca4":"code","fa1ca800":"code","7bb7efcf":"code","db6fc14f":"code","adbdaffa":"code","3f5a30ae":"code","29c5777f":"code","b5373821":"code","10b5b86d":"code","cda536da":"code","a778c9c7":"code","fd54bc19":"code","65d7a4c9":"code","c2a55f1b":"code","2188d205":"code","d56b7e5d":"code","7b42ebe1":"code","971e3d99":"code","5acc4bfa":"code","ed5cab93":"code","ebffcfe6":"code","abd8181e":"code","1f709c45":"code","85c53182":"code","8d6faca1":"code","f006d198":"code","39bc403a":"code","65273244":"code","6647a3e8":"code","babc76dc":"code","30dd9b2d":"code","400f653e":"code","50584804":"code","dd3c7bad":"code","80612bed":"code","1971b5c1":"code","764f5810":"code","433809d2":"code","5047e78b":"markdown","c9430c7c":"markdown","2b0fe7c1":"markdown","21c62809":"markdown","6e2b9c2e":"markdown","5ec27571":"markdown","bd7676ba":"markdown","0b8a75af":"markdown","1b1fed70":"markdown","1cc8cec5":"markdown","166b0ea8":"markdown","0c0b2303":"markdown","8e1ca30d":"markdown","d7cfbfcc":"markdown","008ef32d":"markdown","09273150":"markdown","7ac9e4e3":"markdown","00895f1d":"markdown","4cd975f9":"markdown","786f2504":"markdown","aa23aa01":"markdown","748dcca3":"markdown","d73c6761":"markdown","332fc7e0":"markdown","7658dfed":"markdown","ad06fabc":"markdown","1e47516a":"markdown","0dbe44fb":"markdown","0c6b72ee":"markdown","ff2bc8ac":"markdown","068038c3":"markdown","8f6cea21":"markdown","874225c9":"markdown","a20a2835":"markdown","6d13ed98":"markdown","57cc7198":"markdown","76c2b1b0":"markdown","d139c011":"markdown","34c7f03d":"markdown","caa9bd4e":"markdown","9caf5ee6":"markdown","bd9d663e":"markdown","a1d3d0b9":"markdown","b1ca7e6f":"markdown","ead8a686":"markdown","2aebc787":"markdown","6d0695ed":"markdown","03dbf84b":"markdown","0b8355bc":"markdown","53a6747a":"markdown","296878d1":"markdown","4bf7c90e":"markdown","56cdc8a5":"markdown","0d596a20":"markdown","969a50d4":"markdown","f47ff120":"markdown","62567b7c":"markdown","3f13cb87":"markdown"},"source":{"a04839c5":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\npd.set_option('display.max_rows', 85)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom lightgbm import LGBMRegressor","78da7996":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nfull_data = [df_train, df_test]","b806b0ac":"df_train.describe()","37ea322d":"df_train.head()","e5334f5a":"sns.distplot(df_train['SalePrice'], bins = 20, hist_kws=dict(edgecolor=\"k\", linewidth=2))","95adc1c7":"sns.distplot(np.log(df_train['SalePrice']), bins = 20, hist_kws=dict(edgecolor=\"k\", linewidth=2))","056b7496":"df_train['LSalePrice'] = np.log(df_train['SalePrice'])","c35e50c3":"import statsmodels.api as sm","eff98c69":"fig = sm.qqplot(df_train['LSalePrice'],fit=True, line='45')","c398a612":"from scipy import stats\nm = stats.trim_mean(df_train['LSalePrice'], 0.1)\nprint(\"With 10% clipped on both sides, trimmed mean: {}\".format(m))\nprint(\"Sample mean: {}\".format(np.mean(df_train['LSalePrice'])))","91a264d5":"df_train.dtypes","5b7477c6":"a4_dims = (11.7, 8.27)\nfig, ax = plt.subplots(figsize=a4_dims)\n\ncorr = df_train.corr()\nax = sns.heatmap(corr, vmin=0.5, cmap='coolwarm', ax=ax)","9b5d1c50":"sns.pairplot(df_train[[\"SalePrice\",\"OverallQual\",\"OverallCond\",\"YearBuilt\",\"TotalBsmtSF\",\"BsmtUnfSF\",\"GrLivArea\",\"FullBath\",\"GarageCars\",\"GarageArea\"]])","ce5d2742":"df_train.isnull().sum()","bf59c907":"df_test.isnull().sum()","31bc91ad":"len([ 'LotFrontage', 'Alley', 'BsmtQual','BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2','Electrical','FireplaceQu','GarageType','GarageYrBlt','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature'])","8ad2649a":"len(['MSZoning', 'LotFrontage','Alley','Utilities','Exterior1st','Exterior2nd','MasVnrType','MasVnrArea', 'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath', 'BsmtHalfBath','KitchenQual','Functional','FireplaceQu', 'GarageType', 'GarageYrBlt','GarageFinish','GarageCars','GarageArea','GarageQual','GarageCond'])","4707f377":"df_train['MSSubClass'][df_train['LotFrontage'].isnull()].unique()","158f77d4":"a4_dims = (11.7, 6.27)\nfig, ax = plt.subplots(figsize=a4_dims)\nfig2, ax2 = plt.subplots(figsize=a4_dims)\nsns.boxplot(x=\"MSSubClass\",y=\"LotFrontage\",data=df_train,ax=ax).set_title(\"Training set\")\nsns.boxplot(x=\"MSSubClass\",y=\"LotFrontage\",data=df_test,ax=ax2).set_title(\"Test set\")","2eecf925":"for dataset in full_data:\n    dataset['LotFrontage'] = dataset.groupby('MSSubClass')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","06ad8578":"df_test.groupby('MSSubClass')['LotFrontage'].median()","fa7ee418":"df_test['LotFrontage'][df_test['MSSubClass']==150] = 38","a93b16cb":"missin_cats = ['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', 'MiscFeature','MasVnrType']\ndf_train.update(df_train[missin_cats].fillna(\"NA\"))\ndf_test.update(df_test[missin_cats].fillna(\"NA\"))","1f088f6d":"df_test[df_test['MSZoning'].isnull()]","2f5691f8":"def aggregate(rows,columns,df):\n    column_keys = df[columns].unique()\n    row_keys = df[rows].unique()\n\n    agg = { key : [ len(df[(df[rows]==value) & (df[columns]==key)]) for value in row_keys]\n               for key in column_keys }\n\n    aggdf = pd.DataFrame(agg,index = row_keys)\n    aggdf.index.rename(rows,inplace=True)\n\n    return aggdf\n\na4_dims = (11.7, 6.27)\nfig, ax = plt.subplots(figsize=a4_dims)\naggregate('MSSubClass','MSZoning',df_test).plot(kind='bar',stacked=True, ax=ax)","8ad9ea90":"aggregate('MSSubClass','MSZoning',df_test)","b1e2a0fc":"df_test['MSZoning'][(df_test['MSZoning'].isnull()) & (df_test['MSSubClass']==20) ] = \"RL\"","d76694d7":"df_test['MSZoning'][(df_test['MSZoning'].isnull()) & ( (df_test['MSSubClass']==30) | (df_test['MSSubClass']==70) ) ] = \"RM\"","ce822925":"for col in ['Utilities', 'Exterior1st', 'Exterior2nd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF' ,'TotalBsmtSF', 'KitchenQual', 'Functional','GarageCars', 'GarageArea', 'SaleType','BsmtFullBath','BsmtHalfBath']:\n    df_test[col].fillna(df_test[col].mode()[0],inplace=True)","ca2e12a4":"df_test.isnull().sum()","03b61429":"df_train.isnull().sum()","8186fda7":"df_train['Electrical'].fillna(df_train['Electrical'].mode()[0],inplace=True)","c7c653a3":"for data_set in full_data:\n    data_set.drop(['GarageYrBlt'], axis=1,inplace=True)","9d024544":"a4_dims = (11.7, 6.27)\nfig, ax = plt.subplots(figsize=a4_dims)\nfig2, ax2 = plt.subplots(figsize=a4_dims)\nsns.boxplot(x=\"MasVnrType\",y=\"MasVnrArea\",data=df_train,ax=ax).set_title(\"Training set\")\nsns.boxplot(x=\"MasVnrType\",y=\"MasVnrArea\",data=df_test,ax=ax2).set_title(\"Test set\")","f8dca31f":"df_train[df_train['MasVnrArea'].isnull()]","0f15529a":"for dataset in full_data:\n    dataset['MasVnrArea'] = dataset.groupby('MasVnrType')['MasVnrArea'].transform(lambda x: x.fillna(x.median()))","88dcf3eb":"df_train['MasVnrArea'][df_train['MasVnrArea'].isnull()] = 0","18fcc636":"sum(df_train.isnull().sum())","8904d062":"sum(df_test.isnull().sum())","af06fbde":"df_train['MSSubClass'].unique()","81c5aca4":"df_test['MSSubClass'].unique()","fa1ca800":"df_train['train'] = 1\ndf_test['train'] = 0","7bb7efcf":"combined_dataset = pd.concat([df_train,df_test])","db6fc14f":"categories={} # contains all the levels in those feature columns\ncategorical_feature_names =   [ 'MSSubClass' , 'MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', \n                               'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', \n                               'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', \n                               'CentralAir', 'Electrical', 'GarageType', 'PavedDrive', 'MiscFeature', 'SaleType', \n                               'SaleCondition', 'LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond',\n                               'ExterQual', 'ExterCond', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n                               'BsmtFinType2', 'HeatingQC', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', \n                               'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces' , \n                               'FireplaceQu',  'BsmtQual', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', \n                               'Fence',]\n\nfor f in categorical_feature_names:\n    # to make sure the type is indeed category\n    combined_dataset[f] = combined_dataset[f].astype('category')\n    categories[f] = combined_dataset[f].cat.categories\n\nnew_combined_dataset = pd.get_dummies(combined_dataset,columns=categorical_feature_names,drop_first=True)\n","adbdaffa":"df_newtrain = new_combined_dataset[new_combined_dataset['train'] == 1]\ndf_newtest = new_combined_dataset[new_combined_dataset['train'] == 0]\ndf_newtrain.drop([\"train\"],axis=1,inplace=True)\ndf_newtest.drop([\"train\",\"LSalePrice\",\"SalePrice\"],axis=1,inplace=True)","3f5a30ae":"df_newtrain.shape","29c5777f":"df_newtest.shape","b5373821":"new_fulldata = [df_newtrain, df_newtest]\nfor dataset in new_fulldata:\n    for col in ['YearBuilt','YearRemodAdd']:\n        dataset['HouseAge'] = 2018 - dataset['YearBuilt']\n        dataset['RemodAge'] = 2018 - dataset['YearRemodAdd']\n        dataset.drop(['YearBuilt','YearRemodAdd'],axis=1)","10b5b86d":"lambgrid = np.linspace(0.01, 100, num=1000,endpoint=True)","cda536da":"y_train = df_train['LSalePrice']","a778c9c7":"train_id = df_newtrain['Id']\ntest_id = df_newtest['Id']","fd54bc19":"X_train = df_newtrain.drop(['SalePrice','LSalePrice','Id'],axis=1)","65d7a4c9":"df_newtest.drop(['Id'],axis=1,inplace=True)","c2a55f1b":"#Ridge Regression:\nfrom sklearn.linear_model import RidgeCV\n# clf = RidgeCV(alphas=lambgrid, cv=5)\nclf = make_pipeline(RobustScaler(),RidgeCV(alphas=lambgrid, cv=5))","2188d205":"from sklearn.linear_model import Ridge\ncoefs = []\nfor a in lambgrid:\n    ridge = Ridge(alpha=a, fit_intercept=False)\n    ridge.fit(X_train, y_train)\n    coefs.append(ridge.coef_)","d56b7e5d":"a4_dims = (11.7, 8.27)\nfig, ax = plt.subplots(figsize=a4_dims)\nax.plot(lambgrid, coefs)\nax.set_xscale('log')\nax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\nplt.xlabel('alpha')\nplt.ylabel('weights')\nplt.title('Ridge coefficients as a function of the regularization')\nplt.axis('tight')\nplt.show()","7b42ebe1":"clf.fit(X_train,y_train)","971e3d99":"preds_ridge = clf.predict(df_newtest)","5acc4bfa":"from sklearn.model_selection import cross_val_score\nprint('Ridge Regression Cross Validation Score: %s' % (\n                      cross_val_score(clf, X_train, y_train,scoring='neg_mean_squared_error').mean()))","ed5cab93":"#LASSO:\nfrom sklearn.linear_model import LassoCV\n# lasso_clf = LassoCV(alphas=lambgrid, cv=5)\nlasso_clf = make_pipeline(RobustScaler(), LassoCV(alphas=lambgrid, cv=5))","ebffcfe6":"lasso_clf.fit(X_train,y_train)","abd8181e":"preds_lasso = lasso_clf.predict(df_newtest)","1f709c45":"# lasso_alpha = lasso_clf.alpha_\n# from sklearn.linear_model import Lasso\n\nprint('Lasso Cross Validation Score: %s' % (\n                      cross_val_score(lasso_clf, X_train, y_train,scoring='neg_mean_squared_error').mean()))","85c53182":"#ElasticNet:\nfrom sklearn.linear_model import ElasticNetCV\n# elastic_clf = ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1],alphas=lambgrid, cv=5)\nelastic_clf = make_pipeline(RobustScaler(), ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1],alphas=lambgrid, cv=5))","8d6faca1":"elastic_clf.fit(X_train,y_train)","f006d198":"preds_elastic = elastic_clf.predict(df_newtest)","39bc403a":"# elastic_alpha = elastic_clf.alpha_\n# from sklearn.linear_model import ElasticNet\n\nprint('ElasticNet Cross Validation Score: %s' % (\n                      cross_val_score(elastic_clf, X_train, y_train,scoring='neg_mean_squared_error').mean()))","65273244":"import xgboost as xgb\nxgtrain = xgb.DMatrix(X_train,label=y_train)\nxgb_clf = xgb.XGBRegressor(n_estimators=1000, learning_rate=.03, max_depth=3, max_features=.04, min_samples_split=4,\n                           min_samples_leaf=3, loss='huber', subsample=1.0, random_state=0)\nxgb_param = xgb_clf.get_xgb_params()","6647a3e8":"print('XGB Regression Cross Validation Score: %s' % (\n                      cross_val_score(xgb_clf, X_train, y_train,scoring='neg_mean_squared_error').mean()))","babc76dc":"xgb_clf.fit(X_train,y_train)","30dd9b2d":"preds_xgb = xgb_clf.predict(df_newtest)","400f653e":"# Fit Light Gradient Boosting:\nlightgbm = LGBMRegressor(objective='regression', \n                         num_leaves=4,\n                         learning_rate=0.01, \n                         n_estimators=5000,\n                         max_bin=200, \n                         bagging_fraction=0.75,\n                         bagging_freq=5, \n                         bagging_seed=7,\n                         feature_fraction=0.2,\n                         feature_fraction_seed=7,\n                         verbose=-1)\n\nlightgbm.fit(X_train,y_train)","50584804":"preds_lgb = lightgbm.predict(df_newtest)","dd3c7bad":"## Stacking\nfrom mlxtend.regressor import StackingCVRegressor\n\n\nridge = make_pipeline(RobustScaler(),RidgeCV(alphas=lambgrid, cv=5))\nlasso = make_pipeline(RobustScaler(), LassoCV(alphas=lambgrid, cv=5))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(l1_ratio=[.1, .5, .7, .9, .95, .99, 1],alphas=lambgrid, cv=5))\nlightgbm = LGBMRegressor(objective='regression', \n                                       num_leaves=4,\n                                       learning_rate=0.01, \n                                       n_estimators=5000,\n                                       max_bin=200, \n                                       bagging_fraction=0.75,\n                                       bagging_freq=5, \n                                       bagging_seed=7,\n                                       feature_fraction=0.2,\n                                       feature_fraction_seed=7,\n                                       verbose=-1,\n                                       )\nxgboost = xgb.XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)\n\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, xgboost, lightgbm),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)\nstack_gen.fit(np.array(X_train), np.array(y_train))\nstacked_preds = stack_gen.predict(np.array(df_newtest))","80612bed":"## Blending:\n\ndef blend_models_predict(preds_ridge, preds_xgb, preds_lgb):\n    return (np.exp(preds_ridge) + np.exp(preds_xgb) + np.exp(preds_lgb))\/3","1971b5c1":"submission = pd.DataFrame({'Id': test_id, 'SalePrice': np.exp(preds_xgb)})","764f5810":"# Stack Ridge Regression and XGBoost Regression Predictions\nblend_preds = blend_models_predict(preds_ridge, preds_xgb, preds_lgb)\nblend_submission = pd.DataFrame({'Id': test_id, 'SalePrice': blend_preds})\nstacked_submission = pd.DataFrame({'Id': test_id, 'SalePrice': np.exp(stacked_preds)})","433809d2":"# Save Stacked and XGBoost Regression Predictions\nsubmission.to_csv('Submission.csv',index=False)\nstacked_submission.to_csv('Stacked_Submission.csv', index=False)\nblend_submission.to_csv('Blend_Submission.csv', index=False)","5047e78b":"<a id=\"subsection-three\"><\/a>\n## Exploring Correlations\n---\n\n[Back to Table of Content](#toc)","c9430c7c":"<font size=\"3\">\nEver wonder how houses are priced? Market forces work to reduce divergences between housing prices. However, sometimes, the process gets inefficient due to lack of speed in information dissemination and inaccuracies in estimations. Using regression techniques to price houses aim to address this issue. In this notebook, I aim to provide an example of how we can apply regression techniques to such a problem, end-to-end, from eda and feature engineering to building our model\n<\/font>","2b0fe7c1":"<a id=\"section-six\"><\/a>\nStacking and Blending\n==\n\n<font size=\"3\">\n\n<ins> Stacking <\/ins>    \n    \nStacking involves building a metamodel, that weighs our models' predictions together, to give our final predictions    \n    \nLet's try stacking all our models together using StackingCVRegressor. Stacked generalization consists in stacking the output of individual estimator and use a regressor to compute the final prediction. Read more about it at: \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.StackingRegressor.html\n    \n  \nHere, we will use StackingCVRegressor for greater stability of results. \nFrom the documentation: \n\"In the standard stacking procedure, the first-level regressors are fit to the same training set that is used prepare the inputs for the second-level regressor, which may lead to overfitting. The StackingCVRegressor, however, uses the concept of out-of-fold predictions: the dataset is split into k folds, and in k successive rounds, k-1 folds are used to fit the first level regressor. In each round, the first-level regressors are then applied to the remaining 1 subset that was not used for model fitting in each iteration. The resulting predictions are then stacked and provided -- as input data -- to the second-level regressor. After the training of the StackingCVRegressor, the first-level regressors are fit to the entire dataset for optimal predictions.\"\n    \n![image.png](attachment:image.png)\n    \n    \nIn a nutshell, rather than fitting our metaregressor solely on seen data, we fit it on both seen and unseen data using KFolds Cross-Validation which provides a better indication of generalization against test data\n\n    \n<ins> Blending <\/ins>   \n    \nBlending involves taking simple\/weighted averages of our predictions to give our final predictions\n    \n[Back to Table of Content](#toc)","21c62809":"**<ins>\n<font size=\"3\">\nFinding out the data type of each column:\n    <\/font>\n <\/ins>**","6e2b9c2e":"<font size=\"3\">\nAs you can see from the example aboove, the training and test sets may have different categories within each categorical variable. What we have to do is to merge the 2 data sets temporarily to process the categorical columns and then seperate them out again\n    <\/font>","5ec27571":"<font size=\"3\">\nWe can impute 20:1-STORY 1946 & NEWER ALL STYLES with RL, since it is the mode in the category. However, Subclasses 30 and 70 would be trickier.\n    <\/font>","bd7676ba":"<font size=\"3\">\nAs an illustration, below is a graph of weights against alphas.\nThe larger alpha is, the bigger the regularization effect and hence, the smaller the coefficients.\n    <\/font>","0b8a75af":"<font size=\"3\">\n    \n```python\n['Utilities', 'Exterior1st', 'Exterior2nd', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF' ,'TotalBsmtSF', \n 'KitchenQual', 'Functional','GarageCars', 'GarageArea', 'SaleType']\n```","1b1fed70":"<font size=\"3\">\nThe reason why some categorical columns aren't imputed above is because they are exclusive on either data sets. Therefore, we need to be careful as imputing on either dataset will create a unforeseen category, for example, if we create a new category in the test dataset, the model trained on the training set may be unusable on the test set.\n    <\/font>","1cc8cec5":"<a id=\"toc\"><\/a>\n\n# Table of Contents\n<font size=\"3\">    \n* [Introduction](#section-zero)\n* [Executive Summary](#section-one)\n* [Resources Checklist](#section-onea)\n* [Exploratory Data Analysis](#section-two)\n    - [Ensuring Approximate Normality of Dependent Variable](#subsection-one)\n    - [Dealing with Outliers](#subsection-two)\n    - [Exploring Correlations](#subsection-three)\n* [Imputing Missing Values](#section-three)\n* [Data Wrangling](#section-four)\n* [Model Fitting](#section-five)\n* [Stacking & Blending](#section-six)  ","166b0ea8":"<font size=\"3\">\nFor the remaining columns that are categorical in test set, since they are at most missing a couple of values, we can fill them in by the mode of the data.\n    <\/font>","0c0b2303":"# Advanced Housing Regression","8e1ca30d":"<font size=\"3\">\nThat concludes Imputing of Missing Values. Next, we need to do some Data Wrangling, especially for categorical variables, using techniques such as One-Hot Encoding before we can train our model.\n    <\/font>","d7cfbfcc":"<a id=\"section-zero\"><\/a>\nIntroduction\n==\n\n<font size=\"3\">\nThis notebook aims to provide an in-depth exploratory data analysis and feature engineering to regression modelling  \n      <br\/><br\/>\n<\/font>\n    \n<font size=\"3\">\nIf this kernel has helped you in any way, please upvote, it provides me a lot of motivation to continue providing more in-depth analyses and to share what I have learnt about modelling. Any feedback is also greatly welcomed, please comment it down below\n    <br\/><br\/>\n<\/font>\n\n<font size=\"3\">\nNote that this notebook is still a work in progress, I am currently working to improve accuracy further :)\n<\/font>","008ef32d":"<font size=\"3\">\nFrom description of data (data_description.txt),<br\/><br\/>\n    <\/font>\n\n<font size=\"3\">\n<ins>categorical variables:<\/ins> <br \/>\n    \n    \n```python    \n[ 'MSSubClass' , 'MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2',\n  'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', \n  'Heating', 'CentralAir', 'Electrical', 'GarageType', 'PavedDrive', 'MiscFeature', 'SaleType', 'SaleCondition']\n```   \n    \n  \n\n<ins>ordinal variables:<\/ins>\n```python  \n[ 'LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtCond', 'BsmtExposure', \n  'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'Bedroom', 'Kitchen', \n  'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces' , 'FireplaceQu',  'BsmtQual', 'GarageFinish', 'GarageQual', \n  'GarageCond', 'PoolQC', 'Fence']\n```\n\n  \n\n  <ins>continous variables are:<\/ins>\n    \n```python \n[ 'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',  '1stFlrSF',  '2ndFlrSF',  \n  'LowQualFinSF',  'GrLivArea', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', \n  'ScreenPorch', 'PoolArea', 'MiscVal' ]\n```  \n\n  <ins>other variables (Date, etc..) :<\/ins> \n```python \n[' YearBuilt', 'YearRemodAdd', 'GarageYrBlt', 'MoSold', 'YrSold']\n```","09273150":"<font size=\"3\">\nIntroducing ElasticNet:\n<\/font>","7ac9e4e3":"<font size=\"3\">\nExtreme Gradient Boosting has dethroned Ridge Regression!\n    <\/font>","00895f1d":"<font size=\"3\">\n\nFor our training set, we are left with \n```\nElectrical and GarageYrBlt\n``` \n\n<br \/><br \/>\nFor the test set, we are left with: \n\n```\nMSZoning, LotFrontage, Utilities, Exterior1st, Exterior2nd, MasVnrType, MasVnrArea, BsmtFinSF1,\nBsmtUnfSF,TotalBsmtSF, BsmtFullBath, BsmtHalfBath, KitchenQual, Functional, GarageYrBlt, \nGarageCars, GarageArea, SaleType\n```","4cd975f9":"<font size=\"3\">\nIntuition: LotFrontage depends on the type of dwelling. Some dwellings will naturally have less access to the street (for more secluded kinds of dwellings).\n\nExamining the boxplot, we can see that, indeed there are variations in LotFrontage among different kinds of dwellings.\n\nThus, missing values of LotFrontage can be imputed","786f2504":"<font size=\"3\">\nThe scatterplots exhibit some interesting patterns. <br \/><br \/>\n\n1) There seems to be strong linear correlation between Above grade (ground) living area, TotalBsmtSF and Sale Price, except for a couple of outliers.\n\n2) For ordinal variables, OverallQual factors in significantly. Number of Full Bathrooms above grade shows a strong, significant pattern with SalePrice, along with GarageCars, although for GarageCars, the largest size tends to have the lowest SalePrice which is interesting. It could be that the largest garage would be difficult to maintain","aa23aa01":"<font size=\"3\">\nIntuitively, Year that garage is built shouldn't affect SalePrice significantly though we do not know for sure. However, to make life easier, we will drop this column in this analysis.\n    <\/font>","748dcca3":"<a id=\"section-three\"><\/a>\n**Imputing Missing Values:**\n==\n\n[Back to Table of Content](#toc)","d73c6761":"<a id=\"section-four\"><\/a>\nData Wrangling\n==\n\n[Back to Table of Content](#toc)","332fc7e0":"<font size=\"3\">\nWe can check if there is a relationship between MSZoning and MSSubClass\n    <\/font>","7658dfed":"<font size=\"3\">\nIn view of time, we impute house with SubClass 30 & 70 as \"RM\" since it is the mode.\n    <\/font>","ad06fabc":"<a id=\"section-onea\"><\/a>\nResources Checklist\n==\n\n<font size=\"3\">\n    \nIf you are not a beginner in pandas and modelling, please skip this section\n    \n<ins> Beginners' Resources <\/ins> \n\n* New to Python? Please go through: https:\/\/www.kaggle.com\/learn\/python\n* New to Pandas? Please go through: https:\/\/www.kaggle.com\/learn\/pandas\n* New to Machine Learning? Please go through: \n    * https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning\n    * https:\/\/www.kaggle.com\/learn\/intermediate-machine-learning\n* Notebook Formatting: https:\/\/www.kaggle.com\/chrisbow\/formatting-notebooks-with-markdown-tutorial\n    \n    \n \n<ins> Intermediate Resources <\/ins> \n    \nI found these articles to be helpful in understanding regression: \n    \n* Linear Regression: https:\/\/towardsdatascience.com\/linear-regression-understanding-the-theory-7e53ac2831b5\n* Ridge, LASSO and ElasticNet: https:\/\/www.datacamp.com\/community\/tutorials\/tutorial-ridge-lasso-elastic-net\n    \n<ins> Documentations <\/ins>\n* Scikit-learn's documentation homepage: https:\/\/scikit-learn.org\/stable\/\n* Scikit-learn's supervised learning documentation: https:\/\/scikit-learn.org\/stable\/supervised_learning.html#supervised-learning\n* XGBoost documentation: https:\/\/xgboost.readthedocs.io\/en\/latest\/\n* LGBM Documentation: https:\/\/lightgbm.readthedocs.io\/en\/latest\/\n    \n    \nKnow of any other great resources? Please comment down below so I can add them to this section\n    \n    \n[Back to Table of Content](#toc)","1e47516a":"<a id=\"section-two\"><\/a>\n**Exploratory Data Analysis:**\n==\n\n<font size=\"3\">\n1. Ensuring Approximate Normality of dependent variable <br\/>\n2. Dealing with Outliers  <br\/>\n3. Exploring Correlations  <br\/>\n    \n<br\/>\n    \n[Back to Table of Content](#toc)","0dbe44fb":"<font size=\"3\">\nInformation is not useful for finding out if column represents continous\/categorical\/ordinal variable. In this case, the only ways are to manually explore each column and read data description.\n    <\/font>","0c6b72ee":"<font size=\"3\">\nLooks like Ridge Regression wins, followed by ElasticNet then Lasso.\n\nCan we do even better? How about Gradient Boosted Regression?","ff2bc8ac":"<a id=\"subsection-two\"><\/a>\n## Dealing with Outliers\n---\n\n[Back to Table of Content](#toc)","068038c3":"<font size=\"3\">\nLot Frontage \u2713\n    <\/font>\n","8f6cea21":"<font size=\"3\">\nThere are 3 types of variables we have to deal with: categorical, ordinal and date variables.\n\nFor categorical, we need to perform one-hot encoding to implement regression.\n\nFor ordinal, we need to assign values to each level.\n\nFor the purpose of this analysis, to be conservative, we encode ordinal variables (in other words, we will just treat ordinal variables like categorical variables).\n\nFor date, we need to either treat it as a categorical variable or drop the column","874225c9":"<font size=\"3\">\nWe need to understand our data better. To do so, we can observe patterns between our independent variables and the dependent variable. However, there are many independent variables. We need a smart way to analyse our explanatory variables instead of analysing them one by one.\n    <\/font>","a20a2835":"<font size=\"3\">\n<ins>Columns with missing values:<\/ins> <br \/><br \/>\n    <ins> Training set: <\/ins><br \/><br \/>\n    \n```python\n[ 'LotFrontage', 'Alley', 'BsmtQual','BsmtCond', 'BsmtExposure', 'BsmtFinType1', \n  'BsmtFinType2','Electrical','FireplaceQu','GarageType','GarageYrBlt',\n  'GarageFinish', 'GarageQual','GarageCond','PoolQC','Fence','MiscFeature']\n```\n\n<ins> Test set: <\/ins><br \/>\n```python\n['MSZoning', 'LotFrontage','Alley','Utilities','Exterior1st','Exterior2nd','MasVnrType','MasVnrArea', \n'BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinSF1','BsmtFinType2','BsmtFinSF2',\n'BsmtUnfSF','TotalBsmtSF','BsmtFullBath', 'BsmtHalfBath','KitchenQual','Functional','FireplaceQu', \n'GarageType', 'GarageYrBlt','GarageFinish','GarageCars','GarageArea','GarageQual','GarageCond']\n```","6d13ed98":"<font size=\"3\">\nIt seems that our dependent variable has a skewed distribution and not an approximately normal distribution. We will need to transform our dependent variable to increase accuracy of prediction.\n    <\/font>","57cc7198":"<font size=\"3\">\nLooking at our heatmap, Important factors include OverallQual, LowQualFinSF, GarageCars\n<\/font>\n\n<font size=\"3\">\nThis makes a lot of sense. Overall Quality of material of a house will obviously affect SalePrice significantly along with amount of floor that are low quality in sq feet and size of garage.\n<\/font>","76c2b1b0":"<font size=\"3\">\nLet's see if we can get a lower CV score with LASSO or  ElasticNet.\n    <\/font>","d139c011":"<a id=\"subsection-one\"><\/a>\n## Ensuring Approximate Normality of dependent variable\n---\n\n[Back to Table of Content](#toc)","34c7f03d":"<font size=\"3\">\nlog(SalePrice) is a more regular distribution with less skewness. We can use log(SalePrice) as our dependent variable)\n    <\/font>","caa9bd4e":"<font size=\"3\">\nNow, we can finally train models on our data!\n<\/font>","9caf5ee6":"<font size=\"3\">\nColumns that are categorical in training set and test set that have missing values:\n    \n```python\n['Alley', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', \n 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence', \n 'MiscFeature','MasVnrType']\n```\n\nThe missing values could represent an important piece of information hence it would be unwise to simply drop these columns\n\nOn account of loss of information should we choose to drop these columns, and to make our life easier, let's create a separate category in each of these columns for the values that are missing","bd9d663e":"<font size=\"3\">\nWe are left with MasVnrArea to impute for both training and test data.\n    <\/font>","a1d3d0b9":"**<font size=\"3\">\nDealing with dates:\n<\/font>**","b1ca7e6f":"![image.png](attachment:image.png)","ead8a686":"<a id=\"section-five\"><\/a>\nModel Fitting\n==\n<font size=\"3\">\n\nFor our model fitting, we will define Pipelines for our sklearn models. \n\n<ins>What are sklearn Pipelines?<\/ins>\n* sklearn Pipelines sequentially apply a list of transforms and a final estimator. You can save pipelines as pkl file for ease of access. It simpifies your training code and improves readability.  \n* See: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html for more details.\n\nIn each Pipeline, we define a RobustScaler.\n\n<ins>What is RobustScaler?<\/ins>  \n* RobustScaler removes the median and scales the data according to the quantile range (defaults to IQR: Interquartile Range). This is to make our data robust to outliers. \n* See: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html for more details\n\n[Back to Table of Content](#toc)","2aebc787":"<font size=\"3\">\nLasso has a larger mean squared error as compared to Ridge. Looks like Ridge Regression is leading!\n    <\/font>","6d0695ed":"<font size=\"3\">\nImpute remaining categorical variables in test set:\n    <\/font>","03dbf84b":"<font size=\"3\">\nQQline represents a normal distribution. The scatterpoints align closely with the QQline, except for the tails. We can conclude that log(SalePrice) is approximately normally distributed\n<\/font>","0b8355bc":"<font size=\"3\">\nMasonry Veneer Area does not seem seperable by Masonry Veneer Type. Nevertheless, logically, it should depend on the Masonry Veneer Type and there are nuances between different Masonry Types. Given that this is the only logical link that seems present at the moment, we impute by Masonry Veneer Type.\n    <\/font>","53a6747a":"<font size=\"3\">\nThere are a staggering 81 features present in this dataset!\n<\/font>","296878d1":"<a id=\"section-one\"><\/a>\nExecutive Summary\n==\n\n<font size=\"3\">\n\nIn this notebook, I worked on feature engineering and imputing missing values. There are over 80 features in this dataset, which makes it a mammoth task for beginners. Along the way, I try to add explanations of features to make it as beginner-friendly as possible.\n\nThe models I used are:\n* Lasso regression model \n* XGBoost model\n* LGBM model","4bf7c90e":"<font size=\"3\">\nAs a reminder:<br \/><br \/>\n  \n__categorical variables__: <br \/>\n```python\n[ 'MSSubClass' , 'MSZoning', 'Street', 'Alley', 'LandContour', 'LotConfig', 'Neighborhood', 'Condition1', 'Condition2', \n  'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation', 'Heating', \n  'CentralAir', 'Electrical', 'GarageType', 'PavedDrive', 'MiscFeature', 'SaleType', 'SaleCondition']\n```\n  \n__ordinal variables__: <br \/>\n```python\n[ 'LotShape', 'Utilities', 'LandSlope', 'OverallQual', 'OverallCond', 'ExterQual', 'ExterCond', 'BsmtCond', 'BsmtExposure', \n  'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', \n  'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Functional', 'Fireplaces' , 'FireplaceQu',  'basement', 'GarageFinish', \n  'GarageQual', 'GarageCond', 'PoolQC', 'Fence']\n```\n  \n__other variables__ (Date, etc..) : <br \/>\n```python\n[' YearBuilt', 'YearRemodAdd',  'MoSold', 'YrSold']\n```\n  \nMoSold and YrSold can be treated as categorical variables. We will deal with the rest of the date variables later","56cdc8a5":"<font size=\"3\">\nMasonryType is not reported, we cannot impute for training set. There is no other available option except to impute as 0.\n    <\/font>","0d596a20":"<font size=3>\nThere are 17 columns with missing values for training data and 29 columns with missing values for test data!","969a50d4":"<font size=\"3\">\nTrimmed mean is close to sample mean, this suggests there are no extreme outliers\n    <\/font>","f47ff120":"<font size=\"3\">\nWe can take current year and subtract it by year built. this will give us the age of the house. \nFor YearRemodAdd, it will tell us when was the last time the house was remodelled which would indicate information\nabout the condition of the house.\n<\/font>\n","62567b7c":"<font size=\"3\">\nThere is 1 missing value for LotFrontage left and that is for MSSubClass 150. We have little information to impute this value, however, we know it is a PUD.  A reasonable value to impute will be to take average of the medians of the 1-story and 2-story PUDs.\n    <\/font>","3f13cb87":"<font size=\"3\">\nA really useful tool to inspect correlations between variables is a heatmap. This allows us to find out which independent variable is highly correlated with the dependent variable.\n    <\/font>"}}