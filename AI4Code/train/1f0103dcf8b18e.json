{"cell_type":{"9a6c98c8":"code","5b222c7c":"code","8d97e18b":"code","94429f44":"code","7fbff5e9":"code","cae0f6d1":"code","7a21c4be":"code","a0694cfa":"code","fa88c677":"code","e4c562b9":"code","2733e9e5":"code","3a65cb7e":"code","c950e419":"code","2a598983":"code","544a3656":"code","a530de62":"code","d45c6598":"code","8794a5e8":"code","450e7533":"code","280f3fe0":"code","5136da79":"code","ba8fc552":"code","9b422674":"code","738d7315":"code","e1fefa5d":"code","98f33143":"code","bffa8e0b":"code","16df48f2":"code","a30a072e":"code","492f0bbd":"code","9d7dc9a8":"code","200126d0":"code","c02cfe3b":"code","54fee0cb":"markdown","5c31b257":"markdown","a2990409":"markdown","df6b8425":"markdown","d45d87a9":"markdown","35224b57":"markdown","3f43aa7e":"markdown"},"source":{"9a6c98c8":"import pandas as pd\nimport numpy as np\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import names, stopwords\nimport unicodedata\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report\nfrom sklearn.linear_model import LogisticRegression","5b222c7c":"tweet_train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntweet_test_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nId = tweet_test_df['id']","8d97e18b":"tweet_train_df.head()","94429f44":"tweet_train_df.isna().sum()","7fbff5e9":"tweet_train_df.info()","cae0f6d1":"tweet_train_df.shape","7a21c4be":"def data_imputation(data):\n    data['keyword'].fillna(' ', inplace=True)\n    data['location'].fillna(' ', inplace=True)\n    return data","a0694cfa":"tweet_train_df = data_imputation(tweet_train_df)","fa88c677":"tweet_test_df = data_imputation(tweet_test_df)","e4c562b9":"tweet_train_df.isna().sum()","2733e9e5":"tweet_train_df['text'] = tweet_train_df['text'] +' '+ tweet_train_df['location'] +' '+ tweet_train_df['keyword']\ntweet_test_df['text'] = tweet_test_df['text'] +' '+ tweet_test_df['location'] +' '+ tweet_test_df['keyword']","3a65cb7e":"target = tweet_train_df['target']","c950e419":"tweet_train_df.drop(['target', 'location', 'keyword', 'id'], axis=1, inplace=True)\ntweet_test_df.drop(['location', 'keyword', 'id'], axis=1, inplace=True)","2a598983":"lemmetizer = WordNetLemmatizer()","544a3656":"all_names = set(names.words())","a530de62":"stop_words = set(stopwords.words('english'))","d45c6598":"tf_idf = TfidfVectorizer(min_df=0.1, max_df=0.7)","8794a5e8":"def cleaned_string(string):\n    # Removing all the digits\n    string = re.sub(r'\\d', '', string)\n    \n    # Removing accented data\n    string = unicodedata.normalize('NFKD', string).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n    # Removing Mentions\n    string = re.sub(r'@\\w+', ' ', string)\n    \n    # Removing links \n    string = re.sub(r'(https?:\\\/\\\/)?([\\da-zA-Z\\.-\\\/\\#\\:]+)\\.([\\da-zA-Z\\.\\\/\\:\\#]{0,9})([\\\/\\w \\.-\\\/\\:\\#]*)', ' ', string)\n    \n    # Removing all the digits special caharacters\n    string = re.sub(r'\\W', ' ', string)\n        \n    \n    # Removing double whitespaces\n    string = re.sub(r'\\s+', ' ', string, flags=re.I)\n    \n\n    \n    string = string.strip()\n    \n    #Removing all Single characters\n    string = re.sub(r'\\^[a-zA-Z]\\s+','' , string)\n    \n    \n    # Lemmetizing the string and removing stop words\n    string = string.split()\n    string = [lemmetizer.lemmatize(word) for word in string if word not in stop_words and word not in all_names]\n    string = ' '.join(string)\n    \n    # Lowercasing all data\n    string = string.lower()\n        \n    return string","450e7533":"def clean_text(data):\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            data.iloc[i, j] = cleaned_string(data.iloc[i, j])\n    return data\n            \n            \n    ","280f3fe0":"tweet_cleaned_test_df = clean_text(tweet_test_df)","5136da79":"tweet_cleaned_test_df.shape","ba8fc552":"tweet_cleaned_test_df.head()","9b422674":"tweet_cleaned_train_df = clean_text(tweet_train_df)","738d7315":"tweet_train_df.shape","e1fefa5d":"tweet_cleaned_train_df.head()","98f33143":"X_train, X_valid, y_train, y_valid = train_test_split(tweet_cleaned_train_df['text'], target,random_state = 0)\n","bffa8e0b":"catboost = LogisticRegression()","16df48f2":"pipeline_sgd = Pipeline([\n    ('tfidf',  TfidfVectorizer()),\n    ('nb', catboost,)\n])","a30a072e":"model = pipeline_sgd.fit(X_train, y_train)","492f0bbd":"y_predict = model.predict(X_valid)","9d7dc9a8":"print(classification_report(y_valid, y_predict))","200126d0":"y_pred_test = model.predict(tweet_cleaned_test_df['text'])","c02cfe3b":"# Saving result on test set\noutput = pd.DataFrame({'Id': Id,\n                       'target': y_pred_test})\n\noutput.to_csv(r'submission.csv', index=False)","54fee0cb":"All the missing columns are filled with appropriate values, later maybe we will change value of keyword dynamically for each row accordingly for now it's Na.\nMissing locations are set to Unknown","5c31b257":"# Text Filtering\nFiltering Keyword, location and text columns by removing numbers, hashtags, names, Url and mentions","a2990409":"# Data Familiarization","df6b8425":"Keyword and location  contains lots of Missing values.","d45d87a9":"7613 rows of tweets with 4 columns id, keyword, location and text all object","35224b57":"# Data Imputation","3f43aa7e":"# Importing Files"}}