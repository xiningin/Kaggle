{"cell_type":{"5893b750":"code","780e6caf":"code","5393b4c1":"code","d1b013e0":"code","0e8c2c78":"code","6fc6a98f":"code","b498353f":"code","55095383":"code","bfe6459f":"code","9d299d13":"code","eb7e76f5":"code","536807a3":"code","90277fe7":"code","f7d5c245":"code","4d4ba5d2":"code","77f01075":"code","db3c0882":"code","34ba77f8":"code","af4b853f":"code","c41c4985":"code","dad2a17e":"code","87471cb7":"code","760e2824":"code","78f283fd":"code","2074aa6e":"code","80c9b137":"code","fa4c6187":"code","523f6c82":"code","e8bb232e":"markdown","3d5d3342":"markdown","e18c94e0":"markdown","b489ae62":"markdown","74c8c04a":"markdown"},"source":{"5893b750":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","780e6caf":"import pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport PIL\nimport PIL.Image\nimport pathlib\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","5393b4c1":"input_dir = pathlib.Path('..\/input\/as5input\/assignment5\/')","d1b013e0":"training_md = pd.read_csv(pathlib.Path(input_dir, 'assignment5_training_data_metadata.csv')).drop(columns = [\"id\", \"cause\"])\ntraining_md = training_md.fillna('Normal')\ntraining_md = training_md.replace({'type' : {'Normal' : 1, 'Virus' : 2, 'bacteria' : 3, 'Stress-Smoking' : 4}})\ntraining_md","0e8c2c78":"training_md.groupby('type').count()","6fc6a98f":"testing_md = pd.read_csv(pathlib.Path(input_dir, 'assignment5_test_data_metadata.csv'))\nsample_md = pd.read_csv('..\/input\/as5input\/assignment5\/assignment5_submission.csv')","b498353f":"data_dir = pathlib.Path(input_dir, 'images', 'images', 'train')","55095383":"training_imgs = list(data_dir.glob(r'*\/*.jpeg'))\nimg_count = len(training_imgs)\nprint(img_count)","bfe6459f":"batch_size = 32\nimg_height = 180\nimg_width = 180","9d299d13":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,  \n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","eb7e76f5":"val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,  \n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","536807a3":"class_names = train_ds.class_names\nprint(class_names)","90277fe7":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","f7d5c245":"num_classes = 4\n\nmodel = tf.keras.Sequential([\n  layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(img_height, img_width, 3)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])","4d4ba5d2":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel.build()\nmodel.summary()","77f01075":"epochs=10\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","db3c0882":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","34ba77f8":"data_augmentation = keras.Sequential(\n  [\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape=(img_height, \n                                                              img_width,\n                                                              3)),\n    layers.experimental.preprocessing.RandomRotation(0.1),\n    layers.experimental.preprocessing.RandomZoom(0.1),\n  ]\n)","af4b853f":"model_tuned = tf.keras.Sequential([\n  data_augmentation,\n  layers.experimental.preprocessing.Rescaling(1.\/255),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Dropout(0.2),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(num_classes)\n])","c41c4985":"model_tuned.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nmodel_tuned.build()\nmodel_tuned.summary()","dad2a17e":"epochs = 15\nhistory = model_tuned.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","87471cb7":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","760e2824":"test_dir = pathlib.Path('..\/input\/as5input\/assignment5\/images\/images\/test')\ntest_images = list(test_dir.glob('*\/*'))\nimg_count_2 = len(test_images)\nprint(img_count_2)","78f283fd":"test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  shuffle = False,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n\nprob_model = tf.keras.Sequential([model_tuned, tf.keras.layers.Softmax()])\npredictions = prob_model.predict(test_ds)\n\ntest_paths = [pathlib.Path(x) for x in test_ds.file_paths]\ntest_frame = pd.DataFrame({'image_name': test_paths, 'type' : np.argmax(predictions, axis=1)+1})\n\nsubmission_frame = test_frame.merge(training_md.reset_index().rename(columns={'index' : 'id'})[['id', 'image_name']], on='image_name')\nsubmission_frame = submission_frame[['id', 'image_name', 'type']]\nsubmission_frame = submission_frame.sort_values('id').reset_index(drop = True)\nsubmission_frame","2074aa6e":"predictions","80c9b137":"test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  test_dir,\n  label_mode = None,  \n  shuffle=False,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)\n\npredictions = prob_model.predict(test_ds)\n\ntest_paths = [pathlib.Path(x) for x in test_ds.file_paths]\ntest_frame = pd.DataFrame({'image_name': test_paths, 'type' : np.argmax(predictions, axis=1)+1})\n\n#submission_frame = test_frame.merge(testing_md.reset_index().rename(columns={'index' : 'id'})[['id', 'image_name']], on='image_name')\n#submission_frame = submission_frame[['id', 'type']]\n#submission_frame = submission_frame.sort_values('id').reset_index(drop = True)\n#submission_frame","fa4c6187":"submission = { 'id': testing_md.id, 'type' : np.argmax(predictions, axis=1)+1}\nsubmission = pd.DataFrame(submission)\nsubmission","523f6c82":"submission.to_csv('as5_submission.csv', index = False)","e8bb232e":"# Loading Data","3d5d3342":"# Datasets","e18c94e0":"# Testing the Model","b489ae62":"# Building the Model","74c8c04a":"# Modifying the Model"}}