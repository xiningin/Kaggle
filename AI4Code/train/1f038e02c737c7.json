{"cell_type":{"cbab353f":"code","e5ed552f":"code","f1dd41bc":"code","35c90b5e":"code","9f28deff":"code","7db68afc":"code","2674f65d":"code","c638fa77":"code","30c68265":"code","470848e7":"code","ca14898b":"code","cf7c4bd8":"code","12249daa":"code","67d15625":"code","5f47d6c3":"code","6e4ecf86":"code","0f1feb53":"code","9d5114fb":"code","469aea00":"code","c1bb1b43":"code","7a16bdc6":"code","d185b172":"code","7d59449a":"code","bd818ae7":"code","86fee4ac":"code","d90e07b3":"code","fe3397f4":"code","2f8523b1":"code","8423b1b1":"code","de98cd0f":"code","6f02f09b":"code","ca839882":"code","ee65548b":"code","c2225737":"code","6122c8ac":"code","89de8fcb":"code","1cf24a6e":"code","5331aecd":"code","4e245e2d":"code","7271e609":"code","b42243ab":"code","dfb612d6":"code","595dfcd0":"code","6b38e338":"markdown","8c665339":"markdown","c54c1a61":"markdown","6390019a":"markdown","bff71d3e":"markdown","3f48acd9":"markdown","cd254a5e":"markdown","67d31cf0":"markdown","3ff66fde":"markdown","eb186361":"markdown","05d3d9b5":"markdown","ab96b5ca":"markdown","eef1c20e":"markdown","11774330":"markdown","605f45de":"markdown","1464f644":"markdown","d16f53b5":"markdown","4ad96f30":"markdown","f614b067":"markdown","dd934b56":"markdown","ecbcbb08":"markdown","e6c45b71":"markdown","230036f9":"markdown","c41de9a0":"markdown"},"source":{"cbab353f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e5ed552f":"train_transaction_df = pd.read_csv('\/kaggle\/input\/train_transaction.csv')\ntrain_identity_df = pd.read_csv('\/kaggle\/input\/train_identity.csv')","f1dd41bc":"train_transaction_df.head()","35c90b5e":"train_identity_df.head()","9f28deff":"df_train = train_transaction_df.merge(train_identity_df, on='TransactionID', how='left')","7db68afc":"df_train.shape","2674f65d":"df_train.head().transpose()","c638fa77":"df_train.memory_usage().sum()","30c68265":"# Get the categorical and numeric columns\ncat_cols = [\n    'ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n    'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'DeviceType', 'DeviceInfo',\n] + [f'M{n}' for n in range(1, 10)] + [f'id_{n}' for n in range(12, 39)]\nnum_cols = list(set(df_train.columns) - set(cat_cols))","470848e7":"a = df_train[num_cols].isnull().any()\ntrain_null_num_cols = a[a].index","ca14898b":"nas = {}\nfor n in train_null_num_cols:\n    df_train[f'{n}_isna'] = df_train[n].isnull()\n    median = df_train[n].median()\n    df_train[n].fillna(median, inplace=True)\n    nas[n] = median","cf7c4bd8":"integer_cols = []\nfor c in num_cols:\n    try:\n        if df_train[c].fillna(-1.0).apply(float.is_integer).all():\n            integer_cols += [c]\n    except Exception as e:\n        print(\"error: \", c, e)","12249daa":"stats = df_train[integer_cols].describe().transpose()\nstats","67d15625":"int8columns = stats[stats['max'] < 256].index\nprint(int8columns.shape)\nprint(int8columns)\nint16columns = stats[(stats['max'] >= 256) & (stats['max'] <= 32767)].index\nprint(int16columns.shape)\nprint(int16columns)","5f47d6c3":"for c in int8columns:\n    df_train[c] = df_train[c].astype('int8')\n    \nfor c in int16columns:\n    df_train[c] = df_train[c].astype('int16')","6e4ecf86":"df_train.memory_usage().sum()","0f1feb53":"test_transaction_df = pd.read_csv('\/kaggle\/input\/test_transaction.csv')\ntest_identity_df = pd.read_csv('\/kaggle\/input\/test_identity.csv')\ndf_test = test_transaction_df.merge(test_identity_df, on='TransactionID', how='left')","9d5114fb":"for k, v in nas.items():\n    df_test[f'{k}_isna'] = df_test[k].isnull()\n    df_test[k].fillna(v, inplace=True)","469aea00":"test_num_cols = list(set(num_cols) - set(['isFraud']))\na = df_test[test_num_cols].isnull().any()\ntest_null_num_cols = a[a].index","c1bb1b43":"for n in test_null_num_cols:\n    df_test[n].fillna(df_train[n].median(), inplace=True)  # use the training set's median!","7a16bdc6":"# copied from above cells\n\ninteger_cols = []\nfor c in test_num_cols:\n    try:\n        if df_test[c].fillna(-1.0).apply(float.is_integer).all():\n            integer_cols += [c]\n    except Exception as e:\n        print(\"error: \", c, e)\nstats = df_test[integer_cols].describe().transpose()\nint8columns = stats[stats['max'] < 256].index\nint16columns = stats[(stats['max'] >= 256) & (stats['max'] <= 32767)].index\nfor c in int8columns:\n    df_test[c] = df_test[c].astype('int8')\n    \nfor c in int16columns:\n    df_test[c] = df_test[c].astype('int16')","d185b172":"for c in cat_cols:\n    df_train[c] = df_train[c].fillna(\"missing\")\n    \nfor c in cat_cols:   \n    df_test[c] = df_test[c].fillna(\"missing\")","7d59449a":"cats = {}\nfor c in cat_cols:\n    df_train[c] = df_train[c].astype(\"category\")\n    df_train[c].cat.add_categories('unknown', inplace=True)\n    cats[c] = df_train[c].cat.categories","bd818ae7":"for k, v in cats.items():\n    df_test[k][~df_test[k].isin(v)] = 'unknown'","86fee4ac":"from pandas.api.types import CategoricalDtype\n\nfor k, v in cats.items():\n    new_dtype = CategoricalDtype(categories=v, ordered=True)\n    df_test[k] = df_test[k].astype(new_dtype)","d90e07b3":"for c in cat_cols:\n    df_train[c] = df_train[c].cat.codes\n    df_test[c] = df_test[c].cat.codes\n    ","fe3397f4":"df_train.to_feather('df_train')","2f8523b1":"df_test.to_feather('df_test')","8423b1b1":"idx = int(len(df_train) * 0.8)\ntraining_set, validation_set = df_train[:idx], df_train[idx:]","de98cd0f":"y_train = training_set['isFraud']\nX_train = training_set.drop('isFraud', axis=1)\ny_valid = validation_set['isFraud']\nX_valid = validation_set.drop('isFraud', axis=1)","6f02f09b":"print(X_train.shape, y_train.shape)","ca839882":"print(X_valid.shape, y_valid.shape)","ee65548b":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import roc_auc_score","c2225737":"training_sample = training_set[-100000:]\ny_train_sample = training_sample['isFraud']\nX_train_sample = training_sample.drop('isFraud', axis=1)","6122c8ac":"model = RandomForestRegressor(\n    n_estimators=400, max_features=0.3,\n    min_samples_leaf=20, n_jobs=-1, verbose=1)","89de8fcb":"model.fit(X_train_sample, y_train_sample)","1cf24a6e":"preds_valid = model.predict(X_valid)","5331aecd":"roc_auc_score(y_valid, preds_valid)","4e245e2d":"model = RandomForestRegressor(\n    n_estimators=400, max_features=0.3,\n    min_samples_leaf=20, n_jobs=-1, verbose=1)","7271e609":"y = df_train['isFraud']\nX = df_train.drop('isFraud', axis=1)","b42243ab":"model.fit(X, y)","dfb612d6":"y_preds = model.predict(df_test)","595dfcd0":"submission = pd.read_csv('\/kaggle\/input\/sample_submission.csv')\nsubmission['isFraud'] = y_preds\nsubmission.to_csv('submission.csv', index=False)","6b38e338":"## Validation set\n\nNow we can start training our model. But how do we know if a model is good or not? We commonly use something called a validation set, that is separate to the test set. The reason we have a holdout set is that we use the validation set to choose our model (even if we don't use it for training), otherwise our model will overfit. If you're unfamiliar with this, I suggest reading on overfitting and underfitting.\n\nThe data description seems to indicate that the data is time ordered, so we don't really want a random split. So let's hold out a portion of the bottom rows to use as our validation set, and the rest as our training set.","8c665339":"## Submission","c54c1a61":"Using the whole training set is too time-consuming for quick iteration, so we can use a sample. \n\nWe could use a random sample, but since this is time-ordered, I'm guessing the more recent rows would give us better predictive value. So let's just grab the bottom rows.","6390019a":"We added some columns in our training set and replaced missing values with the medians. We need to add those same columns, and also add the median from the training set for those missing values (the same ones).","bff71d3e":"So we can see here that there are some very small ranges there -- not all of them will need to be `float64`. Let's downcast them.","3f48acd9":"Next we'll convert the columns in the training set to categorical.","cd254a5e":"Now we're more or less done with the minimum preprocessing required. Let's save our progress to a feather file, so that we don't have to go through it again!","67d31cf0":"As described in the other kernel, some of these columns are `float64` by default due to the presence of some `NaN` values.\n\nThe fact that they are `NaN` might be meaningful in this context, so completely replacing them (this is called imputation), doesn't sound like a great idea. However, we can add a boolean column to mark that the column has been replaced, and hopefully the training algorithm is smart enough to take care of it. There's no guarantee that it will, though! So you should always challenge your assumptions (e.g. maybe just dropping the columns could give you similar results, and train faster).\n\nAlso bear in mind we'll need to use the same values we're using to impute on the training set on the test set. ","3ff66fde":"# Beginner's Random Forests example\n\nThis is a very simple Random Forests example meant for beginners. This is not meant to achieve a high score, merely a starting point on which to start, without complicated techniques.\n\nIt's recommended that you finish the Kaggle Learn courses (introduction and intermediate machine learning).","eb186361":"Looks like we shaved a whole gig.\n\nWe're not done yet, we need to make sure we do the same thing on the test set. Let's read it in now, merge the tables, and impute it the same way.","05d3d9b5":"There will be some errors printed, but that's normal because some numeric columns are already integers. I'm too lazy to fix that right now.\n\nLet's look at some stats.","ab96b5ca":"Now, we can downcast numeric columns in the same way","eef1c20e":"Unfortunately, we're not done yet. We might have some missing values on other numeric columns we didn't anticipate. \n\nIn practice, we don't always have a \"test set\". The test set might be new observations that come in the future, could be a list of observations or a single one, so we can't really use statistics from the test set to impute the missing values. So we need to use values from the training set.\n\nWe'll use the median as well.","11774330":"In order to use these files for training, we'll need to do what's sometimes called denormalising the data. We can do this by doing a left join on both tables using the DataFrame's `merge()` method.","605f45de":"## Files\n\nAs can be seen the training data contains two files, `train_transaction.csv` and `train_identity.csv`. These two tables are related to each other via the column `TransactionID`. ","1464f644":"## Memory reduction\n\nAs discussed in [my other kernel](https:\/\/www.kaggle.com\/yoongkang\/beginner-memory-reduction-techniques), parsing the training dataset with default settings could take up to 2GBs of memory unnecessarily. With a few techniques (also discussed in the linked kernel) we can cut this down by about a gigabyte. \n\nIn this kernel, we'll use similar techniques, if you want to see my reasoning for this please refer to the other kernel.\n\nFirst we need to determine which numeric columns we have so that we can downcast them (cast from float64 to another type that requires less memory).","d16f53b5":"Let's do a sanity check whenever we do something like this. Make sure the shape contains the same number of rows and the combined columns:","4ad96f30":"Now that we have a decent model, we can actually train on the whole dataset, including the validation set.","f614b067":"Looks like that worked! But we're now using a lot of RAM (look at the sidebar of your kernel). My kernel currently says I'm at 5 gigabytes, and we haven't even read the test set yet!\n\nWhile cleaning the data, it's possible we may need to make copies of (some sections) of the data, so this is obviously not ideal.","dd934b56":"## Categorical values\n\nOkay, we've reduced memory, but we still need to deal with categorical values. As machine learning algorithms don't understand things like strings, we need to convert them into numbers. This is called encoding.\n\nWe could use either label encoding, which replaces each category into a numerical representation, or use one hot encoding which creates a separate column for each category. In general, one hot encoding performs better. However, in this case we have columns with very high cardinality -- and since we have a large dataset, it's probably more practical to use label encoding which we'll do.\n\nTwo things we need to deal with for label encoding are missing values and unknown values. Missing values means the data is simply not there, whereas unknown values are values in the test set that we don't have in the training set.\n\nFor missing values, we'll just replace them with a label, e.g. the string `\"missing\"`. That's pretty straightforward.\n\nFor unknown values, that requires a bit more thought. The main question is whether or not we have all the categories a priori. If we know all the possible categories beforehand (i.e. fixed categories like gender, state, postcodes) then we can go ahead and devise a mapping beforehand for all possible values. However, sometimes categories only come in the future, like mobile phone models. In the latter case, we have no way of knowing all the possible future values, and thus we can't map them -- so we'll need another strategy, i.e. replace them with a different label like the string `\"unknown\"`. We'll be doing that.","ecbcbb08":"First, we'll replace missing values with the string `\"missing\"` (we actually don't need to do this since pandas does it automatically, but I like to give it an explicit label, makes it easier to see).","e6c45b71":"## Training the model\n\nNow we can finally train a model. You can iterate on this part.","230036f9":"Then we'll convert the test set.","c41de9a0":"Now that we've removed all the `NaN` values, we can downcast the columns to the lowest precision.\n\nFirst we'll need to know which columns are integers, though! The following snippet does just that."}}