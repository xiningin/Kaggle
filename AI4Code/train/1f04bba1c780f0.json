{"cell_type":{"9b4a34f4":"code","9a186f1a":"code","fe6acf96":"code","759621f4":"code","6043c51b":"code","5155c51d":"code","025a00d0":"code","4a237b16":"code","3f735315":"code","d6064fd6":"code","a2279dfb":"code","e6400384":"code","d7ff85c3":"code","b201b11a":"code","7b263226":"code","c664395f":"code","34640268":"code","c42b2df5":"code","7c30c889":"code","f6029164":"code","26d13ef1":"code","fb9cb6e5":"code","23963a7d":"code","33b8bdd2":"code","bf3d91c5":"code","35719661":"code","39666edf":"code","79e55ee6":"code","d57f69c6":"code","1b2057de":"code","0e16af13":"code","3c990601":"code","e6d73025":"code","01d5f43b":"code","aa642af2":"code","f34e807d":"code","dd424058":"code","c662081c":"code","02ec3b76":"code","64317776":"code","25f67f37":"code","7c2e3e6c":"code","a9f8514e":"code","f32240d6":"code","8c3cc856":"code","50946b41":"code","8dcbf781":"code","c1877f45":"code","7f4b74cc":"code","668a6c34":"code","c46f8536":"code","26d364bf":"code","5a83dd25":"code","e358dc02":"code","f4291a98":"code","f479007a":"code","646111ec":"code","b48cf5d6":"code","802cf53c":"code","4e1fb211":"code","579020bc":"code","6892802b":"code","5e44614d":"code","226da4fa":"code","555a5337":"code","8572c362":"code","b98b89af":"code","34ee3e9a":"code","7fe55986":"markdown","b1742b23":"markdown","74b35cee":"markdown","931ac4d2":"markdown","6d5b9f2e":"markdown","91112848":"markdown","981b7faf":"markdown","a287616e":"markdown","e9ae496a":"markdown","1e1ecea2":"markdown","3f48df08":"markdown","e5927c02":"markdown","aced61d2":"markdown","585e6570":"markdown","84a6b91a":"markdown","23a9cf42":"markdown","a2afca13":"markdown","5a4b1325":"markdown","46d31cd7":"markdown","cf470aaf":"markdown","767acaaa":"markdown","f35490fd":"markdown","e6765d86":"markdown","397fc607":"markdown","e2b50e52":"markdown","4bdb2db7":"markdown","609f1a93":"markdown"},"source":{"9b4a34f4":"\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline \n\n## Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\n\n## Model evaluators\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import roc_curve;","9a186f1a":"df = pd.read_csv(\"\/kaggle\/input\/heart-disease-uci\/heart.csv\") # 'DataFrame' shortened to 'df'\ndf.shape # (rows, columns)","fe6acf96":"df.head()","759621f4":"\ndf.head(10)","6043c51b":"# No. of positive and negative patients in our samples\ndf.target.value_counts()","5155c51d":"# Normalized value counts\ndf.target.value_counts(normalize=True)","025a00d0":"# Plot the value counts with a bar graph\ndf.target.value_counts().plot(kind=\"bar\", color=[\"purple\", \"magenta\"]);","4a237b16":"df.info()","3f735315":"df.describe()","d6064fd6":"df.sex.value_counts()","a2279dfb":"pd.crosstab(df.target, df.age)","e6400384":"# Compare target column with sex column\npd.crosstab(df.target, df.sex)","d7ff85c3":"\npd.crosstab(df.target, df.sex).plot(kind=\"bar\", figsize=(10,6), color=[\"salmon\", \"lightblue\"])\n\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = Disease, 1 = No Disease\")\nplt.ylabel(\"Amount\")\nplt.legend([\"Female\", \"Male\"])\nplt.xticks(rotation=0);","b201b11a":"\nplt.figure(figsize=(10,6))\n\n# For positve examples\nplt.scatter(df.age[df.target==0], \n            df.thalach[df.target==0], \n            c=\"salmon\") # define it as a scatter figure\n\n# Now for negative examples, \nplt.scatter(df.age[df.target==1], \n            df.thalach[df.target==1], \n            c=\"lightblue\") \n\n\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.legend([\"Disease\", \"No Disease\"])\nplt.ylabel(\"Max Heart Rate\");","7b263226":"# Histograms to check age distribution \ndf.age.plot.hist();","c664395f":"pd.crosstab(df.cp, df.target)","34640268":"\npd.crosstab(df.cp, df.target).plot(kind=\"bar\", \n                                   figsize=(10,6), \n                                   color=[\"lightblue\", \"salmon\"])\n\n\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"Disease\", \"No disease\"])\nplt.xticks(rotation = 0);","c42b2df5":"# Find the correlation between our independent variables\ncorr_matrix = df.corr()\ncorr_matrix ","7c30c889":"\ncorr_matrix = df.corr()\nfig, ax=plt.subplots(figsize=(15, 15))\nax=sns.heatmap(corr_matrix, \n            annot=True, \n            linewidths=0.5, \n            fmt= \".2f\", \n            cmap=\"YlGnBu\");\nbottom, top=ax.get_ylim()\nax.set_ylim(bottom+0.5, top-0.5)","f6029164":"df.head()","26d13ef1":"# Everything except target variable\nX = df.drop(\"target\", axis=1)\n\n# Target variable\ny = df.target.values","fb9cb6e5":"# Independent variables (no target column)\nX.head()","23963a7d":"# Targets\ny","33b8bdd2":"\nnp.random.seed(42)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X,  \n                                                    y, \n                                                    test_size = 0.2) ","bf3d91c5":"X_train.head()","35719661":"y_train, len(y_train)","39666edf":"X_test.head()","79e55ee6":"y_test, len(y_test)","d57f69c6":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(), \n          \"Random Forest\": RandomForestClassifier(), \"Decision Tree\":DecisionTreeClassifier()}\n\n# Create function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n\n    \n    np.random.seed(42)\n    \n    model_scores = {}\n    \n    for name, model in models.items():\n        \n        model.fit(X_train, y_train)\n        \n        model_scores[name] = model.score(X_test, y_test)*100\n    return model_scores","1b2057de":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores","0e16af13":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.plot.bar();","3c990601":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Different RandomForestClassifier hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","e6d73025":"\nnp.random.seed(42)\n\n\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n\nrs_log_reg.fit(X_train, y_train);","01d5f43b":"rs_log_reg.best_params_","aa642af2":"rs_log_reg.score(X_test, y_test)","f34e807d":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model\nrs_rf.fit(X_train, y_train);","dd424058":"\nrs_rf.best_params_","c662081c":"\nrs_rf.score(X_test, y_test)","02ec3b76":"\nlog_reg_grid = {\"penalty\" :['l2'],\n\"C\":np.logspace(-4,4,30),\n\"class_weight\":[{1:0.5, 0:0.5}, {1:0.4, 0:0.6}, {1:0.6, 0:0.4}, {1:0.7, 0:0.3}],\n\"solver\": ['liblinear', 'saga','sag','newton-cg','lbfgs'],\"max_iter\":[10] }\n\n\n\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n\ngs_log_reg.fit(X_train, y_train);","64317776":"# Check the best parameters\ngs_log_reg.best_params_","25f67f37":"# Evaluate the model\ngs_log_reg.score(X_test, y_test)","7c2e3e6c":"from sklearn.ensemble import AdaBoostClassifier\nadaboost=AdaBoostClassifier(base_estimator=LogisticRegression(C= 2.592943797404667,\n class_weight= {1: 0.5, 0: 0.5},\n max_iter= 10,\n penalty= 'l2',\n solver= 'liblinear'),n_estimators=100)\nadaboost.fit(X_train,y_train)\nadaboost.score(X_test,y_test)\n\n","a9f8514e":"from sklearn.metrics import roc_curve\n\ny_preds = gs_log_reg.predict(X_test)","f32240d6":"y_preds","8c3cc856":"y_test","50946b41":"y_probs=gs_log_reg.predict_proba(X_test)\ny_probs_positive=y_probs[:,1]\ny_probs_positive","8dcbf781":"\nfrom sklearn.metrics import auc\n\nfpr, tpr, thresholds= roc_curve(y_test, y_probs_positive)\ndef plot_roc_curve(fpr,tpr):\n plt.plot(fpr, tpr, color=\"orange\",label=\"ROC\")\n plt.plot([0,1],[0,1],color=\"darkblue\",linestyle=\"--\",label=\"Guessing\")\n plt.xlabel(\"False positive rate\")\n plt.ylabel(\"True positive rate\")\n plt.title(\"Receiver Operating Characterisitics curve\")\n plt.legend()\n plt.show()\nplot_roc_curve(fpr,tpr)\n","c1877f45":"roc_auc=auc(fpr,tpr)\nroc_auc","7f4b74cc":"# Display confusion matrix\nprint(confusion_matrix(y_test, y_preds))","668a6c34":"\nimport seaborn as sns\nsns.set(font_scale=1.5) \ndef plot_conf_mat(y_test, y_preds):\n    \n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, # Annotate the boxes\n                     cbar=True)\n    plt.xlabel(\"true label\")\n    plt.ylabel(\"predicted label\")\n    bottom,top=ax.get_ylim()\n    ax.set_ylim(bottom+0.5, top-0.5)\n    \nplot_conf_mat(y_test, y_preds)","c46f8536":"prec=precision_score(y_test,y_preds)\nprec","26d364bf":"rec=recall_score(y_test,y_preds)\nrec","5a83dd25":"# Show classification report\nprint(classification_report(y_test, y_preds))","e358dc02":"rs_rf.best_params_\n","f4291a98":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nrf=RandomForestClassifier(n_estimators=560,\n min_samples_split=12,\n min_samples_leaf=15,\n max_depth=3)\ncv_acc=np.mean(cross_val_score(rf,X,y,cv=5,scoring=\"accuracy\"))\ncv_prec=np.mean(cross_val_score(rf,X,y,cv=5,scoring=\"precision\"))\ncv_recall=np.mean(cross_val_score(rf,X,y,cv=5,scoring=\"recall\"))\ncv_f1=np.mean(cross_val_score(rf,X,y,cv=5,scoring=\"f1\"))\ncv_acc,cv_prec,cv_recall,cv_f1\n\n","f479007a":"cv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                            \"Precision\": cv_prec,\n                            \"Recall\": cv_recall,\n                            \"F1\": cv_f1},\n                          index=[0])\ncv_metrics.T.plot.bar(title=\"Random Forest Cross-Validated Metrics\", legend=False);","646111ec":"\ngs_log_reg.best_params_","b48cf5d6":"\nfrom sklearn.model_selection import cross_val_score\n\nclf = LogisticRegression(C=0.23357214690901212,\n                         solver=\"liblinear\")","802cf53c":"\ncv_acc = cross_val_score(clf,\n                         X,\n                         y,\n                         cv=5, \n                         scoring=\"accuracy\")","4e1fb211":"cv_acc = np.mean(cv_acc)\ncv_acc","579020bc":"# Cross-validated precision score\ncv_precision = np.mean(cross_val_score(clf,\n                                       X,\n                                       y,\n                                       cv=5,\n                                       scoring=\"precision\")) \ncv_precision","6892802b":"\ncv_recall = np.mean(cross_val_score(clf,\n                                    X,\n                                    y,\n                                    cv=5, \n                                    scoring=\"recall\")) \ncv_recall","5e44614d":"\ncv_f1 = np.mean(cross_val_score(clf,\n                                X,\n                                y,\n                                cv=5, \n                                scoring=\"f1\")) \ncv_f1","226da4fa":"# Visualizing cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cv_acc,\n                            \"Precision\": cv_precision,\n                            \"Recall\": cv_recall,\n                            \"F1\": cv_f1},\n                          index=[0])\ncv_metrics.T.plot.bar(title=\"Logistic Regression Cross-Validated Metrics\", legend=False);","555a5337":"\nclf.fit(X_train, y_train);","8572c362":"# Check feature importance\nclf.coef_","b98b89af":"# Match features to columns\nfeatures_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeatures_dict","34ee3e9a":"# Visualize feature importance\nfeatures_df = pd.DataFrame(features_dict, index=[0])\nfeatures_df.T.plot.bar(title=\"Feature Importance\", legend=False);","7fe55986":"\n\nSince there are about 100 women and 72 of them have a postive value of heart disease being present, we might infer, based on this one variable if the participant is a woman, there's a 75% chance she does not have heart disease.\n\nAs for males, there's about 200 total with around half indicating a presence of heart disease. So we might predict, if the participant is male, 50% of the time he will have heart disease.\n","b1742b23":"We first use `RandomizedSearchCV` to try and tune our `LogisticRegression` model.\n\n","74b35cee":"Beautiful, we can see we're using 242 samples to train on. Let's look at our test data.","931ac4d2":"From the above visualization, it is clear that males are at a higher risk of having heart disease with more than 50 percent of the included patients having the disease.","6d5b9f2e":"### Age vs Max Heart rate for Heart Disease\n\n","91112848":"All our predictions is picked up by the model. As sex variable decreases towards 0 (female), target variable points towards 1 ( no disease). As max heart rate increases, probability of having heart disease decreases.","981b7faf":"And we've got 61 examples we'll test our model(s) on. Let's build some.","a287616e":"### Model choices\n\nNow we've got our data prepared, we can start to fit models. We'll be using the following and comparing their results.\n\n1. Logistic Regression - [`LogisticRegression()`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)\n2. K-Nearest Neighbors - [`KNeighboursClassifier()`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)\n3. RandomForest - [`RandomForestClassifier()`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)","e9ae496a":"Since these two values are nearly equal, our `target` column is **balanced**. An **unbalanced** target column, having different number of counts in each label, can be harder to model than a balanced set.\n","1e1ecea2":"Because Logistic Regression is returning a better accuracy score, I use GridSearchCV\non it.","3f48df08":"# Model Comparison\n\n","e5927c02":"# Problem Definition\n\n\n> Given clinical parameters about a patient, can we predict whether or not they have heart disease?\n\n\n\nThe following is the data dictionary for the dataset.\n\n1. age - age in years \n2. sex - (1 = male; 0 = female) \n3. cp - chest pain type \n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\n    * anything above 130-140 is typically cause for concern\n5. chol - serum cholestoral in mg\/dl \n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) \n    * '>126' mg\/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    * 0: Nothing to note\n    * 1: ST-T Wave abnormality\n        - can range from mild symptoms to severe problems\n        - signals non-normal heart beat\n    * 2: Possible or definite left ventricular hypertrophy\n        - Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved \n9. exang - exercise induced angina (1 = yes; 0 = no) \n10. oldpeak - ST depression induced by exercise relative to rest \n    * looks at stress of heart during excercise\n    * unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    * 0: Upsloping: better heart rate with excercise (uncommon)\n    * 1: Flatsloping: minimal change (typical healthy heart)\n    * 2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy \n    * colored vessel means the doctor can see the blood passing through\n    * the more blood movement the better (no clots)\n13. thal - thalium stress result\n    * 1,3: normal\n    * 6: fixed defect: used to be defect but ok now\n    * 7: reversable defect: no proper blood movement when excercising \n14. target - have disease or not (1=no heart disease, 0=heart disease) (= the predicted attribute)\n\n","aced61d2":"# Evaluating other metrics","585e6570":"I'll do the same for `RandomForestClassifier`.","84a6b91a":"**It is important to note that 0 implies heart disease while 1 implies no heart disease. Hence from the confusion matrix, TPR is not 0.87 but 0.89. Similarly FPR is not 0.11 but 0.12. Similarly, ","23a9cf42":"### Heart Disease Frequency per Chest Pain Type\n","a2afca13":"There are 207 males and 96 females in our study.","5a4b1325":"# Predicting Heart Disease using Machine Learning\n\n","46d31cd7":"It is not a perfect normal distribution but sways to the right.","cf470aaf":"\n\nFor patients without heart disease, it seems the younger someone is, the higher their max heart rate (dots are higher on the left of the graph and decreases somewhat linearly), and it decreases with age. There is no such fixed pattern in patients with heart disease.\n","767acaaa":"Maximum heart rate (thalach) and chest pain type have high positive correlation with having no heart disease. (remember 1= no heart disease). ","f35490fd":"# Modeling\n\n","e6765d86":"Since KNN, Decision Tree, AdaBoost gives relatively quite low accuracy value I decide to ignore them.\n\n### Tuning models with RandomizedSearchCV\n\n","397fc607":"## Using AdaBoost Classifier","e2b50e52":"\n\nRemember from our data dictionary what the different levels of chest pain are.\n\n3. cp - chest pain type \n    * 0: Typical angina: chest pain related decrease blood supply to the heart\n    * 1: Atypical angina: chest pain not related to heart\n    * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n    * 3: Asymptomatic: chest pain not showing signs of disease\n\nThe bargraph validates the data dictionary.","4bdb2db7":"# Age Vs Sex for heart disease and other crosstabs","609f1a93":"# Data Exploration (exploratory data analysis or EDA)"}}