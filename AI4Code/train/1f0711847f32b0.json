{"cell_type":{"4f2c6aee":"code","128a9fb4":"code","01d79fc8":"code","c8b24abf":"code","ecde4121":"code","29743cf5":"code","1895c2d4":"code","8cb590d0":"code","2cbcd2c8":"code","af02f934":"code","9bd85c9a":"code","accc0103":"code","48aa8ca4":"code","c82aad3b":"code","4f49ca87":"code","1adbf61d":"code","43d9efc8":"code","f0bfde37":"code","597aa56a":"code","e02e29ce":"code","3d595b20":"code","8f004b71":"code","c0557c03":"code","06fe4b36":"code","05037abc":"code","dddf6425":"code","83f459bc":"code","4f888a3c":"code","de5e4431":"code","b799d174":"code","7032fecb":"code","7c5023f5":"code","d8ae4fbd":"code","5d8cb3e9":"code","416d169c":"code","4ade3014":"code","7b3c9c99":"code","b3be2eca":"code","032036bc":"code","1f862ff7":"code","0170e08c":"code","e8436d9e":"code","ad02769a":"code","a3e34abc":"markdown","0ef78efe":"markdown","e85e3086":"markdown","9c82dc69":"markdown","853e00d9":"markdown","07675e43":"markdown","d27d4bb0":"markdown","07b7b54c":"markdown","51982701":"markdown","487d5f5a":"markdown","eb2ba0be":"markdown","80da0d66":"markdown","2665ef14":"markdown","dec893f2":"markdown","556371f5":"markdown","bc4b54ce":"markdown","4531c309":"markdown","9b6c3ad3":"markdown","a7fe88d0":"markdown","7d658494":"markdown","2ac323b7":"markdown","9a580b45":"markdown","1adeadee":"markdown","99fcea35":"markdown","016a78a6":"markdown","e8a10179":"markdown","28c784c8":"markdown","ef6a9d74":"markdown","8c36f929":"markdown","fcf32a1f":"markdown","7eb661c5":"markdown"},"source":{"4f2c6aee":"# Importing libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection  import SelectKBest,chi2\nfrom sklearn.metrics import confusion_matrix,classification_report,roc_auc_score,accuracy_score\n\nimport warnings\nwarnings.filterwarnings('ignore')","128a9fb4":"data = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndata.head()","01d79fc8":"data.info()","c8b24abf":"data.describe()","ecde4121":"# Searching for duplicate values.\nduplicate = data[data.duplicated()]\nif duplicate.shape[0] == 0 :\n    print(\"Dataset has no duplicate values.\")\nelse :\n    print(\"Dropping duplicates in data.\")\n    data.drop_duplicates(inplace=True)\n    print(\"Duplicate Rows :\")\n    duplicate","29743cf5":"# Checking is there's missing values\ndata.isnull().sum()","1895c2d4":"corr = data.corr()\nplt.figure(figsize=(10, 6))\nsns.heatmap(corr, annot=True,cmap='Blues')\nprint(\"Correlation Plot of the Heat Failure Prediction\")","8cb590d0":"print(\"The study on this dat is done on the people between {0} and {1} of age.\".format(data['Age'].min(),data['Age'].max()))","2cbcd2c8":"plt.figure(figsize=(15,10))\nfor i,col in enumerate(data.columns,1):\n    plt.subplot(4,3,i)\n    plt.title(f\"Distribution of {col} Data\")\n    sns.histplot(data[col],kde=True)\n    plt.tight_layout()\n    plt.plot()","af02f934":"plt.title(f\"Distribution of Age\")\nsns.histplot(data[\"Age\"],kde=True)\nplt.tight_layout()\nplt.plot()","9bd85c9a":"print(\"Sex Ratio in Data\")\nprint(data[\"Sex\"].value_counts())\n\nplt.pie(data[\"Sex\"].value_counts(), \n        labels = [\"male\", \"female\"], \n        colors = [\"b\",\"hotpink\"], \n        autopct='%1.1f%%', \n        explode = [0,0.2], \n        startangle = 180,\n        shadow = True)\n\nplt.legend()\nplt.show() ","accc0103":"# Shows the Distribution of Heat Diseases with respect to male and female\nfig = px.histogram(data, \n                 x=\"HeartDisease\",\n                 color=\"Sex\",\n                 hover_data=data.columns,\n                 title=\"Distribution of Heart Diseases\",\n                 barmode=\"group\")\nfig.show()","48aa8ca4":"night_colors = ['#F9B1B8',  '#EE4355',  '#B60618','#820815']\nvalues = data.ChestPainType.value_counts()\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=[x for x in data.ChestPainType.value_counts().index],\n                             values=values, \n                             hole=.3,\n                             pull=[0, 0, 0.06, 0])])\n\nfig.update_layout(\n    title_text=\"Chest pain type \")\nfig.update_traces(marker=dict(colors=night_colors))\nfig.show()","c82aad3b":"fig=px.histogram(data,\n                 x=\"ChestPainType\",\n                 color=\"Sex\",\n                 hover_data=data.columns,\n                 title=\"Types of Chest Pain\"\n                )\nfig.show()","4f49ca87":"fig=px.histogram(data,\n                 x=\"RestingECG\",\n                 hover_data=data.columns,\n                 title=\"Distribution of Resting ECG\")\nfig.show()","1adbf61d":"sns.pairplot(data,diag_kind='kde',hue=\"HeartDisease\",corner = 'True')","43d9efc8":"print(data[\"HeartDisease\"].value_counts())\ndata[\"HeartDisease\"].value_counts().plot(kind=\"pie\", \n                                         autopct='%1.1f%%', \n                                         figsize=(6,6))\nprint(r\"Pie Chart with target variable 'Heart Disease' in Data\")","f0bfde37":"fig = px.box(data,y=\"Age\",x=\"HeartDisease\",title=f\"Distrubution of Age\")\nfig.show()","597aa56a":"fig = px.box(data,y=\"RestingBP\",x=\"HeartDisease\",title=f\"Distrubution of RestingBP\",color=\"Sex\")\nfig.show()","e02e29ce":"fig = px.box(data,y=\"Cholesterol\",x=\"HeartDisease\",title=f\"Distrubution of Cholesterol\")\nfig.show()","3d595b20":"fig = px.box(data,y=\"Oldpeak\",x=\"HeartDisease\",title=f\"Distrubution of Oldpeak\")\nfig.show()","8f004b71":"fig = px.box(data,y=\"MaxHR\",x=\"HeartDisease\",title=f\"Distrubution of MaxHR\")\nfig.show()","c0557c03":"# Making copy of orignal data to work on\ndata_w = data","06fe4b36":"numerical = data_w.drop(['HeartDisease'], axis=1).select_dtypes('number').columns\ncategorical = data_w.select_dtypes('object').columns\nprint(\"Numerical features in data are : {}\".format(numerical))\nprint(\"\\nCategorical features in data are : {}\".format(categorical))","05037abc":"data_w.info()","dddf6425":"# Checking for NULLs in the data\ndata_w.isnull().sum()","83f459bc":"# Getting numerical columns\nnumerical_col=[]\nfor column in data.columns:\n    if((data_w[column].dtype!='object') & (len(data_w[column].unique())>2)):\n        numerical_col.append(column)\nnumerical_col = pd.DataFrame(data_w[numerical_col])\nnumerical_col ","4f888a3c":"# Using Min-max scaling\nfor column in numerical_col:\n    data_w[column]=MinMaxScaler().fit_transform(data_w[column].values.reshape(-1,1))\ndata_w","de5e4431":"# lets first differentiate the categorical columns\ncategorical_col=data_w.drop(numerical_col,axis=1)\ncategorical_col","b799d174":"# Using label encoder\nprint(\"Encoding categorical variables : \")\nfor column in categorical_col.drop('HeartDisease',axis=1) :\n    data_w[column]=preprocessing.LabelEncoder().fit_transform(data_w[column])\ndata_w","7032fecb":"data_w.corr()","7c5023f5":"# Correlation matrix for all the columns in data\nplt.figure(figsize=(15,10))\nsns.heatmap(data_w.corr(),annot=True,cmap = 'Blues')\nplt.show()","d8ae4fbd":"best=SelectKBest(chi2,k=8)\nbest.fit(data_w.drop('HeartDisease',axis=1),data_w['HeartDisease'])","5d8cb3e9":"best.scores_","416d169c":"mask = best.get_support()\nselected_col = data_w.drop('HeartDisease',axis=1).columns[mask]\nselected_col","4ade3014":"from sklearn.model_selection import train_test_split\nX = data_w[selected_col]\ny = data_w['HeartDisease']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","7b3c9c99":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators=50)\nrfc.fit(X_train, y_train)\n\nytrain_p = rfc.predict(X_train)\nytest_p = rfc.predict(X_test)\n\nprint('Using total {} features with Random Forest Classifier: '.format(X_train.shape[1]))\n\nprint('Training Accuracy: {}%'.format(round(accuracy_score(y_train, ytrain_p)*100,2)))#, normalize=False))\nprint('Test accuracy: {}%'.format(round(accuracy_score(y_test, ytest_p)*100,2)))#, normalize=False))\n\nprint(\"\\nTrain classification report : \\n\",classification_report(y_train, ytrain_p))\nprint(\"Test classification report : \\n\",classification_report(y_test, ytest_p))","b3be2eca":"confusion_matrix = pd.crosstab(y_test, ytest_p, \n                               rownames=['Actual'], colnames=['Predicted'])\n                               #,margins = True)\nplt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix\/np.sum(confusion_matrix), annot=True, \n            fmt='.2%', cmap='Blues')\nplt.show()","032036bc":"# Using Logistic Regression for classification\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\nytrain_p = logreg.predict(X_train)\nytest_p = logreg.predict(X_test)\n\nprint('Using total {} features with Logistic Regressor Classifier : '.format(X_train.shape[1]))\n\nprint('Training Accuracy: {}%'.format(round(accuracy_score(y_train, ytrain_p)*100,2)))#, normalize=False))\nprint('Test accuracy: {}%'.format(round(accuracy_score(y_test, ytest_p)*100,2)))#, normalize=False))\n\nprint(\"\\nTrain classification report : \\n\",classification_report(y_train, ytrain_p))\nprint(\"Test classification report : \\n\",classification_report(y_test, ytest_p))\n\nconfusion_matrix = pd.crosstab(y_test, ytest_p, \n                               rownames=['Actual'], colnames=['Predicted'])\n                               #,margins = True)\nplt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix\/np.sum(confusion_matrix), annot=True, \n            fmt='.2%', cmap='Blues')\nplt.show()","1f862ff7":"# Using KNN Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=4)\nknn.fit(X_train, y_train)\n\nytrain_p = knn.predict(X_train)\nytest_p = knn.predict(X_test)\n\nprint('Using total {} features with K-NN Classifier : '.format(X_train.shape[1]))\n\nprint('Training Accuracy: {}%'.format(round(accuracy_score(y_train, ytrain_p)*100,2)))#, normalize=False))\nprint('Test accuracy: {}%'.format(round(accuracy_score(y_test, ytest_p)*100,2)))#, normalize=False))\n\nprint(\"\\nTrain classification report : \\n\",classification_report(y_train, ytrain_p))\nprint(\"Test classification report : \\n\",classification_report(y_test, ytest_p))\n\nconfusion_matrix = pd.crosstab(y_test, ytest_p, \n                               rownames=['Actual'], colnames=['Predicted'])\n                               #,margins = True)\nplt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix\/np.sum(confusion_matrix), annot=True, \n            fmt='.2%', cmap='Blues')\nplt.show()","0170e08c":"# Using Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier()\nclf = clf.fit(X_train,y_train)\n\nytrain_p = clf.predict(X_train)\nytest_p = clf.predict(X_test)\n\nprint('Using total {} features with Decision Tree Classifier : '.format(X_train.shape[1]))\n\nprint('Training Accuracy: {}%'.format(round(accuracy_score(y_train, ytrain_p)*100,2)))#, normalize=False))\nprint('Test accuracy: {}%'.format(round(accuracy_score(y_test, ytest_p)*100,2)))#, normalize=False))\n\nprint(\"\\nTrain classification report : \\n\",classification_report(y_train, ytrain_p))\nprint(\"Test classification report : \\n\",classification_report(y_test, ytest_p))\n\nconfusion_matrix = pd.crosstab(y_test, ytest_p, \n                               rownames=['Actual'], colnames=['Predicted'])\n                               #,margins = True)\nplt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix\/np.sum(confusion_matrix), annot=True, \n            fmt='.2%', cmap='Blues')\nplt.show()","e8436d9e":"#Using SVM for classification\nfrom sklearn import svm\n\nsv = svm.SVC(C=2, gamma = 'auto')\nsv.fit(X_train,y_train)\n\nytrain_p = sv.predict(X_train)\nytest_p = sv.predict(X_test)\n\nprint('Using total {} features with SVM Classifier : '.format(X_train.shape[1]))\n\nprint('Training Accuracy: {}%'.format(round(accuracy_score(y_train, ytrain_p)*100,2)))#, normalize=False))\nprint('Test accuracy: {}%'.format(round(accuracy_score(y_test, ytest_p)*100,2)))#, normalize=False))\n\nprint(\"\\nTrain classification report : \\n\",classification_report(y_train, ytrain_p))\nprint(\"Test classification report : \\n\",classification_report(y_test, ytest_p))\n\nconfusion_matrix = pd.crosstab(y_test, ytest_p, \n                               rownames=['Actual'], colnames=['Predicted'])\n                               #,margins = True)\nplt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix\/np.sum(confusion_matrix), annot=True, \n            fmt='.2%', cmap='Blues')\nplt.show()","ad02769a":"# Using Naive Bayes for Classification\nfrom sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\ngnb = gnb.fit(X_train, y_train)\n\nytrain_p = gnb.predict(X_train)\nytest_p = gnb.predict(X_test)\n\nprint('Using total {} features with SVM Classifier : '.format(X_train.shape[1]))\n\nprint('Training Accuracy: {}%'.format(round(accuracy_score(y_train, ytrain_p)*100,2)))#, normalize=False))\nprint('Test accuracy: {}%'.format(round(accuracy_score(y_test, ytest_p)*100,2)))#, normalize=False))\n\nprint(\"\\nTrain classification report : \\n\",classification_report(y_train, ytrain_p))\nprint(\"Test classification report : \\n\",classification_report(y_test, ytest_p))\n\nconfusion_matrix = pd.crosstab(y_test, ytest_p, \n                               rownames=['Actual'], colnames=['Predicted'])\n                               #,margins = True)\nplt.figure(figsize=(8,6))\nsns.heatmap(confusion_matrix\/np.sum(confusion_matrix), annot=True, \n            fmt='.2%', cmap='Blues')\nplt.show()","a3e34abc":"<h3>Using Naive Bayes","0ef78efe":"<a id=\"5\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Outliers<\/h2>\n\nA box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables.The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution.The box plot (a.k.a. box and whisker diagram) is a standardized way of displaying the distribution of data based on the five number summary:<br>\n\n- Minimum\n- First quartile\n- Median\n- Third quartile\n- Maximum.\n\nIn the simplest box plot the central rectangle spans the first quartile to the third quartile (the interquartile range or IQR).A segment inside the rectangle shows the median and \u201cwhiskers\u201d above and below the box show the locations of the minimum and maximum.<br>","e85e3086":"<a id=\"9\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Prediction (with all classifiers)<\/h2>","9c82dc69":"So we can see our data does not have any null values but in case we have missing values, we can remove the data as well.<br>","853e00d9":"<a id=\"6.1\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Handling Null Values<\/h2>\n\nIn any real-world dataset, there are always few null values. It doesn\u2019t really matter whether it is a regression, classification or any other kind of problem, no model can handle these NULL or NaN values on its own so we need to intervene.<br>\n\nIn python NULL is reprsented with NaN. So don\u2019t get confused between these two,they can be used interchangably.<br>","07675e43":"# Heart Failure Prediction\n\nWe are implementing Heart Failure Prediction machine learning problem in this notebook.<br>\nUsing the dataset available [here](https:\/\/www.kaggle.com\/fedesoriano\/heart-failure-prediction), we will classify whether the patient face heart failure or not.<br>\n","d27d4bb0":"<a id=\"6\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Data Preprocessing<\/h2>\n\nData preprocessing is an integral step in Machine Learning as the quality of data and the useful information that can be derived from it directly affects the ability of our model to learn; therefore, it is extremely important that we preprocess our data before feeding it into our model.<br>\n\nThe concepts that I will cover in this article are :<br>\n\n- [Handling Null Values](#6.1)\n- [Feature Scaling](#6.2)\n- [Handling Categorical Variables](#6.3)\n","07b7b54c":"<a id=\"2\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Data<\/h2>\n\n<a href=\"#toc\" aria-pressed=\"true\" data-toggle=\"popover\">Table of Contents<\/a>\n\n**Age:** age of the patient [years]\n\n**Sex:** sex of the patient [M: Male, F: Female]\n\n**ChestPainType:** chest pain type [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n\n**RestingBP:** resting blood pressure [mm Hg]\n\n**Cholesterol:** serum cholesterol [mm\/dl]\n\n**FastingBS:** fasting blood sugar [1: if FastingBS > 120 mg\/dl, 0: otherwise]\n\n**RestingECG:** resting electrocardiogram results [Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n\n**MaxHR:** maximum heart rate achieved [Numeric value between 60 and 202]\n\n**ExerciseAngina:** exercise-induced angina [Y: Yes, N: No]\n\n**Oldpeak:** oldpeak = ST [Numeric value measured in depression]\n\n**ST_Slope:** the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n\n**HeartDisease:** output class [1: heart disease, 0: Normal]","51982701":"<h3> Using Decision Tree","487d5f5a":"<a id=\"toc\"><\/a>\n\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Table of Contents<\/h2>\n\n- [ Preface](#0)<br>\n- [ Context](#1)<br>\n- [ Data](#2)<br>\n- [ Importing Data](#3)<br>\n- [ Data Exploration](#4)<br>\n- [Correlation Matrix](#4.1)<br>\n- [ Outliers](#5)<br>\n- [ Data Processing](#6)<br>\n- [ Feature Selection](#7)<br>\n- [ Splitting Data](#8)<br>\n- [ Prediction (with all classifiers)](#9)<br>\n- [ Conclusion](#10)<br>","eb2ba0be":"<h3>Using Logistic Regression","80da0d66":"<h3>Using Random Forest Classifier","2665ef14":"<a id=\"3\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Importing... the dataset<\/h2>\n<a href=\"#toc\" aria-pressed=\"true\" data-toggle=\"popover\">Table of Contents<\/a>","dec893f2":"<a id=\"4.1\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Correlation Matrix<\/h2>\n\nIts necessary to remove correlated variables to improve your model.One can find correlations using pandas \u201c.corr()\u201d function and can visualize the correlation matrix using plotly express.<br>\n- Lighter shades represents positive correlation<br>\n- Darker shades represents negative correlation<br>","556371f5":"<a id=\"10\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Conclusion<\/h2>","bc4b54ce":"<h3> Using SVM","4531c309":"Lets check the correlation to get the idea of what features to select for model creation","9b6c3ad3":"<a id=\"8\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Splitting Data<\/h2>\n","a7fe88d0":"<h3>\ud83d\udee0 Work in progress... If you liked it so far, please don't forget to comment and upvote. Thank you!<br>\nAll the best\ud83e\udd18<br>","7d658494":"<h3>Using K-NN","2ac323b7":"Categorical variables\/features are any feature type can be classified into two major types:<br>\n\n- Nominal<br>\n\nNominal variables are variables that have two or more categories which do not have any kind of order associated with them. For example, if gender is classified into two groups, i.e. male and female, it can be considered as a nominal variable.<br>\n\n- Ordinal<br>\n\nOrdinal variables, on the other hand, have \u201clevels\u201d or categories with a particular order associated with them. For example, an ordinal categorical variable can be a feature with three different levels: low, medium and high. Order is important.<br>","9a580b45":"<h4>In this notebook, we have made a gentle classification using visuals to analyse the data, labelencoder to encode categorical variables, min-max scaler to scale numerical features and different classifers for prediction.<br>","1adeadee":"Now to check the linearity of the variables it is a good practice to plot distribution graph and look for skewness of features. Kernel density estimate (kde) is a quite useful tool for plotting the shape of a distribution.","99fcea35":"<a id=\"4\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Data Exploration<\/h2>\n","016a78a6":"<a id=\"6.2\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Feature Scaling<\/h2>\n","e8a10179":"<a id=\"6.3\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Handling Categorical Features<\/h2>","28c784c8":"<a id=\"7\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Feature Selection<\/h2>\n\n\n- Feature selection using Kbest selector and chi2 test (Selecting 8 best features out of all the features)<br>","ef6a9d74":"- We can see from the evaluated accuracies of all the classifiers that Logistic Regression and SVM are classifying best for this dataset.<br>\n- The accuracy can be increased with hyperparameter tuning.<br>\n","8c36f929":"- Surprisingly, cholesterol has negative correlation with the target variable HeartDisease.<br>\nIt should be highly correlated with the HeartDisease according to the studies.<br>","fcf32a1f":"<a id=\"1\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Context<\/h2>\n\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.<br>\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.<br>","7eb661c5":"<a id=\"0\"><\/a>\n<h2 data-toggle=\"list\" role=\"tab\" aria-controls=\"home\">Preface<\/h2>\n\nGiven a dataset of Heart Failure Prediction with 11 clinical features for predicting heart disease events."}}