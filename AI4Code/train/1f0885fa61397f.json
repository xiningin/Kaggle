{"cell_type":{"d2c57b2c":"code","c6bfff03":"code","c64ce87e":"code","08f9b0c1":"code","f9265bf7":"code","4317a0b4":"code","d9555d5e":"code","5345c623":"code","1b6d91fa":"code","c8ac3226":"code","72eec632":"code","6ed87efc":"code","3230c060":"code","9b085255":"code","52f18df1":"code","9e6ab5af":"code","6092dbb9":"code","25cd994d":"code","1061fb91":"code","37349854":"code","4b949096":"code","5da1fb8a":"code","90757feb":"code","fd752dfe":"code","f8682a0b":"code","e75688ff":"code","430585a5":"code","440b768a":"code","40567e4e":"code","9cf69750":"code","9919e183":"code","1142f874":"code","9f7748b5":"code","ec80a218":"code","cdb22fce":"code","fca6b4fb":"code","99da4361":"code","5ce1c4e5":"code","e8c84029":"code","d49d4ea1":"code","b7762ac6":"code","128259e8":"code","bb854051":"code","c6a1f186":"code","d533b0db":"code","48c76b1e":"code","cda15a92":"code","b25f013f":"code","13a937c4":"code","e154e968":"code","8a704961":"code","61dc23aa":"code","93a6dd59":"code","e8dbcf68":"code","0ec9d903":"code","bb8f11b6":"code","0b1aee6c":"code","c3dcf11f":"code","cfbfc4a1":"code","e2020377":"code","7fea360d":"code","17bd4a0b":"code","cc5db09f":"code","b02909bc":"code","588cd091":"code","d76bf9e0":"code","92e4efc4":"code","e2232721":"code","3b4071f6":"code","d6dea618":"code","1158a4e1":"code","74b067a0":"markdown","8533ee37":"markdown","f6fc6cc6":"markdown","ddb3daf1":"markdown","d25d3ca1":"markdown","d542a431":"markdown","f89e49fc":"markdown","23cd60e0":"markdown","dd0b1a83":"markdown","ae2a312d":"markdown","ff6fae89":"markdown","5c5fd143":"markdown","d6ae9a06":"markdown","ee429e82":"markdown"},"source":{"d2c57b2c":"# Import packages\n# Basic packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Evaluation and bayesian optimization\nfrom math import floor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import make_scorer, accuracy_score, classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold\nfrom hyperopt import hp, fmin, tpe\nfrom bayes_opt import BayesianOptimization\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option(\"display.max_columns\", None)","c6bfff03":"# Make scorer: accuracy\naccuracy = make_scorer(accuracy_score)","c64ce87e":"# Load dataset\ntrainSet = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntestSet = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/test.csv')\nsubmitSet = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/sample_submission.csv')\n\ntrainSet.head()","08f9b0c1":"# missing values\ndesc = trainSet.describe()\ndesc = desc.loc['count', ]\ndesc = pd.DataFrame(desc).sort_values('count')\nprint(desc.head(10))\n\n# data structure\nobj = pd.DataFrame(trainSet.dtypes == object)\nobj = obj.loc[obj[0]==True,]\nprint(obj)","f9265bf7":"# Drop columns with lacking data\ntrain = trainSet.drop(columns=['Id','idhogar','rez_esc', 'v18q1', 'v2a1', 'dependency', 'edjefe', 'edjefa'])\n\n# Drop rows with missing values\ntrain = train.dropna(axis=0)\n\nprint(train.shape)\ntrain.head()","4317a0b4":"# train validation split for feature selection\nX_train0, X_val0, y_train, y_val = train_test_split(train.drop(columns=['Target'], axis=0),\n                                                  train['Target'],\n                                                  test_size=0.2, random_state=123,\n                                                  stratify=train['Target'])","d9555d5e":"import xgboost as xgb\n\n# Feature selection with XGBoost\nselection =  xgb.XGBClassifier(random_state=123, nthread=-1)\nselection.fit(X_train0, y_train)\nsele_pred = selection.predict(X_val0)\nprint('Accuracy: ' + str(accuracy_score(y_val, sele_pred)))","5345c623":"print(classification_report(y_val, sele_pred))\nprint(confusion_matrix(y_val, sele_pred))","1b6d91fa":"# Feature importances\nFeature_sel = pd.DataFrame({'feature':X_train0.columns,\n                            'importance':list(selection.feature_importances_)}).sort_values('importance').reset_index(drop=True)\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_sel, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","c8ac3226":"# Select features with high importance\nselected = ['tipovivi5', 'hogar_mayor', 'abastaguano', 'epared2', 'area1',\n       'tipovivi1', 'elimbasu2', 'refrig', 'mobilephone', 'energcocinar2',\n       'pisocemento', 'pareddes', 'elimbasu1', 'etecho2', 'lugar4',\n       'paredmad', 'paredzinc', 'lugar1', 'tamviv', 'lugar3',\n       'television', 'rooms', 'epared1', 'tipovivi3', 'etecho1',\n       'SQBedjefe', 'epared3', 'parentesco9', 'bedrooms', 'r4m2',\n       'overcrowding', 'sanitario5', 'paredzocalo', 'eviv1', 'paredpreb',\n       'etecho3', 'abastaguadentro', 'r4m3', 'techozinc', 'pisomadera',\n       'sanitario3', 'eviv2', 'tamhog', 'v14a', 'r4t2', 'public',\n       'lugar5', 'elimbasu3', 'r4m1', 'hacdor', 'r4t3', 'energcocinar4',\n       'r4h1', 'sanitario2', 'hogar_adul', 'r4h2', 'cielorazo',\n       'qmobilephone', 'tipovivi4', 'pisomoscer', 'meaneduc', 'tipovivi2',\n       'paredblolad', 'computer', 'r4t1', 'pisonotiene', 'SQBdependency',\n       'eviv3', 'hacapo', 'hogar_nin', 'v18q']","72eec632":"# train validation split\nX_train, X_val, y_train, y_val = train_test_split(train[selected], train['Target'],\n                                                  test_size=0.2, random_state=123,\n                                                  stratify=train['Target'])","6ed87efc":"# Scaling\nscaler = MinMaxScaler()\nX_trainS = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n\nX_valS = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)","3230c060":"from sklearn.linear_model import LogisticRegression","9b085255":"# Hyperparameter-tuning: Bayesian Optimization, hyperopt\nspace_log = {'penalty': hp.choice('penalty', ['l2', 'none']),\n             'C': hp.loguniform('C', np.log(0.01), np.log(1000)),\n             'fit_intercept': hp.choice('fit_intercept',[True, False]),\n             'solver':hp.choice('solver', ['newton-cg', 'lbfgs', 'sag', 'saga'])}\n\ndef log_cl_bo(params_log):\n    params_log = {'penalty': params_log['penalty'],\n                  'C': params_log['C'],\n                  'fit_intercept': params_log['fit_intercept'],\n                  'solver': params_log['solver']}\n    \n    log_bo = LogisticRegression(random_state=123, **params_log)\n    best_score = cross_val_score(log_bo, X_trainS, y_train, scoring=accuracy, cv=5).mean()\n    return 1 - best_score\n\nlog_best_param = fmin(fn=log_cl_bo,\n                space=space_log,\n                max_evals=20,\n                rstate=np.random.RandomState(42),\n                algo=tpe.suggest)","52f18df1":"# Best hyperparameters\nparams_log = log_best_param\npenaltyL = ['l2', 'none']\nfit_interceptL = [True, False]\nsolverL = ['newton-cg', 'lbfgs', 'sag', 'saga']\n\nparams_log['fit_intercept'] = fit_interceptL[round(params_log['fit_intercept'])]\nparams_log['penalty'] = penaltyL[round(params_log['penalty'])]\nparams_log['solver'] = solverL[round(params_log['solver'])]\nparams_log","9e6ab5af":"# Fit the training data\nlog_hyp =  LogisticRegression(**params_log, random_state=123)\nlog_hyp.fit(X_trainS, y_train)\n\n# Predict the validation data\npred_log = log_hyp.predict(X_valS)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_log)))","6092dbb9":"# Prediction Result\nprint('confusion_matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_log), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_log))","25cd994d":"# Features Coefficients\nFeature_log = pd.DataFrame({'features': list(X_trainS.columns), 'coefficient':list(log_hyp.coef_[0])}).sort_values('coefficient')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_log, x='features', y='coefficient')\nplt.xticks(rotation=90)\nplt.show()","1061fb91":"from sklearn.naive_bayes import GaussianNB","37349854":"# Hyperparameter-tuning: Grid Search\nvar_smoothing = [1e-11, 1e-10, 1e-9, 1e-8, 1e-7]\nparam_nb={'var_smoothing':var_smoothing}\nnb_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_nb, scoring=accuracy, cv=5)\n\nnb_grid.fit(X_train, y_train)\n\nprint('Best score: ' + str(nb_grid.best_score_))\nprint('Best parameter {}'.format(nb_grid.best_params_))","4b949096":"# Fit the training data\nnb_hyp = GaussianNB(var_smoothing=nb_grid.best_params_['var_smoothing'])\nnb_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_nb = nb_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_nb)))","5da1fb8a":"# Prediction Result\nprint('confusion_matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_nb), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_nb))","90757feb":"from sklearn.naive_bayes import BernoulliNB","fd752dfe":"# Hyperparameter-tuning: Grid Search\nparam_nbBer={'alpha':[0.2,0.4,0.6,0.8,1],\n             'fit_prior':[True, False]}\n\nnbBer_grid = GridSearchCV(estimator=BernoulliNB(), param_grid=param_nbBer, scoring=accuracy, cv=5)\n\nnbBer_grid.fit(X_train, y_train)\n\nprint('Best score: ' + str(nbBer_grid.best_score_))\nprint('Best parameter {}'.format(nbBer_grid.best_params_))","f8682a0b":"# Fit the training data\nnbBer_hyp = BernoulliNB(alpha=nbBer_grid.best_params_['alpha'], fit_prior=nbBer_grid.best_params_['fit_prior'])\nnbBer_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_nbBer = nbBer_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_nb)))","e75688ff":"# Prediction Result\nprint('confusion_matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_nbBer), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_nbBer))","430585a5":"from sklearn.neighbors import KNeighborsClassifier","440b768a":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef knn_cl_bo(n_neighbors, weights, p):\n    params_knn = {}\n    weightsL = ['uniform', 'distance']\n    \n    params_knn['n_neighbors'] = round(n_neighbors)\n    params_knn['weights'] = weightsL[round(weights)]\n    params_knn['p'] = round(p)\n    \n    score = cross_val_score(KNeighborsClassifier(**params_knn),\n                             X_trainS, y_train, cv=5, scoring=accuracy).mean()\n    return score\n\n# Set hyperparameters spaces\nparams_knn ={\n    'n_neighbors':(3, 20),\n    'weights':(0, 1),\n    'p':(1, 2)}\n\n# Run Bayesian Optimization\nknn_bo = BayesianOptimization(knn_cl_bo, params_knn, random_state=111)\nknn_bo.maximize(init_points=4, n_iter=25)","40567e4e":"# Best hyperparameters\nparams_knn = knn_bo.max['params']\nweightsL = ['uniform', 'distance']\nparams_knn['n_neighbors'] = round(params_knn['n_neighbors'])\nparams_knn['weights'] = weightsL[round(params_knn['weights'])]\nparams_knn['p'] = round(params_knn['p'])\nparams_knn","9cf69750":"# Fit the training data\nknn_hyp = KNeighborsClassifier(**params_knn)\nknn_hyp.fit(X_trainS, y_train)\n\n# Predict the validation data\npred_knn = knn_hyp.predict(X_valS)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_knn)))","9919e183":"# Prediction Result\nprint('confusion_matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_knn), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_knn))","1142f874":"from sklearn.svm import SVC","9f7748b5":"# Hyperparameter-tuning: Grid Search\nparams_svm = {'C':[0.01,0.1,1,10]}\nsvm_grid = GridSearchCV(estimator=SVC(), param_grid=params_svm,\n                         scoring=accuracy, cv=5)\n\n# Subset only 100 rows for training SVM for simplicity\n# SVM requires a very long time if the observations are too many.\nsvm_grid.fit(X_trainS.iloc[0:100,], y_train.iloc[0:100,])\n\nprint('Best score: ' + str(svm_grid.best_score_))\nprint('Best parameter {}'.format(svm_grid.best_params_))","ec80a218":"# Fit the training data\nsvm_hyp =  SVC(**svm_grid.best_params_, random_state=123)\nsvm_hyp.fit(X_trainS.iloc[0:100,], y_train.iloc[0:100,])\n\n# Predict the validation data\npred_svm = svm_hyp.predict(X_valS)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_svm)))","cdb22fce":"# Prediction Result\nprint('confusion_matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_svm), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_svm))\n# SVM is trained from too little observations. \n# The model is not yet well-trained.","fca6b4fb":"from sklearn.tree import DecisionTreeClassifier","99da4361":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef dt_cl_bo(criterion, splitter, max_depth, min_samples_split, min_samples_leaf):\n    params_dt = {}\n    criterionL = ['gini', 'entropy']\n    splitterL = ['best', 'random']\n    \n    params_dt['criterion'] = criterionL[round(criterion)]\n    params_dt['splitter'] = splitterL[round(splitter)]\n    params_dt['max_depth'] = round(max_depth)\n    params_dt['min_samples_split'] = round(min_samples_split)\n    params_dt['min_samples_leaf'] = round(min_samples_leaf)\n    \n    score = cross_val_score(DecisionTreeClassifier(random_state=123, **params_dt),\n                            X_train, y_train, scoring=accuracy, cv=5).mean()\n    return score\n\n# Set hyperparameters spaces\nparams_dt ={\n    'criterion':(0, 1),\n    'splitter':(0, 1),\n    'max_depth':(4, 15),\n    'min_samples_split':(2, 10),\n    'min_samples_leaf': (2, 10)\n}\n\n# Run Bayesian Optimization\ndt_bo = BayesianOptimization(dt_cl_bo, params_dt, random_state=123)\ndt_bo.maximize(init_points=4, n_iter=25)","5ce1c4e5":"# Best hyperparameters\nparams_dt = dt_bo.max['params']\n\ncriterionL = ['gini', 'entropy']\nsplitterL = ['best', 'random']\n\nparams_dt['criterion'] = criterionL[int(round(params_dt['criterion']))]\nparams_dt['splitter'] = splitterL[int(round(params_dt['splitter']))]\nparams_dt['max_depth'] = round(params_dt['max_depth'])\nparams_dt['min_samples_split'] = round(params_dt['min_samples_split'])\nparams_dt['min_samples_leaf'] = round(params_dt['min_samples_leaf'])\nparams_dt","e8c84029":"# Fit the training data\ndt_hyp =  DecisionTreeClassifier(**params_dt, random_state=123)\ndt_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_dt = dt_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_dt)))","d49d4ea1":"# Prediction Result\nprint('confusion_matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_dt), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_dt))","b7762ac6":"# Feature importances\nFeature_dt = pd.DataFrame({'feature':X_train.columns, 'importance':list(dt_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_dt, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","128259e8":"# Here is the example of tree with only 3 max_depth\n# THe optimum max-depth is 15. This tree is plotted with only 3 depth to make it simpler\nparams_dt2 = {'criterion': 'entropy', 'max_depth': 3, 'min_samples_leaf': 2,\n              'min_samples_split': 2, 'splitter': 'best'}\n\ndt_hyp2 =  DecisionTreeClassifier(**params_dt2, random_state=123)\ndt_hyp2.fit(X_train, y_train)\n\nplt.figure(figsize=(16,8))\nplot_tree(dt_hyp2, fontsize=10)\nplt.show()","bb854051":"from sklearn.ensemble import RandomForestClassifier","c6a1f186":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef rf_cl_bo(n_estimators, criterion, max_depth, min_samples_split, min_samples_leaf,):\n    params_rf = {}\n    criterionL = ['gini', 'entropy']\n    \n    params_rf['n_estimators'] = round(n_estimators)\n    params_rf['criterion'] = criterionL[round(criterion)]\n    params_rf['max_depth'] = round(max_depth)\n    params_rf['min_samples_split'] = round(min_samples_split)\n    params_rf['min_samples_leaf'] = round(min_samples_leaf)\n    \n    score = cross_val_score(RandomForestClassifier(random_state=123, **params_rf),\n                             X_train, y_train, scoring=accuracy, cv=5).mean()\n    return score\n\n# Set hyperparameters spaces\nparams_rf ={\n    'n_estimators':(70, 150),\n    'criterion':(0, 1),\n    'max_depth':(4, 20),\n    'min_samples_split':(2, 10),\n    'min_samples_leaf': (2, 10)\n}\n\n# Run Bayesian Optimization\nrf_bo = BayesianOptimization(rf_cl_bo, params_rf, random_state=111)\nrf_bo.maximize(init_points=4, n_iter=25)","d533b0db":"# Best hyperparameters\nparams_rf = rf_bo.max['params']\ncriterionL = ['gini', 'entropy']\n\nparams_rf['n_estimators'] = round(params_rf['n_estimators'])\nparams_rf['criterion'] = criterionL[int(round(params_rf['criterion']))]\nparams_rf['max_depth'] = round(params_rf['max_depth'])\nparams_rf['min_samples_split'] = round(params_rf['min_samples_split'])\nparams_rf['min_samples_leaf'] = round(params_rf['min_samples_leaf'])\nparams_rf","48c76b1e":"# Fit the training data\nrf_hyp =  RandomForestClassifier(**params_rf, random_state=123)\nrf_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_rf = rf_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_rf)))","cda15a92":"# Prediction Result\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_rf), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_rf))","b25f013f":"# Feature importances\nFeature_rf = pd.DataFrame({'feature':X_train.columns, 'importance':list(rf_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_rf, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","13a937c4":"from sklearn.ensemble import GradientBoostingClassifier","e154e968":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef gbm_cl_bo(max_depth, max_features, learning_rate, n_estimators, subsample):\n    params_gbm = {}\n    \n    params_gbm['max_depth'] = round(max_depth)\n    params_gbm['max_features'] = max_features\n    params_gbm['learning_rate'] = learning_rate\n    params_gbm['n_estimators'] = round(n_estimators)\n    params_gbm['subsample'] = subsample\n    \n    score = cross_val_score(GradientBoostingClassifier(random_state=123, **params_gbm),\n                             X_train, y_train, scoring=accuracy, cv=5).mean()\n    return score\n\n# Set hyperparameters spaces\nparams_gbm ={\n    'max_depth':(3, 10),\n    'max_features':(0.8, 1),\n    'learning_rate':(0.01, 1),\n    'n_estimators':(80, 150),\n    'subsample': (0.8, 1)\n}\n\n# Run Bayesian Optimization\ngbm_bo = BayesianOptimization(gbm_cl_bo, params_gbm, random_state=111)\ngbm_bo.maximize(init_points=4, n_iter=25)","8a704961":"# Best hyperparameters\nparams_gbm = gbm_bo.max['params']\nparams_gbm['max_depth'] = round(params_gbm['max_depth'])\nparams_gbm['n_estimators'] = round(params_gbm['n_estimators'])\nparams_gbm","61dc23aa":"# Fit the training data\ngbm_hyp =  GradientBoostingClassifier(**params_gbm, random_state=123)\ngbm_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_gbm = gbm_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_gbm)))","93a6dd59":"# Prediction Result\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_nbBer), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_gbm))","e8dbcf68":"# Feature importances\nFeature_gbm = pd.DataFrame({'feature':X_train.columns, 'importance':list(gbm_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=Feature_gbm, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","0ec9d903":"import lightgbm","bb8f11b6":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef lgbm_cl_bo(max_depth, subsample, colsample_bytree,min_child_weight, learning_rate, num_leaves_percentage):\n    params_lgbm = {'objective': 'multiclass'}\n    \n    params_lgbm['max_depth'] = round(max_depth)\n    params_lgbm['subsample'] = subsample\n    params_lgbm['colsample_bytree'] = colsample_bytree\n    params_lgbm['min_child_weight'] = min_child_weight\n    params_lgbm['learning_rate'] = learning_rate\n    params_lgbm['num_leaves'] = round((2**round(max_depth))*num_leaves_percentage)\n    \n    lgbm_bo = lightgbm.LGBMClassifier(random_state=123, **params_lgbm)\n    score = cross_val_score(lgbm_bo, X_train, y_train, scoring=accuracy, cv=5).mean()\n    return score\n\n# Set parameters distribution\nparams_lgbm ={\n    'min_child_weight':(1e-5, 1e-1),\n    'subsample':(0.5, 1),\n    'colsample_bytree':(0.5, 1),\n    'max_depth': (3, 15),\n    'learning_rate': (0.01, 0.5),\n    'num_leaves_percentage':(0.5,0.9)\n}\n\n# Run Bayesian Optimization\nlgbm_bo = BayesianOptimization(lgbm_cl_bo, params_lgbm, random_state=111)\nlgbm_bo.maximize(init_points=4, n_iter=25)","0b1aee6c":"# Best hyperparameters\nparams_lgbm = lgbm_bo.max['params']\nparams_lgbm['objective'] = 'multiclass'\nparams_lgbm['max_depth'] = int(params_lgbm['max_depth'])\nparams_lgbm['num_leaves'] = round((2**round(params_lgbm['max_depth']))*params_lgbm['num_leaves_percentage'])\ndel params_lgbm[\"num_leaves_percentage\"]\nparams_lgbm","c3dcf11f":"# Fit the training data\nlgbm_hyp =  lightgbm.LGBMClassifier(**params_lgbm, random_state=123, n_jobs=-1)\nlgbm_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_lgbm = lgbm_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_lgbm)))","cfbfc4a1":"# Prediction Result\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_lgbm), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_lgbm))","e2020377":"# Feature importances\nFeatureLgbm = pd.DataFrame({'feature':X_train.columns, 'importance':list(gbm_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=FeatureLgbm, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","7fea360d":"from xgboost import XGBClassifier","17bd4a0b":"# Hyperparameter tuning: Bayesian Optimization\ndef xgb_cl_bo(n_estimators, max_depth, learning_rate, gamma, min_child_weight, subsample, colsample_bytree):\n    params_xgb = {\n    'objective': 'multi:softmax',\n    'eval_metric':'mlogloss',\n    'nthread':-1\n    }\n    params_xgb['n_estimators'] = round(n_estimators)\n    params_xgb['max_depth'] = round(max_depth)\n    params_xgb['learning_rate'] = learning_rate\n    params_xgb['gamma'] = gamma\n    params_xgb['min_child_weight'] = round(min_child_weight)\n    params_xgb['subsample'] = subsample\n    params_xgb['colsample_bytree'] = colsample_bytree\n        \n    score = cross_val_score(XGBClassifier(random_state=123, **params_xgb),\n                            X_train, y_train, scoring=accuracy, cv=5).mean()\n    return score\n\n# Set parameters distribution\nparams_xgb ={\n    'n_estimators':(80, 150),\n    'max_depth': (3, 15),\n    'learning_rate': (0.01, 0.5),\n    'gamma':(0, 10),\n    'min_child_weight':(3, 20),\n    'subsample':(0.5, 1),\n    'colsample_bytree':(0.1, 1)\n}\n\n# Run Bayesian Optimization\nxgb_bo = BayesianOptimization(xgb_cl_bo, params_xgb, random_state=111)\nxgb_bo.maximize(init_points=4, n_iter=25)","cc5db09f":"# Best hyperparameters\nparams_xgb = xgb_bo.max['params']\nparams_xgb['objective'] = 'multi:softmax'\nparams_xgb['n_jobs'] = -1\nparams_xgb['n_estimators'] = round(params_xgb['n_estimators'])\nparams_xgb['max_depth'] = round(params_xgb['max_depth'])\nparams_xgb['min_child_weight'] = round(params_xgb['min_child_weight'])\nparams_xgb","b02909bc":"# Fit the training data\nxgb_hyp =  XGBClassifier(**params_xgb, random_state=123, nthread=-1)\nxgb_hyp.fit(X_train, y_train)\n\n# Predict the validation data\npred_xgb = xgb_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_xgb)))","588cd091":"# Prediction Result\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_xgb), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_xgb))","d76bf9e0":"# Feature importances\nFeatureXgb = pd.DataFrame({'feature':X_train.columns, 'importance':list(xgb_hyp.feature_importances_)}).sort_values('importance')\nplt.figure(figsize=(16,4))\nsns.barplot(data=FeatureXgb, x='feature', y='importance')\nplt.xticks(rotation=90)\nplt.show()","92e4efc4":"# Deep Learning packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Dropout\nfrom keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.layers import LeakyReLU\nLeakyReLU = LeakyReLU(alpha=0.1)","e2232721":"# Hyperparameter-tuning: Bayesian Optimization, bayes_opt\ndef nn_cl_bo(neurons, activation, optimizer, learning_rate, batch_size, epochs,\n              layers1, layers2, normalization, dropout, dropout_rate):\n    optimizerL = ['Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD', 'SGD']\n    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n               'elu', 'exponential', LeakyReLU, LeakyReLU]\n        \n    neurons = round(neurons)\n    activation = activationL[floor(activation)]\n    optimizer = optimizerD[optimizerL[floor(optimizer)]]\n    batch_size = round(batch_size)\n    epochs = round(epochs)\n    layers1 = round(layers1)\n    layers2 = round(layers2)\n        \n    def nn_cl_fun():\n        nn = Sequential()\n        nn.add(Dense(neurons, input_dim=X_train.shape[1], activation=activation))\n        if normalization > 0.5:\n            nn.add(BatchNormalization())\n        for i in range(layers1):\n            nn.add(Dense(neurons, activation=activation))\n        if dropout > 0.5:\n            nn.add(Dropout(dropout_rate, seed=123))\n        for i in range(layers2):\n            nn.add(Dense(neurons, activation=activation))\n        nn.add(Dense(4, activation='softmax'))\n        nn.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n        return nn\n        \n    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n    \n    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n    score = cross_val_score(nn, X_train, y_train, scoring=accuracy, cv=kfold, fit_params={'callbacks':[es]}).mean()\n    \n    return score\n\n# Set hyperparameters spaces\nparams_nn ={\n    'neurons': (10, 100),\n    'activation':(0, 9),\n    'optimizer':(0,7),\n    'learning_rate':(0.01, 1),\n    'batch_size':(500, 1000),\n    'epochs':(200, 1000),\n    'layers1':(1,3),\n    'layers2':(1,3),\n    'normalization':(0,1),\n    'dropout':(0,1),\n    'dropout_rate':(0,0.3)\n}\n\n# Run Bayesian Optimization\nnn_bo = BayesianOptimization(nn_cl_bo, params_nn, random_state=123)\nnn_bo.maximize(init_points=4, n_iter=25)","3b4071f6":"# Best hyperparameters\nparams_nn = nn_bo.max['params']\n\nlearning_rate = params_nn['learning_rate']\noptimizerL = ['Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD', 'SGD']\noptimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n             'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n             'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n             'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\nactivationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n               'elu', 'exponential', LeakyReLU, LeakyReLU]\nparams_nn['activation'] = activationL[round(params_nn['activation'])]\nparams_nn['batch_size'] = round(params_nn['batch_size'])\nparams_nn['epochs'] = round(params_nn['epochs'])\nparams_nn['layers1'] = round(params_nn['layers1'])\nparams_nn['layers2'] = round(params_nn['layers2'])\nparams_nn['neurons'] = round(params_nn['neurons'])\nparams_nn['optimizer'] = optimizerD[optimizerL[round(params_nn['optimizer'])]]\n\nparams_nn","d6dea618":"# Fitting the training data\ndef nn_cl_fun():\n    nn = Sequential()\n    nn.add(Dense(params_nn['neurons'], input_dim=X_train.shape[1], activation=params_nn['activation']))\n    if params_nn['normalization'] > 0.5:\n        nn.add(BatchNormalization())\n    for i in range(params_nn['layers1']):\n        nn.add(Dense(params_nn['neurons'], activation=params_nn['activation']))\n    if params_nn['dropout'] > 0.5:\n        nn.add(Dropout(params_nn['dropout_rate'], seed=123))\n    for i in range(params_nn['layers2']):\n        nn.add(Dense(params_nn['neurons'], activation=params_nn['activation']))\n    nn.add(Dense(4, activation='softmax'))\n    nn.compile(loss='categorical_crossentropy', optimizer=params_nn['optimizer'], metrics=['accuracy'])\n    return nn\n        \nes = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\nnn_hyp = KerasClassifier(build_fn=nn_cl_fun, epochs=params_nn['epochs'], batch_size=params_nn['batch_size'], verbose=0)\n\nnn_hyp.fit(X_train, y_train, verbose=0)\n\n# Predict the validation data\npred_nn = nn_hyp.predict(X_val)\n\n# Compute the accuracy\nprint('Accuracy: ' + str(accuracy_score(y_val, pred_nn)))","1158a4e1":"# Prediction Result\nprint('Confusion Matrix')\nprint(pd.DataFrame(confusion_matrix(y_val, pred_nn), index=[1,2,3,4], columns=[1,2,3,4]))\nprint('')\nprint('Classification Report')\nprint(classification_report(y_val, pred_nn))","74b067a0":"# 7. Gradient Boosting Machine","8533ee37":"The task is to predict which poverty class each household is in. There are 4 classes of poverty level: 1 = extreme poverty, 2 = moderate poverty, 3 = vulnerable households, and 4 = non vulnerable households.","f6fc6cc6":"# \ud83d\udcb0 Multi-Class Classification-Accuracy-Poverty Level\nThis notebook provides commonly used Machine Learning algorithms. The task is multi-class classification. Feature generation or selection is just simply performed. The objective of this notebook is to serve as a cheat sheet.\n\nTen Machine Learning algorithms are developed to predict with accuracy as the scorer. All algorithms are applied with hyperparameter-tuning to search for the optimum model evaluation results. The hyperparameter-tuning methods consist of GridSearchCV and Bayesian Optimization (using bayes_opt or hyperopt packages) with 5-fold cross-validation.\n\nThe optimum hyperparameters are then used to train the training dataset and predict the unseen validation dataset. The model is evaluated using accuracy, followed by the confusion matrix and classification report. Useful attributes of the models are also displayed, such as the coefficients or feature importances.","ddb3daf1":"# 3. K Nearest Neighbors","d25d3ca1":"# 5. Decision Tree","d542a431":"All tree-based algorithms create tree-like structures. Decision Tree has 1 tree-like structure. The other tree-based algorithms have more than 1 trees. In this notebook, only Decision Tree is plotted for its tree.","f89e49fc":"# 9. XGBoost","23cd60e0":"# 8. LightGBM","dd0b1a83":"# 6. Random Forest","ae2a312d":"# 4. Support Vector Machine","ff6fae89":"# 2a. Naive Bayes (Gaussian)","5c5fd143":"# 10. Neural Network (Deep Learning)","d6ae9a06":"# 1. Logistic Regression","ee429e82":"# 2b. naive Bayes (Bernoulli)"}}