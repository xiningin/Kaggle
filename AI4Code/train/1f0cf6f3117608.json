{"cell_type":{"152d2553":"code","1c9c5c51":"code","9040cdfb":"code","17a5007e":"code","87a2ef95":"code","ce43fe2b":"code","0b9c068d":"code","f71ad31d":"code","021e1fdd":"code","5e5ac93f":"code","1ef51d62":"code","de13b240":"code","0e5af408":"code","4a5c7df0":"code","081dc15a":"code","1f1886c4":"code","7bef015c":"code","c32319fc":"code","15ebfc46":"markdown","cdaaa624":"markdown","2d6aea97":"markdown","78cf59a2":"markdown","05bad26d":"markdown","ab82fa50":"markdown","52446d87":"markdown","c454ebcb":"markdown","9603ccab":"markdown","e280e585":"markdown"},"source":{"152d2553":"!pip install tensorflow\nimport os","1c9c5c51":"#base_dir = '..\/input\/mechanical-tools-dataset\/Mechanical Tools Image dataset'\n\ntrain_dir = os.path.join('..\/input\/mechanical-tools-dataset\/train_data_V2\/train_data_V2')\nvalidation_dir = os.path.join('..\/input\/mechanical-tools-dataset\/validation_data_V2\/validation_data_V2')\n\n# Directory with our training screwdriver\/wrench pictures\ntrain_screwdriver_dir = os.path.join('..\/input\/mechanical-tools-dataset\/train_data_V2\/train_data_V2\/screwdriver')\ntrain_wrench_dir = os.path.join('..\/input\/mechanical-tools-dataset\/train_data_V2\/train_data_V2\/wrench')\ntrain_hammer_dir = os.path.join('..\/input\/mechanical-tools-dataset\/train_data_V2\/train_data_V2\/hammer')\n\n# Directory with our validation screwdriver\/wrench pictures\nvalidation_screwdriver_dir = os.path.join('..\/input\/mechanical-tools-dataset\/validation_data_V2\/validation_data_V2\/screwdriver')\nvalidation_wrench_dir = os.path.join('..\/input\/mechanical-tools-dataset\/validation_data_V2\/validation_data_V2\/wrench')\nvalidation_hammer_dir = os.path.join('..\/input\/mechanical-tools-dataset\/validation_data_V2\/validation_data_V2\/hammer')","9040cdfb":"train_screwdriver_fnames = os.listdir(train_screwdriver_dir )\ntrain_wrench_fnames = os.listdir( train_wrench_dir)\ntrain_hammer_fnames = os.listdir( train_hammer_dir )\n\nprint(train_screwdriver_fnames[:20])\nprint(train_wrench_fnames[:20])\nprint(train_hammer_fnames[:20])","17a5007e":"print('total training screwdriver images :', len(os.listdir(train_screwdriver_dir)))\nprint('total training wrench images :', len(os.listdir(train_wrench_dir)))\nprint('total training hammer images :', len(os.listdir(train_hammer_dir)))\n\n\nprint('total validation screwdriver images :', len(os.listdir( validation_screwdriver_dir ) ))\nprint('total validation wrench images :', len(os.listdir( validation_wrench_dir) ))\nprint('total validation hammer images :', len(os.listdir( validation_hammer_dir) ))","87a2ef95":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\n# Parameters for our graph; we'll output images in a 10x10 configuration\nnrows = 10\nncols = 10\n\n# Index for iterating over images\npic_index = 0","ce43fe2b":"# Set up matplotlib fig, and size it to fit 4x4 pics\nfig = plt.gcf()\nfig.set_size_inches(ncols * 4, nrows * 4)\n\npic_index += 10\nnext_screwdriver_pix = [os.path.join(train_screwdriver_dir, fname) \n                for fname in train_screwdriver_fnames[pic_index-10:pic_index]]\nnext_wrench_pix = [os.path.join(train_wrench_dir, fname) \n                for fname in train_wrench_fnames[pic_index-10:pic_index]]\nnext_hammer_pix = [os.path.join(train_hammer_dir, fname) \n                for fname in train_hammer_fnames[pic_index-10:pic_index]]\n\nfor i, img_path in enumerate(next_screwdriver_pix+next_wrench_pix+next_hammer_pix):\n  # Set up subplot; subplot indices start at 1\n  sp = plt.subplot(nrows, ncols, i + 1)\n  sp.axis('Off') # Don't show axes (or gridlines)\n\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n\nplt.show()","0b9c068d":"import tensorflow as tf","f71ad31d":"import os\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\nlocal_weights_file = '..\/input\/inceptionv3\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n\npre_trained_model = InceptionV3(input_shape = (300,300, 3), \n                                include_top = False, \n                                weights = None)\n\npre_trained_model.load_weights(local_weights_file)\n\nfor layer in pre_trained_model.layers:\n  layer.trainable = False\n  \n# pre_trained_model.summary()\n\nlast_layer = pre_trained_model.get_layer('mixed7')\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output","021e1fdd":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\n# All images will be rescaled by 1.\/255.\ntest_datagen  = ImageDataGenerator( rescale = 1.0\/255. )\n\n# Add our data-augmentation parameters to ImageDataGenerator\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255.,\n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\ntest_datagen = train_datagen = ImageDataGenerator(rescale = 1.\/255.,\n                                   rotation_range = 40,\n                                   width_shift_range = 0.2,\n                                   height_shift_range = 0.2,\n                                   shear_range = 0.2,\n                                   zoom_range = 0.2,\n                                   horizontal_flip = True)\n\n# --------------------\n# Flow training images in batches of 20 using train_datagen generator\n# --------------------\ntrain_generator = train_datagen.flow_from_directory(train_dir,\n                                                    batch_size=30,\n                                                    class_mode='categorical',\n                                                    target_size=(300,300))     \n# --------------------\n# Flow validation images in batches of 20 using test_datagen generator\n# --------------------\nvalidation_generator =  test_datagen.flow_from_directory(validation_dir,\n                                                         batch_size=30,\n                                                         class_mode  = 'categorical',\n                                                         target_size = (300,300))","5e5ac93f":"from tensorflow.keras.optimizers import RMSprop\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)\n# Add a fully connected layer with 1,024 hidden units and ReLU activation\nx = layers.Dense(128, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)                  \n# Add a final sigmoid layer for classification\nx = layers.Dense(3, activation='softmax')(x)           \n\nmodel = Model( pre_trained_model.input, x) \n\nmodel.compile(optimizer = RMSprop(lr=0.0001), \n              loss = 'categorical_crossentropy', \n              metrics = ['accuracy'])\n","1ef51d62":"model.summary()","de13b240":"history = model.fit(train_generator,\n                              validation_data=validation_generator,\n                              steps_per_epoch=20,\n                              epochs=40,\n                              validation_steps=10,\n                              verbose=2)","0e5af408":"#-----------------------------------------------------------\n# Retrieve a list of list results on training and test data\n# sets for each training epoch\n#-----------------------------------------------------------\nacc      = history.history[     'accuracy' ]\nval_acc  = history.history[ 'val_accuracy' ]\nloss     = history.history[    'loss' ]\nval_loss = history.history['val_loss' ]\n\nepochs   = range(len(acc)) # Get number of epochs\n\n#------------------------------------------------\n# Plot training and validation accuracy per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     acc )\nplt.plot  ( epochs, val_acc )\nplt.title ('Training and validation accuracy')\nplt.figure()\n\n#------------------------------------------------\n# Plot training and validation loss per epoch\n#------------------------------------------------\nplt.plot  ( epochs,     loss )\nplt.plot  ( epochs, val_loss )\nplt.title ('Training and validation loss'  )","4a5c7df0":"\nimport cv2\nimg = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Screwdriver (1415).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict(img)\nprint(classes)","081dc15a":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Screwdriver (1401).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict(img)\nprint(classes)","1f1886c4":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Screwdriver (1402).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict(img)\nprint(classes)","7bef015c":"img = cv2.imread('..\/input\/mechanical-tools-dataset\/test_data\/test_data\/Wrench (286).JPEG')\nplt.imshow(img)\nimg = cv2.resize(img,(300, 300))\nimg = np.reshape(img,[1,300, 300,3])\n\nclasses = model.predict(img)\nprint(classes)","c32319fc":"import numpy as np\nimport random\nfrom tensorflow.keras.preprocessing.image import img_to_array, load_img\n\n# Let's define a new Model that will take an image as input, and will output\n# intermediate representations for all layers in the previous model after\n# the first.\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\n#visualization_model = Model(img_input, successive_outputs)\nvisualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n# Let's prepare a random input image from the training set.\nwrench_img_files = [os.path.join(train_wrench_dir, f) for f in train_wrench_fnames]\nscrewdriver_img_files = [os.path.join(train_wrench_dir, f) for f in train_screwdriver_fnames]\nimg_path = random.choice(wrench_img_files + screwdriver_img_files)\n\nimg = load_img(img_path, target_size=(300,300))  # this is a PIL image\nx = img_to_array(img)  # Numpy array with shape (150, 150, 3)\nx = x.reshape((1,) + x.shape)  # Numpy array with shape (1, 150, 150, 3)\n\n# Rescale by 1\/255\nx \/= 255\n\n# Let's run our image through our network, thus obtaining all\n# intermediate representations for this image.\nsuccessive_feature_maps = visualization_model.predict(x)\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers[1:]]\n\n# Now let's display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n    if len(feature_map.shape) == 4:\n    # Just do this for the conv \/ maxpool layers, not the fully-connected layers\n        n_features = feature_map.shape[-1]  # number of features in feature map\n    # The feature map has shape (1, size, size, n_features)\n        size = feature_map.shape[1]\n    # We will tile our images in this matrix\n        display_grid = np.zeros((size, size * n_features))\n        for i in range(n_features):\n      # Postprocess the feature to make it visually palatable\n            x = feature_map[0, :, :, i]\n            x -= x.mean()\n            x \/= x.std()\n            x *= 64\n            x += 128\n            x = np.clip(x, 0, 255).astype('uint8')\n      # We'll tile each filter into this big horizontal grid\n            display_grid[:, i * size : (i + 1) * size] = x\n    # Display the grid\n        scale = 20. \/ n_features\n        plt.figure(figsize=(scale * n_features, scale))\n        plt.title(layer_name)\n        plt.grid(False)\n        plt.imshow(display_grid, aspect='auto', cmap='summer')","15ebfc46":"**The \"output shape\" column shows how the size of your feature map evolves in each successive layer. The convolution layers reduce the size of the feature maps by a bit due to padding, and each pooling layer halves the dimensions.**","cdaaa624":"**Now let's take a look at a few pictures to get a better sense of what they look like. First, configure the matplot parameters:**","2d6aea97":"# Defining each of these directories","78cf59a2":"# Data Preprocessing","05bad26d":"# Pre-Trained Model","ab82fa50":"**We then add convolutional layers as in the previous example, and flatten the final result to feed into the densely connected layers.**\n**Note that because we are facing a two-class classification problem, i.e. a binary classification problem, we will end our network with a sigmoid activation, so that the output of our network will be a single scalar between 0 and 1, encoding the probability that the current image is class 1 (as opposed to class 0).**","52446d87":"**Let's find out the number of wrench and pliers images in the directory**","c454ebcb":"**Now, display a batch of 50 wrench and 50 pliers pictures. You can rerun the cell to see a fresh batch each time:**","9603ccab":"**Now, let's see what the filenames look like in the training directories:**","e280e585":"**Visualizing Intermediate Representations\nTo get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.\nLet's pick a random image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images.**"}}