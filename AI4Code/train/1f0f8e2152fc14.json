{"cell_type":{"bab9bb1d":"code","f27cdb62":"code","a6f4d0ae":"code","25962dbf":"code","fdd5b392":"code","7c943b5f":"code","e7171f90":"code","37e85487":"code","bf918944":"code","402313cd":"code","abea6f94":"code","7f4931fb":"code","a514e2ea":"code","88f63804":"code","eb7c0d74":"code","7861abaa":"code","a4256ef7":"code","cddce633":"code","f56bdc75":"code","8203a8aa":"code","fa5bd718":"code","39a1ae6a":"code","3867fb13":"code","b2e17948":"code","8054859e":"code","7b8f4bf6":"code","ff5a822d":"code","d6bfe899":"code","bb02c7b7":"code","69ee794d":"code","39d49e28":"code","566ad7e7":"code","04c5cee1":"code","9468cafa":"code","bf309f1b":"code","279579a1":"code","ebbf374d":"code","9a2c6cf3":"code","9114f6ff":"code","2ff226b7":"code","056bd72c":"code","5a3225da":"code","25f72329":"code","ada13526":"code","28df023d":"code","1a8a1f98":"code","731b8147":"code","7f4a8688":"code","7956dddc":"code","b9477347":"code","94fc98de":"code","64a531e6":"code","7ef7b9a1":"code","41b4ee64":"code","bea175ea":"code","d7d20f2f":"code","0a4ad78f":"code","eaa9d1c5":"code","a2fff857":"code","dfe9ffab":"code","dd7f267f":"code","3b17f88c":"markdown","3e3fa403":"markdown","43a207d6":"markdown","50ab41c7":"markdown","59437d29":"markdown","69c81c0f":"markdown","d5bdf3ad":"markdown"},"source":{"bab9bb1d":"import numpy as np\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.express as px\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import *\nstemmer = PorterStemmer()\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import auc\nfrom xgboost import XGBClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f27cdb62":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","a6f4d0ae":"print('Training data shape (rows, cols): ', df_train.shape)\ndf_train.head()","25962dbf":"# keyword and location columns have some nulls\ndf_train.info()","fdd5b392":"print('Test data shape (rows, cols): ', df_test.shape)\ndf_test.head()","7c943b5f":"# keyword and location columns have some nulls\ndf_test.info()","e7171f90":"# Null check\ndf_train['keyword'].isnull().value_counts() \/ df_train.shape[0]","37e85487":"df_train['location'].isnull().value_counts() \/ df_train.shape[0]","bf918944":"df_train['text'].isnull().value_counts() \/ df_train.shape[0]","402313cd":"df_test['keyword'].isnull().value_counts() \/ df_test.shape[0]","abea6f94":"df_test['location'].isnull().value_counts() \/ df_test.shape[0]","7f4931fb":"df_test['text'].isnull().value_counts() \/ df_test.shape[0]","a514e2ea":"# Target Distribution (0 or 1)\ndist_class = df_train['target'].value_counts()\nlabels = ['Non-disaster tweet', 'Disaster tweet']\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n\nsns.barplot(x=dist_class.index, y=dist_class, ax=ax1).set_title(\"Target Count\")\n\nax2.pie(dist_class,\n        labels=labels,\n        counterclock=False,\n        startangle=90,\n        autopct='%1.1f%%',\n        pctdistance=0.7)\nplt.title(\"Target Frequency Proportion\")\nplt.show","88f63804":"disaster_tweet_length = df_train[df_train['target']==1]['text'].str.len()\nnondisaster_tweet_length = df_train[df_train['target']==0]['text'].str.len()\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n\nax1.hist(disaster_tweet_length, color='red')\nax1.set_title(\"Disaster Tweets\")\n\nax2.hist(nondisaster_tweet_length, color='green')\nax2.set_title(\"Non-Disaster Tweets\")\n\nfig.suptitle(\"Characters in tweets\")\nplt.show()","eb7c0d74":"disaster_tweet_words = df_train[df_train['target']==1]['text'].str.split().map(lambda x: len(x))\nnondisaster_tweet_words = df_train[df_train['target']==0]['text'].str.split().map(lambda x: len(x))\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,6))\n\nax1.hist(disaster_tweet_words, color='red')\nax1.set_title(\"Disaster Tweets\")\n\nax2.hist(nondisaster_tweet_words, color='green')\nax2.set_title(\"Non-Disaster Tweets\")\n\nfig.suptitle(\"Words in tweets\")\nplt.show()","7861abaa":"df_train_keyword = pd.DataFrame({\n    'keyword': df_train['keyword'].value_counts().index,\n    'count': df_train['keyword'].value_counts().values\n})\n\ndf_train_location = pd.DataFrame({\n    'location': df_train['location'].value_counts().index,\n    'count': df_train['location'].value_counts().values\n})\n\nprint('Number fo unique keywords in training data: ', df_train_keyword.shape[0])\n\npx.bar(\n    df_train_keyword,\n    x='keyword',\n    y='count',\n    title=\"Each unique keyword count in training data\"\n).show()\n\npx.bar(\n    df_train_location,\n    x=df_train_location['location'][:20],\n    y=df_train_location['count'][:20],\n    title=\"Top 20 location countin training data\"\n).show()","a4256ef7":"df_train[df_train['target'] == 1]['keyword'].value_counts()","cddce633":"df_train[df_train['target'] == 0]['keyword'].value_counts()","f56bdc75":"df_train[df_train['target'] == 1]['location'].value_counts()","8203a8aa":"df_train[df_train['target'] == 0]['location'].value_counts()","fa5bd718":"df_test_keyword = pd.DataFrame({\n    'keyword': df_test['keyword'].value_counts().index,\n    'count': df_test['keyword'].value_counts().values\n})\n\ndf_test_location = pd.DataFrame({\n    'location': df_test['location'].value_counts().index,\n    'count': df_test['location'].value_counts().values\n})\n\nprint('Number fo unique keywords in test data: ', df_test_keyword.shape[0])\n\npx.bar(\n    df_test_keyword,\n    x='keyword',\n    y='count',\n    title=\"Each unique keyword count in test data\"\n).show()\n\npx.bar(\n    df_test_location,\n    x=df_test_location['location'][:20],\n    y=df_test_location['count'][:20],\n    title=\"Top 20 location count in test data\"\n).show()","39a1ae6a":"disaster_tweet = dict(df_train[df_train['target']==1]['keyword'].value_counts())\n\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(stopwords=stopwords, width=800, height=400, background_color=\"white\").generate_from_frequencies(disaster_tweet)\n\nplt.figure(figsize=[10,6])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","3867fb13":"nondisaster_tweet = dict(df_train[df_train['target']==0]['keyword'].value_counts())\n\nwordcloud = WordCloud(stopwords=stopwords, width=800, height=400, background_color=\"white\").generate_from_frequencies(nondisaster_tweet)\n\nplt.figure(figsize=[10,6])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","b2e17948":"test_tweet = dict(df_test['keyword'].value_counts())\n\nwordcloud = WordCloud(stopwords=stopwords, width=800, height=400, background_color=\"white\").generate_from_frequencies(test_tweet)\n\nplt.figure(figsize=[10,6])\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","8054859e":"# https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nimport string\ndef remove_punc(text):\n    table = str.maketrans('','',string.punctuation)\n    return text.translate(table)","7b8f4bf6":"# https:\/\/github.com\/rishabhverma17\/sms_slang_translator\/blob\/master\/slang.txt\nslang_abbrev_dict = {\n    'AFAIK': 'As Far As I Know',\n    'AFK': 'Away From Keyboard',\n    'ASAP': 'As Soon As Possible',\n    'ATK': 'At The Keyboard',\n    'ATM': 'At The Moment',\n    'A3': 'Anytime, Anywhere, Anyplace',\n    'BAK': 'Back At Keyboard',\n    'BBL': 'Be Back Later',\n    'BBS': 'Be Back Soon',\n    'BFN': 'Bye For Now',\n    'B4N': 'Bye For Now',\n    'BRB': 'Be Right Back',\n    'BRT': 'Be Right There',\n    'BTW': 'By The Way',\n    'B4': 'Before',\n    'B4N': 'Bye For Now',\n    'CU': 'See You',\n    'CUL8R': 'See You Later',\n    'CYA': 'See You',\n    'FAQ': 'Frequently Asked Questions',\n    'FC': 'Fingers Crossed',\n    'FWIW': 'For What It\\'s Worth',\n    'FYI': 'For Your Information',\n    'GAL': 'Get A Life',\n    'GG': 'Good Game',\n    'GN': 'Good Night',\n    'GMTA': 'Great Minds Think Alike',\n    'GR8': 'Great!',\n    'G9': 'Genius',\n    'IC': 'I See',\n    'ICQ': 'I Seek you',\n    'ILU': 'I Love You',\n    'IMHO': 'In My Humble Opinion',\n    'IMO': 'In My Opinion',\n    'IOW': 'In Other Words',\n    'IRL': 'In Real Life',\n    'KISS': 'Keep It Simple, Stupid',\n    'LDR': 'Long Distance Relationship',\n    'LMAO': 'Laugh My Ass Off',\n    'LOL': 'Laughing Out Loud',\n    'LTNS': 'Long Time No See',\n    'L8R': 'Later',\n    'MTE': 'My Thoughts Exactly',\n    'M8': 'Mate',\n    'NRN': 'No Reply Necessary',\n    'OIC': 'Oh I See',\n    'OMG': 'Oh My God',\n    'PITA': 'Pain In The Ass',\n    'PRT': 'Party',\n    'PRW': 'Parents Are Watching',\n    'QPSA?': 'Que Pasa?',\n    'ROFL': 'Rolling On The Floor Laughing',\n    'ROFLOL': 'Rolling On The Floor Laughing Out Loud',\n    'ROTFLMAO': 'Rolling On The Floor Laughing My Ass Off',\n    'SK8': 'Skate',\n    'STATS': 'Your sex and age',\n    'ASL': 'Age, Sex, Location',\n    'THX': 'Thank You',\n    'TTFN': 'Ta-Ta For Now!',\n    'TTYL': 'Talk To You Later',\n    'U': 'You',\n    'U2': 'You Too',\n    'U4E': 'Yours For Ever',\n    'WB': 'Welcome Back',\n    'WTF': 'What The Fuck',\n    'WTG': 'Way To Go!',\n    'WUF': 'Where Are You From?',\n    'W8': 'Wait',\n    '7K': 'Sick:-D Laugher'\n}\n\ndef unslang(text):\n    if text.upper() in slang_abbrev_dict.keys():\n        return slang_abbrev_dict[text.upper()]\n    else:\n        return text","ff5a822d":"def tokenization(text):\n    text = re.split('\\W+', text)\n    return text\n\nstopword = nltk.corpus.stopwords.words('english')\ndef remove_stopwords(text):\n    text = [word for word in text if word not in stopword]\n    return text\n\ndef stemming(text):\n    text = [stemmer.stem(word) for word in text]\n    return text","d6bfe899":"for datas in [df_train, df_test]:\n    datas['cleaned_text'] = datas['text'].apply(lambda x : remove_url(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : remove_html(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : remove_emoji(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : unslang(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : remove_punc(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : tokenization(x.lower()))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : remove_stopwords(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : stemming(x))\n    datas['cleaned_text'] = datas['cleaned_text'].apply(lambda x : ' '.join(x))","bb02c7b7":"df_train.head(10)","69ee794d":"df_train['text'][100]","39d49e28":"df_train['cleaned_text'][100]","566ad7e7":"df_test.head(10)","04c5cee1":"df_test['text'][100]","9468cafa":"df_test['cleaned_text'][100]","bf309f1b":"vectorizer = CountVectorizer(analyzer='word', binary=True)\nvectorizer.fit(df_train['cleaned_text'])","279579a1":"# df_train_feature = df_train[['keyword', 'location', 'cleaned_text']]\n# X = vectorizer.transform(df_train_feature).todense()\nX = vectorizer.transform(df_train['cleaned_text']).todense()\ny = df_train['target'].values","ebbf374d":"print(X.shape)\nprint(y.shape)","9a2c6cf3":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=0)","9114f6ff":"# from sklearn.model_selection import GridSearchCV\n\n# params = {\n#     'max_depth': list(range(5, 11)),\n#     'learning_rate': list(np.arange(0.05, 0.30, 0.05)),\n#     'gamma': list(np.arange(0.01, 0.06, 0.01)),\n#     'min_child_weight': list(range(1, 6)),\n    \n#     # fixed params\n#     'n_estimators' : [1500],\n#     'n_jobs': [4],\n#     'objective': ['binary:logistic'],\n#     'eval_metric' : ['logloss'],\n#     'random_state': [42]\n# }\n\n# model = XGBClassifier(tree_method='gpu_hist')\n# cv = GridSearchCV(model, params, cv=5, n_jobs=4, scoring='roc_auc')\n\n# cv.fit(X_train, y_train)","2ff226b7":"xgb_classifir = XGBClassifier(tree_method='gpu_hist',\n                              learning_rate=0.1,\n                              num_round=1000,\n                              max_depth=10,\n                              min_child_weight=2,\n                              colsample_bytree=0.8,\n                              subsample=0.9,\n                              gamma=0.4,\n                              reg_alpha=1e-5,\n                              reg_lambda=1,\n                              n_estimators=2000,\n                              objective='binary:logistic',\n                              eval_metric=[\"auc\", \"logloss\", \"error\"],\n                              early_stopping_rounds=50)\n\n# https:\/\/www.coursera.org\/learn\/competitive-data-science\/lecture\/wzi5a\/hyperparameter-tuning-ii","056bd72c":"%%time\nxgb_classifir.fit(X_train, y_train, eval_set=[(X_train, y_train),(X_valid, y_valid)])","5a3225da":"y_pred_xgb = xgb_classifir.predict(X_valid)","25f72329":"type(y_pred_xgb)","ada13526":"print(y_pred_xgb)","28df023d":"confusion_matrix(y_valid, y_pred_xgb)","1a8a1f98":"accuracy_score(y_valid, y_pred_xgb)","731b8147":"f1_score(y_valid, y_pred_xgb)","7f4a8688":"fpr, tpr, _ = roc_curve(y_valid, y_pred_xgb)\nauc_score = auc(fpr, tpr)","7956dddc":"# clear current figure\nplt.clf()\n\nplt.title('ROC Curve')\nplt.plot(fpr, tpr, marker='.', label='AUC = {:.2f}'.format(auc_score))\n\n# it's helpful to add a diagonal to indicate where chance \n# scores lie (i.e. just flipping a coin)\nplt.plot([0,1],[0,1],'r--')\n\nplt.xlim([-0.1,1.1])\nplt.ylim([-0.1,1.1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\n\nplt.legend(loc='lower right')\nplt.show()","b9477347":"predictions = [round(value) for value in y_pred_xgb]","94fc98de":"# retrieve performance metrics\nresults = xgb_classifir.evals_result()\nepochs = len(results['validation_0']['auc'])\nx_axis = range(0, epochs)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18,6))\n\n# plot auc\nax1.plot(x_axis, results['validation_0']['auc'], label='Train')\nax1.plot(x_axis, results['validation_1']['auc'], label='Validation')\nax1.set_title(\"XGBoost AUC\")\nax1.legend()\n\n# plot log loss\nax2.plot(x_axis, results['validation_0']['logloss'], label='Train')\nax2.plot(x_axis, results['validation_1']['logloss'], label='Validation')\nax2.set_title(\"XGBoost Log Loss\")\nax2.legend()\n\n# plot classification error\nax3.plot(x_axis, results['validation_0']['error'], label='Train')\nax3.plot(x_axis, results['validation_1']['error'], label='Validation')\nax3.set_title(\"XGBoost Classification Error\")\nax3.legend()","64a531e6":"# Suppport Vecter Machine\nfrom sklearn.svm import SVC\n\nsvm_classifier = SVC(kernel='rbf')\nsvm_classifier.fit(X_train, y_train)","7ef7b9a1":"y_pred_svm = svm_classifier.predict(X_valid)\nprint(y_pred_svm)","41b4ee64":"# K-Nearest neighbour\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn_classifier = KNeighborsClassifier(n_neighbors = 7,weights = 'distance',algorithm = 'brute')\nknn_classifier.fit(X_train, y_train)","bea175ea":"y_pred_knn = knn_classifier.predict(X_valid)\nprint(y_pred_knn)","d7d20f2f":"X_testset = vectorizer.transform(df_test['cleaned_text']).todense()","0a4ad78f":"print(X_testset.shape)","eaa9d1c5":"y_test_pred_ave = (xgb_classifir.predict(X_testset) + svm_classifier.predict(X_testset) + knn_classifier.predict(X_testset)) \/ 3\nprint(y_test_pred_ave)","a2fff857":"y_test_pred = np.where(y_test_pred_ave >= 0.5, 1, 0)\nprint(y_test_pred)","dfe9ffab":"submission_file = pd.DataFrame({'id': df_test['id'], 'target': y_test_pred})\nsubmission_file","dd7f267f":"submission_file.to_csv('submission_xgb_20200130.csv', index = False)","3b17f88c":"## Modeling and Evaluation","3e3fa403":"## Exploratory Data Analysis (EDA)\n##### Columns\n- id : a unique identifier for each tweet\n- text : the text of the tweet\n- location : the location the tweet was sent from (may be blank)\n- keyword : a particular keyword from the tweet (may be blank)\n- target : in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)","43a207d6":"## Submission","50ab41c7":"About how to tune the hyperparameters of XGBClassifier, I used ['Complete Guide to Parameter Tuning in XGBoost with codes in Python'](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/) as reference.  \nActually I'd like to use GridSearchCV, however I gave up using it due to kernel memory issues.","59437d29":"## Feature Engineering (Data Cleaning? Data Pre-Processing?)","69c81c0f":"## Trying to use ensamble method (the simplest bagging?)\nReferrence: https:\/\/www.coursera.org\/learn\/competitive-data-science\/lecture\/MJKCi\/introduction-into-ensemble-methods","d5bdf3ad":"Future work: Use of DNN, also reserach for speeding up lerning on the kernel.\n\n### <font color=\"Red\">**If you like this kernel, please upvote:)**<\/font>"}}