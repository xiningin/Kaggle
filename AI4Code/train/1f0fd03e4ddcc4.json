{"cell_type":{"dc57c2e0":"code","f105d66c":"code","c7bb4ecf":"code","5c0c7a8a":"code","f36636c9":"code","435413ba":"code","66e583cc":"code","b9f4b2e4":"code","1c0b571b":"code","a155cedf":"code","25e5ac2a":"code","f3f970a8":"code","3241b20d":"code","3921f03b":"code","57a59bdb":"code","56a550d8":"code","57d36746":"code","1af1e2c6":"code","b9abc94d":"code","521c7163":"code","5763c0c5":"code","d8beced4":"code","fee2498c":"code","2596abc5":"code","e87905ee":"code","0147fb8e":"code","a063e49b":"code","58a295f2":"markdown","21a1953f":"markdown","50e8cbdb":"markdown","bb2642ab":"markdown"},"source":{"dc57c2e0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport xgboost\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom datetime import datetime\nfrom scipy.special import logsumexp\n\nfrom catboost import Pool, cv, CatBoostClassifier, CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error, classification_report, roc_auc_score\nos.environ[\"OMP_NUM_THREADS\"] = \"8\"","f105d66c":"train = pd.read_csv(\"\/kaggle\/input\/caltech-cs155-2020\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/caltech-cs155-2020\/test.csv\")\ndf = pd.concat([train,test],sort=False)","c7bb4ecf":"from sklearn import preprocessing\neug_df = df.copy()\neug_df['opened_position_qty '] = np.log1p(eug_df['opened_position_qty '])\neug_df['closed_position_qty'] = np.log1p(eug_df['closed_position_qty'])\neug_df['transacted_qty'] = np.log1p(eug_df['transacted_qty'])\neug_df['bid1vol'] = np.log1p(eug_df['bid1vol'])\neug_df['bid2vol'] = np.log1p(eug_df['bid2vol'])\neug_df['bid3vol'] = np.log1p(eug_df['bid3vol'])\neug_df['bid4vol'] = np.log1p(eug_df['bid4vol'])\neug_df['bid5vol'] = np.log1p(eug_df['bid5vol'])\neug_df['ask1vol'] = np.log1p(eug_df['ask1vol'])\neug_df['ask2vol'] = np.log1p(eug_df['ask2vol'])\neug_df['ask3vol'] = np.log1p(eug_df['ask3vol'])\neug_df['ask4vol'] = np.log1p(eug_df['ask4vol'])\neug_df['ask5vol'] = np.log1p(eug_df['ask5vol'])","5c0c7a8a":"bid_cols = ['bid1','bid2', 'bid3', 'bid4', 'bid5']\nbid_vol_cols = ['bid1vol', 'bid2vol', 'bid3vol', 'bid4vol', 'bid5vol']\nask_cols = ['ask1', 'ask2', 'ask3', 'ask4', 'ask5',]\nask_vol_cols = ['ask1vol','ask2vol', 'ask3vol', 'ask4vol', 'ask5vol']\n\ngroup_cols = {\"bid_cols\":bid_cols,\"bid_vol_cols\":bid_vol_cols,\"ask_cols\":ask_cols,\"ask_vol_cols\":ask_vol_cols}","f36636c9":"#watch out here\ndf = eug_df\nfor group in group_cols.keys():\n    print(group)\n    df[f\"{group}_max\"] = df[group_cols[group]].max(axis=1)\n    df[f\"{group}_min\"] = df[group_cols[group]].min(axis=1)\n    df[f\"{group}_spread\"] = df[f\"{group}_max\"].div(df[f\"{group}_min\"])\n    \ndf[\"last_price_div__mid\"] = df[\"last_price\"].div(df[\"mid\"])\n#Add weighted mid-price for each price level\n#https:\/\/wwwf.imperial.ac.uk\/~ajacquie\/Gatheral60\/Slides\/Gatheral60%20-%20Stoikov.pdf\nfor i in range(1,6):\n    I = df['bid'+str(i)+'vol'].div(df['bid'+str(i)+'vol'].add(df['ask'+str(i)+'vol']))\n    print(type(I))\n    df[\"weighted_mid\"+str(i)] = (df['ask'+str(i)]*I).add((1-I)*(df['bid'+str(i)]))","435413ba":"train = df.loc[~df.y.isna()]\nprint(f\"train shape {train.shape[0]}\")\ntest = df.loc[df.y.isna()]\nprint(f\"test shape {test.shape[0]}\")","66e583cc":"test.head()","b9f4b2e4":"X_trn = train.drop([\"id\",\"y\"],axis=1)\nY_trn = train[\"y\"]\nX_tst = test.drop(['id','y'],axis=1)","1c0b571b":"def auc(model, data, labels): #function to calculate AUC score\n    return (metrics.roc_auc_score(labels,model.predict_proba(data)[:,1]))","a155cedf":"from sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV\n\n# Parameter Tuning\nmodel = xgboost.XGBClassifier()\nparam_dist = {\"max_depth\": [5,6],\n              \"gamma\":[5],\n              \"reg_lambda\":[0.5],\n              \"subsample\" : [0.8],\n              \"colsample_bytree\":[0.4],\n              \"n_estimators\": [400,500],\n              \"learning_rate\": [0.005,0.001, 0.01]}\ngrid_search = GridSearchCV(model, param_grid=param_dist, cv = 3, \n                                   verbose=10, n_jobs=-1)\n# param_dist = {\"max_depth\": [1,2],\n#               \"gamma\":[0,1,5],\n#               \"subsample\" : [0.8,0.9],\n#               \"colsample_bytree\":[0.4,0.7],\n#               \"n_estimators\": [40,60,100,140],\n#               \"learning_rate\": [0.0001, 0.001, 0.01]}","25e5ac2a":"# X_trn,Y_trn = shuffle(X_trn,Y_trn)\n# grid_search.fit(X_trn,Y_trn)","f3f970a8":"grid_search.best_score_","3241b20d":"model = xgboost.XGBClassifier()","3921f03b":"model","57a59bdb":"model.set_params(max_depth=6,reg_lambda=0.3,gamma=5,n_estimators=230,subsample=0.8,\n                learning_rate=0.0065,colsample_bytree=0.7)","56a550d8":"from sklearn.utils import shuffle\ntrain_x, train_y = X_trn[:473000],Y_trn[:473000]\ntest_x, test_y = X_trn[473000:],Y_trn[473000:]\ntrain_x, train_y = shuffle(train_x, train_y)\ntest_x, test_y = shuffle(test_x, test_y)","57d36746":"#(LR,ESt): (0.01,140) --> (0.08, 180) --> (0.08, 180)","1af1e2c6":"# model.set_params(max_depth=6,reg_lambda=0.3,gamma=5,n_estimators=240,subsample=0.8,\n#                 learning_rate=0.006,colsample_bytree=0.7)\n# 0.6655715231113145 and 0.62905\n#model.set_params(max_depth=6,reg_lambda=0.4,gamma=5,n_estimators=240,subsample=0.8,\n#                learning_rate=0.005,colsample_bytree=0.7)\n#0.62841 \n\n# model.set_params(max_depth=5,reg_lambda=0.4,gamma=5,n_estimators=240,subsample=0.8,\n#                 learning_rate=0.008,colsample_bytree=0.7)\n#0.6589190615078132\n# model.set_params(max_depth=6,reg_lambda=0.35,gamma=5,n_estimators=240,subsample=0.8,\n#                 learning_rate=0.008,colsample_bytree=0.7)\n#0.660641222049973\n# model.set_params(max_depth=6,reg_lambda=0.35,gamma=5,n_estimators=230,subsample=0.9,\n#                 learning_rate=0.008,colsample_bytree=0.7)\n#0.6604407247044543\n# model.set_params(max_depth=6,reg_lambda=0.35,gamma=5,n_estimators=300,subsample=0.9,\n#                 learning_rate=0.004,colsample_bytree=0.7)\n#0.659337715341376 and 0.62825\n# model.set_params(max_depth=6,reg_lambda=0.25,gamma=5,n_estimators=350,subsample=0.9,\n#                 learning_rate=0.004,colsample_bytree=0.7)\n# 0.6602872467808225\n#adding log1p to base:\n#worsens ein tho\n","b9abc94d":"model.fit(train_x,train_y)","521c7163":"roc_auc_score(test_y,model.predict_proba(test_x)[:,1]) ","5763c0c5":"X_trn,Y_trn = shuffle(X_trn,Y_trn)\nmodel.fit(X_trn,Y_trn)","d8beced4":"preds = model.predict_proba(X_tst)[:,1]\ndataframe = pd.DataFrame({'id':test['id'],'Predicted':preds}) #preds to dataframe\ndataframe.to_csv('last.csv',index=False) #dataframe to CSV file","fee2498c":"feature_importances = model.get_feature_importance(train_pool)\nfeature_names = X.columns\nfor score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n    if score > 0.2:\n        print('{0}: {1:.2f}'.format(name, score))","2596abc5":"import shap\nshap.initjs()\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(train_pool)\n\n# visualize the training set predictions\n# SHAP plots for all the data is very slow, so we'll only do it for a sample. Taking the head instead of a random sample is dangerous! \nshap.force_plot(explainer.expected_value,shap_values[0,:300], X.iloc[0,:300])","e87905ee":"# summarize the effects of all the features\nshap.summary_plot(shap_values, X)","0147fb8e":"## todo : PDP features +- from shap","a063e49b":"test[\"Predicted\"] = model.predict(test.drop([\"id\",\"date\",\"y\"],axis=1),prediction_type='Probability')[:,1]\ntest[[\"id\",\"Predicted\"]].to_csv(\"submission.csv\",index=False)","58a295f2":"## Features importances\n","21a1953f":"## export predictions","50e8cbdb":"# Split back into train and test, and build model","bb2642ab":"* Additional features could include: rank, which bid number is the max\/min, etc' \n* features between the aggregated features (e.g. max bid div max ask..)"}}