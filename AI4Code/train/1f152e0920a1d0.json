{"cell_type":{"c14578d2":"code","6d009cc4":"code","70a64777":"code","392ebfb5":"code","ef02d964":"code","643470d6":"code","ec278acf":"code","d73f39d3":"code","a0008667":"code","d7108dfb":"code","52a748af":"code","7c5bb652":"code","6de8c383":"code","9c11977c":"code","868f0426":"code","919f8e8d":"code","756cf602":"markdown","7a0c19b0":"markdown","607c863e":"markdown","263dc321":"markdown","52f29652":"markdown","7c11ed98":"markdown","5e151c18":"markdown","21d10663":"markdown","4fa47167":"markdown","33df67dc":"markdown","1167c42d":"markdown","77ceef23":"markdown","0e55b3fd":"markdown","4f69868e":"markdown","0b604c65":"markdown","eda3188f":"markdown","fa071d5e":"markdown","485679b4":"markdown","72c81134":"markdown","56c51efe":"markdown","c6a62f88":"markdown","f9fb5555":"markdown"},"source":{"c14578d2":"from itertools import zip_longest\nfrom typing import List, Tuple, Callable, Optional\n\n# Vector Operations\nimport numpy as np\n\n# For Generating Datasets\nfrom sklearn.datasets import make_circles, make_moons, make_blobs, make_gaussian_quantiles\nfrom sklearn.model_selection import train_test_split\n\n# Progress Bar\nfrom tqdm.auto import tqdm\n\n# Plotting Diagrams\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n\n# Plotting Animations\nfrom matplotlib.animation import FuncAnimation\nfrom IPython.display import HTML","6d009cc4":"# Increase figure size\nplt.rcParams[\"figure.figsize\"] = (12, 8)","70a64777":"ALL_DATA_TYPE = [\"checkboard\", \"circle\", \"mul_circle\", \"moon\", \"blob\", \"spiral\"]\n\ndef gen_data(name, per_class_data_cnt=500, class_cnt=3, show_data=False):\n    n_samples = per_class_data_cnt * class_cnt\n    if name == \"circle\":\n        class_cnt = 2\n        X, Y = make_circles(n_samples=n_samples, noise=0.2, factor=0.3)\n    elif name == \"mul_circle\":\n        X, Y = make_gaussian_quantiles(n_samples=n_samples, n_features=2, n_classes=class_cnt)\n    elif name == \"moon\":\n        class_cnt = 2\n        X, Y = make_moons(n_samples=n_samples, noise=0.1)\n    elif name == \"blob\":\n        X, Y = make_blobs(n_samples=n_samples, n_features=2, centers=class_cnt)\n    elif name == \"checkboard\":\n        class_cnt = 2\n        X = np.zeros((n_samples, 2))\n        Y = np.zeros(n_samples)\n        offsets = [((1, 1), 0), ((1, -1), 1), ((-1, 1), 1), ((-1, -1), 0)]\n        for bid, (offset, y) in enumerate(offsets):\n            idx = range(per_class_data_cnt*bid\/\/2, per_class_data_cnt*(bid+1)\/\/2)\n            X[idx] = (np.random.rand(per_class_data_cnt\/\/2, 2) + 0.05) * np.array(offset)\n            Y[idx] = y\n    elif name == \"spiral\":\n        X = np.zeros((n_samples, 2))\n        Y = np.zeros(n_samples)\n        for cid in range(class_cnt):\n            r = np.linspace(0.0, 1, per_class_data_cnt) # radius\n            t = np.linspace(cid*4, (cid+1)*4, per_class_data_cnt) + np.random.randn(per_class_data_cnt)*0.3 # theta\n            idx = range(per_class_data_cnt*cid, per_class_data_cnt*(cid+1))\n            X[idx] = np.c_[r*np.sin(t), r*np.cos(t)]\n            Y[idx] = cid\n    else:\n        raise ValueError(\"Unknown Data Name!\")\n\n    if show_data:\n        plt.scatter(X[:, 0], X[:, 1], c=Y, s=20)\n        plt.show()\n        \n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n    X_train, X_valid, Y_train, Y_valid = train_test_split(X_train, Y_train, test_size=0.25)\n\n    # Change index to one hot (Assumes index 0 ~ class_cnt)\n    # [1, 2, 0, 1] -> [[0, 1, 0], [0, 0, 1], [1, 0, 0], [0, 1, 0]]\n    def one_hot(x, class_cnt):\n        oh = np.zeros((len(x), class_cnt))\n        oh[np.arange(len(x)), [int(i) for i in x]] = 1\n        return oh.reshape(-1, class_cnt, 1)\n\n    training_data = list(zip(X_train.reshape(-1, 2, 1), one_hot(Y_train, class_cnt)))\n    validation_data = list(zip(X_valid.reshape(-1, 2, 1), one_hot(Y_valid, class_cnt)))\n    testing_data = list(zip(X_test.reshape(-1, 2, 1), one_hot(Y_test, class_cnt)))\n    \n    return class_cnt, training_data, validation_data, testing_data","392ebfb5":"class_cnt, training_data, validation_data, testing_data = gen_data(\"spiral\", class_cnt=5, show_data=True)","ef02d964":"# Activation Functions\nclass AF():\n    @staticmethod\n    def linear(x, d=False):\n        return 1 if d else x\n\n    @staticmethod\n    def sigmoid(x, d=False):\n        ### TODO ###\n        pass\n\n    @staticmethod\n    def tanh(x, d=False):\n        ### TODO ###\n        pass\n\n    @staticmethod\n    def relu(x, d=False):\n        ### TODO ###\n        pass\n\n    @staticmethod\n    def leaky_relu(x, d=False):\n        ### TODO ###\n        pass\n\n    @staticmethod\n    def selu(x, d=False):\n        ### TODO ###\n        pass\n\n    @staticmethod\n    def softmax(x, d=False):\n        if d:\n            print(\"We should not need this!\")\n            return -1\n        else:\n            # Need to use stable softmax\n            ### TODO ###\n            pass","643470d6":"# Loss Functions\nclass LF():\n    @staticmethod\n    def softmax_crossentropy(activation_output, y, d=False):\n        if d:\n            ### TODO ###\n            pass\n        else:\n            ### TODO ###\n            pass","ec278acf":"class SimpleNN():\n\n    def __init__(self, structure: List[int], af: Callable):\n        # Save the model structure\n        self.layer_structure = structure\n        self.num_layers = len(structure)\n        self.af = af\n\n        # Initialize the weight and bias\n        # Weight Initialization: http:\/\/cs231n.github.io\/neural-networks-2\/\n        # For weights:\n        #   - Standard Normal: np.random.randn\n        #   - Normalized Standard Normal: np.random.randn \/ np.sqrt(<previous_layer_neuron_cnt>)\n        #   - Gaussian Normal: np.random.normal\n        # For bias:\n        #   - All zeros: np.zeros\n        #   - Standard Normal: np.random.randn\n        #   - Gaussian Normal: np.random.normal\n        self.weights = [np.random.randn(nl, pl) for pl, nl in zip(structure[:-1], structure[1:])]\n        self.biases = [np.zeros((l, 1)) for l in structure[1:]]\n\n    def empty_structure(self):\n        # Return empty structure for weight and bias\n        return [np.zeros(w.shape) for w in self.weights], [np.zeros(b.shape) for b in self.biases]\n\n    def activation_func(self, l: int, *args, **kargs):\n        # Use softmax on last layer\n        return AF.softmax(*args, **kargs) if l == self.num_layers-1 else self.af(*args, **kargs)\n\n    def loss_func(self, *args, **kargs):\n        return LF.softmax_crossentropy(*args, **kargs)\n\n    def forward(self, x, return_activations=False):\n        activations, before_activations = [x], []\n\n        # Run through each layer\n        ### TODO ###\n        #  Use self.activation_func\n\n        # Return value history for backprop usage\n        return (activations, before_activations) if return_activations else activations[-1]\n\n    def backprop(self, x, y):\n        # Init empty data structure to store delta\n        delta_w, delta_b = self.empty_structure()\n        \n        # Feed forward pass\n        activations, before_activations = self.forward(x, return_activations=True)\n\n        # Backward pass\n        ### TODO ###\n        # Use self.activation_func, self.loss_func\n\n        return delta_w, delta_b\n        \n    def update_one_batch_optimizer(self, mini_batch, optimizer):\n        # Init empty data structure to store delta\n        batch_delta_w, batch_delta_b = self.empty_structure()\n    \n        # Run through the batch of data\n        for x, ohy in mini_batch:\n            delta_w, delta_b = self.backprop(x, ohy)\n            batch_delta_w = [bd+d for bd, d in zip(batch_delta_w, delta_w)]\n            batch_delta_b = [bd+d for bd, d in zip(batch_delta_b, delta_b)]\n        \n        # Average the delta among the same batch\n        batch_delta_w = [bd\/len(mini_batch) for bd in batch_delta_w]\n        batch_delta_b = [bd\/len(mini_batch) for bd in batch_delta_b]\n        \n        # Change the weights and biases by using the optimizer\n        self.weights, self.biases = optimizer.step(self.weights, self.biases, batch_delta_w, batch_delta_b)\n \n    def loss_one(self, x, y):\n        return self.loss_func(self.forward(x), y)\n    \n    def loss(self, evaluate_data):\n        return sum([self.loss_one(x, y) for x, y in evaluate_data]) \/ len(evaluate_data)\n\n    def evaluate_one(self, x):\n        return np.argmax(self.forward(x))\n\n    def evaluate(self, evaluate_data):\n        evaluation_result = [(x, np.argmax(y), self.evaluate_one(x)) for x, y in evaluate_data]\n        accuracy = sum([y==pred for x, y, pred in evaluation_result]) \/ len(evaluate_data)\n        return accuracy, evaluation_result","d73f39d3":"class Optimizer():\n    def __init__(self, empty_model_structure):\n        # Save the empty model structure\n        self.empty_model = model.empty_structure()\n\n    def step(weights, biases, batch_delta_w, batch_delta_b):\n        # Update Weight and Bias accordingly to the average of deltas of the whole batch\n        weights = [w+bd for w, bd in zip(weights, batch_delta_w)]\n        biases = [b+bd for b, bd in zip(biases, batch_delta_b)]\n        return weights, biases\n\nclass SGD(Optimizer):\n    def __init__(self, empty_model_structure,\n                 lr: float = 0.01):\n        # Call Optimizer's init\n        super(SGD, self).__init__(empty_model_structure)\n        # Save Parameters\n        self.lr = lr\n\n    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n        # Multiply delta by lr\n        ### TODO ###\n\n        # Update Weight and Bias accordingly\n        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n        return weights, biases\n\nclass SGDMomentum(Optimizer):\n    def __init__(self, empty_model_structure,\n                 lr: float = 0.01, momentum: float = 0.9):\n        # Call Optimizer's init\n        super(SGDMomentum, self).__init__(empty_model_structure)\n        # Save Parameters\n        self.lr = lr\n        self.momentum = momentum\n        # Initial momentum storage\n        self.momentum_w, self.momentum_b = self.empty_model\n\n    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n        # Multiply delta by lr and add previous batch momentum\n        ### TODO ###\n\n        # Save current delta for future momentum usage\n        ### TODO ###\n\n        # Update Weight and Bias accordingly\n        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n        return weights, biases\n\nclass SGDNesterovMomentum(Optimizer):\n    def __init__(self, empty_model_structure,\n                 lr: float = 0.01, momentum: float = 0.9):\n        # Call Optimizer's init\n        super(SGDNesterovMomentum, self).__init__(empty_model_structure)\n        # Save Parameters\n        self.lr = lr\n        self.momentum = momentum\n        # Initial momentum storage\n        self.momentum_w, self.momentum_b = self.empty_model\n\n    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n        # Save current momentum first\n        prev_momentum_w, prev_momentum_b = self.momentum_w, self.momentum_b\n\n        # Multiply delta by lr and add previous batch momentum\n        ### TODO ###\n\n        # Save current delta for future momentum usage\n        ### TODO ###\n\n        # Modify the delta to accomplish look ahead\n        ### TODO ###\n        \n        # Update Weight and Bias accordingly\n        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n        return weights, biases\n\nclass Adagrad(Optimizer):\n    EPS = 1e-6\n    def __init__(self, empty_model_structure,\n                 lr: float = 0.01):\n        # Call Optimizer's init\n        super(Adagrad, self).__init__(empty_model_structure)\n        # Save Parameters\n        self.lr = lr\n        # Initial leraning rate storage\n        self.lr_w, self.lr_b = self.empty_model\n        \n    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n        # Modify learning rate for each parameter according to gradient\n        ### TODO ###\n\n        # Multiply by lr with respect to learning rate of each parameter\n        ### TODO ###\n\n        # Update Weight and Bias accordingly\n        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n        return weights, biases\n    \nclass RMSprop(Optimizer):\n    EPS = 1e-6\n    def __init__(self, empty_model_structure,\n                 lr: float = 0.01, decay_rate: float = 0.99):\n        # Call Optimizer's init\n        super(RMSprop, self).__init__(empty_model_structure)\n        # Save Parameters\n        self.lr = lr\n        self.decay_rate = decay_rate\n        # Initial leraning rate storage\n        self.lr_w, self.lr_b = self.empty_model\n\n    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n        # Modify learning rate for each parameter according to gradient\n        ### TODO ###\n\n        # Multiply by lr with respect to learning rate of each parameter\n        ### TODO ###\n\n        # Update Weight and Bias accordingly\n        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n        return weights, biases\n    \nclass Adam(Optimizer):\n    EPS = 1e-8\n    def __init__(self, empty_model_structure,\n                 lr: float = 0.01, beta1: float = 0.9, beta2: float = 0.999, bias_correction: bool = True):\n        # Call Optimizer's init\n        super(Adam, self).__init__(empty_model_structure)\n        # Save Parameters\n        self.lr = lr\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.bias_correction = bias_correction\n        # Initial leraning rate and momentum storage\n        self.lr_w, self.lr_b = self.empty_model\n        self.momentum_w, self.momentum_b = self.empty_model\n        if self.bias_correction:\n            # Save the amount of steps ran\n            self.optimize_steps = 1\n\n    def step(self, weights, biases, batch_delta_w, batch_delta_b):\n        # Modify momentum according to delta with decay\n        ### TODO ###\n\n        # Modify learning rate for each parameter according to gradient\n        ### TODO ###\n\n        # Multiply by lr with respect to learning rate of each parameter\n        ### TODO ###\n\n        # Update Weight and Bias accordingly\n        weights = [w+bd for w, bd in zip(weights, ret_batch_delta_w)]\n        biases = [b+bd for b, bd in zip(biases, ret_batch_delta_b)]\n        return weights, biases","a0008667":"class StopScheduler():\n    def __init__(self):\n        pass\n    \n    def step(self, validation_loss):\n        # Never Stop\n        return False\n\nclass EarlyStopScheduler(StopScheduler):\n    def __init__(self, patience=10, threshold=1e-4):\n        # Save Parameters\n        self.patience = patience\n        self.threshold = threshold\n        # Save past loss\n        self.past_loss = []\n\n    def step(self, validation_loss):\n        # Append loss to past loss\n        self.past_loss.append(validation_loss)\n\n        # Early stop if no loss improved more than threshold% in the last patience steps\n        ### TODO ###\n\n        return False","d7108dfb":"class LRScheduler():\n    def __init__(self):\n        pass\n    \n    def step(self, current_lr, validation_loss):\n        # Return the same learning rate\n        return current_lr\n\nclass ReduceLROnPlateau(LRScheduler):\n    def __init__(self, decay_factor=0.5, patience=5, min_lr=0.0001, threshold=1e-4):\n        # Save Parameters\n        self.decay_factor = decay_factor\n        self.patience = patience\n        self.min_lr = min_lr\n        self.threshold = threshold\n        # Save past loss\n        self.past_loss = []\n\n    def step(self, current_lr, validation_loss):\n        # Append loss to past loss\n        self.past_loss.append(validation_loss)\n\n        # Decay learning rate if no loss improved more than threshold% in the last patience steps\n        ### TODO ###\n\n        return current_lr","52a748af":"# Group iterable into size of n for mini-batch\n# [1, 2, 3, 4, 5] -> n=2 -> [[1, 2], [3, 4], [5]]\ndef grouper(iterable, n):\n    ### TODO ###\n    return []","7c5bb652":"def train_model(model: SimpleNN,\n                optimizer: Optimizer,\n                lr_scheduler: LRScheduler,\n                earlystop_scheduler: StopScheduler,\n                training_data, validation_data,\n                epochs: int = 100,\n                mini_batch_size: int = 10,\n                log_period: int = 10,\n                draw_process: bool = False):\n\n    # Record loss and accuracy\n    loss_his, acc_his  = [], []\n    # If draw_process, store evaluation history and boundary table\n    if draw_process:\n        print(\"Draw process will take time to eval models and save boundary!\")\n        print(\"Use larger log periods to save time!\")\n        eval_his = []\n\n    # Run through the epochs\n    progress_bar = tqdm(range(epochs))\n    for epoch in progress_bar:\n        # Create batches using grouper function and run through the batches\n        for mini_batch in grouper(training_data, mini_batch_size):\n            model.update_one_batch_optimizer(mini_batch, optimizer)\n\n        # Calculate loss and accuracy every counter epochs\n        if epoch%log_period == 0:\n            loss_his.append((model.loss(training_data), model.loss(validation_data)))\n            te, ve = model.evaluate(training_data), model.evaluate(validation_data)\n            acc_his.append((te[0], ve[0]))\n            if draw_process:\n                eval_his.append((calculate_model_boundary(model, te[1]), calculate_model_boundary(model, ve[1])))\n\n            # Update Progress Bar Description\n            desc = f\"Train Loss: {loss_his[-1][0]:.3f}, Accuracy: {acc_his[-1][0]:.3f}\"\n            progress_bar.set_description(desc)\n            \n            # Check if Early Stopping is needed using validation data\n            if earlystop_scheduler.step(loss_his[-1][1]):\n                print(\"Early Stopped!\")\n                return (loss_his, acc_his, eval_his) if draw_process else (loss_his, acc_his)\n                \n            # Update learning rate according to validation loss \n            optimizer.lr = lr_scheduler.step(optimizer.lr, loss_his[-1][1])\n\n    return (loss_his, acc_his, eval_his) if draw_process else (loss_his, acc_his)","6de8c383":"def calculate_model_boundary(model: SimpleNN, evaluation_result):\n    # Separate evaluation_result into buckets\n    correct, error = [[], [], []], [[], [], []]\n    for X, Y, pred in evaluation_result:\n        bucket = correct if Y == pred else error\n        bucket[0].append(X[0].item())\n        bucket[1].append(X[1].item())\n        bucket[2].append(Y)\n    # Calculate model boundary limits\n    x_min, x_max = min(correct[0]+error[0]), max(correct[0]+error[0])\n    y_min, y_max = min(correct[1]+error[1]), max(correct[1]+error[1])\n    step = max(x_max-x_min, y_max-y_min)\/100\n    xx, yy = np.meshgrid(np.arange(x_min-step*10, x_max+step*10, step), np.arange(y_min-step*10, y_max+step*10, step))\n    # Evaluate mesh grid points on model\n    z = np.array([model.evaluate_one(x.reshape(-1, 1)) for x in np.c_[xx.ravel(), yy.ravel()]]).reshape(xx.shape)\n    return (correct, error, xx, yy, z)\n\ndef draw_model_boundary(model_boundary, title: str = \"Metric\", prev_ax = None):\n    correct, error, xx, yy, z = model_boundary\n    # Init Plot if no ax\n    if prev_ax is None:\n        fig, ax = plt.subplots()\n    else:\n        ax = prev_ax\n    cmap=plt.cm.Spectral\n    # Plot boundary\n    ax.contourf(xx, yy, z, alpha=0.4, cmap=cmap)\n    # Plot Correct and Error Points from evaluation_result\n    ax.scatter(correct[0], correct[1], c=correct[2], s=30, marker='o', label='correct', cmap=cmap)\n    ax.scatter(error[0], error[1], c=error[2], s=30, marker='x', label='wrong', cmap=cmap)\n    # Set the limit on the chart\n    ax.set_xlim([xx.min(), xx.max()])\n    ax.set_ylim([yy.min(), yy.max()])\n    # Show legends\n    ax.legend()\n    if prev_ax is None:\n        plt.show()\n\ndef draw_metrics_history(metrics: List[Tuple[float, ...]], metric_names: Tuple[str, ...],\n                         current_step: int, title: str = \"Metric\", prev_ax = None,\n                         y_lim: Optional[List[float]] = None, tolog=False, ma_step=2):\n    # Init Plot if no ax\n    if prev_ax is None:\n        fig, ax = plt.subplots()\n    else:\n        ax = prev_ax\n    # Plot each of the lines\n    assert len(metrics[0]) == len(metric_names), \"Each metric shall have a name!\"\n    # Get data of each metric\n    metric_datas = np.log(np.array(metrics).T) if tolog else np.array(metrics).T\n    for metric_name, metric_data in zip(metric_names, metric_datas):\n        # Calculate moving average for better plot\n        mov_avg_metric = np.concatenate([metric_data[:ma_step-1], np.convolve(metric_data, np.ones(ma_step), 'valid') \/ ma_step])\n        ax.plot(mov_avg_metric[:current_step+1], label=metric_name)\n    # Set the limit on the chart\n    ax.set_xlim([0, len(metrics)-1])\n    if y_lim is not None:\n        ax.set_ylim(y_lim)\n    else:\n        ax.set_ylim([metric_datas.min()-0.5, metric_datas.max()+0.5])\n    # Set title\n    ax.set_title(title)\n    ax.legend()\n    if prev_ax is None:\n        plt.show()\n\ndef draw_process(model, loss_his, acc_his, eval_his, train=False, interval=200):\n    # Get the history of train or valid\n    evaluate_data_id = 0 if train else 1\n    evaluate_data = [his[evaluate_data_id] for his in eval_his]\n    print(\"This will take some time to plot!\")\n    print(f\"Total plots: {len(evaluate_data)}\")\n    # Prepare the animation\n    fig = plt.figure(constrained_layout=True)\n    gs = gridspec.GridSpec(ncols=2, nrows=5, figure=fig)\n    ax1 = fig.add_subplot(gs[:-1, :])\n    ax2 = fig.add_subplot(gs[-1, 0])\n    ax3 = fig.add_subplot(gs[-1, 1])\n    def update(i):\n        # Draw Model Boundary\n        ax1.clear()\n        ax1_title = f'Train Data Evaluation History: {i:04}' if train else f'Validation Data Evaluation History: {i:04}'\n        draw_model_boundary(evaluate_data[i], ax1_title, prev_ax=ax1)\n        # Draw Loss Change\n        ax2.clear()\n        ax2_title = \"Loss History (Y is log of loss)\"\n        draw_metrics_history(loss_his, (\"Train\", \"Valid\"), i, ax2_title, prev_ax=ax2, ma_step=2, tolog=True)\n        # Draw Accuracy Change\n        ax3.clear()\n        ax3_title = \"Accuracy History\"\n        draw_metrics_history(acc_his, (\"Train\", \"Valid\"), i, ax3_title, prev_ax=ax3, ma_step=2, y_lim=[0, 1])\n    anim = FuncAnimation(fig, update, frames=range(0, len(evaluate_data)), interval=interval, blit=False)\n    return HTML(anim.to_jshtml())","9c11977c":"# Generate the dataset\nclass_cnt, training_data, validation_data, testing_data = gen_data(\"mul_circle\")\nprint(f\"Class Count: {class_cnt}\")\n\n# Setup the model structure\nnetwork = [2] + [4, 4] + [class_cnt]  # Input Dimension + Layers + Output Class Count\nmodel = SimpleNN(network, AF.relu)\n\n# Setup the optimzer and schedulers\noptimizer = SGD(model.empty_structure(), lr=0.01)\nlr_scheduler = LRScheduler()\nstop_scheduler = StopScheduler()\n\n# Run the training process\n# loss_his, acc_his, eval_his = train_model(model, optimizer, lr_scheduler, stop_scheduler,\n#                                           training_data, validation_data,\n#                                           epochs=100, mini_batch_size=10, log_period=10, draw_process=True)","868f0426":"# Draw the training process\n# draw_process(model, loss_his, acc_his, eval_his)","919f8e8d":"# Final evaluation on testing data\n# accuracy, evaluation_result = model.evaluate(testing_data)\n# print(\"Accuracy:\", accuracy)\n# draw_model_boundary(calculate_model_boundary(model, evaluation_result))","756cf602":"#### Try out different Gradient Descent Methods\n- Must: Batch, Stochastic, Mini-batch with different batch size","7a0c19b0":"### Basic Run","607c863e":"### There are 6 generated datasets that you can use to try out your model\n\n- Must try:\n    - checkboard: XOR like Pattern\n    - mul_circle: class_cnt rings of Data\n    - spiral: Spiral Shape (From CS231n)\n- Others:\n    - moon: Two semi-circle\n    - circle: Two rings of Data\n    - blob: class_cnt Guassian Distribution","263dc321":"### The NN Model","52f29652":"#### Try out different daasets\n- Must: \"checkboard\", \"spiral\", \"mul_circle\"\n- Others: \"circle\", \"moon\", \"blob\"","7c11ed98":"* ### All the Optimzers\n\nhttp:\/\/cs231n.github.io\/neural-networks-3\/","5e151c18":"### Experiments\n\nDo some experiments on the effect of different settings","21d10663":"#### Try out Different Optimizers\n- Must: SGD, SGD+Momentum, RMSprop, Adam\n- Others: SGD+NesterovMomentum, Adagrad","4fa47167":"#### Must: Try out Early Stopping","33df67dc":"#### Others: Try out different weight and bias initialization\n- Eg. weights: Standard Normal, Normalized Standard Normal, Gaussian Normal\n- Eg. bias: All zeros, Standard Normal, Gaussian Normal","1167c42d":"### The Main Training Loop\n\nThis is where the whole training process comes together","77ceef23":"#### Try out different activation functions\n- Must: AF.linear, AF.sigmoid, AF.tanh, AF.relu\n- Others: AF.leaky_relu, AF.selu ...","0e55b3fd":"#### Must: Try out Learning Rate Adjustment","4f69868e":"### Demo of Spiral Data","0b604c65":"### How to apporach this assignment\n\nYou can find all the blanks with the `### TODO ###` tag.\n\n#### Current Structure of the codebase\n- Toy Dataset Generation: gen_data (checkboard, mul_circle, spiral, moon, circle, blob)\n- Activation Functions Class: AF (linear, sigmoid, tanh, relu, softmax, leaky_relu, selu)\n- Loss Function Class: LF (We will only implement Softmax + Cross Entropy this time)\n- The NN Model: SimpleNN (Defined a NN with list of neuron count and activation function)\n- Optimizer Classes: Optimizer (SGD, SGDMomentum, SGDNesterovMomentum, Adagrad, RMSprop, Adam)\n- Training Scheduler: StopScheduler (EarlyStopScheduler)\n- Learning Rate Scheduler: LRScheduler (ReduceLROnPlateau)\n- Utility Functions: grouper (Group data into mini-batches)\n- Training Loop: train_model (The main function to train a model)\n- Animation functions for the training process: draw_process, calculate_model_boundary, draw_model_boundary, draw_metrics_history\n    - You can skip understanding these\n\n#### Step 1\n- Get an understanding of the Training Loop\n    - We first call gen_data to get: class_cnt, training_data, validation_data, testing_data\n    - We define our model structure and inital our SimpleNN model\n    - We define our optimizer, lr_scheduler, stop_scheduler\n    - We run through train_model with (model, optimizer, lr_scheduler, stop_scheduler, training_data, validation_data)\n    - We can look at training process with draw_process\n    - We evaluate our model through model.evaluate and look at accuracy and model boundary using draw_model_boundary\n\n#### Step 2\n- Understand the SimpleNN model\n    - `__init__` will init the weights and biases\n    - `empty_structure` will return an empty model will all weight and biases to be zero (Will be helpful later on)\n    - `activation_func` and `loss_func` are wrappers functions to call the real functions\n    - `forward` and `backprop` are the forward and backward pass of the neural net\n        - `return_activations` in the forward pass is for usage in the backward pass\n        - We return the delta of weight and biases for this x and y in `backprop`, this will be passed to the optimzer to decide the real delta\n    - `update_one_batch_optimizer` is for getting the average delta to the model in the mini-batch\n        - We will pass the delta to the optimizer and let it decide how it will change the weight and biases\n    - `loss_one` and `loss` is for returning the loss\n    - `evaluate_one` and `evaluate` is for returning the evaluation result\n\n#### Step 3\n- Make the `Basic Run` loop work by filling in some blanks\n    - Implement forward\/backward pass in SimpleNN\n        - The forward and backward pass needs to be in vector form\n    - Implement one of the Activation Functions (Eg. Relu)\n        - The parameter d means derivative\n    - Implement the Loss Function\n        - We implement Softmax + Cross Entropy combined loss\n    - Implement SGD to have one working optimizer\n        - Naive SGD will just be to multiply the delta by the learning rate and update the weight and biases\n    - Implement grouper function for making mini-batches\n\n#### Step 4\n- We shall be able to run through the `Basic Run` loop and see the training process happen!\n\n#### Step 5\n- Implment other Activation Functions\n    - linear, sigmoid, tanh, relu, softmax, leaky_relu, selu\n- Implement other Optimizers\n    - SGDMomentum, SGDNesterovMomentum, Adagrad, RMSprop, Adam\n- Implement other weights and biases initalization methods\n    - Normalized Standard Normal\n- Implment early stopping and learning rate decay\n    - EarlyStopScheduler\n    - ReduceLROnPlateau\n    \n#### Step Final\n- Finish with the experiments detailed at the bottom\n    - Try out different Toy Datasets\n        - Must: \"checkboard\", \"spiral\", \"mul_circle\"\n        - Others: \"circle\", \"moon\", \"blob\"\n    - Try out different activation functions\n        - Must: AF.linear, AF.sigmoid, AF.tanh, AF.relu\n        - Others: AF.leaky_relu, AF.selu ...\n    - Try out Different Optimizers\n        - Must: SGD, SGD+Momentum, RMSprop, Adam\n        - Others: SGD+NesterovMomentum, Adagrad\n    - Try out different Gradient Descent Methods\n        - Must: Batch, Stochastic, Mini-batch with different batch size\n    - Try out different model structures (deep vs shallow \/ wide vs thin)\n        - Must: (4)x1, (2)x4, (4)x2\n        - Others: (4)x3 ...\n    - Try out other weights and biases initalization methods\n    - Try out early stopping\n    - Try out learning rate decay","eda3188f":"### Learning Rate Scheduler","fa071d5e":"#### Try out different model structures (deep vs shallow \/ wide vs thin)\n- Must: (4)x1, (2)x4, (4)x2\n- Others: (4)x3 ...","485679b4":"### Import Libaries\nSince we imported numpy, you shall not need any more libaries","72c81134":"### Early Stopping Scheduler","56c51efe":"### Utility Function","c6a62f88":"### List of Activation and Loss Functions","f9fb5555":"### These are code use to draw the animations\n\nYou can skip understanding them..."}}