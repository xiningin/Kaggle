{"cell_type":{"8aa1bb6e":"code","5338507b":"code","145f894f":"code","982b0c25":"code","d3146952":"code","7d9241e2":"code","fd0e3ec9":"code","25442633":"code","5dc0b51e":"code","cb70ce95":"code","d2abc499":"code","d32014a7":"code","63538dbd":"code","7ad4c519":"code","6d6a0947":"code","65c02324":"code","6248cb77":"code","fd7afff9":"code","d431188c":"code","3913ddff":"code","f3eb4e6a":"code","5571e8b3":"code","a4e461ce":"code","50891ad7":"code","19a062ba":"code","323393f0":"code","846700dd":"code","ff84c102":"code","b2dc2d12":"code","2961dfa0":"code","ccab4944":"code","1fea7059":"code","a78051cc":"code","d0960f9e":"code","331bceb0":"code","1fc50316":"code","32d05d1a":"code","77434839":"code","72b64ea8":"code","f716548d":"code","a4b10015":"code","074aee00":"code","a2768e35":"code","02a24065":"code","1cb1c5ba":"code","05145478":"code","d28dac71":"code","f8c5a302":"code","f02f9e7e":"code","d1c881ec":"code","43705caa":"code","0724eb0e":"code","c2019e07":"code","1565c47f":"code","eb2e1392":"code","6191f929":"code","6520e9a4":"code","7648c65a":"code","e65828f0":"code","f475f723":"code","e20026db":"code","172b1ccd":"code","298e0ce8":"code","f897fa06":"code","cb190231":"markdown","03514bdb":"markdown","28b3fefa":"markdown","f01bb7b7":"markdown","4f6cf46e":"markdown","3367448f":"markdown","bef105f3":"markdown","61f0c991":"markdown","90d619bb":"markdown","7865341d":"markdown","62aeec11":"markdown","3dca8581":"markdown","e1ebac65":"markdown","7e0bc496":"markdown","6f71f2fe":"markdown","e9ffd278":"markdown","1b0a761f":"markdown","712707ce":"markdown","26317723":"markdown","4fdeff80":"markdown","7ee36ce0":"markdown","513ee4e6":"markdown","bc46d264":"markdown","a1638392":"markdown","5b3b2c68":"markdown","ee5a6622":"markdown","b45cf10a":"markdown","05ea6c88":"markdown","35a6d579":"markdown","72dab537":"markdown","4b0107d2":"markdown","c77314cf":"markdown","9ce68180":"markdown","d783fc4f":"markdown","bba54695":"markdown"},"source":{"8aa1bb6e":"Step 1: Reading and Understanding the Data\nStep 2: Cleaning the Data\n        Missing Value check\n        Data type check\n        Duplicate check\nStep 3: Data Visualization\n        Boxplot\n        Pairplot\nStep 4: Data Preparation\n        Dummy Variable\nStep 5: Splitting the Data into Training and Testing Sets\n        Rescaling\nStep 6: Building a Linear Model\n        RFE\n        VIF\nStep 7: Residual Analysis of the train data\nStep 8: Making Predictions Using the Final Model\nstep 9: RMSE","5338507b":"# import all libraries and dependencies for dataframe\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom statsmodels.graphics.regressionplots import influence_plot\nimport matplotlib.pyplot as plt\ndata = pd.read_csv(\"..\/input\/toyota-corolla\/Toyoto_Corrola.csv\")\ndata.head()","145f894f":"data_1=data\ndata_1.head()","982b0c25":"#Understanding the dataframe\ndata_1.shape","d3146952":"# information of the data\ndata_1.info()","7d9241e2":"# description of the data\ndata_1.describe()","fd0e3ec9":"# dropping car_ID based on business knowledge\ndata_1=data.drop(['Model', 'Id'], axis = 1) ","25442633":"# Datatypes\ndata_1.dtypes","5dc0b51e":"# Outlier Analysis of target variable with maximum amount of Inconsistency\n\noutliers = ['Price']\nplt.rcParams['figure.figsize'] = [8,8]\nsns.boxplot(data=data_1[outliers], orient=\"v\", palette=\"Set1\" ,whis=1.5,saturation=1, width=0.7)\nplt.title(\"Outliers Variable Distribution\", fontsize = 14, fontweight = 'bold')\nplt.ylabel(\"Price Range\", fontweight = 'bold')\nplt.xlabel(\"Continuous Variable\", fontweight = 'bold')\ndata_1.shape","cb70ce95":"# checking for duplicates\ndata_1.loc[data_1.duplicated()]","d2abc499":"data_1=data_1.drop_duplicates()","d32014a7":"# Visualizing the different car names available\n\nplt.rcParams['figure.figsize'] = [15,8]\nax=data_1['Age_08_04'].value_counts().plot(kind='bar',stacked=True, colormap = 'Set1')\nax.title.set_text('CarAge')\nplt.xlabel(\"Age of the Car\",fontweight = 'bold')\nplt.ylabel(\"Count of Cars\",fontweight = 'bold')","63538dbd":"plt.figure(figsize=(8,8))\nplt.title('Car Price Distribution Plot')\nsns.distplot(data_1['Price'])","7ad4c519":"ax = sns.pairplot(data_1)","6d6a0947":"plt.figure(figsize=(20, 15))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'Doors', y = 'Price', data = data_1)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'Cylinders', y = 'Price', data = data_1)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'Gears', y = 'Price', data = data_1)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'HP', y = 'Price', data = data_1)\nplt.show()","65c02324":"plt.figure(figsize=(20, 6))\nplt.hist(data_1['KM'],facecolor =\"peru\",edgecolor =\"blue\",bins =100)\nplt.ylabel(\"Frequency\");\nplt.xlabel(\" Total KM\")\nplt.show()","6248cb77":"plt.figure(figsize=(20, 6))\nplt.hist(data_1['Weight'],facecolor =\"yellow\",edgecolor =\"blue\",bins =15)\nplt.ylabel(\"Frequency\");\nplt.xlabel(\" Total Weight\")\nplt.show()","fd7afff9":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Doors', y = 'Price', hue = 'Gears', data = data_1)\nplt.show()","d431188c":"plt.figure(figsize=(20, 4))\n\ndf_autox = pd.DataFrame(data_1.groupby(['Doors'])['Price'].mean().sort_values(ascending = False))\ndf_autox.plot.bar()\nplt.title('Car Doors vs Average Price')\nplt.show()","3913ddff":"# Get the dummy variables for the categorical feature and store it in a new variable - 'dummies'\n\ndummies = pd.get_dummies(data_1['Doors'])\ndummies.shape","f3eb4e6a":"dummies = pd.get_dummies(data_1['Gears'])\ndummies.shape","5571e8b3":"data_2=pd.get_dummies(data_1,columns=['Doors','Gears'])","a4e461ce":"data_2.head()","50891ad7":"from sklearn.model_selection import train_test_split\ndf_train, df_test = train_test_split(data_2, train_size = 0.7, test_size = 0.3, random_state = 100)","19a062ba":"df_train.head()","323393f0":"from sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = preprocessing.StandardScaler()","846700dd":"import warnings\nwarnings.filterwarnings(\"ignore\")\nsig_num_col=['Age_08_04','KM','HP','Cylinders','Weight','Price']\ndf_train[sig_num_col] = scaler.fit_transform(df_train[sig_num_col])","ff84c102":"df_train.head()","b2dc2d12":"# Let's check the correlation coefficients to see which variables are highly correlated\nplt.figure(figsize = (10, 10))\nsns.heatmap(df_train.corr(), cmap=\"RdYlGn\")\nplt.show()","2961dfa0":"df_train.corr()","ccab4944":"import seaborn as sns\nfig = plt.figure(figsize=(10,3))\nsns.set_style(style='darkgrid')\nsns.pairplot(data_1)","1fea7059":"y_train = df_train.pop('Price')\nX_train = df_train","a78051cc":"X_train_1 = X_train['Age_08_04']","d0960f9e":"import statsmodels.api as sm\n# Add a constant\nX_train_1c = sm.add_constant(X_train_1)\n# Create a first fitted model\nlr_1 = sm.OLS(y_train, X_train_1c).fit()","331bceb0":"# Check parameters created\nlr_1.params","1fc50316":"# Let's visualise the data with a scatter plot and the fitted regression line\n\nplt.scatter(X_train_1c.iloc[:, 1], y_train)\nplt.plot(X_train_1c.iloc[:, 1], -8.735410e-01*X_train_1c.iloc[:, 1]+-6.765422e-17, 'r')\nplt.show()","32d05d1a":"# Print a summary of the linear regression model obtained\nprint(lr_1.summary())","77434839":"X_train_2 = X_train[['Age_08_04', 'KM']]\n","72b64ea8":"# Add a constant\nX_train_2c = sm.add_constant(X_train_2)\n# Create a second fitted model\nlr_2 = sm.OLS(y_train, X_train_2c).fit()","f716548d":"lr_2.params","a4b10015":"print(lr_2.summary())","074aee00":"X_train_3 = X_train[['Age_08_04', 'KM','Weight']]","a2768e35":"# Add a constant\nX_train_3c = sm.add_constant(X_train_3)\n# Create a third fitted model\nlr_3 = sm.OLS(y_train, X_train_3c).fit()","02a24065":"lr_3.params","1cb1c5ba":"print(lr_3.summary())","05145478":"X_train_4 = X_train[['Age_08_04', 'KM','Weight','HP']]","d28dac71":"# Add a constant\nX_train_4c = sm.add_constant(X_train_4)\n# Create a third fitted model\nlr_4 = sm.OLS(y_train, X_train_4c).fit()","f8c5a302":"print(lr_4.summary())","f02f9e7e":"X_train_4 = X_train[['Age_08_04', 'KM','Weight','HP']]","d1c881ec":"import statsmodels.formula.api as smf \nrsq_age = smf.ols('Age_08_04~KM+HP+Weight',data=X_train_4).fit().rsquared  \nvif_age = 1\/(1-rsq_age) # 16.33\n\nrsq_KM = smf.ols('KM~Age_08_04+HP+Weight',data=X_train_4).fit().rsquared  \nvif_KM = 1\/(1-rsq_KM) # 564.98\n\nrsq_HP = smf.ols('HP~Age_08_04+KM+Weight',data=X_train_4).fit().rsquared  \nvif_HP = 1\/(1-rsq_HP) #  564.84\n\n\nrsq_weight = smf.ols('Weight~Age_08_04+HP+KM',data=X_train_4).fit().rsquared  \nvif_weight = 1\/(1-rsq_weight) #  16.35\n\n           # Storing vif values in a data frame\nd1 = {'Variables':['Age','KM','HP','weight'],'VIF':[vif_age,vif_KM,vif_HP,vif_weight]}\nVif_frame = pd.DataFrame(d1)  \nVif_frame","43705caa":"X_train_5 = X_train[['Age_08_04', 'KM','Weight','HP','Cylinders']]","0724eb0e":"# Add a constant\nX_train_5c = sm.add_constant(X_train_5)\n# Create a third fitted model\nlr_5 = sm.OLS(y_train, X_train_5c).fit()","c2019e07":"print(lr_5.summary())","1565c47f":"X_train_5 = X_train[['Age_08_04', 'KM','Weight','HP','Cylinders']]","eb2e1392":"import statsmodels.formula.api as smf \nrsq_age = smf.ols('Age_08_04~KM+HP+Weight+Cylinders',data=X_train_5).fit().rsquared  \nvif_age = 1\/(1-rsq_age) \n\nrsq_KM = smf.ols('KM~Age_08_04+HP+Weight+Cylinders',data=X_train_5).fit().rsquared  \nvif_KM = 1\/(1-rsq_KM)\n\nrsq_HP = smf.ols('HP~Age_08_04+KM+Weight+Cylinders',data=X_train_5).fit().rsquared  \nvif_HP = 1\/(1-rsq_HP)\n\n\nrsq_weight = smf.ols('Weight~Age_08_04+HP+KM+Cylinders',data=X_train_5).fit().rsquared  \nvif_weight = 1\/(1-rsq_weight) \n\nrsq_Cylinders = smf.ols('Cylinders~Age_08_04+HP+KM+Weight',data=X_train_5).fit().rsquared  \nvif_Cylinders=1\/(1-rsq_Cylinders)\n\n           # Storing vif values in a data frame\nd1 = {'Variables':['Age','KM','HP','weight','Cylinders'],'VIF':[vif_age,vif_KM,vif_HP,vif_weight,vif_Cylinders]}\nVif_frame = pd.DataFrame(d1)  \nVif_frame","6191f929":"# Predicting the price of training set.\ny_train_price2 = lr_4.predict(X_train_4c)","6520e9a4":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price2), bins = 20)\nfig.suptitle('Error Terms Analysis', fontsize = 20)                   \nplt.xlabel('Errors', fontsize = 18)","7648c65a":"import warnings\nwarnings.filterwarnings(\"ignore\")\ndf_test[sig_num_col] = scaler.transform(df_test[sig_num_col])\ndf_test.shape","e65828f0":"y_test = df_test.pop('Price')\nX_test = df_test","f475f723":"# Adding constant\nX_test_1 = sm.add_constant(X_test)\nX_test_new = X_test_1[X_train_4c.columns]","e20026db":"X_test = df_test\nX_test_2 = X_test_1[X_train_5c.columns]","172b1ccd":"# Making predictions using the final model\ny_pred2 = lr_5.predict(X_test_2)","298e0ce8":"# Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.scatter(y_test,y_pred2)\nfig.suptitle('y_test vs y_pred2', fontsize=20)   \nplt.xlabel('y_test ', fontsize=18)                       \nplt.ylabel('y_pred2', fontsize=16)    ","f897fa06":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred2)","cb190231":"Let's see scatterplot for few correlated variables vs price.","03514bdb":"Model 5 Conclusions:\n    R-sqaured and Adjusted R-squared - 0.856 and 0.856 .\n    p-values - p-values for all the coefficients seem to be less than the significance level of 0.05. - meaning that all the\n    predictors are statistically significant.","28b3fefa":"# Step 7: Residual Analysis of the train data","f01bb7b7":"# Rescaling the Features","4f6cf46e":"Insights:\nMass concentration of vehicles covered the distance between 50,000- 1,00,000\n","3367448f":"We have achieved a R-squared of 0.854 by manually picking the highly correlated variables. Now lets use RFE to select the independent variables which accurately predicts the dependent variable price.","bef105f3":"Insights:\nThe mass concentration of weight of vehicle is between 1000 - 1100\nThere are less no of vehicle with weight more than 1150\n","61f0c991":"# Step 5: Splitting the Data into Training and Testing Sets","90d619bb":"Now that we have fitted the model and checked the normality of error terms, it's time to go ahead and make predictions using the model.","7865341d":"# Step 8: Making Predictions Using the Final Model","62aeec11":"Visualizing the distribution of car prices.","3dca8581":"Dividing into X and Y sets for the model building\u00b6","e1ebac65":"Adding another variable\nThe R-squared value obtained is 0.763 . Since we have so many variables, we can clearly do better than this. So let's go ahead and add the other highly correlated variable, i.e. KM.","7e0bc496":"It is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients.\nThere are two common ways of rescaling:\n->Min-Max scaling\n->Standardisation (mean-0, sigma-1)","6f71f2fe":"# Step 4: Data Preparation","e9ffd278":"Average Price:","1b0a761f":"Insights:\nMore number of cars are of age 68 and very less number of car is of age 6.\nAverage age  of car is 55.947075\n","712707ce":"# Step 2: Cleaning the Data","26317723":"Insight: The average price of car with Doors 5 is highest and gears with 2 is lowest.","4fdeff80":"# Step 1 Reading and Understanding the Data","7ee36ce0":"Visualising few more Categorical Variables\nBoxplot of all the categorical variables","513ee4e6":"Insights:\n    Cars with doors 5, the price of car with gear 5 is lesser than car with gear 6.\n    Cars with doors 4, there with no car with gear other than 5.","bc46d264":" We need to do some basic cleansing activity in order to feed our model the correct data.","a1638392":"->Age_08_04    \n->KM           \n->Doors        \n->Cylinders    \n->Gears        \n->Weight       ","5b3b2c68":"# Significant variables after Visualization","ee5a6622":"Insights: There are some price ranges above 17000 which can be termed as outliers but lets not remove it rather we will use standarization scaling.","b45cf10a":"Insights:\n        HP,Doors,Cylinders,Gears,Weight doesn't show any significant trend with price.\n        KM,age - seem to have a significant negative correlation with price.","05ea6c88":"Insights\nThe cars with Doors 5 are costly than with the 4 and 3.Cars with doors 3 and 4 are almost equal.\nDoors isn't affecting the price much.\nFor all the records no of cylinders are 4.\nThe selling for cars with gears 5 are more sold than 3,4,6.\nThe price of car is directly proportional to the HP in most cases.","35a6d579":"Insights:The plots seems to be right skewed, the prices of almost all cars looks like less than 15000.","72dab537":"Pairplot of all the numeric variables","4b0107d2":"# Step 6: Building a Linear Model","c77314cf":"So, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of Model), let us plot the histogram of it.","9ce68180":"The R-squared incresed from 0.763 to 0.785","d783fc4f":"# Step 3: Data Visualization","bba54695":"Equation of Line to predict the Car prices values.\nCarprice = \u22121.804e-16-0.6339\u00d7Age_08-0.1952\u00d7KM+0.2581\u00d7Weight+0.1386\u00d7HP\n"}}