{"cell_type":{"4def1af5":"code","373125af":"code","6d3970c7":"code","872ad47a":"code","6eae2018":"code","d1de81ec":"code","d4feb220":"code","0f57be0f":"code","e10bcb23":"code","45d1a0a4":"code","78ac14bf":"code","717d26d9":"code","9364fd46":"code","cb66d0bf":"code","eea671bb":"code","eefc0ebd":"code","fb060a42":"code","5b895dac":"code","5defffcc":"code","db182fe6":"code","6cbc3c44":"code","543b4c9d":"code","5025cdee":"code","2e2d567e":"code","5a5bc3ee":"code","4227fbef":"code","5ed4597e":"code","a88d8c0a":"code","83185e2b":"code","e25dc89c":"code","cca0b252":"code","8468f9b7":"code","59103324":"code","e2b44dbc":"code","604da9ea":"code","991786dc":"code","3d3a5501":"code","2a95f254":"code","7c8a2d24":"code","1f7df111":"code","3c4fabe0":"code","4e18db94":"code","9558b82a":"code","528905d7":"code","97172786":"code","034400e6":"code","19cded8f":"code","ed8d9be9":"code","0591d09a":"code","f1b1baa1":"code","9a978d3c":"code","25da42bd":"code","6ce17fff":"code","0f5edca8":"code","d0a64dc7":"code","0f734c8a":"markdown","84266389":"markdown","45b77a41":"markdown","4b682f4c":"markdown","66725e72":"markdown","8c4500ba":"markdown","3f610a15":"markdown","f70f33d4":"markdown","8207e973":"markdown","3b0530f7":"markdown","a2767764":"markdown","12e7a4a4":"markdown","f9928674":"markdown","a83cafc6":"markdown","caab6a61":"markdown","4bddb063":"markdown","389265dd":"markdown","01b4ec5a":"markdown","ab203c32":"markdown","6d70478c":"markdown","bff08d1a":"markdown","91a138ca":"markdown","3d6bc199":"markdown","35bac6f1":"markdown","e7324b95":"markdown","b049439d":"markdown","548d81b6":"markdown","d4a12fe1":"markdown","708b981a":"markdown","2ca129eb":"markdown","216eed8b":"markdown","38cc9819":"markdown","2b6264c8":"markdown","b4ff2260":"markdown"},"source":{"4def1af5":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","373125af":"#Import data : train & test\ntrain=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest=pd.read_csv(\"..\/input\/titanic\/test.csv\")","6d3970c7":"train.head(3)","872ad47a":"train.info()","6eae2018":"train.shape","d1de81ec":"# Percentage of Missing values\n(100*train.isnull().sum()\/train.shape[0]).round(2)","d4feb220":"age_skewness=train[\"Age\"].skew()\nprint(\"Skewness of Age= \",age_skewness.round(2))","0f57be0f":"train_data=train.copy()\ntrain_data[\"Age\"].fillna(train[\"Age\"].mean(skipna=True),inplace=True)\ntrain_data[\"Embarked\"].fillna(train[\"Embarked\"].value_counts().idxmax(),inplace=True)\ntrain_data.drop('Cabin',axis=1, inplace=True)","e10bcb23":"#\"Percentage of Missing values \n(100*train_data.isnull().sum()\/train_data.shape[0]).round(2)","45d1a0a4":"train_data.corr()","78ac14bf":"a=sns.heatmap(train_data.corr(),annot=True,cmap='Blues')","717d26d9":"from sklearn.preprocessing import OneHotEncoder\n\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(train_data[[\"Sex\",\"Embarked\"]]))\nOH_cols_train.index =train_data.index # One-hot encoding removed index; put it back","9364fd46":"#Create new data set: trin_data_pp\ntrain_data_pp=pd.concat([train_data,OH_cols_train],axis=1)\ntrain_data_pp.head(2) #notice that new variable have no names : 0 ,1,2,3,4, so lets name them","cb66d0bf":"train_data_pp.columns","eea671bb":"train_data_pp.columns=['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch',\n                       'Ticket','Fare','Embarked',\"Sex2\",\"Sex3\",\"Embark2\",\"Embark3\",\"Embark4\"]","eefc0ebd":"train_data_pp.head(2)","fb060a42":"from sklearn.preprocessing import StandardScaler\nsc_X=StandardScaler()\nX_s=pd.DataFrame(sc_X.fit_transform(pd.DataFrame(train_data_pp[\"Fare\"])))\nX_s.columns=[\"Fare2\"]\nX_s.index=train_data_pp.index\nX_s.head(5)","5b895dac":"train_data_pp[\"Fare2\"]=X_s","5defffcc":"train_data_pp.tail(3)","db182fe6":"from sklearn.linear_model import LogisticRegression\nX=train_data_pp[[ 'Sex2', 'Age',\"SibSp\",\"Parch\",\"Fare2\" ,'Embark3', 'Embark2']]\ny=train_data_pp[\"Survived\"]\nlgsct=LogisticRegression(solver='lbfgs')\nlgsct=lgsct.fit(X,y)","6cbc3c44":"print(\"Train error:\", (lgsct.score(X,y)*100).round(3))","543b4c9d":"from sklearn.model_selection import cross_val_score\nscores_accuracy = cross_val_score(lgsct, X, y, cv=10, scoring='accuracy')\nprint(\"Test error :\", (scores_accuracy.mean()*100).round(3))","5025cdee":"lgsct2=LogisticRegression(solver='lbfgs').fit(X,y)\nprint(\"Train error:\", (lgsct2.score(X,y)*100).round(3))","2e2d567e":"from sklearn import metrics\ny_predicted=lgsct.predict_proba(X)[:, 1]\nscore=[]\nthreshold=[]\nfor i in np.arange(0.4,0.85,0.05):\n    y_predict=np.where(y_predicted>i,1,0)\n    score.append(metrics.accuracy_score(y,y_predict))\n    threshold.append(i)","5a5bc3ee":"plt.figure(figsize=(12,5))\na=sns.lineplot(threshold,score,marker=\"*\")","4227fbef":"X2=train_data_pp[[ 'Sex2', 'Age',\"SibSp\",\"Parch\" ,'Embark3', 'Embark2']]\nlgsct3=LogisticRegression(solver='lbfgs')\nlgsct3=lgsct.fit(X2,y)","5ed4597e":"print(\"Train error:\", (lgsct3.score(X2,y)*100).round(3))","a88d8c0a":"scores_accuracy = cross_val_score(lgsct3, X2, y, cv=10, scoring='accuracy')\nprint(\"Test error :\", (scores_accuracy.mean()*100).round(3))","83185e2b":"from sklearn.neighbors import KNeighborsClassifier","e25dc89c":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=10)\nknn_cv.fit(X,y)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))","cca0b252":"knn=KNeighborsClassifier(3)\nknn.fit(X,y)\nprint(\"KNN train error\",(knn.score(X,y)*100).round(3))","8468f9b7":"from sklearn.metrics import confusion_matrix\ny_predict = knn.predict(X)\nconfusion_matrix(y,y_predict)\npd.crosstab(y, y_predict, rownames=['True'], colnames=['Predicted'], margins=True)","59103324":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis","e2b44dbc":"lda=LinearDiscriminantAnalysis().fit(X,y)","604da9ea":"print(\"LDA train error\",(lda.score(X,y)*100).round(3))","991786dc":"scores_accuracy = cross_val_score(LinearDiscriminantAnalysis(), X, y, cv=10, scoring='accuracy')\nprint(\"LDA test error\",(scores_accuracy.mean()*100).round(3))","3d3a5501":"y_pred = lda.predict(X)\nconfusion_matrix(y,y_pred)\npd.crosstab(y, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","2a95f254":"qda = QuadraticDiscriminantAnalysis().fit(X, y)\nscores_accuracy = cross_val_score(QuadraticDiscriminantAnalysis(), X, y, cv=10, scoring='accuracy')\nprint(\"QDA train error\",(qda.score(X,y)*100).round(3))\nprint(\"QDA test error\",(scores_accuracy.mean()*100).round(3))","7c8a2d24":"y_pred = qda.predict(X)\nconfusion_matrix(y,y_pred)\npd.crosstab(y, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","1f7df111":"train_data[\"Sex2\"]=train_data.Sex.factorize()[0]\ntrain_data[\"Embarked2\"]=train_data.Embarked.factorize()[0]","3c4fabe0":"train_data.head(2)","4e18db94":"from sklearn import tree\nX= train_data[[\"Pclass\",\"Sex2\", \"Age\",\"SibSp\", \"Parch\" , \"Fare\" ,\"Embarked2\"]]\ny=train_data[\"Survived\"]","9558b82a":"l=[]\nfor i in range(1,30):\n    print(i)\n    k=[]\n    for j in range(1,30): \n            clf=tree.DecisionTreeClassifier(min_samples_leaf=j, max_depth=i)\n            scores=cross_val_score(clf,X,y,cv=5)\n            k.append(scores.mean())\n    k=pd.Series(k)\n    print((k.max(),k.idxmax()))\n    l.append((k.max(),k.idxmax()))","528905d7":"max(l)","97172786":"clf=tree.DecisionTreeClassifier(min_samples_leaf=6, max_depth=7)\nprint(\"DT test error: \",(cross_val_score(clf,X,y,cv=5).mean()*100).round(3))\nclf.fit(X,y)\nprint(\"DT train error: \", (clf.score(X,y)*100).round(3))","034400e6":"from sklearn.ensemble import RandomForestClassifier","19cded8f":"l=[]\nfor i in range(1,30):\n    print(i)\n    k=[]\n    for j in range(1,30): \n            clf=RandomForestClassifier(min_samples_leaf=j, max_depth=i,n_estimators=30,max_features=\"sqrt\")\n            scores=cross_val_score(clf,X,y,cv=5)\n            k.append(scores.mean())\n    k=pd.Series(k)\n    print((k.max(),k.idxmax()))\n    l.append((k.max(),k.idxmax()))","ed8d9be9":"clf=RandomForestClassifier(min_samples_leaf=3, max_depth=17,n_estimators=30,max_features=\"sqrt\")\nprint(\"RF test error: \",(cross_val_score(clf,X,y,cv=5).mean()*100).round(3))\nclf.fit(X,y)\nprint(\"RF train error: \", (clf.score(X,y)*100).round(3))","0591d09a":"test_data=test.copy()\ntest_data[\"Age\"].fillna(test[\"Age\"].median(skipna=True),inplace=True)\ntest_data[\"Fare\"].fillna(test[\"Fare\"].median(skipna=True),inplace=True)\ntest_data[\"Embarked\"].fillna(test[\"Embarked\"].value_counts().idxmax(),inplace=True)\ntest_data.drop('Cabin',axis=1, inplace=True)\ntest_data[\"Sex2\"]=test_data.Sex.factorize()[0]\ntest_data[\"Embarked2\"]=test_data.Embarked.factorize()[0]","f1b1baa1":"sc_X=StandardScaler()\nX_s=pd.DataFrame(sc_X.fit_transform(pd.DataFrame(test_data[\"Fare\"])))\nX_s.columns=[\"Fare2\"]\nX_s.index=test_data.index","9a978d3c":"test_data[\"Fare2\"]=X_s\ntest_data.head(2)","25da42bd":"W=test_data[[\"Pclass\",\"Sex2\", \"Age\",\"SibSp\", \"Parch\" , \"Fare\" ,\"Embarked2\"]]","6ce17fff":"y_predict=clf.predict(W)","0f5edca8":"export=pd.DataFrame(test_data['PassengerId'])\nexport['Survived']=y_predict\nexport.head()","d0a64dc7":"export[[\"PassengerId\",\"Survived\"]].to_csv(r'Submission1.csv',index=False)","0f734c8a":"We can use confusion matrix to measure the accuracy","84266389":"`c\/c : The threshold has an effect on the accuracy of prediction. the threshold 60% is better than the 50% threshold`","45b77a41":"**But, when is the skewness too much?  The rule of thumb seems to be:**\n\n- If the skewness is between -0.5 and 0.5, the data are fairly symmetrical\n- If the skewness is between -1 and \u2013 0.5 or between 0.5 and 1, the data are moderately skewed\n- If the skewness is less than -1 or greater than 1, the data are highly skewed","4b682f4c":"`We have seen earlier that we can drop \"Fare\" because it's correlated with other variables. Let's try so te see if that has a significant improvement or not`","66725e72":"## 5- Random Forest","8c4500ba":"First, we use GridSearch to choose the best parameter k","3f610a15":"## Prediction : The test data ","f70f33d4":"`c\/c: Sex and Embarked are two nominal variables, so we can use One-Hot encoding `","8207e973":"## 2- K-NN","3b0530f7":"### > Step 1: Find missing values","a2767764":"So the idea is to choose the values that max the train error ","12e7a4a4":"## 3- LDA & QDA","f9928674":"## Missing values: \nIn this section we will try to work with missing values.\n\nWe will do that in 3 steps: \n   - Find missing values\n   - Choose the appropriate methode to handle missing values\n   - Dealing with messing values\n","a83cafc6":"Does the solver have any impact on training error: ","caab6a61":"`c\/c:we should look at correlated variables because they create a lit of troubles for some machine learning algoeithms like Logistic regression. \nWe notice that the variable Fare is correlated with Pclass (it was expected: better classes are more expensive than bad ones).Parch and SibSp are also correlated, maybe we can use some feature engineering to combine them`","4bddb063":"In decision trees there is two important variables:`min_samples_leaf` and `max_depth`.\n\nmin_samples_leaf: The minimum number of samples required to be at a leaf node.\n\nmax_depth: The minimum number of samples required to be at a leaf node.","389265dd":"**One last thing it may be helpful if we scale \"Fare\" to have mean 0 and variance 1** ","01b4ec5a":"`c\/c: The skewness of Age is less than 0.5 so we should choose the mean to handle messing values.\n For the last variable,<Embarked>, we have one choice, because it's not numerical, we use the mode.`","ab203c32":"## 4- Decision Trees","6d70478c":"# Modeling: Machine Learning algorithms\n\nWe gonna focus on these algorithms: \n   - Logistic regression\n   - K-NN\n   - LDA & QDA\n   - Decision trees\n   - Random Forests\n\nWe gonna also use some useful methodes such as :\n   - Cross-validation\n   - Feature selection\n   - Handling categoric variables","bff08d1a":"In this notebook I will present some machine learning that I used for my submessions for Titanic challenge.\n\n       - Step 1: Handle missing values\n       - Step 2: Handle categorical variables\n       - Step 3: Use Logistic regression\n       - Step 4: Use KNN\n       - Step 5: Use LDA and QDA\n       - Step 6: Use Decision trees\n       - Step 7: Use random forest","91a138ca":"The impact of the threshold: By default, in predction the logistic regression uses the 50% threshold.\n    Let try other thresholds:","3d6bc199":"### > Step 3: Dealing with messing values","35bac6f1":"## Data Exploration","e7324b95":"- Drop variables with messing values: we use this option only of the number of messing values is very large. In our case, we have morer than 77% of Cabin's observations are messing, so we will drop it or simply ignore it.\n- Imputation using mean or median for **_Numerical variables_** : we choose to use the mean if the variable doesn't have any skewness or if its distrubution looks like normal(Gaussian) distribution. In the other hand, we use the median If the data does exhibit some skewness. \n- Imputation using the mode (the most frequent value) for **_Categoric variables_** .","b049439d":"In random forest we should look at those parameters:\n\n    - min_samples_leaf\n    - max_depth\n    - n_estimators:The number of trees in the forest, we choose 30 \n    - max_features=the rule says \"sqrt\" for classification and p\/3 for regression (p number of variables)","548d81b6":"### > Step 2: Appropriate methode to handle messing values\n\nIn handling missing values we generaly one of these technics: ","d4a12fe1":"`c\/c :3 variables with missing values: Age, Cabin and Embarked`\n","708b981a":"## 1- Logistic regression:","2ca129eb":"# Remark: \n  - In my submissions, I used label encoding for Pclass and this gives much better results\n  - Until now, the random forest is the methode that gives me the best score.(0.78468)\n\nResources: \n    - Kaggle courses & notebooks shared by kagglers\n    - Introduction to statistical learning","216eed8b":"Data exploration is an art.But I will not spend much time on it because the aim of this notebook is to explore different machine algorithms that we can use ","38cc9819":"For desicion trees I will not use One-hot encoding for categorical variables. Instead I'will use label encoding.It gives much better results","2b6264c8":"Before we start applying machine learning algorithms, we should look at our variables, especially if we are working with catedorical variables. Because some functions do not accept string as observations. \nIn handling non numerical values, we generaly use two approaches( see Kaggle courses for more info):\n - Label encoding assigns each unique value to a different integer.This approach assumes an ordering of the categories: **_ordinal variables_**. \n - One-hot encoding creates new columns indicating the presence (or absence) of each possible value in the original data.ne-hot encoding does not assume an ordering of the categories. Thus, you can expect this approach to work particularly well if there is no clear ordering in the categorical data. **_nominal variables_**. ","b4ff2260":"`c\/c : The test error had improved`"}}