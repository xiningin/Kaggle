{"cell_type":{"fadfe1cf":"code","db05e1e6":"code","dad915b8":"code","a3b04440":"code","558fd8ff":"code","9f6d38aa":"code","2450a327":"code","61945a72":"code","cca84a66":"code","c3213619":"code","88cb1cda":"code","f7b27cc4":"code","1cde522d":"code","d711e9eb":"code","be2b0069":"code","599d9dde":"markdown","11c72038":"markdown","032846eb":"markdown","ad82ec55":"markdown","f51b1360":"markdown","5bb79f3b":"markdown","53ab49b9":"markdown","05679fe3":"markdown","42bee8ed":"markdown","87584047":"markdown","c16ad7aa":"markdown","6c2ddcab":"markdown","3cebf161":"markdown","a9dfa053":"markdown","dc3857ac":"markdown","9ba0610b":"markdown"},"source":{"fadfe1cf":"# Import packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport sklearn.model_selection\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","db05e1e6":"# Import data\n\nsample_submission_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\ntrain_data_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndata_to_predict_df = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\ntrain_data_df.describe()\n# test_data_df.head()","dad915b8":"# Convert data into desired format\n\nx = train_data_df.iloc[:,1:].values   # convert dataframe into numpy array taking only pixel data\nx = x \/ 255.0                       # convert to greyscale\ny = train_data_df.iloc[:,0].values  # convert dataframe into numpy array taking only label data\nprint(y.shape)\nprint(y)\nprint(x.shape)\nprint(x)","a3b04440":"x_train, x_validation, y_train, y_validation = train_test_split(x, y, test_size=0.2, random_state=42)\n\nprint(x_train.shape, y_train.shape, x_validation.shape, y_validation.shape)","558fd8ff":"# visualise data\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    one_image = x_train[i].reshape(28,28)\n    plt.imshow(one_image, cmap=plt.cm.binary)\n    plt.xlabel(y_train[i])\nplt.show()","9f6d38aa":"# Define model\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(784,)),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(10)\n])\n\n# Compile Model\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n\n# Fit model\nmodel.fit(x_train, y_train, epochs=20)\n\ntest_loss, test_acc = model.evaluate(x_validation,  y_validation, verbose=2)\n\nprint('\\nTest accuracy:', test_acc)","2450a327":"# Predictions\nprobability_model = tf.keras.Sequential([model,tf.keras.layers.Softmax()])\npredictions = probability_model.predict(x_validation)     #returns probabliliy distribution of predictions\nprint(np.argmax(predictions[0]),y_validation[0])\nprint(y_validation)\n","61945a72":"# Display results and probablity distribution\ndef plot_image(i, predictions_array, true_label, img):\n    true_label, img = true_label[i], img[i]\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n\n    plt.imshow(img.reshape(28,28), cmap=plt.cm.binary)\n\n    predicted_label = np.argmax(predictions_array)\n    if predicted_label == true_label:\n        color = 'blue'\n    else:\n        color = 'red'\n\n    plt.xlabel(\"{} {:2.0f}% ({})\".format(predicted_label,\n                                100*np.max(predictions_array),\n                                true_label),\n                                color=color)\n\ndef plot_value_array(i, predictions_array, true_label):\n    true_label = true_label[i]\n    plt.grid(False)\n    plt.xticks(range(10))\n    plt.yticks([])\n    thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n    plt.ylim([0, 1])\n    predicted_label = np.argmax(predictions_array)\n\n    thisplot[predicted_label].set_color('red')\n    thisplot[true_label].set_color('blue')\n    \n# Plot the first X test images, their predicted labels, and the true labels.\n# Color correct predictions in blue and incorrect predictions in red.\nnum_rows = 5\nnum_cols = 3\nnum_images = num_rows*num_cols\nplt.figure(figsize=(2*2*num_cols, 2*num_rows))\nfor i in range(num_images):\n    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n    plot_image(i, predictions[i], y_validation, x_validation)\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plot_value_array(i, predictions[i], y_validation)\nplt.tight_layout()\nplt.show()","cca84a66":"# Display Wrong Predictions\nprediction_mismatch = []\nx_mismatch = []\ny_mismatch = []\nfor i in range(len(y_validation)):\n    if np.argmax(predictions[i]) != y_validation[i]:\n        prediction_mismatch.append(predictions[i])\n        x_mismatch.append(x_validation[i])\n        y_mismatch.append(y_validation[i])\n    \n# Plot the first X test images, their predicted labels, and the true labels.\n# Color correct predictions in blue and incorrect predictions in red.\nnum_rows = 5\nnum_cols = 3\nnum_images = num_rows*num_cols\nplt.figure(figsize=(2*2*num_cols, 2*num_rows))\nfor i in range(num_images):\n    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n    plot_image(i, prediction_mismatch[i], y_mismatch, x_mismatch)\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plot_value_array(i, prediction_mismatch[i], y_mismatch)\nplt.tight_layout()\nplt.show()","c3213619":"# Data Augmentation - creating slightly trasnformed data and feeding into model - more data = more accuracy (hopefully)\ndatagen = ImageDataGenerator(\n        featurewise_center=False, # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n# Reshape to 4 dimensions so that it can be input into flow\nx_train_aug = x_train.reshape(x_train.shape[0],28,28,1)\n# y_train_aug = y_train.reshape(x_train.shape[0],28,28,1)\n\n#datagen.fit(X_train)\ntrain_gen = datagen.flow(x_train_aug,y_train, batch_size=128)\n# train_gen.shape\n","88cb1cda":"# prediction model inlcuding data augmentation  - WORK IN PROGRESS\n\n# train_gen\n\n# x_validation = x_validation.values.reshape(-1,28,28,1)\n# y_validation = y_validation.values.reshape(-1,28,28,1)\n\n# data_aug_model = model.fit(train_gen,epochs = 20) #,validation_data = (x_validation,y_validation),verbose = 2)\n\n# # test_loss, test_acc = data_aug_model.evaluate(x_validation,  y_validation, verbose=2)\n\n# print('\\nTest accuracy:', test_acc)","f7b27cc4":"# Make predictions for submission\ndata_to_predict = data_to_predict_df.values\nresults = model.predict(data_to_predict)\nresults = np.argmax(results,axis = 1)\n\nresults","1cde522d":"# visualise results\nplt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    one_image = data_to_predict[i].reshape(28,28)\n    plt.imshow(one_image, cmap=plt.cm.binary)\n    plt.xlabel(results[i])\nplt.show()","d711e9eb":"sample_submission_df","be2b0069":"results = pd.Series(results,name=\"Label\")\n# Save the final result in cnn_mnist_submission.csv\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n\nsubmission.to_csv(\"cnn_mnist_submission.csv\",index=False)\nsubmission","599d9dde":"# MNIST Digit Recogniser\n1. Data Preparation\n - 1.1 Import Packages\n - 1.2 Import Data\n - 1.3 Convert Data into Desired Format\n2. Visualise Data\n3. Model\n - 3.1 Define, Compile and Fit Model\n4. Validation\n - 4.1 Map out Predictions Made for Validation Data\n - 4.2 Display Result with Probability Distribution\n - 4.3 Display Wrong Predictions\n5. Data Augmentation\n6. Final Predictions\n - 6.1 Make Predictions for Submission\n - 6.2 Submit Predictions","11c72038":"### Submit Predictions\nFinally, we can create our prediction file, ready for submission.","032846eb":"### Visualise Data\nIt's important to know what our data actually looks like to give us an idea of what we're working with!","ad82ec55":"We'll use the example submission file to determine what format we should have the data in.","f51b1360":"### Model Predictions after Data Augmentation (Work in Progress)","5bb79f3b":"### Convert Data into Desired Format\nSince the tensorflow neural network model, which we will use later, works well with numpy arrays, that is what we will convert our data into.","53ab49b9":"### Display Wrong Predictions\nLet's also do the same for our wrong predictions as it might give us some insight on why\/how wrong our model is. As we can see, some of our predictions see reasonably wrong. As in its clear why they were predicted wrong as the numbers are very unusualy written. However some of our wrong predictions are very wrong and would be easily labelled by a human. Clearly more work is needed to be done.","05679fe3":"### Import MNIST Digit Data","42bee8ed":"## Validation\n### Map out Predictions Made for Validation Data\n","87584047":"### Visualise Final Predictions","c16ad7aa":"### Split Data into Training and Validation Data\nTraining data will be used when fitting our model. Validation data will be used to test\/validate the accuracy of our model. In this case, we are randomly splitting the data out into these 2 different scenarios, with an 80:20 split. \n\nTypically you want the majority to be put into training data as more data usually means a more accurate model. However, we don't want our validation data to be too small as this may output in inaccurate model accuracy when doing the validation.","6c2ddcab":"## Data Preparation\n\n### Import Packages\/Libraries","3cebf161":"## Data Augmentation (Work in Progress)\nWe can use data augmentation to increase the model accuracy. In this case we will randomly: rotate images, zoom into images, shift images horizontally and shift images vertically. This will still preserve the integrity of the digit image but will give us more data to feed the model and fit the model against to make it more accurate.","a9dfa053":"## Model\n### Define, Compile and Fit Model\nWe will use a basic Neural Network structure with 784 nodes in the input layer (representing 28x28=784 pixels), 512 nodes in the hidden layer and 10 nodes in the output layer (representing our numbers 0-9)\n","dc3857ac":"### Display Result with Probability Distribution\nThe output of the model can be displayed as a probablility distribution. Each number 0-9 has a certain associated probabability for every image e.g. 1% chance of being the number 0, 50% chance of being the number 1, 3% chance of being the number 2 etc.","9ba0610b":"## Final Predictions\n### Make Predictions for Submission\nUsing our model, we can now make final predictions for submission."}}