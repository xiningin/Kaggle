{"cell_type":{"076d9f8a":"code","3e1927a9":"code","abd197a0":"code","961c65a4":"code","2321768c":"code","4f0fa5fa":"code","ffb702d0":"code","c4592784":"code","44b31e25":"code","7a94f602":"code","d808a5b9":"code","194506f5":"code","63acd6c5":"code","3892115d":"code","a311ce52":"code","0bfbd416":"code","7ea23a66":"code","e4dbb36a":"code","f8d2dab6":"code","05c486ec":"code","59a0b65a":"code","28c56be0":"code","9170e777":"code","70b7e6c9":"code","dc844bca":"code","2312aeb2":"code","a18a32d2":"code","3a63bf9c":"code","5c02da3e":"code","406042ba":"code","072678f9":"code","c52fc581":"code","d007906b":"code","66f3cfb7":"code","6f1ab4d6":"code","9bb1167c":"code","8ca8671e":"code","02598231":"code","790e2ad7":"code","126de19c":"code","43d1c68a":"code","ff80a5c8":"code","c255b6cd":"code","25ab03ce":"code","1ce0ad55":"code","303a6bb2":"code","7e9ebab6":"code","f02ab524":"markdown","a3b1f83d":"markdown","f8c457a1":"markdown","aa9c09d9":"markdown","cc100d4f":"markdown","e71c7ac6":"markdown","a2d9ccb8":"markdown","ecac0629":"markdown","4577c917":"markdown","2dbd3542":"markdown","d0c5bd49":"markdown","c4daf2c9":"markdown","1e284143":"markdown","cfaf76a3":"markdown","5b181c6f":"markdown","a4512a81":"markdown","421a8ac6":"markdown","581f3939":"markdown","6693c2d1":"markdown","0166fc52":"markdown","34af8e94":"markdown","f456ae49":"markdown","234bafe9":"markdown","27833aae":"markdown","855f7035":"markdown","b886ab85":"markdown","3afe7ded":"markdown","f3077008":"markdown","2c06259d":"markdown","a3d14324":"markdown","b4a7126b":"markdown","ecc840b8":"markdown","36024545":"markdown","9fcac342":"markdown","3c653943":"markdown","70dd3886":"markdown","7efb0031":"markdown","0cc23126":"markdown","55feec6f":"markdown","b370e466":"markdown","f39f6025":"markdown","9742aedb":"markdown","434e483c":"markdown","0b654c61":"markdown","834cd269":"markdown","c028ba05":"markdown"},"source":{"076d9f8a":"# Library to suppress warnings or deprecation notes \nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Libraries to help with reading and manipulating data\nimport numpy as np\nimport pandas as pd\n\n# Libraries to help with data visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Library to split data \nfrom sklearn.model_selection import train_test_split\n\n# Libraries to import decision tree classifier and different ensemble classifiers\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\n\n# Libtune to tune model, get different metric scores\nfrom sklearn import metrics\nfrom sklearn.metrics import  classification_report, accuracy_score, precision_score, recall_score,f1_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay,plot_confusion_matrix #to plot confusion matric\nplt.style.use('ggplot')\nprint('Load Libraries-Done')","3e1927a9":"pima=pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\n# copying data to another varaible to avoid any changes to original data\ndata=pima.copy()\nprint(f'There are {data.shape[0]} rows and {data.shape[1]} columns') # fstring ","abd197a0":"#View the first and last 5 rows of the dataset.\ndata.head()","961c65a4":"#Check datatype\ndata.info()","2321768c":"print (\"#\"*40,\"\\n\",\"Features : \\n\\n\", data.columns.tolist()) #get name of columns\/features\nprint (\"#\"*40,\"\\nMissing values :\\n\\n\", data.isnull().sum().sort_values(ascending=False))\nprint (\"#\"*40,\"\\nUnique values :  \\n\\n\", data.nunique())  #  count of unique values\n","4f0fa5fa":"# Summary of the dataset.\ndata.describe().T","ffb702d0":"def dist_box(data):\n # function plots a combined graph for univariate analysis of continous variable \n #to check spread, central tendency , dispersion and outliers  \n    Name=data.name.upper()\n    fig,(ax_box,ax_dis)  =plt.subplots(nrows=2,sharex=True,gridspec_kw = {\"height_ratios\": (.25, .75)},figsize=(8, 5))\n    mean=data.mean()\n    median=data.median()\n    mode=data.mode().tolist()[0]\n    sns.set_theme(style=\"white\")\n    sns.set_palette(sns.color_palette(\"Set1\", 8))\n    fig.suptitle(\"SPREAD OF DATA FOR \"+ Name  , fontsize=18, fontweight='bold')\n    sns.boxplot(x=data,showmeans=True, orient='h',ax=ax_box)\n    ax_box.set(xlabel='')\n     # just trying to make visualisation better. This will set background to white\n    sns.despine(top=True,right=True,left=True) # to remove side line from graph\n    sns.set_palette(sns.color_palette(\"Set1\", 8))\n    sns.distplot(data,kde=False,ax=ax_dis)\n    ax_dis.axvline(mean, color='r', linestyle='--',linewidth=2)\n    ax_dis.axvline(median, color='g', linestyle='-',linewidth=2)\n    ax_dis.axvline(mode, color='y', linestyle='-',linewidth=2)\n    plt.legend({'Mean':mean,'Median':median,'Mode':mode})\n                    ","c4592784":"#select all quantitative columns for checking the spread\nlist_col= data.select_dtypes(include='number').columns.to_list()\nfor i in range(len(list_col)):\n    dist_box(data[list_col[i]])","44b31e25":"# Making a list of all categorical variables\ncat_columns = ['Pregnancies','Outcome']\ntitle=['Number of Pregancies','Outcome']\nplt.figure(figsize=(15,7))\n\nsns.set_theme(style=\"white\") # just trying to make visualisation better. This will set background to white\nfor i, variable in enumerate(cat_columns):\n                     plt.subplot(1,2,i+1)\n                     order = data[variable].value_counts(ascending=False).index   \n                     #sns.set_palette(list_palette[i]) # to set the palette\n                     sns.set_palette('Set1')\n                     ax=sns.countplot(x=data[variable], data=data )\n                     sns.despine(top=True,right=True,left=True) # to remove side line from graph\n                     for p in ax.patches:\n                           percentage = '{:.1f}%'.format(100 * p.get_height()\/len(data[variable]))\n                           x = p.get_x() + p.get_width() \/ 2 - 0.05\n                           y = p.get_y() + p.get_height()\n                           plt.annotate(percentage, (x, y),ha='center')\n                     plt.tight_layout()\n                     plt.title(title[i].upper())\n                                     \n","7a94f602":"sns.set_palette(sns.color_palette(\"Set1\", 8))\nplt.figure(figsize=(15,7))\nsns.heatmap(data.corr(),annot=True,vmin=-1,vmax=1,cmap='viridis')\nplt.show()","d808a5b9":"sns.set_palette(sns.color_palette(\"Set1\", 8))\nsns.pairplot(data=data,hue=\"Outcome\",corner=True)\nplt.show()","194506f5":"data.columns","63acd6c5":"numeric_columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age',]\nplt.figure(figsize=(15,25))\n\nsns.set_palette(sns.color_palette(\"Set1\", 8))\nfor i, variable in enumerate(numeric_columns):\n        plt.subplot(10,3,i+1)\n        \n        sns.boxplot(x='Outcome',y= data[variable], data=data)     \n        sns.despine(top=True,right=True,left=True) # to remove side line from graph\n        plt.tight_layout()\n        plt.title(variable.upper())","3892115d":"data.loc[data.Glucose == 0, 'Glucose'] = data.Glucose.median()\ndata.loc[data.BloodPressure == 0, 'BloodPressure'] = data.BloodPressure.median()\ndata.loc[data.SkinThickness == 0, 'SkinThickness'] = data.SkinThickness.median()\ndata.loc[data.Insulin == 0, 'Insulin'] = data.Insulin.median()\ndata.loc[data.BMI == 0, 'BMI'] = data.BMI.median()","a311ce52":"X = data.drop('Outcome',axis=1)\ny = data['Outcome'] ","0bfbd416":"# Splitting data into training and test set:\n#The Stratify arguments maintains the original distribution of classes in the target variable while splitting the data into train and test sets.**\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)\nprint(X_train.shape, X_test.shape)","7ea23a66":"y.value_counts(1)","e4dbb36a":"y_test.value_counts(1)","f8d2dab6":"from sklearn.preprocessing import StandardScaler\n# Creating StandardScaler instance\nscaler = StandardScaler()\n\n# Fitting Standard Scaller\nX_scaler = scaler.fit(X_train)\n\n# Scaling data\nX_train_scaled = X_scaler.transform(X_train)\nX_test_scaled = X_scaler.transform(X_test)\n\nX_train_scaled_df = pd.DataFrame(X_train_scaled,columns=X_train.columns)\nX_test_scaled_df = pd.DataFrame(X_test_scaled,columns=X_test.columns)\n\n\nX_train_scaled_df.index=np.arange(len(X_train_scaled_df))\nX_test_scaled_df.index=np.arange(len(X_test_scaled_df))\ny_train.index=np.arange(len(y_train))\ny_test.index=np.arange(len(y_test))\n","05c486ec":"def make_confusion_matrix(y_actual,y_predict,title):\n    fig, ax = plt.subplots(1, 1)\n    \n    cm = confusion_matrix(y_actual, y_predict, labels=[0,1])\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=[\"No\",\"Yes\"])\n    disp.plot(cmap='Reds',colorbar=True,ax=ax)\n    ax.set_title(title)\n    plt.tick_params(axis=u'both', which=u'both',length=0)\n    plt.grid(b=None,axis='both',which='both',visible=False)\n    plt.show()","59a0b65a":"##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision\ndef get_metrics_score(model,X_train_df,X_test_df,y_train_pass,y_test_pass,flag=True):\n    '''\n    model : classifier to predict values of X\n    '''\n    # defining an empty list to store train and test results\n    score_list=[] \n    pred_train = model.predict(X_train_df)\n    pred_test = model.predict(X_test_df)\n    pred_train = np.round(pred_train)\n    pred_test = np.round(pred_test)\n    train_acc = accuracy_score(y_train_pass,pred_train)\n    test_acc = accuracy_score(y_test_pass,pred_test)\n    train_recall = recall_score(y_train_pass,pred_train)\n    test_recall = recall_score(y_test_pass,pred_test)\n    train_precision = precision_score(y_train_pass,pred_train)\n    test_precision = precision_score(y_test_pass,pred_test)\n    train_f1 = f1_score(y_train_pass,pred_train)\n    test_f1 = f1_score(y_test_pass,pred_test)\n    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision,train_f1,test_f1))\n      \n    if flag == True: \n        print(\"\\x1b[0;30;47m \\033[1mMODEL PERFORMANCE\\x1b[0m\")\n        print(\"\\x1b[0;30;47m \\033[1mAccuracy   : Train:\\x1b[0m\",round(train_acc,3),\n              \"\\x1b[0;30;47m \\033[1mTest:\\x1b[0m \",round(test_acc,3))\n        print(\"\\x1b[0;30;47m \\033[1mRecall     : Train:\\x1b[0m\",round(train_recall,3),\n              \"\\x1b[0;30;47m \\033[1mTest:\\x1b[0m\" , round(test_recall,3))\n        \n        print(\"\\x1b[0;30;47m \\033[1mPrecision  : Train:\\x1b[0m\", round(train_precision,3),\n              \"\\x1b[0;30;47m \\033[1mTest:\\x1b[0m \", round(test_precision,3))\n        print(\"\\x1b[0;30;47m \\033[1mF1         : Train:\\x1b[0m\",round(train_f1,3),\n              \"\\x1b[0;30;47m \\033[1mTest:\\x1b[0m\", round(test_f1,3))\n        make_confusion_matrix(y_train_pass,pred_train,\"Confusion Matrix for Train\")     \n        make_confusion_matrix(y_test_pass,pred_test,\"Confusion Matrix for Test\") \n    return score_list # returning the list with train and test scores","28c56be0":"# # defining empty lists to add train and test results\nacc_train = []\nacc_test = []\nrecall_train = []\nrecall_test = []\nprecision_train = []\nprecision_test = []\nf1_train = []\nf1_test = []\n\ndef add_score_model(score):\n     '''Add score to list for comparision'''  \n     acc_train.append(score[0])\n     acc_test.append(score[1])\n     recall_train.append(score[2])\n     recall_test.append(score[3])\n     precision_train.append(score[4])\n     precision_test.append(score[5])\n     f1_train.append(score[6])\n     f1_test.append(score[7])","9170e777":"#Fitting the model\nd_tree = DecisionTreeClassifier(random_state=1)\nd_tree.fit(X_train_scaled_df,y_train)\n\n#Calculating different metrics\nscore_list_dt=get_metrics_score(d_tree,X_train_scaled_df,X_test_scaled_df,y_train,y_test)","70b7e6c9":"add_score_model(score_list_dt)","dc844bca":"#Fitting the model\nrf_estimator = RandomForestClassifier(random_state=1)\nrf_estimator.fit(X_train_scaled_df,y_train)\nscore_list_rf=get_metrics_score(rf_estimator,X_train_scaled_df,X_test_scaled,y_train,y_test)","2312aeb2":"add_score_model(score_list_rf)","a18a32d2":"#Fitting the model\nbagging_classifier = BaggingClassifier(random_state=1)\nbagging_classifier.fit(X_train_scaled_df,y_train)\n\nscore_list_bc=get_metrics_score(bagging_classifier,X_train_scaled_df,X_test_scaled_df,y_train,y_test)","3a63bf9c":"add_score_model(score_list_bc)","5c02da3e":"#Choose the type of classifier. \ndtree_estimator = DecisionTreeClassifier(class_weight={0:0.40,1:0.60},random_state=1)\n\n# Grid of parameters to choose from\nparameters = {'max_depth': [None], \n              'min_samples_leaf': [10, 7, 5],\n              'max_leaf_nodes' : [ 5,7, 10],\n              'min_impurity_decrease': [0.001,0.01,0.1]\n             }\n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(dtree_estimator, parameters, scoring=scorer,n_jobs=-1)\ngrid_obj = grid_obj.fit(X_train_scaled_df, y_train)\n\n# Set the clf to the best combination of parameters\ndtree_estimator = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \ndtree_estimator.fit(X_train_scaled_df, y_train)","406042ba":"score_tune_dt=get_metrics_score(dtree_estimator,X_train_scaled_df,X_test_scaled_df,y_train,y_test)","072678f9":"add_score_model(score_tune_dt)","c52fc581":"# Choose the type of classifier. \nrf_tuned = RandomForestClassifier(class_weight={0:0.35,1:0.65},random_state=1)\n\nparameters = { \"max_depth\":[None],\n              \"n_estimators\": [150,200,250,500],\n    \"min_samples_leaf\": np.arange(5, 10),\n    \"max_features\": ['auto'],\n    \"max_samples\": np.arange(0.3,0.5, 0.7)\n}\n\n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(rf_tuned, parameters, scoring=scorer,cv=5,n_jobs=-1)\ngrid_obj = grid_obj.fit(X_train_scaled_df, y_train)\n\n# Set the clf to the best combination of parameters\nrf_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nrf_tuned.fit(X_train_scaled_df, y_train)","d007906b":"score_tune_rt=get_metrics_score(rf_tuned,X_train_scaled_df,X_test_scaled_df,y_train,y_test)","66f3cfb7":"add_score_model(score_tune_rt)","6f1ab4d6":"# Choose the type of classifier. \nbagging_estimator_tuned = BaggingClassifier(random_state=1)\n\n# Grid of parameters to choose from\nparameters = {'max_samples': [0.7,0.8,0.9,1], \n              'max_features': [0.7,0.8,0.9,1],\n              'n_estimators' : [10,20,30,40,50,90],\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = metrics.make_scorer(metrics.recall_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(bagging_estimator_tuned, parameters, scoring=acc_scorer,cv=5)\ngrid_obj = grid_obj.fit(X_train_scaled_df, y_train)\n\n# Set the clf to the best combination of parameters\nbagging_estimator_tuned = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data.\nbagging_estimator_tuned.fit(X_train_scaled_df, y_train)","9bb1167c":"score_tune_bg=get_metrics_score(bagging_estimator_tuned,X_train_scaled_df,X_test_scaled_df,y_train,y_test)","8ca8671e":"add_score_model(score_tune_bg)","02598231":"from sklearn.svm import SVC\nSVC_Classifier=SVC()\nSVC_Classifier.fit(X_train_scaled_df,y_train)\nscore_svc=get_metrics_score(SVC_Classifier,X_train_scaled_df,X_test_scaled_df,y_train,y_test)\nadd_score_model(score_svc)","790e2ad7":"comparison_frame = pd.DataFrame({'Model':['Decision Tree',\n                                          'Random Forest',\n                                          'Bagging Classifier',\n                                          'Tuned Decision Tree',\n                                          'Tuned Random Forest',\n                                          'Bagging Classifier Tuned','SVC'], \n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test}) \n\n#Sorting models in decreasing order of test recall\ncomparison_frame.sort_values(by='Test_Recall',ascending=False)","126de19c":"feature_names = list(X_train.columns)\nplt.figure(figsize=(20,15))\nfrom sklearn import tree\nfrom sklearn.model_selection import GridSearchCV\nout = tree.plot_tree(dtree_estimator,feature_names=feature_names,filled=True,fontsize=9,node_ids=True,class_names=True)\nfor o in out:\n     arrow = o.arrow_patch\n     if arrow is not None:\n        arrow.set_edgecolor('black')\n        arrow.set_linewidth(1)\nplt.show()","43d1c68a":"# Text report showing the rules of a decision tree -\nfeature_names = list(X_train.columns)\nprint(tree.export_text(dtree_estimator,feature_names=feature_names,show_weights=True))","ff80a5c8":"feature_names = X_train.columns\nimportances = dtree_estimator.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12,8))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='violet', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","c255b6cd":"y_pred = dtree_estimator.predict(X_test_scaled_df)\nmake_confusion_matrix(y_test,y_pred,\"confusion matrix on test\")\nmisclass_df = X_test.copy()\nmisclass_df['Actual']=y_test\nmisclass_df['Predicted'] = y_pred\n","25ab03ce":"plt.pie(data=misclass_df,x=misclass_df[\"Actual\"].value_counts(),autopct='%1.1f%%')","1ce0ad55":"plt.pie(data=misclass_df,x=misclass_df[\"Predicted\"].value_counts(),autopct='%1.1f%%')","303a6bb2":"pd.crosstab(misclass_df['Predicted'],misclass_df['Actual'],margins=True)","7e9ebab6":"# Rows that were classified as Negative when they were actually positive\nfn_rows = misclass_df[(misclass_df['Actual'] == 1) & (misclass_df['Predicted'] == 0)]\nFalse_negative= data[data.index.isin(fn_rows.index.values)].copy()\nFalse_negative.sort_values(by=['Glucose','Age','BMI'])","f02ab524":"**Observations** \n* Data set contains women with an average of `4 `pregnancies and maximum of `17.`\n* Features like Glucose, BloodPressure, SkinThickness, and Insulin have minimum values as 0 which might be data input errors and  should explore it further.\n* Difference between maximum value for features like SkinThickness, Insulin and Age and 3rd quartile which suggest ,there might be outliers present in the data.\n* Average age of women in the dataset is`33` years and median is`29`.","a3b1f83d":"### Handling Missing value \nGlucose, Blood pressure, SkinThickness , Insulin ,BMI had  0 values replacing them  by median of the respective variable","f8c457a1":"* Decision tree is overfitting the training data as there is huge disparity between training and test scores for all the metrics.\n* The test recall is very low i.e. only 58%.","aa9c09d9":"**Hyperparameter tuning**\n\n**max_depth:**The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\n**min_samples_leaf:**The minimum number of samples required to be at a leaf node.\n\n**max_leaf_nodes:** Grow a tree with max_leaf_nodes in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n\n**min_impurity_decrease:**A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n\n","cc100d4f":"## Misclassification Analysis","e71c7ac6":"* Diabetes is more prominent in women with more pregnancies.\n* Higher plasma glucose concentration is seen in Women with diabetes. \n* Higher levels of insulin is found in women having diabetes.\n* Women  with diabetes have higher BMI.\n* Diabetic women have higher diabetes pedigree function value suggesting family history matters.\n* Age does play an important role,Diabetes is more prominent in middle age to older aged women.\n* Skin thickness doesnt seem to play any significant role in likelihood of diabetes.","a2d9ccb8":"**Observations**\n* Number of pregnancies is right-skewed.The boxplot shows that there are few outliers to the right. \n* Plasma glucode is normally distrubted.0 value is an outlier for this variable. \n* The distribution for blood pressure looks fairy normal except few outliers evident from the boxplot. We can see are some observations with 0 blood pressure but a 0 value of blood pressure is not possible and we should treat the 0 value as missing data.Most of the women have normal blood pressure.\n* There are one extreme value of 99 in Skin thickness, many value with 0 value of skin thicknessand we should treat the 0 values as missing data.\n* Insulin is right-skewed. There are some outliers to the right  A 0 value in insulin is not possible. We should treat the 0 values as missing data.75% of women have less than 127 mu U\/ml of insulin concentration and an average of 80 mu U\/ml.\n* BMI is normally distributed with the mean and median of approximately 32.There are some outliers in this variable. A 0 value in mass is not possible we should treat the 0 values as missing data.\n* Diabetes pedigree function  is skewed to the right and there are some outliers in this variable.\n* Age is right-skewed.There are outliers on higher end. ","ecac0629":"## Comparing all the models","4577c917":"### Tuning Bagging Classifier","2dbd3542":"* We can see that Glucose concentration is the most important feature followed by Age and BMI.\n* The tuned decision tree is using only three variable to separate the two classes. ","d0c5bd49":"* Surprisingly, the test recall has decreased after hyperparameter tuning and the  model is still overfitting the training data.\n* The confusion matrix shows that the model predicted 37 false negative patients","c4daf2c9":"# Exploratory Data Analysis","1e284143":"# Objective:\n\nTo build a model to predict whether an individual is at risk of diabetes or not.","cfaf76a3":"### Tuning Random Forest","5b181c6f":"Here are my other notebooks....Do checkout if you find my work helpful, happy learning.\n\n1.[Predicting If Customer will borrow Personal Loan ](http:\/\/www.kaggle.com\/yogidsba\/personal-loan-logistic-regression-decision-tree)\n\n2.[Predict Prices of Used cars](https:\/\/www.kaggle.com\/yogidsba\/predict-used-car-prices-linearregression)\n\n3.[Insurance Claim Hypothesis Testing](http:\/\/www.kaggle.com\/yogidsba\/insurance-claims-eda-hypothesis-testing)\n\n4.[Basic EDA on Covid vaccination](http:\/\/www.kaggle.com\/yogidsba\/basic-eda-on-covid-vaccination)\n\n5.[Pandas Tutorial](http:\/\/www.kaggle.com\/yogidsba\/pandas-function-and-data-analysis)\n\n6.[Case study EDA on cardio good fitness](http:\/\/www.kaggle.com\/yogidsba\/casestudy-eda-for-cardio-good-fitness)","a4512a81":"**Table of Contents**\n    \n- [Problem Statement](#Problem-Statement) \n- [Data Set](#Data-Set)\n- [Question](#Questions)\n- [Read and Understand Data](#Read-and-Understand-Data)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis) \n    - [Univariate Analysis](#Univariate-Analysis) \n    - [Bivariate and Multivariate Analysis](#Bivariate-&-Multivariate-Analysis) \n- [Insights based on EDA](#Insights-based-on-EDA)\n- [Model Building](#Model-building)\n- [Conclusion](#Conclusion) \n\n","421a8ac6":"# Problem Statement\nDiabetes is one of the most frequent diseases worldwide and the number of diabetic patients is growing over the years. The main cause of diabetes remains unknown, yet scientists believe that both genetic factors and environmental lifestyle play a major role in diabetes. \n\nIndividuals with diabetes face a risk of developing some secondary health issues such as heart diseases and nerve damage. Thus, early detection and treatment of diabetes can prevent complications and assist in reducing the risk of severe health problems. \nEven though it's incurable, it can be managed by treatment and medication.\n\n**Motivation :- As a person who suffered from gestational diabetes I want to get better understanding of this disease among women and use machine learning models to identify patients who are at risk of diabetes.**\n\n**This week I started learning about Ensemble techniques so trying to use them**\n","581f3939":"## Insights based on EDA","6693c2d1":"# Model building","0166fc52":"### Decision Tree","34af8e94":"**Observations**\n* Outcome variable class shows a moderate correlation with 'Glucose'.\n* There is a positive correlation between age and number of pregnancies which makes sense.\n* Insulin and skin thickness also shows a moderate positive correlation.\n* We can see that most non-diabetic persons have glucose concentration<=100 and BMI<30 \n* However, there are overlapping distributions for diabetic and non-diabetic persons. We should investigate it further.","f456ae49":"## Split Data","234bafe9":"* The test recall has increased significantly after hyperparameter tuning but the  model is still overfitting the training data.\n* The confusion matrix shows that the model is able to identify majority of patients who are at risk of diabetes.","27833aae":"### Observations on Class","855f7035":"### Tuning Decision Tree","b886ab85":"* Random forest is overfitting the training data as there is huge disparity between training and test scores for all the metrics.\n* The test recall is even lower than the decision tree but has a higher test precision.","3afe7ded":"# Questions\n1. What are the factors that affect the likelihood of diabetes?\n2. Does family history of diabetes , can increases chances of daibetes in patients.?\n3. As number of pregnancies increase , does it increase chances of women getting diabetes.?\n4. Is diabetes more prevalent in women who are obese.?","f3077008":"# DataSet Description:\n\n* Pregnancies: Number of times pregnant\n* Glucose: Plasma glucose concentration over 2 hours in an oral glucose tolerance test\n* BloodPressure: Diastolic blood pressure (mm Hg)\n* SkinThickness: Triceps skinfold thickness (mm)\n* Insulin: 2-Hour serum insulin (mu U\/ml)\n* BMI: Body mass index (weight in kg\/(height in m)^2)\n* Pedigree: Diabetes pedigree function - A function that scores likelihood of diabetes based on family history.\n* Age: Age in years\n* Class: Class variable (0: the person is not diabetic or 1: the person is diabetic)","2c06259d":"### Random Forest","a3d14324":"### Feature importance of tuned decision tree","b4a7126b":"* Tuned decision tree is the best model for our data as it has the highest test recall and giving a generalized performance as compared to other models.False negative cases are 20.","ecc840b8":"**Observations**\n* All variables are integer or float types.\n* There are no missing  values in the dataset.\n* Outcome is our Target Variable.\n","36024545":"**n_estimators:** The number of trees in the forest.\n\n**max_depth:** The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples\n\n**min_samples_leaf:** The minimum number of samples required to be at a leaf node.\n\n**max_features:** The number of features to consider when looking for the best split\nIf int, then consider max_features features at each split.\nIf float, then max_features is a fraction and round(max_features * n_features) features are considered at each split.\nIf \u201cauto\u201d, then max_features=sqrt(n_features).\nIf \u201csqrt\u201d, then max_features=sqrt(n_features) (same as \u201cauto\u201d).\nIf \u201clog2\u201d, then max_features=log2(n_features).\nIf None, then max_features=n_features.\n\n**max_samples:** If bootstrap is True, the number of samples to draw from X to train each base estimator.\n\n\n","9fcac342":"## Conclusion\n* Glucose, Age, and BMI are the most important factors in identifying persons who are at risk of diabetes. Other variables' importance is not significant. \n* Identifying the risk of diabetes at early stages, especially among pregnant women, can help to control the disease and prevent the second health problems.\n* As per the decision tree We can have Profile for women\n    - `Lower Risk Women` :Glucose level <=127 ,Age <=28.\n    - `Higher Risk Women`:Glucose level >100 ,Age >28 \/BMI> 28.5\n    \n    But as per the patterns we see in misclassfied data.Our model is unable to find those case when Age or BMI plays major role than Glucose concentration.\n    So Glucose ,Age and BMI should be  given equal importance in identifying high risk patients.\n * Need to further explore if any other algorthims or more hypertuning parameters help in improving the accuracy of the model along with recall.\n \n ## Recommendation\n  Depending on the patients profile following recommendation to patients can prevent them from getting diabetes.\n* Middle aged to older women have a higher risk of diabetes. They should keep the glucose level in check and take proper precautions.\n* Overweight women have a higher risk of diabetes. They should keep the glucose level in check and exercise regularly. \n","3c653943":"**Observations**\n* The data is slightly imbalanced as there are only ~35% of the women in data who are diabetic and ~65% of women who are not diabetic.\n* The most common number of pregnancies amongst women is 1.\n* Surprisingly, there are many observations with more than 10 pregnancies.","70dd3886":"Percentage of value predicted by our model close to the actual values. Lets find out False Negative and False Positive observations","7efb0031":"* Bagging classifier giving similar performance as random forest.\n* It is also overfitting the training data and lower test recall than decision trees.","0cc23126":"### Univariate analysis","55feec6f":"### Bivariate Analysis","b370e466":"* The test recall has increased to 75 and the decision tree is giving a generalized performance.\n* The confusion matrix shows that false negative cases are decreased.","f39f6025":"## Model evaluation criterion\n\n### Model can make wrong predictions as:\n1. Predicting a person doesn't have diabetes when in reality the person has diabetes.[False Negative, undiagnosed illness]\n2. Predicting a person has diabetes, when in reality the person doesn't have diabetes.[False Postive, require more Testing]\n\n### Which case is more important? \n* Predicting a person doesn't have diabetes, when in reality the person has diabetes is a bigger risk.The person will go undiagnosed resulting into severe helath impact.\n\n### Which metric to optimize?\n* We would want Recall to be maximized, the greater the Recall higher the chances of minimizing false negatives because if a model predicts that a person is at risk of diabetes and in reality, that person doesn't have diabetes then that person can go through further levels of testing to confirm whether the person is actually at risk of diabetes but if we predict that a person is not at risk of diabetes when in reality  the person is at risk of diabetes then that person will go undiagnosed and this would lead to further health problems.","9742aedb":"### Bagging Classifier","434e483c":"# Read and Understand Data","0b654c61":"<center><img src=https:\/\/cdn.diabetesselfmanagement.com\/2019\/05\/Turn-Into.jpg width=\"700\" height=\"400\"> <\/center>\nsource https:\/\/cdn.diabetesselfmanagement.com\/2019\/05\/Turn-Into.jpg ","834cd269":"**Observation**\n* There is not much difference between the blood pressure levels of a diabetic and a non-diabetic person.\n* There is one outlier with very high skin thickness in diabetic patients\n* Skin thickness  doesn't  seem to significant in likelihood of diabetes.","c028ba05":"Model was not able to predict correctly `13` patients whose glucose levels were less than 127 but either there BMI was high or Age.`7` patients who glucose concentration was high but BMI was low were also predicted incorrectly."}}