{"cell_type":{"ca668f7f":"code","796bd7ca":"code","610b966a":"code","849d877a":"code","dd352f2b":"code","14f35df5":"code","6e16e8fc":"code","a1ea14ec":"code","49a80a11":"code","f1391cf9":"code","58521e57":"code","a4a894f3":"code","14290f62":"code","f68a39c6":"code","c5e81481":"code","499da6a1":"code","a6d0f673":"code","006a63dd":"code","642252ed":"code","e577ac30":"code","d1b83979":"code","aca14a8c":"code","f7cee8dc":"markdown","b64faabe":"markdown","a099feab":"markdown","72734717":"markdown","a33ad021":"markdown","8781e88f":"markdown","7de6b181":"markdown","6a2e7d45":"markdown","79519aa4":"markdown","08938122":"markdown","40b5cbdb":"markdown","da154c1a":"markdown","4c70abc6":"markdown"},"source":{"ca668f7f":"%%sh\npip install MulticoreTSNE pysam pyvcf;\napt-get install tabix;","796bd7ca":"import bz2\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objs as go\nimport pysam\nimport requests\nimport seaborn as sns\nimport umap\nimport vcf\n\nfrom plotly.offline import init_notebook_mode, iplot\nfrom MulticoreTSNE import MulticoreTSNE as mTSNE\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\n\nimport warnings\nwarnings.filterwarnings('ignore')\ninit_notebook_mode(connected=True) #plotly","610b966a":"def vcf2df(vcf_fname):\n    \"\"\"Convert a subsetted vcf file to pandas DataFrame\"\"\"\n    vcf_reader = vcf.Reader(filename=vcf_fname)\n    \n    df = pd.DataFrame(index=vcf_reader.samples)\n    for variant in vcf_reader:\n        df[variant.ID] = [call.gt_type if call.gt_type is not None else 3 for call in variant.samples]\n\n    return df","849d877a":"df = vcf2df('\/kaggle\/input\/ancestry-informative-snps\/Kidd.55AISNP.1kG.vcf')","dd352f2b":"df.head(3)","14f35df5":"samples = 'ftp:\/\/ftp.1000genomes.ebi.ac.uk\/vol1\/ftp\/release\/20130502\/integrated_call_samples_v3.20130502.ALL.panel'\ndfsamples = pd.read_csv(samples, sep='\\t')\ndfsamples.set_index('sample', inplace=True)\ndfsamples.drop(['Unnamed: 4', 'Unnamed: 5'], inplace=True, axis=1)\n\n# Each sample is assigned a population, a super population, and a gender\ndfsamples.head(3)","6e16e8fc":"dfaim = pd.read_csv('\/kaggle\/input\/ancestry-informative-snps\/Kidd_55_AISNPs.txt', sep='\\t')\ndfaim.head(3)","a1ea14ec":"%%sh\ncd \/kaggle\/input\/23andme2vcf\/23andme2vcf\/23andme2vcf\/\n\n# you may need to change this variable\nVERSION_23ANDME=4\n\nperl \/kaggle\/input\/23andme2vcf\/23andme2vcf\/23andme2vcf\/23andme2vcf.pl \\\n\/kaggle\/input\/my23andme\/my23andme.txt \\\n\/kaggle\/working\/my23andme.vcf \\\n\"$VERSION_23ANDME\"\n\ncd \/kaggle\/working\n\nbgzip \/kaggle\/working\/my23andme.vcf -f && tabix \/kaggle\/working\/my23andme.vcf.gz","49a80a11":"# make sure the filename is the correct path to your created .vcf file\nmy_vcf_filename = '\/kaggle\/working\/my23andme.vcf.gz'\n\nvcf_reader = vcf.Reader(filename=my_vcf_filename)\n\n# create a small DataFrame for your data\ndfme = pd.DataFrame(index=['me'], columns=df.columns)\ndfme.loc['me'] = np.repeat([3], dfme.shape[1])\n\n# iterate over the AISNPs and query your genotypes at these locations\n# assign them to dfme\nfor i, row in dfaim.iterrows():\n    chrom = row['Chr']\n    pos = row['Build 37 nt position'].replace(',', '')\n    rsid = row['dbSNP rs#']\n    # The file is indexed with tabix, so we can quickly extract the location with `fetch()`\n    for variant in vcf_reader.fetch('chr{}'.format(chrom), int(pos)-1, int(pos)):\n        dfme.loc['me', rsid] = [call.gt_type if call.gt_type is not None else 3 for call in variant.samples][0]","f1391cf9":"# how many missing genotypes? Count 3's\ndfme.T['me'].value_counts()","58521e57":"ncols = len(df.columns)\nohe = OneHotEncoder(categories=[range(4)] * ncols, sparse=False)\n\nX = ohe.fit_transform(df.values)\nX_me = ohe.transform(dfme.values)","a4a894f3":"def reduce_dim(X, X_me, algorithm='PCA', n_components=3):\n    \"\"\"Reduce the dimensionality of the 55 AISNPs\n    :param X: One-hot encoded 1kG 55 AISNPs.\n    :type X: array\n    :param X_me: Your one-hot encoded genotype array\n    :type X_me: array\n    :param algorithm: The type of dimensionality reduction to perform. \n        One of {PCA, UMAP, TSNE}\n    :type algorithm: str \n    :param n_components: The number of components to return in X_red \n    :type n_components: int\n    \n    :returns: The transformed X[m, n] array - reduced to X[m, n_components] by `algorithm`\n    \"\"\"\n    \n    if algorithm == 'PCA':\n        pca = PCA(n_components=n_components).fit(X)\n        X_red = pca.transform(X)\n        X_red_me = pca.transform(X_me)\n        # merge your data into the same table as the reference data\n        X_merged = np.vstack((X_red, X_red_me))\n        df_red = pd.DataFrame(X_merged, \n                              index=df.index.append(dfme.index), \n                              columns=['component1', 'component2', 'component3'])\n    elif algorithm == 'TSNE':\n        # TSNE, Barnes-Hut have dim <= 3\n        if n_components > 3:\n            print('The Barnes-Hut method requires the dimensionaility to be <= 3')\n            return None\n        else:\n            X_merged = np.vstack((X, X_me))\n            X_red = mTSNE(n_components=n_components, n_jobs=4).fit_transform(X_merged)\n            df_red = pd.DataFrame(X_red, \n                                  index=df.index.append(dfme.index), \n                                  columns=['component1', 'component2', 'component3'])\n    elif algorithm == 'UMAP':\n        umap_ = umap.UMAP(n_components=n_components).fit(X)\n        X_red = umap_.transform(X)\n        X_red_me = umap_.transform(X_me) \n        # merge your data into the same table as the reference data\n        X_merged = np.vstack((X_red, X_red_me))\n        df_red = pd.DataFrame(X_merged, \n                              index=df.index.append(dfme.index), \n                              columns=['component1', 'component2', 'component3'])\n    else:\n        return None\n    \n    return df_red","14290f62":"# Here, we choose PCA\ndf_dim = reduce_dim(X, X_me, algorithm='PCA')","f68a39c6":"# add a new record for yourself in dfsamples\ndfsamples.loc['me'] = ['me', 'me', 'male']\ndf_dim = df_dim.join(dfsamples)","c5e81481":"def generate_figure_image(groups, layout):\n    data = []\n\n    for idx, val in groups:\n        if idx == 'me':\n            scatter = go.Scatter3d(\n            name=idx,\n            x=val.loc[:, 'component1'],\n            y=val.loc[:, 'component2'],\n            z=val.loc[:, 'component3'],\n            text=[idx for _ in range(val.loc[:, 'component1'].shape[0])],\n            textposition='middle right',\n            mode='markers',\n            marker=dict(\n                size=12,\n                symbol='diamond'\n                )\n            )\n        else:\n            scatter = go.Scatter3d(\n                name=idx,\n                x=val.loc[:, 'component1'],\n                y=val.loc[:, 'component2'],\n                z=val.loc[:, 'component3'],\n                text=[idx for _ in range(val.loc[:, 'component1'].shape[0])],\n                textposition='middle right',\n                mode='markers',\n                marker=dict(\n                    size=4,\n                    symbol='circle'\n                )\n            )\n        data.append(scatter)\n\n    figure = go.Figure(\n        data=data,\n        layout=layout\n    )\n    return figure","499da6a1":"layout = go.Layout(\n            margin=dict(l=0, r=0, b=0, t=0),\n            scene=dict(\n                xaxis=dict(\n                    title='Component 1',\n                    showgrid=True,\n                    zeroline=False,\n                    showticklabels=True\n                ),\n                yaxis=dict(\n                    title='Component 2',\n                    showgrid=True,\n                    zeroline=False,\n                    showticklabels=True\n                ),\n                zaxis=dict(\n                    title='Component 3',\n                    showgrid=True,\n                    zeroline=False,\n                    showticklabels=True\n                )\n            )\n        )","a6d0f673":"pop_resolution = 'super_pop'\ngroups = df_dim.groupby(pop_resolution)\nfigure = generate_figure_image(groups, layout)","006a63dd":"iplot(figure)","642252ed":"# output a static image\nplt.figure(figsize=(8,8));\nplt.title('Genomic Visualizations');\nsns.scatterplot(x='component1', y='component2', data=df_dim, hue=pop_resolution);\nplt.savefig('my23andme.png')","e577ac30":"df_dim = reduce_dim(X, X_me, algorithm='TSNE')\ndf_dim = df_dim.join(dfsamples)\ngroups = df_dim.groupby(pop_resolution)\nfigure = generate_figure_image(groups, layout)\niplot(figure)","d1b83979":"df_dim = reduce_dim(X, X_me, algorithm='UMAP')\ndf_dim = df_dim.join(dfsamples)\ngroups = df_dim.groupby(pop_resolution)\nfigure = generate_figure_image(groups, layout)\niplot(figure)","aca14a8c":"# remove your genetic data from the working directory\n!rm my23andme.vcf.gz my23andme.vcf.gz.tbi sites_not_in_reference.txt","f7cee8dc":"# Plot with TSNE","b64faabe":"# Convert your 23andMe file to a common genomics file format: .vcf\nYour downloaded file from 23andMe has one of  `v3`, `v4` or `v5` in the filename, you should appropriately change the `VERSION_23ANDME` variable below to be either `3, 4, or 5`. (check the file name of the `.zip` archive if you don't recall, since I instructed you to change the name...)\n\n`\"X sites were not included; ...` is OK to see from the output of the next cell!","a099feab":"# Reduce the dimensionality\nof the reference genotypes and your genotypes -- then add your data to the reference data.\n\nUse `reduce_dim` and try selecting different algorithms from one of `{'PCA', 'TSNE', 'UMAP'}`","72734717":"# Convert your vcf to a DataFrame","a33ad021":"# Genomic Visualization via Dimensionality Reduction\nThis notebook will show you how to embed and visualize your 23andMe genotype data using samples from the [1000 Genomes Project](http:\/\/www.internationalgenome.org\/) as a reference population.  Ancestry-informative snps (single nucleotide polymorphisms) are locations in the genome that have significant variance across global populations. There are several scientific publications which have shared these locations and this kernel uses 55 AISNPs from [Kidd et al.](https:\/\/www.ncbi.nlm.nih.gov\/pubmed?db=pubmed&cmd=Retrieve&dopt=citation&list_uids=24508742) Dimensionality redcution techniques include PCA, t-SNE, and UMAP. \n\nThis kernel depends on a few files in the `input\/` directories:  \n`my23andme\/` -- This directory will have **your** 23andMe file (keep it private)  \n`23andme2vcf\/` -- perl script to convert your 23andMe file to a common genomics file format called [.vcf](https:\/\/samtools.github.io\/hts-specs\/VCFv4.2.pdf)  \n`ancestry_informative_snps\/` -- This directory contains genotype data from the 1000 Genomes Project, the reference population that we'll fit our dimenionality reduction methods on. The directory also contains genomic coordinates for 55 locations in the genome that display significant variance between continental populations.\n\n![3d](https:\/\/s3.amazonaws.com\/ancestry-snps-1kgp\/tgviz3d.png)\n\n# Follow these steps to visualize your genotype data:\n1. Download your raw data from 23andMe using [this link](https:\/\/you.23andme.com\/tools\/data\/download\/).  \n2. Check your email and unzip the `genome_Your_Name_.zip`  \n3. Fork this kernel!   \n4. Click \"+ Add Data\" to add your downloaded genotype file from 23andMe.  \n    a. Name the Dataset `my23andme`  \n    b. Rename the file to `my23andme.txt`  \n    c. You should keep this dataset private because multiple users can't have similarly named public datasets (I think)\n5. Follow along below :)\n \n![gif](http:\/\/g.recordit.co\/gcJtejs1NN.gif)\n\n# Share screenshots of your 3D graph in the comments to show the diversity of the Kaggle community\nIf you're interested in learning more about the pre-processing techniques or interactive visualization with plotly's Dash, I created an [app](http:\/\/tgviz.herokuapp.com\/) with the [source code hosted on GitHub](https:\/\/github.com\/arvkevi\/tgviz). ","8781e88f":"# Read the 55 ancestry-informative snp locations into a DataFrame","7de6b181":"# One Hot Encode genotypes","6a2e7d45":"# Convert the 1000 Genomes Project genotypes to a DataFrame","79519aa4":"These aren't really class labels, but they represent the populations and subpopulations that the 1000 Genomes Project samples identified as.  \nA more detailed description of these populations can be found [here](http:\/\/www.internationalgenome.org\/faq\/which-populations-are-part-your-study\/).","08938122":"# Plot with UMAP","40b5cbdb":"# Inspect the encoded DataFrame\nGenotype Encodings:  \n0 = reference allele \/ reference allele    \n1 = reference allele \/ alternate allele  \n2 = alternate allele \/ alternate allele  \n3 = Unknown\n\nExample:  \nSample `HG00096` (row 1) at position `rs7554936` (column 2) is `1`, meaning this person has a `C\/T` at this location.\nIf you [lookup this snp in gnomad](https:\/\/gnomad.broadinstitute.org\/variant\/1-151122489-C-T), notice in the **Population Frequencies** table, this snp is an excellent discriminator between `East Asian`(85% allele frequency) and `South Asian` (0% allele frequency) populations.","da154c1a":"# Join the samples DataFrame with the reduced dimensions DataFrame\n\nThis will add the population \"labels\" in the dimensionality-reduced DataFrame","4c70abc6":"# Plot with Plotly!"}}