{"cell_type":{"1d54fff6":"code","0a3a2795":"code","f5e43317":"code","36420739":"code","06c9eab8":"code","df3c3f6b":"code","7a869d07":"code","2985b8fc":"code","38c02564":"code","007bc660":"code","13f70166":"code","8bae4885":"code","3a005304":"code","ef9ca4ae":"code","ef19af3b":"code","a44ccb22":"code","35a81eb4":"code","ffe53941":"code","8b0eff8e":"code","ab4efa7f":"code","f9962ecc":"code","7cdba616":"code","4383a9a4":"code","c131e7a4":"markdown","f1d663b6":"markdown","c8c5d7d1":"markdown"},"source":{"1d54fff6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","0a3a2795":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', None)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport datetime\nimport lightgbm as lgb\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, GridSearchCV, RepeatedStratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nimport os\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nimport json\nimport ast\nimport time\nfrom sklearn import linear_model\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport shap\n\nfrom mlxtend.feature_selection import SequentialFeatureSelector as SFS\nfrom mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.feature_selection import GenericUnivariateSelect, SelectPercentile, SelectKBest, f_classif, mutual_info_classif, RFE\nimport statsmodels.api as sm\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom catboost import CatBoostClassifier","f5e43317":"train = pd.read_csv('..\/input\/dont-overfit-ii\/train.csv')\ntest = pd.read_csv('..\/input\/dont-overfit-ii\/test.csv')\nlabels = train.columns.drop(['id', 'target'])\ntest_id=test['id']\ntest_features = test.drop(['id'],axis=1)\ntrain.head()","36420739":"test.head()","06c9eab8":"print(\"Train Shape: \" , train.shape , \"\\nTest Shape:\" , test.shape)","df3c3f6b":"train['target'].value_counts().plot(kind='bar', title='Count (target)')","7a869d07":"train[train.columns[2:]].std().plot('hist')\nplt.title('Distribution of the Standard Deviations of the Features')","2985b8fc":"train[train.columns[2:]].mean().plot('hist')\nplt.title('Distribution of the Means of the Features')","38c02564":"print('Distributions of the first 28 columns')\nplt.figure(figsize=(26,24))\nfor i, col in enumerate(list(train.columns)[2:30]):\n    plt.subplot(7,4,i+1)\n    plt.hist(train[col])\n    plt.title(col)","007bc660":"X_train = train.drop(['id','target'], axis=1)\ny_train = train['target']\nX_test = test.drop(['id'], axis=1)","13f70166":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(ratio='minority', n_jobs=-1)\nX_sm, y_sm = smote.fit_resample(X_train, y_train)\n#outputs X_sm and y_sm as ndarrays, need to convert back to df\nX_train = pd.DataFrame(X_sm, columns=labels)\ny_train = pd.DataFrame(y_sm, columns=['target'])","8bae4885":"cols = X_train.columns\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_train = pd.DataFrame(X_train, columns = cols)\nX = X_train\ny = y_train\n\nX_test = scaler.transform(X_test)\nX_test = pd.DataFrame(X_test, columns = cols)","3a005304":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)","ef9ca4ae":"y_t = pd.Series(y_train.iloc[:,0], name=\"training\")\ny_t.value_counts().plot('bar')\nplt.title('Count of y_train target variable after SMOTE')","ef19af3b":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import ShuffleSplit, GridSearchCV\nfrom sklearn.metrics import log_loss","a44ccb22":"cv_sets = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)\nclassifiers = [RandomForestClassifier(), SVC(), KNeighborsClassifier()]\nparams = [{'n_estimators': [3, 10,30]},\n         {'kernel':('linear','poly','sigmoid','rbf'), 'C':[0.01,0.05,0.025,0.07,0.09,1.0], 'gamma':['scale'], 'probability':[True]},\n         {'n_neighbors': [3,5,7,9]}]","35a81eb4":"best_estimators = []\nfor classifier, param in zip(classifiers,params):\n    grid = GridSearchCV(classifier,param,cv=cv_sets)\n    grid = grid.fit(X_train,y_train)\n    best_estimators.append(grid.best_estimator_)","ffe53941":"for estimator in best_estimators:\n    estimator.fit(X_train, y_train)\n    name = estimator.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    print('**Training set**')\n    train_predictions = estimator.predict(X_train)\n    acc = accuracy_score(y_train, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    train_predictions = estimator.predict_proba(X_train)\n    ll = log_loss(y_train, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    \n    print('**Validation set**')\n    train_predictions = estimator.predict(X_val)\n    acc = accuracy_score(y_val, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    train_predictions = estimator.predict_proba(X_val)\n    ll = log_loss(y_val, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    \nprint(\"=\"*30)","8b0eff8e":"pred = best_estimators[1].predict(X_test)","ab4efa7f":"submission = pd.DataFrame(pred, index = test_id, columns=['target'])","f9962ecc":"submission","7cdba616":"submission['target']=submission['target'].astype('int64')","4383a9a4":"submission.to_csv('submission.csv', index=False)","c131e7a4":"## EDA","f1d663b6":"## Loading Data","c8c5d7d1":"## Modeling"}}