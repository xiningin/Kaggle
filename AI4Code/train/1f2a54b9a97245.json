{"cell_type":{"edd99e45":"code","f53e097d":"code","7d44ceb0":"code","50abd73b":"code","e421293e":"code","f57e5988":"code","b6a78ffd":"code","7b3f9855":"code","06d71c9b":"code","c75cd1e9":"code","9aba4711":"code","b2830291":"code","2d680072":"code","8dba69d0":"code","b6a42ea8":"code","8c34288f":"code","7ef5414f":"code","03afd5a2":"code","83372032":"code","fb818b50":"code","ef34e8f9":"code","c22ee8db":"code","2e5ea82c":"code","e88ef67e":"code","89ab0fa4":"code","bebbf051":"code","9d05345a":"code","2018b366":"code","212b0f97":"code","42ecf832":"code","ac38cc40":"code","cc2b83e1":"code","e9164a48":"code","339e61a3":"code","f852e534":"code","432ee1dd":"code","30508ebc":"code","bd65ea87":"code","b9941fb3":"code","91acc98d":"code","9a856feb":"code","424396f3":"code","49fccbe2":"code","408a2884":"markdown","6db0c7d7":"markdown","edb3a877":"markdown","cf86a834":"markdown","c985ad06":"markdown","4e31dc2d":"markdown","0a0d5263":"markdown","d52d0c1a":"markdown","51859abe":"markdown","ac3463b9":"markdown","5878b97e":"markdown","392cfccb":"markdown","2f50fa8b":"markdown","807de156":"markdown","6478b721":"markdown","49fd253e":"markdown","3e6f8428":"markdown","1bcb2bde":"markdown","f88fffc2":"markdown","da4d9690":"markdown","09569917":"markdown","e69f3af9":"markdown","9a0e6d3f":"markdown","b9ae8a87":"markdown","b7d9d777":"markdown","c73ab0c7":"markdown","7df9fb26":"markdown","6d756426":"markdown","776496d5":"markdown","85a6a209":"markdown","b649cd83":"markdown","2765a6f5":"markdown","5485400e":"markdown","a9afee75":"markdown","66807eb0":"markdown","eda3d93f":"markdown","0bf70c49":"markdown","189cafc8":"markdown","b5516e05":"markdown","cdfb6b14":"markdown","8ac2d0cd":"markdown"},"source":{"edd99e45":"#Importing all the necessary libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom yellowbrick.cluster import KElbowVisualizer\nimport matplotlib.cm as cm\nfrom mpl_toolkits.mplot3d import Axes3D\nimport random\nseed=2022\nrandom.seed(seed)\n","f53e097d":"retail = pd.read_csv('..\/input\/online-retail-customer-clustering\/OnlineRetail.csv', sep=\",\", encoding=\"ISO-8859-1\", header=0)\nretail.head()","7d44ceb0":"retail.describe()","50abd73b":"print(retail[retail['UnitPrice']<0].head(3),'\\n\\n')\nprint(retail[retail['UnitPrice']<0].describe())","e421293e":"retail[retail['UnitPrice']==0].describe()","f57e5988":"retail[retail['Quantity']<=0][0:5]","b6a78ffd":"retail=retail[(retail['Quantity']>0) & (retail['UnitPrice']>0) ]\nprint(retail.isnull().sum())","7b3f9855":"retail[(retail.CustomerID.isnull())].head(5)","06d71c9b":"x=retail[(retail.CustomerID.isnull())].InvoiceNo.unique() #get all the unique invoice numbers where customer\nprint(len(x))\nresponse=0\nfor i in x:\n    if len(retail[   (retail.CustomerID.notnull()) & (retail.InvoiceNo==i)    ]['CustomerID'])>0: \n        response+=1\nprint(response)","c75cd1e9":"\nretail=retail.dropna() #Dropping all the rows with null customer ids","9aba4711":"retail['Amount']=retail['Quantity']*retail['UnitPrice'] # Creating 'Amount' column to see total transaction amount","b2830291":"data_monetary=retail.groupby('CustomerID')['Amount'].sum()\ndata_monetary=data_monetary.reset_index()\ndata_monetary=data_monetary.rename(columns={'Amount':'Monetary'})\ndata_monetary.head()","2d680072":"plt.hist(data_monetary['Monetary'],bins=10,density=True,log=True)","8dba69d0":"data_frequency=retail.groupby('CustomerID')['InvoiceNo'].count()\ndata_frequency=data_frequency.reset_index()\ndata_frequency=data_frequency.rename(columns={'InvoiceNo':'Frequency'})\ndata_frequency.head()","b6a42ea8":"plt.hist(data_frequency['Frequency'],bins=10,density=True,log=True)","8c34288f":"plt.scatter(data_frequency.Frequency,data_monetary['Monetary'])","7ef5414f":"retail['InvoiceDate'] = pd.to_datetime(retail['InvoiceDate'],format='%d-%m-%Y %H:%M')\nmax_date=max(retail['InvoiceDate'])\nretail['Recency']=max_date-retail['InvoiceDate']\ndata_recency=retail.groupby('CustomerID')['Recency'].min()\ndata_recency=data_recency.reset_index()\ndata_recency['Recency']=data_recency['Recency'].dt.days\ndata_recency.head(5)","03afd5a2":"plt.hist(data_recency['Recency'],bins=10,density=True,log=True)","83372032":"# Using random bootstrapping we can see what xth percentiles in our data look like. \ndef bootstrap_sample(amounts):\n    return np.random.choice(amounts, len(amounts), replace=True)\n\ndef percentile(sample,percent):\n     return np.percentile(sample, percent)\n\ndef bootstrap_confidence_interval(data,percent):\n    \"\"\"\n    Creates list of 10000 xth percentile bootstrap replicates. \n    \"\"\"\n    bs_samples = np.empty(10000)\n    \n    for i in range(10000):\n        bs_samples[i] = percentile(bootstrap_sample(data),percent)\n\n    return bs_samples","fb818b50":"precentiles={\n    'amount':[],\n    'frequency':[],\n    'recency':[]\n    }\nfor i in [20,40,60,80]:\n    precentiles['amount'].append(np.percentile(bootstrap_confidence_interval(data_monetary['Monetary'],i),95))\n    precentiles['frequency'].append(np.percentile(bootstrap_confidence_interval(data_frequency['Frequency'],i),95))\n    precentiles['recency'].append(np.percentile(bootstrap_confidence_interval(data_recency['Recency'],i),95))\nprecentiles","ef34e8f9":"#Merging all our dataframes to get final RFM data\ndf = data_frequency.merge(data_monetary, on='CustomerID', how='inner').merge(data_recency, on='CustomerID', how='inner')\ndf.head()","c22ee8db":"# create a list of our conditions, more frequency is rated more.\nconditions = [\n    (df['Frequency'] <= precentiles['frequency'][0]),\n    (df['Frequency'] > precentiles['frequency'][0]) & (df['Frequency'] <= precentiles['frequency'][1]) ,\n    (df['Frequency'] > precentiles['frequency'][1]) & (df['Frequency'] <= precentiles['frequency'][2])  ,\n    (df['Frequency'] > precentiles['frequency'][2]) & (df['Frequency'] <= precentiles['frequency'][3])  ,\n    (df['Frequency'] > precentiles['frequency'][3])\n    ]\n\n# create a list of the values we want to assign for each condition\nvalues = [1, 2, 3, 4,5]\n\n# create a new column and use np.select to assign values to it using our lists as arguments\ndf['fscore'] = np.select(conditions, values)\n\n# create a list of our conditions, recency is rated in the opposite order\nconditions = [\n    (df['Recency'] <= precentiles['recency'][0]),\n    (df['Recency'] > precentiles['recency'][0]) & (df['Recency'] <= precentiles['recency'][1]) ,\n    (df['Recency'] > precentiles['recency'][1]) & (df['Recency'] <= precentiles['recency'][2])  ,\n    (df['Recency'] > precentiles['recency'][2]) & (df['Recency'] <= precentiles['recency'][3])  ,\n    (df['Recency'] > precentiles['recency'][3])\n    ]\n\n# create a list of the values we want to assign for each condition\nvalues = [5,4,3,2,1]\n\n# create a new column and use np.select to assign values to it using our lists as arguments\ndf['rscore'] = np.select(conditions, values)\n\n\n# create a list of our conditions, more Amount is rated more\nconditions = [\n    (df['Monetary'] <= precentiles['amount'][0]),\n    (df['Monetary'] > precentiles['amount'][0]) & (df['Monetary'] <= precentiles['amount'][1]) ,\n    (df['Monetary'] > precentiles['amount'][1]) & (df['Monetary'] <= precentiles['amount'][2])  ,\n    (df['Monetary'] > precentiles['amount'][2]) & (df['Monetary'] <= precentiles['amount'][3])  ,\n    (df['Monetary'] > precentiles['amount'][3])\n    ]\n\n# create a list of the values we want to assign for each condition\nvalues = [1, 2, 3, 4,5]\n\n# create a new column and use np.select to assign values to it using our lists as arguments\ndf['ascore'] = np.select(conditions, values)\n\ndf.head()","2e5ea82c":"df1 = data_frequency.merge(data_monetary, on='CustomerID', how='inner').merge(data_recency, on='CustomerID', how='inner')\nfinal_data_scaled=df1.drop(labels=['CustomerID'],axis=1)\nfinal_data_scaled= (final_data_scaled-final_data_scaled.mean())\/(final_data_scaled.std())\nfinal_data_scaled.head(2)","e88ef67e":"model = KMeans()\nvisualizer = KElbowVisualizer(model, k=(2,3,4,5,6))\nvisualizer.fit(final_data_scaled)        # Fit the data to the visualizer\nvisualizer.show() ","89ab0fa4":"range_n_clusters = [2, 3, 4, 5, 6]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, ax = plt.subplots()\n\n\n    # The 1st subplot is the silhouette plot\n\n    ax.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax.set_ylim([1, len(final_data_scaled) +100])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(final_data_scaled)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(final_data_scaled, cluster_labels)\n    print(\n        \"For n_clusters =\",\n        n_clusters,\n        \"The average silhouette_score is :\",\n        silhouette_avg,\n    )\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(final_data_scaled, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        ax.fill_betweenx(\n            np.arange(y_lower, y_upper),\n            0,\n            ith_cluster_silhouette_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.8,\n        )\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax.set_title(\"The silhouette plot for the various clusters.\")\n    ax.set_xlabel(\"The silhouette coefficient values\")\n    ax.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n    plt.suptitle(\n        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n        % n_clusters,\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\nplt.show()","bebbf051":"random.seed(2022)\nmodel = KMeans(n_clusters=3, random_state=0).fit(final_data_scaled)\ndf1['labels']=model.labels_\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(df1['Monetary'], df1['Frequency'],df1['Recency'],c=df1['labels'], cmap='viridis')\nax.set_xlabel('Monetary')\nax.set_ylabel('Frequency')\nax.set_zlabel('Recency')","9d05345a":"sns.boxplot(x=df1['labels'], y=df1['Recency'], data=df1)","2018b366":"df[\"rfm\"] = df['rscore'].astype(str) + ((df[\"fscore\"] + (df[\"ascore\"])\/2).astype(int)).astype(str)","212b0f97":"df=df.drop(labels=['fscore','rscore','ascore'],axis=1)\ndf['rfm']=df['rfm'].astype(int)\ndf.head(5)","42ecf832":"# final_data_scaled_o is just my new dataframe with RFM score added  and then scaled.\nfinal_data_scaled_o=df.drop(labels=['CustomerID'],axis=1)\nfinal_data_scaled_o['Frequency']=(final_data_scaled_o['Frequency']-final_data_scaled_o['Frequency'].mean())\/(final_data_scaled_o['Frequency'].std())\nfinal_data_scaled_o['Monetary']=(final_data_scaled_o['Monetary']-final_data_scaled_o['Monetary'].mean())\/(final_data_scaled_o['Monetary'].std())\nfinal_data_scaled_o['Recency']=(final_data_scaled_o['Recency']-final_data_scaled_o['Recency'].mean())\/(final_data_scaled_o['Recency'].std())\nfinal_data_scaled_o","ac38cc40":"model1 = KMeans()\nvisualizer1 = KElbowVisualizer(model1, k=(2,3,4,5,6))\nvisualizer1.fit(final_data_scaled_o)        # Fit the data to the visualizer\nvisualizer1.show() ","cc2b83e1":"range_n_clusters = [2, 3, 4, 5, 6,7,8,9]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 2 columns\n    fig, ax = plt.subplots()\n    fig.set_size_inches(10, 7)\n\n    # The 1st subplot is the silhouette plot\n\n    ax.set_xlim([-0.1, 1])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax.set_ylim([1, len(final_data_scaled_o) +100])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(final_data_scaled_o)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(final_data_scaled_o, cluster_labels)\n    print(\n        \"For n_clusters =\",\n        n_clusters,\n        \"The average silhouette_score is :\",\n        silhouette_avg,\n    )\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(final_data_scaled_o, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        ax.fill_betweenx(\n            np.arange(y_lower, y_upper),\n            0,\n            ith_cluster_silhouette_values,\n            facecolor=color,\n            edgecolor=color,\n            alpha=0.8,\n        )\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax.set_title(\"The silhouette plot for the various clusters.\")\n    ax.set_xlabel(\"The silhouette coefficient values\")\n    ax.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n    plt.suptitle(\n        \"Silhouette analysis for KMeans clustering on sample data with n_clusters = %d\"\n        % n_clusters,\n        fontsize=14,\n        fontweight=\"bold\",\n    )\n\nplt.show()","e9164a48":"model2 = KMeans(n_clusters=5, random_state=seed).fit(final_data_scaled_o)\ndf['labels']=model2.labels_\nfig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter(df['Monetary'], df['Frequency'],df['Recency'],c=df['labels'], cmap='viridis')\nax.set_xlabel('Monetary')\nax.set_ylabel('Frequency')\nax.set_zlabel('Recency')","339e61a3":"sns.boxplot(x=df['labels'], y=df['Recency'], data=df)","f852e534":"sns.boxplot(x=df['labels'], y=final_data_scaled_o['Frequency'], data=df)","432ee1dd":"sns.boxplot(x=df['labels'], y=final_data_scaled_o['Monetary'], data=df)","30508ebc":"print(df[df['labels']==3],'\\n\\n\\n\\n')\nprint(df[df['labels']==3].describe(),\"\\n\\n\")\n\nprint('Percentage of these customers',((df[df['labels']==3].shape[0])\/df.shape[0])*100)","bd65ea87":"print(df[df['labels']==1].head(),'\\n\\n\\n\\n')\nprint(df[df['labels']==1].describe())\nprint('Percentage of these customers',((df[df['labels']==1].shape[0])\/df.shape[0])*100)","b9941fb3":"print(df[(df['labels']==3) & (df['Recency']<=120) & (df['Frequency']>45)],'\\n\\n')\nprint((df[(df['labels']==3) & (df['Recency']<=120) & (df['Frequency']>45)].shape[0]\/df[df['labels']==3].shape[0])*100)","91acc98d":"print(df[df['labels']==4].head(),'\\n\\n\\n\\n')\nprint(df[df['labels']==4].describe())\nprint('Percentage of these customers',((df[df['labels']==4].shape[0])\/df.shape[0])*100)","9a856feb":"print((df[(df['labels']==4) & (df['Recency']<=53) & (df['Frequency']>65)].shape[0]\/df[df['labels']==4].shape[0])*100)","424396f3":"print(df[df['labels']==2].head(),'\\n\\n\\n\\n')\nprint(df[df['labels']==2].describe())\nprint('Percentage of these customers',((df[df['labels']==2].shape[0])\/df.shape[0])*100)","49fccbe2":"print(df[df['labels']==0].head(),'\\n\\n\\n\\n')\nprint(df[df['labels']==0].describe())\nprint('Percentage of these customers',((df[df['labels']==0].shape[0])\/df.shape[0])*100)","408a2884":"# Label 3, Let us analyze people in this category.","6db0c7d7":"In order to create RFM scores, we need to create qunatiles. Scores will be assigned to each customer based on quantiles.\nIf someone is above 80th percentile, they will have the highest score for frequency and monitary on the other hand if a customer is below 10th percentil, he will have the highest score because we want receny to be the lowest.","edb3a877":"# Analysis without RFM scores","cf86a834":"Recency range - 35 to 73 <br>\nFrequency Range 1 - to 660 <br>\nMonetary Range - 6.2 to 81024 <br>\nPeople in this cluster are labeled as **Can't lose them**. Recency is quite low, means of Frequency and Monetary are high, these customers can ve future potential loyal customers.","c985ad06":"**I will investigate as to what negative price and negative quantity means**, see if there is any information that our data can give us.","4e31dc2d":"# Trying the same approach with RFM scores dataframe","0a0d5263":"# Label 2","d52d0c1a":"When UnitPrice is 0, the mean of quantity is negative, std is very high. It could mean discount or free items.","51859abe":"........................................................................","ac3463b9":"Again, the frequencies are also skewed. Let's check if there is a correlation between frequency and Amount spent.","5878b97e":"Looks like there are no invoice numbers that can give us any customerids back. We have no alternative but to remove all the rows with null customer ids.","392cfccb":"Let us see if Invoice numbers can give us some CustomerIDs back. One way of doing that would be getting invoice numbers of all null customerids and see if that invoice number has any customerid in the whole dataset as every two different customers can not have the same invoice number.","2f50fa8b":"As we can see, in this category, 20% percent customers in this cluster have recency of 115 days or less while average frequency of 45 or higher. This segment of customers are on the verge of being lost and could be saved because frequency and monetary numbers are good","807de156":"After getting the Amount column, we can get monetary data (total of all the transactions of a customer).","6478b721":"We can see the description, **negative unitprice is just debt.**<br>\nAlso, there is no customerid associated with negative prices.\nNow, I will check what negative quantity means.","49fd253e":"Elbow found at k=3, it means that 3 is the optimal number of clusters our elbow method has recommended.<br>\nLet's do some silhouette analysis and see if elbow method and silhouette give same optimal values for k or not.","3e6f8428":"What does 0 unit price mean?","1bcb2bde":"# Reading the Data.","f88fffc2":"People in this caregory are the ones with mean Recency of 272, that means most of the people in this categoty are already lost. But there are still some people with minimum Recency of 186.<br>\nRecency of 186 is okay, given the customers should have Frequency more than the average frequency of people here. Are there any such customers? Let us find out.","da4d9690":"We can see from the plot above, Coustomers labeled 0,2,4 have fairly low recency while lables 1 and 3 means high recency.","09569917":"# Label 4","e69f3af9":"# After having all the data for Recency, Frequency and Monetary we can further investiage our previous findings.","9a0e6d3f":"There is some linear correlation between both but as the we move towards extreme ends of both sides.<br> We will have a look as to why this is happening later on.<br>\nOur next target is geting Recency data, recency means how many days has it been since the customer has made a transaction with us. For recency calculation, we are provided with date and time, we can get the latest date of transaction of a customer and subtract all dates from it.","b9ae8a87":"Recency range - 0 to 14<br>\nFrequency Range - 1 to 7847<br>\nMonetary Range - 35 to 28026<br>\nThese are considered as **loyal** customers with lowest values for recency and highest values for monetary and frequency.","b7d9d777":"Recency range -186  to 373 <br>\nFrequency Range - 1 to 297<br>\nMonetary Range - 3.75 to 77183.60<br>","c73ab0c7":"In this dataframe, I will add 3 new columns based on the scores for RFM.","7df9fb26":"We can see that at k=3, not all clusters are above avg score but at k =3,but our smallest cluster is further divided into 2 parts. Let us go with k=4 and see how the clusters look.","6d756426":"Recency range - 15 to 34 <br>\nFrequency Range - 1 to 1204 <br>\nMonetary Range - 20 to 124914 <br>\nPeople in this cluster are very **Promising**, mean value for recency is 23, for frequency and monetary, mean values are 104 and 1998.","776496d5":"# I will try to replicate this approach and create a new dataset,  I will test kmeans on both my old and new dataset and  analyse the results.","85a6a209":"Recency range - 74 to 185 <br>\nFrequency Range - 1 to 543 <br>\nMonetary Range - 13.0 to 39916 <br>\nPeople in this segment are on **Risk** of being lost.","b649cd83":"Looking at the distributions, all of them are skewed, it will not be easy to detect outliers (if any), in cases like these, using methods like bootstrapping can give powerful results. \nMethods like IQRs are not consistent if the distributions are not normal.<br>\n\nBootstrapping is any method that employs sampling with replacement. We will use sampling with replacement repeatedly to generate many samples of transaction data and build a confidence interval around a sample statistic (e.g. the 99th percentile). The below code can be found[ here](https:\/\/suplari.com\/calculating-outliers-spend-data-bootstrapping\/) with a very nice explanation of what bootstrapping is.","2765a6f5":"***How do the clusters look?***","5485400e":"In this post I am going to talk about Customer Segmentation.<br>\nAn RFM based approach presented in these research papers, [1](https:\/\/www.sciencedirect.com\/science\/article\/pii\/S1319157818304178),[2](https:\/\/arxiv.org\/ftp\/arxiv\/papers\/2008\/2008.08662.pdf).<br>\n\n**RFM analysis**\n\nRecency, frequency and monetary (RFM) analysis is a powerful and recognized technique in database marketing. It is widely used to rank the customers based on their prior purchasing history. RFM analysis finds use in a wide range of applications involving a large number of customers such as online purchase, retailing, etc. This method groups the customers based on three dimensions, recency(R), frequency (F) and monetary (M).\n\nRecency \u2013 When was the last time the customer made a purchase?\nRecency value is the number of days a customer takes between two purchases. A smaller value of recency implies that the customer visits the company repeatedly in a short period. Similarly, a greater value implies that the customer is less likely to visit the company shortly.\n\nFrequency \u2013 How many times did the customer purchase?\nFrequency is defined as the number of purchases a customer makes in a specific period. The higher the value of frequency the more loyal are the customers of the company.\n\nMonetary \u2013 How much money did the customer spend?\nMonetary is defined as the amount of money spent by the customer during a certain period. The higher the amount of money spent the more revenue they give to the company.\nI am going to discuss some approaches on How to deal with retail data.","a9afee75":"# Label 1","66807eb0":"# Outliers - How to determine what customers to consider as outliers?\nIf I have huge transactions, should I be considered an outlier, on the other hand, If I do not have  huge transactions, should I be counted as an outlier and removed from the dataset? Are outliers not important to a company?\n[A better approach proposed in RFM ranking \u2013 An effective approach to customer segmentation](https:\/\/reader.elsevier.com\/reader\/sd\/pii\/S1319157818304178?token=FAFC078DC48D3C1A7822AC89EFFFF0E58E02D64BD5FA1041E13847FA0624ECBBFC8F31CB8982533B5131C6C955708018&originRegion=us-east-1&originCreation=20220124175059) research paper suggests to give a score to each customer depending their RFM values. \n\n![image.png](attachment:dd4592c5-20b0-495a-b1ce-a42f45f769ac.png)","eda3d93f":"Looks like the distribution is skewed and there could be some outliers. I will investigate later. <br>\nNext step is to get frequency data, it tells us how frequent a customer is with transactions. Frequency can be easily calculated using the number of Invoices generated for a particular customer.\n","0bf70c49":"# Label 0","189cafc8":"A lot of null CustomerIDs, let's see if there are any ways if we can get some of these back, as for the Description, we do not need this column for our clustering so I am going to remove this.","b5516e05":"In this segment 19% of people are have recency lower than 53 while frequency higher than 65.","cdfb6b14":"As we have a general idea that if frequency increases, amount spent also increases. So, I will take average of fscore and ascore and concatinate that score with recency score.\nWhat it means is if a customer has a RFM score of 5 2 2, he will have a score of 54.","8ac2d0cd":"I can see that negative quantity means discount or it could also be a free promotional item.<br>\nI am going to remove the transactions with either negative quantity or unitprice less than or equal to zero.<br>\nIf I keep values with unitprice less than or equal to zero, this will increase the frequency of transactions of a customer when in reality the product was given as a promotion<br>\nAfter that, I will take a look at remaining null values."}}