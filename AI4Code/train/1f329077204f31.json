{"cell_type":{"5b2a3358":"code","bde86081":"code","0efdecb2":"code","ddbb3929":"code","ee8248d0":"code","357b2927":"code","ce3662bc":"code","9c9e18f7":"code","1cf2712e":"code","cd010f00":"code","f4992f26":"code","aa1527d8":"code","d2cc67c5":"code","81914ca4":"code","61d09f12":"code","c51d01aa":"code","7f1a3f5d":"code","9ceab4a2":"code","1aecaf22":"code","4d512e64":"code","47202bb2":"markdown","0486f60c":"markdown","8a7f92a9":"markdown","4a4a435f":"markdown","07ea5aa6":"markdown","9eb7e81a":"markdown","97a5bc4e":"markdown","99bb30e7":"markdown","e14abe40":"markdown","87362810":"markdown","ff35af6d":"markdown","ceb879e1":"markdown","57c5d2eb":"markdown","5d6d9676":"markdown","633bd3b4":"markdown","0d1503fb":"markdown","8f615807":"markdown","44d1bb94":"markdown","1f3412ef":"markdown","1fb3015f":"markdown"},"source":{"5b2a3358":"infoColumnsAmount = 78\nlabelsAmountGlobal = 14\nmaxBalancingAmount = 100000\nfileDirectories = ['\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', \n                   '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Monday-WorkingHours.pcap_ISCX.csv',\n                  '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Friday-WorkingHours-Morning.pcap_ISCX.csv',\n                  '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Wednesday-workingHours.pcap_ISCX.csv',\n                  '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv',\n                  '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv',\n                  '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv',\n                  '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Tuesday-WorkingHours.pcap_ISCX.csv']","bde86081":"import gc\nimport datetime\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom tensorflow import keras\nfrom sklearn.metrics import f1_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Activation\nimport keras.backend as K\nimport csv\nimport seaborn as sns\nimport sklearn.metrics\nfrom keras import losses\nimport matplotlib.pyplot as pltMath\nimport scikitplot as plt\nimport sklearn.preprocessing as preproc\nimport sklearn as sk\nfrom sklearn.utils.class_weight import compute_class_weight\nimport matplotlib.pyplot as mat\nfrom sklearn.model_selection import KFold\n#import tensorflow.compat.v1 as tf\ntf.version\n#tf.disable_v2_behavior()\n","0efdecb2":"tree = 0#SoftDecisionTree(max_depth=6,n_features=n_features,n_classes=n_classes,max_leafs=None)\n    # optimizer\noptimizer = 0#tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9,beta2=0.999,epsilon=1e-08).minimize(tree.loss)","ddbb3929":"def f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n\n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n\n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)","ee8248d0":"\ndef readDataSets():\n\n    dataSetFriday = pd.read_csv(filepath_or_buffer = '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', low_memory = False)\n    dataSetMonday = pd.read_csv(filepath_or_buffer = '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Monday-WorkingHours.pcap_ISCX.csv', low_memory = False)\n    dataSetThursday = pd.read_csv(filepath_or_buffer = '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Friday-WorkingHours-Morning.pcap_ISCX.csv', low_memory = False)\n    dataSetWednesday = pd.read_csv(filepath_or_buffer = '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Wednesday-workingHours.pcap_ISCX.csv', low_memory = False)\n    dataSetThursdayMorning = pd.read_csv(filepath_or_buffer = '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv', low_memory = False)#dataSetThursdayAfterNoon = pd.read_csv(filepath_or_buffer = '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv', low_memory = False)\n    dataSetFridayMorning = pd.read_csv(filepath_or_buffer = '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', low_memory = False)\n    dataSetTuesdayWorking = pd.read_csv(filepath_or_buffer = '\/kaggle\/input\/minorfolders\/MachineLearningCVEGood\/Tuesday-WorkingHours.pcap_ISCX.csv', low_memory = False)\n    \n    dataSetRawLoad = pd.concat([dataSetFriday, dataSetMonday, dataSetThursday, dataSetWednesday, dataSetThursdayMorning, dataSetFridayMorning, dataSetTuesdayWorking])\n\n    dataSetModified = pd.get_dummies(data = dataSetRawLoad,columns = [' Label'],dtype = float, drop_first = False) #one hot encoding \u0432 \u0434\u0435\u043b\u0435\n    \n    allAttacksType = dataSetModified.columns[infoColumnsAmount:]\n    allAttacksTypeCorrected = []\n    for attackType in allAttacksType:\n        allAttacksTypeCorrected.append(attackType[7:])\n    labelsAmountGlobal = len(allAttacksType)\n    print(datetime.datetime.now(), \"File read finished ^_^\")\n        \n    return (dataSetModified.to_numpy(dtype = float), allAttacksTypeCorrected)\n","357b2927":"def prepareDataSet(dataSet):\n    \n    dataSet = np.nan_to_num(dataSet)\n    np.random.shuffle(dataSet)\n\n    #\u0421\u043a\u0435\u0439\u043b\u0438\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u0443\u043f\u0440\u043e\u0449\u0435\u043d\u0438\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u0412\u044b\u0447\u0438\u0442\u0430\u0435\u043c \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0438 \u0434\u0435\u043b\u0438\u043c \u043d\u0430 \u0441\u0442\u0430\u043d\u0434\u0430\u0440\u0442\u043d\u043e\u0435 \u043e\u0442\u043a\u043b\u043e\u043d\u0435\u043d\u0438\u0435\n    #scaler = preproc.StandardScaler()\n    #scaler.fit(dataSet[:,:infoColumnsAmount])\n    dataSetOverall = np.concatenate((preproc.normalize(dataSet[:,:infoColumnsAmount], norm='l2'), dataSet[:,infoColumnsAmount:]), axis = 1)#np.concatenate((scaler.transform(dataSet[:,:infoColumnsAmount]), dataSet[:,infoColumnsAmount:]), axis = 1 )\n    print(datetime.datetime.now(), \"Scaling finished ^_^\")\n    # \u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u044f\u0435\u043c \u0432\u0441\u0435 \u0434\u0430\u043d\u043d\u044b\u0435\n    overallLen = len(dataSetOverall)\n    dataAmountToTrain = int(0.8 * overallLen)\n    dataAmountToPredict = int(0.2 * overallLen)\n    \n    dataSetTrain = dataSetOverall[:dataAmountToTrain,:]\n    dataSetPredict = dataSetOverall[(dataAmountToTrain + 1):,:]\n    \n    print(datetime.datetime.now(), \"DataSet prepared ^_^\")\n    \n    return (dataSetTrain, dataSetPredict)","ce3662bc":"def synthesizeShortBalancedDataSet(dataSetInput, position):\n    print(datetime.datetime.now(), \"Balancing started ^_^\")\n    columnsAmount = len(dataSetInput[0])\n    # \u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0432\u0441\u0435 \u043b\u0435\u0439\u0431\u043b\u044b, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0443 \u043d\u0430\u0441 \u0438\u043c\u0435\u044e\u0442\u0441\u044f\n    labelArrays = dataSetInput[:, infoColumnsAmount:]\n    #\u0421\u0447\u0438\u0442\u0430\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0445 \u043b\u0435\u0439\u0431\u043b\u043e\u0432, \u0441\u0440\u0435\u0434\u0438 \u0438\u043c\u0435\u044e\u0449\u0438\u0445\u0441\u044f\n    labelAmounts = len(labelArrays[0]) \n    rareLabels = []\n    baseLabels = []\n    allLabels = [[]]\n    for i in range(0, labelAmounts - 1):\n        allLabels.append([])\n    #\u0433\u0440\u0443\u043f\u043f\u0438\u0440\u0443\u0435\u043c \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u044b \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430 \u043f\u043e \u0442\u0438\u043f\u0430\u043c \u0430\u0442\u0430\u043a\u0438 (\u043c\u0430\u0441\u0441\u0438\u0432 \u043c\u0430\u0441\u0441\u0438\u0432\u043e\u0432 \u0441 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u044b\u043c \u0442\u0438\u043f\u043e\u043c \u0430\u0442\u0430\u043a)\n    for i in range(0, len(labelArrays)):\n        for j in range(0, len(labelArrays[i])):\n            if labelArrays[i][j] == 1:\n                allLabels[j].append(dataSetInput[i])\n    # \u0442\u0435\u043f\u0435\u0440\u044c \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043c\u0430\u0441\u0441\u0438\u0432 \u0441 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0442\u0438\u043f\u043e\u0432 \u043a\u0430\u0436\u0434\u044b\u0445 \u0438\u0437 \u0430\u0442\u0430\u043a\n    lengthArray = []\n    for i in allLabels:\n        lengthArray.append(len(i))\n    result = [np.zeros(columnsAmount)]\n    elementsAmount = max(lengthArray)\n    if elementsAmount > maxBalancingAmount:\n        elementsAmount = maxBalancingAmount\n    #print(elementsAmount)\n    #\u0414\u0430\u043b\u0435\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u043c\u0430\u0433\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e \u0446\u0438\u043a\u043b\u0430 \u0434\u043e\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u0442\u0438\u043f\u044b \u0430\u0442\u0430\u043a \u0441 \u0434\u0435\u0444\u0438\u0446\u0438\u0442\u043d\u044b\u043c \u0447\u0438\u0441\u043b\u043e\u043c \u043b\u0435\u0439\u0431\u043b\u043e\u0432 \u0438\u0445 \u043a\u043e\u043f\u0438\u044f\u043c\u0438\n    for (i,j) in zip(allLabels, lengthArray):\n        cuttedArr = [np.zeros(columnsAmount)]\n        #\u0435\u0441\u043b\u0438 \u044d\u043b\u0435\u043c\u0435\u043d\u0442\u043e\u0432 \u043c\u0435\u043d\u044c\u0448\u0435, \u0447\u0435\u043c \u043d\u0430\u043c \u043a\u0430\u0436\u0435\u0442\u0441\u044f \u0430\u0434\u0435\u043a\u0432\u0430\u0442\u043d\u044b\u043c, \u0442\u043e \u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043a\u043e\u043f\u0438\u0438\n        if (j <  elementsAmount):\n            #\u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043a\u043e\u043f\u0438\u0439\n            if len(i) != 0:\n                arrAmounts = int( elementsAmount \/ len(i))\n                cuttedArr = np.repeat(i, arrAmounts, axis = 0)\n                #for t in range(0, arrAmounts):\n                #    cuttedArr = np.concatenate((cuttedArr, i))\n        else:\n            #\u0435\u0441\u043b\u0438 \u0436\u0435 \u0438\u0445 \u0438 \u0442\u0430\u043a \u043c\u043d\u043e\u0433\u043e, \u0442\u043e \u043f\u0440\u043e\u0441\u0442\u043e \u043e\u0431\u0440\u0435\u0437\u0430\u0435\u043c \u0434\u043e \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u0433\u043e\n            cuttedArr = i[1:]\n        result = np.concatenate((result, cuttedArr))\n    # \u043e\u0431\u0440\u0435\u0437\u0430\u044e \u043d\u0443\u043b\u0435\u0432\u043e\u0439 \u044d\u043b\u0435\u043c\u0435\u043d\u0442 (\u043e\u043d \u0431\u044b\u043b \u043d\u0443\u0436\u0435\u043d \u0434\u043b\u044f \u043f\u0440\u043e\u0431\u043b\u0435\u043c \u0441 \u0442\u0438\u043f\u0438\u0437\u0430\u0446\u0438\u0435\u0439)\n    result = result[1:,:]\n    \n    print(datetime.datetime.now(), \"Balancing finished ^_^\")\n    return result","9c9e18f7":"def createModel(labelsTypeAmount):\n    print(labelsTypeAmount)\n    model = Sequential([\n        Dense(labelsTypeAmount, input_shape=(infoColumnsAmount,)),\n        Dense(labelsTypeAmount),\n        Dense(labelsTypeAmount),\n        Dense(labelsTypeAmount, activation = 'softmax')])\n    \n    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    print(datetime.datetime.now(), \"Model created ^_^\")\n    return model","1cf2712e":" # Initialize the variables (i.e. assign their default value)\n#init = tf.global_variables_initializer()\n#sess = tf.Session()\n#sess.run(init)\n\nn_features = infoColumnsAmount\nn_classes = labelsAmountGlobal\nbatch_size = 25 #60\n    \ndef createDecisionTree():\n    global tree\n    tree = SoftDecisionTree(max_depth=6,n_features=n_features,n_classes=n_classes,max_leafs=None)\n    tree.build_tree()\n    # optimizer\n    global optimizer\n    optimizer = tf.train.AdamOptimizer(learning_rate=0.001,beta1=0.9,beta2=0.999,epsilon=1e-08).minimize(tree.loss)\n    ","cd010f00":"def train(model, dataSet, labelsTypeAmount):\n    \n    inputMatrix = dataSet[:,:infoColumnsAmount]\n    outputMatrix = dataSet[:,infoColumnsAmount:]\n\n    epochsAmount = 1\n    splitsAmount = 10\n    acc = []\n    val_acc = []\n    loss = []\n    val_loss = []\n    \n    # \u0412\u043e\u0442 \u0437\u0434\u0435\u0441\u044c \u043c\u044b \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0443\u0435\u043c K-Fold (\u043f\u0435\u0440\u0435\u0442\u0430\u0441\u043e\u0432\u0430\u043b\u0438 \u0438 \u0440\u0430\u0437\u0440\u0435\u043b\u0430\u043b\u0438 \u043c\u0430\u0441\u0441\u0438\u0432\u044b)\n    for train_index, test_index in KFold(n_splits = splitsAmount, shuffle = False).split(inputMatrix):\n        X_train, X_test = inputMatrix[train_index], inputMatrix[test_index]\n        y_train, y_test = outputMatrix[train_index], outputMatrix[test_index]\n        res = model.fit(np.concatenate((X_train, X_test), axis = 0), np.concatenate((y_train, y_test), axis = 0) , batch_size=1000, validation_split = 0.2, shuffle = True, verbose = 1) #, class_weight = class_weights)\n        gc.collect()\n        for i in range(len(res.history['accuracy'])):   \n            acc.append(res.history['accuracy'][i])\n            val_acc.append(res.history['val_accuracy'][i])\n            loss.append(res.history['loss'][i])\n            val_loss.append(res.history['val_loss'][i])\n            \n    return acc, loss, val_acc, val_loss","f4992f26":"def trainLSTM(model, dataSet, labelsTypeAmount):\n    \n    inputMatrix = dataSet[:,:infoColumnsAmount]\n    outputMatrix = dataSet[:,infoColumnsAmount:]\n    \n    outputMatrixSimplified = np.empty(len(inputMatrix[:,0]), dtype=int)\n    \n    #convert output matrix to single form\n    i = 0\n    for element in outputMatrix:\n        j = 0\n        for k in range(len(element)):\n            if element[j] == 1:\n                break\n            j += 1\n        outputMatrixSimplified[i] = j\n        i += 1\n        \n    print(np.amin(outputMatrixSimplified))\n    print(np.amax(outputMatrixSimplified))\n    acc = []\n    val_acc = []\n    loss = []\n    val_loss = []\n    \n    # \u0412\u043e\u0442 \u0437\u0434\u0435\u0441\u044c \u043c\u044b \u043f\u0440\u0430\u043a\u0442\u0438\u043a\u0443\u0435\u043c K-Fold (\u043f\u0435\u0440\u0435\u0442\u0430\u0441\u043e\u0432\u0430\u043b\u0438 \u0438 \u0440\u0430\u0437\u0440\u0435\u043b\u0430\u043b\u0438 \u043c\u0430\u0441\u0441\u0438\u0432\u044b)\n    model.fit(inputMatrix, outputMatrixSimplified, batch_size=1000, validation_split = 0.2, shuffle = True, verbose = 1)","aa1527d8":"def plotBarChart(value1,validationValue1,description):\n    pltMath.figure(figsize=(20,5))\n    pltMath.bar([1,2],[value1[len(value1) - 1],validationValue1[len(validationValue1) - 1]])\n    pltMath.ylabel(description, fontsize=16)\n    pltMath.title(\"Histogram {}\".format(description))\n    pltMath.show()\n\n    pltMath.plot(value1)\n    pltMath.plot(validationValue1)\n    pltMath.title(description)\n    pltMath.ylabel('accuracy')\n    pltMath.xlabel('epoch')\n    pltMath.legend(['train', 'val'], loc='upper left')\n    pltMath.show()\n","d2cc67c5":"def decodeAttack(resultArray, attackTypeArray) -> str:\n    positionOfPredict = 0\n    isFound = False\n    for result in resultArray:\n        if result == 1:\n            isFound = True\n            break\n        positionOfPredict += 1\n    if isFound:\n        return attackTypeArray[positionOfPredict]\n    else:\n        return \"Unrecognized\"","81914ca4":"def convertToStringForm(predictions, attackTypeArray):\n    elementsAmount = len(predictions[:,0])\n    labelArr = np.empty(elementsAmount, dtype='string')\n    i = 0\n    for prediction in predictions:\n        labelArr[i] = decodeAttack(prediction, attackTypeArray)\n    return labelArr\n        ","61d09f12":"def roundPredictions(predictions):\n    i = 0\n    j = 0\n    for prediction in predictions:\n        for labelPoss in prediction:\n            predictions[i][j] = int(round(predictions[i][j]))\n            j += 1\n        j = 0\n        i += 1\n        \n    return predictions","c51d01aa":"def makePrediction(model, dataSetPredict):\n    print(datetime.datetime.now(),\"Prediction Start ^_^\")\n    predictionsNew = model.predict(dataSetPredict[:,:infoColumnsAmount])\n    roundedPrediction = roundPredictions(predictionsNew)\n    \n    \n    print(datetime.datetime.now(),\"Prediction Finish ^_^\")\n    return roundedPrediction\n    ","7f1a3f5d":"def countAccuracy(predicted, expected):\n    overallAmount = len(predicted)\n    correctAmount = 0\n    i = 0\n    for i in range(0, len(predicted) - 1):\n        if predicted[i].all() == expected[i].all():\n            correctAmount += 1\n    \n    return (correctAmount\/overallAmount)","9ceab4a2":"def buildConfusionMatrix(predictions, expected):\n    plt.metrics.plot_confusion_matrix(\n        expected.argmax(axis=1), \n        predictions.argmax(axis=1))","1aecaf22":"def createLSTMModel():\n    inputTeam1 = tf.keras.Input(shape=(infoColumnsAmount,))\n    \n    team1Branch = tf.keras.layers.Embedding(labelsAmountGlobal, 64)(inputTeam1)\n\n    team1Branch = tf.keras.Model(inputs=inputTeam1, outputs=team1Branch)\n    \n    \n    combined = team1Branch.output\n\n    result = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64))(combined)\n    result = tf.keras.layers.Dense(labelsAmountGlobal, activation=\"softmax\")(result)\n    \n    model = tf.keras.Model(inputs=[team1Branch.input], outputs=result)\n    \n    model.compile(loss='sparse_categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\n    print(datetime.datetime.now(), \"Model LSTM created ^_^\")\n    \n    return model","4d512e64":"print(datetime.datetime.now(),\"Start ^_^\")\ndataSet, correctedLabels = readDataSets()\ngc.collect()\ndataSetTrain, dataSetPredict = prepareDataSet(dataSet)\ngc.collect()\n\nprint(correctedLabels)\n\ntrainedModel = createModel(len(correctedLabels)) #createLSTMModel()#\ntrainedModelLSTM = createLSTMModel()\n\nsplittedSets = np.array_split(dataSetTrain, 10)\n\n#createDecisionTree()\n\n# Kaggle \u043f\u0430\u0434\u0430\u0435\u0442 \u043f\u0440\u0438 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0431\u043e\u043b\u044c\u0448\u043e\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435. \u0424\u0438\u043a\u0441 - \u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043d\u0430 \u0447\u0430\u043d\u043a\u0438 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0438 \u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u043a\u0438\nfor chunk in splittedSets:\n\n    chunkBalanced = synthesizeShortBalancedDataSet(chunk, infoColumnsAmount)\n    gc.collect()\n    #train(trainedModel, chunkBalanced, len(correctedLabels))\n    acc,loss,val_acc,val_loss = train(trainedModel, chunkBalanced, len(correctedLabels))\n    plotBarChart(acc,val_acc,'accuracy comparing')\n    plotBarChart(loss,val_loss,'loss comparing')\n    #plotting\n    #history = train(trainedModel, chunkBalanced, len(correctedLabels))\n    #plotBarChart(0.8,2,'Models accuracy comparing', 'Validation models accuracy comparing', history)\n    #plotVaryingGraph(history) \n    #gc.collect()\n    #trainLSTM(trainedModelLSTM, chunkBalanced, len(correctedLabels))\n    #gc.collect()\n\n# base model prediction\n    \npreidctionResult = makePrediction(trainedModel, dataSetPredict)\n\nexpectedResult = dataSetPredict[:,infoColumnsAmount:]\n\naccuracy = countAccuracy(preidctionResult, expectedResult)\n\nbuildConfusionMatrix(preidctionResult, expectedResult)\n\nprint(accuracy)\n\ngc.collect()\n\n# lstm prediction\n\n#preidctionResult = makePrediction(trainedModelLSTM, dataSetPredict)\n\n#expectedResult = dataSetPredict[:,infoColumnsAmount:]\n\n#print(expectedResult)\n\n#accuracy = countAccuracy(preidctionResult, expectedResult)\n\n#buildConfusionMatrix(preidctionResult, expectedResult)\n\n#print(accuracy)\n\n#gc.collect()","47202bb2":"# Attack types\n'BENIGN' \n\n'Bot'\n\n'DDoS'\n\n'DoS GoldenEye'\n\n'DoS Hulk'\n\n'DoS Slowhttptest'\n\n'DoS slowloris'\n\n'FTP-Patator'\n\n'Heartbleed'\n\n'PortScan'\n\n'SSH-Patator'\n\n'Web Attack \ufffd Brute Force'\n\n'Web Attack \ufffd Sql Injection'\n\n'Web Attack \ufffd XSS'\n","0486f60c":"# Make Prediction","8a7f92a9":"# DataSet Reading from file","4a4a435f":"# Required Imports","07ea5aa6":"# Assembly - standart learning","9eb7e81a":"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432 - 78, \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u043e\u0435 - \u043b\u0435\u0439\u0431\u043b\u044b","97a5bc4e":"# Make Visualisation of training","99bb30e7":"# Prepare Data","e14abe40":"# Make Training models - LSTM model","87362810":"# Accuracy counting","ff35af6d":"# Model creation","ceb879e1":"# Metrics","57c5d2eb":"# Balance data","5d6d9676":"# Result Rounding","633bd3b4":"# Attack Type Decoder","0d1503fb":"# Convert to string form","8f615807":"# LSTM\n\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u0441\u0440\u0430\u0432\u043d\u0435\u043d\u0438\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043b\u0430\u043d\u0438\u0440\u043e\u0432\u0430\u043b\u043e\u0441\u044c \u0432\u0437\u044f\u0442\u044c LSTM. \u041e\u0434\u043d\u0430\u043a\u043e, \u0438\u0437-\u0437\u0430 \u0431\u0430\u0433\u0438 \u0432 TensorFlow \u0441\u0434\u0435\u043b\u0430\u0442\u044c \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u044b\u0439 \u0430\u043d\u0430\u043b\u0438\u0437 \u043d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c - \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u043f\u0430\u0434\u0435\u043d\u0438\u0435 \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0430 \u043f\u0440\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438\/\u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0435. \u0421\u0441\u044b\u043b\u043a\u0430 \u043d\u0430 \u043e\u0431\u0441\u0443\u0436\u0434\u0435\u043d\u0438 \u043f\u043e\u0434\u043e\u0431\u043d\u043e\u0433\u043e \u0431\u0430\u0433\u0430 - https:\/\/github.com\/tensorflow\/tensorflow\/issues\/37254\n\u0423\u0434\u0430\u043b\u043e\u0441\u044c \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442\u044c, \u0447\u0442\u043e LSTM \u043e\u0431\u0443\u0447\u0430\u0435\u0442\u0441\u044f \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e \u0434\u043e\u043b\u044c\u0448\u0435","44d1bb94":"# Constants","1f3412ef":"# Build confusion matrix","1fb3015f":"# Make Training models - standart model"}}