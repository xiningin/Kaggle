{"cell_type":{"175b170b":"code","48ee1bca":"code","d2ceb8d2":"code","b11213ff":"code","476b057b":"code","766cfef9":"code","fcdff916":"code","de3abb99":"code","fd85dfcb":"code","585f3ee4":"code","797eb71f":"markdown","f1202a31":"markdown"},"source":{"175b170b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","48ee1bca":"#importing required packages\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.svm import SVC\nimport sklearn\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler","d2ceb8d2":"df1 = pd.read_csv('..\/input\/Absenteeism_at_work.csv', delimiter=',')#dataset\nfeatures = df1.iloc[:, 1:14]#features used for training and testing\ntarget = df1.iloc[:, 14]#values that has to be predicted\nprint(\"Missing Values: \",np.count_nonzero(features.isnull()))#checking for missing values","b11213ff":"#converting to a numpy array\nfeatures = features.values\ntarget = target.values","476b057b":"#dividing the dataset into training and testing in ratio 70%:30%\nf_train, f_test, t_train, t_test = train_test_split(features, target, test_size = 0.3, random_state = 80)","766cfef9":"#SVM\nsvclassifier = SVC(kernel='linear',C=1.0,gamma=0.1)  \nsvclassifier.fit(f_train, t_train)\ny_pred = svclassifier.predict(f_test) \nprint(accuracy_score(t_test, y_pred) * 100)","fcdff916":"#Artificial Neural Network(ANN)\nscaler = StandardScaler()\nscaler.fit(f_train)\n#transforming the data\ntrain_data = scaler.transform(f_train)\ntest_data = scaler.transform(f_test)\n\nmlp = MLPClassifier(hidden_layer_sizes=(4,4,4),activation='tanh',random_state=120, max_iter=5000)\nmlp.fit(train_data,t_train)\npredictions = mlp.predict(test_data)\nprint(accuracy_score(t_test, predictions) * 100)","de3abb99":"#dividing the dataset into training and testing in ratio 80%:20%\nf_train, f_test, t_train, t_test = train_test_split(features, target, test_size = 0.2, random_state = 100)","fd85dfcb":"#Decision Trees\ncrt_entropy = DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=5,\n                                     max_features=None, max_leaf_nodes=None, min_samples_leaf=0.02,\n                                     min_samples_split=0.05, min_weight_fraction_leaf=0.0,\n                                     presort=False, random_state=130, splitter='random')\ncrt_entropy.fit(f_train, t_train)\npred_entropy = crt_entropy.predict(f_test)\nprint(\"Accuracy = \", accuracy_score(t_test, pred_entropy) * 100)","585f3ee4":"#Random Forest\nrf = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, \n                            min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n                            max_features=0.39, max_leaf_nodes=None, min_impurity_decrease=0.0, \n                            min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, \n                            random_state=600, verbose=0, warm_start=False, class_weight=None)\nrf.fit(f_train, t_train)\npred_rf = rf.predict(f_test)\nprint(\"Accuracy = \", accuracy_score(t_test, pred_rf) * 100)","797eb71f":"from sklearn.decomposition import PCA<br>\npca = PCA(.95)<br>\npca.fit(f_train)<br>\ntrain_img = pca.transform(f_train)<br>\ntest_img = pca.transform(f_test)<br>\n**PERFORMING PCA DIDNT HELP US INCREASE THE ACCURACY SO WE DIDNT INCLUDE IT IN OUR TECHNIQUES.**","f1202a31":"From the various classification techniques applied on this dataset, It is clear that Random Forest Classifier if the best technique with an accuracy of **52.702702702702695%**"}}