{"cell_type":{"ed0543b1":"code","e1fe0745":"code","1f4a20d4":"code","98d37495":"code","0e3acd38":"code","49e00466":"code","10013b51":"code","73d0c326":"code","ec3dcd64":"code","29e174ec":"code","529851ae":"code","faa0caf8":"code","6965e033":"code","3c7689fc":"code","50d29756":"code","efa7bbda":"code","bc29abaa":"code","d1395f44":"code","12e29f18":"code","3d34ac9b":"code","e0c3bc0a":"code","1be6b651":"code","c9013c23":"code","f9579a02":"code","a270c325":"code","1e2c4a89":"code","793fc3d0":"code","ac760736":"code","9ed89ba4":"code","06e5f47c":"code","88de9f09":"code","85039962":"code","2429b79a":"code","c5a5ebca":"code","5b44f7dd":"code","fc1aeb5f":"code","822876c0":"code","49bc4de9":"code","0fd40303":"code","169e175d":"code","b76600cf":"code","1ed29997":"code","c561a47e":"code","c28f523d":"code","2acc1d79":"code","4c1c1d07":"code","5e47429c":"code","1b56acec":"code","6e16f5e2":"code","29bbf2ef":"code","6f5e3aa0":"code","1521e39d":"code","e5bf1b1c":"code","e4cb973f":"code","8537e85f":"code","4a5a0742":"code","515dbb27":"code","35ac8b55":"code","bcb05ea2":"code","3b1a3d72":"code","713eee90":"code","38a429bc":"code","7e772131":"code","f3edde85":"code","049e1e1f":"code","2b711fec":"code","76aea996":"code","29b0c8c0":"code","f2ac3d55":"code","9dde8536":"code","bdae7813":"code","ada9a632":"code","f77af3f7":"code","1e86d109":"code","4e011411":"code","08b864c9":"code","19121457":"code","736fbc57":"code","e0963474":"code","b501abc0":"code","04d85162":"code","3e2575ba":"code","6db2d65f":"code","fcc1fb77":"code","d6dca31f":"markdown","5cdab505":"markdown","cef369c4":"markdown","ee075824":"markdown","125ce633":"markdown","e9af8936":"markdown","0720253e":"markdown","a81fdd72":"markdown","0ef5d028":"markdown","10b81adb":"markdown","71e9015e":"markdown","2c2d693d":"markdown","9d52601e":"markdown","dcc2d768":"markdown","362a58e8":"markdown","79ddd1a0":"markdown","a43db129":"markdown","7b547687":"markdown","cfd85a53":"markdown","0654e484":"markdown","6ecb210c":"markdown"},"source":{"ed0543b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e1fe0745":"import pandas as pd\nimport numpy as np\nimport sys\nimport sklearn\nprint(pd.__version__)\nprint(np.__version__)\nprint(sys.version)\nprint(sklearn.__version__)","1f4a20d4":"# attach the column names to the dataset\ncol_names = [\"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\n    \"dst_bytes\",\"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\n    \"logged_in\",\"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\n    \"num_file_creations\",\"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\n    \"is_host_login\",\"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\n    \"srv_serror_rate\",\"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\n    \"diff_srv_rate\",\"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\"label\"]\n\n# KDDTrain+_2.csv & KDDTest+_2.csv are the datafiles without the last column about the difficulty score\n# these have already been removed.\ndf = pd.read_csv(\"..\/input\/kddddd\/KDDTrain_2.csv\", header=None, names = col_names)\ndf_test = pd.read_csv(\"..\/input\/kddtttttt\/KDDTest_2.csv\", header=None, names = col_names)\n\n# shape, this gives the dimensions of the dataset\nprint('Dimensions of the Training set:',df.shape)\nprint('Dimensions of the Test set:',df_test.shape)","98d37495":"df.head(5)","0e3acd38":"df.describe()","49e00466":"print('Label distribution Training set:')\nprint(df['label'].value_counts())\nprint()\nprint('Label distribution Test set:')\nprint(df_test['label'].value_counts())","10013b51":"# colums that are categorical and not binary yet: protocol_type (column 2), service (column 3), flag (column 4).\n# explore categorical features\nprint('Training set:')\nfor col_name in df.columns:\n    if df[col_name].dtypes == 'object' :\n        unique_cat = len(df[col_name].unique())\n        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))\n\n#see how distributed the feature service is, it is evenly distributed and therefore we need to make dummies for all.\nprint()\nprint('Distribution of categories in service:')\nprint(df['service'].value_counts().sort_values(ascending=False).head())","73d0c326":"# Test set\nprint('Test set:')\nfor col_name in df_test.columns:\n    if df_test[col_name].dtypes == 'object' :\n        unique_cat = len(df_test[col_name].unique())\n        print(\"Feature '{col_name}' has {unique_cat} categories\".format(col_name=col_name, unique_cat=unique_cat))","ec3dcd64":"from sklearn.preprocessing import LabelEncoder,OneHotEncoder\ncategorical_columns=['protocol_type', 'service', 'flag']\n# insert code to get a list of categorical columns into a variable, categorical_columns\ncategorical_columns=['protocol_type', 'service', 'flag'] \n # Get the categorical values into a 2D numpy array\ndf_categorical_values = df[categorical_columns]\ntestdf_categorical_values = df_test[categorical_columns]\ndf_categorical_values.head()","29e174ec":"# protocol type\nunique_protocol=sorted(df.protocol_type.unique())\nstring1 = 'Protocol_type_'\nunique_protocol2=[string1 + x for x in unique_protocol]\n# service\nunique_service=sorted(df.service.unique())\nstring2 = 'service_'\nunique_service2=[string2 + x for x in unique_service]\n# flag\nunique_flag=sorted(df.flag.unique())\nstring3 = 'flag_'\nunique_flag2=[string3 + x for x in unique_flag]\n# put together\ndumcols=unique_protocol2 + unique_service2 + unique_flag2\nprint(dumcols)\n\n#do same for test set\nunique_service_test=sorted(df_test.service.unique())\nunique_service2_test=[string2 + x for x in unique_service_test]\ntestdumcols=unique_protocol2 + unique_service2_test + unique_flag2","529851ae":"df_categorical_values_enc=df_categorical_values.apply(LabelEncoder().fit_transform)\nprint(df_categorical_values_enc.head())\n# test set\ntestdf_categorical_values_enc=testdf_categorical_values.apply(LabelEncoder().fit_transform)","faa0caf8":"enc = OneHotEncoder()\ndf_categorical_values_encenc = enc.fit_transform(df_categorical_values_enc)\ndf_cat_data = pd.DataFrame(df_categorical_values_encenc.toarray(),columns=dumcols)\n# test set\ntestdf_categorical_values_encenc = enc.fit_transform(testdf_categorical_values_enc)\ntestdf_cat_data = pd.DataFrame(testdf_categorical_values_encenc.toarray(),columns=testdumcols)\n\ndf_cat_data.head()","6965e033":"trainservice=df['service'].tolist()\ntestservice= df_test['service'].tolist()\ndifference=list(set(trainservice) - set(testservice))\nstring = 'service_'\ndifference=[string + x for x in difference]\ndifference","3c7689fc":"for col in difference:\n    testdf_cat_data[col] = 0\n\ntestdf_cat_data.shape","50d29756":"newdf=df.join(df_cat_data)\nnewdf.drop('flag', axis=1, inplace=True)\nnewdf.drop('protocol_type', axis=1, inplace=True)\nnewdf.drop('service', axis=1, inplace=True)\n# test data\nnewdf_test=df_test.join(testdf_cat_data)\nnewdf_test.drop('flag', axis=1, inplace=True)\nnewdf_test.drop('protocol_type', axis=1, inplace=True)\nnewdf_test.drop('service', axis=1, inplace=True)\nprint(newdf.shape)\nprint(newdf_test.shape)","efa7bbda":"# take label column\nlabeldf=newdf['label']\nlabeldf_test=newdf_test['label']\n# change the label column\nnewlabeldf=labeldf.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\nnewlabeldf_test=labeldf_test.replace({ 'normal' : 0, 'neptune' : 1 ,'back': 1, 'land': 1, 'pod': 1, 'smurf': 1, 'teardrop': 1,'mailbomb': 1, 'apache2': 1, 'processtable': 1, 'udpstorm': 1, 'worm': 1,\n                           'ipsweep' : 2,'nmap' : 2,'portsweep' : 2,'satan' : 2,'mscan' : 2,'saint' : 2\n                           ,'ftp_write': 3,'guess_passwd': 3,'imap': 3,'multihop': 3,'phf': 3,'spy': 3,'warezclient': 3,'warezmaster': 3,'sendmail': 3,'named': 3,'snmpgetattack': 3,'snmpguess': 3,'xlock': 3,'xsnoop': 3,'httptunnel': 3,\n                           'buffer_overflow': 4,'loadmodule': 4,'perl': 4,'rootkit': 4,'ps': 4,'sqlattack': 4,'xterm': 4})\n# put the new label column back\nnewdf['label'] = newlabeldf\nnewdf_test['label'] = newlabeldf_test\nprint(newdf['label'].head())","bc29abaa":"to_drop_DoS = [2,3,4]\nto_drop_Probe = [1,3,4]\nto_drop_R2L = [1,2,4]\nto_drop_U2R = [1,2,3]\nDoS_df=newdf[~newdf['label'].isin(to_drop_DoS)];\nProbe_df=newdf[~newdf['label'].isin(to_drop_Probe)];\nR2L_df=newdf[~newdf['label'].isin(to_drop_R2L)];\nU2R_df=newdf[~newdf['label'].isin(to_drop_U2R)];\n\n#test\nDoS_df_test=newdf_test[~newdf_test['label'].isin(to_drop_DoS)];\nProbe_df_test=newdf_test[~newdf_test['label'].isin(to_drop_Probe)];\nR2L_df_test=newdf_test[~newdf_test['label'].isin(to_drop_R2L)];\nU2R_df_test=newdf_test[~newdf_test['label'].isin(to_drop_U2R)];\nprint('Train:')\nprint('Dimensions of DoS:' ,DoS_df.shape)\nprint('Dimensions of Probe:' ,Probe_df.shape)\nprint('Dimensions of R2L:' ,R2L_df.shape)\nprint('Dimensions of U2R:' ,U2R_df.shape)\nprint('Test:')\nprint('Dimensions of DoS:' ,DoS_df_test.shape)\nprint('Dimensions of Probe:' ,Probe_df_test.shape)\nprint('Dimensions of R2L:' ,R2L_df_test.shape)\nprint('Dimensions of U2R:' ,U2R_df_test.shape)","d1395f44":"# Split dataframes into X & Y\n# assign X as a dataframe of feautures and Y as a series of outcome variables\nX_DoS = DoS_df.drop('label',1)\nY_DoS = DoS_df.label\nX_Probe = Probe_df.drop('label',1)\nY_Probe = Probe_df.label\nX_R2L = R2L_df.drop('label',1)\nY_R2L = R2L_df.label\nX_U2R = U2R_df.drop('label',1)\nY_U2R = U2R_df.label\n# test set\nX_DoS_test = DoS_df_test.drop('label',1)\nY_DoS_test = DoS_df_test.label\nX_Probe_test = Probe_df_test.drop('label',1)\nY_Probe_test = Probe_df_test.label\nX_R2L_test = R2L_df_test.drop('label',1)\nY_R2L_test = R2L_df_test.label\nX_U2R_test = U2R_df_test.drop('label',1)\nY_U2R_test = U2R_df_test.label","12e29f18":"colNames=list(X_DoS)\ncolNames_test=list(X_DoS_test)","3d34ac9b":"from sklearn import preprocessing\nscaler1 = preprocessing.StandardScaler().fit(X_DoS)\nX_DoS=scaler1.transform(X_DoS) \nscaler2 = preprocessing.StandardScaler().fit(X_Probe)\nX_Probe=scaler2.transform(X_Probe) \nscaler3 = preprocessing.StandardScaler().fit(X_R2L)\nX_R2L=scaler3.transform(X_R2L) \nscaler4 = preprocessing.StandardScaler().fit(X_U2R)\nX_U2R=scaler4.transform(X_U2R) \n# test data\nscaler5 = preprocessing.StandardScaler().fit(X_DoS_test)\nX_DoS_test=scaler5.transform(X_DoS_test) \nscaler6 = preprocessing.StandardScaler().fit(X_Probe_test)\nX_Probe_test=scaler6.transform(X_Probe_test) \nscaler7 = preprocessing.StandardScaler().fit(X_R2L_test)\nX_R2L_test=scaler7.transform(X_R2L_test) \nscaler8 = preprocessing.StandardScaler().fit(X_U2R_test)\nX_U2R_test=scaler8.transform(X_U2R_test) ","e0c3bc0a":"print(X_DoS.std(axis=0))","1be6b651":"X_Probe.std(axis=0);\nX_R2L.std(axis=0);\nX_U2R.std(axis=0);","c9013c23":"#univariate feature selection with ANOVA F-test. using secondPercentile method, then RFE\n#Scikit-learn exposes feature selection routines as objects that implement the transform method\n#SelectPercentile: removes all but a user-specified highest scoring percentage of features\n#f_classif: ANOVA F-value between label\/feature for classification tasks.\nfrom sklearn.feature_selection import SelectPercentile, f_classif\nnp.seterr(divide='ignore', invalid='ignore');\nselector=SelectPercentile(f_classif, percentile=10)\nX_newDoS = selector.fit_transform(X_DoS,Y_DoS)\nX_newDoS.shape","f9579a02":"true=selector.get_support()\nnewcolindex_DoS=[i for i, x in enumerate(true) if x]\nnewcolname_DoS=list( colNames[i] for i in newcolindex_DoS )\nnewcolname_DoS","a270c325":"X_newProbe = selector.fit_transform(X_Probe,Y_Probe)\nX_newProbe.shape","1e2c4a89":"true=selector.get_support()\nnewcolindex_Probe=[i for i, x in enumerate(true) if x]\nnewcolname_Probe=list( colNames[i] for i in newcolindex_Probe )\nnewcolname_Probe","793fc3d0":"X_newR2L = selector.fit_transform(X_R2L,Y_R2L)\nX_newR2L.shape","ac760736":"true=selector.get_support()\nnewcolindex_R2L=[i for i, x in enumerate(true) if x]\nnewcolname_R2L=list( colNames[i] for i in newcolindex_R2L)\nnewcolname_R2L","9ed89ba4":"X_newU2R = selector.fit_transform(X_U2R,Y_U2R)\nX_newU2R.shape","06e5f47c":"true=selector.get_support()\nnewcolindex_U2R=[i for i, x in enumerate(true) if x]\nnewcolname_U2R=list( colNames[i] for i in newcolindex_U2R)\nnewcolname_U2R","88de9f09":"print('Features selected for DoS:',newcolname_DoS)\nprint()\nprint('Features selected for Probe:',newcolname_Probe)\nprint()\nprint('Features selected for R2L:',newcolname_R2L)\nprint()\nprint('Features selected for U2R:',newcolname_U2R)","85039962":"'''from sklearn import preprocessing\nfrom sklearn import utils\n\nlab_enc = preprocessing.LabelEncoder()\nencoded = lab_enc.fit_transform(Y_DoS) '''","2429b79a":"Y_DoS=Y_DoS.astype('int')\nY_Probe=Y_Probe.astype('int')\nY_R2L=Y_R2L.astype('int')\nY_U2R=Y_U2R.astype('int')","c5a5ebca":"from sklearn.feature_selection import RFE\nfrom sklearn.tree import DecisionTreeClassifier\n# Create a decision tree classifier. By convention, clf means 'classifier'\nclf = DecisionTreeClassifier(random_state=0)\n\n#rank all features, i.e continue the elimination until the last one\nrfe = RFE(clf, n_features_to_select=1)\nrfe.fit(X_newDoS, Y_DoS)\nprint (\"DoS Features sorted by their rank:\")\nprint (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_DoS)))","5b44f7dd":"rfe.fit(X_newProbe, Y_Probe)\nprint (\"Probe Features sorted by their rank:\")\nprint (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_Probe)))","fc1aeb5f":"rfe.fit(X_newR2L, Y_R2L)\n \nprint (\"R2L Features sorted by their rank:\")\nprint (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_R2L)))","822876c0":"rfe.fit(X_newU2R, Y_U2R)\n \nprint (\"U2R Features sorted by their rank:\")\nprint (sorted(zip(map(lambda x: round(x, 4), rfe.ranking_), newcolname_U2R)))","49bc4de9":"from sklearn.feature_selection import RFE\nclf = DecisionTreeClassifier(random_state=0)\nrfe = RFE(estimator=clf, n_features_to_select=13, step=1)\nrfe.fit(X_DoS, Y_DoS)\nX_rfeDoS=rfe.transform(X_DoS)\ntrue=rfe.support_\nrfecolindex_DoS=[i for i, x in enumerate(true) if x]\nrfecolname_DoS=list(colNames[i] for i in rfecolindex_DoS)","0fd40303":"rfe.fit(X_Probe, Y_Probe)\nX_rfeProbe=rfe.transform(X_Probe)\ntrue=rfe.support_\nrfecolindex_Probe=[i for i, x in enumerate(true) if x]\nrfecolname_Probe=list(colNames[i] for i in rfecolindex_Probe)","169e175d":"rfe.fit(X_R2L, Y_R2L)\nX_rfeR2L=rfe.transform(X_R2L)\ntrue=rfe.support_\nrfecolindex_R2L=[i for i, x in enumerate(true) if x]\nrfecolname_R2L=list(colNames[i] for i in rfecolindex_R2L)","b76600cf":"rfe.fit(X_U2R, Y_U2R)\nX_rfeU2R=rfe.transform(X_U2R)\ntrue=rfe.support_\nrfecolindex_U2R=[i for i, x in enumerate(true) if x]\nrfecolname_U2R=list(colNames[i] for i in rfecolindex_U2R)","1ed29997":"print('Features selected for DoS:',rfecolname_DoS)\nprint()\nprint('Features selected for Probe:',rfecolname_Probe)\nprint()\nprint('Features selected for R2L:',rfecolname_R2L)\nprint()\nprint('Features selected for U2R:',rfecolname_U2R)","c561a47e":"print(X_rfeDoS.shape)\nprint(X_rfeProbe.shape)\nprint(X_rfeR2L.shape)\nprint(X_rfeU2R.shape)","c28f523d":"print(X_DoS.shape)","2acc1d79":"#All features\n#from sklearn.ensemble import IsolationForest\n#clf_DoS = IsolationForest(random_state=0)\n#clf_Probe = IsolationForest(random_state=0)\n#clf_R2L= IsolationForest(random_state=0)\n#clf_U2R = IsolationForest(random_state=0)\n#clf_DoS.fit(X_DoS, Y_DoS)\n#clf_Probe.fit(X_Probe, Y_Probe)\n#clf_R2L.fit(X_R2L, Y_R2L)\n#clf_U2R.fit(X_U2R, Y_U2R)\nfrom sklearn.neighbors import KNeighborsClassifier\nKNN_DoS=KNeighborsClassifier()\nKNN_Probe=KNeighborsClassifier()\nKNN_R2L=KNeighborsClassifier()\nKNN_U2R=KNeighborsClassifier()\nKNN_DoS.fit(X_DoS,Y_DoS)\nKNN_Probe.fit(X_Probe,Y_Probe)\nKNN_R2L.fit(X_R2L,Y_R2L)\nKNN_U2R.fit(X_U2R,Y_U2R)","4c1c1d07":"# selected features\n#clf_rfeDoS = IsolationForest(random_state=0)\n#clf_rfeProbe = IsolationForest(random_state=0)\n#clf_rfeR2L= IsolationForest(random_state=0)\n#clf_rfeU2R = IsolationForest(random_state=0)\n#clf_rfeDoS.fit(X_rfeDoS, Y_DoS)\n#clf_rfeProbe.fit(X_rfeProbe, Y_Probe)\n#clf_rfeR2L.fit(X_rfeR2L, Y_R2L)\n#clf_rfeU2R.fit(X_rfeU2R, Y_U2R)\nKNN_rfeDoS=KNeighborsClassifier()\nKNN_rfeProbe=KNeighborsClassifier()\nKNN_rfeR2L=KNeighborsClassifier()\nKNN_rfeU2R=KNeighborsClassifier()\nKNN_rfeDoS.fit(X_rfeDoS,Y_DoS)\nKNN_rfeProbe.fit(X_rfeProbe,Y_Probe)\nKNN_rfeR2L.fit(X_rfeR2L,Y_R2L)\nKNN_rfeU2R.fit(X_rfeU2R,Y_U2R)\n\n","5e47429c":"KNN_DoS.predict(X_DoS_test)","1b56acec":"Y_DoS_pred=KNN_DoS.predict(X_DoS_test)\n# Create confusion matrix\npd.crosstab(Y_DoS_test, Y_DoS_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])","6e16f5e2":"Y_Probe_pred=KNN_Probe.predict(X_Probe_test)\n# Create confusion matrix\npd.crosstab(Y_Probe_test, Y_Probe_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])","29bbf2ef":"Y_R2L_pred=KNN_R2L.predict(X_R2L_test)\n# Create confusion matrix\npd.crosstab(Y_R2L_test, Y_R2L_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])","6f5e3aa0":"Y_U2R_pred=KNN_U2R.predict(X_U2R_test)\n# Create confusion matrix\npd.crosstab(Y_U2R_test, Y_U2R_pred, rownames=['Actual attacks'], colnames=['Predicted attacks'])","1521e39d":"from sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\naccuracy = cross_val_score(KNN_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(KNN_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='precision')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(KNN_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='recall')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(KNN_DoS, X_DoS_test, Y_DoS_test, cv=10, scoring='f1')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","e5bf1b1c":"accuracy = cross_val_score(KNN_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(KNN_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='precision_macro')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(KNN_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='recall_macro')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(KNN_Probe, X_Probe_test, Y_Probe_test, cv=10, scoring='f1_macro')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","e4cb973f":"accuracy = cross_val_score(KNN_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(KNN_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='precision_macro')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(KNN_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='recall_macro')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(KNN_R2L, X_R2L_test, Y_R2L_test, cv=10, scoring='f1_macro')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","8537e85f":"accuracy = cross_val_score(KNN_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(KNN_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='precision_macro')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(KNN_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='recall_macro')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(KNN_U2R, X_U2R_test, Y_U2R_test, cv=10, scoring='f1_macro')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","4a5a0742":"# reduce test dataset to 13 features, use only features described in rfecolname_DoS etc.\nX_DoS_test2=X_DoS_test[:,rfecolindex_DoS]\nX_Probe_test2=X_Probe_test[:,rfecolindex_Probe]\nX_R2L_test2=X_R2L_test[:,rfecolindex_R2L]\nX_U2R_test2=X_U2R_test[:,rfecolindex_U2R]\nX_U2R_test2.shape","515dbb27":"Y_DoS_pred2=KNN_rfeDoS.predict(X_DoS_test2)\n# Create confusion matrix\npd.crosstab(Y_DoS_test, Y_DoS_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])","35ac8b55":"Y_Probe_pred2=KNN_rfeProbe.predict(X_Probe_test2)\n# Create confusion matrix\npd.crosstab(Y_Probe_test, Y_Probe_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])","bcb05ea2":"Y_R2L_pred2=KNN_rfeR2L.predict(X_R2L_test2)\n# Create confusion matrix\npd.crosstab(Y_R2L_test, Y_R2L_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])","3b1a3d72":"Y_U2R_pred2=KNN_rfeU2R.predict(X_U2R_test2)\n# Create confusion matrix\npd.crosstab(Y_U2R_test, Y_U2R_pred2, rownames=['Actual attacks'], colnames=['Predicted attacks'])","713eee90":"accuracy = cross_val_score(KNN_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(KNN_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='precision')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(KNN_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='recall')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(KNN_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='f1')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","38a429bc":"accuracy = cross_val_score(KNN_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(KNN_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='precision_macro')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(KNN_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='recall_macro')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(KNN_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='f1_macro')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","7e772131":"accuracy = cross_val_score(KNN_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(KNN_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='precision_macro')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(KNN_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='recall_macro')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(KNN_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='f1_macro')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","f3edde85":"accuracy = cross_val_score(KNN_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))\nprecision = cross_val_score(KNN_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='precision_macro')\nprint(\"Precision: %0.5f (+\/- %0.5f)\" % (precision.mean(), precision.std() * 2))\nrecall = cross_val_score(KNN_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='recall_macro')\nprint(\"Recall: %0.5f (+\/- %0.5f)\" % (recall.mean(), recall.std() * 2))\nf = cross_val_score(KNN_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='f1_macro')\nprint(\"F-measure: %0.5f (+\/- %0.5f)\" % (f.mean(), f.std() * 2))","049e1e1f":"accuracy = cross_val_score(KNN_rfeDoS, X_DoS_test2, Y_DoS_test, cv=2, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","2b711fec":"accuracy = cross_val_score(KNN_rfeDoS, X_DoS_test2, Y_DoS_test, cv=5, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","76aea996":"accuracy = cross_val_score(KNN_rfeDoS, X_DoS_test2, Y_DoS_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","29b0c8c0":"accuracy = cross_val_score(KNN_rfeDoS, X_DoS_test2, Y_DoS_test, cv=30, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","f2ac3d55":"accuracy = cross_val_score(KNN_rfeDoS, X_DoS_test2, Y_DoS_test, cv=50, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","9dde8536":"accuracy = cross_val_score(KNN_rfeProbe, X_Probe_test2, Y_Probe_test, cv=2, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","bdae7813":"accuracy = cross_val_score(KNN_rfeProbe, X_Probe_test2, Y_Probe_test, cv=5, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","ada9a632":"accuracy = cross_val_score(KNN_rfeProbe, X_Probe_test2, Y_Probe_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","f77af3f7":"accuracy = cross_val_score(KNN_rfeProbe, X_Probe_test2, Y_Probe_test, cv=30, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","1e86d109":"accuracy = cross_val_score(KNN_rfeProbe, X_Probe_test2, Y_Probe_test, cv=50, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","4e011411":"accuracy = cross_val_score(KNN_rfeR2L, X_R2L_test2, Y_R2L_test, cv=2, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","08b864c9":"accuracy = cross_val_score(KNN_rfeR2L, X_R2L_test2, Y_R2L_test, cv=5, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","19121457":"accuracy = cross_val_score(KNN_rfeR2L, X_R2L_test2, Y_R2L_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","736fbc57":"accuracy = cross_val_score(KNN_rfeR2L, X_R2L_test2, Y_R2L_test, cv=30, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","e0963474":"accuracy = cross_val_score(KNN_rfeR2L, X_R2L_test2, Y_R2L_test, cv=50, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","b501abc0":"accuracy = cross_val_score(KNN_rfeU2R, X_U2R_test2, Y_U2R_test, cv=2, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","04d85162":"accuracy = cross_val_score(KNN_rfeU2R, X_U2R_test2, Y_U2R_test, cv=5, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","3e2575ba":"accuracy = cross_val_score(KNN_rfeU2R, X_U2R_test2, Y_U2R_test, cv=10, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","6db2d65f":"accuracy = cross_val_score(KNN_rfeU2R, X_U2R_test2, Y_U2R_test, cv=30, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","fcc1fb77":"accuracy = cross_val_score(KNN_rfeU2R, X_U2R_test2, Y_U2R_test, cv=50, scoring='accuracy')\nprint(\"Accuracy: %0.5f (+\/- %0.5f)\" % (accuracy.mean(), accuracy.std() * 2))","d6dca31f":"* R2L","5cdab505":"* U2R","cef369c4":"* R2L","ee075824":"* U2R","125ce633":"\n# Step 5: Prediction & Evaluation (validation):\n\n* Using all Features for each category\n* Confusion Matrix\n* DOS\n","e9af8936":"* PROBE","0720253e":"# Using 13 Features for each category\n* Confusion Matrix\n* DOS","a81fdd72":"# Cross Validation: Accuracy, Precision, Recall, F-measure\n* DOS","0ef5d028":"* R2L","10b81adb":"* PROBE","71e9015e":"* R2L","2c2d693d":"* U2R","9d52601e":"* U2R","dcc2d768":"Building the model using random forest","362a58e8":"# CV 2, 5, 10, 30, 50 fold\n* DOS","79ddd1a0":"# Cross Validation: Accuracy, Precision, Recall, F-measure\n* DOS","a43db129":"* R2L","7b547687":"* PROBE","cfd85a53":"* U2R","0654e484":"* PROBE","6ecb210c":"* PROBE"}}