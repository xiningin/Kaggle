{"cell_type":{"8593c37b":"code","0bd1f920":"code","74028767":"code","542fa9da":"code","f995fa3c":"code","796414f9":"code","2a5a44e2":"code","74a06b8d":"code","fab6f0fd":"code","f4205332":"code","0ca8d09a":"code","3bc9930c":"code","d20fe278":"code","f9ac65c7":"code","656ff09d":"code","ff0ad8d2":"code","bee71391":"code","1585f95c":"code","f64da1ad":"code","cf39c0b0":"code","6049661c":"code","d9cb81c5":"code","d66293b9":"code","d5ef5e82":"code","5ca7f07b":"code","7d44fc33":"code","94db8d45":"code","1039ba76":"code","8fd54ab9":"code","715382d5":"code","60b144e9":"code","75bc60ee":"code","90512e76":"code","72b2f145":"code","420a3327":"code","f18bd36e":"code","10981977":"code","e1e5250b":"code","b5bb763c":"code","23968ea7":"code","c85e1d86":"code","d92a2ca3":"code","706a1ab6":"code","31be2e75":"code","eba50a29":"code","e68678e1":"code","e6266ab0":"code","2640df5c":"code","476eca62":"code","140f49d5":"code","f62a012e":"code","38029b7a":"code","71739c6b":"code","d828fa33":"code","12ad86d8":"code","d675e2c0":"code","76a58d56":"code","0257e7fd":"code","27d8de59":"code","c2ddc619":"code","1e1ddca0":"code","702c4297":"code","50f23178":"code","30284df2":"code","2353ddc0":"code","c71ba1d3":"code","ade67700":"code","640d1720":"code","4209679e":"code","08ab9eff":"code","a4464b6f":"code","4fdbeae1":"code","02c1f7b9":"code","4d0c6d65":"code","4728efb1":"code","17fb3d96":"code","da00fab2":"code","4fd88d23":"code","e20759da":"code","b5ba3106":"code","87c510a6":"markdown","9b3e0d8a":"markdown","61cc4f47":"markdown","5dcaa464":"markdown","1376c615":"markdown","ca44aa22":"markdown","30a85907":"markdown","0df7aa7c":"markdown","4c82747f":"markdown","3fda737f":"markdown","13eb3a42":"markdown","fdab48a4":"markdown","8df52f49":"markdown","a9e678ef":"markdown","0d0b01ca":"markdown","072b9656":"markdown","6ca6f940":"markdown","11db156b":"markdown","23aa9c6a":"markdown","c65df703":"markdown","735d5163":"markdown","6d9d7cea":"markdown","c876aee6":"markdown","cf5b2c2e":"markdown","1aa6fddb":"markdown","bfc3c5fc":"markdown","19c5738f":"markdown","e5825100":"markdown","2d9079be":"markdown","8c460b56":"markdown","4ea9dc27":"markdown","98dbcc73":"markdown","03a825cb":"markdown","4974e788":"markdown","7e3fe1e4":"markdown","5b49d662":"markdown","1ea68a11":"markdown","16a4dbf3":"markdown","7402196a":"markdown","77f102e5":"markdown","ef8b6971":"markdown","fc3175c9":"markdown","e71dea74":"markdown","1ab13a81":"markdown","7b531950":"markdown","c11a5021":"markdown","6da7012e":"markdown","53019fea":"markdown","f1adb6ad":"markdown","a253c70b":"markdown","96686bcf":"markdown","8e336b15":"markdown","c66358c8":"markdown","c2d2c55f":"markdown","d7e2b412":"markdown","21be55c7":"markdown"},"source":{"8593c37b":"import datetime\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport catboost\nfrom catboost import Pool\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")\npd.set_option('display.float_format', lambda x: '%.2f' % x)\nwarnings.filterwarnings(\"ignore\")","0bd1f920":"test = pd.read_csv('..\/input\/test.csv', dtype={'ID': 'int32', 'shop_id': 'int32', \n                                                  'item_id': 'int32'})\nitem_categories = pd.read_csv('..\/input\/item_categories.csv', \n                              dtype={'item_category_name': 'str', 'item_category_id': 'int32'})\nitems = pd.read_csv('..\/input\/items.csv', dtype={'item_name': 'str', 'item_id': 'int32', \n                                                 'item_category_id': 'int32'})\nshops = pd.read_csv('..\/input\/shops.csv', dtype={'shop_name': 'str', 'shop_id': 'int32'})\nsales = pd.read_csv('..\/input\/sales_train.csv', parse_dates=['date'], \n                    dtype={'date': 'str', 'date_block_num': 'int32', 'shop_id': 'int32', \n                          'item_id': 'int32', 'item_price': 'float32', 'item_cnt_day': 'int32'})","74028767":"train = sales.join(items, on='item_id', rsuffix='_').join(shops, on='shop_id', rsuffix='_').join(item_categories, on='item_category_id', rsuffix='_').drop(['item_id_', 'shop_id_', 'item_category_id_'], axis=1)","542fa9da":"print('Train rows: ', train.shape[0])\nprint('Train columns: ', train.shape[1])","f995fa3c":"train.head().T","796414f9":"train.describe()","2a5a44e2":"print('Min date from train set: %s' % train['date'].min().date())\nprint('Max date from train set: %s' % train['date'].max().date())","74a06b8d":"test_shop_ids = test['shop_id'].unique()\ntest_item_ids = test['item_id'].unique()\n# Only shops that exist in test set.\nlk_train = train[train['shop_id'].isin(test_shop_ids)]\n# Only items that exist in test set.\nlk_train = lk_train[lk_train['item_id'].isin(test_item_ids)]","fab6f0fd":"print('Data set size before leaking:', train.shape[0])\nprint('Data set size after leaking:', lk_train.shape[0])","f4205332":"train = train.query('item_price > 0')","0ca8d09a":"# Select only useful features.\ntrain_monthly = lk_train[['date', 'date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'item_cnt_day']]","3bc9930c":"# Group by month in this case \"date_block_num\" and aggregate features.\ntrain_monthly = train_monthly.sort_values('date').groupby(['date_block_num', 'shop_id', 'item_category_id', 'item_id'], as_index=False)\ntrain_monthly = train_monthly.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})\n# Rename features.\ntrain_monthly.columns = ['date_block_num', 'shop_id', 'item_category_id', 'item_id', 'item_price', 'mean_item_price', 'item_cnt', 'mean_item_cnt', 'transactions']","d20fe278":"# Build a data set with all the possible combinations of ['date_block_num','shop_id','item_id'] so we won't have missing records.\nshop_ids = train_monthly['shop_id'].unique()\nitem_ids = train_monthly['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])","f9ac65c7":"# Merge the train set with the complete set (missing records will be filled with 0).\ntrain_monthly = pd.merge(empty_df, train_monthly, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_monthly.fillna(0, inplace=True)","656ff09d":"train_monthly.head().T","ff0ad8d2":"train_monthly.describe().T","bee71391":"# Extract time based features.\ntrain_monthly['year'] = train_monthly['date_block_num'].apply(lambda x: ((x\/\/12) + 2013))\ntrain_monthly['month'] = train_monthly['date_block_num'].apply(lambda x: (x % 12))","1585f95c":"# Grouping data for EDA.\ngp_month_mean = train_monthly.groupby(['month'], as_index=False)['item_cnt'].mean()\ngp_month_sum = train_monthly.groupby(['month'], as_index=False)['item_cnt'].sum()\ngp_category_mean = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].mean()\ngp_category_sum = train_monthly.groupby(['item_category_id'], as_index=False)['item_cnt'].sum()\ngp_shop_mean = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].mean()\ngp_shop_sum = train_monthly.groupby(['shop_id'], as_index=False)['item_cnt'].sum()","f64da1ad":"f, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\nsns.lineplot(x=\"month\", y=\"item_cnt\", data=gp_month_mean, ax=axes[0]).set_title(\"Monthly mean\")\nsns.lineplot(x=\"month\", y=\"item_cnt\", data=gp_month_sum, ax=axes[1]).set_title(\"Monthly sum\")\nplt.show()","cf39c0b0":"f, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\nsns.barplot(x=\"item_category_id\", y=\"item_cnt\", data=gp_category_mean, ax=axes[0], palette=\"rocket\").set_title(\"Monthly mean\")\nsns.barplot(x=\"item_category_id\", y=\"item_cnt\", data=gp_category_sum, ax=axes[1], palette=\"rocket\").set_title(\"Monthly sum\")\nplt.show()","6049661c":"f, axes = plt.subplots(2, 1, figsize=(22, 10), sharex=True)\nsns.barplot(x=\"shop_id\", y=\"item_cnt\", data=gp_shop_mean, ax=axes[0], palette=\"rocket\").set_title(\"Monthly mean\")\nsns.barplot(x=\"shop_id\", y=\"item_cnt\", data=gp_shop_sum, ax=axes[1], palette=\"rocket\").set_title(\"Monthly sum\")\nplt.show()","d9cb81c5":"sns.jointplot(x=\"item_cnt\", y=\"item_price\", data=train_monthly, height=8)\nplt.show()","d66293b9":"sns.jointplot(x=\"item_cnt\", y=\"transactions\", data=train_monthly, height=8)\nplt.show()","d5ef5e82":"plt.subplots(figsize=(22, 8))\nsns.boxplot(train_monthly['item_cnt'])\nplt.show()","5ca7f07b":"train_monthly = train_monthly.query('item_cnt >= 0 and item_cnt <= 20 and item_price < 400000')","7d44fc33":"train_monthly['item_cnt_month'] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_id'])['item_cnt'].shift(-1)","94db8d45":"train_monthly['item_price_unit'] = train_monthly['item_price'] \/\/ train_monthly['item_cnt']\ntrain_monthly['item_price_unit'].fillna(0, inplace=True)","1039ba76":"gp_item_price = train_monthly.sort_values('date_block_num').groupby(['item_id'], as_index=False).agg({'item_price':[np.min, np.max]})\ngp_item_price.columns = ['item_id', 'hist_min_item_price', 'hist_max_item_price']\n\ntrain_monthly = pd.merge(train_monthly, gp_item_price, on='item_id', how='left')","8fd54ab9":"train_monthly['price_increase'] = train_monthly['item_price'] - train_monthly['hist_min_item_price']\ntrain_monthly['price_decrease'] = train_monthly['hist_max_item_price'] - train_monthly['item_price']","715382d5":"# Min value\nf_min = lambda x: x.rolling(window=3, min_periods=1).min()\n# Max value\nf_max = lambda x: x.rolling(window=3, min_periods=1).max()\n# Mean value\nf_mean = lambda x: x.rolling(window=3, min_periods=1).mean()\n# Standard deviation\nf_std = lambda x: x.rolling(window=3, min_periods=1).std()\n\nfunction_list = [f_min, f_max, f_mean, f_std]\nfunction_name = ['min', 'max', 'mean', 'std']\n\nfor i in range(len(function_list)):\n    train_monthly[('item_cnt_%s' % function_name[i])] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].apply(function_list[i])\n\n# Fill the empty std features with 0\ntrain_monthly['item_cnt_std'].fillna(0, inplace=True)","60b144e9":"lag_list = [1, 2, 3]\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly[ft_name] = train_monthly.sort_values('date_block_num').groupby(['shop_id', 'item_category_id', 'item_id'])['item_cnt'].shift(lag)\n    # Fill the empty shifted features with 0\n    train_monthly[ft_name].fillna(0, inplace=True)","75bc60ee":"train_monthly['item_trend'] = train_monthly['item_cnt']\n\nfor lag in lag_list:\n    ft_name = ('item_cnt_shifted%s' % lag)\n    train_monthly['item_trend'] -= train_monthly[ft_name]\n\ntrain_monthly['item_trend'] \/= len(lag_list) + 1","90512e76":"train_monthly.head().T","72b2f145":"train_monthly.describe().T","420a3327":"train_set = train_monthly.query('date_block_num >= 3 and date_block_num < 28').copy()\nvalidation_set = train_monthly.query('date_block_num >= 28 and date_block_num < 33').copy()\ntest_set = train_monthly.query('date_block_num == 33').copy()\n\ntrain_set.dropna(subset=['item_cnt_month'], inplace=True)\nvalidation_set.dropna(subset=['item_cnt_month'], inplace=True)\n\ntrain_set.dropna(inplace=True)\nvalidation_set.dropna(inplace=True)\n\nprint('Train set records:', train_set.shape[0])\nprint('Validation set records:', validation_set.shape[0])\nprint('Test set records:', test_set.shape[0])\n\nprint('Train set records: %s (%.f%% of complete data)' % (train_set.shape[0], ((train_set.shape[0]\/train_monthly.shape[0])*100)))\nprint('Validation set records: %s (%.f%% of complete data)' % (validation_set.shape[0], ((validation_set.shape[0]\/train_monthly.shape[0])*100)))","f18bd36e":"# Shop mean encoding.\ngp_shop_mean = train_set.groupby(['shop_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_mean.columns = ['shop_mean']\ngp_shop_mean.reset_index(inplace=True)\n# Item mean encoding.\ngp_item_mean = train_set.groupby(['item_id']).agg({'item_cnt_month': ['mean']})\ngp_item_mean.columns = ['item_mean']\ngp_item_mean.reset_index(inplace=True)\n# Shop with item mean encoding.\ngp_shop_item_mean = train_set.groupby(['shop_id', 'item_id']).agg({'item_cnt_month': ['mean']})\ngp_shop_item_mean.columns = ['shop_item_mean']\ngp_shop_item_mean.reset_index(inplace=True)\n# Year mean encoding.\ngp_year_mean = train_set.groupby(['year']).agg({'item_cnt_month': ['mean']})\ngp_year_mean.columns = ['year_mean']\ngp_year_mean.reset_index(inplace=True)\n# Month mean encoding.\ngp_month_mean = train_set.groupby(['month']).agg({'item_cnt_month': ['mean']})\ngp_month_mean.columns = ['month_mean']\ngp_month_mean.reset_index(inplace=True)\n\n# Add meand encoding features to train set.\ntrain_set = pd.merge(train_set, gp_shop_mean, on=['shop_id'], how='left')\ntrain_set = pd.merge(train_set, gp_item_mean, on=['item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\ntrain_set = pd.merge(train_set, gp_year_mean, on=['year'], how='left')\ntrain_set = pd.merge(train_set, gp_month_mean, on=['month'], how='left')\n# Add meand encoding features to validation set.\nvalidation_set = pd.merge(validation_set, gp_shop_mean, on=['shop_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_item_mean, on=['item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_shop_item_mean, on=['shop_id', 'item_id'], how='left')\nvalidation_set = pd.merge(validation_set, gp_year_mean, on=['year'], how='left')\nvalidation_set = pd.merge(validation_set, gp_month_mean, on=['month'], how='left')","10981977":"# Create train and validation sets and labels. \nX_train = train_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_train = train_set['item_cnt_month'].astype(int)\nX_validation = validation_set.drop(['item_cnt_month', 'date_block_num'], axis=1)\nY_validation = validation_set['item_cnt_month'].astype(int)","e1e5250b":"# Integer features (used by catboost model).\nint_features = ['shop_id', 'item_id', 'year', 'month']\n\nX_train[int_features] = X_train[int_features].astype('int32')\nX_validation[int_features] = X_validation[int_features].astype('int32')","b5bb763c":"latest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\nX_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\nX_test['year'] = 2015\nX_test['month'] = 9\nX_test.drop('item_cnt_month', axis=1, inplace=True)\nX_test[int_features] = X_test[int_features].astype('int32')\nX_test = X_test[X_train.columns]","23968ea7":"sets = [X_train, X_validation, X_test]\n\n# This was taking too long.\n# Replace missing values with the median of each item.\n# for dataset in sets:\n#     for item_id in dataset['item_id'].unique():\n#         for column in dataset.columns:\n#             item_median = dataset[(dataset['item_id'] == item_id)][column].median()\n#             dataset.loc[(dataset[column].isnull()) & (dataset['item_id'] == item_id), column] = item_median\n\n# Replace missing values with the median of each shop.            \nfor dataset in sets:\n    for shop_id in dataset['shop_id'].unique():\n        for column in dataset.columns:\n            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median\n            \n# Fill remaining missing values on test set with mean.\nX_test.fillna(X_test.mean(), inplace=True)","c85e1d86":"# I'm dropping \"item_category_id\", we don't have it on test set and would be a little hard to create categories for items that exist only on test set.\nX_train.drop(['item_category_id'], axis=1, inplace=True)\nX_validation.drop(['item_category_id'], axis=1, inplace=True)\nX_test.drop(['item_category_id'], axis=1, inplace=True)","d92a2ca3":"X_test.head().T","706a1ab6":"X_test.describe().T","31be2e75":"cat_features = [0, 1, 7, 8]\n\ncatboost_model = CatBoostRegressor(\n    iterations=500,\n    max_ctr_complexity=4,\n    random_seed=0,\n    od_type='Iter',\n    od_wait=25,\n    verbose=50,\n    depth=4\n)\n\ncatboost_model.fit(\n    X_train, Y_train,\n    cat_features=cat_features,\n    eval_set=(X_validation, Y_validation)\n)","eba50a29":"print('Model params:', catboost_model.get_params())","e68678e1":"feature_score = pd.DataFrame(list(zip(X_train.dtypes.index, catboost_model.get_feature_importance(Pool(X_train, label=Y_train, cat_features=cat_features)))), columns=['Feature','Score'])\nfeature_score = feature_score.sort_values(by='Score', ascending=False, inplace=False, kind='quicksort', na_position='last')\n\nplt.rcParams[\"figure.figsize\"] = (19, 6)\nax = feature_score.plot('Feature', 'Score', kind='bar', color='c')\nax.set_title(\"Catboost Feature Importance Ranking\", fontsize = 14)\nax.set_xlabel('')\nrects = ax.patches\nlabels = feature_score['Score'].round(2)\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 0.35, label, ha='center', va='bottom')\n\nplt.show()","e6266ab0":"catboost_train_pred = catboost_model.predict(X_train)\ncatboost_val_pred = catboost_model.predict(X_validation)\ncatboost_test_pred = catboost_model.predict(X_test)","2640df5c":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, catboost_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, catboost_val_pred)))","476eca62":"def model_performance_sc_plot(predictions, labels, title):\n    # Get min and max values of the predictions and labels.\n    min_val = max(max(predictions), max(labels))\n    max_val = min(min(predictions), min(labels))\n    # Create dataframe with predicitons and labels.\n    performance_df = pd.DataFrame({\"Label\":labels})\n    performance_df[\"Prediction\"] = predictions\n    # Plot data\n    sns.jointplot(y=\"Label\", x=\"Prediction\", data=performance_df, kind=\"reg\", height=7)\n    plt.plot([min_val, max_val], [min_val, max_val], 'm--')\n    plt.title(title, fontsize=9)\n    plt.show()\n    \n# model_performance_sc_plot(catboost_train_pred, Y_train, 'Train')\nmodel_performance_sc_plot(catboost_val_pred, Y_validation, 'Validation')","140f49d5":"# Use only part of features on XGBoost.\nxgb_features = ['item_cnt','item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n                'item_cnt_shifted2', 'item_cnt_shifted3', 'shop_mean', \n                'shop_item_mean', 'item_trend', 'mean_item_cnt']\nxgb_train = X_train[xgb_features]\nxgb_val = X_validation[xgb_features]\nxgb_test = X_test[xgb_features]","f62a012e":"xgb_model = XGBRegressor(max_depth=8, \n                         n_estimators=500, \n                         min_child_weight=1000,  \n                         colsample_bytree=0.7, \n                         subsample=0.7, \n                         eta=0.3, \n                         seed=0)\nxgb_model.fit(xgb_train, \n              Y_train, \n              eval_metric=\"rmse\", \n              eval_set=[(xgb_train, Y_train), (xgb_val, Y_validation)], \n              verbose=20, \n              early_stopping_rounds=20)","38029b7a":"plt.rcParams[\"figure.figsize\"] = (15, 6)\nplot_importance(xgb_model)\nplt.show()","71739c6b":"xgb_train_pred = xgb_model.predict(xgb_train)\nxgb_val_pred = xgb_model.predict(xgb_val)\nxgb_test_pred = xgb_model.predict(xgb_test)","d828fa33":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, xgb_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, xgb_val_pred)))","12ad86d8":"# model_performance_sc_plot(xgb_train_pred, Y_train, 'Train')\nmodel_performance_sc_plot(xgb_val_pred, Y_validation, 'Validation')","d675e2c0":"# Use only part of features on random forest.\nrf_features = ['shop_id', 'item_id', 'item_cnt', 'transactions', 'year',\n               'item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1', \n               'shop_mean', 'item_mean', 'item_trend', 'mean_item_cnt']\nrf_train = X_train[rf_features]\nrf_val = X_validation[rf_features]\nrf_test = X_test[rf_features]","76a58d56":"rf_model = RandomForestRegressor(n_estimators=50, max_depth=7, random_state=0, n_jobs=-1)\nrf_model.fit(rf_train, Y_train)","0257e7fd":"rf_train_pred = rf_model.predict(rf_train)\nrf_val_pred = rf_model.predict(rf_val)\nrf_test_pred = rf_model.predict(rf_test)","27d8de59":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, rf_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, rf_val_pred)))","c2ddc619":"# model_performance_sc_plot(rf_train_pred, Y_train, 'Train')\nmodel_performance_sc_plot(rf_val_pred, Y_validation, 'Validation')","1e1ddca0":"# Use only part of features on linear Regression.\nlr_features = ['item_cnt', 'item_cnt_shifted1', 'item_trend', 'mean_item_cnt', 'shop_mean']\nlr_train = X_train[lr_features]\nlr_val = X_validation[lr_features]\nlr_test = X_test[lr_features]","702c4297":"lr_scaler = MinMaxScaler()\nlr_scaler.fit(lr_train)\nlr_train = lr_scaler.transform(lr_train)\nlr_val = lr_scaler.transform(lr_val)\nlr_test = lr_scaler.transform(lr_test)","50f23178":"lr_model = LinearRegression(n_jobs=-1)\nlr_model.fit(lr_train, Y_train)","30284df2":"lr_train_pred = lr_model.predict(lr_train)\nlr_val_pred = lr_model.predict(lr_val)\nlr_test_pred = lr_model.predict(lr_test)","2353ddc0":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train, lr_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, lr_val_pred)))","c71ba1d3":"# model_performance_sc_plot(lr_train_pred, Y_train, 'Train')\nmodel_performance_sc_plot(lr_val_pred, Y_validation, 'Validation')","ade67700":"# Use only part of features on KNN.\nknn_features = ['item_cnt', 'item_cnt_mean', 'item_cnt_std', 'item_cnt_shifted1',\n                'item_cnt_shifted2', 'shop_mean', 'shop_item_mean', \n                'item_trend', 'mean_item_cnt']\n\n# Subsample train set (using the whole data was taking too long).\nX_train_sampled = X_train[:100000]\nY_train_sampled = Y_train[:100000]\n\nknn_train = X_train_sampled[knn_features]\nknn_val = X_validation[knn_features]\nknn_test = X_test[knn_features]","640d1720":"knn_scaler = MinMaxScaler()\nknn_scaler.fit(knn_train)\nknn_train = knn_scaler.transform(knn_train)\nknn_val = knn_scaler.transform(knn_val)\nknn_test = knn_scaler.transform(knn_test)","4209679e":"knn_model = KNeighborsRegressor(n_neighbors=9, leaf_size=13, n_jobs=-1)\nknn_model.fit(knn_train, Y_train_sampled)","08ab9eff":"knn_train_pred = knn_model.predict(knn_train)\nknn_val_pred = knn_model.predict(knn_val)\nknn_test_pred = knn_model.predict(knn_test)","a4464b6f":"print('Train rmse:', np.sqrt(mean_squared_error(Y_train_sampled, knn_train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(Y_validation, knn_val_pred)))","4fdbeae1":"# model_performance_sc_plot(knn_train_pred, Y_train_sampled, 'Train')\nmodel_performance_sc_plot(knn_val_pred, Y_validation, 'Validation')","02c1f7b9":"# Dataset that will be the train set of the ensemble model.\nfirst_level = pd.DataFrame(catboost_val_pred, columns=['catboost'])\nfirst_level['xgbm'] = xgb_val_pred\nfirst_level['random_forest'] = rf_val_pred\nfirst_level['linear_regression'] = lr_val_pred\nfirst_level['knn'] = knn_val_pred\nfirst_level['label'] = Y_validation.values\nfirst_level.head(20)","4d0c6d65":"# Dataset that will be the test set of the ensemble model.\nfirst_level_test = pd.DataFrame(catboost_test_pred, columns=['catboost'])\nfirst_level_test['xgbm'] = xgb_test_pred\nfirst_level_test['random_forest'] = rf_test_pred\nfirst_level_test['linear_regression'] = lr_test_pred\nfirst_level_test['knn'] = knn_test_pred\nfirst_level_test.head()","4728efb1":"meta_model = LinearRegression(n_jobs=-1)","17fb3d96":"# Drop label from dataset.\nfirst_level.drop('label', axis=1, inplace=True)\nmeta_model.fit(first_level, Y_validation)","da00fab2":"ensemble_pred = meta_model.predict(first_level)\nfinal_predictions = meta_model.predict(first_level_test)","4fd88d23":"print('Train rmse:', np.sqrt(mean_squared_error(ensemble_pred, Y_validation)))","e20759da":"model_performance_sc_plot(ensemble_pred, Y_validation, 'Validation')","b5ba3106":"prediction_df = pd.DataFrame(test['ID'], columns=['ID'])\nprediction_df['item_cnt_month'] = final_predictions.clip(0., 20.)\nprediction_df.to_csv('submission.csv', index=False)\nprediction_df.head(10)","87c510a6":"#### How much each item's price changed from its (lowest\/highest) historical price.","9b3e0d8a":"### XGBoost feature importance","61cc4f47":"### Ensembling\n\n* To combine the 1st level model predictions, I'll use a simple linear regression.\n* As I'm only feeding the model with predictions I don't need a complex model.\n\n#### Ensemble architecture:\n* 1st level:\n    * Catboost\n    * XGBM\n    * Random forest\n    * Linear Regression\n    * KNN\n* 2nd level;\n    * Linear Regression\n    \n#### Here is an  image to help the understanding\n \n <img src=\"https:\/\/raw.githubusercontent.com\/dimitreOliveira\/MachineLearning\/master\/Kaggle\/Predict%20Future%20Sales\/Ensemble%20Kaggle.jpg\" width=\"400\">","5dcaa464":"## Linear models\n\n### Linear Regression","1376c615":"### Mean encoding.\n* done after the train\/validation split.","ca44aa22":"Most of the shops have a similar sell rate, but 3 of them have a much higher rate, this may be a indicative of the shop size.","30a85907":"### Catboost feature importance","0df7aa7c":"### 2nd level model as a linear regression\n* This is the model that will combine the other ones to hopefully make an overall better prediction.\n* If the inputs to this mode were more complex, could be a good idea to split the data into train and validation again, this way you can check if the metal model is overfitting.","4c82747f":"### Join data sets","3fda737f":"### What category sells more?","13eb3a42":"### Modeling the data\n\n## Tree based models\n\n### Catboost","fdab48a4":"### Data preprocessing\n* I'm dropping the text features since I won't be doing anything with them.\n* We are asked to predict total sales for every product and store in the next month, and our data is given by day, so let's remove unwanted columns and aggregate the data by month.","8df52f49":"### Random forest","a9e678ef":"### Replacing missing values.","0d0b01ca":"### Feature \"item_cnt\" distribution.","072b9656":"#### Lag based features.","6ca6f940":"### New dataset","11db156b":"### XGBoost","23aa9c6a":"#### Group based features.","c65df703":"We are asking you to predict total sales for every product and store in the next month. By solving this competition you will be able to apply and enhance your data science skills.\n\nYou are provided with daily historical sales data. The task is to forecast the total amount of products sold in every shop for the test set. Note that the list of shops and products slightly changes every month. Creating a robust model that can handle such situations is part of the challenge.\n\n\n\n### Data fields description:\n* ID - an Id that represents a (Shop, Item) tuple within the test set\n* shop_id - unique identifier of a shop\n* item_id - unique identifier of a product\n* item_category_id - unique identifier of item category\n* date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n* date - date in format dd\/mm\/yyyy\n* item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n* item_price - current price of an item\n* item_name - name of item\n* shop_name - name of shop\n* item_category_name - name of item category\n\n\n### Dependencies","735d5163":"As we can see we have a trending increase of item sales count (mean) towards the ending of the year.","6d9d7cea":"### Let's see how linear regression performed with this \"prediction x label\" plot.","c876aee6":"### Time period of the dataset","cf5b2c2e":"#### Normalizing features","1aa6fddb":"#### Rolling window based features (window = 3 months).","bfc3c5fc":"### How sales behaves along the year?","19c5738f":"### Feature engineering\n\n#### Unitary item prices.","e5825100":"## Clustering models\n\n### KNN Regressor","2d9079be":"#### Let's see how the meta model performed with this \"prediction x label\" plot.","8c460b56":"#### Normalizing features","4ea9dc27":"### Create new datasets with the predictions from first level models.\n* Here I'll be using a simple ensembling technique, I'll use the 1st level models predictions as the input for the 2nd level model, this way the 2nd level model will basically use the 1st level models predictions as features and learn where to give more weight.\n* To use this technique I also need to use the 1st level models and make predictions on the test set, so I can use them on the 2nd level model.\n* I could also pass the complete validation set with extra features (the 1st level models prediction) to the 2nd level model and let it do a little more work on finding the solution.","98dbcc73":"Also only few of the categories seems to hold most of the sell count.","03a825cb":"### Let's take a look at the raw data","4974e788":"#### Item sales count trend.","7e3fe1e4":"<H1><center>Predict future sales<\/center><\/H1>","5b49d662":"### Checking for outliers","1ea68a11":"### Train\/validation split\n* As we know the test set in on the future, so we should try to simulate the same distribution on our train\/validation split.\n* Our train set will be the first 3~28 blocks, validation will be last 5 blocks (29~32) and test will be block 33.\n* I'm leaving the first 3 months out because we use a 3 month window to generate features, so these first 3 month won't have really windowed useful features.","16a4dbf3":"### Data cleaning\n\n    Only records with \"item_price\" > 0.","7402196a":"### What shop sells more?","77f102e5":"### EDA","ef8b6971":"### Build test set\nWe want to predict for \"date_block_num\" 34 so our test set will be block 33 and our predictions should reflect block 34 values. In other words we use block 33 because we want to forecast values for block 34.","fc3175c9":"### Let's see how XGBoosting performed with this \"prediction x label\" plot.","e71dea74":"### Dataset after feature engineering","1ab13a81":"### Data leakages\n\nAbout data leakages I'll only be using only the \"shop_id\" and \"item_id\" that appear on the test set.","7b531950":"### Let's see how random forest performed with this \"prediction x label\" plot.","c11a5021":"#### Ensemble model metrics on validation set.","6da7012e":"#### Removing outliers\nI'll treat \"item_cnt\" > 20 and < 0, \"item_price\" >= 400000 as outliers, so I'll remove them.","53019fea":"### Creating the label\nOur label will be the \"item_cnt\" of the next month, as we are dealing with a forecast problem.","f1adb6ad":"#### Output dataframe.","a253c70b":"#### Trained on validation set using the 1st level models predictions as features.","96686bcf":"### Let's see how knn performed with this \"prediction x label\" plot.","8e336b15":"#### To mimic the real behavior of the data we have to create the missing records from the loaded dataset, so for each month we need to create the missing records for each shop and item, since we don't have data for them I'll replace them with 0.","c66358c8":"### Let's see how catboost performed with this \"prediction x label\" plot.\nThe closer the points are to the middle dashed line the better are the predictions.","c2d2c55f":"### Loading data","d7e2b412":"### Test set","21be55c7":"#### Make predictions on test set using the 1st level models predictions as features."}}