{"cell_type":{"4aa42e3e":"code","31840eba":"code","5ed16f92":"code","c24f640e":"code","526aa180":"code","5627a986":"markdown","6399359c":"markdown"},"source":{"4aa42e3e":"import warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport math\nimport random\nimport json\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn import metrics","31840eba":"# Basic training configurations\n# Number of folds for KFold validation strategy\nFOLDS = 5\n# Number of epochs to train each model\nEPOCHS = 130\n# Batch size\nBATCH_SIZE = 64\n# Learning rate\nLR = 0.001\n# Verbosity\nVERBOSE = 2\n# Seed for deterministic results\nSEED = 123\n\n# Function to seed everything\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(SEED)\n\n# Read training, test and sample submission data\ntrain = pd.read_json('..\/input\/stanford-covid-vaccine\/train.json', lines = True)\ntest = pd.read_json('..\/input\/stanford-covid-vaccine\/test.json', lines = True)\nsample_sub = pd.read_csv('..\/input\/stanford-covid-vaccine\/sample_submission.csv')\n\n\n# Target column list\ntarget_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\n\n# Dictionary comprehension to map the token with an specific id\ntoken2int = {x: i for i, x in enumerate('().ACGUBEHIMSX')}\n\n# Preprocesing function to transform features to 3d format\ndef preprocess_inputs(df, cols = ['sequence', 'structure', 'predicted_loop_type']):\n    return np.transpose(\n        np.array(\n            df[cols]\\\n            .applymap(lambda seq: [token2int[x] for x in seq])\\\n            .values\\\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n\n# Transform training feature sequences to a 3d matrix of (x, 107, 3)\ntrain_inputs = preprocess_inputs(train)\n# Transform training targets sequences to a 3d matrix of (x, 68, 5)\ntrain_labels = np.array(train[target_cols].values.tolist()).transpose(0, 2, 1)\n# Get different test sets\npublic_test_df = test[test['seq_length'] == 107]\nprivate_test_df = test[test['seq_length'] == 130]\n# Preprocess the test sets to the same format as our training data\npublic_test = preprocess_inputs(public_test_df)\nprivate_test = preprocess_inputs(private_test_df)","5ed16f92":"# Custom loss_fnc, extracted from https:\/\/www.kaggle.com\/c\/stanford-covid-vaccine\/discussion\/183211\ndef CMCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\n\n# Function to build our wave net model\ndef build_model(seq_len = 107, pred_len = 68, embed_dim = 85, dropout = 0.10):\n    \n    def wave_block(x, filters, kernel_size, n):\n        dilation_rates = [2 ** i for i in range(n)]\n        x = tf.keras.layers.Conv1D(filters = filters, \n                                   kernel_size = 1,\n                                   padding = 'same')(x)\n        res_x = x\n        for dilation_rate in dilation_rates:\n            tanh_out = tf.keras.layers.Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same', \n                              activation = 'tanh', \n                              dilation_rate = dilation_rate)(x)\n            sigm_out = tf.keras.layers.Conv1D(filters = filters,\n                              kernel_size = kernel_size,\n                              padding = 'same',\n                              activation = 'sigmoid', \n                              dilation_rate = dilation_rate)(x)\n            x = tf.keras.layers.Multiply()([tanh_out, sigm_out])\n            x = tf.keras.layers.Conv1D(filters = filters,\n                       kernel_size = 1,\n                       padding = 'same')(x)\n            res_x = tf.keras.layers.Add()([res_x, x])\n        return res_x\n    \n    inputs = tf.keras.layers.Input(shape = (seq_len, 3))\n    embed = tf.keras.layers.Embedding(input_dim = len(token2int), output_dim = embed_dim)(inputs)\n    reshaped = tf.reshape(embed, shape = (-1, embed.shape[1], embed.shape[2] * embed.shape[3]))\n    reshaped = tf.keras.layers.SpatialDropout1D(dropout)(reshaped)\n    \n    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(128, \n                                                          dropout = dropout, \n                                                          return_sequences = True, \n                                                          kernel_initializer = 'orthogonal'))(reshaped)\n    x = wave_block(x, 16, 3, 12)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(dropout)(x)\n\n    x = wave_block(x, 32, 3, 8)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(dropout)(x)\n\n    x = wave_block(x, 64, 3, 4)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(dropout)(x)\n    \n    x = wave_block(x, 128, 3, 1)\n    x = tf.keras.layers.BatchNormalization()(x)\n    x = tf.keras.layers.Dropout(dropout)(x)\n    \n    \n    truncated = x[:, :pred_len]\n    out = tf.keras.layers.Dense(5, activation = 'linear')(truncated)\n    model = tf.keras.models.Model(inputs = inputs, outputs = out)\n    opt = tf.keras.optimizers.Adam(learning_rate = LR)\n    opt = tfa.optimizers.SWA(opt)\n    model.compile(optimizer = opt,\n                  loss = tf.keras.losses.MeanSquaredLogarithmicError(),\n                  metrics = [tf.keras.metrics.RootMeanSquaredError()])\n    \n    return model\n\n# Evaluation metric for this problem (mean columnwise root mean squared error)\ndef mcrmse(y_true, y_pred):\n    y_true_ = y_true.reshape(163200, 5)\n    y_pred_ = y_pred.reshape(163200, 5)\n    y_true0 = y_true_[:, 0]\n    y_true1 = y_true_[:, 1]\n    y_true2 = y_true_[:, 2]\n    y_true3 = y_true_[:, 3]\n    y_true4 = y_true_[:, 4]\n    y_pred0 = y_pred_[:, 0]\n    y_pred1 = y_pred_[:, 1]\n    y_pred2 = y_pred_[:, 2]\n    y_pred3 = y_pred_[:, 3]\n    y_pred4 = y_pred_[:, 4]\n    rmse0 = math.sqrt(metrics.mean_squared_error(y_true0, y_pred0))\n    rmse1 = math.sqrt(metrics.mean_squared_error(y_true1, y_pred1))\n    rmse2 = math.sqrt(metrics.mean_squared_error(y_true2, y_pred2))\n    rmse3 = math.sqrt(metrics.mean_squared_error(y_true3, y_pred3))\n    rmse4 = math.sqrt(metrics.mean_squared_error(y_true4, y_pred4))\n    return np.mean([rmse0, rmse1, rmse2, rmse3, rmse4])\n\n\ndef train_and_evaluate(train_inputs, train_labels, public_test, private_test):\n        \n    oof_preds = np.zeros((train_inputs.shape[0], 68, 5))\n    public_preds = np.zeros((public_test.shape[0], 107, 5))\n    private_preds = np.zeros((private_test.shape[0], 130, 5))\n\n    kfold = KFold(FOLDS, shuffle = True, random_state = SEED)\n    for fold, (train_index, val_index) in enumerate(kfold.split(train_inputs)):\n        \n        print(f'Training fold {fold + 1}')\n    \n        checkpoint = tf.keras.callbacks.ModelCheckpoint(f'fold_{fold + 1}.h5', \n                                                        monitor = 'val_loss',\n                                                        save_best_only = True,\n                                                        save_weights_only = True\n                                                       )\n        # Using learning rate scheduler\n        cb_lr_schedule = tf.keras.callbacks.ReduceLROnPlateau(monitor = 'val_loss', \n                                                              mode = 'min', \n                                                              factor = 0.5, \n                                                              patience = 5, \n                                                              verbose = 1, \n                                                              min_delta = 0.00001\n                                                             )\n    \n        x_train, x_val = train_inputs[train_index], train_inputs[val_index]\n        y_train, y_val = train_labels[train_index], train_labels[val_index]\n        K.clear_session()\n        # Build a truncated model based on train target lengths\n        model = build_model()\n        history = model.fit(x_train, y_train,\n                            validation_data = (x_val, y_val),\n                            batch_size = BATCH_SIZE,\n                            epochs = EPOCHS,\n                            callbacks = [checkpoint, cb_lr_schedule],\n                            verbose = VERBOSE)\n    \n        # Load best model to predict the validation set\n        model.load_weights(f'fold_{fold + 1}.h5')\n        oof_preds[val_index] = model.predict(x_val)\n\n        # Load best model and predict the entire test sequence for the public test\n        short = build_model(seq_len = 107, pred_len = 107)\n        short.load_weights(f'fold_{fold + 1}.h5')\n        public_preds += short.predict(public_test) \/ FOLDS\n\n        # Load best model and predict the entire test sequence for the private test\n        long = build_model(seq_len = 130, pred_len = 130)\n        long.load_weights(f'fold_{fold + 1}.h5')\n        private_preds += long.predict(private_test) \/ FOLDS\n        \n        print('-'*50)\n        print('\\n')\n    \n    # Calculate out of folds predictions\n    mean_col_rmse = mcrmse(train_labels, oof_preds)\n\n    print(f'Our out of folds mean columnwise root mean squared error is {mean_col_rmse}')\n    \n    return public_preds, private_preds","c24f640e":"public_preds, private_preds = train_and_evaluate(train_inputs, train_labels, public_test, private_test)","526aa180":"# Function to get our predictions in the correct format\ndef inference_format(public_test_df, public_preds, private_test_df, private_preds, target_cols):\n    predictions = []\n    for test, preds in [(public_test_df, public_preds), (private_test_df, private_preds)]:\n        for index, uid in enumerate(test['id']):\n            single_pred = preds[index]\n            single_df = pd.DataFrame(single_pred, columns = target_cols)\n            # Add id\n            single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n            predictions.append(single_df)\n            \n    predictions = pd.concat(predictions)\n    return predictions\n\n# Get our predictions in the correct format\npredictions = inference_format(public_test_df, public_preds, private_test_df, private_preds, target_cols)\n# Sanity check\nsubmission = sample_sub[['id_seqpos']].merge(predictions, on = ['id_seqpos'])\nsubmission.to_csv('submission.csv', index = False)\nprint('Submission saved')\nsubmission.head()","5627a986":"# Comments \n\nThanks to this amazing scripts\n\n* https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model\n* https:\/\/www.kaggle.com\/tuckerarrants\/openvaccine-gru-lstm\n\nThis is just an experiment :)","6399359c":"# Important\nFrom the data description tab, we must predict multiple ground truths in this competition, 5 to be exact. While the submission requires all 5, only 3 are scored: reactivity, deg_Mg_pH10 and deg_Mg_50C\n\nLengths:\n\nTrain and Public Test:\n\n* sequence, structure and predicted_loop_type have a length of 107\n* reactivity, deg_Mg_pH10, deg_pH10, deg_Mg_50C and deg_50C have a length of 68\n\nPrivate Test:\n\n* sequence, structure and predicted_loop_type have a length of 130\n* reactivity, deg_Mg_pH10, deg_pH10, deg_Mg_50C and deg_50C have a length of 91\n\nTo build the public test they use a filter (SN_filter = 1). They did not use this filter for the private so using this filter may overfitt the public leaderboard?\n\n"}}