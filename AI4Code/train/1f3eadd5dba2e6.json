{"cell_type":{"b4b50879":"code","15e70280":"code","615dc0ca":"code","82ee6507":"code","ef23dd41":"code","fb9bc54b":"code","efcfe755":"code","7ea3a74f":"code","4736db8c":"code","426c8359":"code","71661f00":"code","2142c84a":"markdown"},"source":{"b4b50879":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev","15e70280":"%%writefile Model.py\nimport transformers\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass TweetModel(transformers.BertPreTrainedModel):\n    def __init__(self, model_path, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(model_path, config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.l0 = nn.Linear(768 * 2, 2) #762\n\n\n    def forward(self,input_ids, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None):\n        _,_, out = self.roberta(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n\n        out = torch.cat((out[-1], out[-2]), dim=-1)\n        out = self.drop_out(out)\n        logits = self.l0(out)\n\n        start_logits, end_logits = logits.split(1, dim=-1)\n\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n\n        return start_logits, end_logits","615dc0ca":"%%writefile dataset.py\n\nimport torch\n\n\ndef find_start_and_end(tweet, selected_text):\n    len_st = len(selected_text)\n    start = None\n    end = None\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[0]):\n        if tweet[ind: ind+len_st] == selected_text:\n            start = ind\n            end = ind + len_st - 1\n            break\n    return start, end\n\ndef process_with_offsets(args, tweet, selected_text, sentiment, tokenizer):\n\n    start_index, end_index = find_start_and_end(tweet, selected_text)\n\n    char_targets = [0]*len(tweet)\n    if start_index != None and end_index != None:\n        for ct in range(start_index, end_index+1):\n            char_targets[ct] = 1\n    \n    encoded = tokenizer.encode_plus(\n                    sentiment,\n                    tweet,\n                    max_length=args.max_seq_len,\n                    pad_to_max_length=True,\n                    return_token_type_ids=True,\n                    return_offsets_mapping=True\n                )\n\n    target_idx = []\n    for j, (offset1, offset2) in enumerate(encoded[\"offset_mapping\"]):\n        if j > 3:\n            if sum(char_targets[offset1:offset2]) > 0:\n                target_idx.append(j)\n\n    encoded[\"start_position\"] = target_idx[0]\n    encoded[\"end_position\"] = target_idx[-1]\n    encoded[\"tweet\"] = tweet\n    encoded[\"selected_text\"] = selected_text\n    encoded[\"sentiment\"] = sentiment\n\n    return encoded\n\n\nclass TweetDataset:\n    def __init__(self, args, tokenizer, df, mode=\"train\", fold=0):\n        \n        self.mode = mode\n\n        if self.mode == \"train\":\n            df = df[~df.kfold.isin([fold])].dropna()\n            self.tweet = df.text.values\n            self.sentiment = df.sentiment.values\n            self.selected_text = df.selected_text.values\n        \n        elif self.mode == \"valid\":\n            df = df[df.kfold.isin([fold])].dropna()\n            self.tweet = df.text.values\n            self.sentiment = df.sentiment.values\n            self.selected_text = df.selected_text.values\n        \n        self.tokenizer = tokenizer\n        self.args = args\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n\n        tweet = str(self.tweet[item])\n        selected_text = str(self.selected_text[item])\n        sentiment = str(self.sentiment[item])\n        \n        features = process_with_offsets(\n                        args=self.args, \n                        tweet=tweet, \n                        selected_text=selected_text, \n                        sentiment=sentiment, \n                        tokenizer=self.tokenizer\n                    )\n        \n        return {\n            \"input_ids\":torch.tensor(features[\"input_ids\"], dtype=torch.long),\n            \"token_type_ids\":torch.tensor(features[\"token_type_ids\"], dtype=torch.long),\n            \"attention_mask\":torch.tensor(features[\"attention_mask\"], dtype=torch.long),\n            \"start_position\":torch.tensor(features[\"start_position\"],dtype=torch.long),\n            \"end_position\":torch.tensor(features[\"end_position\"], dtype=torch.long),\n\n            \"offsets\":features[\"offset_mapping\"],\n            \"tweet\":features[\"tweet\"],\n            \"selected_text\":features[\"selected_text\"],\n            \"sentiment\":features[\"sentiment\"]\n        }","82ee6507":"%%writefile metric.py\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): \n        return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()\n\ndef calculate_jaccard_score(features_dict, start_logits, end_logits, tokenizer):\n\n    binput_ids = to_list(features_dict[\"input_ids\"])\n    btweet = features_dict[\"tweet\"]\n    bselected_text = features_dict[\"selected_text\"]\n    bsentiment = features_dict[\"sentiment\"]\n    boffsets = features_dict[\"offsets\"]\n\n    bstart_logits = np.argmax(F.softmax(start_logits, dim=1).cpu().data.numpy(), axis=1)\n    bend_logits = np.argmax(F.softmax(end_logits, dim=1).cpu().data.numpy(), axis=1)\n\n    jac_list = []\n\n    for i in range(len(btweet)):\n\n        idx_start = bstart_logits[i]\n        idx_end = bend_logits[i]\n        offsets = boffsets[i]\n        input_ids = binput_ids[i]\n        tweet = btweet[i]\n        selected_text = bselected_text[i]\n\n        if idx_end < idx_start:\n            idx_end = idx_start\n\n        filtered_output = tokenizer.decode(input_ids[idx_start:idx_end+1], skip_special_tokens=True)\n\n        if bsentiment[i] == \"neutral\" or len(tweet.split()) < 2:\n            filtered_output = tweet\n        \n        jac = jaccard(selected_text.strip(), filtered_output.strip())\n\n        jac_list.append(jac)\n\n    return np.mean(jac_list)","ef23dd41":"%%writefile utils.py\n\nimport numpy as np\nimport torch\n\nclass EarlyStopping:\n    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n    def __init__(self, patience=7, verbose=False, delta=0):\n        \"\"\"\n        Args:\n            patience (int): How long to wait after last time validation loss improved.\n                            Default: 7\n            verbose (bool): If True, prints a message for each validation loss improvement. \n                            Default: False\n            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n                            Default: 0\n        \"\"\"\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n        self.delta = delta\n\n    def __call__(self, val_loss, model=None):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            if model:\n                self.save_checkpoint(val_loss, model)\n        elif score < self.best_score + self.delta:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            if model:\n                self.save_checkpoint(val_loss, model)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model):\n        '''Saves model when validation loss decrease.'''\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n        torch.save(model.state_dict(), 'checkpoint.pt')\n        self.val_loss_min = val_loss","fb9bc54b":"%%writefile main_tpu.py\n\nimport argparse\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.data import DataLoader, Dataset\nimport random\nimport re\nimport json\nimport transformers\nfrom transformers import (\n    AdamW,\n    get_linear_schedule_with_warmup\n)\n\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\n\nfrom Model import TweetModel\nfrom dataset import TweetDataset\nfrom metric import calculate_jaccard_score\nimport utils\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()\n\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current values\"\"\"\n    def __init__(self):\n        self.reset()\n    \n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\ndef get_position_accuracy(logits, labels):\n    predictions = np.argmax(F.softmax(logits, dim=1).cpu().data.numpy(), axis=1)\n    labels = labels.cpu().data.numpy()\n    total_num = 0\n    sum_correct = 0\n    for i in range(len(labels)):\n        if labels[i] >= 0:\n            total_num += 1\n            if predictions[i] == labels[i]:\n                sum_correct += 1\n    if total_num == 0:\n        total_num = 1e-7\n    return np.float32(sum_correct) \/ total_num, total_num\n\ndef reduce_fn(vals):\n    return sum(vals) \/ len(vals)\n\ndef loss_fn(preds, labels):\n    start_preds, end_preds = preds\n    start_labels, end_labels = labels\n\n    start_loss = nn.CrossEntropyLoss(ignore_index=-1)(start_preds, start_labels)\n    end_loss = nn.CrossEntropyLoss(ignore_index=-1)(end_preds, end_labels)\n    return start_loss, end_loss\n\n\ndef train(args, train_loader, model, device, optimizer,scheduler, epoch, f):\n    total_loss = AverageMeter()\n    losses1 = AverageMeter() # start\n    losses2 = AverageMeter() # end\n    accuracies1 = AverageMeter() # start\n    accuracies2 = AverageMeter() # end\n\n    model.train()\n\n    t = tqdm(train_loader, disable=not xm.is_master_ordinal())\n    for step, d in enumerate(t):\n        \n        input_ids = d[\"input_ids\"].to(device)\n        attention_mask = d[\"attention_mask\"].to(device)\n        token_type_ids = d[\"token_type_ids\"].to(device)\n        start_position = d[\"start_position\"].to(device)\n        end_position = d[\"end_position\"].to(device)\n\n        model.zero_grad()\n\n        logits1, logits2 = model(\n            input_ids=input_ids, \n            attention_mask=attention_mask, \n            token_type_ids=token_type_ids, \n            position_ids=None, \n            head_mask=None\n        )\n\n        y_true = (start_position, end_position)\n        loss1, loss2 = loss_fn((logits1, logits2), (start_position, end_position))\n        loss = loss1 + loss2\n\n        acc1, n_position1 = get_position_accuracy(logits1, start_position)\n        acc2, n_position2 = get_position_accuracy(logits2, end_position)\n\n        total_loss.update(loss.item(), n_position1)\n        losses1.update(loss1.item(), n_position1)\n        losses2.update(loss2.item(), n_position2)\n        accuracies1.update(acc1, n_position1)\n        accuracies2.update(acc2, n_position2)\n\n        \n        loss.backward()\n        xm.optimizer_step(optimizer)\n        scheduler.step()\n        print_loss = xm.mesh_reduce(\"loss_reduce\", total_loss.avg, reduce_fn)\n        print_acc1 = xm.mesh_reduce(\"acc1_reduce\", accuracies1.avg, reduce_fn)\n        print_acc2 = xm.mesh_reduce(\"acc2_reduce\", accuracies2.avg, reduce_fn)\n        t.set_description(f\"Train E:{epoch+1} - Loss:{print_loss:0.2f} - acc1:{print_acc1:0.2f} - acc2:{print_acc2:0.2f}\")\n\n\n    log_ = f\"Epoch : {epoch+1} - train_loss : {total_loss.avg} - \\n \\\n    train_loss1 : {losses1.avg} - train_loss2 : {losses2.avg} - \\n \\\n    train_acc1 : {accuracies1.avg} - train_acc2 : {accuracies2.avg}\"\n\n    f.write(log_ + \"\\n\\n\")\n    f.flush()\n    \n    return total_loss.avg\n\ndef valid(args, valid_loader, model, device, tokenizer, epoch, f):\n    total_loss = AverageMeter()\n    losses1 = AverageMeter() # start\n    losses2 = AverageMeter() # end\n    accuracies1 = AverageMeter() # start\n    accuracies2 = AverageMeter() # end\n\n    jaccard_scores = AverageMeter()\n\n    model.eval()\n\n    with torch.no_grad():\n        t = tqdm(valid_loader, disable=not xm.is_master_ordinal())\n        for step, d in enumerate(t):\n            \n            input_ids = d[\"input_ids\"].to(device)\n            attention_mask = d[\"attention_mask\"].to(device)\n            token_type_ids = d[\"token_type_ids\"].to(device)\n            start_position = d[\"start_position\"].to(device)\n            end_position = d[\"end_position\"].to(device)\n\n            logits1, logits2 = model(\n                input_ids=input_ids, \n                attention_mask=attention_mask, \n                token_type_ids=token_type_ids, \n                position_ids=None, \n                head_mask=None\n            )\n\n            y_true = (start_position, end_position)\n            loss1, loss2 = loss_fn((logits1, logits2), (start_position, end_position))\n            loss = loss1 + loss2\n\n            acc1, n_position1 = get_position_accuracy(logits1, start_position)\n            acc2, n_position2 = get_position_accuracy(logits2, end_position)\n\n            total_loss.update(loss.item(), n_position1)\n            losses1.update(loss1.item(), n_position1)\n            losses2.update(loss2.item(), n_position2)\n            accuracies1.update(acc1, n_position1)\n            accuracies2.update(acc2, n_position2)\n\n            jac_score = calculate_jaccard_score(features_dict=d, start_logits=logits1, end_logits=logits2, tokenizer=tokenizer)\n\n            jaccard_scores.update(jac_score)\n\n            print_loss = xm.mesh_reduce(\"vloss_reduce\", total_loss.avg, reduce_fn)\n            print_jac = xm.mesh_reduce(\"jac_reduce\", jaccard_scores.avg, reduce_fn)\n\n            t.set_description(f\"Eval E:{epoch+1} - Loss:{print_loss:0.2f} - Jac:{print_jac:0.2f}\")\n\n    #print(\"Valid Jaccard Score : \", jaccard_scores.avg)\n    log_ = f\"Epoch : {epoch+1} - valid_loss : {total_loss.avg} - \\n\\\n    valid_loss1 : {losses1.avg} - \\valid_loss2 : {losses2.avg} - \\n\\\n    valid_acc1 : {accuracies1.avg} - \\valid_acc2 : {accuracies2.avg} \"\n\n    f.write(log_ + \"\\n\\n\")\n    f.flush()\n    \n    return jaccard_scores.avg\n\ndef main():\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"local_rank for distributed training on gpus\")\n    parser.add_argument(\"--max_seq_len\", type=int, default=192)\n    parser.add_argument(\"--fold_index\", type=int, default=0)\n    parser.add_argument(\"--learning_rate\", type=float, default=0.00002)\n    parser.add_argument(\"--epochs\", type=int, default=5)\n    parser.add_argument(\"--batch_size\", type=int, default=16)\n    parser.add_argument(\"--model_path\", type=str, default=\"roberta-base\")\n    parser.add_argument(\"--output_dir\", type=str, default=\"\")\n    parser.add_argument(\"--exp_name\", type=str, default=\"\")\n    parser.add_argument(\"--spt_path\", type=str, default=\"\")\n    parser.add_argument(\"--seed\", type=int, default=42)\n\n    args = parser.parse_args()\n\n    # Setting seed\n    seed = args.seed\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n    model_path = args.model_path\n    config = transformers.RobertaConfig.from_pretrained(model_path)\n    config.output_hidden_states = True\n    tokenizer = transformers.RobertaTokenizerFast.from_pretrained(model_path, do_lower_case=True)\n    \n    MX = TweetModel(model_path, config)\n\n    train_df = pd.read_csv(f\"..\/input\/tweet-create-folds\/train_5folds.csv\")\n\n    args.save_path = os.path.join(args.output_dir, args.exp_name)\n\n    if not os.path.exists(args.save_path):\n        os.makedirs(args.save_path)\n\n    f = open(os.path.join(args.save_path, f\"log_f_{args.fold_index}.txt\"), \"w\")\n\n    num_train_dpoints = int((len(train_df)\/5) * 4)\n\n    def run():\n\n        torch.manual_seed(seed)\n\n        device = xm.xla_device()\n        model = MX.to(device)\n\n        # DataLoaders\n        train_dataset = TweetDataset(\n            args=args,\n            df=train_df,\n            mode=\"train\",\n            fold=args.fold_index,\n            tokenizer=tokenizer\n        )\n        train_sampler = torch.utils.data.distributed.DistributedSampler(\n            train_dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=True\n        )\n        train_loader = torch.utils.data.DataLoader(\n            train_dataset,\n            batch_size=args.batch_size,\n            sampler=train_sampler,\n            drop_last=False,\n            num_workers=2\n        )\n\n        valid_dataset = TweetDataset(\n            args=args,\n            df=train_df,\n            mode=\"valid\",\n            fold=args.fold_index,\n            tokenizer=tokenizer\n        )\n        valid_sampler = torch.utils.data.distributed.DistributedSampler(\n            valid_dataset,\n            num_replicas=xm.xrt_world_size(),\n            rank=xm.get_ordinal(),\n            shuffle=False\n        )\n        valid_loader = DataLoader(\n            valid_dataset,\n            batch_size=args.batch_size,\n            sampler=valid_sampler,\n            num_workers=1,\n            drop_last=False\n        )\n\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\n            \"bias\",\n            \"LayerNorm.bias\",\n            \"LayerNorm.weight\"\n        ]\n        optimizer_parameters = [\n            {\n                'params': [\n                    p for n, p in param_optimizer if not any(\n                        nd in n for nd in no_decay\n                    )\n                ], \n            'weight_decay': 0.001\n            },\n            {\n                'params': [\n                    p for n, p in param_optimizer if any(\n                        nd in n for nd in no_decay\n                    )\n                ], \n                'weight_decay': 0.0\n            },\n        ]\n\n        \n\n        num_train_steps = int(\n            num_train_dpoints \/ args.batch_size \/ xm.xrt_world_size() * args.epochs\n        )\n\n        optimizer = AdamW(\n            optimizer_parameters,\n            lr=args.learning_rate * xm.xrt_world_size()\n        )\n\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=0,\n            num_training_steps=num_train_steps\n        )\n\n        xm.master_print(\"Training is Starting ...... \")\n        best_jac = 0\n        #early_stopping = utils.EarlyStopping(patience=2, mode=\"max\", verbose=True)\n\n        for epoch in range(args.epochs):\n            para_loader = pl.ParallelLoader(train_loader, [device])\n            train_loss = train(\n                args, \n                para_loader.per_device_loader(device),\n                model,\n                device,\n                optimizer,\n                scheduler,\n                epoch,\n                f\n            )\n\n            para_loader = pl.ParallelLoader(valid_loader, [device])\n            valid_jac = valid(\n                args, \n                para_loader.per_device_loader(device),\n                model,\n                device,\n                tokenizer,\n                epoch,\n                f\n            )\n\n            jac = xm.mesh_reduce(\"jac_reduce\", valid_jac, reduce_fn)\n            xm.master_print(f\"**** Epoch {epoch+1} **==>** Jaccard = {jac}\")\n\n            log_ = f\"**** Epoch {epoch+1} **==>** Jaccard = {jac}\"\n\n            f.write(log_ + \"\\n\\n\")\n\n            if jac > best_jac:\n                xm.master_print(\"**** Model Improved !!!! Saving Model\")\n                xm.save(model.state_dict(), os.path.join(args.save_path, f\"fold_{args.fold_index}\"))\n                best_jac = jac\n            \n            #early_stopping(jac)\n            \n            #if early_stopping.early_stop:\n            #    print(\"Early stopping\")\n            #    break\n\n\n    def _mp_fn(rank, flags):\n        torch.set_default_tensor_type('torch.FloatTensor')\n        a = run()\n    \n    FLAGS={}\n    xmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')\n\nif __name__ == \"__main__\":\n    main()","efcfe755":"!python main_tpu.py --fold_index=0 \\\n                  --model_path=\"roberta-base\" \\\n                  --output_dir=\"roberta-base\" \\\n                  --exp_name=\"base_seed42\" \\\n                  --batch_size=64 \\\n                  --learning_rate=2e-5 \\\n                  --seed=42","7ea3a74f":"!python main_tpu.py --fold_index=1 \\\n                  --model_path=\"roberta-base\" \\\n                  --output_dir=\"roberta-base\" \\\n                  --exp_name=\"base_seed42\" \\\n                  --batch_size=64 \\\n                  --learning_rate=2e-5 \\\n                  --seed=42","4736db8c":"!python main_tpu.py --fold_index=2 \\\n                  --model_path=\"roberta-base\" \\\n                  --output_dir=\"roberta-base\" \\\n                  --exp_name=\"base_seed42\" \\\n                  --batch_size=64 \\\n                  --learning_rate=2e-5 \\\n                  --seed=42","426c8359":"!python main_tpu.py --fold_index=3 \\\n                  --model_path=\"roberta-base\" \\\n                  --output_dir=\"roberta-base\" \\\n                  --exp_name=\"base_seed42\" \\\n                  --batch_size=64 \\\n                  --learning_rate=2e-5 \\\n                  --seed=42","71661f00":"!python main_tpu.py --fold_index=4 \\\n                  --model_path=\"roberta-base\" \\\n                  --output_dir=\"roberta-base\" \\\n                  --exp_name=\"base_seed42\" \\\n                  --batch_size=64 \\\n                  --learning_rate=2e-5 \\\n                  --seed=42","2142c84a":"##  If you like it upvote it , Thank you \ud83d\ude42"}}