{"cell_type":{"9d40451e":"code","791398cf":"code","f316b935":"code","971e98a5":"code","c49de737":"code","add5233a":"code","565adec1":"code","e78265f4":"code","9cbd06dc":"code","ca48b200":"code","c9aef28f":"code","991f03ac":"code","67b71983":"code","d47d063e":"code","60f02d1b":"code","3205a444":"code","6d5b9507":"code","116669fc":"code","0b3b272d":"code","846f3bf6":"code","a2e5bde1":"code","a7171aae":"code","872290ad":"code","be737f2d":"code","9da84ba3":"code","06d7703e":"code","987871f2":"code","6c11d1b0":"code","41c5da9d":"code","2b78e63b":"code","46a48dbb":"code","5e7bd020":"code","64550254":"code","0a4f03fd":"code","d1310e3c":"code","135eabeb":"code","ebd871a5":"code","f9e80157":"code","a25ea17c":"code","5a361144":"code","dde60259":"code","45e73c7f":"code","79f5ee42":"code","b3057896":"code","5af28850":"markdown","6d1697fc":"markdown","b1536d49":"markdown","df66a3ee":"markdown","a74abe53":"markdown","1f7fcb16":"markdown","6d0dbaea":"markdown","12189aaf":"markdown","3a54b90d":"markdown","64301771":"markdown","5427eece":"markdown","47dead08":"markdown","87cd39cf":"markdown","ed3600ae":"markdown","077e9277":"markdown","57605ad1":"markdown","efb99b10":"markdown","8b76ce92":"markdown","3742b6a7":"markdown","5bf1c1ec":"markdown","9286dfa7":"markdown","849947e0":"markdown","4aa2e921":"markdown","4d3e379c":"markdown","72a3ec57":"markdown","59b5e47b":"markdown","d1724c5f":"markdown","47eacdf7":"markdown","7661a148":"markdown","4485c9e9":"markdown","52fa78fd":"markdown","1a92764b":"markdown","cd5cb748":"markdown","f50edd98":"markdown","a7baa9b4":"markdown","c54700f4":"markdown","55348fe6":"markdown","f4619f97":"markdown","78daa2cf":"markdown","cbbc54a7":"markdown","97754076":"markdown","7fba0da7":"markdown","1a1f3a8c":"markdown","ac11c029":"markdown","cd741d1c":"markdown"},"source":{"9d40451e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as sch","791398cf":"data = pd.read_csv( '\/kaggle\/input\/crime.csv', encoding='latin-1')\n\ndata.head()","f316b935":"data.shape","971e98a5":"data.describe()","c49de737":"data.info()","add5233a":"for column in data.columns : \n    print('Length of unique data for {0} is {1} '.format(column , len(data[column].unique())))\n    ","565adec1":"data.drop(['INCIDENT_NUMBER' , 'OCCURRED_ON_DATE' ,'STREET' , 'OFFENSE_CODE' , 'REPORTING_AREA','Location'],axis=1, inplace=True)\n\ndata.head()","e78265f4":"for column in data.columns : \n    print('Length of unique data for {0} is {1} '.format(column , len(data[column].unique())))","9cbd06dc":"data['Lat code'] = np.round(data['Lat'],2)\ndata['Long code'] = np.round(data['Long'],2)\n\ndata.drop(['Lat','Long'],axis=1, inplace=True)\n","ca48b200":"data.head()","c9aef28f":"for column in data.columns : \n    print('Length of unique data for {0} is {1} '.format(column , len(data[column].unique())))","991f03ac":"data.info()","67b71983":"data['SHOOTING'].unique()","d47d063e":"data['shooting code'] = np.where(data['SHOOTING']=='Y' , 1 , 0)","60f02d1b":"print( 'Shooting Percentage  is {} %'.format(round((data['shooting code'].sum() \/ data.shape[0]) * 100,2)))\n\n","3205a444":"data.drop(['SHOOTING'],axis=1, inplace=True)\n\n\ndata.info()","6d5b9507":"data['DISTRICT'].unique()","116669fc":"data.DISTRICT.fillna('none', inplace=True)","0b3b272d":"data['DISTRICT'].unique()","846f3bf6":"data['UCR_PART'].unique()","a2e5bde1":"data.UCR_PART.fillna('none', inplace=True)","a7171aae":"data['UCR_PART'].unique()","872290ad":"lat_mean = data['Lat code'].sum() \/ 299074\nlong_mean =  data['Long code'].sum() \/ 299074\nprint(round(lat_mean,2))\nprint(round(long_mean,2))","be737f2d":"data['Lat code'].fillna(round(lat_mean,2), inplace=True)\ndata['Long code'].fillna(round(long_mean,2), inplace=True)","9da84ba3":"data.info()","06d7703e":"data.head()","987871f2":"enc  = LabelEncoder()\nenc.fit(data['OFFENSE_CODE_GROUP'])\ndata['Offense Code'] = enc.transform(data['OFFENSE_CODE_GROUP'])\ndata.drop(['OFFENSE_CODE_GROUP'],axis=1, inplace=True)","6c11d1b0":"data.head()","41c5da9d":"enc  = LabelEncoder()\nenc.fit(data['OFFENSE_DESCRIPTION'])\ndata['Offense Desc Code'] = enc.transform(data['OFFENSE_DESCRIPTION'])\ndata.drop(['OFFENSE_DESCRIPTION'],axis=1, inplace=True)\ndata.head()","2b78e63b":"enc  = LabelEncoder()\nenc.fit(data['DISTRICT'])\ndata['District Code'] = enc.transform(data['DISTRICT'])\ndata.drop(['DISTRICT'],axis=1, inplace=True)\ndata.head()","46a48dbb":"enc  = LabelEncoder()\nenc.fit(data['DAY_OF_WEEK'])\ndata['Day Code'] = enc.transform(data['DAY_OF_WEEK'])\ndata.drop(['DAY_OF_WEEK'],axis=1, inplace=True)\ndata.head()","5e7bd020":"enc  = LabelEncoder()\nenc.fit(data['UCR_PART'])\ndata['UCR Code'] = enc.transform(data['UCR_PART'])\ndata.drop(['UCR_PART'],axis=1, inplace=True)\ndata.head()","64550254":"data.describe()","0a4f03fd":"data.info()","d1310e3c":"X_train = data[:250000]\nX_test = data[250000:]","135eabeb":"print('X Train Shape is {}'.format(X_train.shape))\nprint('X Test Shape is {}'.format(X_test.shape))","ebd871a5":"KMeansModel = KMeans(n_clusters=5,init='k-means++', #also can be random\n                     random_state=33,algorithm= 'auto') # also can be full or elkan\nKMeansModel.fit(X_train)","f9e80157":"print('KMeansModel centers are : ' , KMeansModel.cluster_centers_)\nprint('---------------------------------------------------')\nprint('KMeansModel labels are : ' , KMeansModel.labels_[:20])\nprint('---------------------------------------------------')\nprint('KMeansModel intertia is : ' , KMeansModel.inertia_)\nprint('---------------------------------------------------')\nprint('KMeansModel No. of iteration is : ' , KMeansModel.n_iter_)\nprint('---------------------------------------------------')\n","a25ea17c":"#Calculating Prediction\ny_pred = KMeansModel.predict(X_test)\nprint('Predicted Value for KMeansModel is : ' , y_pred[:10])","5a361144":"NearestNeighborsModel = NearestNeighbors(n_neighbors=4,radius=1.0,algorithm='auto')#it can be:ball_tree,kd_tree,brute\nNearestNeighborsModel.fit(X_train)","dde60259":"#Calculating Details\nprint('NearestNeighborsModel Train kneighbors are : ' , NearestNeighborsModel.kneighbors(X_train[: 5]))\nprint('----------------------------------------------------')\nprint('NearestNeighborsModel Train radius kneighbors are : ' , NearestNeighborsModel.radius_neighbors(X_train[:  1]))\nprint('----------------------------------------------------')\nprint('NearestNeighborsModel Test kneighbors are : ' , NearestNeighborsModel.kneighbors(X_test[: 5]))\nprint('----------------------------------------------------')\nprint('NearestNeighborsModel Test  radius kneighbors are : ' , NearestNeighborsModel.radius_neighbors(X_test[:  1]))\nprint('----------------------------------------------------')","45e73c7f":"AggClusteringModel = AgglomerativeClustering(n_clusters=5,affinity='euclidean',# it can be l1,l2,manhattan,cosine,precomputed\n                                             linkage='ward')# it can be complete,average,single\n\ny_pred_train = AggClusteringModel.fit_predict(X_train[:1000])\ny_pred_test = AggClusteringModel.fit_predict(X_test[:1000])\n","79f5ee42":"#draw the Hierarchical graph for Training set\ndendrogram = sch.dendrogram(sch.linkage(X_train[:30], method = 'ward'))# it can be complete,average,single\nplt.title('Training Set')\nplt.xlabel('X Values')\nplt.ylabel('Distances')\nplt.show()\n\n","b3057896":"#draw the Hierarchical graph for Test set\ndendrogram = sch.dendrogram(sch.linkage(X_test[:30], method = 'ward'))# it can be complete,average,single\nplt.title('Test Set')\nplt.xlabel('X Value')\nplt.ylabel('Distances')\nplt.show()\n\n","5af28850":"lets check it now  ","6d1697fc":"____\n\nthen lets use KNN model for unsupervised training","b1536d49":"then UCR part","df66a3ee":"____\n\n# Data Processing\n\na huge amount of nulls at Shooting feature , also exists in ( District , UCR Part , Street , both Lat & Long )\n\nbefore handling nulls or making categorical , we need to look at unique values , so we can drop features which have a very big amount of unique values , which will never help us in categorical features\n\n","a74abe53":"\n# Loading Data\n\nnow lets read the data file , don't forget to specify the encoding to latin-1 to avoid any error","1f7fcb16":"let's check it again ","6d0dbaea":"ok , a very tiny percentage  , so we'll keep the shooting code feature & drop the original shooting feature , also let's check other features nulls ","12189aaf":"# Unsupervised Clustering\n\nby : Hesham Asem\n____\n\nHere we'll take data about criminal accidents , which happend in Boston between 2015 & 2018 , and we need to classify it in unsupervised way , so we can take each segment to handle it later\n\n\nalso you can find the data here : \n\nhttps:\/\/www.kaggle.com\/AnalyzeBoston\/crimes-in-boston\/kernels\n\n\n____\n\nlets first import needed libraries\n\n","3a54b90d":"\n_____\n\n\nlooks a mixed data which containsboth numerical & categorical values , lets check the shape\n\n","64301771":"____\n\nshooting feature got the biggest amount of Nulls , lets see what it contains ","5427eece":"again , we'll have to put any word since we cannot calculate mean , so lets use none","47dead08":"_____\n\nOk , Incident Number is anyway a reference number which will not help in training. . \n\nand the date will not help us so much since we'll keep the data of ( year , month & weekday ). \n\nalso street feature have a huge amount of unique values so we can drop it since we'll keep long & lat , and we'll drop reporting area and ocation for the same reason . \n\nalso we can drop offense code since we'llkeep offense code group to avoid make 222 values at get_dummies .\n\nlets drop unhelpful features . . \n\n\n","87cd39cf":"looks great , letl's repeat it to Offense Description","ed3600ae":"how about the range of numbers of numerical values . . ","077e9277":"now we can draw the dendogram using Scipy , for the first 30 record of training set","57605ad1":"cool , no nulls there , lets have a look to the features","efb99b10":"now it looks more simple & smart , lets see the amount of unique data","8b76ce92":"____\n\nalso we can use Hierarchical clusering , it migh be useful , but we've to limit the using data to a tiny amount , lets say we'll check it in the first 1000 sample size","3742b6a7":"___\n\n# Run the Model\n\n\nso we are ready to run the unsupervised model for it now \n\n\nfirst let's split the data to train & test","5bf1c1ec":"____\ncool , 20 unique Lat & 22 unique Long , which will refer to 440 zones in Boston city , which is very helpful in making clusters \n\n\n___\n\n# Handling Nulls\n\nnow we have to manage all missing data in other features , let's see it now \n","9286dfa7":"____\n\nnow we need to have a look to its attributes","849947e0":"\nok , it either Y for yes there was shooting in the crime , or nan for the crime contain not shooting , ok , let's make a new feature , which will got 1 for shooting & 0 for no shooting","4aa2e921":"and make the last check or nulls","4d3e379c":"____\n\n# Get Dummies\n\nok , before we run the clustering model , we've to convert all categorical values into numerical dummies . \n\nwe have here five categorical features which are : \n\nOffense Groud\n\nOffense Description\n\nDistrict\n\nDay of week\n\nUCR Part\n\n\n\nso we'll use LabelEncoder model from Sklearn to do it quickly , then drop the original feature . \n\n\nlets start with Offense Group . .\n\n\n","72a3ec57":"___\n\n\n# Finally\n\nas we saw now , data processing is the most important step to manipulate data & make it ready for our model \n\n","59b5e47b":"how about the shape ? ","d1724c5f":"____\n\nnow for Lat & Long , first lets get the mean values for them & round it ","47eacdf7":"___\n\nmore than 1700 nulls in the district feature , lets check the unique values","7661a148":"then District","4485c9e9":"now we filled all nulls & it's ready for categorical values , lets have last check it data ","52fa78fd":"now lets check the data","1a92764b":"& day of week","cd5cb748":"how about predicting from x_test","f50edd98":"_____\n\nwhen you look carfeully at the numbers , you can see the Offense Code is not a \"numerical value\" , but looks categorical \n\nalso min year is 2015 & max is 2016\n\nalso for Lat & Long values , almost all of them around (42 , -71) , which refer to Boston city . . \n\nhow about the Nulls ? \n","a7baa9b4":"and we can check it in the first 30 record in the test set","c54700f4":"also we can know the percentage of shooting among all accidents happend here ","55348fe6":"then check the attributes","f4619f97":"so we can fill null values in Lat & Long with that mean ","78daa2cf":"cool , no nulls now , lets move to UCR Part\n","cbbc54a7":"____\n\n# Handling Location\n\nnow for Lat & Long data , it looks it got a huge amount of unique data , so we can never convert it to categorical data in this way , and also we cannot get rid of it , since it's very important imformation for clustering the data . . \n\nso we can round the Lat & Long data to two decimal numbers , so it will - geographically - classify the whole city into specific zones , & will reduce the amount of unique data for them\n","97754076":"ok done , let's have a look","7fba0da7":"since we cannot replace nulls with mean or median for categorical values , so we'll just put the word none instead of nulls , using fillna tool","1a1f3a8c":"check it now","ac11c029":"ok , lets have a look to the data ","cd741d1c":"____\n\nso we'll use 3 models to choose the best one\n\nlets start with Kmeans , from Sklearn"}}