{"cell_type":{"70943c4c":"code","f751cccb":"code","2916e306":"code","477ade7c":"code","5be1c6cc":"code","97a37028":"code","609d7c6d":"code","c41cd3a8":"code","6bff2ac3":"code","68ad4225":"code","4f2b641f":"code","a1a269b8":"code","066d2961":"code","8c4f467b":"code","cc834cb1":"code","5c8575e6":"code","03042119":"code","d418470b":"code","63608d6a":"code","539678ad":"code","169aacc8":"code","e81ce154":"code","1f5fb2af":"code","28f4072e":"code","3de50950":"code","83b716bc":"code","07df109f":"code","4b1abf66":"code","3632a18a":"code","4191c4c2":"code","90e3f2b7":"code","8da4b317":"code","06b2001b":"code","b481be57":"code","1e2da043":"code","07f83063":"code","743a66d0":"code","2f4abb1f":"code","c465206b":"code","4606583a":"code","1220a7d7":"code","edffea32":"code","061c80ed":"code","80535cd3":"code","262e04ac":"code","29d94192":"code","77e4cbfb":"code","2ff22fb4":"code","c8bb85cd":"code","14acf513":"code","03866c4e":"code","71e50d0c":"code","6fa8869d":"code","6bf555a5":"markdown","57a6da4c":"markdown","b5150db6":"markdown","dde2b0a2":"markdown","0bbd2ca7":"markdown","53d441e0":"markdown","4fb8be66":"markdown","0e78bd06":"markdown","91a82524":"markdown","310a0d0e":"markdown","ed87c844":"markdown","18b6655f":"markdown","c179f0d3":"markdown","12dcb3f1":"markdown","6f07d427":"markdown","085a8d87":"markdown","fa25b127":"markdown"},"source":{"70943c4c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Displaying Full Data Frame\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', -1)\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all types of warnings\nsimplefilter(action='ignore', category=UserWarning) \nsimplefilter(action='ignore', category=FutureWarning) \n\n# importing operating system\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f751cccb":"# importing more libraries\nimport json # library to handle JSON files\n\n#!conda install -c conda-forge geopy --yes # uncomment this line if you haven't completed the Foursquare API lab\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n\nimport requests # library to handle requests\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n\n# Matplotlib and associated plotting modules\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n\n# import k-means from clustering stage\n#from sklearn.cluster import KMeans\n\n#!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't completed the Foursquare API lab\nimport folium # map rendering library\n\nprint('Libraries imported.')","2916e306":"# Importing pickle library to read pickle file\nimport pickle","477ade7c":"df=pd.read_pickle(\"..\/input\/time_converted_df.pickle\")\n#df.head()","5be1c6cc":"df.head()","97a37028":"df.shape","609d7c6d":"df['LAT']=df[['LAT']].astype(float)\ndf['LON']=df[['LON']].astype(float)","c41cd3a8":"# Dropping empty files \ndf=df.dropna(subset=['LAT', 'LON','CITY'])\ndf.isnull().sum()","6bff2ac3":"# Texas Coordinates\nlatitude = 31.8160381\nlongitude = -99.5120986","68ad4225":"# create map of Texas using latitude and longitude values\nmap_texas = folium.Map(location=[latitude,longitude], zoom_start=6,tiles='Map Box Control Room')\nmap_texas","4f2b641f":"df['EVENT_TYPE'].unique().tolist()","a1a269b8":"df_accident=df[df['EVENT_TYPE']=='accident'].reset_index(drop=True)\nprint (df_accident.shape)\ndf_accident.head(2)","066d2961":"df_without_accident=df[df['EVENT_TYPE']!='accident'].reset_index(drop=True)\nprint (df_without_accident.shape)\ndf_without_accident.head(2)","8c4f467b":"df_accident1=df_accident.head(1000)\ndf_without_accident1=df_without_accident.head(1000)","cc834cb1":"# create map of Downtown Toronto using latitude and longitude values\nmap_texas = folium.Map(location=[latitude,longitude], zoom_start=6, tiles='Map Box Control Room')\n\n# instantiate a feature group for the incidents in the dataframe\nincidents_accident = folium.map.FeatureGroup()\nlatitudes = list(df_accident1.LAT)\nlongitudes = list(df_accident1.LON)\nlabels = list(df_accident1.EVENT_TYPE)\n\nfor lat, lng, label in zip(latitudes, longitudes, labels):\n    folium.CircleMarker([lat, lng], popup=label).add_to(map_texas)    \n    \n# add incidents to map\nmap_texas.add_child(incidents_accident)\nmap_texas","5c8575e6":"# create map of Downtown Toronto using latitude and longitude values\nmap_texas = folium.Map(location=[latitude,longitude], zoom_start=6, tiles='Map Box Control Room')#, tiles='Stamen Terrain')\n\n# instantiate a feature group for the incidents in the dataframe\nincidents_withoutaccidents = folium.map.FeatureGroup()\nlatitudes = list(df_without_accident1.LAT)\nlongitudes = list(df_without_accident1.LON)\nlabels = list(df_without_accident1.EVENT_TYPE)\n\nfor lat, lng, label in zip(latitudes, longitudes, labels):\n    folium.CircleMarker([lat, lng], popup=label).add_to(map_texas)    \n    \n# add incidents to map\nmap_texas.add_child(incidents_withoutaccidents)\nmap_texas","03042119":"# Creating a cloumn featuring binary values on the basis of accident risks\ndf['EVENT'] =[1 if \"accident\" in x  else 0 for x in df['EVENT_TYPE']]\ndf.head(2)","d418470b":"df1=df[['EVENT_TYPE','LAT','LON', 'EVENT']]","63608d6a":"df1.dtypes","539678ad":"df1= df1.head(1000)","169aacc8":"# Create dict for 'EVENT' binary values, so that we can view two categories\ncolordict = {1: 'red', 0: 'yellow'}","e81ce154":"# create map of dallas using latitude and longitude values\nmap_dallas = folium.Map(location=[32.791163,-96.749703], zoom_start=10, tiles='openstreetmap')\nincidents = folium.map.FeatureGroup()\nfor lat, lon, traffic_q, label,  in zip(df1['LAT'], df1['LON'], df1['EVENT'], df1['EVENT_TYPE']):\n    folium.CircleMarker(\n        [lat, lon],\n        radius=5,\n        popup = (label ),\n        color='r',\n        key_on = traffic_q,\n        threshold_scale=[0,1],\n        fill_color=colordict[traffic_q],\n        fill=True,\n        fill_opacity=0.7\n        ).add_to(map_dallas)\n\nmap_dallas.add_child(incidents)\nmap_dallas","1f5fb2af":"df_sample=df[['EVENT_TYPE', 'LAT']]\ndf_sample.head(2)","28f4072e":"df_sample.set_index(['LAT'],inplace=True)\ndf_sample.head(2)","3de50950":"event_result=df_sample.groupby(level=['LAT'], sort=False).agg(','.join)","83b716bc":"event_result.head(2)","07df109f":"event_result=event_result.reset_index()\nevent_result.head()","4b1abf66":"event_result.shape","3632a18a":"event_result['EVENT'] =[1 if \"accident\" in x  else 0 for x in event_result['EVENT_TYPE']]","4191c4c2":"event_result.head(2)","90e3f2b7":"df2_sample=df[['EVENT_TYPE', 'LON']]\n#df2_sample.head(2)\ndf2_sample.set_index(['LON'],inplace=True)\nevent_result2=df2_sample.groupby(level=['LON'], sort=False).agg(','.join)\nevent_result2.head(2)\nevent_result2=event_result2.reset_index()\nevent_result2.head(2)","8da4b317":"event_result2.shape","06b2001b":"event_result.rename(columns={'EVENT_TYPE':'EVENT TYPE','LAT':'lat', 'EVENT':'ACCIDENT_RISK'},inplace=True)\nframe1=[df,event_result]\nframes_main=pd.concat(frame1, axis=1, sort=False)\nframes_main.head(2)","b481be57":"#droping\nframes_main.drop(['lat', 'EVENT_TYPE'], axis=1, inplace=True)\n#frames_main.head(2)","1e2da043":"main=frames_main[['EVENT TYPE', 'ACCIDENT_RISK']]\nmain.head(2)","07f83063":"main.rename(columns={'EVENT TYPE':'EVENT_TYPE'},inplace=True)\nmain.head(1)","743a66d0":"# missing data\nmain.isnull().sum()","2f4abb1f":"main=main.dropna(subset=['EVENT_TYPE', 'ACCIDENT_RISK'])\nmain.isnull().sum()","c465206b":"#!pip install --upgrade pixiedust\n#import pixiedust","4606583a":"main['EVENT_TYPE'] = main['EVENT_TYPE'].astype('category')\nmain['EVENT_TYPE'] = main['EVENT_TYPE'].cat.codes","1220a7d7":"main.head(2)","edffea32":"main['EVENT_TYPE']=main['EVENT_TYPE'].astype(int)","061c80ed":"# Normalize 'EVENT_TYPE' feature for good view\nx=main['EVENT_TYPE']\/max(main['EVENT_TYPE'])\n#print (x)","80535cd3":"# Ploting two different graphs for visualization aganist the same data frame\nimport seaborn as sns; sns.set()\nax = sns.scatterplot(x=x, y=\"ACCIDENT_RISK\",  hue=\"ACCIDENT_RISK\", data=main)\nax.set_title(\"EVENTS VS RISKS\")\n","262e04ac":"# we can also see residual plot\nax=sns.residplot(x=x, y='ACCIDENT_RISK', data=main)\nax.set_title(\"EVENTS VS RISKS\")","29d94192":"sns.residplot(x='EVENT_TYPE', y='ACCIDENT_RISK', data=main)","77e4cbfb":"# split into input and output variables\nX = main['EVENT_TYPE'].values\nY = main['ACCIDENT_RISK'].values","2ff22fb4":"print (X)\nprint (Y)","c8bb85cd":"# Normalize\nX=X\/max(X)","14acf513":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import adam,sgd\nfrom sklearn.model_selection import train_test_split","03866c4e":"# seed for reproducing same results\nseed = 20\nnp.random.seed(seed)\n\n# split the data into training (80%) and testing (20%)\n(X_train, X_test, Y_train, Y_test) = train_test_split(X, Y, test_size=0.20, random_state=seed)","71e50d0c":"# create the model\nmodel = Sequential()\nmodel.add(Dense(1, input_dim=1, init='uniform', activation='relu'))\nmodel.add(Dense(1, init='uniform', activation='relu'))\nmodel.add(Dense(1, init='uniform', activation='sigmoid'))\n\n# compile the model\nmodel.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['acc'])\n\n# fit the model\nhistory=model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=50, batch_size=512)# verbose=1)","6fa8869d":"import matplotlib.pyplot as plt\n# list all data in history\nprint(history.history.keys())\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","6bf555a5":"# Conclusion\n\n### The accuracy of the model is above 90% percent. It means the model fits well. We can concentrate at those common places which are sharing the common coordinates. It will help us to minimize the accident risks.","57a6da4c":"## Now have created new features, which we call feature engoneering. We will take only two features for rest of the work that is 'EVENT TYPE' & 'ACCIDENT_RISK'","b5150db6":"# Observation & Recommendation\n\n### We observed that there are many places where multiple incidents happened. It means there are chances of accident risk is involved. Like traffic jam in hazard areas, where plenty of chances of a minor accident can happen there.\n\n### We didn't consider the time where an incident has happened. The time factor may also be a good feature to analyze further. It could give us a better probability. \n\n\n","dde2b0a2":"## Comparing the size of two new dataframes based on Latitude and Longitude, its obvious that Longitudes are common, thats why the size of that dataframe is large. We will only persue data for latitude and then merges with longitudes of real data. The latitude dataframe will be enough to predict the accident risks.","0bbd2ca7":"### Now creating a column 'EVENT' om the basis of accident chances in the 'EVENT_TYPE' Feature","53d441e0":"# Preprocessing\n\n### We will explore the pickle file first.\n\n### Then we create two more data frames, one for those places where accidents happened. The other for non-accident incidents.","4fb8be66":"#opt = tf.keras.optimizers.Adam(learning_rate=0.1)\nadam = Adam(lr=0.002,amsgrad=True)\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(129,)),\n    keras.layers.Dense(80, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(.1, input_shape=(129,)),\n    keras.layers.Dense(64, activation=tf.nn.relu),\n    tf.keras.layers.Dropout(.1, input_shape=(129,)),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n])\n#model.add(keras.layers.Dropout(0.5))\n# compile the model\nmodel.compile(loss='binary_crossentropy', optimizer=adam, metrics=['acc'])\n\n# fit the model\nhistory=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=256,verbose=1)","0e78bd06":"### Now for Longitude","91a82524":"### Merging all the events aganist each latitude","310a0d0e":"# Freature Engineering\n\n###  It is cleared now after visualizing the two data frames that the locations are common in incidents. It means we have to find each coordinate against the incidents\n\n### Creating a new data frame consisting of events and coordinates\n\n### We will create separate data frames for both latitude and longitude because latitude may be unique but longitudes may be common. Let see","ed87c844":"### Now we want to see the two categories, 'accident' and 'without accident'","18b6655f":"import tensorflow as tf\nfrom tensorflow import keras\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam,SGD,RMSprop\nfrom sklearn.model_selection import train_test_split","c179f0d3":"# Visualization\n\n### The purpose of creating more data frames is to see the visualization of common places\n\n### We will examine the first 1000 rows of each data frame to see the resemblance. Whether the presence of other incidents on the accident location is there.\n\n### We have to split 'EVENT_TYPE' features into two categories accident places and other places. The purpose is to see the common places, which will help us to further analyze the accidents risks.\n\n### We will create four maps, showing Texas, accident locations,non-accident locations and finally both locations on the same map in the fourth map. \n\n### To distinguivish the two incidents, we will use coloring. Red color will show non-accident places where yellow for accident places.","12dcb3f1":"# Model Designning & Evaluation\n\n\n### Converting the 'EVENT_TYPE' feature into a category, as it has text information.\n\n### We will change text information into a numeric one. Therefore we can visualize the respective data too\n\n### Normalize the data for training and evaluation the data\n\n### 80% data for training and 20% for testing through train-test split(The sci-kit learn package)\n\n### We then apply Deep Learning Module, Neural Network for fitting and evaluate the data","6f07d427":"## Final PLOT","085a8d87":"# Context\n\n### We have a pickle file containing Texas counties information related to road incidents like a traffic jam, accidents, flooding, road construction, etc. There are thirty-two different incidents of information on the data set with time, place, coordinates and many more. The overall data size is more than thirty million observations ","fa25b127":"# Objective\n\n### We have to analyze the incidents and predict the accident risk involve at those locations where the accident and non-accidents incident were happened together at different times on the same location in the past."}}