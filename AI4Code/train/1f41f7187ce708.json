{"cell_type":{"3d4b606a":"code","fda3b63d":"code","084db2b5":"code","01ba9832":"code","116f7c14":"code","cc148342":"code","74960661":"code","f31b0836":"code","de329a09":"code","7fb9ebab":"code","77e8d4bf":"code","ad96ce9b":"code","d2d2fd50":"code","a569a004":"code","22fa3ced":"code","d7791d3c":"code","364cf797":"code","3047c16c":"code","8e856de1":"code","b553eb54":"code","01c1700e":"code","9a359421":"code","677dfcb9":"code","8321fe54":"code","fb7af81d":"code","a5f0fef9":"code","5e79b33d":"code","77f186e7":"code","886b4637":"code","a6186ef8":"code","6f6748da":"code","0420cbdf":"code","db98e90b":"code","92b42e0b":"code","ba4c0a53":"markdown","64551b38":"markdown","57e9203a":"markdown","696e0890":"markdown","8723543e":"markdown","2ec4ee16":"markdown","3fc4474d":"markdown","c3d35fe2":"markdown","4efd02fa":"markdown","658fc2c3":"markdown","373fc0ab":"markdown","bd2a77de":"markdown"},"source":{"3d4b606a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport random\nimport warnings\nfrom sklearn.metrics import confusion_matrix\nwarnings.filterwarnings('ignore')\n\n#File Operation libraries\nimport glob\nfrom pathlib import Path\n\n#Visualisation Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom mlxtend.plotting import plot_confusion_matrix\n\n#Keras\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, AvgPool2D, Input, Dropout, Flatten, BatchNormalization\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.applications.vgg19 import VGG19\n\n#Image Transformation Libraries\nimport cv2\nfrom imgaug import augmenters as iaa\n\n# Any results you write to the current directory are saved as output.","fda3b63d":"print(os.listdir(\"..\/input\/chest_xray\/chest_xray\/\"))","084db2b5":"base_dir = \"..\/input\/chest_xray\/chest_xray\/\"\ntrain_dir = base_dir+'train\/'\ntest_dir = base_dir+'test\/'\nval_dir = base_dir+'val\/'","01ba9832":"def get_df(path):\n    lst = []\n    normal_dir = Path(path + \"NORMAL\")\n    pneumonia_dir = Path(path + \"PNEUMONIA\")\n    normal_data = normal_dir.glob(\"*.jpeg\")\n    pneumonia_data = pneumonia_dir.glob(\"*.jpeg\")\n    for fname in normal_data:\n        lst.append((fname, 0))\n    for fname in pneumonia_data:\n        lst.append((fname, 1))\n    df = pd.DataFrame(lst, columns=['Image', 'Label'], index=None)\n    s = np.arange(df.shape[0])\n    np.random.shuffle(s)\n    df = df.iloc[s,:].reset_index(drop=True)\n    return df","116f7c14":"df_train = get_df(train_dir)\ndf_val = get_df(val_dir)\ndf_test = get_df(test_dir)","cc148342":"df_train.shape, df_val.shape, df_test.shape","74960661":"df_train['Label'].value_counts()","f31b0836":"df_val['Label'].value_counts()","de329a09":"df_test['Label'].value_counts()","7fb9ebab":"fig, ax = plt.subplots(figsize=(8, 8))\nsns.countplot(df_train['Label'])\nax.set_title('Distribution of Images', fontsize=14)\nax.set_xlabel('Label', fontsize=12)\nax.set_ylabel('Count', fontsize=12)\nplt.show()","77e8d4bf":"def transform_image(img_list):\n    img = cv2.resize(img_list, (224, 224))\n    #cv2 reads image in BGR format. Let's convert to RGB\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return img   \n\ndef augment_image(img_list):\n    seq = iaa.OneOf([\n        iaa.Affine(\n            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n            rotate=(-25, 25)\n        ),\n        iaa.Fliplr(),\n        iaa.Multiply((1.2, 1.5))\n    ])\n    return seq.augment_image(img_list)\n\ndef transform_augment_batch(img_path_list, label_list, is_augment=False):\n    img_list = []\n    for i in range(len(img_path_list)):\n        img_list.append(transform_image(cv2.imread(str(img_path_list[i]))))\n    n = len(img_list)\n    if is_augment:\n        for i in range(n):\n            img = img_list[i]\n            img = augment_image(img)\n            img_list.append(img)\n        img_list = np.array(img_list)\n        label_list = np.append(label_list, label_list)\n    return img_list, label_list","ad96ce9b":"def generate_batch_images(df, batch_size):\n    s = np.arange(df.shape[0])\n    np.random.shuffle(s)\n    X_dev = np.array(df_train.iloc[s, 0])\n    Y_dev = np.array(df_train.iloc[s, 1])\n    start_index = 0\n    while start_index < len(X_dev):\n        if start_index+batch_size <= len(X_dev):\n            end_index = start_index+batch_size\n        else:\n            end_index = len(X_dev)\n        #Select image paths in batches\n        x_dev = X_dev[start_index:end_index]\n        y_dev = Y_dev[start_index:end_index]\n        \n        #Transform images and augment\n        x_dev, y_dev = transform_augment_batch(x_dev, y_dev, True)\n        y_dev = y_dev.reshape((len(y_dev), 1))\n        \n        #Normalize\n        x_dev = x_dev \/ 255.0\n        yield x_dev, y_dev","d2d2fd50":"fig, ax = plt.subplots(figsize=(12, 12))\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    img = transform_image(cv2.imread(str(df_train.iloc[i, 0])))\n    plt.imshow(img)\n    if df_train.iloc[i, 1] == 0:\n        plt.title('Normal')\n    else:\n        plt.title('Pneumonia')\n    plt.xticks([])\n    plt.yticks([])\nplt.show()","a569a004":"plt.subplots(figsize=(12, 12))\nfor i in range(9):\n    plt.subplot(3, 3, i+1)\n    img = transform_image(cv2.imread(str(df_train.iloc[i, 0])))\n    img = augment_image(img)\n    plt.imshow(img)\n    if df_train.iloc[i, 1] == 0:\n        plt.title('Normal')\n    else:\n        plt.title('Pneumonia')\n    plt.xticks([])\n    plt.yticks([])\nplt.show()","22fa3ced":"val_labels = np.array(df_val.iloc[:, 1]).reshape((df_val.shape[0], 1))\nval_images, _ = transform_augment_batch(df_val.iloc[:, 0], df_val.iloc[:, 1], False)\nval_images = np.array(val_images)\nval_images = val_images \/ 255.0","d7791d3c":"val_images.shape, val_labels.shape","364cf797":"test_labels = np.array(df_test.iloc[:, 1]).reshape((df_test.shape[0], 1))\ntest_images, _ = transform_augment_batch(df_test.iloc[:, 0], df_test.iloc[:, 1], False)\ntest_images = np.array(test_images)\ntest_images = test_images \/ 255.0","3047c16c":"test_images.shape, test_labels.shape","8e856de1":"# Let's start with the hyperparameters\nbase_learning_rate = 1e-5\nbatch_size=32\nepochs = 8","b553eb54":"def create_model():\n    img_input = Input(shape=(224, 224, 3))\n\n    # Block 1\n    x = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu', name='block1_conv1', trainable=False)(img_input)\n    x = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu', name='block1_conv2', trainable=False)(x)\n    x = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid', name='block1_pool', trainable=False)(x)\n\n    #Block 2\n    x = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', name='block2_conv1',trainable=False)(x)\n    x = Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', name='block2_conv2', trainable=False)(x)\n    x = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid', name='block2_pool', trainable=False)(x)\n\n    #Block 3\n    x = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu', name='block3_conv1',trainable=False)(x)\n    x = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu', name='block3_conv2', trainable=False)(x)\n    x = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu', name='block3_conv3', trainable=False)(x)\n    x = Conv2D(filters=256, kernel_size=(3,3), padding='same', activation='relu', name='block3_conv4', trainable=False)(x)\n    x = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid', name='block3_pool', trainable=False)(x)\n\n    #Block 4\n    x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu', name='block4_conv1')(x)\n    x = BatchNormalization(name='block4_bn1')(x)\n    x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu', name='block4_conv2')(x)\n    x = BatchNormalization(name='block4_bn2')(x)\n    x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu', name='block4_conv3')(x)\n    x = BatchNormalization(name='block4_bn3')(x)\n    x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu', name='block4_conv4')(x)\n    x = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid', name='block4_pool')(x)\n\n    #Block 5\n    x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu', name='block5_conv1')(x)\n    x = BatchNormalization(name='block5_bn1')(x)\n    x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu', name='block5_conv2')(x)\n    x = BatchNormalization(name='block5_bn2')(x)\n    x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu', name='block5_conv3')(x)\n    x = BatchNormalization(name='block5_bn3')(x)\n    x = Conv2D(filters=512, kernel_size=(3,3), padding='same', activation='relu', name='block5_conv4')(x)\n    x = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid', name='block5_pool')(x)\n    \n    #Other layers\n    x = Flatten(name='flatten')(x)\n    x = Dense(4096, activation='relu', name='fc1')(x)\n    x = Dropout(0.7, name='dropout_1')(x)\n    x = Dense(1000, activation='relu', name='fc2')(x)\n    x = Dropout(0.7, name='dropout_2')(x)\n    x = Dense(512, activation='relu', name='fc3')(x)\n    x = Dropout(0.5, name='dropout_3')(x)\n    x = Dense(1, activation='sigmoid', name='predictions')(x)\n    \n    model = Model(inputs=img_input, outputs=x)\n    return model","01c1700e":"base_model = VGG19(weights='imagenet', include_top=False)","9a359421":"base_model.summary()","677dfcb9":"model = create_model()\nmodel.summary()","8321fe54":"layer_count = 0\nfor layer in model.layers:\n    if layer.name[:6] in ['block1', 'block2', 'block3']:\n        model.layers[layer_count].set_weights = base_model.layers[layer_count].get_weights()\n    layer_count += 1","fb7af81d":"adam = Adam(lr = base_learning_rate)\nmodel.compile(loss='binary_crossentropy', metrics=['accuracy'], optimizer=adam)","a5f0fef9":"reduce_lr = ReduceLROnPlateau(monitor='val_acc', \n                                patience=3, \n                                verbose=1, \n                                factor=0.2, \n                                min_lr=1e-7)","5e79b33d":"model_chkpoint = ModelCheckpoint(filepath='vgg_19_best_model', save_best_only=True, save_weights_only=True)","77f186e7":"data_generator = generate_batch_images(df_train, batch_size)","886b4637":"model.fit_generator(data_generator, epochs=epochs, steps_per_epoch=df_train.shape[0]\/batch_size, \n                    callbacks=[reduce_lr, model_chkpoint], validation_data=[val_images, val_labels],\n                    class_weight={0:3, 1:1})","a6186ef8":"test_images.shape","6f6748da":"test_logits = model.predict(test_images, batch_size=16)","0420cbdf":"# Evaluation on test dataset\ntest_loss, test_acc = model.evaluate(test_images, test_labels, batch_size=16)\nprint(\"Loss on test set: \", test_loss)\nprint(\"Accuracy on test set: \", test_acc)","db98e90b":"cm  = confusion_matrix(test_labels, np.round(test_logits))\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8), hide_ticks=True, cmap=plt.cm.Oranges)\nplt.xticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.yticks(range(2), ['Normal', 'Pneumonia'], fontsize=16)\nplt.show()","92b42e0b":"true_negative, false_positive, false_negative, true_positive  = cm.ravel()\nprecision = true_positive \/ (true_positive + false_positive)\nrecall = true_positive \/ (true_positive + false_negative)\n\nprint('Precison of chest X-ray for pneumonia:{:.2f}'.format(precision))\nprint('Recall of chest X-ray for pneumonia:{:.2f}'.format(recall))","ba4c0a53":"Happy coding. Please upvote if you like this kernel.","64551b38":"# Time for some visualisations","57e9203a":"# Model\n\nDefining the model in below function. Let's keep block -1,2,3 frozen and train blocks 4 and 5 along with adding Batch Norm.","696e0890":"A helper function to generate dataframe with path and labels for train, validation and test sets.","8723543e":"# Let's arrange Validation and Test Data ","2ec4ee16":"In this kernel, I will demonstrate the classification of chest X-ray images for pneumonia or normal. This is a binary image classification dataset. I will use Keras VGG-19 transfer learning.\n\nI am also using imgaug for augmentation of images. Please visit my kernel [Data Augmentation in Python, TF, Keras, Imgaug](https:\/\/www.kaggle.com\/curiousprogrammer\/data-augmentation-in-python-tf-keras-imgaug) for more augmentation techniques.\n\nFor implementation using tensorflow hub ResNet-50, please visit my kernel [Chest X-Ray Image Classification - TF Hub ResNet50](https:\/\/www.kaggle.com\/curiousprogrammer\/chest-x-ray-image-classification-tf-hub-resnet50)\n\nLet's start by importing necessary libraries.","3fc4474d":"Let's check precision and recall. In this case, we should get a good recall than precision.","c3d35fe2":"As we visualise the training samples after shuffling, there is a visible difference between the normal samples vs the pneumonia samples.\n\nLet's visualise the samples after some augmentation.","4efd02fa":"# Let's plot the confusion matrix","658fc2c3":"# Deep Learning Model with Keras","373fc0ab":"Now let's use the weights from Keras pre-trained model(VGG19) and apply to first 3 blocks of custom model and freeze it.","bd2a77de":"There is a class imbalance pneumonia cases ~3x times the normal ones.\n\nLet's define a function to resize the images and to augment. Some of the images has 3 channels and some one. Let's make all images to RGB."}}