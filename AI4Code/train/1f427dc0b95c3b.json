{"cell_type":{"62344df4":"code","eeb9f24f":"code","52aeccfd":"code","e06c115e":"code","b4f0abd4":"code","848c99ba":"code","b712eb7b":"code","7eefd30e":"code","e70703f5":"markdown","c550a009":"markdown","f58a9f44":"markdown","814dbd1c":"markdown","98e9b6e2":"markdown","ebbbf074":"markdown","beac4f81":"markdown","95b8a41c":"markdown","a3791252":"markdown","81020a9d":"markdown","aa659708":"markdown","df4a425f":"markdown","a0fe10c7":"markdown","e6b3e4e3":"markdown","569a62ce":"markdown"},"source":{"62344df4":"import pandas as pd\nimport numpy as np\n\ndef compare(results, baseline_tactic, current_tactic):\n    \"\"\"This function compares the results between a current tactic and a baseline.\n    For each result in the current tactic it compares the result and the score.\n\n    :param results: DataFrame with results.\n    :param baseline_tactic: Tactic that will be used as a baseline.\n    :param current_tactic: Tactic with the results to compare.\n    :return: DataFrame with model, and differences in result and score between current and baseline tactics.\n    \"\"\"\n    comparison = pd.DataFrame(columns=['Model',\n                                       'Result',\n                                       'Score'])\n\n    for row in results.query('Tactic == ' + str(current_tactic)).itertuples(index=False):\n        previous = results.query('Tactic == ' + str(baseline_tactic) + ' and Model == \"' + row.Model + '\"')\n        comparison = comparison.append({\n            'Model': row.Model,\n            'Result': '{:.2%}'.format((row.Result - float(previous.Result)) \/ float(previous.Result)),\n            'Score': '{:.2%}'.format((row.Score - float(previous.Score)) \/ float(previous.Score))\n        }, ignore_index=True)\n\n    return comparison\n\nresults = pd.read_csv('..\/input\/tactic-98-results\/results.csv', index_col='Id', engine='python')","eeb9f24f":"compare(results, 1, 3)","52aeccfd":"compare(results, 3, 5)","e06c115e":"compare(results, 3, 6)","b4f0abd4":"compare(results, 6, 7)","848c99ba":"compare(results, 6, 8)","b712eb7b":"compare(results, 6, 9)","7eefd30e":"compare(results, 6, 10)","e70703f5":"# Tactic 09. Feature selection: backward elimination","c550a009":"# Tactic 05. Class weight","f58a9f44":"Only optimal importance has a tiny improve in the score.\n\nFrom 70 features of the tactic 06 model, the optimal importance has 59.\nOnly 11 features drop.\n\nSome features, when added to the model, makes the score increase or decrease.\nIf only the features that makes the score increase are taken,\nthen the score is practically the same,\nwith only 39 features.","814dbd1c":"# Tactic 03. Hyperparameter optimization","98e9b6e2":"# Tactic 10. Feature selection: forward selection","ebbbf074":"Creating more features from the original ones,\nmakes generally the models to have more score.\n\nTwo models: `knn` and `lr` have a worse score with more features. ","beac4f81":"The classes are balanced.\nThere is no need to weight them.\n\nIf the classes in the train set had the save proportion as in the test set,\nthe score would be pretty bad.\n\nThen, it's important to have the classes balanced.","95b8a41c":"# Tactic 07. Outlier detection","a3791252":"Backward elimination has a very close score compared with the original using only a few less features: 61.","81020a9d":"# Tactic 06. Feature engineering","aa659708":"# Introduction\n\nThe aim of this notebook is to analyse the results of each of the different tactics tested.\n\nThere are a huge collection of notebooks, each one with a tactic or part of a tactic.\nThe objective of each tactic is to study, analyse, and get conclusions of an very specific concept.\n\n- [Tactic 00. Baseline](https:\/\/www.kaggle.com\/juanmah\/tactic-00-baseline)\n- [Tactic 01. Test classifiers](https:\/\/www.kaggle.com\/juanmah\/tactic-01-test-classifiers)\n- [Tactic 02. Stack classifiers](https:\/\/www.kaggle.com\/juanmah\/tactic-02-stack-classifiers)\n- [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization):\n - [Tactic 03. Hyperparameter optimization. LR](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lr)\n - [Tactic 03. Hyperparameter optimization. LDA](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lda)\n - [Tactic 03. Hyperparameter optimization. KNN](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-knn)\n - [Tactic 03. Hyperparameter optimization. GNB](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-gnb)\n - [Tactic 03. Hyperparameter optimization. SVC](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-svc)\n - [Tactic 03. Hyperparameter optimization. Bagging](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-bagging)\n - [Tactic 03. Hyperparameter optimization. Xtra-trees](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-xtra-trees)\n - [Tactic 03. Hyperparameter optimization. Adaboost](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-adaboost)\n - [Tactic 03. Hyperparameter optimization. GB](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-gb)\n - [Tactic 03. Hyperparameter optimization. LightGBM](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-lightgbm)\n - [Tactic 03. Hyperparameter optimization. XGBoost](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization-xgboost)\n- [Tactic 04. Stack optimized classifiers](https:\/\/www.kaggle.com\/juanmah\/tactic-04-stack-optimized-classifiers)\n- [Tactic 05. Class weight](https:\/\/www.kaggle.com\/juanmah\/tactic-05-class-weight)\n- [Tactic 06. Feature engineering](https:\/\/www.kaggle.com\/juanmah\/tactic-06-feature-engineering)\n- [Tactic 07. Outlier detection](https:\/\/www.kaggle.com\/juanmah\/tactic-07-outlier-detection)\n- [Tactic 08. Feature selection: optimal importance](https:\/\/www.kaggle.com\/juanmah\/tactic-08-feature-selection-optimal-importance)\n- [Tactic 09. Feature selection: backward elimination](https:\/\/www.kaggle.com\/juanmah\/tactic-09-feature-selection-backward-elimination)\n- [Tactic 10. Feature selection: forward selection](https:\/\/www.kaggle.com\/juanmah\/tactic-10-feature-selection-forward-selection)\n\nThe result table is the collection of all the individual results in each tactic.\n\nThere are two main numbers in this table: result and score.\n**Result** is the internal score (i.e. the score on the train set),\nand **score** is the public score returned by Kaggle (i.e. the score in the test set).","df4a425f":"# Tactic 08. Feature selection: optimal importance","a0fe10c7":"Forward selection has a similar, but lower score, with only 36 features.","e6b3e4e3":"Isolation forest is applied to detect outliers.\n\nRemoving samples that are pointed as outliers don't improve the score.\nThe more samples are removed the worst the score.","569a62ce":"The hyperparameter optimization is a MUST to get a good score.\nAll models can be enhaced by optimizacion.\n\nModels as `lg`, `bg`, `lr` and `gb` are good enough with the default parameters.\nThe rest can be improved a lot.\n\nThis is valid for this data.\nFor other data, probably, the results could be differnt."}}