{"cell_type":{"622b8339":"code","8f4d518c":"code","05d6ce1a":"code","00b31350":"code","742d6674":"code","d252decd":"code","9e57a376":"code","db3fd2b4":"code","54a9b990":"code","35475891":"code","e2498059":"code","dcb72bfc":"code","9a42f26f":"code","88073e66":"code","640a7727":"code","ddecc9e3":"code","d992b60a":"code","5cfebe7b":"code","3a6b2d8a":"code","11242aa2":"code","197a8c5d":"markdown","61039f13":"markdown","7df6ea14":"markdown","cad19fa5":"markdown","2ab9a60f":"markdown","acc7e387":"markdown","19f99e7f":"markdown","1340db70":"markdown","d0bd1371":"markdown","7eb8ff46":"markdown","1c3e245b":"markdown"},"source":{"622b8339":"import numpy as np\nimport plotly\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport plotly.io as pio\nimport plotly.offline as pyo\npio.templates.default='plotly_white'\n\nimport torch\nimport torch.nn as nn\n\nimport transformers \nfrom transformers import AutoModel\nfrom transformers import AdamW\nfrom transformers import (  \n    get_constant_schedule, \n    get_constant_schedule_with_warmup, \n    get_cosine_schedule_with_warmup, \n    get_cosine_with_hard_restarts_schedule_with_warmup,\n    get_linear_schedule_with_warmup,\n    get_polynomial_decay_schedule_with_warmup\n)\n\nepochs = 10","8f4d518c":"class Net(nn.Module):\n    def __init__(self, model_name):\n        super(Net, self).__init__()\n        self.roberta = AutoModel.from_pretrained(model_name)\n        self.classifier = nn.Linear(768, 1)\n    def forward(self, input_ids):\n        outputs = self.roberta(input_ids)\n        sequence_output = outputs[1]\n        return self.classifier(sequence_output)","05d6ce1a":"def get_optimizer_params(model, type='s'):\n    # differential learning rate and weight decay\n    param_optimizer = list(model.named_parameters())\n    learning_rate = 5e-5\n    no_decay = ['bias', 'gamma', 'beta']\n    if type == 's':\n        optimizer_parameters = filter(lambda x: x.requires_grad, model.parameters())\n    elif type == 'i':\n        optimizer_parameters = [\n            {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay)],\n             'weight_decay_rate': 0.01},\n            {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay)],\n             'weight_decay_rate': 0.0},\n            {'params': [p for n, p in model.named_parameters() if \"roberta\" not in n],\n             'lr': 1e-3,\n             'weight_decay_rate':0.01}\n        ]\n    elif type == 'a':\n        group1=['layer.0.','layer.1.','layer.2.','layer.3.']\n        group2=['layer.4.','layer.5.','layer.6.','layer.7.']    \n        group3=['layer.8.','layer.9.','layer.10.','layer.11.']\n        group_all=['layer.0.','layer.1.','layer.2.','layer.3.','layer.4.','layer.5.','layer.6.','layer.7.','layer.8.','layer.9.','layer.10.','layer.11.']\n        optimizer_parameters = [\n            {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay_rate': 0.01},\n            {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay_rate': 0.01, 'lr': learning_rate\/2.6},\n            {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay_rate': 0.01, 'lr': learning_rate},\n            {'params': [p for n, p in model.roberta.named_parameters() if not any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay_rate': 0.01, 'lr': learning_rate*2.6},\n            {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and not any(nd in n for nd in group_all)],'weight_decay_rate': 0.0},\n            {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group1)],'weight_decay_rate': 0.0, 'lr': learning_rate\/2.6},\n            {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group2)],'weight_decay_rate': 0.0, 'lr': learning_rate},\n            {'params': [p for n, p in model.roberta.named_parameters() if any(nd in n for nd in no_decay) and any(nd in n for nd in group3)],'weight_decay_rate': 0.0, 'lr': learning_rate*2.6},\n            {'params': [p for n, p in model.named_parameters() if \"roberta\" not in n], 'lr':1e-3, \"momentum\" : 0.99},\n        ]\n    return optimizer_parameters","00b31350":"def get_default_layout(title):\n    font_style = 'Courier New'\n    layout = {}\n    layout['height'] = 400\n    layout['width'] = 1200\n    layout['template'] = 'plotly_white'\n    layout['dragmode'] = 'zoom'\n    layout['hovermode'] = 'x'\n    layout['hoverlabel'] = {\n        'font_size': 14,\n        'font_family':font_style\n    }\n    layout['font'] = {\n        'size':14,\n        'family':font_style,\n        'color':'rgb(128, 128, 128)'\n    }\n    layout['xaxis'] = {\n        'title': 'Epochs',\n        'showgrid': True,\n        'type': 'linear',\n        'categoryarray': None,\n        'gridwidth': 1,\n        'ticks': 'outside',\n        'showline': True, \n        'showticklabels': True,\n        'tickangle': 0,\n        'tickmode': 'array'\n    }\n    layout['yaxis'] = {\n        'title': 'Learning Rate',\n        'exponentformat':'none',\n        'showgrid': True,\n        'type': 'linear',\n        'categoryarray': None,\n        'gridwidth': 1,\n        'ticks': 'outside',\n        'showline': True, \n        'showticklabels': True,\n        'tickangle': 0,\n        'tickmode': 'array'\n    }\n    layout['title'] = {\n        'text':title,\n        'x': 0.5,\n        'y': 0.95,\n        'xanchor': 'center',\n        'yanchor': 'top',\n        'font': {\n            'family':font_style,\n            'size':14,\n            'color':'black'\n        }\n    }\n    layout['showlegend'] = True\n    layout['legend'] = {\n        'x':0.1,\n        'y':1.1,\n        'orientation':'h',\n        'itemclick': 'toggleothers',\n        'font': {\n            'family':font_style,\n            'size':14,\n            'color':'black'\n        }\n    }\n    return go.Layout(layout)","742d6674":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 's')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_constant_schedule(optimizer)\n\nlearning_rates = []\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates.append(optimizer.param_groups[0][\"lr\"])\n\ntrace = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='LR',\n    marker=dict(color='#3498d5'),\n)\nlayout=get_default_layout('Constant Schedule')\ngo.Figure(data=[trace], layout=layout)","d252decd":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 'i')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_constant_schedule(optimizer)\n\nlearning_rates1, learning_rates2 = [[] for i in range(2)]\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates1.append(optimizer.param_groups[0][\"lr\"])\n    learning_rates2.append(optimizer.param_groups[2][\"lr\"])\n\ntrace1 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates1, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta',\n    marker=dict(color='#3498d5'),\n)\ntrace2 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates2, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Regressor',\n    marker=dict(color='#f29191'),\n)\nlayout=get_default_layout('Constant Schedule')\ngo.Figure(data=[trace1, trace2], layout=layout)","9e57a376":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 's')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=3)\n\nlearning_rates = []\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates.append(optimizer.param_groups[0][\"lr\"])\n\ntrace = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='LR',\n    marker=dict(color='#3498d5'),\n)\nlayout=get_default_layout('Constant Schedule with Warmup')\ngo.Figure(data=[trace], layout=layout)","db3fd2b4":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 'i')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_constant_schedule_with_warmup(optimizer, num_warmup_steps=3)\n\nlearning_rates1, learning_rates2 = [[] for i in range(2)]\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates1.append(optimizer.param_groups[0][\"lr\"])\n    learning_rates2.append(optimizer.param_groups[2][\"lr\"])\n\ntrace1 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates1, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta',\n    marker=dict(color='#3498d5'),\n)\ntrace2 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates2, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Regressor',\n    marker=dict(color='#f29191'),\n)\nlayout=get_default_layout('Constant Schedule with Warmup')\ngo.Figure(data=[trace1, trace2], layout=layout)","54a9b990":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 's')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=10)\n\nlearning_rates = []\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates.append(optimizer.param_groups[0][\"lr\"])\ntrace = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='LR',\n    marker=dict(color='#3498d5'),\n)\nlayout=get_default_layout('Cosine Schedule with Warmup')\ngo.Figure(data=[trace], layout=layout)","35475891":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 'i')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=10)\n\nlearning_rates1, learning_rates2 = [[] for i in range(2)]\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates1.append(optimizer.param_groups[0][\"lr\"])\n    learning_rates2.append(optimizer.param_groups[2][\"lr\"])\n\ntrace1 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates1, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta',\n    marker=dict(color='#3498d5'),\n)\ntrace2 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates2, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Regressor',\n    marker=dict(color='#f29191'),\n)\nlayout=get_default_layout('Cosine Schedule with Warmup')\ngo.Figure(data=[trace1, trace2], layout=layout)","e2498059":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 'a')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=10)\n\nlearning_rates1, learning_rates2, learning_rates3, learning_rates4 = [[] for i in range(4)]\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates1.append(optimizer.param_groups[1][\"lr\"])\n    learning_rates2.append(optimizer.param_groups[2][\"lr\"])\n    learning_rates3.append(optimizer.param_groups[3][\"lr\"])\n    learning_rates4.append(optimizer.param_groups[8][\"lr\"])\n\ntrace1 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates1, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta Layers 1-4',\n    marker=dict(color='#3498d5'),\n)\ntrace2 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates2, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta Layers 5-8',\n    marker=dict(color='#a678de'),\n)\ntrace3 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates3, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta Layers 9-12',\n    marker=dict(color='#6ad49b'),\n)\ntrace4 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates4, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Regressor',\n    marker=dict(color='#f29191'),\n)\nlayout=get_default_layout('Cosine Schedule with Warmup')\ngo.Figure(data=[trace1, trace2, trace3, trace4], layout=layout)","dcb72bfc":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 's')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=10, num_cycles=5)\n\nlearning_rates = []\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates.append(optimizer.param_groups[0][\"lr\"])\ntrace = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='LR',\n    marker=dict(color='#3498d5'),\n)\nlayout=get_default_layout('Cosine Schedule with Hard Restarts with Warmup')\ngo.Figure(data=[trace], layout=layout)","9a42f26f":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 'i')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=10, num_cycles=5)\n\nlearning_rates1, learning_rates2 = [[] for i in range(2)]\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates1.append(optimizer.param_groups[0][\"lr\"])\n    learning_rates2.append(optimizer.param_groups[2][\"lr\"])\n\ntrace1 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates1, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta',\n    marker=dict(color='#3498d5'),\n)\ntrace2 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates2, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Regressor',\n    marker=dict(color='#f29191'),\n)\nlayout=get_default_layout('Cosine Schedule with Hard Restarts with Warmup')\ngo.Figure(data=[trace1, trace2], layout=layout)","88073e66":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 'a')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_cosine_with_hard_restarts_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=10, num_cycles=5)\n\nlearning_rates1, learning_rates2, learning_rates3, learning_rates4 = [[] for i in range(4)]\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates1.append(optimizer.param_groups[1][\"lr\"])\n    learning_rates2.append(optimizer.param_groups[2][\"lr\"])\n    learning_rates3.append(optimizer.param_groups[3][\"lr\"])\n    learning_rates4.append(optimizer.param_groups[8][\"lr\"])\n\ntrace1 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates1, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta Layers 1-4',\n    marker=dict(color='#3498d5'),\n)\ntrace2 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates2, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta Layers 5-8',\n    marker=dict(color='#a678de'),\n)\ntrace3 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates3, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta Layers 9-12',\n    marker=dict(color='#6ad49b'),\n)\ntrace4 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates4, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Regressor',\n    marker=dict(color='#f29191'),\n)\nlayout=get_default_layout('Cosine Schedule with Hard Restarts with Warmup')\ngo.Figure(data=[trace1, trace2, trace3, trace4], layout=layout)","640a7727":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 's')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=10)\n\nlearning_rates = []\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates.append(optimizer.param_groups[0][\"lr\"])\ntrace = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='LR',\n    marker=dict(color='#3498d5'),\n)\nlayout=get_default_layout('Linear Schedule with Warmup')\ngo.Figure(data=[trace], layout=layout)","ddecc9e3":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 'i')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=10)\n\nlearning_rates1, learning_rates2 = [[] for i in range(2)]\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates1.append(optimizer.param_groups[0][\"lr\"])\n    learning_rates2.append(optimizer.param_groups[2][\"lr\"])\n\ntrace1 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates1, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta',\n    marker=dict(color='#3498d5'),\n)\ntrace2 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates2, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Regressor',\n    marker=dict(color='#f29191'),\n)\nlayout=get_default_layout('Linear Schedule with Warmup')\ngo.Figure(data=[trace1, trace2], layout=layout)","d992b60a":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 'a')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=10)\n\nlearning_rates1, learning_rates2, learning_rates3, learning_rates4 = [[] for i in range(4)]\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates1.append(optimizer.param_groups[1][\"lr\"])\n    learning_rates2.append(optimizer.param_groups[2][\"lr\"])\n    learning_rates3.append(optimizer.param_groups[3][\"lr\"])\n    learning_rates4.append(optimizer.param_groups[8][\"lr\"])\n\ntrace1 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates1, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta Layers 1-4',\n    marker=dict(color='#3498d5'),\n)\ntrace2 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates2, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta Layers 5-8',\n    marker=dict(color='#a678de'),\n)\ntrace3 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates3, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta Layers 9-12',\n    marker=dict(color='#6ad49b'),\n)\ntrace4 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates4, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Regressor',\n    marker=dict(color='#f29191'),\n)\nlayout=get_default_layout('Linear Schedule with Warmup')\ngo.Figure(data=[trace1, trace2, trace3, trace4], layout=layout)","5cfebe7b":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 's')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=10, power=2)\n\nlearning_rates = []\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates.append(optimizer.param_groups[0][\"lr\"])\n\ntrace = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='LR',\n    marker=dict(color='#3498d5'),\n)\nlayout=get_default_layout('Polynomial Decay Schedule with Warmup')\ngo.Figure(data=[trace], layout=layout)","3a6b2d8a":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 'i')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=10, power=2)\n\nlearning_rates1, learning_rates2 = [[] for i in range(2)]\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates1.append(optimizer.param_groups[0][\"lr\"])\n    learning_rates2.append(optimizer.param_groups[2][\"lr\"])\n\ntrace1 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates1, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta',\n    marker=dict(color='#3498d5'),\n)\ntrace2 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates2, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Regressor',\n    marker=dict(color='#f29191'),\n)\nlayout=get_default_layout('Polynomial Decay Schedule with Warmup')\ngo.Figure(data=[trace1, trace2], layout=layout)","11242aa2":"model = Net('roberta-base')\nparameters = get_optimizer_params(model, 'a')\nkwargs = {\n    'betas': (0.9, 0.999),\n    'eps': 1e-08\n}\noptimizer = AdamW(parameters, lr=5e-5, **kwargs)\nscheduler = get_polynomial_decay_schedule_with_warmup(optimizer, num_warmup_steps=3, num_training_steps=10, power=2)\n\nlearning_rates1, learning_rates2, learning_rates3, learning_rates4 = [[] for i in range(4)]\nfor i in range(epochs):\n    optimizer.step()\n    scheduler.step()\n    learning_rates1.append(optimizer.param_groups[1][\"lr\"])\n    learning_rates2.append(optimizer.param_groups[2][\"lr\"])\n    learning_rates3.append(optimizer.param_groups[3][\"lr\"])\n    learning_rates4.append(optimizer.param_groups[8][\"lr\"])\n\ntrace1 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates1, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta Layers 1-4',\n    marker=dict(color='#3498d5'),\n)\ntrace2 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates2, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta Layers 5-8',\n    marker=dict(color='#a678de'),\n)\ntrace3 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates3, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Roberta Layers 9-12',\n    marker=dict(color='#6ad49b'),\n)\ntrace4 = go.Scatter(\n    x=np.arange(0, epochs, 1), \n    y=learning_rates4, \n    texttemplate=\"%{y:.6f}\",\n    mode='markers+lines',\n    name='Regressor',\n    marker=dict(color='#f29191'),\n)\nlayout=get_default_layout('Polynomial Decay Schedule with Warmup')\ngo.Figure(data=[trace1, trace2, trace3, trace4], layout=layout)","197a8c5d":"### Import Dependencies","61039f13":"### Differential \/ Discriminative Learning Rate\n\n#### Introduction\n1. The intuitive idea of **Differential Learning Rate** is that the embeddings start out in the first layer and have no contextual information. As the embeddings move deeper into the network, they pick up more general and contextual information with each layer. As we approach the final layers, however, we start picking up information that is specific to Transformer\u2019s pre-training tasks (e.g. RoBERTa's \"Masked Language Model\" (MLM) and \"Next Sentence Prediction\" (NSP). \n\n\n2. Thus we can fine-tune the layers with different learning rates i.e. lower learning rates in the early layers, slightly higher in middle layers and higher at the top layers. \n\n\n3. We can also train task specific layers with a completely different and much higher learning rate than the transformer model since it hasn't been trained before and needs to learn faster.\n\n\n\n#### Strategies for setting Learning Rate\n\n1. `Unified Learning Rate for Complete Model (s)` - In this strategy we will set a single learning rate (in this case - 5e-5) for the complete model which is usually done.\n\n2. `Differential Learning Rate for Transformer and Task-Specic Layer (i)` - In this strategy we will have two different lr's. One unified for the complete transformer model (RoBERTa) and other one for the task-specific layer (Regressor).\n\n3. `Differential Learning Rate for Transformer Layers and Task-Specific Layer (a)` - Here we will set different different lr's for different transformer layers as well. I have grouped layers 1-4, 4-8, 8-12 and set learning rates accordingly as dicussed above. Other than that, I also set different and higher learning rate for the regressor.","7df6ea14":"### Constant Schedule with Warmup\n\n> Create a schedule with a constant learning rate preceded by a warmup period during which the learning rate increases linearly between 0 and the initial lr set in the optimizer.\n\n","cad19fa5":"### Linear Schedule with Warmup\nCreate a schedule with a learning rate that decreases linearly from the initial lr set in the optimizer to 0, after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.","2ab9a60f":"### Helper Function\n\nThis defines a default layout with `title` as the only parameter which can be changed. We will use this for layout for every graph.","acc7e387":"## Polynomial Decay with Warmup\nCreate a schedule with a learning rate that decreases as a polynomial decay from the initial lr set in the optimizer to end lr defined by lr_end, after a warmup period during which it increases linearly from 0 to the initial lr set in the optimizer.","19f99e7f":"### Constant Schedule\n\n> Create a schedule with a constant learning rate, using the learning rate set in optimizer.","1340db70":"### Cosine with Warmup\n> Create a schedule with a learning rate that decreases following the values of the cosine function between the initial lr set in the optimizer to 0, after a warmup period during which it increases linearly between 0 and the initial lr set in the optimizer.","d0bd1371":"### Cosine With Hard Restarts\nCreate a schedule with a learning rate that decreases following the values of the cosine function between the initial lr set in the optimizer to 0, with several hard restarts, after a warmup period during which it increases linearly between 0 and the initial lr set in the optimizer.","7eb8ff46":"## Guide to HuggingFace Schedulers & Differential LRs\n\n### What's New?\n1. [SWA, Apex AMP & Interpreting Transformers in Torch](https:\/\/www.kaggle.com\/rhtsingh\/swa-apex-amp-interpreting-transformers-in-torch) notebook is an implementation of the Stochastic Weight Averaging technique with NVIDIA Apex on transformers using PyTorch. The notebook also implements how to interactively interpret Transformers using LIT (Language Interpretability Tool) a platform for NLP model understanding.   \nIt has in-depth explanations and code implementations for,\n - SWA \n - Apex AMP\n - Weighted Layer Pooling\n - MADGRAD Optimizer\n - Grouped LLRD\n - Language Interpretibility Tool\n    - Attention Visualization\n    - Saliency Maps\n    - Integrated Gradients\n    - LIME \n    - Embedding Space (UMAP & PCA)\n    - Counterfactual generation\n    - And many more ...\n\n\n2. [Utilizing Transformer Representations Efficiently](https:\/\/www.kaggle.com\/rhtsingh\/utilizing-transformer-representations-efficiently) notebook will show many different ways these outputs and hidden representations can be utilized to do much more than just adding an output layer. It has code implementations and detailed explanations for all the below techniques,\n - Pooler Output  \n - Last Hidden State Output  \n    - CLS Embeddings  \n    - Mean Pooling  \n    - Max Pooling  \n    - Mean + Max Pooling  \n    - Conv1D Pooling  \n - Hidden Layers Output  \n    - Layerwise CLS Embeddings  \n    - Concatenate Pooling  \n    - Weighted Layer Pooling  \n    - LSTM \/ GRU Pooling  \n    - Attention Pooling  \n    - WKPooling  \n\n3. [On Stability of Few-Sample Transformer Fine-Tuning](https:\/\/www.kaggle.com\/rhtsingh\/on-stability-of-few-sample-transformer-fine-tuning) notebook goes over various remedies to increase few-sample fine-tuning stability and they show a significant performance improvement over simple finetuning methods. The methods explained in the notebook are - \n - Debiasing Omission In BertADAM\n - Re-Initializing Transformer Layers\n - Utilizing Intermediate Layers\n - Layer-wise Learning Rate Decay (LLRD) \n - Mixout Regularization\n - Pre-trained Weight Decay\n - Stochastic Weight Averaging. \n \n4. [Speeding up Transformer w\/ Optimization Strategies](https:\/\/www.kaggle.com\/rhtsingh\/speeding-up-transformer-w-optimization-strategies) notebook explains in-depth 5 optimization strategies with code. All these techniques are promising and can improve the model performance both in terms of speed and accuracy.\n   - Dynamic Padding and Uniform Length Batching\n   - Gradient Accumulation\n   - Freeze Embedding\n   - Numeric Precision Reduction\n   - Gradient Checkpointing  ","1c3e245b":"### Model"}}