{"cell_type":{"37ec3b72":"code","9f29b4b6":"code","efadfae9":"code","56ecd9cf":"code","e1b50ead":"code","e37f955e":"code","19e4285f":"code","fcc89673":"code","79e664c3":"code","4b8431c1":"code","80a5d75e":"code","bf69540e":"code","30837ec0":"code","e64a81e9":"code","53335817":"code","10989285":"code","ca47852f":"code","2989a2e4":"code","71f93a49":"code","6d10d666":"code","4db5a668":"code","9d365d10":"code","cceb9a3f":"code","f20e4288":"code","cfda0610":"code","b08a691f":"code","6083866d":"code","3e8819ad":"code","c462662a":"code","1e689177":"code","b7d2b8d7":"code","80ebc12e":"code","7d254f67":"code","5ce781cd":"code","3a6ab822":"code","25a46014":"markdown","8ca8a5a1":"markdown","b7b23844":"markdown","1d8f2222":"markdown"},"source":{"37ec3b72":"! pip install pytorch-lightning==0.8.1","9f29b4b6":"import os\nimport random as rn\nfrom glob import glob\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\n\nimport torchvision\nfrom torchvision import transforms\n\nimport torch\nimport torch.nn as nn\n\nfrom torch.nn import functional as F\nfrom torch.autograd import Variable\n\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom pytorch_lightning.callbacks import EarlyStopping\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning import Trainer\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","efadfae9":"#fix random seed\nos.environ['PYTHONHASHSEED'] = '0'\nnp.random.seed(42)\nrn.seed(12345)\ntorch.manual_seed(2019)\ntorch.cuda.manual_seed(2019)\ntorch.cuda.manual_seed_all(2019)\ntorch.backends.cudnn.deterministic = True","56ecd9cf":"print(os.listdir(\"..\/input\/data\"))","e1b50ead":"all_xray_df = pd.read_csv('..\/input\/data\/Data_Entry_2017.csv')\n\n#all_xray_df = all_xray_df.iloc[:1000]\n\nall_image_paths = {os.path.basename(x): x for x in glob(os.path.join('..', 'input\/data', 'images*', '*', '*.png'))}\n\nprint('Scans found:', len(all_image_paths), ', Total Headers', all_xray_df.shape[0])\nall_xray_df['path'] = all_xray_df['Image Index'].map(all_image_paths.get)\n#all_xray_df['Patient Age'] = all_xray_df['Patient Age'].map(lambda x: int(x[:-1]))\nall_xray_df.sample(3)","e37f955e":"label_counts = all_xray_df['Finding Labels'].value_counts()[:15]\nfig, ax1 = plt.subplots(1,1,figsize = (12, 8))\nax1.bar(np.arange(len(label_counts))+0.5, label_counts)\nax1.set_xticks(np.arange(len(label_counts))+0.5)\n_ = ax1.set_xticklabels(label_counts.index, rotation = 90)","19e4285f":"labels = ['Cardiomegaly', \n          'Emphysema', \n          'Effusion', \n          'Hernia', \n          'Infiltration', \n          'Mass', \n          'Nodule', \n          'Atelectasis',\n          'Pneumothorax',\n          'Pleural_Thickening', \n          'Pneumonia', \n          'Fibrosis', \n          'Edema', \n          'Consolidation']","fcc89673":"drop_column = ['Patient Age','Patient Gender','View Position','Follow-up #','OriginalImagePixelSpacing[x','y]','OriginalImage[Width','Height]','Unnamed: 11']","79e664c3":"all_xray_df['Finding Labels'] = all_xray_df['Finding Labels'].map(lambda x: x.replace('No Finding', ''))\nfrom itertools import chain\nall_labels = np.unique(list(chain(*all_xray_df['Finding Labels'].map(lambda x: x.split('|')).tolist())))\nall_labels = [x for x in all_labels if len(x)>0]\nprint('All Labels ({}): {}'.format(len(all_labels), all_labels))\nfor c_label in all_labels:\n    if len(c_label)>1: # leave out empty labels\n        all_xray_df[c_label] = all_xray_df['Finding Labels'].map(lambda finding: 1 if c_label in finding else 0)\n\n\nall_xray_df.sample(3)","4b8431c1":"all_xray_df = all_xray_df.drop(drop_column,axis=1)\nall_xray_df.sample(3)","80a5d75e":"label_counts = 100*np.mean(all_xray_df[all_labels].values,0)\nfig, ax1 = plt.subplots(1,1,figsize = (12, 8))\nax1.bar(np.arange(len(label_counts))+0.5, label_counts)\nax1.set_xticks(np.arange(len(label_counts))+0.5)\nax1.set_xticklabels(all_labels, rotation = 90)\nax1.set_title('Adjusted Frequency of Diseases in Patient Group')\n_ = ax1.set_ylabel('Frequency (%)')","bf69540e":"all_xray_df['disease_vec'] = all_xray_df.apply(lambda x: [x[all_labels].values], 1).map(lambda x: x[0])","30837ec0":"train_df, valid_df, test_df = np.split(all_xray_df.sample(frac=1), [int(.6*len(all_xray_df)), int(.8*len(all_xray_df))])","e64a81e9":"print('train', train_df.shape[0], 'validation', valid_df.shape[0], 'test', test_df.shape[0])\ntrain_df.sample(3)","53335817":"def compute_class_freqs(labels):\n    \"\"\"\n    Compute positive and negative frequences for each class.\n\n    Args:\n        labels (np.array): matrix of labels, size (num_examples, num_classes)\n    Returns:\n        positive_frequencies (np.array): array of positive frequences for each\n                                         class, size (num_classes)\n        negative_frequencies (np.array): array of negative frequences for each\n                                         class, size (num_classes)\n    \"\"\"\n    \n    # total number of patients (rows)\n    N = len(labels)\n    \n    positive_frequencies = np.sum(labels, axis=0) \/ N\n    negative_frequencies = 1 - positive_frequencies\n\n    return positive_frequencies, negative_frequencies\n\n","10989285":"train_labels = []\nds_len = train_df.shape[0]\n\nfor inx in range(ds_len):\n    row = train_df.iloc[inx]\n    vec = np.array(row['disease_vec'], dtype=np.int)\n    train_labels.append(vec)\n\n","ca47852f":"freq_pos, freq_neg = compute_class_freqs(train_labels)\nfreq_pos\n\n\ndata = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": freq_pos})\ndata = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} for l,v in enumerate(freq_neg)], ignore_index=True)\nplt.xticks(rotation=90)\nf = sns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data)\n","2989a2e4":"def weighted_loss(y_true, y_pred, pos_weights, neg_weights, epsilon=1e-7):\n        \"\"\"\n        Return weighted loss value. \n\n        Args:\n            y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes)\n            y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes)\n        Returns:\n            loss (Tensor): overall scalar loss summed across all classes\n        \"\"\"\n        # initialize loss to zero\n        loss = 0.0\n\n        for i in range(len(pos_weights)):\n            # for each class, add average weighted loss for that class \n            loss += -(torch.mean( pos_weights[i] * y_true[:,i] * torch.log(y_pred[:,i] + epsilon) + \\\n                                neg_weights[i] * (1 - y_true[:,i]) * torch.log(1 - y_pred[:,i] + epsilon), axis = 0))\n            \n        return loss","71f93a49":"pos_weights = freq_neg\nneg_weights = freq_pos\npos_contribution = freq_pos * pos_weights \nneg_contribution = freq_neg * neg_weights","6d10d666":"data = pd.DataFrame({\"Class\": labels, \"Label\": \"Positive\", \"Value\": pos_contribution})\ndata = data.append([{\"Class\": labels[l], \"Label\": \"Negative\", \"Value\": v} \n                        for l,v in enumerate(neg_contribution)], ignore_index=True)\nplt.xticks(rotation=90)\nsns.barplot(x=\"Class\", y=\"Value\", hue=\"Label\" ,data=data);","4db5a668":"ids_train = train_df['Patient ID'].values\nids_valid = valid_df['Patient ID'].values\n","9d365d10":"# Create a \"set\" datastructure of the training set id's to identify unique id's\nids_train_set = set(ids_train)\nprint(f'There are {len(ids_train_set)} unique Patient IDs in the training set')\n# Create a \"set\" datastructure of the validation set id's to identify unique id's\nids_valid_set = set(ids_valid)\nprint(f'There are {len(ids_valid_set)} unique Patient IDs in the training set')","cceb9a3f":"# Identify patient overlap by looking at the intersection between the sets\npatient_overlap = list(ids_train_set.intersection(ids_valid_set))\nn_overlap = len(patient_overlap)\n# print(f'There are {n_overlap} Patient IDs in both the training and validation sets')\n# print('')\n# print(f'These patients are in both the training and validation datasets:')\n# print(f'{patient_overlap}')","f20e4288":"train_overlap_idxs = []\nvalid_overlap_idxs = []\nfor idx in range(n_overlap):\n    train_overlap_idxs.extend(train_df.index[train_df['Patient ID'] == patient_overlap[idx]].tolist())\n    valid_overlap_idxs.extend(valid_df.index[valid_df['Patient ID'] == patient_overlap[idx]].tolist())\n    \n# print(f'These are the indices of overlapping patients in the training set: ')\n# print(f'{train_overlap_idxs}')\n# print(f'These are the indices of overlapping patients in the validation set: ')\n# print(f'{valid_overlap_idxs}')","cfda0610":"# Drop the overlapping rows from the validation set\nvalid_df.drop(valid_overlap_idxs, inplace=True)","b08a691f":"# Extract patient id's for the validation set\nids_valid = valid_df['Patient ID'].values\n# Create a \"set\" datastructure of the validation set id's to identify unique id's\nids_valid_set = set(ids_valid)\nprint(f'There are {len(ids_valid_set)} unique Patient IDs in the training set')","6083866d":"# Identify patient overlap by looking at the intersection between the sets\npatient_overlap = list(ids_train_set.intersection(ids_valid_set))\nn_overlap = len(patient_overlap)\nprint(f'There are {n_overlap} Patient IDs in both the training and validation sets')","3e8819ad":"def check_for_leakage(df1, df2, patient_col):\n    \"\"\"\n    Return True if there any patients are in both df1 and df2.\n\n    Args:\n        df1 (dataframe): dataframe describing first dataset\n        df2 (dataframe): dataframe describing second dataset\n        patient_col (str): string name of column with patient IDs\n    \n    Returns:\n        leakage (bool): True if there is leakage, otherwise False\n    \"\"\"\n \n    df1_patients_unique = set(df1[patient_col].unique().tolist())\n    df2_patients_unique = set(df2[patient_col].unique().tolist())\n    \n    patients_in_both_groups = df1_patients_unique.intersection(df2_patients_unique)\n\n    # leakage contains true if there is patient overlap, otherwise false.\n    leakage = len(patients_in_both_groups) >= 1 # boolean (true if there is at least 1 patient in both groups)\n    \n    return leakage","c462662a":"print(\"leakage between train and test: {}\".format(check_for_leakage(train_df, valid_df, 'Patient ID')))","1e689177":"from PIL import Image\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, data_frame, transforms=None):\n        self.data_frame = data_frame\n        self.transforms = transforms\n        self.len = data_frame.shape[0]\n\n    def __len__(self):\n        return self.len\n\n    def __getitem__(self, index):\n        row = self.data_frame.iloc[index]\n        address = row['path']\n        x = Image.open(address).convert('RGB')\n        \n        vec = np.array(row['disease_vec'], dtype=np.float)\n        y = torch.FloatTensor(vec)\n        \n        if self.transforms:\n            x = self.transforms(x)\n        return x, y\n    \n\ntrain_transform = transforms.Compose([ \n    transforms.RandomRotation(20),\n    transforms.RandomResizedCrop(224, scale=(0.63, 1)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])\n\ntest_transform = transforms.Compose([ \n    transforms.Resize(230),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(), \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) ])\n\ntransform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n)\ndsetTrain = CustomDataset(train_df, train_transform) \ndsetVal = CustomDataset(valid_df, test_transform) \ndsetTest = CustomDataset(test_df, test_transform)\n\ntrainloader = torch.utils.data.DataLoader( dataset = dsetTrain, batch_size = 12, shuffle = True, num_workers = 8 )\nvalloader = torch.utils.data.DataLoader( dataset = dsetVal, batch_size = 12, shuffle = False, num_workers = 8 )\ntestloader = torch.utils.data.DataLoader( dataset = dsetTest, batch_size = 100, shuffle = False, num_workers = 8 )\n","b7d2b8d7":"class DensModel(LightningModule):\n\n    def __init__(self):\n        super().__init__()\n               \n        #self.metric = Accuracy()\n        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n        \n        feature_extracting = True\n        self.set_parameter_requires_grad(self.densenet121, feature_extracting)\n\n        num_ftrs = self.densenet121.classifier.in_features\n\n        self.densenet121.classifier = nn.Sequential(\n            nn.Linear(num_ftrs, 14),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.densenet121(x)\n        return x\n\n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam (model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=1e-5)\n        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience = 5, mode = 'min')\n\n        return [optimizer], [scheduler]\n\n    def set_parameter_requires_grad(self, model, feature_extracting):\n        if feature_extracting:\n            for param in model.parameters():\n                param.requires_grad = False\n\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n#         loss = F.binary_cross_entropy(y_hat, y, size_average = True)\n        loss = weighted_loss(y, y_hat, pos_weights, neg_weights, epsilon=1e-7)\n        \n        tensorboard_logs = {'train_loss': loss}\n        return {'loss': loss, 'log': tensorboard_logs}\n\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n        \n        #loss = F.binary_cross_entropy(y_hat, y, size_average = True)\n        loss = weighted_loss(y, y_hat, pos_weights, neg_weights, epsilon=1e-7)\n        #acc = self.metric(y_hat, y)\n    \n        return {'val_loss': loss}\n\n\n    def validation_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        \n        tensorboard_logs = {'val_loss': avg_loss}\n        return {'avg_val_loss': avg_loss, 'log': tensorboard_logs}\n\n\n    def test_step(self, batch, batch_idx):\n        x, y = batch\n        y_hat = self(x)\n\n#         loss = F.binary_cross_entropy(y_hat, y, size_average = True)\n        \n        loss = weighted_loss(y, y_hat, pos_weights, neg_weights, epsilon=1e-7)\n\n        return {'test_loss': loss}\n\n\n    def test_end(self, outputs):\n        avg_loss = torch.stack([x['test_loss'] for x in outputs]).mean()\n        tensorboard_logs = {'test_loss': avg_loss}\n        return {'avg_test_loss': avg_loss, 'log': tensorboard_logs}\n\n\n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n        tensorboard_logs = {'val_loss': avg_loss}\n        return {'val_loss': avg_loss, 'log': tensorboard_logs}\n\n\n","80ebc12e":"model = DensModel()\n\nearly_stopping = EarlyStopping('val_loss')\ncheckpoint_callback = ModelCheckpoint(verbose=False, monitor='avg_val_loss', mode='min')\n\ntrainer = Trainer(gpus=1, max_epochs=5, checkpoint_callback=checkpoint_callback, early_stop_callback=early_stopping)\ntrainer.fit(model, trainloader, valloader)\n\ntrainer.test(model, test_dataloaders=testloader)\n\n\n\n","7d254f67":"def get_roc_curve(labels, predicted_vals, gt_labels):\n    auc_roc_vals = []\n    for i in range(len(labels)):\n        try:\n            gt = gt_labels[:, i]\n            pred = predicted_vals[:, i]\n            auc_roc = roc_auc_score(gt, pred)\n            auc_roc_vals.append(auc_roc)\n            fpr_rf, tpr_rf, _ = roc_curve(gt, pred)\n            plt.figure(1, figsize=(10, 10))\n            plt.plot([0, 1], [0, 1], 'k--')\n            plt.plot(fpr_rf, tpr_rf,\n                     label=labels[i] + \" (\" + str(round(auc_roc, 3)) + \")\")\n            plt.xlabel('False positive rate')\n            plt.ylabel('True positive rate')\n            plt.title('ROC curve')\n            plt.legend(loc='best')\n        except:\n            print(\n                f\"Error in generating ROC curve for {labels[i]}. \"\n                f\"Dataset lacks enough examples.\"\n            )\n    plt.show()\n    return auc_roc_vals","5ce781cd":"# for x, y in testloader:\n    \n#     model.eval()\n#     x = torch.autograd.Variable(x).cuda()\n    \n    \n#     with torch.no_grad():\n#         outputs = model(x)\n#         print(outputs.shape)\n        \n#         gt_labels = y.cpu().numpy()\n#         predicted_vals = outputs.cpu().numpy()\n        \n#         for c_label, s_count in zip(all_labels, 100*np.mean(y.numpy(),0)):\n#             print('%s: %2.2f%%' % (c_label, s_count))\n#         break","3a6ab822":"# from sklearn.metrics import roc_curve, auc\n# test_Y = y.numpy()\n# pred_Y = predicted_vals\n# fig, c_ax = plt.subplots(1,1, figsize = (9, 9))\n\n# for (idx, c_label) in enumerate(all_labels):\n#     fpr, tpr, thresholds = roc_curve(test_Y[:,idx].astype(int), pred_Y[:,idx])\n#     c_ax.plot(fpr, tpr, label = '%s (AUC:%0.2f)'  % (c_label, auc(fpr, tpr)))\n\n# c_ax.legend()\n# c_ax.set_xlabel('False Positive Rate')\n# c_ax.set_ylabel('True Positive Rate')\n# fig.savefig('barely_trained_net.png')\n","25a46014":"# **Preventing Data Leakage**","8ca8a5a1":"# **Creating Dataset**","b7b23844":"# ****Prepare dataset****","1d8f2222":"# Computing Class Frequencies"}}