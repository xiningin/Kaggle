{"cell_type":{"f033d510":"code","6190a036":"code","fa01f0d7":"code","e81f2e7a":"code","c0e85d02":"code","f5fcc54e":"code","66fe4c0d":"code","8588e350":"code","28e67bf3":"code","9ae7c587":"code","8e65e7af":"code","64e97cba":"code","20ca6780":"code","82a54467":"code","535e2327":"code","1f6398fc":"code","be45b726":"code","212e309c":"code","37573f3e":"code","10a77659":"code","a7570cee":"markdown","8e563d57":"markdown","93c230d4":"markdown","019e67d9":"markdown","81afdb53":"markdown","e668d57a":"markdown"},"source":{"f033d510":"import os\nimport gc\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nimport numpy as np\nimport torch.nn.functional as F\nfrom torch.optim import lr_scheduler\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nimport transformers\nimport tokenizers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm.autonotebook import tqdm\nimport utils","6190a036":"apex_on = True\nif apex_on:\n    ! pip install --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ..\/input\/nvidiaapex\/.\nif apex_on:\n    from apex import amp","fa01f0d7":"cdf_threshold = .5","e81f2e7a":"class config:\n    TRAIN_BATCH_SIZE = 48\n    VALID_BATCH_SIZE = 8\n    if apex_on:\n        TRAIN_BATCH_SIZE *= 2\n        VALID_BATCH_SIZE *= 2\n    EPOCHS = 4\n    LR = 1e-4\n    ROBERTA_PATH = \"..\/input\/roberta-base\/\"\n    MODEL_PATH = \"model.pth\"\n    TRAINING_FILE = \"..\/input\/tweet-train-folds\/train_folds.csv\"\n    TOKENIZER = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=f\"..\/input\/roberta-base\/vocab.json\", \n    merges_file=f\"..\/input\/roberta-base\/merges.txt\", \n    lowercase=True,\n    add_prefix_space=True    \n    )","c0e85d02":"def process_data(tweet, selected_text, sentiment, tokenizer):\n    tweet = \" \" + \" \".join(str(tweet).split())\n    selected_text = \" \" + \" \".join(str(selected_text).split())\n\n    len_st = len(selected_text) - 1\n    idx0 = None\n    idx1 = None\n\n    for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n        if \" \" + tweet[ind: ind+len_st] == selected_text:\n            idx0 = ind\n            idx1 = ind + len_st - 1\n            break\n\n    char_targets = [0] * len(tweet)\n    if idx0 != None and idx1 != None:\n        for ct in range(idx0, idx1 + 1):\n            char_targets[ct] = 1\n    \n    tok_tweet = tokenizer.encode(tweet)\n    input_ids_orig = tok_tweet.ids\n    tweet_offsets = tok_tweet.offsets\n    \n    target_idx = []\n    for j, (offset1, offset2) in enumerate(tweet_offsets):\n        if sum(char_targets[offset1: offset2]) > 0:\n            target_idx.append(j)\n    \n    targets_start = target_idx[0]\n    targets_end = target_idx[-1]\n\n    sentiment_id = {\n        'positive': 1313,\n        'negative': 2430,\n        'neutral': 7974\n    }\n    \n    input_ids = [0] + [sentiment_id[sentiment]] + [2] + [2] + input_ids_orig + [2]\n    token_type_ids = [0, 0, 0, 0] + [0] * (len(input_ids_orig) + 1)\n    mask = [1] * len(token_type_ids)\n    tweet_offsets = [(0, 0)] * 4 + tweet_offsets + [(0, 0)]\n    targets_start += 4\n    targets_end += 4\n    return {\n        'ids': input_ids,\n        'mask': mask,\n        'token_type_ids': token_type_ids,\n        'targets_start': targets_start,\n        'targets_end': targets_end,\n        'orig_tweet': tweet,\n        'orig_selected': selected_text,\n        'sentiment': sentiment,\n        'offsets': tweet_offsets\n    }","f5fcc54e":"class TweetDataset:\n    def __init__(self, tweet, sentiment, selected_text):\n        self.tweet = tweet\n        self.sentiment = sentiment\n        self.selected_text = selected_text\n        self.tokenizer = config.TOKENIZER\n    \n    def __len__(self):\n        return len(self.tweet)\n\n    def __getitem__(self, item):\n        data = process_data(\n            self.tweet[item], \n            self.selected_text[item], \n            self.sentiment[item],\n            self.tokenizer\n        )\n        \n        return data","66fe4c0d":"class TweetModel(transformers.RobertaModel):\n    def __init__(self, conf):\n        super(TweetModel, self).__init__(conf)\n        self.roberta = transformers.RobertaModel.from_pretrained(config.ROBERTA_PATH + \"pytorch_model.bin\", config=conf)\n        self.drop_out = nn.Dropout(0.1)\n        self.conv1 = torch.nn.Conv1d(768, 128, 3, stride=1, padding=1, dilation=1, groups=1, bias=True, padding_mode='zeros')\n        self.conv2 = torch.nn.Conv1d(128, 2, 3, stride=1, padding=1, dilation=1, groups=1, bias=True, padding_mode='zeros')\n        self.l0 = nn.Linear(768, 2)\n        torch.nn.init.normal_(self.l0.weight, std=0.02)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, _, out = self.roberta(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids\n        )        \n        out = torch.cat([out[-1][:,:,:, None], out[-2][:,:,:, None]], axis = -1)\n        out, _ = torch.max((out), dim=-1)\n        \n        out = self.drop_out(out)\n        \n        conv1_logits = self.conv1(out.transpose(1, 2))\n        conv2_logits = self.conv2(conv1_logits)\n        logits = conv2_logits.transpose(1, 2)\n        start_logits, end_logits = logits.split(1, dim=-1)\n        start_logits = start_logits.squeeze(-1)\n        end_logits = end_logits.squeeze(-1)\n    \n        return start_logits, end_logits","8588e350":"def loss_fn(start_logits, end_logits, start_positions, end_positions):\n    #span loss\n    loss_fct = nn.CrossEntropyLoss()\n    start_loss = loss_fct(start_logits, start_positions)\n    end_loss = loss_fct(end_logits, end_positions)\n    total_loss =  start_loss + end_loss\n    return total_loss","28e67bf3":"def train_fn(data_loader, model, optimizer, device, scheduler=None):\n    model.train()\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    start_preds = []\n    end_preds = []\n    start_labels = []\n    end_labels = []\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        offsets = d[\"offsets\"]\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        model.zero_grad()\n        outputs_start, outputs_end = model(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids,\n        )\n                \n        loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n        if apex_on:\n            with amp.scale_loss(loss, optimizer) as scaled_loss:\n                scaled_loss.backward()\n        else:\n            loss.backward()\n        optimizer.step()\n        scheduler.step()\n\n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n        start_preds.append(outputs_start)\n        end_preds.append(outputs_end)\n        start_labels.append(targets_start)\n        end_labels.append(targets_end)\n        jaccard_scores = []\n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            jaccard_score, _ = calculate_jaccard_score(\n                original_tweet=tweet,\n                target_string=selected_tweet,\n                sentiment_val=tweet_sentiment,\n                idx_start=np.where(outputs_start[px, :].cumsum(axis = 0) > cdf_threshold)[0].min(),\n                idx_end=np.where(outputs_end[px, :][::-1].cumsum(axis = 0)[::-1] > cdf_threshold)[0].max(),\n                offsets=offsets[px]\n            )\n            jaccard_scores.append(jaccard_score)\n\n        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n        losses.update(loss.item(), ids.size(0))\n        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n    return start_preds, end_preds, start_labels, end_labels","9ae7c587":"def calculate_jaccard_score(\n    original_tweet, \n    target_string, \n    sentiment_val, \n    idx_start, \n    idx_end, \n    offsets,\n    verbose=False):\n    \n    if idx_end < idx_start:\n        idx_end = idx_start\n    if idx_start < 0:\n        idx_start = 0\n    filtered_output  = \"\"\n    \n    for ix in range(idx_start, idx_end + 1):\n        filtered_output += original_tweet[offsets[ix][0]: offsets[ix][1]]\n        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n            filtered_output += \" \"\n    if sentiment_val == \"neutral\" or len(original_tweet.split()) < 2:\n        filtered_output = original_tweet\n\n    jac = utils.jaccard(target_string.strip(), filtered_output.strip())\n    return jac, filtered_output\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    losses = utils.AverageMeter()\n    jaccards = utils.AverageMeter()\n    \n    start_preds = []\n    end_preds = []\n    start_labels = []\n    end_labels = []\n    \n    with torch.no_grad():\n        tk0 = tqdm(data_loader, total=len(data_loader))\n        for bi, d in enumerate(tk0):\n            ids = d[\"ids\"]\n            token_type_ids = d[\"token_type_ids\"]\n            mask = d[\"mask\"]\n            sentiment = d[\"sentiment\"]\n            orig_selected = d[\"orig_selected\"]\n            orig_tweet = d[\"orig_tweet\"]\n            targets_start = d[\"targets_start\"]\n            targets_end = d[\"targets_end\"]\n            offsets = d[\"offsets\"].numpy()\n\n            ids = ids.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets_start = targets_start.to(device, dtype=torch.long)\n            targets_end = targets_end.to(device, dtype=torch.long)\n\n            outputs_start, outputs_end = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            \n            loss = loss_fn(outputs_start, outputs_end, targets_start, targets_end)\n            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n            \n            start_preds.append(outputs_start)\n            end_preds.append(outputs_end)\n            start_labels.append(targets_start)\n            end_labels.append(targets_end)\n            \n            best_score = 0\n            jaccard_scores = []\n            for px, tweet in enumerate(orig_tweet):\n                selected_tweet = orig_selected[px]\n                tweet_sentiment = sentiment[px]\n                jaccard_score, _ = calculate_jaccard_score(\n                    original_tweet=tweet,\n                    target_string=selected_tweet,\n                    sentiment_val=tweet_sentiment,\n                    idx_start=np.where(outputs_start[px, :].cumsum(axis = 0) > cdf_threshold)[0].min(),\n                    idx_end=np.where(outputs_end[px, :][::-1].cumsum(axis = 0)[::-1] > cdf_threshold)[0].max(),\n                    offsets=offsets[px]\n                )\n                jaccard_scores.append(jaccard_score)\n\n            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n            losses.update(loss.item(), ids.size(0))\n            tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg)\n    print(f\"Jaccard = {jaccards.avg}\")\n    return jaccards.avg, start_preds, end_preds, start_labels, end_labels","8e65e7af":"def collate_fn(data):\n    data = {k: [dic[k] for dic in data] for k in data[0]}\n    max_len = 0\n    for i in data[\"ids\"]:\n        str_len = len(i)\n        if str_len > max_len:\n            max_len = str_len\n    for i in range(len(data[\"ids\"])):\n        padding_length = max_len - len(data[\"ids\"][i])\n        if padding_length > 0:\n            data[\"ids\"][i] = data[\"ids\"][i] + ([1] * padding_length)\n            data[\"mask\"][i] = data[\"mask\"][i] + ([0] * padding_length)\n            data[\"token_type_ids\"][i] = data[\"token_type_ids\"][i] + ([0] * padding_length)\n            data[\"offsets\"][i] = data[\"offsets\"][i] + ([(0, 0)] * padding_length)\n    return {\n            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            'orig_tweet': data[\"orig_tweet\"],\n            'orig_selected': data[\"orig_selected\"],\n            'sentiment': data[\"sentiment\"],\n            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n            }","64e97cba":"def run(fold):\n    dfx = pd.read_csv(config.TRAINING_FILE)\n#     dfx['text'] = [clean_html(tweet) for tweet in dfx['text'].values]\n#     dfx['selected_text'] = [clean_html(tweet) for tweet in dfx['selected_text'].values]\n    \n    df_train = dfx[dfx.kfold != fold].reset_index(drop=True)\n    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = TweetDataset(\n        tweet=df_train.text.values,\n        sentiment=df_train.sentiment.values,\n        selected_text=df_train.selected_text.values\n    )\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=config.TRAIN_BATCH_SIZE,\n        num_workers=2,\n        collate_fn = collate_fn\n    )\n\n    valid_dataset = TweetDataset(\n        tweet=df_valid.text.values,\n        sentiment=df_valid.sentiment.values,\n        selected_text=df_valid.selected_text.values\n    )\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=config.VALID_BATCH_SIZE,\n        num_workers=2,\n        collate_fn = collate_fn\n    )\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n    else:\n        device = torch.device(\"cpu\")\n    model_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH + \"config.json\")\n    model_config.output_hidden_states = True\n    model = TweetModel(conf=model_config)\n    model.to(device)\n\n    num_train_steps = int(len(df_train) \/ config.TRAIN_BATCH_SIZE * config.EPOCHS)\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n    ]\n    optimizer = AdamW(optimizer_parameters, lr=config.LR)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, \n        num_warmup_steps=0, \n        num_training_steps=num_train_steps\n    )\n\n    es = utils.EarlyStopping(patience=2, mode=\"max\")\n    print(f\"Training is Starting for fold={fold}\")\n    if apex_on:\n        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n    for epoch in range(config.EPOCHS):\n        if epoch > 1:\n            for name, param in model.named_parameters():                \n                param.requires_grad = True\n        train_start_preds, train_end_preds, train_start_labels, train_end_labels = train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n        jaccard, eval_start_preds, eval_end_preds, eval_start_labels, eval_end_labels = eval_fn(valid_data_loader, model, device)\n        print(f\"Jaccard Score = {jaccard}\")\n        es(jaccard, model, model_path=f\"model_{fold}.pth\")\n        if es.early_stop:\n            print(\"Early stopping\")\n            break\n    del model, optimizer, scheduler, dfx, df_train, df_valid, train_dataset, valid_dataset, train_data_loader, valid_data_loader\n    gc.collect()\n    return train_start_preds, train_end_preds, train_start_labels, train_end_labels, eval_start_preds, eval_end_preds, eval_start_labels, eval_end_labels","20ca6780":"# run(fold=0)\nfor i in range(5):\n    train_start_preds, train_end_preds, train_start_labels, train_end_labels, eval_start_preds, eval_end_preds, eval_start_labels, eval_end_labels = run(fold=i)","82a54467":"import matplotlib.pyplot as plt\nfor i in range(10):\n    plt.plot(train_start_preds[i][0].cumsum(), label='Train Start Hist')\n    plt.scatter(train_start_labels[i].cpu().numpy()[0], [1], alpha=0.5, label='Train Start Label')\n    plt.scatter(train_start_preds[i][0].argmax(), 1, alpha=0.5, label='Train Start Pred')\n    plt.scatter(np.where(train_start_preds[i][0].cumsum(axis = 0) > cdf_threshold)[0].min(), [1], label = \"Hist Start pred\")\n    plt.scatter(np.where(train_end_preds[i][0][::-1].cumsum(axis = 0)[::-1] > cdf_threshold)[0].max(), [1], label = \"Hist End pred\")\n    plt.plot(train_end_preds[i][0][::-1].cumsum()[::-1], label='Train End Hist', alpha=0.5)\n    plt.scatter(train_end_labels[i].cpu().numpy()[0], [1], label='Max End Label', alpha=0.5)\n    plt.scatter(train_end_preds[i][0].argmax(), 1, label='Max End Pred', alpha=0.5)\n    plt.legend()\n    plt.show()","535e2327":"df_test = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/test.csv\")\ndf_test.loc[:, \"selected_text\"] = df_test.text.values","1f6398fc":"device = torch.device(\"cuda\")\nmodel_config = transformers.RobertaConfig.from_pretrained(config.ROBERTA_PATH + \"config.json\")\nmodel_config.output_hidden_states = True","be45b726":"model1 = TweetModel(conf=model_config)\nmodel1.to(device)\nmodel1.load_state_dict(torch.load(\"model_0.pth\"))\nmodel1.eval()\n\nmodel2 = TweetModel(conf=model_config)\nmodel2.to(device)\nmodel2.load_state_dict(torch.load(\"model_1.pth\"))\nmodel2.eval()\n\nmodel3 = TweetModel(conf=model_config)\nmodel3.to(device)\nmodel3.load_state_dict(torch.load(\"model_2.pth\"))\nmodel3.eval()\n\nmodel4 = TweetModel(conf=model_config)\nmodel4.to(device)\nmodel4.load_state_dict(torch.load(\"model_3.pth\"))\nmodel4.eval()\n\nmodel5 = TweetModel(conf=model_config)\nmodel5.to(device)\nmodel5.load_state_dict(torch.load(\"model_4.pth\"))\nmodel5.eval()","212e309c":"final_output = []\n\ntest_dataset = TweetDataset(\n        tweet=df_test.text.values,\n        sentiment=df_test.sentiment.values,\n        selected_text=df_test.selected_text.values\n)\n\ndata_loader = torch.utils.data.DataLoader(\n    test_dataset,\n    shuffle=False,\n    batch_size=config.VALID_BATCH_SIZE,\n    num_workers=1,\n    collate_fn = collate_fn\n)\n\nwith torch.no_grad():\n    tk0 = tqdm(data_loader, total=len(data_loader))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        token_type_ids = d[\"token_type_ids\"]\n        mask = d[\"mask\"]\n        sentiment = d[\"sentiment\"]\n        orig_selected = d[\"orig_selected\"]\n        orig_tweet = d[\"orig_tweet\"]\n        targets_start = d[\"targets_start\"]\n        targets_end = d[\"targets_end\"]\n        offsets = d[\"offsets\"].numpy()\n\n        ids = ids.to(device, dtype=torch.long)\n        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets_start = targets_start.to(device, dtype=torch.long)\n        targets_end = targets_end.to(device, dtype=torch.long)\n\n        outputs_start1, outputs_end1 = model1(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start2, outputs_end2 = model2(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start3, outputs_end3 = model3(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start4, outputs_end4 = model4(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        \n        outputs_start5, outputs_end5 = model5(\n            ids=ids,\n            mask=mask,\n            token_type_ids=token_type_ids\n        )\n        outputs_start = (\n            outputs_start1 \n            + outputs_start2 \n            + outputs_start3 \n            + outputs_start4 \n            + outputs_start5\n        ) \/ 5\n        outputs_end = (\n            outputs_end1 \n            + outputs_end2 \n            + outputs_end3 \n            + outputs_end4 \n            + outputs_end5\n        ) \/ 5\n        outputs_start = outputs_start\n        outputs_end = outputs_end\n        \n        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n    \n        for px, tweet in enumerate(orig_tweet):\n            selected_tweet = orig_selected[px]\n            tweet_sentiment = sentiment[px]\n            _, output_sentence = calculate_jaccard_score(\n                original_tweet=tweet,\n                target_string=selected_tweet,\n                sentiment_val=tweet_sentiment,\n                    idx_start=np.where(outputs_start[px, :].cumsum(axis = 0) > cdf_threshold)[0].min(),\n                    idx_end=np.where(outputs_end[px, :][::-1].cumsum(axis = 0)[::-1] > cdf_threshold)[0].max(),\n                offsets=offsets[px]\n            )\n            final_output.append(output_sentence)","37573f3e":"plt.plot(outputs_start[0].cumsum(), label='Train Start Hist')\nplt.scatter(outputs_start[0].argmax(), 1, alpha=0.5, label='Train Start Pred')\nplt.scatter(np.where(outputs_start[0].cumsum(axis = 0) > cdf_threshold)[0].min(), [1], label = \"hist start pred\")\nplt.scatter(np.where(outputs_end[0][::-1].cumsum(axis = 0)[::-1] > cdf_threshold)[0].max(), [1], label = \"hist end pred\")\nplt.plot(outputs_end[0][::-1].cumsum()[::-1], label='Train End Hist', alpha=0.5)\nplt.scatter(outputs_end[0].argmax(), 1, label='Max End Pred', alpha=0.5)\nplt.legend()\nplt.show()","10a77659":"sample = pd.read_csv(\"..\/input\/tweet-sentiment-extraction\/sample_submission.csv\")\nsample.loc[:, 'selected_text'] = final_output\nsample.to_csv(\"submission.csv\", index=False)","a7570cee":"Doing dynamic padding in collate_fn. When batches are formed they are dynamically padded to the maximum length of the batch. As opposed to the static max length which wastes significant amounts of compute on batches that have samples much shorter than the max length. This can take epoch time significantly down. ","8e563d57":"- Added in Nvidia Apex to allow mixed precision and larger batch size\n- Added dynamic padding so that each batch is not padded to the same length. Batches padded to the max length of the batch. Significantly speeds up training\n- Add histogram\/cdf based span selection instead of max based span selection\n","93c230d4":"# CDF based span selection\nThe current common approach in the public kernels is to look across all of the tokens output by the model and then select the start and end token that has the maximum output probability. I want to propose a slightly alternate approach where we choose the the start and end span tokens based on a cumulative distribution function. Since we can interpret the models loosely as probabilities and we know they must sum to one due to the softmax function used we can make slightly different decisions.\n\nWith the way the kernels are currently configured if we had a model output of [0.15, 0.1, 0.1, 0.2, 0.25, 0.1, 0.1] our model would select the index of the 0.3 token (5th index), but if we look at the accumulation [0.15, 0.25, 0.35, 0.55, 0.8, 0.9, 1.0] we would find that the model is saying that there is a 55% chance that the correct start token is the 4th or lower. This will yield a wider and more conservative span. Using this logic instead of selecting the maximum output we can select based on a minimum threshold. A reasonable one is likely 0.5 because then the model is effectively saying it is more confident that the span starts before that point. We can do this for both the start span and the end span. With the end span we simply reverse the predictions and then unreverse them after accumulating the probabilities of the predictions\n\nIn many cases this selection process will make the same decisions as the max method like when a single token gets a 0.5+ output then it is impossible for any other token to be selected even with the cumulative distribution selection, but it does make a difference in a non-trivial amount of cases. \n","019e67d9":"# Do the evaluation on test data","81afdb53":"Tunable parameter to adjust how aggressive\/conservative span selection is","e668d57a":"A simple visualization showing the CDF and the different choices for the start and end token based on the max and cdf based span selection. X-axis being the index of the tokens and y axis being cumulative output from the model"}}