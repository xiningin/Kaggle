{"cell_type":{"84771c5b":"code","8699dfc8":"code","f27d9907":"code","c893c826":"code","e3adc712":"code","3e05b249":"code","1feb0185":"code","79ec1831":"code","9733bbbf":"code","f1e8ac3a":"code","092e0272":"code","5434a4ec":"markdown","725f1eb3":"markdown","ea3fe0b2":"markdown","cc8e8291":"markdown","c333367f":"markdown","c28722f2":"markdown","18fa2da6":"markdown","bce6754d":"markdown","ebd886c3":"markdown","cbe32956":"markdown","a78aeac4":"markdown"},"source":{"84771c5b":"import matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport pandas as pd","8699dfc8":"nRowsRead = None \ndata = pd.read_csv('\/kaggle\/input\/abalone.csv', delimiter=',', nrows = nRowsRead)\ndata.dataframeName = 'abalone.csv'\nnRow, nCol = data.shape\nprint(f'There are {nRow} rows and {nCol} columns')","f27d9907":"data.isnull().sum()","c893c826":"data = data.sample(n = 1000)","e3adc712":"data['Age'] = data['Rings']+1.5  # l'et\u00e0 corrisponde al numero di anelli +1.5\ndata.drop('Rings', axis = 1, inplace = True)\n","3e05b249":"data.head(10)","1feb0185":"data = pd.get_dummies(data)\n\nranges = (0, 8.5, 14.5, 30) \nclasses_names = ['young', 'adult', 'old']\ndata['Age'] = pd.cut(data['Age'], bins = ranges, labels = classes_names)\n\ndata.head(10)\n\n\n","79ec1831":"from sklearn.model_selection import train_test_split\nfrom sklearn import tree\n\ntrain, test = train_test_split(data, test_size = 0.2)\n\ntrain_X = train.drop('Age', axis = 1)\ntest_X = test.drop('Age', axis = 1)\ntrain_y = train['Age']\ntest_y = test['Age']\n\n","9733bbbf":"depths = [1, 3, 5, 8, 10, 20, 40]\n\nfor d in depths:\n    print(\"\\n\")\n    t = tree.DecisionTreeClassifier(max_depth = d, random_state = 1234) # random_state deve essere sempre uguale per avere modelli uguali\n    t.fit(train_X, train_y)\n    print(\"score = \", round(t.score(test_X, test_y), 4), \" con max_depth = \", d, \" e effettiva profondit\u00e0 = \", t.tree_.max_depth)\n\n\nt = tree.DecisionTreeClassifier(random_state = 1234) # random_state deve essere sempre uguale per avere modelli uguali, max_depth non impostata\nt.fit(train_X, train_y)\nprint(\"score = \", round(t.score(test_X, test_y), 4), \" con profondit\u00e0 = \", t.tree_.max_depth)","f1e8ac3a":"min_samples = [40, 30, 20, 10, 8, 5, 2]\n\nfor m in min_samples:\n    print(\"\\n\")\n    t = tree.DecisionTreeClassifier(min_samples_leaf = m, random_state = 1234) # random_state deve essere sempre uguale per avere modelli uguali\n    t.fit(train_X, train_y)\n    print(\"score = \", round(t.score(test_X, test_y), 4), \" con min_samples_leaf = \", m)\n","092e0272":"def post_prune(tree, index, soglia):\n    if tree.children_left[index] != -1: # bottom-up\n        post_prune(tree, tree.children_left[index], soglia)\n        post_prune(tree, tree.children_right[index], soglia)\n    if tree.value[index].min() < soglia: \n        tree.children_left[index] = -1 # -1 = foglia\n        tree.children_right[index] = -1\n        \nmin_samples = [40, 30, 20, 10, 8, 5, 2] # uso le stesse soglie\nt = tree.DecisionTreeClassifier(random_state = 1234) # random_state deve essere sempre uguale per avere modelli uguali\nt.fit(train_X, train_y)\nprint(\"score prima del pruning = \", round(t.score(test_X, test_y), 4))\n\nfor m in min_samples:\n    print(\"\\n\")\n    post_prune(t.tree_, 0, m)\n    print(\"score dopo pruning = \" , round(t.score(test_X, test_y), 4))\n    t = tree.DecisionTreeClassifier(max_depth = 3, random_state = 1234)\n    t.fit(train_X, train_y)\n\n\n\n\n\n\n","5434a4ec":"Splitto in training e test set","725f1eb3":"In questo caso il pruning sembra rivelarsi efficace nel ridurre l'overfitting causato da un albero di decisione troppo alto e dunque troppo complesso.","ea3fe0b2":"# Decision tree in sklearn - overfitting\nObiettivo di questo notebook \u00e8 vedere come il parametro **max_depth** influisca sull'accuratezza e sul problema dell'overfitting.  Poi verr\u00e0 analizzato il parametro **min_samples_leaf**, che verr\u00e0 confrontato con una tecnica simile che per\u00f2 verr\u00e0 utilizzata nel post pruning e non durante la creazione dell'albero.","cc8e8291":"#### Max_depth\nCreo l'albero di decisione per diversi valori di max_depth e stampo gli score, assieme alla reale profondit\u00e0 dell'albero.","c333367f":"Il modello migliore sembra l'albero che ha profondit\u00e0 massima 3. Aumentando la profondit\u00e0 massima consentita, infatti, lo score fiminuisce e ci\u00f2 \u00e8 sintomo di **overfitting**: l'albero risulta troppo complesso e si adatta troppo ai dati di training, perdendo capacit\u00e0 di predire sul set di test. Si pu\u00f2 notare che anche se viene lasciata piena libert\u00e0 all'algoritmo, l'albero generato non ha una profondit\u00e0 maggiore a quella di quello generato con max_depth = 40 e spesso (dipende dalle osservazioni prese) nemmeno di quello generato con max_depth = 20.","c28722f2":"#### Min_samples_leaf\n**min_samples_leaf** \u00e8 un attributo che viene utilizzato per limitare l'espansione dell'albero durante la sua generazione, proprio come max_depth; in questo caso, il suo valore indica il minimo numero di esempi che un nodo deve etichettare per essere considerato foglia.\n\nIn questo caso utilizzo il modello senza limite di profondit\u00e0 in quanto pi\u00f9 complesso e pi\u00f9 adatto ad essere \"ridotto\".","18fa2da6":"In questo caso \u00e8 difficile capire quanto il parametro influenzi i risultati, non c'\u00e8 un'ascesa o una discesa precisa dello score.","bce6754d":"#### Operazioni preliminari sui dati \n\ndataset in: \/kaggle\/input\/abalone.csv","ebd886c3":"#### Post-pruning\nDi seguito applicher\u00f2 il post pruning per \"tagliare\" i nodi che hanno un numero di esempi minori di una certa soglia, ma in questo caso DOPO aver costruito l'albero.","cbe32956":"#### Preparo i dati per il training\n\nUso One-Hot Encoding per codificare i valori categorici del predictor *Sex* in valori numerici; inoltre, trasformo i valori contiui del target *Age* in 3 categorie: giovane, adulto e vecchio.","a78aeac4":"Prendo casualmente 1000 osservazioni:"}}