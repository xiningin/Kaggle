{"cell_type":{"77487ca1":"code","337dc489":"code","a80938b5":"code","a9659bad":"code","2174468b":"code","e8bc34bd":"code","83e3eb6c":"code","48897d34":"code","69630f03":"code","5e66658e":"code","ff9061f6":"code","70057690":"code","863f9579":"code","85f90a0e":"code","e7f5a127":"code","e984e0f8":"code","a4cd20db":"code","47104d97":"code","2d9d216d":"code","46013bc7":"code","69835fe6":"code","b62287ce":"code","a7e32d1d":"code","46f35c6e":"code","cebb36e0":"code","21e281ab":"code","3fc4ec7b":"code","381876fa":"code","7d929e59":"code","0c95eac4":"code","739e081a":"code","def2c470":"code","9c44576d":"code","66f052b0":"code","ddb44ec4":"code","9c1c45cc":"code","6ae747e1":"code","74b27ecd":"code","633a971f":"code","95807908":"code","93f1255d":"code","289386db":"code","2e426a00":"code","8bad209c":"code","4972e6a2":"code","3ce68406":"code","b1548acf":"code","edf3adaf":"code","9a166f20":"code","242535fd":"code","0e2a2d90":"code","fdd10d0b":"code","42745aeb":"code","839ef48e":"code","b029c0b6":"code","45a98e74":"code","236fd4cc":"code","69a4374f":"code","f37bc735":"code","d3827668":"code","61744d45":"code","a6b001ff":"code","c4c46050":"code","2a3f49a9":"code","4fd06c28":"code","c3123397":"code","3bb90e4c":"code","0546ea44":"code","6fa6c335":"code","f33943a1":"code","0a8e936b":"code","ecae99c1":"code","8e177fdc":"code","c9a019e0":"code","030335ad":"code","bfad2c73":"code","64382d48":"code","45079878":"code","e3e67eea":"code","a67d4eec":"code","845c34ff":"code","8dd96919":"code","da589579":"code","199fa72e":"code","a426ed7f":"code","d70992cf":"code","16c0515d":"code","e3dcaacd":"code","ddfd752e":"code","154c4f35":"code","3174219a":"code","6a385833":"code","58754a99":"code","91649d57":"code","91941e91":"code","ebcb2618":"code","3ccb2509":"code","c710dc00":"code","d5ac435f":"code","653f8c78":"code","1536818f":"code","6325824f":"code","e7892977":"code","1ed31bcf":"code","3283a926":"code","aecf513f":"code","69f1132d":"code","6527e633":"code","2190baac":"code","ea41c11b":"code","a15653e0":"code","0ffc70ae":"code","4b536fbf":"code","fca3ab5e":"code","69bb9aec":"code","8dc83ca9":"markdown","46247145":"markdown","b8383ea0":"markdown","7bb8d6ab":"markdown","127c358e":"markdown","c4d09ffb":"markdown","8160e9af":"markdown","8ba846d9":"markdown","6c295a26":"markdown","ae539d89":"markdown","5c33442a":"markdown","c70736fe":"markdown","0a156cf5":"markdown","0c586505":"markdown","038a1960":"markdown","2049ca66":"markdown","296f72e3":"markdown","2defa02d":"markdown","725d27d3":"markdown","ae95ada3":"markdown","33f3bdf1":"markdown","dc73d924":"markdown","c4e6b51a":"markdown","43a719df":"markdown","5e3b58b4":"markdown","bc88fcd8":"markdown","b3c25538":"markdown","0a1374df":"markdown","9ce2804d":"markdown","a2dcacbe":"markdown","4ffe1336":"markdown","4b824a60":"markdown","2cf4b10a":"markdown","b550df11":"markdown","634535c7":"markdown","b62f99cb":"markdown","ee688372":"markdown","d8608a4c":"markdown","fb4c42ff":"markdown","1d034175":"markdown","529c49cb":"markdown","eff5a135":"markdown","495601e3":"markdown"},"source":{"77487ca1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","337dc489":"import warnings\n\ndef fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()","a80938b5":"df=pd.read_csv(\"..\/input\/loan-approval-prediction\/Training Dataset.csv\")\ndf.head(10)","a9659bad":"df.shape","2174468b":"df.describe()","e8bc34bd":"df.info()","83e3eb6c":"#Counting all unique values of column \"Loan_Status\" from dataset \n\ndf[\"Loan_Status\"].value_counts().plot(kind='bar',color='c')","48897d34":"df.isnull().sum()","69630f03":"df.isnull().sum().plot(kind='bar',color = 'blue')","5e66658e":"df['Dependents']=df['Dependents'].fillna(df['Dependents'].mode()[0])\ndf['Gender']=df['Gender'].fillna(df['Gender'].mode()[0])\ndf['Married']=df['Married'].fillna(df['Married'].mode()[0])\ndf['Self_Employed']=df['Self_Employed'].fillna(df['Self_Employed'].mode()[0])","ff9061f6":"df['Credit_History']=df['Credit_History'].fillna(df['Credit_History'].mean())\ndf['Loan_Amount_Term']=df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mean())\ndf['LoanAmount']=df['LoanAmount'].fillna(df['LoanAmount'].mean())","70057690":"df.isnull().sum().plot(kind='bar',color = 'black')","863f9579":"df = df.drop(columns=['Loan_ID']) ## Dropping Loan ID","85f90a0e":"categorical_columns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area','Credit_History','Loan_Amount_Term']\n\n\nprint(\"Categorical Columns in training dataset based on datatypes {}\".format(categorical_columns))\n\n","e7f5a127":"### Data Visualization libraries\nsns.set_theme(style=\"darkgrid\")\nfig,axes = plt.subplots(4,2,figsize=(12,15))\nfor idx,cat_col in enumerate(categorical_columns):\n    row,col = idx\/\/2,idx%2\n    sns.countplot(x=cat_col,data=df,hue='Loan_Status',ax=axes[row,col],palette=\"Dark2\")\n\nplt.subplots_adjust(hspace=1)","e984e0f8":"numerical_columns = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']\nprint(\"Numerical Columns in training dataset based on datatypes {}\".format(numerical_columns))      ","a4cd20db":"fig,axes = plt.subplots(1,3,figsize=(17,5))\nfor idx,cat_col in enumerate(numerical_columns):\n    sns.boxplot(y=cat_col,data=df,x='Loan_Status',ax=axes[idx])\n\nprint(df[numerical_columns].describe())\nplt.subplots_adjust(hspace=1)","47104d97":"plt.figure(figsize=(7,4)) #7 is the size of the width and 4 is parts.... \nsns.heatmap(df.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()\n","2d9d216d":"dummy_Gender = pd.get_dummies(df['Gender'], prefix = 'Gender')\ndummy_Married = pd.get_dummies(df['Married'], prefix = \"Married\")\ndummy_Education = pd.get_dummies(df['Education'], prefix = \"Education\")\ndummy_Self_Employed = pd.get_dummies(df['Self_Employed'], prefix = \"Selfemployed\")\ndummy_Property_Area = pd.get_dummies(df['Property_Area'], prefix = \"Property\")\ndummy_Dependents = pd.get_dummies(df['Dependents'], prefix = \"Dependents\")\ndummy_Loan_status = pd.get_dummies(df['Loan_Status'], prefix = \"Approve\")","46013bc7":"frames = [df,dummy_Gender,dummy_Married,dummy_Education,dummy_Self_Employed,dummy_Property_Area,dummy_Dependents,dummy_Loan_status]","69835fe6":"df_train = pd.concat(frames, axis = 1)","b62287ce":"df_train.head(10)","a7e32d1d":"df_train.shape","46f35c6e":"df_train = df_train.drop(columns = [ 'Gender', 'Married', 'Dependents', 'Education','Self_Employed', 'Property_Area','Loan_Status','Approve_N'])","cebb36e0":"df_train.columns","21e281ab":"df_train.shape","3fc4ec7b":"df=pd.read_csv(\"..\/input\/loan-approval-prediction\/Test Dataset.csv\")\ndf.head(10)","381876fa":"df.shape","7d929e59":"df.describe()","0c95eac4":"df.info()","739e081a":"df.isnull().sum()","def2c470":"df.isnull().sum().plot(kind='bar',color = 'black')","9c44576d":"df['Gender']=df['Gender'].fillna(df['Gender'].mode()[0])\ndf['Dependents']=df['Dependents'].fillna(df['Dependents'].mode()[0])\ndf['Self_Employed']=df['Self_Employed'].fillna(df['Self_Employed'].mode()[0])\n","66f052b0":"df['LoanAmount']=df['LoanAmount'].fillna(df['LoanAmount'].mean())\ndf['Loan_Amount_Term']=df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mean())\ndf['Credit_History']=df['Credit_History'].fillna(df['Credit_History'].mean())\n","ddb44ec4":"df.isnull().sum().plot(kind='bar',color = 'blue')","9c1c45cc":"df = df.drop(columns=['Loan_ID']) ## Dropping Loan ID","6ae747e1":"df.describe()","74b27ecd":"#Checking the correlation...........................\nplt.figure(figsize=(7,4)) #7 is the size of the width and 4 is parts.... \nsns.heatmap(df.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\nplt.show()","633a971f":"dummy_Gender = pd.get_dummies(df['Gender'], prefix = 'Gender')\ndummy_Married = pd.get_dummies(df['Married'], prefix = \"Married\")\ndummy_Education = pd.get_dummies(df['Education'], prefix = \"Education\")\ndummy_Self_Employed = pd.get_dummies(df['Self_Employed'], prefix = \"Selfemployed\")\ndummy_Property_Area = pd.get_dummies(df['Property_Area'], prefix = \"Property\")\ndummy_Dependents = pd.get_dummies(df['Dependents'], prefix = \"Dependents\")\n\n\n\n\nframes = [df,dummy_Gender,dummy_Married,dummy_Education,dummy_Self_Employed,dummy_Property_Area,dummy_Dependents]\ndf_test = pd.concat(frames, axis = 1)\n\ndf_test.head()","95807908":"df_test = df_test.drop(columns = [ 'Gender', 'Married', 'Dependents', 'Education','Self_Employed', 'Property_Area'])\ndf_test.columns","93f1255d":"df_test.shape","289386db":"df_test.shape","2e426a00":"Final_df=pd.concat([df_train,df_test],axis=0)\nFinal_df.head()","8bad209c":"Final_df.shape","4972e6a2":"Final_df['Approve_Y']","3ce68406":"Final_df['Approve_Y'].value_counts().plot(kind='bar')","b1548acf":"Final_df[\"Approve_Y\"].isna().sum()","edf3adaf":"Train_df=Final_df.iloc[:614,:]\nTest_df=Final_df.iloc[614:,:]","9a166f20":"Train_df.head()","242535fd":"Train_df.shape","0e2a2d90":"Test_df.head()","fdd10d0b":"Test_df.shape","42745aeb":"X_train=Train_df.drop(['Approve_Y'],axis=1)\ny_train=Train_df['Approve_Y']","839ef48e":"X_train.shape","b029c0b6":"y_train.shape","45a98e74":"from sklearn.linear_model import LogisticRegression ,Lasso\nfrom sklearn.ensemble import RandomForestClassifier ,AdaBoostClassifier,BaggingClassifier,ExtraTreesClassifier,GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve ,KFold\nfrom sklearn.metrics import roc_curve,accuracy_score,f1_score,auc,confusion_matrix,roc_auc_score\nfrom xgboost.sklearn import XGBClassifier","236fd4cc":"# =============================================================================\n# Cross validation on differnet set of algorithm!!!\n# =============================================================================\n################################################################\nkfold = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)\n\n\nrs = 15\nclrs = []\n\nclrs.append(AdaBoostClassifier(random_state=rs))\nclrs.append(GradientBoostingClassifier(random_state=rs))\nclrs.append(RandomForestClassifier(random_state=rs))\nclrs.append(LogisticRegression(random_state = rs))\nclrs.append(ExtraTreesClassifier(random_state = rs))\n\n\ncv_results = []\nfor clr in clrs :\n    cv_results.append(cross_val_score(clr, X_train, y_train , scoring = 'accuracy', cv = kfold, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \ncv_df = pd.DataFrame({\"CrossVal_Score_Means\":cv_means,\"CrossValerrors\": cv_std,\"Algo\":[\"RandomForestClassifier\",\"Logistic Regression\",\"AdaBoostClassifier\",\"Gradient Boosting\",'ExtraTreesClassifier']})","69a4374f":"g = sns.barplot(\"CrossVal_Score_Means\",\"Algo\",data = cv_df,orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\nprint(cv_df)","f37bc735":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_train,y_train,test_size=0.2,random_state=4)","d3827668":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 0)\n\ngsGBC.fit(X_train,y_train)\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_,gsGBC.best_params_","61744d45":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [2, 3],\n              \"min_samples_split\": [7, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [True],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\n\nrf_param_grid = { \n    'max_features':['auto'], 'oob_score':[True], 'random_state':[1],\n    \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5], \"min_samples_split\" : [ 4, 10 ], \"n_estimators\": [ 100, 400, 700]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 0)\n\ngsRFC.fit(X_train,y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_, gsRFC.best_params_","a6b001ff":"# Logistic regression Parameters tunning \nLRClr = LogisticRegression()\n\n\n## Search grid for optimal parameters\nLRClr_param_grid = {'penalty':['l2','l1'],'C':[1,10,100],'random_state':[rs]}\n\ngsLRClr=GridSearchCV(LRClr,param_grid = LRClr_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 0)\n\n\n\ngsLRClr.fit(X_train,y_train)\n\nLRClr_best = gsLRClr.best_estimator_\n\n# Best score\ngsLRClr.best_score_, gsLRClr.best_params_","c4c46050":"from sklearn.decomposition import PCA\npca = PCA()\npca.fit_transform(X_train)","2a3f49a9":"#covariance\ncovariance=pca.get_covariance()","4fd06c28":"explained_variance=pca.explained_variance_\nexplained_variance","c3123397":"len(explained_variance)","3bb90e4c":"with plt.style.context('dark_background'):\n    plt.figure(figsize=(24, 12))\n    \n    plt.bar(range(20), explained_variance, alpha=0.5, align='center',label='individual explained variance')\n    plt.ylabel('Explained variance ratio')\n    plt.xlabel('Principal components')\n    plt.legend(loc='best')\n    plt.tight_layout()","0546ea44":"plt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","6fa6c335":"pca=PCA(n_components=18)\npca.fit_transform(X_train)\n","f33943a1":"\nrs = 15\nclrs = []\n\nclrs.append(AdaBoostClassifier(random_state=rs))\nclrs.append(GradientBoostingClassifier(random_state=rs))\nclrs.append(RandomForestClassifier(random_state=rs))\nclrs.append(LogisticRegression(random_state = rs))\nclrs.append(ExtraTreesClassifier(random_state = rs))\n\n\ncv_results = []\nfor clr in clrs :\n    cv_results.append(cross_val_score(clr, X_train, y_train , scoring = 'accuracy', cv = kfold, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \ncv_df = pd.DataFrame({\"CrossVal_Score_Means\":cv_means,\"CrossValerrors\": cv_std,\"Algo\":[\"RandomForestClassifier\",\"Logistic Regression\",\"AdaBoostClassifier\",\"Gradient Boosting\",'ExtraTreesClassifier']})","0a8e936b":"g = sns.barplot(\"CrossVal_Score_Means\",\"Algo\",data = cv_df,orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\nprint(cv_df)","ecae99c1":"#Training dataset....\ndf1=pd.read_csv(\"..\/input\/loan-approval-prediction\/Training Dataset.csv\")\ndf1.head()","8e177fdc":"df1.isnull().sum().plot(kind='bar',color = 'red')","c9a019e0":"#Lets handle its missing values\ndf1['Credit_History']=df1['Credit_History'].fillna(df1['Credit_History'].mean())\ndf1['Loan_Amount_Term']=df1['Loan_Amount_Term'].fillna(df1['Loan_Amount_Term'].mean())\ndf1['LoanAmount']=df1['LoanAmount'].fillna(df1['LoanAmount'].mean())\ndf1['Dependents']=df1['Dependents'].fillna(df1['Dependents'].mode()[0])\ndf1['Gender']=df1['Gender'].fillna(df1['Gender'].mode()[0])\ndf1['Married']=df1['Married'].fillna(df1['Married'].mode()[0])\ndf1['Self_Employed']=df1['Self_Employed'].fillna(df1['Self_Employed'].mode()[0])","030335ad":"df1.isnull().sum().plot(kind='bar',color = 'GREEN')","bfad2c73":"df1 = df1.drop(columns=['Loan_ID']) ## Dropping Loan ID","64382d48":"#Testing dataset....\ndf2=pd.read_csv(\"..\/input\/loan-approval-prediction\/Test Dataset.csv\")\ndf2.head()","45079878":"df2.isnull().sum().plot(kind='bar',color = 'red')","e3e67eea":"df2['Gender']=df2['Gender'].fillna(df2['Gender'].mode()[0])\ndf2['Dependents']=df2['Dependents'].fillna(df2['Dependents'].mode()[0])\ndf2['Self_Employed']=df2['Self_Employed'].fillna(df2['Self_Employed'].mode()[0])\ndf2['LoanAmount']=df2['LoanAmount'].fillna(df2['LoanAmount'].mean())\ndf2['Loan_Amount_Term']=df2['Loan_Amount_Term'].fillna(df2['Loan_Amount_Term'].mean())\ndf2['Credit_History']=df2['Credit_History'].fillna(df2['Credit_History'].mean())","a67d4eec":"df2.isnull().sum().plot(kind='bar',color = 'Green')","845c34ff":"## Dropping Loan ID\ndf2=df2.drop(columns=['Loan_ID']) ","8dd96919":"data=pd.concat([df1,df2],axis=0) ","da589579":"data.head()","199fa72e":"data.columns","a426ed7f":"import scipy.stats as stats\nfrom scipy.stats import chi2_contingency\n\nclass ChiSquare:\n    def __init__(self, dataframe):\n        self.data = dataframe\n        self.p = None #P-Value\n        self.chi2 = None #Chi Test Statistic\n        self.dof = None\n        \n        self.dfObserved = None\n        self.dfExpected = None\n        \n    def _print_chisquare_result(self, colX, alpha):\n        result = \"\"\n        if self.p<alpha:\n            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n        else:\n            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n\n        print(result)\n        \n    def TestIndependence(self,colX,colY, alpha=0.05):\n        X = self.data[colX].astype(str)\n        Y = self.data[colY].astype(str)\n        \n        self.dfObserved = pd.crosstab(Y,X) \n        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n        self.p = p\n        self.chi2 = chi2\n        self.dof = dof \n        \n        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n        \n        self._print_chisquare_result(colX,alpha)\n","d70992cf":"data.columns","16c0515d":"#Initialize ChiSquare Class\ncT = ChiSquare(data)\n\n#Feature Selection\ntestColumns = ['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed',\n       'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount',\n       'Loan_Amount_Term', 'Credit_History', 'Property_Area', 'Loan_Status']\n      \n       \nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY=\"Loan_Status\" )","e3dcaacd":"#Drop insignficant variables\n\ndata.drop(columns=['Gender', 'Married', 'Dependents', 'Education', 'Self_Employed',\n       'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount'],inplace = True)","ddfd752e":"#Encoding Categorical Variables\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()\nfor col in data.columns:\n    data[col] = labelencoder.fit_transform(data[col])\n    \n    #Now one hot encoding\ndata=pd.get_dummies(data, columns=[\"Loan_Amount_Term\",\"Credit_History\",\"Property_Area\"],drop_first=False)","154c4f35":"data.head()","3174219a":"data.columns","6a385833":"#Rearranged the order of the dataframe....\ndata = data[['Loan_Amount_Term_0', 'Loan_Amount_Term_1',\n       'Loan_Amount_Term_2', 'Loan_Amount_Term_3', 'Loan_Amount_Term_4',\n       'Loan_Amount_Term_5', 'Loan_Amount_Term_6', 'Loan_Amount_Term_7',\n       'Loan_Amount_Term_8', 'Loan_Amount_Term_9', 'Loan_Amount_Term_10',\n       'Loan_Amount_Term_11', 'Loan_Amount_Term_12', 'Loan_Amount_Term_13',\n       'Credit_History_0', 'Credit_History_1', 'Credit_History_2',\n       'Credit_History_3', 'Property_Area_0', 'Property_Area_1',\n       'Property_Area_2','Loan_Status']]\ndata.head()","58754a99":"#Separating features and label\nX=data.drop(['Loan_Status'],axis=1)\ny=data['Loan_Status']","91649d57":"rs = 15\nclrs = []\n\nclrs.append(AdaBoostClassifier(random_state=rs))\nclrs.append(GradientBoostingClassifier(random_state=rs))\nclrs.append(RandomForestClassifier(random_state=rs))\nclrs.append(LogisticRegression(random_state = rs))\nclrs.append(ExtraTreesClassifier(random_state = rs))\n\n\ncv_results = []\nfor clr in clrs :\n    cv_results.append(cross_val_score(clr, X_train, y_train , scoring = 'accuracy', cv = kfold, n_jobs=-1))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \ncv_df = pd.DataFrame({\"CrossVal_Score_Means\":cv_means,\"CrossValerrors\": cv_std,\"Algo\":[\"RandomForestClassifier\",\"Logistic Regression\",\"AdaBoostClassifier\",\"Gradient Boosting\",'ExtraTreesClassifier']})","91941e91":"g = sns.barplot(\"CrossVal_Score_Means\",\"Algo\",data = cv_df,orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\nprint(cv_df)","ebcb2618":"#Training dataset....\ndf1=pd.read_csv(\"..\/input\/loan-approval-prediction\/Training Dataset.csv\")\ndf1.head()","3ccb2509":"df1.isnull().sum()","c710dc00":"#Lets handle its missing values\ndf1['Credit_History']=df1['Credit_History'].fillna(df1['Credit_History'].mean())\ndf1['Loan_Amount_Term']=df1['Loan_Amount_Term'].fillna(df1['Loan_Amount_Term'].mean())\ndf1['LoanAmount']=df1['LoanAmount'].fillna(df1['LoanAmount'].mean())\ndf1['Dependents']=df1['Dependents'].fillna(df1['Dependents'].mode()[0])\ndf1['Gender']=df1['Gender'].fillna(df1['Gender'].mode()[0])\ndf1['Married']=df1['Married'].fillna(df1['Married'].mode()[0])\ndf1['Self_Employed']=df1['Self_Employed'].fillna(df1['Self_Employed'].mode()[0])","d5ac435f":"df1.isnull().sum()","653f8c78":"df2=pd.read_csv(\"..\/input\/loan-approval-prediction\/Test Dataset.csv\")\ndf2.head()","1536818f":"df2.isnull().sum()","6325824f":"df2['Gender']=df2['Gender'].fillna(df2['Gender'].mode()[0])\ndf2['Dependents']=df2['Dependents'].fillna(df2['Dependents'].mode()[0])\ndf2['Self_Employed']=df2['Self_Employed'].fillna(df2['Self_Employed'].mode()[0])\ndf2['LoanAmount']=df2['LoanAmount'].fillna(df2['LoanAmount'].mean())\ndf2['Loan_Amount_Term']=df2['Loan_Amount_Term'].fillna(df2['Loan_Amount_Term'].mean())\ndf2['Credit_History']=df2['Credit_History'].fillna(df2['Credit_History'].mean())","e7892977":"df2.isnull().sum()","1ed31bcf":"data=pd.concat([df1,df2],axis=0) ","3283a926":"data.head()","aecf513f":"data.columns","69f1132d":"#Drop insignficant variables\ndata = data.drop(columns = ['Loan_ID', 'Gender', 'Married', 'Dependents', 'Education',\n       'Self_Employed', 'ApplicantIncome', 'CoapplicantIncome', 'LoanAmount'])\n","6527e633":"data.head()","2190baac":"#Encoding Categorical Variables\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder=LabelEncoder()\nfor col in data.columns:\n    data[col] = labelencoder.fit_transform(data[col])\n    \n    #Now one hot encoding\ndata=pd.get_dummies(data, columns=[\"Loan_Amount_Term\",\"Credit_History\",\"Property_Area\"],drop_first=False)","ea41c11b":"data.head()","a15653e0":"#Rearranged the order of the dataframe....\ndata = data[['Loan_Amount_Term_0', 'Loan_Amount_Term_1',\n       'Loan_Amount_Term_2', 'Loan_Amount_Term_3', 'Loan_Amount_Term_4',\n       'Loan_Amount_Term_5', 'Loan_Amount_Term_6', 'Loan_Amount_Term_7',\n       'Loan_Amount_Term_8', 'Loan_Amount_Term_9', 'Loan_Amount_Term_10',\n       'Loan_Amount_Term_11', 'Loan_Amount_Term_12', 'Loan_Amount_Term_13',\n       'Credit_History_0', 'Credit_History_1', 'Credit_History_2',\n       'Credit_History_3', 'Property_Area_0', 'Property_Area_1',\n       'Property_Area_2','Loan_Status']]\ndata.head()","0ffc70ae":"#Separating features and label\nX=data.drop(['Loan_Status'],axis=1)\ny=data['Loan_Status']","4b536fbf":"y.value_counts()","fca3ab5e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=47)","69bb9aec":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= -1, verbose = 0)\n\ngsGBC.fit(X_train,y_train)\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_,gsGBC.best_params_","8dc83ca9":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQZnfoaUoZZCjTnaWT56VrbRwmuFRYDcfLMzQ&usqp=CAU)","46247145":"# *Observations:-*\n\nLooks like missing values are in\n\n\n* Gender=               2.99%\n* Dependents=           2.72%\n* Self_Employed=        6.26%\n* LoanAmount=           1.36%\n* Loan_Amount_Term=     1.63%\n* Credit_History=       7.90%","b8383ea0":"# Dataset contain the following dtypes:-\n\n\n1.**Int64**:- '**ApplicantIncome**'\n\n2.**Float64**:- '**CoapplicantIncome**', '**LoanAmount**', '**Loan_Amount_Term**', '**Credit_History**'.\n\n3.**Object**:- '**Loan_ID**', '**Gender**', '**Married**', '**Dependents**', '**Education**', '**Self_Employed**', '**Property_Area**'.","7bb8d6ab":"# **Observations:-**\n\nFrom the above plot we get:-\n\n1.Loan_Status(Y) = 422(68.72%) \n\n2.Loan_Status(N)=192(31.27%)\n","127c358e":"# *Lets check the target variable....*","c4d09ffb":"# *Lets work on testing dataset..*","8160e9af":"# *Observations:-*\n\nLooks like missing values are in\n\n\n* Gender=               2.1%\n* Married=             0.48%\n* Dependents=           2.44%\n* Self_Employed=        5.21%\n* LoanAmount=           3.58%\n* Loan_Amount_Term=     2.28%\n* Credit_History=       8.14%\n\n","8ba846d9":"# Accuracy seems not to be improving even after implementation of PCA","6c295a26":"# Lets work on Training dataset.......","ae539d89":"# As per above scores *Gradient Boosting* seems to be performing well hence will go with the hypertunning for these classifiers.\n\n\n   **CrossVal_Score_Means**   **CrossValerrors**                     **Algo**\n   \n0        -      0.778409   -     0.037567 - RandomForestClassifier\n\n1          -    0.773603    -    0.034709   -  Logistic Regression\n\n2          -    0.778495   -     0.032646   -   AdaBoostClassifier\n\n3      -        0.785095    -    0.051307    -   Gradient Boosting\n\n4         -     0.742695 -       0.037110   - ExtraTreesClassifier\n\n\n\nLets take **\"Gradient Boosting\"**,**\"RandomForestClassifier\"**,**\"Logistic Regression\"**\n","5c33442a":"# After Hyper parameter tunning...We get\n\n1. Gradient boosting tunning-0.7128172924378635\n\n2. RFC Parameters tunning-0.8127313590692755\n\n3. Logistic regression Parameters tunning-0.8046007403490216","c70736fe":"->We can see that the last 2 components has less amount of variance .Performing PCA by taking 18 components with maximum Variance.","0a156cf5":"Hello! I am Sonali singh and in this kernel we will explore loan approval dataset.\n\nPlease Upvote this kernel if you like content.","0c586505":"# Testing dataset.....","038a1960":"# *Preprocessing Data:-*\n\nInput data needs to be pre-processed before we feed it to model. \nNow its time for:-\n\n**Encoding Categorical Features.**","2049ca66":"# *Lets Handle Missing values* ","296f72e3":"# Observations:-\n\nNow,df_test is quality training input to machine learning algorithm \ud83d\ude01 \ud83d\ude01","2defa02d":"# *Problem statement:-*\n\n\nCompany wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. \n\nThese details are:- Loan_ID,Gender,Married,Dependents,Education,Self_Employed,ApplicantIncome,CoapplicantIncome,LoanAmount,Loan_Amount_Term,Credit_History,Property_Area,Loan_Status.\t \n\n**To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers.**\n\nHere they have provided a partial data set.****","725d27d3":"# *Observation:-*\n\nZero Missing value left.....","ae95ada3":"# *Lets Handle Missing values* ","33f3bdf1":"# *Dataset Description:-*\n\n1.LoanID= Unique Loan ID \n\n2.Gender= Male\/ Female \n\n3.Married= Applicant married (Y\/N) \n\n4.Dependents= Number of dependents \n\n5.Education= Applicant Education (Graduate\/ Under Graduate) \n\n6.SelfEmployed= Self-employed (Y\/N)\n\n7.ApplicantIncome= Applicant income\n\n8.CoapplicantIncome= Coapplicant income\n\n9.LoanAmount= Loan amount in thousands\n\n10.LoanAmountTerm= Term of the loan in months\n\n11.CreditHistory= Credit history meets guidelines \n\n12.PropertyArea= Urban\/ Semi-Urban\/ Rural\n\n13.LoanStatus= (Target) Loan approved (Y\/N)\n\n","dc73d924":"# *Observations:-*\n\n1.We can see there are total 13 columns including target variable, all of them are self explanatory.\n\n2.We also see some missing values, lets take stock of missing columns and what are the possible values for categorical and numerical columns.","c4e6b51a":"# Final dataset......\n\nLets Concatenate the two data set....","43a719df":"# *Preprocessing Data:-*\n\nInput data needs to be pre-processed before we feed it to model. \nNow its time for:-\n\n**Encoding Categorical Features.**","5e3b58b4":"# Second Approach :\n\n'''Hypertunning doesn't seem to be helping much for above case so will perform PCA and see if accuracy improves or not'''....\n\n->Checking the important variables using PCA....\n","bc88fcd8":"# *About Company:-*\n\n\nDream Housing Finance company deals in all home loans.They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.","b3c25538":"\n# *Observations:-*\n*\nNow,df_train is quality training input to machine learning algorithm \ud83d\ude01 \ud83d\ude01","0a1374df":"# *Observation:-*\nFor Numercical Columns, there is no significant relation to Loan approval status.","9ce2804d":"# *Observation:-*\n\nZero Missing value left.....","a2dcacbe":"# to be continued...","4ffe1336":"# *Let's also analyze Categorical Columns:-*","4b824a60":"# *Now lets check the co-relation between variable.*\n","2cf4b10a":"# First Approach :\nFeatures are categorical so I will prefer using tree based classifiers since it select the best feature automatically","b550df11":"# *Observations:-*\n\n**Plots above convey following things about the dataset:**\n\n* Loan Approval Status: About 2\/3rd of applicants have been granted loan.\n\n* Sex: There are more Men than Women (approx. 3x).\n\n* Martial Status: 2\/3rd of the population in the dataset is Marred; Married applicants are more likely to be granted loans.\n\n* Dependents: Majority of the population have zero dependents and are also likely to accepted for loan.\n\n* Education: About 5\/6th of the population is Graduate and graduates have higher propotion of loan approval.\n\n* Employment: 5\/6th of population is not self employed.\n\n* Property Area: More applicants from Semi-urban and also likely to be granted loans.\n\n* Applicant with credit history are far more likely to be accepted.\n\n* Loan Amount Term: Majority of the loans taken are for 360 Months (30 years)","634535c7":"# Third Approach :\nEven though tree based classifiers select features with more information gain automatically, but anyway I will try to reduce features manually by with implementation of chi-square and will see if accuracy improves or not.","b62f99cb":"# *Loan Approval Prediction:-*","ee688372":"# ------------------------------------------------------------------------","d8608a4c":"# *Now, let's also analyze Numerical Columns:-*","fb4c42ff":"# *Observations:-*\n\n1.We can see there are total 13 columns including target variable, all of them are self explanatory.\n\n2.We also see some missing values, lets take stock of missing columns and what are the possible values for categorical and numerical columns.","1d034175":"# *Prediciton and selecting the right algorithm..*","529c49cb":"# Final Ananysis......\n\nOverall AdaBoostClassifier and Gradient Boosting is performing better with few hypertunning.Further I will Use AdaBoostClassifier and Gradient Boosting for predictions with the best set of parameters we got above while hypertunning with grid search....","eff5a135":"# Dataset contain the following dtypes:-\n\n\n1.**Int64**:- '**ApplicantIncome**'\n\n2.**Float64**:- '**CoapplicantIncome**', '**LoanAmount**', '**Loan_Amount_Term**', '**Credit_History**'.\n\n3.**Object**:- '**Loan_ID**', '**Gender**', '**Married**', '**Dependents**', '**Education**', '**Self_Employed**', '**Property_Area**', '**Loan_Status**'.\n\n","495601e3":"# Training dataset...."}}