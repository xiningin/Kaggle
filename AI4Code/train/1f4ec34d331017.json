{"cell_type":{"dd8624e2":"code","88496197":"code","f21030be":"code","2dcce510":"code","7bf4bc42":"code","fc6c2fe3":"code","6d377ead":"code","8539482e":"code","34092c6b":"code","6b95954c":"code","51bd7499":"code","11609d83":"code","86fd3f90":"code","1e2855c5":"code","c4e6d604":"code","dbbc2be8":"code","fbd37670":"code","c4c8a83b":"code","627a8a66":"code","3bf28301":"code","e22bda13":"code","34eee2c4":"code","e34bcb0a":"code","e2922098":"code","46c3addb":"code","a2797d97":"code","b9a11225":"code","fec433ec":"code","3776ced0":"code","f16fd9fa":"code","2d7f1285":"code","5864fe33":"code","d3f3fd4b":"code","a7da8d00":"code","4266cdc7":"code","76f19ce1":"code","c0f796e3":"code","59d8e3a9":"code","1fb7631e":"code","5911d2fa":"code","9dcc2b20":"code","84d9a2f8":"code","387fd671":"code","45664be5":"code","babe45f1":"code","618222bf":"code","1102a5e3":"markdown","bf265526":"markdown","ef22a4a2":"markdown","ff880c65":"markdown","ab6c98fe":"markdown","14318000":"markdown"},"source":{"dd8624e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88496197":"df = pd.read_csv('\/kaggle\/input\/real-time-advertisers-auction\/Dataset.csv')","f21030be":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns","2dcce510":"# df.drop(['order_id' , 'line_item_type_id'], axis = 1, inplace=True)\ndf['date'] = df.date.apply(lambda l: pd.Timestamp(l).value)","7bf4bc42":"df.head()","fc6c2fe3":"#calculating CPM\n#calculating the value that the Advertisers Bid for the month of June\n# CPM(the value which was the winning bid value) = \n#((revenue of the publisher*100)\/revenue_share_percentage)\/measurable_impressions)*1000\n\ndef weird_division(n, d):\n    return n \/ d if d else 0\n\ndf['CPM'] = df.apply(lambda x: weird_division(((x['total_revenue']*100)),x['measurable_impressions'])*1000 , axis=1)\n","6d377ead":"del df['total_revenue']","8539482e":"df.tail()","34092c6b":"# we can remove total impressions as well as that is account the same information as measurable impressions \n# also let us try to see if viewable\/measurable impressions are corellated to revenue or not \n\ndf['View\/measurable'] = df.apply(lambda x: weird_division(x['viewable_impressions'],x['measurable_impressions']) , axis=1)\n","6b95954c":"#remove the outliers \n\n# df = df[df['CPM'].between(df['CPM'].quantile(.05), df['CPM'].quantile(.95))]\n\nprint(df.shape)\nprint(df.CPM.quantile(.95))\ndf = df[df.CPM >= 0]\ndf = df[df.CPM < df.CPM.quantile(.95)]\nprint(df.shape)","51bd7499":"df.reset_index(inplace= True)","11609d83":"df.columns","86fd3f90":"import xgboost as xgb","1e2855c5":"from sklearn.model_selection import train_test_split, cross_val_score, KFold\n\nX_cols = ['total_impressions', 'viewable_impressions', 'measurable_impressions', 'revenue_share_percent',\n          'site_id', 'ad_type_id', 'geo_id', 'device_category_id', 'advertiser_id', 'order_id',\n          'line_item_type_id', 'os_id', 'integration_type_id', 'monetization_channel_id', 'ad_unit_id']\ny_cols =['CPM']\n\nX_train = df.loc[df.date < pd.Timestamp('06-22-2019').value][X_cols]\ny_train = df.loc[df.date < pd.Timestamp('06-22-2019').value][y_cols]\nX_test = df.loc[df.date >= pd.Timestamp('06-22-2019').value][X_cols]\ny_test = df.loc[df.date >= pd.Timestamp('06-22-2019').value][y_cols]\n\nX_train.fillna(0, inplace=True)\nX_test.fillna(0, inplace=True)","c4e6d604":"# xgbr = xgb.XGBRegressor(n_estimators=230, learning_rate=0.06, gamma=0, subsample=0.75,#n_estimators=100 #learning_rate=0.08\n#                            colsample_bytree=0.6, max_depth=9, random_state=1)\n# xgbr.fit(X_train.values,y_train.values)","dbbc2be8":"from sklearn.metrics import mean_squared_error\n# predictions = xgbr.predict(X_test.values)\n# mse = mean_squared_error(y_test, predictions)\n# print(\"MSE: %.2f\" % mse)","fbd37670":"# predictions[predictions<0] = 0\n# predictions_xgb = predictions.copy()\n# mean_squared_error(y_test, predictions)","c4c8a83b":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import TimeSeriesSplit, KFold\n\nn_fold = 5\nfolds = KFold(n_splits=n_fold, shuffle=False, random_state=1)\nprediction_xgb_cv = np.zeros(X_test.shape[0])\n\nxgbr = xgb.XGBRegressor(n_estimators=230, learning_rate=0.06, gamma=0, subsample=0.75,#n_estimators=100 #learning_rate=0.08\n                           colsample_bytree=0.6, max_depth=9, random_state=1)\n\nfor fold_n, (train_index, test_index) in enumerate(folds.split(X_train)):\n    print('Fold:', fold_n)\n    X_traincv, X_validcv = X_train.iloc[train_index], X_train.iloc[test_index]\n    Y_traincv, Y_validcv = y_train.iloc[train_index], y_train.iloc[test_index]\n\n    xgbr.fit(X_traincv.values,Y_traincv.values)\n    \n    y_pred = xgbr.predict(X_test.values)\n    prediction_xgb_cv += y_pred\n    \n    print(mean_squared_error(y_test, y_pred))\n    \nprediction_xgb_cv \/= n_fold\nprint('--------------')\nmean_squared_error(y_test, prediction_xgb_cv)","627a8a66":"prediction_xgb_cv[prediction_xgb_cv<0] = 0\nmean_squared_error(y_test, prediction_xgb_cv)","3bf28301":"train = df.loc[df.date < pd.Timestamp('06-22-2019').value].to_csv('train.csv')\ntest = df.loc[df.date >= pd.Timestamp('06-22-2019').value].to_csv('test.csv')","e22bda13":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()","34eee2c4":"train = h2o.import_file(\"train.csv\")\ntest = h2o.import_file(\"test.csv\")","e34bcb0a":"x = ['total_impressions', 'viewable_impressions', 'measurable_impressions', 'revenue_share_percent',\n          'site_id', 'ad_type_id', 'geo_id', 'device_category_id', 'advertiser_id', 'order_id',\n          'line_item_type_id', 'os_id', 'integration_type_id', 'monetization_channel_id', 'ad_unit_id']\ny = \"CPM\"\n\naml = H2OAutoML(max_models=50, seed=1, stopping_metric='MSE', nfolds = 0)\naml.train(x=x, y=y, training_frame=train)","e2922098":"lb = aml.leaderboard\nlb.head(rows=lb.nrows)","46c3addb":"preds = aml.predict(test)","a2797d97":"mean_squared_error(y_test.values, h2o.as_list(preds))","b9a11225":"predictions_h2o = np.array(h2o.as_list(preds).values).reshape(1, len(preds))[0]\npredictions_h2o[predictions_h2o<0] = 0","fec433ec":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold","3776ced0":"cat_cols = ['measurable_impressions', 'revenue_share_percent',\n          'site_id', 'ad_type_id', 'geo_id', 'device_category_id', 'advertiser_id', 'order_id',\n          'line_item_type_id', 'os_id', 'integration_type_id', 'monetization_channel_id', 'ad_unit_id']\nfeatures = cat_cols + ['total_impressions', 'viewable_impressions',]","f16fd9fa":"# from sklearn.preprocessing import StandardScaler\n\n# train = df.loc[df.date < pd.Timestamp('06-22-2019').value]\n# test = df.loc[df.date >= pd.Timestamp('06-22-2019').value]\n\n# scaler = StandardScaler()\n# scaler.fit(train[features])\n# train[features] = pd.DataFrame(scaler.transform(train[features]), columns= features)\n# test[features] = pd.DataFrame(scaler.transform(test[features]), columns= features)\n\n\n\ntrain = df.loc[df.date < pd.Timestamp('06-22-2019').value]\ntest = df.loc[df.date >= pd.Timestamp('06-22-2019').value]\n\ntrain.fillna(0, inplace=True)\ntest.fillna(0, inplace=True)\n\nfor col in cat_cols:\n    train[col] = train[col].astype('category')\n    test[col] = test[col].astype('category')\n    \ntrain, valid = train_test_split(train, test_size=0.05, shuffle=True, random_state=42)","2d7f1285":"params = {\n    'objective': 'regression',\n    'n_estimators': 500,\n    'learning_rate': 0.05,\n    'num_leaves': 250,\n    'metric': 'mse',\n    \n}","5864fe33":"lgb_train = lgb.Dataset(train[features], train[\"CPM\"])\nlgb_valid = lgb.Dataset(valid[features], valid[\"CPM\"])\n\ngbm = lgb.train(params, lgb_train, 25000, \n    valid_sets=[lgb_train, lgb_valid],\n    early_stopping_rounds=100, verbose_eval=1000)\n","d3f3fd4b":"pred = gbm.predict(test[features])\ny_test = test['CPM']\npred[pred<0]=0\npredictions_lgb = pred.copy()\nmean_squared_error(y_test, pred)","a7da8d00":"# gbmcv = lgb.LGBMRegressor(objective = 'regression',  # gbm \u043d\u0438\u043a\u0430\u043a \u0434\u0430\u043b\u0435\u0435 \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e, \u043d\u043e \u0441\u043c\u043e\u0442\u0440\u044e \u043d\u0430 \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u044c \u0444\u0438\u0447\n#                             max_depth = 9,#\u0431\u044b\u043b\u043e  3\n#                             colsample_bytre = 0.8,\n#                             subsample = 0.8,\n#                             learning_rate = 0.1,\n#                             n_estimators = 300, random_state = 1)\n\n# params = {\n#     'objective': 'regression',\n#     'n_estimators': 500,\n#     'learning_rate': 0.05,\n#     'num_leaves': 250,\n#     'metric': 'mse',\n    \n# }\n\n# n_fold = 10\n# folds = KFold(n_splits=n_fold, shuffle=False, random_state=1)\n\n# X_train = df.loc[df.date < pd.Timestamp('06-22-2019').value][X_cols]\n# y_train = df.loc[df.date < pd.Timestamp('06-22-2019').value][y_cols]\n# X_test = df.loc[df.date >= pd.Timestamp('06-22-2019').value][X_cols]\n# y_test = df.loc[df.date >= pd.Timestamp('06-22-2019').value][y_cols]\n\n# X_train.fillna(0, inplace=True)\n# X_test.fillna(0, inplace=True)\n\n# prediction_lgb_cv = np.zeros(X_test.shape[0])\n\n# for fold_n, (train_index, test_index) in enumerate(folds.split(X_train)):\n#     print('Fold:', fold_n)\n#     X_traincv, X_validcv = X_train.iloc[train_index], X_train.iloc[test_index]\n#     Y_traincv, Y_validcv = y_train.iloc[train_index], y_train.iloc[test_index]\n    \n#     lgb_train = lgb.Dataset(X_traincv, Y_traincv)\n#     lgb_valid = lgb.Dataset(X_validcv, Y_validcv)\n\n#     gbmcv = lgb.train(params, lgb_train, 15000, \n#     valid_sets=[lgb_train, lgb_valid],\n#     early_stopping_rounds=100, verbose_eval=50)\n\n# #     gbmcv.fit(X_traincv, Y_traincv, eval_set=[(X_validcv, Y_validcv)], eval_metric='mse', early_stopping_rounds=25)\n    \n#     y_pred = gbmcv.predict(X_test)\n#     prediction_lgb_cv += y_pred\n    \n#     print(mean_squared_error(y_test, y_pred))\n    \n# prediction_lgb_cv \/= n_fold\n# print('--------------')\n# mean_squared_error(y_test, prediction_lgb_cv)","4266cdc7":"# prediction_lgb_cv[prediction_lgb_cv<0] = 0\n# mean_squared_error(y_test, prediction_lgb_cv)","76f19ce1":"# from sklearn.ensemble import RandomForestRegressor\n# from sklearn.ensemble import ExtraTreesRegressor\nfrom catboost import CatBoostRegressor\n\ntrain = df.loc[df.date < pd.Timestamp('06-22-2019').value]\ntest = df.loc[df.date >= pd.Timestamp('06-22-2019').value]\n\n# rfr_model = RandomForestRegressor(n_estimators=500)\n# etr_model = ExtraTreesRegressor(n_estimators=500)\ncat_model = CatBoostRegressor(iterations=15500, cat_features=cat_cols, verbose=500)","c0f796e3":"# rfr_model.fit(train[features], train[\"CPM\"])\n# etr_model.fit(train[features], train[\"CPM\"])\ncat_model.fit(train[features], train[\"CPM\"])","59d8e3a9":"test = df.loc[df.date >= pd.Timestamp('06-22-2019').value][features]\n# rfr_predict = rfr_model.predict(test)\n# etr_predict = etr_model.predict(test)\npredictions_cat = cat_model.predict(test)\npredictions_cat[predictions_cat<0] = 0\n\n# mean_squared_error(y_test, rfr_predict), mean_squared_error(y_test, etr_predict)\nmean_squared_error(y_test, predictions_cat)","1fb7631e":"# cat_model2 = CatBoostRegressor(iterations=2500,random_seed = 1)\n\n# X_train = df.loc[df.date < pd.Timestamp('06-22-2019').value][X_cols]\n# y_train = df.loc[df.date < pd.Timestamp('06-22-2019').value][y_cols]\n# X_test = df.loc[df.date >= pd.Timestamp('06-22-2019').value][X_cols]\n# y_test = df.loc[df.date >= pd.Timestamp('06-22-2019').value][y_cols]\n\n# prediction_cat_cv = np.zeros(X_test.shape[0])\n\n# for fold_n, (train_index, test_index) in enumerate(folds.split(X_train)):\n#     print('Fold:', fold_n)\n#     X_traincv, X_validcv = X_train.iloc[train_index], X_train.iloc[test_index]\n#     Y_traincv, Y_validcv = y_train.iloc[train_index], y_train.iloc[test_index]\n\n#     cat_model2.fit(X_traincv, Y_traincv)\n    \n#     y_pred = cat_model2.predict(X_test)\n#     prediction_cat_cv += y_pred\n    \n#     print(mean_squared_error(y_test, y_pred))\n    \n# prediction_cat_cv \/= n_fold\n# print('--------------')\n# mean_squared_error(y_test, prediction_cat_cv)","5911d2fa":"# prediction_cat_cv[prediction_cat_cv<0] = 0\n# mean_squared_error(y_test, prediction_cat_cv)","9dcc2b20":"import operator\nres = {}\nfor i in np.arange(0,3,step=0.1):\n    for j in np.arange(0,3,step=0.1):\n        for k in np.arange(0,3,step=0.1):\n            for l in np.arange(0,3,step=0.1):\n                if i!=0 or j !=0 or k!=0 or l!=0:\n                    mse = mean_squared_error(y_test, (i*predictions_h2o+j*predictions_lgb+k*predictions_cat+l*prediction_xgb_cv)\/(i+j+k+l))\n                    res[(i,j,k,l)] = mse","84d9a2f8":"min(res.items(), key=operator.itemgetter(1))[0], res[min(res.items(), key=operator.itemgetter(1))[0]]","387fd671":"i, j, k, l = min(res.items(), key=operator.itemgetter(1))[0]\nmean_squared_error(y_test, (i*predictions_h2o+j*predictions_lgb+k*predictions_cat+l*prediction_xgb_cv)\/(i+j+k+l))","45664be5":"import operator\nres = {}\nfor i in np.arange(0,5,step=0.1):\n    for j in np.arange(0,5,step=0.1):\n        for k in np.arange(0,5,step=0.1):\n            if i!=0 or j !=0 or k!=0:\n                mse = mean_squared_error(y_test, (i*predictions_lgb+j*predictions_cat)\/(i+j+k))\n                res[(i,j,k)] = mse","babe45f1":"min(res.items(), key=operator.itemgetter(1))[0], res[min(res.items(), key=operator.itemgetter(1))[0]]","618222bf":"i, j, k = min(res.items(), key=operator.itemgetter(1))[0]\nmean_squared_error(y_test, (i*predictions_lgb+j*predictions_cat)\/(i+j+k))","1102a5e3":"# Hochu zachet & podarok!","bf265526":"# CAT","ef22a4a2":"# H2O","ff880c65":"# LGB","ab6c98fe":"# XGB","14318000":"# Modeling"}}