{"cell_type":{"e190f216":"code","012da1eb":"code","4b72fdb2":"code","b3886ec2":"code","272658a6":"code","c2ab6435":"code","53a95b66":"code","5415718e":"code","2574b5bd":"code","ae231e69":"code","2a01b416":"code","8b3a28a7":"code","c49b9ce3":"code","7b727fb4":"code","cf779579":"markdown","70045503":"markdown","9d37a2d0":"markdown","d3315430":"markdown","c2047679":"markdown","16245bcf":"markdown","3b9ca1e9":"markdown","88f0ea64":"markdown","244f0111":"markdown","d5b1bfe4":"markdown","079a2650":"markdown","dc43a4a1":"markdown","268bc352":"markdown","12fde1f8":"markdown","8b175e19":"markdown","09361d26":"markdown"},"source":{"e190f216":"import tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Conv2DTranspose, Dropout, LeakyReLU, BatchNormalization, Input, Concatenate, Activation, concatenate\nfrom keras.initializers import RandomNormal\nfrom tensorflow.keras.models import Model, load_model, Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.utils import plot_model\nimport numpy as np\nimport cv2\nimport PIL\nfrom PIL import Image\nimport random\nimport h5py\nimport os\nimport plotly.graph_objects as go\nimport matplotlib.pyplot as plt","012da1eb":"def create_model(image_shape):\n    # Prepare the kernel initializer values\n    weight_init = RandomNormal(stddev=0.02)\n    # Prepare the Input layer\n    net_input = Input((image_shape))\n    # Download mobile net, and use it as the base.\n    mobile_net_base = MobileNetV2(\n        include_top=False,\n        input_shape=image_shape,\n        weights='imagenet'\n    )\n    mobilenet = mobile_net_base(net_input)\n    \n    # Encoder block #\n    # 224x224\n    conv1 = Conv2D(64, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(net_input)\n    conv1 = LeakyReLU(alpha=0.2)(conv1)\n    \n    # 112x112\n    conv2 = Conv2D(128, (3, 3), strides=(1, 1), padding='same', kernel_initializer=weight_init)(conv1)\n    conv2 = LeakyReLU(alpha=0.2)(conv2)\n\n    # 112x112\n    conv3 = Conv2D(128, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv2)\n    conv3 =  Activation('relu')(conv3)\n\n    # 56x56\n    conv4 = Conv2D(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv3)\n    conv4 = Activation('relu')(conv4)\n\n    # 28x28\n    conv4_ = Conv2D(256, (3, 3), strides=(1, 1), padding='same', kernel_initializer=weight_init)(conv4)\n    conv4_ = Activation('relu')(conv4_)\n\n    # 28x28\n    conv5 = Conv2D(512, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv4_)\n    conv5 = Activation('relu')(conv5)\n\n    # 14x14\n    conv5_ = Conv2D(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(conv5)\n    conv5_ = Activation('relu')(conv5_)\n    \n    #7x7\n    # Fusion layer - Connects MobileNet with our encoder\n    conc = concatenate([mobilenet, conv5_])\n    fusion = Conv2D(512, (1, 1), padding='same', kernel_initializer=weight_init)(conc)\n    fusion = Activation('relu')(fusion)\n    \n    # Skip fusion layer\n    skip_fusion = concatenate([fusion, conv5_])\n    \n    # Decoder block #\n    # 7x7\n    decoder = Conv2DTranspose(512, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(skip_fusion)\n    decoder = Activation('relu')(decoder)\n    decoder = Dropout(0.25)(decoder)\n\n    # Skip layer from conv5 (with added dropout)\n    skip_4_drop = Dropout(0.25)(conv5)\n    skip_4 = concatenate([decoder, skip_4_drop])\n    \n    # 14x14\n    decoder = Conv2DTranspose(256, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(skip_4)\n    decoder = Activation('relu')(decoder)\n    decoder = Dropout(0.25)(decoder)\n\n    # Skip layer from conv4_ (with added dropout)\n    skip_3_drop = Dropout(0.25)(conv4_)\n    skip_3 = concatenate([decoder, skip_3_drop])\n    \n    # 28x28\n    decoder = Conv2DTranspose(128, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(skip_3)\n    decoder = Activation('relu')(decoder)\n    decoder = Dropout(0.25)(decoder)\n\n    # 56x56\n    decoder = Conv2DTranspose(64, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(decoder)\n    decoder = Activation('relu')(decoder)\n    decoder = Dropout(0.25)(decoder)\n\n    # 112x112\n    decoder = Conv2DTranspose(64, (3, 3), strides=(1, 1), padding='same', kernel_initializer=weight_init)(decoder)\n    decoder = Activation('relu')(decoder)\n\n    # 112x112\n    decoder = Conv2DTranspose(32, (3, 3), strides=(2, 2), padding='same', kernel_initializer=weight_init)(decoder)\n    decoder = Activation('relu')(decoder)\n    \n    # 224x224\n    # Ooutput layer, with 2 channels (a and b)\n    output_layer = Conv2D(2, (1, 1), activation='tanh')(decoder)\n\n    model = Model(net_input, output_layer)\n    model.compile(Adam(lr=0.0002), loss='mse', metrics=['accuracy'])\n    \n    return model","4b72fdb2":"model = create_model((224, 224, 3))\nplot_model(model, 'model_diagram.png')\nplt.figure(figsize=(160, 60))\nplt.imshow(Image.open('model_diagram.png'))","b3886ec2":"def graph_training_data(epochs, training_data, validation_data, y_label, title):\n\n    fig = go.Figure()\n\n    fig.add_trace(\n        go.Scatter(\n            x=np.arange(1, epochs+1), mode='lines+markers', y=training_data,\n            marker=dict(color=\"mediumpurple\"), name=\"Training\"))\n\n    fig.add_trace(\n        go.Scatter(\n            x=np.arange(1, epochs+1), mode='lines+markers', y=validation_data,\n            marker=dict(color=\"forestgreen\"), name=\"Validation\"))\n\n    fig.update_layout(title_text=title, yaxis_title=y_label,\n                      xaxis_title=\"Epochs\", template=\"plotly_white\")\n    fig.show()","272658a6":"# Get prediction from the model based of the 'L' grayscale image\ndef get_pred(model, image_l):\n    # Repeat the L value to match input shape\n    image_l_R = np.repeat(image_l[..., np.newaxis], 3, -1)\n    image_l_R = image_l_R.reshape((1, 224, 224, 3))\n    # Normalize the input\n    image_l_R = (image_l_R.astype('float32') - 127.5) \/ 127.5\n    # Make prediction\n    prediction = model.predict(image_l_R)\n    # Normalize the output\n    pred = (prediction[0].astype('float32') * 127.5) + 127.5\n    \n    return pred","c2ab6435":"# Combine an 'L' grayscale image with an 'AB' image, and convert to RGB for display or use\ndef get_LAB(image_l, image_ab):\n    image_l = image_l.reshape((224, 224, 1))\n    image_lab = np.concatenate((image_l, image_ab), axis=2)\n    image_lab = image_lab.astype(\"uint8\")\n    image_rgb = cv2.cvtColor(image_lab, cv2.COLOR_LAB2RGB)\n    image_rgb = Image.fromarray(image_rgb)\n    return image_rgb","53a95b66":"# Create some samples of black and white images combined, to show input\/output\ndef create_sample(model, images_gray, amount):\n    path = \"\/kaggle\/working\/\"\n    samples = []\n    for i in range(amount):\n        # Select random images\n        r = random.randint(0, images_gray.shape[0])\n        # Get the model's prediction\n        pred = get_pred(model, images_gray[r])\n        # Combine input and output to LAB image\n        image = get_LAB(images_gray[r], pred)\n        # Get number of images in output folder\n        count = len(os.listdir(path))\n        # Create new combined image and save it\n        new_image = Image.new('RGB', (448, 224))\n        gray_image = Image.fromarray(images_gray[r])\n        new_image.paste(gray_image, (0,0))\n        new_image.paste(image, (224, 0))\n        # Saving the image with the current count of images (to make it unique)\n        # and the index of the image, so that it can be found if needed\n        new_image.save(path + str(count)+('_%i.png' % r))\n        samples.append(new_image)\n    return samples","5415718e":"def train(model, gray, ab, epochs, batch_size):\n    # Setup the training input data (grayscale images)\n    train_in = gray\n    # Convert the shape from (224, 224, 1) to (224, 224, 3) by copying the value to match MobileNet's requirements\n    train_in = np.repeat(train_in[..., np.newaxis], 3, -1)\n    \n    train_out = ab\n    # Normalize the data\n    train_in = (train_in.astype('float32') - 127.5) \/ 127.5\n    train_out = (train_out.astype('float32') - 127.5) \/ 127.5\n\n    history = model.fit(\n        train_in,\n        train_out,\n        epochs=epochs,\n        validation_split=0.05,\n        batch_size=batch_size\n    )\n    \n    return history","2574b5bd":"images_gray = np.load(\"..\/input\/image-colorization\/l\/gray_scale.npy\")\nimages_ab = np.load(\"..\/input\/image-colorization\/ab\/ab\/ab1.npy\")\n\n# Set batch size and epochs for training run\nBATCH_SIZE = 32\nEPOCHS = 30\n\n# Create the model through the function above\nmodel = create_model((224, 224, 3))\n\n# Train the model and keep history for graphing\nhistory = train(model, images_gray[:3000], images_ab[:3000], EPOCHS, BATCH_SIZE)","ae231e69":"graph_training_data(EPOCHS, history.history['loss'], history.history['val_loss'], 'Loss', \"Loss while training\")\ngraph_training_data(EPOCHS, history.history['accuracy'], history.history['val_accuracy'], 'Accuracy', \"Accuracy while training\")","2a01b416":"# Create 10 sample images. These images are both returned in a list, and saved.\nsamples = create_sample(model, images_gray, 5)\nfor image in samples:\n    plt.figure()\n    plt.imshow(np.array(image))","8b3a28a7":"path = '..\/input\/imagecolorization-samplesmodel\/ImageColorization_Samples'\nsuccess = os.listdir(path + '\/Successful')\nfor img in success:\n    plt.figure()\n    plt.imshow(Image.open(path +'\/Successful\/' + img))","c49b9ce3":"path = '..\/input\/imagecolorization-samplesmodel\/ImageColorization_Samples'\nsuccess = os.listdir(path + '\/Failed')\nfor img in success:\n    plt.figure()\n    plt.imshow(Image.open(path +'\/Failed\/' + img))","7b727fb4":"path = '..\/input\/imagecolorization-samplesmodel\/'\nimages = os.listdir(path)\nfor img in images:\n    if (img == 'ImageColorization_Samples'): continue\n    plt.figure()\n    plt.imshow(Image.open(path + img))","cf779579":"# Further Improvements\n\n**Reimplementing BatchNormalization**\n\n>BatchNormalization was removed in the final version of the model, this greatly improved the training. Though adding it back into strategic places could improve the training optimization, and could lead to better results. More experimentation with BatchNormalization is needed.\n\n\n\n**Improved Training**\n\n>Although the model trained outside of Kaggle saw many more images than the example given above, it had uneven training in regards to which part of the dataset it was trained on, and only 20,000 out of the 25,000 images were used. The images were handled 'manually' with slicing of the array before training sessions. A better way of managing the images, allowing the net to train on the entire dataset should greatly improve the model's results, perhaps a method using tf.keras.ImageDataGenerator would be successful.\n\n\n\n**Better Fusion Layer**\n\n>The 'fusion layer' in this case is a simple concatenate layer between mobilenet and the output of the encoder. Using a more specific and considered approach may yield to better results. An example of this is done in the [Koalarization paper](https:\/\/arxiv.org\/pdf\/1712.03400.pdf).\n\n\n\n**Improved Normalization**\n\n>It was difficult to find the exact details of how the data is represented in the dataset, and led to some confusion. In this example, and the values had 127.5 subtracted, and then were divided by 127.5. In RGB, this would lead to an interval of [-1, 1], though it is not the case here and leads to a slightly different interval for the AB values. A full understanding of this and a better normalization function could lead to better results.\n\n","70045503":"# Utility functions","9d37a2d0":"# Historical Photographs\n\nHere are samples from using the model on historical photographs.","d3315430":"# Imports","c2047679":"# Creating Samples\n\nHere are some images generated from the model through Kaggle, by the code shown above.","16245bcf":"# Graphing the training data\n","3b9ca1e9":"# Samples from better training\n\nHere are some samples given by the same model but trained with more images from the dataset (using both ab1 and ab2), and trained for much longer.\nThe model was trained for around 10 hours over time, and using varying parts of the image dataset.","88f0ea64":"# Training function\n\nTraining is kept simple in this example, as it only uses ab1.\nThe model could be greatly improved from this example by managing the training to include ab2 and ab3. ","244f0111":"# Final Thoughts\n\nIf you have made it this far, thank you for reading, hope it was helpful\n\nI am still a beginner with neural nets and CNNs in particular, and this is mainly for personal experimentations.\nI would appreciate any feedback or criticisms, particularly if I am making important mistakes that could easily be avoided.\n\nIf you have any questions, comments, or suggestions, do not hesitate to comment them below.","d5b1bfe4":"# Model Diagram\n","079a2650":"# Understanding the samples\n\nAs you can see from the images above, the model is not very accurate at this point.\nThe model mostly generates brown images, as brown represents the average color and thus is the first step in training.\n\nIt should be noted, however, that the model begins to notice the difference between objects, and may assign different colors to different parts of the image. This is a good sign that more training should prove successful, as the model is begining to differentiate the objects and assign them different colors.","dc43a4a1":"# Poor Results\n\nWhile the model provides some good images, it still fails to colorize some images. The ratio seems to be about 60-40, where 60% of the images look 'somewhat realistic,' and 40% would be considered unrealistic.\n\nAs can be seen in the images below, the model failed to create a realistic looking image for these inputs. While some colors have been added, the image is not complete.\n","268bc352":"# Image Colorization\n\nThe purpose of this notebook is to attempt to create an Image Colorization convolutional neural network.\nThe objective is to create quality colorization, while learning along the way the variables which affect the results the most\nand trying to optimize the net to the best of my ability.\n\n\nI am still just a beginner with neural nets, but will try to create a starting point for image colorization, and provide an explanation to my results and decisions throughout the process of attempting Image Colorization.\n\nThis example will only train for 30 epochs and show results for that. Some images from training the same model for much longer (around 10 hours) will be shown as well, to examplify what the model can achieve. The model trained for 10 hours was also trained on many more images than the model shown in this example.\n\nEven after more training on a larger dataset, the results vary from image to image, some good and poor results will be shown to give a better idea of the net at that point. ","12fde1f8":"# Main - Training the model\n\nIn this example the model is only trained using the first 3,000 images. It is recommended to use as many images as possible from the dataset to train the model. A script to manage that and RAM usage would be recommended. \n\nThis example will only run on a small amount of data for few epochs, to facilitate the run in Kaggle, and simplify this notebook.","8b175e19":"# Results\n\nA steady increase in Accuracy and a steady decrease in loss can be seen in the graphs above. This indicates the model is learning steadily.\nThe validation accuracy should also increase, but may lag behind as the model has to learn to generalize the dataset. In this case it might not be since the model is overfitting due to the lack of training data, however, increasing the data the model sees should solve that issue.\n\nWhile an increase in validation accuracy \/ decrease in loss is important, the model can have unstable values throughout the training while still providing successful images, success as determined by looking at the quality of the images manually.\n\nFurthermore since the images differ greatly, there has been better results from a model with lower validation accuracy than one with higher accuracy, though it is correlated to the quality of the images, it should be taken as general feedback, not literally.","09361d26":"# Creating the model\n\nOriginally the model was a MobileNet pretrained model as an encoder, with a Conv2DTranspose based decoder attached at the end. While having a couple successful images, the model mostly produced brown images, as it was the simplest solution to provide the average color throughout the image. The model did not seem to move past this local minimum despite more training. This appeared as a sudden drop in Loss overtime, followed by a 'flatline' showing little improvement.\n\nAfter further research, this [research paper](https:\/\/arxiv.org\/abs\/1712.03400) was found which used a combination of a pretrained model (Inception-Resnet-v2) and a CNN encoder-decoder.\nThe model shown in the paper seemed more successful, and it was decided to follow their concept and combined MobileNetV2 with a Convolutional encoder and decoder.\nAlthough Inception-resnet-v2 has greater accuracy than mobilenet on ImageNet, it also has many more parameters, so mobilenet was chosen, as the experiment is limited by GPU memory.\n\nFurther changes included using less conv2d layers, to limit my GPU usage. Conv2DTranpose layers were also use in place of Conv2D layers with UpSampling.\n\nSkip layers were also added, since they have been found useful in Generative Adversial Networks, as demonstratred by [this paper](http:\/\/https:\/\/arxiv.org\/pdf\/1901.08954.pdf) where they claimed that \"the findings indicate that skip connections provide more stable training.\"\nDropout was also added to each skip layer, in an attempt to reduce the model's dependence on them, and prevent overfitting, hoping to encourage the network to rely more on mobilenet.\n\nThe model at this point had BatchNormalization between every layer, but despite increased training, continued to produce mainly brown images. Removing batchnormalization between the layers appeared to stabilize training, though adding some to few blocks might still improve the model.\n\nIn regards to the loss function, this model uses MSE with a tanh activation. There have been other examples online that went with a softmax activation and categorical crossentropy, but in this case it appeared to be unstable in training and showed little improvements in results.\n\nBelow is the code for the final model, which is also shown as a diagram further down.\n"}}