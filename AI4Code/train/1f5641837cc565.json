{"cell_type":{"3131c760":"code","070d57cf":"code","518c2337":"code","53228a5f":"code","cf54423b":"code","d94ea8a7":"code","3bd4503e":"code","f705a966":"code","cb9e414b":"code","d843d19b":"code","8b8af187":"code","b2d56e03":"code","5e8b8cf0":"code","fee105d0":"code","4089c5ac":"code","17f40afe":"code","30583eae":"code","07e46541":"code","b78cfc15":"code","11d246cb":"code","a7a99f36":"code","130b9f9e":"code","9711b07a":"code","4263fa95":"code","1cef84c8":"code","e4dbb94a":"code","b80b826c":"code","20f70c30":"code","543c3173":"code","a9338f4a":"code","ec52c474":"code","6254291d":"code","767d6110":"code","880039b0":"code","68eb42fd":"code","e7f96501":"markdown","d3d6711e":"markdown","bb686e42":"markdown","6b5320a4":"markdown","8439de63":"markdown","6173f004":"markdown","84837330":"markdown","c4abf588":"markdown","9694dc14":"markdown","e0bf9083":"markdown","5ad772b0":"markdown","4aa9b069":"markdown","7cdc63e8":"markdown","a06a123b":"markdown","69dddc24":"markdown","90871cf8":"markdown","47f1b0d2":"markdown","379a3432":"markdown"},"source":{"3131c760":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\nprint('setup complete')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","070d57cf":"from sklearn.datasets import load_boston\nboston=load_boston()","518c2337":"df=pd.DataFrame(boston.data)","53228a5f":"df.head()","cf54423b":"\ndf.columns = boston.feature_names\ndf.head()","d94ea8a7":"#adding target variable to dataframe\ndf['PRICE']=boston.target\n","3bd4503e":"df.head()","f705a966":"df.describe()","cb9e414b":"df.shape","d843d19b":"df.info()","8b8af187":"df.isnull().sum().sum()","b2d56e03":"\ndf[df.isnull().any(axis=1)]","5e8b8cf0":"df.columns\n","fee105d0":"sns.pairplot(df)","4089c5ac":"sns.distplot(df['PRICE'])","17f40afe":"fig, ax = plt.subplots(figsize=(12,12)) \nsns.heatmap(df.corr(),annot=True,cmap=\"YlGnBu\")\n\n","30583eae":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nX = df.drop(['PRICE'], axis=1)\ny = df['PRICE']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=101)\n\nlm_model= LinearRegression() #create lr object (model)\nlm_model.fit(X_train,y_train) #train the model","07e46541":"print(lm_model.intercept_)\n","b78cfc15":"predictions=lm_model.predict(X_train) \n","11d246cb":"lm_model.coef_","a7a99f36":"cdf=pd.DataFrame(lm_model.coef_, X_train.columns, columns=['Coeff'])\ncdf","130b9f9e":"metrics.mean_absolute_error(y_train, predictions)","9711b07a":"metrics.mean_squared_error(y_train, predictions)","4263fa95":"np.sqrt(metrics.mean_squared_error(y_train, predictions))","1cef84c8":"print('MSE', metrics.mean_squared_error(y_train, predictions))\nprint('MAE', metrics.mean_absolute_error(y_train, predictions))\nprint('RMSE', np.sqrt(metrics.mean_squared_error(y_train, predictions)))","e4dbb94a":"#visualizating actual prices vs predicted values\nplt.scatter(y_train, predictions)\nplt.xlabel('Prices')\nplt.ylabel('Predicted Prices')\nplt.title('Actual vs Predicted Prices')\nplt.show()\n\n\n","b80b826c":"# Checking residuals\nplt.scatter(predictions,y_train-predictions)\nplt.title(\"Predicted vs Residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()\n\n#values clustered around zero.  No pattern visible\n#equally distributed residuals indicates the model was correct for your data","20f70c30":"sns.distplot(y_train-predictions)\n#histogram of our residuals\n#difference between the actual values (y_test) and predicted values (X_test)\n#equally distributed residuals indicates the model was correct for your data\n#if it is not equally distrubted look back and consider choosing a different model\n#checking normality of errors","543c3173":"testpredictions=lm_model.predict(X_test)\n\nplt.scatter(y_test, testpredictions)\n#y_test has the correct prices of the house\n#X_test are the predictions","a9338f4a":"sns.distplot((y_test-testpredictions))","ec52c474":"y_test\n#contains the correct prices of the house\n#how far off are the predictions(X_test)from the actual prices (y_test)?","6254291d":"print('MSE', metrics.mean_squared_error(y_test, testpredictions))\nprint('MAE', metrics.mean_absolute_error(y_test, testpredictions))\nprint('RMSE', np.sqrt(metrics.mean_squared_error(y_test, testpredictions)))","767d6110":"X.columns","880039b0":"lm_model.coef_","68eb42fd":"cdf=pd.DataFrame(lm_model.coef_, X_train.columns, columns=['Coeff'])\n\ncdf","e7f96501":"14 columns and 506 rows","d3d6711e":"Adding the feature names to the dataframe","bb686e42":"# EXPLORATORY DATA ANALYSIS","6b5320a4":"scatterplot looks similar to training data scatterplot","8439de63":"Coefficiant means if you increase the Avg. Area Income by 1 unit the coef will increase by numeric value\nLet's make a variable cdf = coefficient dataframe","6173f004":"# TRAINING THE MODEL","84837330":"Mean Absolute Error (MAE) is the mean of the absolute value of the errors. Measures the difference between 2 constant variables. Is the easiest to understand, because it's the average error. Here actual and predicted values of y.\n\nMean Squared Error (MSE) is the mean of the squared errors:MSE is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world. \n\nRoot Mean Squared Error (RMSE) is the square root of the mean of the squared errors:RMSE** is even more popular than MSE, because RMSE is interpretable in the y units. \n\nThese are loss functions because you want to minimize them to create the best model","c4abf588":"Generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution, excluding NaN values","9694dc14":"From correlation matrix, we see RM has the highest collerelation. The column has a correlation score above 0.5 with PRICE which is a good indication of using as a predictors. ","e0bf9083":"# EVALUATING THE MODEL\n**CALCULATING ERRORS**","5ad772b0":"# LINEAR REGRESSSION\n\nRegression models describe the relationship between variables by fitting a line to the observed data.\n\nSimple linear regression is used to estimate the relationship between two quantitative variables. You can use simple linear regression when you want to know:\n\nHow strong the relationship is between two variables (x and y)\nOne variable, x, is known as the predictor variable. The other variable, y, is known as the response variable (e.g. the relationship between rainfall and soil erosion).\n\nThe value of the dependent variable at a certain value of the independent variable (e.g. the amount of soil erosion at a certain level of rainfall).\n\nUsing linear regression, we can find the line that best \u201cfits\u201d our data. This line is known as the least squares regression line and it can be used to help us understand the relationships between weight and height.\n\n\u0177 = b0 + b1x\n\n![image.png](attachment:image.png)\n\n","4aa9b069":"Good. No null values","7cdc63e8":"Let's load the dataset from sklearn","a06a123b":"# REGRESSION EVALUATION METRICS\n***MODEL EVALUATION ON TRAINING DATA**","69dddc24":"### Columns\n* CRIM per capita crime rate by town\n* ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS proportion of non-retail business acres per town\n* CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n* NOX nitric oxides concentration (parts per 10 million)\n* RM average number of rooms per dwelling\n* AGE proportion of owner-occupied units built prior to 1940\n* DIS weighted distances to five Boston employment centres\n* RAD index of accessibility to radial highways\n* TAX full-value property-tax rate per 10,000usd\n* PTRATIO pupil-teacher ratio by town\n* B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n* LSTAT % lower status of the population\n","90871cf8":"# PREDICTING THE MODEL","47f1b0d2":"# SPLITTING THE DATA","379a3432":"* Adding target variable [Price] to dataframe. This is what we will try to predict\n* Owner occupied homes in the $1000s\n* Boston housing prices in the 1970's"}}