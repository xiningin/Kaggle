{"cell_type":{"c54ea837":"code","9d8959b2":"code","aceadb9e":"code","a0bde00f":"code","f71e7e19":"code","c0e0cf03":"code","aa852c14":"code","e3323d02":"code","9c58254f":"code","2c40c00a":"code","d05f0ac3":"code","72588ccc":"code","30ad159e":"code","2f6f6e14":"code","41123ef4":"code","395aa79b":"code","201440c9":"code","4684ab79":"code","3945ed10":"code","c322d920":"code","5f9ebdde":"code","6b959f4c":"code","33037595":"code","caf96006":"code","7f3e53c5":"code","26e89a63":"code","ddf0898d":"code","ec8b727d":"code","828f60bd":"code","6f1ed9dc":"code","e8e96306":"code","29715c5d":"code","1b89a100":"markdown","1be1f43f":"markdown","44bdd047":"markdown","e65dce49":"markdown","d642dd1f":"markdown","0e01077f":"markdown","73065697":"markdown","bfe1fada":"markdown","1b3dad69":"markdown","1ceaf2dd":"markdown","9a3cedc4":"markdown","a3077199":"markdown","99fab4fe":"markdown","ddebbef0":"markdown","4b258cd1":"markdown","f363d531":"markdown","febf681b":"markdown","3e73cc36":"markdown"},"source":{"c54ea837":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9d8959b2":"from sklearn.utils import shuffle\nimport re\nimport string\nimport nltk\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.metrics import plot_confusion_matrix,precision_score,recall_score\nfrom sklearn.model_selection import cross_val_score,train_test_split\n","aceadb9e":"fake=pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\")\ntrue=pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\")","a0bde00f":"#making a news target column\nfake['target']=1\ntrue['target']=0","f71e7e19":"Combine=pd.concat([fake,true])\n#shuffling the data \ndf=shuffle(Combine)\ndf=df.reset_index()","c0e0cf03":"print(f\"shape of dataframe:{df.shape}\")","aa852c14":"df.isna().sum()","e3323d02":"df.info()","9c58254f":"df.head(10)","2c40c00a":"def clean_text(text):\n    text=text.lower()\n    text=re.sub(f\"[{string.punctuation}]\",\" \",text)\n    text=re.sub(\"\\[.*?\\]\",\" \",text)   \n    text=re.sub(\"\\w\\d\\w\",\" \",text)\n    text=re.sub(\"\\(.*?\\)\",\" \",text)\n    text=re.sub(\"[\\\"\\\"]\",\" \",text) \n    text=re.sub(' t ',' ',text)\n    text=re.sub(' s ',' ',text)\n    text=re.sub(\"[^a-zA-Z]\",\" \",text)\n    return text","d05f0ac3":"#applying the process\ndf[\"text\"]=df['text'].apply(lambda x:clean_text(x))\ndf['title']=df['title'].apply(lambda x:clean_text(x))","72588ccc":"df['title'][443:579]","30ad159e":"X=df['title']\nY=df['target']\n","2f6f6e14":"x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.1)","41123ef4":"Count_vec=CountVectorizer(stop_words='english',max_features=60)","395aa79b":"X_Count=Count_vec.fit_transform(x_train).toarray()","201440c9":"Count_vec.get_feature_names()[:12]","4684ab79":"X_Count.shape","3945ed10":"Count_df=pd.DataFrame(X_Count,columns=Count_vec.get_feature_names())","c322d920":"Count_df.head(10)","5f9ebdde":"naive_bayes_Countvec=MultinomialNB()\nscores_cout=cross_val_score(naive_bayes_Countvec,X_Count,y_train,cv=5,scoring='f1')","6b959f4c":"scores_cout","33037595":"naive_bayes_Countvec.fit(X_Count,y_train)","caf96006":"naive_bayes_tfidf=MultinomialNB()\ntfidf_vec=TfidfVectorizer(stop_words='english')\nX_tf=tfidf_vec.fit_transform(x_train)\nscores_tf=cross_val_score(naive_bayes_tfidf,X_tf,y_train,cv=5,scoring='f1')","7f3e53c5":"scores_tf","26e89a63":"naive_bayes_tfidf.fit(X_tf,y_train)","ddf0898d":"test_count=Count_vec.transform(x_test)\nplot_confusion_matrix(naive_bayes_Countvec,test_count,y_test)\n","ec8b727d":"y_pred_count=naive_bayes_Countvec.predict(test_count)","828f60bd":"print(\"Naive_Bayes_Model with Countvectorizer : \\n\")\nprint(f\"precision:{precision_score(y_test,y_pred_count)} \\n recall:{recall_score(y_test,y_pred_count)}\")","6f1ed9dc":"test_tf=tfidf_vec.transform(x_test)\nplot_confusion_matrix(naive_bayes_tfidf,test_tf,y_test)","e8e96306":"y_pred_tf=naive_bayes_tfidf.predict(test_tf)\n","29715c5d":"print(\"Naive_Bayes_model with Tfidf: \\n\")\nprint(f\"precsion:{precision_score(y_test,y_pred_tf)} \\n recall:{recall_score(y_test,y_pred_tf)}\")","1b89a100":"## Reading Data ","1be1f43f":"## Preprocessing  ","44bdd047":"Since i didn't remove the stopwords at the preprocessing step so when using countvectorizer i mention the stopwords parameter in countvectorizer which when creating features didn't consider stopwords. ","e65dce49":"Take a look at the some random rows of \"title\" after cleaning","d642dd1f":"## Imports ","0e01077f":"So our precision  and recall  gets improved when using tf-idfvectorizer.In other words the tendency of model to correctly classsify the class of news gets improved.","73065697":"**Checking for Nans**","bfe1fada":"So with tf-idf vectorizer i get much better results.Let's fit the model.","1b3dad69":"In this part we will preprocess the data to make it more efficient to consumed by our model, We will standardised the text into lowercase ,remove all the text inbetween  square brackets, remove all the words tha contain digits or punctuation in between.When we remove the \ninverted comma from word \"don't\" then we are left with \"don t\" so i should remove the \" t \" from this also.","1ceaf2dd":"If you like this kernel please show your support by upvoting this kernel. As your upvotes encourage me do more on Kaggle.If you find any mistake or any suggestion to make this kernel better  let me know in comments below.   ","9a3cedc4":"## Model Building","a3077199":"Fake news is very serious issue in today's society.Many social sites not fact checking the news it's very easy for anyone to spread rumours ,hoaxes  or something derogatory remarks again anybody without any proper proof.When identifying a source of information, one must look at many attributes, including but not limited to the content of the email and social media engagements. specifically, the language is typically more inflammatory in fake news than real.\n","99fab4fe":" Reserving a small part of data to check the performance  of my model","ddebbef0":"Above the numbers at diagonal shows the correctly classified data.So our model correctly  classified  1243 true news as true and 2131 false as false and 293 false news as true and 823 true news as false.This is when i use Countvectorizer to vectorize the text.    ","4b258cd1":"So after using countvectorizer let's try tf-idfvectorizer to vectorize the text and compare the results with Naive bayes. ","f363d531":"Take a look at the top 10 rows of the merged dataset","febf681b":"So we get a decent  score using Naive bayes on Countvectorizer.Now let's fit the model   ","3e73cc36":"## The data\nThere are two csv files namely \"true.csv\" and \"False.csv\".First we have to merge both of them into a single dataset.Then we will shuffle it .The columns in both data are same namely:\n* title   : the title of article\n* text    : the text of article\n* subject : the subject of article\n* date    : the date on which article posted\n\nOne more column that i added is target which contain the class of news.\"1\" for fake news and \"0\" for true news."}}