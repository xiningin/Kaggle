{"cell_type":{"88a81d86":"code","dbf2ba70":"code","7c4cfeb1":"code","1b72a6b2":"code","2be16421":"code","8a5c06bf":"code","fea80a62":"code","18f44964":"code","78983f3c":"code","52afdef5":"code","690f7433":"code","a4a6966d":"code","58facb01":"code","afc97a7f":"code","555b25fb":"code","447243d8":"code","eb726d8e":"code","655a5c97":"code","a004a9e9":"code","1484f22b":"code","cfc91251":"code","d3eb809a":"code","3c20a2e1":"code","12707d4c":"code","f8348cf1":"code","55083c05":"code","ffde939e":"code","74577976":"code","9ab70749":"code","1f28f0cf":"code","f17ea1ac":"code","ee9b755d":"code","92683e52":"code","d6493302":"code","35a41bff":"code","e99e9e06":"code","9d7ddee5":"code","edec2910":"code","9a87b61a":"code","d1ec107e":"code","cf5f26a5":"code","e8d7f1e9":"code","7e3c4f9b":"code","b1e7e246":"code","39ab4f8b":"code","80697ddc":"code","5bad1b42":"code","1afc306d":"code","d8408611":"code","79395d52":"code","32375105":"code","02c54157":"code","c82606aa":"code","14c80ff0":"code","50a5f419":"code","0b57c1c7":"code","4043c542":"code","85b7bef8":"code","1996e1d3":"code","a51f2a8c":"code","db4a4e57":"code","d0298af0":"code","bf5e124c":"code","ea8fa0de":"code","7352c4ce":"code","dbaba8b2":"code","f4fc73c6":"code","bda18f70":"code","d3b20920":"code","bda4b181":"code","db9a428b":"code","754ef5d4":"code","d4f51150":"code","62ad42b7":"code","2c6f3397":"code","88e9e90a":"code","8edfae6f":"code","d8b9728b":"code","be594040":"code","5d5159fb":"code","fe9067f6":"code","d04352bd":"code","13802c79":"code","100ed40b":"code","e8dc6193":"code","731d8d0b":"code","99d80992":"code","9c296161":"code","19a41f26":"code","13c3461d":"code","3af348bf":"code","0dc60c87":"code","43164dde":"code","2a19aecf":"code","5cd6f0bf":"code","dae65cc2":"code","06d1b530":"code","232cae0c":"code","59f65d38":"code","90e1fdea":"code","dcee6dfa":"code","e5de1bae":"code","2cb3af23":"code","f76e0a3b":"code","7696b696":"code","7411841c":"code","12fadb6e":"code","212d2d54":"code","cbe706ee":"code","d528e02c":"code","04389f4b":"code","99d99daa":"code","6eb7ccb8":"code","dd7b1711":"code","1395f12e":"code","3f021bf7":"code","29cd9d7d":"code","bdc61412":"code","e2fb74ff":"code","74206b40":"code","d83182bd":"code","fb48bea2":"code","018531ce":"code","997153f5":"code","c5724655":"code","1308cdb2":"code","6b51d675":"code","b657e9ce":"code","385f4029":"code","c140f46c":"code","81c3d7c9":"code","bba36e4a":"code","ae5efce4":"code","198dde5d":"code","a025406f":"code","3fb52302":"code","7ab5149c":"code","38e71792":"code","a562eb60":"code","40bffba8":"code","145eb8cd":"code","742f6011":"code","c47d3e3d":"code","305a2629":"code","2c705e6f":"code","05f09a17":"code","1b0c8bb7":"code","8a71dbc4":"code","c3c47d87":"code","c4a59f4e":"code","cadffe97":"code","6e4d961e":"code","a799e106":"code","2742b59f":"code","c5f27c9e":"code","aff06bcd":"code","4e87553e":"code","1b3ed759":"code","a7d77058":"code","5d4836ff":"code","23b3afdf":"code","4c4907db":"code","3e7216d0":"code","e6482308":"code","882f1d19":"code","9c969d32":"code","807db559":"code","1414771d":"code","6877f832":"code","b9b8d870":"code","931f2f27":"code","af3ed614":"code","d1b7a213":"code","10d28f45":"code","bee6912a":"code","25d86089":"code","10af42ea":"code","fb33d7cc":"code","13e89070":"code","d20aa924":"code","6473f758":"code","f669aa13":"code","5c9fa3e9":"code","f1af83d4":"code","ea06b0f4":"code","8b6e0895":"code","de4b3fad":"code","e0271d7f":"code","37e0e973":"code","4b63579f":"code","b5e408cb":"code","3a29ff68":"code","b9096bbf":"code","b2138386":"code","61d59d3a":"code","02fac206":"code","f768d6c8":"code","1eb853c9":"code","8355d3a7":"code","d6989b2b":"code","e004db4b":"code","b7eb89ce":"code","21c77107":"code","08a15f57":"code","d2b61473":"code","e9ce2233":"code","9c297475":"code","15da56e0":"code","285e8ada":"code","2386d8ff":"code","fafa5fdc":"code","e9dca642":"code","9d1cd6a7":"code","c22d18c3":"code","4ea736a3":"code","e2175f08":"code","0d855171":"code","f8c2617b":"code","54e898f6":"code","4710f15d":"code","a7b8af78":"code","aacffffd":"code","8b715664":"code","ce5b1092":"code","78b9d9cc":"code","3720cc28":"code","c25600ce":"code","e2228ef5":"code","ec2bab49":"code","ca3928fb":"code","7232ad08":"code","a30c09fd":"code","3330c2f9":"code","705b4d20":"code","7ee673da":"code","99cc5c0d":"code","d23caddf":"code","76545e02":"code","01588a91":"code","fb7883da":"code","a3735727":"code","8e84c6d4":"code","1eaa8ddc":"code","6228141d":"code","2fb24edd":"code","41f95b44":"code","84db42ed":"code","599ea61e":"code","267e2d04":"code","42df7d60":"code","63404653":"code","25d663a1":"code","906969da":"code","7211875c":"code","d783fcad":"code","badd52b8":"code","ed8a9065":"code","7d6ef09e":"code","76d191ad":"code","1ae2151f":"code","d06f987b":"code","e3776164":"code","864322a7":"code","5c7a8ea7":"code","f33ffe07":"code","a86c4752":"code","3239edd4":"code","557c3f8d":"code","bd6b8583":"code","bed6a7a8":"code","9e6b0885":"code","f286ee0b":"code","b8156b12":"code","b746ca8b":"code","f3a9b85d":"code","1043a11d":"code","603e54cf":"code","cb051553":"code","35cab145":"code","accc1e3d":"code","6a2f90ec":"code","1b3a9419":"code","7fe8b8f8":"code","b46ec227":"code","74bbe757":"code","7ef2d689":"code","440c0f58":"code","2be82731":"code","8cd332d9":"code","1802e646":"code","a1b2b7e9":"code","63453319":"code","b80f8f15":"code","3c32247f":"code","3de6aff9":"code","532fa6ff":"code","20c599cb":"code","e174ae22":"code","0fa37e9d":"code","a0898e0b":"code","c76be12c":"code","22ba46f4":"code","0ed8497a":"code","0c504219":"code","b1747ad0":"code","2ca24169":"code","1d7c1a05":"code","f99f4843":"code","0d43d9da":"code","ad3f1476":"code","36cf2faa":"code","80ead9c9":"code","11e68a66":"code","cfd9763d":"code","5bdc8427":"code","9470d64e":"code","b4e15cce":"code","c6bcadab":"code","aff3e56d":"code","39a3e811":"code","76bba856":"code","46224972":"code","ca6e4342":"code","7a79962a":"code","74d0a715":"code","aebb4010":"code","c532c126":"code","ceeb394f":"code","c5c9d9bf":"code","6752635e":"code","4c9f63b1":"code","0c2a741f":"code","9ae1b231":"code","02beb7c5":"code","410aff72":"code","599078e1":"code","b885f94a":"code","b6755de5":"code","987d7d5d":"code","3f12e188":"code","ef49f737":"code","56df09e8":"code","38ee82ba":"code","9378692b":"code","5ce4c105":"code","5d0514b7":"code","8ad4d19e":"code","ad2d37e9":"code","b188a62b":"code","6f1dbe92":"code","748c8170":"code","cbf1d2ef":"code","70dfe3a9":"code","b97fce2a":"code","c48eebc9":"code","39245037":"code","1f51af62":"code","9a050c64":"code","da66e951":"code","e903ac42":"code","4cd93243":"code","b0af1ba2":"code","62581b0d":"code","7c28d407":"code","8fbfc183":"code","be1d74e3":"code","3938cff7":"code","becc36aa":"code","8efc9d4c":"code","d3ca4afb":"code","83b1ff81":"code","052331be":"code","312fa187":"code","b3d842e4":"code","2af5794f":"code","bad890d0":"code","df67c937":"code","ea743bf7":"code","be38100e":"code","e64160b6":"code","cfd2f1e7":"code","78114da3":"code","8cd7006f":"code","ecd09024":"code","fa2209e4":"code","cf2a0d44":"markdown","ede9b87e":"markdown","3d98b938":"markdown","be756ea9":"markdown","955b7659":"markdown","68f61b4c":"markdown","e829d810":"markdown","5466ecfd":"markdown","0e0c8961":"markdown","8c35294d":"markdown","833b23f6":"markdown","175ecf5d":"markdown","53607bd1":"markdown","100c2841":"markdown","e07c54a6":"markdown","c11b346f":"markdown","4da0b5ea":"markdown","45cb92be":"markdown","6ba75a34":"markdown","a5654b36":"markdown","963ca389":"markdown","f18d4f42":"markdown","c61f5bf8":"markdown","e8b430ed":"markdown","82a4b8b2":"markdown","c8fefb00":"markdown","e911c0eb":"markdown","38cf4838":"markdown","e6ad2fff":"markdown","4ae8fc21":"markdown","32f8d27b":"markdown","5669ff21":"markdown","5809fcc9":"markdown","48309f02":"markdown","6aea9a0e":"markdown","3edd844b":"markdown","5d8544fb":"markdown","65ab5d04":"markdown","9730857f":"markdown","c7f65a61":"markdown","91025cc1":"markdown","3e953519":"markdown","a844dee9":"markdown","d30129f5":"markdown","19bf2b27":"markdown","4642042f":"markdown","4a2a9311":"markdown","68799d09":"markdown","0bdde0e3":"markdown","c1d9eaed":"markdown","de147fed":"markdown","6ee2b3b0":"markdown","77a76b0a":"markdown","674e3cc7":"markdown","4d9ccfee":"markdown","cb109ac8":"markdown","a0ce51aa":"markdown","f57b2a2d":"markdown","699bc7b4":"markdown","a4a38065":"markdown","34fba710":"markdown","82caec30":"markdown","121d57f6":"markdown","4eeb6a85":"markdown","28cdc125":"markdown","866882cf":"markdown","c3463410":"markdown","ab8fec93":"markdown","392b48df":"markdown","59bf0e54":"markdown","4aae7e2c":"markdown","a0d4ba17":"markdown","5a717b93":"markdown","ff3c0899":"markdown","f315898f":"markdown","581c2dc6":"markdown","4b300c68":"markdown","72e8b0d9":"markdown","4c16a0d1":"markdown","d2531973":"markdown","3e41320c":"markdown","00c68369":"markdown","3ba57a25":"markdown","37558006":"markdown","37db3755":"markdown","3c5254af":"markdown","0143875b":"markdown","aa4d75be":"markdown","5f78f6f9":"markdown","1de51b64":"markdown","ba9914b6":"markdown","482009ee":"markdown","bbddd816":"markdown","5fa54e39":"markdown","8db2f00a":"markdown","19b5285f":"markdown","79ca62a4":"markdown","24630050":"markdown","22877657":"markdown","88987e7e":"markdown","0118cb24":"markdown","e8286d44":"markdown","1b7c88a9":"markdown","fb022cab":"markdown","455fd30d":"markdown","f9675de5":"markdown","bd7283ba":"markdown","ef343c54":"markdown","acb373e1":"markdown","b1a1729a":"markdown","239fdb45":"markdown","22616b84":"markdown","9a0894ae":"markdown","8026c752":"markdown","7e77fdba":"markdown"},"source":{"88a81d86":"# Load libraries\nimport numpy as np\nimport pandas as pd\n\nimport datetime as dt\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\nimport seaborn as sns\nsns.set(style=\"ticks\")\n%matplotlib inline\n\nfrom scipy.stats import norm\nfrom scipy import stats\n\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_colwidth', -1)","dbf2ba70":"data=pd.read_excel(r'..\/input\/telecom-chum\/Data+Dictionary-+Telecom+Churn+Case+Study.xlsx')\ndata","7c4cfeb1":"telecom=pd.read_csv(r'..\/input\/telecom-chum\/telecom_churn_data.csv')\ntelecom.head()","1b72a6b2":"telecom.info(verbose=True, null_counts=True)","2be16421":"telecom.shape","8a5c06bf":"telecom.describe()","fea80a62":"telecom['total_data_rech_6'] = telecom['total_rech_data_6'] * telecom['av_rech_amt_data_6']\ntelecom['total_data_rech_7'] = telecom['total_rech_data_7'] * telecom['av_rech_amt_data_7']\ntelecom.head()","18f44964":"telecom['amt_data_6'] = telecom[['total_rech_amt_6','total_data_rech_6']].sum(axis=1)\ntelecom['amt_data_7'] = telecom[['total_rech_amt_7','total_data_rech_7']].sum(axis=1)\ntelecom['AVG_amt_data_6_7'] = telecom[['amt_data_6','amt_data_7']].mean(axis=1)\ntelecom.head()","78983f3c":"telecom[['total_rech_amt_6','total_rech_amt_7','AVG_amt_data_6_7']].quantile(np.linspace(.05, 1, 19, 0))","52afdef5":"highvalue = telecom[(telecom['AVG_amt_data_6_7']> telecom['AVG_amt_data_6_7'].quantile(0.7))]\nhighvalue.head()\n","690f7433":"highvalue.info()","a4a6966d":"highvalue.shape","58facb01":"highvalue.describe()","afc97a7f":"highvalue['churn_flag'] = np.where(\n    ((highvalue['total_ic_mou_9'] == 0.00) | (highvalue['total_og_mou_9'] == 0.00))\n    & ((highvalue['vol_2g_mb_9'] == 0.00) | (highvalue['vol_3g_mb_9'] == 0.00)),\n    1, 0 )\nhighvalue.head()","555b25fb":"highvalue['churn_flag'].value_counts()","447243d8":"highvalue['churn_flag'].value_counts()* 100\/highvalue.shape[0]","eb726d8e":"highvalue.shape","655a5c97":"highvalue = highvalue.drop(highvalue.filter(regex='_9|sep', axis = 1).columns, axis=1)\nhighvalue.head()","a004a9e9":"highvalue.shape","1484f22b":"highvalue.info(verbose=True, null_counts=True)","cfc91251":"highvalue.isna().sum().sort_values(ascending=False)","d3eb809a":"round((highvalue.isna().sum()*100\/highvalue.shape[0]),2).sort_values(ascending=False)","3c20a2e1":"unique_stats = pd.DataFrame(highvalue.nunique()).reset_index().rename(columns = {'index': 'feature', 0: 'nunique'})\nprint(unique_stats[unique_stats['nunique'] == 1])\n\n","12707d4c":"print('%d features with a single unique value.\\n' % len(unique_stats[unique_stats['nunique'] == 1]))","f8348cf1":"highvalue.shape","55083c05":"highvalue = highvalue.drop(columns = list(unique_stats[unique_stats['nunique'] == 1]['feature']))\nhighvalue.head()","ffde939e":"highvalue.shape","74577976":"round((highvalue.isna().sum()*100\/highvalue.shape[0]),2).sort_values(ascending=False)","9ab70749":"highvalue['arpu_3g_8'].isna().sum()","1f28f0cf":"round(highvalue['arpu_3g_8'].isna().sum()*100\/highvalue.shape[0],2)","f17ea1ac":"highvalue['arpu_3g_8'].describe()","ee9b755d":"ax=sns.distplot(highvalue['arpu_3g_8'],kde=False)\nax.set_yscale('log')\nplt.show()","92683e52":"highvalue[highvalue['arpu_3g_8'].isna()][['date_of_last_rech_data_8','total_rech_data_8','max_rech_data_8','count_rech_2g_8', \n                                          'count_rech_3g_8', 'av_rech_amt_data_8', 'vol_2g_mb_8', 'vol_3g_mb_8', 'arpu_2g_8',\n                                          'night_pck_user_8', 'monthly_2g_8', 'sachet_2g_8', 'monthly_3g_8', 'sachet_3g_8',\n                                          'fb_user_8']].nunique()","d6493302":"highvalue[highvalue['arpu_3g_8'].isna()][['date_of_last_rech_data_8','total_rech_data_8','max_rech_data_8','count_rech_2g_8', \n                                          'count_rech_3g_8', 'av_rech_amt_data_8', 'vol_2g_mb_8', 'vol_3g_mb_8', 'arpu_2g_8',\n                                          'night_pck_user_8', 'monthly_2g_8', 'sachet_2g_8', 'monthly_3g_8', 'sachet_3g_8',\n                                          'fb_user_8']].head()","35a41bff":"highvalue['arpu_3g_8'].fillna(0,inplace=True)\nhighvalue['arpu_3g_8'].isna().sum()","e99e9e06":"round((highvalue.isna().sum()*100\/highvalue.shape[0]),2).sort_values(ascending=False)","9d7ddee5":"highvalue['isd_og_mou_8'].isna().sum()","edec2910":"round((highvalue['isd_og_mou_8'].isna().sum()*100\/highvalue.shape[0]),2)","9a87b61a":"highvalue[highvalue['isd_og_mou_8'].isna()][['loc_og_mou_8', 'std_og_mou_8', \n                                             'spl_og_mou_8', 'og_others_8', \n                                             'total_og_mou_8']].nunique()","d1ec107e":"highvalue[highvalue['isd_og_mou_8'].isna()][['loc_og_mou_8', 'std_og_mou_8', \n                                             'spl_og_mou_8', 'og_others_8', \n                                             'total_og_mou_8']].head()","cf5f26a5":"highvalue.fillna(0,inplace=True)","e8d7f1e9":"highvalue.isna().values.any()","7e3c4f9b":"highvalueo=highvalue.copy()\nhighvalueo.drop_duplicates(subset=None, inplace=True)\nhighvalueo.shape","b1e7e246":"del highvalueo\nhighvalue.shape","39ab4f8b":"round((highvalue['churn_flag'].value_counts()*100 \/ highvalue.shape[0]),2)","80697ddc":"plt.figure(figsize=(10,5))\nax=highvalue['churn_flag'].value_counts().plot(kind = 'bar')\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 45)\nplt.ylabel('Count')\nplt.xlabel('Churn Status')\nplt.title('Churn Status Distribution',fontsize=14)\nplt.show()","5bad1b42":"highvalue['mobile_number'].value_counts().sort_values(ascending = False).head()","1afc306d":"sns.pairplot(data=highvalue[['arpu_6','arpu_7','arpu_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","d8408611":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 4))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].arpu_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].arpu_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","79395d52":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12,6))\nsns.boxplot(x='churn_flag', y='arpu_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='arpu_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","32375105":"sns.pairplot(data=highvalue[['onnet_mou_6','onnet_mou_7','onnet_mou_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","02c54157":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].onnet_mou_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].onnet_mou_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","c82606aa":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='onnet_mou_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='onnet_mou_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","14c80ff0":"sns.pairplot(data=highvalue[['offnet_mou_6','offnet_mou_7','offnet_mou_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","50a5f419":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].offnet_mou_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].offnet_mou_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","0b57c1c7":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='offnet_mou_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='offnet_mou_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","4043c542":"sns.pairplot(data=highvalue[['roam_ic_mou_6','roam_ic_mou_7','roam_ic_mou_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","85b7bef8":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].roam_ic_mou_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].roam_ic_mou_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","1996e1d3":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='roam_ic_mou_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='roam_ic_mou_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","a51f2a8c":"sns.pairplot(data=highvalue[['loc_og_mou_6','loc_og_mou_7','loc_og_mou_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","db4a4e57":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].loc_og_mou_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].loc_og_mou_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","d0298af0":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 4))\nsns.boxplot(x='churn_flag', y='loc_og_mou_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='loc_og_mou_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","bf5e124c":"sns.pairplot(data=highvalue[['std_og_mou_6','std_og_mou_7','std_og_mou_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","ea8fa0de":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].std_og_mou_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].std_og_mou_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","7352c4ce":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='std_og_mou_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='std_og_mou_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","dbaba8b2":"sns.pairplot(data=highvalue[['isd_og_mou_6','isd_og_mou_7','isd_og_mou_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","f4fc73c6":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].isd_og_mou_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].isd_og_mou_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","bda18f70":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='isd_og_mou_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='isd_og_mou_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","d3b20920":"sns.pairplot(data=highvalue[['total_og_mou_6','total_og_mou_7','total_og_mou_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","bda4b181":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].total_og_mou_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].total_og_mou_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","db9a428b":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='total_og_mou_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='total_og_mou_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","754ef5d4":"sns.pairplot(data=highvalue[['loc_ic_mou_6','loc_ic_mou_7','loc_ic_mou_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","d4f51150":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].loc_ic_mou_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].loc_ic_mou_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","62ad42b7":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 4))\nsns.boxplot(x='churn_flag', y='loc_ic_mou_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='loc_ic_mou_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","2c6f3397":"sns.pairplot(data=highvalue[['std_ic_mou_6','std_ic_mou_7','std_ic_mou_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","88e9e90a":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].std_ic_mou_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].std_ic_mou_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","8edfae6f":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='std_ic_mou_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='std_ic_mou_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","d8b9728b":"sns.pairplot(data=highvalue[['total_ic_mou_6','total_ic_mou_7','total_ic_mou_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","be594040":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].total_ic_mou_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].total_ic_mou_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","5d5159fb":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12,6))\nsns.boxplot(x='churn_flag', y='total_ic_mou_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='total_ic_mou_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","fe9067f6":"sns.pairplot(data=highvalue[['total_rech_num_6','total_rech_num_7','total_rech_num_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","d04352bd":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].total_rech_num_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].total_rech_num_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","13802c79":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='total_rech_num_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='total_rech_num_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","100ed40b":"sns.pairplot(data=highvalue[['total_rech_amt_6','total_rech_amt_7','total_rech_amt_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","e8dc6193":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].total_rech_amt_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].total_rech_amt_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","731d8d0b":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='total_rech_amt_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='total_rech_amt_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","99d80992":"sns.pairplot(data=highvalue[['total_rech_data_6','total_rech_data_7','total_rech_data_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","9c296161":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].total_rech_data_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].total_rech_data_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","19a41f26":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='total_rech_data_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='total_rech_data_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","13c3461d":"sns.pairplot(data=highvalue[['vol_2g_mb_6','vol_2g_mb_7','vol_2g_mb_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","3af348bf":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].vol_2g_mb_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].vol_2g_mb_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","0dc60c87":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='vol_2g_mb_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='vol_2g_mb_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","43164dde":"sns.pairplot(data=highvalue[['vol_3g_mb_6','vol_3g_mb_7','vol_3g_mb_8','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","2a19aecf":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].vol_3g_mb_6, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].vol_3g_mb_6, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","5cd6f0bf":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='vol_3g_mb_6', data=highvalue)\nsns.stripplot(x='churn_flag', y='vol_3g_mb_6', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","dae65cc2":"sns.pairplot(data=highvalue[['jun_vbc_3g','jul_vbc_3g','aug_vbc_3g','churn_flag']],hue='churn_flag',diag_kind='None')\nplt.show()","06d1b530":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(12, 6))\nax = sns.distplot(highvalue[highvalue['churn_flag']==1].jun_vbc_3g, bins = 30, ax = axes[0], kde = False)\nax.set_title('Churn')\nax.grid()\nax = sns.distplot(highvalue[highvalue['churn_flag']==0].jun_vbc_3g, bins = 30, ax = axes[1], kde = False)\nax.set_title('Non-Churn')\nax.grid()\nplt.show()","232cae0c":"fig, axes = plt.subplots(nrows=1, ncols=1,figsize=(12, 6))\nsns.boxplot(x='churn_flag', y='jun_vbc_3g', data=highvalue)\nsns.stripplot(x='churn_flag', y='jun_vbc_3g', data=highvalue, jitter=True, edgecolor=\"gray\")\nplt.show()","59f65d38":"highvalue.info(verbose=True, null_counts=True)","90e1fdea":"#Let's first start formating date features","dcee6dfa":"highvalue['date_of_last_rech_6'] = pd.to_datetime(highvalue['date_of_last_rech_6'])\nhighvalue['date_of_last_rech_7'] = pd.to_datetime(highvalue['date_of_last_rech_7'])\nhighvalue['date_of_last_rech_8'] = pd.to_datetime(highvalue['date_of_last_rech_8'])\nhighvalue.info(verbose=True, null_counts=True)","e5de1bae":"highvalue['date_of_last_rech_data_6'] = pd.to_datetime(highvalue['date_of_last_rech_data_6'])\nhighvalue['date_of_last_rech_data_7'] = pd.to_datetime(highvalue['date_of_last_rech_data_7'])\nhighvalue['date_of_last_rech_data_8'] = pd.to_datetime(highvalue['date_of_last_rech_data_8'])\nhighvalue.info(verbose=True, null_counts=True)","2cb3af23":"highvalue['mobile_number'] = highvalue['mobile_number'].astype(str)\nhighvalue.info(verbose=True, null_counts=True)","f76e0a3b":"# Now let's create new features from date column\nhighvalue['date_of_last_rech_day_6'] = highvalue['date_of_last_rech_6'].dt.day\nhighvalue['date_of_last_rech_dayofwk_6'] = highvalue['date_of_last_rech_6'].dt.dayofweek.astype(str)\nhighvalue.info(verbose=True, null_counts=True)","7696b696":"highvalue['date_of_last_rech_day_7'] = highvalue['date_of_last_rech_7'].dt.day\nhighvalue['date_of_last_rech_dayofwk_7'] = highvalue['date_of_last_rech_7'].dt.dayofweek.astype(str)\nhighvalue.info(verbose=True, null_counts=True)","7411841c":"highvalue['date_of_last_rech_day_8'] = highvalue['date_of_last_rech_8'].dt.day\nhighvalue['date_of_last_rech_dayofwk_8'] = highvalue['date_of_last_rech_8'].dt.dayofweek.astype(str)\nhighvalue.info(verbose=True, null_counts=True)","12fadb6e":"highvalue['date_of_last_rech_data_day_6'] = highvalue['date_of_last_rech_data_6'].dt.day\nhighvalue['date_of_last_rech_data_dayofwk_6'] = highvalue['date_of_last_rech_data_6'].dt.dayofweek.astype(str)\nhighvalue.info(verbose=True, null_counts=True)","212d2d54":"highvalue['date_of_last_rech_data_day_7'] = highvalue['date_of_last_rech_data_7'].dt.day\nhighvalue['date_of_last_rech_data_dayofwk_7'] = highvalue['date_of_last_rech_data_7'].dt.dayofweek.astype(str)\nhighvalue.info(verbose=True, null_counts=True)","cbe706ee":"highvalue['date_of_last_rech_data_day_8'] = highvalue['date_of_last_rech_data_8'].dt.day\nhighvalue['date_of_last_rech_data_dayofwk_8'] = highvalue['date_of_last_rech_data_8'].dt.dayofweek.astype(str)\nhighvalue.info(verbose=True, null_counts=True)","d528e02c":"highvalue.describe()","04389f4b":"joincorr= highvalue.corr()\nhighvalue_corr = joincorr.stack().reset_index().sort_values(by = 0, ascending = False)\nhighvalue_corr[((highvalue_corr[0] < 1) & (highvalue_corr[0] >= 0.4)) | \n               ((highvalue_corr[0] <= -0.4) & (highvalue_corr[0] > -1))]","99d99daa":"highvalue['AVG_arpu_6_7'] = highvalue[['arpu_6','arpu_7']].mean(axis=1)\nhighvalue['is_arpu_flag'] = np.where((highvalue['arpu_8'] > highvalue['AVG_arpu_6_7']), 0, 1)\nhighvalue.head()","6eb7ccb8":"highvalue['AVG_onnet_mou_6_7'] = highvalue[['onnet_mou_6','onnet_mou_7']].mean(axis=1)\nhighvalue['is_onnet_mou_flag'] = np.where((highvalue['onnet_mou_8'] > highvalue['AVG_onnet_mou_6_7']), 0, 1)\nhighvalue.head()","dd7b1711":"highvalue['AVG_offnet_mou_6_7'] = highvalue[['offnet_mou_6','offnet_mou_7']].mean(axis=1)\nhighvalue['is_offnet_mou_flag'] = np.where((highvalue['offnet_mou_8'] > highvalue['AVG_offnet_mou_6_7']), 0, 1)\nhighvalue.head()","1395f12e":"highvalue['AVG_roam_ic_mou_6_7'] = highvalue[['roam_ic_mou_6','roam_ic_mou_7']].mean(axis=1)\nhighvalue['is_roam_ic_mou_flag'] = np.where((highvalue['roam_ic_mou_8'] > highvalue['AVG_roam_ic_mou_6_7']), 0, 1)\nhighvalue.head()","3f021bf7":"highvalue['AVG_roam_og_mou_6_7'] = highvalue[['roam_og_mou_6','roam_og_mou_7']].mean(axis=1)\nhighvalue['is_roam_og_mou_flag'] = np.where((highvalue['roam_og_mou_8'] > highvalue['AVG_roam_og_mou_6_7']), 0, 1)\nhighvalue.head()","29cd9d7d":"highvalue['AVG_loc_og_t2t_mou_6_7'] = highvalue[['loc_og_t2t_mou_6','loc_og_t2t_mou_7']].mean(axis=1)\nhighvalue['is_loc_og_t2t_mou_flag'] = np.where((highvalue['loc_og_t2t_mou_8'] > highvalue['AVG_loc_og_t2t_mou_6_7']), 0, 1)\nhighvalue.head()","bdc61412":"highvalue['AVG_loc_og_t2m_mou_6_7'] = highvalue[['loc_og_t2m_mou_6','loc_og_t2m_mou_7']].mean(axis=1)\nhighvalue['is_loc_og_t2m_mou_flag'] = np.where((highvalue['loc_og_t2m_mou_8'] > highvalue['AVG_loc_og_t2m_mou_6_7']), 0, 1)\nhighvalue.head()","e2fb74ff":"highvalue['AVG_loc_og_t2f_mou_6_7'] = highvalue[['loc_og_t2f_mou_6','loc_og_t2f_mou_7']].mean(axis=1)\nhighvalue['is_loc_og_t2f_mou_flag'] = np.where((highvalue['loc_og_t2f_mou_8'] > highvalue['AVG_loc_og_t2f_mou_6_7']), 0, 1)\nhighvalue.head()","74206b40":"highvalue['AVG_loc_og_t2c_mou_6_7'] = highvalue[['loc_og_t2c_mou_6','loc_og_t2c_mou_7']].mean(axis=1)\nhighvalue['is_loc_og_t2c_mou_flag'] = np.where((highvalue['loc_og_t2c_mou_8'] > highvalue['AVG_loc_og_t2c_mou_6_7']), 0, 1)\nhighvalue.head()","d83182bd":"highvalue['AVG_std_og_t2t_mou_6_7'] = highvalue[['std_og_t2t_mou_6','std_og_t2t_mou_7']].mean(axis=1)\nhighvalue['is_std_og_t2t_mou_flag'] = np.where((highvalue['std_og_t2t_mou_8'] > highvalue['AVG_std_og_t2t_mou_6_7']), 0, 1)\nhighvalue.head()\n\n","fb48bea2":"highvalue['AVG_std_og_t2m_mou_6_7'] = highvalue[['std_og_t2m_mou_6','std_og_t2m_mou_7']].mean(axis=1)\nhighvalue['is_std_og_t2m_mou_flag'] = np.where((highvalue['std_og_t2m_mou_8'] > highvalue['AVG_std_og_t2m_mou_6_7']), 0, 1)\nhighvalue.head()\n\n","018531ce":"highvalue['AVG_std_og_t2f_mou_6_7'] = highvalue[['std_og_t2f_mou_6','std_og_t2f_mou_7']].mean(axis=1)\nhighvalue['is_std_og_t2f_mou_flag'] = np.where((highvalue['std_og_t2f_mou_8'] > highvalue['AVG_std_og_t2f_mou_6_7']), 0, 1)\nhighvalue.head()\n\n","997153f5":"highvalue['AVG_isd_og_mou_6_7'] = highvalue[['isd_og_mou_6','isd_og_mou_7']].mean(axis=1)\nhighvalue['is_isd_og_mou_flag'] = np.where((highvalue['isd_og_mou_8'] > highvalue['AVG_isd_og_mou_6_7']), 0, 1)\nhighvalue.head()\n\n","c5724655":"highvalue['AVG_spl_og_mou_6_7'] = highvalue[['spl_og_mou_6','spl_og_mou_7']].mean(axis=1)\nhighvalue['is_spl_og_mou_flag'] = np.where((highvalue['spl_og_mou_8'] > highvalue['AVG_spl_og_mou_6_7']), 0, 1)\nhighvalue.head()\n\n","1308cdb2":"highvalue['AVG_og_others_6_7'] = highvalue[['og_others_6','og_others_7']].mean(axis=1)\nhighvalue['is_og_others_flag'] = np.where((highvalue['og_others_8'] > highvalue['AVG_og_others_6_7']), 0, 1)\nhighvalue.head()","6b51d675":"highvalue['AVG_loc_ic_t2t_mou_6_7'] = highvalue[['loc_ic_t2t_mou_6','loc_ic_t2t_mou_7']].mean(axis=1)\nhighvalue['is_loc_ic_t2t_mou_flag'] = np.where((highvalue['loc_ic_t2t_mou_8'] > highvalue['AVG_loc_ic_t2t_mou_6_7']), 0, 1)\nhighvalue.head()\n","b657e9ce":"highvalue['AVG_loc_ic_t2m_mou_6_7'] = highvalue[['loc_ic_t2m_mou_6','loc_ic_t2m_mou_7']].mean(axis=1)\nhighvalue['is_loc_ic_t2m_mou_flag'] = np.where((highvalue['loc_ic_t2m_mou_8'] > highvalue['AVG_loc_ic_t2m_mou_6_7']), 0, 1)\nhighvalue.head()\n","385f4029":"highvalue['AVG_loc_ic_t2f_mou_6_7'] = highvalue[['loc_ic_t2f_mou_6','loc_ic_t2f_mou_7']].mean(axis=1)\nhighvalue['is_loc_ic_t2f_mou_flag'] = np.where((highvalue['loc_ic_t2f_mou_8'] > highvalue['AVG_loc_ic_t2f_mou_6_7']), 0, 1)\nhighvalue.head()","c140f46c":"highvalue['AVG_std_ic_t2t_mou_6_7'] = highvalue[['std_ic_t2t_mou_6','std_ic_t2t_mou_7']].mean(axis=1)\nhighvalue['is_std_ic_t2t_mou_flag'] = np.where((highvalue['std_ic_t2t_mou_8'] > highvalue['AVG_std_ic_t2t_mou_6_7']), 0, 1)\nhighvalue.head()","81c3d7c9":"highvalue['AVG_std_ic_t2m_mou_6_7'] = highvalue[['std_ic_t2m_mou_6','std_ic_t2m_mou_7']].mean(axis=1)\nhighvalue['is_std_ic_t2m_mou_flag'] = np.where((highvalue['std_ic_t2m_mou_8'] > highvalue['AVG_std_ic_t2m_mou_6_7']), 0, 1)\nhighvalue.head()","bba36e4a":"highvalue['AVG_std_ic_t2f_mou_6_7'] = highvalue[['std_ic_t2f_mou_6','std_ic_t2f_mou_7']].mean(axis=1)\nhighvalue['is_std_ic_t2f_mou_flag'] = np.where((highvalue['std_ic_t2f_mou_8'] > highvalue['AVG_std_ic_t2f_mou_6_7']), 0, 1)\nhighvalue.head()","ae5efce4":"highvalue['AVG_spl_ic_mou_6_7'] = highvalue[['spl_ic_mou_6','spl_ic_mou_7']].mean(axis=1)\nhighvalue['is_spl_ic_mou_flag'] = np.where((highvalue['spl_ic_mou_8'] > highvalue['AVG_spl_ic_mou_6_7']), 0, 1)\nhighvalue.head()","198dde5d":"highvalue['AVG_isd_ic_mou_6_7'] = highvalue[['isd_ic_mou_6','isd_ic_mou_7']].mean(axis=1)\nhighvalue['is_isd_ic_mou_flag'] = np.where((highvalue['isd_ic_mou_8'] > highvalue['AVG_isd_ic_mou_6_7']), 0, 1)\nhighvalue.head()","a025406f":"\nhighvalue['AVG_ic_others_6_7'] = highvalue[['ic_others_6','ic_others_7']].mean(axis=1)\nhighvalue['is_ic_others_flag'] = np.where((highvalue['ic_others_8'] > highvalue['AVG_ic_others_6_7']), 0, 1)\nhighvalue.head()","3fb52302":"\nhighvalue['AVG_total_rech_amt_6_7'] = highvalue[['total_rech_amt_6','total_rech_amt_7']].mean(axis=1)\nhighvalue['is_total_rech_amt_flag'] = np.where((highvalue['total_rech_amt_8'] > highvalue['AVG_total_rech_amt_6_7']), 0, 1)\nhighvalue.head()","7ab5149c":"\nhighvalue['AVG_vol_2g_mb_6_7'] = highvalue[['vol_2g_mb_6','vol_2g_mb_7']].mean(axis=1)\nhighvalue['is_vol_2g_mb_flag'] = np.where((highvalue['vol_2g_mb_8'] > highvalue['AVG_vol_2g_mb_6_7']), 0, 1)\nhighvalue.head()","38e71792":"\nhighvalue['AVG_vol_3g_mb_6_7'] = highvalue[['vol_3g_mb_6','vol_3g_mb_7']].mean(axis=1)\nhighvalue['is_vol_3g_mb_flag'] = np.where((highvalue['vol_3g_mb_8'] > highvalue['AVG_vol_3g_mb_6_7']), 0, 1)\nhighvalue.head()","a562eb60":"\nhighvalue['AVG_arpu_3g_6_7'] = highvalue[['arpu_3g_6','arpu_3g_7']].mean(axis=1)\nhighvalue['is_arpu_3g_flag'] = np.where((highvalue['arpu_3g_8'] > highvalue['AVG_arpu_3g_6_7']), 0, 1)\nhighvalue.head()","40bffba8":"highvalue['AVG_arpu_2g_6_7'] = highvalue[['arpu_2g_6','arpu_2g_7']].mean(axis=1)\nhighvalue['is_arpu_2g_flag'] = np.where((highvalue['arpu_2g_8'] > highvalue['AVG_arpu_2g_6_7']), 0, 1)\nhighvalue.head()","145eb8cd":"highvalue['AVG_vbc_3g_6_7'] = highvalue[['jun_vbc_3g','jul_vbc_3g']].mean(axis=1)\nhighvalue['is_vbc_3g_flag'] = np.where((highvalue['aug_vbc_3g'] > highvalue['AVG_vbc_3g_6_7']), 0, 1)\nhighvalue.head()","742f6011":"highvalue['AVG_loc_og_mou_6_7'] = highvalue[['loc_og_mou_6','loc_og_mou_7']].mean(axis=1)\nhighvalue['is_loc_og_mou_flag'] = np.where((highvalue['loc_og_mou_8'] > highvalue['AVG_loc_og_mou_6_7']), 0, 1)\nhighvalue.head()","c47d3e3d":"highvalue['AVG_std_og_mou_6_7'] = highvalue[['std_og_mou_6','std_og_mou_7']].mean(axis=1)\nhighvalue['is_std_og_mou_flag'] = np.where((highvalue['std_og_mou_8'] > highvalue['AVG_std_og_mou_6_7']), 0, 1)\nhighvalue.head()","305a2629":"highvalue['AVG_total_og_mou_6_7'] = highvalue[['total_og_mou_6','total_og_mou_7']].mean(axis=1)\nhighvalue['is_total_og_mou_flag'] = np.where((highvalue['total_og_mou_8'] > highvalue['AVG_total_og_mou_6_7']), 0, 1)\nhighvalue.head()","2c705e6f":"\nhighvalue['AVG_loc_ic_mou_6_7'] = highvalue[['loc_ic_mou_6','loc_ic_mou_7']].mean(axis=1)\nhighvalue['is_loc_ic_mou_flag'] = np.where((highvalue['loc_ic_mou_8'] > highvalue['AVG_loc_ic_mou_6_7']), 0, 1)\nhighvalue.head()","05f09a17":"\nhighvalue['AVG_std_ic_mou_6_7'] = highvalue[['std_ic_mou_6','std_ic_mou_7']].mean(axis=1)\nhighvalue['is_std_ic_mou_flag'] = np.where((highvalue['std_ic_mou_8'] > highvalue['AVG_std_ic_mou_6_7']), 0, 1)\nhighvalue.head()","1b0c8bb7":"\nhighvalue['AVG_total_ic_mou_6_7'] = highvalue[['total_ic_mou_6','total_ic_mou_7']].mean(axis=1)\nhighvalue['is_total_ic_mou_flag'] = np.where((highvalue['total_ic_mou_8'] > highvalue['AVG_total_ic_mou_6_7']), 0, 1)\nhighvalue.head()","8a71dbc4":"\nhighvalue['AVG_night_pck_user_6_7'] = highvalue[['night_pck_user_6','night_pck_user_7']].mean(axis=1)\nhighvalue['is_night_pck_user_flag'] = np.where((highvalue['night_pck_user_8'] > highvalue['AVG_night_pck_user_6_7']), 0, 1)\nhighvalue.head()","c3c47d87":"\nhighvalue['AVG_monthly_2g_6_7'] = highvalue[['monthly_2g_6','monthly_2g_7']].mean(axis=1)\nhighvalue['is_monthly_2g_flag'] = np.where((highvalue['monthly_2g_8'] > highvalue['AVG_monthly_2g_6_7']), 0, 1)\nhighvalue.head()","c4a59f4e":"\nhighvalue['AVG_sachet_2g_6_7'] = highvalue[['sachet_2g_6','sachet_2g_7']].mean(axis=1)\nhighvalue['is_sachet_2g_flag'] = np.where((highvalue['sachet_2g_8'] > highvalue['AVG_sachet_2g_6_7']), 0, 1)\nhighvalue.head()","cadffe97":"highvalue['AVG_monthly_3g_6_7'] = highvalue[['monthly_3g_6','monthly_3g_7']].mean(axis=1)\nhighvalue['is_monthly_3g_flag'] = np.where((highvalue['monthly_3g_8'] > highvalue['AVG_monthly_3g_6_7']), 0, 1)\nhighvalue.head()","6e4d961e":"highvalue['AVG_sachet_3g_6_7'] = highvalue[['sachet_3g_6','sachet_3g_7']].mean(axis=1)\nhighvalue['is_sachet_3g_flag'] = np.where((highvalue['sachet_3g_8'] > highvalue['AVG_sachet_3g_6_7']), 0, 1)\nhighvalue.head()","a799e106":"highvalue['AVG_fb_user_6_7'] = highvalue[['fb_user_6','fb_user_7']].mean(axis=1)\nhighvalue['is_fb_user_flag'] = np.where((highvalue['fb_user_8'] > highvalue['AVG_fb_user_6_7']), 0, 1)\nhighvalue.head()","2742b59f":"# Create month on month change features to understand any risk associated with the churn\nhighvalue['loc_og_t2t_mou_7diff6'] = highvalue['loc_og_t2t_mou_7'] - highvalue['loc_og_t2t_mou_6']\nhighvalue['loc_og_t2m_mou_7diff6'] = highvalue['loc_og_t2m_mou_7'] - highvalue['loc_og_t2m_mou_6']\nhighvalue['loc_og_t2f_mou_7diff6'] = highvalue['loc_og_t2f_mou_7'] - highvalue['loc_og_t2f_mou_6']\nhighvalue['loc_og_t2c_mou_7diff6'] = highvalue['loc_og_t2c_mou_7'] - highvalue['loc_og_t2c_mou_6']\nhighvalue['loc_og_mou_7diff6'] = highvalue['loc_og_mou_7'] - highvalue['loc_og_mou_6']\nhighvalue['std_og_t2t_mou_7diff6'] = highvalue['std_og_t2t_mou_7'] - highvalue['std_og_t2t_mou_6']\nhighvalue['std_og_t2m_mou_7diff6'] = highvalue['std_og_t2m_mou_7'] - highvalue['std_og_t2m_mou_6']\nhighvalue['std_og_t2f_mou_7diff6'] = highvalue['std_og_t2f_mou_7'] - highvalue['std_og_t2f_mou_6']\nhighvalue['std_og_mou_7diff6'] = highvalue['std_og_mou_7'] - highvalue['std_og_mou_6']\nhighvalue['loc_og_mou_7diff6'] = highvalue['loc_og_mou_7'] - highvalue['loc_og_mou_6']\nhighvalue['std_og_mou_7diff6'] = highvalue['std_og_mou_7'] - highvalue['std_og_mou_6']\nhighvalue['isd_og_mou_7diff6'] = highvalue['isd_og_mou_7'] - highvalue['isd_og_mou_6']\nhighvalue['spl_og_mou_7diff6'] = highvalue['spl_og_mou_7'] - highvalue['spl_og_mou_6']\nhighvalue['og_others_7diff6'] = highvalue['og_others_7'] - highvalue['og_others_6']\nhighvalue['total_og_mou_7diff6'] = highvalue['total_og_mou_7'] - highvalue['total_og_mou_6']\nhighvalue.head()","c5f27c9e":"\nhighvalue['loc_og_t2t_mou_8diff7'] = highvalue['loc_og_t2t_mou_8'] - highvalue['loc_og_t2t_mou_7']\nhighvalue['loc_og_t2m_mou_8diff7'] = highvalue['loc_og_t2m_mou_8'] - highvalue['loc_og_t2m_mou_7']\nhighvalue['loc_og_t2f_mou_8diff7'] = highvalue['loc_og_t2f_mou_8'] - highvalue['loc_og_t2f_mou_7']\nhighvalue['loc_og_t2c_mou_8diff7'] = highvalue['loc_og_t2c_mou_8'] - highvalue['loc_og_t2c_mou_7']\nhighvalue['loc_og_mou_8diff7'] = highvalue['loc_og_mou_8'] - highvalue['loc_og_mou_7']\nhighvalue['std_og_t2t_mou_8diff7'] = highvalue['std_og_t2t_mou_8'] - highvalue['std_og_t2t_mou_7']\nhighvalue['std_og_t2m_mou_8diff7'] = highvalue['std_og_t2m_mou_8'] - highvalue['std_og_t2m_mou_7']\nhighvalue['std_og_t2f_mou_8diff7'] = highvalue['std_og_t2f_mou_8'] - highvalue['std_og_t2f_mou_7']\nhighvalue['std_og_mou_8diff7'] = highvalue['std_og_mou_8'] - highvalue['std_og_mou_7']\nhighvalue['loc_og_mou_8diff7'] = highvalue['loc_og_mou_8'] - highvalue['loc_og_mou_7']\nhighvalue['std_og_mou_8diff7'] = highvalue['std_og_mou_8'] - highvalue['std_og_mou_7']\nhighvalue['isd_og_mou_8diff7'] = highvalue['isd_og_mou_8'] - highvalue['isd_og_mou_7']\nhighvalue['spl_og_mou_8diff7'] = highvalue['spl_og_mou_8'] - highvalue['spl_og_mou_7']\nhighvalue['og_others_8diff7'] = highvalue['og_others_8'] - highvalue['og_others_7']\nhighvalue['total_og_mou_8diff7'] = highvalue['total_og_mou_8'] - highvalue['total_og_mou_7']\nhighvalue.head()","aff06bcd":"\nhighvalue['loc_ic_t2t_mou_7diff6'] = highvalue['loc_ic_t2t_mou_7'] - highvalue['loc_ic_t2t_mou_6']\nhighvalue['loc_ic_t2m_mou_7diff6'] = highvalue['loc_ic_t2m_mou_7'] - highvalue['loc_ic_t2m_mou_6']\nhighvalue['loc_ic_t2f_mou_7diff6'] = highvalue['loc_ic_t2f_mou_7'] - highvalue['loc_ic_t2f_mou_6']\nhighvalue['loc_ic_mou_7diff6'] = highvalue['loc_ic_mou_7'] - highvalue['loc_ic_mou_6']\nhighvalue['std_ic_t2t_mou_7diff6'] = highvalue['std_ic_t2t_mou_7'] - highvalue['std_ic_t2t_mou_6']\nhighvalue['std_ic_t2m_mou_7diff6'] = highvalue['std_ic_t2m_mou_7'] - highvalue['std_ic_t2m_mou_6']\nhighvalue['std_ic_t2f_mou_7diff6'] = highvalue['std_ic_t2f_mou_7'] - highvalue['std_ic_t2f_mou_6']\nhighvalue['std_ic_mou_7diff6'] = highvalue['std_ic_mou_7'] - highvalue['std_ic_mou_6']\nhighvalue['loc_ic_mou_7diff6'] = highvalue['loc_ic_mou_7'] - highvalue['loc_ic_mou_6']\nhighvalue['std_ic_mou_7diff6'] = highvalue['std_ic_mou_7'] - highvalue['std_ic_mou_6']\nhighvalue['spl_ic_mou_7diff6'] = highvalue['spl_ic_mou_7'] - highvalue['spl_ic_mou_6']\nhighvalue['isd_ic_mou_7diff6'] = highvalue['isd_ic_mou_7'] - highvalue['isd_ic_mou_6']\nhighvalue['ic_others_7diff6'] = highvalue['ic_others_7'] - highvalue['ic_others_6']\nhighvalue['total_ic_mou_7diff6'] = highvalue['total_ic_mou_7'] - highvalue['total_ic_mou_6']\nhighvalue.head()","4e87553e":"\nhighvalue['loc_ic_t2t_mou_8diff7'] = highvalue['loc_ic_t2t_mou_8'] - highvalue['loc_ic_t2t_mou_7']\nhighvalue['loc_ic_t2m_mou_8diff7'] = highvalue['loc_ic_t2m_mou_8'] - highvalue['loc_ic_t2m_mou_7']\nhighvalue['loc_ic_t2f_mou_8diff7'] = highvalue['loc_ic_t2f_mou_8'] - highvalue['loc_ic_t2f_mou_7']\nhighvalue['loc_ic_mou_8diff7'] = highvalue['loc_ic_mou_8'] - highvalue['loc_ic_mou_7']\nhighvalue['std_ic_t2t_mou_8diff7'] = highvalue['std_ic_t2t_mou_8'] - highvalue['std_ic_t2t_mou_7']\nhighvalue['std_ic_t2m_mou_8diff7'] = highvalue['std_ic_t2m_mou_8'] - highvalue['std_ic_t2m_mou_7']\nhighvalue['std_ic_t2f_mou_8diff7'] = highvalue['std_ic_t2f_mou_8'] - highvalue['std_ic_t2f_mou_7']\nhighvalue['std_ic_mou_8diff7'] = highvalue['std_ic_mou_8'] - highvalue['std_ic_mou_7']\nhighvalue['loc_ic_mou_8diff7'] = highvalue['loc_ic_mou_8'] - highvalue['loc_ic_mou_7']\nhighvalue['std_ic_mou_8diff7'] = highvalue['std_ic_mou_8'] - highvalue['std_ic_mou_7']\nhighvalue['spl_ic_mou_8diff7'] = highvalue['spl_ic_mou_8'] - highvalue['spl_ic_mou_7']\nhighvalue['isd_ic_mou_8diff7'] = highvalue['isd_ic_mou_8'] - highvalue['isd_ic_mou_7']\nhighvalue['ic_others_8diff7'] = highvalue['ic_others_8'] - highvalue['ic_others_7']\nhighvalue['total_ic_mou_8diff7'] = highvalue['total_ic_mou_8'] - highvalue['total_ic_mou_7']\nhighvalue.head()","1b3ed759":"\nhighvalue['onnet_mou_7diff6'] = highvalue['onnet_mou_7'] - highvalue['onnet_mou_6']\nhighvalue['onnet_mou_8diff7'] = highvalue['onnet_mou_8'] - highvalue['onnet_mou_7']\nhighvalue.head()","a7d77058":"\nhighvalue['offnet_mou_7diff6'] = highvalue['offnet_mou_7'] - highvalue['offnet_mou_6']\nhighvalue['offnet_mou_8diff7'] = highvalue['offnet_mou_8'] - highvalue['offnet_mou_7']\nhighvalue.head()","5d4836ff":"\nhighvalue['roam_ic_mou_7diff6'] = highvalue['roam_ic_mou_7'] - highvalue['roam_ic_mou_6']\nhighvalue['roam_ic_mou_8diff7'] = highvalue['roam_ic_mou_8'] - highvalue['roam_ic_mou_7']\nhighvalue.head()","23b3afdf":"highvalue['roam_og_mou_7diff6'] = highvalue['roam_og_mou_7'] - highvalue['roam_og_mou_6']\nhighvalue['roam_og_mou_8diff7'] = highvalue['roam_og_mou_8'] - highvalue['roam_og_mou_7']\nhighvalue.head()","4c4907db":"\nhighvalue['total_rech_amt_7diff6'] = highvalue['total_rech_amt_7'] - highvalue['total_rech_amt_6']\nhighvalue['total_rech_amt_8diff7'] = highvalue['total_rech_amt_8'] - highvalue['total_rech_amt_7']\nhighvalue.head()","3e7216d0":"highvalue['date_of_last_rech_7diff6'] = (highvalue['date_of_last_rech_7'] - highvalue['date_of_last_rech_6']).dt.days\nhighvalue['date_of_last_rech_8diff7'] = (highvalue['date_of_last_rech_8'] - highvalue['date_of_last_rech_7']).dt.days\nhighvalue.head()","e6482308":"\nhighvalue['date_of_last_rech_data_7diff6'] = (highvalue['date_of_last_rech_data_7'] - highvalue['date_of_last_rech_data_6']).dt.days\nhighvalue['date_of_last_rech_data_8diff7'] = (highvalue['date_of_last_rech_data_8'] - highvalue['date_of_last_rech_data_7']).dt.days\nhighvalue.head()","882f1d19":"\nhighvalue['vol_2g_mb_7diff6'] = highvalue['vol_2g_mb_7'] - highvalue['vol_2g_mb_6']\nhighvalue['vol_2g_mb_8diff7'] = highvalue['vol_2g_mb_8'] - highvalue['vol_2g_mb_7']\nhighvalue.head()","9c969d32":"\nhighvalue['vol_3g_mb_7diff6'] = highvalue['vol_3g_mb_7'] - highvalue['vol_3g_mb_6']\nhighvalue['vol_3g_mb_8diff7'] = highvalue['vol_3g_mb_8'] - highvalue['vol_3g_mb_7']\nhighvalue.head()","807db559":"\nhighvalue['vbc_3g_7diff6'] = highvalue['jul_vbc_3g'] - highvalue['jun_vbc_3g']\nhighvalue['vbc_3g_8diff7'] = highvalue['aug_vbc_3g'] - highvalue['jul_vbc_3g']\nhighvalue.head()","1414771d":"highvalue.info(verbose=True, null_counts=True)","6877f832":"highvalue.drop(list(highvalue.select_dtypes(include=['datetime64']).columns),axis=1,inplace=True)\nhighvalue.info(verbose=True, null_counts=True)","b9b8d870":"highvalue.drop(['arpu_6','arpu_7','onnet_mou_6','onnet_mou_7','offnet_mou_6','offnet_mou_7',\n                'roam_ic_mou_6','roam_ic_mou_7','roam_og_mou_6','roam_og_mou_7',\n                 'loc_og_t2t_mou_6','loc_og_t2t_mou_7','loc_og_t2m_mou_6','loc_og_t2m_mou_7',\n                 'loc_og_t2f_mou_6','loc_og_t2f_mou_7','loc_og_t2c_mou_6','loc_og_t2c_mou_7',\n                 'std_og_t2t_mou_6','std_og_t2t_mou_7', 'std_og_t2m_mou_6','std_og_t2m_mou_7',\n                 'std_og_t2f_mou_6','std_og_t2f_mou_7','isd_og_mou_6','isd_og_mou_7',\n                 'spl_og_mou_6','spl_og_mou_7', 'og_others_6','og_others_7',\n                 'loc_ic_t2t_mou_6','loc_ic_t2t_mou_7','loc_ic_t2m_mou_6','loc_ic_t2m_mou_7',\n                 'loc_ic_t2f_mou_6','loc_ic_t2f_mou_7', 'std_ic_t2t_mou_6','std_ic_t2t_mou_7',\n                 'std_ic_t2m_mou_6','std_ic_t2m_mou_7', 'std_ic_t2f_mou_6','std_ic_t2f_mou_7',\n                 'spl_ic_mou_6','spl_ic_mou_7', 'isd_ic_mou_6','isd_ic_mou_7',\n                 'ic_others_6','ic_others_7', 'total_rech_amt_6','total_rech_amt_7',\n                 'vol_2g_mb_6','vol_2g_mb_7', 'vol_3g_mb_6','vol_3g_mb_7',\n                 'arpu_3g_6','arpu_3g_7', 'arpu_2g_6','arpu_2g_7',\n                 'jun_vbc_3g','jul_vbc_3g', 'loc_og_mou_6', 'loc_og_mou_7','std_og_mou_6', 'std_og_mou_7',\n                  'total_og_mou_6', 'total_og_mou_7','loc_ic_mou_6', 'loc_ic_mou_7',\n                  'std_ic_mou_6', 'std_ic_mou_7','total_ic_mou_6', 'total_ic_mou_7',\n                  'total_rech_num_6', 'total_rech_num_7','night_pck_user_6', 'night_pck_user_7',\n                  'monthly_2g_6', 'monthly_2g_7','sachet_2g_6', 'sachet_2g_7',\n                  'monthly_3g_6', 'monthly_3g_7','sachet_3g_6', 'sachet_3g_7','fb_user_6', 'fb_user_7'\n        ], axis=1, inplace=True)\nhighvalue.info(verbose=True, null_counts=True)","931f2f27":"highvalue.shape","af3ed614":"correlation_matrix = highvalue.corr()\nAbsoluteCorrelationMatrix = correlation_matrix.abs()\nAbsoluteCorrelationMatrix = AbsoluteCorrelationMatrix.where(np.triu(np.ones(AbsoluteCorrelationMatrix.shape), k=1).astype(np.bool))\nhighCorrelatedIndices = np.where(AbsoluteCorrelationMatrix > 0.8)\ncorrelated_pairs = [(AbsoluteCorrelationMatrix.index[x], AbsoluteCorrelationMatrix.columns[y]) \n                     for x,y in zip(*highCorrelatedIndices) if x!=y and x < y]\ncorrelated_pairs","d1b7a213":"print(\"Total Number of correlated pairs: \", len(correlated_pairs))","10d28f45":"corr_matrix = highvalue.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.80)]\nto_drop ","bee6912a":"highvalue.drop(columns=to_drop, axis=1,inplace=True)\nhighvalue.info(verbose=True, null_counts=True)","25d86089":"highvalue.shape","10af42ea":"highvalue.drop(columns = list(highvalue.select_dtypes(include=['category']).columns), axis =1, inplace = True)\nhighvalue.info(verbose=True, null_counts=True)","fb33d7cc":"list(highvalue.select_dtypes(include=[object]).columns)","13e89070":"highvalue.drop(columns = list(highvalue.select_dtypes(include=[object]).columns), axis = 1, inplace = True)\nhighvalue.info(verbose=True, null_counts=True)","d20aa924":"#Replace all missing values with zero as discussed above","6473f758":"highvalue.replace([np.inf, -np.inf], np.nan,inplace=True)\nhighvalue.fillna(0,inplace=True)\nhighvalue.info(verbose=True, null_counts=True)","f669aa13":"highvalue.isnull().values.any()","5c9fa3e9":"highvalue.shape","f1af83d4":"highvalue.skew()","ea06b0f4":"X = highvalue[highvalue.columns[~highvalue.columns.isin(['churn_flag'])]]\nX.head()","8b6e0895":"Y = highvalue['churn_flag']\nY.head()","de4b3fad":"from sklearn.preprocessing import PowerTransformer\nscaler = PowerTransformer()\nX_scale= scaler.fit_transform(X)\n","e0271d7f":"# Splitting the data into train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_scale,Y, train_size=0.7,random_state=42)","37e0e973":"print(\"Training dataset size       \",X_train.shape)\nprint(\"Training dataset target size\",y_train.shape)\nprint(\"Test dataset size           \",X_test.shape)\nprint(\"Test dataset target size    \",y_test.shape)","4b63579f":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","b5e408cb":"#Doing the PCA on the train data\npca.fit(X_train)","3a29ff68":"pca.components_","b9096bbf":"colnames = list(X.columns)\npcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\npcs_df.head()","b2138386":"%matplotlib inline\nfig = plt.figure(figsize = (20,10))\nplt.scatter(pcs_df.PC1, pcs_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pcs_df.Feature):\n    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\nplt.tight_layout()\nplt.show()","61d59d3a":"pca.explained_variance_ratio_","02fac206":"#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.vlines(x=90, ymax=1, ymin=0, colors=\"r\", linestyles=\"--\")\nplt.hlines(y=.90, xmax=186, xmin=0, colors=\"g\", linestyles=\"--\")\nplt.grid()\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","f768d6c8":"print(\"pca.explained_variance_ratio_: \",pca.explained_variance_ratio_.round(3)*100)","1eb853c9":"print (pca.explained_variance_ratio_.cumsum())","8355d3a7":"#Using incremental PCA for efficiency - saves a lot of time on larger datasets\nfrom sklearn.decomposition import IncrementalPCA\npca_final = IncrementalPCA(n_components=90)","d6989b2b":"df_train_pca = pca_final.fit_transform(X_train)\ndf_train_pca.shape","e004db4b":"#creating correlation matrix for the principal components\ncorrmat = np.corrcoef(df_train_pca.transpose())","b7eb89ce":"#plotting the correlation matrix\n%matplotlib inline\nplt.figure(figsize = (10,10))\nsns.heatmap(corrmat,cmap='rainbow')\nplt.show()","21c77107":"# 1s -> 0s in diagonals\ncorrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\nprint(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)\n# we see that correlations are indeed very close to 0","08a15f57":"#Applying selected components to the test data\ndf_test_pca = pca_final.transform(X_test)\ndf_test_pca.shape","d2b61473":"#Training the model on the train data\nfrom sklearn.linear_model import LogisticRegression\n\nlearner_pca = LogisticRegression()\nmodel_pca = learner_pca.fit(df_train_pca,y_train)","e9ce2233":"#Making prediction on the test data\npred_probs_test = model_pca.predict_proba(df_test_pca)[:,1]\n\"{:3.3}\".format(metrics.roc_auc_score(y_test, pred_probs_test))","9c297475":"def plotLiftChart(actual, predicted,title_str):\n    plt.figure(figsize = (20,10))\n    df_dict = {'actual': list (actual), 'pred': list(predicted)}\n    df = pd.DataFrame(df_dict)\n    pred_ranks = pd.qcut(df['pred'].rank(method='first'), 100, labels=False)\n    actual_ranks = pd.qcut(df['actual'].rank(method='first'), 100, labels=False)\n    pred_percentiles = df.groupby(pred_ranks).mean()\n    actual_percentiles = df.groupby(actual_ranks).mean()\n    plt.title(title_str)\n    plt.plot(np.arange(.01, 1.01, .01), np.array(pred_percentiles['pred']),\n             color='darkorange', lw=2, label='Prediction')\n    plt.plot(np.arange(.01, 1.01, .01), np.array(pred_percentiles['actual']),\n             color='navy', lw=2, linestyle='--', label='Actual')\n    plt.grid()\n    plt.ylabel('Target Percentile')\n    plt.xlabel('Population Percentile')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([-0.05, 1.05])\n    from pylab import rcParams\n    rcParams['figure.figsize'] = 40, 10\n\n    plt.legend(loc=\"best\")\n    plt.show()","15da56e0":"plotLiftChart(y_test.values,pred_probs_test,\"Simple Logistic Regression\")\n","285e8ada":"pca_last1 = PCA(n_components=50)\ndf_train_pca1 = pca_last1.fit_transform(X_train)\ndf_test_pca1 = pca_last1.transform(X_test)\ndf_test_pca1.shape","2386d8ff":"#training the regression model\nlearner_pca1 = LogisticRegression()\nmodel_pca1 = learner_pca1.fit(df_train_pca1,y_train)\n#Making prediction on the test data\npred_probs_test1 = model_pca1.predict_proba(df_test_pca1)[:,1]\n\"{:3.3f}\".format(metrics.roc_auc_score(y_test, pred_probs_test1))","fafa5fdc":"pca_last2 = PCA(n_components=15)\ndf_train_pca2 = pca_last2.fit_transform(X_train)\ndf_test_pca2 = pca_last2.transform(X_test)\ndf_test_pca2.shape","e9dca642":"#training the regression model\nlearner_pca2 = LogisticRegression()\nmodel_pca2 = learner_pca2.fit(df_train_pca2,y_train)\n#Making prediction on the test data\npred_probs_test2 = model_pca2.predict_proba(df_test_pca2)[:,1]\n\"{:2.2f}\".format(metrics.roc_auc_score(y_test, pred_probs_test2))","9d1cd6a7":"logmodel_CW_dict = LogisticRegression(class_weight='balanced',penalty='l2',random_state=42,solver='newton-cg',C=10000000,n_jobs=-1)","c22d18c3":"logmodel_CW_dict.fit(df_train_pca,y_train)","4ea736a3":"predictions = logmodel_CW_dict.predict(df_test_pca)\nprediction_probs = logmodel_CW_dict.predict_proba(df_test_pca)[:,1]\nprint(classification_report(y_test, predictions))\n","e2175f08":"accuracy = metrics.accuracy_score(y_test, predictions)\nprint(\"Accuracy for the test dataset\",'{:.2%}'.format(accuracy) )\n","0d855171":"fpr, tpr, threshold = metrics.roc_curve(y_test, predictions)\nroc_auc = metrics.auc(fpr, tpr)\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","f8c2617b":"print(\"ROC for the test dataset\",'{:.2%}'.format(roc_auc))\n","54e898f6":"cm = confusion_matrix(y_test, predictions)\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Train Data')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","4710f15d":"plotLiftChart(y_test.values,prediction_probs,\"Logistic Regression with penaly L2\")","a7b8af78":"from sklearn.model_selection  import StratifiedKFold\nstratefied_Kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)","aacffffd":"from sklearn.linear_model import LogisticRegressionCV\n\nsearchCV = LogisticRegressionCV(\n        Cs= list(np.power(10.0, np.arange(-10, 10)))\n        ,penalty='l2'\n        ,scoring='roc_auc'\n        ,cv=stratefied_Kfold\n        ,random_state=42\n        ,max_iter=100\n        ,fit_intercept=True\n        ,solver='newton-cg'\n        ,tol=1\n        ,verbose = 2\n        ,n_jobs = -1\n    \n    )","8b715664":"searchCV.fit(df_train_pca,y_train)","ce5b1092":"print(\"final selected 1\/lambda is \", searchCV.C_)\n","78b9d9cc":"print(\"final selected lambda is \", 1\/searchCV.C_)","3720cc28":"#train data\npreds_L2Search = searchCV.predict(df_test_pca)\npreds_L2Search_probs = searchCV.predict_proba(df_test_pca)[:,1]\n\nprint(classification_report(y_test, preds_L2Search))\n","c25600ce":"accuracy = metrics.accuracy_score(y_test, preds_L2Search)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy) )\n\n","e2228ef5":"fpr, tpr, threshold = metrics.roc_curve(y_test, preds_L2Search_probs)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","ec2bab49":"cm = confusion_matrix(y_test, preds_L2Search)\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","ca3928fb":"plotLiftChart(y_test.values,preds_L2Search_probs,\"L2 penalty Logistic Regression with gridsearch \")","7232ad08":"from sklearn.ensemble import RandomForestClassifier\nRFC_balanced = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None,\n                                      min_samples_split=500, min_samples_leaf=1, \n                                      min_weight_fraction_leaf=0.0, max_features='auto', \n                                      max_leaf_nodes=None, min_impurity_split=0,\n                                      bootstrap=True, oob_score=False, n_jobs=-1, \n                                      random_state=42, verbose=1, warm_start=False, \n                                      class_weight=\"balanced_subsample\")\n","a30c09fd":"TrainedRFC= RFC_balanced.fit(df_train_pca,y_train)","3330c2f9":"preds_RFC=TrainedRFC.predict(df_test_pca)\npreds_probs_RFC=TrainedRFC.predict_proba(df_test_pca)[:,1]\n\nprint(classification_report(y_test, preds_RFC))\n","705b4d20":"accuracy = metrics.accuracy_score(y_test, preds_RFC)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy) )\n\n","7ee673da":"fpr, tpr, threshold = metrics.roc_curve(y_test, preds_probs_RFC)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","99cc5c0d":"cm = confusion_matrix(y_test, preds_RFC)\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","d23caddf":"plotLiftChart(y_test.values,preds_probs_RFC,\"Random Forest\")","76545e02":"importances = TrainedRFC.feature_importances_\ncol_names =  X.columns\n\nsorted_feature_importance = pd.DataFrame(sorted(zip(importances, list(col_names)), reverse=True))\nsorted_feature_importance.rename(columns = {0:'Feature Importance Score', 1:'Feature Name'}, inplace = True) \nsorted_feature_importance","01588a91":"plt.figure(figsize = (20,5))\nsns.barplot(x='Feature Name', y='Feature Importance Score',data=sorted_feature_importance[0:20])\nplt.xticks(rotation = 90)\nplt.title('Random Forest Feature Importances')\nplt.show()","fb7883da":"from sklearn import ensemble\n\n# Fit classifier with out-of-bag estimates\nparams = {'n_estimators': 250, 'max_depth': 3, 'subsample': 0.5,\n          'learning_rate': 0.01, 'min_samples_leaf': 12, 'random_state': 42}\nGBC = ensemble.GradientBoostingClassifier(**params)","a3735727":"GBC.fit(df_train_pca,y_train)","8e84c6d4":"preds_GBC = GBC.predict(df_test_pca)\npreds_probs_GBC = GBC.predict_proba(df_test_pca)[:,1]\n\nprint(classification_report(y_test, preds_GBC))","1eaa8ddc":"accuracy = metrics.accuracy_score(y_test, preds_GBC)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy))","6228141d":"fpr, tpr, threshold = metrics.roc_curve(y_test, preds_probs_GBC)\nroc_auc = metrics.auc(fpr, tpr)\nplt.figure(figsize = (8,8))\nprint(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","2fb24edd":"plt.figure(figsize = (8,8))\ncm = confusion_matrix(y_test, preds_GBC)\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","41f95b44":"plotLiftChart(y_test.values,preds_probs_GBC,\"Gradient Boosting\")","84db42ed":"from sklearn.model_selection import GridSearchCV   #Perforing grid search\n\n# Fit classifier with out-of-bag estimates\nparam_test = {'n_estimators':range(200,400,50), 'max_depth':range(4,10,2)}\n\ngsearch1 = GridSearchCV(\n    estimator = ensemble.GradientBoostingClassifier(\n        learning_rate=0.05,\n        min_samples_split=20,\n        min_samples_leaf=10,\n        subsample=0.1,\n        random_state=42,verbose = 2,), param_grid = param_test, scoring='roc_auc',n_jobs=-1,iid=False, cv=None,verbose=2)\n","599ea61e":"gsearch1.fit(df_train_pca,y_train)","267e2d04":"gsearch1.best_params_, gsearch1.best_score_","42df7d60":"preds_GBC_HT = gsearch1.predict(df_test_pca)\npreds_GBC_probs_HT = gsearch1.predict_proba(df_test_pca)[:,1]\n\nprint(classification_report(y_test, preds_GBC_HT))","63404653":"accuracy = metrics.accuracy_score(y_test, preds_GBC_HT)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy) )","25d663a1":"fpr, tpr, threshold = metrics.roc_curve(y_test, preds_GBC_probs_HT)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","906969da":"cm = confusion_matrix(y_test, preds_GBC_HT)\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","7211875c":"plotLiftChart(y_test.values,preds_GBC_probs_HT,\"Gradient Boosting with hyper parameter tuning\")","d783fcad":"param_test = {'min_samples_leaf': range(10,101,10)}\n\ngsearch1 = GridSearchCV(\n    estimator = ensemble.GradientBoostingClassifier(\n        n_estimators = 100,\n        learning_rate=0.05,\n        max_depth = 5,\n        min_samples_split=100,\n        subsample=0.1,\n        random_state=42,verbose = 2,), param_grid = param_test, scoring='roc_auc',n_jobs=-1,iid=False, cv=None,verbose=2)\n","badd52b8":"gsearch1.fit(df_train_pca,y_train)","ed8a9065":"gsearch1.best_params_, gsearch1.best_score_","7d6ef09e":"preds_GBC_HT = gsearch1.predict(df_test_pca)\npreds_GBC_probs_HT = gsearch1.predict_proba(df_test_pca)[:,1]\n\nprint(classification_report(y_test, preds_GBC_HT))","76d191ad":"accuracy = metrics.accuracy_score(y_test, preds_GBC_HT)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy) )","1ae2151f":"fpr, tpr, threshold = metrics.roc_curve(y_test, preds_GBC_probs_HT)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","d06f987b":"cm = confusion_matrix(y_test, preds_GBC_HT)\nplt.clf()\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","e3776164":"plotLiftChart(y_test.values,preds_GBC_probs_HT,\"Gradient Boosting with hyper parameter tuning\")","864322a7":"from sklearn import svm\n# rbf kernel with other hyperparameters kept to default \nsvm_rbf = svm.SVC(kernel='rbf')\nsvm_rbf.fit(df_train_pca, y_train)\n# predict\npredictions = svm_rbf.predict(df_test_pca)\nprint(\"Classification report: \")\nprint(classification_report(y_test,predictions))","5c7a8ea7":"\naccuracy = metrics.accuracy_score(y_test, predictions)\nprint(\"\\nAccuracy for the test dataset\",'{:.1%}'.format(accuracy) )\n","f33ffe07":"fpr, tpr, threshold = metrics.roc_curve(y_test, predictions)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"\\nROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","a86c4752":"X_train.shape","3239edd4":"from sklearn.model_selection import train_test_split\n\n# We should specify 'random_state' so that the train and test data set always have the same rows, respectively\n\nnp.random.seed(0)\ntrain, test = train_test_split(highvalue, train_size = 0.7, random_state = 42)","557c3f8d":"train.shape,test.shape","bd6b8583":"X_train=train.drop('churn_flag',axis=1)\nX_test=test.drop('churn_flag',axis=1)\ny_train=train['churn_flag']\ny_test=test['churn_flag']","bed6a7a8":"X_train.head()","9e6b0885":"from sklearn.preprocessing import PowerTransformer\nscaler = PowerTransformer()\nX_train[:] = scaler.fit_transform(X_train[:])\nX_train.head()","f286ee0b":"X_test[:] = scaler.transform(X_test[:])\nX_test.head()","b8156b12":"from sklearn.feature_selection import RFE\n# Running RFE with the output number of the variable equal to 90\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\n\nrfe = RFE(lr, 90)             # running RFE\nrfe = rfe.fit(X_train, y_train)","b746ca8b":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","f3a9b85d":"col = X_train.columns[rfe.support_]\ncol","1043a11d":"X_train.columns[~rfe.support_]","603e54cf":"#Assign the 90 columns to X_train_rfe\n\nX_train_rfe = X_train[col]","cb051553":"#Associate the new 90 columns to X_train and X_test for further analysis\n\nX_train = X_train_rfe[X_train_rfe.columns]\nX_test =  X_test[X_train.columns]","35cab145":"# xgboost","accc1e3d":"params = {\n    'max_depth': [9,10,11],\n    'n_estimators':[250,275,300],\n    }","6a2f90ec":"import xgboost as xgb\nxgb= xgb.XGBClassifier(random_state=42,learning_rate=0.05)\ngrid_search = GridSearchCV(estimator=xgb,\n                           param_grid=params,\n                           cv=5,\n                           n_jobs=-1, \n                           verbose=2)","1b3a9419":"%%time\ngrid_search.fit(X_train, y_train)\n","7fe8b8f8":"grid_search.best_score_","b46ec227":"xgb_best = grid_search.best_estimator_\nxgb_best","74bbe757":"import xgboost as xgb\nxgb_model = xgb.XGBClassifier(\n    n_estimators=275,\n    learning_rate=0.05,\n    n_jobs=-1,\n    max_depth=9,\n    colsample_bytree=1\n)\n\nxgb_model.fit(X=X_train,y=y_train)","7ef2d689":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(xgb_model, X_train, y_train, scoring = 'roc_auc')\nprint(\"Scores:\", scores)\n","440c0f58":"print(\"Mean:\", scores.mean())\n","2be82731":"print(\"Standard Deviation:\", scores.std())","8cd332d9":"predictions_train = xgb_model.predict(X_train)\nprint(classification_report(y_train, predictions_train))\n","1802e646":"predictions = xgb_model.predict(X_test)\nprint(classification_report(y_test, predictions))","a1b2b7e9":"def Performance(Model,Y,X):\n    # Perforamnce of the model\n    fpr, tpr, _ = roc_curve(Y, Model.predict_proba(X)[:,1])\n    AUC  = auc(fpr, tpr)\n    print ('the AUC is : %0.4f' %  AUC)\n    plt.figure(figsize = (8,8))\n    plt.plot(fpr, tpr, label='ROC curve (area = %0.4f)' % AUC)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()","63453319":"Performance(Model=xgb_model,Y=y_test,X=X_test)","b80f8f15":"plotLiftChart(y_test.values,xgb_model.predict_proba(X_test)[:,1],\"XGBoost with tuned parameters\")","3c32247f":"params = {\n    'max_depth': [10,15,20],\n    'n_estimators':[175,200,225]\n    }","3de6aff9":"from sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(random_state=42)\ngrid_search = GridSearchCV(estimator=rf,\n                           param_grid=params,\n                           cv=5,\n                           n_jobs=-1, verbose=2)","532fa6ff":"%%time\ngrid_search.fit(X_train, y_train)\n","20c599cb":"grid_search.best_score_","e174ae22":"rf_best = grid_search.best_estimator_\nrf_best","0fa37e9d":"from sklearn.ensemble import RandomForestClassifier\n\nRF2=RandomForestClassifier(n_estimators=225, criterion= 'entropy', max_depth=10,random_state=42, class_weight='balanced')\nRF2.fit(X_train, y_train)","a0898e0b":"scores = cross_val_score(RF2, X_train, y_train, scoring = 'roc_auc')\nprint(\"Scores:\", scores)\n","c76be12c":"print(\"Mean:\", scores.mean())\n","22ba46f4":"print(\"Standard Deviation:\", scores.std())","0ed8497a":"predictions_train = RF2.predict(X_train)\nprint(classification_report(y_train, predictions_train))\n","0c504219":"predictions = RF2.predict(X_test)\nprint(classification_report(y_test, predictions))","b1747ad0":"preds_probs_RFC=RF2.predict_proba(X_test)[:,1]\nPerformance(Model=RF2,Y=y_test,X=X_test)","2ca24169":"plotLiftChart(y_test.values,preds_probs_RFC,\"Random Forest with tuned parameters\")","1d7c1a05":"importances = RF2.feature_importances_\ncol_names =  X.columns\nsorted_feature_importance = pd.DataFrame(sorted(zip(importances, list(col_names)), reverse=True))\nsorted_feature_importance.rename(columns = {0:'Feature Importance Score', 1:'Feature Name'}, inplace = True) \nsorted_feature_importance\n\n","f99f4843":"plt.figure(figsize = (20,5))\nsns.barplot(x='Feature Name', y='Feature Importance Score',data=sorted_feature_importance[0:20])\nplt.xticks(rotation = 90)\nplt.title('Random Forest Feature Importances')\nplt.show()","0d43d9da":"from sklearn import svm \n\nsvm_linear = svm.SVC(kernel='linear')\nsvm_linear.fit(X_train, y_train)","ad3f1476":"preds_SVM_lin = svm_linear.predict(X_test)\nprint(classification_report(y_test, preds_SVM_lin))","36cf2faa":"accuracy = metrics.accuracy_score(y_test, preds_SVM_lin)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy) )","80ead9c9":"fpr, tpr, threshold = metrics.roc_curve(y_test, preds_SVM_lin)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","11e68a66":"cm = confusion_matrix(y_test, preds_SVM_lin)\nplt.figure(figsize = (8,8))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","cfd9763d":"svm_rbf = svm.SVC(kernel='rbf')\nsvm_rbf.fit(X_train, y_train)","5bdc8427":"preds_SVM_rbf = svm_rbf.predict(X_test)\nprint(classification_report(y_test, preds_SVM_rbf))","9470d64e":"accuracy = metrics.accuracy_score(y_test, preds_SVM_rbf)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy) )","b4e15cce":"fpr, tpr, threshold = metrics.roc_curve(y_test, preds_SVM_rbf)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","c6bcadab":"cm = confusion_matrix(y_test, preds_SVM_rbf)\nplt.figure(figsize = (8,8))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","aff3e56d":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n# base estimator: a weak learner with max_depth=2\nshallow_tree = DecisionTreeClassifier(max_depth=2, random_state = 42)\n# fit the shallow decision tree \nshallow_tree.fit(X_train, y_train)\n# test error\ny_pred = shallow_tree.predict(X_test)\nscore = metrics.accuracy_score(y_test, y_pred)\nscore","39a3e811":"# adaboost with the tree as base estimator\n\nestimators = list(range(50, 101, 2))\n\nabc_scores = []\nfor n_est in estimators:\n    ABC = AdaBoostClassifier(\n    base_estimator=shallow_tree, \n    n_estimators = n_est)\n    \n    ABC.fit(X_train, y_train)\n    y_pred = ABC.predict(X_test)\n    score = metrics.accuracy_score(y_test, y_pred)\n    abc_scores.append(score)","76bba856":"abc_scores","46224972":"#after seeing the above score we can say that adaboost is providing better results when estimate is 96\nABC = AdaBoostClassifier(\nbase_estimator=shallow_tree, \n    n_estimators = 96)\n    \nABC.fit(X_train, y_train)\npredictions = ABC.predict(X_test)\nscore = metrics.accuracy_score(y_test, predictions)\naccuracy = metrics.accuracy_score(y_test, predictions)\nprint(\"Classification Report:\")\nprint(classification_report(y_test,predictions))","ca6e4342":"fpr, tpr, threshold = metrics.roc_curve(y_test, predictions)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy) )","7a79962a":"print(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","74d0a715":"cm = confusion_matrix(y_test, predictions)\nplt.figure(figsize = (8,8))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","aebb4010":"# training the NB model and making predictions\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.preprocessing import MinMaxScaler\nMinmaxscaler = MinMaxScaler()\nX_scaled = Minmaxscaler.fit_transform(X)\n\nX_train_NB, X_test_NB, y_train_NB, y_test_NB = train_test_split(X_scaled,Y, train_size=0.8,test_size=0.2,random_state=111)\n\nmnb = MultinomialNB()\n\n# fit\nmnb.fit(X_train_NB,y_train_NB)\n\n# predict class\npredictions = mnb.predict(X_test_NB)\n\n# predict probabilities\ny_pred_proba = mnb.predict_proba(X_test_NB)\naccuracy = metrics.accuracy_score(y_test_NB, predictions)\nprint(\"Classification Report:\")\nprint(classification_report(y_test_NB,predictions))","c532c126":"fpr, tpr, threshold = metrics.roc_curve(y_test_NB, predictions)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy) )\n","ceeb394f":"print(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","c5c9d9bf":"from imblearn.over_sampling import SMOTE\nprint(\"Number transactions X_train dataset: \", X_train.shape)\n","6752635e":"print(\"Number transactions y_train dataset: \", y_train.shape)\n","4c9f63b1":"print(\"Number transactions X_test dataset: \", X_test.shape)\n","0c2a741f":"print(\"Number transactions y_test dataset: \", y_test.shape)","9ae1b231":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\n","02beb7c5":"print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))","410aff72":"sm = SMOTE(random_state=42)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))","599078e1":"print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))","b885f94a":"print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n","b6755de5":"print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","987d7d5d":"from sklearn.linear_model import LogisticRegressionCV\n\nsearchCV = LogisticRegressionCV(\n        Cs= list(np.power(10.0, np.arange(-10, 10)))\n        ,penalty='l2'\n        ,scoring='roc_auc'\n        ,cv=stratefied_Kfold\n        ,random_state=42\n        ,max_iter=100\n        ,fit_intercept=True\n        ,solver='newton-cg'\n        ,tol=1\n        ,verbose = 2\n        ,n_jobs = -1\n    \n    )","3f12e188":"searchCV.fit(X_train_res,y_train_res)","ef49f737":"#train data\npreds_L2Search = searchCV.predict(X_test)\npreds_L2Search_probs = searchCV.predict_proba(X_test)[:,1]\n\nprint(classification_report(y_test, preds_L2Search))","56df09e8":"accuracy = metrics.accuracy_score(y_test, preds_L2Search)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy) )","38ee82ba":"fpr, tpr, threshold = metrics.roc_curve(y_test, preds_L2Search_probs)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","9378692b":"cm = confusion_matrix(y_test, preds_L2Search)\nplt.figure(figsize = (8,8))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Train Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","5ce4c105":"plotLiftChart(y_test.values,preds_L2Search_probs,\"Logistic Regression with penaly L2\")","5d0514b7":"TrainedRFC= RFC_balanced.fit(X_train_res,y_train_res)","8ad4d19e":"preds_RFC=TrainedRFC.predict(X_test)\npreds_probs_RFC=TrainedRFC.predict_proba(X_test)[:,1]\n\nprint(classification_report(y_test, preds_RFC))","ad2d37e9":"accuracy = metrics.accuracy_score(y_test, preds_RFC)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy) )","b188a62b":"fpr, tpr, threshold = metrics.roc_curve(y_test, preds_probs_RFC)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","6f1dbe92":"cm = confusion_matrix(y_test, preds_RFC)\nplt.figure(figsize = (8,8))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","748c8170":"plotLiftChart(y_test.values,preds_probs_RFC,\"Random Forest with SMOTE\")","cbf1d2ef":"importances = TrainedRFC.feature_importances_\ncol_names =  X.columns\nsorted_feature_importance = pd.DataFrame(sorted(zip(importances, list(col_names)), reverse=True))\nsorted_feature_importance.rename(columns = {0:'Feature Importance Score', 1:'Feature Name'}, inplace = True) \nsorted_feature_importance","70dfe3a9":"plt.figure(figsize = (20,5))\nsns.barplot(x='Feature Name', y='Feature Importance Score',data=sorted_feature_importance[0:20])\nplt.xticks(rotation = 90)\nplt.title('Random Forest Feature Importances')\nplt.show()","b97fce2a":"xgb_model = xgb.XGBClassifier(\n    n_estimators=275,\n    learning_rate=0.05,\n    n_jobs=-1,\n    max_depth=9,\n    colsample_bytree=1\n)\n\nxgb_model1 = xgb_model.fit(X_train_res,y_train_res)","c48eebc9":"predictions_train = xgb_model1.predict(X_train_res)\nprint(classification_report(y_train_res, predictions_train))\n","39245037":"predictions = xgb_model1.predict(X_test)\nprint(classification_report(y_test, predictions))","1f51af62":"from sklearn.metrics import accuracy_score,precision_recall_fscore_support,fbeta_score\naccuracy = accuracy_score(y_test, predictions)\nprecision_recall_fscore_support(y_test, predictions, average='macro')\nprecision, recall, f1score,blah  = precision_recall_fscore_support(y_test, predictions, average='macro')\nfbetascore = fbeta_score(y_test, predictions, average='weighted', beta=0.8)\ntn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\nspecificity = tn \/ (tn+fp)\nsensitivity = tp \/ (tp+fn)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","9a050c64":"print(\"Precision: %.2f%%\" % (precision*100))\n","da66e951":"print(\"Recall: %.2f%%\" % (recall*100))\n","e903ac42":"print(\"F1 Score: %.2f%%\" % (f1score*100))\n","4cd93243":"print(\"F Beta score: %.2f%%\" % (fbetascore*100))\n","b0af1ba2":"print(\"Specificity: %.2f%%\" % (specificity*100))\n","62581b0d":"print(\"Sensitivty: %.2f%%\" % (sensitivity*100))","7c28d407":"Performance(Model=xgb_model1,Y=y_test,X=X_test)","8fbfc183":"plotLiftChart(y_test.values,xgb_model1.predict_proba(X_test)[:,1],\"XGBoost on SMOTE\")","be1d74e3":"importances = xgb_model1.feature_importances_\ncol_names =  X.columns\n\nsorted_feature_importance = pd.DataFrame(sorted(zip(importances, list(col_names)), reverse=True))\nsorted_feature_importance.rename(columns = {0:'Feature Importance Score', 1:'Feature Name'}, inplace = True) \nsorted_feature_importance\n\n\n","3938cff7":"plt.figure(figsize = (20,5))\nsns.barplot(x='Feature Name', y='Feature Importance Score',data=sorted_feature_importance[0:20])\nplt.xticks(rotation = 90)\nplt.title('XGB Feature Importances')\nplt.show()","becc36aa":"svm_linear = svm.SVC(kernel='linear')\nsvm_linear.fit(X_train_res, y_train_res)","8efc9d4c":"preds_SVM_li_RS = svm_linear.predict(X_test)\n\nprint(classification_report(y_test, preds_SVM_li_RS))","d3ca4afb":"accuracy = metrics.accuracy_score(y_test, preds_SVM_li_RS)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy) )","83b1ff81":"fpr, tpr, threshold = metrics.roc_curve(y_test, preds_SVM_li_RS)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","052331be":"cm = confusion_matrix(y_test, preds_SVM_li_RS)\nplt.figure(figsize = (8,8))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","312fa187":"svm_rbf = svm.SVC(kernel='rbf')\nsvm_rbf.fit(X_train_res, y_train_res)","b3d842e4":"preds_SVM_RBF_RS = svm_rbf.predict(X_test)\nprint(classification_report(y_test, preds_SVM_RBF_RS))","2af5794f":"accuracy = metrics.accuracy_score(y_test, preds_SVM_RBF_RS)\nprint(\"Accuracy for the test dataset\",'{:.1%}'.format(accuracy) )","bad890d0":"fpr, tpr, threshold = metrics.roc_curve(y_test, preds_SVM_RBF_RS)\nroc_auc = metrics.auc(fpr, tpr)\nprint(\"ROC for the test dataset\",'{:.1%}'.format(roc_auc))\nplt.figure(figsize = (8,8))\nplt.grid()\nplt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc )\nplt.legend(loc=4)\nplt.show()","df67c937":"cm = confusion_matrix(y_test, preds_SVM_RBF_RS)\nplt.figure(figsize = (8,8))\nplt.imshow(cm, interpolation='nearest', cmap=plt.cm.Pastel1)\nclassNames = ['Non-churn','Churn']\nplt.title('Confusion Matrix - Test Data')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\ntick_marks = np.arange(len(classNames))\nplt.xticks(tick_marks, classNames, rotation=45)\nplt.yticks(tick_marks, classNames)\ns = [['TN','FP'], ['FN', 'TP']]\n \nfor i in range(2):\n    for j in range(2):\n        plt.text(j,i, str(s[i][j])+\" = \"+str(cm[i][j]),fontsize=12)\nplt.show()","ea743bf7":"importances = RF2.feature_importances_\ncol_names =  X.columns\n\nsorted_feature_importance = pd.DataFrame(sorted(zip(importances, list(col_names)), reverse=True))\nsorted_feature_importance.rename(columns = {0:'Feature Importance Score', 1:'Feature Name'}, inplace = True) \nsorted_feature_importance","be38100e":"imp_features = sorted_feature_importance.head(20)\nimp_features","e64160b6":"X_imp = highvalue[list(imp_features['Feature Name'].unique())]\nY_imp = highvalue['churn_flag']","cfd2f1e7":"X_imp_std = scaler.fit_transform(X_imp)\nX_imp_train, X_imp_test, y_imp_train, y_imp_test = train_test_split(X_imp_std,Y_imp, train_size=0.8,test_size=0.2,random_state=42)","78114da3":"RF_imp=RandomForestClassifier(n_estimators=225, criterion= 'entropy', max_depth=10,random_state=42, class_weight='balanced')\nRF_imp.fit(X_imp_train, y_imp_train)","8cd7006f":"RF_imp_pred_test = RF_imp.predict(X_imp_test)\nprint(classification_report(y_imp_test, RF_imp_pred_test))","ecd09024":"Performance(Model=RF_imp,Y=y_imp_test,X=X_imp_test)","fa2209e4":"imp_features","cf2a0d44":"Create month on month change features to understand any risk associated with it","ede9b87e":"## High-value Churn","3d98b938":"# Derived features","be756ea9":"churn_flag","955b7659":"\nIn the Indian and the southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\n\n \n\nIn this project, we will define high-value customers based on a certain metric (mentioned later below) and predict churn only on high-value customers.","68f61b4c":"# Duplicate Check","e829d810":"### Drop date related features as we derived new features out of it","5466ecfd":"# Exploratory Analysis","0e0c8961":"We train our random forest algorithm on the top 20 features that we selected above and then we evaluated the performance of our algorithm on the training and testing sets. We can see the AUC & F1-score are pretty similar on training & test datasets which means our model is not overfitting.","8c35294d":"# isd_og \n(international calling)","833b23f6":"Let's check if we have missing values in the dataset?","175ecf5d":"Create new features from the date columns\n\nThis may help us identifying if a particular day in a month or a week causing any issue","53607bd1":"#### Change in min_samples_leaf\n\nNow let's tune min_samples_leaf\n- min_samples_leaf : (default=1)The minimum number of samples required to be at a leaf node:\n    - If int, then consider min_samples_leaf as the minimum number.\n    - If float, then min_samples_leaf is a percentage and ceil(min_samples_leaf * n_samples) are the minimum number of samples for each node.","100c2841":"As per the definition of high value customers are those whose recharge amount should be more than or equal to 70th percentile.\n\nBut if we use greater than or equal to, we end up getting a little above 30K records as against to 29.9K records.\nSo, if we consider only greater than, then we get 29.9K records.\nTherefore we have considered greater than to filter high value customers","e07c54a6":"We will split the data into a training and a test part.\n\nThe models will be trained on the training data set and tested on the test data set","c11b346f":"We can observe the top features selected by the XG Boost & Random Forest is similar. So, we can conclude that these are the important indicators of churn","4da0b5ea":"## Recommendations to Business ","45cb92be":"## Summary about the models","6ba75a34":"# Naive Bayes","a5654b36":"# total_rech_num_\n(Number of times a subscriber recharged his mobile)","963ca389":"# loc_ic_ \n(incoming local calls)","f18d4f42":"We can infer that the customers hasn't bought the data at all in 8th month.\n\nLet's fill the gap as 0","c61f5bf8":"#### Important features or indicators","e8b430ed":"# std_ic_ \n(Outside circle incoming calls)","82a4b8b2":"Let's start with one of the highest missing feature arpu_3g_8","c8fefb00":"# Data Preparation","e911c0eb":"There are various ways to define churn, such as:\n\n**Revenue-based churn:** Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as \u2018customers who have generated less than INR 4 per month in total\/average\/median revenue\u2019.\n\n \n\nThe main shortcoming of this definition is that there are customers who only receive calls\/SMSes from their wage-earning counterparts, i.e. they don\u2019t generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas.\n\n \n\n**Usage-based churn:** Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.\n\n \n\nA potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a \u2018two-months zero usage\u2019 period, predicting churn could be useless since by that time the customer would have already switched to another operator.\n\n \n\nIn this project, we will use the **usage-based definition** to define churn.\n\n","38cf4838":"Cross validating the models showed that the accuracy values were in fact not arbitary and proofed that both models are performing very well.\n\nA high mean corresponds to a more stable performance and a low standard deviation corresponds to smaller range of results.","e6ad2fff":"Using 186 variables is computational expensive , so we will use best 90 features for model building & evaluating models ","4ae8fc21":"The dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively. \n\n\nThe **business objective** is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.","32f8d27b":"### Support Vector Machine\n\n\nSupport Vector Machine (SVM) is an algorithm used for classification problems similar to Logistic Regression (LR). LR and SVM with linear Kernel generally perform comparably in practice. <br> The objective of the support vector machine algorithm is to find the hyperplane that has the maximum margin in an N-dimensional space that distinctly classifies the data points. Data points falling on either side of the hyperplane can be attributed to different classes. <br> The dimension of the hyperplane depends upon the number of features. If the number of input features is 2, then the hyperplane is just a line. If the number of input features is 3, then the hyperplane becomes a two-dimensional plane. Support vectors are data points that are closer to the hyperplane and influence the position and orientation of the hyperplane. Using these support vectors, we maximize the margin of the classifier. Deleting the support vectors will change the position of the hyperplane.","5669ff21":"### SVM with Non linear kernal\n\nRadial Basis Function (RBF)","5809fcc9":"# PCA","48309f02":"We will now train different models on this dataset.\n\nScaling all values will reduce the distortion due to exceptionally high values and make algorithms converge faster","6aea9a0e":"# Ada Boost(Adaptive Boost)","3edd844b":"# Random Forest","5d8544fb":"We can observe that the range is huge.\nWe can't go with either mean or median as they can skew\/distort the whole scenario.\nLet's check data related variables of 8th month to decide on filling up the missing values   ","65ab5d04":"### Resampling techniques for Balancing the data set","9730857f":"### Imbalanced Data sets\n- Machine Learning algorithms tend to produce unsatisfactory classifiers when faced with imbalanced datasets.\n    For any imbalanced data set, if the event to be predicted belongs to the minority class and the event rate is less than 5%,\n    it is usually referred to as a rare event.\n- Standard classifier algorithms like Decision Tree and Logistic Regression have a bias towards classes which have number of instances. They tend to only predict the majority class data. The features of the minority class are treated as noise and are often ignored. Thus, there is a high probability of misclassification of the minority class as compared to the majority class\n\n\n### Approaches to handle Imbalanced data sets\n- Resampling Technique\n- Algorithmic Ensemble techinique\n","c7f65a61":"# total_ic_ \n(All incoming calls received by a person)","91025cc1":"### Gradient Boosting with Hyperparameter tuning\n\nLet's now try to tune hyperparameters in gradient boosting classifier. Start with with two hyperparameters - n_estimators and max_depth.\n\n- n_estimators: integer, optional (default=100): The number of trees in the forest\n- max_depth : integer or None, optional (default=None)The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples\n\n#### Change in estimators & max_depth","3e953519":"The goal of EDA is to determine what our data can tell us!!! In this section, we make plots and find patterns, relationships etc.","a844dee9":"The following data preparation steps are crucial for this problem:\n\n \n\n**1. Derive new features**\n\nThis is one of the most important parts of data preparation since good features are often the differentiators between good and bad models. Use your business understanding to derive features you think could be important indicators of churn.\n\n \n\n**2. Filter high-value customers**\n\nAs mentioned above, you need to predict churn only for the high-value customers. Define high-value customers as follows: Those who have recharged with an amount more than or equal to X, where X is the **70th percentile** of the average recharge amount in the first two months (the good phase).\n\n \n\nAfter filtering the high-value customers, you should get about 29.9k rows.\n\n \n\n**3. Tag churners and remove attributes of the churn phase**\n\nNow tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. The attributes you need to use to tag churners are:\n\n- total_ic_mou_9\n\n- total_og_mou_9\n\n- vol_2g_mb_9\n\n- vol_3g_mb_9\n\n\nAfter tagging churners, **remove all the attributes corresponding to the churn phase** (all attributes having \u2018 _9\u2019, etc. in their names).\n\n \n\n","d30129f5":"## Model Building\n\n#### On original features (instead of principal components)","19bf2b27":"### Create Stratefied Kfold Samples","4642042f":"- In here we used 90 components, which could nearly explain 90% variance in dataset\n- We were able to achieve 91% Area under the curve on the test set\n\nLet's build couple of more models with different number of components. Let's see if the results changes","4a2a9311":"It's clearly evident from the above table that most of the missing values are from mobile data related only. We can follow the same approach and fill the gap.","68799d09":"# Offnet_mou \n(Calls outside of the operator network)","0bdde0e3":"The flags 0 & 1 are Non-churn and churn respectively.\n\nWe can clearly see that the churn customers are 10.87% only.\n\nSo, we can infer that the dataset is an imbalanced dataset.","c1d9eaed":"#### Fact \nImbalance means that the number of data points available for the classes is different. Let's say if there are two classes, then balanced data would mean 50% points for each of the class. For most machine learning techniques, little imbalance is not a problem. So, if there are 55% points for one class and 45% for the other class, it should not cause any significant performance degradation. Only when the class imbalance is high, e.g. 80%-90% points for one class and 20%-10% for the other, standard optimization criteria or performance measures may not be as effective and would need modification.","de147fed":"### Random Forest\n\nOn resampled dataset","6ee2b3b0":"# std_og_mou \n(Outside calling circle\/zone calls)","77a76b0a":"n observations with p features can be interpreted as n points in a p-dimensional space. PCA aims to project this space into a q-dimensional subspace (with q<p) with as little information loss as possible.\n\nIt does so by finding the q directions in which the n points vary the most (the principal components). It then projects the original data points into the q-dimensional subspace. PCA returns a n x q dimensional matrix.\n\nUsing PCA on our data will decrease the amount of operations during training and testing","674e3cc7":"# Feature Selection","4d9ccfee":"# Problem Statement","cb109ac8":"### Lift Metric\n<br>\nWhat is Lift measure? <br>\n\nLift measures expected benefit with the predictive model compared to a base line model \u2013 in essence a random choice. It is the ratio of proportion of the gain to the proportion of random expectation at any given decile level. The random expectation at the xth decile is x%.","a0ce51aa":"# total_og_\n(All outgoing calls by the customer)","f57b2a2d":"## Missing value treatment","699bc7b4":"#### What feature selection techniques could be used to reduce the number of features? \n\nThere are several ways for feature selection:\nTo begin with all feature selection starts with some business domain knowledge. Variables that are relevant, available prior to modeling should be used. \n- Variables with no variance or single value should be eliminated. We did such an analysis of understanding the data and eliminated the variables. \n- Nominal data with no specific relevant to the modeling exercise can be removed- like mobile_number. \n- Variables like Date can be converted to numerical by extracting features like day of a week, day of a month etc.\n- Categorical variables have to be encoded as dummy variables\n\nOther more automated methods for feature selection are:\n- Filter Methods \u2013 features are selected on the basis of their scores in various statistical tests for their relationship with the outcome variable. Eg: Correlation, Anova, Linear Discriminant analysis, Chi-square test \n- Wrapper Methods \u2013 Is like a search problem and is iterative.  We add\/delete a subset of features, train a model iteratively until the best possible model is found. Examples are:\n    - Forward selection\n    - Backward selection\n    - Recursive feature elimination\n\n\n3)\tEmbedded Methods - Embedded methods combine the qualities of both filter and wrapper methods. These Algorithms that have their own built-in feature selection methodology implement it. Examples are:\n    - Lasso Regression\n    - Ridge Regression\n    - Random Forest with Gini coefficient\n    - Gradient boosting with feature importance\n","a4a38065":"# Modelling","34fba710":"There are two main models of payment in the telecom industry - **postpaid** (customers pay a monthly\/annual bill after using the services) and **prepaid** (customers pay\/recharge with a certain amount in advance and then use the services).\n\n \n\nIn the postpaid model, when customers want to switch to another operator, they usually inform the existing operator to terminate the services, and you directly know that this is an instance of churn.\n\n \n\nHowever, in the prepaid model, customers who want to switch to another network can simply stop using the services without any notice, and it is hard to know whether someone has actually churned or is simply not using the services temporarily (e.g. someone may be on a trip abroad for a month or two and then intend to resume using the services again).\n\n \n\nThus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term \u2018churn\u2019 should be defined carefully.  Also, prepaid is the most common model in India and southeast Asia, while postpaid is more common in Europe in North America.\n\n \n\nThis project is based on the Indian and Southeast Asian market.\n\n \n\n","82caec30":"# mobile_number","121d57f6":"### Random Forest \n<br>\n\nRandom forest algorithm is based on a concept called bagging - bootstrap aggregating. In random forest, we build a number of decision trees on bootstrap training samples. The samples are chosen, as split candidates from all the X variables (predictors). <br>\nSo, the random forest at each split is allowed to consider a subset of the predictors. If not, One or Few strong predictors, might always become the most important predictor leading to similar looking trees. Which leads to no reduction in the variance. This process can be thought of as de-correlating the trees or feature bagging. Because of this, the Random forest can also be used to rank the importance of variables. <br>\nSimilarly bootstrapping of sample rows is also done. <br>\n\n\nLet's first fit a random forest model with default hyperparameters.","4eeb6a85":"- From the above we can clearly infer that calls related incoming & outgoing within a circle is key for identifying churn customers\n- Age on network is also a key indicator for identifying the churn, if aon is less than 500 days and their usage is reduce then the customer is going to be churned.\n- Especially the usage during the action phase when compared to good month (7th month) has reduced.\n- The usage of mobile data has also reduced when compared with 7th month. So, data usage & amount can also be the other features for understanding the churn behavior.\n- The other key factor is if the recharge amount shows a dip in the action phase is sign of churn.\n","28cdc125":"### Gradient boosting\n<br>\nBoosting is a method of converting weak learners into strong learners. In boosting, each new tree is a fit on a modified version of the original data set. <br> \n\nGradient boosting uses the concept of boosting. It also fits separate decision trees to the training data set but the trees are grown sequentially essentially to fix the mistakes of the previous tree. This is why, boosting is called the ensemble of weak prediction models. Suppose the gradient boosting model starts with a weak model Fm. In the next iteration it tries to add to the previous weak model, in such a way, that the prediction becomes stronger, i.e; <br>\n>                           Fm+1(x) =  Fm (x) + h(x)= y\n\nHere h is essentially a residual from the previous model, so gradient boosting is a generalization of the gradient decent algorithm that reduces the residuals.","866882cf":"# Load the data","c3463410":"Let's check the correlation between features","ab8fec93":"## Choice of Evaluation Metric\n\nChoosing a metric is one of the key criteria for our analysis, as the dataset is imbalanced one\n\n\n**Accuracy = correct predictions \/ total predictions = (TP + TN)\/ (all)**\n\nFor a data set which is balanced, this might make a good measure. For an imbalanced dataset like the current one, where the negative class is heavier, this measure may show a high accuracy but fail to serve the purpose of the modeling exercise. We need to be able to predict non-churn i.e.; positive outcomes correctly to help business. So Accuracy may not be the best measure here.\n\n**Recall(sensitivity) = ability to identify Positives as true =  TP\/(TP + FN)**\n\nConsidering that we wish to clearly identify customers who might churn vs non-churn, recall as a metric with its ability to identify true positives might be more apt. But this does not capture models ability to mis-classify negative class as positive i.e.; False positive rate. So, if we choose a model purely based on recall, we might be blind sighted by a huge number of false positive.\n\n**Precision = TP\/ (TP + FP) = Positive predictive value:**\n\nIt is important to not only understand the rate of identifying positives as true but also identify False positives. If business is to make a decision on marketing spend to retain customers to all positive output of the model, it becomes important that the false positive rate should not be too high. So Precision as a metric - which provides the rate of true positive \/total predicted positives is an apt metric to look at here.\n\n**F Score = 2 * Precision * Recall\/ (Precision * Recall):**\n\nFscore is the harmonic mean of Precision and Recall and provides the ability to balance the need between having a better true positive rate and not too high a false positive rate. \n\n**Area under the Curve:**\n\nAUC or in the classification world \u2013 AUC ROC \u2013 Area under the curve for  Receiver Operating Characteristic curve is generally used to compare two different models. It used two metrics from the confusion matrix the true positive rate and false positive rate.\n\n**True positive rate (TPR), recall** \n\nIt's described above is proportion of positive data points correctly identified as positive by the model\n\n**False positive rate (FPR)**\n\nIt's defined as FP\/(FP+TN) is all the negative data points that are mistakenly identified as positive as a proportion of all negative data points. \n\nThe TPR and FPR metrics are calculated for several threshold values and a curve is plotted with these. The resulting curve is called ROC.\n\nAUC is the computation of the area under ROC curve. \n\nFor a model selection, we always look for maximizing the AUC for the classifier. As this value is comparable across models.","392b48df":"# arpu \n(Average Revenue per user)","59bf0e54":"Customers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are **three phases**  of customer lifecycle :\n\nThe \u2018good\u2019 phase: In this phase, the customer is happy with the service and behaves as usual.\n\nThe \u2018action\u2019 phase: The customer experience starts to sore in this phase, for e.g. he\/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the \u2018good\u2019 months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor\u2019s offer\/improving the service quality etc.)\n\nThe \u2018churn\u2019 phase: In this phase, the customer is said to have churned. We **define churn based on this phase**. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1\/0 based on this phase, you discard all data corresponding to this phase.\n\n \n\nIn this case, since we are working over a four-month window, the first two months are the \u2018good\u2019 phase, the third month is the \u2018action\u2019 phase, while the fourth month is the \u2018churn\u2019 phase.\n\n \n\n","4aae7e2c":"### Logistic Regression - with penalty","a0d4ba17":"Let's check the other missing feature 'isd_og_mou_8'","5a717b93":"# vol_3g_mb_\nInternet usage in MB","ff3c0899":"### arpu_3g_8","f315898f":"#### Difference between random forest and gradient boosting\n\n| Random Forest | Gradient Boosting |\n| --- | --- |\n| Many trees \u2013 different samples \u2013 aggregated voting for class prediction | Sequential trees \u2013 each improving on the prior tree \u2013 each tree improves classification |\n| Reduces variance, with minimal increase in bias | Minimizes bias |\n| Independent trees | Dependent trees |\n| Bagging  + Decision Trees | Boosting + Gradient Descent |\n| Hyperparameters \u2013 well established, easy to apply | Hyperparameters \u2013 need care in configuration |","581c2dc6":"## Definitions of Churn","4b300c68":"The data dictionary contains meanings of abbreviations. Some frequent ones are loc (local), IC (incoming), OG (outgoing), T2T (telecom operator to telecom operator), T2O (telecom operator to another operator), RECH (recharge) etc.\n\n \n\nThe attributes containing 6, 7, 8, 9 as suffixes imply that those correspond to the months 6, 7, 8, 9 respectively.","72e8b0d9":"No Duplicates Found","4c16a0d1":"# roam_ic_mou\n(Customer in roaming zone)","d2531973":"## Data Dictionary","3e41320c":"Let's drop all features related to churn phase i.e. 9th month","00c68369":"### SVM Linear\n\nOn a resampled data","3ba57a25":"## Understanding and Defining Churn","37558006":"# total_rech_amt_ \nTotal recharge amount in a particular month","37db3755":"### Resampling Techniques:\n- Random under sampling\n    - Balance class distribution by randomly eliminating majority class examples.  This is done until the majority and minority class instances are balanced out.\n    \n- Random Over sampling\n    - Increase the number of instances in the minority class by randomly replicating them in order to present a higher representation of the minority class in the sample.\n    \n- Cluster based over sampling\n    - the K-means clustering algorithm is independently applied to minority and majority class instances. This is to identify clusters in the dataset. Subsequently, each cluster is oversampled such that all clusters of the same class have an equal number of instances and all classes have the same size.\n    \n- Synthetic Minority Over sampling Technique (SMOTE)\n    - This technique is followed to avoid overfitting which occurs when exact replicas of minority instances are added to the main dataset. A subset of data is taken from the minority class as an example and then new synthetic similar instances are created. These synthetic instances are then added to the original dataset. The new dataset is used as a sample to train the classification models.\n    \n- Modified Synthetic Minority Oversampling Technique (MSMOTE)\n    - While the basic flow of MSOMTE is the same as that of SMOTE (discussed in the previous section).  In MSMOTE the strategy of selecting nearest neighbors is different from SMOTE. The algorithm randomly selects a data point from the k nearest neighbors for the security sample, selects the nearest neighbor from the border samples and does nothing for latent noise.\n\n### Algorithmic Ensemble Technique:\n- Bagging Based\n    - Bagging is an abbreviation of Bootstrap Aggregating. The conventional bagging algorithm involves generating \u2018n\u2019 different bootstrap training samples with replacement. And training the algorithm on each bootstrapped algorithm separately and then aggregating the predictions at the end\n    - Bagging is used for reducing Overfitting in order to create strong learners for generating accurate predictions. Unlike boosting, bagging allows replacement in the bootstrapped sample.\n    \n- Boosting Based\n     - Boosting is an ensemble technique to combine weak learners to create a strong learner that can make accurate predictions. \n      - Boosting starts out with a base classifier \/ weak classifier that is prepared on the training data.\n- Different Boosting Methods:\n      - Ada Boost\n      - Gradient Tree Boosting\n      - XG Boost","3c5254af":"## Logistic Regression with penalty\n\nOn resampled dataset","0143875b":"## Business Problem Overview","aa4d75be":"## Churn Customers","5f78f6f9":"We can infer that the mobile_number is an unique column, means it's not repeated multiple times. So we can safely ignore as it's not going to add any value for our analysis","1de51b64":"In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, **customer retention** has now become even more important than customer acquisition.\n\n \n\nFor many incumbent operators, retaining high profitable customers is the number one business goal.\n\n \n\nTo reduce customer churn, telecom companies need to **predict which customers are at high risk of churn.**\n\n \n\nIn this project, we will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\n\n \n\n","ba9914b6":"We have carried out the study for the rest of features as well and concluded that the missing values can be imputed with 0 values.","482009ee":"# Feature Scaling","bbddd816":"# SVM using Non-linear kernal","5fa54e39":"# total_rech_data_ \nTotal number of times mobile data has been recharged","8db2f00a":"# loc_og_mou \n(within the circle outgoing calls)","19b5285f":"The regular K-Fold cross validation may not be applicable for an imbalanced dataset. As the folds created may not retain class proportions, even if they did, most classifiers need some manipulation of the dataset to balance it (oversampling of positive class or under sampling of negative class).<br>\nTo Cross validate an imbalanced dataset, we might have to apply stratified cross validation. Stratification seeks to ensure that each fold is representative of all classes of the data. <br>\n- Random Sampling: The given dataset is highly imbalanced. Churn classes is only 10% of the entire dataset. With such a dataset random sampling might not be able to pick up enough data of both the classes \n- Stratified Sampling: Stratified sampling will be able to sample enough data of both the classes. But, considering that the churn class is only 10% of the entire dataset, stratified sampling might significantly reduce the number of cases, so it should ideally be coupled with a resampling technique to ensure balance in data.","79ca62a4":"### SVM Non-linear (RBF)\n\nOn resampled data","24630050":"### XG Boost\n\nOn resampled data","22877657":"# vol_2g_mb_\nInternet usage in MB","88987e7e":"### Logistic Regression\n<br>\nLogistic Regression is a classification algorithm. It is used to predict a binary outcome or Multi class outcome, given a set of independent variables. Logistic regression is a special case of linear regression when the outcome variable is categorical, where we are using log of odds as dependent variable. In simple words, it predicts the probability of occurrence of an event by fitting data to a logit function. <br>\n\nLet's first fit a model with default hyperparameters.","0118cb24":"From the above we can notice high correlation between same features and only month differs.\n\nWe can eliminate as we will lose information of either good or action month.\n\nCreate new features from good month and compare them with action phase","e8286d44":"Identifying churn high value customers based on the last month i.e. month 9\n\n","1b7c88a9":"Build models to predict churn. The predictive model that you\u2019re going to build will serve two purposes:\n\nIt will be used to predict whether a high-value customer will churn or not, in near future (i.e. churn phase). By knowing this, the company can take action steps such as providing special plans, discounts on recharge etc.\n\nIt will be used to identify important variables that are strong predictors of churn. These variables may also indicate why customers choose to switch to other networks.\n\n \n\nIn some cases, both of the above-stated goals can be achieved by a single machine learning model. But here, you have a large number of attributes, and thus you should try using a dimensionality reduction technique such as PCA and then build a predictive model. After PCA, you can use any classification model.\n\n \n\nAlso, since the rate of churn is typically low (about 5-10%, this is called class-imbalance) - try using techniques to handle class imbalance. \n\n \n\nYou can take the following suggestive steps to build the model:\n\n1. Preprocess data (convert columns to appropriate formats, handle missing values, etc.)\n\n2. Conduct appropriate exploratory analysis to extract useful insights (whether directly useful for business or for eventual modelling\/feature engineering).\n\n3. Derive new features.\n\n4. Reduce the number of variables using PCA.\n\n5. Train a variety of models, tune model hyperparameters, etc. (handle class imbalance using appropriate techniques).\n\n6. Evaluate the models using appropriate evaluation metrics. Note that is is more important to identify churners than the non-churners accurately - choose an appropriate evaluation metric which reflects this business goal.\n\n7. Finally, choose a model based on some evaluation metric.\n\n \n\nThe above model will only be able to achieve one of the two goals - to predict customers who will churn. We can\u2019t use the above model to identify the important features for churn. That\u2019s because PCA usually creates components which are not easy to interpret.\n\n \n\nTherefore, build another model with the main objective of identifying important predictor attributes which help the business understand indicators of churn. A good choice to identify important variables is a **logistic regression model** or a model from the **tree family**. In case of logistic regression, make sure to handle multi-collinearity.\n\n \n\nAfter identifying important predictors, display them visually - you can use plots, summary tables etc. - whatever you think best conveys the importance of features.\n\n \n\nFinally, **recommend strategies to manage customer churn** based on your observations.\n\n \n\n","fb022cab":"### Random Over Sampling","455fd30d":"## Understanding Customer Behaviour During Churn","f9675de5":"## Understanding the Business Objective and the Data","bd7283ba":"# onnet_mou \n(On network\/ within the same network usage)","ef343c54":"Since we have the good data for both month 6, 7 as average, we don't need the raw columns, so dropping them","acb373e1":"## Churn definition","b1a1729a":"We can see few observations across the features, which have a different behavior. \n\nNearly 2.5% to 10% are tagged as outliers based on the feature.\n\nWe check the pattern between churn vs non-churn it's same across them and also it's same across the features as well. \n\nTherefore, we shouldn't exclude them from the model.","239fdb45":"# vbc_3g_\nVolume based cost - paid as per usage","22616b84":"From the above we can impute the missing values of isd_og_mou_8 with 0\n\n","9a0894ae":"# Model Building","8026c752":"We can clearly evident the range varies from 0.0 to 5681.54\n\nIt looks like the presence of outliers\n\nLet's check the all outgoing related features of 8th month","7e77fdba":"- We trained different machine learning models to solve this classification problems. \n- Both XGBoost and Random Forest Classifier performed the best when compared with other models.\n- The **Algorithmic Ensemble Techniques** was able to provide the same performance of the models using the resampling techniques data (the performance is almost the same). So, with balanced class option & stratified K-fold we can still get the same scores without increasing the dataset size\n- We considered F1-score and Area under the curve as the primary metrics for the evaluation "}}