{"cell_type":{"9bf77509":"code","872956cd":"code","98f1753e":"code","a1d0a991":"code","717eef74":"code","23de7236":"code","63f0e285":"code","090fb402":"markdown","0f98c3cc":"markdown","51570321":"markdown","5cb49fcb":"markdown"},"source":{"9bf77509":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport os, gc, random\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_columns', 140) #\u6700\u5927\u8868\u793a\u5217\u6570\u306e\u6307\u5b9a\n\n\n# fix seed\nseed = 2021\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)","872956cd":"from torch.utils.data import Dataset\nfrom torch import nn\n \nclass JSMP_Dataset(Dataset):\n     \n    def __init__(self, file_path, window_size):\n        # valiables\n        self.file_path = file_path\n        self.window_size = window_size\n        \n        # read csv\n        train = pd.read_csv(file_path)\n        \n        # pre processing\n        train = train.query('date > 85').reset_index(drop = True) \n        #train = train[train['weight'] != 0]\n        train.fillna(train.mean(),inplace=True)\n        train['action'] = ((train['resp'].values) > 0).astype(int)\n        \n        resp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp', 'resp_4']\n        self.features = [c for c in train.columns if \"feature\" in c]\n        self.f_mean = np.mean(train[self.features[1:]].values,axis=0)\n        \n        self.X_train = train.loc[:, train.columns.str.contains('feature')].values\n        self.y_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\n        \n        self.X_train = torch.from_numpy(self.X_train).float()\n        self.y_train = torch.from_numpy(self.y_train).float()\n        \n        # reduce memory\n        del train\n        gc.collect()\n \n    def __len__(self):\n        return len(self.X_train) - self.window_size\n     \n    def __getitem__(self, i):\n        data = self.X_train[i:(i+ self.window_size), :] \n        label = self.y_train[i + self.window_size - 1]\n \n        return data, label","98f1753e":"window_size = 5\nfile_path = '\/kaggle\/input\/jane-street-market-prediction\/train.csv'\nds = JSMP_Dataset(file_path, window_size)","a1d0a991":"import torch\nimport torch.nn as nn\nfrom torch.nn.utils import weight_norm\n\n\nclass Chomp1d(nn.Module):\n    def __init__(self, chomp_size):\n        super(Chomp1d, self).__init__()\n        self.chomp_size = chomp_size\n\n    def forward(self, x):\n        return x[:, :, :-self.chomp_size].contiguous()\n\n\nclass TemporalBlock(nn.Module):\n    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n        super(TemporalBlock, self).__init__()\n        self.conv1 = weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        self.chomp1 = Chomp1d(padding)\n        self.relu1 = nn.ReLU()\n        self.dropout1 = nn.Dropout(dropout)\n\n        self.conv2 = weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,\n                                           stride=stride, padding=padding, dilation=dilation))\n        self.chomp2 = Chomp1d(padding)\n        self.relu2 = nn.ReLU()\n        self.dropout2 = nn.Dropout(dropout)\n\n        self.net = nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,\n                                 self.conv2, self.chomp2, self.relu2, self.dropout2)\n        self.downsample = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n        self.relu = nn.ReLU()\n        self.init_weights()\n\n    def init_weights(self):\n        self.conv1.weight.data.normal_(0, 0.01)\n        self.conv2.weight.data.normal_(0, 0.01)\n        if self.downsample is not None:\n            self.downsample.weight.data.normal_(0, 0.01)\n\n    def forward(self, x):\n        out = self.net(x)\n        res = x if self.downsample is None else self.downsample(x)\n        return self.relu(out + res)\n\n\nclass TemporalConvNet(nn.Module):\n    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n        super(TemporalConvNet, self).__init__()\n        layers = []\n        num_levels = len(num_channels)\n        for i in range(num_levels):\n            dilation_size = 2 ** i\n            in_channels = num_inputs if i == 0 else num_channels[i-1]\n            out_channels = num_channels[i]\n            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n\n        self.network = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.network(x)","717eef74":"class TCN(nn.Module):\n    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n        super(TCN, self).__init__()\n        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n        \n        self.fc1 = nn.Linear(130 * num_channels[-1], 128)\n        self.dropout1 = nn.Dropout(dropout)\n        self.batch_norm1 = nn.BatchNorm1d(128)\n        self.LeakyReLU1 = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        \n        self.fc2 = nn.Linear(128, 128)\n        self.dropout2 = nn.Dropout(dropout)\n        self.batch_norm2 = nn.BatchNorm1d(128)\n        self.LeakyReLU2 = nn.LeakyReLU(negative_slope=0.01, inplace=True)\n        \n        self.fc3 = nn.Linear(128, output_size)\n        \n    def forward(self, inputs):\n        \"\"\"Inputs have to have dimension (N, C_in, L_in)\"\"\"\n        y1 = self.tcn(inputs)  # input should have dimension (N, C, L)\n        y1 = torch.flatten(y1, start_dim=1)\n        \n        y1 = self.fc1(y1)\n        y1 = self.batch_norm1(y1)\n        y1 = self.LeakyReLU1(y1)\n        y1 = self.dropout1(y1)\n        \n        y1 = self.fc2(y1)\n        y1 = self.batch_norm2(y1)\n        y1 = self.LeakyReLU2(y1)\n        y1 = self.dropout2(y1)\n        \n        o = self.fc3(y1)\n        return torch.sigmoid(o)","23de7236":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('use devise:', device)\n\n# load model\nnet1 = TCN(input_size=5, output_size=5, num_channels=[16, 8, 4, 2], kernel_size=2, dropout=0.5)\nnet2 = TCN(input_size=5, output_size=5, num_channels=[16, 8, 4, 2], kernel_size=2, dropout=0.5)\n\nnet1.load_state_dict(torch.load('\/kaggle\/input\/jsmp-tcn-pytorch\/best_accuracy_model.mdl', map_location=torch.device(device)))\nnet2.load_state_dict(torch.load('\/kaggle\/input\/jsmp-tcn-pytorch\/best_loss_model.mdl', map_location=torch.device(device)))\n\nnet1.eval()\nnet2.eval()","63f0e285":"th = 0.5\n\nimport janestreet\nenv = janestreet.make_env()\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('use devise:', device)\n\nfor i, (test_df, pred_df) in enumerate(env.iter_test()):\n    x_tt = test_df.loc[:, ds.features].values\n    if np.isnan(x_tt[:, 1:].sum()):\n        x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * ds.f_mean\n    \n    # make window data\n    if i == 0:\n        x_window = x_tt.copy()\n    elif i < window_size: \n        x_window = np.concatenate([x_window, x_tt], axis=0)\n    else:\n        x_window = np.concatenate([x_window[1:, :], x_tt], axis=0)\n    \n    if i < window_size - 1:\n        # pass \n        pred_df.action = 0\n    else:\n        # prediction\n        if test_df['weight'].item() > 0:\n            inputs = torch.Tensor(x_window).unsqueeze(0).to(device)\n            outputs = (net1(inputs) + net2(inputs)) \/ 2\n            pred = (torch.median(outputs, axis=1).values > th).long()\n            pred_df.action = pred.item()\n            #print(pred.item())\n        else:\n            pred_df.action = 0\n        \n    env.predict(pred_df)","090fb402":"## Network(TCN)\nhttps:\/\/github.com\/locuslab\/TCN\/blob\/master\/TCN\/tcn.py","0f98c3cc":"class TCN(nn.Module):\n    def __init__(self, input_size, output_size, num_channels, kernel_size, dropout):\n        super(TCN, self).__init__()\n        self.tcn = TemporalConvNet(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n        self.fc = nn.Linear(130 * num_channels[-1], output_size)\n\n    def forward(self, inputs):\n        \"\"\"Inputs have to have dimension (N, C_in, L_in)\"\"\"\n        y1 = self.tcn(inputs)  # input should have dimension (N, C, L)\n        #y1 = torch.flatten(y1, start_dim=1)\n        #o = self.fc(y1)\n        return y1#torch.sigmoid(o)","51570321":"## Dataset Class","5cb49fcb":"## Inference"}}