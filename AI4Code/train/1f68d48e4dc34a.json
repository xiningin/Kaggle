{"cell_type":{"2c064020":"code","73aadaca":"code","081f5fcd":"code","a39e6c07":"code","73b84c1f":"code","abb51f4d":"code","52b7ff0b":"code","d018cd44":"code","f8dd0390":"code","a954fc74":"code","b0eb56c3":"code","b8b0fe71":"code","97114188":"code","bcb5bbc3":"code","b7e520a6":"code","e97dfeff":"code","d8a45b4f":"code","5b28f3da":"markdown","3ed224a2":"markdown","f32c85cb":"markdown","9a3bb0d9":"markdown","76791995":"markdown","34fd1168":"markdown"},"source":{"2c064020":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport os \nimport requests, json \nimport wandb \nimport pickle \nimport time \nfrom kaggle_secrets import UserSecretsClient \n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error \n\nfrom lightgbm import LGBMRegressor","73aadaca":"config = dict(\n    n_fold = 5, \n    verbose = 100, \n    seed = 42 , \n    \n    infra = \"kaggle\", \n    competition = None, \n    model_name = \"lgb\", \n    early_stopping_rounds = 30, \n    train = True, \n    inference = True, \n    type = \"train\",\n    project = \"covid19-transition\"\n)\n\n","081f5fcd":"# db \nuser_secrets = UserSecretsClient()\nwandb_api = user_secrets.get_secret(\"wandb_api\") \n\nwandb.login(key=wandb_api)\n\nrun = wandb.init(\n    project = config[\"project\"], \n    name = config[\"model_name\"], \n    config = config, \n    group = config[\"model_name\"], \n    job_type = \"train\"\n)\n\n\n# slack \nuser_secrets = UserSecretsClient()\nurl = user_secrets.get_secret(\"WEB_HOOK_URL\") \n\n\ndef slack(txt):\n    requests.post(url, data=json.dumps({\n        \"username\": \"kaggle\", \n        \"text\": txt \n    }))","a39e6c07":"train = pd.read_csv(\"..\/input\/covid19-japan-time-series-transition\/step2\/train.csv\")\ntest = pd.read_csv(\"..\/input\/covid19-japan-time-series-transition\/step2\/test.csv\")\n\ntrain.head()","73b84c1f":"test.head()","abb51f4d":"train.prefecture.unique()","52b7ff0b":"test.prefecture.unique()","d018cd44":"train = train.fillna(0)\ntest = test.fillna(0)\n\nprint(train.isnull().sum() \/ train.shape[0])","f8dd0390":"ts = time.time()\n\ntrain[\"date\"] = pd.to_datetime(train.date)\ntest[\"date\"] = pd.to_datetime(test.date)\n\ntrain[\"year\"] = train.date.dt.year \ntrain[\"month\"] = train.date.dt.month \ntrain[\"week\"] = train.date.dt.dayofweek \ntrain[\"holiday\"] = train.date.apply(lambda x: 1 if x in [5, 6] else 0)\n\ntest[\"year\"] = test.date.dt.year \ntest[\"month\"] = test.date.dt.month \ntest[\"week\"] = test.date.dt.dayofweek \ntest[\"holiday\"] = test.date.apply(lambda x: 1 if x in [5, 6] else 0)\n\nuse_col = [\"death\", \"requiring_inpatient\", \"tobe_confirmed\", \"severe\"]\n\nfor col in use_col:\n    train[col+\"_lag1\"] = train.groupby(\"prefecture\")[col].shift(1).fillna(0)\n    train[col+\"_lag7\"] = train.groupby(\"prefecture\")[col].shift(7).fillna(0)    \n    train[col+\"_lag30\"] = train.groupby(\"prefecture\")[col].shift(30).fillna(0)    \n    train[col+\"_cumsum\"] = train.groupby(\"prefecture\")[col].cumsum().fillna(0)    \n    train[col+\"_avg7\"] = train.groupby(\"prefecture\")[col].rolling(window=7).mean().reset_index(drop=True).fillna(0)\n    train[col+\"_avg30\"] = train.groupby(\"prefecture\")[col].rolling(window=30).mean().reset_index(drop=True).fillna(0)    \n\n    test[col+\"_lag1\"] = test.groupby(\"prefecture\")[col].shift(1).fillna(0)\n    test[col+\"_lag7\"] = test.groupby(\"prefecture\")[col].shift(7).fillna(0)    \n    test[col+\"_lag30\"] = test.groupby(\"prefecture\")[col].shift(30).fillna(0)    \n    test[col+\"_cumsum\"] = test.groupby(\"prefecture\")[col].cumsum().fillna(0)    \n    test[col+\"_avg7\"] = test.groupby(\"prefecture\")[col].rolling(window=7).mean().reset_index(drop=True).fillna(0)\n    test[col+\"_avg30\"] = test.groupby(\"prefecture\")[col].rolling(window=30).mean().reset_index(drop=True).fillna(0)\n    \nend = time.time()\n\nprint(f\"dilation {end-ts}s\")\n                                                                                               ","a954fc74":"date = train.groupby(\"date\").mean().loc[:, use_col]\ndate.columns = [c + str(\"_date\") for c in use_col]\ntrain = pd.merge(train, date, how=\"left\", left_on=\"date\", right_index=True)\ntest = pd.merge(test, date, how=\"left\", left_on=\"date\", right_index=True)","b0eb56c3":"train.corr().loc[:, [\"newly_confirmed\"]].style.background_gradient(cmap=\"coolwarm\")","b8b0fe71":"\n'''\nMake prediction with a simple regression model \n'''\n\nparams = {'objective': 'regression',\n          'learning_rate': 0.25,\n          \"boosting_type\": \"gbdt\",\n          'min_data_in_leaf':600,\n          'max_bin': 196,\n          #'device':'gpu',\n          'feature_fraction':0.4,\n          'lambda_l1':36, 'lambda_l2':80,\n          'max_depth':16,\n          'num_leaves':1000,\n          \"metric\": 'mae',\n          'n_jobs': -1\n         }\n\n\nkf = GroupKFold(n_splits=config[\"n_fold\"])\npredict_val, predict_test, val_idx = [], [], []\n\nfor fold, (tr, va) in enumerate(kf.split(train, train.newly_confirmed, train.prefecture)):\n    print(\">\"*15+f\"fold: {fold}\"+ \"<\"*15)\n    x_train, x_val = train.iloc[tr].drop([\"date\", \"newly_confirmed\", \"prefecture\"], axis=1), train.iloc[va].drop([\"date\", \"newly_confirmed\", \"prefecture\"], axis=1)\n    y_train, y_val = train.iloc[tr][\"newly_confirmed\"], train.iloc[va][\"newly_confirmed\"]\n    x_test = test.drop([\"prefecture\", \"date\"], axis=1).copy()\n    \n    model = LGBMRegressor(**params, random_state=config[\"seed\"], n_estimators=8000)\n    model.fit(x_train, y_train, \n             eval_set=[(x_train, y_train), (x_val, y_val)], \n             early_stopping_rounds=30, \n             verbose=config[\"verbose\"])\n    \n    pred_v = model.predict(x_val).flatten()\n    pred_t = model.predict(x_test).flatten()\n    \n    predict_val.append(pred_v)\n    predict_test.append(pred_t)\n    val_idx.append(va)\n    \n    print(f\" Fold: {fold} | MAE: {mean_squared_error(pred_v, y_val.values.ravel(), squared=False)}\")\n\npredict_val = np.concatenate(predict_val)\nval_idx = np.concatenate(val_idx)\nval_idx = np.argsort(val_idx)\npredict_val = predict_val[val_idx]\npredict_test = np.mean(predict_test, 0)\n    \nprint(\"#\"*100)\nprint(f\"CV score: {mean_squared_error(predict_val, train[['newly_confirmed']].values.ravel(), squared=False)}\")\nprint(\"#\"*100)\n\nslack(\"success Train.\")","97114188":"def viz_predict(pred):\n    dfs = train.copy()\n    dfs = dfs[[\"date\", \"prefecture\", \"newly_confirmed\"]]\n    dfs[\"predict\"] = pred \n    \n    pref = dfs.prefecture.unique()\n    \n    fig, axes = plt.subplots(5, 5, figsize=(12, 12))\n    ax = axes.ravel()\n    \n    for i in range(25):\n        x = dfs[dfs.prefecture == pref[i]]\n        x.set_index(\"date\").plot(ax=ax[i])\n        ax[i].set_title(pref[i])\n        \n    plt.tight_layout()\n    \n    \ndef viz_predict_test(pred):\n    dfs = test.copy()\n    dfs = dfs[[\"date\", \"prefecture\"]]\n    dfs[\"predict\"] = pred \n    \n    pref = dfs.prefecture.unique()\n    \n    fig, axes = plt.subplots(2, 3, figsize=(8, 8))\n    ax = axes.ravel()\n    \n    for i in range(5):\n        x = dfs[dfs.prefecture == pref[i]]\n        x.set_index(\"date\").plot(ax=ax[i])\n        ax[i].set_title(pref[i])\n        \n    plt.tight_layout()\n        ","bcb5bbc3":"viz_predict(predict_val)","b7e520a6":"viz_predict_test(predict_test)","e97dfeff":"sub = pd.read_csv(\"..\/input\/covid19-japan-time-series-transition\/step2\/sample_submission.csv\")\nsub[\"newly_confirmed\"] = predict_test\nsub.to_csv(\"submission.csv\", index=False)","d8a45b4f":"slack(\"done.\")","5b28f3da":"# Evaluate ","3ed224a2":"# Training ","f32c85cb":"# Tools ","9a3bb0d9":"# Const ","76791995":"# Submit","34fd1168":"# Feature Engineering "}}