{"cell_type":{"ee2a7510":"code","379ae22a":"code","9405dbcf":"code","487ae01c":"code","e9e1b64d":"code","8fb626ad":"code","e606c667":"code","67813805":"code","83841d45":"code","2639c017":"code","166f8d14":"code","a524e3b4":"code","8280a4f2":"code","cdba6000":"code","171004b7":"code","05bea36b":"code","9acb200d":"code","c29a87b6":"code","36fdc829":"code","3e4bcbd3":"code","62b10df2":"markdown","73a62ad5":"markdown","7427bb17":"markdown","02033fd1":"markdown","4b7a5bf4":"markdown"},"source":{"ee2a7510":"!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/rps.zip \\\n    -O \/tmp\/rps.zip\n  \n!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/rps-test-set.zip \\\n    -O \/tmp\/rps-test-set.zip","379ae22a":"import os\nimport zipfile\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","9405dbcf":"local_zip = '\/tmp\/rps.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('\/tmp')\nzip_ref.close()","487ae01c":"local_zip2 = '\/tmp\/rps-test-set.zip'\nzip_ref = zipfile.ZipFile(local_zip2, 'r')\nzip_ref.extractall('\/tmp')\nzip_ref.close()","e9e1b64d":"rock_dir = os.path.join('\/tmp\/rps\/rock')\npaper_dir = os.path.join('\/tmp\/rps\/paper')\nscissors_dir = os.path.join('\/tmp\/rps\/scissors')","8fb626ad":"print('total training rock images:', len(os.listdir(rock_dir)))\nprint('total training paper images:', len(os.listdir(paper_dir)))\nprint('total training scissors images:', len(os.listdir(scissors_dir)))\n\nrock_files = os.listdir(rock_dir)\nprint(rock_files[:10])\n\npaper_files = os.listdir(paper_dir)\nprint(paper_files[:10])\n\nscissors_files = os.listdir(scissors_dir)\nprint(scissors_files[:10])","e606c667":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\npic_index = 2\n\nnext_rock = [os.path.join(rock_dir, fname) \n                for fname in rock_files[pic_index-2:pic_index]]\nnext_paper = [os.path.join(paper_dir, fname) \n                for fname in paper_files[pic_index-2:pic_index]]\nnext_scissors = [os.path.join(scissors_dir, fname) \n                for fname in scissors_files[pic_index-2:pic_index]]","67813805":"for i, img_path in enumerate(next_rock+next_paper+next_scissors):\n  #print(img_path)\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n  plt.axis('Off')\n  plt.show()","83841d45":"TRAINING_DIR = '\/tmp\/rps\/'\ntraining_datagen = ImageDataGenerator(\n    rescale = 1\/255,\n    rotation_range = 40,\n    width_shift_range = 0.2,\n    height_shift_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True,\n    shear_range = 0.2,\n    fill_mode = 'nearest'\n)","2639c017":"VALIDATION_DIR = \"\/tmp\/rps-test-set\/\"\nvalidation_datagen = ImageDataGenerator(rescale = 1\/255)","166f8d14":"train_generator = training_datagen.flow_from_directory(\n    TRAINING_DIR,\n    target_size = (150,150),\n    class_mode = 'categorical',\n    batch_size =126,\n)\n\nvalidation_generator = validation_datagen.flow_from_directory(\n\tVALIDATION_DIR,\n\ttarget_size=(150,150),\n\tclass_mode='categorical',\n  batch_size=126\n)","a524e3b4":"model = tf.keras.Sequential([\n                             tf.keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=(150,150,3)),\n                             tf.keras.layers.MaxPool2D(2,2),\n                             tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n                             tf.keras.layers.MaxPool2D(2,2),\n                             tf.keras.layers.Conv2D(128,(3,3),activation='relu'),\n                             tf.keras.layers.MaxPool2D(2,2),tf.keras.layers.Conv2D(128,(3,3),activation='relu'),\n                             tf.keras.layers.MaxPool2D(2,2),\n                             tf.keras.layers.Flatten(),\n                             tf.keras.layers.Dropout(0.5),\n                             tf.keras.layers.Dense(512,activation='relu'),\n                             tf.keras.layers.Dense(3,activation='softmax')  \n])","8280a4f2":"model.summary()","cdba6000":"model.compile(loss = 'categorical_crossentropy',optimizer = 'rmsprop',metrics = ['accuracy'])","171004b7":"#ON the accelerator to GPU for fast result","05bea36b":"history = model.fit(train_generator,epochs = 30,steps_per_epoch=20,validation_data=validation_generator,verbose = 1,validation_steps=3)","9acb200d":"import matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']","c29a87b6":"epochs = range(len(acc))\nplt.plot(epochs,acc,'r',label = 'Training_accuracy')\nplt.plot(epochs,val_acc,'b',label = 'Validation_accuracy')\nplt.title('Training and Validation Accuracy')\nplt.legend(loc=0)\nplt.figure()\n\nplt.show()","36fdc829":"model.save('rps.h5')","3e4bcbd3":"# Run this code on Google Collab: For Uploading image file on notebook \n# Collab url: https:\/\/colab.research.google.com\/drive\/1iwOO4hmcfXYGZXAiR-rb1cNhVzscoKaf?usp=sharing\nimport numpy as np\nfrom google.colab import files\nfrom keras.preprocessing import image\n\nuploaded = files.upload()\n\nfor fn in uploaded.keys():\n \n  # predicting images\n  path = fn\n  img = image.load_img(path, target_size=(150, 150))\n  x = image.img_to_array(img)\n  x = np.expand_dims(x, axis=0)\n\n  images = np.vstack([x])\n  classes = model.predict(images, batch_size=10)\n  print(fn)\n  print(classes)","62b10df2":"#  Data Augmentation","73a62ad5":"# Importing packages","7427bb17":"# Dataset Imported","02033fd1":"## Run this notebook in google collab for better performance and analysis","4b7a5bf4":"# Reading data"}}