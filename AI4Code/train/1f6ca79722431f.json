{"cell_type":{"0f074afd":"code","607a5ec1":"code","29e24527":"code","dfca8d0a":"code","b60dc573":"code","8a685563":"code","ad82e7d9":"code","be9b1e36":"code","c8c20461":"code","0220376f":"code","a14a4e37":"code","7375fe46":"code","5962ea23":"code","56428037":"code","d12c8e76":"code","f7eca554":"code","cd2638c1":"code","60291636":"code","a9db1f60":"code","c568bf9c":"code","3e0d8e0b":"code","34f5687f":"code","f84a7771":"code","9dab5e51":"code","95e0098e":"code","9ebded68":"code","102cd0c1":"code","3735103d":"code","cae1d8bb":"code","badbbd9d":"code","cdd5e7b9":"markdown","d8994e29":"markdown","190ab5ec":"markdown","9b8b7bf4":"markdown","ef821e86":"markdown","c4e98cb4":"markdown","6a96a154":"markdown","0fc5a7c1":"markdown","794450b9":"markdown","6f26e47b":"markdown","4943a93f":"markdown","dc8c0969":"markdown","1237088b":"markdown","8da45fd2":"markdown","1238b8c7":"markdown","a9e66c51":"markdown","b60d64b3":"markdown","4e0565ed":"markdown","5beb66e5":"markdown","2cc6ce35":"markdown","f0faed2d":"markdown","bced9146":"markdown","732d0fbd":"markdown","b8dee97e":"markdown","11fe0f10":"markdown"},"source":{"0f074afd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","607a5ec1":"dataset = pd.read_csv('..\/input\/heart.csv')","29e24527":"dataset.head()","dfca8d0a":"dataset.info()","b60dc573":"dataset.describe()","8a685563":"dataset.shape","ad82e7d9":"sns.countplot(x='sex',data=dataset)","be9b1e36":"plt.figure(figsize=(8,6))\nexplode =[0.1,0]\nlabels='Male','Female'\nplt.pie(dataset['sex'].value_counts(),explode=explode,autopct='%1.1f%%',labels=labels,shadow=True,startangle=140)","c8c20461":"plt.figure(figsize=(10,6))\nexplode=[0.1,0,0,0]\nlabels='Pain-Type 0','Pain Type-1','Pain-Type2','Pain-Type3'\nplt.pie(dataset['cp'].value_counts(),explode=explode,labels=labels,autopct='%1.1f%%',shadow=True,startangle=140)","0220376f":"sns.boxplot(dataset['trestbps'],orient='v',color='Magenta')","a14a4e37":"sns.boxplot(dataset['chol'],orient='v',color='Magenta')","7375fe46":"\n#dataset.plot.scatter(x='age',y='trestbps')\nplt.figure(figsize=(20,10))\nsns.boxplot(x='age',y='trestbps',data=dataset)","5962ea23":"plt.figure(figsize=(20,10))\nsns.boxplot(x='age',y='thalach',data=dataset)","56428037":"sns.set()\ncol=['age','trestbps','chol','thalach']\nsns.pairplot(dataset[col])\nplt.show()","d12c8e76":"plt.figure(figsize=(15,10))\nsns.heatmap(dataset.corr(),annot=True,cmap='YlGnBu')","f7eca554":"sex = pd.get_dummies(dataset['sex'],prefix='sex',drop_first=True)\nfbs = pd.get_dummies(dataset['fbs'],prefix='fbs',drop_first=True)\nrestecg = pd.get_dummies(dataset['restecg'],prefix='restecg',drop_first=True)\nexang = pd.get_dummies(dataset['exang'],prefix='exang',drop_first=True)\ncp = pd.get_dummies(dataset['cp'],prefix='cp',drop_first=True)\nslope = pd.get_dummies(dataset['slope'],prefix='slope',drop_first=True)\nthal = pd.get_dummies(dataset['thal'],prefix='thal',drop_first=True)\n\ndataset = pd.concat([dataset,sex,fbs,restecg,exang,cp,slope,thal],axis=1)\n\n\n\n#Will do a quick check if it worked or not :P\ndataset.head()\n","cd2638c1":"dataset = dataset.drop(columns=['sex','fbs','restecg','exang','cp','slope','thal'])\ndataset.head()","60291636":"X= dataset.drop('target',axis=1)\ny = dataset['target'].values","a9db1f60":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=0)","c568bf9c":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test  = sc.transform(X_test)","3e0d8e0b":"from sklearn.decomposition import PCA\npca = PCA(n_components=None,random_state=0)\nX_train = pca.fit_transform(X_train)\nX_test =pca.transform(X_test)\n\npca.explained_variance_ratio_\n","34f5687f":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nlr_score = lr.score(X_test,y_test)\n\n","f84a7771":"from sklearn.svm import SVC\nsv = SVC(kernel ='rbf',random_state=0)\nsv.fit(X_train,y_train)\nsv_pred = sv.predict(X_test)\nsv_score = sv.score(X_test,y_test)","9dab5e51":"from sklearn.ensemble import RandomForestClassifier\nrf_regressor = RandomForestClassifier(n_estimators = 1000, random_state = 0)\nrf_regressor.fit(X_train, y_train)\nrf_pred = rf_regressor.predict(X_test)\nrf_score = rf_regressor.score(X_test,y_test)\n","95e0098e":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=2)\nknn.fit(X_train,y_train)\nknn_score = knn.score(X_test,y_test)\n","9ebded68":"from sklearn.naive_bayes import GaussianNB\nnv = GaussianNB()\nnv.fit(X_train,y_train)\nnv_sc = nv.score(X_test,y_test)","102cd0c1":"\nprint(\"Logistic Regression Model Score is \",round(lr_score*100))\nprint(\"SVC Model Score is \",round(sv_score*100))\n#print(\"Decision tree  Regression Model Score is \",round(tr_regressor.score(X_test,y_test)*100))\nprint(\"Random Forest Regression Model Score is \",round(rf_score*100))\n\nprint(\"KNeighbors Classifiers Model score is\",round(knn_score*100))\nprint(\"Naive Bayes model score is\",round(nv_sc*100))\n","3735103d":"\n\nfrom sklearn.model_selection import cross_val_score\naccuracies_lr = cross_val_score(estimator = lr,X = X_train,y = y_train,cv = 10)\naccuracies_sv = cross_val_score(estimator = sv,X = X_train,y = y_train,cv = 10)\naccuracies_rf = cross_val_score(estimator = rf_regressor,X = X_train,y = y_train,cv = 10)\n\naccuracies_knn = cross_val_score(estimator = knn,X = X_train,y = y_train,cv = 10)\naccuracies_nv = cross_val_score(estimator = nv,X = X_train,y = y_train,cv = 10)\n\nprint(\"Mean Accuracies based on cross val score for logistic regression\",round(accuracies_lr.mean()*100))\nprint(\"Mean Accuracies based on cross val score for SVM \",round(accuracies_sv.mean()*100))\nprint(\"Mean Accuracies based on cross val score for Random Forest\",round(accuracies_rf.mean()*100))\n\nprint(\"Mean Accuracies based on cross val score for KNN\",round(accuracies_knn.mean()*100))\nprint(\"Mean Accuracies based on cross val score for Naive Bayes\",round(accuracies_nv.mean()*100))\n","cae1d8bb":"\ncm_lr = confusion_matrix(y_test,y_pred)\ncm_lr\n","badbbd9d":"cm_rf = confusion_matrix(y_test,rf_pred)\ncm_rf","cdd5e7b9":"**Conclusion**\n\nThough there are weak co relation between variables and also exists other model, but Logistic Model and Random Forest much better than other model. \n\n\nPlease ****Upvote**** my work if you like it :)\n\n","d8994e29":"**Confusion Matrix** for Random Forest is as below\n","190ab5ec":"Lets build a heat map to check the co relation between variables. From the below it is evident that hardly strong co realtion exists between variables. ","9b8b7bf4":"**Making Predictions**\n\n","ef821e86":"**KNN**","c4e98cb4":"Extracting the dependent (Y) and X variables.\n\n","6a96a154":"> Lets Import the heart.csv file into a pandas dataframe\n","0fc5a7c1":"**Random Forest Classifier**","794450b9":"**PCA component **\n\n","6f26e47b":"**Logistic Regression**","4943a93f":"**Train Test Splitting**\n\nWe will split the data into train test based on 80:20 ","dc8c0969":"\nage :            age in years\n\nsex:            (1 = male; 0 = female)\n\ncp:              chest pain type\n\ntrestbps       resting blood pressure (in mm Hg on admission to the hospital)\n\ncholserum   cholestoral in mg\/dl\n\nfbs(fasting blood sugar > 120 mg\/dl): (1 = true; 0 = false)\n\nrestecg       resting electrocardiographic results\n\nthalachmaximum heart rate achieved\n\nexangexercise induced angina (1 = yes; 0 = no)\n\noldpeakST depression induced by exercise relative to rest\n\nslope        the slope of the peak exercise ST segment\n\nca             number of major vessels (0-3) colored by flourosopy\n\nthal           3 = normal; 6 = fixed defect; 7 = reversable defect\n\ntarget       1 or 0\n","1237088b":"**Gender Ratio:**\n\nLets see percentage wise ratio of dataset for gender.","8da45fd2":"**Model Score**","1238b8c7":"**Naive Bayes**","a9e66c51":"First of all we need an insight of the data what it contains and how to interpret the data into a more meaningful statistics. So we will first check the contents using head. Head() will give 5 rows as default from the top. We can also use tail, which provides data from the bottom. For now we will stick to head(). ","b60d64b3":"**Creating Dummy Variables:**\n\nFrom the above we can see there are categorical values, which includes: sex,cp,fbs etc.\n\nSo we will create dummy variables. We will also use prefix so that categorical columns when converted are recognized properly.","4e0565ed":"**Cross Validation Score with 10 iteration**","5beb66e5":"**Gender distribution in the file using Seaborn**","2cc6ce35":"Let's check the shape of the data, i.e count and columns available\n\n","f0faed2d":"Dropping the columns since we have already converted the categorical data and taken care the dummy trap above","bced9146":"**Chest Pain** : We can see there are different pain type, so lets build a pie chart which will show the data distribution.\n","732d0fbd":"**Standard Scaler**\n\nLets Standarize the data before fitting the data into the model.Standardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data.\n\n","b8dee97e":"**Confusion Matrix:**\n\nLogistic Regression and Random Forest since this performs a better model in comparison to other\n\n","11fe0f10":"**Support Vector **"}}