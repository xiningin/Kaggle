{"cell_type":{"40462db4":"code","e77eec75":"code","eff2ea47":"code","007bc4b4":"code","4b38d344":"code","e8f32028":"code","be34429f":"code","890abc17":"code","85bb8485":"code","4fbda298":"code","6f31e5e5":"code","7c7e293d":"code","547ba72a":"code","182cdfef":"code","4bfde2e9":"code","46027517":"code","5d3b352b":"code","6e44b194":"code","b6b9765f":"code","76ae91c3":"code","78ae3a89":"code","02803f1b":"code","c43a639d":"code","cbf122fb":"code","67514275":"code","f8fc1fcb":"code","c18c3b17":"code","ec9adb9b":"code","91378d01":"code","38016e81":"code","e14580e7":"code","66714663":"code","0bee7ad0":"code","5020bc39":"code","4d609f34":"code","e1c5b7f2":"code","a5c27382":"code","3dbf3b57":"code","6eff4d81":"code","8e81cd64":"code","e667fd74":"code","87a5ca6a":"code","e6252d1e":"code","07fec58b":"code","98389cbf":"code","8078a0fa":"code","842e72bd":"code","dddea1c9":"code","1e06c8b7":"code","d55e0f1a":"code","539c235a":"code","973d243b":"code","2e3e99df":"code","711585a8":"code","4054b497":"code","6f682c64":"code","138e5772":"code","83ba57a8":"code","236f529e":"markdown","d1138468":"markdown","e0200667":"markdown","81e71154":"markdown","ad0aa6f6":"markdown","27576adc":"markdown","afafdc71":"markdown","73bac91e":"markdown","259bd4ae":"markdown","86b83d03":"markdown"},"source":{"40462db4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd \nimport datetime\nimport random\n# data processing, CSV file I\/O (e.g. pd.read_csv)\n\n#importing visualization lib.\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport matplotlib.pyplot as plt\nimport plotly\n\n\n#stas and model\nfrom scipy import stats\nfrom scipy.special import boxcox1p\nfrom scipy.stats import norm,skew\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e77eec75":"train = pd.read_excel('..\/input\/Data_Train.xlsx')\ntest = pd.read_excel('..\/input\/Data_Test.xlsx')\n\ntrain.shape, test.shape","eff2ea47":"train.head()","007bc4b4":"sns.set_style(\"white\")\nsns.set_color_codes(palette=\"deep\")\nf,ax= plt.subplots(figsize = (8,7))\nsns.distplot(train['Price'])\nax.set(xlabel='Price')\nax.set(ylabel='Frequency')\nax.set(title = 'Price Distribution')\nplt.show()","4b38d344":"print('Skewness = %f' %train['Price'].skew())\nprint('Kurtiosis = %f' %train['Price'].kurt())","e8f32028":"train['Price'].describe()","be34429f":"log_train = np.log1p(train['Price'])\nlog_train.describe()","890abc17":"train.head()","85bb8485":"fig,ax = plt.subplots()\nax.scatter(x = train['Kilometers_Driven'], y = train['Price'])\nplt.ylabel('Price of the car')\nplt.xlabel('Kilometers Driven')\nplt.show()","4fbda298":"#deleting outliers \ntrain = train.drop(train[(train['Kilometers_Driven']>5000000) & (train['Price']<100)].index)\ntrain = train.drop(train[(train['Kilometers_Driven']<200000) & (train['Price']>120)].index)\n","6f31e5e5":"fig,ax = plt.subplots()\nax.scatter(x = train['Kilometers_Driven'], y = train['Price'])\nplt.ylabel('Price of the car')\nplt.xlabel('Kilometers Driven')\nplt.show()","7c7e293d":"fig,ax = plt.subplots()\nax.scatter(x = train['Year'], y = train['Price'])\nplt.ylabel('Price of the car')\nplt.xlabel('Kilometers Driven')\nplt.show()","547ba72a":"sns.distplot(train['Price'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['Price'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['Price'], plot=plt)\nplt.show()","182cdfef":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"Price\"] = np.log1p(train[\"Price\"])\n\n#Check the new distribution \nsns.distplot(train['Price'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['Price'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['Price'], plot=plt)\nplt.show()","4bfde2e9":"y_train = train['Price'].values\nall_data= pd.concat((train,test)).reset_index(drop = True)\nall_data.drop(['Price'], axis = 1, inplace = True)","46027517":"all_data.shape","5d3b352b":"all_data_na = (all_data.isnull().sum()\/len(all_data))*100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({\"Missing ratio \": all_data_na})\nmissing_data.head(20)","6e44b194":"f,ax = plt.subplots(figsize =(8,10))\nplt.xticks(rotation = '90')\nsns.barplot(x = all_data_na.index, y = all_data_na)\nplt.xlabel(\"Features\")\nplt.ylabel(\"Missing percentages\")","b6b9765f":"all_data.drop(['New_Price'], axis = 1, inplace = True)","76ae91c3":"all_data.head()","78ae3a89":"all_data.dtypes","02803f1b":"all_data['Engine'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\nall_data['Power'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\nall_data['Mileage'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')","c43a639d":"all_data.head()","cbf122fb":"all_data['Fuel_Type'] = all_data['Fuel_Type'].apply(str)\nall_data['Engine'] = pd.to_numeric(all_data['Engine'])\nall_data['Mileage'] = pd.to_numeric(all_data['Mileage'])\nall_data['Power'] = pd.to_numeric(all_data['Power'])\nall_data['Seats'] = all_data['Seats'].apply(str)\nall_data['Year'] = all_data['Year'].apply(str)\n","67514275":"vlist = ['CNG', 'LPG']\nCNG = all_data[all_data['Fuel_Type'].isin(vlist)]\nCNG['Mileage'] = CNG['Mileage']*0.719\nCNG.head()","f8fc1fcb":"all_data.update(CNG)\nall_data.head()","c18c3b17":"all_data[\"Engine\"] = all_data[\"Engine\"].transform(\n    lambda x: x.fillna(x.median()))\nall_data[\"Power\"] = all_data[\"Power\"].transform(\n    lambda x: x.fillna(x.median()))\nall_data[\"Seats\"] = all_data[\"Seats\"].transform(\n    lambda x: x.fillna(x.mode()))\nall_data[\"Mileage\"] = all_data[\"Mileage\"].transform(\n    lambda x: x.fillna(x.median()))\n","ec9adb9b":"all_data_na = (all_data.isnull().sum()\/len(all_data))*100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({\"Missing ratio \": all_data_na})\nmissing_data.head(20)","91378d01":"from sklearn.preprocessing import LabelEncoder\n\ncols = ('Owner_Type', 'Seats', 'Year', 'Transmission')\nfor c in cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(all_data[c].values))\n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n\nall_data.drop(['Name'], axis = 1, inplace = True)\nall_data.head()\n","38016e81":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","e14580e7":"skewness = skewness[skewness >2]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)","66714663":"final_data = pd.get_dummies(data=all_data, columns=['Fuel_Type', 'Location'])\n#final_data = pd.get_dummies(all_data)\nfinal_data.head()","0bee7ad0":"ntrain = train.shape[0]\nntest = test.shape[0]\ntrain = final_data[:ntrain]\ntest = final_data[ntrain:]","5020bc39":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","4d609f34":"#Validation function\nn_folds = 10\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","e1c5b7f2":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","a5c27382":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","3dbf3b57":"KRR = KernelRidge(alpha=0.6, kernel='linear', degree=2, coef0=2.5)","6eff4d81":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","8e81cd64":"model_xgb = xgb.XGBRegressor(learning_rate=0.05, max_depth=3, \n                             n_estimators=3000\n                            )","e667fd74":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.02, n_estimators=3000)","87a5ca6a":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e6252d1e":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","07fec58b":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","98389cbf":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8078a0fa":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","842e72bd":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","dddea1c9":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","1e06c8b7":"averaged_models = AveragingModels(models = (ENet, GBoost, model_lgb,model_xgb))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","d55e0f1a":"\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","539c235a":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","973d243b":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","2e3e99df":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","711585a8":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","4054b497":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","6f682c64":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.10 +\n               xgb_train_pred*0.80 + lgb_train_pred*0.10 ))","138e5772":"ensemble = stacked_pred*0.15 + xgb_pred*0.70 + lgb_pred*0.15","83ba57a8":"sub = pd.DataFrame()\nsub['Price'] = ensemble\nsub.to_csv('submission.csv',index=False)","236f529e":"dropping the new_price column and  adding the mode to rest other columns.","d1138468":"**Modelling**","e0200667":"So we'll go with Log transformations only","81e71154":"Applying box cox transformation of highly skewed features","ad0aa6f6":"LIGHT GBM","27576adc":"XGBOOST\n","afafdc71":"Missing data check","73bac91e":"0.719 ","259bd4ae":"* Price is majorly right skewed\nfinding the skewness and kurtiosis","86b83d03":"TARGET VARIABLE"}}