{"cell_type":{"2972a346":"code","4a22a60e":"code","3cad9a8d":"code","791bbb3b":"code","54d7ae9b":"code","6bc14087":"code","a92fd2f1":"code","55a02ef9":"code","9d3f6f52":"code","6c4b59fe":"code","1ecb4ebd":"code","f369a13f":"code","0a3116ec":"code","c911c030":"code","b32045b4":"code","ba1afd61":"code","0b32f69c":"code","825da6c4":"code","8cb781bd":"code","c7bebdee":"code","bf127c9d":"code","6a738f91":"code","99038426":"code","399aea10":"markdown","b784a931":"markdown","380b3285":"markdown","9a179df7":"markdown","b49593b0":"markdown","8dae355f":"markdown","edecdfd0":"markdown","212b175d":"markdown","1463987c":"markdown","e8cea4e6":"markdown","28916244":"markdown","febb9785":"markdown","0a6dac0d":"markdown"},"source":{"2972a346":"import pandas as pd\nimport numpy as np\nimport os\nimport ast\nfrom sklearn.model_selection import GroupKFold\nfrom string import Template\nimport json\nimport torch\nfrom shutil import copyfile\nimport greatbarrierreef\nimport importlib\nimport random\nimport cv2\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom IPython.display import display","4a22a60e":"!git clone https:\/\/github.com\/Megvii-BaseDetection\/YOLOX -q\n\n%cd YOLOX\n!pip install -U pip && pip install -r requirements.txt\n!pip install -v -e . ","3cad9a8d":"!pip install 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'","791bbb3b":"N_SPLITS = 5\nFOLD = random.randint(0, N_SPLITS - 1)\nprint(FOLD)","54d7ae9b":"train = pd.read_csv(\"\/kaggle\/input\/tensorflow-great-barrier-reef\/train.csv\")\ntrain.head()","6bc14087":"train['annotations'] = train['annotations'].apply(lambda x: ast.literal_eval(x))\ntrain['box'] = train['annotations'].apply(lambda x: [list(y.values()) for y in x])\ntrain['image_path'] = \"video_\" + train['video_id'].astype(str) + \"\/\" + train['video_frame'].astype(str) + \".jpg\"\ntrain['fold'] = -1\ntrain = train[train['annotations'].str.len() > 0].reset_index(drop = True)","a92fd2f1":"kf = GroupKFold(n_splits=N_SPLITS)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(train, y = train.video_id.tolist(), groups = train.sequence)):\n    train.loc[val_idx, 'fold'] = fold\ntrain['fold'] = train['fold'].astype(int)\ntrain.head()","55a02ef9":"!mkdir \/kaggle\/working\/dataset\n!mkdir \/kaggle\/working\/dataset\/images\n!mkdir \/kaggle\/working\/dataset\/images\/train2017\n!mkdir \/kaggle\/working\/dataset\/images\/val2017\n!mkdir \/kaggle\/working\/dataset\/images\/annotations","9d3f6f52":"for i in range(len(train)):\n    row = train.loc[i]\n    if(row.fold != FOLD):\n        copyfile(f'\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/{row.image_path}', f'\/kaggle\/working\/dataset\/images\/train2017\/{row.image_id}.jpg')\n    else:\n        copyfile(f'\/kaggle\/input\/tensorflow-great-barrier-reef\/train_images\/{row.image_path}', f'\/kaggle\/working\/dataset\/images\/val2017\/{row.image_id}.jpg')","6c4b59fe":"#COCO:\ndef datasetToCOCO(dataset):\n    annotation = 0\n    json = {\n        \"info\": [],\n        \"licenses\": [],\n        \"categories\": [],\n        \"images\": [],\n        \"annotations\": []\n    }\n    info = {\n        \"year\": \"2021\",\n        \"version\": \"1\",\n        \"description\": \"COTS dataset - COCO format\",\n        \"contributor\": \"\",\n        \"url\": \"https:\/\/kaggle.com\",\n        \"date_created\": \"2021-11-30T15:01:26+00:00\"\n    }\n    json['info'].append(info)\n    licenses = {\n        \"id\": 1,\n        \"url\": \"\",\n        \"name\": \"Unknown\"\n    }\n    json['licenses'].append(licenses)\n    categories = {\n        \"id\": 0, \n        \"name\": \"starfish\", \n        \"supercategory\": \"none\"\n    }\n    json[\"categories\"].append(categories)\n    for row in dataset.itertuples():\n        images = {\n            \"id\": row[0],\n            \"license\": 1,\n            \"file_name\": row.image_id + '.jpg',\n            \"height\": 720,\n            \"width\": 1280,\n            \"date_captured\": \"2021-11-30T15:01:26+00:00\"\n        }\n        json['images'].append(images)\n        boxes = row.box\n        for box in boxes:\n            width = box[2]\n            height = box[3]\n            if (box[0] + box[2] > 1280):\n                width = 1280 - box[0] \n            if (box[1] + box[3] > 720):\n                height = 720 - box[1] \n            annotations = {\n                \"id\": annotation,\n                \"image_id\": row[0],\n                \"category_id\": 0,\n                \"bbox\": [box[0], box[1], width, height],\n                \"area\": box[2] * box[3],\n                \"segmentation\": [],\n                \"iscrowd\": 0\n            }\n            annotation += 1\n            json['annotations'].append(annotations)\n    return json","1ecb4ebd":"train_coco = datasetToCOCO(train[train['fold'] != FOLD])\nwith open(f\"\/kaggle\/working\/dataset\/images\/annotations\/train.json\", 'w') as f:\n    output_json = json.dumps(train_coco)\n    f.write(output_json)\nval_coco = datasetToCOCO(train[train['fold'] == FOLD])\nwith open(f\"\/kaggle\/working\/dataset\/images\/annotations\/val.json\", 'w') as f:\n    output_json = json.dumps(val_coco)\n    f.write(output_json)","f369a13f":"template = '''\n#!\/usr\/bin\/env python3\n# -*- coding:utf-8 -*-\n# Copyright (c) Megvii, Inc. and its affiliates.\n\nimport os\n\nfrom yolox.exp import Exp as MyExp\n\n\nclass Exp(MyExp):\n    def __init__(self):\n        super(Exp, self).__init__()\n        self.num_classes = 1\n        self.depth = 0.33\n        self.width = 0.50\n        self.exp_name = os.path.split(os.path.realpath(__file__))[1].split(\".\")[0]\n        self.data_dir = \"\/kaggle\/working\/dataset\/images\/\"\n        self.train_ann = 'train.json'\n        self.val_ann = 'val.json'\n        self.max_epoch = $max_epoch\n        #self.eval_interval = 1\n        #self.data_num_workers = 2\n        #self.input_size = (960, 960)\n        #self.test_size = (960, 960)\n        #self.no_aug_epochs = 2\n        #self.mosaic_scale = (0.5, 1.5)\n        #self.random_size = (10, 20)\n'''","0a3116ec":"pipeline = Template(template).substitute(max_epoch = 20)\nwith open('cots_config.py', 'w') as f:\n    f.write(pipeline)","c911c030":"#VERSION 14: Added voc_cls to see if any differences arise.\nvoc_cls = '''\nVOC_CLASSES = (\n  \"starfish\",\n)\n'''\nwith open('\/kaggle\/working\/YOLOX\/yolox\/data\/datasets\/voc_classes.py', 'w') as f:\n    f.write(voc_cls)\n\ncoco_cls = '''\nCOCO_CLASSES = (\n  \"starfish\",\n)\n'''\nwith open('\/kaggle\/working\/YOLOX\/yolox\/data\/datasets\/coco_classes.py', 'w') as f:\n    f.write(coco_cls)","b32045b4":"sh = 'wget https:\/\/github.com\/Megvii-BaseDetection\/storage\/releases\/download\/0.0.1\/yolox_s.pth'\nMODEL = 'yolox_s.pth'\nwith open('script.sh', 'w') as file:\n    file.write(sh)\n!bash script.sh","ba1afd61":"!cp \/kaggle\/working\/YOLOX\/tools\/train.py .\/","0b32f69c":"!python train.py -f cots_config.py -d 1 -b 32 --fp16 -o -c {MODEL}","825da6c4":"from yolox.utils import postprocess\nfrom yolox.data.data_augment import ValTransform","8cb781bd":"current_exp = importlib.import_module('cots_config')\nexp = current_exp.Exp()\ntest_size = (640, 640)\nnum_classes = 1\nconfthre = 0.01\nnmsthre = 0.65\nmodel = exp.get_model()\nmodel.cuda()\nmodel.eval()\nckpt = torch.load(\"\/kaggle\/working\/YOLOX\/YOLOX_outputs\/cots_config\/best_ckpt.pth\", map_location=\"cpu\")\nmodel.load_state_dict(ckpt[\"model\"])","c7bebdee":"%cd \/kaggle\/working\/\ndef yolox_inference(img, model, test_size): \n    bboxes = []\n    bbclasses = []\n    scores = []\n    \n    preproc = ValTransform(legacy = False)\n\n    tensor_img, _ = preproc(img, None, test_size)\n    tensor_img = torch.from_numpy(tensor_img).unsqueeze(0)\n    tensor_img = tensor_img.float()\n    tensor_img = tensor_img.cuda()\n\n    with torch.no_grad():\n        outputs = model(tensor_img)\n        outputs = postprocess(\n                    outputs, num_classes, confthre,\n                    nmsthre, class_agnostic=True\n                )\n\n    if outputs[0] is None:\n        return [], [], []\n    \n    outputs = outputs[0].cpu()\n    bboxes = outputs[:, 0:4]\n\n    bboxes \/= min(test_size[0] \/ img.shape[0], test_size[1] \/ img.shape[1])\n    bbclasses = outputs[:, 6]\n    scores = outputs[:, 4] * outputs[:, 5]\n    \n    return bboxes, bbclasses, scores","bf127c9d":"env = greatbarrierreef.make_env()   # initialize the environment\niter_test = env.iter_test()    # an iterator which loops over the test set and sample submission","6a738f91":"submission_dict = {\n    'id': [],\n    'prediction_string': [],\n}\n\nfor (image_np, sample_prediction_df) in iter_test:\n \n    bboxes, bbclasses, scores = yolox_inference(image_np, model, test_size)\n    \n    predictions = []\n    for i in range(len(bboxes)):\n        box = bboxes[i]\n        cls_id = int(bbclasses[i])\n        score = scores[i]\n        if score < confthre:\n            continue\n        x_min = int(box[0])\n        y_min = int(box[1])\n        x_max = int(box[2])\n        y_max = int(box[3])\n        \n        bbox_width = x_max - x_min\n        bbox_height = y_max - y_min\n        \n        predictions.append('{:.2f} {} {} {} {}'.format(score, x_min, y_min, bbox_width, bbox_height))\n    \n    prediction_str = ' '.join(predictions)\n    sample_prediction_df['annotations'] = prediction_str\n    env.predict(sample_prediction_df)\n\n    print('Prediction:', prediction_str)","99038426":"sub_df = pd.read_csv('submission.csv')\nsub_df.head()","399aea10":"# 6. Training","b784a931":"# 5. Weights","380b3285":"# 1. Installing YOLOX Libraries","9a179df7":"There are two kinds of evaluators that can be used: the COCO evaluator and the VOC evaluator, both requiring specific formats. Only the COCO evaluator will be made for the time being. ","b49593b0":"# 7. Evaluation","8dae355f":"# 3. Convert Dataset to YOLOX Format","edecdfd0":"**EDIT Version 12**: Because we will be cutting out any entries that do not contain any starfish, we will use GroupKFolds to create our folds. ","212b175d":"The following was taken from Remek Kina's excellent training pipeline notebook: https:\/\/www.kaggle.com\/remekkinas\/yolox-training-pipeline-cots-dataset-lb-0-507","1463987c":"There are multiple kinds of YOLOX models, but for the time being, we will just focus on YOLOX-s.","e8cea4e6":"# 4. Apply Evaluator","28916244":"Credit over to Remek Kinas for the following code: \n\nCode: https:\/\/www.kaggle.com\/remekkinas\/yolox-inference-on-kaggle-for-cots-lb-0-507?scriptVersionId=81625924","febb9785":"And now to import the weights file for YOLOX-s.","0a6dac0d":"# 2. Splitting Data Into Training\/Validation"}}