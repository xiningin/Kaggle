{"cell_type":{"a4a22934":"code","9bcbe314":"code","c6a62d59":"code","7247ec77":"code","8a4a546f":"code","62e50f28":"code","d37cd9ff":"code","4f1ab637":"code","a36a0226":"code","4c68b6b9":"code","3d24013d":"code","23ab654c":"code","0bdb608c":"code","5dcb8a88":"code","3f1b2f64":"code","c58cc39d":"code","b53a474a":"code","fd1e5629":"code","a016cf3f":"code","f325f467":"code","bc04595b":"code","f5bb24de":"code","8b4f8440":"code","2b77616f":"code","fddae3af":"code","ddf1a6ef":"code","c097598b":"code","72c8c527":"code","de6ca8ce":"code","e39a8b98":"code","01ea8b70":"markdown","1561cba7":"markdown","85f21284":"markdown","9c6d0f7d":"markdown","37a7eb31":"markdown","961112bf":"markdown","fd1dad90":"markdown","67f485af":"markdown","d4929625":"markdown","362e300a":"markdown","c06d2882":"markdown","47c862b1":"markdown","58f488c8":"markdown","dec86f30":"markdown","2bf240a3":"markdown","94f0b108":"markdown","b1bdc65f":"markdown","674a34dd":"markdown","2a126ff0":"markdown","9e877ea7":"markdown","be4a40ab":"markdown"},"source":{"a4a22934":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm_notebook\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9bcbe314":"train = pd.read_csv('..\/input\/labeledTrainData.tsv', header = 0, \n                    delimiter= \"\\t\", quoting = 3)\nprint(train.shape)\ndisplay(train.head())","c6a62d59":"\nexample1 = BeautifulSoup(train['review'][0])\nprint(train['review'][0])\nprint(\"\\n\\n\")\nprint(example1.get_text())","7247ec77":"import re\nletters_only = re.sub(\"[^a-zA-Z]\", \" \", example1.get_text())\nprint(letters_only)","8a4a546f":"lower_case = letters_only.lower()\nwords = lower_case.split()\nprint(words)","62e50f28":"import nltk\nfrom nltk.corpus import stopwords\n\nwords = [w for w in words if not w in stopwords.words(\"english\")]\nprint(words)","d37cd9ff":"stops = set(stopwords.words(\"english\"))\ndef review_to_words(review):\n    #1. Remove HTML TAGS\n    review = BeautifulSoup(review).get_text()\n    #2. Remove punctuation and numbers\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review)\n    #3. Convert to lower case and split into words\n    lower_case = letters_only.lower()\n    words = lower_case.split()\n    #4. Remove Stop Words\n    words = [w for w in words if not w in stops]\n    #5. Return string\n    return \" \".join(words)","4f1ab637":"review_to_words(train['review'][0])","a36a0226":"num_reviews = train.shape[0]\ntrain_corpus = []\nfor i in tqdm_notebook(range(num_reviews)):\n    train_corpus.append(review_to_words(train['review'][i]))","4c68b6b9":"len(train_corpus[-1].split())","3d24013d":"from sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer(analyzer = 'word', tokenizer = None, \n                            preprocessor = None, stop_words = None, \n                            max_features = 8000)","23ab654c":"vectorizer.fit(train_corpus)","0bdb608c":"X_train = vectorizer.transform(train_corpus).toarray()\ny_train = train.sentiment.values.reshape(-1,1)\nprint(X_train.shape, y_train.shape)","5dcb8a88":"print(len(train_corpus[0]))\nprint(np.sum(X_train, axis = 1))","3f1b2f64":"from sklearn.cross_validation import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_train , y_train.reshape(-1, ), \n                                                test_size = 0.2,\n                                                stratify = y_train.reshape(-1, ), \n                                                random_state = 42)\nprint(\"Training Data = \", X_train.shape, y_train.shape)\nprint(\"Validation Date = \", X_val.shape, y_val.shape)","c58cc39d":"test = pd.read_csv(\"..\/input\/testData.tsv\", delimiter = \"\\t\", \n                  header = 0, quoting = 3)\nprint(test.shape)\ntest.head()","b53a474a":"num_test_reviews = test.shape[0]\ntest_corpus = []\nfor i in tqdm_notebook(range(num_test_reviews)):\n    test_corpus.append(review_to_words(test['review'][i]))\nprint(len(test))","fd1e5629":"X_test = vectorizer.transform(test_corpus).toarray()\nprint(X_test.shape)\nX_test","a016cf3f":"print(X_train)","f325f467":"from sklearn import naive_bayes\nfrom sklearn.metrics import confusion_matrix\nclfrNB = naive_bayes.MultinomialNB()\nclfrNB.fit(X_train, y_train.reshape(-1, ))\ny_pred = clfrNB.predict(X_train)\ncm_train = confusion_matrix(y_train, y_pred)\ny_pred = clfrNB.predict(X_val)\ncm_val = confusion_matrix(y_val, y_pred)\nprint(\"Training Accuracy = \", clfrNB.score(X_train, y_train))\nprint(\"Validation Accuracy\", clfrNB.score(X_val, y_val))\nprint(cm_train)\nprint(cm_val)","bc04595b":"y_test = clfrNB.predict(X_test)\nprint(y_test.shape)\ny_test","f5bb24de":"sample = pd.read_csv(\"..\/input\/sampleSubmission.csv\", header = 0)\nsample.head()","8b4f8440":"sub = pd.DataFrame()\nsub['id'] = sample['id'].values\nsub['sentiment'] = y_test\nsub.to_csv(\"naiveBayesPred.csv\", index = False)","2b77616f":"sub.head()","fddae3af":"from sklearn.ensemble import RandomForestClassifier\nclfrRMC = RandomForestClassifier(n_estimators= 500, criterion = \"gini\", \n                                max_features = \"auto\", max_depth = 8, \n                                min_samples_split = 2, random_state = 42)\nclfrRMC.fit(X_train, y_train.reshape(-1, ))\ny_pred = clfrRMC.predict(X_train)\ncm_train = confusion_matrix(y_train, y_pred)\ny_pred = clfrRMC.predict(X_val)\ncm_val = confusion_matrix(y_val, y_pred)\nprint(\"Training Accuracy = \", clfrRMC.score(X_train, y_train))\nprint(\"Validation Accuracy = \", clfrRMC.score(X_val, y_val))\nprint(cm_train)\nprint(cm_val)","ddf1a6ef":"y_test = clfrRMC.predict(X_test)\nprint(y_test.shape)\nprint(y_test)","c097598b":"sub = pd.DataFrame()\nsub['id'] = sample['id']\nsub['sentiment'] = y_test\nsub.to_csv(\"randomForestPred.csv\", index = False)\nprint(sub.shape)\nsub.head()","72c8c527":"from sklearn.linear_model import LogisticRegression\nclfrLR = LogisticRegression(penalty = 'l2', C = 0.005)\nclfrLR.fit(X_train, y_train)\ny_pred = clfrLR.predict(X_train)\ncm_train = confusion_matrix(y_train, y_pred)\ny_pred = clfrLR.predict(X_val)\ncm_val = confusion_matrix(y_val, y_pred)\nprint(\"Training Accuracy = \", clfrLR.score(X_train, y_train))\nprint(\"Validation Accuracy = \", clfrLR.score(X_val, y_val))\nprint(cm_train)\nprint(cm_val)","de6ca8ce":"y_test = clfrLR.predict(X_test)\nprint(y_test.shape)\nprint(y_test)","e39a8b98":"sub = pd.DataFrame()\nsub['id'] = sample['id']\nsub['sentiment'] = y_test\nsub.to_csv(\"LogisticPrediction.csv\", index = False)\nprint(sub.shape)\nsub.head()","01ea8b70":"### Reading Data","1561cba7":"#### Prepare Test Data","85f21284":"##### Train-Validation Split","9c6d0f7d":"##### Remove Stop Words(a, the, this, is, and ...etc.)","37a7eb31":"##### Tokenization: Convert to lower case and split into words","961112bf":"##### Prediction on Test Data","fd1dad90":"### Text Preprocessing","67f485af":"### Logistic Regression","d4929625":"##### Save Prediction","362e300a":"##### Remove Punctuation and Numbers","c06d2882":"##### Prediction on test data","47c862b1":"#### DEMO\n##### Remove HTML TAGS Using Beautiful Soup","58f488c8":"##### Save Prediction ","dec86f30":"#### Prepare Training Data","2bf240a3":"### Training using Random Forest Classifier","94f0b108":"##### Get Training X and y","b1bdc65f":"#### Prediction on Test Data","674a34dd":"##### Count Vectorizer","2a126ff0":"##### Save Prediction as csv file","9e877ea7":"### Training Using Naive Bayes Classifier","be4a40ab":"##### Function which takes a review, does preprocessing and returns words"}}