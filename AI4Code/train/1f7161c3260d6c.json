{"cell_type":{"a2975e6c":"code","4e0c6057":"code","25f7c97f":"code","900e53b8":"code","162d18bd":"code","eed59f6b":"code","70cd18c0":"code","8aaa07d5":"code","d0941cf9":"code","76b502b0":"code","b176a882":"code","56e3a7cf":"code","80743360":"code","bd33f629":"code","1505b6fa":"code","be74fbf4":"code","34aa2d9e":"code","19330ac8":"code","ef9d4599":"code","2e264cc8":"code","8b0f33ba":"code","8618fd95":"code","9c3efbe3":"code","60963a65":"code","c1dce26d":"code","6f0a066d":"code","b66d2318":"code","1d4e4ab3":"code","ef1b96cf":"code","186ec57c":"code","0e8b06c5":"code","20a522b6":"code","17f6e2c6":"code","9419ac1f":"code","7b3fcb65":"code","5220fd8e":"code","8a8b7c2e":"code","5b14054d":"code","2b3ba0c4":"code","71933b68":"markdown","f2c3636e":"markdown","3b027522":"markdown","917c9d13":"markdown","30726969":"markdown"},"source":{"a2975e6c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e0c6057":"from category_encoders import CountEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\n \nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M","25f7c97f":"train_df = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_df_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\n#train_df_noscored = pd.read_csv(\"\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv\")\n\ntest_df = pd.read_csv(\"\/kaggle\/input\/lish-moa\/test_features.csv\")\nsample_sub = pd.read_csv(\"\/kaggle\/input\/lish-moa\/sample_submission.csv\")","900e53b8":"train_df.head(2)","162d18bd":"test_nulls = test_df['cp_type'] == 'ctl_vehicle'","eed59f6b":"train_df = train_df.iloc[:, 1:]\nc_e = CountEncoder(cols=['cp_type', 'cp_dose'])\ntrain_df = c_e.fit_transform(train_df)\ntrain_df.head()","70cd18c0":"test_df = test_df.iloc[:, 1:]\ntest_df = c_e.transform(test_df)\ntest_df.head()","8aaa07d5":"def multi_log_loss(y_true, y_pred):\n    losses = []\n    for col in y_true.columns:\n        losses.append(log_loss(y_true.loc[:, col], y_pred.loc[:, col]))\n    return np.mean(losses)","d0941cf9":"def multi_log_loss_bag(y_true, y_pred):\n    losses = []\n    for col in y_true.columns:\n        try:\n            losses.append(log_loss(y_true.loc[:, col], y_pred.loc[:, col]))\n        except:\n            continue\n    return np.mean(losses)","76b502b0":"train_df_scored = train_df_scored.iloc[:, 1:]","b176a882":"N_FEATS = train_df.shape[1]\nN_TARGETS = train_df_scored.shape[1]","56e3a7cf":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_df, train_df_scored, test_size=0.33, random_state=42)","80743360":"SEED = 1234\nEPOCHS = 15\nBATCH_SIZE = 128\nLR = 0.0005","bd33f629":"def create_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(N_FEATS),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.1),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.7),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.3),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(N_TARGETS, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = LR), loss='binary_crossentropy', metrics=[\"accuracy\"])\n    return model","1505b6fa":"model = create_model()\n\nmodel.fit(train_df,\n      train_df_scored,\n      epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2\n     )","be74fbf4":"pred = model.predict(X_test)\n\ny_pred = y_test.copy()\ny_pred.iloc[:, :] = pred\n\nmulti_log_loss_bag(y_test, y_pred)","34aa2d9e":"def create_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(N_FEATS),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(N_TARGETS, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = LR), loss='binary_crossentropy', metrics=[\"accuracy\"])\n    return model","19330ac8":"model = create_model()\n\nmodel.fit(train_df,\n      train_df_scored,\n      epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2\n     )","ef9d4599":"pred = model.predict(X_test)\n\ny_pred = y_test.copy()\ny_pred.iloc[:, :] = pred\n\nmulti_log_loss_bag(y_test, y_pred)","2e264cc8":"def create_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(N_FEATS),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.4),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=\"sigmoid\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.6),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(N_TARGETS, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = LR), loss='binary_crossentropy', metrics=[\"accuracy\"])\n    return model","8b0f33ba":"model = create_model()\n\nmodel.fit(train_df,\n      train_df_scored,\n      epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2\n     )","8618fd95":"pred = model.predict(X_test)\n\ny_pred = y_test.copy()\ny_pred.iloc[:, :] = pred\n\nmulti_log_loss_bag(y_test, y_pred)","9c3efbe3":"def create_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(N_FEATS),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(N_TARGETS, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = LR), loss='binary_crossentropy', metrics=[\"accuracy\"])\n    return model","60963a65":"model = create_model()\n\nmodel.fit(train_df,\n      train_df_scored,\n      epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2\n     )","c1dce26d":"pred = model.predict(X_test)\n\ny_pred = y_test.copy()\ny_pred.iloc[:, :] = pred\n\nmulti_log_loss_bag(y_test, y_pred)","6f0a066d":"def create_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(N_FEATS),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"sigmoid\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"sigmoid\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(N_TARGETS, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = LR), loss='binary_crossentropy', metrics=[\"accuracy\"])\n    return model","b66d2318":"model = create_model()\n\nmodel.fit(train_df,\n      train_df_scored,\n      epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2\n     )","1d4e4ab3":"pred = model.predict(X_test)\n\ny_pred = y_test.copy()\ny_pred.iloc[:, :] = pred\n\nmulti_log_loss_bag(y_test, y_pred)","ef1b96cf":"def create_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(N_FEATS),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.2),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(2048, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dropout(0.5),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(N_TARGETS, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = LR), loss='binary_crossentropy', metrics=[\"accuracy\"])\n    return model","186ec57c":"def create_model():\n    model = tf.keras.Sequential([\n    tf.keras.layers.Input(N_FEATS),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(1024, activation=\"relu\")),\n    tf.keras.layers.BatchNormalization(),\n    tfa.layers.WeightNormalization(tf.keras.layers.Dense(N_TARGETS, activation=\"sigmoid\"))\n    ])\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = LR), loss='binary_crossentropy', metrics=[\"accuracy\"])\n    return model","0e8b06c5":"train_df.head(2)","20a522b6":"SEED = 1234\nEPOCHS = 15\nBATCH_SIZE = 128\nLR = 0.0005","17f6e2c6":"model = create_model()\n\nmodel.fit(train_df,\n      train_df_scored,\n      epochs=EPOCHS, batch_size=BATCH_SIZE, verbose=2\n     )","9419ac1f":"pred = model.predict(test_df)","7b3fcb65":"sample_sub.iloc[:, 1:] = 0","5220fd8e":"sample_sub.iloc[:, 1:] = pred","8a8b7c2e":"sample_sub.iloc[:, 1:][test_nulls] = 0","5b14054d":"sample_sub","2b3ba0c4":"sample_sub.to_csv('submission.csv', index=False)","71933b68":"\u0423\u0431\u0435\u0440\u0451\u043c \u0430\u0439\u0434\u0438\u0448\u043d\u0438\u043a\u0438 \u0438 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0444\u0438\u0447\u0438","f2c3636e":"\u0424\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435","3b027522":"\u0421\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435","917c9d13":"\u041f\u0440\u043e\u0441\u0442\u0430\u0432\u0438\u043c \u043d\u0443\u043b\u0438 \u0432\u0441\u0435\u043c, \u0443 \u043a\u043e\u0433\u043e cp_type == 'ctl_vehicle', \u0447\u0442\u043e\u0431\u044b \u0443\u043b\u0443\u0447\u0448\u0438\u0442\u044c \u0440\u0435\u0448\u0435\u043d\u0438\u0435\n","30726969":"\u0412\u0432\u0435\u0434\u0451\u043c \u043d\u0430\u0448 \u043b\u043e\u0441\u0441"}}