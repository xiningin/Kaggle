{"cell_type":{"910ae6e6":"code","47fbff4f":"code","c31fb4d9":"code","1660c1d4":"code","374be1da":"code","85d75a72":"code","3f3a5b7a":"code","4871750d":"code","52934335":"code","fb328bdf":"code","6faaa979":"code","e0153e10":"code","182e924e":"markdown","1ece0dbd":"markdown","8a30d86e":"markdown","10057d5c":"markdown","b7e05a2f":"markdown","85d81bc3":"markdown","c85a1131":"markdown","c4992ae7":"markdown"},"source":{"910ae6e6":"import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset\nfrom skimage import io\n\n\nclass CatsAndDogsDataset(Dataset):\n    def __init__(self, csv_file, root_dir, transform=None\n                 ):\n        self.annotations = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.annotations)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])\n        image = io.imread(img_path)\n        y_label = torch.tensor(int(self.annotations.iloc[idx, 1]))\n        if self.transform:\n            image = self.transform(image)\n\n        return image, y_label\n\n    \n","47fbff4f":"import zipfile\nwith zipfile.ZipFile(\"..\/input\/dogs-vs-cats\/train.zip\",\"r\") as z:\n    z.extractall(\".\")","c31fb4d9":"import matplotlib.pyplot as plt\n\ndef loss_plot(epochs, loss):\n    plt.plot(epochs, loss)","1660c1d4":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\n# hyperparameter for rnn\n# inputsize means depth or features\nlearning_rate = 0.001\nbatch_size = 16\nnum_epochs = 1\n\n\nclass Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n\n\ndataset = CatsAndDogsDataset(csv_file=\"..\/input\/catvsdog\/catvsdog.csv\", root_dir='.\/train',\n                             transform=transforms.Compose([transforms.ToTensor(),transforms.Resize([100, 100]) ]))\ntrain_set, test_set = torch.utils.data.random_split(dataset, [20000, 5000])\ntrain_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n\n# initialize network\nmodel = torchvision.models.googlenet(pretrained=True)\nmodel.to(device)\n\n# loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epochs):\n    print(\"epoch\",epoch)\n    for batch_index, (data, targets) in enumerate(train_loader):\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        # forward\n        # print(data.shape)\n        scores = model(data)\n        loss = criterion(scores, targets)\n        \n        if batch_index % 10 == 0:\n            print(scores.shape)\n        \n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()\n    print(\"loss = \", loss)\n\n\ndef check_accuracy(loader, model,train=1):\n    if train:\n        print(\"training model\")\n    else:\n        print(\"testing model\")\n\n    num_correct = 0\n    num_samples = 0\n    model.eval()\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device=device)\n            y = y.to(device=device)\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n\n        print(\"accuracy = \", (float(num_correct) \/ float(num_samples)) * 100)\n    model.train()\n\n\ncheck_accuracy(train_loader, model)\ncheck_accuracy(test_loader, model,0)\n","374be1da":"import sys\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ninput_size = 100\nnum_classes = 2\nlearning_rate = 0.001\nbatch_size = 128\nnum_epoch = 2\n\ndataset = CatsAndDogsDataset(csv_file=\"..\/input\/catvsdog\/catvsdog.csv\", root_dir='.\/train',\n                             transform=transforms.Compose([transforms.ToTensor(), transforms.Resize([100, 100])]))\ntrain_set, test_set = torch.utils.data.random_split(dataset, [20000, 5000])\ntrain_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n\n\nclass NN(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(NN, self).__init__()\n        self.fc1 = nn.Linear(input_size*300, 500)\n        self.fc2 = nn.Linear(500, 50)\n        self.fc3 = nn.Linear(50, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\n\nmodel = NN(input_size, num_classes)\nmodel.to(device)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epoch):\n    losses=[]\n    batch_idxs=[]\n    for batch_idx, (data, targets) in enumerate(train_loader):\n#         print(batch_idx)\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        data = data.reshape(data.shape[0], -1)\n\n        # forward\n        scores = model(data)\n        loss = criterion(scores, targets)\n        \n        if batch_idx % 10 ==0:\n            print(loss,score)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()\n        \n#     loss_plot(batch_idxs,losses)\n","85d75a72":"\ndef check_accuracy(loader, model):\n    num_correct = 0\n    total_sample = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device)\n            x = x.reshape(x.shape[0], -1)\n\n            scores = model(x)\n            print(scores)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            total_sample += predictions.size(0)\n\n        print(\"accuracy = \", (float(num_correct) \/ float(total_sample)) * 100)\n    model.train()\n\n\ncheck_accuracy(train_loader, model)\ncheck_accuracy(test_loader, model)","3f3a5b7a":"import sys\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nin_channels = 3\nnum_classes = 2\nlearning_rate = 0.0001\nbatch_size = 64\nnum_epoch = 5\n\nclass CNN(nn.Module):\n    def __init__(self, in_channles, num_classes):\n        super(CNN, self).__init__()\n        self.conv6 = nn.Conv2d(in_channels=in_channles, out_channels=6, kernel_size=(3, 3), stride=(1, 1),\n                               padding=(1, 1))\n        self.pool1 = nn.MaxPool2d(2, stride=2)\n        self.conv1 = nn.Conv2d(in_channels=6, out_channels=8, kernel_size=(3, 3), stride=(2, 2),\n                               padding=(0, 0))\n        self.pool2 = nn.MaxPool2d(2, stride=2)\n        self.conv2 = nn.Conv2d(in_channels=8, out_channels=17, kernel_size=(3, 3), stride=(1, 1),\n                               padding=(0, 0))\n        self.pool3 = nn.MaxPool2d(2, stride=2)\n        self.fc1 = nn.Linear(5 * 5 * 17, num_classes)\n\n    def forward(self, x):\n        x = F.relu(self.conv6(x))\n#         print(x.shape)\n        x = self.pool1(x)\n#         print(x.shape)\n        x = F.relu(self.conv1(x))\n#         print(x.shape)\n        x = self.pool2(x)\n#         print(x.shape)\n        x = F.relu(self.conv2(x))\n#         print(x.shape)\n        x = self.pool3(x)\n#         print(x.shape)\n#         x = F.relu(self.conv3(x))\n# #         print(x.shape)\n#         x = F.relu(self.conv4(x))\n# #         print(x.shape)\n#         x = F.relu(self.conv5(x))\n#         print(x.shape)\n        x = x.reshape(x.shape[0], -1)\n        x = F.softmax(self.fc1(x),dim=1)\n        return x\n    \n\ndataset = CatsAndDogsDataset(csv_file=\"..\/input\/catvsdog\/catvsdog.csv\", root_dir='.\/train',\n                             transform=transforms.Compose([transforms.ToTensor(), transforms.Resize([101, 101])]))\ntrain_set, test_set = torch.utils.data.random_split(dataset, [20000, 5000])\ntrain_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n\nmodel = CNN(in_channels, num_classes).to(device)\n# model = torchvision.models.vgg16(pretrained=True)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","4871750d":"for epoch in range(num_epoch):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        # forward\n        scores = model(data)\n        loss = criterion(scores, targets)\n        \n        if batch_idx%100==0:\n            print(loss)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        optimizer.step()","52934335":"\ndef check_accuracy(loader, model):\n    num_correct = 0\n    total_sample = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            total_sample += predictions.size(0)\n\n        print(\"accuracy = \", (float(num_correct) \/ float(total_sample)) * 100)\n    model.train()\n\n\ncheck_accuracy(train_loader, model)\ncheck_accuracy(test_loader, model)","fb328bdf":"import sys\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\nimport torchvision.datasets as datasets\nimport torchvision.transforms as transforms\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\ninput_size = 100\nnum_layers=2\nsequence_length=100\nhidden_nodes = 100\nnum_classes = 2\nlearning_rate = 0.001\nbatch_size = 128\nnum_epoch = 5\n\nclass RNN(nn.Module):\n    def __init__(self, input_size, hidden_nodes,num_layers,num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size=hidden_nodes\n        self.num_layers=num_layers\n        self.rnn=nn.RNN(input_size,hidden_nodes,num_layers,batch_first=True)\n        self.fc = nn.Linear(hidden_nodes * sequence_length, num_classes)\n\n    def forward(self, x):\n        h0 = torch.zeros(self.num_layers,x.size(0),self.hidden_size).to(device)\n        x,_=self.rnn(x,h0)\n        x=x.reshape(x.shape[0],-1)\n        x = self.fc(x)\n        \n        return x\n    \n\ndataset = CatsAndDogsDataset(csv_file=\"..\/input\/catvsdog\/catvsdog.csv\", root_dir='.\/train',\n                             transform=transforms.Compose([transforms.ToTensor(), transforms.Resize([100, 100])]))\ntrain_set, test_set = torch.utils.data.random_split(dataset, [20000, 5000])\ntrain_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)\n\n\nmodel = RNN(input_size, hidden_nodes,num_layers,num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)","6faaa979":"for epoch in range(num_epoch):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        data = data[:,-1,:,:]\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        # forward\n        scores = model(data)\n        loss = criterion(scores, targets)\n        \n        if batch_idx%100==0:\n            print(loss)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        \n        optimizer.step()","e0153e10":"\ndef check_accuracy(loader, model):\n    num_correct = 0\n    total_sample = 0\n    model.eval()\n\n    with torch.no_grad():\n        for x, y in loader:\n            x=x[:,-1,:,:]\n            x = x.to(device)\n            y = y.to(device)\n\n            scores = model(x)\n            _, predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            total_sample += predictions.size(0)\n\n        print(\"accuracy = \", (float(num_correct) \/ float(total_sample)) * 100)\n    model.train()\n\n\ncheck_accuracy(train_loader, model)\ncheck_accuracy(test_loader, model)","182e924e":"data set class defined","1ece0dbd":"CNN model training","8a30d86e":"zip file extraction of dog vs cat data set","10057d5c":"accuracy check for nn model","b7e05a2f":"basic NN model implementation","85d81bc3":"CNN model implementation","c85a1131":"googlenet model usage ","c4992ae7":"CNN model accuracy check"}}