{"cell_type":{"1813c3e7":"code","80909b0d":"code","54b24a2e":"code","12920b64":"code","5d1b89a1":"code","55472da0":"code","8703e8b2":"code","1b0e2d18":"code","7ed13b97":"code","dc2c89c9":"code","bc4fee48":"code","f26aac2d":"code","56fb9a09":"code","ac4271cf":"code","552e863f":"code","19de8f77":"code","6486e2fd":"code","0ca1f1e7":"code","31380efa":"code","f57517bd":"code","0fe4d300":"code","7f2d0838":"code","56d73d52":"code","e7b9264f":"code","aad1cf46":"code","dd804175":"code","74ab91e6":"code","5bcaadec":"code","8167e7e1":"code","23392a70":"code","457391be":"code","bc65d9f4":"code","481c05dd":"code","c47bf70f":"code","28cf5e38":"code","cec48e96":"code","240c6392":"code","68887129":"code","edb028f2":"code","674f788d":"code","cba00577":"code","df4e6de5":"code","5e2be4b3":"code","0cc0e9b0":"code","77d8455e":"code","c4147a5f":"code","9b058cc6":"code","f663fd0c":"code","ab642b1f":"markdown","93f6764f":"markdown","7b65543a":"markdown","9cdf31a8":"markdown","790b676a":"markdown","9458f075":"markdown","11195ae3":"markdown","3d1641ec":"markdown","c5704e75":"markdown","58e16267":"markdown","a5fa7b66":"markdown","685c5aa8":"markdown","d8bb8728":"markdown","65d54568":"markdown","c2a3248c":"markdown","6d25a225":"markdown","b778c859":"markdown","1a448587":"markdown","2e962939":"markdown","aa4f7c6c":"markdown","5553c89e":"markdown","8ec8e5bd":"markdown","d15a5d86":"markdown","4bc7e5a8":"markdown","ab2b0799":"markdown","ead1088a":"markdown"},"source":{"1813c3e7":"#%%\nimport time\nnotebookstart = time.time()\n\nimport warnings\nwarnings.filterwarnings('ignore')","80909b0d":"#%%\nrunningOnColab = False\nrunningOnKaggle = True\nrunningLocal = False","54b24a2e":"#%%\nroot_dir = ''\n'''\ntry:\n    from google.colab import drive, files\n    drive.mount('\/content\/drive')\n    root_dir = '\/content\/drive\/My Drive\/Colab Notebooks\/tensorflow-2-projects\/'\n\n    # To upload kaggle.json to mounted directory\n    # files.upload();\n\n    # Create directory for kaggle.json key file and copy file\n    get_ipython().system('mkdir -p ~\/.kaggle')\n    # In my case, 'Colab Notebooks' directory already contains my kaggle.json\n    get_ipython().system(\"cp '{root_dir}..\/kaggle.json' ~\/.kaggle\/\")\n\n    # Change the permission\n    get_ipython().system('chmod 600 ~\/.kaggle\/kaggle.json')\nexcept:\n    print('No GOOGLE DRIVE connection. Using local dataset(s).')\n'''","12920b64":"#%%\nif runningOnColab == True:\n    print('Enabling TensorFlow 2.x')\n    get_ipython().run_line_magic('tensorflow_version', '2.x')","5d1b89a1":"#%%\nimport os, sys, random, gc, shutil\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mimage\nimport matplotlib.colors as mcolors\nimport seaborn as sns\n\nimport cv2\n\nimport tensorflow as tf\n\nfrom collections import defaultdict\nfrom PIL import Image, ImageFont\nfrom skimage.transform import resize\nfrom zipfile import ZipFile\nfrom matplotlib import font_manager as fm\nfrom tqdm.autonotebook import trange, tqdm\n\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold","55472da0":"#%%\nprint(tf.version.VERSION)\nprint('GPU is', 'available' if\n      tf.config.experimental.list_physical_devices('GPU') else 'NOT AVAILABLE')\ndevice_name = tf.test.gpu_device_name()\nprint('Found GPU at: {}'.format(device_name))","8703e8b2":"#%%\nseed = 321\nrandom.seed(seed)\nnp.random.seed(seed)\ntf.random.set_seed(seed)\n\n# Config the matplotlib backend as plotting inline in IPython\nget_ipython().run_line_magic('matplotlib', 'inline')\n\nplt.figure(figsize=(14, 11))\nplt.style.use('seaborn')\n\ncolors = mcolors.TABLEAU_COLORS\nsns.set(\n    rc={\n        'figure.figsize': (14, 11),\n        'figure.facecolor': 'whitesmoke',\n        'axes.facecolor': 'whitesmoke',\n        'grid.color': 'slategrey',\n    })\nsns.color_palette(palette=list(colors.values()));","1b0e2d18":"#%%\nif runningOnKaggle == True:\n    root_dir = '\/kaggle\/input'\n    for dirname, _, filenames in os.walk(root_dir):\n        for filename in filenames:\n            print(os.path.join(dirname, filename))\n\n    root_dir = '\/kaggle\/input\/bengaliai-cv19'\nelse:\n    root_dir = ''","7ed13b97":"#%%\nif runningOnColab == True:\n    get_ipython().system('kaggle competitions download -c bengaliai-cv19')","dc2c89c9":"#%%\nif runningOnColab == True:\n    get_ipython().system('kaggle datasets download -d kaushal2896\/bengali-fonts')","bc4fee48":"#%%\nif runningOnColab == True:\n    get_ipython().system('kaggle datasets download -d kaushal2896\/kalpurush-fonts')","f26aac2d":"#%%\nif runningOnColab == True:\n    if os.path.exists('train.csv'):\n        print('Zip file is extracted already')\n    else:\n        print('Extracting the Zip file(s) ...')\n        for file in tqdm(os.listdir()):\n            if file.endswith('.zip'):\n                with ZipFile(file, 'r') as datazipfile:\n                    datazipfile.extractall()\n                    datazipfile.close()\n                    print(f'{file} Zip file extracted successfully')","56fb9a09":"#%%\nif runningOnKaggle == True:\n    bengalifont = ImageFont.truetype(\n        '\/kaggle\/input\/kalpurush-fonts\/kalpurush-2.ttf')\n    fontproperties = fm.FontProperties(\n        fname='\/kaggle\/input\/kalpurush-fonts\/kalpurush-2.ttf')\n    print(fontproperties.get_name())\nelse:\n    bengalifont = ImageFont.truetype(root_dir + 'kalpurush-2.ttf')\n    fontproperties = fm.FontProperties(fname=root_dir + 'kalpurush-2.ttf')\n    print(fontproperties.get_name())","ac4271cf":"#%%\ndf_traincsv = pd.read_csv(os.path.join(root_dir, 'train.csv'))","552e863f":"#%%\ndf_classmap = pd.read_csv(os.path.join(root_dir, 'class_map.csv'))","19de8f77":"#%%\ndf_traincsv.shape","6486e2fd":"#%%\ndf_traincsv.head(10)","0ca1f1e7":"#%%\nprint(\n    f'Number of unique grapheme_root: {df_traincsv[\"grapheme_root\"].nunique()}'\n)\nprint(\n    f'Number of unique vowel_diacritic: {df_traincsv[\"vowel_diacritic\"].nunique()}'\n)\nprint(\n    f'Number of unique consonant_diacritic: {df_traincsv[\"consonant_diacritic\"].nunique()}'\n)","31380efa":"#%%\ndf_classmap.shape","f57517bd":"#%%\ndf_classmap.head(15)","0fe4d300":"#%%\ndf_classmap.tail(15)","7f2d0838":"#%%\ndf_classmap.loc[df_classmap['component_type'] == 'grapheme_root',\n                'component'].values","56d73d52":"#%%\ndf_classmap.loc[df_classmap['component_type'] == 'vowel_diacritic',\n                'component'].values","e7b9264f":"#%%\ndf_classmap.loc[df_classmap['component_type'] == 'consonant_diacritic',\n                'component'].values","aad1cf46":"#%%\nsns.countplot(x=df_classmap['component_type'])\nplt.xticks(\n    np.arange(3), ('grapheme_root', 'vowel_diacritic', 'consonant_diacritic'))\nplt.title('Bengali Language Component Type(s) - Distribution', fontsize=18)\nplt.show()","dd804175":"#%%\nsns.countplot(\n    y=df_traincsv['grapheme_root'],\n    order=df_traincsv['grapheme_root'].value_counts().index[:40],\n    orient='h',\n    palette=sns.color_palette())\nplt.xticks(rotation=45)\nplt.show()","74ab91e6":"#%%\nsns.countplot(\n    y=df_traincsv['vowel_diacritic'],\n    order=df_traincsv['vowel_diacritic'].value_counts().index[:15],\n    orient='h',\n    palette=sns.color_palette())\nplt.xticks(rotation=45)\nplt.show()","5bcaadec":"#%%\nsns.countplot(\n    y=df_traincsv['consonant_diacritic'],\n    order=df_traincsv['consonant_diacritic'].value_counts().index[:10],\n    orient='h')\nplt.xticks(rotation=45)\nplt.show()","8167e7e1":"#%%\ndf_traincsv.isnull().sum(axis=0)","23392a70":"#%%\ndf_classmap.isnull().sum(axis=0)","457391be":"#%%\ndf_traincsv.drop(labels=['image_id',  'grapheme'], axis=1, inplace=True)\ngc.collect()","bc65d9f4":"#%%\nORIGINAL_IMAGE_HEIGHT = 137\nORIGINAL_IMAGE_WIDTH = 236\n\nIMAGE_SIZE = 64\n\nNO_OF_CHANNELS = 1","481c05dd":"#%%\ndef sizeof_fmt(num, suffix='B'):\n    ''' by Fred Cirera,  https:\/\/stackoverflow.com\/a\/1094933\/1870254, modified'''\n    for unit in ['', 'Ki', 'Mi', 'Gi', 'Ti', 'Pi', 'Ei', 'Zi']:\n        if abs(num) < 1024.0:\n            return \"%3.1f %s%s\" % (num, unit, suffix)\n        num \/= 1024.0\n    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n\n\ndef merge_dict(dict_list):\n    dd = defaultdict(list)\n    for d in dict_list:\n        for key, value in d.items():\n            if not hasattr(value, '__iter__'):\n                value = (value, )\n            [dd[key].append(v) for v in value]\n    return dict(dd)","c47bf70f":"#%%\nclass AllImageDataReader():\n    def __init__(self):\n        pass\n\n    def getImages(in_image_height,\n                  in_image_width,\n                  out_image_size=128,\n                  n_channels=3,\n                  return_index=False):\n        df_trainimagedata = np.empty(\n            (0, IMAGE_SIZE, IMAGE_SIZE, 1), dtype=np.uint8)\n        for i in tqdm(range(4), desc='Reading parquet'):\n            if return_index == True:\n                df_imagedata_tmp = pd.read_parquet(\n                    os.path.join(root_dir,\n                                 'train_image_data_{}.parquet'.format(i)))\n                df_indexdata_tmp = df_imagedata_tmp.pop('image_id')\n            else:\n                df_imagedata_tmp = pd.read_parquet(\n                    os.path.join(root_dir,\n                                 'train_image_data_{}.parquet'.format(i)),\n                    columns=[str(x) for x in range(32332)])\n            df_imagedata_tmp = 255 - df_imagedata_tmp.values.reshape(\n                -1, in_image_height, in_image_width).astype(np.uint8)\n            imgarr_tmp = []\n            for row in tqdm(\n                    range(df_imagedata_tmp.shape[0]), desc='Loading images'):\n                imgarr = cv2.resize(\n                    df_imagedata_tmp[row], (out_image_size, out_image_size),\n                    interpolation=cv2.INTER_CUBIC)\n                imgarr[imgarr < 28] = 0\n                imgarr_tmp.append(imgarr)\n            df_trainimagedata = np.vstack(\n                (df_trainimagedata,\n                 np.asarray(imgarr_tmp, dtype=np.uint8).reshape(\n                     -1, out_image_size, out_image_size, n_channels)))\n            del imgarr_tmp\n            del df_imagedata_tmp\n            gc.collect()\n\n        print(\n            f'Returning images of shape {df_trainimagedata.shape}, type {type(df_trainimagedata)} of {df_trainimagedata.dtype}'\n        )\n        if return_index == True:\n            return df_indexdata_tmp, df_trainimagedata\n        else:\n            return df_trainimagedata\n\n\ndf_trainimagedata = AllImageDataReader.getImages(\n    in_image_height=ORIGINAL_IMAGE_HEIGHT,\n    in_image_width=ORIGINAL_IMAGE_WIDTH,\n    out_image_size=IMAGE_SIZE,\n    n_channels=NO_OF_CHANNELS,\n    return_index=False)\n\nY_consonant_diacritic = tf.keras.utils.to_categorical(\n    df_traincsv['consonant_diacritic'], 7)\nY_grapheme_root = tf.keras.utils.to_categorical(df_traincsv['grapheme_root'],\n                                                168)\nY_vowel_diacritic = tf.keras.utils.to_categorical(\n    df_traincsv['vowel_diacritic'], 11)","28cf5e38":"#%%\nclass MultiOutputDataGenerator(\n        tf.keras.preprocessing.image.ImageDataGenerator):\n    def flow(\n            self,\n            x,\n            y=None,\n            batch_size=32,\n            shuffle=True,\n            sample_weight=None,\n            seed=None,\n            save_to_dir=None,\n            save_prefix='',\n            save_format='png',\n            subset=None,\n    ):\n\n        targets = None\n        target_lengths = {}\n        ordered_outputs = []\n\n        for (output, target) in y.items():\n            if targets is None:\n                targets = target\n            else:\n                targets = np.concatenate((targets, target), axis=1)\n\n            target_lengths[output] = target.shape[1]\n            ordered_outputs.append(output)\n\n        for (flowx, flowy) in super().flow(\n                x, targets, batch_size=batch_size, shuffle=shuffle):\n\n            target_dict = {}\n            i = 0\n\n            for output in ordered_outputs:\n                target_length = target_lengths[output]\n                target_dict[output] = flowy[:, i:i + target_length]\n                i += target_length\n\n            yield (flowx, target_dict)\n\n\ndatagenerator = MultiOutputDataGenerator(\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    zoom_range=0.1,\n    rotation_range=20,\n    shear_range=0.1,\n    fill_mode=\"nearest\")","cec48e96":"#%%\nclass ConvolutionModelGenerator():\n    def __init__(self):\n        pass\n\n    def buildModel(image_size=128, n_channels=1):\n        base_model_input = tf.keras.layers.Input(\n            shape=(image_size, image_size, n_channels))\n        base_model = tf.keras.layers.Conv2D(\n            filters=64, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model_input)\n        base_model = tf.keras.layers.ZeroPadding2D((1, 1))(base_model)\n        base_model = tf.keras.layers.Conv2D(\n            filters=64, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.BatchNormalization(\n            momentum=0.15)(base_model)\n        base_model = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(base_model)\n        base_model = tf.keras.layers.Conv2D(\n            filters=64, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.Dropout(rate=0.35)(base_model)\n\n        base_model = tf.keras.layers.Conv2D(\n            filters=128, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.ZeroPadding2D((1, 1))(base_model)\n        base_model = tf.keras.layers.Conv2D(\n            filters=128, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.BatchNormalization(\n            momentum=0.15)(base_model)\n        base_model = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(base_model)\n        base_model = tf.keras.layers.Conv2D(\n            filters=128, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.BatchNormalization(\n            momentum=0.15)(base_model)\n        base_model = tf.keras.layers.Dropout(rate=0.35)(base_model)\n\n        base_model = tf.keras.layers.Conv2D(\n            filters=256, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.ZeroPadding2D((1, 1))(base_model)\n        base_model = tf.keras.layers.Conv2D(\n            filters=256, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.BatchNormalization(\n            momentum=0.15)(base_model)\n        base_model = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(base_model)\n        base_model = tf.keras.layers.Conv2D(\n            filters=256, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.BatchNormalization(\n            momentum=0.15)(base_model)\n        base_model = tf.keras.layers.Dropout(rate=0.35)(base_model)\n\n        base_model = tf.keras.layers.Conv2D(\n            filters=512, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.ZeroPadding2D((1, 1))(base_model)\n        base_model = tf.keras.layers.Conv2D(\n            filters=512, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.BatchNormalization(\n            momentum=0.15)(base_model)\n        base_model = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(base_model)\n        base_model = tf.keras.layers.Conv2D(\n            filters=512, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.BatchNormalization(\n            momentum=0.15)(base_model)\n        base_model = tf.keras.layers.Dropout(rate=0.35)(base_model)\n\n        base_model = tf.keras.layers.Conv2D(\n            filters=512, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.ZeroPadding2D((1, 1))(base_model)\n        base_model = tf.keras.layers.Conv2D(\n            filters=512, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.BatchNormalization(\n            momentum=0.15)(base_model)\n        base_model = tf.keras.layers.MaxPool2D(pool_size=(2, 2))(base_model)\n        base_model = tf.keras.layers.Conv2D(\n            filters=512, kernel_size=(3, 3), padding='same',\n            activation='relu')(base_model)\n        base_model = tf.keras.layers.BatchNormalization(\n            momentum=0.15)(base_model)\n        base_model = tf.keras.layers.Dropout(rate=0.35)(base_model)\n\n        base_model = tf.keras.layers.Flatten()(base_model)\n        base_model = tf.keras.layers.Dense(2048, activation=\"relu\")(base_model)\n        base_model = tf.keras.layers.Dropout(rate=0.35)(base_model)\n        base_model = tf.keras.layers.Dense(1024, activation=\"relu\")(base_model)\n        base_model = tf.keras.layers.Dropout(rate=0.35)(base_model)\n        base_model = tf.keras.layers.Dense(512, activation=\"relu\")(base_model)\n\n        prediction_layer_consonant_diacritic = tf.keras.layers.Dense(\n            units=7, activation='softmax',\n            name='consonant_diacritic')(base_model)\n        prediction_layer_grapheme_root = tf.keras.layers.Dense(\n            units=168, activation='softmax', name='grapheme_root')(base_model)\n        prediction_layer_vowel_diacritic = tf.keras.layers.Dense(\n            units=11, activation='softmax', name='vowel_diacritic')(base_model)\n\n        model = tf.keras.models.Model(\n            inputs=base_model_input,\n            outputs=[\n                prediction_layer_consonant_diacritic,\n                prediction_layer_grapheme_root,\n                prediction_layer_vowel_diacritic\n            ])\n\n        model.compile(\n            optimizer='adam',\n            loss='categorical_crossentropy',\n            metrics=['accuracy'])\n\n        plot_model = tf.keras.utils.plot_model(\n            model,\n            to_file='model.png',\n            show_shapes=True,\n            show_layer_names=True,\n            rankdir='TB',\n            expand_nested=True,\n            dpi=96)\n\n        return model, model.summary(), plot_model\n\n\n(model, summary, plot) = ConvolutionModelGenerator.buildModel(\n    image_size=IMAGE_SIZE, n_channels=NO_OF_CHANNELS)","240c6392":"#%%\nplot","68887129":"#%%\nno_of_images = 7\nrandomint = [\n    random.randrange(0, df_trainimagedata.shape[0])\n    for iter in range(no_of_images * no_of_images)\n]\n\nfor i in range(no_of_images * no_of_images):\n    plt.subplot(no_of_images, no_of_images, i + 1)\n    plt.imshow(X=df_trainimagedata[randomint[i]].reshape((IMAGE_SIZE,\n                                                          IMAGE_SIZE)))\n    gr = df_classmap.loc[(df_classmap['component_type'] == 'grapheme_root') & (\n        df_classmap['label'] == df_traincsv.iloc[randomint[i]]['grapheme_root']\n    )]['component'].values[0]\n    vd = df_classmap.loc[\n        (df_classmap['component_type'] == 'vowel_diacritic')\n        & (df_classmap['label'] == df_traincsv.iloc[randomint[i]]\n           ['vowel_diacritic'])]['component'].values[0]\n    cd = df_classmap.loc[\n        (df_classmap['component_type'] == 'consonant_diacritic')\n        & (df_classmap['label'] == df_traincsv.iloc[randomint[i]]\n           ['consonant_diacritic'])]['component'].values[0]\n    plt.title(f'{gr}  {vd}  {cd}', fontproperties=fontproperties, fontsize=18)\n    plt.grid(False)\n    plt.xticks([])\n    plt.yticks([])\n\nplt.tight_layout()\nplt.show()\n\ndel (no_of_images, randomint, gr, vd, cd, i)\ngc.collect()","edb028f2":"#%%\nfor name, size in sorted(\n    ((name, sys.getsizeof(value)) for name, value in globals().items()),\n        key=lambda x: -x[1])[:10]:\n    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))","674f788d":"#%%\nn_splits = 3\nepochs = 100\nbatch_size = 128\n\nhistories = []\n\nmodel_name = 'bengaliai_classification_model.h5'\n\nmodelcheckpoint = tf.keras.callbacks.ModelCheckpoint(\n    filepath=model_name,\n    monitor='val_grapheme_root_accuracy',\n    save_best_only=True,\n    save_weights_only=False,\n    verbose=1)\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    monitor='val_grapheme_root_accuracy',\n    patience=2,\n    restore_best_weights=True)\nreduceLR_grapheme_root = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='grapheme_root_accuracy',\n    patience=2,\n    verbose=1,\n    factor=0.15,\n    min_lr=1e-5)  #0.00001\nreduceLR_vowel_diacritic = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='vowel_diacritic_accuracy',\n    patience=2,\n    verbose=1,\n    factor=0.15,\n    min_lr=1e-5)\nreduceLR_consonant_diacritic = tf.keras.callbacks.ReduceLROnPlateau(\n    monitor='consonant_diacritic_accuracy',\n    patience=2,\n    verbose=1,\n    factor=0.15,\n    min_lr=1e-5)\n\nfor train_index, test_index in tqdm(\n        KFold(n_splits, random_state=seed).split(df_trainimagedata),\n        total=n_splits,\n        desc=\"KFold\"):\n    x_train, x_test = df_trainimagedata[train_index], df_trainimagedata[\n        test_index]\n    cd_train, cd_test = Y_consonant_diacritic[\n        train_index], Y_consonant_diacritic[test_index]\n    gr_train, gr_test = Y_grapheme_root[train_index], Y_grapheme_root[\n        test_index]\n    vd_train, vd_test = Y_vowel_diacritic[train_index], Y_vowel_diacritic[\n        test_index]\n\n    history = model.fit(\n        datagenerator.flow(\n            x_train, {\n                'consonant_diacritic': cd_train,\n                'grapheme_root': gr_train,\n                'vowel_diacritic': vd_train\n            },\n            batch_size=batch_size,\n            seed=seed),\n        epochs=epochs,\n        steps_per_epoch=x_train.shape[0] \/\/ batch_size,\n        validation_data=(x_test, [cd_test, gr_test, vd_test]),\n        callbacks=[\n            modelcheckpoint, early_stopping, reduceLR_grapheme_root,\n            reduceLR_vowel_diacritic, reduceLR_consonant_diacritic\n        ],\n        verbose=2)\n    histories.append(history)\n\n    del (x_train, cd_train, gr_train, vd_train, x_test, cd_test, gr_test,\n         vd_test)\n    gc.collect()","cba00577":"#%%\nhistory_list = [histories[x].history for x in range(len(histories))]\nfinalhistory = merge_dict(history_list)\npd.DataFrame(finalhistory).to_csv('history.csv', index=False)","df4e6de5":"#%%\ndel (df_trainimagedata)\ngc.collect()","5e2be4b3":"#%%\nplt.subplot(2, 2, 1)\nplt.plot(finalhistory['loss'])\nplt.plot(finalhistory['val_loss'])\nplt.title('Model Loss', fontsize=18)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\n\nplt.subplot(2, 2, 2)\nplt.plot(finalhistory['grapheme_root_loss'])\nplt.plot(finalhistory['val_grapheme_root_loss'])\nplt.title('Model Loss (grapheme_root)', fontsize=18)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\n\nplt.subplot(2, 2, 3)\nplt.plot(finalhistory['vowel_diacritic_loss'])\nplt.plot(finalhistory['val_vowel_diacritic_loss'])\nplt.title('Model Loss (vowel_diacritic)', fontsize=18)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\n\nplt.subplot(2, 2, 4)\nplt.plot(finalhistory['consonant_diacritic_loss'])\nplt.plot(finalhistory['val_consonant_diacritic_loss'])\nplt.title('Model Loss (consonant_diacritic)', fontsize=18)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\n\nplt.tight_layout()\nplt.show()","0cc0e9b0":"#%%\nplt.subplot(2, 2, 1)\nplt.plot(finalhistory['grapheme_root_accuracy'])\nplt.plot(finalhistory['val_grapheme_root_accuracy'])\nplt.title('Model Accuracy (grapheme_root)', fontsize=18)\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n\nplt.subplot(2, 2, 2)\nplt.plot(finalhistory['vowel_diacritic_accuracy'])\nplt.plot(finalhistory['val_vowel_diacritic_accuracy'])\nplt.title('Model Accuracy (vowel_diacritic)', fontsize=18)\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n\nplt.subplot(2, 2, 3)\nplt.plot(finalhistory['consonant_diacritic_accuracy'])\nplt.plot(finalhistory['val_consonant_diacritic_accuracy'])\nplt.title('Model Accuracy (consonant_diacritic)', fontsize=18)\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n\nplt.tight_layout()\nplt.show()","77d8455e":"#%%\nif runningOnKaggle == True:\n    model_name = '\/kaggle\/input\/bengaliai-grapheme-classification-64x64-kfold\/bengaliai_classification_model_64_conv_allimgatonce_v10.h5'\n\n\nmodel = tf.keras.models.load_model(model_name)","c4147a5f":"#%%\nclass ImageDataReader():\n    def __init__(self):\n        pass\n\n    def getImages(filepath,\n                  in_image_height,\n                  in_image_width,\n                  out_image_size=128,\n                  n_channels=3,\n                  return_index=False):\n        df_imagedata = np.empty(\n            (0, out_image_size, out_image_size, n_channels), dtype=np.uint8)\n        if return_index == True:\n            df_imagedata_tmp = pd.read_parquet(filepath)\n            df_indexdata = df_imagedata_tmp.pop('image_id')\n        else:\n            df_imagedata_tmp = pd.read_parquet(\n                filepath, columns=[str(x) for x in range(32332)])\n        df_imagedata_tmp = 255 - df_imagedata_tmp.values.reshape(\n            -1, in_image_height, in_image_width).astype(np.uint8)\n        imgarr_tmp = []\n        for row in tqdm(\n                range(df_imagedata_tmp.shape[0]), desc='Loading images'):\n            imgarr = cv2.resize(\n                df_imagedata_tmp[row], (out_image_size, out_image_size),\n                interpolation=cv2.INTER_CUBIC)\n            imgarr[imgarr < 28] = 0\n            imgarr_tmp.append(imgarr)\n        df_imagedata = np.vstack(\n            (df_imagedata, np.asarray(imgarr_tmp, dtype=np.uint8).reshape(\n                -1, out_image_size, out_image_size, n_channels)))\n        del imgarr_tmp\n        del df_imagedata_tmp\n        gc.collect()\n\n        print(\n            f'Returning images of shape {df_imagedata.shape}, type {type(df_imagedata)} of {df_imagedata.dtype}'\n        )\n\n        if return_index == True:\n            return df_indexdata, df_imagedata\n        else:\n            return df_imagedata","9b058cc6":"#%%\npreds_dict = {\n    'consonant_diacritic': [],\n    'grapheme_root': [],\n    'vowel_diacritic': []\n}\ncomponents = ['consonant_diacritic', 'grapheme_root', 'vowel_diacritic']\n\ntarget = []  # model predictions placeholder\nrow_id = []  # row_id place holder\n\nfor i in trange(4):\n    df_testindexdata, df_testimagedata = ImageDataReader.getImages(\n        filepath=os.path.join(root_dir,\n                              'test_image_data_{}.parquet'.format(i)),\n        in_image_height=ORIGINAL_IMAGE_HEIGHT,\n        in_image_width=ORIGINAL_IMAGE_WIDTH,\n        out_image_size=IMAGE_SIZE,\n        n_channels=NO_OF_CHANNELS,\n        return_index=True)\n\n    preds = model.predict(df_testimagedata)\n\n    for i, p in enumerate(preds_dict):\n        preds_dict[p] = np.argmax(preds[i], axis=1)\n\n    for k, id in enumerate(df_testindexdata.values):\n        for i, comp in enumerate(components):\n            id_sample = id + '_' + comp\n            row_id.append(id_sample)\n            target.append(preds_dict[comp][k])\n    del df_testindexdata\n    del df_testimagedata\n    gc.collect()\n\ndf_sample = pd.DataFrame(\n    {\n        'row_id': row_id,\n        'target': target\n    }, columns=['row_id', 'target'])\ndf_sample.to_csv('submission.csv', index=False)\ndf_sample.head(12)","f663fd0c":"#%%\nprint(\"Notebook Runtime: %0.2f Minutes\" % ((time.time() - notebookstart) \/ 60))","ab642b1f":"# Data preprocessing","93f6764f":"# Setup notebook","7b65543a":"## Plot model performances","9cdf31a8":"## Training the model in KFold","790b676a":"## Import libraries","9458f075":"## Drop non-required data column(s)","11195ae3":"## Define generic methods","3d1641ec":"## Verify data","c5704e75":"## Plot data insights","58e16267":"### Load the data","a5fa7b66":"# Download and load data","685c5aa8":"# Bengali.AI Handwritten Grapheme Classification\n---\nAuthor: Bhaveshkumar Thaker\n\n---\n---\nPublic Score: 0.9490\n\n---\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1095143%2Fa9a48686e3f385d9456b59bf2035594c%2Fdesc.png?generation=1576531903599785&alt=media)\n\nBengali is the 5th most spoken language in the world with hundreds of million of speakers. It\u2019s the official language of Bangladesh and the second most spoken language in India. Considering its reach, there\u2019s significant business and educational interest in developing AI that can optically recognize images of the language handwritten. This challenge hopes to improve on approaches to Bengali recognition.\n\nOptical character recognition is particularly challenging for Bengali. While Bengali has 49 letters (to be more specific 11 vowels and 38 consonants) in its alphabet, there are also 18 potential diacritics, or accents. This means that there are many more graphemes, or the smallest units in a written language. The added complexity results in ~13,000 different grapheme variations (compared to English\u2019s 250 graphemic units).\n\n## Classify the components of handwritten Bengali\nThis dataset contains images of individual hand-written Bengali characters. Bengali characters (graphemes) are written by combining three components: a grapheme_root, vowel_diacritic, and consonant_diacritic. Your challenge is to classify the components of the grapheme in each image. There are roughly 10,000 possible graphemes, of which roughly 1,000 are represented in the training set. The test set includes some graphemes that do not exist in train but has no new grapheme components. It takes a lot of volunteers filling out sheets like this to generate a useful amount of real data; focusing the problem on the grapheme components rather than on recognizing whole graphemes should make it possible to assemble a Bengali OCR system without handwriting samples for all 10,000 graphemes.\n\n## Files\n**train.csv**\nimage_id: the foreign key for the parquet files\n\n*   `grapheme_root`: the first of the three target classes\n*   `vowel_diacritic`: the second target class\n*   `consonant_diacritic`: the third target class\n*   `grapheme`: the complete character. Provided for informational purposes only, you should not need to use this.\n\n**(train\/test).parquet**\nEach parquet file contains tens of thousands of 137x236 grayscale images. The images have been provided in the parquet format for I\/O and space efficiency. Each row in the parquet files contains an `image_id` column, and the flattened image.\n\n**class_map.csv**\nMaps the class labels to the actual Bengali grapheme components.","d8bb8728":"## Load best model","65d54568":"### Plot model loss performances","c2a3248c":"## Submit predictions","6d25a225":"# Load best model and make predictions","b778c859":"## Connect to Google Drive or Kaggle if require","1a448587":"## Define defaults","2e962939":"## Install libraries","aa4f7c6c":"*   [ImageDataGenerator for multiple output and single input](https:\/\/github.com\/keras-team\/keras\/issues\/12639#issuecomment-506338552)\n*   [How to use fit_generator with multiple outputs of different type](https:\/\/stackoverflow.com\/a\/41872896)","5553c89e":"## Setup defaults","8ec8e5bd":"## Check for missing data","d15a5d86":"### Plot model accuracy performances","4bc7e5a8":"## Load test data","ab2b0799":"## Data insights","ead1088a":"# Build and train the model"}}