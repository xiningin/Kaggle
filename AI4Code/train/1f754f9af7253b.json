{"cell_type":{"3edf83e9":"code","989d3558":"code","8b9aad6f":"code","dd41dd57":"code","ed2151bd":"code","dd5e60bc":"code","9c23dcca":"code","b317b751":"code","bf5e8c3e":"code","52b6f250":"code","95afabbb":"code","0146dbe0":"code","c9419a07":"code","9767c4a2":"code","d4646caa":"code","ac326248":"code","106bec24":"code","60fd99d2":"code","9862eedf":"code","875418d3":"code","38cfcf06":"markdown","bf15bc8f":"markdown","8c80ad02":"markdown","63148890":"markdown","4cf3e99f":"markdown","581748a9":"markdown","8279a344":"markdown","307b6728":"markdown","ef3064f4":"markdown"},"source":{"3edf83e9":"# Visualisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nimport missingno as msno\n\n\n# Configure visualisations\n%matplotlib inline\nmpl.style.use( 'ggplot' )\nplt.style.use('fivethirtyeight')\nsns.set(context=\"notebook\", palette=\"dark\", style = 'whitegrid' , color_codes=True)\n\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\");\n\n\n# Make Visualizations better\nparams = { \n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'x-large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n}\nplt.rcParams.update(params)","989d3558":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# IMPORTING THE DATA\n# Data set columns\nu_cols = ['user_id', 'age', 'sex', 'occupation', 'zip_code']\nusers = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.user', sep='|', names=u_cols,  encoding='latin-1')\n\n# Removing duplicates\nusers = users.drop_duplicates(keep='first')\n\n# Converting columns to specific data type\nuser_int_columns = ['user_id', 'age']\nusers[user_int_columns] = users[user_int_columns].applymap(np.int64)\n\ntotal_users = int(users.shape[0])","8b9aad6f":"users.head()","dd41dd57":"users.dtypes","ed2151bd":"i_cols = ['movie id', 'movie title', 'release date', 'video release date', 'IMDb URL', 'unknown', 'Action',\n          'Adventure', 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n          'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi', 'Thriller', 'War', 'Western']\n\nmovies = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.item', sep='|', names=i_cols, encoding='latin-1')\n\n# Removing duplicates\nmovies = movies.drop_duplicates(keep='first')\n\n# Dropping the unnecessary columns\nmovies.drop(['Action', 'Adventure', 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary',\n                          'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi',\n                          'Thriller', 'War', 'Western', 'video release date', 'IMDb URL'], axis=1, inplace=True)\n\ntotal_movies = int(movies.shape[0])","dd5e60bc":"movies.head()","9c23dcca":"movies.dtypes","b317b751":"r_cols = ['user_id', 'movie_id', 'rating', 'unix_timestamp']\nratings = pd.read_csv('..\/input\/movielens-100k-dataset\/ml-100k\/u.data', sep='\\t', names=r_cols, encoding='latin-1')\n\n# Dropping the timestamp column\nratings.drop(['unix_timestamp'], axis=1, inplace=True)\n\n# Removing duplicates\nratings = ratings.drop_duplicates(keep='first')\n\n# Converting to appropriate data types\nratings_int_columns = ['user_id', 'movie_id', 'rating']\nratings[ratings_int_columns] = ratings[ratings_int_columns].applymap(np.int64)\n\ntotal_ratings = int(ratings.shape[0])","bf5e8c3e":"ratings.head()","52b6f250":"ratings.dtypes","95afabbb":"frame = ratings.set_index('movie_id').join(movies.set_index('movie id'))\nratings_per_movie = frame['rating'].groupby(frame['movie title']).sum()\n\nprint('MOST RATED MOVIES')\nprint(ratings_per_movie.sort_values(ascending=False))\n\nfrom plotly.offline import init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\n\ndata = frame['rating'].value_counts().sort_index(ascending=False)\ntrace = go.Bar(x = data.index,\n               text = ['{:.1f} %'.format(val) for val in (data.values \/ frame.shape[0] * 100)],\n               textposition = 'auto',\n               textfont = dict(color = '#000000'),\n               y = data.values,\n               )\n# Create layout\nlayout = dict(title = 'Distribution Of {} movie-ratings'.format(frame.shape[0]),\n              xaxis = dict(title = 'Rating'),\n              yaxis = dict(title = 'Count'))\n# Create plot\nfig = go.Figure(data=[trace], layout=layout)\niplot(fig)","0146dbe0":"data_matrix = np.zeros((total_users, total_movies))\nfor line in ratings.itertuples():\n    data_matrix[line[1]-1, line[2]-1] = line[3]\n    \nprint(\"USER_MOVIE RATING MATRIX\")\nprint(data_matrix)","c9419a07":"from sklearn.metrics.pairwise import pairwise_distances \n\nuser_similarity = pairwise_distances(data_matrix, metric='cosine')\nmovie_similarity = pairwise_distances(data_matrix.T, metric='cosine')\n\nprint(\"USER-USER SIMILARITY MATRIX\")\nprint(user_similarity)\nprint('\\n')\nprint(\"MOVIE-MOVIE SIMILARITY MATRIX\")\nprint(movie_similarity)","9767c4a2":"def predict(ratings, similarity, typ='user'):\n    if typ == 'user':\n        mean_user_rating = ratings.mean(axis=1)\n        ratings_diff = (ratings - mean_user_rating[:, np.newaxis])\n        pred = mean_user_rating[:, np.newaxis] + similarity.dot(ratings_diff) \/ np.array([np.abs(similarity).sum(axis=1)]).T\n    elif typ == 'movie':\n        pred = ratings.dot(similarity) \/ np.array([np.abs(similarity).sum(axis=1)])\n    return pred\n\nuser_user_prediction = predict(data_matrix, user_similarity, typ='user')\nmovie_movie_prediction = predict(data_matrix, movie_similarity, typ='movie')\n\nprint('USER-USER PREDICTION MATRIX')\nprint(user_user_prediction)\nprint()\nprint('MOVIE-MOVIE PREDICTION MATRIX')\nprint(movie_movie_prediction)","d4646caa":"from surprise import Reader, Dataset, KNNBasic, SVD, NMF, NormalPredictor, BaselineOnly, KNNWithMeans, KNNWithZScore, KNNBaseline, SVDpp, SlopeOne, CoClustering\nfrom surprise.model_selection import GridSearchCV, cross_validate\n\nreader = Reader(rating_scale=(0.0, 5.0))\ndata = Dataset.load_from_df( ratings[['user_id', 'movie_id', 'rating']], reader = reader )\nsim_options = {'name': 'msd',\n               'user_based': False  # compute  similarities between items\n               }\nalgo = KNNBasic(k=30,sim_options=sim_options)\ncross_validate(algo=algo, data=data, measures=['RMSE'], cv=5, verbose=True)","ac326248":"n_neighbours = [10, 20, 30]\nparam_grid = {'n_neighbours' : n_neighbours,'k':n_neighbours,'user_based':[True,False]}\n\ngs = GridSearchCV(KNNBasic, measures=['RMSE'], param_grid=param_grid)\ngs.fit(data)\n\nprint('\\n\\n\\n')\n# Best RMSE score\nprint('Best Score :', gs.best_score['rmse'])\n\n# Combination of parameters that gave the best RMSE score\nprint('Best Parameters :', gs.best_params['rmse'])","106bec24":"algo = SVD()\ncross_validate(algo=algo, data=data, measures=['RMSE'], cv=5, verbose=True)","60fd99d2":"param_grid = {'n_factors' : [50, 60, 70], 'lr_all' : [0.5, 0.05, 0.01], 'reg_all' : [0.06, 0.04, 0.02]}\n\ngs = GridSearchCV(algo_class=SVD, measures=['RMSE'], param_grid=param_grid)\ngs.fit(data)\n\nprint('\\n\\n\\n')\n# Best RMSE score\nprint('Best Score :', gs.best_score['rmse'])\n\n# Combination of parameters that gave the best RMSE score\nprint('Best Parameters :', gs.best_params['rmse'])","9862eedf":"cross_validate(algo=NMF(), data=data, measures=['RMSE'], cv=5, verbose=True)","875418d3":"param_grid = {'n_factors' : [50, 60, 70],'verbose':[True]}\n\ngs = GridSearchCV(algo_class=NMF, measures=['RMSE'], param_grid=param_grid)\ngs.fit(data)\n\nprint('\\n\\n\\n')\n# Best RMSE score\nprint('Best Score :', gs.best_score['rmse'])\n\n# Combination of parameters that gave the best RMSE score\nprint('Best Parameters :', gs.best_params['rmse'])","38cfcf06":"**SVD**","bf15bc8f":"**NNMF**","8c80ad02":"**MEMORY BASED APPROACH**\n\nMemory based approach directly works with values of recorded interactions, assuming no model, and are essentially based on \nnearest neighbour search. This approach has low bias but high variance. \n\n**Types of memory based approach**\n* **User-user** : These roughly tries to identify users with the most similar \u201cinteractions profile\u201d (nearest neighbours) in order to suggest items that are the most popular among these neighbours (and that are \u201cnew\u201d to our user). This method is said to be \u201cuser-centred\u201d as it represents users based on their interactions with items and evaluate distances between users.\n* **Item-item** : These finds items similar to the ones the user already \u201cpositively\u201d interacted with. Two items are considered to be similar if most of the users that have interacted with both of them did it in a similar way. This method is said to be \u201citem-centred\u201d as it represent items based on interactions users had with them and evaluate distances between those items.\n\n\nImplementation of memory based approach requires the calculation of similarity matrix.\nFor calculation of similarity between items, cosine similarity is being used. \n","63148890":"**RECOMMENDATION SYSTEM**\n\n(This notebook displays the working of a basic movie recommendation system, using various popular algorithms)\n\nThe Purpose of recommendation system is to suggest relevant items to the users. \n\nHere, we have taken the example of a movie recommendation system, which suggests the most relevant movies\nto each user on the basis of users past interactions with other movies. \n\n![Image](https:\/\/miro.medium.com\/max\/1132\/1*N0-ikjPv4RUVvS-6KCgLPg.jpeg)\n","4cf3e99f":"**USER-MOVIE RATING MATRIX**\n\nNext task is to plot a User-Movie Rating Matrix. ","581748a9":"**EXPLORATORY DATA ANALYSIS **\n\nThe first task is to analyze the data set which is going to be used to train our model. In this model, the dataset we have used\nis MovieLens-100K Dataset, which consists of more than 100,000 ratings given by nearly 1,000 users.\n\nWe start by importing the data from the dataset, and cleaning the data.","8279a344":"**To get the most highly rated movies **","307b6728":"**PREDICTION**\n\nAfter getting the similarity matrix, next step is to predict the most relevant movie to suggest for each user.\nFor this, we write a prediction fucntion. ","ef3064f4":"**MODEL BASED APPROACH**\n\nThis method assumes an underlying \u201cgenerative\u201d model that explains the user-item interactions and try to discover it in order to make new predictions. This approach has high bias but low variance.\n"}}