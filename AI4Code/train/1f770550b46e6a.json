{"cell_type":{"2c40b53d":"code","f7910802":"code","021174f1":"code","ede09c62":"code","5f79b07b":"code","9e4e1f9d":"code","37353e3c":"code","0ffe85a0":"code","353cd5be":"code","b685316a":"code","8d31177a":"code","01a41f0f":"code","b796ece0":"code","679a3b11":"markdown","95b2b136":"markdown","62c6b93d":"markdown","a01729e4":"markdown","137adc11":"markdown","ca4a8a7b":"markdown","87c6a3b6":"markdown","646172b2":"markdown","54107e77":"markdown","7f3e225f":"markdown","2bd56887":"markdown"},"source":{"2c40b53d":"#Generic Libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\n#SK Learn Libraries\nimport sklearn\nfrom sklearn.multiclass import OneVsOneClassifier, OneVsRestClassifier   #1vs1 & 1vsRest Classifiers\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\n\nimport gc","f7910802":"#Load Data\nurl = '..\/input\/iris\/Iris.csv'\ndata = pd.read_csv(url, header='infer')\ndata.drop('Id',axis=1,inplace=True)","021174f1":"#Records\nprint(\"Total Records: \", data.shape[0])","ede09c62":"#Records per Species\ndata.Species.value_counts()","5f79b07b":"#Stat Summary\ndata.describe().transpose()","9e4e1f9d":"#Inspect\ndata.head()","37353e3c":"#Encoding Species columns (to numerical values)\ndata['Species'] = data['Species'].astype('category').cat.codes","0ffe85a0":"#Feature & Target Selection\nfeatures = data.select_dtypes('float').columns\ntarget = ['Species']\n\n# Feature& Target  Dataset\nX = data[features]\ny = data[target]","353cd5be":"#Split Parameters\ntest_size = 0.1\n\n#Dataset Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=0) \n\n#Feature Scaling\n#sc = StandardScaler()\n#X_train = sc.fit_transform(X_train)\n#X_test = sc.transform(X_test)\n\n#Reset Index\nX_test = X_test.reset_index(drop=True)\ny_test = y_test.reset_index(drop=True)","b685316a":"#SVC Model\nmodel = SVC(gamma='scale',random_state=0)\n\n#Define 1-vs-1 Strategy \/ Classifier\novo = OneVsOneClassifier(model)\n\n#fit model to training data\novo.fit(X_train, y_train)\n\n#Predications\novo_pred = ovo.predict(X_test)\n\n#Adding Predictions to Test Dataset\novo_df = X_test.copy()\novo_df.insert(4,\"Actual\",y_test, True)\novo_df.insert(5,\"Predicted\",ovo_pred, True)","8d31177a":"#Inspect Test Dataset\novo_df.head()","01a41f0f":"#Define 1-vs-Rest Strategy \/ Classifier\novr = OneVsRestClassifier(model)\n\n#fit model to training data\novr.fit(X_train, y_train)\n\n#Predications\novr_pred = ovr.predict(X_test)\n\n#Adding Predictions to Test Dataset\novr_df = X_test.copy()\novr_df.insert(4,\"Actual\",y_test, True)\novr_df.insert(5,\"Predicted\",ovr_pred, True)","b796ece0":"#Inspect\novr_df.head()","679a3b11":"## Data Split & Feature Scaling\n\nTrain = 90%, Test = 10%","95b2b136":"## Data","62c6b93d":"## Further Reading\n\n**Wiki:** \n* [Multi Class Classification](https:\/\/en.wikipedia.org\/wiki\/Multiclass_classification)\n\n**API:**\n* [Multiclass and multilabel algorithms](https:\/\/scikit-learn.org\/stable\/modules\/multiclass.html)\n* [One vs One Classifier API](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.multiclass.OneVsOneClassifier.html)\n* [One vs Rest Classifier API](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.multiclass.OneVsRestClassifier.html)\n\n","a01729e4":"## Features & Target ","137adc11":"# 1-vs-1 & 1-vs-Rest Multiclass Classification Strategies (Starter Pack)\n\nThis notebook is dedicated to learning about the two seldomly described multiclass classification strategies available in SK Learn library. Algorithms such as the **Logistic Regression** and **Support Vector Machines** were designed for binary classification, however, they do not support classification tasks with more than two classes.\n\nSo, as part of this starter pack together we shall embark on a quest to learn & understand about the 1-vs-1 & 1-vs-Rest strategies for multiclass classification using the standard [IRIS dataset](https:\/\/www.kaggle.com\/uciml\/iris). Furthermore we'll be using SKLearn Libraries to implement these 2 strategies\n\nAs always, I will try to keep this notebook well commented , informative & organized for easy learning\/understanding. Please do consider it to UPVOTE if you find it helpful.\n","ca4a8a7b":"## One vs Rest \n\n**Some Theory:** Also known as one-vs-all, this strategy consists in fitting one classifier per class. For each classifier, the class is fitted against all the other classes. In addition to its computational efficiency (only n_classes classifiers are needed), one advantage of this approach is its interpretability. Since each class is represented by one and one classifier only, it is possible to gain knowledge about the class by inspecting its corresponding classifier. This is the most commonly used strategy for multiclass classification and is a fair default choice.\n\nThis strategy can also be used for multilabel learning, where a classifier is used to predict multiple labels for instance, by fitting on a 2-d matrix in which cell [i,j] is 1 if sample i has label j and 0 otherwise.\n\nIn the multilabel learning literature, \"ovr\" is also known as the binary relevance method\n\n**Some Explanation:** Under this method\/strategy a multi-class classification dataset (in our case IRIS) is split into multiple binary classification chunks\/datasets. Then a binary classifier (in our case SVC) is trained on each of the binary classification datasets and a prediction(s) is made using the model that has the most confidence value. The IRIS dataset will be split into individual datasets for each Species versus every other Species. Following are the details:\n\n* Binary Classification Dataset#1: Iris-setosa v\/s Iris-versicolor\n* Binary Classification Dataset#2: Iris-setosa v\/s Iris-virginica\n* Binary Classification Dataset#3: Iris-versicolor v\/s Iris-virginica\n\n\nEach of the above binary classification model may predict a class membership probability or a probability-like score. The argmax of these scores (class index with the largest score) is then used to predict a class.\n\n\nIf the binary classification models predict a numerical class membership, such as a probability, then the argmax of the sum of the scores (class with the largest sum score) is predicted as the class label.This approach is suggested for support vector machines (SVM) & other kernel-based algorithms. This is believed because the performance of kernel methods does not scale in proportion to the size of the training dataset and using subsets of the training data may counter this effect.\n\nThe scikit-learn library also provides a separate **OneVsRestClassifier class** that allows the one-vs-rest strategy to be used with any classifier. This class can be used with a binary classifier like SVM or Logistic Regression for multi-class classification, or even other classifiers that natively support multi-class classification.\n\n**Some Practical:** The **OneVsRestClassifier class** is very easy to use and requires that a classifier that is to be used for binary classification be provided to the OneVsRestClassifier as an argument. See the demonstration below:\n","87c6a3b6":"## Conclusion\n\nIn this starter pack \/ tutorial, we learned about One-vs-Rest and One-vs-One strategies for multi-class classification.\n\n**Specifically**:\n\n* The One-vs-Rest strategy splits a multi-class classification into one binary classification problem per class.\n* The One-vs-One strategy splits a multi-class classification into one binary classification problem per each pair of classes.","646172b2":"## Libraries","54107e77":"### UPDATED - 28\/08\/20","7f3e225f":"## One vs One \n\n**Some Theory:** OneVsOneClassifier constructs one classifier per pair of classes. At prediction time, the class which received the most votes is selected. In the event of a tie (among two classes with an equal number of votes), it selects the class with the highest aggregate classification confidence by summing over the pair-wise classification confidence levels computed by the underlying binary classifiers. Since it requires to fit n_classes * (n_classes - 1) \/ 2 classifiers, this method is usually slower than one-vs-the-rest, due to its O(n_classes^2) complexity. The decision function is the result of a monotonic transformation of the one-versus-one classification.\n\n**Some Explanation:** Under this method\/strategy a multi-class classification dataset (in our case IRIS) is split into binary classification chunks\/dataset. The IRIS dataset will be split into individual datasets for each Species versus every other Species. Following are the details:\n\n* Binary Classification Dataset#1: Iris-setosa v\/s Iris-versicolor\n* Binary Classification Dataset#2: Iris-setosa v\/s Iris-virginica\n* Binary Classification Dataset#3: Iris-versicolor v\/s Iris-virginica\n\nThe formula for calculating the number of binary datasets & in turn models is:\n\n#### (NumClasses * (NumClasses \u2013 1)) \/ 2\n\nSo, in our case it will be (3 * (3-1)) \/ 2 = 3\n\nEach of the above binary classification model may predict one class label and the model with the most predictions or votes is predicted by the one-vs-one strategy.\n\nIf the binary classification models predict a numerical class membership, such as a probability, then the argmax of the sum of the scores (class with the largest sum score) is predicted as the class label.This approach is suggested for support vector machines (SVM) & other kernel-based algorithms. This is believed because the performance of kernel methods does not scale in proportion to the size of the training dataset and using subsets of the training data may counter this effect.\n\nThe scikit-learn library provides a separate **OneVsOneClassifier class** that allows the one-vs-one strategy to be used with any classifier. This class can be used with a binary classifier like SVM or Logistic Regression for multi-class classification, or even other classifiers that natively support multi-class classification.\n\n**Some Practical:** The **OneVsOneClassifier class** is very easy to use and requires that a classifier that is to be used for binary classification be provided to the OneVsOneClassifier as an argument. See the demonstration below:\n","2bd56887":"## Basic EDA"}}