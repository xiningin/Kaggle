{"cell_type":{"b8e9c8e5":"code","c029eadf":"code","d0927694":"code","30cc1509":"code","5a2ac6d8":"code","9eba75e5":"code","040e1434":"code","3590e1df":"code","500cd79b":"code","d13ffa4c":"code","b503a5fb":"code","776eb3d2":"code","72aa0fe2":"code","eccc3dde":"code","3ec6392d":"code","0f69fda8":"code","e6e7ea60":"code","3680209f":"code","cdb14be2":"code","b2fbc349":"code","9996f3ee":"code","d19172e7":"code","fb56eba7":"code","647b9ec1":"code","dd819601":"code","24aeeb23":"code","bd894983":"code","7567b389":"code","a3e5cc04":"code","9f4022ae":"code","3a078267":"code","c6fa91f6":"code","07ba6024":"code","3e5b4eb9":"code","fe8df77a":"code","72c3c456":"code","9ea8aaa1":"code","a6f2f1b0":"code","3eaa1036":"code","e2a41448":"code","6d0646e1":"markdown","707362dc":"markdown","ec99bed9":"markdown","24274e06":"markdown","aa09e77d":"markdown","749273eb":"markdown","f1ee3460":"markdown","739263fb":"markdown"},"source":{"b8e9c8e5":"# 1. Regular EDA(Exploratory Data Analysis) and Plotting Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n# To appear plots inside the notebook \n%matplotlib inline \n\n# 2. Models from Scikit-learn \nimport sklearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# 3. Model Evaluations\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, classification_report, f1_score\nfrom sklearn.metrics import plot_roc_curve\n\n# 4. Saving a Model\nimport pickle","c029eadf":"df = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")\ndf.info()","d0927694":"df.head()","30cc1509":"df.describe()","5a2ac6d8":"# Total number of classes in target column:\nplt.style.use('seaborn-whitegrid')# it will work by deafult for charts \ndf['target'].value_counts().plot(kind = 'bar', color = [\"salmon\", \"lightblue\"], figsize = (10,6))\nplt.title(\"Heart Disease Frequency\")\nplt.ylabel('Total Number')\nplt.xlabel(\"Yes                                                 No\")\nplt.xticks(rotation = 0);","9eba75e5":"# Heart disease frequency according to sex\npd.crosstab(df.target, df.sex).plot(kind = 'bar', color = [\"salmon\", 'lightblue'], figsize = (10,6))\nplt.title(\"Heart Disease Frequency Acoording To Sex\")\nplt.xlabel(\"0 = No, 1 = Yes\")\nplt.ylabel(\"Total Numbber\")\nplt.legend(['Female', 'Male'])\nplt.xticks(rotation = 0);","040e1434":"# Age vs Max Heart Rate for Heart Disease\nplt.figure(figsize=(10,6))\n\n# Scatter plot with heart disease = 1 values:\nplt.scatter(df.age[df.target == 1],\n                   df.thalach[df.target == 1],\n                          c = \"salmon\")\n# Scatter with with heart disease = 0 values:\nplt.scatter(df.age[df.target == 0],\n          df.thalach[df.target == 0],\n          c = 'lightblue')\n\n# Adding Information\nplt.title(\"Heart Disease as a Function of Max Heart Rate and Age\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Max Heart Rate\")\nplt.legend([\"Heart Disease : Yes\", \"Heart Disease : No\"]);","3590e1df":"# Age Column Distribution \ndf['age'].hist(figsize = (10,6));","500cd79b":"# Heart Disease Frequency Per Chest Pain Type :\npd.crosstab(df.cp,df.target).plot(kind = 'bar', color = (\"salmon\", 'lightblue'), figsize = (10,6))\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Total Number\")\nplt.legend([\"Heart Disease : No\", \"Heart Disease  : Yes\"])\nplt.xticks(rotation = 0);","d13ffa4c":"# Correlation Matrix Using Seaborn Heatmap\ncorr_maxtix = df.corr()\nfig, ax = plt.subplots(figsize = (15,10))\nax = sns.heatmap(corr_maxtix,\n                annot = True,\n                linewidths=0.5,\n                fmt= \".2f\",\n                cmap = \"YlGnBu\") # Yellow, Green, Blue","b503a5fb":"# Random Seed\nnp.random.seed(42)\n\n# Splitting data into X and y\nX = df.drop(\"target\", axis = 1)\ny = df[\"target\"]\n\n# Spliting data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.2)","776eb3d2":"# Putting Models in Dictionary :\nmodels = {\"Logestic Regression\" : LogisticRegression(),\n         \"KNN\" : KNeighborsClassifier(),\n         \"Random Forest Classifier\" : RandomForestClassifier()}\n\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fit and Evaluate Machine Learning Models with our data\n    \"\"\"\n    np.random.seed(42)\n    model_scores = {}  # a dictionary to keep the model score \n    for name, model in models.items():\n        model.fit(X_train, y_train) # fitting data to model\n        model_scores[name] = model.score(X_test, y_test) # evaluating model\n    return model_scores","72aa0fe2":"baseline_model_score = fit_and_score(models = models,\n                                    X_train = X_train,\n                                    X_test = X_test,\n                                    y_train = y_train,\n                                    y_test = y_test)\nbaseline_model_score","eccc3dde":"# Visulaization for model comparsion :\nmodel_comparison = pd.DataFrame(baseline_model_score, index = [\"accuracy\"])\nmodel_comparison.T.plot(kind = 'barh'); # T = transpose","3ec6392d":"# 1. Hyperperameter Tuning (KNN)\ntrain_scores = []\ntest_scores = []\n\n#List of values for neighbors\nneighbors = range(1,21)\n\n# KNN Instance setup\nknn = KNeighborsClassifier()\n\n# Lopping through the range of neigbhors \nfor i in neighbors:\n    knn.set_params(n_neighbors = i)\n    knn.fit(X_train, y_train) # fitting the traning data set \n    train_scores.append(knn.score(X_train, y_train))\n    test_scores.append(knn.score(X_test, y_test))\n\ntrain_scores, test_scores\n\n# Plotting On Graph\nplt.figure(figsize=(10,6))\nplt.plot(neighbors, train_scores, label = \"Train Score\")\nplt.plot(neighbors, test_scores, label = \"Test Score\")\nplt.title(\"KNN Score\")\nplt.xlabel(\"Number of Neighbors\")\nplt.ylabel(\"Model Score\")\nplt.legend();\nprint(f\"Maximum KNN Score on the test data : {max(test_scores) * 100:.2f} %\")\n\n# we will be not pursuing this model further as after tuning its accuracy is stll below Logistic Regression","0f69fda8":"# Creating a grid for hyperperameter tuning:\n\n# Creating Hyperperameter grid for Logestic Regression:\nlog_reg_grid = {'C' : np.logspace(-4,4,20),\n               \"solver\" : [\"liblinear\"] }\n\n# Creating Hyperperameter grid for Random Forest Regression:\nrf_grid = {\"n_estimators\" : np.arange(10,1000,50),\n          \"max_depth\" : [None, 3,  5, 10],\n          \"min_samples_split\" : np.arange(2,20,2),\n          \"min_samples_leaf\" : np.arange(1,22,2)}","e6e7ea60":"# 1. Hyperperameter Tuning Using RandomizedSearchCV - Logistic Regression\nnp.random.seed(42)\n\nrs_log_reg = RandomizedSearchCV(LogisticRegression(),\n                               param_distributions=log_reg_grid,\n                               cv = 5,\n                               n_iter= 20,\n                               verbose= True)\nrs_log_reg.fit(X_train, y_train)\nrs_log_reg.score(X_test, y_test)","3680209f":"# getting the best prams\nrs_log_reg.best_params_","cdb14be2":"# Score of our model, simillar as baseline score \nrs_log_reg.score(X_test, y_test)","b2fbc349":"# 1. Hyperperameter Tuning Using RandomizedSearchCV - RandomForestClassifier\n\nnp.random.seed(42)\n\nrs_rf = RandomizedSearchCV(RandomForestClassifier(),\n                          param_distributions= rf_grid,\n                          cv = 5,\n                          n_iter=20,\n                          verbose= True)\nrs_rf.fit(X_train, y_train)","9996f3ee":"# best params for model \nrs_rf.best_params_","d19172e7":"# Score \nrs_rf.score(X_test, y_test)","fb56eba7":"# As Logistic Regresssion Performed Best, we will move foward with it and improve it:\n# 1. By using GridSearchCV\nnp.random.seed(42)\n\nrs_log_grid = {\"C\" : np.logspace(-4,4,30),\n              \"solver\" : [\"liblinear\"]}\n\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid= rs_log_grid,\n                          cv = 5,\n                          verbose = True)\n\ngs_log_reg.fit(X_train, y_train)","647b9ec1":"gs_log_reg.best_params_","dd819601":"gs_log_reg.score(X_test, y_test)","24aeeb23":"# Making Predictions  ( Always Evaluate on test data sets )\ny_preds = gs_log_reg.predict(X_test)","bd894983":"# ROC and AUC \n\nplot_roc_curve(gs_log_reg, X_test, y_test);","7567b389":"# Confusion Matrix \nsns.set(font_scale = 1.5)\n\ndef plot_conf_matrix(y_test, y_preds):\n    \"\"\"\n    Plotting Confusion Matrix Using Seaborn's Heatmap\n    \"\"\"\n    fig, ax = plt.subplots(figsize = (4,4))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                    annot= True,\n                    cbar = False)\n    plt.xlabel(\"Predicted Label\")\n    plt.ylabel(\"True Label\")\n    plt.title(\"Confusion Matrix - Logistic Regression\")\n\nplot_conf_matrix(y_test,y_preds); # true lables and predicted lables ","a3e5cc04":"# Classification report on basis of one split that we have created above\nlog_reg_class_report = classification_report(y_test, y_preds)\nprint(log_reg_class_report)","9f4022ae":"# Classification report on basis of Cross Validation\n# We will be using our best params for the same\ngs_log_reg.best_params_","3a078267":"# Creating a new classifier with best params \nclf = LogisticRegression(C = 0.20433597178569418,\n                        solver= 'liblinear')","c6fa91f6":"# Cross Val Accuracy \ncv_acc = cross_val_score(clf,\n                        X, y,\n                        cv = 5,\n                        scoring= \"accuracy\")\ncv_acc_mean = cv_acc.mean()\ncv_acc_mean","07ba6024":"# Cross Val Precision\ncv_prec = cross_val_score(clf, \n                          X,y,\n                          cv =5,\n                          scoring=\"precision\")\ncv_prec_mean = cv_prec.mean()\ncv_prec_mean","3e5b4eb9":"# Cross Val Recall\ncv_recall = cross_val_score(clf,\n                           X, y,\n                           cv =5,\n                           scoring=\"recall\")\ncv_recall_mean = cv_recall.mean()\ncv_recall_mean","fe8df77a":"# Cross Val F1 Score\ncv_f1 = cross_val_score(clf,\n                       X, y,\n                       cv =5,\n                       scoring=\"f1\")\ncv_f1_mean = cv_f1.mean()\ncv_f1_mean","72c3c456":"# Visualzation of Cross Val Score \ncv_metrics = pd.DataFrame({\"Accuracy\" : cv_acc_mean,\n                          \"Precision\" : cv_prec_mean,\n                          \"Recall\" : cv_recall_mean,\n                          \"F1\" : cv_f1_mean},\n                         index = [0])\ncv_metrics.T.plot.barh(title = \"Cross Validated Classification Metrics\", legend = False);","9ea8aaa1":"# Fit an instance of Logistic Regression \nclf = LogisticRegression(C = 0.20433597178569418,\n                        solver= 'liblinear')\nclf.fit(X_train, y_train);","a6f2f1b0":"# Check Coef - Coefficient \nclf.coef_","3eaa1036":"# Matching Coefficent of features to columns\nfeature_dict = dict(zip(df.columns, list(clf.coef_[0])))\nfeature_dict","e2a41448":"# Visualization of feature importance\nfeatures_df = pd.DataFrame(feature_dict,index = [0])\nfeatures_df.T.plot.bar(title = 'Feature Importance - Logistic Regression', legend = False)\nplt.xlim(-2,15)\nplt.ylim(-1, 1);","6d0646e1":"#### Lets look at the following :\n    1. Hyperperameter Tuning \n    2. Feature Importance\n    3. Confusion Matrix\n    4. Cross Validation \n    5. Precision\n    6. Recall\n    7. F1 Score\n    8. Classification Report\n    9. Recevier Operating Characterstic Curve (ROC)\n    10. Area Under the Curve (AUC)","707362dc":"#### Model Evaluation : \n- ROC Curve and AUC score\n- Confusion Matrix\n- Classification Report\n- Precission \n- Recall\n- F1 Score\n- Use Cross Validation wher ever possisble","ec99bed9":"#### Splitting Data :","24274e06":"Predicting heart disease using machine learning model :\nBy using the given data and attributes we will be implementing machine learning model.\nMain objective of the model to predcit if a person have heart disease or not. disease during the proof of concept, we will puruse the project.\nWe will try three different models:\nLogistic Regression\nK- Nearest Neighbours Classifier\nRandom Forest Classifier\nThe original data came from the Cleavland data from from UCI Machine Learning Repository.\nThere is also a version of the data available on Kaggle.\nData attribute information :\n1. age - age in years\n2. sex - (1 = male; 0 = female)\n3. cp - chest pain type\n    0: Typical angina: chest pain related decrease blood supply to the heart\n    1: Atypical angina: chest pain not related to heart\n    2: Non-anginal pain: typically esophageal spasms (non heart related)\n    3: Asymptomatic: chest pain not showing signs of disease\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital) anything above 130-140 is  typically cause for concern\n5. chol - serum cholestoral in mg\/dl\n    * serum = LDL + HDL + .2 * triglycerides\n    * above 200 is cause for concern\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n    * '>126' mg\/dL signals diabetes\n7. restecg - resting electrocardiographic results\n    0: Nothing to note\n    1: ST-T Wave abnormality\n    can range from mild symptoms to severe problems\n    signals non-normal heart beat\n    2: Possible or definite left ventricular hypertrophy\n    Enlarged heart's main pumping chamber\n8. thalach - maximum heart rate achieved\n9. exang - exercise induced angina (1 = yes; 0 = no)\n10. oldpeak - ST depression induced by exercise relative to rest looks at stress of heart during excercise unhealthy heart will stress more\n11. slope - the slope of the peak exercise ST segment\n    0: Upsloping: better heart rate with excercise (uncommon)\n    1: Flatsloping: minimal change (typical healthy heart)\n    2: Downslopins: signs of unhealthy heart\n12. ca - number of major vessels (0-3) colored by flourosopy\n    colored vessel means the doctor can see the blood passing through the more blood movement the better (no clots)\n13. thal - thalium stress result\n    1,3: normal\n    6: fixed defect: used to be defect but ok now\n    7: reversable defect: no proper blood movement when excercising\n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)","aa09e77d":"#### Feature Importance :\n- Which all features contributed most towards the model\n- How did they contribute in predicting the target ?","749273eb":"#### Data Exploration (Exploratory Data Analysis(EDA)) :","f1ee3460":"#### Model Comparison :","739263fb":"#### Finding Patterns In Data :"}}