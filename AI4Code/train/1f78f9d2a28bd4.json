{"cell_type":{"0ad955ab":"code","b63372c1":"code","730142ae":"code","0e528de6":"code","286c0abc":"code","03b3e401":"code","3b0482f0":"code","5b225654":"code","33588fae":"code","4eb0706e":"code","33715a70":"markdown","d97668cb":"markdown","33937075":"markdown","994284d6":"markdown","3f6fdeff":"markdown"},"source":{"0ad955ab":"import tensorflow.compat.v1 as tf\nimport numpy as np\nimport warnings\nimport matplotlib.pyplot as plt\n%matplotlib inline","b63372c1":"tf.disable_eager_execution()\nwarnings.filterwarnings('ignore')","730142ae":"#run time step\ntime_step = 10\n#input size\ninput_size = 1\n#run cell size\ncell_size = 32\n# learning rate\nlearning_rate = 0.02","0e528de6":"steps = np.linspace(0, np.pi*2, 100, dtype = np.float32)\n# the data type is float32 beacuse of converting numpy value to float Tensor\n\nx_np = np.sin(steps)\ny_np = np.cos(steps)","286c0abc":"plt.plot(steps, y_np, 'r-', label = 'Target (Cos)')\nplt.plot(steps, x_np, 'b-', label = 'Input (Sin)')\nplt.tight_layout()\nplt.legend(loc = 'best')\nplt.show()","03b3e401":"tf_x = tf.placeholder(tf.float32, [None, time_step, input_size])     # shape (batch, 5, 1)\ntf_y = tf.placeholder(tf.float32, [None, time_step, input_size])     # input y","3b0482f0":"rnn_cell = tf.nn.rnn_cell.BasicRNNCell(num_units = cell_size)\ninit_s = rnn_cell.zero_state(batch_size = 1, dtype = tf.float32)\n\noutputs, final_s = tf.nn.dynamic_rnn(rnn_cell,\n                                    tf_x,                      # Input\n                                    initial_state= init_s,      # The initial hidden Layers\n                                    time_major= False)","5b225654":"outs_2D= tf.reshape(outputs, [-1, cell_size])\nnet_outs2D = tf.layers.dense(outs_2D, input_size)\nouts = tf.reshape(net_outs2D, [-1, time_step, input_size])\n\nloss = tf.losses.mean_squared_error(labels = tf_y, predictions=outs)\ntrain_optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)","33588fae":"sess = tf.Session()\nsess.run(tf.global_variables_initializer())","4eb0706e":"plt.figure(1, figsize=(10, 8))\nplt.ion()\n\nfor step in range(10):\n    start = step * np.pi\n    end = (step + 1) * np.pi\n    \n    steps = np.linspace(start, end, time_step)\n    \n    x = np.sin(steps)[np.newaxis, :, np.newaxis]\n    y = np.cos(steps)[np.newaxis, :, np.newaxis]\n    \n    if 'final_s_' not in globals():\n        feed_dict = {tf_x:x, tf_y:y}\n    else:\n        feed_dict = {tf_x:x, tf_y:y, init_s:final_s_}\n        \n    _, pred_, final_s_ = sess.run([train_optimizer, outs, final_s], feed_dict)\n    \n    \n    plt.plot(steps, y.flatten(), 'r-')\n    plt.plot(steps, pred_.flatten(), 'b-')\n    plt.ylim((-1.2, 1.2))\n    plt.draw()\n    plt.pause(0.05)\n    \n    \nplt.ioff()\nplt.tight_layout()\nplt.show()","33715a70":"# <b style=\"color:red\"> Create Fake Datasets to illustrate the Recurrent Neural Network for Regression <\/b>","d97668cb":"# <b style=\"color:blue\"> Create Recurrent Neural Network (RNN) <\/b>\n\n<b style=\"color:red\">Before dive into code lets know something about RNN.<\/b>\n\nA recurrent neural network (RNN) is a type of artificial neural network commonly used in speech recognition and natural language processing (NLP). RNNs are designed to recognize a data's sequential characteristics and use patterns to predict the next likely scenario.\n\nRNNs are used in deep learning and in the development of models that simulate the activity of neurons in the human brain. They are especially powerful in use cases in which context is critical to predicting an outcome and are distinct from other types of artificial neural networks because they use feedback loops to process a sequence of data that informs the final output, which can also be a sequence of data . These feedback loops allow information to persist; the effect is often described as memory.\n\n# <b style=\"color:'blue\">How recurrent neural networks learn <\/b>\nArtificial neural networks are created with interconnected data processing components that are loosely designed to function like the human brain. They are composed of layers of artificial neurons (network nodes) that  have the capability to process input and forward output to other nodes in the network. The nodes are connected by edges or weights that influence a signal's strength and the network's ultimate output.\n\n<img src=\"https:\/\/qph.fs.quoracdn.net\/main-qimg-5148d598f36b016d039e220bd3cab745.webp\" alt =\" RNN\" >\n\nIn some cases, artificial neural networks process information in a single direction from input to output. These \"feedforward\" neural networks include convolutional neural networks that underpin image recognition systems . RNNs, on the other hand, can be layered to process information in two directions.\n\nLike feedforward neural networks, RNNs can process data from initial input to final output. Unlike feedforward neural networks, RNNs use feedback loops such as Backpropagation Through Time or BPTT throughout the computational process to loop information back into the network. This connects inputs together and is what enables RNNs to process sequential and temporal data.\n\n","33937075":"# <b style=\"color:blue\">Create Hyper-paremeter<\/b>\nA hyperparameter is a parameter whose value is set before the learning process begins. By contrast, the values of other parameters are derived via training the data. For example, Neural Networks has many hyperparameters, including: number of hidden layers.","994284d6":"# <b style=\"color:blue\"> Create Tensorflow Placeholders <\/b>","3f6fdeff":"<b style=\"color:blue\"> Plot the values <\/b>"}}