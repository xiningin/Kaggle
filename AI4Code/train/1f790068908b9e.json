{"cell_type":{"ba016b6a":"code","31e516c3":"code","8d2b474c":"code","75031ae1":"code","01470a41":"code","e6d26ed2":"code","acb74c66":"code","f82318a1":"code","758f5275":"code","8a520c5d":"code","6c147af1":"code","94f51655":"code","d0245be3":"code","c40e87b2":"code","77bdeeaa":"code","b1acb60c":"code","531bdc2e":"code","22567f9e":"code","3780d1b6":"code","c7842628":"code","70fd3d25":"code","4b7ffa3f":"code","39e97aa8":"markdown","fd5be41a":"markdown","31c70437":"markdown","e21d29e8":"markdown","c97c483a":"markdown","8939a8fb":"markdown","2b02c989":"markdown","3d7b7cb1":"markdown","108f6f12":"markdown","c06aa2e3":"markdown","23ddb36d":"markdown","71072929":"markdown","4bbbc4d5":"markdown","90ba0ad9":"markdown"},"source":{"ba016b6a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom scipy import interpolate\nfrom tqdm.auto import tqdm","31e516c3":"data = np.load('..\/input\/ventexp55\/exp055_oof.npz')\npreds = data['preds']\nmasks = data['masks']\npressures = data['pressures']\npressures.shape","8d2b474c":"train = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ntrain_uins = train.u_in.values.reshape(-1,80)[:,:33]\ntrain_ts = train.time_step.values.reshape(-1,80)[:,:33]\ntrain_pressures = train.pressure.values.reshape(-1,80)[:,:33]","75031ae1":"mae = np.abs(masks * (pressures - preds))\nmae = mae.sum(axis=-1) \/ masks.sum(axis=-1)\nmae_sort = np.argsort(mae)[::-1]\nmae","01470a41":"plt.hist(mae, bins=100); plt.show()\nmae[mae_sort[0]]","e6d26ed2":"# Our goal:\nmae.mean(), mae[mae<=1.1].mean()","acb74c66":"# We only need to try and fix the worst 675 predictions to have this\n# Much of an affect on MAE!\n(mae>1.1).sum()","f82318a1":"# To make time easier, we can descritize time into 0.03144001960754393 buckets...\nmean_ts_step = np.diff(train_ts, axis=-1).mean()\n\n# Start by interpolating u_in using cublic interpolation...\nresampled_ts = np.arange(0, train_ts[masks].max(), mean_ts_step \/ 3)\n\nresampled_uins = np.zeros((train_uins.shape[0], resampled_ts.shape[0]))\nresampled_pressures = np.zeros((train_uins.shape[0], resampled_ts.shape[0]))\nresampled_preds = np.zeros((train_uins.shape[0], resampled_ts.shape[0]))\n\nfor idx_sample in tqdm(range(train_uins.shape[0])):\n    # uin is the input\n    # pressure is the target we hope to clip our predictions to\n    interp_uin      = interpolate.Akima1DInterpolator(train_ts[idx_sample], train_uins[idx_sample])\n    interp_pressure = interpolate.Akima1DInterpolator(train_ts[idx_sample], train_pressures[idx_sample])\n    interp_preds    = interpolate.Akima1DInterpolator(train_ts[idx_sample], preds[idx_sample])\n    \n    # NOTE: We need to terminate @ last uout=0 because if we dont, it drops like crazy\n    # and the coslerp gets jacked up. Alternatively, we can make judgement call to only match\n    # first \"n\" ts predictions, e.g. up to 0.8227286338806152, and only apply our post\n    # processing algorithm to that.\n    resampled_uins[idx_sample] = interp_uin(resampled_ts)\n    resampled_pressures[idx_sample] = interp_pressure(resampled_ts)\n    resampled_preds[idx_sample] = interp_preds(resampled_ts)","758f5275":"# Always check, my friends..\nresampled_uins.shape","8a520c5d":"for i in mae_sort[:15]:\n    plt.figure(figsize=(15,5))\n    \n    mae = np.abs(masks[i] * (pressures[i] - preds[i]))\n    mae = mae.sum() \/ masks[i].sum()\n\n    plt.title(f'{mae:.5f}  MAE')\n    plt.plot(resampled_ts, resampled_uins[i], linewidth=1, label='uin')\n    plt.scatter(train_ts[i], train_uins[i], linewidth=1)\n    \n    plt.plot(resampled_ts, resampled_pressures[i], linewidth=1, label='pressure')\n    \n    plt.plot(resampled_ts, resampled_preds[i], linewidth=1, label='preds', linestyle='--')\n    plt.scatter(train_ts[i], pressures[i], linewidth=1)\n    \n    plt.plot()\n    plt.legend()\n    plt.show()","6c147af1":"unique_R = train.R.unique()\nunique_C = train.C.unique()\nRCs = train[['R','C']].values.reshape(-1, 80, 2)[:,0::80].reshape(-1,2)\nRCs","94f51655":"RCs.shape","d0245be3":"# Thanks @cdelotte...\n\nfrom cuml.neighbors import NearestNeighbors \n\nNEIGHBORS = 2  # 2 neighbors so we dont select ourselves\nSIZE = 74      # 74 is the earliest an exhale occurs..\nRCModels = {}\nRCIndices = {}\nRCFilters = {}\n\n# Only select candidates within our R\/C group\nfor R in unique_R:\n    for C in unique_C:\n        rcfilter = np.all(RCs == np.array([R,C]), axis=1)\n        real_indices = np.nonzero(rcfilter)[0]\n        \n        model = NearestNeighbors(n_neighbors=NEIGHBORS, metric='l1')\n        model.fit(resampled_uins[rcfilter, :SIZE])\n        RCModels[(R,C)] = model\n        RCIndices[(R,C)] = real_indices # so we can find them...\n        RCFilters[(R,C)] = rcfilter","c40e87b2":"SHOW = True\n\nfor i in mae_sort[:100]:\n    if SHOW:\n        plt.figure(figsize=(15,5))\n    \n    mae = np.abs(masks[i] * (pressures[i] - preds[i]))\n    mae = mae.sum() \/ masks[i].sum()\n\n    rc_vals = tuple(RCs[i])\n    haystack = resampled_uins[RCFilters[rc_vals], :SIZE]\n    distances, indices = RCModels[rc_vals].kneighbors(resampled_uins[i,:SIZE].reshape(1,-1))\n    # NOTE: If we're grabbing a set, we can use [:,1] instead\n    k_dist = np.round(distances[0,1]).astype(np.int) # dont select self for train set\n    k_idx = indices[0,1] # dont select self for train set.\n    real_idx = RCIndices[rc_vals][k_idx]\n    \n    # We have to asmple (interpolate) from kneighbor -> our timestamp space\n    # To compute MAE\n    # TODO:\n    interp_k_pressures_proj = interpolate.Akima1DInterpolator(train_ts[real_idx], train_pressures[real_idx])\n    k_pressures_proj = interp_k_pressures_proj(train_ts[i])\n    # In case we go out of bounds, fill with the original data\n    k_pressures_proj[np.isnan(k_pressures_proj)] = preds[i, np.isnan(k_pressures_proj)]\n    k_mae = np.abs(masks[i] * (pressures[i] - k_pressures_proj))\n    k_mae = k_mae.sum() \/ masks[i].sum()\n    \n    if SHOW:\n        real_dist = np.abs(resampled_uins[i] - resampled_uins[real_idx]).sum() \/ SIZE\n        plt.title(f'{mae:.5f} MAE.  K-Dist: {k_dist},  K-MAE: {k_mae:.5f}.   RealDist {real_dist}')\n        plt.plot(resampled_ts, resampled_uins[i], linewidth=1, label='uin')\n        plt.plot(resampled_ts, resampled_uins[real_idx], linewidth=1, label='k-uin', c='lightblue')\n        plt.scatter(train_ts[i], train_uins[i], linewidth=1)\n\n        plt.plot(resampled_ts, resampled_pressures[i], linewidth=1, label='pressure')\n        plt.plot(resampled_ts, resampled_pressures[real_idx], linewidth=1, label='k-pressure', linestyle='-')\n\n        plt.plot(resampled_ts, resampled_preds[i], linewidth=1, label='preds', linestyle='--')\n        plt.scatter(train_ts[i], pressures[i], linewidth=1)\n\n        plt.plot()\n        plt.legend()\n        plt.show()","77bdeeaa":"SHOW = False\n\ncols = ['orig_mae', 'k_mae', 'dist', 'R', 'C']\nresults = []\nfor i in tqdm(mae_sort[:675]):\n    if SHOW:\n        plt.figure(figsize=(15,5))\n    \n    mae = np.abs(masks[i] * (pressures[i] - preds[i]))\n    mae = mae.sum() \/ masks[i].sum()\n\n    rc_vals = tuple(RCs[i])\n    haystack = resampled_uins[RCFilters[rc_vals], :SIZE]\n    distances, indices = RCModels[rc_vals].kneighbors(resampled_uins[i,:SIZE].reshape(1,-1))\n    # NOTE: If we're grabbing a set, we can use [:,1] instead\n    k_dist = distances[0,1] \/ SIZE  # dont select self for train set\n    k_idx = indices[0,1] # dont select self for train set.\n    real_idx = RCIndices[rc_vals][k_idx]\n    \n    # We have to asmple (interpolate) from kneighbor -> our timestamp space\n    # To compute MAE\n    # TODO:\n    interp_k_pressures_proj = interpolate.Akima1DInterpolator(train_ts[real_idx], train_pressures[real_idx])\n    k_pressures_proj = interp_k_pressures_proj(train_ts[i])\n    # In case we go out of bounds, fill with the original data\n    k_pressures_proj[np.isnan(k_pressures_proj)] = preds[i, np.isnan(k_pressures_proj)]\n    k_mae = np.abs(masks[i] * (pressures[i] - k_pressures_proj))\n    k_mae = k_mae.sum() \/ masks[i].sum()\n    \n    results.append([\n        mae,\n        k_mae,\n        k_dist,\n        rc_vals[0],\n        rc_vals[1]\n    ])\n    if SHOW:\n        plt.title(f'{mae:.5f} MAE.  K-Dist: {k_dist},  K-MAE: {k_mae:.5f}')\n        plt.plot(resampled_ts, resampled_uins[i], linewidth=1, label='uin')\n        plt.plot(resampled_ts, resampled_uins[real_idx], linewidth=1, label='k-uin', c='lightblue')\n        plt.scatter(train_ts[i], train_uins[i], linewidth=1)\n\n        plt.plot(resampled_ts, resampled_pressures[i], linewidth=1, label='pressure')\n        plt.plot(resampled_ts, resampled_pressures[real_idx], linewidth=1, label='k-pressure', linestyle='-')\n\n        plt.plot(resampled_ts, resampled_preds[i], linewidth=1, label='preds', linestyle='--')\n        plt.scatter(train_ts[i], pressures[i], linewidth=1)\n\n        plt.plot()\n        plt.legend()\n        plt.show()","b1acb60c":"rez = pd.DataFrame(results, columns=cols)\nrez.to_csv('knn_stuff.csv', index=False)\nrez","531bdc2e":"plt.hist(rez.dist, bins=100)\nplt.show()","22567f9e":"for R in train.R.unique():\n    for C in train.C.unique():\n        ds = []\n        vs_old, vs_new = [], []\n        for dist in np.arange(0.1,3, 0.02):\n            look = rez[(rez.dist<dist)&(rez.R==R)&(rez.C==C)]\n            vs_old.append(look.orig_mae.mean())\n            vs_new.append(look.k_mae.mean())\n            ds.append(dist)\n        plt.figure(figsize=(15,4))\n        plt.title(f'k=1 NN MAE for Group: {R}, {C}')\n        plt.plot(ds, vs_old, label='old')\n        plt.plot(ds, vs_new, label='new')\n        plt.legend()\n        plt.show()","3780d1b6":"# Most of our jacked up values occur in R=50\n# Unfortunately, that's also where our algo post-processing fails the hardest...\nrez.R.value_counts()","c7842628":"plt.plot(rez.orig_mae); plt.show()","70fd3d25":"for R in train.R.unique():\n    ds = []\n    vs_old, vs_new = [], []\n    for dist in np.arange(0.08,1, 0.01):\n        look = rez[(rez.dist<dist)&(rez.R==R)&(rez.orig_mae>0.8904941219161745)]\n        vs_old.append(look.orig_mae.mean())\n        vs_new.append(look.k_mae.mean())\n        ds.append(dist)\n    plt.figure(figsize=(15,4))\n    plt.title(f'k=1 NN MAE for Group: {R}, {C}')\n    plt.plot(ds, vs_old, label='old')\n    plt.plot(ds, vs_new, label='new')\n    plt.legend()\n    plt.show()","4b7ffa3f":"look = rez[(rez.dist<1)&(rez.R!=50)]#&(rez.orig_mae>0.8904941219161745)]\nlook.R.value_counts()","39e97aa8":"# To Proceed","fd5be41a":"# Alright now lets bake some of these in so we can derive thresholds...","31c70437":"Build k=1 NN stats for top 1000 worst offenders...","e21d29e8":"Now we attempt to find the nearest neighbor and visualize its pressure. Is it closer to our target pressure than our prediction? Any way to can make that a `f(uin)`?","c97c483a":"Hahaha yeah right. I wish.","8939a8fb":"Do you notice anything? \n\nI don't, but my analysis skills suck.","2b02c989":"# Resampling","3d7b7cb1":"Dist < 50 seems to be consistent...","108f6f12":"# Top 15 Worst Predictions","c06aa2e3":"Only a handful of guys we want to attempt to patch.","23ddb36d":"# Good Neighbors","71072929":"My idea here was that perhaps the magic had to do with the data being messy. If that was the case, if only I could identify which samples needed to be replaced, I could use a nonparametric model like kNN-Regressor with k=1 or =3 or whatever to 'snap' those positions into place where the model fails to do so correctly.\n\nHere, I look at one of my good single model's outputs (not the best one but a good one) and examine its MAE sorted from descending. Then I target the worst predictions for analysis and look at their neighbors.\n\nAn interesting note: time deltas are heterogeneous. Due to this, I first resample everything to a common format before applying neighbors search. Also, I restrict the length of time searched to the area covering up until the earliest breath exhale.","4bbbc4d5":"I find a good average timestep size and then resample to 1\/3 of that so that we have enough resolution to cover samples that aren't nicely aliased by the sampling fs.","90ba0ad9":"# Revisit The Top-100 Trash Predictions"}}