{"cell_type":{"3773e849":"code","adf23a00":"code","6660c09e":"code","30bc37a5":"code","84fc0f9b":"code","9f3d7df1":"code","adc18a6d":"code","54eb7bc0":"code","f4c608a7":"code","439e0059":"code","1d9632eb":"code","291b84f5":"code","8ece23f7":"code","dbbeb2c7":"code","ebcc93f7":"code","fcca1bae":"code","b236cb35":"code","6af214e7":"code","354d01f9":"code","fb7e9d36":"code","bfac3002":"code","1330f40c":"code","d30a10d1":"code","30f74638":"code","0886b471":"code","b4f8bb79":"code","e0f7cd84":"code","ee57147b":"code","da0fe2c0":"code","81f1b610":"code","e7c30712":"markdown","9e622e8e":"markdown","6ed226ca":"markdown","2c918d6a":"markdown","628b44e5":"markdown"},"source":{"3773e849":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","adf23a00":"import numpy as np\nimport pandas as pd\nimport itertools\nimport matplotlib.pyplot as plt\nimport warnings\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import (FunctionTransformer , StandardScaler)\nfrom sklearn.pipeline import Pipeline","6660c09e":"%%bash\npip install seaborn","30bc37a5":"import seaborn as sns","84fc0f9b":"dataset = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/train.csv')\n","9f3d7df1":"dataset.head(10)","adc18a6d":"dataset.dtypes","54eb7bc0":"#summary statistics\ndataset.describe()","f4c608a7":"#grouping based on battery power\ndataset['blue'].value_counts() ","439e0059":"#rows and columns\ndataset.shape","1d9632eb":"dataset.boxplot()","291b84f5":"features = dataset.columns[:-1].tolist()","8ece23f7":"#univariable plot\nfor feat in features:\n    skew = dataset[feat].skew()\n    sns.distplot(dataset[feat], kde= False, label='Skew = %.3f' %(skew), bins=30)\n    plt.legend(loc='best')\n    plt.show()","dbbeb2c7":"#box plot for each feature\nfeature_names = dataset.columns\nfor i in range(len(feature_names)-1):\n    figure  = plt.figure()\n    ax= sns.boxplot(x='blue', y=feature_names[i], data=dataset)\n","ebcc93f7":"#multivariate plot - to examine correation between features\nplt.figure(figsize=(8,8))\nsns.pairplot(dataset[features],palette='coolwarm')\nplt.show","fcca1bae":"#heatmap of correlations\ncorr =dataset[features].corr()\nplt.figure(figsize=(10,10))\nsns.heatmap(corr, cbar= True, square= True, annot= True, fmt='.2f', annot_kws={'size':10},\n            xticklabels=features, yticklabels=features, alpha=0.7, cmap='coolwarm')\nplt.show()","b236cb35":"#checking for null values\ndataset.info()","6af214e7":"dataset['blue'].value_counts().plot(kind='bar')\nplt.title(\"Target Frequency\")\nplt.xlabel(\"blue\")\nplt.ylabel(\"battery_power\")\nplt.show()","354d01f9":"X=dataset.drop('price_range',axis=1)","fb7e9d36":"y=dataset['price_range']","bfac3002":"from sklearn.model_selection import train_test_split","1330f40c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101)","d30a10d1":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier()","30f74638":"dtree.fit(X_train,y_train)","0886b471":"dtree.score(X_test,y_test)","b4f8bb79":"feature_names=['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',\n       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',\n       'touch_screen', 'wifi']\n","e0f7cd84":"sns.countplot(dataset['battery_power'])\nplt.show()","ee57147b":"#data transformation\n#feature scaling\nsc_X = StandardScaler()\nX_train= sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","da0fe2c0":"#confusion matrix \ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()\n","81f1b610":"# prepare models\nmodels = []\nmodels.append((\"DecisionTree\",DecisionTreeClassifier()))\nmodels.append((\"RandomForest\",RandomForestClassifier()))\nmodels.append((\"NaiveBayes\",GaussianNB()))\n\n# evaluate each model in turn\nresults = []\nnames = []\nseed=1\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=None)\n    cv_results = model_selection.cross_val_score(model, X, y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    cross_val_result = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(\"Print the Corss Validation Result {}\".format(name))\n    print(cross_val_result)\n    model.fit(X_train,y_train)\n    y_pred = model.predict(X_test)\n    cm = confusion_matrix(y_test,y_pred)\n    plot_confusion_matrix(cm=cm, classes=[0,1])\n    acc_score = accuracy_score(y_test,y_pred)\n    print(\"Accuracy Score of {} is {}\".format(name,acc_score))\n","e7c30712":"#  **MOBILE PRICE CLASSIFICATION**\n","9e622e8e":"The dataset is about Mobile prices. It has 21 attributes. We have used different classification algorithms\n\nDecision tree\nRandom forest\nNaive Bayes and compared their results","6ed226ca":"# Creating & Training Decision Tree Model","2c918d6a":"# X & Y array","628b44e5":"# Splitting the data"}}