{"cell_type":{"ee5c1b19":"code","d792f9d8":"code","ed63699a":"code","9e226f88":"code","c8fe0a2f":"code","34ccba77":"code","f550fc60":"code","7994e909":"code","581adc39":"code","5fcd54da":"code","fddb186b":"code","2adffcc9":"code","fac739bb":"code","2fcf6b51":"code","5715ba58":"code","4565c0da":"code","47512607":"code","c39d4b08":"code","2002ec68":"code","7fdd0413":"code","472f2656":"code","aae26725":"code","e541d5cd":"code","fd3947a3":"code","21a66498":"code","17a5d893":"code","eb2d1cc8":"code","195ef6cc":"code","65f3bbc9":"code","10f93ce9":"code","9cafd6c7":"code","37c5973a":"code","752cf1af":"code","76b27efc":"code","4e7e97f2":"code","85c9d7f3":"code","7d84cae9":"code","d24f7b50":"code","a76632d1":"code","252e7c23":"code","33448dc7":"markdown","0e2b0dd0":"markdown","2bf910f8":"markdown","9626b307":"markdown","14d514cc":"markdown","8f7be600":"markdown","7e04866b":"markdown","71069de0":"markdown","b1c7c988":"markdown","b5c5200f":"markdown","b5eedbf3":"markdown","a78fa338":"markdown","c912a1f7":"markdown","ef0bf186":"markdown"},"source":{"ee5c1b19":"from datetime import datetime\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nimport os\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom scipy import sparse\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.metrics import mean_squared_error\n\n\nimport xgboost as xgb\nfrom surprise import Reader, Dataset\nfrom surprise import BaselineOnly\nfrom surprise import KNNBaseline\nfrom surprise import SVD\nfrom surprise import SVDpp\nfrom surprise.model_selection import GridSearchCV","d792f9d8":"if not os.path.isfile(\"..\/NetflixRatings.csv\"): \n#This line: \"os.path.isfile(\"..\/NetflixRatings.csv\")\" simply checks that is there a file with the name \"NetflixRatings.csv\" in the \n#in the folder \"\/Data\/\". If the file is present then it return true else false\n    startTime = datetime.now()\n    data = open(\"..\/NetflixRatings.csv\", mode = \"w\") #this line simply creates the file with the name \"NetflixRatings.csv\" in \n    #write mode in the folder \"Data\".\n#     files = ['..\/Data\/combined_data_1.txt','..\/Data\/combined_data_2.txt', '..\/Data\/combined_data_3.txt', '..\/Data\/combined_data_4.txt']\n    files = ['..\/input\/netflix-prize-data\/combined_data_2.txt', '..\/input\/netflix-prize-data\/combined_data_4.txt']\n    for file in files:\n        print(\"Reading from file: \"+str(file)+\"...\")\n        with open(file) as f:  #you can think of this command \"with open(file) as f\" as similar to 'if' statement or a sort of \n            #loop statement. This command says that as long as this file is opened, perform the underneath operation.\n            for line in f:\n                line = line.strip() #line.strip() clears all the leading and trailing spaces from the string, as here each line\n                #that we are reading from a file is a string.\n                #Note first line consist of a movie id followed by a semi-colon, then second line contains custID,rating,date\n                #then third line agains contains custID,rating,date which belong to that movie ID and so on. The format of data\n                #is exactly same as shown above with the heading \"Example Data Point\". Check out above.\n                if line.endswith(\":\"):\n                    movieID = line.replace(\":\", \"\") #this will remove the trailing semi-colon and return us the leading movie ID.\n                else:\n                    #here, in the below code we have first created an empty list with the name \"row \"so that we can insert movie ID \n                    #at the first position and rest customerID, rating and date in second position. After that we have separated all \n                    #four namely movieID, custID, rating and date with comma and converted a single string by joining them with comma.\n                    #then finally written them to our output \".csv\" file.\n                    row = [] \n                    row = [x for x in line.split(\",\")] #custID, rating and date are separated by comma\n                    row.insert(0, movieID)\n                    data.write(\",\".join(row))\n                    data.write(\"\\n\")\n        print(\"Reading of file: \"+str(file)+\" is completed\\n\")\n    data.close()\n    print(\"Total time taken for execution of this code = \"+str(datetime.now() - startTime))","ed63699a":"# creating data frame from our output csv file.\nif not os.path.isfile(\"..\/NetflixData.pkl\"):\n    startTime = datetime.now()\n    Final_Data = pd.read_csv(\"..\/NetflixRatings.csv\", sep=\",\", names = [\"MovieID\",\"CustID\", \"Ratings\", \"Date\"])\n    Final_Data[\"Date\"] = pd.to_datetime(Final_Data[\"Date\"])\n    Final_Data.sort_values(by = \"Date\", inplace = True)\n    print(\"Time taken for execution of above code = \"+str(datetime.now() - startTime))","9e226f88":"# storing pandas dataframe as a picklefile for later use\nif not os.path.isfile(\"..\/NetflixData.pkl\"):\n    Final_Data.to_pickle(\"..\/NetflixData.pkl\")\nelse:\n    Final_Data = pd.read_pickle(\"..\/NetflixData.pkl\")","c8fe0a2f":"Final_Data.head()","34ccba77":"Final_Data.describe()[\"Ratings\"]","f550fc60":"if not os.path.isfile(\"..\/TrainData.pkl\"):\n    Final_Data.iloc[:int(Final_Data.shape[0]*0.80)].to_pickle(\"..\/TrainData.pkl\")\n    Train_Data = pd.read_pickle(\"..\/TrainData.pkl\")\n    Train_Data.reset_index(drop = True, inplace = True)\nelse:\n    Train_Data = pd.read_pickle(\"..\/TrainData.pkl\")\n    Train_Data.reset_index(drop = True, inplace = True)\n\nif not os.path.isfile(\"..\/TestData.pkl\"):\n    Final_Data.iloc[int(Final_Data.shape[0]*0.80):].to_pickle(\"..\/TestData.pkl\")\n    Test_Data = pd.read_pickle(\"..\/TestData.pkl\")\n    Test_Data.reset_index(drop = True, inplace = True)\nelse:\n    Test_Data = pd.read_pickle(\"..\/TestData.pkl\")\n    Test_Data.reset_index(drop = True, inplace = True)","7994e909":"def changingLabels(number):\n    return str(number\/10**6) + \"M\"","581adc39":"plt.figure(figsize = (12, 8))\nax = sns.countplot(x=\"Ratings\", data=Train_Data)\n\nax.set_yticklabels([changingLabels(num) for num in ax.get_yticks()])\n\nplt.tick_params(labelsize = 15)\nplt.title(\"Distribution of Ratings in train data\", fontsize = 20)\nplt.xlabel(\"Ratings\", fontsize = 20)\nplt.ylabel(\"Number of Ratings(Millions)\", fontsize = 20)\nplt.show()","5fcd54da":"startTime = datetime.now()\nprint(\"Creating USER_ITEM sparse matrix for train Data\")\nif os.path.isfile(\"..\/TrainUISparseData.npz\"):\n    print(\"Sparse Data is already present in your disk, no need to create further. Loading Sparse Matrix\")\n    TrainUISparseData = sparse.load_npz(\"..\/TrainUISparseData.npz\")\n    print(\"Shape of Train Sparse matrix = \"+str(TrainUISparseData.shape))\n    \nelse:\n    print(\"We are creating sparse data\")\n    TrainUISparseData = sparse.csr_matrix((Train_Data.Ratings, (Train_Data.CustID, Train_Data.MovieID)))\n    print(\"Creation done. Shape of sparse matrix = \"+str(TrainUISparseData.shape))\n    print(\"Saving it into disk for furthur usage.\")\n    sparse.save_npz(\"..\/TrainUISparseData.npz\", TrainUISparseData)\n    print(\"Done\\n\")\n\nprint(datetime.now() - startTime)","fddb186b":"startTime = datetime.now()\nprint(\"Creating USER_ITEM sparse matrix for test Data\")\nif os.path.isfile(\"..\/TestUISparseData.npz\"):\n    print(\"Sparse Data is already present in your disk, no need to create further. Loading Sparse Matrix\")\n    TestUISparseData = sparse.load_npz(\"..\/TestUISparseData.npz\")\n    print(\"Shape of Test Sparse Matrix = \"+str(TestUISparseData.shape))\nelse:\n    print(\"We are creating sparse data\")\n    TestUISparseData = sparse.csr_matrix((Test_Data.Ratings, (Test_Data.CustID, Test_Data.MovieID)))\n    print(\"Creation done. Shape of sparse matrix = \"+str(TestUISparseData.shape))\n    print(\"Saving it into disk for furthur usage.\")\n    sparse.save_npz(\"..\/TestUISparseData.npz\", TestUISparseData)\n    print(\"Done\\n\")\n\nprint(datetime.now() - startTime)","2adffcc9":"def getAverageRatings(sparseMatrix, if_user):\n    ax = 1 if if_user else 0\n    #axis = 1 means rows and axis = 0 means columns \n    sumOfRatings = sparseMatrix.sum(axis = ax).A1  #this will give an array of sum of all the ratings of user if axis = 1 else \n    #sum of all the ratings of movies if axis = 0\n    noOfRatings = (sparseMatrix!=0).sum(axis = ax).A1  #this will give a boolean True or False array, and True means 1 and False \n    #means 0, and further we are summing it to get the count of all the non-zero cells means length of non-zero cells\n    rows, cols = sparseMatrix.shape\n    averageRatings = {i: sumOfRatings[i]\/noOfRatings[i] for i in range(rows if if_user else cols) if noOfRatings[i]!=0}\n    return averageRatings","fac739bb":"Global_Average_Rating = TrainUISparseData.sum()\/TrainUISparseData.count_nonzero()\nprint(\"Global Average Rating {}\".format(Global_Average_Rating))","2fcf6b51":"AvgRatingUser = getAverageRatings(TrainUISparseData, True)\nprint(\"Average rating of user 25 = {}\".format(AvgRatingUser[25]))","5715ba58":"AvgRatingMovie = getAverageRatings(TrainUISparseData, False)\nprint(\"Average rating of movie 4500 = {}\".format(AvgRatingMovie[4500]))","4565c0da":"start = datetime.now()\n\nif not os.path.isfile(\"..\/m_m_similarity.npz\"):\n    print(\"Movie-Movie Similarity file does not exist in your disk. Creating Movie-Movie Similarity Matrix...\")\n    \n    m_m_similarity = cosine_similarity(TrainUISparseData.T, dense_output = False)\n    print(\"Done\")\n    print(\"Dimension of Matrix = {}\".format(m_m_similarity.shape))\n    print(\"Storing the Movie Similarity matrix on disk for further usage\")\n    sparse.save_npz(\"..\/m_m_similarity.npz\", m_m_similarity)\nelse:\n    print(\"File exists in the disk. Loading the file...\")\n    m_m_similarity = sparse.load_npz(\"..\/m_m_similarity.npz\")\n    print(\"Dimension of Matrix = {}\".format(m_m_similarity.shape))\n    \nprint(datetime.now() - start)","47512607":"def get_sample_sparse_matrix(sparseMatrix, n_users, n_movies):\n    startTime = datetime.now()\n    users, movies, ratings = sparse.find(sparseMatrix)\n    uniq_users = np.unique(users)\n    uniq_movies = np.unique(movies)\n    np.random.seed(15)   #this will give same random number everytime, without replacement\n    userS = np.random.choice(uniq_users, n_users, replace = False)\n    movieS = np.random.choice(uniq_movies, n_movies, replace = False)\n    mask = np.logical_and(np.isin(users, userS), np.isin(movies, movieS))\n    sparse_sample = sparse.csr_matrix((ratings[mask], (users[mask], movies[mask])), \n                                                     shape = (max(userS)+1, max(movieS)+1))\n    print(\"Sparse Matrix creation done. Saving it for later use.\")\n    sparse.save_npz(path, sparse_sample)\n    print(\"Done\")\n    print(\"Shape of Sparse Sampled Matrix = \"+str(sparse_sample.shape))\n    \n    print(datetime.now() - start)\n    return sparse_sample","c39d4b08":"path = \"..\/TrainUISparseData_Sample.npz\"\nif not os.path.isfile(path):\n    print(\"Sample sparse matrix is not present in the disk. We are creating it...\")\n    train_sample_sparse = get_sample_sparse_matrix(TrainUISparseData, 4000, 400)\nelse:\n    print(\"File is already present in the disk. Loading the file...\")\n    train_sample_sparse = sparse.load_npz(path)\n    print(\"File loading done.\")\n    print(\"Shape of Train Sample Sparse Matrix = \"+str(train_sample_sparse.shape))","2002ec68":"path = \"..\/TestUISparseData_Sample.npz\"\nif not os.path.isfile(path):\n    print(\"Sample sparse matrix is not present in the disk. We are creating it...\")\n    test_sample_sparse = get_sample_sparse_matrix(TestUISparseData, 2000, 200)\nelse:\n    print(\"File is already present in the disk. Loading the file...\")\n    test_sample_sparse = sparse.load_npz(path)\n    print(\"File loading done.\")\n    print(\"Shape of Test Sample Sparse Matrix = \"+str(test_sample_sparse.shape))","7fdd0413":"globalAvgMovies = getAverageRatings(train_sample_sparse, False)\nprint(\"Average move rating for movie 14890 is {}\".format(globalAvgMovies[14890]))\n\nglobalAvgUsers = getAverageRatings(train_sample_sparse, True)\nprint(\"Average user rating for user 16879 is {}\".format(globalAvgMovies[16879]))","472f2656":"sample_train_users, sample_train_movies, sample_train_ratings = sparse.find(train_sample_sparse)","aae26725":"if os.path.isfile(\"..\/Ttrain_Regression.csv\"):\n    print(\"File is already present in your disk. You do not have to prepare it again.\")\nelse:\n    startTime = datetime.now()\n    print(\"Preparing Train csv file for {} rows\".format(len(sample_train_ratings)))\n    with open(\"..\/Train_Regression.csv\", mode = \"w\") as data:\n        count = 0\n        for user, movie, rating in zip(sample_train_users, sample_train_movies, sample_train_ratings):\n            row = list()\n            row.append(user)  #appending user ID\n            row.append(movie) #appending movie ID\n            row.append(train_sample_sparse.sum()\/train_sample_sparse.count_nonzero()) #appending global average rating\n\n#----------------------------------Ratings given to \"movie\" by top 5 similar users with \"user\"--------------------#\n            similar_users = cosine_similarity(train_sample_sparse[user], train_sample_sparse).ravel()\n            similar_users_indices = np.argsort(-similar_users)[1:]\n            similar_users_ratings = train_sample_sparse[similar_users_indices, movie].toarray().ravel()\n            top_similar_user_ratings = list(similar_users_ratings[similar_users_ratings != 0][:5])\n            top_similar_user_ratings.extend([globalAvgMovies[movie]]*(5-len(top_similar_user_ratings)))\n            #above line means that if top 5 ratings are not available then rest of the ratings will be filled by \"movie\" average\n            #rating. Let say only 3 out of 5 ratings are available then rest 2 will be \"movie\" average rating.\n            row.extend(top_similar_user_ratings)\n            \n #----------------------------------Ratings given by \"user\" to top 5 similar movies with \"movie\"------------------#\n            similar_movies = cosine_similarity(train_sample_sparse[:,movie].T, train_sample_sparse.T).ravel()\n            similar_movies_indices = np.argsort(-similar_movies)[1:]\n            similar_movies_ratings = train_sample_sparse[user, similar_movies_indices].toarray().ravel()\n            top_similar_movie_ratings = list(similar_movies_ratings[similar_movies_ratings != 0][:5])\n            top_similar_movie_ratings.extend([globalAvgUsers[user]]*(5-len(top_similar_movie_ratings)))\n            #above line means that if top 5 ratings are not available then rest of the ratings will be filled by \"user\" average\n            #rating. Let say only 3 out of 5 ratings are available then rest 2 will be \"user\" average rating.\n            row.extend(top_similar_movie_ratings)\n            \n #----------------------------------Appending \"user\" average, \"movie\" average & rating of \"user\"\"movie\"-----------#\n            row.append(globalAvgUsers[user])\n            row.append(globalAvgMovies[movie])\n            row.append(rating)\n            \n#-----------------------------------Converting rows and appending them as comma separated values to csv file------#\n            data.write(\",\".join(map(str, row)))\n            data.write(\"\\n\")\n    \n            count += 1\n            if count % 2000 == 0:\n                print(\"Done for {}. Time elapsed: {}\".format(count, (datetime.now() - startTime)))\n                \n    print(\"Total Time for {} rows = {}\".format(len(sample_train_ratings), (datetime.now() - startTime)))","e541d5cd":"Train_Reg = pd.read_csv(\"..\/Train_Regression.csv\", names = [\"User_ID\", \"Movie_ID\", \"Global_Average\", \"SUR1\", \"SUR2\", \"SUR3\", \"SUR4\", \"SUR5\", \"SMR1\", \"SMR2\", \"SMR3\", \"SMR4\", \"SMR5\", \"User_Average\", \"Movie_Average\", \"Rating\"])\nTrain_Reg.head()","fd3947a3":"sample_test_users, sample_test_movies, sample_test_ratings = sparse.find(test_sample_sparse)","21a66498":"if os.path.isfile(\"..\/Test_Regression.csv\"):\n    print(\"File is already present in your disk. You do not have to prepare it again.\")\nelse:\n    startTime = datetime.now()\n    print(\"Preparing Test csv file for {} rows\".format(len(sample_test_ratings)))\n    with open(\"..\/Test_Regression.csv\", mode = \"w\") as data:\n        count = 0\n        for user, movie, rating in zip(sample_test_users, sample_test_movies, sample_test_ratings):\n            row = list()\n            row.append(user)  #appending user ID\n            row.append(movie) #appending movie ID\n            row.append(train_sample_sparse.sum()\/train_sample_sparse.count_nonzero()) #appending global average rating\n\n#-----------------------------Ratings given to \"movie\" by top 5 similar users with \"user\"-------------------------#\n            try:\n                similar_users = cosine_similarity(train_sample_sparse[user], train_sample_sparse).ravel()\n                similar_users_indices = np.argsort(-similar_users)[1:]\n                similar_users_ratings = train_sample_sparse[similar_users_indices, movie].toarray().ravel()\n                top_similar_user_ratings = list(similar_users_ratings[similar_users_ratings != 0][:5])\n                top_similar_user_ratings.extend([globalAvgMovies[movie]]*(5-len(top_similar_user_ratings)))\n                #above line means that if top 5 ratings are not available then rest of the ratings will be filled by \"movie\" \n                #average rating. Let say only 3 out of 5 ratings are available then rest 2 will be \"movie\" average rating.\n                row.extend(top_similar_user_ratings)\n            #########Cold Start Problem, for a new user or a new movie#########    \n            except(IndexError, KeyError):\n                global_average_train_rating = [train_sample_sparse.sum()\/train_sample_sparse.count_nonzero()]*5\n                row.extend(global_average_train_rating)\n            except:\n                raise\n                \n #-----------------------------Ratings given by \"user\" to top 5 similar movies with \"movie\"-----------------------#\n            try:\n                similar_movies = cosine_similarity(train_sample_sparse[:,movie].T, train_sample_sparse.T).ravel()\n                similar_movies_indices = np.argsort(-similar_movies)[1:]\n                similar_movies_ratings = train_sample_sparse[user, similar_movies_indices].toarray().ravel()\n                top_similar_movie_ratings = list(similar_movies_ratings[similar_movies_ratings != 0][:5])\n                top_similar_movie_ratings.extend([globalAvgUsers[user]]*(5-len(top_similar_movie_ratings)))\n                #above line means that if top 5 ratings are not available then rest of the ratings will be filled by \"user\" \n                #average rating. Let say only 3 out of 5 ratings are available then rest 2 will be \"user\" average rating.\n                row.extend(top_similar_movie_ratings)\n            #########Cold Start Problem, for a new user or a new movie#########\n            except(IndexError, KeyError):\n                global_average_train_rating = [train_sample_sparse.sum()\/train_sample_sparse.count_nonzero()]*5\n                row.extend(global_average_train_rating)\n            except:\n                raise\n                \n #-----------------------------Appending \"user\" average, \"movie\" average & rating of \"user\"\"movie\"----------------#\n            try:        \n                row.append(globalAvgUsers[user])\n            except (KeyError):\n                global_average_train_rating = train_sample_sparse.sum()\/train_sample_sparse.count_nonzero()\n                row.append(global_average_train_rating)\n            except:\n                raise\n                \n            try:\n                row.append(globalAvgMovies[movie])\n            except(KeyError):\n                global_average_train_rating = train_sample_sparse.sum()\/train_sample_sparse.count_nonzero()\n                row.append(global_average_train_rating)\n            except:\n                raise\n                \n            row.append(rating)\n            \n#------------------------------Converting rows and appending them as comma separated values to csv file-----------#\n            data.write(\",\".join(map(str, row)))\n            data.write(\"\\n\")\n    \n            count += 1\n            if count % 100 == 0:\n                print(\"Done for {}. Time elapsed: {}\".format(count, (datetime.now() - startTime)))\n                \n    print(\"Total Time for {} rows = {}\".format(len(sample_test_ratings), (datetime.now() - startTime)))","17a5d893":"Test_Reg = pd.read_csv(\"..\/Test_Regression.csv\", names = [\"User_ID\", \"Movie_ID\", \"Global_Average\", \"SUR1\", \"SUR2\", \"SUR3\", \"SUR4\", \"SUR5\", \"SMR1\", \"SMR2\", \"SMR3\", \"SMR4\", \"SMR5\", \"User_Average\", \"Movie_Average\", \"Rating\"])\nTest_Reg.head()","eb2d1cc8":"reader = Reader(rating_scale=(1, 5))\n\ndata = Dataset.load_from_df(Train_Reg[['User_ID', 'Movie_ID', 'Rating']], reader)\n\ntrainset = data.build_full_trainset()","195ef6cc":"testset = list(zip(Test_Reg[\"User_ID\"].values, Test_Reg[\"Movie_ID\"].values, Test_Reg[\"Rating\"].values))","65f3bbc9":"error_table = pd.DataFrame(columns = [\"Model\", \"Train RMSE\", \"Train MAPE\", \"Test RMSE\", \"Test MAPE\"])\nmodel_train_evaluation = dict()\nmodel_test_evaluation = dict()","10f93ce9":"def make_table(model_name, rmse_train, mape_train, rmse_test, mape_test):\n    global error_table\n    #All variable assignments in a function store the value in the local symbol table; whereas variable references first look \n    #in the local symbol table, then in the global symbol table, and then in the table of built-in names. Thus, global variables \n    #cannot be directly assigned a value within a function (unless named in a global statement), \n    #although they may be referenced.\n    error_table = error_table.append(pd.DataFrame([[model_name, rmse_train, mape_train, rmse_test, mape_test]], columns = [\"Model\", \"Train RMSE\", \"Train MAPE\", \"Test RMSE\", \"Test MAPE\"]))\n    error_table.reset_index(drop = True, inplace = True)","9cafd6c7":"def error_metrics(y_true, y_pred):\n    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n    mape = np.mean(abs((y_true - y_pred)\/y_true))*100\n    return rmse, mape","37c5973a":"def train_test_xgboost(x_train, x_test, y_train, y_test, model_name):\n    startTime = datetime.now()\n    train_result = dict()\n    test_result = dict()\n    \n    clf = xgb.XGBRegressor(n_estimators = 100, silent = False, n_jobs  = 10)\n    clf.fit(x_train, y_train)\n    \n    print(\"-\"*50)\n    print(\"TRAIN DATA\")\n    y_pred_train = clf.predict(x_train)\n    rmse_train, mape_train = error_metrics(y_train, y_pred_train)\n    print(\"RMSE = {}\".format(rmse_train))\n    print(\"MAPE = {}\".format(mape_train))\n    print(\"-\"*50)\n    train_result = {\"RMSE\": rmse_train, \"MAPE\": mape_train, \"Prediction\": y_pred_train}\n    \n    print(\"TEST DATA\")\n    y_pred_test = clf.predict(x_test)\n    rmse_test, mape_test = error_metrics(y_test, y_pred_test)\n    print(\"RMSE = {}\".format(rmse_test))\n    print(\"MAPE = {}\".format(mape_test))\n    print(\"-\"*50)\n    test_result = {\"RMSE\": rmse_test, \"MAPE\": mape_test, \"Prediction\": y_pred_test}\n        \n    print(\"Time Taken = \"+str(datetime.now() - startTime))\n    \n    plot_importance(xgb, clf)\n    \n    make_table(model_name, rmse_train, mape_train, rmse_test, mape_test)\n    \n    return train_result, test_result","752cf1af":"def plot_importance(model, clf):\n    fig = plt.figure(figsize = (8, 6))\n    ax = fig.add_axes([0,0,1,1])\n    model.plot_importance(clf, ax = ax, height = 0.3)\n    plt.xlabel(\"F Score\", fontsize = 20)\n    plt.ylabel(\"Features\", fontsize = 20)\n    plt.title(\"Feature Importance\", fontsize = 20)\n    plt.tick_params(labelsize = 15)\n    \n    plt.show()","76b27efc":"def get_ratings(predictions):\n    actual = np.array([pred.r_ui for pred in predictions])\n    predicted = np.array([pred.est for pred in predictions])\n    return actual, predicted\n#in surprise prediction of every data point is returned as dictionary like this:\n#\"user: 196        item: 302        r_ui = 4.00   est = 4.06   {'actual_k': 40, 'was_impossible': False}\"\n#In this dictionary, \"r_ui\" is a key for actual rating and \"est\" is a key for predicted rating","4e7e97f2":"def get_error(predictions):\n    actual, predicted = get_ratings(predictions)\n    rmse = np.sqrt(mean_squared_error(actual, predicted)) \n    mape = np.mean(abs((actual - predicted)\/actual))*100\n    return rmse, mape","85c9d7f3":"my_seed = 15\nrandom.seed(my_seed)\nnp.random.seed(my_seed)\n\ndef run_surprise(algo, trainset, testset, model_name):\n    startTime = datetime.now()\n    \n    train = dict()\n    test = dict()\n    \n    algo.fit(trainset)\n    #You can check out above function at \"https:\/\/surprise.readthedocs.io\/en\/stable\/getting_started.html\" in \n    #\"Train-test split and the fit() method\" section\n    \n#-----------------Evaluating Train Data------------------#\n    print(\"-\"*50)\n    print(\"TRAIN DATA\")\n    train_pred = algo.test(trainset.build_testset())\n    #You can check out \"algo.test()\" function at \"https:\/\/surprise.readthedocs.io\/en\/stable\/getting_started.html\" in \n    #\"Train-test split and the fit() method\" section\n    #You can check out \"trainset.build_testset()\" function at \"https:\/\/surprise.readthedocs.io\/en\/stable\/FAQ.html#can-i-use-my-own-dataset-with-surprise-and-can-it-be-a-pandas-dataframe\" in \n    #\"How to get accuracy measures on the training set\" section\n    train_actual, train_predicted = get_ratings(train_pred)\n    train_rmse, train_mape = get_error(train_pred)\n    print(\"RMSE = {}\".format(train_rmse))\n    print(\"MAPE = {}\".format(train_mape))\n    print(\"-\"*50)\n    train = {\"RMSE\": train_rmse, \"MAPE\": train_mape, \"Prediction\": train_predicted}\n    \n#-----------------Evaluating Test Data------------------#\n    print(\"TEST DATA\")\n    test_pred = algo.test(testset)\n    #You can check out \"algo.test()\" function at \"https:\/\/surprise.readthedocs.io\/en\/stable\/getting_started.html\" in \n    #\"Train-test split and the fit() method\" section\n    test_actual, test_predicted = get_ratings(test_pred)\n    test_rmse, test_mape = get_error(test_pred)\n    print(\"RMSE = {}\".format(test_rmse))\n    print(\"MAPE = {}\".format(test_mape))\n    print(\"-\"*50)\n    test = {\"RMSE\": test_rmse, \"MAPE\": test_mape, \"Prediction\": test_predicted}\n    \n    print(\"Time Taken = \"+str(datetime.now() - startTime))\n    \n    make_table(model_name, train_rmse, train_mape, test_rmse, test_mape)\n    \n    return train, test","7d84cae9":"x_train = Train_Reg.drop([\"User_ID\", \"Movie_ID\", \"Rating\"], axis = 1)\n\nx_test = Test_Reg.drop([\"User_ID\", \"Movie_ID\", \"Rating\"], axis = 1)\n\ny_train = Train_Reg[\"Rating\"]\n\ny_test = Test_Reg[\"Rating\"]\n\ntrain_result, test_result = train_test_xgboost(x_train, x_test, y_train, y_test, \"XGBoost_13\")\n\nmodel_train_evaluation[\"XGBoost_13\"] = train_result\nmodel_test_evaluation[\"XGBoost_13\"] = test_result","d24f7b50":"bsl_options = {\"method\":\"sgd\", \"learning_rate\":0.01, \"n_epochs\":25}\n\nalgo = BaselineOnly(bsl_options=bsl_options)\n#You can check the docs of above used functions at:https:\/\/surprise.readthedocs.io\/en\/stable\/prediction_algorithms.html#baseline-estimates-configuration\n#at section \"Baselines estimates configuration\".\n\ntrain_result, test_result = run_surprise(algo, trainset, testset, \"BaselineOnly\")\n\nmodel_train_evaluation[\"BaselineOnly\"] = train_result\nmodel_test_evaluation[\"BaselineOnly\"] = test_result","a76632d1":"param_grid  = {'sim_options':{'name': [\"pearson_baseline\"], \"user_based\": [False], \"min_support\": [2], \"shrinkage\": [60, 80, 80, 140]}, 'k': [5, 20, 40, 80]}\n\ngs = GridSearchCV(KNNBaseline, param_grid, measures=['rmse', 'mae'], cv=3)\n\ngs.fit(data)\n\n# best RMSE score\nprint(gs.best_score['rmse'])\n\n# combination of parameters that gave the best RMSE score\nprint(gs.best_params['rmse'])","252e7c23":"sim_options = {'name':'pearson_baseline', 'user_based':False, 'min_support':2, 'shrinkage':gs.best_params['rmse']['sim_options']['shrinkage']}\n\nbsl_options = {'method': 'sgd'} \n\nalgo = KNNBaseline(k = gs.best_params['rmse']['k'], sim_options = sim_options, bsl_options=bsl_options)\n\ntrain_result, test_result = run_surprise(algo, trainset, testset, \"KNNBaseline_Item\")\n\nmodel_train_evaluation[\"KNNBaseline_Item\"] = train_result\nmodel_test_evaluation[\"KNNBaseline_Item\"] = test_result","33448dc7":"## Computing Movie-Movie Similarity Matrix","0e2b0dd0":"# 2. Exploratory Data Analysis","2bf910f8":"### Average Rating Per User","9626b307":"### Global Average Rating","14d514cc":"# 4. Modeling","8f7be600":"# 3. Creating USER-ITEM sparse matrix from data frame","7e04866b":"## Finding Global average of all movie ratings, Average rating per user, and Average rating per movie","71069de0":"$ \\large \\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{v \\in N^k_i(u)} \\text{sim}(u, v) \\cdot (r_{vi} - b_{vi})} {\\sum\\limits_{v \\in N^k_i(u)} \\text{sim}(u, v)} $","b1c7c988":"## Splitting Data into Train and Test","b5c5200f":"### Surprise KNN-Baseline with Item-Item Similarity","b5eedbf3":"### Average Rating Per Movie","a78fa338":"$ \\large\\hat{r}_{ui} = \\mu + b_u + b_i $","c912a1f7":"# 1. Reading and Storing Data","ef0bf186":"### Surpise(Library) Baseline Model"}}