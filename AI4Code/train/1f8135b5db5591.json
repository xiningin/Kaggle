{"cell_type":{"a1af7a7f":"code","4b475eaf":"code","7cd78630":"code","c3a5ed8c":"code","a315d163":"code","19f95f4e":"code","81429e47":"code","8311da72":"code","3a7e5d55":"code","66d1aa08":"code","6a9387da":"code","6ac08f72":"code","454daa76":"code","2cb9fc7d":"code","4f04cc49":"code","18e30c01":"code","5d05570a":"code","a51ccd3d":"code","22884bc8":"markdown","ab550446":"markdown","9b167577":"markdown","d6b1fd63":"markdown","b3af586e":"markdown","475eaa28":"markdown","396f3409":"markdown","12a273f6":"markdown","8e5cd10a":"markdown","524c2881":"markdown"},"source":{"a1af7a7f":"!pip install --upgrade scikit-learn scikit-learn-intelex --progress-bar off >> pip_installs.log","4b475eaf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Patch sklearn using Intel Extension to speedup computation\n# It has limited coverage but useful to include nonetheless\nfrom sklearnex import patch_sklearn\npatch_sklearn()\n\n# Set sklearn display to diagrams so that pipelines can be displayed\n# better\nfrom sklearn import set_config\nset_config(display = 'diagram')","7cd78630":"full_data = pd.read_csv('..\/input\/easy-peasy-its-lemon-squeezy\/data.csv',\n                        parse_dates = ['DateTimeOfAccident', 'DateReported'])\nfull_data.tail()","c3a5ed8c":"# Missing value?\nfull_data.isnull().sum()","a315d163":"# Handling the missingness by filling with [U]nknown, check the dataset's description\nfull_data['MaritalStatus'] = full_data['MaritalStatus'].fillna('U')","19f95f4e":"# Visuals on most features\n# Boring, bust must be done. It's just a handful of features anyway.\n\n# Import necessary auxiliary stuffs\nfrom matplotlib import ticker, gridspec\n\n# Define size and grid spec\nfig = plt.figure(figsize=(15, 10))\ngs = gridspec.GridSpec(3, 3, figure=fig)\n\nax = fig.add_subplot(gs[0, 0])\nsns.histplot(data = full_data, x = 'WeeklyWages', y = 'UltimateIncurredClaimCost',\n                ax = ax)\nax.set(yscale = 'log',\n       xscale = 'log')\n\nax = fig.add_subplot(gs[0, 1])\nsns.histplot(data = full_data, x = 'InitialIncurredClaimsCost', y = 'UltimateIncurredClaimCost',\n                ax = ax)\nax.set(yscale = 'log',\n       xscale = 'log')\n\nax = fig.add_subplot(gs[0, 2])\nsns.boxenplot(data = full_data, x = 'Gender', y = 'UltimateIncurredClaimCost',\n              ax = ax)\nax.set(yscale = 'log')\n\nax = fig.add_subplot(gs[1, 0])\nsns.boxenplot(data = full_data, x = 'MaritalStatus', y = 'UltimateIncurredClaimCost',\n              order = ['S', 'M', 'U'],\n              ax = ax)\nax.set(yscale = 'log')\n\nax = fig.add_subplot(gs[1, 1])\nsns.boxenplot(data = full_data, x = 'DependentChildren', y = 'UltimateIncurredClaimCost',\n              ax = ax)\nax.set(yscale = 'log')\n\nax = fig.add_subplot(gs[1, 2])\nsns.boxenplot(data = full_data, x = 'DependentsOther', y = 'UltimateIncurredClaimCost',\n              ax = ax)\nax.set(yscale = 'log')\n\nax = fig.add_subplot(gs[2, 0])\nsns.boxenplot(data = full_data, x = 'PartTimeFullTime', y = 'UltimateIncurredClaimCost',\n              ax = ax)\nax.set(yscale = 'log')\n\nax = fig.add_subplot(gs[2, 1])\nsns.scatterplot(data = full_data, x = 'HoursWorkedPerWeek', y = 'UltimateIncurredClaimCost',\n                alpha = 0.05, color = 'black',\n                ax = ax)\nax.set(yscale = 'log',\n       xscale = 'log')\n\nax = fig.add_subplot(gs[2, 2])\nsns.boxenplot(data = full_data, x = 'DaysWorkedPerWeek', y = 'UltimateIncurredClaimCost',\n              ax = ax)\nax.set(yscale = 'log')\n\nplt.tight_layout()\nplt.show()","81429e47":"# Other (personally more interesting) visuals\n\n# Import necessary auxiliary stuffs\nfrom datetime import timedelta\nfrom sklearn.decomposition import PCA\nfrom matplotlib import colors\n\nfig = plt.figure(figsize = (15, 10))\ngs = gridspec.GridSpec(2, 2, figure=fig)\n\n# First three principal components of insurance claim keywords\n_keywords = full_data.loc[:, full_data.columns.str.contains('ClaimDescriptionKeyword_')]\n_pc = pd.DataFrame(PCA(n_components = 3).fit_transform(_keywords), columns = ['PC1', 'PC2', 'PC3'])\n\nax = plt.subplot(gs[0, 0])\nax.axvline(0, color = 'black', linestyle = '--')\nax.axhline(0, color = 'black', linestyle = '--')\nsns.scatterplot(data = _pc, x = 'PC1', y = 'PC2', hue = full_data['UltimateIncurredClaimCost'],\n                palette = 'magma_r', hue_norm = colors.LogNorm(),\n                ax = ax)\n\nax = plt.subplot(gs[1, 0])\nax.axvline(0, color = 'black', linestyle = '--')\nax.axhline(0, color = 'black', linestyle = '--')\nsns.scatterplot(data = _pc, x = 'PC1', y = 'PC3', hue = full_data['UltimateIncurredClaimCost'],\n                palette = 'magma_r', hue_norm = colors.LogNorm(),\n                ax = ax)\n\nax = plt.subplot(gs[1, 1])\nax.axvline(0, color = 'black', linestyle = '--')\nax.axhline(0, color = 'black', linestyle = '--')\nsns.scatterplot(data = _pc, x = 'PC2', y = 'PC3', hue = full_data['UltimateIncurredClaimCost'],\n                palette = 'magma_r', hue_norm = colors.LogNorm(),\n                ax = ax)\n\n\n# Number of days between accident and claim\n_delay = (full_data['DateReported'] - full_data['DateTimeOfAccident']).dt.days\n\nax = plt.subplot(gs[0, 1])\nsns.regplot(x = _delay, y = full_data['UltimateIncurredClaimCost'],\n            scatter_kws = {'alpha': 0.05, 'color': 'black'}, lowess = True,\n            ax = ax)\nax.set(yscale = 'log', xscale = 'log',\n       xlabel = 'Number of days between accident and report')\n\nplt.show()","8311da72":"# Visualize each of the 12 claim description keywords on their own\n# I think mostly not show some clear relationship\n\n_multiplot = pd.concat([_keywords, full_data['UltimateIncurredClaimCost']], axis = 1).melt(id_vars = 'UltimateIncurredClaimCost')\n_multiplot['variable'] = _multiplot['variable'].str.lstrip(\"ClaimDescriptionKeyword_\")\n_multiplot = _multiplot.rename(columns = {'variable': 'ClaimDescriptionKeyword'})\n\ng = sns.FacetGrid(_multiplot, col = 'ClaimDescriptionKeyword', col_wrap = 4)\ng.set(yscale ='log')\ng.map(sns.histplot, 'value', 'UltimateIncurredClaimCost')\ng.map(sns.regplot, 'value', 'UltimateIncurredClaimCost', scatter = False, line_kws = {'color': 'black'})\nplt.show()","3a7e5d55":"# Partition data into train and test sets\n\nfrom sklearn.model_selection import train_test_split\n\nX, X_test, y, y_test = train_test_split(full_data.drop(['UltimateIncurredClaimCost', 'ClaimNumber'], axis = 1),\n                                        full_data['UltimateIncurredClaimCost'],\n                                        random_state = 22_23_07_11)","66d1aa08":"# Baseline model\n# Use linear regression with regularization, i.e. ElasticNet\n# Hyperparameter is set to use the highest-performing form of this baseline.\n# Only original features will be considered in this baseline.\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.linear_model import ElasticNet\n\nbaseline_preproc = make_column_transformer(\n    (OneHotEncoder(handle_unknown = 'ignore', sparse = False), make_column_selector(dtype_include = object)),\n    ('drop', ['DateTimeOfAccident', 'DateReported']),\n    remainder = 'passthrough'\n)\n\nbaseline = Pipeline([\n    ('preproc', baseline_preproc),\n    ('regr', ElasticNet(max_iter = 1_000))\n])","6a9387da":"from skopt import BayesSearchCV, space, plots\n\nparams = {\n    'regr__alpha': space.Real(1e-5, 1e5, prior = 'log-uniform'),\n    'regr__l1_ratio': space.Real(0, 1, prior = 'uniform')\n}\n\nbs = BayesSearchCV(baseline, params, n_iter = 100, cv = 3,\n                   scoring = 'neg_mean_absolute_percentage_error',\n                   refit = False, random_state = 1225)\nbs.fit(X, y)\n\nplots.plot_objective(bs.optimizer_results_[0],\n                     dimensions = ['regr__C', 'regr__l1_ratio'])\nplt.show()","6ac08f72":"baseline.set_params(**bs.best_params_)\nbaseline.fit(X, y)","454daa76":"# Our attempt to develop a better model\n# Hyperparameter tuning and feature engineering will be considered.\n# - Quantile transformation\n# - Principal component\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, QuantileTransformer\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.decomposition import PCA\nfrom sklearn.compose import make_column_transformer, make_column_selector\n\nohe = OneHotEncoder(handle_unknown = 'ignore')\nohe_cols = make_column_selector(dtype_include = object)\n\nqt = QuantileTransformer()\nqt_cols = ['Age', 'WeeklyWages', 'HoursWorkedPerWeek', 'DaysWorkedPerWeek',\n           'InitialIncurredClaimsCost']\n\npca = PCA()\npca_cols = make_column_selector(pattern = '^ClaimDescriptionKeyword_')\n\npreprocessor = make_column_transformer(\n    (ohe, ohe_cols),\n    (qt, qt_cols),\n    (pca, pca_cols),\n    remainder = 'drop'\n)\n\nregr = GradientBoostingRegressor()\n\nmodel = Pipeline([\n    ('preproc', preprocessor),\n    ('regr', regr)\n])\nmodel","2cb9fc7d":"from skopt import BayesSearchCV, space, plots\n\nparams = {\n    'regr__learning_rate': space.Real(0.002, 0.2, prior = 'log-uniform'),\n    'regr__max_depth': space.Integer(1, 15, prior = 'uniform'),\n    'regr__n_estimators': space.Integer(200, 1500, prior = 'uniform'),\n    'regr__subsample': space.Real(0.5, 1, prior = 'uniform')\n}\n\nbs = BayesSearchCV(model, params, n_iter = 25, cv = 3,\n                   scoring = 'neg_mean_absolute_percentage_error',\n                   refit = False, random_state = 1225)\nbs.fit(X, y)\n\nplots.plot_objective(bs.optimizer_results_[0],\n                     dimensions = ['regr__learning_rate', 'regr__max_depth',\n                                   'regr__n_estimators', 'regr__subsample'])\nplt.show()","4f04cc49":"model.set_params(**bs.best_params_)\nmodel.fit(X, y)","18e30c01":"# Compare the MAPE (mean absolute percentage error)\nfrom sklearn.metrics import mean_absolute_percentage_error as MAPE\n\nbaseline_pred = baseline.predict(X_test)\nmodel_pred = model.predict(X_test)\n\nprint(\"Baseline:\")\nprint(MAPE(y_test, baseline_pred))\n\nprint(\"Developed model:\")\nprint(MAPE(y_test, model_pred))","5d05570a":"# Show coefficients for the linear regression\n\n_df = pd.DataFrame({'coef': baseline['regr'].coef_.ravel()})\n_df['variable'] = baseline['preproc'].get_feature_names_out()\n\n\nfig, ax = plt.subplots(figsize = (8, 8))\nsns.barplot(\n    data = _df, x = 'coef', y = 'variable'\n)\nfig.tight_layout()\nplt.show()","a51ccd3d":"# Show permutation importance for the gradient boosting regression\n\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import train_test_split\n\nperm_imp = permutation_importance(\n    model, X_test, y_test, n_repeats=10, random_state=19\n)\n\n_df = pd.DataFrame(perm_imp.importances.T, columns = X.columns).melt(value_name = 'importance')\n_order = X.columns[perm_imp.importances_mean.argsort()[::-1]]\n\nfig, ax = plt.subplots(figsize = (8, 8))\n\nsns.boxplot(\n    data = _df, x = 'importance', y = 'variable',\n    order = _order\n).set(title = 'Permutation importances (on test set)')\nsns.set_style('darkgrid')\n\nfig.tight_layout()\nplt.show()","22884bc8":"# Explore and engineer the features","ab550446":"# Post-mortem: what\/why is important","9b167577":"# Closing\n\nThere you go, folks. I hope we can learn something together from this notebook.\n\nI am open to improvements and suggestions, you can leave them at the comments section.\n\nFeel free to upvote and fork, I'd be very happy to know that you enjoy my work!\n\nKeep learning and happy data-sciencing!","d6b1fd63":"I am most interested in variables `ClaimDescriptionKeyword`. I have gut feeling that originally this group of 12-features is a textual description of the insurance claim explaining what happens to who, how severe, etc. The text was then mapped into a 12-dimensional vector (perhaps as an attempt of anonymizing privacy\/confidential issues). Because of this gut feeling I feel strongly compelled to do PCA on these 12-features.","b3af586e":"Both models perform well, actually. On average, both models predict only several percents away from the true value! Our gradient boosting model is also more performant than our Elasticnet baseline.","475eaa28":"Does our model perform better?\n\nThe moment of truth:","396f3409":"# Establish baseline and develop our model\n\nIn this notebook we will use simplest and well-known model as baseline: **Linear Regression**. To account for multicollinearity and the possible need of feature selection\/regularization, we will implement **Elasticnet** penalty.\n\nTo further establish a better baseline, we will tune it so we get the most powerful form of Elasticnet as our baseline. However, we will not do any feature engineering just yet.\n\nRather than doing grid search, we will do Bayesian optimization because it does not require exact pre-specification of candidates and thus able to find better hyperparameter combination. It also finds it faster relative to grid search, especially when there's many hyperparameters to tune.","12a273f6":"# Partition data","8e5cd10a":"And for our developed model, we will use gradient boosting regression that comes out of the box from scikit-learn. We will also tune most of its parameters and see how these parameters interact with the accuracy and with each other from the resulting plots.","524c2881":"# Overview\n\nThis notebook will take you through:\n\n- exploring the data,\n- engineering the features,\n- establishing a good baseline,\n- developing a more performant model, and\n- tuning your models using Bayesian optimization package `scikit-optimize`.\n\nI welcome any feedback and suggestion of improvements in the comments section.\n\nEnjoy!"}}