{"cell_type":{"78fa9516":"code","1a420a1a":"code","2b0987a0":"code","d52d3099":"code","da32a1d1":"code","bb83efe0":"code","622bfdcf":"code","c01e137f":"code","7f6304e2":"code","f94cc789":"code","4d3899a6":"code","04eaa7c0":"code","d4dd8f07":"code","fcfea4da":"code","c58afc3f":"code","85bdf09a":"code","a1998581":"code","e08b518f":"code","bbc91673":"code","28473d61":"code","d4a121f6":"code","c3382a09":"code","d80bdf90":"code","33296e7a":"code","c34508a9":"code","ad5eaef7":"code","2a4c1d3b":"code","b0e84750":"code","c20f5ba9":"code","6917ee30":"code","0ea2731a":"code","2b9ea00b":"code","25ded736":"code","bcb42369":"code","a21b7c94":"code","348b6a65":"code","b417b9be":"code","80496e89":"code","41e0519e":"code","363e3011":"code","5b49008f":"code","b3712339":"code","a35beeb1":"code","a579ead9":"code","c79d3721":"code","dc6b994f":"code","508ee3b7":"markdown","1c83bdff":"markdown","440bdf35":"markdown","7dde3858":"markdown","9762b7b8":"markdown","4458fa4a":"markdown","5d3bda13":"markdown","2be010a6":"markdown","790d8022":"markdown","ed5a8bd6":"markdown","aca7cc2a":"markdown","8093404a":"markdown","1dce2a17":"markdown","6ef4dd6c":"markdown","d69bc243":"markdown"},"source":{"78fa9516":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\npd.set_option('max_rows', 300)\npd.set_option('max_columns', 300)\n\nimport os\nimport glob","1a420a1a":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'","2b0987a0":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])\/(df['bid_size2'] + df['ask_size2'])\n    return wap","d52d3099":"def log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() ","da32a1d1":"def realized_volatility(series):\n    return np.sqrt(np.sum(series**2))","bb83efe0":"def count_unique(series):\n    return len(np.unique(series))","622bfdcf":"book_train = pd.read_parquet(data_dir + \"book_train.parquet\/stock_id=15\")\nbook_train.head()","c01e137f":"def preprocessor_book(file_path):\n    df = pd.read_parquet(file_path)\n    #calculate return etc\n    df['wap'] = calc_wap(df)\n    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1'])\/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'log_return2':[realized_volatility],\n        'wap_balance':[np.mean],\n        'price_spread':[np.mean],\n        'bid_spread':[np.mean],\n        'ask_spread':[np.mean],\n        'volume_imbalance':[np.mean],\n        'total_volume':[np.mean],\n        'wap':[np.mean],\n            }\n\n    #####groupby \/ all seconds\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    \n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n        \n    ######groupby \/ last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    #create row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n    \n    return df_feature","7f6304e2":"%%time\nfile_path = data_dir + \"book_train.parquet\/stock_id=0\"\npreprocessor_book(file_path)","f94cc789":"trade_train = pd.read_parquet(data_dir + \"trade_train.parquet\/stock_id=0\")\ntrade_train.head(15)","4d3899a6":"def preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n    \n    ######groupby \/ last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n    \n    return df_feature","04eaa7c0":"%%time\nfile_path = data_dir + \"trade_train.parquet\/stock_id=0\"\npreprocessor_trade(file_path)","d4dd8f07":"def preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n     \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df\n","fcfea4da":"list_stock_ids = [0,1]\npreprocessor(list_stock_ids, is_train = True)","c58afc3f":"train = pd.read_csv(data_dir + 'train.csv')","85bdf09a":"train_ids = train.stock_id.unique()","a1998581":"%%time\ndf_train = preprocessor(list_stock_ids= train_ids, is_train = True)","e08b518f":"train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\ndf_train = train.merge(df_train, on = ['row_id'], how = 'left')","bbc91673":"df_train.head()","28473d61":"test = pd.read_csv(data_dir + 'test.csv')","d4a121f6":"test_ids = test.stock_id.unique()","c3382a09":"%%time\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)","d80bdf90":"df_test = test.merge(df_test, on = ['row_id'], how = 'left')","33296e7a":"from sklearn.model_selection import KFold\n#stock_id target encoding\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\n\nstock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \ndf_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n\n#training\ntmp = np.repeat(np.nan, df_train.shape[0])\nkf = KFold(n_splits = 10, shuffle=True,random_state = 19911109)\nfor idx_1, idx_2 in kf.split(df_train):\n    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n\n    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\ndf_train['stock_id_target_enc'] = tmp","c34508a9":"df_train.head()","ad5eaef7":"df_test.head()","2a4c1d3b":"DO_FEAT_IMP = False\nif len(df_test)==3:\n    DO_FEAT_IMP = True","b0e84750":"import lightgbm as lgbm","c20f5ba9":"# ref https:\/\/www.kaggle.com\/corochann\/permutation-importance-for-feature-selection-part1\ndef calc_model_importance(model, feature_names=None, importance_type='gain'):\n    importance_df = pd.DataFrame(model.feature_importance(importance_type=importance_type),\n                                 index=feature_names,\n                                 columns=['importance']).sort_values('importance')\n    return importance_df\n\n\ndef plot_importance(importance_df, title='',\n                    save_filepath=None, figsize=(8, 12)):\n    fig, ax = plt.subplots(figsize=figsize)\n    importance_df.plot.barh(ax=ax)\n    if title:\n        plt.title(title)\n    plt.tight_layout()\n    if save_filepath is None:\n        plt.show()\n    else:\n        plt.savefig(save_filepath)\n    plt.close()","6917ee30":"df_train['stock_id'] = df_train['stock_id'].astype(int)\ndf_test['stock_id'] = df_test['stock_id'].astype(int)","0ea2731a":"X = df_train.drop(['row_id','target'],axis=1)\ny = df_train['target']","2b9ea00b":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef feval_RMSPE(preds, lgbm_train):\n    labels = lgbm_train.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\nparams = {\n      \"objective\": \"rmse\", \n      \"metric\": \"rmse\", \n      \"boosting_type\": \"gbdt\",\n      'early_stopping_rounds': 30,\n      'learning_rate': 0.01,\n      'lambda_l1': 1,\n      'lambda_l2': 1,\n      'feature_fraction': 0.8,\n      'bagging_fraction': 0.8,\n  }","25ded736":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5, random_state=19901028, shuffle=True)\noof = pd.DataFrame()                 # out-of-fold result\nmodels = []                          # models\nscores = 0.0                         # validation score\n\ngain_importance_list = []\nsplit_importance_list = []","bcb42369":"%%time\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n\n    print(\"Fold :\", fold+1)\n    \n    # create dataset\n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    \n    #RMSPE weight\n    weights = 1\/np.square(y_train)\n    lgbm_train = lgbm.Dataset(X_train,y_train,weight = weights)\n\n    weights = 1\/np.square(y_valid)\n    lgbm_valid = lgbm.Dataset(X_valid,y_valid,reference = lgbm_train,weight = weights)\n    \n    # model \n    model = lgbm.train(params=params,\n                      train_set=lgbm_train,\n                      valid_sets=[lgbm_train, lgbm_valid],\n                      num_boost_round=5000,         \n                      feval=feval_RMSPE,\n                      verbose_eval=100,\n                      categorical_feature = ['stock_id']                \n                     )\n    \n    # validation \n    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n\n    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),3)\n    print(f'Performance of the\u3000prediction: , RMSPE: {RMSPE}')\n\n    #keep scores and models\n    scores += RMSPE \/ 5\n    models.append(model)\n    print(\"*\" * 100)\n    \n    # --- calc model feature importance ---\n    if DO_FEAT_IMP:    \n        feature_names = X_train.columns.values.tolist()\n        gain_importance_df = calc_model_importance(\n            model, feature_names=feature_names, importance_type='gain')\n        gain_importance_list.append(gain_importance_df)\n\n        split_importance_df = calc_model_importance(\n            model, feature_names=feature_names, importance_type='split')\n        split_importance_list.append(split_importance_df)","a21b7c94":"scores","348b6a65":"def calc_mean_importance(importance_df_list):\n    mean_importance = np.mean(\n        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n    mean_df = importance_df_list[0].copy()\n    mean_df['importance'] = mean_importance\n    return mean_df","b417b9be":"if DO_FEAT_IMP:\n    mean_gain_df = calc_mean_importance(gain_importance_list)\n    plot_importance(mean_gain_df, title='Model feature importance by gain')\n    mean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\n    mean_gain_df.to_csv('gain_importance_mean.csv', index=False)","80496e89":"if DO_FEAT_IMP:\n    mean_split_df = calc_mean_importance(split_importance_list)\n    plot_importance(mean_split_df, title='Model feature importance by split')\n    mean_split_df = mean_split_df.reset_index().rename(columns={'index': 'feature_names'})\n    mean_split_df.to_csv('split_importance_mean.csv', index=False)","41e0519e":"df_test.columns","363e3011":"df_train.columns","5b49008f":"y_pred = df_test[['row_id']]\nX_test = df_test.drop(['time_id', 'row_id'], axis = 1)","b3712339":"X_test","a35beeb1":"target = np.zeros(len(X_test))\n\n#light gbm models\nfor model in models:\n    pred = model.predict(X_test[X_valid.columns], num_iteration=model.best_iteration)\n    target += pred \/ len(models)","a579ead9":"y_pred = y_pred.assign(target = target)","c79d3721":"y_pred","dc6b994f":"y_pred.to_csv('submission.csv',index = False)","508ee3b7":"## Main function for preprocessing trade data","1c83bdff":"### Cross Validation","440bdf35":"## Combined preprocessor function","7dde3858":"## My approach(work in progress)\n### Feature Engineering\n\nHere are my thoughts on feature engieering with my background knowledge on financial market. \n\n - price_spread: the difference between ask price and bid price. Wide spread means low liquidity, leading to high volatility.\n - volume: the sum of the ask\/bid size. Low volume means low liquidity, leading to high volatility\n - volume_imbalance: the difference between ask size and bid size. Large imbalance means low liquidity for one side, leading to high volatility\n \nAlso, I created features only using last XX seconds to capture the dynamics of volatility further.\n\n\n### Model Building\n- optimize the weight for RMSPE: see this discussion https:\/\/www.kaggle.com\/c\/optiver-realized-volatility-prediction\/discussion\/250324\n- one model for all stocks: model by stock_id does not work well. I am afraid of overfitting as well. stock_id is used as categorical and for target mean encoding.","9762b7b8":"# Test set","4458fa4a":"## Target encoding by stock_id","5d3bda13":"## LightGBM","2be010a6":"## Test set","790d8022":"### Contribution from the commmunity\n**The codes for showing Feature Importance is kindly prepared by this expert: https:\/\/www.kaggle.com\/something4kag\nand extracted by this notebook: https:\/\/www.kaggle.com\/something4kag\/lightgbm-starter-with-fe-and-importance**","ed5a8bd6":"## Preparation","aca7cc2a":"# LightGBM starter with feature engineering idea\n***I am creating this note mostly for myself to re-organize my ideas. Please leave your comments and\/or advice if you find the room I can improve my coding\/model builing to go further.***\n\n\nWe will predict **the realized volatility of the next ten-minutes time window** with two data sets of the last ten minutes (600 seconds).One dataset contains ask and bid prices of almost each second, which allows us to calculate the realized volatility of the last ten minutes.The other dataset contains the actual record of stock trading, which is more sparse.\n\nPlease look at this notebook for the detailed explanation: https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data\n\nAs for EDA, you may find this notebook useful: https:\/\/www.kaggle.com\/chumajin\/optiver-realized-eda-for-starter-english-version\n\nThank you!","8093404a":"## Main function for preprocessing book data","1dce2a17":"## Training set","6ef4dd6c":"## Functions for preprocess","d69bc243":"## Model Building"}}