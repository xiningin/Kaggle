{"cell_type":{"14f40789":"code","64d4c43b":"code","8a24b79c":"code","32f54581":"code","275222d7":"code","7af5e8e0":"code","a195fe47":"code","4fff6148":"code","b9409121":"code","7c32f104":"code","7e80bc0e":"code","c50a5125":"code","9fd086e6":"code","7c935c98":"code","ed446499":"code","4e3be36c":"code","3e14b721":"code","a4c1ac10":"code","88dd0a75":"markdown","b8346887":"markdown","0db0edd3":"markdown","de2e5d5b":"markdown","89eca8c8":"markdown","95421363":"markdown","7a569bd8":"markdown","6b53dd80":"markdown","b183b751":"markdown","84a7b265":"markdown","2da051dd":"markdown","e184717a":"markdown","b33dc48e":"markdown","95a4fd8e":"markdown","365d21e7":"markdown","bef84c0b":"markdown","04ce556c":"markdown"},"source":{"14f40789":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport tensorflow as tf\nfrom tensorflow.python.framework import ops\nfrom tensorflow.examples.tutorials.mnist import input_data\ntf.logging.set_verbosity(tf.logging.ERROR)\n\nprint('TensorFlow version: ', tf.__version__)\n# print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","64d4c43b":"def load_data(path, seed=113, val_split=0.2):\n    \"\"\"This is a helper function to load training and test data.\n    \n    Args:\n        path: path of the files. For kaggle it's ..\/input\/\n        seed: seed value used to randomize data\n        val_split: fraction of training data to be split for validation\n    Return:\n        tuples of training, validation and test data with labels in that order\n    \"\"\"\n    # load test data into features and labels\n    test_data = pd.read_csv(path + 'fashion-mnist_test.csv')\n    x_test, y_test = np.array(test_data.iloc[:, 1:]), np.array(test_data.iloc[:, 0])\n    \n    # load training data into features and labels with a pre-defined split\n    training_data = pd.read_csv(path + 'fashion-mnist_train.csv')\n    xs = np.array(training_data.iloc[:, 1:])\n    labels = np.array(training_data.iloc[:, 0])\n    \n    # it's always a good idea to shuffle the data initially\n    np.random.seed(seed)\n    indices = np.arange(len(xs))\n    np.random.shuffle(indices)\n    xs = xs[indices]\n    labels = labels[indices]\n\n    idx = int(len(xs) * (1 - val_split))\n    x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n    x_val, y_val = np.array(xs[idx:]), np.array(labels[idx:])\n\n    return (x_train, y_train), (x_val, y_val), (x_test, y_test)","8a24b79c":"# load the data\n(train_data, train_labels), (val_data, val_labels), (test_data, test_labels) = load_data('..\/input\/')\n# print shapes to confirm that data was loaded properly\nprint(\"Training data\", train_data.shape, \"Training labels\", train_labels.shape)\nprint(\"Validation data\", val_data.shape, \"Validation labels\", val_labels.shape)\nprint(\"Test data\", test_data.shape, \"Test labels\", test_labels.shape)","32f54581":"# Create a dictionary for each type of label \nlabels = {0 : \"T-shirt\/top\", 1: \"Trouser\", 2: \"Pullover\", 3: \"Dress\", 4: \"Coat\",\n          5: \"Sandal\", 6: \"Shirt\", 7: \"Sneaker\", 8: \"Bag\", 9: \"Ankle Boot\"}\n\ndef get_classes_distribution(data):\n    # Get the count for each label\n    label_counts = data[\"label\"].value_counts()\n\n    # Get total number of samples\n    total_samples = len(data)\n\n\n    # Count the number of items in each class\n    for i in range(len(label_counts)):\n        label = labels[label_counts.index[i]]\n        count = label_counts.values[i]\n        percent = (count \/ total_samples) * 100\n        print(\"{:<20s}:   {} or {}%\".format(label, count, percent))\n\nprint(\"--------------Train Data---------------\")\nget_classes_distribution(pd.read_csv(\"..\/input\/fashion-mnist_train.csv\"))\nprint(\"--------------Test Data---------------\")\nget_classes_distribution(pd.read_csv(\"..\/input\/fashion-mnist_test.csv\"))","275222d7":"IMG_ROWS,IMG_COLS = 28,28\nNUM_CLASSES = 10\nTEST_SIZE = 0.2\nRANDOM_STATE = 2018\n#Model\nNO_EPOCHS = 50\nBATCH_SIZE = 128\n\ndef sample_images_data(data):\n    # An empty list to collect some samples\n    sample_images = []\n    sample_labels = []\n\n    # Iterate over the keys of the labels dictionary defined in the above cell\n    for k in labels.keys():\n        # Get four samples for each category\n        samples = data[data[\"label\"] == k].head(4)\n        # Append the samples to the samples list\n        for j, s in enumerate(samples.values):\n            # First column contain labels, hence index should start from 1\n            img = np.array(samples.iloc[j, 1:]).reshape(IMG_ROWS,IMG_COLS)\n            sample_images.append(img)\n            sample_labels.append(samples.iloc[j, 0])\n\n    print(\"Total number of sample images to plot: \", len(sample_images))\n    return sample_images, sample_labels","7af5e8e0":"def plot_sample_images(data_sample_images,data_sample_labels,cmap=\"Blues\"):\n    # Plot the sample images now\n    f, ax = plt.subplots(5,8, figsize=(16,10))\n\n    for i, img in enumerate(data_sample_images):\n        ax[i\/\/8, i%8].imshow(img, cmap=cmap)\n        ax[i\/\/8, i%8].axis('off')\n        ax[i\/\/8, i%8].set_title(labels[data_sample_labels[i]])\n    plt.show()    \n    \ntrain_sample_images, train_sample_labels = sample_images_data(pd.read_csv(\"..\/input\/fashion-mnist_train.csv\"))\nplot_sample_images(train_sample_images,train_sample_labels, \"Greens\")","a195fe47":"test_sample_images, test_sample_labels = sample_images_data(pd.read_csv(\"..\/input\/fashion-mnist_test.csv\"))\nplot_sample_images(test_sample_images,test_sample_labels)\nplot_sample_images(test_sample_images,test_sample_labels, \"Reds\")","4fff6148":"# pre-process data to change all the feature's shape into an array of one-hot encoded values\nTRAINING_SIZE = len(train_data)\nVAL_SIZE = len(val_data)\nTEST_SIZE = len(test_data)\n\ntrain_data = np.asarray(train_data, dtype=np.float32) \/ 255\ntrain_data = train_data.reshape((TRAINING_SIZE, 28, 28, 1))\n\nval_data = np.asarray(val_data, dtype=np.float32) \/ 255\nval_data = val_data.reshape((VAL_SIZE, 28, 28, 1))\n\ntest_data = np.asarray(test_data, dtype=np.float32) \/ 255\ntest_data = test_data.reshape((TEST_SIZE, 28, 28, 1))","b9409121":"# pre-process label data as categorical columns\nLABEL_DIMENSIONS = 10\n\ntrain_labels = tf.keras.utils.to_categorical(train_labels, LABEL_DIMENSIONS)\nval_labels = tf.keras.utils.to_categorical(val_labels, LABEL_DIMENSIONS)\ntest_labels = tf.keras.utils.to_categorical(test_labels, LABEL_DIMENSIONS)\n\n# cast the labels to floats\ntrain_labels= train_labels.astype(np.float32)\nval_labels = val_labels.astype(np.float32)\ntest_labels = test_labels.astype(np.float32)","7c32f104":"train_data.shape,train_labels.shape","7e80bc0e":"test_data.shape,test_labels.shape","c50a5125":"# Network parameters\nn_hidden_1 = 128 # Units in first hidden layer\nn_hidden_2 = 128 # Units in second hidden layer\nn_input = 784 # Fashion MNIST data input (img shape: 28*28)\nn_classes = 10 # Fashion MNIST total classes (0\u20139 digits)\nn_samples = len(train_data) # Number of examples in training set ","9fd086e6":"# Create placeholders\ndef create_placeholders(n_x, n_y):\n    '''\n    Creates the placeholders for the tensorflow session.\n\n    Arguments:\n    n_x -- scalar, size of an image vector (28*28 = 784)\n    n_y -- scalar, number of classes (10)\n\n    Returns:\n    X -- placeholder for the data input, of shape [n_x, None] and dtype \"float\"\n    Y -- placeholder for the input labels, of shape [n_y, None] and dtype \"float\"\n    '''\n\n    X = tf.placeholder(tf.float32, [n_x, None])\n    Y = tf.placeholder(tf.float32, [n_y, None])\n\n    return X, Y","7c935c98":"def initialize_parameters():\n    '''\n    Initializes parameters to build a neural network with tensorflow. The shapes are:\n                        W1 : [n_hidden_1, n_input]\n                        b1 : [n_hidden_1, 1]\n                        W2 : [n_hidden_2, n_hidden_1]\n                        b2 : [n_hidden_2, 1]\n                        W3 : [n_classes, n_hidden_2]\n                        b3 : [n_classes, 1]\n    \n    Returns:\n    parameters -- a dictionary of tensors containing W1, b1, W2, b2, W3, b3\n    '''\n    \n    # Set random seed for reproducibility\n    tf.set_random_seed(42)\n    \n    # Initialize weights and biases for each layer\n    # First hidden layer\n    W1 = tf.get_variable(\"W1\", [n_hidden_1, n_input], initializer=tf.contrib.layers.xavier_initializer(seed=42))\n    b1 = tf.get_variable(\"b1\", [n_hidden_1, 1], initializer=tf.zeros_initializer())\n    \n    # Second hidden layer\n    W2 = tf.get_variable(\"W2\", [n_hidden_2, n_hidden_1], initializer=tf.contrib.layers.xavier_initializer(seed=42))\n    b2 = tf.get_variable(\"b2\", [n_hidden_2, 1], initializer=tf.zeros_initializer())\n    \n    # Output layer\n    W3 = tf.get_variable(\"W3\", [n_classes, n_hidden_2], initializer=tf.contrib.layers.xavier_initializer(seed=42))\n    b3 = tf.get_variable(\"b3\", [n_classes, 1], initializer=tf.zeros_initializer())\n    \n    # Store initializations as a dictionary of parameters\n    parameters = {\n        \"W1\": W1,\n        \"b1\": b1,\n        \"W2\": W2,\n        \"b2\": b2,\n        \"W3\": W3,\n        \"b3\": b3\n    }\n    \n    return parameters","ed446499":"def forward_propagation(X, parameters):\n    '''\n    Implements the forward propagation for the model: \n    LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SOFTMAX\n    \n    Arguments:\n    X -- input dataset placeholder, of shape (input size, number of examples)\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\"\n                  the shapes are given in initialize_parameters\n    Returns:\n    Z3 -- the output of the last LINEAR unit\n    '''\n    \n    # Retrieve parameters from dictionary\n    W1 = parameters['W1']\n    b1 = parameters['b1']\n    W2 = parameters['W2']\n    b2 = parameters['b2']\n    W3 = parameters['W3']\n    b3 = parameters['b3']\n    \n    # Carry out forward propagation      \n    Z1 = tf.add(tf.matmul(W1,X), b1)     \n    A1 = tf.nn.relu(Z1)                  \n    Z2 = tf.add(tf.matmul(W2,A1), b2)    \n    A2 = tf.nn.relu(Z2)                  \n    Z3 = tf.add(tf.matmul(W3,A2), b3)    \n    \n    return Z3","4e3be36c":"def compute_cost(Z3, Y):\n    '''\n    Computes the cost\n    \n    Arguments:\n    Z3 -- output of forward propagation (output of the last LINEAR unit), of shape (10, number_of_examples)\n    Y -- \"true\" labels vector placeholder, same shape as Z3\n    \n    Returns:\n    cost - Tensor of the cost function\n    '''\n    \n    # Get logits (predictions) and labels\n    logits = tf.transpose(Z3)\n    labels = tf.transpose(Y)\n    \n    # Compute cost\n    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=labels))\n    \n    return cost","3e14b721":"def model(train, test, learning_rate=0.0001, num_epochs=16, minibatch_size=32, print_cost=True, graph_filename='costs'):\n    '''\n    Implements a three-layer tensorflow neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SOFTMAX.\n    \n    Arguments:\n    train -- training set\n    test -- test set\n    learning_rate -- learning rate of the optimization\n    num_epochs -- number of epochs of the optimization loop\n    minibatch_size -- size of a minibatch\n    print_cost -- True to print the cost every epoch\n    \n    Returns:\n    parameters -- parameters learnt by the model. They can then be used to predict.\n    '''\n    \n    # Ensure that model can be rerun without overwriting tf variables\n    ops.reset_default_graph()\n    # For reproducibility\n    tf.set_random_seed(42)\n    seed = 42\n    # Get input and output shapes\n    (n_x, m) = train.images.T.shape\n    n_y = train.labels.T.shape[0]\n    \n    costs = []\n    \n    # Create placeholders of shape (n_x, n_y)\n    X, Y = create_placeholders(n_x, n_y)\n    # Initialize parameters\n    parameters = initialize_parameters()\n    \n    # Forward propagation\n    Z3 = forward_propagation(X, parameters)\n    # Cost function\n    cost = compute_cost(Z3, Y)\n    # Backpropagation (using Adam optimizer)\n    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n    \n    # Initialize variables\n    init = tf.global_variables_initializer()\n    \n    # Start session to compute Tensorflow graph\n    with tf.Session() as sess:\n        \n        # Run initialization\n        sess.run(init)\n        \n        # Training loop\n        for epoch in range(num_epochs):\n            \n            epoch_cost = 0.\n            num_minibatches = int(m \/ minibatch_size)\n            seed = seed + 1\n            \n            for i in range(num_minibatches):\n                \n                # Get next batch of training data and labels\n                minibatch_X, minibatch_Y = train.next_batch(minibatch_size)\n                \n                # Execute optimizer and cost function\n                _, minibatch_cost = sess.run([optimizer, cost], feed_dict={X: minibatch_X.T, Y: minibatch_Y.T})\n                \n                # Update epoch cost\n                epoch_cost += minibatch_cost \/ num_minibatches\n                \n            # Print the cost every epoch\n            if print_cost == True:\n                print(\"Cost after epoch {epoch_num}: {cost}\".format(epoch_num=epoch, cost=epoch_cost))\n                costs.append(epoch_cost)\n        \n        # Plot costs\n        plt.figure(figsize=(16,5))\n        plt.plot(np.squeeze(costs), color='#2A688B')\n        plt.xlim(0, num_epochs-1)\n        plt.ylabel(\"cost\")\n        plt.xlabel(\"iterations\")\n        plt.title(\"learning rate = {rate}\".format(rate=learning_rate))\n        plt.savefig(graph_filename, dpi=300)\n        plt.show()\n        \n        # Save parameters\n        parameters = sess.run(parameters)\n        print(\"Parameters have been trained!\")\n        \n        # Calculate correct predictions\n        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))\n        \n        # Calculate accuracy on test set\n        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n        \n        print (\"Train Accuracy:\", accuracy.eval({X: train.images.T, Y: train.labels.T}))\n        print (\"Test Accuracy:\", accuracy.eval({X: test.images.T, Y: test.labels.T}))\n        \n        return parameters","a4c1ac10":"# train = train_data\n# test = test_data\n\n# parameters = model(train, test, learning_rate=0.0005)","88dd0a75":"### Label Encoding\n---\n\n* Create a dictionary of integer labels and their corresponding text.","b8346887":"### Running Model\n---","0db0edd3":"We need to initialise the weights and biases for each layer in our 3-layer neural network. These parameters will later be updated during training.\n\nLet\u2019s use the Xavier initialisation for our weights, and the Zero initialisation for our biases. After setting these values, we can return them in a dictionary called parameters.\n\nHere are the shapes for the weights and biases in each layer:\n\n* `W1 : [n_hidden_1, n_input]`\n* `b1 : [n_hidden_1, 1]`\n* `W2 : [n_hidden_2, n_hidden_1]`\n* `b2 : [n_hidden_2, 1]`\n* `W3 : [n_classes, n_hidden_2]`\n* `b3 : [n_classes, 1]`","de2e5d5b":"### Test Data sample Images\n---","89eca8c8":"### Train Data sample Images\n---","95421363":"### Define Model\n---","7a569bd8":"### Read Dataset and Label Encoding\n---","6b53dd80":"### Import Dataset\n---\n* The main goal of **Fashion MNIST** is to serve as a drop-in replacement for the **original MNIST**, it can be imported in the same way **MNIST is imported in Tensorflow.**\n\n### Labels\n\n![](https:\/\/pravarmahajan.github.io\/assets\/images\/fashion-MNIST\/labels_table.png)","b183b751":"#### Network parameter setting\n---","84a7b265":"![](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*vjVafYjmT1fBQCffwNadyA.png)","2da051dd":"Fashion MNIST By Zolando Research\n---\n![](https:\/\/proxy-de1.toolur.com\/browse.php?u=0HFKu9ziqlj1wV81GI10deRmWX9gr3cgHBZZ%2FYnzB1dhuGrufV2BzsCiNnVTM8AL8kPIbwFARXzLKqLtFmB3phBUFIxiwFbItu4mcqdjrrDxmP0oUjSZJosVCALDhD%2FLyfEbjViQE2xg25c3Cd3vsu%2BmugHYEs9viA%3D%3D&b=1)\n\n### Fashion MNIST with Tensorflow Only\n---\n* Fashion-MNIST is a dataset of Zalando\u2019s article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28\u00d728 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a direct drop-in replacement of the original MNIST dataset for benchmarking machine learning algorithms.\n![](https:\/\/github.com\/zalandoresearch\/fashion-mnist\/raw\/master\/doc\/img\/embedding.gif)\n\n\n\n### Why It's most interested dataset for Data Science Community?\n---\n\n\n* The original MNIST dataset contains a lot of handwritten digits. People from AI\/ML\/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset they would try on. \u201cIf it doesn\u2019t work on MNIST, it won\u2019t work at all\u201d, they said. \n> \u201cWell, if it does work on MNIST, it may still fail on others.\u201d\n\n* Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset to benchmark machine learning algorithms, as it shares the same image size and the structure of training and testing splits.\n\n\n\n### Dataset Description\n---\n\n* Fashion MNIST shares the shame train-test split structure as MNIST. Whereas in the case of MNIST dataset, the class labels were digits 0-9. The class labels for Fashion MNIST are:\n\n\n| Name  | Content | Examples | Size | Link | MD5 Checksum|\n| --- | --- |--- | --- |--- |--- |\n| `train-images-idx3-ubyte.gz`  | training set images  | 60,000|26 MBytes | [Download](http:\/\/fashion-mnist.s3-website.eu-central-1.amazonaws.com\/train-images-idx3-ubyte.gz)|`8d4fb7e6c68d591d4c3dfef9ec88bf0d`|\n| `train-labels-idx1-ubyte.gz`  | training set labels  |60,000|29 KBytes | [Download](http:\/\/fashion-mnist.s3-website.eu-central-1.amazonaws.com\/train-labels-idx1-ubyte.gz)|`25c81989df183df01b3e8a0aad5dffbe`|\n| `t10k-images-idx3-ubyte.gz`  | test set images  | 10,000|4.3 MBytes | [Download](http:\/\/fashion-mnist.s3-website.eu-central-1.amazonaws.com\/t10k-images-idx3-ubyte.gz)|`bef4ecab320f06d8554ea6380940ec79`|\n| `t10k-labels-idx1-ubyte.gz`  | test set labels  | 10,000| 5.1 KBytes | [Download](http:\/\/fashion-mnist.s3-website.eu-central-1.amazonaws.com\/t10k-labels-idx1-ubyte.gz)|`bb300cfdad3c16e7a12a480ee83cd310`|","e184717a":"### Sample Image Visualization\n---","b33dc48e":"### Forward Propogation\n---","95a4fd8e":"* Next, let\u2019s write a function which takes in some data about the input vector dimensions, and returns [Tensorflow placeholders ](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/placeholder)for our independent and dependent variables (X and Y). We will pass data while running our neural network using these placeholders.","365d21e7":"### Data Shape\n---","bef84c0b":"### Time to Build a Model\n---\n\n* Building a **3-layer Nueral Network** with **128 units** in each **hidden layer**. Both hidden layers will compute a linear function which is then passed into a **ReLU activation function**. Finally, we will use a **Softmax function** on the **output** from our network, to create **10 outputs (1 output for each target class).**\n\n","04ce556c":"### Computing Cost\n---"}}