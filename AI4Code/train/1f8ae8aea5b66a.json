{"cell_type":{"ac45fff1":"code","5f142ab4":"code","38e323ed":"code","3ed514d0":"code","0658cb58":"code","fa9dd775":"code","7c9737c7":"code","ea25a2d9":"code","6ed9ca52":"code","b621672f":"code","a5c0d4a4":"code","bfc3d85c":"code","74ff7aed":"code","abf7777b":"code","b2151112":"code","da448c7c":"code","2bcadd35":"code","3a179096":"code","70f6239a":"markdown","4e96d1fe":"markdown","26927a5f":"markdown","766ae3de":"markdown","d7ea2d12":"markdown","2ea72df0":"markdown"},"source":{"ac45fff1":"# Import packages\nimport re\nimport spacy\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib.pyplot import figure\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom wordcloud import WordCloud\n\n# Custom settings\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\n\ncat_cols = [\"#4f8c9d\", \"#94da40\", \"#7212ff\", \"#31d0a5\", \"#333a9e\", \"#b8b2f0\", \"#1c4c5e\", \"#8bd0eb\", \"#760796\", \"#39970e\"]\nsns.palplot(sns.color_palette(cat_cols))\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nsns.set_palette(cat_cols)\nsns.set_context(\"talk\", font_scale=.9)\n\n# Load data\nnlp = spacy.load('en')\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv', index_col = 0)\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv', index_col = 0)","5f142ab4":"train.head()","38e323ed":"display(train.isnull().sum().sort_values(ascending=False))","3ed514d0":"print('There are ' + str(len(train.keyword.unique())) + ' unique keywords.')","0658cb58":"#Show the top 40 keywords for taget = 1\nfigure(figsize=(16, 16))\n\nplt.subplot(1, 2, 1)\nplt1 = sns.countplot(y=\"keyword\", \n                     data=train,\n                     hue=train.target,\n                     order=train[train.target==1].keyword\n                     .value_counts()\n                     .iloc[:40].index)\nplt1.set_ylabel('')\nplt1.set_title('Top Keywords for Target = 1')\n\n#Show the top 40 keywords for taget = 0\nplt.subplot(1, 2, 2)\nplt2 = sns.countplot(y=\"keyword\", \n                     data=train,\n                     hue=train.target,\n                     order=train[train.target==0]\n                     .keyword.value_counts()\n                     .iloc[:40].index)\nplt2.set_ylabel('')\nplt2.set_title('Top Keywords for Target = 0')\n\nplt.tight_layout(pad=3.0)","fa9dd775":"print('There are ' + str(len(train.location.unique())) + ' unique locations.')","7c9737c7":"#Show the top 10 locations for taget = 1\nfigure(figsize=(16, 6))\n\nplt.subplot(1, 2, 1)\nplt1 = sns.countplot(y=\"location\", \n                     data=train,\n                     hue=train.target,\n                     order=train[train.target==1].location\n                     .value_counts()\n                     .iloc[:10].index)\nplt1.set_ylabel('')\nplt1.set_title('Top Locations for Target = 1')\nplt1.legend(loc='lower right')\n\n#Show the top 10 locations for taget = 0\nplt.subplot(1, 2, 2)\nplt2 = sns.countplot(y=\"location\", \n                     data=train,\n                     hue=train.target,\n                     order=train[train.target==0].location\n                     .value_counts()\n                     .iloc[:10].index)\nplt2.set_ylabel('')\nplt2.set_title('Top Locations for Target = 0')\nplt2.legend(loc='lower right')\n\nplt.tight_layout(pad=3.0)","ea25a2d9":"figure(figsize=(8, 6))\ntrain[\"loc_bool\"] = [\"No Location\" if pd.isnull(x) else \"Location\" for x in train[\"location\"]]\n\nplt1 = sns.countplot(x=\"loc_bool\", \n                     data=train,\n                     hue=train.target)\nplt1.set_ylabel('')\nplt1.set_xlabel('')\nplt1.set_title('');","6ed9ca52":"print('These are examples of disaster tweets: \\n')\nprint(train[train.target==1]['text'][1:20])\n\nprint('\\n')\n\nprint('These are examples of non-disaster tweets: \\n')\nprint(train[train.target==0]['text'][1:20])","b621672f":"# Add boolean variable for URL\ntrain[\"URL\"] = train.text.str.match(r'[a-z]*[:.]+\\S+', '')\n\n# Remove URLs from tweets\ntrain.text = [re.sub(r'[a-z]*[:.]+\\S+', '', x) for x in train.text]\ntrain.text = [re.sub(r'\\&amp', '', x) for x in train.text]\n\n# Create bar chart\nfigure(figsize=(8, 6))\n\ndf_p1 = train[[\"target\", \"URL\"]].groupby([\"target\"]).mean().reset_index()\nplt1 = sns.barplot(x=\"target\", y=\"URL\", data=df_p1)\n\nvals = plt1.get_yticks()\nplt1.set_yticklabels(['{:,.1%}'.format(x) for x in vals])\nplt1.set_ylabel('')\nplt1.set_xlabel('')\nplt1.set_title('Percent of Tweets with URLs');","a5c0d4a4":"train[\"CAPS\"] = [True if x == x.lower() else False for x in train.text]\n\nfigure(figsize=(8, 6))\n\ndf_p1 = train[[\"target\", \"CAPS\"]].groupby([\"target\"]).mean().reset_index()\n\nplt1 = sns.barplot(x=\"target\",\n                   y=\"CAPS\",\n                   data=df_p1)\n\n\nvals = plt1.get_yticks()\nplt1.set_yticklabels(['{:,.1%}'.format(x) for x in vals])\nplt1.set_ylabel('')\nplt1.set_xlabel('')\nplt1.set_title('Percent of Capitalized Tweets');","bfc3d85c":"train[\"Exclaim\"] = [True if '!' in x else False for x in train.text]\n\nfigure(figsize=(8, 6))\n\ndf_p1 = train[[\"target\", \"Exclaim\"]].groupby([\"target\"]).mean().reset_index()\n\nplt1 = sns.barplot(x=\"target\",\n                   y=\"Exclaim\",\n                   data=df_p1)\n\n\nvals = plt1.get_yticks()\nplt1.set_yticklabels(['{:,.1%}'.format(x) for x in vals])\nplt1.set_ylabel('')\nplt1.set_xlabel('')\nplt1.set_title('Percent of Exclamations');","74ff7aed":"#Setting up corpora for explorary analysis of text\ntext_TRUE = nlp(train[train['target']==1]['text'].str.cat(sep=' '))\ntext_FALSE = nlp(train[train['target']==0]['text'].str.cat(sep=' '))","abf7777b":"# Remove stop words, punctuation, and spaces\nall_TRUE = [token.text.lower() for token in text_TRUE\n            if token.is_stop != True \n            and token.is_punct != True \n            and token.text != ' ' \n            and token.text != '  '\n            and token.text != '\\n' \n            and token.text != '\\n\\n']\n\nall_FALSE = [token.text.lower() for token in text_FALSE \n             if token.is_stop != True \n             and token.is_punct != True \n             and token.text != ' ' \n             and token.text != '  '\n             and token.text != '\\n'\n             and token.text != '\\n\\n']\n\n# Create subsets that include only nouns or only verbs\nnouns_TRUE = [token.text.lower() for token in text_TRUE if token.pos_ == \"NOUN\"]\nnouns_FALSE = [token.text.lower() for token in text_FALSE if token.pos_ == \"NOUN\"]\n\nverbs_TRUE = [token.text.lower() for token in text_TRUE if token.pos_ == \"VERB\"]\nverbs_FALSE = [token.text.lower() for token in text_FALSE if token.pos_ == \"VERB\"]","b2151112":"# Find the most common words, nouns, and verbs\ncommon_all_TRUE = pd.DataFrame(Counter(all_TRUE).most_common(20), columns = [\"Word\", \"Frequency\"])\ncommon_all_FALSE = pd.DataFrame(Counter(all_FALSE).most_common(20), columns = [\"Word\", \"Frequency\"])\ncommon_nouns_TRUE = pd.DataFrame(Counter(nouns_TRUE).most_common(20), columns = [\"Word\", \"Frequency\"])\ncommon_nouns_FALSE = pd.DataFrame(Counter(nouns_FALSE).most_common(20), columns = [\"Word\", \"Frequency\"])\ncommon_verbs_TRUE = pd.DataFrame(Counter(verbs_TRUE).most_common(20), columns = [\"Word\", \"Frequency\"])\ncommon_verbs_FALSE = pd.DataFrame(Counter(verbs_FALSE).most_common(20), columns = [\"Word\", \"Frequency\"])","da448c7c":"figure(figsize=(16, 6))\n\nplt.subplot(1, 2, 1)\np1=sns.barplot(x=common_all_TRUE.Frequency, y=common_all_TRUE.Word);\n\nplt.subplot(1, 2, 2)\np2=sns.barplot(x=common_all_FALSE.Frequency, y=common_all_FALSE.Word);\n\np1.set_title('Target = 1');\np2.set_title('Target = 0');\np2.set_ylabel('');","2bcadd35":"figure(figsize=(16, 6))\n\nplt.subplot(1, 2, 1)\np1=sns.barplot(x=common_nouns_TRUE.Frequency, y=common_nouns_TRUE.Word);\n\nplt.subplot(1, 2, 2)\np2=sns.barplot(x=common_nouns_FALSE.Frequency, y=common_nouns_FALSE.Word);\n\np1.set_title('Target = 1');\np2.set_title('Target = 0');\np2.set_ylabel('');","3a179096":"figure(figsize=(16, 6))\n\nplt.subplot(1, 2, 1)\np1=sns.barplot(x=common_verbs_TRUE.Frequency, y=common_verbs_TRUE.Word);\n\nplt.subplot(1, 2, 2)\np2=sns.barplot(x=common_verbs_FALSE.Frequency, y=common_verbs_FALSE.Word);\n\np1.set_title('Target = 1');\np2.set_title('Target = 0');\np2.set_ylabel('');","70f6239a":"## URL\n\nIt appears as if tweets with URLs are slightly more likely to be disaster-related tweets.","4e96d1fe":"# ALL CAPS","26927a5f":"# Tweets ","766ae3de":"# Keywords","d7ea2d12":"# Location\n\nMost tweets in the training data do not have a location. Of the ones that do, location does not appear to be a good predictor of the target variable. Moreover, while location might be useful during training (if a disaster occurred in a particular location), it is probably not generalizable and therefore should not be considered in a model.","2ea72df0":"## Text"}}