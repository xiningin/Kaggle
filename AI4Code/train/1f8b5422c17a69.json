{"cell_type":{"27cd6d38":"code","0481d391":"code","46aef363":"code","3f34e45f":"code","26468b20":"code","97d78a46":"code","df60597d":"code","3aa1308e":"code","b0fe6928":"code","efeb063b":"code","9fd0dcb1":"code","80b56a1f":"code","370dfadb":"code","5447b34a":"code","4cb69975":"code","6247f16f":"code","636d8bf6":"code","f9882c15":"code","80252198":"code","77070dd4":"code","801bbb56":"code","3cc242b4":"code","b82c6e09":"code","55ebc3e6":"code","d0031bff":"code","65a6bf2f":"code","57332d62":"code","5d1d2f6f":"code","f5a2c4e8":"code","3dca5d95":"code","be690e57":"code","f1d58009":"code","71032fb8":"code","8575f412":"code","30560c20":"code","728edc25":"code","73a05428":"code","de2a660c":"code","8b5c5615":"code","7ff57d05":"code","e0fac037":"code","3b522a55":"code","6418f750":"code","f0c89752":"code","ac3c485f":"code","f1508203":"code","d0103ad7":"markdown","8cd968d5":"markdown","fa1e41a3":"markdown","3554c025":"markdown","6c85271a":"markdown","b8d4e163":"markdown","59e110af":"markdown","34d3c7e6":"markdown","671ffaf4":"markdown","76f6ee69":"markdown","9e7662b1":"markdown","c09eb84d":"markdown","9264b0e2":"markdown","a3016691":"markdown","588f214d":"markdown","687bb291":"markdown","f3bcc295":"markdown","71d48cc8":"markdown","a5d151de":"markdown","d01630db":"markdown","29a2297f":"markdown","a1f31b59":"markdown","3f57ddbb":"markdown","14aa765a":"markdown","df96c56c":"markdown","59512a84":"markdown"},"source":{"27cd6d38":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0481d391":"#import library function\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom sklearn.model_selection import  train_test_split\n\nfrom sklearn.metrics import r2_score,mean_squared_error\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, GridSearchCV\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n\n#library for modelling\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge","46aef363":"# import training data frame\n\nprint(\"Training Data Frame\")\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_train.head()\n","3f34e45f":"# import testing data frame\nprint(\"Testing Data Frame\")\ndf_test  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\ndf_test.head()","26468b20":"#Displaying Metadata\/Information about Training and Testing Data\ndf_train.info()","97d78a46":"df_test.info()","df60597d":"#CHECK THE MISSING TRAINING DATA\n#Checking columns with missing data\nMissing_Percent=100*(df_train.isnull().sum()\/len(df_train))\n\n#Sorting the data columns by their percentage in descending order\nMissing_Percent=Missing_Percent[Missing_Percent>0].sort_values(ascending=False).round(1)\n\n#Creating a dataframe to show percentage of missing data and its respective data column in table\nDataFrame=pd.DataFrame(Missing_Percent)\nmissing_percent_table=DataFrame.rename(columns={0:'% of Missing Values'})\nMissingPercent=missing_percent_table\n\n#Displaying Missing Value table\nMissingPercent","3aa1308e":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=Missing_Percent.index, y=Missing_Percent)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","b0fe6928":"print(\"The train data size before dropping Id feature is : {} \".format(df_train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(df_test.shape))","efeb063b":"df_train.drop('Id', axis = 1, inplace = True)\ndf_test.drop('Id', axis = 1, inplace = True)","9fd0dcb1":"print(\"\\nThe train data size after dropping Id feature is : {} \".format(df_train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(df_test.shape))","80b56a1f":"fig, ax = plt.subplots()\nax.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","370dfadb":"#Remove Outliers\ndf_train= df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)","5447b34a":"#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(df_train['GrLivArea'], df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","4cb69975":"df_train['SalePrice'].describe()","6247f16f":"#histogram\nsns.distplot(df_train['SalePrice'])","636d8bf6":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(df_train['SalePrice'])","f9882c15":"print(\"Skewness: %f\" % df_train['SalePrice'].skew())","80252198":"df_train_pearson_correlation = df_train.corr(method='pearson')\nfig = go.Figure(data=go.Heatmap(\n                   x=df_train_pearson_correlation.columns,\n                   y=df_train_pearson_correlation.index,\n                   z=df_train_pearson_correlation.values,\n                   name='pearson',showscale=True,xgap=1,ygap=1,\n                   colorscale='Blackbody'))\nfig.update_layout(height=700, width=900, title_text=\"<b>Pearson Correlation<b>\")\nfig.show()","77070dd4":"#Based on the Heat Map, one can see that Sale Price has a high correlation with dependent variables such as Lot Frontage, OverallQual, GrLivArea,\n\ndf_train_corr = df_train.corr(method='pearson')[\"SalePrice\"].sort_values(ascending = False)\ndf_train_corr","801bbb56":"cor1 = df_train.corr(method = 'pearson')\ncor1_features = cor1.index[cor1['SalePrice'] > 0.5]\nplt.figure(figsize=(15,10))\nsns.heatmap(df_train[cor1_features].corr(),annot = True)\ncor1_features","3cc242b4":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 3)\nplt.show()","b82c6e09":"print(\"Data Visualisation - Overall Qual\")\n\nfig1 = px.scatter(df_train, x=\"OverallQual\", y=\"SalePrice\", marginal_x=\"histogram\", marginal_y=\"rug\")\nfig1.show()\n\nfig2 = px.box(df_train, y=\"OverallQual\")\nfig2.show()\n\nprint(\"Data Visualisation - GrLiv Arena\")\nfig3 = px.scatter(df_train, x=\"GrLivArea\", y=\"SalePrice\",marginal_x=\"histogram\", marginal_y=\"rug\")\nfig3.show()\n\nfig4 = px.box(df_train, y=\"GrLivArea\")\nfig4.show()\n\nprint(\"Data Visualisation - Garage Cars\")\n\nfig5 = px.scatter(df_train, x=\"GarageCars\", y=\"SalePrice\",marginal_x=\"histogram\", marginal_y=\"rug\")\nfig5.show()\n\nfig6 = px.box(df_train, y=\"GarageCars\")\nfig6.show()\n\nprint(\"Data Visualisation - Garage Area \")\n\nfig7 = px.scatter(df_train, x=\"GarageArea\", y=\"SalePrice\",marginal_x=\"histogram\", marginal_y=\"rug\")\nfig7.show()\n\nfig8 = px.box(df_train, y=\"GarageArea\")\nfig8.show()","55ebc3e6":"ntrain = df_train.shape[0]\nntest = df_test.shape[0]\ny_train = df_train.SalePrice.values\nprint(\"y_train shape is : {}\".format(y_train.shape))\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","d0031bff":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","65a6bf2f":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\nall_data = all_data.drop(['Utilities'], axis=1)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","57332d62":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","5d1d2f6f":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","f5a2c4e8":"#Label Encoding some categorical variables that may contain information in their ordering set\nfrom sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","3dca5d95":"# Adding total sqfootage feature which includes the 1st floor and the 2nd floor of the house \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","be690e57":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","f1d58009":"x_train = all_data[:ntrain]\nx_test = all_data[ntrain:]\nprint(\"x_train shape is : {}\".format(x_train.shape))\nprint(\"x_test shape is : {}\".format(x_test.shape))\nprint(\"y_train shape is: {}\".format(y_train.shape))","71032fb8":"from sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x_train, y_train,test_size = .3, random_state=0)\nprint(\"x_train shape is : {}\".format(X_train.shape))\nprint(\"x_test shape is : {}\".format(X_test.shape))\nprint(\"y_train shape is: {}\".format(Y_train.shape))\nprint(\"y_test shape is: {}\".format(Y_test.shape))","8575f412":"## Call in the LinearRegression object\nlin_reg = LinearRegression(normalize=True, n_jobs=-1)\n## fit train and test data. \nlin_reg.fit(x_train, y_train)\n## Predict test data. \ny_train_pred = lin_reg.predict(X_train)\ny_test_pred = lin_reg.predict(X_test)\nprint(\"y_train shape is : {}\".format(y_train.shape))\nprint(\"x_train shape is : {}\".format(x_train.shape))\n\n## get average squared error(MSE) by comparing predicted values with real values. \nprint ('RMSE for Train data %.4f'%np.sqrt(mean_squared_error(Y_train, y_train_pred)))\nprint ('RMSE for Test data %.4f'%np.sqrt(mean_squared_error(Y_test, y_test_pred)))","30560c20":"# parity plot  \nplt.scatter(y_train_pred,Y_train,color='blue')\nplt.title('Linear Regression')\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.plot([10.5,13.5],[10.5,13.5],c='red')\nplt.show()","728edc25":"#Validation function\nn_folds = 10\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(x_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, Y_train, scoring=\"neg_mean_squared_error\", cv = 5))\n    #scores = cross_val_score(model, x_train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf)\n    return(rmse)\n# rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","73a05428":"#SET-UP ALPHA VALUES\n\nalphas_alt = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","de2a660c":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.metrics import mean_squared_error\n\nridge=Ridge()\nparameters= {'alpha':[x for x in alphas_alt]}\n\nridge_reg=GridSearchCV(ridge, param_grid=parameters)\nridge_reg.fit(X_train,Y_train)\nprint(\"The best value of Alpha is: \",ridge_reg.best_params_,ridge_reg.best_score_)\n\ncv_ridge_mean_list =[]\ncv_ridge_std_list =[]\nfor alpha in alphas_alt:\n    ridge_reg = rmsle_cv(Ridge(alpha = alpha))\n    print(\"The alphas is : {}\".format(alpha))\n    print(\"Lasso Score mean is {:.4f}\\n\".format(ridge_reg.mean()))\n    print(\"Lasso Score std is {:.4f}\\n\".format(ridge_reg.std()))\n    cv_ridge_mean_list.append(ridge_reg.mean())\n    cv_ridge_std_list.append(ridge_reg.std())\n\n\ncv_ridge_mean = pd.Series(cv_ridge_mean_list, index = alphas_alt)\ncv_ridge_std = pd.Series(cv_ridge_std_list, index = alphas_alt)\ncv_ridge_mean.plot(title = \"Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmsle\")\nprint(\"\\nRdige score: {:.4f} ({:.4f})\\n\".format(cv_ridge_mean.min(), cv_ridge_std.min()))","8b5c5615":"lasso=Lasso()\nparameters= {'alpha':[x for x in alphas2]}\n\nlasso_reg=GridSearchCV(lasso, param_grid=parameters)\nlasso_reg.fit(X_train,Y_train)\nprint(\"The best value of Alpha is: \",lasso_reg.best_params_,lasso_reg.best_score_)\n\ncv_lasso_mean_list =[]\ncv_lasso_std_list =[]\nfor alpha in alphas2:\n    lasso_reg = rmsle_cv(Lasso(alpha = alpha))\n    print(\"The alphas is : {}\".format(alpha))\n    print(\"Lasso Score mean is {:.4f}\\n\".format(lasso_reg.mean()))\n    print(\"Lasso Score std is {:.4f}\\n\".format(lasso_reg.std()))\n    cv_lasso_mean_list.append(lasso_reg.mean())\n    cv_lasso_std_list.append(lasso_reg.std())\n\ncv_lasso_mean = pd.Series(cv_lasso_mean_list, index = alphas2)\ncv_lasso_std = pd.Series(cv_lasso_std_list, index = alphas2)\ncv_lasso_mean.plot(title = \"Validation\")\nplt.xlabel(\"alpha\")\nplt.ylabel(\"rmsle\")\n\nprint(\"\\nLassoscore: {:.4f} ({:.4f})\\n\".format(cv_lasso_mean.min(), cv_ridge_std.min()))","7ff57d05":"Lasso_model =Lasso(alpha=0.0005)\nLasso_model.fit(x_train,y_train)\ny_pred_train=Lasso_model.predict(X_train)\ny_pred_test=Lasso_model.predict(X_test)\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(Y_train, y_pred_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(Y_test, y_pred_test))))","e0fac037":"coef = pd.Series(Lasso_model.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coef != 0)) + \" variables and eliminated the other \" +  str(sum(coef == 0)) + \" variables\")","3b522a55":"imp_coef = pd.concat([coef.sort_values().head(10),\n                     coef.sort_values().tail(10)])\nplt.rcParams['figure.figsize'] = (8.0, 10.0)\nimp_coef.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")","6418f750":"#let's look at the residuals as well:\nplt.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":Lasso_model.predict(X_train), \"true\":Y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")","f0c89752":"lasso_preds = np.expm1(Lasso_model.predict(X_test))\nlasso_train = np.expm1(Lasso_model.predict(X_train))\nprint(rmsle(lasso_preds, Y_test))\n\nprint('Root Mean Square Error train = ' + str(np.sqrt(mean_squared_error(Y_train, lasso_train))))\nprint('Root Mean Square Error test = ' + str(np.sqrt(mean_squared_error(Y_test, lasso_preds))))","ac3c485f":"y_test=Lasso_model.predict(x_test)\npredictions=np.expm1(y_test)","f1508203":"submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nprediction = pd.DataFrame(predictions)\nfinal_submission = pd.DataFrame({'Id':submission['Id'],'SalePrice':predictions})\n\nfinal_submission.dropna(inplace=True)\n\nfinal_submission['Id']=final_submission['Id'].astype(int)\n\nfinal_submission.to_csv('submission1.csv', index=False)\n\nfinal_submission.head()","d0103ad7":"# ANALYSIS OF DEPENDENT VARIABLE - SALE PRICE\n\nSkewness measures the asymmetry of the normal distribution.\n\n* When the bell curve moves to the left or the right, then distribution is said to be Skewed.\n* If the distributon is negative, then the curve moves to left.\n* If the distribution is positve, then the curve moves to right.\n\n","8cd968d5":"The alpha score is very high, which does not allow to the model to capture all the complexity in the data.","fa1e41a3":"SIMPLE REGRESSION MODEL","3554c025":"# MODEL PREDICTION","6c85271a":"Convert Categorical Data into Numerical Data","b8d4e163":"# EXPLORATORY DATA ANALYSIS\n\nFinding the correlation between the independent variable and the dependent variable\n","59e110af":"# OUTLIER","34d3c7e6":"DEVELOPMENT & VALIDATION OF LASSO AND RIDGE REGRESSION","671ffaf4":"# IMPORT TRAINING & TESTING DATA FRAME","76f6ee69":"# IMPUTE the missing Data\n\n* PoolQC : data description says NA means \"No Pool\". That make sense, given the huge ratio of missing value (+99%) and majority of houses have no Pool at all in general.\n* MiscFeature : data description says NA means \"no misc feature\"\n* Alley : data description says NA means \"no alley access\"\n* Fence : data description says NA means \"no fence\"\n* FireplaceQu : data description says NA means \"no fireplace\"\n* LotFrontage : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.\n* GarageType, GarageFinish, GarageQual and GarageCond : Replacing missing data with None\n* GarageYrBlt, GarageArea and GarageCars : Replacing missing data with 0 (Since No garage = no cars in such garage.)\n* BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath : missing values are likely zero for having no basement\n* BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2 : For all these categorical basement-related features, NaN means that there is no basement.\n* MasVnrArea and MasVnrType : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.\n* MSZoning (The general zoning classification) : 'RL' is by far the most common value. So we can fill in missing values with 'RL'\n* Utilities : For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.\n* Functional : data description says NA means typical\n* Electrical : It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\n* KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\n* Exterior1st and Exterior2nd : Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\n* SaleType : Fill in again with most frequent which is \"WD\"\n* MSSubClass : Na most likely means No building class. We can replace missing values with None","9e7662b1":"JOIN THE TRAIN AND TEST DATA\n\n","c09eb84d":"The Skew has been corrected.\nThe graph appears to be normally distributed.","9264b0e2":"# DEVELOP REGRESSION MODEL","a3016691":"# FEATURE ENGINEERING","588f214d":"# CHECK THE MISSING DATA AND REMOVE THE VARIABLE WHICH MIGHT NOT BE REQUIRED","687bb291":"# IMPORT THE REQUIRED LIBRARY","f3bcc295":"Based on the above heatmap, one can observe that the significant INDEPENDENT variable are as follow: \n\n* OverallQual \n* YearBuilt \n* YearRemodAdd \n* TotalBsmtSF \n* 1stFlrSF \n* GrLivArea \n* FullBath \n* TotRmsAbvGrd \n* GarageYrBlt \n* GarageCars \n* GarageArea","71d48cc8":"Let us use the lasso model for the predcitions and look at the RMSE for train and test data and submit the predcitions","a5d151de":"The alpha for the Lasso is 0.0005, which gives a better value than before","d01630db":"From the above graph, we can observe that the distribution:\u00b6\nDeviate from the normal distribution\nHave positive skewness\nShow peakedness\nThe target variable is right skewed. As (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","29a2297f":"RIDGE REGRESSION","a1f31b59":"CREATE NEW TRAIN AND TEST DATA","3f57ddbb":"# CONVERT THE CATEGORICAL DATA INTO DUMMY VARIABLE","14aa765a":"The right side of the scatter plot displays the outliers. These outliers must be eliminated.","df96c56c":"PARITY PLOT","59512a84":"LASSO REGRESSION"}}