{"cell_type":{"b330ab20":"code","d2b003aa":"code","06acf1db":"code","1522af05":"code","997426ee":"code","36e356d1":"code","9c283ceb":"code","72ecbe57":"code","395d1c9a":"code","e008a0e4":"code","9849b43d":"code","a6c0d351":"code","0d4eb1f3":"code","4d5bf662":"code","dfdb0fc2":"code","6b97f103":"markdown","7c79baa5":"markdown","e89c3a24":"markdown","549973a5":"markdown","ed49f483":"markdown","97db47c6":"markdown","3aeb240f":"markdown","d4339b9c":"markdown","43bd95b9":"markdown","f3deda7f":"markdown","2f424224":"markdown","e88f2a15":"markdown","37226b55":"markdown"},"source":{"b330ab20":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d2b003aa":"df = pd.read_csv(\"..\/input\/weatherAUS.csv\")\ndf.head()","06acf1db":"df.info()","1522af05":"wind_directions_list = df.WindGustDir\nlocations_list = df.Location","997426ee":"df.drop([\"Date\", \"Location\", \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"], axis=1, inplace=True)\ndf.head()","36e356d1":"df.RainToday = [1 if each==\"Yes\" else 0 for each in df.RainToday]\ndf.RainTomorrow = [1 if each==\"Yes\" else 0 for each in df.RainTomorrow]\ndf.head()","9c283ceb":"# completing missing datas\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\nmissingData = df.iloc[:,0:].values\nimputer = imputer.fit(missingData)               # missingDatas[:,0:3]\ncompletedData = imputer.transform(missingData)   # missingDatas[:,0:3]\n\ndf.iloc[:, 0:] = completedData\ndf.head(10)","72ecbe57":"wind_directions_list = wind_directions_list.fillna(\"UNKNOWN\")\nwind_directions_list.tail(10)","395d1c9a":"location_names=[]\nfor each in locations_list:\n    if not each in location_names:\n        location_names.append(each)\nlocation_names.sort()\n\nWindGustDir_names=[]\nfor each in wind_directions_list:\n    if not each in WindGustDir_names:\n        WindGustDir_names.append(each)\nWindGustDir_names.sort()","e008a0e4":"WindGustDir = wind_directions_list.values.reshape(-1,1)\nlocations = locations_list.values.reshape(-1,1)\n\nlabelEncoder = preprocessing.LabelEncoder()\n\nlocations[:, 0] = labelEncoder.fit_transform(locations[:, 0])\nWindGustDir[:, 0] = labelEncoder.fit_transform(WindGustDir[:, 0])\n\noneHotEncoder = preprocessing.OneHotEncoder(categorical_features='all')\n\nlocations = oneHotEncoder.fit_transform(locations).toarray()\nWindGustDir = oneHotEncoder.fit_transform(WindGustDir).toarray()\n\ndfLocations = pd.DataFrame(data=locations, index=range(145460), columns=location_names)\ndfWindGustDir = pd.DataFrame(data=WindGustDir, index=range(145460), columns=WindGustDir_names)\n\ndf_with_wind_dir = pd.concat([dfLocations, df, dfWindGustDir], axis=1)\ndf_without_wind_dir = pd.concat([dfLocations, df], axis=1)\n\nprint(df_with_wind_dir.tail())\nprint(df_without_wind_dir.tail())","9849b43d":"y_no_dir = df_without_wind_dir.RainTomorrow.values.reshape(-1,1)\nx_data_no_dir = df_without_wind_dir.drop([\"RainTomorrow\"], axis=1)\nx_no_dir = (x_data_no_dir - np.min(x_data_no_dir))\/(np.max(x_data_no_dir)-np.min(x_data_no_dir)).values\n\ny_dir = df_with_wind_dir.RainTomorrow.values.reshape(-1,1)\nx_data_dir = df_with_wind_dir.drop([\"RainTomorrow\"], axis=1)\nx_dir = (x_data_dir - np.min(x_data_dir))\/(np.max(x_data_dir)-np.min(x_data_dir)).values","a6c0d351":"from sklearn.model_selection import train_test_split\nx_train_no_dir, x_test_no_dir, y_train_no_dir, y_test_no_dir = train_test_split(x_no_dir, y_no_dir, test_size = 0.2, random_state=42)\nx_train_dir, x_test_dir, y_train_dir, y_test_dir = train_test_split(x_dir, y_dir, test_size = 0.2, random_state=42)","0d4eb1f3":"def initialize_weights_and_bias(dimension):\n    w=np.full((dimension, 1), 0.01)\n    b=0.0\n    return w,b\n\n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n\n\ndef forward_backward_propagation(w, b, x_train, y_train):\n    # forward propagation\n    z=np.dot(w.T, x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_head)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train, ((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost, gradients\n\n\ndef update(w, b, x_train, y_train, learning_rate,number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iteration):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n\n\n # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n\n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension = x_train.shape[0]  \n    w, b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    return 100 - np.mean(np.abs(y_prediction_test - y_test)) * 100    ","4d5bf662":"acc_dir = logistic_regression(x_train_dir.T, y_train_dir.T, x_test_dir.T, y_test_dir.T,learning_rate = 2, num_iterations = 300)\nacc_no_dir = logistic_regression(x_train_no_dir.T, y_train_no_dir.T, x_test_no_dir.T, y_test_no_dir.T,learning_rate = 2, num_iterations = 300)\n\nprint(\"accuracy without wind direction: % {:6.3f}\".format(acc_no_dir))\nprint(\"accuracy with wind direction: % {:6.3f}\".format(acc_dir))","dfdb0fc2":"from sklearn.linear_model import LogisticRegression\n\nlr_dir=LogisticRegression()\nlr_dir.fit(x_train_dir, y_train_dir)\n\nlr_no_dir=LogisticRegression()\nlr_no_dir.fit(x_train_no_dir, y_train_no_dir)\n\nacc_no_dir = lr_no_dir.score(x_test_no_dir, y_test_no_dir)*100\nprint(\"test accuracy without wind direction: % {:6.3f}\".format(acc_no_dir))\n\nacc_dir = lr_dir.score(x_test_dir, y_test_dir)*100\nprint(\"test accuracy with wind direction: % {:6.3f}\".format(acc_dir))","6b97f103":"- Veri setinin makina \u00f6\u011frenmesi algoritmas\u0131na sokulabilmesi i\u00e7in veri setindeki \u00f6zelliklerin say\u0131sal formatta olmas\u0131 gerekmektedir. Bu formatta olmayan \u00f6zellikler ya say\u0131sal veriye \u00e7evirilmeli ya da kullan\u0131lmamal\u0131d\u0131r. Yukar\u0131daki sat\u0131rlarda R\u00fczgar g\u00fcl\u00fc y\u00f6n\u00fc ve lokasyon bilgisi say\u0131sal veriye \u00e7evrilmek i\u00e7in se\u00e7ildi. Ard\u0131ndan kullan\u0131lmayacak di\u011fer string \u00f6zellikler veri setinden silindi. ","7c79baa5":"- sklearn k\u00fct\u00fcphane i\u00e7erisinde haz\u0131r \u015fekilde bulunan logistic regreston fonksiyonu kullan\u0131larak iki veri seti e\u011fitildi ve test edildi. Do\u011fuluk oranlar\u0131 bast\u0131r\u0131ld\u0131.","e89c3a24":"- Yukar\u0131da makina \u00f6\u011frenmesi k\u00fct\u00fcphanesi kullan\u0131lmadan basit matematiksel y\u00f6ntemler ile logistic regression fonksiyonlar\u0131 tan\u0131mland\u0131.","549973a5":"- K\u00fct\u00fcphaneler ve veri seti import edildi.","ed49f483":"- Makina \u00f6\u011frenmesi algoritmas\u0131 i\u00e7in input ve output verileri ayr\u0131ld\u0131. Ard\u0131ndan bu veriler rastgele bir \u015fekilde test ve train verisi olarak tekrardan ayr\u0131ld\u0131.","97db47c6":"- Yukar\u0131da veri seti ile ilgili bilgiler yer almaktad\u0131r. Baz\u0131 sat\u0131rlarda eksik veriler oldu\u011fu g\u00f6r\u00fclmektedir.","3aeb240f":"- R\u00fczgar g\u00fcl\u00fcn\u00fcn y\u00f6n\u00fcn\u00fc i\u00e7eren listedeki eksik veriler \"bilinmiyor\" olarak dolduruluyor.","d4339b9c":"- Her s\u00fctundaki verilerin ortamas\u0131 al\u0131n\u0131p o s\u00fctundaki bo\u015fluklar\u0131 yaz\u0131l\u0131yor. B\u00f6ylece eksik veriler tamamlanm\u0131\u015f oldu.","43bd95b9":"- Yukar\u0131 kategorik veri tipi olan lokasyon ve r\u00fczgar g\u00fcl\u00fc y\u00f6n\u00fc \u00f6zellikler label ve oneHot encoder metodlar\u0131 kullan\u0131larak say\u0131sal veriye \u00e7evrildi.\n- Ard\u0131ndan say\u0131sal veriye \u00e7evrilen bu iki \u00f6zellikle ana dataframe ile birle\u015ftirildi. Burada r\u00fczgar g\u00fcl\u00fc y\u00f6n\u00fc \u00f6zelli\u011fini i\u00e7eren ve i\u00e7ermeyen iki farkl\u0131 veri seti olu\u015fturuldu.","f3deda7f":"- \u0130ki farkl\u0131 veri seti manuel olarak yaz\u0131lan logistic regresyon algoritmas\u0131na sokuldu ve do\u011fruluk oranlar\u0131 yazd\u0131r\u0131ld\u0131.","2f424224":"**CONCLUSION**\n\n- Sklearn k\u00fct\u00fcphanesi kullan\u0131ld\u0131\u011f\u0131nda ekstra bir \u00f6zelli\u011fin gelmesi do\u011fruluk oran\u0131nda ufak miktarda artt\u0131rm\u0131\u015ft\u0131r. \n- Do\u011fruluk oran\u0131n\u0131n y\u00fczde 90'larda kalmas\u0131n\u0131n en \u00f6nemli nedeni veri setinde eksik verilerin bulunmas\u0131 olabilir. Ayr\u0131ca bu eksik verileri tamamlamak i\u00e7in kullan\u0131lan y\u00f6ntemleri de\u011fi\u015ftirerek de do\u011fruluk pay\u0131n\u0131 artt\u0131rmak m\u00fcmk\u00fcnd\u00fcr.","e88f2a15":"- Evet ve hay\u0131r \u015feklinde boolean de\u011fere sahip iki s\u00fctun 1 ve 0 olacak \u015fekilde say\u0131sal veriye \u00e7evrildi.","37226b55":"**INTRODUCTION**\n\nBu kernelde Avustralya hakk\u0131ndaki hava verileri kullan\u0131larak bir \u00f6nceki g\u00fcn\u00fcn verileri sayesinde ya\u011fmur ya\u011f\u0131p ya\u011fmayaca\u011f\u0131 tahmin edilmeye \u00e7al\u0131\u015f\u0131lacak."}}