{"cell_type":{"f76a5a2d":"code","58eb50b3":"code","02ce8b12":"code","0bf5aeee":"code","103e9b63":"code","60ff4e89":"code","d213d332":"code","7195375a":"code","9eac8eb6":"code","33864b7b":"code","0853846e":"code","fd4488ec":"code","74d1414b":"code","a3670a79":"code","f59e89ba":"code","900c975e":"code","7316e643":"code","6b8a8d2c":"code","f02e847c":"code","87b5e748":"code","5333a7c2":"code","3c82d9c5":"code","98ae45a2":"code","399d6120":"markdown","6b3cb0aa":"markdown","5ace9a75":"markdown","a50e1079":"markdown","1a9e29b1":"markdown","faf550a3":"markdown","2059c82c":"markdown","89ea0588":"markdown","713b0636":"markdown","53e3fc9d":"markdown","18d2e58b":"markdown","e6aa4251":"markdown","91f4ecac":"markdown","925f1003":"markdown","bb02f028":"markdown","f0727e30":"markdown","25a5048d":"markdown","49073710":"markdown","4e9e9db8":"markdown","f076223e":"markdown"},"source":{"f76a5a2d":"import datetime\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\n\nfrom itertools import product\n\nfrom sklearn import preprocessing as pr\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow import keras\nfrom tensorflow.keras import backend as K \nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Input, Activation, Dense, Layer, BatchNormalization\n\nglobalSeed=768\nfrom numpy.random import seed \nseed(globalSeed)\ntf.compat.v1.set_random_seed(globalSeed)","58eb50b3":"MetaData = pd.read_csv(r'..\/input\/covid19-metadata\/SARSCov2Metadata.csv')\nMetaData.drop(['PCA_A','PCA_B','VAE_A','VAE_B','ConvVAE_A','ConvVAE_B',],axis=1,inplace=True)\nMetaData.fillna(0,inplace=True)\nMetaData = MetaData[MetaData['SimplifiedGEO']==\"USA\"]","02ce8b12":"MetaData.head()","0bf5aeee":"def GetSampleLoc(Sample,boundaries):\n    cLoc=0\n    for k in range(len(boundaries)-1):\n        if Sample>=boundaries[k] and Sample<boundaries[k+1]:\n            cLoc=k\n            break\n        \n    return cLoc\n\ndef GetEqualizedIndex(Data,bins=100,maxCount=100):\n  \n    cMin,cMax=np.min(Data),np.max(Data)\n    boundaries=np.linspace(cMin,cMax,num=bins+1)\n  \n    SamplesCount=np.zeros(bins)\n    indexContainer = []\n  \n    index=[k for k in range(len(Data))]\n    np.random.shuffle(index)\n  \n    for val in index:\n        dataPoint = Data.iloc[val]\n        cLoc=GetSampleLoc(dataPoint,boundaries)\n      \n        if SamplesCount[cLoc]<=maxCount:\n            indexContainer.append(val)\n            SamplesCount[cLoc]=SamplesCount[cLoc]+1\n      \n    return indexContainer","103e9b63":"fig,axs = plt.subplots(1,2,figsize=(12,7))\n\n_ = axs[0].hist(MetaData['outbreaktime'],bins=52)\n_ = axs[1].hist(MetaData['week'],bins=52)","60ff4e89":"reSamplingIndex = GetEqualizedIndex(MetaData['outbreaktime'],bins=100,maxCount=2000)","d213d332":"fig,axs = plt.subplots(1,2,figsize=(12,7))\n\n_ = axs[0].hist(MetaData['outbreaktime'].iloc[reSamplingIndex],bins=50)\n_ = axs[1].hist(MetaData['week'].iloc[reSamplingIndex],bins=50)","7195375a":"Alphabet = ['A','C','T','G']\nKmerLabels = []\n\nmaxSize = 5\nfor k in range(1,maxSize):\n    \n    KmerLabels.append([''.join(i) for i in product(Alphabet, repeat = k)])\n    \nKmerLabels = [item for sublist in KmerLabels for item in sublist]\nheaders = ['id'] + KmerLabels\n\nKmerData = pd.read_csv('..\/input\/covid19-sequences-extended\/KmerDataExt.csv',usecols = headers)\nKmerData['id'] = [val[0:-2] for val in KmerData['id']]\nKmerData = KmerData.set_index('id')","9eac8eb6":"USIndex = MetaData['id'].iloc[reSamplingIndex]\nUSKmer = KmerData.loc[USIndex.tolist()]\n\nIndex = USKmer.index.tolist()\ntrainIndex,testIndex,_,_ = train_test_split(Index,Index,test_size=0.10,train_size=0.90,random_state=23)","33864b7b":"USKmer","0853846e":"def MakeDenseCoder(InputShape,Units,Latent,UpSampling=False):\n    '''\n    Parameters\n    ----------\n    InputShape : tuple\n        Data shape.\n    Units : list\n        List with the number of dense units per layer.\n    Latent : int\n        Size of the latent space.\n    UpSampling : bool, optional\n        Controls the behaviour of the function, False returns the encoder while True returns the decoder. \n        The default is False.\n\n    Returns\n    -------\n    InputFunction : Keras Model input function\n        Input Used to create the coder.\n    localCoder : Keras Model Object\n        Keras model.\n\n    '''\n    Units.append(Latent)\n    \n    if UpSampling:\n        denseUnits=Units[::-1]\n        Name=\"Decoder\"\n    else:\n        denseUnits=Units\n        Name=\"Encoder\"\n    \n    InputFunction=Input(shape=InputShape)\n    nUnits=len(denseUnits)\n    X=Dense(denseUnits[0],use_bias=False)(InputFunction)\n    X=BatchNormalization()(X)\n    X=Activation('relu')(X)\n    \n    for k in range(1,nUnits-1):\n        X=Dense(denseUnits[k],use_bias=False)(X)\n        X=BatchNormalization()(X)\n        X=Activation('relu')(X)\n    \n    X=Dense(denseUnits[-1],use_bias=False)(X)\n    X=BatchNormalization()(X)\n    \n    if UpSampling:\n        Output=Activation('sigmoid')(X)\n        localCoder=Model(inputs=InputFunction,outputs=Output,name=Name)\n    else:    \n        Output=Activation('relu')(X)\n        localCoder=Model(inputs=InputFunction,outputs=Output,name=Name)\n    \n    return InputFunction,localCoder\n\nclass KLDivergenceLayer(Layer):\n    '''\n    Custom KL loss layer\n    '''\n    def __init__(self,*args,**kwargs):\n        self.annealing = tf.Variable(0.,dtype=tf.float32,trainable = False)\n        self.is_placeholder=True\n        super(KLDivergenceLayer,self).__init__(*args,**kwargs)\n        \n    def call(self,inputs):\n        \n        Mu,LogSigma=inputs\n        klbatch=-0.5*self.annealing*K.sum(1+LogSigma-K.square(Mu)-K.exp(LogSigma),axis=-1)\n        self.add_loss(K.mean(klbatch),inputs=inputs)\n        self.add_metric(klbatch,name='kl_loss',aggregation='mean')\n        \n        return inputs\n\nclass Sampling(Layer):\n    '''\n    Custom sampling layer\n    '''\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        \n    def get_config(self):\n        config = {}\n        base_config = super().get_config()\n        return {**base_config, **config}\n    \n    @tf.autograph.experimental.do_not_convert   \n    def call(self,inputs,**kwargs):\n        \n        Mu,LogSigma=inputs\n        batch=tf.shape(Mu)[0]\n        dim=tf.shape(Mu)[1]\n        epsilon=K.random_normal(shape=(batch,dim))\n\n        return Mu+(K.exp(0.5*LogSigma))*epsilon\n\n#Wrapper function, creates a small Functional keras model \n#Bottleneck of the variational autoencoder \ndef MakeVariationalNetwork(Latent):\n    \n    InputFunction=Input(shape=(Latent,))\n    Mu=Dense(Latent)(InputFunction)\n    LogSigma=Dense(Latent)(InputFunction)\n    Mu,LogSigma=KLDivergenceLayer()([Mu,LogSigma])\n    Output=Sampling()([Mu,LogSigma])\n    variationalBottleneck=Model(inputs=InputFunction,outputs=Output)\n    \n    return InputFunction,variationalBottleneck\n\ndef MakeDenseAutoencoder(InputShape,Units,Latent):\n    \n    InputEncoder,Encoder=MakeDenseCoder(InputShape,Units,Latent)\n    InputDecoder,Decoder=MakeDenseCoder((Latent,),Units,Latent,UpSampling=True)\n    AEoutput=Decoder(Encoder(InputEncoder))\n    AE=Model(inputs=InputEncoder,outputs=AEoutput)\n    \n    return Encoder,Decoder,AE\n\n\ndef MakeVariationalDenseAutoencoder(InputShape,Units,Latent):\n    \n    InputEncoder,Encoder=MakeDenseCoder(InputShape,Units,Latent)\n    InputVAE,VAE=MakeVariationalNetwork(Latent)\n    InputDecoder,Decoder=MakeDenseCoder((Latent,),Units,Latent,UpSampling=True)\n    \n    VAEencoderOutput=VAE(Encoder(InputEncoder))\n    VAEencoder=Model(inputs=InputEncoder,outputs=VAEencoderOutput)\n    \n    VAEOutput=Decoder(VAEencoder(InputEncoder))\n    VAEAE=Model(inputs=InputEncoder,outputs=VAEOutput)\n    \n    return VAEencoder,Decoder,VAEAE","fd4488ec":"class KLAnnealing(keras.callbacks.Callback):\n\n    def __init__(self,position, weigths):\n        super().__init__()\n        self.position = position\n        self.weigths = tf.Variable(weigths,trainable=False,dtype=tf.float32)\n\n    def on_epoch_end(self, epoch,logs=None):\n        \n        weights = self.model.get_weights()\n        weights[self.position] = self.weigths[epoch]\n        self.model.set_weights(weights)\n\n\ndef MakeAnnealingWeights(epochs,cycles,scale=1):\n    \n    pointspercycle = epochs\/\/cycles\n    AnnealingWeights = 1*(1\/(1+np.exp(-1*np.linspace(-10,10,num=pointspercycle))))\n    \n    for k in range(cycles-1):\n        AnnealingWeights = np.append(AnnealingWeights,1*(1\/(1+np.exp(-1*np.linspace(-10,10,num=pointspercycle+1)))))\n        \n    return scale*AnnealingWeights\n","74d1414b":"Units = [340,240,140,40,20,10,5,3]\nLatent = 2\n\nsh = 0.0001#0.00001\nlr = 0.0025\nminlr = 0.00001\nbatchSize = 64\nepochs = 100\ndecay = 3*(lr-minlr)\/epochs\n\nAnnealingWeights = sh*np.ones(epochs+1)\n","a3670a79":"ModelsContainer = []\nHistoryContainer = []\nScalers = []\n\nkf = KFold(n_splits=5,shuffle=True,random_state=44)\nXtrainDS = USKmer[KmerLabels].loc[trainIndex]\n\nfor train_index, test_index in kf.split(trainIndex):\n    \n    trainLabels,testLabels = np.array(trainIndex)[train_index],np.array(trainIndex)[test_index]\n    Xtrain, Xtest = XtrainDS.loc[trainLabels], XtrainDS.loc[testLabels]\n    \n    scaler = pr.MinMaxScaler()\n    scaler.fit(Xtrain)\n    \n    Xtrain = scaler.transform(Xtrain)\n    Xtest = scaler.transform(Xtest)\n    \n    Scalers.append(scaler)\n\n    InputShape = Xtrain.shape[1]\n    VAEENC,VAEDEC,VAEAE = MakeVariationalDenseAutoencoder(InputShape,Units,Latent)\n    KLAposition = [k for k,val in enumerate(VAEAE.get_weights()) if len(val.shape)==0][0]\n\n    VAEAE.compile(Adam(learning_rate=lr,decay=decay),loss='mse')\n    history = VAEAE.fit(x=Xtrain,y=Xtrain,batch_size=batchSize,epochs=epochs,\n                        validation_data=(Xtest,Xtest),callbacks=[KLAnnealing(KLAposition,AnnealingWeights)])\n    \n    ModelsContainer.append(VAEENC)\n    HistoryContainer.append(history)\n\n    tf.compat.v1.set_random_seed(globalSeed)\n","f59e89ba":"XtestDS = USKmer[KmerLabels].loc[testIndex]\nMetaData = MetaData.set_index('id')\n\nfig,axs = plt.subplots(3,5,figsize=(25,12))\n\nfor k,block in enumerate(zip(ModelsContainer,HistoryContainer,Scalers)):\n    \n    mod,hist,scaler = block\n    VariationalRepresentation = mod.predict(scaler.transform(XtestDS))\n    axs[0,k].scatter(VariationalRepresentation[:,0],VariationalRepresentation[:,1],c=MetaData.loc[XtestDS.index.tolist()]['week'],alpha=0.25)\n    axs[0,k].title.set_text('Latent Space (model = ' + str(k) +')')\n    \n    axs[1,k].plot(hist.history['loss'],'k-',label = 'Loss')\n    axs[1,k].plot(hist.history['val_loss'],'r-',label = 'Validation Loss')\n    axs[1,k].title.set_text('Reconstruction loss')\n    \n    axs[2,k].plot(hist.history['kl_loss'],'k-',label = 'Loss')\n    axs[2,k].plot(hist.history['val_kl_loss'],'r-',label = 'Validation Loss')\n    axs[2,k].title.set_text('Kullback\u2013Leibler loss')\n    \nplt.tight_layout()\n    ","900c975e":"fig,axs = plt.subplots(2,5,figsize=(25,12))\n\nfor k,block in enumerate(zip(ModelsContainer,Scalers)):\n    \n    mod,scaler = block\n    VariationalRepresentation = mod.predict(scaler.transform(XtestDS))\n    axs[0,k].scatter(VariationalRepresentation[:,0],VariationalRepresentation[:,1],c=XtestDS['A'],alpha=0.1)\n    axs[0,k].title.set_text('Adenine Content (model = ' + str(k) +')')\n    \n    axs[1,k].scatter(VariationalRepresentation[:,0],VariationalRepresentation[:,1],c=XtestDS['C'],alpha=0.1)\n    axs[1,k].title.set_text('Cytosine Content (model = ' + str(k) +')')","7316e643":"fig,axs = plt.subplots(2,5,figsize=(25,12))\n\nfor k,block in enumerate(zip(ModelsContainer,Scalers)):\n    \n    mod,scaler = block\n    VariationalRepresentation = mod.predict(scaler.transform(XtestDS))\n    axs[0,k].scatter(VariationalRepresentation[:,0],VariationalRepresentation[:,1],c=XtestDS['G'],alpha=0.1)\n    axs[0,k].title.set_text('Guanine Content(model = ' + str(k) +')')\n    \n    axs[1,k].scatter(VariationalRepresentation[:,0],VariationalRepresentation[:,1],c=XtestDS['T'],alpha=0.1)\n    axs[1,k].title.set_text('Thymine\/Uracil Space (model = ' + str(k) +')')\n    ","6b8a8d2c":"USAInfectionData = pd.read_csv('..\/input\/us-covid19-dataset-live-hourlydaily-updates\/States.csv')\nUSAInfectionData[\"datets\"] = pd.to_datetime(USAInfectionData['date'],format='%Y-%m-%d')\nUSAInfectionData[\"year\"] = USAInfectionData[\"datets\"].dt.year\nUSAInfectionData[\"week\"] = USAInfectionData[\"datets\"].dt.isocalendar().week\nUSAInfection2021 = USAInfectionData[USAInfectionData[\"year\"]==2021]","f02e847c":"USATotal2021 = np.array(USAInfectionData[USAInfectionData[\"year\"]==2021].groupby('week')['cases'].sum()\/USAInfectionData[USAInfectionData[\"year\"]==2021].groupby('week')['cases'].sum().max()).reshape((-1,))","87b5e748":"speed = np.diff(USATotal2021[0:-2])\nacceleration = np.diff(USATotal2021[0:-2],n=2)\n\nfig,axes = plt.subplots(1,3,figsize=(18,4))\n\naxes[0].plot(USATotal2021[0:-2],color='red')\naxes[1].plot(speed,color='red')\naxes[2].plot(acceleration,color='red')","5333a7c2":"speed = np.append(speed,[speed[-1],speed[-1],speed[-1]])\nacceleration = np.append(acceleration,[acceleration[-1],acceleration[-1],acceleration[-1],acceleration[-1]])\nweeks = np.sort(MetaData['week'].unique())\n\n\ntoSpeed = dict([(val,sal) for val,sal in zip(weeks,speed) ])\ntoAcceleration = dict([(val,sal) for val,sal in zip(weeks,acceleration)])","3c82d9c5":"MetaData['speed'] = [toSpeed[val] for val in MetaData['week']]\nMetaData['acceleration'] = [toAcceleration[val] for val in MetaData['week']]","98ae45a2":"fig,axs = plt.subplots(2,5,figsize=(25,12))\n\nfor k,block in enumerate(zip(ModelsContainer,Scalers)):\n    \n    mod,scaler = block\n    VariationalRepresentation = mod.predict(scaler.transform(XtestDS))\n    axs[0,k].scatter(VariationalRepresentation[:,0],VariationalRepresentation[:,1],c=MetaData.loc[XtestDS.index.tolist()]['speed'],alpha=0.25)\n    axs[0,k].title.set_text('Latent Space (model = ' + str(k) +')')\n    \n    axs[1,k].scatter(VariationalRepresentation[:,0],VariationalRepresentation[:,1],c=MetaData.loc[XtestDS.index.tolist()]['acceleration'],alpha=0.25)\n    axs[1,k].title.set_text('Latent Space (model = ' + str(k) +')')","399d6120":"## Auxiliary functions","6b3cb0aa":"## Definition","5ace9a75":"Although the bottleneck representation is different in each model, the most easily recognizable pattern is the clear division into two particular seasons or periods. Thus seasonal information is encoded in the different sequences and k-mer based representations can cluster together sequences containing such patterns. Also, small glimpses of other kinds of patterns start to appear, showing the capabilities and possible applications of autoencoders to SARS-Cov-2 viral dynamics. \n\nHowever, specific sequence patterns cannot be found yet, but the resulting clusters will simplify the identification of such patterns. Continuous development of generative models could lead to new insights into SARS-Cov-2 dynamics and evolution or the proposal of new containment measures. ","a50e1079":"The following exemplifies the use of variational autoencoder for a representation learning task and its applications to the seasonal classification of SARS-Cov-2 sequences. ","1a9e29b1":"# Training","faf550a3":"Bottleneck representation, color encode isolation week regardless of the year. This results into a clear separation into two seasons ","2059c82c":"# Adding infection dynamics\n\nA small sequence representation allows to visually represent or encode other pandemic characteristics. Thus it will be helpful if the resulting clusters encode information about the pandemic dynamics. From the number of accumulated cases, we can calculate the COVID-19 spread rate or the COVID-19 acceleration spread. Those two numerical features will help to find specific patterns if they exist. ","89ea0588":"## Sampling \n\nVisualizing the number of sequence samples per time shows a clear bias towards the specific pandemic peaks of the COVID-19 pandemic. Enhancing the bias towards pandemic peaks, hindering the ability to identify specific seasonal components inside the sequence if they exist. ","713b0636":"# Seasonal disentangling of SARS Cov 2","53e3fc9d":"# Data\n\nData consist of two datasets, the first one contains the metadata of the different isolated sequences obtained from the NCBI SARS-Cov-2 resources. While the second one contains the K-mer frequencies of each isolated sequence. \n\nBoth datasets contain SARS-Cov-2 sequences isolated between January 2020 and November 2021. To constrain the samples to analyze only sequences from the USA will be included.","18d2e58b":"## Final Dataset\n\nOne way to eliminate the imbalance or bias in the data set is to resample it to have evenly distributed samples aiming to have a squared distribution. This reduces the number of samples but removes biases. ","e6aa4251":"Two models show small clusters with different guanine content, while the remaining models do not show any particular difference. While Thymine\/Uracil show a small difference between the two main clusters. ","91f4ecac":"Small clusters of high Adenine content can be found in two of the models, while small differences in the content in Cytosine are observed between the two main clusters. ","925f1003":"## Nucleotide content","bb02f028":"## Packages","f0727e30":"The resampled metadata is used to select the appropriate sequences from the K-mer dataset. The resulting K-mer dataset is divided into two, a training data set for k fold cross-validation. And a test data set for downstream analysis and visualization. ","25a5048d":"## K fold cross validation","49073710":"## Speed and acceleration\n\nMost of the models show a clear speed rate difference between the different clusters. Low and high-speed rates cluster together matching the clusters that contain the sequences isolated at the beginning and the end of the year. However, two particular models start to separate and cluster sequences that were isolated at similar speed rates. While acceleration does not show any particular pattern yet, there are a few isolated glimpses in two models that suggest a particular pattern regarding speed or acceleration. ","4e9e9db8":"# Bottleneck representation","f076223e":"# Network \n\nThe final dataset consisting of K-mer frequencies of the sampled sequences is used to train a variational autoencoder. The model architecture consists of fully connected dense layers with a variational layer in the middle, KL divergence loss is scaled by a factor named shrinkage."}}