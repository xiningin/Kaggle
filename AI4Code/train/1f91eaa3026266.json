{"cell_type":{"1a80571b":"code","264b15ea":"code","88ab7644":"code","27f1e130":"code","64e07dc4":"code","f8c03788":"code","7ca1fb58":"code","51e8d805":"code","cf52e906":"code","b7acdd62":"code","3155b5dd":"code","1ccb761d":"code","e6486ae2":"code","c135971f":"code","e4047f12":"code","cb7b6af9":"code","0c0633c4":"code","d63cf393":"code","85e5a96f":"code","434684fe":"code","239ce03f":"code","6068d20a":"code","343ecc16":"code","3c55ba95":"code","66a49d3a":"code","812082d5":"code","d7fb2149":"code","6f26aafc":"code","b1afe586":"code","88c0860e":"code","859207d9":"code","6b5f0169":"code","f503e9b5":"code","09abe97b":"code","5556385b":"code","db828439":"code","108b87f1":"code","da887c7b":"code","60f7e5b5":"code","2c3cade8":"code","29861706":"code","a0424a4a":"code","2d6e4303":"code","cd6983e7":"markdown","10823eb6":"markdown","d3690146":"markdown","76d96230":"markdown","f9f522bb":"markdown","1a7a21e6":"markdown","0af99595":"markdown","af42c190":"markdown","730e9930":"markdown","016322ca":"markdown","1854c6f0":"markdown","7a255378":"markdown","d3c12a64":"markdown","e07c50f8":"markdown","68c2984d":"markdown","745a6b1e":"markdown","1a94bedc":"markdown","9d8ec78f":"markdown","c87dd1f5":"markdown","68a8abd4":"markdown","c19efa48":"markdown","4d9e8ca8":"markdown","fe1aad21":"markdown","832f8afe":"markdown","cbfe9d5d":"markdown","5a178ab4":"markdown","2592e3a7":"markdown","90d41b6c":"markdown","40af6a6a":"markdown","f2d73885":"markdown","c4aee163":"markdown","3e7ecda5":"markdown","6848a597":"markdown"},"source":{"1a80571b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","264b15ea":"#Importing the required libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\n\n#Importing required libraries fo cleaning text\nimport re\nimport nltk\nfrom matplotlib import pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\n\nprint(\"Important libraries loaded successfully\")","88ab7644":"ds_train=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nds_test=pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nprint(\"Train and Test data sets are imported successfully\")","27f1e130":"#Checking number of records in train data set and few records from train data set\nprint(\"Number of records in Train data set\",len(ds_train.index))\nds_train.head()","64e07dc4":"#Distinct keywords in train dataset\ndist_keyword=ds_train['keyword'].value_counts()\ndist_keyword","f8c03788":"#Visualize the keywords\nfig = px.scatter(dist_keyword, x=dist_keyword.values, y=dist_keyword.index,size=dist_keyword.values)\nfig.show()","7ca1fb58":"#Distinct location in train dataset\ndist_location=ds_train['location'].value_counts()\n#Visualize location\nfig = px.scatter(dist_location, y=dist_location.values, x=dist_location.index,size=dist_location.values)\nfig.show()","51e8d805":"# creating bool series True for NaN values for location \nbool_series_location = pd.isnull(ds_train['location']) \n\n# filtering data  \n# displaying data only with location = NaN  \nds_train[bool_series_location]\nprint(\"Number of records with missing location\",len(ds_train[bool_series_location]))","cf52e906":"# creating bool series True for NaN values  \nbool_series_keyword = pd.isnull(ds_train['keyword']) \n# filtering data  \n# displaying data only with Keywords = NaN  \nds_train[bool_series_keyword]\nprint(\"Number of records with missing keywords\",len(ds_train[bool_series_keyword]))","b7acdd62":"# Calculate percentage of missing keywords\nprint('{}% of Kewords are missing from Total Number of Records'.format((len(ds_train[bool_series_keyword])\/len(ds_train.index))*100))","3155b5dd":"#dropping unwanted column 'location'\nds_train=ds_train.drop(['location'],axis=1)\nds_train.head()","1ccb761d":"#dropping missing 'keyword' records from train data set\nds_train=ds_train.drop(ds_train[bool_series_keyword].index,axis=0)\n#Resetting the index after droping the missing records\nds_train=ds_train.reset_index(drop=True)\nprint(\"Number of records after removing missing keywords\",len(ds_train))\nds_train.head()","e6486ae2":"corpus  = []\npstem = PorterStemmer()\nfor i in range(ds_train['text'].shape[0]):\n    #Remove unwanted words\n    text = re.sub(\"[^a-zA-Z]\", ' ', ds_train['text'][i])\n    #Transform words to lowercase\n    text = text.lower()\n    text = text.split()\n    #Remove stopwords then Stemming it\n    text = [pstem.stem(word) for word in text if not word in set(stopwords.words('english'))]\n    text = ' '.join(text)\n    #Append cleaned tweet to corpus\n    corpus.append(text)\n    \nprint(\"Corpus created successfully\")  ","c135971f":"#Create dictionary \nuniqueWords = {}\nfor text in corpus:\n    for word in text.split():\n        if(word in uniqueWords.keys()):\n            uniqueWords[word] += 1\n        else:\n            uniqueWords[word] = 1\n            \n#Convert dictionary to dataFrame\nuniqueWords = pd.DataFrame.from_dict(uniqueWords,orient='index',columns=['WordFrequency'])\nuniqueWords.sort_values(by=['WordFrequency'], inplace=True, ascending=False)\nprint(\"Number of records in Unique Words Data frame are {}\".format(len(uniqueWords)))\nuniqueWords.head(10)\n","e4047f12":"#Get Maximum,Minimum and Mean occurance of a word \nprint(\"Maximum Occurance of a word is {} times\".format(uniqueWords['WordFrequency'].max()))\nprint(\"Minimum Occurance of a word is {} times\".format(uniqueWords['WordFrequency'].min()))\nprint(\"Mean Occurance of a word is {} times\".format(uniqueWords['WordFrequency'].mean()))","cb7b6af9":"uniqueWords=uniqueWords[uniqueWords['WordFrequency']>=20]\nprint(\"Number of records in Unique Words Data frame are {}\".format(len(uniqueWords)))","0c0633c4":"from wordcloud import WordCloud\nwordcloud = WordCloud().generate(\" \".join(corpus))\nplt.imshow(wordcloud,interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","d63cf393":"# Creating the Bag of Words model\nfrom sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features = len(uniqueWords))\n#Create Bag of Words Model , here X represent bag of words\nX = cv.fit_transform(corpus).todense()\ny = ds_train['target'].values","85e5a96f":"#Split the train data set to train and test data\nX_train , X_test , y_train , y_test = train_test_split(X,y,test_size=0.2, random_state=2020)\nprint('Train Data splitted successfully')","434684fe":"# Fitting Gaussian Naive Bayes to the Training set\nclassifier_gnb = GaussianNB()\nclassifier_gnb.fit(X_train, y_train)\n# Predicting the Train data set results\ny_pred_gnb = classifier_gnb.predict(X_test)\n# Making the Confusion Matrix\ncm_gnb = confusion_matrix(y_test, y_pred_gnb)\ncm_gnb","239ce03f":"#Calculating Model Accuracy\nprint('GaussianNB Classifier Accuracy Score is {} for Train Data Set'.format(classifier_gnb.score(X_train, y_train)))\nprint('GaussianNB Classifier Accuracy Score is {} for Test Data Set'.format(classifier_gnb.score(X_test, y_test)))\nprint('GaussianNB Classifier F1 Score is {}'.format(f1_score(y_test, y_pred_gnb)))\n","6068d20a":"# Fitting Gradient Boosting Models to the Training set\nclassifier_gb = GradientBoostingClassifier(loss = 'deviance',\n                                                   learning_rate = 0.01,\n                                                   n_estimators = 100,\n                                                   max_depth = 30,\n                                                   random_state=55)\nclassifier_gb.fit(X_train, y_train)\n# Predicting the Train data set results\ny_pred_gb = classifier_gb.predict(X_test)\n# Making the Confusion Matrix\ncm_gb = confusion_matrix(y_test, y_pred_gb)\ncm_gb","343ecc16":"#Calculating Model Accuracy\nprint('Gradient Boosting Classifier Accuracy Score is {} for Train Data Set'.format(classifier_gb.score(X_train, y_train)))\nprint('Gradient Boosting Classifier Accuracy Score is {} for Test Data Set'.format(classifier_gb.score(X_test, y_test)))\nprint('Gradient Boosting Classifier F1 Score is {} '.format(f1_score(y_test, y_pred_gb)))\n","3c55ba95":"# Fitting K- Nearest neighbour to the Training set\nclassifier_knn = KNeighborsClassifier(n_neighbors = 7,weights = 'distance',algorithm = 'brute')\nclassifier_knn.fit(X_train, y_train)\n# Predicting the Train data set results\ny_pred_knn = classifier_knn.predict(X_test)\n# Making the Confusion Matrix\ncm_knn = confusion_matrix(y_test, y_pred_knn)\ncm_knn","66a49d3a":"#Calculating Model Accuracy\nprint('K-Nearest Neighbour Model Accuracy Score for Train Data set is {}'.format(classifier_knn.score(X_train, y_train)))\nprint('K-Nearest Neighbour Model Accuracy Score for Test Data set is {}'.format(classifier_knn.score(X_test, y_test)))\nprint('K-Nearest Neighbour Model F1 Score is {}'.format(f1_score(y_test, y_pred_knn)))","812082d5":"# Fitting Decision Tree Models to the Training set\nclassifier_dt = DecisionTreeClassifier(criterion= 'entropy',\n                                           max_depth = None, \n                                           splitter='best', \n                                           random_state=55)\nclassifier_dt.fit(X_train, y_train)\n# Predicting the Train data set results\ny_pred_dt = classifier_dt.predict(X_test)\n# Making the Confusion Matrix\ncm_dt = confusion_matrix(y_test, y_pred_dt)\ncm_dt","d7fb2149":"#Calculating Model Accuracy\nprint('DecisionTree Model Accuracy Score for Train Data set is {}'.format(classifier_dt.score(X_train, y_train)))\nprint('DecisionTree Model Accuracy Score for Test Data set is {}'.format(classifier_dt.score(X_test, y_test)))\nprint('DecisionTree Model F1 Score is {}'.format(f1_score(y_test, y_pred_dt)))","6f26aafc":"# Fitting Logistic Regression Model to the Training set\nclassifier_lr = LogisticRegression()\nclassifier_lr.fit(X_train, y_train)\n# Predicting the Train data set results\ny_pred_lr = classifier_lr.predict(X_test)\n# Making the Confusion Matrix\ncm_lr = confusion_matrix(y_test, y_pred_lr)\ncm_lr","b1afe586":"#Calculating Model Accuracy\nprint('Logistic Regression Model Accuracy Score for Train Data set is {}'.format(classifier_lr.score(X_train, y_train)))\nprint('Logistic Regression Model Accuracy Score for Test Data set is {}'.format(classifier_lr.score(X_test, y_test)))\nprint('Logistic Regression Model F1 Score is {}'.format(f1_score(y_test, y_pred_lr)))","88c0860e":"# Fitting XGBoost Model to the Training set\nclassifier_xgb = XGBClassifier(max_depth=6,learning_rate=0.3,n_estimators=1500,objective='binary:logistic',random_state=123,n_jobs=4)\nclassifier_xgb.fit(X_train, y_train)\n# Predicting the Train data set results\ny_pred_xgb = classifier_xgb.predict(X_test)\n# Making the Confusion Matrix\ncm_xgb = confusion_matrix(y_test, y_pred_xgb)\ncm_xgb","859207d9":"print('XG Boost Model Accuracy Score for Train Data set is {}'.format(classifier_xgb.score(X_train, y_train)))\nprint('XG Boost Model Accuracy Score for Test Data set is {}'.format(classifier_xgb.score(X_test, y_test)))\nprint('XG Boost Model F1 Score is {}'.format(f1_score(y_test, y_pred_xgb)))","6b5f0169":"# Fitting multinomial naive bayes Model to the Training set\nclassifier_mnb = MultinomialNB()\nclassifier_mnb.fit(X_train, y_train)\n# Predicting the Train data set results\ny_pred_mnb = classifier_mnb.predict(X_test)\n# Making the Confusion Matrix\ncm_mnb = confusion_matrix(y_test, y_pred_mnb)\ncm_mnb","f503e9b5":"print('MultinomialNB Model Accuracy Score for Train Data set is {}'.format(classifier_mnb.score(X_train, y_train)))\nprint('MultinomialNB Model Accuracy Score for Test Data set is {}'.format(classifier_mnb.score(X_test, y_test)))\nprint('MultinomialNB Model F1 Score is {}'.format(f1_score(y_test, y_pred_mnb)))","09abe97b":"# Fitting Logistic Regression Model to the Training set\nmodels = [('LogisticRegression',classifier_lr),\n                ('XGBoost Classifier',classifier_xgb),\n         ('DecisionTree Classifier',classifier_dt),\n         ('K-Nerarest Neighbour', classifier_knn),\n         ('Gradient Boosting',classifier_gb),\n         ('Gaussian Naive Bayes',classifier_gnb),\n         ('MultinomialNB',classifier_mnb)]\nclassifier_vc = VotingClassifier(voting = 'hard',estimators= models)\nclassifier_vc.fit(X_train, y_train)\n# Predicting the Train data set results\ny_pred_vc = classifier_dt.predict(X_test)\n# Making the Confusion Matrix\ncm_vc = confusion_matrix(y_test, y_pred_vc)\ncm_vc","5556385b":"#Calculating Model Accuracy\nprint('Voting Classifier Model Accuracy Score for Train Data set is {}'.format(classifier_vc.score(X_train, y_train)))\nprint('Voting Classifier Model Accuracy Score for Test Data set is {}'.format(classifier_vc.score(X_test, y_test)))\nprint('Voting Classifier Model F1 Score is {}'.format(f1_score(y_test, y_pred_vc)))","db828439":"#Check number of records in Test Data set\nprint(\"Number of records present in Test Data Set are {}\".format(len(ds_test.index)))\n#Check number of missing Keywords in Test Data set\nprint(\"Number of records without keywords in Test Data are {}\".format(len(ds_test[pd.isnull(ds_test['keyword'])])))\nprint(\"Number of records without location in Test Data are {}\".format(len(ds_test[pd.isnull(ds_test['location'])])))","108b87f1":"#Drop Location column from Test Data\nds_test=ds_test.drop(['location'],axis=1)\nds_test.head()","da887c7b":"#Fitting into test set\nX_testset=cv.transform(ds_test['text']).todense()","60f7e5b5":"#Predict data with classifier created in previous section\ny_test_pred_gnb = classifier_gnb.predict(X_testset)\ny_test_pred_gb = classifier_gb.predict(X_testset)\ny_test_pred_dt = classifier_dt.predict(X_testset)\ny_test_pred_knn = classifier_knn.predict(X_testset)\ny_test_pred_lr = classifier_lr.predict(X_testset)\ny_test_pred_vc = classifier_vc.predict(X_testset)\ny_test_pred_xgb = classifier_xgb.predict(X_testset)\ny_test_pred_mnb = classifier_mnb.predict(X_testset)","2c3cade8":"#Fetching Id to differnt frame\ny_test_id=ds_test[['id']]\n#Converting Id into array\ny_test_id=y_test_id.values\n#Converting 2 dimensional y_test_id into single dimension \ny_test_id=y_test_id.ravel()","29861706":"#Converting 2 dimensional y_test_pred for all predicted results into single dimension \ny_test_pred_gnb=y_test_pred_gnb.ravel()\ny_test_pred_gb=y_test_pred_gb.ravel()\ny_test_pred_dt=y_test_pred_dt.ravel()\ny_test_pred_knn=y_test_pred_knn.ravel()\ny_test_pred_lr=y_test_pred_lr.ravel()\ny_test_pred_vc=y_test_pred_vc.ravel()\ny_test_pred_xgb=y_test_pred_xgb.ravel()\ny_test_pred_mnb=y_test_pred_mnb.ravel()","a0424a4a":"#Creating Submission dataframe\nsubmission_df_gnb=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_gnb})\nsubmission_df_gb=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_gb})\nsubmission_df_dt=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_dt})\nsubmission_df_knn=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_knn})\nsubmission_df_lr=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_lr})\nsubmission_df_vc=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_vc})\nsubmission_df_xgb=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_xgb})\nsubmission_df_mnb=pd.DataFrame({\"id\":y_test_id,\"target\":y_test_pred_mnb})\n\n\n\n#Setting index as Id Column\nsubmission_df_gnb.set_index(\"id\")\nsubmission_df_gb.set_index(\"id\")\nsubmission_df_dt.set_index(\"id\")\nsubmission_df_knn.set_index(\"id\")\nsubmission_df_lr.set_index(\"id\")\nsubmission_df_vc.set_index(\"id\")\nsubmission_df_xgb.set_index(\"id\")\nsubmission_df_mnb.set_index(\"id\")","2d6e4303":"#Converting into CSV file for submission\nsubmission_df_gnb.to_csv(\"submission_gnb.csv\",index=False)\nsubmission_df_gb.to_csv(\"submission_gb.csv\",index=False)\nsubmission_df_dt.to_csv(\"submission_dt.csv\",index=False)\nsubmission_df_knn.to_csv(\"submission_knn.csv\",index=False)\nsubmission_df_lr.to_csv(\"submission_lr.csv\",index=False)\nsubmission_df_vc.to_csv(\"submission_vc.csv\",index=False)\nsubmission_df_xgb.to_csv(\"submission_xgb.csv\",index=False)\nsubmission_df_mnb.to_csv(\"submission_mnb.csv\",index=False)","cd6983e7":"It's quiet evident that some words occured very less in tweets, so we can remove these words from our corpus to decrease dimension of 'Bag of World' model\n\nWe will create a dictionary for this where 'key' refer to a word and 'value' refer to frequency of that word in tweets and data frame from that dictionary","10823eb6":"As you can see the number of records have reduced drastically.\n\nLet's create Word cloud for our corpus","d3690146":"# Data Preprocessing: Basic NLP Techniques","76d96230":"**XGBOOST**","f9f522bb":"# Objective","1a7a21e6":"From Data description and above analysis we can conclude :\n\n* Locations are not automatically generated, they are user inputs and that's why data is not clean and there are too many incoorect and missing values.We can skip the 'location' column from our feature list\n\n* We can consider the 'keyword' column as a feature because there are a lot of unique keywords and missing values are very insignificant (< 1 percentage)","0af99595":"**Multinomial Naive Bayes**","af42c190":"Let's check how many missing values are there 'Location' and 'Keyword' columns","730e9930":"Let's build our models , will try different models and evaluate each of them.\n\nWe will be using **confusion matrix** to evaluate them. Confusion matrix is formed from the four outcomes produced as a result of binary classification\n![image.png](attachment:image.png)","016322ca":"Out of all these models I am getting my best score  which is **~0.76** from Multinomial Naive Bayes Model . This could be improved further.","1854c6f0":"Looks like some values from 'Keyword' and 'Location' columns are missing let's explore these columns","7a255378":"As similar to Train data number of missing location are more and remaining values are also not unique\/correct to keep in the feature list, will drop 'location' column\n\nWe will keep the rows corresponds to missing 'keyword'  because the deletion of records will affect the submission file.","d3c12a64":"> Multinomial Naive Bayes is giving best score out of all models i.e. **0.76**","e07c50f8":"**Decision Tree Model**","68c2984d":"Here, in our feature list, we have not added the 'keyword' column because the value of this column must be there in the corpus and eventually in Bag of Words model so we can skip this.","745a6b1e":"**Voting Classifier Model**\n\nA Voting classifier model combines multiple different models (i.e., sub-estimators) into a single model, which is (ideally) stronger than any of the individual models alone","1a94bedc":"### If you like the kernel please upvote","9d8ec78f":"# Data Exploration","c87dd1f5":"Let's calcualte Model Accuracy Score and F1 score","68a8abd4":"Let's create Corpus from 'Text' coloumn, **Corpus** is a simplified version of our text data that contain clean data. To create Corpus we have to perform the following actions\n\n1.  **Remove unwanted words** : Removal of unwanted words such as **special characters** and **numbers** to get only pure text\n1.  **Transform words to lowercase** : Transform words to lowercase because upper and lower case have diffirent **ASCII** codes\n1. **Remove stopwords** : Stop words are usually the most common words in a language and they will be irrelevant in determining the nature\n1.  **Stemming words** : Stemming is the process of reducing words to their word stem, base or root form\n","c19efa48":"It's a binary classifier predicts all data instances of a test dataset as either positive or negative. This classification (or prediction) produces four outcomes \u2013 true positive, true negative, false positive and false negative.\n\n* True positive (TP): correct positive prediction\n* False positive (FP): incorrect positive prediction\n* True negative (TN): correct negative prediction\n* False negative (FN): incorrect negative prediction\n\n![image.png](attachment:image.png)","4d9e8ca8":"Let's start working on Test Data set","fe1aad21":"Also , we can calulate **F1 score** for all models .\n\n![image.png](attachment:image.png)","832f8afe":"Let's start with **Gaussian Naive Bayes** Model","cbfe9d5d":"Let's start with importing required libraries and dataset","5a178ab4":"# Prediction for Test Data and Submission","2592e3a7":"In this Kernel I am doing EDA on [dataset](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/data) provided for classification of Disaster Tweets [competition](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/overview) . For classification I will be using different classifier model along with Natural Language Processing Techniques for analyzing sentiments in tweets.For submission will evaluate different models based on Accuracy and F1 Score.\n\nAs this is my first NLP work I tried to keep this notebook simple and easy to understand for beginner. If you like the kernel **please upvote**.","90d41b6c":"**Logistic Regression Model**","40af6a6a":"# Data Modeling and Model Evaluation","f2d73885":"**K - Nearest Neighbors Model**","c4aee163":"**Gradient Boosting Model**","3e7ecda5":"Let's create Bag of word model (Sparse Matrix), it contains only unique words from corpus.","6848a597":"From 'unique words' data frame we can say that some words are repeated lot and some repeated very less , we can only keep words which are occuring 20 times or more to keep the dimension of Bag of world model reasonable."}}