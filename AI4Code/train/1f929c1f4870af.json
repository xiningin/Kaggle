{"cell_type":{"d53149aa":"code","6ed12e86":"code","17d2f013":"code","982340c9":"code","4658bf04":"code","e322eee2":"code","6020fc0d":"code","2c65ab72":"code","cd42ee18":"code","2c8b7145":"code","616a303a":"code","7e3a9f43":"code","6f353bd5":"code","6a9230db":"code","c84ea39d":"code","ec4b7cde":"code","8a756606":"code","bb93d2a2":"code","b4c5eb67":"code","9621da7e":"code","a22e913b":"code","43fe1aff":"code","ff98102d":"code","14143291":"code","fd1bfa2c":"code","5ca6d2f9":"code","4a018f9a":"code","09cd45fc":"code","c5c8725d":"code","9df9fe30":"code","99f8513b":"markdown","c985fa41":"markdown","926bcfc7":"markdown","b3eae24a":"markdown","205e392b":"markdown","5ef679cc":"markdown","fd2b67da":"markdown","205e0d24":"markdown","e2ad51d1":"markdown","5a0c2e24":"markdown","0ab819d4":"markdown","0a202d50":"markdown","f346ecd0":"markdown","4146de58":"markdown","b1b71fe4":"markdown","aac71d97":"markdown","a4f84f91":"markdown","de56c441":"markdown","6262eb4e":"markdown","a75e679d":"markdown","8a396462":"markdown"},"source":{"d53149aa":"!pip install -q imbalanced-learn\n\nimport numpy as np\nimport operator\nimport string\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.over_sampling import ADASYN\n\nfrom sklearn import manifold\nfrom sklearn.preprocessing import LabelBinarizer, StandardScaler, Normalizer\nfrom sklearn.model_selection import train_test_split\n\n# Estimadores que vamos testar\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import LabelEncoder\n\n# utilit\u00e1rios para plots\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport folium\nfrom folium import plugins\nfrom folium.plugins import HeatMap\nfrom folium.plugins import FastMarkerCluster\nfrom folium.plugins import MarkerCluster\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nplt.style.use('ggplot')","6ed12e86":"sns.set_style(\"whitegrid\")\nsns.despine()\n\ndef plot_bar(df, title, filename):    \n    p = (\n        'Set2', \n        'Paired', \n        'colorblind', \n        'husl',\n        'Set1', \n        'coolwarm', \n        'RdYlGn', \n        #'spectral'\n    )\n    color = sns.color_palette(np.random.choice(p), len(df))\n    bar   = df.plot(kind='barh',\n                    title=title,\n                    fontsize=8,\n                    figsize=(12,8),\n                    stacked=False,\n                    width=1,\n                    color=color,\n    )\n\n    bar.figure.savefig(filename)\n\n    plt.show()\n\ndef plot_top_crimes(df, column, title, fname, items=0):\n    try:        \n        by_col         = df.groupby(column)\n        col_freq       = by_col.size()\n        col_freq.index = col_freq.index.map(string.capwords)\n        col_freq.sort_values(ascending=True, inplace=True)\n        plot_bar(col_freq[slice(-1, - items, -1)], title, fname)\n    except Exception:\n        plot_bar(df, title, fname)","17d2f013":"train_data = pd.read_csv(\"..\/input\/train.csv\", parse_dates =['Dates'])\ntest_data = pd.read_csv(\"..\/input\/test.csv\", parse_dates =['Dates'])","982340c9":"print('Shape dos dados de treino:',train_data.shape)\nprint('Shape dos dados de teste :',test_data.shape)","4658bf04":"train_data.head(6)","e322eee2":"# separa as datas em ano, m\u00eas, dia, hora, minuto e segundo.\n# cada parte da data em uma coluna separada. Isto aumenta a quantidade de features\n\nfor x in [train_data, test_data]: \n    x['years'] = x['Dates'].dt.year\n    x['months'] = x['Dates'].dt.month\n    x['days'] = x['Dates'].dt.day\n    x['hours'] = x['Dates'].dt.hour\n    x['minutes'] = x['Dates'].dt.minute","6020fc0d":"train_data['XY'] = train_data.X * train_data.Y\ntest_data['XY'] = test_data.X * test_data.Y","2c65ab72":"plot_top_crimes(train_data, 'Category', 'Por categoria', 'category.png')\n# quantidade de crimes associado \u00e0 cada uma das categorias\nprint(train_data.Category.value_counts())","cd42ee18":"plot_top_crimes(train_data, 'Address', 'Principais localiza\u00e7\u00f5es de ocorr\u00eancias',  'location.png', items=50)\nprint(train_data.Address.value_counts())","2c8b7145":"plot_top_crimes(train_data, 'PdDistrict', 'Departamentos com mais atividades',  'police.png')\n# quantidade de incidentes associada \u00e0 cada distrito policial\nprint(train_data.PdDistrict.value_counts())","616a303a":"fig, ((axis1,axis2)) = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(15,4)\n\nsns.countplot(data=train_data, x='days', ax=axis1)\nsns.countplot(data=train_data, x='hours', ax=axis2)\nplt.show()","7e3a9f43":"addr = train_data['Address'].apply(lambda x: ' '.join(x.split(' ')[-2:]))\n\nyear_count=addr.value_counts().reset_index().sort_values(by='index').head(10)\nyear_count.columns=['addr','Count']\n# Create a trace\ntag = (np.array(year_count.addr))\nsizes = (np.array((year_count['Count'] \/ year_count['Count'].sum())*100))\nplt.figure(figsize=(15,8))\n\ntrace = go.Pie(labels=tag, values=sizes)\nlayout = go.Layout(title='Endere\u00e7os com mais incidentes')\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"Inncidentes\")\n","6f353bd5":"data=[]\nfor i in range(2003,2015):\n    year=train_data[train_data['years']==i]\n    year_count=year['months'].value_counts().reset_index().sort_values(by='index')\n    year_count.columns=['months','Count']\n    trace = go.Scatter(\n    x = year_count.months,\n    y = year_count.Count,\n    name = i)\n    data.append(trace)\n    \n\npy.iplot(data, filename='basic-line')","6a9230db":"m = folium.Map(\n    location=[train_data.Y.mean(), train_data.X.mean()],\n    tiles='Cartodb Positron',\n    zoom_start=13\n)\n\nmarker_cluster = MarkerCluster(\n    name='Locais de crimes em San Francisco',\n    overlay=True,\n    control=False,\n    icon_create_function=None\n)\nfor k in range(1000):\n    location = train_data.Y.values[k], train_data.X.values[k]\n    marker = folium.Marker(location=location,icon=folium.Icon(color='green'))\n    popup = train_data.Address.values[k]\n    #popup = train_data.Address.apply(lambda x: ' '.join(x.split(' ')[-2:])).values[k]\n    folium.Popup(popup).add_to(marker)\n    marker_cluster.add_child(marker)\n\nmarker_cluster.add_to(m)\n\nfolium.LayerControl().add_to(m)\n\nm.save(\"cluster.html\")\n\nm","c84ea39d":"new=train_data[train_data['Category']=='LARCENY\/THEFT']\nM= folium.Map(location=[train_data.Y.mean(), train_data.X.mean() ],tiles= \"Stamen Terrain\",\n                    zoom_start = 13) \n\nheat_data = [[[row['Y'],row['X']] \n                for index, row in new.head(1000).iterrows()] \n                 for i in range(0,11)]\n\nhm = plugins.HeatMapWithTime(heat_data,auto_play=True,max_opacity=0.8)\nhm.add_to(M)\n\nhm.save('heatmap.html')\n\nM","ec4b7cde":"# correla\u00e7\u00f5es entre as vari\u00e1veis\ntrain_data.corr().style.format(\"{:.2}\").background_gradient(cmap=plt.get_cmap('tab20c'), axis=1)","8a756606":"def street_addr(x):\n    street=x.split(' ')\n    return (''.join(street[-1]))\n\ntrain_data['Address_Type'] = train_data['Address'].apply(lambda x:street_addr(x))\ntest_data['Address_Type'] = test_data['Address'].apply(lambda x:street_addr(x))\n\nfor x in [train_data,test_data]:\n    x['is_street'] = (x['Address_Type'] == 'ST')\n    x['is_avenue'] = (x['Address_Type'] == 'AV')\n\ntrain_data['is_street'] = train_data['is_street'].apply(lambda x:int(x))\ntrain_data['is_avenue'] = train_data['is_avenue'].apply(lambda x:int(x))\n\ntest_data['is_avenue'] = test_data['is_avenue'].apply(lambda x:int(x))\ntest_data['is_street'] = test_data['is_street'].apply(lambda x:int(x))","bb93d2a2":"def is_block(x):\n    if 'Block' in x:\n        return 1\n    else:\n        return 0\n\ntrain_data['is_block'] = train_data['Address'].apply(lambda x:is_block(x)) \ntest_data['is_block'] = test_data['Address'].apply(lambda x:is_block(x)) ","b4c5eb67":"train_data.head(20)","9621da7e":"category = LabelEncoder()\ntrain_data['Category'] = category.fit_transform(train_data.Category)","a22e913b":"# codifica outras features categoricas, incluindo-as como novas colunas no dataframe\nfeature_cols =['DayOfWeek', 'PdDistrict']\ntrain_data = pd.get_dummies(train_data, columns=feature_cols)\ntest_data = pd.get_dummies(test_data, columns=feature_cols)","43fe1aff":"# N\u00f3s n\u00e3o precisaremos das colunas abaixo, motivo pelo qual irems descart\u00e1-las.\ntrain_data = train_data.drop(['Dates', 'Address', 'Address_Type', 'Resolution'], axis = 1)\ntrain_data = train_data.drop(['Descript'], axis = 1)\ntest_data = test_data.drop(['Address','Address_Type', 'Dates'], axis = 1)","ff98102d":"train_data.head(5)","14143291":"test_data.head(5)","fd1bfa2c":"feature_cols = [x for x in train_data if x!='Category']\nX = train_data[feature_cols]\ny = train_data['Category']\nX_train, x_test,y_train, y_test = train_test_split(X, y)","5ca6d2f9":"del X\ndel y\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nx_test = scaler.transform(x_test)\n\nnormalizer = Normalizer()\nX_train = normalizer.fit_transform(X_train)\nx_test = normalizer.transform(x_test)\n","4a018f9a":"#ros = RandomOverSampler(random_state=42, sampling_strategy='not majority')\n#X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n\n#sm = SMOTE(random_state=42, k_neighbors=3)\n#X_resampled, y_resampled = sm.fit_resample(X_train, y_train)\n\n#ada = ADASYN(random_state=42, n_neighbors=4)\n#X_resampled, y_resampled = ada.fit_resample(X_train, y_train)\n\n#from collections import Counter\n\n#print('shape do dataset original %s' % Counter(y_train))\n#print('shape do dataset com oversampling %s' % Counter(y_resampled))\n\n#print(sorted(Counter(y_resampled).items()))","09cd45fc":"random_forest = RandomForestClassifier(n_estimators=100, max_depth=23)","c5c8725d":"random_forest.fit(X_train, y_train.ravel())\n#random_forest.fit(X_resampled, y_resampled)\npred = random_forest.predict(x_test)\nprint(\"accuracy_score: \", accuracy_score(pred,y_test))\nprint(\"f1_score_weighted\", f1_score(pred,y_test, average='weighted'))","9df9fe30":"#X_test =test_data.drop(['Id'], axis = 1)\npredicted_sub = random_forest.predict_proba(test_data.drop(['Id'], axis = 1))\nsubmission_results = pd.DataFrame(predicted_sub, columns=category.classes_)\nsubmission_results.to_csv('submission.csv', index_label = 'Id')","99f8513b":"# Descri\u00e7\u00e3o dos dados\n\nNossos dados cont\u00e9m registros sobre a ocorr\u00eancia de crimes em San Francisco e s\u00e3o divididos em dois arquivos: treino e teste, sendo que o primeiro conjunto possui **877982** exemplos. Trata-se de um dataset ruidoso e que n\u00e3o \u00e9 f\u00e1cil, o que aumenta o desafio e exige mais destreza de quem est\u00e1 modelando o problema. Isto contribui em grande medida para o enriquecimento das habilidades de qualquer cientista de dados iniciante.\n\n\n* **Dates** - timestamp of the crime incident\n* **Category** - category of the crime incident (only in train.csv). This is the target variable you are going to predict.\n* **Descript** - detailed description of the crime incident (only in train.csv)\n* **DayOfWeek** - the day of the week\n* **PdDistrict** - name of the Police Department District\n* **Resolution** - how the crime incident was resolved (only in train.csv)\n* **Address** - the approximate street address of the crime incident \n* **X** - Longitude\n* **Y** - Latitude\n","c985fa41":"H\u00e1 uma grande quantidade de incidentes cujos endere\u00e7os associados cont\u00e9m este termo **\"Block\"** no endere\u00e7o. N\u00f3s podemos criar mais uma feature categ\u00f3rica a partir disto.","926bcfc7":"# Codificando outras features categoricas\nNosso conjunto de dados possui outras features que tamb\u00e9m s\u00e3o categ\u00f3ricas, como \u00e9 o caso dos dias da semana nos quais ocorrem os incidentes, bem como o departamento de pol\u00edcia associado.  Como estas informa\u00e7\u00f5es est\u00e3o na forma de texto, n\u00f3s iremos codific\u00e1-las numericamente.","b3eae24a":"N\u00famero de crimes por lacalidade (limitamos em 1000 registros)","205e392b":"# Extraindo features a partir das datas\n\nDatas e hor\u00e1rios podem ser bastante informativas. Para o nosso caso, iremos \"quebrar\" as datas em seu componentes mais elementares, de modo a vermos o que determinados dias ou hor\u00e1rios no dizem a respeito da ocorr\u00eancia de crimes. Um outro efeito disto \u00e9 que esta pr\u00e1tica vai aumentar a quantidade de features dispon\u00edveis para o algoritmo. Se tomarmos como exemplo a data **2018-10-28 23:53:00**, n\u00f3s poder\u00edamos quebrar esta data em:\n\n* **ano:** 2018\n* **m\u00eas:** 10\n* **dia:** 28\n* **hora:** 23h\n* **minutos:** 53\n\nNa pr\u00e1tica, isto nos daria mais 5 colunas em nosso dataframe, aumentando a quantidade de informa\u00e7\u00f5es.","5ef679cc":"# Codificando os r\u00f3tulos\n\n\u00c0 princ\u00edpio, as categorias est\u00e3o representadas numa forma textual, ou seja, na forma de dados discretos. Contudo, estas informa\u00e7\u00f5es precisam de uma representa\u00e7\u00e3o num\u00e9rica para que nosso modelo possa trabalhar com elas. N\u00f3s faremos exatamente isto com os dados da coluna **Category**.","fd2b67da":"# Submiss\u00e3o","205e0d24":"Percentual de incidentes por endere\u00e7o","e2ad51d1":"# Extraindo features a partir dos endere\u00e7os\n\n\u00c9 poss\u00edvel que o fato de o crime ter ocorrido numa rua ou numa avenida possa ter um car\u00e1ter discriminat\u00f3rio forte. O fato de que avenidas costumam ser mais movimentadas e monitoradas do que muitas ruas pode inviabilizar a pr\u00e1tica de determinados crimes. \n\nN\u00f3s podemos extrair esta informa\u00e7\u00e3o a partir da coluna **'Address'** presente em nosso Dataframe. A string que corresponde ao endere\u00e7o cont\u00e9m algumas abrevia\u00e7\u00f5es. Entre estas abrevia\u00e7\u00f5es n\u00f3s temos:\n\n* **ST**: Abrevia\u00e7\u00e3o para Street\n* **AV**: Abrevia\u00e7\u00e3o para Avenue\n\nN\u00f3s iremos extrair estas informa\u00e7\u00f5es, transform\u00e1-las em features categ\u00f3ricas e, em seguida, adicionar duas colunas no Dataframe para acomodar estas infos categ\u00f3ricas. Basicamente, estas colunas indicar\u00e3o se o crime ocorreu numa rua ou numa avenida.","5a0c2e24":"# Descartando colunas que n\u00e3o utilizaremos\nN\u00f3s n\u00e3o precisaremos mais das colunas indicadas abaixo.","0ab819d4":"# Tratando classes desbalanceadas\n\nclasses desbalanceadas introduzem um vi\u00e9s nos modelos, com forte tend\u00eancia ao favorecimento da classe majorit\u00e1ria. Abaixo n\u00f3s tentamos resolver este problema por meio de uma t\u00e9cnica de *oversamplig*, que gera dados sint\u00e9ticos com bases nos vizinhos mais pr\u00f3ximos de determinados segmentos da classe minorit\u00e1ria.\n\n**atualiza\u00e7\u00e3o**\n\nNenhum dos m\u00e9todos de oversampling testados abaixo trouxe melhora no desempenho do classificador. Undersampling n\u00e3o seria uma boa alternativa, considerando que perder\u00edamos informa\u00e7\u00e3o de mais das outras classes para balancear estes r\u00f3tulos de acordo com as classes minorit\u00e1rias.","0a202d50":"# Extraindo features a partir das coordenadas\nVamos compor uma nova coluna no Dataframe apartir do produto entre as features X e Y, que correspondem aos valores de latitude e longitude, respectivamente.","f346ecd0":"# Importando o dataset","4146de58":"# Normaliza\u00e7\u00e3o\n\nNormalizar e padronizar os dados nos previne contra problemas envolvendo as escalas dos n\u00fameros. Al\u00e9m disso, vai permitir que nossos dados tenham as propriedades de uma distribui\u00e7\u00e3o normal padr\u00e3o, com m\u00e9dia **0** e desvio padr\u00e3o **1**. Trata-se de um procedimento, a bem dizer, mandat\u00f3rio em muitas tarefas envolvendo an\u00e1lise de dados.","b1b71fe4":"N\u00famero de crimes que ocorreram durante cada m\u00eas e ao longo dos anos registrados no conjunto de dados. ","aac71d97":"Features altamnte correlacionadas deveriam ser evitadas, j\u00e1 que fornecem pouca informa\u00e7\u00e3o extra. Neste caso, podemos checar a correla\u00e7\u00e3o entre features num\u00e9ricas.\n\n**Atualiza\u00e7\u00e3o:** h\u00e1 um correla\u00e7\u00e3o muito fraca entre as features, como pode ser visto abaixo. ","a4f84f91":"## Random Forest\n\nRandom Forest \u00e9 um algor\u00edtmo baseado em ensemble bastante popular, que cria v\u00e1rios classificadores, baseados em \u00e1rvore de decis\u00e3o, em cima dos dados de treino e combina todas as sa\u00eddas destes classificadores para obter uma acur\u00e1cia mais est\u00e1vel.\n\nOutras abordagens testadas aqui, e que n\u00e3o se sa\u00edram melhor do que a Random Forest, foram:\n1. SVM\n2. Multi layer Perceptron\n3. Gradient Boosting\n4. Regress\u00e3o Log\u00edstica\n\nApesar de n\u00f3s estarmos usando a m\u00e9trica acur\u00e1cia (abaixo), \u00e9 bom lembrar que ela n\u00e3o \u00e9 uma medida de desempenho adequada para modelos que lidam com classes desbalanceadas, j\u00e1 que trata todas as classes com igual import\u00e2ncia.\n\n**Melhor score alcan\u00e7ado:**\n\n* **accuracy_score**:  0.3242495888626186\n* **f1_score_weighted** 0.36493050029624635*\n\n**Melhor score p\u00fablico:** 2.71003\n\n**Score p\u00fablico do l\u00edder na competi\u00e7\u00e3o:** 1.95936","de56c441":"# Visualiza\u00e7\u00f5es\nPelas visualiza\u00e7\u00f5es \u00e9 poss\u00edvel notar que os crimes no distrito de  **SOUTHERN** t\u00eam uma frequ\u00eancia mais elevada do que nos outros distritos. Al\u00e9m disso, **entre 01:00 am e 07:00 am os crimes s\u00e3o menos frequentes**. Hor\u00e1rios de **maior pico** est\u00e3o **entre 17:00 e 18:00**.\n\n**Classes desbalanceadas** \n\nUm problema patente pode ser notado com rela\u00e7\u00e3o \u00e0 distribui\u00e7\u00e3o das classes ao longo do conjunto de aprendizagem, onde se pode notar que est\u00e3o **bastante desbalanceadas**. H\u00e1 classes com menos de 10 exemplos, enquanto h\u00e1 outras com 126182, 174900, etc . Classes desbalanceadas introduzem um vi\u00e9s no modelo, que ter\u00e1 a tend\u00eancia de favorecer as classes majorit\u00e1rias.\n\nEliminar as classes menos representativas poder\u00eda resultar em um modelo mais est\u00e1vel. O problema disso \u00e9 que a Kaggle n\u00e3o computaria o nosso score, pois espera que todas as classes estejam presentes no output do nosso modelo.","6262eb4e":"# Modelagem","a75e679d":"**LARCENY\/THEFT** \u00e9 a categoria de crimes com maior n\u00famero de ocorr\u00eancias. Onde estas ocorr\u00eancias mais se concentram?","8a396462":"# Fun\u00e7\u00e3o para plot de alguns gr\u00e1ficos"}}