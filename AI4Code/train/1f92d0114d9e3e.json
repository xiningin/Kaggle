{"cell_type":{"8414210b":"code","b39e9fef":"code","039a17e5":"code","9f56477a":"code","7f33dba9":"code","2df79ca7":"code","f106f62c":"code","a505509d":"code","d6ce70cf":"code","bd1878d1":"code","c9efd15e":"code","2fce009d":"code","e8084906":"code","cddb6cea":"code","77f943b9":"code","cf60239e":"code","2922afad":"code","43a25d90":"code","db817aca":"code","299a87c1":"code","099725e0":"code","2cfe1c0e":"code","3f92891b":"code","850189b4":"code","b3f40ccc":"code","27643a20":"code","dcc17ab9":"code","532d39a0":"code","c3491fad":"code","22915680":"code","e7f35838":"code","33c0cb05":"code","86964c6d":"code","31fa2303":"code","db72b684":"code","48a122d1":"code","362dd1e2":"code","81c47747":"code","bcc5d1a4":"code","4b3e083b":"code","8cdf236e":"code","23dc754a":"code","645abd06":"code","12be6b80":"code","b229f32d":"code","3b172fae":"code","d266810c":"code","e5365f6b":"markdown","12d0a939":"markdown","76b4c45f":"markdown","8c488b48":"markdown","d5edab8c":"markdown","ee3e079b":"markdown"},"source":{"8414210b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b39e9fef":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('..\/input\/car-insurance-data\/Car_Insurance_Claim.csv')","039a17e5":"dataset.head()","9f56477a":"dataset.describe()","7f33dba9":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\ndataset['ANNUAL_MILEAGE'] = imputer.fit_transform(np.array(dataset['ANNUAL_MILEAGE']).reshape(-1,1))\ndataset['CREDIT_SCORE'] = imputer.fit_transform(np.array(dataset['CREDIT_SCORE']).reshape(-1,1))","2df79ca7":"sns.histplot(dataset.DUIS)","f106f62c":"#Lets explore DUIS column\ndataset.SPEEDING_VIOLATIONS.describe()","a505509d":"dataset.PAST_ACCIDENTS.describe()","d6ce70cf":"dataset = pd.get_dummies(dataset, columns=['VEHICLE_OWNERSHIP', 'CHILDREN', 'MARRIED'], drop_first=True)","bd1878d1":"dataset","c9efd15e":"sns.histplot(dataset['CHILDREN_1.0'])","2fce009d":"sns.histplot(dataset.CREDIT_SCORE)\n","e8084906":"dataset['VEHICLE_OWNERSHIP_1.0'].groupby(dataset['VEHICLE_OWNERSHIP_1.0']).count().plot.pie(figsize=(5,5), autopct='%1.1f%%',startangle = 30.)","cddb6cea":"dataset['RACE'].groupby(dataset['RACE']).count().plot.pie(figsize=(5,5), autopct='%1.1f%%',startangle = 30.)","77f943b9":"dataset['AGE'].groupby(dataset['AGE']).count().plot.bar()","cf60239e":"dataset['CHILDREN_1.0'].groupby(dataset['CHILDREN_1.0']).count().plot.pie(figsize = (5,5), autopct='%1.1f%%',startangle = 30.)","2922afad":"dataset['GENDER'].groupby(dataset['GENDER']).count().plot.pie(figsize=(5,5), autopct='%1.1f%%',startangle = 30.)","43a25d90":"dataset['DRIVING_EXPERIENCE'].groupby(dataset['DRIVING_EXPERIENCE']).count().plot.bar(title='Distributions of driving experience in years')","db817aca":"dataset['EDUCATION'].groupby(dataset['EDUCATION']).count().plot.bar()","299a87c1":"dataset['INCOME'].groupby(dataset['INCOME']).count().plot.bar()","099725e0":"dataset['VEHICLE_YEAR'].groupby(dataset['VEHICLE_YEAR']).count().plot.pie(figsize=(5,5), autopct='%1.1f%%',startangle = 30.)","2cfe1c0e":"dataset['VEHICLE_TYPE'].groupby(dataset['VEHICLE_TYPE']).count().plot.pie(figsize=(5,5), autopct='%1.1f%%',startangle = 30.)","3f92891b":"sns.histplot(dataset['ANNUAL_MILEAGE'])","850189b4":"sns.histplot(dataset['SPEEDING_VIOLATIONS'])","b3f40ccc":"sns.histplot(dataset['PAST_ACCIDENTS'])","27643a20":"dataset['POSTAL_CODE'].groupby(dataset['POSTAL_CODE']).count().plot.bar()","dcc17ab9":"dataset.columns","532d39a0":"#Lets encode all the categorical data\n\"\"\"\n1. AGE\n2. GENDER\n3. DRIVING_EXPERIENCE\n4. EDUCATION\n5. INCOME\n6. VEHICLE_YEAR\n\"\"\"\ndataset = pd.get_dummies(dataset, columns = ['AGE', 'GENDER', 'RACE', 'DRIVING_EXPERIENCE', 'EDUCATION','INCOME','VEHICLE_YEAR'], drop_first=True)","c3491fad":"dataset = pd.get_dummies(dataset, columns=['POSTAL_CODE'], drop_first=True)","22915680":"dataset.head()","e7f35838":"df = dataset.drop(columns = ['ID', 'VEHICLE_TYPE','RACE_minority' ])","33c0cb05":"df.drop(columns = ['OUTCOME']).corrwith(df.OUTCOME).plot.bar(figsize = (20,10), grid = True, rot = 45, title='Correlation with the response variable')","86964c6d":"corr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","31fa2303":"from sklearn.preprocessing import MinMaxScaler\nminmxscaler= MinMaxScaler()\ndf['ANNUAL_MILEAGE']  = minmxscaler.fit_transform(np.array(df['ANNUAL_MILEAGE']).reshape(-1,1))","db72b684":"df.head()","48a122d1":"corr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","362dd1e2":"# Import SelectKBest, chi2(score function for classification), f_regression (score function for regression)\nfrom sklearn.feature_selection import SelectKBest, chi2\nX = df.drop(columns = ['OUTCOME'])\ny = df['OUTCOME']","81c47747":"fit=SelectKBest(score_func=chi2,k=10).fit(X,y)","bcc5d1a4":"print(fit.scores_)","4b3e083b":"np.sort(fit.scores_)","8cdf236e":"columns = X.columns[fit.get_support()]","23dc754a":"X = X[columns]","645abd06":"X.head()","12be6b80":"#Here we will chose many classifier and test them among the first dataset that is df_canncer\n\n#splitting the X and y into training set and test set\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\n\nEvaluetionReport = {}\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n\nclassifier_logReg = LogisticRegression(random_state = 0)\nclassifier_logReg.fit(X_train, y_train)\ny_pred_logReg = classifier_logReg.predict(X_test)\ncm_logReg = confusion_matrix(y_test, y_pred_logReg)\nacc_logReg = accuracy_score(y_test, y_pred_logReg)\nEvaluetionReport['Logistic_Regression'] = [cm_logReg, acc_logReg]\n\n#KNN\n\nclassifier_KNN = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier_KNN.fit(X_train, y_train)\ny_pred_KNN = classifier_KNN.predict(X_test)\ncm_KNN = confusion_matrix(y_test, y_pred_KNN)\nacc_KNN = accuracy_score(y_test, y_pred_KNN)\nEvaluetionReport['KNN'] = [cm_KNN, acc_KNN]\n\n#SVM\n\nclassifier_SVM = SVC(kernel = 'linear', random_state = 0)\nclassifier_SVM.fit(X_train, y_train)\ny_pred_SVM = classifier_SVM.predict(X_test)\ncm_SVM = confusion_matrix(y_test, y_pred_SVM)\nacc_SVM = accuracy_score(y_test, y_pred_SVM)\nEvaluetionReport['SVM'] = [cm_SVM, acc_SVM]\n\n#KernelSVM\n\nclassifier_KSVM = SVC(kernel = 'rbf', random_state = 0)\nclassifier_KSVM.fit(X_train, y_train)\ny_pred_KSVM = classifier_KSVM.predict(X_test)\ncm_KSVM = confusion_matrix(y_test, y_pred_KSVM)\nacc_KSVM = accuracy_score(y_test, y_pred_KSVM)\nEvaluetionReport['KSVM'] = [cm_KSVM, acc_KSVM]\n\n#NaiveBayes classifier\nclassifier_NB = GaussianNB()\nclassifier_NB.fit(X_train, y_train)\ny_pred_NB = classifier_NB.predict(X_test)\ncm_NB = confusion_matrix(y_test, y_pred_NB)\nacc_NB = accuracy_score(y_test, y_pred_NB)\nEvaluetionReport['NB'] = [cm_NB, acc_NB]\n\n#RandomForest Classifier\nclassifier_RFC = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier_RFC.fit(X_train, y_train)\ny_pred_RFC = classifier_RFC.predict(X_test)\ncm_RFC = confusion_matrix(y_test, y_pred_RFC)\nacc_RFC = accuracy_score(y_test, y_pred_RFC)\nEvaluetionReport['RFC'] = [cm_RFC, acc_RFC]\n\n#Decision Tree Classifier\nclassifier_DTC = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier_DTC.fit(X_train, y_train)\ny_pred_DTC = classifier_DTC.predict(X_test)\ncm_DTC = confusion_matrix(y_test, y_pred_DTC)\nacc_DTC = accuracy_score(y_test, y_pred_DTC)\nEvaluetionReport['DTC'] = [cm_DTC, acc_DTC]\n\n#XGBoost Classifier\nclassifier_XGB = XGBClassifier()\nclassifier_XGB.fit(X_train, y_train)\ny_pred_XGB = classifier_XGB.predict(X_test)\ncm_XGB = confusion_matrix(y_test, y_pred_XGB)\nacc_XGB = accuracy_score(y_test, y_pred_XGB)\nEvaluetionReport['XGBoost'] = [cm_XGB, acc_XGB]\n\nEvaluetionReport\n","b229f32d":"for evaluationKey,evaluationVal in EvaluetionReport.items():\n    sns.heatmap(evaluationVal[0], annot = True)\n    plt.xlabel(evaluationKey)\n    plt.figure(figsize=(5,5))\n    print('\\n')","3b172fae":"EvaluationDF= pd.DataFrame(EvaluetionReport)","d266810c":"EvaluationDF","e5365f6b":"**Nearly 70% of the population has kids**","12d0a939":"**We can see the distribution of people in age groups in the dataset**","76b4c45f":"****Now we are going to visualise each and every feature and learn some insight about that****","8c488b48":"**Credit score is evenly distributed and is a normal curve**","d5edab8c":"**Since our dataset contains 90% of the RACE as majority we might not need this feature as a predictor, but first we need to see how much it correlates to the outcome feature**","ee3e079b":"Now we are going to see the relation between outcome variable and the features"}}