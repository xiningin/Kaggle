{"cell_type":{"a8fabb24":"code","e9f487a5":"code","93de9672":"code","d7c0d98f":"code","50446a7f":"code","99ab0b70":"code","5d9b5e82":"code","2d7cfdad":"code","bd192fa6":"code","db749e1c":"code","9484ea6b":"code","4369b632":"code","2ffabdd1":"markdown","2078469f":"markdown"},"source":{"a8fabb24":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nimport fastai\nfrom fastai.train import Learner\nfrom fastai.train import DataBunch\nfrom fastai.callbacks import GeneralScheduler, TrainingPhase\nfrom fastai.basic_data import DatasetType\nimport fastprogress\nfrom fastprogress import force_console_behavior\n\nimport numpy as np\nfrom pprint import pprint\nimport pandas as pd\nimport os\nimport time\nimport gc\nimport random\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\nfrom keras.preprocessing import text, sequence\nimport torch\nfrom torch import nn\nfrom torch.utils import data\nfrom torch.nn import functional as F\n\nimport torch.utils.data\nfrom tqdm import tqdm\nimport warnings\nfrom nltk.tokenize.treebank import TreebankWordTokenizer\nfrom scipy.stats import rankdata\n\nfrom gensim.models import KeyedVectors\n\nimport copy","e9f487a5":"def convert_lines(example, max_seq_length,tokenizer):\n    max_seq_length -=2\n    all_tokens = []\n    longer = 0\n    for text in tqdm(example):\n        tokens_a = tokenizer.tokenize(text)\n        if len(tokens_a)>max_seq_length:\n            tokens_a = tokens_a[:max_seq_length]\n            longer += 1\n        one_token = tokenizer.convert_tokens_to_ids([\"[CLS]\"]+tokens_a+[\"[SEP]\"])+[0] * (max_seq_length - len(tokens_a))\n        all_tokens.append(one_token)\n    return np.array(all_tokens)\n\ndef is_interactive():\n    return 'SHLVL' not in os.environ\n\ndef seed_everything(seed=123):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \ndef get_coefs(word, *arr):\n    return word, np.asarray(arr, dtype='float32')\n\n\ndef load_embeddings(path):\n    #with open(path,'rb') as f:\n    emb_arr = KeyedVectors.load(path)\n    return emb_arr\n\ndef build_matrix(word_index, path):\n    embedding_index = load_embeddings(path)\n    embedding_matrix = np.zeros((max_features + 1, 300))\n    unknown_words = []\n    \n    for word, i in word_index.items():\n        if i <= max_features:\n            try:\n                embedding_matrix[i] = embedding_index[word]\n            except KeyError:\n                try:\n                    embedding_matrix[i] = embedding_index[word.lower()]\n                except KeyError:\n                    try:\n                        embedding_matrix[i] = embedding_index[word.title()]\n                    except KeyError:\n                        unknown_words.append(word)\n    return embedding_matrix, unknown_words\n\ndef sigmoid(x):\n    return 1 \/ (1 + np.exp(-x))\n\nclass SpatialDropout(nn.Dropout2d):\n    def forward(self, x):\n        x = x.unsqueeze(2)    # (N, T, 1, K)\n        x = x.permute(0, 3, 2, 1)  # (N, K, 1, T)\n        x = super(SpatialDropout, self).forward(x)  # (N, K, 1, T), some features are masked\n        x = x.permute(0, 3, 2, 1)  # (N, T, 1, K)\n        x = x.squeeze(2)  # (N, T, K)\n        return x\n\ndef train_model(learn,output_dim,lr=0.001,\n                batch_size=512, n_epochs=5):\n    \n    n = len(learn.data.train_dl)\n    phases = [(TrainingPhase(n).schedule_hp('lr', lr * (0.6**(i)))) for i in range(n_epochs)]\n    sched = GeneralScheduler(learn, phases)\n    learn.callbacks.append(sched)\n    for epoch in range(n_epochs):\n        learn.fit(1)\n        learn.save('model_1_{}'.format(epoch))\n        \n    return learn\n\ndef handle_punctuation(x):\n    x = x.translate(remove_dict)\n    x = x.translate(isolate_dict)\n    return x\n\ndef handle_contractions(x):\n    x = tokenizer.tokenize(x)\n    return x\n\ndef fix_quote(x):\n    x = [x_[1:] if x_.startswith(\"'\") else x_ for x_ in x]\n    x = ' '.join(x)\n    return x\n\ndef preprocess(x):\n    x = handle_punctuation(x)\n    x = handle_contractions(x)\n    x = fix_quote(x)\n    return x\n\nclass SequenceBucketCollator():\n    def __init__(self, choose_length, sequence_index, length_index, label_index=None):\n        self.choose_length = choose_length\n        self.sequence_index = sequence_index\n        self.length_index = length_index\n        self.label_index = label_index\n        \n    def __call__(self, batch):\n        batch = [torch.stack(x) for x in list(zip(*batch))]\n        \n        sequences = batch[self.sequence_index]\n        lengths = batch[self.length_index]\n        \n        length = self.choose_length(lengths)\n        mask = torch.arange(start=maxlen, end=0, step=-1) < length\n        padded_sequences = sequences[:, mask]\n        \n        batch[self.sequence_index] = padded_sequences\n        \n        if self.label_index is not None:\n            return [x for i, x in enumerate(batch) if i != self.label_index], batch[self.label_index]\n    \n        return batch\n\n    \nclass SoftmaxPooling(nn.Module):\n    def __init__(self, dim=1):\n        super(self.__class__, self).__init__()\n        self.dim = dim\n        \n    def forward(self, x):\n        return (x * x.softmax(dim=self.dim)).sum(dim=self.dim)\n\n\n# class AttentivePooling(nn.Module):\n#     def __init__(self, dim=-1, input_size=128, hidden_size=32):\n#         super(self.__class__, self).__init__()\n#         self.dim = dim\n#         self.input_size = input_size\n#         self.hidden_size = hidden_size\n#         self.nn_attn = nn.Sequential(\n#             nn.Linear(input_size, hidden_size),\n#             nn.ReLU(),\n#             nn.Linear(hidden_size, 1)\n#         )\n        \n#     def forward(self, x):\n#         return (x * torch.transpose(self.nn_attn(torch.transpose(x, 2, 1)), 1, 2).softmax(dim=self.dim)).sum(dim=self.dim)\n\n\nclass NeuralNet(nn.Module):\n    def __init__(self, embedding_matrix, num_aux_targets):\n        super(NeuralNet, self).__init__()\n        embed_size = embedding_matrix.shape[1]\n        \n        self.embedding = nn.Embedding(max_features, embed_size)\n        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n        self.embedding.weight.requires_grad = False\n        self.embedding_dropout = SpatialDropout(0.3)\n        \n        self.lstm1 = nn.LSTM(embed_size, LSTM_UNITS, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(LSTM_UNITS * 2, LSTM_UNITS, bidirectional=True, batch_first=True)\n    \n#         self.linear1 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n#         self.linear2 = nn.Linear(DENSE_HIDDEN_UNITS, DENSE_HIDDEN_UNITS)\n        \n        self.linear_out = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n            nn.Linear(DENSE_HIDDEN_UNITS, 1)\n        )\n        \n        self.linear_aux_out = nn.Sequential(\n            nn.Dropout(0.5),\n            nn.BatchNorm1d(DENSE_HIDDEN_UNITS),\n            nn.Linear(DENSE_HIDDEN_UNITS, num_aux_targets)\n        )\n        \n        self.softmaxpool = SoftmaxPooling()\n#         self.attnpool = AttentivePooling()\n        \n#         self.conv_extractor1 = nn.Sequential(\n#             nn.Dropout(0.1),\n#             nn.Conv1d(embed_size, 64, kernel_size=3, padding=1),\n#             nn.BatchNorm1d(64),\n#             nn.ELU()\n#         )\n        \n#         self.conv_extractor2 = nn.Sequential(\n#             nn.Dropout(0.1),\n#             nn.Conv1d(embed_size, 64, kernel_size=5, padding=2),\n#             nn.BatchNorm1d(64),\n#             nn.ELU()\n#         )\n        \n    def forward(self, x, lengths=None):\n        h_embedding = self.embedding(x.long())\n        h_embedding = self.embedding_dropout(h_embedding)\n        \n        h_lstm1, _ = self.lstm1(h_embedding)\n        h_lstm2, _ = self.lstm2(h_lstm1)\n        \n        # global average pooling\n        avg_pool = torch.mean(h_lstm2, 1)\n        # global max pooling\n        max_pool, _ = torch.max(h_lstm2, 1)\n        soft_pool = self.softmaxpool(h_lstm2)\n        # attn_pool = self.attnpool(h_lstm2)\n        \n#         conv_feat1 = torch.transpose(self.conv_extractor1(torch.transpose(h_lstm1, 2, 1)), 1, 2)\n        \n        h_conc = torch.cat((max_pool, avg_pool, soft_pool), 1)\n#         h_conc_linear1  = F.relu(self.linear1(h_conc))\n#         except:\n#             print(h_lstm2.size())\n#             print(soft_pool.size())\n#             print(h_conc.size())\n#             raise RuntimeError('hello there')\n#         h_conc_linear2  = F.relu(self.linear2(h_conc))\n        \n        hidden = h_conc\n        \n        result = self.linear_out(hidden)\n        aux_result = self.linear_aux_out(hidden)\n        out = torch.cat([result, aux_result], 1)\n        \n        return out\n    \ndef custom_loss(data, targets):\n    bce_loss_1 = nn.BCEWithLogitsLoss(weight=targets[:,1:2])(data[:,:1],targets[:,:1])\n    bce_loss_2 = nn.BCEWithLogitsLoss()(data[:,1:],targets[:,2:])\n    return (bce_loss_1 * loss_weight) + bce_loss_2\n\ndef reduce_mem_usage(df):\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\ndef ensemble_predictions(predictions, weights, type_=\"linear\"):\n    assert np.isclose(np.sum(weights), 1.0)\n    if type_ == \"linear\":\n        res = np.average(predictions, weights=weights, axis=0)\n    elif type_ == \"harmonic\":\n        res = np.average([1 \/ p for p in predictions], weights=weights, axis=0)\n        return 1 \/ res\n    elif type_ == \"geometric\":\n        numerator = np.average(\n            [np.log(p) for p in predictions], weights=weights, axis=0\n        )\n        res = np.exp(numerator \/ sum(weights))\n        return res\n    elif type_ == \"rank\":\n        res = np.average([rankdata(p) for p in predictions], weights=weights, axis=0)\n        return res \/ (len(res) + 1)\n    return res","93de9672":"warnings.filterwarnings(action='once')\ndevice = torch.device('cuda')\nSEED = 1234\nBATCH_SIZE = 512\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\n\ntqdm.pandas()\nCRAWL_EMBEDDING_PATH = '..\/input\/gensim-embeddings-dataset\/crawl-300d-2M.gensim'\nPARAGRAM_EMBEDDING_PATH = '..\/input\/gensim-embeddings-dataset\/paragram_300_sl999.gensim'\nNUM_MODELS = 1\nLSTM_UNITS = 128\nDENSE_HIDDEN_UNITS = 768\nif not is_interactive():\n    def nop(it, *a, **k):\n        return it\n\n    tqdm = nop\n\n    fastprogress.fastprogress.NO_BAR = True\n    master_bar, progress_bar = force_console_behavior()\n    fastai.basic_train.master_bar, fastai.basic_train.progress_bar = master_bar, progress_bar\n\nseed_everything()","d7c0d98f":"x_train = pd.read_csv('..\/input\/jigsawbiaspreprocessed\/x_train.csv', header=None)[0].astype('str')\ny_aux_train = np.load('..\/input\/jigsawbiaspreprocessed\/y_aux_train.npy')\ny_train = np.load('..\/input\/jigsawbiaspreprocessed\/y_train.npy')\n\nloss_weight = 3.209226860170181\n\nmax_features = 400000","50446a7f":"train = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\nannot_idx = train[train['identity_annotator_count'] > 0].sample(n=48660, random_state=13).index\nnot_annot_idx = train[train['identity_annotator_count'] == 0].sample(n=48660, random_state=13).index\nx_val_idx = list(set(annot_idx).union(set(not_annot_idx)))\nx_train_idx = list(set(x_train.index) - set(x_val_idx))","99ab0b70":"X_train = x_train.loc[x_train_idx]\nY_train = y_train[x_train_idx]\nY_aux_train = y_aux_train[x_train_idx]\nX_val = x_train.loc[x_val_idx]\nY_val = y_train[x_val_idx]\nY_aux_val = y_aux_train[x_val_idx]","5d9b5e82":"tokenizer = text.Tokenizer(num_words = max_features, filters='',lower=False)","2d7cfdad":"tokenizer.fit_on_texts(list(X_train))\n\ncrawl_matrix, unknown_words_crawl = build_matrix(tokenizer.word_index, CRAWL_EMBEDDING_PATH)\nprint('n unknown words (crawl): ', len(unknown_words_crawl))\n\nparagram_matrix, unknown_words_paragram = build_matrix(tokenizer.word_index, PARAGRAM_EMBEDDING_PATH)\nprint('n unknown words (paragram): ', len(unknown_words_paragram))\n\nmax_features = max_features or len(tokenizer.word_index) + 1\nmax_features\n\nembedding_matrix = np.concatenate([crawl_matrix, paragram_matrix], axis=-1)\nprint(embedding_matrix.shape)\n\ndel crawl_matrix\ndel paragram_matrix\ngc.collect()\n\ny_train_torch = torch.tensor(np.hstack([Y_train, Y_aux_train]), dtype=torch.float32)\nX_train = tokenizer.texts_to_sequences(X_train)","bd192fa6":"# https:\/\/stackoverflow.com\/questions\/45735070\/keras-text-preprocessing-saving-tokenizer-object-to-file-for-scoring\nimport pickle\n\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","db749e1c":"lengths = torch.from_numpy(np.array([len(x) for x in X_train]))\n \nmaxlen = 300\nX_train_padded = torch.from_numpy(sequence.pad_sequences(X_train, maxlen=maxlen))","9484ea6b":"batch_size = 512\ntrain_dataset = data.TensorDataset(X_train_padded, lengths, y_train_torch)\nvalid_dataset = data.Subset(train_dataset, indices=[0, 1])\n\ntrain_collator = SequenceBucketCollator(lambda lenghts: lenghts.max(), \n                                        sequence_index=0, \n                                        length_index=1, \n                                        label_index=2)\n\ntrain_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=train_collator)\nvalid_loader = data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=train_collator)\n\ndatabunch = DataBunch(train_dl=train_loader, valid_dl=valid_loader, collate_fn=train_collator)","4369b632":"for model_idx in range(NUM_MODELS):\n    print('Model ', model_idx)\n    seed_everything(1 + model_idx)\n    model = NeuralNet(embedding_matrix, Y_aux_train.shape[-1])\n    learn = Learner(databunch, model, loss_func=custom_loss)\n    train_model(learn,output_dim=7)","2ffabdd1":"**LSTM Part**","2078469f":"Thanks for @christofhenkel @abhishek @iezepov for their great work:\n\nhttps:\/\/www.kaggle.com\/christofhenkel\/how-to-preprocessing-for-glove-part2-usage\nhttps:\/\/www.kaggle.com\/abhishek\/pytorch-bert-inference\nhttps:\/\/www.kaggle.com\/iezepov\/starter-gensim-word-embeddings"}}