{"cell_type":{"6acc332f":"code","c69ae7c1":"code","343e9ef1":"code","6caf6e1f":"code","1bd62673":"code","e879ebc0":"code","e323be82":"code","65869713":"code","70108c6d":"code","15695781":"code","a01c6fa4":"code","96b54431":"code","8284e16c":"code","388eac05":"code","7ac4c2a9":"code","92a91f4d":"code","25dc70ba":"code","207e9acc":"code","a513853e":"code","bd6ea425":"code","92a85e9a":"code","14b76206":"code","6e2a6f42":"code","9209a2e5":"code","001e901f":"code","0a2974a0":"code","e9beaace":"code","20fe3812":"code","9c6d6c34":"code","5925d01b":"markdown","d5c38f21":"markdown","5c66a88b":"markdown","41fdaf02":"markdown","67a7f29d":"markdown","096e03f6":"markdown","5273d2ec":"markdown","abded360":"markdown","d84ecedb":"markdown","5e842d4d":"markdown","3585d9bf":"markdown","2427eea1":"markdown","ff71dda9":"markdown","5c33b18e":"markdown","e1cf5b86":"markdown","a24ce912":"markdown","2cb2dda8":"markdown","f54550bf":"markdown","39bdbd40":"markdown","ff9384ea":"markdown","0419293d":"markdown","9109c752":"markdown","8857fde4":"markdown","7710e118":"markdown","43d4691e":"markdown","26fecc07":"markdown","5e59e49a":"markdown","961e9965":"markdown","44e9c40e":"markdown","66b82e7d":"markdown","a02de343":"markdown","4317caeb":"markdown","e5916cdd":"markdown","57fef34e":"markdown","9322f161":"markdown","deea3b3a":"markdown","0bea4f71":"markdown","bf2737bb":"markdown"},"source":{"6acc332f":"# handle tabular data and matrices \nimport numpy as np\nimport pandas as pd\n#for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#tensorflow for download dataset and some tensor operations\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.utils import to_categorical # convert oh-enc\n#model\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPool2D,BatchNormalization,AvgPool2D\n\n#Regularization\n#data augmentation\nfrom keras.preprocessing.image import ImageDataGenerator\n#earlystopping,Learning Rate decay\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping,LearningRateScheduler\n","c69ae7c1":"mnist = tf.keras.datasets.fashion_mnist \n\n(all_train_images, all_train_labels), (test_images, test_labels) = mnist.load_data()\n\n#divide train dataset as train\/dev for training\ntrain_images,train_labels = all_train_images[:50000], all_train_labels[:50000]\ndev_images, dev_labels = all_train_images[50000:-1],all_train_labels[50000:-1]","343e9ef1":"print(\"Shape of train_images: \",train_images.shape)\nprint(\"Shape of train_labels: \",train_labels.shape)","6caf6e1f":"img_w = train_images.shape[1] #cols\nimg_h = train_images.shape[2] #rows\nch = 1 #channels\nimg_shape = (img_w,img_h,ch)","1bd62673":"#Reshape images for converting dataframe\n\ndf_train_images = pd.DataFrame(train_images.reshape(-1,img_w*img_h*ch))\ndf_train_labels = pd.DataFrame(train_labels)\n\n#Let's look our datasets\n\ndf_train_images.info()\nprint(\"\\n-----\\n\")\ndf_train_labels.info()","e879ebc0":"df_train_images.describe()","e323be82":"#Just examine first 5 rows.\ndf_train_images.head()","65869713":"print(df_train_images.isna().any().sum())\nprint(\"\\n---\\n\")\nprint(df_train_labels.isna().any().sum())","70108c6d":"print(\"Labels: \",df_train_labels[0].unique())","15695781":"'''\n    0 = T-shirt\/top\n    1 = Trouser\n    2 = Pullover\n    3 = Dress\n    4 = Coat\n    5 = Sandal\n    6 = Shirt\n    7 = Sneaker\n    8 = Bag\n    9 = Ankle boot\n'''\nclothes_dict = {0:'T-shirt\/top', \n               1:'Trouser',\n               2:'Pullover',\n               3:'Dress',\n               4:'Coat',\n               5:'Sandal',\n               6:'Shirt',\n               7:'Sneaker',\n               8:'Bag',\n               9:'Ankle boot'}\n","a01c6fa4":"def get_cloth(index):\n    '''\n    Input:\n    index = index of cloth in dict.\n    \n    Output:\n    return value of the index in dict.\n    '''\n    \n    val = clothes_dict.get(index)\n    return val\n\n# Let's write them all for check.\n\nprint(\"\\t Clothes\\n-----------\")\nfor key in clothes_dict.keys():\n    print(key,\" --> \",get_cloth(key))\n    ","96b54431":"print(df_train_labels.value_counts())\n\n\n\nsns.countplot(x=0,data=df_train_labels)\nplt.xlabel(\"Labels\")\nplt.ylabel(\"Counts\")\nplt.title(\"distiribution of labels\")\nplt.show","8284e16c":"plt.figure(figsize=(10,10))\nfor i in range (9):\n  plt.subplot(3,3,i+1)\n  plt.xticks([])\n  plt.yticks([])\n  plt.grid(False)\n  plt.imshow(train_images[i], cmap=plt.cm.binary) #binary - black an white image\n  plt.xlabel(\"Label:{0} = {1}\".format(train_labels[i], clothes_dict[train_labels[i]]))\nplt.show()\n","388eac05":"train_images_reshaped = train_images.reshape(-1,img_w,img_h,ch)\ndev_images_reshaped = dev_images.reshape(-1,img_w,img_h,ch)\n\nprint(\"Train images shape: \",train_images_reshaped.shape)\nprint(\"Test images shape: \",dev_images_reshaped.shape)","7ac4c2a9":"data_generator = ImageDataGenerator(\n    featurewise_center = False,  \n    samplewise_center = False, \n    featurewise_std_normalization = False,\n    samplewise_std_normalization = False,\n    zca_whitening = False, \n    rotation_range = 15, \n    zoom_range = 0.1,\n    width_shift_range = 0.1, \n    height_shift_range = 0.1, \n    horizontal_flip = False,\n    vertical_flip = False,\n    \n)","92a91f4d":"#Normalization \n# x = (x-min) \/ (max-min) , min = 0,black , max = 255,white\n# x = (x - 0 ) \/ (255 - 0) = x\/255. \n\ntrain_images_scaled = train_images_reshaped.astype(\"float32\") \/ 255.\ndev_images_scaled = dev_images_reshaped.astype(\"float32\") \/ 255. \nprint(\"Scaled Training Images Shape: \"+str(train_images_scaled.shape))\nprint(\"Scaled Test Images Shape: \"+str(dev_images_scaled.shape))\n\n","25dc70ba":"#Apply augmentation.\ndata_generator.fit(train_images_scaled)","207e9acc":"#Remember we have 10 labels(classes).\nprint(\"\\tBefore One-hot encoding\")\nprint(\"Test Images Shape : \"+str(train_labels.shape)+\" Value:\"+str(train_labels))\nprint(\"Test Labels Shape : \"+str(dev_labels.shape)+\" Value:\"+str(dev_labels))\n\nprint(\"\\n\\tAfter One-hot encoding\")\n\ntrain_labels_oh_enc = to_categorical(train_labels,10)\ndev_labels_oh_enc = to_categorical(dev_labels,10)\n\nprint(\"Test Images Shape : \"+str(train_labels_oh_enc.shape)+\" Value:\"+str(train_labels_oh_enc[0]))\nprint(\"Test Labels Shape : \"+str(dev_labels_oh_enc.shape)+\" Value:\"+str(dev_labels_oh_enc[1]))\n\n","a513853e":"input_shape = img_shape\nbatch_size = 64\nepoch = 15","bd6ea425":"def base_layer(filter,input_shape):\n  model = Sequential([\n                      Conv2D(filters=filter,kernel_size=(3,3),padding='same',activation='relu',input_shape=input_shape)\n  ],name='InitLayer')\n  return model\ndef conv_layer(filter):\n  model = Sequential([\n                      Conv2D(filters=filter,kernel_size=(3,3),padding='same',activation='relu'),\n                      Conv2D(filters=filter,kernel_size=(3,3),padding='same',activation='relu'),\n                      BatchNormalization(),\n                      MaxPool2D(pool_size=(2,2))\n  ])\n  return model\ndef dense_layer(unit,dropout_rate):\n  model = Sequential([\n                      Dense(units=unit,activation='relu'),\n                      BatchNormalization(),\n                      Dropout(dropout_rate)\n  ])\n  return model\ndef output_layer(classes):\n  model = Sequential([\n                      Dense(classes, activation='softmax')\n  ],name='OutputLayer')\n  return model","92a85e9a":"def yeniModelOlustur(input_shape=img_shape):\n  model = Sequential([\n                      base_layer(16,input_shape),\n                      Conv2D(16,3,padding='same',activation='relu'),\n                      conv_layer(32),\n                      Dropout(0.1),\n                      conv_layer(64),\n                      Dropout(0.2),\n                      conv_layer(128),\n                      Dropout(0.25),\n                      conv_layer(256),\n                      Conv2D(256,3,padding='same',activation='relu'),\n                      Dropout(0.3),\n                      MaxPool2D(pool_size=(2,2),padding='same'),\n                      Flatten(),\n                      dense_layer(128,0.6),\n                      dense_layer(64,0.4),\n                      output_layer(10)\n  ])\n  return model","14b76206":"m = yeniModelOlustur()\nm.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\nm.summary()\n","6e2a6f42":"train_images_final, dev_images_final = train_images_scaled,dev_images_scaled\nhistory = m.fit(train_images_final,train_labels_oh_enc,\n                    batch_size=batch_size,\n                    epochs=epoch,\n                    verbose=1,\n                    validation_data=(dev_images_final,dev_labels_oh_enc))\n","9209a2e5":"test_labels_oh_enc = to_categorical(test_labels,10)\npredict_acc = m.evaluate(test_images.reshape(-1,28,28,1).astype(\"float32\")\/255.,test_labels_oh_enc)\nprint('\\n\\t  Accuracy Rates\\n'+\n      '\\t--------------------\\n'+\n      'Train: ',history.history[\"accuracy\"][-1],\n      '\\nDev: ',history.history['val_accuracy'][-1],\n      '\\nTest: ',predict_acc[1])","001e901f":"#let's visualize\nplt.figure(figsize=(6,4))\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"Loss-Epoch Relation\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Train\",\"Dev\"])\nplt.show()\n\nplt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy - Epoch Relation\")\nplt.legend([\"Train\",\"Dev\"])\nplt.ylim(0.8,1)\nplt.show()\n\n","0a2974a0":"checkpoint_fm = ModelCheckpoint(\"fashion_mnist_model.h5\", save_best_only=True)\n\nearly_stopping_fm = EarlyStopping(patience=10, restore_best_weights=True)\n","e9beaace":"#Learning Rate Decay as Exponential\ndef exponential_decay(learning_rate, decay_step):\n    def exponential_decay_fm(epoch):\n        return learning_rate * 0.1 **(epoch \/ decay_step)\n    return exponential_decay_fm\n\nexponential_decay_fm = exponential_decay(0.01, 10)\nlr_scheduler = LearningRateScheduler(exponential_decay_fm)","20fe3812":"history = m.fit(train_images_final,train_labels_oh_enc,\n                    batch_size=batch_size,\n                    epochs=epoch,\n                    verbose=1,\n                    validation_data=(dev_images_final,dev_labels_oh_enc),\n                callbacks=[checkpoint_fm, early_stopping_fm, lr_scheduler])\n\n","9c6d6c34":"plt.figure(figsize=(6,4))\nplt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])\nplt.title(\"Loss-Epoch Relation\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend([\"Train\",\"Dev\"])\nplt.show()\n\nplt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Accuracy - Epoch Relation\")\nplt.legend([\"Train\",\"Dev\"])\nplt.ylim(0.8,1)\nplt.show()\n\n\n\npredict_acc = m.evaluate(test_images.reshape(-1,28,28,1).astype(\"float32\")\/255.,test_labels_oh_enc)\nprint('\\n\\t  Accuracy oranlar\u0131\\n'+\n      '\\t--------------------\\n'+\n      'Train: ',history.history[\"accuracy\"][-1],\n      '\\nDev: ',history.history['val_accuracy'][-1],\n      '\\nTest: ',predict_acc[1])","5925d01b":"### 4.5. Evaluation","d5c38f21":"### 5.1. EarlyStopping","5c66a88b":"# 2. Data Analysis & EDA","41fdaf02":"### 2.4. Missing Values","67a7f29d":"### 4.3. Define Metrics and Algorithms","096e03f6":"### 3.3. Normalization","5273d2ec":"Let's create our model by using defining blocks.","abded360":"### 4.4. Train","d84ecedb":"Now we can define a dictionary to show our labels. To get quickly result, define a function.","5e842d4d":"Labels' mean is features or type of clothe like t-shirt,dress etc. We can see that 10 features\/classes exist. Let's define them in a dictionary. ","3585d9bf":"Our labels' count are very close to each other. So we probably say that dataset has no class imbalance problem. Good!","2427eea1":"### 3.1. Reshape","ff71dda9":"### 5.3. Train Again","5c33b18e":"### 2.5. Check Labels","e1cf5b86":"# 5. Fine-Tuning Model","a24ce912":"Hello everyone! It's my first notebook. I didnt want to add a info part because it's not so big notebook. I had %93.73 accuracy on Test set. I hope it's clear to understand. ","2cb2dda8":"### 3.2. Data Augmentation","f54550bf":"# 4. Model","39bdbd40":"### 1.1. Import Necessary Libraries","ff9384ea":"Now, we examine some datas in our images dataset.","0419293d":"### 2.6. Class Imbalance","9109c752":"We can examine the results more but for now , just dont forget the min and max value. Because we will use the values for Normalization.","8857fde4":"### 2.3. Statistical Info","7710e118":"### 4.2. Create Model","43d4691e":"Results show that images has 28x28 piksel and counts are 50k. Labels' count is 50k,too.","26fecc07":"0 and 0 it's mean that train_images and train_labels datasets have no NaN\/missing values. That's good. ","5e59e49a":"# 1. Imports","961e9965":"### 2.7. Data Visualization","44e9c40e":"### 3.4. One-hot encoding\n","66b82e7d":"# 3.Preprocessing ","a02de343":"### 5.2. Learning Rate Decay","4317caeb":"### 2.2. General Info about Data","e5916cdd":"### 4.1. Define Blocks","57fef34e":"### 1.2. Load Dataset","9322f161":"### 5.4. Evaluation","deea3b3a":"Firstly, i want to examine my datas in frame.So using pandas,i'll convert my data to DataFrame. Our data's shape is 3D,but DataFrame must be 2D-input. So we should reshape our datasets in images.","0bea4f71":"### 2.1. Shape","bf2737bb":"Let's look the results. **Train images** has 50,000 rows and 784 columns. All datas' type are uint8.And **Train labels** has 50,000 rows and 1 column. And all datas' type are uint8,too."}}