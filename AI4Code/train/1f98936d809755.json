{"cell_type":{"ab020da2":"code","27e3a693":"code","4a8d9fc3":"code","1d86dcd3":"code","0dc9b634":"code","bbdd9465":"code","6d0d5622":"code","35865598":"code","19324491":"code","f77a25cd":"code","fb8eed91":"code","cac908a0":"code","64f32d83":"code","86cac53a":"code","63af2f73":"code","a93fb1e5":"code","af572c78":"code","0ce0a203":"code","c31db6b3":"code","c0410094":"code","b2fb5f4b":"code","b3c3ba6e":"code","a9cc4b40":"code","36a5b39d":"code","3316f58c":"code","fb75f2a4":"code","b4c26dbf":"code","d8626b03":"code","59fa6118":"code","6da04984":"code","b2250dbb":"code","b40481cf":"code","c107329a":"code","8acdc215":"code","a664f09c":"code","eb953a6e":"code","28fc0bb4":"code","4db7739b":"code","9c947ed9":"code","1e608352":"code","4485d995":"code","272f6ff3":"code","ab3cd292":"code","5c4fd893":"code","d96994b3":"code","98cb6d3f":"code","721d2eb6":"code","7f5d63c7":"code","526826f2":"code","b2e83e5a":"code","69b2b99c":"code","5c220744":"code","6efd3406":"code","db76d357":"code","db19c656":"code","ed30de37":"code","ac4c8603":"code","ba6f075d":"code","965ab9f4":"code","f0d87f08":"code","f9d930a0":"code","e1bceafc":"code","affd634e":"markdown","d1fd4668":"markdown","3bdd2e39":"markdown","ef7496ad":"markdown"},"source":{"ab020da2":"# Import required libraries\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nimport seaborn as sns\nimport time\nfrom geopy.distance import great_circle\n\nfrom collections import Counter\nimport re\nimport xgboost as xgb\n\nfrom sklearn.linear_model import LinearRegression as lg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","27e3a693":"# Load the dataset\ndata_initial = pd.read_csv('..\/input\/listings_summary.csv')\n# Print the columns of Initial Dataset loaded\ndata_initial.columns","4a8d9fc3":"# Move the selected Features for analysis into a variable\nfeatures_to_keep = ['id', 'space', 'description', 'host_has_profile_pic',\n                    'latitude', 'longitude', 'property_type', 'room_type', 'accommodates', 'bathrooms',  \n                   'bedrooms', 'bed_type', 'amenities', 'square_feet', 'price', 'cleaning_fee', \n                   'security_deposit', 'extra_people', 'guests_included', 'minimum_nights',  \n                   'instant_bookable', 'cancellation_policy', 'experiences_offered', \n                    'neighborhood_overview','access', 'house_rules']\n\n# Load the Features into a dataset variable\ndata_raw = data_initial[features_to_keep].set_index('id')\n# Check the Shape of the Dataset\nprint(\"The dataset with selected features has {} rows and {} columns.\".format(*data_raw.shape))","1d86dcd3":"# Normalizing the 'room_type' feature\ndata_raw.room_type.value_counts(normalize=True)","0dc9b634":"# Normalizing the 'property_type' feature\ndata_raw.property_type.value_counts(normalize=True)","bbdd9465":"#Print First 3 rows of the selected features\ndata_raw[['price', 'cleaning_fee', 'extra_people', 'security_deposit']].head(3)","6d0d5622":"# Checking for Nan's in 'price' column\ndata_raw.price.isna().sum()","35865598":"# Checking for Nan's in 'cleaning_fee' column\ndata_raw.cleaning_fee.isna().sum()","19324491":"#Replace Nan's with $0.00 for 'cleaning_fee'\ndata_raw.cleaning_fee.fillna('$0.00', inplace=True)\ndata_raw.cleaning_fee.isna().sum()","f77a25cd":"# Checking for Nan's in 'security_deposit' column\ndata_raw.security_deposit.isna().sum()","fb8eed91":"#Replace Nan's with $0.00 for 'security_deposit'\ndata_raw.security_deposit.fillna('$0.00', inplace=True)\ndata_raw.security_deposit.isna().sum()","cac908a0":"# Checking for Nan's in 'extra_people' column\ndata_raw.extra_people.isna().sum()","64f32d83":"# Cleaning up the features using method chaining\ndata_raw.price = data_raw.price.str.replace('$', '').str.replace(',', '').astype(float)\ndata_raw.cleaning_fee = data_raw.cleaning_fee.str.replace('$', '').str.replace(',', '').astype(float)\ndata_raw.security_deposit = data_raw.security_deposit.str.replace('$', '').str.replace(',', '').astype(float)\ndata_raw.extra_people = data_raw.extra_people.str.replace('$', '').str.replace(',', '').astype(float)","86cac53a":"# Analyzing the 'price' feature\ndata_raw['price'].describe()","63af2f73":"green_square = dict(markerfacecolor='g', markeredgecolor='g', marker='.')\ndata_raw['price'].plot(kind='box', xlim=(0, 1000), vert=False, flierprops=green_square, figsize=(16,3));","a93fb1e5":"# Based on the plot, to improve the dataset quality removed listings with prices above 400 and 0.00 \ndata_raw.drop(data_raw[ (data_raw.price > 400) | (data_raw.price == 0) ].index, axis=0, inplace=True)\ndata_raw['price'].describe()\nprint(\"The dataset after price-wise preprocessed has {} rows and {} columns.\".format(*data_raw.shape))","af572c78":"# Viewing all the dataset features for Nan's and Missing Values\ndata_raw.isna().sum()","0ce0a203":"# Droping features with too many Nan's\ndata_raw.drop(columns=['square_feet', 'space','neighborhood_overview','access', 'house_rules'], inplace=True)","c31db6b3":"# Droping rows with Nan's in features 'bathrooms', 'bedrooms'\ndata_raw.dropna(subset=['bathrooms', 'bedrooms', ], inplace=True)","c0410094":"# Replacing Nan's with no for 'host_has_profile_pic' \ndata_raw.host_has_profile_pic.fillna(value='f', inplace=True)\ndata_raw.host_has_profile_pic.unique()","b2fb5f4b":"# Checking the dataset features after dropping features with higher Nan's\ndata_raw.isna().sum()","b3c3ba6e":"# 'description' has lot of Nan's yet there might be useful information regading size\n# Trying to extract size by identyfying numbers followed by text like 'sm' or 'm' and \n# transform it into a new feature 'size'\n# Extract numbers from 'description' feature\ndata_raw['size'] = data_raw['description'].str.extract('(\\d{2,3}\\s?[smSM])', expand=True)\ndata_raw['size'] = data_raw['size'].str.replace(\"\\D\", \"\")\n\n# Now change datatype of size into float\ndata_raw['size'] = data_raw['size'].astype(float)\n","a9cc4b40":"# Dropping 'description' feature\ndata_raw.drop(['description'], axis=1, inplace=True)","36a5b39d":"#Adding a new feature 'distance' to the dataset as location is most important in determining the price\ndef distance_from_midberlin(lat, lon):\n    berlin_centre = (52.5027778, 13.404166666666667)\n    record = (lat, lon)\n    return great_circle(berlin_centre, record).km\n\ndata_raw['distance'] = data_raw.apply(lambda x: distance_from_midberlin(x.latitude, x.longitude), axis=1)","3316f58c":"print(\"After preprocessing for missing values and adding new features the dataset has {} rows and {} columns.\".format(*data_raw.shape))","fb75f2a4":"# Filtering out sub_data to detrmine missing values in 'size' based on related independent features\nsub_data = data_raw[['accommodates', 'bathrooms', 'bedrooms',  'price', 'cleaning_fee', \n                 'security_deposit', 'extra_people', 'guests_included', 'distance', 'size']]","b4c26dbf":"# Split datasets into train and test\ntrain_data = sub_data[sub_data['size'].notnull()]\ntest_data  = sub_data[sub_data['size'].isnull()]\n\n# Define X\nX_train = train_data.drop('size', axis=1)\nX_test  = test_data.drop('size', axis=1)\n\n# Define y\ny_train = train_data['size']","d8626b03":"# Describe train_data, test_data, X_train, x_test and y_train data sets \nprint(\"Shape of Train Data:    \",train_data.shape)\nprint(\"Shape of Test Data:    \",test_data.shape)\nprint(\"\\nShape of X_train:\", X_train.shape)\nprint(\"Shape of X_test:\", X_test.shape)\nprint(\"\\nShape of y_train:\", y_train.shape)","59fa6118":"\n# instantiate the Linear Regression Model\nlinreg = lg()\n\n# Fit Linear regression model to training data\nlinreg.fit(X_train, y_train)","6da04984":"# Make predictions using the model\ny_test = linreg.predict(X_test)","b2250dbb":"# Add the new predicted values to the dataframe 'size' feature\ny_test = pd.DataFrame(y_test)\ny_test.columns = ['size']\nprint(y_test.shape)\ny_test.head()","b40481cf":"print(X_test.shape)\nX_test.head()","c107329a":"# Add the index of X_test to an own dataframe\nprelim_index = pd.DataFrame(X_test.index)\nprelim_index.columns = ['prelim']\n\n# Concat this dataframe with y_test to form our new test dataset\ny_test = pd.concat([y_test, prelim_index], axis=1)\ny_test.set_index(['prelim'], inplace=True)\ny_test.head()\nnew_test_data = pd.concat([X_test, y_test], axis=1)","8acdc215":"# check the new test dataset for Nan's in 'size' feature\nprint(new_test_data.shape)\nnew_test_data['size'].isna().sum()","a664f09c":"# concant train and test data to a new sub dataset\nsub_data_new = pd.concat([new_test_data, train_data], axis=0)\n\nprint(sub_data_new.shape)\nsub_data_new.head()","eb953a6e":"# check if the new sub dataset had Nan's in 'size' column\nsub_data_new['size'].isna().sum()","28fc0bb4":"# prepare the features before concatening\ndata_raw.drop(['accommodates', 'bathrooms', 'bedrooms', 'price', 'cleaning_fee', \n             'security_deposit', 'extra_people', 'guests_included', 'distance', 'size'], \n            axis=1, inplace=True)","4db7739b":"# concate the dataset output from linear regression model to complete original dataframe\ndf = pd.concat([sub_data_new, data_raw], axis=1)\n\nprint(df.shape)","9c947ed9":"# analyze 'size' feature to improve quality of the data\ngreen_square = dict(markerfacecolor='g', markeredgecolor='g', marker='.')\ndf['size'].plot(kind='box', xlim=(0, 1000), vert=False, flierprops=green_square, figsize=(16,4));","1e608352":"# drop the rows with 'size' column values 0 and greater than 400 as they are only few \ndf.drop(df[ (df['size'] == 0.) | (df['size'] > 400.) ].index, axis=0, inplace=True)\nprint(\"The dataset after preprocessing 'size' feature has {} rows and {} columns.\".format(*df.shape))","4485d995":"# Analyzing another important feature 'ameneties' \nresults = Counter()\ndf['amenities'].str.strip('{}')\\\n               .str.replace('\"', '')\\\n               .str.lstrip('\\\"')\\\n               .str.rstrip('\\\"')\\\n               .str.split(',')\\\n               .apply(results.update)\n\nresults.most_common(30)","272f6ff3":"# create a new sub dataframe with 'amenity' and 'count'\nsub_df = pd.DataFrame(results.most_common(30), columns=['amenity', 'count'])","ab3cd292":"# ploting the top 20 amenities \nsub_df.sort_values(by=['count'], ascending=True).plot(kind='barh', x='amenity', y='count',  \n                                                      figsize=(10,10), legend=False, color='green',\n                                                      title='Feature_Amenities')\nplt.xlabel('Count');","5c4fd893":"# adding new features using 'amenities'\ndf['Laptop_friendly_workspace'] = df['amenities'].str.contains('Laptop friendly workspace')\ndf['TV'] = df['amenities'].str.contains('TV')\ndf['Family_kid_friendly'] = df['amenities'].str.contains('Family\/kid friendly')\ndf['Host_greets_you'] = df['amenities'].str.contains('Host greets you')\ndf['Smoking_allowed'] = df['amenities'].str.contains('Smoking allowed')","d96994b3":"# after adding new features drop redundant 'amenities' column \ndf.drop(['amenities'], axis=1, inplace=True)","98cb6d3f":"# Check the exisiting columns on the dataset\ndf.columns","721d2eb6":"# print information of the dataset \ndf.info()","7f5d63c7":"# drop the unhelpful columns\ndf.drop(['latitude', 'longitude', 'property_type'], axis=1, inplace=True)","526826f2":"# convert all string columns into categorical features\nfor col in ['host_has_profile_pic', 'room_type', 'bed_type', 'instant_bookable', \n            'cancellation_policy']:\n    df[col] = df[col].astype('category')","b2e83e5a":"# define target\ntarget = df[[\"price\"]]\n\n# define features \nfeatures = df.drop([\"price\"], axis=1)","69b2b99c":"# identify neumerical features\nnum_fea = features.select_dtypes(include=['float64', 'int64', 'bool']).copy()\n\n# one-hot encoding of categorical features\ncat_fea = features.select_dtypes(include=['category']).copy()\ncat_fea = pd.get_dummies(cat_fea)","5c220744":"# concat numerical and categorical features\nfeatures_recoded = pd.concat([num_fea, cat_fea], axis=1)","6efd3406":"print(features_recoded.shape)\nfeatures_recoded.head(2)","db76d357":"# split dataset into training and test datasets\nX_train, X_test, y_train, y_test = train_test_split(features_recoded, target, test_size=0.2)\n","db19c656":"# Instantiating XGB Regressor\n\nbooster_start = time.time()\n\nbooster = xgb.XGBRegressor()\n\n# create Grid\n                \nparam_grid = {'n_estimators': [200, 300, 400],\n              'learning_rate': [.03, 0.05, .07], \n              'max_depth': [3, 4, 5],\n              'min_child_weight': [4],\n              'colsample_bytree': [0.7, 0.8, 1],\n              'gamma': [0.0, 0.1, 0.2]}\n\n# instantiate the tuned random forest\nbooster_grid_search = GridSearchCV(booster, param_grid, cv=3, n_jobs=-1)\n\n# train the tuned random forest\nbooster_grid_search.fit(X_train, y_train)\n\n# print best estimator parameters found during the grid search\nprint(booster_grid_search.best_params_)\n\nbooster = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, \n                           max_depth=6, n_estimators=200)\nbooster.fit(X_train, y_train)\ntraining_preds_booster = booster.predict(X_train)\nval_preds_booster = booster.predict(X_test)\n\nbooster_end = time.time()\n\n# Printing the results\n\nprint(\"\\nTraining RMSE:\", round(np.sqrt(mean_squared_error(y_train, training_preds_booster)),4))\nprint(\"Validation RMSE:\", round(np.sqrt(mean_squared_error(y_test, val_preds_booster)),4))\nprint(\"\\nTraining r2:\", round(r2_score(y_train, training_preds_booster),4))\nprint(\"Validation r2:\", round(r2_score(y_test, val_preds_booster),4))\nprint(f\"Time taken to run: {round((booster_end - booster_start)\/60,1)} minutes\")\n\n# Producing a dataframe of feature importances\nft_weights_booster = pd.DataFrame(booster.feature_importances_, columns=['weight'], index=features_recoded.columns)\nft_weights_booster.sort_values('weight', inplace=True)\nft_weights_booster\n\n","ed30de37":"# Plotting feature importances\nplt.figure(figsize=(8,20))\nplt.barh(ft_weights_booster.index, ft_weights_booster.weight, align='center') \nplt.title(\"Feature importances in the XGBoost model\", fontsize=14)\nplt.xlabel(\"Feature importance\")\nplt.margins(y=0.01)\nplt.show()","ac4c8603":"# Using Cross Validation with 10 fold \nxg_train = xgb.DMatrix(data=X_train, label=y_train)\n\nparams = {'colsample_bytree': 0.3,'learning_rate': 0.2,\n                'max_depth': 5, 'alpha': 10}\n\ncv_results = xgb.cv(dtrain=xg_train, params=params, nfold=10,\n                    num_boost_round=200,early_stopping_rounds=5,metrics=\"rmse\", as_pandas=True, seed=123)\n\nprint((cv_results[\"test-rmse-mean\"]).tail(1))","ba6f075d":"# plot the important features needed for Simple Model\nfeat_importances = pd.Series(booster.feature_importances_, index=features_recoded.columns)\nfeat_importances.nlargest(10).sort_values().plot(kind='barh', color='green', figsize=(10,5))\nplt.xlabel('Relative Feature Importance with XGBoost');","965ab9f4":"# Print the columns of original dataset\nfeatures_recoded.columns","f0d87f08":"# Define Simple Model Dataset with 10 features\nsimplefeatures_to_keep = ['accommodates', 'bathrooms', 'bedrooms', 'extra_people', 'guests_included', 'size',\n                          'room_type_Private room','room_type_Entire home\/apt','room_type_Shared room', \n                          'cancellation_policy_super_strict_60']\nsimplefeatures = features_recoded[simplefeatures_to_keep]\n\n# Check the Shape of the Dataset with important features for Simple Model\nprint(\"The dataset with selected features has {} rows and {} columns.\".format(*simplefeatures.shape))\n\n# split dataset into training and test datasets\nSimpleX_train, SimpleX_test, Simpley_train, Simpley_test = train_test_split(simplefeatures, target, test_size=0.2)\n\n# Instantiating XGB Regressor for training Simple model\n\nsimplebooster_start = time.time()\n\nsimplebooster = xgb.XGBRegressor()\n\nsimplebooster = xgb.XGBRegressor(colsample_bytree=0.7, gamma=0.2, learning_rate=0.1, \n                           max_depth=6, n_estimators=200, random_state=4)\nsimplebooster.fit(SimpleX_train, Simpley_train)\ntraining_preds_simplebooster = simplebooster.predict(SimpleX_train)\nval_preds_simplebooster = simplebooster.predict(SimpleX_test)\n\nsimplebooster_end = time.time()\n\n\n# Printing the results for Simple Model\n\nprint(\"\\nTraining RMSE:\", round(np.sqrt(mean_squared_error(Simpley_train, training_preds_simplebooster)),4))\nprint(\"Validation RMSE:\", round(np.sqrt(mean_squared_error(Simpley_test, val_preds_simplebooster)),4))\nprint(\"\\nTraining r2:\", round(r2_score(Simpley_train, training_preds_simplebooster),4))\nprint(\"Validation r2:\", round(r2_score(Simpley_test, val_preds_simplebooster),4))\nprint(f\"Time taken to run: {round((simplebooster_end - simplebooster_start)\/60,1)} minutes\")\n","f9d930a0":"# split test and train datasets \nX = features_recoded\ny = df[\"price\"]\n\nX_treetrain, X_treetest, y_treetrain, y_treetest = train_test_split(X, y, test_size=0.2,random_state=42)","e1bceafc":"# instantiate Decision Tree Regression Model\n\ntree_start = time.time()\n\nregr_tree = DecisionTreeRegressor()\n# fit and train model\nregr_tree.fit(X_treetrain, y_treetrain)\n\nval_pred_decisiontree = regr_tree.predict(X_treetest)\n\ntree_end = time.time()\n\n# Printing the results for Decision Tree Regression Model\n\nprint(\"Validation RMSE:\", round(np.sqrt(mean_squared_error(y_treetest, val_pred_decisiontree)),4))\nprint(f\"Time taken to run: {round((tree_end - tree_start),1)} seconds\")","affd634e":"Clean, Normalize and Standardize the Features","d1fd4668":"**Decision Tree Regressor Model**","3bdd2e39":"**XGBoost Regressor**","ef7496ad":"Using Linear Regressor Model for Training the data to predict missing values for 'size'"}}