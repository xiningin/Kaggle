{"cell_type":{"e75ad596":"code","10bcf19a":"code","1d6c925c":"code","abebedbb":"code","2a76af5c":"code","51279160":"code","8876654d":"code","6bb4a11e":"code","999a4789":"code","1ec37187":"markdown","944804d9":"markdown","2caa0273":"markdown","c02a5291":"markdown","85bd042c":"markdown","da6ef36c":"markdown","71a70402":"markdown","05c71a9c":"markdown","5276f59c":"markdown","83ea0570":"markdown"},"source":{"e75ad596":"import pandas as pd\n\n# extract data into dataframe\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n# print columns\ntrain.columns","10bcf19a":"# prints shape which is (1460 * 81) \ntrain.shape\n# prints first few values of the datset with their columns\ntrain.head()","1d6c925c":"# description\ntrain.describe()","abebedbb":"# target \ny_train = train.SalePrice\n\n# drops the Id and SalePrice columns\nX_train = train.drop(['Id','SalePrice'], axis= 1)\n\n# drops Id column from test\nX_test = test.drop(['Id'], axis= 1)\n","2a76af5c":"# code to encode object data(text data) using one-hot encoding(commonly used)\none_hot_encoded_training_data = pd.get_dummies(X_train)\none_hot_encoded_testing_data = pd.get_dummies(X_test)\n\n# align command make sure that the columns in both the datasets are in same order\nfinal_train, final_test = one_hot_encoded_training_data.align(one_hot_encoded_testing_data,join='inner',axis=1)\n# to check the increased number of columns\nfinal_train.shape","51279160":"from xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import svm\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\n# RandomForestRegressor:\nfor n in range(10,200,10):\n    # define pipeline\n    pipeline = make_pipeline(SimpleImputer(), RandomForestRegressor(max_leaf_nodes=n,random_state=1))\n    # cross validation score\n    scores = cross_val_score(pipeline, final_train, y_train, scoring= 'neg_mean_absolute_error')\n    print(n,scores)\n\n# XGBRegressor:\n# define pipeline\npipeline = make_pipeline(SimpleImputer(), XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                            nthread = -1, random_state=1))\n# cross validation score\nscores = cross_val_score(pipeline,final_train, y_train, scoring= 'neg_mean_absolute_error')\nprint('Mean Absolute Error %2f' %(-1 * scores.mean()))\n#Validation function\n\n\n# GradientBoostingRegressor:(just another model)\n# define pipeline\nmy_pipeline = make_pipeline(SimpleImputer(), GradientBoostingRegressor())\n# cross validation score\nscore = cross_val_score(my_pipeline,final_train, y_train, scoring= 'neg_mean_absolute_error')\nprint('Mean Absolute Error %2f' %(-1 * score.mean()))\n","8876654d":"# fit and make predictions\npipeline.fit(final_train,y_train)\npredictions= pipeline.predict(final_test)\n\nprint(predictions)\n","6bb4a11e":"my_submission = pd.DataFrame({'Id': test.Id, 'SalePrice': predictions})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","999a4789":"from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\ndef get_some_data():\n    cols_to_use = ['YearBuilt', 'TotRmsAbvGrd', 'LotArea']\n    data = pd.read_csv('..\/input\/train.csv')\n    y = data.SalePrice\n    X = data[cols_to_use]\n    my_imputer = SimpleImputer()\n    imputed_X = my_imputer.fit_transform(X)\n    return imputed_X, y\n\n\n# get_some_data is defined in hidden cell above.\nX, y = get_some_data()\n# scikit-learn originally implemented partial dependence plots only for Gradient Boosting models\n# this was due to an implementation detail, and a future release will support all model types.\nmy_model = GradientBoostingRegressor()\n# fit the model as usual\nmy_model.fit(X, y)\n# Here we make the plot\nmy_plots = plot_partial_dependence(my_model,       \n                                   features=[0,2], # column numbers of plots we want to show\n                                   X=X,            # raw predictors data.\n                                   feature_names=['YearBuilt', 'TotRmsAbvGrd', 'LotArea'], # labels on graphs\n                                   grid_resolution=10) # number of values to plot on x axis","1ec37187":"Finally the step we all were waiting for a while now, the fitting and predicting. \n\nTo train or to fit is the heart of any machine learning exercise. Training the model means allowing the algorithms in the model to determine the patterns in the training data. It is the job of the model to figure out y_train(target) from X_train(input)\n\nTo predict means the model uses the insights and patterns it has gained from the training step to produce an output that is accurate (atleast for the model I don't want to hurt its feelings). The output or y_test might not be satisfactory at all. It is the job of the machine learning engineers to evaluate and improve it.  ","944804d9":"Describe method provides us with clear outlook of our data. It summarizes the fundamental statistical terms like mean, standard deviation, etc that are helpful in providing us some useful information and at the same time we can analyze if the given data makes sense in real life.  For example, we can look at the mean of the YrSold column which is 2007.82, it takes little common knowledge to not use the model trained with this data in 2018 as it will make very low or high predictions.  ","2caa0273":"Our dataset does not just include numerical or integer values but there are categorical columns as well like SaleCondition. Most machine learning models do not take in these values. It will produce errors unless we encode the categorical data into integer which might increase the number of columns.  Here we are using One hot encoding to perform the conversion.  It is available inside the pandas module. ","c02a5291":"Next we will break down our dataframes further into X and y. As we all know in most mathematical equations X is the variable used to calculate the value of y. X and y are dependent meaning change in value of X changes the value of y. The same applies here. The SalePrice(target) column will be contained in y and all the other columns which are helpful in finding the target(SalePrice) will be stored in X. \n\nWe cannot have SalePrice column in X so drop is used to remove that column. From the previous step, I realized that the Id column is not very important information to consider while predicting the SalePrice therefore I dropped it too. \n\nNow we have training input(X_train) , training target(y_train) and testing input (X_test). The testing target or y_test should be the model's output values. ","85bd042c":"First, import the train and test data then convert it to dataframe using pandas.  We will print the columns to take our first look at the names of the columns and potential features for our machine learning model. ","da6ef36c":"The evaluation step cannot be implemented because our data does not have y_test. We did perform some evaluation using the training data which did help us make better predictions as previously seen. ","71a70402":"Below is the partial dependecy plot which could give much useful information to further improve our model.\n\nFor this version of the kernel it is only able to produce partial dependecy plots for GradientBoostingRegressor. ","05c71a9c":"The shape of the training set is its dimension. There are 1460 data points and 81 columns or features. We can think of this as 1460 rows and 81 columns in a table.  If we check the shape of test data we will get the same number of rows (1460) but the columns would be only 80 as we have to predict the remaining column using our model. \n\nThe head prints the top five values of our dataset. ","5276f59c":"Below is the first machine learning pipeline I implemented after completing the machine learning section of Kaggle learn. \nIt uses XGBRegressor with Simple Imputer to impute the missing values  and One-Hot encoding to convert the categorical data (like SaleCondition) into binary columns.  There are many data analysis techniques and visualization methods that can be applied to the dataset to get the best results. Due to my limited knowledge of those methods for now, I will only be implementing the methods taught in the Kaggle learn course. \n\nSteps:\n*  Extract and modify the data from the .csv file to suit our model's requirement.\n*  Impute and Encode the training and test set if needed.\n*  Define the machine learning model.\n*  Train or fit the model with the training data.\n*  Make predictions on the testing data.\n*  Evaluate the model, if any improvement possible then apply it and refit the model. ","83ea0570":"We covered the categorical values but still have some values or data points that are empty (like houses without Garages will not have GarageArea). The simplest solution to this problem is to just drop those columns but this might lead to loss of critical information that could have largely helped the model to improve its accuracy. Below I have used SimpleImputer that imputes the missing value with the mean of other available ones. \n\nI have compared the cross-validation score of two different models to check which one performs better. The XGBRegressor is way more accurate than the RandomForestRegressor. \n\nThe for loop is used to estimate numer of max_leaf_nodes in RandomForestRegressor that gives the minimum mean absolute error (MAE). \n\nI manually tweaked the parameters of XGBRegressor to get as low MAE as possible. I think some other parameters can be changed too to make it even more accurate. \n\nBoth the imputer and model are defined inside a pipeline which makes it easier and more flexible to apply to different datasets. It reduces bugs and improves readabiity. "}}