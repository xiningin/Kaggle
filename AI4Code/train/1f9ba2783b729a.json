{"cell_type":{"3c0a3026":"code","b767d2a5":"code","278fb847":"code","d93b98dd":"code","2ac49dfb":"code","a9fff14f":"code","3bbbed10":"code","9cc09e92":"code","18edfc8c":"code","c4b4aef0":"code","b49b703e":"code","05adf6e7":"code","bd0616ec":"code","e511d476":"code","9eab930c":"code","1bcf8a6b":"code","b47d3d43":"code","b2c97e1a":"code","75162d22":"code","db84a260":"code","266c42be":"code","6bd3aa94":"code","a107d6cd":"code","e14503bc":"code","74a20470":"code","c9ca8623":"code","3057a051":"code","ee0d083a":"code","ac255bb5":"code","0e345836":"code","6542de2d":"code","5025900d":"code","339e6060":"code","c0a59acd":"code","5cd61584":"code","6aa9e34c":"code","6efc0736":"code","ab5e15d8":"code","6e6cfa63":"code","5aaead5b":"code","377c8756":"code","b4c8a44b":"code","a9fb45e5":"code","e17b1c4c":"code","bae4e37e":"code","3d808e9d":"code","ca5c62f7":"code","e8332882":"code","78d46024":"code","ab50dc15":"code","40a0c673":"code","22c9f9f8":"code","5ebac3f5":"code","1016cd25":"code","7c7a3013":"code","99122739":"markdown","e875000d":"markdown"},"source":{"3c0a3026":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b767d2a5":"# data analysis and wrangling\n\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","278fb847":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ncombine = [train_data, test_data]","d93b98dd":"train_data.head()","2ac49dfb":"train_data.columns","a9fff14f":"train_data.describe()","3bbbed10":"train_data.info()","9cc09e92":"test_data.info()","18edfc8c":"train_data[[\"Pclass\", \"Survived\"]].groupby([\"Pclass\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","c4b4aef0":"train_data[[\"Sex\", \"Survived\"]].groupby([\"Sex\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","b49b703e":"train_data[[\"SibSp\", \"Survived\"]].groupby([\"SibSp\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","05adf6e7":"train_data[[\"Parch\", \"Survived\"]].groupby([\"Parch\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","bd0616ec":"g = sns.FacetGrid(train_data, col=\"Survived\")\ng.map(plt.hist, \"Age\", bins=20)","e511d476":"g = sns.FacetGrid(train_data, col=\"Survived\", row=\"Sex\")\ng.map(plt.hist, \"Fare\", bins=20)","9eab930c":"sns.catplot(x = \"Pclass\", y = \"Survived\", data=train_data, kind = \"bar\")","1bcf8a6b":"sns.catplot(x = \"Parch\", y = \"Survived\", data=train_data, kind= \"bar\")","b47d3d43":"sns.catplot(x = \"Embarked\", y = \"Survived\", data=train_data, kind= \"bar\", hue = \"Sex\")","b2c97e1a":"sns.catplot(x = \"SibSp\", y = \"Survived\", data=train_data, kind= \"bar\")","75162d22":"sns.catplot(x = \"Sex\", y = \"Survived\", data=train_data, kind= \"bar\")","db84a260":"## dropping the ticket and cabin feature \nprint(\"Before\", train_data.shape, test_data.shape, combine[0].shape, combine[1].shape)\n\ntrain_data = train_data.drop([\"Ticket\", \"Cabin\"], axis=1)\ntest_data = test_data.drop([\"Ticket\", \"Cabin\"], axis=1)\ncombine = [train_data, test_data]\n\n\"After\", train_data.shape, test_data.shape, combine[0].shape, combine[1].shape","266c42be":"## extracting the title from the names \nfor dataset in combine:\n    dataset[\"Title\"] = dataset.Name.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n\npd.crosstab(train_data[\"Title\"], train_data[\"Sex\"])","6bd3aa94":"## We can replace many titles with a more common name or classify them as Rare.\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_data[[\"Title\", \"Survived\"]].groupby([\"Title\"], as_index=False).mean()","a107d6cd":"## We can now convert the categorical values.\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_data.head()","e14503bc":"## dropping the name and passenger feature in the train dataset\ntrain_data = train_data.drop([\"Name\", \"PassengerId\"], axis=1)\ntest_data = test_data.drop([\"Name\"], axis=1)\ncombine = [train_data, test_data]\ntrain_data.shape, test_data.shape","74a20470":"for dataset in combine:\n    dataset[\"Sex\"] = dataset[\"Sex\"].map({\"female\": 1, \"male\": 0} ).astype(int)\n\ntrain_data.head()","c9ca8623":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Gender')\ngrid = sns.FacetGrid(train_data, row=\"Pclass\", col=\"Sex\", size=2.2, aspect=1.6)\ngrid.map(plt.hist, \"Age\", alpha=.5, bins=20)\ngrid.add_legend()","3057a051":"## an empty array to guess age value baseed on pclass and gender combinations \nguess_ages = np.zeros((2,3))\nguess_ages","ee0d083a":"## we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_data = dataset[(dataset[\"Sex\"] == i) & \\\n                                  (dataset[\"Pclass\"] == j+1)][\"Age\"].dropna()\n\n            # age_mean = guess_df.mean()\n            # age_std = guess_df.std()\n            # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n            age_guess = guess_data.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    \"Age\"] = guess_ages[i,j]\n\n    dataset[\"Age\"] = dataset[\"Age\"].astype(int)\n\ntrain_data.head()","ac255bb5":"## creating agebands and determinig correlation with survived\ntrain_data[\"AgeBand\"] = pd.cut(train_data[\"Age\"], 5)\ntrain_data[[\"AgeBand\", \"Survived\"]].groupby([\"AgeBand\"], as_index=False).mean().sort_values(by=\"AgeBand\", ascending=True)","0e345836":"## replacing age with ordinal based on the bands \nfor dataset in combine:    \n    dataset.loc[ dataset[\"Age\"] <= 16, \"Age\"] = 0\n    dataset.loc[(dataset[\"Age\"] > 16) & (dataset[\"Age\"] <= 32), \"Age\"] = 1\n    dataset.loc[(dataset[\"Age\"] > 32) & (dataset[\"Age\"] <= 48), \"Age\"] = 2\n    dataset.loc[(dataset[\"Age\"] > 48) & (dataset[\"Age\"] <= 64), \"Age\"] = 3\n    dataset.loc[ dataset[\"Age\"] > 64, \"Age\"] = 4\ntrain_data.head(10)","6542de2d":"train_data = train_data.drop([\"AgeBand\"], axis=1)\ncombine = [train_data, test_data]\ntrain_data.head()","5025900d":"## creating a new feature for family size which combines parch and sibsp\nfor dataset in combine:\n    dataset[\"FamilySize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1\n\ntrain_data[[\"FamilySize\", \"Survived\"]].groupby([\"FamilySize\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","339e6060":"## creating another feature for isalone\nfor dataset in combine:\n    dataset[\"IsAlone\"] = 0\n    dataset.loc[dataset[\"FamilySize\"] == 1, \"IsAlone\"] = 1\n\ntrain_data[[\"IsAlone\", \"Survived\"]].groupby([\"IsAlone\"], as_index=False).mean()","c0a59acd":"train_data = train_data.drop([\"Parch\", \"SibSp\", \"FamilySize\"], axis=1)\ntest_data = test_data.drop([\"Parch\", \"SibSp\", \"FamilySize\"], axis=1)\ncombine = [train_data, test_data]\n\ntrain_data.head()","5cd61584":"# artificial feature combinig age and pclass together\nfor dataset in combine:\n    dataset[\"Age*Class\"] = dataset.Age * dataset.Pclass\n\ntrain_data.loc[:, [\"Age*Class\", \"Age\", \"Pclass\"]].head(10)","6aa9e34c":"## filling missing values with S simply its the most occurance\nfreq_port = train_data.Embarked.dropna().mode()[0]\nfreq_port","6efc0736":"for dataset in combine:\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(freq_port)\n    \ntrain_data[[\"Embarked\", \"Survived\"]].groupby([\"Embarked\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","ab5e15d8":"## creating a new feature for embarked\nfor dataset in combine:\n    dataset[\"Embarked\"] = dataset[\"Embarked\"].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_data.head()","6e6cfa63":"test_data[\"Fare\"].fillna(test_data[\"Fare\"].dropna().median(), inplace=True)\ntest_data.head()","5aaead5b":"## creating a fareband\ntrain_data[\"FareBand\"] = pd.qcut(train_data[\"Fare\"], 4)\ntrain_data[[\"FareBand\", \"Survived\"]].groupby([\"FareBand\"], as_index=False).mean().sort_values(by=\"FareBand\", ascending=True)","377c8756":"## Convert the Fare feature to ordinal values based on the FareBand.\n\nfor dataset in combine:\n    dataset.loc[ dataset[\"Fare\"] <= 7.91, \"Fare\"] = 0\n    dataset.loc[(dataset[\"Fare\"] > 7.91) & (dataset[\"Fare\"] <= 14.454), \"Fare\"] = 1\n    dataset.loc[(dataset[\"Fare\"] > 14.454) & (dataset[\"Fare\"] <= 31), \"Fare\"]   = 2\n    dataset.loc[ dataset[\"Fare\"] > 31, \"Fare\"] = 3\n    dataset[\"Fare\"] = dataset[\"Fare\"].astype(int)\n\ntrain_data = train_data.drop([\"FareBand\"], axis=1)\ncombine = [train_data, test_data]\n    \ntrain_data.head(10)","b4c8a44b":"X_train = train_data.drop(\"Survived\", axis=1)\nY_train = train_data[\"Survived\"]\nX_test  = test_data.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","a9fb45e5":"## Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","e17b1c4c":"coeff_data = pd.DataFrame(train_data.columns.delete(0))\ncoeff_data.columns = [\"Feature\"]\ncoeff_data[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_data.sort_values(by=\"Correlation\", ascending=False)","bae4e37e":"## Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","3d808e9d":"## KNN\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","ca5c62f7":"## Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","e8332882":"## Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","78d46024":"## Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","ab50dc15":"## Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","40a0c673":"## Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","22c9f9f8":"## Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","5ebac3f5":"models = pd.DataFrame({\n    \"Model\": [\"Support Vector Machines\", \"KNN\", \"Logistic Regression\", \n              \"Random Forest\", \"Naive Bayes\", \"Perceptron\", \n              \"Stochastic Gradient Decent\", \"Linear SVC\", \n              \"Decision Tree\"],\n    \"Score\": [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by=\"Score\", ascending=False)","1016cd25":"submission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": Y_pred})\nsubmission.to_csv(\"Titanic_Pred.csv\", index=False)","7c7a3013":"submission.head()","99122739":"**Model Evaluation**","e875000d":"**Modelling and Predicting**"}}