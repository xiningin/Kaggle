{"cell_type":{"0144ad60":"code","c69f1d1f":"code","f4e8bd12":"code","46efa385":"code","89f25bab":"code","f7a7909f":"code","59e4b3fc":"code","7bccfd3a":"code","9e03ba69":"code","2d555eea":"code","eb15072d":"code","b766f27c":"code","33c71159":"code","e6e41467":"code","f9c6602f":"code","1ff0be19":"code","f3e4d717":"code","36116415":"code","f944e7b1":"code","aa07834a":"code","afcc2a27":"code","12c00d9b":"code","7a8c7f9e":"code","32972aa5":"code","ac6fcad7":"code","2a5984fa":"code","eb763121":"code","60c835f2":"code","22756c47":"markdown","f4d8d3fd":"markdown","4a49cb8c":"markdown","032ba77d":"markdown","96184012":"markdown","43fca4da":"markdown","8a22ae6b":"markdown","431e4377":"markdown","e8f5fd16":"markdown","23f25d84":"markdown","9e08ebf0":"markdown","b5f756d5":"markdown","1db8a450":"markdown","e38fdf9a":"markdown","f01f3880":"markdown"},"source":{"0144ad60":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom matplotlib import pyplot\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport nltk\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")\n\n!pip install textstat\nimport textstat\nfrom textstat.textstat import textstatistics\nfrom tabulate import tabulate\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c69f1d1f":"data = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ndata.head()","f4e8bd12":"# view hardest and easiest texts\nn = 2 # number of texts from each side\n\npres_df = pd.DataFrame(columns=['score', 'text'])\npres_dics = {'score':[]}\nsorted_inds = np.argsort(data['target'])\nmin_inds = sorted_inds[:n].values # take first n elements of sorted indices\nmax_inds = sorted_inds[::-1][:n].values # take first n elements from reverced sorted indices\nmid_inds = sorted_inds[[int(len(sorted_inds)\/2) + (i-int(n\/2)) for i in range(n)]].values # just trust me\n\n# print out results\nprint('Easiest:')\nfor ind in max_inds:\n    print('Score: ', data.iloc[ind]['target'])\n    print(data.iloc[ind]['excerpt'])\nprint('Hardest:')\nfor ind in min_inds:\n    print('Score: ', data.iloc[ind]['target'])\n    print(data.iloc[ind]['excerpt'])   \nprint('Medium:')\nfor ind in mid_inds:\n    print('Score: ', data.iloc[ind]['target'])\n    print(data.iloc[ind]['excerpt'])\n","46efa385":"data['chars'] = data['excerpt'].apply(lambda x: len(x))\ndata['words'] = data['excerpt'].apply(lambda x: len(x.split()))\ndata['ACW'] = data['chars'] \/ data['words']\ndata.head()","89f25bab":"# Returns number of sentences in the text\ndef sentence_count(text):\n    doc = nlp(text)\n    sentence_tokens = [sent for sent in doc.sents]\n    return len(sentence_tokens)\n\ndata['sent'] = data['excerpt'].apply(sentence_count)\ndata['ASL'] = data['words'] \/ data['sent']\ndata.head()","f7a7909f":"def syllables_count(word):\n    return textstatistics().syllable_count(word)\n\ndata['syl'] = data['excerpt'].apply(syllables_count)\ndata['ASW'] = data['syl'] \/ data['words']\ndata.head()","59e4b3fc":"from nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(language='english')\n\nfrom nltk.tag import pos_tag","7bccfd3a":"def poly_syllable_count(text):\n    count = 0\n    words = []\n    \n    text = text.lower()\n    \n    #remove proper nouns \n    sentence = nltk.word_tokenize(text)\n    sent = pos_tag(sentence)\n    new_text = [s[0] for s in sent if s[1] != 'NNP' and s[1] != 'NNPS']\n    \n    #remove affixes\n    for word in new_text:\n        words.append(stemmer.stem(word))\n\n        \n    # Count sillables in words\n\n    for word in words:\n        syllable_count = syllables_count(word)\n        if syllable_count >= 3:\n            #print('found!')\n            count += 1\n    return count\n\ndata['poly_syl'] = data['excerpt'].apply(poly_syllable_count)\ndata['PHW'] = data['poly_syl'] \/ data['words']\ndata.head()","9e03ba69":"#retrieve each part of speech count \nfrom collections import defaultdict\nfrom operator import itemgetter\n\ndef pos_count(text, mode='nq'): \n    sentence = nltk.word_tokenize(text)\n    sent = pos_tag(sentence)\n    counts = defaultdict(int)\n    for (word, tag) in sent:\n        counts[tag] += 1\n\n    #sorted(counts.items(), key=itemgetter(1), reverse=True)\n    \n    noun = counts['NNP']+counts['NNS']+counts['NN']+counts['NNPS']\n    pronoun = counts['PRP']+counts['WP']+counts['PRP$']+counts['WP$']\n    preposition = counts ['IN'] \n    participle = counts ['VBG '] \n    adverb = counts['RB']+counts['RBR']+counts['RBS']\n    verb = counts['VB']+counts['VBD']+counts['VBP']+counts['VBZ']\n    \n    if mode == 'nq':\n        # nominal quotient\n        nq = (noun+preposition+participle) \/ (pronoun+adverb+verb)\n        return nq\n    \n    # noun to pronoun quotient\n    noun_pron = noun \/ (pronoun + 0.001)\n    \n    return noun_pron","2d555eea":"data['nq'] = data['excerpt'].apply(pos_count)\ndata['n\/pn'] = data['excerpt'].apply(pos_count, mode='blalba')\ndata.head()","eb15072d":"def def_art(text):\n    definite = 0\n    text = text.lower()\n    for word in text.split():\n        if word == 'the' in text:\n            definite  = definite + 1\n    return definite \n\ndata['def_art'] = data['excerpt'].apply(def_art) \/ data['words']\ndata.head()","b766f27c":"# subbordinate conjunctions\ndef sconj(text):\n    sconj = 0\n    doc = nlp(text)\n    for token in doc:\n        if token.pos_ == \"SCONJ\":\n            sconj += 1\n    return sconj\n\ndata['sconj'] = data['excerpt'].apply(sconj) \/ data['words']\n\n#prepositional modifier\ndef prep(text):\n    prep = 0\n    doc = nlp(text)\n    for token in doc:\n        if token.head.dep_ == \"prep\":\n            prep += 1\n    return prep\n\ndata['prep'] = data['excerpt'].apply(prep) \/ data['sent']\n\n\n# adverbial clause modifier \ndef advcl(text):\n    advcl = 0\n    doc = nlp(text)\n    for token in doc:\n        if token.head.dep_ == \"advcl\":\n            advcl += 1\n    return advcl\n\ndata['advcl'] = data['excerpt'].apply(advcl) \/ data['sent']\n\n\n# conjunctions\ndef conj(text):\n    conj = 0\n    doc = nlp(text)\n    for token in doc:\n        if token.head.dep_ == \"conj\":\n            conj += 1\n    return conj\n\ndata['conj'] = data['excerpt'].apply(conj) \/ data['sent']\n\n\n# clausal complement\ndef ccomp(text):\n    ccomp = 0\n    doc = nlp(text)\n    for token in doc:\n        if token.head.dep_ == \"ccomp\":\n            ccomp += 1\n    return ccomp\ndata['ccomp'] = data['excerpt'].apply(ccomp) \/ data['sent']\n\n\n# relative clause modifier (whose, who)\ndef relcl(text):\n    relcl = 0\n    doc = nlp(text)\n    for token in doc:\n        if token.head.dep_ == \"relcl\":\n            relcl += 1\n    return relcl\n\ndata['relcl'] = data['excerpt'].apply(ccomp) \/ data['sent']\ndata.head()","33c71159":"# sentence depth\n\ndef tree_height(root):\n    \n    if not list(root.children):\n        return 1\n    else:\n        return 1 + max(tree_height(x) for x in root.children)\n    \ndef get_average_heights(text):\n    \n    doc = nlp(text)\n    roots = [sent.root for sent in doc.sents]\n    #print(text)\n    #print(roots)\n    #print([tree_height(root) for root in roots])\n    return np.mean([tree_height(root) for root in roots])\n\ndata['sen_depth'] = data['excerpt'].apply(get_average_heights)\ndata.head()","e6e41467":"names = ['target', 'standard_error', 'words', 'ACW', 'sent', 'ASL', 'syl', 'ASW', 'poly_syl','PHW','nq','n\/pn',\n        'def_art', 'sconj','prep', 'advcl','conj', 'ccomp', 'relcl', 'sen_depth' ]\n        \ncorrelations = data[names].corr()\n# plot correlation matrix\nfig = pyplot.figure(figsize=(20,10))\nax = fig.add_subplot(111)\ncax = ax.matshow(correlations, vmin=-1, vmax=1)\nfig.colorbar(cax)\n\nticks = np.arange(0,20,1) \nax.set_xticks(ticks)\nax.set_yticks(ticks)\nax.set_xticklabels(names) \nax.set_yticklabels(names)\n\npyplot.show()","f9c6602f":"# select relevant features and construct a dataset that consist only of engineered features\nselected_names = ['syl','ACW','ASL','ACW','PHW','nq','n\/pn','def_art','sconj', 'prep','advcl','conj','ccomp','relcl','sen_depth']\n\ny = data['target']\nX = data[selected_names]\nX.head()","1ff0be19":"# define score metrics\nrmse = lambda y_true, y_pred: np.sqrt(mse(y_true, y_pred))\nrmse_loss = lambda Estimator, X, y: rmse(y, Estimator.predict(X))","f3e4d717":"# check performance of a simple Linear Regression on engineered features\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\nfrom sklearn.metrics import mean_squared_error as mse\n\nmodel = LinearRegression()\n\nval_score = cross_val_score(\n    model, \n    X, \n    y, \n    scoring=rmse_loss\n).mean()\n\nprint(f'Train Score for Linear Regression: {val_score}')","36116415":"# normalize features\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X)\nX_norm = scaler.transform(X)\n\nXtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=0)","f944e7b1":"# Compare different algorithms\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression, Ridge, BayesianRidge\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\n\nimport matplotlib.pyplot as plt\n\n# prepare models\nmodels = []\nmodels.append(('LR', LinearRegression())) \nmodels.append(('Ridge', Ridge()))\nmodels.append(('NB', BayesianRidge()))\nmodels.append(('SVM', SVR(C=0.5)))\n\n\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = rmse_loss\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n    label_results = []\n    cv_results = cross_val_score(model, Xtrain, ytrain, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std()) \n    print(msg)\n\n# boxplot algorithm comparison\nfig = plt.figure() \nfig.suptitle('Algorithm Comparison') \nax = fig.add_subplot(111) \nplt.boxplot(results) \nax.set_xticklabels(names) \nax.set_ylabel('ROC-AUC (weighted)') \nplt.show()","aa07834a":"# vectorization\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nX_vect = vectorizer.fit_transform(data['excerpt'])\nX_vect.shape","afcc2a27":"# use dimensionality reduction \nfrom sklearn.decomposition import TruncatedSVD\n\npca = TruncatedSVD(n_components=1000)\nfit = pca.fit(X_vect)\n\n# summarize components\nplt.plot(fit.explained_variance_ratio_) \n#plt.yscale('log')","12c00d9b":"pca = TruncatedSVD(n_components=300)\nfit = pca.fit(X_vect)\nX_vect_r = pca.transform(X_vect)","7a8c7f9e":"# combine vectorized text data with feture engineered data\nX_vect_feat = np.hstack([X_vect_r ,X_norm])\nXtrain, Xtest, ytrain, ytest = train_test_split(X_vect_feat, y, test_size=0.2, random_state=0)","32972aa5":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n\nn_features = Xtrain.shape[1]\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(n_features)),\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(32, activation='relu'), \n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(1)\n])\n\n\n# Default learning rate for the Adam optimizer is 0.001\n# Let's slow down the learning rate by 10.\nlearning_rate = 0.0001\nmodel.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(learning_rate), metrics=[RootMeanSquaredError()])\nmodel.summary()\n","ac6fcad7":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_root_mean_squared_error', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\n\n\nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=5, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)","2a5984fa":"num_epochs = 50\nhistory = model.fit(Xtrain, ytrain, epochs=num_epochs,\n                    callbacks=[early_stopping,learning_rate_reduction],\n                    validation_split=0.1)","eb763121":"def plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n","60c835f2":"plot_graphs(history, \"root_mean_squared_error\")\nplot_graphs(history, \"loss\")","22756c47":"We can see that using only the features we constracted without the text yields reasonable regression performance. Now we will vectorize the text and add extract some features to expand our training data.","f4d8d3fd":"The average number of conjunctions, adn other tags indentifying prepositional phrases and subbordinate clauses are counted as supplementary measurement of syntactical complexity.\n\nSentences with prepositional phrases and subordinated clauses rise text ambiguity. \n\nCounted with spacy","4a49cb8c":"2. ASL - average sentence lenght = Word \/ Sent (used as a measurement of syntactic complexity)\n\nFor that we toxenize text with spacy to break text in sentences","032ba77d":"3. ASW - average number of syllables per word = Syl \/ Word (used as a measurement of morphological complexity)\n\nWe use textstat (already imported) - a library to calculate statistics from text ","96184012":"# Feature Engineering","43fca4da":"The number of definite articles (def_art) provides a measurement of how abstract the text is. Abstract texts have less definite nouns and articles. Counted per text","8a22ae6b":"Now that we constructed all features commonly related to readibility, let's check how they correlate with the target readability score. ","431e4377":"Syntactic depth counts the maximum depth of every sentence.\n\nComplex sentences are less readable and rise text ambiguity.","e8f5fd16":"# EDA\nLet's see what the hardest and the easiest texts from our data look like. Also, what is it like in the middle of that scale?","23f25d84":"# Introduction\nThe task of assessing the readability of a specific text has a long history of rule-based approaches. \nThere is a collection of readability indices out there, which were developed for various purposes, from school reading assignments to legal hurdles proving that those Data Collection Policies and Disclaimers can actually be understood by humans. Most of them rely on features that can be easily extracted from the text, such as \"average sentence length\" or \"average number of syllables per word.\"\nThis notebook constructs those features collected from various readability indices in the wild in the hope of improving the performance of a predictive model. ","9e08ebf0":"First, we will find parameters that are often used in the readability metrics:\n\n1. ACW - average characters per word = Char \/ Word (used as a measurement of word difficulty)","b5f756d5":"With a helpful recource - https:\/\/readabilityformulas.com\/freetests\/six-readability-formulas.php \nwe can check how various readability metrics evaluate the easiest text according to out traning data target:\n\nReadabilty metrics for the text with the highest target score:\n\nFlesch Reading Ease score: 70.7 (text scale)\nFlesch Reading Ease scored your text: fairly easy to read.\n\nGunning Fog: 8.6 (text scale)\nGunning Fog scored your text: fairly easy to read.\n\nFlesch-Kincaid Grade Level: 6.2\nGrade level: Sixth Grade.\n\nThe Coleman-Liau Index: 12\nGrade level: Twelfth Grade\n\nThe SMOG Index: 7.1\nGrade level: Seventh Grade\n\nAutomated Readability Index: 7.9\nGrade level: 12-14 yrs. old (Seventh and Eighth graders)\n\nLinsear Write Formula : 6\nGrade level: Sixth Grade.\n\nSurprisingly, the highest target score is not the easiest according to this metric:\nThe text with target score 1.7113 is judged as 'fairly easy to read', while the one with score 1.583 is 'very easy to read'.\n\nIntuitively, I would agree, as the 1.7113 text includes a difficult word \"Paleontologists\", while the 1.583 one has nothing of the sort.\n","1db8a450":"4. PHW - persentage of hard words (words of three or more syllables, excluding affixes, proper nouns, compound words) \n\nWe use Stemmer from nltk (spacy does not have stemmer, only lemmatizer) and part of speach tagger\n\nHowever, compound words were not removed","e38fdf9a":"Some other measurement of syntactic complexity were decided to use.\nThe following measurements are taken from  https:\/\/www.semanticscholar.org\/paper\/Classification-into-Readability-Levels-%3A-and-Larsson\/04ebb3c027bc3ba74ae1c4fac1047010ed0037a8\n\nNQ and N\/NP measurements show how much information there are in a text.\n\nNominal quotient (NQ) counts the number of nouns, prepositions and participle divided by the number of pronouns, adverbs and verbs, calculated as a measurement per document.  The normal NQ value is 1.0 \n\nNoun to Pronoun quotient (n\/pn) is the number of nouns divided by the number of pronouns in the text, calculated as a measurement per document. Nouns are a part of speech with high information value and pronouns often repeat previous information.","f01f3880":"Finally, we will train a neural network model on the resulting dataset."}}