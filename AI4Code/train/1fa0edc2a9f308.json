{"cell_type":{"df3e3135":"code","d1544159":"code","84617e3f":"code","2504a4f1":"code","eb54e3ef":"code","1085fbf2":"code","97b85d6f":"code","c257b715":"code","cf45a938":"code","380d1f76":"code","7fdd431f":"code","d82f0fd5":"code","e2d387f0":"code","4c3e7db0":"code","3caa3908":"code","8d90f494":"code","e147cf87":"code","7fceb30d":"code","b7692fae":"markdown","ecfd7419":"markdown","b2f8165f":"markdown","5b7b2d99":"markdown","5b8b7a42":"markdown","071e4ca2":"markdown","55ec4757":"markdown","462074e2":"markdown","a679bdfb":"markdown","b10dc9b3":"markdown","3a504cc2":"markdown"},"source":{"df3e3135":"import tensorflow as tf\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\nimport matplotlib.pyplot as plt\nimport pandas as pd","d1544159":"AUTOTUNE = tf.data.experimental.AUTOTUNE\n\ndef load_img(image_path):\n    '''\n    This function helps load the image from the path\n    inputs:\n    path_to_img = The path of the image\n    outputs:\n    the image itself in form of tf.tensor\n    '''\n    # parse image\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_image(image)\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    return image\n\ndef configure_dataset(dataset):\n    return dataset.cache().prefetch(buffer_size=AUTOTUNE)","84617e3f":"data_dir = '..\/input\/flickr-image-dataset\/flickr30k_images'\nimage_dir = f'{data_dir}\/flickr30k_images'\ncsv_file = f'{data_dir}\/results.csv'","2504a4f1":"df = pd.read_csv(csv_file, delimiter='|')\n# Under scrutiny I had found that 19999 had a messed up entry\ndf[' comment_number'][19999] = ' 4'\ndf[' comment'][19999] = ' A dog runs across the grass .'\ndf['image_name'] = image_dir+'\/'+df['image_name']\ndf.head(5)","eb54e3ef":"print(f'[INFO] The shape of dataframe: {df.shape}')\nprint(f'[INFO] The columns in the dataframe: {df.columns}')\nprint(f'[INFO] Unique rows: {len(pd.unique(df[\"image_name\"]))}')","1085fbf2":"# A simple sanity check to figure the duplicacy issue\ndef duplicacy(index):\n    print(f\"There are `{len(df.loc[df['image_name'] == df['image_name'][index]])}` comments for image `{index}`\")\n\n# Change the index to see for yourself\nduplicacy(index=200)","97b85d6f":"image_name = {\n    'image_name':df[df[' comment_number'] == df[' comment_number'][0]]['image_name'].values,\n}\ncomments = {\n    'comment_0':df[df[' comment_number'] == df[' comment_number'][0]][' comment'].values,\n    'comment_1':df[df[' comment_number'] == df[' comment_number'][1]][' comment'].values,\n    'comment_2':df[df[' comment_number'] == df[' comment_number'][2]][' comment'].values,\n    'comment_3':df[df[' comment_number'] == df[' comment_number'][3]][' comment'].values,\n    'comment_4':df[df[' comment_number'] == df[' comment_number'][4]][' comment'].values,\n}","c257b715":"image_name_df = pd.DataFrame.from_dict(image_name)\ncomments_df = pd.DataFrame.from_dict(comments)\n\nimage_name_df_values = image_name_df[image_name_df.columns].astype(str).values\ncomments_df_values = comments_df[comments_df.columns].astype(str).values\n\nimage_name_ds = tf.data.Dataset.from_tensor_slices(image_name_df_values)\ncomments_ds = tf.data.Dataset.from_tensor_slices(comments_df_values)","cf45a938":"VOCAB_SIZE = 10000\nMAX_SEQUENCE_LENGTH = 15\n\nint_vectorize_layer = TextVectorization(\n    max_tokens=VOCAB_SIZE,\n    output_mode='int',\n    output_sequence_length=MAX_SEQUENCE_LENGTH)","380d1f76":"# Adapt the state of the layer to the current data\nint_vectorize_layer.adapt(comments_ds)","7fdd431f":"# Function that will map text to the int embeds\ndef int_vectorize_text(text):\n    text = tf.expand_dims(text, -1)\n    return int_vectorize_layer(text)","d82f0fd5":"text = next(iter(comments_ds))\nprint(\"[INFO] COMMENTS:\",text)\nprint(\"[INFO] `int` VECOTRIZED COMMENTS:\",int_vectorize_text(text))","e2d387f0":"# Build the int comments dataset\nint_comments_ds = comments_ds.map(int_vectorize_text)","4c3e7db0":"# Join the two datasets\n# Image name dataset + int vectorised comments dataset\nname_comments_ds = tf.data.Dataset.zip((image_name_ds, int_comments_ds))","3caa3908":"def process(image_name,comments):\n    \"\"\"\n    This function takes image_name and comments\n    and returns the image and comments.\n    \n    Args:\n        image_name (tensor): The path name to the image\n        comments (tensor): The comments, preferably the int vectorised\n    \"\"\"\n    img = load_img(image_name[0])\n    return img, comments","8d90f494":"train_ds = name_comments_ds.map(process)","e147cf87":"for image, comments in train_ds.take(1):\n    print(image.shape)\n    print(comments.shape)","7fceb30d":"plt.figure(figsize=(10, 10))\nfor image, comments in train_ds.take(2):\n    plt.imshow(image)\n    plt.show()\n    print(comments)","b7692fae":"# Imports\nThe global imports are as follows:\n- tensorflow\n- matplotlib\n- pandas","ecfd7419":"> Sanity check with one caption","b2f8165f":"## Dividing data\nThe thought behind this section is to obtain a `tf.data.Dataset` which consists of elements in this format:\n```python\n{\n    image,\n    comment0,\n    comment1,\n    comment2,\n    comment3,\n    comment4\n}\n```\n\nWith that in mind these are the steps that I have taken to get the dataset done:\n- Make two dataframes. One for image_names and the other for comments.\n- Build two different `tf.data.Dataset` objects.\n- Proprocess the comments dataset.\n- Zip the two datasets together.\n- Map a function to obtain image from image_names and keep the comments as it is.","5b7b2d99":"## DataFrame\nLoad the `results.csv` in the form of a dataframe.\n\nWhile building the notebook I had come across a problem with the `csv` file. The entry at index `19999` was messed up. This is why you can see hard coded values for the respective indices. Doing this makes the code later simpler.","5b8b7a42":"# The Joint Dataset","071e4ca2":"> The unique rows are `31,783` while there are `1,58,915` rows in the dataframe. On scrutiny we will find that each image has 5 comments. This is why there are 5 times the rows as there are unique images. ","55ec4757":"# Data","462074e2":"> Let us get some information from the dataset","a679bdfb":"# Introduction\nIn this notebook I have tried consuming the `Flickr dataset` with `tf.data.Dataset`. This can act as the data pipeline for others to work their model on. The dataframe is built from scratch, so there is a lot of flexibility that the user can get.\n\nI have made the dataset such that each element of the dataset has two components.\n- Image - (height, width, channel)\n- Comments - (5, seq_length)","b10dc9b3":"# Util function\n- Load image: This helps in loading images from the path given\n- configure_dataset: Helps in caching and fetching the dataset","3a504cc2":"## TextVectorization\nHere we create a `TextVectorization` layer. As per the tf tutorial, this layer is capable of `Standardization`, `Tokenization` and `Vectorization` all at once."}}