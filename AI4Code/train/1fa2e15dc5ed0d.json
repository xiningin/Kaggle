{"cell_type":{"780af853":"code","e4f97928":"code","1a1705ab":"code","f776db42":"code","c7f32f6c":"code","1f79c07b":"code","c6623027":"code","dcb65c6d":"code","53036612":"code","d52e761e":"code","023018b5":"code","713f32f9":"code","a8d7b9fb":"code","b195c9b8":"code","9a7919e3":"code","ff352377":"code","9dedfd6b":"code","cf48ef7b":"code","cc15b33d":"code","4369b85c":"code","f6e69831":"code","a195aded":"code","fab8a476":"code","49e817cc":"code","43fb35da":"code","2ab49e2f":"code","42090032":"code","23db6e11":"code","4238a999":"code","8cf06542":"code","4c9d15ad":"code","dccd8c3c":"code","12f9b241":"code","47634360":"markdown","8524c4d5":"markdown","b727f93e":"markdown","08671489":"markdown","5fa7075a":"markdown","fa6960c4":"markdown","183e6d84":"markdown","4383a24c":"markdown","2eaa075e":"markdown","9cfd931b":"markdown","9fba1dfa":"markdown","08e86b9d":"markdown","9f30082e":"markdown","f9df8542":"markdown","288cea7b":"markdown","0f9d233a":"markdown","b802cb5a":"markdown","6091fb08":"markdown","9c3d199a":"markdown","3d6bf4b5":"markdown","27677f06":"markdown","fe24c8a3":"markdown","7a7e107f":"markdown"},"source":{"780af853":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\ninit_notebook_mode(connected=True)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e4f97928":"data = pd.read_csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv')\ndata.head()","1a1705ab":"data.info()","f776db42":"numerical_features = ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']","c7f32f6c":"def plot_distribution(feature):\n    sns.displot(x=feature, data=data, kde=True, color='#244747');\n    plt.figtext(0.2, 1, '%s Distribution'%feature, fontfamily='serif', fontsize=17, fontweight='bold');\n\ndef plot_num_cat(feature, target, figsize=None):\n    fig = plt.figure(figsize=(15,6))\n\n    for value in data[target].unique():\n        sns.kdeplot(data[data[target]==value][feature])\n\n    fig.legend(labels=data[target].unique())\n    plt.title('{} distribution based on {}'.format(feature, target))\n    plt.show()\n    \ndef plot_num_num(feature, target):\n    sns.regplot(x=feature, y=target, data=data, color='#244747')\n    plt.show()\n    \ndef plot_cat_cat(feature, target):\n    plot_data = data.groupby([feature, target])[feature].agg({'count'}).reset_index()\n\n    fig = px.sunburst(plot_data, path=[feature, target], values='count', #color_continuous_scale='gray', color=feature, \n                      title='Affect of {} on Customer {}'.format(feature, target), width = 600, height = 600)\n    \n    fig.update_layout(plot_bgcolor='white', title_font_family='Calibri Black', title_font_color='#221f1f', \n                      title_font_size=22, title_x=0.5)\n    fig.update_traces(textinfo = 'label + percent parent')\n    fig.show()","1f79c07b":"# data is highly skewed and hence taking log transformation\ndf = data.copy()\nskewed_features = ['BALANCE', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS']\n#'PURCHASES_INSTALLMENTS_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY', 'BALANCE_FREQUENCY', 'CASH_ADVANCE_FREQUENCY', \n#'PRC_FULL_PAYMENT', 'TENURE', 'cluster']\\\nfor feature in skewed_features:\n    data[feature] = np.log(1+df[feature])","c6623027":"# looking at the distribution of data\nfor feature in numerical_features:\n    plot_distribution(feature)","dcb65c6d":"plt.figure(figsize=(15, 8))\nsns.heatmap(round(data[numerical_features].corr(method='spearman'), 2), \n            annot=True, mask=None, cmap='GnBu')\nplt.show()","53036612":"corr_mat = data[numerical_features].corr()\ncorr_mat = corr_mat.unstack()\ncorr_mat = corr_mat.sort_values(kind=\"quicksort\").drop_duplicates()\ncorr_mat[corr_mat>0.5]","d52e761e":"from statsmodels.stats.outliers_influence import variance_inflation_factor","023018b5":"# Calculating VIF\nvif = pd.DataFrame()\ndf = data.dropna()\nvif[\"variables\"] = [feature for feature in numerical_features if feature not in ['MINIMUM_PAYMENTS', 'CREDIT_LIMIT', 'PURCHASES_TRX',\n                                                                                'BALANCE', 'PURCHASES_FREQUENCY', 'PAYMENTS',\n                                                                                'PURCHASES', 'CASH_ADVANCE_TRX', 'BALANCE_FREQUENCY',\n                                                                                'INSTALLMENTS_PURCHASES']]\nvif[\"VIF\"] = [variance_inflation_factor(df[vif['variables']].values, i) for i in range(len(vif[\"variables\"]))]\nprint(vif)","713f32f9":"missingValueFeatures = pd.DataFrame({'missing %': data.isnull().sum()*100\/len(data)})\nmissingValueFeatures[missingValueFeatures['missing %']>0]","a8d7b9fb":"# Imputing CREDIT_LIMIT & MINIMUM_PAYMENTS as per BALANCE\nprint('Before Imputation:')\nprint(data[['CREDIT_LIMIT', 'MINIMUM_PAYMENTS']].describe().T)\n\ndata.sort_values(by='BALANCE', inplace=True)\n# now use backfill method to replace\ndata['CREDIT_LIMIT'].fillna(data['CREDIT_LIMIT'].mean(), inplace=True)\ndata['MINIMUM_PAYMENTS'].fillna(method='bfill', inplace=True)\n\nprint('\\nAfter Imputation:')\nprint(data[['CREDIT_LIMIT', 'MINIMUM_PAYMENTS']].describe().T)","b195c9b8":"NumericData = data[[feature for feature in numerical_features if feature not in []]]\nNumericMelt = NumericData.melt()\nplt.figure(figsize=(15,10))\nplt.figtext(0.1, 1, \"Boxplots for Numerical variables\", fontfamily='serif', fontsize=17, fontweight='bold')\nbp = sns.boxplot(x='variable', y='value', data=NumericMelt, palette=['#244247']*len(numerical_features))\nbp.set_xticklabels(bp.get_xticklabels(), rotation=90)\nplt.show()","9a7919e3":"# Percentage of outliers present in each variable\noutlier_percentage = {}\nfor feature in numerical_features:\n    tempData = data.sort_values(by=feature)[feature]\n    Q1, Q3 = tempData.quantile([0.25, 0.75])\n    IQR = Q3 - Q1\n    Lower_range = Q1 - (1.5 * IQR)\n    Upper_range = Q3 + (1.5 * IQR)\n    outlier_percentage[feature] = round((((tempData<(Q1 - 1.5 * IQR)) | (tempData>(Q3 + 1.5 * IQR))).sum()\/tempData.shape[0])*100,2)\noutlier_percentage","ff352377":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, rand_score\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering \nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import SpectralClustering \nfrom sklearn.cluster import DBSCAN","9dedfd6b":"feature_cols = [feature for feature in data.columns if feature not in(['CUST_ID'])]\ntrain_data = data.copy()[feature_cols]\nprint('features used- ', feature_cols)","cf48ef7b":"# rescaling data\nscale = StandardScaler()\ntrain_data = scale.fit_transform(train_data)","cc15b33d":"inertia=[]\nsilhouetteScore = []\nn_clusters = 30\nfor i in range(2, n_clusters):\n    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=0)\n    kmeans.fit(train_data)\n    inertia.append(kmeans.inertia_)\n    silhouetteScore.append(silhouette_score(train_data, kmeans.predict(train_data)))\n\nfig, ax1 = plt.subplots(figsize=(8, 5))\n#fig.text(0.1, 1, 'Skipping ', fontfamily='serif', fontsize=12, fontweight='bold')\nfig.text(0.1, 0.95, 'We want to select a point where Inertia is low & Silhouette Score is high, and the number of clusters is not overwhelming for the business.',\n         fontfamily='serif',fontsize=10)\nfig.text(1.4, 1, 'Inertia', fontweight=\"bold\", fontfamily='serif', fontsize=15, color='#244747')\nfig.text(1.51, 1, \"|\", fontweight=\"bold\", fontfamily='serif', fontsize=15, color='black')\nfig.text(1.53, 1, 'Silhouette Score', fontweight=\"bold\", fontfamily='serif', fontsize=15, color='#91b8bd')\n\nax1.plot(range(2, n_clusters), inertia, '-', color='#244747', linewidth=5)\nax1.plot(range(2, n_clusters), inertia, 'o', color='#91b8bd')\nax1.set_ylabel('Inertia')\n\nax2 = ax1.twinx()\nax2.plot(range(2, n_clusters), silhouetteScore, '-', color='#91b8bd', linewidth=5)\nax2.plot(range(2, n_clusters), silhouetteScore, 'o', color='#244747', alpha=0.8)\nax2.set_ylabel('Silhouette Score')\n\nplt.xlabel('Number of clusters')\nplt.show()","4369b85c":"model = KMeans(n_clusters=7, init='k-means++', random_state=0, algorithm='elkan')\ny = model.fit_predict(train_data)","f6e69831":"dist = 1-cosine_similarity(train_data)\n\npca = PCA(2)\npca.fit(dist)\nX_PCA = pca.transform(dist)\n\n# Visualizing all the clusters \nplt.figure(figsize=(15,8))\nsns.scatterplot(x=X_PCA[:, 0], y=X_PCA[:, 1], \n                hue=y, palette=sns.color_palette('hls', model.cluster_centers_.shape[0]), s=50)\nplt.title('Cluster of Customers', size=15, pad=10)\nplt.legend(loc=0, bbox_to_anchor=[1,1])\nplt.show()","a195aded":"dist = 1-cosine_similarity(train_data)\n\npca = PCA(3)\npca.fit(dist)\nX_PCA = pca.transform(dist)\n\n# Visualizing all the clusters \nfig = px.scatter_3d(x=X_PCA[:,0], y=X_PCA[:,1], z=X_PCA[:,2],\n                    color=y, opacity=0.8)\nfig.show()","fab8a476":"data['cluster'] = y\nfor feature in numerical_features:\n    plot_num_cat(feature, 'cluster')","49e817cc":"plt.figure(figsize = (12, 5))\nplt.text(5, 250, 'The no. of clusters is the no. of vertical lines in the dendrogram cut by a horizontal line that can transverse the maximum distance vertically without intersecting a cluster.',\n         fontfamily='serif',fontsize=12)\ndendo = dendrogram(linkage(train_data, method='ward'))\nplt.plot([250]*20000, color='r')\nplt.plot([150]*20000, color='r')\nplt.show()","43fb35da":"model = AgglomerativeClustering(n_clusters=7, affinity='euclidean', linkage='ward')\ny = model.fit_predict(train_data)","2ab49e2f":"dist = 1-cosine_similarity(train_data)\n\npca = PCA(2)\npca.fit(dist)\nX_PCA = pca.transform(dist)\n\n# Visualizing all the clusters \nplt.figure(figsize=(15,8))\nsns.scatterplot(x=X_PCA[:, 0], y=X_PCA[:, 1], \n                hue=y, palette=sns.color_palette('hls', len(np.unique(y))), s=50)\nplt.title('Cluster of Customers', size=15, pad=10)\nplt.legend(loc=0, bbox_to_anchor=[1,1])\nplt.show()","42090032":"# Number of clusters is determined using elbow method above\nmodel = GaussianMixture(n_components=7)\ny = model.fit_predict(train_data)","23db6e11":"dist = 1-cosine_similarity(train_data)\n\npca = PCA(2)\npca.fit(dist)\nX_PCA = pca.transform(dist)\n\n# Visualizing all the clusters \nplt.figure(figsize=(15,8))\nsns.scatterplot(x=X_PCA[:, 0], y=X_PCA[:, 1], \n                hue=y, palette=sns.color_palette('hls', len(np.unique(y))), s=50)\nplt.title('Cluster of Customers', size=15, pad=10)\nplt.legend(loc=0, bbox_to_anchor=[1,1])\nplt.show()","4238a999":"# Building the clustering model \nmodel = SpectralClustering(n_clusters=7, affinity='rbf') \n  \n# Training the model and Storing the predicted cluster labels \ny = model.fit_predict(train_data)","8cf06542":"dist = 1-cosine_similarity(train_data)\n\npca = PCA(2)\npca.fit(dist)\nX_PCA = pca.transform(dist)\n\n# Visualizing all the clusters \nplt.figure(figsize=(15,8))\nsns.scatterplot(x=X_PCA[:, 0], y=X_PCA[:, 1], \n                hue=y, palette=sns.color_palette('hls', len(np.unique(y))), s=50)\nplt.title('Cluster of Customers', size=15, pad=10)\nplt.legend(loc=0, bbox_to_anchor=[1,1])\nplt.show()","4c9d15ad":"from sklearn.neighbors import NearestNeighbors\n# finding nearest points distance for every row in data\nneigh = NearestNeighbors(n_neighbors=2)\nnbrs = neigh.fit(train_data)\ndistances, indices = nbrs.kneighbors(train_data)\n\n# Plotting K-distance Graph\ndistances = np.sort(distances, axis=0)\ndistances = distances[:,1]\nplt.figure(figsize=(10,5))\np=max(distances)+0.5\nplt.text(-500, p, 'TK-distance Graph', fontfamily='serif', fontsize=15, fontweight='bold')\nplt.text(-500, p-0.25, 'The optimum value of epsilon is at the point of maximum curvature in the K-Distance Graph, which is 1.75 in this case.',\n        fontfamily='serif', fontsize=12)\nplt.plot(distances)\nplt.xlabel('Data Points sorted by distance', fontsize=14)\nplt.ylabel('Epsilon', fontsize=14)\nplt.show()","dccd8c3c":"# Building the clustering model \nmodel = DBSCAN(eps=1.75, min_samples=10) \n  \n# Training the model and Storing the predicted cluster labels \ny = model.fit_predict(train_data)","12f9b241":"dist = 1-cosine_similarity(train_data)\n\npca = PCA(2)\npca.fit(dist)\nX_PCA = pca.transform(dist)\n\n# Visualizing all the clusters \nplt.figure(figsize=(15,8))\nsns.scatterplot(x=X_PCA[:, 0], y=X_PCA[:, 1], \n                hue=y, palette=sns.color_palette('hls', len(np.unique(y))), s=50)\nplt.title('Cluster of Customers', size=15, pad=10)\nplt.legend(loc=0, bbox_to_anchor=[1,1])\nplt.show()","47634360":"## Visualizing the clusters using PCA","8524c4d5":"## Visualizing the clusters using PCA","b727f93e":"* CUSTID : Identification of Credit Card holder (Categorical)\n* BALANCE : Balance amount left in their account to make purchases\n* BALANCEFREQUENCY : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n* PURCHASES : Amount of purchases made from account\n* ONEOFFPURCHASES : Maximum purchase amount done in one-go\n* INSTALLMENTSPURCHASES : Amount of purchase done in installment\n* CASHADVANCE : Cash in advance given by the user\n* PURCHASESFREQUENCY : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n* ONEOFFPURCHASESFREQUENCY : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased)\n* PURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n* CASHADVANCEFREQUENCY : How frequently the cash in advance being paid\n* CASHADVANCETRX : Number of Transactions made with \"Cash in Advanced\"\n* PURCHASESTRX : Numbe of purchase transactions made\n* CREDITLIMIT : Limit of Credit Card for user\n* PAYMENTS : Amount of Payment done by user\n* MINIMUM_PAYMENTS : Minimum amount of payments made by user\n* PRCFULLPAYMENT : Percent of full payment paid by user\n* TENURE : Tenure of credit card service for user","08671489":"### 3D Plot","5fa7075a":"# Model 1: KMeans","fa6960c4":"|COLUMN|CORRELATION|\n|---|---|\n|CREDIT_LIMIT|NA|\n|MINIMUM_PAYMENTS|BALANCE|\n\n**We can impute missing values as per the correlation table above.**","183e6d84":"## Visualizing the clusters using PCA","4383a24c":"Based on Silhoutte Score and Elbow, we can consider clusters to be 7","2eaa075e":"### Using ELBOW Method to figure out number of clusters","9cfd931b":"# CORRELATION","9fba1dfa":"# Analyzing features using VIF","08e86b9d":"# Handling Missing Values","9f30082e":"# EDA","f9df8542":"**Observations-**\nMay be in some other version","288cea7b":"# Model 3: Gaussian Mixture","0f9d233a":"# Model 5: DBSCAN","b802cb5a":"# Looking at Outliers","6091fb08":"# Model 2: Hierarchical Clustering","9c3d199a":"# Model 4: Spectral Clustering","3d6bf4b5":"# Model Interpretation","27677f06":"# Traning ML Model","fe24c8a3":"Let's take log transformation of all features with high outlier values and see how it looks","7a7e107f":"### 2D Plot"}}