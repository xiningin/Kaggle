{"cell_type":{"f63a60b1":"code","4a9b2b47":"code","028bb1e7":"code","6e5e25a6":"code","f4f0b4a7":"code","cd5fff90":"code","024f070c":"code","cef6d421":"markdown","8675903c":"markdown","41e56afc":"markdown"},"source":{"f63a60b1":"import os\nimport re \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport shutil\n        \nshutil.copyfile(src = \"..\/input\/bad-words-for-tweets\/bad_words.py\", dst = \"..\/working\/bad_words.py\")\nfrom bad_words import whole_words","4a9b2b47":"all_data = pd.read_csv(\"\/kaggle\/input\/emotion\/text_emotion.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/train.csv\")\nval = pd.read_csv(\"\/kaggle\/input\/tweet-sentiment-extraction\/test.csv\")\n\n# we use train + test to test the mapping \ntrain = pd.concat([train, val])\ntrain.dropna(how=\"any\", subset=[\"text\"], inplace=True)\ntrain.reset_index(inplace=True, drop=True)\n\nall_data = all_data.rename(columns={\"tweet_id\" : \"textID\", \"content\" : \"text\"})","028bb1e7":"def remove_html(text):\n\ttext = re.sub(\"&quot;\", \"'\", text)\n\ttext = re.sub(\"&gt;\", \">\",  text)\n\ttext = re.sub(\"&lt;\", \"<\", text)\n\ttext = re.sub(\"&le;\", \"\u2264\", text)\n\ttext = re.sub(\"&ge;\", \"\u2265\", text)\n\ttext = re.sub(\"&amp;\", \"&\", text)\n\treturn text\n\ndef find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1\n\ninchars = \"abcdefghijklmnopqrstuvwxyz\u00e5\u00e4*'\u00f60123456789\"\ndef clean_text(text):\n    text = re.sub( \"'\", \"`\", text)\n    text = remove_html(text)\n    text = re.sub( \"@[a-zA-Z0-9]+\", '', str(text))  # sloppy regex to remove @user \n    for word in whole_words:\n        old_txt = text\n        if word.lower() in text.lower():\n            starts = find_all(text.lower(), word.lower())\n            while len(starts) != 0:\n                start = starts[0]\n                end = start+len(word)\n                # skip the word if the preceding or end character is a number or in the alphabet\n                if len(text[:start]) != 0 and text[:start][-1].lower() in inchars:\n                    starts.remove(start)\n                    continue\n                elif len(text[end:]) != 0 and text[end:][0].lower() in inchars:\n                    starts.remove(start)\n                    continue\n                \n                text = text[:start] + \"****\" + text[end:]\n                starts = find_all(text.lower(), word.lower())\n   \n    # only edge case\n    text = re.sub(\" x x \", ' **** ',  text)    \n    return text\n","6e5e25a6":"all_data[\"old_text\"] = all_data.text\nall_data.text = all_data.text.map(clean_text)","f4f0b4a7":"added = 0\nunmapped = 0\nall_texts = train.text.tolist()\nall_ids = train.textID.astype(str).tolist()\nfor idx in range(len(all_data)):\n    text = all_data.text[idx]\n    \n    if text in all_texts:\n        index = all_texts.index(text)\n        all_texts.pop(index)\n        text_id = all_ids.pop(index)\n        all_data.loc[idx, \"aux_id\"] = text_id\n        added += 1\n    else:\n        unmapped += 1\n\nprint(f\"Unmapped:{unmapped} Total:{len(all_data)} Prop: {(added)\/len(all_data)}\")","cd5fff90":"# Get the unique Id's \nauxes = all_data.aux_id.unique().tolist()\n# remove \"nan\" which is the first index \nauxes.pop(0) \n# show the unmapped train+test texts\ntrain[~train.textID.isin(auxes)]","024f070c":"all_data = all_data.replace(r'^\\s*$', np.nan, regex=True)\nall_data = all_data.where(pd.notnull(all_data), None)\n\nindex = 1000000000\nfor idx in range(len(all_data)):\n    if all_data.aux_id[idx] == None:\n        all_data.loc[idx, \"aux_id\"] = f\"p{index}\" \n        index += 1\n\nall_data.rename(columns={\"aux_id\" : \"textID\"}, inplace=True)\nall_data.to_csv(\"all_data.csv\", index=False)","cef6d421":"### Add old_text for reference and then clean the data","8675903c":"### Santiy check that we got all the text in train+test mapped correctly","41e56afc":"### Check how well the mapping worked "}}