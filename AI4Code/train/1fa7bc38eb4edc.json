{"cell_type":{"f7eeabdf":"code","c0d3007b":"code","e320382e":"code","5d9a4e42":"code","52fd2ba1":"code","9f9170f4":"code","6823c845":"code","0111a85d":"code","5900485d":"code","50299517":"code","4c10a0c1":"code","4fa454e2":"code","af21d0ea":"code","d6b9f1e1":"code","8e6febaf":"code","c5983e3f":"code","4f79cd60":"code","9ba9a403":"code","fb6dea63":"code","d99cff42":"code","dfb4efee":"code","1fa24406":"code","ba3eaea7":"code","7900042f":"code","3430d03f":"code","5e6106d3":"code","3cb1fcd3":"code","b728487e":"code","21f6c2be":"code","f8bb7b60":"code","6b13d142":"code","de50cb36":"code","3ba14726":"code","80d56016":"code","bbd750d0":"code","5999975b":"code","6f6ed784":"code","5f77ff57":"code","070e366b":"code","e5df78d8":"code","7e981ea2":"code","fb7236b0":"code","a5ebbb37":"markdown","2ba2cbac":"markdown","cab2dcf5":"markdown","7ed02c11":"markdown","b37f9536":"markdown","b039f0f1":"markdown","053f252d":"markdown","f3feeda2":"markdown","46c012b1":"markdown","f1cabf94":"markdown","d0958e8a":"markdown","d3a88735":"markdown","199b2ed3":"markdown","68b9ffb2":"markdown","06fc8131":"markdown","866b4781":"markdown","396ed2fa":"markdown","27caa0f7":"markdown","14ca171f":"markdown","6c921e62":"markdown","b7e91b06":"markdown","759e5d2e":"markdown","3365b9cd":"markdown","22a0830e":"markdown","cd57b839":"markdown"},"source":{"f7eeabdf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","c0d3007b":"df = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')\n#df.sample((5), random_state=789)\ndf.head(1)","e320382e":"df.info()\n#df.describe()","5d9a4e42":"df.isna() .sum()","52fd2ba1":"print(df.duplicated().sum())","9f9170f4":"onehots = pd.get_dummies(df['Vehicle_Age'], prefix='Vehicle_Age')\ndf = df.join(onehots)","6823c845":"onehots2 = pd.get_dummies(df['Gender'], prefix='Gender')\ndf = df.join(onehots2)","0111a85d":"onehots3 = pd.get_dummies(df['Vehicle_Damage'], prefix='Vehicle_Damage')\ndf = df.join(onehots3)","5900485d":"#df.sample(5)\ndf.info()","50299517":"df = df.drop(['id', 'Gender', 'Vehicle_Damage', 'Vehicle_Age'], axis=1) ","4c10a0c1":"df.info()","4fa454e2":"print(f'Count of rows before filtering outlier: {len(df)}')\n\nfiltered_entries = np.array([True] * len(df))\nfor col in ['Annual_Premium']:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    low_limit = Q1 - (IQR * 1.5)\n    high_limit = Q3 + (IQR * 1.5)\n\n    filtered_entries = ((df[col] >= low_limit) & (df[col] <= high_limit)) & filtered_entries\n    \ndf = df[filtered_entries]\n\nprint(f'Count of rows after filtering outlier: {len(df)}')\n\n# Visualisasi Boxplot \nnumerikready = ['Age', 'Driving_License', 'Previously_Insured', 'Annual_Premium', 'Vintage', 'Response']\nfig, ax = plt.subplots(1,1, figsize=(20,7))\nfor i in range(0, len(numerikready)):\n    plt.subplot(2, np.ceil(len(numerikready)\/2), i+1)\n    sns.boxplot(df[numerikready[i]], color='teal', orient='v')\n    plt.tight_layout()","af21d0ea":"from sklearn.preprocessing import StandardScaler\ndf['Annual_Premium_std'] = StandardScaler().fit_transform(df['Annual_Premium'].values.reshape(len(df), 1))\n\nstd = ['Annual_Premium_std']\n\ndisplay(df[std].describe())","d6b9f1e1":"df = df.drop(['Annual_Premium'], axis=1)\ndf.describe()","8e6febaf":"display(df.sample(1))\nprint('#'*100)\nprint(df['Response'].value_counts())","c5983e3f":"X = df[[col for col in df.columns if (str(df[col].dtype) != 'object') and col not in ['Response']]]\ny = df['Response'].values\nprint(X.shape)\nprint(y.shape)","4f79cd60":"from imblearn import over_sampling\nX_over, y_over = over_sampling.RandomOverSampler().fit_resample(X, y)\ndf_y_over = pd.Series(y_over).value_counts()\ndf_y_over","9ba9a403":"pd.DataFrame(y_over).rename(columns = {0 : 'Response'})","fb6dea63":"df = pd.concat([X_over, pd.DataFrame(y_over).rename(columns = {0 : 'Response'})], axis=1)\ndf","d99cff42":"### this code is optional if you want to export the pre-processed data to csv.\n#df.to_csv('train_pre_processed.csv')","dfb4efee":"from sklearn.ensemble import RandomForestClassifier","1fa24406":"# Split Feature Vector and Label\nX = df.drop(['Response'], axis = 1) # menggunakan semua feature kecuali target\ny = df['Response'] # target \/ label\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test,y_train,y_test = train_test_split(X,\n                                                y,\n                                                test_size = 0.3,\n                                                random_state = 789)","ba3eaea7":"y_test.count()","7900042f":"rf = RandomForestClassifier(n_estimators= 400, max_depth=110, random_state=0)\nrf.fit(X_train, y_train)","3430d03f":"y_predicted = rf.predict(X_test)\ny_predicted","5e6106d3":"# OUTLIERS throwed away \n# Data oversampled on Response == 1\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score\nprint('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\n\nprint('\\naccuracy')\nprint(accuracy_score(y_test, y_predicted))\nprint('\\nprecision')\nprint(precision_score(y_test, y_predicted))\n\n\nprint('\\nclassification report')\nprint(classification_report(y_test, y_predicted)) # generate the precision, recall, f-1 score","3cb1fcd3":"print(\"train Accuracy : \",rf.score(X_train,y_train))\nprint(\"test Accuracy : \",rf.score(X_test,y_test))","b728487e":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_predicted, pos_label=1) # pos_label: label yang kita anggap positive\nprint('Area Under ROC Curve (AUC):', auc(fpr, tpr))","21f6c2be":"plt.subplots(figsize=(10, 6))\nplt.plot(fpr, tpr, 'o-', label=\"ROC curve\")\nplt.plot(np.linspace(0,1,10), np.linspace(0,1,10), label=\"diagonal\")\nfor x, y, txt in zip(fpr, tpr, thresholds):\n    plt.annotate(np.round(txt,2), (x, y-0.04))\nplt.legend(loc=\"upper left\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")","f8bb7b60":"feat_importances = pd.Series(rf.feature_importances_, index=X.columns)\nax = feat_importances.nlargest(10).plot(kind='barh')\nax.invert_yaxis()\nplt.xlabel('score')\nplt.ylabel('feature')\nplt.title('feature importance score')","6b13d142":"from sklearn.neighbors import KNeighborsClassifier\n\n# Split Feature Vector and Label\nX = df.drop(['Response'], axis = 1) # menggunakan semua feature kecuali target\ny = df['Response'] # target \/ label\n\n#Splitting the data into Train and Test\nfrom sklearn.model_selection import train_test_split \nX_train, X_test,y_train,y_test = train_test_split(X,\n                                                y,\n                                                test_size = 0.3,\n                                                random_state = 789)\n\nneigh = KNeighborsClassifier(n_neighbors = 3)\nneigh.fit(X,y)","de50cb36":"y_predicted = neigh.predict(X_test)\ny_predicted","3ba14726":"from sklearn.metrics import classification_report, confusion_matrix\nprint('\\nconfustion matrix') # generate the confusion matrix\nprint(confusion_matrix(y_test, y_predicted))\n\nfrom sklearn.metrics import accuracy_score\nprint('\\naccuracy')\nprint(accuracy_score(y_test, y_predicted))\n\nfrom sklearn.metrics import classification_report\nprint('\\nclassification report')\nprint(classification_report(y_test, y_predicted)) # generate the precision, recall, f-1 score, num","80d56016":"from sklearn.metrics import roc_curve, auc\nfpr, tpr, thresholds = roc_curve(y_test, y_predicted, pos_label=1) # pos_label: label yang kita anggap positive\nprint('Area Under ROC Curve (AUC):', auc(fpr, tpr))\nprint(\"train Accuracy : \",neigh.score(X_train,y_train))\nprint(\"test Accuracy : \",neigh.score(X_test,y_test))","bbd750d0":"dfkaggletest = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')\ndfkaggletest.info()","5999975b":"onehots = pd.get_dummies(dfkaggletest['Vehicle_Age'], prefix='Vehicle_Age')\ndfkaggletest = dfkaggletest.join(onehots)\nonehots2 = pd.get_dummies(dfkaggletest['Gender'], prefix='Gender')\ndfkaggletest = dfkaggletest.join(onehots2)\nonehots3 = pd.get_dummies(dfkaggletest['Vehicle_Damage'], prefix='Vehicle_Damage')\ndfkaggletest = dfkaggletest.join(onehots3)","6f6ed784":"dfkaggletest = dfkaggletest.drop(['id', 'Gender', 'Vehicle_Damage', 'Vehicle_Age'], axis=1) ","5f77ff57":"print(f'Jumlah baris sebelum memfilter outlier: {len(dfkaggletest)}')\n\nfiltered_entries = np.array([True] * len(dfkaggletest))\nfor col in ['Annual_Premium']:\n    Q1 = dfkaggletest[col].quantile(0.25)\n    Q3 = dfkaggletest[col].quantile(0.75)\n    IQR = Q3 - Q1\n    low_limit = Q1 - (IQR * 1.5)\n    high_limit = Q3 + (IQR * 1.5)\n    \n    for i in dfkaggletest[col]:\n        if i > high_limit :\n                dfkaggletest[col] = np.where(dfkaggletest[col] > high_limit, high_limit, dfkaggletest[col])\n        else:\n            i = i\n\ndfkaggletest = dfkaggletest\n\nprint(f'Jumlah baris setelah memfilter outlier: {len(dfkaggletest)}')\n\n# Visualisasi Boxplot \nnumerikready = ['Age', 'Driving_License', 'Previously_Insured', 'Annual_Premium', 'Vintage']#, 'Response']\nfig, ax = plt.subplots(1,1, figsize=(20,7))\nfor i in range(0, len(numerikready)):\n    plt.subplot(2, np.ceil(len(numerikready)\/2), i+1)\n    sns.boxplot(dfkaggletest[numerikready[i]], color='teal', orient='v')\n    plt.tight_layout()","070e366b":"from sklearn.preprocessing import StandardScaler\ndfkaggletest['Annual_Premium_std'] = StandardScaler().fit_transform(dfkaggletest['Annual_Premium'].values.reshape(len(dfkaggletest), 1))\n\nstd = ['Annual_Premium_std']\n\ndfkaggletest = dfkaggletest.drop(['Annual_Premium'], axis=1)\ndfkaggletest.describe()","e5df78d8":"y_predicted = neigh.predict(dfkaggletest)\ny_predicted","7e981ea2":"dfkagglesubmission = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')\ndfid = dfkagglesubmission[['id']]\n\ndfkagglesubmission = pd.concat([dfid, pd.DataFrame(y_predicted).rename(columns = {0 : 'Response'})], axis=1)\n\ndfkagglesubmission","fb7236b0":"dfkagglesubmission.to_csv('test_submission.csv')","a5ebbb37":"## BALANCING CLASS (Response)\n* There is class imbalance in Response feature, so I decide to balancing it. \n* I oversampled the Response == 1 Class. I oversampled it using RandomOverSampler. I use oversampling because it resulted better than undersampling.","2ba2cbac":"### One Hot Encodings\n\n* Vehicle_Age (3 value counts)\n* Gender (2 value counts)\n* Vehicle_Damage (2 value counts)","cab2dcf5":"* Surprisingly in this analysis kNN resulted a little bit better than Random Forest. \n* kNN method also not overfitted nor underfitted because train accuracy score vs test accuracy score is same (0.9465 vs 0.9459).","7ed02c11":"## SAVE to CSV PreProcessed","b37f9536":"### Outliers Method 1 : throw away outliers\n(There are 2 method i used to handle outliers. At ends, this method resulted a little bit better.)","b039f0f1":"Hello world, this is my very first notebook in kaggle. I'm just a totally newbie to programming and Data Science. In this notebook I got result of 0.945 AUC, 95% accuracy and 95% precision which is I personally doubtfull about that result. How come it could get so high compared to other notebooks posted in this ? \nIn this analysis, I oversampled the data because there are class imbalance in Target feature (Response). And I also have not predicting the test dataset yet. \n\nPlease let me know whether it is to good to be true on getting that very high AUC or it is what it is?\n<br>Thank you in advance.\n\nPS, in this notebook I just post the Random Forest and kNN methods as both of them resulted better compared to Logistic Regression and Decision Tree which I managed to get 0.7~0.8 AUC.","053f252d":"## STANDARDIZATION\nI standardize the ``Annual_Premium`` because it has a very large value compared to others features.","f3feeda2":"# PREDICTING THE KAGGLE TEST DATASET","46c012b1":"print(f'Jumlah baris sebelum memfilter outlier: {len(df)}')\n\nfiltered_entries = np.array([True] * len(df))\nfor col in ['Annual_Premium']:\n    Q1 = df[col].quantile(0.25)\n    Q3 = df[col].quantile(0.75)\n    IQR = Q3 - Q1\n    low_limit = Q1 - (IQR * 1.5)\n    high_limit = Q3 + (IQR * 1.5)\n    \n    for i in df[col]:\n        if i > high_limit :\n                df[col] = np.where(df[col] > high_limit, high_limit, df[col])\n        else:\n            i = i\n\ndf = df\n\nprint(f'Jumlah baris setelah memfilter outlier: {len(df)}')\n\n# Visualisasi Boxplot \nnumerikready = ['Age', 'Driving_License', 'Previously_Insured', 'Annual_Premium', 'Vintage', 'Response']\nfig, ax = plt.subplots(1,1, figsize=(20,7))\nfor i in range(0, len(numerikready)):\n    plt.subplot(2, np.ceil(len(numerikready)\/2), i+1)\n    sns.boxplot(df[numerikready[i]], color='teal', orient='v')\n    plt.tight_layout()","f1cabf94":"**Health Insurance Cross Sell Prediction \ud83c\udfe0 \ud83c\udfe5** <br>\nhttps:\/\/www.kaggle.com\/anmolkumar\/health-insurance-cross-sell-prediction","d0958e8a":"## Save the BALANCED dataset","d3a88735":"* The best performing models is kNN, even without tuning hyperparameter already resulted best among others. It has 95% accuracy, and 95% weighted-avg F1 score. \n* kNN model is not overfitted nor underfitted because has the same train vs test accuracy (0.94 vs 0.94). This better than Random Forest which is a little bit overfitted because it has higher difference (0.99 vs 0.94)\n* kNN is faster to run (+- 3.5 minutes) than Random Forest (+- 5 minutes) <- this what it was in my local PC.\n* Best pre-processing methods:\n * Features : All feature is used, with 3 features are one-hot encoded.\n * Outliers : filtered (throw away)\n * Standardized : Yes on Annual_Premium\n * Class balancing : RandomOversampling\n<br> \n<br> \n\n* And also specifically, it has 90% True Positive precision which is obviously will boost conversion ratio after this model is implemented in the company. It will boost sales\/marketing team performance because now they know which customer to be targeted (The Predicted **Yes** Response).","199b2ed3":" **Feature to Use:**\n* **``All 10 Features from Gender to Vintage, because:``**\n * No missing values nor duplicate.\n * We can do feature encoding, outliers handling, standardization, class balancing.","68b9ffb2":"### Evaluation : kNN (k-Nearest Neighbors)","06fc8131":"print('Number of values under outliers :', sum(df['Annual_Premium'] < low_limit))\nprint('Number of values above outliers :', sum(df['Annual_Premium'] > high_limit))","866b4781":"# CONCLUSION","396ed2fa":"### Evaluation of Best Random Forest model based on pre-processing method \nBased on several trial, these are the conditions that resulted best: \n * Features : All feature is used, with 3 features are one-hot encoded.\n * Outliers : filtered (throw away)\n * Standardized : Yes on Annual_Premium\n * Class balancing : RandomOversampling on Response 1 Class\n<br>\n<br>\n * Best n_estimators: 400\n * Best max_depth: 110\n","27caa0f7":"# DATA PRE-PROCESSING:\n## Feature Engineering","14ca171f":"# Load Dataset","6c921e62":"# MODELLING\n## Random Forest\n* I use n_estimators= 400, and max_depth=110 because it resulted best after I tuned hyperparameter with randomized search.\n","b7e91b06":"#### DROP Unused Features\n* drop ID (ga kepake)\n* drop 'Gender' (label encoded)\n* drop 'Vehicle_Damage' (label encoded)\n* drop 'Vehicle_Age' (one hot encoded)","759e5d2e":"## kNN Method","3365b9cd":"## OUTLIERS Handling (Annual Premium)\n* I'm using IQR method to decide outliers, one of the reason is because the data is not normally distributed.","22a0830e":"### Outliers Method 2 : change outliers to minimum or maximum (fence value).\nThis Cell is not run because the Method 1 actually resulted better, but i show the code here. <br>\nDespite this method probably more applicable to predict the test dataset.","cd57b839":"### DROP UnStandardized data"}}