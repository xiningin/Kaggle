{"cell_type":{"4f3d80ff":"code","411761ab":"code","572c7d0a":"code","7b297ba0":"code","127fa99f":"code","0d086e49":"code","97b02c04":"code","28eae130":"code","53680735":"code","0fb304b5":"code","165a0fbe":"code","2d9e4044":"code","b85b78f7":"code","87cbe654":"code","b6ddaac8":"code","48cedee2":"code","6e828a2c":"code","328e037e":"code","989e687e":"code","97e09015":"code","713e1aa5":"code","4254c203":"code","100bc524":"code","95160017":"code","0c30258c":"code","0e9f717b":"code","1fbc7ab0":"code","f1ce3417":"code","81d0e090":"code","c6cf6c80":"code","377c5661":"code","775e7577":"code","aa814c17":"code","5ac8ee7d":"code","a8d1cd10":"code","8fedbf4a":"code","b4242ea6":"code","379dcfdb":"code","d77b6796":"code","9605c6da":"code","3626512d":"code","da754e80":"code","d7321096":"code","d1d535e0":"code","5c1ca1b1":"code","ab01ec3a":"code","135c3f61":"code","99f3d31c":"code","1f6366d8":"code","2715c88c":"code","c15d2545":"code","76971312":"code","3d351a03":"code","bc49dec9":"code","98fba44b":"code","0216f9cc":"code","337c8c38":"code","96478232":"code","5c579c9a":"code","013729e6":"code","02387aff":"code","530b2d80":"code","2963d274":"code","836273ab":"code","b6c8e2a1":"code","6f079684":"code","a2e3912d":"markdown","0e6fa6f3":"markdown","31584a50":"markdown","de07717d":"markdown","0f355806":"markdown","581c56e4":"markdown","7726f960":"markdown","ab7b5804":"markdown","264da563":"markdown","06f30149":"markdown","a1a633b2":"markdown","2f0082f6":"markdown","ecb1a4a0":"markdown","d325e4f3":"markdown","ba480049":"markdown","4488413e":"markdown","cc02ce42":"markdown","88f3c148":"markdown","885e5987":"markdown","3407c4c6":"markdown","87bb3f79":"markdown","7f891181":"markdown","54b020a8":"markdown","24bbba85":"markdown","f43746c3":"markdown","6ab8baba":"markdown","7236145a":"markdown","ea2898dd":"markdown","28262869":"markdown","be7c2189":"markdown","29550648":"markdown","c9466547":"markdown","1008f957":"markdown","1a6a239d":"markdown","12d14f69":"markdown","27b54174":"markdown","67611d58":"markdown","a241f54f":"markdown","5ea2cd32":"markdown","3c22791a":"markdown","e63767d6":"markdown","a88e44d3":"markdown","90fc4caa":"markdown","3ef22015":"markdown","e789344b":"markdown","8de82066":"markdown","7dac5d3d":"markdown","fb85ef4a":"markdown","af4d21bc":"markdown","dff6e537":"markdown","71f81eac":"markdown","1854474f":"markdown","8356dde2":"markdown","6c89d2dc":"markdown","12ce1571":"markdown","c9645726":"markdown","8c4e8d63":"markdown","912b338c":"markdown","b98a6de9":"markdown","1e77c835":"markdown","88b026ae":"markdown","60c192f6":"markdown"},"source":{"4f3d80ff":"# Data Loading and Numerical Operations\nimport pandas as pd\nimport numpy as np\n# Data Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n# Data Resampling\nfrom sklearn.utils import resample\n# Data Feature Selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# Data Splitting\nfrom sklearn.model_selection import train_test_split\n# Data Scaling\nfrom sklearn.preprocessing import MinMaxScaler\n# Data Modeling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_curve, classification_report\n# Hyperparameter Tuning\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n# Ensembling\nfrom mlxtend.classifier import StackingCVClassifier","411761ab":"data = pd.read_csv(\"..\/input\/framingham-heart-study-dataset\/framingham.csv\") # Reading and converting the data into a pandas dataframe\ndata.shape # Calculating the dimensions of the dataset","572c7d0a":"data.head(10)","7b297ba0":"data.info()","127fa99f":"data.isnull().sum()","0d086e49":"data.duplicated().sum()","97b02c04":"print((data[\"glucose\"].mode())[0])","28eae130":"data[\"glucose\"].fillna((data[\"glucose\"].mode())[0], inplace=True)","53680735":"data.dropna(inplace=True)\ndata.isnull().sum()","0fb304b5":"plt.figure(figsize=(20,10), facecolor='w')\nsns.boxplot(data=data)\nplt.show()","165a0fbe":"data['totChol'].max()","2d9e4044":"data['sysBP'].max()","b85b78f7":"data = data[data['totChol']<600.0]\ndata = data[data['sysBP']<295.0]\ndata.shape","87cbe654":"data.describe()","b6ddaac8":"#Checking relationship between variables\ncor=data.corr()\nplt.figure(figsize=(20,10), facecolor='w')\nsns.heatmap(cor,xticklabels=cor.columns,yticklabels=cor.columns,annot=True)\nplt.title(\"Correlation among all the Variables of the Dataset\", size=20)\ncor","48cedee2":"categorical_features = ['male', 'education', 'currentSmoker', 'BPMeds', 'prevalentStroke', 'prevalentHyp', 'diabetes']","6e828a2c":"for feature in categorical_features:\n    print(feature,':')\n    print(data[feature].value_counts())\n    print(\"-----------------\")","328e037e":"num_plots = len(categorical_features)\ntotal_cols = 2\ntotal_rows = num_plots\/\/total_cols + 1\nfig, axs = plt.subplots(nrows=total_rows, ncols=total_cols,\n                        figsize=(7*total_cols, 7*total_rows), facecolor='w', constrained_layout=True)\nfor i, var in enumerate(categorical_features):\n    row = i\/\/total_cols\n    pos = i % total_cols\n    plot = sns.countplot(x=var, data=data, ax=axs[row][pos])","989e687e":"numeric_features = ['cigsPerDay', 'totChol', 'sysBP', 'diaBP', 'BMI', 'heartRate', 'glucose']\nfor feature in numeric_features:\n    plt.figure(figsize=(18, 10), facecolor='w')\n    sns.distplot(data[feature])\n    plt.title('{} Distribution'.format(feature), fontsize=20)\n    plt.show()","97e09015":"num_plots = len(numeric_features)\ntotal_cols = 2\ntotal_rows = num_plots\/\/total_cols + 1\ncolor = ['m', 'g', 'b', 'r', 'y', 'v', 'o']\nfig, axs = plt.subplots(nrows=total_rows, ncols=total_cols,\n                        figsize=(7*total_cols, 7*total_rows), facecolor='w', constrained_layout=True)\nfor i, var in enumerate(numeric_features):\n    row = i\/\/total_cols\n    pos = i % total_cols\n    plot = sns.violinplot(y=var, data=data, ax=axs[row][pos], linewidth=2)","713e1aa5":"#Distribution of outcome variable, Heart Disease\nplt.figure(figsize=(12, 10), facecolor='w')\nplt.subplots_adjust(right=1.5)\nplt.subplot(121)\nsns.countplot(x=\"TenYearCHD\", data=data)\nplt.title(\"Count distribution of TenYearCHD\", size=20)\nplt.subplot(122)\nlabels=[0,1]\nplt.pie(data[\"TenYearCHD\"].value_counts(),autopct=\"%1.1f%%\",labels=labels,colors=[\"lime\",\"red\"])\nplt.show()","4254c203":"#Grouping education and cigsPerDay\n\ngraph_1 = data.groupby(\"education\", as_index=False).cigsPerDay.mean()","100bc524":"plt.figure(figsize=(12,8), facecolor='w')\nsns.regplot(x=graph_1[\"education\"], y=graph_1[\"cigsPerDay\"])\nplt.title(\"Graph showing cigsPerDay in every level of education.\", size=20)\nplt.xlabel(\"education\", size=20)\nplt.ylabel(\"cigsPerDay\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","95160017":"#checking for which gender has more risk of coronary heart disease CHD\n\ngraph_2 = data.groupby(\"male\", as_index=False).TenYearCHD.sum()","0c30258c":"#Ploting the above values\n\nplt.figure(figsize=(12,8), facecolor='w')\nsns.barplot(x=graph_2[\"male\"], y=graph_2[\"TenYearCHD\"])\nplt.title(\"Graph showing which gender has more risk of coronary heart disease CHD\", size=20)\nplt.xlabel(\"Gender\\n0 is female and 1 is male\",size=20)\nplt.ylabel(\"TenYearCHD cases\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","0e9f717b":"#Distribution of current smokers with respect to age\nplt.figure(figsize=(30,15), facecolor='w')\nsns.countplot(x=\"age\",data=data,hue=\"currentSmoker\")\nplt.title(\"Graph showing which age group has more smokers.\", size=30)\nplt.xlabel(\"age\", size=20)\nplt.ylabel(\"age Count\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","1fbc7ab0":"plt.figure(figsize=(30,12), facecolor='w')\nsns.countplot(x=\"TenYearCHD\",data=data,hue=\"cigsPerDay\")\nplt.legend(title='cigsPerDay', fontsize='large')\nplt.title(\"Graph showing the relation between cigsPerDay and risk of coronary heart disease.\", size=30)\nplt.xlabel(\"Risk of TenYearCHD\", size=20)\nplt.ylabel(\"Count of TenYearCHD\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)\nplt.show()","f1ce3417":"# Grouping up the data and ploting it\n\ngraph_3 = data.groupby(\"TenYearCHD\", as_index=False).sysBP.mean()\n\nplt.figure(figsize=(12,8), facecolor='w')\nsns.barplot(x=graph_3[\"TenYearCHD\"], y=graph_3[\"sysBP\"])\nplt.title(\"Graph showing the relation between sysBP and risk of CHD\", size=20)\nplt.xlabel(\"Risk of CHD\", size=20)\nplt.ylabel(\"sysBP\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","81d0e090":"plt.figure(figsize=(12,8), facecolor='w')\nsns.regplot(x=graph_3[\"TenYearCHD\"], y=graph_3[\"sysBP\"])\nplt.title(\"Distribution of sysBP in relation to the risk of CHD\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","c6cf6c80":"# Grouping up the data and ploting it\n\ngraph_4 = data.groupby(\"TenYearCHD\", as_index=False).diaBP.mean()\n\nplt.figure(figsize=(12,8), facecolor='w')\nsns.barplot(x=graph_4[\"TenYearCHD\"], y=graph_4[\"diaBP\"])\nplt.title(\"Graph showing the relation between diaBP and risk of CHD\", size=20)\nplt.xlabel(\"Risk of CHD\", size=20)\nplt.ylabel(\"diaBP\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","377c5661":"plt.figure(figsize=(12,8), facecolor='w')\nsns.regplot(x=graph_4[\"TenYearCHD\"], y=graph_4[\"diaBP\"])\nplt.title(\"Distribution of diaBP in relation to the risk of CHD\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","775e7577":"plt.figure(figsize=(20,10), facecolor='w')\nsns.boxplot(x=\"age\",y=\"totChol\",data=data)\nplt.title(\"Distribution of age with respect to totChol\", size=20)\nplt.show()","aa814c17":"#Plotting a linegraph to check the relationship between age and cigsPerDay, totChol, glucose.\n\ngraph_5 = data.groupby(\"age\").cigsPerDay.mean()\ngraph_6 = data.groupby(\"age\").totChol.mean()\ngraph_7 = data.groupby(\"age\").glucose.mean()\n\nplt.figure(figsize=(16,10), facecolor='w')\nsns.lineplot(data=graph_5, label=\"cigsPerDay\")\nsns.lineplot(data=graph_6, label=\"totChol\")\nsns.lineplot(data=graph_7, label=\"glucose\")\nplt.title(\"Graph showing totChol and cigsPerDay in every age group.\", size=20)\nplt.xlabel(\"age\", size=20)\nplt.ylabel(\"count\", size=20)\nplt.xticks(size=12)\nplt.yticks(size=12)","5ac8ee7d":"#sysBP vs diaBP with respect to currentSmoker and male attributes\n#plt.figure(figsize=(18, 9), facecolor='w')\nsns.lmplot('sysBP', 'diaBP', \n           data=data,\n           hue=\"TenYearCHD\",\n           col=\"male\",row=\"currentSmoker\")\nplt.show()","a8d1cd10":"target1=data[data['TenYearCHD']==1]\ntarget0=data[data['TenYearCHD']==0]","8fedbf4a":"target1=resample(target1,replace=True,n_samples=len(target0),random_state=40)","b4242ea6":"target=pd.concat([target0,target1])","379dcfdb":"target['TenYearCHD'].value_counts()","d77b6796":"data=target\nnp.shape(data)","9605c6da":"#Distribution of heart disease cases in the balanced dataset, the outcome variable\nplt.figure(figsize=(12, 10), facecolor='w')\nplt.subplots_adjust(right=1.5)\nplt.subplot(121)\nsns.countplot(x=\"TenYearCHD\", data=data)\nplt.title(\"Count of TenYearCHD column\", size=20)\nplt.subplot(122)\nlabels=[0,1]\nplt.pie(data[\"TenYearCHD\"].value_counts(),autopct=\"%1.1f%%\",labels=labels,colors=[\"red\",\"lime\"])\nplt.show()","3626512d":"#To idenfify the features that have larger contribution towards the outcome variable, TenYearCHD\nX=data.iloc[:,0:15]\ny=data.iloc[:,-1]\nprint(\"X - \", X.shape, \"\\ny - \", y.shape)","da754e80":"#Apply SelectKBest and extract top 10 features\nbest=SelectKBest(score_func=chi2, k=10)","d7321096":"fit=best.fit(X,y)","d1d535e0":"data_scores=pd.DataFrame(fit.scores_)\ndata_columns=pd.DataFrame(X.columns)","5c1ca1b1":"#Join the two dataframes\nscores=pd.concat([data_columns,data_scores],axis=1)\nscores.columns=['Feature','Score']\nprint(scores.nlargest(11,'Score'))","ab01ec3a":"#To visualize feature selection\nscores=scores.sort_values(by=\"Score\", ascending=False)\nplt.figure(figsize=(20,7), facecolor='w')\nsns.barplot(x='Feature',y='Score',data=scores,palette='BuGn_r')\nplt.title(\"Plot showing the best features in descending order\", size=20)\nplt.show()","135c3f61":"#Select 10 features\nfeatures=scores[\"Feature\"].tolist()[:10]\nfeatures","99f3d31c":"data=data[['sysBP','glucose','age','cigsPerDay','totChol','diaBP','prevalentHyp','male','BPMeds','diabetes','TenYearCHD']]\ndata.head()","1f6366d8":"y = data['TenYearCHD']\nX = data.drop(['TenYearCHD'], axis=1)\ntrain_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.4, random_state=1)","2715c88c":"scaler = MinMaxScaler()\ntrain_x = scaler.fit_transform(train_x)\ntest_x = scaler.transform(test_x)","c15d2545":"m1 = 'LogisticRegression'\nlr = LogisticRegression(random_state=1, max_iter=1000)\nmodel = lr.fit(train_x, train_y)\nlr_predict = lr.predict(test_x)\nlr_conf_matrix = confusion_matrix(test_y, lr_predict)\nlr_acc_score = accuracy_score(test_y, lr_predict)\nprint(\"confussion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(test_y,lr_predict))","76971312":"m2 = 'KNeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors=1)\nmodel = knn.fit(train_x, train_y)\nknn_predict = knn.predict(test_x)\nknn_conf_matrix = confusion_matrix(test_y, knn_predict)\nknn_acc_score = accuracy_score(test_y, knn_predict)\nprint(\"confussion matrix\")\nprint(knn_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of k-NN Classification:\",knn_acc_score*100,'\\n')\nprint(classification_report(test_y, knn_predict))","3d351a03":"m3 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=200, random_state=0,max_depth=12)\nrf.fit(train_x,train_y)\nrf_predicted = rf.predict(test_x)\nrf_conf_matrix = confusion_matrix(test_y, rf_predicted)\nrf_acc_score = accuracy_score(test_y, rf_predicted)\nprint(\"confussion matrix\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(classification_report(test_y,rf_predicted))","bc49dec9":"m4 = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 30)\ndt.fit(train_x,train_y)\ndt_predicted = dt.predict(test_x)\ndt_conf_matrix = confusion_matrix(test_y, dt_predicted)\ndt_acc_score = accuracy_score(test_y, dt_predicted)\nprint(\"confussion matrix\")\nprint(dt_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\nprint(classification_report(test_y,dt_predicted))","98fba44b":"m5 = 'Gradient Boosting Classifier'\ngvc =  GradientBoostingClassifier()\ngvc.fit(train_x,train_y)\ngvc_predicted = gvc.predict(test_x)\ngvc_conf_matrix = confusion_matrix(test_y, gvc_predicted)\ngvc_acc_score = accuracy_score(test_y, gvc_predicted)\nprint(\"confussion matrix\")\nprint(gvc_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Gradient Boosting Classifier:\",gvc_acc_score*100,'\\n')\nprint(classification_report(test_y,gvc_predicted))","0216f9cc":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","337c8c38":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid, \n                               n_iter = 100, \n                               cv = 3, \n                               verbose=2, \n                               random_state=7, \n                               n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(train_x,train_y)","96478232":"rf_hyper = rf_random.best_estimator_\nrf_hyper.fit(train_x,train_y)\nprint(\"Accuracy on training set is : {}\".format(rf_hyper.score(train_x,train_y)))\nprint(\"Accuracy on validation set is : {}\".format(rf_hyper.score(test_x, test_y)))\nrf_predicted = rf_hyper.predict(test_x)\nrf_acc_score = accuracy_score(test_y, rf_predicted)\nprint(\"Accuracy of Hyper-tuned Random Forest Classifier:\",rf_acc_score*100,'\\n')\nprint(classification_report(test_y, rf_predicted))","5c579c9a":"#Number of trees\nn_estimators = [int(i) for i in np.linspace(start=100,stop=1000,num=10)]\n#Number of features to consider at every split\nmax_features = ['auto','sqrt']\n#Maximum number of levels in tree\nmax_depth = [int(i) for i in np.linspace(10, 100, num=10)]\nmax_depth.append(None)\n#Minimum number of samples required to split a node\nmin_samples_split=[2,5,10]\n#Minimum number of samples required at each leaf node\nmin_samples_leaf = [1,2,4]\n\n#Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf}","013729e6":"gb=GradientBoostingClassifier(random_state=0)\n#Random search of parameters, using 3 fold cross validation, \n#search across 100 different combinations\ngb_random = RandomizedSearchCV(estimator=gb, param_distributions=random_grid,\n                              n_iter=100, scoring='f1', \n                              cv=3, verbose=2, random_state=0, n_jobs=-1,\n                              return_train_score=True)\n\n# Fit the random search model\ngb_random.fit(train_x,train_y)","02387aff":"gb_hyper = gb_random.best_estimator_\ngb_hyper.fit(train_x,train_y)\nprint(\"Accuracy on training set is : {}\".format(gb_hyper.score(train_x,train_y)))\nprint(\"Accuracy on validation set is : {}\".format(gb_hyper.score(test_x, test_y)))\ngbc_predicted = gb_hyper.predict(test_x)\ngbc_acc_score = accuracy_score(test_y, gbc_predicted)\nprint(\"Accuracy of Hyper-tuned Gradient Boosting Classifier:\",gbc_acc_score*100,'\\n')\nprint(classification_report(test_y, gbc_predicted))","530b2d80":"lr_false_positive_rate,lr_true_positive_rate,lr_threshold = roc_curve(test_y,lr_predict)\nknn_false_positive_rate,knn_true_positive_rate,knn_threshold = roc_curve(test_y,knn_predict)\nrf_false_positive_rate,rf_true_positive_rate,rf_threshold = roc_curve(test_y,rf_predicted)                                                             \ndt_false_positive_rate,dt_true_positive_rate,dt_threshold = roc_curve(test_y,dt_predicted)\ngbc_false_positive_rate,gbc_true_positive_rate,gbc_threshold = roc_curve(test_y,gbc_predicted)\n\n\nsns.set_style('whitegrid')\nplt.figure(figsize=(15,8), facecolor='w')\nplt.title('Reciever Operating Characterstic Curve')\nplt.plot(lr_false_positive_rate,lr_true_positive_rate,label='Logistic Regression')\nplt.plot(knn_false_positive_rate,knn_true_positive_rate,label='K-Nearest Neighbor')\nplt.plot(rf_false_positive_rate,rf_true_positive_rate,label='Random Forest')\nplt.plot(dt_false_positive_rate,dt_true_positive_rate,label='Desion Tree')\nplt.plot(gbc_false_positive_rate,gbc_true_positive_rate,label='Gradient Boosting Classifier')\nplt.plot([0,1],ls='--')\nplt.plot([0,0],[1,0],c='.5')\nplt.plot([1,1],c='.5')\nplt.ylabel('True positive rate')\nplt.xlabel('False positive rate')\nplt.legend()\nplt.show()","2963d274":"model_ev = pd.DataFrame({'Model': ['Logistic Regression','K-Nearest Neighbour','Random Forest',\n                                   'Decision Tree','Gradient Boosting'], 'Accuracy': [lr_acc_score*100, knn_acc_score*100, \n                                                                                      rf_acc_score*100, dt_acc_score*100,gbc_acc_score*100]})\nmodel_ev","836273ab":"colors = ['red','green','blue','gold','silver']\nplt.figure(figsize=(15,8), facecolor='w')\nplt.title(\"Barplot Representing Accuracy of different models\")\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Models\")\nplt.bar(model_ev['Model'],model_ev['Accuracy'],color = colors)\nplt.show()","b6c8e2a1":"scv=StackingCVClassifier(classifiers=[rf_hyper, gb_hyper, knn, dt], meta_classifier= rf)\ntrain_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=1)\nscv.fit(train_x.values,train_y.values)\nscv_predicted = scv.predict(test_x)\nscv_conf_matrix = confusion_matrix(test_y, scv_predicted)\nscv_acc_score = accuracy_score(test_y, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(test_y,scv_predicted))","6f079684":"model_ev = model_ev.append({\"Model\":\"Stacking Ensemble\", \"Accuracy\":scv_acc_score*100}, ignore_index=True)\nmodel_ev","a2e3912d":"* There is a minor relation between `totChol` and `glucose`.\n* `totChol` has a steep, linear and inverse graph for lower ranges of `age`\n* `cigsPerDay` has a fairly parallel relationship with `age`","0e6fa6f3":"# Conclusion","31584a50":"According to this dataset, `males` have shown a slighly higher risk of coronary heart disease `TenYearCHD`.","de07717d":"### Gradient Boosting Classifier","0f355806":"Dropping all other rows containing missing values","581c56e4":"Among the categorical features:\n* `BPmeds`, `prevalentStroke` and `diabetes` are highly imbalanced.\n* There are four levels of `education` whereas the rest categorical features are all binary\n* The number of Smokers and non-Smokers in `currentSmoker` is almost the same","7726f960":"It shows the number of np.nan or null values or missing values are present in the dataset:\n* education: 105\n* cigsPerDay: 29\n* BPMeds: 53\n* totChol: 50\n* BMI: 19\n* heartRate: 1\n* glucose: 388","ab7b5804":"The distribution is highly imbalanced. As in, the number of negative cases outweigh the number of positive cases. This would lead to class imbalance problem while fitting our models. Therefore, this problem needs to be addressed and taken care of.","264da563":"### Objective: To build a classification model that predicts heart disease in a subject. (note the target column to predict is 'TenYearCHD' where CHD = Coronary heart disease) \n\n### We are going to perform the following steps: \n\n1. Read the file and display columns.\n2. Handle missing values, Outliers and Duplicate Data\n3. Calculate basic statistics of the data (count, mean, std, etc) and exploratory analysts and describe your observations.\n4. Select columns that will be probably important to predict heart disease.\n5. If you remove columns explain why you removed those.\n6. Create training and testing sets (use 60% of the data for the training and reminder for testing).\n7. Build a machine learning model to predict TenYearCHD\nEvaluate the model (f1 score, Accuracy, Precision ,Recall and Confusion Matrix)\n8. Conclude your findings (Model which is giving best f1 score)\n","06f30149":"Filling the missing spaces of `glucose`column with the mode of the data (Mode = 75) present to reduce the number of missing data in our dataset","a1a633b2":"Removable Outliers are detected in `totChol` and `sysBP` columns of our dataset. Outliers in all other numerical columns are important and thus cannot be removed.","2f0082f6":"* Low `cigsPerDay` comes with lower risk of CHD.\n* Those who don't smoke, i.e., with a `cigsPerDay` of 0.0 has a really low risk of contracting the disease\n* Although that is the case, low `cigsPerDay` doesn't actually guarantee a much lower risk of CHD","ecb1a4a0":"* Minor relation found between higher risk of `TenYearCHD` with higher `diaBP` similar to the previous one\n* Majority of people with `diaBP` ranging upto 80.0 has lower chance of contracting the disease.","d325e4f3":"# Predictive Modeling","ba480049":"#### Relation between diaBP and risk of CHD","4488413e":"### 2. Bivariate Analysis","cc02ce42":"Among the numerical features:\n* `totChol`, `sysBP`, `diaBP`and `BMI` has an uniform distribution and the rest are unevenly distributed\n* `cigsPerDay` has a highly uneven distribution with the most data present in 0 \n* `cigsPerDay` and `sysBP` shows quite a bit and slight right skewness respectively.","88f3c148":"#### Which gender has more risk of coronary heart disease CHD","885e5987":"This plot shows the `Features` and their respective `chi-square test` scores","3407c4c6":"#### Categorical Features","87bb3f79":"This shows an overview of the Columns, non-null count and the data types of the dataset","7f891181":"### 3. Multivariate Analysis","54b020a8":"# Exploratory Data Analysis","24bbba85":"The Outlier present in `totChol` is 600.","f43746c3":"* Mid-age groups ranging from the age of 38 - 46 have more number of `currentSmokers`\n* No `currentSmokers` observed below the `age` of 32 \n* maximum age for a `currentSmokers` is 70 ","6ab8baba":"#### Distribution of sysBP vs diaBP with respect to currentSmoker and male attributes","7236145a":"#### Relation between cigsPerDay and risk of coronary heart disease.","ea2898dd":"* There is no linear relationship observed.\n* Level 3 `education` shows the lowest mean.","28262869":"# Classification model to predict 10-year risk of future coronary heart disease (CHD)","be7c2189":"Only these features have strongest influence over the target variable. They are, in particular order:\n* sysBP\n* glucose\n* age\n* cigsPerDay\n* totChol\n* diaBP\n* prevalentHyp\n* male\n* BPMeds\n* diabetes\n","29550648":"Now with the missing values, outliers and duplicate values dealt with, we proceed to perform EDA","c9466547":"# Resampling imbalanced dataset by oversampling positive cases","1008f957":"#### Relation between sysBP and risk of CHD.","1a6a239d":"#### Relationship between age and cigsPerDay, totChol, glucose.","12d14f69":"#### Relation between age and totChol","27b54174":"We use the following different machine learning models for the dataset:\n\n1. Logistic Regressor\n2. K-Nearest Neighbour Classifier\n3. Random Forest Classifier\n4. Decision Tree Classifier\n5. Gradient Boosting Classifier","67611d58":"### About the dataset:\n\nThe dataset is publically available on the Kaggle website, and it is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future coronary heart disease (CHD).The dataset provides the patients\u2019 information. It includes over 4,240 records and 15 attributes.\n","a241f54f":"The above graph plots the relationship between systolic blood pressure and diastolic blood pressure for patients based on their gender and whether they are current smokers or not and plots the best fit line","5ea2cd32":"# Ensembling\n\nIn order to increase the accuracy of the model we use ensembling. Here we use stacking technique. We stack the 4 highest accuracy yielding models to create an ensembled model.","3c22791a":"This is my first notebook. I am trying on this dataset so that even beginners can understand how to do EDA and how to ensemble various models. \n\nSome forget to do EDA and start applying models directly. This can create problem later. So start from EDA first and conclude how our data is distributed and how we can separate by single feauture or multiple feautures. ","e63767d6":"For the same numerical features:\n* `cigsPerDay` has uneven distribution although most of the data is concentrated on `0`\n* The majority portions of the following columns lie in the range: \n    * `totChol`: 150 to 300\n    * `sysBP`: 100 to 150\n    * `diaBP`: 60 to 100\n    * `BMI`: 20 to 30\n    * `heartRate`: 50 to 100\n    * `glucose`: 50 to 150","a88e44d3":"# Feature Selection\n","90fc4caa":"# Data Inspecting and Cleaning ","3ef22015":"# Importing Libraries","e789344b":"### Attributes:\n\n* **sex**: male(0) or female(1);(Nominal)\n* **age**: age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\n* **currentSmoker**: whether or not the patient is a current smoker (Nominal)\n* **cigsPerDay**: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, even half a cigarette.)\n* **BPMeds**: whether or not the patient was on blood pressure medication (Nominal)\n* **prevalentStroke**: whether or not the patient had previously had a stroke (Nominal)\n* **prevalentHyp**: whether or not the patient was hypertensive (Nominal)\n* **diabetes**: whether or not the patient had diabetes (Nominal)\n* **totChol**: total cholesterol level (Continuous)\n* **sysBP**: systolic blood pressure (Continuous)\n* **diaBP**: diastolic blood pressure (Continuous)\n* **BMI**: Body Mass Index (Continuous)\n* **heartRate**: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous because of a large number of possible values.)\n* **glucose**: glucose level (Continuous)\n* **10 year risk of coronary heart disease CHD** (binary: \u201c1\u201d means \u201cYes\u201d, \u201c0\u201d means \u201cNo\u201d) - Target Variable\n","8de82066":"### Random Forest Classifier","7dac5d3d":"# **Model Evaluation**","fb85ef4a":"Thanks for reading my notebook. Hope you like my notebook. Please tell if i can add or do something in this notebook...","af4d21bc":"## Hyperparameter Tuning for best Classifier\n#### Using Randomized Search Cross Validation","dff6e537":"#### Which age group has more smokers.","71f81eac":"* Minor relation of higher risk of `TenYearCHD` found with higher `sysBP`\n* Majority of people with `sysBP` ranging from 72 - 130 has lower chance of contracting the disease.","1854474f":"#### Relationship between education and cigsPerDay","8356dde2":"# Feature Splitting and Scaling","6c89d2dc":"#### Numerical Features","12ce1571":"We divide the dataset into training and test sub-datasets for predictive modeling","c9645726":"An overall Statistical Information is shown\n* It is clearly evident that none of the data is missing in columns.\n* It also shows the mean, standard deviation and other statistical metrices of the dataset \n* It also shows the categorical data of the dataset since they were already converted into discrete numerical values","8c4e8d63":"The boxplots are shifted in an upwardly manner suggesting that aged people have more cholesterol (bad cholesterol in general)","912b338c":"#### Target Variable","b98a6de9":"# Reading Dataset","1e77c835":"The number of positive and negative cases are equal. Hence the classes are now balanced for model fitting","88b026ae":"Compared to all the independent data, the correlation coefficient between education and and target variable TenYearCHD is very low and actually negative.","60c192f6":"### 1. Univariate Analysis"}}