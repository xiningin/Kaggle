{"cell_type":{"a99641dd":"code","950f8d42":"code","a9f66ab9":"code","5a132f4f":"code","47418c2b":"code","77e701f3":"code","e365b81b":"code","91b6496c":"code","1c8b4507":"code","087024f1":"code","1e4022a7":"code","37a8bc55":"code","c26bd5ea":"code","5b18082e":"code","e8efad34":"code","8ec22460":"code","648f331e":"code","49785c04":"code","c9f6aa6b":"code","ddd9e21a":"code","73184480":"code","14e3e289":"code","15542f14":"code","7c82e60e":"code","6c4c7aa1":"code","8ef09699":"code","cf77aec3":"code","b91e67ae":"code","e9f46cc3":"code","c4027c74":"code","e71e48a5":"code","8c169879":"code","73ccfcf9":"code","476a4d65":"code","e3e5b884":"code","e14c5d6a":"code","51013a8e":"code","53043c74":"code","916ef493":"markdown","83742ce1":"markdown","afdd4b7a":"markdown","c81ccb7b":"markdown","c134a2ce":"markdown","dbf35f83":"markdown","5639751d":"markdown","5706fac9":"markdown","3794d807":"markdown","60b03722":"markdown","07033a98":"markdown","458737c9":"markdown","e24fa353":"markdown","ae3ab12e":"markdown"},"source":{"a99641dd":"import numpy as np\nimport pandas as pd\nimport re\nimport nltk\nimport spacy","950f8d42":"train_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_data = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","a9f66ab9":"train_data.head()","5a132f4f":"test_data.head()","47418c2b":"train_data = train_data[['excerpt']]\ntest_data = test_data[['excerpt']]","77e701f3":"train_data['excerpt_lower'] = train_data['excerpt'].str.lower()   # First we need to convert the given texts to string and then apply case conversion methods\ntest_data['excerpt_lower'] = test_data['excerpt'].str.lower()","e365b81b":"train_data.head()","91b6496c":"test_data.head()","1c8b4507":"import requests\n\ndata = requests.get('http:\/\/www.gutenberg.org\/cache\/epub\/8001\/pg8001.html')\ncontent = data.text\nprint(content[2745:3948])","087024f1":"!pip install bs4","1e4022a7":"import re\nfrom bs4 import BeautifulSoup\n\ndef strip_html_tags(text):\n    \"\"\"\n    This function will remove the HTML tags & noise from the scraped data.\n    \"\"\"\n    soup = BeautifulSoup(text, \"html.parser\")\n    [s.extract() for s in soup(['iframe', 'script'])]\n    stripped_text = soup.get_text()\n    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n    return stripped_text\n\nclean_content = strip_html_tags(content)\nprint(clean_content[1163:1957])","37a8bc55":"train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(strip_html_tags)\ntest_data['excerpt_lower'] = test_data['excerpt_lower'].apply(strip_html_tags)","c26bd5ea":"import unicodedata\n\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text","5b18082e":"s = 'S\u00f3m\u011b \u00c1cc\u011bnt\u011bd t\u011bxt'\ns","e8efad34":"remove_accented_chars(s)","8ec22460":"train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(remove_accented_chars)\ntest_data['excerpt_lower'] = test_data['excerpt_lower'].apply(remove_accented_chars)","648f331e":"import re\n\ndef remove_special_characters(text, remove_digits=True):\n    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n    text = re.sub(pattern, '', text)\n    return text","49785c04":"s = \"Well this was fun! See you at 7:30, What do you think!!? #$@@9318@ \ud83d\ude42\ud83d\ude42\ud83d\ude42\"\ns","c9f6aa6b":"remove_special_characters(s, remove_digits=False)","ddd9e21a":"remove_special_characters(s)","73184480":"train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(remove_special_characters)\ntest_data['excerpt_lower'] = test_data['excerpt_lower'].apply(remove_special_characters)","14e3e289":"!pip install contractions\n!pip install textsearch","15542f14":"s = \"Y'all can't expand contractions I'd think! You wouldn't be able to. How'd you do it?\"\ns","7c82e60e":"import contractions\n\nlist(contractions.contractions_dict.items())[:10]","6c4c7aa1":"contractions.fix(s)","8ef09699":"train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(contractions.fix)\ntest_data['excerpt_lower'] = test_data['excerpt_lower'].apply(contractions.fix)","cf77aec3":"# Porter Stemmer\nfrom nltk.stem import PorterStemmer\nps = PorterStemmer()\n\nps.stem('jumping'), ps.stem('jumps'), ps.stem('jumped')","b91e67ae":"ps.stem('lying')","e9f46cc3":"\nps.stem('strange')","c4027c74":"train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(ps.stem)\ntest_data['excerpt_lower'] = test_data['excerpt_lower'].apply(ps.stem)","e71e48a5":"from nltk.stem import WordNetLemmatizer\nwnl = WordNetLemmatizer()","8c169879":"help(wnl.lemmatize)","73ccfcf9":"# lemmatize nouns\nprint(wnl.lemmatize('cars', 'n'))\nprint(wnl.lemmatize('boxes', 'n'))","476a4d65":"train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(wnl.lemmatize)\ntest_data['excerpt_lower'] = test_data['excerpt_lower'].apply(wnl.lemmatize)","e3e5b884":"s = 'The brown foxes are quick and they are jumping over the sleeping lazy dogs!'\ntokens = nltk.word_tokenize(s)\nprint(tokens)","e14c5d6a":"train_data['excerpt_lower'] = train_data['excerpt_lower'].apply(nltk.word_tokenize)\ntest_data['excerpt_lower'] = test_data['excerpt_lower'].apply(nltk.word_tokenize)","51013a8e":"from nltk.corpus import stopwords\n\", \".join(stopwords.words('english'))","53043c74":"STOPWORDS = set(stopwords.words('english'))\ndef remove_stopwords(text):\n    \"\"\"custom function to remove the stopwords\"\"\"\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\ntrain_data[\"excerpt_lower\"] = train_data[\"excerpt_lower\"].apply(lambda text: remove_stopwords(text))\ntest_data[\"excerpt_lower\"] = test_data[\"excerpt_lower\"].apply(lambda text: remove_stopwords(text))","916ef493":"### Removing Special Characters, Numbers & Symbols\nAnother common text pre-processing technique is to remove the special characters, numbers and symbols from texts.\n\n*  Special characters: *&^%#@(!\n*  Numbers: 0-9\n* Symbols: \ud83d\ude42\ud83d\ude42\ud83d\ude42.","83742ce1":"### Expanding Contractions","afdd4b7a":"### Stopword Removal\n","c81ccb7b":"## Importing Libraries\nOne can import all the libraries at a time in single cell or can import libraries on the go wherever needed.","c134a2ce":"### Tokenization\nTokenization is to split the entire paragraph or sentence into single words.","dbf35f83":"### Lower Casing\nThis is one of the basic pre-processing step. This is an important steps to perform as it helps you to convert all the strings into same casing format so that the texts like Lower, lower, and LOWER are considered same. This helps you to reduce the duplication of same word which might be counted as unique words.","5639751d":"# Introduction\nIn this competition, you\u2019ll build algorithms to rate the complexity of reading passages for grade 3-12 classroom use. \n\nIn any machine learning problem, the data cleaning and preprocessing is as important as building a machine learning model, especially when it comes to unstructured data like texts.\n\n![image.png](attachment:96ca3e3c-214a-41f0-9f76-70568097ad53.png)\n\nThis kernel will take you through some of the well known text pre-processing steps like:\n\n*  Lower casing\n*  Removal of HTML tags & noise\n*  Removing accented characters\n*  Removing special characters, numbers & symbols\n*  Handling contractions\n*  Stemming\n*  Lemmatization\n*  Tokenization\n*  Stopwords removal\n\nThese are some text pre-processing steps but we don't generally do all of these steps all the time. These steps are to be selected depending on the context of the data and what exaclty we want to do.\n\nFor example, in sentiment analysis use case, we need not remove the emojis or emoticons as it will convey some important information about the sentiment. Similarly we need to decide based on our use cases.","5706fac9":"### Lemmatization\nLemmatization is similar to stemming in reducing inflected words to their word stem but differs in the way that it makes sure the root word (also called as lemma) belongs to the language.\n\nAs a result, this one is generally slower than stemming process. So depending on the speed requirement, we can choose to use either stemming or lemmatization.\n\nLet us use the WordNetLemmatizer in nltk to lemmatize our sentences","3794d807":"# Credits\n*  https:\/\/www.kaggle.com\/sudalairajkumar\/getting-started-with-text-preprocessing\n*  https:\/\/github.com\/dphi-official\/nlp_essentials\/blob\/master\/notebooks\/01_Text_Wrangling_Examples.ipynb","60b03722":"### Stemming\nStemming is the process of reducing inflected (or sometimes derived) words to their word stem, base or root form\n\nFor example, if there are two words in the corpus walks and walking, then stemming will stem the suffix to make them walk. But say in another example, we have two words console and consoling, the stemmer will remove the suffix and make them consol which is not a proper english word.\n\nThere are several type of stemming algorithms available and one of the famous one is porter stemmer which is widely used. We can use nltk package for the same.","07033a98":"### Removing Accented Characters\nSometimes we may find some accented characters present in the texts we are dealing with. \n\nAccented characters look like: 'S\u00f3m\u011b \u00c1cc\u011bnt\u011bd t\u011bxt'","458737c9":"### Removal of HTML tags & noise\nThis processing steps becomes handy when you are dealing with the scraped data from different websites.","e24fa353":"## Load the dataset","ae3ab12e":"You can observe in the texts above that all the tags like **br** or **img** are removed from the texts."}}