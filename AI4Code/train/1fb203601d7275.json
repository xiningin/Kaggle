{"cell_type":{"8958a827":"code","4832d9f2":"code","3aa79623":"code","be10c2eb":"code","34d547fb":"code","663134fe":"code","7141b255":"code","68c7dbb2":"code","711405f8":"code","20de0520":"code","4b5b40dd":"code","8eb5af72":"code","4319f131":"code","6a74a096":"code","a83d588b":"code","dd5b86e2":"code","6d99d4b7":"code","759e2609":"code","875608fe":"code","1dd988a5":"code","da7ce55e":"code","f65b637b":"markdown","2286d4c8":"markdown","0b395550":"markdown","ffd205d3":"markdown","12111e24":"markdown","0bf757a5":"markdown","b29eadac":"markdown","016f88d5":"markdown","7722a502":"markdown","7e2d8b01":"markdown","ba1bdd01":"markdown","088c626f":"markdown"},"source":{"8958a827":"import os\nimport gc\nimport time\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb","4832d9f2":"# Loading the data\ntrain = pd.read_csv('..\/input\/train.csv', parse_dates=['date'])\ntest = pd.read_csv('..\/input\/test.csv', parse_dates=['date'])\nsample_sub = pd.read_csv('..\/input\/sample_submission.csv')\nprint('Train shape:{}, Test shape:{}'.format(train.shape, test.shape))\ntrain.head()","3aa79623":"# Concatenating train & test\ntrain['train_or_test'] = 'train'\ntest['train_or_test'] = 'test'\ndf = pd.concat([train,test], sort=False)\nprint('Combined df shape:{}'.format(df.shape))\ndel train, test\ngc.collect()","be10c2eb":"# Extracting date features\ndf['dayofmonth'] = df.date.dt.day\ndf['dayofyear'] = df.date.dt.dayofyear\ndf['dayofweek'] = df.date.dt.dayofweek\ndf['month'] = df.date.dt.month\ndf['year'] = df.date.dt.year\ndf['weekofyear'] = df.date.dt.weekofyear\ndf['is_month_start'] = (df.date.dt.is_month_start).astype(int)\ndf['is_month_end'] = (df.date.dt.is_month_end).astype(int)\ndf.head()","34d547fb":"# Sorting the dataframe by store then item then date\ndf.sort_values(by=['store','item','date'], axis=0, inplace=True)","663134fe":"# Creating sales lag features\ndef create_sales_lag_feats(df, gpby_cols, target_col, lags):\n    gpby = df.groupby(gpby_cols)\n    for i in lags:\n        df['_'.join([target_col, 'lag', str(i)])] = \\\n                gpby[target_col].shift(i).values + np.random.normal(scale=5.0, size=(len(df),))\n    return df\n\n# Creating sales rolling mean features\ndef create_sales_rmean_feats(df, gpby_cols, target_col, windows, min_periods=2, shift=1):\n    gpby = df.groupby(gpby_cols)\n    for w in windows:\n        df['_'.join([target_col, 'rmean', str(w)])] = \\\n            gpby[target_col].shift(shift).rolling(window=w, min_periods=min_periods).mean().values +\\\n            np.random.normal(scale=5.0, size=(len(df),))\n    return df\n\n# Creating sales rolling median features\ndef create_sales_rmed_feats(df, gpby_cols, target_col, windows, min_periods=2, shift=1):\n    gpby = df.groupby(gpby_cols)\n    for w in windows:\n        df['_'.join([target_col, 'rmed', str(w)])] = \\\n            gpby[target_col].shift(shift).rolling(window=w, min_periods=min_periods).median().values+\\\n            + np.random.normal(scale=5.0, size=(len(df),))\n    return df","7141b255":"def one_hot_encoder(df, ohe_cols=['store','item','dayofmonth','dayofweek','month','weekofyear']):\n    '''\n    One-Hot Encoder function\n    '''\n    print('Creating OHE features..\\nOld df shape:{}'.format(df.shape))\n    df = pd.get_dummies(df, columns=ohe_cols)\n    print('New df shape:{}'.format(df.shape))\n    return df","68c7dbb2":"# For validation \n# We can choose last 3 months of training period(Oct, Nov, Dec 2017) as our validation set to gauge the performance of the model.\n# OR to keep months also identical to test set we can choose period (Jan, Feb, Mar 2017) as the validation set.\n# Here we will go with the former to keep it simple.\nmasked_series = (df.year==2017) & (df.month.isin([10,11,12]))\ndf.loc[(masked_series), 'train_or_test'] = 'val'\nprint('Validation shape: {}'.format(df.loc[df.train_or_test=='val',:].shape))","711405f8":"# Converting sales of validation period to nan so as to resemble test period\ntrain = df.loc[df.train_or_test.isin(['train','val']), :]\nY_val = train.loc[train.train_or_test=='val', 'sales'].values.reshape((-1))\nY_train = train.loc[train.train_or_test=='train', 'sales'].values.reshape((-1))\ntrain.loc[train.train_or_test=='val', 'sales'] = np.nan\n\n# Creating sales lag, rolling mean, rolling median, ohe features of the above train set\ntrain = create_sales_lag_feats(train, gpby_cols=['store','item'], target_col='sales', \n                               lags=[91,98,105,112,119,126,182,364,546,728])\n# train = create_sales_rmean_feats(train, gpby_cols=['store','item'], target_col='sales', \n#                                  windows=[98,119,182,364], min_periods=2)\ntrain = create_sales_rmed_feats(train, gpby_cols=['store','item'], target_col='sales', \n                                windows=[182,364,546,728], min_periods=2) #98,119,\ntrain = one_hot_encoder(train, ohe_cols=['store','item','dayofweek','month']) \n                        #,'dayofmonth','weekofyear'\n\n# Final train and val datasets\nval = train.loc[train.train_or_test=='val', :]\ntrain = train.loc[train.train_or_test=='train', :]\nprint('Train shape:{}, Val shape:{}'.format(train.shape, val.shape))","20de0520":"avoid_cols = ['date', 'sales', 'train_or_test', 'id', 'year']\ncols = [col for col in train.columns if col not in avoid_cols]\nprint('No of training features: {} \\nAnd they are:{}'.format(len(cols), cols))","4b5b40dd":"def smape(preds, target):\n    '''\n    Function to calculate SMAPE\n    '''\n    smape_val = 0\n    for pred,true in zip(preds, target):\n        if (pred==0) & (true==0):\n            continue\n        else:\n            smape_val += abs(pred-true)\/(abs(pred)+abs(true))\n    smape_val=(200*smape_val)\/len(preds)\n    return smape_val\n\ndef lgbm_smape(preds, train_data):\n    '''\n    Custom Evaluation Function for LGBM\n    '''\n    labels = train_data.get_label()\n    smape_val = smape(preds, labels)\n    return 'SMAPE', smape_val, False","8eb5af72":"# LightGBM parameters\nlgb_params = {'task':'train', 'boosting_type':'gbdt', 'objective':'regression', 'metric': {'rmse'}, \n              'num_leaves': 100, 'learning_rate': 0.05, 'feature_fraction': 0.8, 'verbose': 0, \n              'num_boost_round':15000, 'early_stopping_rounds':30, 'nthread':-1}","4319f131":"# Creating lgbtrain & lgbval\nlgbtrain = lgb.Dataset(data=train.loc[:,cols].values, label=Y_train, feature_name=cols)\nlgbval = lgb.Dataset(data=val.loc[:,cols].values, label=Y_val, reference=lgbtrain, feature_name=cols)","6a74a096":"def lgb_validation(params, lgbtrain, lgbval, X_val, Y_val, verbose_eval):\n    t0 = time.time()\n    evals_result = {}\n    model = lgb.train(params, lgbtrain, num_boost_round=params['num_boost_round'], \n                      valid_sets=[lgbtrain, lgbval], feval=lgbm_smape, \n                      early_stopping_rounds=params['early_stopping_rounds'], evals_result=evals_result, \n                      verbose_eval=verbose_eval)\n    print(model.best_iteration)\n    print('Total time taken to build the model: ', (time.time()-t0)\/60, 'minutes!!')\n    pred_Y_val = model.predict(X_val, num_iteration=model.best_iteration)\n    val_df = pd.DataFrame(columns=['true_Y_val','pred_Y_val'])\n    val_df['pred_Y_val'] = pred_Y_val\n    val_df['true_Y_val'] = Y_val\n    print(val_df.shape)\n    print(val_df.head())\n    print('SMAPE for validation data is:{}'.format(smape(pred_Y_val, Y_val)))\n    return model, val_df","a83d588b":"# Training lightgbm model and validating\nmodel, val_df = lgb_validation(lgb_params, lgbtrain, lgbval, val.loc[:,cols].values, Y_val, verbose_eval=50)","dd5b86e2":"# Let's see top 25 features as identified by the lightgbm model.\nprint(\"Features importance...\")\ngain = model.feature_importance('gain')\nfeat_imp = pd.DataFrame({'feature':model.feature_name(), \n                         'split':model.feature_importance('split'), \n                         'gain':100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\nprint('Top 25 features:\\n', feat_imp.head(25))","6d99d4b7":"# Creating sales lag, rolling mean, rolling median, ohe features of the above train set\ndf = create_sales_lag_feats(df, gpby_cols=['store','item'], target_col='sales', \n                            lags=[91,98,105,112,119,126,182,364,546,728])\n# df = create_sales_rmean_feats(df, gpby_cols=['store','item'], target_col='sales', \n#                               windows=[98,119,182,364], min_periods=2)\ndf = create_sales_rmed_feats(df, gpby_cols=['store','item'], target_col='sales', \n                             windows=[182,364,546,728], min_periods=2) #98,119,\ndf = one_hot_encoder(df, ohe_cols=['store','item','dayofweek','month']) #'dayofmonth',,'weekofyear'\n\n# Final train and test datasets\ntest = df.loc[df.train_or_test=='test', :]\ntrain = df.loc[~(df.train_or_test=='test'), :]\nprint('Train shape:{}, Test shape:{}'.format(train.shape, test.shape))","759e2609":"# LightGBM dataset\nlgbtrain_all = lgb.Dataset(data=train.loc[:,cols].values, label=train.loc[:,'sales'].values.reshape((-1,)), \n                           feature_name=cols)","875608fe":"def lgb_train(params, lgbtrain_all, X_test, num_round):\n    t0 = time.time()\n    model = lgb.train(params, lgbtrain_all, num_boost_round=num_round, feval=lgbm_smape)\n    test_preds = model.predict(X_test, num_iteration=num_round)\n    print('Total time taken in model training: ', (time.time()-t0)\/60, 'minutes!')\n    return model, test_preds","1dd988a5":"# Training lgb model on whole data(train+val)\nlgb_model, test_preds = lgb_train(lgb_params, lgbtrain_all, test.loc[:,cols].values, model.best_iteration+20)\nprint('test_preds shape:{}'.format(test_preds.shape))","da7ce55e":"# Create submission\nsub = test.loc[:,['id','sales']]\nsub['sales'] = test_preds\nsub['id'] = sub.id.astype(int)\nsub.to_csv('submission.csv', index=False)\nsub.head()","f65b637b":"### Date Features","2286d4c8":"## Time-based Validation set","0b395550":"### Features constructed from previous sales values","ffd205d3":"### TODO:\n*  Exponential mean features\n*  Explore other win_type in rolling  ","12111e24":"## Feature Engineering","0bf757a5":"## Loading data","b29eadac":"### Training features","016f88d5":"### OHE of categorical features","7722a502":"## LightGBM Model","7e2d8b01":"## Submission","ba1bdd01":"## Model Validation","088c626f":"## Final Model"}}