{"cell_type":{"1adf441d":"code","fcc2e18c":"code","3cc4268d":"code","9e7a4871":"code","f4ed7fc2":"code","95224fd3":"code","b0e46ce5":"code","9573cbee":"code","cb434cc2":"code","70be1152":"code","bc5a5bff":"code","20e5704e":"code","bf79a21a":"code","4bc0d7e6":"code","85443c53":"code","1246227f":"code","0b49f985":"code","89bf310e":"code","7c56b76c":"code","e7e8e2c5":"code","d3324ee1":"code","726fbb22":"code","56f94a27":"code","372b9efe":"code","6cd06c14":"code","0dab86b9":"code","1f8d9fef":"code","3447cd15":"code","7dcec287":"code","af11fff7":"code","afdf5f5c":"code","1f30d9b9":"code","79f9ef0a":"code","3b547232":"code","79fb81c7":"code","9b806727":"code","b9e1520f":"code","f12b8094":"code","022a0f3f":"code","20487c15":"code","df5efe3e":"code","c8b94098":"code","c591f883":"code","22c4dd7b":"code","b8c29901":"code","0a4d6b2c":"code","cf619fb5":"markdown","79ff4040":"markdown","161dd8eb":"markdown","067bcb8b":"markdown","2e01e8ae":"markdown","5c31cb9d":"markdown","ea244da4":"markdown","324a7624":"markdown","d0c49ec8":"markdown","6f90046f":"markdown","eb3d247e":"markdown","a2079a37":"markdown"},"source":{"1adf441d":"# 1.0 Call libraries\n%reset -f\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n#%matplotlib qt5","fcc2e18c":"# 1.0.1 For measuring time elapsed\nfrom time import time","3cc4268d":"# 1.2 Processing data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import  OneHotEncoder as ohe\nfrom sklearn.preprocessing import StandardScaler as ss\nfrom sklearn.compose import ColumnTransformer as ct","9e7a4871":"# 1.3 Model building\n#    Helpful Hint: Install h2o as: conda install -c h2oai h2o=3.22.1.2\nimport h2o\nfrom h2o.estimators.deeplearning import H2ODeepLearningEstimator","f4ed7fc2":"# 1.4 for ROC graphs & metrics\nimport scikitplot as skplt\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import average_precision_score\nimport sklearn.metrics as metrics","95224fd3":"# 1.5 Misc\nimport gc","b0e46ce5":"# 1.6 Change ipython options to display all data columns\npd.options.display.max_columns = 300","9573cbee":"# 2.0 Read data\n# Note: Test data is already provided in a seperate file\n#os.chdir(\"E:\\\\Data\\\\work\\\\Analytics\\\\FORE\\\\Datasets\\\\fashion_mnist\")\n#mn = pd.read_csv(\"fashion-mnist_train.csv.zip\")\n#mn_test = pd.read_csv(\"fashion-mnist_test.csv.zip\")\n\nmn = pd.read_csv(\"..\/input\/fashion-mnist_train.csv\")\nmn_test = pd.read_csv(\"..\/input\/fashion-mnist_test.csv\")","cb434cc2":"# 2.1 Explore data\nmn.head(3)\nmn.info()                       # # NULLS?? \nmn.isnull().sum()               # Any NULLS - None\n","70be1152":"# 2.1.1 Examine distribution of continuous variables\nmn.describe()                   # Data distributed from 0 - 255\n\nmn.shape                        # (60000,785)\nmn.columns.values               # Target is Ist column: 'label'\nmn.dtypes.value_counts()        # all 785 columns are type int64  4\n","bc5a5bff":"# 2.2 Summary of target feature\nmn['label'].value_counts()     # Uniformly Distributed across classes [0-9]: 6000\n","20e5704e":"# 3.0 quickly view a data row in the form of image\n#    Get the first row excluding first column\n#    First column contains class labels and \n#    other columns contain pixel-intensity values\nabc = mn.values[1, 1:]\nabc.shape    # (784,)\nabc = abc.reshape(28,28)   # Reshape to 28 X 28\n","bf79a21a":"# 4.0 And plot it\nplt.imshow(abc)\nplt.show()\n","4bc0d7e6":"# 4.0 Separation into target\/predictors\n# 4.1 Training Data\ny = mn.iloc[:,0]\nX = mn.iloc[:,1:]\nX.shape              # 60000 X 784\n","85443c53":"# 4.2 Test Data\ny_test = mn_test.iloc[:,0]\nX_test = mn_test.iloc[:,1:]\nX_test.shape              # 10000 X 784\n","1246227f":"# 5.0 Which columns are numerical and which categorical?\nnum_columns = X.select_dtypes(include = ['float64','int64']).columns\nnum_columns\n","0b49f985":"# 6. Start creating transformation objects\n# 6.1 tuple for numeric columns\nnum = (\"numtrans\", ss() , num_columns)\n","89bf310e":"# 6.2 Instantiate column transformer object\ncolTrans = ct([num])\n","7c56b76c":"# 6.3 Fit and transform\nX_trans = colTrans.fit_transform(X)\nX_trans.shape              # 60000 X 784\ntype(X_trans)\n","e7e8e2c5":"# 7.0 Preparing to model data with deeplearning\n#      H2o requires composite data with both predictors\n#      and target\n\nX_train = pd.DataFrame(data=X_trans, columns=num_columns)\ntype(X_train) #pandas.core.frame.DataFrame\n\ny = pd.DataFrame(data=y)\nX = np.hstack((X_train,y))\ntype(X) #numpy.ndarray\n","d3324ee1":"# 8 Delete not needed variables and release memory\ndel(X_trans)\ndel(X_train)\ndel(abc)\ngc.collect()\n","726fbb22":"# 9.1 Start h2o\nh2o.init()\n","56f94a27":"# 9.2 Transform data to h2o dataframe\ndf = h2o.H2OFrame(X)\nlen(df.columns)    # 785\n","372b9efe":"df.shape           # 60000 X 785\n","6cd06c14":"#df.columns\n","0dab86b9":"# 10. Get list of predictor column names and target column names\n#     Column names are given by H2O when we converted array to\n#     H2o dataframe\n# 10.1 X_column names\nX_columns = df.columns[0:784]        # Only column names. No data\nX_columns       # C1 to C784\n","1f8d9fef":"# 10.2 y_column names\ny_columns = df.columns[784]\ny_columns       #C785\n","3447cd15":"df['C785'].head()      # Just to be sure\n","7dcec287":"# 11.1 For classification, target column must be factor\n#      Required by h2o\ndf['C785'] = df['C785'].asfactor()\n","af11fff7":"# 12. Build a deeplearning model on balanced data\n#     http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/deep-learning.html\n\ndl_model = H2ODeepLearningEstimator(epochs=1000,\n                                    distribution = 'multinomial',                 # Response has categories [0-9]\n                                    missing_values_handling = \"MeanImputation\", # Not needed by us\n                                    variable_importances=True,\n                                    nfolds = 2,                           # CV folds\n                                    fold_assignment = \"Stratified\",       # Each fold must be sampled carefully\n                                    keep_cross_validation_predictions = True,  # For analysis\n                                    balance_classes=False,                # SMOTE is not provided by h2o\n                                    standardize = True,                   # z-score standardization\n                                    activation = 'RectifierWithDropout',  # Default dropout is 0.5\n                                    hidden = [100,100],                  # ## more hidden layers -> more complex interactions\n                                    stopping_metric = 'logloss',\n                                    loss = 'CrossEntropy')\n","afdf5f5c":"# 12.1 Train our model\nstart = time()\ndl_model.train(X_columns,\n               y_columns,\n               training_frame = df)\n\n\nend = time()\n(end - start)\/60","1f30d9b9":"# 12.2 Get model summary\nprint(dl_model)\n","79f9ef0a":"# 13. Time to make predictions on actual 'test' data\n#     Create a composite X_test data before transformation to\n#     H2o dataframe.\ny_test = (y_test.values).reshape(len(y_test), 1)     # Needed to hstack\ny_test.shape     # 10000,1\n","3b547232":"# 14.0 Get shape of Test dataset pre-stacking\nX_test.shape     # 10000,784\n","79fb81c7":"# 14.1 Column-wise stack now\nX_test = np.hstack((X_test,y_test))         # cbind data\nX_test.shape     # 10000,785\n","9b806727":"# 14.2 Transform X_test to h2o dataframe\nX_test = h2o.H2OFrame(X_test)\nX_test['C785'] = X_test['C785'].asfactor() #y_test was stacked as 'C785'\n","b9e1520f":"# 15. Make prediction on X_test\nresult = dl_model.predict(X_test[: , 0:784])\nresult.shape       # 10000 X 11\nresult.as_data_frame().head()   # Class-wise predictions\n","f12b8094":"# 16.1 Ground truth\n#      Convert H2O frame back to pandas dataframe\nxe = X_test['C785'].as_data_frame()\nxe['result'] = result[0].as_data_frame()\nxe.columns      #['C785', 'result']\n","022a0f3f":"xe.head()       #See the results of predicted Vs actual labels\n","20487c15":"# 16.2 So compare ground truth with predicted\nout = (xe['result'] == xe['C785'])\nnp.sum(out)\/out.size    # Accuracy ~ 77.15%\n","df5efe3e":"# 17.1 Also create confusion matrix using pandas dataframe\nf  = confusion_matrix( xe['C785'], xe['result'] )\nf.shape     # 10x10 matrix\n","c8b94098":"#17.2 The Matrix of confusion\nf\n","c591f883":"#17.3 How were the classes distributed\nxe['C785'].value_counts()   # Classes are equitably distributed\n","22c4dd7b":"#17.4 How are the predictions distributed\nxe['result'].value_counts() # skewed towards classes 4,8,9, 1\n","b8c29901":"# 18.1 calculate the prediction propbabilities for all thresholds of the classification\npred_probability = result[\"p1\"].as_data_frame()    #  Get probability values and convert to pandas dataframe\npred_probability","0a4d6b2c":"#  18.2 Which columns are of higher importance as per the trained and tested model\nvar_df = pd.DataFrame(dl_model.varimp(),\n             columns=[\"Variable\", \"Relative Importance\", \"Scaled Importance\", \"Percentage\"])\nvar_df.head(10)\n","cf619fb5":"##  Data Modeling","79ff4040":"## Problem Statement:\n### 1. Classify fashion_mnist dataset using H2O deeplearning after reducing dimensionality of its data and after standardizing the integer values. And check how good your classification is.\n### 2. Get feature importance using predicted values from deeplearning model created during the exercise","161dd8eb":"### Summary of Dataset:\n#### 1. Dataset size is (60000,785)\n#### 2. First column is Target value of class and the rest 784 are pixel values of image size: 28 X 28. \n####    That is, each row is one flattened image, say of a type of shoe.","067bcb8b":"##  Data exploration","2e01e8ae":"##  Data Processing\n","5c31cb9d":"### Make Predictions","ea244da4":"# Deep Learning Model Building using H2O\n\n### Warning: if trying on a local machine remember to tone down the epochs and hidden layers\/nodes","324a7624":"## An MNIST-like dataset of 70,000 28x28 labeled fashion images\n### Last amended: 18th March, 2019\n### Kaggle: https:\/\/www.kaggle.com\/zalando-research\/fashionmnist","d0c49ec8":"### How do the prediction results stack up Vs actual","6f90046f":"# Fashion MNIST Data analysis using H2O DeepLearning","eb3d247e":"### Training the Model","a2079a37":"## Results:\n### Accuracy achieved on the test dataset is ~ 77.15%\n### Features:\n### Variable  Relative Importance  Scaled Importance  Percentage\n### 0       C1             1.000000           1.000000    0.011483\n### 1      C29             0.772120           0.772120    0.008867\n### 2      C28             0.662692           0.662692    0.007610\n### 3     C757             0.544359           0.544359    0.006251\n### 4      C57             0.514405           0.514405    0.005907\n### 5      C56             0.440139           0.440139    0.005054\n### 6      C58             0.426320           0.426320    0.004896\n### 7     C141             0.417067           0.417067    0.004789\n### 8      C30             0.409918           0.409918    0.004707\n### 9     C113             0.399486           0.399486    0.004587"}}