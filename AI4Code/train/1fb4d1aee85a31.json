{"cell_type":{"0b901cf5":"code","95876d1c":"code","c32a8d34":"code","40cf3946":"code","d1d5098f":"code","7856d947":"code","0c2a08db":"code","c3c3aa46":"code","d46f8708":"code","b9cceb64":"code","83caa91a":"code","88543d27":"code","9c56b150":"code","7a11d163":"code","c2f26d7f":"code","e3d9a848":"code","8936ee84":"code","4492a98c":"code","343d4187":"code","74c61f27":"code","00476c41":"code","859798f2":"code","3c11912b":"code","f725e5a6":"code","767d3560":"code","25bfab18":"code","9481bd71":"code","d317a29d":"code","931ed2a3":"code","19c3ddfe":"code","a2d681eb":"code","0f34a812":"code","6b6e78a7":"code","18d98a23":"code","66369492":"code","02005c08":"code","85b5b8dc":"code","56d755ed":"code","73242391":"code","8943c0dd":"code","7058dce0":"code","eecab664":"code","8a1c1ccf":"code","8a7300d8":"code","8b4f6a64":"code","4921880b":"code","49835256":"code","dc97f5ec":"code","ecfe101c":"code","134a12bd":"code","7945f35d":"code","f6e5706e":"code","6c30133f":"code","7c2b1a4c":"code","8b042b74":"code","1f4fd34e":"code","3ee989f2":"code","91a1223f":"code","548d40f7":"code","ee37203c":"code","3e3f453e":"code","82d17570":"code","06407252":"code","3a497f16":"code","470e7855":"code","187959b5":"code","0e722815":"code","dd6f53c6":"code","1e7454a1":"code","df8e226a":"code","d5661678":"code","ecac5be6":"code","e238e178":"code","4785505f":"markdown","1d24cc34":"markdown","05ec49f5":"markdown","ec656b35":"markdown","a1389d7c":"markdown","5759ced9":"markdown","b44dce01":"markdown","e11d6101":"markdown","8f9d1e78":"markdown","80ee54fc":"markdown","8ebb4e81":"markdown","ffcff3bb":"markdown","f5bda083":"markdown","34c94598":"markdown"},"source":{"0b901cf5":"# Importing Libraries \nimport numpy as np \nimport pandas as pd \n\n# Importinng Libraries for plot and graphs visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# This is to ensure we use different colors in our graphs and plots during the project execution\ncolors = ['#FF0000','#FF7F00','#FFFF00','#00FF00','#ca7beb','#c8aef8','#9154f8','#cef3f5',\"#0079FF\"]\nsns.palplot(sns.color_palette(colors))\n\n\n# Loading Data for train and test\n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n# Ignore Warning Messages\n\nimport warnings \n\n\n","95876d1c":"# Understand number of rows and colums present in the datasets \n\nprint(\" Shape of train data\",train.shape)\nprint(\" Shape of test data\",test.shape)","c32a8d34":"# we can further examine the data to look at the missing values in the data \n\ntrain.isnull().sum()","40cf3946":"# Let us look at the embarked unique values \n\ntrain['Embarked'].unique()","d1d5098f":"# We can also use visualisation tool seaborn library to look at the data \n\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis', color=colors[4])","7856d947":"# using countplot from seaborn we can look at the survived \nsns.countplot(x='Survived',data=train,color=colors[1])","0c2a08db":"# Further analysis on the above can be done to look at the survived based on the Sex\/Gender of passengers\nsns.countplot(x='Survived',hue='Sex',data=train,color=colors[1])","c3c3aa46":"# This is analysis on looking at the Embarked on the Survived data\nsns.countplot(x='Survived',hue='Embarked',data=train,color=colors[6])","d46f8708":"# Analysis on the Passenger class \nsns.countplot(x='Survived',hue='Pclass',data=train,color=colors[6])","b9cceb64":"# Plot the data to look at the target value distribution on Age in two sets of plots \n# Distribution plot and boxplot \nimport matplotlib.pyplot as plt\n#train['Age'].hist(bins=40,color=colors[1],alpha=0.7)\nplt.figure(figsize=(15,5))\nplt.subplot(121)\nsns.distplot(train.Age.values, bins=40, color=colors[4])\nplt.title('Target Value Distribution \\n',fontsize=15)\nplt.xlabel('Age'); plt.ylabel('Occurances');\n\nplt.subplot(122)\nsns.boxplot(train.Age.values, color=colors[0])\nplt.title('Target Value Distribution \\n',fontsize=15)\nplt.xlabel('Age');\n","83caa91a":"# Plot the data to look at the target value distribution on Fare in two sets of plots \n# Distribution plot and boxplot \nimport matplotlib.pyplot as plt\n#train['Fare'].hist(bins=40,color=colors[1],alpha=0.7)\nplt.figure(figsize=(15,5))\nplt.subplot(121)\nsns.distplot(train.Fare.values, bins=40, color=colors[4])\nplt.title('Target Value Distribution \\n',fontsize=15)\nplt.xlabel('Fare'); plt.ylabel('Occurances');\n\nplt.subplot(122)\nsns.boxplot(train.Fare.values, color=colors[0])\nplt.title('Target Value Distribution \\n',fontsize=15)\nplt.xlabel('Fare');","88543d27":"# Plot the data to look at the target value distribution on Pclass in two sets of plots \n# Distribution plot and boxplot \nplt.figure(figsize=(15,5))\nplt.subplot(121)\nsns.distplot(train.Pclass.values, bins=10, color=colors[4])\nplt.title('Target Value Distribution \\n',fontsize=15)\nplt.xlabel('Pclass'); plt.ylabel('Occurances');\n\nplt.subplot(122)\nsns.boxplot(train.Pclass.values, color=colors[0])\nplt.title('Target Value Distribution \\n',fontsize=15)\nplt.xlabel('Pclass');","9c56b150":"# Plot the data to look at the target value distribution on Parch (Parents\/Siblings on board) in two sets of plots \n# Distribution plot and boxplot \nplt.figure(figsize=(15,5))\nplt.subplot(121)\nsns.distplot(train.Parch.values, bins=10, color=colors[4])\nplt.title('Target Value Distribution \\n',fontsize=15)\nplt.xlabel('Parch'); plt.ylabel('Occurances');\n\nplt.subplot(122)\nsns.boxplot(train.Parch.values, color=colors[0])\nplt.title('Target Value Distribution \\n',fontsize=15)\nplt.xlabel('Parch');","7a11d163":"# BOX plot to look at the average Age \nplt.figure(figsize=(15, 8))\nsns.boxplot(x='Pclass',y='Age',data=train,color=colors[8])","c2f26d7f":"def imp_age(col):\n    Age = col[0]\n    Pclass = col[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 37\n\n        elif Pclass == 2:\n            return 29\n\n        else:\n            return 24\n\n    else:\n        return Age","e3d9a848":"train['Age'] = train[['Age','Pclass']].apply(imp_age,axis=1)","8936ee84":"# Now let us validate if the function is applied to the data available through visualisation tool seaborn library to look at the data \n\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","4492a98c":"# Missing values in 2 rows will be replaced with S \ntrain['Embarked'] = train['Embarked'].fillna('S')","343d4187":"train.isnull().sum()","74c61f27":"train['Embarked'].describe()","00476c41":"# we will drop the third feature Cabin that has got lot of missing values \n\ntrain.drop('Cabin',axis=1,inplace=True)","859798f2":"# Now let us validate through visualisation tool seaborn library to look at the data \n\n# we can clearly see there is no missing data, so we have confirmed that there are no missing\n# data in the figure below. \n\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis', color=colors[4])","3c11912b":"dtype_df = train.dtypes.reset_index()\ndtype_df.columns = [\"Count\", \"Column Type\"]\ndtype_df.groupby(\"Column Type\").aggregate('count').reset_index()","f725e5a6":"train.dtypes[train.dtypes=='object']","767d3560":"Sex_Encode = pd.get_dummies(train.Sex, prefix='Sex', dtype=np.int64)","25bfab18":"Embarked_Encode = pd.get_dummies(train.Embarked,prefix='Embarked', dtype=np.int64)","9481bd71":"train = train.drop('Name',axis=1)","d317a29d":"train = train.drop('Ticket',axis=1)","931ed2a3":"train = train.drop('Sex',axis=1)","19c3ddfe":"train = train.drop('Embarked',axis=1)","a2d681eb":"train.info()","0f34a812":"# use the concat function and merge the features with the master data set \ntrain = pd.concat([train, Embarked_Encode], axis=1)","6b6e78a7":"train = pd.concat([train, Sex_Encode], axis=1)","18d98a23":"train.info()","66369492":"# Start looking at the correlation in the data \ntrain.corr().T","02005c08":"plt.figure(figsize=(12,10))\nsns.heatmap(train.corr(),annot=True)","85b5b8dc":"# Polynomial features \n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived'],axis=1), \n                                                    train['Survived'], test_size=0.20, \n                                                    random_state=10)\nsc=StandardScaler()\npoly= PolynomialFeatures(degree=2)\n\nscaledxtrain = sc.fit_transform(X_train)\nscaledxtest= sc.transform(X_test)\n\npolyxtrain= poly.fit_transform(scaledxtrain)\npolyxtest=poly.transform(scaledxtest)\n\nlr= LogisticRegression()\n#lr.fit(polyxtrain,y_train)\nlr.fit(polyxtrain,y_train)\n\n\n\nprint(\"Training Score R2: \",lr.score(polyxtrain,y_train)*100)\nprint(\"Testing Score R2: \",lr.score(polyxtest,y_test)*100)\n#print(confusion_matrix(y_test,predictions))\n#print(classification_report(y_test,predictions))\n\n","56d755ed":"# with Pipeline and Power transform \n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived'],axis=1), \n                                                    train['Survived'], test_size=0.20, \n                                                    random_state=10)\npipe=Pipeline((\n('pt',PowerTransformer()),\n('poly',PolynomialFeatures(degree=3)),\n('lr',LogisticRegression())\n))\npipe.fit(X_train,y_train)\n\nprint('------------------------------------------------------------------')\nprint(\"Training Score R2: \",pipe.score(X_train,y_train)*100)\nprint(\"Testing Score R2: \",pipe.score(X_test,y_test)*100)\nprint('------------------------------------------------------------------')\n\npredictions = pipe.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\nprint('------------------------------------------------------------------')","73242391":"# with Pipeline and Standard Scalar\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\n\n#x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.20,random_state=10)\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived'],axis=1), \n                                                    train['Survived'], test_size=0.20, \n                                                    random_state=10)\npipe=Pipeline((\n('sc',StandardScaler()),\n('poly',PolynomialFeatures(degree=3)),\n('lr',LogisticRegression())\n))\npipe.fit(X_train,y_train)\n\nprint('------------------------------------------------------------------')\n\nprint(\"Training Score R2: \",pipe.score(X_train,y_train)*100)\nprint(\"Testing Score R2: \",pipe.score(X_test,y_test)*100)\n\nprint('-------------------------------------------------------------------')\npredictions = pipe.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\nprint('------------------------------------------------------------------')","8943c0dd":"# with Pipeline, Power transform & Decision Tree\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived'],axis=1), \n                                                    train['Survived'], test_size=0.20, \n                                                    random_state=13)\npipe=Pipeline((\n('pt',PowerTransformer()),\n('poly',PolynomialFeatures()),\n('dt',DecisionTreeClassifier())\n))\n\npipe.fit(X_train,y_train)\n\nprint('------------------------------------------------------------------')\n\nprint(\"Training Score R2: \",pipe.score(X_train,y_train)*100)\nprint(\"Testing Score R2: \",pipe.score(X_test,y_test)*100)\n\nprint('------------------------------------------------------------------')\npredictions = pipe.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\n\nprint('------------------------------------------------------------------')","7058dce0":"# with Pipeline, Power transform & Random Forest\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.ensemble import RandomForestClassifier\n\n#x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.20,random_state=10)\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived'],axis=1), \n                                                    train['Survived'], test_size=0.20, \n                                                    random_state=10)\npipe=Pipeline((\n('pt',PowerTransformer()),\n#('poly',PolynomialFeatures(degree=1)),\n('rf',RandomForestClassifier(n_estimators=10))\n))\n\npipe.fit(X_train,y_train)\n\nprint('------------------------------------------------------------------')\nprint(\"Training Score R2: \",pipe.score(X_train,y_train)*100)\nprint(\"Testing Score R2: \",pipe.score(X_test,y_test)*100)\nprint('------------------------------------------------------------------')\npredictions = pipe.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\nprint('------------------------------------------------------------------')\n","eecab664":"# with Pipeline, Power transform & XGBoost\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived'],axis=1), \n                                                    train['Survived'], test_size=0.20, \n                                                    random_state=10)\npipe=Pipeline((\n('pt',PowerTransformer()),\n('xg',XGBClassifier(n_estimators=1000))\n))\n\npipe.fit(X_train,y_train)\n\nprint('------------------------------------------------------------------')\nprint(\"Training Score R2: \",pipe.score(X_train,y_train)*100)\nprint(\"Testing Score R2: \",pipe.score(X_test,y_test)*100)\nprint('------------------------------------------------------------------')\npredictions = pipe.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\nprint('------------------------------------------------------------------')\n\n","8a1c1ccf":"# with Pipeline, Power transform & ADABoost\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.ensemble import AdaBoostClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(train.drop(['Survived'],axis=1), \n                                                    train['Survived'], test_size=0.20, \n                                                    random_state=10)\npipe=Pipeline((\n('pt',PowerTransformer()),\n('ad',AdaBoostClassifier(n_estimators=42))\n))\n\npipe.fit(X_train,y_train)\n\nprint('------------------------------------------------------------------')\nprint(\"Training Score R2: \",pipe.score(X_train,y_train)*100)\nprint(\"Testing Score R2: \",pipe.score(X_test,y_test)*100)\nprint('------------------------------------------------------------------')\npredictions = pipe.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))\nprint('------------------------------------------------------------------')","8a7300d8":"predictions = pipe.predict(X_test)\nX_test.head()","8b4f6a64":"predictions","4921880b":"test.shape","49835256":"test.isnull().sum()","dc97f5ec":"sns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis')","ecfe101c":"plt.figure(figsize=(15, 8))\nsns.boxplot(x='Pclass',y='Age',data=test,color=colors[8])","134a12bd":"# Replace the fare with a median value \ntest['Fare'].fillna(test['Fare'].median(), inplace=True)","7945f35d":"# Drop the Cabin feature \ntest = test.drop('Cabin',axis=1)","f6e5706e":"def imp_age_test(col):\n    Age = col[0]\n    Pclass = col[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 42\n\n        elif Pclass == 2:\n            return 29\n\n        else:\n            return 24\n\n    else:\n        return Age","6c30133f":"test['Age'] = test[['Age','Pclass']].apply(imp_age_test,axis=1)","7c2b1a4c":"sns.heatmap(test.isnull(),yticklabels=False,cbar=False,cmap='viridis', color=colors[4])","8b042b74":"dtype_df1 = test.dtypes.reset_index()\ndtype_df1.columns = [\"Count\", \"Column Type\"]\ndtype_df1.groupby(\"Column Type\").aggregate('count').reset_index()","1f4fd34e":"test.dtypes[test.dtypes=='object']","3ee989f2":"Sex_Encode_test = pd.get_dummies(test.Sex, prefix='Sex', dtype=np.int64)\nEmbarked_Encode_test = pd.get_dummies(test.Embarked,prefix='Embarked', dtype=np.int64)","91a1223f":"test = pd.concat([test,Sex_Encode_test],axis=1)","548d40f7":"test = pd.concat([test,Embarked_Encode_test],axis=1)","ee37203c":"test = test.drop('Name',axis=1)","3e3f453e":"test = test.drop('Sex',axis=1)","82d17570":"test = test.drop('Embarked',axis=1)","06407252":"test = test.drop('Ticket',axis=1)","3a497f16":"test.info()","470e7855":"test.head()","187959b5":"train.head()","0e722815":"ad_pred = pipe.predict(test)\nad_pred = [ 1 if y>=0.5 else 0 for y in ad_pred]","dd6f53c6":"test_pred_ad = pd.DataFrame(ad_pred, columns= ['Survived'])","1e7454a1":"new_test_ad = pd.concat([test, test_pred_ad], axis=1, join='inner')","df8e226a":"new_test_ad.head()","d5661678":"ad_pred = new_test_ad[['PassengerId' ,'Survived']]","ecac5be6":"ad_pred.head()","e238e178":"ad_pred.to_csv('pred_ad.csv' , index=False)","4785505f":"### We have completed handling missing data, just to summarize we have done 3 things \n\nAge is imputed with the mean value \nEmbarked had 2 missing values replaced with S \nCabin had more than 77% values missing so we have dropped the feature ","1d24cc34":"## We will prepare test data to predict the results ","05ec49f5":"#### Create a function to replace the missing values in Age ","ec656b35":"# Titanic Data Science Solutions #\n\n\n![image.png](attachment:image.png)\n\n\n\n\n#### There are many amazing notebooks on data science. I have made an first attempt to organise my notebook and explain to explain every step of the process and the reason\/rationale for a particular solution.\n\n#### I would define the datascience project cycle in 9 stages as below.  I have learnt it from my guru. \n\n#### 1. Data Discovery - Understand problem statement\n\n#### 2. Data Ingestion - Acquire data from the various sources (In this case there are 3 sets of data i. training data, ii. testing data, iii. submission data)\n\n#### 3. Data Wrangling - This is a very important step, where you will start looking at data and broadly carry out 2 things \n    i. Data Cleaning:- Handling missing values, Working with Outliers, Data Imputation\n   ii. Data Manipulation:- Rename columns, Data summarizing, Filtering, Sort, Group,  Merge, join, concatenate\n\n#### 4. Data Eploration - ways to look at patterns, characteristics, and points of interest in the data. There are 2 methods to explore the data once it is loaded \n\n    i. Univariate:- this is a technique of comparing and analyzing the dependency of a single predictor and a response variable. This can be done by barplots, histogram, countplots. \n\n    ii. Bivariate:- this is a technique used to find out if there is a relationship between two sets of given values. It usually involves the variables X and Y. This can be done by scatter plots, correlation plots, regression plots\n\n#### 5. Model Selection and Model Building - \n\nModel Selection and building is the task of selecting a statical model from a set of candidate models, given data. In its most basic forms, model selection is one of the fundamental tasks of scientific inquiry. Determining the principle that explains a series of observations is often linked directly to a mathematical model predicting those observations. \n\nThere are two main objectives in inference and learning from data. One is for scientific discovery, understanding of the underlying data-generating mechanism, and interpretation of the nature of the data. Another objective of learning from data is for predicting future or unseen observations. In the second objective, the data scientist does not necessarily concern an accurate probabilistic description of the data. Of course, one may also be interested in both directions.\n\nThis is the step after we have prepared the data. Model Building will commence. Depending on the data type of the target variable (Quantitative & Qualitative) we will be building either classification or regression models. \n\n*Source WikiPedia*\n\n#### 6. Model Evaluation - \n\nVarious methods to evaluate a ML Model. Some of the frequently used methods are as follows. \n\n- Confusion matrix:- it is a performance measurement for machine learning classification problem where output can be two or more classes.\n- Accuracy\n- Precision\n- Recall\n- Specificity\n- F1 score\n- Precision-Recall or PR curve\n- ROC (Receiver Operating Characteristics) curve\n- PR vs ROC curve.\n\nAdd Picture \n\n#### 7. Model Comparison - \n\n\u201cWhen you change the way you look at things, the things you look at change.\u201d \u2015Wayne Dyer\n\nThere are an abyss of different mechanisms to compare the ML models. \n\n*Let us look at some of the complexities*\n\nTime - This can vary for training and testing data \n\nSpace - Amount of memory an Algorithm is using to run the model \n\nSample - Amount of training examples needed to train to guarantee a valid generalisation. \n\n*Bias-vairance*\n\n**Variance** is a measure of how spread out a data set is. It is calculated as the average squared deviation of each number from the mean of a data set\n\nBias errors occur due to the that a model is biased towards a specific data, solution or assumption. \n\nAdd picture \n\n\n![confusion.png](\/Users\/santoshvaidya\/Downloads\/confusion.png)\n\n*Source WikiPedia*\n\n\n\n#### 8. Model Boosting - \n\nML models genrally focus on high quality predictions. However boosting algorithms look for to improve the predictions by training a series of weak models, each compensating the weaknesses of its predecessors.\n\nThere are two widely used Alogs \n1. Adaptive Boosting \n    Adaboost is developed to work with classification problems. Its adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. In some problems it can be less susceptible to the overfitting problem than other learning algorithms. The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner.\n    \n2. Gradient Boosting\n    This is a technique for regression and classification problems. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary\n\n*Source WikiPedia*\n\n\n\n#### 9. Explainable ML (XAI)- \n\nExplainable AI (XAI) is  the results of a solution that can be understood by humans. It contrasts with the concept of the \"black box\" in machine learning where even its development and designers cannot explain why an AI arrived at a specific decision. \n\nThe algorithms used in AI can be differentiated into white-box and black-box machine learning (ML) algorithms. White-box models are ML models that provide results that are understandable for experts in the domain. Black-box models, on the other hand, are extremely hard to explain and can hardly be understood even by domain experts.XAI algorithms are considered to follow the three principles transparency, interpretability and explainability.\n\n*Source WikiPedia*","a1389d7c":"## 1. Data Discovery : Understand the problem \n\n### The challenge\n\n#### The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, we ask you to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).\n\n","5759ced9":"### If you look at the abover there is a Feature which is Object type. There are 4 features Name, Sex, Ticket, Embarked ","b44dce01":"### We will look at the categorical features and convert them to numerical as ML Models only understad '0' & '1'","e11d6101":"### I will work on the Sex and Embarked feature and apply  one hot encoding. \n\n### One Hot Encoding is a process in the data processing that is applied to categorical data, to convert it into a binary vector representation for better use in machine learning algorithms","8f9d1e78":"## 2. Data Ingestion : As part of this process we will load the data and do some basic checks on the dataset  \n\n#### The data\n\nThere are three files in the data: \n(1) train.csv, (2) test.csv, and (3) gender_submission.csv.\n\n\n","80ee54fc":"## 3. Data Wrangling\n\n#### Look at the missing value in age and impute the values \n\nWe found in our earlier analysis, there are missing data to Age. We need to replace the data by calculating mean age of all the passengers through a method called imputation. Following is the visulaisation using box plot that provides a view on the average age and i have take Pclass as a filter. You can look at based on Pclass the average age is between 24, 29 & 37. ","8ebb4e81":"#### There are 3 features where the data is missing 1) Age 2) Cabin 3) Embarked\n\n#### 1) Age:- Roughly 20 % of the data is missing in this feature. We can apply imputation to replace the missing values.\n\n#### 2) Cabin:-  78% of data is missing. We can drop this feature. \n\n#### 3) Embarked:-  Only 2 values are missing. Majorrity of the passengers are boarded from Southhampton, so we will replace 2 missing values with S","ffcff3bb":"### Look at the data there are 3 features with missing data \n\n##### Age: - 20% of the data is missing, we will use the same method that was used for train data and fill the values\n\n#### Fare:- there is only 1 missing value, we can take a median value and replace the same \n\n#### Cabin:- has got lot of null values, we will drop this feature ","f5bda083":"### Data is ready, we will move on to the next steps of implementation of the ML model and the algorith, here i would be doing the following:- \n\n- Model Selection - train test split \n- Standart Scalar and Power Transformed for Standardization of master dataset\n- Use Polynomial features \n- Start using Machine Learning Algos [Logistics Regression, Decision Tree, Random Forest]\n- Evaulate the model testing and training accuracy \n","34c94598":"#### Apply this function to the Age column "}}