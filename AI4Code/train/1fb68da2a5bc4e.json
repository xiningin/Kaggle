{"cell_type":{"3ae5a7ee":"code","df49b68b":"code","b332bdd7":"code","ad38b6e7":"code","b08ae2f7":"code","b74120a0":"code","acb3c613":"code","e3e54e13":"code","924b98c9":"code","293371d9":"code","e8a0e160":"code","3119921f":"markdown"},"source":{"3ae5a7ee":"!nvidia-smi","df49b68b":"!nvcc --version","b332bdd7":"!python --version","ad38b6e7":"import os","b08ae2f7":"os.environ['JAX_PYTHON_VERSION'] = 'cp36'\nos.environ['JAX_CUDA_VERSION'] = 'cuda100'\nos.environ['JAX_PLATFORM'] = 'linux_x86_64'\nos.environ['JAX_BASE_URL'] = 'https:\/\/storage.googleapis.com\/jax-releases'","b74120a0":"!pip install --upgrade $JAX_BASE_URL\/$JAX_CUDA_VERSION\/jaxlib-0.1.36-$JAX_PYTHON_VERSION-none-$JAX_PLATFORM.whl","acb3c613":"!pip install --upgrade jax","e3e54e13":"import unittest\n\ngpu_test = unittest.skipIf(len(os.environ.get('CUDA_VERSION', '')) == 0, 'Not running GPU tests')","924b98c9":"import time\nfrom jax import grad, jit\nimport jax.numpy as np\n\nclass TestJAX(unittest.TestCase):\n    def tanh(self, x):  # Define a function\n      y = np.exp(-2.0 * x)\n      return (1.0 - y) \/ (1.0 + y)\n\n    @gpu_test\n    def test_JAX(self):\n        grad_tanh = grad(self.tanh)\n        ag = grad_tanh(1.0)\n        print(f'JAX autograd test: {ag}')\n        assert ag==0.4199743\n        ","293371d9":"unittest.main(argv=[''], verbosity=2, exit=False)","e8a0e160":"def slow_f(x):\n  # Element-wise ops see a large benefit from fusion\n  return x * x + x * 2.0\n\nx = np.ones((5000, 5000))\nfast_f = jit(slow_f)\n%timeit -n10 -r3 fast_f(x)  # ~ 4.5 ms \/ loop on Titan X\n%timeit -n10 -r3 slow_f(x)  # ~ 14.5 ms \/ loop (also on GPU via JAX)","3119921f":"![](https:\/\/raw.githubusercontent.com\/google\/jax\/master\/images\/jax_logo_250px.png)\n\n# JAX and XLA"}}