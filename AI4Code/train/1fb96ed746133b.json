{"cell_type":{"42e4e4fb":"code","f846c888":"code","95f0d9b5":"code","e4bb3a37":"code","7235430c":"code","0c125126":"code","f813ecc5":"code","49f616e7":"code","4467d4d7":"code","f8c45620":"code","4449a727":"code","9abba950":"code","0e6aba5f":"code","50ca6d39":"code","78e28bff":"code","7933a467":"code","ce5d14df":"code","0db50288":"code","c54a6409":"code","9d098441":"code","73dd7adc":"code","fc4f6dda":"code","aadf7c8a":"code","8438aee1":"code","d7f7db08":"code","f0618828":"code","d8ce23df":"code","cb519385":"code","10e4bc52":"code","9789c1bc":"code","f4443da4":"code","5a8f7e7e":"code","abe71a45":"code","78fa10cf":"code","237ac8c5":"code","638efef8":"code","8fd6e33d":"code","f75b2ae6":"code","7779466e":"code","5877f1a6":"code","2bf8e8ca":"code","c8561843":"code","5fbf0e73":"code","a2378b8a":"code","e547d58d":"code","bf6ad3fc":"code","8876e82f":"code","2b083c7f":"code","cd5bc9f1":"code","d35681ad":"code","a52c2b2a":"code","2d4a2d69":"code","f0245d3f":"markdown","0c615811":"markdown","818075e4":"markdown","089ea920":"markdown","771dcfcd":"markdown","ac990698":"markdown","bc982190":"markdown","5857f2c3":"markdown","48ae3652":"markdown","af727628":"markdown","95449a50":"markdown","f9bd8015":"markdown"},"source":{"42e4e4fb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f846c888":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_validate\nimport warnings # supress warnings\nwarnings.filterwarnings('ignore')","95f0d9b5":"df = pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","e4bb3a37":"df.shape","7235430c":"df.columns","0c125126":"df.isnull().any()","f813ecc5":"df.describe()","49f616e7":"sns.boxplot(df['residual sugar'])","4467d4d7":"from scipy import stats\nz = np.abs(stats.zscore(df))\nred_wines = df[(z < 3).all(axis=1)]\nred_wines.shape","f8c45620":"red_wines.describe()","4449a727":"sns.boxplot(red_wines['residual sugar'])","9abba950":"sns.distplot(red_wines['residual sugar'])","0e6aba5f":"sns.countplot(x='quality', data=red_wines)","50ca6d39":"plt.subplots(figsize=(15, 10))\nsns.heatmap(red_wines.corr(), annot = True, cmap = 'coolwarm')","78e28bff":"red_wines['quality'].value_counts()","7933a467":"X = red_wines.drop('quality',axis=1)\ny = red_wines['quality']","ce5d14df":"y.value_counts()","0db50288":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX, y = oversample.fit_resample(X, y)","c54a6409":"from collections import Counter\ncounter = Counter(y)\nfor k,v in counter.items():\n\tper = v \/ len(y) * 100\n\tprint('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n# plot the distribution\nplt.bar(counter.keys(), counter.values())\nplt.show()\nX.shape,y.shape","9d098441":"X = StandardScaler().fit(X).transform(X)","73dd7adc":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=0)\nprint ('Train set:', X_train.shape, y_train.shape)\nprint ('Test set:', X_test.shape, y_test.shape)","fc4f6dda":"# Instantiate the machine learning classifiers\nlog_model = LogisticRegression()\nKNN_model = KNeighborsClassifier()\nSVC_model = LinearSVC(dual=False)\nGauss_model = GaussianProcessClassifier()\nDecision_model = DecisionTreeClassifier()\nRandFor_model = RandomForestClassifier()\nAda_model = AdaBoostClassifier()\nGaussNB_model = GaussianNB()\nQDA_model = QuadraticDiscriminantAnalysis()\nmlp_model = MLPClassifier()","aadf7c8a":"# Define dictionary with performance metrics\nscoring = {'accuracy':make_scorer(accuracy_score), \n           'precision':make_scorer(precision_score,average='macro'),\n           'recall':make_scorer(recall_score,average='macro'), \n           'f1_score':make_scorer(f1_score,average='macro')}","8438aee1":"log = cross_validate(log_model,X,y,cv=5,scoring=scoring)\nknn = cross_validate(KNN_model,X,y,cv=5,scoring=scoring)\nsvc = cross_validate(SVC_model,X,y,cv=5,scoring=scoring)\ngauss = cross_validate(Gauss_model,X,y,cv=5,scoring=scoring)\ndecision = cross_validate(Decision_model,X,y,cv=5,scoring=scoring)\nrandom = cross_validate(RandFor_model,X,y,cv=5,scoring=scoring)\nada = cross_validate(Ada_model,X,y,cv=5,scoring=scoring)\ngaussNB = cross_validate(GaussNB_model,X,y,cv=5,scoring=scoring)\nqda = cross_validate(QDA_model,X,y,cv=5,scoring=scoring)\nmlp = cross_validate(mlp_model,X,y,cv=5,scoring=scoring)","d7f7db08":"models_scores_table_new = pd.DataFrame({'Logistic Regression':[log['test_accuracy'].mean(),\n                                                               log['test_precision'].mean(),\n                                                               log['test_recall'].mean(),\n                                                               log['test_f1_score'].mean()],\n                                                           \n                                      'KNeighbors Classifier':[knn['test_accuracy'].mean(),\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  knn['test_precision'].mean(),\n                                                              knn['test_recall'].mean(),\n                                                              knn['test_f1_score'].mean()],\n                                       \n                                      'SVC Classifer':[svc['test_accuracy'].mean(),\n                                                       svc['test_precision'].mean(),\n                                                       svc['test_recall'].mean(),\n                                                       svc['test_f1_score'].mean()],\n                                       \n                                      'GaussianProcess Classifier':[gauss['test_accuracy'].mean(),\n                                                                   gauss['test_precision'].mean(),\n                                                                   gauss['test_recall'].mean(),\n                                                                   gauss['test_f1_score'].mean()],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t\t\t  \n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \n                                      'DecisionTree Classifier':[decision['test_accuracy'].mean(),\n                                                                   decision['test_precision'].mean(),\n                                                                   decision['test_recall'].mean(),\n                                                                   decision['test_f1_score'].mean()],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \n\t\t\t\t\t\t\t\t\t  'RandomForest Classifier':[random['test_accuracy'].mean(),\n                                                                   random['test_precision'].mean(),\n                                                                   random['test_recall'].mean(),\n                                                                   random['test_f1_score'].mean()],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \n\t\t\t\t\t\t\t\t\t  'AdaBoost Classifier':[ada['test_accuracy'].mean(),\n                                                                   ada['test_precision'].mean(),\n                                                                   ada['test_recall'].mean(),\n                                                                   ada['test_f1_score'].mean()],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \n\t\t\t\t\t\t\t\t\t  'GaussianNB':[gaussNB['test_accuracy'].mean(),\n                                                                   gaussNB['test_precision'].mean(),\n                                                                   gaussNB['test_recall'].mean(),\n                                                                   gaussNB['test_f1_score'].mean()],\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   \n\t\t\t\t\t\t\t\t\t  'Quadratic Discriminant Analysis':[qda['test_accuracy'].mean(),\n                                                                   qda['test_precision'].mean(),\n                                                                   qda['test_recall'].mean(),\n                                                                   qda['test_f1_score'].mean()],\n                                      'MLP Classifer':[mlp['test_accuracy'].mean(),\n                                                                   mlp['test_precision'].mean(),\n                                                                   mlp['test_recall'].mean(),\n                                                                   mlp['test_f1_score'].mean()]},\n\t\t\t\t\t\t\t\t\t  \n                                       index=['Accuracy', 'Precision', 'Recall', 'F1 Score'])","f0618828":"models_scores_table_new","d8ce23df":"models_scores_table_new['Best Score'] = models_scores_table_new.idxmax(axis=1)","cb519385":"models_scores_table_new","10e4bc52":"from sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import multilabel_confusion_matrix","9789c1bc":"RandFor_model.fit(X_train,y_train)\ny_pred = RandFor_model.predict(X_test)\ny_test.shape,y_pred.shape","f4443da4":"confusion_matrix(y_test,y_pred)","5a8f7e7e":"multilabel_confusion_matrix(y_test,y_pred)","abe71a45":"print(classification_report(y_test, y_pred))","78fa10cf":"from pprint import pprint","237ac8c5":"print('Parameters currently in use:\\n')\npprint(RandFor_model.get_params())","638efef8":"from sklearn.model_selection import RandomizedSearchCV","8fd6e33d":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","f75b2ae6":"# Use the random grid to search for best hyperparameters\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = RandFor_model, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)","7779466e":"rf_random","5877f1a6":"# Fit the random search model\nrf_random.fit(X_train,y_train)","2bf8e8ca":"rf_random.best_params_","c8561843":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy","5fbf0e73":"base_model = RandomForestClassifier()\nbase_model.fit(X_train,y_train)\nbase_accuracy = evaluate(base_model,X_test,y_test)","a2378b8a":"best_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test,y_test)","e547d58d":"print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) \/ base_accuracy))","bf6ad3fc":"from sklearn.model_selection import GridSearchCV\n\n\nparam_grid = {\n    'bootstrap': [False],\n    'max_depth': [60, 70, 80, 90, 100, 110],\n    'max_features': ['auto'],\n    'min_samples_leaf': [1,2,3],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [2000]\n}\n","8876e82f":"# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)","2b083c7f":"grid_search","cd5bc9f1":"grid_search.fit(X_train, y_train)","d35681ad":"grid_search.best_params_","a52c2b2a":"best_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, y_test)","2d4a2d69":"print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) \/ base_accuracy))","f0245d3f":"### Evaluate Random Search","0c615811":"### corrleation between variables","818075e4":"**Random Forest Classifier has the highest score, so it is selected for futher processing** ","089ea920":"### balancing imbalanced Datasets","771dcfcd":"**Resampling**\nA widely adopted technique for dealing with highly unbalanced datasets is called resampling. It consists of removing samples from the majority class (under-sampling) and \/ or adding more examples from the minority class (over-sampling).","ac990698":"### Random Hyperparameter Grid","bc982190":"*number of trees in the forest (n_estimators) and the number of features considered for splitting at each leaf node (max_features)*","5857f2c3":"### Check class imbalances","48ae3652":"## EDA","af727628":"### Hyperparameter Tuning","95449a50":"### Remove outliers","f9bd8015":"### Grid Search with Cross Validation"}}