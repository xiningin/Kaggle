{"cell_type":{"82455a5f":"code","3fcd5348":"code","9fcf7402":"code","7f4ab3e1":"code","8712136d":"code","de8485c5":"code","c1b418ba":"code","c509bfe0":"code","7790d653":"code","1534dd96":"code","6c47a5b4":"code","b8cca545":"code","5869432c":"code","3391fb96":"markdown"},"source":{"82455a5f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3fcd5348":"# torch \uad00\ub828 library \ud638\ucd9c\n\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\n\n# GPU \uc0ac\uc6a9\uc744 \uc704\ud55c device \uc120\uc5b8\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","9fcf7402":"# \ub370\uc774\ud130 \ub85c\ub4dc\n\ntrain = pd.read_csv(\"\/kaggle\/input\/2021-ai-quiz1-p3\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/2021-ai-quiz1-p3\/test.csv\")\nsubmit = pd.read_csv(\"\/kaggle\/input\/2021-ai-quiz1-p3\/submit_sample.csv\")\ntrain","7f4ab3e1":"# \ud559\uc2b5 \ub370\uc774\ud130\uc14b \uad6c\ucd95\n\nx = train.drop([\"Unnamed: 0\", \"0.1\"], axis=1)\ny = train[\"0.1\"]\nx_test = test.drop([\"Unnamed: 0\"], axis=1)","8712136d":"# \ud06c\uac8c \ud280\ub294 \uac12\uc740 \uc5c6\uc5b4 \ubcf4\uc774\ub098, \ub370\uc774\ud130 \uc790\uccb4\uc758 \uc548\uc815\ud654\ub97c \uc704\ud574 \uac00\uc7a5 \uae30\ubcf8\uc774 \ub418\ub294 \uc804\ucc98\ub9ac \uc0ac\uc6a9\n\nfrom sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nx = sc.fit_transform(x)\nx_test = sc.transform(x_test)","de8485c5":"# \ud559\uc2b5 \ub370\uc774\ud130\ub97c tensor\ud654 \uc2dc\ud0a8 \ud6c4 GPU \uc5f0\ub3d9\n\nx = torch.FloatTensor(np.array(x)).to(device)\ny = torch.LongTensor(np.array(y)).to(device)\nx_test = torch.FloatTensor(np.array(x_test)).to(device)","c1b418ba":"# \ud559\uc2b5 \ub370\uc774\ud130 \ud615\ud0dc \ucd9c\ub825\n\nprint(x.shape)\nprint(y.shape)\nprint(x_test.shape)","c509bfe0":"y.unique()","7790d653":"# \ub79c\ub364 \uac12 \uace0\uc815\n\ntorch.manual_seed(1)\n\n# \ucd1d 3\uac1c\uc758 \ub808\uc774\uc5b4 \uc0ac\uc6a9\n\nlinear1 = nn.Linear(150, 450, bias = True)\nlinear2 = nn.Linear(450, 300, bias = True)\nlinear3 = nn.Linear(40, 20, bias = True)\nlinear4 = nn.Linear(300, 7, bias = True)\nrelu = nn.ReLU()\nsigmoid = nn.Sigmoid()\n\n# \ubaa8\ub378 \uad6c\uc131\n\nmodel = nn.Sequential(linear1, relu, linear2, sigmoid, linear4).to(device)\n\n# optimizer, loss\n\noptimizer = optim.Adam(model.parameters(), lr = 0.001)\nloss = nn.CrossEntropyLoss()","1534dd96":"# iteration\n\nfor stop in range(1500):\n    optimizer.zero_grad()\n    hypothesis = model(x)\n    cost = loss(hypothesis, y)\n    cost.backward()\n    optimizer.step()\n    \n    if stop % 100 == 0:\n        print(stop, cost.item())","6c47a5b4":"# \uacb0\uacfc \ud655\uc778\n\nhypothesis = model(x_test)\npred = hypothesis.argmax(dim=1).cpu()\nresult = pred.detach().numpy()","b8cca545":"submit[\"Category\"] = result\nsubmit","5869432c":"submit.to_csv(\"q3submit.csv\", index=False)","3391fb96":"### P3 \ucf54\ub4dc \uc124\uba85\n### Preprocessing\n\ub370\uc774\ud130 \uc790\uccb4\uac00 \uc774\ubbf8 PCA\ub97c \ud1b5\ud574 \uac00\uacf5\uc774 \ub41c \uc0c1\ud0dc\ub77c \ucc98\uc74c\uc5d0\ub294 \uc804\ucc98\ub9ac \uc5c6\uc774 \uc9c4\ud589\ud574\ubcf4\uc558\uc2b5\ub2c8\ub2e4.\nStandardScaler\ub97c \uc0ac\uc6a9\ud588\uc744 \ub54c\ub294 \uc131\ub2a5\uc5d0 \ubcc0\ud654\uac00 \uc5c6\uc5c8\uc2b5\ub2c8\ub2e4.\n### Training\n\uce90\uae00 \ud3c9\uac00\uae30\uc900\uc774 Categorization Accuracy \uc600\uace0, \ubd84\ub958\ud560 \ud074\ub798\uc2a4\uac00 \ucd1d 7\uac1c\uc600\uae30 \ub54c\ubb38\uc5d0 Loss Function\uc740 CrossEntropyLoss\ub97c \uc0ac\uc6a9\ud558\uc600\uc2b5\ub2c8\ub2e4.\n\ucc98\uc74c\uc5d0\ub294 \ub808\uc774\uc5b4\ub97c 4\uac1c\ub85c \uad6c\uc131\ud558\uc600\uc73c\ub098, 3\uac1c\ub85c \uc904\uc600\uc744 \ub54c \ud559\uc2b5\uc774 \ub354 \uc798 \ub418\uc5c8\uc2b5\ub2c8\ub2e4.\n\ubc18\ubcf5\uc758 \uacbd\uc6b0, 1000\uc73c\ub85c \ub450\uace0 \ud588\uc744 \ub54c \ucd5c\uace0\uc810\uc774 \ub098\uc654\uae30 \ub54c\ubb38\uc5d0 \uc870\uae08 \ub354 \ub298\ub9b0 1500\uc73c\ub85c \uc2dc\ub3c4\ud574\ub3c4 \uc131\ub2a5\uc740 \uac19\uc558\uc2b5\ub2c8\ub2e4."}}