{"cell_type":{"72e276a5":"code","b3726a7a":"code","d3f965fd":"code","6a9252e3":"code","47916df2":"code","54e1ea87":"markdown"},"source":{"72e276a5":"%%writefile submission.py\n\nimport random\nimport pydash\nimport numpy as np\nfrom collections import Counter\n\n\ndef get_last_time_result(results=[-1]):\n    if isinstance(results, int): results = [ results ]\n    for n in range(len(history['reward'])):\n        if history['reward'][-n] in results: return n\n    else: return 0\n\ndef get_streak(results=[-1]):\n    if isinstance(results, int): results = [ results ]\n    for n in range(len(history['reward'])):\n        if history['reward'][-n] not in results: return n\n    else: \n        return len(history['reward'])\n\ndef get_previous_streak(results=[-1]):\n    if isinstance(results, int): results = [ results ]\n    for n in range(len(history['reward'])):\n        if history['reward'][-n] not in results: \n            for n in range(n+1, len(history['reward'])):\n                if history['reward'][-n] not in results: \n                    return n            \n            else:\n                return len(history['reward'])\n    else:\n        return 0\n\n    \naction            = 0\nincrement_on_lose = 1\nincrement_on_turn = 1\nhistory   = {\n    \"action\":   [],\n    \"opponent\": [],\n    \"reward\":   [],\n    \"streaks\":  [],\n}\n# observation   =  {'step': 1, 'lastOpponentAction': 1}\n# configuration =  {'episodeSteps': 1000, 'agentTimeout': 60, 'actTimeout': 1, 'runTimeout': 120\ndef sequential_strategies(observation, configuration, nstreak=2):\n    global action\n    global increment_on_lose \n    global increment_on_turn \n    global history\n    if observation.step > 0:\n        history['opponent'].append(observation.lastOpponentAction)\n        reward = (\n            0 if history['action'][-1] == (history['opponent'][-1]) else\n            1 if history['action'][-1] == (history['opponent'][-1] + 1) % configuration.signs else\n            -1\n        )\n        history['reward'].append(reward)\n        lose_streak     = get_streak([-1])\n        draw_streak     = get_streak([0, -1])\n        previous_streak = get_previous_streak([-1])\n        # print('reward',reward)\n        # print('lose_streak', lose_streak)\n        # print('previous_streak', previous_streak)\n\n        # Counter static agent\n        if observation.step > 3 and len(set(history['opponent'])) == 1:\n            increment_on_turn = 0\n            action = list(set(history['opponent']))[0] + 1\n    \n        elif (\n            lose_streak >= nstreak \n         or draw_streak >= nstreak*2\n        ):  \n            increment_on_turn = random.choice([0,1,2])             \n            if lose_streak >= nstreak*2:\n                increment_on_lose += 1\n                history['streaks'].append(0)\n            else:\n                history['streaks'].append(previous_streak)\n            \n            action += increment_on_lose\n\n            if len(history['streaks']) and previous_streak >= history['streaks'][-1] - 1:\n                action += 1        \n\n            \n    action += increment_on_turn\n        \n    history['action'].append(action)\n    return int(action) % configuration.signs\n","b3726a7a":"%run submission.py","d3f965fd":"from kaggle_environments import make\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 100}, debug=True)\nenv.run([\"submission.py\", 1])\nenv.render(mode=\"ipython\", width=600, height=600)","6a9252e3":"from kaggle_environments import make\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 100}, debug=True)\nenv.run([\"submission.py\", lambda obs, conf: obs.step % 3 ])\nenv.render(mode=\"ipython\", width=600, height=600)","47916df2":"from kaggle_environments import make\n\nenv = make(\"rps\", configuration={\"episodeSteps\": 100}, debug=False)\nenv.run([\"submission.py\", \"..\/input\/rock-paper-scissors-decision-tree\/submission.py\" ])\nenv.render(mode=\"ipython\", width=600, height=600)","54e1ea87":"# Rock Paper Scissors - Sequential Strategies\n\nThis agent assumes our opponent is trying to learn our strategy, so we wait until they have learnt what we are doing, then switch to a strategy that will dominate it."}}