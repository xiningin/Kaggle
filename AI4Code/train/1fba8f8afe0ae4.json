{"cell_type":{"b13d3de5":"code","896a8bb0":"code","ae56a73e":"code","26bcb980":"code","1c636cfa":"code","93f7b086":"code","bf64f329":"code","7e5fa53c":"code","836ffe37":"code","ca1727bf":"code","32c09ca5":"code","a4823023":"code","0e45fc1e":"code","7c1825bd":"code","36517c0e":"markdown","e6cdc4d3":"markdown","9d2487ca":"markdown","e481f474":"markdown","f0a6654c":"markdown","c063d345":"markdown","398c5516":"markdown","3b9732d6":"markdown","5ceba046":"markdown","752b7b87":"markdown","c556f8ac":"markdown","3a4b652e":"markdown","d977e0a2":"markdown","56f6c904":"markdown","45432aa2":"markdown","df690fb4":"markdown","d0116d15":"markdown","806de1ab":"markdown","3f0cefa2":"markdown","9d31f91a":"markdown","761dbd99":"markdown","4a1a7f73":"markdown","54377d68":"markdown","5a8eeaf7":"markdown","37876d46":"markdown","5545fecd":"markdown"},"source":{"b13d3de5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom numpy import loadtxt\n\n#Import XGBoost Model\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n#Success\nprint ('Run Successful')\n# Any results you write to the current directory are saved as output.","896a8bb0":"#Import the Pina Indians Diabetes Dataset\ndataset = loadtxt(\"..\/input\/Diabetes.csv\" , delimiter = \",\")\nprint (\"Run Successfully\")","ae56a73e":"#Split the Dataset into X and Y\nX = dataset[:, 0:8]\nY = dataset [:,8]\nprint ('Ran Successfully')","26bcb980":"#Split the Dataset into into Train and Test \nseed = 7\ntest_size = 0.33\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=test_size, random_state = seed)\nprint ('Ran Successfully')","1c636cfa":"#Let's fit our model on the training data\nxgb = XGBClassifier()\nxgb.fit(X_train, Y_train)\nprint('Ran Successfully')","93f7b086":"#Predict usng our model now\npredictions1 = xgb.predict(X_test)","bf64f329":"#Evaluate Predictions\naccuracy = accuracy_score(Y_test, predictions1)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","7e5fa53c":"# split data into train and test sets \nseed = 7\ntest_size = 0.33 \nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\nmodel = XGBClassifier()\neval_set = [(X_test, y_test)]\n\n#Set eval_metrics as logloss, early_stopping_round as 5 \nmodel.fit(X_train, y_train, early_stopping_rounds=5, eval_metric=\"logloss\", eval_set=eval_set, verbose=True) \n# make predictions for test data \ny_predictions = model.predict(X_test)  \n# evaluate predictions\naccuracy = accuracy_score(y_test, y_predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","836ffe37":"# fit model on training data \nmodel = XGBClassifier() \neval_set = [(X_test, y_test)] \n#Set eval_metrics as logloss, early_stopping_round as 5 \nmodel.fit(X_train, y_train, early_stopping_rounds=5, eval_metric=\"error\", eval_set=eval_set, verbose=True) \n# make predictions for test data\ny_predictions = model.predict(X_test) \n# evaluate predictions \naccuracy = accuracy_score(y_test, y_predictions) \nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","ca1727bf":"# plot feature importance using built-in function \n# fit model on training data\nmodel = XGBClassifier() \nmodel.fit(X_train, y_train) \n# plot feature importance \nplot_importance(model) \npyplot.show()","32c09ca5":"# Tune learning_rate \nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import KFold, StratifiedKFold","a4823023":"#Split Dataset\nX = dataset[:,0:8] \nY = dataset[:,8] \n# grid search \nmodel = XGBClassifier() \nlearning_rate = [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3] \nparam_grid = dict(learning_rate=learning_rate) \nkfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7) \ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold) \ngrid_result = grid_search.fit(X, Y) ","0e45fc1e":"# summarize results \nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_)) \nmeans = grid_result.cv_results_['mean_test_score'] \nstds = grid_result.cv_results_['std_test_score'] \nparams = grid_result.cv_results_['params'] \nfor mean, stdev, param in zip(means, stds, params): \n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","7c1825bd":"print ('Thank you all for stopping by to learn and as you comment')","36517c0e":"**WE will fit XGBoost default model on our Train dataset**","e6cdc4d3":"**Course Overview** (what to expect)\n\nThis course stems from a 7-day crash course on XGBoost from one of my Mentors. I would mention him once he approves his mention. The course is divided into 7 parts. Each topic was designed to take the average developer about 30 minutes. You might \ufb01nish some much sooner and others you may choose to go deeper and spend more time for more research into them. You can complete each part as quickly or as slowly as you like. A comfortable schedule may be to complete one lesson per day over a one week period. Highly recommended.  \n\nThe 7 Topics you will cover are as follows:\n\n\u0088 **Introduction to Gradient Boosting.**\n\n\u0088 **Introduction to XGBoost.**\n\n\u0088 **Develop Your First XGBoost Model.**\n\n\u0088 **Monitor Performance and Early Stopping.**\n\n\u0088 **Feature Importance with XGBoost.**\n\n\u0088 **How to Con\ufb01gure Gradient Boosting.**\n\n\u0088 **XGBoost Hyperparameter Tuning.**\n\nWe will be using the UCI Machine Learning Pima-Indians-Diabetes Dataset in this short tutorial.\n\nGrab your **Coffee** and let's explore Extreme Gradient Boosting (XGBoost) together.\n","9d2487ca":"Let's provide a summary report of our result here now. as we conclude this tutorial.","e481f474":"TOPIC 05: **FEATURE IMPORTANCE WITH XGBOOST**","f0a6654c":"TOPIC 04: **MONITOR PERFORMANCE AND EARLY STOPPING**\n\nThe XGBoost model can evaluate and report on the performance on a test set for the model during training. It supports this capability by specifying both a test dataset and an evaluation metric on the call to model.fit() when training the model and specifying verbose output (verbose=True). For example, we can report on the binary classi\ufb01cation error rate (error) on a standalone test set (eval set) while training an XGBoost model.\n\nWe can use this evaluation to stop training once no further improvements have been made to the model. We can do this by setting the early stopping rounds parameter when calling *model.fit()* to the number of iterations that no improvement is seen on the validation dataset before training is stopped. The full example using the Pima Indians Onset of Diabetes dataset is provided below.\n","c063d345":"Topic 03: **DEVELOP YOUR FIRST XGBOOST MODEL**","398c5516":"Grid Search for the best model","3b9732d6":"The scikit-learn framework provides the capability to search combinations of parameters. This capability is provided in the GridSearchCV class and can be used to discover the best way to con\ufb01gure the model for top performance on your problem. For example, we can de\ufb01ne a grid of the number of trees (n estimators) and tree sizes (max depth) to evaluate by de\ufb01ning a grid. And then evaluate each combination of parameters using 10-fold cross-validation.\n\nWe can then review the results to determine the best combination and the general trends in varying the combinations of parameters. This is the best practice when applying XGBoost to your own problems. \n\nThe parameters to consider tuning are:\n\n\u0088* The number and size of trees (n estimators and max depth).\n\n\u0088* The learning rate and number of trees (learning rate and n estimators).\n\n\u0088* The row and column subsampling rates (subsample, colsample bytree and colsample bylevel).\n","5ceba046":"We should consider an overview of our dataset. We will need the profile of each feature here to understand our Dataset","752b7b87":"**Just before You Go...**\n\nYou made it. Well done! Take a moment and look back at how far you have come:\n\n\u0088 You learned about the gradient boosting algorithm and the XGBoost library.\n\n\u0088 You developed your \ufb01rst XGBoost model.\n\n\u0088 You learned how to use advanced features like early stopping and feature importance.\n\n\u0088 You learned how to con\ufb01gure gradient boosted models and how to design controlled experiments to tune XGBoost hyperparameters.\n\nDon\u2019t make light of this, you have come a long way in a short amount of time. This is just the beginning of your journey with XGBoost in Python. Keep practicing and developing your skills. I have not stopped learning and sharing what I've learnt. I'm continually inspired by [https:\/\/www.kaggle.com\/bayoadekanmbi] and www.datasciencenigeria.org. Thanks to https:\/\/www.kaggle.com\/afolaborn for his immense contribution too. The learning continues as i proceed in the Data Science journey.\n","c556f8ac":"A subtle introduction of my Mentor.\n**Enjoy reading this** https:\/\/machinelearningmastery.com\/avoid-overfitting-by-early-stopping-with-xgboost-in-python\/\n\nI have some articles you can read up to better undersatnd XGBoost Model. I will update the links in the next version of this Kernel. I'm still learning too. Do not hesistate to share any Article that could help beginners in comment section. I'll appreciate it. Thanks in advance.\n\nLet's move on to the next topic.","3a4b652e":"A bene\ufb01t of using ensembles of decision tree methods like gradient boosting is that they can automatically provide estimates of feature importance from a trained predictive model. A trained XGBoost model automatically calculates feature importance on your predictive modeling problem. These importance scores are available in the feature importances member variable of the trained model.","d977e0a2":"TOPIC 06: **HOW TO CONFIGURE GRADIENT BOOSTING**\n\n**Gradient boosting** is one of the most powerful techniques for applied machine learning and as such is quickly becoming one of the most popular. **But how do you con\ufb01gure gradient boosting on your problem?**\n\nA number of con\ufb01guration heuristics were published in the original gradient boosting papers. They can be summarized as:\n\n\u0088 Learning rate or shrinkage (learning rate in XGBoost) should be set to 0.1 or lower, and smaller values will require the addition of more trees.\n\n\u0088 The depth of trees (tree depth in XGBoost) should be con\ufb01gured in the range of 2-to-8, where not much bene\ufb01t is seen with deeper trees.\n\n\u0088 Row sampling (subsample in XGBoost) should be con\ufb01gured in the range of 30% to 80% of the training dataset, and compared to a value of 100% for no sampling.\n\nThese are a good starting points when con\ufb01guring your model. A good general con\ufb01guration strategy is as follows:\n\n1. Run the default con\ufb01guration and review plots of the learning curves on the training and validation datasets.\n\n2. If the system is overlearning, decrease the learning rate and\/or increase the number of trees.\n\n3. If the system is underlearning, speed the learning up to be more aggressive by increasing the learning rate and\/or decreasing the number of trees.\n\nOwen Zhang, the former #1 ranked Competitor on Kaggle and now CTO at Data Robot proposes an interesting strategy to con\ufb01gure XGBoost5. He suggests to set the number of trees to a target value such as 100 or 1000, then tune the learning rate to \ufb01nd the best model. This is an e\ufb03cient strategy for quickly \ufb01nding a good model. In the next and \ufb01nal lesson, we will look at an example of tuning the XGBoost hyperparameters.\n\nCheck Owen proposition here - http:\/\/www.slideshare.net\/odsc\/owen-zhangopen-sourcetoolsanddscompetitions1\n","56f6c904":"**Prologue**\n\nXGBoost is an implementation of gradient boosting that is being used to win Machine Learning competitions. It is powerful but it can be hard to get started. In this guide you will discover a 7-Part crash course on **XGBoost with Python**. This mini course is designed for Python machine learning Beginners that are already comfortable with scikit-learn and the SciPy ecosystem. \n\nNow, let\u2019s get started.\n","45432aa2":"Topic 01: **INTRODUCTION TO GRADIENT BOOSTING**\n\nGradient boosting is one of the most powerful techniques for building predictive models. The idea of boosting came out of the idea of whether a weak learner can be modi\ufb01ed to become better. The \ufb01rst realization of boosting that saw great success in application was **Adaptive Boosting** or **AdaBoost** for short. The weak learners in **AdaBoost** are **Decision Trees** with a single split, called **Decision Stumps** for their shortness. AdaBoost and related algorithms were recast in a statistical framework and became known as **Gradient Boosting Machines** (GBM). The statistical framework cast boosting as a *Numerical Optimization* problem where the objective is to minimize the loss of the model by adding *Weak Learners* using a gradient descent-like procedure, hence the name. The Gradient Boosting *algorithm* involves three elements:\n\n\u00881. A loss function to be optimized, such as cross entropy for classi\ufb01cation or mean squared error for regression problems.\n\n\u00882. A weak learner to make predictions, such as a greedily constructed decision tree.\n\n\u00883. An additive model, used to add weak learners to minimize the loss function.\n\nNew weak learners are added to the model in an e\ufb00ort to correct the residual errors of all previous trees. The result is a powerful predictive modeling algorithm, perhaps more powerful than random forest. \n\nHang on there! Now, let's take a look at the XGBoost implementation of gradient boosting.\n","df690fb4":"Hello Fellow Kagglers!\n\nThis is **Abel Ofinni** from **Nigeria**. A **Data Science Nigeria AI+ Community Member** This Kernel is provided to teach XGBoost in its simplistic style to Beginners. So, I will try to make it Beginner-friendly as much as I can. It's a excerpt from recent training which i also intend to save here for future reference. **Kindly upvote this Kernel if found helpful.** Thanks.","d0116d15":"**We shall be developing our first XGB model right away**\n\nYou will need XGBoost installed. Visit the XGBoost Documentation for installation guide here https:\/\/xgboost.readthedocs.io\/en\/latest\/ . \n\nIf you already have it installed, let's move on.","806de1ab":"Now we should tune our model's learning rate","3f0cefa2":"TOPIC 07: XGBOOST HYPERPARAMETER TUNING","9d31f91a":"**If You enjoy this Kernel, kindly upvote**it. It's my first kernel before. I look forward to your awesome comments and upvotes. I love you all.","761dbd99":"Our model feature importance is now plotted. It's pretty simple. just know the simple syntax.","4a1a7f73":"The XGBoost library provides a built-in function to plot features ordered by their importance. The function is called plot importance() and can be used. The importance scores can help us decide what input variables to keep or discard. They can also be used as the basis for automatic feature selection techniques. We will now plot the feature importance scores using the Pima Indians Onset of Diabetes dataset.\n","54377d68":"(http:\/\/)Topic 02: **INTRODUCTION TO XGBOOST**\n\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance. XGBoost stands for **eXtreme Gradient Boosting**. It was developed by ***Tianqi Chen*** and is laser-focused on *computational speed* and *model performance*, as such there are few frills. In addition to supporting all key variations of the technique, the real interest is the speed provided by the careful engineering of the implementation, including:\n\n\u0088* Parallelization of tree construction using all of your CPU cores during training.\n\n\u0088* Distributed Computing for training very large models using a cluster of machines.\n\n\u0088* Out-of-Core Computing for very large datasets that don\u2019t \ufb01t into memory.\n\n\u0088* Cache Optimization of data structures and algorithms to make best use of hardware.\n\nTraditionally, gradient boosting implementations are slow because of the sequential nature in which each tree must be constructed and added to the model. The on performance in the development of XGBoost has resulted in one of the best predictive modeling algorithms that can now harness the full capability of your hardware platform, or very large computers you might rent in the cloud. As such, XGBoost has been a cornerstone in competitive machine learning, being the technique used to win and recommended by winners.\n1 http:\/\/goo.gl\/AHkmWx 2 http:\/\/goo.gl\/sGyGtu\n","5a8eeaf7":"You might have quite a few things to say using the above overview of our dataset. But to keep things short. We have 0 missing data. So, Let's proceed.","37876d46":"With an **Accuracy Score of 77.95%**. We need to improve our Model to perform better. We can move conveniently to our next topic now.\n\nTake a sip, your coffee is now cold or still hot. <\/Winks\/>\n\nLet's move to the next topic","5545fecd":"We have trained our default XGB Model. Let's try and fit it on our test dataset to see how well the trained default XGB Model performed"}}