{"cell_type":{"69de6abb":"code","8e278a67":"code","b039fac1":"code","b9247f8a":"code","415520b1":"code","a973c13d":"code","d4c5076d":"code","f1743c0e":"code","15285c00":"code","65c81fc3":"code","3c3ab098":"code","bf9fa655":"code","c9674d5f":"code","1a7e233f":"code","a5ed30b9":"code","4dfe5d99":"code","c1beb525":"code","ae464c84":"code","31d60b01":"code","05b3a87d":"code","c16193b8":"code","51a88384":"code","e05f74bb":"code","5ebeec7f":"code","9c959732":"code","11a4f2b1":"code","4d316d5f":"code","90474ebc":"code","5ac8bd43":"code","7a6be0fc":"code","480b9ee3":"code","32b57764":"code","64f998eb":"code","c34b8dfb":"code","876bcb0c":"code","0c46e1a8":"code","2a5f69a5":"code","61aad887":"code","4658b497":"code","9dbfcaa9":"code","06fea9c2":"code","c3b323ae":"code","2eda8da3":"code","2bb54236":"code","cd89827d":"code","13aec529":"code","3b00c4ae":"code","a912f693":"code","5490a2ad":"code","5201cbf5":"code","cb136873":"code","dc757f2a":"code","453e313c":"code","54e3d766":"code","97f10a62":"code","13a6de9f":"code","01da6cfa":"code","a5027e60":"code","d1a0e132":"code","a585deed":"code","e9fa50e0":"code","af562644":"code","1f4987ba":"code","f527dba5":"markdown","5e96508e":"markdown","7c57e0f7":"markdown","6d98086a":"markdown","9b8ef5b3":"markdown","008e2681":"markdown","2af2c01a":"markdown","29ef47de":"markdown","c8538dee":"markdown","6a7463cb":"markdown","d7d71254":"markdown","e7e26bc2":"markdown","b8ee3dea":"markdown","6dd1f4e9":"markdown","f12367c6":"markdown","c3e1ac03":"markdown","653b4eb4":"markdown","2839a1b6":"markdown","68134052":"markdown","d8a74c26":"markdown","67a7b235":"markdown","f2f17b56":"markdown","b65dda8d":"markdown","90e507cf":"markdown","c5b53767":"markdown","fdbe160c":"markdown","f4b59a55":"markdown","990ebda1":"markdown","72b34a33":"markdown","1e735909":"markdown","201fccf5":"markdown","cb84cac7":"markdown","075973c8":"markdown","c966677c":"markdown","22a9b865":"markdown","04a17af3":"markdown","e43c145b":"markdown","342b86ed":"markdown","629d94c4":"markdown","2265bc7b":"markdown","f1fa0ff8":"markdown","f73e49ab":"markdown","d0c93b87":"markdown","e5b8d962":"markdown","098a2b42":"markdown"},"source":{"69de6abb":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import LabelEncoder\nimport gc\nimport os\n\nimport re\n\nfrom sklearn.model_selection import KFold, RepeatedKFold\n\nimport time\nimport lightgbm as lgb\n\nfrom sklearn.metrics import mean_squared_error,roc_auc_score\nfrom sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder,StandardScaler,MinMaxScaler,RobustScaler\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostRegressor, Pool, CatBoost\nfrom sklearn.linear_model import LinearRegression,BayesianRidge\nfrom boruta import BorutaPy\n#from keras.utils import to_categorical\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nfrom pylab import rcParams\n\nimport datetime\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nplt.style.use('ggplot')","8e278a67":"def rmse(y_true,y_pred):\n    return np.sqrt(np.power(y_true-y_pred,2).sum()\/len(y_true))\n\ndef replace_id(df,dictionary = False):\n    \n    df['card_id2']  = df.card_id.str.replace('C','')\n    df['card_id2']  = df.card_id2.str.replace('_','')\n    df['card_id2']  = df.card_id2.str.replace('I','')\n    df['card_id2']  = df.card_id2.str.replace('D','')\n    df['card_id2']  = df.card_id2.str.replace('a','10')\n    df['card_id2']  = df.card_id2.str.replace('b','11')\n    df['card_id2']  = df.card_id2.str.replace('c','12')\n    df['card_id2']  = df.card_id2.str.replace('d','13')\n    df['card_id2']  = df.card_id2.str.replace('e','14')\n    df['card_id2']  = df.card_id2.str.replace('f','15').astype(np.float64)   \n    \n    if dictionary:\n        d = df[['card_id','card_id2']]\n        df.drop('card_id',axis=1,inplace=True)\n        df = df.rename(columns = {'card_id2':'card_id'})\n        return df,d\n    df.drop('card_id',axis=1,inplace=True)\n    df = df.rename(columns = {'card_id2':'card_id'})\n    return df\n    \n\ndef replace_month(df):\n    df['month_id']  = df.first_active_month.str.replace(\"-\",'').fillna(201802).astype(int)\n    \n    df['first_active_month'] = pd.to_datetime(df['first_active_month'])\n    df['elapsed_time'] = (datetime.date(2018, 8, 1) - df['first_active_month'].dt.date).dt.days\n    return df\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\ndef dist_holiday(df, col_name, date_holiday, date_ref, period=100):\n    df[col_name] = np.maximum(np.minimum((pd.to_datetime(date_holiday) - df[date_ref]).dt.days, period), 0)\n    \ndef submit(df,name,score):\n    arr = []\n    path = '..\/'\n    for a in os.listdir(path):\n        arr.append(a)\n    arr = [re.sub(\"[^0-9_]\",'',a) for a in arr ]\n    arr = [a.split('_') for a in arr if a not in ['']]\n    arr = [int(a[0]) for a in arr if a[0] not in ['']]\n    pre = str(max(arr))\n    str_dat = str(time.localtime().tm_year)+'_'+str(time.localtime().tm_mon)+'_'+str(time.localtime().tm_mday)\n    str_dat\n    df[['card_id','target']].to_csv(path+pre+'_'+str_dat+'_CV_'+str(score)+'.csv')","b039fac1":"\ndef deanon_purchase(purchase):\n    return np.round(purchase\/0.00150265118 + 497.06,8)\n\ndef deanon_target(target):\n    return np.exp2(target)\n\ndef anon_target(target):\n    return np.log2(target)\n","b9247f8a":"%%time\ntrain = pd.read_csv('..\/inpt\/train.csv')\ntest_final = pd.read_csv('..\/input\/test.csv')\nhist = pd.read_csv('..\/input\/historical_transactions.csv',parse_dates=[\"purchase_date\"])\nmerch = pd.read_csv('..\/input\/merchants.csv')\nnew_t = pd.read_csv('..\/input\/new_merchant_transactions.csv',parse_dates=[\"purchase_date\"])","415520b1":"train = reduce_mem_usage(train)\ntest_final = reduce_mem_usage(test_final)\nhist = reduce_mem_usage(hist)\nmerch = reduce_mem_usage(merch)\nnew_t = reduce_mem_usage(new_t)","a973c13d":"plt.hist(train['target']);","d4c5076d":"print('Sample with target less then -33: ',train[train.target < -33.].card_id.nunique(),'rows')","f1743c0e":"card_with_large_negative = train[train.target < -33.].card_id.unique()\nprint('Cards with large negative:',train[train.target < -33.].card_id.nunique())","15285c00":"%%time\ntrain = replace_month(train)\ntest_final = replace_month(test_final)\ngc.collect()","65c81fc3":"merch.category_1 = merch.category_1.replace({'Y':'1','N':'0'}).astype(int)\nmerch.category_2.fillna(1,inplace=True)\nmerch.category_4 = merch.category_4.replace({'Y':'1','N':'0'}).astype(int)\nmerch.most_recent_sales_range = merch.most_recent_sales_range.replace({'A':'1','B':'2','C':'3','D':'4','E':'5'}).astype(int)\nmerch.most_recent_purchases_range = merch.most_recent_purchases_range.replace({'A':'1','B':'2','C':'3','D':'4','E':'5'}).astype(int)\n","3c3ab098":"merch['numerical_1'] = np.round(merch['numerical_1'] \/ 0.009914905 + 5.79639, 0)\nmerch['numerical_2'] = np.round(merch['numerical_2'] \/ 0.009914905 + 5.79639, 0)\nmerch['avg_purchases_lag3' ] = deanon_purchase(merch['avg_purchases_lag3' ])\nmerch['avg_purchases_lag6' ] = deanon_purchase(merch['avg_purchases_lag6' ])\nmerch['avg_purchases_lag12'] = deanon_purchase(merch['avg_purchases_lag12'])","bf9fa655":"merch.describe().T","c9674d5f":"merch.loc[(merch.avg_purchases_lag3  == np.inf),'avg_purchases_lag3'] = merch[(merch.avg_purchases_lag3  != np.inf)]['avg_purchases_lag3'].mean()\nmerch.loc[(merch.avg_purchases_lag6  == np.inf),'avg_purchases_lag6'] = merch[(merch.avg_purchases_lag6  != np.inf)]['avg_purchases_lag6'].mean()\nmerch.loc[(merch.avg_purchases_lag12 == np.inf),'avg_purchases_lag12'] = merch[(merch.avg_purchases_lag12 != np.inf)]['avg_purchases_lag12'].mean()","1a7e233f":"print(len(merch['merchant_id'].unique()))\nprint(merch['merchant_id'].count())","a5ed30b9":"merch = merch.groupby('merchant_id').mean().reset_index()\nmerch.columns = ['merch_'+a for a in merch.columns]\nmerch = merch.rename(columns={'merch_merchant_id':'merchant_id'})","4dfe5d99":"merch = merch[['merchant_id','merch_avg_sales_lag3',\n       'merch_avg_purchases_lag3','merch_numerical_1','merch_numerical_2']]","c1beb525":"hist  = pd.merge(hist ,merch,on='merchant_id',how='left')\nnew_t = pd.merge(new_t,merch,on='merchant_id',how='left')","ae464c84":"del merch;\ngc.collect()","31d60b01":"auth = hist[hist['authorized_flag']== 'Y']\nnon_auth = hist[hist['authorized_flag']== 'N']\ncard_ln = hist[hist.card_id.isin(card_with_large_negative)]\n","05b3a87d":"%%time\ndef format_transaction(added_1,pre=''):\n    print('Starting formating for',pre)\n    added_1['merchant_id'].fillna('M_ID_00a6ca8a8a',inplace=True) #one missing merchant\n    orig = added_1.copy()\n    #replace object data\n    print('replace object data',end=' - ')\n    added_1.authorized_flag = added_1.authorized_flag.replace({'Y':'1','N':'0'}).astype(int)\n    added_1.category_1 = added_1.category_1.replace({'Y':'1','N':'0'}).astype(int)\n    added_1.category_3 = added_1.category_3.replace({'A':'1','B':'2','C':'3',np.nan:'1'}).astype(int)\n    print('ready')    \n    #circled days, weeks and month\n    print('Circle dates',end=' - ')\n    added_1['cos_doy'  ] = np.cos(added_1.purchase_date.dt.dayofyear*2*np.pi\/365)\n    added_1['sin_doy'  ] = np.cos(added_1.purchase_date.dt.dayofyear*2*np.pi\/365)    \n    added_1['cos_dow'  ] = np.cos(added_1.purchase_date.dt.dayofweek*2*np.pi\/7)\n    added_1['sin_dow'  ] = np.cos(added_1.purchase_date.dt.dayofweek*2*np.pi\/7)    \n    added_1['cos_month'] = np.cos(added_1.purchase_date.dt.month*2*np.pi\/12)\n    added_1['sin_month'] = np.cos(added_1.purchase_date.dt.month*2*np.pi\/12)    \n    added_1['cos_week' ] = np.cos(added_1.purchase_date.dt.week*2*np.pi\/52)\n    added_1['sin_week' ] = np.cos(added_1.purchase_date.dt.week*2*np.pi\/52)\n    print('ready')\n    #added year, month, and yearmonth\n    print('added year, month, and yearmonth',end=' - ')\n    added_1['year'] = added_1.purchase_date.dt.year.astype(str)\n    added_1['weekofyear'] = added_1.purchase_date.dt.weekofyear\n    added_1['dayofweek'] = added_1.purchase_date.dt.dayofweek\n    added_1['month'] = added_1['purchase_date'].dt.month\n    added_1['weekend'] = (added_1.purchase_date.dt.weekday >=5).astype(int)\n    added_1['not_weekend'] = (added_1.purchase_date.dt.weekday <5).astype(int)\n    added_1['hour'] = added_1.purchase_date.dt.hour  \n    added_1.loc[added_1['month']<10,'month'] = '0'+added_1.loc[added_1['month']<10,'month'].astype(str)\n    added_1['month_id'] = added_1['year'].astype(str)+added_1['month'].astype(str)\n    \n    added_1['month_diff']    = ((datetime.datetime.today() - added_1['purchase_date']).dt.days)\/\/30\n    added_1['month_diff']   += added_1['month_lag']  \n    added_1['day']           = added_1['purchase_date'].dt.day\n    added_1['dayofyear']     = added_1['purchase_date'].dt.dayofyear\n    added_1['quarter']       = added_1['purchase_date'].dt.quarter\n    \n    added_1['price']              = added_1['purchase_amount'] \/ added_1['installments']\n    added_1['duration']           = added_1['purchase_amount'] * added_1['month_diff']\n    added_1['amount_month_ratio'] = added_1['purchase_amount'] \/ added_1['month_diff']\n    \n    print('ready')\n    #add holidays    \n    holidays = [\n        ('Christmas_Day_2017', '2017-12-25'),  # Christmas: December 25 2017\n        ('Mothers_Day_2017', '2017-06-04'),  # Mothers Day: May 14 2017\n        ('fathers_day_2017', '2017-08-13'),  # fathers day: August 13 2017\n        ('Children_day_2017', '2017-10-12'),  # Childrens day: October 12 2017\n        ('Valentine_Day_2017', '2017-06-12'),  # Valentine's Day : 12th June, 2017\n        ('Black_Friday_2017', '2017-11-24'),  # Black Friday: 24th November 2017\n        ('Mothers_Day_2018', '2018-05-13'),\n    ]\n    \n    for d_name, d_day in holidays:\n        dist_holiday(added_1, d_name, d_day, 'purchase_date')\n    \n    #deanon purchase\n    print('deanon purchase',end=' - ')\n    added_1['purchase_amount'] = deanon_purchase(added_1['purchase_amount'])\n    print('ready')\n    #add category pairs\n    print('add category pairs',end=' - ')\n    added_1['category_12'] = added_1['category_1']*10 + added_1['category_2']\n    added_1['category_23'] = added_1['category_2']*10 + added_1['category_3']\n    added_1['category_13'] = added_1['category_1']*10 + added_1['category_3'] \n    \n    added_1['category_all'] = added_1['category_1']*100 + added_1['category_2']*10 + added_1['category_3'] \n    \n    added_1['month_diff'] = ((datetime.datetime.today() - added_1['purchase_date']).dt.days)\/\/30\n    added_1['month_diff'] += added_1['month_lag']\n    \n    added_1['month_diff_days'] = ((datetime.datetime.today() - added_1['purchase_date']).dt.days)\/\/1\n    added_1['month_diff_days'] += added_1['month_lag']*30.5\n    \n    added_1 = pd.get_dummies(added_1, columns=['category_3','category_2'])\n    \n    added_1.loc[:, 'purchase_date'] = pd.DatetimeIndex(added_1['purchase_date']).\\\n                                      astype(np.int64) * 1e-9    \n    \n    \n    print('ready')    \n    \n    aggregate = {'installments':['sum','mean','median','std','var','nunique'],\n                 'merchant_category_id':['nunique'],\n                 'merchant_id':['nunique'],\n                 'city_id':['nunique'],\n                 'state_id':['nunique'],\n                 'subsector_id':['nunique'],\n                 \n                 'purchase_amount': ['sum', 'mean','median', 'max', 'min', 'std','var'],\n                 'authorized_flag':['sum'],\n                 \n                 'month_id':['nunique'],\n                 'purchase_date':[np.ptp,'min','max'],\n                 'month_lag':['max','min','mean','median','sum','std','var'],\n                 'weekofyear':['max','min','mean','median','sum','std','var'],\n                 'dayofweek':['max','min','mean','median','sum','std','var'],\n                 'weekend':['sum','mean','median'],\n                 'day':['sum','mean','median'],\n                 'dayofyear':['sum','mean','median'],\n                 'quarter':['sum','mean','median'],\n                 'not_weekend':['sum','mean','median'],\n                 'hour':['mean','median','sum'],\n                 'month_diff':['mean','median','var','min','max'],     \n                 'cos_doy'  :['mean','median','var','min','max'],    \n                 'sin_doy'  :['mean','median','var','min','max'],    \n                 'cos_dow'  :['mean','median','var','min','max'],    \n                 'sin_dow'  :['mean','median','var','min','max'],    \n                 'cos_month':['mean','median','var','min','max'],    \n                 'sin_month':['mean','median','var','min','max'],    \n                 'cos_week' :['mean','median','var','min','max'],    \n                 'sin_week' :['mean','median','var','min','max'],    \n                 \n                 'category_1':['sum'],                 \n                 'category_2_1.0': ['mean','median'],\n                 'category_2_2.0': ['mean','median'],\n                 'category_2_3.0': ['mean','median'],\n                 'category_2_4.0': ['mean','median'],\n                 'category_2_5.0': ['mean','median'],                 \n                 'category_3_1': ['mean','median'],\n                 'category_3_2': ['mean','median'],\n                 'category_3_3': ['mean','median'],                 \n                 'category_12':['mean','median'],\n                 'category_23':['mean','median'],\n                 'category_13':['mean','median'],                  \n                 'category_all':['mean','median'],\n                 'price': ['sum', 'mean','median', 'std','var'],           \n                 'duration': ['sum', 'mean','median', 'std','var'],    \n                 'amount_month_ratio': ['sum', 'mean','median', 'std','var'],\n                 \n                 'merch_avg_sales_lag3':['sum', 'mean','median', 'std','var'],\n                 'merch_avg_purchases_lag3':['sum', 'mean','median', 'std','var'],\n                 'merch_numerical_1':['sum', 'mean','median', 'std','var'],\n                 'merch_numerical_2':['sum', 'mean','median', 'std','var'],\n                 \n                 'Christmas_Day_2017': ['mean', 'sum'],\n                 'Mothers_Day_2017': ['mean', 'sum'],\n                 'fathers_day_2017': ['mean', 'sum'],\n                 'Children_day_2017': ['mean', 'sum'],\n                 'Valentine_Day_2017': ['mean', 'sum'],\n                 'Black_Friday_2017': ['mean', 'sum'],\n                 'Mothers_Day_2018': ['mean', 'sum']\n    }\n\n    agg_month = {\n                'purchase_amount': ['sum', 'mean','median', 'min', 'max', 'std','var'],\n                'installments': ['sum','mean','median','nunique'],                \n                'merch_avg_sales_lag3':['sum', 'mean','median', 'std'],\n                'merch_avg_purchases_lag3':['sum', 'mean','median', 'std'],\n                'merch_numerical_1':['sum', 'mean','median', 'std','var'],\n                'merch_numerical_2':['sum', 'mean','median', 'std','var'],\n                'weekofyear':['max','min','mean','median','sum','std','var'],\n                'dayofweek':['max','min','mean','median','sum','std','var'],\n                'weekend':['sum','mean','median'],\n                'not_weekend':['sum','mean','median'],\n                'hour':['mean','median','sum'],\n                'day':['mean','median','sum']\n                }\n    \n    agg_month2 = {\n                'purchase_amount': ['sum', 'mean','median', 'min', 'max', 'std','var'],\n                'installments': ['sum','mean','median','nunique'],                \n                'merch_avg_sales_lag3':['sum', 'mean','median', 'std'],\n                'merch_avg_purchases_lag3':['sum', 'mean','median', 'std'],   \n                'merch_numerical_1':['sum', 'mean','median', 'std','var'],\n                'merch_numerical_2':['sum', 'mean','median', 'std','var'],\n                'weekofyear':['max','min','mean','median','sum','std','var'],\n                'dayofweek':['max','min','mean','median','sum','std','var'],\n                'weekend':['sum','mean','median'],\n                'not_weekend':['sum','mean','median'],\n                'hour':['mean','median','sum'],\n                'month_diff':['mean','median'],\n                'day':['mean','median','sum']\n                }\n    \n    print('Start_aggr_month',end=' - ')\n    #----\n    #dont forget do it in cicle\n    agg_month_1 = added_1.groupby(['card_id','month_lag']).agg(agg_month)\n    agg_month_1.columns = ['ad1_'.join(col).strip() for col in agg_month_1.columns.values]\n    agg_month_1.reset_index(inplace=True)\n\n    agg_month_1 = agg_month_1.groupby(['card_id']).agg(['mean','median'])\n    agg_month_1.columns = ['ad1_'.join(col).strip() for col in agg_month_1.columns.values]\n    agg_month_1.reset_index(inplace=True)\n    #----\n    agg_month_2 = added_1.groupby(['card_id','month_diff']).agg(agg_month)\n    agg_month_2.columns = ['ad2_'.join(col).strip() for col in agg_month_2.columns.values]\n    agg_month_2.reset_index(inplace=True)\n\n    agg_month_2 = agg_month_2.groupby(['card_id']).agg(['mean','median'])\n    agg_month_2.columns = ['ad2_'.join(col).strip() for col in agg_month_2.columns.values]\n    agg_month_2.reset_index(inplace=True)\n    #----\n    agg_month_3 = added_1.groupby(['card_id','installments']).agg(agg_month2)\n    agg_month_3.columns = ['ad3_'.join(col).strip() for col in agg_month_3.columns.values]\n    agg_month_3.reset_index(inplace=True)\n\n    agg_month_3 = agg_month_3.groupby(['card_id']).agg(['mean','median'])\n    agg_month_3.columns = ['ad3_'.join(col).strip() for col in agg_month_3.columns.values]\n    agg_month_3.reset_index(inplace=True)\n    \n    #----\n    agg_month_4 = added_1.groupby(['card_id','state_id']).agg(agg_month)\n    agg_month_4.columns = ['ad4_'.join(col).strip() for col in agg_month_4.columns.values]\n    agg_month_4.reset_index(inplace=True)\n\n    agg_month_4 = agg_month_4.groupby(['card_id']).agg(['mean','median'])\n    agg_month_4.columns = ['ad4_'.join(col).strip() for col in agg_month_4.columns.values]\n    agg_month_4.reset_index(inplace=True)\n    \n    #----\n    agg_month_5 = added_1.groupby(['card_id','category_all']).agg(agg_month)\n    agg_month_5.columns = ['ad5_'.join(col).strip() for col in agg_month_5.columns.values]\n    agg_month_5.reset_index(inplace=True)\n\n    agg_month_5 = agg_month_5.groupby(['card_id']).agg(['mean','median'])\n    agg_month_5.columns = ['ad5_'.join(col).strip() for col in agg_month_5.columns.values]\n    agg_month_5.reset_index(inplace=True)\n    print ('ready')\n    \n    print (added_1.columns)\n    print('Start_aggr',end=' - ')\n    added_1 = added_1.groupby(['card_id']).agg(aggregate)\n    print ('ready')\n    added_1.columns = [pre+'_'.join(col).strip() for col in added_1.columns.values]\n    added_1.reset_index(inplace=True)\n    print('Start merge',end=' - ')\n    #added_1 = pd.merge(added_1,dummys_to_card,on='card_id',how='left')\n    print ('ready')\n\n    print('Start merge month',end=' - ')\n    added_1 = pd.merge(added_1,agg_month_1,on='card_id',how='left')\n    added_1 = pd.merge(added_1,agg_month_2,on='card_id',how='left')\n    added_1 = pd.merge(added_1,agg_month_3,on='card_id',how='left')\n    added_1 = pd.merge(added_1,agg_month_4,on='card_id',how='left')\n    added_1 = pd.merge(added_1,agg_month_5,on='card_id',how='left')\n    print ('ready')\n    \n    del agg_month_1,agg_month_2,agg_month_3,agg_month_4,agg_month_5;\n    gc.collect();\n    \n    agr_feat = ['category_1','installments','state_id']\n    \n    #orig['category_all'] = orig['category_1']*100 + orig['category_2']*10 + orig['category_3'] \n    \n    for a in agr_feat:\n        print('Start gen features from '+a,end=' - ')\n        agr = orig.groupby(['card_id',a])['purchase_amount'].mean()\n        agr = pd.DataFrame(agr).reset_index().groupby('card_id')['purchase_amount'].agg(['mean','median'])\n        agr.columns = [a+'_purchase_amount_'+col for col in agr.columns.values]\n        agr.reset_index(inplace=True)\n        added_1 = pd.merge(added_1,agr,on='card_id', how='left')\n        print('ready')\n        \n    agr_feat = ['category_1','state_id']\n    for a in agr_feat:\n        print('Start gen features from '+a,end=' - ')\n        agr = orig.groupby(['card_id',a])['installments'].mean()\n        agr = pd.DataFrame(agr).reset_index().groupby('card_id')['installments'].agg(['mean','median'])\n        agr.columns = [a+'_installments_'+col for col in agr.columns.values]\n        agr.reset_index(inplace=True)\n        added_1 = pd.merge(added_1,agr,on='card_id', how='left')\n        print('ready')\n    del orig;\n    return added_1","c16193b8":"%%time\nadded_2 = format_transaction(auth.copy(),'auth')\nadded_3 = format_transaction(non_auth.copy(),'non_auth')","51a88384":"%%time\nadded_4 = format_transaction(new_t.copy(),'new_')","e05f74bb":"added_5 = card_ln.copy()\nadded_5.authorized_flag = added_5.authorized_flag.replace({'Y':'1','N':'0'}).astype(int)\nadded_5.category_1 = added_5.category_1.replace({'Y':'1','N':'0'}).astype(int)\nadded_5.category_3 = added_5.category_3.replace({'A':'1','B':'2','C':'3',np.nan:'4'}).astype(int)\n\nadded_5 = added_5.groupby(['card_id']).agg({'authorized_flag':'mean',\n                                            'city_id':'nunique',\n                                            'category_1':'mean',\n                                            'installments':'mean',\n                                            'category_3':'mean',\n                                            'merchant_category_id':'nunique',\n                                            'merchant_id':'nunique',\n                                            'month_lag':'mean',\n                                            'purchase_amount':'mean',\n                                            'category_2':'mean',\n                                            'state_id':'nunique',\n                                            'subsector_id':'nunique'\n    \n}).reset_index()\nadded_5 = added_5.mean()\ncolumns = added_5.reset_index().T.head(1).values[0]\nadded_5 = added_5.reset_index().T\nadded_5.columns = columns\n# added_5 = added_5[added_5.card_id!='card_id']\nadded_5.columns = ['MEAN_LN_'+col for col in added_5.columns.values]","5ebeec7f":"LN_col = added_5.columns","9c959732":"train = pd.merge(train,added_2,on=['card_id'],how='left')\ntest_final = pd.merge(test_final,added_2,on=['card_id'],how='left')\n\ntrain = pd.merge(train,added_3,on=['card_id'],how='left')\ntest_final = pd.merge(test_final,added_3,on=['card_id'],how='left')\n\ntrain = pd.merge(train,added_4,on=['card_id'],how='left')\ntest_final = pd.merge(test_final,added_4,on=['card_id'],how='left')\n\n\ntrain.fillna(0,inplace=True)\ntest_final.fillna(0,inplace=True)","11a4f2b1":"train['ELO_auth_sum'] = 1\/(1+pow(10,(train.authpurchase_amount_sum-train.non_authpurchase_amount_sum)\/400))\ntrain['ELO_purchase_sum'] = 1\/(1+pow(10,(train.authpurchase_amount_sum+train.non_authpurchase_amount_sum-abs(train.non_authpurchase_amount_sum))\/400))\n\ntrain['ELO_auth_mean'] = 1\/(1+pow(10,(train.authpurchase_amount_mean-train.non_authpurchase_amount_mean)\/400))\ntrain['ELO_purchase_mean'] = 1\/(1+pow(10,(train.authpurchase_amount_mean+train.non_authpurchase_amount_mean-abs(train.non_authpurchase_amount_mean))\/400))","4d316d5f":"test_final['ELO_auth_sum'] = 1\/(1+pow(10,(test_final.authpurchase_amount_sum-test_final.non_authpurchase_amount_sum)\/400))\ntest_final['ELO_purchase_sum'] = 1\/(1+pow(10,(test_final.authpurchase_amount_sum+test_final.non_authpurchase_amount_sum-abs(test_final.non_authpurchase_amount_sum))\/400))\n\ntest_final['ELO_auth_mean'] = 1\/(1+pow(10,(test_final.authpurchase_amount_mean-test_final.non_authpurchase_amount_mean)\/400))\ntest_final['ELO_purchase_mean'] = 1\/(1+pow(10,(test_final.authpurchase_amount_mean+test_final.non_authpurchase_amount_mean-abs(test_final.non_authpurchase_amount_mean))\/400))","90474ebc":"train['ELO_auth_month_diff'] = 1\/(1+pow(10,(train.authmonth_diff_mean-train.non_authmonth_diff_mean)\/400))\ntrain['ELO_auth_new_month_diff'] = 1\/(1+pow(10,(train.authmonth_diff_mean-train.new_month_diff_mean)\/400))\n\ntrain['ELO_auth_month_lag'] = 1\/(1+pow(10,(train.authmonth_lag_mean-train.non_authmonth_lag_mean)\/400))\ntrain['ELO_auth_new_month_lag'] = 1\/(1+pow(10,(train.authmonth_lag_mean-train.new_month_lag_mean)\/400))","5ac8bd43":"test_final['ELO_auth_month_diff'] = 1\/(1+pow(10,(test_final.authmonth_diff_mean-test_final.non_authmonth_diff_mean)\/400))\ntest_final['ELO_auth_new_month_diff'] = 1\/(1+pow(10,(test_final.authmonth_diff_mean-test_final.new_month_diff_mean)\/400))\n\ntest_final['ELO_auth_month_lag'] = 1\/(1+pow(10,(test_final.authmonth_lag_mean-test_final.non_authmonth_lag_mean)\/400))\ntest_final['ELO_auth_new_month_lag'] = 1\/(1+pow(10,(test_final.authmonth_lag_mean-test_final.new_month_lag_mean)\/400))","7a6be0fc":"rating = pd.concat((hist[['card_id','merchant_id','purchase_amount','purchase_date','month_lag','authorized_flag']],new_t[['card_id','merchant_id','purchase_amount','purchase_date','month_lag','authorized_flag']]))\nrating['month_diff'] = ((datetime.datetime.today() - rating['purchase_date']).dt.days)\/\/30\nrating['month_diff'] += rating['month_lag']\nrating.head()","480b9ee3":"rating = rating.sort_values(['card_id','purchase_date'])\nrating['num_of_purchase'] = rating.groupby(['card_id']).cumcount()+1\n\nrating['K_koef'] = 0\nrating.loc[(rating['authorized_flag']=='Y')&(rating['num_of_purchase']<=30),'K_koef'] = 40\nrating.loc[(rating['authorized_flag']=='Y')&(rating['num_of_purchase']>30),'K_koef'] = 20\n\nrating['S_koef'] = 0\nrating.loc[(rating['authorized_flag']=='Y'),'S_koef'] = 1\n\nrating['change_rating'] = rating['K_koef'] * (rating['S_koef'] - rating['purchase_amount'])\nrating['rating_of_card'] = rating.groupby(['card_id'])['change_rating'].cumsum()\n\nrating['change_rating2'] = rating['K_koef'] * (rating['S_koef'] - rating['month_diff'])\nrating['rating_of_card2'] = rating.groupby(['card_id'])['change_rating2'].cumsum()\n\nrating['change_rating3'] = rating['K_koef'] * (rating['S_koef'] - abs(rating['purchase_amount']))\nrating['rating_of_card3'] = rating.groupby(['card_id'])['change_rating3'].cumsum()\n\nrating['change_rating4'] = rating['K_koef'] * (rating['S_koef'] - abs(rating['month_diff']))\nrating['rating_of_card4'] = rating.groupby(['card_id'])['change_rating4'].cumsum()\n","32b57764":"rating = rating.sort_values(['merchant_id','purchase_date'])\nrating['num_of_purchase'] = rating.groupby(['merchant_id']).cumcount()+1\n\nrating['K_koef'] = 0\nrating.loc[(rating['authorized_flag']=='Y')&(rating['num_of_purchase']<=30),'K_koef'] = 40\nrating.loc[(rating['authorized_flag']=='Y')&(rating['num_of_purchase']>30),'K_koef'] = 20\n\nrating['S_koef'] = 0\nrating.loc[(rating['authorized_flag']=='Y'),'S_koef'] = 1\n\nrating['change_rating_merch'] = rating['K_koef'] * (rating['S_koef'] - (rating['purchase_amount']*(-1)))\nrating['rating_of_merch'] = rating.groupby(['merchant_id'])['change_rating_merch'].cumsum()\n\nrating['change_rating_merch2'] = rating['K_koef'] * (rating['S_koef'] - rating['month_diff'])\nrating['rating_of_merch2'] = rating.groupby(['merchant_id'])['change_rating_merch2'].cumsum()\n\nrating['change_rating_merch3'] = rating['K_koef'] * (rating['S_koef'] - abs(rating['purchase_amount']))\nrating['rating_of_merch3'] = rating.groupby(['merchant_id'])['change_rating_merch3'].cumsum()\n\nrating['change_rating_merch4'] = rating['K_koef'] * (rating['S_koef'] - abs(rating['month_diff']))\nrating['rating_of_merch4'] = rating.groupby(['merchant_id'])['change_rating_merch4'].cumsum()","64f998eb":"rating['ELO_1'] = 1\/(1+pow(10,(rating.rating_of_card -rating.rating_of_merch )\/400))\nrating['ELO_2'] = 1\/(1+pow(10,(rating.rating_of_card2-rating.rating_of_merch2)\/400))\nrating['ELO_3'] = 1\/(1+pow(10,(rating.rating_of_card3-rating.rating_of_merch3)\/400))\nrating['ELO_4'] = 1\/(1+pow(10,(rating.rating_of_card4-rating.rating_of_merch4)\/400))\n\nrating['ELO_11'] = 1\/(1+pow(10,(rating.rating_of_card -rating.rating_of_merch )\/400))\nrating['ELO_21'] = 1\/(1+pow(10,(rating.rating_of_card2-rating.rating_of_merch2)\/400))\nrating['ELO_31'] = 1\/(1+pow(10,(rating.rating_of_card3-rating.rating_of_merch3)\/400))\nrating['ELO_41'] = 1\/(1+pow(10,(rating.rating_of_card4-rating.rating_of_merch4)\/400))\n\nrating['ELO_12'] = 1\/(1+pow(10,(rating.rating_of_card -rating.rating_of_merch )\/400))\nrating['ELO_22'] = 1\/(1+pow(10,(rating.rating_of_card2-rating.rating_of_merch2)\/400))\nrating['ELO_32'] = 1\/(1+pow(10,(rating.rating_of_card3-rating.rating_of_merch3)\/400))\nrating['ELO_42'] = 1\/(1+pow(10,(rating.rating_of_card4-rating.rating_of_merch4)\/400))","c34b8dfb":"#rating = replace_id(rating)\nrating_agg = {'S_koef':'mean',\n                 'change_rating'   :'mean',\n                 'rating_of_card' :'max',\n                 'rating_of_card2':'max',\n                 'rating_of_card3':'max',\n                 'rating_of_card4':'max',\n                 'rating_of_merch':'max',\n                 'rating_of_merch2':'max',\n                 'rating_of_merch3':'max',\n                 'rating_of_merch4':'max',\n                 'ELO_1':'mean',\n                 'ELO_2':'mean',\n                 'ELO_3':'mean',\n                 'ELO_4':'mean',\n                 'ELO_11':'max',\n                 'ELO_21':'max',\n                 'ELO_31':'max',\n                 'ELO_41':'max',\n                 'ELO_12':'sum',\n                 'ELO_22':'sum',\n                 'ELO_32':'sum',\n                 'ELO_42':'sum'\n             }\nrating_card = rating.groupby('card_id').agg(rating_agg).reset_index()\nrating_card.head()","876bcb0c":"train = pd.merge(train,rating_card,on='card_id',how='left')\ntest_final = pd.merge(test_final,rating_card,on='card_id',how='left')\n\ntrain.fillna(0,inplace=True)\ntest_final.fillna(0,inplace=True)","0c46e1a8":"union = pd.concat((hist,new_t))\nunion[union.installments==-1]\nunion = union[union.installments != 0].sort_values(['purchase_date'])\nunion['installments_lag'] = union.month_lag+union.installments\nunion['installments_lag2'] = union.month_lag+union.installments\nunion.loc[union.installments_lag<0,'installments_lag'] = 0\nunion['credit_now'] = union.installments_lag*union.purchase_amount\nunion['credit_all'] = union.installments*union.purchase_amount\nunion2 = union.groupby('card_id').agg({'installments_lag':['min','max','sum','mean','var','median']\n                                ,'installments_lag2':['min','max','sum','mean','var','median']\n                            ,'credit_now':['min','max','sum','mean','var','median']\n                            ,'credit_all':['min','max','sum','mean','var','median']})\nunion2.columns = ['install_'.join(col).strip() for col in union2.columns.values]\nunion2.reset_index(inplace=True)\nunion2.fillna(0,inplace=True)\n\ntrain = pd.merge(train,union2,on='card_id',how='left')\ntest_final = pd.merge(test_final,union2,on='card_id',how='left')\ntrain.fillna(0,inplace=True)\ntest_final.fillna(0,inplace=True)","2a5f69a5":"%%time\ntrain.to_csv(r'F:\\Work\\RepoSVNwn\\Data\\train.csv')\ntest_final.to_csv(r'F:\\Work\\RepoSVNwn\\Data\\test_final.csv')","61aad887":"%%time\ntrain = pd.read_csv(r'F:\\Work\\RepoSVNwn\\Data\\train.csv')\ntest_final = pd.read_csv(r'F:\\Work\\RepoSVNwn\\Data\\test_final.csv')","4658b497":"%%time\nprint('Search correlations - ',end='')\nfeatures = [a for a in train.columns if a not in ['target','first_active_month','card_id']] \ncorr_matrix = train[features].corr()\nprint('ready')\ni = 1\nar = []\nfor a in corr_matrix.index:\n    ar.append(i)\n    i+=1\ncorr_matrix['rank'] = ar","9dbfcaa9":"fe_no_corr = []\ncorr_coef = 0.90\nfor a in corr_matrix.columns:\n    if a == 'rank':\n        continue\n    r1 = corr_matrix[(corr_matrix.index ==a)]['rank'].values[0]\n    try:\n        r2 = corr_matrix[(abs(corr_matrix[a]) >= corr_coef)&(corr_matrix.index !=a)]['rank'].values[0]\n    except:\n        r2 = 100000\n    #print(a,'with rank',corr_matrix[(corr_matrix.index ==a)]['rank'].values[0])\n    #print('Correlation with',corr_matrix[(corr_matrix[a] >= corr_coef)&(corr_matrix.index !=a)].index.values,\n    #     'with ranks:',corr_matrix[(corr_matrix[a] >= corr_coef)&(corr_matrix.index !=a)]['rank'].values)\n    if r1<r2:\n        fe_no_corr.append(a)\nprint('='*10)\nprint('Features is',len(fe_no_corr),':',fe_no_corr)\n        ","06fea9c2":"%%time\nf_save = []\nfor a in fe_no_corr:\n    f_save.append(a)\nf_save.append('card_id')\nf_save.append('target')\nf_save.append('first_active_month')\ntrain[f_save].to_csv(r'F:\\Work\\RepoSVNwn\\Data\\train_clear.csv')\ntest_final[[a for a in f_save if a !='target']].to_csv(r'F:\\Work\\RepoSVNwn\\Data\\test_final_clear.csv')","c3b323ae":"train = train[f_save]\ntest_final = test_final[[a for a in f_save if a !='target']]","2eda8da3":"+-----------------+   +------------------+   +------------------+               +----------------+\n|                 |   |                  |   |                  |               |                |\n| new_transaction |   | hist_transaction +-->+ auth_transaction |               |   merch_data   |\n|                 |   |                  |   |                  |               |                |\n+--------+--------+   +--------+---------+   +----+-------------+               +--------+-------+\n         |                     |                  |                                      |\n         |                     v                  |                                      |\n         |            +--------+---------+        |                                      |\n         |            |                  |        |                                      |\n         |            |noauth_transaction|        |                                      |\n         |            |                  |        |                                      |\n         |            +--------+---------+        v                                      |\n         |                     |       +----------+------+                               |\n         |                     +------>+                 |                               |\n         |                             |   train_data    +<------------------------------+\n         +---------------------------->+ ~ 2500 features |\n                                       +---------+-------+\n                                                 |\n                                                 |\n                                                 v\n                                          +------+------+\n                                          |             |\n                                          |    lgbm     |\n                                          |             |\n                                          +------+------+\n                                                 |\n                                                 v\n                                +----------------+-----------------+\n                                |                                  |\n                                | FI best [50,100,150,200,300,400] |\n                                |                                  |\n                                +----------------+-----------------+\n                                                 |\n                                                 |\n                                                 |\n                                                 |\n        +---------------+---------------+--------+--------+---------------+---------------+\n        |               |               |                 |               |               |\n        v               v               v                 v               v               v\n +------+------+ +------+------+ +------+------+   +------+------+ +------+------+ +------+------+\n |  K-fold 10  | |  K-fold 10  | |  K-fold 10  |   |  K-fold 10  | |  K-fold 10  | |  K-fold 10  |\n |  goss lgbm  | |  gbdt lgbm  | |  dart lgbm  |   |  lgbm tuned | |  lgbm tuned | |  lgbm tuned |\n |             | |             | |             |   |     goss    | |     dart    | |     gbrt    |\n +------+------+ +------+------+ +----------+--+   +--+----------+ +------+------+ +------+------+\n        |               |                   |         |                   |               |\n        |               |                   |         |                   |               |\n        |               |                   v         v                   |               |\n        |               |               +---+---------+----+              |               |\n        |               +-------------->+                  +<-------------+               |\n        |                               |   ByesianRidge   |                              |\n        +------------------------------>+                  +<-----------------------------+\n                                        +--------+---------+\n                                                 |\n                                                 v\n                                        +--------+---------+\n                                        |     Submit       |\n                                        | param:outlier if |\n                                        |   target <-15    |\n                                        +------------------+\n","2bb54236":"%%time\ntrain = pd.read_csv('..\/input\/train.csv')\ntrain_cards = train['card_id']\ntrain = pd.read_csv('..\/input\/train_clear.csv')\ntrain['card_id'] = train_cards\ntest_final = pd.read_csv('..\/input\/test_final_clear.csv')","cd89827d":"cards = test_final[['card_id']]\ntrain['outlier'] = 0\ntrain.loc[train.target<-22.,'outlier'] = 1\noutlier = train['outlier']","13aec529":"#train.target = deanon_target(train.target)\ntarget = train.target","3b00c4ae":"features = [a for a in train.columns if a not in ['target','first_active_month','card_id']] \npurchase_column = [a for a in features if 'purchase_amount' in a]\nfor a in [a for a in features if 'rating' in a]:\n    purchase_column.append(a)","a912f693":"for a in purchase_column:\n    train['anon_'+a] = anon_target(train[a])\n    test_final['anon_'+a] = anon_target(test_final[a])\nprint('Done')","5490a2ad":"features = [a for a in train.columns if a not in ['card_id','target','first_active_month','outlier','p1','p1_5','p2_5','pred_valid','pred_valid2','pred_valid3']]","5201cbf5":"%%time \ncat_features = ['feature_1', 'feature_2', 'feature_3']\n\nfolds = KFold(n_splits=5, shuffle=True, random_state=15)\noof = np.zeros(len(train))\npredictions = np.zeros(len(test_final))\nstart = time.time()\n\nprint('ready')\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train[features].values, target.values)):\n    param = {'task': 'train',\n        'boosting': 'gbdt',\n        'objective': 'regression',\n        'metric': 'rmse',\n        'learning_rate': 0.01,\n        'subsample': 0.9855232997390695,\n        'max_depth': 7,\n        'top_rate': 0.9064148448434349,\n        'num_leaves': 20,\n        'min_child_weight': 41.9612869171337,\n        'other_rate': 0.0721768246018207,\n        'reg_alpha': 9.677537745007898,\n        'colsample_bytree': 0.5665320670155495,\n        'min_split_gain': 9.820197773625843,\n        'reg_lambda': 8.2532317400459,\n        'min_data_in_leaf': 21,\n        'verbose': -1,\n        'seed':int(2**fold_*2),\n        'bagging_seed':int(2**fold_*2),\n        'drop_seed':int(2**fold_*2),\n        'lambda_l2':5,\n        'lambda_l1':5,\n        \"feature_fraction\": 0.85,\n        \"bagging_freq\": 1,\n        \"bagging_fraction\": 0.9 ,\n        \"bagging_seed\": int(2**fold_*2),\n        \"max_bin\":4,\n        \"n_jobs\":6\n        }\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n                           label=target.iloc[trn_idx],\n                           categorical_feature=cat_features\n                          )\n    val_data = lgb.Dataset(train.iloc[val_idx][features],\n                           label=target.iloc[val_idx],\n                           categorical_feature=cat_features\n                          )\n    print('Data ready')\n    num_round = 10000\n    clf = lgb.train(param,\n                    trn_data,\n                    num_round,\n                    valid_sets = [trn_data, val_data],\n                    verbose_eval=100,\n                    early_stopping_rounds = 200)\n    \n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    \n    predictions += clf.predict(test_final[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(oof, target)**0.5))\n#print(\"CV score: {:<8.5f}\".format(mean_squared_error(anon_target(np.where(oof<0,0.0000001,oof)), anon_target(np.where(target<0,0.0000001,target)))**0.5))","cb136873":"dictionary = cards\ndictionary['target'] = predictions\n#dictionary.loc[dictionary['target']<-12,'target'] = -33.21928095\ndictionary[['card_id','target']].to_csv('\/all_feature_210219.csv',index=False)\ndictionary[['card_id','target']].head()","dc757f2a":"fi2 = pd.DataFrame(clf.feature_importance())\nfi2.columns = ['importance']\nfi2['feature'] = features\n\nplt.figure(figsize=(14,25))\nsns.barplot(x=\"importance\",\n            y=\"feature\",\n            data=fi2.sort_values(by=\"importance\",\n                                           ascending=False)[:100])","453e313c":"%%time\ncat_col = []\nfor a in train.columns:\n    if train[a].nunique() < 5:\n        if a != 'outlier':\n            cat_col.append(a)\ncat_col","54e3d766":"%%time\noof = []\npredictions = []\n\n\nfor a in [50,100,150,200,300,400]:\n    oof.append(np.zeros(len(train)))\n    predictions.append(np.zeros(len(test_final)))\n    \ni=0\nfor a in [50,100,150,200,300,400]:                       \n    features = fi2.sort_values(by=\"importance\",ascending=False)['feature'][:a]\n    cat_features = []\n\n    folds = KFold(n_splits=8, shuffle=True, random_state=a*2)\n    start = time.time()\n    print('ready')\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train[features].values, target.values)):\n        print(\"fold n\u00b0{}\".format(fold_))\n        param = {'task': 'train',\n            'boosting': 'goss',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'learning_rate': 0.005,\n            'subsample': 0.9855232997390695,\n            'max_depth': 7,\n            'top_rate': 0.9064148448434349,\n            'num_leaves': 63,\n            'min_child_weight': 41.9612869171337,\n            'other_rate': 0.0721768246018207,\n            'reg_alpha': 9.677537745007898,\n            'colsample_bytree': 0.5665320670155495,\n            'min_split_gain': 9.820197773625843,\n            'reg_lambda': 8.2532317400459,\n            'min_data_in_leaf': 21,\n            'verbose': -1,\n            'seed':int(2**fold_*a),\n            'bagging_seed':int(2**fold_\/a),\n            'drop_seed':int(2**fold_+a),\n            \"n_jobs\":20,\n            \"lambda_l1\": 0.05,\n            }\n        trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n                               label=target.iloc[trn_idx],\n                               categorical_feature=cat_features\n                              )\n        val_data = lgb.Dataset(train.iloc[val_idx][features],\n                               label=target.iloc[val_idx],\n                               categorical_feature=cat_features\n                              )\n        print('Data ready')\n        num_round = 10000\n        clf = lgb.train(param,\n                        trn_data,\n                        num_round,\n                        valid_sets = [trn_data, val_data],\n                        verbose_eval=100,\n                        #feval = minowski_dist,\n                        early_stopping_rounds = 200)\n\n        oof[i][val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n        predictions[i] += clf.predict(test_final[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\n    print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof[i], target)**0.5))\n    train['p1_'+str(a)] = oof[i]\n    i+=1\nresult = np.zeros(len(train))\nfor a in range(0,len(oof)):\n    result += oof[a]\/len(oof)\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(result, target)**0.5))","97f10a62":"%%time\noof2 = []\npredictions2 = []\n\nfor a in [50,100,150,200,300,400]:\n    oof2.append(np.zeros(len(train)))\n    predictions2.append(np.zeros(len(test_final)))\n    \ni=0\nfor a in [50,100,150,200,300,400]:                       \n    features = fi2.sort_values(by=\"importance\",ascending=False)['feature'][:a]\n    cat_features = []\n\n    folds = KFold(n_splits = 3,shuffle = True, random_state = 12*a)\n    start = time.time()\n\n    param = {'num_leaves': 31,\n              'min_data_in_leaf': 32, \n              'objective':'regression',\n              'max_depth': -1,\n              'learning_rate': 0.004,\n              \"min_child_samples\": 20,\n              \"boosting\": \"dart\",\n              \"feature_fraction\": 0.9,\n              \"bagging_freq\": 1,\n              \"bagging_fraction\": 0.9 ,\n              \"bagging_seed\": 11,\n              \"metric\": 'rmse',\n              \"lambda_l1\": 0.18,\n              \"nthread\": 24,\n              \"verbosity\": -1}\n    print('ready')\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train[features].values, target.values)):\n        print(\"fold n\u00b0{}\".format(fold_))\n        trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n                               label=target.iloc[trn_idx],\n                               categorical_feature=cat_features\n                              )\n        val_data = lgb.Dataset(train.iloc[val_idx][features],\n                               label=target.iloc[val_idx],\n                               categorical_feature=cat_features\n                              )\n        print('Data ready')\n        num_round = 10000\n        clf = lgb.train(param,\n                        trn_data,\n                        num_round,\n                        valid_sets = [trn_data, val_data],\n                        verbose_eval=100,\n                        #feval = minowski_dist,\n                        early_stopping_rounds = 200)\n\n        oof2[i][val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n        predictions2[i] += clf.predict(test_final[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\n    print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof2[i], target)**0.5))\n    train['p2_'+str(a)] = oof2[i]\n    i+=1\n    \nresult2 = np.zeros(len(train))\nfor a in range(0,len(oof2)):\n    result2 += oof2[a]\/len(oof2)\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(result2, target)**0.5))","13a6de9f":"dictionary = cards\ndictionary['target'] = predictions\ndictionary[['card_id','target']].to_csv('\/dart_150219_cv_3.65983.csv',index=False)\ndictionary[['card_id','target']].head()","01da6cfa":"%%time\noof3 = []\npredictions3 = []\n\nfor a in [50,100,150,200,300,400]:\n    oof3.append(np.zeros(len(train)))\n    predictions3.append(np.zeros(len(test_final)))\n    \ni=0\nfor a in [50,100,150,200,300,400]:                       \n    features = fi2.sort_values(by=\"importance\",ascending=False)['feature'][:a]\n    cat_features = []\n\n    folds = KFold(n_splits = 7,shuffle = True, random_state = 12*a)\n    start = time.time()\n\n    print('ready')\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train[features].values, target.values)):\n        print(\"fold n\u00b0{}\".format(fold_))\n        param = {'num_leaves': 10,\n             'min_data_in_leaf': 32, \n             'objective':'regression',\n             'max_depth': -1,\n             'learning_rate': 0.005,\n             \"min_child_samples\": 20,\n             \"boosting\": \"gbdt\",\n             \"feature_fraction\": 0.9,\n             \"bagging_freq\": 1,\n             \"bagging_fraction\": 0.9 ,\n             \"bagging_seed\": int(2**fold_*a),\n             \"metric\": 'rmse',\n             \"lambda_l1\": 0.11,\n             \"nthread\": 10,\n             \"verbosity\": -1}\n        trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n                               label=target.iloc[trn_idx],\n                               categorical_feature=cat_features\n                              )\n        val_data = lgb.Dataset(train.iloc[val_idx][features],\n                               label=target.iloc[val_idx],\n                               categorical_feature=cat_features\n                              )\n        print('Data ready')\n        num_round = 10000\n        clf = lgb.train(param,\n                        trn_data,\n                        num_round,\n                        valid_sets = [trn_data, val_data],\n                        verbose_eval=100,\n                        #feval = minowski_dist,\n                        early_stopping_rounds = 200)\n\n        oof3[i][val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n        predictions3[i] += clf.predict(test_final[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\n    print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof3[i], target)**0.5))\n    train['p3_'+str(a)] = oof3[i]\n    i+=1\nresult3 = np.zeros(len(train))\nfor a in range(0,len(oof3)):\n    result3 += oof3[a]\/len(oof3)\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(result3, target)**0.5))","a5027e60":"dictionary = cards\ndictionary['target'] = predictions\ndictionary[['card_id','target']].to_csv('\/gbdt_150219_cv_3.65867.csv',index=False)\ndictionary[['card_id','target']].head()","d1a0e132":"%%time\noof5 = []\npredictions5 = []\n\nfor a in [50,100,150,200,300,400]:\n    oof5.append(np.zeros(len(train)))\n    predictions5.append(np.zeros(len(test_final)))\n    \ni=0\nfor a in [50,100,150,200,300,400]:                       \n    features = fi2.sort_values(by=\"importance\",ascending=False)['feature'][:a]\n    cat_features = []\n\n    folds = KFold(n_splits=5, shuffle=True, random_state=a*2)\n    start = time.time()\n    print('ready')\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train[features].values, target.values)):\n        print(\"fold n\u00b0{}\".format(fold_))\n        param = {\n        #Core Parameters\n            'task': 'train',\n            'objective': 'regression',\n            'boosting': 'gbrt',\n            'learning_rate': .01,\n            'num_leaves': 114, #2^max_depth*70%\n            'num_threads': 24,\n            'device_type': 'cpu',\n            'seed': int(2**a*fold_),\n        #Learning Control Parameters\n            'max_depth': 3,\n            'min_data_in_leaf': 20,\n            'min_sum_hessian_in_leaf': 1e-3,\n            'bagging_fraction': .75,\n            'bagging_freq': 2,\n            'bagging_seed': int(2**a*fold_),\n            'feature_fraction': .75,\n            'feature_fraction_seed': int(2**a*fold_),\n            'max_delta_step': 0.0,\n            'lambda_l1': 170.,\n            'lambda_l2': 227.,\n            'min_gain_to_split': 0.0,\n            'min_data_per_group': 100,\n            'max_cat_threshold': 32,\n            'cat_l2': 10.0,\n            'cat_smooth': 10.0,\n            'max_cat_to_onehot': 4,\n            'monotone_constraints': None,\n            'feature_contri': None,\n            'forcedsplits_filename': '',\n            'refit_decay_rate': .9,\n            'verbosity': 1,\n            'max_bin': 171,\n            'min_data_in_bin': 3,\n            'bin_construct_sample_cnt': 200000,\n            'histogram_pool_size': -1.0,\n            'data_random_seed': int(2**a*fold_),\n            'snapshot_freq': -1,\n            'iniscore_filename': '',\n            'valid_data_iniscore': '',\n            'pre_partition': False,\n            'enable_bundle': True,\n            'max_conflict': 0.0,\n            'is_enable_sparse': True,\n            'sparse_threshold': .8,\n            'use_missing': True,\n            'zero_as_missing': True,\n            'two_round': False,\n            'save_binary': False,\n            'enable_load_from_binary_file': True,\n            'header': False,\n            'label_column': '',\n            'weight_columns': '',\n            'group_column': '',\n            'ignore_columns': '',\n            'boost_from_average': True,\n            'reg_sqrt': False,\n            'alpha': .9,\n            'fair_c': 1.0,\n            'poisson_max_delta_step': .7,\n            'tweedie_variance_power': 1.5,\n        #Metric Parameters\n            'metric': 'l2_root',\n            'metric_freq': 1,\n            'is_provide_training_metric': False\n        }\n        trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n                               label=target.iloc[trn_idx],\n                               categorical_feature=cat_features\n                              )\n        val_data = lgb.Dataset(train.iloc[val_idx][features],\n                               label=target.iloc[val_idx],\n                               categorical_feature=cat_features\n                              )\n        print('Data ready')\n        num_round = 10000\n        clf = lgb.train(param,\n                        trn_data,\n                        num_round,\n                        valid_sets = [trn_data, val_data],\n                        verbose_eval=100,\n                        #feval = minowski_dist,\n                        early_stopping_rounds = 200)\n\n        oof5[i][val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n        predictions5[i] += clf.predict(test_final[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\n    print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof5[i], target)**0.5))\n    train['p5_'+str(a)] = oof5[i]\n    i+=1\nresult = np.zeros(len(train))\nfor a in range(0,len(oof5)):\n    result += oof5[a]\/len(oof5)\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(result, target)**0.5))","a585deed":"%%time\noof6 = []\npredicions6 = []\n\nfor a in [50,100,150,200,300,400]:\n    oof6.append(np.zeros(len(train)))\n    predicions6.append(np.zeros(len(test_final)))\n    \ni=0\nfor a in [50,100,150,200,300,400]:                       \n    features = fi2.sort_values(by=\"importance\",ascending=False)['feature'][:a]\n    cat_features = []\n\n    folds = KFold(n_splits=5, shuffle=True, random_state=a*2)\n    start = time.time()\n    print('ready')\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train[features].values, target.values)):\n        print(\"fold n\u00b0{}\".format(fold_))\n        param = {\n        #Core Parameters\n            'task': 'train',\n            'objective': 'regression',\n            'boosting': 'goss',\n            'learning_rate': .01,\n            'num_leaves': 171, #2^max_depth*70%\n            'num_threads': 24,\n            'device_type': 'cpu',\n            'seed': int(2**a*fold_),\n        #Learning Control Parameters\n            'max_depth': 3,\n            'min_data_in_leaf': 20,\n            'min_sum_hessian_in_leaf': 1e-3,\n            'feature_fraction': .75,\n            'feature_fraction_seed': int(2**a*fold_),\n            'max_delta_step': 0.0,\n            'lambda_l1': 114.,\n            'lambda_l2': 199.,\n            'min_gain_to_split': 0.0,\n            'top_rate': .4, #goss\n            'oter_rate': .4, #goss\n            'min_data_per_group': 100,\n            'max_cat_threshold': 32,\n            'cat_l2': 10.0,\n            'cat_smooth': 10.0,\n            'max_cat_to_onehot': 4,\n            'monotone_constraints': None,\n            'feature_contri': None,\n            'forcedsplits_filename': '',\n            'refit_decay_rate': .9,\n        #IO Parameters\n            'verbosity': 1,\n            'max_bin': 171,\n            'min_data_in_bin': 3,\n            'bin_construct_sample_cnt': 200000,\n            'histogram_pool_size': -1.0,\n            'data_random_seed': int(2**a*fold_),\n            'snapshot_freq': -1,\n            'iniscore_filename': '',\n            'valid_data_iniscore': '',\n            'pre_partition': False,\n            'enable_bundle': True,\n            'max_conflict': 0.0,\n            'is_enable_sparse': True,\n            'sparse_threshold': .8,\n            'use_missing': True,\n            'zero_as_missing': True,\n            'two_round': False,\n            'save_binary': False,\n            'enable_load_from_binary_file': True,\n            'header': False,\n            'label_column': '',\n            'weight_columns': '',\n            'group_column': '',\n            'ignore_columns': '',\n            'boost_from_average': True,\n            'reg_sqrt': False,\n            'alpha': .9,\n            'fair_c': 1.0,\n            'poisson_max_delta_step': .7,\n            'tweedie_variance_power': 1.5,\n        #Metric Parameters\n            'metric': 'l2_root',\n            'metric_freq': 1,\n            'is_provide_training_metric': False,\n        #Network Parameters\n            'num_machines': 1,\n            'local_listen_port': 12400,\n            'time-out': 120,\n            'machine_list_filename': '',\n            'machines': '',\n        #GPU Parameters\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'gpu_use_dp': True\n        }\n        \n        trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n                               label=target.iloc[trn_idx],\n                               categorical_feature=cat_features\n                              )\n        val_data = lgb.Dataset(train.iloc[val_idx][features],\n                               label=target.iloc[val_idx],\n                               categorical_feature=cat_features\n                              )\n        print('Data ready')\n        num_round = 10000\n        clf = lgb.train(param,\n                        trn_data,\n                        num_round,\n                        valid_sets = [trn_data, val_data],\n                        verbose_eval=100,\n                        #feval = minowski_dist,\n                        early_stopping_rounds = 200)\n\n        oof6[i][val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n        predicions6[i] += clf.predict(test_final[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\n    print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof6[i], target)**0.5))\n    train['p6_'+str(a)] = oof6[i]\n    i+=1\nresult = np.zeros(len(train))\nfor a in range(0,len(oof6)):\n    result += oof6[a]\/len(oof6)\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(result, target)**0.5))","e9fa50e0":"%%time\noof7 = []\npredicions7 = []\n\nfor a in [50,100,150,200,300,400]:\n    oof7.append(np.zeros(len(train)))\n    predicions7.append(np.zeros(len(test_final)))\n    \ni=0\nfor a in [50,100,150,200,300,400]:                       \n    features = fi2.sort_values(by=\"importance\",ascending=False)['feature'][:a]\n    cat_features = []\n\n    folds = KFold(n_splits=5, shuffle=True, random_state=a*2)\n    start = time.time()\n    print('ready')\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train[features].values, target.values)):\n        print(\"fold n\u00b0{}\".format(fold_))\n        param = {\n        #Core Parameters\n            'task': 'train',\n            'objective': 'regression',\n            'boosting': 'dart',\n            'learning_rate': .1,\n            'num_leaves': 114, #2^max_depth*70%\n            'num_threads': 24,\n            'device_type': 'cpu',\n            'seed': int(2**a*fold_),\n        #Learning Control Parameters\n            'max_depth': 3,\n            'min_data_in_leaf': 20,\n            'min_sum_hessian_in_leaf': 1e-3,\n            'bagging_fraction': .75,\n            'bagging_freq': 2,\n            'bagging_seed': int(2**a*fold_),\n            'feature_fraction': .75,\n            'feature_fraction_seed': int(2**a*fold_),\n            'max_delta_step': 0.0,\n            'lambda_l1': 0.,\n            'lambda_l2': 114.,\n            'min_gain_to_split': 0.0,\n            'drop_rate': .55, #dart\n            'max_dop': 64, #dart\n            'skip_drop': .1, #dart\n            'xgboost_dart_mode': False, #dart\n            'uniform_drop': True, #dart\n            'drop_seed': int(2**a*fold_), #dart\n            'min_data_per_group': 100,\n            'max_cat_threshold': 32,\n            'cat_l2': 10.0,\n            'cat_smooth': 10.0,\n            'max_cat_to_onehot': 4,\n            'monotone_constraints': None,\n            'feature_contri': None,\n            'forcedsplits_filename': '',\n            'refit_decay_rate': .9,\n        #IO Parameters\n            'verbosity': 1,\n            'max_bin': 255,\n            'min_data_in_bin': 3,\n            'bin_construct_sample_cnt': 200000,\n            'histogram_pool_size': -1.0,\n            'data_random_seed': int(2**a*fold_),\n            'snapshot_freq': -1,\n            'iniscore_filename': '',\n            'valid_data_iniscore': '',\n            'pre_partition': False,\n            'enable_bundle': True,\n            'max_conflict': 0.0,\n            'is_enable_sparse': True,\n            'sparse_threshold': .8,\n            'use_missing': True,\n            'zero_as_missing': True,\n            'two_round': False,\n            'save_binary': False,\n            'enable_load_from_binary_file': True,\n            'header': False,\n            'label_column': '',\n            'weight_columns': '',\n            'group_column': '',\n            'ignore_columns': '',\n            'boost_from_average': True,\n            'reg_sqrt': False,\n            'alpha': .9,\n            'fair_c': 1.0,\n            'poisson_max_delta_step': .7,\n            'tweedie_variance_power': 1.5,\n        #Metric Parameters\n            'metric': 'l2_root',\n            'metric_freq': 1,\n            'is_provide_training_metric': False,\n        #Network Parameters\n            'num_machines': 1,\n            'local_listen_port': 12400,\n            'time-out': 120,\n            'machine_list_filename': '',\n            'machines': '',\n        #GPU Parameters\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n            'gpu_use_dp': True\n        }\n        trn_data = lgb.Dataset(train.iloc[trn_idx][features],\n                               label=target.iloc[trn_idx],\n                               categorical_feature=cat_features\n                              )\n        val_data = lgb.Dataset(train.iloc[val_idx][features],\n                               label=target.iloc[val_idx],\n                               categorical_feature=cat_features\n                              )\n        print('Data ready')\n        num_round = 10000\n        clf = lgb.train(param,\n                        trn_data,\n                        num_round,\n                        valid_sets = [trn_data, val_data],\n                        verbose_eval=100,\n                        #feval = minowski_dist,\n                        early_stopping_rounds = 200)\n\n        oof7[i][val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n\n        predicions7[i] += clf.predict(test_final[features], num_iteration=clf.best_iteration) \/ folds.n_splits\n\n    print(\"CV score: {:<8.5f}\".format(mean_squared_error(oof7[i], target)**0.5))\n    train['p1_'+str(a)] = oof7[i]\n    i+=1\nresult = np.zeros(len(train))\nfor a in range(0,len(oof7)):\n    result += oof7[a]\/len(oof7)\nprint(\"CV score: {:<8.5f}\".format(mean_squared_error(result, target)**0.5))","af562644":"train_stack = []\ntest_stack = []\nfor a in range(0,len(oof)):\n    train_stack.append(oof[a])\n    train_stack.append(oof2[a])\n    train_stack.append(oof3[a])\n    \n    train_stack.append(oof5[a])\n    train_stack.append(oof6[a])\n    train_stack.append(oof7[a])\n    \n    test_stack.append(predictions[a])\n    test_stack.append(predictions2[a])\n    test_stack.append(predictions3[a])\n    \n    test_stack.append(predictions5[a])\n    test_stack.append(predicions6[a])\n    test_stack.append(predicions7[a])\n#train_stack.append(oof4)\n#test_stack.append(predictions4)\n\ntrain_stack = np.vstack(train_stack).transpose()\ntest_stack = np.vstack(test_stack).transpose()\n\nfolds = KFold(n_splits=20,shuffle=True,random_state=4520)\noof_stack = np.zeros(train_stack.shape[0])\npredictions_stack = np.zeros(test_stack.shape[0])\n\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_stack, target)):\n    print(\"fold n\u00b0{}\".format(fold_))\n    trn_data, trn_y = train_stack[trn_idx], target.iloc[trn_idx].values\n    val_data, val_y = train_stack[val_idx], target.iloc[val_idx].values\n\n    print(\"-\" * 10 + \"Stacking \" + str(fold_) + \"-\" * 10)\n    clf = BayesianRidge()\n    clf.fit(trn_data, trn_y)\n    \n    oof_stack[val_idx] = clf.predict(val_data)\n    print(\"CV score fold: {:<8.5f}\".format(mean_squared_error(oof_stack[val_idx], target[val_idx])**0.5))\n    predictions_stack += clf.predict(test_stack) \/ folds.n_splits\n\nprint(\"CV score summary:\",np.sqrt(mean_squared_error(target.values, oof_stack)))","1f4987ba":"dictionary = cards\ndictionary['target'] = predictions_stack\nborder = -14.5\nprint('Count outliers:',len(dictionary[dictionary.target<border]))\ndictionary[['card_id','target']].to_csv('..\/stuck_models_no_cat_220219_cv_'+str(round(np.sqrt(mean_squared_error(target.values, oof_stack)),5))+'.csv',index=False)\ndictionary[['card_id','target']].head()","f527dba5":"### Sixth model: Tuned LGBM dart ([Dmitry Kravchuk](https:\/\/www.kaggle.com\/dishkakrauch))\nUsing K-fold with 5 splits and enabled shuffle + using diffrent count of features, selected by feature importance  ","5e96508e":"Here we can see, that we have inf values in avg_purchases_lag3, avg_purchases_lag6, avg_purchases_lag12. Lets replace this on mean values.","7c57e0f7":"[](http:\/\/)Saving this predict","6d98086a":"#### Analysis predict","9b8ef5b3":"We split history data on two parts, because we can see in new transactions data only authorized purchases.  \nAlso i see on this kernel: [Simple Exploration Notebook - Elo](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-elo) and think this is a good idea.  \nplus in separate dataset i get card_ids, that have target with -33.21928095.","008e2681":"We have really many features (something about 2100)  \nLets cut features, that have hight correlation (not good method, because some features may correlate, but have diffrent information)  \n<b>IMPORTANT<\/b>  \nWe get TWO train dataframe, one with this block (5.3) and one without this block. After that a just get mean values of two submit","2af2c01a":"Now we have 469 features. And we will be work with them  \nLet's take only 469 features and save our clear dataset","29ef47de":"### Merchant.csv\nIn many kernel we can see this file is useless, but let's try to get some features from it.","c8538dee":"After many test I get only this features, that may be good in feature importance","6a7463cb":"(added)  \nAfter few days of thinking and discussing all the things we decided that variables we really need are rating of card and border of loyality (positive or negative).  \nBecause if we read description of ELO we see, that rating of player (in this competition card) increase after win by fromula:\n$$R'_A=R_A+K*(S_A-E_A)$$\nwhere $K$ - koef for top 10 =10, for middle = 20, for first 30 games = 40, $S_A$ - points(1 for win, 0.5 for draw, 0 for loose)  \nWe have authorized transaction, nonauthorized transaction and positive or negative.  \nWe split it like this:\n- positive authorized transaction like 1\n- all nonauthorized transaction like 0   \nFor $K$ i get only 20 and 40 koef\nAfter that i get feature like rating of card","d7d71254":"Deanon numerical and purchase","e7e26bc2":"### Fifth model: Tuned LGBM goss (Krauchuk_D)\nUsing K-fold with 5 splits and enabled shuffle + using diffrent count of features, selected by feature importance  ","b8ee3dea":"### Historical and new transactions","6dd1f4e9":"## Something intresting about ELO\nIn Discussions we can found some intrestings thinks about ELO. ([This discussion](https:\/\/www.kaggle.com\/c\/elo-merchant-category-recommendation\/discussion\/75034))  \n[ELO](https:\/\/en.wikipedia.org\/wiki\/Elo_rating_system) - is a method for calculating the relative skill levels of players in zero-sum games such as chess.  \nAlso we can find formula, to calculate ELO rating:   \n$$E_A=\\frac{1}{1+10^{(R_B-R_A)\/400}}$$  \nAs we can see from description of this formula, we need 2 veriables, that mean ratings of 2 parametrs.  \n\nWe try calculate this between this samples:\n- authorized purchase and non authorized purchase\n- month_diff and month_lag as most important features  \n\nBut I'm not sure that these variables are what we need.","f12367c6":"### Formating all datasets","c3e1ac03":"## General information\nThis kernel constists many usefull features (in our opinion).   \nAlso this kernel need more then 16gb RAM while processing dataset (Didn't work at Kaggle).  \nPart of the final solution was based on the following kernels:\n- [Elo EDA and models](https:\/\/www.kaggle.com\/artgor\/elo-eda-and-models\/)\n- [Elo world](https:\/\/www.kaggle.com\/fabiendaniel\/elo-world)\n- [Simple Exploration Notebook - Elo](https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-elo)  \n![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/10445\/logos\/thumb76_76.png?t=2018-10-24-17-14-05)","653b4eb4":"## Load data","2839a1b6":"### Deanon features\n[This kernel](https:\/\/www.kaggle.com\/raddar\/towards-de-anonymizing-the-data-some-insights)","68134052":"Lets see, what of features can be category features","d8a74c26":"### Second model: Dart\nFit main model on all data  \nUsing K-fold with 3 splits and enabled shuffle + using diffrent count of features, selected by feature importance  \nGet only 3 k-fold just because dart calculate can spend many time","67a7b235":"*block for load data if notebook was shutdown or reload*","f2f17b56":"## Functions\nHere you can see functions, that we used to change data format and clar\/prepare it.\n- <b>model_lgb<\/b> - simple lgbm model function\n- <b>rmse<\/b> - RMSE metric calculation function\n- <b>replace_id<\/b> - function for card_id replacement to number - it's usefull for the first iteration.\n- <b>replace_month<\/b> - function to make some time related features like month_id and elapsed_time\n- <b>dist_holiday<\/b> - get from [this kernel](https:\/\/www.kaggle.com\/cttsai\/simple-xgboost-cv-bagging)","b65dda8d":"It will be rating of card. But we need second rating. We think it would be merchant rating.  \nAnd ELO we can calculate with usage card rating vs merchant rating.   ","90e507cf":"Saving produced dataset on disk, for clear memory and work only with data","c5b53767":"Fit main model on all data  \nUsing K-fold with 8 splits and enabled shuffle + using diffrent count of features, selected by feature importance","fdbe160c":"## Training models  ","f4b59a55":"Also we can see, that some merchants have more then one record.  \nOnly 63 merchants have this problem, its less then 0.1% of data, and i just get mean for this samples","990ebda1":"![](http:\/\/)![](http:\/\/)Little restruct features.","72b34a33":"### Lets join all feature to train and test","1e735909":"### Zero model: Predict on all\/no correlate features\nFirst of all I train model to take good features, that can be usefull. I take 400 best features.  \nAlso we save prediction, to use it in future mix.","201fccf5":"### Third model: gbdt\nFit main model on all data  \nUsing K-fold with 3 splits and enabled shuffle + using diffrent count of features, selected by feature importance  ","cb84cac7":"> And finaly add features, that calculate credit facility of each card","075973c8":"### Calculating ELO","c966677c":"### Calculating rating","22a9b865":"### First model: LBGM on goss","04a17af3":"### Drop all high-correlated features","e43c145b":"## Feature engineering","342b86ed":"Do the same with new merch","629d94c4":"### Mix with BayesianRidge\nFor the best mix we use BayesianRidge and ensemble out models","2265bc7b":"### Fourth model: Tuned LGBM gbrt ([Dmitry Kravchuk](https:\/\/www.kaggle.com\/dishkakrauch))\nUsing K-fold with 5 splits and enabled shuffle + using diffrent count of features, selected by feature importance  ","f1fa0ff8":"![](http:\/\/)For card, that have target -33 we make simple features, just like borders.","f73e49ab":"Scheme of training.  ","d0c93b87":"Here we can see, that we have many target near zero and some strange target with value <b>-33.21928095<\/b>.  \nFor RMSE this values can be very important.  \nI take this card_id into separate variable to use this in a future","e5b8d962":"*end load block*","098a2b42":"#### This is the main function, that changes format historical_transaction and new_merchant_transactions \n"}}