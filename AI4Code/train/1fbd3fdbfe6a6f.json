{"cell_type":{"fa86f04d":"code","e9310bf6":"code","397728fb":"code","965ef5e7":"code","b0abb4a5":"code","19e5ca95":"code","b49f00c9":"code","97926b0c":"code","447125f1":"code","e84bfffd":"code","b08a7184":"code","578d5278":"code","1b22efa0":"code","6dbe6530":"code","48c5013e":"code","13c130a9":"code","f4fcf9da":"code","d901f2e1":"code","604d94fb":"code","224bec5d":"code","df2d5bf7":"code","5e039477":"code","e4411533":"code","711f60c1":"code","1cf03fd8":"code","46fb8177":"code","b7016c0a":"code","71f8ff9b":"markdown","343f911d":"markdown","307191ac":"markdown","f31b54c5":"markdown","b5b06e1b":"markdown","34af897e":"markdown","8eb28cda":"markdown","6c2a7deb":"markdown","afcf6c53":"markdown","1b964de9":"markdown","c5eebdb6":"markdown","3d01ab63":"markdown"},"source":{"fa86f04d":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\n%matplotlib inline","e9310bf6":"my_data_dir = r'\/kaggle\/input\/01-eda-and-image-augmentatio-v2\/output\/'","397728fb":"os.listdir(my_data_dir) ","965ef5e7":"# check for any corrupted images from image augmentation steps and delete them\n# code obtained from https:\/\/stackoverflow.com\/questions\/67505710\/pil-unidentifiedimageerror-cannot-identify-image-file-io-bytesio-object\nimport PIL\nfrom pathlib import Path\nfrom PIL import UnidentifiedImageError\n\npath = Path(my_data_dir).rglob(\"*.jpg\")\nfor img_p in path:\n    try:\n        img = PIL.Image.open(img_p)\n    except PIL.UnidentifiedImageError:\n        print(img_p) ","b0abb4a5":"#Making another copies of input data to be able to fit the generator in modelling stage \n#Kaggle notebook doesn't allow to fit the read-only files from input folder\nfrom distutils.dir_util import copy_tree\ncopy_dir = r'\/kaggle\/working\/working_images\/'\ncopy_tree(my_data_dir, copy_dir, verbose = 2)\nfrom IPython.display import clear_output\nclear_output(wait=True)\nprint(\"finished\")","19e5ca95":"#Confirming number of images to ensure that the images have been loaded properly\nsplitted_dir = copy_dir\nprint(\"Training folder : \")\ncategory = []\nnumber_images_train = []\nfor cat in os.listdir(splitted_dir + 'train'):\n    print(\"Number of \" + cat + \" images : \" + str(len(os.listdir(splitted_dir+'train' + '\/'+cat))))\n    category.append(cat)\n    number_images_train.append(len(os.listdir(splitted_dir + 'train' + '\/'+cat)))    \nprint(\"Total Number of Images : \" + str(np.sum(number_images_train)))\n\nprint(\"\\n\\nTesting folder : \")\ncategory = []\nnumber_images_test = []\nfor cat in os.listdir(splitted_dir + 'test'):\n    print(\"Number of \" + cat + \" images : \" + str(len(os.listdir(splitted_dir+'test' + '\/'+cat))))\n    category.append(cat)\n    number_images_test.append(len(os.listdir(splitted_dir + 'test' + '\/'+cat)))    \nprint(\"Total Number of Images : \" + str(np.sum(number_images_test)))\n\nprint(\"\\n\\nValidation folder : \")\ncategory = []\nnumber_images_valid = []\nfor cat in os.listdir(splitted_dir + 'train'):\n    print(\"Number of \" + cat + \" images : \" + str(len(os.listdir(splitted_dir+'val' + '\/'+cat))))\n    category.append(cat)\n    number_images_valid.append(len(os.listdir(splitted_dir + 'val' + '\/'+cat)))    \nprint(\"Total Number of Images : \" + str(np.sum(number_images_valid)))","b49f00c9":"plt.figure(figsize=(20,10))\nplt.subplot(1,3,1)\nplt.pie(number_images_train, labels=category, autopct='%.0f%%')\nplt.title(f'Train - {sum(number_images_train)} files')\nplt.subplot(1,3,2)\nplt.pie(number_images_test, labels=category, autopct='%.0f%%')\nplt.title(f'Test - {sum(number_images_test)} files')\nplt.subplot(1,3,3)\nplt.pie(number_images_valid, labels=category, autopct='%.0f%%')\nplt.title(f'Valid - {sum(number_images_valid)} files')\nplt.show()","97926b0c":"!pip install -q imageio\n!pip install -q opencv-python\n!pip install -q git+https:\/\/github.com\/tensorflow\/docs","447125f1":"import tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow_docs.vis import embed\nimport numpy as np\nimport cv2\n\n# Import matplotlib libraries\nfrom matplotlib import pyplot as plt\nfrom matplotlib.collections import LineCollection\nimport matplotlib.patches as patches\n\n# Some modules to display an animation using imageio.\nimport imageio\nfrom IPython.display import HTML, display","e84bfffd":"model_name = \"movenet_lightning\"\n\nif \"tflite\" in model_name:\n  if \"movenet_lightning_f16\" in model_name:\n    !wget -q -O model.tflite https:\/\/tfhub.dev\/google\/lite-model\/movenet\/singlepose\/lightning\/tflite\/float16\/4?lite-format=tflite\n    input_size = 192\n  elif \"movenet_thunder_f16\" in model_name:\n    !wget -q -O model.tflite https:\/\/tfhub.dev\/google\/lite-model\/movenet\/singlepose\/thunder\/tflite\/float16\/4?lite-format=tflite\n    input_size = 256\n  elif \"movenet_lightning_int8\" in model_name:\n    !wget -q -O model.tflite https:\/\/tfhub.dev\/google\/lite-model\/movenet\/singlepose\/lightning\/tflite\/int8\/4?lite-format=tflite\n    input_size = 192\n  elif \"movenet_thunder_int8\" in model_name:\n    !wget -q -O model.tflite https:\/\/tfhub.dev\/google\/lite-model\/movenet\/singlepose\/thunder\/tflite\/int8\/4?lite-format=tflite\n    input_size = 256\n  else:\n    raise ValueError(\"Unsupported model name: %s\" % model_name)\n\n  # Initialize the TFLite interpreter\n  interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n  interpreter.allocate_tensors()\n\n  def movenet(input_image):\n    \"\"\"Runs detection on an input image.\n\n    Args:\n      input_image: A [1, height, width, 3] tensor represents the input image\n        pixels. Note that the height\/width should already be resized and match the\n        expected input resolution of the model before passing into this function.\n\n    Returns:\n      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n      coordinates and scores.\n    \"\"\"\n    # TF Lite format expects tensor type of uint8.\n    input_image = tf.cast(input_image, dtype=tf.uint8)\n    input_details = interpreter.get_input_details()\n    output_details = interpreter.get_output_details()\n    interpreter.set_tensor(input_details[0]['index'], input_image.numpy())\n    # Invoke inference.\n    interpreter.invoke()\n    # Get the model prediction.\n    keypoints_with_scores = interpreter.get_tensor(output_details[0]['index'])\n    return keypoints_with_scores\n\nelse:\n  if \"movenet_lightning\" in model_name:\n    module = hub.load(\"https:\/\/tfhub.dev\/google\/movenet\/singlepose\/lightning\/4\")\n    input_size = 192\n  elif \"movenet_thunder\" in model_name:\n    module = hub.load(\"https:\/\/tfhub.dev\/google\/movenet\/singlepose\/thunder\/4\")\n    input_size = 256\n  else:\n    raise ValueError(\"Unsupported model name: %s\" % model_name)\n\n  def movenet(input_image):\n    \"\"\"Runs detection on an input image.\n\n    Args:\n      input_image: A [1, height, width, 3] tensor represents the input image\n        pixels. Note that the height\/width should already be resized and match the\n        expected input resolution of the model before passing into this function.\n\n    Returns:\n      A [1, 1, 17, 3] float numpy array representing the predicted keypoint\n      coordinates and scores.\n    \"\"\"\n    model = module.signatures['serving_default']\n\n    # SavedModel format expects tensor type of int32.\n    input_image = tf.cast(input_image, dtype=tf.int32)\n    # Run model inference.\n    outputs = model(input_image)\n    # Output is a [1, 1, 17, 3] tensor.\n    keypoint_with_scores = outputs['output_0'].numpy()\n    return keypoint_with_scores","b08a7184":"def draw_prediction_on_image(\n     image, keypoints_with_scores, crop_region=None, close_figure=False,\n     output_image_height=None):\n   \"\"\"Draws the keypoint predictions on image\"\"\"\n   height, width, channel = image.shape\n   aspect_ratio = float(width) \/ height\n   fig, ax = plt.subplots(figsize=(12 * aspect_ratio, 12))\n   # To remove the huge white borders\n   fig.tight_layout(pad=0)\n   ax.margins(0)\n   ax.set_yticklabels([])\n   ax.set_xticklabels([])\n   plt.axis('off')\n   im = ax.imshow(image)\n   line_segments = LineCollection([], linewidths=(4), linestyle='solid')\n   ax.add_collection(line_segments)\n   # Turn off tick labels\n   scat = ax.scatter([], [], s=60, color='#FF1493', zorder=3)\n   (keypoint_locs, keypoint_edges,\n    edge_colors) = _keypoints_and_edges_for_display(\n        keypoints_with_scores, height, width)\n   line_segments.set_segments(keypoint_edges)\n   line_segments.set_color(edge_colors)\n   if keypoint_edges.shape[0]:\n     line_segments.set_segments(keypoint_edges)\n     line_segments.set_color(edge_colors)\n   if keypoint_locs.shape[0]:\n     scat.set_offsets(keypoint_locs)\n   if crop_region is not None:\n     xmin = max(crop_region['x_min'] * width, 0.0)\n     ymin = max(crop_region['y_min'] * height, 0.0)\n     rec_width = min(crop_region['x_max'], 0.99) * width - xmin\n     rec_height = min(crop_region['y_max'], 0.99) * height - ymin\n     rect = patches.Rectangle(\n         (xmin,ymin),rec_width,rec_height,\n         linewidth=1,edgecolor='b',facecolor='none')\n     ax.add_patch(rect)\n   fig.canvas.draw()\n   image_from_plot = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)\n   image_from_plot = image_from_plot.reshape(\n       fig.canvas.get_width_height()[::-1] + (3,))\n   plt.close(fig)\n   if output_image_height is not None:\n     output_image_width = int(output_image_height \/ height * width)\n     image_from_plot = cv2.resize(\n         image_from_plot, dsize=(output_image_width, output_image_height),\n          interpolation=cv2.INTER_CUBIC)\n   return image_from_plot ","578d5278":"def _keypoints_and_edges_for_display(keypoints_with_score,height,\n                                      width,keypoint_threshold=0.11):\n   \"\"\"Returns high confidence keypoints and edges\"\"\"\n   keypoints_all = []\n   keypoint_edges_all = []\n   edge_colors = []\n   num_instances,_,_,_ = keypoints_with_score.shape\n   for id in range(num_instances):\n     kpts_x = keypoints_with_score[0,id,:,1]\n     kpts_y = keypoints_with_score[0,id,:,0]\n     kpts_scores = keypoints_with_score[0,id,:,2]\n     kpts_abs_xy = np.stack(\n         [width*np.array(kpts_x),height*np.array(kpts_y)],axis=-1)\n     kpts_above_thrs_abs = kpts_abs_xy[kpts_scores > keypoint_threshold,: ]\n     keypoints_all.append(kpts_above_thrs_abs)\n     for edge_pair,color in KEYPOINT_EDGE_INDS_TO_COLOR.items():\n       if (kpts_scores[edge_pair[0]] > keypoint_threshold and \n           kpts_scores[edge_pair[1]] > keypoint_threshold):\n         x_start = kpts_abs_xy[edge_pair[0],0]\n         y_start = kpts_abs_xy[edge_pair[0],1]\n         x_end = kpts_abs_xy[edge_pair[1],0]\n         y_end = kpts_abs_xy[edge_pair[1],1]\n         lien_seg = np.array([[x_start,y_start],[x_end,y_end]])\n         keypoint_edges_all.append(lien_seg)\n         edge_colors.append(color)\n   if keypoints_all:\n     keypoints_xy = np.concatenate(keypoints_all,axis=0)\n   else:\n     keypoints_xy = np.zeros((0,17,2))\n   if keypoint_edges_all:\n     edges_xy = np.stack(keypoint_edges_all,axis=0)\n   else:\n     edges_xy = np.zeros((0,2,2))\n   return keypoints_xy,edges_xy,edge_colors","1b22efa0":" # Dictionary to map joints of body part\n KEYPOINT_DICT = {\n     'nose':0,\n     'left_eye':1,\n     'right_eye':2,\n     'left_ear':3,\n     'right_ear':4,\n     'left_shoulder':5,\n     'right_shoulder':6,\n     'left_elbow':7,\n     'right_elbow':8,\n     'left_wrist':9,\n     'right_wrist':10,\n     'left_hip':11,\n     'right_hip':12,\n     'left_knee':13,\n     'right_knee':14,\n     'left_ankle':15,\n     'right_ankle':16\n } ","6dbe6530":"# map bones to matplotlib color name\nKEYPOINT_EDGE_INDS_TO_COLOR = {\n     (0,1): 'm',\n     (0,2): 'c',\n     (1,3): 'm',\n     (2,4): 'c',\n     (0,5): 'm',\n     (0,6): 'c',\n     (5,7): 'm',\n     (7,9): 'm',\n     (6,8): 'c',\n     (8,10): 'c',\n     (5,6): 'y',\n     (5,11): 'm',\n     (6,12): 'c',\n     (11,12): 'y',\n     (11,13): 'm',\n     (13,15): 'm',\n     (12,14): 'c',\n     (14,16): 'c'\n } ","48c5013e":"import tensorflow as tf\ntree_example_dir =  r'..\/input\/01-eda-and-image-augmentatio-v2\/output\/train\/tree\/00000000.jpg'\n# Load the input image.\nimage_path = tree_example_dir\nimage = tf.io.read_file(image_path)\nimage = tf.image.decode_jpeg(image)","13c130a9":"# Resize and pad the image to keep the aspect ratio and fit the expected size.\ninput_image = tf.expand_dims(image, axis=0)\ninput_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n\n# Run model inference.\nkeypoint_with_scores = movenet(input_image)\n\n# Visualize the predictions with image.\ndisplay_image = tf.expand_dims(image, axis=0)\ndisplay_image = tf.cast(tf.image.resize_with_pad(\n    display_image, 1280, 1280), dtype=tf.int32)\noutput_overlay = draw_prediction_on_image(\n    np.squeeze(display_image.numpy(), axis=0), keypoint_with_scores)\n\nplt.figure(figsize=(20, 20))\nplt.imshow(output_overlay)\n_ = plt.axis('off')","f4fcf9da":"keypoint_with_scores.shape","d901f2e1":"keypoint_with_scores # detail of each keypoint is y_coordinate, x_coordinate, score","604d94fb":"# Function to get the keystroke and save to a dataframe\ndef keypoint_generation(image_path):\n    # Load the input image.\n    image_path = image_path\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image)\n    # Resize and pad the image to keep the aspect ratio and fit the expected size.\n    input_image = tf.expand_dims(image, axis=0)\n    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n    # Run model inference\n    keypoint_with_scores = movenet(input_image)\n\n    return keypoint_with_scores[0][0]","224bec5d":"#Bruteforce to extract all keypoints into a data frame\ndef convert_to_df(list_keypoint, cat):\n    df = pd.DataFrame(list_keypoint.items(), columns = ['image_name','keypoint'])\n    from tqdm.notebook import tqdm_notebook\n    nose_y = []\n    nose_x = []\n    nose_score = []\n    left_eye_y = []\n    left_eye_x = []\n    left_eye_score = []\n    right_eye_y = []\n    right_eye_x = []\n    right_eye_score = []\n    left_ear_y = []\n    left_ear_x = []\n    left_ear_score = []\n    right_ear_y = []\n    right_ear_x = []\n    right_ear_score = []\n    left_shoulder_y = []\n    left_shoulder_x = []\n    left_shoulder_score = []\n    right_shoulder_y = []\n    right_shoulder_x = []\n    right_shoulder_score = []\n    left_elbow_y = []\n    left_elbow_x = []\n    left_elbow_score = []\n    right_elbow_y = []\n    right_elbow_x = []\n    right_elbow_score = []\n    left_wrist_y = []\n    left_wrist_x = []\n    left_wrist_score = []\n    right_wrist_y = []\n    right_wrist_x = []\n    right_wrist_score = []\n    left_hip_y = []\n    left_hip_x = []\n    left_hip_score = []\n    right_hip_y = []\n    right_hip_x = []\n    right_hip_score = []\n    left_knee_y = []\n    left_knee_x = []\n    left_knee_score = []\n    right_knee_y = []\n    right_knee_x = []\n    right_knee_score = []\n    left_ankle_y = []\n    left_ankle_x = []\n    left_ankle_score = []\n    right_ankle_y = []\n    right_ankle_x = []\n    right_ankle_score = []\n    for row in tqdm_notebook(df['keypoint']):\n        nose_y.append(row[0][0])\n        nose_x.append(row[0][1])\n        nose_score.append(row[0][2])\n        left_eye_y.append(row[1][0])\n        left_eye_x.append(row[1][1])\n        left_eye_score.append(row[1][2])\n        right_eye_y.append(row[2][0])\n        right_eye_x.append(row[2][1])\n        right_eye_score.append(row[2][2])\n        left_ear_y.append(row[3][0])\n        left_ear_x.append(row[3][1])\n        left_ear_score.append(row[3][2])\n        right_ear_y.append(row[4][0])\n        right_ear_x.append(row[4][1])\n        right_ear_score.append(row[4][2])\n        left_shoulder_y.append(row[5][0])\n        left_shoulder_x.append(row[5][1])\n        left_shoulder_score.append(row[5][2])\n        right_shoulder_y.append(row[6][0])\n        right_shoulder_x.append(row[6][1])\n        right_shoulder_score.append(row[6][2])\n        left_elbow_y.append(row[7][0])\n        left_elbow_x.append(row[7][1])\n        left_elbow_score.append(row[7][2])\n        right_elbow_y.append(row[8][0])\n        right_elbow_x.append(row[8][1])\n        right_elbow_score.append(row[8][2])\n        left_wrist_y.append(row[9][0])\n        left_wrist_x.append(row[9][1])\n        left_wrist_score.append(row[9][2])\n        right_wrist_y.append(row[10][0])\n        right_wrist_x.append(row[10][1])\n        right_wrist_score.append(row[10][2])\n        left_hip_y.append(row[11][0])\n        left_hip_x.append(row[11][1])\n        left_hip_score.append(row[11][2])\n        right_hip_y.append(row[12][0])\n        right_hip_x.append(row[12][1])\n        right_hip_score.append(row[12][2])\n        left_knee_y.append(row[13][0])\n        left_knee_x.append(row[13][1])\n        left_knee_score.append(row[13][2])\n        right_knee_y.append(row[14][0])\n        right_knee_x.append(row[14][1])\n        right_knee_score.append(row[14][2])\n        left_ankle_y.append(row[15][0])\n        left_ankle_x.append(row[15][1])\n        left_ankle_score.append(row[15][2])\n        right_ankle_y.append(row[16][0])\n        right_ankle_x.append(row[16][1])\n        right_ankle_score.append(row[16][2])\n    df.insert(loc=0, column='category',value=cat)\n    df.insert(loc=3, column='nose_y',value=nose_y)\n    df.insert(loc=4, column='nose_x',value=nose_x)\n    df.insert(loc=5, column='nose_score',value=nose_score)\n    df.insert(loc=6, column='left_eye_y',value=left_eye_y)\n    df.insert(loc=7, column='left_eye_x',value=left_eye_x)\n    df.insert(loc=8, column='left_eye_score',value=left_eye_score)\n    df.insert(loc=9, column='right_eye_y',value=right_eye_y)\n    df.insert(loc=10, column='right_eye_x',value=right_eye_x)\n    df.insert(loc=11, column='right_eye_score',value=right_eye_score)\n    df.insert(loc=12, column='left_ear_y',value=left_ear_y)\n    df.insert(loc=13, column='left_ear_x',value=left_ear_x)\n    df.insert(loc=14, column='left_ear_score',value=left_ear_score)\n    df.insert(loc=15, column='right_ear_y',value=right_ear_y)\n    df.insert(loc=16, column='right_ear_x',value=right_ear_x)\n    df.insert(loc=17, column='right_ear_score',value=right_ear_score)\n    df.insert(loc=18, column='left_shoulder_y',value=left_shoulder_y)\n    df.insert(loc=19, column='left_shoulder_x',value=left_shoulder_x)\n    df.insert(loc=20, column='left_shoulder_score',value=left_shoulder_score)\n    df.insert(loc=21, column='right_shoulder_y',value=right_shoulder_y)\n    df.insert(loc=22, column='right_shoulder_x',value=right_shoulder_x)\n    df.insert(loc=23, column='right_shoulder_score',value=right_shoulder_score)\n    df.insert(loc=24, column='left_elbow_y',value=left_elbow_y)\n    df.insert(loc=25, column='left_elbow_x',value=left_elbow_x)\n    df.insert(loc=26, column='left_elbow_score',value=left_elbow_score)\n    df.insert(loc=27, column='right_elbow_y',value=right_elbow_y)\n    df.insert(loc=28, column='right_elbow_x',value=right_elbow_x)\n    df.insert(loc=29, column='right_elbow_score',value=right_elbow_score)\n    df.insert(loc=30, column='left_wrist_y',value=left_wrist_y)\n    df.insert(loc=31, column='left_wrist_x',value=left_wrist_x)\n    df.insert(loc=32, column='left_wrist_score',value=left_wrist_score)\n    df.insert(loc=33, column='right_wrist_y',value=right_wrist_y)\n    df.insert(loc=34, column='right_wrist_x',value=right_wrist_x)\n    df.insert(loc=35, column='right_wrist_score',value=right_wrist_score)\n    df.insert(loc=36, column='left_hip_y',value=left_hip_y)\n    df.insert(loc=37, column='left_hip_x',value=left_hip_x)\n    df.insert(loc=38, column='left_hip_score',value=left_hip_score)\n    df.insert(loc=39, column='right_hip_y',value=right_hip_y)\n    df.insert(loc=40, column='right_hip_x',value=right_hip_x)\n    df.insert(loc=41, column='right_hip_score',value=right_hip_score)\n    df.insert(loc=42, column='left_knee_y',value=left_knee_y)\n    df.insert(loc=43, column='left_knee_x',value=left_knee_x)\n    df.insert(loc=44, column='left_knee_score',value=left_knee_score)\n    df.insert(loc=45, column='right_knee_y',value=right_knee_y)\n    df.insert(loc=46, column='right_knee_x',value=right_knee_x)\n    df.insert(loc=47, column='right_knee_score',value=right_knee_score)\n    df.insert(loc=48, column='left_ankle_y',value=left_ankle_y)\n    df.insert(loc=49, column='left_ankle_x',value=left_ankle_x)\n    df.insert(loc=50, column='left_ankle_score',value=left_ankle_score)\n    df.insert(loc=51, column='right_ankle_y',value=right_ankle_y)\n    df.insert(loc=52, column='right_ankle_x',value=right_ankle_x)\n    df.insert(loc=53, column='right_ankle_score',value=right_ankle_score)\n   \n    return df","df2d5bf7":"def gen_keypoint_on_image(image_path):\n    import tensorflow as tf\n    tree_example_dir =  image_path\n    # Load the input image.\n    image_path = tree_example_dir\n    image = tf.io.read_file(image_path)\n    image = tf.image.decode_jpeg(image)\n    # Resize and pad the image to keep the aspect ratio and fit the expected size.\n    input_image = tf.expand_dims(image, axis=0)\n    input_image = tf.image.resize_with_pad(input_image, input_size, input_size)\n\n    # Run model inference.\n    keypoint_with_scores = movenet(input_image)\n\n    # Visualize the predictions with image.\n    display_image = tf.expand_dims(image, axis=0)\n    display_image = tf.cast(tf.image.resize_with_pad(\n        display_image, 1280, 1280), dtype=tf.int32)\n    output_overlay = draw_prediction_on_image(\n        np.squeeze(display_image.numpy(), axis=0), keypoint_with_scores)\n\n    plt.figure(figsize=(20, 20))\n    plt.imshow(output_overlay)\n    _ = plt.axis('off')\n    return plt, keypoint_with_scores","5e039477":"plt, a = gen_keypoint_on_image('\/kaggle\/input\/01-eda-and-image-augmentatio-v2\/output\/train\/goddess\/00000000.jpg')\nplt.show()\nprint(a)","e4411533":"df_by_cat = []\nfrom tqdm.notebook import tqdm_notebook\nfor cat in tqdm_notebook(os.listdir(splitted_dir + 'train')):\n    list_keypoint = {}\n    list_cat = []\n    for image in os.listdir(splitted_dir + 'train' + '\/' + cat):\n        image_path = splitted_dir + 'train' + '\/' + cat + '\/' + image\n        try:\n            list_keypoint[image] = keypoint_generation(image_path)\n            list_cat.append(cat)\n        except:\n            print('Error finding keypoints on Image: ' + image_path)\n            continue\n    df_by_cat.append(convert_to_df(list_keypoint, cat))\n\ndf = pd.concat([df_by_cat[0], df_by_cat[1], df_by_cat[2], df_by_cat[3], df_by_cat[4], df_by_cat[5]], axis = 0)\ndf.to_csv(\"\/kaggle\/working\/train_df.csv\")\ndf.info()","711f60c1":"df_by_cat = []\nfrom tqdm.notebook import tqdm_notebook\nfor cat in tqdm_notebook(os.listdir(splitted_dir + 'test')):\n    list_keypoint = {}\n    list_cat = []\n    for image in os.listdir(splitted_dir + 'test' + '\/' + cat):\n        image_path = splitted_dir + 'test' + '\/' + cat + '\/' + image\n        try:\n            list_keypoint[image] = keypoint_generation(image_path)\n            list_cat.append(cat)\n        except:\n            print('Error finding keypoints on Image: ' + image_path)\n            continue\n    df_by_cat.append(convert_to_df(list_keypoint, cat))\n\ndf = pd.concat([df_by_cat[0], df_by_cat[1], df_by_cat[2], df_by_cat[3], df_by_cat[4], df_by_cat[5]], axis = 0)\ndf.to_csv(\"\/kaggle\/working\/test_df.csv\")\ndf.info()","1cf03fd8":"df_by_cat = []\nfrom tqdm.notebook import tqdm_notebook\nfor cat in tqdm_notebook(os.listdir(splitted_dir + 'val')):\n    list_keypoint = {}\n    list_cat = []\n    for image in os.listdir(splitted_dir + 'val' + '\/' + cat):\n        image_path = splitted_dir + 'val' + '\/' + cat + '\/' + image\n        try:\n            list_keypoint[image] = keypoint_generation(image_path)\n            list_cat.append(cat)\n        except:\n            print('Error finding keypoints on Image: ' + image_path)\n            continue\n    df_by_cat.append(convert_to_df(list_keypoint, cat))\n\ndf = pd.concat([df_by_cat[0], df_by_cat[1], df_by_cat[2], df_by_cat[3], df_by_cat[4], df_by_cat[5]], axis = 0)\ndf.to_csv(\"\/kaggle\/working\/val_df.csv\")\ndf.info()","46fb8177":"print(\"finish\")","b7016c0a":"df.head()","71f8ff9b":"# 4. Keypoint generation using tensorflow MoveNet.\nFollowing an easy tutorial from https:\/\/www.tensorflow.org\/hub\/tutorials\/movenet","343f911d":"# 5. Test the function and plot the keypoint on an example image.","307191ac":"# 3. Making another copies of data","f31b54c5":"Helper function from https:\/\/analyticsindiamag.com\/how-to-do-pose-estimation-with-movenet\/","b5b06e1b":"# 7. Generating keypoints for training\/testing\/validating folders and saving them as three separate csv files.","34af897e":"This is the 3rd notebook of total 5 notebooks in this series listed as following:\n1. EDA and image augmentation note books >> https:\/\/www.kaggle.com\/suradechk\/01-eda-and-image-augmentation-v2\n2. Setting up a baseline model using CNN >> https:\/\/www.kaggle.com\/suradechk\/02-baseline-model-using-cnn-v2\n3. Keypoint generation using movenet >> https:\/\/www.kaggle.com\/suradechk\/03-keypoint-movenet-v2\n4. Classification keypoint output using classical ML >> https:\/\/www.kaggle.com\/suradechk\/04-classification-using-keypoints-output-v2\n5. Classification keypoint output using ANN >> https:\/\/www.kaggle.com\/suradechk\/05-classification-using-ann-v2","8eb28cda":"# 1. Obtain the images from 01 EDA and Image augmentation notebook","6c2a7deb":"There are no corrupted images found so far, so we proceed with the next steps.","afcf6c53":"# 2. Check for any corrupted images.","1b964de9":"# 6. Define our custom function to brute force generating keypoints and record them as a dataframe.","c5eebdb6":"https:\/\/www.kaggle.com\/suradechk\/01-eda-and-image-augmentation\/notebook","3d01ab63":"# DAAN570 - Final Project - RSI Prevention by Yoga - Modelling notebook\n<br>Team: 11: Suradech Kongkiatpaiboon and Burq Latif\nCourse: DAAN 570 \u2013 Deep Learning (Fall, 2021) - Penn State World Campus\n\n> Problem statement : Repetitive stress injury is extremely prevalent for anyone who works at the same spot for a long length of time, especially with the development of COVID-19 and the increase in work from home trend. We've identified certain flaws in traditional RSI prevention software on the market, and we've noted that yoga's popularity is growing by the day. The reason for this is the numerous physical, mental, and spiritual advantages that yoga may provide. Many people are following this trend and practice yoga without the help of a professional. However, doing yoga incorrectly or without adequate instruction can lead to serious health problems such as strokes and nerve damage. As a result, adhering to appropriate yoga poses is a vital consideration.\nIn this work, we present a method for identifying the user's postures and providing visual guidance to the user. In order to be more engaging with the user, this procedure is done in real-time and utilizes the traditional webcam on the laptop\/desktop to run the application.\n\nKeywords : Yoga, posture, classification, movenet, keypoint\n\nData Collection:\nWe took some images from open source yoga posture dataset from three following sites and applied basic data cleaning manually (e.g. remove corrupted images, remove misclassified yoga posture images).\n1. Open source dataset from https:\/\/www.kaggle.com\/general\/192938\n2. 3D synthetic dataset from https:\/\/laurencemoroney.com\/2021\/08\/23\/yogapose-dataset.html\n3. Yoga-82 dataset from https:\/\/sites.google.com\/view\/yoga-82\/home"}}