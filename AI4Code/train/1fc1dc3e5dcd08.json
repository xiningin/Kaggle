{"cell_type":{"26814789":"code","6287af7b":"code","e9cbd671":"code","62159ae2":"code","0a04ffbc":"code","c02bcd62":"code","d8c7b45f":"code","66d3e19a":"code","85b31327":"code","90e86424":"code","f05c8f59":"code","5e72c770":"code","0e74a5cf":"code","dedccade":"code","9bdf82a9":"code","96ed3772":"code","cf85d39d":"code","017eae03":"code","506c5faa":"code","a443fd8f":"code","3e9d8cc5":"code","2525f12a":"code","3636b102":"code","680712a6":"code","ced09e4f":"code","20d608d2":"code","ebc6cf11":"code","efc1257a":"code","cce0f675":"code","103000ce":"code","cec4948d":"code","313fbe6c":"code","01b4e612":"code","a73e5def":"code","b2b50960":"code","130639c5":"code","5bee2e2c":"code","5d0ed344":"code","eec5bf1a":"code","78fae4d8":"markdown","1e92e60d":"markdown","e16ac2e1":"markdown","89175c26":"markdown","5810cf5c":"markdown","42148b77":"markdown"},"source":{"26814789":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6287af7b":"import pandas as pd\nimport shutil\nfrom PIL import Image as PImage\nfrom matplotlib import pyplot as plt\nimport keras\nimport tensorflow as tf","e9cbd671":"from keras.datasets import cifar10","62159ae2":"data = tf.keras.datasets.cifar10","0a04ffbc":"(x_train, y_train), (x_test, y_test) = data.load_data()","c02bcd62":"size = x_train.shape","d8c7b45f":"size","66d3e19a":"x_train[0]","85b31327":"plt.imshow(x_train[1])\nplt.show()","90e86424":"x_train[0]","f05c8f59":"samples  = x_train[0]","5e72c770":"from keras.preprocessing.image import ImageDataGenerator\ndatagen = ImageDataGenerator(width_shift_range=[-0.2,0.2],height_shift_range= [-0.2,0.2])","0e74a5cf":"# prepare iterator\nit = datagen.flow(x_train[0:1,:] ,batch_size=1)","dedccade":"# generate samples and plot\nfor i in range(9):\n    # define subplot\n    plt.subplot(330 + 1 + i)\n    # generate batch of images\n    batch = it.next()\n    # convert to unsigned integers for viewing\n    image = batch[0].astype('uint8')\n    # plot raw pixel data\n    plt.imshow(image)","9bdf82a9":"data_lst =  []\nlabel_lst = []","96ed3772":"# prepare iterator\nit = datagen.flow(x_train,y_train ,batch_size=1)","cf85d39d":"i = 0\nfor x_batch,y_batch in it:\n    #plt.subplot(330 + 1 + i)\n    image = x_batch[0].astype('uint8')\n    data_lst.append(image)\n    label_lst.append(y_batch[0])\n    #print(image)\n    #print(y_batch)\n    i= i+1\n    #plt.imshow(image)\n    if(i>999):\n        break","017eae03":"label_lst","506c5faa":"final = []\nfor i in range(1000):\n    final.extend(label_lst[i])","a443fd8f":"final","3e9d8cc5":"from collections import Counter\nfinal = Counter(final)","2525f12a":"final","3636b102":"data_augmented  = np.stack(data_lst,axis=0)","680712a6":"data_augmented.shape","ced09e4f":"final_data  = np.vstack((x_train,data_augmented))","20d608d2":"final_data.shape","ebc6cf11":"label_augmented = np.array(label_lst)","efc1257a":"label_augmented.shape","cce0f675":"y_final = np.vstack((y_train,label_lst))","103000ce":"y_final.shape","cec4948d":"x_train = final_data.astype('float32')\nx_test = x_test.astype('float32')\nx_train \/= 255\nx_test \/= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')","313fbe6c":"num_classes = 10\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_final, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)","01b4e612":"input_shape = (32,32,3)\nbatch_size =100\nepochs =10","a73e5def":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D","b2b50960":"model  = Sequential()\n\nmodel.add(Conv2D(6,(5,5),activation = 'relu',input_shape = input_shape))\nmodel.add((MaxPooling2D(pool_size = (2,2))))\n\nmodel.add(Conv2D(16,(5,5),activation = 'relu'))\nmodel.add(MaxPooling2D(pool_size= (2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(120,activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(84,activation = 'relu'))\n\nmodel.add(Dense(10,activation = 'softmax'))","130639c5":"model.summary()","5bee2e2c":"model.compile(loss= 'categorical_crossentropy',optimizer = 'adam' , metrics = ['accuracy'])\nhistory=model.fit(x_train, y_train,batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(x_test, y_test))","5d0ed344":"%matplotlib notebook\n%matplotlib inline\n\nimport time\n# https:\/\/gist.github.com\/greydanus\/f6eee59eaf1d90fcb3b534a25362cea4\n# https:\/\/stackoverflow.com\/a\/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(x, vy, ty, ax, colors=['b']):\n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    #plt.legend()\n    plt.grid()\n    plt.show()\n    fig.canvas.draw()","eec5bf1a":"fig,ax = plt.subplots(1,1)\nax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n# list of epoch numbers\nx = list(range(1,epochs+1))\n\n# print(history.history.keys())\n# dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n# history = model_drop.fit(X_train, Y_train, batch_size=batch_size, epochs=nb_epoch, verbose=1, validation_data=(X_test, Y_test))\n\n# we will get val_loss and val_acc only when you pass the paramter validation_data\n# val_loss : validation loss\n# val_acc : validation accuracy\n\n# loss : training loss\n# acc : train accuracy\n# for each key in histrory.histrory we will have a list of length equal to number of epochs\n\nvy = history.history['val_accuracy']\nty = history.history['accuracy']\nplt_dynamic(x, vy, ty, ax)","78fae4d8":" https:\/\/www.pyimagesearch.com\/2019\/07\/08\/keras-imagedatagenerator-and-data-augmentation\/\n https:\/\/keras.io\/api\/preprocessing\/image\/\n https:\/\/machinelearningmastery.com\/how-to-configure-image-data-augmentation-when-training-deep-learning-neural-networks\/","1e92e60d":"# As we can see after using data augmentation there is less overfitting as the accuracy of train and test data are almost similar","e16ac2e1":"source used","89175c26":"using lenet","5810cf5c":"using 1st image to check how images are generated using data augmentation","42148b77":"ow using complete x_train and producing 1000 new images"}}