{"cell_type":{"5b4fbd05":"code","c5f96dbe":"code","30ebe1df":"code","17a7f5d4":"code","41a9708c":"code","957f4111":"code","894f8b7f":"code","51377114":"code","90f9d5bf":"code","1d5fce86":"code","16bc1406":"code","b82646d5":"code","b5ee8f33":"code","2097f41d":"code","74dcb494":"code","5a40b9bd":"code","3a86c487":"code","8dc22134":"code","8b336449":"code","56bc20c0":"code","001d1fa6":"code","22257212":"code","56f99881":"code","bc6074f0":"code","28214820":"code","f0906760":"code","892ff059":"code","13b8d280":"code","88bb8c88":"code","355d446b":"code","b9713257":"code","5658a756":"code","35f6e9bb":"code","e75c9f84":"code","028bc9db":"code","759d454b":"code","cbd8966c":"code","bc61aaa3":"markdown","7710fedb":"markdown","60c75699":"markdown","7df6d256":"markdown","ab48094e":"markdown","447cd856":"markdown","cb2e982c":"markdown","cbe023b1":"markdown","862b8ff8":"markdown","041b7650":"markdown","e707c3fc":"markdown","b3a1c6d3":"markdown","98c3e1f3":"markdown","0f7fffec":"markdown","87dd8a60":"markdown","737637c7":"markdown","d42993a6":"markdown","e654da32":"markdown","9802d154":"markdown","f25f3363":"markdown","b95aa526":"markdown","ba5f26bd":"markdown","30c2f4ae":"markdown","eec0e132":"markdown","abbe6583":"markdown","af130325":"markdown","ab4bbf7b":"markdown","06f7d0db":"markdown","122155ea":"markdown","151de83b":"markdown","8dfae70e":"markdown","6f91a7db":"markdown"},"source":{"5b4fbd05":"# import package\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom ta import add_all_ta_features # need to install 'ta' in the terminal first\nfrom ta.utils import dropna\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_error\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c5f96dbe":"# loading data\naapl = pd.read_csv('~\/Downloads\/AAPL.csv') # price data, daily\n\nLIBOR = pd.read_csv('~\/Downloads\/USD3MTD156N-2.csv') # macroeconomic data Libor 3 months, daily\nLIBOR['USD3MTD156N'] = LIBOR['USD3MTD156N'].replace(['.'],0) # replace the missing value with 0\nLIBOR['USD3MTD156N'] = pd.to_numeric(LIBOR['USD3MTD156N']) # convert string to numeric\n\nUSDollar = pd.read_csv('~\/Downloads\/DTWEXBGS.csv') # macroeconomic data traded weight USDollar index, daily\nUSDollar['DTWEXBGS'] = USDollar['DTWEXBGS'].replace(['.'],0) # replace the missing value with 0\nUSDollar['DTWEXBGS'] = pd.to_numeric(USDollar['DTWEXBGS']) # convert string to numeric\n\ninflation = pd.read_csv('~\/Downloads\/phgube42dipgyl0b.csv') # macroeconomic data inflation index, monthly\n\nTech_Pulse = pd.read_csv('~\/Downloads\/SFTPINDM114SFRBSF.csv') # fundamental data Tech Pulse, monthly","30ebe1df":"aapl.tail()","17a7f5d4":"plt.plot(aapl['Adj Close'])\nplt.title('Adjusted Close')","41a9708c":"# Rolling Volatility \nprice_list=pd.Series(aapl['Adj Close']) \nvar=20 \nrollist=price_list.rolling(var) \nrolvol=rollist.std(ddof=0) \nplt.plot(rolvol) \nplt.title('Rolling volatility') \nplt.show()","957f4111":"plt.plot(aapl['High'], 'b') \nplt.plot(aapl['Low'], 'g') \nplt.title('High vs Low')\nplt.show()","894f8b7f":"# adding technical indicators as variables\naapl_ta = add_all_ta_features(aapl, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\",fillna=True)\naapl = aapl[['Date','Open', 'High', 'Low','Adj Close', 'Volume',\n             'volume_obv','volatility_atr','trend_macd','momentum_rsi']]","51377114":"len(aapl_ta.columns)","90f9d5bf":"aapl.head(10)","1d5fce86":"a = LIBOR['USD3MTD156N'][LIBOR['USD3MTD156N']>0]\nplt.plot(a)\nplt.title('LIBOR 3 month')","16bc1406":"a = USDollar['DTWEXBGS'][USDollar['DTWEXBGS']>0]\nplt.plot(a)\nplt.title('Traded weight USDollar index')","b82646d5":"plt.plot(inflation['cpiind'])\nplt.title('U.S. inflation')","b5ee8f33":"plt.plot(Tech_Pulse['SFTPINDM114SFRBSF'])\nplt.title('USDollar index')","2097f41d":"# combine variables together, daily\naapl_daily = pd.merge(left=aapl, right=LIBOR, left_on='Date', right_on='DATE')\naapl_daily = pd.merge(left=aapl_daily, right=USDollar, left_on='Date', right_on='DATE')\naapl_daily = aapl_daily.rename(columns={'USD3MTD156N':'LIBOR', 'DTWEXBGS':'USDollar'})\naapl_daily = aapl_daily.drop(columns=['DATE_x','DATE_y'])\naapl_daily = aapl_daily[['Date', 'Open', 'High', 'Low', 'Volume', 'volume_obv',\n       'volatility_atr', 'trend_macd', 'momentum_rsi', 'LIBOR', 'USDollar', 'Adj Close' ]] # moving column 'Adj Close' to the back\n\naapl_daily = aapl_daily[aapl_daily['LIBOR']>0] # delete zero value columns\naapl_daily = aapl_daily[aapl_daily['USDollar']>0] # delete zero value columns\naapl_daily = aapl_daily[aapl_daily['volatility_atr']>0] # delete zero value columns\naapl_data = aapl_daily.iloc[:,1:].values \naapl_daily.head(10)","74dcb494":"np.shape(aapl_daily)","5a40b9bd":"np.shape(aapl_data)","3a86c487":"# scale the data\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler(feature_range = (0, 1))\naapl_data = sc.fit_transform(aapl_data)","8dc22134":"# split into training and test sets\ntrain_size = int(len(aapl_data) * 0.67)\ntest_size = len(aapl_data) - train_size\ntrain, test = aapl_data[0:train_size,:], aapl_data[train_size:len(aapl_data),:]\nprint(len(train), len(test))","8b336449":"np.shape(train)","56bc20c0":"# create data matrix for different timesteps and return X and Y matrix\ndef create_dataset(dataset, look_back):\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n        a = dataset[i:(i+look_back), :]\n        dataX.append(a)\n        dataY.append(dataset[i + look_back, (np.shape(dataset)[1]-1)])\n    return np.array(dataX), np.array(dataY)","001d1fa6":"# fix random seed for reproducibility\nnp.random.seed(1)","22257212":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(150, activation=tf.nn.tanh)) #compared with relu, tanh is better because the normalization\n#model.add(tf.keras.layers.Dense(100, activation=tf.nn.tanh))\nmodel.add(tf.keras.layers.Dense(1, activation=tf.nn.tanh))\n# find out dropout session is bad\n# three tanh: 12.71 RMSE\n# relu, two tanh: 17.58 RMSE\n# three layer: 12.71 RMSE\n# two layer: 11.96 RMSE\n# unit=50: 12.65 RMSE\n# unit=150: 11.02 RMSE\n# unit=200: 11.42 RMSE\n\nmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\nmodel.fit(train[:1784,:], train[1:1785,-1], epochs=150)","56f99881":"# predicting\ntrainPredict = model.predict(train[:1784,:])\ntestPredict = model.predict(test[:879,:])\n\n# scale the data for later invert predictions\nb = aapl_daily.iloc[:, -1].values\nb = np.reshape(b, (len(b),1))\nsc = MinMaxScaler(feature_range = (0, 1))\nb = sc.fit_transform(b) \n\n# invert predictions\ntrainPredict = sc.inverse_transform(np.reshape(trainPredict, (np.shape(trainPredict)[0], 1)))\ntrainY = aapl_daily.iloc[1:1785, -1].values\ntestPredict = sc.inverse_transform(np.reshape(testPredict, (np.shape(testPredict)[0], 1)))\ntestY = aapl_daily.iloc[1786:2665, -1].values\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY, trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY, testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","bc6074f0":"y_test = aapl_daily.iloc[1785:len(aapl_daily)-1, -1].values\nplt.plot(y_test, color = 'black', label = 'AAPL Stock Price')\nplt.plot(testPredict, color = 'green', label = 'Predicted AAPL Stock Price')\nplt.title('AAPL Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('AAPL Stock Price')\nplt.legend()\nplt.show()","28214820":"# reshape into X=t and Y=t+1\nlook_back = 15\nX_train, y_train = create_dataset(train, look_back)\nX_test, y_test = create_dataset(test, look_back)\n# windon=3: 10.86 RMSE\n# windon=5: 21.38 RMSE\n# windon=10: 10.73 RMSE\n# windon=15: 9.69 RMSE\n# windon=20: 67.52 RMSE\n# windon=30: 68.00 RMSE\n\n# reshape input to be [samples, features]\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[2]*look_back))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[2]*look_back))","f0906760":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(40, activation=tf.nn.relu))\nmodel.add(tf.keras.layers.Dense(1, activation=tf.nn.relu))\n\n# two layer, unit=50: 5.53 RMSE\n# three layer, unit=50: 14.36 RMSE\n# two layer, unit=30: 67.52 RMSE\n# two layer, unit=70: 4.46 RMSE\n\nmodel.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\nmodel.fit(X_train, y_train, epochs=150)","892ff059":"# predicting\ntrainPredict = model.predict(X_train)\ntestPredict = model.predict(X_test)\n\n# invert predictions\ntrainPredict = sc.inverse_transform(np.reshape(trainPredict, (np.shape(trainPredict)[0], 1)))\ntrainY = aapl_daily.iloc[look_back+1:1785, -1].values\ntestPredict = sc.inverse_transform(np.reshape(testPredict, (np.shape(testPredict)[0], 1)))\ntestY = aapl_daily.iloc[1785+look_back+1:2665, -1].values\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY, trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY, testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","13b8d280":"plt.plot(testY, color = 'black', label = 'AAPL Stock Price')\nplt.plot(testPredict, color = 'green', label = 'Predicted AAPL Stock Price')\nplt.title('AAPL Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('AAPL Stock Price')\nplt.legend()\nplt.show()","88bb8c88":"# reshape into X=t and Y=t+1\nlook_back = 1\nX_train, y_train = create_dataset(train, look_back)\nX_test, y_test = create_dataset(test, look_back)\n\n# reshape input to be [samples, time steps, features], because LSTM expects the input data to be this shape\nX_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[2]))\nX_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[2]))","355d446b":"regressor = tf.keras.Sequential([\n    tf.keras.layers.LSTM(units = 50, return_sequences = True, input_shape = (look_back, 11)),\n    tf.keras.layers.Dense(units = 1)\n])\n\n# have dropout: 9.80 RMSE, two LSTM and one Dense\n# no dropout: 5.20 RMSE, two LSTM and one Dense\n# one LSTM and one Dense: 2.28 RMSE and convergence quickly\n# unit=40: 2.67 RMSE\n# unit=45: 2.19 RMSE\n# unit=50: 2.28 RMSE\n# unit=55: 2.40 RMSE\n# unit=60: 2.45 RMSE\n\n# batch=10: 4.82 RMSE\n# batch=10: 3.56 RMSE\n# batch=30: 4.85 RMSE\n\nregressor.summary()\n\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\nregressor.fit(X_train, y_train, epochs = 70, batch_size = 10)","b9713257":"# make predictions\ntrainPredict = regressor.predict(X_train)\ntestPredict = regressor.predict(X_test)\n\n# invert predictions\ntrainPredict = sc.inverse_transform(np.reshape(trainPredict, (np.shape(trainPredict)[0], 1)))\ntrainY = sc.inverse_transform(np.reshape(y_train, (np.shape(trainPredict)[0], 1)))\ntestPredict = sc.inverse_transform(np.reshape(testPredict, (np.shape(testPredict)[0], 1)))\ntestY = sc.inverse_transform(np.reshape(y_test, (np.shape(testPredict)[0], 1)))\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY, trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY, testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","5658a756":"plt.plot(testY, color = 'black', label = 'AAPL Stock Price')\nplt.plot(testPredict, color = 'green', label = 'Predicted AAPL Stock Price')\nplt.title('AAPL Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('AAPL Stock Price')\nplt.legend()\nplt.show()","35f6e9bb":"# fix random seed for reproducibility\nnp.random.seed(1)\n\n# reshape into X=t and Y=t+1\nlook_back = 2\nX_train, y_train = create_dataset(train, look_back)\nX_test, y_test = create_dataset(test, look_back)\n\n# reshape input to be [samples, time steps, features], because LSTM expects the input data to be this shape\nX_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[2]*look_back))\nX_test = np.reshape(X_test, (X_test.shape[0], 1, X_test.shape[2]*look_back))","e75c9f84":"regressor = tf.keras.Sequential([\n    tf.keras.layers.LSTM(units = 55, return_sequences = True, input_shape = (X_train.shape[1], X_train.shape[2])),\n    #tf.keras.layers.Dropout(0.05),\n    \n    #tf.keras.layers.LSTM(units = 50, return_sequences = True),\n    #tf.keras.layers.Dropout(0.2),\n    \n    #tf.keras.layers.LSTM(units = 50, return_sequences = True),\n    #tf.keras.layers.Dropout(0.2),\n    \n    #tf.keras.layers.LSTM(units = 50),\n    #tf.keras.layers.Dropout(0.2),\n    \n    tf.keras.layers.Dense(units = 1)\n])\n\n# two layer, unit=50, batch=30: 12.58 RMSE\n# two layer, unit=45, batch=30: 14.84 RMSE\n# look_back=20: 12.50 RMSE\n# look_back=15: 13.52 RMSE\n# look_back=10: 9.79 RMSE\n# look_back=5: 10.39 RMSE\n# look_back=3: 6.51 RMSE\n# look_back=2: 2.84 RMSE\n# unit=55: 3.68 RMSE\n# unit=45: 3.34 RMSE\nregressor.summary()\n\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\nregressor.fit(X_train, y_train, epochs = 100, batch_size = 30)","028bc9db":"# make predictions\ntrainPredict = regressor.predict(X_train)\ntestPredict = regressor.predict(X_test)\n\n# invert predictions\ntrainPredict = sc.inverse_transform(np.reshape(trainPredict, (np.shape(trainPredict)[0], 1)))\ntrainY = sc.inverse_transform(np.reshape(y_train, (np.shape(trainPredict)[0], 1)))\ntestPredict = sc.inverse_transform(np.reshape(testPredict, (np.shape(testPredict)[0], 1)))\ntestY = sc.inverse_transform(np.reshape(y_test, (np.shape(testPredict)[0], 1)))\n\n# calculate root mean squared error\ntrainScore = math.sqrt(mean_squared_error(trainY, trainPredict[:,0]))\nprint('Train Score: %.2f RMSE' % (trainScore))\ntestScore = math.sqrt(mean_squared_error(testY, testPredict[:,0]))\nprint('Test Score: %.2f RMSE' % (testScore))","759d454b":"plt.plot(testY, color = 'black', label = 'AAPL Stock Price')\nplt.plot(testPredict, color = 'green', label = 'Predicted AAPL Stock Price')\nplt.title('AAPL Stock Price Prediction')\nplt.xlabel('Time')\nplt.ylabel('AAPL Stock Price')\nplt.legend()\nplt.show()","cbd8966c":"df2 = pd.DataFrame(np.array([[0.48, 10.50], [0.52, 4.95], [0.39, 3.93], [0.44, 3.90]]),\n                   columns=['Train RMSE', 'Test RMSE'])\ndf2","bc61aaa3":"Heaton, J. B., Polson, N. G., & Witte, J. H. (2017). Deep learning for finance: deep portfolios. Applied Stochastic Models in Business and Industry, 33(1), 3-12.","7710fedb":"#### 3.3 Data Merging\nWe delete some columes having zero value. The shape of final dataset is (2664, 11), excluding column 'Date'.","60c75699":"#### 3.2 Data Description","7df6d256":"For the model evaluation, we use Root mean square error (RMSE) as measure. Although we have training RMSE, we will focus on test RMSE becasue our problem is predicting future price. The lower test RMSE, the preciser our prediction is. In addition, each model is assumed to be their best moment because we might choose the optimal tuning parameter. Therefore, we hope that each test RMSE reaches its lowest so can represents model's full capacity. In the sense of that, we can fairly rank the four models without any other unconsider factors.\n<br>\nCompared the predicting performance of MLP regression, MLP with window, LSTM regression and LSTM with window, the results are:\n1. Two LSTM models outperform two MLP models: two LSTM mdoels only have 3.93 and 3.90 average errors, much lower than MLPs. \n2. The MLP with window outperforms MLP without window: It makes sense because of having more information at each prediction step. \n3. Having window don't provide much improvement to LSTM.\n<br>\n<br>\nEvaluating different models:\n* MLP: although its average performance is bad among four models, it still has its own advantages. MLP models the price well during the normal market but has a lagging effect in the bull market or when the volatility increases. Besides, MLP model converges quickly in few epoches.\n* MLP with window: by taking additional information to predict, MLP perform well during a volatile market, but it sacrifice a little convergence speed. The model is very unstable.\n* LSTM: this model did show the property of remembering the long-term dependence. During the increasing or decreasing trend, it models well. During the flunctuation, the prediction has error. It is acceptable and we are likely believe that this model is likely optimal.\n* LSTM with window: by having window, the method seems to model the fluctuation a bit well than LSTM, but the improvement is a little.","ab48094e":"#### 4.2 MLP Using the Window Method\nHere, we introduce the window method. We phrase the problem so that multiple, recent time steps can be used to make the prediction for the next time step. The choice of window is another tuning parameter. We found out 15-window is the best. When we have a larger window, we can decrease the number of units in the hidden layer. This is why our best tuning parameters become Unit=40 and 15-window. When we have more information during prediction, we can have a simpler model. Besides, MLP using window method's prediction is not stable. The model takes time to converge.","447cd856":"### 6. Conclusion","cb2e982c":"In this project, our task is predicting the future price of AAPL. We have try four different models, MLP and LSTM with or without window method. From the result of our numerical evaluation, we can have three conclusion:\n1. LSTM model generally outperforms MLP model\n2. The window method greatly enhance the prediction ability of MLP\n3. The window method didn't benefit LSTM\n\n#### critical thinking:\nThe biggest assumption behind our numerical evaluation might not hold. The assumption is that the RMSE fully excludes the effect of having not optimal tuning parameter and our data and target selection, and that it just represents the prediction ability of the model.\n\n#### For future research, we could\n1. include some non-numerical data, such as the headlines or comment from investors in some website. From these txt information, we are easy to find out whether there is emotional bias for this stock's investors and the extent of their emotional bias. Through, we can predict the price more accurate. \n2. try to input more features, such as our 90 different technical indicators. Only through this, we could explore the full potential of deep learning: handling massive dataset.\n3. We have been notice the majority of fundamental and macroecomomic information are monthly collected. By having them, we might have a bigger picture about how price is determined. What is the method of incorprating monthly data in our daily data?","cbe023b1":"#### Critical thinking for numerical evaluation:\n1. For tuning parameter, we didn't try all the combination and we might just have a suboptimal model. This RMSE can't be used to compare. Because, the suboptimal tuning parameters might cause part of RMSE. It makes no sense to compare RMSEs of a optimal model and a suboptimal model.\n2. During fitting, we found that MLP with window is not stable. Under the same tuning parameter, sometimes it gives us a low test RMSE, sometimes it gives us a huge test RMSE. Thus, value of RMSE might not represent the model's average prediction ability.\n3. From other literatures, we found out MLP outperforms LSTM for index prediction (Raj, 2018). It might be the result of different prediction targets or different tuning parameter or many other reasons.\n4. The time frame of data also might affect the RMSE result.\n<br>\n\nThese all might affect our numerical evaluation's conclusion.","862b8ff8":"* #### Technical indicators: \nThe technical indicators are calculated from the price information with the python package 'ta'. The package can incorporate many different indicators, but first we will try four indicators (OBV, ATR, MACD, RSI). They represent  volume, volatility, trend and momentum. Later, we will use all the indicators and find out whether there is a worthy improvement for our prediction.","041b7650":"#### 4.1 MLP Regression\nWe can phrase the problem as a regression problem. Given each day of observation, forecast the next day's adjusted close price. The optimal tuning parameters is two dense layer, unit = 150. Besides, MLP regression model converges fast within few epoches.\n","e707c3fc":"Looking at the prediction graph, our prediction closely follows the real AAPL price. Especially, during the recent years, the predicted price increase significantly as the real price.","b3a1c6d3":"#### 3.5 Data Split into Training and Test Sets: 0.67 \/ 0.33","98c3e1f3":"#### 3.4 Feature Scaling: Normalization\nIn deep learning models, we have to scale our data for optimal performance. We use Scikit- Learn\u2019s MinMaxScaler to scale our data between zero and one.","0f7fffec":"$\\;\\;\\;\\;\\;$ The graph below shows the time series plot of AAPL adjusted close price. The adjusted close price is our prediction target because it considers price $\\;\\;\\;\\;\\;$ split and dividend on the basis of close price. Its trend is not linear and more like exponential. The flunctuation becomes larger as time going.","87dd8a60":"#### 3.6 Creating Data with Timesteps","737637c7":"### 7. Reference list","d42993a6":"### 5. Numerical Evaluation","e654da32":"* #### Macroeconomic data: \nWe choose LIBOR 3 months and Traded weight USDollar index as our macroeconomic data because these two are only daily data among basically monthly macroeconomic information. Apple Inc. as a big international company in the America. It will affected by American interest rate and exchange rate. These two data are the representation of these two factors. The time frame is from 2010-04-26 to 2021-04-19, totally 2866 observations.\n<br>\nThe three months LIBOR increased significantly from 2015 to 2017, then decreased a lot. The traded weight USDollar index increases as time but has a large flunctuation.\n","9802d154":"### 1. Introduction:\nThe task of predicting future stock price is significant and of pratical and theoretical interest. Through the accurate stock price forecast, academia can gain understanding of dynamics of stock price movement and investors can maximize their return. The task has been studied for a long time and is challenging. The complexity of forecast derives from the volatility of stock price, which can be explained by a large scale of different available factors, across price, technical, fundamental and macroeconomic information. The idea behind is that the price is actually the combined result of multiple market participants' choice and these parties make decision by wathcing available clues from different asepects (Heaton, Polson and Witte, 2016).\n<br>\nThe traditional methods, such as ARIMA and GARCH models, has been proposed to solve the time series problem, but they are effective only when the series is stationary. Besides, these models assume a linear relationship between dependent and independent variables. Stationality and linear relationship are not clearly shown in the real data. Therefore, Neural network is introduced and this new idea has several strengths, such as the capacity of dealing with large data, capturing non-linear dependence and no assumption needed.\n<br>\nIn this paper, we present two models, MLP and LSTM, to predict the future stock price of AAPL. The article can be divided into five parts, namely, methodology, data preparation, implementation, evaluation and conclusion.\n<br>\nThrough comparing RMSE, we find out that the prediction ability of LSTM is better than MLP; The window method can increase the accuracy of MLP but not LSTM. However, these findings base a assumption that RMSE can represent and only represent the prediction ability of model, without any effect from the choice of tuning parameters, target, and time frame.\n","f25f3363":"#### 3.1 Data loading and cleaning: including replacing missing value and converting data type","b95aa526":"$\\;\\;\\;\\;\\;$ We also have other related data, inflation index and tech pulse index, but they are monthly data. The tech pulse index is an index of coincident indicators $\\;\\;\\;\\;\\;$ of activity in the U.S. information technology sector. It tracks the health of the tech sector in a timely manner. Because they are monthly data, we don't $\\;\\;\\;\\;\\;$ include them in our dataset first.\n<br>\n$\\;\\;\\;\\;\\;$ Both inflation and value of USDollar are increasing.","ba5f26bd":"### 2. Methodology\nAlthough the stock price prediction is the time series problem, MLP still can be used and perform well according to many literatures. LSTM is a specific type of recurrent neural network (RNN) which is suitable when the data is sequential. The key idea of RNN is parameter sharing, which increases the efficiency of training and might be an advantage over MLP.\n#### 2.1 Multilayer Perceptron\nMultilayer Perceptron (MLP) is one of primary artificial neural networks and it is a network of fully connected neurons between adjoining layers. One MLP typically has one input layer, several hidden layers and one output layer; each neuron of hidden layer is densely connected to all neurons of previous layer and next layer. MLP tries to use this structure to capture the function mapping from input vectors to output vectors. This structure is more flexible and is able to model much more kinds of relationship between input and output than traditional time series models. The linear traditional models can be represented by just a input layer and output layer without any hidden layers, but with hidden layers, MLP can also represent other functions, like inner function. In terms of the connection between two neurons, it represents a weight addition and sum.\n<br>\nIn detail, from the inputer layer to the first hidden layer: $X$ is the input and $H_{1}$ is the first hidden layer; the formula is $H_{1} = f(W_{1}X + B_{1})$, where $W_{1}$ is weight parameters and $B_{1}$ is bias paramters, and the function $f$ is non-linear activation function. From hidden layer to another hidden layer, the formula is $H_{i+1} = a(W_{i}H_{i} + B_{i})$, it follows transformation with different weight and bias parameters. From the last hidden layer to the output layer, $Y$ represents the output and $H_{L}$ is the result from the last hidden layer, the formula is $Y = g(W_{L}H_{L} + B_{L}) $, where $W_{L}$ is the weight parameters and $B_{L}$ is bias parameters of the output layer. \n<br>\nThe small change in weight and bias parameters will have different outputs. Backpropagation algorithm is the method to train MLP and find the weight and bias, which can obtain the real ouput.\n#### 2.2 Long Short Term Memory\nLong Short Term Memory (LSTM) derives from RNN and it usually has a better performance than simple RNN. Upon the structure of simple RNN, LSTM incorporates memory cells, solves the problem of vanishing gradient problem and is easier to learn the long-term dependencies. Simple recurrent neural networks only has one single neural cell and also one recurrent layer connecting to itself. The structure represents a repeated and exactly same transformation happening to the information flow when changing from a cell state to a next cell state. Upon simple RNN, LSTM introduces the memory cell and the gate machanism. The role of gate is to modify the information flowing to next cell state. Each gate can be expressed mathematically, $\\sigma (W_{i}X + b_{i}) $. The output from sigmoid function is between 0 and 1 and it represents the percentage of $C_{t-1}$ should be passed to next state. In LSTM, there are three kinds of gates (forget, input and output) and thus three more layers are added. \n<br>\nForget Gate Layer is the first step of LSTM and it decides how much of $C_{t-1}$ should be forgotten. The formula is $ f_{t} = \\sigma (U_{f}X_{t} + W_{f}h_{t-1} + b_{f})$. $X_{t}$ is the new information inputted at time $t$; $h_{t-1}$ is the output from last hidden layer; $U_{f}$, $W_{f}$ and $b_{f}$ are parameters. $f_{t}$ is the output. If $f_{t} = 0$, the layer discards all information; If $f_{t} = 1$, the layer passes all information $C_{t-1}$.\n<br>\nInput Gate Layer is the second step and decides how much of new information should be stored in memory cell. The first and second formulas are $ i_{t} = \\sigma (U_{i}X_{t} + W_{i}h_{t-1} + b_{i}) $ and $ \\hat{C_{t}} = tanh(U_{c}X_{t} + W_{c}h_{t-1} + b_{c})$. $i_{t}$ is the percentage of new information $\\hat{C_{t}}$ is added into state. The third formula is $ C_{t} = f_{t}*C_{t-1} + i_{t}*\\hat{C_{t}}$. $C_{t}$ is the final cell information to pass through the next state.\n<br>\nOutput Gate Layer is the final step and calculates the output. The formula are $ O_{t} = \\sigma (U_{O}X_{t} + W_{O}h_{t-1} + b_{O}) $ and $ h_{t} = O_{t}*tanh(C_{t})$\n","30c2f4ae":"### Abstract: \nStock price prediction has been popular in both academic and financial fields. The forecast is complex because the market is highly volatile and the price is affected by multiple factors in multiple ways (linear or non-linear). With the advancement of computer science, neural networks are introduced as a revolutionary method. Compared to traditional models, it is more flexible and powerful. Here, we apply two kinds of neural networks, Multilayer Perceptron (MLP) and Long Short Term Memory (LSTM), to predict the stock price.","eec0e132":"### 4. Implementation\nWe will introduce totally four methods, MLP regression, MLP using window, LSTM regression, and LSTM using window. For each method, we will do the parameter tunning experiment and try finding the best parameter combination.\n<br>\nFrom the parameter tunning experiment, we find out:\n1. tanh activation function is the best when we have our data scaled by normalizing them into (0,1).\n2. When the model becomes more complex with more hidden layers or more units, it has very bad performance due to the overfiting.\n3. When the number of features is not large, dropout layer might not be recommendated.","abbe6583":"* #### Price data: \nDaily price data is downloaded from Yahoo. It contains open, high, low, volume, close and adjusted close. It has 2769 observations from 2010-04-26 to 2021-04-23 and 7 columns.","af130325":"### 3. Date preparation\nOur target stock is AAPL and the data collected comes from five different aspects, price, techinical, fundamental and macroeconomic information. When market participants make investment decision of buying or selling AAPL stock, they would consider its current price and available techinical indicators and study balance sheet of AAPL, eg inventory of Apple product and its sale. Besides, some also analyze the current suitation of American economy and consumer electronics industry Apple Inc. is in. These all factors affect behavior of investors which combinedly determine the future price.\n<br>\nThe dataset contains 11 years' data, from 2010-05 to 2021-04. Some of them are daily, others are monthly. Parts of macroeconomic and fundamental information is collected monthly.","ab4bbf7b":"# Project Title: Stock Price Prediction Using Neural Networks","06f7d0db":"$\\;\\;\\;\\;\\;$ The adjusted close price has an increasing flunctuation. Looking at the first graph, the rolling volatilty becomes larger and larger as time going. Looking at $\\;\\;\\;\\;\\;$ the second graph, it shows the high trend and low trend. It is clear that the two lines are more separable as time going.","122155ea":"#### 4.3 LSTM Network for Regression\nThe best tuning parameters are one hidden layer with unit=50. This model takes time to converge.","151de83b":"Raj, R. A. Stock Market Index Prediction Using Multilayer Perceptron and Long Short Term Memory Networks: A Case Study on BSE Sensex.","8dfae70e":"Gao, Penglei; Zhang, Rui; Yang, Xi. 2020. \"The Application of Stock Index Price Prediction with Neural Network\" Math. Comput. Appl. 25, no. 3: 53. https:\/\/doi.org\/10.3390\/mca25030053","6f91a7db":"#### 4.4 LSTM Network for Regression Using the Window Method\nFrom this model, we find out LSTM model don't need a window mthod. By having only one window, the prediction is the best."}}