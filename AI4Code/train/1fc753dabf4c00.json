{"cell_type":{"25c68ea8":"code","214fc614":"code","f88fa659":"code","c00ec861":"code","26c2cee1":"code","2e2fa941":"code","4da23374":"code","42e222cd":"code","1c68bdc2":"code","5d2b62e6":"code","05c9d638":"code","8065331d":"code","6a0b704c":"code","70b84ffc":"code","b0c5b1cc":"code","0ada7060":"code","d2826f0f":"code","316bd189":"code","33f5aa10":"code","1b11327c":"code","5a87b6e8":"code","91f1e050":"code","50769bc0":"code","8b1ce885":"code","521922cb":"code","d3f6b962":"code","dad15fa4":"code","22a88afd":"code","ac1af62d":"code","0f957549":"code","fb6b3454":"code","752850a3":"code","3e66fb18":"code","312a121c":"code","e0598096":"code","ad113ea9":"code","e8402387":"code","6b4c6974":"code","7e44a438":"code","3ecaf66c":"code","9432244a":"code","35f7c8b6":"code","27c93f7f":"code","d26f5a8f":"code","5019d191":"code","5b7c38c1":"code","1075a5eb":"code","529dc5c7":"code","ec7708d4":"code","3f8b0b77":"code","6761d9e2":"code","5ceac909":"code","ba2ab5b9":"code","7116c66a":"code","bf9b4a7e":"code","23ac1e6a":"code","2a8cca19":"code","f5166774":"code","2639f6ef":"markdown","624f2d1d":"markdown","5bb7bc88":"markdown","639fc40c":"markdown","178bc24e":"markdown","a67d2448":"markdown","ae3643fa":"markdown","f843da8f":"markdown","031e15b6":"markdown","f901451c":"markdown","94097899":"markdown","0fa10856":"markdown","07b06fe6":"markdown","63b2a3a2":"markdown","b4f53192":"markdown","98329a39":"markdown","6c75e496":"markdown","7b0f659b":"markdown","215c4238":"markdown","afbd9ecb":"markdown","d5a7d170":"markdown","87990f93":"markdown","e3ff1d83":"markdown","ebafe8a1":"markdown","afe89039":"markdown","964477d9":"markdown","436e59ca":"markdown","964d4409":"markdown","fd6d70ca":"markdown","3c1303db":"markdown","568c7075":"markdown","a99386c1":"markdown"},"source":{"25c68ea8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nfrom scipy.stats.mstats import winsorize\n\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\nfrom sklearn.model_selection import GridSearchCV\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport time","214fc614":"df = pd.read_csv('\/kaggle\/input\/customer-analytics\/Train.csv')\n\ndf.head()","f88fa659":"df.describe(include='all')","c00ec861":"df.info()","26c2cee1":"df.drop('ID', axis=1, inplace=True)\ndf.rename({'Reached.on.Time_Y.N':'Reached_on_Time'}, axis=1, inplace=True)\ndf['Reached_on_Time'].replace({1:'No', 0:'Yes'}, inplace=True)","2e2fa941":"print(\"Percentage of Null Values:\\n\")\nprint(df.isna().sum()*100\/df.shape[0])","4da23374":"print(\"# of Unique Values: \\n\")\nprint(df.nunique())","42e222cd":"print(\"Unique Values:\\n\")\nfor i in range(len(df.nunique())):\n    if df.nunique()[i] < 10:\n        print(\"- \", df.nunique().index[i], \": \", sorted(df.iloc[:, i].unique()), sep='')","1c68bdc2":"print(\"Columns:\")\nfor column in df.columns:\n    print(\"- {}\".format(column))","5d2b62e6":"plt.figure(figsize=(18, 18))\n\nplt.subplot(3, 3, 1)\nsns.countplot(x='Warehouse_block', data=df)\nplt.title('Warehouse Block', fontsize=15)\n\nplt.subplot(3, 3, 2)\nsns.countplot(x='Mode_of_Shipment', data=df)\nplt.title('Mode of Shipment', fontsize=15)\n\nplt.subplot(3, 3, 3)\nsns.countplot(x='Customer_care_calls', data=df)\nplt.title('Customer Care Calls', fontsize=15)\n\nplt.subplot(3, 3, 4)\nsns.countplot(x='Customer_rating', data=df)\nplt.title('Customer Rating', fontsize=15)\n\nplt.subplot(3, 3, 5)\nsns.countplot(x='Prior_purchases', data=df)\nplt.title('Prior Purchases', fontsize=15)\n\nplt.subplot(3, 3, 6)\nsns.countplot(x='Product_importance', data=df)\nplt.title('Product Importance', fontsize=15)\n\nplt.subplot(3, 3, 7)\nsns.countplot(x='Gender', data=df)\nplt.title('Gender', fontsize=15)\n\nplt.show()","05c9d638":"plt.figure(figsize=(18, 18))\n\nplt.subplot(3, 3, 1)\nsns.countplot(x='Warehouse_block', hue='Reached_on_Time', data=df)\nplt.title('Warehouse Block', fontsize=15)\n\nplt.subplot(3, 3, 2)\nsns.countplot(x='Mode_of_Shipment', hue='Reached_on_Time', data=df)\nplt.title('Mode of Shipment', fontsize=15)\n\nplt.subplot(3, 3, 3)\nsns.countplot(x='Customer_care_calls', hue='Reached_on_Time',  data=df)\nplt.title('Customer Care Calls', fontsize=15)\n\nplt.subplot(3, 3, 4)\nsns.countplot(x='Customer_rating', hue='Reached_on_Time',  data=df)\nplt.title('Customer Rating', fontsize=15)\n\nplt.subplot(3, 3, 5)\nsns.countplot(x='Prior_purchases', hue='Reached_on_Time',  data=df)\nplt.title('Prior Purchases', fontsize=15)\n\nplt.subplot(3, 3, 6)\nsns.countplot(x='Product_importance', hue='Reached_on_Time',  data=df)\nplt.title('Product Importance', fontsize=15)\n\nplt.subplot(3, 3, 7)\nsns.countplot(x='Gender', hue='Reached_on_Time',  data=df)\nplt.title('Gender', fontsize=15)\n\nplt.show()","8065331d":"plt.figure(figsize=(12, 8))\n\nplt.subplot(2, 3, 1)\nplt.hist(df['Cost_of_the_Product'], bins=20)\nplt.title('Cost of the Product')\n\nplt.subplot(2, 3, 2)\nplt.hist(df['Discount_offered'], bins=20)\nplt.title('Discount Offered')\n\nplt.subplot(2, 3, 3)\nplt.hist(df['Weight_in_gms'], bins=20)\nplt.title('Weight in gms')\n\nplt.subplot(2, 3, 4)\nplt.boxplot(df['Cost_of_the_Product'])\nplt.title('Cost of the Product')\n\nplt.subplot(2, 3, 5)\nplt.boxplot(df['Discount_offered'])\nplt.title('Discount Offered')\n\nplt.subplot(2, 3, 6)\nplt.boxplot(df['Weight_in_gms'])\nplt.title('Weight in gms')\n\nplt.show()","6a0b704c":"plt.figure(figsize=(18, 12))\n\nplt.subplot(2, 3, 1)\nplt.scatter(df['Cost_of_the_Product'], df['Reached_on_Time'], s=5)\nplt.title(\"Cost of the Product vs Reached on Time\", fontsize=15)\n\nplt.subplot(2, 3, 2)\nplt.scatter(df['Discount_offered'], df['Reached_on_Time'], s=5)\nplt.title(\"Discount offered vs Reached on Time\", fontsize=15)\n\nplt.subplot(2, 3, 3)\nplt.scatter(df['Weight_in_gms'], df['Reached_on_Time'], s=5)\nplt.title(\"Weight in gms vs Reached On Time\", fontsize=15)\n\nplt.subplot(2, 3, 4)\nsns.violinplot(x='Reached_on_Time', y='Cost_of_the_Product', data=df)\nplt.title(\"Cost of the Product vs Reached on Time\", fontsize=15)\n\nplt.subplot(2, 3, 5)\nsns.violinplot(x='Reached_on_Time', y='Discount_offered', data=df)\nplt.title(\"Discount offered vs Reached on Time\", fontsize=15)\n\nplt.subplot(2, 3, 6)\nsns.violinplot(x='Reached_on_Time', y='Weight_in_gms', data=df)\nplt.title(\"Weight in gms vs Reached On Time\", fontsize=15)\n\nplt.show()","70b84ffc":"plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nsns.countplot(x='Reached_on_Time', data=df)\nplt.title('Reached on Time', fontsize=15)\n\nplt.subplot(1, 2, 2)\nplt.pie(df['Reached_on_Time'].value_counts(), labels=['No', 'Yes'], explode=[0.05, 0.05], autopct='%1.2f%%', shadow=True)\nplt.title('Reached on Time', fontsize=15)\n\nplt.show()","b0c5b1cc":"def sum_outliers(X):\n    \"\"\"Outliers are calculated according to the matplotlib.pyplot's standards.\"\"\"\n    IQR = np.quantile(X, q=0.75) - np.quantile(X, q=0.25)\n    upper_whisker = np.quantile(X, q=0.75) + (IQR * 1.5)\n    lower_whisker = np.quantile(X, q=0.25) - (IQR * 1.5)\n    return (X > upper_whisker).sum() + (X < lower_whisker).sum()","0ada7060":"plt.figure(figsize=(24, 12))\n\nplt.subplot(2, 4, 1)\nplt.boxplot(df['Discount_offered'])\nplt.title('Discount Offered')\n\nplt.subplot(2, 4, 2)\nplt.boxplot(np.log(df['Discount_offered']))\nplt.title('Discount Offered (Log Transformation)')\n\nplt.subplot(2, 4, 3)\nplt.boxplot(np.sqrt(df['Discount_offered']))\nplt.title('Discount Offered (Square Root Transformation)')\n\nplt.subplot(2, 4, 4)\nplt.boxplot(winsorize(np.log(df['Discount_offered']),limits=(0.15, 0.15)));\nplt.title('Discount Offered (Log Transformation & Winsorized)')\n\nplt.subplot(2, 4, 5)\nplt.hist(df['Discount_offered'], bins=20)\nplt.title('Discount Offered')\n\nplt.subplot(2, 4, 6)\nplt.hist(np.log(df['Discount_offered']), bins=20)\nplt.title('Discount Offered (Log Transformation)')\n\nplt.subplot(2, 4, 7)\nplt.hist(np.sqrt(df['Discount_offered']), bins=20)\nplt.title('Discount Offered (Square Root Transformation)')\n\nplt.subplot(2, 4, 8)\nplt.hist(winsorize(np.log(df['Discount_offered']),limits=(0.15, 0.15)), bins=20);\nplt.title('Discount Offered (Log Transformation & Winsorized)')\n\n\nplt.show()","d2826f0f":"print(\"Total number of observations: {}\".format(len(df['Discount_offered'])))\nprint(\"Number of outliers in 'Discount_offered': {}\".format(sum_outliers(df['Discount_offered'])))\nprint(\"Number of outliers in 'Discount_offered' (Log Transformation): {}\".format(sum_outliers(np.log(df['Discount_offered']))))\nprint(\"Number of outliers in 'Discount_offered' (Square Root Transformation): {}\".format(sum_outliers(np.sqrt(df['Discount_offered']))))\nprint(\"Number of outliers in 'Discount_offered' (Log Transformation & Winsorized): {}\".format(sum_outliers(winsorize(np.log(df['Discount_offered']),limits=(0.15, 0.15)))))","316bd189":"df['Discount_offered'] = np.array(winsorize(np.log(df['Discount_offered']),limits=(0.15, 0.15)))","33f5aa10":"plt.figure(figsize=(12, 12))\nsns.heatmap(pd.get_dummies(df, drop_first=True).corr(), annot=True, fmt='.3f')\nplt.show()","1b11327c":"pd.get_dummies(df, drop_first=False).corr()['Reached_on_Time_Yes'].sort_values(ascending=False)","5a87b6e8":"df = pd.get_dummies(df, drop_first=True)","91f1e050":"X = df.drop('Reached_on_Time_Yes', axis=1)\ny = df['Reached_on_Time_Yes']","50769bc0":"normalizer = Normalizer()\nX_normalized = pd.DataFrame(normalizer.fit_transform(df.drop('Reached_on_Time_Yes', axis=1)), columns=df.columns[:-1])","8b1ce885":"scaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(df.drop('Reached_on_Time_Yes', axis=1)), columns=df.columns[:-1])","521922cb":"def fit_predict_score(Model, X_train, y_train, X_test, y_test):\n    \"\"\"Fit the model of your choice, predict for test data, and returns classification metrics.\"\"\"\n    model = Model\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n    return train_score, test_score, precision_score(y_test, y_pred), recall_score(y_test, y_pred), f1_score(y_test, y_pred)\n\ndef model_comparison(X, y):\n    \"\"\"Creates a DataFrame comparing Logistic Regression, K-Nearest Neighbors, Decision Tree,\n    Random Forest, AdaBoost, Gradient Boosting, Extra Trees, CatBoost, Support Vector Machines,\n    XGBoost, and LightGBM.\"\"\"\n    \n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n    \n    lr_train_score, lr_test_score, lr_pr, lr_re, lr_f1 = fit_predict_score(LogisticRegression(), X_train, y_train, X_test, y_test)\n    knn_train_score, knn_test_score, knn_pr, knn_re, knn_f1 = fit_predict_score(KNeighborsClassifier(), X_train, y_train, X_test, y_test)\n    dtc_train_score, dtc_test_score, dtc_pr, dtc_re, dtc_f1 = fit_predict_score(DecisionTreeClassifier(), X_train, y_train, X_test, y_test)\n    rfc_train_score, rfc_test_score, rfc_pr, rfc_re, rfc_f1 = fit_predict_score(RandomForestClassifier(), X_train, y_train, X_test, y_test)\n    ada_train_score, ada_test_score, ada_pr, ada_re, ada_f1 = fit_predict_score(AdaBoostClassifier(), X_train, y_train, X_test, y_test)\n    gbc_train_score, gbc_test_score, gbc_pr, gbc_re, gbc_f1 = fit_predict_score(GradientBoostingClassifier(), X_train, y_train, X_test, y_test)\n    xtc_train_score, xtc_test_score, xtc_pr, xtc_re, xtc_f1 = fit_predict_score(ExtraTreesClassifier(), X_train, y_train, X_test, y_test)\n    cbc_train_score, cbc_test_score, cbc_pr, cbc_re, cbc_f1 = fit_predict_score(CatBoostClassifier(verbose=0), X_train, y_train, X_test, y_test)\n    svc_train_score, svc_test_score, svc_pr, svc_re, svc_f1 = fit_predict_score(SVC(), X_train, y_train, X_test, y_test)\n    xgbc_train_score, xgbc_test_score, xgbc_pr, xgbc_re, xgbc_f1 = fit_predict_score(XGBClassifier(verbosity=0), X_train, y_train, X_test, y_test)\n    lgbc_train_score, lgbc_test_score, lgbc_pr, lgbc_re, lgbc_f1 = fit_predict_score(LGBMClassifier(), X_train, y_train, X_test, y_test)\n    \n    models = ['Logistic Regression', 'K-Nearest Neighbors', 'Decision Tree', 'Random Forest', 'AdaBoost',\n              'Gradient Boosting', 'Extra Trees', 'CatBoost', 'Support Vector Machines', 'XGBoost', 'LightGBM']\n    train_score = [lr_train_score, knn_train_score, dtc_train_score, rfc_train_score, ada_train_score,\n                   gbc_train_score, xtc_train_score, cbc_train_score, svc_train_score, xgbc_train_score, lgbc_train_score]\n    test_score = [lr_test_score, knn_test_score, dtc_test_score, rfc_test_score, ada_test_score,\n                  gbc_test_score, xtc_test_score, cbc_test_score, svc_test_score, xgbc_test_score, lgbc_test_score]\n    precision = [lr_pr, knn_pr, dtc_pr, rfc_pr, ada_pr, gbc_pr, xtc_pr, cbc_pr, svc_pr, xgbc_pr, lgbc_pr]\n    recall = [lr_re, knn_re, dtc_re, rfc_re, ada_re, gbc_re, xtc_re, cbc_re, svc_re, xgbc_re, lgbc_re]\n    f1 = [lr_f1, knn_f1, dtc_f1, rfc_f1, ada_f1, gbc_f1, xtc_f1, cbc_f1, svc_f1, xgbc_f1, lgbc_f1]\n    \n    model_comparison = pd.DataFrame(data=[models, train_score, test_score, precision, recall, f1]).T.rename({0: 'Model',\n                                                                                                             1:'Training Score',\n                                                                                                             2: 'Test Score (Accuracy)',\n                                                                                                             3: 'Precision',\n                                                                                                             4: 'Recall',\n                                                                                                             5: 'F1 Score'\n                                                                                                            }, axis=1)\n    \n    return model_comparison","d3f6b962":"print(\"Default DataFrame:\")\ndisplay(model_comparison(X, y))\nprint('-'*40)\nprint(\"\\nNormalized DataFrame:\")\ndisplay(model_comparison(X_normalized, y))\nprint('-'*40)\nprint(\"\\nStandardized DataFrame:\")\ndisplay(model_comparison(X_scaled, y))","dad15fa4":"start = time.time()\n\nparams = {\"C\": [10 ** x for x in range (-5, 5, 1)],\n          \"penalty\": ['l1', 'l2']}\n\nlr_grid = GridSearchCV(estimator=LogisticRegression(),\n                       param_grid = params,\n                       cv = 5,\n                       verbose = 0)\n\nlr_grid.fit(X, y)\n\nend = time.time()","22a88afd":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) \/ 60, 2)))\nprint(\"Best Parameters : \", lr_grid.best_params_)","ac1af62d":"start = time.time()\n\nparams = {\n    \"n_neighbors\": [1, 3, 5, 10, 15, 30, 50],\n    \"weights\": ['uniform', 'distance'],\n    \"metric\": ['minkowski', 'euclidian', 'manhattan']\n}\n\nknn_grid = GridSearchCV(estimator=KNeighborsClassifier(),\n                        param_grid = params,\n                        cv = 5,\n                        verbose = 0)\n\nknn_grid.fit(X, y)\n\nend = time.time()","0f957549":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) \/ 60, 2)))\nprint(\"Best Parameters : \", knn_grid.best_params_)","fb6b3454":"start = time.time()\n\nparams = {\n    'criterion': ['gini', 'entropy'],\n    'max_depth': [i for i in range(1, 10)],\n    'min_samples_split': [i for i in range(1, 10)],\n    'min_samples_leaf': [i for i in range(1, 5)]\n}\n\ndtc_grid = GridSearchCV(estimator = DecisionTreeClassifier(),\n                        param_grid = params,\n                        cv = 5,\n                        verbose = 0)\n\ndtc_grid.fit(X, y)\n\nend = time.time()","752850a3":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) \/ 60, 2)))\nprint(\"Best Parameters : \", dtc_grid.best_params_)","3e66fb18":"start = time.time()\n\nparams = {\n    'n_estimators': [100, 200, 300, 400, 500],\n    'max_depth' : [4, 5, 6, 7, 8, 10, 15],\n    'criterion' :['gini', 'entropy']\n}\n\nrfc_grid = GridSearchCV(estimator = RandomForestClassifier(),\n                        param_grid = params,\n                        cv = 5,\n                        verbose = 0)\n\nrfc_grid.fit(X, y)\n\nend = time.time()","312a121c":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) \/ 60, 2)))\nprint(\"Best Parameters : \", rfc_grid.best_params_)","e0598096":"start = time.time()\n\nparams = {\n    'n_estimators': [10, 50, 100, 200, 500],\n    'learning_rate': [0.1, 0.3, 0.5, 0.7]\n}\n\nada_grid = GridSearchCV(estimator = AdaBoostClassifier(),\n                        param_grid = params,\n                        cv = 5,\n                        verbose = 0)\n\nada_grid.fit(X, y)\n\nend = time.time()","ad113ea9":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) \/ 60, 2)))\nprint(\"Best Parameters : \", ada_grid.best_params_)","e8402387":"start = time.time()\n\nparams = {\n    'learning_rate': [0.1, 0.3, 0.5, 0.8, 1],\n    'max_depth': [1, 3, 5, 7, 10, 15, 25],\n    'subsample': [0.1, 0.3, 0.5, 0.8, 1],\n    'n_estimators' : [50, 100, 250, 500]\n}\n\ngbc_grid = GridSearchCV(estimator = GradientBoostingClassifier(),\n                        param_grid = params,\n                        cv = 3,\n                        verbose = 0)\n\ngbc_grid.fit(X, y)\n\nend = time.time()","6b4c6974":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) \/ 60, 2)))\nprint(\"Best Parameters : \", gbc_grid.best_params_)","7e44a438":"start = time.time()\n\nparams = {\n    'n_estimators' : [50, 75, 100, 125, 150],\n    'max_depth': [i for i in range(1, 10, 2)],\n    'min_samples_leaf': [i for i in range(1, 10, 2)],\n    'min_samples_split': [i for i in range(1, 10, 2)]\n}\n\nxtc_grid = GridSearchCV(estimator = ExtraTreesClassifier(),\n                        param_grid = params,\n                        cv = 3,\n                        verbose = 0)\n\nxtc_grid.fit(X, y)\n\nend = time.time()","3ecaf66c":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) \/ 60, 2)))\nprint(\"Best Parameters : \", xtc_grid.best_params_)","9432244a":"start = time.time()\n\nparams = {\n    'learning_rate': [0.03, 0.1, 0.5],\n    'depth': [4, 6, 10],\n    'l2_leaf_reg': [1, 3, 5, 7, 9]\n}\n\ncbc_grid = GridSearchCV(estimator = CatBoostClassifier(verbose = 0),\n                        param_grid = params,\n                        cv = 3,\n                        verbose = 0)\n\ncbc_grid.fit(X, y)\n\nend = time.time()","35f7c8b6":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) \/ 60, 2)))\nprint(\"Best Parameters : \", cbc_grid.best_params_)","27c93f7f":"start = time.time()\n\nparams = {'C': [10**i for i in range(1, 2)] + [round(0.1**i,5) for i in range(5)]}\n\nsvc_grid = GridSearchCV(estimator = SVC(),\n                        param_grid = params,                        \n                        cv = 5,\n                        verbose = 0)\n\nsvc_grid.fit(X, y)\n\nend = time.time()","d26f5a8f":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) \/ 60, 2)))\nprint(\"Best Parameters : \", svc_grid.best_params_)","5019d191":"X_sample = X.sample(n=3000, random_state=42)\ny_sample = y[X_sample.index]\n\nstart = time.time()\n\nparams = {\n    'learning_rate': [0.1, 0.3, 0.5],\n    'max_depth': [1, 3, 5],\n    'min_child_weight': [1, 3, 5, 7, 9],\n    'subsample': [0.1, 0.3, 0.5, 0.8, 1],\n    'colsample_bytree': [0.1, 0.3, 0.5],\n    'n_estimators' : [100, 200, 300, 400, 500],\n    'objective': ['reg:squarederror']\n}\n\nxgbc_grid = GridSearchCV(estimator = XGBClassifier(),\n                         param_grid = params,\n                         cv = 3,\n                         verbose = 0)\n\nxgbc_grid.fit(X_sample, y_sample)\n\nend = time.time()","5b7c38c1":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) \/ 60, 2)))\nprint('Best Parameters: ', xgbc_grid.best_params_)","1075a5eb":"start = time.time()\n\nparams = {\n    'learning_rate': [10 ** x for x in range (-5, 5, 1)],\n    'n_estimators': [x * 100 for x in range(1, 11)]\n}\n\nlgbc_grid = GridSearchCV(estimator = LGBMClassifier(),\n                        param_grid = params,                        \n                        cv = 3,\n                        verbose = 0)\n\nlgbc_grid.fit(X_normalized, y)\n\nend = time.time()","529dc5c7":"print(\"GridSearchCV Runtime: {} minutes\".format(round((end - start) \/ 60, 2)))\nprint('Best Parameters: ', lgbc_grid.best_params_)","ec7708d4":"print(\"Best Parameters (Logistic Regression): \", lr_grid.best_params_)\nprint(\"Best Parameters (K-Nearest Neighbors): \", knn_grid.best_params_)\nprint(\"Best Parameters (Decision Tree): \", dtc_grid.best_params_)\nprint(\"Best Parameters (Random Forest): \", rfc_grid.best_params_)\nprint(\"Best Parameters (AdaBoost): \", ada_grid.best_params_)\nprint(\"Best Parameters (Gradient Boosting): \", gbc_grid.best_params_)\nprint(\"Best Parameters (Extra Trees): \", xtc_grid.best_params_)\nprint(\"Best Parameters (CatBoost): \", cbc_grid.best_params_)\nprint(\"Best Parameters (SVC): \", svc_grid.best_params_)\nprint('Best Parameters (XGBoost):', xgbc_grid.best_params_)\nprint('Best Parameters (LightGBM): ', lgbc_grid.best_params_)","3f8b0b77":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nlr_train_score, lr_test_score, lr_pr, lr_re, lr_f1 = fit_predict_score(LogisticRegression(C=1, penalty='l2'), X_train, y_train, X_test, y_test)\nknn_train_score, knn_test_score, knn_pr, knn_re, knn_f1 = fit_predict_score(KNeighborsClassifier(metric='minkowski', n_neighbors=3, weights='uniform'), X_train, y_train, X_test, y_test)\ndtc_train_score, dtc_test_score, dtc_pr, dtc_re, dtc_f1 = fit_predict_score(DecisionTreeClassifier(criterion='gini', max_depth=2, min_samples_leaf=1, min_samples_split=2), X_train, y_train, X_test, y_test)\nrfc_train_score, rfc_test_score, rfc_pr, rfc_re, rfc_f1 = fit_predict_score(RandomForestClassifier(criterion='gini', max_depth=15, n_estimators=100), X_train, y_train, X_test, y_test)\nada_train_score, ada_test_score, ada_pr, ada_re, ada_f1 = fit_predict_score(AdaBoostClassifier(learning_rate=0.1, n_estimators=10), X_train, y_train, X_test, y_test)\ngbc_train_score, gbc_test_score, gbc_pr, gbc_re, gbc_f1 = fit_predict_score(GradientBoostingClassifier(learning_rate=0.8, max_depth=5, n_estimators=500, subsample=0.1), X_train, y_train, X_test, y_test)\nxtc_train_score, xtc_test_score, xtc_pr, xtc_re, xtc_f1 = fit_predict_score(ExtraTreesClassifier(max_depth=1, min_samples_leaf=7, min_samples_split=3, n_estimators=75), X_train, y_train, X_test, y_test)\ncbc_train_score, cbc_test_score, cbc_pr, cbc_re, cbc_f1 = fit_predict_score(CatBoostClassifier(verbose = 0, depth=6, l2_leaf_reg=3, learning_rate=0.5), X_train, y_train, X_test, y_test)\nsvc_train_score, svc_test_score, svc_pr, svc_re, svc_f1 = fit_predict_score(SVC(C=10), X_train, y_train, X_test, y_test)\nxgbc_train_score, xgbc_test_score, xgbc_pr, xgbc_re, xgbc_f1 = fit_predict_score(XGBClassifier(colsample_bytree=0.5, learning_rate=0.1, max_depth=1, min_child_weight=7, n_estimators=100, objective='reg:squarederror', subsample=0.5), X_train, y_train, X_test, y_test)\nlgbc_train_score, lgbc_test_score, lgbc_pr, lgbc_re, lgbc_f1 = fit_predict_score(LGBMClassifier(learning_rate=10000, n_estimators=100), X_train, y_train, X_test, y_test)\n\nmodels = ['Logistic Regression', 'K-Nearest Neighbors', 'Decision Tree', 'Random Forest', 'AdaBoost',\n          'Gradient Boosting', 'Extra Trees', 'CatBoost', 'Support Vector Machines', 'XGBoost', 'LightGBM']\ntrain_score = [lr_train_score, knn_train_score, dtc_train_score, rfc_train_score, ada_train_score,\n               gbc_train_score, xtc_train_score, cbc_train_score, svc_train_score, xgbc_train_score, lgbc_train_score]\ntest_score = [lr_test_score, knn_test_score, dtc_test_score, rfc_test_score, ada_test_score,\n              gbc_test_score, xtc_test_score, cbc_test_score, svc_test_score, xgbc_test_score, lgbc_test_score]\nprecision = [lr_pr, knn_pr, dtc_pr, rfc_pr, ada_pr, gbc_pr, xtc_pr, cbc_pr, svc_pr, xgbc_pr, lgbc_pr]\nrecall = [lr_re, knn_re, dtc_re, rfc_re, ada_re, gbc_re, xtc_re, cbc_re, svc_re, xgbc_re, lgbc_re]\nf1 = [lr_f1, knn_f1, dtc_f1, rfc_f1, ada_f1, gbc_f1, xtc_f1, cbc_f1, svc_f1, xgbc_f1, lgbc_f1]\n\ntuned_models = pd.DataFrame(data=[models, train_score, test_score, precision, recall, f1]).T.rename({0: 'Model',\n                                                                                                     1:'Training Score',\n                                                                                                     2: 'Test Score (Accuracy)',\n                                                                                                     3: 'Precision',\n                                                                                                     4: 'Recall',\n                                                                                                     5: 'F1 Score'\n                                                                                                        }, axis=1)","6761d9e2":"print(\"Default Parameters:\")\ndisplay(model_comparison(X, y))\nprint('-'*40)\nprint(\"\\nTuned Parameters:\")\ndisplay(tuned_models)","5ceac909":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42)\n\nprint(\"Shape of train set (X) :\", X_train.shape)\nprint(\"Shape of train set (y) :\", y_train.shape)\nprint(\"Shape of test set  (X) :\", X_test.shape)\nprint(\"Shape of test set  (y) :\", y_test.shape)\n\ninput_shape = X_train.shape[1]","ba2ab5b9":"model = Sequential()\nmodel.add(Dense(16, activation='relu', input_shape = (input_shape,), name = \"Hidden_Layer_1\"))\nmodel.add(Dense(8, activation='relu', name = \"Hidden_Layer_2\"))\nmodel.add(Dense(4, activation='relu', name = \"Hidden_Layer_3\"))\nmodel.add(Dense(2, activation='relu', name = \"Hidden_Layer_4\"))\nmodel.add(Dense(1, activation='sigmoid', name = \"Output\"))\n\nmodel.summary()","7116c66a":"model.compile(optimizer ='adam',\n              loss='binary_crossentropy', \n              metrics =['accuracy'])","bf9b4a7e":"model.fit(X_train, y_train, epochs=100)","23ac1e6a":"y_pred = model.predict(X_test)\ny_pred = (y_pred>0.5)","2a8cca19":"confusion_matrix(y_test, y_pred)","f5166774":"train_score = model.evaluate(X_train, y_train, verbose = 0)[1]\ntest_score = model.evaluate(X_test, y_test, verbose = 0)[1]\n\nprint(\"Training Score: {:.3f}\".format(train_score))\nprint(\"Test Score (Accuracy): {:.3f}\".format(test_score))\nprint(\"Precision: {:.3f}\".format(precision_score(y_test, y_pred)))\nprint(\"Recall: {:.3f}\".format(recall_score(y_test, y_pred)))\nprint(\"F1 Score: {:.3f}\".format(f1_score(y_test, y_pred)))","2639f6ef":"## Numeric Features","624f2d1d":"# Heatmap","5bb7bc88":"# Classification with Artificial Neural Networks (ANN)","639fc40c":"# Best Parameters & Comparison","178bc24e":"# Data Descrition & Cleaning","a67d2448":"## Target Column","ae3643fa":"## Support Vector Machines","f843da8f":"# E-Commerce Shipping - Classification","031e15b6":"# Outliers","f901451c":"As expected, normalizing or standardization the data did not improve the performance of classification models significantly. I will proceed with the unscaled data.","94097899":"## Gradient Boosting","0fa10856":"## XGBoost","07b06fe6":"## Standardization","63b2a3a2":"## Random Forest","b4f53192":"## Normalization","98329a39":"# Hyperparameter Tuning","6c75e496":"## Logistic Regression","7b0f659b":"## Contents:\n\n- Data Description & Cleaning\n- Exploratory Data Analysis (EDA)\n    * Categorical Features\n    * Numerical Features\n    * Target Column\n- Outliers\n    * Log Transformation\n    * Square Root Transformation\n    * Winsorization\n- Heatmap\n- One-Hot-Encoding\n- Scaling\n    * Normalization\n    * Standardization\n- Building Machine Learning Models\n    * Logistic Regression\n    * KNN\n    * Decision Trees\n    * Random Forest\n    * AdaBoost\n    * Gradient Boosting\n    * Extra Trees\n    * CatBoost\n    * Support Vector Machines\n    * XGBoost\n    * LightGBM\n- Hyperparameter Tuning\n    * Logistic Regression\n    * KNN\n    * Decision Trees\n    * Random Forest\n    * AdaBoost\n    * Gradient Boosting\n    * Extra Trees\n    * CatBoost\n    * Support Vector Machines\n    * XGBoost\n    * LightGBM\n- Best Parameters & Comparison\n- Classification with Artificial Neural Networks (ANNs)","215c4238":"# Scaling","afbd9ecb":"# One-Hot-Encoding","d5a7d170":"## Context\nAn international e-commerce company based wants to discover key insights from their customer database. They want to use some of the most advanced machine learning techniques to study their customers. The company sells electronic products.\n\n## Columns\nThe dataset used for model building contained 10999 observations of 12 variables.\nThe data contains the following information:\n\n- **ID**: ID Number of Customers.\n- **Warehouse block**: The Company have big Warehouse which is divided in to block such as A,B,C,D,E.\n- **Mode of shipment**: The Company Ships the products in multiple way such as Ship, Flight and Road.\n- **Customer care calls**: The number of calls made from enquiry for enquiry of the shipment.\n- **Customer rating**: The company has rated from every customer. 1 is the lowest (Worst), 5 is the highest (Best).\n- **Cost of the product**: Cost of the Product in US Dollars.\n- **Prior purchases**: The Number of Prior Purchase.\n- **Product importance**: The company has categorized the product in the various parameter such as low, medium, high.\n- **Gender**: Male and Female.\n- **Discount offered**: Discount offered on that specific product.\n- **Weight in gms**: It is the weight in grams.\n- **Reached on time**: It is the target variable, where 1 Indicates that the product has NOT reached on time and 0 indicates it has reached on time.\n\n\\\n**Data Source:** https:\/\/www.kaggle.com\/prachi13\/customer-analytics","87990f93":"# Exploratory Data Analysis (EDA)","e3ff1d83":"## Cat Boost","ebafe8a1":"- **Weight**, and **Cost** are _positively_, amount of **Discount** is _negatively_ correlated with the target variable.\n- Overall, there doesn't seem to be the problem of **multicollinearity**.","afe89039":"# Building Machine Learning Models\n\n- Logistic Regression\n- KNN\n- Decision Trees\n- Random Forest\n- AdaBoost\n- Gradient Boosting\n- Extra Trees\n- Cat Boost\n- Support Vector Machines\n- XGBoost\n- LightGBM","964477d9":"## Categorical Features","436e59ca":"## AdaBoost","964d4409":"## K-Nearest Neighbors","fd6d70ca":"## Extra Trees","3c1303db":"According to the boxplots created earlier, the only column with outliers seems to be on \"**Discount_offered**\" column. But there seems to be a lot of outliers in this feature, because of the way boxplot defines outliers. Therefore, instead of directly removing or winsorizing those outliers, I will first apply **Log transformation** and **Square root transformation** to see which works better. Then, I will **winsorize** the remaining outliers.","568c7075":"## LightGBM","a99386c1":"## Decision Tree"}}