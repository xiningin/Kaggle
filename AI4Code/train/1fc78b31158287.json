{"cell_type":{"5b9bf4e1":"code","3ae32a7b":"code","a4e58af9":"code","8a4bdb2a":"code","36f175af":"code","9cc4f5b8":"code","11874929":"code","56cd44bd":"code","1530ae80":"code","5c0a5039":"code","1478a3fa":"code","3038e96f":"code","80a2134b":"code","6d3d29ca":"code","a50f78fa":"code","214befb9":"code","97805529":"code","3d9901eb":"code","70050bc4":"code","49e48e76":"code","7b539ae4":"code","6b3d60e9":"code","50961a07":"code","a0af1468":"markdown","b9ae3036":"markdown","90e43a4c":"markdown","d6f8ecbd":"markdown","ff80f8aa":"markdown","7e49cf07":"markdown","234ccfbe":"markdown","be4f3436":"markdown","2985747d":"markdown","78f64772":"markdown","a169ede2":"markdown","27dff19e":"markdown","c103bc26":"markdown","47cf2684":"markdown","280bb2a1":"markdown","f863783a":"markdown","80ab9ab6":"markdown","f8e8b572":"markdown","640143cf":"markdown","7447511a":"markdown","d3cdf2ed":"markdown","79910b3b":"markdown"},"source":{"5b9bf4e1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","3ae32a7b":"X1=np.array([0,0.4,0.1,-0.2,0.5,1.1,-0.2,-0.1,0.2,0.2,-0.4,-0.2,-0.3,0.2,0,-0.6,-0.8,0.3,-0.2,0])\nX2=np.array([4,6,8,12,-2,-1,7,4,-1,-7,2,-13,-3,-7,-3,-1,-1,-3,-6,5])\n","a4e58af9":"X1=pd.DataFrame(X1,columns=['X1'])\nX2=pd.DataFrame(X2,columns=['X2'])\ntarget=pd.DataFrame(np.ones(20),columns=['Target'])\ndata=pd.concat([X1,X2,target],axis=1)\n","8a4bdb2a":"data['Target'].iloc[10:20]=data['Target'].iloc[10:20]*2","36f175af":"Features=data.iloc[:,[0,1]]","9cc4f5b8":"covariance=Features.cov()\ncovariance","11874929":"inverse=np.linalg.inv(covariance)\ninverse","56cd44bd":"class1=data[data['Target']==1]\nclass2=data[data['Target']==2]","1530ae80":"X1_diff=np.average(class1['X1'])-np.average(class2['X1'])\nX2_diff=np.average(class1['X2'])-np.average(class2['X2'])","5c0a5039":"Mean_diff=np.array([X1_diff,X2_diff])\nMean_diff","1478a3fa":"Grand_means=np.average(Features['X1']),np.average(Features['X2'])","3038e96f":"Grand_means=np.array(Grand_means)","80a2134b":"Grand_means","6d3d29ca":"vector=np.dot(inverse,Mean_diff)","a50f78fa":"Cutoff_point=np.dot(vector,Grand_means)","214befb9":"Cutoff_point","97805529":"Slope=-vector[0]\/vector[1]\nIntercept=Cutoff_point\/vector[1]","3d9901eb":"Slope,Intercept","70050bc4":"New_X1=Features['X1']*Slope+Intercept\n","49e48e76":"sns.scatterplot(Features['X1'],Features['X2'],hue=data['Target'])","7b539ae4":"sns.scatterplot(Features['X1'],New_X1,hue=data['Target'])\nplt.scatter(x=Grand_means[0],y=Grand_means[1], color='r')\n","6b3d60e9":"New_x=-0.3*Slope+Intercept","50961a07":"sns.scatterplot(Features['X1'],New_X1,hue=data['Target'])\nplt.scatter(x=Grand_means[0],y=Grand_means[1], color='r')\nplt.scatter(x=-0.3,y=New_x, color='g')\n","a0af1468":"# How does **LDA** work?\n","b9ae3036":"### What About Prediction Right?\n\n\n> Lets Suppose a new data comes say(-0.3,-1) we need to predict it.\n\n","90e43a4c":"### Step 6: We need to get the Slope and Intercept of the vector for ploting and the project the data","d6f8ecbd":"### Step 1:- Covariance matrix","ff80f8aa":"Step 1\u2705","7e49cf07":"After finding the covariance matrix we need to **inverse** it\n\n","234ccfbe":"As you can see there are some miss classificationn the upper blue part has 1 blue mark and the below skin part has 2 blue mark so it means the \n**Accuracy** is 17\/20 = **0.85**.","be4f3436":"### Step 4: We need to find a vector that will divide the data most accuratly when the data set is projected on it.\n\n\n> So we need to use matrix multiplication of:\n1.   **Inverse** of the Covariance matrix.\n2.   Mean Differences.\n\n\n\n","2985747d":"# What is LDA\ud83e\udd14\u2753\n\n> Linear Discriminant Analysis or LDA is a dimensionality reduction technique. It is used as a pre-processing step in Machine Learning and applications of pattern classification. The goal of LDA is to project the features in higher dimensional space onto a lower-dimensional space in order to avoid the curse of dimensionality and also reduce resources and dimensional costs.\n\n\n","78f64772":"So as you can see the green dot falls in the blue(1) classs so it will be predicted as 2","a169ede2":"Creat new varibale which is projected on the vector it can we any of the 2 var(X1,X2)\n","27dff19e":"Step 3\u2705","c103bc26":"Step 4\u2705","47cf2684":"# LDA(Linear Discriminant Analysis)","280bb2a1":"Step 5\u2705","f863783a":"Step 2\u2705","80ab9ab6":"# Why to use LDA?\n\n>1.  Helps in **Classification**.\n2.   Helps in **Dimensionality** **reduction**.\n\n\n\n","f8e8b572":"### Step 2: We need to find the **mean** **difference** between *each class* for respective variable.","640143cf":"### Step 3: We need to find the Grand mean.\n\n\n> Grand mean is bascically the mean of each variable which will help us to find the cutoff point.\n\n\n","7447511a":"Cutoff point and Projected Data","d3cdf2ed":"### Step 5: We need to find the cutoff point.\n\n> Cutoff point is a value which help us the predict.\n\n","79910b3b":"# Step by Step LDA(Classification)"}}