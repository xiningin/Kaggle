{"cell_type":{"2a62ecd4":"code","8bf9f336":"code","d067c842":"code","1827b2c0":"code","ded3ea1e":"code","c11989de":"code","76628bb9":"code","b901cea7":"code","797c94c9":"code","03a57a05":"code","12f80cde":"code","d77599c6":"code","d3a5538f":"code","14788148":"code","a46608b2":"code","9acce687":"code","dccc5366":"code","37c9f1f6":"code","17a6a6c0":"code","dc2ff05f":"code","7793e37f":"code","3acac52d":"code","25325873":"code","6672bbbc":"code","f81381d6":"code","c3ec4982":"code","f4c18922":"code","5f0e5fe7":"code","9542737b":"code","c03cad7c":"code","b311dcd9":"code","51356256":"code","6da6e7a8":"code","426e4640":"code","7e5a3ef7":"code","59750243":"code","7c00eae5":"code","200e53b3":"code","ab14a124":"code","4f3fb0d4":"code","2e0c3a17":"code","009419ae":"code","cfc967d3":"code","e03d41fe":"code","30c61096":"code","48b4f0c6":"code","99ea65aa":"markdown","cb21bf8d":"markdown","8d73468b":"markdown","413c2600":"markdown","c3a5dccc":"markdown","a1fb3941":"markdown","ac5ed3db":"markdown","74d84932":"markdown","a31ee55e":"markdown","630905a2":"markdown","1c9b924c":"markdown","06b96991":"markdown","5f01a5b4":"markdown","eefe8f01":"markdown","db8ee493":"markdown","2ff7e918":"markdown","56624f38":"markdown","b84af304":"markdown","2a0de7e5":"markdown","7e9ad86a":"markdown","e9d8c8ff":"markdown","1738a892":"markdown","c2dce57b":"markdown","4655cd22":"markdown","536b5b62":"markdown","45b034fa":"markdown","219fd2c1":"markdown","ec9f40e1":"markdown","0d4a1b4b":"markdown","b817424c":"markdown"},"source":{"2a62ecd4":"!pip install num2words","8bf9f336":"import tensorflow as tf\nfrom keras import layers\nimport keras\nimport matplotlib.pyplot as plt\n\nimport nltk, re, string\nimport os\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords \nfrom nltk.stem import PorterStemmer       \nfrom num2words import num2words\n\nfrom tqdm import tqdm\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport pandas as pd\nimport random\nfrom collections import Counter\nimport tensorflow_hub as hub\nfrom tensorflow.keras import regularizers\nfrom keras.callbacks import ModelCheckpoint\n\nimport numpy as np\nimport os\nimport pickle\nfrom string import punctuation\nimport requests\n\nplt.style.use('fivethirtyeight') \n%matplotlib inline","d067c842":"url = \"https:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/aclImdb_v1.tar.gz\"\n\ndataset = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", url,\n                                    untar=True, cache_dir='.',\n                                    cache_subdir='')\n\ndataset_dir = os.path.join(os.path.dirname(dataset), 'aclImdb')","1827b2c0":"import os\nimport pandas as pd\n\n# 1 --> pos\n# 0 --> neg\n\ndata = pd.DataFrame()\ncomment = []\nlabel = []\nfor filename in os.listdir(\"aclImdb\/train\/pos\"):\n    with open(\"aclImdb\/train\/pos\/\"+str(filename), 'r', encoding=\"utf-8\") as f: # open in readonly mode \n        comment.append(f.readlines())\n        label.append(1)\n\nfor filename in os.listdir(\"aclImdb\/train\/neg\"):\n    with open(\"aclImdb\/train\/neg\/\"+str(filename), 'r', encoding=\"utf-8\") as f: # open in readonly mode \n        comment.append(f.readlines())\n        label.append(0)\n\nfor filename in os.listdir(\"aclImdb\/test\/pos\"):\n    with open(\"aclImdb\/test\/pos\/\"+str(filename), 'r', encoding=\"utf-8\") as f: # open in readonly mode \n        comment.append(f.readlines())\n        label.append(1)\n\nfor filename in os.listdir(\"aclImdb\/test\/neg\"):\n    with open(\"aclImdb\/test\/neg\/\"+str(filename), 'r', encoding=\"utf-8\") as f: # open in readonly mode \n        comment.append(f.readlines())\n        label.append(0)\n        \ndata['comment'] = comment\ndata['y'] = label","ded3ea1e":"X = pd.DataFrame(list(data['comment']))\nY = pd.DataFrame(list(data['y']))","c11989de":"\"\"\"Module to explore data.\nContains functions to help study, visualize and understand datasets.\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom collections import Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ndef get_num_classes(labels):\n    \"\"\"Gets the total number of classes.\n    # Arguments\n        labels: list, label values.\n            There should be at lease one sample for values in the\n            range (0, num_classes -1)\n    # Returns\n        int, total number of classes.\n    # Raises\n        ValueError: if any label value in the range(0, num_classes - 1)\n            is missing or if number of classes is <= 1.\n    \"\"\"\n    num_classes = max(labels) + 1\n    missing_classes = [i for i in range(num_classes) if i not in labels]\n    if len(missing_classes):\n        raise ValueError('Missing samples with label value(s) '\n                         '{missing_classes}. Please make sure you have '\n                         'at least one sample for every label value '\n                         'in the range(0, {max_class})'.format(\n                            missing_classes=missing_classes,\n                            max_class=num_classes - 1))\n\n    if num_classes <= 1:\n        raise ValueError('Invalid number of labels: {num_classes}.'\n                         'Please make sure there are at least two classes '\n                         'of samples'.format(num_classes=num_classes))\n    return num_classes\n\n\ndef get_num_words_per_sample(sample_texts):\n    \"\"\"Gets the median number of words per sample given corpus.\n    # Arguments\n        sample_texts: list, sample texts.\n    # Returns\n        int, median number of words per sample.\n    \"\"\"\n    num_words = [len(s.split()) for s in sample_texts]\n    return np.median(num_words)\n\n\ndef plot_frequency_distribution_of_ngrams(sample_texts,\n                                          ngram_range=(1, 2),\n                                          num_ngrams=50):\n    \"\"\"Plots the frequency distribution of n-grams.\n    # Arguments\n        samples_texts: list, sample texts.\n        ngram_range: tuple (min, mplt), The range of n-gram values to consider.\n            Min and mplt are the lower and upper bound values for the range.\n        num_ngrams: int, number of n-grams to plot.\n            Top `num_ngrams` frequent n-grams will be plotted.\n    \"\"\"\n    # Create args required for vectorizing.\n    kwargs = {\n            'ngram_range': (1, 1),\n            'dtype': 'int32',\n            'strip_accents': 'unicode',\n            'decode_error': 'replace',\n            'analyzer': 'word',  # Split text into word tokens.\n    }\n    vectorizer = CountVectorizer(**kwargs)\n\n    # This creates a vocabulary (dict, where keys are n-grams and values are\n    # idxices). This also converts every text to an array the length of\n    # vocabulary, where every element idxicates the count of the n-gram\n    # corresponding at that idxex in vocabulary.\n    vectorized_texts = vectorizer.fit_transform(sample_texts)\n\n    # This is the list of all n-grams in the index order from the vocabulary.\n    all_ngrams = list(vectorizer.get_feature_names())\n    num_ngrams = min(num_ngrams, len(all_ngrams))\n    # ngrams = all_ngrams[:num_ngrams]\n\n    # Add up the counts per n-gram ie. column-wise\n    all_counts = vectorized_texts.sum(axis=0).tolist()[0]\n\n    # Sort n-grams and counts by frequency and get top `num_ngrams` ngrams.\n    all_counts, all_ngrams = zip(*[(c, n) for c, n in sorted(\n        zip(all_counts, all_ngrams), reverse=True)])\n    ngrams = list(all_ngrams)[:num_ngrams]\n    counts = list(all_counts)[:num_ngrams]\n\n    idx = np.arange(num_ngrams)\n    plt.bar(idx, counts, width=0.8, color='b')\n    plt.xlabel('N-grams')\n    plt.ylabel('Frequencies')\n    plt.title('Frequency distribution of n-grams')\n    plt.xticks(idx, ngrams, rotation=45)\n    plt.show()\n\n\ndef plot_sample_length_distribution(sample_texts):\n    \"\"\"Plots the sample length distribution.\n    # Arguments\n        samples_texts: list, sample texts.\n    \"\"\"\n    plt.hist([len(s) for s in sample_texts], 50)\n    plt.xlabel('Length of a sample')\n    plt.ylabel('Number of samples')\n    plt.title('Sample length distribution')\n    plt.show()\n\n\ndef plot_class_distribution(labels):\n    \"\"\"Plots the class distribution.\n    # Arguments\n        labels: list, label values.\n            There should be at lease one sample for values in the\n            range (0, num_classes -1)\n    \"\"\"\n    num_classes = get_num_classes(labels)\n    count_map = Counter(labels)\n    counts = [count_map[i] for i in range(num_classes)]\n    idx = np.arange(num_classes)\n    plt.bar(idx, counts, width=0.8, color='b')\n    plt.xlabel('Class')\n    plt.ylabel('Number of samples')\n    plt.title('Class distribution')\n    plt.xticks(idx, idx)\n    plt.show()","76628bb9":"def remove_html(comment):\n    return comment.replace('<br \/>', ' ')\n\ndef lower_comment(comment):\n    return comment.lower()\n\ndef remove_ponctuation(comment):\n    for punc in string.punctuation:\n        comment = comment.replace(punc, '')\n    return comment\n\ndef tokenize_comment(comment):\n    return comment.split()\n\ndef remove_stop_words(comment_tokenized):\n    stopwords_english = stopwords.words('english')\n    comment_tokenized_cleaned = []\n    for word in comment_tokenized:\n        if(word not in stopwords_english):\n            comment_tokenized_cleaned.append(word)\n    return comment_tokenized_cleaned\n\ndef convert_numbers_to_words(comment_tokenized):\n    comment_tokenized_cleaned = []\n    for word in comment_tokenized:  # Go through every word in your tokens list\n        try:\n            comment_tokenized_cleaned.append(num2words(word))\n        except:\n            comment_tokenized_cleaned.append(word)\n    return comment_tokenized_cleaned\n\ndef remove_ponctuation_from_tokenized(comment_tokenized):\n    comment_tokenized_cleaned = []\n    for word in comment_tokenized:\n        comment_tokenized_cleaned.append(remove_ponctuation(word))\n    \n    return comment_tokenized_cleaned\n\ndef stem_words(comment_tokenized):\n    stemmer = PorterStemmer()\n    comment_tokenized_cleaned = []\n    for word in comment_tokenized:  # Go through every word in your tokens list\n        comment_tokenized_cleaned.append(stemmer.stem(word))   # stemming word\n    return comment_tokenized_cleaned\n\n","b901cea7":"# Clean the data.\ncleaned_comments = []\nfor i in tqdm(range (len(list(X[0])))):\n    text = remove_html(list(X[0])[i])\n    text = lower_comment(text)\n    text = remove_ponctuation(text)\n    text_tokenized = tokenize_comment(text)\n    text_tokenized = convert_numbers_to_words(text_tokenized)\n    text_tokenized = remove_stop_words(text_tokenized)\n    text_tokenized = remove_ponctuation_from_tokenized(text_tokenized)\n    text_tokenized = stem_words(text_tokenized)\n    \n    cleaned_comments.append(text_tokenized)","797c94c9":"def get_num_words_per_sample(sample_texts):\n    \"\"\"Gets the median number of words per sample given corpus.\n    # Arguments\n        sample_texts: list, sample texts.\n    # Returns\n        int, median number of words per sample.\n    \"\"\"\n    num_words = [len(s) for s in sample_texts]\n    return np.median(num_words)","03a57a05":"cleaned_comments_sentance = []\nfor sentance in cleaned_comments:\n    ch = \"\"\n    for word in sentance:\n        ch = ch + word + \" \"\n    cleaned_comments_sentance.append(ch)","12f80cde":"get_num_classes(list(Y[0]))  # Number of classes.","d77599c6":"get_num_words_per_sample(cleaned_comments)   # Median number of words per sample.","d3a5538f":"vocabulary_size = 20000\ntext_length = 90","14788148":"plot_frequency_distribution_of_ngrams(cleaned_comments_sentance, num_ngrams=15)","a46608b2":"plot_sample_length_distribution(cleaned_comments_sentance)","9acce687":"plot_class_distribution(list(Y[0]))","dccc5366":"# 561 < 1500\nlen(list(X[0])) \/ get_num_words_per_sample(cleaned_comments)","37c9f1f6":"len(cleaned_comments_sentance)","17a6a6c0":"stopwords_english = stopwords.words('english')\ntfidf = TfidfVectorizer(ngram_range=(2, 2), max_features=10000, stop_words=stopwords_english)   # bigram, a vocabulary that only consider the top max_features ordered by term frequency across the corpus.","dc2ff05f":"cleaned_comments_sentance[0]","7793e37f":"response = tfidf.fit_transform(cleaned_comments_sentance)","3acac52d":"response.shape[0]","25325873":"len(tfidf.get_feature_names())","6672bbbc":"df = pd.DataFrame(response.toarray(), columns=tfidf.get_feature_names())","f81381d6":"df","c3ec4982":"# # Create an SelectKBest object to select features with the best 5000 ANOVA F-Values\n# fvalue_selector = feature_selection.SelectKBest(feature_selection.f_classif, k=5000)   # k: Number of top features to select. The \u201call\u201d option bypasses selection, for use in a parameter search.\n\n# # Apply the SelectKBest object to the features and target\n# response_best = fvalue_selector.fit_transform(response.toarray(), np.array(Y))\n\n# cols = fvalue_selector.get_support(indices=True)\n\n# choosen_columns = []\n# for i in cols:\n#   choosen_columns.append(tfidf.get_feature_names()[i])\n\n# df = pd.DataFrame(response_best, columns=choosen_columns)\n# df","f4c18922":"# Shuffle the data\n# Fisher\u2013Yates shuffle\nfor i in tqdm(range(len(df)-2)):\n  # Shuffle X\n  j = random.randint(i, len(df)-1)\n  aux = df.iloc[j]\n  df.loc[j] = df.iloc[i]\n  df.loc[i] = aux\n  df.reset_index(drop=True, inplace=True)\n\n  # Shuffle Y\n  aux_y = Y.iloc[j]\n  Y.loc[j] = Y.iloc[i]\n  Y.loc[i] = aux_y\n  Y.reset_index(drop=True, inplace=True)\n","5f0e5fe7":"X_train = df[:40000]\nX_test = df[40000:50000]\nY_train = Y[:40000]\nY_test = Y[40000:50000]","9542737b":"def get_optimizer(batch_size_var, X_train):\n    # Many models train better if you gradually reduce the learning rate during training. \n    # Use optimizers.schedules to reduce the learning rate over time\n    N_TRAIN = X_train.shape[0]\n    STEPS_PER_EPOCH = N_TRAIN\/\/batch_size_var\n\n    lr_schedule = tf.keras.optimizers.schedules.InverseTimeDecay(0.001, \n                                                                 decay_steps=STEPS_PER_EPOCH*1000, \n                                                                 decay_rate=1, \n                                                                 staircase=False)\n    \n    return tf.keras.optimizers.Adam(lr_schedule)\n\ndef compile_model(model, loss_func, batch_size_var, epochs_var, X_train, Y_train, X_test, Y_test):\n  model.compile(optimizer=get_optimizer(batch_size_var, X_train), \n                loss=loss_func,\n                metrics=['accuracy'])\n\n  history = model.fit(\n      X_train,\n      Y_train,\n      batch_size=batch_size_var,\n      epochs=epochs_var,\n      # We pass some validation for\n      # monitoring validation loss and metrics\n      # at the end of each epoch\n      validation_data=(X_test, Y_test),\n  )\n\n\n  history_dict = history.history\n  history_dict.keys()\n\n  acc = history_dict['accuracy']\n  val_acc = history_dict['val_accuracy']\n  loss = history_dict['loss']\n  val_loss = history_dict['val_loss']\n\n  epochs = range(1, len(acc) + 1)\n\n  # \"bo\" is for \"blue dot\"\n  plt.plot(epochs, loss, 'bo', label='Training loss')\n  # b is for \"solid blue line\"\n  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n  plt.title('Training and validation loss')\n  plt.xlabel('Epochs')\n  plt.ylabel('Loss')\n  plt.legend()\n\n  plt.show()\n\n  plt.plot(epochs, acc, 'bo', label='Training acc')\n  plt.plot(epochs, val_acc, 'b', label='Validation acc')\n  plt.title('Training and validation accuracy')\n  plt.xlabel('Epochs')\n  plt.ylabel('Accuracy')\n  plt.legend(loc='lower right')\n\n  plt.show()\n  return model","c03cad7c":"input_layer = keras.Input(shape=(X_train.shape[1]))  \nx = layers.Dense(64, activation=\"relu\")(input_layer)\nx = layers.Dense(64, activation=\"relu\")(x)\noutput_layer = layers.Dense(2, activation='softmax')(x)\n\nmodel = keras.Model(inputs=input_layer, outputs=output_layer, name=\"model\")\nmodel.summary()\nkeras.utils.plot_model(model, \"model.png\", show_shapes=True)","b311dcd9":"model = compile_model(model, 'sparse_categorical_crossentropy', 64, 10, X_train, Y_train, X_test, Y_test)","51356256":"test = [\"This movie is really bad\", \"I like this movie\", \"I didn't enjoy watching this movie\", \"I did enjoy watching this movie\", \"It's a waste of time\", \"I enjoyed watching this movie\"]","6da6e7a8":"# Clean the data.\ncleaned_comments_test = []\nfor i in tqdm(range (len(test))):\n    text = remove_html(test[i])\n    text = lower_comment(text)\n    text = remove_ponctuation(text)\n    text_tokenized = tokenize_comment(text)\n    text_tokenized = convert_numbers_to_words(text_tokenized)\n    text_tokenized = remove_stop_words(text_tokenized)\n    text_tokenized = remove_ponctuation_from_tokenized(text_tokenized)\n    text_tokenized = stem_words(text_tokenized)\n    \n    cleaned_comments_test.append(text_tokenized)","426e4640":"cleaned_comments_sentance_test = []\nfor sentance in cleaned_comments_test:\n    ch = \"\"\n    for word in sentance:\n        ch = ch + word + \" \"\n    cleaned_comments_sentance_test.append(ch)","7e5a3ef7":"cleaned_comments_sentance_test","59750243":"response_test = tfidf.transform(cleaned_comments_sentance_test) ","7c00eae5":"df_test = pd.DataFrame(response_test.toarray(), columns=tfidf.get_feature_names())","200e53b3":"df_test","ab14a124":"pred_list = np.argmax(model.predict(df_test), axis=1)   # These are the probabilities of each class (pos or neg)","4f3fb0d4":"for i in range(len(pred_list)):\n  if(pred_list[i]==0):\n    print(test[i]+\" --> neg\")\n  else:\n    print(test[i]+\" --> pos\")\n","2e0c3a17":"# Testing ResNet architecture\n\nembedding = \"https:\/\/tfhub.dev\/google\/tf2-preview\/gnews-swivel-20dim\/1\"\ninput_layer = keras.Input(shape=(), name=\"Input\", dtype=tf.string)  \n\nhub_layer = hub.KerasLayer(embedding, trainable=True, name='embedding')(input_layer)\n\n# Adding dimensions to use SeparableConv2D\nx = tf.expand_dims(hub_layer, -1)\n\nx = layers.SeparableConv1D(32, 3, activation=\"relu\", padding=\"same\")(x)\nx = layers.SeparableConv1D(64, 3, activation=\"relu\", padding=\"same\")(x)\n\nblock_1_output = layers.MaxPooling1D(3, padding=\"same\")(x)\n\nx = layers.SeparableConv1D(64, 3, activation=\"relu\", padding=\"same\")(block_1_output)\nx = layers.SeparableConv1D(64, 3, activation=\"relu\", padding=\"same\")(x)\n\nblock_2_output = layers.add([x, block_1_output])\n\nx = layers.SeparableConv1D(64, 3, activation=\"relu\", padding=\"same\")(block_2_output)\nx = layers.SeparableConv1D(64, 3, activation=\"relu\", padding=\"same\")(x)\n\nblock_3_output = layers.add([x, block_2_output])\n\nx = layers.SeparableConv1D(64, 3, activation=\"relu\", padding=\"same\")(block_3_output)\n\nx = layers.GlobalAveragePooling1D()(x)\nx = layers.Dense(256, activation=\"relu\")(x)\nx = layers.Dropout(0.2)(x)\n\noutput_layer = tf.keras.layers.Dense(2, activation='softmax')(x)\n\n# Remove extra dimensions\n# output_layer = tf.squeeze(output_layer, [0, 1])\n\nsecond_model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"model\")\n\nsecond_model.summary()\nkeras.utils.plot_model(second_model, \"model.png\", show_shapes=True)","009419ae":"embedding = \"https:\/\/tfhub.dev\/google\/tf2-preview\/gnews-swivel-20dim\/1\"\ninput_layer = keras.Input(shape=(), name=\"Input\", dtype=tf.string)  \n\nhub_layer = hub.KerasLayer(embedding, trainable=True, name='embedding')(input_layer)\n\nx = tf.expand_dims(hub_layer, 1)\n\nx = layers.Bidirectional(layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(x)\nx = layers.Bidirectional(layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2, return_sequences=True))(x)\nx = layers.Bidirectional(layers.LSTM(64, activation=\"relu\", dropout=0.2, recurrent_dropout=0.2))(x)\n\noutput_layer = tf.keras.layers.Dense(2, activation='softmax')(x)\n\nthird_model = keras.Model(inputs=input_layer, outputs=output_layer, name=\"model\")\n\nthird_model.summary()\nkeras.utils.plot_model(third_model, \"model.png\", show_shapes=True)","cfc967d3":"from sklearn.model_selection import train_test_split    \n\nX2 = pd.DataFrame(cleaned_comments_sentance)\nY2 = pd.DataFrame(list(data['y']))\nX_train, X_test, Y_train, Y_test = train_test_split(X2, Y2, test_size=0.2)    ","e03d41fe":"Y_train = tf.keras.utils.to_categorical(Y_train, num_classes=2)\nY_test = tf.keras.utils.to_categorical(Y_test, num_classes=2)","30c61096":"second_model = compile_model(second_model, 'categorical_crossentropy', 40, 10, X_train, Y_train, X_test, Y_test)","48b4f0c6":"third_model = compile_model(third_model, 'categorical_crossentropy', 40, 10, X_train, Y_train, X_test, Y_test)","99ea65aa":"Sequence models often have such an embedding layer as their first layer. This layer learns to turn word index sequences into word embedding vectors during the training process, such that each word index gets mapped to a dense vector of real values representing that word\u2019s location in semantic space","cb21bf8d":"![NLP](https:\/\/i.ibb.co\/Xz6DQP5\/image13.png)","8d73468b":"### tf-idf","413c2600":"## Second approach","c3a5dccc":"## First approach","a1fb3941":"In an n-gram vector, text is represented as a collection of unique n-grams: groups of n adjacent tokens (typically, words). Consider the text The mouse ran up the clock. Here, \n* The word unigrams (n = 1) are ['the', 'mouse', 'ran', 'up', 'clock'], \n* The word bigrams (n = 2) are ['the mouse', 'mouse ran', 'ran up', 'up the', 'the clock'], and so on.","ac5ed3db":"![Texte alternatif\u2026](https:\/\/i.ibb.co\/FwT0GWN\/image8.png)","74d84932":"### f_classif","a31ee55e":"TF is frequency counter for a term t in document d, where as DF is the count of occurrences of term t in the document set N. In other words, DF is the number of documents in which the word is present. We consider one occurrence if the term consists in the document at least once, we do not need to know the number of times the term is present.","630905a2":"Models can be broadly classified into two categories: \n\n1. those that use word ordering information (sequence models),\n  \n2. and ones that just see text as \u201cbags\u201d (sets) of words (n-gram models). \n\nTypes of sequence models include convolutional neural networks (CNNs), recurrent neural networks (RNNs), and their variations.\n\nTypes of n-gram models include logistic regression, simple multi- layer perceptrons (MLPs, or fully-connected neural networks), gradient boosted trees and support vector machines.","1c9b924c":"Word embeddings: Words have meaning(s) associated with them. As a result, we can represent word tokens in a dense vector space (~few hundred real numbers), where the location and distance between words indicates how similar they are semantically. This representation is called word embeddings.","06b96991":"## Testing","5f01a5b4":"## The model","eefe8f01":"* One-hot encoding: Every sample text is represented as a vector indicating the presence or absence of a token in the text.\n\n'The mouse ran up the clock' = [1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1]\n\n* Count encoding: Every sample text is represented as a vector indicating the count of a token in the text. Note that the element corresponding to the unigram 'the' (bolded below) now is represented as 2 because the word \u201cthe\u201d appears twice in the text.\n\n'The mouse ran up the clock' = [1, 0, 1, 1, 1, 0, 1, 2, 1, 1, 1, 1]\n\n* Tf-idf encoding: The problem with the above two approaches is that common words that occur in similar frequencies in all documents (i.e., words that are not particularly unique to the text samples in the dataset) are not penalized. For example, words like \u201ca\u201d will occur very frequently in all texts. So a higher token count for \u201cthe\u201d than for other more meaningful words is not very useful.\n\n'The mouse ran up the clock' = [0.33, 0, 0.23, 0.23, 0.23, 0, 0.33, 0.47, 0.33,\n0.23, 0.33, 0.33] (See Scikit-learn TfidfTransformer)","db8ee493":"![NLP](https:\/\/i.ibb.co\/SKtMmbq\/image12.png)","2ff7e918":"![NLP](https:\/\/i.ibb.co\/pbPj8v7\/image7.png)","56624f38":"## Exploring The Data, data preprocessing and vectorization","b84af304":"![Texte alternatif\u2026](https:\/\/i.ibb.co\/5GWdyM9\/image11.png)","2a0de7e5":"Collect Key Metrics Once you\u2019ve verified the data, collect the following important metrics that can help characterize your text classification problem:\n\n* Number of samples: Total number of examples you have in the data.\n\n* Number of classes: Total number of topics or categories in the data.\n\n* Number of samples per class: Number of samples per class (topic\/category). In a balanced dataset, all classes will have a similar number of samples; in an imbalanced dataset, the number of samples in each class will vary widely.\n\n* Number of words per sample: Median number of words in one sample.\n\n* Frequency distribution of words: Distribution showing the frequency (number of occurrences) of each word in the dataset.\n\n* Distribution of sample length: Distribution showing the number of words per sample in the dataset.","7e9ad86a":"The median is the middle number in a sorted, ascending or descending, list of numbers and can be more descriptive of that data set than the average. The median is sometimes used as opposed to the mean when there are outliers in the sequence that might skew the average of the values.","e9d8c8ff":"# Classify movie reviews as positive or negative, based on the text of the review.","1738a892":"SelectKBest: Calculate the score of each feature using the test method you want, and remove all features except the top k of the score based on this feature score (take top k)","c2dce57b":"I skipped this step as the model performed better without it.","4655cd22":"We perform a normalization on the frequency value. we divide the the frequency with the total number of words in the document.","536b5b62":"![Texte alternatif\u2026](https:\/\/i.ibb.co\/4f5SBBM\/image9.png)","45b034fa":"Terminology:\n\n* t \u2014 term (word)\n* d \u2014 document (set of words)\n* N \u2014 count of corpus\n* corpus \u2014 the total document set","219fd2c1":"ANOVA F-value between label\/feature:\n* Regression: Use \"f_regression\"\n\n* Classification: Use \"f_classif\"","ec9f40e1":"1. Calculate the number of samples\/number of words per sample ratio.\n2. If this ratio is less than 1500, tokenize the text as n-grams and use a simple multi-layer perceptron (MLP) model to classify them (left branch in the flowchart below):\n  * Split the samples into word n-grams; convert the n-grams into vectors.\n  * Score the importance of the vectors and then select the top 20K using the * scores.\n  * Build an MLP model.\n3. If the ratio is greater than 1500, tokenize the text as sequences and use a sepCNN model to classify them (right branch in the flowchart below):\n  * Split the samples into words; select the top 20K words based on their frequency.\n  * Convert the samples into word sequence vectors.\n  * If the original number of samples\/number of words per sample ratio is less than 15K, using a fine-tuned pre-trained embedding with the sepCNN model will likely provide the best results.\n4. Measure the model performance with different hyperparameter values to find the best model configuration for the dataset.","0d4a1b4b":"![Texte alternatif\u2026](https:\/\/i.ibb.co\/RQPdyrL\/image10.png)","b817424c":"1. Tokenization: Divide the texts into words or smaller sub-texts, which will enable good generalization of relationship between the texts and the labels. This determines the \u201cvocabulary\u201d of the dataset (set of unique tokens present in the data).\n\n2. Vectorization: Define a good numerical measure to characterize these texts."}}