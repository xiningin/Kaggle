{"cell_type":{"738a2a36":"code","fc940613":"code","7d45bad5":"code","00501460":"code","5afe5a29":"code","8b70909e":"code","7dc22221":"code","32f2b30f":"code","95e84a96":"code","34b2deb9":"code","4ff2f6c9":"code","e2ea9ccf":"code","3b5d540b":"code","4a8957cd":"code","a1814055":"code","5867b29e":"code","e074d88a":"code","fedbfedd":"code","3b7c4594":"code","1e7713c7":"code","4f0bd9ae":"code","e306594e":"code","afc873ce":"code","6359eb4b":"code","8631cf39":"code","865bf566":"code","bed5634b":"code","0d255bef":"code","32f6c601":"code","bf224007":"code","b224c4d5":"code","6e96e514":"code","7fc94765":"code","54cf1d5c":"code","af657a85":"code","3bc439dd":"code","23ce383e":"code","6c666146":"code","8c3fd9f9":"markdown","fdb72e9a":"markdown","f93664e1":"markdown","ee91c609":"markdown","ddd032be":"markdown","ffe399ff":"markdown","5259761f":"markdown","da1a0926":"markdown","d86a2f6c":"markdown","bfeae40d":"markdown","9ce66a37":"markdown","817430e1":"markdown","9a875b0f":"markdown","6c7b8914":"markdown","1dfc1b2d":"markdown","bdd6d317":"markdown"},"source":{"738a2a36":"# import training dataset\nimport pandas as pd\ndf = pd.read_csv(\"..\/input\/feedback-prize-2021\/train.csv\")\ndf.head()","fc940613":"df.shape","7d45bad5":"# load other datasets (this is mainly used for validation purpose)\ntrain_text_path = \"..\/input\/feedback-prize-2021\/train\"\ntest_text_path = \"..\/input\/feedback-prize-2021\/test\"","00501460":"# read .txt file \npath = '..\/input\/feedback-prize-2021\/train\/423A1CA112E2.txt'\nwith open(path, \"r\") as fp:\n    txt = fp.read()\nprint(txt)","5afe5a29":"text_id = df['id'][0]\ntext_id","8b70909e":"def get_text(file_id):\n    # creating a file path\n    a_file = f\"{train_text_path}\/{file_id}.txt\"\n    \n    # read .txt file \n    with open(a_file, \"r\") as fp:\n        txt = fp.read()\n    return txt\n\ntxt = get_text(text_id)\nprint(txt)","7dc22221":"df_example = df[df['id'] == text_id]\ndf_example","32f2b30f":"# Creadits for this part of visualisation _> https:\/\/www.kaggle.com\/thedrcat\nimport spacy\nfrom spacy import displacy\nfrom pylab import cm, matplotlib\n\ncolors = {\n            'Lead': '#8000ff',\n            'Position': '#2b7ff6',\n            'Evidence': '#2adddd',\n            'Claim': '#80ffb4',\n            'Concluding Statement': 'd4dd80',\n            'Counterclaim': '#ff8042',\n            'Rebuttal': '#ff0000'\n         }\n\ndef visualize(example, df):\n    ents = []\n    for i, row in df[df['id'] == example].iterrows():\n        ents.append({\n                        'start': int(row['discourse_start']), \n                         'end': int(row['discourse_end']), \n                         'label': row['discourse_type']\n                    })\n        \n    with open(f'{train_text_path}\/{example}.txt', 'r') as file: data = file.read()\n    doc2 = {\n                \"text\": data,\n                \"ents\": ents,\n                \"title\": example\n            }\n\n    options = {\"ents\": df.discourse_type.unique().tolist(), \"colors\": colors}\n    displacy.render(doc2, style=\"ent\", options=options, manual=True, jupyter=True)","95e84a96":"examples = df['id'].sample(n=1, random_state=42).values.tolist()\nfor ex in examples:\n    visualize(ex,df)\n    print('\\n')","34b2deb9":"# No nulls\ndf.isnull().sum()","4ff2f6c9":"# check value count of response variable\ndf.discourse_type.value_counts()\n\n# optional way\ndf['discourse_type'].value_counts()","e2ea9ccf":"new_df = pd.DataFrame(columns=['text', 'label'])\nnew_df['text'] =  df['discourse_text'] \nnew_df['label'] = df['discourse_type']\nnew_df.head()","3b5d540b":"new_df.shape","4a8957cd":"# select first 5000 obs for fast run on colab\nnew_df = new_df[:5000]\nnew_df.shape","a1814055":"from nltk.stem import PorterStemmer \nfrom nltk.tokenize import word_tokenize \nimport re\nfrom nltk.corpus import stopwords\n\nps = PorterStemmer() \ndef text_process(text):\n    # lower case\n    text = text.lower()\n    # remove emails\n    text= ' '.join([i for i in text.split() if '@' not in i])\n    # remove urls\n    text = re.sub(r'http\\S+|www\\S+', '', text)\n    # remove irrelevant characters other than numbers and space\n    text = re.sub('[^A-Za-z\\s]+', ' ', text)\n    # remove numbers\n    text =  re.sub(r'\\d+', '', text)\n    #remove white spaces\n    text = re.sub(r'\\s+', ' ', text)\n    #remove unwanted words with string\n    text=re.sub(r'\\d+\\W','',text)\n    # Remove HTML Tag\n    text=re.sub(r'<[^>]+>','',text)\n    \n    \n    # Use tokenizer\n    words = word_tokenize(text)\n    # remove Stop words from text\n    stop = stopwords.words('english')\n    words=[x for x in words if x not in stop]\n    text=' '.join(words)\n    return text.strip()","5867b29e":"text = \"Hello\ud83d\ude42... x@xwy.in my s@gmail.com \u201cname\u201d (is) #shweta!!!!  7$  ] 100%  } ] http\/\/:vision.com https:\/\/vision.com and your???,,,,, ???? name \\ is \/ www.fb.com \"\ntext_process(text)","e074d88a":"#apply text processing\n# colum in pandas data\n#1. new_df['text']\n#2. new_df.text\nnew_df = new_df[new_df.text.str.split().str.len() >= 3]\n\nnew_df['text'] = new_df['text'].apply(lambda x: text_process(x))\nnew_df.head(1)","fedbfedd":"new_df['category_id'] = new_df['label'].factorize()[0]\nnew_df.head(1)","3b7c4594":"new_df['category_id'].unique().tolist()","1e7713c7":"# create a dictionary of id and class (this will help in prediction purpose)\nid_to_category = dict(enumerate(new_df['label'].unique().tolist()))\ncategory_to_id = {v: k for k, v in id_to_category.items()}\nprint(id_to_category)\nprint(category_to_id)","4f0bd9ae":"# model evaluation\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import cohen_kappa_score","e306594e":"X = new_df['text'] # features\ny = new_df['category_id'] # labels","afc873ce":"# maths calculation\nimport numpy as np\n\n# word embeddings \/ feature extraction\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer = CountVectorizer()\n\n# Create sample set of documents\ndocs = np.array(['Mirabai has won a silver medal in weight lifting in Tokyo olympics 2021',\n                 'Sindhu has won a bronze medal in badminton in Tokyo olympics'])\n\n# Fit the bag-of-words model df.text\nbag = vectorizer.fit_transform(docs)\n\n# Associate the indices with each unique word\nprint(vectorizer.vocabulary_)\n\nprint(\"---------------------------------\")\n# Print the numerical feature vector\nprint(bag.toarray())","6359eb4b":"from sklearn.feature_extraction.text import TfidfVectorizer\n# create object\ntfidf = TfidfVectorizer()\n\n# assign documents\nd0 = 'VisionNLP for nlp'\nd1 = 'VisionNLP'\nd2 = 'VisionNLP vision nlp'\n# merge documents into a single corpus\nstring = [d0, d1, d2]\n\n# get tf-df values \nresult = tfidf.fit_transform(string)\n\n# get idf values\nprint('\\nidf values:')\nfor ele1, ele2 in zip(tfidf.get_feature_names(), tfidf.idf_):\n    print(ele1, ':', ele2)","8631cf39":"# get indexing\nprint('\\nWord indexes:')\nprint(tfidf.vocabulary_)\n  \n# display tf-idf values\nprint('\\ntf-idf value:')\nprint(result)\n  \n# in matrix form\nprint('\\ntf-idf values in matrix form:')\nprint(result.toarray())","865bf566":"# word embeddings \/ feature extraction\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer()\nfeatures = tfidf.fit_transform(X).toarray()\nlabels = y","bed5634b":"# train and test spliiting\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features, y, test_size=0.33, random_state=42)\nX_train.shape","0d255bef":"y_train.shape","32f6c601":"X_test","bf224007":"# model building\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","b224c4d5":"y_pred = lr.predict( X_test )\ny_pred","6e96e514":"y_test","7fc94765":"print(classification_report(y_test, y_pred))","54cf1d5c":"accuracy_score(y_test, y_pred)","af657a85":"cohen_kappa_score(y_test,y_pred)","3bc439dd":"from sklearn.svm import SVC # SVM\nmodel=SVC(C=1, gamma=1, kernel='linear',probability=True)\n#Train Algorithm\nmodel.fit(X_train, y_train)","23ce383e":"# model evaluation\ny_pred = model.predict( X_test )\nprint('Accuracy: ', accuracy_score(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","6c666146":"# save the model to disk\nimport pickle\nfilename = 'finalized_model.pickle'\npickle.dump(clf, open(filename, 'wb'))","8c3fd9f9":"## For performance and visualization reasons, we'll only use 5,000 sentences from the dataset\n\n### lets learn Text classification zero to BERT","fdb72e9a":"## 2. tf-idf\n\n![image.png](attachment:10ca2f72-1451-45ca-b2a2-74882d6a0097.png)","f93664e1":"## From the [Data tab](https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/data):\n\n> *  id -                 ID code for essay response\n> *  discourse_id -       ID code for discourse element\n> *  discourse_start -    character position where discourse element begins in the essay response\n> *  discourse_end -      character position where discourse element ends in the essay response\n> *  discourse_text -     text of discourse element\n> *  discourse_type -     classification of discourse element\n> *  discourse_type_num - enumerated class label of discourse element\n> *  predictionstring -   the word indices of the training sample, as required for predictions","ee91c609":"# 2. Data processing \n\n## Check For Null Values \nwe can see that there are no null values in the dataset","ddd032be":"tf-idf values: \n- first value in tuple is document index\n- second value in tuple is word index\n","ffe399ff":"# 3. Text processing\n\n- I've given better version of text clearning and procesing(mainly used in quite messy and unclean data like twitter).\n- This data is clean, you can ignore few steps of clearning process.","5259761f":"![image.png](attachment:a082d528-9e97-4e07-bc83-dcd5ca9d02c9.png)","da1a0926":"## Let's see the first example in some more detail","d86a2f6c":"# \ud83d\udcd6 Feedback Prize - Sentence Classifier using Machine learning\n\n![image.png](attachment:89b17c49-c2f0-46c9-bdf0-ca07a6071639.png)\n\n## Please, _DO_ upvote if you find it useful or interesting!! ","bfeae40d":"# load Data","9ce66a37":"- tf-idf is used in the fields of information retrieval (IR) and machine learning text classification.\n- The tf\u2013idf value increases proportionally to the:-\n- - number of times  a word appears in the document and the number of documents in the corpus that contain the word\n- which helps to adjust for the fact that some words appear more frequently in general. \n\n**inverse document frequency: This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is.**\n\n**tf-idf: The higher the score, the more relevant that word is in that particular document.**","817430e1":"# 1. EDA","9a875b0f":"## categorical variable to numerical for model building","6c7b8914":"#### Understanding dataset with simple Amazon product reviews Ex.mobile \n\nReview - amazon product\n- \/ train\n- - review1.txt\n- - review2.txt\n- review1.txt: camera is not good. battery is working good.\n\n-\/ train.csv\n| discourse_text      | discourse_type |\n| camera is not good  | camera |\n| battery is working good | battery |\n\n-----\n- train - model building\n- test - validation --> calculate accuracy\n- test - submission\n\n![image.png](attachment:2e5baebe-200c-48b7-85c7-5c2b04224fb3.png)","1dfc1b2d":"## 1. Bag of words\n- In this algorithm we basically calculates frequecy of each word means how frequent each word appears in different documents\n- Here we will implement Bag of words (BoW) model\n- Bag of words model helps convert the text into numerical representation (numerical feature vectors) , later this feature vector will be used to train ML models\n- The below picture explains the above concepts:\n\nFor example, for the first document, \u201cbird\u201d occured for 5 times, \u201cthe\u201d occured for two times and \u201cabout\u201d occured for 1 time.\n\n![image.png](attachment:64e41225-06b6-4d03-9224-7126b03263bb.png)\n\nIn python we use **CountVectorizer** class from scikit-learn ","bdd6d317":"# 4. Word Emeddings \n\n**In classification of text, for the text we can call sentence, row or document.**\n\n**generate numbers from raw texts**\n\n### context free\n1. Bag of words - simple method\n2. tf-idf - statistical\n\n### context free\n3. word2vec - statistical + deep learning(newural networks)\n4. glove \n\n### contextual\n5. EMLo\n6. bert"}}