{"cell_type":{"958df68e":"code","69105465":"code","32f596ca":"code","e0026a79":"code","3135b586":"code","1312fba4":"code","d1a4384f":"code","bdaa956e":"code","179408f7":"code","12482831":"code","40e51031":"code","dfcad30a":"code","bbf0022d":"code","b4c2a336":"code","cabad6cf":"code","3ef3ab04":"code","5eab12b2":"code","170beb43":"code","5bda5871":"code","5cef6b02":"code","75126bb4":"code","34b0b162":"code","325c7739":"code","4cc793d3":"code","12ebe6a9":"code","4443a36d":"code","ded35117":"code","2707ba25":"code","8c621de4":"code","2b8b9617":"code","d23af384":"code","3dd973a2":"code","d0170f29":"code","e1d3af88":"code","4e146d24":"code","21a7d848":"code","b2704bff":"code","add61671":"code","fbc36b5f":"code","7a9bf635":"code","060ab6bc":"code","cc374969":"code","c8fcb1d2":"code","050782cc":"code","e6d57a4f":"code","9f6911af":"code","face4733":"code","244c036e":"code","1ca6e046":"code","5a4e30c0":"code","dc5e1d79":"code","5806328a":"code","918391c9":"code","db9a9aea":"code","7597fde8":"code","fb78d392":"code","67eed423":"code","655a9d8b":"code","2678d449":"code","0628e35e":"code","07cdcf17":"code","e889d0ce":"code","45b26b4b":"code","6d7c5db6":"code","094dbc02":"code","d6f87c7a":"code","50a58433":"code","932710a5":"code","a79aaac6":"code","2f1d4d4b":"code","53dd5302":"code","335265c0":"code","9ef735d0":"code","4056899f":"code","0bf22162":"code","f824a6ce":"code","67897f61":"code","66fcd90d":"code","40d5f6c3":"code","aadb37b5":"code","3f100e7f":"code","31d1833a":"code","abaa949d":"code","aa85d163":"code","c6cc63c0":"code","b1cd5418":"code","b3bcaacf":"code","1681086f":"code","15d7edb4":"code","341ecb6a":"code","b768fc74":"code","fb98df34":"code","5326dfdd":"code","7757a4d5":"code","c5a8f806":"code","15fab747":"code","fa2344de":"code","4fe1c9b1":"code","5ab6f6e1":"code","90512619":"code","bc42f206":"code","fae220fc":"code","69a2522c":"code","fb6e18c3":"code","27c8163f":"code","2c122480":"code","ea7cb844":"code","d03582c8":"code","19384c0a":"code","248f6072":"code","69187a2f":"markdown","d053a78c":"markdown","5c9e6b96":"markdown","3fafa981":"markdown","b8825a10":"markdown","0b52e8b6":"markdown","54f41989":"markdown","0130af87":"markdown","48c4ddee":"markdown","67fb41b6":"markdown","f8f7dad8":"markdown","43489a31":"markdown","c1d48f02":"markdown","65963923":"markdown","e84c4686":"markdown","68133675":"markdown","4d424ac6":"markdown","401290ec":"markdown","92aca407":"markdown","695093d5":"markdown","80de2c57":"markdown","dd8e37de":"markdown","e8b3b8cd":"markdown","6a1b9185":"markdown","fdb8fbeb":"markdown","6632bbdc":"markdown","732dbcb1":"markdown","84fee3d8":"markdown","37528829":"markdown","f3cced16":"markdown","a43f7851":"markdown","0b0d216f":"markdown","ce393ed8":"markdown","c9ad2e7d":"markdown","ad6f69b2":"markdown","a2e10831":"markdown","33025f15":"markdown","e3777961":"markdown","addb54a2":"markdown","4146b854":"markdown","50447d08":"markdown","6f1e05a6":"markdown","3afcbc31":"markdown","77638e13":"markdown","b39c23fb":"markdown","cdda1e7d":"markdown","9fbcac3e":"markdown","0823efc2":"markdown","4aa41e46":"markdown","b825cdaa":"markdown","0f5ac28e":"markdown","7b43b218":"markdown","bb862415":"markdown","e6826b69":"markdown","40f91933":"markdown","c35e4d51":"markdown","04fec455":"markdown","6d33daf8":"markdown","a769d8f1":"markdown","e9c3fa12":"markdown","b02de61c":"markdown","147a51a7":"markdown","a8541627":"markdown","337af861":"markdown","d1f2f1b2":"markdown","f65f3f72":"markdown","dda38753":"markdown","629dc5bd":"markdown","d2365446":"markdown","c0d7fa45":"markdown","5a5a7735":"markdown","ba160b38":"markdown"},"source":{"958df68e":"# Imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport datetime\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\n\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)\nimport os\nimport gc\ngc.enable()\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score\nfrom scipy.sparse import hstack\nfrom scipy import stats\n%matplotlib inline\nfrom datetime import timedelta\nimport datetime as dt\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [16, 10]\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nimport warnings\nwarnings.filterwarnings('ignore')\nimport urllib        #for url stuff\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text\nfrom IPython.display import display\nfrom tqdm import tqdm\nfrom collections import Counter\nimport ast\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom textblob import TextBlob\nimport scipy.stats as stats\n\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\nimport seaborn as sns #for making plots\nimport matplotlib.pyplot as plt # for plotting\nimport os  # for os commands\n\nimport gensim\nfrom gensim import corpora, models, similarities\nimport logging\nimport tempfile\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom collections import OrderedDict\n\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.manifold import TSNE\n\n\nfrom bokeh.plotting import figure, output_file, show\nfrom bokeh.models import Label\nfrom bokeh.io import output_notebook\noutput_notebook()\n\n\n!pip install chart_studio\nimport plotly\nimport chart_studio.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Data import\ndf = pd.read_csv(\"..\/input\/food-inspections-in-chicago\/Food_Inspections.csv\")","69105465":"%%capture\n!pip install -U folium\n\n# Import necessary packages \nfrom folium import folium, plugins\nfrom IPython.display import HTML\n\n%matplotlib inline","32f596ca":"df.head()","e0026a79":"df.rename(columns={\"License #\": \"license\"}, inplace=True)\n# Extract day, month and year from Inspection Date column\n#pandas datetimeindex docs: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DatetimeIndex.html\n#efficient way to extract year from string format date\ndf['year'] = pd.DatetimeIndex(df['Inspection Date']).year\ndf['month'] = pd.DatetimeIndex(df['Inspection Date']).month\ndf['day'] = pd.DatetimeIndex(df['Inspection Date']).day","3135b586":"df.head()","1312fba4":"df = df[df['Inspection Type'].notna()]\ndf = df[df['Inspection Type']=='Canvass']\n\ndf.shape","d1a4384f":"df.dtypes","bdaa956e":"df.dropna(subset=[\"Inspection Date\", \"license\", \"Latitude\", \"Longitude\"], inplace=True)","179408f7":"df = df[~df.Results.isin([\"Out of Business\", \"Business Not Located\", \"No Entry\", \"Not Ready\"])]","12482831":"len(set(df['DBA Name'].tolist()))","40e51031":"df['DBA Name'].value_counts()[:10]","dfcad30a":"data_risk1=df\nfig,ax=plt.subplots(1,2,figsize=(15,8))\nsns.barplot(x=data_risk1['DBA Name'].value_counts()[:10],y=data_risk1['DBA Name'].value_counts()[:10].index,ax=ax[0])\nax[0].set_title(\"Top 10 Facility Type by the counts of risk\",size=20)\nax[0].set_xlabel('counts',size=18)\n\n\ncount=data_risk1.groupby(['DBA Name'])['Inspection ID'].agg('count').sort_values(ascending=False)\ngroups=list(data_risk1.groupby(['DBA Name'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\ncounts=list(count[:10])\ncounts.append(count.agg(sum)-count[:10].agg('sum'))\ngroups.append('Other')\ntype_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\nclr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\ntype_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\nax[1].set_ylabel('')\nax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))","bbf0022d":"fig,ax=plt.subplots(2,2,figsize=(20,16))\ny=df['DBA Name'].value_counts()[:10].index\nx=df['DBA Name'].value_counts()[:10]\nsns.barplot(x=x,y=y,ax=ax[0,0])\nax[0,0].set_title(\"Top 10 DBA Name by the counts of inspection \",size=20)\nax[0,0].set_xlabel('counts',size=18)\nax[0,0].set_ylabel('')\n\nsns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=df[df['DBA Name']=='SUBWAY'], ax=ax[0,1])\nax[0,1].set_title(\"The distribution of inspections for SUBWAY\",size=20)\nax[0,1].set_xlabel('Longitude')\nax[0,1].set_ylabel('LATITUDE')\n\nsns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'],data=df[df['DBA Name']=='DUNKIN DONUTS'], ax=ax[1,0])\nax[1,0].set_title(\"The distribution of inspections for DUNKIN DONUTS\",size=20)\nax[1,0].set_xlabel('Longitude')\nax[1,0].set_ylabel('LATITUDE')\n\nsns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=df[df['DBA Name']=='7-ELEVEN'], ax=ax[1,1])\nax[1,1].set_title(\"The distribution of inspections for 7-ELEVEN\",size=20)\nax[1,1].set_xlabel('Longitude')\nax[1,1].set_ylabel('LATITUDE')","b4c2a336":"len(set(df['license'].tolist()))","cabad6cf":"len(set(df['Facility Type'].tolist()))","3ef3ab04":"from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n\nx = df['Facility Type'].value_counts().index.values.astype('str')[:10]\ny = df['Facility Type'].value_counts().values[:10]\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y\/len(df))][:10]\n\n\ntrace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Number of Facility Type',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Facility Type'))\nfig=dict(data=[trace1], layout=layout)\niplot(fig)","5eab12b2":"fig,ax=plt.subplots(2,2,figsize=(20,16))\ny=df['Facility Type'].value_counts()[:10].index\nx=df['Facility Type'].value_counts()[:10]\nsns.barplot(x=x,y=y,ax=ax[0,0])\nax[0,0].set_title(\"Top 10 Facility Type by the counts of inspection \",size=20)\nax[0,0].set_xlabel('counts',size=18)\nax[0,0].set_ylabel('')\n\nsns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=df[df['Facility Type']=='Restaurant'], ax=ax[0,1])\nax[0,1].set_title(\"The distribution of inspections for restaurant\",size=20)\nax[0,1].set_xlabel('Longitude')\nax[0,1].set_ylabel('LATITUDE')\n\nsns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'],data=df[df['Facility Type']=='Grocery Store'], ax=ax[1,0])\nax[1,0].set_title(\"The distribution of inspections for Grocery Store\",size=20)\nax[1,0].set_xlabel('Longitude')\nax[1,0].set_ylabel('LATITUDE')\n\nsns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=df[df['Facility Type']=='School'], ax=ax[1,1])\nax[1,1].set_title(\"The distribution of inspections for School\",size=20)\nax[1,1].set_xlabel('Longitude')\nax[1,1].set_ylabel('LATITUDE')","170beb43":"x = df['Risk'].value_counts().index.values.astype('str')\ny = df['Risk'].value_counts().values\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y\/len(df))]\n\n\ntrace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Type of Risk Count',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Risk'))\nfig=dict(data=[trace1], layout=layout)\niplot(fig)","5bda5871":"pd.crosstab([df.month,df.day],[df.Results,df.year],margins=True).style.background_gradient(cmap='summer_r')","5cef6b02":"df.groupby('year').Risk.value_counts().unstack().plot.barh()","75126bb4":"df.groupby('month').Risk.value_counts().unstack().plot.barh()","34b0b162":"df.groupby('year').Results.value_counts().unstack().plot.barh()","325c7739":"data_risk1=df[df.Risk=='Risk 1 (High)']\nfig,ax=plt.subplots(1,2,figsize=(15,8))\nsns.barplot(x=data_risk1['Facility Type'].value_counts()[:10],y=data_risk1['Facility Type'].value_counts()[:10].index,ax=ax[0])\nax[0].set_title(\"Top 10 Facility Type by the counts of risk 1 \",size=20)\nax[0].set_xlabel('counts',size=18)\n\n\ncount=data_risk1.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False)\ngroups=list(data_risk1.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\ncounts=list(count[:10])\ncounts.append(count.agg(sum)-count[:10].agg('sum'))\ngroups.append('Other')\ntype_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\nclr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\ntype_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\nax[1].set_ylabel('')\nax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))","4cc793d3":"import folium \n\ndata_risk1=df[df.Risk=='Risk 1 (High)']\n\ndata_risk1_2000=data_risk1[:2000]\nLong=data_risk1_2000.Longitude.mean()\nLat=data_risk1_2000.Latitude.mean()\nrisk1_map=folium.Map([Lat,Long],zoom_start=12)\n\nrisk1_distribution_map=plugins.MarkerCluster().add_to(risk1_map)\nfor lat,lon,label in zip(data_risk1_2000.Latitude,data_risk1_2000.Longitude,data_risk1_2000['AKA Name']):\n    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(risk1_distribution_map)\nrisk1_map.add_child(risk1_distribution_map)\n\nrisk1_map","12ebe6a9":"data_risk2=df[df.Risk=='Risk 2 (Medium)']\n\nfig,ax=plt.subplots(1,2,figsize=(15,8))\nsns.barplot(x=data_risk2['Facility Type'].value_counts()[:10],y=data_risk2['Facility Type'].value_counts()[:10].index,ax=ax[0])\nax[0].set_title(\"Top 10 Facility Type by the counts of risk 2 \",size=20)\nax[0].set_xlabel('counts',size=18)\n\n\ncount=data_risk2.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False)\ngroups=list(data_risk2.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\ncounts=list(count[:10])\ncounts.append(count.agg(sum)-count[:10].agg('sum'))\ngroups.append('Other')\ntype_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\nclr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\ntype_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\nax[1].set_ylabel('')\nax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))","4443a36d":"data_risk2_2000=data_risk2[:2000]\nLong=data_risk2_2000.Longitude.mean()\nLat=data_risk2_2000.Latitude.mean()\nrisk2_map=folium.Map([Lat,Long],zoom_start=12)\n\nrisk2_distribution_map=plugins.MarkerCluster().add_to(risk2_map)\nfor lat,lon,label in zip(data_risk2_2000.Latitude,data_risk2_2000.Longitude,data_risk2_2000['AKA Name']):\n    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(risk2_distribution_map)\nrisk2_map.add_child(risk2_distribution_map)\n\nrisk2_map","ded35117":"data_risk3=df[df.Risk=='Risk 3 (Low)']\n\nfig,ax=plt.subplots(1,2,figsize=(15,8))\nsns.barplot(x=data_risk3['Facility Type'].value_counts()[:10],y=data_risk3['Facility Type'].value_counts()[:10].index,ax=ax[0])\nax[0].set_title(\"Top 10 Facility Type by the counts of risk 3 \",size=20)\nax[0].set_xlabel('counts',size=18)\n\n\ncount=data_risk3.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False)\ngroups=list(data_risk3.groupby(['Facility Type'])['Inspection ID'].agg('count').sort_values(ascending=False).index[:10])\ncounts=list(count[:10])\ncounts.append(count.agg(sum)-count[:10].agg('sum'))\ngroups.append('Other')\ntype_dict=pd.DataFrame({\"group\":groups,\"counts\":counts})\nclr1=('brown','darksalmon','orange','hotpink','cadetblue','purple','red','gold','forestgreen','blue','plum')\ntype_dict.plot(kind='pie', y='counts', labels=groups,colors=clr1,autopct='%1.1f%%', pctdistance=0.9, radius=1.2,ax=ax[1])\nax[1].set_ylabel('')\nax[1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,1.2))","2707ba25":"data_risk3_2000=data_risk3[:2000]\nLong=data_risk3_2000.Longitude.mean()\nLat=data_risk3_2000.Latitude.mean()\nrisk3_map=folium.Map([Lat,Long],zoom_start=12)\n\nrisk3_distribution_map=plugins.MarkerCluster().add_to(risk3_map)\nfor lat,lon,label in zip(data_risk3_2000.Latitude,data_risk3_2000.Longitude,data_risk3_2000['AKA Name']):\n    folium.Marker(location=[lat,lon],icon=None,popup=label).add_to(risk3_distribution_map)\nrisk3_map.add_child(risk3_distribution_map)\n\nrisk3_map","8c621de4":"latest_data = df.sort_values('Inspection Date', ascending = False).groupby('license').head(1)\nlatest_data.dropna(subset=['Risk', 'Facility Type', 'DBA Name', 'Latitude', 'Longitude'], axis = 0, how = 'all', inplace = True)\nlatest_data = latest_data[(latest_data['Results'] != 'Out of Business') & (latest_data['Results'] != 'Business Not Located')]\nlatest_data['Name'] = latest_data.apply(lambda row: row['AKA Name'] if not pd.isnull(row['AKA Name']) else row['DBA Name'], axis = 1)\nlatest_data['Name'] = latest_data['Name'] + '<br>' + latest_data['Address']","2b8b9617":"risk_color_map = { \"All\": \"rgb(0, 0, 0)\", \"Risk 1 (High)\": \"rgb(255, 0, 0)\", \"Risk 2 (Medium)\": \"rgb(204, 204, 0)\", \"Risk 3 (Low)\": \"rgb(0, 100, 0)\" }\nlatest_data['Risk Color'] = latest_data['Risk'].map(risk_color_map)\n\ninspection_color_map = { \n    \"Pass\": \"rgb(0, 255, 0)\", \n    \"Pass w\/ Conditions\": \"rgb(0, 255, 0)\",\n    \"Fail\": \"rgb(255, 0, 0)\", \n    \"No Entry\": \"rgb(255, 0, 0)\", \n    \"Not Ready\": \"rgb(255, 0, 0)\" }\nlatest_data['Inspection Color'] = latest_data['Results'].map(inspection_color_map)\n    \nlatest_data.reset_index(inplace=True)\nprint(\"Total businesses: {}\".format(latest_data.shape[0]))","d23af384":"# Create and account on mapbox.com and get access token\nmapbox_access_token = \"pk.eyJ1IjoiaGFtZGl0YXJlazAxIiwiYSI6ImNraXl0eG1zODI0dGUydm1tdWoybHFsNmUifQ.JJry5XjNLcMXZTPmeGIKgw\"","3dd973a2":"data = [\n    go.Scattermapbox(\n        lat = latest_data['Latitude'],\n        lon = latest_data['Longitude'],\n        text = latest_data['Name'],\n        hoverinfo = 'text',\n        mode = 'markers',\n        marker = go.scattermapbox.Marker(\n            color = latest_data['Risk Color'],\n            opacity = 0.7,\n            size = 4\n        )\n    )\n]\n\nlayout = go.Layout(\n    mapbox = dict(\n        accesstoken = mapbox_access_token,\n        zoom = 10,\n        center = dict(\n            lat = 41.8781,\n            lon = -87.6298\n        ),\n    ),\n    height = 800,\n    width = 800,\n    title = \"Facilities in Chicago\")\n\nfig = go.Figure(data, layout)\niplot(fig, filename = 'facilities')\n","d0170f29":"data = [\n    go.Scattermapbox(\n        lat = latest_data['Latitude'],\n        lon = latest_data['Longitude'],\n        text = latest_data['Name'],\n        hoverinfo = 'text',\n        mode = 'markers',\n        marker = go.scattermapbox.Marker(\n            color = latest_data['Inspection Color'],\n            opacity = 0.7,\n            size = 4\n        )\n    )\n]\n\nlayout = go.Layout(\n    mapbox = dict(\n        accesstoken = mapbox_access_token,\n        zoom = 10,\n        center = dict(\n            lat = 41.8781,\n            lon = -87.6298\n        ),\n    ),\n    height = 800,\n    width = 800,\n    title = \"Facilities in Chicago\")\n\nfig = go.Figure(data, layout)\niplot(fig, filename = 'facilities')","e1d3af88":"x = df['Results'].value_counts().index.values.astype('str')\ny = df['Results'].value_counts().values\npct = [(\"%.2f\"%(v*100))+\"%\"for v in (y\/len(df))]\n\n\ntrace1 = go.Bar(x=x, y=y, text=pct)\nlayout = dict(title= 'Inspectation Results Count',\n              yaxis = dict(title='Count'),\n              xaxis = dict(title='Results'))\nfig=dict(data=[trace1], layout=layout)\niplot(fig)","4e146d24":"df.groupby(['Risk', 'Results']).size().reset_index(name=\"Frequency\")","21a7d848":"sns.heatmap(pd.crosstab([df.Risk], [df.Results]),\n            cmap=\"YlGnBu\", annot=True, fmt=\".1f\", linewidths=1.0, square=1, cbar=False)","b2704bff":"fig,ax=plt.subplots(2,2,figsize=(20,16))\nx=df.Results.value_counts().index\ny=df.Results.value_counts()\nsns.barplot(x=x,y=y,ax=ax[0,0])\nax[0,0].set_title(\"The counts of Results of inspection \",size=20)\nax[0,0].set_ylabel('counts',size=18)\nax[0,0].set_xlabel('')\n\ndf.groupby(['Results','year'])['Inspection ID'].agg('count').unstack('Results').plot(kind='bar',ax=ax[0,1])\nax[0,1].tick_params(axis='x',labelrotation=360)\nax[0,1].legend(loc=0, ncol=1, fontsize=14,bbox_to_anchor=(1.15,0.75))\nax[0,1].set_title(\"The counts of results of inspection by year \",size=20)\nax[0,1].set_ylabel('counts',size=18)\n\nsns.scatterplot(x='Longitude',y='Latitude',hue='Risk' ,hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'],data=df[df.Results=='Pass'], ax=ax[1,0])\nax[1,0].set_title(\"The distribution of result is pass\",size=20)\nax[1,0].set_xlabel('Longitude')\nax[1,0].set_ylabel('LATITUDE')\n\nsns.scatterplot(x='Longitude',y='Latitude',hue='Risk',hue_order=['Risk 1 (High)','Risk 2 (Medium)','Risk 3 (Low)'] ,data=df[df.Results=='Fail'], ax=ax[1,1])\nax[1,1].set_title(\"The distribution of result is fail\",size=20)\nax[1,1].set_xlabel('Longitude')\nax[1,1].set_ylabel('LATITUDE')","add61671":"df['Restaurant'] = (df['Facility Type'].values == 'Restaurant').astype('int')","fbc36b5f":"sns.heatmap(pd.crosstab([df.Restaurant], [df.Risk]),\n            cmap=\"YlGnBu\", annot=True, fmt=\".1f\", linewidths=1.0, square=1, cbar=False)","7a9bf635":"sns.heatmap(pd.crosstab([df.Restaurant], [df.Results]),\n            cmap=\"YlGnBu\", annot=True, fmt=\".1f\", linewidths=1.0, square=1, cbar=False)","060ab6bc":"df.drop_duplicates(\"Inspection ID\", inplace=True)","cc374969":"df.shape","c8fcb1d2":"# Find minimum and maximum values for latitude and longitude\nla = df['Latitude'].tolist()\nlo =df['Longitude'].tolist()\nprint('The minimum value for the latitude is: '+str(min(la)))\nprint('The maximum value for the latitude is: '+str(max(la)))\nprint('The minimum value for the longitude is: '+str(min(lo)))\nprint('The maximum value for the longitude is: '+str(max(lo)))","050782cc":"m = folium.Map([41.8600, -87.6298], zoom_start=10)\n\n# Convert to (n, 2) nd-array format for heatmap\ninspections_arr = df.sample(20000)[[\"Latitude\", \"Longitude\"]].values\n\n# Plot heatmap\nm.add_child(plugins.HeatMap(inspections_arr.tolist(), radius=10))","e6d57a4f":"# Drop rows with \"nan\" value in violation column\ndf['Violations'] = df['Violations'].astype(str)\ndf = df[df.Violations != \"nan\"]\ndf.shape","9f6911af":"df.iloc[0].Violations","face4733":"import re\nviolators = latest_data.dropna(subset=['Violations'], axis = 0, how = 'all')\nviolations = violators.apply(lambda row: re.findall('\\|\\s([0-9]+)[.]', str(row['Violations'])), axis = 1)\nfirst_violations = violators.apply(lambda row: row['Violations'].split('.')[0], axis = 1)\n\nfor violation, first_violation in zip(violations, first_violations):\n    violation.append(first_violation)\n\nflat_list = [item for sublist in violations for item in sublist]\nunique, counts = np.unique(flat_list, return_counts=True)","244c036e":"violation = []\nviolation_count = []\nfor value, count in zip(unique, counts):\n    if count > 100:\n        violation.append(unique)\n        violation_count.append(count)","1ca6e046":"data = [\n    go.Bar(\n        x = violation,\n        y = violation_count,\n        marker = dict(\n            color = 'rgb(55, 83, 109)'\n        )\n    )\n]\n\nlayout = go.Layout(\n    title = 'Majority Violations',\n)\n\nfig = go.Figure(data = data, layout = layout)\niplot(fig, filename = 'violations')","5a4e30c0":"def split_violations(violations):\n    values_row = pd.Series([])\n    if type(violations) == str:\n        violations = violations.split(' | ')\n        for violation in violations:\n            index = \"v_\" + violation.split('.')[0]\n            values_row[index] = 1\n    return values_row\n\n# Calculate violation values (5 mins), set missing violations to 0\nvalues_data = df.Violations.apply(split_violations).fillna(0)\n\n# Generate column names\ncritical_columns = [(\"v_\" + str(num)) for num in range(1, 15)]\nserious_columns = [(\"v_\" + str(num)) for num in range(15, 30)]\nminor_columns = [(\"v_\" + str(num)) for num in range(30, 45)]\nminor_columns.append(\"v_70\")\n\n# Create complete list of column names\ncolumns = critical_columns + serious_columns + minor_columns\n\n# Create dataframe using column names, violation data and inspection ID\nvalues = pd.DataFrame(values_data, columns=columns)\nvalues['Inspection ID'] = df['Inspection ID']","dc5e1d79":"# Display values dataframe\nprint(values.shape)\nvalues.head()","5806328a":"counts = pd.DataFrame({\n    \"critical_count\": values[critical_columns].sum(axis=1),\n    \"serious_count\": values[serious_columns].sum(axis=1),\n    \"minor_count\": values[minor_columns].sum(axis=1)\n})\n\ncounts['Inspection ID'] = df['Inspection ID']","918391c9":"# Display counts dataframe\nprint(counts.shape)\ncounts.head()","db9a9aea":"%%capture\n!pip install wordcloud","7597fde8":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\n# Extract comments from violations\ndef get_comments(violations):\n    comments = \"\"\n    if type(violations) == str:\n        violations = violations.split(' | ')\n        for violation in violations:\n            violation = violation.split('Comments:')\n            if len(violation) == 2:\n                comments += violation[1]\n    return comments\n\n# Concatenate all comments\ncomments = df.Violations.apply(get_comments).str.cat(sep=\" \")\n\n# Generate wordcloud\ncomments_wordcloud = WordCloud().generate(comments)\n\n# Plot wordcloud\nplt.rcParams['figure.figsize'] = (10, 10)\nplt.imshow(comments_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","fb78d392":"titles = pd.DataFrame({\n    \"v_1\": \"Approved food sources (1)\",\n    \"v_2\": \"Hot\/cold storage facilities (2)\",\n    \"v_3\": \"Hot\/cold storage temp. (3)\",\n    \"v_4\": \"Contaminant protection (4)\",\n    \"v_5\": \"No sick handlers (5)\",\n    \"v_6\": \"Proper hand washing (6)\",\n    \"v_7\": \"Proper utensil washing (7)\",\n    \"v_8\": \"Proper sanitizing solution (8)\",\n    \"v_9\": \"Hot\/cold water supply (9)\",\n    \"v_10\": \"Waste water disposal (10)\",\n    \"v_11\": \"Adequate toilet facilities (11)\",\n    \"v_12\": \"Adequate hand washing facilities (12)\",\n    \"v_13\": \"Control of rodents, other pests (13)\",\n    \"v_14\": \"Correct serious violations (14)\",\n    \"v_15\": \"No re-served food (15)\",\n    \"v_16\": \"Protection from contamination (16)\",\n    \"v_17\": \"Proper thawing (17)\",\n    \"v_18\": \"Pest control, associated areas (18)\",\n    \"v_19\": \"Proper garbage area (19)\",\n    \"v_20\": \"Proper garbage storage (20)\",\n    \"v_21\": \"Oversight of hazardous food (21)\",\n    \"v_22\": \"Dishwasher maintenance (22)\",\n    \"v_23\": \"Scrape before washing (23)\",\n    \"v_24\": \"Proper dishwashers (24)\",\n    \"v_25\": \"Minimize toxic materials (25)\",\n    \"v_26\": \"Adequate customer toilets (26)\",\n    \"v_27\": \"Supplied toilet facilities (27)\",\n    \"v_28\": \"Visible inspection report (28)\",\n    \"v_29\": \"Correct minor violations (29)\",\n    \"v_30\": \"Labelled containers (30)\",\n    \"v_31\": \"Sterile utensils (31)\",\n    \"v_32\": \"Clean, maintain equipment (32)\",\n    \"v_33\": \"Clean, sanitize utensils (33)\",\n    \"v_34\": \"Clean, maintain floor (34)\",\n    \"v_35\": \"Maintain walls & ceiling (35)\",\n    \"v_36\": \"Proper lighting (36)\",\n    \"v_37\": \"Toilet rooms vented (37)\",\n    \"v_38\": \"Proper venting, plumbing (38)\",\n    \"v_39\": \"Linen, clothing storage (39)\",\n    \"v_40\": \"Proper thermometers (40)\",\n    \"v_41\": \"Clean facilities, store supplies (41)\",\n    \"v_42\": \"Ice handling, hairnets, clothes (42)\",\n    \"v_43\": \"Ice equipment storage (43)\",\n    \"v_44\": \"Restrict prep area traffic (44)\",\n    \"v_70\": \"Restrict smoking (70)\"\n}, index=[0])","67eed423":"titles","655a9d8b":"# Change the name of columns in value dataframe by the title values dataframe's columns\n\ntitled_values = values.rename(columns=titles.iloc[0])\n\n# Sum binary values for each violation\nsums = titled_values.drop(\"Inspection ID\", axis=1).sum()\n\n# Generate color list\ncolors = [\"red\"]*15 + [\"orange\"]*14 + [\"green\"]*16\n\n# Sort sums and colors by sum value\nsum_data = pd.DataFrame({\"sums\": sums, \"colors\": colors}).sort_values(\"sums\")\n\n# Plot bar chart\nplt.rcParams['figure.figsize'] = (10, 10)\nax = sum_data.sums.plot(kind=\"barh\", color=sum_data.colors)\nax.set_title(\"Health Code Violations\", fontsize=25)\nax.set_xlabel(\"Violation Count\", fontsize=15)\nax.invert_yaxis()\nplt.show()","2678d449":"titled_values","0628e35e":"# Sort inspections by date\ndf = df.sort_values(by=\"Inspection Date\")\n\n# Only consider inspections with clear results\ndf = df.loc[df.Results.isin([\"Pass\", \"Fail\"])]\n\n# Group inspections by license and shift 1 to find previous results\ndf[\"previous_results\"] = df.groupby(by=\"license\").shift().Results\n\n# Calculate cross tabulation of results and previous results\nchart = pd.crosstab(df.previous_results, df.Results)\n\n# Make Numpy array of total counts of prior fails and passes with the \n# following(post) results\nchart_arr = np.array(chart)\n\n# Create new dataframe from Numpy array to clearly dispay prior and \n# post results\npass_fail_chart = pd.DataFrame({\"Prior Fail\":chart_arr[:,0],\n                                \"Prior Pass\":chart_arr[:,1]})\npass_fail_chart.index = pass_fail_chart.index.rename(\"\")\npass_fail_chart = pass_fail_chart.rename(index={0:\"Post Fail\",1:\"Post Pass\"})\n\n# Display chart\npass_fail_chart","07cdcf17":"fail_fail_probability = (pass_fail_chart.loc[\"Post Fail\", \"Prior Fail\"] \/\npass_fail_chart.loc[\"Post Fail\", :].sum())\n\nprint(str(100*fail_fail_probability) + \" of prior fails resulted in a subsequent fail\")","e889d0ce":"pass_fail_probability = (pass_fail_chart.loc[\"Post Pass\", \"Prior Fail\"] \/\npass_fail_chart.loc[\"Post Pass\", :].sum())\n\nprint(str(100*pass_fail_probability)+ \" of prior passes resulted in a subsequent fail\")","45b26b4b":"print(str(100*(1 - (pass_fail_probability \/ fail_fail_probability))) +\" more likely that a prior fail will predict a subsequent fail than a prior pass\")","6d7c5db6":"set(df['Risk'].tolist())","094dbc02":"# Create temporary dataframe\ntemp = pd.merge(df, values, on=\"Inspection ID\")\n\n# Convert inspection_date to datetime format\ntemp[\"datetime\"] = pd.to_datetime(temp[\"Inspection Date\"])\n\n# Define a function to map fines fees to their values \ndef set_value(row_number, assigned_value): \n    return assigned_value[row_number] \n  \n# Create the dictionary \nfees_dictionary ={'Risk 1 (High)' : 600, 'Risk 2 (Medium)' : 400, 'Risk 3 (Low)' : 200} \n  \n# Add a new column named 'Price' \ntemp['fines'] = temp['Risk'].apply(set_value, args =(fees_dictionary, )) \n\n\n# Count critical violations\ntemp[\"criticals\"] = temp[critical_columns].sum(axis=1)\n\n# Display snapshot of temp dataset\ntemp.head()","d6f87c7a":"import math\n\n# Sort by date\ntemp.sort_values(\"datetime\", inplace=True)\n\n# Calculate statistics for license groups\ndef get_stats(group):\n    days = (group.iloc[-1].datetime - group.iloc[0].datetime).days + 1\n    years = days \/ 365.25\n    inspections = len(group)\n    yearly_inspections = inspections \/ math.ceil(years)\n    fails = len(group[group.Results == \"Fail\"])\n    yearly_fails = fails \/ math.ceil(years)\n    fines = group.fines.sum()\n    yearly_fines = fines \/ math.ceil(years)\n    criticals = group.criticals.sum()\n    yearly_criticals = criticals \/ math.ceil(years)\n    return pd.Series({\n        \"years\": years,\n        \"inspections\": inspections,\n        \"yearly_inspections\": yearly_inspections,\n        \"fails\": fails,\n        \"yearly_fails\": yearly_fails,\n        \"fines\": fines,\n        \"yearly_fines\": yearly_fines,\n        \"criticals\": criticals,\n        \"yearly_criticals\": yearly_criticals\n    })\n\n# Group by license and apply get_stats\nbusinesses = temp.groupby('license').apply(get_stats).reset_index()","50a58433":"# Display snapshot of business stats dataset\nbusinesses.head()","932710a5":"# Define function to plot pareto chart\ndef plot_pareto(series, title, xlabel, ylabel, line_color):\n    index = np.arange(len(series))\n    fig, ax1 = plt.subplots()\n    ax1.bar(index, series)\n    ax1.plot(index, series.cumsum(), color=line_color)\n    ax1.set_xticks(index)\n    ax1.set_xticklabels(series.index)\n    ax1.set_title(title, fontsize=25)\n    ax1.set_xlabel(xlabel, fontsize=15)\n    ax1.set_ylabel(ylabel, fontsize=15)\n    ax2 = ax1.twinx()\n    ax2.set_yticks([1, 2, 3, 4, 5, 5.25])\n    ax2.set_yticklabels([20 for x in range(1,6)])\n    plt.show()\n\n# Count restaurants in each rounded yearly fails bracket\nyearly_fail_counts = businesses.yearly_fails.round(1).value_counts()\n\n# Plot pareto chart\nplt.rcParams['figure.figsize'] = (10, 7)\nplot_pareto(\n    yearly_fail_counts,\n    \"Yearly Failed Inspections\",\n    \"Average Yearly Failed Inspections\",\n    \"Businesses\", \"orange\"\n)","a79aaac6":"# Fraction of businesses failing .5 or more inspections yearly\nlen(businesses.loc[businesses.yearly_fails >= .5]) \/ len(businesses)","2f1d4d4b":"# Fraction of businesses failing 1 or more inspections yearly\nlen(businesses.loc[businesses.yearly_fails >= 1]) \/ len(businesses)","53dd5302":"# Count restaurants in each rounded yearly fails bracket\nyearly_critical_counts = businesses.yearly_criticals.round().value_counts()\n\n# Plot pareto chart\nplot_pareto(\n    yearly_critical_counts,\n    \"Yearly Critical Violations\",\n    \"Average Yearly Critical Violations\",\n    \"Businesses\", \"red\"\n)","335265c0":"# Begin by using the temp dataset to extract the dates of inpections\n# Example visual of the date information\ntemp.datetime[1]","9ef735d0":"# Calculate total fines paid for each month\ntemp[\"month\"] = temp.datetime.apply(lambda x: x.month)\nmonth_fines = temp.groupby(\"month\").fines.sum() \/ temp.fines.sum()\n\n# List months\nmonths = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n\n# Plot bar chart\nfig, ax = plt.subplots()\nindex = np.arange(len(month_fines))\nax.barh(index, month_fines)\nax.set_yticks(index)\nax.set_yticklabels(months, fontsize=15)\nax.set_xticks([x\/100 for x in range(5)])\nax.set_xticklabels([x for x in range(5)])\nax.set_title(\"Fines by Month\", fontsize=25)\nax.set_xlabel(\"Percent of All Fines\", fontsize=15)\n\n# Display chart\nplt.show()","4056899f":"# Count restaurants in each yearly fines bracket\nfine_counts = businesses.yearly_fines.round(-2).value_counts().sort_index()\n\nindex = np.arange(len(fine_counts))\n\n# Plot bar chart\nfig, ax = plt.subplots()\nax.bar(index, fine_counts)\nax.set_xticks(index[::5])\nax.set_xticklabels([x for x in fine_counts.index[::5]])\nax.set_title(\"Yearly Fines\", fontsize=25)\nax.set_xlabel(\"Yearly Amount Paid in $\", fontsize=15)\nax.set_ylabel(\"Businesses\", fontsize=15)\n\n# Display chart\nplt.show()","0bf22162":"# Fraction of businesses paying $600 or more yearly\nlen(businesses.loc[businesses.yearly_fines >= 600]) \/ len(businesses)","f824a6ce":"# Fraction of businesses paying $250 or more yearly\nlen(businesses.loc[businesses.yearly_fines >= 200]) \/ len(businesses)","67897f61":"# Create line graph showing change in total average yearly fines for all businesses\n\n# Calculate total fines paid for each year\n\ntemp[\"year\"] = temp.datetime.apply(lambda x: x.year)\nyear_fines = temp.groupby(\"year\").fines.sum()\n\n\n# Caclulate average yearly fines plot line\nfines_mean = [np.mean(year_fines)]*len(year_fines)\n\n# Create y axis labels\nfines_list = list(year_fines)\nfines_list.sort()\n\n# Format fines axis\ndef millions_format(num, m=1000000):\n    if num % m == 0:\n        num = int(num\/m)\n    else:\n        num = round(float(num\/m), 2)\n    return \"${} million\".format(num)\n\n# Plot line graph\nfig, ax = plt.subplots()\nax.plot(year_fines, label=\"Yearly Fines\", marker=\"D\")\n\n# Display average on graph\nax.text(2010, 2000000, r'Average businesses spend per year:')\nax.text(2010.7, 1930000, millions_format(fines_mean[0]))\n\n# Plot the mean line\nax.plot(year_fines.index, fines_mean, label=\"Mean\", linestyle=\"--\")\n\n# Set axis labels and title\nax.set_xlabel(\"Year\", fontsize=15)\nax.set_ylabel(\"Dollars\", fontsize=15)\nax.set_yticklabels([millions_format(number) for number in fines_list])\nax.set_title(\"Total Fines Per Year\", fontsize=25)\n\n# Make a legend\nlegend = ax.legend(loc='center right')\n\n# Display graph\nplt.show()","66fcd90d":"# Convert inspection_date to datetime format\ndf[\"datetime\"] = pd.to_datetime(df[\"Inspection Date\"])\n\ndf.sort_values(\"datetime\", inplace=True)","40d5f6c3":"df.head(50)","aadb37b5":"train = pd.merge(df, titled_values, on='Inspection ID')\ntrain = pd.merge(train, counts, on='Inspection ID')\n\n# We will use it to show result and location for next inspectations\n# 13452 is the length of test set, we will see it in th next cells\nresult = train[['DBA Name', 'AKA Name', 'Address', 'Zip', 'Latitude', 'Longitude']].tail(13452)","3f100e7f":"print(train.shape)\ntrain.tail(3)","31d1833a":"# labelEncoder present in scikitlearn library \n\nfrom sklearn.preprocessing import LabelEncoder \n\nle = LabelEncoder() \ntrain['DBA Name'] = le.fit_transform(train['DBA Name'])\ntrain['Facility Type'] = train['Facility Type'].astype(str)\ntrain['Facility Type'] = le.fit_transform(train['Facility Type'])\ntrain['Address'] = le.fit_transform(train['Address'])\ntrain['Zip'] = le.fit_transform(train['Zip'])\ntrain['Results'] = (train['Results'].values == 'Fail').astype('int')\ntrain['Risk'] = le.fit_transform(train['Risk'])","abaa949d":"train.head(2)","aa85d163":"train = train.drop(['Inspection ID', 'AKA Name', 'City', 'State', 'Inspection Date', \n              'Inspection Type', 'Violations', 'Location', 'previous_results', 'datetime'], axis = 1)","c6cc63c0":"import xgboost as xgb\nprint(\"XGBoost version:\", xgb.__version__)","b1cd5418":"x = train['Risk'].value_counts().index\ny = train['Risk'].value_counts().values\n\ntrace2 = go.Bar(\n     x=x ,\n     y=y,\n     marker=dict(\n         color=y,\n         colorscale = 'Viridis',\n         reversescale = True\n     ),\n     name=\"Imbalance\",    \n )\nlayout = dict(\n     title=\"Data imbalance - Risk Type\",\n     #width = 900, height = 500,\n     xaxis=go.layout.XAxis(\n     automargin=True),\n     yaxis=dict(\n         showgrid=False,\n         showline=False,\n         showticklabels=True,\n #         domain=[0, 0.85],\n     ), \n)\nfig1 = go.Figure(data=[trace2], layout=layout)\niplot(fig1)","b3bcaacf":"from sklearn.model_selection import train_test_split\n# create dataset\nX = train.drop(['Risk'], axis=1)\ny = train['Risk']\n# split into train test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","1681086f":"clf1 = xgb.XGBClassifier(\n    n_estimators=1000,\n    max_depth=7,\n    learning_rate=0.4,\n    #subsample=0.9,\n    colsample_bytree=0.6,\n    missing=-999,\n    random_state=2020,\n    tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n)\n\n\n# Fit model\n%time clf1.fit(X_train, y_train)\nprint('\/\/\/\/\/\/\/\/\/\/'*10)\nprint('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'*10)\n# make predictions\nfrom sklearn.metrics import accuracy_score\n\nyhat1 = clf1.predict(X_test)\n\n# evaluate predictions\nacc = accuracy_score(y_test, yhat1)\nprint('Accuracy: %.3f' % acc)\n\nprint('\/\/\/\/\/\/\/\/\/\/'*10)\nprint('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'*10)\n# confusion_matrix\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, yhat1)","15d7edb4":"X_train_lgb = X_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\nX_test_lgb = X_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))","341ecb6a":"# lgb_params\n\nSEED = 42\n\nlgb_params = {\n    'bagging_freq': 7,\n    'bagging_fraction': 0.9,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.6,\n    'learning_rate': 0.45,\n    'max_depth': 13,\n    'metric':'multi_logloss',\n    #'min_data_in_leaf': 80,\n    #'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 195,\n    'num_threads': 25,\n    'tree_learner': 'serial',\n    'objective': 'multiclass', \n    'verbosity': 1\n}\n\nimport lightgbm as lgb\nclf2 = lgb.LGBMClassifier(**lgb_params)\n\n\n# Fit model\nclf2.fit(X_train_lgb, y_train)\n\n\n\n# predict the results\nyhat2 = clf2.predict(X_test_lgb)\n\n# view accuracy\naccuracy = accuracy_score(yhat2, y_test)\nprint('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, yhat2)))\n\nprint('\/\/\/\/\/\/\/\/\/\/'*10)\nprint('\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\'*10)\n\n# confusion_matrix\nconfusion_matrix(y_test, yhat2)","b768fc74":"from catboost import CatBoostClassifier\n\n\nclf3 = CatBoostClassifier(iterations=100, learning_rate=0.07, l2_leaf_reg=3.5, \n                           depth=15, rsm=0.98, loss_function= 'MultiClass', eval_metric='Accuracy', \n                           metric_period=20, use_best_model=True,random_seed=42)\n\n\nclf3.fit(X_train, y_train, eval_set=(X_test, y_test))\n\n\n# predict the results\nyhat3 = clf3.predict(X_test)\n\n\n# view accuracy\naccuracy = accuracy_score(yhat3, y_test)\nprint('CatBoostClassifier Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, yhat3)))","fb98df34":"train = pd.merge(df, titled_values, on='Inspection ID')\ntrain = pd.merge(train, counts, on='Inspection ID')","5326dfdd":"train = pd.concat( [train, pd.get_dummies(train['Results']),\n                   pd.get_dummies(train['Risk'])] , axis = 1)\nprint(\"All Data Shape: \", train.shape)","7757a4d5":"train.loc[:, train.columns.str.contains('Risk')].head()","c5a8f806":"train = train.drop(['Inspection ID', 'DBA Name', 'AKA Name', 'Facility Type', 'City', 'Address', \n                    'State', 'Inspection Date', 'Inspection Type', 'Violations', 'Location', \n                    'Zip', 'Results', 'Risk', 'previous_results', 'datetime'], axis = 1)","15fab747":"# create dataset\nX = train.drop(['Risk 1 (High)', 'Risk 2 (Medium)', 'Risk 3 (Low)'], axis=1)\ny = train[['Risk 1 (High)', 'Risk 2 (Medium)', 'Risk 3 (Low)']]\n# split into train test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","fa2344de":"from tensorflow import keras\nfrom tensorflow.keras import layers\n\nmodel = keras.Sequential([\n    layers.BatchNormalization(input_shape=[57]),\n    layers.Dense(512, activation='relu'), \n    layers.BatchNormalization(),\n    layers.Dropout(rate=0.5),\n    layers.Dense(256, activation='relu'), \n    layers.BatchNormalization(),\n    layers.Dropout(rate=0.5),\n    layers.Dense(3, activation='sigmoid'),\n])","4fe1c9b1":"model.summary()","5ab6f6e1":"model.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n)","90512619":"early_stopping = keras.callbacks.EarlyStopping(\n    patience=10,\n    min_delta=0.001,\n    restore_best_weights=True,\n)\n\nhistory = model.fit(\n    X_train, y_train,\n    validation_data=(X_test, y_test),\n    batch_size=256,\n    epochs=100,\n    callbacks=[early_stopping],\n    verbose=1, # hide the output because we have so many epochs\n)","bc42f206":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot(title=\"Cross-entropy\")\nhistory_df.loc[:, ['accuracy', 'val_accuracy']].plot(title=\"Accuracy\")\n\nprint((\"Best Validation Loss: {:0.4f}\" +\\\n      \"\\nBest Validation Accuracy: {:0.4f}\")\\\n      .format(history_df['val_loss'].min(), \n              history_df['val_accuracy'].max()))","fae220fc":"result['Prediction'] = yhat1","69a2522c":"result.head()","fb6e18c3":"result = result[result['Prediction']!=2]","27c8163f":"result = result.sort_values('Prediction', ascending=False)","2c122480":"result['Prediction'] = result['Prediction'].astype(str)\nresult['Name'] = result.apply(lambda row: row['AKA Name'] if not pd.isnull(row['AKA Name']) else row['DBA Name'], axis = 1)\nrisk_color_map = { \"0\": \"rgb(255, 0, 0)\", \"1\": \"rgb(204, 204, 0)\"}\nresult['Risk Color'] = result['Prediction'].map(risk_color_map)\n#result.reset_index(inplace=True)\nprint(\"Total businesses: {}\".format(result.shape[0]))","ea7cb844":"data = [\n    go.Scattermapbox(\n        lat = result['Latitude'],\n        lon = result['Longitude'],\n        text = result['Name'],\n        hoverinfo = 'text',\n        mode = 'markers',\n        marker = go.scattermapbox.Marker(\n            color = result['Risk Color'],\n            opacity = 0.7,\n            size = 4\n        )\n    )\n]\n\nlayout = go.Layout(\n    mapbox = dict(\n        accesstoken = mapbox_access_token,\n        zoom = 10,\n        center = dict(\n            lat = 41.8781,\n            lon = -87.6298\n        ),\n    ),\n    height = 800,\n    width = 800,\n    title = \"Facilities in Chicago\")\n\nfig = go.Figure(data, layout)\niplot(fig, filename = 'facilities')\n","d03582c8":"data = [\n    go.Scattermapbox(\n        lat = result['Latitude'],\n        lon = result['Longitude'],\n        text = result['Name'],\n        hoverinfo = 'text',\n        mode = 'markers',\n        marker = go.scattermapbox.Marker(\n            color = result['Risk Color'],\n            opacity = 0.7,\n            size = 4\n        )\n    )\n]\n\nlayout = go.Layout(\n    mapbox = dict(\n        accesstoken = mapbox_access_token,\n        zoom = 10,\n        center = dict(\n            lat = 41.8781,\n            lon = -87.6298\n        ),\n    ),\n    height = 800,\n    width = 800,\n    title = \"Facilities in Chicago\")\n\nfig = go.Figure(data, layout)\niplot(fig, filename = 'facilities')\n","19384c0a":"def next_inpectations(result, n):\n    for i in range(n):\n        parts = result.iloc[i*12:12*(i+1),:]\n        data = [\n            go.Scattermapbox(\n                lat = parts['Latitude'],\n                lon = parts['Longitude'],\n                text = parts['Name'],\n                hoverinfo = 'text',\n                mode = 'markers',\n                marker = go.scattermapbox.Marker(\n                    color = parts['Risk Color'],\n                    opacity = 0.7,\n                    size = 10\n                )\n            )\n        ]\n\n        layout = go.Layout(\n            mapbox = dict(\n                accesstoken = mapbox_access_token,\n                zoom = 10,\n                center = dict(\n                    lat = 41.8781,\n                    lon = -87.6298\n                ),\n            ),\n            height = 800,\n            width = 800,\n            title = \"Facilities in Chicago\")\n\n        fig = go.Figure(data, layout)\n        iplot(fig, filename = 'facilities')","248f6072":"next_inpectations(result, 5)","69187a2f":"# Visualization for Facility Type based on Risk type","d053a78c":"## We found that 53% of businesses pay 600 dollars or more per year in fines and 97% pay 200 dollars or more.\n\n### Finally, we calculated the total amount businesses spent each year on food inspection violations. After plotting the total fines by year, the average was calculated to provide an overall view of the bottom line potential in helping businesses predict and remedy potential violaions.","5c9e6b96":"## Sort the results in descending  order based on the Prediction column","3fafa981":"## We then plotted a similar pareto chart to explore the number of businesses in each bracket of yearly critical violations:","b8825a10":"# Go deeper with Results Analysis\n\n## Success and Failure\n\n### Next, we take a look to facilities that passed the inspection and the ones that did not.\n\n### We can consider Pass and Pass w\/ Conditions to be positive outcome and the remaining as negative. Taking a look to the previous visualiazation about Results distribuation, we can see that even though there are many facilities with high risk, most pass the inspection none the less. Let's plot these on a map.","0b52e8b6":"# Top 10 Facilities inspected","54f41989":"## Though the chart of fines per year shows an upward trend of fines increasing, there are key factors that can explain this rising cost. Fewer inspections and fewer business licenses will result in less food violation fines. However, as populations grow and weather patterns shift, there will be a higher demand for more food services, more risk for violation 3 (hot and cold food storage at proper temperatures), and overall more opportunities for violations to occur.\n## It's clear that there is a big dump in the chart for 2020, I think because of COVID-19 most Businesses are closed, so there are not many inspections during this period.","0130af87":"### pass\/fail probability vs fail\/fail probability","48c4ddee":"# What is DBA name?\n#### In the U.S., a DBA lets the public know who the real owner of a business is. The DBA is also called a fictitious business name or assumed business name. It got its origins as a form of consumer protection, so dishonest business owners couldn't try to avoid legal trouble by operating under a different name.","67fb41b6":"## We will select only predictions with Risk 1 (High) and Risk 2 (Medium) because they maximize revenue","f8f7dad8":"# ECONOMIC IMPACT","43489a31":"# INSPECTION HISTORY\n\n#### The Chicago team found one of the greatest predictors of critical violations and failed inspections to be the establishment's recent inspection history. To validate this we grouped inspections by license, shifted each group to find the previous inspection and set up a table to compare conditional likelihoods:","c1d48f02":"## Create a Risk Color column which will help in plotting colors for each facility based on Risk.\n\n1. All -> Black\n2. High Risk -> Red\n3. Medium Risk -> Yellow\n4. Low Risk -> Green\n\n## For inspections, I'll crate the Inspection Color column.\n\n1. Pass or Pass w\/ Conditions -> Green\n2. Fail or No Entry or Not Ready -> Red","65963923":"# COMMENTS WORDCLOUD","e84c4686":"# Count of Inspectation Results","68133675":"# Install folium for map visualization","4d424ac6":"# Number of Different types of licences","401290ec":"# Modelingfrom sklearn import preprocessing","92aca407":"# Split violations into binary values for each violation\n","695093d5":"# NLP for VIOLATIONS Description\n\n#### The data contain violations column which contain the number of violations with comment for each one.","80de2c57":"### Drop unuseful features\n\n* Inspection ID\n* AKA Name\n* City\t\n* State\t\n* Inspection Date\t\n* Inspection Type\n* Violations\n* Location","dd8e37de":"# Keras Neural Networks and Deep Learning","e8b3b8cd":"### We then grouped inspections by license (a code shared by all inspections for a business) and for each group determined the age, yearly fails and other statistics:","6a1b9185":"# Number of Different types of Facilities","fdb8fbeb":"### As this plot shows, roughly 20% of businesses experience one or more critical violations each year.\n\n### To see if violations could be easily predicted as a function of seasonal temperature, we plotted the percentage of all fines paid for each month:","6632bbdc":"### Violation fines clearly show strong seasonal variation, with more than twice as much paid in June as in December.\n\n### To explore how much these businesses pay in fines we plotted a histogram of fines paid yearly (600 dollars per critical violation, 200 dollars per serious violation:","732dbcb1":"### Now let's look on type of risk. The are three types of risk: High, Medium and Low (numerically 1, 2 and 3). Most (over 70%) audited spots had high risk. Second one is medium with around 10% participation and the last one is low risk.","84fee3d8":"### Percentage of how may prior passes resulted in post fails","37528829":"# Risk Types Count","f3cced16":"### Percentage of how may prior fails resulted in post fails","a43f7851":"### Consider this: Chicago, a city with nearly three million people and more than 15,000 food establishments, has fewer than three dozen inspectors who are in charge of annually checking the city\u2019s entire lot. \n\n### When inspectors check this entire lot, 15% of these establishments, on average, earn a critical violation. \n\n### Having a critical violation, which generally relates to food temperature control, can drastically increase the odds that a restaurant may start or spread a foodborne illness.   Because of the obvious negative effects this can have on a population, efficiently and effectively targeting food establishments with critical violations is a top public health priority. \n\n### Chicago\u2019s challenging task to quickly locate and address these violations is a prime candidate for optimization with advanced analytics.  It\u2019s also an opportunity that Chicago\u2019s analytics team has been sure to seize as the City pioneers in its use of data. \n\n### The City\u2019s recently completed pilot program to optimize the city\u2019s food inspections process \u2013 conducted by the Chicago Department of Innovation and Technology (DoIT), along with the Department of Public Health (CDPH) and research partnerships with Civic Consulting Alliance and Allstate Insurance \u2013 has been a milestone that has yielded striking results. When using an analytics-based procedure, Chicago was able to discover critical violations, on average, seven days earlier than if they had used the traditional inspection procedure. \n\n### The results have implications not only for Chicago, but for cities anywhere that wish to optimize inspections processes using advanced analytics.  Moreover, Chicago\u2019s collaborative and open method for launching such an initiative provides lessons for other places that wish to start analytics programs of their own.    \n\n### In processing and analyzing the data, Chicago found several key predicting variables that, when observed, indicated there could be a considerable likelihood that a restaurant may earn a critical violation.  These predicting variables include the following:\n\n* Prior history of critical violations\n* Possession of a tobacco and\/or incidental alcohol consumption license \n* Length of time establishment has been operating\n* Length of time since last inspection\n* Location of establishment\n* Nearby garbage and sanitation complaints\n* Nearby burglaries\n* Three day average high temperature\n\n### These predictors were then factored together into a model, which was tested against food inspection procedures via a double-blind post-diction analysis.  In other words, after collecting a set of data, Chicago performed a simulation that used this past data to predict what its future outcome would have been under data-optimized conditions. \n\n\n","0b0d216f":"### We need to apply Label Encoding to some features: Label encoding algorithm is quite simple and it considers an order for encoding, Hence can be used for encoding ordinal data.\n\n* DBA Name\n* Facility Type\n* Address\n* Zip\n* Risk","ce393ed8":"# The next 5 inspectations by the 12 inspectors (60 inspectations in totall).","c9ad2e7d":"### The most occurrence words are PREP, CLEAN, repair, ISSUED, Maintain,...","ad6f69b2":"# Count violations","a2e10831":"# Visualization for Risk 1 (High)","33025f15":"# INSPECTIONS MAP","e3777961":"# Visualization for Results of inspections","addb54a2":"# Visualization for Risk 2 (Medium)","4146b854":"#### To assess whether businesses fail enough inspections for risk analysis to be worthwhile we plotted a pareto chart describing the number of businesses in each yearly fails bracket:","50447d08":"# Restaurant or no Restaurant\n\n#### Add new feature called Restaurant that describe if the Facility Type is restaurant or not","6f1e05a6":"# Types of Risk Analysis Plot\n\n## I'll plot all facilities on the map of Chicago based on the colors that I define.\n\n### The first step is to identify all facilities and take the recent inspections for each facility. I'll also remove all rows where 'Risk', 'Facility Type', 'DBA Name', 'Latitude', 'Longitude' will have null value. Some businesses are no longer operating or are no longer located and thus can be removed too. I'll create a new column Name which extracts the name from AKA Name and DBA Name with preference given to AKA Name.","3afcbc31":"# Visualization for Risk 3 (Low)","77638e13":"# Is the data balanced or not?","b39c23fb":"# Drop rows with missing data","cdda1e7d":"# Number of Different DBA names","9fbcac3e":"# Focus is only on \u201cCanvass\u201d inspections","0823efc2":"#### We found that facilities with a previous failure were almost twice as likely to fail as those with previous passing inspections, supporting the findings of the Chicago team.","4aa41e46":"# The heatmap between Restaurant and Resulats","b825cdaa":"![](https:\/\/1.bp.blogspot.com\/-E_jwjw4zI9A\/X9it-nb3cyI\/AAAAAAAAHb8\/NyNa2Mevt8E8jWLOBBV5ToweaRyYVE9VwCLcBGAsYHQ\/s1200\/restaurant.jpg)","0f5ac28e":"# Data preprocessing","7b43b218":"# Because we have only 12 inspectors, we can create a fuction that will show them next 12 inspectations with location and assign them tasks","bb862415":"### Violation 3 is the majority violation which refers to MANAGEMENT, FOOD EMPLOYEE AND CONDITIONAL EMPLOYEE; KNOWLEDGE, RESPONSIBILITIES AND REPORTING","e6826b69":"# Majority violation\n\n### Let's also check the majority violation that is present among the dataset.","40f91933":"# Build lightgbm model","c35e4d51":"# Only consider successful inspections","04fec455":"# Show suspected locations in the map with Risks High and Medium","6d33daf8":"### As this chart makes clear, the vast majority of violations are minor (30+) and (3) Hot\/cold  storage temp, with only a scattering of serious (15-29) and critical (1-14) violations.","a769d8f1":"# Risk Analysis\n\n### Plot all facilities on the map of Chicago based on the colors we defined above.","e9c3fa12":"### We found that 29% of businesses fail over .5 inspections in a year, with 17% failing 1 or more inspections yearly.","b02de61c":"## Finally, we can do some hyperparameters tuning,ensembling between models, add weather data, give inspectors the shortest path between facilities, so they can win time and money by visiting many facilities, and reducing gas consumption.","147a51a7":"# XGBoost prform better than other models","a8541627":"# CatBoostClassifier","337af861":"# Crosstab for the results each Month-Day-Year","d1f2f1b2":"# Different data types in the dataset","f65f3f72":"# VIOLATIONS TITLES\n\n##### The violation_titles.csv file is created in the 21_calculate_violation_data.ipynb file within the [CODE folder](https:\/\/github.com\/Sustainabilist\/ChicagoDataAnalysis\/blob\/master\/CODE\/21_calculate_violation_data.ipynb).","dda38753":"### We have information about the risk and types of control - time to compare this information. Most popular is combination pass and high risk (almost half of controls). The second one is fail and high risk. Last combiantion is pass with medium risk. It looks like there is no correlation between these two variables.","629dc5bd":"### I'll select the violations that are more than 100 in count.","d2365446":"### Most food facilities got pass (over 50%). The second one is \"Fail\" - over 15 thousand don't get positive opinion after control. Popular is also pass with conditions (over 15% spots got that result). Rarerly are situation when doors are not open or place is not ready for control or even not exist.","c0d7fa45":"# Top 10 DBA Names with their distribution of inspections","5a5a7735":"# Drop duplicates","ba160b38":"#### Over half examples are restaurant with high risk (more than 50%). High risk is the most popular type of risk in both - restaurants and other. Interesting is distirbution of \"low risk\". Low risk is almost always in \"no restaurant\" examples."}}