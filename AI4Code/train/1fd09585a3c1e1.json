{"cell_type":{"70a0a386":"code","1ec2f7be":"code","6a408a4d":"code","f1f79ba5":"code","b40d593c":"code","2a0962e5":"code","3f1336ec":"code","6cd78600":"code","00015b5a":"code","cb929091":"code","f1ea2980":"code","221ed92d":"code","83f488c1":"code","259accb3":"code","9e8855fc":"code","04b1e34e":"code","388741a1":"markdown","8b1c6939":"markdown"},"source":{"70a0a386":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.decomposition import PCA\nfrom collections import Counter\nimport sys\nsys.path.append('..\/input\/iterativestratification')\n\nimport matplotlib.pyplot as plt\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly as py\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)\nimport plotly.express as px\nplt.rcParams['figure.figsize']=(12,5)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndef file_appender(x):\n    return os.path.join(dirname,x)\ntraining_data=pd.read_csv(file_appender('train_features.csv'))\ntesting_data=pd.read_csv(file_appender('test_features.csv'))\nlabels_train=pd.read_csv(file_appender('train_targets_scored.csv'))\nlabels_extra=pd.read_csv(file_appender('train_targets_nonscored.csv'))\nsubmission_sample=pd.read_csv(file_appender('sample_submission.csv'))\n\ngenes=[x for x in training_data.columns if x.startswith('g-')]\ncells=[x for x in training_data.columns if x.startswith('c-')]\n\n","1ec2f7be":"pca=PCA(n_components=4)\npca_result=pca.fit_transform(training_data[cells].values)\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\n\nfig = px.scatter_matrix(\n    pca_result,\n    labels=labels,\n    dimensions=range(4),\n    title=\"TOTAL EXPLAINED VARIANCE CELLS: \"+str(sum(pca.explained_variance_ratio_)*100)\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()","6a408a4d":"pca=PCA(n_components=4)\npca_result=pca.fit_transform(training_data[genes].values)\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca.explained_variance_ratio_ * 100)\n}\n\nfig = px.scatter_matrix(\n    pca_result,\n    labels=labels,\n    dimensions=range(4),\n    title=\"TOTAL EXPLAINED VARIANCE GENES: \"+str(sum(pca.explained_variance_ratio_)*100)\n)\nfig.update_traces(diagonal_visible=False)\nfig.show()","f1f79ba5":"res=np.array(Counter(labels_train[[x for x in labels_train.columns if x!='sig_id']].sum(axis=1)).most_common())\nplt.bar(res[:,0], res[:,1], align='center', alpha=0.5)\nplt.xticks(res[:,0])\nplt.ylabel('no of sample')\nplt.title('How many multi-labels')","b40d593c":"most_common_label=labels_train[[x for x in labels_train.columns if x!='sig_id']].sum(axis=0)\nindexes=list(most_common_label.index)\nvalues=list(most_common_label.values)\n\ndata_dict={}\ndata_dict['label']=indexes\ndata_dict['values']=values\nmost_common=pd.DataFrame(data_dict).sort_values(by='values', ascending=False)\nplt.figure(figsize=(20,5))\nplt.bar(most_common['label'][:50], most_common['values'][:50], align='center', alpha=0.5)\nplt.xticks(most_common['label'][:50], rotation='vertical', fontsize=15)\nplt.ylabel('Count')\nplt.title('Most Popular label')\n","2a0962e5":"most_common_label=labels_train[[x for x in labels_train.columns if x!='sig_id']].sum(axis=0)\nindexes=list(most_common_label.index)\nvalues=list(most_common_label.values)\n\ndata_dict={}\ndata_dict['label']=indexes\ndata_dict['values']=values\nplt.figure(figsize=(20,5))\n\nmost_common=pd.DataFrame(data_dict).sort_values(by='values', ascending=True)\nplt.bar(most_common['label'][:50], most_common['values'][:50], align='center', alpha=0.5)\nplt.xticks(most_common['label'][:50], rotation='vertical', fontsize=15)\nplt.ylabel('Count')\n\nplt.title('Most UNPopular label')","3f1336ec":"list_of_potentials=['cp_type','cp_time','cp_dose']\ndef make_dummies(input_data):\n    result_list=[input_data]\n    for x in list_of_potentials:\n        result_list.append(pd.get_dummies(input_data[x]))\n    return pd.concat(result_list, axis=1)\ntraining_data_for_model=make_dummies(training_data)\ntesting_data_for_model=make_dummies(testing_data)\nfor x in list_of_potentials:\n    training_data_for_model=training_data_for_model.drop(x, axis=1)\n    testing_data_for_model=testing_data_for_model.drop(x,axis=1)","6cd78600":"len(training_data.columns)","00015b5a":"from sklearn.linear_model import LogisticRegression","cb929091":"useful_columns=[x for x in list(training_data_for_model.columns) if x!='sig_id']\ncolumns_labels=[x for x in list(labels_train.columns) if x!='sig_id']\nlabel_dict={}\nfor x in columns_labels:\n    label_dict[x]={}","f1ea2980":"from sklearn.model_selection import KFold\nX=training_data_for_model[useful_columns].values\ny=labels_train[columns_labels].values\nkf1 = KFold(n_splits=5)\n","221ed92d":"def score_calculator(predicted, true_values):\n    a=(true_values*np.log(predicted))+((1-true_values)*(np.log(1-predicted)))\n    a=a\/a.shape[0]\n    b=np.sum(a, axis=0)\n    return -1*b\n    print (b)\n    return sum(b)","83f488c1":"print (\"Starting now...\")\ntester=[]\ntester_true=[]\nfor train_index, test_index in kf1.split(X):\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    print (\"X_train: \"+str(X_train.shape)+\"  Xtest: \"+str(X_test.shape))\n    print (\"y_train: \"+str(y_train.shape)+\"  ytest: \"+str(y_test.shape))\n    predicted_r=[]\n    predicted_p=[]\n    actual=[]\n    \n    \n    \n    \n    for i,x in enumerate(columns_labels):\n        print (\"Now processing \"+str(x)+\" Which is \"+str(i+1)+ \"of \"+str(len(columns_labels)))\n        y=labels_train[x]\n        y=y.values\n        y_train,y_test=y[train_index],y[test_index]\n        model=LogisticRegression(max_iter=1000)\n        issue=False\n        try:\n            model.fit(X_train,y_train)\n            predicted_probability=model.predict_proba(X_test)\n            predicted=model.predict(X_test)\n            predicted_r.append(predicted)\n            predicted_p.append(predicted_probability)\n            actual.append(y_test)\n            label_dict[x]=model\n            issue=False\n        except Exception as e:\n            issue=True\n            print (e)\n            \n    \n    print (\"TOTAL LOG LOSS\")\n    #log_losses=[log_loss(actual[i], predicted_p[i]) for i in range(0, len(predicted_p))]\n    #total_logloss_lib=sum(log_losses)\n\n        \n    log_losses_manual=[score_calculator(predicted_r[i], actual[i]) for i in range(0, len(predicted_r))]\n    total_logloss_manual=sum(log_losses_manual)\n\n        #print (\"total logloss: \"+str(total_logloss_lib))\n    print (\"total logloss_manual: \"+str(total_logloss_manual))\n    break","259accb3":"X_send=testing_data_for_model[useful_columns].values\n\nPredicted_dic={}\nfor x in label_dict:\n    try:\n        model=label_dict[x]\n        result=model.predict(X_send)\n        Predicted_dic[x]=result\n    except:\n        Predicted_dic[x]=[0 for x in range(0, len(X_send))]\n        print (\"error predicting 0 on all\")","9e8855fc":"for x in Predicted_dic:\n    submission_sample[x]=list(Predicted_dic[x])","04b1e34e":"submission_sample.to_csv('submission.csv', index=False)","388741a1":"How much Labels on training_data","8b1c6939":"**Lets perform PCA on the Cells Data first to see how much of data is preserved via it.**"}}