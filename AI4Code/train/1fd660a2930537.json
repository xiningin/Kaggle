{"cell_type":{"5cea38fa":"code","1c2517c2":"code","deaccd41":"code","32621109":"code","c365b252":"code","c52e6713":"code","a42ef532":"code","0bbd24f7":"code","0a69f6b8":"code","17f29fb7":"code","660859cf":"code","cebe8b0f":"code","2092e43e":"code","7c6a6767":"code","c8aee4a6":"code","a9b63067":"code","fe4ec973":"code","b05e8197":"code","4398ae22":"code","f3d207b0":"code","bb555939":"code","bd76050f":"code","9efbf1f2":"code","f058bd34":"code","52bb8953":"code","0f064268":"code","0b835a6f":"code","948d5e05":"code","2f5cf8d5":"code","303bf87d":"code","33479ced":"markdown","37c8eeed":"markdown","5754a178":"markdown","450a6ce2":"markdown","fc1d1dd6":"markdown","a583d26e":"markdown","54b243e2":"markdown","149d861f":"markdown","15609498":"markdown","fc6a4183":"markdown","f7d9c266":"markdown","f9760f5f":"markdown","160b5cfa":"markdown","c54661b9":"markdown","e8570e2c":"markdown","5b92f2a6":"markdown","41d2fa9c":"markdown","5aa602e8":"markdown","f5106d72":"markdown","1931e796":"markdown","22f3d7da":"markdown","f9a26328":"markdown","dc58279e":"markdown","326d0a32":"markdown","1b9bc98e":"markdown","8d425184":"markdown","4094510d":"markdown","4fe9f15d":"markdown"},"source":{"5cea38fa":"%pip install -U -qq scikit-learn\n\nimport os\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, cross_val_predict\nfrom sklearn.metrics import classification_report\nfrom sklearn.neural_network import MLPClassifier\nfrom lightgbm.sklearn import LGBMClassifier\n\nsns.set(\n    style='darkgrid', context='notebook', rc={\n        'figure.frameon': False,\n        'legend.frameon': False,\n        'figure.figsize': (12, 8)\n    }\n)\n\nold_y = pd.read_csv('..\/input\/november21\/train.csv', usecols=['target']).target.astype(np.float32)\ndf = pd.read_parquet(\n    '..\/input\/tps-nov-2021-parquet\/data.pq'\n).dropna().rename(\n    columns={'target': 'after_flip'}\n).assign(\n    before_flip=old_y,\n    chunk=lambda df: df.id \/\/ 60000,\n    flipped=lambda df: df.after_flip != df.before_flip,\n    flips=lambda df: df.flipped.cumsum()\n)\nfolds = StratifiedKFold(5, shuffle=True, random_state=64)","1c2517c2":"df.flips.plot.line(title='cumsum(flipped)');","deaccd41":"df.groupby('chunk').flipped.sum().plot.bar(title='flip count by chunk');","32621109":"ixs = 300\ndf.loc[df.id < ixs, 'flips'].plot.line(label='cumsum(flipped)')\nplt.plot(np.arange(ixs), .26 * np.arange(ixs), label='.26 slope')\nplt.plot(np.arange(ixs), .24 * np.arange(ixs), label='.24 slope')\nplt.legend();","c365b252":"df.flipped.mean(), df.flips.iloc[-1] \/ len(df)","c52e6713":"ixs = 1000\ndf.loc[df.id < ixs, 'flips'].plot.line(label='cumsum(flipped)')\nplt.plot(np.arange(ixs), .26 * np.arange(ixs), label='.26 slope')\nplt.plot(np.arange(ixs), .24 * np.arange(ixs), label='.24 slope')\nplt.legend();","a42ef532":"ixs = 25000\ndf.loc[df.id < ixs, 'flips'].plot.line(label='cumsum(flipped)')\nplt.plot(np.arange(ixs), .26 * np.arange(ixs), label='.26 slope')\nplt.plot(np.arange(ixs), .24 * np.arange(ixs), label='.24 slope')\nplt.legend();","0bbd24f7":"(df.flips - .2512 * df.id).plot.line();","0a69f6b8":"(df.flips - .2512 * df.id).iloc[100000:140000].plot.line();","17f29fb7":"(df.flips - .2512 * df.id).iloc[280000:320000].plot.line();","660859cf":"last_flipped = df.flipped.shift()\n# There was a change whenever the current `flipped` differs from the last\nstate_change = df.flipped != last_flipped\n# Each sample that has the same `state_change.cumsum()` belongs to the same streak\nsample_by_streak_number = state_change.cumsum()","cebe8b0f":"streak_lengths = sample_by_streak_number.value_counts()\n\nsns.countplot(x=streak_lengths).set(\n    title='Streak length volume',\n    ylabel='Count of streaks', xlabel='Streak length'\n);","2092e43e":"pd.set_option('max_rows', 20)\ndf.loc[state_change, 'id'].iloc[:20]","7c6a6767":"len(streak_lengths[streak_lengths > 30]), len(streak_lengths[streak_lengths > 20])","c8aee4a6":"pos_streak_lengths = sample_by_streak_number[\n    df.flipped\n].value_counts()\n\nsns.countplot(x=pos_streak_lengths).set(title='Length of repeated flips streaks');","a9b63067":"n_jobs = min(5, os.cpu_count())\nalways_no_accuracy = (1 - df.flipped.mean())\nlreg = LogisticRegression(class_weight='balanced')\ntree = DecisionTreeClassifier(class_weight='balanced', max_depth=12)\n\nbit_pattern = pd.DataFrame({\n    f'id_{i}': (df.id & (2 ** i)) > 0 for i in range(16)\n})\n\ncross_val_score(\n    lreg, bit_pattern, df.flipped, cv=folds, n_jobs=n_jobs\n)","fe4ec973":"y_pred = cross_val_predict(lreg, bit_pattern, df.flipped, cv=folds, n_jobs=n_jobs)\n\nprint(classification_report(df.flipped, y_pred))","b05e8197":"cross_val_score(\n    tree, bit_pattern, df.flipped, cv=folds, n_jobs=n_jobs\n)","4398ae22":"from sklearn.metrics import PrecisionRecallDisplay\n\ny_pred_proba = cross_val_predict(\n    lreg, bit_pattern, df.flipped, cv=folds, n_jobs=n_jobs, method='predict_proba'\n)\n\n\nPrecisionRecallDisplay.from_predictions(df.flipped, y_pred_proba[:, 1]);","f3d207b0":"y_pred_proba = cross_val_predict(\n    tree, bit_pattern, df.flipped, cv=folds, n_jobs=n_jobs, method='predict_proba'\n)\n\nPrecisionRecallDisplay.from_predictions(df.flipped, y_pred_proba[:, 1]);","bb555939":"bit_pattern = pd.DataFrame({\n    f'id_{i}': (df.id & (2 ** i)) > 0 for i in range(18)\n})\n\ncross_val_score(lreg, bit_pattern, df.flipped, cv=folds, n_jobs=n_jobs)","bd76050f":"cross_val_score(\n    tree, bit_pattern, df.flipped, cv=folds, n_jobs=n_jobs\n)","9efbf1f2":"cum_class_balance = old_y.cumsum() - np.arange(len(old_y)) * old_y.mean()\ncum_class_balance = (cum_class_balance - cum_class_balance.min()) \/ (cum_class_balance.max() - cum_class_balance.min())\n\nplt.plot(cum_class_balance)\nplt.title('Cumulative class balance')\nplt.xlabel('id');","f058bd34":"X = pd.concat([\n    bit_pattern, old_y,\n    cum_class_balance,\n], axis=1)\n\ncross_val_score(\n    lreg, X, df.flipped, cv=folds, n_jobs=n_jobs\n)","52bb8953":"cross_val_score(\n    tree, X, df.flipped, cv=folds, n_jobs=n_jobs\n)","0f064268":"y_pred_proba = cross_val_predict(\n    tree, X, df.flipped, cv=folds, n_jobs=n_jobs, method='predict_proba'\n)\n\nPrecisionRecallDisplay.from_predictions(df.flipped, y_pred_proba[:, 1])\n\nprint(classification_report(df.flipped, y_pred_proba.argmax(axis=1)))","0b835a6f":"feats = [f'f{i}' for i in range(100)]\nX = pd.concat([X, df[feats]], axis=1)\n\ncross_val_score(\n    lreg, X, df.flipped, cv=folds, n_jobs=n_jobs\n)","948d5e05":"cross_val_score(\n    tree, X, df.flipped, cv=folds, n_jobs=n_jobs\n)","2f5cf8d5":"booster = LGBMClassifier(\n    learning_rate=.05, n_estimators=1000, n_jobs=n_jobs, is_unbalance=True\n)\n\ny_pred_proba = cross_val_predict(\n    booster, X.to_numpy(), df.flipped, cv=folds, n_jobs=1, method='predict_proba'\n)\nprint(classification_report(df.flipped, y_pred_proba.argmax(axis=1)))\n\nPrecisionRecallDisplay.from_predictions(df.flipped, y_pred_proba[:, 1]);","303bf87d":"fig, (ax0, ax1, ax2) = plt.subplots(3)\n\n(df.flips - .2512 * df.id).plot.line(ax=ax0)\nax0.set_title('Actual flips - expected flips')\nax0.set_xlabel('id')\n\n(old_y.cumsum() - old_y.mean() * df.id).plot.line(ax=ax1)\nax1.set_title('Old labels: positive - expected positive')\nax1.set_xlabel('id')\n\n(df.after_flip.cumsum() - df.after_flip.mean() * df.id).plot.line(ax=ax2)\nax2.set_title('New labels: positive - expected positive')\nax2.set_xlabel('id')\n\nplt.subplots_adjust(hspace=.75);","33479ced":"No! Bad line! You should be between these two slopes! Go back!\n\nAnyhow, it's random chance, right? This might happen eventually. But it seems odd to me.\n\nThe total number of flips is around 25.12% of the samples total, and we can verify that\nmatches the last `flips` value:","37c8eeed":"A uniform 25.12% chance of a flip would cause an almost straight line that was oscillating\naround 0 here, but this is clearly something else. Let's zoom in on the boundary between the second\nand third chunks:","5754a178":"There are 22 streaks longer than 30, 375 streaks longer than 20. If we were using some\nfair dice to decide whether to flip or not, I think the chance of getting a 42-long streak\n(let alone 2) would be equivalent to `.75 ** 42`, which doesn't seem very likely.\n\nHow long are typically positive streaks (eg. streaks of flipped labels)?","450a6ce2":"We actually have up to 12 in a row. `.25 ** 12` also seems like it shouldn't happen very often.\n\nCan we predict the label flips?\n==\n\nNothing I've tried so far indicates that we can do that based on features or the old _or_ new\nlabels.\n\nEncoding `id` with 16 bits\n--\n\nI've tried with some chunk-based stats, but there's one thing I haven't tried yet, which\nis to directly make use of `id`, so let's do that. We'll simply binary-encode the last 16 bits of\nit and see if that can do something:","fc1d1dd6":"50% accuracy with a balanced class weight, I think that means we're doing random guessing.\n\nLet's investigate a bit more:","a583d26e":"That seems to have harmed the tree somehow. In what way?","54b243e2":"Also doesn't help. Let's check a booster as well, just to have that out of our system:","149d861f":"Of course, we're looking at an awful lot of random draws here. But my gut feeling tells me we\nwould still not expect to see 42 samples in a row with no flipped labels unless there\nwas some non-random component to how the flipped labels were chosen. The long streaks aren't\nincredibly uncommon.\n\nDo the indexes of the state changes look familiar?","15609498":"Right, we simply have 50% recall and class frequency precision. I bet we'd get the same\nresult using a tree?","fc6a4183":"A nice, straight line\n==\n\nAt a distance, everything looks OK. Let's plot the cumulative sum of `old_target != new_target`.\n\nThis should be a fairly straight line. The slope would be close to the flip chance.","f7d9c266":"So, what we just did, was to assign a streak number to every sample. The streak number is\nincremented every time that a sample has a different flipped-state to the previous sample.\n\nThat means we can find out how many samples that were in a streak by just counting how many\nsamples that share streak numbers. Let's do that:","f9760f5f":"Seems pretty close to me!\n\nBut we were only looking at a few hundred samples, let's zoom out a bit more and look at\na thousand, surely it would converge to a .25 slope by then:","160b5cfa":"That doesn't seem so promising. Does the tree get anything right?","c54661b9":"I've tried some variations of these numbers in integer sequence search at [oeis](https:\/\/oeis.org\/),\nbut obviously that wasn't it.\n\nSome more stats about streak lengths:","e8570e2c":"This result doesn't seem useful, yet.\n\nUse features as features\n--\n\nHow about just adding all of the features?","5b92f2a6":"That doesn't seem to help much. Same result with the tree?","41d2fa9c":"Right, so this could find 39% of the flipped labels, but only 26% of the suggestions would be right.\n\nThat's also not very useful.\n\nLooking at flip count by id\n==\n\nOne more time, let's look at flip count by id, together with the label balance for both\nnew and old labels in case some insight jumps out:","5aa602e8":"The variance here is much higher than it should be for so many random draws, if they were \"fair\".\n\nZooming in on the nice straight line\n--\n\nAnd in fact, if we just look a bit closer, it looks even worse.\n\nLet's plot that nice straight line from earlier again, for only the first few hundred IDs, together\nwith a couple of slopes of known inclination:","f5106d72":"I'm not getting any wiser, but there's some pattern here that we wouldn't expect to see\nif the labels were being flipped with independent chance.\n\nIdentifying label-flipping streaks\n==\n\nThere are some very long streaks here. In fact, let's check how long the streaks are.","1931e796":"Use label and label balance as features\n--\n\nThis is the same result. Let's add in two more features -- the pre-flip label, and the cumulative\nsum of the pre-flip label minus the expected number of flips.","22f3d7da":"Nope, still not seeing anything.","f9a26328":"What the flip is happening?\n==\n\nThe distribution of flips across chunks is different than it should be. The samples were\ncertainly not given a uniform 25% chance to flip.","dc58279e":"And the boundary around id = 300 000 looks interesting too:","326d0a32":"Wait, it's going the _other way_ now? This thing isn't a straight line, it's a bunch of bananas\nstuck together with tape and chewing gum.\n\nWhat might make the overall behaviour easier to notice here, is to subtract the .25 slope\nfrom this particular line and plot it again:","1b9bc98e":"Seems about the same. Can we have higher precision for any level of recall at all?","8d425184":"Right? Please? How about 25000?","4094510d":"So, the bit pattern alone probably can't help us, it's probably just random that there exists some\nthreshold where we have really high precision.\n\nEncoding `id` with 18 bits\n--\n\nLet's use a wider bit pattern and try again:","4fe9f15d":"My brain is telling me that this line is exactly straight. The plot is also helping me,\nat id 400 000, it certainly looks like we've got very close to 100 000 flips and we could\neasily have 125 000 flips at id 500 000.\n\nYou wouldn't believe these chunks\n--\n\nIn [this](https:\/\/www.kaggle.com\/c\/tabular-playground-series-nov-2021\/discussion\/286731) post,\nit's show that the data is chunked at 60 000 element borders. And I've noticed that the distribution of flip count\nper chunk looks like it has a lot of variability. Going to draw this again:"}}