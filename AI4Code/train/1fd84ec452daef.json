{"cell_type":{"d98a8c9a":"code","eb5fa296":"code","4b438f89":"code","df0999e5":"code","9ac68f95":"code","10f5609a":"code","b2c1d043":"code","3500a8d9":"code","a2091190":"code","e6e3ecf5":"code","05690d73":"code","a2fbbc63":"code","2fd1decd":"code","c34508e4":"code","6cd0e3a3":"code","4b2f34bb":"code","e59ed23d":"code","3097a7f6":"code","9dce24b4":"code","0dfe06e0":"code","99726818":"code","d2d4cdd4":"code","390ebca4":"code","8e0a012e":"code","f2407a42":"code","7bf12d0a":"code","ac02f608":"code","24505a4d":"code","cce572cb":"code","65eb0549":"code","0acd57fc":"code","cdbe034c":"code","8f0f5cf3":"code","3f4384a6":"code","5d48d156":"code","597c3d5d":"code","f346e3a7":"code","fe8f9932":"code","e53d8806":"code","6515c637":"code","720f07de":"code","ce579029":"code","7bd525e5":"code","7a5d2ad5":"code","e00ce4dd":"code","c45b68c4":"code","10bbc94a":"code","49193145":"code","626b2d06":"code","366bad34":"code","14d8fec1":"code","a77e402b":"code","e831eaf3":"code","39444c32":"code","7ffb3dae":"code","870f4524":"code","d47239d1":"code","467b4765":"code","8d64dbac":"code","774ed479":"code","0c04e71e":"code","ee6ea793":"code","07a33e38":"code","0ea9a546":"code","37f5a9f7":"code","be44cc14":"code","c44a6e29":"code","f75d3842":"code","f8b3e0a9":"code","c714c1d4":"code","b7adf7f8":"code","8b7c9fd4":"code","6172342a":"code","ad1da103":"code","ed8db1c6":"code","88f63c25":"code","eee06ea3":"code","6f165b8f":"code","932e8dba":"code","4addfb33":"code","f72c91e4":"code","0dc23f8f":"code","2208a3dc":"code","3b52414b":"code","14642ba4":"code","cbc3ea54":"code","51481eb8":"code","b2f4599e":"code","35bd0694":"code","3a6f5e19":"code","8d644623":"code","f1ded7b5":"code","e2c4b763":"code","c704f679":"code","767497c6":"code","55c6c3d9":"code","954a731a":"code","591d2eff":"code","9a4b84f2":"code","355f0f4e":"code","41a310ff":"code","200da34a":"code","969faf79":"markdown","ae98f5e6":"markdown","ade6da10":"markdown","a19d574a":"markdown","b2bdb1cd":"markdown","12f80d38":"markdown","3833d9cf":"markdown"},"source":{"d98a8c9a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Any results you write to the current directory are saved as output.","eb5fa296":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test=pd.read_csv(\"..\/\/input\/test.csv\")\n\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nprint(train['RoofMatl'].unique())\nprint(test['RoofMatl'].unique())","4b438f89":"df_test.shape\n#df_train.shape[1]","df0999e5":"df_train.shape","9ac68f95":"\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\n\ndf_all=df_train.drop(['SalePrice'],axis=1).append(df_test)","10f5609a":"df_train.drop(['SalePrice'],axis=1).append(df_test)[:1458].head()","b2c1d043":"#data_input=df_train.drop(['Id','SalePrice'],axis=1)\n#df.drop(['B', 'C'], axis=1)\n#data_output=df_train['SalePrice']\ndf_train.head()\ndf_test.head()\n\n#df_train.drop(['SalePrice'],axis=1).append(df_test)","3500a8d9":"df_all.head()","a2091190":"total=df_all.isnull().sum()\/df_all.isnull().count()\nsum=df_all.isnull().sum()\n#total.head\n#sum.head\nmissing=pd.concat([total,sum],axis=1,keys=['Perc','Sum']).sort_values(by='Perc',ascending=False)\n#missing.keys=['Percentage','Sum']\ncolstodrop=missing[missing['Sum']>0].index\n#colstodrop\nmissing[missing['Sum']>0]\n \n#data_input=data_input.drop(colstodrop,axis=1)\n#data_test=df_test.drop(colstodrop,axis=1)\n#data_input.head(5)\n","e6e3ecf5":"df_all['MiscFeature']=df_all['MiscFeature'].fillna(\"None\")\ndf_all['Alley']=df_all['Alley'].fillna(\"None\")\ndf_all['Fence']=df_all['Fence'].fillna(\"None\")\ndf_all['FireplaceQu']=df_all['FireplaceQu'].fillna(\"None\")\ndf_all['GarageFinish']=df_all['GarageFinish'].fillna(\"None\")\ndf_all['GarageQual']=df_all['GarageFinish'].fillna(\"None\")\ndf_all['GarageType']=df_all['GarageType'].fillna(\"None\")\ndf_all['BsmtCond']=df_all['BsmtCond'].fillna(\"None\")\ndf_all['BsmtExposure']=df_all['BsmtExposure'].fillna(\"Nobase\")\ndf_all['BsmtQual']=df_all['BsmtQual'].fillna(\"None\")\ndf_all['BsmtFinType2']=df_all['BsmtFinType2'].fillna(\"None\")\ndf_all['BsmtFinType1']=df_all['BsmtFinType1'].fillna(\"None\")\ndf_all['GarageCond']=df_all['GarageCond'].fillna(\"None\")\ndf_all['PoolQC']=df_all['PoolQC'].fillna(\"None\")\ndf_all['MasVnrType']=df_all['MasVnrType'].fillna(\"None\")\ndf_all['MasVnrArea']=df_all['MasVnrArea'].fillna(0)\ndf_all['Functional']=df_all['Functional'].fillna(\"Typ\")\n#df_all['Utilities']=df_all['Utilities'].fillna(\"AllPub\")\ndf_all=df_all.drop(['Utilities'],axis=1)\ndf_all['MSZoning']=df_all['MSZoning'].fillna(df_all['MSZoning'].mode()[0])\ndf_all['Electrical']=df_all['Electrical'].fillna(df_all['Electrical'].mode()[0])\ndf_all[\"LotFrontage\"] = df_all.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\nfor col in ['GarageYrBlt','GarageArea','GarageCars']:\n    df_all[col]=df_all[col].fillna(0)\n    \nfor col in ['BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','BsmtFullBath','BsmtHalfBath']:\n    df_all[col]=df_all[col].fillna(0)\n    \nfor col in ['SaleType','Exterior1st','Exterior2nd','KitchenQual']:\n    df_all[col]=df_all[col].fillna(df_all[col].mode()[0])\n    \n    \n#Introduce some new fields\n\n\ndf_all['Total_sqr_footage'] = (df_all['BsmtFinSF1'] + df_all['BsmtFinSF2'] +\n                                 df_all['1stFlrSF'] + df_all['2ndFlrSF'])\n\ndf_all['Total_Bathrooms'] = (df_all['FullBath'] + (0.5 * df_all['HalfBath']) +\n                               df_all['BsmtFullBath'] + (0.5 * df_all['BsmtHalfBath']))\n\ndf_all['Total_porch_sf'] = (df_all['OpenPorchSF'] + df_all['3SsnPorch'] +\n                              df_all['EnclosedPorch'] + df_all['ScreenPorch'] +\n                              df_all['WoodDeckSF'])\n","05690d73":"df_all['Exterior1st'].mode()[0]","a2fbbc63":"#test piece for doing qa on missing values\n#df_all['Utilities'].mode()[0]\ndf_all.isna().sum().sort_values(ascending=False)","2fd1decd":"df_all.head()","c34508e4":"df_all['BsmtFinSF1']","6cd0e3a3":"#colstodrop","4b2f34bb":"df_all.columns","e59ed23d":"#df_all['YrSold']=df_all['YrSold'].apply(str)\n#df_all['MoSold']=df_all['MoSold'].apply(str)\n#df_all['OverallQual']=df_all['OverallQual'].apply(str)\n#df_all['OverallCond']=df_all['OverallCond'].apply(str)\n#df_all['MSSubClass']=df_all['MSSubClass'].apply(str)\n#df_all['YearBuilt']=df_all['YearBuilt'].apply(str)\n#df_all['YearRemodAdd']=df_all['YearRemodAdd'].apply(str)\n#df_all['GarageYrBlt']=df_all['GarageYrBlt'].apply(str)\ndf_all=df_all.drop(['Id'],axis=1)\ndf_all['Totalarea']=df_all['TotalBsmtSF'] + df_all['1stFlrSF'] + df_all['2ndFlrSF']","3097a7f6":"numericcols=df_all.dtypes[df_all.dtypes != object]\ncategorcols=df_all.dtypes[df_all.dtypes == object]","9dce24b4":"numericcols.index","0dfe06e0":"categorcols.index","99726818":"catcols=('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir',  'OverallCond', \n        'YrSold', 'MoSold','MSSubClass')\n#'MSSubClass',\ncatcols","d2d4cdd4":"df_all.shape\n\n\n","390ebca4":"df_all.dtypes[df_all.dtypes == \"object\"].index","8e0a012e":"print(df_all.dtypes.unique())\nprint(df_all['MSSubClass'].dtypes)\n\ndf_all['MSSubClass']","f2407a42":"\ndf_all.shape\n#df_all['MSSubClass'].dtype\n#print(numeric_feats)\n#df_all['MSSubClass']=str(df_all['MSSubClass'])\ndf_all['MSSubClass']=df_all['MSSubClass'].apply(str)","7bf12d0a":"df_all['MSSubClass']","ac02f608":"\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\ndf_all.dtypes\n\n#df_all=df_all.drop('SalePrice',axis=1)\nnumeric_feats = df_all.dtypes[df_all.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = df_all[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)\nskewness=skewness[abs(skewness['Skew'])>0.75]\nskewness\nprint(df_all.shape)","24505a4d":"numeric_feats","cce572cb":"df_all['MiscVal'].skew()","65eb0549":"#skewness=skewness[abs(skewness)>1.25]\n#skewness.count()\n\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax \nskewed_features = skewness.index\n\nlam = 0.15\nfor feat in skewed_features:\n    #df_all[feat] = boxcox1p(df_all[feat], lam)\n    df_all[feat] = boxcox1p(df_all[feat], boxcox_normmax(df_all[feat] + 1))\n    #df_all[feat] = np.log1p(df_all[feat])\n    \n #   df_all[feat] += 1\n  #df_all[feat] = np.log1p(df_all[feat])\n#for feat in skewed_features:\n#    df_all[feat]=np.log1p(df_all[feat])","0acd57fc":"# from sklearn.preprocessing import LabelEncoder\n# for c in catcols:\n#     lbl = LabelEncoder() \n#     lbl.fit(list(df_all[c].values)) \n#     d=c+\"_e\"\n#     #print(d)\n#     df_all[d] = lbl.transform(list(df_all[c].values)) \n#    #df_all[c]=np.log1p(df_all[c])","cdbe034c":"df_all.shape","8f0f5cf3":"def encode(frame, feature):\n    ordering = pd.DataFrame()\n    ordering['val'] = df_all[feature].unique()\n    ordering.index = ordering.val\n    ordering['spmean'] = frame[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']\n    ordering = ordering.sort_values('spmean')\n    ordering['ordering'] = range(1, ordering.shape[0]+1)\n    ordering = ordering['ordering'].to_dict()\n    \n    for cat, o in ordering.items():\n        df_all.loc[df_all[feature] == cat, feature+'_E'] = o\n    \nqual_encoded = []\nfor q in catcols:  \n    encode(df_train, q)\n    #df_train.append(q+'_E')\nprint(qual_encoded)\n","3f4384a6":"#df_train.SalePrice\n#df_all['MSSubClass']\ndf_all.shape","5d48d156":"# def encode2(frame, feature):\n#     ordering = pd.DataFrame()\n#     ordering['val'] = df_all[feature].unique()\n#     ordering.index = ordering.val\n#     ordering['spmean'] = frame[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']\n#     ordering = ordering.sort_values('spmean')\n#     ordering['ordering'] = range(1, ordering.shape[0]+1)\n#     #print(ordering)\n#     ordering = ordering['ordering'].to_dict()\n#     #print(ordering.items())\n#     #print(ordering)\n    \n#     for cat, o in ordering.items():\n#         #df_all.loc[df_all[feature] == cat, feature+'_E'] = o\n#         print(feature,o,cat)\n#         #print(o)\n        \n\n# for q in catcols:  \n#     encode2(df_train, q)\n#     #df_train.append(q+'_E')\n    \n    \n# for c in catcols:\n#     lbl = LabelEncoder() \n#     lbl.fit(list(df_all[c].values)) \n#     df_all[c] = lbl.transform(list(df_all[c].values))\n","597c3d5d":"df_all.head()","f346e3a7":"df_train[['MSZoning','SalePrice']].groupby('MSZoning').mean()\n#frame[[feature, 'SalePrice']].groupby(feature).mean()['SalePrice']","fe8f9932":"df_all.isna().sum().sort_values(ascending=False)\n#df_all[df_all['GarageQual_E'].isna()].head()#['GarageQual']\n#df_all[df_all['GarageQual_E'].isna().GarageQual].head()#['GarageQual']","e53d8806":"#df_train.iloc[:,81:]","6515c637":"#df_all['3SsnPorch']\nnumeric_feats = df_all.dtypes[df_all.dtypes != \"object\"].index\nnumeric_feats\n\nskewed_feats = df_all[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats}) \n#skewness=skewness[abs(skewness['Skew'])>1]\nskewness","720f07de":"skewed_feats","ce579029":"df_all.shape","7bd525e5":"df_all['haspool'] = df_all['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['has2ndfloor'] = df_all['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['hasgarage'] = df_all['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['hasbsmt'] = df_all['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ndf_all['hasfireplace'] = df_all['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","7a5d2ad5":"df_all=pd.get_dummies(df_all).reset_index(drop=True)\ndf_all.shape","e00ce4dd":"skewed_features","c45b68c4":"df_all.shape","10bbc94a":"df_all.columns","49193145":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,LassoCV\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\n#from sklearn.linear_model import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","626b2d06":"from sklearn.linear_model import LinearRegression","366bad34":"linreg = LinearRegression()","14d8fec1":"for cat in df_all.columns:\n    print(cat)\n\nprint(df_train['RoofMatl'].unique())\nprint(df_train['RoofMatl'].unique())","a77e402b":"df_all","e831eaf3":"data_input=df_all[:1458]\ndata_test=df_all[1458:]\ndata_output=np.log1p(df_train['SalePrice']).values\ndata_output.shape","39444c32":"data_test.shape\ndata_output","7ffb3dae":"data_input.shape","870f4524":"data_test.shape","d47239d1":"# for col in data_test.columns:\n#     if data_test[col].sum() == 0:\n#         data_input=data_input.drop(col,axis=1)\n#         data_test =data_test.drop(col,axis=1)\n","467b4765":"print(data_input.shape)\nprint(data_test.shape)\nprint(data_output.shape)","8d64dbac":"\n#data_test=df_test.drop(['Id'],axis=1)\n#data_test=pd.get_dummies(data_test)\n\n#data_output=np.log1p(df_train['SalePrice'])\n#data_input=df_train.drop(['Id','SalePrice'],axis=1)\n#data_input=df_train.drop(colstodrop,axis=1)\n#data_input=pd.get_dummies(data_input)\n#data_=pd.get_dummies(data_input)\n#\n","774ed479":"#traincols=data_test.columns\n#testcols=data_input.columns\n#traincols\n\n#def common_member(a, b): \n#    a_set = set(a) \n#    b_set = set(b) \n#    if (a_set & b_set): \n#        return list(a_set & b_set) \n#    else: \n#        print(\"No common elements\")  \n           \n#modelcols=common_member(traincols, testcols) \n","0c04e71e":"#data_input=data_input[modelcols]\n#modelcols\n#data_input\nprint(data_input.shape)\nprint(data_output.shape)","ee6ea793":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(data_input.values)\n    rmse= np.sqrt(-cross_val_score(model, data_input.values, data_output, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","07a33e38":"data_output","0ea9a546":"linreg.fit(data_input, data_output)\n#linreg_output=linreg.predict(data_input)\nrmsle_cv(linreg).mean()","37f5a9f7":"fig,ax=plt.subplots()\nax.scatter(x=data_output,y=linreg.predict(data_input))","be44cc14":"linreg_output=linreg.predict(data_test)","c44a6e29":"linreg_output[abs(linreg_output)>15]","f75d3842":"data_input.shape\ntype(data_input)","f8b3e0a9":"data_input.iloc[120:290]","c714c1d4":"# data_input.shape\n\n# kf=KFold(5)\n# kf.split(data_input,data_output)\n# k=list(range(100))\n# for x,y in kf.split(data_input,data_output):\n#     print (x,y)\n#     x=list(x)\n#     print(data_input[x])\n#     print(type(x))","b7adf7f8":"# data_input.loc[data_input['1stFlrSF'].isnull()]\n# output.isnull().sum()\n# output[output['MSSubClass'].isnull()]\n# data_input.iloc[523,:]","8b7c9fd4":"# data_input.shape\n# data_output.shape\n\n# #data_output.loc[290]\n# print(predictions.shape)\n\n# #predictions=np.zeros(df_train.shape[0])\n# print(predictions.shape)\n# print(data_output.shape)\n# print(predictions)\n\n\n#predictions=np.zeros((df_train.shape[0],1))","6172342a":"# data_input.head()\n# data_output.head()\ndf_all.shape","ad1da103":"#lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\n\n\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\n#lasso = Lasso(alpha =0.0005, random_state=1)\nlasso.fit(data_input,data_output)\n\nlasso_output_train=lasso.predict(data_input)\nlasso_output=lasso.predict(data_test)\nprint(rmsle_cv(lasso).mean())\nprint(rmsle_cv(lasso).std())","ed8db1c6":"resid=abs(lasso_output_train-data_output)\nout_mean=abs(lasso_output_train-data_output).mean()\nout_std=abs(lasso_output_train-data_output).std()\nz=(resid-out_mean)\/out_std\n#out_border\nz=np.array(z)\n#outliers=\noutliers=np.where(abs(z)>abs(z).std()*1.25)[0]\noutliers","88f63c25":"#detect outliers\n\n#np.where(resid > out_mean+3*out_std  )\n#data_input=data_input.drop([outliers],axis=0)\n#data_input\n# data_input_drop_out=data_input.drop([30,   66,   88,  142,  185,  277,  308,  328,  410,  431,  462,\n#         479,  495,  559,  580,  587,  627,  631,  657,  665,  680,  687,\n#         709,  710,  713,  727,  773,  811,  873,  897,  967,  969, 1021,\n#        1061, 1121, 1138, 1180, 1210, 1211, 1322, 1381, 1430, 1451])\n# data_output_drop_out=np.delete(data_output,[30,   66,   88,  142,  185,  277,  308,  328,  410,  431,  462,\n#         479,  495,  559,  580,  587,  627,  631,  657,  665,  680,  687,\n#         709,  710,  713,  727,  773,  811,  873,  897,  967,  969, 1021,\n#        1061, 1121, 1138, 1180, 1210, 1211, 1322, 1381, 1430, 1451],0)\n# type(data_input)\n# type(data_output)\n\ndata_input_drop_out=data_input.drop([   3,    4,   13,   17,   22,   24,   30,   35,   38,   48,   59,\n         66,   70,   76,   88,   97,  107,  142,  154,  157,  169,  175,\n        181,  185,  193,  198,  216,  217,  218,  223,  225,  238,  242,\n        250,  251,  261,  268,  270,  275,  277,  286,  291,  308,  318,\n        328,  329,  330,  335,  347,  348,  358,  365,  371,  377,  378,\n        393,  397,  401,  410,  418,  431,  439,  441,  445,  451,  457,\n        462,  473,  479,  488,  495,  503,  507,  512,  528,  534,  543,\n        544,  545,  557,  558,  559,  580,  587,  588,  606,  607,  625,\n        627,  629,  631,  638,  651,  657,  661,  665,  668,  679,  680,\n        687,  704,  706,  709,  710,  713,  714,  715,  716,  725,  727,\n        737,  739,  743,  746,  770,  771,  773,  788,  795,  796,  802,\n        807,  808,  811,  854,  863,  873,  884,  895,  897,  901,  914,\n        915,  922,  934,  939,  941,  944,  962,  967,  969,  971,  972,\n        989,  999, 1021, 1025, 1029, 1045, 1048, 1055, 1061, 1064, 1067,\n       1074, 1079, 1091, 1121, 1130, 1138, 1142, 1144, 1149, 1162, 1167,\n       1177, 1178, 1180, 1182, 1183, 1184, 1199, 1201, 1204, 1210, 1211,\n       1214, 1215, 1218, 1224, 1243, 1246, 1251, 1261, 1266, 1275, 1281,\n       1302, 1320, 1322, 1323, 1335, 1336, 1342, 1343, 1348, 1357, 1376,\n       1378, 1380, 1381, 1384, 1413, 1421, 1425, 1430, 1441, 1451])\n\ndata_output_drop_out=np.delete(data_output,[   3,    4,   13,   17,   22,   24,   30,   35,   38,   48,   59,\n         66,   70,   76,   88,   97,  107,  142,  154,  157,  169,  175,\n        181,  185,  193,  198,  216,  217,  218,  223,  225,  238,  242,\n        250,  251,  261,  268,  270,  275,  277,  286,  291,  308,  318,\n        328,  329,  330,  335,  347,  348,  358,  365,  371,  377,  378,\n        393,  397,  401,  410,  418,  431,  439,  441,  445,  451,  457,\n        462,  473,  479,  488,  495,  503,  507,  512,  528,  534,  543,\n        544,  545,  557,  558,  559,  580,  587,  588,  606,  607,  625,\n        627,  629,  631,  638,  651,  657,  661,  665,  668,  679,  680,\n        687,  704,  706,  709,  710,  713,  714,  715,  716,  725,  727,\n        737,  739,  743,  746,  770,  771,  773,  788,  795,  796,  802,\n        807,  808,  811,  854,  863,  873,  884,  895,  897,  901,  914,\n        915,  922,  934,  939,  941,  944,  962,  967,  969,  971,  972,\n        989,  999, 1021, 1025, 1029, 1045, 1048, 1055, 1061, 1064, 1067,\n       1074, 1079, 1091, 1121, 1130, 1138, 1142, 1144, 1149, 1162, 1167,\n       1177, 1178, 1180, 1182, 1183, 1184, 1199, 1201, 1204, 1210, 1211,\n       1214, 1215, 1218, 1224, 1243, 1246, 1251, 1261, 1266, 1275, 1281,\n       1302, 1320, 1322, 1323, 1335, 1336, 1342, 1343, 1348, 1357, 1376,\n       1378, 1380, 1381, 1384, 1413, 1421, 1425, 1430, 1441, 1451],0)","eee06ea3":"print(data_output_drop_out.shape)\nprint(data_input_drop_out.shape)","6f165b8f":"#lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\n\n\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\nkfolds = KFold(n_splits=10, shuffle=True, random_state=42)\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\n#lasso = Lasso(alpha =0.0005, random_state=1)\nlasso.fit(data_input_drop_out,data_output_drop_out)\n\nlasso_output_train=lasso.predict(data_input_drop_out)\nlasso_output=lasso.predict(data_test)\n#print(rmsle_cv(lasso).mean())\n#print(rmsle_cv(lasso).std())\n\nprint(np.sqrt(-cross_val_score(lasso, data_input_drop_out, data_output_drop_out, cv=5, scoring=\"neg_mean_squared_error\")).mean())\n\nprint(np.sqrt(-cross_val_score(lasso, data_input, data_output, cv=5, scoring=\"neg_mean_squared_error\")).mean())","932e8dba":"# xgb_output=model_xgb.predict(lasso_output.reshape(-1,1))\nprint(df_all.shape)","4addfb33":"# xgb_output","f72c91e4":"# df_all.shape","0dc23f8f":"# data_test.shape","2208a3dc":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_xgb.fit(data_input_drop_out,data_output_drop_out)\nxgb_output_train=model_xgb.predict(data_input)\nxgb_output=model_xgb.predict(data_test)\nprint(np.sqrt(-cross_val_score(model_xgb, data_input_drop_out, data_output_drop_out, cv=5, scoring=\"neg_mean_squared_error\")).mean())\n\n#rmsle_cv(model_xgb).mean()","3b52414b":"\n\nfrom sklearn.linear_model import Ridge\n\nrr = Ridge(alpha=13)\nrr.fit(data_input_drop_out, data_output_drop_out)\nnp.sqrt(-cross_val_score(rr, data_input_drop_out, data_output_drop_out, cv=5, scoring=\"neg_mean_squared_error\")).mean()","14642ba4":"\n# from sklearn.ensemble import RandomForestRegressor\n\n# regr = RandomForestRegressor(max_depth=2, random_state=0,\n#                               n_estimators=100)\n# regr.fit(data_input_drop_out,data_output_drop_out)\n# print(np.sqrt(-cross_val_score(regr, data_input_drop_out, data_output_drop_out, cv=5, scoring=\"neg_mean_squared_error\")).mean())\n","cbc3ea54":"ENet =  ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=1)\nENet.fit(data_input_drop_out,data_output_drop_out)\nENet_output_train=ENet.predict(data_input)\nENet_output=ENet.predict(data_test)\n\nprint(np.sqrt(-cross_val_score(ENet, data_input_drop_out, data_output_drop_out, cv=5, scoring=\"neg_mean_squared_error\")).mean())\n\n#rmsle_cv(ENet).mean()","51481eb8":"#averaged_models = AveragingModels(models = (ENet,   lasso))","b2f4599e":"\n# stk_inp_train=np.column_stack([lasso_output_train,ENet_output_train,xgb_output_train])\n# GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n#                                    max_depth=4, max_features='sqrt',\n#                                    min_samples_leaf=15, min_samples_split=10, \n#                                    loss='huber', random_state =5)\n# GBoost.fit(stk_inp_train, data_output)\n\n\n# rmsle_cv(GBoost).mean()\n# #linreg_output=linreg.predict(data_input)\n# #stk_inp_test.shape\n","35bd0694":"#df_all.Id\nstk_inp_train=np.column_stack([lasso_output_train,ENet_output_train])","3a6f5e19":"linreg.fit(stk_inp_train,data_output)\n# print(linreg.coef_)\n#rmsle_cv(linreg).mean()\n# #linreg.predict(stk_inp_test)[abs(linreg.predict(stk_inp_test))>15]","8d644623":"# linreg.predict(stk_inp_train)","f1ded7b5":"# data_test.isnull().sum().sort_values(ascending = False).head(5)","e2c4b763":" testids=df_test['Id']\n# print(testids.shape)\n# print(data_test.shape) \n# print(data_test.columns)\n\n# i=0\n\n#dat_test[data_test.groupby('MSSubClass').sum() == 0","c704f679":"# print(testcolsnoval)","767497c6":"lasso_output =lasso.predict(data_test)\n# ENet_output=ENet.predict(data_test)\n# xgb_output=model_xgb.predict(data_test)\n#xgb_output=model_xgb.predict(stk_inp_test))\n# stk_inp_test=np.column_stack([lasso_output,ENet_output,xgb_output])\n#test_output=GBoost.predict(stk_inp_test) # commenting out to submit lasso\ntest_output=lasso_output\n#test_output=xgb_output","55c6c3d9":"#test_output=0.33*lasso_output+0.33*ENet_output+0.34*xgb_output\n#test_output=xgb_output\n#test_output=lasso_output\n#lasso_output.shape\n\n#test_output=GBoost.predict(stk_inp_test)","954a731a":"results=pd.concat([testids,pd.Series(np.expm1(test_output))],axis=1,keys=['Id','SalePrice'])","591d2eff":"results.head(5)","9a4b84f2":"results.to_csv('..\/working\/submissionslassooutlierremoval4.csv',index=False)\n","355f0f4e":"\nprint(os.listdir(\"..\/working\"))","41a310ff":"df_train.head()","200da34a":"data_test.head()","969faf79":"data_test=data_test.drop(['Id'],axis=1)\ndata_test.shape","ae98f5e6":"> ","ade6da10":"#i=0\n#j=np.zeros(20)\n#for i in range(0,20):\n    #print(i)\n#    j[i]=i\n#    i+=1\n    \nkf=KFold(5)\n#print(j)\n#print(j.shape)\nlasso = Lasso(alpha =0.0005, random_state=1)\nENet =  ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=1)\n\n#print(da)\nprint(data_input.shape[0])\nprint(data_output.shape[0])\npredictions=np.zeros((data_input.shape[0],2))\n\ni=0\nfor x,y in kf.split(data_input,data_output):\n    lasso.fit(data_input.iloc[x],data_output.iloc[x])\n    ENet.fit(data_input,data_output)\n#     print(i)\n#     i+=1\n#     if(data_input.loc[x].isna()):\n    #print(x)\n    #output=data_input.loc[x]\n    #print(data_input.loc[x].isna().sum())\n    #data_input[data_input.isnull().any(axis=1)]\n    #print(data_output.loc[x].shape)\n    #print(y)\n    predictions[y,0]=lasso.predict(data_input.iloc[y])\n    predictions[y,1]=ENet.predict(data_input.iloc[y])\n    #predictions[y,1]=lasso.predict(data_input.loc[y])\n    #print(data_output.loc[x])\n    #data_input.loc[x] \n#print(predictions)\n#xgb_output=model_xgb.predict(data_test)\n#rmsle_cv(model_xgb).mean()\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n#model_xgb.fit(data_input,data_output)\nmodel_xgb.fit(predictions,data_output)\nprint(rmsle_cv(model_xgb).mean())\nprint(rmsle_cv(model_xgb).std())\npredictions.shape    \nprint(predictions)","a19d574a":"    Label Encoding","b2bdb1cd":"PoolQC\t0.996574\t2909\nMiscFeature\t0.964029\t2814\nAlley\t0.932169\t2721\nFence\t0.804385\t2348\nFireplaceQu\t0.486468\t1420\nLotFrontage\t0.166495\t486\nGarageYrBlt\t0.054471\t159\nGarageFinish\t0.054471\t159\nGarageQual\t0.054471\t159\nGarageCond\t0.054471\t159\nGarageType\t0.053786\t157\nBsmtExposure\t0.028092\t82\nBsmtCond\t0.028092\t82\nBsmtQual\t0.027749\t81\nBsmtFinType2\t0.027407\t80\nBsmtFinType1\t0.027064\t79\nMasVnrType\t0.008222\t24\nMasVnrArea\t0.007879\t23\nMSZoning\t0.001370\t4\nFunctional\t0.000685\t2\nBsmtHalfBath\t0.000685\t2\nBsmtFullBath\t0.000685\t2\nUtilities\t0.000685\t2","12f80d38":"data_test=pd.get_dummies(data_test)\ndata_input=pd.get_dummies(data_input)","3833d9cf":"**THanks to this Kernel.\n\nI have used Kernel by this individual to learn a lot.**\n\nhttps:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard"}}