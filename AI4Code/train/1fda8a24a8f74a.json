{"cell_type":{"e4e1b1da":"code","4bb14173":"code","4293cd8f":"code","01f1761b":"code","cfaa9fa3":"code","5b068bac":"code","9ed48c84":"code","09a4b4b3":"code","668307bb":"code","6391b4bc":"code","fb99a596":"code","bbefbb40":"code","d19ac8b2":"code","f7bd082b":"code","e042b9ff":"code","7e70eea3":"code","0b5c99b2":"markdown","06a75454":"markdown","ab54546b":"markdown","6d1c1fb2":"markdown","8455c41f":"markdown","f4ca1342":"markdown","aac5ba0b":"markdown","405c6fc6":"markdown","474f292d":"markdown","5119647e":"markdown","d12e778c":"markdown","46b0eb1f":"markdown","cfaf42b9":"markdown","216f9009":"markdown","0f4cfea2":"markdown"},"source":{"e4e1b1da":"import numpy as np\nnp.random.seed(5)\nimport os\n\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\nfrom keras.optimizers import Adam\nfrom keras.initializers import Constant\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss, classification_report","4bb14173":"dfAvaliacoesAnalisadas = pd.read_csv('..\/input\/avaliao-atendimento\/all.csv')\ndfAvaliacoesAnalisadas.head()","4293cd8f":"def converteCategoria(df, coluna):\n    le = preprocessing.LabelEncoder()\n    le.fit(df[coluna])\n    df[coluna] = le.transform(df[coluna])\n    return le\n\nnum_classes = len(dfAvaliacoesAnalisadas.manifest_atendimento.unique())\n\nlabelEncoderManifAtendimento = converteCategoria(dfAvaliacoesAnalisadas, 'manifest_atendimento')\ndfAvaliacoesAnalisadas['coment'] = dfAvaliacoesAnalisadas['coment'].astype(str)","01f1761b":"dfAvaliacoesAnalisadas.head()","cfaa9fa3":"import re\ndef clean_str(string):\n    \"\"\"\n    Tokenization\/string cleaning for all datasets except for SST.\n    \"\"\"\n    string = re.sub(r\"[^a-zA-Z0-9\u00c0\u00c1\u00c2\u00c3\u00c4\u00c5\u00c7\u00c8\u00c9\u00ca\u00cb\u00cc\u00cd\u00ce\u00cf\u00d2\u00d3\u00d4\u00d5\u00d6\u00d9\u00da\u00db\u00dc\u00dd\u00e0\u00e1\u00e2\u00e3\u00e4\u00e5\u00e7\u00e8\u00e9\u00ea\u00eb\u00ec\u00ed\u00ee\u00ef\u00f0\u00f2\u00f3\u00f4\u00f5\u00f6\u00f9\u00fa\u00fb\u00fc\u00fd\u00ff,!?\\'\\`\\.\\(\\)]\", \" \", string)\n    string = re.sub(r\"INC[0-9]{7,}\", \" <INCIDENTE> \", string)\n    string = re.sub(r\"[+-]?\\d+(?:\\.\\d+)?\", \" <NUMERO> \", string)\n\n    string = re.sub(r\",\", \" , \", string)\n    string = re.sub(r\"\\.\", \" \\. \", string)\n    string = re.sub(r\"!\", \" ! \", string)\n    string = re.sub(r\"\\(\", \" ( \", string)\n    string = re.sub(r\"\\)\", \" ) \", string)\n    string = re.sub(r\"\\?\", \" ? \", string)\n    string = re.sub(r\"\\s{2,}\", \" \", string)\n    return string.strip() #.lower()\n\ndfAvaliacoesAnalisadas['coment'] = dfAvaliacoesAnalisadas['coment'].apply(clean_str)\ndfAvaliacoesAnalisadas['coment'] = dfAvaliacoesAnalisadas['coment'].apply(lambda x : x.split(' '))","5b068bac":"%matplotlib inline\nfrom matplotlib import pyplot as plt\n\nsequence_length = dfAvaliacoesAnalisadas['coment'].apply(len).values\nprint(np.percentile(sequence_length, 99.9))\nprint(np.max(sequence_length))\nplt.hist(sequence_length)","9ed48c84":"def pad_sentence(sentence, sequence_length, padding_word=\"<PAD\/>\"):\n    if len(sentence) > sequence_length:\n        sentence = sentence[:sequence_length]\n    num_padding = sequence_length - len(sentence)\n    new_sentence = sentence + [padding_word] * num_padding\n    return new_sentence\n\ncorte = 200 #np.max(sequence_length)\ndfAvaliacoesAnalisadas['coment'] = dfAvaliacoesAnalisadas['coment'].apply(lambda x : pad_sentence(x, corte))","09a4b4b3":"import itertools\nfrom collections import Counter\n\ndef build_vocab(sentences):\n    \"\"\"\n    Builds a vocabulary mapping from word to index based on the sentences.\n    Returns vocabulary mapping and inverse vocabulary mapping.\n    \"\"\"\n    # Build vocabulary\n    word_counts = Counter(itertools.chain(*sentences))\n    # Mapping from index to word\n    vocabulary_inv = [x[0] for x in word_counts.most_common()]\n    # Mapping from word to index\n    vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n    return (vocabulary, vocabulary_inv)\n\ndef build_input_data(sentences, labels, vocabulary):\n    \"\"\"\n    Maps sentencs and labels to vectors based on a vocabulary.\n    \"\"\"\n    x = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n    y = np.array(labels)\n    return (x, y)\n\ncomments = dfAvaliacoesAnalisadas['coment'].values\nlabels = dfAvaliacoesAnalisadas['manifest_atendimento'].values\n\nvocabulary, vocabulary_inv = build_vocab(comments)\nX, ylabels = build_input_data(comments, labels, vocabulary)","668307bb":"from gensim.models import word2vec\nfrom os.path import join, exists, split\n\ndef train_word2vec(sentence_matrix, vocabulary_inv,\n                   num_features=300, min_word_count=1, context=10):\n    \"\"\"\n    Trains, saves, loads Word2Vec model\n    Returns initial weights for embedding layer.\n   \n    inputs:\n    sentence_matrix # int matrix: num_sentences x max_sentence_len\n    vocabulary_inv  # dict {str:int}\n    num_features    # Word vector dimensionality                      \n    min_word_count  # Minimum word count                        \n    context         # Context window size \n    \"\"\"\n    model_dir = 'word2vec_models'\n    model_name = \"{:d}features_{:d}minwords_{:d}context\".format(num_features, min_word_count, context)\n    model_name = join(model_dir, model_name)\n    if exists(model_name) and False:\n        embedding_model = word2vec.Word2Vec.load(model_name)\n        print('Loading existing Word2Vec model \\'%s\\'' % split(model_name)[-1])\n    else:\n        # Set values for various parameters\n        num_workers = 4       # Number of threads to run in parallel\n        downsampling = 1e-3   # Downsample setting for frequent words\n        \n        # Initialize and train the model\n        print(\"Training Word2Vec model...\")\n        sentences = [[vocabulary_inv[w] for w in s] for s in sentence_matrix]\n        embedding_model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n                            size=num_features, min_count = min_word_count, \\\n                            window = context, sample = downsampling)\n        \n        # If we don't plan to train the model any further, calling \n        # init_sims will make the model much more memory-efficient.\n        embedding_model.init_sims(replace=True)\n        \n        # Saving the model for later use. You can load it later using Word2Vec.load()\n        if not exists(model_dir):\n            os.mkdir(model_dir)\n        print('Saving Word2Vec model \\'%s\\'' % split(model_name)[-1])\n        embedding_model.save(model_name)\n    \n    #  add unknown words\n    embedding_weights = [np.array([embedding_model[w] if w in embedding_model\\\n                                                        else np.random.uniform(-0.25,0.25,embedding_model.vector_size)\\\n                                                        for w in vocabulary_inv])]\n    return embedding_weights","6391b4bc":"embedding_dim = 50\nmin_word_count = 1\ncontext = 10\n\nembedding_weights = train_word2vec(X, vocabulary_inv, embedding_dim, min_word_count, context)","fb99a596":"print(\"Tamanho do vocabul\u00e1rio: {:d}\".format(len(vocabulary)))\nprint(embedding_weights[0].shape)# n\u00famero de palavras x tamanho do vetor definido.","bbefbb40":"filter_sizes = (3, 4, 5) # cada item da lista representa os tamanhos de filtro que usaremos\nnum_filters = 128 # quantidade de filtro para cada um dos tamanhos acima\ndropout_prob = (0.3, 0.5) # probabilidade de cada camada de Dropout\nhidden_dims = 64 # n\u00famero de neur\u00f4nios na camada densa final","d19ac8b2":"def build_model():\n\n    model = Sequential()\n    \n    model.add(Embedding(len(vocabulary), embedding_dim, input_length=corte, embeddings_initializer=Constant(embedding_weights[0])))\n    model.add(Dropout(dropout_prob[0], input_shape=(sequence_length, embedding_dim)))\n\n    for fsz in filter_sizes:\n        model.add(Conv1D(num_filters,\n                             fsz,\n                             padding='valid',\n                             activation='relu'))\n        model.add(MaxPooling1D(2))\n\n    model.add(GlobalMaxPooling1D())\n    model.add(Dense(hidden_dims))\n    model.add(Dropout(dropout_prob[1]))\n    model.add(Activation('relu'))\n    model.add(Dense(hidden_dims))\n    model.add(Dropout(dropout_prob[1]))\n    model.add(Activation('relu'))\n    model.add(Dense(num_classes))\n    model.add(Activation('softmax'))\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    return model","f7bd082b":"from sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\ny = to_categorical(ylabels)\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.20, random_state=42)","e042b9ff":"model = build_model()\n\nmodel.fit(X_train, y_train, batch_size=64, epochs=3, validation_data=(X_valid, y_valid), verbose=1)\n\npreds = model.predict_proba(X_valid) # as previs\u00f5es s\u00e3o probabilidades para cada uma das 3 classes\n\n#conta o n\u00famero de acertos, considerando a classe de maior probabilidade\nacc_score = np.sum(np.argmax(preds,1)==np.argmax(y_valid,1))\/float(len(y_valid))\n#calcula o categorical log-loss\nlog_loss_score= log_loss(y_valid, preds)","7e70eea3":"print('Accuracy: %.4f Categorical log-loss: %.4f' % (acc_score, log_loss_score))","0b5c99b2":"Os dados que utilizaremos referem-se a dados de avalia\u00e7\u00e3o do atendimento do Service Desk da Petrobras. Queremos classificar os coment\u00e1rios feitos pelos usu\u00e1rios entre elogio, neutro ou reclama\u00e7\u00e3o.","06a75454":"Como mostrado acima, 99,9% das senten\u00e7as (avalia\u00e7\u00f5es) possuem menos de 197 palavras. Ent\u00e3o vamos colocar um ponto de corte pr\u00f3ximo a esse (200). As avalia\u00e7\u00f5es com mais de 200 palavras ser\u00e3o truncadas e as com menos, ter\u00e3o a palavra '<pad\/>' completando o final at\u00e9 chegar a 200 palavras.","ab54546b":"## Parte 1 - Carregando e manipulando os dados","6d1c1fb2":"Agora estamos prontos para gerar nossas  *Word embeedings* para nosso vocabul\u00e1rio. V\u00e1rias pesquisas tem demonstrado que usar um modelo baseado no corpus espec\u00edfica da tarefa (no caso, avalia\u00e7\u00f5es de atendimento), funciona melhor do que usar corpus gen\u00e9ricos e grandes como a wikip\u00e9dia.","8455c41f":"Na nossa arquitetura, todas as senten\u00e7as devem ter o mesmo n\u00famero de palavras. Temos de escolher um bom tamanho. Para isso, vamos visualizar um histograma dos tamanhos das senten\u00e7as:","f4ca1342":"Separando os dados de treino e testes.","aac5ba0b":"## Parte 3 - Montando a rede neuronal convolucional","405c6fc6":"Primeiro definimos os par\u00e2metros","474f292d":"Nesse ponto podemos come\u00e7ar a trabalhar com nossos coment\u00e1rios. O primeiro passo \u00e9 remover caracteres indesejados e separar pontua\u00e7\u00e3o das palavras, pois a pontua\u00e7\u00e3o ser\u00e1 tratada no nosso modelo. Tamb\u00e9m converteremos alguns n\u00fameros, emails e incidentes em uma palavra \u00fanica.","5119647e":"# Laborat\u00f3rio 12 - An\u00e1lise de Sentimento utilizando Redes Neuronais Convolucionais","d12e778c":"Treinando nosso modelo e verificando o desempenho","46b0eb1f":"## Parte 2 - Gerando as *Word Embeddings*","cfaf42b9":"Nesse laborat\u00f3rio vamos realizar an\u00e1lise de sentimento utilizando redes neuronais convolucionais, como proposto nesse <a href=\"http:\/\/arxiv.org\/pdf\/1408.5882v2.pdf\">artigo<\/a>. A arquitetura da solu\u00e7\u00e3o pode ser visualizada na figura abaixo:\n\n<img src=\"cnn_sentiment.png\" align=\"center\"\/>","216f9009":"Convertemos a manifesta\u00e7\u00e3o (elogio, neutro, reclama\u00e7\u00e3o) em c\u00f3digos:","0f4cfea2":"Agora que temos nosso modelo de palavras transformado em vetores (*word embeddings*), podemos us\u00e1-lo na nossa rede convolucional."}}