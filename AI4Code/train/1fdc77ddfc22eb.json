{"cell_type":{"2505ecdf":"code","e9b626b7":"code","f4f4b679":"code","0e9916de":"code","4b870799":"code","0ca0943f":"code","3debcb88":"code","83f76860":"code","127936d2":"code","50d320a6":"code","83803c3f":"code","5e6b169e":"code","90fdb13f":"code","087dfaba":"code","dd4c6ad2":"code","d2e6f07f":"code","e526604f":"code","06914cb1":"code","77c816b1":"code","36cf8abd":"code","4c6a64e8":"code","3e45fc7e":"code","b1867b64":"code","f62c0c19":"code","7e52998f":"code","6d806bdf":"markdown","d5b4b80e":"markdown","894158fe":"markdown","b21c366b":"markdown","28ef036c":"markdown","361c85ce":"markdown","5c81381f":"markdown","9a529053":"markdown","bccfbe52":"markdown","fed5af78":"markdown","7be8a01f":"markdown","45e79fd0":"markdown","c8743dc5":"markdown","2c34c8ee":"markdown"},"source":{"2505ecdf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\nfrom sklearn.cluster import MiniBatchKMeans\n\n\n# Pandas setting to display more dataset rows and columns\npd.set_option('display.max_rows', 150)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e9b626b7":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\", low_memory=False)#, nrows=10000)\n# train[\"date_time\"] = pd.to_datetime(train[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\", low_memory=False)\n# test[\"date_time\"] = pd.to_datetime(test[\"date_time\"], format=\"%Y-%m-%d %H:%M:%S\")\ntrain.info(memory_usage=\"deep\")","f4f4b679":"test.info(memory_usage=\"deep\")","0e9916de":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","4b870799":"train.head()","0ca0943f":"train.columns.values","3debcb88":"fig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","83f76860":"train.describe().T","127936d2":"fig, ax = plt.subplots(figsize=(6, 6))\n\nbars = ax.bar(train[\"claim\"].value_counts().index,\n              train[\"claim\"].value_counts().values,\n              color=colors,\n              edgecolor=\"black\",\n              width=0.4)\nax.set_title(\"Claim (target) values distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Claim (target) value\", fontsize=14, labelpad=10)\nax.set_xticks(train[\"claim\"].value_counts().index)\nax.tick_params(axis=\"both\", labelsize=14)\nax.bar_label(bars, [f\"{x:2.2f}%\" for x in train[\"claim\"].value_counts().values\/(len(train)\/100)],\n                 padding=5, fontsize=15)\nax.bar_label(bars, [f\"{x:2d}\" for x in train[\"claim\"].value_counts().values],\n                 padding=-30, fontsize=15)\nax.margins(0.2, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","50d320a6":"fig, ax = plt.subplots(figsize=(16, 6))\n\nbars = ax.bar(train.isna().sum().index,\n              train.isna().sum().values,\n              color=\"lightskyblue\",\n              edgecolor=\"black\",\n              width=0.7)\nax.set_title(\"Missing feature values distribution in the train dataset\", fontsize=20, pad=15)\nax.set_ylabel(\"Missing values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Feature\", fontsize=14, labelpad=10)\nax.set_xticks([x if i%2==0 else \"\" for i, x in enumerate(train.columns.values)])\nax.tick_params(axis=\"x\", rotation=90, labelsize=8)\nax.margins(0.005, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","83803c3f":"fig, ax = plt.subplots(figsize=(16, 6))\n\nbars = ax.bar(test.isna().sum().index,\n              test.isna().sum().values,\n              color=\"lightsteelblue\",\n              edgecolor=\"black\",\n              width=0.7)\nax.set_title(\"Missing feature values distributionin in the test dataset\", fontsize=20, pad=15)\nax.set_ylabel(\"Missing values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Feature\", fontsize=14, labelpad=10)\nax.set_xticks([x if i%2==0 else \"\" for i, x in enumerate(test.columns.values)])\nax.tick_params(axis=\"x\", rotation=90, labelsize=8)\nax.margins(0.005, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","5e6b169e":"df = pd.concat([train.drop([\"id\", \"claim\"], axis=1), test.drop(\"id\", axis=1)], axis=0)\ncolumns = df.columns.values\n\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,130), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=12, pad=5)\n            axs[r, c].set_yticks(axs[r, c].get_yticks())\n            axs[r, c].set_yticklabels([str(int(i\/1000))+\"k\" for i in axs[r, c].get_yticks()])\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n#plt.suptitle(\"Feature values distribution in both datasets\", y=0.99)\nplt.show();","90fdb13f":"print(\"Features with the leas amount of unique values:\")\ntrain.drop([\"id\", \"claim\"], axis=1).nunique().sort_values().head(5)","087dfaba":"# Plot dataframe\ndf = train.drop(\"id\", axis=1).corr().round(5)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(16,16))\nax = sns.heatmap(df, annot=False, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"bold\", \"fontsize\":13})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","dd4c6ad2":"df[(df[\"claim\"]>-0.001) & (df[\"claim\"]<0.001)][\"claim\"]","d2e6f07f":"features = [x for x in train.columns.values if x[0]==\"f\"]","e526604f":"# Counting amount of missing values in each row and adding it as a new feature\ntrain['n_missing'] = train[features].isna().sum(axis=1)\ntest['n_missing'] = test[features].isna().sum(axis=1)","06914cb1":"# Filling missing values with median of each column\nimputer = SimpleImputer(strategy=\"median\")\nfor col in features:\n    train[col] = imputer.fit_transform(np.array(train[col]).reshape(-1,1))\n    test[col] = imputer.transform(np.array(test[col]).reshape(-1,1))","77c816b1":"# Scaling all values to [0,1] range\nscaler = StandardScaler()\nfor col in features:\n    train[col] = scaler.fit_transform(np.array(train[col]).reshape(-1,1))\n    test[col] = scaler.transform(np.array(test[col]).reshape(-1,1))","36cf8abd":"X = train.drop([\"id\", \"claim\"], axis=1)\nX_test = test.drop(\"id\", axis=1)\ny = train[\"claim\"]","4c6a64e8":"# Model hyperparameters\nxgb_params = {'objective': 'binary:logistic',\n              'use_label_encoder': False,\n              'n_estimators': 2600,\n              'learning_rate': 0.04,\n              'subsample': 0.66,\n              'colsample_bytree': 0.1,\n              'max_depth': 8,\n              'booster': 'gbtree',\n              'gamma': 5.5,\n              'reg_alpha': 81.8,\n              'reg_lambda': 72.0,\n              'random_state': 42,\n              'tree_method': 'gpu_hist',\n              'n_jobs': 4}","3e45fc7e":"%%time\n# Training a model on a full dataset\nmodel = XGBClassifier(**xgb_params)\nmodel.fit(X, y,\n          verbose=False)\n# Making probability of class \"1\" predictions\npreds = model.predict_proba(X_test)[:, 1]","b1867b64":"# Making a DataFrame with feature importances\ndf = pd.DataFrame(columns=[\"Feature\", \"Importance\"])\ndf[\"Feature\"] = X.columns\ndf[\"Importance\"] = model.feature_importances_ \/ model.feature_importances_.sum()\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)","f62c0c19":"x = np.arange(0, len(df[\"Feature\"]))\nheight = 0.4\n\nfig, ax = plt.subplots(figsize=(16, 30))\nbars1 = ax.barh(x, df[\"Importance\"], height=height,\n                color=\"mediumorchid\", edgecolor=\"black\")\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature names\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(x)\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\nplt.margins(0.04, 0.01)\nplt.gca().invert_yaxis()","7e52998f":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions[\"claim\"] = preds\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","6d806bdf":"Probably worth a try to drop them and check if it improves the result.","d5b4b80e":"As you can see, both train and test datasets have missing values in every feature excepth for \"id\" and \"claim\". We should take care with them.","894158fe":"# **EDA**","b21c366b":"There are no categorical features in the dataset.\n\nLet's look at feature correlation.","28ef036c":"There is very weak linear correlation between the features. There are some features with relatively low correlation with target value even comparing with other features:","361c85ce":"# **Data import**","5c81381f":"# **Feature importances**","9a529053":"# **Data preprocessing**","bccfbe52":"# **Predictions submission**","fed5af78":"The target value classes are balanced which is good.","7be8a01f":"As you can see, the datasets are well balanced. So target distribution should probably be the same for test predictions.","45e79fd0":"# **Model training**","c8743dc5":"The idea of adding a new feature below is taken from [this notebook](https:\/\/www.kaggle.com\/hiro5299834\/tps-sep-2021-single-lgbm) by [BIZEN](https:\/\/www.kaggle.com\/hiro5299834).","2c34c8ee":"Let's check feature values distribution in the both datasets."}}