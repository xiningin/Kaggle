{"cell_type":{"f1b69a05":"code","f3ab7c4e":"code","bde6ce7b":"code","2d9e037b":"code","995dd2e0":"code","b0eaf22d":"code","f4474887":"code","cf5dd5e7":"code","06d4f490":"code","5c4b33fd":"code","b753e602":"code","55cc9028":"code","ad8ed277":"code","a6e536c0":"code","c4979248":"code","b83b0842":"code","8b5efe3b":"code","6ac2d16b":"code","659f1727":"code","16427d08":"code","3569eecf":"code","22ed6b5b":"code","e1cd4e16":"code","06d15ea4":"code","8c40eaf5":"code","abe07dec":"code","1bdc0c01":"code","25903875":"markdown","eaa498ba":"markdown","aeff4512":"markdown","d6d84a30":"markdown","95f02232":"markdown","53fa2374":"markdown","7d7e382d":"markdown","82549648":"markdown","5b70e20f":"markdown","c456c2ed":"markdown","75dcbe77":"markdown","7b85b593":"markdown","0a024770":"markdown","b33fbe14":"markdown","cc498fbd":"markdown"},"source":{"f1b69a05":"# data processing, CSV file I\/O (e.g. pd.read_csv)\n# For convention, pandas always import using alias pd\nimport pandas as pd \n\n# To visualize graphs\nimport matplotlib.pyplot as plt\n\n# Models used\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Checking available directories\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","f3ab7c4e":"#Load data with pandas\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data_complete = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","bde6ce7b":"train_data.head()\n# View the top rows of our dataset. Default value is 5. \n# If want to you view more or less, you can especify the value like \n# train_data.head(10)","2d9e037b":"test_data_complete.head()","995dd2e0":"# Remove columns that will not be use \ntrain_data = train_data.drop(['Name','Ticket', 'Cabin'], axis=1)\ntest_data_complete = test_data_complete.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n\n# I could use also\n# train_data.drop(['Name','Ticket', 'Cabin'], axis=1, inplace=True)\n# Using this, I don't need to save the modifications in a variable, because parameter inplace=True\n# meaning that all modifications is automatically saved in data set.","b0eaf22d":"# Tranform data using one-hot encoding.\n# This part is important because machine learning models works with numbers, and in this dataset some columns\n# has values objects, like Sex (male or female) and Embarked (Q, S, C).\n\ntrain_data = pd.get_dummies(train_data)\ntest_data_complete = pd.get_dummies(test_data_complete)","f4474887":"train_data.head()","cf5dd5e7":"test_data_complete.head()","06d4f490":"# Verifying null values\ntrain_data.isnull().sum().sort_values(ascending=False)","5c4b33fd":"#Filling null values with mean\ntrain_data['Age'] = train_data['Age'].fillna(train_data['Age'].mean())","b753e602":"test_data_complete.isnull().sum().sort_values(ascending=False)","55cc9028":"test_data_complete['Age'] = test_data_complete['Age'].fillna(test_data_complete['Age'].mean())\ntest_data_complete['Fare'] = test_data_complete['Fare'].fillna(test_data_complete['Fare'].mean())","ad8ed277":"# Separating features and targets\nx_features = train_data.drop(['PassengerId','Survived'], axis=1)\ny_targets = train_data['Survived']","a6e536c0":"x_features.head()","c4979248":"tree_classifier = DecisionTreeClassifier(max_depth=5, random_state=0)\ntree_classifier.fit(x_features, y_targets)","b83b0842":"forest_classifier = RandomForestClassifier(n_estimators=300, max_depth=3, random_state=0)\nforest_classifier.fit(x_features, y_targets)","8b5efe3b":"gbc_classifier = GradientBoostingClassifier(n_estimators=300, max_depth=3, random_state=0)\ngbc_classifier.fit(x_features, y_targets)","6ac2d16b":"# Naive Bayes Classifier\nnaive_classifier = GaussianNB()\nnaive_classifier.fit(x_features, y_targets)","659f1727":"svc_classifier = SVC(C=5,gamma=10)\nsvc_classifier.fit(x_features, y_targets)","16427d08":"knn_classifier = KNeighborsClassifier(n_neighbors=3)\nknn_classifier.fit(x_features, y_targets)","3569eecf":"mlp_classifier = MLPClassifier(hidden_layer_sizes=(7,2), activation='relu',solver='adam', max_iter=300,\n                               random_state=0, batch_size=250)\nmlp_classifier.fit(x_features, y_targets)","22ed6b5b":"results = pd.read_csv(\"\/kaggle\/input\/titanic\/gender_submission.csv\")","e1cd4e16":"test_target = results['Survived']\ntest_data = test_data_complete.drop('PassengerId', axis=1)","06d15ea4":"tree_score_test = tree_classifier.score(test_data, test_target)\nforest_score_test = forest_classifier.score(test_data, test_target)\ngbc_score_test = gbc_classifier.score(test_data, test_target)\nnaive_score_test = naive_classifier.score(test_data, test_target)\nsvc_score_test = svc_classifier.score(test_data, test_target)\nknn_score_test = knn_classifier.score(test_data, test_target)\nmlp_score_test = mlp_classifier.score(test_data, test_target)","8c40eaf5":"# Using matplotlib to visualize the best score\nnames = ['Decision Tree', 'Random Forest', 'GBC', 'Naive Bayes', 'SVC', 'KNN', 'MLP']\nvalues = [tree_score_test, forest_score_test, gbc_score_test, naive_score_test, svc_score_test, \n         knn_score_test, mlp_score_test]\n\nplt.bar(names,values)\nplt.tick_params(axis ='x', rotation = 90)\nplt.show()","abe07dec":"submission = pd.DataFrame()\nsubmission['PassengerId'] = test_data_complete['PassengerId']\nsubmission['Survived'] = forest_classifier.predict(test_data)","1bdc0c01":"submission.to_csv('submission.csv', index=False)\nprint(\"Submission was successfully saved!\")","25903875":"# 3. Machine Learning Models\nMachine Learn Models can be classified basically as supervised learning, unsupervised learning and reinforcement learning. In this kernel I use only supervised models, meaning that inputs and outputs is known. To get different results, you can modify the classifier parameters. All links to documentation is in the description of each classifier. I include too a simple concept for each model and a link with more informations.","eaa498ba":"In this tutorial kernel I've used some machine learning models with Scikit Learn library. This is the sequence of this kernel:\n1. Import packages and load data.\n2. Pre-processing data. \n3. Machine learn models. \n4. Checking models accuracy. \n5. Save submission. \n\nIf I help you with this kernel, don't forget to upvote it. Feel free to fork it and edit as you like.","aeff4512":"> It's important to notice that all pre processing make in train data must be make in test data too.","d6d84a30":"If you liked it, upvote this kernel. ^^  \nIf you have any doubt or suggestion, please make a comment.","95f02232":"# 4. Checking Models Accuracy\nI ran all models with differents configurations, and this is my results.","53fa2374":"### 3.7 MLP Classifier\n> \"A multilayer perceptron (MLP) is a class of feedforward artificial neural network. An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function.\"\n\nConcept available in [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Multilayer_perceptron)\n\nLink to [Documentation MLP Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html)","7d7e382d":"### 3.6 KNN Classifier\n> *\"The KNN algorithm assumes that similar things exist in close proximity. In other words, similar things are near to each other.\"*\n\nConcept available in [Towards Data Science](https:\/\/towardsdatascience.com\/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761)\n\nLink to [Documentation KNN Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)","82549648":"### 3.1 Decision Tree Classifier\n> *\"Decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome.\"*  \n\nConcept available in [DataCamp](https:\/\/www.datacamp.com\/community\/tutorials\/decision-tree-classification-python)\n\nLink to [Documentation Decision Tree](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)","5b70e20f":"### 3.4 Naive Bayes Classifier\n> *\"Naive Bayes classifiers are a collection of classification algorithms based on Bayes\u2019 Theorem. It is not a single algorithm but a family of algorithms where all of them share a common principle, i.e. every pair of features being classified is independent of each other. (...) Bayes\u2019 Theorem finds the probability of an event occurring given the probability of another event that has already occurred.\"*\n\nConcept available in [GeeksforGeeks](https:\/\/www.geeksforgeeks.org\/naive-bayes-classifiers\/)\n\nLink to [Documentation Naive Bayes Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)","c456c2ed":"# 2. Pre-processing data\nIn data science, pre-processing means basically select the fields that we will use, clear the data, handle null values and outliers, scaling data, etc. For this tutorial, I just used basic pre-processing techniques.","75dcbe77":"### 3.3. Gradient Boosting Classifier\n> *\"Gradient Boosting trains many models in a gradual, additive and sequential manner.\"*\n\nConcept available in [Towards Data Science](https:\/\/towardsdatascience.com\/understanding-gradient-boosting-machines-9be756fe76ab)\n\nLink to [Documentation Gradient Boostings Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html)","7b85b593":"### 3.5 Support Vector Machine\n> *\"A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating hyperplane. In other words, given labeled training data, the algorithm outputs an optimal hyperplane which categorizes new examples.\"*\n\nConcept available in [Machine Learning 101](https:\/\/medium.com\/machine-learning-101\/chapter-2-svm-support-vector-machine-theory-f0812effc72)\n\nLink to [Documentation SVC](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC)","0a024770":"### 3.2 Random Forest Classifier\n> *\"Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.\"* \n\nConcept available in [DataCamp](https:\/\/www.datacamp.com\/community\/tutorials\/random-forests-classifier-python)\n\nLink to [Documentation Random Forest](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html) ","b33fbe14":"# 1. Import packages and load data\nInitially, I prefer to use only **pandas** and **sklearn** libraries because I want to make this tutorial simple and easy to understand. You can fork and modify this using packages that you prefer.","cc498fbd":"# 5. Save submission"}}