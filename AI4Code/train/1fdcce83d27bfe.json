{"cell_type":{"effbf5d7":"code","1f04ceaf":"code","186f18af":"code","03a1c4e3":"code","f0950272":"code","41b72c30":"code","ff535a80":"code","8039f169":"code","f240c107":"code","b096f779":"code","5b29d2eb":"code","9b593d4e":"code","12cef779":"code","a785dbe1":"code","361a090e":"code","565c4c8f":"code","c5b5a3a6":"code","aef773e8":"code","dbc21be1":"code","12160423":"code","20ea4593":"code","85309379":"code","d4461f76":"code","5c37df1f":"code","688f324e":"code","df7a4eca":"markdown","62b448ba":"markdown","769fa02c":"markdown","b1682c8e":"markdown","9a59b2fc":"markdown","6ae1ba6c":"markdown","7cffb668":"markdown","66260e0f":"markdown","a2a3aee6":"markdown"},"source":{"effbf5d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1f04ceaf":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline","186f18af":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')","03a1c4e3":"train_df.head()","f0950272":"test_df.head()","41b72c30":"# Shape of train dataset\nprint(f\"Train dataset has: {train_df.shape[0]} rows\")\nprint(f\"Train dataset has: {train_df.shape[1]} columns\")","ff535a80":"train_df.describe()","8039f169":"train_df.info","f240c107":"# Correlation matrix\ncorrelation_train = train_df.corr()\nplt.figure(figsize=(15, 7))\nsns.heatmap(data=correlation_train, square=True, annot=True, cmap='RdYlGn')","b096f779":"# Plotting Survived target - 0: not survived, 1: survived\nsns.countplot(data=train_df, x='Survived')","5b29d2eb":"train_df.isnull().sum()","9b593d4e":"# Printing the percentage of missing values in the train dataset\ntrain_missing = train_df.isnull().sum().sort_values(ascending=False)\npercentage = (train_df.isnull().sum() \/ len(train_df) * 100).sort_values(ascending=False)\nmissing_data = pd.concat([train_missing, percentage], axis=1, keys=['Missing', 'Percent'], join='outer')\nmissing_data.head()","12cef779":"# Filling the age missing values with their mean\ntrain_df['Age'].fillna(train_df.Age.mean(), inplace=True)","a785dbe1":"train_df.Age.isnull().sum()","361a090e":"# Dropping columns\ndrop_columns = ['Cabin', 'Embarked', 'Fare', 'Name', 'Ticket', 'PassengerId']\ntrain_df.drop(drop_columns, axis=1, inplace=True)","565c4c8f":"train_df","c5b5a3a6":"# Numerical features\nnumerical_feat = [column for column in train_df.columns if train_df[column].dtype != 'object']\nprint(numerical_feat)","aef773e8":"# Categorical features\ncategorical_feat = [column for column in train_df.columns if train_df[column].dtype == 'object']\nprint(categorical_feat)","dbc21be1":"# Preparing the dataset\nX = train_df.drop('Survived', axis=1)\ny = train_df.Survived","12160423":" X","20ea4593":"from sklearn.compose import make_column_transformer\nsex_col_transformer = make_column_transformer((OneHotEncoder(), ['Sex']), remainder='passthrough')","85309379":"# Model: LogisticRegression using liblinear solver since we are working with a small dataset\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='liblinear')","d4461f76":"from sklearn.pipeline import make_pipeline\npipeline = make_pipeline(sex_col_transformer, lr)","5c37df1f":"from sklearn.model_selection import KFold\nkfold = KFold(n_splits=10, random_state=None, shuffle=True)\nscore = cross_val_score(pipeline, X, y, cv=kfold, scoring='accuracy')\nprint(f\"Mean: {score.mean()}\")\nprint(f\"Scores: {score}\")\nprint(f\"Standard Deviation: {score.std()}\")","688f324e":"# Out of sample data\nOut_of_sample = X.sample(5, random_state=50)\npipeline.fit(X, y)\npipeline.predict(Out_of_sample)","df7a4eca":"### Pipeline\n\nBuilding the pipeline","62b448ba":"Our goal is to predict how many people have survived and how many have not from the titanic wreckship.\nIn other words, this is a classification problem.","769fa02c":"### Missing Values","b1682c8e":"### EDA - Exploratory Data Analysis","9a59b2fc":"Now we want to transform the 'sex' feature into a numerical one ","6ae1ba6c":"## Titanic - Building a pipeline\n\nThe purpose of this notebook is to be a guide (as much as it could) on how set up a pipeline.\nAs a beginner myself this code is going to be quite straightforward: as i said above it's just a guide.\nHopeful, people would benefit from this work. \nAny further improvement is well accepted.\n","7cffb668":"### Preprocessing and model building\n\nThis is the point of the notebook. We're going to set up a pipeline.\nA pipeline is used to chain steps together. \nHow are we going to procede?\n>- preparing the dataset\n>- transform the categorical data into numerical data\n>- choose a classification model\n>- build the pipeline","66260e0f":"### Numerical features and Categorical features","a2a3aee6":"Since this is a classification problem we need a model for this kind of purpose"}}