{"cell_type":{"89c05807":"code","dc05adcc":"code","ebee6f42":"code","eaec5b8a":"code","88a36fd3":"code","1e8dbe2b":"code","f51fe732":"code","87434c2b":"code","caef4aa4":"code","8ea01e35":"code","1123a321":"code","0adfaa69":"code","765b9cd6":"code","4d98c064":"code","428190c5":"code","487e7a47":"code","84bb6e2c":"code","793ca30e":"code","4134ea90":"code","373ac211":"code","9753b847":"code","d45b01cb":"code","e5f2ddde":"code","bb2f2474":"markdown","838ad2ee":"markdown","d069ed4d":"markdown","70411efd":"markdown","365816e0":"markdown","50a81ef8":"markdown","2fb9bfe0":"markdown","45f59817":"markdown","db7e00dd":"markdown"},"source":{"89c05807":"import re\nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom nltk.tokenize import word_tokenize\nfrom tensorflow.keras.preprocessing import sequence\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.stem import WordNetLemmatizer\nfrom tensorflow.keras.utils import to_categorical","dc05adcc":"df = pd.read_csv('..\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-toxic-comment-train.csv')\ndf = df.drop(labels = ['id','severe_toxic','obscene','threat','insult','identity_hate'], axis=1)\ndf.columns = ['comment','toxic']","ebee6f42":"def clean_document(doc):\n    doc = doc.lower()\n    # tokenization\n    tokens = word_tokenize(doc)\n    stop = stopwords.words('english')\n    bad_tokens = stop + list(punctuation)\n    clean_tokens = [t for t in tokens if t.lower() not in bad_tokens]\n    # lemmatization\n    lemma = WordNetLemmatizer()\n    clean_tokens = [lemma.lemmatize(t) for t in clean_tokens]\n    return ' '.join(clean_tokens)","eaec5b8a":"df.comment = df.comment.apply(clean_document)","88a36fd3":"X = df['comment']\ny = df['toxic']","1e8dbe2b":"y = to_categorical(y, num_classes=2)","f51fe732":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1, random_state=1)","87434c2b":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)","caef4aa4":"vocabulary = tokenizer.index_word\nvocab_len = len(vocabulary)\nvocab_len","8ea01e35":"train_sequence = tokenizer.texts_to_sequences(X_train)","1123a321":"doc_len = []\nfor doc in train_sequence:\n    doc_len.append(len(doc))\nmax(doc_len)","0adfaa69":"np.quantile(doc_len, 0.99)\n","765b9cd6":"max_len = 347","4d98c064":"train_sequence_matrix = sequence.pad_sequences(train_sequence, maxlen= max_len)\ntest_sequence = tokenizer.texts_to_sequences(X_test)\ntest_sequence_matrix = sequence.pad_sequences(test_sequence, maxlen= max_len)","428190c5":"len(train_sequence_matrix)","487e7a47":"print(train_sequence_matrix.shape)\nprint(y_train.shape)","84bb6e2c":"train_sequence_matrix = train_sequence_matrix.reshape(-1,347,1)\ntrain_sequence_matrix.shape","793ca30e":"train_sequence_matrix = train_sequence_matrix.reshape(-1,347,1)\ntrain_sequence_matrix.shape","4134ea90":"def _bytes_feature(value):\n    if isinstance(value, type(tf.constant(0))): # if value ist tensor\n        value = value.numpy() # get value of tensor\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\ndef _int64_feature(value):\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\ndef _float_feature(value):\n    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n\ndef serialize_array(array):\n    array = tf.io.serialize_tensor(array)\n    return array","373ac211":"def parse_text_data(text, label):\n    data = {\n        'text' : _bytes_feature(serialize_array(text)),\n        'label' : _bytes_feature(serialize_array(label))\n    }\n    out = tf.train.Example(features=tf.train.Features(feature=data))\n    return out","9753b847":"def write_text_to_tfr(text_data, label, filename:str=\"text\"):\n    filename= filename+\".tfrecords\"\n    writer = tf.io.TFRecordWriter(filename)\n    count = 0\n    for index in range(len(text_data)):\n        current_text = text_data[index] \n        current_label = label[index]\n        out = parse_text_data(text=current_text, label=current_label)\n        writer.write(out.SerializeToString())\n        count += 1\n    writer.close()\n    print(f\"Wrote {count} elements to TFRecord\")\n    return count","d45b01cb":"write_text_to_tfr(text_data=train_sequence_matrix, label=y_train, filename=\"jigsaw_toxic_comment_train\")","e5f2ddde":"write_text_to_tfr(text_data=test_sequence_matrix, label=y_test, filename=\"jigsaw_toxic_comment_test\")","bb2f2474":"# Divide into test-train","838ad2ee":"# Clean the data","d069ed4d":"# One-hot encoding the labels","70411efd":"## Serializing and Writing \n\nNow, we'll create a dictionary to store the actual image, height, width and depth of the image and the label where we first serialize the array and then convert it to a bytes_feature.  All these `key:value` mappings make up the features for one Example.\n","365816e0":"# Data Processing","50a81ef8":"# Load the data","2fb9bfe0":"## What are TPUs?\nThe Tensor Processing Unit (TPU) is a custom integrated chip, designed specifically to accelerate the process of training machine learning models. \n\n## TPUs for free at Kaggle\n**You can use up to 30 hours per week of TPUs and up to 9h at a time in a single session.**\n**For more info you can visit [here](https:\/\/www.kaggle.com\/docs\/tpu).**\n\n## Why do we need TFRecord format?\nThe TFRecord format is tensorflow's custom data format which is simple for storing a sequence of binary records. The advantages of using TFRecords are amazingly more efficient storage, fast I\/O, self-contained files, etc. The main advantage of TPUs are faster I\/O which results in faster model training.\n\nFor understanding the basics of TFRecords, please visit Ryan Holbrook notebook: [TFRecords Basics](https:\/\/www.kaggle.com\/ryanholbrook\/tfrecords-basics).\n\n**In this notebook you will learn how to convert text dataset into TFRecord format.**\n\n## Useful resources which helped me:\u00b6\n- https:\/\/www.tensorflow.org\/tutorials\/load_data\/tfrecord\n- https:\/\/www.kaggle.com\/mgornergoogle\/five-flowers-with-keras-and-xception-on-tpu\n- https:\/\/towardsdatascience.com\/a-practical-guide-to-tfrecords-584536bc786c\n- https:\/\/www.kaggle.com\/omkargangan\/commonlit-readability-competition\n- https:\/\/cloud.google.com\/blog\/products\/ai-machine-learning\/what-makes-tpus-fine-tuned-for-deep-learning\n- https:\/\/pub.towardsai.net\/writing-tfrecord-files-the-right-way-7c3cee3d7b12","45f59817":"## Feature Creation functions\n\nThe following functions can be used to convert a value to a type compatible which takes a scalar input values and returns a tf.train.Feature.","db7e00dd":"# Imports"}}