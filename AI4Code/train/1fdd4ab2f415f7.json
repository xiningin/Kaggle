{"cell_type":{"95939723":"code","a862a9d5":"code","1c2db271":"code","ea97c2ab":"code","a8e84054":"code","91b54c44":"code","690da2b9":"code","8726cdae":"code","a3931406":"code","38a91b2e":"code","af604222":"code","84ffeda4":"code","c69a5c4b":"code","06b8e014":"code","04cbd5fd":"code","f68e9bfc":"code","ba7b48b4":"code","de577e49":"code","4bd77f8e":"code","0198ff52":"code","d2608f3c":"code","17956886":"code","25171f9f":"code","bf11aa62":"markdown"},"source":{"95939723":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a862a9d5":"data = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")","1c2db271":"data.head()","ea97c2ab":"# Unamed: 32 seems to be useless, so we'll rid it later\ndata.isna().sum()","a8e84054":"# We need to see the distribution of diagnoses to determine bias.\n# E.g. is it 50\/50?\ndata[\"diagnosis\"].value_counts().plot(kind=\"pie\", labels=[\"benign\", \"malignant\"])","91b54c44":"# Lets describe data for malignant v benign to gain insights\ndf_ben = data.loc[data[\"diagnosis\"] == \"B\"]\ndf_mal = data.loc[data[\"diagnosis\"] == \"M\"]","690da2b9":"df_ben.describe()","8726cdae":"df_mal.describe()","a3931406":"data.dtypes","38a91b2e":"# And maybe drop others\nX_data = data.drop(columns=[\"diagnosis\"])","af604222":"# Before creating corr and confusion matrix,\n# we need to transform our output labels\n\nfrom sklearn import preprocessing\n\nle = preprocessing.LabelEncoder()\ny_labels = le.fit_transform(data[\"diagnosis\"])","84ffeda4":"encoded_df = pd.concat([X_data, pd.Series(y_labels, name=\"diagnosis\")], axis=1)","c69a5c4b":"corr = encoded_df.corr()\n(corr[\"diagnosis\"]).abs().sort_values(ascending=False)","06b8e014":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(30, 20))\nsns.heatmap(corr, cmap=\"Greens\",annot=True)","04cbd5fd":"X_data = X_data.drop(columns=[\"Unnamed: 32\"])\nX_data.columns","f68e9bfc":"df_ben = df_ben.drop(columns=[\"Unnamed: 32\"])\ndf_mal = df_mal.drop(columns=[\"Unnamed: 32\"])","ba7b48b4":"import numpy as np\n\n# Return comparable stats\ndef statistics(dataframes, column) -> dict:\n    allStats = dict()\n    \n    for name, df in enumerate(dataframes):\n        allStats[\"df\" + str(name+1) + \" minimum\"] = np.min(df[column])\n        allStats[\"df\" + str(name+1) + \" maximum\"] = np.max(df[column])\n        allStats[\"df\" + str(name+1) + \" mean\"]    = np.mean(df[column])\n    \n    return allStats\n\n# Distinguish between two dfs per feature\ndef distinguish(dataframes, columns=\"all\"):\n    assert dataframes[0].dtype == pd.DataFrame\n    \n    print(\"-----Your dataframes-----\")\n    print()\n    \n    if columns==\"all\":\n        cols = dataframes[0].columns()\n    \n    for column in cols:\n        print(f\"Feature name: {column} --> {statistics(dataframes, column)}\")\n        print()\n            \n    ","de577e49":"distinguish([df_ben.drop(columns=\"diagnosis\"), df_mal.drop(columns=\"diagnosis\")])","4bd77f8e":"\"\"\" What I notice:\n\n- perimeter_mean for malignant seems to be nearly always significantly larger than that of benign\n- every feature has a positive correlation with diagnosis\n\n\"\"\"","0198ff52":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nscaler = StandardScaler()\nX_train, X_test, y_train, y_test = train_test_split(scaler.fit(X_data).transform(X_data), y_labels)","d2608f3c":"def train_models(X_train, y_train):\n    \n    from sklearn.linear_model import LogisticRegression\n    log_model = LogisticRegression().fit(X_train, y_train)\n    \n    from sklearn.neighbors.classification import KNeighborsClassifier\n    knn_model = KNeighborsClassifier().fit(X_train, y_train)\n    \n    from sklearn.tree import DecisionTreeClassifier\n    tree_model = DecisionTreeClassifier().fit(X_train, y_train)\n    \n    return log_model, knn_model, tree_model","17956886":"log_clf, knn_clf, tree_clf = (train_models(X_train, y_train))","25171f9f":"from sklearn.metrics import accuracy_score\n\nprint(\"Accuracy logistic:\", accuracy_score(y_test, log_clf.predict(X_test)))\nprint(\"Accuracy KNN:\", accuracy_score(y_test, knn_clf.predict(X_test)))\nprint(\"Accuracy tree:\", accuracy_score(y_test, tree_clf.predict(X_test)))","bf11aa62":"### So about 60-65% of our training and testing predictions should each be benign"}}