{"cell_type":{"43a2da9b":"code","c107af7b":"code","faabbd13":"code","85eb5c8e":"code","5da060bd":"code","c648c58e":"code","9cb1ce63":"code","21a496ea":"code","06f1641b":"code","f8ca727a":"code","213f9277":"code","f2cfe130":"code","c093262e":"code","7e8af75b":"code","79a1b734":"code","af7e4a05":"code","2f840219":"code","481de2cf":"code","72893072":"code","2a16f5d9":"code","ea194aa6":"code","e866e6e2":"code","6f2cfd57":"code","fd8fb5b8":"code","1df322a7":"code","21547788":"code","28a88a64":"code","44f8fb39":"code","52502421":"code","1d4671f5":"code","aa3a9e60":"code","673a849b":"code","f12bb3c3":"markdown","9dd1970e":"markdown","e653a0f1":"markdown","85c0252a":"markdown","49291862":"markdown","7f40ffa4":"markdown","b0d260b1":"markdown","2baa202a":"markdown","90eaee27":"markdown","2838ca61":"markdown","7576bfc7":"markdown","e2d87588":"markdown","6a017f3d":"markdown","6f56d612":"markdown","af44ac3f":"markdown","ac42aadd":"markdown","5689026f":"markdown","e62c117d":"markdown","b9cb9ad5":"markdown","f55ac763":"markdown","e9dc2f07":"markdown","b18f8663":"markdown","5eeea49d":"markdown","066b2b30":"markdown","73be83f9":"markdown"},"source":{"43a2da9b":"# This Python 3 environment comes with many helpful analytics libraries installed (Hidden Input\/Output)\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c107af7b":"# Installing SweetViz (Hidden Input\/Output)\n\n!pip install sweetviz","faabbd13":"# Importing all the necessary libraries (Hidden Input)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport sweetviz as sv\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import Lasso, LinearRegression, Ridge, ElasticNet\nfrom sklearn.metrics import r2_score, mean_squared_error\nimport warnings\nwarnings.filterwarnings(\"ignore\")\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\n","85eb5c8e":"# Creating the dataframe (Hidden Input)\n\ndf = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n\ndf.head().style.set_properties(**{'background-color': 'black',\n                            'color': 'lawngreen',\n                            'border-color': 'white', 'row_heading':'blue'})","5da060bd":"# Checking the number of rows and columns (Hidden Input)\n\nprint(\"Number of rows: \",df.shape[0])\nprint(\"Number of columns: \",df.shape[1])","c648c58e":"# EDA using SweetViz (Hidden Input)\n\nmy_report = sv.analyze(df)\nmy_report.show_notebook(  w=None, \n                h=None, \n                scale=None,\n                layout='widescreen',\n                filepath=None)","9cb1ce63":"# Columns to be dropped (Dropping columns with more than 40% missing values) (Hidden Input)\n\ncols_to_drop = ['Alley','FireplaceQu','PoolQC','Fence','MiscFeature','Id']\n\n'''\nAlso by looking at the individual features, we can see that the following categorical features- 'Street', 'LandContour',\n'Utilities', 'LandSlope','Condition1', 'Condition2', 'BldgType', 'RoofMatl', 'ExterCond', 'BsmtCond', 'BsmtFinType2',\n'Heating', 'CentralAir', 'Electrical','BsmtHalfBath', 'KitchenAbvGr', 'Functional', 'GarageQual', 'GarageCond',\n'PavedDrive', 'SaleType','SaleCondition' have one value that completely dominates other features. In this case, it is better\nto drop such columns, because deriving insights from them is useless.\n'''\ndrop = ['Street', 'LandContour','Utilities', 'LandSlope','Condition1', 'Condition2', 'BldgType', 'RoofMatl', 'ExterCond',\n        'BsmtCond', 'BsmtFinType2','Heating', 'CentralAir', 'Electrical','BsmtHalfBath', 'KitchenAbvGr', 'Functional', 'GarageQual',\n        'GarageCond','PavedDrive', 'SaleType','SaleCondition']\n\nfor cols in drop:\n    cols_to_drop.append(cols)\n\ndf.drop(columns=cols_to_drop, axis=1, inplace=True)","21a496ea":"# Performing all the above mentioned manipulations (Hidden Input)\n\n# For LotFrongtage:\n\ndf[\"LotFrontage\"].fillna(0, inplace=True)\n\n# For MasVnrType and MasVnrArea:\n\ndf[\"MasVnrType\"].fillna(\"None\", inplace=True)\ndf[\"MasVnrArea\"].fillna(0, inplace=True)\n\n# For BsmtQual, BsmtFinType1 and BsmtExposure:\n\ndf[\"BsmtQual\"].fillna(\"NoBasement\", inplace=True)\ndf[\"BsmtFinType1\"].fillna(\"NoBasement\", inplace=True)\ndf[\"BsmtExposure\"].fillna(\"NoBasement\", inplace=True)\n\n# For GarageType and GarageFinish:\n\ndf[\"GarageType\"].fillna(\"NoGarage\", inplace=True)\ndf[\"GarageFinish\"].fillna(\"NoGarage\", inplace=True)\n\n# For GarageYrBlt\n\ndf[\"GarageYrBlt\"].fillna(0, inplace=True)","06f1641b":"# Checking the Descriptive statistics of the numerical columns (Hidden Input)\n\ndf.describe(percentiles=[0.01,0.1,0.5,0.95,0.98,0.99]).T.style.bar(\n    subset=['mean'],\n    color='lightsalmon').background_gradient(\n    subset=['std'], cmap='plasma').background_gradient(subset=['99%','98%'], cmap='plasma').background_gradient(\n    subset=['max'], cmap='plasma')","f8ca727a":"# Outlier treatment (Hidden Input)\n\nout_cols = [\"LotArea\", \"BsmtFinSF1\", \"TotalBsmtSF\", \"1stFlrSF\", \"GrLivArea\", \"MiscVal\"]\n\nfor col in out_cols:\n    Q3 = df[col].quantile(0.99)\n    df = df[df[col] <= Q3]","213f9277":"df.shape","f2cfe130":"# Reading the test data (Hidden Input)\n\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\ndf_test.head().style.set_properties(**{'background-color': 'black',\n                            'color': 'lawngreen',\n                            'border-color': 'white', 'row_heading':'blue'})","c093262e":"# EDA using SweetViz (Hidden Input)\n\nmy_report = sv.analyze(df_test)\nmy_report.show_notebook(w=None, \n                h=None, \n                scale=None,\n                layout='widescreen',\n                filepath=None)","7e8af75b":"# Columns to be dropped (Dropping columns with more than 40% missing values) (Hidden Input)\n\ncols_to_drop = ['Alley','FireplaceQu','PoolQC','Fence','MiscFeature','Id']\n\n'''\nAlso by looking at the individual features, we can see that the following categorical features- 'Street', 'LandContour',\n'Utilities', 'LandSlope','Condition1', 'Condition2', 'BldgType', 'RoofMatl', 'ExterCond', 'BsmtCond', 'BsmtFinType2',\n'Heating', 'CentralAir', 'Electrical','BsmtHalfBath', 'KitchenAbvGr', 'Functional', 'GarageQual', 'GarageCond',\n'PavedDrive', 'SaleType','SaleCondition' have one value that completely dominates other features. In this case, it is better\nto drop such columns, because deriving insights from them is useless.\n'''\ndrop = ['Street', 'LandContour','Utilities', 'LandSlope','Condition1', 'Condition2', 'BldgType', 'RoofMatl', 'ExterCond',\n        'BsmtCond', 'BsmtFinType2','Heating', 'CentralAir', 'Electrical','BsmtHalfBath', 'KitchenAbvGr', 'Functional', 'GarageQual',\n        'GarageCond','PavedDrive', 'SaleType','SaleCondition']\n\nfor cols in drop:\n    cols_to_drop.append(cols)\n\ndf_test.drop(columns=cols_to_drop, axis=1, inplace=True)","79a1b734":"# Performing all the above mentioned manipulations (Hidden Input)\n\n# For LotFrongtage:\n\ndf_test[\"LotFrontage\"].fillna(0, inplace=True)\n\n# For MsZoning:\n\ndf_test[\"MSZoning\"].fillna(\"RL\", inplace=True)\n\n# For Exterior1st and Exterior2nd:\n\ndf_test[\"Exterior1st\"].fillna(\"VinylSd\", inplace=True)\ndf_test[\"Exterior2nd\"].fillna(\"VinylSd\", inplace=True)\n\n# For MasVnrType and MasVnrArea:\n\ndf_test[\"MasVnrType\"].fillna(\"None\", inplace=True)\ndf_test[\"MasVnrArea\"].fillna(0, inplace=True)\n\n# For BsmtQual, BsmtFinType1, BsmtExposure etc:\n\ndf_test[\"BsmtQual\"].fillna(\"NoBasement\", inplace=True)\ndf_test[\"BsmtFinType1\"].fillna(\"NoBasement\", inplace=True)\ndf_test[\"BsmtExposure\"].fillna(\"NoBasement\", inplace=True)\ndf_test[\"BsmtFinSF1\"].fillna(0, inplace=True)\ndf_test[\"BsmtFinSF2\"].fillna(0, inplace=True)\ndf_test[\"BsmtUnfSF\"].fillna(0, inplace=True)\ndf_test[\"TotalBsmtSF\"].fillna(0, inplace=True)\ndf_test[\"BsmtFullBath\"].fillna(\"0\", inplace=True)\n\n# For GarageType and GarageFinish etc:\n\ndf_test[\"GarageType\"].fillna(\"NoGarage\", inplace=True)\ndf_test[\"GarageFinish\"].fillna(\"NoGarage\", inplace=True)\n\n# For GarageYrBlt etc\n\ndf_test[\"GarageYrBlt\"].fillna(0, inplace=True)\ndf_test[\"GarageArea\"].fillna(480, inplace=True)\ndf_test[\"GarageCars\"].fillna(0, inplace=True)\n\n# For KitchenQual\n\ndf_test[\"KitchenQual\"].fillna(\"TA\", inplace=True)","af7e4a05":"df_test.shape","2f840219":"# Concatenating both train and test data (This is done to handle all the different category value possibilites )\n\n# Thanks to Krish Naik's tutorial (https:\/\/www.youtube.com\/watch?v=vtm35gVP8JU)\n\nfinal_df = pd.concat([df, df_test], axis=0)\n\nfinal_df.shape","481de2cf":"# Creating the dummy variables for all the categorical features\n\ncat_cols = list(final_df.select_dtypes(\"object\"))\n\nfor col in cat_cols:\n    dummy_cols = pd.get_dummies(final_df[col], drop_first=True, prefix=col)\n    final_df = pd.concat([final_df,dummy_cols],axis=1)\n    final_df.drop(columns=col, axis=1, inplace=True)","72893072":"# Removing any duplicates\n\nfinal_df = final_df.loc[:,~final_df.columns.duplicated()]","2a16f5d9":"# Splitting into df_train and df_test\n\ndf_train = final_df.iloc[:1372,:]\ndf_test = final_df.iloc[1372:,:]\n\nprint(df_train.shape)\nprint(df_test.shape)","ea194aa6":"# Dropping the \"SalePrice\" column from df_test\n\ndf_test.drop(columns=\"SalePrice\", axis=1, inplace=True)","e866e6e2":"# Splitting the data into train and test\n\ny_train = df_train.pop(\"SalePrice\")\nX_train = df_train","6f2cfd57":"# Normalizing the data (Reason: Normalizing the data will have even the outlier values in the range of 0 to 1)\n\nnum_cols = list(df_train.select_dtypes([\"int64\",\"float64\"]))\n\nscaler = MinMaxScaler()\n\nX_train[num_cols] = scaler.fit_transform(X_train[num_cols])","fd8fb5b8":"num_cols_test = list(df_test.select_dtypes([\"int64\",\"float64\"]))\n\ndf_test[num_cols_test] = scaler.transform(df_test[num_cols_test])","1df322a7":"# Using RFE to select the features...\n\nlr = LinearRegression()\nlr.fit(X_train,y_train)\nrfe = RFE(lr,70)\nrfe.fit(X_train,y_train)","21547788":"# Creating a column for all the RFE features\n\nrfe_scores = pd.DataFrame(list(zip(X_train.columns,rfe.support_,rfe.ranking_)))\nrfe_scores.columns = ['Column_Names','Status','Rank']\nrfe_scores","28a88a64":"# Columns selected by RFE...\n\nrfe_sel_columns = list(rfe_scores[rfe_scores.Status==True].Column_Names)\n\n# Creating our X_train and X_test with the select columns...\n\nX_train = X_train[rfe_sel_columns]","44f8fb39":"# Using Lasso Regression\n\nls = Lasso(alpha=0.001)\nls.fit(X_train,y_train)\n\ny_train_pred = ls.predict(X_train)\nprint(\"R2 score on the training data: \" + str(r2_score(y_true=y_train,y_pred=y_train_pred)))\nprint(\"RMSE for our training data: \" + str(np.sqrt(mean_squared_error(y_train,y_train_pred))))","52502421":"# Using Ridge Regression\n\nridge = Ridge(alpha=0.001)\nridge.fit(X_train,y_train)\n\ny_train_pred = ridge.predict(X_train)\nprint(\"R2 Score for our training data: \" + str(r2_score(y_train,y_train_pred)))\nprint(\"RMSE for our training data: \" + str(np.sqrt(mean_squared_error(y_train,y_train_pred))))","1d4671f5":"# Using Ridge Regression\n\nen = ElasticNet(alpha=0.001)\nen.fit(X_train,y_train)\n\ny_train_pred = en.predict(X_train)\nprint(\"R2 Score for our training data: \" + str(r2_score(y_train,y_train_pred)))\nprint(\"RMSE for our training data: \" + str(np.sqrt(mean_squared_error(y_train,y_train_pred))))","aa3a9e60":"# Reading the sample_submission.csv\n\nsubmission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","673a849b":"my_predictions = pd.DataFrame(en.predict(df_test[rfe_sel_columns]))\n\ndatasets = pd.concat([submission['Id'],my_predictions], axis=1)\n\ndatasets.columns = ['Id','SalePrice']\n\ndatasets.to_csv('submission.csv', index=False)","f12bb3c3":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"6\" style=\"text-decoration:None\">\n    <h3 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Outlier Treatment <\/h3>\n    <\/a>\n<\/div>","9dd1970e":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"3\" style=\"text-decoration:None\">\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Data Cleaning after Analysis <\/h1>\n    <\/a>\n<\/div>","e653a0f1":"<div class=\"alert alert-success\">\n    <h1 style=\"color:black\">\ud83d\udcc4About the Data:<\/h1><br>\n        <p style=\"color:black;font-size:120%;\"> Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. <\/p>\n            <p style=\"color:black;font-size:120%;\">With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.<\/p>\n    \n<h1 style=\"color:black\">\ud83d\udcc4Problem Statement:<\/h1>   \n    \n<p style=\"color:black;font-size:120%;\">Predicting the price of houses in Ames, Iowa.<\/p>   \n    \n<\/div>","85c0252a":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"11\" style=\"text-decoration:None\">\n    <h3 style=\"text-align:center;font-weight: 20px; color:black;\">\n       ElasticNet Regression <\/h3>\n    <\/a>\n<\/div>","49291862":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"14\" style=\"text-decoration:None\">\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       The Result <\/h1>\n    <\/a>\n<\/div>","7f40ffa4":"<div class=\"alert alert-success\">\n    \n<p style=\"font-size:120%;color:black\">\ud83c\udfaf Firstly, we will drop any columns that have more than <strong>40% missing values<\/strong>.<\/p>\n    \n<p style=\"font-size:120%;color:black\">\ud83c\udfaf Also by looking at the individual features, we can see that the following categorical features- <strong>'Street', 'LandContour','Utilities', 'LandSlope','Condition1', 'Condition2', 'BldgType', 'RoofMatl', 'ExterCond', 'BsmtCond', 'BsmtFinType2', 'Heating', 'CentralAir', 'Electrical','BsmtHalfBath', 'KitchenAbvGr', 'Functional', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType','SaleCondition'<\/strong> have one value that completely dominates other features. In this case, it is better to drop such columns, because deriving insights from them is useless.<\/p>\n    \n<\/div>","b0d260b1":"<div class=\"alert alert-block alert-info\">\n    \n<h1 style=\"text-align:center;font-weight: 20px; color:black;\">\ud83d\udc77\u200d\u2642\ufe0f\u26a0Notebook Still Under Construction\u26a0\ud83d\udc77\u200d\u2642\ufe0f <\/h1>\n    \n<\/div>","2baa202a":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"8\" style=\"text-decoration:None\">\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Model Building <\/h1>\n    <\/a>\n<\/div>","90eaee27":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"2\" style=\"text-decoration:None\">\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Exploratory Data Analysis using SweetViz <\/h1>\n    <\/a>\n<\/div>","2838ca61":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"13\" style=\"text-decoration:None\">\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Predicting on Test Data <\/h1>\n    <\/a>\n<\/div>","7576bfc7":"<div class=\"alert alert-success\">\n<ol>   \n<li style=\"font-size:150%;color:black\"><a href=\"#1\" style=\"text-decoration:None\"> Loading Data <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#2\" style=\"text-decoration:None\"> Performing EDA using SweetViz <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#3\" style=\"text-decoration:None\"> Data Cleaning after Analysis <\/a><\/li>\n<ul>\n<li style=\"font-size:150%;color:black\"><a href=\"#4\" style=\"text-decoration:None\"> Dropping Necessary Columns <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#5\" style=\"text-decoration:None\"> Imputing Missing Values <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#6\" style=\"text-decoration:None\"> Handling Outliers <\/a><\/li>\n<\/ul>    \n<li style=\"font-size:150%;color:black\"><a href=\"#11\" style=\"text-decoration:None\"> Importing and Cleaning the Test Data <\/a><\/li>   \n    \n<li style=\"font-size:150%;color:black\"><a href=\"#7\" style=\"text-decoration:None\"> Preparing the data before Modelling <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#8\" style=\"text-decoration:None\"> Model Building <\/a><\/li>\n<ul>\n<li style=\"font-size:150%;color:black\"><a href=\"#9\" style=\"text-decoration:None\"> Lasso Regression <\/a><\/li>\n    \n<li style=\"font-size:150%;color:black\"><a href=\"#10\" style=\"text-decoration:None\"> Ridge Regression <\/a><\/li>\n<\/ul> \n<\/ol>    \n  <h2 style=\"color:Black;\">Legend:<\/h2>\n    <p style=\"color:Black;font-size:120%\">\ud83c\udfaf: Important Steps or points<\/p>\n    <p style=\"color:Black;font-size:120%\">\ud83d\udc8e: Important tips<\/p>\n    <p style=\"color:Black;font-size:120%\">\u2b50: Reference Links<\/p>\n    <p style=\"color:Black;font-size:120%\">\ud83d\udcc4: Article, thread or informations<\/p>\n<\/div>\n\n    ","e2d87588":"![image.png](attachment:8c67ded7-e1f8-4186-af4b-b04cb3bdb055.png)","6a017f3d":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"1\" style=\"text-decoration:None\">\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Loading the Data <\/h1>\n    <\/a>\n<\/div>","6f56d612":"<div class=\"alert alert-success\">\n    \n<p style=\"font-size:120%;color:black\">\ud83c\udfaf Firstly, for the feature <strong>Lot Frontage<\/strong> we see a lot of missing values. About <strong>18%<\/strong> of the data is missing. When we refer to the Metadata file that we have, this feature refers to the <strong>Linear feet of street connected to property<\/strong>. In this case, the missing values suggest that for the houses without this data are probably those ones, that are situated away from the street. In this case we will <strong>impute the missing values with 0<\/strong>.<\/p>\n    \n<p style=\"font-size:120%;color:black\">\ud83c\udfaf Next, we can observe that there are few missing values for <strong>MasVnrType<\/strong> and <strong>MasVnrArea<\/strong>. Less than <strong>1%<\/strong>. In this case, we will <strong>impute the missing values with None and 0 respectively<\/strong>.<\/p>\n    \n<p style=\"font-size:120%;color:black\">\ud83c\udfaf For <strong>BsmtQual<\/strong>, <strong>BsmtFinType1<\/strong> and <strong>BsmtExposure<\/strong>, there are <strong>3%<\/strong> missing values. This is due to the fact that, not all house will have a basement. In-case of these, we will introduce a new value in the feature: <strong>No Basement<\/strong>.<\/p>\n\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf Lastly, for the features- <strong>GarageType<\/strong>, <strong>GarageYrBlt<\/strong> and <strong>GarageFinish<\/strong>, we have about <strong>3%<\/strong> missing values. This is due to, unavailability of Garage along with the property. In this case, we will impute these missing values by a new value: <strong>No Garage<\/strong> for GarageType and GarageFinish. However, for the <strong>GarageYrBlt<\/strong>, we will impute the missing values here with <strong>0<\/strong> no available year.<\/p> \n    \n<\/div>","af44ac3f":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"9\" style=\"text-decoration:None\">\n    <h3 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Lasso Regression <\/h3>\n    <\/a>\n<\/div>","ac42aadd":" <p style=\"text-align:center; font-size:150%\"><i><strong> \u201cIt's not the size of the house. It's how much love is inside\u201d <\/i>\u2013 Martina McBride <\/strong><\/p>","5689026f":"<div class=\"alert alert-block alert-info\" >\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Will we be able to predict House Prices?<br><\/br> Let's find out... <\/h1>\n<\/div>","e62c117d":"<div class=\"alert alert-success\">\n    \n<p style=\"font-size:120%;color:black\">\ud83c\udfaf By comparing the <strong>98th, 99th<\/strong> and <strong>100th<\/strong> percentile, we can see that the numerical features: <strong>LotArea, BsmtFinSF1, TotalBsmtSF, 1stFlrSF, GrLivArea, MiscVal<\/strong> have outliers.<\/p>\n    \n<p style=\"font-size:120%;color:black\">\ud83c\udfaf On the other hand checking <strong>0th<\/strong> and <strong>1st<\/strong> percentile, we can see that most numerical features do not have outliers.<\/p>\n    \n<\/div>","b9cb9ad5":"<div class=\"alert alert-block alert-info\" >\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Table of Contents <\/h1>\n<\/div>","f55ac763":"<div class=\"alert alert-success\">\n<p style=\"font-size:120%;color:black\">\ud83c\udfaf <strong>SweetViz<\/strong> is an open-source Python library that generates beautiful, high-density visualizations to kickstart EDA (Exploratory Data Analysis) with just two lines of code. Output is a fully self-contained HTML application.<\/p>\n\n<p style=\"font-size:120%;color:black\">The system is built around quickly visualizing target values and comparing datasets. Its goal is to help quick analysis of target characteristics, training vs testing data, and other such data characterization tasks.<\/p>\n    \n<p style=\"font-size:120%;color:black\">\ud83c\udfaf We are going to use SweetViz here, since there are a lot of features. Almost 81.<\/p>\n    \n<p style=\"font-size:120%;color:black\">\u2b50 To understand better follow <a href=\"https:\/\/pypi.org\/project\/sweetviz\/\"> this <\/a> link<\/p>\n<\/div>","e9dc2f07":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"5\" style=\"text-decoration:None\">\n    <h3 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Imputing missing values <\/h3>\n    <\/a>\n<\/div>","b18f8663":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"12\" style=\"text-decoration:None\">\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Importing and Cleaning the Test Data <\/h1>\n    <\/a>\n<\/div>","5eeea49d":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"4\" style=\"text-decoration:None\">\n    <h3 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Dropping Unnecessary Columns <\/h3>\n    <\/a>\n<\/div>","066b2b30":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"7\" style=\"text-decoration:None\">\n    <h1 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Preparing the data before Model Building <\/h1>\n    <\/a>\n<\/div>","73be83f9":"<div class=\"alert alert-block alert-info\" >\n    <a id=\"10\" style=\"text-decoration:None\">\n    <h3 style=\"text-align:center;font-weight: 20px; color:black;\">\n       Ridge Regression <\/h3>\n    <\/a>\n<\/div>"}}