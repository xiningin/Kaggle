{"cell_type":{"2e2e1ec0":"code","9f802e2e":"code","e755d7e2":"code","bc9e173f":"code","310f9299":"code","58f8494b":"code","456564a8":"code","e672ac25":"code","7eeb6838":"code","04d1d84c":"code","ac7601e5":"code","7304c951":"code","0a13af01":"code","41b7614b":"code","0ea82fbc":"code","c996e9a2":"code","8686b086":"code","d3fb24d3":"code","4120e4b6":"code","9f7868aa":"code","e6fa4aa5":"code","c94f1d46":"code","e9a475b6":"code","30522a55":"code","4bf60e7c":"code","5a2735d4":"code","404745cf":"code","591a4a4e":"code","2ce5eb29":"code","0bdbf6ad":"code","23e362df":"code","a5c1841b":"code","8136f991":"code","6f20e358":"code","dce1b9e5":"code","250c275a":"code","e36125a2":"code","5eeb6422":"code","70ea471a":"code","5749cbd4":"code","aa04e742":"code","c3fdd68b":"code","bab9d069":"code","05d5b6ab":"code","ad37e60a":"code","cb742985":"code","26e7d522":"code","541ee7e6":"code","edf8420d":"code","02d273c7":"code","be9ec5ee":"code","ee113d54":"code","4138059c":"code","8b8b3410":"code","e4e0f772":"code","16452534":"code","c2fdcb8e":"code","656d96ca":"code","2b27910d":"code","cfbb42f2":"code","8d687df9":"code","496e0c48":"code","a6dcc87c":"code","c94a0ebb":"code","3b3a0b98":"code","9cc191a5":"code","3e8a1b92":"code","55de5aad":"code","901b4f32":"code","d1a6942c":"code","97517aa5":"code","78423aa3":"code","3ae8c0b5":"code","26cedb76":"code","d24f36ea":"code","02d9a76e":"code","2f3d2195":"code","458f7d1c":"code","7f9d6e86":"code","81e4f522":"code","dabab7bc":"code","916ca5a5":"code","72fc5017":"code","f6464e9d":"code","3487d17a":"code","e4c7ba67":"code","f97663b2":"code","1c2452b7":"code","7b597308":"code","db9692e0":"code","6cbb980b":"code","8a263854":"code","b2a92680":"code","2437e88b":"code","8cd80582":"code","0d1a3ca2":"code","984ed351":"code","e620fa9a":"code","0d15768e":"code","5bea625d":"code","08e99037":"code","5b30414f":"code","b274f6ac":"code","6689a2e6":"code","c0199635":"markdown","211817cd":"markdown","5192000f":"markdown","efbc17c3":"markdown","beaab216":"markdown","7cb2a89a":"markdown","5ac3ff47":"markdown","981066f2":"markdown","e72eabdd":"markdown","2f3ae90b":"markdown","4506b6e2":"markdown","cb859bc2":"markdown","7defaaf1":"markdown","a724f9f3":"markdown","43cfa949":"markdown","21fa4de7":"markdown","dff7f714":"markdown","c39d377b":"markdown","23844d6f":"markdown","6bd7ebd9":"markdown","a44377b0":"markdown","3cca86f4":"markdown","f175de6a":"markdown","fd461cf1":"markdown","4fb0c64b":"markdown","448d42b5":"markdown","e8fa9fab":"markdown","88e9cfed":"markdown","f63afd71":"markdown","d50c6a47":"markdown","6274ea8f":"markdown","d422a0ae":"markdown","d72a8cbe":"markdown","77f6fcc5":"markdown","21a280bd":"markdown","8c6d61f2":"markdown","a8927a6c":"markdown","b9ad8dd6":"markdown","913a3293":"markdown","3aa3c8f5":"markdown","0f1b5d67":"markdown","17c9dfa5":"markdown","6a4c64b5":"markdown","a1b3a2da":"markdown","97457f41":"markdown","8a15952e":"markdown","d4bbe7e3":"markdown","7ae196eb":"markdown","e597d035":"markdown","b54242cf":"markdown","5a4e110e":"markdown","4b5fd39f":"markdown","432fcf2d":"markdown","f22c7f66":"markdown","e46cc4d5":"markdown","70112370":"markdown","39b0e660":"markdown","7aa552b2":"markdown","b1759b0c":"markdown","c350f429":"markdown","b750b716":"markdown","06f61a4e":"markdown","94a9cfd7":"markdown","dce4c6e1":"markdown","8e2521c0":"markdown","459430f8":"markdown","cc5a96fa":"markdown","7722ae17":"markdown","25b9a411":"markdown","37122ce3":"markdown","f78e0bad":"markdown","a2f0ad8a":"markdown","0d7e9032":"markdown","54fbc802":"markdown","f823ea38":"markdown"},"source":{"2e2e1ec0":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime as dt\n\n%matplotlib inline\n\npd.set_option('display.max_rows', 50)\npd.set_option('display.max_columns', 50)\n\nitems = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/items.csv\")\nitem_categories = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv\")\nshops = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/shops.csv\")\nsample_submission = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sample_submission.csv\")\n\ntrain_o = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv\")\ntest_o = pd.read_csv(\"..\/input\/competitive-data-science-predict-future-sales\/test.csv\")","9f802e2e":"!pip install googletrans","e755d7e2":"from googletrans import Translator\n\ntranslator = Translator()\n\ndef translate(df, feature, src, dest):\n    return df[feature].apply(translator.translate, src=src, dest=dest).apply(getattr, args=('text',))","bc9e173f":"item_categories[\"item_category_name_en\"] = translate(item_categories, \"item_category_name\", \"ru\", \"en\")\nshops[\"shop_name_en\"] = translate(shops, \"shop_name\", \"ru\", \"en\")\n\nitem_categories.drop(\"item_category_name\", axis=1, inplace=True)\nshops.drop(\"shop_name\", axis=1, inplace=True)","310f9299":"shops[\"shop_name_en\"].head(10)","58f8494b":"shops[\"city\"] = shops[\"shop_name_en\"].str.replace(\"[!,?,\u00b2]\", \"\").str.lower().str.strip().str.split(\" \").str.get(0)\nshops[\"city\"].value_counts()","456564a8":"not_shops = [\"digital\", \"offsite\", \"emergency\"]\nshops[\"city\"] = shops[\"city\"].apply(lambda x: \"other\" if x in not_shops else x)\nshops.sort_values(\"city\")","e672ac25":"item_categories[\"item_category_name_en\"].head(20)","7eeb6838":"item_categories[\"master_category\"] = item_categories[\"item_category_name_en\"].str.replace(\"[!,?,\u00b2]\", \"\").str.lower().str.strip().str.split(\"-\").str.get(0).str.strip()\nitem_categories[\"master_category\"].value_counts()","04d1d84c":"item_categories[\"master_category\"] = item_categories[\"master_category\"].apply(lambda x: \"payment cards\" if \"payment cards\" in x else x)\nitem_categories[\"master_category\"] = item_categories[\"master_category\"].apply(lambda x: \"games\" if \"games\" in x else x)\nitem_categories[\"master_category\"] = item_categories[\"master_category\"].apply(lambda x: \"blank media\" if \"blank media\" in x else x)\nitem_categories[\"master_category\"].value_counts()","ac7601e5":"items = pd.merge(items, item_categories, how=\"left\", on='item_category_id')\ntrain_o = pd.merge(train_o, items, how=\"left\", on='item_id')\ntrain_o = pd.merge(train_o, shops, how=\"left\", on='shop_id')\n\ntest_o = pd.merge(test_o, items, how=\"left\", on='item_id')\ntest_o = pd.merge(test_o, shops, how=\"left\", on='shop_id')","7304c951":"def downgrade_dtypes(df):\n    float_cols = list(df.dtypes[df.dtypes == \"float64\"].index)\n    int_cols = list(df.dtypes[(df.dtypes == \"int64\") | (df.dtypes == \"int32\")].index)\n\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    \n    return df","0a13af01":"train_s = downgrade_dtypes(train_o)\ntest_s = downgrade_dtypes(test_o)","41b7614b":"train_s.head(10)","0ea82fbc":"def generate_features_overview(df):\n    df_info = pd.DataFrame()\n    df_info[\"type\"] = df.dtypes\n    df_info[\"missing_count\"] = df.isna().sum()\n    df_info[\"missing_perc\"] = (df_info[\"missing_count\"] \/ len(df) * 100).astype(int)\n    df_info = pd.concat([df_info, df.describe(include='all').T], axis=1)\n    \n    return df_info","c996e9a2":"info_df = generate_features_overview(train_s)\ninfo_df_string = info_df.dropna(subset=[\"unique\"], axis=0).dropna(axis=1)\ninfo_df_numeric = info_df.dropna(subset=[\"mean\"], axis=0).dropna(axis=1)","8686b086":"info_df_string","d3fb24d3":"info_df_numeric","4120e4b6":"train_c1 = train_s.copy()\ntrain_c1[\"date\"] = pd.to_datetime(train_s[\"date\"], format=\"%d.%m.%Y\")\ntrain_c1[\"month\"] = train_c1[\"date\"].dt.month\ntrain_c1[\"year\"] = train_c1[\"date\"].dt.year\ntrain_c1[\"weekday\"] = train_c1[\"date\"].dt.weekday\ntrain_c1[\"day\"] = train_c1[\"date\"].dt.day","9f7868aa":"def lineplot(df, X, Y, title):\n    fig, ax = plt.subplots(1, 1, figsize=(20, 7), sharex=True)\n    sns.lineplot(x=X, y=Y, data=df[Y], ax=ax[0]).set_title(title)\n    plt.show()\n    \n    \ndef lineplot_multiple(df, X, Y, title):\n    sns.lineplot(X, 'value', hue='variable', \n             data=pd.melt(df, X))\n    \ndef barplot(df, feature_1, label_1, x_ticks, title=\"\", width=0.4):\n    fig = plt.figure(figsize =(20, 7))\n    ax = fig.add_subplot()\n\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n        \n    plt.bar(range(len(df)), df[feature_1], align='center', label=label_1, color=\"blue\")\n    plt.xticks(range(len(df)), df[x_ticks], size='small')\n    plt.title(title)\n    plt.grid(False)\n    \n    plt.show()\n    \ndef barplot_double(df, features, labels, x_ticks, width=0.4):\n    fig = plt.figure(figsize =(20, 7))\n    ax = fig.add_subplot()\n\n    ax.spines['top'].set_visible(False)\n    ax.spines['right'].set_visible(False)\n    ax.spines['left'].set_visible(False)\n    ax.spines['bottom'].set_visible(False)\n\n    plt.bar(range(len(df)), df[features[0]], align='center', label=labels[0])\n    plt.bar(range(len(df)), df[features[1]], align='center', label=labels[1])\n        \n    plt.xticks(range(len(df)), df[x_ticks], size='small')\n    plt.grid(False)\n    ax.legend(loc='upper right')\n    plt.show()\n\ndef barplot_sns(df, X, Y, hue, title):\n    fig, ax = plt.subplots(1, 1, figsize=(22, 7), sharex=True)\n    tidy = df.melt(id_vars=X).rename(columns=str.title)\n    sns.barplot(x=X, y=Y, hue=hue, data=tidy, ax=ax[0], palette=\"rocket\").set_title(title)\n    plt.show()\n    \ndef barplot_double_axis(df, feature_1, feature_2, label_1, label_2, x_ticks, width=0.4):\n    fig = plt.figure(figsize =(20, 7))\n    ax1 = fig.add_subplot()\n    ax2 = ax1.twinx()\n    \n    ax1.spines['top'].set_visible(False)\n    ax1.spines['right'].set_visible(False)\n    ax1.spines['left'].set_visible(False)\n    ax1.spines['bottom'].set_visible(False)\n    ax1.grid(False)\n\n        \n    ax2.spines['top'].set_visible(False)\n    ax2.spines['right'].set_visible(False)\n    ax2.spines['left'].set_visible(False)\n    ax2.spines['bottom'].set_visible(False)\n    ax2.grid(False)\n\n    ax1.bar(np.arange(len(df)) + (width \/ 2), df[feature_1], width=width, color=\"red\", label=label_1)\n    ax2.bar(np.arange(len(df)) - (width \/ 2), df[feature_2], width=width, color=\"blue\", label=label_2)\n    \n    ax1.legend(loc='upper left', frameon=False)\n    ax2.legend(loc='upper right', frameon=False)\n    \n    plt.xticks(range(len(df)), df[x_ticks], size='small')\n    plt.grid(False)\n\n    plt.show()\n    \n\ndef box_plot(df, features):\n    fig = plt.figure(figsize =(20, 7))\n\n    # Creating axes instance \n    ax = fig.add_subplot()\n\n    # Creating plot\n    data = []\n    for col in features:\n        data.append(df[col])\n        \n    bp = ax.boxplot(data)\n\n    # show plot\n    plt.show()\n    \ndef box_plot_sns(df, X, Y):\n    fig = plt.figure(figsize =(20, 7))\n    ax = fig.add_subplot()\n    sns.boxplot(x = X, y = Y, ax=ax, data = df)\n    \ndef histogram(df, features, bins=10):\n    df[features].plot.hist(bins=bins)","e6fa4aa5":"train_c1[\"item_cnt_day\"].value_counts(bins=10).sort_index()","c94f1d46":"norm = train_c1[\"item_cnt_day\"].value_counts(normalize=True)*100\nnorm.cumsum().head(10)","e9a475b6":"box_plot_sns(train_c1, \"shop_id\", \"item_cnt_day\")","30522a55":"q_outliers = train_c1[train_c1[\"item_cnt_day\"] >= 1000]\nq_outliers","4bf60e7c":"train_c1 = train_c1[train_c1[\"item_cnt_day\"] <= 1000]","5a2735d4":"train_c1[\"item_price\"].value_counts(bins=20).sort_index()","404745cf":"norm = train_c1[\"item_price\"].value_counts(normalize=True)*100\nnorm.cumsum().head(20)","591a4a4e":"box_plot_sns(train_c1, \"shop_id\", \"item_price\")","2ce5eb29":"train_c2 = train_c1[(train_c1[\"item_price\"] > 0) & (train_c1[\"item_price\"] < 100000)]","0bdbf6ad":"def aggregate(df, group_by, aggfunc, features=[]):\n    #agg_types = np.mean, np.max, np.min, np.count_nonzero, np.sum\n    \n    grouped_df = df.groupby(group_by, as_index=False)\n    \n    if len(features) > 0:\n        grouped_df = grouped_df[features]\n\n    df = grouped_df.agg(aggfunc)\n    return df","23e362df":"agg_1 = aggregate(train_c2, [\"date_block_num\"], np.sum)\n\nbarplot(agg_1, \"item_cnt_day\", \"Quantity\", \"date_block_num\", \"Cronological sales\")","a5c1841b":"agg_2 = aggregate(train_c2[train_c2[\"year\"] <= 2014], [\"month\"], np.sum)\n\nbarplot(agg_2, \"item_cnt_day\", \"Quantity\", \"month\", \"SUM of sales by month (2013 + 2014)\")","8136f991":"agg_3 = aggregate(train_c2, [\"weekday\"], np.sum)\nagg_3_m = agg_3.sort_values(\"item_cnt_day\", ascending=False)\nagg_3_m[\"cumsum\"] = agg_3_m[\"item_cnt_day\"].cumsum()\nagg_3_m[\"cumperc\"] = agg_3_m[\"cumsum\"] \/ agg_3_m[\"item_cnt_day\"].sum()\nagg_3_m[[\"weekday\",\"cumperc\"]]","6f20e358":"barplot(agg_3, \"item_cnt_day\", \"Quantity\", \"weekday\", \"Sales quantity by day in week\")","dce1b9e5":"agg_4 = aggregate(train_c2, [\"shop_id\", \"date_block_num\"], np.sum).sort_values(\"date_block_num\")","250c275a":"def comparison_barplot(ids, feature):\n    compare_ids = ids\n    shop_1 = agg_4[agg_4[feature] == compare_ids[0]]\n    shop_2 = agg_4[agg_4[feature] == compare_ids[1]]\n\n    shop_comp = pd.merge(shop_1, shop_2, how=\"outer\", on='date_block_num')\n    shop_comp.fillna(0, inplace=True)\n\n    shop_comp[\"stacked\"] = shop_comp[\"item_cnt_day_x\"] + shop_comp[\"item_cnt_day_y\"]\n\n    fig, ax1 = plt.subplots(figsize=(20, 7))\n    sns.barplot(x='date_block_num', y='stacked', data=shop_comp, ax=ax1, color=\"red\", label=\"{0}: {1}\".format(feature, compare_ids[1]))\n    sns.barplot(x='date_block_num', y='item_cnt_day_x', data=shop_comp, ax=ax1, color=\"blue\", label=\"{0}: {1}\".format(feature, compare_ids[0]))\n    plt.legend()\n    sns.despine(fig)","e36125a2":"comparison_barplot([10, 11], \"shop_id\")","5eeb6422":"comparison_barplot([23, 24], \"shop_id\")","70ea471a":"comparison_barplot([0, 57], \"shop_id\")","5749cbd4":"comparison_barplot([1, 58], \"shop_id\")","aa04e742":"train_c2.loc[train_c2[\"shop_id\"] == 11, 'shop_id'] = 10\ntrain_c2.loc[train_c2[\"shop_id\"] == 0, 'shop_id'] = 57\ntrain_c2.loc[train_c2[\"shop_id\"] == 1, 'shop_id'] = 5","c3fdd68b":"agg_5 = aggregate(train_c2, [\"shop_id\"], np.sum).sort_values(\"item_cnt_day\", ascending=False)\nagg_5[\"cumsum\"] = agg_5[\"item_cnt_day\"].cumsum()\nagg_5[\"cumperc\"] = agg_5[\"cumsum\"] \/ agg_5[\"item_cnt_day\"].sum()\n\nbarplot(agg_5, \"item_cnt_day\", \"Quantity\", \"shop_id\", \"Sales quantity by shop\")\nbarplot(agg_5, \"cumperc\", \"Quantity\", \"shop_id\", \"Sales quantity by shop - cumulative percentage\")","bab9d069":"agg_6 = aggregate(train_c2, [\"city\"], np.sum).sort_values(\"item_cnt_day\", ascending=False)\nagg_6[\"cumsum\"] = agg_6[\"item_cnt_day\"].cumsum()\nagg_6[\"cumperc\"] = agg_6[\"cumsum\"] \/ agg_6[\"item_cnt_day\"].sum()\nbarplot(agg_6, \"item_cnt_day\", \"Quantity\", \"city\", \"Sales quantity by city\")","05d5b6ab":"agg_7 = aggregate(train_c2, [\"item_category_id\"], np.sum).sort_values(\"item_cnt_day\", ascending=False)\nagg_7[\"cumsum\"] = agg_7[\"item_cnt_day\"].cumsum()\nagg_7[\"cumperc\"] = agg_7[\"cumsum\"] \/ agg_7[\"item_cnt_day\"].sum()\n\nbarplot(agg_7, \"item_cnt_day\", \"Quantity\", \"item_category_id\", \"Sales quantity by item category\")\nbarplot(agg_7, \"cumperc\", \"Quantity\", \"item_category_id\", \"Sales quantity by item category - cumulative percentage\")","ad37e60a":"agg_8 = aggregate(train_c2, [\"master_category\"], np.sum).sort_values(\"item_cnt_day\", ascending=False)\n\nbarplot(agg_8, \"item_cnt_day\", \"Quantity\", \"master_category\", \"Sales quantity by master category\")","cb742985":"avg_price_item_shop_month = train_c2.groupby(['item_id', 'shop_id', 'date_block_num']).agg({\"item_price\": \"mean\"})\navg_price_item_shop = train_c2.groupby(['item_id', 'shop_id']).agg({\"item_price\": \"mean\"})\navg_price_item = train_c2.groupby('item_id').agg({\"item_price\": \"mean\"})\navg_price_category = train_c2.groupby('item_category_id').agg({\"item_price\": \"mean\"})","26e7d522":"import scipy\nitem_shop_sales_detail = aggregate(train_c2, [\"item_id\", \"shop_id\"], {\"date_block_num\": [\"min\", \"max\"], \"weekday\": \"median\", \"day\": \"median\", \"month\": \"median\"})\n#item_shop_sales_detail = train_c2.groupby([\"item_id\", \"shop_id\"])[[\"day\", \"weekday\", \"month\"]].agg(lambda x: scipy.stats.mode(x)[0])\nitem_shop_sales_detail.set_index([\"item_id\", \"shop_id\"], inplace=True)","541ee7e6":"item_sales_detail = aggregate(train_c2, \"item_id\", {\"date_block_num\": [\"min\", \"max\"], \"weekday\": \"median\", \"day\": \"median\", \"month\": \"median\"})\nitem_sales_modes = train_c2.groupby(\"item_id\")[[\"day\", \"weekday\", \"month\"]].agg(lambda x: scipy.stats.mode(x)[0])\nitem_sales_detail.set_index(\"item_id\", inplace=True)\nitem_sales_detail[(\"weekday\", \"mode\")] = item_sales_modes[\"weekday\"]\nitem_sales_detail[(\"day\", \"mode\")] = item_sales_modes[\"day\"]\nitem_sales_detail[(\"month\", \"mode\")] = item_sales_modes[\"month\"]\nitem_sales_detail.describe()","edf8420d":"shop_sales_detail = aggregate(train_c2, \"shop_id\", {\"date_block_num\": [\"min\", \"max\"], \"weekday\": \"median\", \"day\": \"median\", \"month\": \"median\"})\nshop_sales_modes = train_c2.groupby(\"shop_id\")[[\"day\", \"weekday\", \"month\"]].agg(lambda x: scipy.stats.mode(x)[0])\nshop_sales_detail.set_index(\"shop_id\", inplace=True)\nshop_sales_detail[(\"weekday\", \"mode\")] = shop_sales_modes[\"weekday\"]\nshop_sales_detail[(\"day\", \"mode\")] = shop_sales_modes[\"day\"]\nshop_sales_detail[(\"month\", \"mode\")] = shop_sales_modes[\"month\"]\nshop_sales_detail.describe()","02d273c7":"train_c3 = train_c2.copy()\ntrain_c3.info()","be9ec5ee":"cols_to_drop_train = [\"date\", \"item_name\", \"item_category_name_en\", \"shop_name_en\", \"weekday\", \"day\"]\ntrain_c3.drop(cols_to_drop_train, axis=1, inplace=True)","ee113d54":"train_c3 = aggregate(train_c3, [\"date_block_num\", \"shop_id\", \"item_id\"], {\"item_cnt_day\": np.sum})\ntrain_c3.rename(columns={\"item_cnt_day\": \"item_cnt_month\"}, inplace=True)","4138059c":"shop_ids = test_s['shop_id'].unique()\nitem_ids = test_s['item_id'].unique()\n\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])","8b8b3410":"train_c4 = pd.merge(empty_df, train_c3, on=['date_block_num','shop_id','item_id'], how='left')\ntrain_c4.fillna(value=0, inplace=True)","e4e0f772":"train_c4 = pd.merge(train_c4, items[[\"item_id\", \"item_category_id\", \"master_category\"]], on=\"item_id\", how=\"left\")\ntrain_c4 = pd.merge(train_c4, shops[[\"shop_id\", \"city\"]], on=\"shop_id\", how=\"left\")\ntrain_c4[\"year\"] = train_c4[\"date_block_num\"] \/\/ 12 + 2013\ntrain_c4[\"month\"] = train_c4[\"date_block_num\"] % 12 + 1\ntrain_c4.loc[train_c4[\"shop_id\"] == 11, 'shop_id'] = 10\ntrain_c4.loc[train_c4[\"shop_id\"] == 0, 'shop_id'] = 57\ntrain_c4.loc[train_c4[\"shop_id\"] == 1, 'shop_id'] = 5","16452534":"train_c4.set_index([\"item_id\", \"shop_id\"], inplace=True)\n\ntrain_c4[\"item_shop_date_block_min\"] = item_shop_sales_detail[(\"date_block_num\", \"min\")]\ntrain_c4[\"item_shop_date_block_max\"] = item_shop_sales_detail[(\"date_block_num\", \"max\")]\ntrain_c4[\"item_shop_weekday_median\"] = item_shop_sales_detail[(\"weekday\", \"median\")] + 1\ntrain_c4[\"item_shop_month_median\"] = item_shop_sales_detail[(\"month\", \"median\")]\n\ntrain_c4 = train_c4.reset_index().set_index(\"item_id\")\ntrain_c4[\"item_date_block_min\"] = item_sales_detail[(\"date_block_num\", \"min\")]\ntrain_c4[\"item_date_block_max\"] = item_sales_detail[(\"date_block_num\", \"max\")]\ntrain_c4[\"item_weekday_median\"] = item_sales_detail[(\"weekday\", \"median\")] + 1\ntrain_c4[\"item_month_median\"] = item_sales_detail[(\"month\", \"median\")]\ntrain_c4[\"item_weekday_mode\"] = item_sales_detail[(\"weekday\", \"mode\")] + 1\ntrain_c4[\"item_month_mode\"] = item_sales_detail[(\"month\", \"mode\")]\n\ntrain_c4 = train_c4.reset_index().set_index(\"shop_id\")\ntrain_c4[\"shop_date_block_min\"] = shop_sales_detail[(\"date_block_num\", \"min\")]\ntrain_c4[\"shop_date_block_max\"] = shop_sales_detail[(\"date_block_num\", \"max\")]\ntrain_c4[\"shop_weekday_median\"] = shop_sales_detail[(\"weekday\", \"median\")] + 1\ntrain_c4[\"shop_month_median\"] = shop_sales_detail[(\"month\", \"median\")]\ntrain_c4[\"shop_weekday_mode\"] = shop_sales_detail[(\"weekday\", \"mode\")] + 1\ntrain_c4[\"shop_month_mode\"] = shop_sales_detail[(\"month\", \"mode\")]\n\ntrain_c4.reset_index(inplace=True)\ntrain_c4.fillna(value=0, inplace=True)","c2fdcb8e":"train_c5 = train_c4.set_index(['item_id', 'shop_id', 'date_block_num'])\ntrain_c5[\"item_price\"] = avg_price_item_shop_month[\"item_price\"]\ntrain_c5[\"item_price\"].isna().sum()","656d96ca":"train_c5 = train_c5.reset_index().set_index(['item_id', 'shop_id'])\ntrain_c5.loc[train_c5['item_price'].isna(), 'item_price'] = avg_price_item_shop[\"item_price\"]\ntrain_c5[\"item_price\"].isna().sum()","2b27910d":"train_c5 = train_c5.reset_index().set_index('item_id')\ntrain_c5.loc[train_c5['item_price'].isna(), 'item_price'] = avg_price_item[\"item_price\"]\ntrain_c5[\"item_price\"].isna().sum()","cfbb42f2":"train_c5[\"avg_item_price\"] = avg_price_item[\"item_price\"]","8d687df9":"train_c5 = train_c5.reset_index().set_index('item_category_id')\ntrain_c5.loc[train_c5['item_price'].isna(), 'item_price'] = avg_price_category[\"item_price\"]\ntrain_c5[\"item_price\"].isna().sum()","496e0c48":"train_c5[\"avg_item_price_perc\"] = (train_c5['item_price'] - train_c5[\"avg_item_price\"]) \/ train_c5[\"avg_item_price\"]\ntrain_c5.fillna(value=0, inplace=True)","a6dcc87c":"train_c5[\"item_cnt_month\"] = train_c5[\"item_cnt_month\"].clip(0., 20.)","c94a0ebb":"avg_q_month = train_c5.groupby(['date_block_num']).agg({\"item_cnt_month\": \"mean\"})\navg_q_month_item = train_c5.groupby(['date_block_num', 'item_id']).agg({\"item_cnt_month\": \"mean\"})\navg_q_month_shop = train_c5.groupby(['date_block_num', 'shop_id']).agg({\"item_cnt_month\": \"mean\"})\navg_q_month_category = train_c5.groupby(['date_block_num', 'item_category_id']).agg({\"item_cnt_month\": \"mean\"})\navg_q_month_city = train_c5.groupby(['date_block_num', 'city']).agg({\"item_cnt_month\": \"mean\"})\navg_q_month_master_category = train_c5.groupby(['date_block_num', 'master_category']).agg({\"item_cnt_month\": \"mean\"})","3b3a0b98":"train_c5 = train_c5.reset_index().set_index('date_block_num')\ntrain_c5[\"avg_month_sales\"] = avg_q_month[\"item_cnt_month\"]\n\ntrain_c5 = train_c5.reset_index().set_index(['date_block_num', 'item_id'])\ntrain_c5[\"avg_month_item_sales\"] = avg_q_month_item[\"item_cnt_month\"]\n\ntrain_c5 = train_c5.reset_index().set_index(['date_block_num', 'shop_id'])\ntrain_c5[\"avg_month_shop_sales\"] = avg_q_month_shop[\"item_cnt_month\"]\n\ntrain_c5 = train_c5.reset_index().set_index(['date_block_num', 'item_category_id'])\ntrain_c5[\"avg_month_category_sales\"] = avg_q_month_category[\"item_cnt_month\"]\n\ntrain_c5 = train_c5.reset_index().set_index(['date_block_num', 'city'])\ntrain_c5[\"avg_month_city_sales\"] = avg_q_month_city[\"item_cnt_month\"]\n\ntrain_c5 = train_c5.reset_index().set_index(['date_block_num', 'master_category'])\ntrain_c5[\"avg_month_master_category_sales\"] = avg_q_month_master_category[\"item_cnt_month\"]\n\ntrain_c5 = train_c5.reset_index()","9cc191a5":"train_c6 = train_c5.copy()","3e8a1b92":"def lag_feature(df, lags, col):\n    tmp = df[['date_block_num','shop_id','item_id',col]]\n    for i in lags:\n        shifted = tmp.copy()\n        shifted.columns = ['date_block_num','shop_id','item_id', col+'_lag_'+str(i)]\n        shifted['date_block_num'] += i\n        df = pd.merge(df, shifted, on=['date_block_num','shop_id','item_id'], how='left')\n    return df","55de5aad":"train_c6 = lag_feature(train_c6, [1, 2, 3], 'item_cnt_month')\ntrain_c6 = lag_feature(train_c6, [1, 2, 3], 'avg_month_item_sales')\ntrain_c6 = lag_feature(train_c6, [1, 2, 3], 'avg_month_shop_sales')\ntrain_c6 = lag_feature(train_c6, [1], 'avg_month_sales')\ntrain_c6 = lag_feature(train_c6, [1], 'avg_month_category_sales')\ntrain_c6 = lag_feature(train_c6, [1], 'avg_month_city_sales')\ntrain_c6 = lag_feature(train_c6, [1], 'avg_month_master_category_sales')","901b4f32":"train_c6 = lag_feature(train_c6, [1], 'item_price')\ntrain_c6 = lag_feature(train_c6, [1], 'avg_item_price_perc')","d1a6942c":"def itemTypes(block_min, block_max):\n    if block_min >= 27:\n        return 1\n    elif block_max < 27:\n        return 2\n    else:\n        return 3","97517aa5":"train_c6[\"item_type\"] = np.vectorize(itemTypes)(train_c6['item_date_block_min'], train_c6['item_date_block_max'])\ntrain_c6[\"item_type\"].value_counts()","78423aa3":"test_c1 = test_s.copy()","3ae8c0b5":"test_c1[\"month\"] = 11\ntest_c1[\"year\"] = 2015\ntest_c1[\"date_block_num\"] = 34\n\ntest_c1.set_index([\"item_id\", \"shop_id\"], inplace=True)\ntest_c1[\"item_shop_date_block_min\"] = item_shop_sales_detail[(\"date_block_num\", \"min\")]\ntest_c1[\"item_shop_date_block_max\"] = item_shop_sales_detail[(\"date_block_num\", \"max\")]\ntest_c1[\"item_shop_weekday_median\"] = item_shop_sales_detail[(\"weekday\", \"median\")] + 1\ntest_c1[\"item_shop_month_median\"] = item_shop_sales_detail[(\"month\", \"median\")]\n\ntest_c1 = test_c1.reset_index().set_index(\"item_id\")\ntest_c1[\"item_date_block_min\"] = item_sales_detail[(\"date_block_num\", \"min\")]\ntest_c1[\"item_date_block_max\"] = item_sales_detail[(\"date_block_num\", \"max\")]\ntest_c1[\"item_weekday_median\"] = item_sales_detail[(\"weekday\", \"median\")] + 1\ntest_c1[\"item_month_median\"] = item_sales_detail[(\"month\", \"median\")]\ntest_c1[\"item_weekday_mode\"] = item_sales_detail[(\"weekday\", \"mode\")] + 1\ntest_c1[\"item_month_mode\"] = item_sales_detail[(\"month\", \"mode\")]\n\ntest_c1 = test_c1.reset_index().set_index(\"shop_id\")\ntest_c1[\"shop_date_block_min\"] = shop_sales_detail[(\"date_block_num\", \"min\")]\ntest_c1[\"shop_date_block_max\"] = shop_sales_detail[(\"date_block_num\", \"max\")]\ntest_c1[\"shop_weekday_median\"] = shop_sales_detail[(\"weekday\", \"median\")] + 1\ntest_c1[\"shop_month_median\"] = shop_sales_detail[(\"month\", \"median\")]\ntest_c1[\"shop_weekday_mode\"] = shop_sales_detail[(\"weekday\", \"mode\")] + 1\ntest_c1[\"shop_month_mode\"] = shop_sales_detail[(\"month\", \"mode\")]\n\ntest_c1.reset_index(inplace=True)\ntest_c1.fillna(value=0, inplace=True)\n\ncols_to_drop_test = [\"item_name\", \"item_category_name_en\", \"shop_name_en\"]\ntest_c1.drop(cols_to_drop_test, axis=1, inplace=True)\n\ntest_c2 = test_c1.reset_index().set_index(\"index\")\ntest_c2.fillna(value=0, inplace=True)","26cedb76":"sales = train_c6.reset_index().set_index(['item_id', 'shop_id'])\nsales_lag_1 = sales[sales[\"date_block_num\"] == 33]\nsales_lag_2 = sales[sales[\"date_block_num\"] == 32]\nsales_lag_3 = sales[sales[\"date_block_num\"] == 31]\nsales_lag_12 = sales[sales[\"date_block_num\"] == 22]\ntest_c2 = test_c2.reset_index().set_index(['item_id', 'shop_id'])\n\ntest_c2[\"item_cnt_month_lag_1\"] = sales_lag_1[\"item_cnt_month\"]\ntest_c2[\"item_price_lag_1\"] = sales_lag_1[\"item_price\"]\ntest_c2[\"avg_item_price_perc_lag_1\"] = sales_lag_1[\"avg_item_price_perc\"]\ntest_c2[\"avg_month_sales_lag_1\"] = sales_lag_1[\"avg_month_sales\"]\ntest_c2[\"avg_month_item_sales_lag_1\"] = sales_lag_1[\"avg_month_item_sales\"]\ntest_c2[\"avg_month_shop_sales_lag_1\"] = sales_lag_1[\"avg_month_shop_sales\"]\ntest_c2[\"avg_month_category_sales_lag_1\"] = sales_lag_1[\"avg_month_category_sales\"]\ntest_c2[\"avg_month_city_sales_lag_1\"] = sales_lag_1[\"avg_month_city_sales\"]\ntest_c2[\"avg_month_master_category_sales_lag_1\"] = sales_lag_1[\"avg_month_master_category_sales\"]\n\ntest_c2[\"item_cnt_month_lag_2\"] = sales_lag_2[\"item_cnt_month\"]\ntest_c2[\"avg_month_item_sales_lag_2\"] = sales_lag_2[\"avg_month_item_sales\"]\ntest_c2[\"avg_month_shop_sales_lag_2\"] = sales_lag_2[\"avg_month_shop_sales\"]\n\ntest_c2[\"item_cnt_month_lag_3\"] = sales_lag_3[\"item_cnt_month\"]\ntest_c2[\"avg_month_item_sales_lag_3\"] = sales_lag_3[\"avg_month_item_sales\"]\ntest_c2[\"avg_month_shop_sales_lag_3\"] = sales_lag_3[\"avg_month_shop_sales\"]\n\ntest_c2[\"item_type\"] = np.vectorize(itemTypes)(test_c2['item_date_block_min'], test_c2['item_date_block_max'])\n\ntest_c2 = test_c2.reset_index().set_index(\"index\")","d24f36ea":"import calendar\n\ndef calculateWeekendDays(month, year):\n    weekend_days = 0\n    for week in calendar.monthcalendar(year, month):\n        for day in week[4:]:\n            if day != 0:\n                weekend_days +=1\n                \n    return weekend_days\n\ndef calculateMonthDays(month, year):\n    month_days = 0\n    for week in calendar.monthcalendar(year, month):\n        for day in week:\n            if day != 0:\n                month_days +=1\n                \n    return month_days","02d9a76e":"calendar_dict = {\"date_block_num\": [], \"weekend_days\": [], \"month_days\": []}\n\nfor year in range (2013, 2016):\n    for month in range(1, 13):\n        calendar_dict[\"date_block_num\"].append((year - 2013)*12 + month - 1)\n        calendar_dict[\"weekend_days\"].append(calculateWeekendDays(month, year))\n        calendar_dict[\"month_days\"].append(calculateMonthDays(month, year))\n\nweekend_days_df = pd.DataFrame(calendar_dict)\nweekend_days_df","2f3d2195":"train_c6 = pd.merge(train_c6, weekend_days_df, how=\"left\", on='date_block_num')\ntest_c2 = pd.merge(test_c2, weekend_days_df, how=\"left\", on='date_block_num')","458f7d1c":"train_c6 = train_c6[train_c6[\"date_block_num\"] > 2]","7f9d6e86":"train_c6.drop([\"item_price\", \"avg_item_price\", \"avg_item_price_perc\"], axis=1, inplace=True)\ntrain_c6.drop([\"avg_month_sales\", \"avg_month_item_sales\", \"avg_month_shop_sales\", \"avg_month_category_sales\", \"avg_month_city_sales\", \"avg_month_master_category_sales\"], axis = 1, inplace=True)\n\ntrain_f = train_c6.copy()\ntest_f = test_c2.copy()","81e4f522":"def create_dummies(df,features):\n    for col in features:\n        dummies = pd.get_dummies(df[col],prefix=col)\n        df = pd.concat([df,dummies],axis=1)\n        df = df.drop(col, axis=1)\n    return df\n\ndef categorize_column(df, features):\n    for col in features:\n        df[col] = df[col].astype('category')\n    return df","dabab7bc":"# train_f = create_dummies(train_f, [\"master_category\"])\n# train_f = create_dummies(train_f, [\"city\"])\n\n# test_f = create_dummies(test_f, [\"master_category\"])\n# test_f = create_dummies(test_f, [\"city\"])\n\ntrain_f = categorize_column(train_f, [\"master_category\"])\ntrain_f = categorize_column(train_f, [\"city\"])\n\ntest_f = categorize_column(test_f, [\"master_category\"])\ntest_f = categorize_column(test_f, [\"city\"])\n\ntrain_f[\"master_category\"] = train_f[\"master_category\"].cat.codes\ntrain_f[\"city\"] = train_f[\"city\"].cat.codes\n\ntest_f[\"master_category\"] = test_f[\"master_category\"].cat.codes\ntest_f[\"city\"] = test_f[\"city\"].cat.codes","916ca5a5":"train_f = downgrade_dtypes(train_f)\ntrain_f.info()\ntest_f = downgrade_dtypes(test_f)\ntest_f.info()","72fc5017":"def show_corr_heatmap(df, method):\n    corr = df.corr(method)\n    \n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(11, 9))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    \n    return corr","f6464e9d":"correlations = show_corr_heatmap(train_f, \"pearson\")","3487d17a":"top_correlation_features = list(correlations[\"item_cnt_month\"].sort_values(ascending=False)[:30].index)\n\ntop = show_corr_heatmap(train_f[top_correlation_features], \"pearson\")","e4c7ba67":"from sklearn.feature_selection import SelectKBest, chi2, f_regression, mutual_info_regression\nfrom sklearn.feature_selection import f_classif, mutual_info_classif\n\ndef get_best_features(df, features, target, function, num_of_features=-1):\n    \n    # Select all features if number is not passed\n    if num_of_features == -1:\n        num_of_features = len(features)\n    \n    # Create the model and fit it with data\n    kBest=SelectKBest(score_func=function,k=num_of_features)\n    kBest.fit(df[features],df[target])\n    \n    # Get columns to keep and create new dataframe with those only\n    cols = kBest.get_support(indices=True)\n    features_df_new = df[features].iloc[:,cols]\n        \n    # Create a dataframe of feature names and scores\n    names = df[features].columns.values[kBest.get_support()]\n    scores = kBest.scores_\n    names_scores = list(zip(names, scores))\n    feature_scores_df = pd.DataFrame(data = names_scores, columns=['feature', 'score'])\n    \n    #Sort the dataframe for better visualization\n    feature_scores_df_sorted = feature_scores_df.sort_values(['score', 'feature'], ascending = [False, True])\n\n    return feature_scores_df_sorted","f97663b2":"from sklearn.feature_selection import chi2, f_regression, mutual_info_regression, f_classif, mutual_info_classif\n\ntarget_feature = \"item_cnt_month\"\nbest_train_features = list(train_f.columns)\nbest_train_features.remove(target_feature)\n\nmethods = [f_regression]\nfor method in methods:\n    best_features_kBest = get_best_features(train_f, best_train_features, target_feature, method)\n    print(best_features_kBest)","1c2452b7":"from sklearn.feature_selection import RFECV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom xgboost.sklearn import XGBRegressor\n\nimport numpy as np\n\ndef select_features_RFECV(df, target):\n    df.dropna(axis=0, inplace=True)\n    \n    df = df.select_dtypes([np.number])\n    \n    all_X = df.drop([target], axis=1)\n    all_y = df[target]\n    \n    clf = LinearRegression()\n    selector = RFECV(clf, cv=5, min_features_to_select=25, scoring='neg_root_mean_squared_error')\n    selector.fit(all_X, all_y)\n\n    optimized_columns = all_X.columns[selector.support_]\n\n    return optimized_columns","7b597308":"best_features_RFECV = select_features_RFECV(train_f, \"item_cnt_month\")\nprint(best_features_RFECV)","db9692e0":"import contextlib\nimport time\n\n@contextlib.contextmanager\ndef timer():\n    start = time.time()\n    \n    yield\n\n    end = time.time()\n    runtime = '{:.2f}s \\n'.format(end - start)\n    print(runtime)","6cbb980b":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import SGDRegressor\nfrom xgboost.sklearn import XGBRegressor\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\ndef select_regression_model(df, features, target, typ):\n    \n    all_X = df[features]\n    all_y = df[target]\n    \n    models = [\n        {\n            \"name\": \"LinearRegression\",\n            \"estimator\": LinearRegression(),\n            \"hyperparameters\":\n                {\n                    \"normalize\": [True, False]\n                }\n         }\n    ]\n    \n    for model in models:\n        with timer():\n            if typ == \"grid\":\n                search = GridSearchCV(model[\"estimator\"], param_grid=model[\"hyperparameters\"], cv=5, scoring='neg_root_mean_squared_error')\n            elif typ == \"random\":\n                search = RandomizedSearchCV(model[\"estimator\"], param_distributions=model[\"hyperparameters\"], n_iter = 1, cv=5, scoring='neg_root_mean_squared_error')\n\n            search.fit(all_X, all_y)\n            model[\"best_params\"] = search.best_params_\n            model[\"best_score\"] = search.best_score_\n            model[\"best_model\"] = search.best_estimator_\n\n    return models","8a263854":"train_f_tr = train_f[(train_f[\"date_block_num\"] < 33)]\ntrain_f_t = train_f[train_f[\"date_block_num\"] == 33]","b2a92680":"correlations[\"item_cnt_month\"].sort_values(ascending=False)","2437e88b":"list(best_features_kBest[\"feature\"][:30].to_numpy())","8cd80582":"list(best_features_RFECV.to_numpy())","0d1a3ca2":"target_feature = \"item_cnt_month\"\nall_train_features = list(train_f.columns)\nall_train_features.remove(target_feature)\nprint(\"Available features: {}\".format(all_train_features))","984ed351":"selected_features = ['date_block_num',\n 'year',\n 'month',\n 'item_shop_date_block_min',\n 'item_shop_date_block_max',\n 'item_shop_weekday_median',\n 'item_weekday_median',\n 'item_month_mode',\n 'shop_weekday_median',\n 'shop_month_median',\n 'item_cnt_month_lag_1',\n 'item_cnt_month_lag_2',\n 'item_cnt_month_lag_3',\n 'avg_month_item_sales_lag_1',\n 'avg_month_item_sales_lag_2',\n 'avg_month_item_sales_lag_3',\n 'avg_month_shop_sales_lag_1',\n 'avg_month_shop_sales_lag_2',\n 'avg_month_shop_sales_lag_3',\n 'avg_month_sales_lag_1',\n 'avg_month_category_sales_lag_1',\n 'avg_month_city_sales_lag_1',\n 'avg_item_price_perc_lag_1',\n 'item_type',\n 'month_days']","e620fa9a":"from sklearn.metrics import mean_squared_error\n\ndef evaluate(result):\n    best_rf_model = result[0][\"best_model\"]\n\n    predictions_tr = best_rf_model.predict(train_f_tr[selected_features])\n    predictions_t = best_rf_model.predict(train_f_t[selected_features])\n\n    rmse_tr = (mean_squared_error(train_f_tr[\"item_cnt_month\"].to_numpy(), predictions_tr.clip(0., 20.))) ** (1\/2)\n    rmse_t = (mean_squared_error(train_f_t[\"item_cnt_month\"].to_numpy(), predictions_t.clip(0., 20.))) ** (1\/2)\n    \n    data.append({\"best_model\": best_rf_model, \"best_score\": result[0][\"best_score\"], \"features\": selected_features,\n                 \"rmse_train\": rmse_tr, \"rmse_test\": rmse_t})","0d15768e":"data = []\n\nresult = select_regression_model(train_f_tr, selected_features, target_feature, \"random\")\nevaluate(result)\n\nprint(data)","5bea625d":"best_rf_model = result[0][\"best_model\"]\nbest_rf_model.fit(train_f[selected_features], train_f[target_feature])","08e99037":"predictions = best_rf_model.predict(test_f[selected_features])","5b30414f":"test_f[\"predictions\"] = predictions\ntest_f[\"predictions\"].describe()","b274f6ac":"def save_submission_file(data, filename=\"submission_13.csv\"):\n    test_ids = data.index\n    predictions = data[\"predictions\"].clip(0., 20.)\n    \n    submission_df = {\"ID\": test_ids,\n                 \"item_cnt_month\": predictions}\n    \n    submission = pd.DataFrame(submission_df)\n    submission.to_csv(filename,index=False)","6689a2e6":"save_submission_file(test_f)","c0199635":"Predictions fast check.","211817cd":"<a id=\"section-5\"><\/a>\n# 5. MODELING #","5192000f":"<a id=\"subsection-3-8\"><\/a>\n### Items features ###\n\nWe will create the item type feature:\n- Old item - no sales in last 6 months\n- New item - first sales in last 6 months\n- Regular items - the rest","efbc17c3":"<a id=\"subsection-5-1\"><\/a>\n### Split the train data ###\n\nSplit our train data into train_train and train_test. We will use the last month to evaluate our model.","beaab216":"We also have some potential duplicates, which we will explore later:\n- Zhukovsky st. Chkalov 39m\u00b2 (id 10 and 11)\n- Moscow TC \"Budenovskiy\" (id 23 and 24)\n- Yakutsk Ordzhonikidze (id 0 and 57)\n- Yakutsk TC \"Central\" (id 1 and 58)","7cb2a89a":"More grouping","5ac3ff47":"Let's add the number of weekend days (friday included) for every month in our data. Also we calculate the number of days in month.","981066f2":"<a id=\"section-4\"><\/a>\n# 4. FEATURE SELECTION #","e72eabdd":"<a id=\"subsection-2-2\"><\/a>\n## Managing date columns ##","2f3ae90b":"<a id=\"subsection-1-3\"><\/a>\n## Data merge ##\n\nMerge item categories, items and shops to train\/test data.","4506b6e2":"Table of contents:\n\n1. [READ, TRANSLATE AND MERGE TABLES](#section-1)\n    - [Shops](#subsection-1-1)\n    - [Categories](#subsection-1-2)\n    - [Data merge](#subsection-1-3)\n2. [DATA UNDERSTANDING AND CLEANING](#section-2)\n    - [Data overview](#subsection-2-1)\n    - [Managing date columns](#subsection-2-2)\n    - [Data Exploration and Visualisation](#subsection-2-3)\n        - [Sales quantity](#subsection-2-3-1)\n        - [Item price](#subsection-2-3-2)\n        - [Cronological sales](#subsection-2-3-3)\n        - [Monthly sales - 2013 & 2014](#subsection-2-3-4)\n        - [Weekday sales](#subsection-2-3-5)\n        - [Shop sales](#subsection-2-3-6)\n        - [Category sales](#subsection-2-3-7)\n3. [FEATURE ENGINEERING](#section-3)\n    - [Average prices calculation](#subsection-3-1)\n    - [First \/ last sale, medians and modes](#subsection-3-2)\n    - [Aggregating train data](#subsection-3-3)\n    - [Stacking train data](#subsection-3-4)\n    - [Price features](#subsection-3-5)\n    - [Mean quantity features](#subsection-3-6)\n    - [Lag features](#subsection-3-7)\n    - [Items features](#subsection-3-8)\n    - [Test data enginnering](#subsection-3-9)\n    - [Calendar related features](#subsection-3-10)\n    - [Final steps](#subsection-3-11)\n4. [FEATURE SELECTION](#section-4)\n    - [Feature correlation](#subsection-4-1)\n    - [Best feature selection with SelectKBest](#subsection-4-2)\n    - [Best feature selection with RFECV](#subsection-4-3)\n5. [MODELING](#section-5)\n    - [Spliting the train data](#subsection-5-1)\n    - [Selecting features to train](#subsection-5-2)\n    - [Training and evaluating](#subsection-5-3)\n    - [Predictions](#subsection-5-4)\n6. [CREATING SUBMISSION FILE](#section-6)","cb859bc2":"The most sales occur on weekend (52%), which we'll consider later.","7defaaf1":"<a id=\"subsection-5-3\"><\/a>\n### Training and evaluating ###","a724f9f3":"Downgrade numeric types for faster calculations.","43cfa949":"<a id=\"section-3\"><\/a>\n# 3. FEATURE ENGINEERING #\n\nWe are predicting the monthly sales data for november 2015, however our train data consists of daily sales. Therefore we will have to aggregate the data by item\/shop\/month, but before that we should create some useful features with non-aggregated data.","21fa4de7":"Remove the top outlier.","dff7f714":"Let's also add average item price in a separate column for potential features such as discounts.","c39d377b":"<a id=\"section-2\"><\/a>\n# 2. DATA UNDERSTANDING AND CLEANING#","23844d6f":"Now lets calculate a new feature - shop item price percentage of average price in all shops.","6bd7ebd9":"Category names begin with the \"master category\". Let's isolate the text before \"-\" and insert it in the separate column named \"master_category\".","a44377b0":"<a id=\"subsection-2-3\"><\/a>\n## Data Exploration and Visualisation ##","3cca86f4":"<a id=\"subsection-3-9\"><\/a>\n### Test data enginnering ###\n\nLet's fill the missing data in our test set.","f175de6a":"Best correlation features","fd461cf1":"<a id=\"subsection-4-1\"><\/a>\n### Feature correlation ###\n\nMethod types:\n- pearson => numerical input - numerical output\n- spearman => numerical input - numerical output\n- kendall => categorical input - numerical output, numerical input - categorical output","4fb0c64b":"Target and train features selection","448d42b5":"We are only comparing the years 2013 and 2014 since we don't have full data for 2015. We can see a massive seasonality effect in winter months, especially december.","e8fa9fab":"Conclusions:\n- No missing values\n- Unique dates is equal to all days in the timeframe from 01.01.2013 to 31.10.2015, which means that we have sales every day\n- We have 21807 different items\n- We have 84 different categories\n- We have 60 different shops\n- We have some outliers in item_price and item_cnt_day columns","88e9cfed":"Append all available data. Clean shops data.","f63afd71":"<a id=\"section-6\"><\/a>\n# 6. CREATING SUBMISSION FILE #","d50c6a47":"We can see that top 5 master categories form the vast majority of sales.","6274ea8f":"We are seeing a decline in sales over the range we are training on, there is also a seasonal effect.","d422a0ae":"Looks like the following shops are duplicated:\n- Zhukovsky st. Chkalov 39m\u00b2 (id 10 and 11)\n- Yakutsk Ordzhonikidze (id 0 and 57)\n- Yakutsk TC \"Central\" (id 1 and 58)\n\nLet's make the folowing shop id replacements:\n- id 11 => id 10\n- id 0 => id 57\n- id 1 => 58","d72a8cbe":"<a id=\"subsection-2-3-2\"><\/a>\n### Item price ###","77f6fcc5":"<a id=\"subsection-3-7\"><\/a>\n### Lag features ###\n\nWe still don't have comparisons of sales against previous months. We should add some, since previous sales are one of the most important features in sales analytics.\n\nWe will add features using a great function from Denis Larionov => https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost:","21a280bd":"Let's also create price lag features, so that we can add them to the test set.","8c6d61f2":"<a id=\"subsection-3-10\"><\/a>\n### Calendar related features ###","a8927a6c":"<a id=\"subsection-4-2\"><\/a>\n### Best feature selection with SelectKBest ###\n\nSearching for best features using SelectKBest.\n\nRegression methods: f_regression, mutual_info_regression\n\nClassification methods: chi2, f_classif, mutual_info_classif","b9ad8dd6":"<a id=\"subsection-3-4\"><\/a>\n### Stacking the train data ###\n\nFirst step:\n- Remove the data not in the test set from train\n- Fill the data with zero sales for all item\/shop\/date_block combo","913a3293":"<a id=\"subsection-2-3-5\"><\/a>\n### Weekday sales ###","3aa3c8f5":"Hyperparameters optimization with the function below using GridSearchCV or RandomizedSearchCV.","0f1b5d67":"We can see that the vast majority (99.59%) of quantities is in top 10 most represented values (-1 to 9). We also see some outliers - especially at shop 12.\n\nLets take a look at the items with the sales higher or equal to 1000 per day.","17c9dfa5":"<a id=\"subsection-4-3\"><\/a>\n\n### Best feature selection with RFECV ###\n\nSearching for best features using RFECV.\n\nWarning: it is a very time consuming process - in my case it took 6 minutes.","6a4c64b5":"<a id=\"subsection-1-1\"><\/a>\n## Shops ##","a1b3a2da":"<a id=\"subsection-2-3-7\"><\/a>\n### Categories sales ###","97457f41":"Best features using RFECV.","8a15952e":"We should first drop some columns that we won't need for modeling.","d4bbe7e3":"Downgrade numeric data types to save memory.\n\nThank you Konstantin Yakovlev (kyakovlev) for this trick!\nhttps:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data","7ae196eb":"Translate shop and category names to English.","e597d035":"We can see that top 20 prices are in the range from 99 to 2599. Setting the price with the 99 ending seems very popular. We can also see 1 outlier with the price far above the others. There is also 1 price below zero. Let's eliminate them.","b54242cf":"<a id=\"subsection-3-5\"><\/a>\n### Price features ###","5a4e110e":"Let's isolate the names which are not cities (spd == St Petersburg) replace them with \"other\":\n- digital\n- offsite\n- emergency","4b5fd39f":"The \"date\" column should first be converted to type date and the following columns should be added, for easier understanding of data:\n- day\n- month\n- year\n- weekday","432fcf2d":"<a id=\"subsection-3-3\"><\/a>\n### Aggregating the data ###","f22c7f66":"New features:\n- mean price for item\/shop\/date_block combo\n- mean price for item\/shop combo\n- mean price for item\n- mean price item_category","e46cc4d5":"<a id=\"subsection-5-4\"><\/a>\n### Predictions ###\n\nWe first select the model which performed optimal and make predictions on actual test set.","70112370":"<a id=\"subsection-2-3-4\"><\/a>\n### Monthly sales - 2013 & 2014 ###","39b0e660":"<a id=\"subsection-3-11\"><\/a>\n### Final steps ###\n\nWe also need to eliminate first three months from training data since it has a lot of missing data in lagged features.","7aa552b2":"We can see that most of the shop names begin with the city name. Let's isolate the first string from names and insert it in the separate column named \"city\"","b1759b0c":"<a id=\"subsection-2-1\"><\/a>\n## Data overview ##","c350f429":"<a id=\"subsection-3-1\"><\/a>\n### Average prices calculation ###","b750b716":"First we create a context manager to manage calculation times.","06f61a4e":"Add first and last sales, weekday and month medians.","94a9cfd7":"<a id=\"subsection-5-2\"><\/a>\n### Selecting features to train ###","dce4c6e1":"Best features using SelectKBest.","8e2521c0":"<a id=\"subsection-3-2\"><\/a>\n### First \/ last sale, medians and modes ###\n\nNew features:\n- first and last sale of item\/shop combo\n- first and last sale of item\n- first and last sale of shop\n\n- weekday, day and month median of item\/shop combo sales count\n- weekday, day and month median of item sales count\n- weekday, day and month median of shop sales count\n\n- weekday, day and month mode of item sales count\n- weekday, day and month mode of shop sales count","459430f8":"<a id=\"subsection-2-3-3\"><\/a>\n### Cronological sales ###","cc5a96fa":"<a id=\"section-1\"><\/a>\n# 1. READ, TRANSLATE AND MERGE TABLES #","7722ae17":"Model evaluation function","25b9a411":"<a id=\"subsection-3-6\"><\/a>\n### Mean quantity features ###\n\nMean quantity features in relation to date_block_num.\n\nIt is very important to filter train data so that it is similar to test data:\n- Clip the sales between 0 and 20","37122ce3":"Aggregation, rename column from item_cnt_day to item_cnt_month.","f78e0bad":"Creating a submission file","a2f0ad8a":"Convert object to numeric columns.","0d7e9032":"<a id=\"subsection-1-2\"><\/a>\n## Categories ##","54fbc802":"<a id=\"subsection-2-3-6\"><\/a>\n\n### Shops sales ###\n\nFirst let's explore some potential duplicates:\n- Zhukovsky st. Chkalov 39m\u00b2 (id 10 and 11)\n- Moscow TC \"Budenovskiy\" (id 23 and 24)\n- Yakutsk Ordzhonikidze (id 0 and 57)\n- Yakutsk TC \"Central\" (id 1 and 58)\n\nWe will plot shop sales pairs on the barplot.","f823ea38":"<a id=\"subsection-2-3-1\"><\/a>\n### Sales quantity ###"}}