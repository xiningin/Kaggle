{"cell_type":{"51d36631":"code","e0dafb6d":"code","a838e6b7":"code","fe06ec98":"code","27e6dec1":"code","3da0338b":"code","112e082d":"code","4aa53514":"code","906f78d6":"code","bdfe05a7":"code","5bbe628b":"code","34a217c3":"code","54423f1d":"code","346c677e":"markdown","218d9687":"markdown","20c8b26c":"markdown","07fdb1a0":"markdown","bcfe0057":"markdown","00e9d279":"markdown","29b62612":"markdown","e5e97ed7":"markdown","0d7174d2":"markdown","a29a6f0c":"markdown"},"source":{"51d36631":"input_path= '\/input\/stanford-dogs-dataset\/images\/Images'","e0dafb6d":"import os\nimport cv2\nimport numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(\"Loaded all libraries\")","a838e6b7":"pip install tf-nightly","fe06ec98":"image_size = (200, 200)\nbatch_size = 32\n\ntrain_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \"\/content\/images\/Images\",\n    validation_split=0.2,\n    subset=\"training\",\n    label_mode = 'int',\n    seed = 1337,\n    image_size=image_size,\n    batch_size=batch_size,\n)\nval_ds = tf.keras.preprocessing.image_dataset_from_directory(\n    \"\/content\/images\/Images\",\n    validation_split=0.2,\n    subset=\"validation\",\n    label_mode = 'int',\n    seed =1337,\n    image_size=image_size,\n    batch_size=batch_size,\n)","27e6dec1":"\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(int(labels[i]))\n        plt.axis(\"off\")","3da0338b":"data_augmentation_train = keras.Sequential(\n    [\n        layers.experimental.preprocessing.RandomFlip(\"horizontal\"),\n        layers.experimental.preprocessing.RandomRotation(0.1),\n        layers.experimental.preprocessing.Rescaling(scale =1.\/255),\n        layers.experimental.preprocessing.RandomHeight(0.1),\n        layers.experimental.preprocessing.RandomWidth(0.1)\n     \n    ]\n)","112e082d":"data_augmentation_test = keras.Sequential(\n    [\n        layers.experimental.preprocessing.Rescaling(scale =1.\/255)\n     \n    ]\n)","4aa53514":"augmented_train_ds = train_ds.map(\n  lambda x, y: (data_augmentation_train(x, training=True), y))\n\naugmented_val_ds = val_ds.map(\n  lambda x, y: (data_augmentation_test(x, training=True), y))","906f78d6":"plt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n    for i in range(9):\n        augmented_images = data_augmentation_train(images)\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n        plt.axis(\"off\")","bdfe05a7":"#Inception V3 \n\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\npre_trained_model = InceptionV3(input_shape=(200,200,3),\n                                               include_top=False,\n                                               weights='imagenet')\n\nfor layer in pre_trained_model.layers:\n  layer.trainable = False\n\npre_trained_model.summary()\n\n","5bbe628b":"import tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras.optimizers import Adam\nfrom keras import regularizers\n\nglobal_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nDl_1 = tf.keras.layers.Dropout(rate = 0.2)\n#pre_prediction_layer = tf.keras.layers.Dense(240, activation='tanh')\n#Dl_2 = tf.keras.layers.Dropout(rate = 0.2)\nprediction_layer = tf.keras.layers.Dense(120,activation='softmax')\n\n#Add dropout Layer\nmodel_V3 = tf.keras.Sequential([\n  pre_trained_model,\n  global_average_layer,\n  Dl_1,\n  #pre_prediction_layer,\n  #Dl_2,\n  prediction_layer\n])\n\nmodel_V3.compile(optimizer='adam',\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n\nmodel_V3.summary()\n\n# Callbacks\n\nlr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=2, mode='max')\nearly_stop = EarlyStopping(monitor='val_loss', min_delta=0.1, patience=1, mode='min')","34a217c3":"hist = model_V3.fit(\n           augmented_train_ds.repeat(), steps_per_epoch=int(8000\/batch_size), \n           epochs=30, validation_data=augmented_val_ds.repeat(), \n           validation_steps=int(2000\/batch_size) , callbacks=[lr_reduce])","54423f1d":"fig, ax = plt.subplots(1, 2, figsize=(10, 3))\nax = ax.ravel()\n\nfor i, met in enumerate(['accuracy', 'loss']):\n    ax[i].plot(hist.history[met])\n    ax[i].plot(hist.history['val_' + met])\n    ax[i].set_title('Model {}'.format(met))\n    ax[i].set_xlabel('epochs')\n    ax[i].set_ylabel(met)\n    ax[i].legend(['train', 'val'])","346c677e":"# **4. TRAINING THE MODEL**","218d9687":"# **3.2 FULLY CONNECTED LAYER**","20c8b26c":"# **1. IMPORTING THE LIBRARIES AND DATA **","07fdb1a0":"**Visualizing the Images **","bcfe0057":"# **ABOUT THE DATASET **\n\nThe Stanford Dogs dataset contains images of 120 breeds of dogs from around the world. This dataset has been built using images and annotation from ImageNet for the task of fine-grained image categorization. It was originally collected for fine-grain image categorization, a challenging problem as certain dog breeds have near identical features or differ in colour and age.\n","00e9d279":"# **5. LOSS AND ACCURACY VISUALIZATION **","29b62612":"Hey Reader , \nI will be performing following steps :\n1. Importing the data \n2. Data Argumentation And Visualization \n3. Importing the INCEPTION V3 ( Transfer learning ) \n4. Fully Connected layer \n5. Model Training \n6. Accuracy And Loss Visualization \n\n\nThe following cells are without the output , as kaggle dosen't support the \"tf nightly\" which is used to import the dataset by the keras generator . \nI am adding link to my Google Colab notebook , in case you want to see the outputs .\n\nhttps:\/\/colab.research.google.com\/drive\/1JxlXsSJqnYTx8ZxEyc_VYGnI4wrHikGT?usp=sharing\n\nLeave your kind comments for enclusive learning !! ","e5e97ed7":"# **3.1 IMPORTING THE INCEPTION V3**","0d7174d2":"# **2. DATA ARGUMENTATION AND VISUALIZATION **","a29a6f0c":"# **3. MODEL PREPARATION **"}}