{"cell_type":{"dc5ec9f0":"code","3c299057":"code","a7fe0875":"code","3433d2b3":"code","e118218e":"code","e3fdb115":"code","d3a3b23d":"code","b97f8e14":"code","07c2bb80":"code","619f9202":"markdown","ea3ae864":"markdown","bd20b4b0":"markdown","b964cdbd":"markdown","a8142ef0":"markdown","7fe61d1f":"markdown","0c16707d":"markdown","3a18220a":"markdown","57f5c215":"markdown","9fb0d1ef":"markdown","8be07759":"markdown","01ae1ce8":"markdown","dae53034":"markdown","b3d2204a":"markdown","45ce235f":"markdown","6ac3e729":"markdown","9f625551":"markdown","849ad7c5":"markdown","b06bea1a":"markdown","f424f7eb":"markdown","db85d5be":"markdown","f21c38d6":"markdown","f0f3d19a":"markdown","9bf18843":"markdown","1b3e5522":"markdown"},"source":{"dc5ec9f0":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier # sklearn lib for decision tree","3c299057":"from sklearn.datasets import load_breast_cancer\ncancer=load_breast_cancer()","a7fe0875":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(cancer.data, cancer.target, stratify=cancer.target, random_state=42)","3433d2b3":"clf=DecisionTreeClassifier() \nclf.fit(x_train,y_train)","e118218e":"print(\"Accuracy on training set: {:.3f}\".format(clf.score(x_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(clf.score(x_test, y_test)))","e3fdb115":"clf2 = DecisionTreeClassifier(max_depth=4, random_state=0)  # Notice max_depth parameter is set to 4 \nclf2.fit(x_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(clf2.score(x_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(clf2.score(x_test, y_test)))","d3a3b23d":"from sklearn import tree","b97f8e14":"plt.figure(figsize=(20,10))\ntree.plot_tree(clf2,class_names=['Malignant','Benign'])","07c2bb80":"def plot_feature_importances_cancer(model):\n    plt.figure(figsize=(10,8))\n    n_features = cancer.data.shape[1]\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), cancer.feature_names)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\nplot_feature_importances_cancer(tree)","619f9202":"**As expected, the accuracy on the training set is 100%\u2014because the leaves are pure,\nthe tree was grown deep enough that it could perfectly memorize all the labels on the\ntraining data. The test set accuracy is slightly worse than for the linear models we\nlooked at previously, which had around 95% accuracy.**\n\n**If we don\u2019t restrict the depth of a decision tree, the tree can become arbitrarily deep\nand complex. Unpruned trees are therefore prone to overfitting and not generalizing\nwell to new data.**\n\n\n**Now let\u2019s apply pre-pruning to the tree, which will stop developing\nthe tree before we perfectly fit to the training data. One option is to stop building the\ntree after a certain depth has been reached. Here we set max_depth=4, meaning only\nfour consecutive questions can be asked (cf. Figures 2-24 and 2-26). Limiting the\ndepth of the tree decreases overfitting. This leads to a lower accuracy on the training\n**","ea3ae864":"Building and Training the model","bd20b4b0":"\nThe breast cancer dataset is a classic and very easy binary classification\n    dataset.\n    The copy of UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is\n    downloaded from:\n    https:\/\/goo.gl\/U2Uwz2","b964cdbd":"![image.png](attachment:image.png)","a8142ef0":"**Spliting the cancer data into train and test datasets**","7fe61d1f":"Instead of looking at the whole tree, which can be time-consuming, there are some useful prop\u2010\nerties that we can derive to summarize the workings of the tree. The most commonly\nused summary is **feature importance**","0c16707d":"# Please Upvote if you found it useful","3a18220a":"![image.png](attachment:image.png)","57f5c215":" **Load data**","9fb0d1ef":"**Limiting the\ndepth of the tree decreases overfitting. This leads to a lower accuracy on the training\nset, but an improvement on the test set:**","8be07759":"# controling complexity of decision tree","01ae1ce8":"# Visualizing the decision tree\nWe can visualize the tree using the **export_graphviz** function from the tree module.\nThis writes a file in the .dot file format, which is a text file format for storing graphs.\n","dae53034":"**The recursive partitioning of the data is repeated until each region in the partition\n(each leaf in the decision tree) only contains a single target value (a single class or a\nsingle regression value).**\n\n**A leaf of the tree that contains data points that all share the\nsame target value is called pure.**","b3d2204a":"**There are two common strategies to prevent overfitting:** \n\n* stopping the creation of the tree early (also called pre-pruning), \n* building the tree but then removing or collapsing nodes that contain little information (also called post-pruning or just pruning).\n","45ce235f":"**Importing relavant libraries**","6ac3e729":"# How decision tree works?","9f625551":"# Basics of Decision Tree","849ad7c5":"# Coding the algorithm\n","b06bea1a":"**To build a tree, the algorithm searches over all possible tests and finds the one that is\nmost informative about the target variable. Figure above shows the first test that is\npicked. Splitting the dataset vertically at x[1]=0.0596 yields the most information.**\n\n**The split is done by testing whether x[1] <= 0.0596,\nindicated by a black line. If the test is true, a point is assigned to the left node, which\ncontains 2 points belonging to class 0 and 32 points belonging to class 1. Otherwise\nthe point is assigned to the right node, which contains 48 points belonging to class 0\nand 18 points belonging to class 1. **\n","f424f7eb":"**Typically, building a tree as described here and continuing until all leaves are pure\nleads to models that are very complex and highly overfit to the training data.**\n\n**The presence of pure leaves mean that a tree is 100% accurate on the training set; each\ndata point in the training set is in a leaf that has the correct majority class.** \n\n**The over\u2010fitting can be seen on the left of Figure above. You can see the regions determined to\nbelong to class 1 in the middle of all the points belonging to class 0. On the other\nhand, there is a small strip predicted as class 0 around the point belonging to class 0\nto the very right.** \n\n**This is not how one would imagine the decision boundary to look,\nand the decision boundary focuses a lot on single outlier points that are far away\nfrom the other points in that class.**","db85d5be":"# Interpretation of the Decision Tree","f21c38d6":"Lets check the score(model fit accuracy\/ how well our model works)","f0f3d19a":"# Decision Tree Tutorial","9bf18843":"![image.png](attachment:image.png)","1b3e5522":"* Decision trees are widely used models for classification and regression tasks.* **They learn a hierarchy of if\/else questions***, leading to a decision.\n* we built a model to distinguish between four classes of animals (hawks, penguins, dolphins, and bears) using the three features \u201chas feath\u2010ers,\u201d \u201ccan fly,\u201d and \u201chas fins.\u201d Instead of building these models by hand, we can learnthem from data using supervised learning.\n* A sample decision tree would look like\n![image.png](attachment:image.png)\n1. each node in the tree either represents a question or a terminal node (also called a** leaf)** that contains the answer.\n2.  The top node, also called the **root**"}}