{"cell_type":{"99604259":"code","102b3aab":"code","2a127d37":"code","8d1e7875":"code","59c8106e":"code","cab266c3":"code","cdf5f3cb":"code","d7b277d8":"code","3acfb34a":"code","7e46e0e2":"code","d764167f":"code","79e7c750":"code","4af4c9d2":"code","c1a96648":"code","f4c246a8":"code","da6797a1":"code","8acbc269":"code","32576047":"code","3a40ccdb":"code","8daeefd6":"code","8c192cf2":"code","36aaa059":"code","f66347df":"code","376fc506":"code","a2df0bb4":"code","5170f0fc":"code","d19f75dc":"code","1f83530b":"code","c6524450":"code","2669dc42":"code","bbada1d8":"code","286919b5":"code","16441d54":"code","93553ab5":"code","64cec4d3":"code","ea5eca57":"code","fa2107f6":"code","c7cb43d6":"code","a0fcd089":"code","35b98fd4":"markdown","f64448ca":"markdown","387499fc":"markdown","0a4aa9a1":"markdown","752308d8":"markdown","29fed1fe":"markdown","fef9b8a1":"markdown","c76e5756":"markdown","76ddf2b3":"markdown","01dd1146":"markdown","bef2383b":"markdown","ed1f9532":"markdown","7bb1ac1b":"markdown","8d6204f8":"markdown","e8bd886a":"markdown","20e078ee":"markdown","8173c288":"markdown","a022699d":"markdown","1c5cf63d":"markdown","9d9b1cf4":"markdown","2df357e2":"markdown","b7d36668":"markdown","a4aadabc":"markdown","0c43261f":"markdown","182c2ec3":"markdown","942bfae8":"markdown","9d2b534d":"markdown","082b72eb":"markdown","8577a873":"markdown","0e0a9ed1":"markdown","6d60bb06":"markdown","08ec5539":"markdown","11e559c9":"markdown","7607eaab":"markdown","6275e84f":"markdown","3812e135":"markdown","232a1a45":"markdown","22411e54":"markdown","80612825":"markdown","c647e718":"markdown","f3027b0f":"markdown","ceabb806":"markdown","2f27beec":"markdown","6087352d":"markdown","7149af64":"markdown","75cb0e30":"markdown","ee49673b":"markdown","874f5e7e":"markdown","e5a69ca4":"markdown"},"source":{"99604259":"import pandas as pd\nimport numpy as np","102b3aab":"import seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","2a127d37":"sns.set_theme(rc = {'grid.linewidth': 0.6, 'grid.color': 'white',\n                    'axes.linewidth': 2, 'axes.facecolor': '#ECECEC', \n                    'axes.labelcolor': '#000000',\n                    'figure.facecolor': '#f7f7f7',\n                    'xtick.color': '#000000', 'ytick.color': '#000000'})","8d1e7875":"df = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')","59c8106e":"df.loc[df['HeartDisease'] == 1, 'HeartDisease'] = 'ill' \ndf.loc[df['HeartDisease'] == 0, 'HeartDisease'] = 'healthy' ","cab266c3":"df.loc[df['Sex'] == 'M', 'Sex'] = 'male'\ndf.loc[df['Sex'] == 'F', 'Sex'] = 'female' ","cdf5f3cb":"df.loc[df['Sex'] == 'M', 'Sex'] = 'male'\ndf.loc[df['Sex'] == 'F', 'Sex'] = 'female' ","d7b277d8":"df.loc[df['ExerciseAngina'] == 'Y', 'ExerciseAngina'] = 'yes'\ndf.loc[df['ExerciseAngina'] == 'N', 'ExerciseAngina'] = 'no' ","3acfb34a":"df = df.rename(columns = {'ChestPainType':'Chest Pain Type', \n                          'RestingBP':'Resting BP', \n                          'FastingBS':'Fasting BS',\n                          'RestingECG':'Resting ECG',\n                          'MaxHR':'Max HR',\n                          'ExerciseAngina':'Exercise Angina',\n                          'ST_Slope':'ST Slope',\n                          'HeartDisease':'Heart Disease'})","7e46e0e2":"Cat_vars = []\nNum_vars = []\n\nfor col in list(df.columns):\n    \n    if ((df[col].dtype == 'float64') | (df[col].dtype == 'int64')) & (df[col].nunique() > 10):\n        \n        Num_vars.append(col)\n    \n    else: Cat_vars.append(col)","d764167f":"Cat_vars.remove('Heart Disease')","79e7c750":"my_palette = ['#00575e', '#4bafb8']\n\nwith plt.rc_context(rc = {'figure.dpi': 350, 'axes.labelsize': 10, \n                          'xtick.labelsize': 9, 'ytick.labelsize': 9}):\n\n    fig_1, ax_1 = plt.subplots(2, 3, figsize = (11, 8.5))\n    \n    for idx, (column, axes) in list(enumerate(zip(Cat_vars, ax_1.flatten()))):\n        \n        order = df.groupby(column).size().sort_values(ascending = True).index.tolist()\n    \n        cplot = sns.countplot(ax = axes, x = df[column], hue = df['Heart Disease'],\n                              order = order, linewidth = 1.5,\n                              edgecolor = 'k', palette = my_palette, alpha = 0.8)\n        \n        axes.get_legend().remove()\n        \n        for p in cplot.patches:\n        \n            cplot.annotate(format(p.get_height(), '.0f'), \n                           (p.get_x() + p.get_width() \/ 2, p.get_height()), \n                           ha = 'center', va = 'center', size = 8, color = 'w',\n                           bbox = dict(boxstyle = 'square, pad = 0.3', \n                           fc = '#6e6e6e', lw = 1, ec = '#6e6e6e'))\n    \n    else:\n    \n        [axes.set_visible(False) for axes in ax_1.flatten()[idx + 1:]]\n    \n    ### A global legend\n    \n    handles, labels = axes.get_legend_handles_labels()\n    fig_1.legend(handles, labels, loc = 'upper center', \n                 bbox_to_anchor = (0.5, 1.06), ncol = 2, fontsize = 10,\n                 title = 'Heart Disease', title_fontsize = 11)        \n\n    plt.tight_layout(pad = 1.5)\n    plt.show()","4af4c9d2":"df[Num_vars].describe().T.round(1)","c1a96648":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 9, \n                          'xtick.labelsize': 7.5, 'ytick.labelsize': 7.5}):\n\n    fig_2, ax_2 = plt.subplots(5, 3, figsize = (11, 17))\n    \n    col_no = 0\n\n    for column in Num_vars:\n        \n        ### KDE plots\n    \n        sns.kdeplot(ax = ax_2[col_no, 0], x = df[column], \n                    hue = df['Heart Disease'],\n                    common_norm = True,\n                    fill = True, alpha = 0.4, palette = my_palette,\n                    linewidth = 1.5)\n        \n        ax_2[col_no, 0].get_legend().remove()\n        \n        ### Quantile-based discretization\n        \n        cuts = pd.qcut(df[column], 4, precision = 0)\n        \n        sns.countplot(ax = ax_2[col_no, 1], x = cuts, \n                      hue = df['Heart Disease'], \n                      linewidth = 1.5, edgecolor = 'k', \n                      palette = my_palette, alpha = 0.8)\n        \n        ax_2[col_no, 1].get_legend().remove()\n        \n        plt.setp(ax_2[col_no, 1].get_xticklabels(), rotation = 45)\n        \n        #### Violinplots & stripplots\n        \n        sns.violinplot(ax = ax_2[col_no, 2], x = df['Heart Disease'], \n                       y = df[column], palette = my_palette, scale = 'width',\n                       linewidth = 0.5, inner = None)\n        \n        plt.setp(ax_2[col_no, 2].collections, alpha = 0.35)\n        \n        sns.stripplot(ax = ax_2[col_no, 2], x = df['Heart Disease'], \n                      y = df[column], palette = my_palette, alpha = 1,\n                      s = 1.8, jitter = 0.1)\n        col_no += 1\n    \n    ### A global legend\n    \n    handles, labels = ax_2[4, 1].get_legend_handles_labels()\n    fig_2.legend(handles, labels, loc = 'upper center', \n                 bbox_to_anchor = (0.5, 1.027), ncol = 2, \n                 fontsize = 9.5, title = 'Heart Disease', title_fontsize = 10)\n    \n    plt.tight_layout(pad = 1.5)\n    plt.show()","f4c246a8":"Combinations = []\n\nfor num in ['Age', 'Max HR', 'Oldpeak']:\n    for cat in ['Sex', 'ST Slope']:\n                \n                Combinations.append([num] + [cat])","da6797a1":"from matplotlib.lines import Line2D","8acbc269":"with plt.rc_context(rc = {'figure.dpi': 500, 'axes.labelsize': 9, \n                          'xtick.labelsize': 7.5, 'ytick.labelsize': 7.5}):\n\n    fig_3, ax_3 = plt.subplots(2, 3, figsize = (13, 7.5))\n\n    for idx, ((y, x), axes) in list(enumerate(zip(Combinations, ax_3.flatten()))):\n\n        sns.violinplot(ax = axes, x = df[x], \n                       y = df[y], hue = df['Heart Disease'],\n                       scale = 'width', inner = None,\n                       linewidth = 0.5, palette = my_palette)\n        \n        plt.setp(axes.collections, alpha = 0.35)\n    \n        sns.stripplot(ax = axes, x = df[x], dodge = True,\n                      y = df[y], hue = df['Heart Disease'],\n                      palette = my_palette, s = 2, alpha = 1,\n                      jitter = 0.12)\n        \n        sns.boxplot(ax = axes, y = df[y], \n                    x = df[x], hue = df['Heart Disease'],\n                    showmeans = False, meanline = False, zorder = 10,\n                    meanprops = {'visible': True},\n                    medianprops = {'color': '#387aff', 'linestyle': '-', 'lw': 1},\n                    whiskerprops = {'visible': False},\n                    showfliers = False, showbox = False, showcaps = False)\n        \n        axes.get_legend().remove()\n    \n        plt.setp(axes.lines, zorder = 100)\n        plt.setp(axes.collections, zorder = 100)\n    \n    ### A global legend\n    \n    healthy = Line2D([0], [0], marker = 'o', color = '#f7f7f7', label = 'healthy',\n                        markerfacecolor = '#00575e', markersize = 8)\n    \n    ill = Line2D([0], [0], marker = 'o', color = '#f7f7f7', label = 'ill',\n                        markerfacecolor = '#4bafb8', markersize = 8)\n    \n    fig_3.legend(handles = [healthy, ill],\n                 loc = 'upper center', bbox_to_anchor = (0.5, 1.04), ncol = 2, \n                 fontsize = 9.5, title = 'Heart Disease', title_fontsize = 9)  \n    \n    plt.tight_layout(pad = 1.5)\n    plt.show()","32576047":"Num_vars_target = Num_vars.copy()\nNum_vars_target.append('Heart Disease')","3a40ccdb":"with plt.rc_context(rc = {'figure.dpi': 400, 'axes.labelsize': 10, \n                          'xtick.labelsize': 8, 'ytick.labelsize': 8}):\n    \n    fig_4 = sns.PairGrid(df[Num_vars_target], diag_sharey = False,\n                         hue = 'Heart Disease', palette = my_palette)\n    \n    fig_4.map_lower(sns.scatterplot, s = 8)\n    \n    fig_4.add_legend()\n    \n    fig_4.map_upper(sns.kdeplot)\n    fig_4.map_diag(sns.kdeplot, fill = True)\n    \n    plt.show()","8daeefd6":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler","8c192cf2":"def PCA_(df, Vars_list, n_components = 10):\n    \n    ### Scaling data\n    \n    scaler = StandardScaler()\n    data_scaled = pd.DataFrame(scaler.fit_transform(df[Vars_list]), \n                               columns = df[Vars_list].columns)\n    \n    ### Initialising PCA\n    \n    pca = PCA(n_components = n_components)\n    \n    pca_fit = pca.fit(data_scaled)\n    pca_transf = pca.transform(data_scaled)\n    \n    return(pca_fit, pca_transf)","36aaa059":"PCA_results = PCA_(df, Num_vars, n_components = len(Num_vars))","f66347df":"PCA_variance = pd.DataFrame({'Explained variance, %': \n                             PCA_results[0].explained_variance_ratio_*100})","376fc506":"with plt.rc_context(rc = {'figure.dpi': 120, 'axes.labelsize': 8, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6}):\n    \n    fig_5, ax_5 = plt.subplots(1, 1, figsize = (4, 3))\n\n    bar = sns.barplot(x = ['PC ' + str(i) for i in range(1, 6)], \n                      y = PCA_variance['Explained variance, %'],\n                      linewidth = 1.5, edgecolor = 'k', color = '#4bafb8', \n                      alpha = 0.8)\n    \n    ### Annotations\n    \n    for p in bar.patches:\n        \n        bar.annotate(format(p.get_height(), '.0f'), \n                     (p.get_x() + p.get_width() \/ 2, p.get_height()), \n                      ha = 'center', va = 'center', size = 8, color = 'w',\n                      xytext = (0, -20), textcoords = 'offset points',\n                      bbox = dict(boxstyle = 'square, pad = 0.4', \n                                  fc = '#6e6e6e', lw = 1, ec = '#6e6e6e'))\n    \n          \n    plt.show()","a2df0bb4":"pd.DataFrame(PCA_results[0].components_, columns = df[Num_vars].columns,\n             index = ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5']).round(2).T","5170f0fc":"PCA_scores = pd.DataFrame(PCA_results[1], columns = ['PC 1', 'PC 2', 'PC 3', 'PC 4', 'PC 5'])","d19f75dc":"with plt.rc_context(rc = {'figure.dpi': 300, 'axes.labelsize': 10, \n                          'xtick.labelsize': 8, 'ytick.labelsize': 8, \n                          'legend.fontsize': 7, 'legend.title_fontsize': 8}):\n    \n    fig_6, ax_6 = plt.subplots(1, 2, figsize = (10, 4))\n    \n    ### PC 1 & PC 2\n    \n    sns.scatterplot(ax = ax_6[0], x = PCA_scores['PC 1'], y = PCA_scores['PC 2'], \n                    hue = df['Heart Disease'], palette = my_palette , s = 9)\n    \n    ax_6[0].get_legend().remove()\n    \n    ax_6[0].axvspan(xmin = 0, xmax = 3.5, \n                    ymin = 0.2, ymax = 0.6, \n                    alpha = 0.25, color = '#ba9e59')\n    \n    ### PC 2 & PC 3\n    \n    sns.scatterplot(ax = ax_6[1], x = PCA_scores['PC 2'], y = PCA_scores['PC 3'], \n                    hue = df['Heart Disease'], palette = my_palette, s = 9)\n    \n    ax_6[1].get_legend().remove()\n    \n    ax_6[1].axvspan(xmin = 1, xmax = 3.5, \n                    ymin = 0.1, ymax = 0.7, \n                    alpha = 0.25, color = '#ba9e59')\n    \n    ### A global legend\n    \n    handles, labels = ax_6[1].get_legend_handles_labels()\n    fig_6.legend(handles, labels, loc = 'upper center', \n                 bbox_to_anchor = (0.5, 1.07), ncol = 2, \n                 fontsize = 8, title = 'Heart Disease', title_fontsize = 9)\n    \n    plt.tight_layout(pad = 1.5)\n    plt.show()","1f83530b":"from sklearn.mixture import GaussianMixture","c6524450":"def Gaussian_mix_claster_tuning(df, Vars_list, max_cluster = 10):\n\n    ### Setting initial parameters\n\n    bic = []\n\n    lowest_bic = np.infty\n    n_components_range = range(1, max_cluster)\n    \n    ### Looping over n_components\n    \n    for n_components in n_components_range:\n    \n        gmm = GaussianMixture(n_components = n_components, covariance_type = 'full')\n        gmm.fit(df[Vars_list])\n        bic.append(gmm.bic(df[Vars_list]))\n        \n    return(bic)","2669dc42":"Cluster_tuning_Gaussian_mix = pd.DataFrame({'Cluster': range(1, 10),\n                             'BIC': Gaussian_mix_claster_tuning(df, Num_vars, \n                                                                max_cluster = 10)})","bbada1d8":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6}):\n    \n    fig_7, ax_7 = plt.subplots(1, 1, figsize = (6, 3))\n\n    sns.lineplot(x = Cluster_tuning_Gaussian_mix['Cluster'], \n                 y = Cluster_tuning_Gaussian_mix['BIC'], \n                 color = '#00575e', marker = 'o', linewidth = 1)\n          \n    plt.show()","286919b5":"Gaussian_model = GaussianMixture(n_components = 3, covariance_type = 'full')\n\ndf['Gaussian_mixture'] = Gaussian_model.fit_predict(df[Num_vars])","16441d54":"from sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler","93553ab5":"def K_means_claster_tuning(df, Vars_list, max_cluster = 10):\n    \n    ### Scaling data\n    \n    scaler = StandardScaler()\n    data_scaled = scaler.fit_transform(df[Vars_list])\n    \n    ### Calculating inertia_ for each number of clusters\n    \n    SSE = []\n    \n    for cluster in range(1, max_cluster):\n        \n        kmeans = KMeans(n_clusters = cluster, init = 'k-means++', random_state = 999)\n        kmeans.fit(data_scaled)\n        SSE.append(kmeans.inertia_)\n        \n    df_plot = pd.DataFrame({'Cluster': range(1, max_cluster), 'SSE': SSE})\n    \n    return(df_plot)","64cec4d3":"Cluster_tuning_k_means = K_means_claster_tuning(df, Num_vars, max_cluster = 10)","ea5eca57":"with plt.rc_context(rc = {'figure.dpi': 110, 'axes.labelsize': 8, \n                          'xtick.labelsize': 6, 'ytick.labelsize': 6}):\n    \n    fig_8, ax_8 = plt.subplots(1, 1, figsize = (6, 3))\n\n    sns.lineplot(x = Cluster_tuning_k_means['Cluster'].astype('int64'), \n                 y = Cluster_tuning_k_means['SSE'], color = '#00575e', \n                 marker = 'o', linewidth = 1)\n    \n    plt.xticks(range(1, 10))\n          \n    plt.show()","fa2107f6":"def K_means_clastering(df_train, Vars_list, n_clusters = 10):\n    \n    ### Scaling data\n    \n    scaler = StandardScaler()\n    df_train_scaled = scaler.fit_transform(df_train[Vars_list])\n    \n    ### Initiating KMeans algorithm\n    \n    kmeans = KMeans(n_clusters = n_clusters, init = 'k-means++', random_state = 999)\n    \n    ### Getting clusters\n    \n    kmeans_results = kmeans.fit_predict(df_train_scaled)\n    \n    ### Saving results\n    \n    df_clusters_train = pd.DataFrame({'Cluster': kmeans_results})\n\n    return(df_clusters_train)","c7cb43d6":"df['K_means'] = K_means_clastering(df, Num_vars, n_clusters = 3)['Cluster']","a0fcd089":"my_palette_2 = ['#00575e', '#4bafb8', '#ba9e59']\n\nwith plt.rc_context(rc = {'figure.dpi': 300, 'axes.labelsize': 9, \n                          'xtick.labelsize': 8, 'ytick.labelsize': 8}):\n    \n    fig_9, ax_9 = plt.subplots(2, 2, figsize = (10, 7))\n    \n    ax_9 = ax_9.flatten()\n    \n    ### PC 1 & PC 2 | PC 2 & PC 3 | k-means\n    \n    sns.scatterplot(ax = ax_9[0], x = PCA_scores['PC 1'], y = PCA_scores['PC 2'], \n                    hue = df['K_means'], palette = my_palette_2, s = 10)\n    \n    ax_9[0].get_legend().remove()\n    ax_9[0].set_title('PC 1 vs PC 2 | k-means', \n                      fontsize = 11, pad = 12)\n    \n    sns.scatterplot(ax = ax_9[1], x = PCA_scores['PC 2'], y = PCA_scores['PC 3'], \n                    hue = df['K_means'], palette = my_palette_2, s = 10)\n    \n    ax_9[1].get_legend().remove()\n    ax_9[1].set_title('PC 2 vs PC 3 | k-means', \n                      fontsize = 11, pad = 12)\n    \n    ### PC 1 & PC 2 | PC 2 & PC 3 | Gaussian mixture\n    \n    sns.scatterplot(ax = ax_9[2], x = PCA_scores['PC 1'], y = PCA_scores['PC 2'], \n                    hue = df['Gaussian_mixture'], palette = my_palette_2, s = 10)\n    \n    ax_9[2].get_legend().remove()\n    ax_9[2].set_title('PC 1 vs PC 2 | Gaussian mixture', \n                      fontsize = 11, pad = 12)\n    \n    sns.scatterplot(ax = ax_9[3], x = PCA_scores['PC 2'], y = PCA_scores['PC 3'], \n                    hue = df['Gaussian_mixture'], palette = my_palette_2, s = 10)\n    \n    ax_9[3].get_legend().remove()\n    ax_9[3].set_title('PC 2 vs PC 3 | Gaussian mixture', \n                      fontsize = 11, pad = 12)\n    \n    ### A global legend\n    \n    handles, labels = ax_9[1].get_legend_handles_labels()\n    fig_9.legend(handles, labels, loc = 'upper center', \n                 bbox_to_anchor = (0.5, 1.04), ncol = 3, \n                 fontsize = 8, title = 'Clusters', title_fontsize = 9)\n    \n    plt.tight_layout(pad = 1.5)\n    plt.show()","35b98fd4":"## Thanks for reading!","f64448ca":"## 2. Clustering <a class=\"anchor\" id = \"III_2\"><\/a>","387499fc":"<h1><center> I. Preparation <\/center><\/h1>","0a4aa9a1":"## 2. Exploring interactions between variables <a class=\"anchor\" id = \"II_2\"><\/a>","752308d8":"After analysing the first 3 PCs, the following conclusions were drawn:\n\n- PC 1 was strongly correlated \u2013 what is considered a strong correlation is arbitrary and should be determined based on the nature of research \u2013 with <span style=\"color:#003ba8\"> \"Age\" <\/span> and <span style=\"color:#003ba8\"> \"Max HR\" <\/span>;\n\n\n- PC 2 decreased with increasing <span style=\"color:#003ba8\"> \"Cholesterol\" <\/span>, which was the primary factor defining PC 2;\n\n\n- PC 3 showed a strong inverse relationship with <span style=\"color:#003ba8\"> \"Resting BP\" <\/span> and a noticeable direct relationship with <span style=\"color:#003ba8\"> \"Oldpeak\" <\/span>.","29fed1fe":"$$\\scriptsize {\\mathrm {PC 2}} = -0.01*{\\mathrm {Age}} - 0.47*{\\mathrm {RestingBP}} - 0.74*{\\mathrm {Cholesterol}} - 0.34*{\\mathrm {MaxHR}} - 0.32*{\\mathrm {Oldpeak}}$$","fef9b8a1":"## 2. Preparing data for EDA <a class=\"anchor\" id = \"I_2\"><\/a>","c76e5756":"From the above graphs the following was concluded:\n\n- <span style=\"color:#003ba8\"> \"Sex\" <\/span>: male patients had a significantly higher rate of illness than women had;\n\n\n- <span style=\"color:#003ba8\"> \"Chest Pain Type\" <\/span>: patients with atypical angina (ATA) or non-anginal pain (NAP) had considerably lower chances of having a heart disease than those who were asymptomatic;\n\n\n- <span style=\"color:#003ba8\"> \"Fasting BS\" <\/span>: about 80% of people with a fasting blood sugar above 120 mg\/dl had a heart disease;\n\n\n- <span style=\"color:#003ba8\"> \"Resting ECG\" <\/span>: having ST-T wave abnormality (ST) or showing probable or definite left ventricular hypertrophy (LVH) was associated with a higher probability of being sick.\n\n\n- <span style=\"color:#003ba8\"> \"Exercise Angina\" <\/span>: more than 85% of patients who suffered from exercise-induced angina had a heart disease;\n\n\n- <span style=\"color:#003ba8\"> \"ST Slope\" <\/span>: categories Up and Flat had an inverse relationship with the target.","76ddf2b3":"The number of clusters was tuned using BIC:","01dd1146":"<div style = \"text-align: justify\">A <span style=\"color:#E85E40\"> kdeplot <\/span>  and a combination of a <span style=\"color:#E85E40\"> stripplot <\/span>  with a <span style=\"color:#E85E40\"> violinplot <\/span>  should provide us with quite a good understanding of numeric predictors. However, it might be useful to bin variables as well. It may help you discover more general trends in your data set. In this case, it was done via quantile-based discretization.<\/div>","bef2383b":"### 2.1 A gaussian mixture model","ed1f9532":"<div style = \"text-align: justify\">For instance, it is evident that the median <span style=\"color:#003ba8\"> \"Age\" <\/span> of either healthy or unhealthy male and female patients was approximately the same. What features should you pick to create such interactions? Well, you probably want to pay closer attention to variables that are highly correlated with the target; however, if you have enough time, you should consider exploring as many variables as you can. Also, it goes without saying that having strong domain knowledge can help you a lot.<\/div>","7bb1ac1b":"Useful articles and documents:\n\n- <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html\">Sklearn documentation<\/a>;\n- <a href=\"http:\/\/strata.uga.edu\/8370\/lecturenotes\/principalComponents.html\">A comprehensible exmaple<\/a>;\n- <a href=\"https:\/\/online.stat.psu.edu\/stat505\/lesson\/11\">A great text guide<\/a>;\n- <a href=\"https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ\">A great video guide<\/a>","8d6204f8":"If you have lots of numeric variables, it might be useful to cluster them so that general trends become more evident.","e8bd886a":"To plot all interactions automatically, I created a list of combined variables that were used in a for loop afterwards.","20e078ee":"Some global parameters for plots were set:","8173c288":"## 1. Exploring variables on their own <a class=\"anchor\" id = \"II_1\"><\/a>","a022699d":"### 2.2 Numeric variables","1c5cf63d":"## 1. Importing basic libraries and setting global parameters <a class=\"anchor\" id = \"I_1\"><\/a>","9d9b1cf4":"<div style = \"text-align: justify\">It can be easily seen that numeric features, by and large, were not highly correlated. The only exceptions were two pairs of predictors: <span style=\"color:#003ba8\"> \"Age\" <\/span> vs <span style=\"color:#003ba8\"> \"Max HR\" <\/span> and <span style=\"color:#003ba8\"> \"Age\" <\/span> vs <span style=\"color:#003ba8\"> \"Resting BP\" <\/span>.<\/div>","2df357e2":"Exploring the 1st <span style=\"color:#E85E40\"> biplot <\/span>, we can see that patients with larger values of PC 1 \u2013 younger patients with higher heart rates \u2013 were more likely to be healthy. The 2nd <span style=\"color:#E85E40\"> biplot <\/span> exposed a cluster of ill patinets with low (0) cholesterol (large PC 2 values).","b7d36668":"### I. Preparation\n\n* [1. Importing basic libraries and setting global parameters](#I_1)\n\n\n* [2. Preparing data for EDA](#I_2)\n\n\n### II. Visualising variables\n\n* [1. Exploring variables on their own](#II_1)\n\n\n* [2. Exploring interactions between variables](#II_2)\n\n\n### III. PCA and Clustering\n\n* [1. PCA](#III_1)\n\n\n* [2. Clustering](#III_2)","a4aadabc":"### 2.2 K-means clastering","0c43261f":"<h1><center> II. Visualising variables <\/center><\/h1>","182c2ec3":"Finally, using the elements of eigenvectors, we can calculate scores:","942bfae8":"To make EDA neater, I renamed some categories and columns:","9d2b534d":"<div style = \"text-align: justify\">In this segment, I focused on exploring relationships between features and the target. Before searching for more complex connections, I believe it makes sense to learn more about variables themselves.<\/div>","082b72eb":"<div style = \"text-align: justify\"> I used <span style=\"color:#E85E40\"> biplots <\/span> to depict complex relationships between numeric predictors, but that time I coloured observations using clusters instead of values of <span style=\"color:#003ba8\"> \"Heart Disease\" <\/span>. <\/div>","8577a873":"<div style = \"text-align: justify\"> According to the graph, the optimal number of clusters was 3 because increasing it further did not show a significant decline in SSE. <\/div>","0e0a9ed1":"<div style = \"text-align: justify\">As regrettable as it was, the 1st PC did not account for the vast majority of the explained variance. Needless to say, the more PCs we have to include in our analysis, the more challenging it becomes to make sense of our data. In this particular case, I took the first 3 principal components. Following that, I examined the elements of eigenvectors (loadings). Since loadings represent the directions of maximum variance in the data, by analysing these values, we can determine how each variable contributes to a particular principal component. The larger the values, the stronger the correlations with respective PCs. Remember, for results to be interpretable this way, we have to standardise data first.<\/div>","6d60bb06":"### 2.1 Categorical and numeric variables","08ec5539":"<div style = \"text-align: justify\">Interpreting n-dimensional data can be a tough task. As a result, it might be incredibly beneficial to reduce the number of dimensions while preserving as much information about your data as possible. That is when PCA comes into play. Instead of working with multiple original numeric variables, we use their linear combinations, paying attention to those that account for as much variation of the original data as possible. Of course, one should remember that choosing linear combinations of predictors based on the maximum variance of your data is not necessarily helpful when it comes to predicting the target variable.<\/div>","11e559c9":"Also, I separated numeric features from categorical ones, which allowed for a more efficient analysis.","7607eaab":"### 2.3 Comparing clustering results","6275e84f":"$$\\scriptsize {\\mathrm {PC 1}} = -0.60*{\\mathrm {Age}} - 0.37*{\\mathrm {RestingBP}} + 0.18*{\\mathrm {Cholesterol}} + 0.54*{\\mathrm {MaxHR}} - 0.42*{\\mathrm {Oldpeak}}$$","3812e135":"<h1><center> III. PCA and Clustering <\/center><\/h1>","232a1a45":"Useful articles and documents:\n\n- <a href=\"https:\/\/towardsdatascience.com\/gaussian-mixture-model-clusterization-how-to-select-the-number-of-components-clusters-553bef45f6e4\">A great text guide<\/a>;\n- <a href=\"https:\/\/scikit-learn.org\/stable\/auto_examples\/mixture\/plot_gmm_selection.html\">How to choose the number of clusters<\/a>","22411e54":"For more in-depth analysis two <span style=\"color:#E85E40\"> biplots <\/span> were produced:","80612825":"<div style = \"text-align: justify\">When you observe that a particular categorical predictor has a strong connection with your target variable, you can learn even more about a phenomenon you are studying by exploring interactions between this categorical variable and different numeric features.<\/div>","c647e718":"<div style = \"text-align: justify\">Analysing the above graphs, one can conclude that higher values of <span style=\"color:#003ba8\"> \"Age\" <\/span>, <span style=\"color:#003ba8\"> \"Max HR\" <\/span> and <span style=\"color:#003ba8\"> \"Oldpeak\" <\/span> (ST depression induced by exercise relative to rest) were associated with a heart disease.<\/div>","f3027b0f":"### 1.1 Categorical variables","ceabb806":"<div style = \"text-align: justify\">Firstly, I analysed eigenvalues or, in other words, the variance explained by each principal component. It was done to determine which principal components were vital for further analysis. We can always visualise explained variance ratios and get a fairly good idea of how many PCs we need.<\/div>","2f27beec":"<div style = \"text-align: justify\">It is definitely worth mentioning that various clustering algorithms will name clusters differently. Thus, we have to pay attention to how clusters are shaped rather than coloured or named.\nWith regard to this data set, it is apparent that both algorithms produced largely comparable clusters. Note, since the k-means algorithm tends to produce spherical shaped clusters, 3 perfectly distinct clusters on the 1st <span style=\"color:#E85E40\"> biplot <\/span> might be a reason for concern.<\/div>","6087352d":"## 1. PCA <a class=\"anchor\" id = \"III_1\"><\/a>","7149af64":"<h1><center> Table of contents <\/center><\/h1>","75cb0e30":"The number of clusters was tuned using sum of squared distances of samples to their closest cluster center (SSE):","ee49673b":"<div style = \"text-align: justify\">In my judgment, ordering categories based on the number of observations they have might be quite handy, as you can quickly observe what features are imbalanced. It was done by grouping variables and setting the order argument for each plot.<\/div>","874f5e7e":"$$\\scriptsize {\\mathrm {PC 3}} = -0.08*{\\mathrm {Age}} - 0.64*{\\mathrm {RestingBP}} + 0.06*{\\mathrm {Cholesterol}} + 0.04*{\\mathrm {MaxHR}} + 0.76*{\\mathrm {Oldpeak}}$$","e5a69ca4":"### 1.2 Numeric variables"}}