{"cell_type":{"a15f0a7f":"code","e1f441c0":"code","bc70eb79":"code","da9d28ed":"code","943c7fa8":"code","8cd20220":"code","11549676":"code","ceca816c":"code","57d2cc87":"code","1c1fa952":"code","f5403cc7":"code","d464eff4":"code","a2add1f8":"code","56284f45":"code","088d9cc3":"code","4392d207":"code","231d68e9":"code","e156c6e2":"code","041a818c":"code","4424458a":"code","bac80744":"code","338d57cc":"code","dab0db08":"code","00e1ee57":"code","6abdf9d5":"code","5e70b36e":"code","c056495a":"code","02f29ced":"code","5526caf2":"code","8250d651":"code","cb220031":"code","e0388982":"code","75dbe65e":"code","9eefda53":"code","0028d100":"code","dc1a852c":"code","6920c922":"code","ea22dd1b":"code","30e56c12":"code","4eb1e6da":"code","cbbaadc4":"code","8ec30c79":"code","15929bbc":"code","48244f34":"code","e60b4016":"code","8ec11b4e":"code","8b8e6787":"code","ef60e6a4":"code","d9b2ea08":"code","ec8812f2":"code","871ea459":"code","95587d6b":"code","cdd67bbf":"code","d21321b6":"code","17d39d08":"code","36137140":"code","c2fdd7d4":"code","3de4027c":"code","2612c1fa":"code","92b120fa":"code","4541f26a":"code","1a8515f9":"code","65608c64":"code","65b090a2":"code","17372c5c":"code","f9488fa0":"code","e3a045a0":"code","b98846a4":"code","43806fc8":"code","89370de8":"code","3f98b48a":"code","9f334ff2":"code","df2e7ad8":"code","c917b739":"code","89da4ce6":"code","010730ba":"code","d7b97768":"code","9d7d9bdc":"code","130cf1b0":"code","c0ed36d2":"code","ef3bdecc":"code","603006a9":"code","5bb289ea":"code","1efa85f7":"code","8e821075":"code","8fbcf6e8":"code","49da3279":"code","f34d48d5":"code","cd9ca946":"code","05d168b6":"code","c1570856":"code","fd5cf673":"code","51774a06":"code","05302296":"code","504c506d":"code","b7dc8e41":"code","860513d9":"code","ca4b6e6b":"code","aa403a13":"code","be10d304":"code","2d39368c":"code","a78bc1a9":"code","341783b1":"code","af3d425b":"code","c43c569e":"code","47127088":"code","bf1040f3":"code","ecce8c14":"code","fa3d4795":"code","a0be3dd5":"code","d683fcf5":"code","a8e9c360":"code","6d1495e8":"markdown","66addf8a":"markdown","fcefac53":"markdown","75120eaf":"markdown","b44536c4":"markdown","07c71f80":"markdown","b9abd849":"markdown","9a4795f0":"markdown","26d48029":"markdown","cfaad0ba":"markdown","50681e01":"markdown","39f5a7f4":"markdown","d5a33fea":"markdown","888cca54":"markdown","f21df199":"markdown","f2309637":"markdown","8da507ef":"markdown","96b22245":"markdown","b6f12539":"markdown","cb463f51":"markdown","d2213cd3":"markdown","4d0fa2c5":"markdown","79d8e994":"markdown","9fbb1404":"markdown","c9f2aaf6":"markdown","1db9dc66":"markdown","d4a76327":"markdown","18ad1cb3":"markdown","96cc561d":"markdown","9c30d79e":"markdown","8fb4e76a":"markdown","edc730bf":"markdown","de2ca239":"markdown","a488f4c5":"markdown","d5ff7afd":"markdown","96864df9":"markdown","de838b71":"markdown","b100ec1a":"markdown","ff29cb7c":"markdown","0749356b":"markdown","884aa4e7":"markdown","4b1e0231":"markdown","765d49e2":"markdown","9f44d6f1":"markdown","0abd4cc5":"markdown","2af6a888":"markdown","778d2514":"markdown","4bd981d4":"markdown","231d0ece":"markdown","3ca1238d":"markdown"},"source":{"a15f0a7f":"# Matplotlib config\n%matplotlib inline\n%config InlineBackend.figure_formats = ['svg']\n%config InlineBackend.rc = {'figure.figsize': (5.0, 4.0)}\n\nimport pandas as pd\nimport numpy as np\nimport csv\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import plot_tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, plot_confusion_matrix\nfrom sklearn.model_selection import GridSearchCV, RepeatedKFold, train_test_split\n\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA\nimport statsmodels.api as sm\n\ninput_file = \"..\/input\/titanic\/train.csv\"\ndf = pd.read_csv(input_file, header = 0, sep = ',', quotechar='\"')\ndf.head()","e1f441c0":"df.columns","bc70eb79":"%matplotlib\ndf.info()\ndf.describe()","da9d28ed":"#Correlation with the \"Survived\"\n\ndf.corr()[\"Survived\"].abs().sort_values(ascending = False)","943c7fa8":"#Age: Age grouped in categories Age_Group = ['Infant','Kid','Young','Adult','Elderly']. Then Age-groups get splitted in 5 single variables, with values 0\/1\n\nbins_Age_Group= [0,4.9999,13.9999,24.9999,54.9999,100]\nlabels_Age_Group = ['Infant','Kid','Young','Adult','Elderly']\ndf['Age_Group'] = pd.cut(df['Age'], bins=bins_Age_Group, labels=labels_Age_Group, right=False)\ndf['Age_Group_ord'] = pd.Categorical(df.Age_Group).codes\n\nbins_Age_Infant = [5,13.9999]\nlabels_Age_Infant = ['Infant']\ndf['Age_Infant'] = pd.cut(df['Age'], bins =bins_Age_Infant, labels =labels_Age_Infant, right=False)\ndf['Age_Infant'] = df['Age_Infant'].notna().astype('int')\n\nbins_Age_Kid = [5,13.9999]\nlabels_Age_Kid = ['Kid']\ndf['Age_Kid'] = pd.cut(df['Age'], bins =bins_Age_Kid, labels =labels_Age_Kid, right=False)\ndf['Age_Kid'] = df['Age_Kid'].notna().astype('int')\n\nbins_Age_Young = [14,24.9999]\nlabels_Age_Young = ['Young']\ndf['Age_Young'] = pd.cut(df['Age'], bins =bins_Age_Young, labels =labels_Age_Young, right=False)\ndf['Age_Young'] = df['Age_Young'].notna().astype('int')\n\nbins_Age_Adult = [25,54.9999]\nlabels_Age_Adult = ['Adult']\ndf['Age_Adult'] = pd.cut(df['Age'], bins =bins_Age_Adult, labels =labels_Age_Adult, right=False)\ndf['Age_Adult'] = df['Age_Adult'].notna().astype('int')\n\nbins_Age_Elderly = [55,100]\nlabels_Age_Elderly = ['Elderly']\ndf['Age_Elderly'] = pd.cut(df['Age'], bins =bins_Age_Elderly, labels =labels_Age_Elderly, right=False)\ndf['Age_Elderly'] = df['Age_Elderly'].notna().astype('int')","8cd20220":"#SibSp and Parch: get aggregated in one 0\/1 single variable SibORParch, which means alone\/not alone on board.\n\nbins_SibORParch = [1,20]\nlabels_SibORParch = ['notalone']\ndf['SibORParch'] = pd.cut(df['SibSp']+df['Parch'], bins =bins_SibORParch, labels =labels_SibORParch, right=False)\ndf['SibORParch'] = df['SibORParch'].notna().astype('int')","11549676":"#Sex: categories as numbers, numerical variable 0\/1 (0 = female, 1 = male)\n\ndf['Sex_ord'] = pd.Categorical(df.Sex).codes","ceca816c":"#Embarked: Embarked [C = Cherbourg, Q = Queenstown, S = Southampton], categories as numbers, as a numerical variable Embarked_ord 0\/1\/2, than Embarked get splitted in 3 single variables, with values 0\/1\n\n#C=0, Q=1, S=2\ndf['Embarked_ord'] = pd.Categorical(df.Embarked).codes\n\n#Cherbourg\nbins_Embk_Cherbourg = [0,0.9999]\nlabels_Embk_Cherbourg = ['Embk_Cherbourg']\ndf['Embk_Cherbourg'] = pd.cut(df['Embarked_ord'], bins =bins_Embk_Cherbourg, labels =labels_Embk_Cherbourg, right=False)\ndf['Embk_Cherbourg'] = df['Embk_Cherbourg'].notna().astype('int')\n\n#Queenstown\nbins_Embk_Queenstown = [1,1.9999]\nlabels_Embk_Queenstown = ['Embk_Queenstown']\ndf['Embk_Queenstown'] = pd.cut(df['Embarked_ord'], bins =bins_Embk_Queenstown, labels =labels_Embk_Queenstown, right=False)\ndf['Embk_Queenstown'] = df['Embk_Queenstown'].notna().astype('int')\n\n#Southampton\nbins_Embk_Southampton = [2,2.9999]\nlabels_Embk_Southampton = ['Embk_Southampton']\ndf['Embk_Southampton'] = pd.cut(df['Embarked_ord'], bins =bins_Embk_Southampton, labels =labels_Embk_Southampton, right=False)\ndf['Embk_Southampton'] = df['Embk_Southampton'].notna().astype('int')","57d2cc87":"#Normalise Fare as new variable 0-1\ndf['Fare_norm'] = df['Fare']\/np.max(df['Fare'])\n\n#Normalise Age as new variable 0-1\n\ndf['Age_norm'] = df['Age']\/np.max(df['Age'])\n\n#Normalise Pclass as new variable 0-1\ndf['Pclass_norm'] = df['Pclass']\/np.max(df['Pclass'])","1c1fa952":"#All the new variavles correlated with the \"Survived\"\n\ndf.corr()[\"Survived\"].abs().sort_values(ascending = False)","f5403cc7":"#Correlation Matrix\n\n#sns.heatmap(df.corr());","d464eff4":"#Write the actual dataframe df in a new csv\ndf.to_csv (r'train_new.csv', index = False, header=True)\n\n#Splitt randomly the new csv in training set (80% of rows) und validation set (20% of rows) csv \nimport random\n\nwith open('train_new.csv') as data:\n    with open('train_training.csv', 'w') as test:\n        with open('train_validation.csv', 'w') as train:\n            header = next(data)\n            train.write(header)\n            test.write(header)\n            \n            for line in data:\n                if random.random() > 0.80:\n                    train.write(line)\n                else:\n                    test.write(line)","a2add1f8":"#Read the training set (80% der data)\ninput_training_file = \".\/train_training.csv\"\ndf_training = pd.read_csv(input_training_file, header = 0, sep = ',', quotechar='\"')\n#df_training.head()","56284f45":"#Read the validation set\ninput_validation_file = \".\/train_validation.csv\"\ndf_validation = pd.read_csv(input_validation_file, header = 0, sep = ',', quotechar='\"')\n#df_validation.head()","088d9cc3":"%matplotlib\ndf_training.info()\ndf_training.describe()","4392d207":"df_training.columns","231d68e9":"# Attention! This code needs long time to run! \n\nX_linsvcgscv_training = df_training[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_linsvcgscv_training = df_training[\"Survived\"]\n\nX_linsvcgscv_validation = df_validation[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_linsvcgscv_validation = df_validation[\"Survived\"]\n\nsc_linsvcgscv = StandardScaler()\nsc_linsvcgscv_training = sc_linsvcgscv.fit(X_linsvcgscv_training)\nsc_linsvcgscv_validation = sc_linsvcgscv.fit(X_linsvcgscv_validation)\n\nX_linsvcgscv_training_scalar = sc_linsvcgscv_training.transform(X_linsvcgscv_training)\nX_linsvcgscv_validation_scalar = sc_linsvcgscv_validation.transform(X_linsvcgscv_validation)\n\nmodel_linsvcgscv = GridSearchCV(LinearSVC(), param_grid = {\n    \"C\": [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1], \n    \"max_iter\": [1000000]\n}, cv = RepeatedKFold())\n\nmodel_linsvcgscv_training = model_linsvcgscv.fit(X_linsvcgscv_training_scalar, y_linsvcgscv_training)\nmodel_linsvcgscv_validation = model_linsvcgscv.fit(X_linsvcgscv_validation_scalar, y_linsvcgscv_validation)\n\nprint('Best model parameter: ' + str(model_linsvcgscv_training.best_params_))\nprint('Best model score: ' + str(model_linsvcgscv_training.best_score_))\n\nprint('Model score: ' + str(model_linsvcgscv_training.score(X_linsvcgscv_training, y_linsvcgscv_training)))\nprint('Model score testing new data: ' + str(model_linsvcgscv_validation.score(X_linsvcgscv_validation, y_linsvcgscv_validation)))","e156c6e2":"#from sklearn.svm import LinearSVC\n\nX_linsvc_training = df_training[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_linsvc_training = df_training[\"Survived\"]\n\nsc_linsvc = StandardScaler()\nsc_linsvc_training = sc_linsvc.fit(X_linsvc_training)\n\nX_linsvc_training_scale = sc_linsvc_training.transform(X_linsvc_training)\n\nmodel_linsvc = LinearSVC(max_iter = 1000000, C = 0.01)\nmodel_linsvc.fit(X_linsvc_training_scale, y_linsvc_training)\n\n#print(model_linsvc.score(X_linsvc_training_scale, y_linsvc_training))","041a818c":"plot_confusion_matrix(model_linsvc, X_linsvc_training, y_linsvc_training, normalize = \"all\")","4424458a":"X_linsvc_validation = df_validation[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_linsvc_validation = model_linsvc.predict(X_linsvc_validation)\n\n#Add new column with the values predicted with the Linear SVC\ndf_validation['Survived_linsvc_validation']=y_linsvc_validation","bac80744":"plot_confusion_matrix(model_linsvc, X_linsvc_validation, y_linsvc_validation, normalize = \"all\")","338d57cc":"#Confusion matrix: the true values from the validation set vs. LinearSVC predicted values for the validation set\n\ny_linsvc_true = df_validation['Survived']\ny_linsvc_pred = df_validation['Survived_linsvc_validation']\n\nlinsvc_validation_cm = confusion_matrix(y_linsvc_true, y_linsvc_pred, normalize = \"all\")\nlinsvc_validation_score = accuracy_score(y_linsvc_true, y_linsvc_pred, normalize = \"all\")\n\nlinsvc_validation_cm","dab0db08":"print('LinSVC training score: ' + str(model_linsvc.score(X_linsvc_training, y_linsvc_training)))\nprint('LinSVC validation score: ' + str(linsvc_validation_score))","00e1ee57":"#from sklearn.svm import SVC\n#from sklearn.preprocessing import StandardScaler\n\nX_rbfsvcgscv_training = df_training[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_rbfsvcgscv_training = df_training[\"Survived\"]\n\nX_rbfsvcgscv_validation = df_validation[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_rbfsvcgscv_validation = df_validation[\"Survived\"]\n\n#Use scalar or PCA for optimizing\nsc_rbfsvcgscv = StandardScaler()\nsc_rbfsvcgscv_training = sc_rbfsvcgscv.fit(X_rbfsvcgscv_training)\nsc_rbfsvcgscv_validation = sc_rbfsvcgscv.fit(X_rbfsvcgscv_validation)\n\nX_rbfsvcgscv_training_scalar = sc_rbfsvcgscv_training.transform(X_rbfsvcgscv_training)\nX_rbfsvcgscv_validation_scalar = sc_rbfsvcgscv_validation.transform(X_rbfsvcgscv_validation)\n\nmodel_rbfsvcgscv = GridSearchCV(SVC(), param_grid = {\n    \"kernel\": [\"rbf\"], \n    \"C\": [20, 25, 30, 35, 40], \n    \"gamma\": [0.0005, 0.001, 0.005, 0.01, 0.05]\n}, cv = RepeatedKFold(), n_jobs = 8)\n\nmodel_rbfsvcgscv_training = model_rbfsvcgscv.fit(X_rbfsvcgscv_training_scalar, y_rbfsvcgscv_training)\nmodel_rbfsvcgscv_validation = model_rbfsvcgscv.fit(X_rbfsvcgscv_validation_scalar, y_rbfsvcgscv_validation)\n\nprint('Best model parameter: ' + str(model_rbfsvcgscv_training.best_params_))\nprint('Best model score: ' + str(model_rbfsvcgscv_training.best_score_))\n\nprint('Model score: ' + str(model_rbfsvcgscv_training.score(X_rbfsvcgscv_training, y_rbfsvcgscv_training)))\nprint('Model score testing new data: ' + str(model_rbfsvcgscv_validation.score(X_rbfsvcgscv_validation, y_rbfsvcgscv_validation)))","6abdf9d5":"#from sklearn.svm import SVC\n\nX_rbfsvc_training = df_training[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_rbfsvc_training = df_training[\"Survived\"]\n\nsc_rbfsvc = StandardScaler()\nsc_rbfsvc_training = sc_rbfsvc.fit(X_rbfsvc_training)\n\nX_rbfsvc_training_scale = sc_rbfsvc_training.transform(X_rbfsvc_training)\n\nmodel_rbfsvc = SVC(kernel = \"rbf\", C = 40, gamma = 0.01)\nmodel_rbfsvc.fit(X_rbfsvc_training_scale, y_rbfsvc_training)\n\n#print(model_rbfsvc.score(X_rbfsvc_training_scale, y_rbfsvc_training))","5e70b36e":"plot_confusion_matrix(model_linsvc, X_rbfsvc_training, y_rbfsvc_training, normalize = \"all\")","c056495a":"X_rbfsvc_validation = df_validation[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_rbfsvc_validation = model_rbfsvc.predict(X_rbfsvc_validation)\n\n#Add new column with the values predicted with the Linear SVC\ndf_validation['Survived_rbfsvc_validation']=y_rbfsvc_validation","02f29ced":"plot_confusion_matrix(model_linsvc, X_rbfsvc_validation, y_rbfsvc_validation, normalize = \"all\")","5526caf2":"#Confusion matrix: the true values from the validation set vs. rbfSVC predicted values for the validation set\n\ny_rbfsvc_true = df_validation['Survived']\ny_rbfsvc_pred = df_validation['Survived_linsvc_validation']\n\nrbfsvc_validation_cm = confusion_matrix(y_rbfsvc_true, y_rbfsvc_pred, normalize = \"all\")\nrbfsvc_validation_score = accuracy_score(y_rbfsvc_true, y_rbfsvc_pred, normalize = \"all\")\n\nrbfsvc_validation_cm","8250d651":"print('RBF SVC training score: ' + str(model_rbfsvc.score(X_rbfsvc_training, y_rbfsvc_training)))\nprint('RBF SVC validation score: ' + str(rbfsvc_validation_score))","cb220031":"#test some parameters with GridSearchCV for the DecisionTreeClassifier\n\n#from sklearn.tree import DecisionTreeClassifier\n\nfeatures_dt = ['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]\nX_dtgscv_training = pd.get_dummies(df_training[features_dt])\ny_dtgscv_training = df_training[\"Survived\"]\nX_dtgscv_validation = pd.get_dummies(df_validation[features_dt])\ny_dtgscv_validation = df_validation[\"Survived\"]\n\nmodel_dtgscv = GridSearchCV(DecisionTreeClassifier(), param_grid = {\n    'max_depth': [23, 24, 25, 26, 27, 28, 29, 30, 31, 32],\n    'min_samples_leaf': [2, 3, 4, 5, 6, 7]\n}, cv = RepeatedKFold())\n\nmodel_dtgscv.fit(X_dtgscv_training, y_dtgscv_training)\n\nprint('Best model parameters: ' + str(model_dtgscv.best_params_))\nprint('Best model score: ' + str(model_dtgscv.best_score_))\n\nprint('Model score: ' + str(model_dtgscv.score(X_dtgscv_training, y_dtgscv_training)))\nprint('Model score testing new data: ' + str(model_dtgscv.score(X_dtgscv_validation, y_dtgscv_validation)))","e0388982":"#pd.DataFrame(model_dtgscv.cv_results_)","75dbe65e":"#Model DecisionTreeClassifier: Train a DecisionTreeClassifier with the training set\n\n#from sklearn.tree import DecisionTreeClassifier\n\n#features_dt = ['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]\nX_dt_training = pd.get_dummies(df_training[features_dt])\ny_dt_training = df_training[\"Survived\"]\n\n#set the best parameters for the decision tree\nmodel_dt = DecisionTreeClassifier(max_depth=32, min_samples_leaf = 6)\nmodel_dt.fit(X_dt_training, y_dt_training)","9eefda53":"#Print the DecisionTree\n#from sklearn.tree import plot_tree\n#import matplotlib.pyplot as plt\n\nplot_tree(model_dt, \n          feature_names = ['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ], \n          class_names = [\"not survived\", \"survived\"],\n          filled = True)\nplt.show()","0028d100":"plot_confusion_matrix(model_dt, X_dt_training, y_dt_training, normalize = \"all\")","dc1a852c":"#Model DecisionTreeClassifier: Validate the fitted DecisionTreeClassifier\n#Predict the validation set with the fitted DecisionTreeClassifier\n\nX_dt_validation = pd.get_dummies(df_validation[features_dt])\ny_dt_validation = model_dt.predict(X_dt_validation)\n\n#Add new column with the values predicted with the DecisionTreeClassifier\ndf_validation['Survived_dt_validation']=y_dt_validation\n#df_validation.head()","6920c922":"plot_confusion_matrix(model_dt, X_dt_validation, y_dt_validation, normalize = \"all\")","ea22dd1b":"#Confusion matrix: the true values from the validation set vs. DecisionTreeClassifier predicted values for the validation set\n\ny_dt_true = df_validation['Survived']\ny_dt_pred = df_validation['Survived_dt_validation']\n\ndt_validation_cm = confusion_matrix(y_dt_true, y_dt_pred, normalize = \"all\")\ndt_validation_score = accuracy_score(y_dt_true, y_dt_pred, normalize = \"all\")\n\ndt_validation_cm","30e56c12":"print('DT training score: ' + str(model_dt.score(X_dt_training, y_dt_training)))\nprint('DT validation score: ' + str(dt_validation_score))","4eb1e6da":"#test some parameters with GridSearchCV for the RandomForestClassifier\n\n#from sklearn.ensemble import RandomForestClassifier\n\nfeatures_rf = ['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]\nX_rfgscv_training = pd.get_dummies(df_training[features_rf])\ny_rfgscv_training = df_training[\"Survived\"]\nX_rfgscv_validation = pd.get_dummies(df_validation[features_rf])\ny_rfgscv_validation = df_validation[\"Survived\"]\n\nmodel_rfgscv = GridSearchCV(RandomForestClassifier(), param_grid = {\n    'max_depth': [11, 12, 13, 14, 15],\n    'min_samples_leaf': [1, 2, 3]\n}, cv = RepeatedKFold())\n\nmodel_rfgscv.fit(X_rfgscv_training, y_rfgscv_training)\n\nprint('Best model parameter: ' + str(model_rfgscv.best_params_))\nprint('Best model score: ' + str(model_rfgscv.best_score_))\n\nprint('Model score: ' + str(model_rfgscv.score(X_rfgscv_training, y_rfgscv_training)))\nprint('Model score testing new data: ' + str(model_rfgscv.score(X_rfgscv_validation, y_rfgscv_validation)))","cbbaadc4":"#Model RandomForestClassifier: Train a RandomForestClassifier model with the training set\n\n#from sklearn.ensemble import RandomForestClassifier\n\n#features_rf = ['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]\nX_rf_training = pd.get_dummies(df_training[features_rf])\ny_rf_training = df_training[\"Survived\"]\n\nmodel_rf = RandomForestClassifier(n_estimators=100, max_depth=13, min_samples_leaf = 2)\nmodel_rf.fit(X_rf_training, y_rf_training)","8ec30c79":"plot_confusion_matrix(model_rf, X_rf_training, y_rf_training, normalize = \"all\")","15929bbc":"#Model RandomForestClassifier: Validate the fitted RandomForestClassifier model\n#Predict the validation set with the fitted RandomForestClassifier model\n\nX_rf_validation = pd.get_dummies(df_validation[features_rf])\ny_rf_validation = model_rf.predict(X_rf_validation)\n\n#Add new column with the values predicted with the RandomForestClassifier model\ndf_validation['Survived_rf_validation']=y_rf_validation\n#df_validation.head()","48244f34":"plot_confusion_matrix(model_rf, X_rf_validation, y_rf_validation, normalize = \"all\")","e60b4016":"#Confusion matrix: the true values from the validation set vs. RandomForestClassifier model predicted values for the validation set\n\ny_rf_true = df_validation['Survived']\ny_rf_pred = df_validation['Survived_rf_validation']\n\nrf_validation_cm = confusion_matrix(y_rf_true, y_rf_pred, normalize = \"all\")\nrf_validation_score = accuracy_score(y_rf_true, y_rf_pred, normalize = \"all\")\n\nrf_validation_cm","8ec11b4e":"print('RF training score: ' + str(model_rf.score(X_rf_training, y_rf_training)))\nprint('RF validation score: ' + str(rf_validation_score))","8b8e6787":"#from sklearn.neighbors import KNeighborsClassifier\n\nX_knngscv_training = df_training[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_knngscv_training = df_training[\"Survived\"]\n\nX_knngscv_validation = df_validation[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_knngscv_validation = df_validation[\"Survived\"]\n\nsc_knngscv = StandardScaler()\nsc_knngscv_training = sc_knngscv.fit(X_knngscv_training)\nsc_knngscv_validation = sc_knngscv.fit(X_knngscv_validation)\n\nX_knngscv_training_scalar = sc_knngscv_training.transform(X_knngscv_training)\nX_knngscv_validation_scalar = sc_knngscv_validation.transform(X_knngscv_validation)\n\n#manhattan_distance (p=1) and euclidean_distance (p=2)\nmodel_knngscv = GridSearchCV(KNeighborsClassifier(), param_grid = {\n    'n_neighbors': [5, 6, 7, 8, 9, 10, 15, 20, 25, 35, 50, 75],\n    'p': [1, 2], \n    #'weights': ['uniform', 'distance']\n}, cv = RepeatedKFold())\n\nmodel_knngscv_training = model_knngscv.fit(X_knngscv_training_scalar, y_knngscv_training)\nmodel_knngscv_validation = model_knngscv.fit(X_knngscv_validation_scalar, y_knngscv_validation)\n\nprint('Best model parameter: ' + str(model_knngscv_training.best_params_))\nprint('Best model score: ' + str(model_knngscv_training.best_score_))\n\nprint('Model score: ' + str(model_knngscv_training.score(X_knngscv_training, y_knngscv_training)))\nprint('Model score testing new data: ' + str(model_knngscv_validation.score(X_knngscv_validation, y_knngscv_validation)))","ef60e6a4":"#from sklearn.neighbors import KNeighborsClassifier\n\nX_knn_training = df_training[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_knn_training = df_training[\"Survived\"]\n\nsc_knn = StandardScaler()\nsc_knn.fit(X_knn_training)\n\nX_knn_training_scaled = sc_knn.transform(X_knn_training)\n\n#manhattan distance p=1\nmodel_knn = KNeighborsClassifier(n_neighbors = 25, p = 2)\nmodel_knn.fit(X_knn_training_scaled, y_knn_training)","d9b2ea08":"#print(model_knn.predict_proba(X_knn_training_scaled))","ec8812f2":"plot_confusion_matrix(model_knn, X_knn_training_scaled, y_knn_training, normalize = \"all\")","871ea459":"X_knn_validation = df_validation[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\nX_knn_validation_scaled = sc_knn.transform(X_knn_validation)\ny_knn_validation = model_knn.predict(X_knn_validation_scaled)\n\ndf_validation['Survived_knn_validation']=y_knn_validation","95587d6b":"plot_confusion_matrix(model_knn, X_knn_validation_scaled, y_knn_validation, normalize = \"all\")","cdd67bbf":"#Confusion matrix: the true values from the validation set vs. KNN model predicted values for the validation set\n\ny_knn_true = df_validation['Survived']\ny_knn_pred = df_validation['Survived_knn_validation']\n\nknn_validation_cm = confusion_matrix(y_knn_true, y_knn_pred, normalize = \"all\")\nknn_validation_score = accuracy_score(y_knn_true, y_knn_pred, normalize = \"all\")\n\nknn_validation_cm","d21321b6":"print('KNN training score: ' + str(model_knn.score(X_knn_training_scaled, y_knn_training)))\nprint('KNN validation score: ' + str(knn_validation_score))","17d39d08":"#from sklearn.linear_model import LogisticRegression\n\nX_lgr_training = df_training[['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_lgr_training = df_training[['Survived']]\n\nmodel_lgr = LogisticRegression(class_weight = \"balancend\")\nmodel_lgr.fit(X_lgr_training, y_lgr_training)","36137140":"plot_confusion_matrix(model_lgr, X_lgr_training, y_lgr_training, normalize = \"all\")","c2fdd7d4":"X_lgr_validation = df_validation[['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\n\ny_lgr_validation = model_lgr.predict(X_lgr_validation)\ndf_validation['Survived_lgr_validation']=y_lgr_validation","3de4027c":"plot_confusion_matrix(model_lgr, X_lgr_validation, y_lgr_validation, normalize = \"all\")","2612c1fa":"#Confusion matrix: the true values from the validation set vs. Logistic Regression model predicted values for the validation set\n\ny_lgr_true = df_validation['Survived']\ny_lgr_pred = df_validation['Survived_lgr_validation']\n\nlgr_validation_cm = confusion_matrix(y_lgr_true, y_lgr_pred, normalize = \"all\")\nlgr_validation_score = accuracy_score(y_lgr_true, y_lgr_pred, normalize = \"all\")\n\nlgr_validation_cm","92b120fa":"print('LgR training score: ' + str(model_lgr.score(X_lgr_training, y_lgr_training)))\nprint('LgR validation score: ' + str(lgr_validation_score))","4541f26a":"#from sklearn.decomposition import PCA\n\nX_pca_lgr_training = df_training[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_pca_lgr_training = df_training[\"Survived\"]\n\npca = PCA(n_components = 2)\n\n# pca.fit(X_pca_lgr_training)\n# X_pca_lgr_training_transformed = pca.transform(X_pca_lgr_training)\n\nX_pca_lgr_training_transformed = pca.fit_transform(X_pca_lgr_training)\n\nmodel_pca_lgr = LogisticRegression(class_weight = \"balancend\")\nmodel_pca_lgr.fit(X_pca_lgr_training_transformed, y_pca_lgr_training)","1a8515f9":"sns.scatterplot(X_pca_lgr_training_transformed[:, 0], X_pca_lgr_training_transformed[:, 1], hue = df_training[\"Survived\"]);","65608c64":"plot_confusion_matrix(model_pca_lgr, X_pca_lgr_training_transformed, y_pca_lgr_training, normalize = \"all\")","65b090a2":"#Dimension reduction and Logistic Regression\n\nX_pca_lgr_validation = df_validation[['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\nX_pca_lgr_validation_transformed = pca.transform(X_pca_lgr_validation)\n\ny_pca_lgr_validation = model_pca_lgr.predict(pca.transform(X_pca_lgr_validation))\ndf_validation['Survived_pca_lgr_validation'] = y_pca_lgr_validation","17372c5c":"sns.scatterplot(X_pca_lgr_validation_transformed[:, 0], X_pca_lgr_validation_transformed[:, 1], hue = df_validation[\"Survived\"]);","f9488fa0":"plot_confusion_matrix(model_pca_lgr, X_pca_lgr_validation_transformed, y_pca_lgr_validation, normalize = \"all\")","e3a045a0":"#Confusion matrix: the true values from the validation set vs. PCA Logistic Regression model predicted values for the validation set\n\ny_pca_lgr_true = df_validation['Survived']\ny_pca_lgr_pred = df_validation['Survived_pca_lgr_validation']\n\npca_lgr_validation_cm = confusion_matrix(y_pca_lgr_true, y_pca_lgr_pred, normalize = \"all\")\npca_lgr_validation_score = accuracy_score(y_pca_lgr_true, y_pca_lgr_pred, normalize = \"all\")\n\npca_lgr_validation_cm","b98846a4":"print('PCA LgR training score: ' + str(model_pca_lgr.score(X_pca_lgr_training_transformed, y_pca_lgr_training)))\nprint('PCA LgR validation score: ' + str(pca_lgr_validation_score))","43806fc8":"#Model OLS: Train an OLS model with the training set\n\n#import statsmodels.api as sm\n\nX_ols_training = df_training[['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\ny_ols_training = df_training[['Survived']]\n\nX1_ols_training = sm.add_constant(X_ols_training)\nmodel_ols = sm.OLS(y_ols_training, X1_ols_training).fit()\n\n#summary\nmodel_ols.summary()","89370de8":"#Model OLS: Validate the fitted OLS model\n#Predict the validation set with the fitted OLS model\n\nX_ols_validation = df_validation[['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\nX1_ols_validation = sm.add_constant(X_ols_validation)\n\ny_ols_validation =  model_ols.predict(X1_ols_validation)\ny_ols_validation =  round(y_ols_validation)\n\n#Add new column with the values predicted with the OLS model\ndf_validation['Survived_ols_validation']=y_ols_validation","3f98b48a":"#Confusion matrix: the true values from the validation set vs. OLS model predicted values for the validation set\n\ny_ols_true = df_validation['Survived']\ny_ols_pred = df_validation['Survived_ols_validation']\n\nols_validation_cm = confusion_matrix(y_ols_true, y_ols_pred, normalize = \"all\")\nols_validation_score = accuracy_score(y_ols_true, y_ols_pred, normalize = \"all\")\n\nols_validation_cm","9f334ff2":"print('OLS validation score: ' + str(ols_validation_score))","df2e7ad8":"#Accuracy\n\nA_linsvc = accuracy_score(y_linsvc_true, y_linsvc_pred)\nA_rbfsvc = accuracy_score(y_rbfsvc_true, y_rbfsvc_pred)\nA_dt = accuracy_score(y_dt_true, y_dt_pred)\nA_rf = accuracy_score(y_rf_true, y_rf_pred)\nA_knn = accuracy_score(y_knn_true, y_knn_pred)\nA_lgr = accuracy_score(y_lgr_true, y_lgr_pred)\nA_pca_lgr = accuracy_score(y_pca_lgr_true, y_pca_lgr_pred)\nA_ols = accuracy_score(y_ols_true, y_ols_pred)\n\nprint(\"Accuracy LinSVC = \" + str(A_linsvc))\nprint(\"Accuracy RBF SVC = \" + str(A_rbfsvc))\nprint(\"Accuracy DT = \" + str(A_dt))\nprint(\"Accuracy RF = \" + str(A_rf))\nprint(\"Accuracy KNN = \" + str(A_knn))\nprint(\"Accuracy LgR = \" + str(A_lgr))\nprint(\"Accuracy PCA LgR = \" + str(A_pca_lgr))\nprint(\"Accuracy OLS = \" + str(A_ols))","c917b739":"#Recall\n\nR_linsvc = recall_score(y_linsvc_true, y_linsvc_pred)\nR_rbfsvc = recall_score(y_rbfsvc_true, y_rbfsvc_pred)\nR_dt = recall_score(y_dt_true, y_dt_pred)\nR_rf = recall_score(y_rf_true, y_rf_pred)\nR_knn = recall_score(y_knn_true, y_knn_pred)\nR_lgr = recall_score(y_lgr_true, y_lgr_pred)\nR_pca_lgr = recall_score(y_pca_lgr_true, y_pca_lgr_pred)\nR_ols = recall_score(y_ols_true, y_ols_pred)\n\nprint(\"Recall LinSVC = \" + str(R_linsvc))\nprint(\"Recall RBF SVC = \" + str(R_rbfsvc))\nprint(\"Recall DT = \" + str(R_dt))\nprint(\"Recall RF = \" + str(R_rf))\nprint(\"Recall KNN = \" + str(R_knn))\nprint(\"Recall LgR = \" + str(R_lgr))\nprint(\"Recall PCA LgR = \" + str(R_pca_lgr))\nprint(\"Recall OLS = \" + str(R_ols))","89da4ce6":"#Precision\n\nP_linsvc = precision_score(y_linsvc_true, y_linsvc_pred)\nP_rbfsvc = precision_score(y_rbfsvc_true, y_rbfsvc_pred)\nP_dt = precision_score(y_dt_true, y_dt_pred)\nP_rf = precision_score(y_rf_true, y_rf_pred)\nP_knn = precision_score(y_knn_true, y_knn_pred)\nP_lgr = precision_score(y_lgr_true, y_lgr_pred)\nP_pca_lgr = precision_score(y_pca_lgr_true, y_pca_lgr_pred)\nP_ols = precision_score(y_ols_true, y_ols_pred)\n\nprint(\"Precision LinSVC = \" + str(P_linsvc))\nprint(\"Precision RBF SVC = \" + str(P_rbfsvc))\nprint(\"Precision DT = \" + str(P_dt))\nprint(\"Precision RF = \" + str(P_rf))\nprint(\"Precision KNN = \" + str(P_knn))\nprint(\"Precision LgR = \" + str(P_lgr))\nprint(\"Precision PCA_LgR = \" + str(P_pca_lgr))\nprint(\"Precision OLS = \" + str(P_ols))","010730ba":"relevant_metrics = pd.DataFrame({\n    'Model': ['LinearSVC', 'RBF SVC', 'Decision Tree', 'Random Forest', 'K-Nearest-Neighbours', 'Logistic Regression', 'PCA Logistic Regression', 'Ordinary Least Squares Linear Regression'],\n    'Accuracy, A': [A_linsvc, A_rbfsvc, A_dt, A_rf, A_knn, A_lgr, A_pca_lgr, A_ols],\n    'Recall, R': [R_linsvc, R_rbfsvc, R_dt, R_rf, R_knn, R_lgr, R_pca_lgr, R_ols],\n    'Precision, P': [P_linsvc, P_rbfsvc, P_dt, P_rf, P_knn, P_lgr, P_pca_lgr, P_ols]})\nbest_model =relevant_metrics.sort_values(by='Accuracy, A', ascending=False)\nbest_model","d7b97768":"#Read the test set\ninput_test_file = \"..\/input\/titanic\/test.csv\"\ndf_test = pd.read_csv(input_test_file, header = 0, sep = ',', quotechar='\"')\ndf_test.head()","9d7d9bdc":"#Open the example set (with 100% accuracy) for comparison\n\ninput_example_file = \"..\/input\/titanic-leaked\/titanic.csv\"\ndf_example = pd.read_csv(input_example_file, header = 0, sep = ',', quotechar='\"')\ndf_example.head()","130cf1b0":"df_test.info()","c0ed36d2":"#Age\nimport numpy as np\n\nbins_Age_Group= [0,4.9999,13.9999,24.9999,54.9999,100]\nlabels_Age_Group = ['Infant','Kid','Young','Adult','Elderly']\ndf_test['Age_Group'] = pd.cut(df_test['Age'], bins=bins_Age_Group, labels=labels_Age_Group, right=False)\n#df['Age_Group_ord'] = pd.Categorical(df.Age_Group).codes\n\nbins_Age_Infant = [5,13.9999]\nlabels_Age_Infant = ['Infant']\ndf_test['Age_Infant'] = pd.cut(df_test['Age'], bins =bins_Age_Infant, labels =labels_Age_Infant, right=False)\ndf_test['Age_Infant'] = df_test['Age_Infant'].notna().astype('int')\n\nbins_Age_Kid = [5,13.9999]\nlabels_Age_Kid = ['Kid']\ndf_test['Age_Kid'] = pd.cut(df_test['Age'], bins =bins_Age_Kid, labels =labels_Age_Kid, right=False)\ndf_test['Age_Kid'] = df_test['Age_Kid'].notna().astype('int')\n\nbins_Age_Young = [14,24.9999]\nlabels_Age_Young = ['Young']\ndf_test['Age_Young'] = pd.cut(df_test['Age'], bins =bins_Age_Young, labels =labels_Age_Young, right=False)\ndf_test['Age_Young'] = df_test['Age_Young'].notna().astype('int')\n\nbins_Age_Adult = [25,54.9999]\nlabels_Age_Adult = ['Adult']\ndf_test['Age_Adult'] = pd.cut(df_test['Age'], bins =bins_Age_Adult, labels =labels_Age_Adult, right=False)\ndf_test['Age_Adult'] = df_test['Age_Adult'].notna().astype('int')\n\nbins_Age_Elderly = [55,100]\nlabels_Age_Elderly = ['Elderly']\ndf_test['Age_Elderly'] = pd.cut(df_test['Age'], bins =bins_Age_Elderly, labels =labels_Age_Elderly, right=False)\ndf_test['Age_Elderly'] = df_test['Age_Elderly'].notna().astype('int')","ef3bdecc":"#SibSp and Parch\n\nbins_SibORParch = [1,20]\nlabels_SibORParch = ['notalone']\ndf_test['SibORParch'] = pd.cut(df_test['SibSp']+df_test['Parch'], bins =bins_SibORParch, labels =labels_SibORParch, right=False)\ndf_test['SibORParch'] = df_test['SibORParch'].notna().astype('int')","603006a9":"#Sex\ndf_test['Sex_ord'] = pd.Categorical(df_test.Sex).codes","5bb289ea":"#Embarked\n\n#C=0, Q=1, S=2\ndf_test['Embarked_ord'] = pd.Categorical(df_test.Embarked).codes\n\n#Cherbourg\nbins_Embk_Cherbourg = [0,0.9999]\nlabels_Embk_Cherbourg = ['Embk_Cherbourg']\ndf_test['Embk_Cherbourg'] = pd.cut(df_test['Embarked_ord'], bins =bins_Embk_Cherbourg, labels =labels_Embk_Cherbourg, right=False)\ndf_test['Embk_Cherbourg'] = df_test['Embk_Cherbourg'].notna().astype('int')\n\n#Queenstown\nbins_Embk_Queenstown = [1,1.9999]\nlabels_Embk_Queenstown = ['Embk_Queenstown']\ndf_test['Embk_Queenstown'] = pd.cut(df_test['Embarked_ord'], bins =bins_Embk_Queenstown, labels =labels_Embk_Queenstown, right=False)\ndf_test['Embk_Queenstown'] = df_test['Embk_Queenstown'].notna().astype('int')\n\n#Southampton\nbins_Embk_Southampton = [2,2.9999]\nlabels_Embk_Southampton = ['Embk_Southampton']\ndf_test['Embk_Southampton'] = pd.cut(df_test['Embarked_ord'], bins =bins_Embk_Southampton, labels =labels_Embk_Southampton, right=False)\ndf_test['Embk_Southampton'] = df_test['Embk_Southampton'].notna().astype('int')","1efa85f7":"#Normalise Fare\ndf_test['Fare'] = df_test['Fare'].fillna(0.0)\ndf_test['Fare_norm'] = df_test['Fare']\/np.max(df_test['Fare'])\n#df_test['Fare_norm'] = df_test['Fare_norm'].fillna(0.0)\n\n#Normalise Age\ndf_test['Age_norm'] = df_test['Age']\/np.max(df_test['Age'])\n\n#Normalise Pclass\ndf_test['Pclass_norm'] = df_test['Pclass']\/np.max(df_test['Pclass'])\n\ndf_test.info()","8e821075":"X_linsvc_test = df_test[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\n\nsc_linsvc = StandardScaler()\nsc_linsvc_test = sc_linsvc.fit(X_linsvc_test)\nX_linsvc_test_scaled = sc_linsvc.transform(X_linsvc_test)\n\ny_linsvc_test = model_linsvc.predict(X_linsvc_test_scaled)\n\ndf_test['Survived_linsvc_test']=y_linsvc_test\n\n#print(model_linsvc.score(X_linsvc_test_scaled, y_linsvc_test))","8fbcf6e8":"plot_confusion_matrix(model_linsvc, X_linsvc_test_scaled, y_linsvc_test, normalize = \"all\")","49da3279":"#Confusion Matrix: Compare the example as the truth vs. my predicted values with the KNN model\n\ny_example_true = df_example['Survived']\ny_example_vs_linsvc_pred = y_linsvc_test\n\nexample_vs_linsvc_test_cm = confusion_matrix(y_example_true, y_example_vs_linsvc_pred, normalize = \"all\")\nexample_vs_linsvc_test_score = accuracy_score(y_example_true, y_example_vs_linsvc_pred, normalize = \"all\")\n\nprint(example_vs_linsvc_test_cm)\nprint('LinSVC test score: ' + str(example_vs_linsvc_test_score))","f34d48d5":"X_rbfsvc_test = df_test[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\n\nsc_rbfsvc = StandardScaler()\nsc_rbfsvc_test = sc_rbfsvc.fit(X_rbfsvc_test)\nX_rbfsvc_test_scaled = sc_rbfsvc.transform(X_rbfsvc_test)\n\ny_rbfsvc_test = model_rbfsvc.predict(X_rbfsvc_test_scaled)\n\ndf_test['Survived_rbfsvc_test']=y_rbfsvc_test\n\n#print(model_rbfsvc.score(X_rbfsvc_test_scaled, y_rbfsvc_test))","cd9ca946":"plot_confusion_matrix(model_rbfsvc, X_rbfsvc_test_scaled, y_rbfsvc_test, normalize = \"all\")","05d168b6":"#Confusion Matrix: Compare the example as the truth vs. my predicted values with the KNN model\n\ny_example_true = df_example['Survived']\ny_example_vs_rbfsvc_pred = y_rbfsvc_test\n\nexample_vs_rbfsvc_test_cm = confusion_matrix(y_example_true, y_example_vs_rbfsvc_pred, normalize = \"all\")\nexample_vs_rbfsvc_test_score = accuracy_score(y_example_true, y_example_vs_rbfsvc_pred, normalize = \"all\")\n\nprint(example_vs_rbfsvc_test_cm)\nprint('RBF SVC test score: ' + str(example_vs_rbfsvc_test_score))","c1570856":"#Model DecisionTree: Predicting the test set with the fitted DecisionTree model\n\nX_dt_test = pd.get_dummies(df_test[features_dt])\n\ny_dt_test = model_dt.predict(X_dt_test)\n\n#Add new column with the values predicted with the RandomForestClassifier model\ndf_test['Survived_dt_test']=y_dt_test\n\n#print(model_dt.score(X_dt_test, y_dt_test))","fd5cf673":"plot_confusion_matrix(model_dt, X_dt_test, y_dt_test, normalize = \"all\")","51774a06":"#Confusion Matrix: Compare the example as the truth vs. my predicted values with the DecisionTreeClassifier model\n#from sklearn.metrics import confusion_matrix\n\ny_example_true = df_example['Survived']\ny_example_vs_dt_pred = y_dt_test\n\nexample_vs_dt_test_cm = confusion_matrix(y_example_true, y_example_vs_dt_pred, normalize = \"all\")\nexample_vs_dt_test_score = accuracy_score(y_example_true, y_example_vs_dt_pred, normalize = \"all\")\n\nprint(example_vs_dt_test_cm)\nprint('DT test score: ' + str(example_vs_dt_test_score))","05302296":"#Model RandomForestClassifier: Predicting the test set with the fitted RandomForestClassifier model\n\nX_rf_test = pd.get_dummies(df_test[features_rf])\n\ny_rf_test = model_rf.predict(X_rf_test)\n\n#Add new column with the values predicted with the RandomForestClassifier model\ndf_test['Survived_rf_test']=y_rf_test\n\n#print(model_rf.score(X_rf_test, y_rf_test))","504c506d":"plot_confusion_matrix(model_rf, X_rf_test, y_rf_test, normalize = \"all\")","b7dc8e41":"#Confusion Matrix: Compare the example as the truth vs. my predicted values with the RandomForestClassifier model\n#from sklearn.metrics import confusion_matrix\n\ny_example_true = df_example['Survived']\ny_example_vs_rf_pred = y_rf_test\n\nexample_vs_rf_test_cm = confusion_matrix(y_example_true, y_example_vs_rf_pred, normalize = \"all\")\nexample_vs_rf_test_score = accuracy_score(y_example_true, y_example_vs_rf_pred, normalize = \"all\")\n\nprint(example_vs_rf_test_cm)\nprint('RF test score: ' + str(example_vs_rf_test_score))","860513d9":"X_knn_test = df_test[['Sex_ord', 'SibORParch', 'Fare', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\n\nsc = StandardScaler()\nsc.fit(X_knn_test)\nX_knn_test_scaled = sc.transform(X_knn_test)\n\ny_knn_test = model_knn.predict(X_knn_test_scaled)\n\ndf_test['Survived_knn_test']=y_knn_test\n\n#print(model_knn.score(X_knn_test_scaled, y_knn_test)","ca4b6e6b":"plot_confusion_matrix(model_knn, X_knn_test_scaled, y_knn_test, normalize = \"all\")","aa403a13":"#Confusion Matrix: Compare the example as the truth vs. my predicted values with the KNN model\n\ny_example_true = df_example['Survived']\ny_example_vs_knn_pred = y_knn_test\n\nexample_vs_knn_test_cm = confusion_matrix(y_example_true, y_example_vs_knn_pred, normalize = \"all\")\nexample_vs_knn_test_score = accuracy_score(y_example_true, y_example_vs_knn_pred, normalize = \"all\")\n\nprint(example_vs_knn_test_cm)\nprint('KNN test score: ' + str(example_vs_knn_test_score))","be10d304":"X_lgr_test = df_test[['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\n\ny_lgr_test = model_lgr.predict(X_lgr_test)\ndf_test['Survived_lgr_test']=y_lgr_test\n\n#print(model_lgr.score(X_lgr_test_transformed, y_lgr_test))","2d39368c":"plot_confusion_matrix(model_lgr, X_lgr_test, y_lgr_test, normalize = \"all\")","a78bc1a9":"#Confusion Matrix: Compare the example as the truth vs. my predicted values with the Logistic Regression model\n#from sklearn.metrics import confusion_matrix\n\ny_example_true = df_example['Survived']\ny_example_vs_lgr_pred = y_lgr_test\n\nexample_vs_lgr_test_cm = confusion_matrix(y_example_true, y_example_vs_lgr_pred, normalize = \"all\")\nexample_vs_lgr_test_score = accuracy_score(y_example_true, y_example_vs_lgr_pred, normalize = \"all\")\n\nprint(example_vs_lgr_test_cm)\nprint('LgR test score: ' + str(example_vs_lgr_test_score))","341783b1":"X_pca_lgr_test = df_test[['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\n\nX_pca_lgr_test_transformed = pca.transform(X_pca_lgr_test)\n\ny_pca_lgr_test = model_pca_lgr.predict(X_pca_lgr_test_transformed)\ndf_test['Survived_pca_lgr_test']=y_pca_lgr_test\n\n#print(model_pca_lgr.score(X_pca_lgr_test_transformed, y_pca_lgr_test))","af3d425b":"plot_confusion_matrix(model_pca_lgr, X_pca_lgr_test_transformed, y_pca_lgr_test, normalize = \"all\")","c43c569e":"#Confusion Matrix: Compare the example as the truth vs. my predicted values with the PCA Logistic Regression model\n#from sklearn.metrics import confusion_matrix\n\ny_example_true = df_example['Survived']\ny_example_vs_pca_lgr_pred = y_pca_lgr_test\n\nexample_vs_pca_lgr_test_cm = confusion_matrix(y_example_true, y_example_vs_pca_lgr_pred, normalize = \"all\")\nexample_vs_pca_lgr_test_score = accuracy_score(y_example_true, y_example_vs_pca_lgr_pred, normalize = \"all\")\n\nprint(example_vs_pca_lgr_test_cm)\nprint('PCA LgR test score: ' + str(example_vs_pca_lgr_test_score))","47127088":"#Model OLS: Predicting the test set with the fitted OLS model\n\n#import statsmodels.api as sm\n\nX_ols_test = df_test[['Fare', 'SibORParch', 'Sex_ord', 'Pclass', 'Embk_Cherbourg', 'Embk_Queenstown', 'Embk_Southampton', 'Age_Infant', 'Age_Kid', 'Age_Young', 'Age_Adult', 'Age_Elderly' ]]\nX1_ols_test = sm.add_constant(X_ols_test)\n\ny_ols_test =  model_ols.predict(X1_ols_test)\ny_ols_test =  round(y_ols_test)\n\n#Add new column with the values predicted with the OLS model\ndf_test['Survived_ols_test']=y_ols_test","bf1040f3":"y_example_true = df_example['Survived']\ny_example_vs_ols_pred = y_ols_test\n\nexample_vs_ols_test_cm = confusion_matrix(y_example_true, y_example_vs_ols_pred, normalize = \"all\")\nexample_vs_ols_test_score = accuracy_score(y_example_true, y_example_vs_ols_pred, normalize = \"all\")\n\nprint(example_vs_ols_test_cm)\nprint('OLS test score: ' + str(example_vs_ols_test_score))","ecce8c14":"#Accuracy\n#from sklearn.metrics import accuracy_score\n\nA_example_vs_linsvc = accuracy_score(y_example_true, y_example_vs_linsvc_pred)\nA_example_vs_rbfsvc = accuracy_score(y_example_true, y_example_vs_rbfsvc_pred)\nA_example_vs_dt = accuracy_score(y_example_true, y_example_vs_dt_pred)\nA_example_vs_rf = accuracy_score(y_example_true, y_example_vs_rf_pred)\nA_example_vs_knn = accuracy_score(y_example_true, y_example_vs_knn_pred)\nA_example_vs_lgr = accuracy_score(y_example_true, y_example_vs_lgr_pred)\nA_example_vs_pca_lgr = accuracy_score(y_example_true, y_example_vs_pca_lgr_pred)\nA_example_vs_ols = accuracy_score(y_example_true, y_example_vs_ols_pred)\n\nprint(\"Accuracy example vs. LinSVC = \" + str(A_example_vs_linsvc))\nprint(\"Accuracy example vs. rbfSVC = \" + str(A_example_vs_rbfsvc))\nprint(\"Accuracy example vs. DT = \" + str(A_example_vs_dt))\nprint(\"Accuracy example vs. RF = \" + str(A_example_vs_rf))\nprint(\"Accuracy example vs. KNN = \" + str(A_example_vs_knn))\nprint(\"Accuracy example vs. LgR = \" + str(A_example_vs_lgr))\nprint(\"Accuracy example vs. PCA LgR = \" + str(A_example_vs_pca_lgr))\nprint(\"Accuracy example vs. OLS = \" + str(A_example_vs_ols))","fa3d4795":"#Recall\n#from sklearn.metrics import recall_score\n\nR_example_vs_linsvc = recall_score(y_example_true, y_example_vs_linsvc_pred)\nR_example_vs_rbfsvc = recall_score(y_example_true, y_example_vs_rbfsvc_pred)\nR_example_vs_dt = recall_score(y_example_true, y_example_vs_dt_pred)\nR_example_vs_rf = recall_score(y_example_true, y_example_vs_rf_pred)\nR_example_vs_knn = recall_score(y_example_true, y_example_vs_knn_pred)\nR_example_vs_lgr = recall_score(y_example_true, y_example_vs_lgr_pred)\nR_example_vs_pca_lgr = recall_score(y_example_true, y_example_vs_pca_lgr_pred)\nR_example_vs_ols = recall_score(y_example_true, y_example_vs_ols_pred)\n\nprint(\"Recall example vs. LinSVC = \" + str(R_example_vs_linsvc))\nprint(\"Recall example vs. rbfSVC = \" + str(R_example_vs_rbfsvc))\nprint(\"Recall example vs. DT = \" + str(R_example_vs_dt))\nprint(\"Recall example vs. RF = \" + str(R_example_vs_rf))\nprint(\"Recall example vs. KNN = \" + str(R_example_vs_knn))\nprint(\"Recall example vs. LgR = \" + str(R_example_vs_lgr))\nprint(\"Recall example vs. PCA LgR = \" + str(R_example_vs_pca_lgr))\nprint(\"Recall example vs. OLS = \" + str(R_example_vs_ols))","a0be3dd5":"#Precision\n#from sklearn.metrics import precision_score\n\nP_example_vs_linsvc = precision_score(y_example_true, y_example_vs_linsvc_pred)\nP_example_vs_rbfsvc = precision_score(y_example_true, y_example_vs_rbfsvc_pred)\nP_example_vs_dt = precision_score(y_example_true, y_example_vs_dt_pred)\nP_example_vs_rf = precision_score(y_example_true, y_example_vs_rf_pred)\nP_example_vs_knn = precision_score(y_example_true, y_example_vs_knn_pred)\nP_example_vs_lgr = precision_score(y_example_true, y_example_vs_lgr_pred)\nP_example_vs_pca_lgr = precision_score(y_example_true, y_example_vs_pca_lgr_pred)\nP_example_vs_ols = precision_score(y_example_true, y_example_vs_ols_pred)\n\nprint(\"Precision example vs. LinSVC = \" + str(P_example_vs_linsvc))\nprint(\"Precision example vs. rbfSVC = \" + str(P_example_vs_rbfsvc))\nprint(\"Precision example vs. DT = \" + str(P_example_vs_dt))\nprint(\"Precision example vs. RF = \" + str(P_example_vs_rf))\nprint(\"Precision example vs. KNN = \" + str(P_example_vs_knn))\nprint(\"Precision example vs. LgR = \" + str(P_example_vs_lgr))\nprint(\"Precision example vs. PCA LgR = \" + str(P_example_vs_pca_lgr))\nprint(\"Precision example vs. OLS = \" + str(P_example_vs_ols))","d683fcf5":"relevant_metrics_pred = pd.DataFrame({\n    'Model': ['LinSVC', 'RBFSVC', 'Decision Tree', 'Random Forest', 'K-Nearest-Neighbours', 'Logistic Regression', 'PCA Logistic Regression', 'OLS'],\n    'Accuracy, A': [A_example_vs_linsvc, A_example_vs_rbfsvc, A_example_vs_dt, A_example_vs_rf, A_example_vs_knn, A_example_vs_lgr, A_example_vs_pca_lgr, A_example_vs_ols],\n    'Recall, R': [R_example_vs_linsvc, R_example_vs_rbfsvc, R_example_vs_dt, R_example_vs_rf, R_example_vs_knn, R_example_vs_lgr, R_example_vs_pca_lgr, R_example_vs_ols],\n    'Precision, P': [P_example_vs_linsvc, P_example_vs_rbfsvc, P_example_vs_dt, P_example_vs_rf, P_example_vs_knn, P_example_vs_lgr, P_example_vs_pca_lgr, P_example_vs_ols]})\nbest_model_pred =relevant_metrics_pred.sort_values(by='Accuracy, A', ascending=False)\nbest_model_pred","a8e9c360":"d = {}\nd['PassengerId']=df_test['PassengerId']\nd['Survived']=df_test['Survived_rbfsvc_test']\n\ndf_rbfsvc_submission = pd.DataFrame(d)\n\ndf_rbfsvc_submission.to_csv (r'titanic_data_submission_RBFSVC_new.csv', index = False, header=True)","6d1495e8":"### <a id='valiPCA-LgR'>4.7.1. Validating the PCA LgR<\/a>","66addf8a":"### <a id='valiLinSVC'>4.1.3. Validating the LinSVC<\/a>","fcefac53":"## <a id='predsummary'>5.10. Prediction summary<\/a>","75120eaf":"## <a id='norm'>2.3. Normalising: Fare, Age, Pclass<\/a>","b44536c4":"### <a id='fitDTparam'>4.3.2. Fitting the DT, GridSearchCV parameters<\/a>","07c71f80":"**<center><font size=6>Titanic - Machine Learning from Disaster<\/font><\/center>**\n***\n\n**date**: 07.01.2021\n\n**Table of Contents**\n- <a href='#read'>1. Reading the data<\/a> \n- <a href='#understand'>2. Understanding and preparing the data<\/a>\n    - <a href='#describe'>2.1. Describing and planning the data<\/a>\n    - <a href='#group'>2.2. Grouping and transforming<\/a>\n    - <a href='#norm'>2.3. Normalising<\/a>\n- <a href='#split'>3. Splitting the data<\/a>\n- <a href='#fit'>4. Fitting and validating the models<\/a>\n    - <a href='#fitLinSVC'>4.1. Fitting the Linear SupportVectorClassifier (LinSVC) and GridSearchCV<\/a>\n        - <a href='#fitLinSVCcgscv'>4.1.1. LinSVC and GridSearchCV<\/a>\n        - <a href='#fitLinSVCparam'>4.1.2. Fitting the LinSVC, GridSearchCV parameters<\/a>\n        - <a href='#valiLinSVC'>4.1.3. Validating the LinSVC<\/a>\n    - <a href='#fitRBFSVC'>4.2. Fitting the RBF SupportVectorClassifier (RBF SVC) and GridSearchCV<\/a>\n        - <a href='#fitRBFSVCgscv'>4.2.1. RBF SVC and GridSearchCV<\/a>\n        - <a href='#fitRBFSVCparam'>4.2.2. Fitting the RBF SVC, GridSearchCV parameters<\/a>\n        - <a href='#valiRBFSVC'>4.2.3. Validating the RBF SVC<\/a>\n    - <a href='#fitDT'>4.3. Fitting the DecisionTreeClassifier (DT) and GridSearchCV<\/a>\n        - <a href='#fitDTgscv'>4.3.1. DT and GridSearchCV<\/a>\n        - <a href='#fitDTparam'>4.3.2. Fitting the DT, GridSearchCV parameters<\/a>\n        - <a href='#valiDT'>4.3.3. Validating the DT<\/a>\n    - <a href='#fitRF'>4.4. Fitting the RandomForestClassifier (RF)<\/a>\n        - <a href='#fitRFgscv'>4.4.1. RF and GridSearchCV<\/a>\n        - <a href='#fitRFparam'>4.4.2. Fitting the RF, GridSearchCV parameters<\/a>\n        - <a href='#valiRF'>4.4.3. Validating the RF<\/a>\n    - <a href='#fitKNN'>4.5. Fitting the K-Nearest-Neighbours (KNN)<\/a>  \n        - <a href='#fitKNNgscv'>4.5.1. KNN and GridSearchCV<\/a>\n        - <a href='#fitKNNparam'>4.5.2. Fitting the KNN, GridSearchCV parameters<\/a>\n        - <a href='#valiKNN'>4.5.3. Validating the KNN<\/a> \n    - <a href='#fitLgR'>4.6. Fitting the Logistic Regression (LgR)<\/a>\n        - <a href='#valiLgR'>4.6.1. Validating the LgR<\/a>    \n    - <a href='#fitPCA-LgR'>4.7. Fitting the Principal Component Analysis (PCA) and Logistic Regression (LgR)<\/a>\n        - <a href='#valiPCA-LgR'>4.7.1. Validating the PCA LgR<\/a> \n    - <a href='#fitOLS'>4.8. Fitting the Ordinary Least Squares Linear Regression (OLS)<\/a>\n        - <a href='#valiOLS'>4.8.1. Validating the OLS<\/a> \n    - <a href='#valiARP'>4.9. Accuracy, Recall, Precision: validation set truth vs. predicted values<\/a>\n    - <a href='#fitsummary'>4.10. Fitting and validating summary<\/a>\n- <a href='#perd'>5. Predicting<\/a>\n    - <a href='#predlinsvc'>5.1. Predicting with LinSVC<\/a>\n    - <a href='#predrbfsvc'>5.2. Predicting with RBF SVC<\/a>\n    - <a href='#predRF'>5.3. Predicting with DT<\/a>\n    - <a href='#predRF'>5.4. Predicting with RF<\/a>\n    - <a href='#predKNN'>5.5. Predicting with KNN<\/a>\n    - <a href='#predLgR'>5.6. Predicting with LgR<\/a>\n    - <a href='#predPCA-LgR'>5.7. Predicting with PCA and LgR<\/a>\n    - <a href='#predOLS'>5.8. Predicting with OLS<\/a>\n    - <a href='#predARP'>5.9. Accuracy, Recall, Precision: example set truth vs. test set predictions<\/a>\n    - <a href='#predsummary'>5.10. Prediction summary<\/a>\n- <a href='#submit'>6. Submitting the data<\/a>","b9abd849":"## <a id='predDT'>5.3. Predicting with DT<\/a>","9a4795f0":"| Variable orig | not missing values | Type | Ranges\/Values | Describing | Preparing\/Transforming | New Variable | New Ranges\/New Values |\n| :- | --- | :- | :- | :- | :- | :- | :- |\n| PassengerId | 891 | int64 | 1-891 | | | | \n| Survived | 891 | int64 | 0\/1 | 0=died, 1=survived | | | \n| Pclass | 891 | int64 | 1\/2\/3 | 1=1st class, 2=2nd class, 3=3rd class | normalising | Pclass_norm| 0-1 | \n| Name | 891 | object | text | | | | \n| Sex | 891 | object | female\/male | female=0, male=1 | categories as numbers | Sex_ord | 0\/1 |\n| Age | 714 | float64 | 0.42-80 Years | | grouping, than splitting age-groups in 5 variables | Age_Group; Age_Infant; Age_Kid; Age_Young; Age_Adults; Age_Elderly; Age_norm | Infant(0-5)\/Kid(5-14)\/Young(14-25)\/Adult(25-55)\/Elderly(55-80); 0\/1; 0\/1; 0\/1; 0\/1; 0\/1; 0-1 |\n| SibSp | 891 | int64 | 0-8 | Sibling = brother, sister, stepbrother, stepsister \/ Spouse = husband, wife | as 0=alone, 1=not alone| SibORParch | 0\/1 |\n| Parch | 891 | int64 | 0-6 | Parent = mother, father \/ Child = daughter, son, stepdaughter, stepson |as 0=alone, 1=not alone| SibORParch | 0\/1 |\n| Ticket | 891 | object | Letters and Numbers | | | |\n| Fare | 891 | float64 | 0-512.3292 Dolars | | normalising | Fare_norm | 0-1 | \n| Cabin | 204 | object | Letters and Numbers | | | |\n| Embarked | 889 | object | C\/Q\/S | C = Cherbourg, Q = Queenstown, S = Southampton | categories as numbers (C=0, Q=1, S=2) and than also as 3 single variables 0\/1 | Embarked_ord; Embk_Cherbourg; Embk_Queenstown; Embk_Southampton | 0\/1\/2; 0\/1; 0\/1; 0\/1 |","26d48029":"# <a id='pred'>5. Predicting<\/a>","cfaad0ba":"## <a id='predRBFSVC'>5.2. Predicting with RBF SVC<\/a>","50681e01":"### <a id='fitLinSVCparam'>4.1.2. Fitting the LinSVC, GridSearchCV parameters<\/a>","39f5a7f4":"### <a id='valiLgR'>4.6.1. Validating the LgR<\/a>","d5a33fea":"### <a id='fitDTgscv'>4.3.1. DT and GridSearchCV<\/a>","888cca54":"# <a id='submit'>6. Submitting the data<\/a>","f21df199":"## <a id='fitPCA-LgR'>4.7. Fitting the Principal Component Analysis and Logistic Regression (PCA LgR)<\/a>","f2309637":"## <a id='predLgR'>5.6. Predicting with LgR<\/a>","8da507ef":"## <a id='valiARP'>4.9. Accuracy, Recall, Precision: validation set truth vs. validation set predictions<\/a>","96b22245":"## <a id='describe'>2.1. Describing and planning the data<\/a>","b6f12539":"### <a id='valiRBFSVC'>4.2.3. Validating the RBF SVM<\/a>","cb463f51":"### <a id='fitRBFSVCparam'>4.2.2. Fitting the RBF SVM, GridSearchCV parameters<\/a>","d2213cd3":"# <a id='fit'>4. Fitting and validating the models<\/a>","4d0fa2c5":"## <a id='fitLinSVC'>4.1. Fitting the Linear SupportVectorClassifier (LinSVC) and GridSearchCV<\/a>","79d8e994":"### <a id='valiKNN'>4.5.3. Validating the KNN<\/a>","9fbb1404":"## <a id='fitsummary'>4.10. Fitting and validating summary<\/a>","c9f2aaf6":"## <a id='fitLgR'>4.6. Fitting the Logistic Regression (LgR)<\/a>","1db9dc66":"# <a id='understand'>2. Understanding and preparing the data<\/a>","d4a76327":"### <a id='fitRBFSVCgscv'>4.2.1. RBF SVM and GridSearchCV<\/a>","18ad1cb3":"# <a id='split'>3. Splitting the data<\/a>","96cc561d":"### <a id='valiRF'>4.4.3. Validating the RF<\/a>","9c30d79e":"### <a id='valiDT'>4.3.3. Validating the DT<\/a>","8fb4e76a":"## <a id='fitRBFSVC'>4.2. Fitting the RBF SVM (RBF SVM) and GridSearchCV<\/a>","edc730bf":"### <a id='fitLinSVCgscv'>4.1.1. LinSVC and GridSearchCV<\/a>","de2ca239":"### <a id='fitRFparam'>4.4.2. Fitting the RF, GridSearchCV parameters<\/a>","a488f4c5":"## <a id='predPCA-LgR'>5.7. Predicting with PCA - LgR<\/a>","d5ff7afd":"## <a id='fitKNN'>4.5. Fitting the K-Nearest-Neighbours (KNN)<\/a>","96864df9":"## <a id='predRF'>5.4. Predicting with RF<\/a>","de838b71":"### <a id='fitKNNparam'>4.5.2. Fitting the KNN, GridSearchCV parameters<\/a>","b100ec1a":"## <a id='fitDT'>4.3. Fitting the DecisionTreeClassifier (DT) and GridSearchCV<\/a>","ff29cb7c":"## <a id='predLlinSVC'>5.1. Predicting with LinSVC<\/a>","0749356b":"## <a id='predARP'>5.9. Accuracy, Recall, Precision: example settruth vs. test set predictions<\/a>","884aa4e7":"## <a id='group'>2.2. Grouping and transforming: Age, SibSp, Parch, Sex, Embarked<\/a>","4b1e0231":"## <a id='fitRF'>4.4. Fitting the RandomForestClassifier (RF) and GridSearchCV<\/a>","765d49e2":"Splitting the data in training and validation sets in new csv files.\n\n| Kaggle set | | Splitted sets |need for | | | | | \n| :- | --- | :- | :- | :- | :- | :- | :- |\n| train set | 80% | training set | fitting the model| | | \n| train set | 20% | validation set | validating the model| | | |  \n| test set | 100%| |predicting the data| | | | |\n| | | | | | | | ","9f44d6f1":"### <a id='fitRFgscv'>4.4.1. RF and GridSearchCV<\/a>","0abd4cc5":"### <a id='valiOLS'>4.8.1. Validating the OLS<\/a>","2af6a888":"## <a id='predOLS'>5.8. Predicting with OLS<\/a>","778d2514":"# <a id='read'>1. Reading the data<\/a>","4bd981d4":"## <a id='fitOLS'>4.8. Fitting the Ordinary Least Squares (OLS)<\/a>","231d0ece":"## <a id='predKNN'>5.5. Predicting with KNN<\/a>","3ca1238d":"### <a id='fitKNNparam'>4.5.1. KNN and GridSearchCV<\/a>"}}