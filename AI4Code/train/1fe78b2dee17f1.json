{"cell_type":{"3471f74c":"code","9b7c70d8":"code","7e0aeaf7":"code","d730b5d0":"code","cd158886":"code","88a2a7b4":"code","eafd9945":"code","2024befa":"code","035b2360":"code","a3cb6683":"code","7ad15280":"code","ad549469":"code","b73c8240":"code","f3019631":"code","8721e3dd":"code","5ca92f20":"code","10744dcd":"code","8b15a3de":"code","026cc21c":"code","8e446f24":"code","0472766d":"code","0a30e8ae":"code","c221e789":"code","7652ae43":"code","55ab7a3f":"code","ea4d6879":"markdown"},"source":{"3471f74c":"from tensorflow.keras.optimizers import Nadam\nfrom sklearn.metrics import mean_squared_error\nimport tensorflow as tf\nimport tensorflow.keras.layers as KL\nfrom datetime import timedelta\nimport numpy as np\nimport pandas as pd\nimport tensorflow.keras.backend as K\n\nimport os\npd.options.display.max_rows = 500\npd.options.display.max_columns = 500\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom scipy.signal import savgol_filter\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.metrics import mean_squared_error, mean_squared_log_error\nfrom sklearn.linear_model import LinearRegression, Ridge\n\nimport datetime\nimport gc\nfrom tqdm import tqdm\n\nimport xgboost as xgb\n\nimport lightgbm as lgb\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndef rmse( yt, yp ):\n    return np.sqrt( np.mean( (yt-yp)**2 ) )\n\nclass CovidModel:\n    def __init__(self):\n        pass\n    \n    def predict_first_day(self, date):\n        return None\n    \n    def predict_next_day(self, yesterday_pred_df):\n        return None\n\n\nclass CovidModelAhmet(CovidModel):\n    def preprocess(self, df, meta_df):\n        df[\"Date\"] = pd.to_datetime(df['Date'])\n\n        df = df.merge(meta_df, on=self.loc_group, how=\"left\")\n        df[\"lat\"] = (df[\"lat\"] \/\/ 30).astype(np.float32).fillna(0)\n        df[\"lon\"] = (df[\"lon\"] \/\/ 60).astype(np.float32).fillna(0)\n\n        df[\"population\"] = np.log1p(df[\"population\"]).fillna(-1)\n        df[\"area\"] = np.log1p(df[\"area\"]).fillna(-1)\n\n        for col in self.loc_group:\n            df[col].fillna(\"\", inplace=True)\n            \n        df['day'] = df.Date.dt.dayofyear\n        df['geo'] = ['_'.join(x) for x in zip(df['Country_Region'], df['Province_State'])]\n        return df\n\n    def get_model(self):\n        \n        def nn_block(input_layer, size, dropout_rate, activation):\n            out_layer = KL.Dense(size, activation=None)(input_layer)\n            out_layer = KL.Activation(activation)(out_layer)\n            out_layer = KL.Dropout(dropout_rate)(out_layer)\n            return out_layer\n    \n        ts_inp = KL.Input(shape=(len(self.ts_features),))\n        global_inp = KL.Input(shape=(len(self.global_features),))\n\n        inp = KL.concatenate([global_inp, ts_inp])\n        hidden_layer = nn_block(inp, 64, 0.0, \"relu\")\n        gate_layer = nn_block(hidden_layer, 32, 0.0, \"sigmoid\")\n        hidden_layer = nn_block(hidden_layer, 32, 0.0, \"relu\")\n        hidden_layer = KL.multiply([hidden_layer, gate_layer])\n\n        out = KL.Dense(len(self.TARGETS), activation=\"linear\")(hidden_layer)\n\n        model = tf.keras.models.Model(inputs=[global_inp, ts_inp], outputs=out)\n        return model\n    \n    def get_input(self, df):\n        return [df[self.global_features], df[self.ts_features]]\n        \n    def train_models(self, df, num_models=20, save=False):\n        \n        def custom_loss(y_true, y_pred):\n            return K.sum(K.sqrt(K.sum(K.square(y_true - y_pred), axis=0, keepdims=True)))\/len(self.TARGETS)\n    \n        models = []\n        for i in range(num_models):\n            model = self.get_model()\n            model.compile(loss=custom_loss, optimizer=Nadam(lr=1e-4))\n            hist = model.fit(self.get_input(df), df[self.TARGETS],\n                             batch_size=2048, epochs=200, verbose=0, shuffle=True)\n            if save:\n                model.save_weights(\"model{}.h5\".format(i))\n            models.append(model)\n        return models\n    \n    \n    def predict_one(self, df):\n        \n        pred = np.zeros((df.shape[0], 2))\n        for model in self.models:\n            pred += model.predict(self.get_input(df))\/len(self.models)\n        pred = np.maximum(pred, df[self.prev_targets].values)\n        pred[:, 0] = np.log1p(np.expm1(pred[:, 0]) + 0.1)\n        pred[:, 1] = np.log1p(np.expm1(pred[:, 1]) + 0.01)\n        return np.clip(pred, None, 15)\n    \n\n    def __init__(self):\n        df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\n        sub_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\n\n        meta_df = pd.read_csv(\"..\/input\/covid19-forecasting-metadata\/region_metadata.csv\")\n\n        self.loc_group = [\"Province_State\", \"Country_Region\"]\n\n        df = self.preprocess(df, meta_df)\n        sub_df = self.preprocess(sub_df, meta_df)\n        \n        df = df.merge(sub_df[[\"ForecastId\", \"Date\", \"geo\"]], how=\"left\", on=[\"Date\", \"geo\"])\n        df = df.append(sub_df[sub_df[\"Date\"] > df[\"Date\"].max()], sort=False)\n        \n        df[\"day\"] = df[\"day\"] - df[\"day\"].min()\n\n        self.TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n        self.prev_targets = ['prev_ConfirmedCases_1', 'prev_Fatalities_1']\n\n        for col in self.TARGETS:\n            df[col] = np.log1p(df[col])\n\n        self.NUM_SHIFT = 7\n\n        self.global_features = [\"lat\", \"lon\", \"population\", \"area\"]\n        self.ts_features = []\n\n        for s in range(1, self.NUM_SHIFT+1):\n            for col in self.TARGETS:\n                df[\"prev_{}_{}\".format(col, s)] = df.groupby(self.loc_group)[col].shift(s)\n                self.ts_features.append(\"prev_{}_{}\".format(col, s))\n\n        self.df = df[df[\"Date\"] >= df[\"Date\"].min() + timedelta(days=self.NUM_SHIFT)].copy()\n\n        \n    def predict_first_day(self, day):\n        self.models = self.train_models(self.df[self.df[\"day\"] < day])\n        \n        temp_df = self.df.loc[self.df[\"day\"] == day].copy()\n        y_pred = self.predict_one(temp_df)\n            \n        self.y_prevs = [None]*self.NUM_SHIFT\n\n        for i in range(1, self.NUM_SHIFT):\n            self.y_prevs[i] = temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]].values\n            \n        temp_df[self.TARGETS] = y_pred\n        self.day = day\n        return temp_df[[\"geo\", \"day\"] + self.TARGETS]\n    \n    \n    def predict_next_day(self, yesterday_pred_df):\n        self.day = self.day + 1\n\n        temp_df = self.df.loc[self.df[\"day\"] == self.day].copy()\n        \n        yesterday_pred_df = temp_df[[\"geo\"]].merge(yesterday_pred_df[[\"geo\"] + self.TARGETS], on=\"geo\", how=\"left\")\n        temp_df[self.prev_targets] = yesterday_pred_df[self.TARGETS].values\n\n        for i in range(2, self.NUM_SHIFT+1):\n            temp_df[['prev_ConfirmedCases_{}'.format(i), 'prev_Fatalities_{}'.format(i)]] = self.y_prevs[i-1]\n\n        y_pred, self.y_prevs = self.predict_one(temp_df), [None, temp_df[self.prev_targets].values] + self.y_prevs[1:-1]\n\n        temp_df[self.TARGETS] = y_pred\n        return temp_df[[\"geo\", \"day\"] + self.TARGETS]\n","9b7c70d8":"class CovidModelCPMP(CovidModel):\n    \n    def __init__(self):\n        train = pd.read_csv('..\/input\/covid19-global-forecasting-week-4\/train.csv')\n        train['Province_State'].fillna('', inplace=True)\n        train['Date'] = pd.to_datetime(train['Date'])\n        train['day'] = train.Date.dt.dayofyear\n        train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n        test = pd.read_csv('..\/input\/covid19-global-forecasting-week-4\/test.csv')\n        test['Province_State'].fillna('', inplace=True)\n        test['Date'] = pd.to_datetime(test['Date'])\n        test['day'] = test.Date.dt.dayofyear\n        test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n        day_min = train['day'].min()\n        train['day'] -= day_min\n        test['day'] -= day_min  \n        self.min_test_val_day = test.day.min()\n        self.max_test_val_day = train.day.max()\n        self.max_test_day = test.day.max()\n\n        train['ForecastId'] = -1\n        test['Id'] = -1\n        test['ConfirmedCases'] = 0\n        test['Fatalities'] = 0    \n        data = pd.concat([train,\n                  test[test.day > self.max_test_val_day][train.columns]\n                 ]).reset_index(drop=True)\n        self.data = data\n        self.train = train\n        self.test = test\n        self.dates = data[data['geo'] == 'France_'].Date.values\n        region_meta = pd.read_csv('..\/input\/covid19-forecasting-metadata\/region_metadata.csv')\n        region_meta['Province_State'].fillna('', inplace=True)\n        region_meta['geo'] = ['_'.join(x) for x in zip(region_meta['Country_Region'], region_meta['Province_State'], )]\n        population = data[['geo']].merge(region_meta, how='left', on='geo').fillna(0)\n        population = population.groupby('geo')[['population']].first()\n        population['population'] = np.log1p(population['population'])\n        self.population = population[['population']].values\n        continents = region_meta['continent']\n        continents = pd.factorize(continents)[0]\n        continents_ids_base = continents.reshape((-1, 1))\n        ohe = OneHotEncoder(sparse=False)\n        self.continents_ids_base = ohe.fit_transform(continents_ids_base)\n        \n        self.geo_data = data.pivot(index='geo', columns='day', values='ForecastId')\n        self.num_geo = self.geo_data.shape[0]\n        self.ConfirmedCases = data.pivot(index='geo', columns='day', values='ConfirmedCases')\n        self.Fatalities = data.pivot(index='geo', columns='day', values='Fatalities')\n        self.cases = np.log1p(self.ConfirmedCases.values)\n        self.deaths = np.log1p(self.Fatalities.values)\n        self.case_threshold = 30\n        \n        self.c_case = 10\n        self.t_case = 100\n        self.c_death = 10\n        self.t_death = 5\n\n        time_cases = self.c_case * (self.cases >= np.log1p(self.t_case)) \n        time_cases = np.cumsum(time_cases, axis=1)\n        self.time_cases = 1 * np.log1p(time_cases) \n\n        time_deaths = self.c_death * (self.deaths >= np.log1p(self.t_death))\n        time_deaths = np.cumsum(time_deaths, axis=1)\n        self.time_deaths = 1 *np.log1p(time_deaths) \n\n        countries = [g.split('_')[0] for g in self.geo_data.index]\n        countries = pd.factorize(countries)[0]\n        country_ids_base = countries.reshape((-1, 1))\n        ohe = OneHotEncoder(sparse=False)\n        self.country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n\n        self.start_lag_death = 13\n        self.end_lag_death = 5\n        self.num_train = 5\n        self.num_lag_case = 14\n        self.lag_period = max(self.start_lag_death, self.num_lag_case)\n        \n        # For tetsing purpose       \n        self.df = train[['geo', 'day', 'ConfirmedCases', 'Fatalities']].copy()\n        self.df.ConfirmedCases = np.log1p(self.df.ConfirmedCases)\n        self.df.Fatalities = np.log1p(self.df.Fatalities)\n        \n    def get_country_ids(self):\n        countries = [g.split('_')[0] for g in self.geo_data.index]\n        countries = pd.factorize(countries)[0]\n        countries[self.cases[:, :self.last_train+1].max(axis=1) < np.log1p(self.case_threshold)] = -1\n        countries = pd.factorize(countries)[0]\n\n\n        country_ids_base = countries.reshape((-1, 1))\n        ohe = OneHotEncoder(sparse=False)\n        country_ids_base = 0.2 * ohe.fit_transform(country_ids_base)\n        return country_ids_base\n    \n    def val_score(self, true, pred):\n        return np.sqrt(mean_squared_error(true.ravel(), pred.ravel()))\n    \n    def get_dataset(self, start_pred, num_train):\n        days = np.arange( start_pred - num_train + 1, start_pred + 1)\n        lag_cases = np.vstack([self.cases[:, d - self.lag_period : d] for d in days])\n        lag_deaths = np.vstack([self.deaths[:, d - self.lag_period : d] for d in days])\n        target_cases = np.vstack([self.cases[:, d : d + 1] for d in days])\n        target_deaths = np.vstack([self.deaths[:, d : d + 1] for d in days])\n        continents_ids = np.vstack([self.continents_ids_base for d in days])\n        country_ids = np.vstack([self.country_ids_base for d in days])\n        population = np.vstack([self.population for d in days])\n        time_case = np.vstack([self.time_cases[:, d - 1: d ] for d in days])\n        time_death = np.vstack([self.time_deaths[:, d - 1 : d ] for d in days])\n        return (lag_cases, lag_deaths, target_cases, target_deaths, \n            continents_ids, country_ids, population, time_case, time_death, days)\n    \n    def update_time(self, time_death, time_case, pred_death, pred_case):\n        new_time_death = np.expm1(time_death) + self.c_death * (pred_death >= np.log1p(self.t_death))\n        new_time_death = 1 *np.log1p(new_time_death) \n        new_time_case = np.expm1(time_case) + self.c_case * (pred_case >= np.log1p(self.t_case))\n        new_time_case = 1 *np.log1p(new_time_case) \n        return new_time_death, new_time_case\n\n    def update_valid_dataset(self, dataset, pred_death, pred_case, pred_day):\n        (lag_cases, lag_deaths, target_cases, target_deaths, \n         continents_ids, country_ids, population, time_case, time_death, days) = dataset\n        if pred_day != days[-1]:\n            print('error', pred_day, days[-1])\n            return None\n        day = days[-1] + 1\n        new_lag_cases = np.hstack([lag_cases[:, 1:], pred_case])\n        new_lag_deaths = np.hstack([lag_deaths[:, 1:], pred_death]) \n        new_target_cases = self.cases[:, day:day+1]\n        new_target_deaths = self.deaths[:, day:day+1] \n        new_continents_ids = continents_ids  \n        new_country_ids = country_ids  \n        new_population = population  \n        new_time_death, new_time_case = self.update_time(time_death, time_case, pred_death, pred_case)\n        new_days = 1 + days\n        return (new_lag_cases, new_lag_deaths, new_target_cases, new_target_deaths, \n            new_continents_ids, new_country_ids, new_population, \n                new_time_case, new_time_death, new_days)\n        \n    def fit_eval(self, dataset, fit):\n        (lag_cases, lag_deaths, target_cases, target_deaths, \n         continents_ids, country_ids, population, \n         time_case, time_death, days) = dataset\n\n        X_death = np.hstack([lag_cases[:, -self.start_lag_death:-self.end_lag_death], \n                             lag_deaths[:, -self.num_lag_case:], \n                             country_ids,\n                             continents_ids,\n                              population,\n                             time_case,\n                             time_death,\n                            ])\n        y_death = target_deaths\n        y_death_prev = lag_deaths[:, -1:]\n        if fit:\n             self.lr_death.fit(X_death, y_death)\n        y_pred_death = self.lr_death.predict(X_death)\n        y_pred_death = np.maximum(y_pred_death, y_death_prev)\n\n        X_case = np.hstack([lag_cases[:, -self.num_lag_case:], \n                            country_ids, \n                            continents_ids,\n                            population,\n                             time_case,\n                             #time_death,\n                           ])\n        y_case = target_cases\n        y_case_prev = lag_cases[:, -1:]\n        if fit:\n            self.lr_case.fit(X_case, y_case)\n        y_pred_case = self.lr_case.predict(X_case)\n        y_pred_case = np.maximum(y_pred_case, y_case_prev)\n\n        return y_pred_death, y_pred_case\n     \n    def get_pred_df(self, val_death_preds, val_case_preds, ):\n        pred_deaths = self.Fatalities.iloc[:, self.start_val:self.start_val+self.num_val].copy()\n        #pred_deaths.iloc[:, :] = np.expm1(val_death_preds)\n        pred_deaths.iloc[:, :] = val_death_preds\n        pred_deaths = pred_deaths.stack().reset_index()\n        pred_deaths.columns = ['geo', 'day', 'Fatalities']\n        pred_deaths\n\n        pred_cases = self.ConfirmedCases.iloc[:, self.start_val:self.start_val+self.num_val].copy()\n        #pred_cases.iloc[:, :] = np.expm1(val_case_preds)\n        pred_cases.iloc[:, :] = val_case_preds\n        pred_cases = pred_cases.stack().reset_index()\n        pred_cases.columns = ['geo', 'day', 'ConfirmedCases']\n        pred_cases\n\n        sub = self.data[['geo', 'day']]\n        sub = sub[sub.day == self.start_val]\n        sub = sub.merge(pred_cases, how='left', on=['geo', 'day'])\n        sub = sub.merge(pred_deaths, how='left', on=['geo', 'day'])\n        sub = sub[(sub.day >= self.start_val) & (sub.day <= self.end_val)]\n        return sub\n    \n    def predict_first_day(self, day):\n        self.start_val = day\n        self.end_val = day + 1\n        self.num_val = self.end_val - self.start_val + 1\n        score = True\n        self.last_train = self.start_val - 1\n        print(self.dates[self.last_train], self.start_val, self.num_val)\n        self.country_ids_base = self.get_country_ids()\n        train_data = self.get_dataset(self.last_train, self.num_train)\n        alpha = 3\n        self.lr_death = Ridge(alpha=alpha, fit_intercept=True)\n        self.lr_case = Ridge(alpha=alpha, fit_intercept=True)\n        _ = self.fit_eval(train_data, fit=True)\n        \n        self.valid_data = self.get_dataset(self.start_val, 1)\n        val_death_preds, val_case_preds = self.fit_eval(self.valid_data, fit=False)\n        df = self.get_pred_df(val_death_preds, val_case_preds)\n        return df\n    \n    def predict_next_day(self, yesterday_pred_df):\n        yesterday_pred_df = yesterday_pred_df.sort_values(by='geo').reset_index(drop=True)\n        if yesterday_pred_df.day.nunique() != 1:\n            print('error', yesterday_pred_df.day.unique())\n            return None\n        pred_death = yesterday_pred_df[['Fatalities']].values\n        pred_case = yesterday_pred_df[['ConfirmedCases']].values\n        pred_day = yesterday_pred_df.day.unique()[0]\n        \n        new_valid_data = self. update_valid_dataset(self.valid_data, \n                                                    pred_death, pred_case, pred_day)\n        if len(new_valid_data) > 0:\n            self.valid_data = new_valid_data\n        self.start_val = pred_day + 1\n        self.end_val = pred_day + 2\n        val_death_preds, val_case_preds = self.fit_eval(self.valid_data, fit=False)\n        df = self.get_pred_df(val_death_preds, val_case_preds)\n         \n        return df","7e0aeaf7":"class CovidModel:\n    def __init__(self):\n        pass\n    \n    def predict_first_day(self, date):\n        return None\n    \n    def predict_next_day(self, yesterday_pred_df):\n        return None\n    \n\nclass CovidModelGIBA(CovidModel):\n    def __init__(self, lag=1, seed=1 ):\n\n        self.lag  = lag\n        self.seed = seed\n        print( 'Lag:', lag, 'Seed:', seed )\n        \n        train = pd.read_csv('..\/input\/covid19-global-forecasting-week-4\/train.csv')\n        train['Date'] = pd.to_datetime( train['Date'] )\n        self.maxdate  = str(train['Date'].max())[:10]\n        self.testdate = str( train['Date'].max() + pd.Timedelta(days=1) )[:10]\n        print( 'Last Date in Train:',self.maxdate, 'Test first Date:',self.testdate )\n        train['Province_State'].fillna('', inplace=True)\n        train['day'] = train.Date.dt.dayofyear\n        self.day_min = train['day'].min()\n        train['day'] -= self.day_min\n        train['geo'] = ['_'.join(x) for x in zip(train['Country_Region'], train['Province_State'])]\n\n        test  = pd.read_csv('..\/input\/covid19-global-forecasting-week-4\/test.csv')\n        test['Date'] = pd.to_datetime( test['Date'] )\n        test['Province_State'].fillna('', inplace=True)\n        test['day'] = test.Date.dt.dayofyear\n        test['day'] -= self.day_min\n        test['geo'] = ['_'.join(x) for x in zip(test['Country_Region'], test['Province_State'])]\n        test['Id'] = -1\n        test['ConfirmedCases'] = 0\n        test['Fatalities'] = 0\n\n        self.trainmaxday  = train['day'].max()\n        self.testday1 = train['day'].max() + 1\n        self.testdayN = test['day'].max()\n        \n        publictest = test.loc[ test.Date > train.Date.max() ].copy()\n        train = pd.concat( (train, publictest ), sort=False )\n        train.sort_values( ['Country_Region','Province_State','Date'], inplace=True )\n        train = train.reset_index(drop=True)\n\n        train['ForecastId'] = pd.merge( train, test, on=['Country_Region','Province_State','Date'], how='left' )['ForecastId_y'].values\n\n        train['cid'] = train['Country_Region'] + '_' + train['Province_State']\n\n        train['log0'] = np.log1p( train['ConfirmedCases'] )\n        train['log1'] = np.log1p( train['Fatalities'] )\n\n        train = train.loc[ (train.log0 > 0) | (train.ForecastId.notnull()) | (train.Date >= '2020-03-17') ].copy()\n        train = train.reset_index(drop=True)\n\n        train['days_since_1case'] = train.groupby('cid')['Id'].cumcount()\n\n        dt = pd.read_csv('..\/input\/covid19-lockdown-dates-by-country\/countryLockdowndates.csv')\n        dt.columns = ['Country_Region','Province_State','Date','Type','Reference']\n        dt = dt.loc[ dt.Date == dt.Date ]\n        dt['Province_State'] = dt['Province_State'].fillna('')\n        dt['Date'] = pd.to_datetime( dt['Date'] )\n        dt['Date'] = dt['Date'] + pd.Timedelta(days=8)\n        dt['Type'] = pd.factorize( dt['Type'] )[0]\n        dt['cid'] = dt['Country_Region'] + '_' + dt['Province_State']\n        del dt['Reference'], dt['Country_Region'], dt['Province_State']\n        train = pd.merge( train, dt, on=['cid','Date'], how='left' )\n        train['Type'] = train.groupby('cid')['Type'].fillna( method='ffill' )\n\n        train['target0'] = np.log1p( train['ConfirmedCases'] )\n        train['target1'] = np.log1p( train['Fatalities'] )\n        # dt = pd.read_csv('..\/input\/covid19-country-data-wk3-release\/Data Join - RELEASE.csv')\n        # dt['Province_State'] = dt['Province_State'].fillna('')\n        # dt['Country_Region'] = dt['Country_Region'].fillna('')\n        # train = pd.merge( train, dt, on=['Country_Region','Province_State'], how='left' )\n        # #Fix\n\n        #print( train.head(4) )\n        #print( train.shape ) \n        \n        self.train = train.copy()\n   \n    \n    def create_features( self, df, valid_day ):\n\n        #df = df.loc[ df.day<=valid_day ].copy()\n        df = df.loc[ df.day>=(valid_day-50) ].copy()\n        \n        df['lag0_1'] = df.groupby('cid')['target0'].shift(self.lag)\n        df['lag0_1'] = df.groupby('cid')['lag0_1'].fillna( method='bfill' )\n\n        df['lag0_8'] = df.groupby('cid')['target0'].shift(8)\n        df['lag0_8'] = df.groupby('cid')['lag0_8'].fillna( method='bfill' )\n        \n        df['lag1_1'] = df.groupby('cid')['target1'].shift(self.lag)\n        df['lag1_1'] = df.groupby('cid')['lag1_1'].fillna( method='bfill' )\n\n        df['m0'] = df.groupby('cid')['lag0_1'].rolling(2).mean().values\n        df['m1'] = df.groupby('cid')['lag0_1'].rolling(3).mean().values\n        df['m2'] = df.groupby('cid')['lag0_1'].rolling(4).mean().values\n        df['m3'] = df.groupby('cid')['lag0_1'].rolling(5).mean().values\n        df['m4'] = df.groupby('cid')['lag0_1'].rolling(7).mean().values\n        df['m5'] = df.groupby('cid')['lag0_1'].rolling(10).mean().values\n        df['m6'] = df.groupby('cid')['lag0_1'].rolling(12).mean().values\n        df['m7'] = df.groupby('cid')['lag0_1'].rolling(16).mean().values\n        df['m8'] = df.groupby('cid')['lag0_1'].rolling(20).mean().values\n        df['m9'] = df.groupby('cid')['lag0_1'].rolling(25).mean().values\n\n        df['n0'] = df.groupby('cid')['lag1_1'].rolling(2).mean().values\n        df['n1'] = df.groupby('cid')['lag1_1'].rolling(3).mean().values\n        df['n2'] = df.groupby('cid')['lag1_1'].rolling(4).mean().values\n        df['n3'] = df.groupby('cid')['lag1_1'].rolling(5).mean().values\n        df['n4'] = df.groupby('cid')['lag1_1'].rolling(7).mean().values\n        df['n5'] = df.groupby('cid')['lag1_1'].rolling(10).mean().values\n        df['n6'] = df.groupby('cid')['lag1_1'].rolling(12).mean().values\n        df['n7'] = df.groupby('cid')['lag1_1'].rolling(16).mean().values\n        df['n8'] = df.groupby('cid')['lag1_1'].rolling(20).mean().values\n\n\n        df['m0'] = df.groupby('cid')['m0'].fillna( method='bfill' )\n        df['m1'] = df.groupby('cid')['m1'].fillna( method='bfill' )\n        df['m2'] = df.groupby('cid')['m2'].fillna( method='bfill' )\n        df['m3'] = df.groupby('cid')['m3'].fillna( method='bfill' )\n        df['m4'] = df.groupby('cid')['m4'].fillna( method='bfill' )\n        df['m5'] = df.groupby('cid')['m5'].fillna( method='bfill' )\n        df['m6'] = df.groupby('cid')['m6'].fillna( method='bfill' )\n        df['m7'] = df.groupby('cid')['m7'].fillna( method='bfill' )\n        df['m8'] = df.groupby('cid')['m8'].fillna( method='bfill' )\n        df['m9'] = df.groupby('cid')['m9'].fillna( method='bfill' )\n\n        df['n0'] = df.groupby('cid')['n0'].fillna( method='bfill' )\n        df['n1'] = df.groupby('cid')['n1'].fillna( method='bfill' )\n        df['n2'] = df.groupby('cid')['n2'].fillna( method='bfill' )\n        df['n3'] = df.groupby('cid')['n3'].fillna( method='bfill' )\n        df['n4'] = df.groupby('cid')['n4'].fillna( method='bfill' )\n        df['n5'] = df.groupby('cid')['n5'].fillna( method='bfill' )\n        df['n6'] = df.groupby('cid')['n6'].fillna( method='bfill' )\n        df['n7'] = df.groupby('cid')['n7'].fillna( method='bfill' )\n        df['n8'] = df.groupby('cid')['n8'].fillna( method='bfill' )\n\n        df['flag_China'] = 1*(df['Country_Region'] == 'China')\n        #df['flag_Italy'] = 1*(df['Country_Region'] == 'Italy')\n        #df['flag_Spain'] = 1*(df['Country_Region'] == 'Spain')\n        df['flag_US']    = 1*(df['Country_Region'] == 'US')\n        #df['flag_Brazil']= 1*(df['Country_Region'] == 'Brazil')\n        \n        df['flag_Kosovo_']   = 1*(df['cid'] == 'Kosovo_')\n        df['flag_Korea']     = 1*(df['cid'] == 'Korea, South_')\n        df['flag_Nepal_']    = 1*(df['cid'] == 'Nepal_')\n        df['flag_Holy See_'] = 1*(df['cid'] == 'Holy See_')\n        df['flag_Suriname_'] = 1*(df['cid'] == 'Suriname_')\n        df['flag_Ghana_']    = 1*(df['cid'] == 'Ghana_')\n        df['flag_Togo_']     = 1*(df['cid'] == 'Togo_')\n        df['flag_Malaysia_'] = 1*(df['cid'] == 'Malaysia_')\n        df['flag_US_Rhode']  = 1*(df['cid'] == 'US_Rhode Island')\n        df['flag_Bolivia_']  = 1*(df['cid'] == 'Bolivia_')\n        df['flag_China_Tib'] = 1*(df['cid'] == 'China_Tibet')\n        df['flag_Bahrain_']  = 1*(df['cid'] == 'Bahrain_')\n        df['flag_Honduras_'] = 1*(df['cid'] == 'Honduras_')\n        df['flag_Bangladesh']= 1*(df['cid'] == 'Bangladesh_')\n        df['flag_Paraguay_'] = 1*(df['cid'] == 'Paraguay_')\n\n        tr = df.loc[ df.day  < valid_day ].copy()\n        vl = df.loc[ df.day == valid_day ].copy()\n\n        tr = tr.loc[ tr.lag0_1 > 0 ].copy()\n\n        maptarget0 = tr.groupby('cid')['target0'].agg( log0_max='max' ).reset_index()\n        maptarget1 = tr.groupby('cid')['target1'].agg( log1_max='max' ).reset_index()\n        vl['log0_max'] = pd.merge( vl, maptarget0, on='cid' , how='left' )['log0_max'].values\n        vl['log1_max'] = pd.merge( vl, maptarget1, on='cid' , how='left' )['log1_max'].values\n        vl['log0_max'] = vl['log0_max'].fillna(0)\n        vl['log1_max'] = vl['log1_max'].fillna(0)\n\n        return tr, vl\n    \n\n    def train_models(self, valid_day = 10 ):\n\n        train = self.train.copy()\n\n        #Fix some anomalities:\n        train.loc[ (train.cid=='China_Guizhou') & (train.Date=='2020-03-17') , 'target0' ] = np.log1p( 146 )\n        train.loc[ (train.cid=='Guyana_')&(train.Date>='2020-03-22')&(train.Date<='2020-03-30') , 'target0' ] = np.log1p( 12 )\n        train.loc[ (train.cid=='US_Virgin Islands')&(train.Date>='2020-03-29')&(train.Date<='2020-03-29') , 'target0' ] = np.log1p( 24 )\n        train.loc[ (train.cid=='US_Virgin Islands')&(train.Date>='2020-03-30')&(train.Date<='2020-03-30') , 'target0' ] = np.log1p( 27 )\n\n        train.loc[ (train.cid=='Iceland_')&(train.Date>='2020-03-15')&(train.Date<='2020-03-15') , 'target1' ] = np.log1p( 0 )\n        train.loc[ (train.cid=='Kazakhstan_')&(train.Date>='2020-03-20')&(train.Date<='2020-03-20') , 'target1' ] = np.log1p( 0 )\n        train.loc[ (train.cid=='Serbia_')&(train.Date>='2020-03-26')&(train.Date<='2020-03-26') , 'target1' ] = np.log1p( 5 )\n        train.loc[ (train.cid=='Serbia_')&(train.Date>='2020-03-27')&(train.Date<='2020-03-27') , 'target1' ] = np.log1p( 6 )\n        train.loc[ (train.cid=='Slovakia_')&(train.Date>='2020-03-22')&(train.Date<='2020-03-31') , 'target1' ] = np.log1p( 1 )\n        train.loc[ (train.cid=='US_Hawaii')&(train.Date>='2020-03-25')&(train.Date<='2020-03-31') , 'target1' ] = np.log1p( 1 )\n\n        param = {\n            'subsample': 1.000,\n            'colsample_bytree': 0.85,\n            'max_depth': 6,\n            'gamma': 0.000,\n            'learning_rate': 0.010,\n            'min_child_weight': 5.00,\n            'reg_alpha': 0.000,\n            'reg_lambda': 0.400,\n            'silent':1,\n            'objective':'reg:squarederror',\n            #'booster':'dart',\n            #'tree_method': 'gpu_hist',\n            'nthread': 12,#-1,\n            'seed': self.seed\n            }    \n        \n        tr, vl = self.create_features( train.copy(), valid_day )\n        #Features for Cases\n        features = [f for f in tr.columns if f not in [\n            #'flag_China','flag_US',\n            #'flag_Kosovo_','flag_Korea','flag_Nepal_','flag_Holy See_','flag_Suriname_','flag_Ghana_','flag_Togo_','flag_Malaysia_','flag_US_Rhode','flag_Bolivia_','flag_China_Tib','flag_Bahrain_','flag_Honduras_','flag_Bangladesh','flag_Paraguay_',\n            'lag0_8',\n            'Id','ConfirmedCases','Fatalities','log0','log1','target0','target1','ypred0','ypred1','Province_State','Country_Region','Date','ForecastId','cid','geo','day',\n            'GDP_region','TRUE POPULATION','pct_in_largest_city',' TFR ',' Avg_age ','latitude','longitude','abs_latitude','temperature', 'humidity',\n            'Personality_pdi','Personality_idv','Personality_mas','Personality_uai','Personality_ltowvs','Personality_assertive','personality_perform','personality_agreeableness',\n            'murder','High_rises','max_high_rises','AIR_CITIES','AIR_AVG','continent_gdp_pc','continent_happiness','continent_generosity','continent_corruption','continent_Life_expectancy'        \n        ] ]\n        self.features0 = features\n        #Features for Fatalities\n        features = [f for f in tr.columns if f not in [\n            'm0','m1','m2','m3',\n            #'flag_China','flag_US',\n            #'flag_Kosovo_','flag_Korea','flag_Nepal_','flag_Holy See_','flag_Suriname_','flag_Ghana_','flag_Togo_','flag_Malaysia_','flag_US_Rhode','flag_Bolivia_','flag_China_Tib','flag_Bahrain_','flag_Honduras_','flag_Bangladesh','flag_Paraguay_',\n            'Id','ConfirmedCases','Fatalities','log0','log1','target0','target1','ypred0','ypred1','Province_State','Country_Region','Date','ForecastId','cid','geo','day',\n            'GDP_region','TRUE POPULATION','pct_in_largest_city',' TFR ',' Avg_age ','latitude','longitude','abs_latitude','temperature', 'humidity',\n            'Personality_pdi','Personality_idv','Personality_mas','Personality_uai','Personality_ltowvs','Personality_assertive','personality_perform','personality_agreeableness',\n            'murder','High_rises','max_high_rises','AIR_CITIES','AIR_AVG','continent_gdp_pc','continent_happiness','continent_generosity','continent_corruption','continent_Life_expectancy'        \n        ] ]\n        self.features1 = features\n        \n\n        nrounds0 = 680\n        nrounds1 = 630\n         #lag 1###############################################################\n        dtrain = xgb.DMatrix( tr[self.features0], tr['target0'] )\n        param['seed'] = self.seed\n        self.model0 = xgb.train( param, dtrain, nrounds0, verbose_eval=0 )\n        param['seed'] = self.seed+1\n        self.model1 = xgb.train( param, dtrain, nrounds0, verbose_eval=0 )\n        \n        dtrain = xgb.DMatrix( tr[self.features1], tr['target1'] )\n        param['seed'] = self.seed\n        self.model2 = xgb.train( param, dtrain, nrounds1, verbose_eval=0 ) \n        param['seed'] = self.seed+1\n        self.model3 = xgb.train( param, dtrain, nrounds1, verbose_eval=0 )\n        \n        self.vl = vl\n        \n        return 1\n    \n        \n    def predict_first_day(self, day ):\n        \n        self.day = day\n        self.train_models( day )\n        \n        dvalid = xgb.DMatrix( self.vl[self.features0] )\n        ypred0 = ( self.model0.predict( dvalid ) + self.model1.predict( dvalid )  ) \/ 2\n        dvalid = xgb.DMatrix( self.vl[self.features1] )\n        ypred1 = ( self.model2.predict( dvalid ) + self.model3.predict( dvalid )  ) \/ 2\n        \n        self.vl['ypred0'] = ypred0\n        self.vl['ypred1'] = ypred1\n        self.vl.loc[ self.vl.ypred0<self.vl.log0_max, 'ypred0'] =  self.vl.loc[ self.vl.ypred0<self.vl.log0_max, 'log0_max']\n        self.vl.loc[ self.vl.ypred1<self.vl.log1_max, 'ypred1'] =  self.vl.loc[ self.vl.ypred1<self.vl.log1_max, 'log1_max']\n        \n        VALID = self.vl[[\"geo\", \"day\", 'ypred0', 'ypred1']].copy()\n        VALID.columns = [\"geo\", \"day\", 'ConfirmedCases', 'Fatalities']        \n        return VALID.reset_index(drop=True)\n    \n    \n    def predict_next_day(self, yesterday ):\n\n        self.day += 1\n        \n        feats = ['geo','day']        \n        self.train[ 'ypred0' ] = pd.merge( self.train[feats], yesterday[feats+['ConfirmedCases']], on=feats, how='left' )['ConfirmedCases'].values\n        self.train.loc[ self.train.ypred0.notnull(), 'target0'] = self.train.loc[ self.train.ypred0.notnull() , 'ypred0']\n\n        self.train[ 'ypred1' ] = pd.merge( self.train[feats], yesterday[feats+['Fatalities']], on=feats, how='left' )['Fatalities'].values\n        self.train.loc[ self.train.ypred1.notnull(), 'target1'] = self.train.loc[ self.train.ypred1.notnull() , 'ypred1']\n        del self.train['ypred0'], self.train['ypred1']\n        \n        tr, vl = self.create_features( self.train.copy(), self.day )        \n        dvalid = xgb.DMatrix( vl[self.features0] )\n        ypred0 = (self.model0.predict( dvalid ) + self.model1.predict( dvalid ) )\/2\n        dvalid = xgb.DMatrix( vl[self.features1] )\n        ypred1 = (self.model2.predict( dvalid ) + self.model3.predict( dvalid ) )\/2\n    \n        vl['ypred0'] = ypred0\n        vl['ypred1'] = ypred1\n        vl.loc[ vl.ypred0<vl.log0_max, 'ypred0'] =  vl.loc[ vl.ypred0<vl.log0_max, 'log0_max']\n        vl.loc[ vl.ypred1<vl.log1_max, 'ypred1'] =  vl.loc[ vl.ypred1<vl.log1_max, 'log1_max']\n        \n        self.vl = vl\n        VALID = vl[[\"geo\", \"day\", 'ypred0', 'ypred1']].copy()\n        VALID.columns = [\"geo\", \"day\", 'ConfirmedCases', 'Fatalities']        \n        return VALID.reset_index(drop=True)","d730b5d0":"## defining constants\nPATH_TRAIN = \"\/kaggle\/input\/covid19-global-forecasting-week-4\/train.csv\"\nPATH_TEST = \"\/kaggle\/input\/covid19-global-forecasting-week-4\/test.csv\"\n\nPATH_SUBMISSION = \"submission.csv\"\nPATH_OUTPUT = \"output.csv\"\n\nPATH_REGION_METADATA = \"\/kaggle\/input\/covid19-forecasting-metadata\/region_metadata.csv\"\nPATH_REGION_DATE_METADATA = \"\/kaggle\/input\/covid19-forecasting-metadata\/region_date_metadata.csv\"\n\nVAL_DAYS = 7\nMAD_FACTOR = 0.5\nDAYS_SINCE_CASES = [1, 10, 50, 100, 500, 1000, 5000, 10000]\n\nSEED = 2357\n\nLGB_PARAMS = {\"objective\": \"regression\",\n              \"num_leaves\": 5,\n              \"learning_rate\": 0.013,\n              \"bagging_fraction\": 0.91,\n              \"feature_fraction\": 0.81,\n              \"reg_alpha\": 0.13,\n              \"reg_lambda\": 0.13,\n              \"metric\": \"rmse\",\n              \"seed\": SEED\n             }","cd158886":"## reading data\ntrain = pd.read_csv(PATH_TRAIN)\ntest = pd.read_csv(PATH_TEST)\n\nregion_metadata = pd.read_csv(PATH_REGION_METADATA)\nregion_date_metadata = pd.read_csv(PATH_REGION_DATE_METADATA)","88a2a7b4":"## preparing data\ntrain = train.merge(test[[\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\"]], on = [\"Province_State\", \"Country_Region\", \"Date\"], how = \"left\")\ntest = test[~test.Date.isin(train.Date.unique())]\n\ndf_panel = pd.concat([train, test], sort = False)\n\n# combining state and country into 'geography'\ndf_panel[\"geography\"] = df_panel.Country_Region.astype(str) + \": \" + df_panel.Province_State.astype(str)\ndf_panel.loc[df_panel.Province_State.isna(), \"geography\"] = df_panel[df_panel.Province_State.isna()].Country_Region\n\n# fixing data issues with cummax\ndf_panel.ConfirmedCases = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].cummax()\ndf_panel.Fatalities = df_panel.groupby(\"geography\")[\"Fatalities\"].cummax()\n\n# merging external metadata\ndf_panel = df_panel.merge(region_metadata, on = [\"Country_Region\", \"Province_State\"])\ndf_panel = df_panel.merge(region_date_metadata, on = [\"Country_Region\", \"Province_State\", \"Date\"], how = \"left\")\n\n# label encoding continent\ndf_panel.continent = LabelEncoder().fit_transform(df_panel.continent)\ndf_panel.Date = pd.to_datetime(df_panel.Date, format = \"%Y-%m-%d\")\n\ndf_panel.sort_values([\"geography\", \"Date\"], inplace = True)","eafd9945":"## feature engineering\nmin_date_train = np.min(df_panel[~df_panel.Id.isna()].Date)\nmax_date_train = np.max(df_panel[~df_panel.Id.isna()].Date)\n\nmin_date_test = np.min(df_panel[~df_panel.ForecastId.isna()].Date)\nmax_date_test = np.max(df_panel[~df_panel.ForecastId.isna()].Date)\n\nn_dates_test = len(df_panel[~df_panel.ForecastId.isna()].Date.unique())\n\nprint(\"Train date range:\", str(min_date_train), \" - \", str(max_date_train))\nprint(\"Test date range:\", str(min_date_test), \" - \", str(max_date_test))\n\n# creating lag features\nfor lag in range(1, 41):\n    df_panel[f\"lag_{lag}_cc\"] = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].shift(lag)\n    df_panel[f\"lag_{lag}_ft\"] = df_panel.groupby(\"geography\")[\"Fatalities\"].shift(lag)\n    df_panel[f\"lag_{lag}_rc\"] = df_panel.groupby(\"geography\")[\"Recoveries\"].shift(lag)\n\nfor case in DAYS_SINCE_CASES:\n    df_panel = df_panel.merge(df_panel[df_panel.ConfirmedCases >= case].groupby(\"geography\")[\"Date\"].min().reset_index().rename(columns = {\"Date\": f\"case_{case}_date\"}), on = \"geography\", how = \"left\")","2024befa":"## function for preparing features\ndef prepare_features(df, gap):\n    \n    df[\"perc_1_ac\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap}_rc\"]) \/ df[f\"lag_{gap}_cc\"]\n    df[\"perc_1_cc\"] = df[f\"lag_{gap}_cc\"] \/ df.population\n    \n    df[\"diff_1_cc\"] = df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 1}_cc\"]\n    df[\"diff_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] - df[f\"lag_{gap + 2}_cc\"]\n    df[\"diff_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] - df[f\"lag_{gap + 3}_cc\"]\n    \n    df[\"diff_1_ft\"] = df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 1}_ft\"]\n    df[\"diff_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] - df[f\"lag_{gap + 2}_ft\"]\n    df[\"diff_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] - df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"diff_123_cc\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 3}_cc\"]) \/ 3\n    df[\"diff_123_ft\"] = (df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 3}_ft\"]) \/ 3\n\n    df[\"diff_change_1_cc\"] = df.diff_1_cc \/ df.diff_2_cc\n    df[\"diff_change_2_cc\"] = df.diff_2_cc \/ df.diff_3_cc\n    \n    df[\"diff_change_1_ft\"] = df.diff_1_ft \/ df.diff_2_ft\n    df[\"diff_change_2_ft\"] = df.diff_2_ft \/ df.diff_3_ft\n\n    df[\"diff_change_12_cc\"] = (df.diff_change_1_cc + df.diff_change_2_cc) \/ 2\n    df[\"diff_change_12_ft\"] = (df.diff_change_1_ft + df.diff_change_2_ft) \/ 2\n    \n    df[\"change_1_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 1}_cc\"]\n    df[\"change_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] \/ df[f\"lag_{gap + 2}_cc\"]\n    df[\"change_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] \/ df[f\"lag_{gap + 3}_cc\"]\n\n    df[\"change_1_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 1}_ft\"]\n    df[\"change_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] \/ df[f\"lag_{gap + 2}_ft\"]\n    df[\"change_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] \/ df[f\"lag_{gap + 3}_ft\"]\n\n    df[\"change_1_3_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 3}_cc\"]\n    df[\"change_1_3_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"change_1_7_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 7}_cc\"]\n    df[\"change_1_7_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 7}_ft\"]\n    \n    for case in DAYS_SINCE_CASES:\n        df[f\"days_since_{case}_case\"] = (df[f\"case_{case}_date\"] - df.Date).astype(\"timedelta64[D]\")\n        df.loc[df[f\"days_since_{case}_case\"] < gap, f\"days_since_{case}_case\"] = np.nan\n\n    df[\"country_flag\"] = df.Province_State.isna().astype(int)\n\n    # target variable is log of change from last known value\n    df[\"target_cc\"] = np.log1p(df.ConfirmedCases - df[f\"lag_{gap}_cc\"])\n    df[\"target_ft\"] = np.log1p(df.Fatalities - df[f\"lag_{gap}_ft\"])\n    \n    features = [\n        f\"lag_{gap}_cc\",\n        f\"lag_{gap}_ft\",\n        f\"lag_{gap}_rc\",\n        \"perc_1_ac\",\n        \"perc_1_cc\",\n        \"diff_1_cc\",\n        \"diff_2_cc\",\n        \"diff_3_cc\",\n        \"diff_1_ft\",\n        \"diff_2_ft\",\n        \"diff_3_ft\",\n        \"diff_123_cc\",\n        \"diff_123_ft\",\n        \"diff_change_1_cc\",\n        \"diff_change_2_cc\",\n        \"diff_change_1_ft\",\n        \"diff_change_2_ft\",\n        \"diff_change_12_cc\",\n        \"diff_change_12_ft\",\n        \"change_1_cc\",\n        \"change_2_cc\",\n        \"change_3_cc\",\n        \"change_1_ft\",\n        \"change_2_ft\",\n        \"change_3_ft\",\n        \"change_1_3_cc\",\n        \"change_1_3_ft\",\n        \"change_1_7_cc\",\n        \"change_1_7_ft\",\n        \"days_since_1_case\",\n        \"days_since_10_case\",\n        \"days_since_50_case\",\n        \"days_since_100_case\",\n        \"days_since_500_case\",\n        \"days_since_1000_case\",\n        \"days_since_5000_case\",\n        \"days_since_10000_case\",\n        \"country_flag\",\n        \"lat\",\n        \"lon\",\n        \"continent\",\n        \"population\",\n        \"area\",\n        \"density\",\n        \"target_cc\",\n        \"target_ft\"\n    ]\n    \n    return df[features]","035b2360":"## function for building and predicting using LGBM model\ndef build_predict_lgbm(df_train, df_test, gap):\n    \n    df_train.dropna(subset = [\"target_cc\", \"target_ft\", f\"lag_{gap}_cc\", f\"lag_{gap}_ft\"], inplace = True)\n    \n    target_cc = df_train.target_cc\n    target_ft = df_train.target_ft\n    \n    test_lag_cc = df_test[f\"lag_{gap}_cc\"].values\n    test_lag_ft = df_test[f\"lag_{gap}_ft\"].values\n    \n    df_train.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    df_test.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    \n    categorical_features = [\"continent\"]\n    \n    dtrain_cc = lgb.Dataset(df_train, label = target_cc, categorical_feature = categorical_features)\n    dtrain_ft = lgb.Dataset(df_train, label = target_ft, categorical_feature = categorical_features)\n\n    model_cc = lgb.train(LGB_PARAMS, train_set = dtrain_cc, num_boost_round = 200)\n    model_ft = lgb.train(LGB_PARAMS, train_set = dtrain_ft, num_boost_round = 200)\n    \n    # inverse transform from log of change from last known value\n    y_pred_cc = np.expm1(model_cc.predict(df_test, num_boost_round = 200)) + test_lag_cc\n    y_pred_ft = np.expm1(model_ft.predict(df_test, num_boost_round = 200)) + test_lag_ft\n    \n    return y_pred_cc, y_pred_ft, model_cc, model_ft","a3cb6683":"## function for predicting moving average decay model\ndef predict_mad(df_test, gap, val = False):\n    \n    df_test[\"avg_diff_cc\"] = (df_test[f\"lag_{gap}_cc\"] - df_test[f\"lag_{gap + 3}_cc\"]) \/ 3\n    df_test[\"avg_diff_ft\"] = (df_test[f\"lag_{gap}_ft\"] - df_test[f\"lag_{gap + 3}_ft\"]) \/ 3\n\n    if val:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) \/ VAL_DAYS\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) \/ VAL_DAYS\n    else:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) \/ n_dates_test\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) \/ n_dates_test\n\n    return y_pred_cc, y_pred_ft","7ad15280":"## building lag x-days models\ndf_train = df_panel[~df_panel.Id.isna()]\ndf_test_full = df_panel[~df_panel.ForecastId.isna()]\n\ndf_preds_val = []\ndf_preds_test = []\n\nfor date in df_test_full.Date.unique():\n    \n    print(\"Processing date:\", date)\n    \n    # ignore date already present in train data\n    if date in df_train.Date.values:\n        df_pred_test = df_test_full.loc[df_test_full.Date == date, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].rename(columns = {\"ConfirmedCases\": \"ConfirmedCases_test\", \"Fatalities\": \"Fatalities_test\"})\n        \n        # multiplying predictions by 41 to not look cool on public LB\n        df_pred_test.ConfirmedCases_test = df_pred_test.ConfirmedCases_test * 41\n        df_pred_test.Fatalities_test = df_pred_test.Fatalities_test * 41\n    else:\n        df_test = df_test_full[df_test_full.Date == date]\n        \n        gap = (pd.Timestamp(date) - max_date_train).days\n        \n        if gap <= VAL_DAYS:\n            val_date = max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")\n\n            df_build = df_train[df_train.Date < val_date]\n            df_val = df_train[df_train.Date == val_date]\n            \n            X_build = prepare_features(df_build, gap)\n            X_val = prepare_features(df_val, gap)\n            \n            y_val_cc_lgb, y_val_ft_lgb, _, _ = build_predict_lgbm(X_build, X_val, gap)\n            y_val_cc_mad, y_val_ft_mad = predict_mad(df_val, gap, val = True)\n            \n            df_pred_val = pd.DataFrame({\"Id\": df_val.Id.values,\n                                        \"ConfirmedCases_val_lgb\": y_val_cc_lgb,\n                                        \"Fatalities_val_lgb\": y_val_ft_lgb,\n                                        \"ConfirmedCases_val_mad\": y_val_cc_mad,\n                                        \"Fatalities_val_mad\": y_val_ft_mad,\n                                       })\n\n            df_preds_val.append(df_pred_val)\n\n        X_train = prepare_features(df_train, gap)\n        X_test = prepare_features(df_test, gap)\n\n        y_test_cc_lgb, y_test_ft_lgb, model_cc, model_ft = build_predict_lgbm(X_train, X_test, gap)\n        y_test_cc_mad, y_test_ft_mad = predict_mad(df_test, gap)\n        \n        if gap == 1:\n            model_1_cc = model_cc\n            model_1_ft = model_ft\n            features_1 = X_train.columns.values\n        elif gap == 14:\n            model_14_cc = model_cc\n            model_14_ft = model_ft\n            features_14 = X_train.columns.values\n        elif gap == 28:\n            model_28_cc = model_cc\n            model_28_ft = model_ft\n            features_28 = X_train.columns.values\n\n        df_pred_test = pd.DataFrame({\"ForecastId\": df_test.ForecastId.values,\n                                     \"ConfirmedCases_test_lgb\": y_test_cc_lgb,\n                                     \"Fatalities_test_lgb\": y_test_ft_lgb,\n                                     \"ConfirmedCases_test_mad\": y_test_cc_mad,\n                                     \"Fatalities_test_mad\": y_test_ft_mad,\n                                    })\n    \n    df_preds_test.append(df_pred_test)","ad549469":"## validation score\ndf_panel = df_panel.merge(pd.concat(df_preds_val, sort = False), on = \"Id\", how = \"left\")\ndf_panel = df_panel.merge(pd.concat(df_preds_test, sort = False), on = \"ForecastId\", how = \"left\")\n\nrmsle_cc_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases_val_lgb)))\nrmsle_ft_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities_val_lgb)))\n\nrmsle_cc_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases_val_mad)))\nrmsle_ft_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities_val_mad)))\n\nprint(\"LGB CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_lgb, 2))\nprint(\"LGB FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_lgb, 2))\nprint(\"LGB Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_lgb + rmsle_ft_lgb) \/ 2, 2))\nprint(\"\\n\")\nprint(\"MAD CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_mad, 2))\nprint(\"MAD FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_mad, 2))\nprint(\"MAD Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_mad + rmsle_ft_mad) \/ 2, 2))","b73c8240":"## preparing submission file\ndf_test = df_panel.loc[~df_panel.ForecastId.isna(), [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\",\n                                                     \"ConfirmedCases_test\", \"ConfirmedCases_test_lgb\", \"ConfirmedCases_test_mad\",\n                                                     \"Fatalities_test\", \"Fatalities_test_lgb\", \"Fatalities_test_mad\"]].reset_index()\n\ndf_test[\"ConfirmedCases\"] = 0.13 * df_test.ConfirmedCases_test_lgb + 0.87 * df_test.ConfirmedCases_test_mad\ndf_test[\"Fatalities\"] = 0.13 * df_test.Fatalities_test_lgb + 0.87 * df_test.Fatalities_test_mad\n\n# Since LGB models don't predict these geographies well\ndf_test.loc[df_test.Country_Region.isin([\"Diamond Princess\", \"MS Zaandam\"]), \"ConfirmedCases\"] = df_test[df_test.Country_Region.isin([\"Diamond Princess\", \"MS Zaandam\"])].ConfirmedCases_test_mad.values\ndf_test.loc[df_test.Country_Region.isin([\"Diamond Princess\", \"MS Zaandam\"]), \"Fatalities\"] = df_test[df_test.Country_Region.isin([\"Diamond Princess\", \"MS Zaandam\"])].Fatalities_test_mad.values\n\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"ConfirmedCases\"] = df_test[df_test.Date.isin(df_train.Date.values)].ConfirmedCases_test.values\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"Fatalities\"] = df_test[df_test.Date.isin(df_train.Date.values)].Fatalities_test.values\n\ndf_submission = df_test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\ndf_submission.ForecastId = df_submission.ForecastId.astype(int)","f3019631":"TARGETS = [\"ConfirmedCases\", \"Fatalities\"]\n\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n\ndf = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/train.csv\")\ndf[TARGETS] = np.log1p(df[TARGETS].values)\nsub_df = pd.read_csv(\"..\/input\/covid19-global-forecasting-week-4\/test.csv\")\n\ndef preprocess(df):\n    for col in [\"Country_Region\", \"Province_State\"]:\n        df[col].fillna(\"\", inplace=True)\n\n    df[\"Date\"] = pd.to_datetime(df['Date'])\n    df['day'] = df.Date.dt.dayofyear\n    df['geo'] = ['_'.join(x) for x in zip(df['Country_Region'], df['Province_State'])]\n    return df\n\ndf = preprocess(df)\nsub_df = preprocess(sub_df)\n\nsub_df[\"day\"] -= df[\"day\"].min()\ndf[\"day\"] -= df[\"day\"].min()","8721e3dd":"TEST_FIRST = sub_df[sub_df[\"Date\"] > df[\"Date\"].max()][\"Date\"].min()\nprint(TEST_FIRST)\nTEST_DAYS = (sub_df[\"Date\"].max() - TEST_FIRST).days + 1\nTEST_FIRST = (TEST_FIRST - df[\"Date\"].min()).days\n\nprint(TEST_FIRST, TEST_DAYS)\n\n\ndef get_blend(pred_dfs, weights, verbose=True):\n    if verbose:\n        for n1, n2 in [(\"cpmp\", \"giba1\"),(\"cpmp\", \"giba2\"), (\"cpmp\", \"ahmet\"), (\"giba1\", \"ahmet\"), (\"giba2\", \"ahmet\")]:\n            print(n1, n2, np.round(rmse(pred_dfs[n1][TARGETS[0]], pred_dfs[n2][TARGETS[0]]), 4), np.round(rmse(pred_dfs[n1][TARGETS[1]], pred_dfs[n2][TARGETS[1]]), 4))\n    \n    blend_df = pred_dfs[\"cpmp\"].copy()\n    blend_df[TARGETS] = 0\n    for name, pred_df in pred_dfs.items():\n        blend_df[TARGETS] += weights[name]*pred_df[TARGETS].values\n        \n    return blend_df\n\n\ncov_models = {\"ahmet\": CovidModelAhmet(), \"cpmp\": CovidModelCPMP(), 'giba1': CovidModelGIBA(lag=1), 'giba2': CovidModelGIBA(lag=2)}\nweights = {\"ahmet\": 0.35, \"cpmp\": 0.30, \"giba1\": 0.175, \"giba2\": 0.175}\npred_dfs = {name: cm.predict_first_day(TEST_FIRST).sort_values(\"geo\") for name, cm in cov_models.items()}\n\n\nblend_df = get_blend(pred_dfs, weights)\neval_df = blend_df.copy()\n\nfor d in range(1, TEST_DAYS):\n    pred_dfs = {name: cm.predict_next_day(blend_df).sort_values(\"geo\") for name, cm in cov_models.items()}\n    blend_df = get_blend(pred_dfs, weights)\n    eval_df = eval_df.append(blend_df)\n    print(d, eval_df.shape, flush=True)","5ca92f20":"eval_df.head()","10744dcd":"print(sub_df.shape)\nsub_df = sub_df.merge(df.append(eval_df, sort=False), on=[\"geo\", \"day\"], how=\"left\")\nprint(sub_df.shape)\nprint(sub_df[TARGETS].isnull().mean())","8b15a3de":"sub_df.head()","026cc21c":"flat = [\n            'China_Anhui',\n            'China_Beijing',\n            'China_Chongqing',\n            'China_Fujian',\n            'China_Gansu',\n            'China_Guangdong',\n            'China_Guangxi',\n            'China_Guizhou',\n            'China_Hainan',\n            'China_Hebei',\n            'China_Heilongjiang',\n            'China_Henan',\n            'China_Hubei',\n            'China_Hunan',\n            'China_Jiangsu',\n            'China_Jiangxi',\n            'China_Jilin',\n            'China_Liaoning',\n            'China_Ningxia',\n            'China_Qinghai',\n            'China_Shaanxi',\n            'China_Shandong',\n            'China_Shanxi',\n            'China_Sichuan',\n            'China_Tibet',\n            'China_Xinjiang',\n            'China_Yunnan',\n            'China_Zhejiang',\n            'Diamond Princess_',\n            'Holy See_', \n        ]     ","8e446f24":"dt = sub_df.loc[ sub_df.Date_x == \"2020-04-07\"  ].copy()\ndt = dt.loc[ dt.geo.isin(flat)  ].copy()\ndt = dt[['geo','Date_x','day','ConfirmedCases','Fatalities']].copy()\ndt = dt.reset_index(drop=True)","0472766d":"sub_df['ow0'] = pd.merge( sub_df, dt, on='geo', how='left' )['ConfirmedCases_y'].values\nsub_df['ow1'] = pd.merge( sub_df, dt, on='geo', how='left' )['Fatalities_y'].values","0a30e8ae":"sub_df.loc[ sub_df.ow0.notnull() & (sub_df.Date_x >= '2020-04-08') , 'ConfirmedCases'  ] = sub_df.loc[ sub_df.ow0.notnull() & (sub_df.Date_x >= '2020-04-08') , 'ow0'  ]\nsub_df.loc[ sub_df.ow1.notnull() & (sub_df.Date_x >= '2020-04-08') , 'Fatalities'  ] = sub_df.loc[ sub_df.ow1.notnull() & (sub_df.Date_x >= '2020-04-08') , 'ow1'  ]","c221e789":"sub_df.sort_values(\"ForecastId\", inplace=True)\ndf_submission.sort_values(\"ForecastId\", inplace=True)","7652ae43":"for t in TARGETS:\n    df_submission[t] = np.expm1(np.log1p(df_submission[t].values)*0.55 + sub_df[t].values*0.45)","55ab7a3f":"df_submission.to_csv(\"submission.csv\", index=False, float_format='%.1f')","ea4d6879":"# BLEND"}}