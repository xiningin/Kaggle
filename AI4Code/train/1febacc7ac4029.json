{"cell_type":{"1390abe8":"code","9d95485f":"code","d9bb6ab7":"code","09849ec0":"code","5a8bee45":"code","73b8d63d":"code","a48f1e8e":"code","97bcfb45":"code","80210309":"code","540cd5ba":"code","f0492578":"code","11f66e51":"code","34e54e1b":"code","c97f9242":"code","5297c40f":"code","5cd5ef46":"code","183961e1":"code","02acc890":"code","c448c993":"code","16ec1876":"code","50a24488":"code","09f9ea6e":"markdown","314e0a0a":"markdown","4bb4281e":"markdown","9c5ab68d":"markdown","2cc9c35a":"markdown","c278df00":"markdown","b25776a3":"markdown","d8eab00f":"markdown","b89e60e9":"markdown"},"source":{"1390abe8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9d95485f":"data = pd.read_csv('..\/input\/voice.csv')","d9bb6ab7":"data.info()","09849ec0":"data.label.value_counts()","5a8bee45":"# Convert label feature: female = 0 male = 1\ndata['label'] = [1 if i=='male' else 0 for i in data.label]\ndata.label.value_counts()","73b8d63d":"# data selection\nx_data = data.drop(['label'], axis=1) # it is a matrix excluding label feature\ny = data.label.values # it is a vector wich contains only label feature","a48f1e8e":"x_data.head()","97bcfb45":"y","80210309":"# normalization of x_data and obtaining x\nx = (x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nx.head()","540cd5ba":"# train test split (we split our data into 2 parts: train and test. Test part is 20% of all data)\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2, random_state=42) # 0.2=20%","f0492578":"# take transpose of all these partial data\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T ","11f66e51":"# initialize w: weight and b: bias\ndimension = 20\ndef initialize(dimension):\n    w = np.full((dimension,1), 0.01)\n    b = 0.0\n    return w,b","34e54e1b":"# sigmoid function\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n\n# check sigmoid function\nsigmoid(0)","c97f9242":"def cost(y_head, y_train):\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost_value = np.sum(loss)\/x_train.shape[1] # for scaling\n    return cost_value","5297c40f":"def forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train)+b\n    y_head = sigmoid(z)\n    cost_value = cost(y_head, y_train)\n    \n    # backward propagation\n    derivative_weight = (np.dot(x_train, ((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = (np.sum(y_head-y_train))\/x_train.shape[1]\n    \n    return cost_value, derivative_weight, derivative_bias","5cd5ef46":"def logistic_regression(x_train, x_test, y_train, y_test, learning_rate, num_iteration):\n    w,b = initialize(dimension)\n    cost_list = []\n    index = []\n    for i in range(num_iteration):\n        cost_value, derivative_weight, derivative_bias = forward_backward_propagation(w,b,x_train,y_train)\n        \n        # updating weight and bias\n        w = w-learning_rate*derivative_weight\n        b = b-learning_rate*derivative_bias\n\n        if i % 10 == 0:\n            index.append(i)\n            cost_list.append(cost_value)\n            print('cost after iteration {}: {}'.format(i,cost_value))\n    \n    # in for loop above, we have obtained final values of parameters(weight and bias): machine has learnt them \n           \n    z_final = np.dot(w.T,x_test)+b\n    z_final_sigmoid = sigmoid(z_final) #z_final value after sigmoid function\n    \n    # prediction\n    y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z_final_sigmoid is bigger than 0.5, our prediction is sign 1 (y_head_=1)\n    # if z_final_sigmoid is smaller than 0.5, our prediction is sign 0 (y_head_=0)\n    for i in range(z_final_sigmoid.shape[1]):\n        if z_final_sigmoid[0,i]<= 0.5:\n            y_prediction[0,i] = 0\n        else:\n            y_prediction[0,i] = 1\n            \n    # print test errors\n    print('test accuracy: {} %'.format(100-np.mean(np.abs(y_prediction-y_test))*100))\n    \n    # plot iteration vs cost function\n    plt.figure(figsize=(15,10))\n    plt.plot(index, cost_list)\n    plt.xticks(index, rotation='vertical')\n    plt.xlabel('number of iteration', fontsize=14)\n    plt.ylabel('cost', fontsize=14)\n    plt.show()          ","183961e1":"# run the program\n# Firstly, learning_rate and num_iteration are chosen randomly. Then it is tuned accordingly\nlogistic_regression(x_train, x_test, y_train, y_test, learning_rate=1.5, num_iteration=200)","02acc890":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression()\nlog_reg.fit(x_train.T,y_train.T)","c448c993":"# prediction of test data\nlog_reg.predict(x_test.T)","16ec1876":"# actual values\ny_test","50a24488":"print('test_accuracy: {}'.format(log_reg.score(x_train.T,y_train.T)))","09f9ea6e":"  * Example of a cost function vs weight\n   <a href=\"http:\/\/imgbb.com\/\"><img src=\"http:\/\/image.ibb.co\/dAaYJH\/7.jpg\" alt=\"7\" border=\"0\"><\/a>","314e0a0a":"# LOGISTIC REGRESSION USING SKLEAR","4bb4281e":"* Computation graph of logistic regression\n<a href=\"http:\/\/ibb.co\/c574qx\"><img src=\"http:\/\/preview.ibb.co\/cxP63H\/5.jpg\" alt=\"5\" border=\"0\"><\/a>\n    * Parameters to be found are weights and bias\n    * Initial values of weight and bias parameters can be chosen arbitrarily\n    * For every iteration, we are going to calculate loss function\n    * Sum of the loss function will be our cost function\n    * We are going to update weight and bias parameters using derivative of cost function and a learning rate\n    * Learning rate is a hyperparameter that is chosen randomly and tuned afterward.\n    * After many iteratios, the cost wil be minimized and we will obtain final weight and bias parameters to be used (our machine will learn them)\n    * Using these final weight and bias parameters we are going to predict a given test data","9c5ab68d":"* Mathematical expression of log loss(error) function is: \n    <a href=\"https:\/\/imgbb.com\/\"><img src=\"https:\/\/image.ibb.co\/eC0JCK\/duzeltme.jpg\" alt=\"duzeltme\" border=\"0\"><\/a>","2cc9c35a":"**Derivatives of cost function wrt w and b**\n\n$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}x(  y_head - y)^T$$\n$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (y_head-y)$$","c278df00":"<a id=\"1\"><\/a> <br>\n# Logistic Regression\n* When we have binary classification( 0 and 1 outputs) we can use logistic regression","b25776a3":"At first glance, we see that 21th value of predicted data is 1, however it is 0 in actual data (y_test). There can be also other wrong predictions","d8eab00f":"**Updating Weight and Bias**\n\n* alpha = learning rate\n* J: cost function\n* w: weight\n* b: bias\n    <a href=\"http:\/\/imgbb.com\/\"><img src=\"http:\/\/image.ibb.co\/hYTTJH\/8.jpg\" alt=\"8\" border=\"0\"><\/a>\n*  Using similar way, we update bias","b89e60e9":"* For sigmoid function please visit\nhttps:\/\/en.wikipedia.org\/wiki\/Sigmoid_function"}}