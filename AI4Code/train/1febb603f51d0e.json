{"cell_type":{"1f20ce6d":"code","d65e01b1":"code","a12055b0":"code","aa591282":"code","19ccb9fc":"code","6fb39e80":"code","3589f433":"code","4bf3cdb7":"code","da3c0215":"code","6a3dc58b":"code","d94f9931":"code","b48a57f8":"code","de60cc18":"code","84a0376e":"code","d3184e62":"code","473df8a0":"code","cb68d859":"code","7bd970ed":"code","8a7e23e3":"code","8ce6a99f":"code","f8c9f93b":"code","b525f70c":"code","9b5fe464":"code","e6913a28":"code","ba7b3251":"code","1f7c625f":"code","ebc934a3":"code","c81d23e0":"code","1a587021":"code","bcf95442":"code","c238aaa5":"markdown","d51706cd":"markdown","5a286112":"markdown","15ad12bc":"markdown","6793a167":"markdown","155c7fdd":"markdown","a951cfc2":"markdown","89bb07c8":"markdown","6f7d4d4c":"markdown","e0bac1bc":"markdown","40f5717f":"markdown","a051138e":"markdown"},"source":{"1f20ce6d":"#import the usual suspects \nimport re\nimport string\nimport numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt \nfrom textblob import TextBlob\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \nfrom nltk.stem.porter import PorterStemmer \nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score","d65e01b1":"train=pd.read_csv(r'..\/input\/nlp-getting-started\/train.csv')\ntrain.head()","a12055b0":"train.info()","aa591282":"train.isnull().sum()","19ccb9fc":"#get the keyword null values \nkeyword_NaN=train.keyword.isnull().sum()\n#get the location null values \nlocation_NaN=train.location.isnull().sum()\n#show the percentages \nprint(f\"Percentage of null values in Keyword: {round(keyword_NaN\/len(train.keyword)*100,2)} %\")\nprint(f\"Percentage of null values in Location :{round(location_NaN\/len(train.location)*100,2)} %\")","6fb39e80":"train.target.value_counts()","3589f433":"plt.figure(figsize=(8,6))\nsb.countplot(x=\"target\",hue=\"target\",data=train)\nplt.legend()","4bf3cdb7":"#get the length of each tweet \ntrain['length']=train.text.apply(lambda x:len(x))\ntrain.head()","da3c0215":"sb.set_style(\"whitegrid\")\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len_1=train[train['target']==1]['text'].str.len()\nax1.hist(tweet_len_1,color='blue')\nax1.set_title('disaster tweets')\ntweet_len_0=train[train['target']==0]['text'].str.len()\nax2.hist(tweet_len_0,color='red')\nax2.set_title('Non disaster tweets')\nfig.suptitle('Length in tweets')\nplt.show()","6a3dc58b":"#plot the text length againt the target \nsb.barplot(x=\"target\",y='length',hue=\"target\",data=train)\nplt.legend( title='Target', bbox_to_anchor=(1.05, 1), loc='upper left')","d94f9931":"train.text[178:185]","b48a57f8":"#define a cleaner function for the tweets \ndef cleaner(text):\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n","de60cc18":"#add the clean_text column to our datframe \ntrain['clean_text']=train.text.apply(lambda s: cleaner(s))\ntrain.head()","84a0376e":"# compare text and cleaned text \nfrom random import randrange\nn=randrange(len(train))\nprint(train.text[n])\nprint('\\n')\nprint(train.clean_text[n])","d3184e62":"#define a funtion to preprocess the text \nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer() \n# remove stopwords function \ndef preprocess(text): \n    #get english stops words \n    stop_words = set(stopwords.words(\"english\")) \n    #tokenize text \n    word_tokens = word_tokenize(text) \n    #remove stop words \n    filtered_text = [word for word in word_tokens if word not in stop_words] \n    #stemming the text \n    stemmed_text=[stemmer.stem(word) for word in filtered_text]\n    #lemmatizing the text \n    final_text=[lemmatizer.lemmatize(word,pos ='v') for word in stemmed_text]\n    return final_text \n","473df8a0":"#train text split \nX=train['clean_text']\ny=train['target']\ntweet_train, tweet_test, target_train, target_test =train_test_split(X, y,test_size=0.2)","cb68d859":"# HELPER FUNCTIONS FOR THE ENTIRE MODELING PROCESS \n#funtion to build the pipeline \ndef build_pipeline(clf):\n    from sklearn.pipeline import Pipeline\n    pipeline=Pipeline([\n    ('bow',CountVectorizer(analyzer=preprocess)),  # strings to token integer counts\n    ('tfidf',TfidfTransformer() ),  # integer counts to weighted TF-IDF scores\n    ('classifier', clf),  # train on TF-IDF vectors w\/ Naive Bayes classifier\n    ])\n    return pipeline\n\n# function to show evauation metrics \ndef show_metrics(y_test,preds):\n    print(\"Metrics Report : \\n\",classification_report(preds,y_test))\n    print('\\n')\n    print(\"Confusion Matrix : \\n \",confusion_matrix(preds,y_test))\n    print('\\n')\n    print(\"Accuracy Score :\",round(accuracy_score(preds,y_test),2))\n    \n# function to train the models \ndef train_model(x_train,y_train,model):\n        # build model \n        pipeline=build_pipeline(model)\n        #fit the model \n        trained_pipeline= pipeline.fit(x_train,y_train)\n        return trained_pipeline\n#function to make predictions        \ndef predict(y_test,trained_pipeline):\n    #predict with the trained pipeline \n    predictions=trained_pipeline.predict(y_test)\n    #show predictions \n    return predictions \n    ","7bd970ed":"## BEGIN THE PROCESS \n# define a dictionary to instanciate all models \nmodels_to_train={'KNN':KNeighborsClassifier(),'NB':MultinomialNB(),\n                 'DT':DecisionTreeClassifier(),'RFC':RandomForestClassifier()}\npreds_dict={}\nscores=[]\nnames=['KNN','NB' ,'DT','RFC']\nfor name,model in models_to_train.items():\n    #fit the model \n    print(f'TRAINING  {name}  MODEL ... \\n')\n    trained_pipe=train_model(tweet_train,target_train,model)\n    #make predictions \n    preds=predict(tweet_test,trained_pipe)\n    #show metrics \n    preds_dict[name]=preds\n    print(f'METRICS REPORT FOR {name} MODEL : \\n')\n    show_metrics(preds,target_test)\n    scores.append(round(accuracy_score(preds,target_test),2))\n    print(f'{name} : DONE' )\n    print('\\n')\n    print('****************************\\n')\n\n#get the list of classifiers scores \nprint(\"LIST OF MODELS WITH ACCURACIES : \\n\")\nscore_per_classifier=zip(names,scores)\nprint(list(score_per_classifier))","8a7e23e3":"# Build the pipline for tweets classification \nnb_pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=preprocess)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w\/ Naive Bayes classifier\n])\n\n#FIT THE MODEL \nnb_pipeline.fit(tweet_train,target_train)\n#Predict the tweet_test\nnb_preds= nb_pipeline.predict(tweet_test)\nnb_preds","8ce6a99f":"# Run some metrics \nprint(\"Metrics Report \\n :\",classification_report(nb_preds,target_test))\nprint('\\n')\nprint(\"Confusion Matrix :\\n\",confusion_matrix(nb_preds,target_test))\nprint('\\n')\nprint(\"Accuracy Score :\",round(accuracy_score(nb_preds,target_test),3))","f8c9f93b":"# Build the pipline for tweets classification \nknn_pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=preprocess)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier',KNeighborsClassifier()),  # train on TF-IDF vectors w\/ Naive Bayes classifier\n])\n#fit the pipeline \nknn_pipeline.fit(tweet_train,target_train)\n#predict \nknn_preds=knn_pipeline.predict(tweet_test)\nknn_preds","b525f70c":"# show metrics \n# Run some metrics \nprint(\"Metrics Report \\n :\",classification_report(knn_preds,target_test))\nprint('\\n')\nprint(\"Confusion Matrix : \\n\",confusion_matrix(knn_preds,target_test))\nprint('\\n')\nprint(\"Accuracy Score :\",round(accuracy_score(knn_preds,target_test),2))","9b5fe464":"# Build the pipline for tweets classification \nrfc_pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=preprocess)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier',RandomForestClassifier()),  # train on TF-IDF vectors w\/ Naive Bayes classifier\n])\n#fit the pipeline \nrfc_pipeline.fit(tweet_train,target_train)\n#predict \nrfc_preds=rfc_pipeline.predict(tweet_test)\nrfc_preds","e6913a28":"# show metrics \nprint(\"Metrics Report \\n :\",classification_report(rfc_preds,target_test))\nprint('\\n')\nprint(\"Confusion Matrix :\",confusion_matrix(rfc_preds,target_test))\nprint('\\n')\nprint(\"Accuracy Score :\",round(accuracy_score(rfc_preds,target_test),2))","ba7b3251":"dt_pipeline = Pipeline([\n    ('bow', CountVectorizer(analyzer=preprocess)),  # strings to token integer counts\n    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n    ('classifier',DecisionTreeClassifier()),  # train on TF-IDF vectors w\/ Naive Bayes classifier\n])\n#fit the pipeline \ndt_pipeline.fit(tweet_train,target_train)\n#predict \ndt_preds=dt_pipeline.predict(tweet_test)\ndt_preds","1f7c625f":"# show metrics \nprint(\"Metrics Report : \\n \",classification_report(dt_preds,target_test))\nprint('\\n')\nprint(\"Confusion Matrix : \\n\",confusion_matrix(dt_preds,target_test))\nprint('\\n')\nprint(\"Accuracy Score :\",round(accuracy_score(dt_preds,target_test),2))","ebc934a3":"# get the testing \ntest=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ntest.head()","c81d23e0":"test.info()","1a587021":"tweets=test['text']\nclean_tweets=tweets.apply(lambda s:cleaner(s))\n#pass the tweets throug the decision tree pipline \npredictions = nb_pipeline.predict(clean_tweets)\n# show predictions \nprint(predictions)","bcf95442":"#get the sample submission dataframe \nsubmission= pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\n#put our predictions into it \nsubmission.target=predictions\n#save the submission \nsubmission.to_csv('submission.csv',index=False)\nprint(\"Submission saved sucesseully !!! \")","c238aaa5":"### Build all Models at once ","d51706cd":"#### Naive Bayes Classifier","5a286112":"The table scores from the table above shows that the Naive Bayes Classifier did a better performance than other moodels.\nSo we will use it as a chosen model to project our predictions on the test data. ","15ad12bc":"### Preprocessing Data ","6793a167":"#### KNN Classifier","155c7fdd":"### Cleaning Data ","a951cfc2":"\n<table class=\"table table-bordered\">\n    <thead>\n      <tr>\n        <th>Classifier<\/th>\n        <th>\u064eAccuracy Score<\/th>\n      <\/tr>\n    <\/thead>\n    <tbody>\n      <tr>\n        <td>KNN<\/td>\n          <td>0.76 <\/td>\n      <\/tr>\n      <tr>\n          <td>Random Forest Classifier <\/td>\n        <td>0.78 <\/td>\n      <\/tr>\n      <tr  style=\"color: green\" >\n    <td> Naive Bayes<\/td>\n        <td>0.79<\/td>\n        <\/tr>\n      <tr>\n        <td>Decision Tree<\/td>\n    <td>0.74<\/td>\n      <\/tr>\n    <\/tbody>\n<\/table>","89bb07c8":"#### Decision Tree Classifier","6f7d4d4c":"As we can see from the sample above , the text tweets contains a lot of punctuations, digits, http links , whitespaces ...\nSo, let's clean the text ","e0bac1bc":"### Evaluation Results ","40f5717f":"#### Random Forest Classifier","a051138e":"### Coding each Model Separately "}}