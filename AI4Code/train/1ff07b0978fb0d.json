{"cell_type":{"3dbe1cb2":"code","73d56518":"code","0712f7f6":"code","cdff8873":"code","617f2dd2":"code","44fa7de7":"code","9123bb7e":"code","c841cac7":"code","1ce36c24":"code","ebf1120f":"code","f773cacf":"code","77d83e31":"code","e5a1144a":"code","6adf5b39":"markdown","9e6c29fc":"markdown","08dc3ddb":"markdown","3cae4425":"markdown","a9abf63c":"markdown","6c79146e":"markdown","925623ce":"markdown","1e6b4a45":"markdown","b72f8b26":"markdown","dcc29d96":"markdown","c90b3d5a":"markdown","a957ecc8":"markdown"},"source":{"3dbe1cb2":"import math\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense, Dropout, Conv1D, Flatten, LeakyReLU, Activation\nfrom tensorflow.keras.models import Model\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.model_selection import StratifiedKFold\nimport transformers\nfrom transformers import RobertaConfig, TFRobertaModel\nimport tokenizers\n\nprint('TF version: ', tf.__version__)","73d56518":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\nprint(train.info())","0712f7f6":"# Maximum length\nlengths = train['text'].apply(lambda x: len(x)).tolist()\nmax(lengths)","cdff8873":"class config:\n    MAX_LEN = 141\n    PAD_ID = 1\n    PATH = '..\/input\/tf-roberta\/'\n    tokenizer = tokenizers.ByteLevelBPETokenizer(\n        vocab_file = PATH+'vocab-roberta-base.json',\n        merges_file = PATH+'merges-roberta-base.txt',\n        lowercase = True,\n        add_prefix_space = True\n    )\n    sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n    n_splits = 5\n    seed = 42\n    epochs = 3\n    tf.random.set_seed(seed)\n    np.random.seed(seed)\n    label_smoothing = 0.1\n    batch_size = 32","617f2dd2":"ct = train.shape[0]\ninput_ids = np.ones((ct, config.MAX_LEN), dtype='int32')\nattention_mask = np.zeros((ct, config.MAX_LEN), dtype='int32')\ntoken_type_ids = np.zeros((ct, config.MAX_LEN), dtype='int32')\nstart_tokens = np.zeros((ct, config.MAX_LEN), dtype='int32')\nend_tokens = np.zeros((ct, config.MAX_LEN), dtype='int32')\n\nfor k in range(train.shape[0]):\n    # Selected text masking\n    text1 = \" \" + \" \".join(train.loc[k, 'text'].split())\n    text2 = \" \".join(train.loc[k, 'selected_text'].split())\n    \n    selected_idx = text1.find(text2)\n    is_selected = np.zeros((len(text1)))\n    is_selected[selected_idx:selected_idx+len(text2)] = 1\n    if text1[selected_idx-1] == \" \":\n        is_selected[selected_idx-1] = 1\n        \n    enc = config.tokenizer.encode(text1)\n    \n    # IDs start and end offsets (A.K.A.: indexes)\n    offsets = []\n    idx = 0\n    for t in enc.ids:\n        w = config.tokenizer.decode([t])\n        offsets.append((idx, idx+len(w)))\n        idx += len(w)\n        \n    # START and END tokens\n    toks = []\n    for i, (a, b) in enumerate(offsets):\n        verification_sum = np.sum(is_selected[a:b])\n        if verification_sum > 0:\n            toks.append(i)\n            \n    sentiment_tok = config.sentiment_id[train.loc[k, 'sentiment']]\n    input_ids[k, :len(enc.ids)+3] = [0, sentiment_tok] + enc.ids + [2]\n    attention_mask[k, :len(enc.ids)+3] = 1\n    if len(toks) > 0:\n        start_tokens[k, toks[0]+2] = 1\n        end_tokens[k, toks[-1]+2]  = 1","44fa7de7":"test = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')\n\nct = test.shape[0]\ninput_ids_test = np.ones((ct, config.MAX_LEN), dtype='int32')\nattention_mask_test = np.zeros((ct, config.MAX_LEN), dtype='int32')\ntoken_type_ids_test = np.zeros((ct, config.MAX_LEN), dtype='int32')\n\nfor k in range(ct):\n    \n    # Input IDs\n    text1 = \" \" + \" \".join(test.loc[k, 'text'].split())\n    enc = config.tokenizer.encode(text1)\n    \n    sentiment_tok = config.sentiment_id[test.loc[k, 'sentiment']]\n    input_ids_test[k, :len(enc.ids)+5] = [0] + enc.ids + [2, 2] + [sentiment_tok] + [2]\n    attention_mask_test[k, :len(enc.ids)+5] = 1\n\ntest.info()\nprint(test.shape)","9123bb7e":"import pickle\n\ndef save_weights(model, dst_fn):\n    weights = model.get_weights()\n    with open(dst_fn, 'wb') as f:\n        pickle.dump(weights, f)\n        \ndef load_weights(model, weight_fn):\n    with open(weight_fn, 'rb') as f:\n        weights = pickle.load(f)\n        \n    model.set_weights(weights)\n    return model\n\ndef loss_fn(y_true, y_pred):\n    ll = tf.shape(y_pred)[1]\n    y_true = y_true[:, :ll]\n    loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred, from_logits=False, label_smoothing=config.label_smoothing)\n    loss = tf.reduce_mean(loss)\n    return loss\n\n# from https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705\/data?#Load-Libraries,-Data,-Tokenizer\n'''def build_model():\n    ids = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((config.MAX_LEN,), dtype=tf.int32)\n\n    roberta_config = RobertaConfig.from_pretrained(config.PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(config.PATH+\n            'pretrained-roberta-base.h5',config=roberta_config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n\n    x1 = tf.keras.layers.Conv1D(1,1)(x[0])\n    print(x1.shape)\n    x1 = tf.keras.layers.Flatten()(x1)\n    print(x1.shape)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    print(x1.shape)\n    \n    x2 = tf.keras.layers.Conv1D(1,1)(x[0])\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model\n'''\n\n# from https:\/\/www.kaggle.com\/khoongweihao\/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34603010\ndef build_model():\n    ids = Input((config.MAX_LEN,), dtype=tf.int32)\n    att = Input((config.MAX_LEN,), dtype=tf.int32)\n    tok = Input((config.MAX_LEN,), dtype=tf.int32)\n    padding = tf.cast(tf.equal(ids, config.PAD_ID), tf.int32)\n    \n    lens = config.MAX_LEN - tf.reduce_sum(padding, -1)\n    max_len = tf.reduce_max(lens)\n    ids_ = ids[:, :max_len]\n    att_ = att[:, :max_len]\n    tok_ = tok[:, :max_len]\n    \n    roberta_config = RobertaConfig.from_pretrained(config.PATH+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(config.PATH+'pretrained-roberta-base.h5', config=roberta_config)\n    \n    x = bert_model(ids_, attention_mask=att_, token_type_ids=tok_) #for non-padded model: (ids, attention_mask=att, token_type_ids=tok)\n    #print(len(x))\n    #x = tf.convert_to_tensor(x[0])\n    #print(x.shape)\n    #print(type(x))\n    #print(type(x[0]))\n    \n    x1 = Dropout(0.15)(x[0])\n    #print(x1.shape)\n    x1 = Conv1D(768, 2, padding='same')(x1)\n    #print(x1.shape)\n    x1 = LeakyReLU()(x1)\n    #print(x1.shape)\n    x1 = Conv1D(64, 2, padding='same')(x1)\n    #print(x1.shape)\n    x1 = Dense(1)(x1)\n    #print(x1.shape)\n    x1 = Flatten()(x1)\n    #print(x1.shape)\n    x1 = Activation('softmax')(x1)\n    #print(x1.shape)\n    \n    x2 = Dropout(0.15)(x[0])\n    x2 = Conv1D(768, 2, padding='same')(x2)\n    x2 = LeakyReLU()(x2)\n    x2 = Conv1D(64, 2, padding='same')(x2)\n    x2 = Dense(1)(x2)\n    x2 = Flatten()(x2)\n    x2 = Activation('softmax')(x2)\n    \n    model = Model(inputs=[ids, att, tok], outputs=[x1, x2])\n    optimizer = Adam(learning_rate=3e-5)\n    model.compile(loss=loss_fn,\n                  optimizer=optimizer)\n  \n    x1_padded = tf.pad(x1, [[0, 0], [0, config.MAX_LEN - max_len]], constant_values=0.)\n    x2_padded = tf.pad(x2, [[0, 0], [0, config.MAX_LEN - max_len]], constant_values=0.)\n    \n    padded_model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1_padded, x2_padded])\n    \n    return model, padded_model","c841cac7":"def jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    if(len(a)==0) & (len(b)==0):\n        return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","1ce36c24":"%%time\njac = []\nVER = 'v0'\nDISPLAY = 1\n\noof_start = np.zeros((input_ids.shape[0], config.MAX_LEN))\noof_end = np.zeros((input_ids.shape[0], config.MAX_LEN))\n\npreds_start_train = np.zeros((input_ids.shape[0], config.MAX_LEN))\npreds_end_train = np.zeros((input_ids.shape[0], config.MAX_LEN))\npreds_start = np.zeros((input_ids_test.shape[0], config.MAX_LEN))\npreds_end = np.zeros((input_ids_test.shape[0], config.MAX_LEN))\n\nskf = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)\nfor fold, (idxText, idxSentValue) in enumerate(skf.split(input_ids, train.sentiment.values)):\n    print('\\n')\n    print('Fold', (fold+1))\n    print('\\n')\n\n    K.clear_session()\n    model, padded_model = build_model()\n\n    sv = tf.keras.callbacks.ModelCheckpoint(\n        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')\n\n    inputText = [input_ids[idxText,], attention_mask[idxText,], token_type_ids[idxText,]]\n    targetText = [start_tokens[idxText,], end_tokens[idxText,]]\n\n    inputSentValue = [input_ids[idxSentValue,], attention_mask[idxSentValue,], token_type_ids[idxSentValue,]]\n    targetSentValue = [start_tokens[idxSentValue,], end_tokens[idxSentValue,]]\n\n    # Sorting validation data\n    shuffleSentValue = np.int32(sorted(range(len(inputSentValue[0])), key=lambda k: (inputSentValue[0][k] == config.PAD_ID).sum(), reverse=True))\n    inputSentValue = [arr[shuffleSentValue] for arr in inputSentValue]\n    targetSentValue = [arr[shuffleSentValue] for arr in targetSentValue]\n\n    weight_fn = '%s-roberta-%i.h5'%(VER,fold)\n    \n    for epoch in range(1, config.epochs + 1):\n        print('\\n')\n        print('Preparing data.')\n        print('\\n')\n        # add random numbers in order to avoid having the same order in each epoch\n        shuffleText = np.int32(sorted(range(len(inputText[0])), key=lambda k: (inputText[0][k] == config.PAD_ID).sum() + np.random.randint(-3, 3), reverse=True))\n        \n        # shuffle in batches, otherwise short batches will always come in the beginning of each epoch\n        num_batches = math.ceil(len(shuffleText) \/ config.batch_size)\n        batch_idxs = np.random.permutation(num_batches)\n        shuffleText_ = []\n        for batch_idx in batch_idxs:\n            shuffleText_.append(shuffleText[batch_idx * config.batch_size: (batch_idx + 1) * config.batch_size])\n        shuffleText = np.concatenate(shuffleText_)\n        \n        # reorder the input data\n        inputText = [arr[shuffleText] for arr in inputText]\n        targetText = [arr[shuffleText] for arr in targetText]\n        \n        print('\\n')\n        print('Fitting the model')\n        print('\\n')\n        #preds = padded_model.predict([input_ids_test,attention_mask_test,token_type_ids_t],verbose=DISPLAY)\n        model.fit(inputText, targetText, epochs=config.epochs, initial_epoch=epoch - 1, \n                  batch_size=config.batch_size, verbose=DISPLAY, callbacks=[], \n                  validation_data=(inputSentValue, targetSentValue), shuffle=False) #don't shuffle in fit\n        save_weights(model, weight_fn)\n        \n    print('\\n')\n    print('Loading model.')\n    print('\\n')\n    #model.load_weights('%s-roberta-%i.h5'%(VER, fold))\n    load_weights(model, weight_fn)\n\n    print('\\n')\n    print('Predicting OOF.')\n    print('\\n')\n    oof_start[idxSentValue,], oof_end[idxSentValue,] = padded_model.predict([input_ids[idxSentValue,], attention_mask[idxSentValue,], token_type_ids[idxSentValue,]], \n                                                                            verbose=DISPLAY)\n    #oof_start[idxSentValue,], oof_end[idxSentValue,] = model.predict([input_ids[idxSentValue,], attention_mask[idxSentValue,], token_type_ids[idxSentValue,]], \n                                                                     #verbose=DISPLAY)\n    \n    #print('\\n')\n    #print('Predicting all Train for Outlier analysis.')\n    #print('\\n')\n    #preds_train = padded_model.predict([input_ids, attention_mask, token_type_ids], verbose=DISPLAY)\n    #preds_start_train += preds_train[0] \/ skf.n_splits\n    #preds_end_train += preds_train[1] \/ skf.n_splits\n\n    print('\\n')\n    print('Predicting test data.')\n    print('\\n')\n    preds = padded_model.predict([input_ids_test, attention_mask_test, token_type_ids_test], verbose=DISPLAY)\n    #preds = model.predict([input_ids_test, attention_mask_test, token_type_ids_test], verbose=DISPLAY)\n    preds_start += preds[0] \/ skf.n_splits\n    preds_end += preds[1] \/ skf.n_splits\n\n    # display fold jaccard\n    all = []\n    for k in idxSentValue:\n        a = np.argmax(oof_start[k,])\n        b = np.argmax(oof_end[k,])\n\n        if a > b:\n            selected_text = train.loc[k, 'text']\n        else:\n            text1 = \" \" + \" \".join(train.loc[k, 'text'].split())\n            enc = config.tokenizer.encode(text1)\n            selected_text = config.tokenizer.decode(enc.ids[a-2:b-1])\n        all.append(jaccard(selected_text, train.loc[k, 'selected_text']))\n    jac_score = np.mean(all)\n    jac.append(jac_score)\n\n    print('\\n')\n    print('>>>>> FOLD', (fold+1), \": \\n\\tJaccard = \", jac_score)\n    print('\\n')","ebf1120f":"print('Overall 5Fold Cross-Validation Jaccard score:', jac_score)","f773cacf":"out_dir = '..\/output\/model\/'\n","77d83e31":"all = []\nfor k in range(input_ids_test.shape[0]):\n    a = np.argmax(preds_start[k, ])\n    b = np.argmax(preds_end[k, ])\n    \n    if a > b:\n        st = test.loc[k, 'text']\n    else:\n        text1 = \" \" + \" \".join(test.loc[k, 'text'].split())\n        enc = config.tokenizer.encode(text1)\n        st = config.tokenizer.decode(enc.ids[a-2:b-1])\n    \n    all.append(st)","e5a1144a":"print(test.shape)\nprint(len(all))\ntest['selected_text'] = all\ntest[['textID', 'selected_text']].to_csv('submission.csv', index=False)\npd.set_option('max_colwidth', 60)\ntest[['textID', 'selected_text']].sample(25)","6adf5b39":"## Metric","9e6c29fc":"## Test data:","08dc3ddb":"# Kaggle submission","3cae4425":"# Shoutouts:","a9abf63c":"## Imports","6c79146e":"## Config","925623ce":"## Training","1e6b4a45":"# Model","b72f8b26":"## Training data:","dcc29d96":"# Data","c90b3d5a":"I would like to thank immensely the authors of many kernels which led me to fully understand the concepts needed for this submission:\n* https:\/\/www.kaggle.com\/abhishek\/bert-base-uncased-using-pytorch#The-Model\n* https:\/\/www.kaggle.com\/koushiksahu\/roberta-extremely-verbosed-for-beginners\n* https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705\/data?#Load-Libraries,-Data,-Tokenizer\n* https:\/\/www.kaggle.com\/khoongweihao\/tse2020-roberta-cnn-random-seed-distribution?scriptVersionId=34603010\n* https:\/\/www.kaggle.com\/vbmokin\/tse2020-roberta-cnn-outlier-analysis-3chr\n* https:\/\/www.kaggle.com\/tanulsingh077\/deep-learning-for-nlp-zero-to-transformers-bert <- specially rich resource\n\nAlso, check out Abishek's YouTube channel videos, which also helped me a lot: \n* https:\/\/www.youtube.com\/watch?v=6a6L_9USZxg\n* https:\/\/www.youtube.com\/watch?v=XaQ0CBlQ4cY\n\nToDo: implement sentencepiece tokenizer (https:\/\/www.youtube.com\/watch?v=U51ranzJBpY)","a957ecc8":"## roBERTa model:"}}