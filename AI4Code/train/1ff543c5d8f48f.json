{"cell_type":{"9422f8e9":"code","0a4a5ee9":"code","90d7051d":"code","8806bb48":"code","90c770e7":"code","c8bddb87":"code","4f21ea8b":"code","9a3dc677":"code","82896a98":"code","2dd4cf1a":"code","57939914":"code","4434c5c1":"code","0f75f571":"code","cce42996":"code","e878fd52":"code","ac16b840":"code","e0002caa":"code","1256a478":"code","160bfb27":"code","c8661797":"code","81a2656d":"code","0a098eb4":"code","d0cbace6":"code","cdcda4c9":"code","b80255ba":"code","1c1dbdeb":"code","99685186":"code","d3c4c4c5":"code","3ca416f2":"code","8cc8f83b":"code","af6277a9":"code","d720b865":"code","a793dce4":"code","c7dcc308":"code","24a92915":"code","ad7f295f":"code","0b31fd92":"code","bd58f0f8":"code","347bfc5c":"code","aa936c01":"code","f9db1a81":"code","b5a1f30d":"markdown","e16055f2":"markdown","f1b5142d":"markdown","45a8ade8":"markdown","81ad36db":"markdown","d8b40cd8":"markdown","4c9043bd":"markdown","a01b36ab":"markdown","b304f70c":"markdown","4407b450":"markdown","50216003":"markdown","c64e9c5f":"markdown","8fed89f8":"markdown","6dc79824":"markdown","d3a705dc":"markdown","14b7e07a":"markdown","41973c0a":"markdown","c6fe76ae":"markdown","589c478c":"markdown"},"source":{"9422f8e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0a4a5ee9":"# visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\n\n# warnings\nimport warnings\nwarnings.filterwarnings('ignore')","90d7051d":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","8806bb48":"# some information about the dataframe\ntrain.info()","90c770e7":"# Data types of columns\ntrain.dtypes","c8bddb87":"# Summary\ntrain.describe() # by default we get the summary of numerical datatype columns","4f21ea8b":"# Sex feature\nsns.barplot(x=train.Sex, y=train.Survived)\nprint(f'females survival percent: ',train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\nprint(f'males survival percent: ',train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","9a3dc677":"# Pclass feature\nsns.barplot(x=train.Pclass, y=train.Survived)\nprint(f'Pclass 1 survival percent: ',train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\nprint(f'Pclass 2 survival percent: ',train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\nprint(f'Pclass 3 survival percent: ',train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","82896a98":"# checking missing values in train dataset\ntrain.isnull().sum()","2dd4cf1a":"# checking missing values in test dataset\ntest.isnull().sum()","57939914":"# Along with cabin columns we also need to remove a few more columns which will be meaningless in future like PassengerId, Name, Ticket, Cabin\ntrain.drop(columns=['PassengerId','Name','Ticket','Cabin'], axis=1, inplace=True)\ntest.drop(columns=['PassengerId','Name','Ticket','Cabin'], axis=1, inplace=True)","4434c5c1":"# Replace the Sex column male with 1 and female with 0 in train and test datasets\ntrain['Sex'] = [1 if gender=='male' else 0 for gender in train['Sex']]\ntest['Sex'] = [1 if gender=='male' else 0 for gender in test['Sex']]","0f75f571":"train.head()","cce42996":"train.info()","e878fd52":"# filling the missing age values in train and test\ntrain.Age.fillna(train.Age.median(), inplace=True)\ntest.Age.fillna(test.Age.median(), inplace=True)","ac16b840":"# grouping the age in train and test\nbins = [0,10,20,40,60,80,100]\ngroups = [0,2,3,4,5,6]\ntrain['AgeGroup'] = pd.cut(train['Age'], bins, labels=groups, include_lowest=True)\ntest['AgeGroup'] = pd.cut(test['Age'], bins, labels=groups, include_lowest=True)","e0002caa":"# so remove the age column from both train and test\ntrain.drop(columns=['Age'], inplace=True)\ntest.drop(columns=['Age'], inplace=True)","1256a478":"# there is a missing value in the Fare column of test dataset so let's fill that first\ntest.Fare.fillna(test.Fare.median(), inplace=True)","160bfb27":"# Now lets group the fare column for both train and test dataset\nbins = [0,100,200,300,400,500,600]\ngroups = [0,1,2,3,4,5]\ntrain['FareGroup'] = pd.cut(train['Fare'], bins, labels=groups, include_lowest=True)\ntest['FareGroup'] = pd.cut(test['Fare'], bins, labels=groups, include_lowest=True)","c8661797":"# Now remove the Fare column from both datasets\ntrain.drop(columns=['Fare'], axis=1, inplace=True)\ntest.drop(columns=['Fare'], axis=1, inplace=True)","81a2656d":"# we have missing values in Embarked column in train dataset so lets fill that first\ntrain.Embarked.fillna(train.Embarked.mode(), inplace=True)","0a098eb4":"train = pd.get_dummies(train, columns=['Pclass','AgeGroup','Embarked'])\ntest = pd.get_dummies(test, columns=['Pclass','AgeGroup','Embarked'])","d0cbace6":"train.head()","cdcda4c9":"predictors = train.drop(columns=['Survived'], axis=1)","b80255ba":"target = train[['Survived']]","1c1dbdeb":"from sklearn.model_selection import train_test_split","99685186":"x_train,x_val,y_train,y_val = train_test_split(predictors,target,test_size=0.2,random_state=123)","d3c4c4c5":"# lets see the shapes\nprint(x_train.shape)\nprint(x_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","3ca416f2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC","8cc8f83b":"# importing accuracy \nfrom sklearn.metrics import accuracy_score","af6277a9":"lr = LogisticRegression()\nlr.fit(x_train,y_train)\npreds = lr.predict(x_val)\nlr_accuracy = accuracy_score(y_val,preds)\nprint(f'Logistic Regression accuracy: {lr_accuracy*100}')","d720b865":"knn = KNeighborsClassifier()\nknn.fit(x_train,y_train)\npreds = knn.predict(x_val)\nknn_accuracy = accuracy_score(y_val, preds)\nprint(f'KNN accuracy: {knn_accuracy*100}')","a793dce4":"dt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\npreds = dt.predict(x_val)\ndt_accuracy = accuracy_score(y_val, preds)\nprint(f'Decission Tree accuracy: {dt_accuracy*100}')","c7dcc308":"rf = RandomForestClassifier()\nrf.fit(x_train,y_train)\npreds = rf.predict(x_val)\nrf_accuracy = accuracy_score(y_val, preds)\nprint(f'RandomForest accuracy: {rf_accuracy*100}')","24a92915":"gbc = GradientBoostingClassifier()\ngbc.fit(x_train,y_train)\npreds = gbc.predict(x_val)\ngbc_accuracy = accuracy_score(y_val, preds)\nprint(f'GradientBoostClassifier accuracy: {gbc_accuracy*100}')","ad7f295f":"svc = SVC()\nsvc.fit(x_train,y_train)\npreds = svc.predict(x_val)\nsvc_accuracy = accuracy_score(y_val, preds)\nprint(f'SVC accuracy: {svc_accuracy*100}')","0b31fd92":"models = pd.DataFrame({'Model':['LogisticRegression','KNN','DecissionTree','RandomForest','GradientBoostClassifier','SVM'],\n         'Accuracy':[lr_accuracy*100,knn_accuracy*100,dt_accuracy*100,rf_accuracy*100,gbc_accuracy*100,svc_accuracy*100]})\nmodels","bd58f0f8":"data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nids = data['PassengerId']","347bfc5c":"preds = dt.predict(test)","aa936c01":"output = pd.DataFrame({'PassengerId':ids, 'Survived':preds})","f9db1a81":"output.to_csv('submission.csv', index=False)","b5a1f30d":"## Create x_train,x_val,y_train,y_val ","e16055f2":"## One-hot encoding","f1b5142d":"# Exploratory data analysis","45a8ade8":"# KNN","81ad36db":"### Importing rest of the required libraries","d8b40cd8":"# Logistic Regression","4c9043bd":"# Decission Tree","a01b36ab":"# SVM","b304f70c":"# RandomForest","4407b450":"# GradientBoostClassifier","50216003":"# <div style='text-align: center'> Titanic Survival Predictions <\/div>","c64e9c5f":"## Importing models","8fed89f8":"## Missing values","6dc79824":"**We see that there are total 891 records and 12 columns, most of them are numerical. We also have a few columns with highest number of missing values like Survived, Age and Cabin**","d3a705dc":"### lets make dataframe of train and test","14b7e07a":"# Modeling","41973c0a":"## Some facts","c6fe76ae":"### lets create the predictors and target variables from trian dataset","589c478c":"**We see that there are missing values in both train and test datasets. There's a great number of missing values in Cabin column so better we remove the column and fill the rest of missing values**"}}