{"cell_type":{"b7509d28":"code","2e72cb52":"code","5eb55a5a":"code","9a809345":"code","83abcfea":"code","3cd98b5b":"code","3d823e5a":"code","aff438b1":"code","ee164d8c":"code","39527c43":"code","beb6fb18":"code","547c15ad":"code","4fc8134e":"code","384de94c":"code","bac07234":"code","f7f862d9":"code","9f3f5385":"code","aca8db32":"code","17bee0ae":"code","fcdda28d":"code","74c8024a":"code","c73de9b5":"code","6cdfd7b6":"code","9d74d967":"code","b62da3e5":"code","75a05f6f":"code","58e77885":"code","bb69c5ee":"code","3d553844":"code","05598982":"code","a3580d53":"code","2d0a7eae":"code","aeed784d":"code","217e06af":"code","b6c6ed14":"code","5f1bf34d":"code","d745fd7b":"code","8313faee":"code","3d3889de":"code","d2de1cd5":"code","91d56ba9":"code","7b02e3d1":"markdown","b6a7fae1":"markdown","99d39579":"markdown","39789f79":"markdown","0a8e7083":"markdown","62a497e3":"markdown","7e6fb6c8":"markdown","4e697ad4":"markdown","8eb24365":"markdown","1e9bfa9d":"markdown","0ffac35c":"markdown","83bde5a0":"markdown","550450b4":"markdown","aba1cf51":"markdown","4b17264e":"markdown","dfc14e43":"markdown","dd2acd01":"markdown","8f509e91":"markdown","6e67f60a":"markdown","c45322b6":"markdown","51879437":"markdown","2b597c1a":"markdown","c423fefe":"markdown","87f18c49":"markdown","59ff8409":"markdown","88167dcd":"markdown","a08a14a6":"markdown","2fdf1e7b":"markdown","f1f38cbc":"markdown","b4d084d5":"markdown","73c3b153":"markdown","91b4a276":"markdown","bbe319c6":"markdown","4e86e3f5":"markdown","f9a2545e":"markdown","ea0bf365":"markdown","76896875":"markdown","fb998c5c":"markdown","45f8c830":"markdown","687dd0f5":"markdown","7f6dbe2c":"markdown","e8890ad1":"markdown"},"source":{"b7509d28":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport os\nfrom textwrap import wrap\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\nsns.set(font_scale=3)","2e72cb52":"def plot_heatmap(data_in, title_in, number):\n    '''\n    Inputs:\n        data_in: Data Frame of objects and floats;\n        title_in: string, chart title;\n        number: boolean, for showing or not showing number values in heatmap.\n    Output: \n        heatmap chart.\n    '''\n    plt.figure(figsize=(30,20))\n    sns.heatmap(data=data_in, annot=number) \n    plt.title('\\n'.join(wrap(title_in)), fontsize=40, fontweight=\"bold\", pad=40)\n    plt.ylabel(\"\")","5eb55a5a":"def barplot(data_in, x_data, y_data, title_in, hue_in, line):\n   '''\n    Inputs:\n        data_in: Data Frame of objects and floats;\n        x_data: float, x axis, number of jobs positions\n        y_data: object, states names\n        title_in: string, chart title (string);\n        hue_in: float, used values threshhold or None\n        line: float, mean value line for bars.\n    Output: \n        bar chart.\n    '''\n    plt.figure(figsize=(15,14))\n    graph = sns.barplot(x=x_data, y=y_data, hue=hue_in, data=data_in, palette=\"twilight\")\n    plt.title('\\n'.join(wrap(title_in)), fontsize=20, fontweight=\"bold\", pad=20)\n    plt.xlabel(\"Number of jobs positions\")\n    graph.axvline(line)","9a809345":"# Create a list of colors (from iWantHue)\ncolors = [\"#E13F29\", \"#D69A80\", \"#D63B59\", \"#AE5552\", \"#CB5C3B\", \"#EB8076\", \"#96624E\"]\n\ndef plot_pie(data_in, title_in, labels_in):\n    '''\n    Inputs:\n        data_in: Data Frame of objects and floats;\n        title_in: string, chart title;\n        labels_in: object, occupation name.\n    Output: \n        pie chart.\n    '''\n    plt.pie(\n        # using data total arrests\n        data_in,\n        # with the labels being officer names\n        labels=labels_in,\n        # with no shadows\n        shadow=True,\n        # with colors\n        colors=colors,\n        # with one slide exploded out\n        explode=(0.15, 0, 0, 0, 0, 0, 0, 0, 0, 0),\n        # with the start angle at 90%\n        startangle=90,\n        # with the percent listed as a fraction\n        autopct='%1.1f%%',\n        )\n    plt.title(label='\\n'.join(wrap(title_in, 40)), fontsize=20, fontweight=\"bold\", loc=\"center\", pad=20)\n#     sns.set(font_scale=1.3)\n    # View the plot drop above\n    plt.axis('equal')\n\n    # View the plot\n    plt.tight_layout()\n#     plt.savefig('Dakota pie.svg', bbox_inches=\"tight\")\n#     plt.show()","83abcfea":"Automation_data = \"..\/input\/occupation-salary-and-likelihood-of-automation\/automation_data_by_state.csv\"\nA_data = pd.read_csv(Automation_data, encoding = \"ISO-8859-1\")\nstate_names = A_data.columns[3:]","3cd98b5b":"salary_data = \"..\/input\/occupation-salary-and-likelihood-of-automation\/occupation_salary.xlsx\"\nS_data = pd.read_excel(salary_data, index_col=\"OCC_CODE\")\n# S_data.shape\n# S_data.isnull().sum() #only in ANNUAL, HOURLY columns a lot of null values.","3d823e5a":"# link = 'https:\/\/www.infoplease.com\/us\/states\/state-population-by-rank'\n# w = pd.read_html(link, header=0)\n# w[0].columns\n# df_1 = w[0]\n# condition = df_1['State'].isin(state_names)\n# population_0 = df_1[condition]","aff438b1":"link = '..\/input\/uspopulation\/US_population_2.csv'\npopulation = pd.read_csv(link, header=0)\npopulation.head() #- Census population - valstijos visos pupuliacijos skai\u010diai.\n# null ver\u010di\u0173 stulpeliuose n\u0117ra","ee164d8c":"S_data_clean = S_data.drop(['ANNUAL', 'HOURLY'], axis=1)","39527c43":"population_sort = population.sort_values(by=['State'])\nstates_pop = population_sort.loc[:, ['State', 'July 2019 Estimate']].reset_index()","beb6fb18":"us_sum = A_data[state_names].sum(axis=1)\nus_sum_DF = pd.DataFrame({'US_workers':us_sum.values})\nOccupation_proba = A_data[['Occupation', 'Probability']]\nUS_O_proba = Occupation_proba.join(us_sum_DF)\nindex_names = US_O_proba[ US_O_proba['US_workers'] == 0 ].index\nUS_worker = US_O_proba.drop(index_names).reset_index()\ncommon_US_work = US_worker.drop(['index'], axis=1)\n\nUS_work = common_US_work.sort_values(by=['US_workers'], ascending=False).reset_index(drop=True)\nUS_sort_probability = common_US_work.sort_values(by=['Probability'], ascending=False).reset_index(drop=True)\n\nA_head = US_sort_probability.head()\nA_tail = US_sort_probability.tail()\nhighest_lowest_prob = pd.concat([A_head, A_tail])","547c15ad":"A_data_clean = A_data.drop(index_names).reset_index()\nA_data_SOC = A_data_clean.set_index('SOC')","4fc8134e":"A_data_prob_sort = A_data_clean.sort_values(by=['Probability'], ascending=False).reset_index()\nA_head_clean = A_data_prob_sort.head()\nA_tail_clean = A_data_prob_sort.tail()\n\nA_data_highest_lowest_prob = pd.concat([A_head_clean, A_tail_clean])\nA_data_highest_lowest_prob_present = A_data_highest_lowest_prob.drop(['level_0', 'index'], axis=1).set_index('Occupation')","384de94c":"# Transform A_data\ncol_names = A_data_clean.columns\nA_trans = pd.DataFrame(A_data_clean.values.T, columns=A_data_clean['SOC'], index=col_names)\nA_tr = A_trans.iloc[4:]\n\n# Number of occupations jobs in states divided by population of state. Then the data frame transformed back\nreliative_popul = A_tr.div(states_pop['July 2019 Estimate'].values,axis=0)\nreliative_popul = reliative_popul.fillna(0)\nA_double_T = reliative_popul.T\n\n# Join occupation and probability coulumns back to transformed data frame.\nA_data_double_T = A_data_SOC[['Occupation', 'Probability']].join(A_double_T)","bac07234":"# Split SOC column to 'Occupation_group_no' and 'Occupation_no'\nSOC_column = A_data_double_T.reset_index()\nSOC_column['SOC'] = SOC_column.SOC.astype(str)\nSOC_column[['Occupation_group_no','Occupation_no']] = SOC_column.SOC.str.split(\"-\",expand=True,)\nGroup_data = SOC_column.copy()\n\n# Group occupation categories by mean probability and occupations groups ratio to population per state\nGroup = Group_data.groupby(['Occupation_group_no']).Probability.mean()\nGroup_states = Group_data.groupby(['Occupation_group_no'])[state_names].sum()\n\n# Titles of occupation groups\ns = pd.Series(['Management', 'Business Operations', 'Computer and Mathematical', 'Architecture and Engineering', 'Life, Physical, and Social Science', 'Community and Social Service', 'Legal', 'Education, Training, and Library', 'Design, Entertainment and Sports', 'Healthcare Practitioners', 'Healthcare Support', 'Protective Service', 'Food Serving Related', 'Cleaning and Maintenance', 'Personal Care and Service', 'Sales and Related', 'Administrative Support', 'Farming, Fishing, and Forestry', 'Construction and Extraction', 'Installation and Repair', 'Production', 'Transportation'], index=['11', '13', '15', '17', '19', '21', '23', '25', '27', '29', '31', '33', '35', '37', '39', '41', '43', '45', '47', '49', '51', '53'])\n\nOccupations_groups_pd = pd.DataFrame({'index':s.index, 'Occupations groups':s.values}) # Make data frame from series\nOccupations_groups = Occupations_groups_pd.set_index('index')\n\n# Join data to one table and sort by probability\nOccupations_groups_join1 = Occupations_groups.join(Group)\nOccupations_groups_join2 = Occupations_groups_join1.join(Group_states)\nOccupations_groups_prob_sort1 = Occupations_groups_join2.sort_values(by=['Probability'], ascending=False).reset_index()\nOccupations_groups_prob_sort = Occupations_groups_prob_sort1.fillna(0)\nOccupations_groups_prob_sort.head()\n\n# Merge Occupations groups and probability columns to one\nOccupations_groups_prob_round = Occupations_groups_prob_sort['Probability'].round(2)\nOccupations_groups_prob_round_df = pd.DataFrame({'Probability':Occupations_groups_prob_round.values})\nOccupations_groups_prob_sort[\"Occupations groups and Probability\"] = Occupations_groups_prob_sort[\"Occupations groups\"] + \" \" + Occupations_groups_prob_round_df[\"Probability\"].astype(str)\nOccupations_groups_plot = Occupations_groups_prob_sort.copy().set_index('Occupations groups and Probability')","f7f862d9":"plot_heatmap(Occupations_groups_plot[state_names], 'Occupation categories and the probability of automation by states', False)","9f3f5385":"plot_heatmap(Occupations_groups_plot[['South Dakota', 'Nevada', 'District of Columbia', 'Massachusetts']],'Occupation categories and probability of automatisation in S. Dakota, Nevada, DC and Massachusetts', True)","aca8db32":"plot_heatmap(A_data_highest_lowest_prob_present[state_names], 'Workers numbers in occupations with 5 highest and 5 lowest probabilities for automation', False)","17bee0ae":"# plt.title(\"US workers numbers and Occupations automatisation probabilities distribution\")\nsns.set_style(\"dark\")\nplt.figure(figsize=(20,20))\nfig = sns.jointplot(x=US_work['Probability'], y=US_work['US_workers'], kind=\"kde\")\n# plt.ylabel(\"Occupations numbers\")\nsns.set(font_scale=1.2)\n# ax.set_ylim(1,31)\n# ax.set_yticks(range(1,32))\n# g.despine(bottom=True, left=True)\nplt.gcf().set_size_inches(12, 8)\nplt.show() ","fcdda28d":"threshhold = 0.7 # accupations automation probability threshhold","74c8024a":"p = A_data_clean.sort_values(by=['Probability'], ascending=False)\nsum_work_per_state = p[state_names].sum()\nStates_sum_DF = pd.DataFrame({'States':sum_work_per_state.index, 'sum_work_positions':sum_work_per_state.values})\nStates_sum_sort =  States_sum_DF.sort_values(by=['sum_work_positions'], ascending=False)","c73de9b5":"barplot(States_sum_sort, 'sum_work_positions', 'States', \"Total number of jobs positions per state\", None, 0)","6cdfd7b6":"p_index = p.reset_index().drop(['level_0', 'index', 'SOC'], axis=1)\np05 = p_index.loc[(p_index.Probability < threshhold)]","9d74d967":"sum_work_per_state05 = p05[state_names].sum()\nStates_sum_DF05 = pd.DataFrame({'States':sum_work_per_state05.index, 'sum_work_positions':sum_work_per_state05.values})\nStates_sum_sort05 =  States_sum_DF05.sort_values(by=['sum_work_positions'], ascending=False)\nStates_sum_sort05.head()","b62da3e5":"barplot(States_sum_sort05, 'sum_work_positions', 'States', \"Total number of jobs positions per state\", None, 0)","75a05f6f":"States_sum_DF['Threshhold'] = 1.0\nStates_sum_DF05['Threshhold'] = threshhold\nStates_sum_DF05.head()","58e77885":"Compare_sums = pd.concat([States_sum_DF, States_sum_DF05])\nCompare_sums_sort = Compare_sums.sort_values(by=['sum_work_positions'], ascending=False)\nCompare_sums_sort","bb69c5ee":"barplot(Compare_sums_sort, 'sum_work_positions', 'States', \"Total number of jobs positions per state now (Threshhold=1.0) and when higher automation probability accupations lost (Threshhold=threshhold)\", 'Threshhold', 0)","3d553844":"States_sum_join = States_sum_DF.join(States_sum_DF05, lsuffix='', rsuffix='0.5')\nStates_sum_drop = States_sum_join.drop(['Threshhold', 'States0.5', 'Threshhold0.5'], axis=1)\nRelative_jobs_drop = ((States_sum_drop['sum_work_positions']-States_sum_drop['sum_work_positions0.5'])\/States_sum_drop['sum_work_positions'])*100\nStates_sum_drop.head()","05598982":"Relative_jobs_drop_DF = pd.DataFrame({'Lost jobs ratio':Relative_jobs_drop.values})\nRelative_jobs_drop_States = States_sum_drop.join(Relative_jobs_drop_DF)\nRelative_jobs_drop_States_sort = Relative_jobs_drop_States.sort_values(by=['sum_work_positions'], ascending=False)\nRelative_jobs_drop_mean = Relative_jobs_drop_States_sort['Lost jobs ratio'].mean()\nRelative_jobs_drop_mean","a3580d53":"barplot(Relative_jobs_drop_States_sort, 'Lost jobs ratio', 'States', \"Lost jobs ratio per state when we lost jobs with automatisation probability equal to 0.7 (Threshhold) or higher\", None, Relative_jobs_drop_mean)","2d0a7eae":"Relative_jobs_drop_States_highest = Relative_jobs_drop_States.sort_values(by=['Lost jobs ratio'], ascending=False)","aeed784d":"barplot(Relative_jobs_drop_States_highest, 'Lost jobs ratio', 'States', \"Lost jobs ratio per state when we lost jobs with automatisation probability equal to 0.7 (Threshhold) or higher\", None, Relative_jobs_drop_mean)","217e06af":"nevada = A_data_clean[['Occupation', 'Probability', 'South Dakota', 'Nevada']].sort_values(by=['Probability'], ascending=False)\nnevada07_full = nevada.loc[(nevada.Probability >= threshhold)].sort_values(by=['Nevada'], ascending=False).reset_index()\nnevada07 = nevada07_full.head(9)\nnevada07_tail = nevada07_full.tail(308).Nevada.sum()\ndf2 = {'Occupation': 'Other', 'Probability': 0, 'South Dakota': 0, 'Nevada': nevada07_tail} \nnevada07 = nevada07.append(df2, ignore_index = True) \nnevada07","b6c6ed14":"plot_pie(nevada07['Nevada'], \"The largest most likely automatable occupations in Nevada\", nevada07['Occupation'])","5f1bf34d":"dakota = A_data_clean[['Occupation', 'Probability', 'South Dakota']].sort_values(by=['Probability'], ascending=False)\ndakota07_full = dakota.loc[(dakota.Probability >= threshhold)].sort_values(by=['South Dakota'], ascending=False).reset_index()\ndakota07 = dakota07_full.head(9)\ndakota07_tail = dakota07_full.tail(308)['South Dakota'].sum()\ndf2 = {'Occupation': 'Other', 'Probability': 0, 'South Dakota': dakota07_tail} \ndakota07 = dakota07.append(df2, ignore_index = True) \ndakota07","d745fd7b":"plot_pie(dakota07['South Dakota'], \"The largest most likely automatable occupations in South Dakota\", dakota07['Occupation'])","8313faee":"DC = A_data_clean[['Occupation', 'Probability', 'District of Columbia']].sort_values(by=['Probability'], ascending=False)\nDC07_full = DC.loc[(DC.Probability >= threshhold)].sort_values(by=['District of Columbia'], ascending=False).reset_index()\nDC07 = DC07_full.head(9)\nDC07_tail = DC07_full.tail(308)['District of Columbia'].sum()\ndf2 = {'Occupation': 'Other', 'Probability': 0, 'District of Columbia': DC07_tail} \nDC07 = DC07.append(df2, ignore_index = True) \nDC07","3d3889de":"plot_pie(DC07['District of Columbia'], \"The largest most likely automatable occupations in District of Columbia\", DC07['Occupation'])","d2de1cd5":"massachusetts = A_data_clean[['Occupation', 'Probability', 'Massachusetts']].sort_values(by=['Probability'], ascending=False)\nmassachusetts07_full = massachusetts.loc[(nevada.Probability >= threshhold)].sort_values(by=['Massachusetts'], ascending=False).reset_index()\nmassachusetts07 = massachusetts07_full.head(9)\nmassachusetts07_tail = massachusetts07_full.tail(308).Massachusetts.sum()\ndf2 = {'Occupation': 'Other', 'Probability': 0, 'Massachusetts': massachusetts07_tail} \nmassachusetts07 = massachusetts07.append(df2, ignore_index = True) \nmassachusetts07","91d56ba9":"plot_pie(massachusetts07['Massachusetts'], \"The largest most likely automatable occupations in Massachusetts\", massachusetts07['Occupation'])","7b02e3d1":"Transform occupation workers numbers per state to occupation workers population ratio. ","b6a7fae1":"# Plot second bar to every state - left absolute occupation positions when threshhold < 0.7.","99d39579":"# Automation_of_occupations_consequences_for_the_USA","39789f79":"Plot pie chart of Nevada data","0a8e7083":"# patrumpint ilgiausiu spec pav. su kodu\nsurast reikiamas specialybes ir jas replace.","62a497e3":"Probability higher than 0.7  representing a \"high risk category, meaning that associated occupations are potentially automatable over some unspecified number of years, perhaps a decade or two\" according to the original research paper. We can look to this probability as to a time frame, where higher propabilty occupations are likely to be automated sooner.","7e6fb6c8":"Sum up all occupation workers for the US. After that I drop occupation lines, where are zero workers in all states. Then I sort data by US workers' numbers and by probability. Take 5 occupations with highest and 5 occupations with lowest automation probabilities.","4e697ad4":"Drop emty columns: ANNUAL, HOURLY","8eb24365":"## Population data","1e9bfa9d":"# Conclusions","0ffac35c":"Import \"Occupation salary\"","83bde5a0":"# South Dakota and Nevada have most jobs losses. Lets look, what are biggest occupations they lost","550450b4":"# II. Data cleaning and preparation for visualisations","aba1cf51":"# How many jobs positions would be lost in States, if we lost accupations which have automation probability equal to 0.7 or higher.","4b17264e":"Questions to analyse:\n\n1. Which occupatios are the most sensitive and the most robust to automatisation (computerisation)?\n2. See, how looks data distribution\n3. What is the jobs loss in the US, if automatisation take out occupations with automatisation probability equal to 0.7 or higher?\n4. Which US states are the most sensitive and the most robust to automatisation?\n5. Compare most common occupations or automatisation","dfc14e43":"Nobody has a crystal ball that can tell the future, but some people don\u2019t need an ancient relic to foresee what\u2019s going to haven, because they are currently building the future in which we will all live.\n\nIt\u2019s true that AI and Automation will wreak havoc among the workforce rending a large part of the population useless and without economic value.\n\nNot only they will take your jobs, but they will make the rich even richer.\n\nIf you are looking for job opportunities which are less likely to be affected by AI or automation, well you\u2019re in the right place.\n\nThat said, it might be wise to consider some of the fields which will see an uptick in productivity in the following years.","dd2acd01":"Lets look, what are reliative loss numbers","8f509e91":"Then I opened the US_population csv here:","6e67f60a":"Plot heat map","c45322b6":"1. The most robust occupations for automatisations: Saugiausios specialyb\u0117s nuo automatizacijos: social service, management, computer and mathematical and medicine. The most sensitive: administrative support, production, farming, fishing, forestry, food serving related.\n2. A bit more occupations have higher probability for automatisation\n3. US would loss around half of all jobs, if automatisation take out occupations with automatisation probability equal to 0.7 or higher?\n4. South Dakota and Nevada are the most sensitive and District of Columbia and Massachusetts are the most robust to automatisation.\n5. Nevada and South Dakota have most occupations with high probability for automation. District of Columbia also has many occupations with low probability for automatisation like management, arts and protective service.","51879437":"To compare occupation numbers in percentage, I need USA population data.\nI couldn't read html in the Kaggle platorm, so I made csv data file in pycharm. The code here:","2b597c1a":"Massachusetts pie chart**","c423fefe":"Massachusetts data","87f18c49":"## Automations data (A_data) preparation","59ff8409":"## Total number of jobs positions per state","88167dcd":"Now let's look to South Dakota","a08a14a6":"5 occupations with highest probability for automatisation, and 5 with lowest probability for automatisation","2fdf1e7b":"# III. Data analysis topic\n# IV. Data analysis topic2","f1f38cbc":"District of Columbia has lowest sensitivity for automation. Let's check the data","b4d084d5":"Plot pie chart of South Dakota data","73c3b153":"Import \"Automation data by state\"","91b4a276":"Sort data by states and take only state and population columns","bbe319c6":"Drop zero lines from (A_data) and set column \"SOC\" as index.","4e86e3f5":"Group occupation to categories by mean probability and occupations groups ratio to population per state.","f9a2545e":"I estimated, that these total job numbers are about 10%, due to jabs position not included. I got data with all jobs where data was not available or there were less than 10 employees were marked as zero.","ea0bf365":"# I. Data import and functions","76896875":"The lost work positions and state population ratio. I take lost work position, when work automation probability ir equal to 0.7 or higher (threshhold >= 0.7).","fb998c5c":"# Heat map plot function\n# Barplot function\n# plt.pie function","45f8c830":"Let's start with Nevada data","687dd0f5":"# **Ar sita distribution grafika palikt**\nPo to pasinaudot atrnkant didel\u0117s automatizavimo specialybes ir turincias dideli skaiciu darbuotoju\nUS occupations number ant accupations probabilities distribution","7f6dbe2c":"## ????Salary data (S_data)","e8890ad1":"District of Columbia pie chart"}}