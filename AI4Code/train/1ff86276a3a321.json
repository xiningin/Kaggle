{"cell_type":{"ae890b8c":"code","dfea6598":"code","25ec2da3":"code","6167fc53":"code","768992cc":"code","1af6643c":"code","851f6aee":"code","f0ab76df":"code","b88bf662":"code","990a8e6d":"code","70c54f4c":"code","5ff6c9c2":"code","03d95c7b":"code","b3475052":"code","22eb0243":"code","9e0df719":"code","9cdf92d7":"code","a8568f8c":"code","fb371a06":"code","837ef39f":"markdown","75942260":"markdown","c64d1f6e":"markdown","9646820f":"markdown","6f949eab":"markdown","4843a554":"markdown","fb63bb08":"markdown","8e1b82ac":"markdown","a6b83f0a":"markdown","3f3f5fbb":"markdown","648b69fb":"markdown","882a5953":"markdown","644b30b9":"markdown","5d8a9ed6":"markdown","dc1f101d":"markdown","0bdd5184":"markdown","c7725046":"markdown"},"source":{"ae890b8c":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","dfea6598":"df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf.head()","25ec2da3":"# lets take out first the label\ntrain_y = df['Survived']\ntrain_y.head()","6167fc53":"# function to filter the age, sex, fare pclass, sibsp, parch columns\ndef get_data(data):\n    # take only this specific column\n    data = data[['Age', 'Sex', 'Fare', 'Pclass', 'SibSp', 'Parch']]\n    \n    # replace male by 1, female by 0\n    data.replace({ 'male' : 1, 'female' : 0 }, inplace=True)\n    \n    # replace null\/nan data by the mean (age and fare columns)\n    data['Fare'].fillna(int(data['Fare'].mean()), inplace=True)\n    data['Age'].fillna(int(data['Age'].mean()), inplace=True)\n    \n    # transform into a numpy array\n    data = data.to_numpy().astype(float)\n    \n    # normalize (make sure the data is between -1 and 1)\n    for i in range(data.shape[1]):\n        data[:,i] = (data[:,i] - data[:,i].mean()) \/ data[:,i].std()\n    \n    return data","768992cc":"train_x = get_data(df)\nprint(\"Train data shape:\", train_x.shape)","1af6643c":"# same for the labels (contains 0 - 1 if the victim survived or not)\nprint(\"Label data shape:\", train_y.shape)","851f6aee":"# the activation function and derivative of the action function\ndef sigmoid(x):\n    return 1\/(1+np.exp(-x))\n\ndef dsigmoid(x):\n    return sigmoid(x) * (1 - sigmoid(x))","f0ab76df":"# the loss function and its derivative\ndef loss_fn(y, y_hat):\n    return 1\/2 * (y - y_hat) ** 2\n\ndef dloss_fn(y, y_hat):\n    return (y - y_hat)","b88bf662":"# number of rows\ninstances = train_x.shape[0]\n\n# number oof columns\nattributes = train_x.shape[1]\n\n# number of hidden node for first layer \nhidden_nodes = 8\n\n# number of hidden node for second layer\nhidden_nodes_two = 4\n\n# number of output labels \noutput_labels = 1","990a8e6d":"# Inititate the weights\/biases\nw1 = np.random.rand(attributes,hidden_nodes)\nb1 = np.random.randn(1, hidden_nodes)\n\nw2 = np.random.rand(hidden_nodes,hidden_nodes_two)\nb2 = np.random.randn(1, hidden_nodes_two)\n\nw3 = np.random.rand(hidden_nodes_two, output_labels)\nb3 = np.random.randn(1, output_labels)\n\ntheta = w1, w2, w3, b1, b2, b3","70c54f4c":"# Neural Network Forward\ndef forward(x, theta):\n    w1, w2, w3, b1, b2, b3 = theta\n    \n    k = np.dot(x, w1) + b1\n    l = sigmoid(k)\n    \n    m = np.dot(l, w2) + b2\n    n = sigmoid(m)\n    \n    o = np.dot(n, w3) + b3\n    p = sigmoid(o)\n    \n    return k, l, m, n, o, p","5ff6c9c2":"# Neural Network Backward\ndef backward(x, y, sigma, theta):\n    k, l, m, n, o, p = sigma\n    w1, w2, w3, b1, b2, b3 = theta\n    \n    # db3 = dloss * dsigm(o) * 1\n    # dw3 = dloss * dsigm(o) * n\n    \n    # db2 = dloss * dsigm(o) * w3 * dsigm(m) * 1\n    # dw2 = dloss * dsigm(o) * w3 * dsigm(m) * l\n    \n    # db1 = dloss * dsigm(o) * w3 * dsigm(m) * w2 * dsigm(k) \n    # dw1 = dloss * dsigm(o) * w3 * dsigm(m) * w2 * dsigm(k) * x\n    \n    dloss = dloss_fn(p, y)\n    dsigm_p = dsigmoid(o)\n    dsigm_n = dsigmoid(m)\n    dsigm_l = dsigmoid(k)\n    \n    db3 = dloss * dsigm_p\n    dw3 = np.dot(n.T, db3)\n    \n    db2 = np.dot(db3, w3.T) * dsigm_n\n    dw2 = np.dot(l.T, db2)\n    \n    db1 = np.dot(db2, w2.T) * dsigm_l\n    dw1 = np.dot(x, db1)\n    \n    return dw1, dw2, dw3, db1, db2, db3","03d95c7b":"# use the avg of the gradients for the derivative of each bias\ndef avg_bias(grads):\n    dw1, dw2, dw3, db1, db2, db3 = grads\n    db1 = db1.mean(axis=0)\n    db2 = db2.mean(axis=0)\n    db3 = db3.mean(axis=0)\n    return dw1, dw2, dw3, db1, db2, db3","b3475052":"# Use the SGD in order to optimize the weights and biases\ndef optimize(theta, grads, lr=0.001):\n    dw1, dw2, dw3, db1, db2, db3 = grads\n    w1, w2, w3, b1, b2, b3 = theta\n    \n    w1 -= dw1 * lr\n    w2 -= dw2 * lr\n    w3 -= dw3 * lr\n    b1 -= db1 * lr\n    b2 -= db2 * lr\n    b3 -= db3 * lr\n    \n    return w1, w2, w3, b1, b2, b3","22eb0243":"# return 1 if the prediction is higher than 0.5\n# return 0 if not\ndef predict(x, theta):\n    predict = forward(x, theta)[-1]\n    return np.where(predict > 0.5, 1, 0)","9e0df719":"# time to train our model\nfor epoch in range(1000):\n    \n    for i in range(len(train_x)):\n        sigma = forward(train_x[i], theta)\n        grads = backward(train_x[i].reshape(6,1), train_y[i], sigma, theta)\n        theta = optimize(theta, avg_bias(grads))\n    \n    if(epoch % 100 == 0):\n        loss = loss_fn(sigma[-1], train_y[i]).mean()\n        print(\"Epoch:{:3d}, Loss:{:1.3f}\"\n                 .format(epoch, loss))","9cdf92d7":"test_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest = get_data(test_df)","a8568f8c":"# Get test data predictions\ntest_preds = predict(test, theta).reshape(-1)\n\n# Add passengers ids to the test predictions\npassenger_ids = test_df['PassengerId'].to_numpy()\n\n# combine passenger ids with the predictions\nfinal_result = np.array(list(map(list, zip(passenger_ids, test_preds))))","fb371a06":"# arraay final_result to dataframe\ndf_final = pd.DataFrame(data=final_result, columns=[\"PassengerId\", \"Survived\"])\n\n# save the result\ndf_final.to_csv('submission.csv', index=False)","837ef39f":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:yellow; background:black; border:1px dashed yellow;\" role=\"tab\" aria-controls=\"home\"><center>Submission<\/center><\/h3>","75942260":"## Backward function","c64d1f6e":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:yellow; background:black; border:1px dashed yellow;\" role=\"tab\" aria-controls=\"home\"><center>Prediction<\/center><\/h3>","9646820f":"## Optimization","6f949eab":"# Neural Network","4843a554":"# Predict & Train","fb63bb08":"# Prepare Data","8e1b82ac":"Shape will show us the number of rows and columns (891 and 6)","a6b83f0a":"To make a quick neural network using the data above,<br>\nwe can easily create a neural network using the following the columns:<br>\n'**Age**', '**Sex**', '**Fare**', '**Pclass**', '**SibSp**', '**Parch**'","3f3f5fbb":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:yellow; background:black; border:1px dashed yellow;\" role=\"tab\" aria-controls=\"home\"><center>Implementation<\/center><\/h3>","648b69fb":"## Loss function","882a5953":"## Activation function","644b30b9":"# Notebook Goal\n\nClassic Titan notebook implemented with a Neural Network from Scratch based on each passenger survival.","5d8a9ed6":"<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style=\"color:yellow; background:black; border:1px dashed yellow;\" role=\"tab\" aria-controls=\"home\"><center>Data<\/center><\/h3>","dc1f101d":"<div style=\"height:200px;width:100%;margin: 0;\">\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/3136\/logos\/header.png\" style=\"width:100%;\" \/>\n<\/div>","0bdd5184":"## Forward function","c7725046":"## Parameters"}}