{"cell_type":{"d43fc739":"code","7b276024":"code","38e70208":"code","456edb88":"code","c24f3544":"code","11079446":"code","f277e985":"code","34f4f0f9":"code","db3045c5":"markdown","837350e2":"markdown","02ebce41":"markdown","8b0afdea":"markdown","f06b1363":"markdown","40bd713a":"markdown","677c510e":"markdown"},"source":{"d43fc739":"#Some standard imports\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# Some more specific imports.\n\nfrom sklearn.metrics import accuracy_score, auc, balanced_accuracy_score, confusion_matrix, f1_score, precision_score, average_precision_score, roc_auc_score,  recall_score,  precision_recall_curve #some scoring functions\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, IsolationForest# Some classifiers\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, cross_validate, train_test_split #Cross validation tools, and a train\/test split utility\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV #Hyper parameter search tools\nfrom hyperopt import hp, fmin, Trials, tpe, STATUS_OK\nimport joblib\nfrom imblearn.under_sampling import  RandomUnderSampler\n","7b276024":"df = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\", delimiter=',')\ndf.dataframeName = 'creditcard.csv'","38e70208":"X = df.iloc[:, 1:30]\ny = df.iloc[:, 30:31]\n\nX.Amount = StandardScaler().fit_transform(X.Amount.values.reshape(-1,1))\n\n\n# Let's split our dataset and see what it looks like:\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size = 0.25)\ntraining_fraud = sum(y_train.values)\ntraining_fraud_pct = sum(y_train.values)\/len(y_train.values)*100\ntest_fraud = sum(y_test.values)\ntest_fraud_pct = sum(y_test.values)\/len(y_test.values)*100\n# Let's split our dataset and see what it looks like:\nX_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, stratify=y, test_size = 0.25)\ntraining_fraud = sum(y_train.values)\ntraining_fraud_pct = sum(y_train.values)\/len(y_train.values)*100\ntest_fraud = sum(y_test.values)\ntest_fraud_pct = sum(y_test.values)\/len(y_test.values)*100\nprint(\"X train: {}\\nX test:  {}\\ny_train: {}\\ny test:  {} \\nFraud in train set: {},   {:2f}%\\nFraud in test set:  {},  {:2f}%\\n\".format( X_train.shape, \n                                                                                                                      X_test.shape, \n                                                                                                                      y_train.shape, \n                                                                                                                      y_test.shape, \n                                                                                                                      training_fraud[0], training_fraud_pct[0],\n                                                                                                                      test_fraud[0], test_fraud_pct[0]))\n","456edb88":"from imblearn.under_sampling import RandomUnderSampler\nX_resample, y_resample = RandomUnderSampler(.005).fit_resample(X_train, y_train.values.ravel())\nprint(X_resample.shape, y_resample.shape, np.sum(y_resample))\n","c24f3544":"joblib.dump((X_resample, y_resample), 'resampled_data')\nX_resample, y_resample = joblib.load('resampled_data')\n\nX_train.shape","11079446":"# let's start over again...\n# but this time, our parameter grid will look a bit different. \n\nparam_grid = {'max_depth':[ 10, 13, 15],\n              'min_samples_split':[3,4,5,6],}\n              \nclf = RandomForestClassifier(n_jobs=-1,n_estimators=500,  verbose=5)                      # We declare an instance of our classifier\n                                                                                        # But instead of fitting it, We pass it, \nclf_cv = GridSearchCV(clf, param_grid, scoring=\"average_precision\",                     # (and our parameter grid) to a new instance\n                      n_jobs=-1, cv=5, verbose=5)                                             # of a grid search object\n","f277e985":"clf_cv.fit(X_resample, y_resample)\nfilename1 , filename2 = 'grid_optimized_random_forest_clf.joblib',  'grid_optimized_random_forest_clf_cv.joblib'\njoblib.dump(clf, filename1)\njoblib.dump(clf_cv, filename2)","34f4f0f9":"joblib.load('grid_optimized_random_forest_clf_cv.joblib')\nresults = pd.DataFrame(clf_cv.cv_results_)\nresults.sort_values(by='rank_test_score')\n\n\n","db3045c5":"## Cross-Validation, Hyperparameters and Modeling\n\nI put this together as a learning exercise for myself, but I hope that others can benefit from the examples that I've strung together. \n\nIf anyone finds it useful, please let me know.  \n","837350e2":"y_pred = clf_cv.predict(X_test)\naverage_precision_score(y_test, y_pred)\nclf_cv.best_params_","02ebce41":"from hyperopt import hp, tpe, STATUS_OK, fmin, Trials #Hyperparameter search using a loss function \n\nhype_grid = {'max_depth': hp.randint('max_depth', 5, 30), \n              'max_features': hp.randint('max_features', 5, 30),\n              'n_estimators': hp.randint('n_estimators',100, 500),\n              'min_samples_split':hp.randint('min_samples_split', 2,5)}\nclf = RandomForestClassifier( verbose=3) \nclf_cv =RandomizedSearchCV(clf, param_grid, scoring=\"average_precision\",     \n                           n_jobs=4, verbose=3, n_iter=5)                  \n                    ","8b0afdea":"joblib.load(filename1)\njoblib.load(filename2)","f06b1363":"# let's start over again...\n# but this time, our parameter grid will look a bit different. \nfrom scipy.stats import randint as sp_rand_int\n\nparam_grid = {'max_depth': sp_rand_int(5,30), \n              'max_features': sp_rand_int(5,30),\n              'n_estimators':sp_rand_int(100,500),\n              'min_samples_split':sp_rand_int(2,5)}\nclf = RandomForestClassifier( verbose=3) \nclf_cv =RandomizedSearchCV(clf, param_grid, scoring=\"average_precision\",     \n                           n_jobs=4, verbose=3, n_iter=5)                  \n                    ","40bd713a":"RandomSearchCV is a bit different. Instead of testing every combination of parameters, it tests random combinations parameters a preset number of times. \n\nFor more information, SciKit Learn has a [more detailed explanation.](https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_randomized_search.html#sphx-glr-auto-examples-model-selection-plot-randomized-search-py)\n\n","677c510e":"clf_cv.fit(X_train, y_train)\nfilename1 , filename2 = 'rand_optimized_random_forest_clf.joblib',  'rand_optimized_random_forest_clf_cv.joblib'\njoblib.dump(clf, filename1)\njoblib.dump(clf_cv, filename2)"}}