{"cell_type":{"427dacdf":"code","7532f817":"code","0b102fda":"code","733710f9":"code","a3da3c7e":"code","6e71efc2":"code","249601b8":"code","923ec187":"code","31be5572":"code","9a0398d5":"code","764376f3":"code","655730f2":"code","86e57bdf":"code","d932072b":"code","4cb2236a":"code","4d68b29e":"code","7281eaea":"code","5476d38b":"code","3821cbf4":"code","39784753":"code","6342b243":"code","394ba35f":"code","ad8ff3a5":"code","b6fa7041":"markdown","38a41b76":"markdown","3b91a8b0":"markdown","e1472a0e":"markdown"},"source":{"427dacdf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7532f817":"import matplotlib.pyplot as pp\n%matplotlib inline","0b102fda":"# Lets create some artificial data for this \nX = np.linspace(1,10, num=50)\nnp.random.seed(101)\nr = r = np.random.uniform(-1, 1, (50,)) *20\ny = (X ** 2) + r\ndf = pd.DataFrame(np.column_stack((X,y)), columns=['feature', 'target'])","733710f9":"df.head()","a3da3c7e":"fig, ax = pp.subplots()\nax.scatter(df['feature'], df['target'], color='r', marker='x')\nax.set_xlabel(df.columns[0])\nax.set_ylabel(df.columns[1])","6e71efc2":"def costfunc(theta, X, y):\n    m = X.shape[0]\n    y_pred = X.dot(theta)\n    J = (((y_pred-y) ** 2).mean()) \/ 2\n    return J","249601b8":"def mapFeatures(X, degree):\n    X_poly = X\n    for i in range(2, degree + 1):\n        X_poly = np.column_stack((X_poly, X ** i))\n    return X_poly","923ec187":"# Lets start by defining the degree 1\ndegree = 1","31be5572":"# get our feature and target variables\nX = df['feature'].values\ny = df['target'].values","9a0398d5":"# generate additonal features (if applicable)\nX = mapFeatures(X, degree)\n\n# Because the degree was 1, we did not generate any polynomial features as confirmed by the shape below\nX.shape","764376f3":"# Add the intercept term\nX = np.column_stack((np.ones(X.shape[0]), X))\nX.shape","655730f2":"# Set initial values of our parameters\ninitial_theta = np.ones(degree + 1).reshape(degree + 1,1)\ninitial_theta.shape","86e57bdf":"# Now we minimize our cost function using scipy\nfrom scipy.optimize import minimize","d932072b":"res = minimize(costfunc, initial_theta, args=(X,y))","4cb2236a":"# Optimized values of coefficients\ntheta = res.x\ntheta","4d68b29e":"# now create points to visualize the fit\nX_pred = np.linspace(X[:,1].min(), X[:,1].max(), 10)\nX_pred = np.column_stack((np.ones(X_pred.shape[0]), mapFeatures(X_pred, degree)))\n\n# Now use our hypothesis to get values of y_pred\ny_pred = X_pred.dot(theta.reshape(theta.shape[0],1))","7281eaea":"# Visualize our fit\nfig, ax = pp.subplots()\nax.scatter(df['feature'], df['target'], color='r', marker='x')\nax.set_xlabel(df.columns[0])\nax.set_ylabel(df.columns[1])\nax.plot(X_pred[:,1], y_pred, color='blue')","5476d38b":"# Now lets try to increase the degree gradually\n\n# Helper function\ndef run_model(df, degree):\n    # get our feature and target variables\n    X = df['feature'].values\n    y = df['target'].values\n    \n    # generate additonal features (if applicable)\n    X = mapFeatures(X, degree)\n    # Add the intercept term\n    X = np.column_stack((np.ones(X.shape[0]), X))\n    initial_theta = np.ones(degree + 1).reshape(degree + 1,1)\n    \n    res = minimize(costfunc, initial_theta, args=(X,y))\n    # Optimized values of coefficients\n    theta = res.x\n    # now create points to visualize the fit\n    X_pred = np.linspace(X[:,1].min(), X[:,1].max(), 10)\n    X_pred = np.column_stack((np.ones(X_pred.shape[0]), mapFeatures(X_pred, degree)))\n\n    # Now use our hypothesis to get values of y_pred\n    y_pred = X_pred.dot(theta.reshape(theta.shape[0],1))\n    # Visualize our fit\n    fig, ax = pp.subplots()\n    ax.scatter(df['feature'], df['target'], color='r', marker='x')\n    ax.set_xlabel(df.columns[0])\n    ax.set_ylabel(df.columns[1])\n    ax.plot(X_pred[:,1], y_pred, color='blue')\n    ","3821cbf4":"# degree = 4\nrun_model(df, 3)","39784753":"run_model(df, 6)","6342b243":"# Regularized Cost Function\ndef costfuncReg(theta, X, y, reg_factor):\n    m = X.shape[0]\n    y_pred = X.dot(theta)\n    J = (((y_pred-y) ** 2).mean()) \/ 2\n    \n    # add regularization term\n    reg_term = (reg_factor * np.sum((theta ** 2)))\/ (2 * m )\n    J = J + reg_term\n    return J","394ba35f":"# New Helper for Regularized Cost Func\n# Helper function\ndef run_model_reg(df, degree, reg_factor):\n    # get our feature and target variables\n    X = df['feature'].values\n    y = df['target'].values\n    \n    # generate additonal features (if applicable)\n    X = mapFeatures(X, degree)\n    # Add the intercept term\n    X = np.column_stack((np.ones(X.shape[0]), X))\n    initial_theta = np.ones(degree + 1).reshape(degree + 1,1)\n    \n    res = minimize(costfuncReg, initial_theta, args=(X,y,reg_factor))\n    # Optimized values of coefficients\n    theta = res.x\n    # now create points to visualize the fit\n    X_pred = np.linspace(X[:,1].min(), X[:,1].max(), 10)\n    X_pred = np.column_stack((np.ones(X_pred.shape[0]), mapFeatures(X_pred, degree)))\n\n    # Now use our hypothesis to get values of y_pred\n    y_pred = X_pred.dot(theta.reshape(theta.shape[0],1))\n    # Visualize our fit\n    fig, ax = pp.subplots()\n    ax.scatter(df['feature'], df['target'], color='r', marker='x')\n    ax.set_xlabel(df.columns[0])\n    ax.set_ylabel(df.columns[1])\n    ax.plot(X_pred[:,1], y_pred, color='blue')","ad8ff3a5":"# lets run the model with degree = 6 and regularization factor = 3\nrun_model_reg(df, 6, 3)","b6fa7041":"Although not very evident but still the regularization term does smoothen out the prediction curve a bit. Depending on the kind of dataset regularization can be an effective technique to prevent overfitting (high variance).","38a41b76":"Although we don't need to map extra higher order polynomial features, we still define the function and pass degree as 1","3b91a8b0":"It seems we can fit a straight line to the data. But we will do in a generalized way.\nThis will help to understand that linear regression is just special case of polynomial regression where the degree is 1.\n\nThe cost function of a linear regression is given by <br>\n$J(\\theta)=\\frac{1} {2m} \\sum_{i=1}^m (h_\\theta(x_i) - y_i)^2$\n<br>\nwhere <br> $h(x) = \\mathbf{\\theta}^\\intercal \\mathbf{x} = \\theta_0 + \\theta_1x$","e1472a0e":"The 6th degree polynomial is slightly overfitting the data. In order to prevent this we use regularization. This is done by penalizing the cost by adding a regularization term to the cost function.\n<br>\nThe regularized cost function is defined as \n        $J(\\theta)=\\frac{1} {2m} \\sum_{i=1}^m (h_\\theta(x_i) - y_i)^2 + \\frac{\\lambda}{2m} \\sum_{j=1}^n[\\theta_j^2]$\n        \nwhere $\\lambda$ = regularization factor and <br>\n           n = number of features\nNOTE: We don't regularize the intercept term. \n"}}