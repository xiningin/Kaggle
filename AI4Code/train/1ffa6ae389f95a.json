{"cell_type":{"eddd3785":"code","9336400f":"code","3554b361":"code","89ca8cc5":"code","2f565979":"code","64530d6e":"code","0bd3af59":"code","338fe29b":"code","eded1adc":"code","530ab0ec":"code","842a455b":"code","b5a1c2b2":"code","5f4b2397":"code","248a2d74":"code","eecc8cbb":"code","0de7ac94":"code","a75fc854":"code","22e7999a":"code","02f3a300":"code","8698993b":"code","f84b5e0c":"code","5870fea3":"code","546c018a":"code","95ec64ab":"code","3822b384":"code","38c671c6":"code","4b691073":"code","e4b3b7ca":"code","f8f37049":"code","fcd2c82e":"code","5ee9db3e":"code","157b511f":"code","beb789b9":"code","5cdc2a34":"markdown","e74aa2c0":"markdown","8a59b8b0":"markdown","37e4e569":"markdown","b13299a7":"markdown","fa7158af":"markdown","12a5766f":"markdown","3dcd8770":"markdown","1826d32b":"markdown","4a616ca2":"markdown","fcde5782":"markdown","ed481b04":"markdown","f4ded349":"markdown","46b9aad2":"markdown","1df9e21d":"markdown"},"source":{"eddd3785":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9336400f":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nplt.style.use('ggplot')\nimport re\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nfrom keras.layers import (LSTM, \n                          Embedding, \n                          BatchNormalization,\n                          Dense, \n                          TimeDistributed, \n                          Dropout, \n                          Bidirectional,\n                          Flatten, \n                          GlobalMaxPool1D)\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import (\n    precision_score, \n    recall_score, \n    f1_score, \n    classification_report,\n    accuracy_score\n)","3554b361":"tweet = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","89ca8cc5":"#Checking the class distribution\nx = tweet.target.value_counts()\nsns.barplot(x.index, x, palette='cool')\nplt.gca().set_ylabel('tweets')","2f565979":"#Number of characters in tweets\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntweet_len = tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='crimson')\nax1.set_title('Disaster tweets')\ntweet_len = tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='skyblue')\nax2.set_title('Non disaster tweets')\nfig.suptitle('Characters in tweets')","64530d6e":"#Number of words in a tweet\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntweet_len = tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len, color='black')\nax1.set_title('Disaster tweets')\ntweet_len = tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='purple')\nax2.set_title('Non disaster tweets')\nfig.suptitle('Words in a tweet')","0bd3af59":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword = tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='darkblue')\nax1.set_title('Disaster')\nword = tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='magenta')\nax2.set_title('Non disaster')\nfig.suptitle('Average word length in each tweet')","338fe29b":"def create_corpus(target):\n    corpus=[]\n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","eded1adc":"def create_corpus_df(tweet, target):\n    corpus=[]\n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","530ab0ec":"#Punctuations in non-disaster class\nplt.figure(figsize=(10,5))\ncorpus = create_corpus(0)\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\nx,y=zip(*dic.items())\nplt.bar(x, y,color='cyan')","842a455b":"#Punctuations in disaster class\nplt.figure(figsize=(10,5))\ncorpus = create_corpus(1)\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\nx,y = zip(*dic.items())\nplt.bar(x, y, color='red')","b5a1c2b2":"#Common words\ncounter = Counter(corpus)\nmost = counter.most_common()\nx=[]\ny=[]\nfor word, count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y, y=x, palette='Greens_r')","5f4b2397":"#Bigram analysis\ndef get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize=(10,5))\ntop_tweet_bigrams = get_top_tweet_bigrams(tweet['text'])[:10]\nx,y = map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x, palette='Reds_r')","248a2d74":"df = pd.concat([tweet,test])\ndf.shape","eecc8cbb":"#Renaming location names\ndf['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\nsns.barplot(y = df['location'].value_counts()[:5].index, x = df['location'].value_counts()[:5],\n            palette='autumn', orient='h')","0de7ac94":"import string\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n# Applying the cleaning function to both test and training datasets\ndf['text'] = df['text'].apply(lambda x: clean_text(x))\n# Let's take a look at the updated text\ndf['text'].head()","a75fc854":"#Removing Emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))","22e7999a":"tweet_1 = tweet.text.values   # returns the tweet text column(train)\ntest_1 = test.text.values     # same but from test test\nsentiments = tweet.target.values   # returns the target column (train)","02f3a300":"word_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(tweet_1)\nvocab_length = len(word_tokenizer.word_index) + 1","8698993b":"def embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)","f84b5e0c":"# padding sequences\nlongest_train = max(tweet_1, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))\npadded_sentences = pad_sequences(embed(tweet_1), length_long_sentence, padding='post')\ntest_sentences = pad_sequences(\n    embed(test_1), \n    length_long_sentence,\n    padding='post'\n)","5870fea3":"X_train, X_test, y_train, y_test = train_test_split(\n    padded_sentences, \n    sentiments, \n    test_size=0.25\n)","546c018a":"embeddings_dictionary = dict()\nembedding_dim = 100\nglove_file = open('..\/input\/glove-file\/glove.6B.100d.txt')\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\nglove_file.close()","95ec64ab":"embedding_matrix = np.zeros((vocab_length, embedding_dim))\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","3822b384":"def BLSTM():\n    model = Sequential()\n    model.add(Embedding(input_dim=embedding_matrix.shape[0], \n                        output_dim=embedding_matrix.shape[1], \n                        weights = [embedding_matrix], \n                        input_length=length_long_sentence))\n    model.add(Bidirectional(LSTM(length_long_sentence, return_sequences = True, recurrent_dropout=0.2)))\n    model.add(GlobalMaxPool1D())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","38c671c6":"print(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","4b691073":"model = BLSTM()\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n      verbose = 1, \n    save_best_only = True\n)\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs = 7,\n    batch_size = 32,\n)","e4b3b7ca":"model.save(\"model1.h5\")","f8f37049":"def metrics(pred_tag, y_test):\n    print(\"F1-score: \", f1_score(pred_tag, y_test))\n    print(\"Precision: \", precision_score(pred_tag, y_test))\n    print(\"Recall: \", recall_score(pred_tag, y_test))\n    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n    print(\"-\"*50)\n    print(classification_report(pred_tag, y_test))\n","fcd2c82e":"def plot(history, arr):\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)\n        ax[idx].set_xlabel('A ',fontsize=16)\n        ax[idx].set_ylabel('B',fontsize=16)\n        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)","5ee9db3e":"loss, accuracy = model.evaluate(X_test, y_test)\nprint('Loss:', loss)\nprint('Accuracy:', accuracy)","157b511f":"preds = model.predict_classes(X_test)\nmetrics(preds, y_test)","beb789b9":"model.load_weights('model1.h5')\npreds = model.predict_classes(X_test)\nmetrics(preds, y_test)","5cdc2a34":"# **Bi-Directional LSTM for disaster prediction**","e74aa2c0":"To obtain a vector representation for words we can use an unsupervised learning algorithm called **GloVe (Global Vectors for Word Representation)**,which focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together\n\n","8a59b8b0":"# Function to plot graph","37e4e569":"# form a embedding_matrix","b13299a7":"#  Function to do Evaluation metrics","fa7158af":"# EDA","12a5766f":"# Defining Bi-LSTM model","3dcd8770":"# Importing libraries","1826d32b":"# Creating a embedding matrix using GLOVE","4a616ca2":"# Training model and defining call backs","fcde5782":"# Importing the datasets ","ed481b04":"# **word tokenizer**","f4ded349":"# **Text Pre-processing**","46b9aad2":"# Model Building","1df9e21d":"# train test split"}}