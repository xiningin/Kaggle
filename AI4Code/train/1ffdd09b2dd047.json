{"cell_type":{"6f8412fa":"code","53969859":"code","07d1b29a":"code","ff8bbc2b":"code","7066e91d":"code","10d740bf":"code","8728c99c":"code","4be5297c":"code","ea744a7e":"code","37caf94b":"code","0ae97935":"code","69e8487f":"code","f6c1a462":"code","3a2ce069":"code","e43b0fd4":"code","17f4f09e":"code","995fd512":"code","cee70cc6":"code","c4d217ad":"code","351f0cbb":"code","2e18d6f2":"code","0beb5458":"code","2dae98d0":"markdown","8e2baf4e":"markdown","8ea3648a":"markdown","fbf32a8a":"markdown","eefea3ec":"markdown","4933b094":"markdown","ed77c603":"markdown","b45d310e":"markdown","ff4eab8f":"markdown","7be488fd":"markdown","fd73e132":"markdown","f2ecdefe":"markdown","35e6d888":"markdown","3d6df4a8":"markdown","06c34a84":"markdown","3f8e5651":"markdown","990776b0":"markdown","ac79e518":"markdown","33559092":"markdown","bf1e63c5":"markdown","7e2606e5":"markdown","0c015d7a":"markdown","53bd0234":"markdown","26dd0620":"markdown"},"source":{"6f8412fa":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","53969859":"train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","07d1b29a":"train.describe().style.background_gradient(\"copper_r\")","ff8bbc2b":"train.drop('id', axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)","7066e91d":"features=[]\ncat_features=[]\ncont_features=[]\n\nfor feature in test.columns:\n    features.append(feature)\n    if test.dtypes[feature] == object or train.dtypes[feature] == 'int8':\n        cat_features.append(feature)\n    else:\n        cont_features.append(feature)\n\nplt.bar([1,2],[len(cat_features),len(cont_features)])\nplt.xticks([1,2],('Categorical','continuous'))\nplt.show()","10d740bf":"print(\"Shape of Train data:\", train.shape)\nprint(\"Shape of Test data:\", test.shape)","8728c99c":"print(\"Missing train data:\", train.isnull().sum().sum(), f\"({train.isnull().sum().sum()\/train.shape[0]}%)\")\nprint(\"Missing test data:\", test.isnull().sum().sum(), f\"({test.isnull().sum().sum()\/test.shape[0]}%)\")","4be5297c":"print(\"Categorical features:\", len(cat_features))\nprint(\"Continuous features:\", len(cont_features))","ea744a7e":"print(\"Memory used by train data:\", train.memory_usage().sum() \/ 1024**2)\nprint(\"Memory used by test data:\", test.memory_usage().sum() \/ 1024**2)","37caf94b":"pd.set_option(\"display.max_columns\", None)","0ae97935":"train.head()","69e8487f":"pie, ax = plt.subplots(figsize=[18,8])\ntrain.groupby('target').size().plot(kind='pie',autopct='%.1f',ax=ax,title='Target distibution')","f6c1a462":"print(\"Train: Red\")\nprint(\"Test: Green\")\nnrows = 20\nncols = 5\ni = 0\n\nfig, ax = plt.subplots(nrows, ncols, figsize = (25, 25))\n\nfor row in range(nrows):\n    for col in range(ncols):\n        sns.histplot(data = train.iloc[:, i],color='r', ax = ax[row, col]).set(ylabel = '')\n        sns.histplot(data = test.iloc[:, i],color='g', ax = ax[row, col]).set(ylabel = '')\n        i += 1","3a2ce069":"train_outliers = ((train - train.min())\/(train.max() - train.min()))\n\nfig, ax = plt.subplots(7, 1, figsize = (25,25))\n\nsns.boxplot(data = train_outliers.iloc[:, 0:15], ax = ax[0])\nsns.boxplot(data = train_outliers.iloc[:, 15:30], ax = ax[1])\nsns.boxplot(data = train_outliers.iloc[:, 30:45], ax = ax[2])\nsns.boxplot(data = train_outliers.iloc[:, 45:60], ax = ax[3])\nsns.boxplot(data = train_outliers.iloc[:, 60:75], ax = ax[4])\nsns.boxplot(data = train_outliers.iloc[:, 75:90], ax = ax[5])\nsns.boxplot(data = train_outliers.iloc[:, 90:101], ax = ax[6])\nplt.show()\n\ndel train_outliers","e43b0fd4":"corr=train.corr()\n\nmask = np.triu(np.ones_like(corr, dtype = bool))\nplt.figure(figsize = (15, 15))\nplt.title('Correlation matrix for features of Training data')\nsns.heatmap(corr,cmap='coolwarm', mask = mask, annot=False, linewidths = .5,square=True, cbar_kws={\"shrink\": .60})\nplt.show()","17f4f09e":"corr[['target']].sort_values(by='target', ascending=False).T.style.background_gradient(cmap=\"copper_r\")","995fd512":"corr_ = abs(corr[['target']].sort_values(by='target', ascending=False))\nfig, axes = plt.subplots(1, 2, figsize=(18, 10))\nfig.suptitle('Correlation to Target')\n\nsns.heatmap(ax=axes[0], data=corr_.iloc[0:50,:], annot=False, cmap='tab20c', linewidth=0.5, xticklabels=corr_.iloc[0:50,:].columns, yticklabels=corr_.iloc[0:50,:].index)\nsns.heatmap(ax=axes[1], data=corr_.iloc[50:,:], annot=False, cmap='tab20c', linewidth=0.5, xticklabels=corr_.iloc[50:100,:].columns, yticklabels=corr_.iloc[50:100,:].index)\nplt.show()","cee70cc6":"from sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMClassifier\n\nX = train[features]\ny = train['target']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y,train_size=0.8,test_size = 0.2,random_state = 0)\n\nlgbm = LGBMClassifier()\nlgbm.fit(X_train, y_train)\n\nimportances_df = pd.DataFrame(lgbm.feature_importances_, columns=['Feature_Importance'],index=X_train.columns)\nimportances_df.sort_values(by=['Feature_Importance'], ascending=False, inplace=True)","c4d217ad":"importances_df.T.style.background_gradient(cmap=\"copper_r\")","351f0cbb":"fig, axes = plt.subplots(1, 2, figsize=(18, 10))\nfig.suptitle('Correlation to Target')\n\nsns.heatmap(ax=axes[0], data=importances_df.iloc[0:50,:], annot=False,cmap='tab20c', linewidth=0.5, xticklabels=importances_df.iloc[0:50,:].columns, yticklabels=importances_df.iloc[0:50,:].index)\nsns.heatmap(ax=axes[1], data=importances_df.iloc[50:,:], annot=False,cmap='tab20c', linewidth=0.5, xticklabels=importances_df.iloc[50:100,:].columns, yticklabels=importances_df.iloc[50:100,:].index)\nplt.show()","2e18d6f2":"from sklearn.metrics import roc_auc_score\n\nprint(roc_auc_score(y_valid, lgbm.predict_proba(X_valid)[:, 1]))","0beb5458":"test_preds = lgbm.predict_proba(test)[:, 1]\nsample_submission['target'] = test_preds\nsample_submission.to_csv('submission.csv', index=False)","2dae98d0":"# Boxplots of continuous features","8e2baf4e":"# Target distribution","8ea3648a":"#### Train and test shape","fbf32a8a":"# Introduction\n\nThe dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting identifying spam emails via various extracted features from the email. Although the features are anonymized, they have properties relating to real-world features.\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.\n\n","eefea3ec":"# Next notebook\n##### What's in it?\n* Training many different models and see which model is performing well.\n* Feature importances of each model.\n\n#### Link: https:\/\/www.kaggle.com\/rigeltal\/tps-11-starter","4933b094":"#### All credits: https:\/\/www.kaggle.com\/davidcoxon\/first-look-at-october-data\n# Distribution of data","ed77c603":"# Observation\n#### * The test dataset is approx equal to the size of the training dataset\n#### * The training dataset is highly representative of the test dataset\n#### * There is no missing data\n#### * There is no binary features\n#### * There is relatively low correlation between features\n#### * There appears to be a relatively high correlation between f34 and target value.\n#### * Feature have show both positive and negative correlations to target classification.\n#### * Feature importance doesn't indicate f34 as an important feature.\n#### * Feature importance indicates f91 as important feature","b45d310e":"#### Missing data","ff4eab8f":"# Importing data","7be488fd":"# Final note\n#### Thank you!\n##### If you like it please upvote it. If you have suggestion please leave it in comment. Even I am beginner looking forward to learn something new. So let me know how can I improve this","fd73e132":"#### Describing the data","f2ecdefe":"# First thoughts\n* This months Tabular Playground Dataset is once again quite large, so managing both cpu usage and ram is going to be an important element of the project.\n* It looks like another classification problem.\n* There is no missing data, so imputing values will not be required.\n* Looks like there is no categorical features\n* Data engineering and feature importance may be important.\n* Its likely that model selection and hyper parameter tuning will be important.\n* Staking, blending and ensambles are likely to be important to get higher scores.","35e6d888":"# Feature correlation","3d6df4a8":"# Features","06c34a84":"# Importing Libraries","3f8e5651":"# Info about data","990776b0":"# Baseline lgbm submission","ac79e518":"The above plot clearly tells that all features are continuous and thus no categorical features this time.","33559092":"#### Feature type","bf1e63c5":"# Glance at train data","7e2606e5":"# Feature importance of LGBM","0c015d7a":"#### Droping id column","53bd0234":"#### Memory used","26dd0620":"# Feature correlation with target"}}