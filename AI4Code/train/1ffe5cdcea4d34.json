{"cell_type":{"1353ce69":"code","bac18817":"code","c906f0b7":"code","fd7681a1":"code","06239c2e":"code","78adbb4d":"code","84d0fc40":"code","1cefa706":"code","7ed03980":"code","fcac9319":"markdown","7a5ecea6":"markdown","d5fd7614":"markdown","a10c3686":"markdown","f685c785":"markdown","1d04bd83":"markdown"},"source":{"1353ce69":"import os, cv2\nimport torch\nimport torchvision\nfrom torchvision import transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn as nn\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nimport albumentations as album\nimport torch.nn.init as init\nimport random\nfrom tqdm import tqdm\nfrom albumentations.pytorch import ToTensor\nimport torch.nn.functional as F\nfrom torch import optim\nfrom torch.utils.tensorboard import SummaryWriter\nimport logging\n!pip install -q -U segmentation-models-pytorch albumentations > \/dev\/null\nimport segmentation_models_pytorch as smp","bac18817":"from glob import glob\n\nmask_files = glob('..\/input\/lgg-mri-segmentation\/kaggle_3m\/*\/*_mask*')\n\ntrain_files = []\nfor i in mask_files:\n    train_files.append(i.replace('_mask',''))\n\nfor i in range(4):   \n    print(mask_files[i])\n    print(train_files[i])","c906f0b7":"def get_training_augmentation():\n    train_transform = [\n        album.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n        album.OneOf(\n            [\n                album.HorizontalFlip(p=1),\n                album.VerticalFlip(p=1),\n                album.RandomRotate90(p=1),\n            ],\n            p=0.5,\n        ),\n    ]\n    return album.Compose(train_transform)\n\n\ndef get_validation_augmentation():\n    # Add sufficient padding to ensure image is divisible by 32\n    test_transform = [\n        album.PadIfNeeded(min_height=320, min_width=320, always_apply=True, border_mode=0),\n    ]\n    return album.Compose(test_transform)\n\n\ndef to_tensor(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')\n\ndef get_preprocessing(preprocessing_fn=None):  \n    _transform = []\n    if preprocessing_fn:\n        _transform.append(album.Lambda(image=preprocessing_fn))\n    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n        \n    return album.Compose(_transform)\n\ndef reverse_one_hot(image):\n    x = np.argmax(image, axis = -1)\n    return x","fd7681a1":"train_img_dir = train_files[:-300]\ntrain_mask_dir = mask_files[:-300]\nval_dir = train_files[-300:]\nval_mask = mask_files[:300]\nclass dataset(torch.utils.data.Dataset):\n    def __init__(self, imgpath, maskpath, augmentation=None,preprocessing=None):\n        self.image_paths = imgpath\n        self.mask_paths = maskpath\n        self.augmentation = augmentation\n        self.preprocessing = preprocessing\n    \n    def __getitem__(self, i):\n        # image\n        image = cv2.cvtColor(cv2.imread(self.image_paths[i]),cv2.COLOR_BGR2RGB)  \n        # mask\n        mask = cv2.imread(self.mask_paths[i],0)\/255\n        \n        background = abs(mask-1.0)\n        mask = cv2.merge((background,mask))\n        \n        # augmentation and transforms\n        if self.augmentation:\n            sample = self.augmentation(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n        \n        # apply preprocessing\n        if self.preprocessing:\n            sample = self.preprocessing(image=image, mask=mask)\n            image, mask = sample['image'], sample['mask']\n            \n        return image, mask\n    def __len__(self):\n        return len(self.image_paths)\n    \nENCODER = 'resnet50'\nENCODER_WEIGHTS = 'imagenet'\nCLASSES = 2 \nACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multiclass segmentation\n\n# create segmentation model with pretrained encoder\nmodel = smp.Unet(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=CLASSES, \n    activation=ACTIVATION,\n)\n    \npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\n# Data\ntrain_dataset = dataset(\n    train_img_dir,\n    train_mask_dir,\n    augmentation=get_training_augmentation(),\n    preprocessing=get_preprocessing(preprocessing_fn),\n)\nvalid_dataset = dataset(\n    val_dir,\n    val_mask,\n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n)\n\n# Dataloader\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\nvalid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=4)\n\nprint(len(train_dataset))\nprint(len(valid_dataset))","06239c2e":"def imshow(img):\n    if type(img)==torch.Tensor and img.shape[0]==3:\n        img=torch.transpose(img,0,1)\n        img=torch.transpose(img,2,1)\n    elif type(img)==torch.Tensor and img.shape[0]==1:\n        img = img[:][:][0]\n    print(img.shape)\n    plt.imshow(img)\n    plt.show()\nimage1, mask1 = train_dataset[296]\n#imshow(image1)\n#imshow(reverse_one_hot(mask1))\nimage1.shape","78adbb4d":"TRAINING = True\n\n# Set num of epochs\nEPOCHS = 50\n\n# Set device: `cuda` or `cpu`\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# define loss function\nloss = smp.utils.losses.DiceLoss()\n\n# define metrics\nmetrics = [\n    smp.utils.metrics.IoU(threshold=0.5),\n]\n\n# define optimizer\noptimizer = torch.optim.Adam([ \n    dict(params=model.parameters(), lr=0.0001),\n])\n\n# define learning rate scheduler (not used in this NB)\nlr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n)","84d0fc40":"train_epoch = smp.utils.train.TrainEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    optimizer=optimizer,\n    device=DEVICE,\n    verbose=True,\n)\nvalid_epoch = smp.utils.train.ValidEpoch(\n    model, \n    loss=loss, \n    metrics=metrics, \n    device=DEVICE,\n    verbose=True,\n)","1cefa706":"if TRAINING:\n\n    best_iou_score = 0.0\n    train_logs_list, valid_logs_list = [], []\n\n    for i in range(0, EPOCHS):\n\n        # Perform training & validation\n        print('\\nEpoch: {}'.format(i))\n        train_logs = train_epoch.run(train_loader)\n        valid_logs = valid_epoch.run(valid_loader)\n        train_logs_list.append(train_logs)\n        valid_logs_list.append(valid_logs)\n\n        # Save model if a better val IoU score is obtained\n        if best_iou_score < valid_logs['iou_score']:\n            best_iou_score = valid_logs['iou_score']\n            torch.save(model, '.\/best_model.pth')\n            print('Model saved!')","7ed03980":"test_dataset = dataset(\n    val_dir,\n    val_mask,\n    augmentation=get_validation_augmentation(), \n    preprocessing=get_preprocessing(preprocessing_fn),\n)\ntest_dataset_vis = dataset(\n    val_dir,\n    val_mask,\n)\n\nids = 20\nmodel.eval()\nwith torch.no_grad():\n    image, gt_mask = test_dataset[ids]\n    image_vis = test_dataset_vis[ids][0].astype('uint8')\n    mask_vis = test_dataset_vis[ids][1].astype('uint8')\n    \n    true_dimensions = image_vis.shape\n    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n    imshow(image_vis)\n    \n    a,b = cv2.split(mask_vis)\n    imshow(b)\n    \n    pred_mask = model(x_tensor)\n    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n    \n    a = pred_mask[1]\n    \n    a[a<0.5]=0\n    a[a>0.5]=1\n    imshow(a)","fcac9319":"# **Dir**","7a5ecea6":"# **Import packages**","d5fd7614":"# **DataLoader**","a10c3686":"# **Visualization**","f685c785":"# **Preprocessing**","1d04bd83":"# **Test for a new image**"}}