{"cell_type":{"569c6688":"code","b3703938":"code","2f54fe29":"code","79c2378c":"code","e2a2513f":"code","bbe823f6":"code","ad383899":"code","1d176e8a":"code","d3dff94d":"code","d71647d7":"code","2c3e79d9":"code","9ab545e5":"code","222b3ed7":"code","100249a0":"code","b4d65feb":"code","f94648d3":"code","378c825d":"code","220b2a7f":"code","1234342d":"code","378a618e":"code","d6581bbe":"code","22f9e5cb":"code","c8a68e00":"code","5e62d64e":"code","a3c18cd5":"code","34d1543c":"markdown","f991af77":"markdown","dcdd751f":"markdown","b5c7de52":"markdown","1f0641e2":"markdown","2c2d6871":"markdown","f3596c46":"markdown","20f5306d":"markdown","e4a65408":"markdown","d57827e3":"markdown","ff8a05cd":"markdown","adb752d2":"markdown","c1f1d63f":"markdown","50018f93":"markdown","c549a290":"markdown","73b222c9":"markdown"},"source":{"569c6688":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b3703938":"# We use this package for preprocessing tweets \n!pip install tweet-preprocessor","2f54fe29":"# Standard import statements\nimport string\nfrom collections import defaultdict\nfrom collections import Counter\nfrom time import time\n\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n\nimport preprocessor as p\nfrom gensim.models import KeyedVectors\nfrom tqdm.notebook import tqdm\nimport spacy\nnlp = spacy.load(\"en_core_web_sm\")","79c2378c":"# the preprocessor package takes care of step 1 \np.set_options(p.OPT.HASHTAG, p.OPT.MENTION, p.OPT.NUMBER, p.OPT.URL, p.OPT.RESERVED)\n\n# the pretrained word vectors for step 5\nmodel = KeyedVectors.load(\"\/kaggle\/input\/gensim-embeddings-dataset\/glove.twitter.27B.200d.gensim\", mmap=\"r\")","e2a2513f":"# load in the dataset\ntrain_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\", encoding='latin1')\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\", encoding='latin1')\ntrain_df.info()","bbe823f6":"def clean_tweet(tweet: str) -> str:\n    trans = str.maketrans(\"\",\"\",string.punctuation)\n    processed_tweet = p.clean(tweet)\n    recoded_tweet = processed_tweet.encode('ascii','ignore').decode('utf-8','ignore') \n    depunct_tweet = recoded_tweet.translate(trans).lower()\n    doc = nlp(depunct_tweet)\n    lemmas = [y.lemma_ if y.lemma_ != \"-PRON-\" else y.orth_ for y in doc]\n    cleaned_tweet = str.join(\" \", lemmas)\n    \n    return cleaned_tweet","ad383899":"# Check the cleaned output on some random tweets \nfor x in (train_df.sample(5)['text']):\n    print(\"RAW   : \", x)\n    print(\"CLEAN : \", clean_tweet(x),\"\\n\") \n","1d176e8a":"# Clean all the tweets in the dataset\ntrain_df['clean_tweet'] = [clean_tweet(x) for x in tqdm(train_df['text'])]","d3dff94d":"# Listing out words that cannot be vectorized \nno_vecs = []\nfor x in train_df['clean_tweet']:\n    no_vecs.extend([y not in model for y in x.split()])\n\nprint(\"Total words in the corpus: \", len(no_vecs))\nprint(\"Total words without vectors: \", sum(no_vecs))\nprint(f\"Information lost: {sum(no_vecs)\/len(no_vecs)*100:0.2f}%\")","d71647d7":"# document vector = average vector of words in the document\ndef doc_to_vec(doc):\n    vec = np.array([model[x]  for x in doc.split() if x in model])\n    if len(vec) == 0:\n        return np.full(200, 0.0)\n    return np.mean(vec, axis=0)","2c3e79d9":"# simple KNN Classifier  \ndef knn_predict(vec, corpus, targets, n=5):\n    if len(corpus) != len(targets):\n        raise Exception(f\" \\\n        Corpus and Targets must have same length\\n \\\n        len(corpus): {len(corpus)} \\\n        len(targets): {len(targets)} \\\n        \")\n    vec = vec.reshape(1,-1)\n    corpus = np.vstack(corpus)\n    targets = np.array(targets)\n\n    sims = cosine_similarity(vec, corpus).flatten()\n    top_n_idx = np.argsort(sims)[::-1][:n]\n    top_n = targets[top_n_idx]\n    pred = Counter(top_n).most_common()[0][0]\n    \n    return pred","9ab545e5":"# Compute vectors for each cleaned tweet\ncorpus = np.array([doc_to_vec(x) for x in tqdm(train_df['clean_tweet'])])\ntrain_df['vector'] = list(corpus)","222b3ed7":"# Split the data into train and test sets \nX_train, X_test, y_train, y_test = train_test_split(\n    train_df['vector'], \n    train_df['target'], \n    test_size=0.1, \n    random_state=42\n)","100249a0":"def knn_fit_predict(X_train, y_train, X_test, verbose=False):\n    start = time()\n    iterator = tqdm(X_test) if verbose else X_test\n    preds = [knn_predict(x, X_train, y_train) for x in iterator]\n    if verbose:\n        print(f\"That took {time()-start:0.2f} seconds\")\n    return preds","b4d65feb":"# Run KNN on the whole Test Dataset\npreds = knn_fit_predict(X_train, y_train, X_test, verbose=True)\nprint(f\"Accuracy: {accuracy_score(preds, y_test):0.2f}\")\nprint(f\"F1 Score: {f1_score(preds, y_test):0.2f}\")","f94648d3":"def get_sign(p, v):\n    # get the sign of vector v with respect to normal p \n    dot_product = np.dot(p, v)\n    sign = np.sign(dot_product)\n    sign = 1 if sign >= 0 else 0\n    return sign\n\ndef get_multi_planar_hashes(planes, v):\n    # calculate the hash of v with respect to set of planes\n    h = [get_sign(plane, v) for plane in planes]    \n    hash_value = sum([2**i * x for i, x in enumerate(h)])\n    return hash_value\n\ndef build_hash_table(corpus, planes):\n    # build a hashtable and assign each training data point to a hash value\n    hash_table = defaultdict(list)\n    for c, vec in corpus.items():\n        hash_value = get_multi_planar_hashes(planes, vec)\n        hash_table[hash_value].append(c)\n    return hash_table\n\ndef get_classmates(vec, planes, hash_table, n=None):\n    # return the IDS of the vectors that have the same hash as v\n    hash_val = get_multi_planar_hashes(planes, vec)\n    idx = hash_table.get(hash_val) \n    neighbor_idx = idx if idx is not None else []\n    if n is not None:\n        neighbor_idx = neighbor_idx[:n]\n    return neighbor_idx\n\n","378c825d":"def get_random_planes_matrix(num_dim, num_planes=10, num_sets=5):\n    # randomly generate a set of hyperplanes\n    random_planes_matrix = [\n        np.random.normal(size=(num_planes,num_dim)) \n        for _ in range(num_sets)\n    ]   \n    return random_planes_matrix\n\ndef get_hash_tables(corpus, planes_matrix):\n    # return an array of multiple hash tables per set of planes\n    tables = [build_hash_table(corpus, p) for p in planes_matrix]\n    return tables \n","220b2a7f":"# Putting it all together\ndef approx_knn_fit_predict(\n    X_train, \n    y_train, \n    X_test, \n    random_state=None, \n    verbose=False,\n    num_planes=10,\n    num_sets=10,\n    num_classmates=100):\n    start = time()\n    \n    if random_state:\n        np.random.seed(random_state)\n    planes_matrix = get_random_planes_matrix(200, num_planes=num_planes, num_sets=num_sets)\n    tables = get_hash_tables(X_train, planes_matrix)\n    \n    candidates = [set({})] * len(X_test)\n    for planes, table in zip(planes_matrix, tables):\n        for i, x in enumerate(X_test): \n            classmates = get_classmates(x, planes, table, n =num_classmates)\n            candidates[i] = candidates[i].union(set(classmates))\n\n    preds = []\n    iterator = zip(X_test, candidates)\n    iterator = tqdm(iterator, total=len(X_test)) if verbose else iterator\n    for x, c in iterator:\n        corpus = X_train[c]\n        y = y_train[c]\n        if len(c)== 0:\n            corpus = X_train\n            y = y_train\n        preds.append(knn_predict(x, corpus, y))\n        \n    if(verbose):\n        print(f\"That took {(time() - start):.2f} seconds\")\n    \n    return preds","1234342d":"preds = approx_knn_fit_predict(X_train, y_train, X_test, random_state=42, verbose=True)\nprint(f\"Accuracy: {accuracy_score(preds, y_test):0.2f}\")\nprint(f\"F1 Score: {f1_score(preds, y_test):0.2f}\")","378a618e":"kf = KFold(n_splits=5, shuffle=True, random_state=42)\n\nknn_time = []\nknn_acc =[]\nknn_f1 = []\n\napprox_knn_time = []\napprox_knn_acc = []\napprox_knn_f1 = []\n\nfor i, (train_i, valid_i) in enumerate(kf.split(train_df)):\n    print(\"Running Fold \",i+1)\n    X_train = train_df.iloc[train_i]['vector']\n    y_train = train_df.iloc[train_i]['target']\n    \n    X_valid = train_df.iloc[valid_i]['vector']\n    y_valid = train_df.iloc[valid_i]['target']\n        \n    print(\"Running KNN\", end=\"...\")\n    start = time()\n    preds = knn_fit_predict(X_train, y_train, X_valid)\n    acc = accuracy_score(y_valid, preds)\n    f1 = f1_score(y_valid, preds)\n    knn_acc.append(acc)\n    knn_f1.append(f1)\n    knn_time.append(time()-start)\n    print(\"Done\")\n    \n    print(\"Running Approximate KNN\",end=\"...\")\n    start = time()\n    preds = approx_knn_fit_predict(X_train, y_train, X_valid, 42)\n    acc = accuracy_score(y_valid, preds)\n    f1 = f1_score(y_valid, preds)\n    approx_knn_acc.append(acc)\n    approx_knn_f1.append(f1)\n    approx_knn_time.append(time()-start)\n    print(\"Done\")\n    print()","d6581bbe":"print(\"KNN Results\")\nprint(f\"Average train time: {np.mean(knn_time):0.2f}\")\nprint(f\"Average accuracy: {np.mean(knn_acc):0.2f}\")\nprint(f\"Average f1 score: {np.mean(knn_f1):0.2f}\")\nprint()\nprint(\"Approx KNN Results\")\nprint(f\"Average train time: {np.mean(approx_knn_time):0.2f}\")\nprint(f\"Average accuracy: {np.mean(approx_knn_acc):0.2f}\")\nprint(f\"Average f1 score: {np.mean(approx_knn_f1):0.2f}\")","22f9e5cb":"# num_planes_choices = [4, 6, 8, 10, 12]\n# num_sets_choices = [2, 4, 8, 16, 24]\n# num_classmates_choices = [10, 50, 100, 1000]\n\n# progress = 0\n# total = len(num_planes_choices)*len(num_sets_choices)*len(num_classmates_choices)\n\n# best_f1 = 0\n# best_f1_params = None \n# best_time = np.inf\n\n# margin = 0.03\n# grid_search_start = time()\n\n# for num_planes in num_planes_choices:\n#     for num_sets in num_sets_choices:\n#         for num_classmates in num_classmates_choices:\n               \n#             planes_matrix = get_random_planes_matrix(200, num_planes=num_planes, num_sets=num_sets)\n#             params = [num_planes, num_sets, num_classmates]\n#             cv_f1 = []\n#             progress += 1\n#             print(f\"Now processing param set {progress}\/{total}\",end='\\r')\n#             for i, (train_i, valid_i) in enumerate(kf.split(train_df)):\n#                 X_train = train_df.iloc[train_i]['vector']\n#                 y_train = train_df.iloc[train_i]['target']\n\n#                 X_valid = train_df.iloc[valid_i]['vector']\n#                 y_valid = train_df.iloc[valid_i]['target']\n                \n#                 start = time()\n#                 preds = approx_knn_fit_predict(\n#                     X_train, \n#                     y_train, \n#                     X_valid, \n#                     random_state=42,     \n#                     num_planes=num_planes,\n#                     num_sets=num_sets,\n#                     num_classmates=num_classmates)\n#                 f1 = f1_score(y_valid, preds)\n#                 time_taken = time() - start\n#                 cv_f1.append(f1)\n                \n#             avg_f1 = np.mean(cv_f1)\n#             if avg_f1 > 0.7 and time_taken < best_time:\n#                 print(\"\\nBest Time Updated to: \", time_taken)\n#                 print(\"F1 Score: \", avg_f1)\n#                 best_f1_params = params \n#                 best_time = time_taken \n\n\n# print(f\"Done. That took {time()-grid_search_start} seconds\")            ","c8a68e00":"best_f1_params","5e62d64e":"# Split the data into train and test sets \nX_train, X_test, y_train, y_test = train_test_split(\n    train_df['vector'], \n    train_df['target'], \n    test_size=0.1, \n    random_state=42\n)\n\n# Running the algorithm with the optimal parameters\npreds = approx_knn_fit_predict(\n    X_train, \n    y_train, \n    X_test, \n    num_planes=12,\n    num_sets =4, \n    num_classmates=1000,\n    random_state=42, \n    verbose=True)\n\nprint(f\"Accuracy: {accuracy_score(preds, y_test):0.2f}\")\nprint(f\"F1 Score: {f1_score(preds, y_test):0.2f}\")","a3c18cd5":"# generating submission file \ntest_df['clean_tweet'] = [clean_tweet(x) for x in test_df['text']]\ntest_df['vector'] = [doc_to_vec(x) for x in test_df['clean_tweet']]\n\npreds = approx_knn_fit_predict(\n    train_df['vector'], \n    train_df['target'], \n    test_df['vector'], \n    num_planes=12,\n    num_sets =8, \n    num_classmates=1000,\n    random_state=42, \n    verbose=True)\n\nresults = pd.DataFrame(zip(test_df['id'], preds), columns =['id', 'target'])\nresults.to_csv('submission.csv', index=False)","34d1543c":"# 5. GRID Search\nThis is a bit of overkill considering the scope of this notebook, but since we've come this far, we might as well push it to its limits! \n\nAn added advantage of Approximate KNN is that we can try to do a brute force search and find the values for number of planes, number of sets and number of neighbors per subspace to consider. If we run the following code, we get the optimal parameters as :\n\n* `num_planes` = 12\n* `num_sets` = 8\n* `num_neighbors` = 1000\n\n\nHowever, this takes a long time to run (about 4400 seconds or about 73 minutes) so I have commented it out. But feel free to try it out yourself and ask any questions you have about it.  ","f991af77":"# 2. Preprocessing\n\nTo extract any sort of meaning from our raw tweet data, we first need to clean it and store it in a format that is usable by our machine learning algorithm. In this notebook, we perform the following steps on our tweet data: \n\n1. Remove Mentions, URLs and Hashtags \n2. Remove Punctuations\n3. Remove unusual, non-alphanumeric characters.\n4. Lemmatization: replace each word with its lemma \n5. Use pre-trained word embeddings to get word vectors\n\n*Note: In Step 3, we remove some weird text like \u00c2\u00c3\u00c2\u00aa that is present in the data ([Reference](https:\/\/www.kaggle.com\/c\/nlp-getting-started\/discussion\/125723#748840))*","dcdd751f":"## KNN Algorithm\nWe implement the algorithm as described in the Introduction. Even though I have implemented it from scratch for the sake of understanding, we can also use the sklearn package to get the predictions as they are highly optimized and easy to use. If you are curious about why we should or should not use sklearn packages, you can read a bit more about it on [this forum post](https:\/\/www.kaggle.com\/questions-and-answers\/208675).","b5c7de52":"## What is Approximate KNN?\n\nApproximate KNN is a variation of KNN that seeks to limit the number of training samples that each new test point is compared with before returning a result. As you can imagine, if we had a set of 1 Million training samples, then for each new test data point, we would have to compute the cosine similarity with 1M training samples, to find the top-K closest labels. This can be computationally expensive. \n\n\n### Algorithm  \n1. Randomly initialize a set of hyperplanes in the same vector space as the vectors \n2. These hyperplanes divide the vector place into sub-spaces. \n3. For each vector in training data, find out which subspace it belongs to \n4. For each test data, find the other vectors in the same subspace and treat them like neighbours \n5. Calculate the distance between the test data and each vector in the new (smaller) set of neighbours \n6. Return the top-K closest labels within the sub-space \n\n### Challenges\n1. How do we store the subspace information? \n2. What if there are not enough samples per subspace? ","1f0641e2":"# 4.  Cross-Validation\n\nTo get reliable results, we will use K Fold Cross-validation to measure the average time, accuracy score as well as the F1 score. \n\nHow it works: We divide the training dataset into K \"buckets\". Each time, we use one of the buckets and the validation data and the other buckets as training data. Each time, we store the F1 score and accuracy and report the average values are being closer to the TRUE scores that we can expect when dealing with new unseen data during the testing phase. Doing so also helps reduce any biases that could be introduced from the train-test split. (i.e, a model may perform better on a certain subset of the training data than others)","2c2d6871":"### Results\nThe model performed reasonably well, considering that we did not have to do any fancy feature engineering or use complicated neural networks. However, these results are not very reliable since how the dataset was split may have introduced some bias. As we will see later in this notebook, using K-Fold Cross Validation can help us get more reliable results. ","f3596c46":"#### Results\nGreat! Even though that took a while, it looks like it was worth it at the end. \nWe finally end up with an **accuracy of 78% and an F1 score of 0.73** with only a few seconds of train time!","20f5306d":"#### Result\nWell, that was much faster! And if we check carefully, we realise that most of the time taken for the prediction was for calculating the hash tables. In a real use-case, we would only be calculating these hash tables once and for all subsequent test samples, we would only compare it with a small subset of training data. \n\nHowever, we see that the accuracy and the F1 score took a bit of a hit. Before we can arrive at any conclusions about whether or not this is worth it, let's generate some more reliable results. ","e4a65408":"# 1. Introduction\n\nIn this notebook, our primary goal is simple: \n#### To clearly understand the difference between traditional K-NN Algorithm and Approximate KNN and discuss when we can choose one over the other. \nHowever, I will not be discussing EDA and different visualizations in this notebook since there are plenty of great notebooks on this topic already. Instead, we will look at some ways to preprocess textual data and discuss the implications of each one. ","d57827e3":"### Information Loss\n\nSince the main focus of this notebook is on the understanding of approximate KNN, we will not be training our word embeddings. Instead, we will use some pre-trained word embeddings available [here](https:\/\/www.kaggle.com\/bertcarremans\/glovetwitter27b100dtxt). However, by doing so, we will not be able to vectorize some words that are not in the vocabulary of the pre-trained model. Let us see just how much data we lose by doing so ","ff8a05cd":"## Approximate KNN\n\nNow we come to the main part of this notebook, where we think about how we can implement each of the steps of the algorithm described above and also discuss how to address the challenges\n\n### Challenge 1\n*How do we store the information about subspaces?*\n\nThe answer to that is \"**Locality Sensitive Hashing**\" \nThe key idea behind the implementation is called locality-sensitive hashing where we map vectors belonging to the same sub-space (step 2) to the same hash. To accomplish this, we calculate the hash as follows:\n\n##### Consider the set of random hyperplanes given by their normal vectors \"P\" and vector \"v\" \n#### Sign(P)\n1. Sign(p,v) = 1, if v lies on or in front of the plane p, 0 otherwise\n2. The position of v with respect to p is given by the sign of their dot product.\n3. If the dot product is 0, v lies on the plane described by normal p, return 1 \n4. If the dot product is positive, v lies in the direction of the normal p, return 1\n5. If the dot product is negative, v lies in the opposite side of the normal p, return 0  \n\n#### Calculate Hash\n1. For each p in P, calculate Sign(p,v)\n2. Step 2 yields an array of 0's and 1's with the length = number of planes \n3. Interpret this array as a binary number and convert to decimal. \n4. Return this decimal as the hash value for v with respect to set of planes P\n\nFor example, if we have 5 random planes and the vector v lies in front of plane 1, 3, 4, on plane 5 and behind plane 2, we get the array [1,0,1,1,1].\n\n![](https:\/\/latex.codecogs.com\/gif.latex?hash%3D2%5E4%20*%201%20&plus;%202%5E3%20*0%20&plus;%202%5E2%20*1%20&plus;%202%5E1%20*%201%20&plus;%202%5E0%20*%201%20%3D%2023)\n","adb752d2":"# 3. Implementation\n","c1f1d63f":"### Challenge 2\n\nWhat if there are not enough samples per subspace? This can happen when the random planes split the vector space in such a way that a new test sample does not have any other train samples that have the same hash. \n\nTo overcome this and also make the algorithm more robust, we initialize multiple sets of random hyperplanes and find out the neighbours for the vector v in each of the subspaces that it resides in (one subspace per set). We then use the combined set of training samples with the same hash as v in each of these subspaces as the neighbours of v. \n\n\nFor example, if we have the set of planes P1, P2, P3. \nFor each P, we calculate the hash table and find the neighbours of v with respect to P. \n\nConsider, we get the following neighbors for v:\n* P1 -> [A, B , D , E]\n* P2 -> [C, D, F]\n* P3 -> [D, E , F, G] \n\nThen the set of neighbours for v will be [A, B, C, D, E, F, G]. \n\nThere is still chance that we might end with no neighbours, meaning all the random sets of hyperplanes ended up isolating v from the rest. This would most likely be indicative of an outlier, rare word or a typo. Provided that our training data is large enough and is a good representation of real data, this would be quite unlikely. But yet, we need to be prepared for it. When we encounter such scenarios, we simply resort to the basic KNN and compare with all samples to find the top-K neighbours","50018f93":"#### Results\nAs we can see below, when we ran K-Means and approximate K-means on identical training and validation data, approximate KNN yielded results that were quite close to KNN while being much faster. Though the time saved might seem small on a dataset containing only 7000 samples, for very large datasets, the fact that we only have to calculate the hash tables ONCE means that all subsequent test data will need to be compared with a fraction of the dataset, giving lightning fast results! ","c549a290":"# 6. Conclusion\nTo wrap things up, let's go over our key takeaways from this notebook:\n\n1. Using tweet-preprocessor and spaCy to clean up textual data \n2. Using Pre-trained word embeddings to save time and effort \n3. Understand what KNN is and how to implement it from scratch \n4. Understand what is Approximate KNN and how it differs from KNN \n5. Understand Locality Sensitive Hashing \n6. Perform Cross-Validation to get reliable results \n7. Perform Grid Search to get the optimum hyperparameters\n\nWhile the impact of these results may not be evident with this small dataset, it scales quite well to larger data, resulting in fast predictions. In the end, the choice of one over the other would ultimately boil down to the use-case in which you wish to apply it. Different applications value the trade-off between speed and accuracy quite differently. So a comparative study often helps to get a better idea of the available options. \n\n\n\n# 7. Future Work \nThroughout this notebook, we applied only a few simple preprocessing steps since achieving a top score was not the focus of this notebook. Some of the ways you could expand on this model to generate better results are: \n\n1. There is a strong signal present in the \"keywords\" column of the dataset. Find some way to include this information in the document embeddings. \n2. Use custom word embeddings (using Keras Embedding Layers) so that we do not lose data \n3. Scale the word vectors by their corresponding TF-IDF values to better represent important words  \n4. Use sentence transformations using models that can capture contextual information. Simple word embeddings do not. \n\n*This was my second ever notebook on Kaggle and I worked hard on it, so thanks for stopping to give it a read! I'm always trying to improve myself so I would love your hear your feedback and criticism on my work so please feel free to leave some comments below. Happy Learning!*","73b222c9":"## What is KNN? \n\nKNN stands for K-Nearest Neighbors Algorithm. It belongs to a subclass of machine learning algorithms known as \"instance-based learning\".\nIn instance-based algorithms, no real \"learning\" takes place during the training phase. But rather, the instances are just stored in a suitable format. During the testing phase, the new unseen data point is compared with the stored training data and a result is generated, based on some similarity metric.\n\n![](http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1531424125\/KNN_final1_ibdm8a.png)\n\nIn KNN, we compare each new test data with all the training data and fetch the top-K closest data points. We then check the classes of these top-K neighbours and return the majority label as the prediction for the test data.  The above visual gives us a good idea of how KNN works. \n\nIn this notebook, we store each tweet as a vector and then use **cosine similarity** for calculating the distance between vectors. \n\nImage Source: [Datacamp](https:\/\/stats.stackexchange.com\/questions\/287425\/why-do-you-need-to-scale-data-in-knn)"}}