{"cell_type":{"ccd6537a":"code","3d56c323":"code","934c518a":"code","d16a9d85":"code","c6fd4e88":"code","d2a489e9":"code","f3a401a1":"code","3f0feba1":"code","5ec24700":"code","62dd4ef4":"code","95b8bdae":"code","7aa50827":"code","ee219c16":"code","06cada38":"code","a7af695c":"code","039177a5":"code","2cb8b29d":"code","d311e553":"code","58fa40f9":"code","6858deec":"code","8a499900":"code","1c40c9bc":"code","aecee577":"code","a18ff6c4":"code","3d80d3ab":"code","35c3a2f9":"code","4ac0e47b":"code","d308e561":"code","3b7b7cc4":"code","da7983e2":"code","aa0c8e40":"code","0790085a":"code","101589c3":"code","5a728779":"code","69fb0d81":"code","e64a1686":"markdown","9de58097":"markdown","5454ba1c":"markdown","b99122cc":"markdown","d3ddf5d0":"markdown","2641f297":"markdown","c2be9170":"markdown","cc508113":"markdown","35556b0d":"markdown","7ad5197a":"markdown","08f2ad40":"markdown","b0f18f49":"markdown","b9cb5328":"markdown","746c6cc9":"markdown"},"source":{"ccd6537a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3d56c323":"df = pd.read_csv('\/kaggle\/input\/ecommerce-data\/data.csv')","934c518a":"df.head()","d16a9d85":"df.shape","c6fd4e88":"df.isnull().any()\ndf.isnull().sum()\n\n#isnull & isna are the same in python\/pandas\ndf.isna().sum()","d2a489e9":"df = df.dropna()","f3a401a1":"#Seeing how the df has changed once the NaN values have been dropped\n\ndf.head()","3f0feba1":"df.shape","5ec24700":"#The most frequent value in a column (of a dataframe) below\ncus_id_1 = df['CustomerID'].value_counts().idxmax()\ncus_id_1","62dd4ef4":"#The amount of the most frequent value\ncus_id_2 = df['CustomerID'].value_counts().max()\ncus_id_2","95b8bdae":"#Unique values in the dataframe\ndf.nunique()","7aa50827":"bought_mar_2010 = df.loc[df['InvoiceDate'] == '12\/1\/2010 8:26']\nbought_mar_2010.shape","ee219c16":"df.loc[df['InvoiceNo'] == 'C569944']","06cada38":"#Correlation function\ndf.corr() #Not really any correlation","a7af695c":"#Creating a new column with total cost of units (quantity * price) - added to dataframe\ndf['TotalPrice'] = df['UnitPrice'] * df['Quantity']\n\ndf['TotalPrice']","039177a5":"#Drop duplicates \n\ndf = df.drop_duplicates()\ndf.shape","2cb8b29d":"cost_p_trans = df.groupby(['InvoiceNo'])['TotalPrice'].sum()\ncost_p_trans","d311e553":"import matplotlib.pyplot as plt","58fa40f9":"# plt.rcParams['figure.figsize'] = (18, 7) #the figure size. The only important is the (18, 7) as it's different than the default figure size\ncolor = plt.cm.copper(np.linspace(0, 1, 40)) #importance: color \ndf['StockCode'].value_counts().head(40).plot.bar(color = color)\nplt.title('Frequency of most bought items itemcode wise', fontsize=25)\nplt.show()","6858deec":"df['Country'].value_counts()[:5] #<- only selecting top 5\n\n#EIRE is Ireland ","8a499900":"plt.figure()\ndf['Country'].value_counts()[:5].plot(kind = 'pie', autopct='%1.1f%%') #autopct -> shows the percentage\n#plt.pie(df['Country'].value_counts()[:5], autopct='%1.1f%%')  #does the same thing but w\/o the country labels\nplt.title('Top 5 countries based on transactions', weight='bold') #makes the title bold\nplt.show()","1c40c9bc":"df.head()","aecee577":"#Import necessary modules\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor","a18ff6c4":"#X, y, and train_test_split\n\nX = df[['UnitPrice','TotalPrice','CustomerID']]\ny = df['Quantity']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.10)","3d80d3ab":"#Average quantity\n\ndf['Quantity'].mean() ","35c3a2f9":"linreg = LinearRegression()\n\nlinreg.fit(X_train, y_train)\npred_reg = linreg.predict(X_test)\nscore_linreg_per = linreg.score(X_test, y_test) * 100\nprint(f'Linear Regression Model Score: {score_linreg_per}%')","4ac0e47b":"#Mean Absolute Error: LINREG\n\nval_mae_linreg = mean_absolute_error(pred_reg, y_test)\nprint(f'The quantity is off by: {val_mae_linreg} (MAE)') # <- How much the prediction off quantity is off by","d308e561":"#Cross validation: LINREG\ncv_results_linreg = cross_val_score(linreg, X, y, cv=5) #this is important so that we're not dependent on the subjective number for our test size of our train_test_split\ncv_results_linreg\nnp.mean(cv_results_linreg)","3b7b7cc4":"def get_mae(max_leaf_nodes, X_train, X_test, y_train, y_test):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(X_train, y_train)\n    preds_val = model.predict(X_test)\n    mae = mean_absolute_error(y_test, preds_val)\n    return(mae)\n\ncandidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n\nfor max_leaf_nodes in candidate_max_leaf_nodes:\n    my_mae = get_mae(max_leaf_nodes, X_train, X_test, y_train, y_test)\n    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))","da7983e2":"#Edit accordingly based on the best leaf nodes\n\ndtr = DecisionTreeRegressor(max_leaf_nodes=500) # <- default number of lead nodes is unlimited\n\ndtr.fit(X_train, y_train)\npred_dtr = dtr.predict(X_test)\nscore_dtr_per = dtr.score(X_test, y_test) * 100\nprint(f'Decision Tree Regressor Model Score: {score_dtr_per}%')","aa0c8e40":"#MAE: DTR\n\nval_mae_dtr = mean_absolute_error(pred_dtr, y_test)\nprint(f'The quantity is off by: {val_mae_dtr} (MAE)')","0790085a":"#CV: DTR\ncv_results_dtr = cross_val_score(dtr, X, y, cv=5) #this is important so that we're not dependent on the subjective number for our test size of our train_test_split\ncv_results_dtr\nnp.mean(cv_results_dtr)","101589c3":"rfg = RandomForestRegressor()\n\nrfg.fit(X_train, y_train)\npred_rfg = rfg.predict(X_test)\nscore_rfg_per = rfg.score(X_test, y_test) * 100\nprint(f'Random Forest Regressor Model Score: {score_rfg_per}%')","5a728779":"#MAE: RFG\n\nval_mae_rfg = mean_absolute_error(pred_rfg, y_test)\nprint(f'The quantity is off by: {val_mae_rfg} (MAE)')","69fb0d81":"#CV: RFG\ncv_results_rfg = cross_val_score(dtr, X, y, cv=5) #this is important so that we're not dependent on the subjective number for our test size of our train_test_split\ncv_results_rfg\nnp.mean(cv_results_rfg)","e64a1686":" **Frequency of most bought items itemcode wise**","9de58097":"**Unique values in df**","5454ba1c":"# Examining the data","b99122cc":"**DecisionTreeRegressor**","d3ddf5d0":"**Indexing**","2641f297":"**Handling NaN values**","c2be9170":"I took inspiration from the following notebook: https:\/\/www.kaggle.com\/srinandhini\/eda-on-market-basket-analysis","cc508113":"# Machine Learning - Predicting Quantity","35556b0d":"**RandomForestRegressor**","7ad5197a":"**Correlation**","08f2ad40":"# **Data Visualization**","b0f18f49":"**Linear Regression**","b9cb5328":"# Top 5 countries: based on transactions","746c6cc9":"***TotalPrice - new column***"}}