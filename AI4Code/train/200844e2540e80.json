{"cell_type":{"1d3423a3":"code","a437f497":"code","69e16e7c":"code","1faffbd3":"code","ec508641":"code","e4767037":"code","05feb443":"markdown","018e8ed7":"markdown","46a0b259":"markdown","ea9f63e0":"markdown","3d12631f":"markdown","24f0e0b5":"markdown","002ec927":"markdown","f2290c7d":"markdown"},"source":{"1d3423a3":"!pip install pycaret","a437f497":"import os\nimport numpy as np \nimport pandas as pd \nfrom pycaret.classification import *\nimport matplotlib.pyplot as plt  \n%matplotlib inline","69e16e7c":"# Read in the training data\ntrain = pd.read_csv('..\/input\/overheadmnist\/version2\/train.csv')\ntrain.dropna(axis = 0, inplace = True)\ntrain.iloc[:, 1:] \/= 255.\n\n# Check for missing values\nprint(train.head().iloc[:, :5])\nprint(f'\\nThere are {train.isna().sum().sum()} missing examples.')","1faffbd3":"model_setup = setup(data = train, target = 'label', n_jobs = -1, \n                     session_id = 42, log_data = True, verbose = True, \n                     fold = 3, use_gpu = True, silent = True)","ec508641":"# Return parameters for top 3 models\nmodel_comp = compare_models(n_select = 3, verbose = True)   ","e4767037":" # View parameters in the final models\nmodel_comp     ","05feb443":"---\n# 6) Results & Discussion\n* CatBoost classifier has very large training time\n* Gradient Boosting family has best performance\n* Generality is not lost by reducing number of cross-validation folds\n\nThe increased number of examples in Version 2 changes the performance from the initial evaluation in the first. Here we see SVM perform poorly, while the gradient boosting models excel. Available models and hyper-parameters used are displayed below.","018e8ed7":"---\n# 3) Load & Format Data\nPycaret performs stratified k-fold cross validation naturally, so there is no need to split into training and validation groups. Pixel values are normalized before model creation.","46a0b259":"---\n# 4) Model Creation\n* This process can take several minutes\n* Reduce folds to avoid notebook timeout","ea9f63e0":"---\n---\n# 1) General Model Investigation\n\nPurpose:\n* Explore models with stratified, 10-fold cross validation using pycaret\n* Select 3 best models for further optimization","3d12631f":"---\n# 2) Installs & Imports\nThe pycaret module is not native and must be fully installed.","24f0e0b5":"---\n# 7) Conclusion\n> 1. CatBoost ---> .8285\n> 2. Light Gradient Boosting Machine ---> .7998\n> 3. Extreme Gradient Boosting ---> .7921\n\nDue to large training time, it may not be worth using CatBoost to get a 3% increase in accuracy. The most promising is xgboost, which performed just below lightgbm but in one-third the time.","002ec927":"## Next Steps\n* Optimize top three models\n* Test results of image formatting\n* Explore feature engineering\n---\n---","f2290c7d":"---\n# 5) Compare Classification Models\n* This takes several hours for large data sets\n* ***GPU REQUIRED***"}}