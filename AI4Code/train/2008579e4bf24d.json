{"cell_type":{"5f6a91ff":"code","626abad2":"code","9b148a41":"code","2f7eeea5":"code","41bba233":"code","baa3a3d9":"code","aff5bd76":"code","e28c39f3":"markdown","3f6edfd8":"markdown","2190e020":"markdown","a0c41128":"markdown","fd417369":"markdown","c5ccd202":"markdown","cba3a4e1":"markdown","3e83fc24":"markdown","36eb0fc0":"markdown"},"source":{"5f6a91ff":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version 1.7 --apt-packages libomp5 libopenblas-dev","626abad2":"! pip install pytorch-lightning==1.1.8","9b148a41":"\nimport torch\nfrom torch.nn import functional as F\nfrom torchvision import transforms\nimport pytorch_lightning as pl\n\nclass ImageClassifier(pl.LightningModule):\n\n    def __init__(self):\n        super(ImageClassifier, self).__init__()\n        # not the best model...\n        self.l1 = torch.nn.Linear(28 * 28, 10)\n\n    def forward(self, x):\n        # called with self(x)\n        return torch.relu(self.l1(x.view(x.size(0), -1)))\n    \n    def training_step(self, batch, batch_nb):\n        # REQUIRED\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log('train_loss', loss)\n        return loss\n\n    def validation_step(self, batch, batch_nb):\n        # OPTIONAL\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log('val_loss', loss, prog_bar=True)\n\n    def test_step(self, batch, batch_nb):\n        # OPTIONAL\n        x, y = batch\n        y_hat = self(x)\n        loss = F.cross_entropy(y_hat, y)\n        self.log('test_loss', loss, prog_bar=True)\n\n    def configure_optimizers(self):\n        # REQUIRED\n        # can return multiple optimizers and learning_rate schedulers\n        # (LBFGS it is automatically supported, no need for closure function)\n        return torch.optim.Adam(self.parameters(), lr=0.001)","2f7eeea5":"import os\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\nfrom torch.utils.data import random_split, DataLoader\n\n# train\/val split\nmnist_train = MNIST('', train=True, download=True, transform=transforms.ToTensor())\nmnist_train, mnist_val = random_split(mnist_train, [55000, 5000])\nmnist_train = DataLoader(mnist_train, batch_size=64, num_workers=4)\nmnist_val = DataLoader(mnist_val, batch_size=64, num_workers=4)\n\n# test split\nmnist_test = MNIST('', train=False, download=True, transform=transforms.ToTensor())\nmnist_test = DataLoader(mnist_test, batch_size=64, num_workers=4)","41bba233":"image_classifier = ImageClassifier()\n\n# most basic trainer, uses good defaults (8 TPU Cores)\n# training only for 20 epochs for demo purposes. For training longer simply adjust number below.\ntrainer = pl.Trainer(tpu_cores=8, max_epochs=2)    \ntrainer.fit(image_classifier, mnist_train, mnist_val)","baa3a3d9":"# ckpt_path=None simply uses the weights your model currently has!\ntrainer.test(image_classifier, ckpt_path=None, test_dataloaders=mnist_test)","aff5bd76":"%reload_ext tensorboard\n%tensorboard --logdir lightning_logs\/","e28c39f3":"# PyTorch Lightning TPU kernel\nUse this kernel to bootstrap a PyTorch project on TPUs using PyTorch Lightning\n\n## What is PyTorch Lightning?\nLightning is simply organized PyTorch code. There's NO new framework to learn.\nFor more details about Lightning visit the repo:\n\nhttps:\/\/github.com\/PyTorchLightning\/pytorch-lightning","3f6edfd8":"---\n## 3. Train on TPU\nThe Trainer automates the rest.\n\nTrains on 8 TPU cores","2190e020":"### Install XLA\nXLA powers the TPU support for PyTorch","a0c41128":"## View logs in tensorboard","fd417369":"## Run test set\nIn this example we used the test set to validation (a big no no), it's just for simplicity.\nIn real training, make sure to split the train set into train\/val and use test for testing.","c5ccd202":"## Install PyTorch Lightning","cba3a4e1":"---\n## 1. Define the LightningModule\nThis is just regular PyTorch organized in a specific format.\n\nNotice the following:\n- no TPU specific code\n- no .to(device)\n- np .cuda()\n\nFor a full intro, read the following:   \nhttps:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/introduction_guide.html","3e83fc24":"### Optional: DataModule\nFYI... \nLightning offers an optional abstraction called a DataModule which is simply a collection of dataloaders.\nFor complex datasets, we recommend you use this instead. It makes your code more reusable.\n\n[LightningDataModule docs](https:\/\/pytorch-lightning.readthedocs.io\/en\/stable\/datamodules.html)","36eb0fc0":"---\n## 2. Data\nLightning uses standard pytorch dataloaders.\nHere we just split the data into 3 sets, val, train, test"}}