{"cell_type":{"34a69d13":"code","c9b4feb9":"code","fead2d37":"code","e123d43d":"code","86f4b27f":"code","9d2b2491":"code","04297b9d":"code","d8ecf030":"code","47a90a3f":"code","e6685d57":"code","614fccb8":"code","33694710":"code","b72cb59a":"code","df5e1459":"code","fd00104c":"code","787f16a8":"code","61d05430":"code","d967d09c":"code","117ebfa5":"code","ff78c36a":"code","c286e569":"code","7496bd9b":"code","e5637503":"code","e46dd35d":"code","c2d3bdd9":"code","c9751ccb":"code","a71cafee":"code","942cebb1":"code","02c875a9":"code","da742083":"code","c88f24a9":"code","39454063":"code","c8a02d8b":"code","53ad7b3b":"code","dde6d10f":"code","f5bcd150":"code","cb7213cc":"code","92c132de":"code","4c16eb89":"code","fc32cde8":"code","fa3afbd3":"code","8061d185":"code","81be665c":"code","2fb7c5df":"code","8d035ea9":"code","17a9dbdf":"code","2685ba20":"code","c6768908":"code","5c929423":"code","82b1bcde":"code","cbedd6eb":"code","a29c9f47":"code","72018872":"code","03da4017":"code","db080bcd":"code","11176681":"code","f0cf1eeb":"code","c9a30b03":"code","8f23f971":"code","a4a0ff92":"code","2348ec76":"code","292d537b":"code","3c879606":"code","d41c1b64":"code","e79b6b6f":"code","bb9584e3":"code","dd56b2b6":"code","cab952e0":"code","9391b2f0":"code","d213b63f":"code","bb41338a":"code","9b585536":"code","fe4eca61":"code","d9565ede":"code","9153e6f8":"code","44949b18":"code","8ddb5b3c":"code","f603279e":"code","1dafafbe":"code","20721e82":"code","57a31457":"code","36a62633":"code","ea109f6b":"markdown","75d05865":"markdown","8b392ba8":"markdown","fa9b8f3c":"markdown","e423ebae":"markdown","f684c596":"markdown","df22743a":"markdown","0e866cf2":"markdown","b7ec546e":"markdown","aac12ae1":"markdown","dbf929cb":"markdown","c3ef6179":"markdown","2ac1538a":"markdown","e788cc7e":"markdown","0fe1427b":"markdown","88004e26":"markdown","4734b124":"markdown","12512827":"markdown","3a8a812d":"markdown","c8560a6a":"markdown","5f6b5d4c":"markdown","49b2324b":"markdown","a3ee4611":"markdown","c7b0c738":"markdown","347c55e4":"markdown","107e1fc2":"markdown","7be0f660":"markdown","df6873dd":"markdown","b1f47b0e":"markdown","78e023da":"markdown","4aea44b8":"markdown","c263a8cc":"markdown","1b3c4de6":"markdown","9b304261":"markdown","ad89a163":"markdown","0caa7f8d":"markdown","40291a04":"markdown","2f99b111":"markdown","7300adf3":"markdown","e8585945":"markdown","866bf4cc":"markdown","f983d92d":"markdown","371c8d8b":"markdown","a169ecd1":"markdown","4b146706":"markdown","5c3bff20":"markdown","c2092b1d":"markdown","8ffc676d":"markdown","6abd8147":"markdown","aba5a408":"markdown","62690765":"markdown","9514b7fc":"markdown","ad786ae0":"markdown","40c447ae":"markdown","de9cc70d":"markdown","a0ec0325":"markdown","359b24c2":"markdown","07a3f2a7":"markdown","a468b55e":"markdown","7da101d7":"markdown","936e9cf9":"markdown","a7b7d081":"markdown"},"source":{"34a69d13":"import nltk\n\n\n#more libraries that can be will be used\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport re","c9b4feb9":"#Below we read in our data for our training set and test parameters\nmbti = pd.read_csv('..\/input\/train.csv')\nmbti_test= pd.read_csv('..\/input\/test.csv')\n\n# List of mbti types \ntype_labels = ['ISTJ', 'ISFJ', 'INFJ', 'INTJ', \n               'ISTP', 'ISFP', 'INFP', 'INTP', \n               'ESTP', 'ESFP', 'ENFP', 'ENTP', \n               'ESTJ', 'ESFJ', 'ENFJ', 'ENTJ']","fead2d37":"mbti.head()","e123d43d":"mbti_test.head()","86f4b27f":"plt.figure(figsize=[16, 9])\nmbti['type'].value_counts().plot(kind = 'bar')\nplt.show()","9d2b2491":"all_mbti = mbti","04297b9d":"all_mbti2 = mbti_test","d8ecf030":"all_mbti=all_mbti.rename(columns={'posts':'post'})","47a90a3f":"all_mbti2=all_mbti2.rename(columns={'posts':'post'})\n","e6685d57":"pattern_url = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\nsubs_url = r'url-web'\nall_mbti['post'] = all_mbti['post'].replace(to_replace = pattern_url, value = subs_url, regex = True)","614fccb8":"all_mbti.head()","33694710":"all_mbti2.head()","b72cb59a":"all_mbti2['post'] = all_mbti2['post'].replace(to_replace = pattern_url, value = subs_url, regex = True)","df5e1459":"all_mbti2.head()","fd00104c":"# first we make everything lower case to remove some noise from capitalisation\nall_mbti['post'] = all_mbti['post'].str.lower()","787f16a8":"all_mbti2['post'] = all_mbti2['post'].str.lower()","61d05430":"import string","d967d09c":"def remove_punctuation(post):\n    return ''.join([l for l in post if l not in string.punctuation])","117ebfa5":"all_mbti['post'] = all_mbti['post'].apply(remove_punctuation)","ff78c36a":"all_mbti2['post'] = all_mbti2['post'].apply(remove_punctuation)","c286e569":"all_mbti.head()","7496bd9b":"from nltk.tokenize import word_tokenize, TreebankWordTokenizer","e5637503":"# we will use the TreeBankWordTokenizer since it is MUCH quicker than the word_tokenise function\ntokeniser = TreebankWordTokenizer()\nall_mbti['tokens'] = all_mbti['post'].apply(tokeniser.tokenize)\nall_mbti2['tokens'] = all_mbti2['post'].apply(tokeniser.tokenize)","e46dd35d":"from nltk import SnowballStemmer, PorterStemmer, LancasterStemmer","c2d3bdd9":"stemmer = SnowballStemmer('english')","c9751ccb":"def mbti_stemmer(words, stemmer):\n    return [stemmer.stem(word) for word in words]    ","a71cafee":"# stem all words in the mbti dataframe\nall_mbti['stem'] = all_mbti['tokens'].apply(mbti_stemmer, args=(stemmer, ))\nall_mbti2['stem'] = all_mbti2['tokens'].apply(mbti_stemmer, args=(stemmer, ))","942cebb1":"all_mbti['mind'] = all_mbti['type'].apply(lambda x: x[0] == 'E').astype('int')\nall_mbti['energy'] = all_mbti['type'].apply(lambda x: x[1] == 'N').astype('int')\nall_mbti['nature'] = all_mbti['type'].apply(lambda x: x[2] == 'T').astype('int')\nall_mbti['tactics'] = all_mbti['type'].apply(lambda x: x[3] == 'J').astype('int')","02c875a9":"all_mbti.head()","da742083":"plt.figure(figsize=[16, 9])\n# mind category\nlabels = ['Extraversion', 'Introversion']\nsizes = [all_mbti['mind'].value_counts()[1], all_mbti['mind'].value_counts()[0]]\n\nfig, ax = plt.subplots(2, 2, figsize=(8, 8))\nax[0, 0].pie(sizes, labels=labels, autopct='%1.1f%%',\n             shadow=False, startangle=90)\nax[0, 0].axis('equal')\n\n# energy category\nlabels = ['Intuitive', 'Observant']\nsizes = [all_mbti['energy'].value_counts()[1],all_mbti['energy'].value_counts()[0]]\n\nax[0, 1].pie(sizes, labels=labels, autopct='%1.1f%%',\n             shadow=False, startangle=90)\nax[0, 1].axis('equal')\n\n# nature category\nlabels = ['Thinking', 'Feeling']\nsizes = [all_mbti['nature'].value_counts()[1], all_mbti['nature'].value_counts()[0]]\n\nax[1, 0].pie(sizes, labels=labels, autopct='%1.1f%%',\n             shadow=False, startangle=90)\nax[1, 0].axis('equal')\n\n# tactics category\nlabels = ['Judging', 'Prospecting']\nsizes = [all_mbti['tactics'].value_counts()[1], all_mbti['tactics'].value_counts()[0]]\n\nax[1, 1].pie(sizes, labels=labels, autopct='%1.1f%%',\n             shadow=False, startangle=90)\nax[1, 1].axis('equal')\nplt.tight_layout()\nplt.show()","c88f24a9":"from nltk.stem import WordNetLemmatizer\nnltk.download('wordnet')\n\nlemmatizer = WordNetLemmatizer()","39454063":"def mbti_lemma(words, lemmatizer):\n    return [lemmatizer.lemmatize(word) for word in words]    ","c8a02d8b":"# lemmatize all words in dataframe\nall_mbti['lemma'] = all_mbti['tokens'].apply(mbti_lemma, args=(lemmatizer, ))\nall_mbti2['lemma'] = all_mbti2['tokens'].apply(mbti_lemma, args=(lemmatizer, ))","53ad7b3b":"from nltk.corpus import stopwords","dde6d10f":"def bag_of_words_count(words, word_dict={}):\n    \"\"\" this function takes in a list of words and returns a dictionary \n        with each word as a key, and the value represents the number of \n        times that word appeared\"\"\"\n    for word in words:\n        if word in word_dict.keys():\n            word_dict[word] += 1\n        else:\n            word_dict[word] = 1\n    return word_dict","f5bcd150":"# here we create a set of dictionaries\n# one for each of the MBTI types\npersonality = {}\nfor pp in type_labels:\n    df = all_mbti.groupby('type')\n    personality[pp] = {}\n    for row in df.get_group(pp)['tokens']:\n        personality[pp] = bag_of_words_count(row, personality[pp])       ","cb7213cc":"f=pd.DataFrame(personality)","92c132de":"f.head()","4c16eb89":"# next we create a list of all of the unique words...\nall_words = set()\nfor pp in type_labels:\n    for word in personality[pp]:\n        all_words.add(word)","fc32cde8":"# so that we can create a dictionary of bag of words for the whole dataset\npersonality['all'] = {}\nfor pp in type_labels:    \n    for word in all_words:\n        if word in personality[pp].keys():\n            if word in personality['all']:\n                personality['all'][word] += personality[pp][word]\n            else:\n                personality['all'][word] = personality[pp][word]","fa3afbd3":"# lets have a look at the distrbution of words\nplt.hist([v for v in personality['all'].values()])","8061d185":"# how many words in total?\nsum([v for v in personality['all'].values()])","81be665c":"# how many words appear only once?\nlen([v for v in personality['all'].values() if v == 1])","2fb7c5df":"# how many words appear more than 100 times?\n# how many words of the total does that account for?\nprint (len([v for v in personality['all'].values() if v >= 100]))\nprint (sum([v for v in personality['all'].values() if v >= 100]))","8d035ea9":"11013261\/11681001","17a9dbdf":"max_count = 100\nword_index = [k for k, v in personality['all'].items() if v > max_count]","2685ba20":"# now let's create one big data frame with the word counts by personality profile\nhm = []\nfor p, p_bow in personality.items():\n    df_bow = pd.DataFrame([(k, v) for k, v in p_bow.items() if k in word_index], columns=['Word', p])\n    df_bow.set_index('Word', inplace=True)\n    hm.append(df_bow)\n\n# create one big data frame\ndf_bow = pd.concat(hm, axis=1)\ndf_bow.fillna(0, inplace=True)","c6768908":"# what are the top 10 words that appear most often?\ndf_bow.sort_values(by='all', ascending=False).head(10)","5c929423":"intro_types = [p for p in type_labels if p[0] == 'I']","82b1bcde":"intro_types","cbedd6eb":"df_bow['I'] = df_bow[intro_types].sum(axis=1)","a29c9f47":"# convert to percentages\nfor col in ['I', 'all']:\n    df_bow[col+'_perc'] = df_bow[col] \/ df_bow[col].sum()","72018872":"df_bow['chi2'] = np.power((df_bow['I_perc'] - df_bow['all_perc']), 2) \/ df_bow['all_perc']","03da4017":"df_bow[['I_perc', 'all_perc', 'chi2']][df_bow['I_perc'] > df_bow['all_perc']].sort_values(by='chi2', ascending=False).head(10)","db080bcd":"df_bow[['I_perc', 'all_perc', 'chi2']][df_bow['I_perc'] < df_bow['all_perc']].sort_values(by='chi2', ascending=False).head(20)","11176681":"from sklearn.feature_extraction.text import CountVectorizer","f0cf1eeb":"vect = CountVectorizer(stop_words='english', ngram_range=(1, 4),max_df=0.5)","c9a30b03":"def untokenize(lst):\n    return ' '.join([x for x in lst])","8f23f971":"all_mbti['untokens']=all_mbti['tokens'].apply(untokenize)\nall_mbti['unstem']=all_mbti['stem'].apply(untokenize)\nall_mbti['unlemma']=all_mbti['lemma'].apply(untokenize)","a4a0ff92":"column='unstem'\n#column='post'\n#column='untokens'\n#column='unlemma'\nX_count = vect.fit_transform(all_mbti[column])","2348ec76":"from sklearn.linear_model import LogisticRegression\nlogreg_mind= LogisticRegression()\nlogreg_energy= LogisticRegression()\nlogreg_nature= LogisticRegression()\nlogreg_tactics= LogisticRegression()\n\n","292d537b":"X = X_count # creating our X variable","3c879606":"y = all_mbti['mind'] # creating our Y mind variable\n","d41c1b64":"y2 = all_mbti['energy'] # creating our Y energy variable\n","e79b6b6f":"y3 = all_mbti['nature'] # creating our Y nature variable\n","bb9584e3":"y4 = all_mbti['tactics'] # creating our Y tactics variable","dd56b2b6":"#tester\nall_mbti2[column]=all_mbti2['stem'].apply(untokenize)","cab952e0":"#tester\nX_test=all_mbti2[column]","9391b2f0":"#tester\nX_count_test = vect.transform(X_test)","d213b63f":"mind_lm_log=logreg_mind.fit(X, y)","bb41338a":"energy_lm_log= logreg_energy.fit(X, y2)\n","9b585536":"nature_lm_log= logreg_nature.fit(X, y3)\n","fe4eca61":"tactics_lm_log= logreg_tactics.fit(X, y4)","d9565ede":"mind_pred= mind_lm_log.predict(X_count_test)\nenergy_pred= energy_lm_log.predict(X_count_test)\nnature_pred=nature_lm_log.predict(X_count_test)\ntactis_pred=tactics_lm_log.predict(X_count_test)","9153e6f8":"all_mbti2['mind']=pd.DataFrame(mind_pred,columns=['mind'])\nall_mbti2['energy']=pd.DataFrame(energy_pred,columns=['energy'])\nall_mbti2['nature']=pd.DataFrame(nature_pred,columns=['nature'])\nall_mbti2['tactics']=pd.DataFrame(tactis_pred,columns=['tactics'])\n","44949b18":"submition_concept=all_mbti2[['id','mind','energy','nature','tactics']]","8ddb5b3c":"submition_concept.head(10)","f603279e":"submition_concept.to_csv('final_submit21.csv',index=False)","1dafafbe":"#from sklearn.svm import SVC\n\n#svc_mind  = SVC(kernel='rbf')\n#svc_energy = SVC(kernel='rbf')\n#svc_nature = SVC(kernel='rbf')\n#svc_tactics = SVC(kernel='rbf')\n","20721e82":"#mind_lm_=svc_mind.fit(X, y)\n#energy_lm_= svc_energy.fit(X, y2)\n#nature_lm_= svc_nature.fit(X, y3)\n#tactics_lm_= svc_tactics.fit(X, y4)","57a31457":"#mind_pred2= mind_lm_.predict(X_count_test)\n#energy_pred2= energy_lm_.predict(X_count_test)\n#nature_pred2=nature_lm_.predict(X_count_test)\n#tactis_pred2=tactics_lm_.predict(X_count_test)","36a62633":"#all_mbti2['mind2']=pd.DataFrame(mind_pred2,columns=['mind'])\n#all_mbti2['energy2']=pd.DataFrame(energy_pred2,columns=['energy'])\n#all_mbti2['nature2']=pd.DataFrame(nature_pred2,columns=['nature'])\n#all_mbti2['tactics2']=pd.DataFrame(tactis_pred2,columns=['tactics'])","ea109f6b":"### Creating binary values for each personality type","75d05865":"Thats not very helpful at all, is it! Its very difficult to extract insight from this data.  Lets see if we can use the $chi^2$ test to see whether Introverts favour the word **`I`**","8b392ba8":"# Conclusion","fa9b8f3c":"# SOME THINGS THAT ARE INTERESSTING TO NOTE...","e423ebae":"N-grams are contiguous sequences of n-items in a sentence. N can be 1, 2 or any other positive integers, although usually we do not consider very large N because those n-grams rarely appears in many different places.\n\nWhen performing machine learning tasks related to natural language processing, we usually need to generate n-grams from input sentences. For example, in text classification tasks, in addition to using each individual token found in the corpus, we may want to add bi-grams or tri-grams as features to represent our documents. This post describes several different ways to generate n-grams quickly from input sentences in Python.\n\nWith CountVectoriser we will use ngrams with a minimum of 1 and a maximum of 4. ","f684c596":"# Now back to our code","df22743a":"This kernel serves as to illustrate the journey of Team 14 (aka: we_arent_for_teens) as we ventured into the world of machine learning. This task required for us to use  Natural Language Processing to convert data into machine learning format. The data that ultimtely got processed would then be used to train a classifier capable of assigning MBTI labels to a person's online forum posts.\n\nEach MBTI personality type consisted of four binary variables, them being the following: \n- Mind: Introverted (I) or Extraverted (E) \n- Energy: Sensing (S) or Intuitive (N) \n- Nature: Feeling (F) or Thinking (T) \n- Tactics: Perceiving (P) or Judging (J)\n","0e866cf2":"#### 1. Introduction\n#### 2. Stemming\n#### 3. Lementization\n#### 4. A quick discussion on stopwords\n#### 5. Modelling","b7ec546e":"Text feature extraction is the process of transforming what is essentially a list of words into a feature set that is usable by a classifier. The NLTK classifiers expect dict style feature sets, so we must therefore transform our text into a dict. The Bag of Words model is the simplest method; it constructs a word presence feature set from all the words of an instance.","aac12ae1":"### Let us venture forward still.","dbf929cb":"### (DISCLAIMER : THE CODE WHICH FOLLOWS MAINLY COMES FROM THE EDSA TRAIN ... \"How do machines understand language\")","c3ef6179":"Now to have a look see as to how whether this code really worked or not...\n","2ac1538a":"# REFERENCES:","e788cc7e":"Natural language processing (NLP) is a field concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data.\n\nChallenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation. It takes a lot for machines to understand us humans, lets hope this kernel shows how Natural Language Process is executed. \n\nBased on the outputs we had above, our Logistic Regression did the best of the two. Our Logisitic Regression model gave us a score of 5.24140 whereass  our Support Vector Machine model gave us a score of 16.8790. ","0fe1427b":"Lets have a look at how many of the different MBTI types we have data for. From the graph below it is really apparent that the data set has had a majority of people that are introvert, we chose to keep it as is based on the fact we don't want to lose data that may be essential","88004e26":"## The reason for all of lemmantizing, stemming etc, is the following...","4734b124":"A tokenizer divides text into a sequence of tokens, which roughly correspond to \"words\". We will use tokenisers to clean up the data, making it ready for analysis. This column we create will be of great assistance when we lemmentize and find the stem","12512827":"### Logistic Regression","3a8a812d":"Lets just quickly have a look-see at our training and and testing tables...","c8560a6a":"Using words that appear more than 100 times seems much more useful!  And this accounts for 94% of all the words!","5f6b5d4c":"In our case we will create multiple columns in which we tokenize, stem and lemmentize our post column. But before we get too ahead of ourselves lets import our data...","49b2324b":"Below we initialise our variables which we ultimately test with.","a3ee4611":"# Explore Data Science Academy - Team 14 JHB Submission \n# (AKA:we_aren't_for_teens)\n ","c7b0c738":"We used the NLTK - natural language toolkit - as our library for building Python programs to work with human language data. The main advantage of this Library is that it provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning,, wrappers for industrial-strength NLP libraries, and an active discussion forum.","347c55e4":"We chose to choose the stemmed column column. Seen as our score on Kaggle gave us a score of 10.2342 when we used the lemma and tokens columns","107e1fc2":"### Library we tried using...","7be0f660":"### Bag of words","df6873dd":"Lets have a quick look at our table before we submit.","b1f47b0e":"# 2. Stemming","78e023da":"## 3. Lemmatization","4aea44b8":"Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. ","c263a8cc":"https:\/\/en.wikipedia.org\/wiki\/Natural_language_processing \n\nhttps:\/\/en.wikipedia.org\/wiki\/Lemmatisation\n\nhttps:\/\/athena2.explore-datascience.net\/student\/sprint-tutorial\/view\/7\/17\/425\n\nhttps:\/\/en.wikipedia.org\/wiki\/Natural_language_processing\n\n\n","1b3c4de6":"### Let's now import the data and clean it up a bit","9b304261":"## now to make our predictions...","ad89a163":"## Some parmaters to first consider...","0caa7f8d":"### Approach...","40291a04":"The CountVectoriser function converts a collection of text documents to a matrix of token counts. This implementation produces a sparse representation of the counts using scipy.sparse.csr_matrix.\nIf you do not provide an a-priori dictionary and you do not use an analyzer that does some kind of feature selection then the number of features will be equal to the vocabulary size found by analyzing the data.","2f99b111":"### Notebook created by Jan Du Doit, Sibusiso Ndimande ,Kagiso Julia Maifala ,Siyanda Dlamini","7300adf3":"### Set Posts to lower case","e8585945":"### Remove punctuation marks","866bf4cc":"### Removing Unneccesary information","f983d92d":"### Another model we tried...","371c8d8b":"# 5. Modelling","a169ecd1":"## now to make our predictions...","4b146706":"And there it is! What can we conclude from this:\n* I is the 9th most introverted word, by expectation\n* Introverts tend to post more urls than extroverted people too! \n* The introverted types are more likely to be written by Introverts, maybe because people post about their own types?","5c3bff20":"### Creating a Tokenised column","c2092b1d":"Stemming is a process where words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix. It is the process of transforming to the root word, that is, it uses an algorithm that removes common words, such as \u201cly,\u201d \u201ces,\u201d \u201ced,\u201d and \u201cs.\u201d \n\n* Porter\n* Lancaster\n* Snowball","8ffc676d":"With text analytics, we have to ensure that our data is in a usable format. We will have to undergo the following steps:\n\n* removing the web-urls\n* making everything lower case\n* removing punctuation\n\n**[Regular expressions](https:\/\/www.regular-expressions.info\/)** are useful for extracting information from text. \n","6abd8147":"## A quick discussion on CountVectorizer... ","aba5a408":"# 4. A quick word on Stop Words...","62690765":"From here we create new columns where we have untokenized the stemm, lemma and token columns. This is done because CountVectoriser runs full sentences instead of arrays. These new columns will still represent the stemmed, tokenized and lemma columns though.","9514b7fc":"There are a lot of words that only appear once! Let's remove them.","ad786ae0":"# 1. Introduction","40c447ae":"### now to fit our model...","de9cc70d":"![image.png](attachment:image.png)","a0ec0325":"The support vector machine (SVM) too fits a linear decision boundary as was the case in logistic regression. \nUnlike logistic regression, using a procedure called the kernel trick, it is possible for an SVM to fit a non-linear decision boundary. \nFurthermore, unlike logistic regression, the SVM can be used for both classification and regression. \nIn sklearn, these are implemented as SVC (Support Vector Classifier) and SVR (Support Vector Regression) respectively.","359b24c2":"The graph shows the various distributions of each personality type.","07a3f2a7":"## The support vector machine","a468b55e":"Most engines are programmed to remove certain words from any index entry. The list of words that are not to be added is called a stop list. Stop words are deemed irrelevant for searching purposes because they occur frequently in the language for which the indexing engine has been tuned. In order to save both space and time, these words are dropped at indexing time and then ignored at search time. In our case, we will choose to use the CountVectoriser that can remove the stopwords - we'll get to that in due time... ","7da101d7":"All that pre-processing allows us to finally do some analysis!  lets see what the 20 most common words in the whole text are. (Remember your first coding challenges?)","936e9cf9":"With regards to our model, finally, we started by using the Logisitic regression. Going down, we define our x variable and finally each of our y variables for the mind, energy, tactics and nature respectivley.","a7b7d081":" On Kaggle, the code above gave a score of 5.2414"}}