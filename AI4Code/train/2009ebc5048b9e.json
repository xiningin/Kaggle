{"cell_type":{"955a26b0":"code","3ab629c2":"code","2c1b42c9":"code","9a9a05db":"code","03d9ce15":"code","d3c678ce":"code","696b2765":"code","20d63cac":"code","eba1229c":"code","0e9ae231":"code","fe03cc66":"code","4e1b7c6d":"code","52c64939":"code","ac97094d":"code","cd2fdf6c":"code","7bdb8dde":"code","f6afb437":"code","dfcd9a99":"code","ccdbad96":"code","eb5a1b26":"code","c96bc9c9":"code","eaa9ace3":"code","852455d1":"code","bff14ca6":"code","eb3d5e6b":"code","5949f282":"code","193001cf":"code","7a545310":"code","ac18f1f8":"code","ac65f27c":"code","8c04a2c3":"code","f1c998b8":"code","f182cf0b":"code","6dfd942b":"code","b249639a":"code","a1610e5b":"code","00e02096":"code","b4c5f6c8":"code","6cd4ebee":"code","205d213b":"code","f971426f":"code","12125bcc":"code","faf79158":"code","a8312cd4":"code","161978ae":"code","f4c9dead":"code","c1493f93":"code","97628692":"code","59510b6b":"code","1e71d822":"code","47fe32f0":"code","cff4d7b5":"code","da9c28cd":"code","5229780c":"markdown","0161d0ca":"markdown","bc468bf2":"markdown","7bc6c919":"markdown"},"source":{"955a26b0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split","3ab629c2":"data = pd.read_csv(\"..\/input\/loan-prediction-problem-dataset\/train_u6lujuX_CVtuZ9i.csv\")","2c1b42c9":"data.head()","9a9a05db":"data.info()","03d9ce15":"#convert 'Credit_History' as an object because it is cointaing just 0 and 1\ndata['Credit_History'] = data['Credit_History'].astype('O')","d3c678ce":"data.describe()","696b2765":"data.describe(include='O')","20d63cac":"X = data.drop(labels=['Loan_ID','Loan_Status'], axis=1)\nX","eba1229c":"Y = data['Loan_Status']\nY ","0e9ae231":"from sklearn.preprocessing import LabelEncoder  \nle = LabelEncoder()\nY = le.fit_transform(Y)","fe03cc66":"X.shape, Y.shape","4e1b7c6d":"X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=0)\nX_train.info()","52c64939":"X_train.isnull().mean().sort_values(ascending=False)","ac97094d":"pd.DataFrame(y_train).isnull().mean()","cd2fdf6c":"features_numeric = ['Loan_Amount_Term','LoanAmount','CoapplicantIncome','ApplicantIncome']\nfeatures_categoric_one = ['Gender','Married','Dependents','Property_Area','Education']\nfeatures_categoric_two = ['Credit_History','Self_Employed']\n","7bdb8dde":"#for mising data more than 5% we will replace nan value with a word \"Missing\"\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer","f6afb437":" imputer_numeric = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n])\n\nimputer_categoric_one = Pipeline(\n    steps=[('imputer',\n            SimpleImputer(strategy='most_frequent'))])\n\nimputer_categoric_two = Pipeline(\n    steps=[('imputer',\n            SimpleImputer(strategy='constant', fill_value='Missing'))])\n\n# then we put the features list and the transformers together\n# using the column transformer\npreprocessor = ColumnTransformer(transformers=[\n    ('imputer_numeric',imputer_numeric,features_numeric),\n    ('imputer_categoric_one',imputer_categoric_one,features_categoric_one),\n    ('imputer_categoric_two',imputer_categoric_two,features_categoric_two)\n])","dfcd9a99":"preprocessor.fit(X_train)","ccdbad96":" preprocessor.transformers","eb5a1b26":"# and we can look at the parameters learnt like this:\n\n# for the numerical imputer\npreprocessor.named_transformers_['imputer_numeric'].named_steps['imputer'].statistics_","c96bc9c9":"preprocessor.named_transformers_['imputer_categoric_one'].named_steps['imputer'].statistics_","eaa9ace3":"preprocessor.named_transformers_['imputer_categoric_two'].named_steps['imputer'].statistics_","852455d1":"#First Transformation\nX_train = preprocessor.transform(X_train)\nX_test = preprocessor.transform(X_test)","bff14ca6":"X_train =  pd.DataFrame(X_train, columns=features_numeric+features_categoric_one+features_categoric_two)\nX_test =  pd.DataFrame(X_test, columns=features_numeric+features_categoric_one+features_categoric_two) ","eb3d5e6b":"X_train.isnull().mean()","5949f282":"X_train['LoanAmount'].nunique()","193001cf":"X_test['Loan_Amount_Term'].unique()\nX_test_cp = X_test[X_test['Loan_Amount_Term'] != 12.0 ]","7a545310":"X_test_cp['LoanAmount'].nunique()","ac18f1f8":"X_test.isnull().mean()","ac65f27c":" X_train.shape, X_test.shape","8c04a2c3":" for col in features_categoric_one+features_categoric_two:\n        print(X_train[col].value_counts())\n        print(\"--------------------------\")","f1c998b8":"from sklearn.preprocessing import OneHotEncoder","f182cf0b":"X_train['Credit_History'] = X_train['Credit_History'].astype(str)\nX_test['Credit_History'] = X_test['Credit_History'].astype(str)","6dfd942b":"ct = ColumnTransformer(\n    [('one_hot_encoder', OneHotEncoder(categories='auto'), [4,5,6,7,8,9,10])],   # The column numbers to be transformed (here is [0] but can be [0, 1, 3])\n    remainder='passthrough'                                         # Leave the rest of the columns untouched\n)","b249639a":"X_train = ct.fit_transform(X_train)\nX_test = ct.transform(X_test)","a1610e5b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nmodels = {\n    'LogisticRegression': LogisticRegression(random_state=42),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'SVC': SVC(random_state=42),\n    'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=1, random_state=42)\n}","00e02096":"\nfrom sklearn.metrics import precision_score, recall_score, f1_score, log_loss, accuracy_score\n\ndef loss(y_true, y_pred, retu=False):\n    pre = precision_score(y_true, y_pred)\n    rec = recall_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    loss = log_loss(y_true, y_pred)\n    acc = accuracy_score(y_true, y_pred)\n    \n    if retu:\n        return pre, rec, f1, loss, acc\n    else:\n        print('  pre: %.3f\\n  rec: %.3f\\n  f1: %.3f\\n  loss: %.3f\\n  acc: %.3f' % (pre, rec, f1, loss, acc))","b4c5f6c8":"# train_eval_train\n\ndef train_eval_train(models, X, y):\n    for name, model in models.items():\n        print(name,':')\n        model.fit(X, y)\n        loss(y, model.predict(X))\n        print('-'*40)\n        \ntrain_eval_train(models, X_train, y_train)\n\n# we can see that best model is LogisticRegression at least for now, \n#SVC is just memorizing the data so it is overfitting .","6cd4ebee":"for name, model in models.items():\n        print(name,':')\n        loss(y_test, model.predict(X_test))\n        print(\"------------------------------\")","205d213b":"#since we are getting maximum accuraccy with Decision tree, we will go ahead with Decison tree,\n#reading submission data\ndata_sub = pd.read_csv(\"..\/input\/loan-prediction-problem-dataset\/test_Y3wMUE5_7gLdaTN.csv\")","f971426f":"data_sub.head()","12125bcc":"data_sub.isnull().mean().sort_values(ascending=False)","faf79158":"loan_id = data_sub[\"Loan_ID\"]\ndata_sub = data_sub.drop(labels=[\"Loan_ID\"], axis =1)\ndata['Credit_History'] = data['Credit_History'].astype('O')","a8312cd4":"data_sub = preprocessor.transform(data_sub)","161978ae":"data_sub =  pd.DataFrame(data_sub, columns=features_numeric+features_categoric_one+features_categoric_two)","f4c9dead":"data_sub.isnull().mean()","c1493f93":"data_sub['Credit_History'] = data_sub['Credit_History'].astype(str)","97628692":"#doing label encoding\nfor col in data_sub.columns:\n    print(data_sub[col].dtype)","59510b6b":"data_sub = ct.transform(data_sub.fillna(method='bfill'))","1e71d822":"final_pred = models['DecisionTreeClassifier'].predict(data_sub)","47fe32f0":"loan_id","cff4d7b5":"d = pd.DataFrame([loan_id.astype(str), final_pred.astype(str)])\nd = d.transpose()","da9c28cd":"d.to_csv('submission1.csv', index=False)","5229780c":"Importing the Dataset","0161d0ca":"removing missing data\n","bc468bf2":"In this notebook, I have tried to create a pipeline model for imputing missing data and doing Label encoding. To reduce the work further you can create the function. This is still work in progrss. I haven't added much markdown becuase each cell is self explaintory. \n\nIn the coming version, I will be adding more graps visualizations and we will try to incease the precison of the model.","7bc6c919":"Features are replaced as belwo rule:\n   1. <= 5% and categorlcal: with most frequent data\n   2. > 5% and categoriacl : with word \"Missing\"\n   3. < 5% and numerical: with mean"}}