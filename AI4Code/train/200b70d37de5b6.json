{"cell_type":{"ed8824d4":"code","e6c1f8b9":"code","29f39cb1":"code","b81f6526":"code","15fea111":"code","007a8db4":"code","76132f8e":"code","69e405ff":"code","cdd3e8e1":"code","536f8aa3":"code","fea81890":"code","d5c9ae29":"code","fd5001b0":"code","adb277a0":"code","2f7367c4":"code","cd11cd20":"code","d788ef4e":"code","aab34154":"code","e78d2aa8":"code","1c218152":"code","ba8c540c":"code","5eb9010d":"code","d636340f":"code","a55e6aeb":"code","7297ee2f":"code","af9ad8ae":"code","3fb0b133":"code","be208600":"code","eccaf2a7":"code","22f7d4c6":"code","a6021d48":"code","8d7694bc":"code","c6311e92":"code","37ca15f4":"code","aa2949cf":"code","a4efc115":"code","abcb2d88":"code","c35127e6":"code","529adb85":"code","344701c3":"code","d36fe766":"code","893eb1ad":"code","c94eb8d3":"code","32a5dd96":"code","33b2867a":"code","68c6dd74":"code","9fc4caf3":"code","d1db3d31":"code","b5fcb8dc":"code","56972518":"code","ee57b45e":"code","6f8f0e26":"code","4d3ab472":"code","8d165aa1":"code","c022b384":"code","5044c192":"code","90eadcb3":"code","4def5bc7":"code","5a965d19":"code","30da645b":"code","9b52bcfa":"code","4cbea463":"code","88aec697":"code","edfc396e":"code","7c1e4193":"code","ffe73bf3":"code","130c5b95":"code","b49549c6":"code","9c146023":"code","525ab609":"markdown","64e0bcce":"markdown","16068a71":"markdown","90db9045":"markdown","b66685f8":"markdown","6ccbaf35":"markdown","b4eb6615":"markdown","627da801":"markdown","966a0a72":"markdown","25e32ebf":"markdown","e6285d26":"markdown","eddd8f35":"markdown","8931d6bf":"markdown","8f466db0":"markdown","79d3f93a":"markdown","b96e412c":"markdown","04949251":"markdown","3854cf57":"markdown","6d505103":"markdown","d81ccf53":"markdown","d5253ccb":"markdown","39269c0b":"markdown","5c5a861b":"markdown","6ddd3a6a":"markdown","0ffaf4cd":"markdown","f22630c7":"markdown","00853c4e":"markdown"},"source":{"ed8824d4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport missingno as msno\nimport re\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e6c1f8b9":"train_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n# train_df = train_df.replace({np.nan: None})\n\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n# test_df = test_df.replace({np.nan: None})","29f39cb1":"train_df.head()","b81f6526":"train_df.tail()","15fea111":"train_df.info()","007a8db4":"train_df.shape","76132f8e":"test_df.head()","69e405ff":"train_df = train_df.drop('PassengerId',axis=1)\ntest_df = test_df.drop(\"PassengerId\",axis=1)","cdd3e8e1":"print(\"Training set columns:\\n\",train_df.columns)\nprint(\"Testing set columns:\\n\",test_df.columns)","536f8aa3":"train_df[\"Embarked\"].unique()","fea81890":"train_df.dtypes","d5c9ae29":"def convert_to_categorical_data_type(df,is_train=True):\n    if is_train:\n        df[\"Survived\"] = pd.Categorical(df[\"Survived\"])\n    df[\"Pclass\"] = pd.Categorical(df[\"Pclass\"])\n    df[\"Sex\"] = pd.Categorical(df[\"Sex\"])\n    df[\"Embarked\"] = pd.Categorical(df[\"Embarked\"])\n    print(df.dtypes)","fd5001b0":"print(\"For training set:\")\nconvert_to_categorical_data_type(train_df)","adb277a0":"print(\"For testing set:\")\nconvert_to_categorical_data_type(test_df,False)","2f7367c4":"msno.bar(train_df)","cd11cd20":"train_df = train_df.drop([\"Cabin\"],axis=1)\ntest_df = test_df.drop([\"Cabin\"],axis=1)","d788ef4e":"train_df = train_df[train_df[\"Embarked\"].notna()]","aab34154":"msno.matrix(train_df)","e78d2aa8":"msno.bar(test_df)","1c218152":"train_df[\"train_set\"] = True\ntest_df[\"Survived\"] = None\ntest_df[\"train_set\"] = False","ba8c540c":"print(train_df.columns)\nprint(test_df.columns)","5eb9010d":"df = pd.concat([train_df,test_df])","d636340f":"df.head()","a55e6aeb":"df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())","7297ee2f":"df[\"Name\"].head(60)","af9ad8ae":"df[\"Title\"] = df[\"Name\"].str.extract(\"([a-zA-Z]*[.])\",expand = False)","3fb0b133":"df[\"Title\"]","be208600":"df.head()","eccaf2a7":"df = df.drop(\"Name\",axis=1)\ndf.columns","22f7d4c6":"df[\"Title\"] = pd.Categorical(df[\"Title\"])\ndf[\"train_set\"] = pd.Categorical(df[\"train_set\"])\ndf.dtypes","a6021d48":"sns.heatmap(df.corr(),annot = True)","8d7694bc":"sns.swarmplot(x = df[\"Survived\"],y = df[\"Age\"])","c6311e92":"sns.swarmplot(x = df[\"Survived\"],y = df[\"SibSp\"])","37ca15f4":"sns.swarmplot(x = df[\"Survived\"],y = df[\"Parch\"])","aa2949cf":"sns.swarmplot(x = df[\"Survived\"],y = df[\"Parch\"]+df[\"SibSp\"])\nplt.title(\"Survived vs number of relatives\")","a4efc115":"df[\"Relatives\"] = df[\"Parch\"] + df[\"SibSp\"]\ndf = df.drop(\"Parch\",axis=1)\ndf = df.drop(\"SibSp\",axis=1)\ndf.columns","abcb2d88":"sns.swarmplot(x = df[\"Survived\"],y = df[\"Fare\"])","c35127e6":"df = df.drop(\"Fare\",axis=1)\ndf.columns","529adb85":"def survived_vs_cat_col_plot(colname,df):\n    plt.subplot(1,2,1)\n    sns.countplot(x = df[\"Survived\"],hue = df[colname])\n    \n    plt.subplot(1,2,2)\n    sns.countplot(x = df[colname])\n    \n    plt.subplots_adjust(left=0.1,right=0.9,wspace=0.4,hspace=0.4)","344701c3":"train_df = df[df[\"train_set\"] == True]\nsurvived_vs_cat_col_plot(\"Pclass\",train_df)","d36fe766":"print(\"On ship:\")\n\nc1 = len(train_df[train_df[\"Pclass\"]==1])\nc2 = len(train_df[train_df[\"Pclass\"]==2])\nc3 = len(train_df[train_df[\"Pclass\"]==3])\nprint(\"Class 1 :\",c1)\nprint(\"Class 2 :\",c2)\nprint(\"Class 3 :\",c3)\n\nsurvived = train_df[train_df[\"Survived\"] == 1]\ns_c1 = len(survived[survived[\"Pclass\"] == 1])\ns_c2 = len(survived[survived[\"Pclass\"] == 2])\ns_c3 = len(survived[survived[\"Pclass\"] == 3])\nprint(\"\\nSurvived:\")\nprint(\"Class1 :\",s_c1)\nprint(\"Class2 :\",s_c2)\nprint(\"Class3 :\",s_c3)\n\nprint(\"\\nPercentage of people of who survived in each class:\")\nprint(\"Class1 :\",(s_c1*100) \/ c1 )\nprint(\"Class2 :\",(s_c2*100) \/ c2 )\nprint(\"Class3 :\",(s_c3*100) \/ c3 )","893eb1ad":"survived_vs_cat_col_plot(\"Sex\",train_df)","c94eb8d3":"survived_vs_cat_col_plot(\"Embarked\",train_df)","32a5dd96":"df = df.drop(\"Embarked\",axis=1)\ndf.columns","33b2867a":"train_df = df[df[\"train_set\"] == True]\ntest_df = df[df[\"train_set\"] == False]","68c6dd74":"survived = train_df[train_df[\"Survived\"]==1]\nsns.countplot(x = survived[\"Title\"])\nplt.xticks(rotation = 90)\nplt.title(\"Distribution of survived passangers according to title\")\nplt.show()","9fc4caf3":"not_survived = train_df[train_df[\"Survived\"]==0]\nsns.countplot(x = not_survived[\"Title\"])\nplt.xticks(rotation = 90)\nplt.title(\"Distribution of not survived passangers according to title\")\nplt.show()","d1db3d31":"sns.countplot(x = train_df[\"Title\"])\nplt.xticks(rotation = 90)\nplt.title(\"Distribution of passangers according to their titles\")\nplt.show()","b5fcb8dc":"df[\"Ticket\"].head(50)","56972518":"df = df.drop(\"Ticket\",axis=1)\ndf.columns","ee57b45e":"df.head()","6f8f0e26":"def category_to_number(colname,df):\n    unique_vals = list(df[colname].unique())\n    category_to_num = {}\n    for i in range(len(unique_vals)):\n        category_to_num[unique_vals[i]] = i\n        df = df.replace(to_replace = unique_vals[i],value = i)\n    return df","4d3ab472":"df = category_to_number(\"Sex\",df)\ndf.head()","8d165aa1":"df = category_to_number(\"Title\",df)\ndf.head()","c022b384":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import tree,svm\nfrom sklearn import metrics","5044c192":"train_df = df[df[\"train_set\"] == True]\ntrain_df = train_df.drop(\"train_set\",axis=1)\ntrain_df.columns","90eadcb3":"test_df = df[df[\"train_set\"] == False]\ntest_df = test_df.drop([\"train_set\",\"Survived\"],axis=1)\ntest_df.columns","4def5bc7":"X = train_df.drop(\"Survived\",axis=1)\ny = train_df[\"Survived\"]","5a965d19":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=1)","30da645b":"print(\"Length of X_train :\",len(X_train))\nprint(\"Length of X_test :\",len(X_test))\nprint(\"Length of y_train :\",len(y_train))\nprint(\"Length of y_test :\",len(y_test))","9b52bcfa":"decision_tree = tree.DecisionTreeClassifier()\ndecision_tree.fit(X_train, y_train)\ny_pred = decision_tree.predict(X_test)","4cbea463":"def display_metrics(y_test,y_pred):\n    confusion_matrix = metrics.confusion_matrix(y_test,y_pred)\n    plt.xticks([0, 1], [0, 1])\n    plt.yticks([0, 1], [0, 1])\n\n    sns.heatmap(pd.DataFrame(confusion_matrix), annot=True)\n    plt.title('Confusion Matrix')\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n    plt.show()\n\n    print('Accuracy:', round(metrics.accuracy_score(y_test, y_pred), 2))\n    print('Precision:', round(metrics.precision_score(y_test, y_pred), 2))\n    print('Recall:', round(metrics.recall_score(y_test, y_pred), 2))\n    print('F1 score:', round(metrics.f1_score(y_test, y_pred), 2))","88aec697":"display_metrics(y_test,y_pred)","edfc396e":"model = svm.SVC()\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\ndisplay_metrics(y_test,y_pred)","7c1e4193":"model = RandomForestClassifier(max_depth=3,random_state=1) # max_depth is chosen as 3, since there are 5 attributes in the dataframe\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_test)\ndisplay_metrics(y_test,y_pred)","ffe73bf3":"y_pred = model.predict(test_df)\nsurvived = pd.Series(y_pred)\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")","130c5b95":"data = {\"PassengerId\":test_df[\"PassengerId\"],\"Survived\":survived}\ntest_df = pd.concat(data,axis=1)","b49549c6":"test_df.head()","9c146023":"test_df.to_csv(\".\/submission.csv\",index=False)","525ab609":"The distribution for port of embarkation is more or less similar for survived and not survived. Thus, we don't gain any valuable insight from this column. Thus, we must drop this column.","64e0bcce":"86 age values are missing in test set. That is approximately, 20.5% of values. The missing age values for the train and test set must be imputed by the mean of the entire data.","16068a71":"The ticket entries mostly seem to be random numbers. In some entries there is a prefix that consitutes letters and special symbols. However, there is not much repetition to decipher a pattern. Thus, we must drop the ticket column.","90db9045":"# Spliting data into training and testing","b66685f8":"There are almost twice as many male passengers as female passengers, however the number of female passengers that survived is far greater. Thus, chances of survival of female passenger > chances of survival of male passenger.","6ccbaf35":"There are certain variables that are categorical but are considered numeric or object in the dataset. Thus, we must convert those variables to categorical data types. Such variables are Survived, Pclass, Sex, Embarked.","b4eb6615":"We can observe that PassangerId is a just a serial numbering of all the passangers. This is likely to have no bearing on whether a passanger survived or not. Thus, we can drop the PassangerId column.","627da801":"Since there are only two rows with value of Embarked missing, we can drop these rows.","966a0a72":"# Plotting survied vs each categorical variable","25e32ebf":"As we can see that there are more families with fewer relatives than there are with more relatives. However, there it seems that a person who is saved is more likely to belong to a smaller family. Thus, let us create a new column indicating total relatives","e6285d26":"Now that all columns have the correct data type, let us inspect for missing values. The training set has 891 rows.","eddd8f35":"177\/891 age values are missing. Approximately 19.86% of values for age are missing. We can impute these missing values. (For training set)","8931d6bf":"# Decision Tree","8f466db0":"Fare patterns are similar for both survived and not survived. Thus, fare is not a useful parameter. Hence, we must drop it.","79d3f93a":"# Plotting survived vs each numeric variable.","b96e412c":"On the ship, more than 60% of class 1 passengers survived, 47% if class 2 passangers survived and 24% of class 3 passengers survived. Thus, chances of survival of class1 passenger > chances of survival of class2 passenger > chances of survival of class3 passenger.","04949251":"The survivors seem to be younger than those that did not survive. The only exception is the one 80 (approximately) year old survivor.","3854cf57":"As we can see the passenger's name is written along with his\/her title. Maybe title had an impact in whether the passenger survived or not. Title occurs immediately after the first comma.","6d505103":"We can observe that people with certain titles are more likely to survive. For example: There are about 200 passangers with the title \"Miss.\", and more than 120 of them survived. However, there 500 people with the title \"Mr.\" and only about 80 of them survived. Using the title column, we can try to guess if a passenger survived or not.","d81ccf53":"To analyze our model we need to know the testing accuracy. Thus, let us split train_df into X_train, X_test, y_train, y_test.","d5253ccb":"Now that we have the titles we can drop the name column and we must convert Title to a categorical column.","39269c0b":"# Support Vector Machine","5c5a861b":"Since, cabin has 687\/891 rows missing, it won't help with predictions. Thus, drop the cabin column.","6ddd3a6a":"Random Forest has shown the best accuracy so far. Thus, I will choose Random Forest as my final model.","0ffaf4cd":"Accuracies achieved so far:<br>\nDecision Tree- 0.79<br>\nSupport Vector Machine- 0.63<br>\nLet us try training the data using Random Forest. I am using random forests because they are are an ensemble learning algorithm that combine the simplicity of decision trees with flexibility.","f22630c7":"There is not much correlation between any two numerical columns. Thus, I will keep all the columns in the dataset.","00853c4e":"Sex, title are categorical but have string data. For every category let us assign a unique number."}}