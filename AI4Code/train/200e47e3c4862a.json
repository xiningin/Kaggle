{"cell_type":{"cdaf70db":"code","41f8ac57":"code","5f8cfd90":"code","9392144e":"code","2cbe068b":"code","26ab2e37":"code","3d760ff4":"code","89729b60":"code","b9a60f21":"code","b93e3f67":"code","b633c0a2":"code","9a3e00c9":"code","2dbf5d79":"code","5539d211":"code","fdeb1205":"code","f8239210":"code","0f365b38":"code","a4b0747a":"code","c8bfa3f1":"markdown","9e6737e9":"markdown","1e370e8b":"markdown","353791c4":"markdown","8c0c64d3":"markdown","06954d3e":"markdown","af600039":"markdown","96e79964":"markdown","a7977d19":"markdown","78ec2efc":"markdown","2f472cdb":"markdown","5a9708f6":"markdown","ef67d260":"markdown","d043fc14":"markdown","bf5ed5f7":"markdown","b08dbc87":"markdown","f4ca1673":"markdown"},"source":{"cdaf70db":"\nimport os\nimport pandas as pd\nimport numpy as np\nfrom scipy.spatial.distance import cdist\nimport subprocess\nimport pickle\nimport ast\nimport numpy as np\nimport io\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport re\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pprint\nimport matplotlib.pyplot as plt\nimport pickle as pkl\n!pip install biobert-embedding\nimport torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom scipy.spatial import distance\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nstop_words = set(stopwords.words('english'))\ndef preprocess_sentence(text):\n    text = text.replace('\/', ' \/ ')\n    text = text.replace('.-', ' .- ')\n    text = text.replace('.', ' . ')\n    text = text.replace('\\'', ' \\' ')\n    text = text.lower()\n\n    tokens = [token for token in word_tokenize(text) if token not in punctuation and token not in stop_words]\n\n    return ' '.join(tokens)\nfrom nltk import tokenize\nfrom IPython.display import display, HTML","41f8ac57":"from biobert_embedding.embedding import BiobertEmbedding\nimport os\nimport torch\nimport logging\nimport tensorflow as tf\nfrom pathlib import Path\nfrom biobert_embedding import downloader\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM\nlogging.basicConfig(filename='app.log', filemode='w',format='%(asctime)s %(message)s', level=logging.INFO)\n\nlogger = logging.getLogger(__name__)\nclass BiobertEmbedding(object):\n    \"\"\"\n    Encoding from BioBERT model (BERT finetuned on PubMed articles).\n    Parameters\n    ----------\n    model : str, default Biobert.\n            pre-trained BERT model\n    \"\"\"\n\n    def __init__(self, model_path=None):\n\n        if model_path is not None:\n            self.model_path = model_path\n        else:\n            self.model_path = downloader.get_BioBert(\"google drive\")\n\n        self.tokens = \"\"\n        self.sentence_tokens = \"\"\n        self.tokenizer = BertTokenizer.from_pretrained(self.model_path)\n        # Load pre-trained model (weights)\n        self.model = BertModel.from_pretrained(self.model_path)\n        self.model.to(device)\n        logger.info(\"Initialization Done !!\")\n\n    def process_text(self, text):\n\n        marked_text = \"[CLS] \" + text + \" [SEP]\"\n        # Tokenize our sentence with the BERT tokenizer.\n        tokenized_text = self.tokenizer.tokenize(marked_text)\n        return tokenized_text\n\n\n    def handle_oov(self, tokenized_text, word_embeddings):\n        embeddings = []\n        tokens = []\n        oov_len = 1\n        for token,word_embedding in zip(tokenized_text, word_embeddings):\n            if token.startswith('##'):\n                token = token[2:]\n                tokens[-1] += token\n                oov_len += 1\n                embeddings[-1] += word_embedding\n            else:\n                if oov_len > 1:\n                    embeddings[-1] \/= oov_len\n                tokens.append(token)\n                embeddings.append(word_embedding)\n        return tokens,embeddings\n\n\n    def eval_fwdprop_biobert(self, tokenized_text):\n\n        # Mark each of the tokens as belonging to sentence \"1\".\n        segments_ids = [1] * len(tokenized_text)\n        # Map the token strings to their vocabulary indeces.\n        indexed_tokens = self.tokenizer.convert_tokens_to_ids(tokenized_text)\n\n        # Convert inputs to PyTorch tensors\n        tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n        segments_tensors = torch.tensor([segments_ids]).to(device)\n\n        # Put the model in \"evaluation\" mode, meaning feed-forward operation.\n        self.model.eval()\n        # Predict hidden states features for each layer\n        with torch.no_grad():\n            encoded_layers, _ = self.model(tokens_tensor, segments_tensors)\n\n        return encoded_layers\n\n\n    def word_vector(self, text, handle_oov=True, filter_extra_tokens=True):\n\n        tokenized_text = self.process_text(text)\n\n        encoded_layers = self.eval_fwdprop_biobert(tokenized_text)\n\n        # Concatenate the tensors for all layers. We use `stack` here to\n        # create a new dimension in the tensor.\n        token_embeddings = torch.stack(encoded_layers, dim=0)\n        token_embeddings = torch.squeeze(token_embeddings, dim=1)\n        # Swap dimensions 0 and 1.\n        token_embeddings = token_embeddings.permute(1,0,2)\n\n        # Stores the token vectors, with shape [22 x 768]\n        word_embeddings = []\n        logger.info(\"Summing last 4 layers for each token\")\n        # For each token in the sentence...\n        for token in token_embeddings:\n\n            # `token` is a [12 x 768] tensor\n            # Sum the vectors from the last four layers.\n            sum_vec = torch.sum(token[-4:], dim=0)\n\n            # Use `sum_vec` to represent `token`.\n            word_embeddings.append(sum_vec)\n\n        self.tokens = tokenized_text\n        if filter_extra_tokens:\n            # filter_spec_tokens: filter [CLS], [SEP] tokens.\n            word_embeddings = word_embeddings[1:-1]\n            self.tokens = tokenized_text[1:-1]\n\n        if handle_oov:\n            self.tokens, word_embeddings = self.handle_oov(self.tokens,word_embeddings)\n        logger.info(self.tokens)\n        logger.info(\"Shape of Word Embeddings = %s\",str(len(word_embeddings)))\n        return word_embeddings\n\n\n\n    def sentence_vector(self,text):\n\n        logger.info(\"Taking last layer embedding of each word.\")\n        logger.info(\"Mean of all words for sentence embedding.\")\n        tokenized_text = self.process_text(text)\n        self.sentence_tokens = tokenized_text\n        encoded_layers = self.eval_fwdprop_biobert(tokenized_text)\n\n        # `encoded_layers` has shape [12 x 1 x 22 x 768]\n        # `token_vecs` is a tensor with shape [22 x 768]\n        token_vecs = encoded_layers[11][0]\n\n        # Calculate the average of all 22 token vectors.\n        sentence_embedding = torch.mean(token_vecs, dim=0)\n        logger.info(\"Shape of Sentence Embeddings = %s\",str(len(sentence_embedding)))\n        return sentence_embedding\n","5f8cfd90":"device.type","9392144e":"pip install -U sentence-transformers","2cbe068b":"model_path = downloader.get_BioBert(\"google drive\")","26ab2e37":"text = \"Breast cancers with HER2 amplification have a higher risk of CNS metastasis and poorer prognosis.\"\\\n\n# Class Initialization (You can set default 'model_path=None' as your finetuned BERT model path while Initialization)\nbiobert = BiobertEmbedding(model_path)\n\nword_embeddings = biobert.word_vector(text)\nsentence_embedding = biobert.sentence_vector(text)\n\nprint(\"Text Tokens: \", biobert.tokens)\n# Text Tokens:  ['breast', 'cancers', 'with', 'her2', 'amplification', 'have', 'a', 'higher', 'risk', 'of', 'cns', 'metastasis', 'and', 'poorer', 'prognosis', '.']\n\nprint ('Shape of Word Embeddings: %d x %d' % (len(word_embeddings), len(word_embeddings[0])))\n# Shape of Word Embeddings: 16 x 768\n\nprint(\"Shape of Sentence Embedding = \",len(sentence_embedding))\n# Shape of Sentence Embedding =  768","3d760ff4":"# Use BERT for mapping tokens to embeddings\nfrom sentence_transformers import models\nfrom sentence_transformers import SentenceTransformer\nword_embedding_model = models.BERT('\/kaggle\/working\/'+model_path.name)\n\n# Apply mean pooling to get one fixed sized sentence vector\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n                               pooling_mode_mean_tokens=True,\n                               pooling_mode_cls_token=True,\n                               pooling_mode_max_tokens=True)\n\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])","89729b60":"from sentence_transformers import SentenceTransformer\n\nsentence_embeddings = model.encode([text])\nprint(\"Shape of Sentence Embedding = \",len(sentence_embedding))\n","b9a60f21":"Task='Task 04'\ndata=pd.read_excel(\"\/kaggle\/input\/task3covid\/task4_results_summary.xlsx\")\n\nqadata=pd.read_excel(\"\/kaggle\/input\/covid-task-qa-answer\/task4_results_summary_QA.xlsx\")\npaper_cluster_mapping=pd.read_excel(\"\/kaggle\/input\/task1-results\/archive (2)\/Final_Clusters_Keywords_UID.xlsx\")\nQuery_Mapping=pd.read_excel(\"\/kaggle\/input\/covid19-task-query\/Covid19_tasks_subtask_query.xlsx\")\nqadata=qadata.rename({'Subtask mapping':'Subtask'},axis=1)\nQuery_Mapping=Query_Mapping.rename({'Queries ':'Queries'},axis=1)\nLook_up=Query_Mapping[Query_Mapping['Task'].str.contains(Task)].merge(qadata,on=['Queries','Subtask'],how='left').reset_index(drop=True)\npaper_cluster_mapping_lookup=paper_cluster_mapping[~(paper_cluster_mapping.Cluster_Names.isna())].groupby('cord_uid')['Cluster_Names'].apply(list).reset_index(name='Clusters')\npaper_cluster_mapping_de_Dupe=paper_cluster_mapping.drop_duplicates('cord_uid')\nmetadata=pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\")\nmetadata=metadata.drop_duplicates('cord_uid')\nqadata['Context_Answer']=qadata['answer-summary(+\/-50 chars)'].apply(lambda x:x.split(\" \", 1)[1])\nqadata=qadata.drop('title',axis=1).merge(metadata,on=['cord_uid'],how='left')\nqadata.Clusters=qadata.Clusters.apply(lambda x : ast.literal_eval(x))\nqadata=qadata.rename({'answer-summary':'answer'},axis=1)","b93e3f67":"meta_df_title_abstract=data\nlen1=meta_df_title_abstract.shape[0]\nlist1=list(range(len1))\nmeta_df_title_abstract['pid']=list1\nmeta_df_title_abstract.head()\nmeta_df_title_abstract['summary_preprocessed']=meta_df_title_abstract['Text'].apply(lambda x:tokenize.sent_tokenize(x))\nnew_data_sent=meta_df_title_abstract['summary_preprocessed'].apply(pd.Series).reset_index().melt(id_vars='index').dropna()[['index', 'value']].set_index('index')\nnew_data_sent=new_data_sent.merge(meta_df_title_abstract[['cord_uid', 'lsid', 'gsid', 'Name', 'Text', 'Subtype', 'summary', 'pid']],right_index=True,left_index=True,how='left')\nnew_data_sent['wrd_cnt']=new_data_sent['value'].str.split().str.len()\nnew_data_sent_strip=new_data_sent[new_data_sent['wrd_cnt']>6]\nprint(\"wrd cnt > 6 \" + str(new_data_sent.shape))\nnew_data_sent_strip=new_data_sent_strip[new_data_sent_strip['wrd_cnt']<100]\nprint(\"wrd cnt < 200 \" + str(new_data_sent_strip.shape))\nnew_data_sent_strip['value_edit']=new_data_sent_strip['value'].apply(lambda x:preprocess_sentence(x))","b633c0a2":"def get_bert_embedding(wr):\n    #try :\n    return biobert.sentence_vector(wr).cpu()\n\ndef get_sent_bert_embedding(wr):\n    #try :\n    return model.encode([wr],show_progress_bar=False)\n\nfrom concurrent.futures import ThreadPoolExecutor\nimport time\ndef process_on_set(files, num_workers,function):\n    def process_file(i):\n        filename_2 = files[i]\n\n        y_pred = function(filename_2)\n        return y_pred\n\n    with ThreadPoolExecutor(max_workers=num_workers) as ex:\n        predictions = ex.map(process_file, range(len(files)))\n\n    return list(predictions)\n\n#xt=new_data_sent_strip['value_edit'].head().apply(lambda x:chk_len(x))","9a3e00c9":"\n%%time\nprint(device.type)\nif device.type=='cuda':\n    xt=new_data_sent_strip['value_edit'].head(1000).apply(lambda x:get_sent_bert_embedding(x))   \nelse:\n    xt = process_on_set(new_data_sent_strip['value_edit'].head(1000).values,4,get_bert_embedding)","2dbf5d79":"%%time\nprint(device.type)\nif device.type=='cuda':\n    xt=new_data_sent_strip['value_edit'].head(1000).apply(lambda x:get_bert_embedding(x))   \nelse:\n    xt = process_on_set(new_data_sent_strip['value_edit'].head(1000).values,4,get_bert_embedding)","5539d211":"%%time\nprint(device.type)\nif device.type=='cuda':\n    xt=new_data_sent_strip['value_edit'].apply(lambda x:get_sent_bert_embedding(x))   \nelse:\n    xt = process_on_set(new_data_sent_strip['value_edit'].values,4,get_bert_embedding)\nnew_data_sent_strip['Embedding']=xt\nimport pickle\nwith open('\/kaggle\/working\/embeddings37912.pickle', 'wb') as handle:\n    pickle.dump(new_data_sent_strip, handle)\nnew_data_sent_strip.to_csv(\"new_data_sent_strip.csv\",index=False)","fdeb1205":"\nquery_embedding_sent_df=new_data_sent_strip.copy()\nComp_reserch_data=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')","f8239210":"def prepare_dataset(df,query,query_id=0,val=0.9):\n    #print(\"Query is \")\n    #print(query_text[query_id])\n    query_embedding_sent_df['match']=df['Embedding'].apply(lambda x:cosine_similarity(x[0].reshape(1,x[0].shape[0]),query[query_id].reshape(1,query[query_id].shape[0]))[0][0])\n    shape_val=0\n    while shape_val<100:\n        query_embedding_sent_df_subset=query_embedding_sent_df[query_embedding_sent_df['match']>val]\n        val=val-0.02\n        shape_val=query_embedding_sent_df_subset.shape[0]\n    original_sentences=query_embedding_sent_df_subset.value.values\n    sentence_vectors=query_embedding_sent_df_subset.Embedding.values\n    cord_uid=query_embedding_sent_df_subset.cord_uid.values\n    #print('Total Sentence := ' + str (sentence_vectors.shape[0]))\n    sentence_vectors_all=[sentence_vectors[k][0] for k in range(sentence_vectors.shape[0])]\n    sim_matrix = cosine_similarity(np.array(sentence_vectors_all))\n    #print(np.mean(sim_matrix))\n    sim_matrix_thresh = np.where(sim_matrix > np.mean(sim_matrix), sim_matrix, 0)\n    return sim_matrix_thresh,original_sentences,cord_uid\n\n\ndef print_diffmethod(function,name,original_sentences,cord_uid):\n#     print(str(name))\n    page_rank_result = pd.DataFrame({'sentence_index':list(function.keys()), 'score':list(function.values()), \n                                    'original_sentence':original_sentences,'cord_uid':cord_uid})\n    page_rank_result['key']=page_rank_result.original_sentence.str.replace(\" \",\"\")\n    page_rank_result=page_rank_result.drop_duplicates(['key'])\n#     page_rank_result.nlargest(10, 'score')\n#     for s in page_rank_result.nlargest(7, 'score')['original_sentence']:\n#         pprint.pprint(s)\n#         pprint.pprint('-------------------------------------------------------------------------------------')\n#     pprint.pprint(\"-\"*40)\n    return page_rank_result[['score','original_sentence','cord_uid']]\n\n","0f365b38":"def print_summary(query_i_text='What are risk reduction strategies?'):\n    query_i_embed=model.encode([query_i_text],show_progress_bar=False)\n\n    sim_matrix_thresh,original_sentences,cord_uid=prepare_dataset(df=query_embedding_sent_df,query=query_i_embed,query_id=0,val=0.9)\n    G = nx.Graph(sim_matrix_thresh)\n    nx.pagerank(G)\n    #nx.draw_networkx(G)\n    t=print_diffmethod(nx.pagerank(G),'Pagerank',original_sentences,cord_uid)\n    #all_search=all_search.merge(paper_cluster_mapping_lookup,on='cord_uid',how='left').fillna('')\n    all_search=t.nlargest(7, 'score').merge(Comp_reserch_data[['cord_uid',  'title', 'license', 'publish_time', 'authors', 'journal']],on=['cord_uid'],how='inner')\n    all_search=all_search.merge(paper_cluster_mapping_lookup,on='cord_uid',how='left').fillna('')\n    \n    display(HTML('<font size=\"4\" color=\"blue\"> <b> Specific question subtask Query Searched : <\/b> <\/font><p> <font size=\"4\">'+query_i_text+'<\/font><p>'))\n\n    #display(HTML(all_search.to_html()))\n    display(HTML(all_search.rename({'original_sentence':'Summary'},axis=1).style.set_properties(subset=['Summary'], \\\n                                         **{'font-weight': 'bold','font-size': '12pt','text-align':\"left\",'background-color': 'lightgrey','color': 'black'}).set_table_styles(\\\n                                         [dict(selector='th', props=[('text-align', 'left'),('font-size', '12pt'),('background-color', 'pink'),('border-style','solid'),('border-width','1px')])]).hide_index().render()))\n\n    display(HTML(\"-------End-----\"*15))\ndef cluster_sentence(query_i_text='What are risk reduction strategies?'):\n    query_i_embed=model.encode([query_i_text],show_progress_bar=False)\n\n    sim_matrix_thresh,original_sentences,cord_uid=prepare_dataset(df=query_embedding_sent_df,query=query_i_embed,query_id=0,val=0.9)\n    G = nx.Graph(sim_matrix_thresh)\n    nx.pagerank(G)\n    #nx.draw_networkx(G)\n    t=print_diffmethod(nx.pagerank(G),'Pagerank',original_sentences,cord_uid)\n\n    all_search=t.nlargest(7, 'score').merge(Comp_reserch_data[['cord_uid',  'title', 'license', 'publish_time', 'authors', 'journal']],on=['cord_uid'],how='inner')\n    all_search=all_search.merge(paper_cluster_mapping_lookup,on='cord_uid',how='left').fillna('')\n    return all_search.Clusters.explode().values\ndef print_qa_bot(quer='What are risk reduction strategies?'):\n    result=qadata[qadata['Question form of queries']==quer][[ 'answer','Context_Answer','Clusters','cord_uid',  'title', 'license', 'publish_time', 'authors', 'journal']]\n    display(HTML(result.style.set_properties(subset=['answer'], \\\n                                                 **{'font-weight': 'bold','font-size': '12pt','text-align':\"left\",'background-color': 'lightgrey','color': 'black'}).set_table_styles(\\\n                                                 [dict(selector='th', props=[('text-align', 'left'),('font-size', '12pt'),('background-color', 'pink'),('border-style','solid'),('border-width','1px')])]).hide_index().render()))\n\n\ndef cluster_qa(query_i_text='What are risk reduction strategies?'):\n    result=qadata[qadata['Question form of queries']==query_i_text][[ 'answer','Context_Answer','Clusters','cord_uid',  'title', 'license', 'publish_time', 'authors', 'journal']]\n\n    return result.Clusters.explode().values\ndef print_clusters(clusters):\n    u, count = np.unique(clusters, return_counts=True)\n\n    count_sort_ind = np.argsort(-count)\n    display(HTML('<font size=\"3\" color=\"black\"> <b> Cluster Name :<\/b> <\/font>' ))\n    for x in zip(u[count_sort_ind],count[count_sort_ind]):\n        \n        if x[0] != '':\n            display(HTML(\"Cluster name is \" +str(x[0])+\" Count:- \" +str(x[1])))\n","a4b0747a":"query_embedding_sent_df=query_embedding_sent_df.merge(paper_cluster_mapping_lookup,on='cord_uid',how='left').fillna('')\nquery_embedding_sent_df\nfor Subtask in Look_up.Subtask.unique():\n    display(HTML('<font size=\"6\" color=\"black\"> <b> Subtask Searched : <\/b> <\/font><p> <font size=\"4\">'+Subtask+'<\/font><p>'))\n    Query_Mapping_Task_query=Look_up[Look_up['Subtask']==Subtask].reset_index(drop=True)\n\n    for Queries in Query_Mapping_Task_query['Queries'].unique():\n        quer=qadata[(qadata['Queries']==Queries) & (qadata['Subtask']==Subtask)]['Question form of queries'].drop_duplicates().values.tolist()\n\n        if len(quer)>0:\n            display(HTML('<font size=\"4\" color=\"blue\"> <b> Top Reserch Clusters : Query <\/b> <\/font><p> <font size=\"4\">'+quer[0]+'<\/font><p>'))\n            cluster_sen=cluster_sentence(quer[0])\n            cluster_qaq=cluster_qa(quer[0])\n            cluster=np.array(cluster_sen.tolist()+cluster_qaq.tolist())\n        else :\n            display(HTML('<font size=\"4\" color=\"blue\"> <b> Top Reserch Clusters : Query <\/b> <\/font><p> <font size=\"4\">'+Queries+'<\/font><p>'))\n            cluster_sen=cluster_sentence(Queries)\n            cluster=np.array(cluster_qaq.tolist())\n        print_clusters(cluster)\n        \n\n        \n        if len(quer)>0:\n            \n            display(HTML('<font size=\"4\" color=\"blue\"> <b> Q&A Top Reserch Clusters : Query <\/b> <\/font><p> <font size=\"4\">'+quer[0]+'<\/font><p>'))\n            print_clusters(cluster_qaq)\n            display(HTML('<font size=\"4\" color=\"blue\"> <b> Top Reserch Q&A : <\/b> <\/font><p> <font size=\"4\">'+'<\/font><p>'))\n            print_qa_bot(quer[0])\n        if len(quer)>0:\n            \n            display(HTML('<font size=\"4\" color=\"blue\"> <b> Sentence Top Reserch Clusters : Query<\/b> <\/font><p> <font size=\"4\">'+quer[0]+'<\/font><p>'))\n            print_clusters(cluster_sen)\n            display(HTML('<font size=\"4\" color=\"blue\"> <b> Top Reserch Sentence : <\/b> <\/font><p> <font size=\"4\">'+'<\/font><p>'))\n            print_summary(quer[0])\n        else :\n            \n            display(HTML('<font size=\"4\" color=\"blue\"> <b> Sentence Top Reserch Clusters : Query<\/b> <\/font><p> <font size=\"4\">'+Queries+'<\/font><p>'))\n            print_clusters(cluster_sen)\n            display(HTML('<font size=\"4\" color=\"blue\"> <b> Top Reserch Sentence : <\/b> <\/font><p> <font size=\"4\">'+'<\/font><p>'))\n            print_summary(Queries)\n\n    ","c8bfa3f1":"# Following approach is designed to generate a Generic solution for NLP reserch task \n![image.png](attachment:image.png)\n## Steps involved :- \n\n## Graph Clustering data creation\n* ### Creation of Citation network graph:- [Base Files_extraction](https:\/\/www.kaggle.com\/ashimak01\/get-file-details)\n* ### Extension of Graph network using cosine Similarity between abstract of reserch paper. ( Code will be available in GIT repo) .\n    We used [BioSentVec](https:\/\/github.com\/ncbi-nlp\/BioSentVec) for this. Due to vec size , we ran this in Colab\n    \n* ### Combine Similarity and Citation Graph [Kernal for Join](https:\/\/www.kaggle.com\/yatinece\/combine-embedding-data-and-citations-article\/data) \n* ### Running Ego split clustering to generate category\/Clusters of reserch present [Kernal Code](https:\/\/www.kaggle.com\/debasmitadas\/unsupervised-clustering-covid-research-papers)\nOnce clusters were created we give name based on top keywords of reserch parpers , topic and search models\n\n\n## Creating search to select best reserch papers to run NLP analysis on\n* ### For every task and sub-task we generated multiple queries. Task-SubTask Query Mapping [Subtask-Query-Data](https:\/\/www.kaggle.com\/yatinece\/covid19-task-query)\n\n* ### A search based system is generated to find mose relevant reserch for each Sub-Task [Link](https:\/\/www.kaggle.com\/sourojit\/cord-biobert-search\/)\n\n## Extractive summarization for reserch paper text\n* ### All search results papers text is summarized to run QA bots. Code avaliable in GitHub repo \n\n* ### Question answer bot was used to generate best answers for Each query . Code available at Github. [Data set](https:\/\/www.kaggle.com\/yatinece\/covid-task-qa-answer) \n\n## Submission notebook\n* ### Final notebook for each task created. \n\n[All submission](https:\/\/www.kaggle.com\/shashankdubeypeace\/aig-covid-nlp)\n## Git repo\n[GitHub Repo](https:\/\/github.com\/Aakash5\/cord19m)\n### Other tools\n#### [Search system based on Category of keywords mentioned in reserch paper](https:\/\/www.kaggle.com\/yatinece\/search-system-for-top-article-using-wikipedia-db)\n\n## Installing and testing sentence embedding extraction modules. Testing For GPU and setting up device. \n### Using Pytorch based Bio-Bert download via biobert-embedding\n\n### **Testing below package to generate embedding** \n[Biobert Reference](https:\/\/github.com\/Overfitter\/biobert_embedding)\n**This package main code is modified to run it on GPU**\n\n[sentence-transformers](https:\/\/github.com\/UKPLab\/sentence-transformers)\n\n\n\n#### *This will select best sentence via Network X graph. Currently using Pagerank method* Other methods can also me used like degree_centrality betweenness_centrality eigenvector_centrality","9e6737e9":"# All results for Task","1e370e8b":"# Sentence transformer","353791c4":"# Check GPU","8c0c64d3":"# Data cleaning","06954d3e":"# Examples: Below is a snapshot of some of the answers against questions related to Task 4\n\n![image.png](attachment:image.png)","af600039":"# Working using sentence transformer for speed and polling selection","96e79964":"# embedding function for Biobert on CPU and GPU","a7977d19":"# Download BioBert","78ec2efc":"# Testing Speed","2f472cdb":"# Code for Search and page rank","5a9708f6":"## Generate embedding for each line\n#### Automatic use parallel processing if GPU is unavailable","ef67d260":"# Search and results code\n** Find best sentence for query**","d043fc14":"# Testing BiobertEmbedding","bf5ed5f7":"# Read papers Selected for Task4 ","b08dbc87":"# Import package and change code to run on GPU\n\n\n","f4ca1673":"# Testing SentenceTransformer"}}