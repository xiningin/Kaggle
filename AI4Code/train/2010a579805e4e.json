{"cell_type":{"96e3c281":"code","1faa1e27":"code","ddf524e3":"code","0fc6b830":"code","2f0876d5":"code","4453ac69":"code","73b7cb2b":"code","2c0f7381":"code","62080f5d":"code","cd60a034":"markdown","12d46c71":"markdown","5622ee2d":"markdown","785717e8":"markdown"},"source":{"96e3c281":"import random\nimport torch\nimport math\nimport matplotlib.pyplot as plt","1faa1e27":"class DynamicNet(torch.nn.Module):\n    def __init__(self):\n        \"\"\"\n        In the constructor we instantiate five parameters and assign them as members.\n        \"\"\"\n        super().__init__()\n        self.a = torch.nn.Parameter(torch.randn(()))\n        self.b = torch.nn.Parameter(torch.randn(()))\n        self.c = torch.nn.Parameter(torch.randn(()))\n        self.d = torch.nn.Parameter(torch.randn(()))\n        self.e = torch.nn.Parameter(torch.randn(()))\n\n    def forward(self, x):\n        \"\"\"\n        For the forward pass of the model, we randomly choose either 4, 5\n        and reuse the e parameter to compute the contribution of these orders.\n\n        Since each forward pass builds a dynamic computation graph, we can use normal\n        Python control-flow operators like loops or conditional statements when\n        defining the forward pass of the model.\n\n        Here we also see that it is perfectly safe to reuse the same parameter many\n        times when defining a computational graph.\n        \"\"\"\n        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n        for exp in range(4, random.randint(4, 6)):\n            y = y + self.e * x ** exp\n        return y\n\n    def string(self):\n        \"\"\"\n        Just like any class in Python, you can also define custom method on PyTorch modules\n        \"\"\"\n        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'","ddf524e3":"x = torch.linspace(-math.pi, math.pi, 2000)\ny = torch.sin(x)","0fc6b830":"plt.plot(x, y)","2f0876d5":"model = DynamicNet()","4453ac69":"criterion = torch.nn.MSELoss(reduction='sum')\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)","73b7cb2b":"for t in range(30000):\n    # Forward pass: Compute predicted y by passing x to the model\n    y_pred = model(x)\n\n    # Compute and print loss\n    loss = criterion(y_pred, y)\n    if t % 2000 == 1999:\n        print(t, loss.item())\n\n    # Zero gradients, perform a backward pass, and update the weights.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\nprint(f'Result: {model.string()}')","2c0f7381":"pred_y = model(x)","62080f5d":"plt.plot(x, y, 'r', label='actuals')\nplt.plot(x, pred_y.detach().numpy(), 'b', label='predicted')","cd60a034":"# Training","12d46c71":"# Control flow and Weight Sharing","5622ee2d":"# Prediction","785717e8":"# Model"}}