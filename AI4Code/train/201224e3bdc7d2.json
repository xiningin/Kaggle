{"cell_type":{"f2b3d0bf":"code","4b199eb0":"code","a389861b":"markdown"},"source":{"f2b3d0bf":"%%writefile submission.py\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nimport numpy as np\nimport random\nimport copy\n\nframe = 0\nopposites = {Action.EAST: Action.WEST, Action.WEST: Action.EAST, Action.NORTH: Action.SOUTH, Action.SOUTH: Action.NORTH}\naction_meanings = {Action.EAST: (1, 0), Action.WEST: (-1, 0), Action.NORTH: (0, -1), Action.SOUTH: (0, 1)}\naction_names = {(1, 0): Action.EAST, (-10, 0): Action.EAST, (-1, 0): Action.WEST, (10, 0): Action.WEST, (0, -1): Action.NORTH, (0, 6): Action.NORTH, (0, -6): Action.SOUTH, (0, 1): Action.SOUTH}\nstrValue = {Action.EAST: 'EAST', Action.WEST: 'WEST', Action.NORTH: 'NORTH', Action.SOUTH: 'SOUTH'}\nall_last_actions = [None, None, None, None]\nrevert_last_actions = [None, None, None, None]\nlast_observation = None\n\nclass Obs:\n    pass\n\ndef setLastActions(observation, configuration):\n    global frame, revert_last_actions, all_last_actions\n    if not frame == 0:\n        for i in range(4):\n            setLastAction(observation, configuration, i)\n    revert_last_actions = copy.deepcopy(all_last_actions)\n\n\ndef revertLastActions():\n    global revert_last_actions, all_last_actions\n    all_last_actions = copy.deepcopy(revert_last_actions)\n\n\ndef setLastAction(observation, configuration, gooseIndex):\n    global last_observation, all_last_actions, action_names\n    if len(observation.geese[gooseIndex]) > 0:\n        oldGooseRow, oldGooseCol = row_col(last_observation.geese[gooseIndex][0], configuration.columns)\n        newGooseRow, newGooseCol = row_col(observation.geese[gooseIndex][0], configuration.columns)\n        all_last_actions[gooseIndex] = action_names[\n            ((newGooseCol - oldGooseCol) % configuration.columns, (newGooseRow - oldGooseRow) % configuration.rows)]\n\n\ndef getValidDirections(observation, configuration, gooseIndex):\n    global all_last_actions, opposites\n    directions = [Action.EAST, Action.WEST, Action.NORTH, Action.SOUTH]\n    returnDirections = []\n    for direction in directions:\n        row, col = getRowColForAction(observation, configuration, gooseIndex, direction)\n        if not willGooseBeThere(observation, configuration, row, col) and not all_last_actions[gooseIndex] == opposites[\n            direction]:\n            returnDirections.append(direction)        \n    if len(returnDirections) == 0:\n        return directions\n    return returnDirections\n\n\ndef randomTurn(observation, configuration, actionOverrides, rewards, fr):\n    newObservation = cloneObservation(observation)\n    for i in range(4):\n        if len(observation.geese[i]) > 0:\n            if i in actionOverrides.keys():\n                newObservation = performActionForGoose(observation, configuration, i, newObservation, actionOverrides[i])\n            else:\n                newObservation = randomActionForGoose(observation, configuration, i, newObservation)\n    checkForCollisions(newObservation, configuration)\n    updateRewards(newObservation, configuration, rewards, fr)\n    hunger(newObservation, fr)\n    return newObservation\n\n\ndef hunger(observation, fr):\n    if fr % 40 == 0:\n        for g, goose in enumerate(observation.geese):\n            goose = goose[0:len(goose)-1]\n            \n\ndef updateRewards(observation, configuration, rewards, fr):\n    for g, goose in enumerate(observation.geese):\n        if len(goose) > 0:\n            rewards[g] = 2 * fr + len(goose)\n\ndef checkForCollisions(observation, configuration):\n    killed = []\n    for g, goose in enumerate(observation.geese):\n        if len(goose) > 0:\n            for o, otherGoose in enumerate(observation.geese):\n                for p, part in enumerate(otherGoose):\n                    if not (o == g and p == 0):\n                        if goose[0] == part:\n                            killed.append(g)\n    for kill in killed:\n        observation.geese[kill] = []\n\n\ndef cloneObservation(observation):\n    newObservation = Obs()\n    newObservation.index = observation.index\n    newObservation.geese = copy.deepcopy(observation.geese)\n    newObservation.food = copy.deepcopy(observation.food)\n    return newObservation\n\n\ndef randomActionForGoose(observation, configuration, gooseIndex, newObservation):\n    validActions = getValidDirections(observation, configuration, gooseIndex)\n    action = random.choice(validActions)\n    row, col = getRowColForAction(observation, configuration, gooseIndex, action)\n    newObservation.geese[gooseIndex] = [row * configuration.columns + col] + newObservation.geese[gooseIndex]\n    if not isFoodThere(observation, configuration, row, col):\n        newObservation.geese[gooseIndex] = newObservation.geese[gooseIndex][0:len(newObservation.geese[gooseIndex])-1]  \n    return newObservation\n\n\ndef performActionForGoose(observation, configuration, gooseIndex, newObservation, action):\n    row, col = getRowColForAction(observation, configuration, gooseIndex, action)\n    newObservation.geese[gooseIndex][:0] = [row * configuration.columns + col]\n    if not isFoodThere(observation, configuration, row, col):\n        newObservation.geese[gooseIndex] = newObservation.geese[gooseIndex][0:len(newObservation.geese[gooseIndex])-1]  \n    return newObservation\n        \n\ndef isFoodThere(observation, configuration, row, col):\n    found = None\n    for f, food in enumerate(observation.food):\n        foodRow, foodCol = row_col(food, configuration.columns)\n        if foodRow == row and foodCol == col:\n            found = f\n    if found is not None:\n        observation.food[found] = random.random() * configuration.columns * configuration.rows\n        return True\n    return False\n\n\ndef willGooseBeThere(observation, configuration, row, col):\n    for goose in observation.geese:\n        for p, part in enumerate(goose):\n            if not p == len(goose) - 1:\n                partRow, partCol = row_col(part, configuration.columns)\n                if partRow == row and partCol == col:\n                    return True\n    return False\n\n\ndef getRowColForAction(observation, configuration, gooseIndex, action):\n    global action_meanings\n    gooseRow, gooseCol = row_col(observation.geese[gooseIndex][0], configuration.columns)\n    actionRow = (gooseRow + action_meanings[action][1]) % configuration.rows\n    actionCol = (gooseCol + action_meanings[action][0]) % configuration.columns\n    return actionRow, actionCol\n\n\ndef simulateMatch(observation, configuration, firstMove, depth):\n    global frame\n    actionOverrides = {observation.index: firstMove}\n    revertLastActions()\n    simulationFrame = frame + 1\n    newObservation = cloneObservation(observation)\n    rewards = [0, 0, 0, 0]\n    count = 0\n    while count < depth:\n        newObservation = randomTurn(newObservation, configuration, actionOverrides, rewards, simulationFrame)\n        actionOverrides = {}\n        simulationFrame += 1\n        count += 1\n    return rewards\n\n\ndef simulateMatches(observation, configuration, numMatches, depth):\n    options = getValidDirections(observation, configuration, observation.index)\n    rewardTotals = []\n    for o, option in enumerate(options):\n        rewardsForOption = [0, 0, 0, 0]\n        for i in range(numMatches):\n            matchRewards = simulateMatch(observation, configuration, option, depth)\n            for j in range(4):\n                rewardsForOption[j] += matchRewards[j]\n        rewardTotals.append(rewardsForOption)\n    scores = []\n    for o, option in enumerate(options):\n        rewards = rewardTotals[o]\n        if len(rewards) <= 0:\n            mean = 0\n        else:\n            mean = sum(rewards) \/ len(rewards)\n        if mean == 0:\n            scores.append(0)\n        else:\n            scores.append(rewards[observation.index] \/ mean)\n    \n    print('frame: ', frame)\n    print('options: ', options)\n    print('scores: ', scores)\n    print('reward totals: ', rewardTotals)\n    print('lengths: ')\n    print('0: ', len(observation.geese[0]))\n    print('1: ', len(observation.geese[1]))\n    print('2: ', len(observation.geese[2]))\n    print('3: ', len(observation.geese[3]))\n\n    return options[scores.index(max(scores))]\n\n\ndef agent(obs_dict, config_dict):\n    global last_observation, all_last_actions, opposites, frame\n    observation = Observation(obs_dict)\n    configuration = Configuration(config_dict)\n    setLastActions(observation, configuration)\n    myLength = len(observation.geese[observation.index])\n    if myLength < 5:\n        my_action = simulateMatches(observation, configuration, 300, 3)\n    elif myLength < 9:\n        my_action = simulateMatches(observation ,configuration, 120, 6)\n    else:\n        my_action = simulateMatches(observation, configuration, 85, 9)\n    \n    last_observation = cloneObservation(observation)\n    frame += 1\n    return strValue[my_action]","4b199eb0":"from kaggle_environments import *\n\nenv = make(\"hungry_geese\", configuration={\"columns\": 11, \"rows\": 7,  \"hunger_rate\": 40, \"min_food\": 2}, debug=False)\noutput = env.run(['submission.py', 'submission.py', 'submission.py', 'submission.py'])\nenv.render(mode='ipython', width=800, height=700)","a389861b":"# **Monte Carlo**\n\n\nThis notebook implements a bare-bones, simple Monte Carlo simulation.  Monte Carlo algorithms use random simulation to evaluate a problem, so in this case we simulate the game progression with random, not-obviously-lethal moves by all geese and compare results for each starting move.  Being a bare-bones implementation, this could serve as a starting point or as part of a larger approach.\n\nA challenge I observed is motivating young geese to eat early in the match, because if the look-ahead depth is too far, they will be too afraid of death to focus on finding food.  To balance that with the need to look ahead into complex scenarios later in the game, I use these look-ahead depths and simulation counts for a strong result without timing out:\n\n* **If length < 5:** 300 simulations with depth of 3\n* **If length < 8:** 120 simulations with depth of 6\n* **If length >= 8:** 85 simulations with depth of 9\n\n**Changelog:**\n- **2\/21\/2021 (v44)**: Added food spawning to simluations.\n"}}