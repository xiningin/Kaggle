{"cell_type":{"aea83b69":"code","ee144dff":"code","eb022d68":"code","b3925c70":"code","d50f5261":"code","5b1db6d6":"code","3725d412":"code","a6d64d99":"code","5a94d1f3":"code","cdcce002":"code","348af249":"code","d92c84ea":"code","91bc8b79":"code","3adf0d88":"code","f65237e2":"markdown","04d37c39":"markdown","781cb90e":"markdown","24faee05":"markdown","84cbc7fc":"markdown","623f4caa":"markdown","c20ca4c5":"markdown","799f535f":"markdown","c1c8742c":"markdown","367367a2":"markdown","f75001e3":"markdown","94d0e782":"markdown","1280738e":"markdown","51324387":"markdown","3132acbc":"markdown","871655f9":"markdown"},"source":{"aea83b69":"import torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())","ee144dff":"!python -m pip install 'git+https:\/\/github.com\/facebookresearch\/detectron2.git'","eb022d68":"import pandas as pd\nimport numpy as np\nimport pandas as pd \nfrom tqdm import tqdm\nfrom tqdm import tqdm_notebook as tqdm # progress bar\nfrom datetime import datetime\nimport time\nimport matplotlib.pyplot as plt\nfrom pycocotools.coco import COCO\nimport os, json, cv2, random\nimport skimage.io as io\nimport copy\nfrom pathlib import Path\nfrom typing import Optional\n\n\n\nfrom tqdm import tqdm\nimport itertools\n\nimport torch\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nfrom glob import glob\nimport numba\nfrom numba import jit\n\nimport warnings\nwarnings.filterwarnings('ignore') #Ignore \"future\" warnings and Data-Frame-Slicing warnings.\n\n\n# detectron2\nfrom detectron2.structures import BoxMode\nfrom detectron2 import model_zoo\nfrom detectron2.config import get_cfg\nfrom detectron2.data import DatasetCatalog, MetadataCatalog\nfrom detectron2.engine import DefaultPredictor, DefaultTrainer, launch\nfrom detectron2.evaluation import COCOEvaluator\nfrom detectron2.structures import BoxMode\nfrom detectron2.utils.visualizer import ColorMode\nfrom detectron2.utils.logger import setup_logger\nfrom detectron2.utils.visualizer import Visualizer\n\nfrom detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\nfrom detectron2.data import detection_utils as utils\n\n\nfrom detectron2.data import DatasetCatalog, MetadataCatalog, build_detection_test_loader, build_detection_train_loader\nfrom detectron2.data import detection_utils as utils\nimport detectron2.data.transforms as T\nfrom detectron2.evaluation import COCOEvaluator, inference_on_dataset\n\nsetup_logger()","b3925c70":"Data_Resister_training=\"sartorius_Cell_train\";\nData_Resister_valid=\"sartorius_Cell_valid\";\nfrom detectron2.data.datasets import register_coco_instances\ndataDir=Path('..\/input\/sartorius-cell-instance-segmentation\/')\n\nregister_coco_instances(Data_Resister_training,{}, '..\/input\/crossvalidationfold5\/coco_cell_train_fold5.json', dataDir)\nregister_coco_instances(Data_Resister_valid,{},'..\/input\/crossvalidationfold5\/coco_cell_valid_fold5.json', dataDir)\n\nmetadata = MetadataCatalog.get(Data_Resister_training)\ndataset_train = DatasetCatalog.get(Data_Resister_training)\ndataset_valid = DatasetCatalog.get(Data_Resister_valid)","d50f5261":"fig, ax = plt.subplots(figsize =(18,11))\nd=dataset_valid[2] \nimg = cv2.imread(d[\"file_name\"])\nprint(img.shape)\nv = Visualizer(img[:, :, ::-1],\n                metadata=metadata, \n                scale=1,\n                instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n    )\nout = v.draw_dataset_dict(d)\nax.grid(False)\nax.axis('off')\nax.imshow(out.get_image()[:, :, ::-1])","5b1db6d6":"def custom_mapper(dataset_dict):\n    dataset_dict = copy.deepcopy(dataset_dict)\n    image = utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")\n    transform_list = [\n            T.RandomBrightness(0.9, 1.1),\n            T.RandomContrast(0.9, 1.1),\n            T.RandomSaturation(0.9, 1.1),\n            T.RandomLighting(0.9),\n            T.RandomFlip(prob=0.5, horizontal=False, vertical=True),\n            T.RandomFlip(prob=0.5, horizontal=True, vertical=False),\n    ]\n    image, transforms = T.apply_transform_gens(transform_list, image)\n    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\n\n    annos = [\n        utils.transform_instance_annotations(obj, transforms, image.shape[:2])\n        for obj in dataset_dict.pop(\"annotations\")\n        if obj.get(\"iscrowd\", 0) == 0\n    ]\n    instances = utils.annotations_to_instances(annos, image.shape[:2])\n    dataset_dict[\"instances\"] = utils.filter_empty_instances(instances)\n    return dataset_dict\nclass AugTrainer(DefaultTrainer):\n    @classmethod\n    def build_train_loader(cls, cfg):\n        return build_detection_train_loader(cfg, mapper=custom_mapper)\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        return MAPIOUEvaluator(dataset_name)","3725d412":"# Taken from https:\/\/www.kaggle.com\/theoviel\/competition-metric-map-iou\nfrom detectron2.evaluation.evaluator import DatasetEvaluator\nimport pycocotools.mask as mask_util\ndef precision_at(threshold, iou):\n    matches = iou > threshold\n    true_positives = np.sum(matches, axis=1) == 1  # Correct objects\n    false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n    false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n    return np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n\ndef score(pred, targ):\n    pred_masks = pred['instances'].pred_masks.cpu().numpy()\n    enc_preds = [mask_util.encode(np.asarray(p, order='F')) for p in pred_masks]\n    enc_targs = list(map(lambda x:x['segmentation'], targ))\n    ious = mask_util.iou(enc_preds, enc_targs, [0]*len(enc_targs))\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, ious)\n        p = tp \/ (tp + fp + fn)\n        prec.append(p)\n    return np.mean(prec)\n\nclass MAPIOUEvaluator(DatasetEvaluator):\n    def __init__(self, dataset_name):\n        dataset_dicts = DatasetCatalog.get(dataset_name)\n        self.annotations_cache = {item['image_id']:item['annotations'] for item in dataset_dicts}\n            \n    def reset(self):\n        self.scores = []\n\n    def process(self, inputs, outputs):\n        for inp, out in zip(inputs, outputs):\n            if len(out['instances']) == 0:\n                self.scores.append(0)    \n            else:\n                targ = self.annotations_cache[inp['image_id']]\n                self.scores.append(score(out, targ))\n\n    def evaluate(self):\n        return {\"MaP IoU\": np.mean(self.scores)}\n\nclass Trainer(DefaultTrainer):\n    @classmethod\n    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n        return MAPIOUEvaluator(dataset_name)\n    \n","a6d64d99":"cfg = get_cfg()\nconfig_name = \"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\" \ncfg.merge_from_file(model_zoo.get_config_file(config_name))\ncfg.DATASETS.TRAIN = (Data_Resister_training,)\ncfg.DATASETS.TEST = (Data_Resister_valid,)\n\ncfg.MODEL.WEIGHTS =\"..\/input\/detectron2cell\/output\/model_final.pth\"\n\n#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(config_name)\n\ncfg.DATALOADER.NUM_WORKERS = 2\ncfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  # 64 is slower but more accurate (128 faster but less accurate)\ncfg.MODEL.ROI_HEADS.NUM_CLASSES = 3 \ncfg.SOLVER.IMS_PER_BATCH = 2 #(2 is per defaults)\ncfg.INPUT.MASK_FORMAT='bitmask'\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n\ncfg.SOLVER.BASE_LR = 0.0005 #(quite high base learning rate but should drop)\n#cfg.SOLVER.MOMENTUM = 0.9\n#cfg.SOLVER.WEIGHT_DECAY = 0.0005\n#cfg.SOLVER.GAMMA = 0.1\n\n    \ncfg.SOLVER.WARMUP_ITERS = 10 #How many iterations to go from 0 to reach base LR\ncfg.SOLVER.MAX_ITER = 2000 #Maximum of iterations 1\ncfg.SOLVER.STEPS = (500, 1000) #At which point to change the LR 0.25,0.5\ncfg.TEST.EVAL_PERIOD = 250\ncfg.SOLVER.CHECKPOINT_PERIOD=250\n\nos.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n#trainer = AugTrainer(cfg) # with  data augmentation  \ntrainer = Trainer(cfg)  # without data augmentation\ntrainer.resume_or_load(resume=False)\ntrainer.train()","5a94d1f3":"evaluator = COCOEvaluator(Data_Resister_valid, cfg, False, output_dir=\".\/output\/\")\ncfg.MODEL.WEIGHTS=\"..\/input\/detectron2cell\/output\/model_final.pth\"\n#cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2   # set a custom testing threshold\n#cfg.INPUT.MASK_FORMAT='polygon'\nval_loader = build_detection_test_loader(cfg, Data_Resister_valid)\ninference_on_dataset(trainer.model, val_loader, evaluator)","cdcce002":"import pandas as pd\nmetrics_df = pd.read_json(\".\/output\/metrics.json\", orient=\"records\", lines=True)\nmdf = metrics_df.sort_values(\"iteration\")\n","348af249":"# 1. Loss curve\nfig, ax = plt.subplots()\n\nmdf1 = mdf[~mdf[\"total_loss\"].isna()]\nax.plot(mdf1[\"iteration\"], mdf1[\"total_loss\"], c=\"C0\", label=\"train\")\nif \"validation_loss\" in mdf.columns:\n    mdf2 = mdf[~mdf[\"validation_loss\"].isna()]\n    ax.plot(mdf2[\"iteration\"], mdf2[\"validation_loss\"], c=\"C1\", label=\"validation\")\n\n# ax.set_ylim([0, 0.5])\nax.legend()\nax.set_title(\"Loss curve\")\nplt.show()","d92c84ea":"# 1. Accuracy curve\nfig, ax = plt.subplots()\n\nmdf1 = mdf[~mdf[\"fast_rcnn\/cls_accuracy\"].isna()]\nax.plot(mdf1[\"iteration\"], mdf1[\"fast_rcnn\/cls_accuracy\"], c=\"C0\", label=\"train\")\n# ax.set_ylim([0, 0.5])\nax.legend()\nax.set_title(\"Accuracy curve\")\nplt.show()","91bc8b79":"cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n#cfg.MODEL.WEIGHTS = \".\/output\/model_final.pth\"\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5   # set a custom testing threshold for this model\ncfg.DATASETS.TEST = (Data_Resister_valid, )\npredictor = DefaultPredictor(cfg)","3adf0d88":"fig, ax = plt.subplots(4, 1, figsize =(20,50))\nindices=[ax[0],ax[1],ax[2],ax[3] ]\ni=-1\nfor d in random.sample(dataset_valid, 4):\n    i=i+1    \n    im = cv2.imread(d[\"file_name\"])\n    outputs = predictor(im)\n    v = Visualizer(im[:, :, ::-1],\n                   metadata=metadata, \n                   scale=1, \n                   instance_mode=ColorMode.IMAGE_BW   # remove the colors of unsegmented pixels. This option is only available for segmentation models\n    )\n    out = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n    indices[i].grid(False)\n    indices[i].imshow(out.get_image()[:, :, ::-1])","f65237e2":"# Data Visualization\n* It's also very easy to visualize prepared training dataset with detectron2.\n* It provides Visualizer class, we can use it to draw an image with mask and bounding box as following.","04d37c39":"# Training","781cb90e":"# Evaluator","24faee05":"![download.jpg](attachment:c2c63055-26f5-4e32-a3c0-c0949c8f0214.jpg)","84cbc7fc":"## Other notebooks in this competition \n- [Sartorius Segmentation - Keras U-Net[Training]](https:\/\/www.kaggle.com\/ammarnassanalhajali\/sartorius-segmentation-keras-u-net-training)\n- [Sartorius Segmentation - Keras U-Net[Inference]](https:\/\/www.kaggle.com\/ammarnassanalhajali\/sartorius-segmentation-keras-u-net-inference\/edit)","623f4caa":"# Sartorius Segmentation - Detectron2 [Training]","c20ca4c5":"# Data Augmentation\nThe dataset is transformed by changing the brighness and flipping the image with 50% probability...etc","799f535f":"## Install Detectron2\n","c1c8742c":"# Predictor","367367a2":"### Hi kagglers, This is `Training` notebook using `Detectron2`.\n[Sartorius Segmentation - Detectron2 [Inference]](https:\/\/www.kaggle.com\/ammarnassanalhajali\/sartorius-segmentation-detectron2-inference) \n\n### Please if this kernel is useful, <font color='red'>please upvote !!<\/font>","f75001e3":"# Accuracy curve","94d0e782":"# importing libraries\n","1280738e":"# Detectron2\nDetectron2 is Facebook AI Research's next generation software system that implements state-of-the-art object detection algorithms. It is a ground-up rewrite of the previous version, Detectron, and it originates from maskrcnn-benchmark","51324387":"# Loading Dataset","3132acbc":"# Evaluator\n* Famouns dataset's evaluator is already implemented in detectron2.\n* For example, many kinds of AP (Average Precision) are calculted in COCOEvaluator.\n* **COCOEvaluator calculates AP with IoU from 0.50 to 0.95**","871655f9":"# Loss curve"}}