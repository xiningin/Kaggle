{"cell_type":{"07a1c456":"code","6f420a94":"code","592a98b3":"code","0c38dc5a":"code","4c92ac44":"code","9e3b3478":"code","c7522e00":"code","482bae48":"code","9602e7e9":"code","ec8e8131":"code","ce1e56b2":"code","9e41a406":"code","d66db2ce":"code","2526e03a":"code","fc4b6c0d":"code","b161a54d":"code","85f768fc":"code","308eb50b":"markdown","e99748f0":"markdown","3662faa9":"markdown","b636ea15":"markdown","a845997b":"markdown","b6e6f988":"markdown","67f3dc8a":"markdown","f67c9832":"markdown","524ff36d":"markdown","557e2da2":"markdown"},"source":{"07a1c456":"from pathlib import Path\n\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","6f420a94":"def load_data(root_path, stock_ids=None):\n    \"\"\"Loads all parquet files from given root path. Will only load stock_ids in stock_ids list\n    if provided\"\"\"\n    # Go through all folders and append parquet data to data\n    data = []\n    for folder in root_path.iterdir():\n        assert len([child for child in folder.iterdir()]) == 1\n        # folders take format stock_id=X\n        stock_id = int(folder.name.split(\"=\")[1])\n        for parquet_file in folder.iterdir():\n            if stock_ids is None or stock_id in stock_ids:\n                df = pd.read_parquet(parquet_file)\n                df[\"stock_id\"] = stock_id\n                data.append(df)\n    return pd.concat(data)","592a98b3":"# Define the root paths for book and trade training data\nroot_path_trade = Path(\"\/kaggle\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\")\nroot_path_book = Path(\"\/kaggle\/input\/optiver-realized-volatility-prediction\/book_train.parquet\")","0c38dc5a":"# First load the full trade dataset into memory\ntrade_df = load_data(root_path_trade)","4c92ac44":"print(trade_df.shape)\ntrade_df.dtypes","9e3b3478":"trade_df.describe()","c7522e00":"from random import sample\n\n# Get some randomly sampled time_ids and stock_ids\ntime_id_sample = sample(list(trade_df[\"time_id\"].unique()), 3)\nstock_id_sample = sample(list(trade_df[\"stock_id\"].unique()), 5)\n\nmask = (trade_df[\"time_id\"].isin(time_id_sample)) & (trade_df[\"stock_id\"].isin(stock_id_sample))\ntrade_df_sample = trade_df[mask]\n\n# Create relplot on the subset of data\nsns.relplot(x=\"seconds_in_bucket\", y=\"price\", hue=\"order_count\", size=\"size\",\n            col=\"time_id\", row=\"stock_id\", sizes=(40,400), data=trade_df_sample)","482bae48":"# Group on stock_id and get the mean and standard deviation of price, order_count and size\ngrouped_trade = trade_df.groupby([\"stock_id\"]).agg({\"price\": [\"mean\", \"std\", \"count\"],  # Count here gives the number of trades\n                                                    \"order_count\": [\"mean\", \"std\", \"sum\"],\n                                                    \"size\": [\"mean\", \"std\", \"sum\"]}).reset_index()","9602e7e9":"grouped_trade.hist(figsize=(15, 15));","ec8e8131":"grouped_trade.describe()","ce1e56b2":"# All the book data won't fit in Kaggle notebook memory so will pick a random stock id\nall_stock_ids = [int(path.name.split(\"=\")[1]) for path in root_path_book.iterdir()]\nstock_id_sample = sample(all_stock_ids, 5)\nprint(f\"Loading data for stock(s):{stock_id_sample}\")\nbook_df = load_data(root_path_book, stock_ids=stock_id_sample)","9e41a406":"# WAP based on the most competitive bid \/ ask prices\nbook_df['wap'] = (book_df['bid_price1'] * book_df['ask_size1'] + book_df['ask_price1'] * book_df['bid_size1']) \/ \\\n                 (book_df['bid_size1']+ book_df['ask_size1'])","d66db2ce":"# Compute the log return for each unique stock_id and time_id pairing\n# NOTE: this takes some time to run\nupdated_data = []\nstock_time_pairs = book_df.groupby([\"time_id\", \"stock_id\"]).size().reset_index().sort_values([\"stock_id\", \"time_id\"])\nfor i, row in stock_time_pairs.iterrows():\n    if i % 2000 == 0:\n        print(f\"Completed {i} of {stock_time_pairs.shape[0]} rows.\")\n    mask = (book_df[\"stock_id\"] == row[\"stock_id\"]) & (book_df[\"time_id\"] == row[\"time_id\"])\n    subset = book_df[mask].copy(deep=True)\n    subset['log_return'] = np.log(subset['wap']).diff()\n    updated_data.append(subset)\n    \nbook_df = pd.concat(updated_data)","2526e03a":"print(book_df.shape)\nbook_df.head()","fc4b6c0d":"# Get some randomly sampled time_ids and stock_ids for plots\ntime_id_sample = sample(list(book_df[\"time_id\"].unique()), 3)\nstock_id_sample = sample(list(book_df[\"stock_id\"].unique()), 5)","b161a54d":"mask = (book_df[\"time_id\"].isin(time_id_sample)) & (book_df[\"stock_id\"].isin(stock_id_sample))\nbook_df_sample = book_df.loc[mask, [\"stock_id\", \"time_id\", \"bid_price1\", \"ask_price1\", \"wap\", \"seconds_in_bucket\", \"log_return\"]]\nbook_df_sample_melted = pd.melt(book_df_sample, id_vars=[\"stock_id\", \"time_id\", \"seconds_in_bucket\"], \n                         value_vars=[\"bid_price1\", \"ask_price1\", \"wap\"])\n\n# Create relplot on the subset of data\nsns.relplot(x=\"seconds_in_bucket\", y=\"value\", hue=\"variable\", \n            col=\"time_id\", row=\"stock_id\",data=book_df_sample_melted, kind=\"line\")","85f768fc":"book_df_sample_melted = pd.melt(book_df_sample, id_vars=[\"stock_id\", \"time_id\", \"seconds_in_bucket\"], \n                         value_vars=[\"log_return\"])\n\n# Create relplot on the subset of data\nsns.relplot(x=\"seconds_in_bucket\", y=\"value\", hue=\"variable\", \n            col=\"time_id\", row=\"stock_id\",data=book_df_sample_melted, kind=\"line\")","308eb50b":"### 2.1 Visualising WAP and bid\/ask prices\n\nWe'll plot some line plots of bid, ask and WAP for another random sample of `stock_id` and `time_id` values.","e99748f0":"# [EDA] Visualising trading data - Optiver\n\nThis notebook contains visualisations of the trade and book datasets provided for the Optiver Realized Volatility Prediction challenge.\n\nWe visualise individual trading sessions for randomly selected stocks, to start to get a feeling for the data and how trading sessions typically behave.\n\nThis is an active notebook and will continually be added to as we progress through the competition.\n","3662faa9":"## 1. Trade data\n\nLet's investigate the trade dataset.\n\nAs a reminder the following are the definitions of the columns:\n\n* `stock_id` - ID code for the stock. Not all stock IDs exist in every time bucket - this is because this dataset shows trades that have taken place in a 10 minute period - less liquid stocks may not be traded in a given 10 min period.\n* `time_id` - ID code for the time bucket. Time IDs are not necessarily sequential but are consistent across all stocks.\n* `seconds_in_bucket` - Number of seconds from the start of the bucket, always starting from 0. Note that since trade and book data are taken from the same time window and trade data is more sparse in general, this field is not necessarily starting from 0.\n* `price` - The average price of executed transactions happening in one second. Prices have been normalized and the average has been weighted by the number of shares traded in each transaction.\n* `size` - The sum number of shares traded.\n* `order_count` - The number of unique trade orders taking place.\n\nThe data is split into buckets as identified by the `time_id` column. Each time bucket represents 10 minutes of trading and the `seconds_in_bucket` column indicates how far through the time bucket each trading event takes place.","b636ea15":"Rerunning the cell above a few times starts to paint a picture of typical trading conditions.\n\n**Some observations**:\n* Usually trading conditions are relatively benign and trading is within a ~0.1% range\n* Sometimes prices trend up\/down but usually just trade in a fairly tight range\n* Very occasionally there are sizeable moves (~2%)\n* Benign trading conditions can have few or many orders taking place - no obvious relationship from eyeballing the data\n* Different stocks trade differently - some have many more trades in a 10 min window than others, order counts and sizes also vary.\n* Usually there are lots of trades in the 10 min period - but for some stocks this is can be low (10s)\n\n**Some questions this brings up that can be explored in further EDA**:\n* Do large price changes occure when order counts \/ size are low?\n* Are sudden price changes triggered by large orders?\n\n### 1.2 Variability in trading conditions across the different stocks \n\nWe will now group the trade data by `stock_id` to explore variability between stocks a bit further","a845997b":"**Observations**:\n\n* 2 order of magnitude range in total number of trades between stocks\n* Some stocks have much greater variability in price than others (~5x bottom to top)\n* Order counts are typically around 3-4 for most stocks on average and not too much variation between stocks.\n* Some stocks have much greater variability in the order count than others\n* A few outlier stocks in terms of order size (maybe low price stocks so size higher for given \\$ value order)","b6e6f988":"We'll compute the weighted average price and the log return. Since the log return formula uses diff we should apply it to each stock \/ time_id individually.","67f3dc8a":"### 1.1 Visualising trading sessions\n\nLet's visualise some of the trading data for a few different stocks and sessions. We will randomly pick `stock_id` and `time_id` values so we can run the cell multiple times to start to get a sense of the dataset.\n\nWe will use relplots from seaborn, where the size of the bubbles indicates the number of shares traded and the colour of the bubble indicates the number of orders traded at that time period.","f67c9832":"**Observations**:\n\n* Some stocks have wide bid ask spreads relative to others which tends to lead to higher log returns and therefore realised volatility\n* time_id seems to affect general trading conditions for all stocks - some time_ids have more liquid trading conditions and some less liquid.\n\n### Visualising log returns\n\nUsing the same sample of `stock_id` and `time_id` we can plot log returns too","524ff36d":"## 2. Order data\n\nLet's look at the order data now. As a reminder the following columns are defined:\n\n* `stock_id` - ID code for the stock. Not all stock IDs exist in every time bucket. Parquet coerces this column to the categorical data type when loaded; you may wish to convert it to int8.\n* `time_id` - ID code for the time bucket. Time IDs are not necessarily sequential but are consistent across all stocks.\n* `seconds_in_bucket` - Number of seconds from the start of the bucket, always starting from 0.\n* `bid_price[1\/2]` - Normalized prices of the most\/second most competitive buy level.\n* `ask_price[1\/2]` - Normalized prices of the most\/second most competitive sell level.\n* `bid_size[1\/2]` - The number of shares on the most\/second most competitive buy level.\n* `ask_size[1\/2]` - The number of shares on the most\/second most competitive sell level.","557e2da2":"### 1.3 Questions for further EDA I'll answer in updates to this notebook\n\n* Which stocks are most traded, which stocks least traded?\n* Which stocks have the largest price fluctuations?\n* Are there periods where all stocks are volatile?\n* Are there periods where individual stocks are particularly volatile?"}}