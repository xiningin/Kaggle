{"cell_type":{"58f84799":"code","760ea3df":"code","42aaf3cd":"code","9fe2e9f8":"code","f5f94ca6":"code","de730407":"code","dc7afeb5":"code","f6b02432":"code","5cab632b":"code","236bc8a6":"code","908b8869":"code","9e803aee":"code","e144e362":"code","3ed712b3":"code","6727be8d":"code","6d607c3c":"code","563acff0":"code","97ca5090":"code","d9ec8f61":"code","c9c26ade":"code","85cdecb8":"code","0693cab3":"code","b1cdc9ae":"code","936a85a7":"code","152621c0":"markdown","db7913d5":"markdown","c79ac64f":"markdown","8258c9c1":"markdown","3c8909e1":"markdown","3c984994":"markdown","34cd6fa6":"markdown","80f1d28a":"markdown","dd1f52d9":"markdown","23b82a4c":"markdown","8ee32103":"markdown","87317bde":"markdown","7a39470e":"markdown","53abf5ee":"markdown","1e65aa70":"markdown","e4afe68a":"markdown"},"source":{"58f84799":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfile_list = []\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/plantpathology-apple-dataset'):\n    for filename in filenames:\n        file_list.append(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","760ea3df":"!pip install -U torch==1.5 torchvision==0.6 -f https:\/\/download.pytorch.org\/whl\/cu101\/torch_stable.html \n!pip install cython pyyaml==5.1\n!pip install -U 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'\nimport torch, torchvision\nprint(torch.__version__, torch.cuda.is_available())\n!gcc --version","42aaf3cd":"# install detectron2:\n!pip install detectron2==0.1.3 -f https:\/\/dl.fbaipublicfiles.com\/detectron2\/wheels\/cu101\/index.html","9fe2e9f8":"!pip list","f5f94ca6":"import sys\n#sys.path.append('\/content\/detectron2_repo')\nimport os\nimport numpy as np\n\nimport math  \nimport detectron2 \nfrom detectron2.engine import DefaultTrainer\nfrom detectron2.config import get_cfg\nfrom detectron2 import model_zoo\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog\nfrom PIL import Image\nimport cv2\n","de730407":"\nimport requests\n\ndef download_file_from_google_drive(id, destination):\n    URL = \"https:\/\/docs.google.com\/uc?export=download\"\n\n    session = requests.Session()\n\n    response = session.get(URL, params = { 'id' : id }, stream = True)\n    token = get_confirm_token(response)\n\n    if token:\n        params = { 'id' : id, 'confirm' : token }\n        response = session.get(URL, params = params, stream = True)\n\n    save_response_content(response, destination)    \n\ndef get_confirm_token(response):\n    for key, value in response.cookies.items():\n        if key.startswith('download_warning'):\n            return value\n\n    return None\n\ndef save_response_content(response, destination):\n    CHUNK_SIZE = 32768\n\n    with open(destination, \"wb\") as f:\n        for chunk in response.iter_content(CHUNK_SIZE):\n            if chunk: # filter out keep-alive new chunks\n                f.write(chunk)","dc7afeb5":"## download the pretrained weights for leaf segmentation.\n\nfile_id = '17AHanttKcR9B4A0m7QZqwAvaWxGrYYQp'\ndestination = '.\/model.pth'\ndownload_file_from_google_drive(file_id, destination)","f6b02432":"#get the predictor\ndef get_predictor():\n    cfg = get_cfg()\n    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation\/mask_rcnn_R_50_FPN_3x.yaml\"))\n    cfg.DATASETS.TRAIN = ()\n    cfg.DATALOADER.NUM_WORKERS = 16\n\n    cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = (8)  # faster, and good enough for this toy dataset\n    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 2  # 3 classes (data, fig, hazelnut)\n\n    os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n    \n    cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"\/kaggle\/working\/model.pth\")\n    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.7   # set the testing threshold for this model\n    predictor = DefaultPredictor(cfg)\n    return predictor","5cab632b":"#function to get the leaf image.\n\ndef get_cropped_leaf(img,predictor,return_mapping=False,resize=None):\n    #convert to numpy    \n    img = np.array(img)[:,:,::-1]\n    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n    \n    #get prediction\n    outputs = predictor(img)\n    \n    #get boxes and masks\n    ins = outputs[\"instances\"]\n    pred_masks = ins.get_fields()[\"pred_masks\"]\n    boxes = ins.get_fields()[\"pred_boxes\"]    \n    \n    #get main leaf mask if the area is >= the mean area of boxes and is closes to the centre \n    \n    masker = pred_masks[np.argmin([calculateDistance(x[0], x[1], int(img.shape[1]\/2), int(img.shape[0]\/2)) for i,x in enumerate(boxes.get_centers()) if (boxes[i].area()>=torch.mean(boxes.area()).to(\"cpu\")).item()])].to(\"cpu\").numpy().astype(np.uint8)\n\n    #mask image\n    mask_out = cv2.bitwise_and(img, img, mask=masker)\n    \n    #find contours and boxes\n    contours, hierarchy = cv2.findContours(masker.copy() ,cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    contour = contours[np.argmax([cv2.contourArea(x) for x in contours])]\n    rotrect = cv2.minAreaRect(contour)\n    box = cv2.boxPoints(rotrect)\n    box = np.int0(box)\n    \n\n    #crop image\n    cropped = get_cropped(rotrect,box,mask_out)\n\n    #resize\n    rotated = MakeLandscape()(Image.fromarray(cropped))\n    \n    if not resize == None:\n        resized = ResizeMe((resize[0],resize[1]))(rotated)\n    else:\n        resized = rotated\n        \n    if return_mapping:\n        img = cv2.drawContours(img, [box], 0, (0,0,255), 10)\n        img = cv2.drawContours(img, contours, -1, (255,150,), 10)\n        return resized, ResizeMe((int(resize[0]),int(resize[1])))(Image.fromarray(img))\n    \n    return resized\n\n#function to crop the image to boxand rotate\n\ndef get_cropped(rotrect,box,image):\n    \n    width = int(rotrect[1][0])\n    height = int(rotrect[1][1])\n\n    src_pts = box.astype(\"float32\")\n    # corrdinate of the points in box points after the rectangle has been\n    # straightened\n    dst_pts = np.array([[0, height-1],\n                        [0, 0],\n                        [width-1, 0],\n                        [width-1, height-1]], dtype=\"float32\")\n\n    # the perspective transformation matrix\n    M = cv2.getPerspectiveTransform(src_pts, dst_pts)\n\n    # directly warp the rotated rectangle to get the straightened rectangle\n    warped = cv2.warpPerspective(image, M, (width, height))\n    return warped\n\ndef calculateDistance(x1,y1,x2,y2):  \n    dist = math.hypot(x2 - x1, y2 - y1)\n    return dist  \n","236bc8a6":"#image manipulations \n\nclass ResizeMe(object):\n    #resize and center image in desired size \n    def __init__(self,desired_size):\n        \n        self.desired_size = desired_size\n        \n    def __call__(self,img):\n    \n        img = np.array(img).astype(np.uint8)\n        \n        desired_ratio = self.desired_size[1] \/ self.desired_size[0]\n        actual_ratio = img.shape[0] \/ img.shape[1]\n\n        desired_ratio1 = self.desired_size[0] \/ self.desired_size[1]\n        actual_ratio1 = img.shape[1] \/ img.shape[0]\n\n        if desired_ratio < actual_ratio:\n            img = cv2.resize(img,(int(self.desired_size[1]*actual_ratio1),self.desired_size[1]),None,interpolation=cv2.INTER_AREA)\n        elif desired_ratio > actual_ratio:\n            img = cv2.resize(img,(self.desired_size[0],int(self.desired_size[0]*actual_ratio)),None,interpolation=cv2.INTER_AREA)\n        else:\n            img = cv2.resize(img,(self.desired_size[0], self.desired_size[1]),None, interpolation=cv2.INTER_AREA)\n            \n        h, w, _ = img.shape\n\n        new_img = np.zeros((self.desired_size[1],self.desired_size[0],3))\n        \n        hh, ww, _ = new_img.shape\n\n        yoff = int((hh-h)\/2)\n        xoff = int((ww-w)\/2)\n        \n        new_img[yoff:yoff+h, xoff:xoff+w,:] = img\n\n        \n        return Image.fromarray(new_img.astype(np.uint8))\n\nclass MakeLandscape():\n    #flip if needed\n    def __init__(self):\n        pass\n    def __call__(self,img):\n        \n        if img.height > img.width:\n            img = np.rot90(np.array(img))\n            img = Image.fromarray(img)\n        return img\n","908b8869":"len(file_list)","9e803aee":"predictor = get_predictor()","e144e362":"img, img1 = get_cropped_leaf(Image.open(\"\/kaggle\/input\/plantpathology-apple-dataset\/images\/Train_128.jpg\"),predictor,return_mapping=True,resize = (800,int(800)))","3ed712b3":"img","6727be8d":"if img1.height > img1.width:\n    img1 = np.rot90(np.array(img1))\n    img1 = Image.fromarray(img1)\nimg1","6d607c3c":"import matplotlib.pyplot as plt\nkek = np.array(img)\nkek_mask = np.array(img1)\nplt.imshow(kek)\nplt.show()\nplt.imshow(kek_mask)\nplt.show()\n\nprint(kek.dtype)\nprint(kek_mask.shape)","563acff0":"final_image = []\n\nfor x in range(75):\n    #select random image\n    file_loc = file_list[np.random.randint(0,len(file_list))]\n    #get outputs from predictor\n    img, img1 = get_cropped_leaf(Image.open(file_loc),predictor,return_mapping=True,resize = (600,int(600*.65)))\n    #stack horizontally\n    stacked = np.hstack([img,img1])\n    #append images\n    final_image.append(stacked)\n","97ca5090":"import matplotlib.pyplot as plt\n%matplotlib inline","d9ec8f61":"for x in final_image:\n    fig = plt.figure(figsize=(20,10))\n    plt.imshow(x)","c9c26ade":"os.makedirs('images')","85cdecb8":"!rmdir 'output'","0693cab3":"path = 'images\/'\n\nfrom tqdm import tqdm\n\nfor i in range(1,len(file_list)):\n    img,img1 = get_cropped_leaf(Image.open(file_list[i]),predictor,return_mapping=True,resize = (800,int(800)))\n    kek = os.path.split(file_list[i])[1]\n    img.save(path+kek) \n    ","b1cdc9ae":"im = Image.open('images\/train_add_127.jpg')\nim","936a85a7":"print(os.path.split(file_list[1])[1])\n","152621c0":"### I hope this has been some help to anyone who reads it. ","db7913d5":"### Basic recipe for inferance ","c79ac64f":"# Imports","8258c9c1":"### First we will list out all the files in the directory for later use.","3c8909e1":"## Detectron2 uses pytorch and makes it amazingly easy to retrain its pretrained model on a custom dataset. I won't go into the method of doing this but there is a wealth of information online. \n\n## The python tool \"labelme\" is great for drawing the masks over your images.\n\n![labelme](https:\/\/i.imgur.com\/phNlpnX.jpg \"Label Me\")\n\n\n","3c984994":"### Import the required libraries.","34cd6fa6":"# Inference Method","80f1d28a":"### Display of multiple images stacked","dd1f52d9":"# How to!","23b82a4c":"# Display","8ee32103":"### Get Images","87317bde":"### Below code is to download the pretrained weights from my google drive.","7a39470e":"# Image manipulations","53abf5ee":"# Model Collection","1e65aa70":"### Load the predictor ","e4afe68a":"# Image segmentation using Detectron2\n\n## Using Detectron2 and mask R-CNN it is possible to isolate each leaf from the input image and extract the most prominent one for later analysis.\n\n### Steps involved:\n\n* Download and install detectron2 and other dependancies\n\n* Hand annotate images with the objects mask (not covered in this kernel) - google \"labelme\" and \"training detectron2 on custom dataset\" \n\n* Train detectron2 (not covered in this kernel) \n\n* Load pretrained weights into detectron2\n\n* Infer image masks and identifiy prominent objects\n\n* Mask and extract from original image\n\n* Crop and rotate object to fit\n\n![Masked1](https:\/\/i.imgur.com\/tnC1ljY.jpg \"Masked Leaf 1\")\n\n![Masked](https:\/\/i.imgur.com\/nemK62H.jpg \"Masked Leaf\")\n\n\n### To Do:\n\n* Futher training of the mask R-CNN model to improve segmentation.\n\n\n"}}