{"cell_type":{"0040bc67":"code","2aa0f3f9":"code","319f3fe4":"code","45de3955":"code","e8420b2c":"code","b3959c00":"code","386550a5":"code","fa06ad25":"code","a4e2ceef":"code","f9196843":"code","3195317f":"code","ff80a7c8":"code","2cf6ae0e":"code","486afe1d":"code","f6c94622":"code","3279e653":"code","4f0bbcdb":"code","389037cc":"code","029ca550":"code","7f352e16":"code","63a51ff3":"code","36d12e2e":"code","d7341b50":"code","a25b7b76":"code","1a76aafc":"code","b7dcaa91":"code","0ce1d024":"code","653468f0":"code","a5d33968":"code","2bd74053":"code","53ebff4b":"code","4a60355c":"code","235522d7":"code","3453b512":"code","2a27daf3":"code","34842823":"code","23f3fca9":"code","99f10eac":"code","73dbd66d":"code","6617da87":"code","5828a078":"code","b927cddb":"code","97241071":"code","87e7b3c2":"code","243a5776":"code","c244be21":"code","9a61e20e":"code","9b50023d":"code","f0dcbba9":"code","c31022b2":"code","49a53d2f":"code","3c1f002c":"code","229a327b":"code","818a9885":"code","27a9abb8":"code","0da566eb":"code","e64e62aa":"code","fd050852":"code","9f1655a9":"code","1ff412b6":"code","b4a3d908":"code","5ea709c1":"code","ef7c3297":"code","e55d1457":"code","1ca27240":"code","55ec35bd":"code","181e4bbf":"code","cde50268":"code","fe3634e8":"code","13818f68":"code","e4bfb707":"code","b9cd3850":"code","167cc87a":"code","7ff91d4e":"code","f6aa6e04":"code","75a17631":"code","be7c2a8a":"code","d57c4d87":"code","837f201c":"code","d7ff3c4e":"code","99152883":"code","133b9013":"markdown","b9d096dc":"markdown","2a7942b9":"markdown","b73b4ba5":"markdown","eb904655":"markdown","1796068f":"markdown","368dd4d7":"markdown","7cdb6b9f":"markdown","fa14c043":"markdown","5420641f":"markdown","4477c110":"markdown","c4d398f7":"markdown","fa9b1c03":"markdown","5ff04bbd":"markdown","368094c8":"markdown","bf3ea616":"markdown","3f88352a":"markdown","f5ff541c":"markdown","e1d6c082":"markdown","aa1787bf":"markdown","b6018e42":"markdown","fe52c090":"markdown","57fd8afe":"markdown","162a6a4f":"markdown","f3738270":"markdown","26b928f8":"markdown","38e93645":"markdown","39913851":"markdown","f550d577":"markdown","c31d8bc7":"markdown","6e2eda61":"markdown","ea26e146":"markdown","dca08479":"markdown","e3bcdb92":"markdown","6fd7de88":"markdown","06115c7f":"markdown","51fdf9b2":"markdown","6e314a30":"markdown","87df1550":"markdown","81ef091e":"markdown","368b2aa8":"markdown","a237cfd1":"markdown","3edbe589":"markdown","d24d26f3":"markdown","d5f304fd":"markdown","8f445c61":"markdown","ef37b421":"markdown","cc0e6da0":"markdown"},"source":{"0040bc67":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","2aa0f3f9":"# Import the necessary packages and libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.max_columns', 50)\n\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nplt.style.use('ggplot')","319f3fe4":"# Read the data\n\ninsurance = pd.read_csv('..\/input\/insurance-premium-prediction\/insurance.csv')\ninsurance.head()","45de3955":"# Checking the shape of the dataframe\n\ninsurance.shape","e8420b2c":"# Checking the info for all the columns\n\ninsurance.info()","b3959c00":"insurance.nunique().sort_values()","386550a5":"# Pairplot of all the numeric variables\n\nsns.pairplot(insurance, vars=['age','bmi','expenses'])\nplt.show()","fa06ad25":"#Boxplot for some of the categorical variables with respect to the target varibale 'expenses'\n\nplt.figure(figsize=(20, 12))\nplt.subplot(2,4,1)\nsns.boxplot(x = 'sex', y = 'expenses', data = insurance)\nplt.subplot(2,4,2)\nsns.boxplot(x = 'children', y = 'expenses', data = insurance)\nplt.subplot(2,4,3)\nsns.boxplot(x = 'smoker', y = 'expenses', data = insurance)\nplt.subplot(2,4,4)\nsns.boxplot(x = 'region', y = 'expenses', data = insurance)\n\nplt.show()","a4e2ceef":"# Analysis between sex and expenses\n\nplt.figure(figsize=(10,4))\nsns.barplot('sex','expenses',data=insurance)\nplt.title('Expenses among the genders',fontsize=12)\nplt.show()","f9196843":"# Analysis between children and expenses\n\nplt.figure(figsize=(10,4))\nsns.barplot('children','expenses',data=insurance)\nplt.title('Expenses with respect to the number of children',fontsize=12)\nplt.show()","3195317f":"# Analysis between smoker and expenses\n\nplt.figure(figsize=(10,4))\nsns.barplot('smoker','expenses',data=insurance)\nplt.title('Expenses with respect to smoker',fontsize=12)\nplt.show()","ff80a7c8":"# Analysis between region and expenses\n\nplt.figure(figsize=(10,4))\nsns.barplot('region','expenses',data=insurance)\nplt.title('Expenses with respect to region',fontsize=12)\nplt.show()","2cf6ae0e":"# Analysis of expenses with age\n\nsns.scatterplot(x='age',y='expenses' ,data=insurance)\nplt.title('Expenses vs Age')\nplt.show()","486afe1d":"# Analysis of expenses with bmi\n\nsns.scatterplot(x='bmi',y='expenses' ,data=insurance)\nplt.title('BMI vs Expenses')\nplt.show()","f6c94622":"# Heatmap to visualise the correlation between the variables\n\nplt.figure(figsize=(10, 5))\nsns.heatmap(insurance.corr(), cmap=\"YlGnBu\", annot = True)\nplt.title(\"Correlation between the variables\")\nplt.show()","3279e653":"# Mapping the variable 'children' for better analysis\n\ninsurance['children'] = insurance.children.map({0:'No Children',1:'One Child',2:'Two Children',3:'Three Children',4:'Four Children',5:'Five Children'})\ninsurance.head()","4f0bbcdb":"children_dummy = pd.get_dummies(insurance.children,drop_first=True)\nregion_dummy = pd.get_dummies(insurance.region,drop_first=True)\nsex_dummy = pd.get_dummies(insurance.sex,drop_first=True)\nsmoker_dummy = pd.get_dummies(insurance.smoker,prefix='smoker',drop_first=True)","389037cc":"# Adding the dummy variables to the original dataframe\n\ninsurance = pd.concat([insurance,children_dummy,region_dummy,sex_dummy,smoker_dummy],axis=1)\ninsurance.head()","029ca550":"# Dropping the original columns - children, region, sex and smoker, since dummy variables have already been created for them\n\ninsurance.drop(['children','region','sex','smoker'], axis = 1, inplace = True)\ninsurance.shape","7f352e16":"np.random.seed(0)\n\ninsurance_train, insurance_test = train_test_split(insurance, train_size = 0.7, random_state = 100)\n\nprint(insurance_train.shape)\nprint(insurance_test.shape)","63a51ff3":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","36d12e2e":"# Creating a list of numeric variables\n\nnum_vars=['age','bmi','expenses']","d7341b50":"# Fit the data\n\ninsurance_train[num_vars] = scaler.fit_transform(insurance_train[num_vars])\ninsurance_train.head()","a25b7b76":"# Checking numeric variables after scaling the features\ninsurance_train.describe()","1a76aafc":"y_train = insurance_train.pop('expenses')\nX_train = insurance_train","b7dcaa91":"col = X_train.columns\ncol","0ce1d024":"# Creating the linear model\n\nlm = LinearRegression()\nlm.fit(X_train, y_train)","653468f0":"# Adding a constant variable \n  \nX_train_1 = X_train[col]","a5d33968":"X_train_1 = sm.add_constant(X_train_1)","2bd74053":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_1).fit()","53ebff4b":"# Summary of our linear model\nprint(lm.summary())","4a60355c":"# Dropping the const variable\n\nX_train_1_ = X_train_1.drop(['const'], axis=1)","235522d7":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_1_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","3453b512":"# Rebuilding the model without 'male'\n\nX_train_2 = X_train_1_.drop(['male'], axis=1)","2a27daf3":"# Adding the contsant variable\n\nX_train_2 = sm.add_constant(X_train_2)","34842823":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_2).fit() ","23f3fca9":"# Summary of the new model\nprint(lm.summary())","99f10eac":"# Dropping the const variable\n\nX_train_2_ = X_train_2.drop(['const'], axis=1)","73dbd66d":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_2_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","6617da87":"# Rebuilding the model without 'Two Children'\n\nX_train_3 = X_train_2_.drop(['Two Children'], axis=1)","5828a078":"# Adding the contsant variable\n\nX_train_3 = sm.add_constant(X_train_3)","b927cddb":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_3).fit()","97241071":"# Summary of the new model\nprint(lm.summary())","87e7b3c2":"# Dropping the const variable\n\nX_train_3_ = X_train_3.drop(['const'], axis=1)","243a5776":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_3_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","c244be21":"# Rebuilding the model without 'Four Children'\n\nX_train_4 = X_train_3_.drop(['Four Children'], axis=1)","9a61e20e":"# Adding the contsant variable\n\nX_train_4 = sm.add_constant(X_train_4)","9b50023d":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_4).fit()","f0dcbba9":"# Summary of the new model\nprint(lm.summary())","c31022b2":"# Dropping the const variable\n\nX_train_4_ = X_train_4.drop(['const'], axis=1)","49a53d2f":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_4_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","3c1f002c":"# Rebuilding the model without 'Three Children'\n\nX_train_5 = X_train_4_.drop(['Three Children'], axis=1)","229a327b":"# Adding the contsant variable\n\nX_train_5 = sm.add_constant(X_train_5)","818a9885":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_5).fit()","27a9abb8":"# Summary of the new model\nprint(lm.summary())","0da566eb":"# Dropping the const variable\n\nX_train_5_ = X_train_5.drop(['const'], axis=1)","e64e62aa":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_5_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","fd050852":"# Rebuilding the model without 'northwest'\n\nX_train_6 = X_train_5_.drop(['northwest'], axis=1)","9f1655a9":"# Adding the contsant variable\n\nX_train_6 = sm.add_constant(X_train_6)","1ff412b6":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_6).fit()","b4a3d908":"# Summary of the new model\nprint(lm.summary())","5ea709c1":"# Dropping the const variable\n\nX_train_6_ = X_train_6.drop(['const'], axis=1)\n","ef7c3297":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_6_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e55d1457":"# Rebuilding the model without 'southeast'\n\nX_train_7 = X_train_6_.drop(['southeast'], axis=1)","1ca27240":"# Adding the contsant variable\n\nX_train_7 = sm.add_constant(X_train_7)","55ec35bd":"# Running the linear model\n\nlm = sm.OLS(y_train,X_train_7).fit()","181e4bbf":"# Summary of the new model\nprint(lm.summary())","cde50268":"# Dropping the const variable\n\nX_train_7_ = X_train_7.drop(['const'], axis=1)","fe3634e8":"# Calculating the VIFs for the new model\n\nvif = pd.DataFrame()\nX = X_train_7_\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","13818f68":"lm = sm.OLS(y_train,X_train_7).fit()  #As obtained previously\ny_train_count = lm.predict(X_train_7)","e4bfb707":"# Plot the histogram of the error terms\n\nfig = plt.figure()\nsns.distplot((y_train - y_train_count), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label\nplt.show()","b9cd3850":"num_vars=['age','bmi','expenses']\n\n# Fit and transform operations are done on the training data but only transform operation will be done on the test data\n\ninsurance_test[num_vars] = scaler.transform(insurance_test[num_vars])\ninsurance_test.describe()","167cc87a":"y_test = insurance_test.pop('expenses')\nX_test = insurance_test","7ff91d4e":"# Creating X_test_m7 dataframe with the final Model 7 in the training dataset\n\nX_test_m7 = sm.add_constant(X_test)","f6aa6e04":"X_test_m7.columns","75a17631":"X_test_m7 = X_test_m7.drop(['Four Children','Three Children','Two Children','northwest','southeast','male'],axis=1)","be7c2a8a":"# Making predictions using the seventh model\n\ny_pred_m7 = lm.predict(X_test_m7)","d57c4d87":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred_m7)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16)      \nplt.show()","837f201c":"# Regression plot\n\nsns.regplot(x = y_test, y = y_pred_m7, fit_reg=True,scatter_kws={\"color\": \"blue\"}, line_kws={\"color\": \"red\"})\n\nplt.title('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)                          # Y-label\nplt.show()","d7ff3c4e":"# Evaluate R-square for test dataset\n\nfrom sklearn.metrics import r2_score\nr2_score(y_test,y_pred_m7)","99152883":"# Adjusted R^2\n# adj r2 = 1-((1-R2)*(n-1)\/(n-p-1))\n\n# n = sample size (in this case the value is 220, as yielded before)\n# p = number of independent variables(in this case the value is 9)\n\nAdj_r2 = 1 - ((1 - 0.7733792659357421) * 401 \/ (402-6-1))\nprint(Adj_r2)","133b9013":"So there are 1338 records and 7 features.","b9d096dc":"### Model 1","2a7942b9":"Categorical Variables - sex, smoker, region, children\n\nThe rest of the variables are continuous in nature.","b73b4ba5":"Observation:-\n'age' is most correlated with the target variable 'expenses'.","eb904655":"It is observed that the error terms are more-or-less normally distributed.","1796068f":"Creating dummy variables for - children, region, sex and smoker","368dd4d7":"In Model 6, 'southeast' is has the highest p-value.","7cdb6b9f":"### Model 4","fa14c043":"Observation:-\nExpenses for the men are comparatively more.   ","5420641f":"### Dividing into X_test and y_test","4477c110":"### Model 3","c4d398f7":"In Model 3, 'Four Children' is insignificant. Hence, this should be dropped now.","fa9b1c03":"### Calculation of R-square and Adjusted R-square values","5ff04bbd":"### Correlation between the variables","368094c8":"### Dividing into X and y sets for the model building","bf3ea616":"Observations:-\n\n1) Expenses are more for a male individual.\n2) Expenses are more for those having 2 or 3 children.\n3) Expenses are more for the smokers.\n4) Expnenses are more those individuals who belong to the south-east region, and least who belong to the south-west region.","3f88352a":"737For the training dataset, the R^2 value was 0.738 and adjusted R^2 value was 0.737.\n\nFor the testing dataset, the R^2 value obtained is 0.773 and adjusted R^2 value obtained is 0.769.\n\n\n\n\nHence the equation of our best fitted line is:-\n\n$ expenses = 0.1912 \\times age + 0.1644 \\times bmi - 0.0214 \\times No   Children -0.0278 \\times One   Child - 0.0160 \\times southwest + 0.3817 \\times smoker_   yes $\n\n\nOverall we have a decent model, but we also acknowledge that we could do better. ","f5ff541c":"In Model 2, 'Two Children' is the m ost insignificant variable, and hence this should be dropped now.","e1d6c082":"### Reading and preparing the data","aa1787bf":"### Splitting the Data into Training and Testing Sets","b6018e42":"Observation:-\nNo such clear trend.","fe52c090":"### Model 7","57fd8afe":"### Visualising the numeric variables","162a6a4f":"### Analysis between the target variable - expenses, and the other variables","f3738270":"In Model 1, 'male' has got high p-value but low VIF, but 'bmi' has got low p-value but high VIF.\n\nSince, 'male' is insignificant with respect to the other variables, hence this is dropped, and not 'bmi'.","26b928f8":"Observation:-\nExpenses are highest for those individuals who belong to the south-east region.","38e93645":"### Identifying the categorical and continuous variables","39913851":"### Building our model\n\nWe will be using the statsmodel to build our model, initially with all the features, and keep on removing them manually, based on p-values and VIF.","f550d577":"### Interpretations\n\n* We have arrived at a very decent model for the the demand for shared bikes with the significant variables.\n\n* We can see that smoking_yes variable is having the highest coefficient of 0.3817, which means if the smoking_yes increases by one unit, the expense increases by 0.3817 units.\n\n* The other significant variables having positive coefficients are age and bmi.\n\n* There are some variables with negative coefficients too, like No Children, One Child and southwest. A negative coefficient suggests that, as the independent variable increases, the dependent variable tends to decrease, and vice-versa.","c31d8bc7":"We will be following the below rule to drop the variables one by one, as per the priorities mentioned by their sequences:-\n\n* We will first check the summary and VIF\n* If a variable has got high p-value(>0.05) as well as high VIF(>5), we need to drop that first\n* If a variable has got high p-value(>0.05) but low VIF(<5), then we need to drop such\n* Still if we have a variable with low p-value(<0.05) but high VIF(>5), we need to drop such at the very end","6e2eda61":"### Model 2","ea26e146":"Observation:-\nIndividuals, with 2 or 3 children, are having the most expenses, while those with 5 children, are having the least expenses.","dca08479":"### Residual Analysis of the train data\n\nSo, now to check if the error terms are also normally distributed (which is infact, one of the major assumptions of linear regression), let us plot the histogram of the error terms and see what it looks like.","e3bcdb92":"Observation:-\nMuch higher expenses are observed for those, whose bmi is between 30 and 40.","6fd7de88":"Comments:-\nIt is observed from the above, that the entire dataset does not contain any missing values.","06115c7f":"### Rescaling the Features using MinMax Scaling","51fdf9b2":"In Model 4, 'Three Children' has got low VIF but high p-value, hence this variable should be dropped now.","6e314a30":"### Model Evaluation","87df1550":"### Model 6","81ef091e":"Observation:-\n\nExpenses for the smokers are much higher than the non-smokers.","368b2aa8":"### Creating Dummy Variables","a237cfd1":"No such clear trend of correlation.","3edbe589":"### Supress Warnings and import all the relevant packages and libraries","d24d26f3":"### Applying the scaling on the test sets","d5f304fd":"Model 7 seems to be the best model achieved so far, with the p-values of all the 6 dependent variables are less than 0.05, and the VIF values are less than 4.\nHence, this model can be finalised, for making the predictions.","8f445c61":"### Model 5","ef37b421":"In Model 5, 'northwest' is the least significant, so dropping that.","cc0e6da0":"### Visualising the categorical variables"}}