{"cell_type":{"b19dc87e":"code","d1b31e02":"code","55d8f884":"code","8e9d6af8":"code","eff74d3c":"code","283e0fac":"code","f53a7262":"code","3615dec8":"code","6c8ea49a":"code","26f15591":"code","069b7d3e":"code","ae1c1b7e":"code","1862267d":"code","b80b1872":"code","c5a984fd":"code","3c83069e":"code","1605efb9":"code","16b02751":"code","96b483ab":"code","ea9a44e6":"code","4a3efc09":"code","e25f2892":"code","d8be2d32":"code","f6678004":"code","336fdf9a":"code","a79e76b4":"code","68d2c4bd":"code","a94bc8f1":"code","d00a3400":"code","5030a76a":"code","b578eeb8":"code","a236938d":"code","ed3de830":"code","160608f6":"code","fd041588":"code","a7a9a7da":"code","1e42e0bc":"code","2095e6f2":"code","f2392471":"code","c254fd85":"code","9ce264ed":"code","7d0c7e44":"code","ff4f1caf":"code","28c3dae7":"code","60050cea":"code","635d0c9c":"code","88e1f82c":"code","8b50d296":"code","74bb86a0":"code","cfd4e611":"code","b47bed64":"code","0112d743":"code","353d38df":"code","dae2f3f7":"code","9a8ea781":"code","4dcf9e1f":"code","5041974b":"code","7da67e0d":"code","2eb04fc7":"code","332b57f5":"code","9ca48d00":"code","e9624ba4":"code","5c0416c5":"code","347ab8c4":"code","edac6790":"code","2d4e48e1":"code","c9f96e2d":"code","7863c46f":"code","fab0e3e0":"code","ef6e153f":"code","25d08517":"code","7724a2e8":"code","8a2569b7":"code","c97e4dbd":"code","077da1c0":"markdown","37d868fc":"markdown","9ee35212":"markdown","bb4a7262":"markdown","6b7cd175":"markdown","5b85595b":"markdown","0df8f0d1":"markdown","00b9859d":"markdown","963cfa54":"markdown","d1698189":"markdown","d7ffb5f5":"markdown","f6713044":"markdown","63e28bed":"markdown","ab5d9e5f":"markdown"},"source":{"b19dc87e":"random_state = 0\nforce_retune = False\nuse_augmentation = True\n\nimport re # regex\nfrom tqdm import tqdm\nimport pickle as pkl\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.utils.testing import ignore_warnings\nfrom sklearn.exceptions import ConvergenceWarning\n\n# a bunch of scalers to try\n# from: https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, QuantileTransformer, Normalizer\n\n# a bunch of classical ML regressors to try\n# list from: https:\/\/www.educative.io\/blog\/scikit-learn-cheat-sheet-classification-regression-methods\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMRegressor\nfrom xgboost.sklearn import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\n\nfrom string import punctuation\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models, callbacks\nimport keras_tuner as kt\nfrom tensorflow.keras.backend import stop_gradient\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport glob\nprint(glob.glob('\/kaggle\/working\/*'))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d1b31e02":"data = pd.read_csv('\/kaggle\/input\/commonlitreadabilityprize\/train.csv')\ndata.head()","55d8f884":"data.info()","8e9d6af8":"print(data.dtypes)","eff74d3c":"# drop url_legal, unique -- they are not in the test dataset, and do not look useful anyways\ndata.drop(['url_legal', 'license'], axis=1, inplace=True)\n\nnoTrain_columns = ['target','standard_error','excerpt','id']","283e0fac":"# data preprocessing\ndef split_data (df, target='target', test_size=0.2, pca=False, indices=None, augmented=False):\n    if pca:\n        pca = PCA()\n        pca_cols = pca.fit_transform(\n            data[data.columns.difference(noTrain_columns)])\n        X = pd.DataFrame(data=pca_cols,\n            columns=['PCA%i' % i for i in range(pca_cols.shape[1])])\n    else:\n        X = df[df.columns.difference(noTrain_columns)]\n        \n    if augmented:\n        y = df[[target,'standard_error']].to_numpy()\n    else:\n        y = df[target]\n    if indices == None:\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n    else:\n        train_indices, test_indices = indices\n        X_train = X.iloc[train_indices]\n        X_test = X.iloc[test_indices]\n        if augmented:\n            y_train = y[train_indices]\n            y_test = y[test_indices]\n        else:\n            y_train = y.iloc[train_indices]\n            y_test = y.iloc[test_indices]\n    return X_train, X_test, y_train, y_test, pca","f53a7262":"# Baseline model\ndef try_SGDR (data, pca=False, indices=None, output=False):\n    X_train, X_test, y_train, y_test, pca = split_data(data, pca=pca, indices=indices)\n    reg = make_pipeline(StandardScaler(),\n                        SGDRegressor(random_state=random_state))\n    reg.fit(X_train, y_train)\n    print(\"RMSE: \", mean_squared_error(y_test, reg.predict(X_test), squared=False))\n    if output:\n        return reg","3615dec8":"# Generate the most basic features\ndata['no_lineBreaks'] = data['excerpt'].str.split('\\n').transform(len)\ndata['no_sentences'] = data['excerpt'].str.split('.').transform(len)\ndata['no_words'] = data['excerpt'].str.split(' ').transform(len)\ndata['no_characters'] = data['excerpt'].apply(len)\ndata.head()","6c8ea49a":"sns.lmplot(data=data, x='no_lineBreaks', y='target')\nsns.lmplot(data=data, x='no_sentences', y='target')\nsns.lmplot(data=data, x='no_words', y='target')\nsns.lmplot(data=data, x='no_characters', y='target')","26f15591":"try_SGDR(data)","069b7d3e":"# potentially useful mean ratios\ndata['mean_sentences_per_lineBreak'] = data['no_sentences'] \/ data['no_lineBreaks']\ndata['mean_words_per_sentence'] = data['no_words'] \/ data['no_sentences']\ndata['mean_characters_per_word'] = data['no_characters'] \/ data['no_words']\ntry_SGDR(data)","ae1c1b7e":"# potentially useful min\/max ratios\nsentences_per_lineBreak = data.excerpt.str.split('\\n').transform(lambda x : [len(y.split('.')) for y in x])\ndata['min_sentences_per_lineBreak'] = sentences_per_lineBreak.apply(min)\ndata['max_sentences_per_lineBreak'] = sentences_per_lineBreak.apply(max)\ndel sentences_per_lineBreak\n\nlineBreaks_per_sentence = data.excerpt.str.split('.').transform(lambda x : [len(y.split('\\n')) for y in x])\ndata['min_lineBreaks_per_sentence'] = lineBreaks_per_sentence.apply(min)\ndata['max_lineBreaks_per_sentence'] = lineBreaks_per_sentence.apply(max)\ndel lineBreaks_per_sentence\n\nwords_per_sentence = data.excerpt.str.split('.').transform(lambda x : [len(y.split(' ')) for y in x])\ndata['min_words_per_sentence'] = words_per_sentence.apply(min)\ndata['max_words_per_sentence'] = words_per_sentence.apply(max)\ndel words_per_sentence\n\nwords_per_lineBreak = data.excerpt.str.split('\\n').transform(lambda x : [len(y.split(' ')) for y in x])\ndata['min_words_per_lineBreak'] = words_per_lineBreak.apply(min)\ndata['max_words_per_lineBreak'] = words_per_lineBreak.apply(max)\ndel words_per_lineBreak\n    \ncharacters_per_word = data.excerpt.str.split(' ').transform(lambda x : [len(y) for y in x])\ndata['min_characters_per_word'] = characters_per_word.apply(min)\ndata['max_characters_per_word'] = characters_per_word.apply(max)\ndel characters_per_word\n\ncharacters_per_sentence = data.excerpt.str.split('.').transform(lambda x : [len(y) for y in x])\ndata['min_characters_per_sentence'] = characters_per_sentence.apply(min)\ndata['max_characters_per_sentence'] = characters_per_sentence.apply(max)\ndel characters_per_sentence\n\ncharacters_per_lineBreak = data.excerpt.str.split('\\n').transform(lambda x : [len(y) for y in x])\ndata['min_characters_per_lineBreak'] = characters_per_lineBreak.apply(min)\ndata['max_characters_per_lineBreak'] = characters_per_lineBreak.apply(max)\ndel characters_per_lineBreak\n\ntry_SGDR(data)","1862267d":"# punctuation marks count in the text\ndifficult_punctuation = \";\\\"'\u2018\u201c:-()[]+?!$&\/\"\ndata['punctuation_count'] = data.excerpt.apply(lambda x : sum([c in difficult_punctuation for c in x]))\ndata['punctuation_frequency'] = data['punctuation_count'] \/ data['no_characters']\ntry_SGDR(data)","b80b1872":"# numbers might indicate a text of technical nature, thus more difficult\nnumber_characters = \"0123456789\"\ndata['number_count'] = data.excerpt.apply(lambda x : sum([c in number_characters for c in x]))\ndata['number_frequency'] = data['number_count'] \/ data['no_characters']\ntry_SGDR(data)","c5a984fd":"# multiple letters might indicate colloquial speech, e.g., aahh, oooh, etc.\ndef count_repeated_characters (word):\n    chars = np.array(list(word))\n    return np.sum(chars[1:] == chars[:-1])\ndata['multiple_count'] = data.excerpt.apply(count_repeated_characters)\ndata['multiple_count_frequency'] = data['multiple_count'] \/ data['no_characters']\ntry_SGDR(data)","3c83069e":"# try with PCA features\ntry_SGDR(data, pca=True)","1605efb9":"# start with something simple: \n# create a list of all words in the training dataset, \n# then save frequency of each, and use that to assess difficulty\n\n# create a word list for each excerpt,\n# remove punctuation and change to lowercase\ndata['word_list'] = data.excerpt.apply(lambda x : re.findall(\"[a-zA-Z]+\", x.lower()))\nif 'word_list' not in noTrain_columns:\n    noTrain_columns += ['word_list',]\n    \nif False: # either calculate word frequency from the (training) dataset, or use an external database\n    # explode each word list into rows\n    word_list = pd.DataFrame({'word':data['word_list'].explode(ignore_index=True)})\n    # count the occurence frequency\n    n_words = word_list.shape[0]\n    word_list['count'] = 0\n    word_list = word_list.groupby('word').count().sort_values('count', ascending=False)\n    word_list \/= n_words\nelse:\n    word_list = pd.read_csv('\/kaggle\/input\/english-word-frequency\/unigram_freq.csv', dtype={'word':str, 'count':int})\n    max_word_length = np.max(word_list.word.apply(lambda x : len(str(x))))\n    word_list.set_index('word', inplace=True)\n    word_list['count'] \/= word_list['count'].sum()\n    \nmax_word_freq = word_list['count'].max()\n    \n# turn the word_list into a dictionary, a function to handle unknown words too\nword_list = word_list.to_dict()['count']\ndef word_freq (word):\n    try:\n        return word_list[word]\n    except Exception as e:\n        return 0.","16b02751":"# now add a mean, min, max frequency of a word in a given excerpt\ndata['word_frequencies'] = data.word_list.apply(lambda x : [word_freq(y) for y in x])\nif 'word_frequencies' not in noTrain_columns:\n    noTrain_columns += ['word_frequencies',]\ndata['mean_word_frequecy'] = data.word_frequencies.apply(np.mean)\ndata['median_word_frequecy'] = data.word_frequencies.apply(np.median)\ndata['min_word_frequecy'] = data.word_frequencies.apply(np.min)\ndata['max_word_frequecy'] = data.word_frequencies.apply(np.max)\ndata['std_word_frequecy'] = data.word_frequencies.apply(np.std)\ntry_SGDR(data)","96b483ab":"# some words are not in the dictionary, may not be English, or could be made up (zigzzzz, huzzah)\ndata['non_word_count'] = data.word_frequencies.apply(lambda x : np.sum(np.array(x) == 0))\ndata['non_word_frequency'] = data['non_word_count'] \/ data['no_words']\ntry_SGDR(data)","ea9a44e6":"sns.displot(data=data, x='no_words')\nprint(data['no_words'].max())","4a3efc09":"# Let's set a constant length of each excerpt, for parsing with NN. If the text is shorter, we will fill it with empty words\nWORDS_PER_EXCERPT = 205","e25f2892":"# Let's start simple, and first only take into account the length of neighboring words\n# (this roughly corresponds to the \"rhythm\" of the text)\n\ncnn = models.Sequential()\ncnn.add(layers.Input(shape=(WORDS_PER_EXCERPT,1)))\ncnn.add(layers.Conv1D(4, (4,), activation='relu'))\ncnn.add(layers.MaxPooling1D((2,)))\ncnn.add(layers.Flatten())\ncnn.add(layers.Dropout(0.2))\ncnn.add(layers.Dense(16, activation='relu'))\ncnn.add(layers.Dropout(0.2))\ncnn.add(layers.Dense(8, activation='relu'))\ncnn.add(layers.Dense(1))","d8be2d32":"# To feed the model, use the first WORDS_PER_EXCERPT words of the excerpt.\n# If less is available, fill with empty\n\ndef nn_preprocess_len (word_list, no_words=WORDS_PER_EXCERPT):\n    # cut to the right length\n    res = [len(x) for x in word_list]\n    if len(res) < no_words:\n        res += [0,] * (no_words - len(res))\n    else:\n        res = res[:no_words]\n    res = np.array(res, dtype=np.float)\n    # normalize\n    res = res * 1.0 \/ max_word_length\n    return res\n\nnn_inputs_len = np.vstack(data.word_list.apply(nn_preprocess_len).to_numpy()).reshape((-1, WORDS_PER_EXCERPT, 1))","f6678004":"# now train the network\n\n# split the data before training, to have the same training sample\n# for both SGDR and cnn\nX_train, X_test, y_train, y_test, pca = split_data(data, \n                                        pca=False, indices=None)\nindices = (y_train.index, y_test.index)\n\n# compile and train the cnn\ncnn.compile(optimizer='adam',\n           loss=tf.keras.losses.MeanSquaredError(),\n           metrics=['RootMeanSquaredError'])\n\nhistory = cnn.fit(nn_inputs_len[indices[0]], y_train,\n                   epochs=10,\n                   validation_data=(nn_inputs_len[indices[1]],y_test))","336fdf9a":"plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\nplt.plot(history.history['val_root_mean_squared_error'], label = 'val_root_mean_squared_error')\nplt.xlabel('Epoch')\nplt.ylabel('RMSE')\nplt.legend(loc='lower right')\n\ntest_loss, test_acc = cnn.evaluate(nn_inputs_len[indices[1]],y_test, verbose=2)","a79e76b4":"def augmented_loss (loss_instance, y, y_val, sample_weight=None):\n    '''This uses the standard_error column to generate a different\n    sample from the target distribution at every training step,\n    to use for loss calculation.'''\n    if y.shape == y_val.shape:\n        y_sample = y\n    else:\n        y_sample = tf.random.normal([1,], y[:,0], y[:,1], y_val.dtype)\n    return loss_instance(stop_gradient(y_sample), y_val, sample_weight)","68d2c4bd":"# not bad, but we can do better with some hyperparameter tuning\n# Note: this is overcounting a lot of combinations, but it does not seem possible to have the trials conditional in the tuner yet...\n\ndef build_cnn_len (hp, augmented=use_augmentation):\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(WORDS_PER_EXCERPT,1)))\n    num_conv_blocks = hp.Int('conv_blocks', 1,5)\n    for i in range(num_conv_blocks):\n        with hp.conditional_scope('conv_blocks', list(range(i+1,5+1))): # num > i\n            nn.add(layers.Conv1D(hp.Int('filters_conv%i' % i,2,16,step=2),\n                (min(nn.output_shape[1], hp.Int('kernel_conv%i' % i,2,16,step=2)),), \n                activation='relu'))\n            pooling_choice = hp.Choice('pooling%i' % i, ['avg', 'max', 'none'])\n            with hp.conditional_scope('pooling%i' % i, ['avg', 'max']):\n                if pooling_choice == 'max':\n                    nn.add(layers.MaxPooling1D(\n                        (min(nn.output_shape[1], hp.Int('kernel_pool%i' % i,2,8,step=2)),)\n                    ))\n                elif pooling_choice == 'avg':\n                    nn.add(layers.AveragePooling1D(\n                        (min(nn.output_shape[1], hp.Int('kernel_pool%i' % i,2,8,step=2)),)\n                    ))\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n    nn.add(layers.Flatten())\n    num_dense_blocks = hp.Int('dense_blocks', 1,3)\n    for i in range(num_dense_blocks):\n        with hp.conditional_scope('dense_blocks', list(range(i+1,3+1))): # num > i\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n            if hp.Choice('batch_norm', [True, False]):\n                nn.add(layers.BatchNormalization())\n            nn.add(layers.Dense(hp.Int('dense%i' % i,4,64,step=4), activation='relu'))\n    if hp.Choice('batch_norm_output', [True, False]):\n        nn.add(layers.BatchNormalization())\n    nn.add(layers.Dense(1))\n    \n    if augmented:\n        loss = (lambda y1,y2,w=None : augmented_loss(tf.keras.losses.MeanSquaredError(), y1,y2,w))\n    else:\n        loss = tf.keras.losses.MeanSquaredError()\n\n    nn.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4,0.3, sampling='log')),\n               loss=loss,\n               metrics=['RootMeanSquaredError'])\n    \n    return nn","a94bc8f1":"\ndef fit_from_hp_dict (build_model, hp_dict, \n                      nn_inputs, indices, y_train, y_test, \n                      early_stopping=True, validation=True,\n                      epochs=1024):\n    '''Using saved HyperParameter.values dict,\n    build the tuned model, train it, and plot diagnostics.'''\n    best_hyperparameters = kt.HyperParameters()\n    best_hyperparameters.values = hp_dict\n    best_model = build_model(best_hyperparameters)\n    if early_stopping:\n        stop_early = tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            patience=16,\n            restore_best_weights=True\n        )\n        callbacks = [stop_early,]\n    else:\n        callbacks = []\n    if validation:\n        validation_data=(nn_inputs[indices[1]],y_test)\n    else:\n        validation_data=None\n    history = best_model.fit(\n        nn_inputs[indices[0]], y_train,\n        epochs=epochs,\n        validation_data=validation_data,\n        callbacks=callbacks\n    )\n    plt.clf()\n    plt.plot(history.history['root_mean_squared_error'], label='root_mean_squared_error')\n    plt.plot(history.history['val_root_mean_squared_error'], label = 'val_root_mean_squared_error')\n    plt.xlabel('Epoch')\n    plt.ylabel('RMSE')\n    plt.legend(loc='lower right')\n    plt.show()\n    plt.close()\n    return best_model","d00a3400":"X_train, X_test, y_train, y_test, pca = split_data(data, \n                                        pca=False, indices=indices,\n                                        augmented=use_augmentation)\nif force_retune: # or not os.path.exists('cnn_word_len.model'):\n    tuner_len = kt.Hyperband(\n        build_cnn_len,\n        objective=kt.Objective('val_root_mean_squared_error',\n                                    direction='min'),\n        max_epochs=32,\n        hyperband_iterations=2,\n        overwrite=True\n    )\n    tuner_len.search(nn_inputs_len[indices[0]], y_train,\n                       epochs=8,\n                       validation_data=(nn_inputs_len[indices[1]],y_test))\n    best_model_len = tuner_len.get_best_models(1)[0]\n    best_hyperparameters_len = tuner_len.get_best_hyperparameters(1)[0]\n    # save for later reuse\n    with open('cnn_word_len.pkl', 'wb') as f:\n        pkl.dump(best_hyperparameters_len, f)\n    best_model_len.save('cnn_word_len.model')\n    print(best_hyperparameters_len.values)\n    print(best_model_len.summary())\nelse:\n    if False: # for some reason, this doesn't work with kaggle notebooks :\/\n        print('Loading best_model_len from previous tuning..', flush=True, end='')\n        with open('cnn_word_len.pkl', 'rb') as f:\n            best_hyperparameters_len = pkl.load(f)\n        best_model_len = models.load_model('cnn_word_len.model')\n    else:\n        print('Setting hyperparameters to best from previous tuning..', flush=True)\n        if use_augmentation:\n            best_hyperparameters_len_dict = {\n                'conv_blocks': 4, 'filters_conv0': 14, \n                'kernel_conv0': 6, 'pooling0': 'none', \n                'dropout': 0.2, 'dense_blocks': 2, \n                'batch_norm': 1, 'dense0': 20, \n                'batch_norm_output': 1, \n                'learning_rate': 0.062226748473994786, \n                'filters_conv1': 12, 'kernel_conv1': 2, \n                'pooling1': 'none', 'filters_conv2': 8, \n                'kernel_conv2': 8, 'pooling2': 'none', \n                'filters_conv3': 10, 'kernel_conv3': 10, \n                'pooling3': 'none', 'dense1': 4, \n                'tuner\/epochs': 32, 'tuner\/initial_epoch': 11,\n                'tuner\/bracket': 1, 'tuner\/round': 1, \n                'tuner\/trial_id': 'd812d9866bd21f4026f92308febbe673'\n            }\n        else:\n            best_hyperparameters_len_dict = {'conv_blocks': 3, \n                'filters_conv0': 12, 'kernel_conv0': 10, \n                'pooling0': 'avg', 'kernel_pool0': 4, \n                'dropout': 0.1, 'dense_blocks': 2, \n                'batch_norm': 1, 'dense0': 16, \n                'batch_norm_output': 0, \n                'learning_rate': 0.00474721153803654, \n                'filters_conv1': 6, 'kernel_conv1': 2, \n                'pooling1': 'none', 'filters_conv2': 4, \n                'kernel_conv2': 12, 'pooling2': 'avg', \n                'kernel_pool2': 4, 'dense1': 32, \n                'tuner\/epochs': 32, 'tuner\/initial_epoch': 11, \n                'tuner\/bracket': 2, 'tuner\/round': 2, \n                'tuner\/trial_id': '27d7de1ebaf2eb206b40a8d999232b00'}\n        best_model_len = fit_from_hp_dict(\n            build_cnn_len, best_hyperparameters_len_dict, \n            nn_inputs_len, indices, y_train, y_test\n        )\n    print('done.')","5030a76a":"data['cnn_word_len'] = best_model_len.predict(nn_inputs_len)\ntry_SGDR(data, indices=indices)","b578eeb8":"print(best_model_len.summary())","a236938d":"# To feed the model, use the first WORDS_PER_EXCERPT words of the excerpt.\n# If less is available, fill with empty\n\ndef nn_preprocess_len_freq (word_list, no_words=WORDS_PER_EXCERPT):\n    # cut to the right length, normalize, extract word frequency\n    res = [[len(x) * 1.0 \/ max_word_length, word_freq(x)] for x in word_list[:no_words]]\n    if len(res) < no_words:\n        res += [[0,0],] * (no_words - len(res))\n    res = np.array(res, dtype=np.float)\n    return res\n\nnn_inputs_len_freq = np.vstack(data.word_list.apply(nn_preprocess_len_freq).to_numpy()).reshape((-1,WORDS_PER_EXCERPT,2))","ed3de830":"nn_inputs_len_freq.shape","160608f6":"# not bad, but we can do better with some hyperparameter tuning\n# Note: this is overcounting a lot of combinations, but it does not seem possible to have the trials conditional in the tuner yet...\n\ndef build_cnn_len_freq (hp, augmented=use_augmentation):\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(WORDS_PER_EXCERPT,2)))\n    num_conv_blocks = hp.Int('conv_blocks', 1,5)\n    for i in range(num_conv_blocks):\n        with hp.conditional_scope('conv_blocks', list(range(i+1,5+1))): # num > i\n            nn.add(layers.Conv1D(hp.Int('filters_conv%i' % i,2,16,step=2),\n                (min(nn.output_shape[1], hp.Int('kernel_conv%i' % i,2,16,step=2)),), \n                activation='relu'))\n            pooling_choice = hp.Choice('pooling%i' % i, ['avg', 'max', 'none'])\n            with hp.conditional_scope('pooling%i' % i, ['avg', 'max']):\n                if pooling_choice == 'max':\n                    nn.add(layers.MaxPooling1D(\n                        (min(nn.output_shape[1], hp.Int('kernel_pool%i' % i,2,8,step=2)),)\n                    ))\n                elif pooling_choice == 'avg':\n                    nn.add(layers.AveragePooling1D(\n                        (min(nn.output_shape[1], hp.Int('kernel_pool%i' % i,2,8,step=2)),)\n                    ))\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n    nn.add(layers.Flatten())\n    num_dense_blocks = hp.Int('dense_blocks', 1,3)\n    for i in range(num_dense_blocks):\n        with hp.conditional_scope('dense_blocks', list(range(i+1,3+1))): # num > i\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n            if hp.Choice('batch_norm', [True, False]):\n                nn.add(layers.BatchNormalization())\n            nn.add(layers.Dense(hp.Int('dense%i' % i,4,64,step=4), activation='relu'))\n    if hp.Choice('batch_norm_output', [True, False]):\n        nn.add(layers.BatchNormalization())\n    nn.add(layers.Dense(1))\n    \n    if augmented:\n        loss = (lambda y1,y2,w=None : augmented_loss(tf.keras.losses.MeanSquaredError(), y1,y2,w))\n    else:\n        loss = tf.keras.losses.MeanSquaredError()\n\n    nn.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4,0.3, sampling='log')),\n               loss=loss,\n               metrics=['RootMeanSquaredError'])\n    \n    return nn","fd041588":"if force_retune: # or not os.path.exists('cnn_word_len_freq.model'):\n    tuner_len_freq = kt.Hyperband(\n        build_cnn_len_freq,\n        objective=kt.Objective('val_root_mean_squared_error',\n                                    direction='min'),\n        max_epochs=32,\n        hyperband_iterations=2,\n        overwrite=True\n    )\n    tuner_len_freq.search(nn_inputs_len_freq[indices[0]], y_train,\n                   epochs=8,\n                   validation_data=(nn_inputs_len_freq[indices[1]],y_test))\n    best_model_len_freq = tuner_len_freq.get_best_models(1)[0]\n    best_hyperparameters_len_freq = tuner_len_freq.get_best_hyperparameters(1)[0]\n    # save for later reuse\n    with open('cnn_word_len_freq.pkl', 'wb') as f:\n        pkl.dump(best_hyperparameters_len_freq, f)\n    best_model_len_freq.save('cnn_word_len_freq.model')\n    print(best_hyperparameters_len_freq.values)\n    print(best_model_len_freq.summary())\nelse:\n    if False:\n        print('Loading best_model_len from previous tuning..', flush=True, end='')\n        with open('cnn_word_len_freq.pkl', 'rb') as f:\n            best_hyperparameters_len_freq = pkl.load(f)\n        best_model_len_freq = models.load_model('cnn_word_len_freq.model')\n    else:\n        print('Setting hyperparameters to best from previous tuning..', flush=True)\n        if use_augmentation:\n            best_hyperparameters_len_freq_dict = {\n                'conv_blocks': 2, 'filters_conv0': 10, \n                'kernel_conv0': 14, 'pooling0': 'max', \n                'kernel_pool0': 4, 'dropout': 0.2, \n                'dense_blocks': 3, 'batch_norm': 1, \n                'dense0': 64, 'batch_norm_output': 1, \n                'learning_rate': 0.0009090024729461547, \n                'filters_conv1': 2, 'kernel_conv1': 14, \n                'pooling1': 'avg', 'kernel_pool1': 8, \n                'dense1': 32, 'dense2': 60, 'tuner\/epochs': 32, \n                'tuner\/initial_epoch': 0, 'tuner\/bracket': 0, \n                'tuner\/round': 0}\n        else:\n            best_hyperparameters_len_freq_dict = {\n                'conv_blocks': 2, 'filters_conv0': 8, \n                'kernel_conv0': 10, 'pooling0': 'none', \n                'dropout': 0.2, 'dense_blocks': 2, 'batch_norm': 1, \n                'dense0': 12, 'batch_norm_output': 0, \n                'learning_rate': 0.018757792810801824, \n                'filters_conv1': 12, 'kernel_conv1': 6, \n                'pooling1': 'avg', 'kernel_pool1': 4, 'dense1': 4, \n                'tuner\/epochs': 32, 'tuner\/initial_epoch': 11, \n                'tuner\/bracket': 1, 'tuner\/round': 1, \n                'tuner\/trial_id': '0011a1157813e370e78f8a237ca72049'}\n        best_model_len_freq = fit_from_hp_dict(\n            build_cnn_len_freq, best_hyperparameters_len_freq_dict, \n            nn_inputs_len_freq, indices, y_train, y_test\n        )\n    print('done.')","a7a9a7da":"data['cnn_word_len+freq'] = best_model_len_freq.predict(nn_inputs_len_freq)\ntry_SGDR(data, indices=indices)","1e42e0bc":"print(best_model_len_freq.summary())","2095e6f2":"embedding_dim = 300 # 50, 100, 200, or 300; see the dataset instructions\n\n# import word embeddings from the word2vec Kaggle dataset\nword2vec = {}\nwith open('\/kaggle\/input\/nlpword2vecembeddingspretrained\/glove.6B.%id.txt' % embedding_dim, 'r') as f:\n    for line in tqdm(f, total=400000):\n        fields = line.split()\n        word2vec[fields[0]] = np.array(fields[1:]).astype(np.float)\ndef word_vec (x):\n    if x in word2vec.keys():\n        return word2vec[x]\n    else:\n        return np.zeros(embedding_dim)","f2392471":"# include word embedding data in our dataframe\ndata['word_embeddings'] = data.word_list.apply(lambda x : np.array([word_vec(y) for y in x]))\nif 'word_embeddings' not in noTrain_columns:\n    noTrain_columns += ['word_embeddings',]","c254fd85":"# some useful statistics with word embeddings we get right away\n\n# the topic of the excerpt\ndata[['mean_embedding%i' % i for i in range(embedding_dim)]] = pd.DataFrame(data.word_embeddings.apply(lambda x : np.mean(x, axis=0).tolist()).to_list())\n\n# the variety of topics touched upon by the excerpt\ndata['stddev_embedding'] = data.word_embeddings.apply(lambda x : np.sum(np.std(x, axis=0)))\n\ntry_SGDR(data, indices=indices)","9ce264ed":"# Does maximum distance between two word embeddings help at all?\ndef max_distance (emb_matrix):\n    max_dist = 0.0\n    n_vectors = emb_matrix.shape[0]\n    for i_vec in range(n_vectors-1):\n        max_dist = max(max_dist, np.max(np.sum((emb_matrix[(i_vec+1):] - emb_matrix[i_vec])**2, axis=1)))\n    return np.sqrt(max_dist)\n\ndata['maxdist_embedding'] = data.word_embeddings.apply(max_distance)\n\ntry_SGDR(data, indices=indices)","7d0c7e44":"# For now, let's use the first WORDS_PER_EXCERPT words again. We will expand it to include the entire excerpts later..\n\ndef nn_preprocess_emb (emb_matrix, no_words=WORDS_PER_EXCERPT):\n    # cut or pad to the right length\n    return np.concatenate([emb_matrix[:no_words,:], np.zeros([max(0,no_words-emb_matrix.shape[0]),emb_matrix.shape[1]])])\n\nnn_inputs_emb = np.vstack(data.word_embeddings.apply(nn_preprocess_emb).to_numpy()).reshape((-1,WORDS_PER_EXCERPT,embedding_dim))","ff4f1caf":"nn_inputs_emb.shape","28c3dae7":"def build_cnn_emb (hp, augmented=use_augmentation):\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(WORDS_PER_EXCERPT,embedding_dim)))\n    num_conv_blocks = hp.Int('conv_blocks', 0,5)\n    for i in range(num_conv_blocks):\n        with hp.conditional_scope('conv_blocks', list(range(i+1,5+1))): # num > i\n            nn.add(layers.Conv1D(hp.Int('filters_conv%i' % i,2,16,step=2),\n                (min(nn.output_shape[1], hp.Int('kernel_conv%i' % i,2,16,step=2)),), \n                activation='relu'))\n            pooling_choice = hp.Choice('pooling%i' % i, ['avg', 'max', 'none'])\n            with hp.conditional_scope('pooling%i' % i, ['avg', 'max']):\n                if pooling_choice == 'max':\n                    nn.add(layers.MaxPooling1D(\n                        (min(nn.output_shape[1], hp.Int('kernel_pool%i' % i,2,8,step=2)),)\n                    ))\n                elif pooling_choice == 'avg':\n                    nn.add(layers.AveragePooling1D(\n                        (min(nn.output_shape[1], hp.Int('kernel_pool%i' % i,2,8,step=2)),)\n                    ))\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n    nn.add(layers.Flatten())\n    num_dense_blocks = hp.Int('dense_blocks', 1,5)\n    for i in range(num_dense_blocks):\n        with hp.conditional_scope('dense_blocks', list(range(i+1,5+1))): # num > i\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n            if hp.Choice('batch_norm', [True, False]):\n                nn.add(layers.BatchNormalization())\n            nn.add(layers.Dense(hp.Int('dense%i' % i,4,64,step=4), activation='relu'))\n    if hp.Choice('batch_norm_output', [True, False]):\n        nn.add(layers.BatchNormalization())\n    nn.add(layers.Dense(1))\n    \n    if augmented:\n        loss = (lambda y1,y2,w=None : augmented_loss(tf.keras.losses.MeanSquaredError(), y1,y2,w))\n    else:\n        loss = tf.keras.losses.MeanSquaredError()\n\n    nn.compile(optimizer=tf.keras.optimizers.Adam(hp.Float('learning_rate', 1e-4,0.3, sampling='log')),\n               loss=loss,\n               metrics=['RootMeanSquaredError'])\n    \n    return nn","60050cea":"if force_retune: # or not os.path.exists('cnn_word_emb.model'):\n    tuner_emb = kt.Hyperband(\n        build_cnn_emb,\n        objective=kt.Objective('val_root_mean_squared_error',\n                                    direction='min'),\n        max_epochs=128,\n        hyperband_iterations=2,\n        overwrite=True\n    )\n    tuner_emb.search(nn_inputs_emb[indices[0]], y_train,\n                   epochs=8,\n                   validation_data=(nn_inputs_emb[indices[1]],y_test))\n    best_model_emb = tuner_emb.get_best_models(1)[0]\n    best_hyperparameters_emb = tuner_emb.get_best_hyperparameters(1)[0]\n    # save for later reuse\n    with open('cnn_word_emb.pkl', 'wb') as f:\n        pkl.dump(best_hyperparameters_emb, f)\n    best_model_emb.save('cnn_word_emb.model')\n    print(best_hyperparameters_emb.values)\n    print(best_model_emb.summary())\nelse:\n    if False:\n        print('Loading best_model_len from previous tuning..', flush=True, end='')\n        with open('cnn_word_emb.pkl', 'rb') as f:\n            best_hyperparameters_emb = pkl.load(f)\n        best_model_emb = models.load_model('cnn_word_emb.model')\n    else:\n        print('Setting hyperparameters to best from previous tuning..', flush=True)\n        if use_augmentation:\n            best_hyperparameters_emb_dict = {\n                'conv_blocks': 3, 'dense_blocks': 2, \n                'dropout': 0.30000000000000004, 'batch_norm': 0, \n                'dense0': 16, 'batch_norm_output': 1, \n                'learning_rate': 0.08197541995276879, \n                'filters_conv0': 2, 'kernel_conv0': 14, \n                'pooling0': 'max', 'kernel_pool0': 4, \n                'filters_conv1': 6, 'kernel_conv1': 12, \n                'pooling1': 'none', 'dense1': 44, \n                'filters_conv2': 14, 'kernel_conv2': 4, \n                'pooling2': 'avg', 'kernel_pool2': 2, \n                'tuner\/epochs': 43, 'tuner\/initial_epoch': 15, \n                'tuner\/bracket': 2, 'tuner\/round': 1, \n                'tuner\/trial_id': 'dbed8e7c1c9d30298fe0430de255d50f'}\n        else:\n            best_hyperparameters_emb_dict = {\n                'conv_blocks': 1, 'dense_blocks': 2, \n                'dropout': 0.30000000000000004, 'batch_norm': 0, \n                'dense0': 48, 'batch_norm_output': 1, \n                'learning_rate': 0.002693667798794543, \n                'dense1': 12, 'filters_conv0': 10, 'kernel_conv0': 4, \n                'pooling0': 'max', 'kernel_pool0': 6, \n                'tuner\/epochs': 43, 'tuner\/initial_epoch': 15, \n                'tuner\/bracket': 4, 'tuner\/round': 3, \n                'tuner\/trial_id': '6ef193d541fe31f3ba90e45aedbaafdf'\n            }\n        best_model_emb = fit_from_hp_dict(\n            build_cnn_emb, best_hyperparameters_emb_dict, \n            nn_inputs_emb, indices, y_train, y_test\n        )\n    print('done.')","635d0c9c":"data['cnn_word_embeddings'] = best_model_emb.predict(nn_inputs_emb)\ntry_SGDR(data, indices=indices)","88e1f82c":"print(best_model_emb.summary())","8b50d296":"def build_lstm_emb (hp, augmented=use_augmentation):\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(WORDS_PER_EXCERPT,embedding_dim)))\n    num_conv_blocks = hp.Int('lstm_blocks', 1,1)\n    for i in range(num_conv_blocks):\n        with hp.conditional_scope('lstm_blocks', list(range(i+1,5+1))): # num > i\n            nn.add(layers.LSTM(\n                hp.Int('lstm_units%i' % i, 8,128, sampling='log')\n            ))\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n    nn.add(layers.Flatten())\n    num_dense_blocks = hp.Int('dense_blocks', 1,5)\n    for i in range(num_dense_blocks):\n        with hp.conditional_scope('dense_blocks', list(range(i+1,5+1))): # num > i\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n            if hp.Choice('batch_norm', [True, False]):\n                nn.add(layers.BatchNormalization())\n            nn.add(layers.Dense(hp.Int('dense%i' % i,4,64,step=4), activation='relu'))\n    if hp.Choice('batch_norm_output', [True, False]):\n        nn.add(layers.BatchNormalization())\n    nn.add(layers.Dense(1))\n    \n    if augmented:\n        loss = (lambda y1,y2,w=None : augmented_loss(tf.keras.losses.MeanSquaredError(), y1,y2,w))\n    else:\n        loss = tf.keras.losses.MeanSquaredError()\n\n    nn.compile(optimizer=tf.keras.optimizers.Adam(\n                hp.Float('learning_rate', 1e-4,0.3, sampling='log')\n               ),\n               loss=loss,\n               metrics=['RootMeanSquaredError'])\n    \n    return nn","74bb86a0":"if force_retune: # or not os.path.exists('lstm_word_emb.model'):\n    tuner_emb = kt.Hyperband(\n        build_lstm_emb,\n        objective=kt.Objective('val_root_mean_squared_error',\n                                    direction='min'),\n        max_epochs=128,\n        hyperband_iterations=2,\n        overwrite=True\n    )\n    tuner_emb.search(nn_inputs_emb[indices[0]], y_train,\n                   epochs=8,\n                   validation_data=(nn_inputs_emb[indices[1]],y_test))\n    best_model_emb_lstm = tuner_emb.get_best_models(1)[0]\n    best_hyperparameters_emb_lstm = tuner_emb.get_best_hyperparameters(1)[0]\n    # save for later reuse\n    with open('lstm_word_emb.pkl', 'wb') as f:\n        pkl.dump(best_hyperparameters_emb_lstm, f)\n    best_model_emb_lstm.save('lstm_word_emb.model')\n    print(best_hyperparameters_emb_lstm.values)\n    print(best_model_emb_lstm.summary())\nelse:\n    if False:\n        print('Loading best_model_len from previous tuning..', flush=True, end='')\n        with open('lstm_word_emb.pkl', 'rb') as f:\n            best_hyperparameters_emb_lstm = pkl.load(f)\n        best_model_emb_lstm = models.load_model('lstm_word_emb.model')\n    else:\n        print('Setting hyperparameters to best from previous tuning..', flush=True)\n        if use_augmentation:\n            best_hyperparameters_emb_lstm_dict = {\n                'lstm_blocks': 1, 'lstm_units0': 55, 'dropout': 0.1, \n                'dense_blocks': 1, 'batch_norm': 1, 'dense0': 36, \n                'batch_norm_output': 0, \n                'learning_rate': 0.0036906795279518277, \n                'tuner\/epochs': 128, 'tuner\/initial_epoch': 0, \n                'tuner\/bracket': 0, 'tuner\/round': 0}\n        else:\n            best_hyperparameters_emb_lstm_dict = {\n                'lstm_blocks': 1, 'lstm_units0': 66, 'dropout': 0.2, \n                'dense_blocks': 3, 'batch_norm': 1, 'dense0': 20, \n                'batch_norm_output': 1, \n                'learning_rate': 0.0022843219066342054, 'dense1': 60, \n                'dense2': 56, 'tuner\/epochs': 43, \n                'tuner\/initial_epoch': 0, 'tuner\/bracket': 1, \n                'tuner\/round': 0\n            }\n        best_model_emb_lstm = fit_from_hp_dict(\n            build_lstm_emb, best_hyperparameters_emb_lstm_dict, \n            nn_inputs_emb, indices, y_train, y_test\n        )\n    print('done.')","cfd4e611":"data['lstm_word_embeddings'] = best_model_emb_lstm.predict(nn_inputs_emb)\ntry_SGDR(data, indices=indices)","b47bed64":"# first, find the best scaler to use with SGDR\nscalers = {\n    'Standard': StandardScaler(),\n    'MinMax': MinMaxScaler(),\n    'Robust': RobustScaler(),\n    'PowerTransf': PowerTransformer(),\n    'QuantileTransf': QuantileTransformer(),\n    'QuantileTransfGauss': QuantileTransformer(output_distribution='normal'),\n    'Normalizer': Normalizer()\n}\nif force_retune:\n    X_train, X_test, y_train, y_test, pca = split_data(data, pca=False, indices=indices)\n    print('Choosing the best scaler:')\n    best_rmse = 1.0e9\n    for scaler in scalers.keys():\n        # test the parameter combination\n        reg = make_pipeline(scalers[scaler],\n                            SGDRegressor(\n                                random_state=random_state\n                            ))\n        reg.fit(X_train, y_train)\n        rmse = mean_squared_error(y_test, reg.predict(X_test), squared=False)\n        print('  -- %s: rmse=%.2f' % (scaler, rmse))\n        if rmse < best_rmse:\n            best_scaler = scaler\n            best_rmse = rmse\n        del reg\n    print('The best scaler for SGDR is %s, with best rmse=%.2f' % (best_scaler, best_rmse))\nelse:\n    best_scaler = 'QuantileTransf'","0112d743":"@ignore_warnings(category=ConvergenceWarning)\ndef tune_SGDR (data, indices=None, output=False, best_scaler='QuantileTransf'):\n    total_trials = 2*4*5*6*3*3\n    print(\"Approx. no of trials to perform: %i\" % total_trials)\n    trial_no = 0\n    best_rmse = 1.0e9\n    best_parameters = None\n    best_pca = None\n    rmse = None\n    for scaler in [best_scaler,]:\n        for pca in [True, False]:\n            X_train, X_test, y_train, y_test, _ = split_data(data, pca=pca, indices=indices)\n            X_train_scaled = scalers[scaler].fit_transform(X_train)\n            X_test_scaled = scalers[scaler].transform(X_test)\n            for loss in ['squared_loss', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive']:\n                for penalty in ['elasticnet',]:\n                    for l1ratio in np.linspace(0.,1.,5):\n                        for alpha in [1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2]:\n                            if loss != 'squared_loss':\n                                epsilons = [1e-3, 1e-2, 1e-1]\n                            else:\n                                epsilons = [0.1,] # ignored\n                            for epsilon in epsilons:\n                                for learning_rate in ['optimal', 'invscaling', 'adaptive']:\n                                    #if learning_rate != 'optimal':\n                                    #    eta0s = [1e-3, 1e-2, 1e-1]\n                                    #else:\n                                    eta0s = [0.01,]\n                                    for eta0 in eta0s:\n                                        #if learning_rate == 'invscaling':\n                                        #    power_ts = np.linspace(0.01,1.0,4)\n                                        #else:\n                                        power_ts = [0.25,]\n                                        for power_t in power_ts:\n                                            hps = {\n                                                'loss':loss, 'penalty':penalty,\n                                                'l1_ratio':l1ratio, 'alpha':alpha,\n                                                'epsilon':epsilon,\n                                                'learning_rate':learning_rate,\n                                                'eta0':eta0, 'power_t':power_t\n                                            }\n                                            # report on progress\n                                            if trial_no % int(total_trials\/100) == 0:\n                                                print('\\n %i %% done -----------------------------------------' % int(100.*trial_no\/total_trials), flush=True)\n                                                if trial_no % int(total_trials\/20) == 0:\n                                                    print('best rmse: ', best_rmse)\n                                                    print('best hps: ', best_parameters)\n                                                    print('best pca: ', best_pca)\n                                                    print('best scaler: ', best_scaler)\n                                                    print('------------------------------------------------------', flush=True)\n                                                    print('current rmse: ', rmse)\n                                                    print('current hps: ', hps)\n                                                    print('current pca: ', pca)\n                                                    print('current scaler: ', scaler)\n                                                    print('------------------------------------------------------', flush=True)\n                                            else:\n                                                print('.', flush=True, end='')\n                                            # test the parameter combination\n                                            reg = SGDRegressor(\n                                                **hps,\n                                                random_state=random_state\n                                            )\n                                            reg.fit(X_train_scaled, y_train)\n                                            rmse = mean_squared_error(y_test, reg.predict(X_test_scaled), squared=False)\n                                            if rmse < best_rmse:\n                                                best_parameters = hps\n                                                best_rmse = rmse\n                                                best_pca = pca\n                                                best_scaler = scaler\n                                            del reg\n                                            trial_no += 1\n    print(\"\\nBest RMSE = %.2e\" % best_rmse)\n    print(\"  best hyperparameters:\\n    \", best_parameters)\n    print(\"  best pca: \", best_pca)\n    print(\"  best scaler: \", best_scaler)\n    if output:\n        return best_parameters, best_pca, best_scaler, reg\n    else:\n        return best_parameters, best_pca, best_scaler","353d38df":"if force_retune:\n    sgdr_best_hps, sgdr_best_pca, sgdr_best_scaler = tune_SGDR(data, indices=indices, best_scaler=best_scaler)\nelse:\n    if use_augmentation:\n        sgdr_best_hps = {\n            'loss': 'epsilon_insensitive', 'penalty': 'elasticnet', \n            'l1_ratio': 0.0, 'alpha': 0.1, 'epsilon': 0.001, \n            'learning_rate': 'invscaling', 'eta0': 0.01, 'power_t': 0.25\n        }\n    else:\n        sgdr_best_hps = {\n            'loss': 'squared_epsilon_insensitive', 'penalty': 'elasticnet', \n            'l1_ratio': 0.0, 'alpha': 0.1, 'epsilon': 0.001, \n            'learning_rate': 'invscaling', 'eta0': 0.01, 'power_t': 0.25\n        }\n    sgdr_best_pca = False\n    sgdr_best_scaler = best_scaler","dae2f3f7":"def try_SGDR_opt (data, indices=None, output=False):\n    X_train, X_test, y_train, y_test, pca = split_data(data, pca=sgdr_best_pca, indices=indices)\n    reg = make_pipeline(scalers[sgdr_best_scaler],\n                        SGDRegressor(\n                            **sgdr_best_hps,\n                            random_state=random_state\n                        ))\n    reg.fit(X_train, y_train)\n    print(\"RMSE: \", mean_squared_error(y_test, reg.predict(X_test), squared=False))\n    if output:\n        return reg\n\ntry_SGDR_opt(data, indices=indices)","9a8ea781":"# Let's also try a bunch of other classical ML regressors, first with default parameters\nX_train, X_test, y_train, y_test, pca = split_data(data, pca=False, indices=indices)\nX_all, _, y_all, _, pca = split_data(data, pca=False, indices=(data.index,[]))\nml_models = {\n    'LR': LinearRegression(),\n    'LGBM': LGBMRegressor(),\n    'XGB': XGBRegressor(),\n    'CatBoost': CatBoostRegressor(silent=True),\n    'KernelRidge': KernelRidge(),\n    'ElasticNet': ElasticNet(),\n    'BayesianRidge': BayesianRidge(),\n    'GradientBoosting': GradientBoostingRegressor(),\n    'SVM': SVR(),\n    'SGDR': SGDRegressor()\n}\nall_model_predictions = pd.DataFrame()\nrmses = {}\nbest_model = None\nbest_rmse = 1.0e9\nfor name in ml_models.keys():\n    if name not in rmses.keys() or force_retune:\n        reg = make_pipeline(StandardScaler(),\n            ml_models[name])\n        reg.fit(X_train, y_train)\n        rmses[name] = mean_squared_error(y_test, reg.predict(X_test), squared=False)\n        if name != 'SGDR':\n            all_model_predictions['prediction_'+name] = reg.predict(X_all)\n        print('%s with scaler: RMSE = %.2f' % (name, rmses[name]))\n        if (rmses[name] < best_rmse):\n            best_model = name\n            best_rmse = rmses[name]\n        reg = make_pipeline(\n            ml_models[name])\n        reg.fit(X_train, y_train)\n        name += '-unscaled'\n        rmses[name] = mean_squared_error(y_test, reg.predict(X_test), squared=False)\n        if name != 'SGDR-unscaled':\n            all_model_predictions['prediction_'+name] = reg.predict(X_all)\n        print('  -- without scaler: RMSE = %.2f' % (rmses[name]))\n        if (rmses[name] < best_rmse):\n            best_model = name\n            best_rmse = rmses[name]\nprint(\"The best model is %s with rmse=%.2f.\" % (best_model, best_rmse))","4dcf9e1f":"# Since we've done the work of training all these models, we can add their predictions to our dataframe too..\nreg = try_SGDR_opt(data, indices=indices, output=True)\nall_model_predictions['prediction_SGDR-opt'] = reg.predict(X_all)\n#data = data.join(all_model_predictions[all_model_predictions.columns.difference(['prediction_SGDR', 'prediction_SGDR-unscaled', 'prediction_'+best_model])])","5041974b":"# first, find the best scaler to use with SVM\nscalers = {\n    'Standard': StandardScaler(),\n    'MinMax': MinMaxScaler(),\n    'Robust': RobustScaler(),\n    'PowerTransf': PowerTransformer(),\n    'QuantileTransf': QuantileTransformer(),\n    'QuantileTransfGauss': QuantileTransformer(output_distribution='normal'),\n    'Normalizer': Normalizer()\n}\nif force_retune:\n    X_train, X_test, y_train, y_test, pca = split_data(data, pca=False, indices=indices)\n    print('Choosing the best scaler:')\n    best_rmse = 1.0e9\n    for scaler in scalers.keys():\n        # test the parameter combination\n        reg = make_pipeline(scalers[scaler],\n                            SVR())\n        reg.fit(X_train, y_train)\n        rmse = mean_squared_error(y_test, reg.predict(X_test), squared=False)\n        print('  -- %s: rmse=%.2f' % (scaler, rmse))\n        if rmse < best_rmse:\n            best_scaler = scaler\n            best_rmse = rmse\n        del reg\n    print('The best scaler for SVM is %s, with best rmse=%.2f' % (best_scaler, best_rmse))\nelse:\n    if use_augmentation:\n        best_scaler = 'QuantileTransf'\n    else:\n        best_scaler = 'Robust'","7da67e0d":"# Tune the SVM model\n@ignore_warnings(category=ConvergenceWarning)\ndef tune_SVM (data, indices=None, output=False, best_scaler='Robust'):\n    total_trials = 4*3*3*2*5*3*2\n    print(\"Approx. no of trials to perform: %i\" % total_trials)\n    trial_no = 0\n    best_rmse = 1.0e9\n    best_parameters = None\n    best_pca = None\n    rmse = None\n    for scaler in [best_scaler,]:\n        for pca in [True, False]:\n            X_train, X_test, y_train, y_test, _ = split_data(data, pca=pca, indices=indices)\n            X_train_scaled = scalers[scaler].fit_transform(X_train)\n            X_test_scaled = scalers[scaler].transform(X_test)\n            for kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n                if kernel == 'poly':\n                    degrees = [2,3,4]\n                else:\n                    degrees = [1,]\n                for degree in degrees:\n                    if kernel in ['rbf', 'poly', 'sigmoid']:\n                        gammas = ['scale', 'auto']\n                    else:\n                        gammas = ['scale',]\n                    for gamma in gammas:\n                        for C in [1e-2, 1e-1, 1, 10, 100]:\n                            for epsilon in [1e-3, 1e-2, 1e-1]:\n                                for shrinking in [True, False]:\n                                    hps = {\n                                        'kernel':kernel, 'degree':degree,\n                                        'gamma':gamma, 'C':C,\n                                        'epsilon':epsilon, 'shrinking':shrinking,\n                                        'max_iter':1e4\n                                    }\n                                    # report on progress\n                                    if trial_no % int(total_trials\/100) == 0:\n                                        print('\\n %i %% done -----------------------------------------' % int(100.*trial_no\/total_trials), flush=True)\n                                        if trial_no % int(total_trials\/20) == 0:\n                                            print('best rmse: ', best_rmse)\n                                            print('best hps: ', best_parameters)\n                                            print('best pca: ', best_pca)\n                                            print('best scaler: ', best_scaler)\n                                            print('------------------------------------------------------', flush=True)\n                                            print('current rmse: ', rmse)\n                                            print('current hps: ', hps)\n                                            print('current pca: ', pca)\n                                            print('current scaler: ', scaler)\n                                            print('------------------------------------------------------', flush=True)\n                                    else:\n                                        print('.', flush=True, end='')\n                                    # test the parameter combination\n                                    reg = SVR(\n                                        **hps\n                                    )\n                                    reg.fit(X_train_scaled, y_train)\n                                    rmse = mean_squared_error(y_test, reg.predict(X_test_scaled), squared=False)\n                                    if rmse < best_rmse:\n                                        best_parameters = hps\n                                        best_rmse = rmse\n                                        best_pca = pca\n                                        best_scaler = scaler\n                                    del reg\n                                    trial_no += 1\n    print(\"\\nBest RMSE = %.2e\" % best_rmse)\n    print(\"  best hyperparameters:\\n    \", best_parameters)\n    print(\"  best pca: \", best_pca)\n    print(\"  best scaler: \", best_scaler)\n    if output:\n        return best_parameters, best_pca, best_scaler, reg\n    else:\n        return best_parameters, best_pca, best_scaler","2eb04fc7":"if force_retune:\n    svm_best_hps, svm_best_pca, svm_best_scaler = tune_SVM(data, indices=indices, best_scaler=best_scaler)\nelse:\n    if use_augmentation:\n        svm_best_hps = {'kernel': 'poly', 'degree': 3, 'gamma': 'auto', 'C': 10, 'epsilon': 0.1, 'shrinking': True}\n    else:\n        svm_best_hps = {'kernel': 'rbf', 'degree': 1, 'gamma': 'scale', 'C': 0.1, 'epsilon': 0.1, 'shrinking': True}\n    svm_best_pca = False\n    svm_best_scaler = best_scaler","332b57f5":"def try_SVM_opt (data, indices=None, output=False):\n    X_train, X_test, y_train, y_test, pca = split_data(data, pca=svm_best_pca, indices=indices)\n    reg = make_pipeline(scalers[svm_best_scaler],\n                        SVR(\n                            **svm_best_hps\n                        ))\n    reg.fit(X_train, y_train)\n    print(\"RMSE: \", mean_squared_error(y_test, reg.predict(X_test), squared=False))\n    if output:\n        return reg\n\nreg = try_SVM_opt(data, indices=indices, output=True)\ntry_SGDR_opt(data, indices=indices)\nall_model_predictions['prediction_SVM-opt'] = reg.predict(X_all)","9ca48d00":"# Does stacking models hierachically help? - not really\nall_model_predictions['target'] = data['target']\ntry_SVM_opt(all_model_predictions, indices=indices)\ntry_SGDR_opt(all_model_predictions, indices=indices)","e9624ba4":"print(noTrain_columns)\ncols_features = data.columns.difference(noTrain_columns)\nprint(cols_features)\n\nno_features = len(cols_features)\n\nnn_inputs_fin = data[cols_features].to_numpy()\nprint(nn_inputs_fin.shape)","5c0416c5":"def build_fin (hp):\n    nn = models.Sequential()\n    nn.add(layers.Input(shape=(no_features,)))\n    nn.add(layers.Dense(hp.Int('dense_base',4,256, sampling='log'), activation='relu'))\n    num_dense_blocks = hp.Int('dense_blocks', 1,7)\n    for i in range(num_dense_blocks):\n        with hp.conditional_scope('dense_blocks', list(range(i+1,7+1))): # num > i\n            nn.add(layers.Dropout(hp.Float('dropout',0.,0.7,step=0.1)))\n            if hp.Choice('batch_norm', [True, False]):\n                nn.add(layers.BatchNormalization())\n            nn.add(layers.Dense(hp.Int('dense%i' % i,4,256, sampling='log'), activation='relu'))\n    if hp.Choice('batch_norm_output', [True, False]):\n        nn.add(layers.BatchNormalization())\n    nn.add(layers.Dense(1))\n\n    nn.compile(optimizer=tf.keras.optimizers.Adam(\n                hp.Float('learning_rate', 1e-4,0.3, sampling='log')\n               ),\n               loss=tf.keras.losses.MeanSquaredError(),\n               metrics=['RootMeanSquaredError'])\n    \n    return nn","347ab8c4":"if force_retune: # or not os.path.exists('lstm_word_emb.model'):\n    tuner_fin = kt.Hyperband(\n        build_fin,\n        objective=kt.Objective('val_root_mean_squared_error',\n                                    direction='min'),\n        max_epochs=128,\n        hyperband_iterations=2,\n        overwrite=True\n    )\n    tuner_fin.search(nn_inputs_fin[indices[0]], y_train,\n                   epochs=8,\n                   validation_data=(nn_inputs_fin[indices[1]],y_test))\n    best_model_fin = tuner_fin.get_best_models(1)[0]\n    best_hyperparameters_fin = tuner_fin.get_best_hyperparameters(1)[0]\n    # save for later reuse\n    with open('fin.pkl', 'wb') as f:\n        pkl.dump(best_hyperparameters_fin, f)\n    best_model_fin.save('fin.model')\n    print(best_hyperparameters_fin.values)\n    print(best_model_fin.summary())\nelse:\n    if False:\n        print('Loading best_model_fin from previous tuning..', flush=True, end='')\n        with open('fin.pkl', 'rb') as f:\n            best_hyperparameters_fin = pkl.load(f)\n        best_model_fin = models.load_model('fin.model')\n    else:\n        print('Setting hyperparameters to best from previous tuning..', flush=True)\n        best_hyperparameters_fin_dict = {\n            'dense_base': 28, 'dense_blocks': 3, 'dropout': 0.1, \n            'batch_norm': 0, 'dense0': 155, 'batch_norm_output': 1, \n            'learning_rate': 0.0031655607588791834, 'dense1': 35, \n            'dense2': 9, 'tuner\/epochs': 128, 'tuner\/initial_epoch': 43, \n            'tuner\/bracket': 4, 'tuner\/round': 4, \n            'tuner\/trial_id': '9a74d2e1699f371fabb097cd347ff640'\n        }\n        best_model_fin = fit_from_hp_dict(\n            build_fin, best_hyperparameters_fin_dict, \n            nn_inputs_fin, indices, y_train, y_test\n        )\n    print('done.')","edac6790":"data['nn_final'] = best_model_fin.predict(nn_inputs_fin)\ntry_SGDR_opt(data, indices=indices)","2d4e48e1":"data = data.drop('nn_final', axis=1)\ntry_SGDR_opt(data, indices=indices)","c9f96e2d":"print(noTrain_columns)\ncols_features = data.columns.difference(noTrain_columns)\nprint(cols_features)","7863c46f":"noTrain_columns_orig = noTrain_columns.copy()\nnoTrain_columns += ['mean_embedding%i' % i for i in range(embedding_dim)]\ntry_SGDR_opt(data, indices=indices)","fab0e3e0":"noTrain_columns = noTrain_columns_orig\ntry_SGDR_opt(data, indices=indices)","ef6e153f":"data_expanded = data.copy()","25d08517":"# add word lengths\ndata_expanded = data_expanded.join(pd.DataFrame(np.squeeze(nn_inputs_len), columns=['word_len_%i' % i for i in range(WORDS_PER_EXCERPT)]))\nprint(data_expanded.columns)\ntry_SGDR_opt(data_expanded, indices=indices)","7724a2e8":"# add word frequencies\ndata_expanded = data_expanded.join(pd.DataFrame(np.squeeze(nn_inputs_len_freq[:,:,1]), columns=['word_freq_%i' % i for i in range(WORDS_PER_EXCERPT)]))\nprint(data_expanded.columns)\ntry_SGDR_opt(data_expanded, indices=indices)","8a2569b7":"# add word embeddings\nembshape = list(nn_inputs_emb.shape)\ndims_to_include = 16 # not enough memory to get the whole thing\nembshape[2] = dims_to_include\ndata_expanded = data_expanded.join(pd.DataFrame(np.squeeze(nn_inputs_emb[:,:,:dims_to_include].reshape((embshape[0],embshape[1]*embshape[2]))), columns=['word_emb_%i' % i for i in range(WORDS_PER_EXCERPT*dims_to_include)]))\nprint(data_expanded.columns)\ntry_SGDR_opt(data_expanded, indices=indices)","c97e4dbd":"try_SGDR(data_expanded, indices=indices)\ntry_SGDR(data_expanded, indices=indices, pca=True)\ntry_SVM_opt(data_expanded, indices=indices)","077da1c0":"Finally, instead of a classical ML regressor, let us use a deep neural network for regression..","37d868fc":"That's great so far! Ok, now let's connect the embeddings to a neural network.","9ee35212":"Oh, SVM seems to work best then. Let us optimize its hyperparameters then. But first, since we've done the work of training all these models, we can add their predictions to our dataframe too.","bb4a7262":"Maybe a bit better, but still rather disappointing..\n\nPerhaps we could experiment a bit with the final classifier..","6b7cd175":"Well, simple counting will probably not get us much further. It's time to teach our model to understand the meaning of the text. For that, we will use word embeddings.","5b85595b":"Hmm.. that's not too good.. A CNN might not be the best choice for text comprehension though.. Perhaps LSTM?","0df8f0d1":"Not really, let's keep them then","00b9859d":"Now, it may be a good idea to take into account relations between nearby words. A convolutional neural network should be great for that. We will use it to generate another column in our feature set.","963cfa54":"We have some columns we don't use. Would they help?","d1698189":"Now, let us add word frequency data to the same neural network, to see if it improves this way...","d7ffb5f5":"There's a lot of columns with mean embeddings. Does removing them help?","f6713044":"Adding min\/max ratios gave us a slight improvement, but otherwise it seems that not much more can be done with simple count statistics of the text.. It's time to start working with text comprehension.","63e28bed":"Well, SGDR is still the best. Let's have one more look at our dataframe. Would adding or removing any of the columns help?","ab5d9e5f":"Not much better. Well, it would appear we should end here, since the competition deadline is just in a few days. I will parse the complete model in another notebook, 2021_CommonLitReadability_final, for clarity. This was fun! ;)"}}