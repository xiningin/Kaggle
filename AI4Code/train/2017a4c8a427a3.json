{"cell_type":{"81180333":"code","6e31e02e":"code","835e0cc2":"code","2cdb31f3":"code","c2fbdbe5":"code","75d4bd06":"code","96fada6e":"code","9962f68d":"code","ce5773f2":"code","1eb237bd":"code","5ece9b27":"markdown","af8aaa0f":"markdown","52517e8c":"markdown","b0e3b33f":"markdown","38579413":"markdown","0f2986ad":"markdown","61027ff7":"markdown","e09c3254":"markdown"},"source":{"81180333":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndataset = pd.read_csv('..\/input\/social-network-ads\/Social_Network_Ads.csv')\ndataset.head()","6e31e02e":"x = dataset.iloc[:, [2,3]]\ny = dataset.iloc[:, 4]\n\n#split test and train set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\n#feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_train)\n\n#building the model\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, Y_train)\n\n#predicting based on the model\ny_pred = classifier.predict(X_test)","835e0cc2":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = Y_train, cv = 10)\n#cv --> number of parts (folds): 10 is the most common practice\n#estimator --> put the trained model.\nprint(accuracies)\nprint(accuracies.mean()) #this is a better evaluation of the model.\nprint(accuracies.std()) #to get a sense of variance and bias","2cdb31f3":"#Applying grid search\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{\"C\": [1, 10, 100, 1000], \"kernel\": ['linear']}, \n              {\"C\": [1, 10, 100, 1000], \"kernel\": ['rbf'], 'gamma': [0.5, 0.1, 0.01, 0.001]}]\n#we use this parameters list of dictionaries to test out many combinations of parameters and models.\n#This dictionary is tailored to SVM model. For different models, different dictionary keys and values should be used.\n#C: penalty parameter that reduces overfitting. Gamma: for optimal kernel\n\n#Use this list to train\ngrid_search = GridSearchCV(estimator = classifier, param_grid = parameters, scoring = 'accuracy', cv = 10, n_jobs = -1)\n#scoring: how models are evaluated | cv: apply K-fold cross validation\n#n_jobs: how much CPU to use. -1 --> all CPU.\n\ngrid_search = grid_search.fit(X_train, Y_train)\n\n#Use attributes of grid_search to get the results\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\n\nprint(best_accuracy)\nprint(best_parameters)","c2fbdbe5":"dataset2 = pd.read_csv('..\/input\/bank-customer-churn-modeling\/Churn_Modelling.csv')\ndataset2.head()","75d4bd06":"#prepare the dataset\nx = dataset2.iloc[:, 3:13]\ny = dataset2.iloc[:, 13]\n\nfrom sklearn.preprocessing import LabelEncoder\nlabelencoder_x = LabelEncoder()\nx.iloc[:, 1] = labelencoder_x.fit_transform(x.iloc[:, 1]) #applying on Geography\n\n#apply encoder on Gender as well\nlabelencoder_x_2 = LabelEncoder()\nx.iloc[:, 2] = labelencoder_x_2.fit_transform(x.iloc[:, 2]) #applying on Gender\n\nfrom keras.utils import to_categorical\nencoded = pd.DataFrame(to_categorical(x.iloc[:, 1]))\n#no need to encode Gender, as there are only two categories\n\nx = pd.concat([encoded, x], axis = 1)\n\n#Dropping the existing \"geography\" category, and one of the onehotcoded columns.\n\nx = x.drop(['Geography', 0], axis = 1)\n\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size = 0.2, random_state = 0)\n\n# ************* Feature selection is not necessary in xgboost, as it is decision-tree algorithm.","96fada6e":"#build the model\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier()\nclassifier.fit(X_train, Y_train)","9962f68d":"#predict the results\ny_pred = classifier.predict(X_test)\ny_pred[:5]","ce5773f2":"#confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, y_pred)\ncm","1eb237bd":"#K-Fold cross validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = Y_train, cv = 10)\nprint(accuracies.mean())\nprint(accuracies.std())","5ece9b27":"Using the mean of all the accuracies is a better estimation of the overall performance of the mode. \n\n# 2. Grid Search\nThis is used to effectively figure out good combination of hyperparameters. We need to examine each model's hyperparameters, and include them inside a dictionary of parameters we want to test. Using this, let's figure out what model is best for this specific dataset, and what parameters should be used!\n\n### Note: We can simply put the Grid Search section after K-Fold CV section.","af8aaa0f":"### Install XGBoost\nTo install XGBoost, [please refer to this official guide](https:\/\/xgboost.readthedocs.io\/en\/latest\/build.html). But on kaggle kernel, xgboost is already installed!\n\n### Prepare data and Build the XGBoost model\n#### Data overview\n","52517e8c":"I want to note that this only scratched the surface of XGBoost. To find out more about this powerful tool, such as learning about parameters, please refer to the official document (The link is above. Just navigate to the homepage!) \n\n----------------\nThank you for reading this kernel. If you found this kernel useful, I would really appreciate if you upvote it or leave a short comment below.","b0e3b33f":"A bank wants to predict if current customers will exit or stay in the bank based on many characteristics of each customer. We need to create a machine learning model to predict this as accurately as possible.\n\n### Build the model","38579413":"# 3. XGBoost\nThe most famous algorithm to boost execution speed on large datasets. ","0f2986ad":"Created by: Sangwook Cheon\n\nDate: Dec 31, 2018\n\nThis is step-by-step guide to Model Selection and Boosting, which I created for reference. I added some useful notes along the way to clarify things. This notebook's content is from A-Z Datascience course, and I hope this will be useful to those who want to review materials covered, or anyone who wants to learn the basics of Model Selection and Boosting.\n\n## Content:\n### 1. K-Fold Cross Validation\n### 2. Grid Search\n### 3. XGBoost\n--------\nThese techniques above are used to improve model performance. \n\n# 1. K-Fold Cross Validation\nThis technique is useful to evaluate bias and variance more accurately. It splits the training set into k groups, and in each iteration, the algorithm chooses different test fold (individual section) for testing. This allows every part of the training set to be used for testing.\n\n### Dataset overview","61027ff7":"**Now Let's apply K-Fold Cross Validation**","e09c3254":"Using this fictional dataset, the company wants to know whether a customer will buy its product based on age and estimated salary.\n\nI will add the Cross Validation part to a standard Kernel SVM model."}}