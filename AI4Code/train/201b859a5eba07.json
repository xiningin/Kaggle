{"cell_type":{"996c1d1c":"code","2661c1a0":"code","1730177d":"code","fe0ca5d3":"code","a2005019":"code","a1cacfa4":"code","1bf9d6af":"code","d8d2b91f":"code","88fcf117":"code","b17eebb0":"code","2c3b729c":"code","b970e04c":"code","04be0665":"code","ee85790c":"code","09ff86db":"code","316aee0a":"markdown","da9c558d":"markdown","f4af824e":"markdown","a0acb162":"markdown","e8ff1ae6":"markdown","2a4d8bf7":"markdown","00e28d75":"markdown","fbd56a94":"markdown","e75904ba":"markdown","2ac4ccb1":"markdown","3b6f4346":"markdown","9868fdda":"markdown","5d14a6c3":"markdown","cf564542":"markdown","a8a982a8":"markdown","a5334f51":"markdown"},"source":{"996c1d1c":"import pandas as pd\n\ntrain = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/train.csv', index_col='id')\ntest = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/test.csv', index_col='id')\nsubmission = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv', index_col='id')","2661c1a0":"# Combine training data with test data\nall_data = pd.concat([train, test], ignore_index=True)\nall_data = all_data.drop('target', axis=1) # Drop target value","1730177d":"all_data['bin_3'] = all_data['bin_3'].map({'F':0, 'T':1})\nall_data['bin_4'] = all_data['bin_4'].map({'N':0, 'Y':1})","fe0ca5d3":"ord1dict = {'Novice':0, 'Contributor':1, \n            'Expert':2, 'Master':3, 'Grandmaster':4}\nord2dict = {'Freezing':0, 'Cold':1, 'Warm':2, \n            'Hot':3, 'Boiling Hot':4, 'Lava Hot':5}\n\nall_data['ord_1'] = all_data['ord_1'].map(ord1dict)\nall_data['ord_2'] = all_data['ord_2'].map(ord2dict)","a2005019":"from sklearn.preprocessing import OrdinalEncoder\n\nord_345 = ['ord_3', 'ord_4', 'ord_5']\n\nord_encoder = OrdinalEncoder() # Create OrdinalEncoder object\n# Apply ordinal encoding\nall_data[ord_345] = ord_encoder.fit_transform(all_data[ord_345])\n\n# Print encoding order by feature\nfor col, categories in zip(ord_345, ord_encoder.categories_):\n    print(col)\n    print(categories)","a1cacfa4":"all_data['nom_5'] = all_data['nom_5'].str[3:]\nall_data['nom_6'] = all_data['nom_6'].str[3:]\nall_data['nom_7'] = all_data['nom_7'].str[3:]\nall_data['nom_8'] = all_data['nom_8'].str[3:]\nall_data['nom_9'] = all_data['nom_9'].str[3:]","1bf9d6af":"nom_cols = ['nom_' + str(i) for i in range(10)] # Nominal features","d8d2b91f":"from sklearn.preprocessing import OneHotEncoder\n\nnom_onehot_encoder = OneHotEncoder(drop='first') # Create OneHotEncoder object\n# Apply one-hot encoding\nencoded_nom_matrix = nom_onehot_encoder.fit_transform(all_data[nom_cols])\n\nall_data = all_data.drop(nom_cols, axis=1) # Drop original nominal features\n\nencoded_nom_matrix","88fcf117":"date_cols  = ['day', 'month'] # Date features\n\ndate_onehot_encoder = OneHotEncoder() # Create OneHotEncoder object\n# Apply one-hot encoding\nencoded_date_matrix = date_onehot_encoder.fit_transform(all_data[date_cols])\n\nall_data = all_data.drop(date_cols, axis=1) # Drop original date features\n\nencoded_date_matrix","b17eebb0":"from sklearn.preprocessing import MinMaxScaler\n\nord_cols = ['ord_' + str(i) for i in range(6)] # ordinal features\n# Min-max normalization\nall_data[ord_cols] = MinMaxScaler().fit_transform(all_data[ord_cols])","2c3b729c":"import scipy\n\n# aggregate encoded and feature scaled data \nall_data_sprs = scipy.sparse.hstack([scipy.sparse.csr_matrix(all_data),\n                                     encoded_nom_matrix,\n                                     encoded_date_matrix],\n                                    format='csr')","b970e04c":"all_data_sprs","04be0665":"num_train = train.shape[0] # Number of train data\n\n# Divide train data and test data\nX_train = all_data_sprs[:num_train]\nX_test = all_data_sprs[num_train:]\n\ny_train = train['target']","ee85790c":"from sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(C=0.125, solver='lbfgs', max_iter=800, verbose=0, n_jobs=-1)\nclf.fit(X_train, y_train)","09ff86db":"y_preds = clf.predict_proba(X_test)[:,1]\n\nsubmission['target'] = y_preds\nsubmission.to_csv('submission.csv')","316aee0a":"### Date Feature Encoding","da9c558d":"### Aggregate encoded and feature scaled data","f4af824e":"### Apply scaling to ordinal features","a0acb162":"#### I applied feature encoding, feature scaling to rank 1st place on the private leaderboard","e8ff1ae6":"### Binary Feature Encoding","2a4d8bf7":"### Ordinal Feature Encoding","00e28d75":"# Categorical Feature Encoding Challenge Top 1 Solution","fbd56a94":"## Feature Encoding","e75904ba":"### Nominal Feature Encoding","2ac4ccb1":"## This is a simple modeling notebook using Logistic Regression. This model reaches the top 1. If you think it's useful, please upvote \ud83d\ude42\n## I also shared [basic EDA notebook for everyone](https:\/\/www.kaggle.com\/werooring\/powerful-and-simple-eda-for-everyone)","3b6f4346":"- [Competition Link](https:\/\/www.kaggle.com\/c\/cat-in-the-dat\/)\n- [Modeling Notebook Reference Link](https:\/\/www.kaggle.com\/dkomyagin\/cat-in-the-dat-0-80285-private-lb-solution)","9868fdda":"### Prediction and Submission","5d14a6c3":"## Model Training\/Submission","cf564542":"### Divide train data and test data","a8a982a8":"### Model Training","a5334f51":"## Feature Scaling"}}