{"cell_type":{"38c08a6a":"code","48bd6172":"code","378ee23e":"code","15ef27a2":"code","4f92aca4":"code","a5de1571":"code","95610a0f":"code","3305df30":"code","27c22996":"code","f6c50e45":"code","7f2155c3":"code","3d3d7024":"code","e83e4f48":"code","befef653":"code","40b9c537":"code","f884680a":"code","f4b304ad":"code","04f0b209":"code","f1ac5ea9":"code","5ee41a55":"code","67157215":"code","d2638940":"markdown","3e2a4de9":"markdown","afe6e468":"markdown","ffbd1d9b":"markdown","11b622cd":"markdown"},"source":{"38c08a6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","48bd6172":"import matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import recall_score\nfrom sklearn import metrics\nimport seaborn as sns\ndef printreport(exp, pred):\n    print(classification_report(exp, pred))\n    print(\"recall score\")\n    print(recall_score(exp,pred,average = 'macro'))\n    ","378ee23e":"gr = pd.read_csv('..\/input\/greedata\/newd.csv')\nx = gr.iloc[:,0:12].values \ny = gr.label.values","15ef27a2":"from sklearn.model_selection import train_test_split\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.3, random_state=0, stratify=y)","4f92aca4":"scaler = StandardScaler()\nx  = scaler.fit_transform(x_train)","a5de1571":"import torch\ninput = torch.FloatTensor(x)\nprint(input)\nlabel = torch.LongTensor(y_train)\nprint(label)","95610a0f":"import torch.nn.functional as Fun\n# defeine BP neural network\nclass Net(torch.nn.Module):\n    def __init__(self, n_feature, n_hidden, n_output):\n        super(Net, self).__init__()\n        self.hidden = torch.nn.Linear(n_feature, n_hidden)\n        self.out = torch.nn.Linear(n_hidden, n_output)\n        \n    def forward(self,x):\n        x = Fun.relu(self.hidden(x))\n        x = self.out(x)\n        return x","3305df30":"net = Net(n_feature=12, n_hidden=25, n_output=5)\noptimizer = torch.optim.SGD(net.parameters(), lr=0.05)\n# SGD: random gradient decend\nloss_func = torch.nn.CrossEntropyLoss()\n# define loss function","27c22996":"for i in range(1000):\n    out = net(input)\n\n    loss = loss_func(out, label)\n    optimizer.zero_grad()\n    # initialize\n    loss.backward()\n    optimizer.step()","f6c50e45":"x  = scaler.fit_transform(x_test)\ninput = torch.FloatTensor(x)\n\nlabel = torch.LongTensor(y_test)","7f2155c3":"out = net(input)\n\nprediction = torch.max(out, 1)[1]\npred_y = prediction.numpy()\ntarget_y = label.data.numpy()\n","3d3d7024":"s = accuracy_score(target_y, pred_y)\nprint('accury')\nprint(s)\n\ncm = confusion_matrix(target_y, pred_y)\nprintreport(target_y, pred_y)\n\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","e83e4f48":"import numpy as np\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom keras.utils import to_categorical\nimport torch.nn.functional as F\n","befef653":"gr = pd.read_csv('..\/input\/greedata\/newd.csv')\nx = gr.iloc[:,0:12].values \ny = gr.label.values","40b9c537":"class Model(nn.Module):\n    def __init__(self, input_dim):\n        super(Model, self).__init__()\n        self.layer1 = nn.Linear(input_dim,20)\n        self.layer2 = nn.Linear(20, 15)\n        self.layer3 = nn.Linear(15, 10)\n        self.layer4 = nn.Linear(10, 5)\n \n\n    def forward(self, x):\n        x = F.relu(self.layer1(x))\n        x = F.relu(self.layer2(x))\n        x = F.relu(self.layer3(x))\n        x = F.softmax(self.layer4(x)) # To check with the loss function\n        return x\n    ","f884680a":"model = Model(x.shape[1])\noptimizer = torch.optim.Adam(model.parameters(), lr=0.05)\nloss_fn = nn.CrossEntropyLoss()\nepochs = 100\n\ndef print_(loss):\n    print (\"The loss calculated: \", loss)","f4b304ad":"x","04f0b209":"model","f1ac5ea9":"features_train,features_test, labels_train, labels_test = train_test_split(x, y, random_state=42, shuffle=True)\n# Not using dataloader\nx_train, y_train = Variable(torch.from_numpy(features_train)).float(), Variable(torch.from_numpy(labels_train)).long()\nprint(y_train)\n\nfor epoch in range(1, epochs+1):\n    print (\"Epoch #\",epoch)\n    y_pred = model(x_train)\n    print(y_pred)\n    loss = loss_fn(y_pred, y_train)\n    print_(loss.item())\n    # Zero gradients\n    optimizer.zero_grad()\n    loss.backward() # Gradients\n    optimizer.step() # Update","5ee41a55":"# x_test  = scaler.fit_transform(x_test)\nx_test = Variable(torch.from_numpy(features_test)).float()\npred = model(x_test)\npred = pred.detach().numpy()","67157215":"print (\"The accuracy is\", accuracy_score(labels_test, np.argmax(pred, axis=1)))","d2638940":"## Back Propagation","3e2a4de9":"# Multi_layer Pytorch","afe6e468":"# Split Data","ffbd1d9b":"# Pytorch","11b622cd":"# Standard Scaler"}}