{"cell_type":{"863582c7":"code","c01262fa":"code","253c00f4":"code","e2eb3384":"code","8331c339":"code","e23b7b32":"code","dd5b67c1":"code","361f2ffb":"code","42af6583":"code","fab384ef":"code","719997b4":"code","54d4429c":"code","dc7b0776":"code","52d51cf7":"code","d70c371f":"code","06a196f4":"code","37129ed3":"code","c2e2493b":"code","b7c26f77":"code","e860cfee":"code","dd467210":"markdown","08cc1d99":"markdown","c33e09d0":"markdown","a8d8c38a":"markdown","aeea8b1c":"markdown","5bfcf39f":"markdown","d69a411c":"markdown","8f8c2a79":"markdown","dbfdfeea":"markdown","1c72f04c":"markdown"},"source":{"863582c7":"import os\nprint(os.listdir(\"..\/input\"))\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline","c01262fa":"df = pd.read_csv(\"..\/input\/glass.csv\")\ndf.head()","253c00f4":"# Display class values\ndf.Type.value_counts().sort_index()","e2eb3384":"# glass_type 1, 2, 3 are window glass\n# glass_type 5, 6, 7 are non-window glass\ndf['household'] = df.Type.map({1:0, 2:0, 3:0, 5:1, 6:1, 7:1})\ndf.head()","8331c339":"plt.scatter(df.Al, df.household)\nplt.xlabel('Al')\nplt.ylabel('household')","e23b7b32":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df[['Al']],df.household,train_size=0.7)","dd5b67c1":"from sklearn.linear_model import LinearRegression\n# Fit the model\nlinear_model = LinearRegression()\nlinear_model = linear_model.fit(X_train, y_train)\n# Create a seperate table to store predictions\nglass_df = X_train[['Al']]\nglass_df['household_actual'] = y_train\n\n# Predict with Linear Regression\nglass_df['household_pred_linear'] = linear_model.predict(X_train)\n","361f2ffb":"# Examine the first 15 linear regression predictions\nlinear_model.predict(X_train)[0:15]","42af6583":"# Plot Linear Regression Line\nsns.regplot(x='Al', y='household_actual', data=glass_df, logistic=False)","fab384ef":"from sklearn.linear_model import LogisticRegression\n# Fit logistic regression model\nlogistic_model = LogisticRegression(class_weight='balanced')\nlogistic_model = logistic_model.fit(X_train, y_train)","719997b4":"# Make class label predictions\nlogistic_model.predict(X_train)[:15]","54d4429c":"# Make class probability predictions\nlogistic_model.predict_proba(X_train)[:15]","dc7b0776":"# Predict with Logistic Regression\nglass_df['household_pred_log'] = logistic_model.predict(X_train)\n\n# Predict Probability with Logistic Regression\nglass_df['household_pred_prob_log'] = logistic_model.predict_proba(X_train)[:,1]","52d51cf7":"# Plot logistic regression line \nsns.regplot(x='Al', y='household_actual', data=glass_df, logistic=True, color='b')","d70c371f":"# Examine the table\nglass_df.head(10)","06a196f4":"# Observe class predictions on test set\nlogistic_model.predict(X_test)\n# Store predictions\npredicted = logistic_model.predict(X_test)","37129ed3":"from sklearn import metrics\n# Print Confusion Matrix\nprint (metrics.confusion_matrix(y_test, predicted))","c2e2493b":"print (metrics.classification_report(y_test, predicted))","b7c26f77":"# Let's use the statsmodel library \nimport statsmodels.api as sm\n\n# Define independent variables\niv = ['RI','Na','Mg','Al','Si','K','Ca','Ba','Fe']\n\n# Fit the logistic regression function\nlogReg = sm.Logit(df.household,df[iv])\nanswer = logReg.fit()","e860cfee":"# Display the parameter coefficients \nnp.exp(answer.params)","dd467210":"Notice there are some numbers below 0 and above 1 (NOT GOOD)","08cc1d99":"**Predicting with Linear Regression**","c33e09d0":"**Model Evaluation**\n","a8d8c38a":"**Convert the target feature into a binary feature**","aeea8b1c":"**Logistic Regression**","5bfcf39f":"**Predict Class Probabilities & Class Predictions**","d69a411c":" **Plot Aluminum (al) vs household**","8f8c2a79":"Linear regression is making predictions outside the range of 0 and 1","dbfdfeea":"**Create Train\/Test Split**","1c72f04c":"**Compare Predictions**\n"}}