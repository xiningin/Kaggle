{"cell_type":{"0fb97692":"code","5ecadba3":"code","42858c44":"code","e12bfd34":"code","915e311d":"code","b0a6cafb":"code","95f1f374":"code","38094aa0":"code","636c4666":"code","413069b3":"code","5e706c46":"code","42044814":"code","b8388a35":"code","94413f07":"code","a67b0c2a":"code","186bb1f8":"code","107f8e44":"code","610a9cc0":"code","cd72d773":"code","bd526429":"code","a01a76f7":"code","f3ae69d0":"code","70c70a4f":"code","0dc17741":"code","a9415ade":"code","b165df81":"code","f71a15f1":"code","9c097188":"code","3882d59a":"code","428de589":"code","e95fdd18":"code","e138e247":"markdown","2da3bc32":"markdown","945018f9":"markdown","200a2f8a":"markdown","dead5511":"markdown","8a8a056e":"markdown","a6911cc1":"markdown","f5ae0eef":"markdown","d2789db5":"markdown","cc514640":"markdown","b3d0a3ce":"markdown","5a2e96a6":"markdown","841c1fab":"markdown","59745cbc":"markdown","a475773b":"markdown","eb5b9d52":"markdown","5c86f708":"markdown","33b90da0":"markdown","ce0497a9":"markdown","5e54e0f9":"markdown"},"source":{"0fb97692":"# Import libraries and create their respective shortcuts\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.feature_selection import mutual_info_regression\n\n# Importing the relevant dataset for the abalone ages estimation\n\ndataset = pd.read_csv('..\/input\/abalone-dataset\/abalone.csv')","5ecadba3":"dataset.head()","42858c44":"dataset.shape","e12bfd34":"dataset.rename(columns = {\"Sex\":\"sex\", \"Length\":\"length\", \"Diameter\":\"diameter\",\"Height\":\"height\", \"Whole weight\":\"whole_weight\",\n                   \"Shucked weight\":\"shucked_weight\", \"Viscera weight\":\"viscera_weight\",\n                   \"Shell weight\":\"shell_weight\", \"Rings\":\"rings\"}, inplace = True)","915e311d":"dataset.describe()","b0a6cafb":"# Notice the minimum value for height is zero. These rows will be excluded.\n\ndataset[dataset['height'] == 0]","95f1f374":"# Remove rows 1257 and 3996\ndataset.drop(index=[1257,3996], inplace = True)","38094aa0":"# Check if rows have been removed (i.e. array shape = 4175, 9)\ndataset.shape","636c4666":"# Create a matrix of features (x; independant variables) and a dependant variable vector (y)\nX = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","413069b3":"from statsmodels.stats.outliers_influence import variance_inflation_factor\ndf = pd.read_csv('..\/input\/abalone-dataset\/abalone.csv')\ndf = df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]\nX_variables = df[['Length','Diameter','Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight']]\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X_variables.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X_variables.values, i) for i in range(len(X_variables.columns))]","5e706c46":"print(vif_data)","42044814":"def add_features(dataset):\n    \n    dataset['eadible_weight'] = dataset['shucked_weight'] - dataset['viscera_weight']\n    dataset['surface_area'] = 3.1415 * (dataset['length'] \/ 2)\n    dataset['internal_weight'] = dataset['whole_weight'] - dataset['shell_weight']\n    dataset['volume'] = dataset['height'] * dataset['diameter'] * dataset['length']\n    return dataset\n\ndataset = add_features(dataset)\n    ","b8388a35":"X = dataset.copy()\ny = X.pop('rings')","94413f07":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers = [('encoder', OneHotEncoder(drop = 'first'), [0])], remainder= 'passthrough')\nX = np.array(ct.fit_transform(X))","a67b0c2a":"print(X)","186bb1f8":"print(y)","107f8e44":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","610a9cc0":"from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(random_state = 42)\nfrom pprint import pprint\n# Look at parameters used by our current forest\nprint('Parameters currently in use:\\n')\npprint(regressor.get_params())","cd72d773":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 1000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","bd526429":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nregressor = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nregressor_random = RandomizedSearchCV(estimator = regressor,\n                                      param_distributions = random_grid,\n                                      n_iter = 20,\n                                      cv = 3,\n                                      verbose=2,\n                                      random_state=42,)\n# Fit the random search model\nregressor_random.fit(X_train, y_train)","a01a76f7":"regressor_random.best_params_","f3ae69d0":"def evaluate(model, X_test, y_test):\n    predictions = model.predict(X_test)\n    errors = abs(predictions - y_test)\n    mape = 100 * np.mean(errors \/ y_test)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy\nbase_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\nbase_model.fit(X_train, y_train)\nbase_accuracy = evaluate(base_model, X_test, y_test)","70c70a4f":"best_random = regressor_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_test, y_test)","0dc17741":"print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) \/ base_accuracy))","a9415ade":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [40, 50, 60, 70],\n    'max_features': [1,2],\n    'min_samples_leaf': [2, 3, 4],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [150, 200, 250]\n}\n# Create a based model\nregressor = RandomForestRegressor()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = regressor, param_grid = param_grid, \n                          cv = 3, n_jobs = 1, verbose = 2)","b165df81":"# Fit the grid search to the data\ngrid_search.fit(X_train, y_train)\ngrid_search.best_params_","f71a15f1":"best_grid = grid_search.best_estimator_\ngrid_accuracy = evaluate(best_grid, X_test, y_test)","9c097188":"print('Improvement of {:0.2f}%.'.format( 100 * (grid_accuracy - base_accuracy) \/ base_accuracy))","3882d59a":"y_pred = best_grid.predict(X_test)","428de589":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred)","e95fdd18":"plt.figure(figsize=(10,10))\nplt.title('Predictions accuracy')\nplt.scatter(y_test, y_pred, c='crimson')\n\n\np1 = max(max(y_pred), max(y_test))\np2 = min(min(y_pred), min(y_test))\nplt.plot([p1, p2], [p1, p2], 'b-')\nplt.xlabel('True Values', fontsize=15)\nplt.ylabel('Predictions', fontsize=15)\nplt.axis('equal')\nplt.show()","e138e247":"**Grid search** with **cross validation** so that hyperparameters can be narrowed down.","2da3bc32":"**Predictions accuracy** ","945018f9":"**Checking for multicollinearity** If there is multicollinearity models such as the multilinear regression would not be suited as it assumes non-multicollinearity. Multicollinearity exists whenever two or more of the predictors in a regression model are moderately or highly correlated and affects the precision of a regression model's prediction. To test for this we will use the variance inflation factor  (VIF), high values of VIF between independent variables represents high correlation.","200a2f8a":"**Dataset analysis** Chech the data for any possible problems which may affect the effectiveness of the model. ","dead5511":"**Data Preprocessing** Import the libraries which are essential in any analysis. Futher, check the data and process it. The abalone's age is equal to the number of rings plus 1.5, another issue in the dataset is the presence of zeros, and possible multicollinearity.","8a8a056e":"**Rename the variables into convetional form** ","a6911cc1":"**Model Selection** As we can see above their is a high amount of multicollinearity with respects to length, diameter, whole weight, and shucked weight. For this reason it would be best to use a regression which can handle this multicollinearity. Therefore, I shall use the **Random Forest Regression** for my model. This way I dont not have to remove these columns from the dataset. Also, it should be noted that this regression doesn't require feature scaling. I'll run 400 different trees due to the 4175 rows of data (approx. 10% of the row amount). You can run a lot more tress and this will not cause overfitting however, it becomes computationally exhausting.","f5ae0eef":"**Visualization** of the predications against the actual values.","d2789db5":"Implement **random search training**.","cc514640":"**Feature engineering** Features will be added in so that certain characteristics in combination can have more effect when training the model. ","b3d0a3ce":"Create a **random hyperparameter grid**.","5a2e96a6":"**Dataset discription** This will be useful to identify problems within the dataset, however only represents the numerical data. Possible problems arise where physical measurements equal zero or negative values. Where these values exist they will be removed from the data set, as the dataset is rather large it should not effect the sample. However, another possible solution would be to add the averages of these variables inplace of the zeros, but this is usually reserved for small datasets.","841c1fab":"**Training the Random Forest Regression model on the whole datase**","59745cbc":"This model provides a week prediction accuracy of 54.2% and is therefore, not suitable for making predications on this problem.","a475773b":"**Problem Description**\nPredicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope -- a laborious task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem. However, for this problem we shall assume that the abalone's physical measurements are sufficient to provide an accurate age prediction. ","eb5b9d52":"Firstly, we will have to check the parameters in use and then we shall train the model, afterwards some parameter tuning will be implemented.","5c86f708":"As expected a strong correlation is observed in length, diameter, whole weight, and shucked weight.","33b90da0":"**Splitting the dataset into a training and test set**","ce0497a9":"What were the **best parameters**?","5e54e0f9":"**Encoding categorical data** The first coloumn of the independent variables contains an categorical data. Sex is categorised by M - males and F - female. This data will be changed into binary vectors. The dummy variable trap will be dealt with by dropping the first column"}}