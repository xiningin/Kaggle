{"cell_type":{"803fcf43":"code","1fdf10ee":"code","99805147":"code","43c97a4c":"code","69fd4bda":"code","bf06b895":"code","3fe05716":"code","423ec8db":"code","ecd9d878":"code","c5bcb06b":"code","e034e4b1":"code","daa5f388":"code","4ec31d24":"code","b64743b9":"code","97b7bd6b":"code","b8bdc248":"code","00a78fff":"code","8dbc6091":"code","8b5b5eeb":"markdown","8dd6ce5e":"markdown"},"source":{"803fcf43":"import numpy as np\nimport tensorflow as tf\nimport keras\n\nfrom keras.models import Sequential,load_model\nfrom keras.layers import Dense, Conv2D ,LSTM, MaxPooling2D , Flatten , Dropout , BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping,ReduceLROnPlateau\n\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (15,5)\nimport seaborn as sns\nimport pandas as pd\nimport os\nimport random\nimport time\nimport os\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nfrom scipy import signal\nfrom scipy.fft import fftshift\n","1fdf10ee":"ACTIONS = [\"kiri\", \"maju\",\"idle\",\"kanan\"]\nreshape = (-1,8, 60)","99805147":"def create_data(starting_dir=\"..\/input\/eeg8chanel\/data8\"):\n    training_data = {}\n    for action in ACTIONS:\n        if action not in training_data:\n            training_data[action] = []\n        data_dir = os.path.join(starting_dir,action)\n        for item in os.listdir(data_dir):\n            data = np.load(os.path.join(data_dir, item))\n            for item in data:\n                training_data[action].append(item)\n\n    lengths = [len(training_data[action]) for action in ACTIONS]\n    print(lengths)\n\n    for action in ACTIONS:\n        np.random.shuffle(training_data[action])  \n        training_data[action] = training_data[action][:min(lengths)]\n\n    lengths = [len(training_data[action]) for action in ACTIONS]\n    print(lengths)\n    combined_data = []\n    for action in ACTIONS:\n        for data in training_data[action]:\n            if action == \"kiri\":\n                combined_data.append([data, [1, 0, 0,0]])\n            elif action == \"maju\":\n                combined_data.append([data, [0, 1, 0, 0]])\n            elif action == \"idle\":\n                combined_data.append([data, [0, 0, 1, 0]])\n            elif action == \"kanan\":\n                combined_data.append([data, [0, 0, 0, 1]])\n\n    np.random.shuffle(combined_data)\n    print(\"length:\",len(combined_data))\n    return combined_data\n","43c97a4c":"print(\"creating training data\")\ntraindata = create_data(starting_dir=\"..\/input\/eeg8chanel\/data8\")\ntrain_X = []\ntrain_y = []\n\nfor X, y in traindata:\n    train_X.append(X)\n    train_y.append(y)\n\ntrain_X = np.array(train_X).reshape(reshape)\ntrain_y = np.array(train_y)\n","69fd4bda":"\nidle = np.load(\"..\/input\/eeg8chanel\/data8\/idle\/1608706768.npy\")\nkanan = np.load(\"..\/input\/eeg8chanel\/data8\/kanan\/1608707012.npy\")\nkiri = np.load(\"..\/input\/eeg8chanel\/data8\/kiri\/1608707050.npy\")\nmaju = np.load(\"..\/input\/eeg8chanel\/data8\/maju\/1608706976.npy\")\n\nkelas1=idle[0][16]\nkelas2=maju[0][16]\nkelas3=kanan[0][16]\nkelas4=kiri[0][16]\n\nkelas1a=idle[175]\nkelas2a=maju[175]\nkelas3a=kanan[175]\nkelas4a=kiri[175]\n\n\n\nf1, t1,Sxx= signal.spectrogram(kelas1,fs=60, window=('tukey', 0.25),\n                             nperseg=2,\n                             noverlap=1,\n                             nfft=None, \n                             detrend='constant',\n                             return_onesided=True, \n                             scaling='density', \n                             axis=-1,\n                             mode='psd',\n                            )\ndbs1 = 10*np.log10(Sxx)\nf2, t2,Sxx= signal.spectrogram(kelas2,fs=120, window=('tukey', 0.25),\n                             nperseg=2,\n                             noverlap=1,\n                             nfft=None, \n                             detrend='constant',\n                             return_onesided=True, \n                             scaling='density', \n                             axis=-1,\n                             mode='psd',\n                            )\ndbs2 = 10*np.log10(Sxx)\nf3, t3,Sxx= signal.spectrogram(kelas3,fs=120, window=('tukey', 0.25),\n                             nperseg=2,\n                             noverlap=1,\n                             nfft=None, \n                             detrend='constant',\n                             return_onesided=True, \n                             scaling='density', \n                             axis=-1,\n                             mode='psd'\n                            )\ndbs3 = 10*np.log10(Sxx)\nf4, t4,Sxx= signal.spectrogram(kelas4,fs=120, window=('tukey', 0.25),\n                             nperseg=2,\n                             noverlap=1,\n                             nfft=None, \n                             detrend='constant',\n                             return_onesided=True, \n                             scaling='density', \n                             axis=-1,\n                             mode='psd'\n                            )  \ndbs4 = 10*np.log10(Sxx)\n\n","bf06b895":"fig, axs = plt.subplots(nrows=4, ncols=4,figsize=(20,10))\naxs = axs.flatten()\n\naxs[0].set_title(\"siggle\")\naxs[0].plot(kelas1)\n\naxs[1].set_title(\"global\")\naxs[1].plot(kelas1a)\n\naxs[2].set_title(\"Spectogram\")\naxs[2].pcolormesh(t1, f1,dbs1,  \n                  shading='gouraud',\n                  cmap='nipy_spectral')\n\ncwt1 = signal.cwt(kelas1,signal.ricker,widths=np.arange(1,50))\ncwt2 = signal.cwt(kelas2,signal.ricker,widths=np.arange(1,50))\ncwt3 = signal.cwt(kelas3,signal.ricker,widths=np.arange(1,50))\ncwt4 = signal.cwt(kelas4,signal.ricker,widths=np.arange(1,50))\naxs[3].set_title(\"wavelet\")\naxs[3].imshow(cwt1,extent=[0,16,0,60],cmap='jet',aspect='auto',vmax=abs(cwt1).max(),vmin=-abs(cwt1).max())\n\naxs[4].plot(kelas2)\naxs[5].plot(kelas2a)\naxs[6].pcolormesh(t2, f2,dbs2,  \n                  shading='gouraud',\n                  cmap='nipy_spectral')\naxs[7].imshow(cwt2,extent=[0,16,0,60],cmap='jet',aspect='auto',vmax=abs(cwt2).max(),vmin=-abs(cwt2).max())\n\naxs[8].plot(kelas3)\naxs[9].plot(kelas3a)\naxs[10].pcolormesh(t3, f3,dbs3,  \n                  shading='gouraud',\n                  cmap='nipy_spectral')\naxs[11].imshow(cwt3,extent=[0,16,0,60],cmap='jet',aspect='auto',vmax=abs(cwt3).max(),vmin=-abs(cwt3).max())\n\naxs[12].plot(kelas4)\naxs[13].plot(kelas4a)\naxs[14].pcolormesh(t4, f4,dbs4,  \n                  shading='gouraud',\n                  cmap='viridis')\naxs[15].imshow(cwt4,extent=[0,16,0,60],cmap='jet',aspect='auto',vmax=abs(cwt4).max(),vmin=-abs(cwt4).max())\nplt.show()\n","3fe05716":"ye=pd.DataFrame(train_y)\n\nye.columns=[\"kiri\", \"maju\",\"idle\",\"kanan\"]\ncategories = list(ye.columns.values)\nsns.set(font_scale = 1)\nplt.figure(figsize=(10,8))\nax= sns.barplot(categories, ye.iloc[:,0:].sum().values)\nplt.title(\"Number of samples labeled as active (1) out of {0} length data\".format((ye.shape[0])),fontsize=20)\n\nplt.ylabel('Number of events', fontsize=18)\nplt.xlabel('Event Type ', fontsize=12)\n#adding the text labels\nrects = ax.patches\nlabels = ye.iloc[:,0:].sum().values\n\nfor rect, label in zip(rects, labels):\n    height = rect.get_height()\n    ax.text(rect.get_x() + rect.get_width()\/2, height + 5, label, ha='center', va='bottom', fontsize=10)\nplt.show()","423ec8db":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 5, verbose=1,factor=0.5, min_lr=0.0001)","ecd9d878":"def lstm_model():\n    model = Sequential()\n\n    model.add(LSTM(64,input_shape = (train_X.shape[1:]), return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n\n    model.add(LSTM(64,input_shape = (train_X.shape[1:]), return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n\n    model.add(LSTM(64,input_shape = (train_X.shape[1:]), return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n \n    model.add(LSTM(64))\n    model.add(Flatten())\n    model.add(Dense(32, activation = \"relu\"))\n    model.add(BatchNormalization())\n    model.add(Dense(4, activation='softmax'))\n   \n  #adam = Adam(lr = 0.001)\n    model.compile(optimizer =\"adam\", loss = \"binary_crossentropy\", \n                  metrics = ['accuracy',tf.keras.metrics.AUC(),\n                             tf.keras.metrics.Precision(),\n                             tf.keras.metrics.PrecisionAtRecall(0.5),\n                             tf.keras.metrics.SpecificityAtSensitivity(0.5),\n                             tf.keras.metrics.SensitivityAtSpecificity(0.5)\n\n                             ])\n\n    return model","c5bcb06b":"model = lstm_model()\nmodel.summary()","e034e4b1":"#set early stopping criteria\npat = 5\n#this is the number of epochs with no improvment after which the training will stop\nearly_stopping = EarlyStopping(monitor='loss', patience=pat, verbose=1)\n\n#define the model checkpoint callback -> this will keep on saving the model as a physical file\nmodel_checkpoint = ModelCheckpoint('subjek1LSTM.h5', verbose=1, save_best_only=True)\n\n#define a function to fit the model\ndef fit_and_evaluate(t_x, val_x, t_y, val_y, EPOCHS=100, BATCH_SIZE=32):\n    model = None\n    model = lstm_model() \n    results = model.fit(t_x, t_y, epochs=EPOCHS, batch_size=BATCH_SIZE, callbacks=[learning_rate_reduction,early_stopping, model_checkpoint], \n              verbose=1, validation_split=0.1)   \n    print(\"Val Score: \", model.evaluate(val_x, val_y))\n    return results\n    ","daa5f388":"n_folds=5\nepochs=100\nbatch_size=32\n\n#save the model history in a list after fitting so that we can plot later\nmodel_history = [] \nfor i in range(n_folds):\n    print(\"Training on Fold: \",i+1)\n    t_x, val_x, t_y, val_y = train_test_split(train_X, train_y, test_size=0.2, \n                                               random_state = np.random.randint(1,1000, 1)[0])\n    model_history.append(fit_and_evaluate(t_x, val_x, t_y, val_y, epochs, batch_size))\n   \n    print(\"=======\"*12, end=\"\\n\\n\\n\")\n","4ec31d24":"\nprint(\"data train x\",t_x.shape)\nprint(\"data train y\",t_y.shape)\n\nprint(\"data tes x\",val_x.shape)\nprint(\"data tes y\",val_y.shape)\n\nmodel = load_model('subjek1LSTM.h5')\nb = model.evaluate(val_x, val_y)\n\nprint('loss',b[0])\nprint('Accuracy',b[1]*100)\nprint('AUC',b[2])\nprint('precision',b[3])\nprint('recall',b[4])\nprint('specificity_at_sensitivity',b[5])\nprint('sensitivity_at_specificity',b[6])","b64743b9":"fig, (ax1, ax2) =  plt.subplots( ncols=2, sharex=True)\nax1.plot(model_history[0].history['accuracy'], label='Training Fold 1 accuration')\nax1.plot(model_history[1].history['accuracy'], label='Training Fold 2 accuration')\nax1.plot(model_history[2].history['accuracy'], label='Training Fold 3 accuration')\nax1.plot(model_history[3].history['accuracy'], label='Training Fold 4 accuration')\nax1.plot(model_history[4].history['accuracy'], label='Training Fold 5 accuration')\nax1.legend()\nax2.plot(model_history[1].history['loss'], label='Training Fold 1 loss')\nax2.plot(model_history[1].history['loss'], label='Training Fold 2 loss')\nax2.plot(model_history[2].history['loss'], label='Training Fold 3 loss')\nax2.plot(model_history[3].history['loss'], label='Training Fold 4 loss ')\nax2.plot(model_history[4].history['loss'], label='Training Fold 5 loss')\nax2.legend()\nplt.show()","97b7bd6b":"\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[0].history['accuracy'], label='Train Accuracy Fold 1', color='black')\nplt1.plot(model_history[0].history['val_accuracy'], label='Val Accuracy Fold 1', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[0].history['loss'], label='Train Loss Fold 1', color='black')\nplt2.plot(model_history[0].history['val_loss'], label='Val Loss Fold 1', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[1].history['accuracy'], label='Train Accuracy Fold 2', color='red')\nplt1.plot(model_history[1].history['val_accuracy'], label='Val Accuracy Fold 2', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[1].history['loss'], label='Train Loss Fold 2', color='red')\nplt2.plot(model_history[1].history['val_loss'], label='Val Loss Fold 2', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[2].history['accuracy'], label='Train Accuracy Fold 3', color='green')\nplt1.plot(model_history[2].history['val_accuracy'], label='Val Accuracy Fold 3', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[2].history['loss'], label='Train Loss Fold 3', color='green')\nplt2.plot(model_history[2].history['val_loss'], label='Val Loss Fold 3', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[3].history['accuracy'], label='Train Accuracy Fold 4', color='blue')\nplt1.plot(model_history[3].history['val_accuracy'], label='Val Accuracy Fold 4', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[3].history['loss'], label='Train Loss Fold 4', color='blue')\nplt2.plot(model_history[3].history['val_loss'], label='Val Loss Fold 4', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()\n\nfig, (plt1, plt2)  =  plt.subplots( ncols=2, sharex=True)\nplt1.plot(model_history[4].history['accuracy'], label='Train Accuracy Fold 5', color='purple')\nplt1.plot(model_history[4].history['val_accuracy'], label='Val Accuracy Fold 5', color='orange', linestyle = \"dashdot\")\nplt1.legend()\nplt2.plot(model_history[4].history['loss'], label='Train Loss Fold 4', color='purple')\nplt2.plot(model_history[4].history['val_loss'], label='Val Loss Fold 4', color='orange', linestyle = \"dashdot\")\nplt2.legend()\nplt.show()","b8bdc248":"\nMODEL_NAME ='subjek1LSTM.h5' \n\n#CLIP = True # if your model was trained with np.clip to clip  values\nCLIP = False\nCLIP_VAL = 8  # if above, what was the value +\/-\n\nmodel = tf.keras.models.load_model(MODEL_NAME)\n\nVALDIR = '..\/input\/eeg8chanel\/data8'\nACTIONS =  [\"idle\",\"maju\",\"kiri\", \"kanan\"]\nPRED_BATCH = 32\n\n\ndef get_val_data(valdir, action, batch_size):\n\n    argmax_dict = {2: 0, 0: 0, 1: 0,3:0}\n    raw_pred_dict = {0: 0, 1: 0, 2: 0,3:0}\n\n    action_dir = os.path.join(valdir, action)\n    for session_file in os.listdir(action_dir):\n        filepath = os.path.join(action_dir,session_file)\n        if CLIP:\n            data = np.clip(np.load(filepath), -CLIP_VAL, CLIP_VAL) \/ CLIP_VAL\n            #print(data)\n        else:\n            data = np.load(filepath) \n        preds = model.predict([data.reshape(reshape)], batch_size=batch_size)\n        \n        for pred in preds:\n            argmax = np.argmax(pred)\n            argmax_dict[argmax] += 1\n            for idx,value in enumerate(pred):\n                raw_pred_dict[idx] += value\n    \n    argmax_pct_dict = {}\n    for i in argmax_dict:\n        total = 0\n        correct = argmax_dict[i]\n        for ii in argmax_dict:\n            total += argmax_dict[ii]\n        argmax_pct_dict[i] = round(correct\/total, 4)\n    return argmax_dict, raw_pred_dict, argmax_pct_dict\n\n\ndef make_conf_mat(none,left,forward, right):\n    action_dict = {\"idle\":none,\"maju\": forward, \"kiri\": left, \"kanan\": right}\n    action_conf_mat = pd.DataFrame(action_dict)\n    actions = [i for i in action_dict]\n\n    fig = plt.figure(figsize=(15, 6))\n    ax = fig.add_subplot(111)\n    ax.matshow(action_conf_mat, cmap=plt.cm.RdYlGn)\n    ax.set_xticklabels([\"\"]+actions)\n    ax.set_yticklabels([\"\"]+actions)\n\n    for idx, i in enumerate(action_dict):\n        for idx2, ii in enumerate(action_dict[i]):\n            ax.text(idx, idx2, f\"{round(float(action_dict[i][ii]),2)}\", va='center', ha='center')\n    # Rotate the tick labels and set their alignment.\n    \n    plt.title(\"Matrik data\")\n    plt.ylabel(\"Predicted Action\")\n    fig.tight_layout()\n    plt.show()\n\n\n\nforward_argmax_dict, forward_raw_pred_dict, forward_argmax_pct_dict = get_val_data(VALDIR, \"maju\", PRED_BATCH)\nnone_argmax_dict, none_raw_pred_dict, none_argmax_pct_dict = get_val_data(VALDIR, \"idle\", PRED_BATCH)\nleft_argmax_dict, left_raw_pred_dict, left_argmax_pct_dict = get_val_data(VALDIR, \"kiri\", PRED_BATCH)\nright_argmax_dict, right_raw_pred_dict, right_argmax_pct_dict = get_val_data(VALDIR, \"kanan\", PRED_BATCH)\nmake_conf_mat(none_argmax_pct_dict,forward_argmax_pct_dict,left_argmax_pct_dict,  right_argmax_pct_dict)\n\n","00a78fff":"x_train,x_test,y_train,y_test=train_test_split(train_X,train_y,test_size=0.2)\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","8dbc6091":"from sklearn.metrics import roc_curve, auc\n\n\ny_score = model.predict(x_test)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfor i in range(4):\n    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n    roc_auc[i] = auc(fpr[i], tpr[i])\n    \n    \n    \nprint(\"roc_auc:\",sum(roc_auc.values())\/4)\nACTIONS =  [\"idle\",\"maju\",\"kiri\", \"kanan\"]\nfor i in range(0,4):\n    plt.figure()\n    fig, ax = plt.subplots(figsize=(10,10))\n    plt.plot(fpr[i], tpr[i],linewidth=3)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([-0.05, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f)' % roc_auc[i])\n    plt.title('%s -ROC curve (area = %0.2f)' % (ACTIONS[i],roc_auc[i]))\n    \n    plt.legend(loc=\"lower right\")\n    plt.show()\n    ","8b5b5eeb":"# LSTM","8dd6ce5e":"# DATA"}}