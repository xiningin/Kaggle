{"cell_type":{"bb49de46":"code","a9319749":"code","952cb038":"code","df294dff":"code","a4bb13ae":"code","a6c0cd3c":"code","d610fc35":"code","3e489f54":"code","287d5abd":"code","0ca302ad":"code","1f40e26c":"code","12126c06":"code","0eb5a0a1":"code","3e26f946":"code","62d8e15a":"code","1b0e1bc9":"code","b58ec225":"code","0def9aa8":"code","2a618d3d":"code","5c9c570d":"code","3cff5d6c":"code","052e1a33":"code","53b3ed6d":"code","c9a8fbb1":"code","54ef4fb0":"code","194387c3":"code","9a0e2fe2":"code","941fc1b9":"code","ab0a359c":"code","e7678702":"code","bc231b08":"code","175e5cac":"code","f801d0f8":"code","ea856c99":"code","07f936e5":"code","369382ac":"code","168da5b3":"code","436948e8":"code","e300aba2":"code","d0a489cc":"code","41af6c57":"code","a32c7935":"code","5af25544":"code","18df9938":"code","7c9aee7a":"code","c54c7efd":"code","489fcdea":"code","337b473c":"code","41345dcd":"code","3bdc08b6":"code","2cbc0221":"code","8bfe9658":"code","5c1df2ea":"code","01a9755b":"code","0e90c530":"code","16a4d628":"code","49497541":"code","e5303097":"code","a80fff26":"code","edf1c941":"code","fe682396":"code","176079f3":"code","b347439a":"code","f80b1ab4":"code","b97e9b9d":"code","0f577b63":"code","c7f947a8":"code","bc82739c":"code","b1141a81":"code","12e4f96f":"code","bbd2ce67":"code","a7ce668b":"code","7c961df9":"code","027ecc98":"code","fd8b4432":"code","70bcca4e":"code","f2a44eee":"code","82aa0f10":"code","2e5aff8d":"code","c642269b":"code","2b307190":"code","c44ab218":"code","98df1f24":"code","07ed6ae2":"code","2ddbb3e2":"markdown","99b6e160":"markdown","07d1512f":"markdown","1d0d1429":"markdown","64f52d29":"markdown","39279e53":"markdown","36c3ef45":"markdown","e6dd364e":"markdown","bd9e220b":"markdown","74142910":"markdown","30b30aa0":"markdown","729bf21d":"markdown","34b0cd7d":"markdown","989211b6":"markdown","93294bdc":"markdown","9ed6aed2":"markdown","60d22e21":"markdown","81fbd95e":"markdown","bc8fcba8":"markdown","0fa967e8":"markdown","70f54ea9":"markdown","b3989e07":"markdown","2a1fbf33":"markdown","b3152bed":"markdown","007c58fb":"markdown","44f976d5":"markdown","c15fd85b":"markdown","cf9d0b4b":"markdown","355ad752":"markdown","b4406ec1":"markdown","e0756312":"markdown","3f327b6e":"markdown","a1706a81":"markdown","2b12ed8b":"markdown","3426c060":"markdown","e5d2e3a8":"markdown","a26143d5":"markdown","eaeec733":"markdown","0ec46de2":"markdown","1293e199":"markdown","cdf481db":"markdown","91afced1":"markdown","f94b11f6":"markdown","7330fc41":"markdown","8b498fae":"markdown","c566bfc5":"markdown","34663a8e":"markdown","9beaff29":"markdown","fdc19243":"markdown","253ad955":"markdown","fbf86df0":"markdown","d81028f1":"markdown","f4b7b78c":"markdown","1974cc6d":"markdown","22cb0ccc":"markdown","5e61d9ee":"markdown","3535e8ae":"markdown","620232a3":"markdown","b13b1cd5":"markdown","efcb1f81":"markdown","9f508ccb":"markdown","8bddb9c5":"markdown","14afd171":"markdown","df54f0f6":"markdown","09c1ccea":"markdown","034341e8":"markdown","160359da":"markdown","1a089e80":"markdown","26f8d0ae":"markdown","7dbbb4c8":"markdown","141ab7d9":"markdown","a5e500b8":"markdown","77801d4e":"markdown","f45f74dc":"markdown","838ac31a":"markdown","0037868e":"markdown","4dad7f57":"markdown","f6a3c7e1":"markdown","ee431b38":"markdown","82315ac6":"markdown","6af87b8f":"markdown","9e6b05f0":"markdown","97a420d3":"markdown","f4e5b0d6":"markdown","4398eb34":"markdown","f42725d0":"markdown","6f39dc4b":"markdown","5d4117b3":"markdown","49a8b372":"markdown","dcdae97f":"markdown","27346106":"markdown","a8ec7ff3":"markdown","b982dd36":"markdown","f19d63e9":"markdown","2243cec1":"markdown","cc5adab0":"markdown","58cfeee4":"markdown","5b83d3ec":"markdown","4b1b7d1b":"markdown","f507bfdd":"markdown","528027f2":"markdown","3f684e17":"markdown","2f6b2783":"markdown","8a9ff93d":"markdown","f53766a4":"markdown","87eee2d8":"markdown","0bbe9bb8":"markdown","ee7b7328":"markdown","cfffdce9":"markdown","a3da8d15":"markdown","748ded3b":"markdown","f8f64428":"markdown","b6b0693c":"markdown","5950468a":"markdown","949244c4":"markdown","85f15a94":"markdown","c296d83c":"markdown","95a974b1":"markdown","0b1cfa42":"markdown","facc1d06":"markdown","c2967899":"markdown","4623d4cc":"markdown","cf4a7feb":"markdown","f588322b":"markdown","691aec8b":"markdown","2079b34c":"markdown","2f98131a":"markdown","6a42f2d4":"markdown","b17e5e62":"markdown","12f89bf7":"markdown","03450e21":"markdown","8e555e80":"markdown","d0dec1e6":"markdown","543796c8":"markdown","179009e9":"markdown","74752850":"markdown","b2b7ba17":"markdown","18d1956e":"markdown","2b18e90d":"markdown","a54f1053":"markdown","08eb95e7":"markdown","06d6f6eb":"markdown","7c651163":"markdown","fc8cd508":"markdown","dfd80b17":"markdown","f2b0a932":"markdown","60f8d3f0":"markdown","111aa0d5":"markdown","4dfcf96a":"markdown","2ce75b00":"markdown","45d57dea":"markdown","b3a526cc":"markdown","b612c91a":"markdown","d746f141":"markdown","dcbb9cd6":"markdown","4e4b12c9":"markdown"},"source":{"bb49de46":"import warnings\nwarnings.filterwarnings('ignore')\n\n#importing the libraries\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = (20.0, 10.0)\nimport seaborn as sns\n\n#pour afficher tous les colonnes d'un tableau\npd.set_option('display.max_columns', None)","a9319749":"data = pd.read_csv('..\/input\/CarPrice_Assignment.csv')\nprint(\"Dimension of our data set is: \")\nprint(data.shape)\ndata.head()","952cb038":"data.info()","df294dff":"#Chaque \u00e9l\u00e9ment du colonne CarName sera diviser en deux String, et on va garder seulement le premier\nCompanyName = data['CarName'].apply(lambda x : x.split(' ')[0])\n\n#Ins\u00e9rer la nouvelle variable comme colonne dans notre dataset\ndata.insert(3,\"CompanyName\",CompanyName)\n\n#Supprimer la colonne CarModel\ndata.drop(['CarName'],axis=1,inplace=True)\n\n#Supprimer la colonne CarID, car elle n'a aucune effet sur notre dataset\ndata.drop(['car_ID'],axis=1,inplace=True)\n\ndata.head()","a4bb13ae":"def get_variable_type(element) :\n    \"\"\"\n     V\u00e9rifier que les colonnes sont de variable continue ou cat\u00e9gorique.\n     L'hypoth\u00e8se est que si:\n                  nombre unique <20 alors on suppose c'est cat\u00e9gorique\n                  nombre unique> = 20 et dtype = [int64 ou float64] alors on suppose c'est continu\n     \"\"\"\n    if element==0:\n        return \"Not Known\"\n    elif element < 20 and element!=0 :\n        return \"Categorical\"\n    elif element >= 20 and element!=0 :\n        return \"Contineous\"\n    \ndef predict_variable_type(metadata_matrix):\n    metadata_matrix[\"Variable_Type\"] = metadata_matrix[\"Valeurs_Uniques_Count\"].apply(get_variable_type).astype(str)\n    metadata_matrix[\"frequency\"] = metadata_matrix[\"Null_Count\"] - metadata_matrix[\"Null_Count\"]\n    metadata_matrix[\"frequency\"].astype(int)\n    return metadata_matrix \n\ndef get_meta_data(dataframe) :\n    \"\"\"\n     M\u00e9thode pour obtenir des m\u00e9tadonn\u00e9es sur n'importe quel dataset transmis\n    \"\"\"\n    metadata_matrix = pd.DataFrame({\n                    'Datatype' : dataframe.dtypes.astype(str), # types de donn\u00e9es de colonnes\n                    'Non_Null_Count': dataframe.count(axis = 0).astype(int), # nombre total d'\u00e9l\u00e9ments dans les colonnes\n                    'Null_Count': dataframe.isnull().sum().astype(int), # total des valeurs nulles dans les colonnes\n                    'Null_Percentage': dataframe.isnull().sum()\/len(dataframe) * 100, # pourcentage de valeurs nulles\n                    'Valeurs_Uniques_Count': dataframe.nunique().astype(int) # nombre de valeurs uniques\n                     })\n    \n    metadata_matrix = predict_variable_type(metadata_matrix)\n    return metadata_matrix\n\ndef list_potential_categorical_type(dataframe,data) :\n    print(\"*********colonnes de type de donn\u00e9es cat\u00e9goriques potentielles*********\")\n    metadata_matrix_categorical = dataframe[dataframe[\"Variable_Type\"] == \"Categorical\"]\n    \n    length = len(metadata_matrix_categorical)\n    if length == 0 :\n        print(\"Aucune colonne cat\u00e9gorique dans un jeu de donn\u00e9es donn\u00e9.\")  \n    else :    \n        metadata_matrix_categorical = metadata_matrix_categorical.filter([\"Datatype\",\"Valeurs_Uniques_Count\"])\n        metadata_matrix_categorical.sort_values([\"Valeurs_Uniques_Count\"], axis=0,ascending=False, inplace=True)\n        col_to_check = metadata_matrix_categorical.index.tolist()\n        name_list = []\n        values_list = []\n        \n        for name in col_to_check :\n            name_list.append(name)\n            values_list.append(data[name].unique())\n        \n        temp = pd.DataFrame({\"index\":name_list,\"Valeurs_Uniques\":values_list})\n        metadata_matrix_categorical = metadata_matrix_categorical.reset_index()\n        metadata_matrix_categorical = pd.merge(metadata_matrix_categorical,temp,how='inner',on='index')\n        display(metadata_matrix_categorical.set_index(\"index\"))","a6c0cd3c":"metadata = get_meta_data(data)\n\n#List potential columns of categorical variables\nlist_potential_categorical_type(metadata,data)","d610fc35":"data.CompanyName.unique()","3e489f54":"data = data.replace(to_replace =\"maxda\", value =\"mazda\") \ndata = data.replace(to_replace =\"porcshce\", value =\"porsche\") \ndata = data.replace(to_replace =\"toyouta\", value =\"toyota\") \ndata = data.replace(to_replace =\"vokswagen\", value =\"volkswagen\") \ndata = data.replace(to_replace =\"vw\", value =\"volkswagen\")\ndata = data.replace(to_replace =\"Nissan\", value =\"nissan\")","287d5abd":"data.CompanyName.unique()","0ca302ad":"plt.title('Car Price Spread')\nsns.boxplot(y=data.price)\nplt.show()\nprint(data.price.describe())","1f40e26c":"plt.title('Car Price Distribution Plot')\nsns.distplot(data.price)\nplt.show()","12126c06":"print(data.price.describe())","0eb5a0a1":"import scipy\nfrom scipy.stats.stats import pearsonr\n\ndef pairplot(x_axis,y_axis) :\n    sns.pairplot(data,x_vars=x_axis,y_vars=y_axis,height=4,aspect=1,kind=\"scatter\")\n    plt.show()","3e26f946":"#Determiner la variable ind\u00e9pendante\ny_vars=['price']","62d8e15a":"x_vars=['wheelbase','curbweight','boreratio']\npairplot(x_vars,y_vars)\nprint(\"At first glance, the 3 variables are positively correlated but spread at higher values.\")\n\np1=data['wheelbase']\np2=data['curbweight']\np3=data['boreratio']\n\npearson_coeff, p_value = pearsonr(p1,data['price'])\nprint('\\nWe can make sure of this by looking at the Coefficient of Correlation')\nprint('\\nCoefficient of Correlation between Price and wheelbase:',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p2,data['price'])\nprint('Correlation coefficient between Price and curbweight:',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p3,data['price'])\nprint('Correlation coefficient between Price and boreratio: ',pearson_coeff*100,'%')","1b0e1bc9":"x_vars=['carlength','carwidth', 'carheight']\npairplot(x_vars,y_vars)\nprint(\"Carlength and Carwidth are more correlated than carheight which is more spread out but positive.\")\n\np1=data['carlength']\np2=data['carwidth']\np3=data['carheight']\n\npearson_coeff, p_value = pearsonr(p1,data['price'])\nprint('\\nWe can make sure of this by looking at the Coefficient of Correlation')\nprint('\\nCorrelation coefficient between Price and carlength:',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p2,data['price'])\nprint('Correlation coefficient between Price and carwidth: ',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p3,data['price'])\nprint('Correlation coefficient between Price and carheight: ',pearson_coeff*100,'%')","b58ec225":"x_vars=['enginesize','horsepower','stroke']\npairplot(x_vars,y_vars)\nprint(\"Enginesize and Horsepower are positively correlated, but Stroke is more spread out (may not be related).\")\n\np1=data['enginesize']\np2=data['horsepower']\np3=data['stroke']\n\npearson_coeff, p_value = pearsonr(p1,data['price'])\nprint('\\nWe can make sure of this by looking at the Coefficient of Correlation')\nprint('\\nCorrelation coefficient between Price and enginesize: ',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p2,data['price'])\nprint('Correlation coefficient between Price and horsepower: ',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p3,data['price'])\nprint('Correlation coefficient between Price and stroke: ',pearson_coeff*100,'%')","0def9aa8":"x_vars=['compressionratio','peakrpm',\"symboling\"]\npairplot(x_vars,y_vars)\nprint(\"Compressionratio, Peakrpm and symboling are not correlated.\")\n\np1=data['compressionratio']\np2=data['peakrpm']\np3=data['symboling']\n\npearson_coeff, p_value = pearsonr(p1,data['price'])\nprint('\\nWe can make sure of this by looking at the Coefficient of Correlation')\nprint('\\nCorrelation coefficient between Price and compressionratio: ',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p2,data['price'])\nprint('Correlation coefficient between Price and peakrpm: ',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p3,data['price'])\nprint('Correlation coefficient between Price and symboling: ',pearson_coeff*100,'%')","2a618d3d":"x_vars=['citympg', 'highwaympg']\npairplot(x_vars,y_vars)\nprint('Citympg & Highwaympg are negatively correlated.\\nThe more prices get lower, the higher the distances get, which means that the cheapest cars have better mileage than expensive cars.')\n\np1=data['citympg']\np2=data['highwaympg']\n\npearson_coeff, p_value = pearsonr(p1,data['price'])\nprint('\\nWe can make sure of this by looking at the Coefficient of Correlation')\nprint('\\nCorrelation coefficient between Price and citympg: ',pearson_coeff*100,'%')\n\npearson_coeff, p_value = pearsonr(p2,data['price'])\nprint('Correlation coefficient between Price and highwaympg: ',pearson_coeff*100,'%')","5c9c570d":"def heatmap(x,y,dataframe):\n    sns.heatmap(dataframe.corr(),cmap=\"OrRd\",annot=True)\n    plt.show()","3cff5d6c":"heatmap(20,12,data)","052e1a33":"dimension_col_list = ['wheelbase', 'carlength', 'carwidth','curbweight']\n\nheatmap(10,10,data.filter(dimension_col_list))","53b3ed6d":"performance_col_list = ['enginesize','boreratio','horsepower']\nheatmap(10,10,data.filter(performance_col_list))","c9a8fbb1":"performance_col_list = ['citympg','highwaympg']\nheatmap(10,10,data.filter(performance_col_list))","54ef4fb0":"plt.figure(figsize=(20,9))\n\nplt.xticks(rotation = 90)\norder = data['CompanyName'].value_counts(ascending=False).index\nsns.countplot(x='CompanyName', data=data, order=order)\n\nplt.show()","194387c3":"plt.figure(figsize=(20, 12))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'fueltype', y = 'price', data = data)\n\nplt.subplot(2,3,2)\nplt.title('Fuel Type Histogram')\norder = data['fueltype'].value_counts(ascending=False).index\nsns.countplot(x='fueltype', data=data, order=order)\n\nplt.show()","9a0e2fe2":"plt.figure(figsize=(20, 12))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'aspiration', y = 'price', data = data)\n\nplt.subplot(2,3,2)\nplt.title('Aspiration Histogram')\norder = data['aspiration'].value_counts(ascending=False).index\nsns.countplot(x='aspiration', data=data, order=order)\n\nplt.show()","941fc1b9":"plt.figure(figsize=(20, 12))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'doornumber', y = 'price', data = data)\n\nplt.subplot(2,3,2)\nplt.title('Door Number Histogram')\norder = data['doornumber'].value_counts(ascending=False).index\nsns.countplot(x='doornumber', data=data, order=order)\n\nplt.show()","ab0a359c":"plt.figure(figsize=(20, 12))\n\nplt.subplot(2,3,1)\nsns.boxplot(x = 'enginelocation', y = 'price', data = data)\n\nplt.subplot(2,3,2)\nplt.title('Engine Location Histogram')\norder = data['enginelocation'].value_counts(ascending=False).index\nsns.countplot(x='enginelocation', data=data, order=order)\n\nplt.show()","e7678702":"plt.subplot(2,3,1)\nsns.boxplot(x='carbody',y='price',data = data)\n\nplt.subplot(2,3,2)\nplt.title('Car Body Histogram')\norder = data['carbody'].value_counts(ascending=False).index\nsns.countplot(x='carbody', data=data, order=order)\n\nplt.show()","bc231b08":"plt.subplot(2,3,1)\nsns.boxplot(x='fuelsystem',y='price',data = data)\n\nplt.subplot(2,3,2)\nplt.title('Fuel System Histogram')\norder = data['fuelsystem'].value_counts(ascending=False).index\nsns.countplot(x='fuelsystem', data=data, order=order)\n\nplt.show()","175e5cac":"plt.subplot(2,3,1)\nsns.boxplot(x='enginetype',y='price',data = data)\n\nplt.subplot(2,3,2)\nplt.title('Engine Type Histogram')\norder = data['enginetype'].value_counts(ascending=False).index\nsns.countplot(x='enginetype', data=data, order=order)\n\nplt.show()","f801d0f8":"plt.subplot(2,3,1)\nsns.boxplot(x='cylindernumber',y='price',data = data)\n\nplt.subplot(2,3,2)\nplt.title('Cylinder Number Histogram')\norder = data['cylindernumber'].value_counts(ascending=False).index\nsns.countplot(x='cylindernumber', data=data, order=order)\n\nplt.show()","ea856c99":"plt.subplot(2,3,1)\nsns.boxplot(x = 'drivewheel', y = 'price', data = data)\n\nplt.subplot(2,3,2)\nplt.title('DriveWheel Histogram')\norder = data['drivewheel'].value_counts(ascending=False).index\nsns.countplot(x='drivewheel', data=data, order=order)\n\nplt.show()","07f936e5":"plt.subplot(2,3,1)\nsns.boxplot(x=data.symboling, y=data.price)\n\n\nplt.subplot(2,3,2)\nplt.title('Symboling Histogram')\norder = data['symboling'].value_counts(ascending=False).index\nsns.countplot(x='symboling', data=data, order=order)\n\nplt.show()","369382ac":"metadata_matrix_dataframe = get_meta_data(data)\nlist_potential_categorical_type(metadata_matrix_dataframe,data)","168da5b3":"data = data.drop(['carheight' ,'stroke' ,'compressionratio' ,'peakrpm' ,'carlength' ,'carwidth' ,'curbweight' ,'enginesize' ,'highwaympg'], axis=1)\ndata.head()","436948e8":"def binary_dummy_replace(x) :\n     return x.map({\"gas\":1,\"diesel\":0,\n                   \"std\":1,\"turbo\":0,\n                   \"two\":1, \"four\":0,\n                   \"front\": 1, \"rear\": 0})\ndef dummies(x,df):  \n    temp = pd.get_dummies(df[x], prefix=x, drop_first = True)\n    \n    #l = temp.columns.values\n    #for nm in l:\n        #newt=x+\"_\"+nm\n        #temp.rename({nm: Replace_Name(x)+\"_\"+nm}, axis=1, inplace=True)\n        \n    #print(temp.columns.values)\n        \n    df = pd.concat([df, temp], axis = 1)\n    df.drop([x], axis = 1, inplace = True)\n    return df","e300aba2":"data = dummies('symboling',data)\ndata = dummies('CompanyName',data)\ndata = dummies('fueltype',data)\ndata = dummies('aspiration',data)\ndata = dummies('doornumber',data)\ndata = dummies('carbody',data)\ndata = dummies('drivewheel',data)\ndata = dummies('enginelocation',data)\ndata = dummies('enginetype',data)\ndata = dummies('cylindernumber',data)\ndata = dummies('fuelsystem',data)","d0a489cc":"data.head()","41af6c57":"from sklearn.model_selection import train_test_split\n\n# We specify this so that the train and test data set always have the same rows, respectively\ncars_train, cars_test= train_test_split(data, train_size=0.67, test_size=0.33, random_state = 0)","a32c7935":"from sklearn.preprocessing import StandardScaler,scale\n#we create an object of the class StandardScaler\nsc = StandardScaler() \n\ncol_to_scale = ['wheelbase','boreratio','horsepower','citympg','price',]\n\ncars_train[col_to_scale] = sc.fit_transform(cars_train[col_to_scale])\ncars_test[col_to_scale] = sc.fit_transform(cars_test[col_to_scale])\n\ncars_train.head()","5af25544":"y_train = cars_train.loc[:,cars_train.columns == 'price']\n\nX_train = cars_train.loc[:, cars_train.columns != 'price']","18df9938":"y_test = cars_test.loc[:,cars_test.columns == 'price']\n\nX_test = cars_test.loc[:, cars_test.columns != 'price']","7c9aee7a":"# Making predictions\nimport statsmodels.api as sm \n\nlm = sm.OLS(y_train,X_train).fit()\n\ny_pred=lm.predict(X_test)","c54c7efd":"resid = y_test - y_pred.to_frame('price')","489fcdea":"fig = plt.figure(figsize=(9,6))\nsns.distplot(resid, bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)\nplt.show()","337b473c":"plt.figure(figsize=(9,9))\nplt.scatter(y_pred, resid)\nplt.hlines(0,-2,4)\nplt.suptitle('Residuals vs Predictions', fontsize=16)\nplt.xlabel('Predictions')\nplt.ylabel('Residuals')","41345dcd":"from scipy import stats\n\ndef normality_of_residuals_test(model):\n    '''\n    Function to establish the normal QQ graph of the residues and perform the Anderson-Darming statistical test to study the normality of the residuals.\n    \n    Arg:\n    * model - OLS models adapted from statsmodels\n    '''\n    sm.ProbPlot(lm.resid).qqplot(line='s');\n    plt.title('Q-Q plot');\n\n    ad = stats.anderson(lm.resid, dist='norm')\n    \n    print(f'----Anderson-Darling test ---- \\nstatistic: {ad.statistic:.4f}, critical value of 5%: {ad.critical_values[2]:.4f}')\n    \nnormality_of_residuals_test(lm)","3bdc08b6":"plt.figure(figsize=(15,9))\nplt.scatter(resid.index, resid.values)\nplt.hlines(0,0,200)\nplt.suptitle('Residuals by order', fontsize=16)\nplt.xlabel('Order')\nplt.ylabel('Residuals')","2cbc0221":"from statsmodels.stats.stattools import durbin_watson","8bfe9658":"print(durbin_watson(resid))","5c1df2ea":"import statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(resid, lags=40 , alpha=0.05)\nacf.show()","01a9755b":"%matplotlib inline\n%config InlineBackend.figure_format ='retina'\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\nsns.set_style('darkgrid')\nsns.mpl.rcParams['figure.figsize'] = (15.0, 9.0)\n\ndef homoscedasticity_test(model):\n    '''\n    Fonction de test de l'homosc\u00e9dasticit\u00e9 des r\u00e9sidus dans un mod\u00e8le de r\u00e9gression lin\u00e9aire.\n\n    Il compare les valeurs r\u00e9siduelles aux valeurs pr\u00e9dites et ex\u00e9cute les tests de Goldfeld-Quandt.\n    \n    Args:\n    * model - fitted OLS model from statsmodels\n    '''\n    fitted_vals = model.predict()\n    resids = model.resid\n\n    #fit_reg=False\n    sns.regplot(x=fitted_vals, y=resids, lowess=True, line_kws={'color': 'red'})\n    plt.suptitle('R\u00e9sidus vs Pr\u00e9dictions', fontsize=16)\n    plt.xlabel('Pr\u00e9dictions')\n    plt.ylabel('R\u00e9sidus')\n\n    print('\\n----Goldfeld-Quandt test ----')\n    name = ['F statistic', 'p-value']\n    test = sms.het_goldfeldquandt(lm.resid, lm.model.exog)\n    print(lzip(name, test))\n    print('\\n----Residuals plots ----')\n\nhomoscedasticity_test(lm)","0e90c530":"fig = plt.figure(figsize=(11,5))\nplt.scatter(y_test,y_pred)\nplt.xlabel('y_test', fontsize=18)\nplt.ylabel('y_pred', fontsize=16)\n\n#Regression Line function\nf = lambda x: x\n\n# x values of line to plot\nx = np.array(y_test)\n\n# plot fit\nplt.plot(x,f(x),lw=2.5, c=\"orange\")\n","16a4d628":"from sklearn.metrics import r2_score \nr2_score(y_test, y_pred)","49497541":"print(lm.summary())","e5303097":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","a80fff26":"regression = LinearRegression()\nregression.fit(X_train,y_train)","edf1c941":"rfe = RFE(regression,10)\nrfe = rfe.fit(X_train,y_train)","fe682396":"for z in range(len(X_train.columns)):\n    print(X_train.columns[z],'\\t\\t\\t',rfe.support_[z])","176079f3":"col = X_train.columns[rfe.support_]\nfor x in col:\n    print(x)","b347439a":"X_train_rfe = X_train[X_train.columns[rfe.support_]]\nX_train_rfe.head()","f80b1ab4":"import statsmodels.api as sm \n\ndef color_code_vif_values(val):\n    \"\"\"\n    Take a scalar and return a string with the property css 'color: red' for 10, black otherwise.\n    \"\"\"\n    if val > 10 : color = 'red' \n    elif val > 5 and val <= 10 : color = 'blue'\n    elif val > 0 and val <= 5 : color = 'darkgreen'\n    else : color = 'black'\n    return 'color: %s' % color\n\ndef drop_col(dataframe,col_to_drop) :\n    dataframe.drop([col_to_drop],axis=1,inplace=True)\n    return dataframe\n\ndef display_vif(x) :\n    #Calculer les VIFs pour le nouveau mod\u00e8le\n    from statsmodels.stats.outliers_influence import variance_inflation_factor\n    vif = pd.DataFrame()\n    X = x\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.set_index(\"Features\")\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    df = pd.DataFrame(vif.VIF).style.applymap(color_code_vif_values)\n    display(df)\n    \nmodel_count = 0\n\ndef statsmodel_summary(y_var,x_var) :\n    global model_count\n    model_count = model_count + 1\n    text = \"*****MODEL - \" + str(model_count)\n    print(text)\n    \n    x_var_const = sm.add_constant(x_var) # adding constant\n    lm = sm.OLS(y_var,x_var_const).fit() # calculating the fit\n    print(lm.summary()) # print summary for analysis\n    display_vif(x_var_const.drop(['const'],axis=1))\n    return x_var_const , lm","b97e9b9d":"lm = statsmodel_summary(y_train,X_train_rfe)","0f577b63":"X_train_rfe = X_train_rfe.drop([\"carbody_sedan\"], axis = 1)\nX_train_rfe.head()","c7f947a8":"lm = statsmodel_summary(y_train,X_train_rfe)","bc82739c":"X_train_rfe = X_train_rfe.drop([\"carbody_wagon\"], axis = 1)\nX_train_rfe.head()","b1141a81":"lm = statsmodel_summary(y_train,X_train_rfe)","12e4f96f":"X_train_rfe = X_train_rfe.drop([\"CompanyName_porsche\"], axis = 1)\nX_train_rfe.head()","bbd2ce67":"lm = statsmodel_summary(y_train,X_train_rfe)","a7ce668b":"#Array containing names of important variables\nfinal_features = list(X_train_rfe.columns)\n\n#Filter the test dataset\nX_test_new = X_test.filter(final_features)\n\nX_test_new.head()","7c961df9":"# Making predictions\nlm = sm.OLS(y_train,X_train_rfe).fit()\n\ny_pred=lm.predict(X_test_new)","027ecc98":"resid = y_test - y_pred.to_frame('price')","fd8b4432":"plt.figure(figsize=(15,9))\nplt.scatter(resid.index, resid.values)\nplt.hlines(0,0,200)","70bcca4e":"print(durbin_watson(resid))","f2a44eee":"import statsmodels.tsa.api as smt\n\nacf = smt.graphics.plot_acf(resid, lags=40 , alpha=0.05)\nacf.show()","82aa0f10":"homoscedasticity_test(lm)","2e5aff8d":"fig = plt.figure(figsize=(9,6))\nsns.distplot(resid, bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)\nplt.show()","c642269b":"plt.scatter(y_pred, resid)\nplt.hlines(0,-2,4)","2b307190":"normality_of_residuals_test(lm)","c44ab218":"fig = plt.figure(figsize=(11,5))\nplt.scatter(y_test,y_pred)\nplt.xlabel('y_test', fontsize=18)\nplt.ylabel('y_pred', fontsize=16)\n\n#Regression Line function\nf = lambda x: x\n\n# x values of line to plot\nx = np.array(y_test)\n\n# plot fit\nplt.plot(x,f(x),lw=2.5, c=\"orange\")","98df1f24":"from sklearn.metrics import r2_score \nr2_score(y_test, y_pred)","07ed6ae2":"print(lm.summary())","2ddbb3e2":"And here is our Dataset, all is numeric","99b6e160":"**3. ACF - Auto-Correlation Function plots**","07d1512f":"**2-B. Homoscedasticity**","1d0d1429":"**Price VS enginesize - horsepower - stroke**","64f52d29":"**Price VS aspiration**","39279e53":"## Problem Description\nA Chinese automobile company **Teclov_chinese** aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts. They have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. Essentially, the company wants to know:\n\n\u2022 Which variables are significant in predicting the price of a car\n\n\u2022 How well those variables describe the price of a car Based on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the American market.\n\n## Business Goal\n\nYou are required to model the price of cars with the available independent\nvariables. It will be used by the management to understand how exactly\nthe prices vary with the independent variables. They can accordingly\nmanipulate the design of the cars, the business strategy etc. to meet certain\nprice levels. Further, the model will be a good way for the management to\nunderstand the pricing dynamics of a new market.\nData Preparation\n\n\u2022 There is a variable named CarName which is comprised of two parts:\n- the first word is the name of 'car company' and the second is the 'car\nmodel'. For example, chevrolet impala has 'chevrolet' as the car\ncompany name and 'impala' as the car model name. You need to\nconsider only company name as the independent variable for model\nbuilding.","36c3ef45":"**==== The 2nd warning says: ===**\n    \n     The smallest eigenvalue is 2.4e-30. This may indicate that there are strong problems of multicollinearity or\n     that the design matrix is singular.\n    \nViolation of hypothesis of multicollinearity. There are correlated variables between them which should not exist","e6dd364e":"**Next step ?**\n\nDetect and treat Outliers (either delete them, or fix them if there are errors) and see if there are improvements.\n\nCheck my Other Kernel Regarding this Step","bd9e220b":"**2-C. Normality of the error distribution**","74142910":"**59** variables used to have R\u00b2> 0.8, which is too much.","30b30aa0":"**Price VS symboling**","729bf21d":"**Evaluation of the test via the comparison of y_pred and y_test**","34b0cd7d":"It seems that ** sedan ** is the most favored.\n\n** hardtop ** has the highest average price.\n\n** >>> ** Note: Existence of Outliers for several values.","989211b6":"What we are looking for is that the errors should be as close as possible to the line","93294bdc":"The average price of a **diesel car** is higher than that of **gas cars**, which explains, according to the histogram, why the company sold more **gas cars** than **diesel cars**.\n\n**>>>** Note: Existence of Outliers for **Gas**","9ed6aed2":"**Is it good ?**\n\nStill not 100% perfect, because of the existence of Outliers that strongly affect our model","60d22e21":"   **a. Examination of the correlation between the variables specific to the dimensions of a car i.e. weight, height etc**","81fbd95e":"We see that all the bars are inside the blue zone, except one, which is acceptable since Durbin_Watson has returned a value very close to 2.0","bc8fcba8":"**Evaluation of the test via the comparison of y_pred and y_test**","0fa967e8":"# Read Data\n","70f54ea9":"===================================================================================================\n\n**P-values** of some variables appear to be greater than 0.05, meaning they are statistically insignificant.","b3989e07":"1. **Plot of residuals against their order.**","2a1fbf33":"**Price VS Wheelbase - curbweight - boreratio**","b3152bed":"As we see, the points are scattered at random. no trend.","007c58fb":"# 2. Evaluation","44f976d5":"P-value analysis: P-value of **carbody_sedan** > **0.05**. We must delete it.","c15fd85b":"**Now let's use our model to make predictions.**","cf9d0b4b":"**** Dataset is clean and no substitution of Null values is required ****\n\n","355ad752":"P-value analysis: P-value of **carbody_wagon** > **0.05**. We must delete it.","b4406ec1":"So we have to adjust things by replacing the values with one identical variable:","e0756312":"To do that, we base ourselves on 2 things:\n\n1.**P-Value:**\n\n    P-Value <= **0.05** means that this particular independent variable greatly improves the fit of the model\n\n    P-value > **0.05** no improvement\n\n2.**VIF (Variance Inflation Factor):**\n\n    VIF > **5**, there is an indication that multicollinearity may be present, but not enough to worry.\n\n    VIF > **10**, there is certainly a multicollinearity among the variables.","3f327b6e":"**Price VS fuelsystem**","a1706a81":"**A. Key assumptions of multiple regression:**","2b12ed8b":"the test rejects H0 that the data follow a normal distribution (stats > critical value of 5%)","3426c060":"Looking at the above histogram, **Toyota** seems to be very popular, followed by **Nissan** and **Mazda**.","e5d2e3a8":"The test of **Durbin-Watson** tests the null hypothesis that the residuals are not dependent (autocorrelated) on each other.\n\nThe test returns a value **d** between 0 and 4.\n\nA value **d = 2**: No autocorrelation detected in the sample.\n\nA value **d <** **2**: indicate a positive autocorrelation.\n\nA **d >** **2** value: indicate a negative autocorrelation.\n\n\nIn general, values of **d ~ 2** indicate that there is no dependence (no autocorrelation) between the residuals.","a26143d5":"# Final Summary - Part 1","eaeec733":"**Using statsmodel package, for detailed statistics**","0ec46de2":"# Exploratory of Data","1293e199":"1. **Q-Q Plot**:\n    \nAn arc-shaped deflection trace with respect to the diagonal implies that the residuals have an excessive asymmetry, meaning that the distribution is not symmetrical, with too many important residuals in one direction.","cdf481db":"**Anderson-Darling Test** and **Q-Q Plot of residuals**","91afced1":"# Dependent variable visualization: Price","f94b11f6":"What we are looking for is that the errors should be as close as possible to the line, wich is the case in the plot below","7330fc41":"**Dummy Variables creation**","8b498fae":"2. **Anderson-Darling Test**","c566bfc5":"P-value analysis: All variables have p-value <0.05\n\nNext step: Delete **CompanyName_porsche** because VIF> 10","34663a8e":"**Wheelbase** , **carlength**, **carwidth** et **curbweight** [ 0.80 - 0.88 ] are very correlated and we have to keep only one between them.","9beaff29":"Residuals (i.e. errors) are simply the difference between predictions and observations","fdc19243":"**2. Examine categorical variables and correct them if spelling errors are found**","253ad955":"**doornumber** values are pretty close, which means the price is not affected by **doornumber**\n\n**>>>** Note: Existence of Outliers in **four** and **two**","fbf86df0":"**Analysis:**\n\nP-Value test **Goldfeld-Quandt** > 0.05, so we accept **H0** saying that the error terms are homoscedastic, which means that the residuals have a constant variance.\n\nAlso, in the plot, the points are scattered randomly, no tendancy to be found, and the line doesn't have the form of an arch","d81028f1":"Creation of X_test_new by keeping only the relevant variables found by RFE.","f4b7b78c":"**Price VS carbody**","1974cc6d":"All the P-Values are under **0.05**. Also, the Previous Alert that we had about existence of Multi-colinearity is not there any more","22cb0ccc":"But there is another variable that is not considered categorical. It's the column \"CompanyName\".\nLet's see are there any repetitive values?","5e61d9ee":"It seems that ** symboling ** ** 0 and 1 ** are the most favored.\n\nCars with ** symboling -1 and -2 ** are the most expensive, which is logical because it means that the car is more secure.\n\n** >>> ** Note: Existence of Outliers for several values.","3535e8ae":"# Cars Prices with Multiple Linear Regression and RFE","620232a3":"# Data preparation","b13b1cd5":"**Analysis:** Data points are a bit more on one side than the other","efcb1f81":"**Horsepower** and **enginesize** are highly correlated and we need to keep only one.","9f508ccb":"**b. Examination of the correlation between the variables specific to the performance of a car**","8bddb9c5":"Call the RFE method to have important variables","14afd171":"# ","df54f0f6":"**Extract important variables**\n\nThe **RFE()** method returns a **\"support_\"** array of variables considered important:\n\nIf **support_** == **True**: Important variable\n\nIf **support_** == **False**: Not important variable","09c1ccea":"A histogram of the residuals (errors) in our model can be used to check if they are normally distributed or not.","034341e8":"**Price VS enginetype**","160359da":"# Visualization of independent variables:","1a089e80":"**Price VS CompanyName**","26f8d0ae":"**2-A. Normality of the error distribution**","7dbbb4c8":"** Remove the uncorrelated variables with Price, and choose only one variable among the variables correlated with it **","141ab7d9":"**1. Separate the CarName variable to two columns: CompanyName and CarModel**","a5e500b8":"It is clear that **rear cars** are very expensive, which is why the company sold more cars with **front** rear.\n\n** >>> ** Note: Existence of Outliers in ** front **","77801d4e":"**II. Checking the multicollinearity between the correlated independent variables above and Price**","f45f74dc":"According to the box-mustache, the price field has an average **around 13K** and a **median around 10k** with the most expensive **car values at 45k** and the **cheapest cars at 5k**.\n\nSince we have mean > median, then our distribution is positively asymmetric, as we can see in the following histogram:","838ac31a":"** mpfi ** is the most favored type of ** fuelsystem **, even though it has the highest average price.\n\n** >>> ** Note: Existence of Outliers for ** mpfi ** and ** 2bbl **","0037868e":"**2-A. Independence of errors: **\n\n   **1.Graphe des r\u00e9sidus contre leur ordre.**","4dad7f57":"**Price VS citympg - highwaympg**","f6a3c7e1":"# ******** Categorical ********","ee431b38":"# Cleaning Data\n","82315ac6":"What must alert us is the existence of a tendency, which tells us that the errors are in fact dependent.","6af87b8f":"# **Residual analysis of the model**","9e6b05f0":"**Price VS drivewheel**","97a420d3":"**Coefficient of determination R\u00b2:**","f4e5b0d6":"**Price VS carlength - carwidth - carheight**","4398eb34":"## RFE","f42725d0":"**Analysis:** The error terms are distributed around zero, which means that the prediction of the model is not random.","6f39dc4b":"**c. Examining the correlation between citympg and highwaympg**","5d4117b3":"The blue trace slightly take form of an arc, but not aggressively","49a8b372":"The ** four-cylinder ** seems to be the most favored.\n\nWe can see that expensive cars have ** eight-cylinder **, and ** four-cylinder ** are the cheapest.\n\n** >>> ** Note: Existence of Outliers for ** four **","dcdae97f":"In the section of Exploring Data, we have identified the relevant independent variables that are correlated with the independent variable Price, and which are not correlated with each other (ie no multi-collinearity), and we said that we need to keep them, and igon the rest.\n\nTo do that, we will use a mixed approach to find the relevant features:\n\n     1.Identify these variables using RFE (Recursive Feature Elimination)\n\n     2.Manual approach to find the right fit","27346106":"**Residual analysis of the model**","a8ec7ff3":"** FWD ** is the most favored, followed by ** RWD **, and ** 4WD ** is the least favored even though it is cheaper than ** RWD **.\n\n** >>> ** Note: Existence of Outliers for several values.","b982dd36":"In reviewing the above data, we found that few company names were identical but misspelled, such as:","f19d63e9":"Another method to check, is to draw a graph of the predictions against the residualss, and see if the points are evenly distributed or not.","2243cec1":"**Reduction of dimension**\n\n7 variables against 59 for **R\u00b2> 0.8** !!! It's much better","cc5adab0":"## Final summary of the model","58cfeee4":"**Price VS fueltype**","5b83d3ec":"** ohc ** is the most favored engine type.\n\n** >>> ** Note: Existence of Outliers for several values.","4b1b7d1b":"We see that all the bars are inside the blue zone, except one, which is acceptable since **Durbin_Watson** returned a value very close to 2.","f507bfdd":"**2-B. Independence of errors**","528027f2":"**Recall:**\n\nIf **R\u00b2 = 0**: the dependent variable Y can not be predicted from the independent variable **X**\n\nIf **R\u00b2 = 1**: the dependent variable Y can be predicted from the independent variable **X**\n\nIf **0 <R\u00b2 <1**: Indicates the percentage at which the dependent variable Y is predictable by **X**","3f684e17":"**I. Check the linear relationship between the dependent variable \"Price\" and the numerical independent variables**","2f6b2783":"**2-C. Homoscedasticity**","8a9ff93d":"3. **ACF - Auto-Correlation Function plots**","f53766a4":"1. 'maxda' Et 'mazda' ================> mazda\n2. 'porsche' Et 'porcshce' ===========> porsche\n3. 'toyota' Et 'toyouta' =============> toyota\n4. 'vokswagen' Et 'volkswagen','vw' ==> volkswagen\n5. 'Nissan' Et 'nissan' ==============> 'nissan'","87eee2d8":"## Prediction and Evaluation","0bbe9bb8":"**Recall:**\n\nIf P-value <= 0.05 ==> We reject the null hypothesis **H0**\n\nIf P-Value> 0.05 ==> The null hypothesis **H0** is true","ee7b7328":"**Analysis:** We can see that the values are ** evenly distributed ** around 0, with only a few ** outliers **.","cfffdce9":"This assumption indicates that the variance of the residuals must be similar for the values of the independent variables.\n\nWe can verify this by plotting the residuals against the predicted values.\n\nTo identify **homo-dedasticity** in the graph, the location of the points should be random and no trend should be visible, and the red regression line in the graph should be as flat as possible (not of an arch form).\n\n**Goldfeld-Quandt test:**\n\nHe tests:\n\n1. Null hypothesis **H0**: the error terms are homoscedastic\n2. Alternative Hypothesis **H1**: The error terms are heteroscedastic.","a3da8d15":"**citympg** and **highwaympg** are highly correlated and we need to keep one of them.","748ded3b":"Since the independent variable (i.e Price) is continuous numerical variable, and there is many dependat variables, we we will use Multiple linear regression","f8f64428":"## 1. Pr\u00e9diction","b6b0693c":"Draw XY scatter plot, and check are they linear or not?","5950468a":"**Coefficient of determination R\u00b2.**","949244c4":"To perform **multiple linear regression**, the following assumptions must be met:\n\n** --- Before model construction: --- **\n\n1. ** Linear relationship: ** The dependent variable Y (i.e Price) has a linear relationship with the independent variables X, and to verify this, one must ensure that the XY dispersion graph is linear.\n\n\n2. ** No multi-collinearity: ** Multiple regression assumes that independent variables X are not strongly correlated with each other. This assumption is tested using ** Variance Inflation Factor (VIF) ** or using ** Correlation Matrix **.\n\n** --- After: Residual analysis of the model --- **\n\n\n3. ** Normality of Error Distribution **\n\n\n4. ** Independence of errors **\n\n\n5. ** Homo-scedasticity **","85f15a94":"At any time in our independent values X, the data points must be fairly close to the line, evenly distributed with only a few outliers.","c296d83c":"all the hypotheses are verified","95a974b1":"Here is our Dataset for learning, after RFE","0b1cfa42":"# 1. Prediction","facc1d06":"We want to see if the value of **ACF** is significant for each bar.\n\nBy calling the function, we indicate the level of significance that interests us (alpha = 0.05 in our case) and the critical zone is drawn on the graph (In blue).\n\nSignificant correlations lie outside this area.\n\n**>>> Note**: First bar is always at 1.","c2967899":"# Is it good ?","4623d4cc":"**Price VS doornumber**","cf4a7feb":"**Analysis:**\n\nP-Value of **Goldfeld-Quandt Test** > 0.05, so we accept **H0** saying that the error terms are homoscedastic, which means that the residuals have a constant variance.\n\nAlso, in the plot, the points are scattered randomly, no tendancy to be found, and the line doesn't have the form of an arc","f588322b":"**What has changed with RFE?**","691aec8b":"The trace does not have an arc shape.\n\nthe test accepts **H0** that the data follow the normal distribution (stats < critical value of 5%)","2079b34c":"**Anderson-Darling Test and Q-Q Plot**","2f98131a":"The average price of cars with **turbo** aspiration is higher than that of **standard** aspiration, which explains, according to the histogram, why the company sells cars with **standard** aspiration more than of cars with **turbo** aspiration.\n\n** >>> ** Note: Existence of Outliers for ** Turbo ** and ** std **","6a42f2d4":"## I. Check the multicollinearity between the variables selected by RFE","b17e5e62":"**Price VS cylindernumber**","12f89bf7":"**Feature Scaling: Train Set & Test Set**","03450e21":"The use of RFE gives us an automated way to select important attributes that can influence the dependent variable (i.e Price).\n\nWe will use a mixed approach here and initially, we will simply use the functionalities returned by RFE as a starting model.","8e555e80":"**Let's use our model now to make predictions.**","d0dec1e6":"**Can we do better?**","543796c8":"**Division into X_test and y_test sets**","179009e9":"This means that the residuals (i.e errors) should not be correlated.","74752850":"Remains the most used by experts, to check the standard of errors.\n\nIt tests:\n\n**H0:** Data follow normal distribution\n\n**H1:** Data does not follow normal distribution\n\nHow to check **H0**?\n\nA level of significance is chosen by us (5% in our case), which is associated with a critical value.\n\nIf the returned **A-D** statistic is greater than the critical value for the chosen significance level, then **H0** must be rejected.","b2b7ba17":"## 2. Evaluation","18d1956e":"# Prediction and Evaluation","2b18e90d":"**Price VS compressionratio - peakrpm - symboling**","a54f1053":"**Conclusion**\n\nWhich means that most of the prices offered by this company are low.\n\nAs seen below, we have **75%** prices are around **16k**, or **25%** between **17k and 45k**.","08eb95e7":"   **1. Cleaning Data**\n   \n   **2. Exploratory of Data**\n   \n   **3. Data preparation**\n   \n   **4. Model Construction**\n   \n   **5. Prediction & Evaluation**\n   \n   **6. RFE**\n   \n   **7. Final Summary**\n   \n   **8. Next step ?**","06d6f6eb":"**Preparation of Train and Test data**","7c651163":"**Price VS enginelocation**","fc8cd508":"**2. Durbin_Watson Test**","dfd80b17":"We can see that the values of these variables are in strings.\n\nHowever, to fit a regression line, we would need numeric values instead of strings. Therefore, we must convert them to 1 and 0 using dummy variables.\n\nWe can do the following :\n\n** fueltype ** {\"** gas **\": 1, \"** diesel **\": 0}\n\n** suction ** {\"** std **\": 1, \"** turbo **\": 0}\n\n** doornumber ** {\"** two **\": 1, \"** oven **\": 0}\n\n** enginelocation ** {\"** front **\": 1, \"** rear **\": 0}","f2b0a932":"As we see, the points are scattered randomly. no tendancy to be found.","60f8d3f0":"    1.R\u00b2 decreased by 0.05 and remains above 0.8 which is good.\n\n    2.The value of the Durbin-watson test has increased, indicating more independence of errors.\n\n    3.The value of the Golden-Quandt test has increased sharply from 0.070 to 0.130, and the red line remains flat. (Much better homo-scedasticity)\n\n    4.No multicollinearity.\n\nBut, we see that the hypothesis of normality of errors is violated.\n\nActual data rarely includes normally distributed errors, and it may not be possible to adapt your data to a model whose errors do not violate the normality assumption.\n\nIt is usually best to focus on violations of other assumptions and \/ or on the influence of some Outliers (who may in any case be responsible for violations of normality).\n\nSo, if I have the choice between, violating the hypothesis of multicollinearity or the hypothesis of normality of the errors. I will choose normality because it is the least sensitive hypothesis among the others.","111aa0d5":"**The columns selected by RFE**","4dfcf96a":"# ******** Numerical ********","2ce75b00":"**Conclusion**\n\n(+) positively correlated variables with Price: ** wheelbase, carlenght, carwidth, curbweight, enginesize, boreratio, horesepower **\n\n(-) negatively correlated variables with Price: ** citympg, highwaympg **\n\nThese variables should be kept for a better model, and the other variables should be ignored as they are not correlated with Price","45d57dea":"All P-Values are less than 0.05, and all VIF values are less than 5.\n\nTurning now to the prediction and evaluation of our model","b3a526cc":"Yes, by using **RFE**","b612c91a":"**Analysis:** The error terms are a little bit asymmetric.","d746f141":"2. **Durbin_Watson** Test","dcbb9cd6":"**Division into X_train and y_train sets for model construction**","4e4b12c9":"We will divide our dataset into ** 67% ** for learning, and ** 33% ** for testing"}}