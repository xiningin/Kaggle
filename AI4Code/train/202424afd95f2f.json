{"cell_type":{"8462a170":"code","fcf39cb7":"code","03296a35":"code","8e11f39c":"code","f0130c4b":"code","11a5c9df":"code","c9f74be2":"code","1324e9c3":"code","a56bdb5f":"code","70c7e815":"code","5e49f5a2":"code","6a044541":"code","7d523090":"code","d3e9e76d":"code","03f5b83d":"code","bce812b5":"code","896a3313":"code","c67604c4":"code","f6d7500f":"code","077f19cf":"code","cf6d32f6":"code","f2fababb":"code","c8d1228f":"code","adf0b876":"code","bbefc174":"code","61b8e863":"code","d6b52e2a":"code","70eb7f33":"code","2761de70":"code","aedd5611":"code","9af14f59":"code","03bbfd22":"code","20f9c653":"code","5f4d7d63":"code","21fd13cf":"code","1854dd77":"code","58d3f666":"code","4726cef0":"code","acbfb267":"code","a4e19e2d":"code","1c49ae13":"code","8e0b1685":"code","967959a3":"code","190b552e":"code","654b64fb":"code","ee5729fb":"code","c5cfd728":"code","81644561":"code","6550b1b1":"code","0629a537":"code","fea6153e":"code","215a48d9":"code","fbae6027":"code","888f12e1":"code","e07509c0":"code","c6ae7fa8":"code","214e411f":"markdown","5d2a7a74":"markdown","47694773":"markdown"},"source":{"8462a170":"#we shall import dependencies\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport matplotlib\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nimport seaborn as sns\nmatplotlib.rcParams['figure.figsize']=(20,10)","fcf39cb7":"#importing the dataset\ndf1=pd.read_csv('\/kaggle\/input\/bengaluru-house-price-data\/Bengaluru_House_Data.csv')\ndf1.head()                ","03296a35":"sns.heatmap(df1.isnull(),yticklabels=False)","8e11f39c":"#we will check what is in dataset\ndf1.shape","f0130c4b":"#lets see a distribution of area types\nsns.countplot(x=df1.area_type)","11a5c9df":"# we will drop certain column from the datafreme, like availability, society,balcony,availability \n#as they are not important in predicting the price\ndf2=df1.drop(['availability',\"society\"],axis=1)\ndf2.head()","c9f74be2":"#now we will find the number of null values in the dataframe\ndf2.isnull().sum()","1324e9c3":"df2[\"balcony\"].fillna(df2[\"balcony\"].median(),inplace=True)\ndf2[\"bath\"].fillna(df2[\"bath\"].median(),inplace=True)","a56bdb5f":"#now we will drop the null values in the dataframe as they are very less\ndf3=df2.dropna()\ndf3.isnull().sum()","70c7e815":"df3.shape","5e49f5a2":"#the size colums has both categorical as well as numerical data, we will see unique values\ndf3['size'].unique()","6a044541":"#now we will create a separate column BHK containing number of rooms\ndf3['bhk']=df3['size'].apply(lambda x:int(x.split(' ')[0]))","7d523090":"df3.head()","d3e9e76d":"\ndf3['bhk'].unique()\n#here rooms like 27,43 and others seems astronomical","03f5b83d":"#now we will find the outliers\ndf3[df3.bhk>20]\n#the outplut seems like an error","bce812b5":"#now we will find the unique values in the total_sqft column\ndf3.total_sqft.unique()\n\n#the total_sqft has range values, this is a hindrance, we will try to remove that in future codes","896a3313":"def is_float(x):\n    try:\n        float(x)\n    except:\n        return False\n    return True","c67604c4":"#now we will see slice the total_sqft column based on the float\ndf3[~df3['total_sqft'].apply(is_float)].head(10)","f6d7500f":"#now we will convert float into range by averaging the sqft ranges\n","077f19cf":"def convert_sqft_to_num(x):\n    tokens=x.split('-')\n    if len(tokens)==2:\n        return(float(tokens[0])+float(tokens[1]))\/2\n    try:\n        return float(x)\n    except:\n        return None\n    ","cf6d32f6":"#now we will taste the function\nconvert_sqft_to_num('2166')","f2fababb":"convert_sqft_to_num('2100-3000')\n#here the output is average of the range","c8d1228f":"#now we will apply this function to the sqft column and creat a new dataframe\ndf4=df3.copy()\ndf4['total_sqft']=df4['total_sqft'].apply(convert_sqft_to_num)\ndf4.head()","adf0b876":"#now we will start with feature engineering techniques and dimensionality reduction techniques\ndf5=df4.copy()\n#now we will create price per sqft\ndf5['price_per_sqft']=df5['price']*100000\/df5['total_sqft']\ndf5.head()","bbefc174":"#now we will explore location column\nlen(df5.location.unique())","61b8e863":"# there are 1304 features, means this is too much feature\n#this is called dimensionality curse\n#we shall reduce this dimensionality as low as possible\ndf5.location=df5.location.apply(lambda x:x.strip())\nlocation_stats=df5.location.value_counts().sort_values(ascending=False)\nlocation_stats","d6b52e2a":"#now we will find the location with less than 10 datapoints\nlen(location_stats[location_stats<=10])","70eb7f33":"location_stats_less_than_10=location_stats[location_stats<=10]\nlocation_stats_less_than_10","2761de70":"len(df5.location.unique())","aedd5611":"df5.location=df5.location.apply(lambda x: 'other'if x in location_stats_less_than_10 else x )\nlen(df5.location.unique())","9af14f59":"#all the locations with less than 10 properties are converted to other location \ndf5.head(10)","03bbfd22":"#now we will start outlier detections and removal\ndf5[df5.total_sqft\/df5.bhk<300].head()","20f9c653":"df5.shape","5f4d7d63":"#now we will negate the criteria from above dataframe\ndf6=df5[~(df5.total_sqft\/df5.bhk<300)]\ndf6.shape","21fd13cf":"#now we will chaek the outliers in price per sqft\ndf6.price_per_sqft.describe()","1854dd77":"# we will filter the data beyond (mean +1std deviation).\ndef remove_pps_outliers(df):\n    df_out=pd.DataFrame()\n    for key,subdf in df.groupby('location'):\n        m=np.mean(subdf.price_per_sqft)\n        st=np.std(subdf.price_per_sqft)\n        reduced_df=subdf[(subdf.price_per_sqft>(m-st))&(subdf.price_per_sqft<=(m+st))]\n        df_out=pd.concat([df_out,reduced_df],ignore_index=True)\n    return df_out\ndf7=remove_pps_outliers(df6)\ndf7.shape","58d3f666":"def plot_scatter_chart(df,location):\n    bhk2=df[(df.location==location)&(df.bhk==2)]\n    bhk3=df[(df.location==location)&(df.bhk==3)]\n    matplotlib.rcParams['figure.figsize']=(15,10)\n    plt.scatter(bhk2.total_sqft,bhk2.price,color='blue',label='2 BHK',s=50)\n    plt.scatter(bhk3.total_sqft,bhk3.price,marker='+',color='green',label='3 BHK',s=50)\n    plt.xlabel('total square feet area')\n    plt.ylabel('price')\n    plt.title(location)\n    plt.legend()\n    \nplot_scatter_chart(df7,'Hebbal')","4726cef0":"def remove_bhk_outliers(df):\n    exclude_indices = np.array([])\n    for location, location_df in df.groupby('location'):\n        bhk_stats = {}\n        for bhk, bhk_df in location_df.groupby('bhk'):\n            bhk_stats[bhk] = {\n                'mean': np.mean(bhk_df.price_per_sqft),\n                'std': np.std(bhk_df.price_per_sqft),\n                'count': bhk_df.shape[0]\n            }\n        for bhk, bhk_df in location_df.groupby('bhk'):\n            stats = bhk_stats.get(bhk-1)\n            if stats and stats['count']>5:\n                exclude_indices = np.append(exclude_indices, bhk_df[bhk_df.price_per_sqft<(stats['mean'])].index.values)\n    return df.drop(exclude_indices,axis='index')\ndf8 = remove_bhk_outliers(df7)\n# df8 = df7.copy()\ndf8.shape","acbfb267":"plot_scatter_chart(df8,'Hebbal')","a4e19e2d":"#now we will pot a histogram with price per squarefeet\nimport matplotlib\nmatplotlib.rcParams['figure.figsize']=(20,10)\nplt.hist(df8.price_per_sqft,rwidth=0.8)\nplt.xlabel('price per squarefoot')\nplt.ylabel('count')\n","1c49ae13":"#now we will explore bathroom feature\ndf8.bath.unique()","8e0b1685":"df8[df8.bath>10]","967959a3":"plt.hist(df8.bath,rwidth=0.9)\nplt.xlabel('number of bathrooms')\nplt.ylabel('count')","190b552e":"#now we will remove any time we have bathrooms greater than number of bedrooms +2 is an outlier\ndf8[df8.bath>df8.bhk+2]\n","654b64fb":"df9=df8[df8.bath<df8.bhk+2]\ndf9.shape","ee5729fb":"#now we will prepare our dataframe for ML training\n#we shall remove price per squarefoot and size\ndf10=df9.drop(['size','price_per_sqft'],axis='columns')\ndf10.head()","c5cfd728":"#we will perform one hot encoding for location column\ndummies=pd.get_dummies(df10,columns=[\"area_type\",\"location\"],drop_first=True)\ndummies.head()","81644561":"df11=dummies\ndf11.head()","6550b1b1":"df11.shape","0629a537":"#will define dependent variable for training\nX=df11.drop('price',axis='columns')\nX.head()","fea6153e":"#will define independent variable for training\ny=df11.price\ny.head()","215a48d9":"s = preprocessing.StandardScaler()\nX1 = s.fit_transform(X)\nX1","fbae6027":"from sklearn.model_selection import train_test_split\nx1_train,x1_test,y_train,y_test=train_test_split(X1,y,test_size=0.25,random_state=42)\n","888f12e1":"from sklearn.linear_model import LinearRegression\nlr_clf=LinearRegression()\nlr_clf.fit(x1_train,y_train)\nlr_clf.score(x1_test,y_test)","e07509c0":"#now we will perform k fold cross validation\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_score\ncv=ShuffleSplit(n_splits=5,test_size=0.20,random_state=42)\ncross_val_score(LinearRegression(),X1,y,cv=cv)","c6ae7fa8":"#now we will perform grid search cv, here we will try multiple model and select best one with good score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeRegressor\n\ndef find_best_model_using_gridsearchcv(X,y):\n    algos = {\n        'linear_regression' : {\n            'model': LinearRegression(),\n            'params': {\n                'normalize': [True, False]\n            }\n        },\n        'lasso': {\n            'model': Lasso(),\n            'params': {\n                'alpha': [1,2],\n                'selection': ['random', 'cyclic']\n            }\n        },\n        'decision_tree': {\n            'model': DecisionTreeRegressor(),\n            'params': {\n                'criterion' : ['mse','friedman_mse'],\n                'splitter': ['best','random']\n            }\n        }\n    }\n    scores = []\n    cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n    for algo_name, config in algos.items():\n        gs =  GridSearchCV(config['model'], config['params'], cv=cv, return_train_score=False)\n        gs.fit(X,y)\n        scores.append({\n            'model': algo_name,\n            'best_score': gs.best_score_,\n            'best_params': gs.best_params_\n        })\n\n    return pd.DataFrame(scores,columns=['model','best_score','best_params'])\n\nfind_best_model_using_gridsearchcv(X1,y)\n    \n","214e411f":"Looks like we got an average score between 83% with Linear_regression which is quite good with our simple model.","5d2a7a74":"Since all the given inputs are of different types we need to apply Standard scalar to make mu=0 and Sigma=1","47694773":"Lets see the distribution of NAN values in the DataFrame"}}