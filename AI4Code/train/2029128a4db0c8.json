{"cell_type":{"b88d8980":"code","c7de7f5d":"code","5ca60e6e":"code","13390b9c":"code","9b690007":"code","702a182b":"code","9b1e9b4c":"code","9a34154a":"code","99db5195":"code","0efbf8da":"code","9cad91ca":"code","52044153":"code","d9210587":"code","1da60938":"code","66747b26":"code","06bc985d":"code","a0a39619":"code","4da6124b":"code","667aebc0":"code","48aa8367":"code","0e0f4821":"code","05034085":"code","1bc0e62f":"code","08640484":"code","0a9e46ab":"code","19114dcd":"markdown","68370f61":"markdown","a95626df":"markdown","5a39a551":"markdown","7942f1c7":"markdown","a42c57c9":"markdown","2c7d33a3":"markdown","037125e0":"markdown","92f66dbe":"markdown","f5a639b0":"markdown","647aab1e":"markdown"},"source":{"b88d8980":"%%capture\n\n%config IPCompleter.greedy=True\nimport pandas as pd\n# import seaborn as sns\nimport numpy as np\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\n!pip install catboost\nimport xgboost  # xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.XGBClassifier\nimport lightgbm # lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html\nimport catboost # catboost.ai\/docs\/concepts\/python-quickstart.html\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom collections import Counter\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c7de7f5d":"%%bash\nwget \"https:\/\/github.com\/alik604\/cyber-security\/raw\/master\/Intrusion-Detection\/KDD%20cup%20'99\/kddcup.data.7z\"\n# repeat after me, GitHub is a Content Delivery Network\n\nsudo apt-get install p7zip-full\n7z x kddcup.data.7z\n\nls","5ca60e6e":"columns = [\"duration\" , \"protocol_type\" , \"service\" , \"flag\" , \"src_bytes\" , \"dst_bytes\" , \"land\" , \"wrong_fragment\" , \"urgent\" , \"hot\" , \"num_failed_logins\" , \"logged_in\" , \"num_compromised\" ,\n           \"root_shell\" , \"su_attempted\" , \"num_root\" , \"num_file_creations\" , \"num_shells\" , \"num_access_files\" , \"num_outbound_cmds\" , \"is_host_login\" , \"is_guest_login\" , \"count\" , \"srv_count\" ,\n           \"serror_rate\" , \"srv_serror_rate\" , \"rerror_rate\" , \"srv_rerror_rate\" , \"same_srv_rate\" , \"diff_srv_rate\" , \"srv_diff_host_rate\" , \"dst_host_count\" , \"dst_host_srv_count\" , \"dst_host_same_srv_rate\" ,\n           \"dst_host_diff_srv_rate\" , \"dst_host_same_src_port_rate\" , \"dst_host_srv_diff_host_rate\" , \"dst_host_serror_rate\" , \"dst_host_srv_serror_rate\" , \"dst_host_rerror_rate\" , \"dst_host_srv_rerror_rate\" , \"xAttack\"]\n\nall = pd.read_csv('.\/kddcup.data.csv', names=columns, index_col=0)\n\nall.head(5)","13390b9c":"X_train, X_test, y_train, y_test = train_test_split(all.drop(['xAttack'], axis=1), all['xAttack'], test_size=0.66, random_state=42)","9b690007":"# X_train=X_train.sample(frac =.50,random_state=1) # TODO. now this is uneeded as I used `train_test_split` above \n\n# X_train = train.drop('xAttack', axis=1)\n# Y_train = train.loc[:,['xAttack']]\n# X_test = test.drop('xAttack', axis=1)\n# Y_test = test.loc[:,['xAttack']]","702a182b":"le = preprocessing.LabelEncoder()\n\nprint(set(list(y_train)))\nprint(Counter(all['xAttack']))\n\ny_train = le.fit_transform(y_train)\ny_test  = le.fit_transform(y_test)\n\n\nfor col in ['protocol_type', 'service', 'flag']:\n  X_train[col] = le.fit_transform(X_train[col])\n  X_test[col] = le.fit_transform(X_test[col])\n\nX_train.head(5)","9b1e9b4c":"# print('\\n')\n# tmp = all.corr().abs()#.sort_values('xAttack')\n# garbage = all['xAttack'].nsmallest(5)\n# garbage\n# garbage = list(garbage.index) + ['num_outbound_cmds','is_host_login']","9a34154a":"# df.ix[2,:]==0 # see where STD== 0, which means all values are the same... which is useless \n\n# con_list = [\n#     'protocol_type', 'service', 'flag', 'land', 'logged_in', 'su_attempted',\n#     'is_host_login', 'is_guest_login'\n# ]\n\n# df = X_train.drop(con_list, axis=1)\n\n# #drop n smallest std features\ndf = X_train.std(axis=0).to_frame()\ntmp = df.nsmallest(5, columns=0)\ntmp = tmp.index.to_list() # list(tmp.transpose().columns)\ntmp\n\n# print(set(tmp + garbage))\n# print(to_drop, len(to_drop))","99db5195":"from sklearn import linear_model\n\nfrom sklearn.ensemble import VotingClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import IsolationForest\n\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier","0efbf8da":"LR = linear_model.LinearRegression()\nLR.fit(X_train, y_train)\nlr_score = LR.score(X_test, y_test)\nprint('Linear regression processing ,,,')\nprint('Linear regression Score: %.2f %%' % lr_score)","9cad91ca":"## to remove low Corr & low STD \n\n# try:  #TODO\n#     X_train = X_train.drop(tmp,axis=1)\n#     X_test = X_test.drop(tmp,axis=1)\n# except:\n#     None\n    \n# X_train.shape\n# X_test.shape","52044153":"AB = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth = 3), n_estimators=50)\nRF = RandomForestClassifier(n_estimators=100, criterion='entropy')\nET = ExtraTreesClassifier(n_estimators=100, criterion='gini', bootstrap=False)\nGB = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=200, max_features='auto')","d9210587":"_ = AB.fit(X_train[:1000], y_train[:1000])\nAB_feature = AB.feature_importances_\nab_score = AB.score(X_test, y_test)\n\nprint('AdaBoostClassifier processing...')\nprint('AdaBoostClassifier Score: %.3f %%' % ab_score)","1da60938":"_ = RF.fit(X_train, y_train)\nRF_feature = RF.feature_importances_\nrf_score = RF.score(X_test, y_test)\n\nprint('RandomForestClassifier processing...')\nprint('RandomForestClassifier Score: %.3f %%' % rf_score)","66747b26":"_ = ET.fit(X_train, y_train)\nET_feature = ET.feature_importances_\net_score = ET.score(X_test, y_test)\n\nprint('ExtraTreesClassifier processing...')\nprint('ExtraTreeClassifier: %.3f %%' % et_score)","06bc985d":"_ = GB.fit(X_train[:2000], y_train[:2000])\nGB_feature = GB.feature_importances_\ngb_score = GB.score(X_test, y_test)\n\nprint('GradientBoostingClassifier processing...')\nprint('GradientBoostingClassifier Score: %.3f %%' % gb_score)","a0a39619":"feature_df = pd.DataFrame({'features': X_train.columns.values, # names\n                           'AdaBoost' : AB_feature,\n                           'RandomForest' : RF_feature,\n                           'ExtraTree' : ET_feature,\n                           'GradientBoost' : GB_feature\n                          })\n#feature_df.features\nfeature_df.head(10)","4da6124b":"n = 5\na_f = feature_df.nlargest(n, 'AdaBoost')\ne_f = feature_df.nlargest(n, 'ExtraTree')\ng_f = feature_df.nlargest(n, 'GradientBoost')\nr_f = feature_df.nlargest(n, 'RandomForest')\n\nresult = pd.concat([a_f, e_f, g_f, r_f])\nresult.drop_duplicates(inplace=True)\n\nresult['features'].values\n\ntop_features = ['dst_host_srv_diff_host_rate', 'srv_count', 'src_bytes', 'hot',\n       'dst_host_diff_srv_rate', 'same_srv_rate', 'dst_host_serror_rate',\n       'protocol_type', 'service', 'flag', 'dst_host_same_src_port_rate',\n       'count', 'dst_host_srv_count', 'logged_in',\n       'dst_host_srv_serror_rate', 'dst_host_same_srv_rate',\n       'serror_rate', 'diff_srv_rate', 'srv_serror_rate']\n\n# garbage = np.argsort(result.transpose().mean())\n# garbage = garbage.sort_index()[:5] # FML... :'(\n# garbage.index\n\n# result = result.drop(garbage.index)\n# result.shape\n\n# arr = X_train.columns.to_numpy()\n# result = result.set_index(np.take(arr,result.index))","667aebc0":"# X_train_SF = X_train[result.index]\n# X_test_SF = X_test[result.index]\n\n\nprint(X_train.shape)\n\n\nselected_features = result['features'].values.tolist()\nX_train_SF = X_train[selected_features]\nX_test_SF = X_test[selected_features]\n\n\n\n# X_train_SF = X_train_SF[:20000]\n# y_train    = y_train[:20000]\n\n\nprint(X_train_SF.shape)\nprint(y_train.size)\n\nnumb_of_train_points = 410000","48aa8367":"DTC = DecisionTreeClassifier()\nRFC = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=-1)\nETC = ExtraTreesClassifier(n_estimators=150, random_state=42, n_jobs=-1)\nXGB = xgboost.XGBClassifier(n_estimators=100, n_jobs=-1)\nGBM = lightgbm.LGBMClassifier(objective='multiclass', n_estimators= 400)\n\nlist_of_CLFs_names = []\nlist_of_CLFs = [DTC, RFC, ETC, XGB, GBM]\nranking = []\n\nfor clf in list_of_CLFs:\n    _ = clf.fit(X_train[:numb_of_train_points],y_train[:numb_of_train_points])\n    pred = clf.score(X_test,y_test)\n    name = str(type(clf)).split(\".\")[-1][:-2]\n    print(\"Acc: %0.5f for the %s\" % (pred, name))\n\n    ranking.append(pred)\n    list_of_CLFs_names.append(name)","0e0f4821":"DTC = DecisionTreeClassifier()\nRFC = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=-1)\nETC = ExtraTreesClassifier(n_estimators=200, random_state=42, n_jobs=-1)\nXGB = xgboost.XGBClassifier(n_estimators=100, n_jobs=-1)\nGBM = lightgbm.LGBMClassifier(objective='multiclass', n_estimators= 400)\n\nlist_of_CLFs_names = []\nlist_of_CLFs = [DTC, RFC, ETC, XGB, GBM]\nranking = []\n\nfor clf in list_of_CLFs:\n    _ = clf.fit(X_train_SF[:numb_of_train_points],y_train[:numb_of_train_points])\n    pred = clf.score(X_test_SF,y_test)\n    name = str(type(clf)).split(\".\")[-1][:-2]\n    print(\"Acc: %0.5f for the %s\" % (pred, name))\n\n    ranking.append(pred)\n    list_of_CLFs_names.append(name)","05034085":"from sklearn.decomposition import * \nwanted_explained_variance_ratio = 0.99\nsteps_down = 4\nwanted_n_components = 5\nfirst_time = True\n\nfor i in range(X_train.shape[1]-1, 1, -steps_down):\n  total_var_ratio = round(np.sum(PCA(n_components=i).fit(X_train).explained_variance_ratio_), 5)\n  print('i =', i, 'with a variance ratio of', total_var_ratio)\n  if total_var_ratio < wanted_explained_variance_ratio and first_time:\n    wanted_n_components = i + steps_down\n    first_time = False\n\nprint(\"We should set n_components to: \", wanted_n_components, \" is that our default choice?\",  first_time)\n\npca = PCA(n_components=wanted_n_components) # \n_ = pca.fit(X_train)\nX_train_PCA = pca.transform(X_train)\nX_test_PCA = pca.transform(X_test)\n\n##=================================================================\nDTC = DecisionTreeClassifier()\nRFC = RandomForestClassifier(n_estimators=150, random_state=42, n_jobs=-1)\nETC = ExtraTreesClassifier(n_estimators=200, random_state=42, n_jobs=-1)\nXGB = xgboost.XGBClassifier(n_estimators=100, n_jobs=-1)\nGBM = lightgbm.LGBMClassifier(objective='multiclass', n_estimators= 400)\n\nlist_of_CLFs_names = []\nlist_of_CLFs = [DTC, RFC, ETC, XGB, GBM]\nranking = []\n\nfor clf in list_of_CLFs:\n    _ = clf.fit(X_train_PCA[:numb_of_train_points], y_train[:numb_of_train_points])\n    pred = clf.score(X_test_PCA, y_test)\n    name = str(type(clf)).split(\".\")[-1][:-2]\n    print(\"Acc: %0.5f for the %s\" % (pred, name))\n\n    ranking.append(pred)\n    list_of_CLFs_names.append(name)\n","1bc0e62f":"# import multiprocessing\n\n# ET = ExtraTreesClassifier(n_estimators=10, criterion='gini', max_features='auto', bootstrap=False) # .76 \n# RF = RandomForestClassifier(n_estimators=25, random_state=1)# .75\n# GB = GradientBoostingClassifier() # .74\n# ET = ExtraTreesClassifier(n_estimators=10, criterion='gini', max_features='auto', bootstrap=False) # .77 # without this lil fucker, Acc: 0.75 [Ensemble]\n# clfList = [ET,RF,GB,ET]\n\n# def spawn(clf):\n#   clf.fit(x,y)\n#   print(\"Done another one!\")\n\n# import time\n# start = time.time()\n\n# if __name__ == '__main__':\n  \n#   for i in clfList:\n#     print(i)\n#     #spawn(i) # 16 secounds \n\n#     # # 16 secounds\n#     # p = threading.Thread(target=spawn, args=(i,))\n#     # p.start()\n#     # p.join()\n    \n#   p=multiprocessing.Pool(6) # 15.65\n#   results = p.map(spawn,clfList) # clfList has 4 models, first 2 are fast, last 2 are slow\n#   results\n\n\n# end = time.time()\n# print(end - start)\n","08640484":"# ET.score(X_test_SF,Y_test)\n","0a9e46ab":"# from sklearn.cluster import KMeans\n# for i in range(10):\n#     tmp = KMeans(i)    \n#     tmp = clf.fit(x,y)\n#     tmp = tmp.score(X_test_SF,Y_test)\n#     print(i,tmp)\n\n","19114dcd":"## I do NOT use Kaggle. up to data code can be found @ https:\/\/github.com\/alik604\/cyber-security\/tree\/master\/Intrusion-Detection","68370f61":"# Intrusion Detection \n","a95626df":"### Remove `selected_features` from our Data","5a39a551":"* https:\/\/stackoverflow.com\/questions\/42920148\/using-sklearn-voting-ensemble-with-partial-fit\n\n* https:\/\/gist.github.com\/tomquisel\/a421235422fdf6b51ec2ccc5e3dee1b4\n\n","7942f1c7":"### Reduce train size for faster trainin, remove when in production","a42c57c9":"### The following few cells are taken from the 'sample code'","2c7d33a3":"nope... that didnt end well. ","037125e0":"## lets multithread this","92f66dbe":"<a href=\"https:\/\/colab.research.google.com\/github\/alik604\/cyber-security\/blob\/master\/Intrusion-Detection\/KDD%20cup%20'99\/Intrusion%20Detection%20V2%2099.99acc%20Multiclass_CLF.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","f5a639b0":"## Drop features with lowest STD","647aab1e":"## Drop features with lowest corr"}}