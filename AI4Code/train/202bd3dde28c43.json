{"cell_type":{"2a83500f":"code","a8ae6608":"code","6db7b32b":"code","7b92db41":"code","bde247da":"code","324f32db":"code","57086f20":"code","39de25ae":"code","8cf9eb9b":"code","9294ae18":"code","b7fbdb8c":"code","69dd611a":"code","d081858f":"code","a6c90656":"code","7ac27cec":"code","753d3d3e":"code","746caed7":"code","0fb3329c":"code","79da5085":"code","988ca8af":"code","7440b390":"code","2deead6f":"code","5fcce478":"code","331450f3":"code","5c7eaba7":"code","6e5f7c18":"code","d5cf472f":"code","b85215a8":"code","ac3a11cb":"code","868ccba7":"code","dc29b672":"markdown","d40b0619":"markdown","2c28d8b9":"markdown","23621715":"markdown","8ab8b451":"markdown","8e0b6099":"markdown","812792f2":"markdown","ed7123b8":"markdown","df82ec52":"markdown","aea67834":"markdown","b7e67baa":"markdown","c126f7b1":"markdown","3b093f92":"markdown","f07b0d41":"markdown","4b8d98c7":"markdown","61ee27d1":"markdown","c7980ff1":"markdown","406b7795":"markdown","89a34f05":"markdown"},"source":{"2a83500f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","a8ae6608":"train_data = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv\")","6db7b32b":"train_data.head()","7b92db41":"train_data.info()","bde247da":"train_data.describe()","324f32db":"cols = list(train_data.columns)\ncols.remove(\"id\")\ncols.remove(\"target\")\nclasses = [f\"Class_{i}\" for i in range(1, 10)]","57086f20":"import seaborn as sns\nimport matplotlib.pyplot as plt","39de25ae":"import cufflinks as cf # for using plotly with pandas\ncf.go_offline()\ncf.set_config_file(offline=False, world_readable=True)","8cf9eb9b":"train_data.describe().loc[\"max\", cols].iplot(kind=\"bar\")","9294ae18":"fig, ax = plt.subplots(figsize=(25, 6))\nsns.boxplot(data=train_data.drop(\"id\", axis=1), ax=ax)","b7fbdb8c":"for i in range(0, 50, 20):\n    col = f\"feature_{i}\"\n    data = train_data[[col, \"target\"]]\n    #data = data[data[col] != 0]\n    sns.countplot(x=col, data=data, #hue='target', #multiple='stack'\n               )\n    plt.yscale(\"log\")\n    plt.show()","69dd611a":"train_data.groupby(\"target\")[\"target\"].count()","d081858f":"sns.heatmap(train_data.drop([\"id\", ], axis=1).corr())","a6c90656":"from sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MaxAbsScaler, OneHotEncoder\nfrom sklearn.metrics import log_loss\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.dummy import DummyClassifier","7ac27cec":"def test_model(model):\n    model.fit(X_train, y_train)\n    stats = just_test_model(model)\n    return stats\n\ndef just_test_model(model):\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n    loss = log_loss(y_test, model.predict_proba(X_test))\n    stats = {\"train\": train_score, \"test\": test_score, \"loss\": loss}\n    return stats","753d3d3e":"def test_model_partial(model, X, y, select=slice(None)):\n    X = X[select]\n    y = y[select]\n    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n    model.fit(X_train, y_train)\n    train_score = model.score(X_train, y_train)\n    test_score = model.score(X_test, y_test)\n    loss = log_loss(y_test, model.predict_proba(X_test))\n    stats = {\"train\": train_score, \"test\": test_score, \"loss\": loss}\n    return stats","746caed7":"def predict_proba_df(model, X, labels=classes):\n    proba = model.predict_proba(X)\n    proba = pd.DataFrame(proba, columns=labels)\n    return proba\n\ndef plot_class_distribution(model, X_test, y_test, labels=classes):\n    proba = predict_proba_df(model, X_test, labels=labels)\n    proba[\"target\"] = y_test.values\n    plot_class_proba_distribution(proba)\n\ndef plot_class_proba_distribution(proba_df):\n    fig, axes = plt.subplots(1, 4, figsize=(20, 2))\n    for (real_val, group), ax in zip(proba_df.groupby(\"target\"), axes):\n        group.pop(\"target\")\n        sns.kdeplot(data=group, fill=True, ax=ax)\n        ax.set_title(real_val)\n        ax.set_ylim(top=10)\n    plt.show()","0fb3329c":"y = train_data.target\nX = train_data.drop([\"target\", \"id\"], axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)","79da5085":"dummy = DummyClassifier(strategy=\"prior\")\nprint(\"prior dummy\", test_model(dummy))","988ca8af":"all_data = pd.concat((train_data, test_data))[cols]\ncategories = [sorted(all_data[col].unique()) for col in all_data]","7440b390":"models = {\"LogReg\": Pipeline([(\"scaler\", StandardScaler()), \n                             (\"model\", LogisticRegression())]),\n                                     }\nfor C in [0.001, 0.01, 0.1, 1.0, 10]:\n    models[f\"LogReg_ohe_C{C}\"] =  Pipeline([(\"ohe\", OneHotEncoder(#drop=\"first\", \n                                                       categories=categories, dtype=\"int\")), \n                             (\"model\", LogisticRegression(max_iter=150, C=C))])\nfor name, model in models.items():\n    print(name, test_model(model))","2deead6f":"def loss_per_class(model):\n    test_data = X_test.copy()\n    test_data[\"target\"] = y_test\n    for idx, group in test_data.groupby(\"target\"):\n        loss = log_loss(group.target, model.predict_proba(group.drop(columns=[\"target\"])), labels=classes)\n        print(idx, \" loss\", loss)\n\nloss_per_class(models[\"LogReg_ohe_C0.01\"])\nplot_class_distribution(models[\"LogReg_ohe_C0.01\"], X_test, y_test)","5fcce478":"%%time\nmodel =  Pipeline([(\"ohe\", OneHotEncoder(categories=categories, dtype=\"int\", sparse=False)),\n                   (\"model\", LogisticRegression(max_iter=200, C=0.01, \n                                                class_weight=\"balanced\",\n                                               ))])\nmodels[\"ohe_log_weighted\"] = model\nmodel = model.fit(X_train, y_train)\njust_test_model(model)\nloss_per_class(model)\nplot_class_distribution(model, X_test, y_test)","331450f3":"bool_array = X.nunique() < 10\ncat_cols = list(X.columns[bool_array])\ncat_cols_idx = np.flatnonzero(bool_array)\n#len(cat_cols_idx)","5c7eaba7":"selected_cats = list(np.array(categories, dtype='object')[cat_cols_idx])\nohe = OneHotEncoder(categories=selected_cats, dtype=\"int\", drop=\"first\", sparse=False)\nct_ohe = ColumnTransformer([(\"ohe\", ohe, cat_cols_idx)],\n                            remainder=\"passthrough\"\n                                              )\nmodel = Pipeline([(\"ct_ohe\", ct_ohe), \n                  (\"scaler\", StandardScaler()),\n                    (\"model\", LogisticRegression(max_iter=100, C=C))])\ntest_model(model)","6e5f7c18":"%%time\nmodel = GradientBoostingClassifier()\nmodels[\"gradboost\"] = model\ntest_model(model)","d5cf472f":"%%time\nforest = RandomForestClassifier(max_depth=10)\nmodels[\"forest\"] = forest\nprint(\"forest\", test_model(forest))","b85215a8":"%%time\nvoter_list = [\"gradboost\", \"forest\", \"LogReg\", \"LogReg_ohe_C0.01\", \"ohe_log_weighted\"]\nvoters = [(voter, models[voter]) for voter in voter_list]\nsc = StackingClassifier(voters)\ntest_model(sc)","ac3a11cb":"def predict_test_data(model):\n    model.fit(X, y)\n    proba = model.predict_proba(test_data.drop([\"id\"], axis=1))\n    predicted = pd.DataFrame(proba, columns=classes)\n    predicted[\"id\"] = test_data.id\n    predicted = predicted[[\"id\"]+classes]\n    return predicted","868ccba7":"predicted = predict_test_data(sc)\npredicted.to_csv(\"stacking_model.csv\", index=False)","dc29b672":"Doesn't look like the features are correlated (although I don't know if all the zeros in the data affects this).\n\nI'm not really sure whether we can eliminate some of the features, or do feature engineering, so I will just use all the features as is in the following","d40b0619":"Below I try out logistic regression considering the features as numerical using `StandardScaler` and as categorical using `OneHotEncoder`.","2c28d8b9":"From the loss it appears that the one hot encoded model did better than considering the features as numerical, although neither seems to improve much compared to the priors dummy. The logistic regressions failed to converge, but I'm choosing to ignore this since they got pretty good scores.\n\nLet's look at whether the imbalance of the dataset might be an issue","23621715":"None of these classifiers did particularly well. Let's just stack them all together...","8ab8b451":"Below I build a list of categories to be used by the one hot encoder. I'm including the test data in case there is a label in there not present in the train data","8e0b6099":"As a baseline I start with a dummy classifier that just predicts probabilities based on how often each class occurs in the training data","812792f2":"As a start I just copied over my notebook from the yabular playground may competition https:\/\/www.kaggle.com\/jenssvensmark\/scikit-stacking-tabular-play-may, adopted to the number of classes in this new dataset, and ran it as is.","ed7123b8":"# Load and inspect data","df82ec52":"Above I just plotted a few selected features. The count appears to change continously with each feature.\n\nAll the features are integers, but there are a lot of features, and a lot of values of each features. I have no idea if these features are categorical, ordinal or numerical.","aea67834":"So the model is heavily imbalanced towards predicting `Class_6`. Let's see what happens if we apply class weights to even this out","b7e67baa":"Imports and a couple of utility functions","c126f7b1":"Not particularly...\n\nOkay, let's just try a bunch of classifiers","3b093f92":"The dataset is quite imbalanced, there are a lot of `Class_2`, and fewer of the other three classes.","f07b0d41":"So, now losses are the same across the classes, but the total loss is somewhat larger than the previous models.","4b8d98c7":"Below I test if assuming that features with less than 10 elements are categorical and that the remainder are numerical would work well","61ee27d1":"... and submit that","c7980ff1":"No missing data, all the features are integers, except the target.\nThere are a lot of features here! And looks like a lot of zeros as well.\nLet's check out the range of values in each feature","406b7795":"# EDA","89a34f05":"# scikit models"}}