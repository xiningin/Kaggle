{"cell_type":{"d132fa5c":"code","c84be671":"code","00e811e2":"code","8ed4957a":"code","7439b510":"code","2d261945":"code","9786b55f":"code","5a402ddd":"code","ff3b6ba2":"code","516701d4":"code","b7b98a66":"code","1d95c00c":"code","fd44176b":"code","534df722":"code","819b4a20":"code","2747993e":"code","bc066a70":"code","a473254a":"code","a2c9babc":"code","14aae37d":"code","7a2e2e50":"code","042a57ee":"code","5bdf422a":"code","ebb86885":"code","f571cce7":"code","437d4e18":"code","a5330153":"code","54ba7a50":"code","d0101f78":"code","38303d60":"code","4a8e3fb7":"code","64fd3a55":"code","c6c5b63c":"code","99a86494":"code","438cf93c":"code","94d5681a":"code","f5cb98b4":"code","71868991":"code","b0c32e71":"code","dd06b1e7":"code","a50abfb8":"code","98e9686b":"markdown","05925be0":"markdown","d4313ef8":"markdown","c9371060":"markdown","85dee9c8":"markdown","73a17207":"markdown","fe93b519":"markdown","2e835737":"markdown","191d3f5d":"markdown","c06b563f":"markdown","7238608b":"markdown","fb987c34":"markdown","0125f4be":"markdown","f144e52e":"markdown","a121fb53":"markdown","74e21584":"markdown","6ce5a22f":"markdown","82062513":"markdown","49b2489c":"markdown"},"source":{"d132fa5c":"%%capture                      \n#delete the first line to see this cells output\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\n%matplotlib inline\nimport seaborn as sns \nfrom scipy.sparse import csr_matrix, hstack\nfrom scipy.stats import probplot\nimport re\nfrom category_encoders import TargetEncoder\n!pip install transformers\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport torch\nimport transformers as ppb\nfrom transformers import DistilBertTokenizer\nimport warnings\nimport gc\nimport tensorflow as tf\n\n#installig NVIDEA Apex for running Bert on GPU\n! pip install -v --no-cache-dir --global-option=\"--cpp_ext\" --global-option=\"--cuda_ext\" ..\/input\/nvidiaapex\/repository\/NVIDIA-apex-39e153a\n\n#assigning GPU to Apax package\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ncolor = sns.color_palette()\nsns.set_style(\"whitegrid\")\nsns.set_context(\"paper\")\nsns.palplot(color)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c84be671":"train = pd.read_csv('..\/input\/adams-dataprep-eda\/train_processed.csv')\ntest = pd.read_csv('..\/input\/adams-dataprep-eda\/test_processed.csv')\nSample_submission = pd.read_csv('..\/input\/adams-df\/ADAMS_NLPtask_SS20\/Sample_submission.csv')","00e811e2":"test.info()","8ed4957a":"#log transform the target variable because it is scewed towards low values as shown in the EDA\ntrain['totalClapCount'] = np.log1p(train['totalClapCount'])\n\n#np.expm1() for reversing log transformation","7439b510":"#change to dtype string as tokenizers require\ntrain['text'] = train['text'].astype(str)\ntrain['title'] = train['title'].astype(str)\ntest['text'] = train['text'].astype(str)\ntest['Header'] = test['Header'].astype(str)\n\n#validation dataset for every model\ntrain_test = train.sample(5000, random_state = 45)\ntrain = train.drop(labels = train_test.index, errors = 'ignore')\n\n#for generating CLS-tokens from text variable\ntrain_text = train.sample(10000, random_state = 22)\n\n#for generating CLS-tokens from header variable\ntrain_header = train.sample(20000, random_state = 777)","2d261945":"def get_cls(max_length, text_source): \n    #pipeline for converting text data to CLS-tokens, concept and implementation partly from source [2]\n    \n    #initialize Bert tokenizer\n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True, add_special_tokens=True,\n                                                max_length=max_length, pad_to_max_length=True)\n    #initialize DistilBertModel\n    model = ppb.DistilBertModel.from_pretrained('distilbert-base-uncased')\n    \n    #save model on GPU\n    model.to(device)\n    \n    \n    ###tokenize, padding, attention_mask, CLS tokens, \n    encoded_dict = tokenizer.batch_encode_plus(\n                    text_source,                       # Sentence to encode.\n                    add_special_tokens = True,      # Add '[CLS]' and '[SEP]'\n                    max_length = max_length,                # Pad & truncate all sentences.\n                    padding=True,\n                    pad_to_max_length=True,                     \n                    return_attention_mask = True,   # Construct attn. masks.\n                    return_tensors = 'pt',          # Return pytorch tensors.\n                    )\n\n    #put encoded_dict on GPU for faster\/more memory efficient processing\n    encoded_dict.to(device)\n   \n\n    #model = model.train()\n\n    with torch.no_grad():\n        last_hidden_states = model(encoded_dict.input_ids, attention_mask= encoded_dict.attention_mask)\n    \n    \n    #read CLS-tokens from the last layer before the output layer\n    features = last_hidden_states[0][:,0,:]\n    \n    \n    #print(\"features extracted\", flush=True)\n    \n    #free up memory to avoid crashing the notebook due to RAM overflow\n    del last_hidden_states\n    del model\n    del encoded_dict\n    gc.collect()\n    return features","9786b55f":"def append_cls_text(text, counter, factor):\n\n    #cocatenate multiple bert outputs\n    g = get_cls(450, train.text[counter : counter + 500]).cpu()\n    counter += 500\n    for i in range(factor):\n        g = np.concatenate((g, get_cls(450, text[counter  : counter + 500]).cpu() ), 0)\n        counter += 500\n    return g","5a402ddd":"#generate cls tokens for training\nfit_text = append_cls_text(train_text['text'], 0, 19)","ff3b6ba2":"#generate cls tokens for testing\npredict_text = append_cls_text(train_test['text'], 0, 9)","516701d4":"torch.cuda.empty_cache()\ngc.collect()","b7b98a66":"my_model = XGBRegressor(eval_metric= 'mae')\nmy_model.fit(fit_text, train_text['totalClapCount'], verbose=False)\ntext_preds = my_model.predict(predict_text)\nprint(mean_absolute_error(np.expm1(train_test['totalClapCount']), np.expm1(text_preds)))","1d95c00c":"print(mean_squared_error(np.expm1(train_test['totalClapCount']), np.expm1(text_preds)))","fd44176b":"forest_model = RandomForestRegressor(criterion='mse', random_state=9)\nforest_model.fit(fit_text, train_text['totalClapCount'])\ntext_preds = forest_model.predict(predict_text)\n    \nprint(mean_absolute_error(np.expm1(train_test['totalClapCount']), np.expm1(text_preds)))\nprint(np.expm1(train_test['totalClapCount']), np.expm1(text_preds))","534df722":"print(mean_squared_error(np.expm1(train_test['totalClapCount']), np.expm1(text_preds)))","819b4a20":"features_sub = get_cls(450, test['text'])\n\n\nfeatures_sub = features_sub.cpu()\nfeatures_sub = features_sub.numpy()\n\ntext_sub = my_model.predict(features_sub)\n\n#map negative predictions to zero\n#text_sub = text_sub.clip(min = 0)","2747993e":"def append_cls_header(text, counter, factor):\n\n    #cocatenate multiple bert outputs:\n    g = get_cls(100, train.text[counter : counter + 2500]).cpu()\n    counter += 2500\n    for i in range(factor):\n        g = np.concatenate((g, get_cls(100, text[counter  : counter + 2500]).cpu() ), 0)\n        counter += 2500\n    return g","bc066a70":"torch.cuda.empty_cache()\ngc.collect()","a473254a":"#header predictions\nfit_header = append_cls_header(train_header['title'], 0, 7)\npredict_header = append_cls_header(train_test['title'], 0, 1)","a2c9babc":"my_model = XGBRegressor()\nmy_model.fit(fit_header, train_header['totalClapCount'], verbose=False)\nheader_preds = my_model.predict(predict_header)\nprint(mean_absolute_error(np.expm1(train_test['totalClapCount']), np.expm1(header_preds)))","14aae37d":"print(mean_squared_error(np.expm1(train_test['totalClapCount']), np.expm1(header_preds)))","7a2e2e50":"features_title = get_cls(100, test['Header'])\nfeatures_title = features_title.cpu()\nfeatures_title = features_title.numpy()\n\nheader_sub = my_model.predict(features_title)\n\n#map negative values to zero\n#header_sub = header_sub.clip(min = 0)","042a57ee":"te = TargetEncoder()\nX_target_encoded = te.fit(train['author'], train['totalClapCount'], handle_missing='return_nan', handle_unknown='return_nan')\n\ntest['author'] = X_target_encoded.transform(test['author'], y=None, override_return_df=False, )\n\ntest['author'].value_counts()\n\ntrain['author'] = X_target_encoded.transform(train['author'], y=None, override_return_df=False,)\ntrain_test['author'] = X_target_encoded.transform(train_test['author'], y=None, override_return_df=False,)","5bdf422a":"features_num = ['responsesCreatedCount', 'year_2017', 'year_2018', 'author']\nlabels_num = train['totalClapCount']","ebb86885":"features_num_sub = ['Responses', 'year_2017', 'year_2018', 'author']","f571cce7":"forest_model = RandomForestRegressor(criterion='mse')\nforest_model.fit(train[features_num], labels_num)\nnum_preds = forest_model.predict(train_test[features_num])\n\n#map negative predictins to zero\n#num_preds = num_preds.clip(min = 0)\nprint(mean_absolute_error(np.expm1(train_test['totalClapCount']), np.expm1(num_preds)))","437d4e18":"print(mean_squared_error(np.expm1(train_test['totalClapCount']), np.expm1(num_preds)))","a5330153":"#predicting on test features\nforest_model.fit(train[features_num], train['totalClapCount'])\n\nnum_sub = forest_model.predict(test[features_num_sub])\n\n#map negative predictions to zero\n#num_sub = num_sub.clip(min = 0)","54ba7a50":"ensemble = [text_preds, header_preds, num_preds]","d0101f78":"ensemble_features = pd.DataFrame(data = ensemble)\nensemble_features = ensemble_features.transpose()","38303d60":"train_features, test_features, train_labels, test_labels = train_test_split(ensemble_features, train_test['totalClapCount'], random_state = 16)\n    \nforest_model = RandomForestRegressor(n_estimators=110, criterion='mae', max_depth=None, min_samples_split=2, min_samples_leaf=1)\nforest_model.fit(train_features, train_labels)\nensemble_preds = forest_model.predict(test_features)\nprint(mean_absolute_error(np.expm1(test_labels), np.expm1(ensemble_preds)))","4a8e3fb7":"print(mean_squared_error(np.expm1(test_labels), np.expm1(ensemble_preds)))","64fd3a55":"ensemble_preds = np.expm1(ensemble_preds)","c6c5b63c":"#mapping all values below 50 is an attempt to fit the predictions to the distribution ot the target variable \n\n#clap = []\n#for i in ensemble_preds:\n#    if i < 50:\n#        clap.append(0)\n#    else:\n#        clap.append(i)  \n\n#print(mean_absolute_error(np.expm1(test_labels), clap))","99a86494":"forest_model = RandomForestRegressor()\nforest_model.fit(ensemble_features, train_test['totalClapCount'])","438cf93c":"ensemble = [text_sub, header_sub, num_sub]\nensemble_features = pd.DataFrame(data = ensemble)\nensemble_features = ensemble_features.transpose()","94d5681a":"ensemble_features.info()","f5cb98b4":"ensemble_features.columns = ['text_sub', 'header_sub', 'num_sub' ]\nensemble_features","71868991":"ensemble_preds = forest_model.predict(ensemble_features)\nensemble_preds = np.expm1(ensemble_preds)","b0c32e71":"#round to integer because claps cannot be float\nensemble_preds = [round(x) for x in ensemble_preds]","dd06b1e7":"Sample_submission['Claps'] = ensemble_preds\nSample_submission.drop(labels = ['index'], axis = 1)","a50abfb8":"Sample_submission.to_csv('Sample_submission.csv',index=False)","98e9686b":"# Making predictions on the test dataset\nNow we will combine the predictions of the text, header and numeric value models for a final prediction. We can combine the three lists to a pandas data frame and use the columns as input features for a RF-Regressor. We are using the predictions and features from the *train* dataset to train\/fit the model and then use the predictions from the *test* dataset to make final predictions for the submission\n\nThe test\/header\/numeric predictions contain some negative values. However, negative values don't cause the ensembling model to predict negative values. If we map the negative values to zero, the accuracy decreases from an MAE of 81 to an MAE of 108. However, this was only proven using a rather small sample size. ","05925be0":"The mapping did not show improvements of the MAE. It will not be used for the final submission. Also the ensembling is predicting a slightly higher MAE than the numeric model itself. For further improvement other ensembling methods could be tested.\n\nNow we train the RF-R on the biggest available dataset after showing the prediction accuracy. This trained model will then be applied to the test dataset to make the final predictions for the submission. Lets's see if we still get negative values. ","d4313ef8":"# Preparing test data for submission","c9371060":"# Text embedding - DistilBERT\nBecause the given task is, to use NLP for \"clap\" prediction, we will create a numeric representation of the articles using a pre-trained BERT model. \n\nThe Bert model, published by Google in 2018, might currently be the most popular choice for language processing. It is using the concept of self-attention, which helps the model to combine the meaning of multiple words into one vector embedding, representing relationships between words more accurately [1]. \n\nThe main problem is, that language processing models are usually designed for classification, question answering and text prediction tasks. Because we are trying to predict numeric values, we have to interpret the output with a different model like regressions.\n\nWhen taking a closer look at the distribution of the target variable it becomes clear, that linear regression models do not fit this task. To achieve good results, on one hand, the model has to predict \"normal\" values between 0 and some hundred claps, on the other hand, it has to predict which articles are going \"viral\". Predictions for lower values could be performed with one regression model. However, also predicting values that are more than 100 times the standard deviation, cannot be performed by a singular regression model. To achieve a low mean squared error, it is likely more important to catch outliners, than to have really accurate predictions on low values. For a high MAE the outliners are not as important. \n\nThe concept of using CLS-tokens from the Bert model for a regression task and the implementation is based on numerous informative articles by Jay Alammar, especially his notebook [2].\n\nCLS, short for classification, meaning that those tokens are used to classify the meaning of a sentence while also representing the words used in a sentence. \n\nThe text data will be pre-processed as required by Bert in the pre-processing notebook.\n\n\nWe are going to use DestilBert, which is a distilled down version of Bert. This is achieved by removing some weights in the neural network and decreasing the number of weights by approximating multiple weights to one, which is slightly less accurate but a lot faster and more memory efficient [3].\nBecause the *DestilBertTokenizer* and *DestilBertModel* calculate using vectors, the GPU should be used for encoding the text data. GPUs are designed for working with vectors, thus encoding can be done faster or using more text data. The Nvidia Apex module can be used for distributed processing like running the Bert model on the GPUs provided by Kaggle.\n\nBecause the DistilBERT model will run in a pyTorch environment on the GPU, a pyTorch implementation of DistilBERT could have been used, removing processing and conversion steps. \n\nThe neural network of DestilBert will not be trained on the training data. It will be used to create a \"machine-readable\" representation of the texts provided. Those embeddings will be used for training different regression models. This should not be a problem because we are using a pre-trained model. Because of limitations of computing power and RAM we can not give all text data into the model at once. This could cause DestilBert to have some unknown words but the pre-trained vocabulary is so big, that this effect should be negligible. ","85dee9c8":"Using a sample of 700 texts, the XGBRegressor predicts with a lower MAE than the RandomForestRegressor. Using a sample size of 10.000, the RF-R is predicting slightly better. Because a big part of parameter tuning can be seen as trial and error, this will not be the focus of this notebook. Also, improvements achieved using the small dataset can't be replicated using the bigger dataset. With the available computing power on Kaggle or Colab it is possible to do fine-tuning, the most efficient approach would be automating it and using between 20-60 iterations with each iteration taking around 10 minutes. Fine-tuning would take 2-4 hours per model, this would add up to 6-12 hours of parameter tuning for the three base models. This is possible but neither does it improve my understanding of NLP significantly, nor does it show my understanding of it.\n\nIn general, prediction accuracy that is better than the standard deviation of the target variable is not a bad sign for accuracy. When we try to predict outliners which are more than 100 times the standard deviation, exceptional results cannot be expected, though. There are also some other reasons, why prediction accuracy on the text data cannot be too accurate.\n\na) Using text data and NLP to predict likes is not the most sensible approach for a robust and accurate model. Language models can be used for classification tasks, sentiment analysis or text generation, not so much for numeric predictions.\n\nb) The variety of the provided data is big, text data alone cannot be used to quantify the popularity. The same articles posted by one really popular publishing entity at 9 pm is likely going to be 1000 times more successful than the same articles posted by a self-publishing professional posted at 6 am.","73a17207":"The MAE reaching a fraction of the standard deviation is a first indicator of the model being somewhat task-specific. The high MSE indicates a low prediction accuracy on articles with a high clap count.\n\nThis throws the important question of training goals. The RF-R and the XGB-R can use different evaluation metrics and can therefore be trained to minimize either the MAE or the MSE. Using the MSE could help catching outliners, or rather articles that went \"viral\" and using the MAE can help to get most predictions accurate while accepting, that outliners will most likely not be detected. However, a significant difference could not be shown working with the text data.\n\nIt might prove helpful to combine both evaluation metrics, text data could be used to predict using the MAE and numeric values could be evaluated using the MSE. ","fe93b519":"# Header analysis <a id=\"5\"><\/a>","2e835737":"# Conclusion\nIn general, we were able to achieve a good but not outstanding accuracy when looking at the MAE as the prediction parameter. The numeric model was the only one capable of catching very high values, thus achieving a lower MSE than the text models. This would also be the strongest criticism of this implementation, that especially the NLP model is not designed and not suitable for catching those outliners. Training the implemented models using MSE as a target would still improve the prediction quality of high values. \n\nAlso, the improvement from the numeric baseline to the ensemble is so small, that it might not justify the necessary computing power for the NLP model, this would depend strongly on the real-world use case.\n\nBecause this is the first time I am working with real-world data, the first time working with language data and my experience with full neural networks is limited, I decided on this ML-heavy implementation. Due to this inexperience with working with big, real-world datasets, the approach of working with two notebooks was chosen, this could have easily been done using one notebook. \n\nFor future work, a model with BERT for sequence classification in combination with the mapped target variable would be an interesting approach. \n\nIt is not likely, that an implementation with the full BERT model would increase the accuracy because we are not training the model and the CLS tokens of the texts should not deviate much or not at all.\n\nThis notebook is missing a cross-fold validation, which is especially necessary when using target encoding. Also, the positive influence of the target encoding is likely going to be smaller predicting on the *test* dataset because the author variable is less accurate here. This will make the predictions made by the NLP model more relevant, though. ","191d3f5d":"# Introduction\nThis notebook constitutes an attempt to predict \"claps\" of articles from medium.com. Frameworks for Natural Language Processing (NLP) will be used, combined with Machine learning models and numeric data analysis will be added where possible. As an indicator of the prediction quality, the mean absolute error (MAE) or the mean squared error (MSE) will be used.\n\nThe first step will be data pre-processing and exploratory data analysis. This analysis should inform the decision on which NLP frameworks could be used and what numeric data from the train- and test-dataset can be combined. Both will be covered in the linked\/submitted notebook (https:\/\/www.kaggle.com\/benediktbnedikt\/adams-dataprep). \n\n**The notebooks can either be executed locally, with the ADAMS-DataPrep\/EDA running first and generating output that is taken as input for this notebook, or they can be viewed and executed on kaggle.com, for saving time and taking advantage of the provided hardware.**\n\nFirst, we should take a look at medium.com. It is a publishing platform for all kinds of technological topics, from working with Raspberry Pis to AI frameworks. Publishing networks and independent authors can both be found on the platform and code can sometimes be part of an article. It can also be used for self-publishing. Because of the wide variety of topics and authors, it might be quite difficult to find meaningful categories and structures in the text data.  \n\nThis notebook will take the processed train and test data frame as input and therefore will be focused on implementing NLP and ML frameworks for predicting \"claps\".\n\nCitations will be used where extended concepts are used and modified. Code and concepts shown in a package documentation will not be mentioned, when using a package one can assume, that the documentation was used as well. ","c06b563f":"Predictions using the header data achieve a similar accuracy as with the text data, this is a positive indicator for reproducibility of this approach. Again, the MAE is indicating an acceptable prediction quality, considering the distribution of the data. However, the MSE accentuates the low prediction accuracy on high clap counts, again. ","7238608b":"# CSV file for submission","fb987c34":"# ML model using CLS tokens","0125f4be":"# Ensembling of text, header and numeric predictions\nFinally, we will combine the predictions from the text, the header and the numeric model by combining those in a pandas data frame and try to improve the prediction quality further. The biggest problem is the small size of the training data. ","f144e52e":"# Predicting on test data for submission","a121fb53":"After fitting a model with the MAE and one with the MSE as a target, we can see that both models do not deviate far from each other, due to faster computing, we will choose the MSE implementation. The MAE deviates by 0.5, the MSE by around 200, which is not much when looking at values of 280.000. ","74e21584":"# Table of Contents\n\n1. [Introduction](#Introduction)\n2. [Text embedding - DistilBERT](#Text-embedding---DistilBERT)\n    - [ML models using CLS tokens](#ML-models-using-CLS-tokens)\n    - [Predicting on test data for submission](#Predicting-on-test-data-for-submission)\n3. [Header analysis](#Header-analysis)\n    - [ML model using CLS tokens](#ML-model-using-CLS-tokens)\n    - [Predicting on test data for submission](#Predicting-on-test-data-for-submission)\n4. [Random-Forest model for numeric analysis](#Random-Forest-model-for-numeric-analysis)\n5. [Ensembling of text, header and numeric predictions](#Ensembling-of-text,-header-and-numeric-predictions)\n    - [Predictions on the test dataset](#Predictions-on-the-test-dataset)\n6. [Conclusion](#Conclusion)\n7. [Bibliography](#Bibliography)","6ce5a22f":"# Bibliography\n\n**ADAMS_Submission:**\n\n[1] Raimi Karim. Towards Data Science. Illustrated: Self-Attention. Retrieved 20.8.2020 from https:\/\/towardsdatascience.com\/illustrated-self-attention-2d627e33b20a\n\n[2] Jay Alammar. A Visual Notebook to Using BERT for the First Time. Retrieved 20.8.2020 from https:\/\/colab.research.google.com\/github\/jalammar\/jalammar.github.io\/blob\/master\/notebooks\/bert\/A_Visual_Notebook_to_Using_BERT_for_the_First_Time.ipynb#scrollTo=q1InADgf5xm2\n\n[3] Victor Sanh. Hugging Face. Smaller, faster, cheaper, lighter: Introducing DistilBERT, a distilled version of BERT. Retrieved 20.8.2020 from https:\/\/medium.com\/huggingface\/distilbert-8cf3380435b5)\n\n\n\n\n\n\n**ADAMS_DataPrep\/EDA:**\n\n[1] Daniel Jeffries. Hackernoon (medium.com). The Cryptocurrency Trading Bible. Retrieved 20.8.2020 from https:\/\/medium.com\/hackernoon\/the-cryptocurrency-trading-bible-43d0c57e3fe6\n\n[2]  Alex Martelli. Stackoverflow. How to make a flat list out of list of lists?. Retrieved 20.8.2020 from https:\/\/stackoverflow.com\/questions\/952914\/how-to-make-a-flat-list-out-of-list-of-lists\n\n[3] \"\u2126mega\". Stackoverflow. How to remove any URL within a string in Python. Retrieved 20.8.2020 from https:\/\/stackoverflow.com\/questions\/11331982\/how-to-remove-any-url-within-a-string-in-python","82062513":"# Random-Forest model for numeric analysis\n\nThe variables have been pre-processed in the linked notebook. Train and test data are in the same format. We still have to apply target encoding on training and testing datasets. Each author will be assigned the average clap count over their published articles.","49b2489c":"# ML models using CLS tokens\nNow we have prepared the text data for the DestilBertModell and used it to generate CLS tokens.\n\nTwo sets of data have been generated, one for training and one for testing. Because we have dropped all entries of the validation dataset from the training dataset, we don't have to be concerned about data overlap and train-test-splitting. \n\nThis data will be analysed with a RandomForestRegressor and an XGBRegressor. Both ML models are chosen because they are easy to use and prior personal experience using both models. Different models likely can achieve a similar or maybe even better score. However the scores of both models no not deviate too much from each other, thus making high improvements with other models not as likely.  "}}