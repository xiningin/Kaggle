{"cell_type":{"874133c2":"code","905a02d2":"code","40f3e713":"code","ba9c76cf":"code","89f6b5c1":"code","86a66f58":"code","1548ddef":"code","5a2d8a52":"code","219cd269":"code","4b6f0379":"code","2aca1a07":"code","b59bc189":"code","66bcf9f1":"code","aa2ac320":"code","c4f6c71f":"code","15a56a82":"code","022a7cd9":"code","2a48eed3":"code","769fa316":"code","f12d6e98":"code","5b585860":"markdown"},"source":{"874133c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib as plt\nimport seaborn as sns\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/\"))","905a02d2":"##Initial data understanding \n#this is training dataset. I will be creating two sample using below data sets as my training and validation dataset. This will be done after preprocessing of the data.\ndataset=pd.read_csv(\"..\/input\/application_train.csv\")\n##test dataset which is to be predicted\ntest=pd.read_csv(\"..\/input\/application_test.csv\")","40f3e713":"#reducing the unique values in occupation by grouping by skill level. This grouping can differ based on more information about each occupation\ndataset['NAME_TYPE_SUITE'].replace({'Children':'Family',\n                                    'Group of people':'Other',\n                                    'Other_A':'Other',\n                                    'Other_B':'Other',\n                                    'Spouse, partner':'Family'},inplace=True)\n\ntest['NAME_TYPE_SUITE'].replace({'Children':'Family',\n                                    'Group of people':'Other',\n                                    'Other_A':'Other',\n                                    'Other_B':'Other',\n                                    'Spouse, partner':'Family'},inplace=True)\ndataset.groupby(['OCCUPATION_TYPE']).SK_ID_CURR.count()\n\ndataset.groupby(['NAME_EDUCATION_TYPE']).SK_ID_CURR.count()\ndataset.groupby(['NAME_EDUCATION_TYPE']).TARGET.mean() \ndataset['NAME_EDUCATION_TYPE'].replace({'Academic degree':'Higher education '},inplace=True)\ntest['NAME_EDUCATION_TYPE'].replace({'Academic degree':'Higher education '},inplace=True)","ba9c76cf":"#reducing the unique values in occupation by grouping by skill level. This grouping can differ based on more information about each occupation\ndataset['OCCUPATION_TYPE'].replace({'High skill tech staff':'High_Skill',\n                                    'Managers':'High_Skill',\n                                    'Accountants':'High_Med_Skill',\n                                    'HR staff':'High_Med_Skill',\n                                    'Core staff':'Med_Skill',\n                                   'Cooking staff':'Med_Skill',\n                                    'Realty agents':'Med_Skill',\n                                    'Sales staff':'Med_Skill',\n                                    'IT staff':'High_Med_Skill',\n                                    'Medicine staff':'High_Med_Skill',\n                                    'Secretaries':'Med_Skill',\n                                    'Security staff':'Med_Skill',\n                                    'Cleaning staff':'Low_Skill',\n                                      'Laborers':'Low_Skill',\n                                      'Low-skill Laborers':'Low_Skill',\n                                      'Cleaning staff':'Low_Skill',\n                                    'Waiters\/barmen staff':'Low_Skill',\n                                    'Private service staff':'Low_Skill',\n                                    'Drivers':'Med_Skill'\n                                   },inplace=True)\ntest['OCCUPATION_TYPE'].replace({'High skill tech staff':'High_Skill',\n                                    'Managers':'High_Skill',\n                                    'Accountants':'High_Med_Skill',\n                                    'HR staff':'High_Med_Skill',\n                                    'Core staff':'Med_Skill',\n                                   'Cooking staff':'Med_Skill',\n                                    'Realty agents':'Med_Skill',\n                                    'Sales staff':'Med_Skill',\n                                    'IT staff':'High_Med_Skill',\n                                    'Medicine staff':'High_Med_Skill',\n                                    'Secretaries':'Med_Skill',\n                                    'Security staff':'Med_Skill',\n                                    'Cleaning staff':'Low_Skill',\n                                      'Laborers':'Low_Skill',\n                                      'Low-skill Laborers':'Low_Skill',\n                                      'Cleaning staff':'Low_Skill',\n                                    'Waiters\/barmen staff':'Low_Skill',\n                                    'Private service staff':'Low_Skill',\n                                    'Drivers':'Med_Skill'\n                                   },inplace=True)\ndataset.groupby(['OCCUPATION_TYPE']).SK_ID_CURR.count()","89f6b5c1":"#grouping\ndataset['NAME_INCOME_TYPE'].replace({'Businessman':'Other','Student':'Other','Maternity leave':'Other'},inplace=True)\ntest['NAME_INCOME_TYPE'].replace({'Businessman':'Other','Student':'Other','Maternity leave':'Other'},inplace=True)","86a66f58":"print('Testing Features shape: ', test.shape)\nprint('Training Features shape: ', dataset.shape)","1548ddef":"# Create a label encoder object\n# sklearn preprocessing for dealing with categorical variables\nfrom sklearn.preprocessing import LabelEncoder\napp_train=dataset\napp_test=test\nle = LabelEncoder()\nle_count = 0\n\n# Iterate through the columns\nfor col in dataset:\n    if dataset[col].dtype == 'object':\n        # If 2 or fewer unique categories\n        if len(list(dataset[col].unique())) <= 2:\n            # Train on the training data\n            le.fit(dataset[col])\n            # Transform both training and testing data\n            app_train[col] = le.transform(dataset[col])\n            app_test[col] = le.transform(test[col])\n            \n            # Keep track of how many columns were label encoded\n            le_count += 1\n            \nprint('%d columns were label encoded.' % le_count)","5a2d8a52":"app_train= pd.get_dummies(app_train)\napp_test= pd.get_dummies(app_test)","219cd269":"app_train.dtypes.value_counts()\napp_test.dtypes.value_counts()","4b6f0379":"# Create an anomalous flag column\napp_train['DAYS_EMPLOYED_ANOM'] = app_train[\"DAYS_EMPLOYED\"] == 365243\n\n# Replace the anomalous values with nan\napp_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\napp_train.fillna(dataset.median(),inplace = True)\n\napp_test['DAYS_EMPLOYED_ANOM'] = test[\"DAYS_EMPLOYED\"] == 365243\napp_test[\"DAYS_EMPLOYED\"].replace({365243: np.nan}, inplace = True)\napp_test.fillna(dataset.median(),inplace = True)","2aca1a07":"#replace all  NaN in the var_list with zero\nVar_List=('OBS_30_CNT_SOCIAL_CIRCLE','OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE',\n        'DAYS_LAST_PHONE_CHANGE','AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK',\n         'AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR')\ndef missing_val_replace(data,Var_List):\n    for col in data:\n        for i in Var_List:\n            if col==i:\n                data[col].fillna(0)\n                print (col)\n    return data\napp_train=missing_val_replace(app_train,Var_List) \n#replace all other NaN with median values\napp_train=app_train.fillna(app_train.median)\napp_test=missing_val_replace(app_test,Var_List) \n#replace all other NaN with median values\napp_test=app_test.fillna(app_test.median)\n\napp_train.dtypes.value_counts()\napp_test.dtypes.value_counts()\n","b59bc189":"train_labels = app_train['TARGET']\n\n# Align the training and testing data, keep only columns present in both dataframes\napp_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\napp_trainv2=app_train\n# Add the target back in\n#app_train['TARGET'] = train_labels\n\nprint('Training Features shape: ', app_train.shape)\nprint('Training Features shape: ', app_trainv2.shape)\nprint('Testing Features shape: ', app_test.shape)","66bcf9f1":"from sklearn.preprocessing import MinMaxScaler, Imputer\n\n# Drop the target from the training data\nif 'TARGET' in app_train:\n    train = app_train.drop(columns=['TARGET'])\nelse:\n    train = app_train.copy()\n    \n# Feature names\nfeatures = list(train.columns)\n\n# Copy of the testing data\ntest = app_test.copy()\n\n# Median imputation of missing values\nimputer = Imputer(strategy = 'median')\n\n# Scale each feature to 0-1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n# Fit on the training data\nimputer.fit(train)\n\n# Transform both training and testing data\ntrain = imputer.transform(train)\ntest = imputer.transform(app_test)\n\n# Repeat with the scaler\nscaler.fit(train)\ntrain = scaler.transform(train)\ntest = scaler.transform(test)\n\nprint('Training data shape: ', train.shape)\nprint('Testing data shape: ', test.shape)","aa2ac320":"from sklearn.linear_model import LogisticRegression\n\n# Make the model with the specified regularization parameter\nlog_reg = LogisticRegression(C = 0.0001)\n\n# Train on the training data\nlog_reg.fit(train, train_labels)","c4f6c71f":"# Make predictions\n# Make sure to select the second column only\nlog_reg_pred = log_reg.predict_proba(test)[:, 1]","15a56a82":"# Submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = log_reg_pred\n\nsubmit.head()","022a7cd9":"# Save the submission to a csv file\nsubmit.to_csv('log_reg_baseline.csv', index = False)","2a48eed3":"from sklearn.ensemble import RandomForestClassifier\n\n# Make the random forest classifier\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 50, verbose = 1, n_jobs = -1)","769fa316":"# Train on the training data\nrandom_forest.fit(train, train_labels)\n\n# Extract feature importances\nfeature_importance_values = random_forest.feature_importances_\nfeature_importances = pd.DataFrame({'feature': features, 'importance': feature_importance_values})\n\n# Make predictions on the test data\npredictions = random_forest.predict_proba(test)[:, 1]","f12d6e98":" #Make a submission dataframe\nsubmit = app_test[['SK_ID_CURR']]\nsubmit['TARGET'] = predictions\nsubmit.to_csv('random_forest_baseline.csv', index = False)\n","5b585860":"I will be using my learning from data exploration excercise at https:\/\/www.kaggle.com\/lokendradevangan\/home-credit-initial-data-exploration  for developing my first model.  I will also borrow information from other Kagglers to improvise my model "}}