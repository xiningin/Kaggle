{"cell_type":{"cbbe46b8":"code","8ee93453":"code","eb0651ee":"code","4c5aa8cc":"code","50c46291":"code","2d3077ab":"code","6846165d":"code","b70adda5":"code","af0abdd2":"code","b92603ff":"code","a2d0866d":"code","312588ff":"code","1d84bfe7":"code","43f5cd1a":"code","df277816":"code","a3ecf1e7":"code","09881ec7":"code","218f745f":"code","2716d3ab":"code","588c3749":"code","6fd37b79":"code","5cc35665":"code","c6c33666":"code","2ed0e158":"code","0813c226":"code","37377cac":"code","84484892":"markdown","33f0c6b6":"markdown","64a6ebe0":"markdown","2e6ea6eb":"markdown","8c140176":"markdown","4f79c706":"markdown","2a9fda04":"markdown","32a2919e":"markdown","3c13a882":"markdown","6bfe13b4":"markdown","264a490b":"markdown","4fe03246":"markdown","1449b417":"markdown","cbda53ab":"markdown","c629d430":"markdown","45244cc3":"markdown","798b92fd":"markdown","d12cb368":"markdown","ae041581":"markdown","dbfcb210":"markdown","3cfcd0e9":"markdown","c4bfcf9c":"markdown","c4ee1ebf":"markdown","595b89d0":"markdown","aca1f7f3":"markdown"},"source":{"cbbe46b8":"#Using Pandas for Reading json\/txt file\nimport pandas as pd\nimport numpy as np","8ee93453":"#Library required for text processing\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer,LancasterStemmer,PorterStemmer","eb0651ee":"#Sklearn Library for prediction\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer,TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score","4c5aa8cc":"ls ..\/input","50c46291":"df=pd.read_json(r'..\/input\/training.json',lines=True,)","2d3077ab":"df=df.drop(0,axis=0).reset_index(drop=True)","6846165d":"df.columns=['Content']","b70adda5":"df.info()\ndf.head(5)","af0abdd2":"df['Content'][0]","b92603ff":"df['Topic']=[dict(x)['topic'] for x in df['Content']]\ndf['Question']=[dict(x)['question'] for x in df['Content']]\ndf['Excerpt']=[dict(x)['excerpt'] for x in df['Content']]","a2d0866d":"df.head(5)","312588ff":"lb=LabelEncoder().fit(df['Topic'])\nY_train=lb.transform(df['Topic'])","1d84bfe7":"def textProcessing(text):\n    #Tokenization\n    tokenizer=RegexpTokenizer(r'\\w+')\n    word_token=tokenizer.tokenize(text.lower())\n    #Remove Stop Words\n    stopWordList=stopwords.words('english')\n    wordListRefined=[]\n    for word in word_token:\n        if word not in stopWordList:\n            wordListRefined.append(word)\n    #print(word_token)\n    #Lemmetization\n    WordList=[]\n    for word in wordListRefined:\n        WordList.append(WordNetLemmatizer().lemmatize(word,pos='v'))\n        #WordList.append(LancasterStemmer().stem(word))\n    return \" \".join(WordList)","43f5cd1a":"df['Text']=[textProcessing(x) for x in df['Excerpt']]\nX_train=df['Text']\ndf.head(5)","df277816":"#______________________Testing Data_________________________________________#\ndf_test=pd.read_json('..\/input\/input00.txt',lines=True)\ndf_test=df_test.drop(0,axis=0).reset_index(drop=True)\ndf_test.columns=['Content']\ndf_test.info()\ndf_test.head(5)","a3ecf1e7":"df_test['Question']=[dict(x)['question'] for x in df_test['Content']]\ndf_test['Excerpt']=[dict(x)['excerpt'] for x in df_test['Content']]\ndf_test['Topic']=pd.read_csv(\"..\/input\/output00.txt\",header=None)","09881ec7":"Y_test=lb.transform(df_test['Topic'])","218f745f":"df_test['Text']=[textProcessing(x) for x in df_test['Excerpt']]\nX_test=df_test['Text']\ndf_test.head(5)","2716d3ab":"cf_fit=CountVectorizer().fit(X_train)\ncf_train=cf_fit.transform(X_train)\ntf_fit=TfidfTransformer().fit(cf_train)\ntf_train=tf_fit.transform(cf_train)\nsvm=SVC(kernel='linear')\nsvm.fit(tf_train,Y_train)\nprint(\"Training Data Accuracy-->\",accuracy_score(Y_train,svm.predict(tf_train)))","588c3749":"cf_test=cf_fit.transform(X_test)\ntf_test=tf_fit.transform(cf_test)\nprint(\"Testing Data Accuracy-->\",accuracy_score(Y_test,svm.predict(tf_test)))","6fd37b79":"pipe_SVM=Pipeline([('tf',TfidfVectorizer(sublinear_tf=True)),('svm',SVC(kernel='linear',C=10))])","5cc35665":"pipe_SVM.fit(X_train,Y_train)\naccuracy_score(Y_train,pipe_SVM.predict(X_train))","c6c33666":"accuracy_score(Y_test,pipe_SVM.predict(X_test))","2ed0e158":"pipe_log=Pipeline([('tf',TfidfVectorizer(sublinear_tf=True)),('svd',TruncatedSVD(n_components=500)),('lg',LogisticRegression(penalty='none',n_jobs=-1,solver='saga'))])","0813c226":"pipe_log.fit(X_train,Y_train)\naccuracy_score(Y_train,pipe_log.predict(X_train))","37377cac":"accuracy_score(Y_test,pipe_log.predict(X_test))","84484892":"Now we have change the form of each word in our refined list.\nFor this we can use Stemming or Lemmetization\nStemming can be done using PorterStemmer or LancasterStemming.\nStemming will reduce the word to its base form irrespective of correct english word where as Lemmetization will use dictionary to bring the word in its base form. Hence Lemmetization is slower process.","33f0c6b6":"Lets see how each entry looks like","64a6ebe0":"RESULTS:\n\nSimple SVM with Linear Kernel ---> Training Acc.=96.71%\n                              ---> Testing Acc.=86.82%\n\nSVM with tuned hyper parameters  ---> Training Acc.=99.98%\n                                 ---> Testing Acc.=84.62%\n\nLogistic Regression with tuned hyper parameters ---> Training Acc.=88.83%\n                                                ---> Testing Acc.=84.47%","2e6ea6eb":"Let's create a function that will do the text processing","8c140176":"Now lets try Logistic Regression with few hyper parameter","4f79c706":"So we have finally created one more columns for processed text which we will use for training purpose.","2a9fda04":"______________________________________________Training Data_______________________________________","32a2919e":"Lets see short info about our dataset","3c13a882":"SVM give higher accuracy on training set as compared to Logistic Regression. But it is computaionally expensive approach. Logistic Regression is much faster.","6bfe13b4":"The Training dataset is json file which consists of topic name,question and excerpt or description about the question. ","264a490b":"Dropped 1st row as it consist of number of entries in files","4fe03246":"Rename of column to Content","1449b417":"We Will use RegexpTokenizer which will break the Excerpt into words, removing all the punctuations as well.\nConvert the Excerpt to lower case.","cbda53ab":"Here,we will create a model without tuning hyper parameters. Lets see the initial performance and later on tune the htper parameters","c629d430":"Lets break down the data into dictionary and create a new column for each key","45244cc3":" Stack Exchange Question Classification Using NLP and Scikit-learn","798b92fd":"Here we will use CountVectorizer to create features and TfidfTransformer to apply Tfidf (Term Frequency-Inverse Document Frequency).\n\nHere Term frequency summarizes how often a given word appears within a document. Inverse Document Frequency downscales words that appear a lot across documents.\n\nCountVectorizer wil create the sparse matrix with count of each word.\n\nTfidfTransformer will provide the most popular word across all document. The inverse document frequencies are calculated for each word. The lower the score more frequent the word observed.","d12cb368":"Here we will firstly use SVM with linear kernel","ae041581":"Lets process the Testing Data same as done for training data","dbfcb210":"We will LabelEncoder to convert target i.e Topic into integer form","3cfcd0e9":"Create a list of stopwords. Stopwords are the not so important words like is am are to in a an etc.\nWe will refine the list of tokens,removing all the stopwords","c4bfcf9c":"Thats how it finally looks","c4ee1ebf":"Now we will tune the hyper parameters and few other tweeks in model.","595b89d0":"We can combine CountVectorizer and TfidfTransformer to TfidfVectorizer.\n\nWe will now introduce SVD for dimentionality reduction.\n\nWe can also use the pipeline from sklearn to combine all the above processes in a single line.","aca1f7f3":"We will use NLTK for text processing and different machine learing of Scikit-Learn library"}}