{"cell_type":{"1e75bcaa":"code","e9818a4c":"code","6d9023ad":"code","745bd6b5":"code","dfee3e55":"code","02993fe4":"code","60c61549":"code","cf289776":"code","80a2e480":"code","7b49a07d":"code","bcb6de1b":"code","9cd819c4":"code","ecd0f9bf":"code","89af19c0":"code","191e00eb":"code","29ca5858":"code","7304f0f4":"code","6e89dd1c":"code","7f2c2b89":"code","efc23773":"code","eb16db55":"code","43f1c302":"code","424531ce":"code","e6149181":"code","ab72bbe2":"code","3e38e3b1":"code","44f062a0":"code","0341f6b4":"code","c80ea0dc":"code","f5d3efec":"markdown","342f1775":"markdown","b0ff5e99":"markdown","8dc07dc3":"markdown","f6c4da56":"markdown","c6d54292":"markdown","b71ae59a":"markdown","189a70cc":"markdown","a2f1297d":"markdown","f47e3fee":"markdown","5196cdd4":"markdown","9461c05d":"markdown","b3142090":"markdown","71ba3fda":"markdown","1ab75d81":"markdown","30f12805":"markdown","0987fd74":"markdown","238392a4":"markdown"},"source":{"1e75bcaa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e9818a4c":"import matplotlib.pyplot as plt, matplotlib.image as mpimg\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import svm, tree","6d9023ad":"# Load Data\n# Baca file .csv ke dalam DataFrame\nlabeled_images = pd.read_csv('..\/input\/train.csv')       \n# iloc (index location) , menyeleksi berdasarkan posisi\n# labeled_images.iloc[0:5000,1:], yang terseleksi kedalam variabel images baris ke-0 sampai 4999 dan kolom ke-1 sampai kolom terakhir\nimages = labeled_images.iloc[0:5000,1:]\n# labeled_images.iloc[0:5000,:1], yang terseleksi kedalam variabel labels baris ke-0 sampai 4999 dan kolom ke-0\nlabels = labeled_images.iloc[0:5000,:1]\n# split array ke dalam himpunan bagian random train dan test\ntrain_images, test_images,train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=0)","745bd6b5":"print(train_labels.head())\n#print(train_labels.index[1])\n#print(train_labels.iloc[0])\n#print(train_labels.iloc[0].values)","dfee3e55":"# now we gonna load the second image, reshape it as matrix than display it\ni=1\n# select i row \nimg=train_images.iloc[i].values\nimg=img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(train_labels.iloc[i,0])","02993fe4":"# np.unique (untuk return elemen unik dari sebuah array yang sudah di sort)\nnumber = np.unique(train_labels)\nprint (number)\n# indeks manual\nindeks = [12,4,18,2,42,9,1,0,11,7]\n# indeks otomatis\nindexnum = [0,0,0,0,0,0,0,0,0,0]\n# mencari indeks dari setiap kelas angka \n# dengan looping sebanyak baris dari train_labels\nfor i in range (len(train_labels)):\n    # variabel labelnum untuk menyimpan kelas angka, dan untuk menemukan index nya menggunakan i\n    # (iloc[i] mencari baris ke i dan .label untuk mencari di dalam kolom label)\n    labelnum = train_labels.iloc[i].label\n    # isi yang berada di dalam indexnum akan direplace dengan lokasi indeks dari setiap kelas angka dari 0-9\n    indexnum[labelnum] = i\n    \nfor i in indexnum:\n    plt.figure()\n    img=train_images.iloc[i].values\n    img=img.reshape((28,28))\n    plt.imshow(img,cmap='gray')\n    plt.title(train_labels.iloc[i,0])","60c61549":"#train_images.iloc[i].describe()\ni=1\nprint(type(train_images))\nprint(type(train_labels))\nplt.hist(train_images.iloc[i])","cf289776":"labelcount = train_labels[\"label\"]\nprint(labelcount.value_counts())","80a2e480":"# create histogram for each class (data merged per class)\n# Todo\n#print(train_labels.iloc[:5])\n#data1 = train_images.iloc[1]\n#data2 = train_images.iloc[3]\n#data1 = np.array(data1)\n#data2 = np.array(data2)\n#data3 = np.append(data1,data2)\n#print(len(data3))\n#plt.hist(data3)","7b49a07d":"label = [[],[],[],[],[],[],[],[],[],[]]\nfor j in range(10):\n    for i in range(len(train_images)):\n        if (train_labels.iloc[i].label == j):\n            data = train_images.iloc[i]\n            data = np.array(data)\n            label[j] = np.append(label[j],data)\n            \n    plt.figure(j)\n    plt.hist(label[j])\n    plt.title(j)\n        \n","bcb6de1b":"# Define model\nclf = svm.SVC()\n# Fit: Capture patterns from provided data.\nclf.fit(train_images, train_labels.values.ravel())\n# Determine how accurate the model's\nclf.score(test_images,test_labels)","9cd819c4":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeRegressor\n\ntree = DecisionTreeRegressor(random_state=0)\ntree.fit(train_images, train_labels)\ntest_predict = tree.predict(test_images)\nprint(mean_absolute_error(test_labels, test_predict))","ecd0f9bf":"print(train_labels.values.ravel())\nprint(np.unique(test_labels)) # to see class number","89af19c0":"test_images[test_images>0]=1\ntrain_images[train_images>0]=1\nimg=train_images.iloc[i].values.reshape((28,28))\nplt.imshow(img,cmap='binary')\nplt.title(train_labels.iloc[i])","191e00eb":"# now plot again the histogram\nplt.hist(train_images.iloc[i])","29ca5858":"clf = svm.SVC()\nclf.fit(train_images, train_labels.values.ravel())\nscoresvm = clf.score(test_images,test_labels)\nprint (scoresvm)","7304f0f4":"# Test again to data test\ntest_data=pd.read_csv('..\/input\/test.csv')\ntest_data[test_data>0]=1\nresults=clf.predict(test_data[0:5000])","6e89dd1c":"# separate code section to view the results\nprint(results)\nprint(len(results))","7f2c2b89":"df = pd.DataFrame(results)\ndf.index.name='ImageId'\ndf.index+=1\ndf.columns=['Label']\ndf.to_csv('results.csv', header=True)","efc23773":"#check if the file created successfully\nprint(os.listdir(\".\"))","eb16db55":"# from https:\/\/www.kaggle.com\/rtatman\/download-a-csv-file-from-a-kernel\n\n# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"data.csv\"):  \n    csv = df.to_csv()\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(df)","43f1c302":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\n# Set the parameters by cross-validation\nparameters = {'gamma': [0.01, 0.001, 0.0001],'C': [1, 10, 100,1000]}\n\n# Create a classifier object with the classifier and parameter candidates\nclf = GridSearchCV(estimator=svm.SVC(), param_grid = parameters)\nclf.fit(train_images,train_labels.values.ravel())","424531ce":"print('Best C:',clf.best_estimator_.C) \nprint('Best Gamma:',clf.best_estimator_.gamma)","e6149181":"#final svm\nbest_c = 10\nbest_gamma = 0.01\nclf_final = svm.SVC(C=best_c,gamma=best_gamma)\nclf_final.fit(train_images, train_labels.values.ravel())\nfinalsvm = clf_final.score(test_images,test_labels)\nprint(clf_final.score(test_images,test_labels))","ab72bbe2":"from sklearn.tree import DecisionTreeClassifier\ndef get_mae_train_classifie(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_train = model.predict(train_X)\n    mae = mean_absolute_error(train_y, preds_train)\n    return(mae)\n\ndef get_mae_test_classifie(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes, random_state=0)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\n\nmaee_train=[]\nmaee_test= []\nleaf_nodes=[5,25,50,70,100,300,500,1000,3000,5000,7000]\nfor max_leaf_nodes in leaf_nodes:\n    my_maetrain = get_mae_train_classifie(max_leaf_nodes, train_images, test_images, train_labels, test_labels)\n    my_maetest = get_mae_test_classifie(max_leaf_nodes, train_images, test_images, train_labels, test_labels)\n    maee_train.append(my_maetrain)\n    maee_test.append(my_maetest)\n\nplt.figure()\nplt.plot(leaf_nodes,maee_test,color=\"red\",label='Validation')\nplt.plot(leaf_nodes,maee_train,color=\"blue\",label='Training')\nplt.xlabel(\"Tree Depth\")\nplt.ylabel(\"MAE\")\nplt.title(\"Decision Tree Classifier\")\nplt.legend()\nplt.show()\n\nprint (maee_train)\nprint (maee_test)","3e38e3b1":"#final model dtclassifier\n\nbest_tree_size=1000\ntreeclassifie = DecisionTreeClassifier(max_leaf_nodes=best_tree_size, random_state=0)\ntreeclassifie.fit(train_images,train_labels)\nscoredtc = treeclassifie.score(test_images,test_labels)\nprint (\"DTC = \",scoredtc)\nprint (\"SVM = \",finalsvm)\n","44f062a0":"# Decision Tree Regressor\ndef get_mae_train_regress(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n    model.fit(train_X, train_y)\n    preds_train = model.predict(train_X)\n    mae = mean_absolute_error(train_y, preds_train)\n    return(mae)\n\ndef get_mae_test_regress(max_leaf_nodes, train_X, val_X, train_y, val_y):\n    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    mae = mean_absolute_error(val_y, preds_val)\n    return(mae)\n\nmaee_train=[]\nmaee_test= []\nleaf_nodes=[5,25,50,70,100,300,500,1000,3000,5000,7000]\nfor max_leaf_nodes in leaf_nodes:\n    my_maetrain = get_mae_train_regress(max_leaf_nodes, train_images, test_images, train_labels, test_labels)\n    my_maetest = get_mae_test_regress(max_leaf_nodes, train_images, test_images, train_labels, test_labels)\n    maee_train.append(my_maetrain)\n    maee_test.append(my_maetest)\n\nplt.figure()\nplt.plot(leaf_nodes,maee_test,color=\"red\",label='Validation')\nplt.plot(leaf_nodes,maee_train,color=\"blue\",label='Training')\nplt.xlabel(\"Tree Depth\")\nplt.ylabel(\"MAE\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n\nprint (maee_train)\nprint (maee_test)\n\n","0341f6b4":"#final model dtregressor\nbest_tree_size=1000\ntreeregres = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=0)\ntreeregres.fit(train_images,train_labels)\nscoredtr = treeregres.score(test_images,test_labels)\nprint (\"DTR = \",scoredtr)\nprint (\"DTC = \",scoredtc)\nprint (\"SVM = \",finalsvm)","c80ea0dc":"# Decision Tree Classifier\n# labeled_images.iloc[0:5000,1:], yang terseleksi kedalam variabel images baris ke-0 sampai 4999 dan kolom ke-1 sampai kolom terakhir\nimages = labeled_images.iloc[0:5000,1:]\n# labeled_images.iloc[0:5000,:1], yang terseleksi kedalam variabel labels baris ke-0 sampai 4999 dan kolom ke-0\nlabels = labeled_images.iloc[0:5000,:1]\ntrain_images, test_images,train_labels, test_labels = train_test_split(images, labels, test_size=0.2, random_state=0)\nbest_tree_size=1000\ntree = DecisionTreeClassifier(max_leaf_nodes=best_tree_size,random_state=0)\ntree.fit(train_images, train_labels)\ntest_predict = tree.predict(test_images)\n#print(mean_absolute_error(test_labels, test_predict))\nscoredtr_ver2 = tree.score(test_images,test_labels)\nprint (\"DTC = \",scoredtr_ver2)\n","f5d3efec":"# Q3\nCan you check in what class does this histogram represent?. How many class are there in total for this digit data?. How about the histogram for other classes\n\n**Jawab :**\n* [Histogram](https:\/\/matplotlib.org\/api\/pyplot_api.html#matplotlib.pyplot.hist) di atas menunjukan bahwa x nilai pixel dan y frekuensi dari  1 sampel kelas angka yaitu angka 6.","342f1775":"## Train the model\n1. Gunakan modul [sklearn.svm](http:\/\/scikit-learn.org\/stable\/modules\/svm.html) untuk membuat [vector classifier](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html).\n2. Melakukan fitting dengan menggunakan metode [fit](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC.fit).\n3. Melihat indikasi akurasi (0-1) dengan menggunakan metode [score](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC.score).","b0ff5e99":"# Improving Performance\nDid you noticed, that the performance is so miniscule in range of ~0.1. Before doing any improvement, we need to analyze what are causes of the problem?. But allow me to reveal one such factor. It was due to pixel length in [0, 255]. Let's see if we capped it into [0,1] how the performance are going to improved.","8dc07dc3":"# Retrain the model\nUsing the now adjusted data, let's retrain our model to see the improvement","f6c4da56":"Now plot the histogram within img","c6d54292":"# Disclaimer\n\nThe data in this notebook is mostly copied from https:\/\/www.kaggle.com\/archaeocharlie\/a-beginner-s-approach-to-classification . I intended to do modification later to the tutorial, so please permit me for using it.","b71ae59a":"# Load Data\nload data dengan menggunakan library pandas .\n1.  Gunakan pandas [read_csv](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.read_csv.html) untuk membaca train.csv ke dalam [DataFrame](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.html).\n2.  Memisahkan* images* dan *labels* untuk *supervised learning*.\n3.  Melakukan [train_test_split](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html) untuk membagi data menjadi 2 set, satu untuk *training* dan satu lagi untuk *testing*.","189a70cc":"# Q4\nIn above, did you see score() function?, open SVM.score() dokumentation at SKLearn,what does it's role?. Does it the same as MAE discussed in class previously?.Ascertain it through running the MAE. Now does score() and mae() prooduce the same results?.\n\n**Jawab :**\n\n1. Fungsi [score()](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html#sklearn.svm.SVC.score) pada svm.SVC menghasilkan rata rata akurasi dari test images dan test labels yang diberikan. hasil dari score adalah **0.1**. Rata-rata akurasi semakin mendekati angka 1 semakin baik. sedangkan\n \n2. Fungsi [mean_absolute_error()](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mean_absolute_error.html) menghasilkan rata rata error dari test labels dan prediksi dari test images. hasil dari mean absolute error adalah **1.033**. rata rata error akan semakin baik jika nilainya mendekati angka 0.","a2f1297d":"*  Untuk mendapatkan nilai gamma dan c yang optimal , saya menggunakan [grid search](http:\/\/http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html ) .\n*  Grid search digunakan untuk pencarian menyeluruh atas nilai parameter yang ditentukan untuk estimator.\n","f47e3fee":"# Q6\nAlhamdulillah, we have completed our experiment. Here's things to do for your next task:\n* What is the overfitting factor of SVM algorithm?. Previously on decision tree regression, the factor was max_leaf nodes. Do similar expriment using SVM by seeking SVM documentation!\n*  Apply Decision Tree Classifier on this dataset, seek the best overfitting factor, then compare it with results of SVM.\n* Apply Decision Tree Regressor on this dataset, seek the best overfitting factor, then compare it with results of SVM & Decision Tree Classifier. Provides the results in table\/chart. I suspect they are basically the same thing.\n* Apply Decision Tree Classifier on the same dataset, use the best overfitting factor & value. But use the unnormalized dataset, before the value normalized to [0,1]\n\n","5196cdd4":"# Prediction labelling\nIn Kaggle competition, we don't usually submit the end test data performance on Kaggle. But what to be submitted is CSV of the prediction label stored in a file.","9461c05d":"# Melihat gambar dari setiap kelas angka\n*  *Image* disini masih berupa 1 dimensi, kemudian diubah menjadi 2 dimensi dengan menggunakan [numpy array ](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.array.html)dan [reshape](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.reshape.html)\n*  Plot gambar dengan menggunakan [pyplot.plot](https:\/\/matplotlib.org\/api\/_as_gen\/matplotlib.pyplot.plot.html) dari library [matplotlib](https:\/\/matplotlib.org\/api\/index.html)","b3142090":"# Q2\nNow plot an image for each image class\n\n**Jawab :**\n*  Plot gambar dari setiap kelas , kelas digit punya 10 kelas, yaitu 0 , 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 dan 9.","71ba3fda":"# Library Import\nFor starter import any machine libary we wanted to use. SKLearn is good choice for beginner, the question is what the algorithm we interested to test. Here's what we are going to need:\n1. At least a classification algorithm (SVM or Decision Tree is a Good Choice)\n2. Matplotlib\n3. Preprocessing tools\n4. Train test split And since we have been import numpy and panda no need to import them.","1ab75d81":"# Q5\nBased on this finding, Can you explain why if the value is capped into [0,1] it improved the performance significantly?. Perharps you need to do several self designed test to see why.\n\n**Jawab :**\n\nMenyederhanakan gambar dengan membuatnya benar benar hitam dan putih (1,0), sebelum nilai pixel di ubah menjadi 1 dan 0 , gambar mengandung warna abu abu menjadikan gambar agak sedikit tidak jelas atau blur, tetapi ketika nilai pixel hanya 0 dan 1 , gambar menjadi jelas dikarenakan warnanya hanya hitam dan putih,  dengan demikian nilai akurasinya meningkat. ","30f12805":"\nNo | Modul | Score\n--- | --- | ---\n1 | Decision Tree Regressor| 0.586\n2 | Decision Tree Classifier | 0.779\n3 | SVM | 0.94","0987fd74":"# Data Download\n\nWe have the file, can listed it but how we are take it from sever. Thus we also need to code the download link.","238392a4":"# Q1\nNotice in the above we used _images.iloc?, can you confirm on the documentation? what is the role?\n\n**Jawab :**\n\nMenggunakan [iloc](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.iloc.html) untuk menyeleksi berdasarkan posisi.\n\n*  `labeled_images.iloc[0:5000,1:]` , data yang terseleksi kedalam variabel *images* dari baris ke-0 sampai 4999 dan kolom ke-1 sampai kolom terakhir.\n*   `labeled_images.iloc[0:5000,:1]` , data yang terseleksi kedalam variabel *labels* dari baris ke-0 sampai 4999 dan kolom ke-0."}}