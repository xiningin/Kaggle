{"cell_type":{"715f50b2":"code","0994fce7":"code","afe5aad6":"code","1f24e0ad":"code","bfd4fb03":"code","59167848":"code","044d1d82":"code","912c6c29":"code","bb1c0718":"code","43824850":"code","92f2dfbf":"code","033c7264":"code","2511f2a7":"code","5e1703ac":"code","a2e219c3":"code","b0e926b7":"code","1eae144e":"code","686e8df7":"code","407e0a6e":"code","16e7930c":"code","3ad3d5eb":"code","93431d12":"code","27860f3d":"code","27e2f628":"code","a055450b":"code","e1a4152b":"code","cb697281":"code","c5b5394c":"code","76be30cd":"code","b951652d":"code","ef312a65":"code","8d672b33":"markdown","1b640edf":"markdown","320950ad":"markdown","9b75ae0e":"markdown","c9119e53":"markdown","7d184ceb":"markdown","cfc7c7b1":"markdown","f59769fa":"markdown","56d11c25":"markdown","f91b3499":"markdown","31cab2d8":"markdown","d58b2617":"markdown","04305953":"markdown","b196152a":"markdown","d14540aa":"markdown","8dd7b707":"markdown","08ca74ad":"markdown","fbc835e7":"markdown","5daa9bf3":"markdown","7d3848a4":"markdown","1706fefe":"markdown","d78e67b4":"markdown"},"source":{"715f50b2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns \nimport matplotlib.pyplot as plt","0994fce7":"## Reading data to dataframes\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n","afe5aad6":"## Combining test and train df into a isngle dataframe\ndata = pd.DataFrame()\ndata = data.combine(train,test)\nprint(data.info())\nprint(\"\\n\\n\")\nprint(data.describe())","1f24e0ad":"data.isna().sum()","bfd4fb03":"## Survival dependence based on numerical variables\npd.pivot_table(data, index = 'Survived', values = ['Age','SibSp','Parch','Fare'])","59167848":"## Comparing impact on survival based on categorical variables\nprint(data[['Sex','Survived']].groupby(['Sex'] , as_index = False).mean())","044d1d82":"# print(data[['Fare','Survived']].groupby(['Fare'] , as_index = False).mean())\n\ndata['Fare_Range'] = pd.qcut(data['Fare'], 4)\n\nprint(data[['Fare_Range','Survived']].groupby(['Fare_Range'] , as_index = False).mean())","912c6c29":"print(data[['Pclass','Survived']].groupby(['Pclass'] , as_index = False).mean())","bb1c0718":"data['Family_size'] = data['Parch'] + data['SibSp'] + 1\n    \nprint(data[['Family_size','Survived']].groupby(['Family_size'] , as_index = False).mean())","43824850":"data['Is_Alone'] = np.where(data['Family_size'] == 1 , 1 , 0)\n\n\nprint(data[['Is_Alone','Survived']].groupby(['Is_Alone'] , as_index = False).mean())\n","92f2dfbf":"data['Titles'] = data['Name'].str.extract(r', (\\w+\\.)')\n\n## Categorizing titles\n\ndata['Titles'] = data['Titles'].replace(['Capt.', 'Col.',  'Don.',  'Dr.', 'Jonkheer.',  'Lady.',  'Major.',  'Master.',\n                                           'Rev.',  'Sir.', np.nan] , 'Special')\n\ndata['Titles'] = data['Titles'].replace(['Mlle.','Mlle','Ms.','Miss.'],'Miss')\ndata['Titles'] = data['Titles'].replace(['Mme.','Mme','Mrs.'],'Mrs')\ndata['Titles'] = data['Titles'].replace('Mr.','Mr')\n\n\nprint(data[['Titles','Survived']].groupby(['Titles'] , as_index = False).mean())","033c7264":"##Replace age nulls with median\ndata['Age'].fillna(data['Age'].median(), inplace=True)\n","2511f2a7":"##Analysing nulls in Embarked column\ndata[data['Embarked'].isna()]\n","5e1703ac":"print(data.loc[(data['Sex'] == \"female\") & (data['Fare'] <=  80)])\n\ndata['Embarked'].fillna(\"S\", inplace = True)","a2e219c3":"## Now that we have more or less removed all the nulls and analysed the data, we will apply the above transformation to our test-train dataset","b0e926b7":"combine = [test,train]","1eae144e":"for dfs in combine:\n    print(dfs)","686e8df7":"for dfs in combine:\n    \n    dfs['Family_size'] = dfs['Parch'] + dfs['SibSp'] + 1\n    \n    \n    dfs['Is_Alone'] = np.where(dfs['Family_size'] == 1 , 1 , 0)\n    \n    \n    dfs['Titles'] = dfs['Name'].str.extract(r', (\\w+\\.)')\n\n    dfs['Titles'] = dfs['Titles'].replace(['Capt.', 'Col.',  'Don.',  'Dr.', 'Jonkheer.',  'Lady.',  'Major.',\n                                           'Rev.',  'Sir.', np.nan] , 'Special')\n    dfs['Titles'] = dfs['Titles'].replace(['Mlle.','Mlle','Ms.','Miss.'],'Miss')\n    dfs['Titles'] = dfs['Titles'].replace(['Mme.','Mme','Mrs.'],'Mrs')\n    dfs['Titles'] = dfs['Titles'].replace(['Mr.','Master.'],'Mr')\n\n    \n    dfs['Age'].fillna(dfs['Age'].median(), inplace=True)\n    \n    \n    dfs['Embarked'].fillna(\"S\", inplace = True)\n    \n## Mapping to numericals\n\n    dfs['Sex'] = dfs['Sex'].replace(['male','female'],[0,1])\n    \n    title_mapping = {\"Miss\" : 1, \"Mr\" : 2, \"Mrs\" : 3, \"Special\" : 4}\n    dfs[\"Titles\"] = dfs[\"Titles\"].map(title_mapping)\n    dfs[\"Titles\"] = dfs[\"Titles\"].fillna(0)\n\n    dfs['Embarked'] = dfs['Embarked'].replace(['S','C','Q'],[0,1,2])\n    \n    \n    dfs.loc[ dfs['Fare'] <= 7.91, 'Fare'] = 0\n    dfs.loc[(dfs['Fare'] > 7.91) & (dfs['Fare'] <= 14.454), 'Fare'] = 1\n    dfs.loc[(dfs['Fare'] > 14.454) & (dfs['Fare'] <= 31), 'Fare']   = 2\n    dfs.loc[ dfs['Fare'] > 31, 'Fare'] = 3\n    dfs['Fare'] = dfs['Fare'].astype(int, errors='ignore').fillna(0)\n    \n    \n    \n    dfs.loc[ dfs['Age'] <= 16, 'Age'] = 0\n    dfs.loc[(dfs['Age'] > 16) & (dfs['Age'] <= 32), 'Age'] = 1\n    dfs.loc[(dfs['Age'] > 32) & (dfs['Age'] <= 48), 'Age'] = 2\n    dfs.loc[(dfs['Age'] > 48) & (dfs['Age'] <= 64), 'Age'] = 3\n    dfs.loc[ dfs['Age'] > 64, 'Age']\n\n    \n    ","407e0a6e":"## Find correlation between features\n\ncorr = train.corr()\ncorr.style.background_gradient(cmap='coolwarm')\n","16e7930c":"print(test)","3ad3d5eb":"drop_dict = ['Name', 'Ticket', 'Cabin', 'SibSp',\\\n                 'Parch','Family_size']\n\n\n\ntrain = train.drop(drop_dict, axis = 1)\ntest = test.drop(drop_dict, axis = 1)\n","93431d12":"print(train)\n\nprint(\"\\n\" + \"__\"+ \"\\n\")\n\nprint(test)","27860f3d":"X_train = train.drop([\"Survived\",\"PassengerId\"], axis=1)\nY_train = train[\"Survived\"]\nX_test  = test.drop(\"PassengerId\", axis=1).copy()\n\n\nX_train.shape, Y_train.shape, X_test.shape","27e2f628":"## KNN prediction test\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\npredn_knn = knn.predict(X_test)\nacc_knn = knn.score(X_train, Y_train)\n\nacc_knn","a055450b":"## Decision Tree test\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\npredn_dt = decision_tree.predict(X_test)\nacc_decision_tree = decision_tree.score(X_train, Y_train)\n\nacc_decision_tree","e1a4152b":"# Random Forest Test\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\npredn_rf = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = random_forest.score(X_train, Y_train)\n\nacc_random_forest","cb697281":"## Gradient Boosting Test\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nGradientBoosting = GradientBoostingClassifier(n_estimators=100)\nGradientBoosting.fit(X_train, Y_train)\npredn_gb = GradientBoosting.predict(X_test)\nGradientBoosting.score(X_train, Y_train)\nacc_GradientBoosting = GradientBoosting.score(X_train, Y_train)\n\nacc_GradientBoosting\n","c5b5394c":"## SVM\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nclf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\nclf.fit(X_train, Y_train)\npredn_svc = clf.predict(X_test)\n\nacc_svc = clf.score(X_train, Y_train)\n\nacc_svc","76be30cd":"## Voting Classifier Test\nfrom sklearn.ensemble import VotingClassifier\n\nensemble = VotingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=3)),\n                                        ('GB',GradientBoostingClassifier(n_estimators=100)),\n                                        ('RF', RandomForestClassifier(n_estimators=100, random_state=0)),\n                                        ('DT', DecisionTreeClassifier(random_state=0)),],\n                           voting='soft').fit(X_train, Y_train)\npredn_vc = ensemble.predict(X_test)\n\nacc_vc = ensemble.score(X_train, Y_train)\n\nacc_vc","b951652d":"table = pd.DataFrame({\n    'Model': ['KNN','Random Forest','Decision Tree','GradientBoosting','SVC','Voting Classifier'],\n    'Score': [ acc_knn, acc_random_forest, acc_decision_tree , acc_GradientBoosting,acc_svc,acc_vc]})\ntable['Score'] = table['Score']*100\n\nprint(table)\n\n\n","ef312a65":"# submission = pd.DataFrame({\n#         \"PassengerId\": test[\"PassengerId\"],\n#         \"Survived\": predn_gb\n#     })\n\n# submission.to_csv(\"submission.csv\" , index = False)\n\n# submission","8d672b33":"Identifying passengers with Sex = Female and Fare<80, to see where majority of the passengers embarked from..\n\nReplacing nulls in column 'Embarked' with 'S'. ","1b640edf":"Import all necessary libraries.\n\nNote: I am not a fan of visualizations(you will rarely find one in my notebook) and I tend to understand the data better when I look at tables and raw numbers.","320950ad":"I combined both the dataframes for my EDA, this helps you understand and identify pattern better and also find edge cases if any","9b75ae0e":"Categorizing titles ","c9119e53":"**2. Understanding the Data**","7d184ceb":"High correlation between SibSP,Parch and Family_Size.","cfc7c7b1":"Table to calculate percentage accuracy for training dataset","f59769fa":"4. **Model Selection**","56d11c25":"Similarly, performing the same exercise for categorical features. Sex of the Passenger seems to have significant impact on his\/her probability of survival.","f91b3499":"1. **Imports**","31cab2d8":"Preparing dataset for Model","d58b2617":"We create a new column 'Family_size' and check it's impact on Survival","04305953":"Read the data into dataframes","b196152a":"Dropping columns not relevant to our models","d14540aa":"From the table in #2, we can deduce that Fare has a high SD(standard deviation). Hense to reduce SD, it might be a better idea to divide it into groups based on a quantile system, pd.qcut() helps us achieve exactly that.\n\n","8dd7b707":"Pclass dependance","08ca74ad":"This returns the null count from each column of our combined dataset. As can be seen the dataset is fairly good, the majority of nulls lie in the three columns namely: \"Age\",\"Cabin\" and \"Embarked\". It might be a good idea to drop the column \"Cabin\"(approx 75% of the data is missing).","fbc835e7":"Trying out different models","5daa9bf3":"3. **Feature Engineering** ","7d3848a4":"**If you find this notebook useful, support with an upvote\ud83d\udc4d**","1706fefe":"Creating a table to understand the dependence of numerical features on Survival. It can be deduced that all these features have some correlation on the probability of survival.","d78e67b4":"Creating new column 'Is_Alone' and checking it's impact on Survival"}}