{"cell_type":{"f0c993c2":"code","43b18a8f":"code","c422354c":"code","01031a9b":"code","477e9de1":"code","b3dec557":"code","ff4c2143":"code","61d746d4":"code","41d08a5f":"code","44792789":"code","81202f81":"code","9ed756e8":"code","b4cc4ebe":"code","68db2067":"code","27e43b98":"code","67807a70":"code","43efef70":"code","752a9791":"code","5ec0a918":"code","1d6828e9":"code","3d400398":"code","c01fd931":"code","235a91de":"code","8b354ac6":"code","5c0092f7":"code","4199846b":"code","19969b2b":"code","4595afd0":"code","47752a80":"code","dd018978":"code","17bd1eca":"code","e8dc4bd9":"code","494a7dbc":"code","cade9896":"code","3ceb8769":"code","3a54e0bf":"code","43906465":"code","8de4b081":"code","4410750f":"code","728e3f74":"code","238587f9":"code","dab50fbd":"code","7895a71a":"code","0444ddf1":"code","644c98b4":"code","086d6cb8":"code","266f4bbc":"code","7ab0d09a":"code","294aea5e":"code","07e3338c":"code","7130d844":"code","c776adec":"code","ffc82734":"code","e9a2fbd9":"code","b22685ac":"code","4f672553":"code","1444b7a7":"code","ae0c3a9c":"code","6f935808":"code","92af06b9":"code","f5d40a2e":"code","5dfbbb6e":"code","07373f4f":"code","bb162734":"code","4c5efdc4":"code","f944fbe6":"code","27b4cf71":"code","eff8b394":"code","4dbd79ee":"code","6e2e0e07":"code","8d06f5da":"code","ca09e4a0":"code","37160861":"code","3f1e1f2c":"code","d97a57ff":"code","3489acb3":"code","4877e067":"code","403c5b76":"markdown","5fd29602":"markdown","8187b221":"markdown","9861f578":"markdown","408509d3":"markdown","fa0ef399":"markdown","5abad07b":"markdown","c4e4388f":"markdown","24f65975":"markdown","98ca0ed5":"markdown","f9d25747":"markdown","24e5b10a":"markdown","a170f8ec":"markdown","7887e87a":"markdown"},"source":{"f0c993c2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score,\\\n                            precision_score, recall_score, roc_curve,\\\n                            roc_auc_score, plot_roc_curve, plot_precision_recall_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nSEED = 41","43b18a8f":"df = pd.read_csv(\"http:\/\/bit.ly\/wkspdata\")\ndf.head()","c422354c":"df.describe()","01031a9b":"#shows the std dev. what % of sample survived. eg 0.48% of sample size survived\n#average age of the groups\n#average family size","477e9de1":"df.info()","b3dec557":"#we can see age,cabin and 'embarked' has a few null values","ff4c2143":"# look how big data is, how much memory is it using, what kind of data type we\n# are working with, whats the range and any missing values","61d746d4":"df.shape","41d08a5f":"df.isna().any().any()\n# df.isna().sum()","44792789":"df.hist(figsize=(12,5), layout=(2,3))\nplt.show()","81202f81":"#frm this histogram, looks slighltly imbalanaced but workable. \n# pclass may be categorical. \n# sibsp, parch and fare looks it is categorical","9ed756e8":"df.plot.kde(figsize=(12,5), layout=(2,3), subplots=True)\nplt.show()","b4cc4ebe":"# kernel density estimation","68db2067":"df.boxplot(figsize=(7,5))\nplt.show()","27e43b98":"#boxplot\n# can see that age and fare has many outliers and abnomalies. We will need to\n# work on them later.","67807a70":"features = ['age', 'sibsp', 'parch', 'fare']\n\nplt.figure(figsize=(15, 4))\nsns.set(font_scale= 1.2)\nsns.set_style('ticks')\n\nfor i, feature in enumerate(features):\n    plt.subplot(1, 4, i+1)\n    sns.violinplot(data=df, y=feature)  \n    \nsns.despine()","43efef70":"# violin plot\n# we can see the distribution not bell-shaped curved\n# can see alot of outliers in sibsp and parch","752a9791":"df.nunique()","5ec0a918":"# we can see the distribution among the unique categorical items","1d6828e9":"df['pclass'].value_counts()","3d400398":"df['sex'].value_counts()","c01fd931":"df['age'].value_counts()","235a91de":"df['sibsp'].value_counts()","8b354ac6":"df['parch'].value_counts()","5c0092f7":"df['fare'].value_counts()","4199846b":"df['cabin'].nunique()","19969b2b":"# to see what rows in 'cabin' are missing a value","4595afd0":"df['cabin'].unique()","47752a80":"df['cabin'].value_counts()","dd018978":"df[df['cabin'].isna()]","17bd1eca":"# see the rows where 'cabin' is NAN (null)","e8dc4bd9":"df['embarked'].value_counts()","494a7dbc":"# to see at which location, these passengers boarded from","cade9896":"_ = sns.countplot(x=\"survived\", data=df)","3ceb8769":"# slight imbalanced but still okay however might cause slight inaccuracy.","3a54e0bf":"cat_features = ['pclass', 'sex', 'sibsp', 'parch', 'embarked']\n\nplt.figure(figsize=(15, 10))\nsns.set(font_scale= 1.2)\nsns.set_style('ticks')\n\nfor i, feature in enumerate(cat_features):\n    plt.subplot(2, 3, i+1)\n    sns.countplot(data=df, x=feature, hue='survived')  \n    \nsns.despine()","43906465":"num_features = ['age', 'fare']\nsns.set_style('white')\n\nplt.figure(figsize=(15, 4))\nfor i, feature in enumerate(num_features):\n    plt.subplot(1, 2, i+1)\n    plt.hist(x=[df[feature][df['survived'] == 1], \n                df[feature][df['survived'] == 0]],\n             stacked=True, \n             label=['Survived', 'Not survived'], \n             bins=20, \n             color=['orange', 'b'])\n    plt.legend()\n    plt.xlabel(f'{feature}', fontsize=15)\n    plt.ylabel('Count', fontsize=15)","8de4b081":"# graph not very clear but suppose to be able to tell whether age and fare plays\n# a factor into surviving","4410750f":"num_features = ['age', 'fare']\n\nplt.figure(figsize=(15, 4))\nsns.set(font_scale= 1.2)\nsns.set_style('ticks')\n\nfor i, feature in enumerate(num_features):\n    plt.subplot(1, 2, i+1)\n    ax = sns.kdeplot(df[feature][df['survived'] == 1], color=\"blue\", shade=True)\n    sns.kdeplot(df[feature][df['survived'] == 0], color=\"red\", shade=True)\n    plt.legend(['Survived', 'Not survived'])\n    ax.set(xlabel=feature)\n    plt.xlim(-10,85) ","728e3f74":"# density curve.\n# suggests that younger people tend to survive. people who paid cheap fares\n# survived less frequently than who paid more.","238587f9":"df = df[[c for c in df if c not in ['survived']] + ['survived']]","dab50fbd":"# correlation matrix heatmap visualization\nsns.set(style=\"white\")\n\n# Generate a mask for the upper triangle\nmatrix = np.triu(df.corr())\n\n# Set up the matplotlib figure to control size of heatmap\nfig, ax = plt.subplots(figsize=(16,12))\n\n# Plot the heatmap\n_ = sns.heatmap(df.corr(), mask=matrix, annot=True, annot_kws={\"size\": 12}, square=True, \n                cmap='coolwarm' , vmin=-1, vmax=1, fmt='.2f') ","7895a71a":"df.drop(['cabin'], axis=1, inplace=True)","0444ddf1":"# drop cabin because alot of missing data and after investigating. it doesn't\n# worth to impute in","644c98b4":"df.drop(['name', 'ticket'], axis=1, inplace=True)","086d6cb8":"# name and ticket not worth imputing it","266f4bbc":"df['age'].fillna(df['age'].median(), inplace=True)\n# df['age'].fillna(df['age'].mean(), inplace=True)","7ab0d09a":"# null values in 'age' filled up with mean","294aea5e":"df['embarked'].fillna(df['embarked'].value_counts().idxmax(), inplace=True)\n# df['embarked'] = df['embarked'].apply(lambda x: 'S' if pd.isna(x) else x)","07e3338c":"# we can choose to get rid of rows that have 'embarked' empty but in this case\n# we impute with the class with highest number.","7130d844":"df.info()","c776adec":"df.isna().sum().sum()","ffc82734":"df = pd.get_dummies(df, columns=['embarked', 'pclass', 'sibsp', 'parch'])\n# df = pd.get_dummies(df, columns=['embarked', 'pclass', 'sibsp', 'parch'], drop_first=True)","e9a2fbd9":"df['sex'] = df['sex'].apply(lambda x: 1 if x=='male' else 0)","b22685ac":"df.info()","4f672553":"df = df[[c for c in df if c not in ['survived']] + ['survived']]","1444b7a7":"# correlation matrix heatmap visualization\nsns.set(style=\"white\")\n\n# Generate a mask for the upper triangle\nmatrix = np.triu(df.corr())\n\n# Set up the matplotlib figure to control size of heatmap\nfig, ax = plt.subplots(figsize=(16,12))\n\n# Plot the heatmap\n_ = sns.heatmap(df.corr(), mask=matrix, annot=True, annot_kws={\"size\": 12}, square=True, \n                 cmap='coolwarm' , vmin=-1, vmax=1, fmt='.2f') ","ae0c3a9c":"# from above, we can see if there is any multi collinearity","6f935808":"X = df.drop(['survived'], axis=1)\ny = df['survived']","92af06b9":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=SEED)","f5d40a2e":"scaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)","5dfbbb6e":"# we normalize\/standardize data so that the ranges of these values are the same.","07373f4f":"%%time\nlr_baseline = LogisticRegression(random_state=SEED)\nscores = cross_val_score(lr_baseline,\n                         X_train_scaled,\n                         y_train,\n                         scoring='accuracy',\n                         cv=5,\n                         n_jobs=-1)","bb162734":"baseline_mean_score = scores.mean()\nprint(\"Baseline Model's score: {:.2f} (avg) {:.2f} (std)\".format(scores.mean(), scores.std()))","4c5efdc4":"# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression(random_state=SEED)))\nmodels.append(('SVM', SVC(random_state=SEED)))\nmodels.append(('RF', RandomForestClassifier(random_state=SEED)))\n\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'","f944fbe6":"%%time\nfor name, model in models:\n#     kfold = KFold(n_splits=10)\n    cv_results = cross_val_score(model, \n                                 X_train_scaled, \n                                 y_train, \n                                 cv=5, \n                                 scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"{:3}: Mean score {:.4f} (Std {:.4f})\".format(name, cv_results.mean(), cv_results.std())\n    print(msg)\n\n# boxplot algorithm comparison\nfig = plt.figure(figsize=(12,6))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.plot([-2,10], [baseline_mean_score, baseline_mean_score])\nplt.xlim(0, 8)\nplt.show()","27b4cf71":"#create a simple box plot to compare the all the five scores\n# LR has very big interqualite range\n# orange line is the mean.\n# Logistic regression is the highest. but it's wild because it can be as high\n# as 0.82 or 0.76\n# SVM has an anomaly (small dot above)\n# because LR is wild and SVM and RF is not, we maybe able to get higher score \n#if we fine-tune SVM and RF.","eff8b394":"%%time\n# Hyperparameter fine-tuning for Support Vector Machine on multi-class dataset\nparameters = {'C': np.logspace(-5, 2, 10),\n              'kernel' : ['linear', 'poly', 'rbf'],\n              'gamma' : ['scale', 'auto'],\n              'degree': [2, 3, 5]\n              }\n    \ngs_clf = GridSearchCV(SVC(random_state=SEED),\n                      parameters, \n                      cv=5,\n                      scoring='accuracy',\n#                       scoring='f1_macro',\n#                       scoring='roc_auc_ovr',\n                      n_jobs=-1)\n_ = gs_clf.fit(X_train_scaled, y_train)\n\nfinal_clf = gs_clf.best_estimator_\n\nprint(gs_clf.best_estimator_)\nprint(gs_clf.best_params_)\nprint(gs_clf.best_score_)","4dbd79ee":"# %%time\n# Hyperparameter fine-tuning for logistic Regression on multi-class dataset\n# parameters = {'C': np.logspace(-5,2,10),\n#                 'max_iter':[100,200,300,400,500]\n#             }\n#\n# gs_clf = GridSearchCV(LogisticRegression(random_state=SEED),\n#                      parameters, \n#                      cv= 5,\n#                      scoring='accuracy'\n#                      scoring='f1_macro'\n#                      scoring='roc_auc_ovr',\n#                      n_jobs=-1)\n#  _ = gs_clf.fit(X_train_scaled, y_train)\n\n# final_clf = gs_clf.best_estimator_\n\n# print(gs_clf.best_estimator_)\n# print(gs_clf.best_params_)\n# print(gs_clf.best_score_)\n","6e2e0e07":"# %%time\n# Hyperparameter fine-tuning for Random Forest on multi-class dataset\n# parameters = {'n_estimators': [200, 500, 800],\n#               'max_depth': [5, 10, None],\n#               'min_samples_split': [3, 5, 10, 15]}\n\n# gs_clf = GridSearchCV(RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=SEED),\n#                       parameters, \n#                       cv=10,\n#                       scoring='accuracy',\n#                       n_jobs=-1)\n# _ = gs_clf.fit(X_train_scaled, y_train)\n\n# final_clf = gs_clf.best_estimator_\n\n# print(gs_clf.best_estimator_)\n# print(gs_clf.best_params_)\n# print(gs_clf.best_score_)","8d06f5da":"y_pred = final_clf.predict(X_test_scaled)\n#when we evaluate, we must also use scaled version\n\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1score = f1_score(y_test, y_pred)\n\nmodels.append({'Name': 'Support Vector Machine',\n               'Accuracy': accuracy,\n               'Precision': precision,\n               'Recall': recall,\n               'F1 Score': f1score})\n\nprint(classification_report(y_test, y_pred))","ca09e4a0":"cm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (8,5))\n\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nsns.heatmap(conf_matrix.T, annot=True, fmt='d', cmap = 'YlGnBu')\n\n# print the scores on training and test set\ncm_title = 'Accuracy Score: {:.2%}'.format(accuracy)\n\n_ = plt.title(cm_title, size = 19)","37160861":"%%time\nrf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=SEED)\nrf.fit(X_train_scaled, y_train)","3f1e1f2c":"#use randomforest to help us find which feature is the most important.","d97a57ff":"# Creating the feature importances dataframe\nfeature_importance = np.array(rf.feature_importances_)\nfeature_names = np.array(X.columns)\n\nfeat_imp = pd.DataFrame({'feature_names':feature_names,'feature_importance':feature_importance})\nfeat_imp_sorted = feat_imp.sort_values('feature_importance', ascending=False)","3489acb3":"plt.figure(figsize=(10,8))\n_ = sns.barplot(x=feat_imp_sorted['feature_importance'], y=feat_imp_sorted['feature_names'])","4877e067":"#age, fare and sex play the most impt.  ","403c5b76":"## Data Preprocessing\/Feature Engineering","5fd29602":"### Split Data (Features\/Target, Train\/Validation\/Testing)","8187b221":"## Post-mortem Analysis","9861f578":"### Evaluate Multiple Models and Select Best Model","408509d3":"## Model Selection","fa0ef399":"### Train a Baseline Model using K-fold Cross Validation\n### (start with Logistic Regression for Classification problems, Linear Regression for Regression problems)","5abad07b":"#### Feature Scaling (Normalization\/Standardization)","c4e4388f":"## Load Database\/Import Data ","24f65975":"## Exploratory Data Analysis (EDA)","98ca0ed5":"### Hyperparameter Fine-tuning for final selected model","f9d25747":"## Import Library and Initialize Global Variables","24e5b10a":"### Evaluate the fine-tuned Model (whether it will generalize well with unseen data - testing dataset)","a170f8ec":"### Transform non-numerical features into numerical representation","7887e87a":"### Select and Impute Features"}}