{"cell_type":{"5ccf0b7c":"code","cbeee39b":"code","1af3812c":"code","7d59d4d7":"code","c3d2dc54":"code","df3716f1":"code","3b629e96":"code","12bd8b32":"code","86ebe0a5":"code","38e68807":"code","2d23d02a":"code","02f15015":"code","9749836f":"code","691ba839":"code","d16a1270":"code","fa856aa3":"code","b8a74bd2":"code","3a459a12":"code","f9735f43":"code","3591b5f5":"code","d7a2a266":"code","44522dda":"markdown","7e9fff5e":"markdown","4252216d":"markdown","0fbebbd0":"markdown","c5a97388":"markdown","9e1866b6":"markdown","ef0408bb":"markdown","a3bd1c83":"markdown","399fa928":"markdown","6bcc8f6b":"markdown","eb233e13":"markdown","86e72a98":"markdown","21067b6c":"markdown","164995bc":"markdown","53159030":"markdown","93a3a9bd":"markdown","df93e033":"markdown","314ae722":"markdown","9ea872b2":"markdown","ed9110e0":"markdown","e65a6407":"markdown","f8569142":"markdown","9c3ab6f7":"markdown","18792aab":"markdown","c71c2f95":"markdown","29769b27":"markdown"},"source":{"5ccf0b7c":"import pandas as pd\n\n# Dummy model and Decision Tree Classifier\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import(\n    cross_validate,\n    train_test_split\n)\nfrom sklearn.tree import (\n    DecisionTreeClassifier,\n    export_graphviz\n)\n\n# Visualization in the EDA section\nimport altair as alt\nalt.renderers.enable('kaggle')\nimport seaborn as sns\n\n# Visualizing the final tree diagram\nimport re\nimport graphviz\nfrom IPython.display import Image","cbeee39b":"# Will be required to download spotify dataset in the machine\nspotify_df = pd.read_csv('\/kaggle\/input\/spotifyclassification\/data.csv', index_col=0)\nspotify_df","1af3812c":"train_df, test_df = train_test_split(spotify_df,\n                                     train_size=0.8,\n                                     random_state=2021)","7d59d4d7":"spotify_df.shape","c3d2dc54":"train_df.shape[0]","df3716f1":"train_df.info()","3b629e96":"train_df.describe()","12bd8b32":"corr_df = train_df.corr('spearman').stack().reset_index(name='corr')\ncorr_df.loc[corr_df['corr'] == 1, 'corr'] = 0  # Remove diagonal\n# Use abs so that we can visualize the impact of negative correaltion  \ncorr_df['abs'] = corr_df['corr'].abs()\ncorr_df.sort_values('abs', ascending=False)","86ebe0a5":"alt.Chart(corr_df).mark_circle().encode(\n    x='level_0',\n    y='level_1',\n    size='abs',\n    color=alt.Color('corr',\n                    scale=alt.Scale(scheme='blueorange',\n                                    domain=(-1, 1))))","38e68807":"sns.violinplot(x=\"energy\",  y=\"target\", data=train_df, orient=\"h\")","2d23d02a":"sns.violinplot(x=\"acousticness\",  y=\"target\", data=train_df, orient=\"h\")","02f15015":"sns.violinplot(x=\"tempo\",  y=\"target\", data=train_df, orient=\"h\")","9749836f":"X = spotify_df.drop(columns=[\"song_title\", \"artist\", \"target\"])\ny = spotify_df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n        train_size=0.8, test_size=0.2, random_state=123)","691ba839":"def cross_validate_return(X_train, y_train, depth):\n    \"\"\"\n    Fits a Decision tree basis the depth provided and \n    returns the accuracies and depth as list\n\n    Parameters\n    ----------\n    X_train: numpy.ndarray\n        The X_train part of the data\n    y_train: numpy.ndarray\n        The y_train part of the data\n    depth: int\n        The depth of the Decision Tree\n\n    Returns\n    -------\n        list of train_score, test_score, depth\n    \"\"\"\n    model = DecisionTreeClassifier(max_depth=depth, random_state=123)\n    scores = cross_validate(model, X_train, y_train, \n                        return_train_score=True, cv=10)\n    return_df = pd.DataFrame(pd.DataFrame(scores).mean()).iloc[2:].T\n    return [round(return_df.train_score[0], 3),\n            round(return_df.test_score[0],3), depth]\n\ndata = []\ni = 1\nwhile i < 26:\n    data.append(cross_validate_return(X_train, y_train, i))\n    i += 1\n\ndf = pd.DataFrame(data, columns=['train_score', 'validation_score',\n                  'max_depth'])\ndf.head(n=5)","d16a1270":"cv_df = df.melt(value_vars=([\"train_score\", \"validation_score\"]), id_vars=[\"max_depth\"])\ncv_df.head(n=5)","fa856aa3":"alt.Chart(cv_df).mark_line().encode(\n    x=alt.X('max_depth', title='Max depth of tree'),\n    y=alt.Y('value', title='Score value',\n           scale=alt.Scale(domain=[0.6, 1.05])), \n    color=alt.Color('variable', title='Score'))","b8a74bd2":"optimal_depth = df.max_depth[df.validation_score.argmax()]\n\nprint(\"We pick the value where the cross-validation error is \"\n      \"minimum (or the cross-validation score is the highest)\"\n      f\" at max_depth = {optimal_depth}\")","3a459a12":"print(f\"Validation score at optimal depth: {df.validation_score[3]}\")","f9735f43":"optimal_model = DecisionTreeClassifier(max_depth=optimal_depth)\noptimal_model.fit(X_train, y_train)\nprint(f\"Score on test set: { optimal_model.score(X_test, y_test):.3f}\")","3591b5f5":"# adapted from https:\/\/stackoverflow.com\/questions\/44821349\/python-graphviz-remove-legend-on-nodes-of-decisiontreeclassifier\n\ndef display_tree(feature_names, tree, counts=False):\n    \"\"\"For binary classification only\"\"\"\n    dot = export_graphviz(\n        tree,\n        out_file=None,\n        feature_names=feature_names,\n        class_names=tree.classes_.astype(str),\n        impurity=False,\n    )\n    # dot = re.sub('(\\\\\\\\nsamples = [0-9]+)(\\\\\\\\nvalue = \\[[0-9]+, [0-9]+\\])(\\\\\\\\nclass = [A-Za-z0-9]+)', '', dot)\n    if counts:\n        dot = re.sub(\"(samples = [0-9]+)\\\\\\\\n\", \"\", dot)\n        dot = re.sub(\"value\", \"counts\", dot)\n    else:\n        dot = re.sub(\n            \"(\\\\\\\\nsamples = [0-9]+)(\\\\\\\\nvalue = \\[[0-9]+, [0-9]+\\])\", \"\", dot\n        )\n        dot = re.sub(\n            \"(samples = [0-9]+)(\\\\\\\\nvalue = \\[[0-9]+, [0-9]+\\])\\\\\\\\n\", \"\", dot\n        )\n    return graphviz.Source(dot)","d7a2a266":"Image(display_tree(X_train.columns, optimal_model).pipe(\"png\"))","44522dda":"Total data frame size has 2017 examples.  \nThe test size data frame has 20% of the examples = 404  \nThe training size data frame has 80% of the examples = 1613","7e9fff5e":"## 5. Data splitting ","4252216d":"For our basic analysis, we are avoiding vectorizing the text columns. This can be incorporated using CountVectorzier or TD-IDF.","0fbebbd0":"Luckily, we do not have any `NAN` values in the dataset. We have a combination of numeric and text features.","c5a97388":"#### **`max_depth` and the Fundamental Tradeoff**","9e1866b6":"- We can see in the plot that we get maximum validation score at max_depth = 4 and the difference between train and validation score is the least at this point as well. This will be our optimal maximum depth. \n- We assume that since it has delivered the best validation score at this depth, the test score will be best at this point as well.","ef0408bb":"### 4.1 Relationship among the features","a3bd1c83":"Test score is less than our cross-validation score. Our optimal depth ends up giving a fair estimate of how much accuracy we can expect from our best model.","399fa928":"## 7. Picking the best value for `max_depth`","6bcc8f6b":"The three standout correlation relationships which can be observed from the graph above are:\n1. `loudness` and `energy`\n2. `acousticness` and `energy`\n3. `danceability` and `valence`\n\nThe good thing is that the relationship is not too strong, and the rest of the features seem fairly unrelated to each other.","eb233e13":"Below we define a function to calculate the validation and train score at a specific depth.","86e72a98":"## 0. Importing packages","21067b6c":"We now melt the df so that we can plot train and validation score separately.","164995bc":"## 8. Model performance on the test set","53159030":"## 3. Preliminary EDA","93a3a9bd":"## 6. Hyperparameter optimization of max_depth","df93e033":"## 4. EDA ","314ae722":"After trying multiple relations between the target and features, we observe that:\n1. There is some bimodality for not liking the song for features `tempo` and `energy`.\n2. `acousticness` is unimodal but it is more dense when the user likes the song.","9ea872b2":"- Increasing the max_depth increases the training accuracy of the Decision Tree as we add more branches\/leaves to the tree and we end up adding all the random quirks of the training set.  \n- Increasing the max_depth increases the validation accuracy till a point and then it starts decreasing as the overfitting on the training set fails to generalize well on the validation set.","ed9110e0":"#  Classification using Decision Tree\n## Decision trees on Spotify Song Attributes dataset ","e65a6407":"## 1. Reading the data CSV","f8569142":"- `instrumentalness` seems the most important feature as it sits at the root and best divides the tree. In our exploratory analysis, we found `danceability` to be a good feature but it is not the best.\n- **Scaling not required:** While using Decision Tree Classification the scale doesn't matter as it does not depend on the variance in the data.\n- **More features can be added:** We can use the Bag of Words technique which throws away the order information of the word and counts the frequencies of each word in it, which is done by assigning a unique number to each word. Alternatively, we can use one-hot encoding to divide the categorical feature into multiple features with labels 0 and 1(when a particular artist is present). This could increase our data size by a big margin if the number of artists is large.","9c3ab6f7":"### 4.2 Relationship between the features and the target","18792aab":"## 2. Data splitting ","c71c2f95":"## 9. Visualizing our optimal Decision Tree!","29769b27":"There are not much outliers in the dataset as can be seen from the ranges above."}}