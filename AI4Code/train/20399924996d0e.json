{"cell_type":{"087064b0":"code","df61c117":"code","5f7b97db":"code","a29fbdd6":"code","548d5ff6":"code","0c5b2ea2":"code","78eab27b":"code","4341faab":"code","17e3bc3e":"code","fad5eadf":"code","d2c55d39":"code","bd67b900":"code","00e7e024":"code","6cce9cd5":"code","3517c85e":"code","409293c5":"code","14faef16":"code","4c3913c0":"code","07ca2c9a":"code","f6688320":"code","724dd8a0":"code","1a8fb962":"code","37144ef0":"code","bfd4abe5":"code","48cc94d4":"code","7a8b4724":"markdown","1df67977":"markdown","ff289560":"markdown","0f2793d4":"markdown","4a4a0d4f":"markdown","34485643":"markdown","da9af49a":"markdown","d5b39914":"markdown","e2d84ce1":"markdown","9a542927":"markdown","1c2c9dc8":"markdown","eb7f272d":"markdown","167f0603":"markdown","c29d7b00":"markdown","0526f355":"markdown","4e071c83":"markdown","44b9ae7d":"markdown","19f620f6":"markdown","e67e696d":"markdown","780f35bc":"markdown","e5c50c7e":"markdown"},"source":{"087064b0":"import pandas as pd\nimport numpy as np\n\ndata_red=pd.read_csv(\"..\/input\/wine-data\/winequality-red.csv\",sep=\",\")\ndata_white=pd.read_csv(\"..\/input\/wine-data\/winequality-white.csv\",sep=\",\")\n\nprint(data_red.head(10))","df61c117":"data_red[\"color\"]=\"red\"\ndata_white[\"color\"]=\"white\"","5f7b97db":"data_red_features = data_red.drop([\"color\"],axis=1)\ndata_white_features = data_white.drop([\"color\"],axis=1)\n\ndata_red_class = data_red.loc[:,\"color\"]\ndata_white_class = data_white.loc[:,\"color\"]\n\nprint(data_red_features.head(5))\nprint(data_red_class.head(5))","a29fbdd6":"data_unity = pd.concat([data_red,data_white],ignore_index=True)\n\nprint(data_unity)\n","548d5ff6":"data_unity.loc[:,\"color\"] = [ 1 if each ==\"red\" else 0 for each in data_unity.loc[:,\"color\"]]\nprint(data_unity)\n","0c5b2ea2":"import matplotlib.pyplot as plt\nx1 = data_unity.iloc[0:100,5].values.reshape(-1,1) # Chlorides\ny1 = data_unity.iloc[0:100,6].values.reshape(-1,1) # Total sulfur dioxide\nplt.scatter(x1,y1)\nplt.xlabel(\"Chlorides\")\nplt.ylabel(\"Total Sulfur Dioxide\")\nplt.show()\n","78eab27b":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x1,y1)\ny1_head = lr.predict(x1)\n\nx1 = data_unity.iloc[0:100,5].values.reshape(-1,1) # Chlorides\ny1 = data_unity.iloc[0:100,6].values.reshape(-1,1) # Total sulfur dioxide\nplt.scatter(x1,y1)\nplt.plot(x1,y1_head,color=\"red\",label=\"linear\")\nplt.xlabel(\"Chlorides\")\nplt.ylabel(\"Total sulfur dioxide\")\nplt.show()","4341faab":"from sklearn.metrics import r2_score\n\nprint(r2_score(y1,y1_head)) ","17e3bc3e":"z1 = data_unity.iloc[0:100,4:6] # Chlorides and free sulfur dioxide data \nprint(z1.head(5))\nmlr = LinearRegression()\nmlr.fit(z1,y1)\nprd=mlr.predict(z1)\nprint(prd[0]) # our prediction data of total sulfer dioxide","fad5eadf":"poly_data = pd.read_csv(\"..\/input\/zombie-population-in-turkey\/zombie-population.txt\",sep = \",\",header = None,names = [\"days\",\"zombie\"])\nx = poly_data.loc[:,\"days\"].values.reshape(-1,1)\ny = poly_data.loc[:,\"zombie\"].values.reshape(-1,1)\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\npoly_reg = PolynomialFeatures(degree=4) # x^n and degree=n=2. \nx_poly = poly_reg.fit_transform(x)\nlr.fit(x_poly,y)\ny_head= lr.predict(x_poly)\n\nplt.scatter(x,y,color =\"red\")\nplt.plot(x,y_head,color = \"green\",label=\"Pol\")\nplt.xlabel(\"Days\")\nplt.ylabel(\"Zombie population(Million)\")\nplt.show()","d2c55d39":"import numpy as np\nxt = np.arange(1,11).reshape(-1,1) \nyt = np.ravel(np.arange(30,130,10).reshape(-1,1))\n\nfrom sklearn.tree import DecisionTreeRegressor\ndtr = DecisionTreeRegressor()\ndtr.fit(xt,yt)\n\nxt_new = np.arange(min(xt),max(xt),0.01).reshape(-1,1)\nyt_head = dtr.predict(xt_new)\n\nplt.scatter(xt,yt,color=\"blue\")\nplt.plot(xt_new,yt_head,color=\"red\")\nplt.xlabel(\"xt\")\nplt.ylabel(\"yt\")\nplt.show()","bd67b900":"xr = np.arange(1,11).reshape(-1,1)\nyr = np.ravel(np.arange(30,130,10).reshape(-1,1))\n\nfrom sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(n_estimators=10000,random_state=1) # n_estimators is the number of trees in the forest.\nrfr.fit(xr,yr)\n\nxr_new = np.arange(min(xr),max(xr),0.01).reshape(-1,1)\nyr_head=rfr.predict(xr_new)\n\nplt.plot(xr_new,yr_head,color = \"red\",label= \"Random forest\")\nplt.scatter(xr,yr,color=\"blue\")\nplt.xlabel(\"xr\")\nplt.ylabel(\"yr\")\nplt.show()","00e7e024":"from sklearn.metrics import r2_score\n\ny_head_tree=dtr.predict(xt)\ny_head_random=rfr.predict(xr)\n\nprint(\"Decision Tree r2 square score:\",r2_score(yt,y_head_tree))\nprint(\"Random Forest r2 square score:\",r2_score(yr,y_head_random))","6cce9cd5":"x = data_unity.drop([\"color\"],axis=1)\ny = data_unity.loc[:,\"color\"].values # Slipt the data two part : Features and class so we can predict them as a feautre\n#and look to classifacation is true or not. \n\nx = (x-np.min(x))\/(np.max(x)-np.min(x)).values # normalization for preclusioning data dominion\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.2 , random_state=42) # test size=0.2 means \n#%20 of our data will be test data\nprint(x_test[0:3])\nprint(x_test.shape)\n\n# We have to transforme to our test and train data for matrix calculate\nx_train = x_train.T\nx_test  = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x Train: \", x_train.shape)\nprint(\"x Test: \", x_test.shape)\nprint(\"y Train: \", y_train.shape)\nprint(\"y Test: \", y_test.shape)\n","3517c85e":"def initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b\n\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","409293c5":"def forward_backward_propagation(w,b,x_train,y_train): #Now we begin to train our data for prediction\n    #forward propagation\n    z = np.dot(w.T,x_train)+b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]\n\n    #backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n\n    return cost,gradients","14faef16":"def update(w, b, x_train, y_train, learning_rate,number_of_iterarion): # Now, we reduce the cost function to find local minima\n    cost_list = []\n    cost_list2 = [] \n    index = []\n\n    \n    for i in range(number_of_iterarion):\n        \n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        #  \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n           \n\n  \n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(fontsize=12)\n    plt.yticks(fontsize=12)\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n","4c3913c0":"def predict(w,b,x_test): # we are choosing boundry( 0 and 1) for our data with a condition\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","07ca2c9a":"def logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations): #magic happen here\n\n    dimension =  x_train.shape[0]  \n    w,b = initialize_weights_and_bias(dimension)\n\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n\n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    print(\"test accuracy:%\",(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n\nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 2000)\n","f6688320":"#Now we use sklearn libray .Much easier thanks god\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T,y_train.T)\nprint(lr.predict(x_test.T[0:100]))\nprint(\"test accuracy :%\",(lr.score(x_test.T,y_test.T)*100))","724dd8a0":"#creating two class with features \nr = data_unity[data_unity.loc[:,\"color\"] == 1]\nw = data_unity[data_unity.loc[:,\"color\"] == 0]\n\n","1a8fb962":"plt.scatter(r.loc[:,\"chlorides\"],r.loc[:,\"total sulfur dioxide\"],color = \"red\",label=\"red\",alpha = 0.3)\nplt.scatter(w.loc[:,\"chlorides\"],w.loc[:,\"total sulfur dioxide\"],color = \"green\",label=\"white\",alpha = 0.3)\nplt.xlabel(\"Chlorides\")\nplt.ylabel(\"Total sulfur dioxide\")\nplt.legend()\nplt.show()","37144ef0":"x = data_unity.drop([\"color\"],axis=1)\ny = data_unity.loc[:,\"color\"] # Slipt the data two part : Features and class so we can predict them as a feautre\n#and look to classifacation is true or not. \n\nx = (x-np.min(x))\/(np.max(x)-np.min(x)).values # normalization for preclusioning data dominion\n\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size= 0.2 , random_state=42) # test size=0.2 means \n#%20 of our data will be test data","bfd4abe5":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=2) # K value\nknn.fit(x_train,y_train)\nknn.predict(x_test)\nprint(\" {} nn score {}\".format(2,knn.score(x_test,y_test)))","48cc94d4":"# find finest k value \n\nlist_k=[]\nfor i in range(1,20):\n    knn1=KNeighborsClassifier(n_neighbors=i)\n    knn1.fit(x_train,y_train)\n    a=list_k.append(knn1.score(x_test,y_test))\n    print(\"{} nn score {}\".format(i,list_k[i-1]))\n\nplt.plot(range(1,20),list_k)\nplt.xlabel(\"Knn\")\nplt.ylabel(\"Accuracy\")\nplt.show() ","7a8b4724":"**Coefficient of Determination**\n*  It determined that how many data points fall within the results of the line formed by the regression equation.\n* When result close to one,  fit is good for examination.\n\n","1df67977":"I think the result show us that our data is not complex one.Therefore decision tree regression is enough to analyse this data.","ff289560":"Now, I will add a class for each wine( I am doing this because  I will \tassemble red and white data after then class name help me that examine the unite data as red and white","0f2793d4":"**Chapter 5 : Random Forest Regression**\n* The random forest model is a type of ensemble learning model that makes predictions by combining decisions from series of \"trees\" It operate by contructing a multitude of decision tress at regression of the indiviula tress.\n* Mathametical formulation :\n    * h(x) = f1(x) + f2(x) + f3(x) + ......... + fn(x)      \n    * fi(x) trees      \n    * h(x) ensemble of tree     ","4a4a0d4f":"We have to arrange these class red and white as binary 0-1. Because machine learning module sklearn, which we will use this all the time , needs to integer. ","34485643":"**Chapter 7 :** KNN Algorithm\n*  KNN algorithm is lassification algorithm.\n* the data points are separated by KNN Algorithm into classes for predictig the classification of  new sample point.\n* We pick a k values, algorithm assigned to the class of nearest distance neighbors. Distance is calculated by euclidean geometry.","da9af49a":"**Chapter Zero : **Data Arrange\n* lets import datas with pandas module\n","d5b39914":"I split datas into class and feature for examination.","e2d84ce1":"**Mathematical formulation** \n* y1_head = b0+b1*x1\n* y1_head our predicted data from the graph\n* x1 is horizantal line data lets say Chlorides\n* b0 is intercept linear fit line(red one) to y_head(free sulfer dioxide axis)\n* b1 is the coefficient which come from slop of fit line( tan(alfa))\n*  Free Sulfer Dioxide \"y_head\" = b0 + b1*Chlorides \"x1\"   ","9a542927":"Graph show that when chlorides value between 0.1 and 0.6 quite likely wine is a red and If the amount of sulfur in the wine is high, the wine is likely to be white.(the two most distinct differences created these two features for red and white wine, unfortunatly data not effective for two feauture classification ).","1c2c9dc8":"**Chapter 2 :** Multiple Regression\n* Multiple regression is an extension of simple linear regression. It use for predicting the value of a dependent variable based on the value of two or more other independent variables.\n*  Let's assume that chlorides , free sulfur dioxide  are independent variable but these two feauteres affect total sulfur dioxide in a wine .Total sulfur dioxide is a dependent variable and we want to predict that.\n","eb7f272d":"**Intoduction**\n*  I create this kernel to improve my knowledge and code skill \n    * I took advantage of internet resources -  mostly lecture videos(DATAI TEAM)-  and information of chapters come from mostly wikipedia. \n*  I hope this kernel help someone to understand this consept. ( I am very new in this area therefore \u0131 have very high chance of doing wrong , please comment if something is wrong. Thank you.)\n","167f0603":"**Chapter One :**Linear Regression\n* linear regression is a linear approach to modelling the relationship between  dependent variable and  independent variables.\n* I arrange our data in two features for studying on  x and y axis","c29d7b00":"Wine data set is not suitable for polynomial regression. Therefore, we will create one.","0526f355":"Now we can unit the data_red and data_white","4e071c83":"k = 7 is finest value for accurate predection ","44b9ae7d":"**The Chapter 4 : Decision Tree Regression**\n* A decision tree is a decision support tool that uses a tree-like graph or model of decisions and their possible consequences, including chance event outcomes, resource costs, and utility. It is one way to display an algorithm that only contains conditional control statements.\n* We will split the data according to \"decision\" like x1 < 15 True or False then x2 > 20 True or False , results will crate split and each split became leaf then together generate tree.","19f620f6":"** Mathematical formulation** \n* y_head = b0+b1 x1+b2  x2\n* We know b0,b1 and x1 \n* b2 is a coeffient of the other independent variable \n* x2 other independent variable which is sulfur dioxide ","e67e696d":"**Chapter 6 :**Logistic Regression Classification\n* logistic model is one where the log-odds of the probability of an event is a linear combination of independent or predictor variables. The two possible dependent variable values are often labelled as \"0\" and \"1\", which represent outcomes such as pass\/fail, win\/lose, alive\/dead or healthy\/sick\n    *  Hypothesis is prediction for given x, y values between 0 and 1. Linear hypothesis  \u03b8^T x transform to sigmoid function g(\u03b8^T x ) = h\u03b8(x) --->(\u03b8^T x = z) and g(z) = 1\/(1+e^-z). Sigmoid function(h\u03b8(x)) give to probability of output 1 or 0 .\n    * Boundary : y=1 if h\u03b8(x)  \u2265 0.5 and y=0 if h\u03b8(x) < 0.5\n    * Cost function is a tool to understand our prediction is how much accurate.\n        * j(\u03b8) = 1\/m[sum(cost(h\u03b8(xi),yi)]--------------------> cost(h\u03b8(x),y) = - log(h\u03b8(x)) if y=1 and cost(h\u03b8(x),y) = -log(1-h\u03b8(x)) if y=0 .\n    * Gradient function create models learn by minimizing cost function and  find to local minima for reduce the prediction's error.\n        * we can reduce cost function's two condition to one.\n            * cost(h\u03b8(x),y) = - y log(h\u03b8(x))-(1-y) log(1-h\u03b8(x))\n        * \u03b8j = \u03b8j - \u03b1 \u2207j(\u03b8) .   \u03b1 is learning rate of algorithm\n* Why we need this :  Linear regression can be fail to classification even data points are linear.  When we add a point in linear distribution , classification algorithm can be mess up, it can predict wrong prediction.\n* Now we can build a algorithm with these hypothesis. But first we have to train part of our data.\n","780f35bc":"Linear regression is not good choose for this data. ","e5c50c7e":"**The Chapter 3 :** Polynomial Regression\n* polynomial regression is a form of regression analysis in which the relationship between the independent variable x and the dependent variable y is modelled as an nth degree polynomial in x.\n* polynomial regression is describe nonlinear data set.\n\n"}}