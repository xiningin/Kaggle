{"cell_type":{"ef777476":"code","db0537b4":"code","f35faf9f":"code","cfd6e065":"code","776fc03f":"code","d8eb1bed":"code","950698a8":"code","9f2adbe0":"code","dccb33f1":"code","c822b50c":"code","e58cea19":"code","8e157322":"code","bef226bf":"code","0fa5765e":"code","4a6cebdc":"code","690b0938":"code","61c74c6d":"code","56f413b3":"code","7a51bec2":"code","1cb7735c":"code","4dca8358":"code","40acdea2":"code","070f4f4d":"code","bcf1cd4e":"code","66d458cf":"code","a22e959c":"code","86cb8e73":"code","ddafe74f":"code","99d6a1a3":"code","a14bf7ce":"code","9810a376":"code","e9475750":"code","f532c967":"code","8952c472":"code","cee86552":"code","e9afa464":"code","3e56552a":"code","d0841b42":"code","6ee4b7f1":"code","a57876af":"code","af63b8fa":"code","2b97a0e9":"code","42e8b787":"code","35a3d6ca":"code","5fd62598":"code","220ed03a":"code","dd6fed58":"code","ad192042":"code","7be94aef":"code","3b3a2eb1":"code","1cb29ef3":"code","59c90937":"code","cb77f59c":"code","31d56dec":"code","f25bd41c":"code","1d64e442":"code","63955e69":"code","cbdf97e3":"code","fefbccbc":"code","97b2c51c":"code","7c291dd8":"code","a139fe42":"code","57458996":"code","c2df5f49":"code","7c3a2aca":"code","ba908dfa":"code","267cc795":"code","d93f62dc":"code","72ac2a62":"code","b843f297":"code","1757e781":"code","d5f42c42":"code","3c9a4bdb":"code","e6991dd4":"code","9fecdb6a":"code","5d4194cd":"code","584efbad":"markdown","5a14c74d":"markdown","8a48c385":"markdown","4a485767":"markdown","e8626caf":"markdown","f56781fc":"markdown","df051ece":"markdown","bc2face0":"markdown","6ecfd907":"markdown","8d29bcc8":"markdown","19aae3a0":"markdown","8c2a02b2":"markdown","eb3309b5":"markdown","bc1d8efa":"markdown","8bd21998":"markdown","00cd19ca":"markdown"},"source":{"ef777476":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n# #         print(os.path.join(dirname, filename))\n#         pass\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","db0537b4":"import numpy as np\nimport os\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.utils import Sequence, to_categorical\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\nimport seaborn as sns","f35faf9f":"patches_df = pd.read_csv('..\/input\/mammography-image-patches\/DDSM_Patch_Labels.csv', index_col=False)\ntrain = patches_df.loc[:, ~patches_df.columns.str.contains('^Unnamed')]\n\ntrain.columns = ['imgfile', 'label']\nlabels = train['label'].values\nlabels = (labels).astype(np.uint8)\nlabels2 = to_categorical(labels)\ntrain_index = np.arange(len(labels))\ncolumns = ['imgfile', 'Normal', 'Benign', 'Malignant']\ntrain_df = pd.DataFrame(index = train_index, columns=columns)\ntrain_df['imgfile'] = train['imgfile'].values\ntrain_df[['Normal', 'Benign', 'Malignant']] = labels2\nddsm_df = train_df\n\nddsm_df","cfd6e065":"patches_df = pd.read_csv('..\/input\/mammography-image-patches\/CBIS_Patch_Labels.csv', index_col=False)\ntrain = patches_df.loc[:, ~patches_df.columns.str.contains('^Unnamed')]\n\ntrain.columns = ['imgfile', 'label']\nlabels = train['label'].values\nlabels = (labels).astype(np.uint8)\nlabels2 = to_categorical(labels)\ntrain_index = np.arange(len(labels))\ncolumns = ['imgfile', 'Normal', 'Benign', 'Malignant']\ntrain_df = pd.DataFrame(index = train_index, columns=columns)\ntrain_df['imgfile'] = train['imgfile'].values\ntrain_df[['Normal', 'Benign', 'Malignant']] = labels2\ncbis_df = train_df\n\ncbis_df","776fc03f":"patches_df = ddsm_df.append([cbis_df], ignore_index=True, sort=False)","d8eb1bed":"patches_df","950698a8":"normal_count = patches_df[\"Normal\"].value_counts()[1]\nbenign_count = patches_df[\"Benign\"].value_counts()[1]\nmalignant_count = patches_df[\"Malignant\"].value_counts()[1]","9f2adbe0":"print('Normal count: ', normal_count)\nprint('Benign count: ', benign_count)\nprint('Malignant count: ', malignant_count)","dccb33f1":"class Generator(tf.keras.utils.Sequence):\n  def __init__(self, df, classes, image_source,\n               batch_size=16, size=(128, 128), \n               shuffle=True, random_state=13):\n    self.df = df\n    self.classes = classes\n    self.batch_size = batch_size\n    self.size = size\n    self.image_source = image_source\n    self.shuffle = shuffle\n    self.random_state = random_state\n    self.on_epoch_end()\n  \n  def __len__(self):\n    return int(np.floor(len(self.paths)\/self.batch_size))\n\n  def __getitem__(self, idx):\n    batch_paths = self.paths[idx*self.batch_size:(idx+1)*self.batch_size]\n    batch_imgs = self.get_data(batch_paths)\n    batch_labels = self.labels[idx*self.batch_size:(idx+1)*self.batch_size]\n    return batch_imgs, batch_labels\n  \n  def get_data(self, paths):\n    imgs = []\n    for path in paths:\n        img = cv2.imread(image_source + path, 0)\n        # img = process(img)\n        image = (img - img.min())\/(img.max() - img.min())\n        image = (image*255).astype(np.uint8)\n        pro_img = image\n        clahe = cv2.createCLAHE(clipLimit=1, tileGridSize=(5,5))\n        img_clip1 = clahe.apply(np.array(pro_img, dtype=np.uint8))\n        img_clip1 = np.expand_dims(cv2.resize(img_clip1, size), axis=2)\n\n\n        clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(5,5))\n        img_clip2 = clahe.apply(np.array(pro_img, dtype=np.uint8))\n        img_clip2 = np.expand_dims(cv2.resize(img_clip2, size), axis=2)\n\n        pro_img = np.expand_dims(cv2.resize(pro_img, size), axis=2)\n        synthetized = cv2.merge([pro_img, img_clip1, img_clip2])\n        synthetized = synthetized.reshape(128, 128, 3)\n        imgs.append(synthetized)\n        \n    return np.array(imgs)\n\n  def true_labels(self):\n    return self.labels\n\n  def on_epoch_end(self):\n    if self.shuffle:\n      dataframe = self.df.sample(frac=1., random_state=self.random_state)\n      self.random_state += 1\n      self.paths = dataframe[\"imgfile\"]\n      self.labels = dataframe[self.classes].values.astype(\"float32\")\n    else:\n      self.paths = self.df[\"imgfile\"]\n      self.labels = self.df[self.classes].values.astype(\"float32\")","c822b50c":"path = 'DDSM_Patches\/cancer_09-B_3078_1_204419.png'\nimage_source = \"..\/input\/mammography-image-patches\/\"\nimg = cv2.imread(image_source + path, 0)\nsize=(128, 128) \n\nprint(img.shape)\nplt.imshow(img)\nplt.show()\n\nimage = (img - img.min())\/(img.max() - img.min())\nimage = (image*255).astype(np.uint8)\npro_img = image\nclahe = cv2.createCLAHE(clipLimit=1, tileGridSize=(5,5))\nimg_clip1 = clahe.apply(np.array(pro_img, dtype=np.uint8))\nimg_clip1 = np.expand_dims(cv2.resize(img_clip1, size), axis=2)\n\n\nclahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(5,5))\nimg_clip2 = clahe.apply(np.array(pro_img, dtype=np.uint8))\nimg_clip2 = np.expand_dims(cv2.resize(img_clip2, size), axis=2)\n\npro_img = np.expand_dims(cv2.resize(pro_img, size), axis=2)\nsynthetized = cv2.merge([pro_img, img_clip1, img_clip2])\nsynthetized = synthetized.reshape(128, 128, 3)\nprint(synthetized.shape)\nplt.imshow(synthetized)\nplt.show()","e58cea19":"class_names = [\"Normal\", \"Benign\", \"Malignant\"]\nbatch_size = 16","8e157322":"def get_sample_counts(dataset, class_names):\n    df = dataset\n    total_count = df.shape[0]\n    labels = df[class_names].values\n    positive_counts = np.sum(labels, axis=0)\n    class_positive_counts = dict(zip(class_names, positive_counts))\n    return total_count, class_positive_counts","bef226bf":"# run if u want to train balanced\ntrain_df = patches_df\nmin_count = min(normal_count, benign_count, malignant_count)\n# normal\nnormal = train_df.loc[train_df['Normal']==1]\nnormal = shuffle(normal, random_state=42)\nnormal = normal[0:min_count]\n\n# benign\nbenign = train_df.loc[train_df['Benign']==1]\nbenign = shuffle(benign, random_state=42)\nbenign = benign[0:min_count]\n\n# malignant\nmalignant = train_df.loc[train_df['Malignant']==1]\nmalignant = shuffle(malignant, random_state=42)\nmalignant = malignant[0:min_count]\n\ntrain_df = normal.append([benign, malignant], ignore_index=True)\ntrain_df = shuffle(train_df)\n\ntrain_counts, train_pos_counts = get_sample_counts(train_df, class_names)\nprint(train_pos_counts)\n\npatches_df = train_df","0fa5765e":"validation_df = patches_df.sample(int(len(patches_df)*0.4),random_state=42)\ntrain_df = patches_df.drop(validation_df.index)","4a6cebdc":"len(validation_df)","690b0938":"train_gen = Generator(train_df, classes=class_names,\n                      image_source=\"..\/input\/mammography-image-patches\/\")","61c74c6d":"val_gen = Generator(validation_df, classes=class_names,\n                      image_source=\"..\/input\/mammography-image-patches\/\")","56f413b3":"print(len(train_gen), len(val_gen)) ","7a51bec2":"def create_model():\n    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False,\n                      input_shape=(128, 128, 3), pooling='avg')\n    x = base_model.output\n    x = tf.keras.layers.Dropout(0.15)(x)\n    preds = tf.keras.layers.Dense(3, activation='softmax')(x)\n    model = tf.keras.Model(inputs=base_model.input, outputs=preds)\n    return model\n\nmodel = create_model()","1cb7735c":"def plot(history, metric):\n  plt.plot(history.history[metric])\n  plt.plot(history.history['val_'+metric])\n  plt.title('model '+metric)\n  plt.legend(['train', 'val'])\n  plt.show()","4dca8358":"for layer in model.layers[:-1]:\n    layer.trainable = False","40acdea2":"ckpt = tf.keras.callbacks.ModelCheckpoint(\"patch_model_1.h5\",\n                                          monitor=\"val_accuracy\", verbose=1, save_best_only=True, save_weights_only=True)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\", tf.keras.metrics.AUC()])","070f4f4d":"h1 = model.fit(train_gen, steps_per_epoch=len(train_gen), epochs=3, validation_data=val_gen, callbacks=[ckpt])","bcf1cd4e":"plot(h1, \"loss\")\nplot(h1, \"accuracy\")","66d458cf":"for layer in model.layers[162:]:\n  layer.trainable = True","a22e959c":"ckpt = tf.keras.callbacks.ModelCheckpoint(\"patch_model_2.h5\",\n                                          monitor=\"val_accuracy\", verbose=1, save_best_only=True, save_weights_only=True)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-4)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\", tf.keras.metrics.AUC()])","86cb8e73":"h2 = model.fit(train_gen, steps_per_epoch=len(train_gen), epochs=10, validation_data=val_gen, callbacks=[ckpt]) ","ddafe74f":"plot(h2, \"loss\")\nplot(h2, \"accuracy\")","99d6a1a3":"model.load_weights(\"patch_model_2.h5\") ","a14bf7ce":"for layer in model.layers[:162]:\n  layer.trainable = True","9810a376":"ckpt = tf.keras.callbacks.ModelCheckpoint(\"patch_model_3.h5\",\n                                          monitor=\"val_accuracy\", verbose=1, save_best_only=True, save_weights_only=True)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nmodel.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=[\"accuracy\", tf.keras.metrics.AUC()])","e9475750":"h3 = model.fit(train_gen, steps_per_epoch=len(train_gen), epochs=8, validation_data=val_gen, callbacks=[ckpt])","f532c967":"plot(h3, \"loss\")\nplot(h3, \"accuracy\")","8952c472":"model.load_weights(\"patch_model_3.h5\")","cee86552":"test_gen = Generator(validation_df, classes=class_names, image_source=\"..\/input\/mammography-image-patches\/\", shuffle=False)\n\ny_hat = model.predict(test_gen, verbose=1)\ny_true = test_gen.true_labels()\n\ny_true = np.argmax(y_true, axis=1)\ny_pred = np.argmax(y_hat, axis=1)","e9afa464":"rep = classification_report(y_true[:len(y_pred)], y_pred, target_names=['Normal', 'Benign', 'Malignant'])\nprint(rep)\n\ndef display_confusion_matrix(y_true, preds, labels):\n    c_mat = confusion_matrix(y_true, preds)\n    disp = ConfusionMatrixDisplay(c_mat, display_labels=labels)\n    disp.plot()\n\ndisplay_confusion_matrix(y_true[:len(y_pred)], y_pred, labels=['Normal', 'Benign', 'Malignant'])","3e56552a":"import os \nos.chdir(r'\/kaggle\/working') \nfrom IPython.display import FileLink \nFileLink(r'patch_model_3.h5')","d0841b42":"ddsm_df = pd.read_csv('..\/input\/ddsm-images-cropped\/DDSM_Labels.csv', index_col=False)\nddsm_df = ddsm_df.loc[:, ~ddsm_df.columns.str.contains('^Unnamed')]\n\nminiddsm_df = pd.read_csv('..\/input\/miniddsm-and-mias-cropped-images\/Mini_DDSM_df.csv', index_col=False)\nminiddsm_df = miniddsm_df.loc[:, ~miniddsm_df.columns.str.contains('^Unnamed')]\n","6ee4b7f1":"ddsm_df","a57876af":"miniddsm_df","af63b8fa":"def add_root_ddsm(x):\n    return '..\/input\/ddsm-images-cropped\/' + x ","2b97a0e9":"ddsm_df['imgfile'] = ddsm_df['imgfile'].apply(add_root_ddsm)","42e8b787":"ddsm_df","35a3d6ca":"def add_root_miniddsm(x):\n    return '..\/input\/miniddsm-and-mias-cropped-images\/' + x ","5fd62598":"miniddsm_df['imgfile'] = miniddsm_df['imgfile'].apply(add_root_miniddsm)","220ed03a":"miniddsm_df","dd6fed58":"df = ddsm_df.append([miniddsm_df], ignore_index=True, sort=False)","ad192042":"df['imgfile'][542]","7be94aef":"normal_count = df[\"Normal\"].value_counts()[1]\nbenign_count = df[\"Benign\"].value_counts()[1]\nmalignant_count = df[\"Malignant\"].value_counts()[1]\n\nprint('Normal count: ', normal_count)\nprint('Benign count: ', benign_count)\nprint('Malignant count: ', malignant_count)","3b3a2eb1":"class Generator_2(tf.keras.utils.Sequence):\n  def __init__(self, df, classes, image_source,\n               batch_size=8, size=(int(768\/2), int(1024\/2)), \n               shuffle=True, random_state=13):\n    self.df = df\n    self.classes = classes\n    self.batch_size = batch_size\n    self.size = size\n    self.image_source = image_source\n    self.shuffle = shuffle\n    self.random_state = random_state\n    self.on_epoch_end()\n  \n  def __len__(self):\n    return int(np.floor(len(self.paths)\/self.batch_size))\n\n  def __getitem__(self, idx):\n    batch_paths = self.paths[idx*self.batch_size:(idx+1)*self.batch_size]\n    batch_imgs = self.get_data(batch_paths)\n    batch_labels = self.labels[idx*self.batch_size:(idx+1)*self.batch_size]\n    return batch_imgs, batch_labels\n  \n  def get_data(self, paths):\n    imgs = []\n    for path in paths:\n#         print(self.image_source)\n#         print(self.image_source + path)\n        img = cv2.imread(self.image_source + path, 0)\n        # img = process(img)\n        image = (img - img.min())\/(img.max() - img.min())\n        image = (image*255).astype(np.uint8)\n        pro_img = image\n        clahe = cv2.createCLAHE(clipLimit=1, tileGridSize=(5,5))\n        img_clip1 = clahe.apply(np.array(pro_img, dtype=np.uint8))\n        img_clip1 = np.expand_dims(cv2.resize(img_clip1, size), axis=2)\n\n\n        clahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(5,5))\n        img_clip2 = clahe.apply(np.array(pro_img, dtype=np.uint8))\n        img_clip2 = np.expand_dims(cv2.resize(img_clip2, size), axis=2)\n\n        pro_img = np.expand_dims(cv2.resize(pro_img, size), axis=2)\n        synthetized = cv2.merge([pro_img, img_clip1, img_clip2])\n#         print(synthetized.shape)\n        imgs.append(synthetized)\n    return np.array(imgs)\n\n  def true_labels(self):\n    return self.labels\n\n  def on_epoch_end(self):\n    if self.shuffle:\n      dataframe = self.df.sample(frac=1., random_state=self.random_state)\n      self.random_state += 1\n      self.paths = dataframe[\"imgfile\"]\n      self.labels = dataframe[self.classes].values.astype(\"float32\")\n    else:\n      self.paths = self.df[\"imgfile\"]\n      self.labels = self.df[self.classes].values.astype(\"float32\")","1cb29ef3":"path = 'DDSM_Images\/benign_03-A_1390_1.RIGHT_CC.LJPEG.1.jpg'\nimage_source = \"..\/input\/ddsm-images-cropped\/\"\nimg = cv2.imread(image_source + path, 0)\nsize=(int(768\/2), int(1024\/2))\n\nprint(img.shape)\nplt.imshow(img)\nplt.show()\n\n# img = process(img)\nimage = (img - img.min())\/(img.max() - img.min())\nimage = (image*255).astype(np.uint8)\npro_img = image\nclahe = cv2.createCLAHE(clipLimit=1, tileGridSize=(5,5))\nimg_clip1 = clahe.apply(np.array(pro_img, dtype=np.uint8))\nimg_clip1 = np.expand_dims(cv2.resize(img_clip1, size), axis=2)\n\n\nclahe = cv2.createCLAHE(clipLimit=2, tileGridSize=(5,5))\nimg_clip2 = clahe.apply(np.array(pro_img, dtype=np.uint8))\nimg_clip2 = np.expand_dims(cv2.resize(img_clip2, size), axis=2)\n\npro_img = np.expand_dims(cv2.resize(pro_img, size), axis=2)\nsynthetized = cv2.merge([pro_img, img_clip1, img_clip2])\n# synthetized = synthetized.reshape(size[0], size[1], 3)\nprint(synthetized.shape)\nplt.imshow(synthetized)\nplt.show()","59c90937":"# run if u want to train balanced\ntrain_df = df\nmin_count = min(normal_count, benign_count, malignant_count)\n# normal\nnormal = train_df.loc[train_df['Normal']==1]\nnormal = shuffle(normal, random_state=42)\nnormal = normal[0:min_count]\n\n# benign\nbenign = train_df.loc[train_df['Benign']==1]\nbenign = shuffle(benign, random_state=42)\nbenign = benign[0:min_count]\n\n# malignant\nmalignant = train_df.loc[train_df['Malignant']==1]\nmalignant = shuffle(malignant, random_state=42)\nmalignant = malignant[0:min_count]\n\ntrain_df = normal.append([benign, malignant], ignore_index=True)\n# train_df = train_df.append(ab, ignore_index=True)\ntrain_df = shuffle(train_df)\n\n# ab = validation_df.loc[validation_df['Abnormal']==1]\n# normal = validation_df.loc[validation_df['Normal']==1]\n# normal = shuffle(normal, random_state=2)\n# normal = normal[0:2000]\n\n# validation_df = normal.append(ab, ignore_index=True)\n# # validation_df = validation_df.append(ab, ignore_index=True)\n# validation_df = shuffle(validation_df)\n\ntrain_counts, train_pos_counts = get_sample_counts(train_df, class_names)\nprint(train_pos_counts)\n# dev_counts, dev_pos_counts = get_sample_counts(validation_df, class_names)\n# print(dev_pos_counts)\n\npatches_df = train_df","cb77f59c":"validation_df = patches_df.sample(int(len(patches_df)*0.3),random_state=42)\ntrain_df = patches_df.drop(validation_df.index)","31d56dec":"len(validation_df)","f25bd41c":"train_gen = Generator_2(train_df, classes=class_names,\n                      image_source=\"\")","1d64e442":"val_gen = Generator_2(validation_df, classes=class_names,\n                      image_source=\"\")","63955e69":"print(len(train_gen), len(val_gen)) ","cbdf97e3":"def create_model():\n  base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False,\n                    input_shape=(128, 128, 3), pooling='avg')\n  x = base_model.output\n  x = Dropout(0.15)(x)\n  preds = Dense(3, activation='softmax')(x)\n  model = Model(inputs=base_model.input, outputs=preds)\n  return model","fefbccbc":"# This code is originally from: https:\/\/github.com\/raghakot\/keras-resnet\n# Modified by Li Shen for DM challenge.\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input,\n    Activation,\n    Dropout,\n    Dense,\n    Flatten\n)\nfrom tensorflow.keras.layers import concatenate, add\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import (\n    MaxPooling2D,\n    AveragePooling2D,\n    GlobalAveragePooling2D\n)\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras import activations\nfrom tensorflow.keras.regularizers import l1, l2, l1_l2\nfrom tensorflow.keras import backend as K\n\nif K.image_data_format() == 'channels_last':\n    ROW_AXIS = 1\n    COL_AXIS = 2\n    CHANNEL_AXIS = 3\nelse:\n    CHANNEL_AXIS = 1\n    ROW_AXIS = 2\n    COL_AXIS = 3\n\n\n# Helper to build a conv -> BN -> relu block\ndef _conv_bn_relu(nb_filter, nb_row, nb_col, strides=(1, 1), \n                  weight_decay=.0001, dropout=.0, last_block=False):\n    def f(input):\n        conv = Conv2D(filters=nb_filter, kernel_size=(nb_row, nb_col), \n                      strides=strides, kernel_initializer=\"he_normal\", \n                      padding=\"same\", kernel_regularizer=l2(weight_decay))(input)\n        norm = BatchNormalization(axis=CHANNEL_AXIS)(conv)\n        if last_block:\n            return norm\n        else:\n            relu = Activation(\"relu\")(norm)\n            return Dropout(dropout)(relu)\n\n    return f\n\n\n# Helper to build a BN -> relu -> conv block\n# This is an improved scheme proposed in http:\/\/arxiv.org\/pdf\/1603.05027v2.pdf\ndef _bn_relu_conv(nb_filter, nb_row, nb_col, strides=(1, 1), \n                  weight_decay=.0001, dropout=.0):\n    def f(input):\n        norm = BatchNormalization(axis=CHANNEL_AXIS)(input)\n        activation = Activation(\"relu\")(norm)\n        activation = Dropout(dropout)(activation)\n        return Conv2D(filters=nb_filter, kernel_size=(nb_row, nb_col), \n                      strides=strides, kernel_initializer=\"he_normal\", \n                      padding=\"same\", \n                      kernel_regularizer=l2(weight_decay))(activation)\n\n    return f\n\n\n# Adds a shortcut between input and residual block and merges them with \"sum\"\ndef _shortcut(input, residual, weight_decay=.0001, dropout=.0, identity=True, \n              strides=(1, 1), with_bn=False, org=False):\n    # Expand channels of shortcut to match residual.\n    # Stride appropriately to match residual (width, height)\n    # Should be int if network architecture is correctly configured.\n    # !!! The dropout argument is just a place holder. \n    # !!! It shall not be applied to identity mapping.\n    # stride_width = input._keras_shape[ROW_AXIS] \/\/ residual._keras_shape[ROW_AXIS]\n    # stride_height = input._keras_shape[COL_AXIS] \/\/ residual._keras_shape[COL_AXIS]\n    # equal_channels = residual._keras_shape[CHANNEL_AXIS] == input._keras_shape[CHANNEL_AXIS]\n\n    shortcut = input\n    # 1 X 1 conv if shape is different. Else identity.\n    # if stride_width > 1 or stride_height > 1 or not equal_channels:\n    if not identity:\n        shortcut = Conv2D(filters=residual.shape[CHANNEL_AXIS],\n                          kernel_size=(1, 1), strides=strides,\n                          kernel_initializer=\"he_normal\", padding=\"valid\", \n                          kernel_regularizer=l2(weight_decay))(input)\n        if with_bn:\n            shortcut = BatchNormalization(axis=CHANNEL_AXIS)(shortcut)\n\n    addition = add([shortcut, residual])\n    if not org:\n        return addition\n    else:\n        relu = Activation(\"relu\")(addition)\n        return Dropout(dropout)(relu)\n\n\n# Builds a residual block with repeating bottleneck blocks.\ndef _residual_block(block_function, nb_filters, repetitions, \n                    is_first_layer=False, shortcut_with_bn=False, \n                    bottleneck_enlarge_factor=4, **kw_args):\n    def f(input):\n        for i in range(repetitions):\n            init_strides = (1, 1)\n            identity = True\n            if i == 0 and not is_first_layer:\n                init_strides = (2, 2)\n            if i == 0:\n                identity = False\n            input = block_function(nb_filters=nb_filters, \n                                   init_strides=init_strides, \n                                   identity=identity, \n                                   shortcut_with_bn=shortcut_with_bn,\n                                   enlarge_factor=bottleneck_enlarge_factor,\n                                   **kw_args)(input)\n        return input\n\n    return f\n\n\n# Basic 3 X 3 convolution blocks.\n# Use for resnet with layers <= 34\n# Follows improved proposed scheme in http:\/\/arxiv.org\/pdf\/1603.05027v2.pdf\ndef basic_block(nb_filters, init_strides=(1, 1), identity=True, \n                shortcut_with_bn=False, enlarge_factor=None, **kw_args):\n    def f(input):\n        conv1 = _bn_relu_conv(nb_filters, 3, 3, strides=init_strides, **kw_args)(input)\n        residual = _bn_relu_conv(nb_filters, 3, 3, **kw_args)(conv1)\n        return _shortcut(input, residual, identity=identity, \n                         strides=init_strides, \n                         with_bn=shortcut_with_bn, **kw_args)\n\n    return f\n\n\ndef basic_block_org(nb_filters, init_strides=(1, 1), identity=True, \n                    shortcut_with_bn=False, enlarge_factor=None, **kw_args):\n    def f(input):\n        conv1 = _conv_bn_relu(nb_filters, 3, 3, strides=init_strides, **kw_args)(input)\n        residual = _conv_bn_relu(nb_filters, 3, 3, last_block=True, **kw_args)(conv1)\n        return _shortcut(input, residual, identity=identity, \n                         strides=init_strides, \n                         with_bn=shortcut_with_bn, org=True, **kw_args)\n\n    return f\n\n\n# Bottleneck architecture for > 34 layer resnet.\n# Follows improved proposed scheme in http:\/\/arxiv.org\/pdf\/1603.05027v2.pdf\n# Returns a final conv layer of nb_filters * 4\ndef bottleneck(nb_filters, init_strides=(1, 1), identity=True, \n               shortcut_with_bn=False, enlarge_factor=4, **kw_args):\n    def f(input):\n        conv_1_1 = _bn_relu_conv(nb_filters, 1, 1, strides=init_strides, **kw_args)(input)\n        conv_3_3 = _bn_relu_conv(nb_filters, 3, 3, **kw_args)(conv_1_1)\n        residual = _bn_relu_conv(nb_filters * enlarge_factor, 1, 1, **kw_args)(conv_3_3)\n        return _shortcut(input, residual, identity=identity, \n                         strides=init_strides, \n                         with_bn=shortcut_with_bn, **kw_args)\n\n    return f\n\n\ndef bottleneck_org(nb_filters, init_strides=(1, 1), identity=True, \n                   shortcut_with_bn=False, enlarge_factor=4, **kw_args):\n    def f(input):\n        conv_1_1 = _conv_bn_relu(nb_filters, 1, 1, strides=init_strides, **kw_args)(input)\n        conv_3_3 = _conv_bn_relu(nb_filters, 3, 3, **kw_args)(conv_1_1)\n        residual = _conv_bn_relu(nb_filters * enlarge_factor, 1, 1, \n                                 last_block=True, **kw_args)(conv_3_3)\n        return _shortcut(input, residual, identity=identity, \n                         strides=init_strides, \n                         with_bn=shortcut_with_bn, org=True, **kw_args)\n\n    return f\n\ndef add_top_layers(model, image_size, patch_net='resnet50', block_type='resnet', \n                   depths=[512,512], repetitions=[1,1], \n                   block_fn=bottleneck_org, nb_class=3, \n                   shortcut_with_bn=True, bottleneck_enlarge_factor=4,\n                   dropout=.0, weight_decay=.0001,\n                   add_heatmap=False, avg_pool_size=(7,7), return_heatmap=False,\n                   add_conv=True, add_shortcut=False,\n                   hm_strides=(1,1), hm_pool_size=(5,5),\n                   fc_init_units=64, fc_layers=2):\n\n    def add_residual_blocks(block):\n        for depth,repetition in zip(depths, repetitions):\n            block = _residual_block(\n                block_fn, depth, repetition,\n                dropout=dropout, weight_decay=weight_decay,\n                shortcut_with_bn=shortcut_with_bn,\n                bottleneck_enlarge_factor=bottleneck_enlarge_factor)(block)\n        pool = GlobalAveragePooling2D()(block)\n        dropped = Dropout(dropout)(pool)\n        return dropped\n\n    last_kept_layer = model.layers[-5]\n    \n    block = last_kept_layer.output\n    channels = 3\n    image_input = Input(shape=(image_size[0], image_size[1], channels))\n    x = _conv_bn_relu(3, 3, 2, strides=(1, 1), weight_decay=.0001, dropout=.0, last_block=False)(image_input)\n    x = _conv_bn_relu(3, 3, 2, strides=(1, 1), weight_decay=.0001, dropout=.0, last_block=False)(x)\n    x = AveragePooling2D(pool_size=(4, 3))(x)\n\n    model0 = Model(inputs=model.inputs, outputs=block)\n    block = model0(x)\n    block = add_residual_blocks(block)\n    dense = Dense(nb_class, kernel_initializer=\"he_normal\", \n                    activation='softmax', \n                    kernel_regularizer=l2(weight_decay))(block)\n    model_addtop = Model(inputs=image_input, outputs=dense)\n\n    return model_addtop","97b2c51c":"tf.keras.backend.clear_session() ","7c291dd8":"class_names = ['Normal', 'Benign', 'Malignant']\nbatch_size = 8\nimage_size = (int(1024\/2), int(768\/2))","a139fe42":"model = create_model()\nmodel.load_weights(\"patch_model_3.h5\")\nimage_model = add_top_layers(model, image_size)","57458996":"ckpt = tf.keras.callbacks.ModelCheckpoint(\"whole_model.h5\",\n                                          monitor=\"val_accuracy\", verbose=1, save_best_only=True, save_weights_only=True)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nimage_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[\"accuracy\"])","c2df5f49":"h1 = image_model.fit(train_gen, steps_per_epoch=len(train_gen), epochs=20, validation_data=val_gen, callbacks=[ckpt]) ","7c3a2aca":"plot(h1, \"loss\")\nplot(h1, \"accuracy\")","ba908dfa":"image_model.load_weights(\"whole_model.h5\") ","267cc795":"test_gen = Generator_2(validation_df, classes=class_names, size=(image_size[1], image_size[0]),\n                     image_source=\"\", shuffle=False)\n\ny_hat = image_model.predict(test_gen, verbose=1)\ny_true = test_gen.true_labels()\n\ny_true = np.argmax(y_true, axis=1)\ny_pred = np.argmax(y_hat, axis=1)","d93f62dc":"rep = classification_report(y_true[:len(y_pred)], y_pred, target_names=['Normal', 'Benign', 'Malignant'])\nprint(rep)\n\ndef display_confusion_matrix(y_true, preds, labels):\n    c_mat = confusion_matrix(y_true, preds)\n    disp = ConfusionMatrixDisplay(c_mat, display_labels=labels)\n    disp.plot()\n\ndisplay_confusion_matrix(y_true[:len(y_pred)], y_pred, labels=['Normal', 'Benign', 'Malignant'])","72ac2a62":"model = create_model()\nimage_model = add_top_layers(model, image_size)","b843f297":"ckpt = tf.keras.callbacks.ModelCheckpoint(\"whole_model_2.h5\",\n                                          monitor=\"val_accuracy\", verbose=1, save_best_only=True, save_weights_only=True)\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\nimage_model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=[\"accuracy\"])","1757e781":"image_model.load_weights(\"whole_model.h5\") ","d5f42c42":"h1 = image_model.fit(train_gen, steps_per_epoch=len(train_gen), epochs=20, validation_data=val_gen, callbacks=[ckpt]) ","3c9a4bdb":"plot(h1, \"loss\")\nplot(h1, \"accuracy\")","e6991dd4":"image_model.load_weights(\"whole_model_2.h5\") ","9fecdb6a":"test_gen = Generator_2(validation_df, classes=class_names, size=(image_size[1], image_size[0]),\n                     image_source=\"\", shuffle=False)\n\ny_hat = image_model.predict(test_gen, verbose=1)\ny_true = test_gen.true_labels()\n\ny_true = np.argmax(y_true, axis=1)\ny_pred = np.argmax(y_hat, axis=1)","5d4194cd":"rep = classification_report(y_true[:len(y_pred)], y_pred, target_names=['Normal', 'Benign', 'Malignant'])\nprint(rep)\n\ndef display_confusion_matrix(y_true, preds, labels):\n    c_mat = confusion_matrix(y_true, preds)\n    disp = ConfusionMatrixDisplay(c_mat, display_labels=labels)\n    disp.plot()\n\ndisplay_confusion_matrix(y_true[:len(y_pred)], y_pred, labels=['Normal', 'Benign', 'Malignant'])","584efbad":"# Training phase 3","5a14c74d":"# Training phase 1\n","8a48c385":"# Load DDSM images","4a485767":"## Train 20 more epochs","e8626caf":"## Lets see an example of patches before and after preprocess","f56781fc":"## Lets see an example of patches before and after preprocess","df051ece":"# Preprocess patches","bc2face0":"# Generate","6ecfd907":"# Second: Train using full images","8d29bcc8":"# Combine two datasets","19aae3a0":"# Generate","8c2a02b2":"# First: Train model using patches","eb3309b5":"# Load DDSM Patches","bc1d8efa":"# Load CBIS Patches","8bd21998":"# Training phase 2","00cd19ca":"# Combine two datasets"}}