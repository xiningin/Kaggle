{"cell_type":{"126f5f73":"code","bdd74d4a":"code","f7cab3ee":"code","6d5c5b34":"code","682d8aa2":"code","ad4d0a63":"code","28b64c31":"code","45cce7fb":"code","63ce3213":"code","a89b49ca":"code","92d5a7f6":"code","3b4539b5":"code","dc2bc426":"code","a9680380":"code","2e0b6d43":"code","119fe51a":"code","8f412c00":"code","da565045":"code","554d40f9":"code","df85af2b":"code","61f288c8":"code","26c44374":"code","c7b52438":"code","a3c55f2c":"markdown","b51067e2":"markdown","e8ab027d":"markdown","5a308c1d":"markdown","ef0ab3b3":"markdown","47c002d9":"markdown","bbe93f10":"markdown","296358d7":"markdown","7fd87ff4":"markdown","1be3d9da":"markdown","fe93fa72":"markdown","ffdf41c1":"markdown"},"source":{"126f5f73":"import warnings\nwarnings.filterwarnings('ignore') # to suppress some matplotlib deprecation warnings\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport ast\nimport cv2\n\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\n\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport os\nimport glob\nimport time\nimport tqdm","bdd74d4a":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\nimport torchvision\nfrom torchvision import transforms, utils","f7cab3ee":"en_dict = {}\npath = '..\/input\/train_simplified\/'\n\nfilenames = glob.glob(os.path.join(path, '*.csv'))\nfilenames = sorted(filenames)\n\ndef encode_files():\n    \"\"\" Encode all label by name of csv_files \"\"\"\n    counter = 0\n    for fn in filenames:\n        en_dict[fn[:-4].split('\/')[-1].replace(' ', '_')] = counter\n        counter += 1\n        \n# collect file names and encode label\nencode_files()\n\ndec_dict = {v: k for k, v in en_dict.items()}\ndef decode_labels(label):\n    return dec_dict[label]\n\ndef get_label(nfile):\n    \"\"\" Return encoded label for class by name of csv_files \"\"\"\n    return en_dict[nfile.replace(' ', '_')[:-4]]","6d5c5b34":"class DoodlesDataset(Dataset):\n    \"\"\"Doodles csv dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, mode='train', nrows=1000, skiprows=None, size=256, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            mode (string): Train or test mode.\n            nrows (int): Number of rows of file to read. Useful for reading pieces of large files.\n            skiprows (list-like or integer or callable): \n                    Line numbers to skip (0-indexed) or number of lines to skip (int) at the start of the file.\n            size (int): Size of output image.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.root_dir = root_dir\n        file = os.path.join(self.root_dir, csv_file)\n        self.size = size\n        self.mode = mode\n        self.doodle = pd.read_csv(file, usecols=['drawing'], nrows=nrows, skiprows=skiprows)\n        self.transform = transform\n        if self.mode == 'train':\n            self.label = get_label(csv_file)\n\n    @staticmethod\n    def _draw(raw_strokes, size=256, lw=6, time_color=True):\n        BASE_SIZE = 256\n        img = np.zeros((BASE_SIZE, BASE_SIZE), np.uint8)\n        for t, stroke in enumerate(raw_strokes):\n            for i in range(len(stroke[0]) - 1):\n                color = 255 - min(t, 10) * 13 if time_color else 255\n                _ = cv2.line(img, (stroke[0][i], stroke[1][i]),\n                             (stroke[0][i + 1], stroke[1][i + 1]), color, lw)\n        if size != BASE_SIZE:\n            return cv2.resize(img, (size, size))\n        else:\n            return img\n    \n    def __len__(self):\n        return len(self.doodle)\n\n    def __getitem__(self, idx):\n        raw_strokes = ast.literal_eval(self.doodle.drawing[idx])\n        sample = self._draw(raw_strokes, size=self.size, lw=2, time_color=True)\n        if self.transform:\n            sample = self.transform(sample)\n        if self.mode == 'train':\n            return (sample[None]\/255).astype('float32'), self.label\n        else:\n            return (sample[None]\/255).astype('float32')","682d8aa2":"SIZE = 224 # for matching to imagenet\n# collect all single csvset in one\nselect_nrows = 10000\ndoodles = ConcatDataset([DoodlesDataset(fn.split('\/')[-1], path, \n                                           nrows=select_nrows, size=SIZE) for fn in filenames])","ad4d0a63":"# select some rows for validation\n# valid_rows = 100\n# validationset = ConcatDataset([DoodlesDataset(fn.split('\/')[-1], path, nrows=valid_rows, size=SIZE,\n#                                            skiprows=range(1, select_nrows+1)) for fn in filenames])","28b64c31":"# total images in set\nprint('Train set:', len(doodles))\n# print('Validation set:', len(validationset))\n# Use the torch dataloader to iterate through the dataset\nloader = DataLoader(doodles, batch_size=128, shuffle=True, num_workers=0)\n# valid_loader = DataLoader(validationset, batch_size=128, shuffle=False, num_workers=0)","45cce7fb":"# functions to show an image\ndef imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n\n# get some images\ndataiter = iter(loader)\nimages, label = dataiter.next()\n\n# show images\nplt.figure(figsize=(16,24))\nimshow(torchvision.utils.make_grid(images[:24]))","63ce3213":"# validation function \ndef validation(lossf, scoref):\n    model.eval()\n    loss, score = 0, 0\n    vlen = len(valid_loader)\n    for x, y in valid_loader:\n        x, y = x.to(device), y.to(device)\n        output = model(x)\n        loss += lossf(output, y).item()\n        score += scoref(output, y)[0].item()\n    model.train()\n    return loss\/vlen, score\/vlen","a89b49ca":"def accuracy(output, target, topk=(3,)):\n    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 \/ batch_size))\n        return res","92d5a7f6":"def mapk(output, target, k=3):\n    \"\"\"\n    Computes the mean average precision at k.\n    \n    Parameters\n    ----------\n    output (torch.Tensor): A Tensor of predicted elements.\n                           Shape: (N,C)  where C = number of classes, N = batch size\n    target (torch.int): A Tensor of elements that are to be predicted. \n                        Shape: (N) where each value is  0\u2264targets[i]\u2264C\u22121\n    k (int, optional): The maximum number of predicted elements\n    \n    Returns\n    -------\n    score (torch.float):  The mean average precision at k over the output\n    \"\"\"\n    with torch.no_grad():\n        batch_size = target.size(0)\n\n        _, pred = output.topk(k, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        for i in range(k):\n            correct[i] = correct[i]*(k-i)\n            \n        score = correct[:k].view(-1).float().sum(0, keepdim=True)\n        score.mul_(1.0 \/ (k * batch_size))\n        return score","3b4539b5":"model = torchvision.models.resnet18(pretrained=True)","dc2bc426":"# Its first and last layers in model\ndef squeeze_weights(m):\n        m.weight.data = m.weight.data.sum(dim=1)[:,None]\n        m.in_channels = 1\n        \nmodel.conv1.apply(squeeze_weights);\n\nnum_classes = 340\nmodel.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)","a9680380":"%%time\n# test with random data\nmodel(torch.randn(12,1,224,224)).size()","2e0b6d43":"device = 'cuda'\nmodel.to(device);","119fe51a":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.002, amsgrad=True)","8f412c00":"# PyTorch scheduler:\n# https:\/\/pytorch.org\/docs\/stable\/optim.html#how-to-adjust-learning-rate\nscheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[5000,12000,18000], gamma=0.5)","da565045":"%%time\nepochs = 1\nlsize = len(loader)\nitr = 1\np_itr = 1000 # print every N iteration\nmodel.train()\ntloss, score = 0, 0\nfor epoch in range(epochs):\n    for x, y in loader:\n        x, y = x.to(device), y.to(device)\n        optimizer.zero_grad()\n        output = model(x)\n        loss = criterion(output, y)\n        loss.backward()\n        optimizer.step()\n        tloss += loss.item()\n        score += mapk(output, y)[0].item()\n        scheduler.step()\n        if itr%p_itr==0:\n            print('Iteration {} -> Train Loss: {:.4f}, MAP@3: {:.3f}'.format(itr, tloss\/p_itr, score\/p_itr))\n            tloss, score = 0, 0\n        itr +=1","554d40f9":"filename_pth='checkpoint_resnet18.pth'\ntorch.save(model.state_dict(), filename_pth)","df85af2b":"testset = DoodlesDataset('test_simplified.csv', '..\/input', mode='test', nrows=None, size=SIZE)\ntestloader = DataLoader(testset, batch_size=128, shuffle=False, num_workers=0)","61f288c8":"model.eval()\nlabels = np.empty((0,3))\nfor x in tqdm.tqdm(testloader):\n    x = x.to(device)\n    output = model(x)\n    _, pred = output.topk(3, 1, True, True)\n    labels = np.concatenate([labels, pred], axis = 0)","26c44374":"%%time\nsubmission = pd.read_csv('..\/input\/test_simplified.csv', index_col='key_id')\nsubmission.drop(['countrycode', 'drawing'], axis=1, inplace=True)\nsubmission['word'] = ''\nfor i, label in enumerate(labels):\n    submission.word.iloc[i] = \" \".join([decode_labels(l) for l in label])","c7b52438":"submission.to_csv('preds_resnet18.csv')","a3c55f2c":"Change number of inputs channels and number of classes. Details about model and code [here](https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/resnet.py)","b51067e2":"Training loop with printing information every 1000 iteration","e8ab027d":"### Load data to our DoodlesDataset","5a308c1d":"### This version cleaned and without validation (only 1 epoch, so don't use validation) ","ef0ab3b3":"### And finally predict for test set ","47c002d9":"Test model using random number. Just checking for service.","bbe93f10":"### Validation set - not used in this version","296358d7":"### Get some images from set","7fd87ff4":"### Save model state","1be3d9da":"### Define metric finction","fe93fa72":"### Create model. Loading pretrained version.","ffdf41c1":"## Dataset class and loader\nAnother example on official  [tutorial](https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html)"}}