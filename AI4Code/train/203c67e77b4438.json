{"cell_type":{"89e767aa":"code","f07aa295":"code","048321b8":"code","2f587e03":"code","eeab806a":"code","b82471e6":"code","8c2275c2":"code","14291a43":"code","7ea91683":"code","24b8a0d0":"code","2e5105ae":"code","dfaea64c":"code","374ee101":"markdown","97df5a1c":"markdown","28dae440":"markdown","de1f411d":"markdown","6e7568d8":"markdown"},"source":{"89e767aa":"import numpy as np\nimport math\nimport seaborn as sns\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn.functional as F \nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.datasets as dsets\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter('ignore')","f07aa295":"path = '..\/input\/cardiovascular-disease-dataset\/cardio_train.csv'\ndf = pd.read_csv(path, \n                 delimiter=';',\n                 index_col='id')\ndf = shuffle(df)\ndf.head()","048321b8":"normalize_columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']\nmean_std = {}\n\nfor column in normalize_columns:\n    mean_std[column] = (df[column].mean(), df[column].std())\n    df[column] = (df[column] - df[column].mean())\/df[column].std()","2f587e03":"fig, heat = plt.subplots(figsize = (14,7))\nheat = sns.heatmap(df.corr())","eeab806a":"df['cardio'].value_counts().plot(kind='bar')\nplt.xticks([0,1], ['No Disease', 'Disease'])\nplt.ylabel('Count')","b82471e6":"class CardioDataset(Dataset):\n    def __init__(self, features, labels):\n        self.features = features\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        x = self.features[idx]\n        y = self.labels[idx]\n        \n        x = torch.tensor(x, dtype=torch.float)\n        y = torch.tensor(np.eye(2)[y], dtype=torch.long)\n        return x, y","8c2275c2":"# Generate indices for splits\ntest_ind = round(len(df)*0.25)\ntrain_ind = test_ind + round(len(df)*0.01)\nunlabeled_ind = train_ind + round(len(df)*0.74)\n\n\n# Partition the data\ntest = df.iloc[:test_ind]\ntrain = df.iloc[test_ind:train_ind]\nunlabeled = df.iloc[train_ind:unlabeled_ind]","14291a43":"def test_train_unlabeled(test, train, unlabeled):\n    X_train = train.drop('cardio', axis=1)\n    X_train = torch.tensor(X_train.values)\n\n    y_train = train.cardio\n    y_train = torch.tensor(y_train.values)\n\n    X_unlabeled = unlabeled.drop('cardio', axis=1)\n    X_unlabeled = torch.tensor(X_unlabeled.values)\n    X_unlabeled = X_unlabeled.type(torch.float)\n\n    X_test = test.drop('cardio', axis=1)\n    X_test = torch.tensor(X_test.values)\n\n    y_test = test.cardio\n    y_test = torch.tensor(y_test.values)\n    \n    return X_train, y_train, X_unlabeled, X_test, y_test","7ea91683":"X_train, y_train, X_unlabeled, X_test, y_test = \\\n                        test_train_unlabeled(test, train, unlabeled)","24b8a0d0":"class LogisticRegression(torch.nn.Module):\n    def __init__(self, input_dim=11, output_dim=2):\n        super(LogisticRegression, self).__init__()\n        self.linear = torch.nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        outputs = self.linear(x)\n        return outputs","2e5105ae":"model = LogisticRegression()\n\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.001)","dfaea64c":"nr_episodes = 10\n\nfor ep in range(nr_episodes):\n\n    X_train, y_train = shuffle(X_train, y_train)\n    X_test, y_test = shuffle(X_test, y_test)\n\n    train_ds = CardioDataset(X_train.clone().detach(), \\\n                             y_train.clone().detach())\n    test_ds = CardioDataset(X_test.clone().detach(), \\\n                            y_test.clone().detach())\n\n    train_loader = DataLoader(train_ds, batch_size=8)\n    test_loader = DataLoader(train_ds, batch_size=8)\n\n    nr_epochs = 100\n\n    for e in range(nr_epochs):\n\n        epoch_accs = 0\n        epoch_loss = 0\n\n        val_epoch_accs = 0\n        val_epoch_loss = 0\n\n        for i, (x, y) in enumerate(train_loader):\n            optimizer.zero_grad()\n            x = x.type(torch.FloatTensor)\n            y = y.type(torch.FloatTensor)\n            y_max = y.max(axis=1)[1]\n\n            outputs = model(x)\n            acc = sum(outputs.max(axis=1)[1] == y_max).item()\n            acc \/= len(x)\n            epoch_accs += acc \/ len(train_loader)\n\n            loss = criterion(outputs, y_max)\n            epoch_loss += loss \/ len(train_loader)\n\n            loss.backward()\n            optimizer.step()\n\n        for i, (x, y) in enumerate(test_loader):\n            x = x.type(torch.FloatTensor)\n            y = y.type(torch.FloatTensor)\n            y_max = y.max(axis=1)[1]\n\n            outputs = model(x)\n            acc = sum(outputs.max(axis=1)[1] == y_max).item()\n            acc \/= len(x)\n            val_epoch_accs += acc \/ len(test_loader)\n\n            loss = criterion(outputs, y_max)\n            val_epoch_loss += loss \/ len(test_loader)\n\n        if((e+1) % 40 == 0):\n            print('i:{:3d}, Epoch:{:4d}, train_loss:{:1.3f}, ' \\\n                  'epoch_acc:{:1.3f}, val_loss:{:1.3f}, val_accs:{:1.3f}'\n                      .format(ep+1, e+1, epoch_loss, epoch_accs,\n                              val_epoch_loss, val_epoch_accs))\n            \n         \n    # add supervised features, labels\n    logits = model(X_unlabeled)\n    pred_probs = F.softmax(logits, dim=1)\n    preds = torch.argmax(logits, dim=1)\n\n    high_prob_indices = torch.where(pred_probs > (0.9 + ep * 0.01))\n    \n    X_train = torch.cat((X_train, X_unlabeled[high_prob_indices[0]]), 0)\n    y_train = torch.cat(([y_train, high_prob_indices[1]]), 0)\n    \n    new_indexes = [a for a in np.arange(len(X_unlabeled)) \n                       if a not in high_prob_indices[0] ]\n    X_unlabeled = X_unlabeled[new_indexes]\n    \n    print('------------- Adding {:}# of features, ' \\\n          '{:}# of unlabeled remaining -------------'\n            .format(len(high_prob_indices[0]), len(X_unlabeled)))","374ee101":"<h1 id=\"model\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <center>Model\n        <a class=\"anchor-link\" href=\"#model\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","97df5a1c":"<h1 id=\"dataset_split\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <center>Dataset Split\n        <a class=\"anchor-link\" href=\"#dataset_split\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","28dae440":"<div>\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/107706\/256873\/21d3eec8c2d5c04b7014f61ae3b516be\/dataset-cover.jpg\" \/>\n<\/div>","de1f411d":"<h1 id=\"dataset\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <center>Dataset Preparation\n        <a class=\"anchor-link\" href=\"#dataset\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>","6e7568d8":"<h1 id=\"training\" style=\"color:black; background:white; border:0.5px dotted;\"> \n    <center>Training\n        <a class=\"anchor-link\" href=\"#training\" target=\"_self\">\u00b6<\/a>\n    <\/center>\n<\/h1>"}}