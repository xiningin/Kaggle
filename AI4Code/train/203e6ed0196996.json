{"cell_type":{"8f061b40":"code","fd0053b5":"code","04a72598":"code","a98d53b6":"code","aed21532":"code","d8fc6623":"code","f6294079":"code","57c21630":"code","3ddd75a0":"code","a40de324":"code","b01cc8c6":"code","ce1c759b":"code","ff337041":"code","49c6a19a":"code","b72a2a38":"code","17c6b844":"code","c313382e":"code","f631a0af":"code","860d4fa0":"code","7e236e44":"code","ab579848":"code","5a1fa2b1":"code","a4b23153":"code","b2b387f6":"code","bcf4cb25":"code","418dc67b":"code","090188f2":"code","1dacfdc7":"code","b5de2064":"code","2916f27c":"code","28cb5051":"code","ba4ab628":"code","66272fb3":"code","90b4c9d6":"code","a5ea946f":"code","d7a17d73":"code","41b14451":"code","6fd5208b":"code","f985d335":"code","f1fd25fb":"code","4daa1550":"code","87ca5738":"code","5d1dca9e":"code","10bbdbba":"code","73e95735":"markdown","7c3eef93":"markdown","12b9b671":"markdown","bb3bf133":"markdown","56be612b":"markdown","4a378900":"markdown"},"source":{"8f061b40":"def plot_model_output(history,epochs):\n    plt.figure()\n    plt.plot(range(epochs,),history.history['loss'],label = 'training_loss')\n    plt.plot(range(epochs,),history.history['val_loss'],label = 'validation_loss')\n    plt.legend()\n    plt.figure()\n    plt.plot(range(epochs,),history.history['acc'],label = 'training_accuracy')\n    plt.plot(range(epochs,),history.history['val_acc'],label = 'validation_accuracy')\n    plt.legend()\n    plt.show()","fd0053b5":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dense, Flatten, Embedding, LSTM\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.datasets import imdb","04a72598":"data = pd.read_csv('\/kaggle\/input\/imdb-review-dataset\/imdb_master.csv',encoding='ISO-8859-1')\nprint(data.columns)","a98d53b6":"sns.countplot(x='label',data=data)","aed21532":"data = data[data.label!='unsup']\nsns.countplot(x='label',data=data)","d8fc6623":"data['out'] = data['label'] ","f6294079":"data['out'][data.out=='neg']=0\ndata['out'][data.out=='pos']=1\n# Another way data['out'] = data['out'].map({1:'pos',0:'neg'})\nnp.unique(data.out)","57c21630":"sns.countplot(y='out',data=data)","3ddd75a0":"req_data = data[['review','out']]\nreq_data.head()","a40de324":"texts = np.array(req_data.review)\nlabels = np.array(req_data.out)","b01cc8c6":"print(texts.shape, labels.shape)","ce1c759b":"# num_words: Top No. of words to be tokenized. Rest will be marked as unknown or ignored.\ntokenizer = Tokenizer(num_words=20000) \n# tokenizing based on \"texts\". This step generates the word_index and map each word to an integer other than 0.\ntokenizer.fit_on_texts(texts)\n\n# generating sequence based on tokenizer's word_index. Each sentence will now be represented by combination of numericals\n# Example: \"Good movie\" may be represented by [22, 37]\nseq = tokenizer.texts_to_sequences(texts)","ff337041":"# padding each numerical representation of sentence to have fixed length.\npadded_seq = np.array(pad_sequences(seq,maxlen=100))\n\n#word_index of each token\nword_index = tokenizer.word_index\n\n# for shuffling\nindices = np.arange(padded_seq.shape[0])\nnp.random.shuffle(indices)\n\n# texts = texts[indices]\n# labels = labels[indices]","49c6a19a":"# this is how word_index looks like. It's a dict where each word has its own unique key \n# with which the word is represented in sequence.\n\nprint(len(word_index))\nprint(padded_seq.shape)\nprint()","b72a2a38":"# GloVe embedding matrix. This is like a pre-trained model. In an embedding matrix, each word is represented by a dense vector.\n","17c6b844":"vec_representations = {}\nf = open('\/kaggle\/input\/glove6b\/glove.6B.100d.txt','r')\n\nsample = True\n\nfor line in f:\n    if sample:\n        print(\"Sample weight of word: \")\n        print(line)\n        sample=False\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:],dtype = 'float32')\n    vec_representations[word] = coefs\n    \nf.close()\n\nprint(f'Found {len(vec_representations)} words')","c313382e":"max_words = 20000\nembedding_dim = 64","f631a0af":"embedding_matrix = np.zeros((max_words,embedding_dim))\nfor word,i in word_index.items():\n    vec_representation = vec_representations.get(word)\n    if vec_representation is not None:\n        embedding_matrix[i]=vec_representation","860d4fa0":"model = Sequential()\nembedding = Embedding(max_words,embedding_dim,input_length = 100,name='embedding')\nmodel.add(embedding)\n#model.add(LSTM(32, return_sequences = True))\nmodel.add(Flatten())\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()","7e236e44":"# model.layers[0].set_weights=[(embedding_matrix)]\n# model.layers[0].trainable=True\nmodel.layers[0].get_weights()","ab579848":"np.asarray(labels).astype(np.uint8)","5a1fa2b1":"model.compile(optimizer='adagrad',loss='binary_crossentropy',metrics=['acc'])\n\n# Change the epochs to 10 here.\nhistory = model.fit(padded_seq,np.asarray(labels).astype(np.uint8),epochs=10,validation_split=0.3)","a4b23153":"plot_model_output(history, 10)","b2b387f6":"import numpy as np","bcf4cb25":"timesteps = 10000 # No. of timesteps in input_sequence\ninput_features = 32 # Dimensionality of input_feature space\noutput_features = 64 # Dimensionality of output_features space\n\ninputs = np.random.random((timesteps,input_features)) #random data for example 10000 X 32\n\nstate_t = np.zeros((output_features,)) # (64,)\n\nW = np.random.random((output_features,input_features)) # Weight matrix for current_state (64 X 32)\nU = np.random.random((output_features,output_features)) #Weight matrix for previous state (64 X 64)\n\nb = np.random.random((output_features,)) # bias to be added to output of each state (64,)\n\nsuccessive_outputs = [] # output of each timestep\n\nfor input_t in inputs:\n    # input_t is a vector of shape (input_features,)\n    output_t = np.tanh(np.dot(W,input_t)+np.dot(U,state_t) + b)\n    successive_outputs.append(output_t)\n    state_t = output_t\n\nfinal_outputs = np.concatenate(successive_outputs,axis=0)\n","418dc67b":"final_outputs","090188f2":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,SimpleRNN,Dense, Flatten\nfrom keras.datasets import imdb\nfrom keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport pandas as pd","1dacfdc7":"model = Sequential()\nmodel.add(Embedding(10000,32)) # 10K is no of unique words and 32 is shape of each word's representation\nmodel.add(SimpleRNN(units=32,return_sequences = True)) #units is dimensionality of output space. https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/SimpleRNN\nmodel.add(SimpleRNN(units=32,return_sequences = True)) \nmodel.add(SimpleRNN(units=32,return_sequences = True)) \nseq = model.add(SimpleRNN(units=32,return_sequences = True)) \nmodel.summary()","b5de2064":"max_features = 10000 # max num of words. others will be marked as unknown\nmaxlen = 500 # maximum length of each review\nbatch_size = 32\n(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)","2916f27c":"x_train = pad_sequences(x_train, maxlen=maxlen)\nx_test = pad_sequences(x_test, maxlen=maxlen)\nindices = list(range(len(x_train)))\nnp.random.shuffle(indices)","28cb5051":"x_train = x_train[indices]\ny_train = y_train[indices]","ba4ab628":"model = Sequential()\nmodel.add(Embedding(max_features,32)) # here 32 is no of bits with which a sentence with maxlen(500 set above) will be represented.\nmodel.add(SimpleRNN(32)) # 32 is output space dimensions\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()","66272fb3":"model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy',metrics=['acc'])\n","90b4c9d6":"history = model.fit(x_train,\n          y_train,\n         epochs=4,\n         batch_size = batch_size,\n         validation_split=0.2)","a5ea946f":"plt.figure()\nplt.plot(range(4,), history.history['acc'],'g',label = 'training acc')\nplt.plot(range(4,), history.history['val_acc'],'*',label = 'val_acc')\nplt.title(\"Training and validation acc\")\nplt.legend()\nplt.show()","d7a17d73":"plt.figure()\nplt.plot(range(4,),history.history['loss'],label = 'training_loss')\nplt.plot(range(4,),history.history['val_loss'],label = 'validation_loss')\nplt.legend()\nplt.show()\n","41b14451":"import numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,SimpleRNN,Dense, Flatten, LSTM\nfrom keras.datasets import imdb\nfrom keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport pandas as pd","6fd5208b":"max_features = 20000 # max num of words. others will be marked as unknown\nmaxlen = 1000 # maximum length of each review\nbatch_size = 64\n(x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=max_features)","f985d335":"x_train = pad_sequences(x_train, maxlen=maxlen)\nx_test = pad_sequences(x_test, maxlen=maxlen)","f1fd25fb":"y_train","4daa1550":"model = Sequential()\nmodel.add(Embedding(max_features,64)) # here 64 is no of bits with which a sentence with maxlen(500 set above) will be represented.\nmodel.add(LSTM(64,dropout=0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.summary()","87ca5738":"model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['acc'])\nhistory = model.fit(x_train,y_train,\n         validation_split = 0.2,\n         epochs = 1,\n         batch_size=batch_size)","5d1dca9e":"plot_model_output(history,1)","10bbdbba":"model.layers[-2].get_weights()","73e95735":"### Glove 100d Parsing","7c3eef93":"# Simple RNN","12b9b671":"# Sentiment Analysis with LSTMs","bb3bf133":"#### Tokenize","56be612b":"Using IMDB data with above RNN","4a378900":"# RNN in Numpy"}}