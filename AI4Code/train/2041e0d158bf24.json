{"cell_type":{"e82e0e97":"code","a5cea979":"code","082ed7ed":"code","da0446c0":"code","04597fd5":"code","2c5569dc":"code","60f43fbf":"code","e94a9ef8":"code","2570c373":"code","c3d00310":"code","5ade6949":"code","c4d07b3e":"code","6a7298c1":"code","b2bfd6ac":"code","02f94212":"code","a58e21c2":"code","0020bcd4":"code","47cfd03b":"code","f5bff22b":"code","2b1389f2":"code","b371e2c7":"code","2e31949f":"code","cc29c64a":"code","48703d58":"code","f1cd0363":"code","19eb8283":"code","e11f97b2":"code","a240bd6a":"code","8b3c4744":"code","885b3846":"markdown","f3800001":"markdown","4ff8d074":"markdown","dc96d6cd":"markdown","10748846":"markdown","7558923b":"markdown","e3096da9":"markdown","b764858f":"markdown","572e425d":"markdown","50d5029d":"markdown","6d817e59":"markdown","57caac0a":"markdown","86809174":"markdown","078aa9ad":"markdown","8950f7ff":"markdown","f34f8419":"markdown","c757a9f7":"markdown","6d059302":"markdown","1ca46792":"markdown"},"source":{"e82e0e97":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","a5cea979":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\nSEED = 42\n\nimport warnings\nwarnings.filterwarnings('ignore')","082ed7ed":"\"\"\" Helper Functions\"\"\"\ndef univariate_analysis_continuous(df, feature):\n    dataType = df[feature].dtype\n    if dataType in ['int64','float64']:\n        print(df[feature].describe())\n        \n        kurt = df[feature].kurt()\n        skew = df[feature].skew()\n        \n        if skew < 0:\n            print(\"\\n{:<10} {:.6f}\\n--------------------\\nThe distribution is skewed left\\n\".format(\"Skew\", skew))\n        elif skew > 0:\n            print(\"\\n{:<10} {:.6f}\\n--------------------\\nThe distribution is skewed right\\n\".format(\"Skew\", skew))\n        else:\n            print(\"No skew\\n\")\n        \n        if kurt < 0:\n            print(\"{:<10} {:.6f}\\n--------------------\\nDistribution is flatter and possess thinner tails\\nIt is flatter and less peaked compared to a normal distribution with few values in it's shorter tail \".format(\"Kurtosis\", kurt))\n        elif kurt > 0:\n            print(\"{:<10} {:.6f}\\n--------------------\\nDistribution is peaked and possess thicker tails\\nIt has higher peak and taller tails compared to a normal distribution\".format(\"Kurtosis\", kurt))\n        else:\n            print(\"{:<10} {:.6f}\".format(\"Kurtosis\", kurt))\n        fig, axs = plt.subplots(ncols=3, figsize=(15,5))\n        sns.boxplot(y = iris[feature], ax=axs[0])\n        sns.distplot(df[feature], ax=axs[1])\n        sns.kdeplot(df[feature], shade=True, ax=axs[2])\n    \n    elif dataType in ['object']:\n        tab = pd.crosstab(index=df[feature], columns=\"count\")\n        print(\"{}\\n\\n{}\".format(tab, tab\/tab.sum()))\n        data =  df[feature].value_counts()\n        labels = df[feature].value_counts().index\n        sns.barplot(x=labels, y=data)    \n\ndef missing_percentage(data):\n    \"\"\"\n    Prints the count of missing values and overall percentage missing for each feature\n    \"\"\"\n    rows, cols = data.shape\n    num_missing = data.isnull().sum()\n    missing_percent = (((data.isnull().sum())\/data.shape[0]) * 100)\n    print(pd.concat([num_missing, missing_percent], axis=1).rename(columns={0:'Num_Missing',1:'Missing_Percent'}).sort_values(by='Missing_Percent', ascending=False)) \n    \ndef color_pairplot(setosa, versicolor, virginica):\n    \"\"\"\n    Given dataframes respective to each species, we plot pairplot that distinguishes between the speicies\n    \"\"\"\n    fig, axs = plt.subplots(ncols=3, nrows=4, figsize=(20,15))\n    features = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\n    fig.subplots_adjust(hspace=.5)\n\n    i = j = 0\n    for feature2 in features:\n        for feature1 in features:\n            if feature1 != feature2:\n                axs[i,j].set_title(str(round(iris[feature1].corr(iris[feature2]), 4))) \n                sns.scatterplot(x=setosa[feature1], y=setosa[feature2], cmap='Red', label='setosa', ax=axs[i,j])\n                sns.scatterplot(x=versicolor[feature1], y=versicolor[feature2], cmap='Purple', label='veriscolor', ax=axs[i,j])\n                sns.scatterplot(x=virginica[feature1], y=virginica[feature2], cmap='Green', label='virginica', ax=axs[i,j])\n                j += 1\n                if j == 3:\n                    i += 1\n                    j = 0\n    ","da0446c0":"iris = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")\niris.drop('Id', axis=1, inplace=True)","04597fd5":"iris.dtypes","2c5569dc":"iris.head()","60f43fbf":"iris.describe()","e94a9ef8":"missing_percentage(iris)","2570c373":"univariate_analysis_continuous(iris, 'SepalLengthCm')","c3d00310":"univariate_analysis_continuous(iris, 'SepalWidthCm')\n","5ade6949":"univariate_analysis_continuous(iris, 'PetalLengthCm')","c4d07b3e":"univariate_analysis_continuous(iris, 'PetalWidthCm')","6a7298c1":"univariate_analysis_continuous(iris, 'Species')","b2bfd6ac":"sns.pairplot(iris)","02f94212":"sns.heatmap(iris.corr(), annot=True, square=True, cmap='coolwarm', annot_kws={'size': 12})","a58e21c2":"setosa = iris[iris['Species'] == 'Iris-setosa']\nversicolor = iris[iris['Species'] == 'Iris-versicolor']\nvirginica = iris[iris['Species'] == 'Iris-virginica']\ncolor_pairplot(setosa, versicolor, virginica)","0020bcd4":"import math\nfrom pprint import pprint # pretty printing\n\n# # Preprocessing\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n\n# Model selection\nfrom sklearn.model_selection import StratifiedKFold\n\n# Models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\n\n# Scoring\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.model_selection import StratifiedKFold","47cfd03b":"def evaluate(model, features, target):\n    \"\"\"\n    Given a model, the test features, and test target, prints the accuracy, confusion matrix, and log_loss metrics while returning accuracy\n    \"\"\"\n    y_pred = model.predict(features)\n    accuracy = model.score(X_test, y_test)\n    confusion_matrix = metrics.confusion_matrix(y_test, y_pred)\n    classification_report = metrics.classification_report(y_test, y_pred)\n    \n    # Note, log_loss takes in probabilities so categorical data must be converted\n    log_loss = metrics.log_loss(y_test, pd.get_dummies(y_pred), eps=1e-15)\n\n    print(\"Accuracy: {}\\n\\nConfusion Matrix:\\n{}\\n\\nClassification Report:\\n{}\\nLog-Loss: {}\\n\".format(accuracy, confusion_matrix, classification_report, log_loss))\n    return accuracy\n\ndef compare(base_model, new_model, features, target):\n    \"\"\"\n    Given two models, test features, and test target, compares the model metrics of the base and new model\n    \"\"\"\n    print(\"--------------\\n| Base Model |\\n--------------\")\n    base_accuracy = evaluate(base_model, features, target)\n    print(\"------------------\\n| 'Better' Model |\\n------------------\")\n    new_accuracy = evaluate(new_model, features, target)\n    \n    print(\"---------------\\n| Improvement |\\n---------------\\n{:4.3}%\".format(100 * (new_accuracy - base_accuracy) \/ base_accuracy))","f5bff22b":"# Split Data\ny = iris['Species']\nX = iris.drop('Species', axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2019)\n\n# Pipeline Setup\nnumeric_transformer = Pipeline(steps=[\n('imputer', SimpleImputer(strategy='median')),\n('scaler', StandardScaler())])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\nnumeric_features = iris.select_dtypes(include=['int64','float64']).columns\ncategorical_features = iris.select_dtypes(include=['object']).drop('Species', axis=1).columns\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])","2b1389f2":"# List of potential classifers we will score for accuracy\nclassifiers = [\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    LogisticRegression(),\n    KNeighborsClassifier(int(math.sqrt(iris.shape[0]))), # K = sqrt(N) where N is the number of samples\n    svm.SVC(),\n    GaussianNB()\n    ]\n\n# Score the classifiers and store them in a tuple containing the model and score they recieved\nclassifier_ranking = []\nfor classifier in classifiers:\n        pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                               ('classifier', classifier)])\n        pipe.fit(X_train, y_train)   \n        classifier_ranking.append((classifier, pipe.score(X_test, y_test)))\n\n# Sort the classifers in descending order\nclassifier_ranking.sort(key = lambda x: x[1], reverse=True)\nfor classifier in classifier_ranking:\n        print(\"{}\\nmodel score: {:.3}\\n\".format(classifier[0], classifier[1]))","b371e2c7":"\nsplit = 4\n# Folds are made by preserving the percentage of samples for each class\nskf = StratifiedKFold(n_splits=split)\n\n\n# For each fold, I train KNN for varying values of K and record their scores\n# Morever, I plot the accuracy of the model of each fold for each varying K value\n\nfold = 1\nfold_scores = []\nfig, axs = plt.subplots(ncols=split, figsize=(25,split))\nfor train_index, test_index in skf.split(X, y):\n    X_train, X_test = X.loc[train_index], X.loc[test_index]\n    y_train, y_test = y.loc[train_index], y.loc[test_index]\n    \n    # Score the model with K where K range from 2 to 30 \n    scores = []\n    x = [x for x in range(2, 31)]\n    for i in range(2, 31):\n        pipe = Pipeline(steps=[('preprocessor', preprocessor),\n                                ('classifier', KNeighborsClassifier(i))])\n        pipe.fit(X_train, y_train)   \n        \n        y_pred = pipe.predict(X_test)\n        scores.append(pipe.score(X_test, y_test))\n        \n    # Record all the scores for the given fold\n    fold_scores.append(scores)\n        \n    # Plot the accuracy of KNN as it varies with K\n    sns.lineplot(x, scores, label=\"fold \" + str(fold), ax=axs[fold - 1])\n    fold += 1","2e31949f":"# A 2D Matrix containing acccuracy for each K in each fold where rows are K and columns are fold\nscore_matrix = np.array(fold_scores).T\n\n# The average accuracy of all folds for each K \nk_averages = np.average(score_matrix, axis=1).T\n\n# Find the index of K with the best accuracy among the averages of all folds\n# Note since our range starts at 0 and K starts at 2, we add 2 to our index to get the correct K\nK_index = np.argmax(k_averages)\nK = K_index + 2\n\nprint(\"KNN's best average accuracy is {} when K = {}\".format(k_averages[K_index], K))","cc29c64a":"# Default values for hyperparameters\nrf = RandomForestClassifier()\npprint(rf.get_params())","48703d58":"# Fill in range for hyperparameters\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt', 'log2']\n\n# Maximum depth of the tree\nmax_depth = [int(x) for x in np.linspace(10, 110, 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 7, 10]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\npprint(random_grid)","f1cd0363":"from sklearn.model_selection import RandomizedSearchCV\n\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\nrf = RandomForestClassifier()\n\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 4, verbose=2, n_jobs = -1, random_state = SEED)\n\n# Fit the random search model\nrf_random.fit(X_train, y_train)","19eb8283":"rf_random.best_params_","e11f97b2":"# Fit our best random model\nbest_random = rf_random.best_estimator_\nbest_random.fit(X_train, y_train)\n\n# Create a baseline model\nbase_model = RandomForestClassifier()\nbase_model.fit(X_train, y_train)\n\ncompare(base_model, best_random, X_test, y_test)","a240bd6a":"from sklearn.model_selection import GridSearchCV\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'bootstrap': [False],\n    'max_depth': [int(x) for x in np.linspace(start = 2, stop = 20, num = 2)],\n    'max_features': ['sqrt'],\n    'min_samples_leaf': [int(x) for x in np.linspace(start = 1, stop = 5, num = 1)],\n    'min_samples_split': [int(x) for x in np.linspace(start = 2, stop = 20, num = 2)],\n    'n_estimators': [int(x) for x in np.linspace(start = 800, stop = 1200, num = 20)]\n}\n\n# base model for grid search\nrf = RandomForestClassifier()\n\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 4, n_jobs = -1, verbose = 2)","8b3c4744":"grid_search.fit(X_train, y_train)\ncompare(base_model, grid_search.best_estimator_, X_test, y_test)","885b3846":"> Grid Search now","f3800001":"# Bivariate Analysis","4ff8d074":"> # PetalWidthCm","dc96d6cd":"# Random Search Training","10748846":"> # SepalLengthCm","7558923b":"> We see there is no improvement from the baseline to the either of our new models. This can be attributed to the fact that we don't have much data to deal with.","e3096da9":"> # PetalLengthCm","b764858f":"# Univariate Analysis","572e425d":"> Since there are so many hyperparameters to choose, creating a wide range of values to test is a good idea. However, it is very computationally expensive. Therefore, we perform a random grid search with the parameter grid on a baseline model first to narrow down our range of hyperparamters","50d5029d":"## Model Selection","6d817e59":"# RandomForestClassifer ","57caac0a":"> Indeed there are rougly two groups as stated above. Versicolor and Virginica are very similar and clump together. In fact, after some research, I found it is difficult to differentiate between Iris versicolor and Iris virginica as both have similar growth habits, floral colors, and bloom times.  Moreover, they are often sold interchangeably in the trade as blue flag iris. However, we can see a clear difference between Versicolor and Virginica as well with virginica seemingly ot be bigger than veriscolor.","86809174":"KNeighbors scores with 100% accuracy, this is very promising. However, let's investigate further to see if this generalizes to other test sets. ","078aa9ad":"> RandomForestClassifer also scored well I will evaluate it's performance on this dataset also\n\n> First, it's important to examine the default hyperparamters for our model before creating our paramater grid to test a range of hyperparameters","8950f7ff":"> # SepalWidthCm","f34f8419":"> To get a general idea of some classification models, we score each model using a pipeline to get a baseline for each","c757a9f7":"## Univariate Analysis Summary\n\n> If we look at the distributions for PetalWidthCm and PetalLengthCm, there seems be two groups. This is more visually apparent when we examine the distributions for **PetalLengthCm** and **PetalWidthCm**. The **PetalLengthCm** is bimodal with peaks at approximately ~1.7 and ~4.8. The **PetalWidthCm** is bimodal with peaks at approximately ~0.2 and ~1.4.\n","6d059302":"> After taking the average of the folds, it turns out our KNN doesn't actually have 100% accuracy.","1ca46792":"# Machine Learning"}}