{"cell_type":{"42032020":"code","fe5c483b":"code","99e46523":"code","fd47a7a3":"code","afa2884b":"code","567de4c7":"code","322c014c":"code","818ce3be":"code","f1747fb6":"code","899636ee":"code","94755023":"code","320e8248":"code","37425208":"code","fe12712d":"code","649c848a":"code","01556744":"code","6df79426":"code","b20b33c1":"code","d338ead1":"code","04b1543d":"code","c4e27fd9":"code","99943143":"code","ab3d8cd8":"code","47e105ee":"code","a6dfbe41":"code","930581a8":"code","9c362a5a":"code","4c0f930c":"code","2e17767e":"code","5fba516a":"code","89727100":"code","b00be91a":"code","85eec194":"code","fc33e0d2":"code","d8808933":"code","8f51eb60":"code","65bb3f10":"code","476857d6":"code","ec032afa":"code","c3a4c0d4":"code","5efa16ad":"code","b150b9ce":"code","50e89469":"code","7d7c65a7":"code","192b3835":"code","361dab0e":"code","5e539800":"code","d1d323ad":"code","065806f0":"code","058ba3d7":"code","ada36606":"code","0e899e23":"code","9a3ea1ea":"code","691d8809":"code","187b674f":"code","8f4f0c7c":"code","5c368fb2":"code","37cb13a3":"markdown","262fc92e":"markdown","5127194c":"markdown","beab9ddd":"markdown","e5b92dae":"markdown","23c5c622":"markdown","537a9999":"markdown","95363cd0":"markdown","a4b89a7f":"markdown","33d0efc5":"markdown","6d526635":"markdown","31cf086e":"markdown","b5403dd2":"markdown","21ed2449":"markdown","bbd9a870":"markdown","47407ff6":"markdown","b71a76d8":"markdown","d841d4a1":"markdown"},"source":{"42032020":"import pandas as pd\npd.set_option('display.max_columns', 500)\nimport numpy as np\n\nfrom scipy.signal import savgol_filter\nimport statsmodels.api as sm\nimport pymc3 as pm\nimport statsmodels.api as sm\nfrom statsmodels.tools import add_constant\nfrom itertools import combinations\n\nimport matplotlib.pyplot as plt\n\n# import seaborn\nimport seaborn as sns\n# settings for seaborn plotting style\nsns.set(color_codes=True)\n# settings for seaborn plot sizes\nsns.set(rc={'figure.figsize':(12,6)})","fe5c483b":"# read the data\ndata = pd.read_csv('..\/input\/nfl-team-stats-20022019-espn\/nfl_team_stats_2002-2019.csv')\n\ndata['score_diff'] = data['score_home'] - data['score_away']\n\ndata['winner'] = (data['score_home'] - data['score_away'])>0\ndata['winner'] = data['winner'].astype(int)\n\n# split string info for third and fourth down attmepts to float succeses and rate\nthird_downs_away = data['third_downs_away'].str.split('-', expand=True)\ndata['third_downs_away'] = third_downs_away[0].to_numpy(dtype=int)\ndata['third_downs_rate_away'] = np.nan_to_num(third_downs_away[0].to_numpy(dtype=float)\/third_downs_away[1].to_numpy(dtype=float), nan=0.0)\ndata['third_downs_attempts_away'] = third_downs_away[1].to_numpy(dtype=int)\n\nthird_downs_home = data['third_downs_home'].str.split('-', expand=True)\ndata['third_downs_home'] = third_downs_home[0].to_numpy(dtype=float)\ndata['third_downs_rate_home'] = np.nan_to_num(third_downs_home[0].to_numpy(dtype=float)\/third_downs_home[1].to_numpy(dtype=float), nan=0.0)\ndata['third_downs_attempts_home'] = third_downs_home[1].to_numpy(dtype=float)\n\n# split string info for third and fourth down attmepts to float succeses and rate\nfourth_downs_away = data['fourth_downs_away'].str.split('-', expand=True)\ndata['fourth_downs_away'] = fourth_downs_away[0].to_numpy(dtype=int)\ndata['fourth_downs_rate_away'] = np.nan_to_num(fourth_downs_away[0].to_numpy(dtype=float)\/fourth_downs_away[1].to_numpy(dtype=float), nan=0.0)\ndata['fourth_downs_attempts_away'] = fourth_downs_away[1].to_numpy(dtype=int)\n\nfourth_downs_home = data['fourth_downs_home'].str.split('-', expand=True)\ndata['fourth_downs_home'] = third_downs_home[0].to_numpy(dtype=float)\ndata['fourth_downs_rate_home'] = np.nan_to_num(third_downs_home[0].to_numpy(dtype=float)\/third_downs_home[1].to_numpy(dtype=float), nan=0.0)\ndata['fourth_downs_attempts_home'] = third_downs_home[1].to_numpy(dtype=float)\n\n# split string info for completions to float succeses and rate\ncomp_att_away = data['comp_att_away'].str.split('-', expand=True)\ndata['complete_passes_away'] = comp_att_away[0].to_numpy(dtype=int)\ndata['pass_completion_rate_away'] = np.nan_to_num(comp_att_away[0].to_numpy(dtype=float)\/comp_att_away[1].to_numpy(dtype=float), nan=0.0)\ndata['pass_attempt_away'] = comp_att_away[1].to_numpy(dtype=int)\ndata.drop('comp_att_away',1, inplace=True)\n\ncomp_att_home = data['comp_att_home'].str.split('-', expand=True)\ndata['complete_passes_home'] = comp_att_home[0].to_numpy(dtype=int)\ndata['pass_completion_rate_home'] = np.nan_to_num(comp_att_home[0].to_numpy(dtype=float)\/comp_att_home[1].to_numpy(dtype=float), nan=0.0)\ndata['pass_attempt_home'] = comp_att_home[1].to_numpy(dtype=int)\ndata.drop('comp_att_home',1, inplace=True)\n\n# split string info for sacks to float succeses and rate\nsacks_away = data['sacks_away'].str.split('-', expand=True)\ndata['sacks_away'] = sacks_away[0].to_numpy(dtype=int)\ndata['pressures_away'] = sacks_away[1].to_numpy(dtype=int)\n\nsacks_home = data['sacks_home'].str.split('-', expand=True)\ndata['sacks_home'] = sacks_home[0].to_numpy(dtype=int)\ndata['pressures_home'] = sacks_home[1].to_numpy(dtype=int)\n\n# split string info for penalties to float succeses and rate\npenalties_away = data['penalties_away'].str.split('-', expand=True)\ndata['penalties_away'] = np.nan_to_num(penalties_away[0].to_numpy(dtype=int), nan=0.0)\ndata['penalties_yards_away'] = np.nan_to_num(penalties_away[1].to_numpy(dtype=int), nan=0.0)\n\npenalties_home = data['penalties_home'].str.split('-', expand=True)\ndata['penalties_home'] = np.nan_to_num(penalties_home[0].to_numpy(dtype=int), nan=0.0)\ndata['penalties_yards_home'] = np.nan_to_num(penalties_home[1].to_numpy(dtype=int), nan=0.0)\n\n# split string info for completions to float succeses and rate\nredzone_away = data['redzone_away'].str.split('-', expand=True)\ndata['redzone_sucess_away'] = redzone_away[0].to_numpy(dtype=int)\ndata['redzone_rate_away'] = np.nan_to_num(redzone_away[0].to_numpy(dtype=float)\/redzone_away[1].to_numpy(dtype=float), nan=0.0)\ndata['redzone_attempts_away'] = redzone_away[1].to_numpy(dtype=int)\ndata.drop('redzone_away',1, inplace=True)\n\nredzone_home = data['redzone_home'].str.split('-', expand=True)\ndata['redzone_sucess_home'] = redzone_home[0].to_numpy(dtype=int)\ndata['redzone_rate_home'] = np.nan_to_num(redzone_home[0].to_numpy(dtype=float)\/redzone_home[1].to_numpy(dtype=float), nan=0.0)\ndata['redzone_attempts_home'] = redzone_home[1].to_numpy(dtype=int)\ndata.drop('redzone_home',1, inplace=True)\n\n# split string info for completions to float succeses and rate\npossession_away = data['possession_away'].str.split(':', expand=True)\ndata['possession_away'] = possession_away[0].to_numpy(dtype=float) * 60 + possession_away[1].to_numpy(dtype=float)\n\npossession_home = data['possession_home'].str.split(':', expand=True)\ndata['possession_home'] = possession_home[0].to_numpy(dtype=float) * 60 + possession_home[1].to_numpy(dtype=float)\n\nday_numbers = pd.to_numeric((pd.to_datetime(data['date']) - pd.to_datetime(data['date'][0])).dt.days,\n                            downcast='integer')\nyears = np.ones(len(day_numbers))*2002\nfor idx in range(1,len(day_numbers)):\n    years[idx] = years[idx-1] + ((day_numbers[idx]-day_numbers[idx-1]) > 30)*1\ndata['season'] = years\n    \n# get basic info\nnRows = data.shape[0]\nnCols = data.shape[1]\ncolumn_names = list(data.columns)\n\n# display the head\ndata.head()","99e46523":"# check that there are no NaNs\ndata.columns[data.isna().any()].tolist()","fd47a7a3":"data.dtypes","afa2884b":"# simple scatter plot\nsns.scatterplot(data['total_yards_home'],data['score_home'],alpha=0.3, hue=data[\"winner\"], palette=\"tab10\");","567de4c7":"# scatterplot with regression line\nsns.regplot(data['total_yards_home'],data['score_home'], marker=\"+\", scatter_kws={'alpha':0.3, 'color':'green'});","322c014c":"def bin_by(x, y, nbins=30, bins = None):\n    \"\"\"\n    Divide the x axis into sections and return groups of y based on its x value\n    \"\"\"\n    if bins is None:\n        bins = np.linspace(x.min(), x.max(), nbins)\n\n    bin_space = (bins[-1] - bins[0])\/(len(bins)-1)\/2\n\n    indicies = np.digitize(x, bins + bin_space)\n\n    output = []\n    for i in range(0, len(bins)):\n        output.append(y[indicies==i])\n    #\n    # prepare a dataframe with cols: median; mean; 1up, 1dn, 2up, 2dn, 3up, 3dn\n    df_names = ['mean', 'median', '5th', '95th', '10th', '90th', '25th', '75th']\n    df = pd.DataFrame(columns = df_names)\n    to_delete = []\n    # for each bin, determine the std ranges\n    for y_set in output:\n        if y_set.size > 0:\n            av = y_set.mean()\n            intervals = np.percentile(y_set, q = [50, 5, 95, 10, 90, 25, 75])\n            res = [av] + list(intervals)\n            df = df.append(pd.DataFrame([res], columns = df_names))\n        else:\n            # just in case there are no elements in the bin\n            to_delete.append(len(df) + 1 + len(to_delete))\n            \n\n    # add x values\n    bins = np.delete(bins, to_delete)\n    df['x'] = bins\n\n    return df\n\ndef custom_percentile_plot(data, x_var, y_var, nbins):\n    # generate random variables\n    x,y = data[x_var],data[y_var]\n\n    # bin the values and determine the envelopes\n    df = bin_by(x, y, nbins=nbins, bins = None)\n\n    # determine the colors\n    cols = ['#EE7550', '#F19463', '#F6B176']\n\n    with plt.style.context('fivethirtyeight'): \n        # plot the 3rd stdv\n        plt.fill_between(df.x, df['5th'], df['95th'], alpha=0.7,color = cols[2])\n        plt.fill_between(df.x, df['10th'], df['90th'], alpha=0.7,color = cols[1])\n        plt.fill_between(df.x, df['25th'], df['75th'], alpha=0.7,color = cols[0])\n        # plt the line\n        plt.plot(df.x, df['median'], color = '1', alpha = 0.7, linewidth = 1)\n        # plot the points\n        plt.scatter(x, y, facecolors='white', edgecolors='0', s = 5, lw = 0.7)\n        plt.title('Percenile Plot (90%, 80%, and 50%)')\n        plt.xlabel(x_var)\n        plt.ylabel(y_var)\n\n    plt.savefig('fig1.png', facecolor='white', edgecolor='none')\n    plt.show()","818ce3be":"custom_percentile_plot(data, 'total_yards_home', 'score_home', 25)","f1747fb6":"# jointplot, with both histograms and regression line in red\ng = sns.jointplot(x=data['total_yards_home'], y=data['score_home'], kind='reg',\n                  joint_kws={'line_kws':{'color':'red'}}, scatter_kws={'alpha':0.1}); \ng.plot_joint(sns.kdeplot, color=\"r\", zorder=0, levels=6);","899636ee":"# make a linear regreesion model to predict the hom team score\nmodel = sm.OLS(data['score_home'], data[['total_yards_home','total_yards_away']])\nresults = model.fit()\nresults.summary()","94755023":"sns.set(rc={'figure.figsize':(12,7)})\ng = sns.regplot(x=data['score_home'], y=data['winner'], logistic=True, scatter_kws={'alpha':0.1})","320e8248":"# make a linear regreesion model to predict the hom team score\nmodel = sm.OLS(data['winner'], data.select_dtypes(['number']).drop(columns=['winner','score_diff','score_home','score_away']))\nresults = model.fit()\nresults.summary()","37425208":"# show the results sorted by coef or p-score\nresults_df = pd.read_html(results.summary().tables[1].as_html(),header=0,index_col=0)[0]\nresults_df.sort_values('coef')\nresults_df.sort_values('P>|t|')","fe12712d":"# Skip to reduce runtime\n#g = sns.pairplot(data.select_dtypes(include=[\"number\"]), hue=\"winner\", palette=\"tab10\", diag_kws={'bw':'1.0'}, plot_kws={'line_kws':{'color':'red'}, 'scatter_kws': {'alpha': 0.1})#","649c848a":"feature_names = ''\nfor feature_name in data.columns:\n    feature_names = feature_names+' | '+feature_name\nprint(feature_names)","01556744":"team_names = np.unique(data.away)\nprint(team_names)","6df79426":"data.head()","b20b33c1":"# create home and away dataframe\ndf_home = data\ndf_away = data\n\n# create a column 'team' with team name and 'opponent' with the oponent temas name\ndf_home = df_home.rename({'home': 'team', 'away': 'opponent'}, axis=1)\ndf_away = df_away.rename({'away': 'team', 'home': 'opponent'}, axis=1)\n# create a column 'home' determining if the game was home game\ndf_home['home'] = 'Y'\ndf_away['home'] = 'N'\n\n# rename the columns to reflect team and opponent instead of home and away\nfor name in column_names[3:]:\n    if 'home' in name:\n        df_home = df_home.rename({name: name.replace('_home','')}, axis=1)\n        df_away = df_away.rename({name: name.replace('_home','_opp')}, axis=1)\n    else:\n        df_home = df_home.rename({name: name.replace('_away','_opp')}, axis=1)\n        df_away = df_away.rename({name: name.replace('_away','')}, axis=1)\n        \ndf_away['winner'] = 1-df_away['winner']\ndf_away['score_diff'] = -df_away['score_diff']\n\ndf = pd.concat([df_home,df_away])\ndf['day_number'] = pd.to_numeric((pd.to_datetime(df['date']) - pd.to_datetime(data['date'][0])).dt.days, downcast='integer')\ndf.sort_values(by=['day_number'], inplace=True)","d338ead1":"# check that there are no NaNs\ndf.columns[df.isna().any()].tolist()","04b1543d":"data.head()","c4e27fd9":"df_home.head()","99943143":"df_away.head()","ab3d8cd8":"df.head()","47e105ee":"team_results = {}\nteam_results_avg = {}     \nfor name in team_names:\n    team_results[name] = df.loc[df['team'] == name].sort_values(by=['day_number'], inplace=False)\n    team_results_avg[name] = df.loc[df['team'] == name].sort_values(by=['day_number'], inplace=False)","a6dfbe41":"# check the result\nteam_results['Bills']","930581a8":"N = 20\n\ncolumn_indices_to_avg = np.where([x in team_results['Bills'].select_dtypes(['number']).columns for x in team_results['Bills'].columns])\n\nfor name in team_names:\n    for idx in range(N,team_results[name].shape[0]):\n        # Determine the days to the previous games, then weights for then weighted average.\n        # Weights drop by about 7% for each previous week, or\n        # by 90% going back to a previous season\n        days = team_results[name].iloc[(idx-N):(idx-1)]['day_number'].to_numpy(dtype=float)\n        days = days-days[-1]\n        coeff = np.exp(days\/200)\/np.sum(np.exp(days\/400))\n        for col_idx in column_indices_to_avg:\n            team_results_avg[name].iloc[idx,col_idx] = np.dot(coeff,team_results[name].iloc[(idx-N):(idx-1),col_idx])\n    ","9c362a5a":"team_results_avg['Bills'][['date','team','home','opponent','winner','score','score_opp','score_diff']]","4c0f930c":"import matplotlib.dates as mdates\n\nsns.set(rc={'figure.figsize':(20,10)})\nfig, ax = plt.subplots()\nplot_ = sns.lineplot(pd.to_datetime(team_results_avg['Bills'].date),team_results_avg['Bills'].score_diff.to_numpy(), label='Bills', marker='o')\nplot_ = sns.lineplot(pd.to_datetime(team_results_avg['Patriots'].date),team_results_avg['Patriots'].score_diff.to_numpy(), label='Patriots', marker='o')\n\n#ax.set_ylim([-15,20]);","2e17767e":"team_results['Bills'].columns[team_results['Bills'].isna().any()].tolist()","5fba516a":"team_results_avg['Bills'].columns[team_results_avg['Bills'].isna().any()].tolist()","89727100":"# Create initial data_home and data_away data frames as empty data frames with the correct column names\ndata_home = pd.DataFrame(columns = team_results_avg['Bills'].columns) \ndata_away = pd.DataFrame(columns = team_results_avg['Bills'].columns) \n\nfor idx in range(data.shape[0]):\n    # get the date for this row\n    date = data.iloc[idx].date\n    # get the home and away team for this row\n    home_team = data.iloc[idx].home\n    away_team = data.iloc[idx].away\n    home_data = team_results_avg[home_team].loc[team_results_avg[home_team]['date'] == date]\n    away_data = team_results_avg[away_team].loc[team_results_avg[away_team]['date'] == date]\n    # concatonate the rows to each data_home, data_away\n    data_home = pd.concat([data_home,home_data])\n    data_away = pd.concat([data_away,away_data])\n    \n# append '_homeAvg' and '_awayAvg' to the home and away datframes\nfor name in data_home.columns:\n    data_home = data_home.rename({name: name+'_homeAvg'}, axis=1)\n    data_away = data_away.rename({name: name+'_awayAvg'}, axis=1)\n\n# drop un-needed columns the home and away datframes\ndata_home = data_home.drop(['date_homeAvg', 'team_homeAvg', 'opponent_homeAvg'], axis=1)\ndata_away = data_away.drop(['date_awayAvg', 'team_awayAvg', 'opponent_awayAvg'], axis=1)","b00be91a":"df = pd.concat([data[['date','home','away','winner','score_home','score_away','score_diff']],data_home,data_away], axis=1, sort=False)","85eec194":"df.columns[df.isna().any()].tolist()","fc33e0d2":"df.head()","d8808933":"df[['date','home','away','winner','score_home','score_away','score_diff','score_homeAvg','score_opp_homeAvg','score_awayAvg','score_opp_awayAvg']].iloc[240:260]","8f51eb60":"nDrop = 500","65bb3f10":"df1 = df[['date','home','away','winner','score_home','score_away','score_diff','score_homeAvg','score_opp_homeAvg','score_awayAvg','score_opp_awayAvg']]","476857d6":"X = sm.add_constant(df1[['score_homeAvg','score_opp_homeAvg','score_awayAvg','score_opp_awayAvg']].iloc[nDrop:])\nY = df1['score_home'].iloc[nDrop:]\n# make a linear regreesion model to predict the hom team score\nmodel = sm.OLS(Y, X)\nresults = model.fit()\nresults.summary()","ec032afa":"pred_score_home = results.predict(X)","c3a4c0d4":"np.sqrt(np.mean((pred_score_home-Y)**2))","5efa16ad":"sns.regplot(pred_score_home,Y, marker=\"+\",scatter_kws={'alpha':0.5});","b150b9ce":"sns.scatterplot(pred_score_home,Y, alpha=0.5);","50e89469":"dfX = df.drop(['date','home','away','winner','score_home','score_away','score_diff','home_awayAvg','home_homeAvg'], axis=1)\ndfX.columns[dfX.isna().any()].tolist()","7d7c65a7":"X = sm.add_constant(dfX.iloc[nDrop:])\nY = df['score_home'].iloc[nDrop:].astype(float)\n# make a linear regreesion model to predict the hom team score\nmodel = sm.OLS(Y, X)\nresults = model.fit()\nresults.summary()\n\npred_score_home = results.predict(X)\nprint('RMS:')\nprint(np.sqrt(np.mean((pred_score_home-Y)**2)))\nsns.regplot(pred_score_home,Y, marker=\"+\",scatter_kws={'alpha':0.5});","192b3835":"results.summary()","361dab0e":"# make a linear regreesion model to predict the hom team score\nmodel = sm.OLS(Y, X)\nresults = model.fit_regularized(method='elastic_net')\n\npred_score_home = results.predict(X)\nprint('RMS:')\nprint(np.sqrt(np.mean((pred_score_home-Y)**2)))\nsns.regplot(pred_score_home,Y, marker=\"+\",scatter_kws={'alpha':0.5});","5e539800":"Y = df1['winner'].iloc[nDrop:]\n\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nclf = LinearDiscriminantAnalysis()\nclf.fit(X, Y)\n\n\npred_winner = clf.predict(X)\nprint('accuracy:')\nprint(1-np.sum(np.abs(pred_winner-Y))\/len(Y))","d1d323ad":"pred_prob_winner = clf.predict_proba(X)\nx = pred_prob_winner[np.where(Y == pred_winner),0].T\ny = pred_prob_winner[np.where(Y != pred_winner),0].T\n\nbins = np.linspace(0, 1, 100)\n\nc = plt.hist(x, bins, alpha=0.5, label='Correct')\nw = plt.hist(y, bins, alpha=0.5, label='Wrong')\nplt.legend(loc='upper right')\nplt.show()","065806f0":"plt.plot(np.linspace(0, 1, 99),c[0]\/(c[0]+w[0]))","058ba3d7":"np.shape(y)","ada36606":"from mpmath import mp\nmp.dps = 50\nclass BMA:\n    \n    def __init__(self, y, X, **kwargs):\n        # Setup the basic variables.\n        self.y = y\n        self.X = X\n        self.names = list(X.columns)\n        self.nRows, self.nCols = np.shape(X)\n        self.likelihoods = mp.zeros(self.nCols,1)\n        self.likelihoods_all = {}\n        self.coefficients_mp = mp.zeros(self.nCols,1)\n        self.coefficients = np.zeros(self.nCols)\n        self.probabilities = np.zeros(self.nCols)\n        # Check the max model size. (Max number of predictor variables to use in a model.)\n        # This can be used to reduce the runtime but not doing an exhaustive sampling.\n        if 'MaxVars' in kwargs.keys():\n            self.MaxVars = kwargs['MaxVars']\n        else:\n            self.MaxVars = self.nCols  \n        # Prepare the priors if they are provided.\n        # The priors are provided for the individual regressor variables.\n        # The prior for a model is the product of the priors on the variables in the model.\n        if 'Priors' in kwargs.keys():\n            if np.size(kwargs['Priors']) == self.nCols:\n                self.Priors = kwargs['Priors']\n            else:\n                print(\"WARNING: Provided priors error.  Using equal priors instead.\")\n                print(\"The priors should be a numpy array of length equal tot he number of regressor variables.\")\n                self.Priors = np.ones(self.nCols)  \n        else:\n            self.Priors = np.ones(self.nCols)  \n        if 'Verbose' in kwargs.keys():\n            self.Verbose = kwargs['Verbose'] \n        else:\n            self.Verbose = False \n        if 'RegType' in kwargs.keys():\n            self.RegType = kwargs['RegType'] \n        else:\n            self.RegType = 'LS' \n        \n    def fit(self):\n        # Perform the Bayesian Model Averaging\n        \n        # Initialize the sum of the likelihoods for all the models to zero.  \n        # This will be the 'normalization' denominator in Bayes Theorem.\n        likelighood_sum = 0\n        \n        # To facilitate iterating through all possible models, we start by iterating thorugh\n        # the number of elements in the model.  \n        max_likelihood = 0\n        for num_elements in np.concatenate((np.arange(1,self.MaxVars+1),np.arange(self.nCols-self.MaxVars+2,self.nCols))): \n            #for num_elements in range(1,self.MaxVars+1): \n            \n            if self.Verbose == True:\n                print(\"Computing BMA for models of size: \", num_elements)\n            \n            # Make a list of all index sets of models of this size.\n            Models_next = list(combinations(list(range(self.nCols)), num_elements)) \n             \n            # Occam's window - compute the candidate models to use for the next iteration\n            # Models_previous: the set of models from the previous iteration that satisfy (likelihhod > max_likelihhod\/20)\n            # Models_next:     the set of candidate models for the next iteration\n            # Models_current:  the set of models from Models_next that can be consturcted by adding one new variable\n            #                    to a model from Models_previous\n            if num_elements == 1:\n                Models_current = Models_next\n                Models_previous = []\n            else:\n                idx_keep = np.zeros(len(Models_next))\n                for M_new,idx in zip(Models_next,range(len(Models_next))):\n                    for M_good in Models_previous:\n                        if(all(x in M_new for x in M_good)):\n                            idx_keep[idx] = 1\n                            break\n                        else:\n                            pass\n                Models_current = np.asarray(Models_next)[np.where(idx_keep==1)].tolist()\n                Models_previous = []\n                        \n            \n            # Iterate through all possible models of the given size.\n            for model_index_set in Models_current:\n                \n                # Compute the linear regression for this given model. \n                model_X = self.X.iloc[:,list(model_index_set)]\n                if self.RegType == 'Logit':\n                    model_regr = sm.Logit(self.y, model_X).fit(disp=0)\n                else:\n                    model_regr = OLS(self.y, model_X).fit()\n                \n                # Compute the likelihood (times the prior) for the model. \n                model_likelihood = mp.exp(-model_regr.bic\/2)*np.prod(self.Priors[list(model_index_set)])\n                    \n                if (model_likelihood > max_likelihood\/20):\n                    if self.Verbose == True:\n                        print(\"Model Variables:\",model_index_set,\"likelihood=\",model_likelihood)\n                    self.likelihoods_all[str(model_index_set)] = model_likelihood\n                    \n                    # Add this likelihood to the running tally of likelihoods.\n                    likelighood_sum = mp.fadd(likelighood_sum, model_likelihood)\n\n                    # Add this likelihood (times the priors) to the running tally\n                    # of likelihoods for each variable in the model.\n                    for idx, i in zip(model_index_set, range(num_elements)):\n                        self.likelihoods[idx] = mp.fadd(self.likelihoods[idx], model_likelihood, prec=1000)\n                        self.coefficients_mp[idx] = mp.fadd(self.coefficients_mp[idx], model_regr.params[i]*model_likelihood, prec=1000)\n                    Models_previous.append(model_index_set) # add this model to the list of good models\n                    max_likelihood = np.max([max_likelihood,model_likelihood]) # get the new max likelihood if it is this model\n                else:\n                    if self.Verbose == True:\n                        print(\"Model Variables:\",model_index_set,\"rejected by Occam's window\")\n                    \n\n        # Divide by the denominator in Bayes theorem to normalize the probabilities \n        # sum to one.\n        self.likelighood_sum = likelighood_sum\n        for idx in range(self.nCols):\n            self.probabilities[idx] = mp.fdiv(self.likelihoods[idx],likelighood_sum, prec=1000)\n            self.coefficients[idx] = mp.fdiv(self.coefficients_mp[idx],likelighood_sum, prec=1000)\n        \n        # Return the new BMA object as an output.\n        return self\n    \n    def predict(self, data):\n        data = np.asarray(data)\n        if self.RegType == 'Logit':\n            try:\n                result = 1\/(1+np.exp(-1*np.dot(self.coefficients,data)))\n            except:\n                result = 1\/(1+np.exp(-1*np.dot(self.coefficients,data.T)))\n        else:\n            try:\n                result = np.dot(self.coefficients,data)\n            except:\n                result = np.dot(self.coefficients,data.T)\n        \n        return result  \n        \n    def summary(self):\n        # Return the BMA results as a data frame for easy viewing.\n        df = pd.DataFrame([self.names, list(self.probabilities), list(self.coefficients)], \n             [\"Variable Name\", \"Probability\", \"Avg. Coefficient\"]).T\n        return df  ","0e899e23":"result = BMA(Y, X, RegType = 'Logit', Verbose=True, MaxVars=3).fit()","9a3ea1ea":"print(result.summary().sort_values('Probability').iloc[-50:])","691d8809":"# predict the y-values from training input data\npred_BMA = result.predict(X)","187b674f":"# compute accuracy\nprint(\"BMA Accuracy: \", np.sum((pred_BMA > 0.5) == Y)\/len(Y))","8f4f0c7c":"Y = df1['score_diff'].iloc[200:]\n# make a linear regreesion model to predict the hom team score\nmodel = sm.OLS(Y, X)\nresults = model.fit()\nresults.summary()\n\npred_score_diff = results.predict(X)\nprint('RMS:')\nprint(np.sqrt(np.mean((pred_score_diff-Y)**2)))\nsns.regplot(pred_score_diff,Y, marker=\"+\",scatter_kws={'alpha':0.5});","5c368fb2":"print('Prediciton Accuracy:')\nnp.sum(pred_score_diff*Y > 0)\/len(Y)","37cb13a3":"Plot the recent-time-averages score diff as a function of date for the Bills and Patriots.","262fc92e":"List all the features.","5127194c":"Create the per-team data frames","beab9ddd":"Logistic Regression Model to see if we can predict the winner based on all statistics other than the scores.  (Using the score would be to 'easy'.)","e5b92dae":"Create a dictionary team_results that will hold the results over all games for each team.","23c5c622":"Check the regression line for this relationship.","537a9999":"CONCLUSION: As expected, the score for the home team is strongly related to teh # yards scored by the home team (p-score fo 0.000m ,coeff of 0.0696) but not strongly related to the yards by the aways team (p-score of 0.1, coeff of -0.0017).  Each 10 additional yards gained by the home team corresponds to about 1\/2 of an additional point.","95363cd0":"Check the data types.","a4b89a7f":"# 4. Now some predictive analytics","33d0efc5":"Create a scatterplot to get peak at a relationship that should be meaningful - score_home as a function of total_yards_home, colored by whether the home team won (winner=1) or not (winner=0)","6d526635":"# 2. Exploratory Data Analysis for the initial data frame","31cf086e":"Create a weighted average of previous games.","b5403dd2":"# 1. Read data and basic setup.","21ed2449":"# 3. Build per-team data frame","bbd9a870":"Read in the data, compute standard numeric stats from some of the string variables, and check the head.","47407ff6":"Pairs Plot of all features","b71a76d8":"Plot data precentiles for the score as a function of the yards for the home team.","d841d4a1":"View basic info about the data using the describe method."}}