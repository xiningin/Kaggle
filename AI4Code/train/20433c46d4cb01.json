{"cell_type":{"7880d921":"code","d85f797a":"code","4b809439":"code","b141a726":"code","11b397df":"code","65d25c35":"code","2548cc8d":"code","4b9c4c76":"code","1ed952af":"code","aca87903":"code","9e796ed5":"code","e0456d2b":"code","d418454d":"code","baebd5ae":"code","85adb859":"code","55556dff":"code","d90cd891":"code","42623003":"code","546fdd95":"code","1d1ceb83":"code","37958bf3":"code","7cb80069":"code","14b4ac71":"code","c4d06d53":"markdown","ced8ad05":"markdown","44e5ac99":"markdown","c4d719be":"markdown","f27233f5":"markdown","65be2b94":"markdown","20552d3c":"markdown","8430b655":"markdown"},"source":{"7880d921":"import nltk\n\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('averaged_perceptron_tagger')","d85f797a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport string\nimport re\n\nimport nltk\nfrom nltk.corpus import wordnet,stopwords\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.cm as cm\n\nimport unicodedata\n#from contractions import CONTRACTION_MAP","4b809439":"inp_df = pd.read_excel('..\/input\/automatic-ticket-assignment-using-nlp\/Automatic Ticket Assignment.xlsx')\ninp_df.head()","b141a726":"inp_df.shape","11b397df":"#Since the caller column is not useful, we can drop it\n\ninp_df.drop(columns='Caller',inplace=True)\ninp_df.head()","65d25c35":"#Check for Null and drop null rows - Since null count is less\n\ninp_df.isnull().sum()\ninp_df.dropna(inplace=True)","2548cc8d":"# Drop duplicate rows\n\ninp_df[inp_df.duplicated(['Short description','Description'])]\ninp_df.drop_duplicates(['Short description','Description'],inplace=True)","4b9c4c76":"inp_df['Assignment group'].value_counts()","1ed952af":"#Plot line graph and view counts of each tickets\n\nplt.subplots(figsize = (20,5))\n\nsns.countplot(x='Assignment group', data=inp_df,order = inp_df['Assignment group'].value_counts().index)\nplt.xlabel('Assignment Group') \nplt.ylabel('Count') \nplt.xticks(rotation=90)\nplt.title('Tickets Distribution')\n\nplt.show()","aca87903":"# Lets skip GRP_0 and visualize the count\n\ntemp_df1 = inp_df[inp_df['Assignment group'] != 'GRP_0']\n\nplt.subplots(figsize = (20,5))\n\nsns.countplot(x='Assignment group', data=temp_df1,order = temp_df1['Assignment group'].value_counts().index)\nplt.xlabel('Assignment Group') \nplt.ylabel('Count') \nplt.xticks(rotation=90)\nplt.title('Tickets Distribution - Excluding GRP_0')\n\nplt.show()","9e796ed5":"temp_df2 = pd.DataFrame(inp_df['Assignment group'].value_counts())\ntemp_df2 = temp_df2.T\ntemp_df2","e0456d2b":"inp_df['Count'] = inp_df.apply(lambda row: temp_df2[row['Assignment group']] , axis=1)\n\ninp_df.loc[inp_df['Count'] <= 200 , \"Assignment group\"] = \"GRP_X\"","d418454d":"#Plot line graph and view counts of each tickets - After GRP_X\n\nplt.subplots(figsize = (20,5))\n\nsns.countplot(x='Assignment group', data=inp_df,order = inp_df['Assignment group'].value_counts().index)\nplt.xlabel('Assignment Group') \nplt.ylabel('Count') \nplt.title('Tickets Distribution - With GRP_X')\n\nplt.show()","baebd5ae":"inp_df['Assignment group'].value_counts()","85adb859":"inp_df.drop(columns='Count',inplace=True)\ninp_df.head()","55556dff":"#Lets create temp csv to see the data as of now\n\n#inp_df.to_csv('temp_data.csv')","d90cd891":"#Lets merge both the columns into Single one\n\ninp_df['Full Description'] = inp_df['Short description'] + ' '+ inp_df['Description']\ninp_df.drop(columns=['Short description','Description'],inplace=True)\ninp_df.head()","42623003":"#Remove numbers\ndef removeNumbers(text):\n    \"\"\" Removes integers \"\"\"\n    text = ''.join([i for i in text if not i.isdigit()])         \n    return text\n\n\n#Replace Contractions\n\ncontraction_patterns = [ (r'won\\'t', 'will not'),(r'didn\\'t', 'did not'),(r'didnt', 'did not'), (r'can\\'t', 'cannot'),(r'cant', 'cannot'), (r'i\\'m', 'i am'), (r'ain\\'t', 'is not'), (r'(\\w+)\\'ll', '\\g<1> will'), (r'(\\w+)n\\'t', '\\g<1> not'),\n                         (r'(\\w+)\\'ve', '\\g<1> have'), (r'(\\w+)\\'s', '\\g<1> is'), (r'(\\w+)\\'re', '\\g<1> are'), (r'(\\w+)\\'d', '\\g<1> would'), (r'&', 'and'), (r'dammit', 'damn it'), (r'dont', 'do not'), (r'wont', 'will not') ]\ndef replaceContraction(text):\n    patterns = [(re.compile(regex), repl) for (regex, repl) in contraction_patterns]\n    for (pattern, repl) in patterns:\n        (text, count) = re.subn(pattern, repl, text)\n    return text\n'''\ndef replaceContraction(text, contraction_mapping=CONTRACTION_MAP):\n    \n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n                                      flags=re.IGNORECASE|re.DOTALL)\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match)\\\n                                if contraction_mapping.get(match)\\\n                                else contraction_mapping.get(match.lower())                       \n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n        \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text\n'''\n\n#Replace Negations with Antonym\ndef replace(word, pos=None):\n    \"\"\" Creates a set of all antonyms for the word and if there is only one antonym, it returns it \"\"\"\n    antonyms = set()\n    for syn in wordnet.synsets(word, pos=pos):\n        for lemma in syn.lemmas():\n            for antonym in lemma.antonyms():\n                antonyms.add(antonym.name())\n    if len(antonyms) == 1:\n        return antonyms.pop()\n    else:\n        return None\n\ndef replaceNegations(text):\n    \"\"\" Finds \"not\" and antonym for the next word and if found, replaces not and the next word with the antonym \"\"\"\n    i, l = 0, len(text)\n    words = []\n    while i < l:\n        word = text[i]\n        if word == 'not' and i+1 < l:\n            ant = replace(text[i+1])\n            if ant:\n                words.append(ant)\n                i += 2\n                continue\n        words.append(word)\n        i += 1\n    return words\n\ndef antonym(text):\n    tokens = nltk.word_tokenize(text)\n    tokens = replaceNegations(tokens)\n    text = \" \".join(tokens)\n    return text\n\n\n#Remove Stopwords\nstoplist = stopwords.words('english')\nstoplist.remove('no')\nstoplist.remove('not')\ndef stp_words(text):\n    finalTokens = []\n    tokens = nltk.word_tokenize(text)\n    for w in tokens:\n        if (w not in stoplist):\n            finalTokens.append(w)\n    text = \" \".join(finalTokens)\n    return text\n\n#Remove mail related words\nmail_words_list = ['hi','hello','com','gmail','cc','regards','thanks']\ndef mail_words(text):\n    finalTokens = []\n    tokens = nltk.word_tokenize(text)\n    for w in tokens:\n        if (w not in mail_words_list):\n            finalTokens.append(w)\n    text = \" \".join(finalTokens)\n    return text\n\n#Lemmatization\nstemmer = PorterStemmer() #set stemmer\nlemmatizer = WordNetLemmatizer() # set lemmatizer\n\ndef nltk_tag_to_wordnet_tag(nltk_tag):\n    if nltk_tag.startswith('J'):\n        return wordnet.ADJ\n    elif nltk_tag.startswith('V'):\n        return wordnet.VERB\n    elif nltk_tag.startswith('N'):\n        return wordnet.NOUN\n    elif nltk_tag.startswith('R'):\n        return wordnet.ADV\n    else:          \n        return None\n\ndef lemmatize_sentence(sentence):\n    #tokenize the sentence and find the POS tag for each token\n    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n    #tuple of (token, wordnet_tag)\n    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n    lemmatized_sentence = []\n    for word, tag in wordnet_tagged:\n        if tag is None:\n            #if there is no available tag, append the token as is\n            lemmatized_sentence.append(word)\n        else:        \n            #else use the tag to lemmatize the token\n            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n    return \" \".join(lemmatized_sentence)\n\n#Remove accented characters\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text","546fdd95":"for index, row in inp_df.iterrows():\n    # remove numbers\n    proc_des = removeNumbers(row['Full Description'])\n    \n    #remove punctuations\n    #translator = str.maketrans('', '', string.punctuation)\n    #proc_des = proc_des.translate(translator) \n    proc_des = re.sub(r\"\\W\", \" \", proc_des, flags=re.I)\n    proc_des = proc_des.replace('_',' ')\n    \n    #replace contractions\n    proc_des = replaceContraction(proc_des)\n    \n    #remove accents\n    proc_des = remove_accented_chars(proc_des)\n    \n    #convert to lower case\n    proc_des = proc_des.lower()\n    \n    #replace negation with antonym - Skipping this for our case, as it couldnt process words like cannot\n    #proc_des = antonym(proc_des)\n    \n    #remove stopwords\n    proc_des = stp_words(proc_des)\n    \n    #remove mail related words\n    proc_des = mail_words(proc_des)\n    \n    #check whether the language is English\n    #lang = detect(proc_des)\n    \n    #lemmatization\n    proc_des = lemmatize_sentence(proc_des)\n    \n    #create new column and add updated data\n    #inp_df.set_value(index, 'Full Description - After', proc_des)\n    inp_df.at[index, 'Full Description - After']= proc_des\n    #inp_df.set_value(index, 'Language', lang)","1d1ceb83":"inp_df.head()","37958bf3":"#Lets create temp csv to see the data as of now\n\n#inp_df.to_csv('temp_data1.csv')","7cb80069":"stopwords = set(STOPWORDS)\n\n#function to create Word Cloud\ndef show_wordcloud(data, title):\n    wordcloud = WordCloud(\n        background_color='black',\n        stopwords=stopwords,\n        max_words=200,\n        max_font_size=40, \n        scale=3,\n        random_state=1 \n    ).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 12))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()","14b4ac71":"for grp in inp_df['Assignment group'].unique().tolist():\n    text_Str = inp_df['Full Description - After'][inp_df['Assignment group'].isin([grp])].tolist()\n    show_wordcloud(text_Str, '\\n'+ grp +' - WORD CLOUD')\n    print('===============================================================================================================')","c4d06d53":"Here we see that almost half of the existing tickets belong to GRP_0","ced8ad05":"Plotting the Tickets count after creating GRP_X","44e5ac99":"From the above figure we see that GRP_0 has more count","c4d719be":"# EDA","f27233f5":"We can group the tickets count < 200 as GRP_X. So modifying data based on that ","65be2b94":"# Text Pre-Processing","20552d3c":"https:\/\/www.kaggle.com\/deffro\/text-pre-processing-techniques\n\nNow clean up the Description column to address the following\n* Convert each character in a sentence to lowercase character\n* Remove HTML Tags\n* Remove punctuations\n* Remove stopwords\n* Remove common words like com, hello","8430b655":"# Visualization - Word Cloud"}}