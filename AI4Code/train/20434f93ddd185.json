{"cell_type":{"3f709492":"code","7cf683a8":"code","e7523349":"code","32b33389":"code","a3e4dfe6":"code","ad231620":"code","acb5c06d":"code","940dbfc4":"code","cf26f4cb":"code","bf23fc05":"code","f983e9da":"code","bc02cf96":"code","4af5f9f4":"code","a0d30a43":"code","7a129af1":"code","05e8ebdb":"code","91a45377":"code","075bc32a":"code","f4d570ea":"code","a1846225":"code","e1277dbb":"code","c96d6c47":"code","20ad9f23":"code","ee130d6c":"code","b4868571":"code","ed8958b5":"code","72e414bf":"markdown","dbadc561":"markdown","a0175fb2":"markdown","9f6367b1":"markdown","5a8e188e":"markdown","30a53722":"markdown","94156c57":"markdown","a5637570":"markdown","b2a6a055":"markdown","5f7872eb":"markdown","b3a4a7aa":"markdown","e98c8aa4":"markdown","5386be70":"markdown","525a3706":"markdown","8ca41f28":"markdown","e1f47630":"markdown","a8850643":"markdown","5a3081a2":"markdown","24b30963":"markdown"},"source":{"3f709492":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","7cf683a8":"df = pd.read_csv('\/kaggle\/input\/did-it-rain-in-seattle-19482017\/seattleWeather_1948-2017.csv')","e7523349":"print(df.shape)\ndf.head()","32b33389":"df['RAIN'].value_counts()","a3e4dfe6":"for column in df:\n    print(column)\n    print(df[column].isnull().sum().sum())","ad231620":"df.dropna(inplace=True)","acb5c06d":"sns.pairplot(df, hue='RAIN')","940dbfc4":"sns.displot(data=df, x='TMIN', y='TMAX', hue='RAIN', kind='kde')","cf26f4cb":"rain = pd.get_dummies(df['RAIN'], drop_first=True)","bf23fc05":"df = df.drop('RAIN', axis=1)\ndf = pd.concat([df, rain], axis=1)\nprint(df.shape)\ndf.head()","f983e9da":"df.columns = ['DATE', 'PRCP', 'TMAX', 'TMIN', 'RAIN']\ndf.head()","bc02cf96":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics","4af5f9f4":"x = df[['PRCP', 'TMAX', 'TMIN']]\ny = df['RAIN']","a0d30a43":"x.shape, y.shape","7a129af1":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)","05e8ebdb":"from sklearn.linear_model import LogisticRegression\nlogistic = LogisticRegression()\nlogistic.fit(x_train, y_train)\nprediction_lr = logistic.predict(x_test)\nprint(classification_report(y_test,prediction_lr))","91a45377":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\ntree.fit(x_train, y_train)\nprediction_dt = tree.predict(x_test)\nprint(classification_report(y_test, prediction_dt))","075bc32a":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier()\nforest.fit(x_train, y_train)\nprediction_rf = forest.predict(x_test)\nprint(classification_report(y_test, prediction_rf))","f4d570ea":"import xgboost\nxgb = xgboost.XGBClassifier()\nxgb.fit(x_train,y_train)\nprediction_xgb = xgb.predict(x_test)\nprint(classification_report(y_test, prediction_xgb))","a1846225":"import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.layers import Dropout","e1277dbb":"x.shape","c96d6c47":"model = Sequential([\n    Dense(32, activation='relu', input_dim=3),\n    Dropout(0.5),\n    Dense(16, activation='relu'),\n    Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\nmodel.summary()","20ad9f23":"history = model.fit(x_train, y_train, batch_size=10, epochs=10,verbose=2)","ee130d6c":"print(history.history.keys())","b4868571":"plt.plot(history.history['accuracy'], label='Accuracy', color='blue')\nplt.plot(history.history['loss'], label='Loss', color='red')\nplt.legend()","ed8958b5":"prediction_nn = model.predict(x_test)\nprediction_nn = [1 if y>=0.5 else 0 for y in prediction_nn]\nprint(classification_report(y_test, prediction_nn))","72e414bf":"Traditional models performed great. This dataset is just perfect for traditional machine learning techniques: big dataset with few number of variables. Finally I want to try the data with a artificial neural network.","dbadc561":"<img src=\"https:\/\/media.giphy.com\/media\/xUPOqo6E1XvWXwlCyQ\/giphy.gif\">","a0175fb2":"This is it for this notebook. Take care.","9f6367b1":"**RANDOM FOREST**","5a8e188e":"I do thing every column is important for this classification problem. Because there are few of them. But it would be nice to see their effect visually.","30a53722":"Now we need to make RAIN column numeric.","94156c57":"**LOGISTIC REGRESSION**","a5637570":"All jokes a side. Supersonics were amazing by the way. In this notebook, you will see a quick cleaning and reshaping to data. Then a little visualization and finally predictions. Enjoy!","b2a6a055":"<img src=\"https:\/\/media.giphy.com\/media\/l46C9VsWTWD9CHocg\/giphy.gif\">","5f7872eb":"**XGBOOST**","b3a4a7aa":"# **DID IT?**","e98c8aa4":"Checking for *null* values.","5386be70":"There are very few *null* values so we can drop them.","525a3706":"It seems an easy classification problem when you see it. But uncertainty of *not raining* is tricky as you can see on the above.","8ca41f28":"**ANN**","e1f47630":"**No. It never rains in Seattle anymore. Not since Supersonics left the city.**","a8850643":"We are good to go.","5a3081a2":"I will make *RAIN* column numeric before I start with predictions but for analysis it is nicer to see the label names.","24b30963":"**DECISION TREE**"}}