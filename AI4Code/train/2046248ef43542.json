{"cell_type":{"5d94bf84":"code","e3179edf":"code","68b6e769":"code","a8577144":"code","37a0fcae":"code","c42ed9f2":"code","f300d634":"code","35263e13":"code","6fe14b87":"code","ba8fed37":"code","df4cc078":"code","62e117ef":"code","0c7e14de":"code","0a22ade6":"code","f0406cfc":"code","66b08582":"code","b938b4db":"code","61b7e285":"code","927c526e":"code","0a47049f":"code","e71d6cd2":"code","1dbb4b23":"code","377cd546":"code","96982ee3":"code","474b1921":"code","35a903b5":"code","6ec8ce7d":"code","390cff9a":"code","9bf7ecd7":"code","4750a17e":"code","a418081b":"code","689a4969":"code","946bd6af":"code","6520c3d6":"code","2ceb7e8e":"code","e35687d8":"code","678da827":"code","47031d79":"code","9d5bdf7e":"code","3128d715":"code","1f6e4e07":"code","ff4224bd":"code","a35fbe93":"code","982504ba":"code","3220b03b":"code","176f6f97":"code","54fc7443":"code","912ca285":"code","b2f3e79f":"code","19216351":"code","184f4911":"code","2220800a":"markdown","f0524de5":"markdown","134af11d":"markdown","72f3581c":"markdown","eaf2345b":"markdown","f3fcd242":"markdown","77aed63c":"markdown","d99a1d97":"markdown","49a01856":"markdown","c9bc6ab7":"markdown","f80c0486":"markdown","7d45b7bf":"markdown","78858f32":"markdown","248f7947":"markdown","10072203":"markdown","235b7b79":"markdown","96a9b43e":"markdown","931d2e0e":"markdown","9a0ee772":"markdown","0407c2b6":"markdown","3fafe995":"markdown","a54180a1":"markdown","03e04766":"markdown","9b9155d2":"markdown"},"source":{"5d94bf84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e3179edf":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","68b6e769":"datasetpath = '\/kaggle\/input\/seoul-bike-rental-ai-pro-iti\/'\n\ndf = pd.read_csv(os.path.join(datasetpath, 'train.csv'))\n\nprint(\"The shape of the dataset is {}.\".format(df.shape))","a8577144":"df.head(5)","37a0fcae":"df.describe()","c42ed9f2":"df.isnull().sum()","f300d634":"df.info()","35263e13":"df[['Day', 'Month', 'Year']] = df[\"Date\"].str.split(\"\/\", expand = True).astype('int')\ndf[['Day', 'Month', 'Year']]","6fe14b87":"from datetime import date\n\ndf[\"Date\"] = pd.to_datetime(df.Date, format=\"%d\/%m\/%Y\")\n\n#extracting passed months since the date\n#df['MonthsPassed'] = (date.today().year - df['Date'].dt.year) * 12 + date.today().month - df['Date'].dt.month # RMSLE increased by 0.01\n\n# extracting the Day Name of the date\ndf['DayName'] = df['Date'].dt.day_name()\ndf.head()","ba8fed37":"df.drop(columns='Date', inplace=True)","df4cc078":"#For Data Encoding\nlst_objects=[]\nfor col in df.columns:\n    if(df[col].dtype == np.object ):\n        #print(col)\n        lst_objects.append(col)\nprint(lst_objects)","62e117ef":"sns.boxplot(data=df, y='y')","0c7e14de":"sns.histplot(data=df, x='y', hue='Seasons', kde=True)","0a22ade6":"ax = sns.boxplot(x='Seasons', y='y', data=df)","f0406cfc":"fig, axes=plt.subplots(nrows=16, ncols=1, figsize=(10,80))\ncol=2 # to avoid columns of ID and y\nfor i in range(16):\n    if df.columns[col] in lst_objects:\n        #sns.histplot(data=df, x='y', hue=df.columns[col], ax=axes[i], kde=True)\n        sns.boxplot(data=df, x=df.columns[col], y='y', ax=axes[i])\n    elif df.columns[col] in ['Hour', 'Day', 'Month', 'Year']:\n        sns.boxplot(data=df, x=df.columns[col], y='y', ax=axes[i])\n    else:\n        sns.scatterplot(data=df, x=df.columns[col], y='y', ax=axes[i])\n    col+=1","66b08582":"fig, axes=plt.subplots(nrows=2, ncols=4, figsize=(40,20))\nlst = ['Temperature(\ufffdC)', 'Humidity(%)', 'Wind speed (m\/s)',\n       'Visibility (10m)', 'Dew point temperature(\ufffdC)',\n       'Solar Radiation (MJ\/m2)', 'Rainfall(mm)', 'Snowfall (cm)']\n\ncol = 3\nfor i in range(2):\n    for j in range(4):\n        sns.histplot(data=df, x=df.columns[col], ax=axes[i,j], kde=True)\n        col += 1","b938b4db":"\"\"\"\ndf_y_median_per_hour = df['y'].groupby(df['Hour']).median()\ndf_y_median_per_hour.to_frame()\n\ndf['y_median_per_hour'] = df_y_median_per_hour[df.Hour].to_numpy()\n\ndf\n\"\"\"","61b7e285":"plt.subplots(figsize=(15,10))\nsns.boxplot(data=df, x='Hour', y='y', hue='Seasons')","927c526e":"df_yMedPerHourSeason = df.groupby(by=['Hour','Seasons']).median()['y']\ndf_yMedPerHourSeason = df_yMedPerHourSeason.to_frame()\n\ndf_yMedPerHourSeason","0a47049f":"_list_ = []\nfor index, row in df.iterrows():\n    _list_.append(df_yMedPerHourSeason.loc[row['Hour'],row['Seasons']].values)\ndf['yMedPerHourSeason'] = np.array(_list_)","e71d6cd2":"df['yMedPerHourSeason'] = np.log(df['yMedPerHourSeason'] + 1)\ndf","1dbb4b23":"sns.histplot(data=df, x='Humidity(%)')","377cd546":"#there is an outlier in the humidity feature as there cant be a humidity of 0%\nfilter = df['Humidity(%)'] == 0\ndf.drop(index = df.loc[filter].index, inplace=True)","96982ee3":"df.shape","474b1921":"corr = df.corr().abs()\ncorr_arr = corr.values\nplt.subplots(figsize=(20,20))\nsns.heatmap(corr_arr, xticklabels=corr.columns, yticklabels=corr.columns, annot=True, cmap=\"Blues\")","35a903b5":"\"\"\"\n#label encoding\nfrom sklearn.preprocessing import LabelEncoder\ndf['Seasons'] = df['Seasons'].astype('category')\nlabelencoder = LabelEncoder()\ndf['Seasons']= labelencoder.fit_transform(df['Seasons'])\n#Adding 1 for all values of label encoding\ndf['Seasons'] += 1\ndf['Seasons']\n\"\"\"","6ec8ce7d":"encoded_col = pd.get_dummies(df[['Seasons', 'DayName']])\ndf = df.join(encoded_col).drop(columns=['Seasons','DayName'])\ndf","390cff9a":"df['Holiday'] = np.where(df.Holiday == 'Holiday', 1, 0)\ndf['Holiday'].sum()","9bf7ecd7":"df['Functioning Day'] = np.where(df['Functioning Day'] == 'Yes', 1, 0)\ndf['Functioning Day'].sum()","4750a17e":"#Min max scalar for numerical data\n\"\"\"\nfrom sklearn.preprocessing import MinMaxScaler\n\nscalar = MinMaxScaler()\nlst = ['Temperature(\ufffdC)','Humidity(%)', 'Wind speed (m\/s)', 'Dew point temperature(\ufffdC)',\n       'Solar Radiation (MJ\/m2)', 'Rainfall(mm)', 'Snowfall (cm)']\ndf[lst] = scalar.fit_transform(df[lst])\ndf[lst]\n\"\"\"","a418081b":"\"\"\"\nfrom sklearn.preprocessing import StandardScaler\n\nscalar = StandardScaler()\nlst = ['Temperature(\ufffdC)','Humidity(%)', 'Wind speed (m\/s)', 'Dew point temperature(\ufffdC)',\n       'Solar Radiation (MJ\/m2)', 'Rainfall(mm)', 'Snowfall (cm)']\ndf[lst] = scalar.fit_transform(df[lst])\ndf[lst]\n\"\"\"","689a4969":"# Use robust scalar for data normalization more robust for many outliers in the data than MinMaxScalar \n# link: https:\/\/towardsdatascience.com\/data-normalization-with-pandas-and-scikit-learn-7c1cc6ed6475\n\"\"\"\nfrom sklearn.preprocessing import RobustScaler\n\nscalar = RobustScaler()\n\nlst = ['Temperature(\ufffdC)','Humidity(%)', 'Wind speed (m\/s)', 'Dew point temperature(\ufffdC)',\n       'Solar Radiation (MJ\/m2)', 'Rainfall(mm)', 'Snowfall (cm)']\ndf[lst] = scalar.fit_transform(df[lst])\ndf[lst]\n\"\"\"","946bd6af":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n\nX_train = train_df.drop(columns=['ID','y'])\ny_train = np.log(train_df['y']+1)\n\nX_val = val_df.drop(columns=['ID','y'])\ny_val = np.log(val_df['y']+1)\n\n# all the data to use in cross validation (kfolds, or any other splits) \n# https:\/\/neptune.ai\/blog\/cross-validation-in-machine-learning-how-to-do-it-right\nX = df.drop(columns=['ID','y'])\ny = np.log(df['y'] + 1)","6520c3d6":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\npred_val= model.predict(X_val)\n\nprint(\"Val RMSLE is :\", mean_squared_error(y_val, pred_val, squared=False)) #squared = false to return RMSE\nprint(\"Val Accuracy Score is :\", model.score(X_val, y_val))","2ceb7e8e":"pred_train= model.predict(X_train)\n\nprint(\"Train RMSLE is :\", mean_squared_error(y_train, pred_train, squared=False)) #squared = false to return RMSE\nprint(\"Train Accuracy Score is :\", model.score(X_train, y_train))","e35687d8":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Define  model as an object of XGBRegressor\n# max_depth = 7 that gets best RMSLE score where max_depth =15 makes it overfitting (worse score) on validation set\nmodel_XGboost = XGBRegressor(n_estimators=1200,max_depth=7,learning_rate=.1,random_state=42,\n                     n_jobs=-1, subsample=1,min_child_weight=0.6)\n\n# Fit the model\nmodel_XGboost.fit(X_train, y_train)\n\n# Get validation predictions using RMSE\npred_val_xgboost = model_XGboost.predict(X_val)\n\nprint(\"Val RMSLE is :\", mean_squared_error(y_val, pred_val_xgboost, squared=False)) #squared = false to return RMSE\nprint(\"Val Accuracy Score is :\", model_XGboost.score(X_val, y_val))","678da827":"pred_train= model_XGboost.predict(X_train)\n\nprint(\"Train RMSLE is :\", mean_squared_error(y_train, pred_train, squared=False)) #squared = false to return RMSE\nprint(\"Train Accuracy Score is :\", model.score(X_train, y_train))","47031d79":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nkfold = KFold(n_splits=10, shuffle=True, random_state=42)\n\nmodel_cv = LinearRegression()\n\nscore = cross_val_score(model_cv, X, y, scoring=\"neg_root_mean_squared_error\", cv=kfold)\n\nprint('Linear regression model using cv = 10 folds')\nprint('RMSLE scores: ', score)\nprint('RMSLE mean(std): {:.5f}({:.5f}) '.format(score.mean(), score.std()))","9d5bdf7e":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nkfold = KFold(n_splits=10, shuffle=True, random_state=42)\n\nmodel_XGboost_cv = XGBRegressor(n_estimators=1200,max_depth=7,learning_rate=.1,random_state=42,\n                     n_jobs=-1, subsample=1,min_child_weight=0.6)\n\nscore = cross_val_score(model_XGboost_cv, X, y, scoring=\"neg_root_mean_squared_error\", cv=kfold)\n\nprint('Xgboost model using cv = 10 folds')\nprint('RMSLE scores: ', score)\nprint('RMSLE mean(std): {:.5f}({:.5f}) '.format(score.mean(), score.std()))","3128d715":"from xgboost import XGBRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import StackingRegressor\n\n\n#from sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\n\nimport time\n\nStackingEstimators = [('lr', LinearRegression()),\n                      ('rt', DecisionTreeRegressor()),\n                      ('knn', KNeighborsRegressor())]\n\n\nStackingModel = StackingRegressor(estimators = StackingEstimators, final_estimator = XGBRegressor())\n\nmodels = [XGBRegressor(), RandomForestRegressor(), ExtraTreesRegressor(), AdaBoostRegressor(),BaggingRegressor(),\n          SVR(), KNeighborsRegressor(), LinearRegression(), BayesianRidge(), StackingModel]\nmodel_names = ['XgboostReg','RandomForestReg','ExtraTressReg','AdaBoostReg','BaggingReg',\n               'SVR','KNeighborsReg', 'LinearReg', 'BayesianRidge', 'StackingReg']\nrmsle = []\ntime_lst =[]\nd={}\n\nkfold = KFold(n_splits=10, shuffle=True, random_state=42)\n\nfor model in range (len(models)):\n    clf = models[model]\n    # using holdout validations \n    #clf.fit(X_train,y_train)\n    #pred_val = clf.predict(X_val)\n    #rmsle.append(mean_squared_error(y_val, pred_val, squared=False))\n\n    #using kfolds for cv of the model\n    print(\"Start fitting {}\".format(model_names[model]))\n    start = time.time()\n    \n    score = cross_val_score(clf, X, y, scoring=\"neg_root_mean_squared_error\", cv=kfold)\n    \n    elp_time = time.time() - start\n    time_lst.append(elp_time)\n    print(\"end fitting {} in {} sec\\n\".format(model_names[model], elp_time))\n    rmsle.append(abs(score.mean()))\n    \nd = {'Modelling Algorithms':model_names, 'RMSLE':rmsle, 'Time(s)':time_lst}\n\nrmsle_frame=pd.DataFrame(d)\nrmsle_frame.sort_values(by='RMSLE')","1f6e4e07":"\"\"\"\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\n\nmodel = XGBRegressor(learning_rate=.1, subsample=1, min_child_weight=0.6)\nn_estimators = list(range(550, 1550, 50))\nprint(\"n_estimator: \", n_estimators)\nmax_depth = [7,9,11]\nprint(\"max_depth: \", max_depth)\n\n#param_grid = dict(n_estimators = n_estimators)\nparam_grid = dict(max_depth=max_depth, n_estimators=n_estimators)\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\ngrid_search = GridSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", cv=kfold, n_jobs=-1, verbose=3)\n\ngrid_result = grid_search.fit(X_train, y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n    \nresults = pd.DataFrame(grid_search.cv_results_)\nresults\n\"\"\"","ff4224bd":"\"\"\"\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold\nfrom sklearn.utils.fixes import loguniform\nfrom scipy.stats import uniform\nfrom xgboost import XGBRegressor\n\nmodel = XGBRegressor()\nn_estimators = list(range(50, 550, 50))\nmax_depth = list(range(5,16))\nlearning_rate = loguniform(1e-4, 1e0)\nsubsample= uniform(0,1)\nmin_child_weight = uniform(0,1)\n\n#param_grid = dict(n_estimators = n_estimators)\nparam_grid = {'n_estimators':n_estimators, 'max_depth':max_depth, 'learning_rate':learning_rate, \n              'subsample':subsample, 'min_child_weight': min_child_weight}\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\ngrid_search = RandomizedSearchCV(model, param_grid, scoring=\"neg_root_mean_squared_error\", n_iter=100, cv=kfold, n_jobs=-1, verbose=2)\n\ngrid_result = grid_search.fit(X, y)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))\n    \nresults = pd.DataFrame(grid_search.cv_results_)\nresults\n\"\"\"","a35fbe93":"#import sklearn\n#sklearn.metrics.SCORERS.keys()","982504ba":"datasetpath = '\/kaggle\/input\/seoul-bike-rental-ai-pro-iti\/'\n\ndf_test = pd.read_csv(os.path.join(datasetpath, 'test.csv'))\n\nprint(\"The shape of the dataset is {}.\".format(df_test.shape))","3220b03b":"df_test.head()","176f6f97":"sns.histplot(df_test, x='Humidity(%)')","54fc7443":"#there is an outlier in the humidity feature as there cant be a humidity of 0%\nfilter = df_test['Humidity(%)'] == 0\ndf_test.loc[filter]","912ca285":"def PreprocessingData(_df):\n    df = _df.copy()\n    \n    df[['Day', 'Month', 'Year']] = df[\"Date\"].str.split(\"\/\", expand = True).astype('int')\n    df[\"Date\"] = pd.to_datetime(df.Date, format=\"%d\/%m\/%Y\")\n    #extracting passed months since the date\n    #df['MonthsPassed'] = (date.today().year - df['Date'].dt.year) * 12 + date.today().month - df['Date'].dt.month  #cuz the rmsle increased\n    #extracting day name of the date\n    df['DayName'] = df['Date'].dt.day_name()\n    df.drop(columns='Date', inplace=True)\n    \n    #removing humidity outlier\n    df['Humidity(%)'].replace(to_replace=0, value=df['Humidity(%)'].median(), inplace=True)\n    \n    #add new feature yMedPerHourSeason\n    _list_ = []\n    for index, row in df.iterrows():\n        _list_.append(df_yMedPerHourSeason.loc[row['Hour'],row['Seasons']].values)\n    df['yMedPerHourSeason'] = np.log(np.array(_list_) + 1)      # shifthing data to log transformation\n    \n    #data enconding\n    \n    ###########################################\n    #             Label Encoding              #\n    ###########################################\n    #df['Seasons'] = df['Seasons'].astype('category')\n    #labelencoder = LabelEncoder()\n    #df['Seasons']= labelencoder.fit_transform(df['Seasons'])\n    #df['Seasons'] += 1\n    \n    ###########################################\n    #             One-hot Encoding            #\n    ###########################################\n    encoded_col = pd.get_dummies(df[['Seasons', 'DayName']])\n    df = df.join(encoded_col).drop(columns=['Seasons','DayName'])\n    \n    #Adding 1 for all values of label encoding\n    df['Holiday'] = np.where(df.Holiday == 'Holiday', 1, 0)\n    df['Functioning Day'] = np.where(df['Functioning Day'] == 'Yes', 1, 0)\n    return df","b2f3e79f":"print(df_test.shape)\ndf_test = PreprocessingData(df_test)\nprint(df_test.shape)","19216351":"#Using XGBoostRegressor\nX_test = df_test.drop(columns=['ID'])\n\ny_test_predicted = model_XGboost.predict(X_test)\n\ndf_test['y'] = np.round(np.exp(y_test_predicted) - 1) #to return the values of log(y+1) to y -> we perform the operation round(e^(log(y+1)) -1)\n\ndf_test.head()","184f4911":"df_test[['ID', 'y']].to_csv('submission.csv', index=False)","2220800a":"Turning yMedPerHourSeason to log transform log(value+1)","f0524de5":"Histogram for numerical features","134af11d":"Resource for feature engineering <br>\nhttps:\/\/towardsdatascience.com\/feature-engineering-for-machine-learning-3a5e293a5114","72f3581c":"Min max scalar for numerical data","eaf2345b":"**Remove outlier in humidity(%) equal zero as it is only one**","f3fcd242":"**Preprocessing Data**","77aed63c":"**HeatMap for numerical features**","d99a1d97":"Xgboost Model","49a01856":"**Evaluting over test data**","c9bc6ab7":"One-hot Encoding","f80c0486":"robust scalar for data normalization","7d45b7bf":"**Data Normalization** <br>\nhttps:\/\/developers.google.com\/machine-learning\/data-prep\/transform\/normalization","78858f32":"Label Encoding","248f7947":"comparing models using simple coding","10072203":"**Adding feature yMedPerHourSeason**","235b7b79":"Linear Regression Model","96a9b43e":"**Data Encoding**","931d2e0e":"Standard scalar (z-score) for numerical data","9a0ee772":"**Visualizing Data**","0407c2b6":"**Using cv score to train model**","3fafe995":"**Splitting Date to train and validation**","a54180a1":"**Check for outliers in humidity(%)**","03e04766":"**Use GridSearchCV for the model**","9b9155d2":"**Use random search for tuning the model**"}}