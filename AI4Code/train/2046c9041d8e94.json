{"cell_type":{"f69c9713":"code","de21eabe":"code","d230969b":"code","3da1f09b":"code","e6ea35f6":"code","a64759f7":"code","9985f987":"code","65b80460":"code","ec286449":"code","9ade736e":"code","adffd8b8":"code","1b589a97":"code","37bd009f":"code","44d00f46":"code","662d619b":"code","da7b5362":"code","cbab2868":"code","70647116":"code","e44e7867":"code","8ddad21c":"code","977d8bf8":"code","ad71683d":"code","458b52b4":"code","23de2691":"code","d61722ee":"code","c8d8f5f8":"code","60464560":"code","7b350538":"code","e2821077":"code","5079ce8b":"code","da4f1a87":"code","d7a25950":"code","c9aa2bc1":"code","f48f8ca3":"code","0372efb6":"code","ef4c5daf":"code","281c02d2":"code","97095f77":"code","8e3129f9":"code","48dff335":"code","afec5728":"code","884b9cb6":"code","a6beb9c3":"code","f2df5032":"markdown","ffc6fbc5":"markdown","71fcd32b":"markdown","bf4dfdf4":"markdown","58fadbef":"markdown","e98ff8b2":"markdown","33c085f5":"markdown","836693c1":"markdown","5393168d":"markdown","7a94ffdb":"markdown","c7718d3f":"markdown","d2811fa8":"markdown","36c96c12":"markdown","80f9371b":"markdown","b9c46412":"markdown","25627263":"markdown","1b2ad211":"markdown","a5c80a91":"markdown","a5f38d38":"markdown","81f27b87":"markdown","4366f149":"markdown","557f28c8":"markdown","3947b6e5":"markdown","e05b1bdc":"markdown","628746c5":"markdown","2a048528":"markdown","063c567f":"markdown","563b590d":"markdown","4941c544":"markdown","6ed8f390":"markdown","54d24efe":"markdown","3209dee2":"markdown"},"source":{"f69c9713":"! git clone https:\/\/github.com\/tueimage\/SE2CNN.git","de21eabe":"! pip install tensorflow-gpu==1.13.1","d230969b":"# Import tensorflow and numpy\nimport tensorflow as tf\nimport numpy as np\nimport math as m\nimport time\nimport glob\n\n# For validation\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\n# For plotting\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\n\n# Add the library to the system path\nimport os,sys\nse2cnn_source =  os.path.join(os.getcwd(),'\/kaggle\/working\/SE2CNN\/')\nif se2cnn_source not in sys.path:\n    sys.path.append(se2cnn_source)\n\n# Import the library\nimport se2cnn.layers","3da1f09b":"# help(se2cnn.layers.z2_se2n)\n# help(se2cnn.layers.se2n_se2n)\n# help(se2cnn.layers.spatial_max_pool)","e6ea35f6":"# Xavier's\/He-Rang-Zhen-Sun initialization for layers that are followed ReLU\ndef weight_initializer(n_in, n_out):\n    return tf.random_normal_initializer(mean=0.0, stddev=m.sqrt(2.0 \/ (n_in))\n    )","a64759f7":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","9985f987":"def size_of(tensor) :\n    # Multiply elements one by one\n    result = 1\n    for x in tensor.get_shape().as_list():\n         result = result * x \n    return result","65b80460":"CANCERtraindata = glob.glob('\/kaggle\/input\/histopathologiccancerwithcsv\/SplitData\/train80\/*.jpeg')\ntrain_data = np.array([np.array(Image.open(fname)) for fname in CANCERtraindata])\n\n# validation data\nCANCERtestdata = glob.glob('\/kaggle\/input\/histopathologiccancerwithcsv\/SplitData\/validation20\/*.jpeg')\neval_data = np.array([np.array(Image.open(fname)) for fname in CANCERtestdata])","ec286449":"''' To read CSV data into a record array in NumPy you can use NumPy modules genfromtxt() function, In this function\u2019s argument, you need to set the delimiter to a comma. '''\n# genfromtxt - function to create arrays from tabular data\n\nfrom numpy import genfromtxt\ntrain_labels_2 = genfromtxt('\/kaggle\/input\/histopathologiccancerwithcsv\/SplitData\/Label80.csv', delimiter=',')\ntrain_labels = train_labels_2[:,1]\neval_labels_2 = genfromtxt('\/kaggle\/input\/histopathologiccancerwithcsv\/SplitData\/Label20.csv', delimiter=',')\neval_labels = eval_labels_2[:,1]","9ade736e":"print(\" Length of train_data \")\nprint(len(train_data))\nprint(\" Length of eval_data \")\nprint(len(eval_data))\n\nprint(\" Length of train_labels \")\nprint(len(train_labels))\nprint(\" Length of eval_labels \")\nprint(len(eval_labels))\n\n#print(' Train data ')\n#print(train_data)\n#print(' Test data ')\n#print(eval_data)\n\n#print(' Train labels ')\n#print(train_labels)\n#print(' Test labels ')\n#print(eval_labels)\n","adffd8b8":"# Reshape to 2D multi-channel images\ntrain_data_2D = train_data.reshape([len(train_data),32,32,1]) # [batch_size, Nx, Ny, Nc]\neval_data_2D = eval_data.reshape([len(eval_data),32,32,1])","1b589a97":"# To know the information about the structure of train_data_2D\n# type\n#print(' Type ',type(train_data_2D))\n# Number of dimensions\n#print('Number of dimensions',train_data_2D.ndim)\n# shape\n#print('shape',train_data_2D.shape)\n# size\n#print('size',train_data_2D.size)\n# type of elements in it\n#print(' type of elements in it',train_data_2D.dtype)\n\n#print(' Train_data_2D ')\n#print(train_data_2D)\n#print(' eval_data_2D ')\n#print(eval_data_2D)","37bd009f":"# Plot the first sample \n\nplt.plot()\nplt.title(' Label : %d ' % train_labels[0])\nplt.imshow(train_data_2D[0,:,:,0], interpolation='nearest')\nplt.show()","44d00f46":"train_data_2D = np.pad(train_data_2D,((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=((0,0),(0,0),(0,0),(0,0)))\neval_data_2D = np.pad(eval_data_2D,((0,0),(2,2),(2,2),(0,0)),'constant', constant_values=((0,0),(0,0),(0,0),(0,0)))","662d619b":"#print(' Train_data_2D ')\n#print(train_data_2D)\n#print(' eval_data_2D ')\n#print(eval_data_2D)","da7b5362":"graph = tf.Graph()\ngraph.as_default()\n#tf.reset_default_graph()\ntf.compat.v1.reset_default_graph()","cbab2868":"Ntheta = 12 # Kernel size in angular direction\nNxy=5       # Kernel size in spatial direction\nNc = 4      # Number of channels in the initial layer","70647116":"#inputs_ph = tf.placeholder( dtype = tf.float32, shape = [None,36,36,1] )\n#labels_ph = tf.placeholder( dtype = tf.int32, shape = [None,] )\ninputs_ph = tf.compat.v1.placeholder( dtype = tf.float32, shape = [None,36,36,1] )\nlabels_ph = tf.compat.v1.placeholder( dtype = tf.int32, shape = [None,] )","e44e7867":"#print(' inputs_ph ')\n#print(inputs_ph)\n#print(' labels_ph ')\n#print(labels_ph)","8ddad21c":"tensor_in = inputs_ph\nNc_in = 1","977d8bf8":"kernels={}","ad71683d":"with tf.variable_scope(\"Layer_{}\".format(1)) as _scope:\n    ## Settings\n    Nc_out = Nc\n\n    ## Perform lifting convolution\n    # The kernels used in the lifting layer\n    kernels_raw = tf.get_variable(\n                        'kernel', \n                        [Nxy,Nxy,Nc_in,Nc_out],\n                        initializer=weight_initializer(Nxy*Nxy*Nc_in,Nc_out))\n    tf.add_to_collection('raw_kernels', kernels_raw)\n    bias = tf.get_variable( # Same bias for all orientations\n                        \"bias\",\n                        [1, 1, 1, 1, Nc_out], \n                        initializer=tf.constant_initializer(value=0.01))\n    # Lifting layer\n    tensor_out, kernels_formatted = se2cnn.layers.z2_se2n(\n                            input_tensor = tensor_in,\n                            kernel = kernels_raw,\n                            orientations_nb = Ntheta)\n    # Add bias\n    tensor_out = tensor_out + bias\n    \n    ## Perform (spatial) max-pooling\n    tensor_out = se2cnn.layers.spatial_max_pool( input_tensor=tensor_out, nbOrientations=Ntheta)\n    \n    ## Apply ReLU\n    tensor_out = tf.nn.relu(tensor_out)\n\n    ## Prepare for the next layer\n    tensor_in = tensor_out\n    Nc_in = Nc_out\n    \n    ## Save kernels for inspection\n    kernels[_scope.name] = kernels_formatted\n","458b52b4":"tensor_in.get_shape()","23de2691":"with tf.variable_scope(\"Layer_{}\".format(2)) as _scope:\n    ## Settings\n    Nc_out = 2*Nc\n\n    ## Perform group convolution\n    # The kernels used in the group convolution layer\n    kernels_raw = tf.get_variable(\n                        'kernel', \n                        [Nxy,Nxy,Ntheta,Nc_in,Nc_out],\n                        initializer=weight_initializer(Nxy*Nxy*Ntheta*Nc_in,Nc_out))\n    tf.add_to_collection('raw_kernels', kernels_raw)\n    bias = tf.get_variable( # Same bias for all orientations\n                        \"bias\",\n                        [1, 1, 1, 1, Nc_out], \n                        initializer=tf.constant_initializer(value=0.01))\n    # The group convolution layer\n    tensor_out, kernels_formatted = se2cnn.layers.se2n_se2n(\n                            input_tensor = tensor_in,\n                            kernel = kernels_raw)\n    tensor_out = tensor_out + bias\n    \n    ## Perform max-pooling\n    tensor_out = se2cnn.layers.spatial_max_pool( input_tensor=tensor_out, nbOrientations=Ntheta)\n    \n    ## Apply ReLU\n    tensor_out = tf.nn.relu(tensor_out)\n\n    ## Prepare for the next layer\n    tensor_in = tensor_out\n    Nc_in = Nc_out\n    \n    ## Save kernels for inspection\n    kernels[_scope.name] = kernels_formatted","d61722ee":"tensor_in.get_shape()","c8d8f5f8":"# Concatenate the orientation and channel dimension\ntensor_in = tf.concat([tensor_in[:,:,:,i,:] for i in range(Ntheta)],3)\nNc_in = tensor_in.get_shape().as_list()[-1]\n\n# 2D convolution layer\nwith tf.variable_scope(\"Layer_{}\".format(3)) as _scope:\n    ## Settings\n    Nc_out = 4*Nc\n\n    ## Perform group convolution\n    # The kernels used in the group convolution layer\n    kernels_raw = tf.get_variable(\n                        'kernel', \n                        [Nxy,Nxy,Nc_in,Nc_out],\n                        initializer=weight_initializer(Nxy*Nxy*Nc_in,Nc_out))\n    tf.add_to_collection('raw_kernels', kernels_raw)\n    bias = tf.get_variable( # Same bias for all orientations\n                        \"bias\",\n                        [1, 1, 1, Nc_out], \n                        initializer=tf.constant_initializer(value=0.01))\n    # Convolution layer\n    tensor_out = tf.nn.conv2d(\n                        input = tensor_in,\n                        filter=kernels_raw,\n                        strides=[1, 1, 1, 1],\n                        padding=\"VALID\")\n    tensor_out = tensor_out + bias\n    \n    ## Apply ReLU\n    tensor_out = tf.nn.relu(tensor_out)\n\n    ## Prepare for the next layer\n    tensor_in = tensor_out\n    Nc_in = Nc_out\n    \n    ## Save kernels for inspection\n    kernels[_scope.name] = kernels_raw","60464560":"tensor_in.get_shape()","7b350538":"# 2D convolution layer\nwith tf.variable_scope(\"Layer_{}\".format(4)) as _scope:\n    ## Settings\n    Nc_out = 128\n\n    ## Perform group convolution\n    # The kernels used in the group convolution layer\n    kernels_raw = tf.get_variable(\n                        'kernel', \n                        [1,1,Nc_in,Nc_out],\n                        initializer=weight_initializer(1*1*Nc_in,Nc_out))\n    tf.add_to_collection('raw_kernels', kernels_raw)\n    bias = tf.get_variable( # Same bias for all orientations\n                        \"bias\",\n                        [1, 1, 1, Nc_out], \n                        initializer=tf.constant_initializer(value=0.01))\n    # Convolution layer\n    tensor_out = tf.nn.conv2d(\n                        input = tensor_in,\n                        filter=kernels_raw,\n                        strides=[1, 1, 1, 1],\n                        padding=\"VALID\")\n    tensor_out = tensor_out + bias\n    \n    ## Apply ReLU\n    tensor_out = tf.nn.relu(tensor_out)\n\n    ## Prepare for the next layer\n    tensor_in = tensor_out\n    Nc_in = Nc_out\n    \n    ## Save kernels for inspection\n    kernels[_scope.name] = kernels_raw\n","e2821077":"tensor_in.get_shape()","5079ce8b":"with tf.variable_scope(\"Layer_{}\".format(5)) as _scope:\n    ## Settings\n    Nc_out = 2\n\n    ## Perform group convolution\n    # The kernels used in the group convolution layer\n    kernels_raw = tf.get_variable(\n                        'kernel', \n                        [1,1,Nc_in,Nc_out],\n                        initializer=weight_initializer(1*1*Nc_in,Nc_out))\n    tf.add_to_collection('raw_kernels', kernels_raw)\n    bias = tf.get_variable( # Same bias for all orientations\n                        \"bias\",\n                        [1, 1, 1, Nc_out], \n                        initializer=tf.constant_initializer(value=0.01))\n\n    \n    ## Convolution layer\n    tensor_out = tf.nn.conv2d(\n                        input = tensor_in,\n                        filter=kernels_raw,\n                        strides=[1, 1, 1, 1],\n                        padding=\"VALID\")\n    tensor_out = tensor_out + bias\n    \n    ## The output logits\n    logits = tensor_out[:,0,0,:]\n    predictions = tf.argmax(input=logits, axis=1)\n    probabilities = tf.nn.softmax(logits)\n    \n    ## Save the kernels for later inspection\n    kernels[_scope.name] = kernels_raw\n","da4f1a87":"logits.get_shape()","d7a25950":"# Cross-entropy loss\nloss = tf.losses.sparse_softmax_cross_entropy(labels=labels_ph, logits=logits)","c9aa2bc1":"#-- Define the l2 loss \nweightDecay=5e-4\n# Get the raw kernels\nvariables_wd = tf.get_collection('raw_kernels')\nprint('-----')\nprint('RAW kernel shapes:')\nfor v in variables_wd: print( \"[{}]: {}, total nr of weights = {}\".format(v.name, v.get_shape(), size_of(v)))\nprint('-----')\nloss_l2 = weightDecay*sum([tf.nn.l2_loss(ker) for ker in variables_wd])","f48f8ca3":"# Configure the Training Op (for TRAIN mode)\noptimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n\ntrain_op = optimizer.minimize(\n    loss=loss + loss_l2,\n    global_step=tf.train.get_global_step())","0372efb6":"#-- Start the (GPU) session\ninitializer = tf.global_variables_initializer()\nsession = tf.Session(graph=tf.get_default_graph()) #-- Session created\nsession.run(initializer)","ef4c5daf":"batch_size=100\nn_epochs=10","281c02d2":"for epoch_nr in range(n_epochs):\n    loss_average = 0\n    data = train_data_2D\n    labels = train_labels\n    # KBatch settings\n    NItPerEpoch = m.floor(len(data)\/batch_size) #number of iterations per epoch\n    samples=np.random.permutation(len(data))\n    # Loop over dataset\n    tStart = time.time()\n    for iteration in range(NItPerEpoch):\n        feed_dict = {\n                inputs_ph: np.array(data[samples[iteration*batch_size:(iteration+1)*batch_size]]),\n                labels_ph: np.array(labels[samples[iteration*batch_size:(iteration+1)*batch_size]])\n                }\n        operators_output = session.run([ loss , train_op ], feed_dict)\n        loss_average += operators_output[0]\/NItPerEpoch\n    tElapsed = time.time() - tStart\n    print('Epoch ' , epoch_nr , ' finished... Average loss = ' , round(loss_average,4) , ', time = ',round(tElapsed,4))","97095f77":"batch_size = 5\nlabels_pred = []\nfor i in range(round(len(eval_data_2D)\/batch_size)):\n    [ labels_pred_batch ] = session.run([ predictions ], { inputs_ph: eval_data_2D[i*batch_size:(i+1)*batch_size] })\n    labels_pred = labels_pred + list(labels_pred_batch)\nlabels_pred = np.array(labels_pred)","8e3129f9":"print(' Compare the first 10 results with the ground truth ')\nprint(' The first 10 predicted results ')\nprint(labels_pred[0:10])\nprint(' The first 10 ground truth (actual results) ')\nprint(eval_labels[0:10])\n","48dff335":"print(' The accuracy (average number of successes) ')\nprint(((labels_pred - eval_labels)**2==0).astype(float).mean())","afec5728":"print(' Total number of errors ')\nprint(((labels_pred - eval_labels)**2>0).astype(float).sum())","884b9cb6":"print(' Error rate ')\nprint(100*((labels_pred - eval_labels)**2>0).astype(float).mean())","a6beb9c3":"cm = confusion_matrix(eval_labels, labels_pred)\nplot_confusion_matrix(cm, range(1))","f2df5032":"Plot a confusion matrix to see what kind of errors are made","ffc6fbc5":"**Confusion matrix plot**","71fcd32b":"# Part 4: Train and test the G-CNN\n\n# Begin session ","bf4dfdf4":"Total nr of errors","58fadbef":"# Optimization ","e98ff8b2":"# Layer 4: Fully connected layer (1x1) ","33c085f5":"# Validation ","836693c1":"By default the data is formatted as flattened arrays. Here were format them as 2D feature maps (with only 1 channel)","5393168d":"Placeholders","7a94ffdb":"Useful functions\n\nThe se2cnn layers\n\nFor useage of the relevant layers defined in se2cnn.layers uncomment and run the following:","c7718d3f":"Error rate","d2811fa8":"# Prepare for the first layer","36c96c12":"# Reference : Bekkers, E., Lafarge, M., Veta, M., Eppenhof, K., Pluim, J., Duits, R.: Roto-translation covariant convolutional networks for medical image analysis. Accepted at MICCAI 2018, arXiv preprint arXiv:1804.03393 (2018). Available at: https:\/\/arxiv.org\/abs\/1804.03393\n\nhttps:\/\/github.com\/tueimage\/SE2CNN","80f9371b":"# About the dataset","b9c46412":"**Size of a tf tensor**","25627263":"# Define the loss and the optimizer ","1b2ad211":"We would like to have the patches to be of size 36x36 such that we can reduce it to 1x1 via 5x5 convolutions and max pooling layers. So, here we pad the images on the left and right with zeros.","a5c80a91":"# Layer 2: SE2-conv, max-pool, relu","a5f38d38":"# Layer 3: 2D fully connected layer (5x5) ","81f27b87":"# Layer 5: Fully connected (1x1) to output ","4366f149":"# Part 3: Build a graph (design the G-CNN)\n\n**Build a graph**","557f28c8":"\nSave the kernels to a library for later inspection","3947b6e5":"The accuracy (average nr of successes)","e05b1bdc":"Loop over the input stack in batch of size \"batch_size\".","628746c5":"# Layer 1: The lifting layer ","2a048528":"# Library used:  The SE(2) group convolutional neural network library\n","063c567f":"Part 2: Load and format the dataset","563b590d":"Settings\n\nKernel size and number of orientations","4941c544":"Compare the first 10 results with the ground truth","6ed8f390":"# Part 1: Load the libraries","54d24efe":"Weight initialization\nFor initialization we use the initialization method for ReLU activation functions as proposed in:\n\nHe, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778).","3209dee2":"In each epoch we pass over all input samples in batch sizes of batch_size"}}