{"cell_type":{"c7e6bf5f":"code","a6c9926c":"code","b5ff6a4e":"code","fa7316b5":"code","af21cab7":"code","4b6482a3":"code","f5f3073b":"code","74511e03":"code","6839a733":"code","6c81dd81":"code","e6e2e963":"code","5031f92d":"code","f72ede27":"code","08081955":"code","5888e768":"code","649de558":"code","4daecca5":"code","38deeda6":"code","1ea77ce8":"code","e8f2ab01":"code","72b9e4ea":"code","7160d251":"code","b1892dde":"code","5669dbab":"code","3d9776d0":"code","cbbcc138":"code","1bf9b9ec":"code","459d8daf":"code","32be5430":"code","c6a9ec91":"code","9e65e0c9":"code","714e6c55":"code","7e18bd51":"code","503b0cb2":"code","01c79315":"code","7d9c54a7":"code","bfa75de9":"code","18195f80":"code","1839c322":"code","d84536d1":"code","44fa83e9":"code","8a306b34":"code","0517349d":"code","f501f8d7":"markdown","7049b29c":"markdown","026ca34a":"markdown","c9da42fd":"markdown","91216a71":"markdown","de58c64c":"markdown","79e6184f":"markdown","07c4c58b":"markdown","4ea71f78":"markdown","89752e58":"markdown","50ec84c9":"markdown","492503d5":"markdown","20421c40":"markdown","14084863":"markdown","6ba8942e":"markdown","6ef3f0b8":"markdown","618b928e":"markdown","0d403704":"markdown","35671a38":"markdown","f46291d3":"markdown","4006df4f":"markdown","ae743837":"markdown","b051578f":"markdown","3731e509":"markdown","e9c490c5":"markdown","f6d74ac0":"markdown","95e99896":"markdown","c3d3aeba":"markdown","436f25dd":"markdown","e808629f":"markdown","4afd7f8d":"markdown","e34211b6":"markdown","f24a44c5":"markdown","6248abd7":"markdown","d1a4070d":"markdown","cc22044f":"markdown","cacfb4b8":"markdown","01b5ca82":"markdown","bda44e50":"markdown","bfa5e0a2":"markdown","d723d9b5":"markdown","c8827a7f":"markdown","cc2f6a1f":"markdown","317a89fc":"markdown","9e389c5d":"markdown","c4c01aa7":"markdown","7ac56819":"markdown","6302b991":"markdown","5698587b":"markdown","d656fdde":"markdown","e0c6c270":"markdown","1884c635":"markdown","c5ad1493":"markdown"},"source":{"c7e6bf5f":"import warnings\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import skew\nfrom math import sqrt\n\n# sklearn tools\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\n# Mute warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a6c9926c":"data_dir = Path(\"..\/input\/house-prices-advanced-regression-techniques\")\ndata = pd.read_csv(data_dir \/ \"train.csv\", index_col=\"Id\")\n\n# figure out a way to test this, then this model will be good!!!!\ntest = pd.read_csv(data_dir \/ \"test.csv\", index_col=\"Id\")","b5ff6a4e":"data.head(7)","fa7316b5":"fig, ax = plt.subplots()\nax.scatter(data['GrLivArea'], data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","af21cab7":"# Looks like the only outliers are around more than 4000 GrLivArea and less than $300000 SalePrice\n\ndata = data.drop(data[(data['GrLivArea']>4000) & (data['SalePrice']<300000)].index)\n\nfig, ax = plt.subplots()\nax.scatter(data['GrLivArea'], data['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","4b6482a3":"sns.distplot(data['SalePrice']);","f5f3073b":"# Its a bit skewed, and for linear models its important that you normalize your data, so im going to log transform it\n\ndata['SalePrice'] = np.log1p(data['SalePrice'])\nsns.distplot(data['SalePrice'])","74511e03":"#creating target\ntarget = data['SalePrice']\ndata= data.drop(axis=1, columns='SalePrice')","6839a733":"print(data.shape)","6c81dd81":"# Splitting all Categorical variables into Ordinal and Nominal\n\n# I may have missed some of the ordinal variables, that is my fault if i did\nordinal = ['MSZoning', 'LotShape', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'HeatingQC', 'CentralAir',\n           'KitchenQual', 'GarageQual', 'GarageCond', 'PoolQC']\n\nprint('# of Ordinal Categoricals: ' + str(len(ordinal)))\n\n\nnominal = [var for var in data.columns if data[var].dtype=='O' and var not in ordinal]\n\nprint('# of Nominal Categoricals: ' + str(len(nominal)))","e6e2e963":"# Grabbing all the features that are related to a year, i will be using these later on\n\nyear_vars = [var for var in data.columns if 'Yr' in var or 'Year' in var]\n\nyear_vars","5031f92d":"# Splitting all Numerical features into Discrete and Continuous,\n\ndiscrete = [var for var in data.columns if data[var].dtype != 'O'\n            # the largest discrete feature 'MSSubClass' has 16 variables\n            and len(data[var].unique()) < 17\n            and var not in year_vars\n            # 'PoolArea' is not discrete, it just has a small # of values\n            and var not in ['PoolArea']\n            ]\n\nprint('# of discrete Numerical Features: ', len(discrete))\n\n\nnumerical = [var for var in data.columns if data[var].dtype != 'O'\n             and var not in discrete\n             and var not in year_vars\n             and var not in ['Id', 'SalePrice']\n            ]\n\nprint('# of Continuous Numerical Features: ', len(numerical))","f72ede27":"# Locating and taking care of missing values\n\n# Displaying Percentage of missing values relative to the specific feature\ndef missing_var(x):\n    for var in x:\n        if data[var].isnull().sum() > 0:\n            print(var, ': ', data[var].isnull().mean())\n\n\nprint('Missing Ordinal Categorical Features: ')\nmissing_var(ordinal)\nprint()\n            \nprint('Missing Nominal Categorical Features: ')\nmissing_var(nominal)\nprint()\n\nprint('Missing Discrete Features: ')\nmissing_var(discrete)\nprint()\n\nprint('Missing Numerical Features: ')\nmissing_var(numerical)\nprint()\n\nprint('Missing year_vars Features: ')\nmissing_var(year_vars)\n","08081955":"# Categorical Features missing more than 5% of its values\noverFivePerc_cat = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature']\n\n# Numerical Features missing more than 5% of its values\noverFivePerc_num = ['LotFrontage']\n\n    \ndef impute_missing_vars(df):\n    # Converting all the categorical features with less than 5% of its data missing into the\n    # most frequent value\n    for var in df[ordinal]:\n        df[var] = df[var].fillna(data[var].mode()[0])\n        \n    for var in df[nominal]:\n        df[var] = df[var].fillna(data[var].mode()[0])\n\n    \n    # Converting all the missing categorical features above 5% into a new value 'None'\n    for var in df[overFivePerc_cat]:\n        df[var] = df[var].fillna('None')\n\n    \n    # Converting all the discrete\/numerical features with less than 5% of its data missing into\n    # the median value, assuming most are skewed\n    for var in df[discrete]:\n        df[var] = df[var].fillna(data[var].median())\n    \n    for var in df[numerical]:\n        df[var] = df[var].fillna(data[var].median())\n\n    \n    # Converting all the missing numerical features over 5% to 0, representing none\n    for var in df[overFivePerc_num]:\n        df[var] = df[var].fillna(0)\n\n    # Finally for the missing year_vars im just going to replace them with the median\n    for var in df[year_vars]:\n        df[var] = df[var].fillna(data[var].median())\n    \n\n    \nimpute_missing_vars(data)\n","5888e768":"print(\"# of missing data values : {}\".format(data.isna().sum()))","649de558":"\ndef elapsed_years(df, var):\n    # capture difference between year variable and\n    # year the house was sold\n    df[var] = df['YrSold'] - df[var]\n    return df\n\nfor var in ['YearBuilt', 'YearRemodAdd', 'GarageYrBlt']:\n    data = elapsed_years(data, var)\n","4daecca5":"# drop YrSold\ndata.drop('YrSold', axis=1, inplace=True)\nyear_vars.remove('YrSold')","38deeda6":"data[year_vars].head()","1ea77ce8":"# checking what numerical features to log transform\n\nfor var in numerical:\n    plt.figure(figsize=(4,2))\n    fig = data[var].hist(bins=60)\n    fig.set_ylabel('number of houses')\n    fig.set_xlabel(var)","e8f2ab01":"# features that need to be logged\nnumToLog = ['LotFrontage','TotalBsmtSF', '1stFlrSF', 'GrLivArea', 'GarageArea']\n\ndef logTransform(df, col):\n    for col in df[numToLog]:\n        df[col] = np.log1p(df[col])\n\nlogTransform(data, numToLog)","72b9e4ea":"data[numToLog]","7160d251":"# I would use Equal Frequency Discretization as it would normalize the data and be better for linear models, \n# but there are to many zeros in some of the variables,\n# so im just going to use Equal Width Discretization instead\n\n\n# numerical features to discretize\nvars_to_discret = [var for var in numerical if var not in numToLog]\n\n\n# equal width\ndef discretize(df, list):\n    for col in list:\n        \n        # need to find the number of bins to split each variable into\n        \n        \n        # getting the range of each variable\n        ageRange = df[col].max() - df[col].min()\n        \n        # grabbing the floored min and max of each variable\n        minValue = int(np.floor( df[col].min()))\n        maxValue = int(np.ceil( df[col].max()))\n\n        # rounding the bin width\n        interValue = int(np.round(ageRange \/ 8))\n        \n        # finding the amount of bins for each variable\n        nBins = [i for i in range(minValue, maxValue + interValue, interValue)]\n        \n        # creating labels\n        labels = ['Bin_' + str(i) for i in range(1, len(nBins))]\n        \n        # cutting each variable into bins\n        df[col] = pd.cut(x=df[col], bins=nBins, labels=labels, include_lowest=True)\n\n        \ndiscretize(data, vars_to_discret)\n","b1892dde":"for var in vars_to_discret:\n    plt.figure(figsize=(4,2))\n    fig = data[var].hist(bins=60)\n    fig.set_ylabel('number of houses')\n    fig.set_xlabel(var)","5669dbab":"data[vars_to_discret]","3d9776d0":"from collections import defaultdict\nfrom sklearn.preprocessing import LabelEncoder\n\nd = defaultdict(LabelEncoder)\n\n# only want to labelEncode nominal, ordinal, and discretized variables!\n\n# fitting to train set\ndata[ordinal] = data[ordinal].apply(lambda x: d[x.name].fit_transform(x))\ndata[nominal] = data[nominal].apply(lambda x: d[x.name].fit_transform(x))\ndata[vars_to_discret] = data[vars_to_discret].apply(lambda x: d[x.name].fit_transform(x))","cbbcc138":"data","1bf9b9ec":"from sklearn.feature_selection import VarianceThreshold\n\n# fitting the threshold to all features of that data,\n# VarianceThreshold finds all features with values taking 95% and above of the feature space \nsel = VarianceThreshold(threshold=0.05)  \nsel.fit(data)\n\n# finding which features have values over the threshold\nquasiConstant = data.columns[~sel.get_support()]\nprint('Features with 1 value taking up 95% or more of the feature space: ', quasiConstant)\n\n#capture feature names of variables we want to keep\nfeatNames = data.columns[sel.get_support()]\n\n#removing the unwanted features\ndata = sel.transform(data)\n\n# trasnforming back into dataframe\ndata = pd.DataFrame(data, columns=featNames)\n\ndata.shape","459d8daf":"# Creating New Features Based on Similarities\n\ndata['NeighborhoodLF'] = data.groupby('Neighborhood')['LotFrontage'].transform('median') \n\ndata['NeighborhoodQual_Cond'] = data.groupby('Neighborhood')['OverallQual'].transform('median') \n\ndata['NeighborhoodHS'] = data.groupby('Neighborhood')['HouseStyle'].transform('median')\n\ndata['TotalBath'] = data['BsmtFullBath'] + (0.5 * data['BsmtHalfBath']) + \\\n                     data['FullBath'] + (0.5 * data['HalfBath'])\n\n\n#creating the new features produces missing values, so im just going to impute the median value for each\nRandomNewVariables = ['NeighborhoodLF', 'NeighborhoodQual_Cond', 'NeighborhoodHS', 'TotalBath']\nfor var in data[RandomNewVariables]:\n        data[var] = data[var].fillna(data[var].median())\n        \ndata.shape","32be5430":"# visualizing the correlations\ncorrmat = data.corr(method='pearson')\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\nfig, ax = plt.subplots()\nfig.set_size_inches(11,11)\nsns.heatmap(corrmat, cmap=cmap)","c6a9ec91":"def correlated(df, threshold):\n    # create the correlation matrix\n    corrMatrix = df.corr()\n    \n    # for each feature in the dataset (columns of the correlation matrix)\n    for i in range(len(corrMatrix.columns)):\n        \n        # check with other features\n        for j in range(i):\n            \n            # if the correlation is higher than a certain threshold\n            if abs(corrMatrix.iloc[i, j]) >= threshold:\n                \n                # print correlation, and variables examined\n                # keep in mind that the columns and rows of the dataframe are identical\n                # so we can identify the features being examned by looking for i,j\n                # in the column names\n                print(abs(corrMatrix.iloc[i, j]), corrMatrix.columns[i], corrMatrix.columns[j])\n                \n\ncorrFeatures = correlated(data, 0.8)","9e65e0c9":"# Exterior1st and Exterior2nd are quite similar, im just going to create a new feature using both of them and then remove both of them\ndata['AllExterior'] = data['Exterior1st'] * data['Exterior2nd']\n\n# TotRmsAbvGrd and GrLivArea i assume are correlated because the total rooms above ground is a big factor\n# when it comes to measuring the general living area, Im just going to remove TotRmsAbvGrd because that is less relavant\n\n# Correlated features to drop\nfeaturesToDrop = ['Exterior1st', 'Exterior2nd', 'TotRmsAbvGrd']\ndata.drop(labels=featuresToDrop, axis=1, inplace=True)\n\n#creating the new feature produces missing values, so im just going to impute the median value for each\nCorrNewVariables = ['AllExterior']\nfor var in data[CorrNewVariables]:\n        data[var] = data[var].fillna(data[var].median())\n\ndata.shape","714e6c55":"# Handle remaining missing values for features by using median as replacement\nprint(\"NA features in data : \" + str(data.isnull().values.sum()))","7e18bd51":"from sklearn.feature_selection import mutual_info_regression\n\ndef make_mi_scores(X, y):\n    mi_scores = mutual_info_regression(X, y)\n    mi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=X.columns)\n    mi_scores = mi_scores.sort_values(ascending=False)\n    return mi_scores\n\nmi_scores = make_mi_scores(data, target)\nmi_scores[::3]  # show a few features with their MI scores","503b0cb2":"\ndata['OverallQual_w_GLA'] = data['OverallQual'] * data['GrLivArea']\n\ndata['overallQuall_w_KQ'] = data['OverallQual'] * data['KitchenQual']\n\ndata['FullBath_up_down'] = data['FullBath'] * data['BsmtFullBath']","01c79315":"data.shape","7d9c54a7":"Xtrain, Xpred, ytrain, ypred = train_test_split(data, target, test_size = 0.3, random_state = 0)\nprint(\"Xtrain : \" + str(Xtrain.shape))\nprint(\"Xpred : \" + str(Xpred.shape))\nprint(\"ytrain : \" + str(ytrain.shape))\nprint(\"ypred : \" + str(ypred.shape))","bfa75de9":"# standardizing the datasets\n\nscaler = StandardScaler()\nscaler.fit(Xtrain)\n\n#trainScaled = scaler.transform(Xtrain)\n#predScaled = scaler.transform(Xpred)\n\n#trainScaled = pd.DataFrame(trainScaled, columns=Xtrain.columns)\n#predScaled = pd.DataFrame(predScaled, columns=Xpred.columns)\n\ntrainScaled = scaler.transform(Xtrain)\npredScaled = scaler.transform(Xpred)\n\nXtrain = pd.DataFrame(trainScaled, columns=Xtrain.columns)\nXpred = pd.DataFrame(predScaled, columns=Xpred.columns)\n","18195f80":"def rmse_cv(model, df, target):\n    rmse = np.sqrt(-cross_val_score(model, df, target, scoring = \"neg_mean_squared_error\", cv = 10))\n    return(rmse)\n\ndef r2_cv(model, df, target):\n    r2 = cross_val_score(model, df, target, scoring = 'r2', cv = 10)\n    return(r2)","1839c322":"lr = LinearRegression()\nlr.fit(Xtrain, ytrain)\n\n# Look at predictions on training and validation set\nprint()\nprint(\"RMSE on Training set :\", rmse_cv(lr, Xtrain, ytrain).mean())\nprint(\"R2 on Training set :\", r2_cv(lr, Xtrain, ytrain).mean())\nprint(\"RMSE on Test set :\", rmse_cv(lr, Xpred, ypred).mean())\nprint(\"R2 on Test set :\", r2_cv(lr, Xpred, ypred).mean())\nprint()\n\nlr_train = lr.predict(Xtrain)\nlr_pred = lr.predict(Xpred)","d84536d1":"# Plot residuals\nplt.scatter(lr_train, lr_train - ytrain, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(lr_pred, lr_pred - ypred, c = \"green\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(lr_train, ytrain, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(lr_pred, ypred, c = \"green\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","44fa83e9":"from sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import GridSearchCV\n\nlasso = Lasso()\n\na_estimators_lasso = [0.01, 0.06, 0.1, 0.3, 0.6, 1, 3, 10, 20, 40, 60, 100, 200]\nparam_grid_lasso = {'alpha': a_estimators_lasso}\n\ngbc_lasso = GridSearchCV(lasso, param_grid_lasso, cv=10)\ngbc_lasso.fit(Xtrain, ytrain)\n\n# Look at predictions on training and validation set\nprint()\nprint(\"RMSE on Training set :\", rmse_cv(gbc_lasso, Xtrain, ytrain).mean())\nprint(\"R2 on Training set :\", r2_cv(gbc_lasso, Xtrain, ytrain).mean())\nprint(\"RMSE on Test set :\", rmse_cv(gbc_lasso, Xpred, ypred).mean())\nprint(\"R2 on Test set :\", r2_cv(gbc_lasso, Xpred, ypred).mean())\nprint()\n\nlasso_train = gbc_lasso.predict(Xtrain)\nlaso_pred = gbc_lasso.predict(Xpred)","8a306b34":"from sklearn.linear_model import Ridge\n\n\nridge = Ridge()\n\na_estimators_ridge = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60]\nparam_grid_ridge = {'alpha': a_estimators_ridge}\n\ngbc_ridge = GridSearchCV(ridge, param_grid_ridge, cv=10)\ngbc_ridge.fit(Xtrain, ytrain)\n\n\n\n# Look at predictions on training and validation set\nprint()\nprint(\"RMSE on Training set :\", rmse_cv(gbc_ridge, Xtrain, ytrain).mean())\nprint(\"R2 on Training set :\", r2_cv(gbc_ridge, Xtrain, ytrain).mean())\nprint(\"RMSE on Test set :\", rmse_cv(gbc_ridge, Xpred, ypred).mean())\nprint(\"R2 on Test set :\", r2_cv(gbc_ridge, Xpred, ypred).mean())\nprint()\n\nridge_train = gbc_ridge.predict(Xtrain)\nridge_pred = gbc_ridge.predict(Xpred)","0517349d":"from sklearn.linear_model import ElasticNet\n\neNet = Ridge()\n\na_estimators_eNet = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60, 100, 200]\nparam_grid_eNet = {'alpha': a_estimators_eNet}\n\ngbc_eNet = GridSearchCV(eNet, param_grid_eNet, cv=10)\ngbc_eNet.fit(Xtrain, ytrain)\n\n\n# Look at predictions on training and validation set\nprint()\nprint(\"RMSE on Training set :\", rmse_cv(gbc_eNet, Xtrain, ytrain).mean())\nprint(\"R2 on Training set :\", r2_cv(gbc_eNet, Xtrain, ytrain).mean())\nprint(\"RMSE on Test set :\", rmse_cv(gbc_eNet, Xpred, ypred).mean())\nprint(\"R2 on Test set :\", r2_cv(gbc_eNet, Xpred, ypred).mean())\nprint()\n\neNet_train = gbc_eNet.predict(Xtrain)\neNet_pred = gbc_eNet.predict(Xpred)","f501f8d7":"I will be testing the models using Root Mean Squared Error and R-Squared. Ideally, lower RMSE and higher R2 values are indicative of a good model, but you should also take into consideration what your model is testing.","7049b29c":"### Creating\/Removing Variables Based on there Correlations","026ca34a":"Some features have only one value taking up most of its feature space, this is not good for linear models, so ill be removing them","c9da42fd":"Good feature subsets contain features highly correlated with the target, and uncorrelated with each other.\nIm going to be making some new variables with the correlations and removing one of the correlated features\nto reduce correlation with other features\n","91216a71":"When a variable is highly cardinal, some categories may only land on the train set which can lead to over-fitting, or only on the test set in which case the model wouldnt know what to do with them, another problem is the variables skewness and outliers Discretization takes care of all of these problems, I will be using an equal width discretizer for the remaining numerical variables","de58c64c":"**Theres a lot of Numerical featuers with a high percentage of only 1 value, I will be taking care of these in later sections!**","79e6184f":"## Taking care of skewed target","07c4c58b":"Lasso Regression is a modification of linear regression. In Lasso, the loss function is modified to minimize the complexity of the model by limiting the sum of the absolute values of the model coefficients (also called the L1-norm).\n\n\nThe loss function for Lasso Regression can be expressed like this:\n\n**Loss function = OLS + alpha * summation (absolute values of the magnitude of the coefficients)**\n\nAlpha is the penalty parameter you need to select\n\n> Using an L1 norm constraint forces some weight values to zero to allow other coefficients to take non-zero values. ","4ea71f78":"> #### My Notebooks ML Process\n> \n> 1. Deal with Outliers\n> 2. Impute missing Values\n> 3. Label Encode\/One Hot encode variables\n> 4. Apply Dimensionality Reduction\n> 5. Feature Selection\n> 6. Apply Models and Test\n","89752e58":"### Linear Regression","50ec84c9":"### Log Transform Numerical Features that need it","492503d5":"Now it shows # of years from the year it was sold, instead of a specific year","20421c40":"#### Splitting data into more understandable categories","14084863":"# My First Project","6ba8942e":"You can create a lot of new features from the features given that could potentially help the model and its performance, im just going to make a few","6ef3f0b8":"### Lasso Regression","618b928e":"### Correlated Variables","0d403704":"Perfect, all the missing values are taken care of","35671a38":"Feature scaling (min\/max, mean\/stdev) is for numerical values so it doesn't matter if its done before or after label encoding; but keep it in mind that you SHOULD NOT do scaling on encoded categorical features.\n\nFor dimensionality reduction or feature selection, you need to have numerical values; so you should do them after label encoding.","f46291d3":"## Handle Missing Values","4006df4f":"Also, if you are building linear models, the bins may not necessarily hold a linear relationship with the target. In this case, it may help improve model performance to treat the bins as categories and encode them, This will be done when i get to encoding","ae743837":"One last thing before moving on to encoding, i need to calculate elapsed time for year_vars, this makes it easier for encoding and processing as i dont need to deal with them as dates","b051578f":"### This notebook will focus on feature engineering and feature selection, and will be tested on some of the most common linear models, Linear, Lasso, Ridge, and Elastic Net Regression at the end.","3731e509":"Im going to split them into categorical and numerical lists respectivly, and also grap all the categories dealing with years for further editing","e9c490c5":"## Splitting into Train and Test set","f6d74ac0":"### Making predictions of the test dataset","95e99896":"#### Feature Selection Summary","c3d3aeba":"## Encoding","436f25dd":"### Constant and Quasi-Constand variables","e808629f":"**I know its prefered that you OneHotEncode when using Linear Models, but I wasnt sure if applying feature selection on OneHotEncoded data was the correct thing to do, so im just going to use Label Encoding for this notebook.**","4afd7f8d":"### Ridge Regression","e34211b6":"### Label Encoding","f24a44c5":"Ridge regression is an extension of linear regression where the loss function is modified to minimize the complexity of the model. This modification is done by adding a penalty parameter that is equivalent to the square of the magnitude of the coefficients. \n\nThe loss function for Ridge Regression can be expressed like this:\n\n**Loss function = OLS + alpha * summation (squared coefficient values)**\n\nAlpha is the parameter you need to select. \n\n> Be cautious, A low alpha value can lead to over-fitting, whereas a high alpha value can lead to under-fitting.","6248abd7":"### Creating New Features Based on the Mutual Information","d1a4070d":"#### That is all for this Notebook, I know its a bit messy but im still learning and playing around with everything. If you read it all the way through and notice any mistakes or errors in my code id love some feedback, Thankyou!","cc22044f":"#### **Im fairly confident Ive made some mistakes. I also think the order of my feature engineering and selection process might be mixed up a bit, so if you read this and notice something wrong any suggestions would be of great help to me!**","cacfb4b8":"### Discretization","01b5ca82":"### Elastic Net Regression","bda44e50":"## Wrap Up ","bfa5e0a2":"## Fitting Model and  Making Predictions","d723d9b5":"### Mutual Information","c8827a7f":"ElasticNet combines the properties of both Lasso and Ridge regression. It works by penalizing the model using both the L1-norm and the L2-norm","cc2f6a1f":"## Feature Selection","317a89fc":"Seems like there were only 8 Constant\/Quasi-Constant features, I just removed them all","9e389c5d":"## Taking Care of Outliers","c4c01aa7":"## Feature Engineering","7ac56819":"**It seems like they all tested fairly well, although i have a feeling im overfitting haha. Im thinking i probably shouldve split the dataset at the start of the notebook so that the train and test data would have been seperate during feature engineering and selection, im not quite sure though. If this is the case and you notice something off please let me now!**","6302b991":"**In general if you dont know why the data is missing its best to only convert data into mean\/median if it is 5% or less of the existing value space in the feature, otherwise just use a missing idicator. This prevents the model from making false assumptions about the data that was not there**","5698587b":"This is the simplist form of regression.\n\nThe Linear Regression equation can be expressed like this:\n\n**y = a1x1 + a2x2 + a3x3 + ..... + anxn + b**\n\n*     y is the target variable.\n*     x1, x2, x3,...xn are the features.\n*     a1, a2, a3,..., an are the coefficients.\n*     b is the parameter of the model. \n\n> The parameters a and b are selected through the Ordinary least squares (OLS) method. It works by minimizing the sum of squares of residuals (actual value - predicted value).","d656fdde":"### Random New Features","e0c6c270":"**After the Feature Selection Process we are left with 75 features, we dropped 11 features, and added 8 new features.**","1884c635":"### Assumptions Before Testing on a Linear Model\n\n* Linear Relationship between predictors(X) and outcome(y)\n* Multivariate normality (X should follow a Gausian distribution)\n* No or little Multicollinearity (X's shouldnt be linearly correlated to one another)\n* Homoscedasticity (variance should be the same)","c5ad1493":"I know theres probably more outliers in other features, in later sections i will be trying to take care of those."}}