{"cell_type":{"8bdb8157":"code","04f93e29":"code","51eaf053":"code","f455b595":"code","bce06eaa":"code","24357030":"code","9da865a2":"code","cc5b6ac8":"code","73137bf8":"code","31b49674":"code","c1a3ff5a":"code","7b83c4bc":"code","10dc0e1c":"code","5ac01e08":"code","e824de9a":"code","57f88b91":"code","8eaa50fe":"code","9d0abd60":"code","a5cbc744":"code","d07b21c7":"code","ad8e4c1e":"code","79586e41":"code","6526dc73":"code","867c0aa5":"code","b1896eb6":"code","903b038b":"code","1dd43e09":"code","b13f9d4b":"code","aa80b686":"code","62929588":"code","a068e30c":"code","925f8247":"code","fa6a5410":"code","7cc72cf9":"code","d10cb53e":"code","07a46a5a":"code","614ca2ec":"code","2da346f8":"code","d340e30e":"code","0181f0a0":"code","39e5e1c7":"code","2a412380":"code","4699dc42":"code","83b4e265":"code","f9411cd6":"code","7aae6170":"code","50ca15d0":"code","36e974f5":"code","ed9bca95":"code","f39a1a39":"code","5149ff0e":"code","8630268f":"code","9e686992":"code","e1073fa1":"code","2466e676":"code","98efae7d":"code","7cd76732":"code","d561885d":"code","f90b1ea9":"code","60c55e8b":"code","c1e31d84":"markdown","3aecc6a3":"markdown","bab4246c":"markdown","e6ec086f":"markdown","0dfadb65":"markdown","cb77e8ca":"markdown","bea183d6":"markdown","38d06195":"markdown","8c6c05cc":"markdown"},"source":{"8bdb8157":"import pandas as pd\nimport numpy  as np","04f93e29":"df=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf","51eaf053":"df.info()","f455b595":"df.keyword.value_counts(dropna=False)","bce06eaa":"df.location.value_counts(dropna=False)","24357030":"df.drop(labels='location',inplace=True,axis=1)","9da865a2":"df.isnull().sum()","cc5b6ac8":"df[df['target']==1].isnull().sum()","73137bf8":"df['keyword'].fillna(df[df['target']==1].keyword.mode(), inplace=True)","31b49674":"df.isnull().sum()","c1a3ff5a":"df[df['target']==0].isnull().sum()","7b83c4bc":"df[df['target']==1].isnull().sum()","10dc0e1c":"df.target.value_counts()","5ac01e08":"df.dropna(inplace=True)","e824de9a":"df.isnull().sum()","57f88b91":"df.info()","8eaa50fe":"X=df.drop('target',axis=1)\ny=df['target']","9d0abd60":"X","a5cbc744":"X.drop(labels='id',inplace=True,axis=1)","d07b21c7":"y","ad8e4c1e":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","79586e41":"X.head()","6526dc73":"# Removing punctuations\nX=X.iloc[:,0:2]\nX.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n\n# Renaming column names for ease of access\nlist1= [i for i in range(2)]\nnew_Index=[str(i) for i in list1]\nX.columns= new_Index\nX.head(5)","867c0aa5":"# Convertng headlines to lower case\nfor index in new_Index:\n    X[index]=X[index].str.lower()\nX.head(1)","b1896eb6":"' '.join(str(x) for x in X.iloc[1,0:2])","903b038b":"tweet = []### Combining both the columns\nfor row in range(0,len(X.index)):\n    tweet.append(' '.join(str(x) for x in X.iloc[row,0:3]))","1dd43e09":"tweet[0]","b13f9d4b":"len(tweet)","aa80b686":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\nfor i in range(len(tweet)):\n    review = re.sub('[^a-zA-Z]', ' ', tweet[i])\n    review = review.lower()\n    review = review.split() \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    corpus.append(review)","62929588":"corpus[0]","a068e30c":"len(corpus)","925f8247":"## implement BAG OF WORDS\ncountvector=CountVectorizer(ngram_range=(2,2))\ntraindataset=countvector.fit_transform(corpus)","fa6a5410":"countvector","7cc72cf9":"traindataset","d10cb53e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(traindataset, y, test_size=0.33, random_state=42)","07a46a5a":"X_train.shape","614ca2ec":"from sklearn.ensemble import RandomForestClassifier\n\n# implement RandomForest Classifier\nrandomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\nrandomclassifier.fit(X_train,y_train)","2da346f8":"predictions = randomclassifier.predict(X_test)\npredictions","d340e30e":"## Import library to check accuracy\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n\nmatrix=confusion_matrix(y_test,predictions)\nprint(matrix)\nscore=accuracy_score(y_test,predictions)\nprint(score)\nreport=classification_report(y_test,predictions)\nprint(report)","0181f0a0":"from sklearn.naive_bayes import MultinomialNB","39e5e1c7":"mnb=MultinomialNB()\nmnb.fit(X_train, y_train)","2a412380":"y_pred = mnb.predict(X_test)\ny_pred","4699dc42":"cm=confusion_matrix(y_test,y_pred)\nprint(cm)\nacc=accuracy_score(y_test,y_pred)\nprint(acc)\nrep=classification_report(y_test,y_pred)\nprint(rep)","83b4e265":"from sklearn.model_selection import cross_val_score\nnb_Kfold_accu = cross_val_score(estimator = mnb, X = X_train, y = y_train, cv = 10)\nnb_Kfold_accu=nb_Kfold_accu.mean()\nprint(\"Accuracy: {:.2f} %\".format(nb_Kfold_accu*100))","f9411cd6":"test=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest","7aae6170":"test.isnull().sum()","50ca15d0":"test.drop(labels=['location'],axis=1,inplace=True)","36e974f5":"test.keyword.value_counts()","ed9bca95":"test['keyword'].fillna('deluged',inplace=True)","f39a1a39":"test.info()","5149ff0e":"df2=test.copy()","8630268f":"test.drop(labels=['id'],axis=1,inplace=True)","9e686992":"# Removing punctuations\ntest=test.iloc[:,0:2]\ntest.replace(\"[^a-zA-Z]\",\" \",regex=True, inplace=True)\n\n# Renaming column names for ease of access\nlist2= [j for j in range(2)]\nnew_Index1=[str(j) for j in list2]\ntest.columns= new_Index1\ntest.head(5)","e1073fa1":"' '.join(str(x) for x in test.iloc[1,0:2])","2466e676":"test_tweet = []\nfor row in range(0,len(test.index)):\n    test_tweet.append(' '.join(str(x) for x in test.iloc[row,0:2]))","98efae7d":"test_tweet[1]","7cd76732":"ps = PorterStemmer()\ntest_corpus = []\nfor i in range(len(test_tweet)):\n    review1 = re.sub('[^a-zA-Z]', ' ',test_tweet[i])\n    review1 = review1.lower()\n    review1 = review1.split() \n    review1 = [ps.stem(word) for word in review1 if not word in stopwords.words('english')]\n    review1 = ' '.join(review1)\n    test_corpus.append(review1)","d561885d":"test_corpus[0]","f90b1ea9":"test_dataset = countvector.transform(test_corpus)\npredict1 = randomclassifier.predict(test_dataset)\npredict1","60c55e8b":"sub= pd.DataFrame({'id':df2.id,\n                      'target':predict1})\n\nsub.to_csv(\"outcome.csv\",index=False,header = True)","c1e31d84":"### Prediction of  test data using the RF model","3aecc6a3":"### Importing TEST Data","bab4246c":"### as we can see,out of 61 null values,42 null values in keywords are in Taregt=1,that means where the diisaster has happened,so we will replace the 42 null values with mode count,remaining will drop which are not necessary for the analysis","e6ec086f":"# Real or Not? NLP with Disaster Tweets\nPredict which Tweets are about real disasters and which ones are not\n\nwe have files which include a train data,test data and sample submission file ,\nWe will be predicting whether a given tweet is about a real disaster or not. If so, predict a 1. If not, predict a 0.\nusing natural language processing.\n\nhere we will split training data into train and test split ,to test our model performance,post that we use our machine learning model to test on unseen data,that is test set.","0dfadb65":"### similarly cleaning out test data like training data for prediction","cb77e8ca":"### We can see we have classes which are balanced,and good for analysis","bea183d6":"### We observed both models Multinomial and random forest ,howver random forest gave us the better results compared to the other,with accuracy of 72 %","38d06195":"## Lets seperate target and independent variables","8c6c05cc":"### we will replace missing values with most number of repeated keywords,that is mode of the list"}}