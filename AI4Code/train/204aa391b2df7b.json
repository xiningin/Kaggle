{"cell_type":{"e06c5cd2":"code","2c31ad32":"code","e7bc8db5":"code","66ff60e5":"code","c3fdb2be":"code","18f617c6":"code","352065ed":"code","31e1a36e":"code","fe5ed2b0":"code","deba7e49":"code","30b389b9":"code","6631d013":"code","3ce96a06":"code","49e0afdb":"code","d8359158":"code","7c9d3ee5":"code","67db0094":"code","8e302bf8":"code","2d5dedf4":"code","ceba1ea3":"code","b11ba4e1":"code","3d854d46":"code","57d257c8":"code","e19aa294":"code","5923e080":"code","215cc145":"code","b5b8672f":"code","6e56f5ae":"code","fac5d223":"code","5e8402fb":"code","1bf5e0af":"code","ef5a813e":"code","dfbe78ba":"code","cb30eba6":"code","a673a941":"markdown","a3a54f7c":"markdown","726d3989":"markdown","b634c5c0":"markdown","badbc6d3":"markdown","612e5516":"markdown","56ae19f2":"markdown","71b94bd5":"markdown","ef99d0f4":"markdown","c19db98d":"markdown","a62ff9f4":"markdown","c51efd2b":"markdown","5abc123c":"markdown","23ef6458":"markdown","6cfb05b4":"markdown"},"source":{"e06c5cd2":"# KimBaek Seyeong\n# Class Data Analysis and Utilization\n\n# Comparison between 2019,12 and 2020,10\nimport re\nimport time\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom multiprocessing import Pool\nimport nltk\nfrom nltk.corpus import stopwords\n\ndf1 = pd.read_csv(\"\/kaggle\/input\/kbs-airbnb-after-preprocessing\/2019_12_after_preprocessing.csv\", low_memory = False)\ndf2 = pd.read_csv(\"\/kaggle\/input\/kbs-airbnb-after-preprocessing\/2020_10_after_preprocessing.csv\", low_memory = False)\ndf1ch = pd.read_csv(\"\/kaggle\/input\/kbs-airbnb-after-preprocessing\/2019_12_changed_compared_with_2020_10_after_preprocessing.csv\", low_memory = False)\ndf2ch = pd.read_csv(\"\/kaggle\/input\/kbs-airbnb-after-preprocessing\/2020_10_changed_compared_with_2019_12_after_preprocessing.csv\", low_memory = False)\ndfad = pd.read_csv(\"\/kaggle\/input\/kbs-airbnb-after-preprocessing\/2020_10_added_compared_with_2019_12_after_preprocessing.csv\", low_memory = False)\ndfdr = pd.read_csv(\"\/kaggle\/input\/kbs-airbnb-after-preprocessing\/2020_10_dropped_compared_with_2019_12_after_preprocessing.csv\", low_memory = False)","2c31ad32":"def use_multiprocess(func, iter, workers):\n    pool = Pool(processes=workers)\n    result = pool.map(func, iter)\n    pool.close()\n    return result","e7bc8db5":"def text_cleaning(data): # multi-string \ud615\uc2dd\uc73c\ub85c \uc800\uc7a5\ub41c \ud30c\uc77c\uc744 \uadf8\ub300\ub85c \uc0ac\uc6a9\ud560 \uacbd\uc6b0\uc758 \ucf54\ub4dc\n    # \uc601\ubb38\uc790 \uc774\uc678 \ubb38\uc790\ub294 \uacf5\ubc31\uc73c\ub85c \ubcc0\ud658\n    only_english = re.sub('[^a-zA-Z]', ' ', str(data))\n \n    # \uc18c\ubb38\uc790 \ubcc0\ud658\n    no_capitals = only_english.lower().split()\n \n    # \ubd88\uc6a9\uc5b4 \uc81c\uac70\n    stops = set(stopwords.words('english'))\n    no_stops = [word for word in no_capitals if not word in stops]\n \n    # \uc5b4\uac04 \ucd94\ucd9c\n    stemmer = nltk.stem.SnowballStemmer('english')\n    stemmer_words = [stemmer.stem(word) for word in no_stops]\n \n    # \uacf5\ubc31\uc73c\ub85c \uad6c\ubd84\ub41c \ubb38\uc790\uc5f4\ub85c \uacb0\ud569\ud558\uc5ec \uacb0\uacfc \ubc18\ud658\n    return ' '.join(stemmer_words)","66ff60e5":"def show_word_count_stat(data):\n    num_word = []\n    num_unique_words = []\n    for item in data:\n        num_word.append(len(str(item).split()))\n        num_unique_words.append(len(set(str(item).split())))\n \n    # \uc77c\ubc18\n    train['num_words'] = pd.Series(num_word)\n    # \uc911\ubcf5 \uc81c\uac70\n    train['num_unique_words'] = pd.Series(num_unique_words)\n    \n    # \uc704\uc758 \uacb0\uacfc \uc800\uc7a5\n    train_df = pd.DataFrame(train)\n    train_df.to_csv(\"train.csv\", index=False)\n \n    x = data[0]\n    x = str(x).split()\n    print(len(x))\n \n    rc('font', family='AppleGothic')\n \n    fig, axes = plt.subplots(ncols=2)\n    fig.set_size_inches(18, 6)\n    print('mean of number of words : ', train['num_words'].mean())\n    print('median of number of words', train['num_words'].median())\n    sns.distplot(train['num_words'], bins=100, ax=axes[0])\n    axes[0].axvline(train['num_words'].median(), linestyle='dashed')\n    axes[0].set_title('distribution of number of words')\n \n    print('mean of number of unique words : ', train['num_unique_words'].mean())\n    print('median of number of unique words', train['num_unique_words'].median())\n    sns.distplot(train['num_unique_words'], bins=100, color='g', ax=axes[1])\n    axes[1].axvline(train['num_unique_words'].median(), linestyle='dashed')\n    axes[1].set_title('distribution of number of unique words')\n \n    plt.show()","c3fdb2be":"if __name__ == '__main__':\n    start_time = time.time()\n    train = pd.read_csv('\/kaggle\/input\/kbs-airbnb-after-preprocessing\/2019_12_after_preprocessing.csv', low_memory = False)\n        \n    clean_processed = use_multiprocess(text_cleaning, train['description'], 3)\n    print('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n \n    show_word_count_stat(clean_processed)","18f617c6":"clean_processed","352065ed":"start_time = time.time()\ncleaned1 = use_multiprocess(text_cleaning, df1['description'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\nstart_time = time.time()\ncleaned2 = use_multiprocess(text_cleaning, df1['neighborhood_overview'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\nstart_time = time.time()\ncleaned3 = use_multiprocess(text_cleaning, df1['amenities'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))","31e1a36e":"start_time = time.time()\ncleaned4 = use_multiprocess(text_cleaning, df2['description'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\nstart_time = time.time()\ncleaned5 = use_multiprocess(text_cleaning, df2['neighborhood_overview'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\nstart_time = time.time()\ncleaned6 = use_multiprocess(text_cleaning, df2['amenities'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))","fe5ed2b0":"start_time = time.time()\ncleaned7 = use_multiprocess(text_cleaning, df1ch['description'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\nstart_time = time.time()\ncleaned8 = use_multiprocess(text_cleaning, df1ch['neighborhood_overview'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\nstart_time = time.time()\ncleaned9 = use_multiprocess(text_cleaning, df1ch['amenities'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))","deba7e49":"start_time = time.time()\ncleaned10 = use_multiprocess(text_cleaning, df2ch['description'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\nstart_time = time.time()\ncleaned11 = use_multiprocess(text_cleaning, df2ch['neighborhood_overview'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\nstart_time = time.time()\ncleaned12 = use_multiprocess(text_cleaning, df2ch['amenities'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))","30b389b9":"start_time = time.time()\ncleaned13 = use_multiprocess(text_cleaning, dfad['description'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\nstart_time = time.time()\ncleaned14 = use_multiprocess(text_cleaning, dfad['neighborhood_overview'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\nstart_time = time.time()\ncleaned15 = use_multiprocess(text_cleaning, dfad['amenities'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))","6631d013":"start_time = time.time()\ncleaned16 = use_multiprocess(text_cleaning, dfdr['description'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\nstart_time = time.time()\ncleaned17 = use_multiprocess(text_cleaning, dfdr['neighborhood_overview'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\nstart_time = time.time()\ncleaned18 = use_multiprocess(text_cleaning, dfdr['amenities'], 3)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))","3ce96a06":"def counter(input_list):\n    word_count = {}\n    for word in input_list:\n        if word in  word_count:\n            word_count[word] += 1\n        else:\n            word_count[word] = 1\n    return word_count\n    \nword_count_test = counter(cleaned1)\nword_count_test","49e0afdb":"# make data frame\ncleaned1_df = pd.DataFrame(cleaned1)\ncleaned2_df = pd.DataFrame(cleaned2)\ncleaned3_df = pd.DataFrame(cleaned3)\ncleaned4_df = pd.DataFrame(cleaned4)\ncleaned5_df = pd.DataFrame(cleaned5)\ncleaned6_df = pd.DataFrame(cleaned6)\ncleaned7_df = pd.DataFrame(cleaned7)\ncleaned8_df = pd.DataFrame(cleaned8)\ncleaned9_df = pd.DataFrame(cleaned9)\ncleaned10_df = pd.DataFrame(cleaned10)\ncleaned11_df = pd.DataFrame(cleaned11)\ncleaned12_df = pd.DataFrame(cleaned12)\ncleaned13_df = pd.DataFrame(cleaned13)\ncleaned14_df = pd.DataFrame(cleaned14)\ncleaned15_df = pd.DataFrame(cleaned15)\ncleaned16_df = pd.DataFrame(cleaned16)\ncleaned17_df = pd.DataFrame(cleaned17)\ncleaned18_df = pd.DataFrame(cleaned18)","d8359158":"cleaned1_df.iloc[0, 0]","7c9d3ee5":"# cleaned1\nstart_time = time.time()\nfor i in range(0, len(cleaned1_df)):\n    name1 = cleaned1_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list1_{}'.format(i)] = name2\n\nlist1 = []\nfor i in range(0, len(cleaned1_df)):\n    list1 = list1 + globals()['list1_{}'.format(i)]\n    \ncount1 = counter(list1)\ncount1 = sorted(count1.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount1","67db0094":"# cleaned2\nstart_time = time.time()\nfor i in range(0, len(cleaned2_df)):\n    name1 = cleaned2_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list2_{}'.format(i)] = name2\n\nlist2 = []\nfor i in range(0, len(cleaned2_df)):\n    list2 = list2 + globals()['list2_{}'.format(i)]\n    \ncount2 = counter(list2)\ncount2 = sorted(count2.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount2","8e302bf8":"# cleaned3\nstart_time = time.time()\nfor i in range(0, len(cleaned3_df)):\n    name1 = cleaned3_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list3_{}'.format(i)] = name2\n\nlist3 = []\nfor i in range(0, len(cleaned3_df)):\n    list3 = list3 + globals()['list3_{}'.format(i)]\n    \ncount3 = counter(list3)\ncount3 = sorted(count3.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount3","2d5dedf4":"# cleaned4\nstart_time = time.time()\nfor i in range(0, len(cleaned4_df)):\n    name1 = cleaned4_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list4_{}'.format(i)] = name2\n\nlist4 = []\nfor i in range(0, len(cleaned4_df)):\n    list4 = list4 + globals()['list4_{}'.format(i)]\n    \ncount4 = counter(list4)\ncount4 = sorted(count4.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount4","ceba1ea3":"# cleaned5\nstart_time = time.time()\nfor i in range(0, len(cleaned5_df)):\n    name1 = cleaned5_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list5_{}'.format(i)] = name2\n\nlist5 = []\nfor i in range(0, len(cleaned5_df)):\n    list5 = list5 + globals()['list5_{}'.format(i)]\n    \ncount5 = counter(list5)\ncount5 = sorted(count5.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount5","b11ba4e1":"# cleaned6\nstart_time = time.time()\nfor i in range(0, len(cleaned6_df)):\n    name1 = cleaned6_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list6_{}'.format(i)] = name2\n\nlist6 = []\nfor i in range(0, len(cleaned6_df)):\n    list6 = list6 + globals()['list6_{}'.format(i)]\n    \ncount6 = counter(list6)\ncount6 = sorted(count6.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount6","3d854d46":"# cleaned7\nstart_time = time.time()\nfor i in range(0, len(cleaned7_df)):\n    name1 = cleaned7_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list7_{}'.format(i)] = name2\n\nlist7 = []\nfor i in range(0, len(cleaned7_df)):\n    list7 = list7 + globals()['list7_{}'.format(i)]\n    \ncount7 = counter(list7)\ncount7 = sorted(count7.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount7","57d257c8":"# cleaned8\nstart_time = time.time()\nfor i in range(0, len(cleaned8_df)):\n    name1 = cleaned8_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list8_{}'.format(i)] = name2\n\nlist8 = []\nfor i in range(0, len(cleaned8_df)):\n    list8 = list8 + globals()['list8_{}'.format(i)]\n    \ncount8 = counter(list8)\ncount8 = sorted(count8.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount8","e19aa294":"# cleaned9\nstart_time = time.time()\nfor i in range(0, len(cleaned9_df)):\n    name1 = cleaned9_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list9_{}'.format(i)] = name2\n\nlist9 = []\nfor i in range(0, len(cleaned9_df)):\n    list9 = list9 + globals()['list9_{}'.format(i)]\n    \ncount9 = counter(list9)\ncount9 = sorted(count9.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount9","5923e080":"# cleaned10\nstart_time = time.time()\nfor i in range(0, len(cleaned10_df)):\n    name1 = cleaned10_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list10_{}'.format(i)] = name2\n\nlist10 = []\nfor i in range(0, len(cleaned10_df)):\n    list10 = list10 + globals()['list10_{}'.format(i)]\n    \ncount10 = counter(list10)\ncount10 = sorted(count10.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount10","215cc145":"# cleaned11\nstart_time = time.time()\nfor i in range(0, len(cleaned11_df)):\n    name1 = cleaned11_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list11_{}'.format(i)] = name2\n\nlist11 = []\nfor i in range(0, len(cleaned11_df)):\n    list11 = list11 + globals()['list11_{}'.format(i)]\n    \ncount11 = counter(list11)\ncount11 = sorted(count11.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount11","b5b8672f":"# cleaned12\nstart_time = time.time()\nfor i in range(0, len(cleaned12_df)):\n    name1 = cleaned12_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list12_{}'.format(i)] = name2\n\nlist12 = []\nfor i in range(0, len(cleaned12_df)):\n    list12 = list12 + globals()['list12_{}'.format(i)]\n    \ncount12 = counter(list12)\ncount12 = sorted(count12.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount12","6e56f5ae":"# cleaned13\nstart_time = time.time()\nfor i in range(0, len(cleaned13_df)):\n    name1 = cleaned13_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list13_{}'.format(i)] = name2\n\nlist13 = []\nfor i in range(0, len(cleaned13_df)):\n    list13 = list13 + globals()['list13_{}'.format(i)]\n    \ncount13 = counter(list13)\ncount13 = sorted(count13.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount13","fac5d223":"# cleaned14\nstart_time = time.time()\nfor i in range(0, len(cleaned14_df)):\n    name1 = cleaned14_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list14_{}'.format(i)] = name2\n\nlist14 = []\nfor i in range(0, len(cleaned14_df)):\n    list14 = list14 + globals()['list14_{}'.format(i)]\n    \ncount14 = counter(list14)\ncount14 = sorted(count14.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount14","5e8402fb":"# cleaned15\nstart_time = time.time()\nfor i in range(0, len(cleaned15_df)):\n    name1 = cleaned15_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list15_{}'.format(i)] = name2\n\nlist15 = []\nfor i in range(0, len(cleaned15_df)):\n    list15 = list15 + globals()['list15_{}'.format(i)]\n    \ncount15 = counter(list15)\ncount15 = sorted(count15.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount15","1bf5e0af":"# cleaned16\nstart_time = time.time()\nfor i in range(0, len(cleaned16_df)):\n    name1 = cleaned16_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list16_{}'.format(i)] = name2\n\nlist16 = []\nfor i in range(0, len(cleaned16_df)):\n    list16 = list16 + globals()['list16_{}'.format(i)]\n    \ncount16 = counter(list16)\ncount16 = sorted(count16.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount16","ef5a813e":"# cleaned17\nstart_time = time.time()\nfor i in range(0, len(cleaned17_df)):\n    name1 = cleaned17_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list17_{}'.format(i)] = name2\n\nlist17 = []\nfor i in range(0, len(cleaned17_df)):\n    list17 = list17 + globals()['list17_{}'.format(i)]\n    \ncount17 = counter(list17)\ncount17 = sorted(count17.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount17","dfbe78ba":"# cleaned18\nstart_time = time.time()\nfor i in range(0, len(cleaned18_df)):\n    name1 = cleaned18_df.iloc[i, 0]\n    name1 = str(name1)\n    name2 = name1.lower().split()\n    globals()['list18_{}'.format(i)] = name2\n\nlist18 = []\nfor i in range(0, len(cleaned18_df)):\n    list18 = list18 + globals()['list18_{}'.format(i)]\n    \ncount18 = counter(list18)\ncount18 = sorted(count18.items(), key=lambda x:x[1], reverse=True)\nprint('\uc2e4\ud589 \uc2dc\uac04 :', (time.time() - start_time))\n\ncount18","cb30eba6":"count1_df = pd.DataFrame(count1)\ncount1_df.to_csv(\"count1.csv\", index=False)\ncount2_df = pd.DataFrame(count2)\ncount2_df.to_csv(\"count2.csv\", index=False)\ncount3_df = pd.DataFrame(count3)\ncount3_df.to_csv(\"count3.csv\", index=False)\n\ncount4_df = pd.DataFrame(count4)\ncount4_df.to_csv(\"count4.csv\", index=False)\ncount5_df = pd.DataFrame(count5)\ncount5_df.to_csv(\"count5.csv\", index=False)\ncount6_df = pd.DataFrame(count6)\ncount6_df.to_csv(\"count6.csv\", index=False)\n\ncount7_df = pd.DataFrame(count7)\ncount7_df.to_csv(\"count7.csv\", index=False)\ncount8_df = pd.DataFrame(count8)\ncount8_df.to_csv(\"count8.csv\", index=False)\ncount9_df = pd.DataFrame(count9)\ncount9_df.to_csv(\"count9.csv\", index=False)\n\ncount10_df = pd.DataFrame(count10)\ncount10_df.to_csv(\"count10.csv\", index=False)\ncount11_df = pd.DataFrame(count11)\ncount11_df.to_csv(\"count11.csv\", index=False)\ncount12_df = pd.DataFrame(count12)\ncount12_df.to_csv(\"count12.csv\", index=False)\n\ncount13_df = pd.DataFrame(count13)\ncount13_df.to_csv(\"count13.csv\", index=False)\ncount14_df = pd.DataFrame(count14)\ncount14_df.to_csv(\"count14.csv\", index=False)\ncount15_df = pd.DataFrame(count15)\ncount15_df.to_csv(\"count15.csv\", index=False)\n\ncount16_df = pd.DataFrame(count16)\ncount16_df.to_csv(\"count16.csv\", index=False)\ncount17_df = pd.DataFrame(count17)\ncount17_df.to_csv(\"count17.csv\", index=False)\ncount18_df = pd.DataFrame(count18)\ncount18_df.to_csv(\"count18.csv\", index=False)","a673a941":"text cleaning \uacfc\uc815\uc744 \ub9c8\uce5c \uac01 \ubc30\uc5f4\ub4e4\uc758 \ub2e8\uc5b4 \ube48\ub3c4\ub97c \ud655\uc778\ud558\uae30 \uc704\ud558\uc5ec counter \ud568\uc218\ub97c \uc791\uc131\ud569\ub2c8\ub2e4.","a3a54f7c":"\ub370\uc774\ud130 \ud504\ub808\uc784 \uc774\ub984 \uba54\ubaa8\n* df1 : 2019\ub144 12\uc6d4 Airbnb \uc81c\uacf5 NY, United States \ub370\uc774\ud130\uc14b\n* df2 : 2020\ub144 10\uc6d4 Airbnb \uc81c\uacf5 NY, United States \ub370\uc774\ud130\uc14b\n* df1ch : 2019\ub144 12\uc6d4 Airbnb \uc81c\uacf5 NY, United States \ub370\uc774\ud130\uc14b \uc911 2020\ub144 10\uc6d4\uc5d0 \ub370\uc774\ud130\uc5d0 \ubcc0\ub3d9\uc0ac\ud56d\uc774 \uc788\ub294 \ub9e4\ubb3c\n* df2ch : 2020\ub144 10\uc6d4 Airbnb \uc81c\uacf5 NY, United States \ub370\uc774\ud130\uc14b \uc911 2019\ub144 12\uc6d4\uacfc \ube44\uad50\ud588\uc744 \ub54c \ub370\uc774\ud130 \ubcc0\ub3d9\uc0ac\ud56d\uc774 \uc788\ub294 \ub9e4\ubb3c\n* dfad : 2019\ub144 12\uc6d4\uc5d0\ub294 \uc5c6\uc5c8\uc73c\ub098 2020\ub144 10\uc6d4\uc5d0 \ucd94\uac00\ub41c \ub370\uc774\ud130 \ubaa8\uc74c \ub370\uc774\ud130\uc14b\n* dfdr : 2019\ub144 12\uc6d4\uc5d0\ub294 \uc788\uc5c8\uc73c\ub098 2020\ub144 10\uc6d4\uc5d0 \uc0ac\ub77c\uc9c4 \ub370\uc774\ud130 \ubaa8\uc74c \ub370\uc774\ud130\uc14b\n\ndescription(2), neighborhood_overview(3), amenities(15)","726d3989":"# Notebook Outline\n\nAnalysis notebook\uc5d0 \uc774\uc5b4 \uc791\uc131\ud569\ub2c8\ub2e4.\n\n\uc774\uc5b4\uc9c0\ub294 notebook\uc5d0\uc11c \uc9c4\ud589\ud560 \uc0ac\ud56d\ub4e4\uc740 \uc544\ub798\uc640 \uac19\uc2b5\ub2c8\ub2e4.\n\n\ud604\uc7ac \uc804\ucc98\ub9ac \uacfc\uc815\uc5d0 \ucd94\uac00\ud574\uc57c \ud560 \ubd80\ubd84:\n* \ubd88\uc6a9\uc5b4(\uc608\uce21\/\ud559\uc2b5\uc5d0 \uc2e4\uc81c\ub85c \uae30\uc5ec\ud558\uc9c0 \uc54a\ub294 \ud14d\uc2a4\ud2b8) \uc81c\uac70\n* \uc5b4\uac04(\uacfc\uac70, \ud604\uc7ac, \ubbf8\ub798\ud615\uc758 \uad6c\ubd84\uc744 \ubb34\ud6a8\ud654 \ud558\uae30 \uc704\ud568) \ucd94\ucd9c\n\n\uc704\uc758 \uacfc\uc815\uc774 \ub05d\ub09c \uc774\ud6c4:\n* \uc804\ucc98\ub9ac \uc644\ub8cc\ud55c \uacb0\uacfc csv \ud30c\uc77c\ub85c output\n* \uadf8\ub8f9 \ub098\ub204\uae30 + \ub098\ub220\uc11c numerical, non-numerical \ub370\uc774\ud130 \uc2dc\uac01\ud654 \ubc0f \ube44\uad50 (\uadf8\ub8f9\ubcc4, \uc2dc\uae30\ubcc4)","b634c5c0":"\ubd88\uc6a9\uc5b4 \ucc98\ub9ac\uc640 \uc5b4\uac04 \ucc98\ub9ac\ub97c \uc704\ud574 \ud574\uc57c \ud558\ub294 \uac83\n* \ubc29\ubc951(\ud558\uace0 \uc2f6\uc740 \ubc29\ud5a5) : \ud604\uc7ac 0: \ub2e8\uc5b4, 1: \ube48\ub3c4 \ub85c \uc800\uc7a5\ub418\uc5b4 \uc788\ub294 \ubd84\ub9ac\ub41c \ub370\uc774\ud130\ud504\ub808\uc784\uc5d0\uc11c \ubd88\uc6a9\uc5b4\uc5d0 \ud574\ub2f9\ud558\ub294 row \uc0ad\uc81c\n* \ubc29\ubc952 : \uae30\uc874\uc758 \uc140\ub809\ud55c column\ub9cc \ucd94\ucd9c\ud55c \ud30c\uc77c\uc5d0\uc11c \uc704\uc640 \uac19\uc740 \ubc29\ud5a5\uc73c\ub85c \ubd88\uc6a9\uc5b4, \uc5b4\uac04\ucc98\ub9ac\ub97c \ud55c \uc774\ud6c4\uc5d0 Analysis notebook \ubc29\ubc95\uc744 \ud65c\uc6a9\ud574 \ub2e8\uc5b4 \ube48\ub3c4 \uc218 \uccb4\ud06c\n\n\ud558\uace0 \uc2f6\uc740 \ubc29\ud5a5\uc740 1\uc774\uae34 \ud558\uc9c0\ub9cc, \ub354 \ube60\ub974\uac8c \uc131\uacf5 \uac00\ub2a5\ud560 \uac83\uc73c\ub85c \ubcf4\uc774\ub294 2\ub85c \uc2dc\uc791\ud558\uaca0\uc2b5\ub2c8\ub2e4.","badbc6d3":"\ub370\uc774\ud130 \ud504\ub808\uc784 \uc774\ub984 \uba54\ubaa8\n* train_df : \uae30\uc874\uc758 dataset\uc5d0 \uba87\uac1c\uc758 \ub2e8\uc5b4\ub97c \uac00\uc84c\ub294\uc9c0\uc640 \uba87\uac1c\uc758 \uace0\uc720 \ub2e8\uc5b4\ub97c \uac00\uc84c\ub294\uc9c0 column\uc774 \ucd94\uac00\ub428","612e5516":"# Non-numerical, Datapreprocessing for Word Count","56ae19f2":"\uc774\uc804 \ub178\ud2b8\ubd81\uc744 \ud1b5\ud574 \uc804\ucc98\ub9ac\uc5d0 \uc18c\uc694\ub418\ub294 \uc2dc\uac04\uc774 \ub370\uc774\ud130\uc5d0 \uc591\uc5d0 \ub530\ub77c \uae38\uc5b4\uc9c8 \uc218 \uc788\uc74c\uc744 \ud655\uc778\ud588\uae30 \ub54c\ubb38\uc5d0, multiprocessing\uc744 \ucc28\uc6a9\ud588\uc2b5\ub2c8\ub2e4.","71b94bd5":"# \uc791\uc131\ud55c notebook \ucc38\uace0\uc6a9 url \ucca8\ubd80\n\n* Preprocessing Airbnb Data 2019\/12 and 2020\/10 : https:\/\/www.kaggle.com\/kimbaekseyeong\/preprocessing-airbnb-data-2019-12-and-2020-10\n* Data Extraction Airbnb Data 2019\/12 and 2020\/10 : https:\/\/www.kaggle.com\/kimbaekseyeong\/extracting-dataset-airbnb-data-2019-12-and-2020-10\n* NB for Regular Expression Test : https:\/\/www.kaggle.com\/kimbaekseyeong\/nb-for-regular-expression-test\n* Analysis Airbnb Data 2019\/12 and 2020\/10 : https:\/\/www.kaggle.com\/kimbaekseyeong\/analysis-airbnb-data-2019-12-and-2020-10","ef99d0f4":"* df1, df2, df1ch, df2ch, dfad, dfdr\n* description(2), neighborhood_overview(3), amenities(15)","c19db98d":"\uc704\uc758 \uacb0\uacfc\ub97c \uc800\uc7a5\ud569\ub2c8\ub2e4.","a62ff9f4":"\uc704\uc758 \uacb0\uacfc\ub97c \ud655\uc778\ud574\ubcf4\uba74, \ud55c \uc6d0\uc18c\ub97c \ub098\ub220\uc11c \ud655\uc778\ud558\ub294 \uac83\uc774 \uc544\ub2c8\ub77c \ud55c \ubc88\uc5d0 \uce74\uc6b4\ud2b8\ub97c \ud558\uae30 \ub54c\ubb38\uc5d0 Analysis notebook\uacfc \uac19\uc774 \ub2e8\uc5b4\ub85c \ub098\ub220 \uc800\uc7a5\ud560 \ud544\uc694\uac00 \uc788\uc74c\uc744 \uc54c \uc218 \uc788\uc2b5\ub2c8\ub2e4.","c51efd2b":"# Continued...\n\n\uc774\uc5b4\uc9c0\ub294 notebook\uc5d0\uc11c\ub294 \uac00\uaca9, \ub8f8\ud0c0\uc785, \uc9c0\uc5ed\uc5d0 \ub530\ub77c groupping\uc744 \uc9c4\ud589\ud55c \ud6c4 \ub2e8\uc5b4\uc758 \ube48\ub3c4\ub97c \uccb4\ud06c\ud558\uaca0\uc2b5\ub2c8\ub2e4.","5abc123c":"\uc774 \ucf54\ub4dc\uc758 \uacb0\uacfc\ub85c \uc5bb\uc744 \uc218 \uc788\ub294 \uac83\uc740 \ud574\ub2f9\ud558\ub294 dataset\uc758 \uc120\ud0dd\ud55c multistring data column\uc5d0\uc11c \uac01 \uc5f4\uc774 \uba87 \uac1c\uc758 \ub2e8\uc5b4\ub97c \uac00\uc9c0\uace0 \uc788\uace0, \uba87 \uac1c\uc758 \uace0\uc720 \ub2e8\uc5b4\ub97c \uac00\uc9c0\uace0 \uc788\ub294\uc9c0 \uc785\ub2c8\ub2e4.\n\n\uc804\ucc98\ub9ac \uad6c\uac04\uc5d0\uc11c \ud65c\uc6a9 \uac00\ub2a5\ud560 \uac83\uc73c\ub85c \ubcf4\uc5ec \ucc38\uace0\ud588\uc2b5\ub2c8\ub2e4.","23ef6458":"# Input Datasets","6cfb05b4":"# Non-numerical, Datapreprocessing for Word Count Example"}}