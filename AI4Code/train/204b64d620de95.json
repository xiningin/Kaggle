{"cell_type":{"0b4d5c18":"code","faf88d47":"code","7d64d307":"code","436cd752":"code","db7eb2f1":"code","ef467866":"code","a51a141f":"code","bce281c5":"code","5d3828d5":"code","52e1c329":"code","57843be9":"code","34bb3e49":"code","851e6095":"code","9153ade4":"code","8f1a63c5":"code","57934352":"code","e6fc0ac8":"code","5e4c9c62":"code","519a46b3":"code","9b563a12":"code","02bf67ec":"code","94031b90":"code","6319ce36":"code","3730c27f":"code","3e51ec3e":"code","a2471ba6":"code","45ff89dc":"markdown","373b7fa9":"markdown","f6902061":"markdown","7697b335":"markdown","f22c90ef":"markdown","67a0bbd0":"markdown","f60510cd":"markdown","9a9630bf":"markdown","74427285":"markdown","1ad33817":"markdown","e783ab77":"markdown","c840261b":"markdown","78894424":"markdown","b6ac4ca9":"markdown","db1be643":"markdown","29f6f438":"markdown","285468b9":"markdown"},"source":{"0b4d5c18":"#core packages\nimport os\nimport joblib\nimport numpy as np\nimport pandas as pd\nimport warnings\n\n#visualisation\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\n\n#setting up options\npd.set_option('display.max_rows', None)\npd.set_option('display.max.columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n#import datasets\ntrain_df = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\n\n#reduce memory usage conversions\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n#reduce memory usage of each dataset\ntrain= reduce_mem_usage(train_df)\ntest= reduce_mem_usage(test_df)","faf88d47":"train.head()","7d64d307":"train.dtypes","436cd752":"print(f'Number of rows: {train_df.shape[0]};  Number of columns: {train_df.shape[1]}; No of missing values: {sum(train_df.isna().sum())}')","db7eb2f1":"for col, i in enumerate(train):\n    print(f'{train.columns[col]} has {train[i].isna().sum()} missing values')","ef467866":"train.describe().T","a51a141f":"test.head()","bce281c5":"test.dtypes","5d3828d5":"print(f'Number of rows: {test_df.shape[0]};  Number of columns: {test_df.shape[1]}; No of missing values: {sum(test_df.isna().sum())}')","52e1c329":"for col, i in enumerate(test):\n    print(f'{test.columns[col]} has {test[i].isna().sum()} missing values')","57843be9":"test.describe()","34bb3e49":"train.describe()","851e6095":"submission.head()","9153ade4":"cats = ['Sex', 'Embarked']\ndrops = ['Name','Cabin', 'Ticket']\ntrain['Embarked'].replace('NaN', 'S', inplace=True)\ntrain['Age'].replace(np.NaN, train['Age'].mean(), inplace=True)\ntest['Age'].replace(np.NaN, test['Age'].mean(), inplace=True)\ntest['Fare'].replace(np.NaN, test['Fare'].mean(), inplace=True)\n\ntrain = pd.get_dummies(train, columns = cats) \ntest = pd.get_dummies(test, columns = cats) \n\ntrain.drop(labels = drops, axis = 1, inplace=True)\ntest.drop(labels = drops, axis = 1, inplace=True)\ntest.head()","8f1a63c5":"integer_features = ['PassengerId', 'Pclass', 'SibSp', 'Parch']\nunique_values_train = pd.DataFrame(train[integer_features].nunique())\nunique_values_train = unique_values_train.reset_index(drop=False)\nunique_values_train.columns = ['Features', 'Count']\n\nunique_values_percent_train = pd.DataFrame(train[integer_features].nunique()\/train.shape[0]*100)\nunique_values_percent_train= unique_values_percent_train.reset_index(drop=False)\nunique_values_percent_train.columns = ['Features', 'Count']\n\nunique_values_test = pd.DataFrame(test[integer_features].nunique())\nunique_values_test = unique_values_test.reset_index(drop=False)\nunique_values_test.columns = ['Features', 'Count']\n\nunique_values_percent_test = pd.DataFrame(test[integer_features].nunique()\/test.shape[0]*100)\nunique_values_percent_test= unique_values_percent_test.reset_index(drop=False)\nunique_values_percent_test.columns = ['Features', 'Count']","57934352":"plt.rcParams['figure.dpi'] = 600 #changes the figure dpi\nfig = plt.figure(figsize=(6,4), facecolor='#f6f5f5') # sets the figure size and background color (facecolor)\ngs = fig.add_gridspec(2,2) # specifices the geometry of where the subplot will be placed\ngs.update(wspace=0.4, hspace=0.5) # space between all the subplots\n\nbackground_colour = '#f6f5f5'\nsns.set_palette(['#4D5ABF']*6)\n\nax0 = fig.add_subplot(gs[0,0])\nfor s in ['right', 'top']:\n    ax0.spines[s].set_visible(False)#\nax0.set_facecolor(background_colour)\nax0_sns = sns.barplot(ax=ax0, y=unique_values_train['Features'], x=unique_values_train['Count'],\n                     zorder=2, linewidth=0, orient='h', saturation = 1, alpha = 1)\nax0_sns.set_xlabel('Unique Values', fontsize = 5, weight='bold')\nax0_sns.set_ylabel('Features', fontsize = 5, weight = 'bold')\nax0_sns.tick_params(labelsize = 4, width=0.5, length =1.5)\nax0_sns.grid(which='major', axis = 'x', zorder = 0, color = '#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis = 'y', zorder = 0, color = '#EEEEEE', linewidth=0.4)\nax0.text(0,-1.5, 'Unique Values - Train Dataset', fontsize = 6, ha='left', va='top',weight = 'bold')\nax0.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n#data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.0f}'\n    x = p.get_x() + p.get_width() + 20\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x,y, value, ha='left', va='center', fontsize=4, bbox = dict(facecolor='none', edgecolor='black', boxstyle = 'round', linewidth=0.3))\n    \nax1 = fig.add_subplot(gs[0,1])\nfor s in ['right', 'top']:\n    ax1.spines[s].set_visible(False)#\nax1.set_facecolor(background_colour)\nax1_sns = sns.barplot(ax=ax1, y=unique_values_percent_train['Features'], x=unique_values_percent_train['Count'],\n                     zorder=2, linewidth=0, orient='h', saturation = 1, alpha = 1)\nax1_sns.set_xlabel('Percent Unique Values', fontsize = 5, weight='bold')\nax1_sns.set_ylabel('Features', fontsize = 5, weight = 'bold')\nax1_sns.tick_params(labelsize = 4, width=0.5, length =1.5)\nax1_sns.grid(which='major', axis = 'x', zorder = 0, color = '#EEEEEE', linewidth=0.4)\nax1_sns.grid(which='major', axis = 'y', zorder = 0, color = '#EEEEEE', linewidth=0.4)\nax1.text(0,-1.5, 'Percent Unique Values - Train Dataset', fontsize = 6, ha='left', va='top',weight = 'bold')\n#data label\nfor p in ax1.patches:\n    value = f'{p.get_width():.2f}'\n    x = p.get_x() + p.get_width() + 0.03\n    y = p.get_y() + p.get_height() \/ 2 \n    ax1.text(x,y, value, ha='left', va='center', fontsize=4, bbox = dict(facecolor='none', edgecolor='black', boxstyle = 'round', linewidth=0.3))\n\nbackground_colour = \"#f6f5f5\"\nsns.set_palette(['#E68619']*6)\n\nax3 = fig.add_subplot(gs[1,0])\nfor s in ['right', 'top']:\n    ax3.spines[s].set_visible(False)#\nax3.set_facecolor(background_colour)\nax3_sns = sns.barplot(ax=ax3, y=unique_values_test['Features'], x=unique_values_test['Count'],\n                     zorder=2, linewidth=0, orient='h', saturation = 1, alpha = 1)\nax3_sns.set_xlabel('Unique Values', fontsize = 5, weight='bold')\nax3_sns.set_ylabel('Features', fontsize = 5, weight = 'bold')\nax3_sns.tick_params(labelsize = 4, width=0.5, length =1.5)\nax3_sns.grid(which='major', axis = 'x', zorder = 0, color = '#EEEEEE', linewidth=0.4)\nax3_sns.grid(which='major', axis = 'y', zorder = 0, color = '#EEEEEE', linewidth=0.4)\nax3.text(0,-1.5, 'Unique Values - Test Dataset', fontsize = 6, ha='left', va='top',weight = 'bold')\nax3.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n#data label\nfor p in ax3.patches:\n    value = f'{p.get_width():,.0f}'\n    x = p.get_x() + p.get_width() + 20\n    y = p.get_y() + p.get_height() \/ 2 \n    ax3.text(x,y, value, ha='left', va='center', fontsize=4, bbox = dict(facecolor='none', edgecolor='black', boxstyle = 'round', linewidth=0.3))\n    \nax4 = fig.add_subplot(gs[1,1])\nfor s in ['right', 'top']:\n    ax4.spines[s].set_visible(False)#\nax4.set_facecolor(background_colour)\nax4_sns = sns.barplot(ax=ax4, y=unique_values_percent_test['Features'], x=unique_values_percent_test['Count'],\n                     zorder=2, linewidth=0, orient='h', saturation = 1, alpha = 1)\nax4_sns.set_xlabel('Percent Unique Values', fontsize = 5, weight='bold')\nax4_sns.set_ylabel('Features', fontsize = 5, weight = 'bold')\nax4_sns.tick_params(labelsize = 4, width=0.5, length =1.5)\nax4_sns.grid(which='major', axis = 'x', zorder = 0, color = '#EEEEEE', linewidth=0.4)\nax4_sns.grid(which='major', axis = 'y', zorder = 0, color = '#EEEEEE', linewidth=0.4)\nax4.text(0,-1.5, 'Percent Unique Values - Test Dataset', fontsize = 6, ha='left', va='top',weight = 'bold')\n#data label\nfor p in ax4.patches:\n    value = f'{p.get_width():.2f}'\n    x = p.get_x() + p.get_width() + 0.03\n    y = p.get_y() + p.get_height() \/ 2 \n    ax4.text(x,y, value, ha='left', va='center', fontsize=4, bbox = dict(facecolor='none', edgecolor='black', boxstyle = 'round', linewidth=0.3))\n    \n","e6fc0ac8":"plt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(10, 10), facecolor='#f6f5f5')\ngs = fig.add_gridspec(5, 2)\ngs.update(wspace=0.3, hspace=0.3)\n\nrun_no = 0\nfor row in range(0,5):\n    for col in range(0,2):\n        locals()['ax'+str(run_no)] = fig.add_subplot(gs[row,col])\n        locals()['ax'+str(run_no)].set_facecolor(background_colour)\n        for s in ['top', 'right']:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no +=1\n\nax0.text(-0.2, 1.4, 'Features Distribution', fontsize = 10, fontweight = 'bold')\nax0.text(-0.2, 1.3, 'Features', fontsize = 6, fontweight='bold')\n\nnotin = ['PassengerId', 'Survived']\nfeatures = list(train.drop(notin, axis=1).columns)\n\nbackground_colour = \"#f6f5f5\"\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()['ax'+str(run_no)], x=train[col], zorder=2, alpha=1, linewidth=1, color = '#4D5ABF')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.kdeplot(ax=locals()[\"ax\"+str(run_no)], x=test[col], zorder=2, alpha=1, linewidth=1, color='#E68619')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize=4, fontweight='bold')\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=4, width=0.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(4)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(4)\n    run_no += 1\n    \nplt.show()","5e4c9c62":"unique_values_target = pd.DataFrame(train['Survived'].value_counts())\nunique_values_target = unique_values_target.reset_index(drop=False)\nunique_values_target.columns = ['Value', 'Count']\n\nunique_values_percent_target = pd.DataFrame(train['Survived'].value_counts()\/train.shape[0]*100)\nunique_values_percent_target= unique_values_percent_target.reset_index(drop=False)\nunique_values_percent_target.columns = ['Value', 'Count']\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(6,0.25), facecolor = '#f6f5f5')\ngs = fig.add_gridspec(1,2)\ngs.update(wspace=0.25, hspace=0)\n\nbackground_colour = '#f6f5f5'\nsns.set_palette(['#4D5ABF']*2)\n\nax0 = fig.add_subplot(gs[0,0])\nfor s in ['right','top']:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_colour)\nax0_sns = sns.barplot(ax=ax0, y=unique_values_target['Value'], x=unique_values_target['Count'],\n                     zorder=2, linewidth=0, orient = 'h', saturation=1, alpha =1)\nax0_sns.set_xlabel(\"No of Occurances\",fontsize=4, weight='bold')\nax0_sns.set_ylabel(\"Unique Value\",fontsize=4, weight='bold')\nax0_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.text(0, -2.8, 'Target Variable', fontsize=6, ha='left', va='top', weight='bold')\nax0.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.0f}'\n    x = p.get_x() + p.get_width() + 25\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='left', va='center', fontsize=2, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n    \nsns.set_palette(['#E68619']*2)\n\nax0 = fig.add_subplot(gs[0, 1])\nfor s in [\"right\", \"top\"]:\n    ax0.spines[s].set_visible(False)\nax0.set_facecolor(background_colour)\nax0_sns = sns.barplot(ax=ax0, y=unique_values_percent_target['Value'], x=unique_values_percent_target['Count'], \n                      zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\nax0_sns.set_xlabel(\"Percentage\",fontsize=4, weight='bold')\nax0_sns.set_ylabel(\"Unique Value\",fontsize=4, weight='bold')\nax0_sns.tick_params(labelsize=4, width=0.5, length=1.5)\nax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\nax0.text(0, -2.8, 'Percentage Target Variable', fontsize=6, ha='left', va='top', weight='bold')\nax0.get_xaxis().set_major_formatter(matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\n# data label\nfor p in ax0.patches:\n    value = f'{p.get_width():,.1f}'\n    x = p.get_x() + p.get_width() + 1\n    y = p.get_y() + p.get_height() \/ 2 \n    ax0.text(x, y, value, ha='left', va='center', fontsize=2, \n            bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n    ","519a46b3":"notin = ['PassengerId', 'Survived']\nfeatures = list(train.drop(notin, axis=1).columns)\n\nplt.rcParams['figure.dpi'] = 600\nfig = plt.figure(figsize=(4,2), facecolor='#f6f5f5')\ngs = fig.add_gridspec(2, 5)\ngs.update(wspace=0.2, hspace=0.4)\n\nbackground_color = \"#f6f5f5\"\nsns.set_palette(['#fcd12a']*2)\n\nrun_no = 0\nfor row in range(0, 2):\n    for col in range(0, 5):\n        locals()[\"ax\"+str(run_no)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(run_no)].set_facecolor(background_color)\n        for s in [\"top\",\"right\"]:\n            locals()[\"ax\"+str(run_no)].spines[s].set_visible(False)\n        run_no += 1\n\nrun_no = 0\nfor col in features:\n    sns.violinplot(ax=locals()[\"ax\"+str(run_no)], y=train[col], x=train['Survived'], \n                     saturation=1, linewidth=0.3, zorder=1, inner='quartile')\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.7)\n    locals()[\"ax\"+str(run_no)].grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.7)\n    locals()[\"ax\"+str(run_no)].set_axisbelow(True) \n    locals()[\"ax\"+str(run_no)].set_ylabel('')\n    locals()[\"ax\"+str(run_no)].set_xlabel(col, fontsize = 3, fontdict=dict(weight='bold'))\n    locals()[\"ax\"+str(run_no)].tick_params(labelsize=2.5, width=0.5, length=1.5)\n    locals()[\"ax\"+str(run_no)].xaxis.offsetText.set_fontsize(2)\n    locals()[\"ax\"+str(run_no)].yaxis.offsetText.set_fontsize(2)\n    run_no += 1\n    \nax0.text(2, 4, 'Features Distribution', fontsize=6, fontweight='bold')\n\nplt.show()","9b563a12":"from sklearn.metrics import accuracy_score\nfrom sklearn import svm\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\n\nfolds = 5\ntrain = train.drop(['PassengerId'], axis=1)\nfeatures = list(train.drop(['Survived'], axis =1).columns)\n\n#Need to fix the missing values before we start to model\n#Missing values in Age, Embarked","02bf67ec":"#train_oof = np.zeros((891,))\n#skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state=9)\n#for fold, (train_idx, val_idx) in enumerate(skf.split(train[features], train['Survived'])):\n#    X_train, X_valid = train.iloc[train_idx], train.iloc[val_idx]\n#    y_train = X_train['Survived']\n#    y_valid = X_valid['Survived']\n#    X_train = X_train.drop('Survived', axis=1)\n#    X_valid = X_valid.drop('Survived', axis=1)\n    \n#    clf = svm.SVC()\n    \n#    clf = clf.fit(X_train, y_train)\n#    temp_oof = clf.predict(X_valid)\n#    train_oof[val_idx] = temp_oof\n#    print(f'Fold{fold} Accuracy: ', accuracy_score(y_valid, temp_oof))\n\n#print(f'OOF accuracy: ', accuracy_score(train['Survived'], train_oof))  ","94031b90":"#params = {'C': [0.1,1,3],\n            #'kernel':('linear', 'rbf', 'poly')\n            #}\n#svc = svm.SVC(verbose = True)\n#CLF = GridSearchCV(svc, params, verbose = 3, cv = 5, n_jobs = -1, scoring = 'accuracy')\n#CLF.fit(train[features], train['Survived'])\n#op = CLF.best_params_\n#print(op)","6319ce36":"train_oof = np.zeros((891,))\nskf = StratifiedKFold(n_splits=folds, shuffle = True, random_state=9)\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train[features], train['Survived'])):\n    X_train, X_valid = train.iloc[train_idx], train.iloc[val_idx]\n    y_train = X_train['Survived']\n    y_valid = X_valid['Survived']\n    X_train = X_train.drop('Survived', axis=1)\n    X_valid = X_valid.drop('Survived', axis=1)\n    \n    clf = svm.SVC(C= 0.1, kernel='linear', verbose=True)\n    \n    clf = clf.fit(X_train, y_train)\n    temp_oof = clf.predict(X_valid)\n    train_oof[val_idx] = temp_oof\n    print(f'Fold{fold} Accuracy: ', accuracy_score(y_valid, temp_oof))\n\nprint(f'OOF accuracy: ', accuracy_score(train['Survived'], train_oof))  ","3730c27f":"b_model = svm.SVC(C= 0.1, kernel='linear', verbose=True)\nb_model.fit(train[features], train['Survived'])\ny_pred = b_model.predict(test[features])","3e51ec3e":"submission['Survived']= y_pred\nsubmission.head()","a2471ba6":"submission.to_csv('Submission.csv',index=False )","45ff89dc":"[back to top](#table-of-contents)\n<a id='3.1'><\/a>\n## 3.2 Test dataset\nHere we investigate the test dataset. We can compare thses to the training set values to see if our datasets look similar or very different. It could give insight into how well the model might perform as this is the dataset used to make predictions.\n\n**Observations**\n- **target**\n    - Of course, target varibale isn't here in the test set.\n- **features**\n    - There are less than half the total number of observations, sitting at `418`. \n    - We only have `414` missing values, with those being in `Age`, `Cabin`, and `Fare`. Perhaps this means the missing values in the training set can be used for predictions, but more likely we will need to inspect both datsets and see how best to replace them.\n    - Overall the values are quite similar. ","373b7fa9":"[back to top](#table-of-contents)\n<a id='3'><\/a>\n# 3. Dataset Overview\nWith this overview hopefully we can get a deel for the data in all three of the loaded datasets. \nWe will look for any missing values and some basic stats on the train and test datasets, while looking at the submission to file to see the format we are expected to submit.\n\n<a id='3.1'><\/a>\n## 3.1 Train dataset\nHere we investigate the training dataset. It will help us get a feel for the target variable (which the test dataset doesn't have), and since this dataset is larger we can see the statistics on the features better. \n\n**Observations**\n- **target**\n    - 'survived' is the target column variable. It only contains values between 1 and 0, and since there are only two possible answers to survived we know this is a classification task.\n    - We can also see the the mean of this column is `0.383838`, which suggests we have an unbalanced dataset.(leaning towards more 0's than 1's.\n- **features**\n    - There are only `11` features, with possible less usable ones as features such as `Name` and `Ticket` might not be useful or have much importance as they contain little information that is extractable.\n    - There are `891` observations in the dataset with `866` missing values. Out of those values `177` of them are missing from `Age`, `687` of them are missing from `Cabin`, and `2` are missing from `Embarked`.\n    - I am going to turn `Embarked` into a categorical feature and potentially get rid of the `Cabin` column as the majority of this data is NaN in both train and test datasets. `Sex` will also be turned categorical, and `Ticket` and `name` will also be gotten rid of. \n","f6902061":"[back to top](#table-of-contents)\n<a id='1'><\/a>\n# 1. Introduction\nThis is my second attempt at the titantic predictio problem after gaining a little more experience. Here I will be practicing my EDA techniques inspired by some amazing EDA I saw here : <a href=\"https:\/\/www.kaggle.com\/dwin183287\/tps-august-2021-eda-base-model\">by Sharlto Cope<\/a>","7697b335":"# Table of Contents\n<a id='table-of-contents'><\/a>\n- [1 Introduction](#1)\n- [2 Preparations](#2)\n- [3 Dataset Overview](#3)\n- [4 Features](#4)\n- [5 Target](#5)\n- [6 Model](#6)\n- [7 Parameter Optimization](#7)\n","f22c90ef":"[back-to-top](#table-of-contents)\n<a id='3.3'><\/a>\n## 3.3 Submission\nThis is just a basic look at the submission file. \nAll it contains is `PassenegerId` and `Survived`","67a0bbd0":"**Observations**\nAccuracy is around 0.65. We will now perform paramter optimization to try and improve this. ","f60510cd":"## 5.2 Feature Distribution by Target\nWe are now going to see if there is any distinct feature distribution for either of the target variables. The following are violinplots\n\n- **Observations**\n    - It looks liek good indicators may be `Sex` and `Pclass`.\n    - I do not know why `Fare` does not work","9a9630bf":"Things are commented out as they take a while to run. The parameters I have didn't create a better model than a baseline I performed a while ago so I need to go back to the drawing board for this one. I will look at including the other variables I didn't use this time. ","74427285":"## 6.2 SVM","1ad33817":"[back to top](#table-of-contents)\n<a id = '7'><\/a>\n# 7. Parameter Optimization\nThe parameters we will be changing are: `C`, `gamma`, `kernel`\n    ","e783ab77":"## 3.4 Cleaning up the Test and Train data.\nHere we create the actegorical variables the we currentlt know of and get rid of the data that seems irrelevant. ","c840261b":"[back to top](#table-of-contents)\n<a id='6'><\/a>\n# 6. Model Building\nThis section is dedicating to building a model for this problem. I will be attempting to use SVM. If it looks like SVM will take too long, then instead I will be using LGBM. \n\n## 6.1 Preparation","78894424":"[back to top](#table-of-contents)\n<a id='4'><\/a>\n# 4. Features\nThe number of features we have ot work with starting off is `11` but this number might reduce in later models.\n\n## 4.1 Unique values\nCounting the number of unique values for both training and test sets.\n\n### 4.1.1 Preparation\nPreparing test and train data for analysis and visualisation.","b6ac4ca9":"[back to top](#table-of-contents)\n# 4.2 Distribution\nShowing the distribution on each feature that is available in the train and test dataset.SInce there are only a few features that we can plot we will only do one section. `Blue` Represents train data, while `Orange` represents test data. \n- We will be looking at everything still in the dataset apart from`PassengerId`, as this is a unique number so has an even distribution. \n\n- **Observations**\n    - All feature Distributions look similar.","db1be643":"[bak to top](#table-of-contents)\n# 5. Target\nWe are going to look at the distribution of the target. It is a class with only two values 1 or 0. \n## 5.1 Target Distribution\n- **Observations** \n    - as stated earlier we have quite an unbalanced dataset, with the `0` class occupying just over `60%` of the target values","29f6f438":"### 4.1.2 Individual Features\n\nCount how many unique values in each integer feature and perform difference calculations on both train and test datasets to see how they differ from one another. \n\n- **Obeservation**\n    - It looks as if we can use all but `PassengerId` as categorical features.","285468b9":"[back to top](#table-of-contents)\n<a id=\"2\"><\/a>\n# 2. Preparations\nThe code that follows prepares the packages and data used in the analysis and modeling.\n    "}}