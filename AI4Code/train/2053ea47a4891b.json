{"cell_type":{"ec1d73cd":"code","cc8fbe6c":"code","0f9611e0":"code","f5d47870":"code","fe90d302":"code","a5ea41b1":"code","1bbd6742":"code","811cf4c5":"code","1c941fa5":"code","fd4f4e68":"code","839dab18":"code","56e63809":"code","bfa0e2db":"code","8460c8b5":"code","45556739":"code","f6117f29":"code","bb0df9f1":"code","7517d9bf":"code","c0f9f889":"code","1f849004":"code","719e413b":"code","4542b157":"code","85c003cb":"code","dbf0b6a5":"code","16344f3e":"code","fef0f66a":"code","6a69c6e0":"code","9b2402a4":"code","a64e0024":"code","905ca643":"code","cac2e5c6":"code","907b2c7c":"code","1ed6491e":"code","fd51e62c":"code","b68ed87e":"code","2d5d0020":"markdown","e95eb2b2":"markdown","47975834":"markdown","258b469e":"markdown","53019e1f":"markdown","4139a09d":"markdown","135228f8":"markdown","280f199d":"markdown","4b71a553":"markdown","4ad32724":"markdown","aed0dc95":"markdown","f031df3e":"markdown","7edd94bd":"markdown","3bb98da3":"markdown","f192bbb9":"markdown","dc4f0285":"markdown","abe6f238":"markdown","ddea8c4e":"markdown","d548bd74":"markdown","b46d455e":"markdown","d3d71425":"markdown","d22d49a4":"markdown","b9a08047":"markdown","bf99d5d2":"markdown"},"source":{"ec1d73cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc8fbe6c":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\") # Loaded training data\ntrain_data.head()","0f9611e0":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","f5d47870":"gender_rate = train_data[[\"Sex\",\"Survived\"]].groupby(['Sex'], as_index = False).mean()\nprint(gender_rate)","fe90d302":"pclass_dependence = train_data[[\"Pclass\", \"Survived\"]].groupby(['Pclass'], as_index = False).mean()\nprint(pclass_dependence)","a5ea41b1":"SibSp_dependence =  train_data[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index = False).mean()\nprint(SibSp_dependence)","1bbd6742":"ParCh_dependence =  train_data[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index = False).mean()\nprint(ParCh_dependence)","811cf4c5":"g = sns.FacetGrid(train_data, col='Survived')\ng.map(plt.hist, 'Age', bins=20)","1c941fa5":"g1 = sns.FacetGrid(train_data, col = 'Survived', row = 'Pclass', height = 2.2, aspect = 1.6)\ng1.map(plt.hist, 'Age', alpha = 0.8, bins = 20) ","fd4f4e68":"g2 = sns.FacetGrid(train_data, row = 'Embarked', height = 2.2, aspect = 1.6)\ng2.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette = 'deep')\ng2.add_legend()","839dab18":"g3 = sns.FacetGrid(train_data, row = 'Embarked', col = 'Survived', height = 2.2, aspect = 1.6)\ng3.map(sns.barplot, 'Sex', 'Fare', alpha = 0.5, ci = None)\ng3.add_legend()","56e63809":"train_data.head()","bfa0e2db":"# Cabin number and ticket number have too many discrepancies i.e. missing and redundant data. Also there is no intuitive conclusion from these.\n# Hence we drop these features. To maintain consistency we will drop them from both the training and test set.\n# Also as SEC(Socio-Economic Class) is already signified by the Pclass we'll not be analyzing titles as part of the passenger names\ntrain_data = train_data.drop(['Ticket','Cabin'], axis = 1)\ntest_data = test_data.drop(['Ticket','Cabin'], axis = 1)\ntrain_data.head()","8460c8b5":"# converting Sex into numerical values\ncombine = [train_data, test_data]\nfor dataset in combine:\n     # changing male\/female to 0\/1 respectively, map takes dictionary as its parameter\n    dataset['Sex'] = dataset['Sex'].map({'male':0, 'female':1}).astype(int)\n\ntrain_data.head()","45556739":"grid = sns.FacetGrid(train_data, row='Pclass', col='Sex', height=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","f6117f29":"guess_ages = np.zeros((2, 3))\n# make the array that will store the predicted values for each value of Sex and Pclass","bb0df9f1":"for dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & (dataset['Pclass'] == j+1)]['Age'].dropna()\n            # select those rows with the certain value of Sex, Pclass. Then select only the Age column with the null values dropped\n            age_guess = guess_df.median()\n            # get the median value from all the data\n            guess_ages[i, j] = int(age_guess\/0.5 + 0.5) * 0.5\n            # Convert random age float to nearest .5 age\n    # Now that we have made the guess matrix, we need to put the appropriate values from the matrix into the column of age wherever the values are missing.\n    \n    for i in range(0, 2):\n        for j in range(0, 3):\n            # Finding the location of missing values and putting in the calculated average\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),'Age'] = guess_ages[i,j]\n    dataset['Age'] = dataset['Age'].astype(int)\n    \ntrain_data.head()","7517d9bf":"train_data['AgeBand'] = pd.cut(train_data['Age'], 5)\ntrain_data[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index = False).mean().sort_values(by = 'AgeBand')\n# Cut the total range of age into 5 bands\n# We want the columns AgeBand and corresponding survival which has been grouped by the AgeBand and it isn't the index","c0f9f889":"for dataset in combine:\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[ (dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[ (dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[ (dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ (dataset['Age'] > 64), 'Age']","1f849004":"train_data = train_data.drop(['AgeBand'], axis = 1)\ncombine = [train_data, test_data]\ntrain_data.head()","719e413b":"for dataset in combine:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\ntrain_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index = False).mean().sort_values(by='Survived', ascending = False)","4542b157":"train_data = train_data.drop(['Parch', 'SibSp'], axis=1)\ntest_data = test_data.drop(['Parch', 'SibSp'], axis=1)\ncombine = [train_data, test_data]\n\ntrain_data.head()","85c003cb":"freq_port = train_data.Embarked.dropna().mode()[0]\nfor dataset in combine:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ntrain_data.head()","dbf0b6a5":"test_data['Fare'].fillna(test_data['Fare'].dropna().median(), inplace=True)\n# We will now create the FareBand\n\ntrain_data['FareBand'] = pd.qcut(train_data['Fare'], 4)\ntrain_data[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n","16344f3e":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_data = train_data.drop(['FareBand'], axis=1)\ncombine = [train_data, test_data]\n    \ntrain_data.head(10)","fef0f66a":"# Firstly we'll change FamilySize to IsAlone, whether the passenger is travelling alone or not\nfor dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# We can now drop FamilySize\ntrain_data = train_data.drop(['FamilySize'], axis=1)\ntest_data = test_data.drop(['FamilySize'], axis=1)\ncombine = [train_data, test_data]\n\ntrain_data.head()","6a69c6e0":"for dataset in combine:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass","9b2402a4":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train_data['Title'], train_data['Sex'])\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n \t'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain_data.head()\n\ntrain_data = train_data.drop(['Name', 'PassengerId'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)\ncombine = [train_data, test_data]","a64e0024":"X_train = train_data.drop(\"Survived\", axis=1)\nY_train = train_data[\"Survived\"]\nX_test  = test_data.drop(\"PassengerId\", axis=1).copy()","905ca643":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","cac2e5c6":"svc = SVC()\nsvc.fit(X_train, Y_train)\nY_pred = svc.predict(X_test)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","907b2c7c":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","1ed6491e":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","fd51e62c":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","b68ed87e":"output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': Y_pred})\noutput.to_csv('my_submission.csv', index=False)","2d5d0020":"The above information didn't give a converge for neither Logistic Regression nor SVM's. Hence we need more features.","e95eb2b2":"**Arguments of FacetGrid**\n\nFacetGrid is used to visualize combined features. We want to see Ticket-class-wise how many survived or not in each age group. So our numbers change over whether one survived or not, hence that's the column. We want to see these numbers Ticket class wise and hence Pclass is the row argument. There would be no problem in changing the rows and columns in this case as both would complete the purpose of intuition. Height changes the height of the plot. And aspect decide the height over width of the plot ratio.\n\n**Arguments of map**\n    1. plt.hist signifies the type of the plot i.e. histogram\n    2. Second argument is the value being plotted on the x-axis while the y-axis gives the count\n    3. Alpha is colour related, more the alpha the darker it is. Lies between 0 and 1.\n    4. Bins defines the number of columns in a unit length. Hence more the number of bins, thinner the bars in the histogram plot.\n    ","47975834":"Now that we have a feature called family Size, we will be dropping SibSp and Parch","258b469e":"# Decision Tree","53019e1f":"Age could be a factor but as its a numerical value unlike the previous ones which were categorical, we will have to visualize it to analyze it. Intuition: Ages could be affected band wise, For eg its more likely for infants and the elderly to be alive as they would get special attention.","4139a09d":"We are almost ready to run our ML model but that can be done on only numerical values. Hence we need to convert all our categorical data into numerical values.","135228f8":"Now based on these bands we will simply assign numeric values to the bands.","280f199d":"For Fare we will be manipulating it exactly as done for the Age feature.","4b71a553":"Let us start by preparing an empty array to contain guessed Age values based on Pclass x Gender combinations. Now we iterate over Sex (0 or 1) and Pclass (1, 2, 3) to calculate guessed values of Age for the six combinations.","4ad32724":"Till this step we didn't get convergence in Logistic Regression or SVM. Both asked for one more column entry. So another feature that could be used is the Title of people in their names. We need to extract them and classify them into Rare ones like Countess, Capt. and the common ones like Mr. \nRare ones will usually belong to higher SEC.","aed0dc95":"Hence confirmed direct dependence parameters:\n    1. Sex\n    2. Ticket Class\n    3. Number of siblings\/Spouses\nThere doesn't seem to be a direct dependence of the number of Parents\/Children parameter.","f031df3e":"# SVM","7edd94bd":"# Logistic Regression\n\n","3bb98da3":"Clearly gender played a role in deciding who survived the Titanic disaster. This makes sense because women and children are always first to be put on a lifeboat. Now the other factors could be the ticket class which signifies Socio-Economic status and a lesser intuitive one, whether they had other family members on board. We shall analyze the data to confirm these intuitions.","f192bbb9":"#  **So the steps we have covered till now**\n    1. Finding factors of dependence, i.e. which clearly affect the outcome of survivability of a passenger.\n    2. Find correlations between these factors. First we found it between Age, Pclass and Survival. Then Embark location, Pclass, Sex and Survival. Clearly we have established that indeed these are correlated.\n    3. The numerical values from Fare is still left. And Fare evidently signfies the Pclass of a passenger. Hence we need to find this correlation too. We will band the Fare just like we did it for age.\n    ","dc4f0285":"# KNN","abe6f238":"Now as it seems that where the passenger embarked from could be a factor, let us analyze that too. We will have to separately do this for males and females. We do not bring age into this as the correlation between age and PClass has already been established. We are now trying to correlate the rest.\n","ddea8c4e":"We can now remove the AgeBand feature.","d548bd74":"# **Create new feature combining existing features**\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.","b46d455e":"# Random Forest","d3d71425":"Now all missing values need to be completed i.e. in Age column there are some missing values. We need to fill them up with values predicted from trends in the rest of the data. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using median values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...","d22d49a4":"We are done with finding correlations. Now we edit out unnecessary data and combine certain parameters.","b9a08047":"Embarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance. We also change our categorical values to numeric.","bf99d5d2":"Now that we have all the values, we can start towards preparing our data for applying our machine learning model. As the model would take only numerical values we need to change all columns into integers. But first we cant analyze with so many different discrete data for ages, hence we will categorize them into age bands.\n"}}