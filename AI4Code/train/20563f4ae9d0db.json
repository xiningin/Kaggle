{"cell_type":{"b4bd54b7":"code","6929915b":"code","6f19df7d":"code","d9cc33ed":"code","fae8f2a8":"code","e47991a2":"code","3ae2b624":"code","4e63e7c1":"code","51349e8a":"code","8d8cf63d":"code","f2b48004":"code","e1a5fb01":"code","d6a35f81":"code","08895e67":"code","951bdadf":"code","2d094aee":"code","c964d0bd":"code","8ed9a08e":"code","7f5daeb5":"code","9cd56a8e":"code","2b812fb5":"code","707c47bf":"code","a1ff5cdb":"code","8f17fac8":"code","a027f163":"code","8a9d3f00":"code","157e15ff":"code","9ffd3962":"code","1ef875a0":"code","216e5ed8":"code","5db993d7":"code","ba48f40d":"code","c4575620":"code","7cff866b":"code","218ca2a2":"code","68bfd879":"code","484ad883":"code","0e4d40de":"code","0d15d937":"code","38722d65":"code","7f727040":"markdown","0245178b":"markdown","f10046e0":"markdown","d0b62a47":"markdown","04687f03":"markdown","3f54e943":"markdown","8a8a587a":"markdown","66187099":"markdown","a685a8ef":"markdown","36431821":"markdown","1f789dd2":"markdown","3462752f":"markdown","78dcad42":"markdown","0c01aeed":"markdown","cb4dd36a":"markdown","55111594":"markdown","b0ce919d":"markdown","237b149c":"markdown","99dbb7f0":"markdown","3dfb39dd":"markdown","005efd3d":"markdown","1ef0cfef":"markdown","b223538e":"markdown","4affc59a":"markdown","64eb78ac":"markdown","182953ed":"markdown","187b6bdb":"markdown","8ad2ddf0":"markdown","eaf60b50":"markdown"},"source":{"b4bd54b7":"# Importing Packages\nimport warnings\nwarnings.filterwarnings(\"ignore\") \nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\nfrom sklearn.model_selection import train_test_split\nimport urllib.request\nimport re\nimport nltk\nnltk.download('stopwords', quiet=True)\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.naive_bayes import MultinomialNB,GaussianNB\nfrom sklearn.metrics import accuracy_score,confusion_matrix,classification_report\nfrom sklearn.linear_model import PassiveAggressiveClassifier,LogisticRegression\nfrom gensim.models import Word2Vec\nfrom sklearn.linear_model import PassiveAggressiveClassifier,LogisticRegression\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout,Input,SpatialDropout1D, Bidirectional\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow import keras\nfrom nltk.stem import WordNetLemmatizer\nfrom imblearn.over_sampling import SMOTE","6929915b":"# Reading Data\ndf = pd.read_csv('..\/input\/twitter-sentiment-dataset\/Twitter_Data.csv')","6f19df7d":"# Data Sample\ndf.sample(5)","d9cc33ed":"# Checking for NA Values\ndf.isnull().sum()","fae8f2a8":"# Distribution of different classes in sentiment\ndef count_values_in_column(data,feature):\n    total=data.loc[:,feature].value_counts(dropna=False)\n    percentage=round(data.loc[:,feature].value_counts(dropna=False,normalize=True)*100,2)\n    return pd.concat([total,percentage],axis=1,keys=[\"Total\",\"Percentage\"])\ncount_values_in_column(df,\"category\")","e47991a2":"# Segrating based on different sentiments\ndf_negative = df[df[\"category\"]==-1]\ndf_positive = df[df[\"category\"]==1]\ndf_neutral = df[df[\"category\"]==0]","3ae2b624":"# create data for Pie Chart\nplt.figure(figsize=(13, 8), dpi=80)\npichart = count_values_in_column(df,\"category\")\nnames= [\"Positive\",\"Neutral\",\"Negative\",\"Nan\"]\nsize=pichart[\"Percentage\"]\n \n# Create a circle for the center of the plot\nmy_circle=plt.Circle( (0,0), 0.5, color='white')\nplt.pie(size, labels=names, colors=['green','blue','red',\"yellow\"])\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.show()\n","4e63e7c1":"# Function to Create Wordcloud\ndef create_wordcloud(text,path):\n    stopwords = set(STOPWORDS)\n    wc = WordCloud(background_color=\"white\",\n    max_words=3000,\n    stopwords=stopwords,\n    random_state=42,\n    width=900, height=500,\n    repeat=True)\n    wc.generate(str(text))\n    wc.to_file(path)\n    print(\"Word Cloud Saved Successfully\")\n    path=path\n    display(Image.open(path))","51349e8a":"# Wordcloud for all tweets\nplt.figure(figsize=(15, 8), dpi=80)\ncreate_wordcloud(df['clean_text'].values,\"all.png\")","8d8cf63d":"# Wordcloud for only positive tweets\nplt.figure(figsize=(15, 8), dpi=80)\ncreate_wordcloud(df_positive['clean_text'].values,\"positive.png\")","f2b48004":"# Wordcloud for only negative tweets\nplt.figure(figsize=(15, 8), dpi=80)\ncreate_wordcloud(df_negative['clean_text'].values,\"negative.png\")","e1a5fb01":"# Wordcloud for only neutral tweets\nplt.figure(figsize=(15, 8), dpi=80)\ncreate_wordcloud(df_neutral['clean_text'].values,\"neutral.png\")","d6a35f81":"# Stemming\nps = PorterStemmer()\n# Initializing Lists\ncorpus = []\nwords = []\nfor i in range(0, len(df)):\n    # Removing characters other than letters\n    review = re.sub('[^a-zA-Z]', ' ', str(df[\"clean_text\"][i]))\n    # Lowering the case all the text\n    review = review.lower()\n    # Splitting into words\n    review = review.split()\n    # Applying Stemming\n    stemmed = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    # Joining words\n    review = ' '.join(stemmed)\n    # Appending all tweets to a list after preprocessing\n    corpus.append(review)\n    # Appending all words for word embeddings\n    words.append(stemmed)","08895e67":"# Corpus sample\ncorpus[1:10]","951bdadf":"# Length \nprint(\"Legth of Corpus:\",len(corpus))","2d094aee":"# Updating created corpus in our dataframe\ndf[\"clean_text\"] = corpus","c964d0bd":"# Dropping NA Values and resetting index\ndf = df.dropna()\ndf = df.reset_index()","8ed9a08e":"# Checking for NA Values after corpus updations\ndf.isna().sum()","7f5daeb5":"# Exporting stemmed sentences\ndf[[\"clean_text\",\"category\"]].to_csv(\"stemmed.csv\",index = False)","9cd56a8e":"# Loading the stemmed sentences\ndf_stemmed = pd.read_csv(\"stemmed.csv\")\n# Extracting corpus\ncorpus = list(df_stemmed[\"clean_text\"])","2b812fb5":"# Applying TFIDF Vectorization\ntfidf = TfidfVectorizer(max_features=5000,ngram_range=(1,3))\nX_tfidf = tfidf.fit_transform(df[\"clean_text\"]).toarray()","707c47bf":"# Independent Variable\nX = df_stemmed[\"clean_text\"]\n# Dependent Varible\nY=df_stemmed[\"category\"]","a1ff5cdb":"df_tfidf = pd.DataFrame(X_tfidf,columns = tfidf.get_feature_names())\ndf_tfidf[\"output\"] = Y\ndf_tfidf.head()\n","8f17fac8":"# Train test Split\nX_train_tfidf,X_test_tfidf,Y_train_tfidf,Y_test_tfidf = train_test_split(X_tfidf,Y,test_size=0.33,random_state = 27)","a027f163":"# Initializing Model\nclassfier_tfidf = MultinomialNB(alpha=0.1)\n# Fitting data\nclassfier_tfidf.fit(X_train_tfidf,Y_train_tfidf)\n# Prediction on test data\nY_pred_tfidf = classfier_tfidf.predict(X_test_tfidf)","8a9d3f00":"# Initializing Model\nlogistic_tfidf = LogisticRegression(solver='liblinear')\n# Fitting data\nlogistic_tfidf.fit(X_train_tfidf,Y_train_tfidf)\n# Prediction on test data\nY_pred_logistic_tfidf = logistic_tfidf.predict(X_test_tfidf)","157e15ff":"acc_log_tfidf = accuracy_score(Y_test_tfidf,Y_pred_logistic_tfidf)\nclassification_log_tfidf = classification_report(Y_test_tfidf,Y_pred_logistic_tfidf)\nconfusion_matrix_log_tfidf = confusion_matrix(Y_test_tfidf,Y_pred_logistic_tfidf)","9ffd3962":"print(\"For Logistic Regression: \\n\")\nprint(\" \\n Accuracy : \",acc_log_tfidf,\"\\n\",\"Classification report \\n\",classification_log_tfidf,\"\\n\",\"Confusion matrix \\n\",confusion_matrix_log_tfidf)","1ef875a0":"acc_tfidf = accuracy_score(Y_test_tfidf,Y_pred_tfidf)\nclassification_tfidf = classification_report(Y_test_tfidf,Y_pred_tfidf)\nconfusion_matrix_tfidf = confusion_matrix(Y_test_tfidf,Y_pred_tfidf)","216e5ed8":"print(\"For Mutinomial Naive Bayes: \\n\")\nprint(\" \\n Accuracy : \",acc_tfidf,\"\\n\",\"Classification report \\n\",classification_tfidf,\"\\n\",\"Confusion matrix \\n\",confusion_matrix_tfidf)","5db993d7":"# Cloning tweets\nmessages=X.copy()","ba48f40d":"# Setting parameter for padding and sequential modelling\nvoc_size = 5000\nembedding_vector_features = 200\nsent_length = 200\nlstm_out = 128","c4575620":"# Tokenization of all words in the vocabulary for all tweets\nonehot_repr=[one_hot(words,voc_size)for words in df[\"clean_text\"]]\nonehot_repr[1:3]","7cff866b":"# Applying Post Padding\nembedded_docs=pad_sequences(onehot_repr,padding='post',maxlen=sent_length)\nprint(embedded_docs)","218ca2a2":"# Changing Negative values to positive\nY = [2 if x == -1 else x for x in Y]\n\n# Converting list to arrays\nX_final=np.array(embedded_docs)\ny_final=np.array(Y)","68bfd879":"# Shape of X,Y\nX_final.shape,y_final.shape","484ad883":"# Train Test Split\nX_train_embed, X_test_embed, Y_train_embed, Y_test_embed = train_test_split(X_final, y_final, test_size=0.33, random_state=27)","0e4d40de":"# Sequential Model\nmodel = Sequential()\n\n# Input layer\nmodel.add(Input(shape=(None,)))\n\n# Embedding layer\nmodel.add(Embedding(voc_size, embedding_vector_features, input_length = sent_length))\n\n# LSTM layer\nmodel.add(Bidirectional(LSTM(lstm_out, dropout=0.2)))\n\n# Fully connected layer\nmodel.add(Dense(128, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation = 'relu'))\n\n# Output layer\nmodel.add(Dense(3, activation = 'softmax'))\n\n# model.summary()\nmodel.summary()","0d15d937":"# Compling model and running\nmodel.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\nhistory = model.fit(X_train_embed,Y_train_embed,validation_data=(X_test_embed,Y_test_embed),\n                    epochs=2, batch_size=256,\n                    verbose=1,shuffle=True)","38722d65":"predictions = model.predict(X_test_embed)\nprint(history.history)","7f727040":"## Insights\n<font color = \"brown\" size = 4><ul>\n    <li>From all these wordclouds of different classes, we can <font color = \"red\"><b>MODI<\/b><\/font> is the common words used <\/li>\n    <li>From positive sentiment wordclouds, we can see that <font color = \"red\"><b>WELCOME, SOLDIERS<\/b><\/font> are the most common words used<\/li>\n    <li>From negative sentiment wordclouds, we can see that <font color = \"red\"><b>ANTI, CRIMINAL, KILLING<\/b><\/font> are the most common words used<\/li>\n    <li>From neutral sentiment wordclouds, we can see that <font color = \"red\"><b>INDIA, PROJECT,CONTINUE<\/b><\/font> are the most common words used<\/li>\n    <\/ul>\n<\/font>","0245178b":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Contents<\/font><\/h4>\n    <\/strong>\n<\/div>\n<font size = 3.5 color = \"blue\">\n    <ol>\n    <br><br><li>Importing Packages<\/li><br>\n    <li>Importing Data<\/li><br>\n    <li>Analysing Data<\/li><br>\n        <ol> <font color = \"red\">\n            <li> Data Overview<\/li>\n            <li> Checking for NA<\/li>\n            <li> Finding out Distribution of different sentiments <\/li>\n            <\/font><\/ol><br>\n    <li>Data Visualization<\/li><br>\n        <ol> <font color = \"red\">\n            <li> Distribution of different sentiments<\/li>\n            <li> Word cloud on different sentiments<\/li>\n            <\/font><\/ol><br>\n    <li>Data Preprocessing<\/li><br>\n        <ol> <font color = \"red\">\n            <li> Tokenization<\/li>\n            <li> Removing unnecessary punctuation, tags<\/li>\n            <li>Converting all letters to lower case<\/li>\n            <li>Stemming<\/li>\n            <\/font><\/ol><br>\n    <li>Feature Extraction<\/li><br>\n         <ol> <font color = \"red\">\n            <li> Bag of words<\/li>\n            <li> TF-IDF Vectorizer<\/li>\n            <\/font><\/ol><br>\n    <li>Training Models<\/li><br>\n         <ol> <font color = \"red\">\n            <li>Naive Bayes<\/li>\n            <li>Logistic Regression<\/li>\n            <\/font><\/ol><br>\n    <li>Evaluation Metrics<\/li><br>\n    <li>Sequential Modelling<\/li><br>\n    <li>Predictions<\/li><br>\n    <\/ol>\n<\/font>","f10046e0":"### Multinomial Naive Bayes","d0b62a47":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Problem Statement<\/font><\/h4>\n    <\/strong>\n<\/div><br>\n<font size = 3.9 color = \"brown\">Classify the tweets by implementing any NLP approach for Sentiment analysis on the provided dataset. The objective is to recognize whether the given tweet is oriented as negative (-1), neutral (0), or positive (1) tone. Focus majorly on unique preprocessing techniques.<\/font>\n\n\n","04687f03":"<center><img src = \"https:\/\/res.cloudinary.com\/qna\/image\/upload\/v1635170410\/sentiment-points.a502b2c_pyfy2i.png\" width = 500><\/center>","3f54e943":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Context<\/font><\/h4>\n    <\/strong>\n<\/div><br>\n<font size = 3.9 color = \"brown\">Sentiment analysis studies the subjective information in an expression, that is, the opinions, appraisals, emotions, or attitudes towards a topic, person or entity.The dataset has three sentiments namely, negative(-1), neutral(0), and positive(+1).<\/font>","8a8a587a":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>6. Feature Extraction<\/font><\/h4>\n    <\/strong>\n<\/div>\n","66187099":"### Logistic Regression","a685a8ef":"<font color = \"brown\" size = 4>In text processing, words of the text represent discrete, categorical features. How do we encode such data in a way which is ready to be used by the algorithms? The mapping from textual data to real valued vectors is called feature extraction. One of the simplest techniques to numerically represent text is Bag of Words & TFIDF Vectorizer.<br><font>\n## Bag of Words (BOW) or Count Vectorizer: \n<font color = \"brown\" size = 4>We make the list of unique words in the text corpus called vocabulary. Then we can represent each sentence or document as a vector with each word represented as 1 for present and 0 for absent from the vocabulary.<\/font>\n## TF-IDF Vectorizer: \n<font color = \"brown\" size = 4>Here, I use TF-IDF Vectorizer for vectorization.The algorithm Term Frequency & Inverse Document Frequency.<br><ul><li>Term Frequency (TF) = (Number of times term t appears in a document)\/(Number of terms in the document)<\/li><br><li>Inverse Document Frequency (IDF) = log(N\/n), where, N is the number of documents and n is the number of documents a term t has appeared in. The IDF of a rare word is high, whereas the IDF of a frequent word is likely to be low. Thus having the effect of highlighting words that are distinct.<\/li><br><li>We can calculate TF-IDF value of a term as = TF * IDF<\/li>\n    <\/ul><\/font>","36431821":"<font color = \"brown\" size = 4> Very Less NA Values\ud83d\ude0a. We can drop them<\/font>","1f789dd2":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>10. Predictions<\/font><\/h4>\n    <\/strong>\n<\/div>\n","3462752f":"## Insights\n<font color = \"brown\" size = 4><ul>\n    <li>In this data, we have more than 40% positive tweets<\/li>\n    <li>Negative Tweets are with low numbers and only 50% in count compared to positive tweets<\/li>\n    <li>Neutral Tweets have a good number in total between positive & negative tweet count<\/li><\/ul>\n<\/font>","78dcad42":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>Data Overview<\/font><\/h4>\n    <\/strong>\n<\/div><br>\n<font size = 3.5 color = \"brown\">The dataset contains information about<br><br>\n    <font color = \"red\">\n        <ol>\n    <li>Tweet - Message Tweeted<\/li><br>\n    <li>label - Sentiment for the tweet<\/li>\n        <\/ol>\n    <\/font>\n<\/font>\n","0c01aeed":"<font color = \"brown\" size = 4><b>For Logistic Regression, we get 83% Accuracy with liblinear as a solver><\/b><\/font> ","cb4dd36a":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>1. Importing Packages<\/font><\/h4>\n    <\/strong>\n<\/div>","55111594":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>7. Data Modelling<\/font><\/h4>\n    <\/strong>\n<\/div>\n","b0ce919d":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>3. Analysing Data<\/font><\/h4>\n    <\/strong>\n<\/div>\n","237b149c":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>8. Evaluation Metrics<\/font><\/h4>\n    <\/strong>\n<\/div>\n","99dbb7f0":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>9. Sequential Modelling<\/font><\/h4>\n    <\/strong>\n<\/div>\n","3dfb39dd":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>2. Importing Data<\/font><\/h4>\n    <\/strong>\n<\/div>","005efd3d":"<font color = \"brown\" size = 4>Sequence modelling is a technique where a neural network takes in a variable number of sequence data and output a variable number of predictions. The input is typically fed into a recurrent neural network (RNN).<br><br>\nIn theory, RNNs are absolutely capable of handling such \u201clong-term dependencies.\u201d A human could carefully pick parameters for them to solve toy problems of this form. Sadly, in practice, RNNs don\u2019t seem to be able to learn them. <br><br>\nLSTM's can solve the problem here<br><br>\nLSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!<br><br>\nLSTMs also have this chain like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four, interacting in a very special way.<br><br><br>\n    <img src = \"https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/img\/LSTM3-chain.png\"><font>","1ef0cfef":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>5. Data Preprocessing<\/font><\/h4>\n    <\/strong>\n<\/div>\n","b223538e":"<font color = \"brown\" size = 4><b>For Mutinomail NB, we get 70% Accuracy with alpha = 0.1<\/b><\/font> ","4affc59a":"### Thank You \ud83e\udd17\n### I hope you had a good time reading my notebook. Pls do support and comment! \ud83d\ude0e","64eb78ac":"<div style=\"background:#c72e57;color:#fff;padding:1em 2em 1.5em 2em;border-radius: 3px;font-weight:bold\">\n    <strong>\n        <h4 style = \"color:#fff\"><font size = 4>4. Data Visualization<\/font><\/h4>\n    <\/strong>\n<\/div>\n","182953ed":"## Steps\n<ul>\n<font color = \"brown\" size = 4><li>Tokenization \u2014 convert sentences to words<\/li>\n<li>Removing unnecessary punctuation, tags<\/li>\n<li>Removing stop words \u2014 frequent words such as \u201dthe\u201d, \u201dis\u201d, etc. that do not have specific semantic<\/li>\n<li>Converting all letters to lower case<\/li>\n<li>Stemming \u2014 words are reduced to a root by removing inflection through dropping unnecessary characters, usually a suffix.<\/li><br><\/font>\n<font color = \"red\" size = 4>The stemmed form of studies is: <b>studi<\/b><br>\nThe stemmed form of studying is: <b>study<\/b>     \n    \n<\/font>\n<\/ul>\n    ","187b6bdb":"<center><h1 class=\"list-group-item list-group-item-success\">Twitter Sentiment Analysis<\/h1><\/center>","8ad2ddf0":"### Parameters of wordcloud function<br>\n<font color = \"brown\" size = 3.5>\n<li>background_color = Color of background<\/li><br>\n<li>max_words = The maximum number of unique words used<\/li><br>\n<li>stopwords = stopword list<\/li><br>\n<li>max_font_size = Maximum font size<\/li><br>\n<li>random_state = To ensure that random numbers are generated in the<\/li><br>\n<li>same order, so the results will be the same even if generated several times<\/li><br>\n<li>width = width size of the output<\/li><br>\n<li>height = height size of the output<\/li><br><\/font>","eaf60b50":"<center><img src=\"https:\/\/miro.medium.com\/proxy\/1*_JW1JaMpK_fVGld8pd1_JQ.gif\"><\/img><\/center><br>"}}