{"cell_type":{"6d04cec7":"code","e4914ff2":"code","5ab85c99":"code","c555bf44":"code","ee77aedf":"code","ac56567f":"code","e0690dc5":"code","568515b1":"code","9d7fef54":"markdown","9d517c3d":"markdown","313f179f":"markdown","12249ef5":"markdown","c1c751ea":"markdown","660052b4":"markdown","bd5c9d42":"markdown","db2cbd22":"markdown"},"source":{"6d04cec7":"import numpy as np \nfrom scipy.special import expit, logit\nimport pandas as pd\nimport matplotlib.pyplot as plt","e4914ff2":"def Cq(score, noise=0.18):\n    '''\n    score - public score, e.g. AUC, accuracy\n    noise - dataset estimated noise level\n    return - evaluation how near the score is to the theoretical limit \n             of dataset with given noise level\n    '''\n    return logit((score-noise)\/((1 - 2 * noise)))","5ab85c99":"noise=0.18\n\nscore = np.linspace(0, 1, 100001)\ny = Cq(score, noise)\nplt.plot(score, y)\nplt.grid()\nplt.ylim(-9, 9)\nplt.xlabel('score')\nplt.ylabel('quality')\nplt.title(f'Classification quality (score), noise={noise}')\nplt.show()","c555bf44":"def improvement(score1, score2, noise):\n    # use 100* for readability of result\n    return 100 * (Cq(score2, noise) - Cp(score1, noise))","ee77aedf":"noise = 0.18\ndelta = 0.0001\nscore = np.linspace(noise, 1-noise+delta, 1001)\nimp = Cq(score, noise) - Cq(score-delta, noise)\nplt.plot(score, imp)\nplt.grid()\n# plt.xlim(-6, 6)\nplt.xlabel('score')\nplt.ylabel('improvement')\nplt.title(f'Classification quality improvement, delta(score)={delta}')\nplt.show()","ac56567f":"noise = 0.18\n  \nscore1, score2 = 0.81800, 0.81804\nprint(f'Improvement from {score1:.5f} to {score2:.5f} is {improvement(score1, score2, noise):.2f}')\n\nscore1, score2 = 0.81835, 0.81839\nprint(f'Improvement from {score1:.5f} to {score2:.5f} is {improvement(score1, score2, noise):.2f}')\n\nscore1, score2 = 0.81880, 0.81884\nprint(f'Improvement from {score1:.5f} to {score2:.5f} is {improvement(score1, score2, noise):.2f}')\n\nscore1, score2 = 0.81995, 0.81999\nprint(f'Improvement from {score1:.5f} to {score2:.5f} is {improvement(score1, score2, noise):.2f}')","e0690dc5":"leaderboard = [\n    (1, 'Laurent Pourchot', 0.81882),\n    (2, 'Youri Matiounine', 0.81880),\n    (3, 'Vasileios Konstantakos', 0.81877),\n    (4, 'Hikmet Sezen', 0.81869),\n    (5, 'Ivan Kontic', 0.81867),\n    (6, 'Martynov Andrey', 0.81861),\n    (7, 'kailai', 0.81861),\n    (8, 'Mohammad Hatoum', 0.81860),\n    (9, 'Oscar Takeshita', 0.81860),\n    (10, 'lilkaskitc', 0.81859),\n    (11, 'DLastStark', 0.81858),\n    (12, 'Dawid Sroczyk', 0.81856),\n    (13, 'Towhidul.Tonmoy', 0.81854),\n    (14, 'Javier Vallejos', 0.81853),\n    (15, 'Realtimshady', 0.81853),\n    (16, 'OmarVivas', 0.81853),\n    (17, 'GB4 GB4', 0.81852),\n    (18, 'Octaplus', 0.81852),\n    (19, 'hiroki', 0.81851),\n    (20, 'aura_tks', 0.81851),\n]\n\ntmp = []\nfor rec in leaderboard:\n    item = {}\n    item['place'] = rec[0]\n    item['name'] = rec[1]\n    item['score'] = rec[2]\n    item['delta score'] = 0.81882 - rec[2]\n    item['Cq'] = Cq(rec[2], noise)\n    item['delta Cq'] = Cq(0.81882, noise) - Cq(rec[2], noise)\n    tmp.append(item)\n    \nlb = pd.DataFrame(tmp)\nlb\n","568515b1":"import seaborn as sns\n\nfig, ax = plt.subplots(1, 2, figsize=(25, 6)) \nsns.barplot(data=lb, y='name', x='score', ax=ax[0])\nsns.barplot(data=lb, y='name', x='Cq', ax=ax[1])\nax[0].set_title('public score')\nax[1].set_title('Cq')","9d7fef54":"# How much is the score improvement?\n\n> The problem : In a classification competition with evaluation metric, say, 'AUC', I improved my public score from 0.81835 to 0.81839. Is it much? \n\n> Sure the higher the score, the harder to improve. But how much better the improvement from 0.81861 to 0.81865 than from 0.60000 to 0.60004?\n\nIt would be nice to have a metric of score improvement. What could it be?\n\nEvery dataset has noise and noise level (maybe zero). So the is theoretical limit of any model capacity, see, for example this old [article](https:\/\/www.aaai.org\/Papers\/KDD\/1995\/KDD95-007.pdf)\n\n[![image.png](attachment:ba91f2d2-e062-425f-99f2-8dd8d0a0126d.png)](http:\/\/)","9d517c3d":"Let's apply **Cp** function to the September 2021 leaderboard (sept 26, 12:00)","313f179f":"#### Some examples","12249ef5":"First, we need to estimate dataset noise level. For example, for September 2021 TPS let's set it to **0.18**. I.e. the theoretical limit of the dataset is estimated as **0.82**\n\nHow Cq depends of score? Let's see.","c1c751ea":"The metric should estimate how near is the score to the theoretical limit. IMHO scipy.special.logit will do as a base function.\n\nLet's define **Cq** (classification quality) function.","660052b4":"Now the differense is visible.\n\n**Please upvote if you like it**","bd5c9d42":"let's see how the improvement with given delta(score) depends of score.","db2cbd22":"The **improvement** function to measure the improvement from lower to higher score."}}