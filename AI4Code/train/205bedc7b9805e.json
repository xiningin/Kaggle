{"cell_type":{"4bc9b857":"code","9b252300":"code","c825fab3":"code","11ac8370":"code","43f156d4":"code","6f157a68":"code","bf6cfad9":"code","6d051100":"code","13709b0d":"code","ca0b4aa8":"code","d6ceb388":"code","0c753ab2":"code","e8916c33":"code","d94a5684":"code","6f808764":"code","4e5d0b97":"code","739b41fd":"code","55bce6cf":"code","a118883c":"code","4d963e90":"code","8479d9b8":"code","4ab880b1":"code","3d32871b":"code","85f270e2":"markdown","18ea635c":"markdown","50e2b6ea":"markdown","3a354077":"markdown","270ef403":"markdown","370c1e77":"markdown","0f40547d":"markdown"},"source":{"4bc9b857":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport torch\nimport keras\nimport glob\nimport cv2\nfrom albumentations import *\nfrom tqdm import tqdm_notebook as tqdm\nimport gc\n\nimport warnings\nwarnings.filterwarnings('ignore')\nPATH = '..\/input\/deepfake-detection-challenge\/'\nprint(os.listdir(PATH))","9b252300":"TRAIN_PATH = 'train_sample_videos\/*.mp4'\nTEST_PATH = 'test_videos\/*.mp4'\ntrain_img = glob.glob(os.path.join(PATH, TRAIN_PATH))\ntest_img = glob.glob(os.path.join(PATH, TEST_PATH))","c825fab3":"def gather_info(train_img):\n    train_img = os.path.join(PATH, f\"train_sample_videos\/{train_img}\")\n\n    cap = cv2.VideoCapture(train_img)\n\n    success, image = cap.read()\n    count = 0\n    first_trn_image = None\n\n    while success:\n        try:\n            success, image = cap.read()\n            if count == 0:\n                first_trn_image = image\n            x, y, z = image.shape\n\n        except:\n            break\n        count += 1\n        \n    cap.release()\n    cv2.destroyAllWindows()\n    \n    return [x,y,z,count], first_trn_image","11ac8370":"meta = glob.glob(os.path.join(PATH, 'train_sample_videos\/*.json'))\nlabel_df = pd.read_json(meta[0]).transpose().reset_index()\nidx = label_df['index']\nimage_meta = []\nfirst_trn_images = []","43f156d4":"label_df.head()","6f157a68":"# gather training image dimensions\nfor i in tqdm(range(len(idx))):\n    test, first = gather_info(idx.values[i])\n    image_meta.append(test)\n    first_trn_images.append(first)","bf6cfad9":"# Display the first image(frame) of each video(.mp4) in sample train\nfor img, lbl in zip(first_trn_images[:100], label_df['label'].values[:100]):\n    # cv2's native channel is BGR, we need to convert to RGB\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.title(f'{lbl}')\n    plt.imshow(img)\n    plt.show()\n    \ngc.collect()","6d051100":"image_meta = np.array(image_meta, dtype='int16')\nlabel_df['height'] = image_meta[:,0]\nlabel_df['width'] = image_meta[:,1]\nlabel_df['channel'] = image_meta[:,2]\nlabel_df['frames'] = image_meta[:,3]","13709b0d":"label_df.head()","ca0b4aa8":"label_df['height'].value_counts()","d6ceb388":"label_df['width'].value_counts()","0c753ab2":"label_df['channel'].value_counts()","e8916c33":"label_df['frames'].value_counts()","d94a5684":"# Sample Train Target Distribution\nplt.title('Count of each ship type')\nsns.countplot(y=label_df['label'].values)\nplt.show()","6f808764":"def gather_test(test_img, i):\n    cap = cv2.VideoCapture(test_img[i])\n\n    success, image = cap.read()\n    count = 0\n    first_tst_image = None\n\n    while success:\n        try:\n            success, image = cap.read()\n            if count == 0:\n                first_tst_image = image\n            x, y, z = image.shape\n\n        except:\n            break\n        count += 1\n        \n    cap.release()\n    cv2.destroyAllWindows()\n    \n    return [x,y,z,count], first_tst_image","4e5d0b97":"test_image_meta = []\nfirst_tst_images = []","739b41fd":"# gather test image dimensions\nfor i in tqdm(range(len(test_img))):\n    test, first = gather_test(test_img, i)\n    test_image_meta.append(test)\n    first_tst_images.append(first)","55bce6cf":"# Display the first image(frame) of each video(.mp4) in test\nfor img in first_tst_images[:100]:\n    # cv2's native channel is BGR, we need to convert to RGB\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.show()\n    \ngc.collect()","a118883c":"test_image_meta = np.array(test_image_meta)\ntest_meta = pd.DataFrame({'height':test_image_meta[:,0],\n                          'width':test_image_meta[:,1],\n                          'channel':test_image_meta[:,2],\n                          'frames':test_image_meta[:,3]})\ntest_meta.head()","4d963e90":"test_meta['height'].value_counts()","8479d9b8":"test_meta['width'].value_counts()","4ab880b1":"test_meta['channel'].value_counts()","3d32871b":"test_meta['frames'].value_counts()","85f270e2":"# Quick EDA and visualization of Deepfake Detection Challenge\n\nLets take a quick look at sample training set.","18ea635c":"# Public Test","50e2b6ea":"## Top 100 Sample Train","3a354077":"# Sample Train","270ef403":"# Summary\n\n* There seems to be two resolutions in both sample train and public test: 1080x1920 and 1920x1080. \n* The number of frames per video is 297\/299 and 298\/299 for train, test respectively, with 299 being the most frequent.\n\n* Right now it's not clear whether private test will be the same distribution.\n\n* What is clear is that this challenge will require a large amount of resources, judging by the size of the frame resolutions.","370c1e77":"## Top 100 Public Test","0f40547d":"## Load Metadata"}}