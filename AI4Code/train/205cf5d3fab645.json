{"cell_type":{"c9623277":"code","dcbcec4d":"code","8f9ca427":"code","dcbc7f42":"code","9dc691d3":"code","b7afb139":"code","c03be32a":"code","e5d6a70c":"code","8c52db0b":"code","7904a979":"code","cbaa6c6c":"code","ffb1a8ef":"code","f73e135a":"code","a23d8f23":"code","d335c63c":"code","d1095bcd":"code","5b4e3026":"code","f265bc52":"code","8bf7b0ea":"code","d897b34e":"code","d920a0e7":"code","5b70071a":"code","abe410d4":"code","db442562":"code","3aa6c5e3":"code","d223d357":"code","754666ab":"code","24ca2331":"code","3c101658":"code","b7425fa6":"code","22ac9a6d":"code","a5ba8595":"code","c8f44e9a":"code","02a82561":"code","c08484d2":"code","9740b1a3":"code","54339b78":"code","636eaf7a":"code","c23ca768":"code","1672d397":"code","dd1e45a8":"code","c615f4a5":"code","1a36cd28":"code","68810823":"code","e69aa51b":"code","b4eb6544":"code","e0828bbd":"code","a0983792":"code","4f9dfeeb":"code","1ea3a677":"code","ef0e0ed6":"code","062396ee":"code","ced555aa":"code","5294d49e":"code","0e29f160":"markdown","725a72a6":"markdown","f23a2571":"markdown","b3916f2b":"markdown","bd2d912a":"markdown","820bf20b":"markdown","442b8537":"markdown","2a79b46d":"markdown","be2dc3b2":"markdown","0fe9dc2f":"markdown","a95016b9":"markdown","c402f49d":"markdown","61449b9c":"markdown","325af8fd":"markdown","767d25de":"markdown"},"source":{"c9623277":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dcbcec4d":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","8f9ca427":"df_train = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\npd.set_option('display.max_columns',None)","dcbc7f42":"df_train.head()","9dc691d3":"df_test.head()","b7afb139":"df_train.info()","c03be32a":"df_test.info()","e5d6a70c":"df_train.describe(include='all')","8c52db0b":"df_test.describe(include='all')","7904a979":"print(df_train.shape)\nprint(df_test.shape)","cbaa6c6c":"df_train.isnull().sum()","ffb1a8ef":"df_test.isnull().sum()","f73e135a":"numerical_features = [feature for feature in df_train.columns if df_train[feature].dtypes=='float64']\nprint('Number of Numerical Features:',len(numerical_features))\nprint('Following are the Numerical Features:')\nprint(numerical_features)","a23d8f23":"df_train[numerical_features].head()","d335c63c":"#First let's observe our target variable\nsns.distplot(df_train['target'])\nplt.title('Target Distribution')\nplt.show()","d1095bcd":"sns.boxplot(y=df_train['target'])\nplt.title('Target Spread')\nplt.show()","5b4e3026":"print(df_train['target'].describe(percentiles = [0.25,0.5,0.75,0.85,0.9,1]))","f265bc52":"#Let's see the distribution of these features and scatterplots with respect to target variable.\nfor feature in numerical_features:\n    plt.figure(figsize=(15,10))\n    plt.subplot(1,2,1)\n    sns.distplot(df_train[feature])\n    plt.xlabel(feature)\n    plt.subplot(1,2,2)\n    plt.scatter(df_train[feature],df_train['target'])\n    plt.xlabel(feature)\n    plt.ylabel('Target')\n    plt.show()","8bf7b0ea":"cat_features = [feature for feature in df_train.columns if df_train[feature].dtypes=='object']\nprint('Number of categorical features:',len(cat_features))\nprint('Following are our categorical features:')\nprint(cat_features)","d897b34e":"df_train[cat_features].head()","d920a0e7":"#Let's see how many categories are present in each feature.\nfor feature in cat_features:\n    print(feature,'has:',len(df_train[feature].unique()),'categories')","5b70071a":"#Let's look at each category in the features.\nfor feature in cat_features:\n    print(df_train[feature].value_counts())\n    print('--------------------------')","abe410d4":"#Let's use barplots for a better visualization.\nfor feature in cat_features:\n    plt.figure(figsize=(15,10))\n    df_train[feature].value_counts().sort_values(ascending=False).plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('Count')\n    plt.show()","db442562":"for feature in numerical_features:\n    plt.figure(figsize=(15,10))\n    sns.boxplot(y=df_train[feature])\n    plt.xlabel(feature)\n    plt.show()","3aa6c5e3":"features = ['cont0','cont6','cont8','target']\nfor feature in features:\n    IQR = df_train[feature].quantile(0.75) - df_train[feature].quantile(0.25)\n    Lower_Bound = df_train[feature].quantile(0.25) - (3 * IQR)\n    Upper_Bound = df_train[feature].quantile(0.75) + (3 * IQR)\n    print(feature,'has outliers when',feature,'is less than {} and more than {}'.format(Lower_Bound,Upper_Bound))","d223d357":"#sns.pairplot(df_train)\n#plt.show()","754666ab":"#Let's see the correlation among the features using heatmap.\nplt.figure(figsize=(20,20))\ncorr_matrix = df_train.corr()\nsns.heatmap(corr_matrix,annot=True,cmap='RdYlGn')\nplt.show()","24ca2331":"#First let's drop the id column from train and test data\ndf_train.drop('id',axis=1,inplace=True)\ndf_test.drop('id',axis=1,inplace=True)","3c101658":"#Splitting the data into training and validation set in order to avoid overfitting and data leakage.\nX = df_train.iloc[:,:-1] #Dependent Features\ny = df_train.iloc[:,-1] #Independent Feature\nfrom sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(X,y,test_size=0.3,random_state=0)\nprint(X_train.shape)\nprint(X_val.shape)\nprint(y_train.shape)\nprint(y_val.shape)","b7425fa6":"X_train.head()","22ac9a6d":"y_train.head()","a5ba8595":"X_val.head()","c8f44e9a":"y_val.head()","02a82561":"import warnings\nwarnings.filterwarnings('ignore')","c08484d2":"#To take care of outliers we'll implement top-coding method and cap the maximum values to neglect the affect of outliers\n#I took this peice of code from this kaggle notebook: https:\/\/www.kaggle.com\/prashant111\/extensive-analysis-eda-fe-modelling\ndef maximum_value(df,feature,max_value): #We've discovered max_value to be considered as an outlier in the above outlier analysis\n    return np.where(df[feature]>max_value,max_value,df[feature]) #Wherever the datapoint is greater than max_value replace it with max_value else leave it be\nfor dataframe in [X_train,X_val]:\n    dataframe['cont0'] = maximum_value(dataframe,'cont0',1.45)\n    dataframe['cont6'] = maximum_value(dataframe,'cont6',1.26)\n    dataframe['cont8'] = maximum_value(dataframe,'cont8',1.42)\ny_train = np.where(y_train>11.68,11.68,y_train)\ny_val = np.where(y_val>11.68,11.68,y_val)","9740b1a3":"def minimum_value(df,feature,min_value): #We've discovered min_value to be considered as an outlier in the above outlier analysis\n    return np.where(df[feature]<min_value,min_value,df[feature]) #Wherever the datapoint is less than min_value replace it with min_value else leave it be\nfor dataframe in [X_train,X_val]:\n    dataframe['cont0'] = minimum_value(dataframe,'cont0',-0.38)\n    dataframe['cont6'] = minimum_value(dataframe,'cont6',-0.34)\n    dataframe['cont8'] = minimum_value(dataframe,'cont8',-0.48)\ny_train = np.where(y_train<4.78,4.78,y_train)\ny_val = np.where(y_val<4.78,4.78,y_val)","54339b78":"#We have a number of categories present in each categorical feature, so we'll use sklearn's OrdinalEncoder.\nfrom sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()\nX_train[cat_features] = encoder.fit_transform(X_train[cat_features])\nX_val[cat_features] = encoder.transform(X_val[cat_features])","636eaf7a":"#Repeating the steps for test data.\ndf_test.head()","c23ca768":"#Outlier Analysis\nfeatures = ['cont0','cont6','cont8']\nfor feature in features:\n    IQR = df_test[feature].quantile(0.75) - df_test[feature].quantile(0.25)\n    Lower_Bound = df_test[feature].quantile(0.25) - (3 * IQR)\n    Upper_Bound = df_test[feature].quantile(0.75) + (3 * IQR)\n    print(feature,'has outliers when',feature,'is less than {} and more than {}'.format(Lower_Bound,Upper_Bound))\n","1672d397":"#Outlier Removal\nfor dataframe in [df_test]:\n    dataframe['cont0'] = maximum_value(dataframe,'cont0',1.44)\n    dataframe['cont6'] = maximum_value(dataframe,'cont6',1.26)\n    dataframe['cont8'] = maximum_value(dataframe,'cont8',1.42)\n\nfor dataframe in [df_test]:\n    dataframe['cont0'] = minimum_value(dataframe,'cont0',-0.37)\n    dataframe['cont6'] = minimum_value(dataframe,'cont6',-0.35)\n    dataframe['cont8'] = minimum_value(dataframe,'cont8',-0.48)","dd1e45a8":"#Encoding Categorical Variables.\ndf_test[cat_features] = encoder.transform(df_test[cat_features])","c615f4a5":"df_test.head()","1a36cd28":"from xgboost import XGBRegressor\nxgb_model = XGBRegressor(random_state=0,tree_method='gpu_hist')\nxgb_model.fit(X_train,y_train)\npredictions = xgb_model.predict(X_val)","68810823":"from sklearn.metrics import mean_squared_error, mean_absolute_error\nprint('MSE:',mean_squared_error(y_val,predictions))\nprint('MAE:',mean_absolute_error(y_val,predictions))","e69aa51b":"sns.distplot(y_val-predictions)\nplt.show()","b4eb6544":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nhyperparameter_grid = {'n_estimators':[10,50,300,750,1200,1300,1500],\n                       'max_depth':[2,3,5,10,15],\n                       'learning_rate':[0.05,0.1,0.15,0.2],\n                       'min_child_weight':[1,2,3,4],\n                       'colsample_bytree':[0.5,0.6,0.7,0.8,0.9,1],\n                       'reg_alpha':[5,15,30,40,50],\n                       'subsample':[0.5,0.6,0.7,0.8,0.9,1]   \n                      }\nprint(hyperparameter_grid)\n    ","e0828bbd":"xgb_random = RandomizedSearchCV(estimator=xgb_model,\n                               param_distributions=hyperparameter_grid,\n                               scoring = 'neg_mean_squared_error',\n                               n_iter = 10,\n                               cv = 5,\n                               verbose = False,\n                               random_state = 0,\n                               n_jobs = -1\n                               )\nxgb_random.fit(X_train,y_train)","a0983792":"print(xgb_random.best_params_)","4f9dfeeb":"print(xgb_random.best_score_)","1ea3a677":"optimized_model = XGBRegressor(n_estimators = 1500,\n                               tree_method = 'gpu_hist',\n                               learning_rate = '0.15',\n                               max_depth = 2,\n                               min_child_weight = 3,\n                               subsample = 0.6,\n                               reg_alpha = 30,\n                               colsample_bytree = 0.7,\n                               eval_metric = 'rmse',\n                               n_jobs = -1,\n                               random_state = 0,\n                               booster = 'gbtree'\n                              )","ef0e0ed6":"optimized_model.fit(X_train,y_train)\npredictions = optimized_model.predict(X_val)","062396ee":"print('MSE:',mean_squared_error(y_val,predictions))\nprint('MAE:',mean_absolute_error(y_val,predictions))","ced555aa":"final_pred = optimized_model.predict(df_test)\nfinal_pred","5294d49e":"test = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/test.csv')\noutput = pd.DataFrame({'id':test['id'],'target':final_pred})\noutput.to_csv('my_submission.csv',index=False)\nprint('Your Submission was successfully saved!')\n","0e29f160":"## 1. EDA","725a72a6":"### 3. Model Building using XgBoost","f23a2571":"### 3.1 Hyperparameter Optimization - RandomizedSearchCV","b3916f2b":"### 1.4 Multivariate Analysis","bd2d912a":"**Observations:** 85% of the target values are less than 9 and the remaining 15% are between 9 to 10.5","820bf20b":"XgBoost doesn't require feature scaling so we can proceed to the model building part.Now that our data is preprocessed, we can start building the model!","442b8537":"**Observations:** We can see quite a bit of outliers are present in some of the features in particular, cont0, cont6, cont8 and target.Let's see them using IQR.","2a79b46d":"## 2. Feature Engineering","be2dc3b2":"### 2.1 Handling Outliers in Numerical Features","0fe9dc2f":"**Observations:** The plot is skewed, indicating most of the targt values range between 6-10. The datapoints are far spread out from the median, indicating high variance.","a95016b9":"### 2.2 Encoding Categorical Features","c402f49d":"### 1.1 Numerical Features","61449b9c":"**Observations:** Some of the categorical features have significant imbalance with respect to each category present in them.","325af8fd":"### 1.2 Categorical Features ","767d25de":"### 1.3 Outlier Analysis"}}