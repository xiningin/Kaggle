{"cell_type":{"9cb06b96":"code","2216b7a5":"code","891f8056":"code","93173812":"code","121c8f51":"code","fdc216c1":"code","04fd76d8":"code","aac2f268":"code","dbd1c21c":"code","66056747":"markdown"},"source":{"9cb06b96":"!pip install --upgrade pip\n!pip uninstall -y allennlp\n!pip install transformers==4.1.1 typer\n!pip install -U pytorch-lightning\n!pip install pytorch-lightning-spells","2216b7a5":"!mkdir -p \/src\/finetuning-t5\n!git clone https:\/\/github.com\/ceshine\/finetuning-t5.git -b master \/src\/finetuning-t5","891f8056":"%cd \/src\/finetuning-t5\/mnli","93173812":"!mkdir -p data\/kaggle\n!cp -r \/kaggle\/input\/contradictory-my-dear-watson\/* data\/kaggle\n!ls data\/kaggle\/","121c8f51":"!mkdir -p cache\/kaggle\/\n!python preprocess\/preprocess_kaggle.py\n!python preprocess\/tokenize_dataset.py kaggle --tokenizer-name google\/mt5-base","fdc216c1":"!SEED=9923 python train.py --t5-model google\/mt5-base --batch-size 32 --grad-accu 1 \\\n        --epochs 10 --lr 1e-3 --disable-progress-bar --dataset kaggle \\\n        --max-len 128 --valid-frequency 0.5 --full-model ","04fd76d8":"!ls cache\/\n!mv cache\/tb_logs \/kaggle\/working\n!mv cache\/mt5-base_best \/kaggle\/working","aac2f268":"!ls \/kaggle\/working","dbd1c21c":"!python kaggle_inference.py \/kaggle\/working\/mt5-base_best\n!cp submission.csv \/kaggle\/working","66056747":"[Training code is from this repo of mine](https:\/\/github.com\/ceshine\/finetuning-t5\/tree\/mt5-classifier-single-token\/mnli). Currently my fine-tunes mT5 models still underperforms comparing to the BERT and XLM models."}}