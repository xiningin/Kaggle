{"cell_type":{"3cebd4b9":"code","2bb2d060":"code","46186df8":"code","aad97931":"code","6191131c":"code","b9e9e947":"code","73ed0127":"code","d4037049":"code","dbd6111e":"code","ce272bc8":"code","f42bd5c4":"code","c88b62a9":"code","8d5dbbd6":"code","21ecba4b":"code","46e6e21d":"code","34b659f1":"code","31e5683f":"code","b97d2cb9":"code","35116df2":"code","d1e15b5d":"code","367811bb":"code","195607a9":"code","1bc64971":"code","8b73a053":"code","d598d339":"code","bf5a103e":"code","d120214a":"code","68bbcdf5":"code","ee42bc9f":"code","c67eccd5":"code","ce6a793d":"code","f3cf2954":"code","f0297b7e":"code","b5d6c1f3":"code","ed0e4a51":"code","ddbc4858":"code","c1a21c8b":"code","5ea0a1f1":"code","f7999842":"code","07252986":"code","6cd66d0b":"code","388b84ae":"code","9cddce1e":"code","06d7fb9f":"code","f77d753b":"code","02626ff0":"code","d3586bb3":"code","a23da068":"code","9d3835eb":"code","1cb05f1f":"code","65a24bb2":"code","5a16c746":"code","e9f8f7f0":"code","8c2fe012":"code","c2f6c427":"code","79b68bab":"code","417a7347":"code","78ce551b":"code","c27aa40b":"markdown","174ec148":"markdown","76a35282":"markdown","2f2e7243":"markdown","184495f4":"markdown","e138fe9d":"markdown","3f0bee84":"markdown","38ea4628":"markdown","a19b6128":"markdown","53751e7c":"markdown","e24027ed":"markdown","f995dbdd":"markdown","dad3e889":"markdown","2629a0bb":"markdown","7852dd5b":"markdown","929cdcf0":"markdown","6f7747d9":"markdown","818e2c5d":"markdown","41e5c00a":"markdown","0ab4a83e":"markdown","8d18ec0b":"markdown","af66b61c":"markdown","e8368411":"markdown","e69b2399":"markdown","8bedca8a":"markdown","80b37c7d":"markdown","7bb44c9d":"markdown","f929a831":"markdown","e1ac2221":"markdown","fb0ae470":"markdown","5b7a3217":"markdown","9893c882":"markdown","744f2e10":"markdown","1f0fe922":"markdown","93559955":"markdown","472770e1":"markdown","44ed6894":"markdown","ff0d95ea":"markdown","1af96e31":"markdown","a34e14eb":"markdown","9ae09c19":"markdown","3212a00d":"markdown"},"source":{"3cebd4b9":"# data analysis packages - let us do staff\nimport pandas as pd\nimport numpy as np\n\n#visualization packages - let us make things look pretty\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n#ML packages - cause what's the point without this?\n#models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, ElasticNet\nfrom sklearn.linear_model import SGDClassifier as SGD\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\nimport xgboost as xgb\n\n#around\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import train_test_split\nimport random\n#metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error as MSE\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nrandom.seed(42)","2bb2d060":"train_ds = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_ds = pd.read_csv('..\/input\/titanic\/test.csv')","46186df8":"train_ds.info()","aad97931":"train_ds.head()","6191131c":"train_ds.describe()","b9e9e947":"train_ds.describe(include=['O'])\n","73ed0127":"#calculate gender based survival rate:\nmales_survival_rate = train_ds[(train_ds['Sex'] == 'male')]['Survived'].mean() * 100\nfemales_survival_rate = train_ds[(train_ds['Sex'] == 'female')]['Survived'].mean() * 100\nprint(\"males survival rate: \" + str(males_survival_rate)+ \"%\\nfemales survival rate: \"+str(females_survival_rate)+\"%\")","d4037049":"emb_sr = train_ds[['Embarked','Survived']].groupby(['Embarked']).mean()\nprint(emb_sr)\nplt.bar(['C','Q','S'], emb_sr['Survived'])\nplt.show()\n\npcls_sr = train_ds[['Pclass','Survived']].groupby(['Pclass']).mean()\nprint(pcls_sr)\nplt.bar(['1','2','3'], pcls_sr['Survived'])\nplt.show()\n\nparch_sr = train_ds[['Parch','Survived']].groupby(['Parch'], as_index=False).mean()\nprint(parch_sr)\nplt.bar(parch_sr['Parch'], parch_sr['Survived'])\nplt.show()\n\nsibsp_sr = train_ds[['SibSp','Survived']].groupby(['SibSp'], as_index=False).mean()\nprint(sibsp_sr)\nplt.bar(sibsp_sr['SibSp'], sibsp_sr['Survived'])\nplt.show()","dbd6111e":"train_ds[['Cabin', 'Survived']].groupby(['Cabin']).mean().head()","ce272bc8":"sur_ages = train_ds[train_ds['Survived'] == 1]['Age']\nper_ages = train_ds[train_ds['Survived'] == 0]['Age']\nper_total = train_ds[:]['Age']\nplt.hist(per_total, 16, alpha=0.2, label='total', color='lightgray')\nplt.hist(sur_ages, 16, alpha=0.4, label='survivors', color='blue')\nplt.hist(per_ages, 16, alpha=0.4, label='perished', color='darkred')\nplt.legend(loc='upper right')\nplt.show()","f42bd5c4":"sur_fares = train_ds[(train_ds['Survived'] == 1)&(train_ds['Fare'] <= 200)]['Fare']\nper_fares = train_ds[(train_ds['Survived'] == 0)&(train_ds['Fare'] <= 200)]['Fare']\nfares_tot = train_ds[(train_ds['Fare'] <= 200)]['Fare']\nplt.hist(fares_tot, 15, alpha=0.5, label='total', color='lightgray')\nplt.hist(sur_fares, 15, alpha=0.5, label='survivors', color='green')\n# plt.hist(per_fares, 15, alpha=0.4, label='perished', color='darkred')\nplt.legend(loc='upper right')\nplt.show()","c88b62a9":"train_ds[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in train_ds['Cabin'] ])\n\ntest_ds[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in test_ds['Cabin'] ])\n","8d5dbbd6":"train_ds['Deck'] = 'C'\ntrain_ds.loc[(train_ds['Cabin'] == 'A') | (train_ds['Cabin'] == 'B') | (train_ds['Cabin'] == 'K') | (train_ds['Cabin'] == 'L') | (train_ds['Cabin'] == 'M') | (train_ds['Cabin'] == 'O') | (train_ds['Cabin'] == 'P'), 'Deck'] = 'D'\ntrain_ds.loc[(train_ds['Cabin'] == 'C') | (train_ds['Cabin'] == 'D') | (train_ds['Cabin'] == 'E') | (train_ds['Cabin'] == 'F') | (train_ds['Cabin'] == 'J'), 'Deck'] = 'E'\n\ntest_ds['Deck'] = 'C'\ntest_ds.loc[(test_ds['Cabin'] == 'A') | (test_ds['Cabin'] == 'B') | (test_ds['Cabin'] == 'K') | (test_ds['Cabin'] == 'L') | (test_ds['Cabin'] == 'M') | (test_ds['Cabin'] == 'O') | (test_ds['Cabin'] == 'P'), 'Deck'] = 'D'\ntest_ds.loc[(test_ds['Cabin'] == 'C') | (test_ds['Cabin'] == 'D') | (test_ds['Cabin'] == 'E') | (test_ds['Cabin'] == 'F') | (test_ds['Cabin'] == 'J'), 'Deck'] = 'E'\nprint(test_ds.head())","21ecba4b":"print(train_ds['Name'].head(10))\nprint(\"\\n\")\n# Now we noticed that after each title comes '.' so we'll use this to select only the title word:\n\ntrain_ds['Title'] = train_ds.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\nprint(train_ds.groupby('Title')['Title'].count())\nprint(\"\\n\")\n# Lets group together things that are practically the same, \ntrain_ds.loc[(train_ds['Title'] == 'Ms') | (train_ds['Title'] == 'Mlle'), 'Title'] = 'Miss'\ntrain_ds.loc[(train_ds['Title'] == 'Mme'), 'Title'] = 'Mrs'\nprint(train_ds.groupby('Title')['Title'].count())\nprint(\"\\n\")\n\n#As we have many titles with a very small number of samples, we want to group them together to avoid overfitting\ntrain_ds['Title'] = train_ds['Title'].replace(['Capt', 'Col','Countess', 'Don', 'Dr', 'Jonkheer', 'Lady', 'Major', 'Rev', 'Sir'], 'Special')\nprint(train_ds.groupby('Title')['Title'].count())\nprint(\"\\n\")","46e6e21d":"# making this change in the Test set as well to see it looks around the same\ntest_ds['Title'] = test_ds.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\nprint(test_ds.groupby('Title')['Title'].count())\nprint(\"\\n\")\n\ntest_ds.loc[(test_ds['Title'] == 'Ms'), 'Title'] = 'Miss'\ntest_ds['Title'] = test_ds['Title'].replace(['Col', 'Dona', 'Dr','Rev'], 'Special')\nprint(test_ds.groupby('Title')['Title'].count())\nprint(\"\\n\")","34b659f1":"#updating the datasest:\ntrain_ds = train_ds.drop('Name', axis=1)\ntest_ds = test_ds.drop('Name', axis=1)","31e5683f":"train_ds.loc[train_ds['Embarked'].isnull(),'Embarked'] = 'S'\nprint(train_ds.info())","b97d2cb9":"Sex_mapping = {\"female\": 0, \"male\": 1}\ntrain_ds['Sex'] = train_ds['Sex'].map(Sex_mapping)\ntest_ds['Sex'] = test_ds['Sex'].map(Sex_mapping)\ntrain_ds = pd.get_dummies(train_ds[['Survived','Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare','Deck','Cabin']])\ntest_ds = pd.get_dummies(test_ds[['PassengerId','Sex','Pclass', 'Embarked', 'Title','SibSp', 'Parch', 'Age', 'Fare','Deck','Cabin']])\n\ntrain_cols = train_ds.columns.tolist()\ntest_cols = test_ds.columns.tolist()\nfor col in train_cols:\n    if col not in test_cols:\n        test_ds.loc[:,col] = 0\nfor col in test_cols:\n    if col not in train_cols:\n        train_ds.loc[:,col] = 0\n        \ntrain_ds = train_ds.drop('PassengerId', axis=1)\ntest_ds = test_ds.drop('Survived', axis=1)","35116df2":"joined_ds = pd.concat(objs=[train_ds.drop('Survived', axis=1), test_ds.drop('PassengerId', axis=1)], axis=0).reset_index(drop=True)\nAges_to_train = joined_ds[joined_ds['Age'] >= 0]\nAges_to_train = Ages_to_train.dropna()\nX = Ages_to_train.dropna().drop('Age', axis=1)\ny = Ages_to_train['Age'].dropna()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\nmodel = GridSearchCV(Ridge(normalize=True), {'alpha': np.logspace(-1, 1, 21)}, n_jobs=-1, cv=5)\nmodel.fit(X_train, y_train)\n\n#from this link: https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\nguess_ages = np.zeros((2,3))\nX_test.loc['Age_guessed'] = 0\nfor i in range(0, 2):\n    for j in range(0, 3):\n        guess_df = y_train[(X_train['Sex'] == i) & \\\n                              (X_train['Pclass'] == j+1)]\n        age_guess = guess_df.median()\n\n        # Convert random age float to nearest .5 age\n        guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n        X_test.loc[(X_test.Sex == i) & (X_test.Pclass == j+1),'Age_guessed'] = guess_ages[i,j]\n\n # my part   \ny_guessed = X_test['Age_guessed'].dropna()\ny_ridged = model.predict(X_test.dropna().drop('Age_guessed', axis=1))\nprint(\"the guessed MSE score is:\" + str(MSE(y_test, y_guessed)))\nprint(\"My MSE score is:\" + str(MSE(y_test, y_ridged)))\nX_test = X_test.drop('Age_guessed', axis=1)","d1e15b5d":"joined_ds = pd.concat(objs=[train_ds.drop('Survived', axis=1), test_ds.drop('PassengerId', axis=1)], axis=0).reset_index(drop=True)\nAges_to_fill = train_ds[train_ds['Age'].isnull()]\nAges_to_train = joined_ds[joined_ds['Age'] >= 0]\nAges_to_train = Ages_to_train.dropna()\nX_train_complete = Ages_to_train.drop('Age', axis=1)\ny_train_complete = Ages_to_train['Age']\nX_missing = Ages_to_fill.drop(['Age', 'Survived'], axis=1)\n\n# lab_enc = preprocessing.LabelEncoder()\n# y_enc = lab_enc.fit_transform(y_train).ravel()\n\nmodel = GridSearchCV(Ridge(normalize=True), {'alpha': np.logspace(-1, 1, 21)}, n_jobs=-1, cv=5)\nmodel.fit(X_train, y_train)\n# print(model.best_params_, model.best_score_)\n\nmissing_ages = model.predict(X_missing)\ntrain_ds.loc[train_ds['Age'].isnull(),'Age'] = missing_ages\n","367811bb":"cols = test_ds.columns.tolist()\ncols = cols[:-5] + cols[-4:-3] + cols[-5:-4] + cols[-3:]\ntest_ds = test_ds[cols]\nprint(test_ds.info())\nprint(test_ds.head(10))","195607a9":"Ages_to_fill_t = test_ds[test_ds['Age'].isnull()]\nmissing_ages_t = model.predict(Ages_to_fill_t.drop(['Age', 'PassengerId'], axis=1))\ntest_ds.loc[test_ds['Age'].isnull(),'Age'] = missing_ages_t\ntest_ds.loc[test_ds['Fare'].isnull(),'Fare'] = test_ds['Fare'].mean()\nprint(test_ds.info())\n","1bc64971":"X = train_ds.drop('Survived', axis=1)\ny = train_ds['Survived']\nmodels = []\nmodel_names = []\nmodel_scores = []\n\nk_results = []\nk_results.append(0.69)\nfor i in range(24):\n    knn_model = KNN(n_neighbors=(i+1))\n    k_results.append(cross_val_score(knn_model,X,y,cv=5).mean())\nplt.plot(k_results)\nplt.show()","8b73a053":"knn_model = KNN(n_neighbors=3)\nprint(cross_val_score(knn_model,X,y,cv=5).mean())\nknn_model.fit(X,y)\nprint(knn_model.predict(test_ds.drop('PassengerId', axis=1).head()))\n\n\nmodels.append(knn_model)\nmodel_names.append('KNN(k=3)')\nmodel_scores.append(cross_val_score(knn_model,X,y,cv=5).mean())","d598d339":"logreg_model = LogisticRegression()\nprint(cross_val_score(logreg_model,X,y,cv=5).mean())\n\nmodels.append(logreg_model)\nmodel_names.append('logistic regression')\nmodel_scores.append(cross_val_score(logreg_model,X,y,cv=5).mean())","bf5a103e":"SVC_model = SVC(gamma='scale')\nprint(cross_val_score(SVC_model,X,y,cv=5).mean())\n\nmodels.append(SVC_model)\nmodel_names.append('SVC')\nmodel_scores.append(cross_val_score(SVC_model,X,y,cv=5).mean())","d120214a":"SGD_model = SGD()\nprint(cross_val_score(SGD_model,X,y,cv=5).mean())\n\nmodels.append(SGD_model)\nmodel_names.append('SGD')\nmodel_scores.append(cross_val_score(SGD_model,X,y,cv=5).mean())","68bbcdf5":"est_results = []\nest_results.append(0.8)\nfor i in range(10):\n    RF_model = RandomForestClassifier(n_estimators=((i+1)*50))\n    est_results.append(cross_val_score(RF_model,X,y,cv=5).mean())\nplt.plot(est_results)\nplt.show()","ee42bc9f":"RF_model = RandomForestClassifier(n_estimators=50)\nprint(cross_val_score(RF_model,X,y,cv=5).mean())\n\nmodels.append(RF_model)\nmodel_names.append('Random forest')\nmodel_scores.append(cross_val_score(RF_model,X,y,cv=5).mean())","c67eccd5":"percept_model = Perceptron()\nprint(cross_val_score(percept_model,X,y,cv=5).mean())\n\nmodels.append(percept_model)\nmodel_names.append('Perceptron')\nmodel_scores.append(cross_val_score(percept_model,X,y,cv=5).mean())","ce6a793d":"dt = DecisionTreeClassifier(max_depth=2)\nest_results = []\nest_results.append(0.8)\nfor i in range(20):\n    ada = AdaBoostClassifier(base_estimator=dt, n_estimators=((i+1)*10))\n    est_results.append(cross_val_score(ada,X,y,cv=5).mean())\nplt.plot(est_results)\nplt.show()\n","f3cf2954":"ada_model = AdaBoostClassifier(n_estimators=50)\nprint(cross_val_score(ada_model,X,y,cv=5).mean())\n\nmodels.append(ada_model)\nmodel_names.append('Adaboost')\nmodel_scores.append(cross_val_score(ada_model,X,y,cv=5).mean())","f0297b7e":"GB_model = GradientBoostingClassifier(n_estimators=200, max_depth=2)\nprint(cross_val_score(GB_model,X,y,cv=5).mean())\n\nmodels.append(GB_model)\nmodel_names.append('GradientBoost')\nmodel_scores.append(cross_val_score(GB_model,X,y,cv=5).mean())","b5d6c1f3":"xgb_model = xgb.XGBClassifier(n_estimators= 2000, max_depth= 4, min_child_weight= 2, gamma=0.9,                        \n subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1)\nprint(cross_val_score(xgb_model,X,y,cv=5).mean())\n\nmodels.append(xgb_model)\nmodel_names.append('XGboost')\nmodel_scores.append(cross_val_score(xgb_model,X,y,cv=5).mean())","ed0e4a51":"res = pd.DataFrame({'Model':model_names,'Score':model_scores})\nres.sort_values(by='Score', ascending=False)","ddbc4858":"def tune_and_eval(X_train, y_train, cv=5):\n    gb_param = {\"n_estimators\": [10+(i*10) for i in range(31)], \"max_depth\": [2,3],}\n    ada_param = {\"n_estimators\": [10+(i*5) for i in range(31)], \"base_estimator\": [DecisionTreeClassifier(max_depth=2),DecisionTreeClassifier(max_depth=3)],}\n    logreg_param = {'C': np.logspace(-3, 3, 21)}\n    knn_param = {'n_neighbors': [i+3 for i in range(13)]}\n    \n    \n    inner_models = []\n    inner_models.append(GridSearchCV(GradientBoostingClassifier(),gb_param, n_jobs=-1, cv=cv))\n    inner_models.append(GridSearchCV(RandomForestClassifier(),gb_param, n_jobs=-1, cv=cv))\n    inner_models.append(GridSearchCV(AdaBoostClassifier(),ada_param, n_jobs=-1, cv=cv))\n    inner_models.append(GridSearchCV(LogisticRegression(), logreg_param, n_jobs=-1, cv=cv))\n    inner_models.append(GridSearchCV(SVC(), logreg_param, n_jobs=-1, cv=cv))\n    inner_models.append(GridSearchCV(KNN(), knn_param, n_jobs=-1, cv=cv))\n    \n    names = ['Gradient boosting','Random Forest','Adaboosting', 'logistic regression', 'SVC', 'KNN', 'XGBoost']\n    scores = []\n    params = []\n    for idx, mod in enumerate(inner_models):\n        print(\"tuning \"+str(names[idx])) # for time feeling. can be commented out.\n        mod.fit(X_train,y_train)\n        scores.append(mod.best_score_)\n        params.append(mod.best_params_)\n    \n    xgb_model = xgb.XGBClassifier(n_estimators= 2000, max_depth= 4, min_child_weight= 2, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1)\n    scores.append(cross_val_score(xgb_model,X,y,cv=5).mean())\n    params.append('Untuned')\n    res = pd.DataFrame({'Model':names,'Score':scores, 'Params':params})\n    res = res.sort_values(by='Score', ascending=False)\n    return res    ","c1a21c8b":"X = train_ds.drop('Survived', axis=1)\ny = train_ds['Survived']\nprint(tune_and_eval(X,y, cv=5))","5ea0a1f1":"for dataset in [train_ds, test_ds]:\n    for i in range(16):\n        dataset.loc[(dataset['Age'] > (i*5)) & (dataset['Age'] <= ((i+1)*5)), 'Age'] = i\n    dataset['Age'] = dataset['Age'].astype(int)\n","f7999842":"print(pd.qcut(train_ds['Fare'],5))\n\nfor dataset in [train_ds, test_ds]:\n    dataset.loc[ dataset['Fare'] <= 7.85, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.85) & (dataset['Fare'] <= 10.5), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 10.5) & (dataset['Fare'] <= 21.679), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 21.679) & (dataset['Fare'] <= 39.688), 'Fare']   = 3\n    dataset.loc[ dataset['Fare'] > 39.688, 'Fare'] = 4\n    dataset['Fare'] = dataset['Fare'].astype(int)","07252986":"X = train_ds.drop('Survived', axis=1)\ny = train_ds['Survived']\nprint(tune_and_eval(X,y, cv=5))","6cd66d0b":"train_ds['Fam_size'] = train_ds['Parch'] + train_ds['SibSp'] + 1\n\n#adjusting the test_set\ntest_ds['Fam_size'] = test_ds['Parch'] + test_ds['SibSp'] + 1","388b84ae":"X = train_ds.drop('Survived', axis=1)\ny = train_ds['Survived']\nprint(tune_and_eval(X,y, cv=5))","9cddce1e":"main_features = train_ds[['Survived','Sex', 'Pclass','Age','Fare','SibSp','Parch','Fam_size','Embarked_S','Embarked_C','Embarked_Q','Title_Special','Title_Mr','Title_Miss','Title_Mrs','Title_Master','Cabin_X']]\nplt.figure(figsize=(14,12))\ncolormap = plt.cm.RdBu\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(main_features.astype(float).corr(), square=True, cmap=colormap, annot=True)","06d7fb9f":"for dataset in [train_ds, test_ds]:\n    for i in range(16):\n        dataset.loc[(dataset['Age'] >= (i*16)) & (dataset['Age'] < ((i+1)*16)), 'Age_group'] = i\n    dataset.loc[(dataset['Age']>=80), 'Age_group'] = 5           \n    dataset['Age'] = dataset['Age'].astype(int)\n\nfor dataset in [train_ds, test_ds]:\n    dataset.loc[:,'Alone'] = 0\n    dataset.loc[(dataset['Fam_size'] == 1), 'Alone'] = 1\n\nmain_features = train_ds[['Survived','Sex', 'Pclass','Age','Fare','SibSp','Parch','Fam_size','Embarked_S','Embarked_C','Embarked_Q','Cabin_X','Alone','Age_group']]\n\n    \nplt.figure(figsize=(14,12))\ncolormap = plt.cm.RdBu\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(main_features.astype(float).corr(), square=True, cmap=colormap, annot=True)","f77d753b":"for dataset in [train_ds, test_ds]:\n    dataset.loc[:,'Infant'] = 0\n    dataset.loc[(dataset['Age'] <= 4), 'Infant'] = 1\n    dataset.loc[:,'Kid'] = 0\n    dataset.loc[((dataset['Age'] > 4) & (dataset['Age'] <= 8)), 'Kid'] = 1    \n    dataset.loc[:,'cared_and_careless'] = 0\n    dataset.loc[(dataset['Parch'] <= 1)&(dataset['Title_Mrs'] == 1), 'cared_and_careless'] = 1 \n\nfeatures = train_ds[['Survived','Infant','Kid', 'cared_and_careless']]  \nplt.figure(figsize=(14,12))\ncolormap = plt.cm.RdBu\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(features.astype(float).corr(), square=True, cmap=colormap, annot=True)","02626ff0":"#it seems only cared_and_careless should help, but let's check:\nX = train_ds.drop('Survived', axis=1)\ny = train_ds['Survived']\nprint(tune_and_eval(X, y, cv=5).iloc[:,0:2])\n\ntrain_ds = train_ds.drop(['Infant','Kid'], axis=1)\ntest_ds = test_ds.drop(['Infant','Kid'], axis=1)\nX = train_ds.drop('Survived', axis=1)\ny = train_ds['Survived']\nprint(tune_and_eval(X,y, cv=5).iloc[:,0:2])","d3586bb3":"#lets drop some features that only distract (Embarked_q is not needed as it is the same of 00 on the other results)\ntest_ds = test_ds.drop(['Age','SibSp','Parch','Embarked_Q'], axis=1)\ntrain_ds = train_ds.drop(['Age','SibSp','Parch','Embarked_Q'], axis=1)","a23da068":"X = train_ds.drop('Survived', axis=1)\ny = train_ds['Survived']\nres = tune_and_eval(X,y, cv=5)\nprint(res)\nprint(res.iloc[6])","9d3835eb":"plt.figure(figsize=(14,12))\ncolormap = plt.cm.RdBu\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train_ds.astype(float).corr(), square=True, cmap=colormap, annot=True)","1cb05f1f":"#Playground:\n\nexperiments = []\nres = []\nexperiments.append(train_ds.drop('Fam_size', axis=1))\nexperiments.append(train_ds.drop('Title_Special', axis=1))\nexperiments.append(train_ds.drop(['Title_Special','Title_Master'], axis=1))\nexperiments.append(train_ds.drop('Age_group', axis=1))\nexperiments.append(train_ds.drop('Cabin_G', axis=1))\nexperiments.append(train_ds.drop(['Cabin_T','Cabin_F','Cabin_A','Cabin_G'], axis=1))\n\nexperiments.append(train_ds.drop(['Title_Special','Age_group'], axis=1))\nexperiments.append(train_ds.drop(['Cabin_G','Age_group'], axis=1))\nexperiments.append(train_ds.drop(['Title_Special','Cabin_G'], axis=1))\nexperiments.append(train_ds.drop(['Fam_size','Age_group'], axis=1))\nexperiments.append(train_ds.drop(['Fam_size','Cabin_G'], axis=1))\nexperiments.append(train_ds.drop(['Fam_size','Title_Special'], axis=1))\n\n\nexperiments.append(train_ds.drop(['Title_Special','Age_group', 'Cabin_G'], axis=1))\nexperiments.append(train_ds.drop(['Fam_size','Age_group', 'Cabin_G'], axis=1))\nexperiments.append(train_ds.drop(['Title_Special','Fam_size', 'Cabin_G'], axis=1))\nexperiments.append(train_ds.drop(['Fam_size','Title_Special','Age_group'], axis=1))\n\nexperiments.append(train_ds.drop(['Fam_size','Title_Special','Age_group','Cabin_G'], axis=1))\nexperiments.append(train_ds.drop(['Fam_size','Title_Special','Age_group','Cabin_G','Title_Master','Cabin_T','Cabin_F','Cabin_A',], axis=1))\n\n\nfor exp in experiments:\n    X_exp = exp.drop('Survived', axis=1)\n    y_exp = exp['Survived']\n    res.append(tune_and_eval(X_exp, y_exp, cv=5))","65a24bb2":"for i in range(len(res)):\n    print(\"experiment \"+str(i))\n    print(res[i].iloc[:,1].max(), res[i].iloc[:,1].mean())","5a16c746":"print(res[7])","e9f8f7f0":"# xgb_param = {\"n_estimators\": [1500+(i*250) for i in range(5)], \"max_depth\": [4,5,6], \"min_child_weight\":[1,2],\"gamma\":[0,0.5,0.9,1.5],\"subsample\":[0.8,1],\"colsample_bytree\":[0.8,1],\"objective\":['binary:logistic'], \"nthread\": [-1],}\n# tuned_xgb = GridSearchCV(xgb.XGBClassifier(),xgb_param, n_jobs=-1, cv=5)\n\n# X_7 = experiments[7].drop('Survived', axis=1)\n# y_7 = experiments[7]['Survived']\n# tuned_xgb.fit(X_7, y_7)\n# print(tuned_xgb.best_score_)\n# print(tuned_xgb.best_params_)\n","8c2fe012":"# names = ['Gradient boosting','Random Forest','Adaboosting', 'logistic regression', 'SVC', 'KNN', 'XGBoost']\n# ens_models = []\n# ens_models.append(GradientBoostingClassifier(n_estimators=30, max_depth=3))\n# ens_models.append(RandomForestClassifier(n_estimators=160, max_depth=3))\n# ens_models.append(AdaBoostClassifier(n_estimators=10, base_estimator=DecisionTreeClassifier(max_depth=2)))\n# ens_models.append(LogisticRegression(C=0.501187))\n# ens_models.append(SVC(C=1,probability=True))\n# ens_models.append(KNN(n_neighbors=13))\n# ens_models.append(xgb.XGBClassifier(n_estimators= 2000, max_depth= 4, min_child_weight= 2, gamma=0.9, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', nthread= -1, scale_pos_weight=1, probability=True))\n\n# est=[]\n# for idx, mod in enumerate(ens_models):\n#     est.append((names[idx],mod))\n\n# ens_model = VotingClassifier(est, voting='soft', n_jobs=-1)\n# print(cross_val_score(ens_model,X_7,y_7,cv=5).mean()) # this is mean as opposing to the former max scores!\n","c2f6c427":"# print(X_7.info())\n# print(test_ds.info())","79b68bab":"# ens_model.fit(X_7,y_7)\n\n# # test_ds = test_ds.drop(['Cabin_G','Age_group'], axis=1)\n# ids = test_ds['PassengerId']\n# real_test = test_ds.drop('PassengerId', axis=1)\n# real_pred = ens_model.predict(real_test)\n\n# submission = pd.DataFrame({'PassengerId':ids, 'Survived':real_pred})\n# print(submission)\n\n","417a7347":"# submission.to_csv('.\/submission.csv', index=False) # this one has done 0.789\n","78ce551b":"svc = SVC()\nsvc.fit(X_7,y_7)\nsvc_submission = svc.predict(real_test)\n(pd.DataFrame({'PassengerId':ids, 'Survived':svc_submission})).to_csv('.\/SVC_submission.csv', index=False)","c27aa40b":"Let's take a look over the benchmark model we produced.\n\nWe can see that the optimization process improved some models, even though slightly. The \"slightly\" part shows our first scan was not in vain and we did almost get to the optimal hyperparameters values earlier.\nIt might show on certain occasion we even performed worse than earlier. How can it be?\nremember that each score comes from a mean of a different cross validation, therefor from a different random train-test-split. As we didn't specify a random_state, we might get a partition that is particulary bad for a specific model. If we keep it in mind and the score is qpproximately the same, it's fine for us.\n\nNow that we have a way of checking the quality of our changes - it's time for some more data playing-time!","174ec148":"You might be wondering why I didn't take out the Name column. Is there something to take from it? Well, It's true there is no reason the name itself would have any correlation to the chance whether the person will survive, but if we'll recall the first scan of the data, some of the names have titles included. The title might indicate the status of the person, and so it may be another feature to use. let's see if we can extract it to be another usefull feature:","76a35282":"Before we get to deal with Age (spoiler, we are going to use a small model there), we'll have to map all of the \"objects\" columns to numbers so the models will be able to work with them.\nIf we are doing it, we might as well skip another level. Should the choice of numeric indexes be able to influence the model? \nlet's say we are mapping titles ordinaly like this: {\"Mr\": 0, \"Miss\": 1, \"Mrs\": 2, \"Master\": 3, \"Special\": 4}\nis the relation between Mr and Mrs is similar to the relation between Mrs and Special?\nobviously not, that's why we'll convert them to multiple binary features, like a one hot vector","2f2e7243":"That's better. what about Age and Embarked? \n\nRemember that we found in the exploration part that the vast majority of embarked is categorized 'S'? That's ok, I know you don't. But that's exactly why it is there. You can go back and check.\n\nIn this case because of the small number of missing samples and the fact it's a categorical feature, we will fill the missing data with the most common - 'S'","184495f4":"Now we'll create a function that would do what we've done so far. As any change in the data requires retuning of the models hyperparameters, and we some changes might help a model over another, we'll just retune all of the models (except the last two that was far behind, and because 7 is more than enough) and print the table like above.\n\n(Note: I'm deliberately ignoring tuining the XGBoost as it has many hyperparameters and takes a very long time to tune, usually achieving approximately the same results as the generic tune i'm using here.)\n\nNotice I print the starting time of each tune so we'll have a feeling of what cost us more. It's another tool to choose how much to dig in for each model.","e138fe9d":"### models\n\nI'm planning on running through as many models as possible. That is because the dataset is really small so we can afford the computation time, and because it's a great way to learn and get intuition of the different types of models out there. I won't expand about how each model is working as there are loads of information about it over the net. you can start from wikipedia over the names if you wish.\n\nLets start with one of the most basic models, so we'll have some reference towrads what is good and what is bad\nwe'll begin with KNN(K nearest neighbors):","3f0bee84":"It's a common practice to convert the continous features into descrete ones, so let's group them up, and see if it makes a difference:","38ea4628":"Great!\n\nSo we have a proof our way works, and even better than some sophisticated methods usually used. Lets fill the real holes now","a19b6128":"### Data expansion and Feature engineering","53751e7c":"It's a good time for a quick reminder that this is niether the final model nor a optimally tuned one. It is mainly a tool to get the feeling of our options. After that we'll define our desired model and a function that tunes it to the data, so we'll have a way to compare changes we do in the data frame.\nBut first let's summerize what we have so far.","e24027ed":"Nice! Even now a model as simple as KNN gives as an accuracy of over 80% for the right K. let's make some sanity checks and save the model with the right K for later.\nThe graph above show's us the optimal k for this current setting. It is a hyperparameter we tune, so you'll see that if we want to change parts of the training data we'll redo this check.","f995dbdd":"Whoa! what a comeback for the SVC. It seems that it did hurt slightly some of the upper models, and helped alot to the lower ones. The damage to the models that are not XGB is not big, so we can keep it meanwhile. If we would like to work with the XGB at the end, we can redo the change or tune it properly. Lets generate some new data:","dad3e889":"So, we mapped the the categorical features into dummies, deleted the unnecessary parts, and filled some holes. Before we proceed lets make a sanity check over the test set to see if it's on the same page using our favorite intro functions:","2629a0bb":"we could keep exploring like that forever, but it seems we've found our best combination for the while. \nLet's have a look:","7852dd5b":"It seems to only have worsen the general achievements... Why?  We added some new data so isn't that supposed to help? The answer lies in the correlation between the data we've added. Lets look over pearson's correlation table to see what are the correlations.\n\nWe can see that there no features with high correlation apart from fare and Pclass (very logical when you think about it) and from the Gender_base title's and Sex(I'm ignoring breakdown of an original feature). Generally it is a negative thing as it is put more weight over one of the aspects in the model, but we'll let it through as we've seen it is an important factor.\n\nwe also see that fam_size and age has a very low correlation to survived, our target variable. That's a thing we wish to avoid and probably distracted the model as wev'e seen earlier. It might be caused by the complexity of the feature and the amount of different group. Let's try to see if we can bring up the correlation.","929cdcf0":"why are there only 7 columns now? Did the others sink with the Titanic? \nFear not! Remember that out of the 12 columns only 7 were numeric and the rest were strings. Describe in its default state gives us  information about the numeric columns. \nlets look on this information for a second:\n* The survival rate in the dataset is approx 38% (The mean represent number of survivers \/ number of passengers)\n* Notice that the vast majority of the passengers were relatively young. Most of the passengers under 30, and more than 75% under 40. \nIn a few blocks we'll sink in(pun intended) for some calculation of ourselves to gather some more interesting statistical values. But first let's have a look of the non-numeric values.\n\n\n","6f7747d9":"Wow, what a drastic difference of the survival rate! no wonder Leo didn't make it...\n\nGiggles aside, apart from the intesreting facts, this check has a very valuable information for us.\n\nIt shows that we would be using the gender as an important feature trying to predict who would survive. Had the check returned approx the same survival rate, we would know that the gender does not influence on a single person's chance for safety.\n\nLet's see if there are more features that shows strong correlation to the survival rate:","818e2c5d":"OK! It's finally time to stop messing around and start modeling. Later on we might go back and try to extract some extra features to see if it brings up the models results.\n","41e5c00a":"# here, far, where-ML you are...","0ab4a83e":"Neat! it seems that it did confuse most of the models... (remember that XGboost isn't tuned)\nwe're better off without them.","8d18ec0b":"We get to see the first five rows of our dataset. \nNotice the 3 NaN values at the cabin column. They are not a type of a cabin, but indicator for a missing value. As we have seen earlier, the cabin has only 204 actual values out of 891 entries, so we could expect to see some holes when taking 5 lines.\n\nAlso, we can see how some of the examples look like and how to extract information out of the row data. for example, names start with surname followed by \",\" then Title followd by \".\" and then first and middle name. might come in handy!","af66b61c":"Nice. It doesn't seem we are making a great process with the feature engineering though... Let's see if we can tune XGBoost in a reasonable time, try to ensemble those models together, and call it a day.\n\nNote: the following code block is computentially expensive. You shouldn't run it without GPU, asn even with one it might take several minutes. Spoiler: It doesn't improve the result","e8368411":"A moment before we discard The irrelevant questions, let's try to think in real terms to see if they can help us answer questions that have meaning in life.\nIs he\/she married? \nDoes he\/she have kids to worry about?\nIs He\/She a child?\nlet's create some answers and see if it makes good features.","e69b2399":"\n\n### data exploration\n","8bedca8a":"Age is a more difficult column to fill. A naive approach would be to fill it with random numbers. Or fill every gap with the mean or median, adding some statistical noise. But we can manage better than that. if we can find correlation between other features to the age, we can \"guess\" it using other features of the data, solving it as a mini regression problem.\nRight now I simply chose a random regression model. Later on when we'll have a model in hand, we'll go back to feature engineering and see if we can improve the model by playing with the model here. \n\nWe do want to check it is a fine solution to this problem, or at least prove that it's better than random guessing with mean+noise. So we'll also create an alternative set values, using this method, and calculate the same metrics as the ridge regression to see which one scores better.\n\nfor this, I'll use an even more sophisticated method that is implemented on many notebooks on kaggle, taken this time from this link: https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n\n","80b37c7d":"As a good data scientist, we mustn't rush through to write models and try to understand the dataset we posses. Here I used some of the classic and most basic techniques to understand the sort of data we are dealing with.","7bb44c9d":"Great. Now we can have a feeling about non-numeric features as well. \n* most of the passengers were male (65% for the matter of fact. You can reach the conclusion yourself by frequency\/count)\n* Ticket and Cabin are not unique. Meaning there are multiple people sharing a cabin - reasonable - and multiple people sharing a ticket?... seems a bit strange, we'll come back to that later.\n* Embarked is a trinary value; can get three possible vaules, of which S is the most frequently used.\n\nNow lets have some fun checking some statistics of our own:","f929a831":"\n\n### some imports \n(no choice, we have to)\n","e1ac2221":"## correcting the data","fb0ae470":"Nice! It seems Pclass and SibSp have a good potential, though we must remember that bars with zero height may simply indicate that there weren't that many people in this group in the first place.\n\nLets have a look on the cabin. we would expect it to be an important feature, but it has some problems.","5b7a3217":"For the last part of this exercise we'd take a look over the feature inportance of each of our models. Using this method would help us understand the approach each model take, and get a feeling over which features really help.","9893c882":"From the info() we can take few immediate insights:\n* We have total of 12 columns in our data sets. 1 target variable(survived), and 11 features.\n* Three of the features have missing attributes - cabin, age and Embarked (less then 891 values)\n* 5 of the features are not of numeric type but strings(objects)\n\nThe second insight is the important one, as it requires us to fill in the missing data, which we will attend to later on.\n\nNow lets see how a data row looks like:","744f2e10":"Start here!\n\nThis kernel was created as a part of a ML training progress. \nTt was used to show the way I tackle a new question and the path I take along the data. this means few things:\n1. As this problem was solved and explained many times over, this kernel might not provide new thrilling insights. If you are an experienced reader and you are familiar with the titanic problem, the main thing that might be interesting is the way I chose to fill in the gaps in the data, facing it as a smaller regression problem.\n2. if you are new to ML, this notebook might provide an easy and intuitive way to face a problem of that sort. No complicated math and equations guarenteed, and it has all the reasonable efforts that one might use. It does not contain extended textual explaination though.\n3. I left inside also trials that didn't improve the score. I believe it is important for understanding the process, and they might be more effective on other problems.\n\n\nplease enjoy :)","1f0fe922":"Interesting. It seems that there are cabins that were saved completely and others that were lost in the deeps.\n\nBut remember that cabin was the column that was filled with most holes. In addition, we must consider the possibilty that some of the cabins there were occupied by a single person. That won't contribute much to our models...\n\nConsidering the fact that the number of correct values is so low, we are probably better of without it, or at least need to make some serious changes in the way we approach it.","93559955":"First a couple of words of about the choice of parameters in this histogram:\n\nAt the age groups, the number of bins was chosen to be 16 so it would create age groups with the size of 5, as it's a common and intuitive number to work with. Here it took few checks till I found a number that would be visible and logical. The reason the samples with fare greater than 200 were cut, is because they were scarce, continued the same trend, and made the graph less readable. You are welcomed to try and remove this condition to see if you feel the difference. \n\nOne of the roles of a good data scientist is to make the data accessible to non-scientific reader, including with these little tricks.\n\nNow pay attantion that unlike Age, here there are differences between the fare and the survival rate behavior. Especially in the first three bins. While the lowest fares had a huge death rate, as the fare goes up, the survivors number stay the same as the total drops.\nWe'll take it into consideration further on.","472770e1":"As Parch and SibSp refers to family members on board, We'll just merge them to a new column, Fam_size.","44ed6894":"Last but not least for this part is the age. We would expect the age groups to be important predictors as they might indicate on physical capabilities.\n\nContinuous data such as age, that we would like to represnt in groups is best visualized by a Histogram. Lets have a look. ","ff0d95ea":"Notice that we didn't calculate the survival rate. The reason is we wouldn't know how large is the age group is in total. For example, in the case above, the 75-80 group would have 100% survival rate. but that obviously not what we would like to build our assumptions\/models upon. Especially when 70-75 has 0%, and is larger in quantity. In fact it seems that the person\/few people in the last age group survival is due to luck\/other feature rather then tendency.\n\nWe are looking for the age groups that have the largest difference between the two plots. And those that has a large enough number of people in the age group.\n\nThe first group we notice is infants(0-5) that counterintuitively have a rather large survival rate. We can assume that they recieved extra care and caution by the adults and were prioritized.\n\nThe second group is 25-30, that can be extended to 15-30. It also goes against our expectation of survival of the fittest. After that, we can see that the ratio keeps approx the same with a slightly larger number of deads, but the total amount of passengers going down with every age group. (that's a good way to reassure the data makes sense and fits the information we checked earlier.\n\nNotice that in general Age doesn't seem like a good feature. The histograms of the perished, survivals and people in general looks the same. Apart from the edges we talked about, it seems that there is no interesting information the models can gain from the Age feature\n\nNow lets apply a similar methodology on Fare:","1af96e31":"Oops, it seems that the test set has some holes as well!\nLuckily, wev'e just finished building a way to reconstruct ages. We'll use it on the dataset, and fill the missing fare with the mean (since it's only one missing sample)","a34e14eb":"And if we are dealing with the cabins already, even though it's not yet time for feature engineering, lets take a look over the mapping from cabins to decks taken from wikipedia:\n![image.png](attachment:image.png)\n\nWe can use it to create a feature deck that would take the cabin feature one step forward. We'll refer to all the cabinless people as if they were in deck C.","9ae09c19":"I know we are all eager to dive into the machine learning part, so we'll stop with the data exploration for now. But keep in mind that it's common and even recommended to go on deeper than that and phrase coherent assumptions to be checked further on.\n\nBefore we start with modeling though, we still have some data gaps to fill, fix, or edit.\n\nLet's see if we can get something out of ticket and cabin, that seems our most useless columns. remember the many holes in the cabin column? It might simply indicate that those people didn't have a cabin and lived in a common deck. Let's replace all the NaNs with a new value to indicate that. Also because there are very few people in each cabin, we'll group them together into groups by the prefixes (to refer to a deck and not a single cabin). This is also what we'll do with the tickets, with the assumption that the prefix is related to the deck to which the person belongs to. Remember that even if those assumptions are incorrect, it's always easier to drop data than reproduce it. if our checks later on would show that this information doesn't help us we'll drop it. \n\nIn addition, passenger id doesn't help us here in the training dataset. It's purpose is to get clear results on tests, so we'll drop it meanwhile.\nlet's do it:\n","3212a00d":"## practicing classic ML over the titanic data-set from 0 to 100"}}