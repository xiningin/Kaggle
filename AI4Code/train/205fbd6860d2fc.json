{"cell_type":{"6ccd467c":"code","12a01fe9":"code","2576063f":"code","e3ffe3c7":"code","5927b51b":"code","3c453704":"code","dd1ac151":"code","0650fafc":"code","7cfdb785":"code","4eba74b7":"code","dacdb372":"code","c5fc1c1f":"code","0b05f8a4":"code","9b9a71ed":"code","455bfa10":"code","7d89b1a0":"code","0816c249":"code","0c805e08":"code","c08c918f":"code","c5e873d2":"code","a2fdebb7":"code","38c0b82d":"code","3a09f209":"code","ec6cdf9f":"code","c8aec2c5":"code","7292dbce":"code","74eca1ca":"code","548be977":"code","eb5bf034":"code","4695af83":"code","e1218eed":"code","1800c7ca":"code","07de5b10":"code","28a24c50":"markdown","24b4a17e":"markdown","ebf38426":"markdown","fc73e4c4":"markdown","db0fe14f":"markdown","2d779c23":"markdown","ee42b8ba":"markdown","60669aa4":"markdown","13301e3c":"markdown"},"source":{"6ccd467c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","12a01fe9":"#Importing all libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2576063f":"# Load the data, so we have to convert the date column into date \n# and we have to drop the duplicates entries\/row using code below\n\ndata =  pd.read_csv('..\/input\/quality-prediction-in-a-mining-process\/MiningProcess_Flotation_Plant_Database.csv',\n                   decimal=\",\",\n                    parse_dates=[\"date\"],\n                    infer_datetime_format=True).drop_duplicates()\ndata.info()","e3ffe3c7":"#Check the shape of data (row and column)\ndata.shape","5927b51b":"#check the data if there any missing value or not\ndata.isnull().sum()","3c453704":"#Display the data, and observe what kind of the data is this\ndata.head()","dd1ac151":"#We use heatmap to visualize the corealtion between each features\n\nplt.figure(figsize=(30, 30))\ncor= data.corr()\ncorelation = sns.heatmap(cor, annot=True, cmap=\"RdYlGn\")","0650fafc":"#Drop data that there are no significant corelation on dependent feature\n#Make Correlation with output variable\ncor_target = abs(cor[\"% Silica Concentrate\"])\n\n#Selecting highly correlated features\nrelevant_features = cor_target[cor_target>0.15]\nrelevant_features","7cfdb785":"#We pick the 2 biggest corelation exclude target\nrelevant_features = relevant_features.nlargest(n=3)","4eba74b7":"#Make a data from the relevant features\ndata = pd.DataFrame(data, columns=relevant_features.index)\ndata.head()","dacdb372":"#Checking The Outlier in our data\nsns.boxplot(data['Flotation Column 01 Air Flow'])","c5fc1c1f":"#Checking The Outlier in our data\nsns.boxplot(data['% Iron Concentrate'])","0b05f8a4":"#Checking The Outlier in our data\nsns.boxplot(data['% Silica Concentrate'])","9b9a71ed":"#Dropping the outlier with Percentiles\nfor i in data:\n    upper_lim = data[i].quantile(.95)\n    lower_lim = data[i].quantile(.05)\n\n    data = data[(data[i] < upper_lim) & (data[i] > lower_lim)]","455bfa10":"# Before we split into train and test data, as we can see, the data have differents in units and magnitude\n# So to make it at the same magnitude we can scaling the data\n\ny = data['% Silica Concentrate']\nX = data.drop(['% Silica Concentrate'], axis=1)\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)","7d89b1a0":"# After we scaled the data, and the data have the same magnitude\n# we can split the data into Train & Test\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_scaled,\n                                                    y,\n                                                    test_size=0.3,\n                                                   random_state=30)","0816c249":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\n\ny_pred_linreg = lin_reg.predict(X_test)","0c805e08":"from sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\n\nridge=Ridge()\nparameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\nridge_regressor=GridSearchCV(ridge,parameters,scoring='neg_mean_squared_error',cv=5)\nridge_regressor.fit(X_train,y_train)\ny_pred_ridge = ridge_regressor.predict(X_test)","c08c918f":"from sklearn.linear_model import Lasso\n\nlasso=Lasso()\nparameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\nlasso_regressor=GridSearchCV(lasso,parameters,scoring='neg_mean_squared_error',cv=5)\n\nlasso_regressor.fit(X_train,y_train)\ny_pred_lasso = lasso_regressor.predict(X_test)","c5e873d2":"import xgboost as xgb\nxgb = xgb.XGBRegressor(objective=\"reg:linear\", random_state=42)\nxgb.fit(X_train, y_train)\ny_pred_xgb = xgb.predict(X_test)","a2fdebb7":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import r2_score\n\n#LINEAR REGRESSION\nMSE = mean_squared_error(y_test, y_pred_linreg)\nprint('Our Linear Regression mean squared error is: ',MSE)\nMAE = mean_absolute_error(y_test, y_pred_linreg)\nprint('Our Linear Regression mean absolute error is: ',MAE)\nR2 = r2_score(y_test, y_pred_linreg) \nprint('Our Linear Regression R2 score is: ', R2)\nprint('Our Linear Regreesion Root Mean Squared Error is:', np.sqrt(mean_squared_error(y_test, y_pred_linreg)))\nprint('-'*100)\nprint('-'*100)\n#RIDGE REGRESSION\nMSE = mean_squared_error(y_test, y_pred_ridge)\nprint('Our Rdige Regression mean squared error is: ',MSE)\nMAE = mean_absolute_error(y_test, y_pred_ridge)\nprint('Our Ridge Regression mean absolute error is: ',MAE)\nR2 = r2_score(y_test, y_pred_ridge) \nprint('Our Ridge Regression R2 score is: ', R2)\nprint('Our Ridge Regression Root Mean Squared Error is:', np.sqrt(mean_squared_error(y_test, y_pred_ridge)))\nprint('-'*100)\nprint('-'*100)\n#LASSO REGRESSION\nMSE = mean_squared_error(y_test, y_pred_lasso)\nprint('Our Lasso Regression mean squared error is: ',MSE)\nMAE = mean_absolute_error(y_test, y_pred_lasso)\nprint('Our Lasso Regression mean absolute error is: ',MAE)\nR2 = r2_score(y_test, y_pred_lasso) \nprint('Our Lasso Regression R2 score is: ', R2)\nprint('Our Lasso Regression Root Mean Squared Error is:', np.sqrt(mean_squared_error(y_test, y_pred_lasso)))\nprint('-'*100)\nprint('-'*100)\n#XGBOOST\nMSE = mean_squared_error(y_test, y_pred_xgb)\nprint('Our XGBoost mean squared error is: ',MSE)\nMAE = mean_absolute_error(y_test, y_pred_xgb)\nprint('Our XGBoost mean absolute error is: ',MAE)\nR2 = r2_score(y_test, y_pred_xgb) \nprint('Our XGBoost R2 score is: ', R2)\nprint('Our XGBoost Root Mean Squared Error is:', np.sqrt(mean_squared_error(y_test, y_pred_xgb)))","38c0b82d":"#Cecking Multicolinearity\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\npd.DataFrame({'vif': vif[0:]}, index=X_train.columns).T","3a09f209":"#Checking Normality\nresidual = y_test - y_pred_xgb\nsns.distplot(residual)","ec6cdf9f":"#Checking Normality\nimport scipy as sp\nfig, ax = plt.subplots(figsize=(6,2.5))\n_, (__, ___, r) = sp.stats.probplot(residual, plot=ax, fit=True)","c8aec2c5":"#Checking Homoscedacity\nsns.scatterplot(y_pred_xgb, residual)\nplt.hlines(y=0, xmin= 1, xmax=5)\nplt.xlabel('Residual')\nplt.ylabel('Prediksi')\nplt.title('Residual Plot')","7292dbce":"# Hyper Parameter Tunning\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparams={\n \"learning_rate\"    : [0.01, 0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7, 0.9, 1.0 ]\n    \n}","74eca1ca":"#Using Randomized Search CV to look the best parameter\nrandom_search= RandomizedSearchCV(estimator=xgb,\n                                param_distributions=params,\n                                cv=5, n_iter=50,\n                                scoring = 'r2',n_jobs = 4,\n                                verbose = 1, \n                                return_train_score = True,\n                                random_state=42)","548be977":"#Train Hyperparameter into our Data\nrandom_search.fit(X_train, y_train)","eb5bf034":"#Ceck the best estimator\nrandom_search.best_estimator_","4695af83":"#Using the best hyperparameter into our model\nimport xgboost as xgb\nxgb = xgb.XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1.0, gamma=0.2, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.25, max_delta_step=0, max_depth=10,\n             min_child_weight=7, monotone_constraints=None,\n             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n             objective='reg:linear', random_state=42, reg_alpha=0, reg_lambda=1,\n             scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)\nxgb.fit(X_train, y_train)\ny_pred_xgb_tunning = xgb.predict(X_test)","e1218eed":"#Check Metrics after tunning\nMSE = mean_squared_error(y_test, y_pred_xgb_tunning)\nprint('Our XGBoost after tunning mean squared error is: ',MSE)\nMAE = mean_absolute_error(y_test, y_pred_xgb_tunning)\nprint('Our XGBoost after tunning mean absolute error is: ',MAE)\nR2 = r2_score(y_test, y_pred_xgb_tunning) \nprint('Our XGBoost after tunning R2 score is: ', R2)\nprint('Our XGBoost after tunning Root Mean Squared Error is:', np.sqrt(mean_squared_error(y_test, y_pred_xgb_tunning)))","1800c7ca":"#Visualize The Actual Data and our Prediction\nresult = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred_xgb_tunning})\nresult.head(20)","07de5b10":"#Visualize using scatter plot\nfig = plt.figure(figsize=(20, 10))\nax = fig.add_subplot(111)\nax.set(title=\"XG Boost Tunning\", xlabel=\"Aktual\", ylabel=\"Prediksi\")\nax.scatter(y_test, y_pred_xgb_tunning)\nax.plot([0,max(y_test)], [0,max(y_pred_xgb_tunning)], color='r')\nfig.show()","28a24c50":"**Ridge Regression**","24b4a17e":"# 4. Tunning Model","ebf38426":"# 2. Modelling","fc73e4c4":"**XGBoost**","db0fe14f":"# 3. Evaluation","2d779c23":"**Lasso Regression**","ee42b8ba":"**Linear Regression**","60669aa4":"# **------------------------- THE CONTEXT ----------------------------------------**\n\nBefore we begin our project, we have to understand the data and the problem, what is the problem that we want to solve? is the data ready to process or we have to clean it? and so on. So let's break it down:\n\n**The Data**\n\nThe data came from The Flotation Plant. The data have several columns,the first column is date, the second and third columns are quality measures of the iron ore pulp right before it is fed into the flotation plant. Column 4 until column 8 are the most important variables that impact in the ore quality in the end of the process. From column 9 until column 22, we can see process data (level and air flow inside the flotation columns, which also impact in ore quality. The last two columns are the final iron ore pulp quality measurement from the lab. \n\n**The Problem**\n1. The aim is to predict the % Silica in Concentrate ever minute\n2. can we predict % Silica in Concentrate without using % Iron Concentrate?\n\n**The Steps**\n1. Data Preprocessing\n2. Modelling\u00a0\n3. Evaluating\n4. Tunning Model","13301e3c":"# 1. Data Preprocessing"}}