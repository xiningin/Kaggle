{"cell_type":{"5cf9710d":"code","6214eca3":"code","d77c11fb":"code","fe7b7dfc":"code","4ed4b320":"code","966ae679":"code","0ff106f5":"code","c50703b7":"code","79bf6444":"code","81d4f2f3":"code","297ac4cc":"code","9b23bbf0":"code","c2849fa1":"code","bdf4a3db":"code","b952fc02":"code","6a6e4571":"code","1c2240cb":"code","7fccef48":"code","8fc6cd7f":"code","b5aa4e84":"code","c43cf2f3":"code","c6abe760":"code","37d65e5a":"code","cfc39c9f":"code","ded8d4d1":"code","dd593032":"code","c22d877d":"code","4852fcea":"code","66a5bc50":"code","c058f4e0":"code","48294b0f":"code","b58a8197":"code","9cec681c":"code","04faced0":"code","35352945":"code","7b0f917a":"code","ad4a7f44":"markdown","b8fd0c77":"markdown","b1f253e1":"markdown","c0d07a93":"markdown","77788098":"markdown","0f6b5fce":"markdown","e509d350":"markdown","3ffccd8e":"markdown","ab853de2":"markdown","b649653e":"markdown","2ff34b1d":"markdown","79eeebcb":"markdown","790dbb27":"markdown","b40967e3":"markdown","03e9f435":"markdown","638324ec":"markdown","71664ffb":"markdown","e7a06814":"markdown","467b8c41":"markdown","e6c13f29":"markdown"},"source":{"5cf9710d":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings ('ignore')\n\n%matplotlib inline","6214eca3":"train= pd.read_csv('..\/input\/train.csv')\ntest= pd.read_csv('..\/input\/test.csv')","d77c11fb":"train.head()","fe7b7dfc":"sns.distplot(train['Age'].dropna(), bins = 30)","4ed4b320":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,6))\nsns.countplot(data=train, x='Survived', hue='Sex', ax=ax1)\nsns.countplot(data=train, x='Survived', hue='Pclass', ax=ax2)","966ae679":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(10,6))\nsns.countplot(data=train, x='Survived', hue='Parch', ax=ax1)\nsns.countplot(data=train, x='Survived', hue='SibSp', ax=ax2)","0ff106f5":"sns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')\n\n#Find missing values\ntotal_miss_train = train.isnull().sum()\nperc_miss_train = total_miss_train\/train.isnull().count()*100\nmissing_data_train = pd.DataFrame(({'Total missing train':total_miss_train,\n                            '% missing':perc_miss_train}))\nmissing_data_train.sort_values(by='Total missing train',ascending=False).head(2)","c50703b7":"sns.set(style=\"darkgrid\")\nplt.figure (figsize=(12,7))\nsns.boxplot(data=train, y='Age', x='Pclass')","79bf6444":"def impute_age (cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass==1:\n            return 37\n        elif Pclass==2:\n            return 29\n        else:\n            return 24\n    else: return Age","81d4f2f3":"train['Age']= train[['Age', 'Pclass']].apply(impute_age, axis=1)","297ac4cc":"sex=pd.get_dummies(train['Sex'], drop_first=True)\nembark = pd.get_dummies(train['Embarked'], drop_first=True)\ntrain = pd.concat([train,sex,embark],axis=1)","9b23bbf0":"train.head()","c2849fa1":"sns.set(style=\"darkgrid\")\nplt.figure (figsize=(12,7))\nsns.boxplot(data=test, y='Age', x='Pclass')","bdf4a3db":"def impute_age (cols):\n    Age=cols[0]\n    Pclass=cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass==1:\n            return 42\n        elif Pclass==2:\n            return 27\n        else:\n            return 25\n    else: return Age","b952fc02":"test['Age']= test[['Age', 'Pclass']].apply(impute_age, axis=1)","6a6e4571":"sex=pd.get_dummies(test['Sex'], drop_first=True)\nembark = pd.get_dummies(test['Embarked'], drop_first=True)\ntest = pd.concat([test,sex,embark],axis=1)","1c2240cb":"test.head()","7fccef48":"features = ['Pclass','Age','SibSp', 'Parch', 'Fare', 'male', 'Q', 'S']\ntarget = ['Survived']","8fc6cd7f":"# Check for missing values in the test dataset\ntrain[features].isnull().sum()","b5aa4e84":"# Check for missing values in the test dataset\ntest[features].isnull().sum()","c43cf2f3":"# Fill them in with 0\ntest[features]=test[features].replace(np.NAN, 0)\n#test[\"Survived\"] = \"\"","c6abe760":"data_correlation = train.corr()\nmask = np.array(data_correlation)\nmask[np.tril_indices_from(mask)] = False\nfig = plt.subplots(figsize=(20,10))\nsns.heatmap(data_correlation, mask=mask, vmax=1, square=True, annot=True)","37d65e5a":"# Model Selection\nfrom sklearn.model_selection import train_test_split\n\nX = train[features]\ny = train[target]\nX_train,X_test,y_train,y_test = train_test_split(X,y,random_state=101)\n#X_train = train[features]\n#y_train = train[target]\n#X_test = test[features]","cfc39c9f":"#Selection of algorithm \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier \nfrom sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import model_selection\n\nmodels = [LogisticRegression(),\n          SGDClassifier(),\n          DecisionTreeClassifier(), \n          GradientBoostingClassifier(),\n          RandomForestClassifier(),\n          BaggingClassifier(),\n          svm.SVC(),\n          GaussianNB()]\n\ndef test_algorithms(model):\n    kfold = model_selection.KFold(n_splits=10, random_state=101)\n    predicted = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring='roc_auc')\n    print(predicted.mean())\n    \nfor model in models:\n    test_algorithms(model)","ded8d4d1":"from sklearn.metrics import roc_curve, auc\nlearning_rates = [1, 0.5, 0.25, 0.1, 0.05, 0.01]\ntrain_results = []\ntest_results = []\nfor eta in learning_rates:\n   model = GradientBoostingClassifier(learning_rate=eta)\n   model.fit(X_train, y_train)\n   train_pred = model.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = model.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(learning_rates, train_results, 'b', label='Train AUC')\nline2, = plt.plot(learning_rates, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('learning rate')\nplt.show()\n","dd593032":"n_estimators = [1, 2, 4, 8, 16, 32, 64, 100, 200, 300, 500]\ntrain_results = []\ntest_results = []\nfor estimator in n_estimators:\n   model = GradientBoostingClassifier(n_estimators=estimator)\n   model.fit(X_train, y_train)\n   train_pred = model.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = model.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(n_estimators, train_results, 'b', label='Train AUC')\nline2, = plt.plot(n_estimators, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('n_estimators')\nplt.show()\n","c22d877d":"max_depths = np.linspace(1, 32, 32, endpoint=True)\ntrain_results = []\ntest_results = []\nfor max_depth in max_depths:\n   model = GradientBoostingClassifier(max_depth=max_depth)\n   model.fit(X_train, y_train)\n   train_pred = model.predict(X_train)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_train, train_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   train_results.append(roc_auc)\n   y_pred = model.predict(X_test)\n   false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_pred)\n   roc_auc = auc(false_positive_rate, true_positive_rate)\n   test_results.append(roc_auc)\nfrom matplotlib.legend_handler import HandlerLine2D\nline1, = plt.plot(max_depths, train_results, 'b', label='Train AUC')\nline2, = plt.plot(max_depths, test_results, 'r', label='Test AUC')\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel('AUC score')\nplt.xlabel('Tree depth')\nplt.show()","4852fcea":"from sklearn.model_selection import GridSearchCV\nparam_grid = {'n_estimators':[10,20,30,50, 100, 200, 300],'max_depth':[3,5,7,9,11,13,14], 'learning_rate': [0.1, 0.15, 0.2, 0.25, 0.3]}\ngrid_rf = GridSearchCV(GradientBoostingClassifier(),param_grid,cv=10,scoring='roc_auc').fit(X_train,y_train)\nprint('Best parameter: {}'.format(grid_rf.best_params_))\nprint('Best score: {:.2f}'.format((grid_rf.best_score_)))","66a5bc50":"gbc = GradientBoostingClassifier(max_depth=3, n_estimators=20,learning_rate=0.15)\ngbc.fit(X_train,y_train)\ny_pred = gbc.predict(X_test)","c058f4e0":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","48294b0f":"print(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","b58a8197":"from sklearn.metrics import roc_curve, auc, roc_auc_score\n# predict probabilities\nprobs = gbc.predict_proba(X_test)\n# keep probabilities for the positive outcome only\nprobs = probs[:, 1]\n# calculate AUC\nauc = roc_auc_score(y_test, probs)\nprint('AUC: %.3f' % auc)\n# calculate roc curve\nfpr, tpr, thresholds = roc_curve(y_test, probs)\n# plot no skill\nplt.plot([0, 1], [0, 1], linestyle='--')\n# plot the roc curve for the model\nplt.plot(fpr, tpr, marker='.')\n# show the plot\nplt.show()\n","9cec681c":"from sklearn.model_selection import learning_curve\ntrain_sizes, train_scores, valid_scores = learning_curve(GradientBoostingClassifier(max_depth=3, n_estimators=20, learning_rate=0.15), X_train, y_train, cv=3, scoring='roc_auc')\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\nvalid_scores_mean = np.mean(valid_scores, axis=1)\nvalid_scores_std = np.std(valid_scores, axis=1)\n\nplt.figure()\nplt.plot(train_sizes,valid_scores_mean,label='valid')\nplt.plot(train_sizes,train_scores_mean,label='train')\nplt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.3,color=\"g\")\nplt.fill_between(train_sizes, valid_scores_mean - valid_scores_std,valid_scores_mean + valid_scores_std, alpha=0.3, color=\"b\")\nplt.xlabel('Number of samples')\nplt.ylabel('ROC_AUC')\nplt.legend()","04faced0":"gbc = GradientBoostingClassifier(max_depth=3, n_estimators=20,learning_rate=0.15)\ngbc.fit(train[features],train[target])\ny_pred = gbc.predict(test[features])","35352945":"#Create a  DataFrame with the passengers ids and our prediction regarding whether they survived or not\nsubmission = pd.DataFrame({'PassengerId':test['PassengerId'],'Survived':y_pred})\nsubmission.head()","7b0f917a":"#Convert DataFrame to a csv file that can be uploaded\n#This is saved in the same directory as your notebook\nfilename = 'Titanic Predictions 1.csv'\n\nsubmission.to_csv(filename,index=False)\n\nprint('Saved file: ' + filename)","ad4a7f44":"## 1. Import libraries ","b8fd0c77":"## 5. Hyperparameter tuning ","b1f253e1":"As a final step, we use the 'test' dataseet that was provided to make predictions.","c0d07a93":"The following variables will be used as features for the model that will predict for the target variable 'Survived'. ","77788098":"## 4. Model Selection\nIn this section, we test the performance of seven different classification on the training data: DecisionTreeClassifier, LogisticRegression, SGDClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier. ","0f6b5fce":"Next, we tuned the hyperparameters based on the best performance obtained. A preliminary analysis is shown here for the learning rate, the number of estimators and the max depth. ","e509d350":"Similarly for the 'test' dataset, we impute the missing 'Age' values and work similarly for the other variables.","3ffccd8e":"### Explore the data\nIn this section, we'll be exploring the data, looking for missing values, filling them in, discarding data that is redundant. ","ab853de2":"## 6. Model Performance","b649653e":"The missing 'Embarked' values will be treated as a dummy variable, and so will 'Sex', while data that is redundant and cannot be of use for the model, such as 'PassengerId', 'Name', 'Cabin' and 'Ticket' will not be used as features, as shown furhter down. ","2ff34b1d":"Next, we plot the learning curve, which shows the validation and training score of an estimator for varying numbers of training samples. It is a tool to find out how much we benefit from adding more training data and whether the estimator suffers more from a variance error or a bias error. Since both the validation score and the training score converge to a value with increasing size of the training set, we will not benefit much from more training data.","79eeebcb":"In more detail:\n* A 10-fold CV was chosen as resampling method, while the area under the Receiver Operating Characteristic curve (ROC_AUC) was chosen as a scoring criterion. The GradientBoostingClassifier was found to outperform the other models. \n\n* Other criteria (not presented here) were also tested, such as the F1 score, the Recall, the Precision and the Accuracy of the model. It was observed that the metrics F1 score, Recall and Precision demonstrated similar classification reports to the results obrained by the ROC_AUC cirterion, while Accuracy performed worse. This is to be expected, as Accuracy can in principle be misleading because it does not account for class imbalance in the 'test' dataset. ","790dbb27":"As a next step, we plot the ROC_AUC of the model, which comes up to 86.3%.","b40967e3":"## 3. Exploratory analysis \n### Visualize the data\n* The majority of passengers were between 20 and 40 years old, traveling alone (no family, siblings, parents, kids) \n* The ones who did not survive were predominately men, who were 3rd class passengers","03e9f435":"## 7. Predictions","638324ec":"Indeed, the feautre variables seem be reasonably correlated with the target variable 'Survived', as shown from the correlation matrix below.","71664ffb":"In what follows, parameter optimization is done automatically for a range of values chosen based on the aforemetioned preliminary analysis. The best parameters, along with the best score are printed.","e7a06814":"For the missing 'Age' data, we will impute values based on the median value in each class, as shown below:","467b8c41":"For 'learning_rate': 0.15, 'max_depth': 3, 'n_estimators': 20, we re-run the model and calculate the precision, recall, f1-score and the roc_auc of the 'test' dataset.","e6c13f29":"## 2. Read the data"}}