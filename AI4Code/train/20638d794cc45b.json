{"cell_type":{"8dfc579d":"code","1fb2a837":"code","b4426808":"code","5b09aa5e":"code","917b1545":"code","0baca25f":"code","2191b3a6":"code","7ffd6521":"code","7cd780de":"code","5172e879":"code","8a3770a8":"code","bc18047a":"code","f1d6d5cf":"code","58933e9d":"markdown","9fe8a7bc":"markdown","988a58a6":"markdown","6fac7209":"markdown","bb4a012e":"markdown","ebf48bbb":"markdown","92b33b39":"markdown","a2c13c44":"markdown","cc5d4dda":"markdown","42d73a08":"markdown"},"source":{"8dfc579d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1fb2a837":"import seaborn as sns\nfrom sklearn.preprocessing import MinMaxScaler, normalize, StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, r2_score\nfrom sklearn.svm import SVC\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam, RMSprop\nfrom keras.losses import sparse_categorical_crossentropy, binary_crossentropy\nfrom keras.metrics import Accuracy","b4426808":"data = pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\", engine='python')\n","5b09aa5e":"y = data['quality']\ndata.drop(['quality'], inplace=True, axis=1)\ndata.head()\n","917b1545":"plt.figure(figsize=(10,10))\nsns.heatmap(data.corr())\n","0baca25f":"citricacid = data['fixed acidity'] * data['citric acid']\ncitric_acidity = pd.DataFrame(citricacid, columns=['citric_accidity'])\n\ndensity_acidity = data['fixed acidity'] * data['density']\ndensity_acidity = pd.DataFrame(density_acidity, columns=['density_acidity'])\n\n\ndatafinal = data.join(citric_acidity).join(density_acidity)\n\n","2191b3a6":"bins = (2, 6, 8)\ngnames = ['bad', 'nice']\ny = pd.cut(y, bins = bins, labels = gnames)","7ffd6521":"enc = LabelEncoder()\nyenc = enc.fit_transform(y)","7cd780de":"xtrain, xtest, ytrain, ytest = train_test_split(datafinal, yenc, train_size=0.7, test_size=0.3)\n","5172e879":"scale = StandardScaler()\n\nscaledtrain = scale.fit_transform(xtrain)\nscaledtest = scale.transform(xtest)","8a3770a8":"NeuralModel = Sequential([\n                          Dense(128, activation='relu', input_shape=(13,)),\n                          Dense(32, activation='relu'),\n                          Dense(64, activation='relu'),\n                          Dense(64, activation='relu'),\n                          Dense(64, activation='relu'),\n                          Dense(1, activation='sigmoid')\n])","bc18047a":"rms = Adam(lr=0.0003)\n\nNeuralModel.compile(optimizer=rms, loss='binary_crossentropy', metrics=['accuracy'])","f1d6d5cf":"hist = NeuralModel.fit(scaledtrain, ytrain, epochs=100, validation_data=(scaledtest,ytest))","58933e9d":"We now define the label column:","9fe8a7bc":"Reading the data:","988a58a6":"Lets now combine two columns with a high correlation:\n","6fac7209":"99% accuracy on the training set while, the accuracy on the test data is 90%. This model does not seem to overfit that badly. Thank you for going through this notebook! I really hope it helps :') ","bb4a012e":"We encode the labels of nice and bad into two values of 0 and 1:","ebf48bbb":"Forming basis for binary classification on whether the wine is bad or nice:","92b33b39":"Lets now find the correlation of the features using a heatmap:","a2c13c44":"Lets find the predictions using this model:\n","cc5d4dda":"We now define the data as an array of values between 1 and 0 using the MinMaxScaler function:","42d73a08":"Using the train_test_split function we divide the labels and data into training and testing data:"}}