{"cell_type":{"9817f0d2":"code","41ce32c6":"code","bb4acaca":"code","a9930890":"code","a866f05f":"code","5aa757b3":"code","510999f0":"code","a5d0dc90":"code","89bb3807":"code","6a8cc744":"code","37d3c90b":"code","5ee1b731":"code","e604e09f":"code","38c4a1f3":"code","f8201e8e":"code","6751a949":"code","b2a2ce4d":"code","1fa83e56":"code","4dff69c3":"code","0d54a961":"markdown","f7226ea4":"markdown","7651724d":"markdown","59dac84f":"markdown","3bc49dae":"markdown","7377a0f8":"markdown","5eb4c7ff":"markdown","05fc57d1":"markdown","60cf0bb0":"markdown","c5606eae":"markdown","bfcc0fc4":"markdown","0e783a60":"markdown","535495f1":"markdown","324dca02":"markdown","e0db95d0":"markdown","e39201a1":"markdown","2f2e80b9":"markdown","8876c221":"markdown","33e24c59":"markdown","02e5bdfd":"markdown","d1354a8b":"markdown","48a63b67":"markdown","2a6a27c8":"markdown"},"source":{"9817f0d2":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n# disable warnings\nimport warnings\nwarnings.filterwarnings('ignore')","41ce32c6":"filename = '..\/input\/diabetes.csv'\ndata=pd.read_csv(filename)","bb4acaca":"print(data.columns) # to know all the features(variables) we got in our data","a9930890":"print(data.head()) #the first 5 rows","a866f05f":"data['Outcome'].describe()\n","5aa757b3":"data['Outcome'].hist(figsize=(7,7))","510999f0":"\n#borrowed from my friend Ayoub Benaissa.\ndef plot_diabetic_per_feature(data, feature):\n    grouped_by_Outcome = data[feature].groupby(data[\"Outcome\"])\n    diabetic_per_feature = pd.DataFrame({\"Sick\": grouped_by_Outcome.get_group(1),\n                                        \"Not Sick\": grouped_by_Outcome.get_group(0),\n                                        })\n    hist = diabetic_per_feature.plot.hist(bins=60, alpha=0.6)\n    hist.set_xlabel(feature)\n    plt.show()\n    \n\n","a5d0dc90":"plot_diabetic_per_feature(data, \"Age\")\n","89bb3807":"plot_diabetic_per_feature(data, \"Glucose\")\n","6a8cc744":"print(data[\"Glucose\"].min())","37d3c90b":"plot_diabetic_per_feature(data, \"BMI\")\n","5ee1b731":"import seaborn as sns #the librery we'll use for the job xD\n\ncorrmat = data.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, cbar=True, annot=True, square=True, vmax=.8);","e604e09f":"sns.set()\ncols = ['Pregnancies','Glucose','BloodPressure','Insulin','BMI','DiabetesPedigreeFunction','Age','Outcome']\nsns.pairplot(data[cols], size = 2.5)\nplt.show();","38c4a1f3":"data.min()\n","f8201e8e":"data = data.drop(data[data['Glucose'] == 0].index)\ndata = data.drop(data[data['SkinThickness'] == 0].index) # even it will be deleted xD\ndata = data.drop(data[data['BloodPressure'] == 0].index) #same\ndata = data.drop(data[data['BMI'] == 0].index)\ndata = data.drop(data[data['Insulin'] == 0].index)\n\nprint(data.min()) # let's check\n\n\n","6751a949":"#just the libreries we need\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier  \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import train_test_split \n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import svm\nfrom sklearn.metrics import accuracy_score, make_scorer,f1_score, precision_score, recall_score, confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n#preparing the data\ncols = ['Pregnancies','Glucose','DiabetesPedigreeFunction','Insulin','BMI','Age']\n\nY=data['Outcome']\n#rescaledX = StandardScaler().fit_transform(data[cols])\n#X=pd.DataFrame(data = rescaledX, columns= cols)\nX=data[cols]\n\n# I deleted BloodPressure and Skinthikness\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y, random_state = 25, test_size = 0.2)\n\n\n\n\nsvm1 = svm.SVC(kernel='linear')\nsvm2 = svm.SVC(kernel='rbf') \nlr = LogisticRegression()\nrf = RandomForestClassifier()\nknn=KNeighborsClassifier()\nmodels = {\"Logistic Regression\": lr,\"Random Forest\": rf, \"svm linear\": svm1 , \"svm rbf\": svm2,\"KNeighborsClassifier\": knn }\nl=[]\nfor model in models:\n    l.append(make_pipeline(Imputer(),  models[model]))\n#Finally get the cross-validation scores\ni=0\n\nfor Classifier in l:    \n    accuracy = cross_val_score(Classifier,X_train,Y_train,scoring='accuracy',cv=10)\n    print(\"===\", [*models][i] , \"===\")\n    print(\"accuracy = \",accuracy)\n    print(\"accuracy.mean = \", accuracy.mean())\n    print(\"accuracy.variance = \", accuracy.var())\n    i=i+1\n    print(\"\")\n    \n\n","b2a2ce4d":"\nlr = LogisticRegression()\nlr.fit(X_train,Y_train)\npredictions = lr.predict(X_test)\nsns.heatmap(confusion_matrix(Y_test, predictions), annot=True, cmap=\"YlGn\")\nplt.title(' LogisticRegression ')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\nprint(\"the f1 score for Logistic Regression is :\",(f1_score(Y_test, predictions, average=\"macro\")))\nprint(\"the precision score is :\",(precision_score(Y_test, predictions, average=\"macro\")))\nprint(\"the recall score is :\",(recall_score(Y_test, predictions, average=\"macro\")))   \n","1fa83e56":"\nrf = RandomForestClassifier()\nrf.fit(X_train,Y_train)\npredictions = rf.predict(X_test)\nsns.heatmap(confusion_matrix(Y_test, predictions), annot=True, cmap=\"YlGn\")\nplt.title(' Random Forest Classifier ')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\nprint(\"the f1 score for Random Forest Classifier is :\",(f1_score(Y_test, predictions, average=\"macro\")))\nprint(\"the precision score is :\",(precision_score(Y_test, predictions, average=\"macro\")))\nprint(\"the recall score is :\",(recall_score(Y_test, predictions, average=\"macro\")))   ","4dff69c3":"svm = svm.SVC(kernel='linear')\nsvm.fit(X_train,Y_train)\npredictions = svm.predict(X_test)\nsns.heatmap(confusion_matrix(Y_test, predictions), annot=True, cmap=\"YlGn\")\nplt.title(' SVM kernel(linear) ')\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.show()\nprint(\"the f1 score for SVM linear is :\",(f1_score(Y_test, predictions, average=\"macro\")))\nprint(\"the precision score is :\",(precision_score(Y_test, predictions, average=\"macro\")))\nprint(\"the recall score is :\",(recall_score(Y_test, predictions, average=\"macro\")))   ","0d54a961":"opss! that's a huge amount of graphs to analyse but let's carry on :\n* bloodpressure and age tend to have a relation and that's kinda obviose since most of aged ppl have bloodpressure\n* Glucose and insulin have very strong relation makes me think about deleting the inslin futuere\n* in pregnancy\/age we notice some kind of a liniar line in the right bottom \n\nthat's it, no need to repeat the past notes ' even tho i did '\n\n","f7226ea4":"that true, I don't think that a person can have 0 Glucose and still alive, so i'll delete it later + It also may effect the accuracy of the model.\n\n\nWhat about the **BMI** ( it's basically the weight of the person with kg) :\n","7651724d":"# Preparing the data\/Selecting a model\n\nsince our data is only about 700 row of data, I'll go with k-fold cross validation (better that test\/train in accuracy, but take more  computation time)","59dac84f":"*Intersting*, as u see, The older the persons,The higher the number of diabetics.\nofc that's based from the dataset ' my doctor friend told me there's 2 types of Diabetes, so maybe this data concern the one who target the olders\"\n","3bc49dae":"let's start with the age :","7377a0f8":"#  ' Diabetes ' feature Relationships !!!!\n\n**Diabetes** love Aged,ppl with high level of Glucose  \" based from what I learned from high School\", \nSo let's start with them, by that i mean there relation with Diabetes.\n\n","5eb4c7ff":"## Prepare , load the data \n\nfirst things first, import the modules needed","05fc57d1":"It's okay to have 0 Pregnancies, for the rest, we'll delete the rows containing the 0 values:\n","60cf0bb0":"let's take a very quick look at our data","c5606eae":"Random Forest and support vector machine (linear) both have pretty good f1 score but I'll go with the **Random forest** since it have  higher **fscore** and lower ** false positive values**, and it can be more tuned later.\n\n\n**thanks for reading  and to the next version\/kernels **\n\n\n","bfcc0fc4":"now the histogram of the outcome : \n","0e783a60":"*excellent*, NEXT !!","535495f1":"Same ... the more Glucose you have to more likley to have Diabetes, i guess that will be pretty obvious for you  if you know bit of biology.\nWe notice the odd 0 information, let's confirm: \n\n","324dca02":"our outcome (1 and 0) isn't balanced, but not very much so I think it's okay to use accuracy to defince the best models then compare them by F1 score\n\nnotes:\n* I split the data into test\/train sets , I did the cv on the train test then I'll pick the best models and test them in the test set ( overfitting avoiding      level:99999 )  and compare them by f1 score \/ recall \/ precision\n* Logistic Regression got the higher accuracy mean and  low variance \n* Random Forest and  svm linear  both seems to be good models\n* svm rbf get around 0 variance but I don't actually care since it has pretty low accuracy.mean (checking its table all the values are less than 0.7) \n\nI'll go with  Logistic Regression,  Random Forest and  svm linear:","e0db95d0":"**Correlation matrix  : **\n","e39201a1":"### Scatter plots between 'Outcome' and correlated variables","2f2e80b9":"# Outliers \nOutliers is also something that we should be aware of. Why? Because outliers can markedly affect our models.\n\nlet's see what we can do :","8876c221":"Alright, let's break this matrix down to some few notes : \n* Glucose, Age and  BMI are the most Correlated features with the 'Outcome'\n* Bloodpressure, SkinThikness have tiny Correlation with the outcome, hummm !\n* check how the SkinThikness and BMI Correlated, make me think of rolling it out since mose of the fat ppl tends to have thicc skin\n* Age with Pregnancies are the most Correlated features\n* Insulin with Glucuse ' BIOLOGY  :) \"\n* DiabetesPedigreeFunction bit Correlated with most of them ' I am not sure with feature really mean\"\n* finnaly SkinThikness with Insulin, that's odd !","33e24c59":"# Think beyond the box\nlet's think beyond the box, how is that? \n    with :\n* Correlation matrix\n* Scatter plots between the most correlated variables\n","02e5bdfd":" ## First things first: analysing the outcome\n \n by the outcome we mean whether the person is diabetic or no (1 = yes; 0 = no)\n\ndescriptive statistics summary :\n\n \n ","d1354a8b":"# Pima indianas Diabetes Database\n\nHello, today i am going to  estimate the liklihood of person to be diabetic, this is my first MachineLearning real application away from the theory.\n \n","48a63b67":"we can easily see that  the Non-Diabetic persons is more than the diabetics by what it seems the half\nthat's mean accuracy is no more fine way to mesure how well our models doing.\n","2a6a27c8":"very reasonable results, same notes from the 2 above, we can also notice the 0 odd info so we'll delete it late for the same reasons."}}