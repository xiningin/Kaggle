{"cell_type":{"0e21105b":"code","5819483c":"code","04fc7a09":"code","295886f9":"code","e41975b5":"code","6ab28715":"code","6fe068cc":"code","54a17329":"code","2902d47a":"code","6cc3efbe":"code","e300a15a":"code","b869aad0":"code","67369b21":"code","8aebbffd":"code","bb24c9d1":"code","47a62625":"code","8fde1454":"code","c459c7b3":"code","8a6e707a":"code","b2e36837":"code","32bd818d":"code","53c70429":"code","8dae7da1":"code","673f0481":"code","d859fb1b":"code","d1f1dd58":"code","d1997cc5":"code","4064e05c":"code","7a964024":"code","a4c9d896":"code","c0767007":"code","a029f2d3":"code","8a0834cd":"code","23e023a2":"code","a6162b42":"code","4ab03705":"code","a20081dd":"code","e74e3172":"code","45f1bdf9":"code","0636251c":"code","405fb1d3":"code","d2609d99":"code","85d6ca8f":"code","11c0ae20":"code","5144c880":"code","dbd38d85":"code","d5f5b7f8":"code","c671aca4":"code","cd50b598":"code","32beebf7":"code","14cb06b6":"markdown","ee8b5f74":"markdown","e6a0dbf0":"markdown"},"source":{"0e21105b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport string\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix,accuracy_score, classification_report\nfrom numpy import concatenate\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.semi_supervised import LabelPropagation\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nimport numpy as np\nimport tensorflow as tf\n","5819483c":"import numpy as np\nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","04fc7a09":"import os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport matplotlib.pyplot as plt\nimport torch as nn\nfrom sklearn.model_selection import train_test_split\nfrom torch.nn import Dropout , Linear ","295886f9":"import pandas as pd\njob=pd.read_csv('..\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv')\njob.head()","e41975b5":"import matplotlib\nmatplotlib.style.use('seaborn-bright') \nimport matplotlib.pyplot as plt\nfrom IPython.display import set_matplotlib_formats\nset_matplotlib_formats(\"svg\")\nimport seaborn as sns\npkmn_type_colors = ['#78C850',  # Grass\n                    '#F08030',  # Fire\n                    '#6890F0',  # Water\n                    '#A8B820',  # Bug\n                    '#A8A878',  # Normal\n                    '#A040A0',  # Poison\n                    '#F8D030',  # Electric\n                    '#E0C068',  # Ground\n                    '#EE99AC',  # Fairy\n                    '#C03028',  # Fighting\n                    '#F85888',  # Psychic\n                    '#B8A038',  # Rock \n                   ]","6ab28715":"data = job[job['fraudulent'] == 1]\ndata = data['location'].value_counts()[:10]\n\nplt.xticks(rotation=70)\n\nplt.bar(data.index , np.array(data.values) , color = pkmn_type_colors )","6fe068cc":"data = job['fraudulent'].value_counts()\n\n\n#define Seaborn color palette to use\n\n#create pie chart\nplt.pie(data, labels = ['fraudulent' , 'Non fraudulent'] , colors  = pkmn_type_colors[1:3], explode = [ 0.05, 0.05],  autopct='%.0f%%' ,  pctdistance=0.5)\nplt.title('Data Distribution of fake posts')\nplt.show()","54a17329":"job=job[['description','fraudulent']]\njob=job.dropna()\njob.head()","2902d47a":"X=job['description'].values\ny=job['fraudulent'].values\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42,stratify=y)","6cc3efbe":"X_train","e300a15a":"import re\ndef text_preprocessing(text):\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n    text = re.sub(r'[0-9]+' , '' ,text)\n    text = re.sub(r'\\s([@][\\w_-]+)', '', text).strip()\n    text = re.sub(r'&amp;', '&', text)\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub(r'\\s+', ' ', text).strip()\n    text = text.replace(\"#\" , \" \")\n    encoded_string = text.encode(\"ascii\", \"ignore\")\n    decode_string = encoded_string.decode()\n    return decode_string","b869aad0":"print('Original sentence :'  , X_train[1])\nprint('Processed Sentence :' , text_preprocessing(X_train[1]))","67369b21":"X = []\nfor items in X_train:\n    X.append(text_preprocessing(items))","8aebbffd":"commonWord = ' '.join(X)","bb24c9d1":"from wordcloud import WordCloud,STOPWORDS\nwordcloud = WordCloud(stopwords=STOPWORDS,\n                      background_color='black',\n                      width=3000,\n                      height=2500\n                     ).generate(commonWord)","47a62625":"plt.figure(1,figsize=(12, 12))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","8fde1454":"# make pipeline\npipe = make_pipeline(TfidfVectorizer(),\n                    LogisticRegression())\n# make param grid\nparam_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}\nmodel = GridSearchCV(pipe, param_grid, cv=5)\nmodel.fit(X_train,y_train)\n\n# make prediction and print accuracy\nprediction = model.predict(X_test)\nprint(f\"Accuracy score is {accuracy_score(y_test, prediction):.2f}\")\nprint(classification_report(y_test, prediction))","c459c7b3":"import torch\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","8a6e707a":"pip install transformers==4.6.0","b2e36837":"from transformers import BertTokenizer\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\n# Create a function to tokenize a set of texts\ndef preprocessing_for_bert(data):\n    \"\"\"Perform required preprocessing steps for pretrained BERT.\n    @param    data (np.array): Array of texts to be processed.\n    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n                  tokens should be attended to by the model.\n    \"\"\"\n    # Create empty lists to store outputs\n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    for sent in data:\n        # `encode_plus` will:\n        #    (1) Tokenize the sentence\n        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n        #    (3) Truncate\/Pad sentence to max length\n        #    (4) Map tokens to their IDs\n        #    (5) Create attention mask\n        #    (6) Return a dictionary of outputs\n        encoded_sent = tokenizer.encode_plus(\n            text=text_preprocessing(sent),  # Preprocess sentence\n            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n            max_length=MAX_LEN,                  # Max length to truncate\/pad\n            pad_to_max_length=True,         # Pad sentence to max length\n            #return_tensors='pt',           # Return PyTorch tensor\n            return_attention_mask=True      # Return attention mask\n            )\n        \n        # Add the outputs to the lists\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks","32bd818d":"MAX_LEN = 512\n\ntoken_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\nprint('Original: ', X[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_test)","53c70429":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_test)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 16\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","8dae7da1":"%time\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\n# Create the BertClassfier class\nclass BertClassifier(nn.Module):\n    def __init__(self, freeze_bert=False):\n        super(BertClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in, H, D_out = 768, 50, 2\n\n        # Instantiate BERT model\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        # self.LSTM = nn.LSTM(D_in,D_in,bidirectional=True)\n        # self.clf = nn.Linear(D_in*2,2)\n\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            # nn.LSTM(D_in,D_in)\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits","673f0481":"from transformers import AdamW, get_linear_schedule_with_warmup\n\ndef initialize_model(epochs=4):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    # Instantiate Bert Classifier\n    bert_classifier = BertClassifier(freeze_bert=False)\n\n    # Tell PyTorch to run the model on GPU\n    bert_classifier.to(device)\n\n    # Create the optimizer\n    optimizer = AdamW(bert_classifier.parameters(),\n                      lr=5e-5,    # Default learning rate\n                      eps=1e-8    # Default epsilon value\n                      )\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","d859fb1b":"import random\nimport time\n\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss \/ batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss \/ len(train_dataloader)\n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    torch.save(model,\"mymodelenglish.h5\")\n    torch.save(model.state_dict(), \"mymodelenglish2.h5\")\n    \n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    \n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","d1f1dd58":"import gc\n\ngc.collect()\n\ntorch.cuda.empty_cache()","d1997cc5":"set_seed(42)    # Set seed for reproducibility\nbert_classifier, optimizer, scheduler = initialize_model(epochs=3)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=3, evaluation=True)","4064e05c":"import torch.nn.functional as F","7a964024":"def bert_predict(model, test_dataloader):\n    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n    on the test set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n\n    return probs","a4c9d896":"from sklearn.metrics import accuracy_score, roc_curve, auc\n\ndef evaluate_roc(probs, y_true):\n    \"\"\"\n    - Print AUC and accuracy on the test set\n    - Plot ROC\n    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n    \"\"\"\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n    print(f'AUC: {roc_auc:.4f}')\n       \n    # Get accuracy over the test set\n    y_pred = np.where(preds >= 0.5, 1, 0)\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f'Accuracy: {accuracy*100:.2f}%')\n    print(classification_report(y_true, y_pred))\n    \n    # Plot ROC AUC\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","c0767007":"probs = bert_predict(bert_classifier, val_dataloader)\nevaluate_roc(probs, y_test)","a029f2d3":"X=list(job['description'])\ny=list(job['fraudulent'])\nX1_train, X1_test, y1_train, y1_test = train_test_split(X, y, test_size = 0.20, random_state = 42,stratify=y)","8a0834cd":"from transformers import DistilBertTokenizerFast\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","23e023a2":"train_encodings = tokenizer(X1_train, truncation=True, padding=True)\ntest_encodings = tokenizer(X1_test, truncation=True, padding=True)\n","a6162b42":"import tensorflow as tf\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(train_encodings),\n    y_train\n))\n\ntest_dataset = tf.data.Dataset.from_tensor_slices((\n    dict(test_encodings),\n    y_test\n))","4ab03705":"from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n\ntraining_args = TFTrainingArguments(\n    output_dir='.\/results',          # output directory\n    num_train_epochs=2,              # total number of training epochs\n    per_device_train_batch_size=16,  # batch size per device during training\n    per_device_eval_batch_size=16,   # batch size for evaluation\n    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n    weight_decay=0.1,               # strength of weight decay\n    logging_steps=100,\n    \n)","a20081dd":"with training_args.strategy.scope():\n    trainer_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n\ntrainer = TFTrainer(\n    model=trainer_model,                 # the instantiated \ud83e\udd17 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=test_dataset,             # evaluation dataset\n)\n\n\ntrainer.train()\n","e74e3172":"trainer.evaluate(test_dataset)","45f1bdf9":"trainer.predict(test_dataset)","0636251c":"from sklearn.metrics import classification_report\ntarget_names = ['Real','Fake']\nprint(classification_report(y_test,\n                            trainer.predict(test_dataset)[1],target_names=target_names\n                           )\n     )","405fb1d3":"!pip install --upgrade keras","d2609d99":"import os\nimport pandas as pd\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout,Conv1D,Flatten,Concatenate\nfrom keras.models import Model\nfrom keras import optimizers\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\nfrom nltk.tokenize import wordpunct_tokenize\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier, LogisticRegressionCV\nfrom sklearn.svm import LinearSVC, NuSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import metrics\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\nfrom keras.layers import Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model , load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers","85d6ca8f":"from tensorflow.keras import optimizers","11c0ae20":"# Embedding parameter set\nembed_size = 500 # how big is each word vector\nmax_features = 60000 # how many unique words to use (i.e num rows in embedding vector)","5144c880":"y_train1 = y_train.astype(float)\ny_val1 = y_test.astype(float)","dbd38d85":"max_len=512\nX_train1 = pad_sequences(train_inputs, maxlen=max_len)\nX_val1 = pad_sequences(val_inputs, maxlen=max_len)","d5f5b7f8":"\ninp = tf.keras.Input(shape=(max_len,))\nx = Embedding(max_features, embed_size)(inp)\nx = LSTM(4, return_sequences=True, dropout=0.2, recurrent_dropout=0.1)(x)\nx = Conv1D(16,4,activation='relu')(x)\nx = Flatten()(x)\nx = Dense(100, activation=\"relu\")(x)\nx = Dropout(0.1)(x)\nx = Dense(1, activation=\"sigmoid\")(x)\nmodel = Model(inputs=inp, outputs=x)\nmodel.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr = 0.001,decay = 1e-06), metrics=['accuracy'])\ncheckpoint = ModelCheckpoint('\/kaggle\/output\/model', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]\nmodel.summary()","c671aca4":"model.fit(X_train1, y_train1, batch_size=32, epochs=5,\n          callbacks=callbacks_list, verbose=1 )","cd50b598":"y_pred = model.predict(X_val1)\ny_pred = y_pred.round()","32beebf7":"f1_score = metrics.f1_score(y_val1, y_pred)\n\nprint(f\"Accuracy score is {accuracy_score(y_val1, y_pred):.2f}\")\nprint(classification_report(y_val1, y_pred))","14cb06b6":"We are required to:\n1. Add special tokens to the start and end of each sentence.\n2. Pad & truncate all sentences to a single constant length.\n3. Explicitly differentiate real tokens from padding tokens with the \u201cattention mask\u201d.","ee8b5f74":"**Project : Fake Job Posting Detection**","e6a0dbf0":"* Transformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc in 100+ languages. Its aim is to make cutting-edge NLP easier to use for everyone.\n\n* Transformers provides APIs to quickly download and use those pretrained models on a given text, fine-tune them on your own datasets then share them with the community on our model hub. At the same time, each python module defining an architecture can be used as a standalone and modified to enable quick research experiments.\n\n* Transformers is backed by the two most popular deep learning libraries, PyTorch and TensorFlow, with a seamless integration between them, allowing you to train your models with one then load it for inference with the other."}}