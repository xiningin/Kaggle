{"cell_type":{"d5e8b91d":"code","df29c008":"code","b24754f7":"code","9f6f118f":"code","f10f7cd6":"code","d4db3c4a":"code","ecd3121b":"code","680b0852":"code","ba0cecbb":"code","2b08e22f":"code","02f60b79":"code","c52112ca":"code","cb397738":"code","00681d57":"code","16ee1ae6":"code","dc831067":"code","70e04624":"code","90768eeb":"code","2b670956":"code","76d8bcbc":"code","3391cc19":"code","07e936c9":"code","36ed8dec":"code","2221698f":"code","8dd3ba64":"code","25869cfd":"code","6a53d77b":"code","96a4dbeb":"code","d3006f79":"code","6bd2bd87":"code","f2d025ad":"code","c80aec45":"code","6e23fcf8":"code","5abf7b27":"code","2fe58f5a":"code","4271fdd4":"code","55acffea":"code","dc4bab4e":"code","3c6143e4":"code","b0a5789e":"code","442256dd":"code","2541e4a3":"code","4ef917f6":"code","0389b798":"code","b2ae8699":"code","b16cf243":"code","0c4de299":"code","63e9b7f8":"code","8286b766":"code","f319c1ee":"code","5382cf30":"code","74e674b3":"code","c703b7ac":"code","98a9ab8c":"code","b20ebb92":"code","e9014539":"code","7bbaa381":"code","5f6e1f54":"code","a7b8c5a6":"code","b9c2b6da":"code","0a6ddf9c":"code","3bf1288c":"code","ad423db8":"code","d7a1ce83":"code","d65f38e1":"code","df5e5099":"code","249936de":"code","ff81f051":"code","05152119":"code","8b2271f7":"code","b929d947":"code","3524e665":"code","0ec37c1d":"code","1c79e638":"code","f2a76b27":"code","68866053":"code","e848f100":"code","c3fdfa78":"code","ff588021":"code","b0d76aef":"code","d55bbc3d":"code","c5e0b1e1":"code","1f438d5a":"code","a2fb8e6c":"code","7a610f29":"code","15043809":"code","0a2bae2e":"code","6f4f7c9d":"code","bbf44093":"code","01ec8b43":"code","3f02b7ea":"code","57596c49":"code","73ab8252":"code","173843eb":"code","06a0f1c0":"code","0a137d66":"code","ae4604a3":"code","34a05280":"code","a7c66381":"code","a4d56ca0":"code","f64b5390":"code","c213b56e":"code","d7259921":"code","92433ecb":"code","3fc6e10e":"code","85d779dd":"code","92837502":"code","99e1c258":"code","acd55390":"code","476c5e8c":"code","48a4bcea":"code","bcec3e8d":"code","286d309e":"code","067c1078":"code","d7261d3a":"code","2f0004b0":"code","8cda3c27":"code","634279b9":"code","88c5d1b5":"code","dcceaf8b":"code","f2337457":"code","f3761f12":"code","e5c4982b":"code","8ddee502":"code","ef6eb66f":"code","01c04d77":"code","d020372c":"code","22368a78":"code","ce405d20":"code","aeda0bf2":"code","41507269":"code","250a15cb":"code","0bee1ae2":"code","d38bbc47":"code","a343eb0d":"code","1c02bb04":"code","6181af8f":"code","f28e4f3d":"code","7eae668d":"code","5ca65a04":"code","7be1c030":"code","7ac04d09":"code","701ca46e":"markdown","accf94cf":"markdown","5fe1afb3":"markdown","5dedeba3":"markdown","dfd56be4":"markdown","dc519da2":"markdown","0d3e545d":"markdown","8cd3b5fa":"markdown","ce21c090":"markdown","c0d8f61a":"markdown","18673090":"markdown","7b23fd66":"markdown","0e5228d3":"markdown","023823da":"markdown","0788706b":"markdown","e9af7e9e":"markdown","a6f5293f":"markdown","4560edab":"markdown","505222cd":"markdown","b5de7cca":"markdown","798b06c2":"markdown","ca0d93a5":"markdown","9ee411e4":"markdown","a55c9614":"markdown","afde692e":"markdown","9be2b9d0":"markdown","c84605bf":"markdown","e0b7a2a8":"markdown","60c39e90":"markdown","0c614051":"markdown","c4347d0e":"markdown","7da3f5c8":"markdown","f95f965b":"markdown","6cd452c3":"markdown","8405b76c":"markdown","ddad3a15":"markdown","c898f1ee":"markdown","e07ffbe1":"markdown","432058ea":"markdown","18d58c8d":"markdown","6b92664f":"markdown","f51cebea":"markdown","1f3cb181":"markdown","d9509446":"markdown","cf86f78c":"markdown","21d5b974":"markdown","396fa848":"markdown","bde10ae7":"markdown","7a45d409":"markdown","fc6e3359":"markdown","936a236e":"markdown","e4de2a61":"markdown","b11ad6af":"markdown","386e92c9":"markdown","489821bb":"markdown","4ef945f3":"markdown","e1f4cbe6":"markdown","29d38148":"markdown","43edc245":"markdown","f00ab829":"markdown","6b7e253c":"markdown","4dfedb72":"markdown","aa53e2d7":"markdown","ce7383f4":"markdown","e2b6e388":"markdown","a881b08c":"markdown","5ea0b43b":"markdown","1bc48c99":"markdown","d33b3890":"markdown","d94d73c5":"markdown","15a4c53f":"markdown","163a77be":"markdown","8d937e3d":"markdown","10fbc789":"markdown","75f2dcd3":"markdown","211bce28":"markdown","5c2be2e0":"markdown","c25c2927":"markdown","8caa9427":"markdown","8b6a2ca2":"markdown","01752a42":"markdown","bb9950f4":"markdown","b36c2498":"markdown","a783820f":"markdown","a20ef205":"markdown","a0f6e227":"markdown"},"source":{"d5e8b91d":"import warnings  \nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)","df29c008":"from shutil import copyfile\n\ncopyfile(src='\/kaggle\/input\/ads-py\/libraryplot.py', dst='\/kaggle\/working\/libraryplot.py')","b24754f7":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9f6f118f":"from libraryplot import *","f10f7cd6":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d4db3c4a":"%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfruits = pd.read_table('..\/input\/ads-py\/fruit_colors_with_data.txt')","ecd3121b":"fruits.head()","680b0852":"# create a mapping from fruit label value to fruit name to make results easier to interpret\nlookup_fruit_name = dict(zip(fruits.fruit_label.unique(), fruits.fruit_name.unique()))   \nlookup_fruit_name","ba0cecbb":"# plotting a scatter matrix\nfrom matplotlib import cm\n\nX = fruits[['height', 'width', 'mass', 'color_score']]\ny = fruits['fruit_label']\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\ncmap = cm.get_cmap('gnuplot')\nscatter = pd.scatter_matrix(X_train, c= y_train, marker = 'o', s=40, hist_kwds={'bins':15}, figsize=(9,9), cmap=cmap)","2b08e22f":"# plotting a 3D scatter plot\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection = '3d')\nax.scatter(X_train['width'], X_train['height'], X_train['color_score'], c = y_train, marker = 'o', s=100)\nax.set_xlabel('width')\nax.set_ylabel('height')\nax.set_zlabel('color_score')\nplt.show()","02f60b79":"# For this example, we use the mass, width, and height features of each fruit instance\nX = fruits[['mass', 'width', 'height']]\ny = fruits['fruit_label']\n\n# default is 75% \/ 25% train-test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","c52112ca":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors = 5)","cb397738":"knn.fit(X_train, y_train)","00681d57":"knn.score(X_test, y_test)","16ee1ae6":"# first example: a small fruit with mass 20g, width 4.3 cm, height 5.5 cm\nfruit_prediction = knn.predict([[20, 4.3, 5.5]])\nlookup_fruit_name[fruit_prediction[0]]","dc831067":"# second example: a larger, elongated fruit with mass 100g, width 6.3 cm, height 8.5 cm\nfruit_prediction = knn.predict([[100, 6.3, 8.5]])\nlookup_fruit_name[fruit_prediction[0]]","70e04624":"from libraryplot import plot_fruit_knn\n\nplot_fruit_knn(X_train, y_train, 5, 'uniform')   # we choose 5 nearest neighbors","90768eeb":"k_range = range(1,20)\nscores = []\n\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    scores.append(knn.score(X_test, y_test))\n\nplt.figure()\nplt.xlabel('k')\nplt.ylabel('accuracy')\nplt.scatter(k_range, scores)\nplt.xticks([0,5,10,15,20]);","2b670956":"t = [0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2]\n\nknn = KNeighborsClassifier(n_neighbors = 5)\n\nplt.figure()\n\nfor s in t:\n\n    scores = []\n    for i in range(1,100):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-s)\n        knn.fit(X_train, y_train)\n        scores.append(knn.score(X_test, y_test))\n    plt.plot(s, np.mean(scores), 'bo')\n\nplt.xlabel('Training set proportion (%)')\nplt.ylabel('accuracy');","76d8bcbc":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\n\nnp.set_printoptions(precision=2)\n\n\nfruits = pd.read_table('..\/input\/ads-py\/fruit_colors_with_data.txt')\n\nfeature_names_fruits = ['height', 'width', 'mass', 'color_score']\nX_fruits = fruits[feature_names_fruits]\ny_fruits = fruits['fruit_label']\ntarget_names_fruits = ['apple', 'mandarin', 'orange', 'lemon']\n\nX_fruits_2d = fruits[['height', 'width']]\ny_fruits_2d = fruits['fruit_label']\n\nX_train, X_test, y_train, y_test = train_test_split(X_fruits, y_fruits, random_state=0)\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n# we must apply the scaling to the test set that we computed for the training set\nX_test_scaled = scaler.transform(X_test)\n\nknn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(X_train_scaled, y_train)\nprint('Accuracy of K-NN classifier on training set: {:.2f}'\n     .format(knn.score(X_train_scaled, y_train)))\nprint('Accuracy of K-NN classifier on test set: {:.2f}'\n     .format(knn.score(X_test_scaled, y_test)))\n\nexample_fruit = [[5.5, 2.2, 10, 0.70]]\nprint('Predicted fruit type for ', example_fruit, ' is ', \n      target_names_fruits[knn.predict(example_fruit)[0]-1])","3391cc19":"from sklearn.datasets import make_classification, make_blobs\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.datasets import load_breast_cancer\n\ncmap_bold = ListedColormap(['#FFFF00', '#00FF00', '#0000FF','#000000'])\n\n\n# synthetic dataset for simple regression\nfrom sklearn.datasets import make_regression\nplt.figure()\nplt.title('Sample regression problem with one input variable')\nX_R1, y_R1 = make_regression(n_samples = 100, n_features=1,\n                            n_informative=1, bias = 150.0,\n                            noise = 30, random_state=0)\nplt.scatter(X_R1, y_R1, marker= 'o', s=50)\nplt.show()\n\n\n# synthetic dataset for more complex regression\nfrom sklearn.datasets import make_friedman1\nplt.figure()\nplt.title('Complex regression problem with one input variable')\nX_F1, y_F1 = make_friedman1(n_samples = 100,\n                           n_features = 7, random_state=0)\n\nplt.scatter(X_F1[:, 2], y_F1, marker= 'o', s=50)\nplt.show()\n\n# synthetic dataset for classification (binary) \nplt.figure()\nplt.title('Sample binary classification problem with two informative features')\nX_C2, y_C2 = make_classification(n_samples = 100, n_features=2,\n                                n_redundant=0, n_informative=2,\n                                n_clusters_per_class=1, flip_y = 0.1,\n                                class_sep = 0.5, random_state=0)\nplt.scatter(X_C2[:, 0], X_C2[:, 1], c=y_C2,\n           marker= 'o', s=50, cmap=cmap_bold)\nplt.show()\n\n\n# more difficult synthetic dataset for classification (binary) \n# with classes that are not linearly separable\nX_D2, y_D2 = make_blobs(n_samples = 100, n_features = 2, centers = 8,\n                       cluster_std = 1.3, random_state = 4)\ny_D2 = y_D2 % 2\nplt.figure()\nplt.title('Sample binary classification problem with non-linearly separable classes')\nplt.scatter(X_D2[:,0], X_D2[:,1], c=y_D2,\n           marker= 'o', s=50, cmap=cmap_bold)\nplt.show()\n\n\n# Breast cancer dataset for classification\ncancer = load_breast_cancer()\n(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n\n\n# Communities and Crime dataset\ndef load_crime_dataset():\n    # Communities and Crime dataset for regression\n    # https:\/\/archive.ics.uci.edu\/ml\/datasets\/Communities+and+Crime+Unnormalized\n\n    crime = pd.read_table('..\/input\/ads-py\/CommViolPredUnnormalizedData.txt', sep=',', na_values='?')\n    # remove features with poor coverage or lower relevance, and keep ViolentCrimesPerPop target column\n    columns_to_keep = [5, 6] + list(range(11,26)) + list(range(32, 103)) + [145]  \n    crime = crime.ix[:,columns_to_keep].dropna()\n\n    X_crime = crime.ix[:,range(0,88)]\n    y_crime = crime['ViolentCrimesPerPop']\n\n    return (X_crime, y_crime)\n\n(X_crime, y_crime) = load_crime_dataset()","07e936c9":"from libraryplot import plot_two_class_knn\n\nX_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n                                                   random_state=0)\n\nplot_two_class_knn(X_train, y_train, 1, 'uniform', X_test, y_test)\nplot_two_class_knn(X_train, y_train, 3, 'uniform', X_test, y_test)\nplot_two_class_knn(X_train, y_train, 11, 'uniform', X_test, y_test)","36ed8dec":"from sklearn.neighbors import KNeighborsRegressor\n\nX_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1, random_state = 0)\n\nknnreg = KNeighborsRegressor(n_neighbors = 5).fit(X_train, y_train)\n\nprint(knnreg.predict(X_test))\nprint('R-squared test score: {:.3f}'\n     .format(knnreg.score(X_test, y_test)))","2221698f":"fig, subaxes = plt.subplots(1, 2, figsize=(8,4))\nX_predict_input = np.linspace(-3, 3, 50).reshape(-1,1)\nX_train, X_test, y_train, y_test = train_test_split(X_R1[0::5], y_R1[0::5], random_state = 0)\n\nfor thisaxis, K in zip(subaxes, [1, 3]):\n    knnreg = KNeighborsRegressor(n_neighbors = K).fit(X_train, y_train)\n    y_predict_output = knnreg.predict(X_predict_input)\n    thisaxis.set_xlim([-2.5, 0.75])\n    thisaxis.plot(X_predict_input, y_predict_output, '^', markersize = 10,\n                 label='Predicted', alpha=0.8)\n    thisaxis.plot(X_train, y_train, 'o', label='True Value', alpha=0.8)\n    thisaxis.set_xlabel('Input feature')\n    thisaxis.set_ylabel('Target value')\n    thisaxis.set_title('KNN regression (K={})'.format(K))\n    thisaxis.legend()\nplt.tight_layout()","8dd3ba64":"# plot k-NN regression on sample dataset for different values of K\nfig, subaxes = plt.subplots(5, 1, figsize=(5,20))\nX_predict_input = np.linspace(-3, 3, 500).reshape(-1,1)\nX_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,random_state = 0)\n\nnok = []\nRsq_train = []\nRsq_test = []\nfor thisaxis, K in zip(subaxes, [1, 3, 7, 15, 55]):\n    knnreg = KNeighborsRegressor(n_neighbors = K).fit(X_train, y_train)\n    y_predict_output = knnreg.predict(X_predict_input)\n    train_score = knnreg.score(X_train, y_train)\n    test_score = knnreg.score(X_test, y_test)\n    thisaxis.plot(X_predict_input, y_predict_output)\n    thisaxis.plot(X_train, y_train, 'o', alpha=0.9, label='Train')\n    thisaxis.plot(X_test, y_test, '^', alpha=0.9, label='Test')\n    thisaxis.set_xlabel('Input feature')\n    thisaxis.set_ylabel('Target value')\n    thisaxis.set_title('KNN Regression (K={})\\nTrain $R^2 = {:.3f}$,  Test $R^2 = {:.3f}$'\n                      .format(K, train_score, test_score))\n    nok.append(K)\n    Rsq_train.append(train_score)\n    Rsq_test.append(test_score)\n    thisaxis.legend()\n    plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)\n","25869cfd":"np.max(Rsq_test)","6a53d77b":"from sklearn.linear_model import LinearRegression\n\nX_train, X_test, y_train, y_test = train_test_split(X_R1, y_R1,random_state = 0)\n\nlinreg = LinearRegression().fit(X_train, y_train)\n\nprint('linear model coeff (w): {}'.format(linreg.coef_))\nprint('linear model intercept (b): {:.3f}'.format(linreg.intercept_))\nprint('R-squared score (training): {:.3f}'.format(linreg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(linreg.score(X_test, y_test)))","96a4dbeb":"plt.figure(figsize=(5,4))\nplt.scatter(X_R1, y_R1, marker= 'o', s=50, alpha=0.8)\nplt.plot(X_R1, linreg.coef_ * X_R1 + linreg.intercept_, 'r-')\nplt.title('Least-squares linear regression')\nplt.xlabel('Feature value (x)')\nplt.ylabel('Target value (y)')\nplt.show()","d3006f79":"X_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,random_state = 0)\nlinreg = LinearRegression().fit(X_train, y_train)\n\nprint('Crime dataset')\nprint('linear model intercept: {}'.format(linreg.intercept_))\nprint('linear model coeff:\\n{}'.format(linreg.coef_))\nprint('R-squared score (training): {:.3f}'.format(linreg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(linreg.score(X_test, y_test)))","6bd2bd87":"from sklearn.linear_model import Ridge\nX_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n                                                   random_state = 0)\n\nlinridge = Ridge(alpha=20.0).fit(X_train, y_train)\n\nprint('Crime dataset')\nprint('ridge regression linear model intercept: {}'.format(linridge.intercept_))\nprint('ridge regression linear model coeff:\\n{}'.format(linridge.coef_))\nprint('R-squared score (training): {:.3f}'.format(linridge.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(linridge.score(X_test, y_test)))\nprint('Number of non-zero features: {}'.format(np.sum(linridge.coef_ != 0)))","f2d025ad":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nfrom sklearn.linear_model import Ridge\nX_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n                                                   random_state = 0)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlinridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n\nprint('Crime dataset')\nprint('ridge regression linear model intercept: {}'.format(linridge.intercept_))\nprint('ridge regression linear model coeff:\\n{}'.format(linridge.coef_))\nprint('R-squared score (training): {:.3f}'.format(linridge.score(X_train_scaled, y_train)))\nprint('R-squared score (test): {:.3f}'.format(linridge.score(X_test_scaled, y_test)))\nprint('Number of non-zero features: {}'.format(np.sum(linridge.coef_ != 0)))","c80aec45":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nfrom sklearn.linear_model import Ridge\nX_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n                                                   random_state = 0)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlinridge = Ridge(alpha=20.0).fit(X_train_scaled, y_train)\n\nprint('Crime dataset')\nprint('ridge regression linear model intercept: {}'.format(linridge.intercept_))\nprint('ridge regression linear model coeff:\\n{}'.format(linridge.coef_))\nprint('R-squared score (training): {:.3f}'.format(linridge.score(X_train_scaled, y_train)))\nprint('R-squared score (test): {:.3f}'.format(linridge.score(X_test_scaled, y_test)))\nprint('Number of non-zero features: {}'.format(np.sum(linridge.coef_ != 0)))","6e23fcf8":"print('Ridge regression: effect of alpha regularization parameter\\n')\nfor this_alpha in [0, 1, 10, 20, 50, 100, 1000]:\n    linridge = Ridge(alpha = this_alpha).fit(X_train_scaled, y_train)\n    r2_train = linridge.score(X_train_scaled, y_train)\n    r2_test = linridge.score(X_test_scaled, y_test)\n    num_coeff_bigger = np.sum(abs(linridge.coef_) > 1.0)\n    print('Alpha = {:.2f}\\nnum abs(coeff) > 1.0: {}, \\\nr-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n         .format(this_alpha, num_coeff_bigger, r2_train, r2_test))","5abf7b27":"from sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nX_train, X_test, y_train, y_test = train_test_split(X_crime, y_crime,\n                                                   random_state = 0)\n\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nlinlasso = Lasso(alpha=2.0, max_iter = 10000).fit(X_train_scaled, y_train)\n\nprint('Crime dataset')\nprint('lasso regression linear model intercept: {}'.format(linlasso.intercept_))\nprint('lasso regression linear model coeff:\\n{}'.format(linlasso.coef_))\nprint('Non-zero features: {}'.format(np.sum(linlasso.coef_ != 0)))\nprint('R-squared score (training): {:.3f}'.format(linlasso.score(X_train_scaled, y_train)))\nprint('R-squared score (test): {:.3f}\\n'.format(linlasso.score(X_test_scaled, y_test)))\nprint('Features with non-zero weight (sorted by absolute magnitude):')\n\nfor e in sorted (list(zip(list(X_crime), linlasso.coef_)),\n                key = lambda e: -abs(e[1])):\n    if e[1] != 0:\n        print('\\t{}, {:.3f}'.format(e[0], e[1]))","2fe58f5a":"print('Lasso regression: effect of alpha regularization parameter on number of features kept in final model\\n')\n\nfor alpha in [0.5, 1, 2, 3, 5, 10, 20, 50]:\n    linlasso = Lasso(alpha, max_iter = 10000).fit(X_train_scaled, y_train)\n    r2_train = linlasso.score(X_train_scaled, y_train)\n    r2_test = linlasso.score(X_test_scaled, y_test)\n    \n    print('Alpha = {:.2f}\\nFeatures kept: {}, r-squared training: {:.2f}, r-squared test: {:.2f}\\n'\n         .format(alpha, np.sum(linlasso.coef_ != 0), r2_train, r2_test))","4271fdd4":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_F1, y_F1, random_state = 0)\n\nlinreg = LinearRegression().fit(X_train, y_train)\n\nprint('linear model coeff (w): {}'.format(linreg.coef_))\nprint('linear model intercept (b): {:.3f}'.format(linreg.intercept_))\nprint('R-squared score (training): {:.3f}'.format(linreg.score(X_train, y_train)))\nprint('R-squared score (test): {:.3f}'.format(linreg.score(X_test, y_test)))","55acffea":"from sklearn.linear_model import LogisticRegression\nfrom libraryplot import plot_class_regions_for_classifier_subplot\n\nfig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\ny_fruits_apple = y_fruits_2d == 1   # make into a binary problem: apples vs everything else\nX_train, X_test, y_train, y_test = (\ntrain_test_split(X_fruits_2d.as_matrix(),\n                y_fruits_apple.as_matrix(),\n                random_state = 0))\n\nclf = LogisticRegression(C=100).fit(X_train, y_train)\nplot_class_regions_for_classifier_subplot(clf, X_train, y_train, None,None,\n                'Logistic regression for binary classification\\nFruit dataset: Apple vs others',subaxes)\n\nh = 6\nw = 8\nprint('A fruit with height {} and width {} is predicted to be: {}'\n     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\n\nh = 10\nw = 7\nprint('A fruit with height {} and width {} is predicted to be: {}'\n     .format(h,w, ['not an apple', 'an apple'][clf.predict([[h,w]])[0]]))\nsubaxes.set_xlabel('height')\nsubaxes.set_ylabel('width')\n\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))","dc4bab4e":"from sklearn.linear_model import LogisticRegression\nfrom libraryplot import plot_class_regions_for_classifier_subplot\n\n\nX_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2,\n                                                   random_state = 0)\n\nfig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\nclf = LogisticRegression().fit(X_train, y_train)\ntitle = 'Logistic regression, simple synthetic dataset C = {:.3f}'.format(1.0)\nplot_class_regions_for_classifier_subplot(clf, X_train, y_train,None, None, title, subaxes)\n\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))","3c6143e4":"X_train, X_test, y_train, y_test = (train_test_split(X_fruits_2d.as_matrix(), y_fruits_apple.as_matrix(),random_state=0))\n\nfig, subaxes = plt.subplots(3, 1, figsize=(4, 10))\n\nfor this_C, subplot in zip([0.1, 1, 100], subaxes):\n    clf = LogisticRegression(C=this_C).fit(X_train, y_train)\n    title ='Logistic regression (apple vs rest), C = {:.3f}'.format(this_C)\n    \n    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n                                             X_test, y_test, title,\n                                             subplot)\nplt.tight_layout()","b0a5789e":"from sklearn.linear_model import LogisticRegression\n\nX_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n\nclf = LogisticRegression().fit(X_train, y_train)\nprint('Breast cancer dataset')\nprint('Accuracy of Logistic regression classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of Logistic regression classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))","442256dd":"from sklearn.svm import SVC\nfrom libraryplot import plot_class_regions_for_classifier_subplot\n\n\nX_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\n\nfig, subaxes = plt.subplots(1, 1, figsize=(7, 5))\nthis_C = 1.0\nclf = SVC(kernel = 'linear', C=this_C).fit(X_train, y_train)\ntitle = 'Linear SVC, C = {:.3f}'.format(this_C)\nplot_class_regions_for_classifier_subplot(clf, X_train, y_train, None, None, title, subaxes)","2541e4a3":"from sklearn.svm import LinearSVC\nfrom libraryplot import plot_class_regions_for_classifier\n\nX_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state = 0)\nfig, subaxes = plt.subplots(1, 2, figsize=(8, 4))\n\nfor this_C, subplot in zip([0.00001, 100], subaxes):\n    clf = LinearSVC(C=this_C).fit(X_train, y_train)\n    title = 'Linear SVC, C = {:.5f}'.format(this_C)\n    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n                                             None, None, title, subplot)\nplt.tight_layout()","4ef917f6":"from sklearn.svm import LinearSVC\nX_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n\nclf = LinearSVC().fit(X_train, y_train)\nprint('Breast cancer dataset')\nprint('Accuracy of Linear SVC classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of Linear SVC classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))","0389b798":"from sklearn.svm import LinearSVC\n\nX_train, X_test, y_train, y_test = train_test_split(X_fruits_2d, y_fruits_2d, random_state = 0)\n\nclf = LinearSVC(C=5, random_state = 67).fit(X_train, y_train)\nprint('Coefficients:\\n', clf.coef_)\nprint('Intercepts:\\n', clf.intercept_)","b2ae8699":"# plt.figure(figsize=(6,6))\n# colors = ['r', 'g', 'b', 'y']\n# cmap_fruits = ListedColormap(['#FF0000', '#00FF00', '#0000FF','#FFFF00'])\n\n# plt.scatter(X_fruits_2d[['height']], X_fruits_2d[['width']],\n#            c=y_fruits_2d, cmap=cmap_fruits, edgecolor = 'black', alpha=.7)\n\n# x_0_range = np.linspace(-10, 15)\n\n# for w, b, color in zip(clf.coef_, clf.intercept_, ['r', 'g', 'b', 'y']):\n#     plt.plot(x_0_range, -(x_0_range * w[0] + b) \/ w[1], c=color, alpha=.8)\n    \n# plt.legend(target_names_fruits)\n# plt.xlabel('height')\n# plt.ylabel('width')\n# plt.xlim(-2, 12)\n# plt.ylim(-2, 15)\n# plt.show()","b16cf243":"from sklearn.svm import SVC\nfrom libraryplot import plot_class_regions_for_classifier\n\nX_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n\n# The default SVC kernel is radial basis function (RBF)\nplot_class_regions_for_classifier(SVC().fit(X_train, y_train),\n                                 X_train, y_train, None, None,\n                                 'Support Vector Classifier: RBF kernel')\n\n# Compare decision boundries with polynomial kernel, degree = 3\nplot_class_regions_for_classifier(SVC(kernel = 'poly', degree = 3)\n                                 .fit(X_train, y_train), X_train,\n                                 y_train, None, None,\n                                 'Support Vector Classifier: Polynomial kernel, degree = 3')","0c4de299":"from libraryplot import plot_class_regions_for_classifier\n\nX_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\nfig, subaxes = plt.subplots(3, 1, figsize=(4, 11))\n\nfor this_gamma, subplot in zip([0.01, 1.0, 10.0], subaxes):\n    clf = SVC(kernel = 'rbf', gamma=this_gamma).fit(X_train, y_train)\n    title = 'Support Vector Classifier: \\nRBF kernel, gamma = {:.2f}'.format(this_gamma)\n    plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n                                             None, None, title, subplot)\n    plt.tight_layout()","63e9b7f8":"from sklearn.svm import SVC\nfrom libraryplot import plot_class_regions_for_classifier_subplot\n\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\nfig, subaxes = plt.subplots(3, 4, figsize=(15, 10), dpi=50)\n\nfor this_gamma, this_axis in zip([0.01, 1, 5], subaxes):\n    \n    for this_C, subplot in zip([0.1, 1, 15, 250], this_axis):\n        title = 'gamma = {:.2f}, C = {:.2f}'.format(this_gamma, this_C)\n        clf = SVC(kernel = 'rbf', gamma = this_gamma,\n                 C = this_C).fit(X_train, y_train)\n        plot_class_regions_for_classifier_subplot(clf, X_train, y_train,\n                                                 X_test, y_test, title,\n                                                 subplot)\n        plt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","8286b766":"from sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer,\n                                                   random_state = 0)\n\nclf = SVC(C=10).fit(X_train, y_train)\nprint('Breast cancer dataset (unnormalized features)')\nprint('Accuracy of RBF-kernel SVC on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of RBF-kernel SVC on test set: {:.2f}'.format(clf.score(X_test, y_test)))","f319c1ee":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nclf = SVC(C=10).fit(X_train_scaled, y_train)\nprint('Breast cancer dataset (normalized with MinMax scaling)')\nprint('RBF-kernel SVC (with MinMax scaling) training set accuracy: {:.2f}'.format(clf.score(X_train_scaled, y_train)))\nprint('RBF-kernel SVC (with MinMax scaling) test set accuracy: {:.2f}'.format(clf.score(X_test_scaled, y_test)))","5382cf30":"from sklearn.model_selection import cross_val_score\n\nclf = KNeighborsClassifier(n_neighbors = 5)\nX = X_fruits_2d.as_matrix()\ny = y_fruits_2d.as_matrix()\ncv_scores = cross_val_score(clf, X, y)\n\nprint('Cross-validation scores (3-fold):', cv_scores)\nprint('Mean cross-validation score (3-fold): {:.3f}'\n     .format(np.mean(cv_scores)))","74e674b3":"from sklearn.svm import SVC\nfrom sklearn.model_selection import validation_curve\n\nparam_range = np.logspace(-3, 3, 4)\ntrain_scores, test_scores = validation_curve(SVC(), X, y,\n                                            param_name='gamma',param_range=param_range, cv=3)\n\nprint(train_scores)\nprint(test_scores)","c703b7ac":"# This code based on scikit-learn validation_plot example\n#  See:  http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_validation_curve.html\nplt.figure()\n\ntrain_scores_mean = np.mean(train_scores, axis=1)\ntrain_scores_std = np.std(train_scores, axis=1)\ntest_scores_mean = np.mean(test_scores, axis=1)\ntest_scores_std = np.std(test_scores, axis=1)\n\nplt.title('Validation Curve with SVM')\nplt.xlabel('$\\gamma$ (gamma)')\nplt.ylabel('Score')\nplt.ylim(0.0, 1.1)\nlw = 2\n\nplt.semilogx(param_range, train_scores_mean, label='Training score',\n            color='darkorange', lw=lw)\n\nplt.fill_between(param_range, train_scores_mean - train_scores_std,\n                train_scores_mean + train_scores_std, alpha=0.2,\n                color='darkorange', lw=lw)\n\nplt.semilogx(param_range, test_scores_mean, label='Cross-validation score',\n            color='navy', lw=lw)\n\nplt.fill_between(param_range, test_scores_mean - test_scores_std,\n                test_scores_mean + test_scores_std, alpha=0.2,\n                color='navy', lw=lw)\n\nplt.legend(loc='best')\nplt.show()","98a9ab8c":"from sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom libraryplot import plot_decision_tree\nfrom sklearn.model_selection import train_test_split\n\n\niris = load_iris()\n\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 3)\nclf = DecisionTreeClassifier().fit(X_train, y_train)\n\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))","b20ebb92":"clf2 = DecisionTreeClassifier(max_depth = 3).fit(X_train, y_train)\n\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'.format(clf2.score(X_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'.format(clf2.score(X_test, y_test)))","e9014539":"plot_decision_tree(clf, iris.feature_names, iris.target_names)\n","7bbaa381":"plot_decision_tree(clf2, iris.feature_names, iris.target_names)","5f6e1f54":"from libraryplot import plot_feature_importances\n\nplt.figure(figsize=(10,4), dpi=80)\nplot_feature_importances(clf, iris.feature_names)\nplt.show()\n\nprint('Feature importances: {}'.format(clf.feature_importances_))","a7b8c5a6":"from sklearn.tree import DecisionTreeClassifier\nfrom libraryplot import plot_class_regions_for_classifier_subplot\n\nX_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state = 0)\nfig, subaxes = plt.subplots(6, 1, figsize=(6, 32))\n\npair_list = [[0,1], [0,2], [0,3], [1,2], [1,3], [2,3]]\ntree_max_depth = 4\n\nfor pair, axis in zip(pair_list, subaxes):\n    X = X_train[:, pair]\n    y = y_train\n    \n    clf = DecisionTreeClassifier(max_depth=tree_max_depth).fit(X, y)\n    title = 'Decision Tree, max_depth = {:d}'.format(tree_max_depth)\n    plot_class_regions_for_classifier_subplot(clf, X, y, None,\n                                             None, title, axis,\n                                             iris.target_names)\n    \n    axis.set_xlabel(iris.feature_names[pair[0]])\n    axis.set_ylabel(iris.feature_names[pair[1]])\n    \nplt.tight_layout()\nplt.show()","b9c2b6da":"from sklearn.tree import DecisionTreeClassifier\nfrom libraryplot import plot_decision_tree\nfrom libraryplot import plot_feature_importances\n\nX_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n\nclf = DecisionTreeClassifier(max_depth = 4, min_samples_leaf = 8, random_state = 0).fit(X_train, y_train)\n\nplot_decision_tree(clf, cancer.feature_names, cancer.target_names)","0a6ddf9c":"print('Breast cancer dataset: decision tree')\nprint('Accuracy of DT classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of DT classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))\n\nplt.figure(figsize=(10,6),dpi=80)\nplot_feature_importances(clf, cancer.feature_names)\nplt.tight_layout()\nplt.show()","3bf1288c":"from sklearn.tree import DecisionTreeClassifier\nfrom libraryplot import plot_decision_tree\nfrom libraryplot import plot_feature_importances\n\nX_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n\nclf = DecisionTreeClassifier(random_state = 0).fit(X_train, y_train)\n\nplot_decision_tree(clf, cancer.feature_names, cancer.target_names)","ad423db8":"print('Breast cancer dataset: decision tree')\nprint('Accuracy of DT classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of DT classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))\n\nplt.figure(figsize=(10,6),dpi=80)\nplot_feature_importances(clf, cancer.feature_names)\nplt.tight_layout()\nplt.show()","d7a1ce83":"from sklearn.ensemble import RandomForestClassifier\nfrom libraryplot import plot_class_regions_for_classifier_subplot\n\nX_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n\nclf = RandomForestClassifier().fit(X_train, y_train)\nfig, subaxes = plt.subplots(1, 1, figsize=(6, 6))\ntitle = 'Random Forest Classifier, complex binary dataset, default settings'\nplot_class_regions_for_classifier_subplot(clf, X_train, y_train, X_test, y_test, title, subaxes)\nplt.show()","d65f38e1":"# def plot_class_regions_for_classifier_subplot_forndarray(clf, X, y, X_test, y_test, title, subplot, target_names = None, plot_decision_regions = True):\n\n#     numClasses = numpy.amax(y) + 1\n#     color_list_light = ['#FFFFAA', '#EFEFEF', '#AAFFAA', '#AAAAFF']\n#     color_list_bold = ['#EEEE00', '#000000', '#00CC00', '#0000CC']\n#     cmap_light = ListedColormap(color_list_light[0:numClasses])\n#     cmap_bold  = ListedColormap(color_list_bold[0:numClasses])\n\n#     h = 0.03\n#     k = 0.5\n#     x_plot_adjust = 0.1\n#     y_plot_adjust = 0.1\n#     plot_symbol_size = 50\n\n#     x_min = X[:, 0].min()\n#     x_max = X[:, 0].max()\n#     y_min = X[:, 1].min()\n#     y_max = X[:, 1].max()\n#     x2, y2 = numpy.meshgrid(numpy.arange(x_min-k, x_max+k, h), numpy.arange(y_min-k, y_max+k, h))\n\n#     P = clf.predict(numpy.c_[x2.ravel(), y2.ravel()])\n#     P = P.reshape(x2.shape)\n\n#     if plot_decision_regions:\n#         subplot.contourf(x2, y2, P, cmap=cmap_light, alpha = 0.8)\n\n#     subplot.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold, s=plot_symbol_size, edgecolor = 'black')\n#     subplot.set_xlim(x_min - x_plot_adjust, x_max + x_plot_adjust)\n#     subplot.set_ylim(y_min - y_plot_adjust, y_max + y_plot_adjust)\n\n#     if (X_test is not None):\n#         subplot.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cmap_bold, s=plot_symbol_size, marker='^', edgecolor = 'black')\n#         train_score = clf.score(X, y)\n#         test_score  = clf.score(X_test, y_test)\n#         title = title + \"\\nTrain score = {:.2f}, Test score = {:.2f}\".format(train_score, test_score)\n\n#     subplot.set_title(title)\n\n#     if (target_names is not None):\n#         legend_handles = []\n#         for i in range(0, len(target_names)):\n#             patch = mpatches.Patch(color=color_list_bold[i], label=target_names[i])\n#             legend_handles.append(patch)\n#         subplot.legend(loc=0, handles=legend_handles)\n","df5e5099":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom libraryplot import plot_class_regions_for_classifier_subplot\n\nX_train, X_test, y_train, y_test = train_test_split(X_fruits,y_fruits,random_state = 0)\n\nfig, subaxes = plt.subplots(6, 1, figsize=(5, 25))\ntitle = 'Random Forest, fruits dataset, default settings'\npair_list = [[0,1], [0,2], [0,3], [1,2], [1,3], [2,3]]\n\nfor pair, axis in zip(pair_list, subaxes):\n    X = X_train.iloc[:, pair]\n    y = y_train\n    \n    clf = RandomForestClassifier().fit(X, y)\n    plot_class_regions_for_classifier_subplot(clf, X.as_matrix(), y.as_matrix(),\n                                                         None,None, title, axis,target_names_fruits)\n    \n    axis.set_xlabel(feature_names_fruits[pair[0]])\n    axis.set_ylabel(feature_names_fruits[pair[1]])\n    \nplt.tight_layout()\nplt.show()","249936de":"clf = RandomForestClassifier(n_estimators = 10,random_state=0).fit(X_train, y_train)\n\nprint('Random Forest, Fruit dataset, default settings\\n')\nprint('Accuracy of RF classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of RF classifier on test set    : {:.2f}'.format(clf.score(X_test, y_test)))","ff81f051":"from sklearn.ensemble import RandomForestClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n\nclf = RandomForestClassifier(max_features = 8, random_state = 0)\nclf.fit(X_train, y_train)\n\nprint('Breast cancer dataset')\nprint('Accuracy of RF classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of RF classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))","05152119":"# if 'pandas.core.frame.DataFrame' in str(type(X_fruits)):\n    \n# elif 'numpy.ndarray' in str(type(X_fruits.as_matrix())):","8b2271f7":"from sklearn.ensemble import GradientBoostingClassifier\nfrom libraryplot import plot_class_regions_for_classifier_subplot\n\nX_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state = 0)\n\nclf = GradientBoostingClassifier().fit(X_train, y_train)\n\nfig, subaxes = plt.subplots(1, 1, figsize=(6, 6))\ntitle = 'GBDT, complex binary dataset, default settings'\nplot_class_regions_for_classifier_subplot(clf, X_train, y_train, X_test,y_test, title, subaxes)\n\nplt.show()","b929d947":"from libraryplot import plot_class_regions_for_classifier_subplot\n\nX_train, X_test, y_train, y_test = train_test_split(X_fruits.as_matrix(),y_fruits.as_matrix(),random_state = 0)\n\nfig, subaxes = plt.subplots(6, 1, figsize=(6, 32))\npair_list = [[0,1], [0,2], [0,3], [1,2], [1,3], [2,3]]\n\nfor pair, axis in zip(pair_list, subaxes):\n    X = X_train[:, pair]\n    y = y_train\n    \n    clf = GradientBoostingClassifier().fit(X, y)\n    plot_class_regions_for_classifier_subplot(clf, X, y, None, None, title, axis, target_names_fruits)\n    \n    axis.set_xlabel(feature_names_fruits[pair[0]])\n    axis.set_ylabel(feature_names_fruits[pair[1]])\n    \nplt.tight_layout()\nplt.show()","3524e665":"clf = GradientBoostingClassifier().fit(X_train, y_train)\n\nprint('GBDT, Fruit dataset, default settings')\nprint('Accuracy of GBDT classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of GBDT classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))","0ec37c1d":"from sklearn.ensemble import GradientBoostingClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state=0)\n\nclf = GradientBoostingClassifier(random_state=0)\nclf.fit(X_train, y_train)\n\nprint('Breast cancer dataset (learning_rate=0.1, max_depth=3)')\nprint('Accuracy of GBDT classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of GBDT classifier on test set: {:.2f}\\n'.format(clf.score(X_test, y_test)))","1c79e638":"from sklearn.ensemble import GradientBoostingClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state=0)\n\nclf = GradientBoostingClassifier(learning_rate=0.1, max_depth=2, random_state=0)\nclf.fit(X_train, y_train)\n\nprint('Breast cancer dataset (learning_rate=0.01, max_depth=2)')\nprint('Accuracy of GBDT classifier on training set: {:.2f}'.format(clf.score(X_train, y_train)))\nprint('Accuracy of GBDT classifier on test set: {:.2f}'.format(clf.score(X_test, y_test)))","f2a76b27":"xrange = np.linspace(-2, 2, 200)\n\nplt.figure(figsize=(7,6))\nplt.plot(xrange, np.maximum(xrange, 0), label = 'relu')\nplt.plot(xrange, np.tanh(xrange), label = 'tanh')\nplt.plot(xrange, 1 \/ (1 + np.exp(-xrange)), label = 'logistic')\nplt.legend()\nplt.title('Neural network activation functions')\nplt.xlabel('Input value (x)')\nplt.ylabel('Activation function output')\nplt.show()","68866053":"from sklearn.neural_network import MLPClassifier\n\nX_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\nfig, subaxes = plt.subplots(3, 1, figsize=(6,18))\n\nfor units, axis in zip([1,10,100], subaxes):\n    nnclf = MLPClassifier(hidden_layer_sizes= [units], solver='lbfgs', random_state = 0).fit(X_train, y_train)\n    \n    title = 'Dataset 1: Neural net classifier, 1 layer, {} units'.format(units)\n    plot_class_regions_for_classifier_subplot(nnclf, X_train, y_train,X_test, y_test, title, axis)\n    plt.tight_layout()","e848f100":"X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n\nnnclf = MLPClassifier(hidden_layer_sizes = [10, 10], solver='lbfgs',random_state = 0).fit(X_train, y_train)\n\ntitle = 'Dataset 1: Neural net classifier, 2 layers, 10\/10 units'\nplot_class_regions_for_classifier(nnclf, X_train, y_train, X_test, y_test,title)","c3fdfa78":"X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n\nfig, subaxes = plt.subplots(4, 1, figsize=(6, 23))\n\nfor this_alpha, axis in zip([0.01, 0.1, 1.0, 5.0], subaxes):\n    nnclf = MLPClassifier(solver='lbfgs', activation = 'tanh',alpha = this_alpha,\n                         hidden_layer_sizes = [100, 100],random_state = 0).fit(X_train, y_train)\n    \n    title = 'Dataset 2: NN classifier, alpha = {:.3f} '.format(this_alpha)\n    plot_class_regions_for_classifier_subplot(nnclf, X_train, y_train,X_test, y_test, title, axis)\n    plt.tight_layout()","ff588021":"X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\n\nfig, subaxes = plt.subplots(3, 1, figsize=(6,18))\n\nfor this_activation, axis in zip(['logistic', 'tanh', 'relu'], subaxes):\n    nnclf = MLPClassifier(solver='lbfgs', activation = this_activation, alpha = 0.1, hidden_layer_sizes = [10, 10],\n                         random_state = 0).fit(X_train, y_train)\n    \n    title = 'Dataset 2: NN classifier, 2 layers 10\/10, {} activation function'.format(this_activation)\n    plot_class_regions_for_classifier_subplot(nnclf, X_train, y_train,X_test, y_test, title, axis)\n    plt.tight_layout()","b0d76aef":"from sklearn.neural_network import MLPRegressor\n\nfig, subaxes = plt.subplots(2, 3, figsize=(11,8), dpi=70)\n\nX_predict_input = np.linspace(-3, 3, 50).reshape(-1,1)\nX_train, X_test, y_train, y_test = train_test_split(X_R1[0::5], y_R1[0::5], random_state = 0)\n\nfor thisaxisrow, thisactivation in zip(subaxes, ['tanh', 'relu']):\n    for thisalpha, thisaxis in zip([0.0001, 1.0, 100], thisaxisrow):\n        mlpreg = MLPRegressor(hidden_layer_sizes = [100,100],activation = thisactivation,\n                             alpha = thisalpha,solver = 'lbfgs').fit(X_train, y_train)\n        \n        y_predict_output = mlpreg.predict(X_predict_input)\n        \n        thisaxis.set_xlim([-2.5, 0.75])\n        thisaxis.plot(X_predict_input, y_predict_output,'^', markersize = 10)\n        thisaxis.plot(X_train, y_train, 'o')\n        thisaxis.set_xlabel('Input feature')\n        thisaxis.set_ylabel('Target value')\n        thisaxis.set_title('MLP regression\\nalpha={}, activation={})'.format(thisalpha, thisactivation))\n        plt.tight_layout()","d55bbc3d":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\n\nX_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nclf = MLPClassifier(hidden_layer_sizes = [100, 100], alpha = 5.0,random_state = 0, solver='lbfgs').fit(X_train_scaled, y_train)\n\nprint('Breast cancer dataset')\nprint('Accuracy of NN classifier on training set: {:.2f}'.format(clf.score(X_train_scaled, y_train)))\nprint('Accuracy of NN classifier on test set: {:.2f}'.format(clf.score(X_test_scaled, y_test)))","c5e0b1e1":"from sklearn.naive_bayes import GaussianNB\nfrom libraryplot  import plot_class_regions_for_classifier\n\nX_train, X_test, y_train, y_test = train_test_split(X_C2, y_C2, random_state=0)\nnbclf = GaussianNB().fit(X_train, y_train)\n\nplot_class_regions_for_classifier(nbclf, X_train, y_train, X_test, y_test, 'Gaussian Naive Bayes classifier: Dataset 1')","1f438d5a":"X_train, X_test, y_train, y_test = train_test_split(X_D2, y_D2, random_state=0)\nnbclf = GaussianNB().fit(X_train, y_train)\n\nplot_class_regions_for_classifier(nbclf, X_train, y_train, X_test, y_test, 'Gaussian Naive Bayes classifier: Dataset 2')","a2fb8e6c":"X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, random_state = 0)\n\nnbclf = GaussianNB().fit(X_train, y_train)\n\nprint('Breast cancer dataset')\nprint('Accuracy of GaussianNB classifier on training set: {:.2f}'.format(nbclf.score(X_train, y_train)))\nprint('Accuracy of GaussianNB classifier on test set: {:.2f}'.format(nbclf.score(X_test, y_test)))","7a610f29":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_digits\n\ndataset = load_digits()\nX, y = dataset.data, dataset.target\n\nfor class_name, class_count in zip(dataset.target_names, np.bincount(dataset.target)):\n    print(class_name,class_count)","15043809":"# Creating a dataset with imbalanced binary classes:  \n# Negative class (0) - 'not digit 1' & Positive class (1) - 'digit 1'\ny_binary_imbalanced = y.copy()\ny_binary_imbalanced[y_binary_imbalanced != 1] = 0\n\nprint('Original labels:\\t', y[1:30])\nprint('New binary labels:\\t', y_binary_imbalanced[1:30])","0a2bae2e":"np.bincount(y_binary_imbalanced)    # Negative class (0) is the most frequent class","6f4f7c9d":"X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n\n# Accuracy of Support Vector Machine classifier\nfrom sklearn.svm import SVC\n\nsvm = SVC(kernel='rbf', C=1).fit(X_train, y_train)\nsvm.score(X_test, y_test)","bbf44093":"from sklearn.dummy import DummyClassifier\n\n# Negative class (0) is most frequent\ndummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\n# Therefore the dummy 'most_frequent' classifier always predicts class 0\ny_dummy_predictions = dummy_majority.predict(X_test)\n \nprint(y_dummy_predictions)","01ec8b43":"dummy_majority.score(X_test, y_test)","3f02b7ea":"svm = SVC(kernel='linear', C=1).fit(X_train, y_train)\nsvm.score(X_test, y_test)","57596c49":"from sklearn.metrics import confusion_matrix\n\n# Negative class (0) is most frequent\ndummy_majority = DummyClassifier(strategy = 'most_frequent').fit(X_train, y_train)\ny_majority_predicted = dummy_majority.predict(X_test)\nconfusion = confusion_matrix(y_test, y_majority_predicted)\n\nprint('Most frequent class (dummy classifier)\\n', confusion)","73ab8252":"# produces random predictions w\/ same class proportion as training set\ndummy_classprop = DummyClassifier(strategy='stratified').fit(X_train, y_train)\ny_classprop_predicted = dummy_classprop.predict(X_test)\nconfusion = confusion_matrix(y_test, y_classprop_predicted)\n\nprint('Random class-proportional prediction (dummy classifier)\\n', confusion)","173843eb":"svm = SVC(kernel='linear', C=1).fit(X_train, y_train)\nsvm_predicted = svm.predict(X_test)\nconfusion = confusion_matrix(y_test, svm_predicted)\n\nprint('Support vector machine classifier (linear kernel, C=1)\\n', confusion)","06a0f1c0":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression().fit(X_train, y_train)\nlr_predicted = lr.predict(X_test)\nconfusion = confusion_matrix(y_test, lr_predicted)\n\nprint('Logistic regression classifier (default settings)\\n', confusion)","0a137d66":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\ntree_predicted = dt.predict(X_test)\nconfusion = confusion_matrix(y_test, tree_predicted)\n\nprint('Decision tree classifier (max_depth = 2)\\n', confusion)","ae4604a3":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n# Accuracy = TP + TN \/ (TP + TN + FP + FN)\n# Precision = TP \/ (TP + FP)\n# Recall = TP \/ (TP + FN)  Also known as sensitivity, or True Positive Rate\n# F1 = 2 * Precision * Recall \/ (Precision + Recall) \nprint('Accuracy: {:.2f}'.format(accuracy_score(y_test, tree_predicted)))\nprint('Precision: {:.2f}'.format(precision_score(y_test, tree_predicted)))\nprint('Recall: {:.2f}'.format(recall_score(y_test, tree_predicted)))\nprint('F1: {:.2f}'.format(f1_score(y_test, tree_predicted)))","34a05280":"# Combined report with all above metrics\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, tree_predicted, target_names=['not 1', '1']))","a7c66381":"print('Random class-proportional (dummy)\\n', \n      classification_report(y_test, y_classprop_predicted, target_names=['not 1', '1']))\nprint('SVM\\n', \n      classification_report(y_test, svm_predicted, target_names = ['not 1', '1']))\nprint('Logistic regression\\n', \n      classification_report(y_test, lr_predicted, target_names = ['not 1', '1']))\nprint('Decision tree\\n', \n      classification_report(y_test, tree_predicted, target_names = ['not 1', '1']))","a4d56ca0":"X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\ny_scores_lr = lr.fit(X_train, y_train).decision_function(X_test)\ny_score_list = list(zip(y_test[0:20], y_scores_lr[0:20]))\n\n# show the decision_function scores for first 20 instances\ny_score_list","f64b5390":"X_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\ny_proba_lr = lr.fit(X_train, y_train).predict_proba(X_test)\ny_proba_list = list(zip(y_test[0:20], y_proba_lr[0:20,1]))\n\n# show the probability of positive class for first 20 instances\ny_proba_list","c213b56e":"from sklearn.metrics import precision_recall_curve\n\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores_lr)\nclosest_zero = np.argmin(np.abs(thresholds))\nclosest_zero_p = precision[closest_zero]\nclosest_zero_r = recall[closest_zero]\n\nplt.figure()\nplt.xlim([0.0, 1.01])\nplt.ylim([0.0, 1.01])\nplt.plot(precision, recall, label='Precision-Recall Curve')\nplt.plot(closest_zero_p, closest_zero_r, 'o', markersize = 12, fillstyle = 'none', c='r', mew=3)\nplt.xlabel('Precision', fontsize=16)\nplt.ylabel('Recall', fontsize=16)\nplt.axes().set_aspect('equal')\nplt.show()","d7259921":"from sklearn.metrics import roc_curve, auc\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n\ny_score_lr = lr.fit(X_train, y_train).decision_function(X_test)\nfpr_lr, tpr_lr, _ = roc_curve(y_test, y_score_lr)\nroc_auc_lr = auc(fpr_lr, tpr_lr)\n\nplt.figure()\nplt.xlim([-0.01, 1.00])\nplt.ylim([-0.01, 1.01])\nplt.plot(fpr_lr, tpr_lr, lw=3, label='LogRegr ROC curve (area = {:0.2f})'.format(roc_auc_lr))\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.title('ROC curve (1-of-10 digits classifier)', fontsize=16)\nplt.legend(loc='lower right', fontsize=13)\nplt.plot([0, 1], [0, 1], color='navy', lw=3, linestyle='--')\nplt.axes().set_aspect('equal')\nplt.show()","92433ecb":"from matplotlib import cm\n\nX_train, X_test, y_train, y_test = train_test_split(X, y_binary_imbalanced, random_state=0)\n\nplt.figure()\nplt.xlim([-0.01, 1.00])\nplt.ylim([-0.01, 1.01])\nfor g in [0.01, 0.1, 0.20, 1]:\n    svm = SVC(gamma=g).fit(X_train, y_train)\n    y_score_svm = svm.decision_function(X_test)\n    fpr_svm, tpr_svm, _ = roc_curve(y_test, y_score_svm)\n    roc_auc_svm = auc(fpr_svm, tpr_svm)\n    accuracy_svm = svm.score(X_test, y_test)\n    print(\"gamma = {:.2f}  accuracy = {:.2f}   AUC = {:.2f}\".format(g, accuracy_svm, \n                                                                    roc_auc_svm))\n    plt.plot(fpr_svm, tpr_svm, lw=3, alpha=0.7, \n             label='SVM (gamma = {:0.2f}, area = {:0.2f})'.format(g, roc_auc_svm))\n\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate (Recall)', fontsize=16)\nplt.plot([0, 1], [0, 1], color='k', lw=0.5, linestyle='--')\nplt.legend(loc=\"lower right\", fontsize=11)\nplt.title('ROC curve: (1-of-10 digits classifier)', fontsize=16)\nplt.axes().set_aspect('equal')\n\nplt.show()","3fc6e10e":"dataset = load_digits()\nX, y = dataset.data, dataset.target\nX_train_mc, X_test_mc, y_train_mc, y_test_mc = train_test_split(X, y, random_state=0)\n\n\nsvm = SVC(kernel = 'linear').fit(X_train_mc, y_train_mc)\nsvm_predicted_mc = svm.predict(X_test_mc)\nconfusion_mc = confusion_matrix(y_test_mc, svm_predicted_mc)\ndf_cm = pd.DataFrame(confusion_mc, \n                     index = [i for i in range(0,10)], columns = [i for i in range(0,10)])\n\nplt.figure(figsize=(5.5,4))\nsns.heatmap(df_cm, annot=True)\nplt.title('SVM Linear Kernel \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test_mc, \n                                                                       svm_predicted_mc)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n\n\nsvm = SVC(kernel = 'rbf').fit(X_train_mc, y_train_mc)\nsvm_predicted_mc = svm.predict(X_test_mc)\nconfusion_mc = confusion_matrix(y_test_mc, svm_predicted_mc)\ndf_cm = pd.DataFrame(confusion_mc, index = [i for i in range(0,10)],\n                  columns = [i for i in range(0,10)])\n\nplt.figure(figsize = (5.5,4))\nsns.heatmap(df_cm, annot=True)\nplt.title('SVM RBF Kernel \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test_mc, \n                                                                    svm_predicted_mc)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label');","85d779dd":"print(classification_report(y_test_mc, svm_predicted_mc))","92837502":"print('Micro-averaged precision = {:.2f} (treat instances equally)'\n      .format(precision_score(y_test_mc, svm_predicted_mc, average = 'micro')))\nprint('Macro-averaged precision = {:.2f} (treat classes equally)'\n      .format(precision_score(y_test_mc, svm_predicted_mc, average = 'macro')))","99e1c258":"print('Micro-averaged f1 = {:.2f} (treat instances equally)'\n      .format(f1_score(y_test_mc, svm_predicted_mc, average = 'micro')))\nprint('Macro-averaged f1 = {:.2f} (treat classes equally)'\n      .format(f1_score(y_test_mc, svm_predicted_mc, average = 'macro')))","acd55390":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import datasets\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.dummy import DummyRegressor\n\ndiabetes = datasets.load_diabetes()\n\nX = diabetes.data[:, None, 6]\ny = diabetes.target\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nlm = LinearRegression().fit(X_train, y_train)\nlm_dummy_mean = DummyRegressor(strategy = 'mean').fit(X_train, y_train)\n\ny_predict = lm.predict(X_test)\ny_predict_dummy_mean = lm_dummy_mean.predict(X_test)\n\nprint('Linear model, coefficients: ', lm.coef_)\nprint(\"Mean squared error (dummy): {:.2f}\".format(mean_squared_error(y_test, \n                                                                     y_predict_dummy_mean)))\nprint(\"Mean squared error (linear model): {:.2f}\".format(mean_squared_error(y_test, y_predict)))\nprint(\"r2_score (dummy): {:.2f}\".format(r2_score(y_test, y_predict_dummy_mean)))\nprint(\"r2_score (linear model): {:.2f}\".format(r2_score(y_test, y_predict)))\n\n# Plot outputs\nplt.scatter(X_test, y_test,  color='black')\nplt.plot(X_test, y_predict, color='green', linewidth=2)\nplt.plot(X_test, y_predict_dummy_mean, color='red', linestyle = 'dashed', \n         linewidth=2, label = 'dummy')\n\nplt.show()","476c5e8c":"from sklearn.model_selection import cross_val_score\nfrom sklearn.svm import SVC\n\ndataset = load_digits()\n# again, making this a binary problem with 'digit 1' as positive class \n# and 'not 1' as negative class\nX, y = dataset.data, dataset.target == 1\nclf = SVC(kernel='linear', C=1)\n\n# accuracy is the default scoring metric\nprint('Cross-validation (accuracy)', cross_val_score(clf, X, y, cv=5))\n# use AUC as scoring metric\nprint('Cross-validation (AUC)', cross_val_score(clf, X, y, cv=5, scoring = 'roc_auc'))\n# use recall as scoring metric\nprint('Cross-validation (recall)', cross_val_score(clf, X, y, cv=5, scoring = 'recall'))","48a4bcea":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\n\ndataset = load_digits()\nX, y = dataset.data, dataset.target == 1\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\nclf = SVC(kernel='rbf')\ngrid_values = {'gamma': [0.001, 0.01, 0.05, 0.1, 1, 10, 100]}\n\n# default metric to optimize over grid parameters: accuracy\ngrid_clf_acc = GridSearchCV(clf, param_grid = grid_values)\ngrid_clf_acc.fit(X_train, y_train)\ny_decision_fn_scores_acc = grid_clf_acc.decision_function(X_test) \n\nprint('Grid best parameter (max. accuracy): ', grid_clf_acc.best_params_)\nprint('Grid best score (accuracy): ', grid_clf_acc.best_score_)\n\n# alternative metric to optimize over grid parameters: AUC\ngrid_clf_auc = GridSearchCV(clf, param_grid = grid_values, scoring = 'roc_auc')\ngrid_clf_auc.fit(X_train, y_train)\ny_decision_fn_scores_auc = grid_clf_auc.decision_function(X_test) \n\nprint('Test set AUC: ', roc_auc_score(y_test, y_decision_fn_scores_auc))\nprint('Grid best parameter (max. AUC): ', grid_clf_auc.best_params_)\nprint('Grid best score (AUC): ', grid_clf_auc.best_score_)\n","bcec3e8d":"from sklearn.metrics.scorer import SCORERS\n\nprint(sorted(list(SCORERS.keys())))","286d309e":"from sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\nfrom libraryplot import plot_class_regions_for_classifier_subplot\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\n\ndataset = load_digits()\nX, y = dataset.data, dataset.target == 1\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# Create a two-feature input vector matching the example plot above\n# We jitter the points (add a small amount of random noise) in case there are areas\n# in feature space where many instances have the same features.\njitter_delta = 0.25\nX_twovar_train = X_train[:,[20,59]]+ np.random.rand(X_train.shape[0], 2) - jitter_delta\nX_twovar_test  = X_test[:,[20,59]] + np.random.rand(X_test.shape[0], 2) - jitter_delta\n\nclf = SVC(kernel = 'linear').fit(X_twovar_train, y_train)\ngrid_values = {'class_weight':['balanced', {1:2},{1:3},{1:4},{1:5},{1:10},{1:20},{1:50}]}\nplt.figure(figsize=(9,6))\nfor i, eval_metric in enumerate(('precision','recall', 'f1','roc_auc')):\n    grid_clf_custom = GridSearchCV(clf, param_grid=grid_values, scoring=eval_metric)\n    grid_clf_custom.fit(X_twovar_train, y_train)\n    print('Grid best parameter (max. {0}): {1}'\n          .format(eval_metric, grid_clf_custom.best_params_))\n    print('Grid best score ({0}): {1}'\n          .format(eval_metric, grid_clf_custom.best_score_))\n    plt.subplots_adjust(wspace=0.3, hspace=0.3)\n    plot_class_regions_for_classifier_subplot(grid_clf_custom, X_twovar_test, y_test, None,\n                                             None, None,  plt.subplot(2, 2, i+1))\n    \n    plt.title(eval_metric+'-oriented SVC')\nplt.tight_layout()\nplt.show()","067c1078":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import precision_recall_curve\nfrom libraryplot import plot_class_regions_for_classifier\nfrom sklearn.svm import SVC\n\ndataset = load_digits()\nX, y = dataset.data, dataset.target == 1\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n\n# create a two-feature input vector matching the example plot above\njitter_delta = 0.25\nX_twovar_train = X_train[:,[20,59]]+ np.random.rand(X_train.shape[0], 2) - jitter_delta\nX_twovar_test  = X_test[:,[20,59]] + np.random.rand(X_test.shape[0], 2) - jitter_delta\n\nclf = SVC(kernel='linear', class_weight='balanced').fit(X_twovar_train, y_train)\n\ny_scores = clf.decision_function(X_twovar_test)\n\nprecision, recall, thresholds = precision_recall_curve(y_test, y_scores)\nclosest_zero = np.argmin(np.abs(thresholds))\nclosest_zero_p = precision[closest_zero]\nclosest_zero_r = recall[closest_zero]\n\nplt.figure(1)\nplot_class_regions_for_classifier(clf, X_twovar_test, y_test)\n#ax.set_title(\"SVC, class_weight = 'balanced', optimized for accuracy\")\nplt.show()\n\nplt.figure()\nplt.xlim([0.0, 1.01])\nplt.ylim([0.0, 1.01])\nplt.title (\"Precision-recall curve: SVC, class_weight = 'balanced'\")\nplt.plot(precision, recall, label = 'Precision-Recall Curve')\nplt.plot(closest_zero_p, closest_zero_r, 'o', markersize=12, fillstyle='none', c='r', mew=3)\nplt.xlabel('Precision', fontsize=16)\nplt.ylabel('Recall', fontsize=16)\nplt.axes().set_aspect('equal')\nplt.show()\nprint('At zero threshold, precision: {:.2f}, recall: {:.2f}'\n      .format(closest_zero_p, closest_zero_r))","d7261d3a":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()\n(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n\nfruits = pd.read_table('..\/input\/ads-py\/fruit_colors_with_data.txt')\nX_fruits = fruits[['mass','width','height', 'color_score']]\ny_fruits = fruits[['fruit_label']] - 1","2f0004b0":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA","8cda3c27":"X_cancer[0]","634279b9":"X_normalized = StandardScaler().fit(X_cancer).transform(X_cancer)\nX_normalized[0]","88c5d1b5":"pca = PCA(n_components=2).fit(X_normalized)\n\nX_pca = pca.transform(X_normalized)\nX_pca[0]","dcceaf8b":"print(X_cancer.shape, X_pca.shape)","f2337457":"fig = plt.figure(figsize=(10,7))\nplt.imshow(pca.components_, interpolation='none', cmap='plasma')\nfeature_names = list(cancer.feature_names)\n\nplt.gca().set_xticks(np.arange(-.5, len(feature_names)));\nplt.gca().set_yticks(np.arange(.5,2));\nplt.gca().set_xticklabels(feature_names, rotation = 90, ha='left', fontsize = 12);\nplt.gca().set_yticklabels(['First_PC', 'Second_PC'], va='bottom', fontsize = 12);\nplt.colorbar(orientation = 'horizontal', ticks=[pca.components_.min(), 0, pca.components_.max()], pad=0.65);","f3761f12":"X_fruits.head().as_matrix","e5c4982b":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nX_normalized = StandardScaler().fit(X_fruits).transform(X_fruits)\nX_normalized[:2,:]","8ddee502":"pca = PCA(n_components=2).fit(X_normalized)\nX_pca = pca.transform(X_normalized)\nX_pca[:2,:]","ef6eb66f":"print(X_fruits.shape)\nprint(X_normalized.shape)\nprint(X_pca.shape)","01c04d77":"print(X_pca.shape)\nprint(y_fruits.shape)","d020372c":"def plot_labelled_scatter_modified(X, y, class_labels, xlabel='X', ylabel='Y',title='Title'):\n    num_labels = len(class_labels)\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n\n    marker_array = ['o', '^', '*']\n    color_array = ['#FFFF00', '#00AAFF', '#000000', '#FF00AA']\n    cmap_bold = ListedColormap(color_array)\n    bnorm = BoundaryNorm(numpy.arange(0, num_labels + 1, 1), ncolors=num_labels)\n    plt.figure()\n\n    plt.scatter(X[:, 0], X[:, 1], s=65, cmap=cmap_bold, norm = bnorm, alpha = 0.40, edgecolor='black', lw = 1)\n\n    plt.xlim(x_min, x_max)\n    plt.ylim(y_min, y_max)\n\n    h = []\n    for c in range(0, num_labels):\n        h.append(mpatches.Patch(color=color_array[c], label=class_labels[c]))\n    plt.legend(handles=h)\n\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n    plt.show()\n","22368a78":"#from libraryplot import plot_labelled_scatter\n\nplot_labelled_scatter_modified(X_pca, y_fruits, ['apple', 'mandarin', 'orange', 'lemon'],\n            'First Principal Componant','Second Principal Componant','Fruits Dataset PCA (n_componants = 2)')","ce405d20":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import MDS","aeda0bf2":"X_fruits_normalized = StandardScaler().fit(X_fruits).transform(X_fruits)\nX_fruits_normalized[:2,:]","41507269":"mds = MDS(n_components=2)\nX_fruits_mds = mds.fit_transform(X_fruits_normalized)\nX_fruits_mds[:2,:]","250a15cb":"print(X_fruits.shape)\nprint(X_fruits_normalized.shape)\nprint(X_fruits_mds.shape)","0bee1ae2":"#from libraryplot import plot_labelled_scatter\nplot_labelled_scatter_modified(X_fruits_mds, y_fruits, ['apple', 'mandarin', 'orange', 'lemon'], \n                               'First MDS feature', 'Second MDS feature', 'Fruit sample dataset MDS')","d38bbc47":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import MDS\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer()\n(X_cancer, y_cancer) = load_breast_cancer(return_X_y = True)\n\n# each feature should be centered (zero mean) and with unit variance\nX_normalized = StandardScaler().fit(X_cancer).transform(X_cancer)  \n\nmds = MDS(n_components = 2)\n\nX_mds = mds.fit_transform(X_normalized)\n\nfrom libraryplot import plot_labelled_scatter\nplot_labelled_scatter(X_mds, y_cancer, ['malignant', 'benign'])\n# plot_labelled_scatter(X_mds, y_cancer, ['malignant', 'benign'], 'First MDS dimension',\n#                      'Second MDS dimension', 'Breast Cancer Dataset MDS (n_components = 2)')","a343eb0d":"from sklearn.manifold import TSNE\nfrom libraryplot import plot_labelled_scatter\n\ntsne = TSNE(random_state = 0)\n\nX_tsne = tsne.fit_transform(X_fruits_normalized)\n\nplot_labelled_scatter_modified(X_tsne, y_fruits, ['apple', 'mandarin', 'orange', 'lemon'],\n                            'First t-SNE feature', 'Second t-SNE feature', 'Fruits dataset t-SNE')","1c02bb04":"tsne = TSNE(random_state = 0)\n\nX_tsne = tsne.fit_transform(X_normalized)\n\nplot_labelled_scatter(X_tsne, y_cancer, ['malignant', 'benign'])\n# plot_labelled_scatter(X_tsne, y_cancer, ['malignant', 'benign'], 'First t-SNE feature', \n#                             'Second t-SNE feature', 'Breast cancer dataset t-SNE')","6181af8f":"from sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\n\nX,y = make_blobs(random_state = 10)\n\nkmeans = KMeans(n_clusters = 3)\nkmeans.fit(X)\n\nplot_labelled_scatter(X, kmeans.labels_, ['Cluster 1 ','Cluster 2','Cluster 3'])","f28e4f3d":"from sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\n\nfruits = pd.read_table('..\/input\/ads-py\/fruit_colors_with_data.txt')\nX_fruits = fruits[['mass','width','height', 'color_score']].as_matrix()\ny_fruits = fruits[['fruit_label']] - 1\n\nprint(type(X_fruits),type(y_fruits))\nX_normalized = MinMaxScaler().fit(X_fruits).transform(X_fruits)\n\nkmeans = KMeans(n_clusters=4, random_state = 0)\nkmeans.fit(X_normalized)\n\nplot_labelled_scatter(X_fruits_normalized, kmeans.labels_, ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4'])","7eae668d":"from sklearn.datasets import make_blobs\nfrom sklearn.cluster import AgglomerativeClustering\n\nX,y = make_blobs(random_state = 10)\n\ncls = AgglomerativeClustering(n_clusters=3)\ncls_assignment = cls.fit_predict(X)\n\nplot_labelled_scatter(X, cls_assignment, ['Cluster 1', 'Cluster 2', 'Cluster 3'])","5ca65a04":"X, y = make_blobs(random_state = 10, n_samples = 10)\nplot_labelled_scatter(X, y, ['Cluster 1', 'Cluster 2', 'Cluster 3'])\nprint(X)","7be1c030":"from scipy.cluster.hierarchy import ward, dendrogram\ntry:\n    plt.figure()\n    dendrogram(ward(X))\n    plt.show()\nexcept:\n    pass","7ac04d09":"from sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_blobs\n\nX, y = make_blobs(random_state = 9, n_samples = 25)\ndbscan = DBSCAN(eps=2, min_samples=2)\n\ncls = dbscan.fit_predict(X)\nprint(\"Cluster membership values:\\n{}\".format(cls))\n\nplot_labelled_scatter(X, cls + 1, ['Noise', 'Cluster 0', 'Cluster 1', 'Cluster 2'])","701ca46e":"## Application to real dataset","accf94cf":"## Multi-class results on the fruit dataset","5fe1afb3":"## Support Vector Machine with RBF kernel: using both C and gamma parameter ","5dedeba3":"## Ridge regression with regularization parameter: alpha","dfd56be4":"# Linear Regression - Example Plot","dc519da2":"## Clustering\n### K-means","0d3e545d":"# Two-feature classification example using the digits dataset\n## Optimizing a classifier using different evaluation metrics","8cd3b5fa":"#### Multidimensional Scaling on Breast Cancer dataset","ce21c090":"## Lasso regression with regularization parameter: alpha","c0d8f61a":"#### Gradient-boosted decision trees on a real-world dataset","18673090":"#### Application to real-world dataset for classification","7b23fd66":"# Generating Synthetics Datasets ","0e5228d3":"### Application to real world dataset","023823da":"# Lasso regression","0788706b":"### Agglomerative Clustering","e9af7e9e":"## Application to real dataset","a6f5293f":"# Cross-validation\n# Example based on k-NN classifier with fruit dataset (2 features)","4560edab":"# Evaluation for Classification\n## Preamble","505222cd":"#### Gradient boosted decision trees on the fruit dataset\n","b5de7cca":"### A note on performing cross-validation for more advanced scenarios.\n\nIn some cases (e.g. when feature values have very different ranges), we've seen the need to scale or normalize the training and test sets before use with a classifier. The proper way to do cross-validation when you need to scale the data is *not* to scale the entire dataset with a single transform, since this will indirectly leak information into the training data about the whole dataset, including the test data (see the lecture on data leakage later in the course).  Instead, scaling\/normalizing must be computed and applied for each cross-validation fold separately.  To do this, the easiest way in scikit-learn is to use *pipelines*.  While these are beyond the scope of this course, further information is available in the scikit-learn documentation here:\n\nhttp:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\n\nor the Pipeline section in the recommended textbook: Introduction to Machine Learning with Python by Andreas C. M\u00fcller and Sarah Guido (O'Reilly Media).","798b06c2":"### Neural networks: Regression","ca0d93a5":"# Linear Regression","9ee411e4":"## Logistic regression on simple synthetic dataset","a55c9614":"## Micro- vs. macro-averaged metrics","afde692e":"## Multi-class classification with linear models\n## LinearSVC with M classes generates M one vs rest classifiers.","9be2b9d0":"## Ridge regression with feature normalization","c84605bf":"## Neural Networks\n#### Activation funcitons","e0b7a2a8":"## Linear Support Vector Machine","60c39e90":"# Precision-recall curves","0c614051":"## Logistic regression regularization: C parameter","c4347d0e":"## Dummy Classifiers\n\nDummyClassifier is a classifier that makes predictions using simple rules, which can be useful as a baseline for comparison against actual classifiers, especially with imbalanced classes.","7da3f5c8":"### Neural Networks : Classification\n#### Synthetic dataset 1: single hidden layer","f95f965b":"#### Decision Trees on a real-world dataset (Breast Cancer Dataset)","6cd452c3":"#### PCA on the fruit dataset (for comparison)","8405b76c":"# Evaluation measures for multi-class classification\n## Multi-class confusion matrix","ddad3a15":"## Application of SVMs to a real dataset: normalized data with feature preprocessing using minmax scaling","c898f1ee":"#### each feature should be centered (zero mean) and with unit variance","e07ffbe1":"## Getting Started with Unsupervised Learning","432058ea":"## Application of SVMs to a real dataset: unnormalized data","18d58c8d":"## Visualizing decision trees","6b92664f":"#### Regularization parameter: alpha","f51cebea":"## Naive Bayes Classifiers","1f3cb181":"## Validation curve example","d9509446":"# Ridge Regression","cf86f78c":"## Dimentionality Reduction and Manifold Learning\n### Principal Componant Analysis (PCA)\n#### Using PCA to find the first two principal componants in the Breast Cancer dataset","21d5b974":"# Model selection using evaluation metrics\n## Cross-validation example","396fa848":"k-NN 4: which distance function?   https:\/\/www.youtube.com\/watch?v=_EEcjn0Uirw\n\nMinkowski Distance https:\/\/en.wikipedia.org\/wiki\/Minkowski_distance","bde10ae7":"# Confusion matrices\n## Binary (two-class) confusion matrix\n","7a45d409":"## Linear Support Vector Machine: C parameter","fc6e3359":"#### Random Forests on a real-world dataset","936a236e":"#### t-SNE on the breast cancer dataset","e4de2a61":"# Regression evaluation metrics","b11ad6af":"#### Synthetic dataset 1: two hidden layers","386e92c9":"### Manifold Learning\n#### Multidimensional scaling (MDS) on the fruits dataset","489821bb":"### This is just the first draft version. Will include some more of my own code with lots of updates in parts 2 and 3","4ef945f3":"# Kernelized Support Vector Machines\n# Classification","e1f4cbe6":"# Multi-class classification report","29d38148":"## Decision Trees","43edc245":"## Polynomial regression","f00ab829":"## Distance Measurements Between Data Points\n\n<h3>This parameter specifies how the distance between data points in the classification\/clustering input is measured.         The options are:<\/h3>\n\n1. <b>Euclidean<\/b>: Use the standard Euclidean (as-the-crow-flies) distance.\n\n2. <b>Euclidean Squared<\/b>: Use the Euclidean squared distance in cases where you would use regular Euclidean distance in Jarvis-Patrick or K-Means clustering.\n\n3. <b>Manhattan<\/b>: Use the Manhattan (city-block) distance.\n\n4. <b>Pearson Correlation<\/b>: Use the Pearson Correlation coefficient to cluster together genes or samples with similar behavior; genes or samples with opposite behavior are assigned to different clusters.\n\n5. <b>Pearson Squared<\/b>: Use the squared Pearson Correlation coefficient to cluster together genes with similar or opposite behaviors (i.e. genes that are highly correlated and those that are highly anti-correlated are clustered together).\n\n6. <b>Chebychev<\/b>: Use Chebychev distance to cluster together genes that do not show dramatic expression differences in any samples; genes with a large expression difference in at least one sample are assigned to different clusters.\n\n7. <b>Spearman<\/b>: Use Spearman Correlation to cluster together genes whose expression profiles have similar shapes or show similar general trends (e.g. increasing expression with time), but whose expression levels may be very different.","6b7e253c":"#### Create a dendrogram (using scipy)\nThis dendrogram plot is based on the dataset created in the previous step with make_blobs, but only 10 samples have been selected","4dfedb72":"# Grid search example","aa53e2d7":"## Feature importance","ce7383f4":"# Regression ","e2b6e388":"And here's the dendrogram corresponding to agglomerative clustering of the 10 points above using Ward's method. The index 0..9 of the points corresponds to the index of the points in the X array above. For example, point 0 (5.69, -9.47) and point 9 (5.43, -9.76) are the closest two points and are clustered first.","a881b08c":"Example showing k-means used to find n clusters in the fruits dataset.  Note that in general, it's important to scale the individual features before applying k-means clustering.","5ea0b43b":"### t_SNE on the fruit dataset\nSome dimensionality reduction methods may be less successful on some datasets. Here, it doesn't work as well at finding structure in the small fruits dataset, compared to other methods like MDS.","1bc48c99":"### Before applying PCA, each feature should be centered (zero mean) and with unit variance","d33b3890":"## Logistic regression\n#### Logistic regression for binary classification on fruits dataset using height, width features (positive class: apple, negative class: others)","d94d73c5":"# ROC curves, Area-Under-Curve (AUC)","15a4c53f":"## Support Vector Machine with RBF kernel: gamma parameter","163a77be":"### Random Forests","8d937e3d":"# Support Vector Machines","10fbc789":"# Evaluation metrics for binary classification","75f2dcd3":"# Classification ","211bce28":"## Precision-recall curve for the default SVC classifier (with balanced class weights)","5c2be2e0":"#### Binarization","c25c2927":"## Evaluation metrics supported for model selection","8caa9427":"## Setting max decision tree depth to help avoid overfitting","8b6a2ca2":"#### The effect of different choices of activation function","01752a42":"# Decision functions","bb9950f4":"### DBSCAN Clustering","b36c2498":"#### plotting the magnitude of each feature value for the first two principal componants","a783820f":"# Supervised Learning","a20ef205":"## Gradient Boosted Decision Trees","a0f6e227":"### Random forest: Fruit dataset"}}