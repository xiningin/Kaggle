{"cell_type":{"9838028c":"code","7c3b6963":"code","893a4506":"code","36b15c23":"code","3b8f17ee":"code","23551b87":"code","2c99f486":"code","69dfed5b":"code","379b56d2":"code","18798b38":"code","c9d93820":"code","f9b0a45a":"code","6e77ae59":"code","05a1164d":"code","9cf37409":"code","5b02ddc5":"markdown","83e41f9d":"markdown","86c4e725":"markdown","829caaf6":"markdown","085319b4":"markdown","9482ffc5":"markdown"},"source":{"9838028c":"# Imports\nimport torch\nimport torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\nimport torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\nimport torch.nn.functional as F  # All functions that don't have any parameters\nfrom torch.utils.data import (\n    DataLoader,\n)  # Gives easier dataset managment and creates mini batches\nimport torchvision.datasets as datasets  # Has standard datasets we can import in a nice way\nimport torchvision.transforms as transforms  # Transformations we can perform on our dataset","7c3b6963":"class LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.relu = nn.ReLU()\n        self.pool = nn.AvgPool2d(kernel_size=(2,2),stride=(2,2))\n        self.conv1 = nn.Conv2d(in_channels=1,out_channels =6,kernel_size=(5,5),stride=(1,1),padding=(0,0))\n        self.conv2 = nn.Conv2d(in_channels=6,out_channels=16,kernel_size=(5,5),stride=(1,1),padding=(0,0))\n        self.conv3 = nn.Conv2d(in_channels=16,out_channels=120,kernel_size=(5,5),stride=(1,1),padding=(0,0))\n        self.linear1 = nn.Linear(120,84)\n        self.linear2 = nn.Linear(84,10)\n        \n\n    def forward(self, x):\n        x = self.relu(self.conv1(x))\n        x = self.pool(x)\n        x = self.relu(self.conv2(x))\n        x = self.pool(x)\n        x = self.relu(\n            self.conv3(x)\n        )  # num_examples x 120 x 1 x 1 --> num_examples x 120\n        x = x.reshape(x.shape[0], -1)\n        x = self.relu(self.linear1(x))\n        x = self.linear2(x)\n        return x","893a4506":"def test_lenet():\n    x = torch.randn(64, 1, 32, 32)\n    model = LeNet()\n    return model(x)\n\n\nif __name__ == \"__main__\":\n    out = test_lenet()\n    print(out.shape)","36b15c23":"# set device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","3b8f17ee":"#Hyperparameter\nin_channel = 1\nnum_classes = 10\nlearning_rate = 0.001\nbatch_size = 64\nnum_epochs = 10","23551b87":"!mkdir dataset\n!ls","2c99f486":"!pwd","69dfed5b":"# define transforms\ntransforms = transforms.Compose([transforms.Resize((32, 32)),\n                                 transforms.ToTensor()])","379b56d2":"#Load data\ntrain_dataset = datasets.MNIST(root = '\/kaggle\/working\/dataset\/',train = True ,transform =transforms,download = True)\ntrain_loader = DataLoader(dataset = train_dataset,batch_size = batch_size,shuffle = True)","18798b38":"test_dataset = datasets.MNIST(root = '\/kaggle\/working\/dataset\/',train = False,transform = transforms,download = True)\ntest_loader = DataLoader(dataset = test_dataset,batch_size = batch_size,shuffle = True)","c9d93820":"# Initialize network\n\nmodel = LeNet().to(device)","f9b0a45a":"#loss and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(),lr = learning_rate)","6e77ae59":"# Train Network\nfor epoch in range(num_epochs):\n    for batch_idx, (data, targets) in enumerate(train_loader):\n        # Get data to cuda if possible\n        data = data.to(device=device)\n        targets = targets.to(device=device)\n\n        # forward\n        scores = model(data)\n        loss = criterion(scores, targets)\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n\n        # gradient descent or adam step\n        optimizer.step()","05a1164d":"# Check accuracy on training & test to see how good our model\ndef check_accuracy(loader,model):\n    if loader.dataset.train:\n        print(\"Checking accuracy on training data\")\n    else:\n        print(\"Cehcking accuracy on test data\")\n    \n    num_correct = 0\n    num_samples = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for x,y in loader:\n            x = x.to(device = device)\n            y = y.to(device = device)\n            \n            scores = model(x)\n            _,predictions = scores.max(1)\n            num_correct += (predictions == y).sum()\n            num_samples += predictions.size(0)\n        print(f\"Got {num_correct} \/ {num_samples} with accuracy {float(num_correct)\/float(num_samples)*100:.2f}\")\n    model.train()","9cf37409":"check_accuracy(train_loader, model)\ncheck_accuracy(test_loader, model)\n","5b02ddc5":"```\nLeNet Architecture\nf=filter,s=stride,p=padding\n1*32*32 Input -> f(5*5),s = 1,p= 0 -> avg pool s=2,p=0 -> f(5*5),s=1,p=0 -> avg pool s=2,p=0\n-> Conv 5*5 to 120 channels x Linear 84 x linear 10\n```","83e41f9d":"![image.png](attachment:image.png)","86c4e725":"## About\n\n### The LeNet architecture was first introduced by LeCun et al. in their 1998 paper, Gradient-Based Learning Applied to Document Recognition.As the name of the paper suggests, the authors\u2019 implementation of LeNet was used primarily for OCR and character recognition in documents.\n\n### The LeNet architecture is straightforward and small, (in terms of memory footprint), making it perfect for teaching the basics of CNNs \u2014 it can even run on the CPU (if your system does not have a suitable GPU), making it a great \u201cfirst CNN\u201d.\n\n\n","829caaf6":"### In this notebook we will try to recreate LE-NET from scratch.","085319b4":"# LeNet \u2013 Convolutional Neural Network in Python","9482ffc5":"# Architecture Summary of LeNet"}}