{"cell_type":{"3a917eba":"code","6fc8be3a":"code","72d107df":"code","929af8fc":"code","6df28361":"code","1218d464":"code","944bba15":"code","d4470b3c":"markdown","59da069c":"markdown","2b9adfe6":"markdown","e67db544":"markdown","55790065":"markdown","97365999":"markdown","7090dad6":"markdown","3ec5ab6f":"markdown","6296022f":"markdown","022d6bd1":"markdown","2ba8a5bf":"markdown","70757f2c":"markdown","46946ca5":"markdown","b56ae110":"markdown","aeeaedd5":"markdown","7d3549e7":"markdown","eb012803":"markdown","3757841d":"markdown","72a225e4":"markdown","2cb7aec2":"markdown","a2422444":"markdown","4d9f8b9f":"markdown","9927ec8f":"markdown","86943f9e":"markdown","9711e76b":"markdown","77022ccd":"markdown","4a5e01ae":"markdown","5cd6fe97":"markdown","fc7b5bf8":"markdown","5693056e":"markdown","111865b0":"markdown","889eee9c":"markdown","d61fd548":"markdown","1e7ecfb4":"markdown","1e6707b1":"markdown","b393e2c0":"markdown","642bdf43":"markdown"},"source":{"3a917eba":"import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport seaborn as sns\n\nwarnings.filterwarnings(\"ignore\")\noptuna.logging.set_verbosity(optuna.logging.WARNING)","6fc8be3a":"import datatable as dt  # pip install datatable\n\nframe = dt.fread(\"https:\/\/raw.githubusercontent.com\/BexTuychiev\/medium_stories\/master\/2021\/august\/2_popular_kaggle_packages\/data\/station_day.csv\")\nframe.head(5)","72d107df":"type(frame)","929af8fc":"from datatable import by, f, sum\n\ntips = sns.load_dataset(\"tips\")\nframe = dt.Frame(tips)\nframe[:, sum(f.total_bill), by(f.size)]","6df28361":"import optuna  # pip install optuna\n\n\ndef objective(trial):\n    x = trial.suggest_float(\"x\", -7, 7)\n    y = trial.suggest_float(\"y\", -7, 7)\n    return (x - 1) ** 2 + (y + 3) ** 2\n\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=200)  # number of iterations\n\nstudy.best_params","1218d464":"study.best_value","944bba15":"import shap  # pip install shap\nimport xgboost as xgb\n\n# Load and train a model\nX, y = shap.datasets.diabetes()\nclf = xgb.XGBRegressor().fit(X, y)\n\n# Explain model's predictions with SHAP\nexplainer = shap.Explainer(clf)\nshap_values = explainer(X)\n\n# Visualize the predictions' explanation\nshap.plots.beeswarm(shap_values)","d4470b3c":"Above is a 100k row dataset with 75 features projected to 2D using a package called UMAP. Each dot represents a single sample in a classification problem and is color-encoded based on their class.\n\nMassive datasets like these can make you miserable during EDA, mainly because of the computation and time expenses they come with. So, it is important that each plot you create is spot-on and reveals something significant about the data.\n\nI think that's one of the reasons why UMAP (Uniform Manifold Approximation and Projection) is so well-received on Kaggle. It is efficient, low-code, and lets you take a real \"look\" at the data from a high-dimensional perspective:\n\n<p float=\"left\">\n  <img src=\"https:\/\/umap-learn.readthedocs.io\/en\/latest\/_images\/plotting_21_2.png\" width=\"250\" height=\"250\">\n  <img src=\"https:\/\/umap-learn.readthedocs.io\/en\/latest\/_images\/plotting_32_2.png\" width=\"250\" height=\"250\"> \n  <img src=\"https:\/\/umap-learn.readthedocs.io\/en\/latest\/_images\/plotting_34_2.png\" width=\"250\" height=\"250\">\n<\/p","59da069c":"Kaggle is a hot spot for what is trending in data science and machine learning.\n\nDue to its competitiveness, the top players are constantly looking for new tools, technologies, and frameworks that give them an edge over others. If a new package or an algorithm delivers actionable value, there is a good chance it receives immediate adoption and becomes popular.\n\nThis post is about 7 of such trendies packages that are direct replacements for many tools and technologies that are either outdated or need urgent upgrades.","2b9adfe6":"![](https:\/\/shap.readthedocs.io\/en\/latest\/_images\/shap_header.png)","e67db544":"An insight like this will free you from the manual task of selecting a base model, a time much better spent on tasks like feature engineering.\n\n### \ud83d\udee0 GitHub and documentation\n- https:\/\/lazypredict.readthedocs.io\/en\/latest\/index.html\n- https:\/\/github.com\/shankarpandala\/lazypredict","55790065":"Lazypredict is one of the best one-liner packages I have ever seen.\n\nUsing the library, you can train almost all Sklearn models plus XGBoost and LightGBM in a single line of code. It only has two estimators\u200a-\u200aone for regression and one for classification. Fitting either one on a dataset with a given target will evaluate more than 30 base models and generate a report with their rankings on several popular metrics.\n\n### \ud83d\udcbb Demo","97365999":"If you thought GPUs are deep learning-exclusive, you are *horribly* mistaken.\n\nThe cuDF library, created by the open-source platform RAPIDs, enables you to run tabular manipulation operations on one or more GPUs.\n\nUnlike datatable, cuDF has a very similar API to Pandas, thus offering a less steep learning curve. As it is standard with GPUs, the library is super fast, giving it an edge over datatable when combined with its Pandas-like API.\n\nThe only hassle when using cuDF is its installation\u200a-\u200ait requires:\n- CUDA Toolkit 11.0+\n- NVIDIA driver 450.80.02+\n- Pascal architecture or better (Compute Capability >=6.0)\n\nIf you want to try out the library without installation limitations, Kaggle kernels are a great option. Here is a [notebook](https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets?scriptVersionId=49328159&cellId=14) to get you started.\n\n### \ud83d\udee0 GitHub and documentation\n- https:\/\/docs.rapids.ai\/api\/cudf\/stable\/\n- https:\/\/github.com\/rapidsai\/cudf\n\n### \ud83d\udcbb Demo\nHere is a snippet from the documentation that shows a simple GroupBy operation on the tips dataset:","7090dad6":"## Here are my latest viral hits...\n- [20 Burning XGBoost FAQs Answered to Use the Library Like a Pro](https:\/\/towardsdatascience.com\/20-burning-xgboost-faqs-answered-to-use-the-library-like-a-pro-f8013b8df3e4?source=your_stories_page-------------------------------------)\n- [25 Pandas Functions You Didn\u2019t Know Existed | P(Guarantee) = 0.8](https:\/\/towardsdatascience.com\/25-pandas-functions-you-didnt-know-existed-p-guarantee-0-8-1a05dcaad5d0?source=your_stories_page-------------------------------------)\n- [Matplotlib vs. Plotly: Let\u2019s Decide Once and for All\n](https:\/\/towardsdatascience.com\/matplotlib-vs-plotly-lets-decide-once-and-for-all-ad25a5e43322?source=your_stories_page-------------------------------------)","3ec5ab6f":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*ZoH5jGuQiKKhxeZYNHcwDA.png)","6296022f":"When I look at plots like these, they remind me of why I got into data science in the first place\u200a-\u200adata is beautiful!\n\n### \ud83d\udee0 GitHub and documentation\n- https:\/\/umap-learn.readthedocs.io\/en\/latest\/\n- https:\/\/github.com\/lmcinnes\/umap\n\n### \ud83d\udd2c Papers\n- [UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction](https:\/\/arxiv.org\/abs\/1802.03426)\n\n### \ud83d\udcbb Demo\n\nUMAP offers an easy, Sklearn-compatible API. After importing the `UMAP` module, call its `fit` on the feature and target arrays (`X`, `y`) to project them to 2D by default:","022d6bd1":"## Setup","2ba8a5bf":"```python\nimport umap  # pip install umap-learn\n\n# Create the mapper\nmapper = umap.UMAP()\n# Fit to the data\nmapper.fit(X, y)\n\n# Plot as a scatterplot\numap.plot.points(mapper)\n```","70757f2c":"A simple GroupBy operation:","46946ca5":"# 7 Coolest Python Packages Top Kagglers Are Using Without Telling You\n## Let me expose the secrets...\n![](https:\/\/cdn-images-1.medium.com\/max\/1200\/1*5IFgojJ4nU8f0YKTcjWDrg.jpeg)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Photo by \n        <a href='https:\/\/www.pexels.com\/@miphotography?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Miesha Maiden<\/a>\n        on \n        <a href='https:\/\/www.pexels.com\/photo\/pineapple-with-brown-sunglasses-459601\/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels.<\/a> All images are by author unless specified otherwise.\n    <\/strong>\n<\/figcaption>","b56ae110":"# 5. SHAP","aeeaedd5":"The main data structure in `datatable` is `Frame` (as in DataFrame). ","7d3549e7":"Normally, I am against any library or tool that takes a programmer away from writing actual code. But, since auto-EDA libraries are all the rage now on Kaggle, I had to include this section for completeness.\n\nInitially, this section was supposed to be only about AutoViz, which uses XGBoost under the hood to display the most important information of the dataset (that's why I chose it). Later, I decided to include a few others as well.\n\nHere is a list of the best auto EDA libraries I have found:\n- **DataPrep** - the most comprehensive auto EDA \\[[GitHub](https:\/\/github.com\/sfu-db\/dataprep), [Documentation](https:\/\/docs.dataprep.ai\/index.html)\\]\n- **AutoViz** - the fastest auto EDA \\[[GitHub](https:\/\/github.com\/AutoViML\/AutoViz)]\n- **PandasProfiling** - the earliest and one of the best auto EDA tools \\[[GitHub](https:\/\/github.com\/pandas-profiling\/pandas-profiling), [Documentation](https:\/\/pandas-profiling.github.io\/pandas-profiling\/docs\/master\/rtd\/)\\]\n- **Lux** - the most user-friendly and luxurious EDA \\[[GitHub](https:\/\/github.com\/lux-org\/lux), [Documentation](https:\/\/lux-api.readthedocs.io\/en\/latest\/)\\]\n\n### \ud83d\udcbb Demo\nIf you want to see how each package performs EDA, check out this great [notebook](https:\/\/www.kaggle.com\/andreshg\/automatic-eda-libraries-comparisson\/notebook#6.-%F0%9F%93%8A-D-Tale-%F0%9F%93%9A) on Kaggle.","eb012803":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*1sUCX4FKCLjJzbEqMjg2YQ.png)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        <a href='https:\/\/www.kaggle.com\/subinium\/tps-jun-this-is-original-eda-viz\/notebook?scriptVersionId=64865915&cellId=37'>Link to the code of the plot.<\/a>\n    <\/strong>\n<\/figcaption>","3757841d":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*HfCFXaFA0cS2uLw1Gv3x9w.png)","72a225e4":"# 6. Rapids cuDF","2cb7aec2":"![](https:\/\/miro.medium.com\/max\/1400\/0*IBkpkOCS0anhUHWp.png)","a2422444":"Explainable AI (XAI) is one of the strongest trends in the ML and AI sphere. Companies and businesses are starting to be fussy over the adoption of AI solutions due to their \"black box\" nature.\n\nHey, no one can blame them. If data scientists themselves are coming up with tools to understand the models they created, the worries and suspicions of business owners are entirely justified.\n\nOne of those tools that often come up in grandmasters' notebooks on Kaggle is SHAP.\n\nSHAP (Shapley Additive exPlanations) is an approach to explain how a model works using concepts from game theory. At its score, SHAP uses something called Shapley values to explain:\n- Which features in the model are the most important\n- The model's decisions behind any single prediction. For example, asking which features led to this particular output.\n\nThe most notable aspects of SHAP are its unified theme and unique plots that break down the mechanics of any model and neural network. Here is an example plot that shows the feature importances in terms of Shapley values for a single prediction:\n\n![](https:\/\/raw.githubusercontent.com\/slundberg\/shap\/master\/docs\/artwork\/boston_waterfall.png)","4d9f8b9f":"# 1. UMAP","9927ec8f":"# 2. Datatable","86943f9e":"The most important parameters of `UMAP` estimator are `n_neighbors` and `min_dist` (minimum distance). Think of `n_neighbors` as a handle that controls the zoom level of the projection. `min_dist` is the minimum distance between each projected point.\n\nIf you wish to project to a higher dimension, you can tweak `n_components` just like in  Sklearn's `PCA`.","9711e76b":"As dataset sizes are getting bigger, people are paying more attention to out-of-memory, multi-threaded data preprocessing tools to escape the performance limitations of Pandas.\n\nOne of the most promising tools in this regard is `datatable`, inspired by R's `data.table` package. It is developed by H2O.ai to support parallel-computing and out-of-memory operations on big data (up to 100 GB), as required by today's machine learning applications.\n\nWhile `datatable` does not have as large a suite of manipulation functions as pandas, it is found to heavily outperform it on most common operations. In an [experiment](https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets) done on a 100M row dataset, datatable manages to read the data into memory in just over a minute, 9 times faster than pandas.","77022ccd":"The available tools and packages to execute data science tasks are endless. Everyone has the right to be overwhelmed by this.\n\nBut I sincerely hope the tools outlined in this posted helped narrow down your focus to what is trending and delivering results in the hands of members of the largest ML community.","4a5e01ae":"![image.png](attachment:41542a5f-77e6-4b56-8898-9f8cbc519658.png)","5cd6fe97":"# 3. Lazypredict","fc7b5bf8":"One of the more recent libraries I have added to my skill-stack is Kagglers' favorite - Optuna. \n\nOptuna is a next-generation automatic hyperparameter tuning framework, designed to work on virtually any model and neural network available in today's ML and Deep learning packages. \n\nIt offers several advantages over similar tools like GridSearch, TPOT, HyperOPT, etc:\n- Platform-agnostic: has APIs to work with any framework, including XGBoost, LightGBM, CatBoost, Sklearn, Keras, TensorFlow, PyTorch, etc.\n- A large suite of optimization algorithms with early stopping and pruning features baked in\n- Easy parallelization with little or no changes to the code\n- Built in support to visually explore tuning history and the importance of each hyperparameter.\n\nMy most favorite feature is its ability to pause\/resume\/save search histories. Optuna keeps track of all previous rounds of tuning and you can resume the search for however long you want until you get the performance you want. \n\nBesides, you can make Optuna RAM-independent for massive datasets and searching by storing results in a local or a remote database by just adding an extra parameter.\n\n### \ud83d\udee0 GitHub and documentation\n- https:\/\/github.com\/optuna\/optuna\n- https:\/\/optuna.readthedocs.io\/en\/stable\/\n\n### \ud83d\udd2c Papers\n- [Optuna: A Next-generation Hyperparameter Optimization Framework](https:\/\/arxiv.org\/abs\/1907.10902)\n\n### \ud83d\udcbb Demo","5693056e":"For the sake of simplicity, we are trying to optimize the function $(x - 1)^2 + (y + 3)^2$. As you can see, the tuned values for `x` and `y` are pretty close to the optimal (1, -3). \n\nHyperparamter tuning for real estimators is a bit more involved, so why don't you check out my detailed guide:\n\nhttps:\/\/www.kaggle.com\/bextuychiev\/no-bs-guide-to-hyperparameter-tuning-with-optuna","111865b0":"# 7. Automatic EDA libraries","889eee9c":"Trust me, SHAP has way cooler plots. It is such a powerful tool that the Kaggle platform has an [entire free course](https:\/\/www.kaggle.com\/learn\/machine-learning-explainability) built around it. \n\n### \ud83d\udee0 GitHub and documentation\n- https:\/\/shap.readthedocs.io\/en\/latest\/index.html\n- https:\/\/github.com\/slundberg\/shap\n\n### \ud83d\udd2c Papers\n- [A Unified Approach to Interpreting Model Predictions](https:\/\/proceedings.neurips.cc\/paper\/2017\/hash\/8a20a8621978632d76c43dfd28b67767-Abstract.html)\n- [From local explanations to global understanding with explainable AI for trees](https:\/\/www.nature.com\/articles\/s42256-019-0138-9)\n- [Explainable machine-learning predictions for the prevention of hypoxaemia during surgery](https:\/\/www.nature.com\/articles\/s41551-018-0304-0)\n\n### \ud83d\udcbb Demo\nHere is a short snippet to create a beeswarm plot of all predictions from the classic Diabeters dataset:","d61fd548":"## Summary","1e7ecfb4":"### \ud83d\udee0 GitHub and documentation\n- https:\/\/github.com\/h2oai\/datatable\n- https:\/\/datatable.readthedocs.io\/en\/latest\/?badge=latest\n\n### \ud83d\udcbb Demo","1e6707b1":"```python\nfrom lazypredict.Supervised import (  # pip install lazypredict\n    LazyClassifier,\n    LazyRegressor,\n)\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\n\n# Load data and split\nX, y = load_boston(return_X_y=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Fit LazyRegressor\nreg = LazyRegressor(ignore_warnings=True, random_state=1121218, verbose=False)\nmodels, predictions = reg.fit(X_train, X_test, y_train, y_test)  # pass all sets\n\nmodels.head(10)\n```","b393e2c0":"```python\nimport cudf, io, requests\nfrom io import StringIO\n\nurl = \"https:\/\/github.com\/plotly\/datasets\/raw\/master\/tips.csv\"\ncontent = requests.get(url).content.decode('utf-8')\n\ntips_df = cudf.read_csv(StringIO(content))\ntips_df['tip_percentage'] = tips_df['tip'] \/ tips_df['total_bill'] * 100\n\n# display average tip by dining party size\nprint(tips_df.groupby('size').tip_percentage.mean())\n```","642bdf43":"# 4. Optuna"}}