{"cell_type":{"1931fe55":"code","1c8d5419":"code","c852290f":"code","637c353b":"code","dc0c775d":"code","70eeb81b":"code","f25800c1":"code","3e223319":"code","43676ab5":"code","17c0dc53":"code","0a8aef8b":"code","1f3bb74a":"code","d79ea316":"code","ef4c2f2b":"code","266c7a4c":"code","de81bbff":"code","08f3ee09":"code","01e59302":"code","d7e9202a":"code","7c2b8806":"code","f9b455ae":"code","79351ebf":"code","43fd46e4":"code","dcc7b398":"code","6b1a8f10":"code","9af280f1":"code","9d73abbe":"markdown","766920df":"markdown","0f5fc4aa":"markdown","2078cfb8":"markdown","d0f50f59":"markdown"},"source":{"1931fe55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1c8d5419":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport datetime\nfrom sklearn.preprocessing import LabelEncoder\nfrom math import sqrt\n\npd.set_option('display.max_columns', 0)\npd.set_option('display.max_rows', 500)\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score\n\n\nfrom sklearn import tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\n","c852290f":"churn_df=pd.read_csv(\"..\/input\/churn.csv\")","637c353b":"churn_df.shape","dc0c775d":"churn_df.isna().sum()","70eeb81b":"churn_df.dtypes.value_counts()","f25800c1":"churn_df=churn_df.rename(columns={'Churn?':'churn'})\nchurn_df=churn_df.rename(columns={\"Int'l Plan\":\"Intl Plan\"})\nchurn_df.columns","3e223319":"churn_df['churn'].value_counts()","43676ab5":"churn_df['churn']=churn_df['churn'].apply(lambda x:1 if x==\"True.\" else 0 )\nchurn_df['Intl Plan']=churn_df['Intl Plan'].apply(lambda x:1 if x==\"yes\" else 0 )\nchurn_df['VMail Plan']=churn_df['VMail Plan'].apply(lambda x:1 if x==\"yes\" else 0 )\nchurn_df['VMail Plan'].value_counts()\n","17c0dc53":"churn_df.head(10)","0a8aef8b":"churn_df_dropped=churn_df.drop(['State','Area Code','Phone'],axis=1)","1f3bb74a":"churn_df_dropped.columns","d79ea316":"X=churn_df_dropped.drop(columns=['churn'])\ny=churn_df_dropped[['churn']]","ef4c2f2b":"X_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size = 0.2, random_state = 100)\n","266c7a4c":"# using logistic regression\nfrom sklearn import metrics\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ntrain_Pred = logreg.predict(X_train)\ntest_pred = logreg.predict(X_test)\nprint(\"Accuracy of Logistic regression for train\",metrics.accuracy_score(y_train,train_Pred))\nprint(\"Accuracy of Logistic regression for test\",metrics.accuracy_score(y_test,test_pred))","de81bbff":"# Using KNN \ny_train=np.ravel(y_train)\ny_test=np.ravel(y_test)\naccuracy_train_dict={}\naccuracy_test_dict={}\ndf_len=round(sqrt(len(churn_df_dropped)))\nfor k in range(3,df_len):\n    K_value = k+1\n    neigh = KNeighborsClassifier(n_neighbors = K_value, weights='uniform', algorithm='auto')\n    neigh.fit(X_train, y_train) \n    y_pred_train = neigh.predict(X_train)\n    y_pred_test = neigh.predict(X_test)    \n    train_accuracy=accuracy_score(y_train,y_pred_train)*100\n    test_accuracy=accuracy_score(y_test,y_pred_test)*100\n    accuracy_train_dict.update(({k:train_accuracy}))\n    accuracy_test_dict.update(({k:test_accuracy}))\n    print (\"Accuracy for train :\",train_accuracy ,\" and test :\",test_accuracy,\"% for K-Value:\",K_value)","08f3ee09":"# using Naive bayes\nfrom sklearn.naive_bayes import GaussianNB\nNB=GaussianNB()\nNB.fit(X_train, y_train)\ntrain_pred=NB.predict(X_train)\ntest_pred=NB.predict(X_test)\nprint(\"Accuracy of Naive bayes train set\",accuracy_score(train_pred,y_train))\nprint(\"Accuracy of Naive bayes test set\",accuracy_score(test_pred,y_test))","01e59302":"## Boosting\nimport matplotlib.pyplot as plt\n# Adaboost Classifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\ndtree = DecisionTreeClassifier(criterion='gini',max_depth=1)\n\nadabst_fit = AdaBoostClassifier(base_estimator= dtree,\n        n_estimators=5000,learning_rate=0.05,random_state=42)\n\nadabst_fit.fit(X_train, y_train)\n\n#print (\"\\nAdaBoost - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train,adabst_fit.predict(X_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \nprint (\"\\nAdaBoost  - Train accuracy\",round(accuracy_score(y_train,adabst_fit.predict(X_train)),3))\n#print (\"\\nAdaBoost  - Train Classification Report\\n\",classification_report(y_train,adabst_fit.predict(X_train)))\n\n#print (\"\\n\\nAdaBoost  - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test,adabst_fit.predict(X_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \nprint (\"\\nAdaBoost  - Test accuracy\",round(accuracy_score(y_test,adabst_fit.predict(X_test)),3))\n#print (\"\\nAdaBoost - Test Classification Report\\n\",classification_report(y_test,adabst_fit.predict(X_test)))","d7e9202a":"# Gradientboost Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbc_fit = GradientBoostingClassifier(loss='deviance',learning_rate=0.05,n_estimators=5000,\n                                     min_samples_split=2,min_samples_leaf=1,max_depth=1,random_state=42 )\ngbc_fit.fit(X_train,y_train)\n\n#print (\"\\nGradient Boost - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train,gbc_fit.predict(X_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \nprint (\"\\nGradient Boost - Train accuracy\",round(accuracy_score(y_train,gbc_fit.predict(X_train)),3))\n#print (\"\\nGradient Boost  - Train Classification Report\\n\",classification_report(y_train,gbc_fit.predict(X_train)))\n\n#print (\"\\n\\nGradient Boost - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test,gbc_fit.predict(X_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \nprint (\"\\nGradient Boost - Test accuracy\",round(accuracy_score(y_test,gbc_fit.predict(X_test)),3))\n#print (\"\\nGradient Boost - Test Classification Report\\n\",classification_report(y_test,gbc_fit.predict(X_test)))\n","7c2b8806":"# Xgboost Classifier\nimport xgboost as xgb\n\nxgb_fit = xgb.XGBClassifier(max_depth=2, n_estimators=5000, learning_rate=0.05)\nxgb_fit.fit(X_train, y_train)\n\n#print (\"\\nXGBoost - Train Confusion Matrix\\n\\n\",pd.crosstab(y_train,xgb_fit.predict(X_train),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \nprint (\"\\nXGBoost - Train accuracy\",round(accuracy_score(y_train,xgb_fit.predict(X_train)),3))\n#print (\"\\nXGBoost  - Train Classification Report\\n\",classification_report(y_train,xgb_fit.predict(X_train)))\n\n#print (\"\\n\\nXGBoost - Test Confusion Matrix\\n\\n\",pd.crosstab(y_test,xgb_fit.predict(X_test),rownames = [\"Actuall\"],colnames = [\"Predicted\"]))      \nprint (\"\\nXGBoost - Test accuracy\",round(accuracy_score(y_test,xgb_fit.predict(X_test)),3))\n#print (\"\\nXGBoost - Test Classification Report\\n\",classification_report(y_test,xgb_fit.predict(X_test)))","f9b455ae":"from sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Choose the type of classifier. \nclf = DecisionTreeClassifier()\n\n\n# Choose some parameter combinations to try\nparameters = {'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [i for i in range(1,20)], \n              'min_samples_split': [i for i in range(2,10)],\n              'min_samples_leaf': [i for i in range(2,10)]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nprint(clf.fit(X_train, y_train))\n\n","79351ebf":"#Predict target value and find accuracy score\ny_pred_train = clf.predict(X_train)\nprint(\"Accuracy score of train is \",accuracy_score(y_train, y_pred_train))\ny_pred = clf.predict(X_test)\nprint(\"Accuracy score of test is \",accuracy_score(y_test, y_pred))","43fd46e4":"# using Naive Bayes\nfrom sklearn.svm import SVC\ndef fit_predict(train, test, y_train, y_test, scaler, kernel = 'linear', C = 1.0, degree = 3):\n    train_scaled = scaler.fit_transform(train)\n    test_scaled = scaler.transform(test)        \n    lr = SVC(kernel = kernel, degree = degree, C = C)\n    lr.fit(train_scaled, y_train)\n    y_pred = lr.predict(test_scaled)\n    print(accuracy_score(y_test, y_pred))\n","dcc7b398":"y_train=np.ravel(y_train)\ny_test=np.ravel(y_test)\nfor kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n    print('Accuracy score using {0} kernel:'.format(kernel), end = ' ')\n    fit_predict(X_train, X_test, y_train, y_test, StandardScaler(), kernel)","6b1a8f10":"for \u0441 in np.logspace(-1,3 ,base = 2, num = 6):\n    print('Accuracy score using penalty = {0} with rbf kernel:'.format(\u0441), end = ' ')\n    fit_predict(X_train, X_test, y_train, y_test, StandardScaler(), 'poly', \u0441)","9af280f1":"for degree in range(2, 6):\n    print('Accuracy score using degree = {0} with poly kernel:'.format(degree), end = ' ')\n    fit_predict(X_train, X_test, y_train, y_test, StandardScaler(), 'poly', 1.5, degree = degree)","9d73abbe":"**Kernel tuning**","766920df":"**Import necessary package**","0f5fc4aa":"**Choosing degree for poly kernel**","2078cfb8":"**From the above iteration we can see accuracy good for K-value : 5 **","d0f50f59":"**Penalty tuning**\n"}}