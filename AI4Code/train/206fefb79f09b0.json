{"cell_type":{"64a4e7eb":"code","42c3320b":"code","da890434":"code","1fa31224":"code","e457302d":"code","de0b67d3":"code","ccf2905c":"code","b16aba87":"code","bd7ca49d":"code","17648105":"code","0fc49eab":"code","111a84fb":"code","46573f61":"code","76e6122e":"code","cde67a7b":"code","b69082d5":"code","268cdde2":"code","573dfedf":"code","75beee7e":"code","55017289":"code","fb4d00b9":"code","89c2ee28":"code","43616c39":"code","09d9c838":"code","2c16683a":"code","18e16902":"code","8aac2c5d":"code","f313208b":"code","7100fb70":"code","79458a85":"code","c5309af5":"code","bcffd830":"code","99dad7e1":"code","0d31fc02":"code","47989f21":"code","93d8553d":"code","51bfc336":"code","1efaaf44":"code","11e65909":"code","fa6f61f0":"code","22464b35":"code","1ada8e99":"code","864ef661":"code","bbf249bb":"code","d6c178c5":"code","663ea5d6":"code","06426f95":"code","2cb50d93":"code","3dcd2f40":"code","a060d82b":"code","c80710b2":"code","2846f075":"code","616f5377":"code","89384333":"markdown","2985100a":"markdown","419769e9":"markdown","190fcd88":"markdown","4c5d75b0":"markdown","22612890":"markdown","f39fb1bc":"markdown","14c0194a":"markdown","961930dd":"markdown","419ce777":"markdown","58e2a3a2":"markdown","6e644022":"markdown","eb9f0a6f":"markdown","dada6ad3":"markdown","351fc0a1":"markdown","0cbbcf06":"markdown","8c3055a9":"markdown","ad1db56f":"markdown","226ab427":"markdown","af8e9c0a":"markdown","3bb1c10a":"markdown","da260cfd":"markdown","d58cc9e8":"markdown","df59323e":"markdown","94f6efd2":"markdown","8327aebb":"markdown","412a58e7":"markdown","abbb6770":"markdown","884c8545":"markdown"},"source":{"64a4e7eb":"!pip install xgboost==1.5.0\n!pip install shap\n!pip install optuna\n!pip install seaborn\n!pip install pandas_profiling==3.1.0","42c3320b":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","da890434":"import numpy as np \nimport pandas as pd \n\nfrom pandas_profiling import ProfileReport\n\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom xgboost import XGBClassifier, XGBRegressor\nimport xgboost as xgb\n\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, \\\n                                SGDClassifier, RidgeClassifier, PassiveAggressiveClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error, classification_report, f1_score, roc_auc_score, accuracy_score\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder, RobustScaler, PowerTransformer\nfrom sklearn.utils import shuffle\nfrom sklearn import metrics\n\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nimport optuna\nfrom optuna.samplers import TPESampler\n\nfrom tqdm.notebook import tqdm\nimport gc\nimport shap\nimport pickle\n\n%matplotlib inline\n\n#plt.rcParams['figure.dpi'] = 100\n#plt.rcParams.update({'font.size': 16})\n\n# load JS visualization code to notebook\nshap.initjs()","1fa31224":"path_with_data = '\/kaggle\/input\/tabular-playground-series-nov-2021\/'\npath_to_data = '\/kaggle\/working\/'","e457302d":"DEBUG = False\nTRAIN_MODEL = True\nINFER_TEST = True\nONE_FOLD_ONLY = False\nCOMPUTE_IMPORTANCE = True\nOOF = True","de0b67d3":"train, test, sub = pd.read_csv(path_with_data + \"train.csv\", index_col=\"id\"), \\\n    pd.read_csv(path_with_data + \"test.csv\", index_col=\"id\"), \\\n    pd.read_csv(path_with_data + \"sample_submission.csv\")\n\nif DEBUG:\n    train = train[:50000]\n    test = test[:50000]\n\nprint(f'Train shape: {train.shape}')\nprint(f'Test shape: {test.shape}')\n\ntarget = 'target'","ccf2905c":"train.head(5)","b16aba87":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","bd7ca49d":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","17648105":"profile = ProfileReport(train, title=\"Pandas Profiling Report\", explorative=False, minimal=True, dark_mode=True)\nprofile","0fc49eab":"#sns.relplot(data=train, x=train['f0'], y=train['f1'], kind='scatter', hue='target')\n#sns.displot(data=train, x='f1', kind='hist', hue='target')\n#sns.displot(data=train, x='f1', kind='kde', hue='target', fill=True)\n#sns.jointplot(data=train, x=train['f0'], y=train['f1'], kind='scatter', hue='target')\n#sns.pairplot(data=train[['f0', 'f1', 'f2', 'f3']], hue='target')  # very slow","111a84fb":"predictors_amount = 20 + 1  # should div by 4  + 1\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Spearman Correlation of Features', y=1.05, size=15)\ncorrmat = train.corr(method='spearman').abs()\ncols = corrmat.nlargest(predictors_amount, target)[target].index\ncm = abs(np.corrcoef(train[cols].values.T))\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nmost_correlated = list(set(cols) - set([target]))","46573f61":"# plot the first most correlated features \n\ni = 1\ncols_amount = 4\nrows_amount = int(len(most_correlated) \/ cols_amount) \nplt.figure()\nfig, ax = plt.subplots(rows_amount, cols_amount, figsize=(20, 22))\nfor feature in most_correlated:\n    plt.subplot(rows_amount, cols_amount, i)\n    sns.histplot(train[feature],color=\"blue\", kde=True, bins=100, label='train_'+feature)\n    sns.histplot(test[feature],color=\"olive\", kde=True, bins=100, label='test_'+feature)\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","76e6122e":"sns.boxplot(data=train[most_correlated])","cde67a7b":"columns = train.columns\npreproc = dict()\npreproc['target'] = target","b69082d5":"to_drop = [target]","268cdde2":"features = [col for col in train.columns if col not in to_drop ]\npreproc['features'] = features","573dfedf":"# Threshold for removing correlated variables\nthreshold = 0.90\n# Absolute value correlation matrix\ncorr_matrix = train[features].corr(method='spearman').abs()\n# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n# Select columns with correlations above threshold\nhighly_correlated = [column for column in upper.columns if any(upper[column] > threshold)]\nfeatures = [col for col in features if col not in highly_correlated]\npreproc['features'] = features","75beee7e":"threshold = 0\nzero_std = train[features].std().index[train[features].std() <= threshold]\nfeatures = [col for col in features if col not in zero_std]    \npreproc['features'] = features","55017289":"threshold = 1  # in %\nzero_cv = (100 * train[features].std() \/ train[features].mean()).index[(100 * train[features].std() \/ train[features].mean()) <= threshold]\nfeatures = [col for col in features if col not in zero_cv]\npreproc['features'] = features","fb4d00b9":"scaler = RobustScaler()\nscaler.fit(train[features])\ntrain[features] = scaler.transform(train[features])\ntest[features] = scaler.transform(test[features])\npreproc['scaler'] = scaler","89c2ee28":"if 0:\n  pt = PowerTransformer()\n  pt.fit(train[features])\n  train[features] = pt.transform(train[features])\n  test[features] = pt.transform(test[features])\n  preproc['power_transformer'] = pt","43616c39":"# plot the first most correlated features \n\ni = 1\ncols_amount = 4\nrows_amount = int(len(most_correlated) \/ cols_amount) \nplt.figure()\nfig, ax = plt.subplots(rows_amount, cols_amount, figsize=(20, 22))\nfor feature in most_correlated:\n    plt.subplot(rows_amount, cols_amount, i)\n    sns.histplot(train[feature],color=\"blue\", kde=True, bins=100, label='train_'+feature)\n    sns.histplot(test[feature],color=\"olive\", kde=True, bins=100, label='test_'+feature)\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","09d9c838":"features = preproc['features']\nX_train, X_test, y_train, y_test = train_test_split(train[features], \n                                                    train[target],\n                                                    stratify=train[target], \n                                                    test_size=0.25, \n                                                    random_state=42)","2c16683a":"clfs = {\n        'Logistic Regression': LogisticRegression(random_state=0), \n        'Naive Bayes': GaussianNB(),\n        #'SVM': SVC(gamma='auto'),\n        #'Random Forest': RandomForestClassifier(random_state=0),\n        'SGD Classifier': SGDClassifier(random_state=0),\n        'Ridge': RidgeClassifier(random_state=0),\n        'Passive Aggressive Classifier': PassiveAggressiveClassifier(random_state=0),\n        #'KNN': KNeighborsClassifier(),\n        #'MLP': MLPClassifier(),\n        'Decision Tree': DecisionTreeClassifier()\n       }","18e16902":"for clf_name in clfs:   \n    clf = clfs[clf_name].fit(X_train, y_train)\n    y_pred = clf.predict(X_test)   \n    print(f'{clf_name}: F1 = {f1_score(y_test, y_pred)}, AUC = {roc_auc_score(y_test, y_pred)}, Accuracy = {accuracy_score(y_test, y_pred)}')\n    ","8aac2c5d":"clfs = {\n        'XGBoost': XGBClassifier(tree_method='gpu_hist', predictor='gpu_predictor'),\n        'LGB': LGBMClassifier()\n       }","f313208b":"for clf_name in clfs:   \n    clf = clfs[clf_name].fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(f'{clf_name}: F1 = {f1_score(y_test, y_pred)}, AUC = {roc_auc_score(y_test, y_pred)}, Accuracy = {accuracy_score(y_test, y_pred)}')","7100fb70":"baseline_model = LogisticRegression(random_state=0)\nbaseline_model.fit(X_train, y_train)","79458a85":"preds = baseline_model.predict(test[features])\ny_pred = baseline_model.predict(X_test) \nprint(f'LR: F1 = {f1_score(y_test, y_pred)}, AUC = {roc_auc_score(y_test, y_pred)}, Accuracy = {accuracy_score(y_test, y_pred)}')","c5309af5":"sub['target'] = preds\nsub.to_csv(path_to_data + 'submission_bl.csv', index=False)","bcffd830":"train_oof = np.zeros((train.shape[0],))\ntest_preds = 0\ntrain_oof.shape","99dad7e1":"xgb_params= {\n        \"objective\": \"binary:logistic\",\n        \"eval_metric\": \"error\",  \n        \"seed\": 2001,\n        'tree_method': \"gpu_hist\",\n        'predictor': 'gpu_predictor'\n    }","0d31fc02":"test_xgb = xgb.DMatrix(test[features])","47989f21":"NUM_FOLDS = 5\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train[features], train[target]))):\n        #print(f'Fold {f}')\n        train_df, val_df = train[features].iloc[train_ind], train[features].iloc[val_ind]\n        train_target, val_target = train[target].iloc[train_ind], train[target].iloc[val_ind]\n                      \n        train_df = xgb.DMatrix(train_df, label=train_target)\n        val_df = xgb.DMatrix(val_df, label=val_target)\n        \n        model =  xgb.train(xgb_params, train_df, 100)\n        temp_oof = model.predict(val_df)\n        temp_test = model.predict(test_xgb)\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/NUM_FOLDS\n        \n        print(accuracy_score(np.round(temp_oof), val_target))","93d8553d":"%%time\nshap_preds = model.predict(test_xgb, pred_contribs=True)","51bfc336":"# summarize the effects of all the features\nshap.summary_plot(shap_preds[:,:-1], test[features])","1efaaf44":"shap.summary_plot(shap_preds[:,:-1], test[features], plot_type=\"bar\")","11e65909":"%%time\nshap_interactions = model.predict(xgb.DMatrix(test[features][:50000]), pred_interactions=True)","fa6f61f0":"def plot_top_k_interactions(feature_names, shap_interactions, k):\n    # Get the mean absolute contribution for each feature interaction\n    aggregate_interactions = np.mean(np.abs(shap_interactions[:, :-1, :-1]), axis=0)\n    interactions = []\n    for i in range(aggregate_interactions.shape[0]):\n        for j in range(aggregate_interactions.shape[1]):\n            if j < i:\n                interactions.append(\n                    (feature_names[i] + \"*\" + feature_names[j], aggregate_interactions[i][j] * 2))\n    # sort by magnitude\n    interactions.sort(key=lambda x: x[1], reverse=True)\n    interaction_features, interaction_values = map(tuple, zip(*interactions))\n    plt.bar(interaction_features[:k], interaction_values[:k])\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show()\n    return interaction_features\n\ninteractions_to_add = 10    \ninteraction_features = plot_top_k_interactions(features, shap_interactions, interactions_to_add)","22464b35":"def add_new_features(df, interaction_features, amount_of_features):\n    features_list = interaction_features[:amount_of_features]\n    for feat in features_list: \n      first_name, second_name = feat.split('*')\n      df[feat] = df[first_name]*df[second_name]\n    return df, features_list","1ada8e99":"train, features_added = add_new_features(train, interaction_features, interactions_to_add)\ntest, _ = add_new_features(test, interaction_features, interactions_to_add)\nfeatures += list(features_added)\n\ndel test_xgb\ndel shap_interactions\ngc.collect()","864ef661":"features_added","bbf249bb":"scaler = RobustScaler()\nscaler.fit(train[features])\ntrain[features] = scaler.transform(train[features])\ntest[features] = scaler.transform(test[features])\npreproc['scaler'] = scaler","d6c178c5":"X_train, X_test, y_train, y_test = train_test_split(train[features], \n                                                    train[target],\n                                                    stratify=train[target], \n                                                    test_size=0.25, \n                                                    random_state=42)","663ea5d6":"baseline_model_af = LogisticRegression(random_state=0)\nbaseline_model_af.fit(X_train, y_train)\npreds = baseline_model_af.predict(test[features])\ny_pred = baseline_model_af.predict(X_test) \nprint(f'LR: F1 = {f1_score(y_test, y_pred)}, AUC = {roc_auc_score(y_test, y_pred)}, Accuracy = {accuracy_score(y_test, y_pred)}')","06426f95":"sub['target'] = preds\nsub.to_csv(path_to_data + 'submission_blaf.csv', index=False)","2cb50d93":"# HPO using opuna\n\ndef lr_objective(trial):\n    params = {\n        'C': trial.suggest_loguniform('C', 1e-8, 1000.0),        \n        'solver': trial.suggest_categorical('solver', ['lbfgs', 'liblinear']), \n        'random_state': 42,\n        'penalty' : 'l2',         \n    }\n    \n    X_train, X_val, y_train, y_val = train_test_split(train[features], train[target], test_size = 0.25, random_state = 42)\n    \n    model = LogisticRegression(**params)    \n    model.fit(X_train, y_train)\n    pred_val = model.predict(X_val)\n    \n    return roc_auc_score(y_val, pred_val)","3dcd2f40":"sampler = TPESampler(seed = 42)\nstudy = optuna.create_study(study_name = 'LR optimization',\n                            direction = 'maximize',\n                            sampler = sampler)\nstudy.optimize(lr_objective, n_trials = 10)\n\nprint(\"Best AUC:\", study.best_value)\nprint(\"Best params:\", study.best_params)","a060d82b":"if 1:\n    params = study.best_params\nelse:\n    params = {'C': 0.00045858194103088424, 'solver': 'liblinear'}","c80710b2":"#EPOCH = 250\n#BATCH_SIZE = 512\nNUM_FOLDS = 5\nCOLS = features.copy()\n\nkf = StratifiedKFold(n_splits=NUM_FOLDS, shuffle=True, random_state=42)\ntest_preds = []\noof_preds = []\nfor fold, (train_idx, test_idx) in enumerate(kf.split(train[features], train[target])):\n        \n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[features].iloc[train_idx], train[features].iloc[test_idx]\n        y_train, y_valid = train[target].iloc[train_idx], train[target].iloc[test_idx]\n        \n        filename = f\"folds{fold}.pkl\"\n        \n        if TRAIN_MODEL:\n            #model = LogisticRegression(C = params['C'], solver = params['solver'])\n            model = LogisticRegression(**params)\n            model.fit(X_train, y_train)\n            pickle.dump(model, open(path_to_data + filename, 'wb'))                                    \n            \n        else:                  \n            model = pickle.load(open(path_to_data + filename, 'rb'))                  \n    \n        if OOF:\n            print(' Predicting OOF data...')                \n            oof = model.predict(X_valid)\n            baseline_accuracy = accuracy_score(y_valid, oof)            \n            oof_preds.append(baseline_accuracy)\n            print('OOF Accuracy = {0}'.format(baseline_accuracy))\n            print(' Done!')\n                       \n        if INFER_TEST:\n            print(' Predicting test data...')\n            preds = model.predict(test[features])\n            test_preds.append(np.array(preds))\n            print(' Done!')\n                    \n        if COMPUTE_IMPORTANCE:\n            # from  https:\/\/www.kaggle.com\/cdeotte\/lstm-feature-importance\n            results = []\n            print(' Computing feature importance...')\n            \n            # COMPUTE BASELINE (NO SHUFFLE)\n            oof = model.predict(X_valid)\n            baseline_accuracy = accuracy_score(y_valid, oof)\n            results.append({'feature':'BASELINE','accuracy':baseline_accuracy})\n                                    \n            for k in tqdm(range(len(COLS))):\n                \n                # SHUFFLE FEATURE K\n                save_col = X_valid.copy()\n                np.random.shuffle(X_valid[COLS[k]].values)\n                                \n                # COMPUTE OOF Accuracy WITH FEATURE K SHUFFLED\n                oof = model.predict(X_valid)\n                acc = accuracy_score(y_valid, oof)\n                results.append({'feature':COLS[k],'accuracy':acc})                               \n                \n                X_valid = save_col.copy()\n         \n            # DISPLAY FEATURE IMPORTANCE\n            print()\n            df = pd.DataFrame(results)\n            df = df.sort_values('accuracy')\n            plt.figure(figsize=(10,20))\n            plt.barh(np.arange(len(COLS)+1),df.accuracy)\n            plt.yticks(np.arange(len(COLS)+1),df.feature.values)\n            plt.title('Feature Importance',size=16)\n            plt.ylim((-1,len(COLS)+1))\n            plt.plot([baseline_accuracy,baseline_accuracy],[-1,len(COLS)+1], '--', color='orange',\n                     label=f'Baseline OOF\\naccuracy={baseline_accuracy:.3f}')\n            plt.xlabel(f'Fold {fold+1} OOF accuracy with feature permuted',size=14)\n            plt.ylabel('Feature',size=14)\n            plt.legend()\n            plt.show()\n                               \n            # SAVE LSTM FEATURE IMPORTANCE\n            df = df.sort_values('accuracy',ascending=False)\n            df.to_csv(f'feature_importance_fold_{fold+1}.csv',index=False)\n                               \n        # ONLY DO ONE FOLD\n        if ONE_FOLD_ONLY: break","2846f075":"y_pred_proba = model.predict_proba(X_valid)[:, 1]\nfpr, tpr, _ = metrics.roc_curve(y_valid,  y_pred_proba)\nauc = metrics.roc_auc_score(y_valid, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data, auc for last fold = \" + str(round(auc*100,2)))\nplt.legend(loc=4)\nplt.show()","616f5377":"sub['target'] = sum(test_preds) \/ NUM_FOLDS\nsub.to_csv(path_to_data + 'submission.csv', index=False)","89384333":"# Full Pipeline for classic & modern ML (sklearn API), SHAP-based variables interaction with GPU acceleration.","2985100a":"#### Distribution Plots with changes","419769e9":"#### Scaler transform","190fcd88":"## Classic ML baselines","4c5d75b0":"Sources:\n\nhttps:\/\/www.kaggle.com\/rumasinha\/featureselectionanddiffmodelexperiments\nhttps:\/\/www.kaggle.com\/tunguz\/tps-02-21-feature-importance-with-xgboost-and-shap\nhttps:\/\/www.kaggle.com\/hamzaghanmi\/make-it-simple","22612890":"#### Pandas profiler","f39fb1bc":"### Memory reducing","14c0194a":"#### intermediate conclusion\nBaseline Logistic regression model gives us 0.734 on LB, which is within 95% of max LB result (0.75091)","961930dd":"### Feature preprocessing","419ce777":"## Add new features using XGBoost and SHAP","58e2a3a2":"### Plot of roc curve for the last fold","6e644022":"#### Scaler transform","eb9f0a6f":"## Cross-validation with optimized params","dada6ad3":"## Modern ML baselines","351fc0a1":"#### Collinear (highly correlated) features","0cbbcf06":"#### Select features","8c3055a9":"#### Power transform","ad1db56f":"### Submission prepare","226ab427":"Final conclusion: the best baseline mode is the best without feature engineering :)","af8e9c0a":"## Best Baseline model","3bb1c10a":"#### Seaborn plots","da260cfd":"### Simple EDA","d58cc9e8":"## Baseline model with added features","df59323e":"#### Zero coefficient of variantion","94f6efd2":"Contents:\n\n    - Simple basic EDA\n    \n    - Feature preprocessing\n    \n    - Classic ML baselines\n    \n    - Modern ML baselines\n    \n    - Best Baseline model\n    \n    - Add new features using XGBoost and SHAP\n    \n    - Baseline model with added features\n    \n    - Hyperparameters tuning (Optuna)\n    \n    - Cross-validation with optimized params\n    \n    - Submission prepare","8327aebb":"## Hyperparameters tuning (Optuna)","412a58e7":"#### Data split","abbb6770":"#### Correlation heatmap","884c8545":"#### Zero standard deviation"}}