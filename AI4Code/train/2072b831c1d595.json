{"cell_type":{"7236657a":"code","f666a006":"code","6fc0205f":"code","72b1f059":"code","59e6a34b":"code","20522736":"code","7049f5b3":"code","04cba621":"code","2143f450":"code","60291db9":"code","07496153":"code","789363e1":"code","7fa97b98":"code","9ccbc522":"code","f6b172a4":"code","5b712877":"code","c28f14c5":"code","a1865dbb":"code","89c88113":"code","a1aba70c":"code","319a7d73":"code","d9b591ac":"code","a7fd0d24":"code","d612f7d6":"code","49d889e3":"code","1cf335cd":"code","b7db4ed6":"code","f13a7441":"code","fb8ed430":"code","38e2828c":"code","4ddff1c9":"code","02efae72":"code","922614d6":"code","342fdb7e":"code","5edcce08":"code","30e18206":"code","927937f9":"code","e64b6a0d":"code","ab51be0e":"code","382ef3ae":"code","f712a11f":"code","b0a0983c":"code","a7776d28":"code","58039532":"code","d3d7a98d":"code","99155db7":"code","8c4aefb7":"code","0843b14f":"markdown","5ae84c56":"markdown","fec12499":"markdown","0e2c0360":"markdown","04af7317":"markdown","f1a8ffe7":"markdown","bbe2fef5":"markdown","0ee6f0ed":"markdown","a0650402":"markdown","1747da3b":"markdown","dc8b7d5f":"markdown","0a1e1824":"markdown","d8887c5f":"markdown","4a032779":"markdown","b3c5d58b":"markdown","67c3f2d2":"markdown","5485787d":"markdown","eecba541":"markdown","ec15c6e7":"markdown","d8c3249b":"markdown","3328fe14":"markdown","d18ee18a":"markdown","84be3033":"markdown","acdc4f1f":"markdown","326afd14":"markdown","0349cd3a":"markdown","3e44a90c":"markdown","830de1d7":"markdown","865debd3":"markdown","af715d50":"markdown","f25b8393":"markdown","2853cc74":"markdown","98e19936":"markdown","945d133a":"markdown","66ae9fde":"markdown"},"source":{"7236657a":"!pip install tensorflow_datasets\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport spacy\nimport re\nimport nltk\nimport seaborn as sns\n\nimport tensorflow as tf\nimport tensorflow_datasets as tfds\nfrom tensorflow import keras\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import defaultdict\nfrom collections import Counter\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import model_selection, naive_bayes\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.preprocessing import MaxAbsScaler\nfrom scipy import stats\nfrom sklearn.metrics import make_scorer, roc_auc_score\nfrom scipy.linalg import svd\nfrom numpy import diag\nfrom scipy.sparse import csr_matrix\n\nfrom numpy import zeros\nfrom sklearn import svm\n\nfrom nlp_functions import *\n","f666a006":"np.random.seed(500)","6fc0205f":"def encode_train(text_tensor, label):\n    text = text_tensor.numpy()[0]\n    encoded_text = encoder.encode(text)\n    return encoded_text, label\n\ndef encode_map_fn_train(text, label):\n    return tf.py_function(encode_train, inp=[text, label], Tout=(tf.int64, tf.int64))\n\ndef encode_unseen(text_tensor):\n    text = text_tensor.numpy()[0]\n    encoded_text = encoder.encode(text)\n    return encoded_text, 1\n\ndef encode_map_fn_unseen(text):\n    return tf.py_function(encode_unseen, inp=[text], Tout=(tf.int64, 1))","72b1f059":"disaster_original = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndisaster_original.head(5)","59e6a34b":"disaster_original['target'].unique()","20522736":"disaster_original.shape","7049f5b3":"unseen_data = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nunseen_id = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","04cba621":"disaster_original = drop_columns(disaster_original)\nunseen_data = unseen_data.drop(columns=['id', 'keyword', 'location'])\ndisaster_original.head(5)","2143f450":"unseen_data['text'] = unseen_data['text'].replace(\"\", \"empty\")","60291db9":"disaster_tweets = disaster_original.copy()\n\nplt.figure(figsize=(10, 5))\nplt.hist(disaster_tweets['target'])\nplt.title('Histogram of Target Variable')\nplt.xlabel('No \/ Yes')\nplt.ylabel('Frequency of target value')\nplt.show()","07496153":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nfor train_index, test_index in split.split(disaster_tweets, disaster_tweets['target']):\n    \n    strat_training_set = disaster_tweets.loc[train_index]\n    strat_testing_set = disaster_tweets.loc[test_index]","789363e1":"plt.figure(figsize=(10, 5))\nplt.hist(strat_testing_set['target'])\nplt.xlabel('No \/ Yes')\nplt.ylabel('Frequency of target value')\nplt.title('Histogram of Target value - Stratified Test Set')\nplt.show()","7fa97b98":"plt.figure(figsize=(10, 5))\nplt.hist(strat_training_set['target'])\nplt.xlabel('No \/ Yes')\nplt.ylabel('Frequency of target value')\nplt.title('Histogram of Target value - Stratified Train Set')\nplt.show()","9ccbc522":"strat_training_set = pre_process(strat_training_set)\nstrat_testing_set = pre_process(strat_testing_set)\nunseen_data = pre_process(unseen_data)","f6b172a4":"X_train = strat_training_set.drop(columns=['target'])\nY_train = strat_training_set['target']\n\nX_test = strat_testing_set.drop(columns=['target'])\nY_test = strat_testing_set['target']","5b712877":"disaster_tweets = pre_process(disaster_tweets)\n\ntf_idf_vect = TfidfVectorizer(max_features=300, sublinear_tf=True)\ntf_idf_vect.fit(disaster_tweets['text'])\n\nX_train_tfidf = tf_idf_vect.transform(X_train['text'])\nX_test_tfidf = tf_idf_vect.transform(X_test['text'])","c28f14c5":"tSVD = TruncatedSVD(n_components=3)\ndata_3d = tSVD.fit_transform(X_train_tfidf)\n\nsvd_df = pd.DataFrame()\nsvd_df['svd_one'] = data_3d[:, 0]\nsvd_df['svd_two'] = data_3d[:, 1]\nsvd_df['svd_three'] = data_3d[:, 2]\n","a1865dbb":"plot_vectors(svd_df, strat_training_set)","89c88113":"nb = naive_bayes.MultinomialNB()\nnb.fit(X_train_tfidf, Y_train)","a1aba70c":"nb_validation_predictions = nb.predict(X_test_tfidf)\nnb_training_predictions = nb.predict(X_train_tfidf)","319a7d73":"print(f\"Naive Bayes Basline Validation Accuracy: {accuracy_score(nb_validation_predictions, Y_test) * 100}\")\nprint(f\"Naive Bayes Basline Training Accuracy: {accuracy_score(nb_training_predictions, Y_train) * 100}\")","d9b591ac":"scaler = MaxAbsScaler()\nX_train_svm_scaled = scaler.fit_transform(X_train_tfidf)\nX_test_svm_scaled = scaler.fit_transform(X_test_tfidf)","a7fd0d24":"Y_train_svm = pd.Series(np.where(Y_train == 0, -1, 1))\nY_test_svm = pd.Series(np.where(Y_test == 0, -1, 1))","d612f7d6":"svm_ = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\nsvm_.fit(X_train_svm_scaled, Y_train_svm)","49d889e3":"svm_validation_prediction = svm_.predict(X_test_svm_scaled)\nsvm_training_predictions = svm_.predict(X_train_svm_scaled)","1cf335cd":"print(f\"Support Vector Machines - Training Accuracy: {accuracy_score(svm_training_predictions, Y_train_svm)}\")\nprint(f\"Support Vector Machines - Validation Accuract: {accuracy_score(svm_validation_prediction, Y_test_svm)}\")","b7db4ed6":"svm_rand_opt_2 = svm.SVC(C=3.2291456156839677,\n                      gamma=0.4856666290873001,\n                        tol=0.7120239961045746,\n                        kernel='rbf')\nsvm_rand_opt_2.fit(X_train_svm_scaled, Y_train_svm)","f13a7441":"svm_rand_valid_predictions = svm_rand_opt_2.predict(X_test_svm_scaled)\nsvm_rand_train_predictions = svm_rand_opt_2.predict(X_train_svm_scaled)","fb8ed430":"print(f\"Support Vector Machines - Training Accuracy: {accuracy_score(svm_rand_train_predictions, Y_train_svm)}\")\nprint(f\"Support Vector Machines - Validation Accuract: {accuracy_score(svm_rand_valid_predictions, Y_test_svm)}\")","38e2828c":"unseen_text = unseen_data['text']\ndisaster_text = disaster_original['text']\nall_tweets_concat = pd.concat([unseen_text, disaster_text], axis=0)","4ddff1c9":"target = disaster_tweets.pop('target')\ntweets_raw = tf.data.Dataset.from_tensor_slices((disaster_tweets.values, target.values))\ntweets_unseen = tf.data.Dataset.from_tensor_slices((unseen_data.values))\nunseen_raw_all = tf.data.Dataset.from_tensor_slices((all_tweets_concat.values))","02efae72":"for tw in tweets_raw.take(3):\n    tf.print(tw[0].numpy()[0][ :50], tw[1])","922614d6":"tweets_raw = tweets_raw.shuffle(7613, reshuffle_each_iteration=False)\ntweets_valid = tweets_raw.take(2283)\ntweets_train = tweets_raw.skip(2283)","342fdb7e":"tokeniser = tfds.features.text.Tokenizer()\ntoken_counts = Counter()\n\nfor example in unseen_raw_all:\n    tokens = tokeniser.tokenize(example.numpy())\n    token_counts.update(tokens)","5edcce08":"print(f\"Vocab SIze: {len(token_counts)}\")","30e18206":"encoder = tfds.features.text.TokenTextEncoder(token_counts)\nexample_string = \"This is an example\"\nprint(f\"Exmaple String: {example_string}\")\nprint(f\"Encoded String: {encoder.encode(example_string)}\")","927937f9":"tweets_train = tweets_train.map(encode_map_fn_train)\ntweets_valid = tweets_valid.map(encode_map_fn_train)\ntweets_unseen_map = tweets_unseen.map(encode_map_fn_unseen)","e64b6a0d":"print(\"Example Sequences and their length:\\n\")\nexample = tweets_train.take(8)\nfor ex in example:\n    print(f\"Individual Size: {ex[0].shape}\")","ab51be0e":"print(\"Batched examples and the sequence length:\\n\")\nbatched_example = example.padded_batch(4, padded_shapes=([-1], []))\nfor batch in batched_example:\n    print(f\"Batch dimension: {batch[0].shape}\")","382ef3ae":"tweets_train = tweets_train.padded_batch(32, padded_shapes=([-1], []))\ntweets_valid = tweets_valid.padded_batch(32, padded_shapes=([-1], []))\ntweets_unseen_batched = tweets_unseen_map.padded_batch(32, padded_shapes=([-1], []))","f712a11f":"embedding_dimension = 200\nvocab_size = len(token_counts) + 2\n\ntf.random.set_seed(42)\n\nbi_lstm_model = tf.keras.Sequential([\n    \n    tf.keras.layers.Embedding(\n        \n        input_dim=vocab_size,\n        output_dim=embedding_dimension,\n        name='embed-layer'),\n    \n    tf.keras.layers.Bidirectional(\n        \n        tf.keras.layers.LSTM(64, name='lstm-layer'),\n        name='bidir-lstm'),\n    \n    \n    \n    tf.keras.layers.Dense(64, activation='relu'),\n    \n    tf.keras.layers.Dropout(0.2),\n    \n    tf.keras.layers.Dense(1, activation='sigmoid')\n    \n])","b0a0983c":"bi_lstm_model.summary()","a7776d28":"bi_lstm_model.compile(optimizer=tf.keras.optimizers.Nadam(1e-3),\n                     loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n                     metrics=['accuracy'])","58039532":"history = bi_lstm_model.fit_generator(tweets_train, validation_data=tweets_valid,\n                           epochs=50, callbacks=[keras.callbacks.EarlyStopping(patience=8)])","d3d7a98d":"predictions = history.model.predict_classes(tweets_unseen_batched, batch_size=None)","99155db7":"output_df = pd.DataFrame(unseen_id['id'])\noutput_df['target'] = predictions","8c4aefb7":"out = output_df.to_csv('RNN.csv')","0843b14f":"<br>\n\n<h2><ins>4. Building Models<\/ins><\/h2><br>\n\n![lego_small.jpg](attachment:lego_small.jpg)\n\n\n<h3><ins>4.1. Model 1 - Naive Bayes <\/ins><\/h3><br>\n\nTo begin our analysis we considered `Naive Bayes` as an initial model, a popular classifier for unstructured text with proven ability in the field. \n\n`Naive Bayes` performs well with a small amount of data and uses prior knowledge to calculate a posterior probability, represented by a probability distribution which reflects the likelihood that a specific instance belongs to a particular class.\n\nThe algorithm is said to be \"naive\" as it makes some assumptions about our environement; firstly, each variable is statistically independent of one another, and secondly, each feature has equal importance. These two prerequisites are very rarely found in the real world, however with these simplyfing assumptions we can infer the likelhood of a particular event using the foundational theory of Baysian decision making and conditional probilities.  \n\n<br>\n","5ae84c56":"<br>\n\n<h3><ins>2.6. Splitting the Stratified Samples<\/ins><\/h3><br>\n\nOur final task in this part of the data processing simply invovles seperting our independent and dependent variables. \n\n<br>","fec12499":"<t><h1><ins>Disaster Tweet Classification Challange - Frank Mitchell<\/ins><\/h1><br><br>\n\n![changed.jpg](attachment:changed.jpg)\n\n<h2><ins>1. Introduction<\/ins><\/h2><br>\n\n\nAs an increasingly popular platform for up-to-date news around the world, Twitter has become synonymous with real-time event monitoring. However, it relies on the (often) short description of events from everyday members of the public. These short messages contain colloqualisms, shorthand and acronymns that can be deeply ambigious. \n\nTake the following tweets,\n\n`Sonar Festival is pure, absolute fire!!!`<br>\n\n\n`A massive fire is destroying half the world right now!!`<br>\n\nBoth make refernce to the concept of `fire`, with one completely innocous, and the other a vital peice of information for the public at large. By using machine learning and neural network techniques we aim to identify tweets that make reference to genuine disasters.<br>\n\n<br>\n\n\n<h3><ins>1.1 Libraries<\/ins><\/h3><br>\n\nFirst we need to import the necessary libraries, an `NLP` package with great functionality and versatility, the `Natural Language ToolKit (NLTK)`.\n\n---\n","0e2c0360":"<br>\n\n<h4><ins>4.2.3 Interpreting Random Search Best Model Results - Support Vector Machines<\/ins><\/h4><br>\n\nWith our optimised parameters we have increased our accuracy on the validation data by a small margin. What's intersting is that our gap between training and validation has lengthened considerably, indicating that these parameters are actually slightly overfitting. \n\nThis could indicate that the model would benefit from further exploiration of the `C` parameter to induce further regularisation and induce some smoothing over our predictive function. Further measures to mitigate this effect could be addding more data or reducing the complexity of the model through experimentation with different `kernels`. \n\n\n\n<br>","04af7317":"<br>\n\n<h3><ins>5.2. Gathering All Tweets<\/ins><\/h3><br>\n\nAs with our statistical machine learning models, we must represent our tweets in numerical format to infer any meaningful predictions. This involves representing our tweets as vectors of scalar numbers, represented by a unique index number. These index numbers are created by compiling all of the unique words in our corpus(every tweet available) into a vocabualry. \n\nOnce compiled, this vocabulary is used (along with some nifty `Tensorflow` wrappers) to convert each tweet into a series of its corresponding index numbers. This method of encoding our text is preferable to a one hot encoding approach (where our features == the size of the vocabulary, and each instance has a count of the number of times that word appears in a tweet). One of the issues with one hot encoding is that it produces a very sparse matrix, making it harder for the network to map inputs to outputs. \n\n<br>","f1a8ffe7":"<br>\n\n<h3><ins>5.3. Tensorflow Datasets<\/ins><\/h3><br>\n\nHere we have maintained a low-level workflow, using in-built `Tensorflow` methods to create a dataframe from our csv. Although this involves a bit more programming, the benefits will be felt when the model enters production, as the preprocessing of textual data is baked into the model architecture. \n\n<br>","bbe2fef5":"<br>\n\n<h3><ins>2.4. Startified Sampling<\/ins><\/h3><br>\n\nThe slight imbalance of target values above (in favour of no disaster) indicate that we should conduct `Stratified Sampling`. Using pre-built `SKLearn` methods this technique maintains the same distribution of independent variables throughout each of our stratified samples (training and validation). \n\nHere we opted for an initial `80\/20` split due to the relatively low number of tweets, however this parameter can have a direct affect on model accruacy and should be considered in the same respect as every other hyperparameter. \n\n<br>","0ee6f0ed":"<br>\n\n<h4><ins>4.2.2. Baseline Model - Support Vector Machines<\/ins><\/h4><br>\n\nWith our labels prepared and the features vectorised we can go ahead and fit our first `Support Vector Machine` model using the default parameters. Provided we have an acceptable baseline we can further explore the paramter space within the model. \n\nAs evidenced by our accuracy results below, `Support Vector Machines` is an even better fit to our data, maintaining a small difference between training and validation, and scoring a higher accuracy overall. This improved predictive power warrants further exploration of the parameter space. \n\n<br>\n","a0650402":"<br>\n\n<h3><ins>2.3. Target Variable Distribution<\/ins><\/h3><br>\n\nWith any classification task we need to check the distribution of our dependent variable. We want to ensure that our target is evenly distributed amongst the training and validation sets. Its important that we make a copy of the data to work on, this keeps the original dataset completely untouched. \n\n<br>\n","1747da3b":"<h4><ins>4.1.1 Baseline Model - Naive Bayes<\/ins><\/h4><br>\n\nWith our data prepared we can instantiate a `Naive Bayes` classifier by invoking an `SKLearn` class, initially maintaining default parameters to establish a baseline performance. We will use this baseline performance as a means of deciding whether to further explore the parameter space of `Naive Bayes`, or consider a different model for our problem.\n\n<br>","dc8b7d5f":"<br>\n\n<h3><ins>5.9. RNN Results<\/ins><\/h3><br>\n\nOur initial model is performing similiarily to our statistical machine learning models with approx 78% accuracy on the validation set. Through examination of the `validation` and `accuracy` scores we can begin to diagnose the models shortcomings and aim to ahieve a higher accuracy of classification. \n\n<br>","0a1e1824":"<br>\n\n<h3><ins>1.3 TensorWrapper Functions<\/ins><\/h3><br>\n\nThese functions allow us to call the `.map` method on a tensorflow dataset. \n\nThey are held here, rather than in the utility script, as the `tensorflow encoder` needs to be accessed by the model and passing it through the wrapper parameters was just as messy. \n\nAll other custom functions are imported from our utility script, `nlp_functions`.<br>\n<br>","d8887c5f":"<br>\n\n<h4><ins>4.1.2 Baseline Results - Naive Bayes<\/ins><\/h4><br>\n\nOur baseline `Naive Bayes` model performed relatively well (although there is plenty of room for improvement) achieving `76.49%` accuracy on the unseen validation data. What's intersting is that our training accuracy has a very similiar value to our validation accuracy, indicating that the model is indeed a good fit. \n\nIt may be that we are already pushing the limits of accuracy with our limited dataset, `NLP` tasks traditionally perform better when exposed to more data.\n\nThe `GuassianNB` object also cannot be tuned using `Gridsearch` or `RandomSearch` as the only arguments the object accepts are alternative `priors`. For this reason we decided to move forward with a different model implementation. \n\n<br>\n\n---\n","4a032779":"<br>\n\n<h2><ins>2. Initial data exploration<\/ins><\/h2><br>\n\n![eda_small.jpg](attachment:eda_small.jpg)\n\n<br>\n\nWith our libraries prepared it is now time to import the data and begin our initial examination. Our aim here is to check the varioua features included within the data and take a closer look at our target variable `target`. \n\nWe begin by loading the data and examining the top 5 instances. \n\n<br>\n\n<h3><ins>2.1. Import data<\/ins><\/h3><br>\n\nA simple call to an in-built Pandas method to load the data and check how many values appear in our target column. \n\nAs we can see, there are only two possible values for our target variable, making this a `binary classification` task. \n\n<br>\n\n","b3c5d58b":"<br>\n\nIt's also important that we import the unseen data so that all transformations can be similary applied. \n\n<br>","67c3f2d2":"<br>\n\n<h3><ins>2.2. Drop Columns<\/ins><\/h3><br>\n\nThe `id`, `keyword` and `location` columns are all entirley null, and intuitively shouldn't add any value to the model's ability to classify the body of the tweet as disaster `1` or no disaster `0`.\n\nWe drop the columns and recheck the first five entries. \n\n<br>","5485787d":"<br>\n\nWith our samples ready we can now visualise the distribution of the dependent variable across each of these samples and compare them with the original dataset. \n\n<br>","eecba541":"<br>\n\n<h4><ins>4.2.1 Changing the Labels - Support Vector Machines<\/ins><\/h4><br>\n\nWith `Support Vector Machines` we need to change the labels to `-1` and `1` (as oppossed to `0` and `1`). This is to make the math more manageable by allowing us to express the margin (or how close a data point is to the seperating hyperplane) with a single equation. \n\n<br>","ec15c6e7":"<br>\n\nThis results in a sparse matrix of numerical values to represent each of the tweets. Using this sparse matrix we can visualise our tweets in vector space and assign a hue based on the target variable. \n\n<br>","d8c3249b":"<br>\n\n<h3><ins>5.4. Training and Validation Sets<\/ins><\/h3><br>\n\nHaving gathered all of our data (features and labels) into a `Tensor` object, we can now go ahead and split this into seperate `training` and `validation` sets. Not only does this allow our model to check its accuracy after each epoch, it is also crucial to have these two scores to help diagnose `overfitting` or `underfitting`. Here we have opted for roughly 70 \/ 30 train\/validation split, but as mentioned above this ratio has a direct impact on model performance and should be taken into consideration when optimising the model.  \n\n<br>\n","3328fe14":"<br>\n\nHere we print out the first five instances to ensure that our data has been succesfully converted into a `Tensor`. \n\n<br>","d18ee18a":"<h2><ins>3. Word Vectorisation<\/ins><\/h2><br><br>\n\n![data_small.jpg](attachment:data_small.jpg)\n\n<br>\n\n<h3><ins>3.1 TF-IDF Vectoriser<\/ins><\/h3><br>\n\nAs an initial method of numerically representing each tweet we opted for TF-IDF, which measure the count of words in a particular document but balances this with the appearence of that word over the whole corpus (every tweet included in the data). \n\nAn alternative (and possibly better vector technique) would be to uslise a `word2vec` model to embed our tweets and assign vector values. This is something we plan to explore in the future, however the ease with which we can implement TF-IDF using built in `SKLearn` classes and functions make it a good starting point to begin fitting our model. \n\nTo achieve this vectorisation we first preprocess all of our tweets (seen and unseen data) and instantiate a `TDidfVectorizer` object which will perform the necessary calculations to assign `TF-IDF` values to each of our documents. We then use this trained model to assign features to our stratified samples. \n\n<br>","84be3033":"<br>\n\n<h2><ins>5. Recurrent Neural Network<\/ins><\/h2><br>\n\n![brain_small.jpg](attachment:brain_small.jpg)\n\n<h3><ins>5.1. Introduction<\/ins><\/h3><br>\n\nMoving on from statistical Machine Learning models we now consider the `Recurrent Neural Network` architecture as a means of classifying disaster tweets. This type of neural model is known for its success in classifying (and generating) text data. \n\nHere we consider the tweets as `sequential` data, and by doing so we are saying that the order of words are important when it comes to classifying a tweet. We use a `many to one` category of sequence model, meaning we have a sequence of many data points and are classifying one label. \n\nThe difference between a normal, fully-connected feedforward network and a `Recurrent Neural Net` is that an RNN's hidden layer recieves its input from both the input layer at the current time step, and the hidden layer from the previous time step. This allows the network to have a `memory of past events`, utilising `Long Short Term Memory Cells (LSTM)` to capture long range interactions. \n\n\n<br>","acdc4f1f":"<br>\n\n<h3><ins>5.8. Recurrent Bidirectional Model<\/ins><\/h3><br>\n\nWith the preliminary steps complete we now start building our model, opting for a `bidirectional` architecture using `LSTM`. The `bidirectional` wrapper makes two passes over each input sequence (a forward pass and a backward pass), effectively allowing the model to examine the sequence in both directions. \n\nInitially we have opted for an `embedding dimension` size of 200 and a `Dropout` rate of 0.2 to help avoid `overfitting`. These parameters, together with the `Nadam` optimiser can all be tuned to achieve higher accuracy (as well as experimenting with various architectures). For now, we have opted for a simple network consisting of\n\n1. An embedding layer.\n2. A recurrenct LSTM layer in a bidirectional wrapper.\n3. A fully connected layer.\n4. A dropout layer.\n5. A final fully connected output layer. \n\nThe choice of activation function and weight initialisation will also have a direct affect on model performance and should be considered when optimising.\n\n<br>","326afd14":"<br>\n\n<h3><ins>5.6. Map Vector Function<\/ins><\/h3><br>\n\nThe beauty of using the `py_function` call within `Tensorflow` is that it simply maps our function to the `Tensorflow data object`, meaning the conversion happens wihin the `.fit` and `.predict` method. Utilising our custom functions we can achieve this with one line of code for each of our datasets. \n\n<br>\n    ","0349cd3a":"<br>\n\n<h3><ins>4.2 Model 2 - Support Vector Machines<\/ins><\/h3><br>\n\nOur next consideration for classifying tweets was inspired by the visualisation of our data points in vector space. As we can see from the graph above, the data has been nicely segmented by our `Singular Value Decompsition`, with a clear plane discernable that looks to be a good fit for our data. \n\nAs a binary classifier for unstructured text, `Support Vector Machines` also has a good reputation for handling this type of problem. The model aims to create a seperating hyperplane between the classes in the data, acting as our decision boundary for new, unseen data. \n\nThe data points that lie on the threshold of this hyperplane are known as `support vectors`, and we are aiming to maximise the margin between these `support vectors` (for each class) and the seperating hyperplane. The algorithm makes predictions by taking the dot product of our weights and the instance inputs, feeding this to a simple activation function (such as `sigmoid`) and receiving a class label as output. \n\n`Support Vector Machines` finds the correct weights and sloping factor to disect our data with the optimum seperating hyperplane. \n\n\nOne other point to note is that `Support Vector Machines` is very sensitive to feature scaling, so we conduct `MinMaxScaling` on our features.\n\n<br>\n\n","3e44a90c":"<br>\n\n<h3><ins>5.5.Tokenise the Tweets<\/ins><\/h3><br>\n\nNow we build our vocabulary of words, assigning each unique word an index and then using this to create tweet vectors for each of our tweets. Also demonstrated below is a short example of changing a sentence to a vector. \n\nOnce complete, it's important that we use the low-level `Tensorflow` method `py_function`. This allows us to use the `map` method of dataset objects on a native `Tensorflow` data structure. The first function, `encode` treats the input tensors as if `eager execution` is enabled, allowing the second function, `encode_map_fn` to wrap the first and covert it to a `Tensorflow` operator.\n\nAs mentioned above, the benefit of such an approach means the preprocessing of text data is baked into the model, making for a more efficient, tidier deployment. \n\n<br>\n\n","830de1d7":"<br>\n\nFrom our visulalisation we can see the `TF-IDF` has split the data quite well, showing a relatively clear split between our target values. \n\nThis graph further evidences the effectiveness of word embeddings as a means of facilitating learning from unstructred text data. \n\n<br>","865debd3":"<br>\n\n<h3><ins>3.2. Visualise the Vectors<\/ins><\/h3><br>\n\nUsing `Singular Value Decomposition` we can reduce the number of dimensions in the data and visualise the tweets in three-dimensional vector space, colouring each instance based on the target variable. \n\n<br>","af715d50":"<br>\n\n<h3><ins>2.5. Text preprocessing<\/ins><\/h3><br>\n\nTo begin to understand any text data it's important to represent the data in an appropriate format. This means taking ech tweet and embedding the text in vector space using the `TF-IDF (Term Frequency- Invert Document Frequency)` distance metric. `TF-IDF` creates a sparse matrix of `documents * words`, with a words TF-IDF value assigned to each field. \n\nIn order to embed tweets we first need to clean the text, which involves a number of steps;\n\n1. Make all text lowercase.\n2. Tokensie the text (turn each tweet into a list of string tokens).\n3. Remove stopwords (removing redundant words like 'and', 'the' or 'a').\n4. Stemming the text (taking each word down to its root).\n\nIn order to ensure easy and efficient reporducability, these formatting techniques are performed by our custom function. \n\n<br>","f25b8393":"<br>\n\n<h3><ins>5.7. Padding Our Sequences<\/ins><\/h3><br>\n\nOur `Recurrent Neural Network` can handle input with different sequence lengths (tweets of varying size), however to process the `batches` properly we must ensure that each of the sequences in a batch are all matched in length. Thankfully, this is easily achieved using more `Tensorflow` operations.\n\nBelow we dictate how many instances per batch, then `Tensorflow` will automatically gather instances into batches, recording the size of the largest sequence in the batch and then padding the other instances accordingly. We demonstrate this function below before applying to our training, validation and unseen data. \n\n<br>","2853cc74":"<br>\n\n<h4><ins>4.2.3 Random Search CV - Support Vector Machines<\/ins><\/h4><br>\n\nHaving found a model that provides an acceptable baseline accuracy, we can further explore the depths of the model's paramter space using `RandomSearchCV`. This `SKLearn` object allows us to easily test a wide range of hyperparamter values, record the results, and choose the best fitting model for our data. \n\nTo conduct a `RandomSearch` we simply set up a parameter grid (a dictionary containing hyperparameter values for the `RandomSearch` to iterate over). Once complete the best model is saved into our instantiated `RandomSearch` object. \n\nFor our `RandomSearch` we are going to explore three parameters, `C`, `gamma` and `tol`:\n\n1. C: This is a regularisation parameter (a technique used to induce a smoothing effect on our function and avoid overfitting.\n\n2. Gamma: This parameter is the kernel coefficient, used to configure the sensitivity to differences in feature vectors.\n\n3. Tol: Tolerance for stopping criterion \n\nDue to the time-intensive nature of Random Search we have opted to explore only these three parameters. \n\n---\n\n\n<b><i>For the sake of compute and process time I have included the code block for the Random Search below. This search yielded the following results:<\/i><\/b>\n    \n    1. C = 3.2291456156839677\n    2. gamma = 0.4856666290873001,\n    4. tol = 0.7120239961045746\n    \n\n<b><i>These parameter values were used to fit the model below<\/i><\/b>\n\n\n---\n\n\n\n`rand_grid = {\n    \"C\": stats.uniform(2, 10),\n    \"gamma\": stats.uniform(0.1, 1),\n    \"tol\": stats.uniform(0.1, 1)\n}`\n\n`scorer = make_scorer(roc_auc_score)\n svm_rand = svm.SVC()\n rand_search = RandomizedSearchCV(svm_rand, param_distributions=rand_grid,\n                                cv=10, scoring=scorer,\n                                return_train_score=True)\n search = rand_search.fit(X_train_svm_scaled, Y_train_svm)`\n\n\n---\n\n<br>","98e19936":"<br>\n\n<h2><ins>6. Further Research<\/ins><\/h2><br>\n\nWith our `RNN` producing an acceptable baseline we can now begin exploring some improvements, such as:\n\n1. Different embeddings.\n2. RandomSearch wrapper to tune hyperparameters.\n3. Further research on state-of-the-art architectures for document classification. \n4. Deep Convolutional Neural Network for detecting salient features at any point in the document. \n\n\n---","945d133a":"<br>\n\n<h3><ins>1.2 Set Random Seed<\/ins><\/h3><br>\n\nA short piece of boilerplate code to set our random seeds and ensure reproducability of results.\n\n<br>","66ae9fde":"<br> \n\nThe unseen data includes two tweets that consist of blank strings, causing issues with the `Recurrent Neural Network` tweet vectors, and so must be replaced with at least one character. In this instance we have chosen to replace the blank string with the word `empty`.\n\n<br>"}}