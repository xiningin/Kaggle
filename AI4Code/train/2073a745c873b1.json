{"cell_type":{"741870b8":"code","51c229ee":"code","d3b042ba":"code","8acad9a7":"code","e455486d":"code","cc02f873":"code","bfd6094b":"code","6cba9d0c":"code","7865a9e7":"code","ca58a182":"code","6257b289":"code","8e173d81":"code","c6402d26":"code","57faee4e":"code","90e9ff34":"code","0e87b960":"code","a0319845":"code","d092bcf8":"code","19f39610":"code","c90b5b66":"code","54337e56":"code","63023a68":"code","165a40de":"code","8dd410da":"markdown","6fc75001":"markdown","dad0f6c2":"markdown","c5c1d7be":"markdown","95c63a27":"markdown","7a36208f":"markdown","6303b65a":"markdown","a93bf82e":"markdown","0f8a575b":"markdown","05a7be0f":"markdown","074a42b0":"markdown","2f44abc6":"markdown","be881e02":"markdown","c0e3fab4":"markdown","b67adca6":"markdown","d73f963c":"markdown"},"source":{"741870b8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","51c229ee":"data=pd.read_csv('\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Training Data.csv')\ndata.info()","d3b042ba":"data.head()","8acad9a7":"import matplotlib.pyplot as plt\n\ntotal=list(data.Risk_Flag.value_counts())\nFlag0=total[0]\nFlag1=total[1]\n\nplt.figure(figsize=(8,8))\nplt.pie([Flag0, Flag1], labels=['Non-Risk:\\n%d total' %Flag0,'Risk:\\n%d total' %Flag1], autopct='%1.2f%%')","e455486d":"import seaborn as sns\ng=sns.catplot(x='STATE', data=data, height=12, aspect=1.5, kind='count', palette='deep')\ng.set_xticklabels(rotation=60)","cc02f873":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,12))\nplt.xticks(rotation=60)\nsns.barplot(x='STATE', y='Risk_Flag', data=data, palette='deep')","bfd6094b":"sns.catplot(x='Experience', y='Income', data=data, kind=\"violin\", height=8, aspect=1.6, palette='deep')","6cba9d0c":"sns.displot(x='Age', data=data, height=8, aspect=1.5, hue='Risk_Flag', bins=20)","7865a9e7":"#binnig ages\ndata['Age_group']=pd.qcut(data.Age,5)\n\ng = sns.FacetGrid(data=data, row='House_Ownership', col='Married\/Single', height=5, aspect=1.5)\ng.map_dataframe(sns.barplot,x='Age_group', y='Risk_Flag', ci=None)\ng.set_xticklabels(rotation=60)","ca58a182":"#encode categorical\ndata['Married\/Single']=data['Married\/Single'].map({'single':0, 'married':1})\ndata['House_Ownership']=data['House_Ownership'].map({'norent_noown':0, 'rented':1, 'owned':2})\ndata['Car_Ownership']=data['Car_Ownership'].map({'no':0, 'yes':1})","6257b289":"#Each of variables below are negatively correlated with the Risk_Flag\ndata.corr().Risk_Flag.drop(['Risk_Flag','Id']).plot.bar()","8e173d81":"#get dummies and drop columns to avoid multicollinearity\ndummies=pd.get_dummies(data[['STATE', 'Profession']])\ndummies.drop(dummies.columns[[0, -1]], axis=1, inplace=True)","c6402d26":"#Selected features\nfeatures=['Income', 'Age', 'Experience', \n          'CURRENT_JOB_YRS', 'CURRENT_HOUSE_YRS', 'Married\/Single','House_Ownership', 'Car_Ownership']\nX=pd.concat([data[features], dummies], axis=1)\ny=data['Risk_Flag']\n\nX.head()","57faee4e":"#splitting the dataset\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","90e9ff34":"from imblearn.ensemble import BalancedRandomForestClassifier\nfrom sklearn.metrics import f1_score, accuracy_score, roc_auc_score, plot_roc_curve, plot_confusion_matrix\n\nbrf=BalancedRandomForestClassifier().fit(X_train, y_train)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (16,6))\nplt.title('asfafasf')   \nax1.set_title('Confusion matrix (Balanced RF)')\nax2.set_title('ROC curve (Balanced RF)')\nax2.plot([0,1], [0,1], 'g--', alpha=0.25)\n    \nplot_confusion_matrix(brf, X_test, y_test, cmap=plt.cm.Blues, normalize='true', ax=ax1)\nplot_roc_curve(brf, X_test, y_test, ax=ax2)\n\ny_pred = brf.predict(X_test)\n\nacc_brf=accuracy_score(y_test, y_pred)\nf1_brf=f1_score(y_test, y_pred)\nroc_brf=roc_auc_score(y_test, y_pred)\nprint('Roc_Auc score: %.3f' %roc_brf)    ","0e87b960":"from imblearn.over_sampling import ADASYN \n\nprint ('Initial size:', X_train.shape)\n\nada = ADASYN(random_state=42)\nX_ada, y_ada = ada.fit_resample(X_train, y_train)\n        \nprint ('Resampled size:', X_ada.shape)","a0319845":"from sklearn.ensemble import RandomForestClassifier\n\nrf_ada=RandomForestClassifier().fit(X_ada, y_ada)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (16,6))  \nax1.set_title('Confusion matrix (RF and ADASYN)')\nax2.set_title('ROC curve (RF and ADASYN)')\nax2.plot([0,1], [0,1], 'g--', alpha=0.25)\n    \nplot_confusion_matrix(rf_ada,X_test, y_test, cmap=plt.cm.Blues, normalize='true', ax=ax1)\nplot_roc_curve(rf_ada, X_test, y_test, ax=ax2)\n\ny_pred = rf_ada.predict(X_test)\n\nacc_ada=accuracy_score(y_test, y_pred)\nf1_ada=f1_score(y_test, y_pred)\nroc_ada=roc_auc_score(y_test, y_pred)\nprint('Roc_Auc score: %.3f' %roc_ada)    ","d092bcf8":"from imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\n\nprint ('Initial size:', X_train.shape)\n\nsmt=SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\nX_smt, y_smt = smt.fit_resample(X_train, y_train)\n        \nprint ('Resampled size:', X_smt.shape)","19f39610":"rf_smt=RandomForestClassifier().fit(X_smt, y_smt)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (16,6))\n    \nax1.set_title('Confusion matrix (RF and SMOTETomek)')\nax2.set_title('ROC curve (RF and SMOTETomek)')\nax2.plot([0,1], [0,1], 'g--', alpha=0.25)\n    \nplot_confusion_matrix(rf_smt,X_test, y_test, cmap=plt.cm.Blues, normalize='true', ax=ax1)\nplot_roc_curve(rf_smt, X_test, y_test, ax=ax2)\n\ny_pred = rf_smt.predict(X_test)\n\nacc_smt=accuracy_score(y_test, y_pred)\nf1_smt=f1_score(y_test, y_pred)\nroc_smt=roc_auc_score(y_test, y_pred)\nprint('Roc_Auc score: %.3f' %roc_smt)    ","c90b5b66":"#Threshold change not allowed in RF, so we need to detemine the probability of belonging to classes\ny_prob = rf_smt.predict_proba(X_test)\nthreshold=[x for x in np.linspace(0.5, 0.95, 10)]\nroc=[]\nacc=[]\nfor t in threshold:\n    y_t=[0 if x[0]>t else 1 for x in y_prob]\n    roc.append(roc_auc_score(y_test, y_t))\n    acc.append(accuracy_score(y_test, y_t))","54337e56":"plt.figure(figsize=(12,8))\nplt.title('ROC AUC and Accuracy vs. Threshold')\nplt.plot(threshold, roc, label='ROC AUC Score')\nplt.plot(threshold, acc, label='Accuracy Score')\nplt.xlabel(\"Probabability threshold for non-risk class\")\nplt.ylabel(\"Score\")\nplt.legend(loc='lower left')","63023a68":"from sklearn.metrics import confusion_matrix\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize = (16,6))\n\ny_t=[0 if x[0]>0.75 else 1 for x in y_prob]\n\nacc_tr=accuracy_score(y_test, y_t)\nf1_tr=f1_score(y_test, y_t)\nroc_tr=roc_auc_score(y_test, y_t)\n\nax1.set_title('Confusion matrix of the model (RF and SMOTETomek) \\nwith 0.75 probability threshold')\nax2.set_title('Confusion matrix of the standard model (RF and SMOTETomek) \\nwith 0.5 probability threshold')\n\nsns.heatmap(confusion_matrix(y_test, y_t), annot=True, fmt='d',cmap=plt.cm.Blues, ax=ax1)\nsns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d',cmap=plt.cm.Blues, ax=ax2)\nax1.set_xlabel('Predicted Risk Flag')\nax1.set_ylabel('True Risk Flag')\nax2.set_xlabel('Predicted Risk Flag')\nax2.set_ylabel('True Risk Flag')","165a40de":"results=pd.DataFrame.from_dict({'Balanced RF': [roc_brf,acc_brf,f1_brf], 'RF and ADASYN': [roc_ada,acc_ada,f1_ada], \n              'RF and SMOTETomek': [roc_smt,acc_smt,f1_smt], 'RF and SMOTETomek (0.75 Tr.)':[roc_tr,acc_tr,f1_tr]}, \n                       orient='index', columns=['ROC AUC', 'Accuracy', 'F1 score'])\nprint(results)","8dd410da":"## \ud83d\udcca Portion of risky loans by age, marital status and house ownership","6fc75001":"## \ud83d\udcca Ratio of risky loans","dad0f6c2":"## \ud83d\udcca Count of loans by Ages","c5c1d7be":"As seen above, the threshold of 75% allows to detect the majority of risky clients at the cost of almost doubling False-Positive predictions.","95c63a27":" # Random Forest with random undersampling\n A balanced random forest classifier (BalancedRandomForestClassifier) randomly under-samples each boostrap sample to balance it.","7a36208f":"# EDA","6303b65a":"# Intro\n* According to the given task, this notebook is dedicated to the problem of classifying possible defaulters. <br>\n* The following is a brief exploratory data analysis, feature engineering, and methods for classifying unbalanced datasets. <br>\n* As a measure of the quality of the model, roc_auc_score on 20% of train_dataset is accepted.","a93bf82e":"## \ud83d\udcca Proportion of risky loans by state","0f8a575b":"# Feature Engineering","05a7be0f":"## \ud83d\udcca Income vs Experience","074a42b0":"# Models overview\nSince the data is imbalanced, let's try some methods of resampling the dataset such as:\n* Random under-sampling using Balanced Random Forest Classifier;\n* Over-sampling using Adaptive Synthetic (ADASYN) algorithm;\n* Over-sampling using SMOTE and under-sampling with Tomek links (SMOTETomek). \n\nIn all cases, the Random Forest Classifier will be used.\n\nFor more information about handling the imbalanced datasets visit [imbalanced-learn.org](https:\/\/imbalanced-learn.org\/stable\/introduction.html) ","2f44abc6":"# Conclusion\nThe applied methods allow achieving comparable classification results, which can probably be improved by hyperparameters tuning.<br>\nChanging the probability threshold can help to increase some model metrics, but leads to decrease of another metrics.","be881e02":"# Threshold moving\nWhat if we need to identify as accurately as possible all risky loans? In this case, we can change the threshold of belonging to the risky class. This will increase in the number of False-Positive predictions and decrease False-Negatives. <br> \nThe choice of the threshold depends on our strategy: do we want to have a larger number of potential clients or do we need to have minimal risks, even at the cost of losing a part of reliable borrowers. ","c0e3fab4":"# Over-sampling  with ADASYN\nThe essential idea of Adaptive Synthetic (ADASYN) is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. [[1]](https:\/\/ieeexplore.ieee.org\/document\/4633969)","b67adca6":"# Combination of over- and under-sampling using SMOTE and Tomek links\nSMOTE-Tomek Links method combines the SMOTE ability to generate synthetic data for minority class and Tomek Links ability to remove the data that are identified as Tomek links from the majority class (that is, samples of data from the majority class that is closest with the minority class data). [[2]](https:\/\/towardsdatascience.com\/imbalanced-classification-in-python-smote-tomek-links-method-6e48dfe69bbc)","d73f963c":"## \ud83d\udcca Number of loans by state"}}