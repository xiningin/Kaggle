{"cell_type":{"89638390":"code","915335f4":"code","0a9e6891":"code","23f430f8":"code","4dad52fa":"code","cd7093a6":"code","0442b7cc":"code","293aa0e2":"code","a98394f6":"code","89c170cb":"code","c590e5f2":"code","ee23c94a":"code","0feacf85":"code","8c1d52d0":"code","8f1da176":"code","56bdaff9":"code","cc657e7a":"code","8a5941d8":"code","3ae207ba":"code","b495e18f":"code","b3e553ad":"code","ef834bef":"code","7bf178cc":"code","bce330cc":"code","34b63801":"code","87482aba":"code","b1780a90":"code","bdf808e5":"markdown","e04f8f25":"markdown","a8311bd9":"markdown","ff56eca5":"markdown","80c47737":"markdown","0ffcfb17":"markdown","c1d2ca07":"markdown","40c34988":"markdown","e1f0ac1c":"markdown"},"source":{"89638390":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","915335f4":"import pandas as pd\nimport numpy as np\nimport re\nfrom nltk import word_tokenize\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n","0a9e6891":"cols = ['sentiment','id','date','query_string','user','text']\ndf = pd.read_csv('..\/input\/training.1600000.processed.noemoticon.csv', encoding='latin1', names=cols)","23f430f8":"#knowing the data \ndf.info()\ndf['sentiment'].value_counts()\n","4dad52fa":"df.head()\n","cd7093a6":"pat1 = '@[^ ]+'\npat2 = 'http[^ ]+'\npat3 = 'www.[^ ]+'\npat4 = '#[^ ]+'\npat5 = '[0-9]'\n\ncombined_pat = '|'.join((pat1, pat2, pat3, pat4, pat5))\n","0442b7cc":"# Cleaning \n\nclean_tweet_texts = []\nfor t in df['text']:\n    t = t.lower()\n    stripped = re.sub(combined_pat, '', t)\n    tokens = word_tokenize(stripped)\n    words = [x for x  in tokens if len(x) > 1]\n    sentences = \" \".join(words)\n    negations = re.sub(\"n't\", \"not\", sentences)\n    \n    clean_tweet_texts.append(negations)\n","293aa0e2":"clean_df = pd.DataFrame(clean_tweet_texts, columns=['text'])\nclean_df['sentiment'] = df['sentiment'].replace({4:1})\nclean_df.head()\nclean_df.info()","a98394f6":"neg_tweets = clean_df[clean_df['sentiment']==0]\npos_tweets = clean_df[clean_df['sentiment']==1]\n","89c170cb":"# Getting the value count for every word\nneg = neg_tweets.text.str.split(expand=True).stack().value_counts()\npos = pos_tweets.text.str.split(expand=True).stack().value_counts()\n\n# Transforming to lists\nvalues_neg = neg.keys().tolist()\ncounts_neg = neg.tolist()\n\nvalues_pos = pos.keys().tolist()\ncounts_pos = pos.tolist()\n\nplt.bar(values_neg[0:10], counts_neg[0:10])\nplt.title('Top 10 Negative Words')\nplt.show()\n\nplt.bar(values_pos[0:10], counts_pos[0:10])\nplt.title('Top 10 Positive Words')\n\nplt.show()\n\n","c590e5f2":"cv = CountVectorizer(stop_words='english', binary=False, ngram_range=(1,1))\n\nneg_cv = cv.fit_transform(neg_tweets['text'].tolist())\npos_cv = cv.fit_transform(pos_tweets['text'].tolist())\n","ee23c94a":"freqs_neg = zip(cv.get_feature_names(), neg_cv.sum(axis=0).tolist()[0])\nfreqs_pos = zip(cv.get_feature_names(), pos_cv.sum(axis=0).tolist()[0])\n","0feacf85":"list_freq_neg = list(freqs_neg)\nlist_freq_pos = list(freqs_pos)","8c1d52d0":"list_freq_neg.sort(key=lambda tup: tup[1], reverse=True)\nlist_freq_pos.sort(key=lambda tup: tup[1], reverse=True)\n","8f1da176":"cv_words_neg = [i[0] for i in list_freq_neg]\ncv_counts_neg = [i[1] for i in list_freq_neg]\n\ncv_words_pos = [i[0] for i in list_freq_pos]\ncv_counts_pos = [i[1] for i in list_freq_pos]\n","56bdaff9":"plt.bar(cv_words_neg[0:10], cv_counts_neg[0:10])\nplt.xticks(rotation='vertical')\nplt.title('Top Negative Words With CountVectorizer')\nplt.show()\n\nplt.bar(cv_words_pos[0:10], cv_counts_pos[0:10])\nplt.xticks(rotation='vertical')\nplt.title('Top Positive Words With CountVectorizer')\nplt.show()\n\n","cc657e7a":"tv = TfidfVectorizer(stop_words='english', binary=False, ngram_range=(1,3))\n\nneg_tv = tv.fit_transform(neg_tweets['text'].tolist())\npos_tv = tv.fit_transform(pos_tweets['text'].tolist())\n","8a5941d8":"freqs_neg_tv = zip(tv.get_feature_names(), neg_tv.sum(axis=0).tolist()[0])\nfreqs_pos_tv = zip(tv.get_feature_names(), pos_tv.sum(axis=0).tolist()[0])\nlist_freq_neg_tv = list(freqs_neg_tv)\nlist_freq_pos_tv = list(freqs_pos_tv)\n","3ae207ba":"list_freq_neg_tv.sort(key=lambda tup: tup[1], reverse=True)\nlist_freq_pos_tv.sort(key=lambda tup: tup[1], reverse=True)\n\ncv_words_neg_tv = [i[0] for i in list_freq_neg_tv]\ncv_counts_neg_tv = [i[1] for i in list_freq_neg_tv]\n\ncv_words_pos_tv = [i[0] for i in list_freq_pos_tv]\ncv_counts_pos_tv = [i[1] for i in list_freq_pos_tv]\n","b495e18f":"plt.bar(cv_words_neg_tv[0:10], cv_counts_neg_tv[0:10])\nplt.xticks(rotation='vertical')\nplt.title('Top Negative Words With tf-idf')\nplt.show()\n\nplt.bar(cv_words_pos_tv[0:10], cv_counts_pos_tv[0:10])\nplt.xticks(rotation='vertical')\nplt.title('Top Positive Words with tf-idf')\nplt.show()\n\n","b3e553ad":"x = clean_df['text']\ny = clean_df['sentiment']\n","ef834bef":"cv = CountVectorizer(stop_words='english', binary=False, ngram_range=(1,3))\nx_cv = cv.fit_transform(x)\n","7bf178cc":"x_train_cv, x_test_cv, y_train_cv, y_test_cv = train_test_split(x_cv, y, test_size=0.2, random_state=0)\nfrom sklearn.linear_model import LogisticRegression\nlog_cv = LogisticRegression() \nlog_cv.fit(x_train_cv,y_train_cv)\n","bce330cc":"from sklearn.metrics import confusion_matrix\ny_pred_cv = log_cv.predict(x_test_cv)\nprint(confusion_matrix(y_test_cv,y_pred_cv))\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test_cv,y_pred_cv))\n","34b63801":"tv = TfidfVectorizer(stop_words='english', binary=False, ngram_range=(1,3))\nx_tv = tv.fit_transform(x)\nx_train_tv, x_test_tv, y_train_tv, y_test_tv = train_test_split(x_tv, y, test_size=0.2, random_state=0)","87482aba":"log_tv = LogisticRegression() \nlog_tv.fit(x_train_tv,y_train_tv)\n","b1780a90":"y_pred_tv = log_tv.predict(x_test_tv)\nprint(confusion_matrix(y_test_tv,y_pred_tv))\nprint(classification_report(y_test_tv,y_pred_tv))\n","bdf808e5":"## Extracting the clean df and exploring it","e04f8f25":"## Plot The Top 10 Words Before applying any weighting\n","a8311bd9":"## Apply tf-idf vectorizer and then plot top 10 words\n","ff56eca5":"## Apply Logistic Regression With tf-idf\n","80c47737":"# Import And Explore the Data","0ffcfb17":"# Define Patterns to clean the data","c1d2ca07":"## Apply CountVectorizer and then plot top 10 Words \n","40c34988":"## Apply Logistic Regression with CountVectorizer\n","e1f0ac1c":"### Extrcting Negative & Positive tweets"}}