{"cell_type":{"15991b1d":"code","a87d9fac":"code","13d5e45c":"code","f22c0208":"code","bfddee7a":"code","b928808c":"code","264edc10":"code","b2905be0":"code","0afcc155":"code","58da43b1":"code","ad8e909a":"code","057f9959":"code","9d1dc49b":"code","c3c28e19":"code","140972aa":"code","40be6d8d":"code","9cbbd282":"code","447b5b0d":"code","73961070":"markdown","122bd2a3":"markdown","c95af63e":"markdown","820bcfd3":"markdown","104ecf8d":"markdown","f78b970c":"markdown","46f30f8f":"markdown","5bf67240":"markdown","ccf9af80":"markdown","1951515b":"markdown","da99be02":"markdown","93de8c0b":"markdown","f937bffb":"markdown"},"source":{"15991b1d":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport os\nimport csv\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","a87d9fac":"true=pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')\nfake=pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\ntrue.head()","13d5e45c":"true['result']=1\nfake['result']=0","f22c0208":"true.head()","bfddee7a":"df=pd.concat([true,fake])\ndf.tail()","b928808c":"df.isna().sum()","264edc10":"df['text']=df['title']+\"\"+df['text']+\"\"+df['subject']\ndel df['title']\ndel df['date']\ndel df['subject']\ndf.head()","b2905be0":"sentence = df['text'].values.tolist()\nresult= df['result'].values.tolist()\n","0afcc155":"X_train, X_test, Y_train,Y_test= train_test_split(sentence, result, test_size=0.2)\n","58da43b1":"Y_train=np.array(Y_train)\nY_test=np.array(Y_test)","ad8e909a":"tokenizer=Tokenizer(num_words=10000, oov_token='<OOV>')\ntokenizer.fit_on_texts(X_train)\nword_index=tokenizer.word_index\nsequences=tokenizer.texts_to_sequences(X_train)\npadded_train=pad_sequences(sequences,5000,truncating='post')\n\n\n","057f9959":"sequences_test=tokenizer.texts_to_sequences(X_test)\npadded_test=pad_sequences(sequences_test,5000,truncating='post')\n","9d1dc49b":"padded_test.shape","c3c28e19":"Y_test.shape","140972aa":"model= tf.keras.Sequential([\n    tf.keras.layers.Embedding(10000,16,input_length=5000),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1,activation='sigmoid')\n])","40be6d8d":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","9cbbd282":"history=model.fit(padded_train, Y_train, epochs=10, validation_data=(padded_test, Y_test))\n","447b5b0d":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","73961070":"We will categorise the news into true=1 and fake=0","122bd2a3":"We have no null values","c95af63e":"Let us concatenate these two dataframes","820bcfd3":"5) CONVOLUTIONAL NEURAL NETWORK","104ecf8d":"**FAKE AND REAL NEWS **\n\nThis dataset has 17903 uniques news titles and their corresponding text. \nIt has been categorized into two .csv files, one for True and the other one for Fake.\n\nOur aim to to train a model to predict if a particular news is real of fake","f78b970c":"Let us import the two csv files","46f30f8f":"**2) DATA PREPROCESSING**","5bf67240":"**3) TRAIN TEST SPLIT**","ccf9af80":"We have a split size of 0.2, hence 20% of the data will be used for testing.","1951515b":"**1) IMPORT ALL LIBRARIES**","da99be02":"Tokenization: is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph.\n\nPadding: When processing sequence data, it is very common for individual samples to have different lengths. Hence, we pad the sequences to make an array with each row being the vectorized values of each sentence.","93de8c0b":"**4) NLP TECHNIQUES**","f937bffb":"We have a great accuracy with just about 3 epochs!\n\nThis dataset can be found on Kaggle."}}