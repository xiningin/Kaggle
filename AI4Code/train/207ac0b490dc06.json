{"cell_type":{"6642c5bc":"code","3b1b4fc8":"code","1f1dd03b":"code","0e61825b":"code","c03f1681":"code","c7995ed9":"code","72993849":"code","c396c63d":"code","2be8d8e9":"code","b8018cd2":"code","6944984f":"code","334874d5":"code","6492ede5":"code","876168a4":"code","4a5548f4":"code","04669478":"code","ad894976":"code","60b9d13d":"code","489fda27":"code","c9321913":"code","cf2a0ea8":"code","eef86eb8":"code","5e65c30d":"code","19515783":"code","472951c4":"code","22acc5b8":"code","5742a0dc":"code","1441c9c2":"code","d0c78118":"code","1d838f06":"code","dd815c28":"code","04044448":"code","84b7c20f":"code","9ee48842":"code","6e5e1ca5":"code","594f7ebb":"code","9908ab33":"code","5828fcee":"code","bb2a3577":"code","d86505ae":"markdown","9ac8d3ff":"markdown","6d7a93b2":"markdown","c8a0f8c9":"markdown","18d9d264":"markdown","93c9f70e":"markdown","c2731852":"markdown","65e6bd6a":"markdown","b362377c":"markdown","9069a3f4":"markdown","d67eea98":"markdown","4cdbd728":"markdown","9c120928":"markdown","b644f348":"markdown","53b753c9":"markdown","5ee42f43":"markdown","0ef5d473":"markdown","ba290023":"markdown","f65f4803":"markdown","eaf8bec5":"markdown"},"source":{"6642c5bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b1b4fc8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.utils import class_weight\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import KFold\nfrom sklearn import preprocessing\nnp.random.seed(0)\nimport sklearn.model_selection\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nplt.style.use('ggplot')","1f1dd03b":"df = pd.read_csv('\/kaggle\/input\/residential-power-usage-3years-data-timeseries\/power_usage_2016_to_2020.csv')\ndf1 = pd.read_csv('\/kaggle\/input\/residential-power-usage-3years-data-timeseries\/weather_2016_2020_daily.csv')","0e61825b":"df.head()","c03f1681":"df.info()","c7995ed9":"df.isnull().sum()","72993849":"df['StartDate'] = pd.to_datetime(df['StartDate'])","c396c63d":"df['year'] = df['StartDate'].dt.year\ndf['month'] = df['StartDate'].dt.month\ndf['week'] = df['StartDate'].dt.week\ndf['day'] = df['StartDate'].dt.day\ndf['hour'] = df['StartDate'].dt.hour\n#df['minute'] = df['StartDate'].dt.minute\n#df['seconds']= df['StartDate'].dt.second","2be8d8e9":"df.drop('StartDate',axis=1,inplace=True)","b8018cd2":"X=df.drop('notes',axis=1)\ny=df['notes']","6944984f":"X.head(),y.head()","334874d5":"X=df.drop('notes',axis=1)\ny=df['notes']","6492ede5":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\ny_true = pd.Series(le.fit_transform(y))","876168a4":"df['notes'].value_counts(normalize=True)","4a5548f4":"# summarize distribution\ncounter = Counter(y_true)\nfor k,v in counter.items():\n    per = v \/ len(y_true) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n# plot the distribution\nplt.bar(counter.keys(), counter.values())\nplt.show()","04669478":"X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.4, random_state=1,stratify=y)","ad894976":"y_train.value_counts(normalize=True)","60b9d13d":"le = preprocessing.LabelEncoder()\ny_train = pd.Series(le.fit_transform(y_train))\ny_test = pd.Series(le.fit_transform(y_test))","489fda27":"# evaluate a model\ndef evaluate_model(X, y, model):\n\t# define evaluation procedure\n\tcv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n\t# evaluate model\n\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n\treturn scores","c9321913":"# define the reference model\nmodel = RandomForestClassifier()\n# evaluate the model\nscores = evaluate_model(X_train, y_train, model)\n# summarize performance\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","cf2a0ea8":"# Make predictions for the test set\nmodel.fit(X_train, y_train)\ny_pred_test = model.predict(X_test)","eef86eb8":"# View accuracy score\naccuracy_score(y_test, y_pred_test)","5e65c30d":"# View confusion matrix for test data and predictions\nconfusion_matrix(y_test, y_pred_test)","19515783":"# Get and reshape confusion matrix data\nmatrix = confusion_matrix(y_test, y_pred_test)\nmatrix = matrix.astype('float') \/ matrix.sum(axis=1)[:, np.newaxis]\n\n# Build the plot\nplt.figure(figsize=(16,7))\nsns.set(font_scale=1.4)\nsns.heatmap(matrix, annot=True, annot_kws={'size':10},\n            cmap=plt.cm.Greens, linewidths=0.2)\n\n# Add labels to the plot\nclass_names = ['weekday', 'weekend', 'COVID_lockdown', \n               'vacation']\ntick_marks = np.arange(len(class_names))\ntick_marks2 = tick_marks + 0.5\nplt.xticks(tick_marks, class_names, rotation=25)\nplt.yticks(tick_marks2, class_names, rotation=0)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.title('Confusion Matrix for Random Forest Model')\nplt.show()","472951c4":"# View the classification report for test data and predictions\nprint(classification_report(y_test, y_pred_test))","22acc5b8":"from sklearn.utils import class_weight\nclass_weight = class_weight.compute_class_weight('balanced',np.unique(y_train),y_train)","5742a0dc":"class_weight","1441c9c2":"class_list = [0,1,2,3]\nzip_iterator = zip(class_list, class_weight)\na_dictionary = dict(zip_iterator)\nprint(a_dictionary)","d0c78118":"# define the reference model\nmodel = RandomForestClassifier(class_weight = a_dictionary)\n# evaluate the model\nscores = evaluate_model(X_train, y_train, model)\n# summarize performance\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","1d838f06":"# Make predictions for the test set\nmodel.fit(X_train, y_train)\ny_pred_test = model.predict(X_test)","dd815c28":"# View accuracy score\naccuracy_score(y_test, y_pred_test)","04044448":"weights = y_true.value_counts()\/len(y_true)\nclass_list = [0,1,2,3]\nzip_iterator = zip(class_list, weights)\na_dictionary = dict(zip_iterator)\nprint(a_dictionary)","84b7c20f":"# define the reference model\nmodel = RandomForestClassifier(class_weight = a_dictionary)\n# evaluate the model\nscores = evaluate_model(X_train, y_train, model)\n# summarize performance\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","9ee48842":"# Make predictions for the test set\nmodel.fit(X_train, y_train)\ny_pred_test = model.predict(X_test)","6e5e1ca5":"# View accuracy score\naccuracy_score(y_test, y_pred_test)","594f7ebb":"mu=0.15\n# random labels_dict\nlabels_dict = y_true.value_counts().to_dict()\ntotal = sum(labels_dict.values())\nkeys = labels_dict.keys()\nweight = dict()\nweight\n\nfor i in keys:\n    score = np.log(mu*total\/float(labels_dict[i]))\n    weight[i] = score if score > 1 else 1\n    \nweight","9908ab33":"# define the reference model\nmodel = RandomForestClassifier(class_weight = weight)\n# evaluate the model\nscores = evaluate_model(X_train, y_train, model)\n# summarize performance\nprint('Mean Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","5828fcee":"# Make predictions for the test set\nmodel.fit(X_train, y_train)\ny_pred_test = model.predict(X_test)","bb2a3577":"# View accuracy score\naccuracy_score(y_test, y_pred_test)","d86505ae":"### Label Encoding of Target Variable","9ac8d3ff":"### Balancing Data for Classification","6d7a93b2":"We can get class weights using sklearn to compute the class weight. By adding those weight to the minority classes while training the model, can help the performance while classifying the classes.","c8a0f8c9":"This confusion matrix would be a lot easier to read if it had some labels and even a color scale to help us spot the biggest and smallest values","18d9d264":"Accuracy from cross validation and general accuracy arethe same.This model has an accuracy score of 99.95% on the test data. That seems pretty impressive, but remember that accuracy is not a great measure of classifier performance when the classes are imbalanced","93c9f70e":"### Classification report","c2731852":"Dividing the no. of counts of each class with the no. of rows","65e6bd6a":"Now it\u2019s easy to see that our classifier struggled at predicting the weekend label","b362377c":"Random Forest is an ensemble of decision trees. The single decision tree is very sensitive to data variations. It can easily overfit to noise in the data. The Random Forest with only one tree will overfit to data as well because it is the same as a single decision tree. When we add trees to the Random Forest then the tendency to overfitting should decrease (thanks to bagging and random feature selection). However, the generalization error will not go to zero. The variance of generalization error will approach to zero with more trees added but the bias will not! It is a useful feature, which tells us that the more trees in the RF the better","9069a3f4":"This is one of the preferable methods of choosing weights.\nlabels_dict is the dictionary object contains counts of each class.\nThe log function smooths the weights for the imbalanced class.","d67eea98":"### Confusion matrix","4cdbd728":"#### Encoding the labels","9c120928":"### Counts to Length Ratio:","b644f348":"#### Evaluate Model with cross validation","53b753c9":"There are two important things I want to point out in the code above. First is that I set a random_state; this ensures that if I have to rerun my code, I\u2019ll get the exact same train-test split, so my results won\u2019t change\nThe second thing I want to point out is stratify=y. This tells train_test_split to make sure that the training and test datasets contain examples of each class in the same proportions as in the original dataset.","5ee42f43":"### Random Forest Classification","0ef5d473":"### Sklearn Utils","ba290023":"### Smoothen Weights Technique","f65f4803":"The purpose is to avoid SMOTE.\nSMOTE is not very good for high dimensionality data\nOverlapping of classes may happen and can introduce more noise to the data.\nSo, to skip this problem, we can assign weights for the class manually with the \u2018class_weight\u2019 parameter.\nBelow processes do the same","eaf8bec5":"from the above plot it is clear that the data is imbalanced."}}