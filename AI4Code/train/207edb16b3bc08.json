{"cell_type":{"f6be242e":"code","e45f6552":"code","fea8fc1a":"code","f54bea6c":"code","ef34cf71":"code","93dab6a4":"code","153d8f00":"code","584f82ab":"code","8772e2fc":"code","eaa84262":"code","f2c7130a":"code","fd3f3c05":"code","c9907024":"code","21b78a96":"code","378fd321":"code","717ed2c8":"code","c96edb68":"code","3544ea4c":"markdown","0a015cb5":"markdown","df5d73ec":"markdown","2253d3d7":"markdown","a7f9dfd8":"markdown","309d3ede":"markdown","6b202862":"markdown","b6acc5b4":"markdown","0a118b7f":"markdown"},"source":{"f6be242e":"# Import the necessary modules\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split","e45f6552":"# Load the CSV file containing the experience vs salary data into a pandas dataframe\ndf = pd.read_csv(\"..\/input\/sample-salary-data-for-simple-linear-regression\/Salary_Data.csv\")\n\n# Inspect the first 10 rows\ndf.head(10)","fea8fc1a":"# Just a small adjustment in the column names\ndf.rename(columns = {\"YearsExperience\" : \"years\", \"Salary\" : \"salary\"}, inplace = True)\n\n# Inspect the data again\ndf.head()","f54bea6c":"# Review some of the important statistics of the data\ndf.describe()","ef34cf71":"# Check the correlation coefficient between years and salary\ndf.corr()","93dab6a4":"# Let's display a regression plot to visualise our data\nplt.figure(figsize = (12, 6))\nsns.regplot(data = df, x = \"years\", y = \"salary\", color = \"red\")\n\n# Label the plot\nplt.title(\"Regression Plot: Years of Experience vs Salary\")\nplt.xlabel(\"Years of Experience\")\nplt.ylabel(\"Salary\")\nplt.show()","153d8f00":"# Set years as the feature set (X) and salary as the target set (y)\nX, y = np.array(df[[\"years\"]]), np.array(df[[\"salary\"]])\n# X is being assigned a 2D array (by using double square brackets) because the fit method I'm going to use later to train\n# the model expects the feature matrix to be 2D.\n# The same is optional for the target matrix in case it is a single variable, but I'm doing this for making sure that\n# I don't forget it when I'll be dealing with multiple targets\n\n# Check the shapes of the above\nprint(X.shape, y.shape)","584f82ab":"# I'll be using train-test split on the above data for obtaining the training data and the testing data\ntrainX, testX, trainy, testy = train_test_split(X, y, test_size = 1\/4)\n\n# Review the splitted data\nprint(trainX, trainy)\nprint(testX, testy)","8772e2fc":"# Instantiate the linear regression object\nregr = LinearRegression()","eaa84262":"# Fit (train) the model with our training data\nregr.fit(trainX, trainy)\n\n# Print the coefficients matrix and the intercepts vector of the regression line\nprint(regr.coef_, regr.intercept_)","f2c7130a":"# Predict the values for testX\n# I'll assign the array of predicted values to a new variable testyCap\ntestyCap = regr.predict(testX)\n\n# View the array\ntestyCap","fd3f3c05":"# Let's compare the true values versus the predicted ones for the test set\n# For this purpose, let's create a dataframe\ndfComparison = pd.DataFrame(dict(X = testX[:, 0], trueValues = testy[:, 0], predicted = testyCap[:, 0]))\ndfComparison","c9907024":"# Now I'll evaluate the model using various statistical methods\n\n# Mean Absolute Error\nmae = np.mean(np.absolute(testyCap - testy))\n# Mean Squared Error\nmse = np.mean((testyCap - testy) ** 2)\n# Root Mean Squared Error\nrmse = np.sqrt(mse)\n# Relative Absolute Error\nrae = np.sum(np.absolute(testyCap - testy)) \/ np.sum(np.absolute(testy - np.mean(testy)))\n# Relative Squared Error\nrse = np.sum((testyCap - testy) ** 2) \/ np.sum((testy - np.mean(testy)) ** 2)\n# R2 Score (calculated)\nr2 = 1 - rse\n# R2 Score (using method)\\\nr2M = r2_score(testy, testyCap)\n# Variance score (equal to r2)\nvarScore = regr.score(testX, testy)","21b78a96":"# Print the stats\nprint(\"\"\"\/\nMAE: %f\nMSE: %f\nRMSE: %f\nRAE: %f\nRSE: %f\nR2 Score (calculated): %f\nR2 Score (using method): %f\nVariance Score: %f\n\"\"\"%(mae, mse, rmse, rae, rse, r2, r2M, varScore))","378fd321":"# Let's visualise the trainX, trainy and testX, testy separately\nfig, axes = plt.subplots(2, 1, figsize = (10, 10))\nax1, ax2 = axes\nfig.suptitle(\"Regression Plots for Two Different Sets\")\n\n# For training set\nsns.regplot(trainX, trainy, color = \"blue\", ax = ax1)\nax1.set_title(\"Training Set\")\nax1.set_xlabel(\"trainX\")\nax1.set_ylabel(\"trainy\")\n\n# For test set\nsns.regplot(testX, testy, color = \"red\", ax = ax2)\nax2.set_title(\"Test Set\")\nax2.set_xlabel(\"testX\")\nax2.set_ylabel(\"testy\")\n\nplt.show()","717ed2c8":"# Let's predict our model for some sample data\nsampleX = np.array([1, 2, 1.7, 3, 2.5, 0.2, 0, 9.1, 5.6, 6.3, 4.9])\nsampley = regr.coef_[0, 0] * sampleX + regr.intercept_[0]\n\n# Feed it into a dataframe\ndfSample = pd.DataFrame({\"X\" : sampleX, \"y\" : sampley})\n# View it\ndfSample","c96edb68":"# Visualising just for fun\ndfSample.plot(kind = \"scatter\", x = \"X\", y = \"y\", color = \"blue\", figsize = (10, 8), marker = \"+\")\nplt.plot(sampleX, sampley, color = \"red\", alpha = 0.4)\nplt.show()","3544ea4c":"<h3>Model Training<\/h3>","0a015cb5":"<h1>Salary Prediction from Experience<\/h1>\n\nby <b>Santanu Sikder<\/b>","df5d73ec":"From the above statistics including the correlation coefficient (almost equal to 1), it is clear that **years** and **salary** are very well related (positively) and suitable for linear regression.","2253d3d7":"<h3>Model Evaluation<\/h3>","a7f9dfd8":"<h3>Processing Data for Model Training<\/h3>","309d3ede":"So the above visualisation further validates our decision for using linear regression.","6b202862":"<h3>Testing our model<\/h3>","b6acc5b4":"<h3>Data Analysis and Visualisation<\/h3>","0a118b7f":"<h3>Data Loading<\/h3>"}}