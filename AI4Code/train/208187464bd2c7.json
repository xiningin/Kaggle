{"cell_type":{"4945db33":"code","0fb6be2a":"code","4e457705":"code","24957470":"code","e32d133f":"code","3303d6d8":"code","a9d0bd83":"markdown","28a0fefb":"markdown","6d3ab432":"markdown","0c10d5d6":"markdown","61ecff9a":"markdown"},"source":{"4945db33":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport fasttext\nimport bz2\nimport csv\nfrom sklearn.metrics import roc_auc_score\nimport os\nprint(os.listdir(\"..\/input\"))","0fb6be2a":"# Load the training data \ndata = bz2.BZ2File(\"..\/input\/train.ft.txt.bz2\")\ndata = data.readlines()\ndata = [x.decode('utf-8') for x in data]\nprint(len(data)) ","4e457705":"# 3.6mil rows! Lets inspect a few records to see the format and get a feel for the data\ndata[1:5]","24957470":"# Data Prep\ndata = pd.DataFrame(data)\ndata.to_csv(\"train.txt\", index=False, sep=' ', header=False, quoting=csv.QUOTE_NONE, quotechar=\"\", escapechar=\" \")\n\n# Modelling\n# This routine takes about 5 to 10 minutes \nmodel = fasttext.train_supervised('train.txt',label_prefix='__label__', thread=4, epoch = 10)\nprint(model.labels, 'are the labels or targets the model is predicting')","e32d133f":"# Load the test data \ntest = bz2.BZ2File(\"..\/input\/test.ft.txt.bz2\")\ntest = test.readlines()\ntest = [x.decode('utf-8') for x in test]\nprint(len(test), 'number of records in the test set') \n\n# To run the predict function, we need to remove the __label__1 and __label__2 from the testset.  \nnew = [w.replace('__label__2 ', '') for w in test]\nnew = [w.replace('__label__1 ', '') for w in new]\nnew = [w.replace('\\n', '') for w in new]\n\n# Use the predict function \npred = model.predict(new)\n\n# check the first record outputs\nprint(pred[0][0], 'is the predicted label')\nprint(pred[0][1], 'is the probability score')","3303d6d8":"# Lets recode the actual targets to 1's and 0's from both the test set and the actual predictions  \nlabels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test]\npred_labels = [0 if x == ['__label__1'] else 1 for x in pred[0]]\n\n# run the accuracy measure. \nprint(roc_auc_score(labels, pred_labels))","a9d0bd83":"# Data prep and modelling\nA slight inconvenience with the FastText model is the need to save the dataset into a text file. And the annoying encoding of the \"____label__ ____#__\". Basically, the target and the text is all in the same cell. They are distinguished by the prefix of '____label__ ____#__'. Lets say if have 2 labels and one is 'Ham' and the other 'Spam', then your labels would be '____label__ ____Ham__' and '____label__ ____Spam__'. You can include as many labels as well, not just 2.   \n\nThankfully, this dataset has already been formated in that way as you can see from the first 5 records I printed out. We just need to write it out to disk. ","28a0fefb":"## 91.7%\n91.7% absolute accuracy score with only just a few lines of code. Running the evaluation metric using the Probability score would yeild even higher scores but I wanted to keep it inline with the rest of the kernels so its a fair comparison. The most popular Kernel here is the CuDNNLSTM which yielded 93.7%\n\nPerhaps the most challenging bit about using FastText is just the slightly annoying data preparation step to encode the '__labels__'. Just like any data science projects, data prep is the hard yard. Otherwise, rest is pretty straight foward. I'll post another kernel on a different dataset in future and run through the processing steps to get the dataset into the correct format. And some other model tuning process.\n\nIf you like this kernel please give me an upvote! ","6d3ab432":"\n# FastText supervised model 91.7%\n## One of the fastest and most accessible text classifier to anyone, without GPU\nFastText is well known for its distributed representation, which ultimately gets used as an embedding layer in a typical Deep Learning model such as a CNN or an LSTM. However, many don't know that FastText is also a supervised model. To prove the point, this Amazon dataset has been created to support the FastText format. And yet, 6 months later, no one has even tried to post a kernel for using FastText supervised model. What many also don't know is that, it is in fact a pretty good supervised model. Probably one of the fastest and the best out there without using a GPU. I'll cut straight to the chase and demonstrate how this is done. For a full writeup that's about to come soon, check out my blog post here:\n<br\/> https:\/\/mungingdata.wordpress.com\/\n\nAlso, a very accessible paper that introduces the viability of the FastText supervised model from the original authors [here](https:\/\/arxiv.org\/pdf\/1607.01759.pdf)\n\nOk so, lets begin. Its going to be fast trust me","0c10d5d6":"# Apply predictions\nOk after about 10 minutes or so, the model is finished. Now lets apply the predictions to the test dataset. Thankfully, we don't have to write out a physical text file to do the prediction. You could if you want to, but I'm just going to use the data object","61ecff9a":"# Evaluation \nOk so we have our predictions, now lets measure how well we have done? "}}