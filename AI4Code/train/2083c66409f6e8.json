{"cell_type":{"3c66ee0a":"code","dc4a0720":"code","eec28059":"code","64f5cb40":"code","76203c6b":"code","b1e80983":"code","094bdb6a":"code","510dfc79":"code","12812702":"code","a4413121":"code","5ac12a35":"code","f439662d":"code","b9364fdc":"code","0940f4a3":"code","2a0ae62a":"markdown","4d6b4735":"markdown","cc00033d":"markdown","713cb3f0":"markdown","26957d16":"markdown","325e1e0e":"markdown","f418b889":"markdown","94f63411":"markdown","196aee41":"markdown","1d344c69":"markdown","b920b1a6":"markdown","aac25580":"markdown","7132e7bb":"markdown","a468f24d":"markdown","916576d1":"markdown","f998a7ba":"markdown","8c6e5ec0":"markdown","419ad950":"markdown","6f165736":"markdown","fee4a185":"markdown","49920cc1":"markdown","38e5ffaa":"markdown","eebe6253":"markdown","525a9185":"markdown","07d20f0b":"markdown","d444add3":"markdown","e07fb1b2":"markdown","58bb24a6":"markdown","85034f89":"markdown","1d4dfa70":"markdown","7dcd2eb5":"markdown","520a0766":"markdown","19330d60":"markdown","74e38178":"markdown"},"source":{"3c66ee0a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc4a0720":"df = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')\ndf.head()","eec28059":"p = df.hist(figsize=(20,20))","64f5cb40":"from sklearn.preprocessing import StandardScaler\n\ndf['amount_scaled'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1,1))\ndf['time_scaled'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1,1))\ndf.drop(['Time','Amount'], axis=1, inplace=True)","76203c6b":"print('Number of fraud cases: {}, with percentage: {:.2f}%'.format(len(df[df.Class==1]), (len(df[df.Class==1])*100\/len(df))))\nprint('Number of normal cases: {}, with percentage: {:.2f}%'.format(len(df[df.Class==0]), (len(df[df.Class==0])*100\/len(df))))","b1e80983":"\nmissing_val_count_by_column = (df.isnull().sum())\nnum_cols_with_missing = len(missing_val_count_by_column[missing_val_count_by_column > 0])\nprint('Number of columns with missing values: ', num_cols_with_missing)","094bdb6a":"from sklearn.model_selection import train_test_split\nimport random\n\nseed = 1\nrandom.seed(seed)\nnp.random.seed(seed)\nrandom_state=np.random.RandomState(seed)\n\nX = df.drop('Class', axis=1)\ny = df['Class']\nX = X.values\ny = y.values\n\nidx_norm = y == 0\nidx_out = y == 1\n        \nX_train_norm, X_test_norm, y_train_norm, y_test_norm = train_test_split(X[idx_norm], y[idx_norm], test_size=0.4, random_state=random_state)\nX_train_out, X_test_out, y_train_out, y_test_out = train_test_split(X[idx_out], y[idx_out], test_size=0.4, random_state=random_state)\nX_train = np.concatenate((X_train_norm, X_train_out))\ny_train = np.concatenate((y_train_norm, y_train_out))\n\nX_test = np.concatenate((X_test_norm, X_test_out))\ny_test = np.concatenate((y_test_norm, y_test_out))\n","510dfc79":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\n\nlog_reg = LogisticRegression(penalty='l1', C=10, solver='liblinear')\nlog_reg.fit(X_train, y_train)\n\nlog_reg_pred = log_reg.predict(X_test)\n\nfpr, tpr, thresold = roc_curve(y_test, log_reg_pred)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(9,7))\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (AUC = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve for imbalanced logistic regression')\nplt.legend(loc=\"lower right\")","12812702":"from numpy.random import permutation\n\n#now we do downsampling of the normal calss data\nnp.random.RandomState(seed)\nperm = permutation(len(X_train_norm))\nX_train_norm = X_train_norm[perm]\ny_train_norm = y_train_norm[perm]\nX_train_norm_downsmp = X_train_norm[:len(X_train_out)]\ny_train_norm_downsmp = y_train_norm[:len(X_train_out)]\n\n\nX_train_downsmp = np.concatenate((X_train_norm_downsmp, X_train_out))\ny_train_downsmp = np.concatenate((y_train_norm_downsmp, y_train_out))\n\n#log_reg = LogisticRegression(penalty='l1', C=10, solver='liblinear')\nlog_reg.fit(X_train_downsmp, y_train_downsmp)\n\nlog_reg_pred = log_reg.predict(X_test)\n\nfpr, tpr, thresold = roc_curve(y_test, log_reg_pred)\nroc_auc = auc(fpr, tpr)\n\nplt.figure(figsize=(9,7))\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (AUC = %0.4f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve for balanced logistic regression')\nplt.legend(loc=\"lower right\")","a4413121":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.utils.data import TensorDataset\n\ninput_dim = X_train.shape[1]\n\nclass AutoEncoder(nn.Module):\n    \n    def __init__(self):\n        super(AutoEncoder, self).__init__()\n        #encoder\n        self.enc_layer1 = nn.Linear(input_dim,15)\n        self.enc_layer2 = nn.Linear(15,10)\n        #Decoder\n        self.dec_layer1 = nn.Linear(10,15)\n        self.dec_layer2 = nn.Linear(15,input_dim)\n        \n        \n    def forward(self,x):\n        x = F.relu(self.enc_layer1(x))\n        x = F.relu(self.enc_layer2(x))\n        x = F.relu(self.dec_layer1(x))\n        x = F.relu(self.dec_layer2(x))\n        \n        return x\n\nae = AutoEncoder()\nprint(ae)","5ac12a35":"X_train_torch = torch.from_numpy(X_train).type(torch.FloatTensor) #note that we used X_train which contains data from both classes\ny_train_torch = torch.from_numpy(y_train)\n\nX_test_torch = torch.from_numpy(X_test).type(torch.FloatTensor)\ny_test_torch = torch.from_numpy(y_test)\n\ntrain = TensorDataset(X_train_torch,y_train_torch)\ntest = TensorDataset(X_test_torch,y_test_torch)\n\ntrain_dataloader = torch.utils.data.DataLoader(train,batch_size=100,shuffle=True, num_workers=3)\ntest_dataloader = torch.utils.data.DataLoader(test,batch_size=50,shuffle=True, num_workers=3)","f439662d":"torch.manual_seed(seed)\n\nloss_func = nn.MSELoss()\noptimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n\nepochs = 10\n\n#begin training\nfor epoch in range(epochs):\n    for batch_idx, (data,target) in enumerate(train_dataloader):\n        data = torch.autograd.Variable(data)\n        optimizer.zero_grad()\n        pred = ae(data)\n        loss = loss_func(pred, data)\n        loss.backward()\n        optimizer.step()","b9364fdc":"ae.eval()\npredictions = []\nfor batch_idx, (data,target) in enumerate(test_dataloader):\n        data = torch.autograd.Variable(data)\n        pred = ae(data)\n        for prediction in pred:\n            predictions.append(prediction.detach().numpy())\n            \nmse = np.mean(np.power(X_test - predictions, 2), axis=1)\n\nfpr_ae, tpr_ae, thresold = roc_curve(y_test, mse)\nroc_auc_ae = auc(fpr_ae, tpr_ae)\n\nplt.figure(figsize=(9,7))\nlw = 2\nplt.plot(fpr_ae, tpr_ae, lw=lw, label='Autoencoder (AUC = %0.4f)' % roc_auc_ae)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve for autoencoder')\nplt.legend(loc=\"lower right\")\nplt.show()","0940f4a3":"X_train_norm_torch = torch.from_numpy(X_train_norm).type(torch.FloatTensor)\ny_train_norm_torch = torch.from_numpy(y_train_norm)\n\nX_test_torch = torch.from_numpy(X_test).type(torch.FloatTensor)\ny_test_torch = torch.from_numpy(y_test)\n\ntrain = TensorDataset(X_train_norm_torch,y_train_norm_torch)\ntest = TensorDataset(X_test_torch,y_test_torch)\n\ntrain_dataloader = torch.utils.data.DataLoader(train,batch_size=100,shuffle=True, num_workers=3)\ntest_dataloader = torch.utils.data.DataLoader(test,batch_size=50,shuffle=True, num_workers=3)\n\ntorch.manual_seed(seed)\n\nae = AutoEncoder()\n\nloss_func = nn.MSELoss()\noptimizer = torch.optim.Adam(ae.parameters(), lr=1e-3)\n\nepochs = 10\nfor epoch in range(epochs):\n    \n    for batch_idx, (data,target) in enumerate(train_dataloader):\n        data = torch.autograd.Variable(data)\n        optimizer.zero_grad()\n        pred = ae(data)\n        loss = loss_func(pred, data)\n        loss.backward()\n        optimizer.step()\n        \n        \nae.eval()\npredictions = []\nfor batch_idx, (data,target) in enumerate(test_dataloader):\n        data = torch.autograd.Variable(data)\n        pred = ae(data)\n        for prediction in pred:\n            predictions.append(prediction.detach().numpy())\n            \nmse = np.mean(np.power(X_test - predictions, 2), axis=1)\nfpr_ae, tpr_ae, thresold = roc_curve(y_test, mse)\nroc_auc_ae = auc(fpr_ae, tpr_ae)\n\nplt.figure(figsize=(9,7))\nlw = 2\nplt.plot(fpr_ae, tpr_ae, lw=lw, label='Autoencoder (AUC = %0.4f)' % roc_auc_ae)\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC curve for autoencoder')\nplt.legend(loc=\"lower right\")\nplt.show()","2a0ae62a":"Next, we evaluate a method that belong to the category of one-class classification. The method is called **Deep SVDD**. It was introduced by [Ruff et al. 2018](http:\/\/proceedings.mlr.press\/v80\/ruff18a\/ruff18a.pdf). The main idea of Deep SVDD is to transform the input points through a multilayer neural network into an enclosing hypersphere with minimal radius. Assuming the normal examples are similar, this projection of the input points extracts the common factors of variation in the normal examples, which leads to the anomalous points being projected away from the center of the sphere since they don't share the common factors of variation as the normal points. The training objective of Deep SVDD is \n\n$$\\min_W \\frac{1}{n} \\sum_{i=1}^{n} \\left \\| \\phi(x_i;W) -c \\right \\|^2,$$\n\nwhere $\\phi(x_i;W)$ is the neural network function (parameterized by $W$) applied to example $x_i$, and $c$ is the center of the hypersphere. There is an additional regularization term in the objective but I omitted it here for simplicity.\n\nTo evaluate the anomalousness of a test example $x$, Deep SVDD uses the distance from $\\phi(x;W)$ and the center of the hypersphere as an anomaly score as follows:\n\n$$s(x) = \\left \\| \\phi(x;W) -c \\right \\| .$$","4d6b4735":"# Dataset Analysis","cc00033d":"Now we build our unbalanced logistic regression model.","713cb3f0":"![roc_oc_clean.png](attachment:roc_oc_clean.png)","26957d16":"Next, let's see more closely how the class label is distributed.","325e1e0e":"In this category, we will experiment with a logistic regression classifer. There are different ways for handling the imbalance in training data in supervised learning. One common and simple technique is resampling (downsampling, oversampling). In this notebook, I try downsampling. Maybe in later versions I will test oversampling as well. Another possible technique is giving higher weights for prediction errors in the anomalous class during training.\n\nTo see the effect of imbalanced data on the performance of the classifier, we will build our first model with the whole dataset as it is, and see how much it improves after balancing the data.\n\nBefore building our models, we prepare the data for usage and take out a proportion of the data for testing purposes. We will not touch the test data during the training nor use it for model selection, it will only be used for reporting the performance of the final selected model on unseen data.","f418b889":"That's a very good AUC measure given that the anomalous data were included in the training data for the autoencoder. Out of criousity, let's see how much it will improve if we used a clean training data consisting only of normal examples.","94f63411":"We see that the features 'V1' to 'V28'  are normalized and all have mean 0. The features 'Time' and 'Amount' though are not normalized, so we normalize them manually. ","196aee41":"![roc.png](attachment:roc.png)","1d344c69":"Before training let's prepare the dataset in pytorch format.","b920b1a6":"The AUC is almost the same. This means that the autoencoder is very robust to the presence of anomalous data and can be trained efficiently with noisy unlabeled data.\n\nOptimizing for the autoencoder hyperparameters will possibly improve the above results. Also, there are other variants of the autoencoder which are used in anomaly detection that I might try in the future. Examples are [variational autoencoders](https:\/\/www.semanticscholar.org\/paper\/Variational-Autoencoder-based-Anomaly-Detection-An-Cho\/061146b1d7938d7a8dae70e3531a00fceb3c78e8), and [autoencoder ensembles](https:\/\/saketsathe.net\/downloads\/autoencode.pdf).","aac25580":"The AUC is less than that of the autoencoder. Let's see how Deep SVDD performs when trained on clean data (normal examples only).","7132e7bb":"The AUC is improved significantly when trained on normal data alone. It seems that Deep SVDD is trying hard to fit all the training examples into the enclosing hypersphere. Or maybe if I tried to optimize the hyperparameters I would get better results than those. That could be a possible future work as well.","a468f24d":"Note that for the logistic regression model I only plotted the one trained on downsampled data, and for autoencoder and Deep SVDD I only plot the models trained on normal class alone (just to simplify the plot, otherwise it would be a mess).\n\nOne last word before you leave. Remember that these numbers and this ranking of the methods do not mean that this is necessarily the real ranking of these methods in general. This is only the performance of these methods on this dataset. And these numbers could even go higher if we did hyperparameter optimization for each method. But overall, we got a rough idea of how these methods perform and how they tackle the anomaly detection problem.","916576d1":"The method we are going to explore in this category is called **Deep SAD** (short for Deep Semi-supervised Anomaly Detection). It was proposed by [Ruff et al. 2019](https:\/\/arxiv.org\/pdf\/1906.02694.pdf). It is an extension of the previously discussed Deep SVDD where it incorporates the anomalous examples in the training such that the anomalous examples are projected far away from the center of the hypersphere. Assume we have $n$ unlabeled examples and $m$ labeled examples with the labels being -1 for anomalous and +1 for normal. The objective of Deep SAD is as follows:\n\n$$\\min_W \\frac{1}{n+m} \\sum_{i=1}^{n} \\left \\| \\phi(x_i;W) -c \\right \\|^2 + \\frac{\\eta}{n+m} \\sum_{j=1}^{m}\\left ( \\left \\| \\phi(\\tilde{x}_j;W) -c \\right \\|^2  \\right )^{\\tilde{y}_j},$$\n\nwhere $\\eta$ is a hyperparameter that controls the balance between the labeled and unlabeled data. Note that the first term in the above objective resembles that of the deep one-class objective and is used for the unlabeled data. The second term is used for the labeled data, where we observe that, when $\\tilde{y}_j=1$ (i.e. normal example) the optimizer minimizes the distance between the projected example and the center of the hypersphere, on the other hand, when  $\\tilde{y}_j=-1$ (i.e. anomalous example) the optimizer maximizes the distance between the projected example and the center of the hypersphere. As a result, we get a better separation between the normal and anomalous examples in the latent projection space.\n\nThe source code of Deep SAD is publicly available [here](https:\/\/github.com\/lukasruff\/Deep-SAD-PyTorch). As in the previous section, I present here the results immediately to save you the training time. Most of the hyperparameters are left as it is in the original code. Now let's have a look at the performance of Deep SAD.","f998a7ba":"In this notebook, we tested four main methods, logistic regression, autoencoders, deep one-class classification, and semi-supervised deep one-class classification. We saw how the unbalanced data can affect the model performance, and we saw different techniques for handling and working around this problem. Below is a graph that combines the ROC curves of all 4 methods in one plot. ","8c6e5ec0":"Clearly, the dataset is highly imbalanced, which is something to be expected in this kind of problems. The way we handle this imbalance will be different in each method we use. We will come to that later.\n\nLastly, let's check if the dataset has any missing values before we use it for training.","419ad950":"# Fraud Detection - Comparison of Different Techniques","6f165736":"Now let's evaluate the model on the test set.","fee4a185":"Well, as you can see, Deep SAD has much better performance than all previous methods. It has a near perfect AUC of 0.9881. In my opinion, the basic idea of Deep SAD is the most elegant of all the previously explored methods.","49920cc1":"# Unsupervised Learning","38e5ffaa":"#  Supervised Learning ","eebe6253":"The source code of Deep SVDD is publicly available [here](https:\/\/github.com\/lukasruff\/Deep-SVDD). To save you the long training time I present here the results that I obtained. Most hyperparameter choices are left to their default values in the original code. \n\nBelow I show the ROC plot for Deep SVDD trained on mixed data (normal + anomalous).","525a9185":"In this notebook, I explore and experiment with different methods for anomaly detection. I plan to explore at least one method in each learning paradigm, namely, supervised, unsupervised, and semi-supervised learning. In supervised learning I will test a simple linear classifier (**logistic regression**) with the traditional technique of downsampling to handle data imbalance. In unsupervised learning, I will test **autoencoders**, and **deep one-class classification**. In semi-supervised learning, I will test a recent method called **Deep Semi-supervised Anomaly Detection** (Deep SAD) which is an extension of deep one-class classification. More details about these methods will follow. \n\nNote that this notebook is not meant to be an extensive research of all state-of-the-art methods in anomaly detection, nor it is meant to give a statement about the best method for fraud detection problem. My aim is to explore some different techniques to gain understanding of these methods and of the fraud detection problem in general, and also to have fun.","07d20f0b":"![roc.png](attachment:roc.png)","d444add3":"The dataset is a collection of credit card transactions for some European cardholders, where every transaction is given a label to indicate whether it was a normal or fraud transaction. The data is provided on Kaggle by the Machine Learning Group of ULB (Universit\u00e9 Libre de Bruxelles). \n\nFirst, we load the dataset and do basic analysis to understand the data we are dealing with.","e07fb1b2":"We see that, to get high recall (i.e. to detect all frauds) we will have to suffer from high false positive rate (i.e. innocent and authentic cardholders being identified as frauds). \nLet's see if balancing the data by downsampling will enhance the performance of our model. We will randomly select a subsample of the non-fraud examples so that we have equivalent examples for each class.","58bb24a6":"# Semi-Supervised Learning","85034f89":"That's great! downsampling introduced a significant improvement in the performance of the model. We get now an AUC=0.9201 as compared to AUC=0.8146 with the imbalanced data. I would say that's a very good performance for such a simple linear classifier (we will see later if introducing nonlinearity would help). In future versions of this notebook I will use **cross validation** and **grid search** for selecting the best model.","1d4dfa70":"![roc_oc_mixed.png](attachment:roc_oc_mixed.png)","7dcd2eb5":"# Summary","520a0766":"We know from the dataset providers that the features 'V1' to 'V28' are the principal components obtained after applying PCA to the original data. The 'Time' feature represents the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount (probably in Euros). The feature 'Class' is the label of the transaction with '1' for fraud and '0' for normal.\n\nNow let's have a look at all features to see how they are distributed.","19330d60":"Good news, we don't have any missing values. So, let's begin modeling...","74e38178":"In this category, I will explore two methods, The first is autoencoders, and the second is deep one-class classification. Some people categorize these methods as semi-supervised since we usually use the data examples belonging to the normal class (hence we need labeled normal data). However, the main premise that these methods are built upon, is that they can work with unlabeled data even if this data is polluted with some anomalous examples since it is assumed that the majority of the data are normal and the anomalous examples are few (which is usually true). Therefore, these methods should extract the common factors of variation in the normal class and should not be greatly affected by the presence of some noisy outliers. We will put this assumption to test now and train these models with all the training set (normal + anomalous) and see how they perform.\n\nWe begin with autoencoders. The main idea, in brief, is that the autoencoder tries to learn to reconstruct the input examples as faithful as possible, and in doing so it will learn a latent representation of the data that captures the repeated patterns in this data. After training, we present a test example to the autoencoder which tries to reconstruct it. The reconstruction error (distance between the reconstructed and original example) is computed. The anomalous test example should have high reconstruction error as compared to normal examples since it should have different latent representation. \n\nWe will build a simple autoencoder consisting of two-layers encoder and two-layers decoder.\n\n"}}