{"cell_type":{"c91dafa3":"code","cda2af70":"code","3a2fe320":"code","563bab4d":"code","74ba904c":"code","2a8a9542":"code","640250ba":"code","d79bc62c":"code","1ab92868":"code","fe76b822":"code","ca34ab67":"code","76ceac13":"markdown","b8a73219":"markdown","ef49ecc6":"markdown","18e445c7":"markdown","8289667c":"markdown","8292f627":"markdown","9c74602d":"markdown","305c279f":"markdown"},"source":{"c91dafa3":"from fastai.vision.all import *\nimport pandas as pd\nfrom PIL import Image, ImageEnhance\nfrom pathlib import Path","cda2af70":"cwd = Path('..\/input\/medical-mnist')\nfnames = get_image_files(cwd)\nfnames[0]","3a2fe320":"# Get the label from the parent of each image filename\ndf = pd.DataFrame()\nfor fname in fnames:\n    df = df.append({'fname':fname, 'label': fname.parent.name}, ignore_index=True)\ndf.head()","563bab4d":"# This leads to an imbalance in the data, so let's just take a sample\ndf.label = df.label.replace({'AbdomenCT':'BodyCT', 'ChestCT':'BodyCT'})\ndf.label.value_counts()","74ba904c":"# Taking a sample of the data also allows for faster prototyping\n\n# Get a list of the labels\nlabels = df.label.unique()\n\n# Size of our samples\nsample_size = 1000\n\n# New dataframe\nsampled_df = pd.DataFrame()\nfor label in labels:\n    sampled_df = sampled_df.append(df[df.label==label].sample(sample_size))\n\n# Replace our dataframe with the sampled version\ndf = sampled_df\n\n#Let's look at the label distribution\ndf.label.value_counts()","2a8a9542":"#batch size\nbs = 256\n\ndls = ImageDataLoaders.from_df(df, path='', bs=bs, batch_tfms=[*aug_transforms(),Normalize.from_stats(*imagenet_stats)])\ndls.vocab","640250ba":"dls.show_batch(nrows=1, ncols=3)","d79bc62c":"# We'll use squeezenet as the base model since it's very small. Version 1_1 is somewhat faster and more efficient than the original squeezenet\nlearn = cnn_learner(dls, squeezenet1_1, metrics=accuracy)","1ab92868":"# Choose the number of epochs\nepochs = 4\nlearn.fine_tune(epochs)","fe76b822":"# Use the ClassificationInterpretation class to get the top_losses\n\ninterp = ClassificationInterpretation.from_learner(learn)\nlosses,idxs = interp.top_losses()\nlen(dls.valid_ds)==len(losses)==len(idxs)\ninterp.plot_confusion_matrix(figsize=(7,7))","ca34ab67":"# View top losses\ninterp.plot_top_losses(k=6, figsize=(10,10))","76ceac13":"### Create dataframe from filenames","b8a73219":"### The fast.ai library is used to load the data and train the model. \n- Abdomen and Chest CT categories are combined into one new category\n- Only a sample of the data is used for prototyping\n- Squeezenet is used as the basemodel","ef49ecc6":"### Let's look at the predictions\n\n","18e445c7":"### Create learner","8289667c":"### Create Image Data Loader","8292f627":"### Train the model","9c74602d":"### Domain specific knowledge\n- Abdomen CT scans contain some slices from the lower lung\n- Chest CT scans contain some slices from the upper abdomen\n- Because of this , there are slices of the lung labeled Abdomen CT and vice versa\n- Simple solution - combine into one category called Body CT","305c279f":"### Use a fast.ai function to get the image files"}}