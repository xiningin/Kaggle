{"cell_type":{"664c4db9":"code","19263945":"code","9f205c5b":"code","b678a3fb":"code","0bd4c7ff":"code","8cb6353e":"code","90affe48":"code","285448e6":"code","0320c252":"code","9653fca0":"code","e14386fd":"code","024b7a9d":"code","784e4e0e":"code","c13432e5":"code","49064bd9":"code","d304b5a5":"code","b0c7d5f5":"code","13c6e7c7":"code","6ce96c4e":"code","4c2311cc":"code","d25893b3":"code","24083a24":"code","beb75b5f":"code","66b82414":"code","5aeffe34":"code","9c18ed94":"code","d344517e":"code","873dcad0":"code","6693dece":"code","55824e9f":"code","82e84586":"code","7ee149ec":"code","cbf348f8":"code","603f7daf":"code","456d0528":"markdown","de6629cd":"markdown","9d357ef7":"markdown","2edb9105":"markdown","8f8ce0a8":"markdown","1fc7154a":"markdown","8c2ed808":"markdown","d62e46d6":"markdown","5a86b212":"markdown","3adefd9e":"markdown","55a108f6":"markdown","009d2ec4":"markdown","63e9a7a8":"markdown","5008e7bc":"markdown","f3c68cae":"markdown","4baf1b02":"markdown","c6bd11aa":"markdown","c1806fd4":"markdown","f6e1c952":"markdown","1d7ccb9f":"markdown","f360db52":"markdown","d16eecb6":"markdown","e0639987":"markdown","2eb0357b":"markdown","ec999131":"markdown","da79d91e":"markdown","b2f85149":"markdown","9d7153b6":"markdown","b522da28":"markdown","f9cf81ed":"markdown","98f9ebba":"markdown","65ceaade":"markdown","b08192fe":"markdown","3747baf2":"markdown","c33d20f4":"markdown","ac27f412":"markdown","9274c934":"markdown","a4afae63":"markdown","de649e6d":"markdown","fd500ce0":"markdown","a5fd44f8":"markdown","5359f202":"markdown","c340a21a":"markdown","f80f2a9d":"markdown","e8ef9367":"markdown"},"source":{"664c4db9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to","19263945":"import numpy as np  # linear algebra\nimport pandas as pd  # read and wrangle dataframes\nimport matplotlib.pyplot as plt # visualization\nimport seaborn as sns # statistical visualizations and aesthetics\nfrom sklearn.base import TransformerMixin # To create new classes for transformations\nfrom sklearn.preprocessing import (FunctionTransformer, StandardScaler) # preprocessing \nfrom sklearn.decomposition import PCA # dimensionality reduction\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nfrom scipy.stats import boxcox # data transform\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import (train_test_split, KFold , StratifiedKFold, \n                                     cross_val_score, GridSearchCV, \n                                     learning_curve, validation_curve) # model selection modules\nfrom sklearn.pipeline import Pipeline # streaming pipelines\nfrom sklearn.base import BaseEstimator, TransformerMixin # To create a box-cox transformation class\nfrom collections import Counter\nimport warnings\n# load models\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import (XGBClassifier, plot_importance)\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom time import time\n\n%matplotlib inline \nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')","9f205c5b":"df = pd.read_csv(r'\/kaggle\/input\/epitope-prediction\/input_bcell.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/epitope-prediction\/input_sars.csv')\n\nprint(df.shape)","b678a3fb":"df.head(15)","0bd4c7ff":"df.dtypes","8cb6353e":"peptide_seq_train = df['peptide_seq'].value_counts().sort_values()\npeptide_seq_test = test['peptide_seq'].value_counts().sort_values()\n\nprotein_seq_train = df['protein_seq'].value_counts().sort_values()\nprotein_seq_test = test['protein_seq'].value_counts().sort_values()\n\n\nparent_protein_id_train = df['parent_protein_id'].value_counts().sort_values()\nparent_protein_id_test = test['parent_protein_id'].value_counts().sort_values()\n\nnum_same_peptide = 0\nnum_protein = 0\nnum_parent_protein = 0\n\nfor value in peptide_seq_train:\n    if value in peptide_seq_test:\n        num_protein += 1\n        \nfor value in protein_seq_train:\n    if value in protein_seq_test:\n        num_same_peptide += 1\n        \nfor value in parent_protein_id_train:\n    if value in parent_protein_id_test:\n        num_parent_protein += 1\n        \nprint(\"num of same peptide seq : \", num_same_peptide)\n\nprint(\"num of same protein seq : \", num_same_peptide)\n\nprint(\"num of same parent protein id : \", num_parent_protein)","90affe48":"#drop the cat cols \ndf.drop(['peptide_seq', 'protein_seq', 'parent_protein_id'], axis=1, inplace=True)\n\nfeatures = df.columns[:-1].tolist()","285448e6":"df.describe()","0320c252":"df['target'].value_counts() * 100 \/ len(df)","9653fca0":"for feat in features:\n    skew = df[feat].skew()\n    sns.distplot(df[feat], kde= False, label='Skew = %.3f' %(skew), bins=30)\n    plt.legend(loc='best')\n    plt.show()","e14386fd":"# Detect observations with more than one outlier\n\ndef outlier_hunt(df):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than 2 outliers. \n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in df.columns.tolist():\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        \n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        \n        # Interquartile rrange (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > 2 )\n    \n    return multiple_outliers   \n\nprint('The dataset contains %d observations with more than 2 outliers' %(len(outlier_hunt(df[features]))))   ","024b7a9d":"plt.figure(figsize=(8,6))\nsns.boxplot(df[features])\nplt.show()","784e4e0e":"# plt.figure(figsize=(8,8))\n# sns.pairplot(df[features],palette='coolwarm')\n# plt.show()","c13432e5":"corr = df[features].corr()\nplt.figure(figsize=(16,16))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           xticklabels= features, yticklabels= features, alpha = 0.7,   cmap= 'coolwarm')\nplt.show()","49064bd9":"length = df['end_position'] - df['start_position']\ndf.drop(['start_position'], axis=1, inplace=True)\ndf['length'] = length\n\nfeatures = df.drop('target', axis=1).columns.tolist()\n\nprint('std of lenth : ',np.std(length))\n\nplt.figure(figsize=(8,6))\nsns.boxplot(length)\nplt.show()","d304b5a5":"df.info()","b0c7d5f5":"outlier_indices = outlier_hunt(df[features])\ndf = df.drop(outlier_indices).reset_index(drop=True)\nprint(df.shape)","13c6e7c7":"for feat in features:\n    skew = df[feat].skew()\n    sns.distplot(df[feat], kde=False, label='Skew = %.3f' %(skew), bins=30)\n    plt.legend(loc='best')\n    plt.show()","6ce96c4e":"df['target'].value_counts()","4c2311cc":"sns.countplot(df['target'])\nplt.show()","d25893b3":"# Define X as features and y as lablels\nX = df[features] \ny = df['target'] \n# set a seed and a test size for splitting the dataset \nseed = 7\ntest_size = 0.2\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size , random_state = seed)","24083a24":"features_boxcox = []\n\nfor feature in features:\n    bc_transformed, _ = boxcox(df[feature]+10)  # shift by 1 to avoid computing log of negative values\n    features_boxcox.append(bc_transformed)\n\nfeatures_boxcox = np.column_stack(features_boxcox)\ndf_bc = pd.DataFrame(data=features_boxcox, columns=features)\ndf_bc['target'] = df['target']","beb75b5f":"df_bc.describe()","66b82414":"for feature in features:\n    fig, ax = plt.subplots(1,2,figsize=(7,3.5))    \n    ax[0].hist(df[feature], color='blue', bins=30, alpha=0.3, label='Skew = %s' %(str(round(df[feature].skew(),3))) )\n    ax[0].set_title(str(feature))   \n    ax[0].legend(loc=0)\n    ax[1].hist(df_bc[feature], color='red', bins=30, alpha=0.3, label='Skew = %s' %(str(round(df_bc[feature].skew(),3))) )\n    ax[1].set_title(str(feature)+' after a Box-Cox transformation')\n    ax[1].legend(loc=0)\n    plt.show()","5aeffe34":"# check if skew is closer to zero after a box-cox transform\nfor feature in features:\n    delta = np.abs( df_bc[feature].skew() \/ df[feature].skew() )\n    if delta < 1.0 :\n        print('Feature %20s is less skewed after a Box-Cox transform' %(feature))\n    else:\n        print('Feature %20s is more skewed after a Box-Cox transform'  %(feature))","9c18ed94":"model_importances = XGBClassifier()\nstart = time()\nmodel_importances.fit(X_train, y_train)\nprint('Elapsed time to train XGBoost  %.3f seconds' %(time()-start))\nplot_importance(model_importances)\nplt.show()","d344517e":"pca = PCA(random_state = seed)\npca.fit(X_train)\nvar_exp = pca.explained_variance_ratio_\ncum_var_exp = np.cumsum(var_exp)\nplt.figure(figsize=(8,6))\nplt.bar(range(1,len(cum_var_exp)+1), var_exp, align= 'center', label= 'individual variance explained', \\\n       alpha = 0.7)\nplt.step(range(1,len(cum_var_exp)+1), cum_var_exp, where = 'mid' , label= 'cumulative variance explained', \\\n        color= 'red')\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.xticks(np.arange(1,len(var_exp)+1,1))\nplt.legend(loc='center right')\nplt.show()\n\n# Cumulative variance explained\nfor i, sum in enumerate(cum_var_exp):\n    print(\"PC\" + str(i+1), \"Cumulative variance: \",cum_var_exp[i]*100)","873dcad0":"n_components = 5\npipelines = []\nn_estimators = 200\n\n#print(df.shape)\npipelines.append( ('SVC',\n                   Pipeline([\n                              ('sc', StandardScaler()),\n#                               ('pca', PCA(n_components = n_components, random_state=seed ) ),\n                             ('SVC', SVC(random_state=seed))]) ) )\n\n\npipelines.append(('KNN',\n                  Pipeline([ \n                              ('sc', StandardScaler()),\n#                             ('pca', PCA(n_components = n_components, random_state=seed ) ),\n                            ('KNN', KNeighborsClassifier()) ])))\n\npipelines.append( ('RF',\n                   Pipeline([\n                              ('sc', StandardScaler()),\n#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('RF', RandomForestClassifier(random_state=seed, n_estimators=n_estimators)) ]) ))\n\n\npipelines.append( ('Ada',\n                   Pipeline([ \n                              ('sc', StandardScaler()),\n#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                    ('Ada', AdaBoostClassifier(random_state=seed,  n_estimators=n_estimators)) ]) ))\n\npipelines.append( ('ET',\n                   Pipeline([\n                              ('sc', StandardScaler()),\n#                              ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('ET', ExtraTreesClassifier(random_state=seed, n_estimators=n_estimators)) ]) ))\npipelines.append( ('GB',\n                   Pipeline([ \n                             ('sc', StandardScaler()),\n#                             ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('GB', GradientBoostingClassifier(random_state=seed)) ]) ))\n\npipelines.append( ('LR',\n                   Pipeline([\n                              ('sc', StandardScaler()),\n#                               ('pca', PCA(n_components = n_components, random_state=seed ) ), \n                             ('LR', LogisticRegression(random_state=seed)) ]) ))\n\nresults, names, times  = [], [] , []\nnum_folds = 10\nscoring = 'accuracy'\n\nfor name, model in pipelines:\n    start = time()\n    kfold = StratifiedKFold(n_splits=num_folds, random_state=seed)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring = scoring,\n                                n_jobs=-1) \n    t_elapsed = time() - start\n    results.append(cv_results)\n    names.append(name)\n    times.append(t_elapsed)\n    msg = \"%s: %f (+\/- %f) performed in %f seconds\" % (name, 100*cv_results.mean(), \n                                                       100*cv_results.std(), t_elapsed)\n    print(msg)\n\n\nfig = plt.figure(figsize=(12,8))    \nfig.suptitle(\"Algorithms comparison\")\nax = fig.add_subplot(1,1,1)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","6693dece":"# Create a pipeline with a Random forest classifier\npipe_rfc = Pipeline([ \n                      ('scl', StandardScaler()), \n                    ('rfc', RandomForestClassifier(random_state=seed, n_jobs=-1) )])\n\n# Set the grid parameters\nparam_grid_rfc =  [ {\n    'rfc__n_estimators': [100, 200,300,400], # number of estimators\n    #'rfc__criterion': ['gini', 'entropy'],   # Splitting criterion\n    'rfc__max_features':[0.05 , 0.1], # maximum features used at each split\n    'rfc__max_depth': [None, 5], # Max depth of the trees\n    'rfc__min_samples_split': [0.005, 0.01], # mininal samples in leafs\n    }]\n# Use 10 fold CV\nkfold = StratifiedKFold(n_splits=num_folds, random_state= seed)\ngrid_rfc = GridSearchCV(pipe_rfc, param_grid= param_grid_rfc, cv=kfold, scoring=scoring, verbose= 1, n_jobs=-1)\n\n#Fit the pipeline\nstart = time()\ngrid_rfc = grid_rfc.fit(X_train, y_train)\nend = time()\n\nprint(\"RFC grid search took %.3f seconds\" %(end-start))\n\n# Best score and best parameters\nprint('-------Best score----------')\nprint(grid_rfc.best_score_ * 100.0)\nprint('-------Best params----------')\nprint(grid_rfc.best_params_)","55824e9f":"# Let's define some utility functions to plot the learning & validation curves\n\ndef plot_learning_curve(train_sizes, train_scores, test_scores, title, alpha=0.1):\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.plot(train_sizes, train_mean, label='train score', color='blue', marker='o')\n    plt.fill_between(train_sizes,train_mean + train_std,\n                    train_mean - train_std, color='blue', alpha=alpha)\n    plt.plot(train_sizes, test_mean, label='test score', color='red',marker='o')\n    plt.fill_between(train_sizes,test_mean + test_std, test_mean - test_std , color='red', alpha=alpha)\n    plt.title(title)\n    plt.xlabel('Number of training points')\n    plt.ylabel('Accuracy')\n    plt.grid(ls='--')\n    plt.legend(loc='best')\n    plt.show()    \n    \ndef plot_validation_curve(param_range, train_scores, test_scores, title, alpha=0.1):\n    train_mean = np.mean(train_scores, axis=1)\n    train_std = np.std(train_scores, axis=1)\n    test_mean = np.mean(test_scores, axis=1)\n    test_std = np.std(test_scores, axis=1)\n    plt.plot(param_range, train_mean, label='train score', color='blue', marker='o')\n    plt.fill_between(param_range,train_mean + train_std,\n                    train_mean - train_std, color='blue', alpha=alpha)\n    plt.plot(param_range, test_mean, label='test score', color='red', marker='o')\n    plt.fill_between(param_range,test_mean + test_std, test_mean - test_std , color='red', alpha=alpha)\n    plt.title(title)\n    plt.grid(ls='--')\n    plt.xlabel('Parameter value')\n    plt.ylabel('Accuracy')\n    plt.legend(loc='best')\n    plt.show()    ","82e84586":"plt.figure(figsize=(9,6))\n\ntrain_sizes, train_scores, test_scores = learning_curve(\n              estimator= grid_rfc.best_estimator_ , X= X_train, y = y_train, \n                train_sizes=np.arange(0.1,1.1,0.1), cv= 10,  scoring='accuracy', n_jobs= - 1)\n\nplot_learning_curve(train_sizes, train_scores, test_scores, title='Learning curve for RFC')","7ee149ec":"stand = StandardScaler()\n\nXt = stand.fit_transform(X_train)\nXv = stand.transform(X_test)\n\nrnd = RandomForestClassifier(max_features=0.05, random_state=seed, n_estimators=100, max_depth= None,min_samples_split=0.005)\n\nrnd.fit(Xt, y_train)\n","cbf348f8":"pred = rnd.predict(Xv)","603f7daf":"print(classification_report(y_test, pred))","456d0528":"### Loading the libraries ","de6629cd":"## 2. Summarize data","9d357ef7":"### - Compare Algorithms","2edb9105":"Now it's time to compare the performance of different machine learning algorithms. We'll use 10-folds cross-validation to assess the performance of each model with the metric being the classification accuracy. Pipelines encompassing Standarization and PCA are used in order to avoid data leakage. Standarization is not performed for tree-based methods.","8f8ce0a8":"#### * **Univariate plots**","1fc7154a":"The Box-Cox transform seems to do a good job in reducing the skews of the different distributions of features.  However, it does not lead to the normalization of the feature distributions. Trial and error showed that it doesn't lead to an improvement of the performance of the used algorithms.  Next, let's explore dimensionality reduction techniques.","8c2ed808":"Let's go ahead and examine a heatmap of the correlations.","d62e46d6":"The dataset is pretty unbalanced. The instances of class 0  constitute more than 72 % while class 1 with only 27 %.","5a86b212":"###  Data Visualization","3adefd9e":"\n**Observation:** The best performances are achieved by RF. However, RF also yields a wide distribution. It is worthy to continue our study by tuning RF. \n\nLogistic Regression performs badly. This might be due to the fact that the data is not normally distributed as these algorithms perform well when data that is normally distributed.","55a108f6":"### - Split-out validation dataset","009d2ec4":"The dataset consists of 14907 observations","63e9a7a8":"Aha! there exists some 457 observations with multiple outliers.  These  could harm the efficiency of our learning algorithms. We'll make sure to get rid of these in the next sections.\n\nLet's examine the boxplots for the several distributions.","5008e7bc":"Let's examine if a Box-Cox transform can contribute to the normalization of some features. It should be emphasized that all transformations should only be done on the training set to avoid data snooping. Otherwise the test error estimation will be biased.","f3c68cae":"This dataset is clean; there aren't any missing values in it.","4baf1b02":"It appears that about 99.7 % of the variance can be explained with the first 1 principal components. However feeding the PCA features to the learning algorithms did not contribute to a better performance. This might be due to the non-linearites that PCA is not able to capture.","c6bd11aa":"Let's first summarize the distribution of the numerical \nvariables. ","c1806fd4":"## 4. Evaluate Algorithms","f6e1c952":"There is some features is no normally distributed. The features emini, end_position, stability and start_position exhibit the highest skew coefficients. Moreover, the distribution of stability and others seem to contain outliers. Let's identify the indices of the observations containing outliers using [Turkey's method](http:\/\/datapigtechnologies.com\/blog\/index.php\/highlighting-outliers-in-your-data-with-the-tukey-method\/).","1d7ccb9f":"#### **XGBoost**","f360db52":"## 3. Prepare data","d16eecb6":"# Glass Type Classification with Machine Learning","e0639987":"Let's go ahead and perform a PCA on the features to decorrelate the ones that are linearly dependent and then let's plot the cumulative explained variance.","2eb0357b":"### ### - Hunting and removing multiple outliers\n\nLet's remove the observations containing multiple outliers with the function we created in the previous section.","ec999131":"Let us first begin by loading the libraries that we'll use in the notebook","da79d91e":"let check the cols with object type","b2f85149":"Let's go ahead an look at the distribution of the different features of this dataset.","9d7153b6":"#### * **Multivariate plots**","b522da28":"## 1. Prepare Problem","f9cf81ed":"  Hey there and welcome to this kernel! My name is Walid. I'm a data science newbie and this is my first Kaggle notebook. I have been putting a lot of effort recently to start my journey in data science. I benefited a lot from the notebooks of other awesome kagglers and I hope you benefit this notebook and learn new stuff from it.\n\n<hr >\n\n# Contents\n\n## 1) Prepare Problem\n\n * Load libraries\n\n * Load and explore the shape of the dataset\n\n## 2) Summarize Data\n\n* Descriptive statistics\n\n* Data visualization\n\n## 3) Prepare Data\n\n* Data Cleaning\n\n* Split-out validation dataset\n\n*  Data transformation  \n\n## 4) Evaluate Algorithms\n\n* Dimensionality reduction\n\n* Compare Algorithms\n\n## 5) Improve Accuracy\n\n* Algorithm Tuning\n\n## 6) Diagnose the performance of the best algorithms\n\n* Diagnose overfitting by plotting the learning and validation curves\n* Further tuning\n\n## 7) Finalize Model\n\n* Create standalone model on entire training dataset\n\n* Predictions on test dataset\n\n<hr \/>","98f9ebba":"#### **PCA**","65ceaade":"### - Dimensionality reduction","b08192fe":"It appears that no main features dominate the importance in the XGBoost modeling of the problem. ","3747baf2":"Let's now plot a distribution of the classes.","c33d20f4":"### - Data cleaning","ac27f412":"There seems to be a strong positive correlation between start_position and end_position. This could be a hint to perform Principal component analysis in order to decorrelate some of the input features.","9274c934":"Removing observations with multiple outliers (more than 2) leaves us with 14577 observations to learn from. Let's now see how our distributions look like.","a4afae63":"Let's now proceed by drawing a pairplot to visually examine the correlation between the features.","de649e6d":"### Tuning Random Forests\n\nFor random forest, we can tune the number of grown trees (n_estimators), the trees' depth (max_depth), the criterion of splitting (gini or entropy) and so on.... Let's start tuning these.","fd500ce0":"### - Data transformation  ","a5fd44f8":"## 5. Algorithm tuning","5359f202":"### Loading and exploring the shape of the dataset","c340a21a":"### Descriptive statistics","f80f2a9d":"The features are not on the same scale. For example start_position has a mean of 308.845173 while aromaticity has a mean value of 0.077143. Features should be on the same scale for algorithms such as logistic regression (gradient descent) to converge smoothly. Let's go ahead and check the distribution of the data types.","e8ef9367":"we find that we can't encode this cat cols cs they don't share the same value\n\nthey are like ids for the instances"}}