{"cell_type":{"fd013b93":"code","e6efc80e":"code","e938868c":"code","f92e3721":"code","2947284f":"code","2b192385":"code","110cdf50":"code","552eea1d":"code","f418e4a9":"code","d041871f":"code","95447407":"code","9963f5cb":"code","667b16bd":"code","b165b5a8":"code","c13e4c09":"code","e2c88c97":"code","a1394ee5":"code","29456d69":"code","c191d170":"code","c2862f30":"code","5903ae1f":"code","60752c71":"code","9a1c33db":"code","fe030659":"code","c2991393":"code","d3b85a02":"code","2c975cfe":"code","dd01feb9":"code","96a5eef5":"code","ec8d3b99":"code","f22efe61":"code","6d1fc305":"code","9bafcf09":"code","fcb8f307":"code","635a1962":"code","4879783e":"code","1b4360e7":"code","0d1e0277":"code","68ea2769":"code","67b5966b":"code","935ffeb1":"code","d7eaafe3":"code","cc293846":"code","3d101ddf":"code","6c397940":"code","3782e088":"code","cab9518e":"code","57ae9727":"code","273e5397":"code","3b58e0e6":"code","67a941e8":"code","6751ae80":"code","6e04641c":"code","e432b244":"code","3d82c7ca":"code","e7514e07":"code","651ddefe":"code","61c34f0e":"code","912b6b7a":"code","69b66e49":"code","446fb92f":"code","8e212904":"code","fc238d2f":"code","96cf0df0":"code","4214c2a7":"code","75cb1bd4":"markdown","8635ab28":"markdown","2593ed93":"markdown","1614a917":"markdown","ef60b334":"markdown","8a712514":"markdown","1c098ee9":"markdown","f70ae902":"markdown","29219b79":"markdown","eb9ae593":"markdown","64e09a52":"markdown","889cbe73":"markdown","a03f5836":"markdown","a5d17d96":"markdown","80a2a708":"markdown","52073d47":"markdown","b48d7c61":"markdown","c307c084":"markdown","4ea2b5ae":"markdown"},"source":{"fd013b93":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib.pyplot import xticks\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\nfrom nltk.stem import WordNetLemmatizer\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.util import ngrams\nfrom collections import defaultdict\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, Callback\nimport tensorflow as tf\nfrom sklearn.metrics import f1_score\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom keras.preprocessing.sequence import pad_sequences\nfrom numpy import array\nfrom numpy import asarray\nfrom numpy import zeros\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Flatten,Embedding,Activation,Dropout\nfrom keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D,LSTM\nfrom keras.layers import Bidirectional","e6efc80e":"train= pd.read_csv('..\/input\/bitcoin-tweets-14m\/cleanprep.csv',nrows=20000)\ntrain.shape","e938868c":"train.columns=['date','name','text','sentiment','polarity']\ntrain=train.drop(['date','name','polarity'],axis=1)\ntrain.head()","f92e3721":"#lets save stopwords in a variable\nstop = list(stopwords.words(\"english\"))\nprint(stop)","2947284f":"# save list of punctuation\/special characters in a variable\npunctuation = list(string.punctuation)\nprint(punctuation)","2b192385":"# create an object to convert the words to its lemma form\nlemma = WordNetLemmatizer()","110cdf50":"# lets make a combine list of stopwords and punctuations\nsw_pun = stop + punctuation","552eea1d":"# function to preprocess the messages\ndef preprocess(tweet):\n    tweet = re.sub(r\"https?:\\\/\\\/t.co\\\/[A-Za-z0-9]+\", \"\", tweet) # removing urls \n    tweet = re.sub('[^\\w]',' ',tweet) # remove embedded special characters in words (for example #earthquake)         \n    #tweet = re.sub('[\\d]','',tweet) # this will remove numeric characters\n    tweet = tweet.lower()\n    words = tweet.split()  \n    sentence = \"\"\n    for word in words:     \n        if word not in (sw_pun):  # removing stopwords & punctuations                \n            word = lemma.lemmatize(word,pos = 'v')  # converting to lemma    \n            if len(word) > 3: # we will consider words with length  greater than 3 only\n                sentence = sentence + word + ' '             \n    return(sentence)","f418e4a9":"# apply preprocessing functions on the train and test datasets\ntrain['text'] = train['text'].apply(lambda s : preprocess(s))","d041871f":"# function to remove emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","95447407":"# applying the function on the train and the test datasets\ntrain['text'] = train['text'].apply(lambda s : remove_emoji(s))","9963f5cb":"# function to create vocab\nfrom collections import Counter\ndef create_vocab(df):\n    vocab = Counter()\n    for i in range(df.shape[0]):\n        vocab.update(df.text[i].split())\n    return(vocab)","667b16bd":"# concatenate training and testing datasets\nmaster=train.reset_index(drop=True)\n\n# call vocabulary creation function on master dataset\nvocab = create_vocab(master)\n\n# lets check the no. of words in the vocabulary\nlen(vocab)","b165b5a8":"# lets check the most common 50 words in the vocabulary\nvocab.most_common(50)\n","c13e4c09":"# create the final vocab by considering words with more than one occurence\nfinal_vocab = []\nmin_occur = 2\nfor k,v in vocab.items():\n    if v >= min_occur:\n        final_vocab.append(k)","e2c88c97":"# lets check the no. of the words in the final vocabulary\nvocab_size = len(final_vocab)\nvocab_size","a1394ee5":"# function to filter the dataset, keep only words which are present in the vocab\ndef filter(tweet):\n    sentence = \"\"\n    for word in tweet.split():  \n        if word in final_vocab:\n            sentence = sentence + word + ' '\n    return(sentence)","29456d69":"# apply filter function on the train and test datasets\ntrain['text'] = train['text'].apply(lambda s : filter(s))\ntrain.sample(10)","c191d170":"from keras.preprocessing.text import Tokenizer\n# fit a tokenizer\ndef create_tokenizer(lines):\n    # num_words = vocab_size will create a tokenizer,configured to only take into account the vocab_size(6025)\n    tokenizer = Tokenizer(num_words=vocab_size)\n    # Build th word index, Turns strings into lists of integer indices\n    tokenizer.fit_on_texts(lines) \n    return tokenizer","c2862f30":"# create and apply tokenizer on the training dataset\ntokenizer = create_tokenizer(train.text)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","5903ae1f":"# converting texts into vectors\ntrain_text = tokenizer.texts_to_matrix(train.text, mode = 'freq')","60752c71":"# Test train split \nX_train, X_test, y_train, y_test = train_test_split(train_text, train.sentiment, test_size = 0.2, random_state = 42)","9a1c33db":"def define_model(n_words):\n    # define network\n    model = Sequential()\n    model.add(Dense(1024, input_shape=(n_words,), activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(512,activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(256,activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Dense(1, activation='sigmoid'))\n    \n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics = ['accuracy'])\n    \n    model.summary()\n    \n    return model","fe030659":"X_train.shape","c2991393":"callbacks_list = [EarlyStopping(monitor='accuracy',patience=10,),\n                  ModelCheckpoint(filepath='.\/NN.h5',monitor='val_loss',save_best_only=True)]","d3b85a02":"# create the model\nn_words = X_train.shape[1]\nmodel = define_model(n_words)","2c975cfe":"history = model.fit(X_train,y_train,epochs=20,\n                    verbose=2,\n                    callbacks=callbacks_list,\n                    validation_split=0.2)","dd01feb9":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","96a5eef5":"import keras\n\n# load the model from disk\nloaded_model_NN = keras.models.load_model('.\/NN.h5',custom_objects=dependencies)\n\n# prediction on the test dataset\n#X_test_Set = tokenizer.texts_to_matrix(X_test, mode = 'freq')\ny_pred = loaded_model_NN.predict_classes(X_test)","ec8d3b99":"'''\n\ntest=pd.read_csv('..\/input\/tweets\/live_tweet.csv')\n\ntest_id = test.tweet\ntest.drop([\"id\",\"date\",\"name\"],1,inplace = True)\n\n#apply tokenizer on the test dataset\ntest_set = tokenizer.texts_to_matrix(test.text, mode = 'freq')\n\n# make predictions on the test dataset\ny_test_pred = loaded_model_NN.predict_classes(test_set)\n\n\nresult = pd.DataFrame()\nresult.to_csv('prediction_NN.csv',index=False)\n'''","f22efe61":"from keras.layers import Embedding\n# La couche Embedding prend au moins deux arguments: le nombre de jetons tokens et la dimension des embeddings (here, 64).\nembedding_layer = Embedding(vocab_size, 64)","6d1fc305":"# Nombre de mots \u00e0 consid\u00e9rer comme caract\u00e9ristiques\nmax_features = vocab_size\n\n# Coupe le texte apr\u00e8s ce nombre de mots (parmi les max_features les mots les plus courants)\nmaxlen = 100","9bafcf09":"# cr\u00e9er et appliquer un tokenizer sur l'ensemble de donn\u00e9es d'entra\u00eenement\ntokenizer = create_tokenizer(train.text)","fcb8f307":"from keras import preprocessing\n# conversion de texte en s\u00e9quences\nsequences = tokenizer.texts_to_sequences(train.text)\nfor i in [20,300,43]:\n    print('La phrase % a \u00e9t\u00e9 transcrite en '%tokenizer.sequences_to_texts([sequences[i]]),sequences[i])","635a1962":"# Turns the lists of integers into a 2D integer tensor of shape (samples, maxlen), padding shorter sequences with 0s\ntrain_text = preprocessing.sequence.pad_sequences(sequences, maxlen=maxlen)","4879783e":"# Test train split \nX_train, X_test, y_train, y_test = train_test_split(train_text, train.sentiment, test_size = 0.2, random_state = 42)","1b4360e7":"model = Sequential()\n# Sp\u00e9cifie la longueur d'entr\u00e9e maximale du Embedding layer afin que vous puissiez ult\u00e9rieurement aplatir les entr\u00e9es embedded. \n\n# Apr\u00e8s le calque Embedding, les activations ont la forme (samples, maxlen, 8)\nmodel.add(Embedding(vocab_size, 8, input_length=maxlen))\n\n# Flattens the 3D tensor of embeddings into a 2D tensor of shape (samples, maxlen * 8)\nmodel.add(Flatten())\n\n# Dense layer for classification\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\nmodel.summary()","0d1e0277":"callbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='.\/embd.h5',monitor='val_loss',\n                                  save_best_only=True)]","68ea2769":"# train the model\nhistory = model.fit(np.asarray(X_train).astype(np.float32),y_train,\n                    epochs=10,batch_size=32,\n                    callbacks=callbacks_list,validation_split=0.2)","67b5966b":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","935ffeb1":"# load the model from disk\nloaded_model_embd = keras.models.load_model('.\/embd.h5',custom_objects=dependencies)","d7eaafe3":"from keras.layers import Embedding, SimpleRNN\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 32))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","cc293846":"callbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='.\/SRNN.h5',monitor='val_loss',save_best_only=True)]","3d101ddf":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train,\n                    epochs=20,batch_size=32,\n                    callbacks=callbacks_list,\n                    validation_split=0.2)","6c397940":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","3782e088":"\n# load the model from disk\nloaded_model_SRNN = keras.models.load_model('.\/SRNN.h5',custom_objects=dependencies)\ny_pred = loaded_model_SRNN.predict_classes(X_test)","cab9518e":"max_words=20000\n\nfrom keras.layers import Embedding, SimpleRNN\nmodel = Sequential()\nmodel.add(Embedding(max_words, 32))\nmodel.add(SimpleRNN(32,return_sequences=True))\nmodel.add(SimpleRNN(32,return_sequences=True))\nmodel.add(SimpleRNN(32,return_sequences=True))\nmodel.add(SimpleRNN(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","57ae9727":"callbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='.\/STRNN.h5',monitor='val_loss',save_best_only=True)]","273e5397":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n\nhistory = model.fit(X_train, y_train,\n                    epochs=20,batch_size=32,\n                    callbacks=callbacks_list,\n                    validation_split=0.2)","3b58e0e6":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","67a941e8":"from keras.layers import LSTM\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(LSTM(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n\ncallbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='.\/LSTM.h5',monitor='val_loss',save_best_only=True)]","6751ae80":"history = model.fit(X_train, y_train,epochs=20,\n                    batch_size=32,callbacks=callbacks_list,\n                    validation_split=0.2)","6e04641c":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","e432b244":"model = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(32,return_sequences=True)))\nmodel.add(Bidirectional(LSTM(32)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n\ncallbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='.\/BILSTM.h5',monitor='val_loss',save_best_only=True)]","3d82c7ca":"history = model.fit(X_train, y_train,epochs=20,\n                    batch_size=32,callbacks=callbacks_list,\n                    validation_split=0.2)","e7514e07":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","651ddefe":"from keras.layers import GRU\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(GRU(32))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n\ncallbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='.\/GRU.h5',monitor='val_loss',save_best_only=True)]","61c34f0e":"history = model.fit(X_train, y_train,epochs=20,\n                    batch_size=32,callbacks=callbacks_list,\n                    validation_split=0.2)","912b6b7a":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","69b66e49":"from keras.layers import GRU\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()\n\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n\ncallbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='.\/SGRU.h5',monitor='val_loss',save_best_only=True)]","446fb92f":"history = model.fit(X_train, y_train,epochs=20,\n                    batch_size=32,callbacks=callbacks_list,\n                    validation_split=0.2)","8e212904":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","fc238d2f":"from keras.layers import GRU\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 32))\nmodel.add(GRU(32,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\nmodel.add(GRU(32,return_sequences=True))\nmodel.add(GRU(32,dropout=0.2,recurrent_dropout=0.2,return_sequences=True))\nmodel.add(GRU(32))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()\n\nmodel.compile(optimizer='rmsprop',loss='binary_crossentropy',metrics=['accuracy'])\n\ncallbacks_list = [EarlyStopping(monitor='accuracy',patience=1,),\n                  ModelCheckpoint(filepath='.\/DSGRU.h5',monitor='val_loss',save_best_only=True)]","96cf0df0":"history = model.fit(X_train, y_train,epochs=20,\n                    batch_size=32,callbacks=callbacks_list,\n                    validation_split=0.2)","4214c2a7":"# check model performance\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","75cb1bd4":"## Stacked GRU with Dropouts","8635ab28":"Neural Network with Embedding layer seems to the best model for this classification task.","2593ed93":"## SIMPLE RNN","1614a917":"lets consider only those words which have appeared more than once in the corpus\n","ef60b334":"## Neural Network\n\nNous allons cr\u00e9er un r\u00e9seau de neurones artificiels, les sentiments sont \u00e9valu\u00e9s par la fonction f1, qui ne sont pas affich\u00e9s par d\u00e9faut apr\u00e8s chaque \u00e9poque, donc cr\u00e9ons une fonction pour obtenir le score.","8a712514":"# Predictions on the test dataset","1c098ee9":"## Stacked GRU","f70ae902":"# Model utilisant les Word Embeddings\n\nUn autre moyen populaire et puissant d'associer un vecteur \u00e0 un mot est l'utilisation de vecteurs de mots denses, \u00e9galement appel\u00e9s `word embeddings`. \n\nLa couche Embedding est un dictionnaire qui mappe des indices entiers (qui repr\u00e9sentent des mots sp\u00e9cifiques) \u00e0 des vecteurs denses. Il prend des entiers en entr\u00e9e, il recherche ces entiers dans un dictionnaire interne et il renvoie les vecteurs associ\u00e9s. Il s\u2019agit en fait d\u2019une recherche dans le dictionnaire.\n\nAlors que les vecteurs obtenus par encodage one-hot sont binaires, clairsem\u00e9s (principalement constitu\u00e9s de z\u00e9ros) et de tr\u00e8s haute dimension (m\u00eame dimensionnalit\u00e9 que le nombre de mots dans le vocabulaire), les embeddings de mots sont des vecteurs \u00e0 virgule flottante de faible dimension (c'est-\u00e0-dire , vecteurs denses, par opposition aux vecteurs clairsem\u00e9s); \n\nContrairement aux vecteurs de mots obtenus via un encodage one-hot, les embeddings de mots sont appris \u00e0 partir de donn\u00e9es. Il est courant de voir des word embeddings de dimensions 256 , 512 ou 1 024 lorsqu'il s'agit de vocabulaires tr\u00e8s volumineux.\n\nD'autre part, one-hot encoding conduisent g\u00e9n\u00e9ralement \u00e0 des vecteurs de 20 000 dimensions ou plus (capturant un vocabulaire de 6 025 tokens). Ainsi, les word embeddings regroupent plus d'informations dans beaucoup moins de dimensions.","29219b79":"## Stack multiple SimpleRNN layers","eb9ae593":"# Model Building & Evaluation","64e09a52":"## LSTM","889cbe73":"# Vocabulary creation\nLets create our own vocabulary","a03f5836":"# Data Preprocessing","a5d17d96":"### Neural Network with Embedding Layer","80a2a708":"## GRU","52073d47":"Nous allons maintenant appliquer la fonction text_to_matrix () pour convertir du texte en vecteurs.\n\nLa fonction text_to_matrix () sur le Tokenizer peut \u00eatre utilis\u00e9e pour cr\u00e9er un vecteur par document fourni par entr\u00e9e. La longueur des vecteurs est la taille totale du vocabulaire, qui est de 6025 ici (nous avons pass\u00e9 6025 en tant que num_words dans tokenizer)\n\n\n* \u2018binary\u2018: Si chaque mot est pr\u00e9sent ou non dans le document. C'est la valeur par d\u00e9faut.\n* \u2018count\u2018: Le nombre de chaque mot dans le document.\n* \u2018tfidf\u2018: The Text Frequency-Inverse DocumentFrequency (TF-IDF) scoring for each word \n* \u2018freq\u2018: La fr\u00e9quence de chaque mot dans le document.","b48d7c61":"## Bi-Direction LSTM","c307c084":"Now lets apply this vocab on our train and test datasets, we will keep only those words in training and testing datasets which appear in the vocabulary","4ea2b5ae":"nous ne gardons que les 20 premiers mots de chaque tweets. Mais notez que le simple aplatissement des s\u00e9quences incorpor\u00e9es et la formation d'une seule couche dense sur le dessus conduit \u00e0 un mod\u00e8le qui traite chaque mot de la s\u00e9quence d'entr\u00e9e s\u00e9par\u00e9ment, sans prendre en compte les relations entre les mots et la structure des phrases (par exemple, ce mod\u00e8le traiterait probablement les deux \" ce film est une bombe \u00bbet\u00ab ce film est la bombe \u00bbcomme \u00e9tant des critiques n\u00e9gatives). \n\nIl est pr\u00e9f\u00e9rable d\u2019ajouter des couche recurrent layers ou 1D convolutional layers au-dessus des embedded sequences pour apprendre les fonctionnalit\u00e9s qui prennent en compte chaque s\u00e9quence dans son ensemble."}}