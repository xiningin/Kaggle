{"cell_type":{"e24c9052":"code","dc7d36a5":"code","b2e6f325":"code","9131aaff":"code","c86b3e41":"code","4213e868":"code","c58bfe68":"code","ff630ea6":"code","85b28603":"code","160378ec":"code","8a3baaea":"code","e05ccfba":"code","0ba6bb93":"code","6efaa426":"code","8b074c54":"code","6c8f0056":"code","43d88ccb":"code","40e45fdd":"code","6dcf5082":"code","94452b50":"code","7daa7f82":"code","a105902c":"code","9a0cced4":"code","ecb937ca":"code","8e09acee":"markdown","2f686b55":"markdown","34f3a2c0":"markdown","ca96cac0":"markdown","8fd0a75f":"markdown","0091511e":"markdown","d536c119":"markdown","cd878315":"markdown","3eea7dd3":"markdown","fa2f42a5":"markdown","13a18706":"markdown","867d534e":"markdown","fc6c8484":"markdown","b59d951c":"markdown","8e31f640":"markdown","2b0b9a4d":"markdown","4eb21185":"markdown","926ab197":"markdown"},"source":{"e24c9052":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\nimport warnings \nwarnings.filterwarnings(\"ignore\")\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc7d36a5":"train = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")\nprint(train.shape)\ntrain.head()","b2e6f325":"test = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/test.csv\")\nprint(test.shape)\ntest.head()","9131aaff":"y_train = train[\"label\"]\nx_train = train.drop([\"label\"],axis = 1)","c86b3e41":"y_train.shape","4213e868":"plt.figure(figsize = (15,7))\nsns.countplot(y_train,palette = \"icefire\")\nplt.title(\"Number of digit classes\")\ny_train.value_counts()","c58bfe68":"# plot some samples\nimg = x_train.iloc[0].values\nimg = img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(train.iloc[0,0])\nplt.axis(\"off\")\nplt.show()","ff630ea6":"# plot some samples\nimg = x_train.iloc[51].values\nimg = img.reshape((28,28))\nplt.imshow(img,cmap='gray')\nplt.title(train.iloc[51,0])\nplt.axis(\"off\")\nplt.show()","85b28603":"x_train = x_train \/ 255.0\ntest = test \/ 255\nprint(x_train.shape)\nprint(test.shape)","160378ec":"y_train.shape","8a3baaea":"x_train = x_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)\nprint(\"x_train: \",x_train.shape)\nprint(\"test: \",test.shape)","e05ccfba":"from keras.utils.np_utils import to_categorical # convert to one-hot-encoding \/ label encoding\ny_train = to_categorical(y_train,num_classes = 10)","0ba6bb93":"y_train.shape","6efaa426":"y_train","8b074c54":"from sklearn.model_selection import train_test_split\nx_train,x_val,y_train,y_val = train_test_split(x_train,y_train,test_size = 0.1,random_state = 2)\n\nprint(\"x_train shape: \",x_train.shape)\nprint(\"x_val shape: \",x_val.shape)\nprint(\"y_train shape: \",y_train.shape)\nprint(\"y_val shape: \",y_val.shape)","6c8f0056":"from sklearn.metrics import confusion_matrix\nimport itertools\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Dropout,Flatten,Conv2D,MaxPool2D,BatchNormalization\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nmodel = Sequential()\n#\nmodel.add(Conv2D(filters = 32,kernel_size = (5,5),padding = \"Same\",\n                activation = \"relu\",input_shape = (28,28,1)))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization(\n    axis=-1,\n    momentum=0.99,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer=\"zeros\",\n    gamma_initializer=\"ones\",\n    moving_mean_initializer=\"zeros\",\n    moving_variance_initializer=\"ones\"\n))\n\nmodel.add(Conv2D(filters = 64,kernel_size = (3,3),padding = \"Same\",\n                activation = \"relu\"))\nmodel.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization(\n    axis=-1,\n    momentum=0.99,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer=\"zeros\",\n    gamma_initializer=\"ones\",\n    moving_mean_initializer=\"zeros\",\n    moving_variance_initializer=\"ones\"\n))\n\nmodel.add(Conv2D(filters = 128,kernel_size = (3,3),padding = \"Same\",\n                activation = \"relu\"))\nmodel.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization(\n    axis=-1,\n    momentum=0.99,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer=\"zeros\",\n    gamma_initializer=\"ones\",\n    moving_mean_initializer=\"zeros\",\n    moving_variance_initializer=\"ones\"\n))\n\nmodel.add(Conv2D(filters = 256,kernel_size = (3,3),padding = \"Same\",\n                activation = \"relu\"))\nmodel.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\nmodel.add(Dropout(0.25))\nmodel.add(BatchNormalization(\n    axis=-1,\n    momentum=0.99,\n    epsilon=0.001,\n    center=True,\n    scale=True,\n    beta_initializer=\"zeros\",\n    gamma_initializer=\"ones\",\n    moving_mean_initializer=\"zeros\",\n    moving_variance_initializer=\"ones\"\n))\n\n\n# Fully Connected\nmodel.add(Flatten())\n\nmodel.add(Dense(256,activation = \"relu\")) #hidden layer\nmodel.add(BatchNormalization())\n          \nmodel.add(Dense(120,activation = \"relu\"))\nmodel.add(BatchNormalization())\n          \nmodel.add(Dense(120,activation = \"relu\"))\nmodel.add(BatchNormalization())\n          \nmodel.add(Dense(120,activation = \"relu\"))\nmodel.add(BatchNormalization())\n          \nmodel.add(Dense(100,activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\n\nmodel.add(Dense(10,activation = \"softmax\")) #output layer","43d88ccb":"optimizer = Adam(lr = 0.001,beta_1=0.9, beta_2=0.999) # the optimizer tries to find the best learning rate for our model.","40e45fdd":"model.compile(optimizer=optimizer,loss = \"categorical_crossentropy\",metrics = [\"accuracy\"])","6dcf5082":"epochs = 60\nbatch_size = 250","94452b50":"datagen = ImageDataGenerator(rotation_range=0.9,\n                            zoom_range= 0.5,\n                            width_shift_range= 0.9,\n                            height_shift_range=0.5)\ndatagen.fit(x_train)","7daa7f82":"x_train.shape","a105902c":"history = model.fit_generator(datagen.flow(x_train,y_train,batch_size = batch_size),\n                             epochs=epochs,validation_data= (x_val,y_val),steps_per_epoch=x_train.shape[0] \/\/ batch_size)","9a0cced4":"# Plot the loss and accuracy curves for training and validation\nplt.plot(history.history[\"val_loss\"],color = \"g\",label = \"validation loss\")\nplt.title(\"Test Loss\")\nplt.xlabel(\"Number of Epochs\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()\n","ecb937ca":"import seaborn as sns\n# Predict the values from the validation dataset\ny_pred = model.predict(x_val)\n# Convert predictions classes to one hot vectors\ny_pred_classes = np.argmax(y_pred,axis = 1)\n# Convert validation observations to one hot vectors \ny_true = np.argmax(y_val,axis = 1)\n# Compute the confusion matrix\nconfusion_mtx = confusion_matrix(y_true,y_pred_classes)\n# plot the cf\nf,ax = plt.subplots(figsize = (8,8))\nsns.heatmap(confusion_mtx,annot = True,linewidths=0.01,cmap = \"Greens\",linecolor = \"gray\",fmt = \".1f\",ax = ax)\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.title(\"Confusion Matrix\")\nplt.show()","8e09acee":"### -- DIGIT RECOGNIZER DATASET WITH CNN USING KERAS --\n1. [IMPORT THE DATA AND A QUICK LOOK](#1)\n     * [Getting the Test Data](#2)\n     * [Creating x_train and y_train](#3)\n     * [A quick visualization](#4)\n     * [Looking for sample pictures](#5)\n     \n     \n1. [EDITS FOR MAKING THE DATA SUITABLE FOR FUTURE](#6)     \n     * [Setting the shapes](#7)\n     * [Train-Test-Split](#8)\n1. [IMPORTING LIBRARIES AND BUILDING THE MODEL](#9)\n     * [Defining the Optimizer](#10)\n     * [Compiling](#11)\n     * [Epochs and Batch Size](#12)\n     * [Data Augmentation](#13)\n1. [FITTING THE MODEL](#14)\n1. [VISUALIZATION](#15)","2f686b55":"<a id = \"9\"><\/a>\n# Import Necessary Libraries and Create the Model","34f3a2c0":"<a id = \"8\"><\/a>\n### Train Test Split","ca96cac0":"<a id = \"4\"><\/a>\n> ### A Quick Visualization","8fd0a75f":"<a id = \"14\"><\/a>\n# Fit the model","0091511e":"<a id = \"15\"><\/a>\n## Lets see what the loss looks like","d536c119":"<a id = \"2\"><\/a>\n### Lets get our test data","cd878315":"<a id = \"13\"><\/a>\n### Data Augmentation\n * To avoid overfitting we take one pic and by changing it produce new pics from that","3eea7dd3":"<a id = \"5\"><\/a>\n### Lets look at what we have..","fa2f42a5":"<a id = \"10\"><\/a>\n### We will define the optimizer","13a18706":"## Achived 98.64% Accuracy!","867d534e":"<a id = \"6\"><\/a>\n# Lets Start Having Our Data Suitable for the Models","fc6c8484":"<a id = \"12\"><\/a>\n## Epochs and Batch Size\n* Say you have a dataset of 10 examples (or samples). You have a batch size of 2, and you've specified you want the algorithm to run for 3 epochs. Therefore, in each epoch, you have 5 batches (10\/2 = 5). Each batch gets passed through the algorithm, therefore you have 5 iterations per epoch.\n* reference: https:\/\/stackoverflow.com\/questions\/4752626\/epoch-vs-iteration-when-training-neural-networks","b59d951c":"<a id = \"3\"><\/a>\n### Lets Create x_train and y_train","8e31f640":"<a id = \"1\"><\/a>\n# IMPORT THE DATA AND A QUICK LOOK","2b0b9a4d":"<a id = \"7\"><\/a>\n### We should set the shapes for implementation","4eb21185":"<a id = \"11\"><\/a>\n### Compile Model\n* Categorical crossenthropy for multi class","926ab197":"## Lets visualize the confusion matrix by using SNS heatmap"}}