{"cell_type":{"26dcce96":"code","68fe6693":"code","45f02159":"code","84896d10":"code","ec0360fa":"code","e5856a79":"code","83590c27":"code","a5fb390f":"code","e3e0fc7d":"code","24a282e1":"code","e4696c07":"code","d5033546":"code","9150cfdb":"code","d2aaf04d":"code","e6fc1a76":"code","99882546":"code","297a782a":"code","d2540a57":"code","e508db5b":"code","95ede4ef":"code","dfc0693b":"code","24c97b1f":"code","a3cb0e59":"markdown","36b0ad18":"markdown","3ba5f540":"markdown","6993defc":"markdown","556c0d11":"markdown","642e21be":"markdown","2c39df56":"markdown","9f4cfc34":"markdown","ea30b33c":"markdown","083810b3":"markdown","e51d9d03":"markdown","1822291c":"markdown","102cebaa":"markdown","ae8c9f55":"markdown","f21d880a":"markdown","d28cac4f":"markdown","8e9dcd22":"markdown","bfd8cbe4":"markdown"},"source":{"26dcce96":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.preprocessing import OrdinalEncoder \nfrom sklearn.preprocessing import StandardScaler \nfrom scipy import stats as st\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import r2_score\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","68fe6693":"data_mov = pd.read_csv('\/kaggle\/input\/movietweetings\/movies.dat', delimiter='::', engine='python', header=None, names = ['Movie ID', 'Movie Title', 'Genre'])\ndata_us = pd.read_csv('\/kaggle\/input\/movietweetings\/users.dat', delimiter='::', engine='python', header=None, names = ['User ID', 'Twitter ID'])\ndata_rat = pd.read_csv('\/kaggle\/input\/movietweetings\/ratings.dat', delimiter='::', engine='python', header=None, names = ['User ID', 'Movie ID', 'Rating', 'Rating Timestamp'])","45f02159":"print(data_mov.info())\nprint(data_us.info())\nprint(data_rat.info())","84896d10":"data_mov = data_mov.dropna()\ndata_mov = data_mov.drop_duplicates().reset_index(drop=True)","ec0360fa":"print(data_mov.info())","e5856a79":"data_mov.head()","83590c27":"year = data_mov['Movie Title'].str.split('(',expand=True)[1]\ndata_mov['Year'] = year.str.split(')',expand=True)[0]\ndata_mov['Year'] =data_mov['Year'].astype('int')\ndata_mov['Movie Title'] = data_mov['Movie Title'].str.split('(',expand=True)[0]","a5fb390f":"data_mov.head(10)","e3e0fc7d":"data_rat['Rating'].hist(color='red',alpha=0.3)","24a282e1":"data.boxplot('Rating')","e4696c07":"data_mov","d5033546":"name_len=[]\nfor i in range(len(data_mov['Movie Title'])):\n     name_len.append(len(data_mov['Movie Title'][i]))\ndata_mov['name_length']  = name_len        ","9150cfdb":"data_mov.pivot_table(index = 'Year',values = \n                     'name_length').reset_index().plot(grid=True,\n                    x= 'Year',y = 'name_length',kind='scatter',\n                    figsize=[12,3] , alpha= 0.4,color = 'red')","d2aaf04d":"data = data_mov.merge(data_rat,on ='Movie ID',how='left').merge(data_us,on='User ID',how='left')\ndata['Rating'] = data['Rating']\/2","e6fc1a76":"data","99882546":"data.pivot_table(index =['Year','Movie Title'],\nvalues=['User ID','Rating'],aggfunc ={'User ID':'count',\n        'Rating':'mean'}).reset_index().sort_values(['User ID','Rating'],ascending=False).head(20)","297a782a":"data.info()","d2540a57":"data['Genre'] = data['Genre'].str.split('|')\ngenres = data['Genre'].str.join('|').str.get_dummies()\ngenres = genres.reset_index()\ngenres = genres.drop(['index'],axis=1)","e508db5b":"data= pd.concat([data,genres],axis = 1)\ndata = data.drop('Genre',axis = 1)","95ede4ef":"target = data['Rating']\nfeatures = data.drop(['Rating','Movie Title'], axis=1)\n\ntarget_train, target_valid, features_train, features_valid = train_test_split(target,features, test_size=0.25, random_state=42)","dfc0693b":"numeric = ['Movie ID', 'Year', 'name_length', 'User ID',\n       'Rating Timestamp', 'Twitter ID', 'Action', 'Adult', 'Adventure',\n       'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama',\n       'Family', 'Fantasy', 'Film-Noir', 'Game-Show', 'History', 'Horror',\n       'Music', 'Musical', 'Mystery', 'News', 'Reality-TV', 'Romance',\n       'Sci-Fi', 'Short', 'Sport', 'Talk-Show', 'Thriller', 'War', 'Western']\npd.options.mode.chained_assignment = None\n# ... (\u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0438 \u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445) ...\nscaler = StandardScaler()\nscaler.fit(features_train[numeric])\nfeatures_train[numeric]= scaler.transform(features_train[numeric])\nfeatures_valid[numeric]= scaler.transform(features_valid[numeric])","24c97b1f":"model = RandomForestRegressor(n_estimators=50, max_depth=19, random_state=12345)\nmodel.fit(features_train, target_train)\npredicted_valid = model.predict(features_valid)\nprint(\"R2 =\", r2_score(target_valid,predicted_valid))","a3cb0e59":"*We read the data, get 3 separate datasets.*","36b0ad18":"*We create a different column for each genre so as not to get into the dammi trap.*","3ba5f540":"*Let's take a closer look at how long the movie titles have changed over the years.*","6993defc":"*There is lost data in the movies dataset in the Genres column, otherwise all data is clean enough and ready for research analysis.*","556c0d11":"*Remove missed data and throw out all duplicate data.*","642e21be":"*We divide the data in the column Movie Title and highlight the year of the painting in a separate column.*","2c39df56":"*Now data is clear*","9f4cfc34":"*We make a rating of films for all years based on the number of reactions in Twitter and ratings.*","ea30b33c":"*It's the highest R2 , I've tried several models here indicated the most optimal one with effective number of estimators and depth.*","083810b3":"### Read data. General information","e51d9d03":"*I'm connecting all the datasets for the convenience of further research.*","1822291c":"### Reserch data","102cebaa":"### Modeling","ae8c9f55":"*From this histogram you can see that the data is not distributed normally and all user ratings are mostly close to high scores.*","f21d880a":"### **The dataset**\nSince this dataset will be updated regularly we have structured the dataset in different folders \/latest and \/snapshots. The \/latest folder will always contain the complete dataset as available at the time of the commit, while the \/snapshots contain fixed portions of the dataset to allow experimentation and reproducibility of research. The 10K snapshot represents the ratings from the first 10,000 collected tweets, 20K the first 20,000, and so on.\n\nThe dataset files are modeled after the [MovieLens dataset] (http:\/\/www.grouplens.org\/node\/73) to make them as interchangeable as possible. There are three files: users.dat, items.dat and ratings.dat.\n\n### users.dat\n\nContains the mapping of the users ids on their true Twitter id in the following format: userid::twitter_id. For example:\n\n1::177651718\n\nWe provide the Twitter id and not the Twitter @handle (username) because while the @handle can be changed, the id will always remain the same. Conversions from Twitter id to @handle can be done by means of an online tool like [Tweeterid] (http:\/\/tweeterid.com\/) or simply through the Twitter API itself. The mapping provided here again facilitates additional metadata enrichment.\n\n### movies.dat\n\nContains the items (i.e., movies) that were rated in the tweets, together with their genre metadata in the following format: movie_id::movie_title (movie_year)::genre|genre|genre. For example:\n\n0110912::Pulp Fiction (1994)::Crime|Thriller\n\nThe file is UTF-8 encoded to deal with the many foreign movie titles contained in tweets.\n\n### ratings.dat\n\nIn this file the extracted ratings are stored in the following format: user_id::movie_id::rating::rating_timestamp. For example:\n\n14927::0110912::9::1375657563\n\nThe ratings contained in the tweets are scaled from 0 to 10, as is the norm on the IMDb platform. To prevent information loss we have chosen to not down-scale this rating value, so all rating values of this dataset are contained in the interval [0,10].","d28cac4f":"*All data is ready for study.*","8e9dcd22":"*We can see that at the beginning of the film industry name of films was quite long further name averaged, in fact, the data at the beginning of the film industry is very small and now the data and films are many so the length of the names in the modern period averaged.*","bfd8cbe4":"*From the boxplot distribution, we can see that the median estimates are in the region of 3.5 , there are some emissions in the estimate less than 0.5. Apparently, someone did not like the movie at all.*"}}