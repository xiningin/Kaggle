{"cell_type":{"26f06095":"code","716ad7dd":"code","65da14f8":"code","cc693407":"code","1d0a63bc":"code","117c512f":"code","bff60199":"code","70fe276d":"code","c4989346":"code","32b74d07":"code","0d0016a0":"code","7cd89f2f":"code","ea89c3f7":"code","ed0a2c1d":"code","0db83005":"code","71aa00bb":"code","92a51bb0":"code","6957828c":"code","ba6a14d0":"code","27d1e845":"code","c0c6d597":"code","8205bdb7":"code","89c3346e":"code","9a41443f":"code","13bd56b8":"code","7e63b439":"code","f2fb349d":"code","8eadbeb6":"code","99ddd915":"code","5cfb4544":"code","13a8b8c6":"code","af874599":"code","11a7043a":"code","bd23e3e8":"code","ea55b9d0":"code","0d7c6e45":"code","0c416fac":"code","d1eb88a8":"code","ff30948c":"code","200aae83":"code","d252ecef":"code","edce6dee":"code","a289eaa0":"code","1116e0e0":"code","fe9e1754":"code","f4ab7b54":"code","b1a542c5":"code","9b719f34":"code","77e013c1":"code","ffecc127":"code","90ceb9de":"code","9f0ed403":"code","bb0ddcc4":"code","1d19935d":"code","c8a6c8d4":"code","530d7754":"code","a4267fd4":"code","2d5faffa":"code","9783bb8d":"code","718867a1":"code","097d7826":"code","de8512ba":"code","ad62fdc8":"code","436b9c1a":"code","ac4f3918":"code","0013470c":"code","258fdae3":"code","b1bb6b37":"code","deeabf69":"code","af77a800":"code","1a25b5ff":"code","e7445d1a":"code","26115274":"code","b6ebd1f6":"markdown","1b84bd59":"markdown","316813d2":"markdown","0071ae1e":"markdown","76851715":"markdown","e7e2b906":"markdown","0e0d67d5":"markdown","9aad6282":"markdown","c38d7032":"markdown","dd94faf8":"markdown","1223f0a5":"markdown","0dbca57c":"markdown","43f95fe6":"markdown","e56f35d2":"markdown","f9ef092c":"markdown","d1ab23ee":"markdown"},"source":{"26f06095":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom datetime import datetime as dt\n\n# For Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# To Scale our data\nfrom sklearn.preprocessing import scale\n\n# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.datasets import fetch_mldata\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","716ad7dd":"mnist = pd.read_csv(\"..\/input\/train.csv\",  sep = ',',encoding = \"ISO-8859-1\", header= 0)\nholdout = pd.read_csv(\"..\/input\/test.csv\",  sep = ',',encoding = \"ISO-8859-1\", header= 0)\n\nprint(\"Dimensions of train: {}\".format(mnist.shape))\nprint(\"Dimensions of test: {}\".format(holdout.shape))","65da14f8":"mnist.head()","cc693407":"# Checking for total count and percentage of null values in all columns of the dataframe.\n\ntotal = pd.DataFrame(mnist.isnull().sum().sort_values(ascending=False), columns=['Total'])\npercentage = pd.DataFrame(round(100*(mnist.isnull().sum()\/mnist.shape[0]),2).sort_values(ascending=False)\\\n                          ,columns=['Percentage'])\npd.concat([total, percentage], axis = 1).head()","1d0a63bc":"from sklearn.model_selection import train_test_split","117c512f":"# Putting feature variable to X\nX = mnist.drop(['label'], axis=1)\n\nX.head()","bff60199":"# Putting response variable to y\ny = mnist['label']\n\ny.head()","70fe276d":"from sklearn.preprocessing import StandardScaler","c4989346":"scaler = StandardScaler()\n\nX = scaler.fit_transform(X)\n\npd.DataFrame(X).head()","32b74d07":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","0d0016a0":"pd.DataFrame(X_test).head()","7cd89f2f":"import numpy as np\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,4))\nfor index, (image, label) in enumerate(zip(X_train[0:5], y_train[0:5])):\n plt.subplot(1, 5, index + 1)\n plt.imshow(np.reshape(image, (28,28)), cmap=plt.cm.gray)\n plt.title('Training: %i\\n' % label, fontsize = 20)","ea89c3f7":"X_train.shape\n# We have 30 variables after creating our dummy variables for our categories","ed0a2c1d":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca = PCA(svd_solver='randomized', random_state=42)","0db83005":"#Doing the PCA on the train data\npca.fit(X_train)","71aa00bb":"pca.n_components_","92a51bb0":"pca.components_","6957828c":"colnames = list(pd.DataFrame(X_train).columns)\npcs_df = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'Feature':colnames})\npcs_df.head()","ba6a14d0":"%matplotlib inline\nfig = plt.figure(figsize = (10,10))\nplt.scatter(pcs_df.PC1, pcs_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pcs_df.Feature):\n    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\nplt.tight_layout()\nplt.show()","27d1e845":"pd.options.display.float_format = '{:.2f}'.format\npca.explained_variance_ratio_","c0c6d597":"#Making the screeplot - plotting the cumulative variance against the number of components\n%matplotlib inline\nfig = plt.figure(figsize = (12,8))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","8205bdb7":"X_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)","89c3346e":"from sklearn.linear_model import LogisticRegression","9a41443f":"logisticRegr = LogisticRegression(solver = 'lbfgs')","13bd56b8":"logisticRegr.fit(X_train_pca, y_train)","7e63b439":"logisticRegr.predict(X_train_pca[0:10])","f2fb349d":"logisticRegr.predict(X_train_pca)","8eadbeb6":"score = logisticRegr.score(X_train_pca, y_train)\nprint(score)","99ddd915":"logisticRegr.predict(X_test_pca)","5cfb4544":"score = logisticRegr.score(X_test_pca, y_test)\nprint(score)","13a8b8c6":"X_train.shape\n# We have 30 variables after creating our dummy variables for our categories","af874599":"#Improting the PCA module\nfrom sklearn.decomposition import PCA\npca_last = PCA(0.90)","11a7043a":"#Doing the PCA on the train data\npca_last.fit(X_train)","bd23e3e8":"pca_last.n_components_","ea55b9d0":"pca_last.components_","0d7c6e45":"colnames = list(pd.DataFrame(X_train).columns)\npcs_df = pd.DataFrame({'PC1':pca_last.components_[0],'PC2':pca_last.components_[1], 'Feature':colnames})\npcs_df.head()","0c416fac":"%matplotlib inline\nfig = plt.figure(figsize = (10,10))\nplt.scatter(pcs_df.PC1, pcs_df.PC2)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nfor i, txt in enumerate(pcs_df.Feature):\n    plt.annotate(txt, (pcs_df.PC1[i],pcs_df.PC2[i]))\nplt.tight_layout()\nplt.show()","d1eb88a8":"X_train_pca = pca_last.transform(X_train)\nX_test_pca = pca_last.transform(X_test)","ff30948c":"from sklearn.linear_model import LogisticRegression","200aae83":"logisticRegr = LogisticRegression(solver = 'lbfgs')","d252ecef":"model_pca = logisticRegr.fit(X_train_pca, y_train)\nmodel_pca","edce6dee":"logisticRegr.predict(X_train_pca[0:10])","a289eaa0":"predictions = logisticRegr.predict(X_train_pca)\npredictions","1116e0e0":"score = logisticRegr.score(X_train_pca, y_train)\nprint(score)","fe9e1754":"%matplotlib inline\nfig = plt.figure(figsize = (8,8))\nplt.scatter(X_train_pca[:,0], X_train_pca[:,1], c = y_train)\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.tight_layout()\nplt.show()","f4ab7b54":"%matplotlib notebook\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure(figsize=(8,8))\nax = Axes3D(fig)\nax = plt.axes(projection='3d')\nax.scatter(X_train_pca[:,2], X_train_pca[:,0], X_train_pca[:,1],zdir='z', s=20, marker = 'o', c=y_train)\nax.set_xlabel('Principal Component 1')\nax.set_ylabel('Principal Component 2')\nax.set_zlabel('Principal Component 3')\nplt.tight_layout()\nplt.show()","b1a542c5":"import statsmodels.api as sm\n# Logistic regression model\nlogpca = sm.GLM(y_train,(sm.add_constant(X_train_pca)), family = sm.families.Binomial())\nlogpca.fit().summary()","9b719f34":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import metrics","77e013c1":"cm = metrics.confusion_matrix(y_train, predictions)\nprint(cm)","ffecc127":"import matplotlib.pyplot as plt\nfrom mlxtend.plotting import plot_confusion_matrix\n\nfig, ax = plot_confusion_matrix(conf_mat=cm)\nplt.show()","90ceb9de":"print(metrics.accuracy_score(y_train, predictions))","9f0ed403":"recall = np.diag(cm) \/ np.sum(cm, axis = 1)\nrecall","bb0ddcc4":"from sklearn.metrics import precision_score, recall_score","1d19935d":"recall_score(y_train, predictions,average='macro')","c8a6c8d4":"precision = np.diag(cm) \/ np.sum(cm, axis = 0)\nprecision","530d7754":"precision_score(y_train, predictions,average='macro')","a4267fd4":"from sklearn.metrics import precision_recall_fscore_support as score\n\nprecision, recall, fscore, support = score(y_train, predictions)\n\nprint('precision: {}'.format(precision))\nprint('recall: {}'.format(recall))\nprint('fscore: {}'.format(fscore))\nprint('support: {}'.format(support))","2d5faffa":"from sklearn.metrics import classification_report\nprint(classification_report(y_train, predictions))","9783bb8d":"predict_test = logisticRegr.predict(X_test_pca)\npredict_test","718867a1":"score = logisticRegr.score(X_test_pca, y_test)\nprint(score)","097d7826":"print(metrics.accuracy_score(y_test, predict_test))","de8512ba":"recall_score(y_test, predict_test,average='macro')","ad62fdc8":"precision_score(y_test, predict_test,average='macro')","436b9c1a":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, predict_test))","ac4f3918":"holdout.head()","0013470c":"holdout.shape","258fdae3":"# Checking for total count and percentage of null values in all columns of the dataframe.\n\ntotal = pd.DataFrame(holdout.isnull().sum().sort_values(ascending=False), columns=['Total'])\npercentage = pd.DataFrame(round(100*(holdout.isnull().sum()\/holdout.shape[0]),2).sort_values(ascending=False)\\\n                          ,columns=['Percentage'])\npd.concat([total, percentage], axis = 1).head()","b1bb6b37":"from sklearn.preprocessing import StandardScaler","deeabf69":"holdout = scaler.transform(holdout)\n\npd.DataFrame(holdout).head()","af77a800":"holdout_pca = pca_last.transform(holdout)\npd.DataFrame(holdout_pca).head()","1a25b5ff":"predict_holdout = logisticRegr.predict(holdout_pca)\npredict_holdout","e7445d1a":"holdout_ids = np.arange(1,holdout.shape[0]+1)\nsubmission_df = {\"ImageId\": holdout_ids,\"Label\": predict_holdout}\nsubmission = pd.DataFrame(submission_df)\nsubmission.head()","26115274":"submission.to_csv(\"submission.csv\",index=False)","b6ebd1f6":"## Predictions on the test sample","1b84bd59":"## Calculating precision","316813d2":"**Let's plot the principal components and try to make sense of them**\n    **- We'll plot original features on the first 2 principal components as axe**","0071ae1e":"## Preparing Holdout Data","76851715":"## Confusion Matrix (Digits Dataset)","e7e2b906":"## Calculating accuracy","0e0d67d5":"## Calculating recall","9aad6282":"## overall measures of precision","c38d7032":"**Feature Scaling**","dd94faf8":"**Showing the Images and Labels (MNIST)**","1223f0a5":"**Test-Train Split**","0dbca57c":"## Confusion Matrix in Seaborn","43f95fe6":"## Feature Scaling","e56f35d2":"## Creating Kaggle submission file","f9ef092c":"***Looking at the screeplot to assess the number of needed principal components***","d1ab23ee":"**PCA on the data**"}}