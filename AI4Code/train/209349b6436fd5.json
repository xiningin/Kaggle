{"cell_type":{"199ad654":"code","f42bbabd":"code","7194ca86":"code","e921a219":"code","b22ec302":"code","da43c7d2":"code","2cab9f4c":"code","13b9f715":"code","f396c7d8":"code","9c2552ea":"code","09f9ea55":"code","453fa37c":"code","a52f7c56":"code","20f534ec":"code","217f064b":"code","e1ad0077":"code","e5369dac":"code","c7bd9e0b":"code","4401066a":"code","1c4b69a9":"code","e87ed5f3":"code","94b23bf1":"code","2da5c7a1":"code","73b73a6e":"code","805717a4":"code","38035d1c":"code","fe85f44c":"code","0907480a":"code","5069d6ba":"code","d882ab20":"code","7935775f":"code","ca769a30":"code","99a7fff2":"code","fc3fa812":"code","cf1166cd":"code","1ff1854f":"code","d07a38cb":"code","e512c701":"code","8d7834cf":"markdown","c190c085":"markdown","87e8f1b9":"markdown","8e6b37d1":"markdown","76345e4d":"markdown","25c2a2fc":"markdown","398952a1":"markdown","cd4a1e6f":"markdown","a2ee4d41":"markdown","cb8c6c19":"markdown","69e6731e":"markdown","9e7fc773":"markdown","651cbdfb":"markdown","d8034a4d":"markdown","f936d4af":"markdown","e98d3397":"markdown","dc434076":"markdown","db07d0df":"markdown","d9f3c7f5":"markdown","4c7d8731":"markdown","4899cf80":"markdown","9f8d168d":"markdown","0b7efe68":"markdown","6b8645c8":"markdown"},"source":{"199ad654":"#Install latest version of the package as  the defualt version is not working fine\n!pip install seaborn==0.11.0","f42bbabd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os, gc\nprint(os.listdir(\"..\/input\"))\n\nimport seaborn as sns\nsns.set_theme(style=\"darkgrid\")\n\nimport matplotlib.pyplot as plt\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\n#setting for plot fonts \nSMALL_SIZE = 14\nMEDIUM_SIZE = 16\nBIGGER_SIZE = 18\n\nplt.rc('font', size=SMALL_SIZE)          # controls default text sizes\nplt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\nplt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\nplt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\nplt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\nplt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n","7194ca86":"RANDOM_STATE = 42\nDEBUG_MODE = False  # Load fewer samples to save time for quick testing\nTARGET = 'isFraud'\nSHOW_GRAPHS = True","e921a219":"%%time\n\n# Load fewer samples to save time for quick testing\nif DEBUG_MODE:\n    nrows = 50000\nelse:\n    nrows = None\n        \ndata_path = '\/kaggle\/input\/ieee-fraud-detection\/'\ntrain_identity = pd.read_csv(os.path.join(data_path, 'train_identity.csv'))\ntrain_transaction = pd.read_csv(os.path.join(data_path, 'train_transaction.csv'), nrows = nrows)\ntest_identity = pd.read_csv(os.path.join(data_path, 'test_identity.csv'))\ntest_transaction =pd.read_csv(os.path.join(data_path, 'test_transaction.csv'), nrows = nrows)\nprint('Train Identity Data - rows:', train_identity.shape[0], \n      'columns:', train_identity.shape[1])\nprint('Train Transaction Data - rows:', train_transaction.shape[0], \n      'columns:', train_transaction.shape[1])\nprint('Test Identity Data - rows:', test_identity.shape[0], \n      'columns:', test_identity.shape[1])\nprint('Test Transaction Data - rows:', test_transaction.shape[0], \n      'columns:', test_transaction.shape[1])","b22ec302":"train_transaction.head()","da43c7d2":"\ndef column_properties(df):\n    columns_prop = pd.DataFrame()\n    columns_prop['column'] = df.columns.tolist()\n    columns_prop['count_non_null'] = df.count().values\n    columns_prop['count_null'] = df.isnull().sum().values\n    columns_prop['perc_null'] = columns_prop['count_null'] * 100 \/ df.shape[0]\n\n    #using df.nunique() is memory intensive and slow resulting in kernal death\n    unique_list = []\n    for col in df.columns.tolist():\n        unique_list.append(df[col].value_counts().shape[0])\n    columns_prop['count_unique'] =  unique_list\n    \n    columns_prop['dtype'] = df.dtypes.values\n    columns_prop.set_index('column', inplace = True)\n    return columns_prop\n","2cab9f4c":"column_properties(train_transaction).T","13b9f715":"train_identity.head()","f396c7d8":"test_identity.head()","9c2552ea":"# the columns name for training set and test are not same,we will correct columns names of test set using traning column name\nidentity_col_names =  train_identity.columns.tolist()\ntest_identity.columns = identity_col_names\nprint(test_identity.columns.tolist())","09f9ea55":"test_identity.head()","453fa37c":"column_properties(train_identity).T","a52f7c56":"%%time\ntrain = pd.merge(train_transaction, train_identity, on= 'TransactionID', how = 'left')\ntest = pd.merge(test_transaction, test_identity, on= 'TransactionID', how = 'left')\ndel train_transaction, train_identity, test_transaction, test_identity\ngc.collect()\ntrain.shape","20f534ec":"%%time\ncol_prop_trn = column_properties(train)\ncol_prop_tst = column_properties(test)\n\n#Find number of  columns with more that 50% missing values\nmissing_cols_50 = col_prop_trn[col_prop_trn['perc_null'] > 50]\nprint('Count of columns with more than 50% missing values {}'.format(missing_cols_50.shape[0]))\nmissing_cols_50.sort_values(by = 'perc_null', ascending = False).T\n","217f064b":"\nplt.figure(figsize= (12,6))\ns = sns.histplot(col_prop_trn['perc_null'])\ns.set_title('Distribution of missing values by percentage ')\ns.set(xlabel = '% Missing Values')\nplt.show()","e1ad0077":"def get_target_dist(train):\n    df = train.groupby(TARGET).agg({TARGET:['count'] })\n    df.columns = ['count']\n\n    df['percent'] = df['count'] *100 \/ df['count'].sum()\n    df['percent'] = df['percent'].round(2)\n    return df","e5369dac":"target_dist = get_target_dist(train)\ntarget_dist","c7bd9e0b":"def plot_taget_dist(target_df):\n    fig, (ax1, ax2)  =  plt.subplots(ncols = 2, figsize = (12, 6))\n    s = sns.barplot(ax = ax1, x = target_df.index, y = \"count\", data = target_df)\n    s.set_title('Target Value Count')\n    s = sns.barplot(ax = ax2, x = target_df.index, y = \"percent\", data =target_df)\n    s.set_title('Target Value Distribution')","4401066a":"plot_taget_dist(target_dist)","1c4b69a9":"\n\n\n\ncat_cols = ['DeviceType', 'DeviceInfo', 'ProductCD', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain']\ncat_cols +=  ['M' + str(i) for i in range(1,10)]\ncat_cols += ['card' + str(i) for i in range(1,7)]\ncat_cols += ['id_' + str(i) for i in range(12,39)]\ncolumn_properties(train[cat_cols]).T\n","e87ed5f3":"train[cat_cols].head()","94b23bf1":"%%time\n\n\nall_cols = train.columns.tolist()\nnum_cols = [x for x in all_cols if x not in cat_cols]\n\nnum_cols.remove('TransactionID')\nnum_cols.remove(TARGET)\ntrain[num_cols].describe()","2da5c7a1":"def plot_categorical_data(col, data, top_n = 10, display_data = False ):\n    \n    # Prpare a dataframe for count and postive classs percent givel colums\n    df_data = data[[col, TARGET]].copy()    \n    df = df_data.groupby(col).agg({col:['count'], TARGET:['sum']})\n    df.columns = ['count', 'fraud_count']\n\n    df['fraud_perc'] = df['fraud_count'] * 100 \/ df['count']\n    df['fraude_perc'] = df['fraud_perc'].round(2)\n    \n#    % missing values in the columns to be displayed in title\n    null_perc = (df_data.shape[0]- df['count'].sum())  \/ df_data.shape[0]\n\n    width = 18\n    height = 6\n\n#   select only top n categories\n    df_disp = df.sort_values(by ='count', ascending= False).head(top_n )\n\n    fig, (ax1, ax2)  =  plt.subplots(ncols = 2, figsize = (width,height))\n    fig.suptitle('Plots for {} (Missing Values: {:.2%})'.format(col, null_perc))\n    \n#   Display Sort order should be by descending value of count\n    plot_order = df_disp.sort_values(by='count', ascending=False).index.values\n\n#   Display Bar chart for frequency count of top_n categories\n    s = sns.barplot(ax = ax1,  y = df_disp.index, x = df_disp['count'], order=plot_order, orient = 'h'  )\n    s.set_title('Count for {}'.format(col))\n    \n#   Display Bar chart for perecnt of positive class for top categories\n    s = sns.barplot(ax = ax2,  y = df_disp.index, x = df_disp['fraud_perc'], order=plot_order , orient = 'h'    )\n    s.set(xlabel='Fraud Percent')\n    s.set_title('% Fraud {}'.format(col))\n    plt.show()\n    if display_data:\n        return df","73b73a6e":"if SHOW_GRAPHS:\n    for col in cat_cols:    \n        plot_categorical_data(col, train, top_n = 10, display_data = False )","805717a4":"def plot_numeric_data(df, col, target_col, remove_outliers = False):\n   \n    df = df[[col, target_col]].copy()\n    df.dropna(subset=[col], inplace =True)\n    \n    #Remove Outliers: keep only the ones that are within +3 to -3 standard deviations in the column \n    if remove_outliers:       \n       \n        df = df[np.abs(df[col]-df[col].mean()) <= (3*df[col].std())]\n       \n\n\n    fig, (ax1, ax2,ax3)  =  plt.subplots(ncols = 3, figsize = (24,4))\n    fig.suptitle('Plots for {}'.format(col))\n    \n    #Display Density Plot\n    sns.distplot(df[col], color = 'b',  kde = True ,  ax = ax1 )\n    plt.ylabel('Density')\n\n\n    # Display Box Plot for feature\n    sns.boxplot(x = col , data = df,ax = ax2)\n   \n    #Display Density Plot for Fraud Vs NotFraud\n    sns.distplot(df[df[target_col] == 0][col], color = 'b', label = 'NotFraud',ax = ax3)\n    sns.distplot(df[df[target_col] == 1][col], color = 'r', label = 'Fraud',ax = ax3)\n    plt.legend(loc = 'best')\n    plt.ylabel('Density NotFraud vs Fraud')\n\n    plt.show()","38035d1c":"%%time\n#Get subset of numerical columns only\nnum_cols_filtered   = col_prop_trn[col_prop_trn.index.isin(num_cols)]\n\n#only select those coluumns which a missing value % less than a threshold(30%)\nnum_cols_filtered = num_cols_filtered[num_cols_filtered.perc_null < 30]\n\n# Select columns with >= 100 distinct values\nnum_cols_filtered = num_cols_filtered[num_cols_filtered.count_unique >= 100]\n\nprint('Number of features:{}'.format(len(num_cols_filtered.index.tolist())))\n\n","fe85f44c":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nif SHOW_GRAPHS:\n    for col in num_cols_filtered.index.tolist():  \n       plot_numeric_data(df= train, col= col, target_col = TARGET, remove_outliers = True)","0907480a":"# Concatenate the tranining and test dataset by appending\ndata_all = train.append(test, ignore_index = True, sort=False)\n\n# Do ordinal encoding for categorical features\nfor col in cat_cols:\n    data_all[col], uniques = pd.factorize(data_all[col])\n    #the factorize sets null values to -1, so convert them back to null, as we want LGB to handle null values\n    data_all[col] = data_all[col].replace(-1, np.nan)\n    \n#Create submission pandas dataframe \nsub = pd.DataFrame()\nsub['TransactionID'] = test.TransactionID\n\n# free the memory which is not required as it can exceed the physical ram \ndel train, test\ngc.collect()\n","5069d6ba":"from sklearn.model_selection import train_test_split\n\n#For test set target value will be null\nX_train =  data_all[data_all[TARGET].notnull()]\nX_test  =  data_all[data_all[TARGET].isnull()]\ndel data_all\ngc.collect()\n\n#get the labels for traning set\ny_train = X_train[TARGET]\n\n# Remove ID and TARGET column from train and test set\nX_train = X_train.drop(['TransactionID', TARGET], axis = 1)\nX_test = X_test.drop(['TransactionID',   TARGET], axis = 1)\n\n# Split the training set into training and validation set. \n# We will use first 80% of data as traningg set and last 20% as validation set.\n# Since the data is sorted in time according to transaction timestamp, we should not use random split.\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.2, shuffle=False, \n                                                      random_state = RANDOM_STATE)\n\nprint('Train shape{} Valid Shape{}, Test Shape {}'.format(X_train.shape, X_valid.shape, X_test.shape))","d882ab20":"%%time\nimport lightgbm as lgb\n\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_valid  = lgb.Dataset(X_valid, y_valid)\nearly_stopping_rounds = 200\nlgb_results = {}\n\nparams = {}\n\nparams['learning_rate'] = 0.06\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'binary'\nparams['seed'] =  RANDOM_STATE\nparams['metric'] =    'auc'\nparams['is_unbalance'] = True\nparams['bagging_fraction'] = 0.8\nparams['bagging_freq'] = 1\nparams['feature_fraction'] = 0.8\nparams['max_bin'] = 127\n\nmodel = lgb.train(params,\n                  lgb_train,\n                  num_boost_round = 10000,\n                  valid_sets =  [lgb_train,lgb_valid],\n                  early_stopping_rounds = early_stopping_rounds,                    \n                  categorical_feature = cat_cols,\n                  evals_result = lgb_results,\n                  verbose_eval = 100\n                   )\n\n\ny_pred_test = model.predict(X_test)\nsub['isFraud'] = y_pred_test\nsub.to_csv('lgb_sub.csv', index=False)","7935775f":"from sklearn.metrics import accuracy_score, roc_auc_score,f1_score, precision_score, recall_score,confusion_matrix\n\ndef validation_results(y_valid, y_prob, verbose = True):   \n    scores = {}                      \n    y_pred_class =  [0  if x < 0.5 else 1 for x in y_prob]\n    scores['val_accuracy']  = accuracy_score(y_valid, y_pred_class)\n    scores['val_auc']       = roc_auc_score(y_valid, y_prob)\n    scores['val_f1']        =   f1_score(y_valid, y_pred_class, average = 'binary')\n    scores['val_precision'] = precision_score(y_valid, y_pred_class)\n    scores['val_recall']    = recall_score(y_valid, y_pred_class)\n    \n    cm = confusion_matrix(y_valid, y_pred_class)\n    cm_df = pd.DataFrame(cm, columns=np.unique(y_valid), index = np.unique(y_valid))\n    if verbose:\n        print('\\nValidation Accuracy      {:0.5f}'.format( scores['val_accuracy'] ))\n        print('Validation   AUC         {:0.5f}'.format( scores['val_auc']   ))\n        print('Validation Precision     {:0.5f}'.format(scores['val_precision']))\n        print('Validation Recall        {:0.5f}'.format(scores['val_recall']))\n        print('Validation  F1           %0.5f' %scores['val_f1'] )\n    return scores , cm_df","ca769a30":"y_prob = model.predict(X_valid)\nresults, cm_df  = validation_results(y_valid, y_prob, verbose = True)\n","99a7fff2":"\ncm_df.index.name = 'Actual'\ncm_df.columns.name = 'Predicted'\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(cm_df, cmap=\"Blues\", annot=True,annot_kws={\"size\": 16}, fmt='g')# font size\nplt.show()","fc3fa812":"\ndef plot_lgb_scores(lgb_results):\n    train_res = lgb_results['training']['auc']\n    valid_res = lgb_results['valid_1']['auc']\n    ntrees = range(1, len(train_res) + 1)\n\n    plt.figure(figsize = (12, 6))\n    plt.plot(ntrees, train_res , 'b', label = 'Training')\n    plt.plot(ntrees, valid_res, 'r', label = 'Validation')\n    plt.xlabel('Number of Trees', fontsize = 14)\n    plt.ylabel('AUC Score', fontsize = 14)\n    plt.legend(fontsize = 14)\n    plt.show()\n    \n\n    \n","cf1166cd":"plot_lgb_scores(lgb_results)","1ff1854f":"def plot_feature_imp(model, top_n = 30):\n    feature_imp = pd.DataFrame()\n    feature_imp['feature'] = model.feature_name()\n    feature_imp['importance']  = model.feature_importance()\n    feature_imp = feature_imp.sort_values(['importance'], ascending = False)\n    feature_imp_disp = feature_imp.head(top_n)\n    plt.figure(figsize=(10, 12))\n    sns.barplot(x=\"importance\", y=\"feature\", data=feature_imp_disp)\n    plt.title('LightGBM Features')\n    plt.show() \n#     return feature_imp","d07a38cb":"plot_feature_imp(model, top_n = 20)","e512c701":"y_prob_test = model.predict(X_test)\nsub['isFraud'] = y_prob_test\nsub.to_csv('lgb_sub.csv', index=False)\nsub.head()","8d7834cf":"## Read Data\nhttps:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203#latest-586800\n\n","c190c085":"## About the Project","87e8f1b9":"### Display Confusion Matrix","8e6b37d1":"## Merge Data\nMake a join between transaction data and identity data which are connected by key 'TransactionID","76345e4d":"## CONSTANTS","25c2a2fc":"## Numeric Columns\n* From list of all columns remove categorical columns, Target Value, and ID, this will give us numerical columns\n* Display the statistical properties of numeric columns","398952a1":"## EDA Categorical Features\nEDA Categorical Features\nPlot top 10 categories for category counts and percentage of fraud transactions. The header of each plot will also show percentage of missing values\n\n1. DeviceType: Mobile devices has higher fraud rate at 10.17 % compared to desktop at 6.52 % though 76% of data is missing for this category.\n1. DeviceInfo: Product C has high rate of fraud at 11.6% compared to 3.5% avergae rate for whole training dataset.\n1. addr2: Category 65 though has only 82 records, but the fraud rate is abnormally high at 53%.\n1. P_emaildomain: Purchaser email domain outlook.com has a relatively high fraud rate at 9.46%\n1. R_emaildomain: Recipient email domain outlook.com has a relatively high fraud rate at 16.51% followed by gmail.com at 11.92%. Gmail is top category with count of 57K.\n1. card4:Discover branded cards have highest fraud rate at 7.73% and american express has lowest at 2.87%\n1. card6: credit cards have a higer rate of fraud at 6.68% compared to debit card at 2.43%\n1. id_33: This feature represents screen resolution and there is one odd resolution 2208 X 1242 which has highest fraud rate at 9.84 and count of 4900. Quick google search reveals that this resolution corresponds to iPhone 6","cd4a1e6f":"## Create Test, Train and Validation sets\n* From combined dataset split the training and test datasets and seperate the target and features\n* Split the training set into training and validation set. We will use first 80% of data as training set and last 20% as validation set.\n* Since the data is sorted in time according to transaction timestamp, we should not use random split.","a2ee4d41":"## Data Pre-Processing\n* Concatenate the tranining and test dataset by appending . This is done so that we can apply pre-processing steps to combined set.\n* Convert the categorcal features from string to int using ordinal encoding. For example convert ['A', 'B'. 'C'] to [1,2,3]\n* Create a dataframe sub for submission of test scores, we will later fill it with predictions on test set","cb8c6c19":"## Summary\n* Perform EDA for numeric and categorical features.\n* Do basic data preprocessing steps to convert categorical variables to integers using ordinal encoding.\n* Train on a LightGBM model with 80% data and 20% validation data without any feature engineering using all features.\n* Use metric AUC to evaluate the performance of model.\n* Our basic models achieved AUC of 0.906929 on public test and AUC of  0.882078 on private test.\n","69e6731e":"* x-axis represnt the perecntage of of missing values\n* y-axis represent the count of such columns\n1. There are around 115 columns which have missing value in range of 0 to 10 %\n1. There are around 125 columns which have missing value 70 to 80 %","9e7fc773":"### Plot Feature Importance\nDisplay top 20 features ","651cbdfb":"## EDA Numeric Features\n\n* Since there are many columns we will restrict to features having less than 30% null values\n* We will also filter those feature aving less than 100 unique values\n","d8034a4d":"### Display Training vs Validation scores","f936d4af":"> ## Target Value\n3.5 % of transactions are fraud which indicates that its imbalanced dataset","e98d3397":"## Import Libraries","dc434076":"## Categorical Columns\nCreate list of categorical columns based on decsription below\n<br>https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/101203#latest-586800","db07d0df":"## Missing Values\nThere are 214 columns with more than 50% of missing values. Display all such columns and their properties\n\n","d9f3c7f5":"### Transaction Data\n\n","4c7d8731":"### Identity Data\n* Variables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions.\n* As we can see that the columns name for training set and test are not same,we will correct columns names of test set using traning column name","4899cf80":"## Display Results","9f8d168d":"## Train LightGBM Model\n* LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient https:\/\/lightgbm.readthedocs.io\/en\/latest\/Features.html\n* Train on first 80% of dataset and evaluate on next 20 % as data is sorted in time\n* Set 'is_unbalance' = True to handle the unbalanced nature of dataset\n* Set categorical_feature parameter with list  of categorical columns. LightGBM can handle them efficiently without need to to one-hot-encoding\n* No imputation of missing values is necessary as LightGBM can use optimized strategies automatically\n* This is basic model, no feature selection or feature engineering would be done. We will train using all features","0b7efe68":"### Predict on Test Set \nAlso write the results as csv file","6b8645c8":"* Vesta Corporation provided the dataset for this competition. Vesta Corporation is the forerunner in guaranteed e-commerce payment solutions. Founded in 1995.\n \n* In this competition, the aim is to benchmark machine learning models on a challenging large-scale dataset. \n* The data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features. \n* The machine learning model will alert the fraudulent transaction for millions of people around the world, helping hundreds of thousands of businesses reduce their fraud loss and increase their revenue. \n* The training dataset consists of more than 400 features and 5.9 Million samples. This is supervised binary classification problem and goal is to predict if a credit card transaction is Fraud based on input features mentioned below\n\n**Evaluation**\n* The model is evaluated on AUC ROC score. The notebook will produce an output csv file with TransactionID and predicted probabilties on test set,  which will be automatically evaluted by Kaggle.\n\n### Transaction Table \n* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n* TransactionAMT: transaction payment amount in USD\n* ProductCD: product code, the product for each transaction\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n* addr: address\n* dist: distance\n* P_ and (R__) emaildomain: purchaser and recipient email domain\n* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n* D1-D15: timedelta, such as days between previous transaction, etc.\n* M1-M9: match, such as names on card and address, etc.\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n\n<br>  **Categorical Features:**\n* ProductCD\n* card1 - card6\n* addr1, addr2\n* P_emaildomain\n* R_emaildomain\n* M1 - M9\n\n### Identity Table \n* Variables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions.\n* They're collected by Vesta\u2019s fraud protection system and digital security partners.\n* (The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n<br> **Categorical Features:**\n* DeviceType\n* DeviceInfo\n* id_12 - id_38\n\n"}}