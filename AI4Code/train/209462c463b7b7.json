{"cell_type":{"0843c14c":"code","cb70def5":"code","043e0f96":"code","28648098":"code","390d278d":"code","d0f630f3":"code","3ee18a06":"code","5657400e":"code","aafef534":"code","91ec6cb5":"code","68cb99b0":"code","3bf40ad1":"code","ca8544b5":"code","1fa3b063":"code","d7ed6f25":"code","8a6516dd":"code","63809bd4":"code","518ccf85":"code","265b8e64":"code","57d90ff2":"markdown","a7823424":"markdown","29c29edc":"markdown","f41b5919":"markdown","ff60413a":"markdown","334858ec":"markdown","ef3887df":"markdown"},"source":{"0843c14c":"import os\nimport sys\nimport gc\nimport random\nimport re\nimport string\nfrom tqdm.notebook import tqdm\nimport numpy as np\n\nsys.path.extend(['..\/input\/transformer\/', '..\/input\/sacremoses\/sacremoses-master\/'])\n\nimport pandas as pd\nfrom bs4 import BeautifulSoup\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom scipy.stats import spearmanr\n    \nimport torch\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler, TensorDataset, Dataset\nfrom torch import nn\nfrom torch.nn import Module\nfrom torch.nn import functional as f\n\nimport transformers\nfrom transformers import BertTokenizer, BertConfig, BertModel, XLNetConfig, XLNetModel\nfrom transformers import RobertaConfig, RobertaModel, DistilBertModel, DistilBertConfig\nfrom transformers.tokenization_bert import BasicTokenizer, whitespace_tokenize\nfrom transformers.optimization import AdamW, get_linear_schedule_with_warmup\n\nfrom nltk.corpus import stopwords\n\nstop_word = set(stopwords.words('english'))\nprint(len(stop_word))\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)","cb70def5":"csv_path = '..\/input\/nlp-getting-started\/'\ntrain_path = csv_path + 'train.csv'\ntest_path = csv_path + 'test.csv'\nsubmission_path = csv_path + 'sample_submission.csv'\n\npath_model = '..\/input\/pretrained-bert-models-for-pytorch\/'\nmodel_file = path_model + 'bert-base-uncased\/pytorch_model.bin'\nconfig_file = path_model + 'bert-base-uncased\/bert_config.json'\nvocab_file = path_model + 'bert-base-uncased-vocab.txt'\n\npath_model = '..\/input\/pretrained-bert-models-for-pytorch\/'\nmodel_file_large = path_model + 'bert-large-uncased\/pytorch_model.bin'\nconfig_file_large = path_model + 'bert-large-uncased\/bert_config.json'\nvocab_file_large = path_model + 'bert-large-uncased-vocab.txt'\n\npath_roberta = '..\/input\/roberta-transformers-pytorch\/roberta-base\/'\nconfig_roberta = path_roberta + 'config.json'\nvocab_roberta = path_roberta + 'vocab.json'\nmodel_roberta = path_roberta + 'pytorch_model.bin'\n\npath_distilroberta = '..\/input\/roberta-transformers-pytorch\/distilroberta-base\/'\nconfig_distilroberta = path_distilroberta + 'config.json'\nvocab_distilroberta = path_distilroberta + 'vocab.json'\nmodel_distilroberta = path_distilroberta + 'pytorch_model.bin'\n\npath_xlnet = '..\/input\/xlnet-pretrained-models-pytorch\/'\nconfig_xlnet = path_xlnet + 'xlnet-base-cased-config.json'\nvocab_xlnet = path_model + 'bert-base-uncased-vocab.txt'\nmodel_xlnet = path_xlnet + 'xlnet-base-cased-pytorch_model.bin'\n\nmodel_use = 'bert'\nnum_model = 7","043e0f96":"tokenize = BertTokenizer.from_pretrained(vocab_file, do_lower_case=True, do_basic_tokenize=True)","28648098":"train_csv = pd.read_csv(train_path)\ntrain_csv = train_csv[['id', 'text', 'target']]\ntrain_csv.head(10)","390d278d":"test_csv = pd.read_csv(test_path)\ntest_csv = test_csv[['id', 'text']]\ntest_csv['target'] = 0\ntest_csv.head(10)","d0f630f3":"%%time\ntext_length = test_csv['text'].apply(lambda x: len(tokenize.tokenize(x)))\n\nplt.figure(figsize=(10, 8))\nsns.distplot(text_length)\nprint(f'max lenth of text: {max(text_length)}')","3ee18a06":"submission = pd.read_csv(submission_path)\nsubmission.head()","5657400e":"#clean data\npuncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', '\\xa0', '\\t',\n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', '\\u3000', '\\u202f',\n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', '\u00ab',\n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\nmispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\"}\n\npuncts = puncts + list(string.punctuation)\n\ndef clean_text(x):\n    x = str(x).replace(\"\\n\",\"\")\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x\n\n\ndef clean_numbers(x):\n    x = re.sub('\\d+', ' ', x)\n    return x\n\n\ndef replace_typical_misspell(text):\n    mispellings_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n\n    def replace(match):\n        return mispell_dict[match.group(0)]\n\n    return mispellings_re.sub(replace, text)\n\ndef remove_space(string):\n    string = BeautifulSoup(string).text.strip().lower()\n    string = re.sub(r'((http)\\S+)', 'http', string)\n    string = re.sub(r'\\s+', ' ', string)\n    return string\n\n\ndef clean_data(df, columns: list):\n    \n    for col in columns:\n        df[col] = df[col].apply(lambda x: remove_space(x).lower())        \n        df[col] = df[col].apply(lambda x: replace_typical_misspell(x))\n        df[col] = df[col].apply(lambda x: clean_text(x))\n        \n    return df","aafef534":"%%time\ntest = test_csv.loc[:, ['id', 'text']]\ntrain = train_csv.loc[:, ['text', 'target']]\n\ntest = clean_data(test, ['text'])\ntrain = clean_data(train, ['text'])\ntest.head()","91ec6cb5":"class QueryDataset(Dataset):\n    \n    def __init__(self, data, is_train=True, max_length=512):\n        \n        super(QueryDataset, self).__init__()\n        \n        self.max_length = max_length\n        self.data = data\n        self.is_train = is_train\n        self.tokenizer = BertTokenizer.from_pretrained(vocab_file_large, do_lower_case=True, do_basic_tokenize=True)\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        token_ids = self.get_token_ids(idx)\n        \n        if self.is_train:\n            label = torch.tensor(self.data.loc[idx, 'target'], dtype=torch.float32)\n            return token_ids, label\n        else:\n            return token_ids\n        \n    \n    def get_token_ids(self, idx):\n        \n        token = self.tokenizer.tokenize(self.data.loc[idx, 'text'])\n        \n        max_seq_length = self.max_length - 2        \n        if len(token) > max_seq_length:           \n            token = token[:max_seq_length]                                                             \n                                                         \n        token = ['[CLS]'] + token + ['[SEP]']\n        token_ids_org = self.tokenizer.convert_tokens_to_ids(token)\n       \n        if len(token_ids_org) < self.max_length:\n            token_ids = token_ids_org + [0]*(self.max_length - len(token_ids_org))\n        else:\n            token_ids = token_ids_org[:self.max_length]\n            \n        token_ids = torch.tensor(token_ids)\n        del token_ids_org\n        return token_ids\n                \n    def collate_fn(self, batch):\n                \n        if self.is_train:\n            token_ids = torch.stack([x[0] for x in batch])\n            label = torch.stack([x[1] for x in batch])\n            return token_ids, label\n        else:\n            token_ids = torch.stack([x for x in batch])\n            return token_ids","68cb99b0":"test_dataset = QueryDataset(test, is_train=False)\ntest_ld = DataLoader(test_dataset, batch_size=8,\n                     shuffle=False, num_workers=0, collate_fn=test_dataset.collate_fn)\n\nprint(len(test_ld))","3bf40ad1":"class BertLinear(Module):\n    \n    def __init__(self, model_name, max_length=512, num_class=1):\n        super(BertLinear, self).__init__()\n        \n        if model_name == 'bert':            \n            config = BertConfig.from_json_file(config_file)\n            self.bert = BertModel.from_pretrained(model_file, config=config)\n            \n        elif model_name == 'bert-large':\n            config = BertConfig.from_json_file(config_file_large)\n            self.bert = BertModel.from_pretrained(model_file_large, config=config)\n            \n        elif model_name == 'robert':\n            config = RobertaConfig()\n            self.bert = RobertaModel(config=config)\n            \n        elif model_name == 'distilrobert':\n            config = DistilBertConfig()\n            self.bert = DistilBertModel(config=config)\n            \n        elif model_name == 'xlnet':\n            config = XLNetConfig()\n            self.bert = XLNetModel(config=config)\n                                               \n        self.bert.config.max_position_embeddings=max_length            \n        self.dropout = nn.Dropout(0.2)\n        self.fc = nn.Sequential(nn.ReLU(inplace=True),\n                                nn.Linear(3*config.hidden_size, num_class))\n    \n    def forward(self, input_ids, segment_ids=None):\n        \n        attention_mask = (input_ids > 0).float()\n        segment_ids = torch.zeros_like(input_ids)\n        layer, pooler = self.bert(input_ids=input_ids,\n                                  attention_mask=attention_mask,\n                                  token_type_ids=segment_ids)\n        \n        avg_pool = torch.mean(layer, 1)\n        max_pool, _ = torch.max(layer, 1)\n    \n        pooler = torch.cat((avg_pool, max_pool, pooler), 1)\n        \n        output = self.dropout(pooler)  \n        logits = self.fc(output)\n        \n        return logits\n    \n\ndef load_model(model_name, path_model, load_weight=True):\n    models = []    \n    model = BertLinear(model_name).to(device)\n    \n    if load_weight:\n        for weight in sorted(os.listdir(path_model)):\n            if 'pth' in weight:\n                weight_path = os.path.join(path_model, weight)\n                state = torch.load(weight_path, map_location=lambda storage, loc: storage)\n                models.append(state)\n    else:\n        for i in range(num_model):            \n            models.append(model.state_dict())\n        \n    return models\n\nbase_model = load_model(model_use, path_model='..\/input\/tweet-bert')","ca8544b5":"def loss_fn(pred, expected):\n    return 0.7*f.mse_loss(torch.sigmoid(pred), expected) + 0.3*f.binary_cross_entropy_with_logits(pred, expected)","1fa3b063":"class Trainer(object):\n    \n    def __init__(self, base_model, model_name='bert',\n                 weight_decay=0.1, learning_rate=2e-5):\n        \n        self.learning_rate = learning_rate\n        self.weight_decay = weight_decay\n        \n        self.model_name = model_name\n        self.base_model = base_model\n        self.cretion = loss_fn\n    \n    def train(self, folds, epochs, train, check_number=5):\n        \n        model = BertLinear(self.model_name).to(device)\n        score_val_max = [0]*folds\n        for fold, (train_index, val_index) in enumerate(KFold(n_splits=folds, shuffle=True, random_state=37).split(train)):\n            print(f'fold: {fold}')\n            val_score_max = 0\n        \n            train_df = train.iloc[train_index]\n            train_df.reset_index(inplace=True, drop=True)\n            \n            val_df = train.iloc[val_index]\n            val_df.reset_index(inplace=True, drop=True)\n            \n            model.load_state_dict(self.base_model[fold])\n            \n            optimizer = AdamW(model.parameters(),\n                              lr=self.learning_rate,\n                              weight_decay=self.weight_decay,\n                              correct_bias=False)            \n            \n            train_dataset = QueryDataset(train_df)\n            train_ld = DataLoader(train_dataset, batch_size=8, shuffle=True,\n                                  num_workers=0, collate_fn=train_dataset.collate_fn)\n            \n            val_dataset = QueryDataset(val_df)\n            val_ld = DataLoader(val_dataset, batch_size=8, shuffle=True,\n                                num_workers=0, collate_fn=val_dataset.collate_fn)\n            \n            schedule = get_linear_schedule_with_warmup(optimizer,\n                                                       num_warmup_steps=0.5,\n                                                       num_training_steps=epochs*len(val_ld))\n            \n            del val_dataset, train_dataset, val_df, train_df\n            model.zero_grad()\n            check_score = 0\n            for epoch in range(epochs):\n                print(f'Epoch: {epoch}')\n                train_loss = 0\n                val_loss = 0\n\n                model.train()\n                for token_ids, label in tqdm(train_ld):\n\n                    optimizer.zero_grad()\n                    token_ids, label = token_ids.to(device), label.unsqueeze(1).to(device)\n                    output = model(token_ids)\n                    loss = self.cretion(output, label)\n                    loss.backward()\n\n                    train_loss += loss.item()\n                    optimizer.step()\n                    schedule.step()\n                    del token_ids, label\n                    \n                train_loss = train_loss\/len(train_ld)\n                torch.cuda.empty_cache()\n                gc.collect()\n                \n                # evaluate process\n                model.eval()\n                score_val = 0\n                with torch.no_grad():\n                    for token_ids, label in tqdm(val_ld):\n                        token_ids, label = token_ids.to(device), label.unsqueeze(1).to(device)\n\n                        output = model(token_ids)\n                        loss = self.cretion(output, label)\n                        score_val += torch.sum((torch.sigmoid(output) >= 0.5).float()==label).item()\/output.size(0)\n                        val_loss += loss.item()\n                    \n                    score_val = score_val\/len(val_ld)\n                    val_loss = val_loss\/len(val_ld)             \n                    \n                    \n                print(f'train_loss: {train_loss:.4f}, valid_loss: {val_loss:.4f}, valid_score: {score_val:.4f}')\n                schedule.step(val_loss)\n\n                if score_val >= val_score_max:\n                    score_val_max[fold] = score_val\n                    check_score+=1\n                    print(f'Validation score increased ({val_score_max:.4f} --> {score_val:.4f}). Saving model...')\n                    val_score_max = score_val\n                    check_score = 0\n                    torch.save(model.state_dict(), f'model_fold_{str(fold)}.pth')\n                else:\n                    check_score += 1\n                    print(f'{check_score} epochs of decreasing val_score')\n\n                    if check_score > check_number:\n                        print('Stopping trainning!')                    \n                        break\n                        \n            del optimizer, schedule, train_ld, val_ld\n            torch.cuda.empty_cache()\n            \n            gc.collect()\n        \n        return score_val_max\n            \n    def predict(self, test_ld, submission, threshold=0.5):\n        \n        model = BertLinear(self.model_name).to(device)  \n        list_predict = []\n        \n        for token_ids in tqdm(test_ld):\n            predicts = []\n            for index_model, model_param in enumerate(self.base_model):\n                model.load_state_dict(model_param)\n                \n                model.eval()\n                with torch.no_grad():\n                    token_ids = token_ids.to(device)                    \n                    predict_prob = torch.sigmoid(model(token_ids))\n                    predict = (predict_prob>threshold).float().cpu().numpy()\n                    \n                predicts.append(predict) \n                \n            predicts = np.sum(predicts, axis=0)\n            list_predict.extend(np.where(predicts>3, 1, 0))\n                \n        submission.target = np.array(list_predict).flatten()\n            \n        return submission        ","d7ed6f25":"train_process = Trainer(base_model, model_use)\n\ntest_csv = train_process.predict(test_ld=test_ld, submission=test_csv, threshold=0.5)\ntrain_csv = pd.concat([train_csv, test_csv]).sort_values('id')\n\ntrain_csv.head()","8a6516dd":"train = train_csv.loc[:, ['text', 'target']]\ntrain = clean_data(train, ['text'])\n\ndel train_csv, test_csv","63809bd4":"score_val_max = train_process.train(folds=num_model,\n                                    epochs=3,\n                                    train=train,\n                                    check_number=3)\n\n\ndel base_model, train\n\ntorch.cuda.empty_cache()\ngc.collect()","518ccf85":"base_model1 = load_model(model_use, path_model='.')","265b8e64":"train_process = Trainer(base_model1, model_use)\nsubmission = train_process.predict(test_ld=test_ld, submission=submission, threshold=0.5)\n\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head(20)","57d90ff2":"## Test file","a7823424":"## Train file","29c29edc":"## Preprocess","f41b5919":"## Submission file","ff60413a":"## Create dataset","334858ec":"## Create model","ef3887df":"## Train process"}}