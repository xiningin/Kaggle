{"cell_type":{"d82ed711":"code","d41256a9":"code","febc1f94":"code","823e9fcf":"code","349e3384":"code","0ea2ba7c":"code","3d665162":"code","716d505f":"code","9e8b0472":"code","48ce13e8":"code","a4db85cb":"code","58b20ef0":"code","ed060852":"code","001507b5":"code","48fe4bee":"code","05e936cb":"code","3bd678a8":"code","238f232c":"code","d25c72dd":"code","682bd0ef":"code","fa9c09b0":"code","ef5020f2":"code","2ecc016d":"code","c673af5b":"code","dd9c915a":"code","0476ce68":"code","f0ca807d":"code","337e42d1":"code","637c89ca":"code","60278638":"code","b6fe1eb1":"markdown","79208df0":"markdown","63446bc7":"markdown","c6d186cc":"markdown","6be9d029":"markdown"},"source":{"d82ed711":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d41256a9":"# Run this code for the first time, to install the libraries\n# import sys\n# !{sys.executable} -m pip install folium\n# !{sys.executable} -m pip install plotly","febc1f94":"import pandas as pd\nimport numpy as np\n\n\n# remove unnecessary columns\ndf = pd.read_csv(\"..\/input\/novel-corona-virus-2019-dataset\/covid_19_data.csv\")\ndf = df.drop(['SNo','Last Update'], axis=1)\ndf.ObservationDate = df.ObservationDate.apply(pd.to_datetime)\ndf.sort_values(by='ObservationDate',ascending=False)","823e9fcf":"df.ObservationDate.unique()  # timeframe is from 22nd Jan to 25th Feb with no missing days\ndf.ObservationDate.isnull().any()  # no missing values for Observation date\n\ndf[['Confirmed','Deaths','Recovered']].isnull().any()  # no missing values for ['Confirmed','Deaths','Recovered']\ndf['Country\/Region'].isnull().any()  # no missing values for Country\/Region\n\ndf['Province\/State'].isnull().any()  # missing values for Province\/State","349e3384":"df[df['Confirmed'] < 0 ]  # no invalid \/ negative cases\ndf[df['Deaths'] < 0 ]  # no invalid \/ negative values\ndf[df['Recovered'] < 0 ]  # no invalid \/ negative values\n\n\n# rename countries and provinces\ndf[\"Country\/Region\"].replace({\"Iran (Islamic Republic of)\": \"Iran\", \"Viet Nam\":\"Vietnam\"}, inplace=True)\n\ndf[df['Province\/State'].isnull()]['Country\/Region'].unique()  # list of countries without provinces\/state","0ea2ba7c":"df['Country\/Region'].replace({\"Taipei and environs\": \"Taiwan\"}, inplace=True)\ndf[~df['Province\/State'].isnull()]['Country\/Region'].unique() # list of countries with provinces\/state\n# the 2 lists of countries with and without provinces\/state are mutually exclusive. No incorrect entries or errors in country names","3d665162":"df.groupby(['Country\/Region','Province\/State']).size()  # list of provinces for each country","716d505f":"import plotly.graph_objects as go\n\nglobal_df = df.groupby('ObservationDate').sum()\nglobal_df['mortality rate'] = round(global_df['Deaths'] \/ global_df['Confirmed'],4)*100\nfig = go.Figure()\nfig.update_layout(template='plotly_dark')\nfig.add_trace(go.Scatter(x=global_df.index, \n                         y=global_df['mortality rate'],\n                         mode='lines+markers',\n                         name='Mortality Rate',\n                         line=dict(color='red', width=2)))\n\n\nfig.update_layout(\n    title=\"Global Mortality Rate\",\n    xaxis_title=\"Mortality Rate\",\n    yaxis_title=\"Date\",\n    font=dict(\n        family=\"Arial\",\n        size=16,\n        color=\"white\"\n    ))\n\n    \nfig.show()","9e8b0472":"global_df = df.groupby('ObservationDate').sum()\nfig = go.Figure()\nfig.update_layout(template='plotly_dark')\nfig.add_trace(go.Scatter(x=global_df.index, \n                         y=global_df['Deaths'],\n                         mode='lines+markers',\n                         name='Deaths',\n                         line=dict(color='red', width=2)))\nfig.add_trace(go.Scatter(x=global_df.index, \n                         y=global_df['Confirmed'],\n                         mode='lines+markers',\n                         name='Confirmed',\n                         line=dict(color='blue', width=2)))\nfig.add_trace(go.Scatter(x=global_df.index, \n                         y=global_df['Recovered'],\n                         mode='lines+markers',\n                         name='Recovered',\n                         line=dict(color='green', width=2)))\n\nfig.update_layout(\n    title=\"Global Number of Confirmed\/Death\/Recovered cases\",\n    xaxis_title=\"Number of cases\",\n    yaxis_title=\"Date\",\n    font=dict(\n        family=\"Arial\",\n        size=16,\n        color=\"white\"\n    ))\nfig.show()","48ce13e8":"latest_df = df[df.ObservationDate == '2020-03-10']\ncountry_df = latest_df.groupby('Country\/Region').sum()\nfig = go.Figure()\nfig.update_layout(template='plotly_dark')\nfig.add_trace(go.Bar(\n    y=country_df.index,\n    x=country_df.Confirmed,\n    name='Confirmed',\n    orientation='h',\n    marker=dict(\n        color='rgba(246, 78, 139, 0.6)',\n        line=dict(color='rgba(246, 78, 139, 1.0)', width=3)\n    )\n))\nfig.add_trace(go.Bar(\n    y=country_df.index,\n    x=country_df.Deaths,\n    name='Deaths',\n    orientation='h',\n    marker=dict(\n        color='rgba(58, 71, 80, 0.6)',\n        line=dict(color='rgba(58, 71, 80, 1.0)', width=3)\n    )\n))\n\nfig.update_layout(barmode='stack')\nfig.update_layout(\n    title=\"Number of Confirmed\/Death\/Recovered cases for each country\",\n    yaxis_title=\"Country names\",\n    xaxis_title=\"Number of cases\",\n    font=dict(\n        family=\"Arial\",\n        size=10,\n        color=\"white\"\n    ))\nfig.show()","a4db85cb":"latest_df = df[(df.ObservationDate == '2020-03-10') & (df['Country\/Region'] == 'Mainland China')]\nlatest_df = latest_df.groupby('Province\/State').sum()\n\nfig = go.Figure()\nfig.update_layout(template='plotly_dark')\nfig.add_trace(go.Bar(\n    y=latest_df.index,\n    x=latest_df.Confirmed,\n    name='Confirmed',\n    orientation='h',\n    marker=dict(\n        color='rgba(246, 78, 139, 0.6)',\n        line=dict(color='rgba(246, 78, 139, 1.0)', width=3)\n    )\n))\nfig.add_trace(go.Bar(\n    y=latest_df.index,\n    x=latest_df.Deaths,\n    name='Deaths',\n    orientation='h',\n    marker=dict(\n        color='rgba(58, 71, 80, 0.6)',\n        line=dict(color='rgba(58, 71, 80, 1.0)', width=3)\n    )\n))\n\nfig.update_layout(barmode='stack')\nfig.update_layout(\n    title=\"Number of Confirmed\/Death\/Recovered cases in various provinces across China\",\n    yaxis_title=\"Province names\",\n    xaxis_title=\"Number of cases\",\n    font=dict(\n        family=\"Arial\",\n        size=12,\n        color=\"white\"\n    ))\nfig.show()","58b20ef0":"country_unique_df = df.groupby('ObservationDate')['Country\/Region'].nunique()\ncountry_unique_df = pd.DataFrame({'ObservationDate':country_unique_df.index, 'Country Number':country_unique_df.values})\ncountry_unique_df\n\n\nfig = go.Figure()\nfig.update_layout(template='plotly_dark')\nfig.add_trace(go.Scatter(x=country_unique_df.ObservationDate, \n                         y=country_unique_df['Country Number'],\n                         mode='lines+markers',\n                         name='Number of unique countries infected with COVID-19',\n                         line=dict(color='red', width=2)))\n\n\nfig.update_layout(\n    title=\"Number of unique countries infected with COVID-19\",\n    yaxis_title=\"Number of Countries\",\n    xaxis_title=\"Date\",\n    font=dict(\n        family=\"Arial\",\n        size=16,\n        color=\"white\"\n    ))\nfig.show()","ed060852":"import folium\nfrom folium import plugins\n\nconfirmed_df = pd.read_csv(\"..\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_confirmed.csv\")\nconfirmed_df\nlatest_confirmed_df = confirmed_df[['Province\/State', 'Country\/Region', 'Lat', 'Long', '3\/10\/20']]\n\nm = folium.Map(location=[10, -20], zoom_start=2.6)\nuse_colours = ['orange','#d6604d','#b2182b','#67001f']\n\nfor lat, lon, value, country, province in zip(latest_confirmed_df['Lat'], latest_confirmed_df['Long'], latest_confirmed_df['3\/10\/20'], latest_confirmed_df['Country\/Region'], latest_confirmed_df['Province\/State']):\n    if not province:\n        popup = ('<strong>Country<\/strong>: ' + str(country).capitalize() + '<br>' '<strong>Confirmed Cases<\/strong>: ' + str(value) + '<br>')\n    else:\n        popup = ('<strong>Country<\/strong>: ' + str(country).capitalize() + '<br>' '<strong>Province<\/strong>: ' + str(province).capitalize() + '<br>' '<strong>Confirmed Cases<\/strong>: ' + str(value) + '<br>')        \n    if value >= 5000:\n        color = use_colours[3]\n    elif value >= 1000:\n        color = use_colours[2]\n    elif value >= 500:\n        color = use_colours[1]\n    else:\n        color = use_colours[0]\n    folium.CircleMarker([lat, lon],\n                        radius=10,\n                        popup = popup,\n                        color=color,\n                        fill_color=color,\n                        fill_opacity=0.7 ).add_to(m)\nminimap = plugins.MiniMap()\nm.add_child(minimap)\nm\npath='covid19map.html'\nm.save(path)","001507b5":"confirmed_df = pd.read_csv(\"..\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_confirmed.csv\")\ndate_list = list(confirmed_df.columns)[4:]\nfinal_date_list = []\nfor i in date_list:\n    k = i.split('\/')\n    if len(k[1])==1:\n        final_date_list.append('2020' + '-' + '0' + k[0] + '-' + '0' + k[1])\n    else:\n        final_date_list.append('2020' + '-' + '0' + k[0] + '-' + k[1])\n\nfinal_date_list = ['Province\/State','Country\/Region','Lat', 'Long'] + final_date_list\nconfirmed_df.columns = final_date_list\nconfirmed_df","48fe4bee":"from datetime import datetime\nimport matplotlib.pylab as plt\nfrom statsmodels.tsa.stattools import adfuller\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import acf, pacf","05e936cb":"confirmed_df = pd.read_csv(\"..\/input\/novel-corona-virus-2019-dataset\/time_series_covid_19_confirmed.csv\")\nconfirmed_df = confirmed_df.drop(labels=['Province\/State', 'Country\/Region', 'Lat', 'Long'], axis=1)\nconfirmed_df = confirmed_df.T\nconfirmed_df['confirmed'] = confirmed_df.sum(axis=1)\nconfirmed_df = confirmed_df['confirmed']\n\ntrain_size = int(len(confirmed_df) * 0.95)\ntrain, test = confirmed_df[0:train_size], confirmed_df[train_size:]\n","3bd678a8":"plt.plot(train)\nplt.xlabel('Date')\nplt.ylabel('Cumulative number of confirmed cases')\nplt.xticks(fontsize=8, rotation=90)\nplt.title('Cumulative number of confirmed cases between January to March')\nplt.show()","238f232c":"def test_stationarity(timeseries):\n    #Determing rolling statistics\n    rolmean = timeseries.rolling(window=7).mean()\n    rolstd = timeseries.rolling(window=7).std()\n\n    #Plot rolling statistics:\n    \n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.xticks(fontsize=10, rotation=90)\n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)","d25c72dd":"test_stationarity(train)\n# Since this dataset has a cumulative number of confirmed cases,\n# both Rolling Mean and Rolling Standard Deviation are not constant.\n# Hence dataset is not stationary","682bd0ef":"def difference(dataset, interval=1):\n    index = list(dataset.index)\n    diff = list()\n    for i in range(interval, len(dataset)):\n        value = dataset.values[i] - dataset.values[i - interval]\n        diff.append(value)\n    return (diff)","fa9c09b0":"# Difference will remove the cumulative number of confirmed cases and only present the number of confirmed cases pe\ndiff = difference(train)\n\nplt.plot(diff)\nplt.xlabel('Number of days')\nplt.ylabel('Number of confirmed cases')\nplt.title('Differencing: Subtracting previous observation from the current observation')\nplt.show()\n# Differencing is a popular and widely used data transform for making time series data stationary.\n# Differencing is performed by subtracting the previous observation from the current observation.","ef5020f2":"X = diff\nresult = adfuller(X)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n    \n# We carry out Dickey-Fuller test, to test for stationarity in the dataset. \n# The results show that te test static value -4.480739 is smaller than the critical value at 5% of -2.924\n# We can reject null hypothesis and conclude that differenced series is stationary.\n# At least one level of differencing is required.","2ecc016d":"diff = difference(train)\ndiff_df = pd.DataFrame(diff)\ntrain1 = pd.DataFrame(train)\ntrain1['date'] = train1.index\ndiff_df.index = train1.date[1:]\ndiff_df\n\ntest_stationarity(diff_df)\n# Since this dataset has a cumulative number of confirmed cases,\n# both Rolling Mean and Rolling Standard Deviation are not constant.\n# Hence dataset is not stationary","c673af5b":"from statsmodels.tsa.seasonal import seasonal_decompose\ndiff = difference(train)\ndiff_df = pd.DataFrame(diff)\ntrain1 = pd.DataFrame(train)\ntrain1['date'] = train1.index\ndiff_df.index = train1.date[1:]\n\ndiff_df1 = pd.DataFrame(diff_df)\ndiff_df1.reset_index(inplace=True)\ndiff_df1['date'] = pd.to_datetime(diff_df1['date'])\ndiff_df1.index = diff_df1['date']\ndiff_df1 = diff_df1.drop(columns=['date'],axis=1)\n\n\ndecomposition = seasonal_decompose(diff_df1)\n\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.subplot(411)\nplt.plot(diff, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.show()\n\n# taking the differenced dataset, there is still some trends of seasonality","dd9c915a":"from statsmodels.tsa.stattools import acf, pacf\n\nlag_acf = acf(diff, nlags=20)\nlag_pacf = pacf(diff, nlags=20, method='ols')\n\nplt.subplot(121) \nplt.plot(lag_acf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(diff)),linestyle='--',color='gray')\nplt.title('Autocorrelation Function')\nplt.show()\n\nplt.subplot(122)\nplt.plot(lag_pacf)\nplt.axhline(y=0,linestyle='--',color='gray')\nplt.axhline(y=-1.96\/np.sqrt(len(diff)),linestyle='--',color='gray')\nplt.axhline(y=1.96\/np.sqrt(len(diff)),linestyle='--',color='gray')\nplt.title('Partial Autocorrelation Function')\nplt.show()\nplt.tight_layout()","0476ce68":"import warnings\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# evaluate an ARIMA model for a given order (p,d,q) and return RMSE\ndef evaluate_arima_model(X, arima_order):\n    # prepare training dataset\n    X = X.astype('float32')\n    train_size = int(len(X) * 0.50)\n    train, test = X[0:train_size], X[train_size:]\n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    for t in range(len(test)):\n        model = ARIMA(history, order=arima_order)\n        model_fit = model.fit(disp=0)\n        yhat = model_fit.forecast()[0]\n        predictions.append(yhat)\n        history.append(test[t])\n    # calculate out of sample error\n    rmse = sqrt(mean_squared_error(test, predictions))\n    return rmse\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype('float32')\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    rmse = evaluate_arima_model(dataset, order)\n                    if rmse < best_score:\n                        best_score, best_cfg = rmse, order\n                    print('ARIMA%s RMSE=%.3f' % (order,rmse))\n                except:\n                    continue\n    print('Best ARIMA%s RMSE=%.3f' % (best_cfg, best_score))\n\n# evaluate parameters\ndiff = difference(train)\ndiff_df = pd.DataFrame(diff)\n\np_values = range(0,8)\nd_values = range(0, 4)\nq_values = range(0, 8)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models(diff_df.values, p_values, d_values, q_values)","f0ca807d":"# evaluate an ARIMA model for a given order (p,d,q) and return AIC\ndef evaluate_arima_model_aic(X, arima_order):\n    # prepare training dataset\n    X = X.astype('float32')\n    train_size = int(len(X) * 0.50)\n    train, test = X[0:train_size], X[train_size:]\n    history = [x for x in train]\n    # make predictions\n    predictions = list()\n    for t in range(len(test)):\n        model = ARIMA(history, order=arima_order)\n        model_fit = model.fit(disp=0)\n        yhat = model_fit.forecast()[0]\n        predictions.append(yhat)\n        history.append(test[t])\n    # calculate out of sample error\n    return model_fit.aic\n\n# evaluate combinations of p, d and q values for an ARIMA model\ndef evaluate_models_aic(dataset, p_values, d_values, q_values):\n    dataset = dataset.astype('float32')\n    best_score, best_cfg = float(\"inf\"), None\n    for p in p_values:\n        for d in d_values:\n            for q in q_values:\n                order = (p,d,q)\n                try:\n                    aic = evaluate_arima_model(dataset, order)\n                    if aic < best_score:\n                        best_score, best_cfg = aic, order\n                    print('ARIMA%s AIC=%.3f' % (order,aic))\n                except:\n                    continue\n    print('Best ARIMA%s AIC=%.3f' % (best_cfg, best_score))\n\n# evaluate parameters\ndiff = difference(train)\ndiff_df = pd.DataFrame(diff)\n\np_values = range(0,8)\nd_values = range(0, 4)\nq_values = range(0, 8)\nwarnings.filterwarnings(\"ignore\")\nevaluate_models_aic(diff_df.values, p_values, d_values, q_values)","337e42d1":"best_cfg = (0, 1, 0)\nhistory = [float(x) for x in diff_df.values]\nmodel = ARIMA(history, order=best_cfg)\nmodel_fit = model.fit(disp=0)\nprint(model_fit.summary())","637c89ca":"diff_test = difference(test)\ndiff_df_test = pd.DataFrame(diff_test)\ntest1 = pd.DataFrame(test)\ntest1['date'] = test1.index\ndiff_df_test.index = test1.date[1:]\ndiff_df_test\n\n\n# walk-forward validation\nbest_cfg = (0, 1, 0)\nhistory = [float(x) for x in diff_df.values]\npredictions = list()\nfor i in range(len(diff_df_test)):\n    # predict\n    model = ARIMA(history, order=best_cfg)\n    model_fit = model.fit(disp=0)\n    yhat = model_fit.forecast()[0]\n    predictions.append(yhat)\n    # observation\n    obs = diff_df_test.values[i]\n    history.append(obs)\n# errors\nresiduals = [diff_df_test.values[i]-predictions[i] for i in range(len(diff_df_test))]\nresiduals = pd.DataFrame(residuals)\nplt.figure()\nplt.subplot(211)\nresiduals.hist(ax=plt.gca())\nplt.subplot(212)\nresiduals.plot(kind='kde', ax=plt.gca())\nplt.show()","60278638":"from statsmodels.tsa.arima_model import ARIMAResults\nfrom scipy.stats import boxcox\nfrom sklearn.metrics import mean_squared_error\nfrom math import exp\nfrom math import log\nimport numpy\n\nhistory = [float(x) for x in diff_df.values]\n# predict\n\ndef invert_difference(test, diff):\n    pred = list()\n    value = test[0]\n    for i in range(0, len(diff)):\n        value += float(diff.values[i])\n        pred.append(value)\n    return numpy.array(pred)\n\n\nmodel = ARIMA(history, order=(0,1,0))\nmodel_fit = model.fit()\n\nyhat = model_fit.forecast(steps=1)[0]\nyhat = yhat + test.values[0]\nprint('Predicted next day volume on 12th March = %i' % (yhat))","b6fe1eb1":"#### Autocorrelation Function (ACF)\n\nIt is a measure of the correlation between the the TS with a lagged version of itself. For instance at lag 5, ACF would compare series at time instant \u2018t1\u2019\u2026\u2019t2\u2019 with series at instant \u2018t1-5\u2019\u2026\u2019t2-5\u2019 (t1-5 and t2 being end points).\n\n#### Partial Autocorrelation Function (PACF)\n\nThis measures the correlation between the TS with a lagged version of itself but after eliminating the variations already explained by the intervening comparisons. Eg at lag 5, it will check the correlation but remove the effects already explained by lags 1 to 4.","79208df0":"<a name=\"arima\"><\/a>\n\n### Using Time Series based models: ARIMA\n\nAssumptions of ARIMA model:\n\n- The dataset is time dependent and univariate.\n- The dataset should be stationary: constant mean, variance and autocovariance that is time independent.\n- The model parameters are constant over time\n- The error process is homoscedastic (constant) over time. (Residuals are uncorrelated and normally distributed)","63446bc7":"# Analysis of Novel Corona Virus 2019 (COVID-19) Dataset\n\n\n### Objective\nWe would like to analyse the impact of COVID-19 on different countries across the world. By understanding the historical information and current numbers of confirmed, death, recovered cases, we can assess the growth rate of COVID-19 across different countries and determine whether the spread of COVID-19 is slowing or increasing in specific countries. We would like to predict the future rate of spread and number of deaths using various time series models.   \n\n---\n\n### Context\nFrom World Health Organization - On 31 December 2019, WHO was alerted to several cases of pneumonia in Wuhan City, Hubei Province of China. The virus did not match any other known virus. This raised concern because when a virus is new, we do not know how it affects people.\n\nSo daily level information on the affected people can give some interesting insights when it is made available to the broader data science community.\n\nJohns Hopkins University has made an excellent dashboard using the affected cases data. This data is extracted from the same link and made available in csv format.\n\n---\n\n### Content\n2019 Novel Coronavirus (2019-nCoV) is a virus (more specifically, a coronavirus) identified as the cause of an outbreak of respiratory illness first detected in Wuhan, China. Early on, many of the patients in the outbreak in Wuhan, China reportedly had some link to a large seafood and animal market, suggesting animal-to-person spread. However, a growing number of patients reportedly have not had exposure to animal markets, indicating person-to-person spread is occurring. At this time, it\u2019s unclear how easily or sustainably this virus is spreading between people - CDC\n\nThis dataset has daily level information on the number of affected cases, deaths and recovery from 2019 novel coronavirus. Please note that this is a time series data and so the number of cases on any given day is the cumulative number.\n\nThe data is available from 22 Jan, 2020.\n\nData at individual level obtained from the below two sources\n\n- https:\/\/docs.google.com\/spreadsheets\/d\/1itaohdPiAeniCXNlntNztZ_oRvjh0HsGuJXUJWET008\/edit#gid=0\n- https:\/\/docs.google.com\/spreadsheets\/d\/e\/2PACX-1vQU0SIALScXx8VXDX7yKNKWWPKE1YjFlWc6VTEVSN45CklWWf-uWmprQIyLtoPDA18tX9cFDr-aQ9S6\/pubhtml\n\n\n---\n\n### Acknowledgements\nJohns Hopkins university has made the data available in google sheets format here. Sincere thanks to them.\n\nThanks to WHO, CDC, NHC and DXY for making the data available in first place.","c6d186cc":"<a name=\"EDA\"><\/a>\n### Exploratory Data Analysis\n\nWe would first like to understand the number of confirmed \/ deaths \/ recovered cases to assess the global and local impact of COVID-19. We also like to identify the factors that influence the spread of COVID-19 and specific demographics have a high frequency or high growth rate of COVID-19.\n\nData processing steps: \n- Check for missing values in confirmed \/ deaths \/ recovered cases\n- Check for missing values or errors in Country \/ Province \/ Date formating\n- Check for invalid values in confirmed \/ deaths \/ recovered cases\n\nSome questions we would like to investigate:\n- What is the global growth rate in terms of confirmed \/ deaths \/ recovered cases?\n- What are the countries with the most number of confirmed \/ deaths \/ recovered cases?\n- What are the countries with the fastest rate of growth for confirmed cases?\n- What are the countries with the highest rate of mortality? Are these countries well-equipped to tackle the spread of COVID-19?\n- What is the rate of spread to different countries (based on first confirmed cases in countries)?  \n- World map visualization in terms of confirmed cases around the world.\n- Correlations of spread of COVID-19 between countries","6be9d029":"### Table of Contents\n\n* [ Exploratory Data Analysis and Data Processing ](#EDA)\n\n* [ Visualization of global map for COVID-19 cases](#map)\n\n* [ Prediction of the spread of COVID-19 across countries with ARIMA](#predict)\n"}}