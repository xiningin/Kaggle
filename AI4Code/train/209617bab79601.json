{"cell_type":{"6c1f6a6e":"code","e351d8b1":"code","90df7845":"code","729976de":"code","7596f862":"code","b64663d9":"code","93fc9109":"code","7b6a7b7b":"code","2639cf4c":"code","96745975":"code","d6077285":"code","48bd69c3":"code","056e490a":"code","6f24183e":"code","d02af5a3":"code","f5b20033":"code","bcbcb9de":"code","54d2764f":"code","38eeadc1":"code","f2d902eb":"code","4090b556":"code","247ea758":"code","b9e56635":"code","4c0c5f70":"code","82bce057":"markdown","6108dbbd":"markdown","429da63f":"markdown"},"source":{"6c1f6a6e":"import numpy as np # Matrix operations\nimport pandas as pd # Data Frame processing\nimport tensorflow as tf # Backend library for neural nets processing\nfrom keras import models, layers # Framework for neural nets creating\nimport os, shutil # filesystem operations \nfrom os import listdir\nfrom os.path import isfile, join\nimport random\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import multilabel_confusion_matrix\nfrom sklearn.metrics import confusion_matrix","e351d8b1":"tf.__version__","90df7845":"# GPU checking\ndevice_names = tf.test.gpu_device_name()\ndevice_names","729976de":"def create_subfolders(base_dir, class_names):\n    \"\"\"\n    Create subfolders in the base folder\n\n    Parameters\n    ----------\n    base_folder : string with output folder\n    class_names :  list of class names\n    \n    Return\n    ----------\n    created_dirs : list of created folder names    \n    \n    Example\n    ----------\n    create_subfolders(\"\/kaggle\/working\/data\", ['cats', 'dogs'])\n\n    \"\"\"\n    \n    subdir_list = ['train', 'validation', 'test']\n    created_dirs = []\n    try:\n        os.mkdir(base_dir)\n    \n        for names in subdir_list:\n            sub_dir = os.path.join(base_dir, names)\n            os.mkdir(sub_dir)\n            for class_name in class_names:\n                class_dir = os.path.join(sub_dir, class_name)\n                os.mkdir(class_dir)\n                created_dirs.append(class_dir)\n\n    except: print(\"Subfolders already exist\")\n    else: print('Subfolders are created')\n    return created_dirs\n        ","7596f862":"def data_split(data, test_ratio=0.3):\n    \"\"\"\n    Splitting the dataset\n\n    Parameters\n    ----------\n    data : original list with data\n    test_ratio : float test-train splitting ratio \n    \n    Return\n    ----------\n    train_data, val_data, test_data : list of created splits    \n \n    \"\"\"\n    \n    all_img_amount = len(data)\n    test_img_amount = val_img_amount = int(test_ratio*all_img_amount)\n    train_img_amount = int(all_img_amount - test_img_amount - val_img_amount) \n\n    random.shuffle(data)\n    train_data = data[:train_img_amount]\n    val_data = data[train_img_amount:train_img_amount+val_img_amount]\n    test_data = data[train_img_amount+val_img_amount:]\n    return train_data, val_data, test_data","b64663d9":"def data_split_list(base_dir, class_list, test_ratio=0.3):\n    \"\"\"\n    Create splitting for all class lists\n\n    Parameters\n    ----------\n    base_dir : dictionary with base train and test directories, e.g. {'train':\"..\/input\/dogs-vs-cats\/train\/train\", 'test':\"..\/input\/dogs-vs-cats\/test\/test\"}\n    class_list :  list of class names, e.g. class_list = ['cat', 'dog']\n    test_ratio : float test-train splitting ratio\n    \n    Return\n    ----------\n    train_data, val_data, test_data : list of lists with file names    \n    \n    \"\"\"\n    \n    all_train = []\n    train_data = []\n    val_data = []\n    test_data = []\n\n    for class_name in class_list:\n        class_files = [f for f in listdir(base_dir['train']) if isfile(join(base_dir['train'], f)) and (class_name in f)]\n        all_train.append(class_files)\n\n    all_test = [f for f in listdir(base_dir['test']) if isfile(join(base_dir['test'], f))]\n    for i in range(len(class_list)):\n        train, val, test = data_split(all_train[i])\n        train_data.append(train)\n        val_data.append(val)\n        test_data.append(test)\n    return [train_data, val_data, test_data]","93fc9109":"def copy_images_to_folders(base_dir, dataset_dirs, datasets):\n    \"\"\"\n    Distributes original images to appropriate folders. This is alternative to \n    train_test splitting method. It is assumed that files in the original _dataset_dir\n    includes classes in their names\n\n    Parameters\n    ----------\n    base_dir : base folder with original images\n    dataset_dirs : ratio for train\/test splitting like following\n                    ['\/kaggle\/working\/data\/train\/cats', \n                    '\/kaggle\/working\/data\/train\/dogs', \n                    '\/kaggle\/working\/data\/validation\/cats', \n                    '\/kaggle\/working\/data\/validation\/dogs', \n                    '\/kaggle\/working\/data\/test\/cats', \n                    '\/kaggle\/working\/data\/test\/dogs']\n    datasets : list of datasets with splitted images like following\n                    [train_data, val_data, test_data]\n            \n    \"\"\"\n     \n    class_amount = np.shape(datasets[0])[0]\n    dir_count = -1\n    \n    for dataset in datasets:\n        for i in range(class_amount):\n            dir_count = dir_count + 1\n            print(dir_count)\n            for filename in dataset[i]:\n                source = os.path.join(base_dir, filename)\n                destination = os.path.join(dataset_dirs[dir_count], filename)\n                shutil.copyfile(source, destination)\n    ","7b6a7b7b":"def _round(vec, threshold):\n    output = []\n    for i in vec:\n        if i >= threshold:\n            output.append(np.ceil(i))\n        else:\n            output.append(np.floor(i))\n    return np.array(output)\n\ndef plot_confusion_matrix(cm,\n                      classes, \n                      normalized=False, \n                      title=None, \n                      cmap=plt.cm.Blues,\n                      size=(2,2)):\n    fig, ax = plt.subplots(figsize=size)\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)","2639cf4c":"original_dataset_dir = \"..\/input\/dogs-vs-cats\/train\/train\" \nbase_dir_dic = {'train':\"..\/input\/dogs-vs-cats\/train\/train\", 'test':\"..\/input\/dogs-vs-cats\/test\/test\"}\nclasses_list = ['cat', 'dog']\nsub_dirs = create_subfolders(\"\/kaggle\/working\/data\/\", classes_list)\ndatasets = data_split_list(base_dir_dic, classes_list)\n","96745975":"sub_dirs","d6077285":"copy_images_to_folders(base_dir_dic['train'], sub_dirs, datasets)","48bd69c3":"#Making a convolutional model\nmodel = models.Sequential()\nmodel.add(layers.Flatten(input_shape=(150, 150, 3)))\nmodel.add(layers.Dense(512, activation=\"relu\"))\nmodel.add(layers.Dense(128, activation=\"relu\"))\nmodel.add(layers.Dense(64, activation = \"relu\"))\nmodel.add(layers.Dense(1, activation = \"sigmoid\"))","056e490a":"model.summary()","6f24183e":"model.compile(\n  loss = \"binary_crossentropy\",\n  optimizer = optimizers.RMSprop(lr = 1e-4),\n  metrics = [\"acc\"]\n)","d02af5a3":"#Fetching train data and validation data and processing the data\ntrain_datagen = ImageDataGenerator(rescale = 1.00 \/ 255.0)\nval_datagen = ImageDataGenerator(rescale = 1.00 \/ 255.0)\ntest_datagen = ImageDataGenerator(rescale = 1.00 \/ 255.0)\n\ntrain_generator = train_datagen.flow_from_directory(\n    '\/kaggle\/working\/data\/train',\n    target_size = (150, 150),\n    batch_size = 20,\n    class_mode = \"binary\"\n\n)\n\nvalidation_generator = val_datagen.flow_from_directory(\n    '\/kaggle\/working\/data\/validation',\n    target_size = (150, 150),\n    batch_size = 20,\n    class_mode = \"binary\"\n)\n\ntest_generator = test_datagen.flow_from_directory(\n    '\/kaggle\/working\/data\/test',\n    target_size = (150, 150),\n    batch_size = 20,\n    class_mode = \"binary\"\n)","f5b20033":"#Training the model with train data and judging this training with validation data\nhistory = model.fit(\n    train_generator,\n    steps_per_epoch = 100,\n    epochs = 100,\n    validation_data = validation_generator,\n    validation_steps = 50   \n)","bcbcb9de":"#Train accuracy and validation accuracy vs epoch graph\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, label='Training acc')\nplt.plot(epochs, val_acc, label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.plot(epochs, loss, label='Training loss')\nplt.plot(epochs, val_loss, label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()","54d2764f":"y_pred = model.predict(test_generator)","38eeadc1":"y_pred = _round(y_pred, 0.5)","f2d902eb":"y_true = test_generator.classes","4090b556":"mcm = multilabel_confusion_matrix(y_true, y_pred)\ncmn = confusion_matrix(y_true, y_pred, normalize='true')","247ea758":"print(cmn)","b9e56635":"labels = ['dog', 'cat']","4c0c5f70":"plot_confusion_matrix(cmn,\n                      labels, \n                      normalized=True, \n                      title=\"Model Performance\", \n                      cmap=plt.cm.Blues,\n                      size=(2,2))","82bce057":"\u041f\u043e\u0447\u0430\u0442\u043a\u043e\u0432\u0456 \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u043d\u044f \u0440\u043e\u0437\u043f\u043e\u0434\u0456\u043b\u044f\u044e\u0442\u044c\u0441\u044f \u0437\u0430 \u043f\u0456\u0434\u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0456\u044f\u043c\u0438 train, validation, test. \u0412 \u043a\u043e\u0436\u043d\u0456\u0439 \u0437 \u044f\u043a\u0438\u0445 \u0441\u0442\u0432\u043e\u0440\u044e\u044e\u0442\u044c\u0441\u044f \u043f\u0456\u0434\u0434\u0438\u0440\u0435\u043a\u0442\u043e\u0440\u0456\u0457 \u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u044c \u043a\u043e\u0436\u043d\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0443\n","6108dbbd":"\u041f\u0456\u0434\u043a\u043b\u044e\u0447\u0435\u043d\u043d\u044f \u0431\u0456\u0431\u043b\u0456\u043e\u0442\u0435\u043a \u0434\u043b\u044f \u043c\u0430\u043d\u0456\u043f\u0443\u043b\u044f\u0446\u0456\u0439 \u0437 \u0434\u0430\u043d\u0438\u043c\u0438 \u0442\u0430 \u0441\u0442\u0432\u043e\u0440\u0435\u043d\u043d\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u0438\u0445 \u043c\u0435\u0440\u0435\u0436","429da63f":"\u041f\u043e\u0431\u0443\u0434\u043e\u0432\u0430 \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u043e\u0457 \u043c\u0435\u0440\u0435\u0436\u0456"}}