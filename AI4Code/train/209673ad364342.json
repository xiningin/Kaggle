{"cell_type":{"ff8e7a67":"code","623da215":"code","9a7ef4ce":"code","e068324d":"code","f92c1a35":"code","7edf899f":"code","0e5b392e":"code","3da0d74a":"code","1de1bab9":"code","c108019a":"code","f3e39d0e":"code","56167fec":"code","9f71d60c":"code","94b1dbb7":"code","db5f7327":"code","f692cdec":"code","67a1d167":"code","16058e78":"code","1ddd3241":"code","12abd14f":"code","edb36d17":"code","27d34850":"code","6b97fa6e":"code","c7f2106e":"code","e56b79bc":"code","8f71ae4f":"code","29b0fd43":"code","caf4d614":"code","7081f3dd":"code","1f1a4402":"code","2283a5f7":"code","77d5af3a":"code","86dc4ff2":"code","8d701da1":"code","8620205b":"code","ef6dcc8f":"code","65b2a016":"code","87190b72":"code","113f2075":"code","799c9b4d":"code","427643b8":"code","935cbf46":"code","e70f78c3":"code","0b4946e4":"code","de8238f1":"code","58dc71ef":"code","d83534e0":"code","4f036573":"code","0f6db25c":"code","16ee8f71":"code","f1251417":"code","c61544bf":"code","a13e09ae":"code","248fd860":"code","6582bea7":"code","42adcd14":"code","46b478d5":"code","60189276":"code","7cfaa95c":"code","b0341686":"code","e06bc97d":"code","a6e75bbd":"code","37c3ab6e":"code","de90ed34":"code","75d736a9":"code","f1f8e2d0":"code","748816e9":"code","fd07d98d":"code","1d8cfd27":"code","1618d74f":"code","fbd0c056":"code","abe79b02":"code","18909542":"code","86e628bb":"code","e0c06e34":"code","238ddd5a":"code","2a867e57":"code","bfcbba10":"code","bf5099b5":"code","69c351d6":"code","8927af48":"code","14bd0975":"code","ed475332":"code","1cfa5421":"code","4481f69d":"code","42595333":"markdown","d306b8f4":"markdown","122d4920":"markdown","65b61d88":"markdown","b147cf9b":"markdown","959c1049":"markdown","8415d776":"markdown","1f2c7717":"markdown","23c21753":"markdown","f3894404":"markdown","8b884e0d":"markdown","5641ce76":"markdown","b46514b0":"markdown","2be175cf":"markdown","696ccfe4":"markdown","3abdd921":"markdown","d2cc6452":"markdown","d8456c64":"markdown","d6336270":"markdown","29d56490":"markdown","22f3f584":"markdown","b793da85":"markdown","ff5742db":"markdown","cfe83dc3":"markdown","8e9eb80f":"markdown","9a2951df":"markdown","66e2faba":"markdown","a8801900":"markdown","81792cec":"markdown","275f68e8":"markdown","afc1e590":"markdown","6999c97d":"markdown","657c70ad":"markdown","123bdced":"markdown","c2d6d0c3":"markdown","11661864":"markdown","e1e1c3bd":"markdown","9b2f00cf":"markdown","76b996dd":"markdown","236455e7":"markdown","fb80b8a9":"markdown","a33f5c96":"markdown","e4057299":"markdown","7cff5fe8":"markdown","1d550f69":"markdown","a7383304":"markdown","54fe3aa4":"markdown","340c236b":"markdown","d55718f0":"markdown","089373c6":"markdown","c93a585a":"markdown"},"source":{"ff8e7a67":"# Manipula\u00e7\u00e3o dos dados\nimport pandas as pd\nimport numpy as np\n\n# Visualiza\u00e7\u00e3o \nimport matplotlib.pyplot as plt\nimport os\nimport time\n%matplotlib inline\n\n# Processamento\nimport sklearn\nfrom sklearn import preprocessing as prep\n\n# Classificadores\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neural_network import MLPClassifier\n\n# Avalia\u00e7\u00e3o\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score","623da215":"os.listdir(\"..\/input\/uci-adult\")","9a7ef4ce":"adult = pd.read_csv(\"..\/input\/uci-adult\/adult.data\",\n        names = [\n            \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n            \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n            \"Hours per week\", \"Country\", \"Target\"],\n        sep= r'\\s*,\\s*',\n        engine= 'python',\n        na_values= \"?\")\n\nadult.shape","e068324d":"adult.head()","f92c1a35":"adult.describe()","7edf899f":"adult[\"Workclass\"].value_counts().plot(kind = \"bar\")","0e5b392e":"adult[\"Martial Status\"].value_counts()","3da0d74a":"adult[\"Occupation\"].value_counts().plot(kind = 'bar')","1de1bab9":"adult[\"Relationship\"].value_counts().plot(kind = 'bar')","c108019a":"adult[\"Race\"].value_counts().plot(kind = 'pie')","f3e39d0e":"adult[\"Sex\"].value_counts().plot(kind = 'pie')","56167fec":"adult[\"Country\"].value_counts()","9f71d60c":"adult.isnull().sum()","94b1dbb7":"moda = adult['Workclass'].describe().top\nadult['Workclass'] = adult['Workclass'].fillna(moda)\n\nmoda = adult['Occupation'].describe().top\nadult['Occupation'] = adult['Occupation'].fillna(moda)\n\nmoda = adult['Country'].describe().top\nadult['Country'] = adult['Country'].fillna(moda)\n\nadult.isnull().sum()","db5f7327":"testAdult = pd.read_csv(\"..\/input\/uci-adult\/adult.test\",\n            names = [\n            \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n            \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n            \"Hours per week\", \"Country\", \"Target\"],\n        sep= r'\\s*,\\s*',\n        engine= 'python',\n        na_values= \"?\").drop(0, axis = 0).reset_index(drop = True)\n\ntestAdult.shape","f692cdec":"testAdult.head()","67a1d167":"testAdult.isnull().sum()","16058e78":"moda = testAdult['Workclass'].describe().top\ntestAdult['Workclass'] = testAdult['Workclass'].fillna(moda)\n\nmoda = testAdult['Occupation'].describe().top\ntestAdult['Occupation'] = testAdult['Occupation'].fillna(moda)\n\nmoda = testAdult['Country'].describe().top\ntestAdult['Country'] = testAdult['Country'].fillna(moda)\n\ntestAdult.isnull().sum()","1ddd3241":"adult.describe()","12abd14f":"testAdult.describe()","edb36d17":"# N\u00e3o Num\u00e9ricos relevantes\nnNumber = [\"Workclass\", \"Occupation\", \"Race\", \"Target\"]\n\nadult[nNumber] = adult[nNumber].apply(prep.LabelEncoder().fit_transform)\n\ntestAdult[nNumber] = testAdult[nNumber].apply(prep.LabelEncoder().fit_transform)","27d34850":"adult.head()","6b97fa6e":"atributos = [\"Age\", \"Education-Num\", \"Capital Gain\", \"Capital Loss\", \"Hours per week\"]\n\nx_train = adult[atributos]\ny_train = adult.Target\n\nx_test = testAdult[atributos]\ny_test = testAdult.Target","c7f2106e":"knn = KNeighborsClassifier(n_neighbors = 5)\n\nscores = cross_val_score(knn, x_train, y_train, cv=10)\nscores.mean()","e56b79bc":"knn.fit(x_train, y_train)\n\ny_predict = knn.predict(x_test)\n\naccuracy_score(y_test, y_predict)","8f71ae4f":"knn = KNeighborsClassifier(n_neighbors = 25)\n\nscores = cross_val_score(knn, x_train, y_train, cv=10)\nscores.mean()","29b0fd43":"knn.fit(x_train, y_train)\n\ny_predict = knn.predict(x_test)\n\naccuracy_score(y_test, y_predict)","caf4d614":"atributos = [\"Age\", \"Workclass\", \"Education-Num\", \"Occupation\", \"Race\", \"Capital Gain\", \"Capital Loss\", \"Hours per week\"]\n\nx_train = adult[atributos]\ny_train = adult.Target\n\nx_test = testAdult[atributos]\ny_test = testAdult.Target","7081f3dd":"knn = KNeighborsClassifier(n_neighbors = 25)\n\nscores = cross_val_score(knn, x_train, y_train, cv=10)\nscores.mean()","1f1a4402":"knn.fit(x_train, y_train)\n\ny_predict = knn.predict(x_test)\n\naccuracy_score(y_test, y_predict)","2283a5f7":"    inf = 1\n    sup = 35\n\n    scores_media = []\n    aux = 0\n    k_max = 0\n\n    i = 0\n    for k in range(inf, sup):\n        knn = KNeighborsClassifier(n_neighbors = k)\n        scores = cross_val_score(knn, x_train, y_train, cv=10)\n        scores_media.append(scores.mean())\n\n        if scores_media[i] > aux:\n            k_max = k\n            aux = scores_media[i]\n\n        i = i + 1\n\n    print(k_max)","77d5af3a":"x = np.arange(1, sup)\n\nplt.figure(figsize=(10, 5))\nplt.plot(x, scores_media, '--', color = 'red', linewidth = 2)\nplt.plot(k_max, scores_media[k_max], 'o')\n\nplt.xlabel('k')\nplt.ylabel('Acur\u00e1cia')\nplt.title('Perfomance do algoritmo conforme o valor de k')","86dc4ff2":"print('Acur\u00e1cia para k = {0} : {1:2.2f}%'.format(k_max, 100 * scores_media[k_max]))","8d701da1":"k = k_max","8620205b":"# In\u00edcio da contagem\nstart = time.time()\n\nknn = KNeighborsClassifier(n_neighbors = k)\n\nknn.fit(x_train, y_train)\n\n# T\u00e9rmino da contagem\nend = time.time()","ef6dcc8f":"time_knn = end - start\nprint (\"[knn] Tempo necess\u00e1rio em segundos:\", time_knn)","65b2a016":"predict_knn = knn.predict(x_test)\n\nprint(confusion_matrix(y_test, predict_knn))\nprint(classification_report(y_test, predict_knn))\n\nacc_knn = accuracy_score(y_test, predict_knn)\nprint('Accuracy: ', acc_knn)","87190b72":"# Num\u00e9ricos e N\u00e3o Num\u00e9ricos\natributos = [\"Age\", \"Workclass\", \"Education-Num\", \"Occupation\", \"Race\", \"Capital Gain\", \"Capital Loss\", \"Hours per week\"]\n\nx_train = adult[atributos]\ny_train = adult['Target']\n\nx_test = testAdult[atributos]\ny_test = testAdult['Target']","113f2075":"# In\u00edcio da contagem\nstart = time.time()\n\nlr = LogisticRegression(penalty = 'l2', solver = 'newton-cg')\n\nlr_scores = cross_val_score(lr, x_train, y_train, cv=10)\n\nlr.fit(x_train, y_train)\n\n# T\u00e9rmino da contagem\nend = time.time()","799c9b4d":"time_lr = end - start\n\nprint (\"[LR] Tempo Necess\u00e1rio em segundos:\", time_lr)\nlr_scores","427643b8":"predict_lr = lr.predict(x_test)\n\nprint(confusion_matrix(y_test, predict_lr))\nprint(classification_report(y_test, predict_lr))\n\nacc_lr = accuracy_score(y_test, predict_lr)\nprint('Accuracy: ', acc_lr)","935cbf46":"# In\u00edcio da contagem\nstart = time.time()\n\nrf = RandomForestClassifier(n_estimators = 700, max_depth = 12)\n\nrf_scores = cross_val_score(rf, x_train, y_train, cv=10)\n\nrf.fit(x_train, y_train)\n\n# T\u00e9rmino da contagem\nend = time.time()","e70f78c3":"time_rf = end - start\n\nprint (\"[RF] Tempo Necess\u00e1rio em segundos:\", time_rf)\nrf_scores","0b4946e4":"predict_rf = rf.predict(x_test)\n\nprint(confusion_matrix(y_test, predict_rf))\nprint(classification_report(y_test, predict_rf))\n\nacc_rf = accuracy_score(y_test, predict_rf)\nprint('Accuracy: ', acc_rf)","de8238f1":"# In\u00edcio da contagem\nstart = time.time()\n\nsvm = SVC(gamma = 'scale', kernel = 'rbf')\n\nsvm_scores = cross_val_score(svm, x_train, y_train, cv=10)\n\nsvm.fit(x_train,y_train)\n\n# T\u00e9rmino da contagem\nend = time.time()","58dc71ef":"time_svm = end-start\n\nprint (\"[SVM] Tempo Necess\u00e1rio em segundos:\", time_svm)\nsvm_scores","d83534e0":"predict_svm = svm.predict(x_test)\n\nprint(confusion_matrix(y_test, predict_svm))\nprint(classification_report(y_test, predict_svm))\n\nacc_svm = accuracy_score(y_test, predict_svm)\nprint('Accuracy: ', acc_svm)","4f036573":"# In\u00edcio da contagem\nstart = time.time()\n\nboost = AdaBoostClassifier(n_estimators=100)\n\nboost_scores = cross_val_score(boost, x_train, y_train, cv=10)\n\nboost.fit(x_train,y_train)\n\n# T\u00e9rmino da contagem\nend = time.time()","0f6db25c":"time_boost = end-start\n\nprint (\"[AdaBoost] Tempo Necess\u00e1rio em segundos:\", time_boost)\nboost_scores","16ee8f71":"predict_boost = boost.predict(x_test)\n\nprint(confusion_matrix(y_test, predict_boost))\nprint(classification_report(y_test, predict_boost))\n\nacc_boost = accuracy_score(y_test, predict_boost)\nprint('Accuracy: ', acc_boost)","f1251417":"# In\u00edcio da contagem\nstart = time.time()\n\nmlp = MLPClassifier(activation='relu', solver = 'adam')\n\nmlp_scores = cross_val_score(mlp, x_train, y_train, cv=10)\n\nmlp.fit(x_train,y_train)\n\n# T\u00e9rmino da contagem\nend = time.time()","c61544bf":"time_mlp = end-start\n\nprint (\"[MLP] Time elapsed:\", time_mlp)\nmlp_scores","a13e09ae":"predict_mlp = mlp.predict(x_test)\n\nprint(confusion_matrix(y_test, predict_mlp))\nprint(classification_report(y_test, predict_mlp))\n\nacc_mlp = accuracy_score(y_test, predict_mlp)\nprint('Accuracy: ', acc_mlp)","248fd860":"classificadores = ['kNN', 'Logistic Regression', 'Random Forest', 'SVM', 'AdaBoost', 'MLP']\n\nacc = [acc_knn, acc_lr, acc_rf, acc_svm, acc_boost, acc_mlp]","6582bea7":"plt.figure(figsize=(10, 5))\nplt.plot(classificadores, acc, color = 'grey', linewidth = 2, marker = 'o', markersize = 12, markerfacecolor = 'orange')\n\nplt.xlabel('Classificadores')\nplt.ylabel('Acur\u00e1cia')\nplt.title('Perfomance do Classificador')","42adcd14":"time = [time_knn, time_lr, time_rf, time_svm, time_boost, time_mlp]","46b478d5":"plt.figure(figsize=(10, 5))\nplt.plot(classificadores, time, color = 'grey', linewidth = 2, marker = 'o', markersize = 12, markerfacecolor = 'orange')\n\nplt.xlabel('Classificadores')\nplt.ylabel('Tempo de Processamento')\nplt.title('Tempo de Processamento para cada Classificador')","60189276":"# Manipula\u00e7\u00e3o dos dados\nimport pandas as pd\nimport numpy as np\n\n# Visualiza\u00e7\u00e3o \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport time\n%matplotlib inline\n\n# Estat\u00edstica\nimport scipy\nfrom scipy import stats\n\n# Regressores\nimport sklearn\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import tree\n\n# Avalia\u00e7\u00e3o\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error, make_scorer","7cfaa95c":"os.listdir(\"..\/input\/base-californiahousing\")","b0341686":"data = pd.read_csv(\"..\/input\/base-californiahousing\/train.csv\", na_values= \"?\")\n\ndata.head()","e06bc97d":"data.describe()","a6e75bbd":"data.shape","37c3ab6e":"test = pd.read_csv(\"..\/input\/base-californiahousing\/test.csv\", na_values= \"?\")\n\ntest.head()","de90ed34":"test.shape","75d736a9":"data.isnull().sum()","f1f8e2d0":"test.isnull().sum()","748816e9":"aux = data.drop('Id', axis = 1).corr()\nplt.figure(figsize=(12,9))\nsns.heatmap(aux)","fd07d98d":"data_aux = data.drop('Id', axis = 1)","1d8cfd27":"correlation = []\nparam = []\n\nfor i in data_aux.columns:\n    if i != 'median_house_value':\n        corr = stats.pearsonr(data_aux[i], data_aux['median_house_value'])[0]\n        correlation.append(corr)\n        param.append(i)\n    \n    ","1618d74f":"dados = pd.DataFrame({'Correla\u00e7\u00e3o': correlation, 'Par\u00e2metros': param})\ndados = dados.set_index('Par\u00e2metros')\n\ndados","fbd0c056":"features = ['latitude', 'median_age', 'total_rooms', 'median_income']","abe79b02":"x_train = data[features]\ny_train = data['median_house_value']\n\nx_test = test[features]","18909542":"rl = LinearRegression(normalize = 'True')\n\nrl_scores = cross_val_score(rl, x_train, y_train, cv=10)\n\nrl.fit(x_train, y_train)","86e628bb":"rl_scores.mean()","e0c06e34":"y_test = rl.predict(x_test)\n\ny_test","238ddd5a":"data_y = pd.DataFrame(y_test, columns = ['Pre\u00e7o M\u00e9dio Esperado'])\n\nrl_predict = pd.concat([x_test, data_y], axis=1)\n\nrl_predict.head()","2a867e57":"rl.score(x_train, y_train)","bfcbba10":"knn_reg = KNeighborsRegressor(n_neighbors = 30)\n\nknn_reg_scores = cross_val_score(knn_reg, x_train, y_train, cv=10)\n\nknn_reg.fit(x_train, y_train)","bf5099b5":"knn_reg_scores.mean()","69c351d6":"y_test = knn_reg.predict(x_test)\n\ny_test","8927af48":"data_y = pd.DataFrame(y_test, columns = ['Pre\u00e7o M\u00e9dio Esperado'])\n\nknn_predict = pd.concat([x_test, data_y], axis=1)\n\nknn_predict.head()","14bd0975":"arvore = tree.DecisionTreeRegressor()\n\narvore_scores = cross_val_score(knn_reg, x_train, y_train, cv=10)\n\narvore.fit(x_train, y_train)","ed475332":"arvore_scores.mean()","1cfa5421":"y_test = arvore.predict(x_test)\n\ny_test","4481f69d":"data_y = pd.DataFrame(y_test, columns = ['Pre\u00e7o M\u00e9dio Esperado'])\n\narvore_predict = pd.concat([x_test, data_y], axis=1)\n\narvore_predict.head()","42595333":"No gr\u00e1fico acima pode-se observar a acur\u00e1cia obtida atrav\u00e9s da valida\u00e7\u00e3o cruzada conforme o valor de k.","d306b8f4":"Ser\u00e3o testados os seguintes regressores:\n\n- Regress\u00e3o Linear\n- kNN Regressor\n- \u00c1rvore de Decis\u00e3o","122d4920":"An\u00e1lise dos dados faltantes","65b61d88":"#### 4.21 Regress\u00e3o Linear","b147cf9b":"#### 1.22 Teste com o melhor hiperpar\u00e2metro","959c1049":"#### 1.13 Dados de Teste","8415d776":"### 2.4 AdaBoost","1f2c7717":"## 2. Outros Classificadores","23c21753":"As features 'Workclass', 'Occupation' e 'Country' concentram os dados faltantes\n\n* Estrat\u00e9gia: inserir a moda da feature no lugar dos dados faltantes","f3894404":"#### 1.21 Treinamento e Valida\u00e7\u00e3o Cruzada para encontrar o melhor hiperpar\u00e2metro k","8b884e0d":"### 1.2 Classificador k-NN","5641ce76":"* Terceiro Teste: k = 25 usando dados n\u00famericos e n\u00e3o num\u00e9ricos","b46514b0":"Predi\u00e7\u00e3o","2be175cf":"Pelos resultados \u00e9 percept\u00edvel que a mescla de atributos num\u00e9ricos com n\u00e3o num\u00e9ricos apresentou melhor resultado. Agora deve-se encontrar o melhor hiperpar\u00e2metro k","696ccfe4":"Dentre os regressores utilizados a **Regress\u00e3o Linear** teve o melhor desempenho, levando em considera\u00e7\u00e3o a acur\u00e1cia na valida\u00e7\u00e3o cruzada. Devido aos resultados obtidos, conclui-se que n\u00e3o s\u00e3o bons regressores ao problema, uma vez que o melhor resultado obtido apresenta menos de 60% de predi\u00e7\u00e3o correta. No entanto foram testes interessantes no quesito de fixar os conceitos acerca da Regress\u00e3o estudados ao longo da disciplina.","3abdd921":"### 2.2 Random Forest","d2cc6452":"### 3.2 Tempo de Processamento","d8456c64":"### 1.1 Entendendo os dados","d6336270":"#### 4.22 kNN Regressor","29d56490":"### 4.2 Treino dos Regressores","22f3f584":"* Primeiro Teste: k = 5 usando somente dados n\u00famericos","b793da85":"#### 1.14 Transforma\u00e7\u00e3o dos dados n\u00e3o-num\u00e9ricos em valores num\u00e9ricos","ff5742db":"Predi\u00e7\u00e3o a partir da Regress\u00e3o Linear criada","cfe83dc3":"#### 4.3 Vis\u00e3o geral sobre os resultados dos regressores","8e9eb80f":"#### 1.12 An\u00e1lise dos dados faltantes","9a2951df":"A biblioteca sklearn possui um m\u00f3dulo que nos permite usar um modelo de Rede Neural simples, mais precisamente uma MLP, que \u00e9 tratado como uma caixa preta, um dos motivos pelo qual a biblioteca n\u00e3o est\u00e1 entre as mais utilizadas para esse fim.","66e2faba":"## 1. Base Adult","a8801900":"A estrat\u00e9gia de tratamento de dados faltantes ser\u00e1 a mesma que foi aplicada aos dados de treino.\nComo a feature \"fnlwgt\" n\u00e3o \u00e9 relevante, somente a feature \"Relationship\" ser\u00e1 tratada. Em \"Target\" a exclus\u00e3o das linhas \u00e9 mais conveniente.","81792cec":"#### 1.11 An\u00e1lise das features mais relevantes","275f68e8":"Predi\u00e7\u00e3o","afc1e590":"## 4. Base CaliforniaHousing - Extra","6999c97d":"### 2.5 Multilayer Perceptron (MLP)","657c70ad":"Dataset de Teste","123bdced":"O algoritmo mais famoso de Boosting \u00e9 o AdaBoost e tamb\u00e9m foi apresentado na disciplina PMR3508, logo, \u00e9 interessante visualizar sua perfomance para o problema em quest\u00e3o.","c2d6d0c3":"## 3. An\u00e1lise dos Resultados e Conclus\u00f5es","11661864":"### 3.1 Acur\u00e1cia","e1e1c3bd":"### 3.3 An\u00e1lise dos Classificadores","9b2f00cf":"# PMR3508 Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es\n                                 Base Adult - T\u00e9cnicas de Classifica\u00e7\u00e3o\n                                 Base Base CaliforniaHousing - T\u00e9cnicas de Regress\u00e3o\n\n* Autor: PMR3508-2019-42","76b996dd":"Conclus\u00f5es\n\n* Na feature \"Country\" predomina USA, logo, \u00e9 descart\u00e1vel\n\n* \"Education\" descart\u00e1vel, pois h\u00e1 \"Education-Num\"\n\n* \"fnlwgt\" ser\u00e1 descartado, pois n\u00e3o \u00e9 relevante para a predi\u00e7\u00e3o","236455e7":"* Segundo Teste: k = 25 usando somente dados n\u00famericos","fb80b8a9":"### 2.3 Suport Vector Machine","a33f5c96":"Imports necess\u00e1rios:","e4057299":"Mapa de Calor","7cff5fe8":"Coeficiente de Determina\u00e7\u00e3o (R^2) da predi\u00e7\u00e3o","1d550f69":"A partir do Mapa de Calor e do Coeficiente de Correla\u00e7\u00e3o de Person, conclui-se que os atributos\/features mais relevantes para a regress\u00e3o s\u00e3o:\n\n- latitude\n- median_age\n- total_rooms\n- median_income","a7383304":"Coeficiente de Person entre as features e o atributo Target","54fe3aa4":"#### 4.23 \u00c1rvore de Decis\u00e3o","340c236b":"O otimizador que melhor se saiu em termos de acur\u00e1cia foi o 'newton-cg', apesar de ter apresentado problema na converg\u00eancia do algoritmo","d55718f0":"### 4.1 Entendendo os Dados","089373c6":"A seguir segue uma an\u00e1lise geral de cada classificador acerca dos resultados obtidos, tendo como destaque a dificuldade de gerar o classificador, interpretabilidade, tempo de processamento no treinamento e desempenho nas m\u00e9tricas utilizadas.\n\nO primeiro classificador testado foi o **K-Nearest Neighbors (kNN)**, sendo o primeiro classificador estudado na disciplina PMR3508 e que fez parte do 1\u00ba Trabalho atuando justamente sobre o Dataset Adult. Dentre os classificadores utilizados foi aquele que apresentou menor tempo de processamento devido a sua simplicidade, al\u00e9m da melhor acur\u00e1cia, uma vez que apresenta apenas um principal hiperpar\u00e2metro (n\u00famero de vizinhos - k), logo, por valida\u00e7\u00e3o cruzada \u00e9 muito simples de encontrar o melhor hiper\u00e2metro que maximiza a acur\u00e1cia. Em termos de interpretabilidade \u00e9 facilmente compreendido, uma vez que o conceito por tr\u00e1s \u00e9 bem simpl\u00f3rio, apesar de eficiente.\n\nEm seguida foi utilizado a **Regress\u00e3o Log\u00edstica**, t\u00e9cnica bem famosa da fam\u00edlia das regress\u00f5es e que teoricamente se encaixa bem ao problema em quest\u00e3o, uma vez que trabalha com vari\u00e1vel dependente categ\u00f3rica. Apresentou pequeno tempo de processamento sendo o segundo mais r\u00e1pido, por\u00e9m com acur\u00e1cia baixa. Sua interpretabilidade \u00e9 f\u00e1cil, uma vez que apresenta o conceito de regress\u00e3o, ou seja, h\u00e1 a atribui\u00e7\u00e3o de maiores pesos \u00e0s vari\u00e1veis independentes (features) mais relevantes para a classifica\u00e7\u00e3o.\n\nO terceiro algoritmo utilizado foi o **Random Forest** (Floresta Aleat\u00f3ria), sendo este mais complexo que os citamos anteriormente, o que tamb\u00e9m explica o maior tempo de processamento no treinamento, al\u00e9m de ter apresentado uma acur\u00e1cia bem interessante. Tal algoritmo \u00e9 baseado no conceito de \u00c1rvores de Decis\u00e3o, uma vez que v\u00e1rias \u00e1rvores s\u00e3o utilizadas para fazer a classifica\u00e7\u00e3o e a m\u00e9dia delas \u00e9 computada para se tomar a decis\u00e3o. Apesar da constru\u00e7\u00e3o do Random Forest envolver uma composi\u00e7\u00e3o de \u00e1rvores e que para cada \u00e1rvore s\u00e3o escolhidos atributos aleat\u00f3rios, a interpretabilidade ainda \u00e9 um dos pontos de destaque, pois a \u00c1rvore de Decis\u00e3o em si \u00e9 f\u00e1cil de entender e a Floresta Aleat\u00f3ria leva em considera\u00e7\u00e3o a maioria das escolhas de suas \u00e1rvores. \n\nO quarto algoritmo utilizado foi o **Suport Vector Machine** (SVM). No geral foi o pior classificador tanto na acur\u00e1cia quanto em tempo de processamento no treinamento, segundo o segundo mais demorado . Por se tratar de um dos algoritmos mais complexos testados, sua interpretabilidade \u00e9 ruim, principalmente levando em considera\u00e7\u00e3o a atua\u00e7\u00e3o do Kernel, isto \u00e9, hiperpar\u00e2metro que permite tonrar o SVM mais efetivo para espa\u00e7os de maior dimens\u00e3o utilizando fun\u00e7\u00f5es envolvendo os atributos.\n\nEm seguida foi utilizado um famoso algoritmo de Boosting, o **AdaBoost**. Dentre todos os classificadores utilizados foi aquele com o segundo melhor desempenho, levando em considera\u00e7\u00e3o o tempo de processamento no treinamento e a acur\u00e1cia. Seu funcionamento se baseia na utiliza\u00e7\u00e3o de classificadores simples para determinadas amostras do dataset, sendo que apresentam um comportamento sequencial, isto \u00e9, as classifica\u00e7\u00f5es s\u00e3o realizadas sequencialmente e sempre levando em considera\u00e7\u00e3o o que j\u00e1 foi feito anteriormente, sendo uma forma de se adequar aos erros. A ideia em si \u00e9 simples, mas em termos de interpretabilidade n\u00e3o.\n\nPor \u00faltimo procurou-se utilizar uma rede neural b\u00e1sica chamada **Multilayer Perceptron** (MLP). Sua interpretabilidade \u00e9 ruim, uma vez que para leigos \u00e9 uma \"caixa preta\". Apresentou uma acur\u00e1cia razo\u00e1vel, por\u00e9m o tempo de processamento foi o maior entre os classificadores, o que explicita a sua complexidade. A dificuldade na cria\u00e7\u00e3o do classificador \u00e9 o principal detalhe a ser destacado, uma vez que h\u00e1 muitos hiperpar\u00e2metros e a escolha s\u00e1bia deles \u00e9 fundamental para um bom resultado.","c93a585a":"### 2.1 Regress\u00e3o Log\u00edstica"}}