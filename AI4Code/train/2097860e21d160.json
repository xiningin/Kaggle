{"cell_type":{"a7ff5f6e":"code","b282e70e":"code","3383b32e":"code","5b9813ad":"code","4b5a0148":"code","ad1bca1f":"code","5f63b805":"code","2c5f29ee":"code","2443b3ea":"code","b0115709":"code","2693d0e4":"code","e6a46610":"code","b63d75a6":"code","2187fc4e":"code","abb4a7cb":"code","63556b13":"code","a9af4bcd":"code","ee9d17b6":"code","8a1d7a0f":"code","d917ab67":"code","40d82197":"code","390ee950":"code","701184f1":"code","f34567f7":"code","121cc686":"code","36e72acd":"code","961d3d34":"code","008da8b8":"code","1d185e09":"code","4ed7bbf5":"code","879d7b36":"code","bce4ea2a":"code","127748d6":"code","45218ea1":"code","c180865c":"code","4d7621b5":"markdown","413dbc0c":"markdown","37cb3cbd":"markdown","e2f20ecf":"markdown","13a01ac1":"markdown","79be8954":"markdown","cd910dbe":"markdown","7eb73c53":"markdown","7f485a69":"markdown","5181077e":"markdown","15cfe3af":"markdown","4538eab1":"markdown","d9699e77":"markdown","9aed04aa":"markdown","8fd964db":"markdown","3a08a9b2":"markdown","0b9c1bb5":"markdown","77e43935":"markdown","016ee8a2":"markdown","347a6f74":"markdown","5e442a85":"markdown","6500cf12":"markdown","d9e8d46c":"markdown","e17a42bb":"markdown","67a39349":"markdown","5c67a65b":"markdown","b48b3d1c":"markdown","b33ecd2d":"markdown","47b965b3":"markdown","32c73bc9":"markdown","2a2fd100":"markdown","2f86b361":"markdown","a0b5f956":"markdown","88bf4697":"markdown","68c8b2f1":"markdown","3d61d597":"markdown","0e768df9":"markdown","90726bad":"markdown","95e0970f":"markdown","74d95817":"markdown","502d4d33":"markdown","673923a5":"markdown","80e2eed5":"markdown","f280ff1c":"markdown","b9d597e3":"markdown","c97fcfac":"markdown","a7ddf79e":"markdown"},"source":{"a7ff5f6e":"import logging\nimport time\nimport warnings\n\nimport catboost as cb\nimport datatable as dt\nimport joblib\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport seaborn as sns\nimport shap\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.compose import (\n    ColumnTransformer,\n    make_column_selector,\n    make_column_transformer,\n)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import log_loss, mean_squared_error\nfrom sklearn.model_selection import (\n    KFold,\n    StratifiedKFold,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\", level=logging.INFO\n)\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"float_format\", \"{:.5f}\".format)","b282e70e":"# For regression\ndiamonds = sns.load_dataset(\"diamonds\")\n\nX, y = diamonds.drop(\"price\", axis=1), diamonds[[\"price\"]].values.flatten()\n\n# Encode cats\noe = OrdinalEncoder()\ncats = X.select_dtypes(exclude=np.number).columns.tolist()\nX.loc[:, cats] = oe.fit_transform(X[cats])\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X, y, test_size=0.15, random_state=1121218\n)","3383b32e":"X_train.head()","5b9813ad":"X_train.shape, X_valid.shape","4b5a0148":"model = xgb.XGBRegressor(n_estimators=1000, tree_method=\"gpu_hist\").fit(\n    X_train, y_train\n)","ad1bca1f":"preds = model.predict(X_valid)\nrmse = mean_squared_error(y_valid, preds, squared=False)","5f63b805":"rmse","2c5f29ee":"# Create a tree explainer\nxgb_explainer = shap.TreeExplainer(\n    model, X_train, feature_names=X_train.columns.tolist()\n)","2443b3ea":"xgb_explainer","b0115709":"%%time\n\n# Shap values with tree explainer\nshap_values = xgb_explainer.shap_values(X_train, y_train)","2693d0e4":"shap_values.shape","e6a46610":"%%time\n\n# Shap values with XGBoost core moedl\nbooster_xgb = model.get_booster()\nshap_values_xgb = booster_xgb.predict(xgb.DMatrix(X_train, y_train), pred_contribs=True)","b63d75a6":"shap_values_xgb.shape","2187fc4e":"shap_values_xgb = shap_values_xgb[:, :-1]\n\npd.DataFrame(shap_values_xgb, columns=X_train.columns.tolist()).head()","abb4a7cb":"shap.summary_plot(\n    shap_values_xgb, X_train, feature_names=X_train.columns, plot_type=\"bar\"\n)","63556b13":"pd.DataFrame(shap_values_xgb, columns=X_train.columns)[\"carat\"].abs().mean()","a9af4bcd":"xgb.plot_importance(booster_xgb);","ee9d17b6":"fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n\nfor ax, imp_type in zip(axes.flatten(), [\"weight\", \"gain\", \"cover\"]):\n    xgb.plot_importance(\n        booster_xgb,\n        ax=ax,\n        importance_type=imp_type,\n        title=f\"Importance type - {imp_type}\",\n    )\n\nplt.show();","8a1d7a0f":"shap.summary_plot(shap_values_xgb, X_train, feature_names=X_train.columns);","d917ab67":"shap.dependence_plot(\"carat\", shap_values_xgb, X_train, interaction_index=None)","40d82197":"shap.dependence_plot(\"carat\", shap_values_xgb, X_train, interaction_index=\"auto\");","390ee950":"shap.dependence_plot(\"color\", shap_values_xgb, X_train, feature_names=X_train.columns);","701184f1":"%%time\n\n# SHAP interactions with XGB\ninteractions_xgb = booster_xgb.predict(\n    xgb.DMatrix(X_train, y_train), pred_interactions=True\n)","f34567f7":"interactions_xgb.shape","121cc686":"def get_top_k_interactions(feature_names, shap_interactions, k):\n    # Get the mean absolute contribution for each feature interaction\n    aggregate_interactions = np.mean(np.abs(shap_interactions[:, :-1, :-1]), axis=0)\n    interactions = []\n    for i in range(aggregate_interactions.shape[0]):\n        for j in range(aggregate_interactions.shape[1]):\n            if j < i:\n                interactions.append(\n                    (\n                        feature_names[i] + \"-\" + feature_names[j],\n                        aggregate_interactions[i][j] * 2,\n                    )\n                )\n    # sort by magnitude\n    interactions.sort(key=lambda x: x[1], reverse=True)\n    interaction_features, interaction_values = map(tuple, zip(*interactions))\n\n    return interaction_features[:k], interaction_values[:k]\n\n\ntop_10_inter_feats, top_10_inter_vals = get_top_k_interactions(\n    X_train.columns, interactions_xgb, 10\n)","36e72acd":"top_10_inter_feats","961d3d34":"def plot_interaction_pairs(pairs, values):\n    plt.bar(pairs, values)\n    plt.xticks(rotation=90)\n    plt.tight_layout()\n    plt.show();","008da8b8":"top_10_inter_feats, top_10_inter_vals = get_top_k_interactions(\n    X_train.columns, interactions_xgb, 10\n)\n\nplot_interaction_pairs(top_10_inter_feats, top_10_inter_vals)","1d185e09":"random_idx = np.random.randint(len(X_train))\nrandom_idx","4ed7bbf5":"%%time\n\n# Recalculate SHAP values\nshap_explainer_values = xgb_explainer(X_train, y_train)","879d7b36":"type(shap_explainer_values)","bce4ea2a":"shap.waterfall_plot(shap_explainer_values[6559])","127748d6":"shap.waterfall_plot(shap_explainer_values[4652])","45218ea1":"shap.initjs()  # don't forget to enable JavaScript\n\nshap.force_plot(shap_explainer_values[4652])","c180865c":"shap.force_plot(shap_explainer_values[6559])","4d7621b5":"Finally, we get to the local interpretability section. It is all about explaining why the model got to a particular decision for a sample.\n\nLet's choose a random diamond and its predicted price to explain:","413dbc0c":"We can get a deeper insight into each feature's effect on the entire dataset with dependence plots. Let's see an example and explain it later:","37cb3cbd":"But that's not much different than the feature importances plot you would get from XGBoost:","e2f20ecf":"As we can see, the interactions between y and carat is much stronger than others. Even though this plot doesn't mean anything to us, it might be possible for a domain expert to decipher this relationship and use it to diagnose the model better.\n\nFor example, if your model tried to classify molecules' reactions to different chemical stimuli, a plot like this can be helpful because it might show which chemical properties of the molecules are interacting with the stimulus. This will tell a lot to the domain expert running the experiments since they know what types of chemicals interact and whether the model could capture their behavior.","13a01ac1":"It seems that the carat interacts with the clarity of the diamonds much stronger than other features.\n\nLet's now create a dependence plot for categorical features:","79be8954":"This plot aligns with what we saw in the summary plot before. As carat increases, its SHAP value increases. By changing the `interaction_index` parameter to `auto`, we can color the points with a feature that most strongly interacts with carat:","cd910dbe":"After extracting the core booster model of XGBoost, it only took about a second to calculate Shapley values for 45k samples:","7eb73c53":"# Setup","7f485a69":"Now, let's finally take a peek behind the curtains and calculate the Shapley values for the training set.\n\nWe start by creating an explainer object for our model:","5181077e":"Now, `top_10_inter_feats` contains 10 of the strongest interactions between all possible pairs of features:","15cfe3af":"That's where we are wrong. You can't trust feature importances from XGBoost because they are inconsistent across different calculations. Watch how feature importances change with the calculation type:","4538eab1":"Let's see which physical measurements of diamonds are the most important when determining price:","d9699e77":"SHAP (SHapley Additive exPlanations) is a Python package based on the 2016 NIPS paper about SHAP values. The premise of this paper and Shapley values comes from approaches in game theory.\n\nOne of the questions often posed in games is that in a group of n players with different skillsets, how do we divide a prize so that everyone gets a fair share based on their skill set? Depending on the number of players, their time of joining the game, and their different contributions to the outcome, this type of calculation can become horribly complex.\n\nBut what does game theory have to do with machine learning? Well, we could reframe the above question so that it becomes \"Given a prediction, how do we most accurately measure each feature's contribution?\" Yes, it is like asking feature importances of a model, but the answer the Shapley values give is much more sophisticated.\n\nSpecifically, Shapley values can help you in:\n1. *Global model interpretability* - imagine you work for a bank and build a classification model for loan applications. Your manager wants you to explain what (and how) different factors influence the decisions of your model. Using SHAP values, you can give a concrete answer with details of which features lead to more loans and which features lead to more rejections. You make your manager happy because now, he can draw up basic guidelines for future bank customers to increase their chances of getting a loan. More loans mean more money means a happier manager means a higher salary for you.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/900\/1*tLpb2EdoT_-ONIA9o1U7-g.png)\n\n2. *Local interpretability* - your model rejects one of the applications submitted to the bank a few days ago. The customer claims he followed all the guidelines and was sure to get a loan from your bank. Now, you are legally obligated to explain why your model rejected that particular candidate. Using Shapley values, every case can be analyzed independently, without worrying about its connections to other samples in the data. In other words, you have local interpretability. You extract the Shapley values for the complaining customer and show them what parts of their application caused the rejection. You prove them wrong with a plot like this:\n\n![](https:\/\/cdn-images-1.medium.com\/max\/900\/1*HTUJCmyAfYfv7iv6-soGGg.png)\n\n\nSo, how do you calculate the mighty Shapley values? That's where we start using the SHAP package.","9aed04aa":"# Global Feature Importances with SHAP","8fd964db":"This is just an ordered, organized version of waterfall plots. All negative and positive bars are grouped to either side of the predicted price. Again, the base value shows the mean price, and the bars show how much each feature property shifts that value.","3a08a9b2":"![](https:\/\/cdn-images-1.medium.com\/max\/900\/1*m0-c5e45bQH7bigxoSOnyQ.gif)","0b9c1bb5":"The exact mathematical details of calculating Shapley values deserve an article of its own. Therefore, for now, I will be standing on the shoulder of giants and refer you to their posts. They are guaranteed to solidify your understanding of the concepts ([1](https:\/\/towardsdatascience.com\/shap-explained-the-way-i-wish-someone-explained-it-to-me-ab81cc69ef30), [2](https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d)). \n\nIn practice, however, you will rarely refer to the math behind Shapley values. The reason is that all the magical details are nicely packaged inside SHAP. So, let's look at our very first example.\n\nUsing the Diamonds dataset built into Seaborn, we will be predicting diamond prices using several physical measurements. I processed the dataset beforehand and divided it into train and validation sets. Here is the training set:","77e43935":"# Local interpretability","016ee8a2":"# What is SHAP and Shapley values?","347a6f74":"By setting `pred_interactions` to True, we get SHAP interaction values in only 12 seconds. It is a 3D array, with the last column axes being the bias terms:","5e442a85":"# How to calculate Shapley values with SHAP?","6500cf12":"# Exploring each feature with dependence plots","d9e8d46c":"Now, let's explain the random diamond we picked out with a waterfall plot:","e17a42bb":"# Feature Interactions with Shapley values","67a39349":"> Note that LightGBM also has GPU support for SHAP values in its `predict` method. In CatBoost, it is achieved by calling `get_feature_importances` method on the model with `type` set to `ShapValues`.","5c67a65b":"But wait\u200a-\u200athe Shap values from the tree explainer had nine columns; this one has 10! Don't worry; we can safely ignore the last column for now, as it just contains the bias term which XGBoost adds by default:","b48b3d1c":"We got the Shapley values; now what? Now, we get plottin'.","b33ecd2d":"In contrast, feature importances obtained from Shapley values are consistent and trustworthy.\n\nWe won't also stop here. In the above plots, we only looked at absolute values of importance. We don't know which feature positively or negatively influences the model. Let's do that with SHAP `summary_plot`:","47b965b3":"One of the most fantastic attributes of SHAP and Shapley values is their ability to find relationships between features accurately. We have already got a taste of that in the last section when SHAP found the most robust interacting feature in dependence plots.\n\nWe can go a step further and find all feature interactions ordered by their interaction strength. For that, we need a different set of values\u200a-\u200aSHAP interaction values.\n\nThey can be calculated using the `shap_interaction_values` of the tree explainer object like so:\n\n```python\ninteractions = xgb_explainer.shap_interaction_values(X_train, y)\n```\n\nbut this is even more time-consuming than regular SHAP values. So, we will turn to GPUs once more with XGBoost:","32c73bc9":"OK, it looks like we will be looking at the 6559th (changed because of random seed) diamond in the training data. Let's start:","2a2fd100":"We can create another function that plots these pairs based on their interaction strengths:","2f86b361":"# Model Explainability with SHAP: A Guide to Those Who Are Serious About Machine Learning\n# How to Explain Any Model With SHAP Even If You Are a Beginner\n## Explain any black box model to non-technical people\n![](https:\/\/cdn-images-1.medium.com\/max\/1350\/1*TCsozDAfZrM2VjnwMID2zw.jpeg)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Photo by \n        <a href='https:\/\/www.pexels.com\/@iriser?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Irina Iriser<\/a>\n        on \n        <a href=https:\/\/www.pexels.com\/photo\/blue-and-red-jellyfish-artwork-1086583\/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels''><\/a>\n    <\/strong>\n<\/figcaption>","a0b5f956":"The carat stands out as the driving factor for a diamond's price. Reading the axis title below, we see that the importances are just the average absolute Shapley values for a feature. We can check that below:","88bf4697":"Now we got the interactions; what do we do? Frankly, even the SHAP documentation doesn't outline a reasonable use-case for interactions, but we get help from others. Specifically, we will use a function I have learned from [4x Kaggle Grandmaster Bojan Tunguz](https:\/\/www.kaggle.com\/tunguz) to find the strongest feature interactions in our dataset and plot them:","68c8b2f1":"# Motivation","3d61d597":"# Summary","0e768df9":"Cut, color, and clarity are categorical features. They are encoded ordinally as their orders have meaning to the context and, ultimately, the model decision.\n\nAs a baseline, we fit an XGBRegressor model and evaluate the performance with Root Mean Squared Error:","90726bad":"Here is how to interpret the above plot:\n\n1. The left vertical axis denotes feature names, ordered based on importance from top to bottom.\n2. The horizontal axis represents the magnitude of the SHAP values for predictions.\n3. The vertical right axis represents the actual magnitude of a feature as it appears in the dataset and colors the points.\n\nWe see that as carat increases, its effect on the model is more positive. The same is true for y feature. The x and z features are a bit tricky with a cluster of mixed points around the center.","95e0970f":"This diamond is much cheaper than the previous one, mainly because its carat is much lower, as can be seen above.\n\nThere is another plot to explain local interpretability. The SHAP calls it force plot, and it looks like this:","74d95817":"Today, you can't just come up to your boss and say, \"Here is my best model. Let's put it into production and be happy!\". No, it doesn't work that way now. Companies and businesses are being picky over adopting AI solutions because of their \"black box\" nature. They **demand** model explainability.\n\nIf ML specialists are coming up with tools to understand and explain the models *they* created, the concerns and suspicions of non-technical folks are entirely justified. One of those tools introduced a few years ago is SHAP. It can break down the mechanics of any machine learning model and deep neural net to make them understandable to anyone.\n\nToday, we will learn how SHAP works and how you can use it for classical ML tasks in your practice.","502d4d33":"We first recalculate the SHAP values using the explainer object. This is different than `shap_values` function, because this time, the Shapley values are returned with a few more properties we need for local interpretability:","673923a5":"`TreeExplainer` is a special class of SHAP, optimized to work with any tree-based model in Sklearn, XGBoost, LightGBM, CatBoost, and so on. You can use `KernelExplainer` for any other type of model, though it is slower than tree explainers.\n\nThis tree explainer has many methods, one of which is `shap_values`:","80e2eed5":"This plot also goes in hand with the summary plot. The latest color categories affect the prices negatively while interacting with carat.\n\nI will let you explore the dependence plots for other features below:","f280ff1c":"As I have said, calculating Shapley values is a complex process, which is why it took ~22 mins (local machine) for just 45k observations on the CPU. For large modern datasets with hundreds of features and millions of samples, the calculation can take days. So, we turn to GPUs to calculate the SHAP values.\n\nAs of now, GPU support is not stable in SHAP, but we have a workaround. The `predict` method of the core XGBoost model has `pred_contribs` argument, which, when set to True, calculates SHAP values on GPUs:","b9d597e3":"<p float=\"left\">\n  <img src=\"https:\/\/cdn-images-1.medium.com\/max\/450\/1*D5bClhlRvUfPTT-D41jBEw.png\" width=\"250\" height=\"250\"\/>\n  <img src=\"https:\/\/cdn-images-1.medium.com\/max\/450\/1*2z6GYbYhCAZujHw3_0GFcQ.png\" width=\"250\" height=\"250\"\/> \n  <img src=\"https:\/\/cdn-images-1.medium.com\/max\/450\/1*rjecV-75_5YftqCKeyNEZA.png\" width=\"250\" height=\"250\"\/>\n<\/p>\n<p float=\"left\">\n  <img src=\"https:\/\/cdn-images-1.medium.com\/max\/450\/1*pM59RXcHnHY5x-OOgYfgSw.png\" width=\"250\" height=\"250\"\/>\n  <img src=\"https:\/\/cdn-images-1.medium.com\/max\/450\/1*5F59k8ujenHhYqWLCGhGjg.png\" width=\"250\" height=\"250\"\/>\n  <img src=\"https:\/\/cdn-images-1.medium.com\/max\/450\/1*EMZLpKV1pPaXcNP5W--acg.png\" width=\"250\" height=\"250\"\/>\n<\/p>","c97fcfac":"Now, you can come up to your boss and say, \"Here is my best model, and here is the explanation of why it is the best and how it works. Let's put it into production and be happy!\" Hopefully, the response you get will be much more positive.\n\n![medium_cta.gif](https:\/\/cdn-images-1.medium.com\/max\/900\/1*KeMS7gxVGsgx8KC36rSTcg.gif)","a7ddf79e":"The `E[f(x)] = 3826.69` is the mean prediction of diamond prices for the train set, e.g. `preds.mean()`.  The `f(x) = 3250.015` is the predicted price for the diamond.\n\nThe thin line in the middle denotes the mean prediction. The vertical axis shows the feature values of the 6559th diamond. The bars represent how each feature property shifted the price from the mean prediction. The red bars represent positive shifts; the blue bars represent negative shifts.\n\nLet's look at another diamond for completeness:"}}