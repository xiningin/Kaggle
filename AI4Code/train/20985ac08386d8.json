{"cell_type":{"13bdd813":"code","5c802582":"code","a48484ca":"code","bbe41c70":"code","7b3032d0":"code","6567b2ec":"code","3df65e33":"code","c0cacff3":"code","1d59d92b":"code","73580551":"code","22e22af6":"code","ce1e4619":"code","1c3a6102":"code","07348301":"code","eae8a9da":"code","5c4e7fb1":"markdown","ebf46f99":"markdown","3daf86f9":"markdown","09a002fc":"markdown","1f134fe7":"markdown","2492213b":"markdown","c3f09532":"markdown","f31b5b89":"markdown","45fb02fb":"markdown","68adb796":"markdown","d3bfb339":"markdown","d9ae903e":"markdown","f29cbe9a":"markdown","5c95ac0e":"markdown","80967ced":"markdown","f8ecf66b":"markdown","914e0378":"markdown","b0125f25":"markdown","3d696c66":"markdown","35a7daca":"markdown","080ba86f":"markdown","371225f7":"markdown","93e0b3d5":"markdown","8ecf87ed":"markdown","72f626b7":"markdown"},"source":{"13bdd813":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib.pyplot import step, show\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5c802582":"data0 = pd.read_csv('..\/input\/titanic\/train.csv', index_col='PassengerId')\ndata_test0 = pd.read_csv('..\/input\/titanic\/test.csv', index_col='PassengerId')\n\ndata0.head()","a48484ca":"data = data0.drop(['Name', 'Cabin', 'Ticket'], axis=1)\ndata_test = data_test0.drop(['Name', 'Cabin', 'Ticket'], axis=1)\n\ndata.head()","bbe41c70":"data.dropna(axis=0, subset=['Survived', 'Pclass','Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked'], inplace=True)\ndata.tail()","7b3032d0":"import matplotlib.ticker as ticker\n\n# Set the width and height of the figure\nplt.figure(figsize=(15,6))\n\n# Add title\nplt.title(\"Distribution of Survived Ages\")\n\n# Bar chart showing Distribution of Survived Ages\ng = sns.barplot(x=data['Age'], y=data['Survived'])\ng.xaxis.set_major_locator(ticker.MultipleLocator(5))\ng.xaxis.set_major_formatter(ticker.ScalarFormatter())\n\n# Add label for vertical axis\nplt.ylabel(\"Survived or died\")","6567b2ec":"# Set the width and height of the figure\nplt.figure(figsize=(14,7))\n\n# Add title\nplt.title(\"Scatter Plots of Survived Ages in regards of genders\")\n\n# Scatter Plots showing Survived Ages in regards of genders\nsns.scatterplot(x=data['Age'], y=data['Survived'], hue=data['Sex'])","3df65e33":"from sklearn.preprocessing import OrdinalEncoder\n\n\n# Get list of categorical variables\ns = (data.dtypes == 'object')\nobject_cols = list(s[s].index)\n\ns_test = (data_test.dtypes == 'object')\nobject_cols_test = list(s_test[s_test].index)\n\nordinal_encoder = OrdinalEncoder()\ndata[object_cols] = ordinal_encoder.fit_transform(data[object_cols])\ndata_test[object_cols_test] = ordinal_encoder.fit_transform(data_test[object_cols_test])\n\ndata_test.head()","c0cacff3":"import xgboost as xgb\nfrom xgboost import plot_importance, plot_tree\n\n# Select subset of predictors\ncols_to_use = ['Pclass','Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked']\nX = data[cols_to_use]\n\n# Select target\ny = data[['Survived']]\n\n# Separate data into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.70, test_size=0.30)\n\n_ = y_valid \\\n    .rename(columns={\"Survived\": \"TEST SET\"}) \\\n    .join(y_train.rename(columns={'Survived': 'TRAINING SET'}), how='outer') \\\n    .plot(figsize=(16,5), title='Survived', style='-')\n\nreg = xgb.XGBRegressor(n_estimators=800)\nreg.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)], verbose=False)\n\n","1d59d92b":"_ = plot_importance(reg, height=0.8)","73580551":"y_test = reg.predict(X_valid)\n\n\nplt.figure(figsize=(14,6))\nsns.lineplot(x = X_valid.index, y = y_valid['Survived'], label=\"Validation\")\nax2 = plt.twinx()\nsns.lineplot(x = X_valid.index, y = y_test, ax=ax2, label=\"Prediction\", color=\"r\")\n","22e22af6":"for i in range(len(y_test)) :\n    if y_test[i] < 0.5:\n        y_test[i] = 0\n    else:\n        y_test[i] = 1\n\nplt.figure(figsize=(14,6))\nsns.lineplot(x = X_valid.index, y = y_valid['Survived'], label=\"Validation\")\nax2 = plt.twinx()\nsns.lineplot(x = X_valid.index, y = y_test, ax=ax2, label=\"Prediction\", color=\"r\")","ce1e4619":"from sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom sklearn.metrics import accuracy_score\n\nRMSE = mean_squared_error(y_true=y_valid['Survived'], y_pred=y_test)\nMAE = mean_absolute_error(y_true=y_valid['Survived'], y_pred=y_test)\nAc_sc = accuracy_score(y_valid, y_test)\n\nprint('RMSE:', RMSE)\nprint('MAE:', MAE)\nprint('Accuracy score:', Ac_sc)","1c3a6102":"#the test.csv file data was already pre processed eariler, the variable who's storing this called data_test\n\n# Select subset of predictors\nX_pred = data_test[cols_to_use]\n\nour_prediction = reg.predict(X_pred)\n\nplt.figure(figsize=(14,6))\nsns.lineplot(x = X_pred.index, y = our_prediction, label=\"Validation\")","07348301":"for i in range(len(our_prediction)) :\n    if our_prediction[i] < 0.5:\n        our_prediction[i] = 0\n    else:\n        our_prediction[i] = 1\n\nplt.figure(figsize=(14,6))\nsns.lineplot(x = X_pred.index, y = our_prediction, label=\"Validation\")","eae8a9da":"df = pd.DataFrame({'PassengerId': data_test.index,\n                   'Survived': our_prediction})\n\ndf.to_csv('mycsvfile.csv',index=False, decimal='')\ndf","5c4e7fb1":"# XGBoost model ","ebf46f99":"As we can guess, the column \"Name\" and \"Cabin\", i.e. name of the passenger and thier cabin number, will not be correlated to whether the passanger will survive or not. so we will drop this column.\n\nThe same thing can be said about the Ticket number, the coulmn \"Ticket\" cannot be converted into a categorical variable column, since every ticket has it own letters and numbers combination.","3daf86f9":"> Scatter Plots of Survived Ages in regard to genders","09a002fc":"The RMSE shows that our model is 15% inaccurate or 85% accurate, which can be considered as a good value for our prediction.","1f134fe7":"# Categorical Variables\n\nLets now convert the 'Embarked' and 'Sex' column to categorical variables, this can be done based on the informations given in the Data Dictionary i.e. C = Cherbourg, Q = Queenstown, S = Southampton. or Male Female.\n\nWe will do that with the simple Ordinal encoding approche, which assigns each unique value to a different integer.","2492213b":"# Feature Importances\n\nWe can now see which feature the model rely on to fit the target, feature importances plot is a great way to better understand which of the features is correlated to the targer.","c3f09532":"as the scatter plot shows, the majority of survivors were females, no matter what age they had, they were in the first lines of passangers that had the chance to be evacuated with the few rowboats available.\n\n\"Women and children first\", known to a lesser extent as the Birkenhead Drill, is a code of conduct dating from 1852, whereby the lives of women and children were to be saved first in a life-threatening situation, typically abandoning ship, when survival resources such as lifeboats were limited. [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Women_and_children_first)","f31b5b89":"# Prediction for submition\nFinally we will predict the test.csv set, then generate a csv (gender_submission.csv) in order to submite our prediction and get a score!","45fb02fb":"Lets convert this prediction to a binary result in order to submit it.","68adb796":"Now that we have all the features set to the right format (i.e. categorical variables), we can start training our model using XGBoost (Gradient boosting) which is a method that goes through cycles to iteratively add models into an ensemble.\n\nIt begins by initializing the ensemble with a single model, whose predictions can be pretty naive. (Even if its predictions are wildly inaccurate, subsequent additions to the ensemble will address those errors.)","d3bfb339":"# Data pre-processing for better understanding.","d9ae903e":"Bingo! our model rely more on Age and Fare to fit the target, however we can see that sex was the 3rd most used feature, that explain the hypothesis we discussed above about when women and children were to be saved first in a life-threatening situation.","f29cbe9a":"We should generate a csv file with exactly 2 columns:\n* PassengerId (sorted in any order)\n* Survived (contains your binary predictions: 1 for survived, 0 for deceased)","5c95ac0e":"The plot above shows non-binary outpout values for the predicted set, so later we will consider every predicted value that greater than .5 as a survived result, and the opposite is true.","80967ced":"# Error Metrics\nIn order for us to evaluate how well our model is doing, we will calcute :\n1. Root Mean Square Error (RMSE).\n2. Mean absolute error (MAE).\n3. Accuracy score. ([Accuracy](https:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision#In_binary_classification))\n\nBoth are negativly-oriented scores, means lower values are better.  \n","f8ecf66b":"As we noticed above, probably the survivors within the fraction of age between 20 and 70 were all or mostly ladies, to show this we will performe a Scatter plot to shows the three features (i.e. survived, age, gender) within the same plot. lets do that!","914e0378":"# Forecast on Validation Set\n\nForecasting the X_valid set we split earlier, plotting this forecasted y_test in comparaison to the real y_valid set will reveal how well our model is doing. ","b0125f25":"> Distribution of Survived Ages","3d696c66":"> Variable Notes\n* pclass: A proxy for socio-economic status (SES)\n  * 1st = Upper\n  * 2nd = Middle\n  * 3rd = Lower\n\n\n* age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n* sibsp: The dataset defines family relations in this way\n  * Sibling = brother, sister, stepbrother, stepsister\n  * Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n\n* parch: The dataset defines family relations in this way\n  * Parent = mother, father\n  * Child = daughter, son, stepdaughter, stepson\n  \n  \n* Some children travelled only with a nanny, therefore parch=0 for them.","35a7daca":"# Data Plotting","080ba86f":"Data cleaning, plotting and pre-processing using pandas, numpy, and seaborn.","371225f7":"We first will select features we will be using to make predictions, as well as the target, then we will split the data (70\/30 ratio) into training and validation sets, then using XGBoost the model will fit the features to the target which is in our case even the passanger survived the titanic sinking or not.","93e0b3d5":"In this step we will plot the different features, thier distributions, scatter plots..etc.","8ecf87ed":"Now lets drop the NaN values from the every given feature  ","72f626b7":"As we can see, the passangers whose age was between 0-20 had more chance of surviving, as we all see in the movie, they tried to evacuate ladies and childrens first. so probably that what happend irl too.  \n\nWe can notice a slitly increase for ages greater than 75."}}