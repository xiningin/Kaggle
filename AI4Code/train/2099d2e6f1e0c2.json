{"cell_type":{"215da2a4":"code","0f0bc3c8":"code","ad69b870":"code","304e262f":"code","069ef6b2":"code","a6b6644d":"code","91073ac8":"code","1b1ac479":"code","600058e1":"code","0bd02cdc":"code","11bdc6c2":"code","64e19bcd":"code","11bd1920":"code","8cc3709e":"code","6b8333a1":"code","35a2a9e9":"code","6a6ab3a8":"code","8530c18c":"code","a77b8ff2":"code","32b4ba26":"code","146e4292":"code","603bb7b6":"code","b049ac37":"code","5e2d7c9b":"code","38b6c545":"code","48dec1aa":"code","7f9cc19a":"code","85a027ca":"code","1eaa741c":"code","072a763f":"code","0e2e925b":"code","e34d5c7a":"code","045b9926":"code","dfb461ba":"code","86f9bd58":"code","09cf9413":"code","deffde15":"code","599595c8":"code","b16d2113":"code","87f2ad60":"code","2783c5e8":"code","95cf8289":"code","ab864e64":"code","2fe59c08":"code","cea118ab":"code","e5be2748":"markdown","9bff839b":"markdown","1552d164":"markdown"},"source":{"215da2a4":"# Import the libraries\n\nimport pandas as pd\nimport numpy as np\nimport io ","0f0bc3c8":"# Load the dataset \n\npeople = pd.read_csv('..\/input\/predicting-red-hat-business-value\/people.csv.zip', sep = ',')\npeople.head()\n\n# We see much categorical data\n# The informacion is anonymized","ad69b870":"activity = pd.read_csv('..\/input\/predicting-red-hat-business-value\/act_train.csv.zip', sep = ',')\nactivity.head()","304e262f":"# Show the shape\n\nprint(people.shape)\n\n# Show the null percent\n\n100*people.isnull().sum()\/people.shape[0]","069ef6b2":"# We repet with the other dataset \n\nprint(activity.shape)\n100*activity.isnull().sum()\/activity.shape[0]\n\n# We gonna delete the columns 90% null and fill ","a6b6644d":"activity.drop(columns=['char_1','char_2','char_3','char_4','char_5','char_6','char_7','char_8','char_9'],inplace=True)\n\nprint(activity.shape)","91073ac8":"activity.head() ","1b1ac479":"# Fill char_10 with the mode\n\nactivity['char_10'] = activity['char_10'].fillna(activity[\"char_10\"].mode()[0])","600058e1":"# We data is clean of null dates\n\n100*activity.isnull().sum()\/activity.shape[0]","0bd02cdc":"# Rename the columns \n\nactivity = activity.rename(columns={\"date\":\"data_activity\",\"char_10\":\"activity_type\"})\nactivity.head()","11bdc6c2":"# We gonna use merge to join the dataframes\n\nall_data = activity.merge(people,on=[\"people_id\"], how=\"inner\")\nall_data.shape","64e19bcd":"# Show the target  \n\nall_data[\"outcome\"]","11bd1920":"#  Show the distribucion in the target \n\n100*all_data[\"outcome\"].value_counts()\/all_data.shape[0]","8cc3709e":"# Show the type of variable \n\ntypes = pd.DataFrame(all_data.dtypes)\nprint(\"Types of variables: \", types.groupby(0).size())\n\n# We have to convert float to int ","6b8333a1":"all_data = all_data.replace({False: 0, True: 1})","35a2a9e9":"# Is ready\n\ntypes = pd.DataFrame(all_data.dtypes)\nprint(\"Types of variables replace: \", types.groupby(0).size()) ","6a6ab3a8":"# We gonna apply one second replace, As the identifier people_id it consists of a prefix \"ppl_\" followed by a unique number per user. \n# In this case, it is enough to cut the prefix to transform this variable into a numeric one.\n\nall_data.people_id = all_data.people_id.str.slice(start=4).astype(float).astype(int)\n\ntypes = pd.DataFrame(all_data.dtypes)\nprint(\"Second replace: \",types.groupby(0).size())","8530c18c":"all_data[[\"activity_id\", \"activity_category\", \"group_1\", \"activity_type\"]].head(3)","a77b8ff2":"# And We have to do the same for those variables \n\nall_data.activity_id = all_data.activity_id.str.slice(start=5).astype(float).astype(int)\nall_data.activity_category = all_data.activity_category.str.slice(start=5).astype(float).astype(int)\nall_data.group_1 = all_data.group_1.str.slice(start=6).astype(float).astype(int)\nall_data.activity_type = all_data.activity_type.str.slice(start=5).astype(float).astype(int)\n\ntypes = pd.DataFrame(all_data.dtypes)\nprint(\"Thith\",types.groupby(0).size()) ","32b4ba26":"all_data.head()","146e4292":"# We are going to evaluate the number of different variables\n\ncategorics = types.index[types[0] == 'O'].values \nfor line in categorics:\n    print(\"The variable \"+ line +\"contine: \", str(len(all_data[line].unique()))+\" distinct values\")","603bb7b6":"all_data.head()","b049ac37":"all_data.date","5e2d7c9b":"# We gonna create stationary variables\n\n# convert the object variable to datetime \nall_data[\"date\"] = pd.to_datetime(all_data[\"date\"])\n\n# Create new variables \nall_data[\"day\"] = all_data[\"date\"].dt.day\nall_data[\"day_of_week\"] = all_data[\"date\"].dt.weekday\nall_data[\"week\"] = all_data[\"date\"].dt.week\nall_data[\"month\"] = all_data[\"date\"].dt.month\nall_data[\"trimester\"] = all_data[\"date\"].dt.quarter\nall_data[\"year\"] = all_data[\"date\"].dt.year","38b6c545":"all_data.head()","48dec1aa":"# Repet the same but with data_activity\n\nall_data[\"data_activity\"] = pd.to_datetime(all_data[\"data_activity\"])\nall_data[\"activity_day\"] = all_data[\"data_activity\"].dt.day\nall_data[\"activity_day_of_week\"] = all_data[\"data_activity\"].dt.weekday\nall_data[\"activity_week\"] = all_data[\"data_activity\"].dt.week\nall_data[\"activity_month\"] = all_data[\"data_activity\"].dt.month\nall_data[\"activity_trimester\"] = all_data[\"data_activity\"].dt.quarter\nall_data[\"activity_year\"] = all_data[\"data_activity\"].dt.year","7f9cc19a":"#Delete the original date columns\n\ndel(all_data[\"date\"])\ndel(all_data[\"data_activity\"])\n\ntypes = pd.DataFrame(all_data.dtypes)\nprint(\"Types of variables later of 4to remplace\",types.groupby(0).size())","85a027ca":"all_data.head()","1eaa741c":"all_data.dtypes","072a763f":"# We are going to evaluate the number of different variables again \n\ncategorics = types.index[types[0] == 'O'].values \nfor line in categorics:\n    print(\"The variable \"+ line +\"contine: \", str(len(all_data[line].unique()))+\" distinct values\")","0e2e925b":"# We gonna use one hot encoder for the rest of variables \n\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\n\n# Define dataframe's function and the column to return a dataframe later OHE\ndef crea_OneHotEncoding(df, column):\n  le = LabelEncoder()\n  le_ajustado=le.fit_transform(df[column]).reshape(-1,1)\n  encoder = OneHotEncoder(sparse=False)\n  column = [column+ \"_\"+ str(i) for i in le.classes_]\n  data = encoder.fit_transform(le_ajustado)\n  return(pd.DataFrame(data,columns =column))","e34d5c7a":"numeric_columns = list(set(types.index[types[0] ==\"int64\"].values) - set([\"outcome\"]))\nall_data_finish = all_data[numeric_columns]\nobjetive = all_data[\"outcome\"]\n\ncategories = types.index[types[0] == 'O'].values\nfor column in categories:\n  df = crea_OneHotEncoding(all_data,column)\n  all_data_finish = pd.concat([all_data_finish,df],axis=1)\n  print(\"Column \",column, \" tranform!\")\n\nprint(\"Finish size:\",all_data_finish.shape)","045b9926":"all_data_finish.dtypes.head(40)","dfb461ba":"#all_data_finish['char_13'] = np.asarray(all_data_finish['char_13']).astype(np.float32)","86f9bd58":"#X = np.asarray(X).astype(np.float32)\n'''\nall_data_finish['char_25'] = np.asarray(all_data_finish['char_25']).astype(np.float32)\nall_data_finish['week'] = np.asarray(all_data_finish['week']).astype(np.float32)\nall_data_finish['activity_month'] = np.asarray(all_data_finish['activity_month']).astype(np.float32)\nall_data_finish['char_37'] = np.asarray(all_data_finish['char_37']).astype(np.float32)\nall_data_finish['activity_category'] = np.asarray(all_data_finish['activity_category']).astype(np.float32)\nall_data_finish['char_31'] = np.asarray(all_data_finish['char_31']).astype(np.float32)\nall_data_finish['char_19'] = np.asarray(all_data_finish['char_19']).astype(np.float32)\nall_data_finish['char_30'] = np.asarray(all_data_finish['char_30']).astype(np.float32)\nall_data_finish['day'] = np.asarray(all_data_finish['day']).astype(np.float32)\nall_data_finish['char_21'] = np.asarray(all_data_finish['char_21']).astype(np.float32)\nall_data_finish['char_10'] = np.asarray(all_data_finish['char_10']).astype(np.float32)\nall_data_finish['activity_week'] = np.asarray(all_data_finish['activity_week']).astype(np.float32)\nall_data_finish['char_12'] = np.asarray(all_data_finish['char_12']).astype(np.float32)\nall_data_finish['trimester'] = np.asarray(all_data_finish['trimester']).astype(np.float32)\n\nall_data_finish['char_24'] = np.asarray(all_data_finish['char_24']).astype(np.float32)\nall_data_finish['char_38'] = np.asarray(all_data_finish['char_38']).astype(np.float32)\nall_data_finish['activity_type'] = np.asarray(all_data_finish['activity_type']).astype(np.float32)\nall_data_finish['activity_id'] = np.asarray(all_data_finish['activity_id']).astype(np.float32)\nall_data_finish['day_of_week'] = np.asarray(all_data_finish['day_of_week']).astype(np.float32)\nall_data_finish['char_22'] = np.asarray(all_data_finish['char_22']).astype(np.float32)\nall_data_finish['char_27'] = np.asarray(all_data_finish['char_27']).astype(np.float32)\nall_data_finish['char_34'] = np.asarray(all_data_finish['char_34']).astype(np.float32)\nall_data_finish['activity_day_of_week'] = np.asarray(all_data_finish['activity_day_of_week']).astype(np.float32)\nall_data_finish['people_id'] = np.asarray(all_data_finish['people_id']).astype(np.float32)\nall_data_finish['char_36'] = np.asarray(all_data_finish['char_36']).astype(np.float32)\nall_data_finish['char_32'] = np.asarray(all_data_finish['char_32']).astype(np.float32)\nall_data_finish['month'] = np.asarray(all_data_finish['month']).astype(np.float32)\nall_data_finish['year'] = np.asarray(all_data_finish['year']).astype(np.float32)\nall_data_finish['char_14'] = np.asarray(all_data_finish['char_14']).astype(np.float32)\nall_data_finish['activity_year'] = np.asarray(all_data_finish['activity_year']).astype(np.float32)\nall_data_finish['activity_day'] = np.asarray(all_data_finish['activity_day']).astype(np.float32)\nall_data_finish['char_17'] = np.asarray(all_data_finish['char_17']).astype(np.float32)\nall_data_finish['char_14'] = np.asarray(all_data_finish['char_14']).astype(np.float32)\nall_data_finish['char_23'] = np.asarray(all_data_finish['char_23']).astype(np.float32)\nall_data_finish['char_16'] = np.asarray(all_data_finish['char_16']).astype(np.float32)\nall_data_finish['char_26'] = np.asarray(all_data_finish['char_26']).astype(np.float32)\nall_data_finish['char_20'] = np.asarray(all_data_finish['char_20']).astype(np.float32)\n'''","09cf9413":"'''\nall_data_finish['char_29'] = np.asarray(all_data_finish['char_29']).astype(np.float32)\nall_data_finish['char_11'] = np.asarray(all_data_finish['char_11']).astype(np.float32)\nall_data_finish['group_1'] = np.asarray(all_data_finish['group_1']).astype(np.float32) \n'''","deffde15":"all_data_finish.dtypes.head(40)","599595c8":"objetive.head()","b16d2113":"all_data_finish.dtypes.head()","87f2ad60":"from sklearn.model_selection import train_test_split\n\n# Separte train set and test set  \nx_train, x_test, y_train, y_test = train_test_split(all_data_finish,objetive, test_size=0.2,random_state=2020)\n\n# Create validation set\nx_train, x_val, y_train, y_val = train_test_split(x_train,y_train, test_size=0.1, random_state=2020)","2783c5e8":"print(\"Shape of x_train:\",x_train.shape)\nprint(\"Shape of x_test:\",x_test.shape)\nprint(\"Shape of x_val:\",x_val.shape)\nprint(\"Shape of y_train:\",y_train.shape)\nprint(\"Shape of y_test:\",y_test.shape)\nprint(\"Shape of y_val:\",y_val.shape)","95cf8289":"# We gonna use binary_crossentropy like loss function, sigmoid like wake-up function and the metric for evaluation will be the precision \"accuracy\"\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import plot_model\n\n# Create the neuronal network \nmodel = Sequential()\nmodel.add(Dense(256,input_dim = x_train.shape[1],activation=\"relu\"))\nmodel.add(Dense(256,activation=\"relu\"))\nmodel.add(Dense(1,activation = \"sigmoid\")) \nmodel.compile(optimizer = \"Adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n\nprint(model.summary()) ","ab864e64":"plot_model(model, to_file='model.png',show_shapes=True)","2fe59c08":"model.fit(x_train,y_train, validation_data = (x_val,y_val),epochs=5, batch_size=128)","cea118ab":"# Neuronal Network with Two Layers \nmodel = Sequential()\nmodel.add(Dense(512,input_dim = x_train.shape[1],activation=\"relu\"))\nmodel.add(Dense(512,activation=\"relu\"))\nmodel.add(Dense(256,activation=\"relu\"))\nmodel.add(Dense(1,activation = \"sigmoid\"))\nmodel.compile(optimizer = \"Adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\nmodel.fit(x_train,y_train, validation_data = (x_val,y_val),epochs=3, batch_size=64)","e5be2748":"# Content\n\n<div class=\"alert alert-block alert-warning\" style=\"margin-top: 20px\">\n    <ol>\n        <li><a href=\"#ingenieria\">Data Engineering<\/a><\/li>          \n        <li><a href=\"#architecture\">Proposed architecture<\/a><\/li>\n        <li><a href=\"#evaluaion\">Model evaluation<\/a><\/li>\n    <\/ol>\n<\/div>\n<br>\n<hr> ","9bff839b":"# <h1 id=\"ingenieria\">Data Engineering<\/h1>","1552d164":"# <h1 id=\"architecture\">Proposed Architecture<\/h1>"}}