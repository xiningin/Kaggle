{"cell_type":{"a7886242":"code","07d6f33a":"code","2ff6fa35":"code","a760fcb1":"code","7bf32a1b":"code","4374ddee":"code","745b64eb":"code","0039c7d4":"code","3e1b5e0d":"code","fc3e0d6e":"code","a2bc1c00":"code","86625910":"code","c0d466d7":"code","04ae1238":"code","93c592b1":"code","509c7b5d":"code","9feecdd5":"code","9843faae":"code","dfff1e27":"code","c5343468":"code","7a884b75":"code","56117faa":"code","71f40a85":"code","b723d03d":"code","04fa56c0":"code","60e5513e":"code","1c89567f":"code","26663a8c":"code","63c4cc51":"code","1fa3c27a":"code","101053da":"code","1c957bfa":"code","b8bee3cc":"markdown","059227c8":"markdown","e770552f":"markdown","8f1994a5":"markdown","688e8109":"markdown","a857ac74":"markdown","710c72ca":"markdown","fa1fe630":"markdown","ab1df87f":"markdown","e86c5a2c":"markdown","3f6d8671":"markdown","6bebd6a6":"markdown","59ac31b0":"markdown","3bca8461":"markdown","8c891260":"markdown","71084551":"markdown","f7795e08":"markdown","e00ce510":"markdown"},"source":{"a7886242":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.impute import SimpleImputer # missing data imputing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, precision_score\nimport matplotlib.pyplot as plt\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nimport os\nprint(os.listdir(\"..\/input\"))","07d6f33a":"# read train and test data\ndata_train = pd.read_csv('..\/input\/train.csv')\ndata_test = pd.read_csv('..\/input\/test.csv')","2ff6fa35":"data_train.head()","a760fcb1":"data_test.head()","7bf32a1b":"# check column datatypes\ndata_train.info()","4374ddee":"# check for missing values\nprint(\"Missing Values in data_train: \", data_train.isnull().sum(), sep = \"\\n\")\nprint()\nprint(\"Missing Values in data_test: \", data_test.isnull().sum(), sep = \"\\n\")","745b64eb":"# check age distribution in both datasets\nplt.subplot(1, 2, 1)\ndata_train.Age.hist()\nplt.xlabel('Age (data_train)')\nplt.ylabel('# of passengers')\n\nplt.subplot(1, 2, 2)\ndata_test.Age.hist()\nplt.xlabel('Age (data_test)')\nplt.ylabel('# of passengers')\n\n# Ages are continuously distributed with a single mode. Filling the missing values\n# with the most frequent value is appropriate.\n\n# fill missing values for Age with the most frequent value\nimp_age = SimpleImputer(missing_values = np.nan, strategy='most_frequent')\n\n# for data_train\nimp_age.fit(data_train[['Age']])\ndata_train['Age'] = imp_age.transform(data_train[['Age']])\n\n# for data_test\nimp_age.fit(data_test[['Age']])\ndata_test['Age'] = imp_age.transform(data_test[['Age']])","0039c7d4":"# check cabin info in both datasets\nprint(\"Cabin values in data_train: \", data_train.Cabin.unique().size)\nprint()\nprint(\"Cabin values in data_test: \", data_test.Cabin.unique().size)\n\n# Cabin has many discrete values. Adding a new discrete value \"Unknown\" to this\n# category will not have significant impact.\n\n# fill missing values for Cabin with \"Unknown\"\ndata_train.Cabin.fillna(\"Unknown\", inplace = True)\ndata_test.Cabin.fillna(\"Unknown\", inplace = True)","3e1b5e0d":"# check embark info in data_train\nprint(data_train.Embarked.value_counts())\n\n# Only 3 categories found in this column. Missing values are filled by the\n# most frequent value.\ndata_train.Embarked.fillna(\"S\", inplace = True)","fc3e0d6e":"# check fair info in data_test\ndata_test.Fare.hist()\nplt.xlabel('Fare (data_test)')\nplt.ylabel('# of passengers')\n\n# A large amount of passangers didn't pay their fare.\n# The missing values in the Fare column are filled by 0.\ndata_test.Fare.fillna(0, inplace = True)","a2bc1c00":"# Check the missing values again after imputing\nprint(\"Missing Values in data_train: \", data_train.isnull().sum(), sep = \"\\n\")\nprint()\nprint(\"Missing Values in data_test: \", data_test.isnull().sum(), sep = \"\\n\")","86625910":"# visulize the relationship between pclass and survive\npclass_survive_crosstbl = pd.crosstab(data_train.Pclass, data_train.Survived)\n\n# print(pclass_survive_crosstbl)\n\npassanger_num_pclass = pclass_survive_crosstbl.sum(axis = 1)\n\n# calculate survivor rate for each pclass\npclass_survive_crosstbl = pclass_survive_crosstbl.divide(passanger_num_pclass, axis = 0).round(2)\n\npclass_survive_crosstbl.plot(kind = \"bar\", stacked = True)\nplt.xlabel('pclass (data_train)')\nplt.ylabel('Survival Rate')","c0d466d7":"# extract titles for the passengers\ntitle_train = data_train.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ntitle_test = data_test.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\nprint(title_train.unique())\nprint()\nprint(title_test.unique())\nprint()\n\n# merge the titles by social status\ntitle_to_replace = {'Mrs': 'Ordinary_female', 'Miss': 'Ordinary_female', \n                    'Mme': 'Ordinary_female', 'Ms': 'Ordinary_female', \n                    'Mlle': 'Ordinary_female', 'Mr': 'Ordinary_male', \n                    'Master': 'Ordinary_male', 'Capt': 'Official', \n                    'Major': 'Official', 'Dr': 'Official', \n                    'Col': 'Official', 'Rev': 'Official', \n                    'Don': 'Noble_male', 'Jonkheer': 'Noble_male', \n                    'Sir': 'Noble_male', 'Dona': 'Noble_female', \n                    'Lady': 'Noble_female', 'Countess': 'Noble_female'}\n# add title column in data_train\ndata_train['Title'] = title_train.map(title_to_replace)\ndata_test['Title'] = title_test.map(title_to_replace)\n\n# check if the Tile column matches the Name column\nprint(data_train[['Name', 'Title']].head())\nprint()\nprint(data_test[['Name', 'Title']].head())\n\n# remove the Name column\ndata_train = data_train.drop('Name', axis = 1)\ndata_test = data_test.drop('Name', axis = 1)","04ae1238":"# visulize the relationship between Title and survive\ntitle_survive_crosstbl = pd.crosstab(data_train.Title, data_train.Survived)\n\n# print(title_survive_crosstbl)\n\npassanger_num_title = title_survive_crosstbl.sum(axis = 1)\n\n# calculate survivor rate for each title\ntitle_survive_crosstbl = title_survive_crosstbl.divide(passanger_num_title, axis = 0).round(2)\n\ntitle_survive_crosstbl.plot(kind = \"bar\", stacked = True)\nplt.xlabel('Title (data_train)')\nplt.ylabel('Survival Rate')","93c592b1":"# visulize the relationship between sex and survive\nsex_survive_crosstbl = pd.crosstab(data_train.Sex, data_train.Survived)\n\n# print(sex_survive_crosstbl)\n# calculate survivor rate for each sex\nsex_survive_crosstbl = sex_survive_crosstbl.divide(sex_survive_crosstbl.sum(axis = 1), axis = 0)\n\nsex_survive_crosstbl.plot(kind = \"bar\", stacked = True)\nplt.xlabel('Sex (data_train)')\nplt.ylabel('Survival Rate')","509c7b5d":"# Segment the Age column\nage_qcut_train = pd.cut(data_train.Age, [0, 20, 40, 60, 80])\nage_qcut_test = pd.cut(data_test.Age, [0, 20, 40, 60, 80])\n\n# encode the age bins for data_train\nle = LabelEncoder()\nle.fit(age_qcut_train)\ndata_train['Age_bins'] = le.transform(age_qcut_train)\n\n# for data_test\ndata_test['Age_bins'] = le.transform(age_qcut_test)\n\n # visulize the relationship between age and survive\nage_survive_crosstbl = pd.crosstab(data_train.Age_bins, data_train.Survived)\n# print(age_survive_crosstbl)\n# calculate survivor rate for each age\nage_survive_crosstbl = age_survive_crosstbl.divide(age_survive_crosstbl.sum(axis = 1), axis = 0)\nage_survive_crosstbl.plot(kind = \"bar\", stacked = True)\nplt.xlabel('Age_bins (data_train)')\nplt.ylabel('Survival Rate')\nplt.xticks(np.arange(0, 4), ('0~20', '20~40', '40~60', '60~80'))","9feecdd5":"# create new family size column by combining SibSp and Parch column\ndata_train['famsz'] = data_train.SibSp + data_train.Parch + 1\ndata_test['famsz'] = data_test.SibSp + data_test.Parch + 1\n\n# visulize the relationship between famsz and survive\nfamsz_survive_crosstbl = pd.crosstab(data_train.famsz, data_train.Survived)\n\n# print(famsz_survive_crosstbl)\n# calculate survivor rate for each famsz\nfamsz_survive_crosstbl = famsz_survive_crosstbl.divide(famsz_survive_crosstbl.sum(axis = 1), axis = 0)\n\nfamsz_survive_crosstbl.plot(kind = \"bar\", stacked = True)\nplt.xlabel('Family Size (data_train)')\nplt.ylabel('Survival Rate')","9843faae":"# calculate fare\/person\ndata_train['FarePP'] = data_train['Fare'] \/ data_train['famsz']\ndata_test['FarePP'] = data_test['Fare'] \/ data_test['famsz']\n\n# Segment the FarePP column\nfarepp_qcut_train = pd.cut(data_train.FarePP, [0, 5, 10, 20, 30, 600], include_lowest = True)\nfarepp_qcut_test = pd.cut(data_test.FarePP, [0, 5, 10, 20, 30, 600], include_lowest = True)\n\nfarepp_qcut_train.value_counts()\n# encode the farepp bins for data_train\nle = LabelEncoder()\nle.fit(farepp_qcut_train)\ndata_train['FarePP_bins'] = le.transform(farepp_qcut_train)\n\n# for data_test\ndata_test['FarePP_bins'] = le.transform(farepp_qcut_test)","dfff1e27":"# visulize the relationship between farepp and survive\nfarepp_survive_crosstbl = pd.crosstab(data_train.FarePP_bins, data_train.Survived)\n\n# print(farepp_survive_crosstbl)\n# calculate survivor rate for each farepp\nfarepp_survive_crosstbl = farepp_survive_crosstbl.divide(farepp_survive_crosstbl.sum(axis = 1), axis = 0)\n\nfarepp_survive_crosstbl.plot(kind = \"bar\", stacked = True)\nplt.xlabel('FarePP_bins (data_train)')\nplt.ylabel('Survival Rate')\nplt.xticks(np.arange(0, 5), ('0~5', '5~10', '10~20', '20~30', '30~600'))","c5343468":"# Extract Deck info from the Cabin column\ndata_train['Deck'] = data_train.Cabin.str.slice(0,1)\ndata_test['Deck'] = data_test.Cabin.str.slice(0,1)\n\n# visulize the relationship between deck and survive\ndeck_survive_crosstbl = pd.crosstab(data_train.Deck, data_train.Survived)\n\n# calculate survivor rate for each deck\ndeck_survive_crosstbl = deck_survive_crosstbl.divide(deck_survive_crosstbl.sum(axis = 1), axis = 0)\n\ndeck_survive_crosstbl.plot(kind = \"bar\", stacked = True)\nplt.xlabel('Deck (data_train)')\nplt.ylabel('Survival Rate')","7a884b75":"# visulize the relationship between embark and survive\nembark_survive_crosstbl = pd.crosstab(data_train.Embarked, data_train.Survived)\n\n# calculate survivor rate for each embark\nembark_survive_crosstbl = embark_survive_crosstbl.divide(embark_survive_crosstbl.sum(axis = 1), axis = 0)\n\nembark_survive_crosstbl.plot(kind = \"bar\", stacked = True)\nplt.xlabel('Embark (data_train)')\nplt.ylabel('Survival Rate')","56117faa":"# drop non-informative or redundant columns\nPassengerId_train = data_train.PassengerId\nPassengerId_test = data_test.PassengerId\n\ndata_train = data_train.drop(['PassengerId', 'Age', 'Ticket', 'Fare', 'Cabin', 'FarePP'], axis = 1)\ndata_test = data_test.drop(['PassengerId', 'Age', 'Ticket', 'Fare', 'Cabin', 'FarePP'], axis = 1)","71f40a85":"# check processed data (train)\ndata_train.head()","b723d03d":"# check processed data (test)\ndata_test.head()","04fa56c0":"# one hot encoding for Pclass\nPclass_one_hot_train = pd.get_dummies(data_train['Pclass'], prefix='Pclass')\nPclass_one_hot_test = pd.get_dummies(data_test['Pclass'], prefix='Pclass')\n\n# one hot encoding for Sex\nSex_one_hot_train = pd.get_dummies(data_train['Sex'], prefix='Sex')\nSex_one_hot_test = pd.get_dummies(data_test['Sex'], prefix='Sex')\n\n# one hot encoding for SibSp\nSibSp_one_hot_train = pd.get_dummies(data_train['SibSp'], prefix='SibSp')\nSibSp_one_hot_test = pd.get_dummies(data_test['SibSp'], prefix='SibSp')\n\n# one hot encoding for Parch\nParch_one_hot_train = pd.get_dummies(data_train['Parch'], prefix='Parch')\nParch_one_hot_test = pd.get_dummies(data_test['Parch'], prefix='Parch')\n\n# one hot encoding for Embarked\nEmbarked_one_hot_train = pd.get_dummies(data_train['Embarked'], prefix='Embarked')\nEmbarked_one_hot_test = pd.get_dummies(data_test['Embarked'], prefix='Embarked')\n\n# one hot encoding for Title\nTitle_one_hot_train = pd.get_dummies(data_train['Title'], prefix='Title')\nTitle_one_hot_test = pd.get_dummies(data_test['Title'], prefix='Title')\n\n# one hot encoding for Age_bins\nAge_bins_one_hot_train = pd.get_dummies(data_train['Age_bins'], prefix='Age_bins')\nAge_bins_one_hot_test = pd.get_dummies(data_test['Age_bins'], prefix='Age_bins')\n\n# one hot encoding for famsz\nfamsz_one_hot_train = pd.get_dummies(data_train['famsz'], prefix='famsz')\nfamsz_one_hot_test = pd.get_dummies(data_test['famsz'], prefix='famsz')\n\n# one hot encoding for FarePP_bins\nFarePP_bins_one_hot_train = pd.get_dummies(data_train['FarePP_bins'], prefix='FarePP_bins')\nFarePP_bins_one_hot_test = pd.get_dummies(data_test['FarePP_bins'], prefix='FarePP_bins')\n\n# one hot encoding for Deck\nDeck_one_hot_train = pd.get_dummies(data_train['Deck'], prefix='Deck')\nDeck_one_hot_test = pd.get_dummies(data_test['Deck'], prefix='Deck')\n\n# join the data frames\n# for data_train\none_hot_train = pd.concat([Pclass_one_hot_train, Sex_one_hot_train, SibSp_one_hot_train,\n                           Parch_one_hot_train, Embarked_one_hot_train, Title_one_hot_train,\n                           Age_bins_one_hot_train, famsz_one_hot_train, FarePP_bins_one_hot_train,\n                          Deck_one_hot_train, data_train.Survived], axis = 1, sort = False)\n\n# for data_test\none_hot_test = pd.concat([Pclass_one_hot_test, Sex_one_hot_test, SibSp_one_hot_test,\n                           Parch_one_hot_test, Embarked_one_hot_test, Title_one_hot_test,\n                           Age_bins_one_hot_test, famsz_one_hot_test, FarePP_bins_one_hot_test,\n                          Deck_one_hot_test], axis = 1, sort = False)","60e5513e":"# check if the columns are same between one_hot_train and one_hot_test\nset(list(one_hot_train)) ^ set(list(one_hot_test))","1c89567f":"# check the one hot encoded data (train)\n# no Deck_T  & Title_Noble_male in data_test\none_hot_train = one_hot_train.drop('Deck_T', axis = 1)\none_hot_train = one_hot_train.drop('Title_Noble_male', axis = 1)\n\nprint(one_hot_train.shape)\none_hot_train.head()","26663a8c":"# check the one hot encoded data (test)\n# no Parch_9 in the data_train\none_hot_test = one_hot_test.drop('Parch_9', axis = 1)\n\nprint(one_hot_test.shape)\none_hot_test.head()","63c4cc51":"# split data_train\ny = one_hot_train.Survived\nX = one_hot_train.drop('Survived', axis=1)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 12)","1fa3c27a":"# xgboost parameter tuning (RandomizedSearchCV)\n\nparams_xgb = {'min_child_weight': range(5,10),\n           'gamma': [i\/10.0 for i in range(0,10, 2)],\n           'subsample': [i\/10.0 for i in range(5, 10)],\n           'colsample_bytree': [i\/10.0 for i in range(5, 10)],\n           'max_depth': range(5,10),\n           'n_estimators': [400, 600, 1000, 1500],\n           'learning_rate': [0.1, 0.01, 0.001]}\n\nmy_xgb = xgb.XGBClassifier(silent = 1, nthread = 1)\n\n# split train for parameter tuning\nskf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 12)\n\n# tuning with random search\nrandom_search_xgb = RandomizedSearchCV(my_xgb, param_distributions = params_xgb,\n                                   n_iter = 10,\n                                   scoring = 'accuracy', n_jobs = 4,\n                                   cv = skf.split(X_train,y_train),\n                                   verbose = False, random_state = 12)\n\n# predict with tuned xgboost\nrandom_search_xgb.fit(X_train, y_train)\npredictions_xgb = random_search_xgb.predict(X_test)\n\n# accuracy check\naccuracy_xgb = accuracy_score(y_test, predictions_xgb)\n\nprecision_xgb = precision_score(y_test, predictions_xgb)\n\nprint('Accuracy (xgb): ', accuracy_xgb)\nprint('Precision (xgb): ', precision_xgb)\nprint('Best Parameters (xgb): ', random_search_xgb.best_params_)","101053da":"# lgbm parameter tuning (RandomizedSearchCV)\n\nparams_lgbm = {'max_depth' : range(5,10),\n               'num_leaves': [2**i for i in range(5, 10)],\n               'max_bin': [100, 300, 500],\n               'subsample_for_bin': [50, 100, 200],\n               'min_child_weight': range(5,10),\n               'min_child_samples': range(5,10),\n               'min_split_gain': [i\/10.0 for i in range(0, 10)],\n               'colsample_bytree': [i\/10.0 for i in range(5, 10)],\n               'n_estimators': [400, 600, 1000, 1500],\n               'subsample': [i\/10.0 for i in range(5, 10)]}\n\nmy_lgbm = lgb.LGBMClassifier(boosting_type = 'dart', objective = 'binary', nthread = 1,\n                             learning_rate = 0.01, scale_pos_weight = 1.1,\n                             num_class = 1, metric = 'accuracy')\n\nrandom_search_lgbm = RandomizedSearchCV(my_lgbm, param_distributions = params_lgbm,\n                                   n_iter = 10,\n                                   scoring = 'accuracy', n_jobs = 4,\n                                   cv = skf.split(X_train,y_train),\n                                   verbose = False, random_state = 12)\n\n# predict with tuned lgbm\nrandom_search_lgbm.fit(X_train, y_train)\npredictions_lgbm = random_search_lgbm.predict(X_test)\n\n# classification_report(y_test, predictions)\naccuracy_lgbm = accuracy_score(y_test, predictions_lgbm)\nprecision_lgbm = precision_score(y_test, predictions_lgbm)\n\nprint('Accuracy: ', accuracy_lgbm)\nprint('Precision: ', precision_lgbm)\nprint('Best Parameters (lgbm): ', random_search_lgbm.best_params_)","1c957bfa":"# make prediction for data_test\npredictions_xgb_test = random_search_xgb.predict(one_hot_test)\n\npredictions_xgb_test_df = pd.DataFrame({'PassengerId': PassengerId_test,\n                                         'Survived': predictions_xgb_test})\n\n# write the results\npredictions_xgb_test_df.to_csv('titanic_submission.csv', sep=',', index = False)","b8bee3cc":"## Family Size and Survive","059227c8":"## Final Prediction","e770552f":"## Pclass and Survive","8f1994a5":"## Title and Survive","688e8109":"## Drop non-informative or redundant columns","a857ac74":"## Fare and Survive","710c72ca":"## Deck and Survive","fa1fe630":"## LightGBM and Parameter Tuning","ab1df87f":"# A glance at the data","e86c5a2c":"## Handle missing values","3f6d8671":"## Sex and Survive","6bebd6a6":"## XGBoost and Parameter Tuning","59ac31b0":"## Embark and Survive","3bca8461":"## Sex and Survive","8c891260":"# Model Building","71084551":"# Exploratory Data Analysis","f7795e08":"## One Hot Encoding","e00ce510":"# Data processing"}}