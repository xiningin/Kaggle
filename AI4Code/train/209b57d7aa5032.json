{"cell_type":{"9b233904":"code","e6bbc470":"code","c062951c":"code","9238c08e":"code","afa71ad5":"code","56f00704":"code","19cbc495":"code","a50a11ea":"code","1995c705":"code","ad8145c1":"code","e466333a":"code","b08e4c64":"code","77cb8254":"code","4bca0043":"code","0d8536c3":"code","3b0e129e":"code","aeaad401":"code","4b280648":"code","8041d89d":"code","fcee5eed":"code","e2af9157":"code","fe2b1104":"code","e53895fb":"code","83654569":"code","bf3cad74":"code","24a43700":"code","bf3be2a8":"code","1c645368":"code","aca527bb":"code","a42044e9":"code","9c255138":"code","f0ed2320":"code","f5705838":"code","619dda90":"markdown","72c50cf0":"markdown","366c6690":"markdown","f4f07804":"markdown","259867ee":"markdown","0bc6604e":"markdown","38e0dc35":"markdown","07d16163":"markdown","d977cbd6":"markdown","dfaaf111":"markdown","f1126702":"markdown","b5af966c":"markdown","de503f58":"markdown","5b514595":"markdown","9a2af423":"markdown","26693502":"markdown","bc1bf67e":"markdown","a61662f2":"markdown","513e6be0":"markdown","5bafdce8":"markdown","21815d88":"markdown","e3efaf9f":"markdown","7dc18529":"markdown","4c7d4bf4":"markdown","18d9bae4":"markdown","4efad354":"markdown"},"source":{"9b233904":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport os\nimport csv\nimport datetime as datetime\nimport json \nimport datetime as dttm #time\/date manipulations\nimport sklearn\n#import matplotlib.pyplot as plot#Matplotlib\n#import seaborn as sns\nimport numpy as np\nimport math\nimport keras\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\nimport seaborn as sns\nimport matplotlib.pylab as plab\n\nfrom sklearn.model_selection import train_test_split\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e6bbc470":"## load the data\nsales_train = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsell_prices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')\n\n#Print dimensions of dataframe\nprint(sales_train.shape)\nprint(sell_prices.shape)\nprint(calendar.shape)\n\n\n#Print first few rows of the data frame\nprint(sales_train.head())\nprint(sell_prices.head())\nprint(calendar.head())","c062951c":"#Check counts of non-zero values in all the time series - \nsales_prods= sales_train.loc[:, 'd_1' : ]\nzero = sales_prods.apply(lambda x: x == 0)\n\nk=zero.sum(axis=1)\nk=k\/1913\nsales_train['zero_days']=k\n#sales_train\n\nhobbies=sales_train.loc[sales_train['cat_id']=='HOBBIES']\nfoods=sales_train.loc[sales_train['cat_id']=='FOODS']\nhousehold=sales_train.loc[sales_train['cat_id']=='HOUSEHOLD']\n#print(sales_train.groupby(['cat_id']).agg({'zero_days': ['mean','median']}).reset_index())\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=hobbies['zero_days'],name='Hobbies'))\nfig.add_trace(go.Histogram(x=foods['zero_days'],name='Foods'))\nfig.add_trace(go.Histogram(x=household['zero_days'],name='Household'))\n\n# Overlay all histograms\nfig.update_layout(barmode='overlay',title_text='Distribution of Zero Values Across Product Categories',xaxis_title_text='Proportion of Zero Values',yaxis_title_text='Count')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.1)\n\nfig.show()","9238c08e":"#Checking how zero values vary by state\ntx=sales_train.loc[sales_train['state_id']=='TX']\nca=sales_train.loc[sales_train['state_id']=='CA']\nwi=sales_train.loc[sales_train['state_id']=='WI']\n\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=tx['zero_days'],name='TX'))\nfig.add_trace(go.Histogram(x=ca['zero_days'],name='CA'))\nfig.add_trace(go.Histogram(x=wi['zero_days'],name='WI'))\n\n\n# Overlay all histograms\nfig.update_layout(barmode='overlay',title_text='Distribution of Zero Values Across States',xaxis_title_text='Proportion of Zero Values',yaxis_title_text='Count')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.3)\n\nfig.show()","afa71ad5":"#Sample dataframe to navigate tough waters ahead\nprint(sales_train.shape)\n\nrows=int(sales_train.shape[0]\/4)\nsales_train=sales_train.sample(rows, replace=True)\nprint(sales_train.shape)\n\n#Data transpose to convert dataset from wide format to long format\nsales_train_l=sales_train.melt(['id','item_id','dept_id','cat_id','store_id','state_id'], var_name='d', value_name='qty')\n\nprint(sales_train_l.shape)\n\nsales_train_l.head()","56f00704":"#Join sales time series with calendar dates \ncols = ['date','wm_yr_wk','d','weekday','month','year','event_type_1','snap_CA','snap_TX','snap_WI']\ncalendar_dts = calendar[cols]\n\ncalendar_dts.head()\nsales_train_l_dt = pd.merge(left=sales_train_l,right=calendar_dts,left_on='d', right_on='d')\n\ncols = ['date','d','weekday','month','year']\ncalendar_dts = calendar[cols]\n\n#calendar_dts.head()\n#sales_train_l_dt = pd.merge(left=sales_train_l,right=calendar,left_on='d', right_on='d')\n\nsales_train_l_dt['event_type_1']=sales_train_l_dt['event_type_1'].fillna('No Event')\n\nprint(sales_train_l.shape)\nprint(sales_train_l_dt.shape)\nsales_train_l_dt.head(5)","19cbc495":"#Installing and loading the calendar plot package\n!pip install calplot\n\nimport calplot","a50a11ea":"#Checking number of SNAP days in a year across the three states\nsnappy=calendar.groupby(['year','month']).agg({'snap_CA': ['sum'],'snap_TX': ['sum'],'snap_WI': ['sum']}).reset_index()\nsnappy","1995c705":"#plotting calendar heatmap for California\n\nx=calendar[['date','snap_CA']]\nevents=x['snap_CA']\nevents.index=pd.to_datetime(x['date'])\ncalplot.calplot(events)","ad8145c1":"#Check relationship between SNAP days and sales. Are there more products being sold on snap days?\n\nsnap=sales_train_l_dt.groupby(['store_id','cat_id','date','snap_CA','snap_TX','snap_WI']).agg({'qty': ['sum']}).reset_index()\n\nstate=['CA','TX','WI']\nl=[]\n\nfor i in range(3):\n  a=snap['store_id'].str.contains(state[i],case=False,regex=True) \n  snap_st=snap.loc[a]\n  snap_st.columns=['store_id','cat_id','date','snap_CA','snap_TX','snap_WI','qty']\n  col=\"snap_\"+state[i]\n  snap_st_gp=snap_st.groupby([col,'cat_id']).agg({'qty': ['sum'],'date': ['nunique']}).reset_index()\n  snap_st_gp.columns=[col,'cat_id','qty','unique_dates']\n  snap_st_gp['qty_day']=snap_st_gp['qty']\/snap_st_gp['unique_dates']\n  snap_st_gp['state']=state[i]\n  snap_st_gp.columns=['SNAP','cat_id','qty','unique_dates','qty_day','state']\n  l.append(snap_st_gp)\n  #print(snap_st_gp)\n    \nsnap_append = pd.concat(l).reset_index(drop=True)\nsnap_append=snap_append[['SNAP','cat_id','qty_day','state']]\n#snap_append\n\nsnap_append['comb']=snap_append['cat_id']+' '+snap_append['state']\ndel snap_append['state']\ndel snap_append['cat_id']\npivot_snap=snap_append.pivot(index='comb', columns='SNAP', values='qty_day')\npivot_snap['comb']=pivot_snap.index\npivot_snap.reset_index(drop=True)\npivot_snap.columns=['snap_0','snap_1','comb']\n\npivot_snap = pivot_snap.set_index('comb')\npivot_snap=pivot_snap.div(pivot_snap.sum(axis=1), axis=0)\npivot_snap=pivot_snap.reset_index()\n\n\n#Plot stacked bar chart\nfig = go.Figure(data=[\n    go.Bar(name='Avg. Qty. Non-SNAP Day', x=pivot_snap['comb'], y=pivot_snap['snap_0']),\n    go.Bar(name='Avg. Qty. SNAP Day', x=pivot_snap['comb'], y=pivot_snap['snap_1'])\n])\n# Change the bar mode\nfig.update_layout(barmode='stack',title_text='Impact of SNAP days on Sales',xaxis_title_text='State_Category',yaxis_title_text='Percentage'\n)\n\nfig.show()","e466333a":"output=calendar.groupby('event_type_1').agg({'date': ['nunique']}).reset_index()\noutput.columns=['event_type_1','days']\n\nfig = go.Figure(data=[\n    go.Bar(name='Avg. Qty. Non-SNAP Day', x=output['event_type_1'], y=output['days'])\n])\nfig.update_layout(title_text='Number of days by Event Type',xaxis_title_text='Event Type',yaxis_title_text='No. of Days'\n)\n\nfig.show()","b08e4c64":"output=calendar.groupby(['event_type_1','event_name_1']).agg({'date': ['nunique']}).reset_index()\noutput.columns=['event_type_1','event_name_1','days']\n\n\nfig = px.bar(output, x=\"event_name_1\", y=\"days\", color='event_type_1')\nfig.update_layout(title_text='Number of days by Event Name',xaxis_title_text='Event Type',yaxis_title_text='No. of Days')\n\n\nfig.show()","77cb8254":"#Check relationship between Event days and sales. Are there more products being sold on event days?\nevent=sales_train_l_dt.groupby(['cat_id','state_id','event_type_1']).agg({'qty': ['sum'],'date': ['nunique']}).reset_index()\nevent.columns=['cat_id','state_id','event_type_1','qty','days']\nevent['qty_per_day']=event['qty']\/event['days']\n\nevent_pivot=event.groupby(['cat_id','state_id','event_type_1'])['qty_per_day'].sum().unstack('event_type_1')\n\nevent_pivot.reset_index(inplace=True)\nevent_pivot['cat_state']=event_pivot['cat_id']+\" \" +event_pivot['state_id']\nevent_pivot.index=event_pivot['cat_state']\n\ndel event_pivot['cat_id']\ndel event_pivot['state_id']\ndel event_pivot['cat_state']\n\nevent_pivot=(event_pivot.T \/ event_pivot.T.sum()).T\nax = sns.heatmap(event_pivot,linewidths=.15,cmap='YlGnBu')","4bca0043":"#Correlation between price and demand\n\nsales_train_p_dt = pd.merge(left=sales_train_l_dt,right=sell_prices,left_on=['store_id','item_id','wm_yr_wk'],right_on=['store_id','item_id','wm_yr_wk'])\n\nsales_group=sales_train_p_dt.groupby(['item_id','month','year']).agg({'sell_price': ['median'],'qty':['sum']}).reset_index()\nsales_group.columns=['item_id','month','year','price','qty']\n\nsales_group.head()\nitems=sales_group.item_id.unique() \nl=[]\nfor i in range(len(items)):\n  df=sales_group.loc[sales_group['item_id']==items[i]]\n  cor=df['price'].corr(df['qty'])\n  l.append(cor)\n\nser=pd.Series(l)\nser=ser.fillna(0)\nitems=pd.Series(items)\n#ser\ncorframe=pd.concat([items, ser], axis=1)\ncorframe.columns=['item_id','correlation']\ncorframe.shape[0]\ncat=[]\nfor i in range(corframe.shape[0]):\n   cat.append(corframe['item_id'].str.split('_')[i][0])\n\ncorframe['cat']=pd.Series(cat)\n\nfoods=corframe.loc[corframe['cat']=='FOODS']\nhobbies=corframe.loc[corframe['cat']=='HOBBIES']\nhousehold=corframe.loc[corframe['cat']=='HOUSEHOLD']\n\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=foods['correlation'],name='FOODS'))\nfig.add_trace(go.Histogram(x=hobbies['correlation'],name='HOBBIES'))\nfig.add_trace(go.Histogram(x=household['correlation'],name='HOUSEHOLD'))\n\n\n# Overlay all histograms\nfig.update_layout(barmode='overlay',title_text='Distribution of Correlation Between Price & Sales Values Across Products',xaxis_title_text='Correlation',yaxis_title_text='Count of Item_IDs')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.3)\n\nfig.show()","0d8536c3":"    \ngroup_price_cat = sales_train_p_dt.groupby(['date','cat_id'],as_index=False)['sell_price'].mean().dropna()\ngroup_price_cat.head()\n\n#fig = px.line(group_price_cat, x=\"date\", y=\"sell_price\", color='cat_id')\n\nfig = make_subplots(rows=1, cols=3)\n\nfig.add_trace(\n    go.Scatter(x=group_price_cat[group_price_cat['cat_id']=='HOBBIES']['date'], y=group_price_cat[group_price_cat['cat_id']=='HOBBIES']['sell_price'],name='HOBBIES'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=group_price_cat[group_price_cat['cat_id']=='HOUSEHOLD']['date'], y=group_price_cat[group_price_cat['cat_id']=='HOUSEHOLD']['sell_price'],name='HOUSEHOLD'),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Scatter(x=group_price_cat[group_price_cat['cat_id']=='FOODS']['date'], y=group_price_cat[group_price_cat['cat_id']=='FOODS']['sell_price'],name='FOODS'),\n    row=1, col=3\n)\n\nfig.update_layout(title_text='Avg. Sell Prices across categories over time',xaxis_title_text='date',yaxis_title_text='Sell Price($)')\nfig.show()\n\n","3b0e129e":"cal_sub=calendar.drop_duplicates(subset='wm_yr_wk', keep=\"first\")\ncal_sub=cal_sub[['wm_yr_wk','date']]\n\nsell_prices_cal=pd.merge(left=sell_prices,right=cal_sub,left_on='wm_yr_wk', right_on='wm_yr_wk')\n#sell_prices_cal.head(1)\nsell_prices_cal['cat_id'] = sell_prices_cal['item_id'].str.split('_').str[0]\n\nunique_prods=sell_prices_cal.groupby(['date','cat_id'],as_index=False).agg({'item_id':['nunique']})\nunique_prods.columns=['date','cat_id','unique_items']\n\nfig = px.line(unique_prods, x=\"date\", y=\"unique_items\", color='cat_id')\nfig.update_layout(title_text='Total Unique Products in a Category over time',xaxis_title_text='date',yaxis_title_text='Unique Prods Count')\n\nfig.show()\n\n","aeaad401":"#Product prices by store\nimport plotly.express as px\n\ngroup_price_cat = sales_train_p_dt.groupby(['store_id','cat_id','item_id'],as_index=False)['sell_price'].mean().dropna()\ngroup_price_cat.head()\n\nfig = px.violin(group_price_cat, x='store_id', color='cat_id', y='sell_price',box=True, hover_name='item_id') \nfig.update_layout(template='seaborn',title='Distribution of Item Prices across Stores',legend_title_text='Category')\n\nfig.show()","4b280648":"group_qty_cat = sales_train_p_dt.groupby(['year','date','state_id','store_id'],as_index=False)['qty'].sum().dropna()\n\nfig = px.violin(group_qty_cat, x='store_id', color='state_id',y='qty',box=True) \nfig.update_layout(template='seaborn',title='Distribution of Quantity sold for Stores',legend_title_text='State')\n\nfig.show()","8041d89d":"#Correlation between sales for product categories across states\nstate_daily=sales_train_l_dt.groupby(['state_id','cat_id','date'],as_index=False)['qty'].sum().dropna()\nstate_daily['cat_state']=state_daily['state_id']+\" \"+state_daily['cat_id']\n\n\ndel state_daily['cat_id']\ndel state_daily['state_id']\nk=pd.pivot_table(state_daily, values='qty', index=['date'], columns=['cat_state'])\ncorr=k.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","fcee5eed":"#Outputs for plotting\ndaily=sales_train_l_dt.groupby('date').agg({'qty': ['sum']}).reset_index()\ndaily.columns=['date','qty']\ndaily.head()\n\n#State level sales \nstate=sales_train_l_dt.groupby(['state_id','date']).agg({'qty': ['sum']}).reset_index()\nstate.columns=['state_id','date','qty']\nstate.head()\n\n#weekday vs weekend\nweekday=sales_train_l_dt.groupby(['weekday']).agg({'qty': ['sum']}).reset_index()\nweekday.columns=['weekday','qty']\nweekday.head()\n\n\n#category\ncat=sales_train_l_dt.groupby(['cat_id','date']).agg({'qty': ['sum']}).reset_index()\ncat.columns=['cat_id','date','qty']\ncat.head()\n\n#Outputs for plotting\ndaily=sales_train_l_dt.groupby('date').agg({'qty': ['sum']}).reset_index()\ndaily.columns=['date','qty']\ndaily.head()\n\n#State level sales \nstate=sales_train_l_dt.groupby(['state_id','date']).agg({'qty': ['sum']}).reset_index()\nstate.columns=['state_id','date','qty']\nstate.head()\n\n#weekday vs weekend\nweekday=sales_train_l_dt.groupby(['weekday']).agg({'qty': ['sum']}).reset_index()\nweekday.columns=['weekday','qty']\nweekday.head()\n\nweekday_cat=sales_train_l_dt.groupby(['weekday','cat_id']).agg({'qty': ['sum']}).reset_index()\nweekday_cat.columns=['weekday','cat_id','qty']\nweekday_cat.head()\n\n\n\n#monthly\nmonth=sales_train_l_dt.groupby(['month']).agg({'qty': ['sum']}).reset_index()\nmonth.columns=['month','qty']\nmonth.head()\n\n#monthly cat\nmonth_cat=sales_train_l_dt.groupby(['month','cat_id']).agg({'qty': ['sum']}).reset_index()\nmonth_cat.columns=['month','cat_id','qty']\nmonth_cat.head()\n\n\n#store\nstore=sales_train_l_dt.groupby(['store_id','year']).agg({'qty': ['sum']}).reset_index()\nstore.columns=['store_id','year','qty']\nstore.head()","e2af9157":"import plotly.graph_objects as go\nfig = go.Figure( go.Scatter(x=daily['date'], y=daily['qty'] ) )\nfig.update_layout(title='Time series for overall quantity sold',xaxis_title_text='Date',yaxis_title_text='Qty Sold')\n\nfig.show()","fe2b1104":"# Create traces\nfig = go.Figure()\nstate_list=['CA','TX','WI']\nfor i in range(3):\n  subset=state[state['state_id']==state_list[i]]\n  fig.add_trace(go.Scatter(x=subset['date'], y=subset['qty'],\n                    mode='lines',\n                    name=state_list[i]))\n\nfig.update_layout(title='Time series for overall quantity across States',xaxis_title_text='Date',yaxis_title_text='Qty Sold')\nfig.show()","e53895fb":"from plotly.subplots import make_subplots\n\ncats = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n\n\nweekday['weekday'] = pd.Categorical(weekday['weekday'], categories=cats, ordered=True)\nweekday_cat['weekday'] = pd.Categorical(weekday_cat['weekday'], categories=cats, ordered=True)\n\nweekday = weekday.sort_values('weekday')\nweekday_cat = weekday_cat.sort_values('weekday')\n\nfig = make_subplots(rows=4, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=weekday['weekday'], y=weekday['qty'],name='OVERALL'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=weekday_cat[weekday_cat['cat_id']=='HOBBIES']['weekday'], y=weekday_cat[weekday_cat['cat_id']=='HOBBIES']['qty'],name='HOBBIES'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(x=weekday_cat[weekday_cat['cat_id']=='FOODS']['weekday'], y=weekday_cat[weekday_cat['cat_id']=='FOODS']['qty'],name='FOODS'),\n    row=3, col=1\n)\nfig.add_trace(\n    go.Scatter(x=weekday_cat[weekday_cat['cat_id']=='HOUSEHOLD']['weekday'], y=weekday_cat[weekday_cat['cat_id']=='HOUSEHOLD']['qty'],name='HOUSEHOLD'),\n    row=4, col=1\n)\n\n\nfig.update_layout(height=600, width=800, title_text=\"Aggregated Sales Volume by Day of Week\")\nfig.show()","83654569":"from plotly.subplots import make_subplots\n\nfig = make_subplots(rows=4, cols=1)\n\nfig.add_trace(\n    go.Scatter(x=month['month'], y=month['qty'],name='OVERALL'),\n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Scatter(x=month_cat[month_cat['cat_id']=='HOBBIES']['month'], y=month_cat[month_cat['cat_id']=='HOBBIES']['qty'],name='HOBBIES'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(x=month_cat[month_cat['cat_id']=='FOODS']['month'], y=month_cat[month_cat['cat_id']=='FOODS']['qty'],name='FOODS'),\n    row=3, col=1\n)\nfig.add_trace(\n    go.Scatter(x=month_cat[month_cat['cat_id']=='HOUSEHOLD']['month'], y=month_cat[month_cat['cat_id']=='HOUSEHOLD']['qty'],name='HOUSEHOLD'),\n    row=4, col=1\n)\n\n\nfig.update_layout(height=600, width=800, title_text=\"Aggregated Sales Volume by Day of Month\")\nfig.show()","bf3cad74":"# Create traces for stores\nstore_list=store.store_id.unique()\nyear_list=store.year.unique()\n\ndata=[]\nfor i in range(len(year_list)):\n  data.append(go.Bar(name=str(year_list[i]), x=store_list, y=store.loc[(store['year']==year_list[i]),'qty']))\n           \nfig = go.Figure(data)\n# Change the bar mode\nfig.update_layout(barmode='stack',title='Overall Quantity across Stores over time ',xaxis_title_text='Store',yaxis_title_text='Qty Sold')\n\nfig.show()","24a43700":"#Function to implement exponential smoothing\ndef exponential_smoothing(series, alpha):\n    \"\"\"\n        series - dataset with timestamps\n        alpha - float [0.0, 1.0], smoothing parameter\n    \"\"\"\n    result = [series[0]] # first value is same as series\n   # print(result)\n    for n in range(1, len(series)):\n        result.append(alpha * series[n] + (1 - alpha) * result[n-1])\n    return result\n\n#Croston's Method - https:\/\/medium.com\/analytics-vidhya\/croston-forecast-model-for-intermittent-demand-360287a17f5f\ndef Croston_TSB(ts,extra_periods=1,alpha=0.5,beta=0.7):\n    d = np.array(ts) # Transform the input into a numpy array\n    cols = len(d) # Historical period length\n    d = np.append(d,[np.nan]*extra_periods) # Append np.nan into the demand array to cover future periods\n    \n    #level (a), probability(p) and forecast (f)\n    a,p,f = np.full((3,cols+extra_periods),np.nan)\n# Initialization\n    first_occurence = np.argmax(d[:cols]>0)\n    a[0] = d[first_occurence]\n    p[0] = 1\/(1 + first_occurence)\n    f[0] = p[0]*a[0]\n                 \n    # Create all the t+1 forecasts\n    for t in range(0,cols): \n        if d[t] > 0:\n            a[t+1] = alpha*d[t] + (1-alpha)*a[t] \n            p[t+1] = beta*(1) + (1-beta)*p[t]  \n        else:\n            a[t+1] = a[t]\n            p[t+1] = (1-beta)*p[t]       \n        f[t+1] = p[t+1]*a[t+1]\n        \n    # Future Forecast\n    a[cols+1:cols+extra_periods] = a[cols]\n    p[cols+1:cols+extra_periods] = p[cols]\n    f[cols+1:cols+extra_periods] = f[cols]\n                      \n    df = pd.DataFrame.from_dict({\"Demand\":d,\"Forecast\":f,\"Period\":p,\"Level\":a,\"Error\":d-f})\n    return df","bf3be2a8":"from random import randrange\n\n# Create traces\nfig = go.Figure()\nrand=randrange(1,20000)\ntemp=sales_train_l_dt.iloc[rand]\n#print(temp)\n#print(type(temp))\nitem=temp['item_id']\nstore=temp['store_id']\nprint(item)\n\nsubset=sales_train_l_dt[(sales_train_l_dt['item_id']==item) & (sales_train_l_dt['store_id']==store)]\nprint(subset.shape)\n\nsma = subset.qty.rolling(30).mean()\nres=exponential_smoothing(subset['qty'].reset_index(drop=True), 0.05)\n#k\nprint(len(res))\n\n#Croston Forecast\ncros=Croston_TSB(subset['qty'].reset_index(drop=True),extra_periods=28)\ncros_forecast=cros['Forecast']\nprint('Croston sum')\nprint(np.sum(cros_forecast))\n\nprint('Actual sum')\nprint(np.sum(subset['qty']))\n\n\nfig.add_trace(go.Scatter(x=subset['date'], y=subset['qty'],\n                     mode='lines',\n                     name=item))\n\nfig.add_trace(go.Scatter(x=subset['date'], y=sma,\n                      mode='lines',\n                      name='SMA_30'))\n\nfig.add_trace(go.Scatter(x=subset['date'], y=res,\n                      mode='lines',\n                      name='EXP'))\n\nfig.add_trace(go.Scatter(x=subset['date'], y=cros_forecast,\n                      mode='lines',\n                      name='Croston'))\n\nfig.update_layout(barmode='stack',title='Time Series Forecasts for a Random Time Series ',xaxis_title_text='Date',yaxis_title_text='Qty Sold')\n\n\nfig.show()","1c645368":"#Naive - Just pick the last sales value \nnaive=sales_train.iloc[:, -1]\nnaive=naive.reset_index(drop=True)\n\n#print(naive)\n\nk=pd.Series(naive[0])\nk=k.repeat(28).reset_index(drop=True)\n#print(type(k))\nnaive_time_series_all=[]\nnaive_time_series_all.extend([naive for i in range(28)])","aca527bb":"#Define Root Mean Squared Error\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nx=[]\n\nactual=sales_train.iloc[:, -28:]\ntime_series_all=sales_train.iloc[:, 7:]\n#moving average\nmoving_average_series_all=[]\n\n#Exponential Series Average \nexponential_average_series_all=[]\n\nfor i in range(time_series_all.shape[0]):\n  series=time_series_all.iloc[i].rolling(28).mean()  \n  moving_average_series_all.append(series.tail(28))\n\n#Seasonal Naive\nseasonal_naive=sales_train.iloc[:, -365:-337]\n\n#Exponential Average Smoothing\n\nfor i in range(time_series_all.shape[0]):\n  series=time_series_all.iloc[i]\n  output=exponential_smoothing(series,0.1)\n  output=pd.Series(output)\n  exponential_average_series_all.append(output.tail(28))","a42044e9":"#30 day average\nave_28=actual.apply(np.mean,axis=1)\nave_28=ave_28.reset_index(drop=True)\n\n#Croston Method\ncroston_series_all=[]\n\nfor i in range(time_series_all.shape[0]):\n  series=time_series_all.iloc[i]\n  res=Croston_TSB(series,extra_periods=1)\n  output=res['Forecast']\n  #print(np.sum(output[-29:-1]))\n  croston_series_all.append(output[-29:-1])","9c255138":"#Compute RMS values for all methods\nrms_naive=[]\nrms_ave_28=[]\nrms_seasonal_naive=[]\nrms_ma=[]\nrms_es=[]\nrms_croston=[]\n\nfor i in range(7622):\n  y_actual=time_series_all.iloc[i,-28:]\n  y_predicted_naive=pd.Series(naive[i])\n  y_predicted_naive=y_predicted_naive.repeat(28).reset_index(drop=True)\n  y_predicted_ave_28=pd.Series(ave_28[i])\n  y_predicted_ave_28=y_predicted_ave_28.repeat(28).reset_index(drop=True)\n  y_predicted_seasonal_naive=seasonal_naive.iloc[i,:]\n  y_predicted_ma=moving_average_series_all[i]\n  y_predicted_es=exponential_average_series_all[i].tail(28)\n  y_predicted_croston=croston_series_all[i].tail(28)\n  \n  rms_naive.append(sqrt(mean_squared_error(y_actual, y_predicted_naive)))\n  rms_ave_28.append(sqrt(mean_squared_error(y_actual, y_predicted_ave_28)))  \n  rms_seasonal_naive.append(sqrt(mean_squared_error(y_actual, y_predicted_seasonal_naive)))\n  rms_ma.append(sqrt(mean_squared_error(y_actual, y_predicted_ma)))\n  rms_es.append(sqrt(mean_squared_error(y_actual, y_predicted_es)))\n  rms_croston.append(sqrt(mean_squared_error(y_actual, y_predicted_croston)))\n\nprint('Error with Naive')\nprint(sum(rms_naive))\n\nprint('Error with Ave 28')\nprint(sum(rms_ave_28))\n\nprint('Error with Seasonal Naive')\nprint(sum(rms_seasonal_naive))\n\nprint('Error with MA')\nprint(sum(rms_ma))\n\nprint('Error with ES')\nprint(sum(rms_es))\n\nprint('Error with Croston Method')\nprint(sum(rms_croston))","f0ed2320":"#Import raw data and extract columns\nsales_train = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ntime_series_all=sales_train.iloc[:, 7:]\nexponential_average_series_all=[]\ncroston_series_all=[]\n\n#Croston Method\nfor i in range(time_series_all.shape[0]):\n  series=time_series_all.iloc[i]\n  res=Croston_TSB(series,extra_periods=1)\n  output=res['Forecast']\n  croston_series_all.append(output.tail(1))\n\nvec=pd.Series(croston_series_all).reset_index(drop=True)\nvec=vec.astype('float64')","f5705838":"#Prepare final submission file\nfor i in range(int(sample_submission.shape[0]\/2)):\n   sample_submission.iloc[i,1:29]=vec[i]\n\nsample_submission.to_csv('submission.csv',index=False)","619dda90":"* There are **50+ National & Religious event days** but only about **18 Sporting event days** in the calendar\n* **Superbowl and NBA Finals** are the only sporting days available in the dataset. All these events are recorded every year in the calendar.\n* Easter & Cinco De Mayo are classified as Cultural Days whereas Lent & Ramadan are classified as religious days\n* Father's Day is recorded only for 4 years in the calendar.","72c50cf0":"### Key Takeaways from Above Charts\n\nAbove line & bar charts give us an interesting insight into how sales have been varying over time - \n* Overall quantity sold has been **increasing consistently over time**, there is a steep decline every year during **December**, likely during Christmas period when stores would be shut. The overall trend for increase in sales is consistent across all three states\n* Overall Qty of product sold is **significantly higher on weekends compared to weekdays**, with Wednesdays recording least number of overall sales. Mondays generally tend to have higher sales than other weekdays as well, likely due to spillover effect from the weekend.\n* Weekly sales patterns for all the three categories is very similar\n* **Sales remain highest from a period of Feb to April**\n* Sales tend to peak in **March** followed by a steady decline before picking up again and peaking in **August** before declining again before the end of the year\n* **California** has the **highest** volume of sales - likely due to higher count of stores\n* **CA_1 & CA_3** have the highest volume of sales across all stores and **CA_4 and WI_1** has the least volume of sales. CA_3 showed a very steep increase in sales in 2015.\n* Sales for Hobbies tends to pick up in October, likely due to Halloween in October","366c6690":"<a name=\"DataTransformations\"><\/a>\n\nThe chart above suggests proportion of zero values across all three states follows a similar distribution therefore proportion of zero values isn't impacted by state. \n\n## Data Transformations\n\n### Sampling sales train\n\nI initially tried to run my code on the entire data frame, but it failed due to overusage of memory. There are two days to deal with this situation - \n1. **Downcasting** to reduce storage for individual data frames\n2. Take a **random sample** of records in the dataframe\n\nTo learn more about how downcasting can actually reduce the storage required by the dataframe, read this excellent kernel [here](https:\/\/www.kaggle.com\/anshuls235\/m5-forecasting-eda-feature-engineering)\n\nFor this kernel i took the second approach of taking a **25% percent random sample** of the data frame.This will hopefully allow us to run code faster while providing similar patterns about the data as the entire dataset. We will also be **transposing** the dataframe from wide format to thin format to allow for easier data analysis involving summary and joins.\n\n### Melting Sales Data\n\nWe will be converting the sales prices dataframe from wide format to long format. This will bring all the daily sales values to one column from 1913 separate columns and will allow us to run summary operations in an easier manner. This will also allow us to run join operations with other datasets like **calendar** and  **sell_prices** on date columns in a easier manner. Python's melt function allows us to do this in just a single line of code. \n\nYou can read [this](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/reshaping.html) Pandas documentation here to understand different kinds of functions used for data transformations\n\n![image.png](attachment:image.png)\nSimilar to the image above we will keep columns ``'id','item_id','dept_id','cat_id','store_id','state_id'`` as id columns transforming the daily sales columns and converting it into a sales column, with another additional column added to track the date ","f4f07804":"# M5 Forecasting Competition\n![](http:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/18599\/logos\/thumb76_76.png?t=2020-02-07-00-58-17)\n\nFor the last few weeks I have been running some analyses on the raw data for M5 forecasting challenge. The objective of this notebook is to provide a overview of the raw data files and establishing some benchmarks using basic time series forecasting techniques. I also made a submission using the Croston method - which is suitable for time series with intermittent demand.","259867ee":"The output above shows that **Exponential Smoothing provides least amount of RMSE** whereas **Naive Forecasting provides the highest RMSE**. Croston Forecast also has a RMSE of 11213.","0bc6604e":"# Table of Contents - \n* [Importing the raw datasets & basic overview of datasets](#introduction)\n* [Data Transformations](#DataTransformations)\n* [Impact of SNAP days on sales](#snap)\n* [Impact of events on Sales](#events)\n* [Relationship between Price & Demand](#corprice)\n* [Variation in prices over time and across stores](#pricevar)\n* [Time Series Analysis - Overall trend & Seasonality](#tsa)\n* [Benchmarking using basic time series forecasting techniques(Naive, Seasonal Naive, Moving Average, Exponential Smoothing, Croston Method)](#forecast)\n* [Forecasting using Croston Method](#croston)","38e0dc35":"Following intersting insights are available from the plot above - \n* Lines showing increase in product counts follows a **similar trend across all the three product categories, which is different to the price trends for each of the categories we saw earlier**\n* Overall count of **unique products has nearly doubled** from 2011 to 2016 across all three categories. The increase in unique products is not steady over time and occurs at specific points\n* The overall count of unique products has remained **constant over last 1.5 years** in the dataset, from start of 2015 to mid 2016\n* The stability in total unique products count can also be linked to the ```sell_prices``` in the chart above, wherein the price has also remained relatively constant starting from 2015. This indicates that fluctuations in avg. prices in a category are likely due to introduction of newer products, which shifts the average level of a time series.\n* Slope of unique products count decreases over the time period\n","07d16163":"Each data point in the histogram represents a unique ```item_id```. The histogram above reveals that even though there is a huge variation for overall correlation, peak for all the three product categories is centered around zero, which means that for majority of the products there is no correlation between price and demand. This could probably be due to product prices remaining constant over a long period of time. Now let's try and have a deeper look at overall price patterns over time and also total products available within the stores. \n\n","d977cbd6":"Following patterns emerge from the above chart - \n1. Across all three states - food products sell most on **Sporting** events compared to any other events\n2. National Event days are not popular for sales of any kind of products\n3. **Hobbies** products sell more on **Cultural days** in **CA & TX** whereas in **WI** they sell more on **No Event & Religious days**\n4. **Household** products are most likely to be sold on **No Event days** i.e. normal days\n\n\n# Relationship between Price & Demand<a name=\"corprice\"><\/a>\n\nGenerally for any kind of sales data price and demand have an inverse correlation. Intuitively if a price of a product goes up its demand should go down. Here we will try and run this analysis for our sales data. We will be measuring the correlation between monthly aggregated sales quantity for a product and the average price of the item for the month.","dfaaf111":"Following things are apparent from the line charts -\n* The average sales price trend for each of the three product categories are quite different. While **Hobbies and Foods show an overall increasing trend**, the prices for Household went down from 2011 to 2014, before picking up again in 2015\n* In general we see an average increase of around 20% in product prices for Foods & Hobbies, whereas **average Household prices have dropped compared to 2011 levels **\n* Product prices in each of the **three categories has remained relatively constant since start of 2015**\n* Prices for Hobbies categories increased sharply between Jun 2012 to August 2012, rising from 4.6 to 5.1\n* Prices for Household categories dropped consistently from Mar 2012 to December 2013, before rising up again ","f1126702":"The above chart tells us the following things - \n* Average price variation in **Foods categories** is the **least** while it is highest for **Household** products\n* For each category even though majority of products are priced within a similar range, the long lines at the top indicate that there are **outlier** products as well which are priced significantly higher than other products in the same category.\n* The product prices for each of the categories roughly follows a very similar distribution across all the 10 stores\n\nThe above chart is called Violin Plot. Violin plots are similar to box plots, except that they also show the probability density of the data at different values, usually smoothed by a kernel density estimator. You can learn more about violin plots here - [https:\/\/towardsdatascience.com\/violin-plots-explained-fb1d115e023d](http:\/\/)\n\nI picked up the usage of violin chars for this dataset from this kernel - https:\/\/www.kaggle.com\/anshuls235\/m5-forecasting-eda-feature-engineering\n\n### Variation in Sales of Products across categories & stores\n\nHaving had a look at product prices across different stores, let's also have a look at how product quantity sold varies across different stores","b5af966c":"# Croston's Method for Time Series Forecasting<a name=\"croston\"><\/a>\n\nAs seen earlier while inspecting the raw data, we saw that majority of the individual sales values in time series are zero. This is something called as time series with intermittent demand. Link here defines intermittent demand as follows - \n\n*Intermittent demand or ID (also known as sporadic demand) comes about when a product experiences several periods\nof zero demand. Often in these situations, when demand occurs it is small, and sometimes highly variable in size.*\n\n\nCroston's Method was the first method developed to forecast time series with intermittent demand. His insight was that estimating demand probability (via interval size) and demand size separately was more intuitive and accurate. His idea could be summarized in three simple steps:\n- Evaluate the average demand level when there is a demand occurrence.\n- Evaluate the average time between two demand occurrences.\n- Forecast the demand as the demand level (when there is an occurrence) multiplied by the probability to have an occurrence.\n\nLet\u2019s do this step-by-step with an example:\n\n1. If we estimate that we have a demand occurrence every 4 periods on average (i.e. a 25% chance to have an occurrence),\n2. and we estimate that \u2014 \u2014 when there is a demand occurrence \u2014 \u2014 the average demand level is 5 units,\n3. then, we forecast the demand to be 5 x 0.25 = 1.25 per period going forward.\nThe Croston model answers the question How much demand will we have on average per period?\n\nYou can learn more about implementation of Croston's method in Python here - [https:\/\/medium.com\/analytics-vidhya\/croston-forecast-model-for-intermittent-demand-360287a17f5f](http:\/\/)","de503f58":"Each point on the histogram above corresponds to a time series. As we can see above - across all the three different product categories - the proportion of zero values is quite high. But between the three different categories - the peaks of histograms are quite different. **Generally Food products have least proportion of zero values whereas Household products have the highest proportion of zero values**","5b514595":"## Time Series Plots<a name=\"tsa\"><\/a>\n\nHaving analyzed the tabular data so far and gaining some understanding around how sales vary across different stores, let's also look at time series visualization for different stores and categories and understand how overall sales vary over a period of time. Let's look at following time series plots - \n\n* Overall sales with time \n* State level sales over time\n* Variation of sales by day of week\n* Variation of sales by month of year\n* Store level sales over time","9a2af423":"\n\n## Final Submission - \n\nIn the cell below - we will import the sales data again - since we had sampled the original ```sales_train``` dataset. We will import it again and compute the final forecasts using Croston's method and assign it to the ```sample_submission``` dataset.","26693502":"## Importing the Raw Datasets<a name=\"introduction\"><\/a>\n\nThe Competiton provides the following files - \n\n```calendar.csv```- Contains information about the dates on which the products are sold.\n\n```sales_train_validation.csv``` - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n\n```sample_submission.csv``` - The correct format for submissions. Reference the Evaluation tab for more info.\n\n```sell_prices.csv``` - Contains information about the price of the products sold per store and date.","bc1bf67e":"## Next Steps -\n\n* Tweak parameters for Croston method to improve scores\n* Test out ARIMA models for this dataset\n* Use learnings from the EDA above to generate features & train a regression models\n* Learn more about Dynamic Regression Models & Poisson Regression and test out these models on the dataset\n\nHope you liked the kernel. Please provide your feedback and suggestions in the comments. Happy Learning :)","a61662f2":"* Same product categories across stores are correlated to each other for sales\n* Correlation between sales for **Foods & Hobbies is generally quite low**\n","513e6be0":"## Basic Methods of Time Series Forecasting<a name=\"forecast\"><\/a>\n\nI've only just started learning time series forecasting, and entering this competition provided an opportunity to learn some of those basic techniques. I found this resource to be really good in getting a beginner level understanding of basic methods -  https:\/\/otexts.com\/fpp2\/intro.html\n\nBelow we will try out following things with time series techniques - \n* Plot these methods for an individual time series and notice how the fit looks like for different methods \n* Compute RMSE for techniques like **Naive, Seasonal Naive, Moving Average, Exponential Smoothing & Croston's Method** and compare these values\n\nFollowing are the explanation of some of these basic techniques - \n* **Naive** - For na\u00efve forecasts, we simply set all forecasts to be the value of the last observation\n* **Seasonal Naive** - For seasonal naive we set each forecast to be equal to the last observed value from the same season of the year \n* **Moving Average** - The forecasts of all future values are equal to the average (or \u201cmean\u201d) of the historical data\n\n\nIn the cell below - we will randomly pick up a ```item_id``` and plot the actual time series and also the Simple Moving Average Forecast, Exponential Smoothing & Croston's forecast","5bafdce8":"The above chart tells us following things - \n* For **Foods** category in all the three states - ```avg_qty_day``` is higher for SNAP days than non-SNAP days. This is most noticeable in **WI** where the ```avg_qty_day``` is 12% higher for SNAP days\n* There seems to be **no impact** of SNAP days on sales of **Hobbies & Household **categories across all the three states. ``avg_qty_day``` remains same on SNAP\/non-SNAP days\n\n## Variation in sales on Event days<a name=\"events\"><\/a>\n\nSimilar to SNAP - let's also take a look at relationship between events days and sales. We will compare average daily sales for different product categories across different types of events and try to understand which products sell more on different types of events. Before that let's first have a quick look at different types of events recorded in the calendar dataset.","21815d88":"# Variation in prices of Products across categories & stores<a name=\"pricevar\"><\/a>\n\nTo understand the variations in prices of products better let's also have a look at how average product prices vary across different product categories and stores.","e3efaf9f":"## Overview of data - \n\nQuick look at the dataset reveals the following details - \n\n### sales_train-\n* The raw data provides item_id level daily sales for **30,490 items across different product categories across different stores**\n* Each time series provides data for **1913 unique dates**. Upon closer inspection you also find that **lot of values in the time series are actually zero, which indicates that this maybe a time series with intermittent demand**\n* The sales data is provided for 3 categories - **Hobbies, Foods & Household**. Each of the product falls under one of these three categories\n* The sales data is provided for 10 stores across 3 states - **4 in CA, 3 in WI & 3 in TX**\n\n\n### calendar -\n* The calendar data contains data for **1969 unique dates**, all the dates within ```sales_train``` are included in this dataset\n* The calendar dataset provides details around which dates fall on various cultural, religious, sporting events etc. \n* Additionally there are three flag columns signifying whether a particular date is SNAP day or not in the state. A quick google search reveals that SNAP refers to Supplemental Nutritional Assistance Program - which is a means for low income households to purchase food at discount prices during certain days in a month. For more details read here - [https:\/\/en.wikipedia.org\/wiki\/Supplemental_Nutrition_Assistance_Program](http:\/\/)\n\n### sell prices -\n* This dataset has more than **6.8 Million unique records**\n* This dataset provides daily price values for all items available in the sales_train dataset across all stores\n* Prices are available from the date a product was launched\n\n### sample submission -\n* Sample submission file format. For this competition we need to submit 28 days of daily forecast values of qty sold for each of the time series. \n\n\n## Potential Areas of Analysis \n\nGiven all we now have a basic understanding of the different data files, we can now look at further areas of analysis to get a deeper understanding of variation of the time series for different products using the other raw datasets provided.\n\n* In the printed output above we saw that there are lot of zero values within the time series. Let's also check the distribution of zero values across all time series in the datasets and find out how many of the values are zero\n* How does the distribution of **zero** sales values varies across different states?\n* What is the impact of **SNAP** days on overall sales? Do we see more sales for food items as compared to other days? \n* What is the impact of **events** on overall sales of products?\n* Is there a **correlation** between item price values and overall sales?\n* How does the total number of **unique products** sold across stores change over time?\n* How do product prices change over time?\n* How do item **price** values vary across product categories and stores?\n* How do overall **sales** vary across stores? Which stores sell the **least** and **most** amount of items?","7dc18529":"## Variation in sales on SNAP days<a name=\"snap\"><\/a>\n\nAs we mentioned above - **SNAP provides nutrition benefits to supplement the food budget of needy families so they can purchase healthy food and move towards self-sufficiency.** The USFDA website [here](https:\/\/www.fns.usda.gov\/snap\/supplemental-nutrition-assistance-program) provides details about the SNAP program - listing the following products that are available at a discounted price - \n* Fruits and vegetables;\n* Meat, poultry, and fish;\n* Dairy products;\n* Breads and cereals;\n* Other foods such as snack foods and non-alcoholic beverages; and\n* Seeds and plants, which produce food for the household to eat.\n\nThis means that only food products are available at a discounted prices during SNAP sales. \n\nNow let's have a look at relationship between SNAP days and sales. We will compare average daily sales for different product categories on SNAP\/non-SNAP days and try and see which products sell more on SNAP days. Our initial hypothesis is that food products should sell more on SNAP days(because of discounted prices) - let's see if it holds true.","4c7d4bf4":"Following things are visible from plots above - \n* There are **10 SNAP days in every month of every year in dataset**\n* All the **SNAP days occur together in a month**, i.e. 10 consecutive days in a month will be SNAP days\n* SNAP days generally tend to start around second week of the month\n* There are no fixed dates for SNAP every month, SNAP days **occur on different dates every month**. SNAP Days also occur on different dates in same month for different years.\n\n\nI plotted the calendar heatmap for California, you could plot the same heatmap for other states as well by changing the SNAP column in the cell above.","18d9bae4":"Zooming into the above time series tells us that Croston's method fits the above time series best compared to other methods and forecasts peaks accurately compared to other methods. But rather than being dependent on the visual output let's also compute RMSE for each of the techniques and find out which one is the best. We'll be following this process using each of the techniques mentioned above -\n1. Train a separate time series forecasting model using the sales_data as training_data for each time series in our dataset\n2. Use last 28 days data as test data to measure the accuracy of the forecast generated using our model\n2. Compute overall RMSE for each of the models across all time series and identify the model which generates least amount of error","4efad354":"Chart above tells us - \n* **Store number 3 in California** sold the **maximum** number of items whereas **store number 4** sold the **least** number of items\n* All stores across **Texas** roughly sold uniform number of items\n* Store 2 in **Wisconsin** sold more items than other stores in the state\n* On a good day, stores were able to sell in excess of **2000** items. **Median** sales values for all stores is in excess of **500 and lies below 1500**.\n\nLet's now also have a look at how sales product categories across states are correlated to each other"}}