{"cell_type":{"83b16f99":"code","b8883631":"code","b74eacd2":"code","92fc2bdb":"code","5668093c":"code","fd0cb59f":"code","841d0a94":"code","f1b6dfeb":"code","00640501":"code","18144a84":"code","f6c2664a":"code","edee4465":"code","1edd5cb2":"code","272c6ea0":"code","530a265c":"code","be9f2656":"code","88329ea8":"code","9e1972bf":"code","3830d4b6":"code","18d44f7e":"code","72d3dccb":"code","5430494d":"code","734a3377":"code","87629144":"code","745c18b4":"code","14428c5c":"code","b0d64b51":"code","e60ffd5e":"code","4bacaeb2":"code","69a64b7e":"code","db41d8fe":"code","13c534ec":"code","0f93fb8b":"code","55828ebd":"code","84669443":"code","3308e90a":"code","e9eba826":"code","bba888ed":"code","b4b22ffd":"code","3e670e79":"code","d9286aa2":"code","5b17afb9":"code","6cca1803":"code","c3138432":"code","ad592931":"code","8ead2657":"code","5f0dc74a":"code","dda31541":"code","9a2a28a4":"code","e48a397d":"markdown","868b9671":"markdown","910fe859":"markdown","7bf81995":"markdown","07038f99":"markdown","ab1f022a":"markdown","dcfdb8ba":"markdown","31c7bd72":"markdown","0e6d1172":"markdown","845e34f5":"markdown","01c0eb81":"markdown","27312cb0":"markdown","d1a0671e":"markdown","c4055d83":"markdown","259e4f0d":"markdown","cf50238b":"markdown","654b69f6":"markdown","bcd012e4":"markdown","a462dece":"markdown","7c265104":"markdown","ee5f90f5":"markdown","ea8ca2df":"markdown","2decf060":"markdown","c05cff4f":"markdown","4974f982":"markdown","b452bcb8":"markdown","994a8795":"markdown","9861e4f1":"markdown","07536eab":"markdown","b65d76b6":"markdown","4c2cd1bc":"markdown","ded75268":"markdown","e77d254c":"markdown","2d8c0c6b":"markdown","b4f6c2e1":"markdown"},"source":{"83b16f99":"# Import the required Packages\nimport pandas as pd\nimport matplotlib \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.manifold import TSNE\nfrom IPython.core.interactiveshell import InteractiveShell\nimport warnings\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.manifold import TSNE\nwarnings.filterwarnings('ignore') # to supress seaborn warnings\npd.options.display.max_columns = None # Remove pandas display column number limit\n#InteractiveShell.ast_node_interactivity = \"all\" # Display all values of a jupyter notebook cell","b8883631":"# Read the data into DataFrames.\ntrain = pd.read_csv(\"..\/input\/credit_card_default_TRAIN.csv\",index_col=0)\ntest = pd.read_csv(\"credit_card_default_TEST.csv\",index_col=0)","b74eacd2":"# Breif look at the data\ntrain.head()","92fc2bdb":"test.head()","5668093c":"# Fix Header of the data, row 0 serves as more sensible header names\ndef fix_header(data):\n    new_header = data.iloc[0]    # take the first row for the header\n    data = data[1:]              # take the data without the header row\n    data.columns = new_header    # set the header row as the df header\n    data.rename(columns={'default payment next month':'DEFAULTER'}, inplace=True) # change column name\n    return data\n\ntrain = fix_header(train)\ntest = fix_header(test)","fd0cb59f":"# look at the data with fixed header\ntrain.head()","841d0a94":"test.head()","f1b6dfeb":"# Check for Null values in the datasets\ntrain.isnull().values.any(),test.isnull().values.any()","00640501":"# Combine Train and Test Data set for further analysis & preprocessing\ntrain['Type'] = 'Train'\ntest['Type'] = 'Test'\nfulldata = pd.concat([train,test],axis=0) ","18144a84":"#fulldata.shape","f6c2664a":"fulldata.describe()","edee4465":"fulldata.EDUCATION.value_counts()","1edd5cb2":"fulldata.EDUCATION[fulldata.EDUCATION=='0']='4'\nfulldata.EDUCATION[fulldata.EDUCATION=='5']='4'\nfulldata.EDUCATION[fulldata.EDUCATION=='6']='4'\nfulldata.EDUCATION.unique()","272c6ea0":"fulldata.MARRIAGE.value_counts()","530a265c":"fulldata.MARRIAGE[fulldata.MARRIAGE=='0']='3'\nfulldata.MARRIAGE.unique()","be9f2656":"fulldata.SEX.unique()","88329ea8":"# Check for values less than 0\n(fulldata.AGE[fulldata.AGE<0].count(),\nfulldata.LIMIT_BAL[fulldata.LIMIT_BAL<0].count())","9e1972bf":"# Split back the combined data to train & test\ntrain=fulldata[fulldata['Type']=='Train']\ntest=fulldata[fulldata['Type']=='Test']","3830d4b6":"# drop the non numeric column\ntrain.drop(['Type'],axis = 1, inplace=True)\ntest.drop(['Type'],axis = 1, inplace=True)\n\n# Change variables to type float\ntrain = train.astype(float)\ntest = test.astype(float)","18d44f7e":"train.DEFAULTER.mean()*100","72d3dccb":"#Set df4 equal to a set of a sample of 1000 deafault and 1000 non-default observations.\ndf2 = train[train.DEFAULTER == 0].sample(n = 1000)\ndf3 = train[train.DEFAULTER == 1].sample(n = 1000)\ndf4 = pd.concat([df2, df3], axis = 0)\n\n#Scale features to improve the training ability of TSNE.\nstandard_scaler = StandardScaler()\ndf4_std = standard_scaler.fit_transform(df4)\n\n#Set y equal to the target values.\ny = df4.DEFAULTER\n\ntsne = TSNE(n_components=2, random_state=0)\nx_test_2d = tsne.fit_transform(df4_std)\n\n#Build the scatter plot with the two types of transactions.\ncolor_map = {0:'red', 1:'blue'}\nplt.figure()\nfor idx, cl in enumerate(np.unique(y)):\n    plt.scatter(x = x_test_2d[y==cl,0], y = x_test_2d[y==cl,1], c = color_map[idx], label = cl)\nplt.xlabel('X in t-SNE')\nplt.ylabel('Y in t-SNE')\nplt.legend(loc='upper right')\nplt.title('t-SNE visualization of train data')\nplt.show()","5430494d":"cor = train.corr()\nplt.figure(figsize=(18,18))\nsns.heatmap(cor, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 10},\n            xticklabels=cor.columns.values,\n            yticklabels=cor.columns.values)","734a3377":"train['BILL_PAY_RATIO1'] = (train['BILL_AMT1']-train['PAY_AMT1'])\/train['LIMIT_BAL']\ntrain['BILL_PAY_RATIO2'] = (train['BILL_AMT2']-train['PAY_AMT2'])\/train['LIMIT_BAL']\ntrain['BILL_PAY_RATIO3'] = (train['BILL_AMT3']-train['PAY_AMT3'])\/train['LIMIT_BAL']\ntrain['BILL_PAY_RATIO4'] = (train['BILL_AMT4']-train['PAY_AMT4'])\/train['LIMIT_BAL']\ntrain['BILL_PAY_RATIO5'] = (train['BILL_AMT5']-train['PAY_AMT5'])\/train['LIMIT_BAL']\ntrain['BILL_PAY_RATIO6'] = (train['BILL_AMT6']-train['PAY_AMT6'])\/train['LIMIT_BAL']\n\ntest['BILL_PAY_RATIO1'] = (test['BILL_AMT1']-test['PAY_AMT1'])\/test['LIMIT_BAL']\ntest['BILL_PAY_RATIO2'] = (test['BILL_AMT2']-test['PAY_AMT2'])\/test['LIMIT_BAL']\ntest['BILL_PAY_RATIO3'] = (test['BILL_AMT3']-test['PAY_AMT3'])\/test['LIMIT_BAL']\ntest['BILL_PAY_RATIO4'] = (test['BILL_AMT4']-test['PAY_AMT4'])\/test['LIMIT_BAL']\ntest['BILL_PAY_RATIO5'] = (test['BILL_AMT5']-test['PAY_AMT5'])\/test['LIMIT_BAL']\ntest['BILL_PAY_RATIO6'] = (test['BILL_AMT6']-test['PAY_AMT6'])\/test['LIMIT_BAL']","87629144":"train.head()","745c18b4":"# function to make a bar plot\ndef plot0(col1, col2, tittle, xticks, train):\n    dt = train.groupby(col1).agg([np.mean])*100.0\n    dt = dt[col2].reset_index()\n    f, ax = plt.subplots(figsize=(5, 5))\n    sns.barplot(x=col1, y=\"mean\", data=dt)\n    ax.set(xlabel=\"\", ylabel=\"Defaulter %\")\n    ax.set_title(label=tittle, fontsize=15)\n    ax.set_xticklabels(xticks, fontsize=11)","14428c5c":"#Crosstab\nsex_crosstab = pd.crosstab(train['DEFAULTER'], train['SEX'], margins=True, normalize=False)\nnew_index = {0: 'Non-default', 1: 'Default', }\nnew_columns = {1 : 'Male', 2 : 'Female'}\nsex_crosstab.rename(index=new_index, columns=new_columns, inplace=True)\nsex_crosstab\/sex_crosstab.loc['All']","b0d64b51":"#Bar Chart\ncol1 = \"SEX\"\ncol2 = \"DEFAULTER\"\ntittle = \"% of Defaulters by Sex\"\nxticks = [\"Male\", \"Female\"]\nplot0(col1, col2, tittle, xticks, train)","e60ffd5e":"#Crosstab\nmarital_crosstab = pd.crosstab(train['DEFAULTER'], train['MARRIAGE'], margins=True, normalize=False)\nnew_index = {0: 'Non-default', 1: 'Default', }\nnew_columns = {1 : 'Married', 2 : 'Single', 3:'Others'}\nmarital_crosstab.rename(index=new_index, columns=new_columns, inplace=True)\nmarital_crosstab\/marital_crosstab.loc['All']","4bacaeb2":"#Bar Chart\ncol1 = \"MARRIAGE\"\ncol2 = \"DEFAULTER\"\ntittle = \"% of Defaulters by Marital Status\"\nxticks = [\"Married\", \"Single\", \"Other\"]\nplot0(col1, col2, tittle, xticks, train)","69a64b7e":"#Crosstab\neducation_crosstab = pd.crosstab(train['DEFAULTER'], train['EDUCATION'], margins=True, normalize=False)\nnew_index = {0: 'Non-default', 1: 'Default', }\nnew_columns = {1 : 'Graduate school', 2 : 'University', 3 : 'High school', 4 : 'Others'}\neducation_crosstab.rename(index=new_index, columns=new_columns, inplace=True)\neducation_crosstab\/education_crosstab.loc['All']","db41d8fe":"#Bar Chart\ncol1 = \"EDUCATION\"\ncol2 = \"DEFAULTER\"\ntittle = \"% of Defaulters by Education Level\"\nxticks = [\"Graduate\", \"University\", \"High school\", \"Others\"]\nplot0(col1, col2, tittle, xticks, train)","13c534ec":"defaulters = train[train[\"DEFAULTER\"] == 1]\nnon_defaulters = train[train[\"DEFAULTER\"] == 0]\ndefaulters[\"Defaulter\"] = defaulters[\"AGE\"]\nnon_defaulters[\"Non Defaulter\"] = non_defaulters[\"AGE\"]\nf, ax = plt.subplots(figsize=(12, 6))\nax = sns.kdeplot(defaulters[\"Defaulter\"], shade=True, color=\"r\")\nax = sns.kdeplot(non_defaulters[\"Non Defaulter\"], shade=True, color=\"g\")","0f93fb8b":"defaulters = train[train[\"DEFAULTER\"] == 1]\nnon_defaulters = train[train[\"DEFAULTER\"] == 0]\ndefaulters[\"Defaulter\"] = defaulters[\"LIMIT_BAL\"]\nnon_defaulters[\"Non Defaulter\"] = non_defaulters[\"LIMIT_BAL\"]\nf, ax = plt.subplots(figsize=(12, 6))\nax = sns.kdeplot(defaulters[\"Defaulter\"], shade=True, color=\"r\")\nax = sns.kdeplot(non_defaulters[\"Non Defaulter\"], shade=True, color=\"b\")","55828ebd":"# function to make a scatter plot\ndef plot1(label_list, label_dict, data, col,tittle,xlabel,ylabel, ticks):\n  df = {}\n  for i in label_list:\n      df[i] = data.groupby([i, col]).size().unstack()\n      df[i] = df[i][df[i].sum(axis=1)>25]\n      df[i] = df[i].div(df[i].sum(axis=1), axis='index') # Calculate proportions\n      df[i].sort_index(ascending=False, inplace=True)\n          \n  sns.set_palette(sns.light_palette(\"red\", reverse=True))   # plot\n  fig, ax = plt.subplots(1, 1, figsize=(6,4))\n\n  for i in label_list:\n      ax.scatter(x=df[i].index, y=df[i][1], label=label_dict.get(i), s=100, edgecolor='k', lw=1)          \n\n  ax.set_ylim([0, 1])\n  plt.xticks(ticks, rotation=0)\n  ax.xaxis.set_ticks(ticks=ticks, minor=False)\n  ax.grid(b=True, which='major', color='0.4', linestyle='--')\n  lgd = plt.legend(loc='center left', bbox_to_anchor=(1.0, 0.5), fontsize=14)\n  for tick in ax.yaxis.get_major_ticks():\n      tick.label.set_fontsize(14) \n  for tick in ax.xaxis.get_major_ticks():\n      tick.label.set_fontsize(14) \n  for spine in ax.spines.values():\n      spine.set_edgecolor('k')\n  sns.set_palette(sns.light_palette(\"green\", reverse=True))\n  plt.title(tittle, fontsize=17, y = 1.05) \n  plt.ylabel(xlabel, fontsize=14)\n  plt.xlabel(ylabel, fontsize=14)\n  plt.show()","84669443":"label_list =['PAY_0',  'PAY_2',  'PAY_3',  'PAY_4',  'PAY_5',  'PAY_6']\nlabel_dict ={'PAY_0': 'PAY_0 - Sep, 2005', \n             'PAY_2': 'PAY_2 - Aug, 2005', \n             'PAY_3': 'PAY_3 - Jul, 2005', \n             'PAY_4': 'PAY_4 - Jun, 2005',  \n             'PAY_5': 'PAY_5 - May, 2005',  \n             'PAY_6': 'PAY_6 - Apr, 2005'}\n\ncol = 'DEFAULTER'\ntittle = 'Proportion of Defaulters Versus Repayment Status'\nxlabel = 'Proportion of Defaulters'\nylabel = 'Repayment Status'\nticks = [-2, -1, 0, 1, 2, 3, 4, 5, 6, 7, 8,9]\nplot1(label_list, label_dict, train, col,tittle,xlabel,ylabel,ticks)","3308e90a":"label_list =['BILL_AMT1', 'BILL_AMT2', 'BILL_AMT3', 'BILL_AMT4', 'BILL_AMT5', 'BILL_AMT6']\nlabel_dict ={'BILL_AMT1': 'BILL_AMT1 - Sep, 2005',  \n             'BILL_AMT2': 'BILL_AMT2 - Aug, 2005',\n             'BILL_AMT3': 'BILL_AMT3 - Jul, 2005', \n             'BILL_AMT4': 'BILL_AMT4 - Jun, 2005',  \n             'BILL_AMT5': 'BILL_AMT5 - May, 2005', \n             'BILL_AMT6': 'BILL_AMT6 - Apr, 2005'}\n\ncol = 'DEFAULTER'\ntittle = 'Proportion of Defaulters Versus Bill Amount'\nxlabel = 'Proportion of Defaulters'\nylabel = 'Bill Amount'\nticks = []\nfor i in range(0, 3000, 500):\n    ticks.append(round(i,1))\nplot1(label_list, label_dict, train, col,tittle,xlabel,ylabel,ticks)","e9eba826":"from sklearn.cross_validation import train_test_split\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.grid_search import GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn import preprocessing, metrics\nfrom xgboost import XGBClassifier\nwarnings.filterwarnings('ignore') # to supress warnings","bba888ed":"x = train.drop(['DEFAULTER'],axis = 1)\ny = train.DEFAULTER\n\n# rescale the metrics to the same mean and standard deviation\nscaler = preprocessing.StandardScaler()\nx = scaler.fit(x).transform(x)\n\n# Further divide the train data into train test split 70% & 30% respectively\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, stratify=y, random_state=2)","b4b22ffd":"# list of different classifiers we are going to test\nclfs = {\n'LogisticRegression' : LogisticRegression(),\n'GaussianNB': GaussianNB(),\n'RandomForest': RandomForestClassifier(),\n'DecisionTreeClassifier': DecisionTreeClassifier(),\n'SVM': SVC(),\n'KNeighborsClassifier': KNeighborsClassifier(),\n'GradientBoosting': GradientBoostingClassifier(),\n'XGBClassifier': XGBClassifier()\n}","3e670e79":"# code block to test all models in clfs and generate a report\nmodels_report = pd.DataFrame(columns = ['Model', 'Precision_score', 'Recall_score','F1_score', 'Accuracy'])\n\nfor clf, clf_name in zip(clfs.values(), clfs.keys()):\n    clf.fit(x_train,y_train)\n    y_pred = clf.predict(x_test)\n    y_score = clf.score(x_test,y_test)\n    \n    #print('Calculating {}'.format(clf_name))\n    t = pd.Series({ \n                     'Model': clf_name,\n                     'Precision_score': metrics.precision_score(y_test, y_pred),\n                     'Recall_score': metrics.recall_score(y_test, y_pred),\n                     'F1_score': metrics.f1_score(y_test, y_pred),\n                     'Accuracy': metrics.accuracy_score(y_test, y_pred)}\n                   )\n\n    models_report = models_report.append(t, ignore_index = True)\n\nmodels_report","d9286aa2":"# Function to optimize model using gridsearch \ndef gridsearch(model, params,x_train, x_test, y_train, y_test, kfold):\n    gs = GridSearchCV(model, params, scoring='accuracy', n_jobs=-1, cv=kfold)\n    gs.fit(x_train, y_train)\n    print 'Best params: ', gs.best_params_\n    print 'Best AUC on Train set: ', gs.best_score_\n    print 'Best AUC on Test set: ', gs.score(x_test, y_test)\n\n# Function to generate confusion matrix\ndef confmat(pred, y_test):\n    conmat = np.array(confusion_matrix(y_test, pred, labels=[1,0]))\n    conf = pd.DataFrame(conmat, index=['Defaulter', 'Not Defaulter'],\n                             columns=['Predicted Defaulter', 'Predicted Not Defaulter'])\n    print conf\n\n# Function to plot roc curve\ndef roc(prob, y_test):\n    y_score = prob\n    fpr = dict()\n    tpr = dict()\n    roc_auc=dict()\n    fpr[1], tpr[1], _ = roc_curve(y_test, y_score)\n    roc_auc[1] = auc(fpr[1], tpr[1])\n    plt.figure(figsize=[7,7])\n    plt.plot(fpr[1], tpr[1], label='Roc curve (area=%0.2f)' %roc_auc[1], linewidth=4)\n    plt.plot([1,0], [1,0], 'k--', linewidth=4)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel('False Positive rate', fontsize=15)\n    plt.ylabel('True Positive rate', fontsize=15)\n    plt.title('ROC curve for Credit Default', fontsize=16)\n    plt.legend(loc='Lower Right')\n    plt.show()\n    \ndef model(md, x_train, y_train,x_test, y_test):\n    md.fit(x_train, y_train)\n    pred = md.predict(x_test)\n    #prob = md.predict_proba(x_test)[:,1]\n    print ' ' \n    print 'Accuracy on Train set: ', md.score(x_train, y_train)\n    print 'Accuracy on Test set: ', md.score(x_test, y_test)\n    print ' '\n    print(classification_report(y_test, pred))\n    print ' '\n    print 'Confusion Matrix'\n    confmat(pred, y_test)\n    #roc(prob, y_test)\n    return md","5b17afb9":"# Use gridsearch to fine tune the parameters\ngb = GradientBoostingClassifier()\ngb_params = {'n_estimators': [100,200,300],'learning_rate' : [0.01, 0.02, 0.05, 0.1]}\ngridsearch(gb, gb_params,x_train, x_test, y_train, y_test,5)","6cca1803":"# feature selection with the best model from grid search\ngb = GradientBoostingClassifier(learning_rate= 0.02, max_depth= 7,n_estimators=300, max_features = 0.9,min_samples_leaf = 5)\nmodel_gb = model(gb, x_train, y_train,x_test, y_test)","c3138432":"# Use gridsearch to fine tune the parameters\nxgb = XGBClassifier()\nxgb_params = {'n_estimators':[200,300],'learning_rate':[0.05,0.02], 'max_depth':[4],'min_child_weight':[0],'gamma':[0]}\ngridsearch(xgb, xgb_params,x_train, x_test, y_train, y_test,5)","ad592931":"# feature selection with the best model from grid search\nxgb = XGBClassifier(\n learning_rate =0.05,\n n_estimators=200,\n max_depth=4,\n min_child_weight=0,\n gamma=0,\n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread=1,\n scale_pos_weight=1,\n seed=27)\nmodel_xgb = model(xgb, x_train, y_train,x_test, y_test)","8ead2657":"#Predict final values on Test data set\ntest['PREDICTED_STATUS']=np.int_(model_gb.predict(test.drop(['DEFAULTER'],axis = 1)))\ntest.index.names = ['ID']","5f0dc74a":"#test.head()","dda31541":"test['PREDICTED_STATUS'].to_csv(\"credit_card_default_TRAIN_Predict.csv\")","9a2a28a4":"test.DEFAULTER.mean()*100","e48a397d":"- **According to description we should have values 1,2,3 thus we will change 0 to 3 i.e. others**","868b9671":"- **Now let us check the correlation between different features**","910fe859":"### Feature Engineering \n\nThe regression coefficients are positive i.e. log-odds of defaulters increase as the ratio of  $\\left(\\frac{\\text{bill amount} - \\text{pay amount}}{\\text{credit limit}}\\right)$ increases. Hence we can add below 6 features.","7bf81995":"## 7. Does the history of credit card bill amount has a correlation with the % of defaulters ?\n\n\nObservations:\n- The proportion of defaulters is positively correlated with bill amount in recent months.","07038f99":"**Here we are first trying out below listed classification models to get the first look at accuracy**","ab1f022a":"## 2. How does Marital Status effect the proportion of defaulters ?\n- _Below we plot the % of Defaulters by Marital Status._**We see that in the dataset Married people are slightly more likely to default.**\n\nObservations:\n- Approximately 24.2% of the Married people defaulted.\n- Approximately 21.2% of the Single people defaulted.","dcfdb8ba":"Figure shows that 'BILL_AMTX' are highly correlated to each other, but very less correlation to target label 'DEFAULTER'. When data is huge to save computational resource, such features can be dropped without losing significant prediction power.\n\nPayment statuses 'PAY' show highest contribution to the defaulter label.","31c7bd72":"- **We can see above that PAY_0,PAY_2...have high positive correlation to DEFAULTER and LIMIT_BAL has pretty high negative correlation**","0e6d1172":"- **Combine Train and Test Data set for further analysis & preprocessing**","845e34f5":"- **DataSets do not have any Null values**","01c0eb81":"- ### Goal of the study is to create a model that predicts if a client will default on credit card payment in next month.\n- ### This is a Supervised binary classification problem. Where Defaulter Yes(1) or No(0) is the dependant variable","27312cb0":"The classification metrics of iterest for this fairly imbalanced dataset are: \n- precision = tp \/ (tp + fp)\n- recall = tp \/ (tp + fn)\n- f1 = 2(precision)(recall) \/ (precision + recall)\n- Roc curve area\n\nDepending upon banks operational costs & ideology a large bank may follow the principal that fewer False Positives are preferable over a few more False Negatives to be able to lend more & spend less on investigations on the contrary a conservative approach would go with the opposite i.e more accuracy.\n\n### Therefore we see that XGBoost trains with little higher accuracy and auc score than GradientBoost. We will use XGBoost for final predictions. i.e. fewer False Positives are preferable over a few more False Negatives","d1a0671e":"### XGboost","c4055d83":"### From above report we can see that highest accuracy is given by XGboost followed by GradientBoosting, let us compare both.","259e4f0d":"- **From Above Table we can deduce that some columns have extra values which might not be correct According to the Description given i.e.**\n - EDUCATION has 7 unique values instead of 4\n - MARRIAGE has 4 unique values instead of 3","cf50238b":"# 1. Initialize Libraries, Load Data & Preprocess","654b69f6":"### Visualize Data with t-SNE\n\nt-SNE is a technique for dimensionality reduction that is well suited to visualise high-dimensional datasets. Lets have a first look on the map that will set some expectations for the prediction accuracy i.e. if our dataset has many overlaps it would be good if our model achieves an accuracy of 60-70%.!","bcd012e4":"## The below Report is divided into 3 main sections namely:\n\n### 1. Initialize Libraries, Load Data & Preprocess\n### 2. Exploratory Data Analysis and Visualization\n### 3. Predictive Modeling","a462dece":"# 3. Predictive Modeling","7c265104":"**The plot reveals a rather mixed up dataset which means we should not expect very accurate model.**","ee5f90f5":"# 2. Exploratory Data Analysis and Visualization","ea8ca2df":"- **Fix the Header of the data**","2decf060":"**Data Set Information:** The training data contains 22500 observations with the predictor variables as well as the response variable. The test set contains 7500 observations with the response variable removed.\n\n**Task:** Predict the response variable (default status) for the test data.\n\n**IMPORTANT:** Please include the variable \"ID\" in the prediction, so that model accuracy can be evaluated.\n\n**Variable descriptions:** This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. \n\nThis study reviewed the literature and used the following 23 variables as explanatory variables: \n- **X1:** Amount of the given credit (NT dollar): it includes both the individual consumer credit and his\/her family (supplementary) credit. \n- **X2:** Gender (1 = male; 2 = female). \n- **X3:** Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). \n- **X4:** Marital status (1 = married; 2 = single; 3 = others). \n- **X5:** Age (year). \n- **X6 - X11:** History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: \n- **X6** = the repayment status in September, 2005; \n- **X7** = the repayment status in August, 2005; . . .;\n- **X11** = the repayment status in April, 2005. The measurement scale for the repayment status is: \n - -1 = pay duly; \n - 1 = payment delay for one month; \n - 2 = payment delay for two months; . . .; \n - 8 = payment delay for eight months; \n - 9 = payment delay for nine months and above. \n - -2 = indicates no consumption in the month, and a value of \n - 0 = indicates the use of revolving credit (equivalent to prepayment)\n- **X12-X17:** Amount of bill statement (NT dollar). \n- **X12** = amount of bill statement in September, 2005; \n- **X13** = amount of bill statement in August, 2005; . . .; \n- **X17** = amount of bill statement in April, 2005. \n- **X18-X23:** Amount of previous payment (NT dollar). \n- **X18** = amount paid in September, 2005; \n- **X19** = amount paid in August, 2005; . . .;\n- **X23** = amount paid in April, 2005. ","c05cff4f":"To find the predictability of a defaulter our main objective is to find what features can play a role to predict a credit card defaulter? Therefore we need to find answers to some questions like:\n- **1.** Is the % of defaulters significantly different between male & female ?\n- **2.** How does Marital Status effect the proportion of defaulters ?\n- **3.** Does the Level of Education play a role in the % of defaulters ?\n- **4.** Which age group constitutes for higher proportion of defaulters ?\n- **5.** Is the number of defaulters correlated with credit limit ?\n- **6.** Is there a pattern in past repayment statuses which can help predict probability of a defaulter ?\n- **7.** Does the history of credit card bill amount has a correlation with the % of defaulters ?","4974f982":"## 4. Which age group constitutes for higher proportion of defaulters ?\n- _Below we can see the Defaulters distribution by Age_ **Majority of defaulters fall in the age group of 25 to 35**\n\n\nObservations:\n- Defaulters seems to increase from the early 20s to the early 30s.\n- Defaulters seems to decrease from the early 40s onward.","b452bcb8":"## 3. Does the Level of Education play a role in the % of defaulters ?**\n- _Below we plot the % of Defaulters by Education._**We can see that higher the education less likely is the person to default.**\n\nObservations:\n- Approximately 25.8% of defaulters studied upto High School.\n- Approximately 23.7% of defaulters studied upto University.\n- Approximately 19.7% of defaulters studied upto Graduate School.","994a8795":"- **According to description we should have values 1,2,3,4 thus we will change 5,6,0 to 4 i.e. others**","9861e4f1":"### Parameter tuning\n\nThere are a few parameters that require tuning to improve the performance. I use GridSearchCV method to test model through a series of parameter values.","07536eab":"Save the output to csv file in desired format","b65d76b6":"# Credit Default Prediction","4c2cd1bc":"### GradientBoosting","ded75268":"## 6. Is there a pattern in past repayment statuses which can help predict probability of a defaulter ?\n\nObservations:\n- The proportion of defaulters in delinquency bucket 2 or more i.e. with payment delay for 2 or more months are much higher.","e77d254c":"## 1. Is the % of defaulters significantly different between male & female ?\n- _Below we plot the % of Defaulters by Gender._**Apparently we see that males are slightly more likely to default.**\n\nObservations:\n- Approximately 24.2% of the males defaulted.\n- Approximately 20.8% of the females defaulted.","2d8c0c6b":"## 5. Is the number of defaulters correlated with credit limit ?\n- _Below we can see the Defaulters distribution by Credit Limit_ **we can see that people with lower credit balance tend to default more**","b4f6c2e1":"- 22.61 % of people are defaulters in the Train data"}}