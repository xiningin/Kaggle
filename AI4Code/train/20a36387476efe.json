{"cell_type":{"b8493bec":"code","d9b202e9":"code","3cd8697c":"code","68a7aeb4":"code","acaa75ec":"code","8785c85b":"markdown","2d7e98bd":"markdown","ca65fdf0":"markdown","05db785c":"markdown","e815af7f":"markdown","ef4a9cd3":"markdown"},"source":{"b8493bec":"import pickle\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import StratifiedKFold\nimport tokenizers\nimport math\nimport re\nfrom numba import jit\nfrom tqdm import tqdm\n\n\nMAX_LEN = 120\nPATH = '..\/input\/tf-roberta\/'\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=PATH+'vocab-roberta-base.json', \n    merges_file=PATH+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1809, 'negative': 3392, 'neutral': 14058}\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","d9b202e9":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\n\ntext = train['text'].values\nselected_text = train['selected_text'].values.copy()\nsentiments = train['sentiment'].values\n\nct = train.shape[0]\ninput_ids = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\nstart_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\nend_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n\n\"\"\"\nfor k in tqdm(range(train.shape[0])):\n    ss = text[k].find(selected_text[k])\n    if text[k][max(ss - 2, 0):ss] == '  ':\n        ss -= 2\n    if ss > 0  and text[k][ss - 1] == ' ':\n        ss -= 1\n\n    ee = ss + len(selected_text[k])\n\n    #if re.match(r' [^ ]', text[k]) is not None:\n    if len(text[k]) > 0 and text[k][0] == ' ':  # \u5909\u66f4\u7b87\u6240 guchio\n        #ee -= 1\n        front_spaces = re.findall(\"^ +[^ ]\", text[k]) # \u5909\u66f4\u7b87\u6240 guchio\n        ee -= front_spaces[0].count(' ') # \u5909\u66f4\u7b87\u6240 guchio\n        for cnt_base in re.findall(\"[^ ]  +[^ ]\", text[k][:ee].strip()): # \u5909\u66f4\u7b87\u6240 guchio\n            ee -= cnt_base[2:].count(' ') # \u5909\u66f4\u7b87\u6240 guchio\n    ss = max(0, ss)\n    if '  ' in text[k][:ss] and sentiments[k] != 'neutral':\n        text1 = \" \".join(text[k].split())\n        sel = text1[ss:ee].strip()\n        if len(sel) > 1 and sel[-2] == ' ':\n            sel = sel[:-2]\n\n        selected_text[k] = sel\n        \n    text1 = \" \"+\" \".join(text[k].split())\n    text2 = \" \".join(selected_text[k].split()).lstrip(\".,;:\")\n\n    idx = text1.find(text2)\n    if idx != -1:\n        chars = np.zeros((len(text1)))\n        chars[idx:idx+len(text2)]=1\n        if text1[idx-1]==' ': chars[idx-1] = 1 \n    else:\n        import pdb;pdb.set_trace()\n        chars = np.ones((len(text1)))\n    enc = tokenizer.encode(text1) \n\n    # ID_OFFSETS\n    offsets = enc.offsets\n\n    # START END TOKENS\n    _toks = []\n\n    for i,(a,b) in enumerate(offsets):\n        sm = np.mean(chars[a:b])\n        # if (sm > 0.6 and chars[a] != 0):  # \u3053\u3046\u3059\u308b\u3068\u82e5\u5e72\u4f38\u3073\u308b\u3051\u3069...\n        if (sm > 0.5 and chars[a] != 0): \n            _toks.append(i) \n\n    toks = _toks\n    s_tok = sentiment_id[sentiments[k]]\n    input_ids[k,:len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1\n\"\"\"\n\nfor k in tqdm(range(train.shape[0])):\n    ss = text[k].find(selected_text[k])\n    if text[k][max(ss - 2, 0):ss] == '  ':\n        ss -= 2\n    if ss > 0  and text[k][ss - 1] == ' ':\n        ss -= 1\n\n    ee = ss + len(selected_text[k])\n\n    if re.match(r' [^ ]', text[k]) is not None:\n        ee -= 1\n\n    ss = max(0, ss)\n    if '  ' in text[k][:ss] and sentiments[k] != 'neutral':\n        text1 = \" \".join(text[k].split())\n        sel = text1[ss:ee].strip()\n        if len(sel) > 1 and sel[-2] == ' ':\n            sel = sel[:-2]\n\n        selected_text[k] = sel\n\n    text1 = \" \"+\" \".join(text[k].split())\n    text2 = \" \".join(selected_text[k].split()).lstrip(\".,;:\")\n\n    idx = text1.find(text2)\n    if idx != -1:\n        chars = np.zeros((len(text1)))\n        chars[idx:idx+len(text2)]=1\n        if text1[idx-1]==' ': chars[idx-1] = 1 \n    else:\n        import pdb;pdb.set_trace()\n        chars = np.ones((len(text1)))\n    enc = tokenizer.encode(text1) \n\n    # ID_OFFSETS\n    offsets = enc.offsets\n\n    # START END TOKENS\n    _toks = []\n\n    for i,(a,b) in enumerate(offsets):\n        sm = np.mean(chars[a:b])\n        #if (sm > 0.6 and chars[a] != 0):  # \u3053\u3046\u3059\u308b\u3068\u82e5\u5e72\u4f38\u3073\u308b\u3051\u3069...\n        if (sm > 0.5 and chars[a] != 0): \n            _toks.append(i)\n\n    toks = _toks\n    s_tok = sentiment_id[sentiments[k]]\n    input_ids[k, :len(enc.ids)+3] = [0, s_tok] + enc.ids + [2]\n    attention_mask[k,:len(enc.ids)+3] = 1\n    if len(toks)>0:\n        start_tokens[k,toks[0]+2] = 1\n        end_tokens[k,toks[-1]+2] = 1  \n","3cd8697c":"\nimport re\ndef modify_punc_length(text, selected_text):\n    m = re.search(r'[!\\.\\?]+$', selected_text)        \n    if m is None:\n        return selected_text\n    \n    conti_punc = len(m.group())\n\n    if conti_punc >= 4:\n        selected_text = selected_text[:-(conti_punc-2)]\n    elif conti_punc == 1:# \u5143\u306etext\u3092\u63a2\u3057\u306b\u884c\u304f\n        tmp = re.sub(r\"([\\\\\\*\\+\\.\\?\\{\\}\\(\\)\\[\\]\\^\\$\\|])\", r\"\\\\\\g<0>\", selected_text)\n        pat = re.sub(r\" \", \" +\", tmp)\n        m = re.search(pat, text)\n        f_idx0 = m.start()\n        f_idx1 = m.end()\n\n        if f_idx1 != len(text) and text[f_idx1] in (\"!\", \".\", \"?\"):\n            f_idx1 += 1\n            selected_text = text[f_idx0:f_idx1]\n    return selected_text\n\n\nimport math\ndef postprocess(row):\n    if row.original_text == '':\n        return row.normalized_text.strip()\n    original_text = row.original_text.replace('\\t', '')\n    y_start_char = row.y_start_char\n    y_end_char = row.y_end_char\n    y_selected_text = row.normalized_text[y_start_char:y_end_char].strip()\n    if (y_end_char < len(row.normalized_text) and row.sentiment != 'neutral' and\n        y_selected_text[-1] == '.' and\n        (row.normalized_text[y_end_char] == '.' or \n         y_selected_text[-2] == '.')):\n        y_selected_text = re.sub('\\.+$', '..', y_selected_text)\n\n    tmp = re.sub(r\"([\\\\\\*\\+\\.\\?\\{\\}\\(\\)\\[\\]\\^\\$\\|])\", r\"\\\\\\g<0>\", y_selected_text)\n    pat = re.sub(r\" \", \" +\", tmp)\n    m = re.search(pat, original_text)\n    if m is None:\n        print(row.normalized_text[y_start_char:y_end_char].strip())\n        print(row.normalized_text)\n        print(y_selected_text)\n    ss2 = m.start()\n    ee2 = m.end()\n    \n    # 'neutral' \u304a\u3088\u3073\u307b\u307c\u6587\u66f8\u5168\u4f53\u304c\u629c\u304d\u51fa\u3055\u308c\u308b\u3082\u306e\n    if row.sentiment == 'neutral' or ((ee2 - ss2) \/ len(original_text) > 0.75 and  (ee2 - ss2) > 9):\n        if len(original_text) > 0 and original_text[0] != '_' and ss2 < 5:\n            ss2 = 0 \n        if (ee2 < len(original_text)-1 and original_text[ee2:ee2+2] in ('..', '!!', '??', '((', '))')):\n            ee2 += 1\n        st =  original_text[ss2:ee2].lstrip(' \u00bd\u00bf')\n        y_selected_text = st #re.sub(r' .$', '', st)#.strip('`') ###  \u3053\u306e\u4e00\u884c\u8ffd\u52a0\n                \n    else:\n        if original_text[:int((ss2+ee2) * 0.5) + 1].count('  ') > 0:\n            ss = y_start_char\n            ee = y_end_char + 1\n            if ss > 1 and original_text[ss-1:ss+1] == '..' and  original_text[ss+1] != '.':\n                ss -= 1\n            st = original_text[ss:ee]#.lstrip(' \u00bd\u00bf')\n            y_selected_text = re.sub(r' .$', '', st)#.strip('`') ###  \u3053\u306e\u4e00\u884c\u8ffd\u52a0\n        else:\n            if (ee2 < len(original_text)-1 and original_text[ee2:ee2+2] in ('..', '!!', '??', '((', '))')):\n                ee2 += 1\n            # \u5148\u982d\u306e\u7a7a\u767d\u5206\u5f8c\u9000\n            if  original_text[0] == ' ':\n                ss2 -= 1\n\n            y_selected_text = original_text[ss2:ee2].strip(' \u00bd')\n\n            if row.normalized_text[:y_end_char + 5] == \" \" + row.original_text[:ee2 + 4]: # \u7c21\u5358\u306e\u305f\u3081\u3001\u9577\u3055\u304c\u540c\u3058\u5834\u5408\u306b\u9650\u5b9a\u3057\u3066\u3044\u308b\n                y_selected_text = modify_punc_length(original_text, y_selected_text)\n            \n            \n    return y_selected_text","68a7aeb4":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\n\ntext = train['text'].values\nselected_text = train['selected_text'].values.copy()\nsentiments = train['sentiment'].values\nids = train['textID'].values\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\nall = []\nlist_st = []\nall_nn = []\nall_p = []\nall_n = []\n\nfrom collections import namedtuple\nRow = namedtuple('Row', ['original_text', 'normalized_text', 'sentiment', 'y_start_char', 'y_end_char'])\nfor k in tqdm(list(range(train.shape[0]))):\n    text0 = text[k]\n    text1 = \" \" + \" \".join(text[k].split())\n    enc = tokenizer.encode(text1)\n\n    aa = np.argmax(start_tokens[k])\n    bb = np.argmax(end_tokens[k])\n\n    ss = enc.offsets[aa - 2][0]\n    ee = enc.offsets[bb - 2][1] \n    st = text1[ss:ee].strip()\n\n    row = Row(\n        original_text=text0,\n        normalized_text=text1,\n        sentiment=sentiments[k],\n        y_start_char=ss,\n        y_end_char=ee,\n    )\n    try:\n        st = postprocess(row)\n    except Exception as e:\n        raise e\n        print(k)\n\n    list_st.append(st)\n    sc = jaccard(st,selected_text[k])\n    if sentiments[k] == 'neutral':\n        all_nn.append(sc)\n    elif sentiments[k] == 'positive':\n        all_p.append(sc)\n    else:\n        all_n.append(sc)\n\n    all.append(sc)\nprint(a, b, '>>>> FOLD Jaccard all =',np.mean(all))#, np.mean(all_nn), np.mean(all_p), np.mean(all_n))\nprint('>>>> FOLD Jaccard neutral =',np.mean(all_nn))\nprint('>>>> FOLD Jaccard positive =',np.mean(all_p))\nprint('>>>> FOLD Jaccard negative =',np.mean(all_n))","acaa75ec":"train = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv').fillna('')\ntext = train['text'].values\nselected_text = train['selected_text'].values.copy()\nsentiments = train['sentiment'].values\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    if (len(a)==0) & (len(b)==0): return 0.5\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\nall = []\nlist_st = []\nall_nn = []\nall_p = []\nall_n = []\n\nDIR = '..\/input\/tweet-cv-ens-0609\/'\n\noof_chr_start = np.load(DIR + 'oof_chr_start.npy')\noof_chr_end = np.load(DIR + 'oof_chr_end.npy')\n\n\nfrom collections import namedtuple\nRow = namedtuple('Row', ['original_text', 'normalized_text', 'sentiment', 'y_start_char', 'y_end_char'])\nfor k in tqdm(list(range(train.shape[0]))):\n    text0 = text[k]\n    text1 = \" \" + \" \".join(text[k].split())\n    enc = tokenizer.encode(text1)\n\n    start_prob = oof_chr_start[k]\n    end_prob = oof_chr_end[k] + np.arange(141) * 1.0e-15\n\n    y_start_char = start_prob.argmax()\n    end_prob[:y_start_char] = 0\n    y_end_char = end_prob.argmax() + 1\n            \n\n    ss = y_start_char\n    ee = y_end_char\n    st = text1[ss:ee].strip()\n\n        \n    row = Row(\n        original_text=text0,\n        normalized_text=text1,\n        sentiment=sentiments[k],\n        y_start_char=ss,\n        y_end_char=ee,\n    )\n    try:\n        st = postprocess(row)\n    except:\n        print(k)\n\n    list_st.append(st)\n    sc = jaccard(st,selected_text[k])\n    if sentiments[k] == 'neutral':\n        all_nn.append(sc)\n    elif sentiments[k] == 'positive':\n        all_p.append(sc)\n    else:\n        all_n.append(sc)\n\n    all.append(sc)\nprint('>>>> FOLD Jaccard all =',np.mean(all))#, np.mean(all_nn), np.mean(all_p), np.mean(all_n))\nprint('>>>> FOLD Jaccard neutral =',np.mean(all_nn))\nprint('>>>> FOLD Jaccard positive =',np.mean(all_p))\nprint('>>>> FOLD Jaccard negative =',np.mean(all_n))","8785c85b":"# CV inference","2d7e98bd":"* >>>> FOLD Jaccard all = 0.9796338924844558\n* >>>> FOLD Jaccard neutral = 0.996154667866922\n* >>>> FOLD Jaccard positive = 0.969079424824062\n* >>>> FOLD Jaccard negative = 0.967668908646805","ca65fdf0":"* >>>> FOLD Jaccard all = 0.9798332533206786\n* >>>> FOLD Jaccard neutral = 0.9961648726550028\n* >>>> FOLD Jaccard positive = 0.9695626620283051\n* >>>> FOLD Jaccard negative = 0.9678254485028062","05db785c":"# preprocess","e815af7f":"# Postprocess functions","ef4a9cd3":"# Inference"}}