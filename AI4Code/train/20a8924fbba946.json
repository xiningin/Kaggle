{"cell_type":{"595ea9f9":"code","30241018":"code","6c8d3973":"code","383e616c":"code","fa87bc0c":"code","769c972c":"code","9a8aec9b":"code","76cb3fdb":"code","97527a18":"code","0a1a4812":"markdown"},"source":{"595ea9f9":"#importing all the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\n%matplotlib inline\n\n#importing dataset using panda\ndataset = pd.read_csv('..\/input\/kc_house_data.csv')\n#to see what my dataset is comprised of\ndataset.head()","30241018":"#checking if any value is missing\nprint(dataset.isnull().any())","6c8d3973":"#checking for categorical data\nprint(dataset.dtypes)","383e616c":"#dropping the id and date column\ndataset = dataset.drop(['id','date'], axis = 1)","fa87bc0c":"#understanding the distribution with seaborn\nwith sns.plotting_context(\"notebook\",font_scale=2.5):\n    g = sns.pairplot(dataset[['sqft_lot','sqft_above','price','sqft_living','bedrooms']], \n                 hue='bedrooms', palette='tab20',size=6)\ng.set(xticklabels=[]);","769c972c":"#separating independent and dependent variable\nX = dataset.iloc[:,1:].values\ny = dataset.iloc[:,0].values\n#splitting dataset into training and testing dataset\nfrom sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1\/3, random_state = 0)","9a8aec9b":"X.shape","76cb3fdb":"from sklearn.linear_model import LinearRegression\nregressor = LinearRegression()\nregressor.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = regressor.predict(X_test)","97527a18":"#Backward Elimination\nimport statsmodels.formula.api as sm\ndef backwardElimination(x, SL):\n    numVars = len(x[0])\n    temp = np.zeros((21613,19)).astype(int)\n    for i in range(0, numVars):\n        regressor_OLS = sm.OLS(y, x).fit()\n        maxVar = max(regressor_OLS.pvalues).astype(float)\n        adjR_before = regressor_OLS.rsquared_adj.astype(float)\n        if maxVar > SL:\n            for j in range(0, numVars - i):\n                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n                    temp[:,j] = x[:, j]\n                    x = np.delete(x, j, 1)\n                    tmp_regressor = sm.OLS(y, x).fit()\n                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\n                    if (adjR_before >= adjR_after):\n                        x_rollback = np.hstack((x, temp[:,[0,j]]))\n                        x_rollback = np.delete(x_rollback, j, 1)\n                        print (regressor_OLS.summary())\n                        return x_rollback\n                    else:\n                        continue\n    regressor_OLS.summary()\n    return x\n \nSL = 0.05\nX_opt = X[:, [0, 1, 2, 3, 4, 5,6,7,8,9,10,11,12,13,14,15,16,17]]\nX_Modeled = backwardElimination(X_opt, SL)","0a1a4812":"# House Prices using Backward Elimination\n\nJust started with machine learning. I have used backward Elimination to check the usefulness of dependent variables."}}