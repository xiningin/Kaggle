{"cell_type":{"2f09dd93":"code","8fef2629":"code","687ec753":"code","4217935c":"code","66da5ccd":"code","24bd66ee":"code","d5e1a0cc":"code","0a918804":"code","2dd66197":"code","0f9e42a4":"code","c70befdf":"code","c8d9e4ba":"code","9509d41f":"code","352ce6c0":"code","f2383885":"code","d7dab5a0":"code","2eb47d8c":"code","2c5c9591":"code","e96f770e":"code","1c1168f9":"code","93c26b3b":"code","81216172":"code","8b36a0db":"code","ed35a3a3":"code","a4e38b00":"code","9c898e62":"code","33f859d1":"markdown","9b24f6bf":"markdown","e98ba8f4":"markdown","24caa697":"markdown","3db307e6":"markdown","373cc0bc":"markdown","37231ff4":"markdown","98d1ee48":"markdown","100fdab0":"markdown","21583c5b":"markdown","138d1ca0":"markdown","a626ec01":"markdown"},"source":{"2f09dd93":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler\nfrom xgboost import XGBRegressor\nimport random\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8fef2629":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\", low_memory=False)\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\", low_memory=False)\ntrain.info(memory_usage=\"deep\")","687ec753":"test.info(memory_usage=\"deep\")","4217935c":"train.head(10)","66da5ccd":"# Colors to be used for plots\ncolors = [\"lightcoral\", \"sandybrown\", \"darkorange\", \"mediumseagreen\",\n          \"lightseagreen\", \"cornflowerblue\", \"mediumpurple\", \"palevioletred\",\n          \"lightskyblue\", \"sandybrown\", \"yellowgreen\", \"indianred\",\n          \"lightsteelblue\", \"mediumorchid\", \"deepskyblue\"]","24bd66ee":"# Comparing the datasets length\nfig, ax = plt.subplots(figsize=(5, 5))\npie = ax.pie([len(train), len(test)],\n             labels=[\"Train dataset\", \"Test dataset\"],\n             colors=[\"salmon\", \"teal\"],\n             textprops={\"fontsize\": 15},\n             autopct='%1.1f%%')\nax.axis(\"equal\")\nax.set_title(\"Dataset length comparison\", fontsize=18)\nfig.set_facecolor('white')\nplt.show();","d5e1a0cc":"# Statistical description of the train dataset\ntrain.describe(percentiles=[0.1, 0.25, 0.5, 0.75, 0.9]).T","0a918804":"# Checking if there are missing values in the datasets\ntrain.isna().sum().sum(), test.isna().sum().sum()","2dd66197":"fig, ax = plt.subplots(figsize=(16, 8))\n\nbars = ax.hist(train[\"target\"],\n               bins=100,\n               color=\"palevioletred\",\n               edgecolor=\"black\")\nax.set_title(\"Target distribution\", fontsize=20, pad=15)\nax.set_ylabel(\"Amount of values\", fontsize=14, labelpad=15)\nax.set_xlabel(\"Target value\", fontsize=14, labelpad=10)\nax.margins(0.025, 0.12)\nax.grid(axis=\"y\")\n\nplt.show();","0f9e42a4":"print(f\"{(train['target'] < 5).sum() \/ len(train) * 100:.3f}% of the target values are less than 5\")","c70befdf":"# Lists of categorical and numerical feature columns\ncat_features = [\"cat\" + str(i) for i in range(10)]\nnum_features = [\"cont\" + str(i) for i in range(14)]","c8d9e4ba":"# Combined dataframe containing numerical features only\ndf = pd.concat([train[num_features], test[num_features]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n            # Train data histogram\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"deepskyblue\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            # Test data histogram\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   color=\"palevioletred\",\n                                   edgecolor=\"black\",\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n                                  \n        i+=1\n# plt.suptitle(\"Numerical feature values distribution in both datasets\", y=0.99)\nplt.show();","9509d41f":"# Combined dataframe containing categorical features only\ndf = pd.concat([train[cat_features], test[cat_features]], axis=0)\ncolumns = df.columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.2, wspace=0.25)\n\n# Plots counter\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(cat_features): # If there is no more data columns to make plots from\n            axs[r, c].set_visible(False) # Hiding axes so there will be clean background\n        else:\n\n            values = df[cat_features[i]].value_counts().sort_index(ascending=False).index\n            bars_pos = np.arange(0, len(values))\n            if len(values)<4:\n                height=0.1\n            else:\n                height=0.3\n\n            bars1 = axs[r, c].barh(bars_pos+height\/2,\n                                   [train[train[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                   height=height,\n                                   color=\"teal\",\n                                   edgecolor=\"black\",\n                                   label=\"Train Dataset\")\n            bars2 = axs[r, c].barh(bars_pos-height\/2,\n                                   [test[test[cat_features[i]]==x][cat_features[i]].count() for x in values],\n                                   height=height,\n                                   color=\"salmon\",\n                                   edgecolor=\"black\",\n                                   label=\"Test Dataset\")\n            y_labels = [str(x) for x in values]\n\n            axs[r, c].set_title(cat_features[i], fontsize=14, pad=1)\n            axs[r, c].set_xlim(0, len(train[\"id\"])+50)\n            axs[r, c].set_yticks(bars_pos)\n            axs[r, c].set_yticklabels(y_labels)\n            axs[r, c].tick_params(axis=\"y\", labelsize=10)\n            axs[r, c].tick_params(axis=\"x\", labelsize=10)\n            axs[r, c].grid(axis=\"x\")\n            axs[r, c].legend(fontsize=12)\n            axs[r, c].margins(0.1, 0.02)\n                                  \n        i+=1\n\n#plt.suptitle(\"Categorical feature values distribution in both datasets\", y=0.99)\nplt.show();","352ce6c0":"# Bars position should be numerical because there will be arithmetical operations with them\nbars_pos = np.arange(len(cat_features))\n\nwidth=0.3\nfig, ax = plt.subplots(figsize=(14, 6))\n# Making two bar objects. One is on the left from bar position and the other one is on the right\nbars1 = ax.bar(bars_pos-width\/2,\n               train[cat_features].nunique().values,\n               width=width,\n               color=\"darkorange\", edgecolor=\"black\")\nbars2 = ax.bar(bars_pos+width\/2,\n               test[cat_features].nunique().values,\n               width=width,\n               color=\"steelblue\", edgecolor=\"black\")\nax.set_title(\"Amount of values in categorical features\", fontsize=20, pad=15)\nax.set_xlabel(\"Categorical feature\", fontsize=15, labelpad=15)\nax.set_ylabel(\"Amount of values\", fontsize=15, labelpad=15)\nax.set_xticks(bars_pos)\nax.set_xticklabels(cat_features, fontsize=12)\nax.tick_params(axis=\"y\", labelsize=12)\nax.grid(axis=\"y\")\nplt.margins(0.01, 0.05)","f2383885":"# Checking if test data doesn't contain categories that are not present in the train dataset\nfor col in cat_features:\n    print(set(train[col].value_counts().index) == set(test[col].value_counts().index))","d7dab5a0":"# Plot dataframe\ndf = train.drop(\"id\", axis=1)\n\n# Encoding categorical features with OrdinalEncoder\nfor col in cat_features:\n    encoder = OrdinalEncoder()\n    df[col] = encoder.fit_transform(np.array(df[col]).reshape(-1, 1))\n\n# Calculatin correlation values\ndf = df.corr().round(2)\n\n# Mask to hide upper-right part of plot as it is a duplicate\nmask = np.zeros_like(df)\nmask[np.triu_indices_from(mask)] = True\n\n# Making a plot\nplt.figure(figsize=(14,14))\nax = sns.heatmap(df, annot=True, mask=mask, cmap=\"RdBu\", annot_kws={\"weight\": \"normal\", \"fontsize\":9})\nax.set_title(\"Feature correlation heatmap\", fontsize=17)\nplt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n         rotation_mode=\"anchor\", weight=\"normal\")\nplt.setp(ax.get_yticklabels(), weight=\"normal\",\n         rotation_mode=\"anchor\", rotation=0, ha=\"right\")\nplt.show();","2eb47d8c":"columns = train.drop([\"id\", \"target\"], axis=1).columns.values\n\n# Calculating required amount of rows to display all feature plots\ncols = 4\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,20), sharex=False)\n\n# Adding some distance between plots\nplt.subplots_adjust(hspace = 0.3)\n\ni=0\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            scatter = axs[r, c].scatter(train[columns[i]].values,\n                                        train[\"target\"],\n                                        color=random.choice(colors))\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=11)\n            axs[r, c].tick_params(axis=\"x\", labelsize=11)\n                                  \n        i+=1\n# plt.suptitle(\"Features vs target\", y=0.99)\nplt.show();","2c5c9591":"# Creating new features from dummies and concatenating them with original categorical features\ndf = pd.get_dummies(train.append(test)[cat_features])\ndf = pd.concat([df, train.append(test)[cat_features]], axis=1)\n# A list of useful features as per BorutaShap\nuseful = [\"cat8_E\", \"cat1_A\", \"cat8_C\", \"cat3_C\", \"cat1\", \"cat5\", \"cat8\"]\ndf","e96f770e":"# Encoding original categorical features with OrdinalEncoder\nfor col in cat_features:\n    encoder = OrdinalEncoder()\n    df[col] = encoder.fit_transform(np.array(df[col]).reshape(-1, 1))","1c1168f9":"X = pd.concat([train[num_features], df.iloc[:len(train)][useful]], axis=1)\nX_test = pd.concat([test[num_features], df.iloc[len(train):][useful]], axis=1)\ny = train[\"target\"]","93c26b3b":"X","81216172":"# Model hyperparameters\nxgb_params = {'n_estimators': 10000,\n              'learning_rate': 0.35,\n              'subsample': 0.926,\n              'colsample_bytree': 0.84,\n              'max_depth': 2,\n              'booster': 'gbtree', \n              'reg_lambda': 35.1,\n              'reg_alpha': 34.9,\n              'random_state': 42,\n              'n_jobs': 4}","8b36a0db":"%%time\n# Setting up fold parameters\nsplits = 10\nskf = KFold(n_splits=splits, shuffle=True, random_state=42)\n\n# Creating an array of zeros for storing \"out of fold\" predictions\noof_preds = np.zeros((X.shape[0],))\npreds = 0\nmodel_fi = 0\ntotal_mean_rmse = 0\n\n# Generating folds and making training and prediction for each of 10 folds\nfor num, (train_idx, valid_idx) in enumerate(skf.split(X)):\n    X_train, X_valid = X.loc[train_idx], X.loc[valid_idx]\n    y_train, y_valid = y.loc[train_idx], y.loc[valid_idx]\n    \n    model = XGBRegressor(**xgb_params)\n    model.fit(X_train, y_train,\n              verbose=False,\n              # These three parameters will stop training before a model starts overfitting \n              eval_set=[(X_train, y_train), (X_valid, y_valid)],\n              eval_metric=\"rmse\",\n              early_stopping_rounds=100,\n              )\n    \n    # Getting mean test data predictions (i.e. devided by number of splits)\n    preds += model.predict(X_test) \/ splits\n    \n    # Getting mean feature importances (i.e. devided by number of splits)\n    model_fi += model.feature_importances_ \/ splits\n    \n    # Getting validation data predictions. Each fold model makes predictions on an unseen data.\n    # So in the end it will be completely filled with unseen data predictions.\n    # It will be used to evaluate hyperparameters performance only.\n    oof_preds[valid_idx] = model.predict(X_valid)\n    \n    # Getting score for a fold model\n    fold_rmse = mean_squared_error(y_valid, oof_preds[valid_idx], squared=False)\n    print(f\"Fold {num} RMSE: {fold_rmse}\")\n\n    # Getting mean score of all fold models (i.e. devided by number of splits)\n    total_mean_rmse += fold_rmse \/ splits\n    \nprint(f\"\\nOverall RMSE: {total_mean_rmse}\")","ed35a3a3":"# Creating a dataframe to be used for plotting\ndf = pd.DataFrame()\ndf[\"Feature\"] = X.columns\n# Extracting feature importances from the trained model\ndf[\"Importance\"] = model_fi \/ model_fi.sum()\n# Sorting the dataframe by feature importance\ndf.sort_values(\"Importance\", axis=0, ascending=False, inplace=True)","a4e38b00":"fig, ax = plt.subplots(figsize=(13, 10))\nbars = ax.barh(df[\"Feature\"], df[\"Importance\"], height=0.4,\n               color=\"mediumorchid\", edgecolor=\"black\")\nax.set_title(\"Feature importances\", fontsize=30, pad=15)\nax.set_ylabel(\"Feature name\", fontsize=20, labelpad=15)\nax.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax.set_yticks(df[\"Feature\"])\nax.set_yticklabels(df[\"Feature\"], fontsize=15)\nax.tick_params(axis=\"x\", labelsize=15)\nax.grid(axis=\"x\")\n# Adding labels on top\nax2 = ax.secondary_xaxis('top')\nax2.set_xlabel(\"Feature importance\", fontsize=20, labelpad=15)\nax2.tick_params(axis=\"x\", labelsize=15)\n\n# Inverting y axis direction so the values are decreasing\nplt.gca().invert_yaxis()","9c898e62":"predictions = pd.DataFrame()\npredictions[\"id\"] = test[\"id\"]\npredictions[\"target\"] = preds\n\npredictions.to_csv('submission.csv', index=False, header=predictions.columns)\npredictions.head()","33f859d1":"Feature selection below is made basing on [Feature selection with Boruta-SHAP](https:\/\/www.kaggle.com\/lucamassaron\/tutorial-feature-selection-with-boruta-shap) notebook by Luca Massaron","9b24f6bf":"# **Feature importances**","e98ba8f4":"# **Data import**","24caa697":"As you can see, target column is very weakly correlated with all features.\n\nLet's visualize each feature vs target.","3db307e6":"# **Model training**","373cc0bc":"So the datasets are pretty well balanced. Let's look at feature correlation.","37231ff4":"There are no missing value in the both datasets.\n\nLet's check target distribution.","98d1ee48":"# **Data preprocessing**","100fdab0":"Let's check if the datasets have different amount of categories in categorical features.","21583c5b":"# **Predictions submission**","138d1ca0":"The dataset contains categorical and numerical values. Let's see values distribution for these categories.","a626ec01":"# **EDA**"}}