{"cell_type":{"4b09d90f":"code","ff821998":"code","70917bb6":"code","2a388dc3":"code","33dfff24":"code","cceef864":"code","00c74e1c":"code","4c3a544d":"code","88f2a3b4":"code","81268e93":"code","0a93a162":"code","d7e2add9":"code","24297c33":"code","fd7ef8f1":"code","aeb66fca":"code","de87be35":"code","28b18e3a":"code","5bbe921b":"code","5c46a6d4":"markdown"},"source":{"4b09d90f":"from PIL import Image, ImageDraw, ImageFont\nfrom os import listdir\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport cv2\nfrom skimage.feature import hog\nimport os\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import np_utils\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.python import keras\nfrom keras.models import Model,load_model\nprint(os.listdir(\"..\/input\/\"))","ff821998":"fontsize = 50\n\n# From https:\/\/www.google.com\/get\/noto\/\n!wget -q --show-progress https:\/\/noto-website-2.storage.googleapis.com\/pkgs\/NotoSansCJKjp-hinted.zip\n!unzip -p NotoSansCJKjp-hinted.zip NotoSansCJKjp-Regular.otf > NotoSansCJKjp-Regular.otf\n!rm NotoSansCJKjp-hinted.zip\n\nfont = ImageFont.truetype('.\/NotoSansCJKjp-Regular.otf', fontsize, encoding='utf-8')","70917bb6":"df_train = pd.read_csv('..\/input\/kuzushiji-recognition\/train.csv')\nunicode_map = {codepoint: char for codepoint, char in pd.read_csv('..\/input\/kuzushiji-recognition\/unicode_translation.csv').values}\nunicode_map","2a388dc3":"reversed_unicode_map = dict(map(reversed, unicode_map.items()))\nreversed_unicode_map","33dfff24":"reversed_unicode_map['\u3048']","cceef864":"# This function takes in a filename of an image, and the labels in the string format given in a submission csv, and returns an image with the characters and predictions annotated.\ndef Extract_Data():\n    X_=[]\n    y_=[]\n    # Convert annotation string to array #300\n    for img, labels in tqdm(df_train[:420].values):\n        try:\n            image_fn = '..\/input\/kuzushiji-recognition\/train_images\/{}.jpg'.format(img)\n            labels = np.array(labels.split(' ')).reshape(-1, 5)\n            # Read image\n            imsource = Image.open(image_fn).convert('RGBA')\n            bbox_canvas = Image.new('RGBA', imsource.size)\n            char_canvas = Image.new('RGBA', imsource.size)\n            bbox_draw = ImageDraw.Draw(bbox_canvas) # Separate canvases for boxes and chars so a box doesn't cut off a character\n            char_draw = ImageDraw.Draw(char_canvas)\n\n            for codepoint, x, y, w, h in labels:\n                x, y, w, h = int(x), int(y), int(w), int(h)\n                char = unicode_map[codepoint] # Convert codepoint to actual unicode character\n\n                # Draw bounding box around character, and unicode character next to it\n                #bbox_draw.rectangle((x-10, y-10, x+10, y+10), fill=(255, 0, 0, 255))\n                #char_draw.text((x+25, y-fontsize*(3\/4)), char, fill=(255, 0, 0, 255), font=font)\n                Croped_image = imsource.crop((x, y, x+w, y+h))\n                image = Croped_image.resize((300,300))\n                image = np.asarray(image)\n                image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n                ret,th1 = cv2.threshold(image,155,255,cv2.THRESH_BINARY_INV)\n                X_.append(th1)\n                y_.append(str(unicode_map[codepoint]))\n        except:\n            pass\n    X_ = np.array(X_)\n    y_ = np.array(y_)\n\n    '''imsource = Image.alpha_composite(Image.alpha_composite(imsource, bbox_canvas), char_canvas)\n    imsource = imsource.convert(\"RGB\") '''# Remove alpha for saving in jpg format.\n    return X_,y_","00c74e1c":"XX_,yy_ = Extract_Data()","4c3a544d":"IMG_ROWS=300\nIMG_COLS=300\ndef PreProcessData(X,y):\n    lb = LabelEncoder()\n    y_integer = lb.fit_transform(y)\n    out_y = np_utils.to_categorical(y_integer)\n    num_images = X.shape[0]\n    out_x = X.reshape(num_images, IMG_ROWS, IMG_COLS, 1)\n    #out_x = x_shaped_array \/ 255\n    return out_x, out_y","88f2a3b4":"lb = LabelEncoder()\ny_integer = lb.fit_transform(yy_)","81268e93":"Model_ = load_model('..\/input\/kuzushijirecognitionweight\/model_Kuzushiji.h5')","0a93a162":"IMG_ROWS=300\nIMG_COLS=300\ndef SubmissionKuzushiji(imagePath):\n    img = cv2.imread(imagePath)\n    im_grey = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    ret, im_th = cv2.threshold(im_grey, 130, 255, cv2.THRESH_BINARY_INV)\n    ctrs,_ = cv2.findContours(im_th.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n    rects = [cv2.boundingRect(ctr) for ctr in ctrs]\n    Kuzushijis = []\n    for rect in rects:\n        leng = int(rect[3] * 1.6)\n        pt1 = int(rect[1] + rect[3]\/\/2 - leng\/\/ 2)\n        pt2 = int(rect[0] + rect[2]\/\/2 - leng\/\/ 2)\n        roi = im_th[pt1:pt1+leng, pt2:pt2+leng]\n        if roi.size>7000:\n            roi = cv2.resize(roi, (300,300))\n            ret,th1 = cv2.threshold(roi,155,255,cv2.THRESH_BINARY)\n            ProcessImage = th1.reshape(1,IMG_ROWS, IMG_COLS, 1)\n            y_pred = Model_.predict(ProcessImage)\n            y_true = np.argmax(y_pred,axis=1)\n            Kuzushiji = lb.inverse_transform(y_true)\n            Unicode_kuzushiji = reversed_unicode_map[str(Kuzushiji[0])]\n            #print(Kuzushiji[0])\n            Kuzushijis.append(Unicode_kuzushiji+\" \"+str(rect[0])+\" \"+str(rect[1]))\n    result=' '.join(Kuzushijis)\n    return result","d7e2add9":"results = SubmissionKuzushiji('..\/input\/kuzushiji-recognition\/test_images\/test_001c37e2.jpg')\nresults","24297c33":"Images = sorted(os.listdir(\"..\/input\/kuzushiji-recognition\/test_images\"))","fd7ef8f1":"len(Images)","aeb66fca":"labels=[]\nimage_id=[]\n#print(os.listdir(\"..\/input\/kuzushiji-recognition\/test_images\"))\nfor img in tqdm(Images[:830]):\n    recognition = SubmissionKuzushiji('..\/input\/kuzushiji-recognition\/test_images\/'+img)\n    image_id.append(img[:-4])\n    labels.append(recognition)\nfor img in tqdm(Images[830:1660]):\n    recognition = SubmissionKuzushiji('..\/input\/kuzushiji-recognition\/test_images\/'+img)\n    image_id.append(img[:-4])\n    labels.append(recognition)\nfor img in tqdm(Images[1660:2490]):\n    recognition = SubmissionKuzushiji('..\/input\/kuzushiji-recognition\/test_images\/'+img)\n    image_id.append(img[:-4])\n    labels.append(recognition)\nfor img in tqdm(Images[2490:3320]):\n    recognition = SubmissionKuzushiji('..\/input\/kuzushiji-recognition\/test_images\/'+img)\n    image_id.append(img[:-4])\n    labels.append(recognition)\nfor img in tqdm(Images[3320:4150]):\n    recognition = SubmissionKuzushiji('..\/input\/kuzushiji-recognition\/test_images\/'+img)\n    image_id.append(img[:-4])\n    labels.append(recognition)","de87be35":"len(labels)","28b18e3a":"my_submission = pd.DataFrame({'image_id': image_id, 'labels': labels})\nmy_submission.to_csv('SubmissionVictorKuzushiji.csv', index=False)","5bbe921b":"my_submission.head()","5c46a6d4":"## Kuzushiji Recognition with the concept of Hand-Written digit recognition\nThis kernel is the second part of my previous kernel [Kuzushiji Recognition just like Digit Recognition](https:\/\/www.kaggle.com\/basu369victor\/kuzushiji-recognition-just-like-digit-recognition), in this kernel I have only performed prediction from the model which I created previously and performed the submission.\n<br>For a more detailed explanation of this model please visit my previous kernel, there I have explained everything part by part in details.\n<br><br>**Arigatho Gozaimas**<br>**( eng sub: Thank you)**."}}