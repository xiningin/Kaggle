{"cell_type":{"19a04cfb":"code","a68fdd11":"code","73cc8726":"code","16141a62":"code","210e2785":"code","0d82c6b5":"code","92ed9ae2":"code","1bf11ac3":"markdown","337afb7b":"markdown","e205870c":"markdown","7ab3022a":"markdown","a1de5a7d":"markdown","151648b6":"markdown","28521b39":"markdown","2efd9b81":"markdown","7a26f01f":"markdown","b9a7734a":"markdown","bed1785a":"markdown","5c1bb0e6":"markdown","9ecf141d":"markdown","62d5bc33":"markdown","467ae894":"markdown","6be297b7":"markdown","3a89bcdf":"markdown","8c6daa21":"markdown","173d05c0":"markdown","10c1ba05":"markdown","4d8a70e1":"markdown","27e3e6ff":"markdown","c1a041ab":"markdown","6962c2c9":"markdown","0a222dac":"markdown","c8566bfd":"markdown","49a214eb":"markdown","e6a30c70":"markdown","4483ef17":"markdown","65d51186":"markdown","ecfa8cdf":"markdown"},"source":{"19a04cfb":"import requests\nfrom bs4 import BeautifulSoup","a68fdd11":"url = 'https:\/\/www.ratemyprofessors.com\/ShowRatings.jsp?tid=941931'","73cc8726":"page = requests.get(url)","16141a62":"page","210e2785":"soup = BeautifulSoup(page.text, \"html.parser\")","0d82c6b5":"proftags = soup.findAll(\"span\", {\"class\": \"Tag-bs9vf4-0\" })\nproftags","92ed9ae2":"for mytag in proftags:\n    print(mytag.get_text())","1bf11ac3":"Here we remove all the HTML tags and convert it to a text format, this can be done with the help of get_text method placed inside a for loop. This converts the HTML into the text format.","337afb7b":"**Step 1: Importing the required libraries**","e205870c":"Here is the place where you shall add the tags that you are looking for, to get the tag name all you have to do is to right click on the respected tag or click Ctrl-Shift-I on the tag in the webpage, then a page with selected tag will open for you to your right-hand side as shown below:","7ab3022a":"### Let\u2019s scrape the data from the web using Python.","a1de5a7d":"![alt text](https:\/\/i.kym-cdn.com\/entries\/icons\/original\/000\/026\/575\/5ac80d20dfc1e.image.jpg)","151648b6":"\n\n---\n\n","28521b39":"Here we use the requests library by passing \u201curl\u201d as a parameter, be careful don\u2019t run this multiple times. If you get like Response 200 then its success, if you get something else then there is something wrong with maybe the code or your browser I don\u2019t know.","2efd9b81":"![alt text](http:\/\/i64.tinypic.com\/35klron.png)\n","7a26f01f":"\n\n---\n\n","b9a7734a":"You can then copy the HTML tag and class if any, and then place it inside the soup.findAll method. In this case, the HTML tag is \u201cspan\u201d and class is \u201ctag-box-choosetags\u201d","bed1785a":"\n\n---\n\n","5c1bb0e6":"Let us import few important libraries such as Requests and BeautifulSoup.","9ecf141d":"You might be wondering what tags to extract, well in the Rate My Professor website every professor will have his\/her respected tags such as (hilarious, heavy homework, study hard or fail, etc.), we will just try to extra these tags in this tutorials as shown below.","62d5bc33":"Let\u2019s get started.","467ae894":"\n**Step 3: Making a request to the website using the requests library.**\n","6be297b7":"**Hence we got the above information that we were looking for. We got all the tags of the professor. This is how we scrape the data from the internet by using Requests and Beautiful Soup libraries. To be frank this is my professor who teaches the subject \u201cData Science\u201d. He is one of the best professors in the entire university. I like his teaching and his style.**","3a89bcdf":"**Step 5: Using soup.findAll method to get the respected tag that we are looking for.**\n\n\n","8c6daa21":"![alt text](https:\/\/raw.githubusercontent.com\/pluralsight\/guides\/master\/images\/310d6edd-b569-408a-a61d-f6d9a9a9eb61.png)","173d05c0":"**Step 2: Getting the URL and storing it in a variable.**","10c1ba05":"## The most useful libraries required for web scraping are:\n\n1. [Beautiful Soup.](https:\/\/www.crummy.com\/software\/BeautifulSoup\/bs4\/doc\/?source=post_page---------------------------)\n\n2. [Requests.](https:\/\/2.python-requests.org\/en\/master\/?source=post_page---------------------------)","4d8a70e1":"Here we use the BeautifulSoup by passing the page.text as a parameter and using the HTML parser. You can try to print the soup, but printing the soup doesn\u2019t give you the answer, rather it contains huge chunks of HTML data, so I decided not to show it here.","27e3e6ff":"**What is Web Scraping ?**\n\nWeb Scraping (also termed as Scraping, Data Extraction, Data Harvesting, etc.) is a technique used to extract data from the websites. Sometimes web scraping can be very useful wherein we can get the data that we are looking for straight from the web, but sometimes it a bad way to do it, because it\u2019s like stealing the precious data from the website without their permission, but limit your scraping process to once or twice so that this can avoid you from falling in trouble.\n\n\n","c1a041ab":"**Step 4: Using the Beautiful Soup library to get the HTML (raw) data from the website.**","6962c2c9":"**Step 6: Removing all the HTML tags and converting it to a plain text format.** ","0a222dac":"\n\n---\n\n","c8566bfd":"## These are the steps that we would be following throughout this tutorial:\n\n1. Importing the required libraries.\n\n2. Getting the URL and storing it in a variable.\n\n3. Making a request to the website using the requests library.\n\n4. Using the Beautiful Soup library to get the HTML (raw) data from the website.\n\n5. Using soup.findAll method to get the respected tag that we are looking for.\n\n6. Removing all the HTML tags and converting it to a plain text format.","49a214eb":"Let us store the URL of the professor in the variable named \u201curl\u201d. The URL of the website can be found here: \u201c[Rate My Professor](https:\/\/www.ratemyprofessors.com\/ShowRatings.jsp?tid=1986099&source=post_page---------------------------)\u201d.","e6a30c70":"Before we begin make sure to scrape the data at a slow pace, and you can also use a VPN service to get a different IP address, this prevents your IP address from banning, but I hope you guys will follow the instructions. Here is an [article](https:\/\/hackernoon.com\/how-to-scrape-a-website-without-getting-blacklisted-271a605a0d94?source=post_page---------------------------) that will let you know how to scrape a website without getting blacklisted. One important thing in this tutorial there is no point of me explaining each and every line of code, which is not needed here because python code is self-explanatory. However, I will try not to confuse you, and make things clear in an easy way. So I wrote this tutorial in such a way that everybody can understand irrespective of their programming background. Moreover, the entire source code can be found in my GitHub. There might be numerous tutorials available on the internet, but this tutorial is easy to understand because I have tried to explain the code as much as possible, some parts are a mechanical process, wherein you just have to follow them, just let me know if you have any doubt in the comments section down below.","4483ef17":"\n\n---\n\n","65d51186":"Today we will be scraping \u201c[Rate My Professor](https:\/\/www.ratemyprofessors.com\/)\u201d website. A little insight about Rate My Professor website, it is a website that contains a rating of school, professors and universities. You can search for any professor or school and get their ratings before taking or joining to their courses. It\u2019s a handy feature which helps to know more about your professor or the university that you want to join. In this tutorial, we shall see how to scrape and to extract a specific professor\u2019s tag. I warn you guys this is not illegal but the mass scraping of data from the website can lead your IP address being blocked. Just do it once or twice, but don\u2019t just foolishly put it in a loop and try to put request inside the loop.","ecfa8cdf":"**Thank you guys for spending your time reading my tutorial, stay tuned for more updates. Let me know what is your opinion about this tutorial in the comment section below. Also if you have any doubts regarding the code, comment section is all yours. Have a nice day.**"}}