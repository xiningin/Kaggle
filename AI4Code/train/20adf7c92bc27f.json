{"cell_type":{"cdf0f813":"code","df6abe24":"code","4937f180":"code","93372491":"code","efcb5858":"code","500edecc":"code","0ffba7fd":"code","2f35422b":"code","df0ba2f4":"code","87522402":"code","b6e23e6f":"code","82c68f03":"code","a9dd7891":"code","cbb2f35a":"code","d43f2fb4":"code","e5b01a4c":"code","737b8bb5":"code","a802ef3a":"code","229e477a":"code","a50f3af0":"code","d43f47f9":"code","78941f17":"code","0adb7ca1":"code","4a7e6b6e":"code","2c28e248":"code","bbcdcf0b":"code","e8249d31":"markdown","e0613f05":"markdown","387beab9":"markdown","a255ec24":"markdown","96f06210":"markdown","b53183e2":"markdown","16ea661b":"markdown","cce4e052":"markdown","0fbda6c7":"markdown","914dd1bf":"markdown","9cbe477e":"markdown","8b8f879a":"markdown","3749e47d":"markdown","a99c4cb7":"markdown"},"source":{"cdf0f813":"# Package for data science\nimport pandas as pd\nimport numpy as np\n\n# Package for visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Packages for machine learning\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import SGDRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import StackingRegressor\n\n# Others\nimport warnings\nwarnings.filterwarnings(\"ignore\")","df6abe24":"data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndata.head()","4937f180":"num_cols = data.select_dtypes(exclude='object').columns.tolist()\ncat_cols = data.select_dtypes(include='object').columns.tolist()\nprint('The dataset contains:', data.dtypes.unique())\nprint('There are', len(num_cols), 'numerical columns')\nprint('There are', len(cat_cols), 'categorical features')","93372491":"plt.figure(figsize=(15,8))\nsns.heatmap(data.isnull(), cbar = False, cmap=\"gray\")","efcb5858":"#pd.set_option('display.max_rows', None)\ndata.isnull().sum().sort_values(ascending=False)\nmissing = pd.DataFrame(data.isnull().sum().sort_values(ascending=False), columns=['No.of missing values'])\nmissing ['% missing'] = (missing\/len(data)).round(3)*100\nmissing[missing['No.of missing values']>0]","500edecc":"cols_to_drop = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']\ndata.drop(cols_to_drop, axis=1, inplace=True)","0ffba7fd":"target = data.iloc[:,-1]\ntarget.describe()","2f35422b":"### Chekcing target varlable\nfig, ax  = plt.subplots(1,2,figsize=(12,4))\nfig.patch.set_facecolor( '#F2F2F2')\n\nsns.histplot(target, bins=30, color='#8abbd0', kde=True, ax=ax[0])\nax[0].patch.set_facecolor( '#F2F2F2')\nax[0].lines[0].set_color('#F97A1F')\n\nsns.boxplot(x=target, ax=ax[1], color='#8abbd0')\nax[1].patch.set_facecolor( '#F2F2F2')\n\nplt.tight_layout()","df0ba2f4":"# Checking statstical summary for numerical columns\ndata.describe()","87522402":"# Checking correlation between different numerical featuers\nfig = plt.figure(figsize=(15,12)) \ncorr = data.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(data.corr(), cmap='Blues', mask=mask, linewidth=0.05, square=True)","b6e23e6f":"# Identify feature pairs that have correlation > 0.8  and < - 0.8\ncorrelation_table = pd.DataFrame(data.corr().unstack().sort_values().drop_duplicates(), columns=['correlation'])\ncorrelation_table[(correlation_table['correlation'] >0.8) | (correlation_table['correlation'] < -0.8)].sort_values(by='correlation', ascending = False)","82c68f03":"cols_to_drop = ['1stFlrSF', 'GrLivArea', 'GarageYrBlt', 'GarageCars','Id']\ndata.drop(cols_to_drop, axis=1, inplace=True)","a9dd7891":"# Plots on numerical features to check data quality and data distribution\nnum_cols = data.select_dtypes(exclude='object').columns.tolist()[:-1]\ncolor ='#8abbd0'\nfor feature in num_cols:\n    fig, ax = plt.subplots(1,4, figsize=(15,3))\n    fig.patch.set_facecolor( '#F2F2F2')\n    sns.histplot(data[feature], bins=20, ax=ax[0],  color=color, kde=True)\n    ax[0].lines[0].set_color('#F97A1F')\n    sns.kdeplot(x=feature,data=data,ax=ax[1],shade=True, alpha=0.3)\n    sns.boxplot(x=feature, data=data,ax=ax[2], color=color)\n    \n    sns.regplot(x=feature, y='SalePrice', data=data, ax=ax[3], color=color, scatter_kws={\"s\": 8}, line_kws={'color':'#F97A1F'})\n    correlation = np.corrcoef(data['SalePrice'], data[feature])[0][1]\n    ax[3].text(x=0.95, y=0.9, \n               s = ['correlation=', \"{:.2f}\".format(correlation)], \n               ha='right', transform=ax[3].transAxes)\n            \n    plt.suptitle(feature, fontfamily='serif', fontsize=16, color='#173b56', fontweight='bold')\n    plt.tight_layout()","cbb2f35a":"# Modify features\ndata['BuiltAge'] = 2010 - data['YearBuilt']\ndata ['RemodAge'] = 2010 - data['YearRemodAdd']","d43f2fb4":"# Numerical columns to keep\nnum_cols_to_keep = ['LotFrontage', 'LotArea', 'OverallQual' ,'BuiltAge' ,'RemodAge', 'TotalBsmtSF','FullBath', 'TotRmsAbvGrd', 'GarageArea']","e5b01a4c":"# Plots on catagorical features to check data quality and data distribution\ncat_cols = data.select_dtypes(include='object').columns.tolist()\n\n#palette = ['#8abbd0', '#FB9851', '#36E2BD','#D0E1E1']\n\nfor feature in cat_cols:\n    fig, ax = plt.subplots(1,4, figsize=(15,3))\n    fig.patch.set_facecolor('#F2F2F2')\n    \n    sns.countplot(y=feature, data=data, alpha=0.8, ax=ax[0])\n    sns.violinplot(y='SalePrice', x=feature, data=data, ax=ax[1])\n    sns.boxplot(y='SalePrice', x=feature, data=data, ax=ax[2])\n    sns.pointplot(y='SalePrice', x=feature, data=data, ax=ax[3])\n    \n    plt.suptitle(feature, fontfamily='serif', fontsize=16, color='#173b56', fontweight='bold')\n    plt.tight_layout()","737b8bb5":"# categorical columns to keep\ncat_cols_to_keep = ['LotShape', 'Neighborhood', 'MasVnrType', 'ExterQual', 'Foundation' ,'BsmtQual',\n                                     'BsmtExposure', 'BsmtFinType1', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n                                     'GarageType', 'GarageFinish', 'PavedDrive']","a802ef3a":"# create new dataframe that includes filtered features only\ncols_to_keep = [num_cols_to_keep, cat_cols_to_keep, ['SalePrice']]\ncols_to_keep = [element for sublist in cols_to_keep for element in sublist]\ndata1 = data[cols_to_keep]","229e477a":"print('The new dataframe has', len(data1.columns),'columns')","a50f3af0":"# Data preprocessing\n\n# Extract new numerical and categorical columns\nnum_cols = data1.select_dtypes(exclude='object').columns.tolist()\ncat_cols = data1.select_dtypes(include='object').columns.tolist()\n\n# Convert categorical values to numerical values\nle = LabelEncoder()\ndata1[cat_cols] = data1[cat_cols].astype('str').apply(le.fit_transform)\n\n# Impute missing values for categorical features\nmode_values=data1[cat_cols].mode()\ndata1[cat_cols] = data1[cat_cols].fillna(value=mode_values)\n\n# Impute missing values for numerical features\nmedian_values = data1[num_cols].median()\ndata1[num_cols] = data1[num_cols].fillna(value=median_values)","d43f47f9":"X = data1.iloc[:, :-1]\ny = data1.iloc[:, -1]\n\n# Split train test data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)","78941f17":"# Function to determine feature importance for tree-based models\ndef feature_importance(name, reg):\n    importances =reg.feature_importances_\n    importance_table = np.array([[name], importances])\n    importance_table=[element for sublist in importance_table for element in sublist]\n    return importance_table","0adb7ca1":"# Function to determine feature coefficient \ndef feature_coefficient(name, reg):\n    importances = reg.coef_\n    importance_table = np.array([[name], importances])\n    importance_table=[element for sublist in importance_table for element in sublist]\n    return importance_table  ","4a7e6b6e":"# Create and instantiate regressors\nregs = [\n            ('LinearReg', LinearRegression()),\n            ('Ridge', Ridge()),\n            ('Lasso', Lasso()),\n#            ('SGD',SGDRegressor()),\n            ('BayesianRidge', BayesianRidge()),\n            ('RF', RandomForestRegressor()),\n            ('DT', DecisionTreeRegressor()),\n            ('GradientBoost', GradientBoostingRegressor()),\n           ]\n\n# Create empty lists to store model scores and feature coefficient\/importance\nscores = []\nimportance_tables = []\n\n# Fitting and training models\nfor  name, reg in regs:\n    # Train models\n    reg.fit(X_train, y_train)  \n    # Predict target value     \n    y_pred = reg.predict(X_test) \n    \n    # Summary of how different models perform\n    score = reg.score(X_test, y_test)\n    scores.append([name, score])\n    \n    # Result to show importance of different features\n    if name == 'RF' or name =='DT':\n        importance = feature_importance(name, reg)\n        importance_tables.append(importance)\n    if name in ['LinearReg', 'Ridge', 'Lasso', 'SGD', 'BayesianRidge']:\n        importance = feature_coefficient(name, reg)\n        importance_tables.append(importance)","2c28e248":"# Compare model performance\nscores = pd.DataFrame(scores)\nscores.columns = ['Model', 'R2']\nscores = scores.sort_values(by='R2',ascending=False)\npd.options.display.float_format = '{:.2f}'.format\nscores","bbcdcf0b":"# Plot feature importance for different models\ncols_name = [['Model'], X.columns]\ncols_name=[element for sublist in cols_name for element in sublist]\nimportance_tables = pd.DataFrame(importance_tables, columns= cols_name)\nimportance_tables1 = importance_tables.set_index('Model')\n    \nfig = plt.figure(figsize=(15,18))\nfig.patch.set_facecolor('#F2F2F2')\n\nfor i, (col_name, row) in enumerate(importance_tables1.iterrows()):\n    indices = np.argsort(row)\n    ax = plt.subplot(3,4, i+1)\n    ax.barh(range(len(row)), row[indices], color='#8abbd0')\n    ax.set_yticks(range(len(row)))\n    ax.set_yticklabels(row.index[indices])    \n    ax.set_title(col_name)\n   \nplt.suptitle('Feature Importance\/coefficent for different models', \n                     y=0.99, va = 'top',\n                     fontfamily='serif', fontsize=20, color='#173b56', fontweight='bold')\nplt.tight_layout()","e8249d31":"\n### <font color='\t#00c7c7'> Thanks for reading. Please upvote if you like this notebook. Thanks.<\/font>\n","e0613f05":"## <font color='\t#2E45B8'>  2.2 Chekcing numerical features<\/font>","387beab9":"# <font color='\t#2E45B8'> 3. Machine Learning<\/font>\n## <font color='\t#2E45B8'> 3.1 Data preprocessing<\/font>\n\n- Scikit-learn only works with numerical number, so we will need to convert categorical features to numerical vallues\n    - I will use LabelEncoder here\n- There are some missing values\n    - I will impute missing categorical values with mode\n    - I will impute missing numerical values with mean","a255ec24":"# <font color='\t#2E45B8'> 2. EDA<\/font>\n## <font color='\t#2E45B8'>  2.1 Checking target variable<\/font>","96f06210":"#### Comment\n- Random forest is the best performing model\n- `OverallQual` is the most important factor in determining house prices","b53183e2":"#### Comment:\nWe have plotted charts to show features distribution (the hist plot and kde plot), feature statistics (the box plot) and its relationship with `SalePrice` (the scatter plot). We will examine each numerical feature one by one, and hopefully to eliminate features that are less likely to impact `SalePrice`\n\n|Feature   |Drop\/Keep   |Reason   |\n|:-----|:----- |:----- |\n|`MSSubClass`| Drop | low correlation with`SalePrice`|\n|`LotFrontage`| Keep | has 'na' in the dataset. impute na values and check again|\n|`LotArea`| Keep | remove outliers and check again|            \n|`OverallQual` | Keep | strong correlation|\n|`OverallCond` |Drop | weak correlation|\n|`YearBuilt` | Keep | good correlation|\n|`YearRemodAdd` |Keep | good correlation|\n|`MasVnrArea`  | Drop | more than half of dataset is 0|\n|`BsmtFinSF1`  | Drop | more than half of dataset is 0|\n|`BsmtFinSF2` | Drop | majority value is zero|\n|`BsmtUnfSF` | Drop | low correlation|\n|`TotalBsmtSF` |Keep | good correlation|\n|`2ndFlrSF` | Drop | more than half of dataset is 0|\n|`LowQualFinSF` | Drop | more than half of dataset is 0|\n|`BsmtFullBath` | Drop | weak correlation|\n|`BsmtHalfBath` | Drop | weak correlation|\n|`FullBath` | Keep | good correlation|\n|`HalfBath` | Drop | low correlation|\n|`BedroomAbvGr` | Drop | low correlation|\n|`KitchenAbvGr` | Drop | low correlation|\n|`TotRmsAbvGrd` | Keep | good correlation|\n|`Fireplaces` | Drop | low correlation|\n|`GarageArea` | Keep | good correlation|\n|`WoodDeckSF` | Drop | more than half of dataset is 0|\n|`OpenPorchSF` | Drop | more than half of dataset is 0|\n|`EnclosedPorch` | Drop | more than half of dataset is 0|\n|`3SsnPorch` | Drop | more than half of dataset is 0|\n|`ScreenPorch` | Drop | more than half of dataset is 0|\n|`PoolArea` | Drop | more than half of dataset is 0|\n|`MiscVal` | Drop | more than half of dataset is 0|\n|`MoSold` | Drop | low correlation|\n|`YrSold` | Drop | low correlation|\n\nSince there are two 'kept' features related to year, we will modify this to age instead. As maximum value for both `YearBuilt` and `YearRemodAdd` are 2010.  We assume that was the year that the data is released. We will replace `YearBuilt` and `YearRemodAdd` with `BuiltAge` and `RemodAge` by subtracting them from year 2010\n\nAfter the above analysis, we will only keep the following numerical features: `LotFrontage`, `LotArea`, `OverallQual` ,`BuiltAge` , `RemodAge`, `TotalBsmtSF`,`FullBath`,`TotRmsAbvGrd`,`GarageArea`","16ea661b":"## <font color='\t#2E45B8'> 3.2 Model prediction and evaluation<\/font>\n\nWe have selected a list of regressors, all with default hyperparameter settings. We will see which regressor has the best performance. ","cce4e052":"# <font color='\t#2E45B8'> 1. Data Preparation<\/font>\n## <font color='\t#2E45B8'>  1.1 Import libraries and data<\/font>","0fbda6c7":"#### Comment\n- The following pairs show strong correlation\n- `GarageArea` & `GarageCars`:\t~0.882475\n- `YearBuilt` &\t`GarageYrBlt`:\t~0.825667\n- `TotRmsAbvGrd` &\t`GrLivArea`:\t~0.825489\n- `TotalBsmtSF` &\t`1stFlrSF`:\t~0.819530\n- We will remove following columns [`1stFlrSF`, `GrLivArea`, `GarageYrBlt`, `GarageCars`]\n- We will also drop the `Id` column\n- We also see that some features have strong correlation with the target variable, we will drop features that have less impact on `SalePrice` after analysing numerical features in the next section.","914dd1bf":"### <font color='\t#2E45B8'> 1.2.2 Explore missing values<\/font>","9cbe477e":"#### Comment\nSimilar to the numerical feature analysis, we will examin the categorical features one by one, and remove less relevent features. We have a countplot, showing the total no. of elements within each categorical feature. The violinplot and box plots show the distribution of values at each level of the categorical variables. The mean values of elements within categorical feature are illustrated in pointplot and their average can be easily compared with. \n\n|Feature   |Drop\/Keep   |Reason   |\n|:-----|:----- |:----- |\n|`MSZoning` | Drop | majority is `RL`|\n|`Street`| Drop | majority is `Pave`|\n|`LotShape` | Keep | `Reg` is the cheapest|\n|`LandContour`| Drop | majority is `Lv1`|\n|`Utilities` | Drop | majority is `AllPub`|\n|`LotConfig`  | Drop | majority is `Inside`|\n|`LandSlope`| Drop | majority is `Gd`|\n|`Neighborhood` | Keep | some areas are significantly more expensive than others|\n|`Condition1`| Drop | majority is `Gd`|\n|`Condition2`| Drop | majority is `Norm`|\n|`BldgType`| Drop | majority is `1Farm`|\n|`HouseStyle` | Drop | majority is `2Story` and `1Story`|\n|`RoofStyle` | Drop | majority is `Gable` and `Hip`|\n|`RoofMatl` | Drop | majority is `Compshg`|\n|`Exterior1st`| Drop | mean for different items does not vary much|\n|`Exterior2nd`| Drop | mean for different items does not vary much|\n|`MasVnrType` | Keep | `Stone` significantly more expensive than `None`|\n|`ExterQual`| Keep | `Ex` is significantly more expensive than `TA`|\n|`ExterCond`| Drop | majority is `TA`|\n|`Foundation`| Keep | `PConc` is significantly more expensive than `CBlock`|\n|`BsmtQual`| Keep | `Ex` is more expensive than others|\n|`BsmtCond`| Drop | majority is `TA`|\n|`BsmtExposure` | Keep | `No` is significantly cheaper than others|\n|`BsmtFinType1`| Keep | `GLQ` is significantly more expensive than others|\n|`BsmtFinType2`| Drop | majority is `Unf`|\n|`Heating`| Drop | majority is `GasA`|\n|`HeatingQC`| Keep | `Ex` is more expensive than others|\n|`CentralAir`| Keep | `Y` is more expensive than `N`|\n|`Electrical`| Keep | `SBrkr` is more expensive than `N`|\n|`KitchenQual`| Keep | significant difference in mean|\n|`Functional`| Drop | majority is `Typ`|\n|`GarageType`| Keep | `Attchd` is more expensive than `Detchd`|\n|`GarageFinish`| Keep | significant difference in mean|\n|`GarageQual`| Drop | majority is `TA`|\n\nWe will keep the following categorical features: `LotShape`,`Neighborhood`,`MasVnrType`,`ExterQual`,`Foundation`,`BsmtQual`,`BsmtExposure`,`BsmtFinType1`,`HeatingQC`,`CentralAir`,`Electrical`,\n`KitchenQual`,`GarageType`,`GarageFinish`,`PavedDrive`","8b8f879a":"## <font color='\t#2E45B8'> 1.2 Data inspection\u00b6<\/font>\n### <font color='\t#2E45B8'> 1.2.1 Data types<\/font>","3749e47d":"#### Comment\n- Out of 81 columns, 19 columns have missing values. \n- In particular, more than 45% of numbers are missing within `PoolQC`, `MiscFeature`, `Alley`, `Fence`, `FireplaceQu` columns\n- Therefore, we will drop these features","a99c4cb7":"## <font color='\t#2E45B8'>  2.3 Chekcing categorical features<\/font>"}}