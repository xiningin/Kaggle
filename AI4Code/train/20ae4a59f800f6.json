{"cell_type":{"8a2f2d22":"code","8f0689ea":"code","e4e9b496":"code","de547a63":"code","e0167bcb":"code","601ad2d3":"code","b0216be9":"code","a4c02fa4":"code","ac0695ba":"code","77310dcd":"code","a9af0f9a":"code","c00bc6ba":"code","d72826e4":"code","25f96901":"code","892b9cb7":"code","72555765":"code","ac43eff7":"code","55165492":"code","025fb957":"markdown","c01668ef":"markdown","21739c95":"markdown","bd6d2c64":"markdown","56c069fb":"markdown","b7b2e5e3":"markdown","d50cf94e":"markdown","54bdebe3":"markdown","41aed1c2":"markdown","97753118":"markdown","7960884f":"markdown","69ecda74":"markdown"},"source":{"8a2f2d22":"import pandas as pd, numpy as np\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","8f0689ea":"train = pd.read_csv('..\/input\/train.csv.zip')\ntest = pd.read_csv('..\/input\/test.csv.zip')\nsubm = pd.read_csv('..\/input\/sample_submission.csv.zip')","e4e9b496":"train.head(10)","de547a63":"train['comment_text'][0]","e0167bcb":"train['comment_text'][6]","601ad2d3":"lens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","b0216be9":"lens.hist();","a4c02fa4":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain['none'] = 1-train[label_cols].max(axis=1)\ntrain.describe()","ac0695ba":"len(train),len(test)","77310dcd":"COMMENT = 'comment_text'\ntrain[COMMENT].fillna(\"unknown\", inplace=True)\ntest[COMMENT].fillna(\"unknown\", inplace=True)","a9af0f9a":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()","c00bc6ba":"n = train.shape[0]\nvec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n               smooth_idf=1, sublinear_tf=1 )\ntrn_term_doc = vec.fit_transform(train[COMMENT])\ntest_term_doc = vec.transform(test[COMMENT])","d72826e4":"trn_term_doc, test_term_doc","25f96901":"def pr(y_i, y):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)","892b9cb7":"x = trn_term_doc\ntest_x = test_term_doc","72555765":"def get_mdl(y):\n    y = y.values\n    r = np.log(pr(1,y) \/ pr(0,y))\n    m = LogisticRegression(C=4, dual=True)\n    x_nb = x.multiply(r)\n    return m.fit(x_nb, y), r","ac43eff7":"preds = np.zeros((len(test), len(label_cols)))\n\nfor i, j in enumerate(label_cols):\n    print('fit', j)\n    m,r = get_mdl(train[j])\n    preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]","55165492":"submid = pd.DataFrame({'id': subm[\"id\"]})\nsubmission = pd.concat([submid, pd.DataFrame(preds, columns = label_cols)], axis=1)\nsubmission.to_csv('submission.csv', index=False)","025fb957":"## Building the model\n\n<!--\nWe'll start by creating a *bag of words* representation, as a *term document matrix*. We'll use ngrams, as suggested in the NBSVM paper.\n-->\n\n*bag of words* \ud45c\ud604\uc744 *term document matrix*\ub85c \ub9cc\ub4e4\uba74\uc11c \uc2dc\uc791\ud560 \uac83\uc774\ub2e4. NBSVR \ub17c\ubb38\uc5d0\uc11c \uc81c\uc548\ud55c ngram\uc744 \uc0ac\uc6a9\ud560 \uac83\uc774\ub2e4.","c01668ef":"<!--\nThe length of the comments varies a lot.\n-->\n\n\ub313\uae00\uc758 \uae38\uc774\ub294 \ub9ce\uc774 \ub2e4\ub974\ub2e4.","21739c95":"<!--\nHere's the basic naive bayes feature equation:\n-->\n\n\uae30\ubcf8\uc801\uc778 naive bayes feature \uc2dd:","bd6d2c64":"<!--\nFit a model for one dependent at a time:\n-->\n\n\ud55c \ubc88\uc5d0 \ud558\ub098\uc758 \uc758\uc874 \ubaa8\ub378\uc744 \uc801\uc6a9\ud55c\ub2e4.","56c069fb":"<!--\nWe'll create a list of all the labels to predict, and we'll also create a 'none' label so we can see how many comments have no labels. We can then summarize the dataset.\n-->\n\n\uc608\uce21\uc744 \uc704\ud55c \ubaa8\ub4e0 label \ubaa9\ub85d\uc744 \ub9cc\ub4e4 \uac83\uc774\ub2e4. \uadf8\ub9ac\uace0 'none' \ub77c\ubca8\ub3c4 \ub9cc\ub4e4 \uac83\uc774\ub2e4. \uadf8\ub798\uc11c \uc5bc\ub9c8\ub098 \ub9ce\uc740 \ub313\uae00\ub4e4\uc5d0 label\uc774 \uc5c6\ub294 \uc9c0 \ud655\uc778\ud560 \uc218 \uc788\ub2e4. \uadf8\ub7ec\uba74 \ub370\uc774\ud130\uc14b\uc744 \uc694\uc57d\ud560 \uc218\ub3c4 \uc788\ub2e4.","b7b2e5e3":"<!--\nAnd finally, create the submission file.\n-->\n\n\uadf8\ub9ac\uace0 \ub9c8\uc9c0\ub9c9\uc73c\ub85c, submission file\uc744 \ub9cc\ub4e0\ub2e4.","d50cf94e":"<!--\nThis creates a *sparse matrix* with only a small number of non-zero elements (*stored elements* in the representation  below).\n-->\n\n\uc774\uac83\uc740 0\uc774 \uc544\ub2cc \uc6d0\uc18c\uac00 \ub9e4\uc6b0 \uc801\uc740 *sparse matrix* \ub97c \ub9cc\ub4e4\uc5b4\ub0b8\ub2e4. (\uc544\ub798 \ud45c\ud604\uc5d0\uc11c *stored elements*)","54bdebe3":"## Looking at the data\n\n<!--\nThe training data contains a row per comment, with an id, the text of the comment, and 6 different labels that we'll try to predict.\n-->\n\n\ud6c8\ub828 \ub370\uc774\ud130\ub294 \ub313\uae00 \ub2f9 id\uc640 \ub313\uae00\uc758 \ud14d\uc2a4\ud2b8\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ud558\ub098\uc758 \ud589\uc73c\ub85c \ub418\uc5b4\uc788\ub2e4. \uadf8\ub9ac\uace0 \uc6b0\ub9ac\uac00 \uc608\uce21\ud574\uc57c\ud560 6\uac1c\uc758 \ub2e4\ub978 label\uc774 \uc788\ub2e4.","41aed1c2":"<!--\nHere's a couple of examples of comments, one toxic, and one with no labels.\n-->\n\ntoxic\uc778 \uac83\ub4e4\uacfc \uc544\ub2cc \uac83\ub4e4\uc758 \uba87 \uac00\uc9c0\uc758 \ub313\uae00 \uc608\uc2dc\uac00 \uc788\ub2e4.","97753118":"## Introduction\n\n<!--\nThis kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline for the [Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge) competition. NBSVM was introduced by Sida Wang and Chris Manning in the paper [Baselines and Bigrams: Simple, Good Sentiment and Topic Classi\ufb01cation](https:\/\/nlp.stanford.edu\/pubs\/sidaw12_simple_sentiment.pdf). In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes).\n\nIf you're not familiar with naive bayes and bag of words matrices, I've made a preview available of one of fast.ai's upcoming *Practical Machine Learning* course videos, which introduces this topic. Here is a link to the section of the video which discusses this: [Naive Bayes video](https:\/\/youtu.be\/37sFIak42Sc?t=3745).\n-->\n\n\uc774 \ub178\ud2b8\ubd81\uc740 [Toxic Comment Classification Challenge](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge) \ub300\ud68c\uc5d0\uc11c \uc88b\uc740 \ubca0\uc774\uc2a4 \ub77c\uc778\uc744 \ub9cc\ub4e4\uae30 \uc704\ud574 NBSVM(Naive Bayes - Support Vecotor Machine)\ub97c \uc0ac\uc6a9\ud558\ub294 \ubc95\uc744 \ubcf4\uc5ec\uc900\ub2e4. NBSVM\uc740 Sida Wang\uacfc Chris Manning\uc774 [Baselines and Bigrams: Simple, Good Sentiment and Topic Classi\ufb01cation](https:\/\/nlp.stanford.edu\/pubs\/sidaw12_simple_sentiment.pdf) \ub17c\ubb38\uc5d0\uc11c \ub3c4\uc785\ud588\ub2e4. \uc774 \ub178\ud2b8\ubd81\uc5d0\uc11c, sklearn\uc758 SVM \ub300\uc2e0\uc5d0 logistic regression \uc0ac\uc6a9\ud55c\ub2e4. \uc2e4\uc81c\ub85c, \uc774 \ub458\uc740 \uac70\uc758 \ub3d9\uc77c\ud558\uc9c0\ub9cc (sklearn\uc740 liblinear \ub77c\uc774\ube0c\ub7ec\ub9ac\ub97c \uc0ac\uc6a9\ud55c\ub2e4.)\n\n\ub9cc\uc57d \uc5ec\ub7ec\ubd84\uc774 naive bayes\uc640 bag of words matrices\uc5d0 \uc775\uc219\ud558\uc9c0 \uc54a\ub294\ub2e4\uba74, fast.ai\uc758 \uace7 \uc62c\ub824\uc9c8 *Practical Machine Learning* \ucf54\uc2a4 \ube44\ub514\uc624\uc5d0 \uc911 \ud558\ub098\ub97c \ubbf8\ub9ac\ubcf4\uae30\ub85c \ub9cc\ub4e4\uc5c8\ub2e4. \uac70\uae30\uc11c \uc774 \uc8fc\uc81c\uc5d0 \ub300\ud55c \uc124\uba85\ud55c\ub2e4. \uc774\uac83\uc740 \ube44\ub514\uc624\uc5d0\uc11c \uc598\uae30\ud560 \uc139\uc158\uc758 \ub9c1\ud06c\uc774\ub2e4. [Naive Bayes video](https:\/\/youtu.be\/37sFIak42Sc?t=3745).","7960884f":"<!--\nIt turns out that using TF-IDF gives even better priors than the binarized features used in the paper. I don't think this has been mentioned in any paper before, but it improves leaderboard score from 0.59 to 0.55.\n-->\n\nTF-IDF\ub97c \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ub17c\ubb38\uc5d0\uc11c \uc0ac\uc6a9\ub41c binarized features \ubcf4\ub2e4 \ud6e8\uc52c \ub354 \uc88b\ub2e4\uace0 \uc54c\ub824\uc84c\ub2e4. \uc774\uc804\uc5d0\ub294 \uc5b4\ub5a4 \ub17c\ubb38\uc5d0\uc11c\ub3c4 \uc774\uac83\uc774 \uc5b8\uae09\ub418\uc5c8\ub2e4\uace0 \uc0dd\uac01\ud558\uc9c0 \uc54a\ub294\ub2e4. \uadf8\ub7ec\ub098 \uc774\uac83\uc774 leaderboard \uc810\uc218\ub97c 0.59\uc5d0\uc11c 0.55\ub85c \uc0c1\uc2b9\uc2dc\ucf30\ub2e4.","69ecda74":"<!--\nThere are a few empty comments that we need to get rid of, otherwise sklearn will complain.\n-->\n\n\uc81c\uac70\ud574\uc57c\ud560 \uba87 \uac1c\uc758 \ube44\uc5b4\uc788\ub294 \ub313\uae00\uc774 \uc788\ub2e4. \uc81c\uac70\ub97c \uc548 \ud55c\ub2e4\uba74 sklearn\uc774 \uc791\ub3d9\ud558\uc9c0 \uc54a\uc744 \uac83\uc774\ub2e4."}}