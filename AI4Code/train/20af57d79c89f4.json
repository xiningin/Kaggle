{"cell_type":{"174400d5":"code","7328d49c":"code","e8d3d40e":"code","a6a990c1":"code","c653ebd5":"code","2e6f1de7":"code","314e11e8":"code","0a7b3f35":"code","a84e55a3":"code","a080175e":"code","8b41b1b7":"code","1c75bc2d":"markdown","03dee247":"markdown","e8a676c2":"markdown"},"source":{"174400d5":"import json\nimport pandas as pd","7328d49c":"!gsutil cp -r gs:\/\/bert-nq\/tiny-dev .\n!gunzip tiny-dev\/*\n!ls tiny-dev -hl","e8d3d40e":"predictions = pd.read_csv('..\/input\/tinydev\/ken_predictions.csv', na_filter=False).set_index('example_id')","a6a990c1":"def long_annotations(example):\n    longs = [('%s:%s' % (l['start_token'],l['end_token']))\n                for l in [a['long_answer'] for a in example['annotations']]\n                if not l['candidate_index'] == -1\n            ]\n    return longs #list of long annotations","c653ebd5":"def short_annotations(example):\n    shorts = [('%s:%s' % (s['start_token'],s['end_token']))\n              for s in \n              # sum(list_of_lists, []) is not very efficient gives an easy flat map for short lists\n              sum([a['short_answers'] for a in example['annotations']], [])\n             ]\n    return shorts #list of short annotations","2e6f1de7":"def yes_nos(example):\n    return [\n        yesno for yesno in [a['yes_no_answer'] for a in example['annotations']]\n        if not yesno == 'NONE'\n    ]","314e11e8":"# This is the critical method where I guess at the competition metric.\nclass Score():\n    def __init__(self):\n        self.TP = 0\n        self.FP = 0\n        self.FN = 0\n        self.TN = 0\n    def F1(self):\n        return 2 * self.TP \/ (2 * self.TP + self.FP + self.FN)\n    def increment(self, prediction, annotations, yes_nos):\n        if prediction in yes_nos:\n            print(prediction, yes_nos)\n            self.TP += 1\n        elif len(prediction) > 0:\n            if prediction in annotations:\n                self.TP += 1\n            else:\n                self.FP += 1\n        elif len(annotations) == 0:\n            self.TN += 1\n        else:\n            self.FN +=1\n    def scores(self):\n        return 'TP = {}   FP = {}   FN = {}   TN = {}   F1 = {:.2f}'.format(\n            self.TP, self.FP, self.FN, self.TN, self.F1())","0a7b3f35":"long_score = Score()\nshort_score = Score()\ntotal_score = Score()\nfor example in map(json.loads, open('tiny-dev\/nq-dev-sample.jsonl', 'r')):\n    long_pred = predictions.loc[str(example['example_id']) + '_long', 'PredictionString']\n    long_score.increment(long_pred, long_annotations(example), [])\n    total_score.increment(long_pred, long_annotations(example), [])\n    short_pred = predictions.loc[str(example['example_id']) + '_short', 'PredictionString']\n    short_score.increment(short_pred, short_annotations(example), yes_nos(example))\n    total_score.increment(short_pred, short_annotations(example), [])","a84e55a3":"print(short_score.scores())","a080175e":"print(long_score.scores())","8b41b1b7":"print(total_score.scores() + ' (LB score)')","1c75bc2d":"Please excuse the naming confusion, but theres also a `tinydev` dataset which is a submission csv based on my model's predictions from `nq-dev-sample.jsonl` downloaded above. To evaluate your own model, just make a private dataset with your own csv predictions from this json.","03dee247":"## Evaluation Metric\n\nThere has been some discussion in a number of thread forums. This is not a definitive kernel as I am just as unclear as many participants. However, it does seem to achieve a similar score on the `tiny-dev` set to what the same model achieves on the public test set on the leaderboard. So it could be something close to the metric being used for the LB.","e8a676c2":"The `tiny-dev` set fron NQ is quite useful and can be downloaded from google storage as follows:"}}