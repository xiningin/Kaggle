{"cell_type":{"84d54b04":"code","4f974fdb":"code","950a43f0":"code","1ab16913":"code","8912df13":"code","195e2fd3":"code","828cad77":"code","f14aae45":"code","fbf2dd84":"code","d937dc0b":"code","6e3300a0":"code","f1b8ab59":"code","c34804fa":"code","85f60c6d":"code","129d556d":"code","81e813b5":"code","327219a4":"code","687c1b2f":"code","4f758fe8":"code","e45ba333":"code","c7782aa6":"code","593c6989":"code","d0dd881e":"markdown","a724398b":"markdown","e344ae1a":"markdown","2d29122d":"markdown","a4135cec":"markdown","4fa04431":"markdown","9f40e5b8":"markdown","c3c14c9d":"markdown","ee19d2f8":"markdown","d6dfdccf":"markdown","cbacab35":"markdown"},"source":{"84d54b04":"!pip install -q vit-pytorch\n!pip install -q nystrom-attention","4f974fdb":"# Python library used for working with arrays.\nimport numpy as np\n\n# Python library to interact with the file system.\nimport os\n\n# Software library written for data manipulation and analysis. \nimport pandas as pd\n\n# fastai library for computer vision tasks\nfrom fastai.vision.all import *\nfrom fastai.metrics import *\n\n# Developing and training neural network based deep learning models.\nimport torch\n\n# Vision Transformer\nfrom vit_pytorch.efficient import ViT\n\n# Nystromformer\nfrom nystrom_attention import Nystromformer","950a43f0":"dataset_path = Path('..\/input\/ranzcr-clip-catheter-line-classification')\nos.listdir(dataset_path)","1ab16913":"train_df = pd.read_csv(dataset_path\/'train.csv')","8912df13":"train_df.head()","195e2fd3":"train_df['path'] = train_df['StudyInstanceUID'].map(lambda x:str(dataset_path\/'train'\/x)+'.jpg')\ntrain_df = train_df.drop(columns=['StudyInstanceUID'])\ntrain_df.head(10)","828cad77":"# Transforms we need to do for each image in the dataset (ex: resizing).\nitem_tfms = RandomResizedCrop(224, min_scale=0.75, ratio=(1.,1.)) \n\n# Transforms that can take place on a batch of images (ex: many augmentations).\nbatch_tfms = [*aug_transforms(size=224, max_warp=0), Normalize.from_stats(*imagenet_stats)]","f14aae45":"label_names = list(train_df.columns[:11])","fbf2dd84":"data = DataBlock(blocks=(ImageBlock, MultiCategoryBlock(encoded=True, vocab=label_names)), # multi-label target\n                 splitter = RandomSplitter(seed=42),# split data into training and validation subsets.\n                 get_x = ColReader(12),# obtain the input images.\n                 get_y = ColReader(list(range(11))), # obtain the targets.\n                 item_tfms = item_tfms,\n                 batch_tfms = batch_tfms)","d937dc0b":"dls = data.dataloaders(train_df,bs=16)\n\n# We can call show_batch() to see what a sample of a batch looks like.\ndls.show_batch()","6e3300a0":"efficient_transformer = Nystromformer(\n    # Last dimension of output tensor after linear transformation nn.Linear(..., dim).\n    dim = 128,\n    # Number of Transformer blocks.\n    depth = 6,\n    # Number of heads in Multi-head Attention layer.\n    heads = 8,\n    # # number of landmarks\n    num_landmarks = 256\n)","f1b8ab59":"model = ViT(\n    # Last dimension of output tensor after linear transformation nn.Linear(..., dim).\n    dim = 128,\n    # #If you have rectangular images, make sure your image size is the maximum of the width and height\n    image_size = 224,\n    # n = (image_size \/\/ patch_size) ** 2 and n must be greater than 16.\n    patch_size = 16,\n    # Number of classes to classify.\n    num_classes = 11,\n    # plugin your own sparse attention transformer (Linformer\/Reformer\/Nystromformer)\n    transformer = efficient_transformer\n)","c34804fa":"# Group together some dls, a model, and metrics to handle training\nlearn = Learner(dls, model, metrics = [accuracy_multi]) # Compute accuracy when input and target are the same size.","85f60c6d":"# Choosing a good learning rate\nlearn.lr_find()","129d556d":"# We can use the fine_tune function to train a model with this given learning rate\nlearn.fine_tune(1,base_lr=0.0002290867705596611)","81e813b5":"sample_df = pd.read_csv(dataset_path\/'sample_submission.csv')\nsample_df.head()","327219a4":"_sample_df = sample_df.copy()\n_sample_df['PatientID'] = 'None'\n_sample_df['path'] = _sample_df['StudyInstanceUID'].map(lambda x:str(dataset_path\/'test'\/x)+'.jpg')\n_sample_df = _sample_df.drop(columns=['StudyInstanceUID'])\ntest_dl = dls.test_dl(_sample_df)\n","687c1b2f":"test_dl.show_batch()","4f758fe8":"# Return predictions on the ds_idx dataset or dl using Test Time Augmentation\npreds, _ = learn.tta(dl=test_dl,n=8)","e45ba333":"submission_df = sample_df\nfor i in range(len(submission_df)):\n    for j in range(len(label_names)):\n        submission_df.iloc[i, j+1] = preds[i][j].numpy().astype(np.float32)","c7782aa6":"submission_df.head(10)","593c6989":"submission_df.to_csv(f'submission.csv', index=False)","d0dd881e":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Test Time Augmentation<\/p>","a724398b":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Nystromformer<\/p>","e344ae1a":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Test Time Augmentation<\/p>\n\n![](https:\/\/preview.ibb.co\/kH61v0\/pipeline.png)\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Similar to what Data Augmentation is doing to the training set, the purpose of Test Time Augmentation is to perform random modifications to the test images. Thus, instead of showing the regular, \u201cclean\u201d images, only once to the trained model, we will show it the augmented images several times. We will then average the predictions of each corresponding image and take that as our final guess.<br><br>The reason why it works is that, by averaging our predictions, on randomly modified images, we are also averaging the errors. The error can be big in a single vector, leading to a wrong answer, but when averaged, only the correct answer stand out.<\/p>","2d29122d":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Install and Import Libraries<\/p>","a4135cec":"<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spacing: 1px; background-color: #f6f5f5; color :#6666ff; border-radius: 200px 200px; text-align:center\">Vision Transformers<\/h1>\n\n![](https:\/\/neurohive.io\/wp-content\/uploads\/2020\/10\/archhhh2-770x388.png)\n\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">We split an image into fixed-size patches, linearly embed each of them, add position embeddings, and feed the resulting sequence of vectors to a standard Transformer encoder. In order to perform classification, we use the standard approach of adding an extra learnable \u201cclassification token\u201d to the sequence.<br><br>A major challenge of applying Transformers without CNN to images is applying Self-Attention between pixels. ViT has overcome this problem by segmenting images into small patches (like 16x16 as implemented in this notebook).<br><br>Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.<\/p>","4fa04431":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Load Training Data<\/p>","9f40e5b8":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Image Augmentation on Dataset<\/p>","c3c14c9d":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Training<\/p>","ee19d2f8":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Load Test File<\/p>","d6dfdccf":"![](https:\/\/raw.githubusercontent.com\/lucidrains\/nystrom-attention\/master\/diagram.png)\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">The proposed architecture of efficient self-attention via Nystrom approximation.Each box represents an input, output, or intermediate matrix.The variable name and the size of the matrix are inside box. \u00d7 denotes matrix multiplication, and + denotes matrix addition.<br><br>The orange colored boxes are those matrices used in the Nystrom approximation. The green boxes are the skip connection added in parrallel to the approximation.The dashed bounding box illustrates the three matrices of Nystroom approximate softmax matrix in self-attention.","cbacab35":"<p p style = \"font-family: garamond; font-size:40px; font-style: normal;background-color: #f6f5f5; color :#ff0066; border-radius: 10px 10px; text-align:center\">Upvote the kernel if you find it insightful!<\/p>"}}