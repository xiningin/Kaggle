{"cell_type":{"bd5cb9b6":"code","250164ce":"code","fcc7132d":"code","7fd168f9":"code","7d9e0f93":"code","b31b04f0":"code","df197e04":"code","640052f6":"code","56e1758f":"code","0db6b1db":"code","943028e1":"code","b6583b0e":"code","56bdc1ca":"code","c812f7eb":"code","26d7a271":"code","decfe609":"code","0f7da71b":"markdown"},"source":{"bd5cb9b6":"!pip install -U kaggle_environments cpprb","250164ce":"import gc\nfrom multiprocessing import set_start_method, cpu_count, Process, Event, SimpleQueue\n\nimport time\n\nimport numpy as np\nimport tensorflow as tf\nimport cpprb # Replay Buffer Library: https:\/\/ymd_h.gitlab.io\/cpprb\/\nfrom tqdm.notebook import tqdm\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\nfrom kaggle_environments import make\n\n# %load_ext tensorboard\n# %tensorboard --logdir logs","fcc7132d":"# Global config\n\n\nGOOSE = -1.0\nRISK = GOOSE\/2 # Half of GOOSE (= -0.5)\nNONE = 0.0\nFOOD = 1.0\n\nact_shape = 4\n\nWIDTH = 11\nHEIGHT = 7\n\nxc = WIDTH\/\/2 + 1\nyc = HEIGHT\/\/2 + 1\n\nEAST_idx  = (xc+1,yc  )\nNORTH_idx = (xc  ,yc-1)\nWEST_idx  = (xc-1,yc  )\nSOUTH_idx = (xc  ,yc+1)\n\nAROUND = ([xc+1,xc  ,xc-1,xc  ],\n          [yc  ,yc-1,yc  ,yc+1])\n\ncode2dir = {0:'EAST', 1:'NORTH', 2:'WEST', 3:'SOUTH'}\n\ndir2code = {\"EAST\":0, \"NORTH\":1, \"WEST\":2, \"SOUTH\":3}\n\npossible = {\"EAST\" : [\"EAST\",\"NORTH\",       \"SOUTH\"],\n            \"NORTH\": [\"EAST\",\"NORTH\",\"WEST\"        ],\n            \"WEST\" : [       \"NORTH\",\"WEST\",\"SOUTH\"],\n            \"SOUTH\": [\"EAST\",        \"WEST\",\"SOUTH\"]}","7fd168f9":"def create_model():\n    model = tf.keras.Sequential([tf.keras.layers.Dense(100,activation=\"relu\",input_shape=(WIDTH*HEIGHT,)),\n                                 tf.keras.layers.Dense(100,activation=\"relu\"),\n                                 tf.keras.layers.Dense(100,activation=\"relu\"),\n                                 tf.keras.layers.Dense(act_shape)])\n    return model","7d9e0f93":"def Q_func(model,obs,act):\n    act_shape = tf.constant(4)\n    return tf.reduce_sum(model(obs) * tf.one_hot(act,depth=act_shape), axis=1)\n\ndef Q1_func(model,next_obs,rew,done):\n    gamma = tf.constant(0.99)\n    return gamma*tf.reduce_max(model(next_obs),axis=1)*(1.0-done) + rew\n\n@tf.function\ndef train_then_absTD(model,target,obs,act,rew,next_obs,done,weights):\n    with tf.GradientTape() as tape:\n        tape.watch(model.trainable_weights)\n        Q = Q_func(model,obs,act)\n        yQ1_r = Q1_func(target,next_obs,rew,done)\n        absTD = tf.abs(Q - yQ1_r)\n        loss = tf.reduce_mean(tf.where(absTD <= 1.0, 0.5*tf.square(absTD), absTD))\n\n    grad = tape.gradient(loss,model.trainable_weights)\n    opt.apply_gradients(zip(grad,model.trainable_weights))\n\n    Qnew = Q_func(model,obs,act)\n    return tf.math.maximum(tf.abs(Qnew - yQ1_r), tf.constant(1.0))\n\n#@tf.function # Problem with multiprocessing\ndef abs_TD(model,target,obs,act,rew,next_obs,done):\n    Q = Q_func(model,obs,act)\n    yQ1_r = Q1_func(target,next_obs,rew,done)\n    return tf.math.maximum(tf.abs(Q - yQ1_r), tf.constant(1.0))","b31b04f0":"def pos(index):\n    return index%WIDTH, index\/\/WIDTH\n\ndef centering(z,dz,Z):\n    z += dz\n    if z < 0:\n        z += Z\n    elif z >= Z:\n        z -= Z\n    return z\n    \n\ndef encode_board(obs,act=\"NORTH\",idx=0):\n    \"\"\"\n    Player goose is always set at the center\n    \"\"\"\n    board = np.zeros((WIDTH,HEIGHT))\n\n    if len(obs[\"geese\"][idx]) == 0:\n        return board\n        \n    x0, y0 = pos(obs[\"geese\"][idx][0])\n    dx = xc - x0\n    dy = yc - y0\n    \n    for i, goose in enumerate(obs[\"geese\"]):\n        if len(goose) == 0:\n            continue\n\n        for g in goose[:-1]:\n            x, y = pos(g)\n            x = centering(x,dx,WIDTH)\n            y = centering(y,dy,HEIGHT)       \n            board[x,y] = GOOSE\n        \n        # Enemy Tail as Risk (self tail is not risk)\n        if i != idx:\n            x, y = pos(goose[-1])\n            x = centering(x,dx,WIDTH)\n            y = centering(y,dy,HEIGHT)\n            board[x,y] = RISK\n\n    for food in obs[\"food\"]:\n        x, y = pos(food)\n        x = centering(x,dx,WIDTH)\n        y = centering(y,dy,HEIGHT)\n        board[x,y] = FOOD\n\n    # Set RISK for around enemy geese head\n    for i, goose in enumerate(obs[\"geese\"]):\n        if (i == idx) or (len(goose) == 0):\n            continue\n        x, y = pos(goose[0])\n        x = centering(x,dx,WIDTH)\n        y = centering(y,dy,HEIGHT)\n        \n        yr = y+1 if y < HEIGHT-1 else 0\n        if board[x,yr] != GOOSE:\n            board[x,yr] = RISK\n            \n        yr = y-1 if y > 0 else HEIGHT-1\n        if board[x,yr] != GOOSE:\n            board[x,yr] = RISK\n        \n        xr = x+1 if x < WIDTH-1 else 0\n        if board[xr,y] != GOOSE:\n            board[xr,y] = RISK\n        \n        xr = x-1 if x > 0 else WIDTH-1\n        if board[xr,y] != GOOSE:\n            board[xr,y] = RISK\n        \n    board[xc,yc] = len(obs[\"geese\"][idx]) # self length\n\n    # Avoid Body Hit add psudo GOOSE\n    if act == \"EAST\":\n        board[WEST_idx] = GOOSE\n    elif act == \"NORTH\":\n        board[SOUTH_idx] = GOOSE\n    elif act == \"WEST\":\n        board[EAST_idx] = GOOSE\n    elif act == \"SOUTH\":\n        board[NORTH_idx] = GOOSE\n    else:\n        raise\n    \n    return board","df197e04":"def get_obs_action(model,states,idx=0, train=False):\n    act = states[idx][\"action\"]\n\n    if states[idx][\"status\"] != \"ACTIVE\":\n        return None, act\n    \n    board = encode_board(states[0][\"observation\"],act=act,idx=idx)\n\n    OK = (board[AROUND] != GOOSE)                \n    # e-greedy\n    if train and (np.random.random() < 0.1):\n        ok = [i for i,o in enumerate(OK) if o]\n        if len(ok) > 0:\n            new_act = code2dir[np.random.choice(ok)]\n        else:\n            new_act = act\n        return board, new_act\n\n\n    Q = tf.squeeze(model(board.reshape(1,-1))).numpy()\n    SAFE = (board[AROUND] >= 0)\n\n    safe_q = Q[SAFE]\n    if safe_q.shape[0] > 0:\n        new_act = [i for i,s in enumerate(SAFE) if s][safe_q.argmax()]\n    else:\n        ok_q = Q[OK]\n        if ok_q.shape[0] > 0:\n            new_act = [i for i,o in enumerate(OK) if o][ok_q.argmax()]\n        else:\n            new_act = dir2code[act]\n                \n    return board, code2dir[new_act]","640052f6":"def get_obs_action_greedy(states,idx=0):\n    act = states[idx][\"action\"]\n    \n    if states[idx][\"status\"] != \"ACTIVE\":\n        return None, act\n    \n    board = encode_board(states[0][\"observation\"],act=act,idx=idx)\n    \n    obs = states[0][\"observation\"]\n\n    if len(obs[\"geese\"][idx]) == 0 or len(obs[\"food\"]) == 0:\n        return board, act\n    \n    x0, y0 = pos(obs[\"geese\"][idx][0])\n    \n    min_len = WIDTH + HEIGHT\n    min_i = 0\n    NG = (board[AROUND] == GOOSE)\n    for i, food in enumerate(obs[\"food\"]):\n        x, y = pos(food)\n        \n        dx = x - x0\n        dy = y - y0\n        L = abs(dx) + abs(dy)\n        \n        if dx == 0:\n            if (dy > 0) and NG[dir2code[\"SOUTH\"]]:\n                L += 2\n            elif (dy < 0) and NG[dir2code[\"NORTH\"]]:\n                L += 2\n        if dy == 0:\n            if (dx > 0) and NG[dir2code[\"EAST\"]]:\n                L += 2\n            elif (dx < 0) and NG[dir2code[\"WEST\"]]:\n                L += 2\n            \n        if L < min_len:\n            min_len = L\n            min_i = i\n\n    food = obs[\"food\"][min_i]\n    x, y = pos(food)\n\n    if (x > x0):\n        return board, \"EAST\"\n    \n    if (x < x0):\n        return board, \"WEST\"\n    \n    if (y > y0):\n        return board, \"SOUTH\"\n    \n    if (y < y0):\n        return board, \"NORTH\"\n    \n    return board, act","56e1758f":"def create_buffer(buffer_size,env_dict,alpha,eps=1e-4):\n    return cpprb.MPPrioritizedReplayBuffer(buffer_size,env_dict,alpha=alpha,eps=eps)","0db6b1db":"def explorer(global_rb,env_dict,is_training_done,queue):\n    local_buffer_size = int(1e+2)\n    local_rb = cpprb.ReplayBuffer(local_buffer_size+4,env_dict)\n\n    model = create_model()\n    target = tf.keras.models.clone_model(model)\n    env = make(\"hungry_geese\", debug=False)\n    \n    states = env.reset(4)\n    while not is_training_done.is_set():\n        if not queue.empty():\n            w,wt = queue.get()\n            model.set_weights(w)\n            target.set_weights(wt)\n\n        board_act = [get_obs_action(model,states,i,train=True) for i in range(4)]\n\n        states = env.step([a for b,a in board_act])\n\n        for i, (b, a) in enumerate(board_act):\n            if b is None:\n                continue\n                \n            g_len = len(states[0][\"observation\"][\"geese\"][i])\n            if g_len == 0:\n                rew = -1 # Die\n            elif states[i][\"status\"] != \"ACTIVE\":\n                rew = 1 # Survive\n            elif g_len == 1:\n                rew = -0.5 # Almost Starve\n            else:\n                rew = 0 # Others\n\n            local_rb.add(obs=b.ravel(),\n                         act=dir2code[a],\n                         next_obs=encode_board(states[0][\"observation\"],act=a,idx=i).ravel(),\n                         #rew=states[i][\"reward\"],\n                         rew=rew,\n                         done=(states[i][\"status\"] != \"ACTIVE\"))\n\n        if all(s[\"status\"] != \"ACTIVE\" for s in states):\n            states = env.reset(4)\n            local_rb.on_episode_end()\n\n        if local_rb.get_stored_size() >= local_buffer_size:\n            sample = local_rb.get_all_transitions()\n            global_rb.add(**sample,\n                          priorities=abs_TD(model,target,\n                                            tf.constant(sample[\"obs\"]),\n                                            tf.constant(sample[\"act\"].ravel()),\n                                            tf.constant(sample[\"rew\"].ravel()),\n                                            tf.constant(sample[\"next_obs\"]),\n                                            tf.constant(sample[\"done\"].ravel())))\n            local_rb.clear()","943028e1":"%%time\n\n# Training\nn_warming = 100\nn_train_step = int(1e+5)\nbatch_size = 64\n\nwriter = tf.summary.create_file_writer(\".\/logs\")\n\n# Replay Buffer \nbuffer_size = 1e+6\nenv_dict = {\"obs\": {\"shape\": (WIDTH*HEIGHT)},\n            \"act\": {\"dtype\": int},\n            \"next_obs\": {\"shape\": (WIDTH*HEIGHT)},\n            \"rew\": {},\n            \"done\": {}}\nalpha = 0.5\neps = 0.0\nrb = create_buffer(buffer_size, env_dict,alpha,eps)\n\n# Model\ntarget_update = 50\n\n\nmodel = create_model()\ntarget = tf.keras.models.clone_model(model)\n\nopt = tf.keras.optimizers.Adam()\n\n# Ape-X\nexplorer_update_freq = 100\nn_explorer = cpu_count() - 1\n\n\nis_training_done = Event()\nis_training_done.clear()\n\nqs = [SimpleQueue() for _ in range(n_explorer)]\nps = [Process(target=explorer,\n              args=[rb,env_dict,is_training_done,q])\n      for q in qs]\n\nfor p in ps:\n    p.start()\n\nprint(\"warm-up\")\nwhile rb.get_stored_size() < n_warming:\n    time.sleep(1)\n\n\nprint(\"training\")\n    \nepoch = 0\nfor i in tqdm(range(n_train_step)):        \n    sample = rb.sample(batch_size,beta=0.0)\n    \n    absTD = train_then_absTD(model,target,\n                             tf.constant(sample[\"obs\"]),\n                             tf.constant(sample[\"act\"].ravel()),\n                             tf.constant(sample[\"rew\"].ravel()),\n                             tf.constant(sample[\"next_obs\"]),\n                             tf.constant(sample[\"done\"].ravel()),\n                             tf.constant(sample[\"weights\"].ravel()))\n    rb.update_priorities(sample[\"indexes\"],absTD)\n        \n    if i % target_update == 0:\n        target.set_weights(model.get_weights())\n        \n    if i % explorer_update_freq == 0:\n        w = model.get_weights()\n        wt = target.get_weights()\n        for q in qs:\n            q.put((w,wt))\n\n    \nis_training_done.set()\n\n!mkdir -p sub\nmodel.save(\"sub\/model\")\n\nfor p in ps:\n    p.join()","b6583b0e":"test_env = make(\"hungry_geese\", debug=True)\n\nfor _ in range(4):\n    states = test_env.reset(4)\n    while any(s[\"status\"] == \"ACTIVE\" for s in states):\n        board_act = [get_obs_action(model,states,i) for i in range(4)]\n        states = test_env.step([a for b,a in board_act])\n\n    test_env.render(mode='ipython',width=WIDTH*50,height=HEIGHT*50)","56bdc1ca":"%%writefile sub\/main.py\n\nimport sys\nimport os\n\nsys.path.append(\"\/kaggle_simulations\/agent\")\nworking_dir = \"\/kaggle_simulations\/agent\"\n\nif os.path.exists(\"sub\/model\"):\n    model_f = \"sub\/model\"\nelif os.path.exists(os.path.join(working_dir,\"model\")):\n    model_f = os.path.join(working_dir,\"model\")\nelse:\n    raise ValueError(\"No model file\")\n    \nprint(model_f)\n\n\nimport numpy as np\nimport tensorflow as tf\n\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n\nGOOSE = -1.0\nRISK = GOOSE\/2 # Half of GOOSE (= -0.5)\nNONE = 0.0\nFOOD = 1.0\n\nact_shape = 4\n\nWIDTH = 11\nHEIGHT = 7\n\nxc = WIDTH\/\/2 + 1\nyc = HEIGHT\/\/2 + 1\n\nEAST_idx  = (xc+1,yc  )\nNORTH_idx = (xc  ,yc-1)\nWEST_idx  = (xc-1,yc  )\nSOUTH_idx = (xc  ,yc+1)\n\n\nAROUND = ([xc+1,xc  ,xc-1,xc  ],\n          [yc  ,yc-1,yc  ,yc+1])\n\n\ncode2dir = {0:'EAST', 1:'NORTH', 2:'WEST', 3:'SOUTH'}\ndir2code = {\"EAST\":0, \"NORTH\": 1, \"WEST\":2, \"SOUTH\": 3}\n\n\npolicy = tf.keras.models.load_model(model_f)\nLAST_ACT = \"NORTH\"\n\ndef pos(index):\n    return index%WIDTH, index\/\/WIDTH\n\ndef centering(z,dz,Z):\n    z += dz\n    if z < 0:\n        z += Z\n    elif z >= Z:\n        z -= Z\n    return z\n    \n\ndef encode_board(obs,idx=0):\n    \"\"\"\n    Player goose is always set at the center\n    \"\"\"\n    global LAST_ACT\n    act = LAST_ACT\n\n    board = np.zeros((WIDTH,HEIGHT))\n\n    if len(obs[\"geese\"][idx]) == 0:\n        return board\n        \n    x0, y0 = pos(obs[\"geese\"][idx][0])\n    dx = xc - x0\n    dy = yc - y0\n    \n    for i, goose in enumerate(obs[\"geese\"]):\n        if len(goose) == 0:\n            continue\n\n        for g in goose[:-1]:\n            x, y = pos(g)\n            x = centering(x,dx,WIDTH)\n            y = centering(y,dy,HEIGHT)\n            board[x,y] = GOOSE\n\n        # Enemy Tail as Risk (self tail is not risk)\n        if i != idx:\n            x, y = pos(goose[-1])\n            x = centering(x,dx,WIDTH)\n            y = centering(y,dy,HEIGHT)\n            board[x,y] = RISK\n\n            \n    for food in obs[\"food\"]:\n        x, y = pos(food)\n        x = centering(x,dx,WIDTH)\n        y = centering(y,dy,HEIGHT)\n        board[x,y] = FOOD\n        \n    # Set RISK for around enemy geese head\n    for i, goose in enumerate(obs[\"geese\"]):\n        if (i == idx) or (len(goose) == 0):\n            continue\n        x, y = pos(goose[0])\n        x = centering(x,dx,WIDTH)\n        y = centering(y,dy,HEIGHT)\n        \n        yr = y+1 if y < HEIGHT-1 else 0\n        if board[x,yr] != GOOSE:\n            board[x,yr] = RISK\n            \n        yr = y-1 if y > 0 else HEIGHT-1\n        if board[x,yr] != GOOSE:\n            board[x,yr] = RISK\n        \n        xr = x+1 if x < WIDTH-1 else 0\n        if board[xr,y] != GOOSE:\n            board[xr,y] = RISK\n        \n        xr = x-1 if x > 0 else WIDTH-1\n        if board[xr,y] != GOOSE:\n            board[xr,y] = RISK\n\n    board[xc,yc] = len(obs[\"geese\"][idx]) # self length\n\n    # Avoid Body Hit add psudo GOOSE\n    if act == \"EAST\":\n        board[WEST_idx] = GOOSE\n    elif act == \"NORTH\":\n        board[SOUTH_idx] = GOOSE\n    elif act == \"WEST\":\n        board[EAST_idx] = GOOSE\n    elif act == \"SOUTH\":\n        board[NORTH_idx] = GOOSE\n    else:\n        raise\n\n    return board\n\n\ndef get_action(obs_dict,config_dict):\n    global policy\n    global LAST_ACT\n    \n    idx = Observation(obs_dict).index\n    board = encode_board(obs_dict,idx)\n\n    Q = tf.squeeze(policy(board.reshape(1,-1))).numpy()\n    OK = (board[AROUND] != GOOSE)        \n    SAFE = (board[AROUND] >= 0)\n\n    safe_q = Q[SAFE]\n    if safe_q.shape[0] > 0:\n        new_act = [i for i,s in enumerate(SAFE) if s][safe_q.argmax()]\n    else:\n        ok_q = Q[OK]\n        if ok_q.shape[0] > 0:\n            new_act = [i for i,o in enumerate(OK) if o][ok_q.argmax()]\n        else:\n            new_act = dir2code[LAST_ACT]\n\n    LAST_ACT = code2dir[new_act]\n    return LAST_ACT","c812f7eb":"# Test with self\n\ntest_env.run([\"sub\/main.py\",\"sub\/main.py\",\"sub\/main.py\",\"sub\/main.py\"])\ntest_env.render(mode='ipython',width=WIDTH*50,height=HEIGHT*50)","26d7a271":"# Test with sample agent\n\ntest_env.run([\"sub\/main.py\",\"sub\/main.py\",\"..\/input\/hungry-geese\/agent.py\",\"..\/input\/hungry-geese\/agent.py\"])\ntest_env.render(mode='ipython',width=WIDTH*50,height=HEIGHT*50)","decfe609":"import tarfile\nimport os.path\n\ndef make_tarfile(output_filename, source_dir):\n    with tarfile.open(output_filename, \"w:gz\") as tar:\n        tar.add(source_dir, arcname=os.path.basename(source_dir))\n\nmake_tarfile('submission.tar.gz', '.\/sub\/')","0f7da71b":"* Learn Q function by DQN with Ape-X (3 explorers + 1 learner)\n  * Encode board\n    * Player position is always center\n    * Player length is set at center position\n    * Around enemy heads are marked as RISK\n    * Geese tails are marked as RISK (When the goose eat food, then the tail will stay there)\n  * Each explorer has 4 e-greedy actors (Battle with self-copy)\n  * Los Adjusted Prioritized experience replay (LAP) https:\/\/arxiv.org\/abs\/2007.06049\n    * Huber Loss + p = max(|TD|, 1)\n* Reward\n  * -1 for Die\n  * -0.5 for Length == 1 (to avoid starving)\n  * 1 for Win (Live alone)\n  * 0 for others\n* Rule based Safe Guard to avoid Body Hit and Collision\n  * Goose also tries to avoid to taking RISK\n* Submit multiple files (code + model file) under this instruction https:\/\/www.kaggle.com\/c\/google-football\/discussion\/191257\n"}}