{"cell_type":{"709bf80d":"code","3b20d430":"code","860dba1b":"code","3f08a598":"code","e4477743":"code","f03c0828":"code","f109af2f":"code","217fc980":"code","0c46ff66":"code","0b612fd3":"code","95d12e1d":"code","790fbaf3":"code","55b3d481":"code","aa3a5ffb":"code","53a5ee4b":"code","0299dcec":"code","e32b840f":"code","bf2c4d88":"code","b50c010f":"markdown","f024e7ae":"markdown","7edf14a6":"markdown","526ae473":"markdown","b5e8636a":"markdown","4d5cecb2":"markdown","49b3dc19":"markdown","78e3270a":"markdown","5e83ad17":"markdown","5c9aa174":"markdown","d67748b8":"markdown","cadceb61":"markdown","97235038":"markdown"},"source":{"709bf80d":"#loading libraries\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport math\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\nimport pandas as pd","3b20d430":"#load data using pandas read_csv function\nzomato_data=pd.read_csv('..\/input\/zomato-bangalore-restaurants\/zomato.csv')\nzomato_data = zomato_data.rename(columns={'approx_cost(for two people)':'approx_cost'})\n","860dba1b":"#Plot number of restaurants exist at specific location\n\ndictLocations = {}\n\n#fetch all unique locations in array\nlocations = zomato_data[\"location\"].dropna().unique()\n#loop through each location and set the total number restaurant exist at that location\nfor i in locations:\n    dictLocations[i]=len(np.where(zomato_data[\"location\"]==i)[0])\n#set the plot paramters    \nplt.figure(figsize=(15,20))\nplt.barh(range(len(dictLocations)), list(dictLocations.values()))\nplt.yticks(range(len(dictLocations)), list(dictLocations.keys()))\n\nplt.ylabel('Locations')\nplt.xlabel('Number of Restaurants')\n\nplt.show()","3f08a598":"#initialize dictionery for first pie chart and send length of Yes\/No paramater\ndictOrderOnline={}\ndictOrderOnline['Yes']=len(np.where(zomato_data[\"online_order\"]=='Yes')[0])\ndictOrderOnline['No']=len(np.where(zomato_data[\"online_order\"]=='No')[0])\n\n#initialize dictionery for second pie chart and send length of Yes\/No paramater\ndictBookTable={}\ndictBookTable['Yes']=len(np.where(zomato_data[\"book_table\"]=='Yes')[0])\ndictBookTable['No']=len(np.where(zomato_data[\"book_table\"]=='No')[0])\n\nexplode = (0, 0) \n## plotting the graph\nfig1, (ax1,ax2) = plt.subplots(2,1)\nax1.pie(dictOrderOnline.values(), explode=explode, labels=dictOrderOnline.keys(), autopct='%1.1f%%',\n        shadow=True, startangle=90)\n \nax2.pie(dictBookTable.values(), explode=explode, labels=dictBookTable.keys(), autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\nax1.title.set_text(\"Is Online Ordering Available\")\nax2.title.set_text(\"Option Available to Book Table\")\n\nplt.show()\n","e4477743":"#initilize the dictionery for listed types of restaurants\ndiclistedTypes = {}\nlistedRestTypes=zomato_data[\"listed_in(type)\"].dropna().unique()\nfor i in listedRestTypes:\n    diclistedTypes[i]=len(np.where(zomato_data[\"listed_in(type)\"]==i)[0])\n\n#plotting the graph \nfig1, ax1= plt.subplots(1,1)\nax1.pie(diclistedTypes.values(), labels=diclistedTypes.keys(), autopct='%1.1f%%',\n        shadow=True, startangle=90,radius=4)\nax1.title.set_text(\"Percentage of different types of restaurant\")\nplt.show()\n        ","f03c0828":"dicRestTypes = {}\n\n#fetch all unique restaurant types\nrestType = zomato_data[\"rest_type\"].dropna().unique()\nfor i in restType:\n    indexes = np.where(zomato_data[\"rest_type\"]==i)[0]\n    totalCost=0;\n    #running the nested loop to fetch all cost of specific restaurant types\n    for j in indexes:\n        cost = zomato_data[\"approx_cost\"][j]\n        if(type(cost) is not float):\n            cost = cost.replace(',','')\n        elif(math.isnan(cost)):\n            cost = 0\n        totalCost = totalCost+float(cost)\n    #setting the average of cost in dictionery\n    dicRestTypes[i]=totalCost\/len(np.where(i==zomato_data[\"rest_type\"])[0])\n\n#plotting the graph\nplt.figure(figsize=(10,20))\nplt.barh(range(len(dicRestTypes)), list(dicRestTypes.values()),color=['red'])\nplt.yticks(range(len(dicRestTypes)), list(dicRestTypes.keys()))\n\nplt.ylabel('Restaurant Types')\nplt.xlabel('Average cost for Two people')\nplt.show()","f109af2f":"print(\"Number of records before removing duplicates: \"+str(len(zomato_data)))\n#dropping all duplicates except first. The data is removed on basis of name\nzomato_data.drop_duplicates(subset =\"name\", keep = 'first', inplace = True) \nprint(\"Number of records after removing duplicates: \"+str(len(zomato_data)))\n","217fc980":"import operator\nlocations = zomato_data[\"location\"].dropna().unique()\n#cleaning the rate attribute data\nzomato_data['rate'] = zomato_data['rate'].str.strip()\nzomato_data['rate'] = zomato_data['rate'].str.replace('-','0')\nzomato_data['rate'] = zomato_data['rate'].str.replace('NEW','0')\nzomato_data['rate'] = zomato_data['rate'].str.replace('\/5','')\nzomato_data['rate'] = zomato_data['rate'].fillna(value=0)\nzomato_data['rate'] = zomato_data['rate'].str.strip()\n\n#initialize dictionery of highest rated neighbourhood\ndicHighestRatedNeighbourhood= {}\n#loop through each location and caluclate avergage of rate by each location\nfor i in locations:\n     locationIndexes = np.where(zomato_data[\"location\"]==i)[0]\n     count = 0\n     rate = 0.0\n     for j in locationIndexes:\n         try:\n            if(zomato_data[\"rate\"][j].strip()!='0'):\n                rate = rate + float(zomato_data[\"rate\"][j].strip())\n                count = count + 1\n         except:\n            continue\n     if(count != 0):\n        dicHighestRatedNeighbourhood[i] = rate\/count\n\n#sort dictionery of highest rated neigbourhood in ascending order   \nsortedRates = sorted(dicHighestRatedNeighbourhood.items(), key=operator.itemgetter(1))\n#access the last index for highest rated neighbourhood\nhighestRatedNeighbour = sortedRates[len(sortedRates)-1]\nprint('The highest(average rated) neighbourhood is '+highestRatedNeighbour[0]+' with '+str(highestRatedNeighbour[1])+' rating')\n\n#initialize dictionery of food types and restaurant types with 0\ndictPopularFoodTypes= {k: 0 for k in zomato_data[\"cuisines\"].dropna().unique()} \ndictPopularRestTypes= {k: 0 for k in zomato_data[\"rest_type\"].dropna().unique()} \n\n#set the location indexes of heighest rated neighbourhood and increment the fround food type and rest type by 1\nlocationIndexes = np.where(zomato_data[\"location\"]==highestRatedNeighbour[0])[0]\nfor i in locationIndexes:\n    try:\n        dictPopularFoodTypes[zomato_data[\"cuisines\"][j]]+=1\n    except:\n        count = 0\n    try:\n        dictPopularRestTypes[zomato_data[\"cuisines\"][j]]+=1\n    except:\n        count = 0\n\n# sort the dictioneries    \nsortedpopularFoodTypes = sorted(dictPopularFoodTypes.items(), key=operator.itemgetter(1))\nsortedpopularRestTypes = sorted(dictPopularRestTypes.items(), key=operator.itemgetter(1))\n\n#print the output\nprint('Following are the charatersticks of '+highestRatedNeighbour[0])\nprint('Popular food offered: '+sortedpopularFoodTypes[len(sortedpopularFoodTypes)-1][0])\nprint('Popular Restaurant Types: '+sortedpopularRestTypes[len(sortedpopularRestTypes)-1][0])\n","0c46ff66":"#Preparing data for training\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.model_selection import train_test_split\n\n#cleaning the data\nzomato_data['approx_cost'] = zomato_data['approx_cost'].str.replace(',','')\nzomato_data['approx_cost'] = zomato_data['approx_cost'].fillna(value=0)\nzomato_data['rate'] = zomato_data['rate'].fillna(value=0)\n\n#making a seperate copy for training so graph data won't get affected on compiling again\nzomato_data_copy = zomato_data.copy()\n\n#removing other columns\ntry:\n    zomato_data_copy = zomato_data_copy.drop(['url', 'address', 'name', 'online_order','book_table','votes','phone','dish_liked','reviews_list','menu_item','listed_in(type)','listed_in(city)'], axis=1)\nexcept:\n    print('column already dropped')\n    \n#encode training data\nencoded_data = pd.get_dummies(zomato_data_copy,columns=[\"location\",\"cuisines\",\"rest_type\"],prefix=[\"location\",\"cuisines\",\"rest_type\"])\ntarget=encoded_data[\"approx_cost\"]\ntarget = target.astype(float)\n\n#retrive the features excluding target variable\nfeatures = encoded_data[encoded_data.columns.drop('approx_cost')]\n\n#spliting the data\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n\nX_train = X_train.astype(float)\nX_test = X_test.astype(float)\ny_train = y_train.astype(float)\ny_test = y_test.astype(float)\n","0b612fd3":"#implementing Linear Regression\nfrom sklearn import linear_model\n\nlinear_regression = linear_model.LinearRegression()\nlinear_regression.fit(X_train, y_train)\ny_pred = linear_regression.predict(X_test)\ny_pred_train = linear_regression.predict(X_train)\n\nprint(\"Checking error\/score on training Data\")\nprint(\"Mean Square Error: \"+str(mean_squared_error(y_train, y_pred_train)))\nprint(\"R2 score: \"+str(r2_score(y_train, y_pred_train)))\n\nr2_linear_regression_train = r2_score(y_train, y_pred_train)\nms_linear_regression_train = mean_squared_error(y_train, y_pred_train)\n\nprint(\"\\n\\nChecking error\/score on test Data\")\nprint(\"Mean Square Error: \"+str(mean_squared_error(y_test, y_pred)))\nprint(\"R2 score: \"+str(r2_score(y_test, y_pred)))\n\nr2_linear_regression_test = r2_score(y_test, y_pred)\nms_linear_regression_test = mean_squared_error(y_test, y_pred)\n\n#cross validation of Linear regression\nfrom sklearn.model_selection import cross_val_score\ncross_validation_linear_regression = cross_val_score(linear_regression,X_train,y_train,cv=5)\nprint(\"Output from cross-validation: \" +str(cross_validation_linear_regression.mean()))\n","95d12e1d":"from sklearn.ensemble import RandomForestRegressor\nrandom_forest_model = RandomForestRegressor()\nrandom_forest_model.fit(X_train,y_train)\ny_pred = random_forest_model.predict(X_test)\ny_pred_train = random_forest_model.predict(X_train)\n\nprint(\"Checking error\/score on training Data\")\nprint(\"Mean Square Error: \"+str(mean_squared_error(y_train, y_pred_train)))\nprint(\"R2 score: \"+str(r2_score(y_train, y_pred_train)))\n\nr2_random_forest_train = r2_score(y_train, y_pred_train)\nms_random_forest_train = mean_squared_error(y_train, y_pred_train)\n\nprint(\"\\n\\nChecking error\/score on test Data\")\nprint(\"Mean Square Error: \"+str(mean_squared_error(y_test, y_pred)))\nprint(\"R2 score: \"+str(r2_score(y_test, y_pred)))\n\nr2_random_forest_test = r2_score(y_test, y_pred)\nms_random_forest_test = mean_squared_error(y_test, y_pred)\n\n#cross validation of Random forest\ncross_validation_random_forest = cross_val_score(random_forest_model,X_train,y_train,cv=5)\nprint(\"Output from cross-validation:\")\ncross_validation_random_forest.mean()\n\n","790fbaf3":"#XG Boost Implementation\n\nimport xgboost as xgb\n\nxb = xgb.XGBRegressor(silent=True)\nxb.fit(X_train, y_train)\ny_pred = xb.predict(X_test)\ny_pred_train = xb.predict(X_train)\n\nprint(\"Checking error\/score on training Data\")\nprint(\"Mean Square Error: \"+str(mean_squared_error(y_train, y_pred_train)))\nprint(\"R2 score: \"+str(r2_score(y_train, y_pred_train)))\n\nr2_xg_boost_train = r2_score(y_train, y_pred_train)\nms_xg_boost_train = mean_squared_error(y_train, y_pred_train)\n\nprint(\"\\n\\nChecking error\/score on test Data\")\nprint(\"Mean Square Error: \"+str(mean_squared_error(y_test, y_pred)))\nprint(\"R2 score: \"+str(r2_score(y_test, y_pred)))\n\nr2_xg_boost_test = r2_score(y_test, y_pred)\nms_xg_boost_test = mean_squared_error(y_test, y_pred)\n\ntarget = target.astype(float)\nfeatures = features.astype(float)\n\n#cross validation of XG Boost\ncross_validation_xg_boost = cross_val_score(xb,X_train,y_train,cv=5)\nprint(\"Output from cross-validation:\")\ncross_validation_xg_boost.mean()\n","55b3d481":"#plotting cross validation graphs\ncvs  = (cross_validation_random_forest.mean(),cross_validation_xg_boost.mean())\n\nind = np.arange(len(cvs)) \nwidth = 0.3\nfig, ax = plt.subplots()\nrects1 = ax.bar(ind - width\/2, cvs, width)\n\n\nax.set_ylabel('Cross Validation')\nax.set_title('Cross Validation')\nax.set_xticks(ind)\nax.set_xticklabels(('Random Forest', 'XG Boost'))\n\n\nfig.tight_layout()\n\nplt.show()\n\n\ncvs  = (cross_validation_linear_regression.mean(),0)\nind = np.arange(len(cvs)) \nwidth = 0.3\nfig, ax = plt.subplots()\nrects1 = ax.bar(ind - width\/2, cvs, width)\n\nax.set_ylabel('Cross Validation')\nax.set_title('Cross Validation')\nax.set_xticks(ind)\nax.set_xticklabels(('Linear Regression',''))\nfig.tight_layout()\nplt.show()","aa3a5ffb":"#Model Tuning with GridSearch on Linear Regression\n\n\nfrom sklearn.model_selection import GridSearchCV\nlinear_regression = linear_model.LinearRegression()\n\ngrid_parameters = {'normalize':[True,False],'fit_intercept':[True,False],'copy_X':[True, False]}\nlr_model = GridSearchCV(estimator=linear_regression,param_grid=grid_parameters,cv=5,n_jobs=-1)\nlr_model.fit(X_train,y_train)\n\ny_pred = lr_model.predict(X_test)\ny_pred_train = lr_model.predict(X_train)\n\nprint(\"Checking error\/score on training Data\")\nprint(\"Mean Square Error: \"+str(mean_squared_error(y_train, y_pred_train)))\nprint(\"R2 score: \"+str(r2_score(y_train, y_pred_train)))\n\nr2_linear_regression_train = r2_score(y_train, y_pred_train)\nms_linear_regression_train = mean_squared_error(y_train, y_pred_train)\n\nprint(\"\\n\\nChecking error\/score on test Data\")\nprint(\"Mean Square Error: \"+str(mean_squared_error(y_test, y_pred)))\nprint(\"R2 score: \"+str(r2_score(y_test, y_pred)))\n\nr2_linear_regression_test_after = r2_score(y_test, y_pred)\nms_linear_regression_test_after = mean_squared_error(y_test, y_pred)\n","53a5ee4b":"#Model Tuning with GridSearch on Random Forest\n\nfrom sklearn.model_selection import GridSearchCV\nrandom_forest = RandomForestRegressor()\n\ngrid_parameters = {\"min_samples_split\":[3,6,9],\"max_features\":[\"auto\",\"sqrt\",\"log2\"],\"n_estimators\":[8,16,24]}\nrf_model = GridSearchCV(estimator=random_forest,param_grid=grid_parameters,cv=5,n_jobs=-1)\nrf_model.fit(X_train,y_train)\n\ny_pred = rf_model.predict(X_test)\ny_pred_train = rf_model.predict(X_train)\n\nprint(\"Checking error\/score on training Data\")\nprint(\"Mean Square Error: \"+str(mean_squared_error(y_train, y_pred_train)))\nprint(\"R2 score: \"+str(r2_score(y_train, y_pred_train)))\n\nr2_random_forest_train = r2_score(y_train, y_pred_train)\nms_random_forest_train = mean_squared_error(y_train, y_pred_train)\n\nprint(\"\\n\\nChecking error\/score on test Data\")\nprint(\"Mean Square Error: \"+str(mean_squared_error(y_test, y_pred)))\nprint(\"R2 score: \"+str(r2_score(y_test, y_pred)))\n\nr2_random_forest_test_after = r2_score(y_test, y_pred)\nms_random_forest_test_after = mean_squared_error(y_test, y_pred)\n\n#cross validation of Random forest\ncross_validation_random_forest = cross_val_score(rf_model,X_train,y_train,cv=5)\nprint(\"Output from cross-validation:\")\ncross_validation_random_forest.mean()\n","0299dcec":"#Mode Tuning with GridSearch on XGBoost\nxb = xgb.XGBRegressor(silent=True)\n\ngrid_parameters = {\"min_samples_split\":[3,6,9],\"max_features\":[\"auto\",\"sqrt\",\"log2\"],\"n_estimators\":[8,16,24]}\nxb_model = GridSearchCV(estimator=xb,param_grid=grid_parameters,cv=5,n_jobs=-1)\n\nxb_model.fit(X_train, y_train)\ny_pred = xb_model.predict(X_test)\ny_pred_train = xb_model.predict(X_train)\n\nprint(\"Checking error\/score on training Data\")\nprint(\"Mean Square Error: \"+str(mean_squared_error(y_train, y_pred_train)))\nprint(\"R2 score: \"+str(r2_score(y_train, y_pred_train)))\n\nr2_xg_boost_train = r2_score(y_train, y_pred_train)\nms_xg_boost_train = mean_squared_error(y_train, y_pred_train)\n\nprint(\"\\n\\nChecking error\/score on test Data\")\nprint(\"Mean Square Error: \"+str(mean_squared_error(y_test, y_pred)))\nprint(\"R2 score: \"+str(r2_score(y_test, y_pred)))\n\nr2_xg_boost_test_after = r2_score(y_test, y_pred)\nms_xg_boost_test_after = mean_squared_error(y_test, y_pred)\n\n","e32b840f":"#plotting R2 Score showing changes in R2 Score before tuning and After tuning\n\nbefore_tuning  = (r2_random_forest_test , r2_xg_boost_test)\nafter_tuning = (r2_random_forest_test_after , r2_xg_boost_test_after)\n\nind = np.arange(len(before_tuning)) \n\n\nwidth = 0.3\nfig, ax = plt.subplots()\nrects1 = ax.bar(ind - width\/2, before_tuning, width,label='Before Tuning')\nrects2 = ax.bar(ind + width\/2, after_tuning, width,label='After Tuning')\n\n\nax.set_ylabel('R2 Score')\nax.set_title('Comparison of R2 Score Before and After Parameter Tuning')\nax.set_xticks(ind)\nax.set_xticklabels(('Random Forest', 'XG Boost'))\nax.legend()\n\n\nfig.tight_layout()\n\nplt.show()","bf2c4d88":"#plotting Mean Square Error showing changes in Mean Square Error before tuning and After tuning\n\nbefore_tuning  = (ms_random_forest_test , ms_xg_boost_test)\nafter_tuning = (ms_random_forest_test_after , ms_xg_boost_test_after)\n\nind = np.arange(len(before_tuning)) \n\n\nwidth = 0.3\nfig, ax = plt.subplots()\nrects1 = ax.bar(ind - width\/2, before_tuning, width,label='Before Tuning')\nrects2 = ax.bar(ind + width\/2, after_tuning, width,label='After Tuning')\n\n\nax.set_ylabel('Mean Square Error')\nax.set_title('Comparison of Mean Square Error Before and After Parameter Tuning')\nax.set_xticks(ind)\nax.set_xticklabels(('Random Forest', 'XG Boost'))\nax.legend()\n\n\nfig.tight_layout()\n\nplt.show()","b50c010f":"# References\n\n1. ProvostFoster, & FawcettTom. (n.d.). Data Science for Business: What you need to know about data mining and data-analytic thinking. K\u00f6ln: O`Reilly.\n\n2. Grouped bar chart with labels\u00b6. (n.d.). Retrieved from https:\/\/matplotlib.org\/gallery\/lines_bars_and_markers\/barchart.html#sphx-glr-gallery-lines-bars-and-markers-barchart-py\n\n3. Bardak, B. (n.d.). GridSearchCV Regression vs Linear Regression vs Stats.model OLS. Retrieved from https:\/\/stats.stackexchange.com\/questions\/153131\/gridsearchcv-regression-vs-linear-regression-vs-stats-model-ols\n\n4. Basic pie chart\u00b6. (n.d.). Retrieved from https:\/\/matplotlib.org\/3.1.0\/gallery\/pie_and_polar_charts\/pie_features.html#sphx-glr-gallery-pie-and-polar-charts-pie-features-py\n\n5. How XGBoost Works. (n.d.). Retrieved from https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-HowItWorks.html\n\n6. Lab3_solutions1 [.ipynb]. (n.d.). Https:\/\/dal.brightspace.com.\n\n7. Lab2 [.ipynb]. (n.d.). Https:\/\/dal.brightspace.com.\n\n8. Poddar, H. (2019, March 31). Zomato Bangalore Restaurants. Retrieved from https:\/\/www.kaggle.com\/himanshupoddar\/zomato-bangalore-restaurants","f024e7ae":"### b) Exploring the trends in data\n#### Total number of Restaurants in specific neighbourhood location\nBTM neighbour location has highest number of restaurants, while HSR and Koramangala 5th Block neighbourhood location has most number of restaurant after BTM.","7edf14a6":"### e) Plotting cross validation graph on built models\n\nThe graphs below dipicts that random forest and xgboost have ideal cross validation values, which represents that model is neither underfitted or overfitted. Although there is a cross validation plot for linear regression, but ideally this plot is not used for linear regression. \n\nThe plot of metrics and their description is mentioned in part f)\n","526ae473":"# Building the Model to Forecast Cost of Meal for Two People\n\n## a) Type of Task Solving\nThis type of task is regression because we are predicting numeric value and the target variable is cost (numeric quanitity).\n\n## b) Choosen Models\n\n### i) Linear regression\nThe reason I choosed linear regression because it reduces the mean square of the errors and it puts strong penalty on large errors.\n### ii) Random Forest\nThe random forest result is based on multiple decision trees and each tree is trained separately. The result is based on votes from different decsion tree, therefore it yields better result.\n\n### iii) Xg-boost\nThe XG Boost is based on gradient boosting and it is popular in term of accurate results. The training process  is iterative. This works on adding new trees that notes the error of previous tree and combine the results with previous tree to make more accurrate results.\n\n## c) Which metrics choosed to evaluate the model\n\n### i) R2 Score\nR2 Score determines how close is data to regression line.\n\n### ii) Mean Square Error\nThis metric is the mean of square of predictions.\n\n## d) How to make sure that not to overfitting\n\nTo make sure that model is not overfitting, I have used cross-validation. Cross-validation will allow to estimate generalization performance and also gives statistical value such as mean. Overfitting can also be checked by performing test on training data. The higher accurracies may be due to memorization of data.\n\n\n","b5e8636a":"#### Percentage of restaurants that facilitate with online ordering\nApproximately 58.9% Restaurants facilitates with online ordering and 41.1% does not facilitate online ordering.\n#### Percentage of restaurants that allows to book table\nOnly 12.5% Restaurants facilitates with option to book table, while rest of 87.5% does not prefer to provide this facility.","4d5cecb2":"## 2. Data Preprocessing and Understanding\n### a) Loading the data","49b3dc19":"## f)Tuning the Model using grid search\n\nThe grid search is used to tune paramters as shown below. Grid search works on passing the hyperparameters. The value of hyperparameter is explicitly defined before training the model. This values does not depend on the training data.  Grid search is better in tuning parameters on random forest. Its R2 score is slightly increased while mean-square is slightly decreased. The Grid search is improving results because it finds optimal parameters which increases the accurracy. ","78e3270a":"### Percentage of different types of restaurants\nApproximately 50.2% Restaurants are based on delivery while 34.4% are based on dine-out.","5e83ad17":"### Are there attributes useless at this point\n\nYes, there are some attributes which provides no essential information such as address, URL and phone","5c9aa174":"### Average Cost of two People per restaurant type\nThe graph depicts that Restaurant type fine dining, microbrewery, bar and Lounge has highest average cost for two people.","d67748b8":"## Project Objective:\nExploring zomato dataset and using XG Boost, Random Forest and Linear Regression that forecasts the approximate cost of a meal for two people using the attributes location, rating, restaurant type, and cuisine.\n","cadceb61":"## Neighbour hood with highest average rating and their characteristics\n\nThe highest (average rated) neighbourhood is North Bangalore with 4.2 rating. The  popular food offered by this neighbourhood are North Indian, Chinese, Arabian, Momos, and popular restaurant types are Bar and Lounge.\n","97235038":"## Removing duplicate resturants in data"}}