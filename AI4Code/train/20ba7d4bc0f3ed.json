{"cell_type":{"7273409b":"code","dd335ae1":"code","6c7a485f":"code","43864be8":"code","44b06dc3":"code","ff24c23b":"code","c2a6e284":"code","a60f8497":"code","6e4a4fc8":"code","b09c7ce8":"code","e855349d":"code","8badc63d":"code","5749e9cb":"code","0f81a529":"code","56f6eb73":"code","6c0b7cd6":"code","d602e12e":"code","aa315699":"code","5a3f5ccb":"code","a85f0f47":"code","6bfede52":"code","ad180b90":"code","6c52afc0":"code","274b009b":"code","c6f6a603":"code","3dc92f05":"code","90a05f76":"code","fb271de1":"code","6a83e2a7":"code","2318d899":"code","5cbd7c14":"markdown","7350abdb":"markdown","1756bca2":"markdown","17d32e1b":"markdown","148046c4":"markdown","0fc66174":"markdown","a8c07ddc":"markdown","d385064e":"markdown","44c26b3c":"markdown","01746b30":"markdown","2eb4746f":"markdown","de40fe41":"markdown","bedd4e4b":"markdown","e8b27b39":"markdown","a3770534":"markdown","7d3c4e0d":"markdown","921720c2":"markdown","7b44ce60":"markdown","7e10fe7a":"markdown","6d30f461":"markdown","1e936f3f":"markdown","7bf10d8d":"markdown","1bd27d96":"markdown","720935c6":"markdown","284dbc97":"markdown","484c603d":"markdown","344ae3a3":"markdown","6e587f0c":"markdown","16acdc5c":"markdown","f78f4abc":"markdown","5e56c787":"markdown","b59045a9":"markdown","42eb64cc":"markdown","b1609f0a":"markdown","4cb11e1c":"markdown","ae45ec2e":"markdown","ad375bf4":"markdown","269938e6":"markdown","089a2a45":"markdown","5fe96de4":"markdown","55b09a77":"markdown","d9df47c4":"markdown","d7b5d3f3":"markdown","95491248":"markdown","500f8639":"markdown","950f00fd":"markdown","114e260b":"markdown","ab900bbf":"markdown","1a2ec87a":"markdown","fa09aabb":"markdown","e7ec6181":"markdown","6d9f5b67":"markdown","49907bd8":"markdown","12d9ffb0":"markdown","78cae386":"markdown","020f65e2":"markdown","fa505530":"markdown","fab5aa5d":"markdown","cae5283b":"markdown","2166b78c":"markdown","dcac889a":"markdown","24a4ecc7":"markdown","720616dd":"markdown","2cc6700a":"markdown","98e99c40":"markdown","114c3949":"markdown","cba7e487":"markdown","3a9d0819":"markdown"},"source":{"7273409b":"import itertools\nimport warnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom sklearn.metrics import mean_absolute_error\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nimport statsmodels.tsa.api as smt\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\n\n\nwarnings.filterwarnings('ignore')","dd335ae1":"data = sm.datasets.co2.load_pandas()\ny = data.data\ny.index.sort_values()\n\ny = y['co2'].resample('MS').mean()","6c7a485f":"y.head()","43864be8":"y = y.fillna(y.bfill())\ny.head()\n\ny.plot(figsize=(15, 6))\nplt.show()","44b06dc3":"# Train set.\ntrain = y[:'1997-12-01']\nprint(len(train))  \n\n# Test set.\ntest = y['1998-01-01':]\nprint(len(test))  ","ff24c23b":"def is_stationary(y):\n\n    # \"HO: Non-stationary\"\n    # \"H1: Stationary\"\n\n    p_value = sm.tsa.stattools.adfuller(y)[1]\n    if p_value < 0.05:\n        print(F\"Result: Stationary (H0: non-stationary, p-value: {round(p_value, 3)})\")\n    else:\n        print(F\"Result: Non-Stationary (H0: non-stationary, p-value: {round(p_value, 3)})\")\n\n\nis_stationary(y)","c2a6e284":"def ts_decompose(y, model=\"additive\", stationary=False):\n    result = seasonal_decompose(y, model=model)\n    fig, axes = plt.subplots(4, 1, sharex=True, sharey=False)\n    fig.set_figheight(10)\n    fig.set_figwidth(15)\n\n    axes[0].set_title(\"Decomposition for \" + model + \" model\")\n    axes[0].plot(y, 'k', label='Original ' + model)\n    axes[0].legend(loc='upper left')\n\n    axes[1].plot(result.trend, label='Trend')\n    axes[1].legend(loc='upper left')\n\n    axes[2].plot(result.seasonal, 'g', label='Seasonality & Mean: ' + str(round(result.seasonal.mean(), 4)))\n    axes[2].legend(loc='upper left')\n\n    axes[3].plot(result.resid, 'r', label='Residuals & Mean: ' + str(round(result.resid.mean(), 4)))\n    axes[3].legend(loc='upper left')\n    plt.show(block=True)\n\n    if stationary:\n        is_stationary(y)\n        \nfor model in [\"additive\", \"multiplicative\"]:\n    ts_decompose(y, model, stationary=True)","a60f8497":"ses_model = SimpleExpSmoothing(train).fit(smoothing_level=0.5)\n\ny_pred = ses_model.forecast(48)\n\nmean_absolute_error(test, y_pred)\n\n\ntrain.plot(title=\"Single Exponential Smoothing\")\ntest.plot()\ny_pred.plot()\nplt.show()","6e4a4fc8":"train[\"1985\":].plot(title=\"Single Exponential Smoothing\")\ntest.plot()\ny_pred.plot()\nplt.show()","b09c7ce8":"def plot_co2(train, test, y_pred, title):\n    mae = mean_absolute_error(test, y_pred)\n    train[\"1985\":].plot(legend=True, label=\"TRAIN\", title=f\"{title}, MAE: {round(mae,2)}\")\n    test.plot(legend=True, label=\"TEST\", figsize=(6, 4))\n    y_pred.plot(legend=True, label=\"PREDICTION\")\n    plt.show()\n\n\nplot_co2(train, test, y_pred, \"Single Exponential Smoothing\")","e855349d":"def ses_optimizer(train, alphas, step=48):\n    best_alpha, best_mae = None, float(\"inf\")\n\n    for alpha in alphas:\n        ses_model = SimpleExpSmoothing(train).fit(smoothing_level=alpha)\n        y_pred = ses_model.forecast(step)\n        mae = mean_absolute_error(test, y_pred)\n\n        if mae < best_mae:\n            best_alpha, best_mae = alpha, mae\n\n        print(\"alpha:\", round(alpha, 2), \"mae:\", round(mae, 4))\n    print(\"best_alpha:\", round(best_alpha, 2), \"best_mae:\", round(best_mae, 4))\n    return best_alpha, best_mae\n\nalphas = np.arange(0.8, 1, 0.01)\nses_optimizer(train, alphas)\n\nbest_alpha, best_mae = ses_optimizer(train, alphas)","8badc63d":"ses_model = SimpleExpSmoothing(train).fit(smoothing_level=best_alpha)\ny_pred = ses_model.forecast(48)\n\nplot_co2(train, test, y_pred, \"Single Exponential Smoothing\")\nmean_absolute_error(test, y_pred)","5749e9cb":"des_model = ExponentialSmoothing(train, trend=\"add\").fit(smoothing_level=0.5,\n                                                         smoothing_trend=0.5)\n\ny_pred = des_model.forecast(48)\n\nplot_co2(train, test, y_pred, \"Double Exponential Smoothing\")\n\ndes_model.params","0f81a529":"def des_optimizer(train, alphas, betas, step=48):\n    best_alpha, best_beta, best_mae = None, None, float(\"inf\")\n    for alpha in alphas:\n        for beta in betas:\n            des_model = ExponentialSmoothing(train, trend=\"add\").fit(smoothing_level=alpha,\n                                                                     smoothing_slope=beta)\n            y_pred = des_model.forecast(step)\n            mae = mean_absolute_error(test, y_pred)\n            if mae < best_mae:\n                best_alpha, best_beta, best_mae = alpha, beta, mae\n            print(\"alpha:\", round(alpha, 2), \"beta:\", round(beta, 2), \"mae:\", round(mae, 4))\n    print(\"best_alpha:\", round(best_alpha, 2), \"best_beta:\", round(best_beta, 2), \"best_mae:\", round(best_mae, 4))\n    return best_alpha, best_beta, best_mae\n\n\nalphas = np.arange(0.01, 1, 0.10)\nbetas = np.arange(0.01, 1, 0.10)\n\nbest_alpha, best_beta, best_mae = des_optimizer(train, alphas, betas)","56f6eb73":"final_des_model = ExponentialSmoothing(train, trend=\"add\").fit(smoothing_level=best_alpha,\n                                                               smoothing_slope=best_beta)\n\ny_pred = final_des_model.forecast(48)\n\nplot_co2(train, test, y_pred, \"Double Exponential Smoothing\")","6c0b7cd6":"tes_model = ExponentialSmoothing(train,\n                                 trend=\"add\",\n                                 seasonal=\"add\",\n                                 seasonal_periods=12).fit(smoothing_level=0.5,\n                                                          smoothing_slope=0.5,\n                                                          smoothing_seasonal=0.5)\n\n\ny_pred = tes_model.forecast(48)\n\nplot_co2(train, test, y_pred, \"Triple Exponential Smoothing\")","d602e12e":"def tes_optimizer(train, abg, step=48):\n    best_alpha, best_beta, best_gamma, best_mae = None, None, None, float(\"inf\")\n\n    for comb in abg:\n        tes_model = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=12).\\\n            fit(smoothing_level=comb[0], smoothing_slope=comb[1], smoothing_seasonal=comb[2])\n        y_pred = tes_model.forecast(step)\n        mae = mean_absolute_error(test, y_pred)\n        if mae < best_mae:\n            best_alpha, best_beta, best_gamma, best_mae = comb[0], comb[1], comb[2], mae\n        print([round(comb[0], 2), round(comb[1], 2), round(comb[2], 2), round(mae, 2)])\n\n    print(\"best_alpha:\", round(best_alpha, 2), \"best_beta:\", round(best_beta, 2), \"best_gamma:\", round(best_gamma, 2),\n          \"best_mae:\", round(best_mae, 4))\n\n    return best_alpha, best_beta, best_gamma, best_mae\n\n\nalphas = betas = gammas = np.arange(0.10, 1, 0.20)\nabg = list(itertools.product(alphas, betas, gammas))\n\n\nbest_alpha, best_beta, best_gamma, best_mae = tes_optimizer(train, abg)","aa315699":"final_tes_model = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=12).\\\n            fit(smoothing_level=best_alpha, smoothing_slope=best_beta, smoothing_seasonal=best_gamma)\n\ny_pred = final_tes_model.forecast(48)\n\nplot_co2(train, test, y_pred, \"Triple Exponential Smoothing\")","5a3f5ccb":"from matplotlib import pyplot\nfrom statsmodels.graphics.tsaplots import plot_acf\nplot_acf(y, lags=31)\npyplot.show()","a85f0f47":"arima_model = ARIMA(train, order=(1, 1, 1)).fit(disp=0)\narima_model.summary()\n\ny_pred = arima_model.forecast(48)[0]\ny_pred = pd.Series(y_pred, index=test.index)\n\n\nplot_co2(train, test, y_pred, \"ARIMA\")\n\narima_model.plot_predict(dynamic=False)\nplt.show()","6bfede52":"# producing p and q combinations\np = d = q = range(0, 4)\npdq = list(itertools.product(p, d, q))","ad180b90":"def arima_optimizer_aic(train, orders):\n    best_aic, best_params = float(\"inf\"), None\n    for order in orders:\n        try:\n            arma_model_result = ARIMA(train, order).fit(disp=0)\n            aic = arma_model_result.aic\n            if aic < best_aic:\n                best_aic, best_params = aic, order\n            print('ARIMA%s AIC=%.2f' % (order, aic))\n        except:\n            continue\n    print('Best ARIMA%s AIC=%.2f' % (best_params, best_aic))\n    return best_params\n\n\nbest_params_aic = arima_optimizer_aic(train, pdq)","6c52afc0":"arima_model = ARIMA(train, best_params_aic).fit(disp=0)\ny_pred = arima_model.forecast(48)[0]\ny_pred = pd.Series(y_pred, index=test.index)\n\nplot_co2(train, test, y_pred, \"ARIMA\")","274b009b":"def acf_pacf(y, lags=30):\n    plt.figure(figsize=(12, 7))\n    layout = (2, 2)\n    ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n    acf_ax = plt.subplot2grid(layout, (1, 0))\n    pacf_ax = plt.subplot2grid(layout, (1, 1))\n    y.plot(ax=ts_ax)\n\n    # Stationarity test (HO: Series is not Stationary. H1: Series is Stationary.)\n    p_value = sm.tsa.stattools.adfuller(y)[1]\n    ts_ax.set_title('Time Series Analysis Plots\\n Dickey-Fuller: p={0:.5f}'.format(p_value))\n    smt.graphics.plot_acf(y, lags=lags, ax=acf_ax)\n    smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax)\n    plt.tight_layout()\n    plt.show()\n\nacf_pacf(y)","c6f6a603":"df_diff = y.diff()\ndf_diff.dropna(inplace=True)\n\nacf_pacf(df_diff)","3dc92f05":"model = SARIMAX(train, order=(1, 0, 1), seasonal_order=(0, 0, 0, 12))\nsarima_model = model.fit(disp=0)\n\ny_pred_test = sarima_model.get_forecast(steps=48)\ny_pred = y_pred_test.predicted_mean\ny_pred = pd.Series(y_pred, index=test.index)\n\nplot_co2(train, test, y_pred, \"SARIMA\")","90a05f76":"p = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\ndef sarima_optimizer_aic(train, pdq, seasonal_pdq):\n    best_aic, best_order, best_seasonal_order = float(\"inf\"), float(\"inf\"), None\n    for param in pdq:\n        for param_seasonal in seasonal_pdq:\n            try:\n                sarimax_model = SARIMAX(train, order=param, seasonal_order=param_seasonal)\n                results = sarimax_model.fit(disp=0)\n                aic = results.aic\n                if aic < best_aic:\n                    best_aic, best_order, best_seasonal_order = aic, param, param_seasonal\n                print('SARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, aic))\n            except:\n                continue\n    print('SARIMA{}x{}12 - AIC:{}'.format(best_order, best_seasonal_order, best_aic))\n    return best_order, best_seasonal_order\n\n\nbest_order, best_seasonal_order = sarima_optimizer_aic(train, pdq, seasonal_pdq)","fb271de1":"model = SARIMAX(train, order=best_order, seasonal_order=best_seasonal_order)\nsarima_final_model = model.fit(disp=0)\n\ny_pred_test = sarima_final_model.get_forecast(steps=48)\n\n# MAE\ny_pred = y_pred_test.predicted_mean\ny_pred = pd.Series(y_pred, index=test.index)\n\nplot_co2(train, test, y_pred, \"SARIMA\")","6a83e2a7":"sarima_final_model.plot_diagnostics(figsize=(15, 12))\nplt.show()","2318d899":"p = d = q = range(0, 2)\npdq = list(itertools.product(p, d, q))\nseasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n\ndef sarima_optimizer_mae(train, pdq, seasonal_pdq):\n    best_mae, best_order, best_seasonal_order = float(\"inf\"), float(\"inf\"), None\n    for param in pdq:\n        for param_seasonal in seasonal_pdq:\n            try:\n                model = SARIMAX(train, order=param, seasonal_order=param_seasonal)\n                sarima_model = model.fit(disp=0)\n                y_pred_test = sarima_model.get_forecast(steps=48)\n                y_pred = y_pred_test.predicted_mean\n                mae = mean_absolute_error(test, y_pred)\n\n                # mae = fit_model_sarima(train, val, param, param_seasonal)\n\n                if mae < best_mae:\n                    best_mae, best_order, best_seasonal_order = mae, param, param_seasonal\n                print('SARIMA{}x{}12 - MAE:{}'.format(param, param_seasonal, mae))\n            except:\n                continue\n    print('SARIMA{}x{}12 - MAE:{}'.format(best_order, best_seasonal_order, best_mae))\n    return best_order, best_seasonal_order\n\nbest_order, best_seasonal_order = sarima_optimizer_mae(train, pdq, seasonal_pdq)\n\nmodel = SARIMAX(train, order=best_order, seasonal_order=best_seasonal_order)\nsarima_final_model = model.fit(disp=0)\n\ny_pred_test = sarima_final_model.get_forecast(steps=48)\ny_pred = y_pred_test.predicted_mean\ny_pred = pd.Series(y_pred, index=test.index)\n\n\nplot_co2(train, test, y_pred, \"SARIMA\")","5cbd7c14":"Double Exponential Smoothing is an extension to Exponential Smoothing that explicitly adds support for trends in the univariate time series.\n\nIn addition to the alpha parameter for controlling smoothing factor for the level, an additional smoothing factor is added to control the decay of the influence of the change in trend called beta (b).\n\nThe method supports trends that change in different ways: an additive and a multiplicative, depending on whether the trend is linear or exponential respectively.\n\nDouble Exponential Smoothing with an additive trend is classically referred to as Holt\u2019s linear trend model, named for the developer of the method Charles Holt.\n\nAdditive Trend: Double Exponential Smoothing with a linear trend.\nMultiplicative Trend: Double Exponential Smoothing with an exponential trend.\nFor longer range (multi-step) forecasts, the trend may continue on unrealistically. As such, it can be useful to dampen the trend over time.\n\nDampening means reducing the size of the trend over future time steps down to a straight line (no trend).[3]\n\n\n![image.png](attachment:ecce7906-9bc9-494b-960c-fb9fdaff45e8.png)","7350abdb":"If ACF width \"DECREASE\" relative to the delays and PACF \"CUT\" after the p delay means it's an AR(p) pattern.\n\nIf the ACF width q \"CUT\" after the delay and the PACF width \"DECREASE\" according to the delays, it means it's a MA(q) pattern.\n\nIf the widths of ACF and PACF are decreasing according to the delays, it means it is an ARMA model.","1756bca2":"# 1) AR(p): Autoregression","17d32e1b":"This is a model that is combined from the AR and MA models. In this model, the impact of previous lags along with the residuals is considered for forecasting the future values of the time series. Here \u03b2 represents the coefficients of the AR model and \u03b1 represents the coefficients of the MA model.","148046c4":"**Final DES Model**","0fc66174":"**Multiplicative Method Components:**","a8c07ddc":"![image.png](attachment:d2b45ef1-ce27-4a8c-9cd8-699bcba138af.png)","d385064e":"# Modelling the Time Series","44c26b3c":"![image.png](attachment:c81a6659-43f0-416f-b67d-60a685b51b45.png)","01746b30":"# How to differentiate when to use ACF and PACF?","2eb4746f":"**Hyperparameter Optimization**","de40fe41":"# ARMA Model(p,q) = AR(p) + MA(q)","bedd4e4b":"![image.png](attachment:aac13632-e0f9-4779-a0c9-7f0c07800809.png)","e8b27b39":"**Determining Model Grade Based on ACF & PACF Charts**","a3770534":"**Final SES Model**","7d3c4e0d":"# ACF for General MA(q) Models","921720c2":"**Final Model**","7b44ce60":"Triple Exponential Smoothing is an extension of Exponential Smoothing that explicitly adds support for seasonality to the univariate time series.\n\nThis method is sometimes called Holt-Winters Exponential Smoothing, named for two contributors to the method: Charles Holt and Peter Winters.\n\nIn addition to the alpha and beta smoothing factors, a new parameter is added called gamma (g) that controls the influence on the seasonal component.\n\nAs with the trend, the seasonality may be modeled as either an additive or multiplicative process for a linear or exponential change in the seasonality.\n\nAdditive Seasonality: Triple Exponential Smoothing with a linear seasonality.\n\nMultiplicative Seasonality: Triple Exponential Smoothing with an exponential seasonality.\n\n\n\n\n\nTriple exponential smoothing is the most advanced variation of exponential smoothing and through configuration, it can also develop double and single exponential smoothing models.[3]","7e10fe7a":"# Importing Libraries","6d30f461":"# STATISTICAL METHODS","1e936f3f":"A property of MA(q) models in general is that there are nonzero autocorrelations for the first q lags and autocorrelations = 0 for all lags > q.","7bf10d8d":"![image.png](attachment:1bb30423-d7c9-4029-b723-14c64870908a.png)","1bd27d96":"# 2) MA(q): Moving Average","720935c6":"**Examining the Statistical Outputs of the Model**","284dbc97":"# REFERENCES","484c603d":"AR, MA, ARMA, and ARIMA models are used to forecast the observation at (t+1) based on the historical data of previous time spots recorded for the same observation. However, it is necessary to make sure that the time series is stationary over the historical data of observation overtime period. If the time series is not stationary then we could apply the differencing factor on the records and see if the graph of the time series is a stationary overtime period.","344ae3a3":"# ARIMA Model","6e587f0c":"Auto Correlation function takes into consideration of all the past observations irrespective of its effect on the future or present time period. It calculates the correlation between the t and (t-k) time period. It includes all the lags or intervals between t and (t-k) time periods. Correlation is always calculated using the Pearson Correlation formula.[3]","16acdc5c":"![image.png](attachment:a47ffc76-7ede-4f34-a022-d16d452256fd.png)","f78f4abc":"**ARIMA(p, d, q): (Autoregressive Integrated Moving Average)**","5e56c787":"# SARIMA(p, d, q): (Seasonal Autoregressive Integrated Moving-Average)","b59045a9":"# 4) Triple Exponential Smoothing Holt-Witers","42eb64cc":"The time period at t is impacted by the unexpected external factors at various slots t-1, t-2, t-3, \u2026.., t-k. These unexpected impacts are known as Errors or Residuals. The impact of previous time spots is decided by the coefficient factor \u03b1 at that particular period of time. The price of a share of any particular company X may depend on some company merger that happened overnight or maybe the company resulted in shutdown due to bankruptcy. This kind of model calculates the residuals or errors of past time series and calculates the present or future values in the series in know as Moving Average (MA) model.\n\nConsider an example of Cake distribution during my birthday. Let's assume that your mom asks you to bring pastries to the party. Every year you miss judging the no of invites to the party and end upbringing more or less no of cakes as per requirement. The difference in the actual and expected results in the error. So you want to avoid the error for this year hence we apply the moving average model on the time series and calculate the no of pastries needed this year based on past collective errors. Next, calculate the ACF values of all the lags in the time series. If the value of the ACF of any particular month is more than a significant value only those values will be considered for the model analysis.\nFor e.g in the above figure the values 1,2, 3 up to 12 displays the total error(ACF) of count in pastries current month w.r.t the given the lag t by considering all the in-between lags between time t and current month. If we consider two significant values above the threshold then the model will be termed as MA(2). [3]","b1609f0a":"**Dickey-Fuller Test**","4cb11e1c":"**Hyperparameter Optimization**","ae45ec2e":"**Additive Method Components:**","ad375bf4":"# Dataset","269938e6":"**Final TES Model**","089a2a45":"# Autocorrelation (ACF)","5fe96de4":"Period of Record: March 1958 - December 2001\n\nAtmospheric CO2 from Continuous Air Samples at Mauna Loa Observatory, Hawaii, U.S.A.","55b09a77":"**SARIMA(p, d, q): (Seasonal Autoregressive Integrated Moving-Average)**","d9df47c4":"# 3) Double Exponential Smoothing","d7b5d3f3":"There are several ways to model a time seris to make predictions. It will be mentioned to the topics as below:\n\n* Moving Average\n\n* Weighted Average\n\n**1. Exponential Smoothing Methods**\n\n-Single Exponential Smoothing (Contains stationarity)\n\n-Double Exponential Smoothing (Contains Level + Trend)\n\n-Triple Exponential Smoothing Holt-Witers (Contains Level + Tred + Seasonality)\n\n**2. Exponential Statistical Methods**\n\n-AR, MA, ARMA (Contains stationarity)\n\n-ARIMA (Contains Level + Trend)\n\n-SARIMA (Contains Level + Tred + Seasonality)","95491248":"Time series data are data that are kept in chronological order in a discrete manner at equal time intervals. For example, a one-year credit card statement can be considered as time series data for us. Because the extract is a data type that is kept at equal intervals and in chronological order. The fact that the data are ordered according to time brings about the correlation between the neighboring variables.\n","500f8639":"**Hyperparameter Optimization**","950f00fd":"The time period at t is impacted by the observation at various slots t-1, t-2, t-3, \u2026.., t-k. The impact of previous time spots is decided by the coefficient factor at that particular period of time. The price of a share of any particular company X may depend on all the previous share prices in the time series. This kind of model calculates the regression of past time series and calculates the present or future values in the series in know as Auto Regression (AR) model.\n\nConsider an example of a milk distribution company that produces milk every month in the country. We want to calculate the amount of milk to be produced current month considering the milk generated in the last year. We begin by calculating the PACF values of all the 12 lags with respect to the current month. If the value of the PACF of any particular month is more than a significant value only those values will be considered for the model analysis.\n\nFor e.g in the above figure the values 1,2, 3 up to 12 displays the direct effect(PACF) of the milk production in the current month w.r.t the given the lag t. If we consider two significant values above the threshold then the model will be termed as AR(2).","114e260b":"**Dividing into Test and Train Set**","ab900bbf":"[1] https:\/\/statisticsbyjim.com\/time-series\/moving-averages-smoothing\/\n\n[2] https:\/\/machinelearningmastery.com\/exponential-smoothing-for-time-series-forecasting-in-python\/\n\n[3] https:\/\/towardsdatascience.com\/time-series-models-d9266f8ac7b0\n\n[4] https:\/\/otexts.com\/fpp2\/holt-winters.html","1a2ec87a":"Single Exponential Smoothing, SES for short, also called Simple Exponential Smoothing, is a time series forecasting method for univariate data without a trend or seasonality.\n\nIt requires a single parameter, called alpha (a), also called the smoothing factor or smoothing coefficient.\n\nThis parameter controls the rate at which the influence of the observations at prior time steps decay exponentially. Alpha is often set to a value between 0 and 1. Large values mean that the model pays attention mainly to the most recent past observations, whereas smaller values mean more of the history is taken into account when making a prediction.[2]","fa09aabb":"**Hyperparameter Optimization**","e7ec6181":"# 2) Single Exponential Smoothing","6d9f5b67":"**SARIMA Optimization for MAE**","49907bd8":"# 1) Moving Average","12d9ffb0":"**Hyperparameters:**\n\nAlpha: Smoothing factor for the level.\n\nBeta: Smoothing factor for the trend.\n\nGamma: Smoothing factor for the seasonality.\n\nTrend Type: Additive or multiplicative.\n\nDampen Type: Additive or multiplicative.\n\nPhi: Damping coefficient.\n\nSeasonality Type: Additive or multiplicative.\n\nPeriod: Time steps in seasonal period.","78cae386":"We know that in order to apply the various models we must in the beginning convert the series into Stationary Time Series. In order to achieve the same, we apply the differencing or Integrated method where we subtract the t-1 value from t values of time series. After applying the first differencing if we are still unable to get the Stationary time series then we again apply the second-order differencing.\nThe ARIMA model is quite similar to the ARMA model other than the fact that it includes one more factor known as Integrated( I ) i.e. differencing which stands for I in the ARIMA model. So in short ARIMA model is a combination of a number of differences already applied on the model in order to make it stationary, the number of previous lags along with residuals errors in order to forecast future values.[3]","020f65e2":"Additive Dampening: Dampen a trend linearly.\n\nMultiplicative Dampening: Dampen the trend exponentially.\n\n**Hyperparameters:**\n\nAlpha: Smoothing factor for the level.\n\nBeta: Smoothing factor for the trend.\n\nTrend Type: Additive or multiplicative.\n\nDampen Type: Additive or multiplicative.\n\nPhi: Damping coefficient.","fa505530":"# Partial Autocorrelation Function (PACF)","fab5aa5d":"**Final Model**","cae5283b":"**Hyperparameter Optimization**","2166b78c":"Moving averages can smooth time series data, reveal underlying trends, and identify components for use in statistical modeling. Smoothing is the process of removing random variations that appear as coarseness in a plot of raw time series data. It reduces the noise to emphasize the signal that can contain trends and cycles. Analysts also refer to the smoothing process as filtering the data.\n\nDeveloped in the 1920s, the moving average is the oldest process for smoothing data and continues to be a useful tool today. This method relies on the notion that observations close in time are likely to have similar values. Consequently, the averaging removes random variation, or noise, from the data.[1]","dcac889a":"**Determining Model Grade with ACF and PACF Chart**","24a4ecc7":"# CO2 Emission Forecasting with SES, DES, TES, ARIMA and SARIMA","720616dd":"![image.png](attachment:b29b3464-d7f0-42c6-93b2-dff38201b389.png)","2cc6700a":"The PACF determines the partial correlation between time period t and t-k. It doesn\u2019t take into consideration all the time lags between t and t-k. For e.g. let's assume that today's stock price may be dependent on 3 days prior stock price but it might not take into consideration yesterday's stock price closure. Hence we consider only the time lags having a direct impact on future time period by neglecting the insignificant time lags in between the two-time slots t and t-k.[3]","98e99c40":"![image.png](attachment:382ce073-b6be-4cff-a6d0-8f84e2473524.png)","114c3949":"![image.png](attachment:34a0655d-7221-4465-92b2-0b672dec2aa0.png)","cba7e487":"# SMOOTHING METHODS","3a9d0819":"Let's take an example of sweets sale and income generated in a village over a year. Under the assumption that every 2 months there is a festival in the village, we take out the historical data of sweets sale and income generated for 12 months. If we plot the time as month then we can observe that when it comes to calculating the sweets sale we are interested in only alternate months as the sale of sweets increases every two months. But if we are to consider the income generated next month then we have to take into consideration all the 12 months of last year.\nSo in the above situation, we will use ACF to find out the income generated in the future but we will be using PACF to find out the sweets sold in the next month.[3]"}}