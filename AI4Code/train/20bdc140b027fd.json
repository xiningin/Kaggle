{"cell_type":{"469613c8":"code","87055d1c":"code","ee674f6e":"code","35cadd65":"code","0ba913c1":"code","85b0316d":"code","d618a06c":"code","bb8332b0":"code","55b38147":"markdown","c25dfe79":"markdown","27f3c69e":"markdown","bc4368df":"markdown","84536305":"markdown","cb5ae319":"markdown"},"source":{"469613c8":"from os import walk\nimport os\nimport random\nimport shutil\nfrom trainingmonitor import TrainingMonitor\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport kerastuner as kt\nfrom kerastuner.tuners import Hyperband\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import MaxPooling2D\nfrom tensorflow.python.keras.layers import Activation\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.optimizers import Adam","87055d1c":"#Global Parameters\nEPOCHS = 50\nBATCH_SIZE = 32\nEARLY_STOPPING_PATIENCE = 5 #If the accuracy does not increase after this many epochs we will break the process and continue with next to save resources\nBN_AXIS= -1 #axis=-1 implies channel last ordering[rows][cols][channels].\nNUM_CLASSES = 2 # fire or no_fire\nTRAIN_TEST_SPLIT_RATIO = 0.8 #80% training data and 20% test data","ee674f6e":"#Deleting previously created output directories\nif os.path.exists('.\/train_custom\/'):\n    shutil.rmtree('.\/train_custom\/')\nif os.path.exists('.\/test_custom\/'):\n    shutil.rmtree('.\/test_custom\/')\n\n#Creating output directories\nos.makedirs('.\/train_custom\/fire')\nos.makedirs('.\/train_custom\/no_fire')\nos.makedirs('.\/test_custom\/fire')\nos.makedirs('.\/test_custom\/no_fire')\n\n#Working for \"fire\" class\nallFiles = next(walk('..\/input\/fire-dataset\/fire_dataset\/fire_images'), (None, None, []))[2]  #Getting all file names under folder\ntraining_files = random.sample(allFiles, int(TRAIN_TEST_SPLIT_RATIO*len(allFiles))) #Getting files for training\nfor filename in os.listdir('..\/input\/fire-dataset\/fire_dataset\/fire_images'):\n    if filename in training_files:\n        shutil.copy('..\/input\/fire-dataset\/fire_dataset\/fire_images\/'+filename, '.\/train_custom\/fire')\n    else:\n        shutil.copy('..\/input\/fire-dataset\/fire_dataset\/fire_images\/'+filename, '.\/test_custom\/fire')\n        \n\n#Working for \"no_fire\" class\nallFiles = next(walk('..\/input\/fire-dataset\/fire_dataset\/non_fire_images'), (None, None, []))[2]  #Getting all file names under folder\ntraining_files = random.sample(allFiles, int(TRAIN_TEST_SPLIT_RATIO*len(allFiles))) #Getting files for training\nfor filename in os.listdir('..\/input\/fire-dataset\/fire_dataset\/non_fire_images'):\n    if filename in training_files:\n        shutil.copy('..\/input\/fire-dataset\/fire_dataset\/non_fire_images\/'+filename, '.\/train_custom\/no_fire')\n    else:\n        shutil.copy('..\/input\/fire-dataset\/fire_dataset\/non_fire_images\/'+filename, '.\/test_custom\/no_fire')","35cadd65":"print(\"Loading Images using ImageDataGenerator...\")\ntraindatagen = ImageDataGenerator(zoom_range=0.15, width_shift_range=0.2, height_shift_range=0.2, shear_range=0.15, fill_mode=\"nearest\", rescale=1.\/255)\ntrain_generator = traindatagen.flow_from_directory(directory=\".\/train_custom\", target_size=(128, 128), batch_size=BATCH_SIZE, class_mode=\"categorical\")\n\n#print indices of classes\nprint(train_generator.class_indices)\n\n#No data augmentation on test or validation images\ntestdatagen = ImageDataGenerator(rescale=1.\/255)\ntest_generator = testdatagen.flow_from_directory(directory=\".\/test_custom\", target_size=(128, 128), batch_size=BATCH_SIZE, class_mode=\"categorical\")\n#ImageDataGenerator does hot encoding on class labels based on class_mode parameter\n\n# confirm the iterator works\nbatch_train_x, batch_train_y = train_generator.next()\nprint(\"Loading of images completed!\")","0ba913c1":"#The build_model function. The function takes a \"tuner\" object as parameter which allows us to tune our parameters\ndef build_model(tuner):\n    \n    model = Sequential()\n   \n    #######First CONV => RELU => POOL layer set######\n    model.add(Conv2D(tuner.Int(\"conv1\", min_value=32, max_value=96, step=32),(3, 3), padding=\"same\", input_shape=(128, 128, 3)))  #====>[32, 96] step of 32<=====\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization(axis=BN_AXIS))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    \n    #######Second CONV => RELU => POOL layer set######\n    # Note that the number of filters should increase as we go deeper into the network as we want to learn more features\n    model.add(Conv2D(tuner.Int(\"conv2\", min_value=64, max_value=128, step=32),(3, 3), padding=\"same\"))  #===>[64, 128] step of 32<=====\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization(axis=BN_AXIS))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    \n    #######Fully connected layer(or Dense Layer)######\n    model.add(Flatten())\n    model.add(Dense(tuner.Int(\"fc\", min_value=256, max_value=768, step=256))) #===>[256, 768] step of 256<=====\n    model.add(Activation(\"relu\"))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    #Softmax classifier\n    model.add(Dense(NUM_CLASSES))\n    model.add(Activation(\"softmax\"))\n    \n    #Learning Rate initialization\n    lr = tuner.Choice(\"learning_rate\", values=[1e-1, 1e-2, 1e-3]) #===>Either 1e-1 or 1e-2 or 1e-3<=====\n    opt = Adam(learning_rate=lr)\n    \n    #Compile the model\n    model.compile(optimizer=opt, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    \n    #Return the model\n    return model","85b0316d":"#To visualize the accuracy and loss curves after each epoch\nmonitor= TrainingMonitor(\".\/perEpochGraph.png\")\n\n# To stop the training in current approach if the Accuracy does not incerease after \"EARLY_STOPPING_PATIENCE\" epochs\nearlyStopper = EarlyStopping(monitor=\"val_loss\", patience=EARLY_STOPPING_PATIENCE, restore_best_weights=True)","d618a06c":"print(\"Instantiating a random search tuner object...\")\ntuner = kt.Hyperband(build_model, objective=\"val_accuracy\", max_epochs=EPOCHS, factor=3, seed=42, directory='.\/', project_name='my_hyperband_tuner')\n# Our objective is to increase val_accuracy\n# everytime you run a deep learning model, the initializations and updates are random. So seed value allows some constant start point","bb8332b0":"# perform the hyperparameter search\nprint(\"[INFO] performing hyperparameter search...\")\ntuner.search(train_generator, validation_data=test_generator, batch_size=BATCH_SIZE, callbacks=[earlyStopper], epochs=EPOCHS)\n\n# grab the best hyperparameters\nbestHP = tuner.get_best_hyperparameters(num_trials=1)[0]\nprint(\"Best value of conv1: {}\".format(bestHP.get(\"conv1\")))\nprint(\"Best value of conv2: {}\".format(bestHP.get(\"conv2\")))\nprint(\"Best value of FC: {}\".format(bestHP.get(\"fc\")))\nprint(\"Best value of LR: {:.4f}\".format(bestHP.get(\"learning_rate\")))","55b38147":"# Search the best hyper-parameters\nOne important thing to note here is that although you may have define EPOCHS=50, still the Tuner might run the trial for 2 or 3 epochs. This is because of the EarlyStopping provision. The tuner didn't see the accuracy increasing and hence reduced the number of epochs automatically.","c25dfe79":"# The Regular Dataset import work","27f3c69e":"# Creating Tuner Object\nThe Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + logfactor(max_epochs) and rounding it up to the nearest integer.","bc4368df":"# The function to build our model using \"Tuner\" Object","84536305":"# Making ImageDataGenerator compatible directory(Optional)\nI really like using ImageDataGenerator as it handles most of the stuff and is very optimized. So I always take some time out to make the dataset compatible with directory structure needed for ImageDataGenerator's working. Also since this step is not counted to training time, so feel free to use any approach here as long as the output directory strucuture is compatible with ImageDataGenerator.","cb5ae319":"# Callbacks"}}