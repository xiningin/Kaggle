{"cell_type":{"7ad0d39f":"code","dca9743e":"code","407d23e7":"code","8c9a6cd0":"code","d16ba082":"code","eac6464c":"code","be834c6a":"code","e17d9216":"code","3eaa4f18":"code","444e1e23":"code","320b9e0a":"code","de2d888a":"code","89f24a79":"code","801c1749":"code","103848c6":"code","f052b1da":"code","4c9d85d9":"code","4d46e42b":"code","c4caf7b7":"code","31057b5f":"code","75bc7d1d":"code","a4085bdc":"code","ca226aab":"code","7cd3054e":"code","0529133c":"code","b4e246af":"code","257c41b7":"code","18ecd376":"code","939d6ee8":"code","35ea9928":"code","50425ecd":"code","dce1e660":"code","71ed07b6":"code","a8fcc973":"code","702396b5":"code","0da495df":"code","76e6c0c8":"code","f8cb58c4":"code","c24c46d7":"code","f5d75d95":"code","2c176979":"markdown","ff8bae75":"markdown","cecc39fa":"markdown","f075d12e":"markdown","acb2afe5":"markdown"},"source":{"7ad0d39f":"import keras\nfrom keras.models import *\nfrom keras.layers import *\nfrom keras.optimizers import SGD\nimport cv2\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os","dca9743e":"def model1_smallervgg(size = 224):\n    model = Sequential()\n    model.add(Conv2D(32, (3,3), padding = 'same', input_shape = (size, size, 3) , activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((3,3)))\n    model.add(Dropout(0.25))\n    \n    model.add(Conv2D(64, (3,3), padding = 'same', activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, (3,3), padding = 'same', activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2,2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Conv2D(128, (3,3), padding = 'same', activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(128, (3,3), padding = 'same', activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(MaxPooling2D((2,2)))\n    model.add(Dropout(0.25))\n    \n    model.add(Flatten())\n    model.add(Dense(1024, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    \n    model.add(Dense(1, activation = 'sigmoid'))\n    #opt = keras.optimizers.Adam(lr=lr, decay=lr\/epochs)\n    #model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n    \n    return model","407d23e7":"model1 = model1_smallervgg()","8c9a6cd0":"model1.load_weights('..\/input\/gender-classification-using-smallervgg\/model1.h5')","d16ba082":"model1.summary()","eac6464c":"\"\"\"females = ['https:\/\/media-exp1.licdn.com\/dms\/image\/C4D03AQFjvXb55AwrTA\/profile-displayphoto-shrink_200_200\/0?e=1592438400&v=beta&t=FjMIH5j4dSlFgDU9b0Ed810-yPR4WIvkAiW3HhFwrfY',\n          'https:\/\/media-exp1.licdn.com\/dms\/image\/C4D03AQECKT9M0okpKw\/profile-displayphoto-shrink_200_200\/0?e=1592438400&v=beta&t=1EjMz3PtI8gdBwQNWLAQPBkFNoJsWDHgiJ99pSVVUV8',\n          'https:\/\/media-exp1.licdn.com\/dms\/image\/C4D03AQEdnvvCRfSK4A\/profile-displayphoto-shrink_200_200\/0?e=1592438400&v=beta&t=7eyiTt8fUFkYJJMVb1Lu71mTdk8r5hnRMlog1e8XacE',\n          'https:\/\/media-exp1.licdn.com\/dms\/image\/C4D03AQFaf_NpuCGm_w\/profile-displayphoto-shrink_200_200\/0?e=1592438400&v=beta&t=-xWIpbuI7n4YRCyS1naUfe4_HvkWPZp07n1yOEAmZdY',\n           'https:\/\/media-exp1.licdn.com\/dms\/image\/C4D03AQHxkJVCPNS21A\/profile-displayphoto-shrink_200_200\/0?e=1592438400&v=beta&t=siM5a690BB5gtPeC3kjgnjxnfjeR-sEZrTEWbeUj8Pk'\n          ]\n\nmales = ['https:\/\/media-exp1.licdn.com\/dms\/image\/C4E03AQFOwIgCwqTKjQ\/profile-displayphoto-shrink_200_200\/0?e=1592438400&v=beta&t=RtMpQFCWzAeOsofTIVLjlVXA5S60hnZM4anCQ5gqY5g',\n          'https:\/\/media-exp1.licdn.com\/dms\/image\/C4E03AQHvhRIROINsgQ\/profile-displayphoto-shrink_200_200\/0?e=1592438400&v=beta&t=HjyaltGVVxN-T9WERA_UngeldQhbbfgPuqHfsmgZIrU',\n        'https:\/\/media-exp1.licdn.com\/dms\/image\/C4D03AQGHxRl5xyoMog\/profile-displayphoto-shrink_200_200\/0?e=1592438400&v=beta&t=bn6r8YCY-_YHSw3t9hmTAGlaQHSQlh436DPe-O4fceI',\n         'https:\/\/media-exp1.licdn.com\/dms\/image\/C5603AQENDOWr3WPJtQ\/profile-displayphoto-shrink_200_200\/0?e=1592438400&v=beta&t=2eaJwZmWGULN7VfWPF8KYcrXIHYMK33gp_AKJglvDIo',\n         'https:\/\/media-exp1.licdn.com\/dms\/image\/C5103AQGUmvpQ-mYMgg\/profile-displayphoto-shrink_200_200\/0?e=1592438400&v=beta&t=NJDTkBW570g5EFDOs-2fI9wuInCCL8XspO2IQinJxV8'\n        ]\n\"\"\"\n\nmales = ['https:\/\/media-exp1.licdn.com\/dms\/image\/C4E03AQFM3An-mg8WJg\/profile-displayphoto-shrink_400_400\/0\/1608028624557?e=1619654400&v=beta&t=PwbFoiLgbviHYKP80_BKqyyY-xNRva-6RjaLZu8FpwI',\n        'https:\/\/media-exp1.licdn.com\/dms\/image\/C5603AQFjZ6QcvBoKAA\/profile-displayphoto-shrink_400_400\/0\/1517700257318?e=1619654400&v=beta&t=ukx1PfmggdZ-jpBlBuqIlh7jXACdTrtopFWuSxm9P7Q',\n        'https:\/\/media-exp1.licdn.com\/dms\/image\/C4E03AQE7UQZki6PSnw\/profile-displayphoto-shrink_400_400\/0\/1532274852514?e=1619654400&v=beta&t=cB0ZiCR-jt1cgVzHKHwb7Hl4A4qm6X3AAi3CNvJ9wXs',\n        'https:\/\/media-exp1.licdn.com\/dms\/image\/C5603AQHAc0sHx24szg\/profile-displayphoto-shrink_400_400\/0\/1583639996352?e=1619654400&v=beta&t=VxXPlOwYEfvBDi1szv1ij1aGRrRV03v_ZrnO7CaNkg0',\n        'https:\/\/media-exp1.licdn.com\/dms\/image\/C5603AQGBQJvp9dTYMA\/profile-displayphoto-shrink_400_400\/0\/1516317129014?e=1619654400&v=beta&t=zAs_xCNS7VP2TO6lKZR8SDqeRVl7xZB2AnpwwiSORIM']\n\n\nfemales = ['https:\/\/media-exp1.licdn.com\/dms\/image\/C5603AQFEAZDaOfwwMw\/profile-displayphoto-shrink_400_400\/0\/1613620248158?e=1619654400&v=beta&t=WrWYxO8PhlK7G3dRIKQnqpTrIHqPBnvJ4_6z4fRzTYA',\n          'https:\/\/media-exp1.licdn.com\/dms\/image\/C4D03AQGUStJdQ1tc5g\/profile-displayphoto-shrink_400_400\/0\/1516535529437?e=1619654400&v=beta&t=7cPvPWR9EBtZCD2W8AteL1Vqc9HykFe8aS3U86akaWU',\n          'https:\/\/media-exp1.licdn.com\/dms\/image\/C5603AQHsW_BHJ6VgDw\/profile-displayphoto-shrink_400_400\/0\/1595952381691?e=1619654400&v=beta&t=tpoCxmioJG7bxk3NxxK4JqEgTHNAorkwQY9mLcX-6q4',\n          'https:\/\/media-exp1.licdn.com\/dms\/image\/C5603AQG3Gpou4PgHRQ\/profile-displayphoto-shrink_400_400\/0\/1604680887339?e=1619654400&v=beta&t=SoD-bSgzOEJOZejBhwXGpfozZjO0C2ZnbC3sRHsvn9M',\n          'https:\/\/media-exp1.licdn.com\/dms\/image\/C5603AQENdcoHklYY8g\/profile-displayphoto-shrink_400_400\/0\/1518028565515?e=1619654400&v=beta&t=-fM_miUI6-EiXCBI1IrhPvCXMNtoWsRt6OqZ9AmqO90']","be834c6a":"females[0]","e17d9216":"from PIL import Image\nimport requests\nfrom io import BytesIO\n#url = 'https:\/\/media-exp1.licdn.com\/dms\/image\/C5103AQF10FzP5dk7yw\/profile-displayphoto-shrink_200_200\/0?e=1592438400&v=beta&t=RU5adt3zuPIMVGFm83XokMR5uRwwbPOdFnJgtHwnur8'\ndef image_loader_url(url, size = 100):\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    #print(img,'\\n\\n\\n')\n    img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n    #print(img.shape)\n    img = cv2.resize(img, (size, size))\n    #plt.imshow(img[:,:,::-1])\n    img = img.astype('float32')\n    img = img\/255.0\n    return img","3eaa4f18":"lst = []\nf = 0\nm = 0\nfor i in range(10):\n    if i % 2 == 0:\n        lst.append(image_loader_url(females[f],224))\n        f+= 1\n    else:\n        lst.append(image_loader_url(males[m], 224))\n        m += 1\nlst = np.array(lst)\n        ","444e1e23":"lst.shape","320b9e0a":"\"\"\"females = [os.path.join('..\/input\/linkedin-profile-pic-data\/LinkedIn Test Images\/Female',i) for i in os.listdir('..\/input\/linkedin-profile-pic-data\/LinkedIn Test Images\/Female')]\nfemales\"\"\"","de2d888a":"\"\"\"males = [os.path.join('..\/input\/linkedin-profile-pic-data\/LinkedIn Test Images\/Male',i) for i in os.listdir('..\/input\/linkedin-profile-pic-data\/LinkedIn Test Images\/Male')]\nmales\"\"\"","89f24a79":"\"\"\"lst = []\nf = 0\nm = 0\nfor i in range(10):\n    if i % 2 == 0:\n        lst.append(np.array(Image.open(females[f]).convert('RGB'))[:,:,::-1])\n        f+= 1\n    else:\n        lst.append(np.array(Image.open(males[m]).convert('RGB'))[:,:,::-1])\n        m += 1\nlst = np.array(lst)\"\"\"","801c1749":"lst","103848c6":"lst.shape","f052b1da":"print('Image #1')\nplt.imshow(lst[0][:,:,::-1])","4c9d85d9":"print('Image #2')\nplt.imshow(lst[1][:,:,::-1])","4d46e42b":"print('Image #3')\nplt.imshow(lst[2][:,:,::-1])","c4caf7b7":"print('Image #4')\nplt.imshow(lst[3][:,:,::-1])","31057b5f":"print('Image #5')\nplt.imshow(lst[4][:,:,::-1])","75bc7d1d":"print('Image #6')\nplt.imshow(lst[5][:,:,::-1])","a4085bdc":"print('Image #7')\nplt.imshow(lst[6][:,:,::-1])","ca226aab":"print('Image #8')\nplt.imshow(lst[7][:,:,::-1])","7cd3054e":"print('Image #9')\nplt.imshow(lst[8][:,:,::-1])","0529133c":"print('Image #10')\nplt.imshow(lst[9][:,:,::-1])","b4e246af":"actual = ['Female', 'Male','Female', 'Male','Female', 'Male','Female', 'Male','Female', 'Male']","257c41b7":"preds = model1.predict_classes(lst)\nresult = []\nfor i, item in enumerate(preds):\n    if item == 1:\n        result.append('Male')\n    else:\n        result.append('Female')","18ecd376":"result","939d6ee8":"score = 0\nfor i in zip(actual, result):\n    if i[0] == i[1]:\n        score +=1 \n\nprint('Accuracy :' , score\/10)","35ea9928":"import numpy as np\nimport matplotlib.pyplot as plt\n\nw=10\nh=10\nfig=plt.figure(figsize=(20, 20))\ncolumns = 4\nrows = 5\nj = 0\nfig.suptitle('Result of the model prediction')\n\nfor i in range(1, columns*rows +1):\n    \n    #img = Image.fromarray(lst[j][:,:,::-1])\n    img = lst[j][:, :, ::-1]\n    fig.add_subplot(rows, columns, i).set_title('Pred->' + result[j] +  ', ' + 'Actual ->' +  actual[j])\n    plt.imshow(img)\n    j+=1\n    if j == 10:\n        break\nplt.show()","50425ecd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nimport seaborn as sns\nimport umap\nfrom PIL import Image\nfrom scipy import misc\nfrom os import listdir\nfrom os.path import isfile, join\nimport numpy as np\nfrom scipy import misc\nfrom random import shuffle\nfrom collections import Counter\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras.utils.np_utils import to_categorical","dce1e660":"model2 = tf.keras.Sequential()\n\n# Must define the input shape in the first layer of the neural network\nmodel2.add(tf.keras.layers.Conv2D(filters=64, kernel_size=2, padding='same', activation='relu', input_shape=(128,128,3))) \nmodel2.add(tf.keras.layers.MaxPooling2D(pool_size=2))\nmodel2.add(tf.keras.layers.Dropout(0.3))\n\nmodel2.add(tf.keras.layers.Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\nmodel2.add(tf.keras.layers.MaxPooling2D(pool_size=2))\nmodel2.add(tf.keras.layers.Dropout(0.3))\n\nmodel2.add(tf.keras.layers.Flatten())\nmodel2.add(tf.keras.layers.Dense(256, activation='relu'))\nmodel2.add(tf.keras.layers.Dropout(0.5))\nmodel2.add(tf.keras.layers.Dense(2, activation='sigmoid'))\n\n# Take a look at the model summary\nmodel2.summary()","71ed07b6":"model2.load_weights('..\/input\/gender-classification-using-cnn-model\/model2.h5')","a8fcc973":"\"\"\"lst = []\nf = 0\nm = 0\nfor i in range(10):\n    if i % 2 == 0:\n        lst.append(image_loader_url(females[f],32))\n        f+= 1\n    else:\n        lst.append(image_loader_url(males[m], 32))\n        m += 1\nlst = np.array(lst)\n        \"\"\"","702396b5":"\"\"\"lst = []\nf = 0\nm = 0\nfor i in range(10):\n    if i % 2 == 0:\n        lst.append(np.array(Image.open(females[f]).resize((128,128)).convert('RGB'))[:,:,::-1])\n        f+= 1\n    else:\n        lst.append(np.array(Image.open(males[m]).resize((128,128)).convert('RGB'))[:,:,::-1])\n        m += 1\nlst = np.array(lst)\"\"\"","0da495df":"lst = []\nf = 0\nm = 0\nfor i in range(10):\n    if i % 2 == 0:\n        lst.append(image_loader_url(females[f],128))\n        f+= 1\n    else:\n        lst.append(image_loader_url(males[m], 128))\n        m += 1\nlst = np.array(lst)\n        ","76e6c0c8":"preds = model2.predict_classes(lst)\nresult = []\nfor i, item in enumerate(preds):\n    if item == 0:\n        result.append('Male')\n    else:\n        result.append('Female')","f8cb58c4":"result","c24c46d7":"score = 0\nfor i in zip(actual, result):\n    if i[0] == i[1]:\n        score +=1 \n\nprint('Accuracy :' , score\/10)","f5d75d95":"import numpy as np\nimport matplotlib.pyplot as plt\n\nw=10\nh=10\nfig=plt.figure(figsize=(20, 20))\ncolumns = 4\nrows = 5\nj = 0\nfig.suptitle('Result of the model prediction')\n\nfor i in range(1, columns*rows +1):\n    \n    #img = Image.fromarray(lst[j][:,:,::-1])\n    img = lst[j][:,:,::-1]\n    fig.add_subplot(rows, columns, i).set_title('Pred->' + result[j] +  ', ' + 'Actual ->' +  actual[j])\n    plt.imshow(img)\n    j+=1\n    if j == 10:\n        break\nplt.show()","2c176979":"**Resources**\n\nhttps:\/\/towardsdatascience.com\/building-a-convolutional-neural-network-male-vs-female-50347e2fa88b\n\nhttps:\/\/www.kaggle.com\/uysimty\/keras-cnn-dog-or-cat-classification\n\nhttps:\/\/machinelearningmastery.com\/how-to-develop-a-convolutional-neural-network-to-classify-photos-of-dogs-and-cats\/\n\nhttps:\/\/github.com\/arunponnusamy\/gender-detection-keras\/blob\/master\/model\/smallervggnet.py\n\n","ff8bae75":"Conclusion Column (keep adding after every experiment)\n\n- smallervgg -> extensive data augmentation (DONE)\n1. Cats and Dogs can be easily distinguished due to the distinctive feature differences as they are two different animal.\n2. Males and Females of humans are hard to distinguish that's why the best of neural networks are also failing.\n\n - running siamese network will help but it is computationally expensive for training and deployment (DUE)\n 1. try running siamese net and see the results obtained\n \n- Ran someone's nb, same problem as before. The model is dataset specific. Doesn't work in real world. Works well for the dataset, it not working in real world. (DONE)\n     1. Try extracting out the faces from the test set images (DUE)\n     2. Try to work on identification of different dog breeds to the highest level of accuracy. Work on similar grounds for this dataset. (DUE) (WORKING ON)\n     ","cecc39fa":"**Quick look at the test set**","f075d12e":"Model 2 (CNN - UTKFace)","acb2afe5":"A Quick look at the Test Dataset"}}