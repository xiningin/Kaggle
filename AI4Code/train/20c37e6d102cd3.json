{"cell_type":{"2a95c5cb":"code","887e16ff":"code","bb6810b4":"code","81c57de9":"code","198826b9":"code","56538e70":"code","9d99780d":"code","d43a6788":"code","3f222ca7":"code","44e36c18":"code","ed2c8fdd":"code","71770bf0":"code","2f8d6364":"code","626bfe2e":"markdown","21e0cd9f":"markdown","029619fd":"markdown"},"source":{"2a95c5cb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport json\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","887e16ff":"from sklearn.model_selection import train_test_split\n\nREVIEWS_LIMIT = 100000 #300000\n\ndef load_rows(filepath, nrows = None, func = None) -> pd.DataFrame :\n    with open(filepath) as json_file:\n        count = 0\n        objs = []\n        line = json_file.readline()\n        while (nrows is None or count < nrows) and line:\n            count += 1\n            obj = json.loads(line)\n            if func != None :\n                func(obj)\n            objs.append(obj)\n            line = json_file.readline()\n        return pd.DataFrame(objs)\n    \n# Aggiunge la classe della recensione\ndef add_sentiment(obj) :\n    if (obj[\"stars\"] <= 3):\n        obj[\"label\"] = 0\n    else:\n        obj[\"label\"] = 1\n        \nreviews = load_rows('..\/input\/yelp-dataset\/yelp_academic_dataset_review.json', REVIEWS_LIMIT, add_sentiment)\nprint('Review objects loaded. Count = {}'.format(reviews.shape[0]))\n\nreviews['text_length'] = reviews['text'].apply(lambda x:len(x.split()))\n\n# 80% train, 20% test\nreviews_train, reviews_test = train_test_split(reviews, test_size = 0.2)\n\n# Solo text, label\nreviews_train = reviews_train[['text', 'label']]\nreviews_test = reviews_test[['text', 'label']]\ndisplay(reviews_train.head(2))\ndisplay(reviews_test.head(2))\n\n#with pd.option_context('display.max_colwidth', None):\n#  display(reviews_train)","bb6810b4":"from tqdm import tqdm\nimport re\nimport copy\nimport nltk\nfrom nltk.corpus import stopwords \nfrom nltk.stem import WordNetLemmatizer\n\npd.options.mode.chained_assignment = None  # default='warn'\n\ndef contractions(sent):\n    sent = re.sub(r\"ain't\", \"am not\", sent)\n    sent = re.sub(r\"aren't\", \"are not\", sent)\n    sent = re.sub(r\"can't\", \"can not\", sent)\n    sent = re.sub(r\"can't've\", \"can not have\", sent)\n    sent = re.sub(r\"'cause\", \"because\", sent)\n    sent = re.sub(r\"could've\", \"could have\", sent)\n    sent = re.sub(r\"couldn't\", \"could not\", sent)\n    sent = re.sub(r\"couldn't've\", \"could not have\", sent)\n    sent = re.sub(r\"doesn't\", \"does not\", sent)\n    sent = re.sub(r\"hadn't\", \"had not\", sent)\n    sent = re.sub(r\"hadn't've\", \"had not have\", sent)\n    sent = re.sub(r\"hasn't\", \"has not\", sent)\n    sent = re.sub(r\"haven't\", \"have not\", sent)\n    sent = re.sub(r\"he'd\", \"he had\", sent)\n    sent = re.sub(r\"he'd've\", \"he would have\", sent)\n    sent = re.sub(r\"he'll\", \"he will\", sent)\n    sent = re.sub(r\"he'll've\", \"he will have\", sent)\n    sent = re.sub(r\"he's\", \"he has\", sent)\n    sent = re.sub(r\"how'd\", \"how did\", sent)\n    sent = re.sub(r\"how'd'y\", \"how do you\", sent)\n    sent = re.sub(r\"how'll\", \"how will\", sent)\n    sent = re.sub(r\"how's\", \"how has\", sent)\n    sent = re.sub(r\"i'd\", \"i had\", sent)\n    sent = re.sub(r\"i'd've\", \"i would have\", sent)\n    sent = re.sub(r\"i'll\", \"i shall\", sent)\n    sent = re.sub(r\"i'll've\", \"i shall have\", sent)\n    sent = re.sub(r\"i'm\", \"i am\", sent)\n    sent = re.sub(r\"i've\", \"i have\", sent)\n    sent = re.sub(r\"isn't\", \"is not\", sent)\n    sent = re.sub(r\"it'd\", \"it had\", sent)\n    sent = re.sub(r\"it'd've\", \"it would have\", sent)\n    sent = re.sub(r\"it'll\", \"it shall\", sent)\n    sent = re.sub(r\"it'll've\", \"it shall have\", sent)\n    sent = re.sub(r\"it's\", \"it is\", sent)\n    sent = re.sub(r\"let's\", \"let us\", sent)\n    sent = re.sub(r\"ma'am\", \"madam\", sent)\n    sent = re.sub(r\"mayn't\", \"may not\", sent)\n    sent = re.sub(r\"might've\", \"might have\", sent)\n    sent = re.sub(r\"mightn't\", \"might not\", sent)\n    sent = re.sub(r\"mightn't've\", \"might not have\", sent)\n    sent = re.sub(r\"must've\", \"must have\", sent)\n    sent = re.sub(r\"mustn't\", \"must not\", sent)\n    sent = re.sub(r\"mustn't've\", \"must not have\", sent)\n    sent = re.sub(r\"needn't\", \"need not\", sent)\n    sent = re.sub(r\"needn't've\", \"need not have\", sent)\n    sent = re.sub(r\"o'clock\", \"of the clock\", sent)\n    sent = re.sub(r\"oughtn't\", \"ought not\", sent)\n    sent = re.sub(r\"oughtn't've\", \"ought not have\", sent)\n    sent = re.sub(r\"shan't\", \"shall not\", sent)\n    sent = re.sub(r\"sha'n't\", \"shall not\", sent)\n    sent = re.sub(r\"shan't've\", \"shall not have\", sent)\n    sent = re.sub(r\"she'd\", \"she had\", sent)\n    sent = re.sub(r\"she'd've\", \"she would have\", sent)\n    sent = re.sub(r\"she'll\", \"she shall\", sent)\n    sent = re.sub(r\"she'll've\", \"she shall have\", sent)\n    sent = re.sub(r\"she's\", \"she has\", sent)\n    sent = re.sub(r\"should've\", \"should have\", sent)\n    sent = re.sub(r\"shouldn't\", \"should not\", sent)\n    sent = re.sub(r\"shouldn't've\", \"should not have\", sent)\n    sent = re.sub(r\"so've\", \"so have\", sent)\n    sent = re.sub(r\"so's\", \"so as\", sent)\n    sent = re.sub(r\"that'd\", \"that would\", sent)\n    sent = re.sub(r\"that'd've\", \"that would have\", sent)\n    sent = re.sub(r\"that's\", \"that has\", sent)\n    sent = re.sub(r\"there'd\", \"there had\", sent)\n    sent = re.sub(r\"there'd've\", \"there would have\", sent)\n    sent = re.sub(r\"there's\", \"there has\", sent)\n    sent = re.sub(r\"they'd\", \"they had\", sent)\n    sent = re.sub(r\"they'd've\", \"they would have\", sent)\n    sent = re.sub(r\"they'll\", \"they shall\", sent)\n    sent = re.sub(r\"they'll've\", \"they shall have\", sent)\n    sent = re.sub(r\"they're\", \"they are\", sent)\n    sent = re.sub(r\"they've\", \"they have\", sent)\n    sent = re.sub(r\"to've\", \"to have\", sent)\n    sent = re.sub(r\"wasn't\", \"was not\", sent)\n    sent = re.sub(r\"we'd\", \"we had\", sent)\n    sent = re.sub(r\"we'd've\", \"we would have\", sent)\n    sent = re.sub(r\"we'll\", \"we will\", sent)\n    sent = re.sub(r\"we'll've\", \"we will have\", sent)\n    sent = re.sub(r\"we're\", \"we are\", sent)\n    sent = re.sub(r\"we've\", \"we have\", sent)\n    sent = re.sub(r\"weren't\", \"were not\", sent)\n    sent = re.sub(r\"what'll\", \"what shall\", sent)\n    sent = re.sub(r\"what'll've\", \"what shall have\", sent)\n    sent = re.sub(r\"what're\", \"what are\", sent)\n    sent = re.sub(r\"what's\", \"what has\", sent)\n    sent = re.sub(r\"what've\", \"what have\", sent)\n    sent = re.sub(r\"when's\", \"when has\", sent)\n    sent = re.sub(r\"when've\", \"when have\", sent)\n    sent = re.sub(r\"where'd\", \"where did\", sent)\n    sent = re.sub(r\"where's\", \"where has\", sent)\n    sent = re.sub(r\"where've\", \"where have\", sent)\n    sent = re.sub(r\"who'll\", \"who shall\", sent)\n    sent = re.sub(r\"who'll've\", \"who shall have\", sent)\n    sent = re.sub(r\"who's\", \"who has\", sent)\n    sent = re.sub(r\"who've\", \"who have\", sent)\n    sent = re.sub(r\"why's\", \"why has\", sent)\n    sent = re.sub(r\"why've\", \"why have\", sent)\n    sent = re.sub(r\"will've\", \"will have\", sent)\n    sent = re.sub(r\"won't\", \"will not\", sent)\n    sent = re.sub(r\"won't've\", \"will not have\", sent)\n    sent = re.sub(r\"would've\", \"would have\", sent)\n    sent = re.sub(r\"wouldn't\", \"would not\", sent)\n    sent = re.sub(r\"wouldn't've\", \"would not have\", sent)\n    sent = re.sub(r\"y'all\", \"you all\", sent)\n    sent = re.sub(r\"y'all'd\", \"you all would\", sent)\n    sent = re.sub(r\"y'all'd've\", \"you all would have\", sent)\n    sent = re.sub(r\"y'all're\", \"you all are\", sent)\n    sent = re.sub(r\"y'all've\", \"you all have\", sent)\n    sent = re.sub(r\"you'd\", \"you had\", sent)\n    sent = re.sub(r\"you'd've\", \"you would have\", sent)\n    sent = re.sub(r\"you'll\", \"you shall\", sent)\n    sent = re.sub(r\"you'll've\", \"you shall have\", sent)\n    sent = re.sub(r\"how's\", \"how has\", sent)\n    sent = re.sub(r\"you're\", \"you are\", sent)\n    sent = re.sub(r\"you've\", \"you have\", sent)\n    sent = re.sub(r\"didn't\", \"did not\", sent)\n    sent = re.sub(r\"don't\", \"do not\", sent)\n    sent = re.sub(r\"'\",\"\",sent)\n    sent = re.sub(r\". . .\",\"\",sent)\n    return(sent)\n\n## Function for removing unwanted text\ndef processing(data_1):\n \n    for index, row in tqdm(data_1.iterrows()):\n        stri = \"\"\n## Code to remove digit with word pattern\n        cle = re.sub(r'([\\d]+[a-zA-Z]+)|([a-zA-Z]+[\\d]+)', \"\", row[\"text\"])\n## Code to remove only digit patter\n        cle = re.sub(r\"(^|\\s)(\\-?\\d+(?:\\.\\d)*|\\d+|[\\d]+[A-Za-z]+)\",\" \", cle.lower())\n## Code to remove every symbols except characters\n        cle = re.sub('[^A-Za-z\\']+', \" \", cle)\n## Code for concatinating strings\n        stri = stri + cle\n## Code for calling contraction function\n        stri = contractions(stri)\n        data_1[\"text\"][index] = stri\n    return(data_1)\n\n## Function for stopwords removal and lemitizing the word\ndef lema_stopw(data_l):\n    var2 = copy.deepcopy(data_l)\n    lemmatizer = WordNetLemmatizer()\n    stop_words = set(stopwords.words('english')) - set(['no', 'not'])\n    for index, row in tqdm(var2.iterrows()):\n        sent = ''\n        for e in row[\"text\"].split():\n            if e not in stop_words:\n                e = lemmatizer.lemmatize(e, pos =\"a\")\n                sent = ' '.join([sent,e])\n        var2[\"text\"][index] = sent\n    return(var2)\n\nreviews_train = processing(reviews_train)\nreviews_test = processing(reviews_test)\nreviews_train.head(5)","81c57de9":"reviews_train = lema_stopw(reviews_train)\nreviews_test = lema_stopw(reviews_test)\nreviews_train.head(5)","198826b9":"# Text vectorization\n# Bigram Counts\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom joblib import dump, load # used for saving and loading sklearn objects\n\n\nbigram_vectorizer = CountVectorizer(ngram_range=(1, 2))\nbigram_vectorizer.fit(reviews_train['text'].values)\n\n\nX_train_bigram = bigram_vectorizer.transform(reviews_train['text'].values)\nX_test_bigram = bigram_vectorizer.transform(reviews_test['text'].values)\n\n# Bigram Tf-Idf\n\nbigram_tf_idf_transformer = TfidfTransformer()\nbigram_tf_idf_transformer.fit(X_train_bigram)\n\nX_train_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_train_bigram)\ny_train = reviews_train['label'].values\n\nX_test_bigram_tf_idf = bigram_tf_idf_transformer.transform(X_test_bigram)\ny_test = reviews_test['label'].values\n","56538e70":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import uniform\nimport time\n\n# loss, learning rate, initial learning rate, penalty and alpha\n\nparams = {\n    \"n_neighbors\" : [3, 5, 10],\n    \"weights\" : [\"uniform\", \"distance\"]\n}\n\ngrid_search_knn_bigram = GridSearchCV(\n    estimator= KNeighborsClassifier(n_jobs=-1),\n    cv=5,\n    param_grid = params,\n    scoring='f1',\n    verbose=1,\n    return_train_score = True\n)\n\ngrid_search_knn_tf_idf = GridSearchCV(\n    estimator= KNeighborsClassifier(n_jobs=-1),\n    cv=5,\n    param_grid = params,\n    verbose=1,\n    scoring=\"f1\",\n    return_train_score = True\n)\n\nstart_time = time.time()\ngrid_search_knn_bigram.fit(X_train_bigram, y_train)\nprint(\"--- Ended in %s minutes ---\" % ((time.time() - start_time)\/60))\n\nstart_time = time.time()\ngrid_search_knn_tf_idf.fit(X_train_bigram_tf_idf, y_train)\nprint(\"--- Ended in %s minutes ---\" % ((time.time() - start_time)\/60))\n\nprint(f'\\nKNN Bigram')\nprint(f'Best params: {grid_search_knn_bigram.best_params_}')\nprint(f'Best score: {grid_search_knn_bigram.best_score_}')\n\nprint(f'\\nKNN Bigram Tf-Idf')\nprint(f'Best params: {grid_search_knn_tf_idf.best_params_}')\nprint(f'Best score: {grid_search_knn_tf_idf.best_score_}')\n\nknn_classifier_bigram = grid_search_knn_bigram.best_estimator_\nknn_classifier_tf_idf = grid_search_knn_tf_idf.best_estimator_","9d99780d":"from sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\n\ndef evaluate(y_test,y_pred):\n    print(\"Precision Score of the model:\", precision_score(y_test,y_pred)*100)\n    print(\"Recall Score of the model:\", recall_score(y_test,y_pred)*100)\n    print(\"Acuracy score of the model:\",accuracy_score(y_test,y_pred)*100)\n    print(\"F1 score of the model:\",f1_score(y_test,y_pred)*100)\n    \ndef set_labels(cf_matrix):\n    group_names = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                    cf_matrix.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cf_matrix.flatten()\/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n              zip(group_names,group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    return labels","d43a6788":"from sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import plot_precision_recall_curve\nimport seaborn as sn\n\nprint(\"KNN Bigram\")\ny_pred = knn_classifier_bigram.predict(X_test_bigram)\n\nprint(classification_report(y_test, y_pred))\n\ncf_matrix =confusion_matrix(y_test, y_pred)\nsn.heatmap(cf_matrix, annot=set_labels(cf_matrix), fmt=\"\")","3f222ca7":"evaluate(y_test,y_pred)","44e36c18":"disp = plot_precision_recall_curve(knn_classifier_bigram, X_test_bigram, y_test)\ndisp.ax_.set_title(f'KNN Bigram Precision-Recall curve')","ed2c8fdd":"print(\"KNN Bigram Tf-Idf\")\ny_pred = knn_classifier_tf_idf.predict(X_test_bigram)\n\nprint(classification_report(y_test, y_pred))\n\ncf_matrix =confusion_matrix(y_test, y_pred)\nsn.heatmap(cf_matrix, annot=set_labels(cf_matrix), fmt=\"\")","71770bf0":"evaluate(y_test,y_pred)","2f8d6364":"disp = plot_precision_recall_curve(knn_classifier_tf_idf, X_test_bigram, y_test)\ndisp.ax_.set_title(f'KNN Bigram Tf-Idf Precision-Recall curve')","626bfe2e":"# Classificatore KNN","21e0cd9f":"## KNN: Test","029619fd":"## KNN: Grid search CV"}}