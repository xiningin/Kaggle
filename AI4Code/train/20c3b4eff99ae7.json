{"cell_type":{"0ef4cdc6":"code","916bfe77":"code","f9757b59":"code","005d5c49":"code","99bc434b":"code","7a0f5df1":"code","eedc6bf7":"code","4d8e0012":"code","c363e05a":"code","42b16017":"code","c0df8c5f":"code","43e521bf":"code","e3484db4":"code","cd48f040":"code","6c7d7366":"code","389dce69":"code","27586f5d":"code","970f7640":"markdown","944762e8":"markdown","46e106e3":"markdown","ef72a636":"markdown"},"source":{"0ef4cdc6":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport xgboost as xgb\nimport sklearn\nimport tqdm\nimport random\nimport janestreet","916bfe77":"SEED= 9899\nrandom.seed(SEED)\nnp.random.seed(SEED)","f9757b59":"train = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/train.csv\")\nexample_test = pd.read_csv('..\/input\/jane-street-market-prediction\/example_test.csv')\nsample_prediction_df = pd.read_csv('..\/input\/jane-street-market-prediction\/example_sample_submission.csv')","005d5c49":"train.head(5)","99bc434b":"def eda(data):\n    print(\"----------Top-5- Record----------\")\n    print(data.head(5))\n    print(\"-----------Information-----------\")\n    print(data.info())\n    print(\"-----------Data Types-----------\")\n    print(data.dtypes)\n    print(\"----------Missing value-----------\")\n    print(data.isnull().sum())\n    print(\"----------Null value-----------\")\n    print(data.isna().sum())\n    print(\"----------Shape of Data----------\")\n    print(data.shape)\n\ndef graph_insight(data):\n    print(set(data.dtypes.tolist()))\n    df_num = data.select_dtypes(include = ['float64', 'int64'])\n    df_num.hist(figsize=(16, 16), bins=50, xlabelsize=8, ylabelsize=8);\n    \ndef drop_duplicate(data, subset):\n    print('Before drop shape:', data.shape)\n    before = data.shape[0]\n    data.drop_duplicates(subset,keep='first', inplace=True) #subset is list where you have to put all column for duplicate check\n    data.reset_index(drop=True, inplace=True)\n    print('After drop shape:', data.shape)\n    after = data.shape[0]\n    print('Total Duplicate:', before-after)","7a0f5df1":"graph_insight(train)","eedc6bf7":"eda(train)","4d8e0012":"train['resp_1'] = (((train['resp_1'].values)*train['weight']) > 0).astype(int)\ntrain['resp_2'] = (((train['resp_2'].values)*train['weight']) > 0).astype(int)\ntrain['resp_3'] = (((train['resp_3'].values)*train['weight']) > 0).astype(int)\ntrain['resp_4'] = (((train['resp_4'].values)*train['weight']) > 0).astype(int)","c363e05a":"train = train[train['weight'] != 0]\n\ntrain = train.query('date > 85').reset_index(drop = True) \n\ntrain = train.astype({c: np.float32 for c in train.select_dtypes(include='float64').columns}) #limit memory use\n\ntrain['action'] = ((train['weight'].values * train['resp'].values) > 0).astype('int')\n\ntrain.fillna(train.mean(),inplace=True)\n\nfeatures = [c for c in train.columns if 'feature' in c]\n\ndf_train = train.sample(frac=0.8, random_state=0)\ndf_valid = train.drop(df_train.index)\n\n\nX_train = df_train.loc[:, df_train.columns.str.contains('feature')]\nX_valid = df_valid.loc[:, df_valid.columns.str.contains('feature')]\ny_train = df_train['action']\ny_valid = df_valid['action']\n","42b16017":"len(features)","c0df8c5f":"feature = 'feature_11'\nsns.lmplot(\n    x=feature, y=\"action\", hue=\"feature_0\", col=\"feature_0\",\n    data=df_train, scatter_kws={\"edgecolor\": 'w'}, col_wrap=3, height=4,\n);","43e521bf":"corrmat = X_train.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(20,20))\n#plot heat map\ng=sns.heatmap(X_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n#corrmat.to_csv('correlation.csv')","e3484db4":"import xgboost as xgb\nprint(\"XGBoost version:\", xgb.__version__)","cd48f040":"selected_features = [\n'feature_0','feature_1','feature_3','feature_6','feature_20','feature_27','feature_31','feature_37','feature_39','feature_41','feature_42','feature_43','feature_44','feature_45','feature_60','feature_62','feature_83','feature_107']","6c7d7366":"clf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.01,\n    gamma = 0.3,\n    min_child_weight=5,\n    random_state=SEED,\n    subsample=0.8, \n    colsample_bytree= 0.8,\n    eval_metric = \"error\",\n    use_label_encoder=False,\n    scale_pos_weight=1,\n    nthread=4,\n    tree_method='gpu_hist'  # THE MAGICAL PARAMETER\n)\n%time clf.fit(X_train[selected_features], y_train)\n","389dce69":"import time\nfrom tqdm.notebook import tqdm","27586f5d":"TRAINING = True\n\nstart_time = time.time()\n\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\n# count = 0\nfor (test_df, sample_prediction_df) in iter_test:\n    if test_df['weight'].item() > 0:\n        X_test = test_df.loc[:, features]\n        X_test.fillna(X_test.mean(),inplace=True)\n        select_X_test = X_test[selected_features]\n        y_preds = clf.predict(select_X_test)\n        sample_prediction_df.action = y_preds.astype(int)\n    else:\n        sample_prediction_df.action = 0\n    env.predict(sample_prediction_df)\n        \nprint(f\"took: {time.time() - start_time} seconds\")","970f7640":"The feature selection by finding threshold for the feature importance and then removing the correlated features helps to increase the score from 0.5635 to 0.5695.","944762e8":"feature_0 does not  seem to have any impact on the results as being the only categorical variable.","46e106e3":"the below features are selected after removing the correlared features from the ones with top feature importance scores.","ef72a636":"There are many rounds of optimizing the XGBoost parameters and the ones in baseline and clf have the optimized parameters only."}}