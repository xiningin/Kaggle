{"cell_type":{"4f13031d":"code","041fa506":"code","f9a38ac8":"code","2c4a8b21":"code","e8f7c6bb":"code","b30b9391":"code","4cffb990":"code","17d0b57c":"code","eca5bb76":"code","03deba32":"code","6f0e4cab":"code","33fa4c72":"code","a0d8d1b6":"code","a15452fa":"code","78f312b3":"markdown","79c6b70e":"markdown"},"source":{"4f13031d":"# Import libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport category_encoders as ce\nimport timeit\nimport category_encoders\nimport os\nfrom math import sqrt\nfrom sklearn.preprocessing import OneHotEncoder, Imputer, StandardScaler, MinMaxScaler, LabelEncoder, normalize\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge\nfrom sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV,  cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.scorer import make_scorer\nfrom sklearn_pandas import CategoricalImputer\nfrom tpot import TPOTRegressor\n\n%matplotlib inline\n\npd.set_option('display.max_rows', 20)\npd.set_option('display.max_columns', 300)","041fa506":"df = pd.read_csv(\"..\/input\/train.csv\")\nprint(df.head())","f9a38ac8":"print(df.info())\nprint(df.describe())","2c4a8b21":"# break into X and y dataframes\nX = df.reindex(columns=[x for x in df.columns.values if x != 'SalePrice'])        # separate out X\ny = df.reindex(columns=['SalePrice'])   # separate out y\ny = np.ravel(y)                     # flatten the y array\n\n# make list of numeric and string columns\nnumeric_cols = [] # could still have ordinal data\nstring_cols = []  # could have ordinal or nominal data\n\nfor col in X.columns:\n    if (X.dtypes[col] == np.int64 or X.dtypes[col] == np.int32 or X.dtypes[col] == np.float64):\n        numeric_cols.append(col)      # True integer or float columns\n    \n    if (X.dtypes[col] == np.object):  # Nominal and ordinal columns\n        string_cols.append(col)","e8f7c6bb":"print(X[string_cols].head(2))","b30b9391":"# impute missing values for string columns using sklearnpandas CategoricalImputer for string data\n# s_imputer = CategoricalImputer(strategy=\"fixed_value\", replacement=\"missing\") \n# use above line as soon as sklearn-pandas updated\n# s_imputer = CategoricalImputer()\nX_string = X[string_cols]\n# print(type(X_string))\n# X_string = (s_imputer.fit_transform(X_string)\n\n# or X_string = X_string.apply(s_imputer.fit_transform)\n\n# X_string = pd.DataFrame(X_string, columns = string_cols)\nX_string = X_string.fillna(\"missing\")","4cffb990":"# encode the X columns string values as integers\nX_string = X_string.apply(LabelEncoder().fit_transform)  ","17d0b57c":"print(X.head(2))","eca5bb76":"# imputing missing values with most freqent values for numeric columns\nn_imputer = Imputer(missing_values='NaN', copy = True, strategy = 'most_frequent') # imputing with most frequent because some of these numeric columns are ordinal\n\nX_numeric = X[numeric_cols]\nX_numeric = n_imputer.fit_transform(X_numeric)\nX_numeric = pd.DataFrame(X_numeric, columns = numeric_cols)","03deba32":"# add the string and numeric dataframes back together\nX = pd.concat([X_numeric, X_string], axis=1, join_axes=[X_numeric.index])","6f0e4cab":"X.info()","33fa4c72":"# convert to numpy array so that if gets XGBoost algorithm doesn't throw \n# ValueError: feature_names mismatch: ...\n# see https:\/\/github.com\/dmlc\/xgboost\/issues\/2334\nX = X.values","a0d8d1b6":"# split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, random_state = 55)","a15452fa":"# Make a custom metric function for TPOT\n# Root mean squared logarithmic error is how Kaggle scores this task\n# Can't use custom scorer with n_jobs > 1.  Known issue.\n\n# def custom_rmsle(y_true, y_pred):\n#     return np.sqrt(np.mean((np.log(1 + y_pred) - np.log(1 + y_true))**2))\n\n# Make a custom scorer from the custom metric function\n# rmsle = make_scorer(custom_rmsle, greater_is_better=False)\n\n# Number of pipelines is very small below so that we can quickly commit on Kaggle\n\n# instantiate tpot \ntpot = TPOTRegressor(verbosity=3,  \n                    random_state=55, \n                    #scoring=rmsle,\n                    periodic_checkpoint_folder=\"intermediate_results\",\n                    n_jobs=-1, \n                    warm_start = True,\n                    generations=20, \n                    population_size=80,\n                    early_stop=8)\ntimes = []\nscores = []\nwinning_pipes = []\n\n# run 2 iterations\nfor x in range(1):\n    start_time = timeit.default_timer()\n    tpot.fit(X_train, y_train)\n    elapsed = timeit.default_timer() - start_time\n    times.append(elapsed)\n    winning_pipes.append(tpot.fitted_pipeline_)\n    scores.append(tpot.score(X_test, y_test))\n    tpot.export('tpot_ames.py')\n\n# output results\ntimes = [time\/60 for time in times]\nprint('Times:', times)\nprint('Scores:', scores)   \nprint('Winning pipelines:', winning_pipes)","78f312b3":"# TPOT Automated ML Exploration with Ames Housing Regression\n## By Jeff Hale\n\nLet's see how TPOT does with a regression task with minimal data preparation. See my [Medium article on TPOT](https:\/\/medium.com\/p\/4c063b3e5de9\/) for more information.","79c6b70e":"## Setup\nLet's import the libraries and methods we'll need and set some options to make data and charts display nicely."}}