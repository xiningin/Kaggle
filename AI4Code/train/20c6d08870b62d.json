{"cell_type":{"05c0151b":"code","fdbce761":"code","5a0b7826":"code","969d91dd":"code","1aa25425":"code","9c9c8949":"code","f713fe4d":"code","b92cfa5e":"code","ea2cf9a3":"code","cfd1a9a6":"code","4df6e4aa":"code","34d2d03c":"code","a09c0618":"code","f137adff":"code","ce9f20ff":"code","e3ebb323":"code","0f6193bc":"code","3f0e6534":"code","60bdd99f":"code","ea0ddd7e":"code","9438bba7":"markdown","bcee9806":"markdown","b00f98da":"markdown","e8afec9b":"markdown","2a343842":"markdown","ef538c61":"markdown","ef1c7eb5":"markdown","fe32b9a4":"markdown","95b9fae7":"markdown","8b298033":"markdown","54cfcfab":"markdown","0650c752":"markdown"},"source":{"05c0151b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt  \nfrom sklearn.model_selection import train_test_split \nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fdbce761":"fish=pd.read_csv('..\/input\/fish-market\/Fish.csv')\nfish.head()","5a0b7826":"fish.columns","969d91dd":"fish.shape","1aa25425":"fish.describe()","9c9c8949":"fish.info()","f713fe4d":"fish.isnull().sum()","b92cfa5e":"duplicate=fish.duplicated()\nprint(duplicate.sum())\n","ea2cf9a3":"fig, ax = plt.subplots(ncols=6, figsize=(20, 4))\n\nvariables = fish.columns[1:]\nfor i, var in enumerate(variables):\n    sns.histplot(data=fish, x=var, ax=ax[i])\nplt.show()","cfd1a9a6":"Fish = pd.get_dummies(fish[\"Species\"])\nFish.shape\n","4df6e4aa":"fish = pd.concat([fish, Fish], axis = 1)\nfish= fish.drop(\"Species\", axis =1)\nfish.head()\n","34d2d03c":"fish.shape","a09c0618":"X=fish.drop('Weight',axis=1)\ny = fish['Weight']\nX.shape,y.shape","f137adff":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","ce9f20ff":"plt.figure(figsize=(12, 8))\ncorr_mx = fish.corr() # correlation matrix\nmatrix = np.triu(corr_mx) # take lower correlation matrix\nsns.heatmap(corr_mx, mask=matrix,annot=True)","e3ebb323":"corr_mx[\"Weight\"].sort_values(ascending=False)","0f6193bc":"from sklearn.neighbors import KNeighborsRegressor\ntrain_score_arr= []\nval_score_arr = []\nk_values=np.arange(1,10,1)\n\nfor k in k_values:\n    \n    model = KNeighborsRegressor(n_neighbors=k)\n    model.fit(X_train,y_train)  \n    \n    train_score = model.score(X_train, y_train) \n    train_score_arr.append(train_score*100)\n    \n    val_score = model.score(X_test, y_test)\n    val_score_arr.append(val_score*100)\n    \n    print(\"k=%d, train_accuracy=%.2f%%, test_accuracy=%.2f%%\" % (k, train_score * 100, val_score*100))\n    ","3f0e6534":"plt.figure(figsize=(10,6))\nplt.title('Accuracy vs knearest neighbour using default parameters')\nplt.plot(k_values,train_score_arr,color='green',marker='o',linestyle='dashed',linewidth=2,label='training accuracy')\nplt.plot(k_values,val_score_arr,'b',marker='^',linestyle='dashed',linewidth=2,label='validation accuracy')\nplt.legend()","60bdd99f":"train_score_d = []\nval_score_d = []\nk_values=np.arange(1,10,1)\n\nfor k in k_values:\n    \n    model = KNeighborsRegressor(n_neighbors=k,weights='distance')\n    model.fit(X_train,y_train)  \n    \n    train_score = model.score(X_train, y_train) \n    train_score_d.append(train_score*100)\n    \n    val_score = model.score(X_test, y_test)\n    val_score_d.append(val_score*100)\n    \n    print(\"k=%d, train_accuracy=%.2f%%, test_accuracy=%.2f%%\" % (k, train_score * 100, val_score*100))","ea0ddd7e":"plt.figure(figsize=(10,6))\nplt.title('Accuracy vs knearest neighbours by keeping weights as distance')\nplt.plot(k_values,train_score_d,color='green',marker='o',linestyle='dashed',linewidth=2,label='training accuracy')\nplt.plot(k_values,val_score_d,'b',marker='^',linestyle='dashed',linewidth=2,label='validation accuracy')\nplt.legend()","9438bba7":"## KNNRegressor keeping weights as distance","bcee9806":"### Handling Missing values","b00f98da":"For k=2 it is giving best accuracy. Distance weights performs better than uniform better.","e8afec9b":"In above graph we can see that best suitable k value is 1. After k=2 accuracy is decreasing gradually.","2a343842":"There are no null values.","ef538c61":"### Handling duplicate values","ef1c7eb5":"## KNNRegressor using default parameters","fe32b9a4":"### Splitting dataset into x and y","95b9fae7":"No duplicate values ","8b298033":"Basic EDA","54cfcfab":"### Handling categorical column by One Hot Encoding","0650c752":"### Correlation between all the attributes"}}