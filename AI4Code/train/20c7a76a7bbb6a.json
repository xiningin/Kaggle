{"cell_type":{"4ffbf93b":"code","8b920eb6":"code","016d0365":"code","c211f267":"code","7b546e53":"code","67fe91ba":"code","16d0d2a7":"code","7cd97990":"code","7c5d724c":"code","65f700ac":"code","27775fee":"code","2dc7206a":"code","d537645d":"code","2a197476":"code","5a70c28e":"code","fc928c19":"code","0a945605":"code","4a7430cf":"code","814033f4":"code","e9b65cd3":"code","08d26871":"code","9d44ac96":"code","f452456d":"code","ce4f3e67":"code","f43b53c4":"code","d60d4fd1":"code","498dbf14":"code","01d01489":"code","f43024e4":"code","952b4914":"code","f0d84ccd":"code","813080c3":"code","5528c785":"code","371516ab":"code","88f2bc58":"code","b260120e":"markdown","17bec928":"markdown","06e3c61d":"markdown","a40dc262":"markdown","ee93f29b":"markdown","2fcd2151":"markdown","8da8857e":"markdown","8e1d23dd":"markdown","35c99c34":"markdown","2189ff6b":"markdown","8373434f":"markdown","eb6e03ae":"markdown","db6cebeb":"markdown","adad156e":"markdown","534e25ec":"markdown","8a347234":"markdown","f9135f80":"markdown","c7123234":"markdown","b7252c3b":"markdown","5418af5a":"markdown","3017aa92":"markdown","d752b462":"markdown","5fae68a1":"markdown","20bd7d45":"markdown","192eda04":"markdown","95f339b4":"markdown","e88dae68":"markdown","969c6215":"markdown","116e58f2":"markdown","843e52d2":"markdown","d616e946":"markdown","b803ff53":"markdown","f1e6824c":"markdown","ab864aea":"markdown"},"source":{"4ffbf93b":"#Including Libraries\nfrom pandas_profiling import ProfileReport\nimport pandas as pd\nimport numpy as np\n\n#Note you may need to install pandas_profiling - Run anaconda prompt as admin and run:\n#'conda install -c conda-forge pandas-profiling'","8b920eb6":"# google colab things\n## from google.colab import files\n## files.upload()\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        data = os.path.join(dirname, filename)\n#Importing Data \ndf = pd.read_csv(data)","016d0365":"df.head()","c211f267":"#Generating Profile Report\nprofile = ProfileReport(df)\n\n#Outputing Profile Report to html file in current directory\nprofile","7b546e53":"df['default.payment.next.month'].hist()","67fe91ba":"PP = (df['PAY_AMT1'] + df['PAY_AMT2'] + \\\n                    df['PAY_AMT3'] + df['PAY_AMT4'] + \\\n                    df['PAY_AMT5'] + df['PAY_AMT6']) \/ \\\n            (df['BILL_AMT1'] + df['BILL_AMT2'] + \\\n                    df['BILL_AMT4'] + df['BILL_AMT3'] + \\\n                    df['BILL_AMT5'] + df['BILL_AMT6'])\n\ndf_PP = pd.DataFrame(data=PP, columns=[\"PP\"])\n\n# Correct outliers.\ndf_PP.loc[(df_PP.PP > 1), 'PP']=1\ndf_PP.loc[(df_PP.PP < 0), 'PP']=0\n\n# Correct divide by zero error. Indicates that they are paying, but not being billed.\ndf_PP = df_PP.fillna(1)\ndf_PP.head()","16d0d2a7":"# Ratio of a customers average Bill amount per month and their Credit Limit.\nAVG_BILL_CL = ((df['BILL_AMT1'] + \\\n                df['BILL_AMT2'] + \\\n                df['BILL_AMT3'] + \\\n                df['BILL_AMT4'] + \\\n                df['BILL_AMT5'] + \\\n                df['BILL_AMT6']) \/ 6) \/ \\\n                df['LIMIT_BAL']\n\n# Create dataframe.\ndf_AVG_BILL_CL = pd.DataFrame(data=AVG_BILL_CL, columns=[\"AVG_BILL_CL\"])\ndf_AVG_BILL_CL.head()","7cd97990":"# Ratio of a customers average Payment amount per month and their Credit Limit.\n\nAVG_PAY_CL = ((df['PAY_AMT1'] + \\\n               df['PAY_AMT2'] + \\\n               df['PAY_AMT3'] + \\\n               df['PAY_AMT4'] + \\\n               df['PAY_AMT5'] + \\\n               df['PAY_AMT6']) \/ 6) \/ \\\n               df['LIMIT_BAL']\n\n# Create dataframe.\ndf_AVG_PAY_CL = pd.DataFrame(data=AVG_PAY_CL, columns=[\"AVG_PAY_CL\"])\ndf_AVG_PAY_CL.head()","7c5d724c":"NUM_DELAYS = df.apply(lambda row: \\\n                        (1 if row.PAY_0 > 0 else 0) + \\\n                        (1 if row.PAY_2 > 0 else 0) + \\\n                        (1 if row.PAY_3 > 0 else 0) + \\\n                        (1 if row.PAY_4 > 0 else 0) + \\\n                        (1 if row.PAY_5 > 0 else 0) + \\\n                        (1 if row.PAY_6 > 0 else 0) \\\n                    , axis=1)\n\n# Create dataframe.\ndf_NUM_DELAYS = pd.DataFrame(data=NUM_DELAYS, columns=[\"NUM_DELAYS\"])\ndf_NUM_DELAYS.head()","65f700ac":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_NUM_DELAYS_TARGET = pd.concat([df['default.payment.next.month'],df_NUM_DELAYS], axis=1)\n\ndf_default_one = df_NUM_DELAYS_TARGET[df_NUM_DELAYS_TARGET['default.payment.next.month']== 1]\nsns.distplot(df_default_one['NUM_DELAYS'], kde=False, label=\"Default\")\n\ndf_default_zero = df_NUM_DELAYS_TARGET[df_NUM_DELAYS_TARGET['default.payment.next.month']== 0]\nsns.distplot(df_default_zero['NUM_DELAYS'], kde=False, label=\"Not Default\")\n\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Number of Delays for Defaults and Non-Defaulting')\nplt.xlabel('NUM_DELAYS')\nplt.ylabel('FREQUENCY')","27775fee":"df_NUM_DELAYS_not_zero = df_NUM_DELAYS_TARGET[df_NUM_DELAYS_TARGET['NUM_DELAYS'] > 0]\n\ndf_default_one = df_NUM_DELAYS_not_zero[df_NUM_DELAYS_not_zero['default.payment.next.month']== 1]\nsns.distplot(df_default_one['NUM_DELAYS'], kde=False, label=\"Default\")\n\ndf_default_zero = df_NUM_DELAYS_not_zero[df_NUM_DELAYS_not_zero['default.payment.next.month']== 0]\nsns.distplot(df_default_zero['NUM_DELAYS'], kde=False, label=\"Not Default\")\n\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Number of Delays for Defaults and Non-Defaulting')\nplt.xlabel('NUM_DELAYS')\nplt.ylabel('FREQUENCY')","2dc7206a":"#Preping Education Feature for One-Hot\n\neducation_list = list(np.linspace(0,0,len(df)))\ntemp_list = list(df['EDUCATION'].values)\nfor i in range(len(df)):\n    if temp_list[i] == 1:\n        education_list[i] = 'Graduate'\n    elif temp_list[i] == 2:\n        education_list[i] = 'Undergraduate'\n    elif temp_list[i] == 3:\n        education_list[i] = 'High School'\n    elif temp_list[i] == 4:\n        education_list[i] = 'Others'\n    else:\n        # 5 and 6 represent \"Unknown\".\n        education_list[i] = 'Unknown'\neducation_list = pd.DataFrame(data=education_list, columns=[\"EDUCATION\"])\n\n# One Hot Encode Education, Marriage and Sex\ndf_education_one_hot = pd.get_dummies(education_list['EDUCATION'], prefix=\"EDUCATION\")\ndf_marriage_one_hot = pd.get_dummies(df['MARRIAGE'], prefix=\"MARRIAGE\")\ndf_sex_one_hot = pd.get_dummies(df['SEX'], prefix=\"SEX\")","d537645d":"#Making a Quantile Cut for Bucketing of LIMIT BAL\ncut_df = pd.qcut(df['LIMIT_BAL'], q=3)\ndf_quantile_LimBal = pd.DataFrame(data=cut_df, columns=[\"LIMIT_BAL\"])\n\ndf_limbal = pd.get_dummies(df_quantile_LimBal['LIMIT_BAL'], prefix=\"LIMIT_BAL\")\n\ndisplay(df_limbal.head())\n\n#Bucketing Age Feature by Manual Interval Selection\nage_list = list(np.linspace(0,0,len(df)))\ntemp_list = list(df['AGE'].values)\nfor i in range(len(df)):\n    if temp_list[i] >= 20 and temp_list[i] < 30:\n        age_list[i] = '[20-30)'\n    elif temp_list[i] >= 30 and temp_list[i] < 40:\n        age_list[i] = '[30-40)'\n    elif temp_list[i] >= 40 and temp_list[i] < 50:\n        age_list[i] = '[40-50)'\n    elif temp_list[i] >= 50 and temp_list[i] < 60:\n        age_list[i] = '[50-60)'\n    elif temp_list[i] >= 60 and temp_list[i] < 70:\n        age_list[i] = '[60-70)'\n    else:\n        age_list[i] = '[70-Death)'\n\ndf_age = pd.DataFrame(data=age_list, columns=[\"AGE\"])\ndf_age = pd.get_dummies(df_age['AGE'], prefix=\"AGE\")\n\ndf_age.head()","2a197476":"# Calculate their bill change between each month. Normalize with their total LIMIT_BAL.\nBILL_CHANGE_1 = (df['BILL_AMT1'] - df['BILL_AMT2'])\/df['LIMIT_BAL']\nBILL_CHANGE_2 = (df['BILL_AMT2'] - df['BILL_AMT3'])\/df['LIMIT_BAL']\nBILL_CHANGE_3 = (df['BILL_AMT3'] - df['BILL_AMT4'])\/df['LIMIT_BAL']\nBILL_CHANGE_4 = (df['BILL_AMT4'] - df['BILL_AMT5'])\/df['LIMIT_BAL']\nBILL_CHANGE_5 = (df['BILL_AMT5'] - df['BILL_AMT6'])\/df['LIMIT_BAL']\n\n# Create dataframe.\ndf_BILL_CHANGE_1 = pd.DataFrame(data=BILL_CHANGE_1, columns=[\"BILL_CHANGE_1\"])\ndf_BILL_CHANGE_2 = pd.DataFrame(data=BILL_CHANGE_2, columns=[\"BILL_CHANGE_2\"])\ndf_BILL_CHANGE_3 = pd.DataFrame(data=BILL_CHANGE_3, columns=[\"BILL_CHANGE_3\"])\ndf_BILL_CHANGE_4 = pd.DataFrame(data=BILL_CHANGE_4, columns=[\"BILL_CHANGE_4\"])\ndf_BILL_CHANGE_5 = pd.DataFrame(data=BILL_CHANGE_5, columns=[\"BILL_CHANGE_5\"])\n\ndf_BILL_CHANGE = pd.concat([df_BILL_CHANGE_1,df_BILL_CHANGE_2,df_BILL_CHANGE_3,df_BILL_CHANGE_4,df_BILL_CHANGE_5], axis=1)\n\ndf_BILL_CHANGE.head()","5a70c28e":"# Calculate their bill change between each month. Normalize with their total LIMIT_BAL.\nPAY_CHANGE_1 = (df['PAY_AMT1'] - df['PAY_AMT2'])\/df['LIMIT_BAL']\nPAY_CHANGE_2 = (df['PAY_AMT2'] - df['PAY_AMT3'])\/df['LIMIT_BAL']\nPAY_CHANGE_3 = (df['PAY_AMT3'] - df['PAY_AMT4'])\/df['LIMIT_BAL']\nPAY_CHANGE_4 = (df['PAY_AMT4'] - df['PAY_AMT5'])\/df['LIMIT_BAL']\nPAY_CHANGE_5 = (df['PAY_AMT5'] - df['PAY_AMT6'])\/df['LIMIT_BAL']\n\n# Create dataframe.\ndf_PAY_CHANGE_1 = pd.DataFrame(data=PAY_CHANGE_1, columns=[\"PAY_CHANGE_1\"])\ndf_PAY_CHANGE_2 = pd.DataFrame(data=PAY_CHANGE_2, columns=[\"PAY_CHANGE_2\"])\ndf_PAY_CHANGE_3 = pd.DataFrame(data=PAY_CHANGE_3, columns=[\"PAY_CHANGE_3\"])\ndf_PAY_CHANGE_4 = pd.DataFrame(data=PAY_CHANGE_4, columns=[\"PAY_CHANGE_4\"])\ndf_PAY_CHANGE_5 = pd.DataFrame(data=PAY_CHANGE_5, columns=[\"PAY_CHANGEbb_5\"])\n\ndf_PAY_CHANGE = pd.concat([df_PAY_CHANGE_1,df_PAY_CHANGE_2,df_PAY_CHANGE_3,df_PAY_CHANGE_4,df_PAY_CHANGE_5], axis=1)\n\ndf_PAY_CHANGE.head()","fc928c19":"#Creating Current_Delay Feature\nindicies = [list(df.columns).index(col) for col in list(df.columns) if 'PAY_' in col and len(col) == 5]\nPAY_data = df[list(df.columns)[indicies[0]:indicies[-1]+1]]\n\nCurDelay_list = list(np.linspace(0,0,len(df)))\n\nfor i in range(len(df)):\n    if PAY_data.iloc[i][0] >= 1:\n        CurDelay_list[i] = 1\n        continue\n    elif PAY_data.iloc[i][1] >= 2:\n        CurDelay_list[i] = 1\n        continue\n    elif PAY_data.iloc[i][2] >= 3:\n        CurDelay_list[i] = 1\n        continue\n    elif PAY_data.iloc[i][3] >= 4:\n        CurDelay_list[i] = 1\n        continue\n    elif PAY_data.iloc[i][4] >= 5:\n        CurDelay_list[i] = 1\n        continue\n    elif PAY_data.iloc[i][5] >= 6:\n        CurDelay_list[i] = 1\n        continue\n\ndf_CurDelay = pd.DataFrame(CurDelay_list, columns=['CUR_DELAY'])","0a945605":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\ndf_CUR_DELAY_TARGET = pd.concat([df['default.payment.next.month'],df_CurDelay], axis=1)\n\ndf_default_one = df_CUR_DELAY_TARGET[df_CUR_DELAY_TARGET['default.payment.next.month']== 1]\nsns.distplot(df_default_one['CUR_DELAY'], kde=False, label=\"Default\")\n\ndf_default_zero = df_CUR_DELAY_TARGET[df_CUR_DELAY_TARGET['default.payment.next.month']== 0]\nsns.distplot(df_default_zero['CUR_DELAY'], kde=False, label=\"Not Default\")\n\n# Plot formatting\nplt.legend(prop={'size': 12})\nplt.title('Number of Current Delay for Defaults and Non-Defaulting')\nplt.xlabel('CUR_DELAY')\nplt.ylabel('FREQUENCY')","4a7430cf":"df_data = pd.concat([df_PP, df_AVG_BILL_CL, df_AVG_PAY_CL, df_NUM_DELAYS, df_education_one_hot, df_marriage_one_hot, df_sex_one_hot, df_limbal, df_age, df_BILL_CHANGE, df_PAY_CHANGE, df_CurDelay, df['default.payment.next.month']], axis=1)","814033f4":"column_mapping = {}\nfor col in list(df_data.columns):\n\n    new_col = col.replace(']', ')')\n    new_col = new_col.replace('[', '(')\n    new_col = new_col.replace(', ', '-')\n    new_col = new_col.replace(' ', '')\n    column_mapping[col] = new_col\n\ndf_data = df_data.rename(columns=column_mapping)","e9b65cd3":"import pandas as pd\nfrom sklearn import preprocessing\n\nx = df_data.values #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf_data = pd.DataFrame(x_scaled, columns=df_data.columns)","08d26871":"# Display all columns.\npd.set_option('display.max_columns', 999)\ndisplay(df_data.describe())\n\ndisplay(df_data.head())","9d44ac96":"#Generating Profile Report\nprofile = ProfileReport(df_data)\n\n#Outputing Profile Report to html file in current directory\nprofile","f452456d":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import confusion_matrix","ce4f3e67":"# Split data into train, validation, and testing set.\n\nX = df_data.drop('default.payment.next.month', axis=1)\nY = df_data['default.payment.next.month']\n\nseed = 7\ntest_size = 0.3\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\nX_validation, X_test, y_validation, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=seed)","f43b53c4":"type(X_train)","d60d4fd1":"X_train.head()\n[4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,34]","498dbf14":"from imblearn.over_sampling import SMOTENC\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\n# Random Undersampling\nrus = RandomUnderSampler(random_state=77)\nrus_x_train, rus_y_train = rus.fit_resample(X_train, y_train)\n\n# Random Oversampling\nros = RandomOverSampler(random_state=77)\nros_x_train, ros_y_train = ros.fit_resample(X_train, y_train)\n\n# SMOTE\n## Note: We have categorical features.\nsm = SMOTENC(random_state=77, categorical_features=[4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,34])\nsm_x_train, sm_y_train = sm.fit_resample(X_train, y_train)\n\n# Put all the different datasets into a single list so we can iterate over it.\ntrain_sample_labels = [\"Unbalanced\", \"Undersample\", \"Oversample\", \"SMOTE\"]\ntrain_samples = [(X_train, y_train), (rus_x_train, rus_y_train), (ros_x_train, ros_y_train), (sm_x_train, sm_y_train)]","01d01489":"def test_models(model_choice, param_dict, model_name):\n    ensemble = []\n\n    n = 0\n    for sample in train_samples: \n        # Train model.\n        model = model_choice(**param_dict)\n        model.fit(sample[0], sample[1])\n\n        # Make predictions on validation data.\n        preds = model.predict(X_validation)\n        preds_prob = model.predict_proba(X_validation)[:,1]\n\n        # Calculate metrics (auc, f1, accuracy, precision)\n        fpr, tpr, thresholds = roc_curve(y_validation, preds_prob)\n        roc_auc = auc(fpr, tpr)\n        acc = accuracy_score(y_validation, preds)\n\n        tn, fp, fn, tp = confusion_matrix(y_validation, preds).ravel()\n        precision = tp\/(tp+fp)\n        recall = tp\/(tp+fn)\n\n        f1 = (1 + 1) * (precision*recall)\/(1*precision+recall)\n        f2 = (1 + 2) * (precision*recall)\/(2*precision+recall)\n        f3 = (1 + 3) * (precision*recall)\/(3*precision+recall)\n\n        p, r, thresholds = precision_recall_curve(y_validation, preds_prob)\n        f1_sc = [] \n        for i, threshold in enumerate(thresholds):\n            f = 2*(p[i]*r[i])\/(p[i]+r[i])\n\n            f1_sc.append((f, threshold))\n\n        # Find maximum f1. Set threshold to this.\n        max_f = max(f1_sc, key=lambda item:item[0])\n\n        ensemble.append([roc_auc, max_f, f1, f2, f3, precision, recall, acc, model_name, train_sample_labels[n], model])\n\n        # Create plot.\n        label='%s (AUC = %0.2f)' % (train_sample_labels[n], roc_auc)\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.plot(fpr, tpr, label=label, color=np.random.rand(3))\n        n += 1\n\n    # Show plot.\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(f'{model_name} ROC Curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    # Show all sampling methods metrics.\n    return ensemble","f43024e4":"from sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier","952b4914":"xgb_params =  {'objective':'binary:logistic',\n                  'learning_rate':0.01,\n                  'max_depth':6,\n                  'min_child_weight':14,\n                  'n_estimators':100,\n                  'nthreads':1, \n                  'subsample':1.0}\n\nrf_params = {'bootstrap':'True',\n              'criterion':'gini',\n              'max_features':.9,\n              'min_samples_leaf':10,\n              'min_samples_split':11,\n              'n_estimators':100\n            }\n\nxt_params = {'bootstrap':'False',\n             'criterion':'entropy',\n             'max_features':0.45,\n             'min_samples_leaf':9,\n             'min_samples_split':5,\n             'n_estimators':100\n            }\n\nlr_params = {}\n\nmlp_params = {}\n\nmodels_to_test = [(xgb.XGBClassifier, xgb_params, \"XGBoost\"), \n                  (RandomForestClassifier, rf_params, \"RandomForest\"),\n                 (ExtraTreesClassifier, xt_params, \"ExtraTrees\"),\n                 (LogisticRegression, lr_params, \"LogisticRegression\"),\n                 (MLPClassifier, mlp_params, \"MLP\")]\n\ntotal_ensemble = []\nfor m in models_to_test:\n    total_ensemble.append(test_models(m[0], m[1], m[2]))","f0d84ccd":"# Collapse list.\nte = [item for sublist in total_ensemble for item in sublist]\n\n# Sort on f1 score.\nte.sort(key=lambda x: x[2], reverse=True)\n\nfor classifier in te:\n    print(\"\\nModel: {}\\n\\tSampling Method: {}\\n\\tAUC: {}\\n\\tMax f1 & thresh: {}\\n\\tf1: {}\\n\\tf2: {}\\n\\tf3: {}\\n\\tPrecision: {}\\n\\tRecall: {}\\n\\tAccuracy: {}\".format(classifier[8], classifier[9], classifier[0], classifier[1], classifier[2],classifier[3],classifier[4],classifier[5],classifier[6],classifier[7],classifier[8]))","813080c3":"# Grab threshold value that maximizes the f1 score when oversampling.\nthreshold = te[0][1][1]\nmodel_name = te[0][8]\nsampling_technique = te[0][9]\n\n# Grab best model object.\nbest_model = te[0][10]\n\nprint(\"Running: {} with {}\\n\".format(model_name, sampling_technique))\n\n# Make predictions on Testing data.\npreds_prob = best_model.predict_proba(X_test)[:,1]\n\n# Create prediction using threshold that maximizes f1 score.\npreds = (preds_prob >= threshold).astype(bool)\n\n# Calculate metrics (auc, f1, accuracy, precision)\nfpr, tpr, thresholds = roc_curve(y_test, preds_prob)\nroc_auc = auc(fpr, tpr)\nacc = accuracy_score(y_test, preds)\n\ntn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\nprecision = tp\/(tp+fp)\nrecall = tp\/(tp+fn)\n\nf1 = (1 + 1) * (precision*recall)\/(1*precision+recall)\nf2 = (1 + 2) * (precision*recall)\/(2*precision+recall)\nf3 = (1 + 3) * (precision*recall)\/(3*precision+recall)\n\n# Create plot for AUC\nlabel='AUC = %0.2f)' % (roc_auc)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr, label=label, color=np.random.rand(3))\n\n# Show plot.\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(f'{model_name} ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\naverage_precision = average_precision_score(y_test, preds_prob)    \ndisp = plot_precision_recall_curve(best_model, X_test, y_test)\ndisp.ax_.set_title('2-class Precision-Recall curve: '\n               'AP={0:0.2f}'.format(average_precision))\n\n# Print Confusion Matrix.\ntn, fp, fn, tp = confusion_matrix(y_test, preds, labels=[0,1]).ravel()\nprint(\"Confusion Matrix:\")\nprint(\"-----------\")\nprint(tp,\"|\",fn)\nprint(\"-----------\")\nprint(fp,\"|\",tn)\nprint(\"-----------\\n\")\n\nprint(\"AUC: {}\\nf1: {}\\nf2: {}\\nf3: {}\\nAccuracy: {}\\nPrecision: {}\\nRecall: {}\".format(roc_auc, f1, f2, f3, acc, precision, recall))","5528c785":"import shap\nprint(shap.__version__)\n\n# May need to run the following as system admin in anaconda command prompt\n## conda install -c conda-forge shap","371516ab":"if sampling_technique == \"Unbalanced\":\n    features = train_samples[0][0]\nelif sampling_technique == \"Undersample\":\n    features = train_samples[1][0]\nelif sampling_technique == \"Undersample\":\n    features = train_samples[1][0]\nelse:\n    features = train_samples[2][0]\n\nshap_values = shap.TreeExplainer(best_model).shap_values(features)\nshap.summary_plot(shap_values, features, plot_type=\"bar\")","88f2bc58":"shap.summary_plot(shap_values, features) # Change to Red and Green","b260120e":"## BILL_CHANGE\n\nCalculate their bill change between each month. Normalize with their total LIMIT_BAL.\n\n## Why?\n\nIf they suddenly owe more, they might be more likely to default.\n\n> NOTE: Higher values means that they spent more than the previous month.","17bec928":"## Sampling techniques\nOur data-set is unbalanced, so we can try some different sampling techniques to combat this issue.\n\n* Undersampling\n* Oversampling\n* SMOTE\n\nBelow we are generating different data sets with each of the above sampling methods.","06e3c61d":"According to this graph, if a customer has never defaulted, they are very likely to not default again.","a40dc262":"# Feature Importance:\n\n### SHAP (SHapley Additive exPlanations)\nSHAP measures the impact of variables taking into account the interaction with other variables. \n\nReference: \n* https:\/\/towardsdatascience.com\/explain-your-model-with-the-shap-values-bc36aac4de3d\n\n* https:\/\/blog.datascienceheroes.com\/how-to-interpret-shap-values-in-r\/Which features are more important to XGBoost?","ee93f29b":"# Modeling","2fcd2151":"# Exploratory Data Analysis","8da8857e":"## Build Model that Maximizes f1 score. \n\nRun with the best threshold value too.","8e1d23dd":"# Predictive Analytics on Credit Card Defaults\n\n### Authors\n**Group 3**\n\nCameron Wonchoba, Matthew Tate\n\n### Date\n2\/17\/2020","35c99c34":"# Stitch DataFrames together.","2189ff6b":"### Models Tested\n* XGBoost\n* Random Forest\n* Extra Trees\n* Logisitic Regression\n* Multilayer perceptron (Artificial Neural Network)","8373434f":"###  PP (Percentage Paid)\nPercentage of total amount due over 6 months paid.\n\n#### Why?\nThis will capture the customers typical payment behavior providing a sense of \"reliablility of payee\".","eb6e03ae":"# Change Feature Names\nFeature names can't have brackets, or commas.","db6cebeb":"### Normalize Data\n\nNormalizing data is necessary for some models that use mathematical relationships between variables.\n\nMethod of Normalizing:\n* MinMaxScaler\n    * x<sub>i<\/sub> = (x<sub>i<\/sub> - x<sub>min<\/sub>) \/ (x<sub>max<\/sub> - x<sub>min<\/sub>)","adad156e":"### Investigate the Target Variable","534e25ec":"There is still predictive power when someone has more 1 or more delays, but the most predictive element is whether they have ever delayed or not.","8a347234":"### NUM_DELAYS\nNumber of times a customer delay their bill payment. \n\n##### Why?\nThis will quantify a customers willingness to pay on time. This feature tells us how many times a customer didn't exhibit perfect behaivor in the past.","f9135f80":"# Overview\n\n### Buisness Problem\nBefore we go into the buisness problem, it is important to understand what *default payments* are. According to [creditcards.com](https:\/\/www.creditcards.com\/credit-card-news\/glossary\/term-default.php), **default payments are when people fail to make a payment on debt before the due date.**  \n\nThe impacts of a default payment often are more severe for the defaultee than they are for the credit lender.  The credit lender will be required to take action against the defaultee but should be able to recoupe their loss through levies, garnishments, fees, etc.\n\nIn order to better serve the intests of the customer and the lender, predictive analytics is required to determine whether or not a given customer will default on their next monthly payment.  This will allow the lender to better understand their customers and their risk factors.\n\n##### Actionable Decision:\nEach customer will be split into two categories:\n1. Will Default\n2. Will not Default\n\nCredit Card companies can perform a \"Risk Assessment\" of their customers to identify customers who are at a higher risk of not paying back their loans.\n\nSplitting up customers into these two classes will allow Credit Card companies to identify \"high risk\" individuals and intervene in a strategic way.\n\n### Model Objective\nBased on the data, identify if a customer will default a payment or not.\n\nThis is a **Binary Classification** problem (predicting whether an individual will default a payment or not).\n\nThe following models are good for Binary Classification problems:\n* Two-Class Averaged Perceptron\n* Two-Class Decision Forest\n* Two-Class Logistic Regression\n* Two-Class Boosted Decision Tree\n* Two-Class Neural Network\n\n### Dataset Description\n\n##### Where?\nThis dataset was retrieved from [here](https:\/\/www.kaggle.com\/uciml\/default-of-credit-card-clients-dataset\/data).\n\n##### Description\n\nThis dataset contains information on default payments, demographic factors, credit data, history of payment, and bill statements of credit card clients in Taiwan from April 2005 to September 2005.\n","c7123234":"# Feature Engineering","b7252c3b":"## Exploring NUM_DELAYS and its predictive power.","5418af5a":"### AVG_BILL_CL\nRatio of a customers average Bill amount per month and normalized by their Credit Limit.\n\n##### Why?\nThis will capture how much of their credit limit each customer spends per month. This will allow us to see if the customer is spending most of the spending limit or not.","3017aa92":"### Imbalanced Target Variable\nOur target variable is imbalanced.\n\nThis affects our:\n* Evaluation Metrics\n* Sampling Techniques","d752b462":"# Bucketing\n\nThrow the following variables into buckets.\n* LIMIT_BAL\n* AGE\n\n### Why?\nSmall difference in these values do not matter. However, understanding what groups theses values fall into is important.","5fae68a1":"### Split into Train, Validation, Test","20bd7d45":"## How to read the graph above\nEvery observations training data is plotted above. \n\n**Feature importance:** Variables are ranked in descending order.\n\n**Impact:** The horizontal location shows whether the effect of that value is associated with a higher or lower prediction.\n\n**Original value:** Color shows whether that variable is high (in red) or low (in blue) for that observation.","192eda04":"If a client is in a current delay state, they are much likely to default next month.","95f339b4":"## Pandas Profiling","e88dae68":"## Evaluation Metrics:\n\nIn evaluating metrics, the decision was made to assert the most importance to the f1 score.  As we were unable to determine whether or not to weigh false negatives with more importance, we opted to treat precision and recall equally; this is best accomplished through the maximization of f1-score.\n\nGeneral Form of F-Score:\n\n`f_beta = (1 + beta^2) * (recall * precision) \/ (beta^2 * precision + recall)`\n\nThus f1 score (beta = 1):\n\n`f_1 = 2 * ((precision * recall) \/ (precision + recall))`\n\nWhere:\n\n`precision = TP \/ (TP + FP)`\n\n`recall = TP \/ (TP + FN)`\n\nIf we were to choose a beta value \">1\" then we would be asserting larger importance to maximizing recall and thus also minimizing false negatives.  A beta value of  '<1' (but '>0') would have an inverse effect: maximization of precision would be more important and thus minimization of false positives.\n\n#### In Business Terms:\n**False Positives** = Model Predicts that a given customer will default and they do not.\n\n**False Negative** = Model Predicts that a given customer will not default and they do.\n\nWithout further information on the risk\/costs that the lender will incur for a default payment, we could not justify a weighting of importance to either.","969c6215":"## PAY_CHANGE\n\nCalculate their bill change from two months ago to last month.\n\n## Why?\n\nIf they pay more, they might be more or less likely to default.\n\n> NOTE: Higher values means that they spent more than the previous month.","116e58f2":"### AVG_PAY_CL\nRatio of a customers average Payment amount per month normalized by their Credit Limit.\n\n##### Why?\nThis will capture how much of their credit limit each customer pays per month. This will allow us to see if the customer is able to pay their credit limit or not.","843e52d2":"# One-Hot-Encoding.\n\nOne-Hot-Encode \n* EDUCATION\n* MARRIAGE \n* SEX\n\n### Why?\nThese are categorical features. They can't be represented as numbers (only bit values).","d616e946":"# Conclusions\n\n## Model Choice:\n* XGBoost\n* Oversampling\n* AUC: 0.77\n* F1: 0.54\n* Accuracy: 0.79\n\n\n## Maching Learning Techniques:\n* Pandas Profiling \n* Feature Engineering\n* TPOT\n* Sci-kit learn\n* Combating Imbalanced data\n* Evaluation methods matter\n\n## Characterstics of Clients who Default (from SHAP)\n\n* They're in a current delay\n* They have a lot of previous delays.\n* They don't pay their bills in the past.\n* They spend more of their credit limit\n* They have a credit limit between 9,999-80,000","b803ff53":"<h3 align=\"left\">1) Dataset field Definition<\/h3>\n\n<h4 align=\"left\">There are 25 variables:<\/h4><br>\n1) ID: ID of each client <br>\n2) LIMIT_BAL: Amount of given credit in NT dollars (includes individual and family\/supplementary credit <br>\n3) SEX: Gender (1=male, 2=female) <br>\n4) EDUCATION: (1=graduate school, 2=university, 3=high school, 4=others, 5=unknown, 6=unknown) <br>\n5) MARRIAGE: Marital status (1=married, 2=single, 3=others) <br>\n6) AGE: Age in years <br>\n7)  PAY_0: Repayment status in September, 2005 (-1=pay duly, 1=payment delay for one month, 2=payment delay for two months, \u2026 8=payment delay for eight months, 9=payment delay for nine months and above) <br>\n8) PAY_2: Repayment status in August, 2005 (scale same as above) <br>\n9) PAY_3: Repayment status in July, 2005 (scale same as above) <br>\n10) PAY_4: Repayment status in June, 2005 (scale same as above) <br>\n11) PAY_5: Repayment status in May, 2005 (scale same as above) <br>\n12) PAY_6: Repayment status in April, 2005 (scale same as above) <br>\n13) BILL_AMT1: Amount of bill statement in September, 2005 (NT dollar) <br>\n14) BILL_AMT2: Amount of bill statement in August, 2005 (NT dollar) <br>\n15) BILL_AMT3: Amount of bill statement in July, 2005 (NT dollar) <br>\n16) BILL_AMT4: Amount of bill statement in June, 2005 (NT dollar) <br>\n17) BILL_AMT5: Amount of bill statement in May, 2005 (NT dollar) <br>\n18) BILL_AMT6: Amount of bill statement in April, 2005 (NT dollar) <br>\n19) PAY_AMT1: Amount of previous payment in September, 2005 (NT dollar) <br>\n20) PAY_AMT2: Amount of previous payment in August, 2005 (NT dollar) <br>\n21) PAY_AMT3: Amount of previous payment in July, 2005 (NT dollar) <br>\n22) PAY_AMT4: Amount of previous payment in June, 2005 (NT dollar) <br>\n23) PAY_AMT5: Amount of previous payment in May, 2005 (NT dollar) <br>\n24) PAY_AMT6: Amount of previous payment in April, 2005 (NT dollar) <br>\n25) default.payment.next.month: Default payment (1=yes, 0=no) <br>","f1e6824c":"## CUR_DELAY\n\nTells us whether the customer is in a current delay or not.\n\n### Why?\n\nIf the customer is in a current delay, they may be less likely to default because they don't have to pay the next month.","ab864aea":"## Test models\nAttempted Models:\n* XGBoost\n* Random Forest\n* Extra Trees\n* Logisitic Regression\n* Multilayer perceptron (Artificial Neural Network)\n\nEach model is sent through a pipeline that builds 4 models (one for each sampling technique)."}}