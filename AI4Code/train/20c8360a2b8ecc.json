{"cell_type":{"79232507":"code","39359222":"code","2cfee090":"code","a3d7f9b2":"code","82c58932":"code","1c47722f":"code","96281b78":"code","12406d6d":"code","e0175edc":"code","e0d713e7":"code","3c29b93c":"code","15c47ea4":"code","bc130306":"code","994088f1":"code","43c7a758":"code","9c5a8fe6":"markdown","60ea0e61":"markdown","497a3491":"markdown","d22092f9":"markdown","ca1e9429":"markdown","cd1e86fd":"markdown","d85ad620":"markdown","71adfef7":"markdown","ae18b387":"markdown","2843db0f":"markdown"},"source":{"79232507":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","39359222":"os.environ[\"WANDB_DISABLED\"] = \"true\"\n!pip install jiwer","2cfee090":"import glob\n\npath = '..\/input\/notebook-1-prepare-training-dataset\/train\/*'\n\ntrain = []\nfor file in glob.glob(path):\n    # make label from filename\n    filename = os.path.basename(file)\n    label = filename.split('_')[0]\n    train.append([file, label])\n\ndf = pd.DataFrame(train, columns=['file_name', 'text'])\ndf","a3d7f9b2":"from sklearn.model_selection import train_test_split\n\ntrain_df, test_df = train_test_split(df, test_size=0.15)\n# we reset the indices to start from zero\ntrain_df.reset_index(drop=True, inplace=True)\ntest_df.reset_index(drop=True, inplace=True)","82c58932":"import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\n\nclass IAMDataset(Dataset):\n    def __init__(self, root_dir, df, processor, max_target_length=128):\n        self.root_dir = root_dir\n        self.df = df\n        self.processor = processor\n        self.max_target_length = max_target_length\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        # get file name + text \n        file_name = self.df['file_name'][idx]\n        text = self.df['text'][idx]\n        # prepare image (i.e. resize + normalize)\n        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n        pixel_values = self.processor(image, return_tensors=\"pt\").pixel_values\n        # add labels (input_ids) by encoding the text\n        labels = self.processor.tokenizer(text, \n                                          padding=\"max_length\", \n                                          max_length=self.max_target_length).input_ids\n        # important: make sure that PAD tokens are ignored by the loss function\n        labels = [label if label != self.processor.tokenizer.pad_token_id else -100 for label in labels]\n\n        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n        return encoding","1c47722f":"from transformers import TrOCRProcessor\n\nprocessor = TrOCRProcessor.from_pretrained(\"microsoft\/trocr-base-handwritten\")\ntrain_dataset = IAMDataset(root_dir='',\n                           df=train_df,\n                           processor=processor)\neval_dataset = IAMDataset(root_dir='',\n                           df=test_df,\n                           processor=processor)","96281b78":"print(\"Number of training examples:\", len(train_dataset))\nprint(\"Number of validation examples:\", len(eval_dataset))","12406d6d":"encoding = train_dataset[0]\nfor k,v in encoding.items():\n  print(k, v.shape)","e0175edc":"image = Image.open(train_dataset.root_dir + train_df['file_name'][0]).convert(\"RGB\")\nprint('Label: '+train_df['text'][0])\nimage","e0d713e7":"labels = encoding['labels']\nprint(labels)\n\nlabels[labels == -100] = processor.tokenizer.pad_token_id\nlabel_str = processor.decode(labels, skip_special_tokens=True)\nprint('Decoded Label:', label_str)","3c29b93c":"from datasets import load_metric\n\ncer_metric = load_metric(\"cer\")\n\ndef compute_metrics(pred):\n    labels_ids = pred.label_ids\n    pred_ids = pred.predictions\n\n    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n\n    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n\n    return {\"cer\": cer}","15c47ea4":"from transformers import VisionEncoderDecoderModel\n\nmodel = VisionEncoderDecoderModel.from_pretrained(\"microsoft\/trocr-base-stage1\")","bc130306":"# set special tokens used for creating the decoder_input_ids from the labels\nmodel.config.decoder_start_token_id = processor.tokenizer.cls_token_id\nmodel.config.pad_token_id = processor.tokenizer.pad_token_id\n# make sure vocab size is set correctly\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# set beam search parameters\nmodel.config.eos_token_id = processor.tokenizer.sep_token_id\nmodel.config.max_length = 10\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 3\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4","994088f1":"from transformers import default_data_collator\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n\ntraining_args = Seq2SeqTrainingArguments(\n    num_train_epochs=1,\n    predict_with_generate=True,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    fp16=True, \n    output_dir=\".\",\n    logging_steps=2,\n    save_steps=1000,\n    eval_steps=200,\n    save_total_limit=1,\n)\n\n# instantiate trainer\ntrainer = Seq2SeqTrainer(\n    model=model,\n    tokenizer=processor.feature_extractor,\n    args=training_args,\n    compute_metrics=compute_metrics,\n    train_dataset=train_dataset,\n    eval_dataset=eval_dataset,\n    data_collator=default_data_collator,\n)\n\ntrainer.train()","43c7a758":"os.makedirs(\"model\/\")\nmodel.save_pretrained(\"model\/\")","9c5a8fe6":"## View first training data item","60ea0e61":"## Metric: CER","497a3491":"## Split Train \/ Test","d22092f9":"## Save Model","ca1e9429":"## Prepare dataset processor","cd1e86fd":"## Setup WANDB, install jiwer","d85ad620":"## Run Seq2Seq Trainer","71adfef7":"## Prepare training dataframe","ae18b387":"## Ref: Fine-tune TrOCR on the IAM Handwriting Database\nhttps:\/\/github.com\/NielsRogge\/Transformers-Tutorials\/blob\/master\/TrOCR\/Fine_tune_TrOCR_on_IAM_Handwriting_Database_using_Seq2SeqTrainer.ipynb","2843db0f":"## Load pretrained model (microsoft\/trocr-base-stage1)"}}