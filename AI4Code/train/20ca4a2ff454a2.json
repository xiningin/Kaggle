{"cell_type":{"fcf35c11":"code","6b85d1f6":"code","928cfee9":"code","9d9c8527":"code","6374a8a6":"code","c86919ed":"code","665f7f0b":"code","219c79a4":"code","ee7035d5":"code","cc99c162":"code","d81ccae8":"code","0c588670":"code","273850e4":"code","68642d6d":"code","a5166d70":"code","e5955b4c":"code","9f7fc03d":"code","62e9046a":"markdown","5b54d278":"markdown","17ce1b0b":"markdown","676f7f7d":"markdown","b6ad4936":"markdown","c00b35fa":"markdown","97eed092":"markdown","9e584ef7":"markdown","80d0edee":"markdown","8e0acab1":"markdown","95d6497b":"markdown","c2bb08b6":"markdown","aa92aa30":"markdown","19e10648":"markdown","544598c3":"markdown","15775922":"markdown"},"source":{"fcf35c11":"import os #to access files\nimport pandas as pd #to work with dataframes\nimport numpy as np #just a tradition\nfrom sklearn.model_selection import StratifiedKFold #for cross-validation\nfrom sklearn.metrics import roc_auc_score #this is we are trying to increase\nimport matplotlib.pyplot as plt #we will plot something at the end)\nimport seaborn as sns #same reason\nimport lightgbm as lgb #the model we gonna use","6b85d1f6":"%%time\nPATH_TO_DATA = '..\/input\/'\n\ndf_train_features = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                             'train_features.csv'), \n                                    index_col='match_id_hash')\ndf_train_targets = pd.read_csv(os.path.join(PATH_TO_DATA, \n                                            'train_targets.csv'), \n                                   index_col='match_id_hash')\ndf_test_features = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_features.csv'), \n                                   index_col='match_id_hash')","928cfee9":"df_train_features.head(2)","9d9c8527":"df_train_targets.head(2)","6374a8a6":"df_test_features.head(2)","c86919ed":"#turn to X and y notations for train data and target\nX = df_train_features.values\ny = df_train_targets['radiant_win'].values #extract the colomn we need","665f7f0b":"#this is to make sure we have \"ujson\" and \"tqdm\"\ntry:\n    import ujson as json\nexcept ModuleNotFoundError:\n    import json\n    print ('Please install ujson to read JSON oblects faster')\n    \ntry:\n    from tqdm import tqdm_notebook\nexcept ModuleNotFoundError:\n    tqdm_notebook = lambda x: x\n    print ('Please install tqdm to track progress with Python loops')","219c79a4":"#a helper function, we will use it in next cell\ndef read_matches(matches_file):\n    \n    MATCHES_COUNT = {\n        'test_matches.jsonl': 10000,\n        'train_matches.jsonl': 39675,\n    }\n    _, filename = os.path.split(matches_file)\n    total_matches = MATCHES_COUNT.get(filename)\n    \n    with open(matches_file) as fin:\n        for line in tqdm_notebook(fin, total=total_matches):\n            yield json.loads(line)","ee7035d5":"def add_new_features(df_features, matches_file):\n    \n    # Process raw data and add new features\n    for match in read_matches(matches_file):\n        match_id_hash = match['match_id_hash']\n\n        # Counting ruined towers for both teams\n        radiant_tower_kills = 0\n        dire_tower_kills = 0\n        for objective in match['objectives']:\n            if objective['type'] == 'CHAT_MESSAGE_TOWER_KILL':\n                if objective['team'] == 2:\n                    radiant_tower_kills += 1\n                if objective['team'] == 3:\n                    dire_tower_kills += 1\n\n        # Write new features\n        df_features.loc[match_id_hash, 'radiant_tower_kills'] = radiant_tower_kills\n        df_features.loc[match_id_hash, 'dire_tower_kills'] = dire_tower_kills\n        df_features.loc[match_id_hash, 'diff_tower_kills'] = radiant_tower_kills - dire_tower_kills\n        \n        #let's add one more\n        df_features.loc[match_id_hash, 'ratio_tower_kills'] = radiant_tower_kills \/ (0.01+dire_tower_kills)\n        # ... here you can add more features ...\n        ","cc99c162":"%%time\n# copy the dataframe with features\ndf_train_features_extended = df_train_features.copy()\ndf_test_features_extended = df_test_features.copy()\n\n# add new features\nadd_new_features(df_train_features_extended, os.path.join(PATH_TO_DATA, 'train_matches.jsonl'))\nadd_new_features(df_test_features_extended, os.path.join(PATH_TO_DATA, 'test_matches.jsonl'))","d81ccae8":"#Just a shorter names for data\nnewtrain=df_train_features_extended\nnewtest=df_test_features_extended\ntarget=pd.DataFrame(y)","0c588670":"#lastly, check the shapes, Andrew Ng approved)\nnewtrain.shape,target.shape, newtest.shape","273850e4":"features=newtrain.columns","68642d6d":"param = {\n        'bagging_freq': 5,  #handling overfitting\n        'bagging_fraction': 0.5,  #handling overfitting - adding some noise\n        'boost_from_average':'false',\n        'boost': 'gbdt',\n        'feature_fraction': 0.05, #handling overfitting\n        'learning_rate': 0.01,  #the changes between one auc and a better one gets really small thus a small learning rate performs better\n        'max_depth': -1,  \n        'metric':'auc',\n        'min_data_in_leaf': 50,\n        'min_sum_hessian_in_leaf': 10.0,\n        'num_leaves': 10,\n        'num_threads': 5,\n        'tree_learner': 'serial',\n        'objective': 'binary', \n        'verbosity': 1\n    }","a5166d70":"%%time\n#divide training data into train and validaton folds\nfolds = StratifiedKFold(n_splits=5, shuffle=False, random_state=17)\n\n#placeholder for out-of-fold, i.e. validation scores\noof = np.zeros(len(newtrain))\n\n#for predictions\npredictions = np.zeros(len(newtest))\n\n#and for feature importance\nfeature_importance_df = pd.DataFrame()\n\n#RUN THE LOOP OVER FOLDS\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(newtrain.values, target.values)):\n    \n    X_train, y_train = newtrain.iloc[trn_idx], target.iloc[trn_idx]\n    X_valid, y_valid = newtrain.iloc[val_idx], target.iloc[val_idx]\n    \n    print(\"Computing Fold {}\".format(fold_))\n    trn_data = lgb.Dataset(X_train, label=y_train)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n\n    \n    num_round = 5000 \n    verbose=1000 \n    stop=500 \n    \n    #TRAIN THE MODEL\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=verbose, early_stopping_rounds = stop)\n    \n    #CALCULATE PREDICTION FOR VALIDATION SET\n    oof[val_idx] = clf.predict(newtrain.iloc[val_idx], num_iteration=clf.best_iteration)\n    \n    #FEATURE IMPORTANCE\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    #CALCULATE PREDICTIONS FOR TEST DATA, using best_iteration on the fold\n    predictions += clf.predict(newtest, num_iteration=clf.best_iteration) \/ folds.n_splits\n\n#print overall cross-validatino score\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))","e5955b4c":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:150].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","9f7fc03d":"df_submission = pd.DataFrame({'radiant_win_prob': predictions}, \n                                 index=df_test_features.index)\nimport datetime\nsubmission_filename = 'submission_{}.csv'.format(\n    datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S'))\ndf_submission.to_csv(submission_filename)\nprint('Submission saved to {}'.format(submission_filename))","62e9046a":"# Finally, let's run the model","5b54d278":"# What's next?\n\n* try to tune parameters, it will definitely improve your LB score\n* try to come up with good features\n* read other kernels\n* try other models as well","17ce1b0b":"# here, imagine some cool picture about dota2)","676f7f7d":"## Let's read the data: train, target and test","b6ad4936":"## Feature Importance","c00b35fa":"### Hopefully, this kernel was usefull. Feel free to fork, comment and upvote!","97eed092":"## In this kernel you will learn how to implement LightGBM + Kfold technique which results in higher score!","9e584ef7":"## Lets have a look what are these data look like:","80d0edee":"After running the LightGBM model, we will visualize something called \"feature importance\", which  kind of shows which features and how much they affected the final result. For this reason we need to store feature names:","8e0acab1":"I have no idea what these features mean...I prefer FIFA)","95d6497b":"**LightGBM** is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\n* Faster training speed and higher efficiency.\n* Lower memory usage.\n* Better accuracy.\n* Support of parallel and GPU learning.\n* Capable of handling large-scale data.\n\nYou can read full documentation [here](https:\/\/lightgbm.readthedocs.io\/en\/latest\/)","c2bb08b6":"### Now, let's import all required packages ","aa92aa30":"## Noow, let's define LightGBM parameters. \n\nPersonally, I understand only some of these parameters. So, these are some random set up. Maybe it is better to look up the official documentation. Tuning these parameters definitely will increase your score.\n\nInvestigation in process...","19e10648":"From these feature importance chart, we can see that some features play significant role in making the prediction than others. Maybe dropping out less affecting features is a good idea. But still need more investigation of dota2 features...","544598c3":"Now, we define a function which adds some new features:\n\nPS: all of these are from \"how to start\" kernel by @yorko","15775922":"## Prepare submission file"}}