{"cell_type":{"741cd978":"code","aab1bfe6":"code","eecd460c":"code","cd0a3964":"code","d9830af6":"code","e38d680f":"code","be377060":"code","ce5c7567":"code","4976131d":"code","0abbdcbb":"code","78350458":"code","c6924047":"code","9e26af43":"code","49e071c4":"code","febafc1c":"code","363780b7":"code","3f334de1":"code","18694674":"code","42d715ff":"code","7ae6b534":"code","ea3a87dc":"code","2a7839fd":"code","7bae7f1f":"code","efa99428":"code","55621ed6":"code","c630cfd6":"code","cfca8c52":"code","d4a7c32e":"code","25f2f2f4":"code","bb0e7584":"code","d76cea7d":"code","79e832e9":"code","79bc2c0a":"code","ec1b8621":"code","d9029daa":"code","078a3381":"code","82323513":"code","baeac692":"code","2baf2940":"code","c348614f":"code","b97acbd3":"code","2f46cb5d":"code","abdea7d0":"code","d28118af":"code","74d51e89":"code","a01d1020":"code","f964fb8a":"code","b90c7d3d":"code","4b5e2adf":"code","a97ac0eb":"code","d019b3da":"markdown","325f943d":"markdown","b916335f":"markdown","293bcc02":"markdown","56a1093c":"markdown","8aef0c10":"markdown","3a6ba15b":"markdown","9948ce50":"markdown","7cee85a9":"markdown","14a7e9e7":"markdown","7ec48a0e":"markdown","1546c515":"markdown","b879b880":"markdown","03aaba90":"markdown","215bef89":"markdown","13ba1e51":"markdown","b011db60":"markdown","1b07bb8b":"markdown","35913600":"markdown","2f7de9aa":"markdown","4b6de8ff":"markdown","d214d73f":"markdown","9c7b4833":"markdown"},"source":{"741cd978":"#Importing required packages.\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\nimport matplotlib\n","aab1bfe6":"#Loading dataset\nwine = pd.read_csv('..\/input\/winequality-red.csv')","eecd460c":"#Let's check how the data is distributed\nwine.head()","cd0a3964":"#Information about the data columns\nwine.info()","d9830af6":"#Here we see that fixed acidity does not give any specification to classify the quality.\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'fixed acidity', data = wine)","e38d680f":"#Here we see that its quite a downing trend in the volatile acidity as we go higher the quality \nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'volatile acidity', data = wine)","be377060":"#Composition of citric acid go higher as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'citric acid', data = wine)","ce5c7567":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'residual sugar', data = wine)","4976131d":"#Composition of chloride also go down as we go higher in the quality of the wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'chlorides', data = wine)","0abbdcbb":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'free sulfur dioxide', data = wine)","78350458":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'total sulfur dioxide', data = wine)","c6924047":"#Sulphates level goes higher with the quality of wine\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'sulphates', data = wine)","9e26af43":"#Alcohol level also goes higher as te quality of wine increases\nfig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'alcohol', data = wine)","49e071c4":"wine.quality.describe()","febafc1c":"#Making binary classificaion for the response variable.\n#Dividing wine as good and bad by giving the limit for the quality\nbins = (2,6,8)\ngroup_names = ['bad', 'good']\nwine['quality'] = pd.cut(wine['quality'], bins = bins, labels = group_names)","363780b7":"#Now lets assign a labels to our quality variable\nlabel_quality = LabelEncoder() ","3f334de1":"#Bad becomes 0 and good becomes 1 \nwine['quality'] = label_quality.fit_transform(wine['quality'])","18694674":"wine['quality'].value_counts()","42d715ff":"sns.countplot(wine['quality'])","7ae6b534":"#Now seperate the dataset as response variable and feature variabes\nX = wine.drop('quality', axis = 1)\ny = wine.quality","ea3a87dc":"#Train and Test splitting of data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","2a7839fd":"#Applying Standard scaling to get optimized result\nsc = StandardScaler()","7bae7f1f":"# X_train = sc.fit_transform(X_train)\n# X_test = sc.fit_transform(X_test)\n#commented on purpose to demonstrate something","efa99428":"type(X_train)","55621ed6":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)","c630cfd6":"#Testing time\nprint(classification_report(y_test,pred_logreg))","cfca8c52":"#Confusion matrix for the random forest classification\nprint(confusion_matrix(y_test, pred_logreg))","d4a7c32e":"import eli5\neli5.show_weights(logreg)\n#uses permutation importance to compute feature weights","25f2f2f4":"feat_names=wine.columns[:-1].tolist()","bb0e7584":"feat_names","d76cea7d":"eli5.show_weights(logreg, feature_names=feat_names)","79e832e9":"import numpy as np\ni = np.random.randint(1,100)","79bc2c0a":"i","ec1b8621":"X_test.iloc[i]","d9029daa":"y_test.iloc[i]","078a3381":"eli5.show_prediction(logreg, \n                     X_test.iloc[i],\n                     feature_names=feat_names, show_feature_values=True)","82323513":"rfc=RandomForestClassifier(n_estimators=200)\nrfm = rfc.fit(X_train,y_train)\npred_rfc= rfc.predict(X_test)","baeac692":"#Testing time\nprint(classification_report(y_test,pred_rfc))","2baf2940":"print(confusion_matrix(y_test, pred_rfc))","c348614f":"#looking at eli5 first\neli5.show_weights(rfm, \n                  feature_names=feat_names)","b97acbd3":"from lime.lime_tabular import LimeTabularExplainer\n","2f46cb5d":"X_train.head()","abdea7d0":"explainer = LimeTabularExplainer(X_train.values,\n                                 mode=\"classification\",\n                                 feature_names=X_train.columns.tolist(),\n                                 categorical_names=None,\n                                 categorical_features=None,\n                                 discretize_continuous=True,\n                                 random_state=42)","d28118af":"prob=lambda x:rfm.predict_proba(X_test[[i]]).astype(float)\n","74d51e89":"X_test.iloc[i]","a01d1020":"#prediction function\npred_fn = lambda x: rfm.predict_proba(x).astype(float)","f964fb8a":"explanation = explainer.explain_instance(X_test.iloc[i], pred_fn)\nexplanation.show_in_notebook(show_table=True, show_all=False,)\nprint(explanation.score)","b90c7d3d":"!pip install https:\/\/github.com\/adebayoj\/fairml\/archive\/master.zip\n# Installing another package called fairML","4b5e2adf":"from fairml import audit_model\nimportances, _ = audit_model(rfm.predict, X_test)\nprint(importances)","a97ac0eb":"#inbuilt methods to visualize it\ntotal, _ = audit_model(logreg.predict, X_test)\n# print feature importance\nprint(total)\n\n# generate feature dependence plot\nfrom fairml import plot_dependencies\nfig = plot_dependencies(\n    total.median(),\n    reverse_values=False,\n    title=\"FairML feature dependence\"\n)\nplt.savefig(\"fairml_ldp.eps\", transparent=False, bbox_inches='tight')","d019b3da":"## Simple Logistic Regression","325f943d":"#This Notebook contains a collection of methods to weave explainability into AI systems.\nIt can come handy while studying models,debugging ML pipelines and for exploratory purposes. \nA system is only as good as its creator. Today, we acknowledge the limitations of our craft and attempt to understand it better.","b916335f":"The parameters passed to the explainer are:\n\nTraining set sans one hot encoding\nmode: the explainer can be used for classification or regression\nfeature_names: list of labels for our features\ncategorical_features: list of indexes of categorical features\ncategorical_names: dict mapping each index of categorical feature to a list of corresponding labels\ndicretize_continuous: will discretize numerical values into buckets that can be used for explanation. For instance it can tell us that the decision was made because distance is in bucket [5km, 10km] instead of telling us distance is an importante feature.","293bcc02":"A decent accuracy!","56a1093c":"> The basic idea behind FairML is to measure a model\u2019s dependence on its inputs by changing them. If a small change to an input feature dramatically changes the output, the model is sensitive to the feature.\n> Think sensitivty analysis that we studied in economics\/Calculus in schools.\nRemember orthogonal vectors?\nSource: https:\/\/blog.fastforwardlabs.com\/2017\/03\/09\/fairml-auditing-black-box-predictive-models.html","8aef0c10":"# Visual EDA","3a6ba15b":"# Explain to me like I'm 5","9948ce50":"So this is for the whole model.Let's nitpick a particular observation","7cee85a9":"ELI5 understands text processing utilities from scikit-learn and can highlight text data accordingly. It also allows to debug scikit-learn pipelines which contain HashingVectorizer, by undoing hashing.\n\nXGBoost - show feature importances and explain predictions of XGBClassifier, XGBRegressor and xgboost.Booster.\n\nLightGBM - show feature importances and explain predictions of LGBMClassifier and LGBMRegressor.\n\nCatBoost - show feature importances of CatBoostClassifier and CatBoostRegressor.\n\nlightning - explain weights and predictions of lightning classifiers and regressors.\n\nsklearn-crfsuite. ELI5 allows to check weights of sklearn_crfsuite.CRF models.\n\nKeras - explain predictions of image classifiers via Grad-CAM visualizations.\n\nSource: https:\/\/eli5.readthedocs.io\/en\/latest\/overview.html\n    ","14a7e9e7":"**References:**\n* Bonus resource: a Textbook on Fairness in ML: https:\/\/fairmlbook.org\/pdf\/fairmlbook.pdf\n* Image source: https:\/\/www.publichealthnotes.com\/equity-vs-equality\/\n* https:\/\/blog.fastforwardlabs.com\/2017\/03\/09\/fairml-auditing-black-box-predictive-models.html\n","7ec48a0e":"##  Hello all. Welcome to XAI day by Open Data science conference, New Delhi.\n\n**Give us an upvote if you find it useful.**\n\nWarm regards,\nTeam ODSC New Delhi","1546c515":"That gives us the weights associated to each feature, that can be seen as the contribution of each feature into predicting that the class will be y=1 (the client will subscribe after the campaign).\n\nThe names for each features aren't really helping though, we can pass a list of column names to eli5 but we'll need to do a little gymnastics first to extract names from our preprocessor in the pipeline (since we've generated new features on the fly with the one hot encoder)","b879b880":"![](http:\/\/www.publichealthnotes.com\/wp-content\/uploads\/2017\/05\/Equality-Vs-Equity..final-edit-1.jpg)","03aaba90":"Columns description\n* fixed acidity:most acids involved with wine or fixed or nonvolatile (do not evaporate readily)\n* \n* volatile acidity:the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste\n* \n* citric acid:found in small quantities, citric acid can add 'freshness' and flavor to wines\n* \n* residual sugar:the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram\/liter and wines with greater than 45 grams\/liter are considered sweet\n* \n* chlorides:the amount of salt in the wine\n* \n* free sulfur dioxide:the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents microbial growth and the oxidation of wine\n* \n* total sulfur dioxide:amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2 concentrations over 50 ppm, SO2 becomes evident in the nose and taste of wine\n* \n* density:the density of water is close to that of water depending on the percent alcohol and sugar content\n* \n* pH:describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the pH scale\n* \n* sulphates:a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and antioxidant\n* \n* alcohol:the percent alcohol content of the wine\n* \n* quality:output variable (based on sensory data, score between 0 and 10)","215bef89":"    ### Simple Logistic Regression","13ba1e51":"Source: https:\/\/www.kaggle.com\/uciml\/red-wine-quality-cortez-et-al-2009\n\nThis dataset is also available from the UCI machine learning repository, https:\/\/archive.ics.uci.edu\/ml\/datasets\/wine+quality , I just shared it to kaggle for convenience. (If I am mistaken and the public license type disallowed me from doing so, I will take this down if requested.)\n\nContent\nFor more information, read [Cortez et al., 2009].\nInput variables (based on physicochemical tests):\n1 - fixed acidity\n2 - volatile acidity\n3 - citric acid\n4 - residual sugar\n5 - chlorides\n6 - free sulfur dioxide\n7 - total sulfur dioxide\n8 - density\n9 - pH\n10 - sulphates\n11 - alcohol\nOutput variable (based on sensory data):\n12 - quality (score between 0 and 10)","b011db60":"## LIME :Local Interpretable Model-Agnostic Explanations\nLime works on the principle of local fidelity ie that a point behaves in the same manner as that of its immediate neighbors. Fort his purpose,Lime is lighter than ELI5 and often faster.","1b07bb8b":"## Dataset","35913600":"The explainer is all set up to explain observations!\nLet's use the old i","2f7de9aa":"This is simple.sign=direction of relationship and coefficients= weights","4b6de8ff":"# Moving on,let's talk about the concept of fairness.How do you define fair?","d214d73f":"#### **Before moving to sophisticated methods,Let's start with the basics.Graphical representations are very powerful for explaining otherwise complex things.Humans have evolved to process visual stimuli faster than numbers,so lets look at the distribution of some of our variables**","9c7b4833":"# Hi I'm LIME,I'm the infamous gossip monger. If I dont know you,I'll take your neighbour's word for it!! \n[Kriti is awesome though! Be like KD] <3"}}