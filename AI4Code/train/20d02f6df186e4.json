{"cell_type":{"16cfd960":"code","a6b52bcc":"code","0ed2624e":"code","a5d1b996":"code","49c31db8":"code","155739c4":"code","be50234d":"code","b33dcbc8":"code","1e5af9f3":"code","08208b56":"code","b3103915":"code","6a5e6d16":"code","a3540682":"code","7d348943":"code","8cd103e1":"code","a4323026":"code","fea8e9ca":"code","fe0c58e7":"code","bff6364c":"code","498e1b9f":"code","54e56d8c":"code","bf9369ef":"code","e86f8cf3":"code","6015e805":"code","05c3711a":"code","c279275e":"code","980fce5c":"code","0da9934f":"code","d4fc26f7":"code","a6d59f23":"code","79dfada1":"code","2b119106":"code","be385d55":"markdown","eaba9903":"markdown","3a8f5c6d":"markdown","8e1f0e73":"markdown","b9e4c44d":"markdown","9be1a2f1":"markdown","aa18ee42":"markdown","9773ed01":"markdown","54d3c1eb":"markdown","72b6159f":"markdown","8b9d937c":"markdown","01bffa3b":"markdown","02f279fa":"markdown","2a840d4c":"markdown","d270ddfb":"markdown","6df70da9":"markdown","534f89b6":"markdown","ffbfe293":"markdown","20ca57a7":"markdown","6cbe1a54":"markdown","5c53618d":"markdown","67e74753":"markdown","c0e11b44":"markdown","52ead4bc":"markdown","773c21ed":"markdown","9560eb28":"markdown","f689ce6b":"markdown","02e51c8d":"markdown","32959cab":"markdown","43bb10bd":"markdown","9c87378d":"markdown","8a45bbc2":"markdown","0ca794cc":"markdown","04e764c8":"markdown","2de3aa9c":"markdown"},"source":{"16cfd960":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, BatchNormalization\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\nfrom keras.utils import plot_model\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\n%matplotlib inline ","a6b52bcc":"IMG_ROWS = 28\nIMG_COLS = 28\nNUM_CLASSES = 10\nTEST_SIZE = 0.1\nRANDOM_STATE = 2018\n#Model\nNO_EPOCHS = 100\nBATCH_SIZE = 128\n\nIS_LOCAL = False\n\nimport os\n\nif(IS_LOCAL):\n    PATH=\"..\/input\/kuzushiji\/\"\nelse:\n    PATH=\"..\/input\/\"\nprint(os.listdir(PATH))","0ed2624e":"train_images = np.load(PATH+'kmnist-train-imgs.npz')['arr_0']\ntest_images = np.load(PATH+'kmnist-test-imgs.npz')['arr_0']\ntrain_labels = np.load(PATH+'kmnist-train-labels.npz')['arr_0']\ntest_labels = np.load(PATH+'kmnist-test-labels.npz')['arr_0']","a5d1b996":"char_df = pd.read_csv(PATH+'kmnist_classmap.csv', encoding = 'utf-8')","49c31db8":"print(\"KMNIST train shape:\", train_images.shape)\nprint(\"KMNIST test shape:\", test_images.shape)\nprint(\"KMNIST train shape:\", train_labels.shape)\nprint(\"KMNIST test shape:\", test_labels.shape)","155739c4":"print(\"KMNIST character map shape:\", char_df.shape)","be50234d":"char_df","b33dcbc8":"print('Percent for each category:',np.bincount(train_labels)\/len(train_labels)*100)","1e5af9f3":"labels = char_df['char']\nf, ax = plt.subplots(1,1, figsize=(8,6))\ng = sns.countplot(train_labels)\ng.set_title(\"Number of labels for each class\")\ng.set_xticklabels(labels)\nplt.show()    ","08208b56":"def plot_sample_images_data(images, labels):\n    plt.figure(figsize=(12,12))\n    for i in tqdm_notebook(range(10)):\n        imgs = images[np.where(labels == i)]\n        lbls = labels[np.where(labels == i)]\n        for j in range(10):\n            plt.subplot(10,10,i*10+j+1)\n            plt.xticks([])\n            plt.yticks([])\n            plt.grid(False)\n            plt.imshow(imgs[j], cmap=plt.cm.binary)\n            plt.xlabel(lbls[j])","b3103915":"plot_sample_images_data(train_images, train_labels)","6a5e6d16":"plot_sample_images_data(test_images, test_labels)","a3540682":"# data preprocessing\ndef data_preprocessing(images, labels):\n    out_y = keras.utils.to_categorical(labels, NUM_CLASSES)\n    num_images = images.shape[0]\n    x_shaped_array = images.reshape(num_images, IMG_ROWS, IMG_COLS, 1)\n    out_x = x_shaped_array \/ 255\n    return out_x, out_y","7d348943":"X, y = data_preprocessing(train_images, train_labels)\nX_test, y_test = data_preprocessing(test_images, test_labels)","8cd103e1":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)","a4323026":"print(\"KMNIST train -  rows:\",X_train.shape[0],\" columns:\", X_train.shape[1:4])\nprint(\"KMNIST valid -  rows:\",X_val.shape[0],\" columns:\", X_val.shape[1:4])\nprint(\"KMNIST test -  rows:\",X_test.shape[0],\" columns:\", X_test.shape[1:4])","fea8e9ca":"def plot_count_per_class(yd):\n    ydf = pd.DataFrame(yd)\n    f, ax = plt.subplots(1,1, figsize=(12,4))\n    g = sns.countplot(ydf[0], order = np.arange(0,10))\n    g.set_title(\"Number of items for each class\")\n    g.set_xlabel(\"Category\")\n            \n    plt.show()  \n\ndef get_count_per_class(yd):\n    ydf = pd.DataFrame(yd)\n    # Get the count for each label\n    label_counts = ydf[0].value_counts()\n\n    # Get total number of samples\n    total_samples = len(yd)\n\n\n    # Count the number of items in each class\n    for i in range(len(label_counts)):\n        label = label_counts.index[i]\n        label_char = char_df[char_df['index']==label]['char'].item()\n        count = label_counts.values[i]\n        percent = (count \/ total_samples) * 100\n        print(\"{}({}):   {} or {}%\".format(label, label_char, count, percent))\n    \nplot_count_per_class(np.argmax(y_train,axis=1))\nget_count_per_class(np.argmax(y_train,axis=1))","fe0c58e7":"plot_count_per_class(np.argmax(y_val,axis=1))\nget_count_per_class(np.argmax(y_val,axis=1))","bff6364c":"# Model\nmodel = Sequential()\n# Add convolution 2D\nmodel.add(Conv2D(32, kernel_size=(3, 3),activation='relu', padding=\"same\",\n        kernel_initializer='he_normal',input_shape=(IMG_ROWS, IMG_COLS, 1)))\n\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(32,kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(BatchNormalization())\n# Add dropouts to the model\nmodel.add(Dropout(0.4))\nmodel.add(Conv2D(64, kernel_size=(3, 3), strides=2,padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(64, kernel_size=(3, 3), strides=2,padding='same', activation='relu'))\n# Add dropouts to the model\nmodel.add(Dropout(0.4))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\n# Add dropouts to the model\nmodel.add(Dropout(0.4))\nmodel.add(Dense(NUM_CLASSES, activation='softmax'))","498e1b9f":"# Compile the model\nmodel.compile(loss = \"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])","54e56d8c":"model.summary()","bf9369ef":"history = model.fit(X_train, y_train,\n          batch_size=BATCH_SIZE,\n          epochs=NO_EPOCHS,\n          verbose=1,\n          validation_data=(X_val, y_val))","e86f8cf3":"def plot_accuracy_and_loss(train_model):\n    hist = train_model.history\n    acc = hist['acc']\n    val_acc = hist['val_acc']\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    epochs = range(len(acc))\n    f, ax = plt.subplots(1,2, figsize=(14,6))\n    ax[0].plot(epochs, acc, 'g', label='Training accuracy')\n    ax[0].plot(epochs, val_acc, 'r', label='Validation accuracy')\n    ax[0].set_title('Training and validation accuracy')\n    ax[0].legend()\n    ax[1].plot(epochs, loss, 'g', label='Training loss')\n    ax[1].plot(epochs, val_loss, 'r', label='Validation loss')\n    ax[1].set_title('Training and validation loss')\n    ax[1].legend()\n    plt.show()\nplot_accuracy_and_loss(history)","6015e805":"#get the predictions for the test data\npredicted_classes = model.predict_classes(X_val)\n#get the indices to be plotted\ny_true = np.argmax(y_val,axis=1)","05c3711a":"correct = np.nonzero(predicted_classes==y_true)[0]\nincorrect = np.nonzero(predicted_classes!=y_true)[0]","c279275e":"print(\"Correct predicted classes:\",correct.shape[0])\nprint(\"Incorrect predicted classes:\",incorrect.shape[0])","980fce5c":"target_names = [\"Class {} ({}):\".format(i, char_df[char_df['index']==i]['char'].item()) for i in range(NUM_CLASSES)]\nprint(classification_report(y_true, predicted_classes, target_names=target_names))","0da9934f":"def plot_images(data_index,cmap=\"Blues\"):\n    # Plot the sample images now\n    f, ax = plt.subplots(5,5, figsize=(12,12))\n\n    for i, indx in enumerate(data_index[:25]):\n        ax[i\/\/5, i%5].imshow(X_val[indx].reshape(IMG_ROWS,IMG_COLS), cmap=cmap)\n        ax[i\/\/5, i%5].axis('off')\n        ax[i\/\/5, i%5].set_title(\"True:{}  Pred:{}\".format(y_true[indx],predicted_classes[indx]))\n    plt.show()    \n\nplot_images(correct, \"Greens\")","d4fc26f7":"plot_images(incorrect, \"Reds\")","a6d59f23":"#get the predictions for the test data\npredicted_classes = model.predict_classes(X_test)\n#get the indices to be plotted\ny_true = np.argmax(y_test,axis=1)\ncorrect = np.nonzero(predicted_classes==y_true)[0]\nincorrect = np.nonzero(predicted_classes!=y_true)[0]\nprint(\"Correct predicted classes:\",correct.shape[0])\nprint(\"Incorrect predicted classes:\",incorrect.shape[0])\ntarget_names = [\"Class {} ({}):\".format(i, char_df[char_df['index']==i]['char'].item()) for i in range(NUM_CLASSES)]\nprint(classification_report(y_true, predicted_classes, target_names=target_names))","79dfada1":"plot_images(correct, \"Greens\")","2b119106":"plot_images(incorrect, \"Reds\")","be385d55":"### Inspect the model\n\nLet's check the model we initialized.","eaba9903":"## <a id=\"41\">Class distribution<\/a>\n\nLet's see how many number of images are in each class. \n\n### Train set images class distribution","3a8f5c6d":"Let's visualize the images from the test set that were incorrecly classified (25 images).","8e1f0e73":"## <a id=\"52\">Train the model<\/a>\n\n### Build the model   \n\n\n\nWe will use a **Sequential** model.\n* The **Sequential** model is a linear stack of layers. It can be first initialized and then we add layers using **add** method or we can add all layers at init stage. The layers added are as follows:\n\n* **Conv2D** is a 2D Convolutional layer (i.e. spatial convolution over images). The parameters used are:\n * filters - the number of filters (Kernels) used with this layer; here filters = 32;\n * kernel_size - the dimmension of the Kernel: (3 x 3);\n * activation - is the activation function used, in this case `relu`;\n * kernel_initializer - the function used for initializing the kernel;\n * input_shape - is the shape of the image presented to the CNN: in our case is 28 x 28\n The input and output of the **Conv2D** is a 4D tensor.\n* **Conv2D** with the following parameters:\n * filters: 32;\n * kernel_size: (3 x 3);\n * activation: `relu`;\n* **MaxPooling2D** is a Max pooling operation for spatial data. Parameters used here are:\n * *pool_size*, in this case (2,2), representing the factors by which to downscale in both directions;\n \n* **Dropout**. Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, which helps prevent overfitting. The parameter used is:\n * *rate*, set here to 0.25. \n \n* **Conv2D** with the following parameters:\n * filters: 64;\n * kernel_size : (3 x 3);\n * activation : `relu`;\n \n* **MaxPooling2D** with parameter:\n * *pool_size* : (2,2);\n\n* **Dropout**. with parameter:\n * *rate* : 0.25;\n \n* **Conv2D** with the following parameters:\n * filters: 128;\n * kernel_size : (3 x 3);\n * activation : `relu`;\n\n* **Dropout**. with parameter:\n * *rate* : 0.4;\n \n* **Flatten**. This layer Flattens the input. Does not affect the batch size. It is used without parameters;\n\n* **Dense**. This layer is a regular fully-connected NN layer. It is used without parameters;\n * units - this is a positive integer, with the meaning: dimensionality of the output space; in this case is: 128;\n * activation - activation function : `relu`;\n\n* **Dropout**. with parameter:\n * *rate* : 0.3;\n \n* **Dense**. This is the final layer (fully connected). It is used with the parameters:\n * units: the number of classes (in our case 10);\n * activation : `softmax`; for this final layer it is used `softmax` activation (standard for multiclass classification)\n \n\nThen we compile the model, specifying as well the following parameters:\n* *loss*;\n* *optimizer*;\n* *metrics*. \n","b9e4c44d":"Let's visualize the images from the validation set that were incorrecly classified (25 images).","9be1a2f1":"### Compile the model\nWe then compile the model, with the layers and optimized defined.","aa18ee42":"## <a id=\"54\">Validation accuracy \/ class<\/a>\n\nLet's see in detail how well are the validation set classes predicted.","9773ed01":"The dimmension of the character set data file for KMNIST are:","54d3c1eb":"\n## <a id=\"53\">Validation accuracy and  loss <\/a>\n\n\nWe plot accuracy for validation set compared with the accuracy of training set, for each epoch, on the same graph. Then, we plot loss for validation set compared with the loss for training set. \n","72b6159f":"## <a id=\"51\">Prepare the model<\/a>\n\n## Data preprocessing\n\nFirst we will do a data preprocessing to prepare for the model.\n\nWe reshape the numpy arrays for images to associate to each image a (28 x 28 x 1) array, with values normalized.","8b9d937c":"The dimmension of the processed train, validation and test set are as following:","01bffa3b":"Validation accuracy is above 0.99 for most of the classes. Only two classes have lower accuracy, above 0.98. \n\nLet's visualize few images from the validation set that were correctly classified (25 images).","02f279fa":"# <a id=\"7\">Conclusions<\/a>\n\nWith a complex sequential model with multiple convolution layers and **100** epochs for the training, we obtained a precision  of approximatelly **0.99**  for the validation set and approximatelly  **0.96** for the **test**.\n","2a840d4c":"<h1><center><font size=\"6\">Classifying Cursive Hiragana (\u5d29\u3057\u5b57) KMNIST using CNN<\/font><\/center><\/h1>\n\n\n<img src=\"http:\/\/su-cultural-history.up.n.seesaa.net\/su-cultural-history\/image\/P1030717.JPG?d=a48\" width=\"400\"><\/img>\n\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Load packages<\/a>  \n- <a href='#3'>Read the data<\/a>  \n- <a href='#4'>Data exploration<\/a>\n    - <a href='#41'>Class distribution<\/a>\n    - <a href='#42'>Images samples<\/a>\n- <a href='#5'>Model<\/a>  \n    - <a href='#51'>Prepare the model<\/a>  \n    - <a href='#52'>Train the model<\/a>  \n    - <a href='#53'>Validation accuracy and loss<\/a>  \n    - <a href='#54'>Validation accuracy per class<\/a>  \n- <a href='#6'>Test prediction<\/a>     \n- <a href='#7'>Conclusions<\/a>\n- <a href='#8'>References<\/a>","d270ddfb":"## Parameters","6df70da9":"# <a id=\"3\">Read the data<\/a>\n\nWe will read the two data files containing the 10-class data, KMNIST, similar to MNIST.\n\nThere are 10 different classes of images, one class for each number between 0 and 9. \n\nImage dimmensions are **28**x**28**.   \n\nThe train set and test set are given in two separate numpy arrays.   \n\nWe are also reading the labels for train and test sets.\n\nAditionally, we will read the character class map for KMNIST, so that we can display the actual characters corresponding to the labels.\n","534f89b6":"Let's show the character map:","ffbfe293":"## <a id=\"42\">Sample images<\/a>\n\n### Train dataset images\n\nLet's plot some samples for the images.","20ca57a7":"We process both the train_data and the test_data.","6cbe1a54":"## Split train in train and validation set\n\nWe further split the train set in train and validation set. The validation set will be 10% from the original train set, therefore the split will be train\/validation of 0.9\/0.1.","5c53618d":"We can observe that the training is not overfitting, validation accuracy is not decreasing after a certain number of epochs. We obtained final accuracy around 0.987. As well, the validation loss is not increasing after a certain number of epochs, as would have been expected in the case of overfitting. We achieved this by using the 3 Dropout layers inserted in our model. There are other strategies as well, for example by using a variable learning rate or data augmentation images. For the sake of simplicity and in order to keep the calculation very fast (the Kernel complete the training for 50 epochs in less than 10 min), we did not included these techniques for now.","67e74753":"\nWeighted averages for **precision**, **recall** and **f1-score** for the test are 0.96.\n\nAccuracy obtained is **0.955**.\n\nLet's visualize few of the images from the test set that were correctly classified (25 images).","c0e11b44":"# <a id=\"5\">Model<\/a>\n\nWe start with preparing the model.","52ead4bc":"# <a id=\"1\">Introduction<\/a>  \n\n\n## Dataset\n\nMNIST (\"Modified National Institute of Standards and Technology\") is the de facto \u201chello world\u201d dataset of computer vision. Since its release in 1999, this classic dataset of handwritten images has served as the basis for benchmarking classification algorithms.   \n\n\nKMNIST (Kuzushiji-MNIST or Cursive hiragana-MNIST)  was introduced as an alternative to MNIST. It contains images with the first entries from the 10 main Japanese hiragana character groups.\n\n\n\n## Content\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total.   \n\nThe images are storred in numpy arrays of 60,000 x 28 x 28 and 10,000 x 28 x 28, respectively. The labels are also stored in two numpy arrays, one for train and another for the test set.\n","773c21ed":" <a id=\"8\">References<\/a>\n\n[1] Yan LeCun, MNIST Database, http:\/\/yann.lecun.com\/exdb\/mnist\/  \n[2] DanB, CollinMoris, Deep Learning From Scratch, https:\/\/www.kaggle.com\/dansbecker\/deep-learning-from-scratch  \n[3] DanB, Dropout and Strides for Larger Models, https:\/\/www.kaggle.com\/dansbecker\/dropout-and-strides-for-larger-models  \n[4] BGO, CNN with Keras, https:\/\/www.kaggle.com\/bugraokcu\/cnn-with-keras    \n[5] Gabriel Preda, Simple introduction to CNN for MNIST (99.37%), https:\/\/www.kaggle.com\/gpreda\/simple-introduction-to-cnn-for-mnist-99-37  \n[6] Anokas, KMNIST-MNIST replacement, https:\/\/www.kaggle.com\/aakashnain\/kmnist-mnist-replacement    \n[7] Megan Risdal, Starter: Kuzushiji-MNIST, https:\/\/www.kaggle.com\/mrisdal\/starter-kuzushiji-mnist-ed86cfac-1   \n[8] Kuzushiji-MNIST, project Github repo, https:\/\/github.com\/rois-codh\/kmnist   \n[9] Tarin Clanuwat, Mikel Bober-Irizar, Asanobu Kitamoto, Alex Lamb, Kazuaki Yamamoto, David Ha, Deep Learning for Classical Japanese Literature, https:\/\/arxiv.org\/abs\/1812.01718  \n\n","9560eb28":"Let's see also the class distribution of validation set.","f689ce6b":"### Test set images\n\nLet's plot now a selection of the train set images. We will just randomly select some of the test images. In this dataset, also the test images have labels associated.","02e51c8d":"![](http:\/\/)The dimmension of the original  train,  test set are as following:","32959cab":"We identify the predicted class for each image by selecting the column with the highest predicted value.","43bb10bd":"### Run the model\n\nWe run the model with the training set. We are also using the validation set (a subset from the orginal training set) for validation.","9c87378d":"# <a id=\"2\">Load packages<\/a>","8a45bbc2":"The classes are equaly distributed in the train set (being 10% each, or 6000 items).  Let's also plot a graph for these.","0ca794cc":"# <a id=\"6\">Test prediction<\/a>\n\nLet's use the trained model to predict the labels for the test images.","04e764c8":"# <a id=\"4\">Data exploration<\/a>","2de3aa9c":"Let's check the class imbalance for the resulted training set."}}