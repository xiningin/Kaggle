{"cell_type":{"37aad7f5":"code","a0a7d129":"code","41551fed":"code","37ccdb6a":"code","49fb3b34":"code","d6e9a297":"code","f2d4b938":"code","07e8652a":"code","5789cd35":"code","0644666a":"code","f6f48769":"code","3307a7b8":"code","4d8d1cc1":"code","a151930d":"code","97c4c4ee":"code","482f2a5e":"code","f6bad070":"code","fd8564da":"code","18e657a2":"code","3f47f743":"code","46125775":"code","dc4d06a7":"code","108dcadc":"code","70d789c3":"code","d293c74d":"code","63b13f99":"code","842c105e":"code","d08f5c41":"code","76b5a38f":"code","509322d2":"code","4271d1ff":"markdown","19c36496":"markdown","e8901677":"markdown","67225aa8":"markdown","ba46c2e3":"markdown","7be85d4c":"markdown","9fce4d33":"markdown","33dc564c":"markdown","711f13e4":"markdown","814b8ea6":"markdown","ad7c507d":"markdown","3b3266db":"markdown","5fb77207":"markdown","b935bede":"markdown","9c8fd63b":"markdown","1f19f5a5":"markdown","947a5c75":"markdown","bb8d4063":"markdown","82f8fd2f":"markdown","893776cd":"markdown","708e89e8":"markdown","6d214832":"markdown","f9a48ffc":"markdown","612c82db":"markdown","c557d8a0":"markdown","d3a31de1":"markdown","f586bb28":"markdown","c412fc6d":"markdown","e6670c92":"markdown","725e3105":"markdown","8fcdd39e":"markdown","bcef3a5c":"markdown","d6f055de":"markdown"},"source":{"37aad7f5":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\nfrom matplotlib.cm import rainbow\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport os\nimport gc\n\nimport re\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","a0a7d129":"dataset=pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ndataset.head()","41551fed":"dataset.info()\n","37ccdb6a":"dataset.describe()\n#print(dataset['sex'].head())","49fb3b34":"print('~> Have not heart disease (target = 0):\\n   {}%'.format(100 - round(dataset['target'].mean()*100, 2)))\nprint('\\n~> Have heart disease (target= 1):\\n   {}%'.format(round(dataset['target'].mean()*100, 2)))","d6e9a297":"rcParams['figure.figsize'] = 20, 14\nplt.matshow(dataset.corr())\nplt.yticks(np.arange(dataset.shape[1]), dataset.columns)\nplt.xticks(np.arange(dataset.shape[1]), dataset.columns)\nplt.colorbar()","f2d4b938":"plt.figure(figsize=(6,6))\ndataset.groupby(\"target\").count().plot.bar()","07e8652a":"dataset.hist()","5789cd35":"import seaborn as sns\nsns.pairplot(dataset, palette='rainbow')","0644666a":"import seaborn as sns\nplt.figure(figsize=(12, 8))\nplt.subplot(1,2,1)\nsns.violinplot(x = 'target', y = 'chol', data = dataset[0:])\nplt.show()","f6f48769":"sns.lmplot(x='chol',y='target',data=dataset)","3307a7b8":"plt.figure(figsize=(5, 5))\nunique_variations = dataset['chol'].value_counts()\nprint('Number of Unique chol:', unique_variations.shape[0])\n# the top 10 variations that occured most\nprint(unique_variations.head(10))\ns = sum(unique_variations.values);\nh = unique_variations.values\/s;\nplt.plot(h, label=\"Histrogram of Variations\")\nplt.xlabel('Index of a chol')\nplt.ylabel('Number of Occurances')\nplt.legend()\nplt.grid()\nplt.show()\nc = np.cumsum(h)\nprint(c)\nplt.figure(figsize=(5, 5))\nplt.plot(c,label='Cumulative distribution of Variations')\nplt.grid()\nplt.legend()\nplt.show()","4d8d1cc1":"rcParams['figure.figsize'] = 8,6\nplt.bar(dataset['target'].unique(), dataset['target'].value_counts(), color = ['red', 'green'])\nplt.xticks([0, 1])\nplt.xlabel('Target Classes')\nplt.ylabel('Count')\nplt.title('Count of each Target Class')","a151930d":"nan_rows = dataset[dataset.isnull().any(1)]\nprint (nan_rows)","97c4c4ee":"categorical_feature_mask = dataset.dtypes==object\n# filter categorical columns using mask and turn it into alist\ncategorical_cols = dataset.columns[categorical_feature_mask].tolist()\nprint(categorical_cols)\nprint(\"number of categorical features \",len(categorical_cols))","482f2a5e":"standardScaler = StandardScaler()\ncolumns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\ndataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])","f6bad070":"y = dataset['target']\nX = dataset.drop(['target'], axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 0)","fd8564da":"# This function plots the confusion matrices given y_i, y_i_hat.\ndef plot_confusion_matrix(test_y, predict_y):\n    C = confusion_matrix(test_y, predict_y)\n    # C = 9,9 matrix, each cell (i,j) represents number of points of class i are predicted class j\n    \n    A =(((C.T)\/(C.sum(axis=1))).T)\n    \n    \n    B =(C\/C.sum(axis=0))\n  \n    plt.figure(figsize=(20,4))\n    \n    labels = [1,2]\n    # representing A in heatmap format\n    cmap=sns.light_palette(\"blue\")\n    plt.subplot(1, 3, 1)\n    sns.heatmap(C, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Confusion matrix\")\n    \n    plt.subplot(1, 3, 2)\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Precision matrix\")\n    \n    plt.subplot(1, 3, 3)\n    # representing B in heatmap format\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels)\n    plt.xlabel('Predicted Class')\n    plt.ylabel('Original Class')\n    plt.title(\"Recall matrix\")\n    \n    plt.show()","18e657a2":"knn_scores = []\nfor k in range(1,21):\n    knn_classifier = KNeighborsClassifier(n_neighbors = k)\n    knn_classifier.fit(X_train, y_train)\n    knn_scores.append(knn_classifier.score(X_test, y_test))","3f47f743":"from sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nplt.figure(figsize=(8,5))\nplt.plot([k for k in range(1, 21)], knn_scores, color = 'red')\nfor i in range(1,21):\n    plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]))\nplt.xticks([i for i in range(1, 21)])\nplt.xlabel('Number of Neighbors (K)')\nplt.ylabel('Scores')\nplt.title('K Neighbors Classifier scores for different K values')\n\n#############we get k best value that is 8\n#now again train our model\nclf=KNeighborsClassifier(n_neighbors=8)\nclf.fit(X_train,y_train)\nclf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nclf.fit(X_train, y_train)\npredict_y=clf.predict_proba(X_test)\nprint(\"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n#########Plot confusion atrix\nprint(len(predict_y))\n#print(len(y_test))\nplot_confusion_matrix(y_test, clf.predict(X_test))\n\n","46125775":"svc_scores = []\nkernels = ['linear', 'poly', 'rbf', 'sigmoid']\nfor i in range(len(kernels)):\n    svc_classifier = SVC(kernel = kernels[i])\n    svc_classifier.fit(X_train, y_train)\n    svc_scores.append(svc_classifier.score(X_test, y_test))","dc4d06a7":"\ncolors = rainbow(np.linspace(0, 1, len(kernels)))\nplt.bar(kernels, svc_scores, color = colors)\nfor i in range(len(kernels)):\n    plt.text(i, svc_scores[i], svc_scores[i])\nplt.xlabel('Kernels')\nplt.ylabel('Scores')\nplt.title('Support Vector Classifier scores for different kernels')\n##################### with best kernel \n\n\nclf=SVC(kernel ='linear')\n#clf.fit(X_train,y_train)\nclf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nclf.fit(X_train, y_train)\npredict_y=clf.predict_proba(X_test)\nprint(\"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n#########Plot confusion atrix\nprint(len(predict_y))\n#print(len(y_test))\nplot_confusion_matrix(y_test, clf.predict(X_test))","108dcadc":"dt_scores = []\nfor i in range(1, len(X.columns) + 1):\n    dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)\n    dt_classifier.fit(X_train, y_train)\n    dt_scores.append(dt_classifier.score(X_test, y_test))","70d789c3":"plt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')\nfor i in range(1, len(X.columns) + 1):\n    plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))\nplt.xticks([i for i in range(1, len(X.columns) + 1)])\nplt.xlabel('Max features')\nplt.ylabel('Scores')\nplt.title('Decision Tree Classifier scores for different number of maximum features')\n\nclf=DecisionTreeClassifier(max_features = 10, random_state = 0)\nclf.fit(X_train,y_train)\nclf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nclf.fit(X_train, y_train)\npredict_y=clf.predict_proba(X_test)\nprint(\"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n#########Plot confusion atrix\nprint(len(predict_y))\n#print(len(y_test))\nplot_confusion_matrix(y_test, clf.predict(X_test))","d293c74d":"rf_scores = []\nestimators = [10, 100, 200, 500, 1000]\nfor i in estimators:\n    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)\n    rf_classifier.fit(X_train, y_train)\n    rf_scores.append(rf_classifier.score(X_test, y_test))","63b13f99":"colors = rainbow(np.linspace(0, 1, len(estimators)))\nplt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)\nfor i in range(len(estimators)):\n    plt.text(i, rf_scores[i], rf_scores[i])\nplt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])\nplt.xlabel('Number of estimators')\nplt.ylabel('Scores')\nplt.title('Random Forest Classifier scores for different number of estimators')\n\nclf=RandomForestClassifier(n_estimators = 500, random_state = 0)\nclf.fit(X_train,y_train)\nclf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nclf.fit(X_train, y_train)\npredict_y=clf.predict_proba(X_test)\nprint(\"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\n#########Plot confusion atrix\nprint(len(predict_y))\n#print(len(y_test))\nplot_confusion_matrix(y_test, clf.predict(X_test))\n","842c105e":"import xgboost as xgb\nparams = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_test = xgb.DMatrix(X_test, label=y_test)\n\nwatchlist = [(d_train, 'train'), (d_test, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n\nxgdmat = xgb.DMatrix(X_train,y_train)\npredict_y = bst.predict(d_test)\nprint(\"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nprint(len(predict_y))\n#print(len(y_test))\nplot_confusion_matrix(y_test, clf.predict(X_test))","d08f5c41":"\nalpha = [10 ** x for x in range(-5, 2)] # hyperparam for SGD classifier.\n\n\n\nlog_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train, y_train)\n    sig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    sig_clf.fit(X_train, y_train)\n    predict_y = sig_clf.predict_proba(X_test)\n    log_error_array.append(log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\n\nfig, ax = plt.subplots()\nax.plot(alpha, log_error_array,c='g')\nfor i, txt in enumerate(np.round(log_error_array,3)):\n    ax.annotate((alpha[i],np.round(txt,3)), (alpha[i],log_error_array[i]))\nplt.grid()\nplt.title(\"Cross Validation Error for each alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Error measure\")\nplt.show()\n\n\nbest_alpha = np.argmin(log_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train, y_train)\nsig_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\nsig_clf.fit(X_train, y_train)\n\npredict_y = sig_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, predict_y, labels=clf.classes_, eps=1e-15))\npredict_y = sig_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, predict_y, labels=clf.classes_, eps=1e-15))\npredicted_y =np.argmax(predict_y,axis=1)\nprint(\"Total number of data points :\", len(predicted_y))\nplot_confusion_matrix(y_test, predicted_y)","76b5a38f":"clf=RandomForestClassifier(n_estimators = 500, random_state = 0)\nclf.fit(X_train,y_train)\nfeatures = X_train.columns\nimportances = clf.feature_importances_\nindices = (np.argsort(importances))[-25:]\nplt.figure(figsize=(10,12))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='r', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","509322d2":"error_rate=np.array([0.3828,0.4158,0.3972,0.5409,0.3803,0.4135])\nplt.figure(figsize=(16,5))\nprint(error_rate)\n\n#plt.scatter(error_rate,range(1,7))\n#seed = 7\n# prepare models\nmodels = ['LR','XGBOOST','RF','DT','SVM','KNN']\nplt.xlabel(models)\nplt.plot(error_rate)\nlowest_loss=np.argmin(error_rate)\nprint(\"lowest logg loss : \",min(error_rate))\nprint(models[lowest_loss])","4271d1ff":"### Conclusion\n\nIn this project, I used Machine Learning to predict whether a person is suffering from a heart disease. After importing the data, I analysed it using plots. Then, I did generated dummy variables for categorical features and scaled other features. (one hot encoding)\nI then applied four Machine Learning algorithms, `K Neighbors Classifier`, `Support Vector Classifier`, `Decision Tree Classifier`,`XGBOOST`,`logistic Regression`, `Random Forest Classifier`. I varied parameters across each model to improve their scores.\nIn the end, `Support Vector Classifier` achieved the lowest score of log loss of `0.3803`.","19c36496":"Since we cannot find categorical features by code then we have to select manually like this\n\n","e8901677":"#### Decision Tree Classifier\n\nHere, I'll use the Decision Tree Classifier to model the problem at hand. I'll vary between a set of `max_features` and see which returns the best accuracy.","67225aa8":"The scale of each feature column is different and quite varied as well. While the maximum for `age` reaches 77, the maximum of `chol` (serum cholestoral) is 564.","ba46c2e3":"# Performance metrics\n**Confusion,**\n**Recall,**\n**Precision**","7be85d4c":"**Univarirate Analysis of chol feature**<br\/>\nsame can be done for few more features","9fce4d33":"The `linear` kernel performed the best, being slightly better than `rbf` kernel.","33dc564c":"The model achieved the best accuracy at three values of maximum features, `2`, `4` and `18`.","711f13e4":"# Machine Learning Model\n\nI'll now import `train_test_split` to split our dataset into training and testing datasets. Then, I'll import all Machine Learning models I'll be using to train and test the data.","814b8ea6":"# Data Processing\n\nAfter exploring the dataset, I observed that I need to convert some categorical variables into dummy variables and scale all the values before training the Machine Learning models.\nFirst, I'll use the `get_dummies` method to create dummy columns for categorical variables.","ad7c507d":"#### Support Vector Classifier\n\nThere are several kernels for Support Vector Classifier. I'll test some of them and check which has the best score.","3b3266db":"Looks like the dataset has a total of 303 rows and there are no missing values. There are a total of `13 features` along with one target value which we wish to find.","5fb77207":"It's always a good practice to work with a dataset where the target classes are of approximately equal size. Thus, let's check for the same.","b935bede":"# Heart Disease Prediction Model\n\nIn this machine learning project, I have collected the dataset from Kaggle (https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci) and I will be using Machine Learning to make predictions on whether a person is suffering from Heart Disease or not.","9c8fd63b":"The model is trained and the scores are recorded. Let's plot a bar plot to compare the scores.","1f19f5a5":"Taking a look at the histograms above, I can see that each feature has a different range of distribution. Thus, using scaling before our predictions should be of great use. Also, the categorical features do stand out.","947a5c75":"From the plot above, it is clear that the maximum score achieved was `0.87` for the 8 neighbors.","bb8d4063":"# Comparing Models","82f8fd2f":"#### K Neighbors Classifier\n\nThe classification score varies based on different values of neighbors that we choose. Thus, I'll plot a score graph for different values of K (neighbors) and check when do I achieve the best score.","893776cd":"The dataset is now loaded into the variable `dataset`. I'll just take a glimpse of the data using the `desribe()` and `info()` methods before I actually start processing and visualizing it.","708e89e8":"I selected the maximum number of features from 1 to 30 for split. Now, let's see the scores for each of those cases.","6d214832":"I'll now plot a bar plot of scores for each kernel and see which performed the best.","f9a48ffc":"#### XGBOOST\n\nNow, I'll use the ensemble method, XGBOOST , to create the model and vary the number of estimators to see their effect.","612c82db":"#### Logistic Regression\n\nNow, I'll use the ensemble method, Logistic Regression , to create the model and vary the number of estimators to see their effect.","c557d8a0":"The two classes are not exactly 50% each but the ratio is good enough to continue without dropping\/increasing our data.","d3a31de1":"I have the scores for different neighbor values in the array `knn_scores`. I'll now plot it and see for which value of K did I get the best scores.","f586bb28":"# Exploratory Data Analysis\n\nNow, we can use visualizations to better understand our data and then look at any processing we might want to do.","c412fc6d":"# Import dataset\n\nNow that we have all the libraries we will need, I can import the dataset and take a look at it. The dataset is stored in the file `dataset.csv`. I'll use the pandas `read_csv` method to read the dataset.","e6670c92":"### Import libraries\n\nLet's first import all the necessary libraries. I'll use `numpy` and `pandas` to start with. For visualization, I will use `pyplot` subpackage of `matplotlib`, use `rcParams` to add styling to the plots and `rainbow` for colors. For implementing Machine Learning models and processing of data, I will use the `sklearn` library.\n\n\n### Data Overview\nTaken data from facebook's recruting challenge on kaggle \ndata contains two columns source and destination eac edge in graph \n    - Data columns (total 13 columns):  \n    \n  \n### Performance metric for supervised learning:  \n- Both precision and recall is important \n- Confusion matrix\n\n","725e3105":"Now, I will use the `StandardScaler` from `sklearn` to scale my dataset.","8fcdd39e":"# Feature Importance","bcef3a5c":"#### Random Forest Classifier\n\nNow, I'll use the ensemble method, Random Forest Classifier, to create the model and vary the number of estimators to see their effect.","d6f055de":"Taking a look at the correlation matrix above, it's easy to see that a few features have negative correlation with the target value while some have positive.\nNext, I'll take a look at the histograms for each variable."}}