{"cell_type":{"8812221b":"code","95c59d06":"code","f6785a77":"code","b605ac09":"code","beb986cb":"code","f5e608fd":"code","c76af7d0":"code","8149a457":"code","2e14d620":"code","ef467814":"code","a2d3f4c6":"code","720fdd5a":"code","6dfcf8e7":"code","355208e7":"code","5f029483":"code","25c2260e":"code","7a397aad":"code","5cd429ba":"code","e3a1517f":"code","f7d8df94":"code","794420e8":"code","73e84820":"code","7b7531ce":"code","f8900114":"code","11fd014d":"code","763dde67":"code","8e40f8f1":"code","176a142d":"code","ec28c1c1":"code","12b26c16":"code","dab9fb9e":"code","40434b77":"code","40848322":"code","4beddf61":"code","135310fd":"markdown","59b46ac6":"markdown","df6eaac0":"markdown","b10183cf":"markdown","4c2a187b":"markdown","e8a806e3":"markdown","316407ff":"markdown","2687bda7":"markdown","3d84ec3d":"markdown","b1f34220":"markdown"},"source":{"8812221b":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\nfrom keras.layers import Input, Dense, Reshape, Flatten, Dropout, LeakyReLU\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D, UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam\nfrom keras import backend as K\n\nimport os\nimport argparse\nimport glob \n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport sys\n\nimport numpy as np\n\nTRANSPARENT_GEMS = [\n    'Garnet Red', 'Diamond', 'Quartz Rutilated', 'Topaz', 'Quartz Smoky', 'Citrine', 'Tanzanite', 'Tsavorite',\n    'Hessonite', 'Quartz Lemon', 'Peridot', 'Ametrine', 'Sphene', 'Morganite', 'Zircon', 'Grossular', 'Benitoite',\n    'Diaspore', 'Rhodolite', 'Iolite', 'Pyrope', 'Amethyst', 'Almandine', 'Sapphire Blue', 'Spessartite',\n    'Chrome Diopside', 'Tourmaline', 'Quartz Beer', 'Chrysoberyl', 'Sapphire Yellow', 'Andradite', 'Kyanite',\n    'Andalusite', 'Beryl Golden', 'Danburite', 'Kunzite', 'Quartz Rose', 'Sapphire Pink', 'Aquamarine',\n    'Sapphire Purple', 'Alexandrite', 'Spodumene', 'Ruby', 'Emerald', 'Hiddenite', 'Goshenite', 'Bixbite'\n]\n\nclass DCGAN():\n    def __init__(self, img_rows=128, img_cols=128, channels=4, latent_dim=3, loss='binary_crossentropy', name='earth'):\n        self.name = name\n\n        # Input shape\n        self.img_rows = img_rows\n        self.img_cols = img_cols\n        self.channels = channels\n        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n        self.latent_dim = latent_dim\n        self.loss = loss\n\n        self.optimizer = Adam(0.0005, 0.6)\n        #self.optimizer = Adam(0.0002, 0.5)\n\n        # Build and compile the discriminator\n        self.discriminator = self.build_discriminator()\n\n        # Build the generator\n        self.generator = self.build_generator()\n\n        # Build the GAN\n        self.build_combined()\n        \n    def build_combined(self):\n        self.discriminator.compile(loss='binary_crossentropy',\n                optimizer=self.optimizer,\n                metrics=['accuracy'])\n        \n        # The generator takes noise as input and generates imgs\n        z = Input(shape=(self.latent_dim,))\n        img = self.generator(z)\n\n        # For the combined model we will only train the generator\n        self.discriminator.trainable = False\n\n        # The discriminator takes generated images as input and determines validity\n        valid = self.discriminator(img)\n        # The combined model  (stacked generator and discriminator)\n        # Trains the generator to fool the discriminator\n        self.combined = Model(z, valid)\n        self.combined.compile(loss=self.loss, optimizer=self.optimizer)    \n    \n    def load_weights(self, generator_file=None, discriminator_file=None):\n\n        if generator_file:\n            generator = self.build_generator()\n            generator.load_weights(generator_file)\n            self.generator = generator\n            print('generator weights loaded')\n    \n        if discriminator_file:\n            discriminator = self.build_discriminator()\n            discriminator.load_weights(discriminator_file)\n            self.discriminator = discriminator\n            print('discriminator weights loaded')\n\n        if generator_file or discriminator_file: \n            self.build_combined() \n            print('build compaied ')\n\n    def build_generator(self):\n\n        model = Sequential()\n        #model.add(Dense(128, activation=\"relu\", input_dim=self.latent_dim, name=\"generator_input\") )\n        #model.add(Dropout(0.1))\n        \n        #model.add(Dense(128 * 16 * 16, activation=\"relu\", input_dim=self.latent_dim, name=\"generator_input\") )\n        model.add(Dense(128 * 32 * 32, activation=\"relu\", input_dim=self.latent_dim, name=\"generator_input\"))\n        model.add(Dropout(0.1))\n        #model.add(Reshape((16, 16, 128)))\n        model.add(Reshape((32, 32, 128)))\n        #model.add(UpSampling2D())\n\n        model.add(Conv2D(128, kernel_size=5, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n        model.add(Dropout(0.1))\n        model.add(UpSampling2D())\n        \n        model.add(Conv2D(128, kernel_size=5, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n        model.add(UpSampling2D())\n\n        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n        \n        model.add(Conv2D(64, kernel_size=4, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n        \n        #model.add(UpSampling2D())\n\n        model.add(Conv2D(self.channels, kernel_size=3, padding=\"same\", activation=\"sigmoid\", name=\"generator_output\"))\n\n        model.summary()\n\n        noise = Input(shape=(self.latent_dim,))\n        img = model(noise)\n\n        return Model(noise, img, name=\"generator\")\n\n    def build_discriminator(self):\n\n        model = Sequential()\n\n        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=self.img_shape, padding=\"same\"))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Conv2D(128, kernel_size=3, strides=1, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        model.add(Flatten())\n\n        #model.add(Dense(32, activation='relu'))\n        model.add(Dense(1, activation='sigmoid'))\n\n        model.summary()\n\n        img = Input(shape=self.img_shape)\n        validity = model(img)\n\n        discrim = Model(img, validity)\n\n        return discrim\n\n    def train(self, X_train, epochs, batch_size=128, save_interval=100):\n\n        # Adversarial ground truths\n        valid = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n\n        for epoch in range(epochs):\n\n            # ---------------------\n            #  Train Discriminator\n            # ---------------------\n\n            # Select a random half of images\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            imgs = X_train[idx]\n\n            # Sample noise and generate a batch of new images\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_imgs = self.generator.predict(noise)\n\n            # Train the discriminator (real classified as ones and generated as zeros)\n            d_loss_real = self.discriminator.train_on_batch(imgs, valid)\n            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n            # ---------------------\n            #  Train Generator\n            # ---------------------\n\n            # Train the generator (wants discriminator to mistake images as real)\n            g_loss = self.combined.train_on_batch(noise, valid)\n\n            # Plot the progress\n            if epoch % 10 == 0:\n                print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n\n            # If at save interval => save generated image samples\n            if epoch % save_interval == 0:\n                if not os.path.exists('\/kaggle\/working\/images'):\n                    os.mkdir('\/kaggle\/working\/images')\n        \n                self.save_imgs( \"\/kaggle\/working\/images\/{}_{:05d}.png\".format(self.name,epoch) )\n                # self.combined.save_weights(\"combined_weights ({}).h5\".format(self.name)) # https:\/\/github.com\/keras-team\/keras\/issues\/10949\n                self.generator.save_weights(\"\/kaggle\/working\/generator ({}).h5\".format(self.name))\n                self.discriminator.save_weights(\"\/kaggle\/working\/discriminator ({}).h5\".format(self.name))\n\n    def save_imgs(self, name=''):\n        r, c = 4, 4\n        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n\n        # replace the first two latent variables with known values\n        #for i in range(r):\n        #    for j in range(c):\n        #        noise[4*i+j][0] = i\/(r-1)-0.5\n        #        noise[4*i+j][1] = j\/(c-1)-0.5\n\n        gen_imgs = self.generator.predict(noise)\n\n        fig, axs = plt.subplots(r, c, figsize=(6.72,6.72))\n        plt.subplots_adjust(left=0.05,bottom=0.05,right=0.95,top=0.95, wspace=0.2, hspace=0.2)\n\n        cnt = 0\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(gen_imgs[cnt])\n                axs[i,j].axis('off')\n                cnt += 1\n\n        if name:\n            fig.savefig(name, facecolor='black' )\n        else: \n            fig.savefig('{}.png'.format(self.name), facecolor='black' )\n\n        plt.close()\n    \n\ndef export_model(saver, model, model_name, input_node_names, output_node_name):\n    from tensorflow.python.tools import freeze_graph\n    from tensorflow.python.tools import optimize_for_inference_lib\n    \n    if not os.path.exists('\/kaggle\/working\/out'):\n        os.mkdir('\/kaggle\/working\/out')\n\n    tf.train.write_graph(K.get_session().graph_def, '\/kaggle\/working\/out', model_name + '_graph.pbtxt')\n\n    saver.save(K.get_session(), 'out\/' + model_name + '.chkp')\n\n    freeze_graph.freeze_graph('\/kaggle\/working\/out\/' + model_name + '_graph.pbtxt', None, False,\n                              '\/kaggle\/working\/out\/' + model_name + '.chkp', output_node_name,\n                              \"save\/restore_all\", \"save\/Const:0\",\n                              '\/kaggle\/working\/out\/frozen_' + model_name + '.bytes', True, \"\")\n\n    input_graph_def = tf.GraphDef()\n    with tf.gfile.Open('\/kaggle\/working\/out\/frozen_' + model_name + '.bytes', \"rb\") as f:\n        input_graph_def.ParseFromString(f.read())\n\n    output_graph_def = optimize_for_inference_lib.optimize_for_inference(\n            input_graph_def, input_node_names, [output_node_name],\n            tf.float32.as_datatype_enum)\n\n    with tf.gfile.FastGFile('\/kaggle\/working\/out\/opt_' + model_name + '.bytes', \"wb\") as f:\n        f.write(output_graph_def.SerializeToString())\n\n    print(\"graph saved!\")","95c59d06":"import numpy as np\nimport pandas as pd\n\nimport os\n\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import array_to_img\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg","f6785a77":"train_images = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/gemstones-images\/train'):\n    if dirname.split('\/')[-1] in TRANSPARENT_GEMS:\n        for filename in filenames:\n            train_images.append(os.path.join(dirname, filename))\n        \n#test_images = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/gemstones-images\/test'):\n    if dirname.split('\/')[-1] in TRANSPARENT_GEMS:\n        for filename in filenames:\n            train_images.append(os.path.join(dirname, filename))","b605ac09":"SCALE = 128\n\nall_img = []\nfor image in train_images:\n        _,extension = os.path.splitext(image)\n        if(extension==\".jpg\" or extension==\".jpeg\" or extension==\".png\"):\n            img=load_img(image)\n            img=img.resize((SCALE,SCALE), Image.LANCZOS)\n            x=img_to_array(img)\n            all_img.append(x)\n\n'''\nall_img_test = []\nfor image in test_images:\n        _,extension = os.path.splitext(image)\n        if(extension==\".jpg\" or extension==\".jpeg\" or extension==\".png\"):\n            img=load_img(image)\n            img=img.resize((SCALE,SCALE), Image.LANCZOS)\n            x=img_to_array(img)\n            all_img_test.append(x)   \n'''","beb986cb":"all_img=np.asarray(all_img,dtype=\"float\")\n#all_img_test=np.asarray(all_img_test,dtype=\"float\")\n\ntrain=all_img\/255\n#test=all_img_test\/255","f5e608fd":"fig, axs = plt.subplots(4, 4)\nfor i in range(4):\n    for j in range(4):\n        axs[i,j].imshow( train[ np.random.randint(train.shape[0]) ] )\n        axs[i,j].axis('off')\nplt.show()","c76af7d0":"dcgan = DCGAN(img_rows = train[0].shape[0],\n                    img_cols = train[0].shape[1],\n                    channels = train[0].shape[2], \n                    latent_dim = 256,\n                    name='gems_128_256_new')\ndcgan.train(train, epochs=30000, batch_size=32, save_interval=500)","8149a457":"dcgan = DCGAN(img_rows = train[0].shape[0],\n                    img_cols = train[0].shape[1],\n                    channels = train[0].shape[2], \n                    latent_dim = 256)\ndcgan.load_weights(\n    generator_file='\/kaggle\/input\/gem-gans\/generator (gems_256_128_30000).h5',\n    discriminator_file='\/kaggle\/input\/gem-gans\/discriminator (gems_256_128_30000).h5'\n)","2e14d620":"noise = np.random.normal(0, 1, (16, dcgan.latent_dim))\ngen_imgs = dcgan.generator.predict(noise)","ef467814":"fig, axs = plt.subplots(4, 4)\nfor i in range(4):\n    for j in range(4):\n        axs[i,j].imshow(gen_imgs[i*4+j])\n        axs[i,j].axis('off')\nplt.show()","a2d3f4c6":"plt.imshow(gen_imgs[0])","720fdd5a":"import matplotlib\nimport cv2\n\ndef remove_background(image_array):\n    matplotlib.image.imsave(\"\/kaggle\/working\/image1.png\", image_array)\n    \n    #== Parameters           \n    BLUR = 21\n    CANNY_THRESH_1 = 100\n    CANNY_THRESH_2 = 220\n    MASK_DILATE_ITER = 5\n    MASK_ERODE_ITER = 7\n    MASK_COLOR = (0.0,0.0,0.0) # In BGR format\n\n\n    #-- Read image\n    img = cv2.imread(\"\/kaggle\/working\/image1.png\")\n    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n\n    #-- Edge detection \n    edges = cv2.Canny(gray, CANNY_THRESH_1, CANNY_THRESH_2)\n    edges = cv2.dilate(edges, None)\n    edges = cv2.erode(edges, None)\n\n    #-- Find contours in edges, sort by area \n    contour_info = []\n    contours, _ = cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n    for c in contours:\n        contour_info.append((\n            c,\n            cv2.isContourConvex(c),\n            cv2.contourArea(c),\n        ))\n    contour_info = sorted(contour_info, key=lambda c: c[2], reverse=True)\n    max_contour = contour_info[0]\n\n    #-- Create empty mask, draw filled polygon on it corresponding to largest contour ----\n    # Mask is black, polygon is white\n    mask = np.zeros(edges.shape)\n    cv2.fillConvexPoly(mask, max_contour[0], (255))\n\n    #-- Smooth mask, then blur it\n    mask = cv2.dilate(mask, None, iterations=MASK_DILATE_ITER)\n    mask = cv2.erode(mask, None, iterations=MASK_ERODE_ITER)\n    mask = cv2.GaussianBlur(mask, (BLUR, BLUR), 0)\n    mask_stack = np.dstack([mask]*3)    # Create 3-channel alpha mask\n\n    #-- Blend masked img into MASK_COLOR background\n    mask_stack  = mask_stack.astype('float32') \/ 255.0         \n    img         = img.astype('float32') \/ 255.0    \n    masked = (mask_stack * img) + ((1-mask_stack) * MASK_COLOR)  \n    masked = (masked * 255).astype('uint8')\n    #matplotlib.image.imsave(\"\/kaggle\/working\/image2.png\", cv2.cvtColor(masked,cv2.COLOR_BGR2RGB))\n    \n    # split image into channels\n    c_red, c_green, c_blue = cv2.split(img)\n\n    # merge with mask got on one of a previous steps\n    img_a = cv2.merge((c_blue, c_green, c_red, mask.astype('float32') \/ 255.0))\n    #matplotlib.image.imsave(\"\/kaggle\/working\/image2.png\", img_a)\n\n    return img_a, masked","6dfcf8e7":"from pylab import rcParams\nrcParams['figure.figsize'] = 9, 9","355208e7":"from PIL import Image\n\n# Open Paddington\nimg = Image.open(\"\/kaggle\/input\/art-and-nature-pics\/lavender.jpg\")\n\n# Resize smoothly down to 16x16 pixels\nimgSmall = img.resize((round(img.width\/200),round(img.height\/200)),resample=Image.BILINEAR)\n\n# Scale back up using NEAREST to original size\nresult = imgSmall.resize(img.size,Image.NEAREST)\n\n# Save\nresult.save('\/kaggle\/working\/pixelated.png')","5f029483":"plt.imshow(imgSmall)","25c2260e":"imgArr = np.array(imgSmall)","7a397aad":"imgArr.shape[0] * imgArr.shape[1]","5cd429ba":"color = imgArr[6,6]","e3a1517f":"!pip install colorthief","f7d8df94":"from colorthief import ColorThief","794420e8":"color_thief = ColorThief('\/kaggle\/working\/image2.png')\ndominant_color = color_thief.get_color(quality=1)","73e84820":"dominant_color","7b7531ce":"from IPython import display\n\nTARGET = 1600\nBATCH_SIZE = 9\ncounter = 495\ngem_colors = []\n\nwhile counter<TARGET:\n    print(counter)\n    noise = np.random.normal(0, 1, (BATCH_SIZE, dcgan.latent_dim))\n    gen_imgs = dcgan.generator.predict(noise)\n    clean_imgs = []\n    black_imgs = []\n    for j in range(BATCH_SIZE):\n        img = gen_imgs[j]\n        img_clean, img_black = remove_background(img)\n        clean_imgs.append(img_clean)\n        black_imgs.append(img_black)\n            \n    fig, axs = plt.subplots(3, 3)\n    for i in range(3):\n        for j in range(3):\n            axs[i,j].imshow(black_imgs[i*3+j])\n            axs[i,j].axis('off')\n    plt.show()\n    \n    selected = input()\n    if len(selected)>0:\n        for s in map(int, selected.split(',')):\n            matplotlib.image.imsave(\"\/kaggle\/working\/gem_{}.png\".format(counter), clean_imgs[s-1])\n            counter+=1\n    \n    display.clear_output(wait=True)","f8900114":"!pip install colorthief\nfrom colorthief import ColorThief\n\ngems = []\ngems_colors = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/gems-curated'):\n    for filename in filenames:\n        gems.append(filename)\n        color_thief = ColorThief(filename)\n        dominant_color = color_thief.get_color(quality=1)\n        gems_colors.append(dominant_color)","11fd014d":"matches = np.zeros((imgArr.shape[0], imgArr.shape[1]), dtype=int)\ntaken = []\n\nfor i in range(imgArr.shape[0]):\n    for j in range(imgArr.shape[1]):\n        color = imgArr[i,j]\n        #if sum(color) < 150:\n            #matches[i, j] = -1\n            #continue\n        min_dist = np.inf\n        for jj, c in enumerate(gems_colors):\n            if jj in taken:\n                continue\n            else:\n                dist = (((color[0] - c[0])**2 + (color[1] - c[1])**2 + (color[2] - c[2])**2)*1.0)**0.5\n                if dist < min_dist:\n                    min_index = jj\n                    min_dist = dist\n        matches[i,j] = min_index\n        taken.append(min_index)","763dde67":"matches","8e40f8f1":"final = Image.new('RGB', (imgArr.shape[1] * 256, imgArr.shape[0] * 256), (32, 32, 32))\nfor i in range(imgArr.shape[0]):\n    for j in range(imgArr.shape[1]):\n        match = matches[i,j]\n        if match == -1:\n            continue        \n        match_img = Image.open(gems[match])\n        final.paste(match_img, (j*256, i*256), match_img)\n\nfinal.save(\"\/kaggle\/working\/final.png\")","176a142d":"def rgb_to_hsv(r, g, b):\n    r, g, b = r\/255.0, g\/255.0, b\/255.0\n    mx = max(r, g, b)\n    mn = min(r, g, b)\n    df = mx-mn\n    if mx == mn:\n        h = 0\n    elif mx == r:\n        h = (60 * ((g-b)\/df) + 360) % 360\n    elif mx == g:\n        h = (60 * ((b-r)\/df) + 120) % 360\n    elif mx == b:\n        h = (60 * ((r-g)\/df) + 240) % 360\n    if mx == 0:\n        s = 0\n    else:\n        s = (df\/mx)*100\n    v = mx*100\n    return h, s, v\n\nprint(rgb_to_hsv(255, 255, 255))\nprint(rgb_to_hsv(0, 215, 0))","ec28c1c1":"gems_hues = list(map(lambda c: rgb_to_hsv(*c)[0], gems_colors))","12b26c16":"gems_sorted = [x for _,x in sorted(zip(gems_hues,gems))]","dab9fb9e":"len(gems_sorted)**0.5","40434b77":"final = Image.new('RGB', (192 * 22, 192 * 22), (255, 255, 255))\nfor i in range(22):\n    for j in range(22):     \n        match_img = Image.open(gems_sorted[22*i+j])\n        final.paste(match_img, (32+j*192, 32+i*192), match_img)\n        \nfinal.save(\"\/kaggle\/working\/final.png\")","40848322":"im = Image.open(\"\/kaggle\/working\/final.png\")\nimArr = np.array(im)","4beddf61":"for i in range(22-5):\n    for j in range(22-5):\n        img_part = imArr[i*192:(i+5)*192, j*192:(j+5)*192]\n        matplotlib.image.imsave(\"\/kaggle\/working\/final_{}_{}.png\".format(i,j), img_part)","135310fd":"# Hue sort","59b46ac6":"# Getting gem colors","df6eaac0":"# Generating images, gems selection, getting gem colors","b10183cf":"# Background removal","4c2a187b":"# Loading and applying best model","e8a806e3":"# Getting gem colors","316407ff":"# Training","2687bda7":"# Creating the final image","3d84ec3d":"# Matching colors","b1f34220":"# Image pixelating"}}