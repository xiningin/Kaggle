{"cell_type":{"a90de8ff":"code","89178439":"code","1273de91":"code","bbe53b5b":"code","2f33430d":"code","552cab93":"code","842bc550":"code","3b432998":"code","87c5b40c":"code","736beb48":"code","de60fa90":"code","6dae1389":"code","905d658d":"code","e03326ab":"code","63a6c082":"code","10b72357":"code","bd6b89bf":"code","223bbcdf":"code","0b95f941":"code","e6dc166d":"code","b3c3d12f":"code","5eb3e12d":"code","22716392":"code","64806f78":"code","02895011":"code","747ded5c":"code","51c1d4b8":"code","2c1453d8":"code","a07c3017":"code","325ab8a9":"code","d1f25566":"code","1bc0a4bd":"code","28c80e82":"code","279a8a62":"code","dec4371e":"code","1162f4fc":"code","f955106d":"code","d581d606":"code","86b9c83d":"code","9cdae3de":"code","83c62d0d":"code","376aa220":"code","0f882cf6":"code","9e92ca5d":"code","b1108c5e":"code","f45b1a7a":"code","1d2185c1":"code","0b8868a5":"code","05d743bc":"code","1b9f972a":"code","4fdf3536":"code","c11a3dc3":"code","a2e4aa8e":"code","0c7e9885":"code","c62cebcf":"code","2aca8217":"code","06fd7a4c":"code","977b3c83":"code","0c21cbea":"code","b3897e87":"code","9a10df4a":"code","9746012b":"code","f203531d":"code","6f646591":"code","5872298c":"code","d3ff6c95":"code","bc7246f0":"code","f8b6c3c4":"code","148e7480":"code","23013b9f":"code","3c21ee40":"code","aa453d45":"code","73e33f36":"code","81a6a43a":"code","fe51ac98":"code","dfa9f8fa":"code","0bc2b977":"code","24f25b8f":"code","f4f3b58d":"code","fd926212":"code","23ef1822":"code","8b6c38a6":"code","22b61484":"code","f00d3f47":"code","e138239d":"code","a9fa1b21":"code","31841d59":"code","515eef1c":"code","c1d578d7":"code","4ad71d68":"code","e9598fe7":"code","07b63200":"code","a97487f6":"code","b4f1f7a0":"code","f9165f97":"code","e8accfb4":"code","34d4337d":"code","07ceb304":"code","c0c952fb":"code","efc7c091":"code","e9d4d987":"code","e1fd2912":"code","6ea7547f":"code","1b9673da":"code","7f9d7711":"code","f4a84bc2":"code","5640d672":"code","f7478845":"code","f5d188ff":"code","2e157de9":"code","ff89e089":"code","e61bc4c4":"code","4f746173":"code","7d94844a":"code","dda08715":"code","63b2c044":"code","5f5c2818":"code","56e0faf3":"code","be1b2590":"code","fb8320da":"code","454fdd1f":"code","9c9e16be":"code","ae6b7fa8":"code","7821c379":"code","ea591211":"code","77e5d138":"code","f3a1b066":"code","b43dfed5":"code","c6dfb0ee":"code","43162e84":"code","0271e67d":"code","00897c54":"code","168bdc92":"code","0cf98343":"code","175810f6":"code","f4b6d2be":"code","593702eb":"code","28e2c89f":"code","a6ae1b0a":"code","bd8c6b38":"code","703e9b0b":"code","45d6895d":"code","aaecceda":"code","3e62f359":"code","64756902":"code","f9085f3a":"code","88b5fc2d":"code","2671e57f":"code","2fe3f972":"code","d681eb7b":"code","aac1085f":"code","1a708274":"code","c5938f48":"code","221217b0":"code","2643d84d":"code","98e4af78":"code","51bd9f97":"code","e82c9cea":"code","e9299559":"code","709db611":"code","f5e8d33c":"markdown","072d9f48":"markdown","7f0e14b8":"markdown","c45741eb":"markdown","c63221be":"markdown","5987d6f5":"markdown","c558e091":"markdown","ff9fc437":"markdown","cd23e563":"markdown","c23f1e96":"markdown","b14a1c7a":"markdown","8218c612":"markdown","ec0b138d":"markdown","db4715ec":"markdown","9cd82b3a":"markdown","32199d4b":"markdown","db2ae5ff":"markdown","c1edd4ac":"markdown","8e1d360e":"markdown","88274a23":"markdown","f3fbc79d":"markdown","f8217c0c":"markdown","ba19aef7":"markdown","3bcaf3ea":"markdown","fa01ac6d":"markdown","dd0dfb8c":"markdown","74acb2ea":"markdown","f26cd8c0":"markdown","027fcd51":"markdown","2e5b01cb":"markdown","40dbc862":"markdown","872d155e":"markdown","68e191cf":"markdown","7261924e":"markdown","216c2888":"markdown","2dff9d0d":"markdown","8c875748":"markdown","67181f24":"markdown","ced14bc9":"markdown","999d1028":"markdown","5b1cf56d":"markdown","00d380d3":"markdown","efafb648":"markdown","2ae25d56":"markdown","e8eb7048":"markdown","5dc1fc66":"markdown","7494ba59":"markdown","77961006":"markdown","0297affb":"markdown","b15ed2d7":"markdown","d3b9ba78":"markdown","bee20ce6":"markdown","4e4af351":"markdown","21e1c6d8":"markdown","2335f4fe":"markdown","cef0894e":"markdown","7596745e":"markdown","34f151a8":"markdown","9150daa1":"markdown","cb0d9faa":"markdown","2b13f7d0":"markdown","0b615db7":"markdown","50f92579":"markdown","10d90201":"markdown","3fa74d50":"markdown","604f95a9":"markdown","4be02a1b":"markdown","1957a8f3":"markdown","ea206e8e":"markdown","1b2687df":"markdown","ceceffc8":"markdown","843c79a7":"markdown","09b9aa8e":"markdown","a80e5f21":"markdown","7243106b":"markdown","39d5d953":"markdown","b959094b":"markdown","b2c44322":"markdown","e1701db2":"markdown","63218492":"markdown","f6f15f0b":"markdown","a81a280c":"markdown","6b20e767":"markdown","b17a2579":"markdown","883091ba":"markdown","9d4fc9d0":"markdown","71903aa2":"markdown","91e29cb3":"markdown","ab48d59d":"markdown","496e8fcf":"markdown","b0b911b0":"markdown","9d395aab":"markdown","23d1fe0c":"markdown","44c59f5f":"markdown","9dca27bf":"markdown","8aa8514e":"markdown","85a62438":"markdown","d71b50b3":"markdown","b9e8883f":"markdown","92175b08":"markdown","2df7e4cb":"markdown","9bc859f0":"markdown","32c43195":"markdown","3bb5f32d":"markdown","4ae54fb6":"markdown","850fa967":"markdown","a995765f":"markdown","5041dd12":"markdown","f977c50e":"markdown","c8e43182":"markdown","9031b0b0":"markdown","072897ac":"markdown","58a58997":"markdown","2753cb87":"markdown","249b98e5":"markdown","db544f68":"markdown","08482aa6":"markdown","363a88e4":"markdown","3b66d23b":"markdown"},"source":{"a90de8ff":"# from google.colab import drive\n# drive.mount('\/content\/drive')","89178439":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom nltk.stem.porter import PorterStemmer\n\nimport re\n# Tutorial about Python regular expressions: https:\/\/pymotw.com\/2\/re\/\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nfrom gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nimport pickle\n\nfrom tqdm import tqdm\nimport os\n\n# from plotly import plotly\nimport plotly.offline as offline\nimport plotly.graph_objs as go\noffline.init_notebook_mode()\nfrom collections import Counter\nfrom sklearn import linear_model\nfrom sklearn.decomposition import TruncatedSVD\n# import os\n# print(os.listdir(\"..\/input\"))\nprint(\"DONE ---------------------------------------\")","1273de91":"# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))","bbe53b5b":"# project_data = pd.read_csv('\/content\/drive\/My Drive\/Colab Notebooks\/train_data.csv') \n# resource_data = pd.read_csv('\/content\/drive\/My Drive\/Colab Notebooks\/resources.csv')\nproject_data = pd.read_csv('\/kaggle\/input\/donorschoosedataset\/train_data.csv') \nresource_data = pd.read_csv('\/kaggle\/input\/donorschoosedataset\/resources.csv')\n# project_data = pd.read_csv('..\/train_data.csv') \n# resource_data = pd.read_csv('..\/resources.csv')\n\nprint(\"Done\")","2f33430d":"project_data.shape","552cab93":"project_data.columns.values","842bc550":"prefixlist=project_data['teacher_prefix'].values\nprefixlist=list(prefixlist)\ncleanedPrefixList = [x for x in project_data['teacher_prefix'] if x != float('nan')] ## Cleaning the NULL Values in the list -> https:\/\/stackoverflow.com\/a\/50297200\/4433839\n\nlen(cleanedPrefixList)\n# print(len(prefixlist))","3b432998":"## Converting to Nan and Droping -> https:\/\/stackoverflow.com\/a\/29314880\/4433839\n\n# df[df['B'].str.strip().astype(bool)] \/\/ for deleting EMPTY STRINGS.\nproject_data.dropna(subset=['teacher_prefix'], inplace=True)\nproject_data.shape","87c5b40c":"project_data['teacher_prefix'].head(10)","736beb48":"print(\"Number of data points in train data\", resource_data.shape)\nprint(resource_data.columns.values)\nresource_data.head(2)","de60fa90":"project_data.columns","6dae1389":"project_data.shape","905d658d":"price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nproject_data = pd.merge(project_data, price_data, on='id', how='left')","e03326ab":"catogories = list(project_data['project_subject_categories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\ncat_list = []\nfor i in catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp+=j.strip()+\" \" #\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_') # we are replacing the & value into \n    cat_list.append(temp.strip())\n    \nproject_data['clean_categories'] = cat_list\nproject_data.drop(['project_subject_categories'], axis=1, inplace=True)\n\nfrom collections import Counter\nmy_counter = Counter()\nfor word in project_data['clean_categories'].values:\n    my_counter.update(word.split())\n\ncat_dict = dict(my_counter)\nsorted_cat_dict = dict(sorted(cat_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")\n","63a6c082":"cat_dict","10b72357":"sub_catogories = list(project_data['project_subject_subcategories'].values)\n# remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\n\nsub_cat_list = []\nfor i in sub_catogories:\n    temp = \"\"\n    # consider we have text like this \"Math & Science, Warmth, Care & Hunger\"\n    for j in i.split(','): # it will split it in three parts [\"Math & Science\", \"Warmth\", \"Care & Hunger\"]\n        if 'The' in j.split(): # this will split each of the catogory based on space \"Math & Science\"=> \"Math\",\"&\", \"Science\"\n            j=j.replace('The','') # if we have the words \"The\" we are going to replace it with ''(i.e removing 'The')\n        j = j.replace(' ','') # we are placeing all the ' '(space) with ''(empty) ex:\"Math & Science\"=>\"Math&Science\"\n        temp +=j.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n        temp = temp.replace('&','_')\n    sub_cat_list.append(temp.strip())\n\nproject_data['clean_subcategories'] = sub_cat_list\nproject_data.drop(['project_subject_subcategories'], axis=1, inplace=True)\n\n# count of all the words in corpus python: https:\/\/stackoverflow.com\/a\/22898595\/4084039\nmy_counter = Counter()\nfor word in project_data['clean_subcategories'].values:\n    my_counter.update(word.split())\n    \nsub_cat_dict = dict(my_counter)\nsorted_sub_cat_dict = dict(sorted(sub_cat_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")","bd6b89bf":"# this code removes \" \" and \"-\". ie Grades 3-5 -> grage3to5\n#  remove special characters from list of strings python: https:\/\/stackoverflow.com\/a\/47301924\/4084039\n\n# https:\/\/www.geeksforgeeks.org\/removing-stop-words-nltk-python\/\n# https:\/\/stackoverflow.com\/questions\/23669024\/how-to-strip-a-specific-word-from-a-string\n# https:\/\/stackoverflow.com\/questions\/8270092\/remove-all-whitespace-in-a-string-in-python\nclean_grades=[]\nfor project_grade in project_data['project_grade_category'].values:\n    project_grade=str(project_grade).lower().strip().replace(' ','').replace('-','to')\n    \n    clean_grades.append(project_grade.strip())\n\nproject_data['clean_project_grade_category']=clean_grades\nproject_data.drop(['project_grade_category'],axis=1,inplace=True)\n\nmy_counter = Counter()\nfor word in project_data['clean_project_grade_category'].values:\n    my_counter.update(word.split())\n    \ngrade_dict = dict(my_counter)\nsorted_project_grade_cat_dict = dict(sorted(grade_dict.items(), key=lambda kv: kv[1]))\nprint(\"done\")\n","223bbcdf":"# merge two column text dataframe: \nproject_data[\"essay\"] = project_data[\"project_essay_1\"].map(str) +\\\n                        project_data[\"project_essay_2\"].map(str) + \\\n                        project_data[\"project_essay_3\"].map(str) + \\\n                        project_data[\"project_essay_4\"].map(str)","0b95f941":"project_data.head(2)","e6dc166d":"#### Text PreProcessing Functions","b3c3d12f":"# https:\/\/stackoverflow.com\/a\/47091490\/4084039\nimport re\n\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","5eb3e12d":"# https:\/\/gist.github.com\/sebleier\/554280\n# we are removing the words from the stop words list: 'no', 'nor', 'not'\nstopwords= ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"]","22716392":"# Combining all the above stundents \nfrom tqdm import tqdm\npreprocessed_essays = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['essay'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e.lower() not in stopwords)\n    preprocessed_essays.append(sent.lower().strip())","64806f78":"project_data.shape","02895011":"## new column added as PreProcessed_Essays and older unProcessed essays column is deleted\nproject_data['preprocessed_essays'] = preprocessed_essays\n# project_data.drop(['essay'], axis=1, inplace=True)","747ded5c":"project_data.columns","51c1d4b8":"# after preprocesing\npreprocessed_essays[20000]","2c1453d8":"# similarly you can preprocess the titles also\n# similarly you can preprocess the titles also\n# similarly you can preprocess the titles also\nfrom tqdm import tqdm\npreprocessed_titles = []\n# tqdm is for printing the status bar\nfor sentance in tqdm(project_data['project_title'].values):\n    sent = decontracted(sentance)\n    sent = sent.replace('\\\\r', ' ')\n    sent = sent.replace('\\\\\"', ' ')\n    sent = sent.replace('\\\\n', ' ')\n    sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n    # https:\/\/gist.github.com\/sebleier\/554280\n    sent = ' '.join(e for e in sent.split() if e not in stopwords)\n    preprocessed_titles.append(sent.lower().strip())","a07c3017":"#https:\/\/stackoverflow.com\/questions\/26666919\/add-column-in-dataframe-from-list\/3849072\nproject_data['preprocessed_titles'] = preprocessed_titles\n# project_data.drop(['project_title'], axis=1, inplace=True)","325ab8a9":"project_data.columns","d1f25566":"project_data.shape","1bc0a4bd":"price_data = resource_data.groupby('id').agg({'price':'sum', 'quantity':'sum'}).reset_index()\nprice_data.head(2)","28c80e82":"project_data = pd.merge(project_data, price_data, on='id', how='left')","279a8a62":"project_data.shape","dec4371e":"# project_data['']\nfor col_type, new_col in [('project_title', 'title_size'), ('essay', 'essay_size')]:\n    print(\"Now in: \",col_type)\n    col_data = project_data[col_type]\n    print(col_data.head(10))\n    col_size = []\n    for sen in col_data:\n        sen = decontracted(sen)\n        col_size.append(len(sen.split()))\n    project_data[new_col] = col_size\n    col_size.clear()","1162f4fc":"project_data.shape","f955106d":"project_data.columns","d581d606":"project_data['title_size'].head(10)","86b9c83d":"project_data['essay_size'].head(10)","9cdae3de":"project_bkp=project_data.copy()","83c62d0d":"project_bkp.shape","376aa220":"## taking random samples of 100k datapoints\nproject_data = project_bkp.sample(n = 20000) \n# resource_data = pd.read_csv('..\/resources.csv')\n\nproject_data.shape\n\n# y_value_counts = row1['project_is_approved'].value_counts()\ny_value_counts = project_data['project_is_approved'].value_counts()\nprint(\"Number of projects thar are approved for funding:     \", y_value_counts[1],\" -> \",round(y_value_counts[1]\/(y_value_counts[1]+y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects thar are not approved for funding: \", y_value_counts[0],\" -> \",round(y_value_counts[0]\/(y_value_counts[1]+y_value_counts[0])*100,2),\"%\")","0f882cf6":"# # X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n# from sklearn.model_selection import train_test_split\n# x_train,x_test,y_train,y_test=train_test_split(\n#     project_data.drop('project_is_approved', axis=1),\n#     project_data['project_is_approved'].values,\n#     test_size=0.3,\n#     random_state=42,\n#     stratify=project_data[['project_is_approved']])\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(\n    project_data,\n    project_data['project_is_approved'],\n    test_size=0.2,\n    random_state=42,\n    stratify=project_data[['project_is_approved']])\n\nprint(\"x_train: \",x_train.shape)\nprint(\"x_test : \",x_test.shape)\nprint(\"y_train: \",y_train.shape)\nprint(\"y_test : \",y_test.shape)","9e92ca5d":"# x_train, x_cv, y_train, y_cv = train_test_split(x_train, y_train, test_size=0.25, stratify=y_train)\n\n# print(\"x_train: \",x_train.shape)\n# print(\"y_train: \",y_train.shape)\n# print(\"x_cv   : \",x_cv.shape)\n# print(\"x_cv   : \",y_cv.shape)\n# print(\"x_test : \",x_test.shape)\n# print(\"y_test : \",y_test.shape)","b1108c5e":"# y_value_counts = row1['project_is_approved'].value_counts()\nprint(\"X_TRAIN-------------------------\")\nx_train_y_value_counts = x_train['project_is_approved'].value_counts()\nprint(\"Number of projects that are approved for funding    \", x_train_y_value_counts[1],\" -> \",round(x_train_y_value_counts[1]\/(x_train_y_value_counts[1]+x_train_y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects that are not approved for funding \",x_train_y_value_counts[0],\" -> \",round(x_train_y_value_counts[0]\/(x_train_y_value_counts[1]+x_train_y_value_counts[0])*100,2),\"%\")\nprint(\"\\n\")\n# y_value_counts = row1['project_is_approved'].value_counts()\nprint(\"X_TEST--------------------------\")\nx_test_y_value_counts = x_test['project_is_approved'].value_counts()\nprint(\"Number of projects that are approved for funding    \", x_test_y_value_counts[1],\" -> \",round(x_test_y_value_counts[1]\/(x_test_y_value_counts[1]+x_test_y_value_counts[0])*100,2),\"%\")\nprint(\"Number of projects that are not approved for funding \",x_test_y_value_counts[0],\" -> \",round(x_test_y_value_counts[0]\/(x_test_y_value_counts[1]+x_test_y_value_counts[0])*100,2),\"%\")\nprint(\"\\n\")\n# y_value_counts = row1['project_is_approved'].value_counts()\n# print(\"X_CV----------------------------\")\n# x_cv_y_value_counts = x_cv['project_is_approved'].value_counts()\n# print(\"Number of projects that are approved for funding    \", x_cv_y_value_counts[1],\" -> \",round(x_cv_y_value_counts[1]\/(x_cv_y_value_counts[1]+x_cv_y_value_counts[0])*100),2,\"%\")\n# print(\"Number of projects that are not approved for funding \",x_cv_y_value_counts[0],\" -> \",round(x_cv_y_value_counts[0]\/(x_cv_y_value_counts[1]+x_cv_y_value_counts[0])*100),2,\"%\")\n# print(\"\\n\")","f45b1a7a":"# from sklearn.utils import resample\n\n# ## splitting x_train in their respective classes\n# x_train_majority=x_train[x_train.project_is_approved==1]\n# x_train_minority=x_train[x_train.project_is_approved==0]\n\n# print(\"No. of points in the Training Dataset : \", x_train.shape)\n# print(\"No. of points in the majority class 1 : \",len(x_train_majority))\n# print(\"No. of points in the minority class 0 : \",len(x_train_minority))\n\n# print(x_train_majority.shape)\n# print(x_train_minority.shape)\n\n# ## Resampling with replacement\n# x_train_minority_upsampled=resample(\n#     x_train_minority,\n#     replace=True,\n\n#     n_samples=len(x_train_majority),\n#     random_state=123)\n\n# print(\"Resampled Minority class details\")\n# print(\"Type:  \",type(x_train_minority_upsampled))\n# print(\"Shape: \",x_train_minority_upsampled.shape)\n# print(\"\\n\")\n# ## Concatinating our Upsampled Minority class with the existing Majority class\n# x_train_upsampled=pd.concat([x_train_majority,x_train_minority_upsampled])\n\n# print(\"Upsampled Training data\")\n# print(\"Total number of Class labels\")\n# print(x_train_upsampled.project_is_approved.value_counts())\n# print(\"\\n\")\n# print(\"Old Training IMBALANCED Dataset Shape         : \", x_train.shape)\n# print(\"New Training BALANCED Upsampled Dataset Shape : \",x_train_upsampled.shape)\n\n# x_train_upsampled.to_csv ('x_train_upsampled_csv.csv',index=False)","1d2185c1":"x_train.shape","0b8868a5":"x_test.shape","05d743bc":"project_data.columns","1b9f972a":"# we use count vectorizer to convert the values into one \nfrom sklearn.feature_extraction.text import CountVectorizer\n# vectorizer_sub = CountVectorizer(vocabulary=list(sorted_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer_sub = CountVectorizer( lowercase=False, binary=True)\n\nvectorizer_sub.fit(x_train['clean_categories'].values)\n\nx_train_categories_one_hot = vectorizer_sub.transform(x_train['clean_categories'].values)\n# x_cv_categories_one_hot    = vectorizer_sub.transform(x_cv['clean_categories'].values)\nx_test_categories_one_hot  = vectorizer_sub.transform(x_test['clean_categories'].values)\n\n\nprint(vectorizer_sub.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> categories: x_train: \",x_train_categories_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> categories: x_cv   : \",x_cv_categories_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> categories: x_test : \",x_test_categories_one_hot.shape)","4fdf3536":"# we use count vectorizer to convert the values into one \n# vectorizer_sub_sub = CountVectorizer(vocabulary=list(sorted_sub_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer_sub_sub = CountVectorizer( lowercase=False, binary=True)\n\nvectorizer_sub_sub.fit(x_train['clean_subcategories'].values)\n\nx_train_sub_categories_one_hot = vectorizer_sub_sub.transform(x_train['clean_subcategories'].values)\n# x_cv_sub_categories_one_hot    = vectorizer_sub_sub.transform(x_cv['clean_subcategories'].values)\nx_test_sub_categories_one_hot  = vectorizer_sub_sub.transform(x_test['clean_subcategories'].values)\n\nprint(vectorizer_sub_sub.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> sub_categories: x_train: \",x_train_sub_categories_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> sub_categories: x_cv   : \",x_cv_sub_categories_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> sub_categories: x_test : \",x_test_sub_categories_one_hot.shape)","c11a3dc3":"my_counter = Counter()\nfor state in project_data['school_state'].values:\n    my_counter.update(state.split())","a2e4aa8e":"school_state_cat_dict = dict(my_counter)\nsorted_school_state_cat_dict = dict(sorted(school_state_cat_dict.items(), key=lambda kv: kv[1]))","0c7e9885":"from scipy import sparse ## Exporting Sparse Matrix to NPZ File -> https:\/\/stackoverflow.com\/questions\/8955448\/save-load-scipy-sparse-csr-matrix-in-portable-data-format\nstatelist=list(project_data['school_state'].values)\n# vectorizer_state = CountVectorizer(vocabulary=set(statelist), lowercase=False, binary=True)\nvectorizer_state = CountVectorizer( lowercase=False, binary=True)\n\nvectorizer_state.fit(x_train['school_state'])\n\nx_train_school_state_one_hot = vectorizer_state.transform(x_train['school_state'].values)\n# x_cv_school_state_one_hot    = vectorizer_state.transform(x_cv['school_state'].values)\nx_test_school_state_one_hot  = vectorizer_state.transform(x_test['school_state'].values)\n\nprint(vectorizer_state.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> school_state: x_train: \",x_train_school_state_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> school_state: x_cv   : \",x_cv_school_state_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> school_state: x_test : \",x_test_school_state_one_hot.shape)\n# school_one_hot = vectorizer.transform(statelist)\n# print(\"Shape of matrix after one hot encodig \",school_one_hot.shape)\n# print(type(school_one_hot))\n# sparse.save_npz(\"school_one_hot_export.npz\", school_one_hot) \n# print(school_one_hot.toarray())","c62cebcf":"# prefixlist=project_data['teacher_prefix'].values\n# prefixlist=list(prefixlist)\n# cleanedPrefixList = [x for x in prefixlist if x == x] ## Cleaning the NULL Values in the list -> https:\/\/stackoverflow.com\/a\/50297200\/4433839\n\n# ## preprocessing the prefix to remove the SPACES,- else the vectors will be just 0's. Try adding - and see\n# prefix_nospace_list = []\n# for i in cleanedPrefixList:\n#     temp = \"\"\n#     i = i.replace('.','') # we are placeing all the '.'(dot) with ''(empty) ex:\"Mr.\"=>\"Mr\"\n#     temp +=i.strip()+\" \"#\" abc \".strip() will return \"abc\", remove the trailing spaces\n#     prefix_nospace_list.append(temp.strip())\n\n# cleanedPrefixList=prefix_nospace_list\n\n# vectorizer = CountVectorizer(vocabulary=set(cleanedPrefixList), lowercase=False, binary=True)\n# vectorizer.fit(cleanedPrefixList)\n# print(vectorizer.get_feature_names())\n# prefix_one_hot = vectorizer.transform(cleanedPrefixList)\n# print(\"Shape of matrix after one hot encodig \",prefix_one_hot.shape)\n# prefix_one_hot_ar=prefix_one_hot.todense()\n\n# ##code to export to csv -> https:\/\/stackoverflow.com\/a\/54637996\/4433839\n# # prefixcsv=pd.DataFrame(prefix_one_hot.toarray())\n# # prefixcsv.to_csv('prefix.csv', index=None,header=None)","2aca8217":"my_counter = Counter()\nfor teacher_prefix in project_data['teacher_prefix'].values:\n    teacher_prefix = str(teacher_prefix).lower().replace('.','').strip()\n    \n    my_counter.update(teacher_prefix.split())\nteacher_prefix_cat_dict = dict(my_counter)\nsorted_teacher_prefix_cat_dict = dict(sorted(teacher_prefix_cat_dict.items(), key=lambda kv: kv[1]))","06fd7a4c":"sorted_teacher_prefix_cat_dict.keys()","977b3c83":"from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer_prefix = CountVectorizer(vocabulary=list(sorted_teacher_prefix_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer_prefix = CountVectorizer(lowercase=False, binary=True)\n\nvectorizer_prefix.fit(x_train['teacher_prefix'].values)\n\nx_train_prefix_one_hot = vectorizer_prefix.transform(x_train['teacher_prefix'].values)\n# x_cv_prefix_one_hot    = vectorizer_prefix.transform(x_cv['teacher_prefix'].values)\nx_test_prefix_one_hot  = vectorizer_prefix.transform(x_test['teacher_prefix'].values)\n\nprint(vectorizer_prefix.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> prefix: x_train: \",x_train_prefix_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> prefix: x_test : \",x_test_prefix_one_hot.shape)","0c21cbea":"from sklearn.feature_extraction.text import CountVectorizer\n# vectorizer_grade = CountVectorizer(vocabulary=list(sorted_project_grade_cat_dict.keys()), lowercase=False, binary=True)\nvectorizer_grade = CountVectorizer(lowercase=False, binary=True)\n\nvectorizer_grade.fit(x_train['clean_project_grade_category'].values)\n\nx_train_grade_category_one_hot = vectorizer_grade.transform(x_train['clean_project_grade_category'].values)\n# x_cv_grade_category_one_hot    = vectorizer_grade.transform(x_cv['clean_project_grade_category'].values)\nx_test_grade_category_one_hot  = vectorizer_grade.transform(x_test['clean_project_grade_category'].values)\n\nprint(vectorizer_grade.get_feature_names())\n\nprint(\"Shape of matrix after one hot encoding -> project_grade: x_train : \",x_train_grade_category_one_hot.shape)\n# print(\"Shape of matrix after one hot encoding -> project_grade: x_cv    : \",x_cv_grade_category_one_hot.shape)\nprint(\"Shape of matrix after one hot encoding -> project_grade: x_test  : \",x_test_grade_category_one_hot.shape)\n","b3897e87":"type(x_train_grade_category_one_hot)","9a10df4a":"x_train_grade_category_one_hot.toarray()","9746012b":"x_train['price_x'].head(10)","f203531d":"x_train['price_y'].head(10)","6f646591":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['price_x'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_price_normalized = normalizer.transform(x_train['price_x'].values.reshape(1, -1)).reshape(-1,1)\nx_test_price_normalized  = normalizer.transform(x_test['price_x'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_train: \",x_train_prefix_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_test : \",x_test_prefix_one_hot.shape)","5872298c":"type(x_train_price_normalized)","d3ff6c95":"x_train_price_normalized","bc7246f0":"# # check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# # standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# # from sklearn.preprocessing import StandardScaler\n# from sklearn.preprocessing import Normalizer\n\n# # price_standardized = standardScalar.fit(project_data['price'].values)\n# # this will rise the error\n# # ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# # Reshape your data either using array.reshape(-1, 1)\n# # transformer = Normalizer().fit(X)\n# normalizer = Normalizer()\n\n# normalizer.fit(x_train_upsampled['price'].values.reshape(-1,1)) # finding the mean and standard deviation of this data\n# # print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# # Now normalize the data\n\n# x_train_price_normalized = normalizer.transform(x_train_upsampled['price'].values.reshape(-1, 1))\n# x_test_price_normalized  = normalizer.transform(x_test['price'].values.reshape(-1, 1))\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(-1, 1))\n\n\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_train: \",x_train_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teacher Prefix: x_test : \",x_test_prefix_one_hot.shape)","f8b6c3c4":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\nfrom sklearn.preprocessing import StandardScaler\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n\nteacher_previous_proj_normalizer = Normalizer()\n# normalizer = Normalizer()\n\nteacher_previous_proj_normalizer.fit(x_train['teacher_number_of_previously_posted_projects'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {teacher_previous_proj_scalar.mean_[0]}, Standard deviation : {np.sqrt(teacher_previous_proj_scalar.var_[0])}\")\n\n# Now standardize the data with above maen and variance.\nx_train_teacher_previous_proj_normalized = teacher_previous_proj_normalizer.transform(x_train['teacher_number_of_previously_posted_projects'].values.reshape(1,- 1)).reshape(-1,1)\nx_test_teacher_previous_proj_normalized  = teacher_previous_proj_normalizer.transform(x_test['teacher_number_of_previously_posted_projects'].values.reshape(1,- 1)).reshape(-1,1)\n# x_cv_teacher_previous_proj_normalized    = teacher_previous_proj_normalizer.transform(x_cv['teacher_number_of_previously_posted_projects'].values.reshape(1,- 1)).reshape(-1,1)\n\n# print(\"Shape of matrix after normalization -> Teachers Previous Projects: x_train:  \",x_train_prefix_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teachers Previous Projects: x_cv   :  \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Teachers Previous Projects: x_test :  \",x_test_prefix_one_hot.shape)\n","148e7480":"x_train_teacher_previous_proj_normalized","23013b9f":"x_train.columns","3c21ee40":"project_data['title_size'].head(10)","aa453d45":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['title_size'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_title_normalized = normalizer.transform(x_train['title_size'].values.reshape(1, -1)).reshape(-1,1)\nx_test_title_normalized  = normalizer.transform(x_test['title_size'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\n# print(\"Shape of matrix after normalization -> Project Title: x_train: \",x_train_title_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Project Title: x_test : \",x_test_title_one_hot.shape)","73e33f36":"type(x_train_title_normalized)","81a6a43a":"x_train_title_normalized","fe51ac98":"project_data['essay_size']","dfa9f8fa":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['essay_size'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_essay_normalized = normalizer.transform(x_train['essay_size'].values.reshape(1, -1)).reshape(-1,1)\nx_test_essay_normalized  = normalizer.transform(x_test['essay_size'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\n# print(\"Shape of matrix after normalization -> Project Title: x_train: \",x_train_title_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Project Title: x_test : \",x_test_title_one_hot.shape)","0bc2b977":"x_train_essay_normalized","24f25b8f":"project_data['quantity_x']","f4f3b58d":"# check this one: https:\/\/www.youtube.com\/watch?v=0HOqOcln3Z4&t=530s\n# standardization sklearn: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n# from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\n\n# price_standardized = standardScalar.fit(project_data['price'].values)\n# this will rise the error\n# ValueError: Expected 2D array, got 1D array instead: array=[725.05 213.03 329.   ... 399.   287.73   5.5 ].\n# Reshape your data either using array.reshape(-1, 1)\n# transformer = Normalizer().fit(X)\nnormalizer = Normalizer()\n\nnormalizer.fit(x_train['quantity_x'].values.reshape(1,-1)) # finding the mean and standard deviation of this data\n# print(f\"Mean : {price_scalar.mean_[0]}, Standard deviation : {np.sqrt(price_scalar.var_[0])}\")\n\n# Now normalize the data\n\nx_train_quantity_normalized = normalizer.transform(x_train['quantity_y'].values.reshape(1, -1)).reshape(-1,1)\nx_test_quantity_normalized  = normalizer.transform(x_test['quantity_y'].values.reshape(1, -1)).reshape(-1,1)\n# x_cv_price_normalized    = normalizer.transform(x_cv['price'].values.reshape(1, -1)).reshape(-1,1)\n\n\n# print(\"Shape of matrix after normalization -> Project Title: x_train: \",x_train_title_one_hot.shape)\n# # print(\"Shape of matrix after normalization -> Teacher Prefix: x_cv   : \",x_cv_prefix_one_hot.shape)\n# print(\"Shape of matrix after normalization -> Project Title: x_test : \",x_test_title_one_hot.shape)","fd926212":"# error aa gaya","23ef1822":"# We are considering only the words which appeared in at least 10 documents(rows or projects).\nvectorizer_essay_bow = CountVectorizer(min_df=10,max_features=2000)\n\nvectorizer_essay_bow.fit(x_train['preprocessed_essays'])\n\nx_train_essays_bow = vectorizer_essay_bow.transform(x_train['preprocessed_essays'])\n# x_cv_essays_bow    = vectorizer_essay_bow.transform(x_cv['preprocessed_essays'])\nx_test_essays_bow  = vectorizer_essay_bow.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after BOW -> Essays: x_train: \",x_train_essays_bow.shape)\n# print(\"Shape of matrix after BOW -> Essays: x_cv   : \",x_cv_essays_bow.shape)\nprint(\"Shape of matrix after BOW -> Essays: x_test : \",x_test_essays_bow.shape)","8b6c38a6":"## bigram using countvectorizer example -> https:\/\/stackoverflow.com\/a\/24006054\/4433839\n# v = CountVectorizer(ngram_range=( 2,2))\n# print(v.fit([\"an apple a day keeps the doctor away\"]).vocabulary_)\n\n# We are considering only the words which appeared in at least 10 documents(rows or projects).\nvectorizer_essay_bow_bigram = CountVectorizer(ngram_range=(2,2),min_df=10,max_features=2000)\n# vectorizer_essay_bow_bigram = CountVectorizer(ngram_range=(2,2),min_df=10,max_features=5000)\n\nvectorizer_essay_bow_bigram.fit(x_train['preprocessed_essays'])\n\nx_train_essays_bow_bigram = vectorizer_essay_bow_bigram.transform(x_train['preprocessed_essays'])\n# x_cv_essays_bow    = vectorizer_essay_bow.transform(x_cv['preprocessed_essays'])\nx_test_essays_bow_bigram  = vectorizer_essay_bow_bigram.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after BOW -> Essays: x_train: \",x_train_essays_bow_bigram.shape)\n# print(\"Shape of matrix after BOW -> Essays: x_cv   : \",x_cv_essays_bow.shape)\nprint(\"Shape of matrix after BOW -> Essays: x_test : \",x_test_essays_bow_bigram.shape)\n","22b61484":"# print(vectorizer_essay_bow_bigram.vocabulary_)\nprint(type(x_train_essays_bow_bigram))","f00d3f47":"print(type(vectorizer_essay_bow_bigram))","e138239d":"vectorizer_title_bow = CountVectorizer(min_df=2)\n\nvectorizer_title_bow.fit(x_train['preprocessed_titles'])\n\nx_train_titles_bow = vectorizer_title_bow.transform(x_train['preprocessed_titles'])\n# x_cv_titles_bow    = vectorizer_title_bow.transform(x_cv['preprocessed_titles'])\nx_test_titles_bow  = vectorizer_title_bow.transform(x_test['preprocessed_titles'])\n\nprint(\"Shape of matrix after BOW -> Title: x_train: \",x_train_titles_bow.shape)\n# print(\"Shape of matrix after BOW -> Title: x_cv   : \",x_cv_titles_bow.shape)\nprint(\"Shape of matrix after BOW -> Title: x_test : \",x_test_titles_bow.shape)","a9fa1b21":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_essay_tfidf_unigram = TfidfVectorizer(min_df=10,max_features=2000)\n\nvectorizer_essay_tfidf_unigram.fit(x_train['preprocessed_essays'])\n\nx_train_essays_tfidf_unigram = vectorizer_essay_tfidf_unigram.transform(x_train['preprocessed_essays'])\n# x_cv_essays_tfidf    = vectorizer_essay_tfidf.transform(x_cv['preprocessed_essays'])\nx_test_essays_tfidf_unigram  = vectorizer_essay_tfidf_unigram.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after TF-IDF -> Essay: x_train: \",x_train_essays_tfidf_unigram.shape)\n# print(\"Shape of matrix after TF-IDF -> Essay: x_cv   : \",x_cv_essays_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Essay: x_test : \",x_test_essays_tfidf_unigram.shape)","31841d59":"type(x_train_essays_tfidf_unigram)","515eef1c":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_essay_tfidf_bigram = TfidfVectorizer(ngram_range=(2,2),min_df=10,max_features=2000)\n# vectorizer_essay_tfidf_bigram = TfidfVectorizer(ngram_range=(2,2),min_df=10,max_features=5000)\n\nvectorizer_essay_tfidf_bigram.fit(x_train['preprocessed_essays'])\n\nx_train_essays_tfidf_bigram = vectorizer_essay_tfidf_bigram.transform(x_train['preprocessed_essays'])\n# x_cv_essays_tfidf    = vectorizer_essay_tfidf.transform(x_cv['preprocessed_essays'])\nx_test_essays_tfidf_bigram  = vectorizer_essay_tfidf_bigram.transform(x_test['preprocessed_essays'])\n\nprint(\"Shape of matrix after TF-IDF -> Essay: x_train: \",x_train_essays_tfidf_bigram.shape)\n# print(\"Shape of matrix after TF-IDF -> Essay: x_cv   : \",x_cv_essays_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Essay: x_test : \",x_test_essays_tfidf_bigram.shape)","c1d578d7":"from sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer_title_tfidf = TfidfVectorizer(min_df=2)\n\nvectorizer_title_tfidf.fit(x_train['preprocessed_titles'])\n\nx_train_titles_tfidf = vectorizer_title_tfidf.transform(x_train['preprocessed_titles'])\n# x_cv_titles_tfidf    = vectorizer_title_tfidf.transform(x_cv['preprocessed_titles'])\nx_test_titles_tfidf  = vectorizer_title_tfidf.transform(x_test['preprocessed_titles'])\n\nprint(\"Shape of matrix after TF-IDF -> Title: x_train: \",x_train_titles_tfidf.shape)\n# print(\"Shape of matrix after TF-IDF -> Title: x_cv   : \",x_cv_titles_tfidf.shape)\nprint(\"Shape of matrix after TF-IDF -> Title: x_test : \",x_test_titles_tfidf.shape)\n\n# Code for testing and checking the generated vectors\n# v1 = vectorizer.transform([preprocessed_titles[0]]).toarray()[0]\n# text_title_tfidf=pd.DataFrame(v1)\n# text_title_tfidf.to_csv('text_title_tfidf.csv', index=None,header=None)","4ad71d68":"# print(STOP)","e9598fe7":"'''\n# Reading glove vectors in python: https:\/\/stackoverflow.com\/a\/38230349\/4084039\ndef loadGloveModel(gloveFile):\n    print (\"Loading Glove Model\")\n    f = open(gloveFile,'r', encoding=\"utf8\")\n    model = {}\n    for line in tqdm(f):\n        splitLine = line.split()\n        word = splitLine[0]\n        embedding = np.array([float(val) for val in splitLine[1:]])\n        model[word] = embedding\n    print (\"Done.\",len(model),\" words loaded!\")\n    return model\nmodel = loadGloveModel('glove.42B.300d.txt')\n\n# ============================\nOutput:\n    \nLoading Glove Model\n1917495it [06:32, 4879.69it\/s]\nDone. 1917495  words loaded!\n\n# ============================\n\nwords = []\nfor i in preproced_texts:\n    words.extend(i.split(' '))\n\nfor i in preproced_titles:\n    words.extend(i.split(' '))\nprint(\"all the words in the coupus\", len(words))\nwords = set(words)\nprint(\"the unique words in the coupus\", len(words))\n\ninter_words = set(model.keys()).intersection(words)\nprint(\"The number of words that are present in both glove vectors and our coupus\", \\\n      len(inter_words),\"(\",np.round(len(inter_words)\/len(words)*100,3),\"%)\")\n\nwords_courpus = {}\nwords_glove = set(model.keys())\nfor i in words:\n    if i in words_glove:\n        words_courpus[i] = model[i]\nprint(\"word 2 vec length\", len(words_courpus))\n\n\n# stronging variables into pickle files python: http:\/\/www.jessicayung.com\/how-to-use-pickle-to-save-and-load-variables-in-python\/\n\nimport pickle\nwith open('glove_vectors', 'wb') as f:\n    pickle.dump(words_courpus, f)\n\n\n'''\n\n\n# words_train_essays = []\n# for i in preprocessed_essays_train :\n#     words_train_essays.extend(i.split(' '))\n\n# ## Find the total number of words in the Train data of Essays.\n# print(\"all the words in the corpus\", len(words_train_essays))\n\n# ## Find the unique words in this set of words\n# words_train_essay = set(words_train_essays)\n# print(\"the unique words in the corpus\", len(words_train_essay))\n\n# ## Find the words present in both Glove Vectors as well as our corpus.\n# inter_words = set(model.keys()).intersection(words_train_essay)\n# print(\"The number of words that are present in both glove vectors and our corpus are {} which is nearly {}% \".format(len(inter_words), np.round((float(len(inter_words))\/len(words_train_essay))*100)))\n\n# words_corpus_train_essay = {}\n# words_glove = set(model.keys())\n# for i in words_train_essay:\n#     if i in words_glove:\n#         words_corpus_train_essay[i] = model[i]\n# print(\"word 2 vec length\", len(words_corpus_train_essay))","07b63200":"# stronging variables into pickle files python: http:\/\/www.jessicayung.com\/how-to-use-pickle-to-save-and-load-variables-in-python\/\n# make sure you have the glove_vectors file\n# with open('\/content\/drive\/My Drive\/Colab Notebooks\/glove_vectors', 'rb') as f:\nwith open('\/kaggle\/input\/donorschoosedataset\/glove_vectors\/glove_vectors', 'rb') as f:\n# with open('..\/glove_vectors', 'rb') as f:\n    model = pickle.load(f)\n    glove_words =  set(model.keys())","a97487f6":"# average Word2Vec\n# compute average word2vec for each review.\nx_train_essays_avg_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\n# for sentence in tqdm(x_train_upsampled['preprocessed_essays']): # for each review\/sentence\nfor sentence in tqdm(x_train['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_train_essays_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Essay: x_train: \",len(x_train_essays_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Essay: x_train: \",len(x_train_essays_avg_w2v_vectors[0]))","b4f1f7a0":"# ## NOT DONE CV\n\n# x_cv_essays_avg_w2v_vectors = [];\n# for sentence in tqdm(x_cv['preprocessed_essays']): # for each review\/sentence\n#     vector = np.zeros(300) # as word vectors are of zero length\n#     cnt_words =0; # num of words with a valid vector in the sentence\/review\n#     for word in sentence.split(): # for each word in a review\/sentence\n#         if word in glove_words:\n#             vector += model[word]\n#             cnt_words += 1\n#     if cnt_words != 0:\n#         vector \/= cnt_words\n#     x_cv_essays_avg_w2v_vectors.append(vector)\n\n# print(\"Total number of vectors  : AVG-W2V -> Essay: x_cv   : \",len(x_cv_essays_avg_w2v_vectors))\n# print(\"Length of a Single vector: AVG-W2V -> Essay: x_cv   : \",len(x_cv_essays_avg_w2v_vectors[0]))","f9165f97":"x_test_essays_avg_w2v_vectors = [];\nfor sentence in tqdm(x_test['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_test_essays_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Essay: x_test : \",len(x_test_essays_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Essay: x_test : \",len(x_test_essays_avg_w2v_vectors[0]))","e8accfb4":"x_train_titles_avg_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\n# for sentence in tqdm(x_train_upsampled['preprocessed_titles']): # for each review\/sentence\nfor sentence in tqdm(x_train['preprocessed_titles']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_train_titles_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Titles: x_train: \",len(x_train_titles_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Titles: x_train: \",len(x_train_titles_avg_w2v_vectors[0]))","34d4337d":"# ### NOT DONE HERE\n\n\n# x_cv_titles_avg_w2v_vectors = [];\n# for sentence in tqdm(x_cv['preprocessed_titles']): # for each review\/sentence\n#     vector = np.zeros(300) # as word vectors are of zero length\n#     cnt_words =0; # num of words with a valid vector in the sentence\/review\n#     for word in sentence.split(): # for each word in a review\/sentence\n#         if word in glove_words:\n#             vector += model[word]\n#             cnt_words += 1\n#     if cnt_words != 0:\n#         vector \/= cnt_words\n#     x_cv_titles_avg_w2v_vectors.append(vector)\n\n# print(\"Total number of vectors  : AVG-W2V -> Titles: x_cv   : \",len(x_cv_titles_avg_w2v_vectors))\n# print(\"Length of a Single vector: AVG-W2V -> Titles: x_cv   : \",len(x_cv_titles_avg_w2v_vectors[0]))","07ceb304":"x_test_titles_avg_w2v_vectors = [];\nfor sentence in tqdm(x_test['preprocessed_titles']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    cnt_words =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if word in glove_words:\n            vector += model[word]\n            cnt_words += 1\n    if cnt_words != 0:\n        vector \/= cnt_words\n    x_test_titles_avg_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : AVG-W2V -> Titles: x_test : \",len(x_test_titles_avg_w2v_vectors))\nprint(\"Length of a Single vector: AVG-W2V -> Titles: x_test : \",len(x_test_titles_avg_w2v_vectors[0]))","c0c952fb":"# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\ntfidf_model = TfidfVectorizer()\n# tfidf_model.fit(x_train_upsampled['preprocessed_essays'])\ntfidf_model.fit(x_train['preprocessed_essays'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\ntfidf_words = set(tfidf_model.get_feature_names())","efc7c091":"# average Word2Vec\n# compute average word2vec for each review.\nx_train_essays_tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\n# for sentence in tqdm(x_train_upsampled['preprocessed_essays']): # for each review\/sentence\nfor sentence in tqdm(x_train['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    x_train_essays_tfidf_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Essays: x_train : \",len(x_train_essays_tfidf_w2v_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Essays: x_train : \",len(x_train_essays_tfidf_w2v_vectors[0]))","e9d4d987":"# ### NOT DONE HERE\n\n\n# x_cv_essays_tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\n# for sentence in tqdm(x_cv['preprocessed_essays']): # for each review\/sentence\n#     vector = np.zeros(300) # as word vectors are of zero length\n#     tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n#     for word in sentence.split(): # for each word in a review\/sentence\n#         if (word in glove_words) and (word in tfidf_words):\n#             vec = model[word] # getting the vector for each word\n#             # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n#             tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n#             vector += (vec * tf_idf) # calculating tfidf weighted w2v\n#             tf_idf_weight += tf_idf\n#     if tf_idf_weight != 0:\n#         vector \/= tf_idf_weight\n#     x_cv_essays_tfidf_w2v_vectors.append(vector)\n\n# print(\"Total number of vectors  : TFIDF-W2V -> Essays: x_cv : \",len(x_cv_essays_tfidf_w2v_vectors))\n# print(\"Length of a Single vector: TFIDF-W2V -> Essays: x_cv : \",len(x_cv_essays_tfidf_w2v_vectors[0]))","e1fd2912":"x_test_essays_tfidf_w2v_vectors = []; # the avg-w2v for each sentence\/review is stored in this list\nfor sentence in tqdm(x_test['preprocessed_essays']): # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_weight =0; # num of words with a valid vector in the sentence\/review\n    for word in sentence.split(): # for each word in a review\/sentence\n        if (word in glove_words) and (word in tfidf_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_weight += tf_idf\n    if tf_idf_weight != 0:\n        vector \/= tf_idf_weight\n    x_test_essays_tfidf_w2v_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Essays: x_test : \",len(x_test_essays_tfidf_w2v_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Essays: x_test : \",len(x_test_essays_tfidf_w2v_vectors[0]))","6ea7547f":"# S = [\"abc def pqr\", \"def def def abc\", \"pqr pqr def\"]\ntfidf_title_model = TfidfVectorizer()\n# tfidf_title_model.fit(x_train_upsampled['preprocessed_titles'])\ntfidf_title_model.fit(x_train['preprocessed_titles'])\n# we are converting a dictionary with word as a key, and the idf as a value\ndictionary = dict(zip(tfidf_title_model.get_feature_names(), list(tfidf_title_model.idf_)))\ntfidf_title_words = set(tfidf_title_model.get_feature_names())","1b9673da":"# average Word2Vec\n# compute average word2vec for each title.\nx_train_tfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\n# for sentence in x_train_upsampled['preprocessed_titles']: # for each review\/sentence\nfor sentence in x_train['preprocessed_titles']: # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_title_weight =0; # num of words with a valid vector in the title\n    for word in sentence.split(): # for each word in a title\n        if (word in glove_words) and (word in tfidf_title_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_title_weight += tf_idf\n    if tf_idf_title_weight != 0:\n        vector \/= tf_idf_title_weight\n    x_train_tfidf_w2v_title_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Titles: x_train : \",len(x_train_tfidf_w2v_title_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Titles: x_train : \",len(x_train_tfidf_w2v_title_vectors[0]))","7f9d7711":"# ### NOT DONE HERE\n\n\n# # average Word2Vec\n# # compute average word2vec for each title.\n# x_cv_tfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\n# for sentence in x_cv['preprocessed_titles']: # for each review\/sentence\n#     vector = np.zeros(300) # as word vectors are of zero length\n#     tf_idf_title_weight =0; # num of words with a valid vector in the title\n#     for word in sentence.split(): # for each word in a title\n#         if (word in glove_words) and (word in tfidf_title_words):\n#             vec = model[word] # getting the vector for each word\n#             # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n#             tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n#             vector += (vec * tf_idf) # calculating tfidf weighted w2v\n#             tf_idf_title_weight += tf_idf\n#     if tf_idf_title_weight != 0:\n#         vector \/= tf_idf_title_weight\n#     x_cv_tfidf_w2v_title_vectors.append(vector)\n\n# print(\"Total number of vectors  : TFIDF-W2V -> Titles: x_cv : \",len(x_cv_tfidf_w2v_title_vectors))\n# print(\"Length of a Single vector: TFIDF-W2V -> Titles: x_cv : \",len(x_cv_tfidf_w2v_title_vectors[0]))","f4a84bc2":"# average Word2Vec\n# compute average word2vec for each title.\nx_test_tfidf_w2v_title_vectors = []; # the avg-w2v for each title is stored in this list\nfor sentence in x_test['preprocessed_titles']: # for each review\/sentence\n    vector = np.zeros(300) # as word vectors are of zero length\n    tf_idf_title_weight =0; # num of words with a valid vector in the title\n    for word in sentence.split(): # for each word in a title\n        if (word in glove_words) and (word in tfidf_title_words):\n            vec = model[word] # getting the vector for each word\n            # here we are multiplying idf value(dictionary[word]) and the tf value((sentence.count(word)\/len(sentence.split())))\n            tf_idf = dictionary[word]*(sentence.count(word)\/len(sentence.split())) # getting the tfidf value for each word\n            vector += (vec * tf_idf) # calculating tfidf weighted w2v\n            tf_idf_title_weight += tf_idf\n    if tf_idf_title_weight != 0:\n        vector \/= tf_idf_title_weight\n    x_test_tfidf_w2v_title_vectors.append(vector)\n\nprint(\"Total number of vectors  : TFIDF-W2V -> Titles: x_test : \",len(x_test_tfidf_w2v_title_vectors))\nprint(\"Length of a Single vector: TFIDF-W2V -> Titles: x_test : \",len(x_test_tfidf_w2v_title_vectors[0]))","5640d672":"from scipy.sparse import hstack\n\nx_train_onehot = hstack((x_train_categories_one_hot, x_train_sub_categories_one_hot, x_train_school_state_one_hot, x_train_prefix_one_hot, x_train_grade_category_one_hot, x_train_price_normalized, x_train_teacher_previous_proj_normalized))\n# x_cv_onehot    = hstack((x_cv_categories_one_hot, x_cv_sub_categories_one_hot, x_cv_school_state_one_hot, x_cv_prefix_one_hot, x_cv_grade_category_one_hot,x_cv_price_normalized, x_cv_teacher_previous_proj_normalized ))\nx_test_onehot  = hstack((x_test_categories_one_hot, x_test_sub_categories_one_hot, x_test_school_state_one_hot, x_test_prefix_one_hot, x_test_grade_category_one_hot, x_test_price_normalized, x_test_teacher_previous_proj_normalized))\n\nprint(\"Type -> One Hot -> x_train: \",type(x_train_onehot))\nprint(\"Type -> One Hot -> x_test : \",type(x_test_onehot))\n# print(\"Type -> One Hot -> x_cv        : \",type(x_cv_onehot))\nprint(\"\\n\")\nprint(\"Shape -> One Hot -> x_train: \",x_train_onehot.shape)\nprint(\"Shape -> One Hot -> x_test : \",x_test_onehot.shape)\n# print(\"Shape -> One Hot -> x_cv         : \",x_cv_onehot.shape)","f7478845":"x_train_onehot.shape","f5d188ff":"x_train_onehot_bow = hstack((x_train_onehot,x_train_titles_bow,x_train_essays_bow)).tocsr()### Merging all ONE HOT features\n# x_cv_onehot_bow    = hstack((x_cv_onehot, x_cv_titles_bow, x_cv_essays_bow)).tocsr()### Merging all ONE HOT features\nx_test_onehot_bow  = hstack((x_test_onehot, x_test_titles_bow, x_test_essays_bow)).tocsr()### Merging all ONE HOT features\nprint(\"Type -> One Hot BOW -> x_train_cv_test: \",type(x_train_onehot_bow))\n# print(\"Type -> One Hot BOW -> cv             : \",type(x_cv_onehot_bow))\nprint(\"Type -> One Hot BOW -> x_test         : \",type(x_test_onehot_bow))\nprint(\"\\n\")\nprint(\"Shape -> One Hot BOW -> x_train_cv_test: \",x_train_onehot_bow.shape)\n# print(\"Shape -> One Hot BOW -> cv             : \",x_cv_onehot_bow.shape)\nprint(\"Shape -> One Hot BOW -> x_test         : \",x_test_onehot_bow.shape)","2e157de9":"x_train_onehot_tfidf = hstack((x_train_onehot,x_train_titles_tfidf, x_train_essays_tfidf_unigram)).tocsr()\n# x_cv_onehot_tfidf    = hstack((x_cv_onehot,x_cv_titles_tfidf, x_cv_essays_tfidf)).tocsr()\nx_test_onehot_tfidf  = hstack((x_test_onehot,x_test_titles_tfidf, x_test_essays_tfidf_unigram)).tocsr()\nprint(\"Type -> One Hot TFIDF -> x_train_cv_test: \",type(x_train_onehot_tfidf))\n# print(\"Type -> One Hot TFIDF -> cv             : \",type(x_cv_onehot_tfidf))\nprint(\"Type -> One Hot TFIDF -> x_test         : \",type(x_test_onehot_tfidf))\nprint(\"\\n\")\nprint(\"Shape -> One Hot TFIDF -> x_train_cv_test: \",x_train_onehot_tfidf.shape)\n# print(\"Shape -> One Hot TFIDF -> cv             : \",x_cv_onehot_tfidf.shape)\nprint(\"Shape -> One Hot TFIDF -> x_test         : \",x_test_onehot_tfidf.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","ff89e089":"x_train_onehot_avg_w2v = hstack((x_train_onehot, x_train_titles_avg_w2v_vectors, x_train_essays_avg_w2v_vectors)).tocsr()\n# x_cv_onehot_avg_w2v    = hstack((x_cv_onehot, x_cv_titles_avg_w2v_vectors, x_cv_essays_avg_w2v_vectors)).tocsr()\nx_test_onehot_avg_w2v  = hstack((x_test_onehot, x_test_titles_avg_w2v_vectors, x_test_essays_avg_w2v_vectors)).tocsr()\nprint(\"Type -> One Hot AVG W2V-> x_train_cv_test: \",type(x_train_onehot_avg_w2v))\n# print(\"Type -> One Hot AVG W2V-> cv             : \",type(x_cv_onehot_avg_w2v))\nprint(\"Type -> One Hot AVG W2V-> x_test         : \",type(x_test_onehot_avg_w2v))\nprint(\"\\n\")\nprint(\"Shape -> One Hot AVG W2V-> x_train_cv_test: \",x_train_onehot_avg_w2v.shape)\n# print(\"Shape -> One Hot AVG W2V-> cv             : \",x_cv_onehot_avg_w2v.shape)\nprint(\"Shape -> One Hot AVG W2V-> x_test         : \",x_test_onehot_avg_w2v.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","e61bc4c4":"x_train_onehot_tfidf_w2v = hstack((x_train_onehot, x_train_tfidf_w2v_title_vectors, x_train_essays_tfidf_w2v_vectors)).tocsr()\n# x_cv_onehot_tfidf_w2v    = hstack((x_cv_onehot, x_cv_tfidf_w2v_title_vectors, x_cv_essays_tfidf_w2v_vectors)).tocsr()\nx_test_onehot_tfidf_w2v  = hstack((x_test_onehot, x_test_tfidf_w2v_title_vectors, x_test_essays_tfidf_w2v_vectors)).tocsr()\nprint(\"Type -> One Hot TFIDF W2V -> x_train_cv_test: \",type(x_train_onehot_tfidf_w2v))\n# print(\"Type -> One Hot TFIDF W2V -> cv             : \",type(x_cv_onehot_tfidf_w2v))\nprint(\"Type -> One Hot TFIDF W2V -> x_test         : \",type(x_test_onehot_tfidf_w2v))\nprint(\"\\n\")\nprint(\"Shape -> One Hot TFIDF W2V -> x_train_cv_test: \",x_train_onehot_tfidf_w2v.shape)\n# print(\"Shape -> One Hot TFIDF W2V -> cv             : \",x_cv_onehot_tfidf_w2v.shape)\nprint(\"Shape -> One Hot TFIDF W2V -> x_test         : \",x_test_onehot_tfidf_w2v.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","4f746173":"x_train_onehot_tfidf_forSVD = hstack((x_train_onehot, x_train_essays_tfidf_unigram)).tocsr()\n# x_cv_onehot_tfidf    = hstack((x_cv_onehot,x_cv_titles_tfidf, x_cv_essays_tfidf)).tocsr()\nx_test_onehot_tfidf_forSVD  = hstack((x_test_onehot, x_test_essays_tfidf_unigram)).tocsr()\nprint(\"Type -> One Hot TFIDF - Only Essays (ForSVD) -> x_train_cv_test: \",type(x_train_onehot_tfidf_forSVD))\n# print(\"Type -> One Hot TFIDF -> cv             : \",type(x_cv_onehot_tfidf))\nprint(\"Type -> One Hot TFIDF - Only Essays (ForSVD) -> x_test         : \",type(x_test_onehot_tfidf_forSVD))\nprint(\"\\n\")\nprint(\"Shape -> One Hot TFIDF - Only Essays (ForSVD) -> x_train_cv_test: \",x_train_onehot_tfidf_forSVD.shape)\n# print(\"Shape -> One Hot TFIDF -> cv             : \",x_cv_onehot_tfidf.shape)\nprint(\"Shape -> One Hot TFIDF - Only Essays (ForSVD) -> x_test         : \",x_test_onehot_tfidf_forSVD.shape)### SET 1:  Merging ONE HOT with BOW (Title and Essay) features","7d94844a":"print(\"Done till here\")","dda08715":"# print(error aa ayaa)","63b2c044":"## SVM\nfrom sklearn.model_selection import GridSearchCV\n\nsvm_bow = linear_model.SGDClassifier(loss='hinge',class_weight=\"balanced\")\nparameters = {'alpha':[0.0001,0.001,0.01,0.1,1,10,100,1000],'penalty':['l1','l2']}\nclf = GridSearchCV(svm_bow, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n\nclf.fit(x_train_onehot_bow,y_train)\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score']\ncv_auc_std= clf.cv_results_['std_test_score']\nbestAlpha_1=clf.best_params_['alpha']\nbestPenalty_1=clf.best_params_['penalty']\nbestScore_1=clf.best_score_\nprint(\"BEST ALPHA: \",clf.best_params_['alpha'],\" BEST SCORE: \",clf.best_score_,\"BEST REGULARIZER: \",clf.best_params_['penalty']) #clf.best_estimator_.alpha","5f5c2818":"alphas = [0.0001,0.001,0.01,0.1,1,10,100,1000]\ntrain_auc_l1=[train_auc[i] for i in range(0,len(train_auc),2)]\ntrain_auc_l2=[train_auc[i] for i in range(1,len(train_auc),2)]\n\ncv_auc_l1=[cv_auc[i] for i in range(0,len(cv_auc),2)]\ncv_auc_l2=[cv_auc[i] for i in range(1,len(cv_auc),2)]\n\nlog_alphas =[]\nfor a in tqdm(alphas):\n#     b = math.log(a)\n    b = np.log10(a)\n    log_alphas.append(b)\nplt.figure(figsize=(10,5))\nplt.plot(log_alphas, train_auc_l1, label='Train AUC L1')\nplt.plot(log_alphas, train_auc_l2, label='Train AUC L2')\nplt.plot(log_alphas, cv_auc_l1, label='CV AUC L1')\nplt.plot(log_alphas, cv_auc_l2, label='CV AUC L2')\n\nplt.scatter(log_alphas, train_auc_l1, label='Train AUC L1 points')\nplt.scatter(log_alphas, train_auc_l2, label='Train AUC L2 points')\nplt.scatter(log_alphas, cv_auc_l1, label='CV AUC L1 points')\nplt.scatter(log_alphas, cv_auc_l2, label='CV AUC L2 points')\nplt.legend()\nplt.xlabel(\"Alpha: Hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"Alpha: Hyperparameter v\/s AUC\")\nplt.grid()\nplt.show()","56e0faf3":"from sklearn.linear_model import SGDClassifier","be1b2590":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\nsvm_bow_testModel = SGDClassifier(loss='hinge',penalty=bestPenalty_1,alpha=bestAlpha_1,class_weight=\"balanced\")\nsvm_bow_testModel.fit(x_train_onehot_bow, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_bow_testModel, x_train_onehot_bow)\ny_train_pred=svm_bow_testModel.decision_function(x_train_onehot_bow)\ny_test_pred=svm_bow_testModel.decision_function(x_test_onehot_bow)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set1_train=auc(train_fpr, train_tpr)\nauc_set1_test=auc(test_fpr, test_tpr)\n\n\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","fb8320da":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","454fdd1f":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","9c9e16be":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","ae6b7fa8":"## SVM\nfrom sklearn.model_selection import GridSearchCV\n\nsvm_tfidf = linear_model.SGDClassifier(loss='hinge',class_weight=\"balanced\")\nparameters = {'alpha':[0.0001,0.001,0.01,0.1,1,10,100,1000],'penalty':['l1','l2']}\nclf = GridSearchCV(svm_tfidf, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n\nclf.fit(x_train_onehot_tfidf,y_train)\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score']\ncv_auc_std= clf.cv_results_['std_test_score']\nbestAlpha_2=clf.best_params_['alpha']\nbestPenalty_2=clf.best_params_['penalty']\nbestScore_2=clf.best_score_\nprint(\"BEST ALPHA: \",clf.best_params_['alpha'],\" BEST SCORE: \",clf.best_score_,\"BEST REGULARIZER: \",clf.best_params_['penalty']) #clf.best_estimator_.alpha","7821c379":"# alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\nalphas = [0.0001,0.001,0.01,0.1,1,10,100,1000]\ntrain_auc_l1=[train_auc[i] for i in range(0,len(train_auc),2)]\ntrain_auc_l2=[train_auc[i] for i in range(1,len(train_auc),2)]\n\ncv_auc_l1=[cv_auc[i] for i in range(0,len(cv_auc),2)]\ncv_auc_l2=[cv_auc[i] for i in range(1,len(cv_auc),2)]\n\nlog_alphas =[]\nfor a in tqdm(alphas):\n#     b = math.log(a)\n    b = np.log10(a)\n    log_alphas.append(b)\nplt.figure(figsize=(10,5))\nplt.plot(log_alphas, train_auc_l1, label='Train AUC L1')\nplt.plot(log_alphas, train_auc_l2, label='Train AUC L2')\nplt.plot(log_alphas, cv_auc_l1, label='CV AUC L1')\nplt.plot(log_alphas, cv_auc_l2, label='CV AUC L2')\n\nplt.scatter(log_alphas, train_auc_l1, label='Train AUC L1 points')\nplt.scatter(log_alphas, train_auc_l2, label='Train AUC L2 points')\nplt.scatter(log_alphas, cv_auc_l1, label='CV AUC L1 points')\nplt.scatter(log_alphas, cv_auc_l2, label='CV AUC L2 points')\nplt.legend()\nplt.xlabel(\"Alpha: Hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"Alpha: Hyperparameter v\/s AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax = plt.subplot()\nax.set_facecolor(\"white\")\n# plt.grid()\nplt.show()","ea591211":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\nsvm_tfidf_testModel = SGDClassifier(loss='hinge',penalty=bestPenalty_2,alpha=bestAlpha_2,class_weight=\"balanced\")\nsvm_tfidf_testModel.fit(x_train_onehot_tfidf, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_bow_testModel, x_train_onehot_bow)\ny_train_pred=svm_tfidf_testModel.decision_function(x_train_onehot_tfidf)\ny_test_pred=svm_tfidf_testModel.decision_function(x_test_onehot_tfidf)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set2_train=auc(train_fpr, train_tpr)\nauc_set2_test=auc(test_fpr, test_tpr)\n\n\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","77e5d138":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","f3a1b066":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","b43dfed5":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","c6dfb0ee":"## SVM\nfrom sklearn.model_selection import GridSearchCV\n\nsvm_avg_w2v = linear_model.SGDClassifier(loss='hinge',class_weight=\"balanced\")\nparameters = {'alpha':[0.0001,0.001,0.01,0.1,1,10,100,1000],'penalty':['l1','l2']}\nclf = GridSearchCV(svm_avg_w2v, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n\nclf.fit(x_train_onehot_avg_w2v,y_train)\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score']\ncv_auc_std= clf.cv_results_['std_test_score']\nbestAlpha_3=clf.best_params_['alpha']\nbestPenalty_3=clf.best_params_['penalty']\nbestScore_3=clf.best_score_\nprint(\"BEST ALPHA: \",clf.best_params_['alpha'],\" BEST SCORE: \",clf.best_score_,\"BEST REGULARIZER: \",clf.best_params_['penalty']) #clf.best_estimator_.alpha","43162e84":"# alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\nalphas = [0.0001,0.001,0.01,0.1,1,10,100,1000]\ntrain_auc_l1=[train_auc[i] for i in range(0,len(train_auc),2)]\ntrain_auc_l2=[train_auc[i] for i in range(1,len(train_auc),2)]\n\ncv_auc_l1=[cv_auc[i] for i in range(0,len(cv_auc),2)]\ncv_auc_l2=[cv_auc[i] for i in range(1,len(cv_auc),2)]\n\nlog_alphas =[]\nfor a in tqdm(alphas):\n#     b = math.log(a)\n    b = np.log10(a)\n    log_alphas.append(b)\nplt.figure(figsize=(10,5))\nplt.plot(log_alphas, train_auc_l1, label='Train AUC L1')\nplt.plot(log_alphas, train_auc_l2, label='Train AUC L2')\nplt.plot(log_alphas, cv_auc_l1, label='CV AUC L1')\nplt.plot(log_alphas, cv_auc_l2, label='CV AUC L2')\n\nplt.scatter(log_alphas, train_auc_l1, label='Train AUC L1 points')\nplt.scatter(log_alphas, train_auc_l2, label='Train AUC L2 points')\nplt.scatter(log_alphas, cv_auc_l1, label='CV AUC L1 points')\nplt.scatter(log_alphas, cv_auc_l2, label='CV AUC L2 points')\nplt.legend()\nplt.xlabel(\"Alpha: Hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"Alpha: Hyperparameter v\/s AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax = plt.subplot()\nax.set_facecolor(\"white\")\n# plt.grid()\nplt.show()","0271e67d":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\nsvm_avg_w2v_testModel = SGDClassifier(loss='hinge',penalty=bestPenalty_3,alpha=bestAlpha_3,class_weight=\"balanced\")\nsvm_avg_w2v_testModel.fit(x_train_onehot_avg_w2v, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_bow_testModel, x_train_onehot_bow)\ny_train_pred=svm_avg_w2v_testModel.decision_function(x_train_onehot_avg_w2v)\ny_test_pred=svm_avg_w2v_testModel.decision_function(x_test_onehot_avg_w2v)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set3_train=auc(train_fpr, train_tpr)\nauc_set3_test=auc(test_fpr, test_tpr)\n\n\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","00897c54":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","168bdc92":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","0cf98343":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","175810f6":"## SVM\nfrom sklearn.model_selection import GridSearchCV\n\nsvm_tfidf_w2v = linear_model.SGDClassifier(loss='hinge',class_weight=\"balanced\")\nparameters = {'alpha':[0.0001,0.001,0.01,0.1,1,10,100,1000],'penalty':['l1','l2']}\nclf = GridSearchCV(svm_tfidf_w2v, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n\nclf.fit(x_train_onehot_tfidf_w2v,y_train)\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score']\ncv_auc_std= clf.cv_results_['std_test_score']\nbestAlpha_4=clf.best_params_['alpha']\nbestPenalty_4=clf.best_params_['penalty']\nbestScore_4=clf.best_score_\nprint(\"BEST ALPHA: \",clf.best_params_['alpha'],\" BEST SCORE: \",clf.best_score_,\"BEST REGULARIZER: \",clf.best_params_['penalty']) #clf.best_estimator_.alpha","f4b6d2be":"# alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\nalphas = [0.0001,0.001,0.01,0.1,1,10,100,1000]\ntrain_auc_l1=[train_auc[i] for i in range(0,len(train_auc),2)]\ntrain_auc_l2=[train_auc[i] for i in range(1,len(train_auc),2)]\n\ncv_auc_l1=[cv_auc[i] for i in range(0,len(cv_auc),2)]\ncv_auc_l2=[cv_auc[i] for i in range(1,len(cv_auc),2)]\n\nlog_alphas =[]\nfor a in tqdm(alphas):\n#     b = math.log(a)\n    b = np.log10(a)\n    log_alphas.append(b)\nplt.figure(figsize=(10,7))\nplt.plot(log_alphas, train_auc_l1, label='Train AUC L1')\nplt.plot(log_alphas, train_auc_l2, label='Train AUC L2')\nplt.plot(log_alphas, cv_auc_l1, label='CV AUC L1')\nplt.plot(log_alphas, cv_auc_l2, label='CV AUC L2')\n\nplt.scatter(log_alphas, train_auc_l1, label='Train AUC L1 points')\nplt.scatter(log_alphas, train_auc_l2, label='Train AUC L2 points')\nplt.scatter(log_alphas, cv_auc_l1, label='CV AUC L1 points')\nplt.scatter(log_alphas, cv_auc_l2, label='CV AUC L2 points')\nplt.legend()\nplt.xlabel(\"Alpha: Hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"Alpha: Hyperparameter v\/s AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax = plt.subplot()\nax.set_facecolor(\"white\")\nplt.show()","593702eb":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\nsvm_tfidf_w2v_testModel = SGDClassifier(loss='hinge',penalty=bestPenalty_4,alpha=bestAlpha_4,class_weight=\"balanced\")\nsvm_tfidf_w2v_testModel.fit(x_train_onehot_tfidf_w2v, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_bow_testModel, x_train_onehot_bow)\ny_train_pred=svm_tfidf_w2v_testModel.decision_function(x_train_onehot_tfidf_w2v)\ny_test_pred=svm_tfidf_w2v_testModel.decision_function(x_test_onehot_tfidf_w2v)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set4_train=auc(train_fpr, train_tpr)\nauc_set4_test=auc(test_fpr, test_tpr)\n\n\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","28e2c89f":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","a6ae1b0a":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","bd8c6b38":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# nltk.download('vader_lexicon')\n\nsid = SentimentIntensityAnalyzer()\n\nessays = x_train['essay']\nessays_sentiments = []\n\nfor essay in tqdm(essays):\n    res = sid.polarity_scores(essay)\n    essays_sentiments.append(res['compound']) #Considering compound as a criteria.\n\nx_train['essay_sentiment_train'] = essays_sentiments","703e9b0b":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n# nltk.download('vader_lexicon')\n\nsid = SentimentIntensityAnalyzer()\n\nessays_test = x_test['essay']\nessays_sentiments = []\n\nfor essay in tqdm(essays_test):\n    res = sid.polarity_scores(essay)\n    essays_sentiments.append(res['compound']) #Considering compound as a criteria.\n\nx_test['essay_sentiment_test'] = essays_sentiments","45d6895d":"sentiment_test=x_test['essay_sentiment_test'].values.reshape(-1,1)\nsentiment_train=x_train['essay_sentiment_train'].values.reshape(-1,1)","aaecceda":"# error aa gaya","3e62f359":"x_train_essays_tfidf_unigram.shape","64756902":"from sklearn.decomposition import TruncatedSVD\n\n# dimensions=[4,16,64,128,256,500,1000,1500,2000,3000,4000,5000,7000,9000,10000,12000,14000]\n# dimensions=[1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,177]\ndimensions=[4,16,64,128,256,500,1000,1500]\nvariance_sum=[]\n\nfor i in tqdm(dimensions):\n    svd = TruncatedSVD(n_components=i, n_iter=5)\n    svd.fit(x_train_essays_tfidf_unigram)\n    variance_sum.append(svd.explained_variance_ratio_.sum())","f9085f3a":"# dimensions=[3000,5000,7000]\n# variance_sum=[]\n\n# for i in tqdm(dimensions):\n#     svd = TruncatedSVD(n_components=i, n_iter=5)\n#     svd.fit(x_train_essays_tfidf_unigram)\n#     variance_sum.append(svd.explained_variance_ratio_.sum())","88b5fc2d":"plt.xlabel(\"Number of Components\")\nplt.ylabel(\"Percentage of Variance Explained\")\nplt.title(\"Variance Explained v\/s Number of Components\")\nplt.plot(dimensions,variance_sum,lw=2)\nplt.show()","2671e57f":"dimensions=[4,16,64,128,256,500,1000,1500,1600,1700,1800,1900,1999]\nvariance_sum=[]\n\nfor i in tqdm(dimensions):\n    svd = TruncatedSVD(n_components=i, n_iter=5)\n    svd.fit(x_train_essays_tfidf_unigram)\n    variance_sum.append(svd.explained_variance_ratio_.sum())","2fe3f972":"plt.xlabel(\"Number of Components\")\nplt.ylabel(\"Percentage of Variance Explained\")\nplt.title(\"Variance Explained v\/s Number of Components\")\nplt.plot(dimensions,variance_sum,lw=2)\nplt.show()","d681eb7b":"# error aa gaya","aac1085f":"optimalDimension=1999","1a708274":"svd = TruncatedSVD(n_components=optimalDimension, n_iter=5)\nsvd.fit(x_train_essays_tfidf_unigram)\nx_train_essays_tfidf_unigram_svd=svd.transform(x_train_essays_tfidf_unigram)\nx_test_essays_tfidf_unigram_svd=svd.transform(x_test_essays_tfidf_unigram)\n\n","c5938f48":"from scipy.sparse import hstack\n\nx_train_onehot_withTruncatedEssay = hstack((x_train_school_state_one_hot, x_train_categories_one_hot, x_train_sub_categories_one_hot, x_train_grade_category_one_hot, x_train_prefix_one_hot,x_train_quantity_normalized,x_train_teacher_previous_proj_normalized, x_train_price_normalized, sentiment_train,x_train_title_normalized,x_train_essay_normalized,x_train_essays_tfidf_unigram_svd))\n# x_cv_onehot    = hstack((x_cv_categories_one_hot, x_cv_sub_categories_one_hot, x_cv_school_state_one_hot, x_cv_prefix_one_hot, x_cv_grade_category_one_hot,x_cv_price_normalized, x_cv_teacher_previous_proj_normalized ))\nx_test_onehot_withTruncatedEssay  = hstack((x_test_school_state_one_hot, x_test_categories_one_hot, x_test_sub_categories_one_hot, x_test_grade_category_one_hot, x_test_prefix_one_hot,x_test_quantity_normalized,x_test_teacher_previous_proj_normalized, x_test_price_normalized, sentiment_test,x_test_title_normalized,x_test_essay_normalized,x_test_essays_tfidf_unigram_svd))\n\nprint(\"Type -> One Hot -> x_train: \",type(x_train_onehot_withTruncatedEssay))\nprint(\"Type -> One Hot -> x_test : \",type(x_test_onehot_withTruncatedEssay))\n# print(\"Type -> One Hot -> x_cv        : \",type(x_cv_onehot))\nprint(\"\\n\")\nprint(\"Shape -> One Hot -> x_train: \",x_train_onehot_withTruncatedEssay.shape)\nprint(\"Shape -> One Hot -> x_test : \",x_test_onehot_withTruncatedEssay.shape)\n# print(\"Shape -> One Hot -> x_cv         : \",x_cv_onehot.shape)","221217b0":"## SVM\nfrom sklearn.model_selection import GridSearchCV\n\nsvm_truncated_tfidf = linear_model.SGDClassifier(loss='hinge',class_weight=\"balanced\")\nparameters = {'alpha':[0.0001,0.001,0.01,0.1,1,10,100,1000],'penalty':['l1','l2']}\nclf = GridSearchCV(svm_truncated_tfidf, parameters, cv= 3, scoring='roc_auc',verbose=1,return_train_score=True,n_jobs=-1)\n\nclf.fit(x_train_onehot_withTruncatedEssay,y_train)\ntrain_auc= clf.cv_results_['mean_train_score']\ntrain_auc_std= clf.cv_results_['std_train_score']\ncv_auc = clf.cv_results_['mean_test_score']\ncv_auc_std= clf.cv_results_['std_test_score']\nbestAlpha_5=clf.best_params_['alpha']\nbestPenalty_5=clf.best_params_['penalty']\nbestScore_5=clf.best_score_\nprint(\"BEST ALPHA: \",clf.best_params_['alpha'],\" BEST SCORE: \",clf.best_score_,\"BEST REGULARIZER: \",clf.best_params_['penalty']) #clf.best_estimator_.alpha","2643d84d":"# alphas = [0.00001, 0.00005, 0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 2500, 5000, 10000]\nalphas = [0.0001,0.001,0.01,0.1,1,10,100,1000]\ntrain_auc_l1=[train_auc[i] for i in range(0,len(train_auc),2)]\ntrain_auc_l2=[train_auc[i] for i in range(1,len(train_auc),2)]\n\ncv_auc_l1=[cv_auc[i] for i in range(0,len(cv_auc),2)]\ncv_auc_l2=[cv_auc[i] for i in range(1,len(cv_auc),2)]\n\nlog_alphas =[]\nfor a in tqdm(alphas):\n#     b = math.log(a)\n    b = np.log10(a)\n    log_alphas.append(b)\nplt.figure(figsize=(10,7))\nplt.plot(log_alphas, train_auc_l1, label='Train AUC L1')\nplt.plot(log_alphas, train_auc_l2, label='Train AUC L2')\nplt.plot(log_alphas, cv_auc_l1, label='CV AUC L1')\nplt.plot(log_alphas, cv_auc_l2, label='CV AUC L2')\n\nplt.scatter(log_alphas, train_auc_l1, label='Train AUC L1 points')\nplt.scatter(log_alphas, train_auc_l2, label='Train AUC L2 points')\nplt.scatter(log_alphas, cv_auc_l1, label='CV AUC L1 points')\nplt.scatter(log_alphas, cv_auc_l2, label='CV AUC L2 points')\nplt.legend()\nplt.xlabel(\"Alpha: Hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"Alpha: Hyperparameter v\/s AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax = plt.subplot()\nax.set_facecolor(\"white\")\nplt.show()","98e4af78":"#https:\/\/scikitlearn.org\/stable\/modules\/generated\/sklearn.metrics.roc_curve.html#sklearn.metrics.roc_curve\nfrom sklearn.metrics import roc_curve, auc\n\nsvm_truncated_tfidf_testModel = SGDClassifier(loss='hinge',penalty=bestPenalty_5,alpha=bestAlpha_5,class_weight=\"balanced\")\nsvm_truncated_tfidf_testModel.fit(x_train_onehot_withTruncatedEssay, y_train)\n# roc_auc_score(y_true, y_score) the 2nd parameter should be probability estimates of the positive class\n# not the predicted outputs\n# y_train_pred = batch_predict(mnb_bow_testModel, x_train_onehot_bow)\ny_train_pred=svm_truncated_tfidf_testModel.decision_function(x_train_onehot_withTruncatedEssay)\ny_test_pred=svm_truncated_tfidf_testModel.decision_function(x_test_onehot_withTruncatedEssay)\n\ntrain_fpr, train_tpr, tr_thresholds = roc_curve(y_train, y_train_pred)\ntest_fpr, test_tpr, te_thresholds = roc_curve(y_test, y_test_pred)\n\nax = plt.subplot()\n\nauc_set5_train=auc(train_fpr, train_tpr)\nauc_set5_test=auc(test_fpr, test_tpr)\n\n\nax.plot(train_fpr, train_tpr, label=\"Train AUC =\"+str(auc(train_fpr, train_tpr)))\nax.plot(test_fpr, test_tpr, label=\"Test AUC =\"+str(auc(test_fpr, test_tpr)))\nplt.legend()\nplt.xlabel(\"False Positive Rate(FPR)\")\nplt.ylabel(\"True Positive Rate(TPR)\")\nplt.title(\"AUC\")\nplt.grid(b=True, which='major', color='k', linestyle=':')\nax.set_facecolor(\"white\")\nplt.show()","51bd9f97":"def predict(proba, threshould, fpr, tpr):\n    t = threshould[np.argmax(tpr*(1-fpr))]\n    # (tpr*(1-fpr)) will be maximum if your fpr is very low and tpr is very high\n    print(\"the maximum value of tpr*(1-fpr)\", max(tpr*(1-fpr)), \"for threshold\", np.round(t,3))\n    predictions = []\n    for i in proba:\n        if i>=t:\n            predictions.append(1)\n        else:\n            predictions.append(0)\n    return predictions","e82c9cea":"## TRAIN\nprint(\"=\"*100)\nfrom sklearn.metrics import confusion_matrix\nprint(\"Train confusion matrix\")\nprint(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds, train_fpr, train_tpr)))\n\nconf_matr_df_train = pd.DataFrame(confusion_matrix(y_train, predict(y_train_pred, tr_thresholds,train_fpr, train_tpr)), range(2),range(2))\n\n## Heatmaps -> https:\/\/likegeeks.com\/seaborn-heatmap-tutorial\/\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_train, annot=True,annot_kws={\"size\": 26}, fmt='g',cmap=\"YlGnBu\")","e9299559":"## TEST\nprint(\"=\"*100)\nprint(\"Test confusion matrix\")\nprint(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)))\n\nconf_matr_df_test = pd.DataFrame(confusion_matrix(y_test, predict(y_test_pred, tr_thresholds, test_fpr, test_tpr)), range(2),range(2))\nsns.set(font_scale=1.4)#for label size\nsns.heatmap(conf_matr_df_test, annot=True,annot_kws={\"size\": 16}, fmt='g')","709db611":"from prettytable import PrettyTable\n    \nx = PrettyTable()\n\nx.field_names = [\"Vectorizer\", \"Model\", \"Hyperparameter: Alpha\",\"Regularizer\",\"Train AUC\", \"Test AUC\"]\n# auc_set2_train=auc(train_fpr, train_tpr)\n# auc_set2_test=auc(test_fpr, test_tpr)\n\nx.add_row([\"BOW\", \"SVM\", bestAlpha_1, bestPenalty_1,round(auc_set1_train,2),round(auc_set1_test,2)])\nx.add_row([\"TF-IDF\", \"SVM\", bestAlpha_2, bestPenalty_2,round(auc_set2_train,2),round(auc_set2_test,2)])\nx.add_row([\"Avg W2V\", \"SVM\", bestAlpha_3, bestPenalty_3,round(auc_set3_train,2),round(auc_set3_test,2)])\nx.add_row([\"TFIDF W2V\", \"SVM\", bestAlpha_4, bestPenalty_4,round(auc_set4_train,2),round(auc_set4_test,2)])\nx.add_row([\"Truncated SVM\", \"SVM\", bestAlpha_5, bestPenalty_5,round(auc_set5_train,2),round(auc_set5_test,2)])\n\nprint(x)","f5e8d33c":"**Conclusion:** Now the number of rows reduced from 109248 to 109245 in project_data.","072d9f48":"### ->->-> 8.1.3.2: <font color='red'> SET 1<\/font> Confusion Matrix: Test","7f0e14b8":"### ->->-> 8.5.3.1: <font color='red'> SET 5<\/font> Confusion Matrix: Train","c45741eb":"## -> 7.2: SET 1:  Merging All ONE HOT with BOW (Title and Essay) features","c63221be":"## ->-> 6.3.4.1: ESSAYS","5987d6f5":"### ->->-> 8.4.3.2: <font color='red'> SET 4<\/font> Confusion Matrix: Test","c558e091":"### ->->-> 8.1.3.2: <font color='red'> SET 2<\/font> Confusion Matrix: Test","ff9fc437":"\n## ->-> 8.1.1: <font color='red'> SET 1<\/font> Hyper parameter tuning to find best 'Alpha' and Best 'Regularizer L1 or L2' using GRIDSEARCHCV","cd23e563":"## ->->-> 6.3.1.1: BOW: Essays (Train, CV, Test)","c23f1e96":"## -> 7.6: SET 6:  Merging All ONE HOT with TF-IDF (Unigram) (Only Essay) features","b14a1c7a":"## ->-> 6.2.1: Normalizing Numerical data: Price","8218c612":"**Observation:** \n1. 3 Rows had NaN values and they are not considered, thus the number of rows reduced from 109248 to 109245.","ec0b138d":"Reference: https:\/\/elitedatascience.com\/imbalanced-classes","db4715ec":"# -> 8.4:<font color='red'> SET 4<\/font>  Applying SVM on TFIDF_W2V.","9cd82b3a":"# Performing sentiment analysis","32199d4b":"# 8. SVM","db2ae5ff":"## -> 7.4: SET 3:  Merging All ONE HOT with AVG W2V (Title and Essay) features","c1edd4ac":"## ->-> 6.1.1: Vectorizing Categorical data: Clean Subject Categories","8e1d360e":"###   ->->-> 6.3.4.1. B: TF-IDF W2V: Essays -> CV","88274a23":"## -> 3.1: Text Preprocessing: Essays","f3fbc79d":"\n## ->-> 8.5.2: <font color='red'> SET 5<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","f8217c0c":"## -> 2.1: Preprocessing: Project Subject Categories","ba19aef7":"## -> 1.1: REMOVING NaN:<br>\n**As it is clearly metioned in the dataset details that TEACHER_PREFIX has NaN values, we need to handle this at the very beginning to avoid any problems in our future analysis.**","3bcaf3ea":"\n## ->-> 8.2.2: <font color='red'> SET 2<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","fa01ac6d":"###   ->->-> 6.3.4.1. C: TF-IDF W2V: Essays -> Test","dd0dfb8c":"**Query 1.1: PreProcessing Teacher Prefix Done <br>\nAction Taken: Removed '.' from the prefixes and converted to lower case**","74acb2ea":"### -> 4.3: Details of our Training, CV and Test datasets.","f26cd8c0":"**Query 1.2: PreProcessing Project Grade Done <br>\nAction Taken: Removed ' ' and '-' from the grades and converted to lower case**","027fcd51":"**Observation:** 3 Rows had NaN values and they are not considered, thus the number of rows reduced from 109248 to 109245. <br>\n**Action:** We can safely delete these, as 3 is very very small and its deletion wont impact as the original dataset is very large. <br>\nStep1: Convert all the empty strings with Nan \/\/ Not required as its NaN not empty string <br> \nStep2: Drop rows having NaN values","2e5b01cb":"\n## ->-> 8.4.3: <font color='red'> SET 4<\/font> Confusion Matrix","40dbc862":"we are going to consider\n\n       - school_state : categorical data\n       - clean_categories : categorical data\n       - clean_subcategories : categorical data\n       - project_grade_category : categorical data\n       - teacher_prefix : categorical data\n       \n       - project_title : text data\n       - text : text data\n       - project_resource_summary: text data (optinal)\n       \n       - quantity : numerical (optinal)\n       - teacher_number_of_previously_posted_projects : numerical\n       - price : numerical","872d155e":"## ->->-> 6.3.2.2: TF-IDF: Title (Train, CV, Test)","68e191cf":"### ->->-> 8.1.3.1: <font color='red'> SET 1<\/font> Confusion Matrix: Train","7261924e":"# Applying Truncated SVD on Essay","216c2888":"## ->->-> 6.3.2.0: TF-IDF (Unigram): Essays (Train, CV, Test)","2dff9d0d":"\n## ->-> 8.2.3: <font color='red'> SET 2<\/font> Confusion Matrix","8c875748":"## Merging all the non text features.","67181f24":"## ->->-> 6.3.1.2: BOW: Title (Train, CV, Test)","ced14bc9":"## -> 3.2: Text Preprocessing: Title","999d1028":"## 1. READING DATA","5b1cf56d":"## -> 3.3: Calculating the size of Title and Essay","00d380d3":"According to Andrew Ng, in the Coursera MOOC on Introduction to Machine Learning, the general rule of thumb is to partition the data set into the ratio of ***3:1:1 (60:20:20)*** for training, validation and testing respectively.","efafb648":"###   ->->->6.3.3.2. C: Avg W2V: Title -> Test","2ae25d56":"## ->-> 6.1.2: Vectorizing Categorical data: Clean Subject Sub-Categories","e8eb7048":"## SVM: DonorsChoose Dataset\n\n### A. DATA INFORMATION\n### B. OBJECTIVE\n## 1. READING THE DATASET\n- **1.1 Removing Nan**\n- **1.2 Adding Quantity in Dataset**\n\n## 2. PREPROCESSING \n- **2.1 Preprocessing: Project Subject Categories**\n- **2.2 Preprocessing: Project Subject Sub Categories**\n- **2.3 Preprocessing: Project Grade**\n\n## 3. TEXT PROCESSING\n- **3.1 Text Preprocessing: Essays**\n- **3.2 Text Preprocessing: Title**\n- **3.3 Text Preprocessing: Calculating Size of Title and Essay**\n\n## 4. SAMPLING\n- **4.1 Taking Sample from the complete dataset.**\n- **4.2 Splitting the dataset into Train, CV and Test datasets. (60:20:20)**\n    - 4.2.1: Splitting the FULL DATASET into Training and Test sets. (80:20)\n    - 4.2.2: Splitting the TRAINING Dataset further into Training and CV sets. (Not required  as we are using GridSearch)\n- **4.3 Details of our Training, CV and Test datasets.**\n\n## 5. UPSAMPLING THE MINORITY CLASS WITH REPLACEMENT STRATEGY (Not required  as we are using GridSearch)\n\n## 6. PREPARING DATA FOR MODELS\n- **6.1: VECTORIZING CATEGORICAL DATA**\n    - 6.1.1: Vectorizing Categorical data: Clean Subject Categories.\n    - 6.1.2: Vectorizing Categorical data: Clean Subject Sub-Categories.\n    - 6.1.3: Vectorizing Categorical data: School State.\n    - 6.1.4: Vectorizing Categorical data: Teacher Prefix.\n    - 6.1.5: Vectorizing Categorical data: Project Grade\n        \n- **6.2: VECTORIZING NUMERICAL DATA**\n    - 6.2.1: Standarizing Numerical data: Price\n    - 6.2.2: Standarizing Numerical data: Teacher's Previous Projects\n    - 6.2.3: Standarizing Numerical data: Title Size\n    - 6.2.4: Standarizing Numerical data: Essay Size\n    - 6.2.5: Standarizing Numerical data: Quantity\n    \n        \n- **6.3: VECTORIZING TEXT DATA**\n    - **6.3.1: BOW**\n        - 6.3.1.1: BOW: Essays (Train, CV, Test)\n        - 6.3.1.2: BOW: Title (Train, CV, Test)\n            \n    - **6.3.2: TF-IDF**\n        - 6.3.2.1: TF-IDF: Essays (Train, CV, Test)\n        - 6.3.2.2: TF-IDF: Title (Train, CV, Test)\n        \n    - **6.3.3: AVG Word2VecF**\n        - 6.3.3.1: Avg Word2Vec: Essays (A. Train, B. CV, C. Test)\n        - 6.3.3.2: Avg Word2Vec: Title (A. Train, B. CV, C. Test)\n    - **6.3.4: TF-IDF Word2VecF**\n        - 6.3.4.1: TF-IDF Word2Vec: Essays (A. Train, B. CV, C. Test)\n        - 6.3.4.2: TF-IDF Word2Vec: Title (A. Train, B. CV, C. Test)\n                \n## 7. MERGING FEATURES\n- 7.1: Merging all ONE HOT features.\n- 7.2: SET 1: Merging All ONE HOT with BOW (Title and Essay) features.\n- 7.3: SET 2: Merging All ONE HOT with TF-IDF (Title and Essay) features.\n- 7.3: SET 3: Merging All ONE HOT with Avg W2v (Title and Essay) features.\n- 7.4: SET 4: Merging All ONE HOT with TF-IDF W2v (Title and Essay) features.\n    \n## 8. SVM\n- **8.1: SET 1 Applying SVM on BOW.**\n    - 8.1.1: SET 1 Hyper parameter tuning to find the best 'Alpha' and Best 'Regularizer L1 or L2'.\n    - 8.1.2: SET 1 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.1.3: SET 1 Confusion Matrix\n        - 8.1.3.1: SET 1 Confusion Matrix: Train\n        - 8.1.3.2: SET 1 Confusion Matrix: Test\n            \n- **8.2: SET 2 Applying SVM on TF-IDF.**\n    - 8.2.1: SET 2 Hyper parameter tuning to find the best 'Alpha' and Best 'Regularizer L1 or L2'.\n    - 8.2.2: SET 2 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.2.3: SET 2 Confusion Matrix\n        - 8.2.3.1: SET 2 Confusion Matrix: Train\n        - 8.2.3.2: SET 2 Confusion Matrix: Test\n        \n- **8.3: SET 3 Applying SVM on Avg W2V.**\n    - 8.3.1: SET 3 Hyper parameter tuning to find the best 'Alpha' and Best 'Regularizer L1 or L2'.\n    - 8.3.2: SET 3 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.3.3: SET 3 Confusion Matrix\n        - 8.3.3.1: SET 3 Confusion Matrix: Train\n        - 8.3.3.2: SET 3 Confusion Matrix: Test\n        \n- **8.4: SET 4 Applying SVM on TF-IDF W2V.**\n    - 8.4.1: SET 4 Hyper parameter tuning to find the best 'Alpha' and Best 'Regularizer L1 or L2'.\n    - 8.4.2: SET 4 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.4.3: SET 4 Confusion Matrix\n        - 8.4.3.1: SET 4 Confusion Matrix: Train\n        - 8.4.3.2: SET 4 Confusion Matrix: Test\n        \n- **8.5: SET 5 Applying SVM on Categorical+Numerical+Truncated TFIDF(Essay)**\n    - **Performing Sentiment Analysis**\n    - **Applying Truncated SVD on Essay TFIDF**\n    - **Selecting the Optimal N-Factor(Dimesion) using Elbow Method**\n    - **Merging all Categorical + Numerical + Truncated IFIDF(Essay) features.**\n    - 8.5.1: SET 5 Hyper parameter tuning to find the best 'Alpha' and Best 'Regularizer L1 or L2'.\n    - 8.5.2: SET 5 TESTING the performance of the model on test data, plotting ROC Curves.\n    - 8.5.3: SET 5 Confusion Matrix\n        - 8.5.3.1: SET 5 Confusion Matrix: Train\n        - 8.5.3.2: SET 5 Confusion Matrix: Test\n\n\n## 9. CONCLUSION ","5dc1fc66":"# -> 6.1: VECTORIZING CATEGORICAL DATA","7494ba59":"# 6. PREPARING DATA FOR MODELS","77961006":"# 2: PRE-PROCESSING","0297affb":"**Observation:**\n1. Dataset is highly **IMBALANCED**.\n1. Approved Class (1) is the Majority class. And the Majority class portion in our sampled dataset: ~85%\n1. Unapproved class (0) is the Minority class. And the Minority class portion in our sampled dataset: ~15%","b15ed2d7":"\n## ->-> 8.3.3: <font color='red'> SET 3<\/font> Confusion Matrix","d3b9ba78":"# 4. SAMPLING\n## -> 4.1: Taking Sample from the complete dataset\n## NOTE: A sample of 20000 Datapoints is taken due to lack computational resource.","bee20ce6":"\n## ->-> 8.4.1: <font color='red'> SET 4<\/font> Hyper parameter tuning to find best 'Alpha' and Best 'Regularizer L1 or L2' using GRIDSEARCHCV using GRIDSEARCHCV","4e4af351":"\n## ->-> 8.3.1: <font color='red'> SET 3<\/font> Hyper parameter tuning to find best 'Alpha' and Best 'Regularizer L1 or L2' using GRIDSEARCHCV using GRIDSEARCHCV","21e1c6d8":"## ->-> 6.1.3 Vectorizing Categorical data: School State","2335f4fe":"## -> 7.1: Merging all ONE HOT features","cef0894e":"## ->-> 6.1.5 Vectorizing Categorical data: Project Grade","7596745e":"###  ->->->6.3.3.1. C: Avg W2V: Essays -> Test","34f151a8":"#### Merging Project Essays 1 2 3 4 into Essays","9150daa1":"# -> 8.1:<font color='red'> SET 1<\/font>  Applying SVM on BOW (Set 1).","cb0d9faa":"## ->-> 6.2.3: Normalizing Numerical data: Title Size","2b13f7d0":"### ->->-> 8.4.3.1: <font color='red'> SET 4<\/font> Confusion Matrix: Train","0b615db7":"#### -> Merging Price with Project Data","50f92579":"## ->-> 6.2.2: Normalizing Numerical data: Teacher's Previous Projects","10d90201":"## A. DATA INFORMATION \n### <br>About the DonorsChoose Data Set\n\nThe `train.csv` data set provided by DonorsChoose contains the following features:\n\nFeature | Description \n----------|---------------\n**`project_id`** | A unique identifier for the proposed project. **Example:** `p036502`   \n**`project_title`**    | Title of the project. **Examples:**<br><ul><li><code>Art Will Make You Happy!<\/code><\/li><li><code>First Grade Fun<\/code><\/li><\/ul> \n**`project_grade_category`** | Grade level of students for which the project is targeted. One of the following enumerated values: <br\/><ul><li><code>Grades PreK-2<\/code><\/li><li><code>Grades 3-5<\/code><\/li><li><code>Grades 6-8<\/code><\/li><li><code>Grades 9-12<\/code><\/li><\/ul>  \n **`project_subject_categories`** | One or more (comma-separated) subject categories for the project from the following enumerated list of values:  <br\/><ul><li><code>Applied Learning<\/code><\/li><li><code>Care &amp; Hunger<\/code><\/li><li><code>Health &amp; Sports<\/code><\/li><li><code>History &amp; Civics<\/code><\/li><li><code>Literacy &amp; Language<\/code><\/li><li><code>Math &amp; Science<\/code><\/li><li><code>Music &amp; The Arts<\/code><\/li><li><code>Special Needs<\/code><\/li><li><code>Warmth<\/code><\/li><\/ul><br\/> **Examples:** <br\/><ul><li><code>Music &amp; The Arts<\/code><\/li><li><code>Literacy &amp; Language, Math &amp; Science<\/code><\/li>  \n  **`school_state`** | State where school is located ([Two-letter U.S. postal code](https:\/\/en.wikipedia.org\/wiki\/List_of_U.S._state_abbreviations#Postal_codes)). **Example:** `WY`\n**`project_subject_subcategories`** | One or more (comma-separated) subject subcategories for the project. **Examples:** <br\/><ul><li><code>Literacy<\/code><\/li><li><code>Literature &amp; Writing, Social Sciences<\/code><\/li><\/ul> \n**`project_resource_summary`** | An explanation of the resources needed for the project. **Example:** <br\/><ul><li><code>My students need hands on literacy materials to manage sensory needs!<\/code<\/li><\/ul> \n**`project_essay_1`**    | First application essay<sup>*<\/sup>  \n**`project_essay_2`**    | Second application essay<sup>*<\/sup> \n**`project_essay_3`**    | Third application essay<sup>*<\/sup> \n**`project_essay_4`**    | Fourth application essay<sup>*<\/sup> \n**`project_submitted_datetime`** | Datetime when project application was submitted. **Example:** `2016-04-28 12:43:56.245`   \n**`teacher_id`** | A unique identifier for the teacher of the proposed project. **Example:** `bdf8baa8fedef6bfeec7ae4ff1c15c56`  \n**`teacher_prefix`** | Teacher's title. One of the following enumerated values: <br\/><ul><li><code>nan<\/code><\/li><li><code>Dr.<\/code><\/li><li><code>Mr.<\/code><\/li><li><code>Mrs.<\/code><\/li><li><code>Ms.<\/code><\/li><li><code>Teacher.<\/code><\/li><\/ul>  \n**`teacher_number_of_previously_posted_projects`** | Number of project applications previously submitted by the same teacher. **Example:** `2` \n\n<sup>*<\/sup> See the section <b>Notes on the Essay Data<\/b> for more details about these features.\n\nAdditionally, the `resources.csv` data set provides more data about the resources required for each project. Each line in this file represents a resource required by a project:\n\nFeature | Description \n----------|---------------\n**`id`** | A `project_id` value from the `train.csv` file.  **Example:** `p036502`   \n**`description`** | Desciption of the resource. **Example:** `Tenor Saxophone Reeds, Box of 25`   \n**`quantity`** | Quantity of the resource required. **Example:** `3`   \n**`price`** | Price of the resource required. **Example:** `9.95`   \n\n**Note:** Many projects require multiple resources. The `id` value corresponds to a `project_id` in train.csv, so you use it as a key to retrieve all resources needed for a project:\n\nThe data set contains the following label (the value you will attempt to predict):\n\nLabel | Description\n----------|---------------\n`project_is_approved` | A binary flag indicating whether DonorsChoose approved the project. A value of `0` indicates the project was not approved, and a value of `1` indicates the project was approved.","3fa74d50":"###   ->->-> 6.3.4.2. B: TF-IDF W2V: Title -> CV","604f95a9":"###   ->->-> 6.3.4.2. C: TF-IDF W2V: Title -> Test","4be02a1b":"### ->->-> 8.3.3.1: <font color='red'> SET 3<\/font> Confusion Matrix: Train","1957a8f3":"###   ->->-> 6.3.4.2. A: TF-IDF W2V: Title -> Train","ea206e8e":"\n## ->-> 8.1.2: <font color='red'> SET 1<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","1b2687df":"# -> 8.5:<font color='red'> SET 5<\/font>  Applying SVM on ALL EXCEPT TEXT.","ceceffc8":"###  ->->->6.3.3.1. B:  Avg W2V: Essays -> CV","843c79a7":"### -> -> 4.2.2: Splitting the TRAINING Dataset further into Training and CV sets.","09b9aa8e":"## ->-> 6.3.4.2 TITLE","a80e5f21":"###    -> -> 4.2.1: Splitting the FULL DATASET into Training and Test sets. (80:20)","7243106b":"**Teacher Prefix has NAN values, that needs to be cleaned.\nRef: https:\/\/stackoverflow.com\/a\/50297200\/4433839**","39d5d953":"###   ->->-> 6.3.4.1. A: TF-IDF W2V: Essays -> Train","b959094b":"### ->->-> 8.2.3.1: <font color='red'> SET 2<\/font> Confusion Matrix: Train","b2c44322":"###   ->->->6.3.3.2. B: Avg W2V: Title -> CV","e1701db2":"## ->-> 6.1.4 Vectorizing Categorical data: Teacher Prefix","63218492":"## Printing the Explained Variance of the Truncated SVM\n","f6f15f0b":"# -> 6.2: VECTORIZING NUMERICAL DATA","a81a280c":"# B. OBJECTIVE\nThe primary objective is to implement the k-Nearest Neighbor Algo on the DonorChoose Dataset and measure the accuracy on the Test dataset.","6b20e767":"### ->->-> 8.3.3.2: <font color='red'> SET 3<\/font> Confusion Matrix: Test","b17a2579":"# -> 8.2:<font color='red'> SET 2<\/font>  Applying SVM on TFIDF.","883091ba":"## ->-> 6.2.4: Normalizing Numerical data: Essay Size","9d4fc9d0":"\n## ->-> 8.1.3: <font color='red'> SET 1<\/font> Confusion Matrix","71903aa2":"**Conclusion:**\n1. Resampling is performed on the Training data.\n1. Training data in now **BALANCED**.","91e29cb3":"**Observation:**\n1. The proportion of Majority class of 85% and Minority class of 15% is maintained in Training, CV and Testing dataset.","ab48d59d":"# 7. MERGING FEATURES","496e8fcf":"# ->-> 6.3.1: BOW","b0b911b0":"\n## ->-> 8.4.2: <font color='red'> SET 4<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","9d395aab":"\n## ->-> 8.5.1: <font color='red'> SET 5<\/font> Hyper parameter tuning to find best 'Alpha' and Best 'Regularizer L1 or L2' using GRIDSEARCHCV using GRIDSEARCHCV","23d1fe0c":"# 5. UPSAMPLING THE MINORITY CLASS WITH REPLACEMENT STRATEGY: NOT DONE ","44c59f5f":"### ->->-> 8.5.3.2: <font color='red'> SET 5<\/font> Confusion Matrix: Test","9dca27bf":"#### Checking total number of enteries with NaN values","8aa8514e":"###   ->->->6.3.3.2. A: Avg W2V: Title -> Train","85a62438":"# ->-> 6.3.2: TF-IDF","d71b50b3":"## ->->-> 6.3.2.1: TF-IDF (BiGrams): Essays (Train, CV, Test)","b9e8883f":"# 9. CONCLUSION","92175b08":"# -> 6.3.3:  AVG Word2Vec","2df7e4cb":"\n## ->-> 8.5.3: <font color='red'> SET 5<\/font> Confusion Matrix","9bc859f0":"\n## ->-> 8.2.1: <font color='red'> SET 2<\/font> Hyper parameter tuning to find best 'Alpha' and Best 'Regularizer L1 or L2' using GRIDSEARCHCV","32c43195":"**Conlusion**\n1. **UPSAMPLING** needs to be done on the Minority class to avoid problems related to Imbalanced dataset.\n1. Upsampling will be done by _**\"Resample with replacement strategy\"**_","3bb5f32d":"# ->6.3.4: TFIDF - W2V","4ae54fb6":"## ->-> 6.3.3.2: TITLE","850fa967":"## ->-> 6.3.3.1: Avg W2v: ESSAYS","a995765f":"###  ->->->6.3.3.1. A:  Avg W2V: Essays -> Train","5041dd12":"## -> 2.3: Preprocessing: Project Grade Category","f977c50e":"## -> 7.5: SET 4:  Merging All ONE HOT with TF-IDF W2V (Title and Essay) features","c8e43182":"## -> 7.3: SET 2:  Merging All ONE HOT with TF-IDF (Title and Essay) features","9031b0b0":"\n## ->-> 8.3.2: <font color='red'> SET 3<\/font> TESTING the performance of the model on test data, plotting ROC Curves.","072897ac":"- https:\/\/www.appliedaicourse.com\/course\/applied-ai-course-online\/lessons\/handling-categorical-and-numerical-features\/","58a58997":"## -> 4.2: Splitting the dataset into Train, CV and Test datasets. (60:20:20)","2753cb87":"## ->-> 6.2.5: Normalizing Numerical data: Quantity","249b98e5":"# 3. TEXT PROCESSING","db544f68":"# -> 8.3:<font color='red'> SET 3<\/font>  Applying SVM on AvgW2V.","08482aa6":"## Adding Quantity in the dataset","363a88e4":"## -> 2.2: Preprocessing: Project Subject Sub Categories","3b66d23b":"# -> 6.3: VECTORIZING TEXT DATA"}}