{"cell_type":{"f5f6ba7c":"code","1c7a9222":"code","ab992c4d":"code","37e87ea8":"code","3d879d49":"code","a6f2b3d1":"code","1ee3fd5e":"code","2fa4a688":"code","8d1879bf":"code","fad50652":"code","58d3ee5c":"code","c7e4c7c9":"code","c3ef0b09":"code","2f2b1411":"code","ef540060":"code","f8d4875d":"code","8c85a274":"code","9a75963c":"code","f5649a65":"code","4fb912b4":"code","60cc5f29":"code","a0860055":"code","195d2560":"code","36ba8752":"code","581be017":"code","1e6b1e60":"code","bd54ba7a":"code","c088e921":"code","d0ddd9d7":"code","03e6d395":"code","bcc886b4":"code","bcb98ddd":"code","48433f45":"code","8f2b9565":"code","645ab49d":"markdown","7e1e17e0":"markdown","2bc19896":"markdown","ed17c743":"markdown","8d919f0c":"markdown","1b11a86f":"markdown","1749aec0":"markdown","93423853":"markdown","ffd8952b":"markdown","fc54193a":"markdown","aa3d3e1e":"markdown","c6451c74":"markdown","d973668b":"markdown","aa10dcf7":"markdown","e69685e7":"markdown","53080527":"markdown","ece7e884":"markdown","96d1b474":"markdown","53ae4edf":"markdown","be8ff147":"markdown","daf61d46":"markdown","25ec6545":"markdown","af7dcd75":"markdown","50b252f5":"markdown","1ac9cf33":"markdown","33cfb5e0":"markdown","899ac4b0":"markdown","cfff1636":"markdown","8bc5f736":"markdown"},"source":{"f5f6ba7c":"# Basic packages\nimport pathlib\nimport numpy as np \nimport pandas as pd\nimport altair as alt\nimport matplotlib\nimport seaborn as sns\nimport functools\n\n# Sklearn packages\nfrom sklearn import linear_model, metrics, ensemble, model_selection, preprocessing\n\n# Boosting packages\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# Parameter Tuning packages\nimport optuna","1c7a9222":"path = pathlib.Path('\/kaggle\/input\/30-days-of-ml')","ab992c4d":"list(path.iterdir())","37e87ea8":"train = pd.read_csv(f'{path}\/train.csv')\ntest = pd.read_csv(f'{path}\/test.csv')\nsample = pd.read_csv(f'{path}\/sample_submission.csv')","3d879d49":"# function to compute rmse.\ndef rmse(y, pred): return round(metrics.mean_squared_error(y, pred, squared=False), 6)\n\n# function to fit and evaluate model.\ndef mfe(model, xtrain, ytrain, xval, yval):\n    model.fit(xtrain, ytrain)\n    preds_train = model.predict(xtrain)\n    preds_val = model.predict(xval)\n    print(f'RMSE Train: {rmse(ytrain, preds_train)} RMSE Valid: {rmse(yval, preds_val)}')\n    \n# function to print result of an optuna study.   \ndef result(study):\n    print(\"Number of finished trials: \", len(study.trials))\n    print(\"Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n\n# function to write predictions.        \ndef submit(preds, fname=None):\n    df_preds = pd.DataFrame({'id': sample.id.values, 'target': preds})\n    df_preds.to_csv(fname, index=False)\n    return 'Predictions exported to csv'        ","a6f2b3d1":"train.head(3)","1ee3fd5e":"train.isnull().sum().any()","2fa4a688":"# Define features and target.\nkey = ['id']\ncat = [col for col in train.columns if col.startswith('cat')]\ncont = [col for col in train.columns if col.startswith('cont')]\nfeats = cat + cont\ntarget = ['target']\n\n# Create base data set X & y\nX = train.loc[:, feats]\ny = train.loc[:, target].values.flatten()\n\n# Prepare test dataset\nxtest = test.loc[:, feats]\nxtest = xtest.copy()\n\n# Prepare train and validation dataset\nxtrain, xval, ytrain, yval = model_selection.train_test_split(X, y, test_size=.25, random_state=42, shuffle=True)\n\n# avoid warning\nxtrain = xtrain.copy()\nxval = xval.copy()\n\noe = preprocessing.OrdinalEncoder(categories='auto', dtype=np.int32)\nxtrain.loc[:, cat] = oe.fit_transform(xtrain.loc[:, cat])\nxval.loc[:, cat] = oe.transform(xval.loc[:, cat])\nxtest.loc[:, cat] = oe.transform(xtest.loc[:, cat])","8d1879bf":"# Compelete dataset to be used to training before submission.\nx_all =  train.loc[:, feats]; y_all = train.loc[:, target].values.flatten()","fad50652":"# Define the train, validation and test Dmatrix objects.\ndtrain = xgb.DMatrix(xtrain, label=ytrain)\ndval = xgb.DMatrix(xval, label=yval)\ndtest = xgb.DMatrix(xtest)","58d3ee5c":"# Define the model params.\nparams = {\n    # tune\n    'eta': 0.2,\n    'gamma': 1,\n    'max_depth':6 ,\n    'min_child_weight': 1,\n    'subsample': 1,\n    'colsample_bytree': 1,\n    'colsample_bylevel': 1,\n    'colsample_bynode': 1, \n    'lambda': 1,\n    'alpha': 0,\n    # settings\n    'tree_method': 'approx',\n    'objective': 'reg:squarederror',\n    'eval_metric':'rmse',\n    'seed': 42\n} ","c7e4c7c9":"%%time\nm1 = xgb.train(\n    params, \n    dtrain, \n    num_boost_round = 500, \n    evals = [(dtrain, 'train'), (dval, 'val')], \n    early_stopping_rounds = 30,\n    verbose_eval = 50\n)","c3ef0b09":"%%time\n# eta: 0.1\nxgb.cv(\n    params, #  Booster params.\n    dtrain, # Data to be trained.\n    num_boost_round=1000, # Number of boosting iterations.\n    nfold=5, # Number of folds in CV\n    metrics='rmse', # Evaluation metrics to be watched in CV\n    early_stopping_rounds=50, \n    verbose_eval=50,  \n    seed=0)","2f2b1411":"%%time\n# eta: 0.2\nxgb.cv(\n    params, #  Booster params.\n    dtrain, # Data to be trained.\n    num_boost_round=1000, # Number of boosting iterations.\n    nfold=5, # Number of folds in CV\n    metrics='rmse', # Evaluation metrics to be watched in CV\n    early_stopping_rounds=50, \n    verbose_eval=50,  \n    seed=0)","ef540060":"bst","f8d4875d":"params = {\n    'eta': 0.05,\n    'gamma': 1,\n    'max_depth':4,\n    'min_child_weight': 7,\n    'subsample': 1,\n    'colsample_bytree': 0.7128975448248305,\n    'colsample_bylevel': 0.962957311362518,\n    'lambda':  0.7518860170628275,\n    'alpha': 0.10918875912028536,\n    'tree_method': 'exact',\n    'objective': 'reg:squarederror',\n    'eval_metric':'rmse',\n    'seed': 42,\n    \n} ","8c85a274":"%%time\nm = xgb.train(\n    params, \n    dtrain, \n    num_boost_round = 2000, \n    evals = [(dtrain, 'train'), (dval, 'val')], \n    early_stopping_rounds = 30,\n    verbose_eval = 50\n)","9a75963c":"%%time\nfinal_preds = []\nkf = model_selection.KFold(n_splits=10)\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X=X, y=y)):\n    xtrain = X.iloc[trn_idx]; ytrain = y[trn_idx]\n    xval = X.iloc[val_idx]; yval = y[val_idx]\n    xtrain = xtrain.copy(); xval=xval.copy()\n    xtest = test.loc[:, feats]; xtest = xtest.copy()\n    \n    # preprocessing\n    oe = preprocessing.OrdinalEncoder(categories='auto', dtype=np.int32)\n    xtrain.loc[:, cat] = oe.fit_transform(xtrain.loc[:, cat])\n    xval.loc[:, cat] = oe.transform(xval.loc[:, cat])\n    xtest.loc[:, cat] = oe.transform(xtest.loc[:, cat])\n    \n    dtrain = xgb.DMatrix(xtrain, label=ytrain)\n    dval = xgb.DMatrix(xval, label=yval)\n    dtest = xgb.DMatrix(xtest)\n    \n    model = xgb.train(params, dtrain, num_boost_round=1450)\n    preds_val = model.predict(dval)\n    preds_test = model.predict(dtest)\n    final_preds.append(preds_test)\n    print(fold, rmse(yval, preds_val))","f5649a65":"preds = np.mean(np.column_stack(final_preds), axis=1)","4fb912b4":"submit(preds, fname='aug18_10fold_1.csv')","60cc5f29":"# Function used for optimization using KFold cross validation.\ndef xgb_optimize(trial, X=X, y=y):\n    \n    # prepare the data.\n    kf = model_selection.KFold(n_splits=5)\n    errors = []\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X=X, y=y)):\n        xtrain = X.iloc[train_idx]; ytrain = y[train_idx]\n        xval = X.iloc[val_idx]; yval = y[val_idx]\n        xtrain = xtrain.copy(); xval = xval.copy()\n        \n        # preprocessing\n        oe = preprocessing.OrdinalEncoder(categories='auto', dtype=np.int32)\n        xtrain.loc[:, cat] = oe.fit_transform(xtrain.loc[:, cat])\n        xval.loc[:, cat] = oe.transform(xval.loc[:, cat])\n        \n        # prepare Dmatrix\n        dtrain = xgb.DMatrix(xtrain, label=ytrain)\n        dval = xgb.DMatrix(xval, label=yval)\n        \n        # define parameter space\n        params = {\n            'eta': 0.2,\n            'gamma': 1, #trial.suggest_float('gamma', 1e-3, 1.0, log=True),\n            'max_depth': trial.suggest_int('max_depth', 3, 15),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n            'subsample': 1, #trial.suggest_float('subsample', 0.7, 1.0),\n            'colsample_bytree': 1, #0.7128975448248305, #trial.suggest_float('colsample_bytree', 0.5, 1.0),\n            'colsample_bylevel': 1, #0.962957311362518, #trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n            'lambda':1, #trial.suggest_float('lambda', 0, 1),\n            'alpha': 0, #trial.suggest_float('alpha', 0, 1),\n            'booster':'gbtree',\n            'tree_method': 'exact',\n            'objective': 'reg:squarederror',\n            'eval_metric':'rmse',\n            'seed': 42,\n        }   \n        \n        # train the model\n        model = xgb.train(params, dtrain,  num_boost_round = 98)\n        preds = model.predict(dval)\n        fold_error = rmse(yval, preds)\n        errors.append(fold_error)\n\n    return np.mean(errors)\n","a0860055":"# Function used for optimization with a single train and validation split.\ndef xgb_optimize(trial, X=X, y=y):        \n    xtrain, xval, ytrain, yval = model_selection.train_test_split(X, y, test_size=.25, shuffle=True, random_state=42)\n    xtrain = xtrain.copy(); xval = xval.copy()\n    \n    # preprocessing\n    oe = preprocessing.OrdinalEncoder(categories='auto', dtype=np.int32)\n    xtrain.loc[:, cat] = oe.fit_transform(xtrain.loc[:, cat])\n    xval.loc[:, cat] = oe.transform(xval.loc[:, cat])\n    \n    # prepare Dmatrix\n    dtrain = xgb.DMatrix(xtrain, label=ytrain)\n    dval = xgb.DMatrix(xval, label=yval)\n    \n    # define parameter space\n    params = {\n        # tune\n        'eta': 0.2,\n        'gamma': 1, #trial.suggest_float('gamma', 1e-3, 1.0, log=True),\n        'max_depth': 4, #trial.suggest_int('max_depth', 3, 15),\n        'min_child_weight':7, #trial.suggest_int('min_child_weight', 1, 20),\n        'subsample': 1, #trial.suggest_float('subsample', 0.7, 1.0),\n        'colsample_bytree': 0.7128975448248305, #trial.suggest_float('colsample_bytree', 0.5, 1.0),\n        'colsample_bylevel': 0.962957311362518, #trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n        'lambda': trial.suggest_float('lambda', 0, 1),\n        'alpha': trial.suggest_float('alpha', 0, 1),\n        # settings\n        'booster':'gbtree',\n        'tree_method': 'exact',\n        'objective': 'reg:squarederror',\n        'eval_metric':'rmse',\n        'seed': 42,\n    } \n    \n    # train the model\n    model = xgb.train(params, dtrain,  num_boost_round = 142)\n    preds = model.predict(dval)\n    return rmse(yval, preds)","195d2560":"%%time\nobjective = functools.partial(xgb_optimize, X=X, y=y)\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=20)\nresult(study)","36ba8752":"%%time\nobjective = functools.partial(xgb_optimize, X=X, y=y)\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\nresult(study)","581be017":"%%time\nobjective = functools.partial(xgb_optimize, X=X, y=y)\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nresult(study)","1e6b1e60":"%%time\nobjective = functools.partial(xgb_optimize, X=X, y=y)\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nresult(study)","bd54ba7a":"%%time\nobjective = functools.partial(xgb_optimize, X=X, y=y)\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)\nresult(study)","c088e921":"%%time\nobjective = functools.partial(xgb_optimize, X=X, y=y)\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\nresult(study)","d0ddd9d7":"m = lgb.LGBMRegressor(\n    num_leaves=31, \n    max_depth=- 1, \n    learning_rate=0.1, \n    n_estimators=100, \n    min_child_weight=0.001, \n    min_child_samples=20, \n    subsample=1.0, \n    subsample_freq=0, \n    colsample_bytree=1.0, \n    reg_alpha=0.0, \n    reg_lambda=0.0, \n    random_state=42, \n    n_jobs=- 1,\n)","03e6d395":"m.fit(\n    xtrain, \n    ytrain, \n    eval_set=[(xtrain, ytrain), (xval, yval)], \n    eval_names=['train', 'valid'], \n    eval_metric='rmse', \n    early_stopping_rounds=30, \n    verbose=10, \n    categorical_feature='auto', \n)","bcc886b4":"%%time\nlgb_preds = []\nkf = model_selection.KFold(n_splits=10)\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X=X, y=y)):\n    xtrain = X.iloc[trn_idx]; ytrain = y[trn_idx]\n    xval = X.iloc[val_idx]; yval = y[val_idx]\n    xtrain = xtrain.copy(); xval=xval.copy()\n    xtest = test.loc[:, feats]; xtest = xtest.copy()\n    \n    # preprocessing\n    oe = preprocessing.OrdinalEncoder(categories='auto', dtype=np.int32)\n    xtrain.loc[:, cat] = oe.fit_transform(xtrain.loc[:, cat])\n    xval.loc[:, cat] = oe.transform(xval.loc[:, cat])\n    xtest.loc[:, cat] = oe.transform(xtest.loc[:, cat])\n    \n    m.fit(xtrain, ytrain)\n    preds_val = model.predict(xval)\n    preds_test = model.predict(xtest)\n    lgb_preds.append(preds_test)\n    print(fold, rmse(yval, preds_val))","bcb98ddd":"preds = np.mean(np.column_stack(final_preds), axis=1)","48433f45":"submit(preds, fname='lgb1.csv')","8f2b9565":"lgb.fit(\n    xtrain, \n    ytrain, \n    eval_set=[(xtrain, ytrain), (xval, yval)], \n    eval_names=['train', 'valid'], \n    eval_metric='auc', \n    early_stopping_rounds=10, \n    verbose=10, \n    categorical_feature='auto', \n)","645ab49d":"## Iteration with tuned params","7e1e17e0":"# LightGBM","2bc19896":"### Experiment-1","ed17c743":"# Scratchpad","8d919f0c":"**Trial 1**","1b11a86f":"**Result** <\/br>\nBest trial:\n  Value: 0.721623\n  Params: \n    gamma: 0.4647127907135867","1749aec0":"Result\n","93423853":"### Experiment - 4","ffd8952b":"**Params Used** <\/br>    \nparams = {\n'eta': 0.2,\n'gamma': .465, \n'max_depth': 4, \n'min_child_weight':7,\n'subsample': 1, \n'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.5, 1.0),\n'lambda': 1, \n'alpha': 0, \n'booster':'gbtree',\n'tree_method': 'exact',\n'objective': 'reg:squarederror',\n'eval_metric':'rmse',\n'seed': 42,\n} \n","fc54193a":"# Data Preparation","aa3d3e1e":"**Trial 2**","c6451c74":"params space used <\/br>\nparams = {\ntune\n 'round': 98\n 'eta': 0.2, \n 'gamma':1,\n 'max_depth': trial.suggest_int('max_depth', 3, 15),\n 'min_child_weight': trial.suggest_int('min_child_weight', 1, 20),\n 'subsample': 1, \n 'colsample_bytree': 1, \n 'colsample_bylevel': 1, \n 'lambda': 1, \n 'alpha': 0, \n 'booster':'gbtree',\n 'tree_method': 'exact',\n 'objective': 'reg:squarederror',\n 'eval_metric':'rmse',\n 'seed': 42,\n } \n","d973668b":"### Using KFold cross validation","aa10dcf7":"**Trial 2** <br\/>\nChanged gamma to 1 from .465","e69685e7":"params = {\n'eta': 0.2,\n'gamma': 1, \n'max_depth': 4, \n'min_child_weight':7, \n'subsample': 1, \n'colsample_bytree': 0.7128975448248305, \n'colsample_bylevel': 0.962957311362518, \n'lambda': trial.suggest_float('lambda', 0, 1, log=True),\n'alpha': trial.suggest_float('alpha', 0, 1, log=True),\n'booster':'gbtree',\n'tree_method': 'exact',\n'objective': 'reg:squarederror',\n'eval_metric':'rmse',\n'seed': 42,\n} ","53080527":"**Result**\n* [0]\ttrain-rmse:5.47025\tval-rmse:5.47039\n* [50]\ttrain-rmse:0.70840\tval-rmse:0.72609\n* [87]\ttrain-rmse:0.69508\tval-rmse:0.72650","ece7e884":"### Experiment-2","96d1b474":"# EDA","53ae4edf":"# XGBoost","be8ff147":"# Optuna: Hyperparameter Tuning ","daf61d46":"**Result** <\/br>\nBest trial:\n  Value: 0.721472\n  Params: \n    max_depth: 4\n    min_child_weight: 7","25ec6545":"### XGBoost Cross Validation","af7dcd75":"### Experiment - 3","50b252f5":"## Objective function","1ac9cf33":"**Result** <br\/>\nBest trial: <br\/>\n  Value: **0.721346** <br\/>\n  Params: <br\/>\n    colsample_bytree: 0.7128975448248305 <\/br>\n    colsample_bylevel: 0.962957311362518\n","33cfb5e0":"# [30 Days of ML](https:\/\/www.kaggle.com\/c\/30-days-of-ml\/overview\/evaluation)","899ac4b0":"## Iteration 1","cfff1636":"# Utility Code","8bc5f736":"**Params**  <br\/>\nparams = {\n        # tune\n        'eta': 0.2, \n        'gamma': trial.suggest_float('gamma', 1e-3, 1.0, log=True),\n        'max_depth': 4, \n        'min_child_weight':7, \n        'subsample': 1, \n        'colsample_bytree': 1, \n        'colsample_bylevel': 1,\n        'lambda': 1, \n        'alpha': 0, \n        'booster':'gbtree',\n        'tree_method': 'exact',\n        'objective': 'reg:squarederror',\n        'eval_metric':'rmse',\n        'seed': 42,\n    } "}}