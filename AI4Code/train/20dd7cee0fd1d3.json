{"cell_type":{"18f5f6bd":"code","99a0f156":"code","00a6bc04":"code","d22869f5":"code","2edf4ca9":"code","4a66993d":"code","140db3ea":"code","f88f6a04":"code","6631add1":"code","72404af0":"code","7604a48e":"code","453f008b":"code","d103b120":"code","712ee0d8":"markdown","ab151a8f":"markdown","39e91fdd":"markdown","3fc05b0d":"markdown","fbdab7a1":"markdown","65ce2406":"markdown","388e875d":"markdown","8a9120b3":"markdown","67772d48":"markdown","1950a1ec":"markdown","0416f5b8":"markdown","3b41ce5a":"markdown"},"source":{"18f5f6bd":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","99a0f156":"train = pd.read_csv(r'..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntrain","00a6bc04":"from sklearn.preprocessing import LabelEncoder\n\n# Read in the data\ntrain = pd.read_csv(r'..\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest = pd.read_csv(r'..\/input\/tabular-playground-series-mar-2021\/test.csv')\n\n# Adding labels to distinguish the data after it has been concatenated \ntrain['label'] = 'train'\ntest['label'] = 'score'\n\n# Putting the data together\nconcat_df = pd.concat([train, test])\n\n# Labeling the categorical and continuous columns for identification\ncategorical_cols=['cat'+str(i) for i in range(19)]\ncontinous_cols=['cont'+str(i) for i in range(11)]\n\n# Encoding all the categorical variables\nfor e in categorical_cols:\n    le = LabelEncoder()\n    concat_df[e]=le.fit_transform(concat_df[e])\n    train[e]=le.transform(train[e])\n    test[e]=le.transform(test[e])\n\n# Removing the variables with exceptionally low frequencies from the training dataset\nthreshold = 10 # Any frequency less than 10 will be removed\nvalue_counts_cat10 = train['cat10'].value_counts()\nto_remove = value_counts_cat10[value_counts_cat10 <= threshold].index\ntrain['cat10'].replace(to_remove, np.nan, inplace=True)\n    \n# Removing the variables with exceptionally low frequencies from the testing dataset\nvalue_counts_cat10 = test['cat10'].value_counts()\nto_remove = value_counts_cat10[value_counts_cat10 <= threshold].index\ntest['cat10'].replace(to_remove, np.nan, inplace=True)\n\n# Creating a target \ntarget=train['target']\n\n# Drop the labels from the beginning to determine which is which\ntrain = train.drop('label', axis=1)\ntest = test.drop('label', axis=1)","d22869f5":"from pandas_profiling import ProfileReport\n\ntrain_profile = ProfileReport(train, 'EDA')\ntrain_profile","2edf4ca9":"for e in continous_cols:\n    print(e, 'Skew Value: ', train[e].skew())","4a66993d":"from scipy import stats\n\ntrain_cont7_box_cox = stats.boxcox(train['cont7'])[0]\nprint('Cont 7 box_cox_skew: ', pd.Series(train_cont7_box_cox).skew())\ntrain_cont8_box_cox = stats.boxcox(train['cont8'])[0]\nprint('Cont 8 box_cox_skew: ', pd.Series(train_cont8_box_cox).skew())\ntrain_cont9_box_cox = stats.boxcox(train['cont9'])[0]\nprint('Cont 9 box_cox_skew: ', pd.Series(train_cont9_box_cox).skew())\ntrain_cont10_box_cox = stats.boxcox(train['cont10'])[0]\nprint('Cont 10 box_cox_skew: ', pd.Series(train_cont10_box_cox).skew())","140db3ea":"to_be_transformed_cols = ['cont7', 'cont8', 'cont9', 'cont10']\ntransformed_cols = [train_cont7_box_cox,\n                    train_cont8_box_cox,\n                    train_cont9_box_cox,\n                    train_cont10_box_cox]\n\ntrain['cont7'] = train_cont7_box_cox\ntrain['cont8'] = train_cont8_box_cox\ntrain['cont9'] = train_cont9_box_cox\ntrain['cont10'] = train_cont10_box_cox","f88f6a04":"from pandas_profiling import ProfileReport\n\ntrain_profile = ProfileReport(train, 'EDA')\ntrain_profile","6631add1":"from lightgbm import LGBMClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# Making the dataset for training and testing\nX = train.drop('target', axis=1)\nY = train['target']\n\n# Splitting the data into training and testing \n# Saving 20% of the data for testing\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n\n# Storing the data as a LightGBM dataset\nd_train = lgb.Dataset(x_train, label=y_train)\n\ndef tuninglightgbm(num_leaves, learning_rate, max_depth):\n    \n    # Setting the parameters for the model\n    # Setting the tested variables to the variables as indicated\n    params = {}\n    params['learning_rate']=learning_rate\n    params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n    params['num_leaves']=num_leaves\n    params['objective']='binary' #Binary target feature\n    params['metric']='auc' \n    params['max_depth']=max_depth\n    \n    # Training the model and then scoring it\n    lightgbm_model = lgb.train(params, d_train, 500)\n    y_pred_tuning_lightgbm = lightgbm_model.predict(x_test)\n    auc_score = roc_auc_score(y_test, y_pred_tuning_lightgbm)\n    \n    # These values will be reported back to the function and the best will be recorded\n    parameters = []\n    parameters = [auc_score]\n    parameters.append(num_leaves)\n    parameters.append(learning_rate)\n    parameters.append(max_depth)\n    \n    return parameters   \n    \n# Values being tested    \nnum_leaves = [10, 15, 20]\nlearning_rate = [0.01, 0.1, 0.2, 0.3]\nmax_depth = [10, 15, 20]\n\nbest_auc_score = 0\n\n# Iterating from all the values one by one\nfor i in range(len(num_leaves)):\n    for j in range(len(learning_rate)):\n        for k in range(len(max_depth)):\n            parameters = tuninglightgbm(num_leaves[i], learning_rate[j], max_depth[k])\n            \n            # The best set of values will have the highest AUC by the end.\n            if parameters[0] > best_auc_score:\n                best_auc_score = parameters[0]\n                best_num_leaves = parameters[1]\n                best_learning_rate = parameters[2]\n                best_max_depth = parameters[3]\n\n# Reporting the best scores overall\nprint('Best AUC: %.6f' % best_auc_score)\nprint('Best Number of Leaves: ', best_num_leaves)\nprint('Best Learning Rate: ', best_learning_rate)\nprint('Best Max Depth: ', best_max_depth)","72404af0":"from catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# Making the dataset for training and testing\nX = train.drop('target', axis=1)\nY = train['target']\n\n# Splitting the data into training and testing \n# Saving 20% of the data for testing\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n\ndef tuningcatboost(learning_rate, max_depth):\n    \n    \n    # Setting the tested variables to the variables as indicated\n    model = CatBoostClassifier(iterations=1000,\n                               learning_rate=learning_rate,\n                               max_depth=max_depth\n                              )\n\n    model.fit(x_train, y_train, verbose=False)\n\n    y_pred_catboost = model.predict_proba(x_test)[:,1]\n    \n    auc_score = roc_auc_score(y_test, y_pred_catboost)\n    \n    # These values will be reported back to the function and the best will be recorded\n    parameters = []\n    parameters = [auc_score]\n    parameters.append(learning_rate)\n    parameters.append(max_depth)\n    \n    return parameters   \n    \n# These are the values being tested for CatBoost    \nlearning_rate = [0.05, 0.1, 0.2, 0.3]\nmax_depth = [6, 8, 10]\n\nbest_auc_score = 0\n\nfor i in range(len(learning_rate)):\n        for k in range(len(max_depth)):\n            parameters = tuningcatboost(learning_rate[i], max_depth[k])\n            \n            # The best set of values will have the highest AUC by the end.\n            if parameters[0] > best_auc_score:\n                best_auc_score = parameters[0]\n                best_learning_rate = parameters[1]\n                best_max_depth = parameters[2]\n                \n# Reporting the best scores at the end\nprint('Best AUC: %.6f' % best_auc_score)\nprint('Best Learning Rate: ', best_learning_rate)\nprint('Best Max Depth: ', best_max_depth)","7604a48e":"from xgboost import XGBClassifier\n\n# XGBoost performs better with one-hot encoding\n# This is marking the categorical columns\ncategorical_cols=['cat'+str(i) for i in range(19)]\n\n# Creating a new dataframe object with one-hot encoding\ntrain_one_hot = pd.get_dummies(train, columns=categorical_cols)\nprint(train_one_hot.shape)\n\n# Making the datasets for training and testing\nX = train_one_hot.drop('target', axis=1)\nY = train_one_hot['target']\n\n# Splitting the data into training and testing \n# Saving 20% of the data for testing\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n\ndef tuningxgb(learning_rate, depth):\n    params={\n        'n_estimators':500,\n        'objective': 'binary:logistic',\n        'learning_rate': learning_rate, # Testing the best learning rate\n        'gamma':0.1,\n        'subsample':0.8,\n        'colsample_bytree':0.3,\n        'min_child_weight':3,\n        'max_depth': depth, # Testing the best depth\n        'seed':1024,\n        }\n\n    model = XGBClassifier(**params, early_stopping_rounds=100)\n    \n    model.fit(x_train, y_train)\n\n    y_pred_xgb = model.predict_proba(x_test)[:,1]\n    \n    auc_score = roc_auc_score(y_test, y_pred_xgb)\n    \n    # These values will be reported back to the function and the best will be recorded\n    parameters = []\n    parameters = [auc_score]\n    parameters.append(learning_rate)\n    parameters.append(depth)\n    \n    return parameters \n    \n# These are the values being tested for CatBoost\nlearning_rate = [0.05, 0.1, 0.2, 0.3]\nmax_depth = [6, 8, 10]\n\nbest_auc_score = 0\n\nfor i in range(len(learning_rate)):\n    for k in range(len(max_depth)):\n        parameters = tuningxgb(learning_rate[i], max_depth[k])\n        \n        # The best set of values will have the highest AUC by the end.\n        if parameters[0] > best_auc_score:\n            best_auc_score = parameters[0]\n            best_learning_rate = parameters[1]\n            best_max_depth = parameters[2]\n\n# Reporting the best scores at the end\nprint('Best AUC: %.6f' % best_auc_score)\nprint('Best Learning Rate: ', best_learning_rate)\nprint('Best Max Depth: ', best_max_depth)","453f008b":"from lightgbm import LGBMClassifier\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\n# Making the dataset for training and testing\nX = train.drop('target', axis=1)\nY = train['target']\n\n# Splitting the data into training and testing \n# Saving 20% of the data for testing\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n\n# Storing the data as a LightGBM dataset\nd_train = lgb.Dataset(x_train, label=y_train)\n\ndef tuninglightgbm(num_leaves, learning_rate, max_depth):\n    \n    # Setting the parameters for the model\n    # Setting the tested variables to the variables as indicated\n    params = {}\n    params['learning_rate']=learning_rate\n    params['boosting_type']='gbdt' #GradientBoostingDecisionTree\n    params['num_leaves']=num_leaves\n    params['objective']='binary' #Binary target feature\n    params['metric']='auc' \n    params['max_depth']=max_depth\n    \n    # Training the model and then scoring it\n    lightgbm_model = lgb.train(params, d_train, 500)\n    y_pred_lightgbm = lightgbm_model.predict(x_test)\n    \n    return y_pred_lightgbm\n\ny_pred_lightgbm = tuninglightgbm(20, 0.1, 10)\n\n\n# *******************************************\n# Starting CatBoost\n# *******************************************\n\ndef tuningcatboost(learning_rate, max_depth):\n    \n    \n    # Setting the tested variables to the variables as indicated\n    model = CatBoostClassifier(iterations=1000,\n                               learning_rate=learning_rate,\n                               max_depth=max_depth\n                              )\n\n    model.fit(x_train, y_train, verbose=False)\n\n    y_pred_catboost = model.predict_proba(x_test)[:,1]\n    \n    auc_score = roc_auc_score(y_test, y_pred_catboost)\n    return y_pred_catboost\n    \ny_pred_catboost = tuningcatboost(0.05, 8)\n\n\n# *******************************************\n# Starting XGBoost\n# *******************************************\n\ncategorical_cols=['cat'+str(i) for i in range(19)]\n\ntrain_one_hot = pd.get_dummies(train, columns=categorical_cols)\nprint(train_one_hot.shape)\n\nX = train_one_hot.drop('target', axis=1)\nY = train_one_hot['target']\n\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n\ndef tuningxgb(learning_rate, depth):\n    params={\n        'n_estimators':500,\n        'objective': 'binary:logistic',\n        'learning_rate': learning_rate,\n        'gamma':0.1,\n        'subsample':0.8,\n        'colsample_bytree':0.3,\n        'min_child_weight':3,\n        'max_depth': depth,\n        'seed':1024,\n        }\n\n    model = XGBClassifier(**params, early_stopping_rounds=100)\n    \n    model.fit(x_train, y_train)\n\n    y_pred_xgb = model.predict_proba(x_test)[:,1]\n    \n    auc_score = roc_auc_score(y_test, y_pred_xgb)\n    print('XGBoost AUC: ', auc_score)\n    \n    return y_pred_xgb\n\ny_pred_xgb = tuningxgb(0.05, 10)\n\ny_pred_avg = (y_pred_xgb + y_pred_lightgbm + y_pred_catboost) \/ 3\ny_pred_avg\nroc_auc_score(y_test, y_pred_avg)","d103b120":"y_pred_avg = (3.5*y_pred_xgb + y_pred_lightgbm + y_pred_catboost) \/ 5.5\ny_pred_avg\nroc_auc_score(y_test, y_pred_avg)","712ee0d8":"Best AUC: 0.890524\n\nBest Learning Rate:  0.05\n\nBest Max Depth:  8","ab151a8f":"Early on into this competition, I realized there were some variables in the testing dataset that were not contained within the training dataset. The machine learning algorithms were not working correctly because they were encountering variables they had not trained for, and that was causing the model problems.\n\nTo alleviate this, I completed the following steps:\n- read the training and testing data in as separate variables\n- Marked with variable was training data and which was testing data\n- Concatenated the dataframes\n- Marked which columns contained categorical and continuous variables\n- Encoded the categorical columns with the concatenated dataframe\n- Removed any categorical variable with frequencies 10 or less in training and test data\n- Droped the training and testing labels","39e91fdd":"From this, I transformed the continuous variables 7-10 using the box-cox methodology since none of the data for these variables has a negative value. ","3fc05b0d":"After playing with the weights of the three different approaches, I found that if XGB were considered the dominant method, I had the best results.\n\nFrom here, I trained all three methods on the entirety of the training data and then used XGBoost as weight 3.5 with the others unadjusted for my highest score for the March Competition.","fbdab7a1":"Hello! If you're reading this, thanks for taking the time to look over my work. This is the abridged version of what I did for the March competition, and I was pretty happy with how well I did (top 38%) since it's only my second competition. If you see anything you would do differently or improve upon, please feel free to let me know!","65ce2406":"Best AUC: 0.889088\n\nBest Number of Leaves:  10\n\nBest Learning Rate:  0.2\n\nBest Max Depth:  10","388e875d":"Double checking that everything worked for these variables.","8a9120b3":"Reading in and glancing over the training file","67772d48":"By combining all three of these methods, I have a AUC of 0.8923692458946146, which is actually slightly worse than the XGBoost method by itself. Based on this, I'm going to see how different weights may affect the overall outcome of the model.","1950a1ec":"For this competition, I initially tested XGB, LightGBM, and CatBoost to see what their performances looked like. After seeing they were all performing pretty similarly, I decided to combine the three methods for my submission to this competition. In order to start this process, I looked for the best parameters using the code below.","0416f5b8":"After looking through the data, I found that several of the variables were not as normal as I would like, so I checked the skewness for each. Any variable that had a skew outside of [-0.5,0.5] I adjusted until it fell within that parameter.\n\nWhile there were other variables that were obviously not normal (such as cont3 and cont4), I found that after transforming them, I got a decrease in performance. Given this, I have not included those transformations here.","3b41ce5a":"Best AUC: 0.893018\n\nBest Learning Rate:  0.05\n\nBest Max Depth:  10"}}