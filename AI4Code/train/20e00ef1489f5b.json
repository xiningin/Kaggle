{"cell_type":{"ca17de10":"code","a5a2f290":"code","ec513f46":"code","071a010d":"code","919f8e69":"code","5b8c4e14":"code","6d0f5e52":"code","802802e7":"code","5e170891":"code","b79c8aed":"code","32c046c5":"code","99b0a1e9":"code","4b52104f":"code","80425702":"code","e8d70b15":"code","36b59577":"code","38a2f515":"code","218b6d38":"code","0a9542f8":"code","f4ad5c0b":"code","f8a5288d":"code","4732f663":"code","afdcd95b":"code","089a0365":"code","c3e6204d":"code","5bb82bff":"code","aa4f8b10":"code","ab02b287":"code","33f50713":"code","dc90ac18":"code","77586d4e":"code","f09c71c2":"code","e0ffb45e":"code","f992f378":"code","30f73fdb":"code","7b7cffa3":"code","5a6a30a0":"code","b93454fc":"code","9c84bda3":"code","64f16695":"code","b792b008":"code","8cc826d3":"code","a16be645":"code","c7d5f342":"code","c553f65a":"code","5f100d8b":"code","3c9c475b":"code","96ce9e0e":"code","a9d6c571":"code","bbccd093":"code","3114c650":"code","736164a4":"code","b8cce0ce":"code","968e2cf4":"code","f20baee4":"code","67095a73":"code","446c35ab":"code","3e5d2d4e":"code","6128db1d":"code","93362140":"code","edfdef85":"code","4e72ae16":"code","2c6205d9":"code","a6ff2ed9":"code","59f8c3e0":"code","e252a81e":"code","27d8b071":"code","70b8b12d":"code","cf39bb10":"code","e5faeb8e":"code","0476e319":"code","09f6a82c":"code","6ff3714a":"code","529f3014":"code","8c7939d8":"code","9cf94372":"code","b8409cbd":"code","cadc2db6":"code","4976a844":"code","366a3a40":"code","e0fd26b1":"code","a802dc9a":"code","7165ee9d":"markdown","43c88105":"markdown","11b14ea0":"markdown","3a5765af":"markdown","29c8f5a8":"markdown","8fd36f9b":"markdown","84cef9fe":"markdown","e717273c":"markdown","89afb1a5":"markdown","88e3f7e9":"markdown"},"source":{"ca17de10":"# Importing libraries\n\nimport numpy as np,pandas as pd, seaborn as sns, matplotlib.pyplot as plt","a5a2f290":"import warnings\nwarnings.filterwarnings(\"ignore\")","ec513f46":"# Reading the data\n\ndf = pd.read_csv(\"..\/input\/leads-of-x-education\/Leads.csv\")","071a010d":"pd.set_option('display.max_columns',500)","919f8e69":"df.head()","5b8c4e14":"df.describe()","6d0f5e52":"df.info()","802802e7":"# Checking the null % in the columns\n\n((df.isnull().sum()\/9240)*100).round(2)","5e170891":"a = list(round(100*(df.isnull().sum()\/len(df.index)),2)>35)","b79c8aed":"# droping the column which has null values more than 35%\n\ndf.drop(df.loc[:,a],1,inplace = True)\ndf.info()","32c046c5":"((df.isnull().sum()\/9240)*100)","99b0a1e9":"# Replacing the \"Select\" as np.nan\n\ndf = df.replace('Select',np.nan)\n((df.isnull().sum()\/9240)*100)","4b52104f":"# Droping the some irrelevant and most null values columns\n\ndf.drop(['Lead Profile','City','Country','How did you hear about X Education'],1,inplace=True)\ndf.info()","80425702":"# Cheking the inblance data in each column\n\nfor c in df:\n    print(df[c].astype('category').value_counts())","e8d70b15":"# Droping the most inbalanced columns\n\ndf.drop(df.iloc[:,13:26],1,inplace=True)\ndf.info()","36b59577":"# Droping the row which has null values\n\ndf = df[~pd.isnull(df['What is your current occupation'])]","38a2f515":"df.isnull().sum()","218b6d38":"# Droping the null row of totalvisits colums\n\ndf = df[~pd.isnull(df['TotalVisits'])]\ndf.drop(['Prospect ID','Lead Number'],1,inplace=True)\ndf.info()","0a9542f8":"# Checking the balance of the target variable\n\n(sum(df['Converted'])\/len(df['Converted'].index))*100","f4ad5c0b":"plt.figure(figsize=[20,8])\nplt.subplot(1,2,1)\nsns.countplot(x = 'Lead Source',hue = 'Converted',data = df)\nplt.xticks(rotation = 90)\n\nplt.subplot(1,2,2)\nsns.countplot(x = 'Lead Origin',hue = 'Converted',data = df)\nplt.xticks(rotation = 90)\n\nplt.show()","f8a5288d":"plt.figure(figsize=[18,6])\nplt.subplot(1,2,1)\nsns.countplot(x = 'Do Not Email',hue = 'Converted',data = df)\nplt.xticks(rotation = 90)\n\nplt.subplot(1,2,2)\nsns.countplot(x = 'Do Not Call',hue = 'Converted',data = df)\nplt.xticks(rotation = 90)\n\nplt.show()","4732f663":"plt.figure(figsize=[18,6])\nplt.subplot(1,2,1)\nsns.countplot(x = 'Last Activity',hue = 'Converted',data = df)\nplt.xticks(rotation = 90)\n\nplt.subplot(1,2,2)\nsns.countplot(x = 'What is your current occupation',hue = 'Converted',data = df)\nplt.xticks(rotation = 90)\n\nplt.show()","afdcd95b":"sns.countplot(x = 'Last Notable Activity',hue ='Converted',data = df)\nplt.xticks(rotation = 90)\nplt.show()","089a0365":"plt.figure(figsize=[10,8])\nsns.countplot(x = 'Specialization',hue ='Converted',data = df)\nplt.xticks(rotation = 90)\nplt.show()","c3e6204d":"# Creating the dummies of catagorical variable\n\ndf = pd.get_dummies(df,drop_first=True)","5bb82bff":"df.info()","aa4f8b10":"from sklearn.model_selection import train_test_split","ab02b287":"X = df.drop(['Converted'],1)\ny = df['Converted']","33f50713":"X_train,X_test,y_train,y_test = train_test_split(X,y,train_size = 0.7,test_size = 0.3,random_state = 100)","dc90ac18":"from sklearn.preprocessing import MinMaxScaler","77586d4e":"# Scaling the dataframe \n\nscaler = MinMaxScaler()\nX_train.iloc[:,:3] = scaler.fit_transform(X_train.iloc[:,:3])\nX_train.head()","f09c71c2":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFE","e0ffb45e":"logreg = LogisticRegression()","f992f378":"# Slelceting the 15 best variables  \n\nrfe = RFE(logreg,15)\nrfe = rfe.fit(X_train,y_train)","30f73fdb":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","7b7cffa3":"col = X_train.columns[rfe.support_]\ncol","5a6a30a0":"X_train = X_train[col]","b93454fc":"# importing the statemodel\n\nimport statsmodels.api as sm","9c84bda3":"# Building the logistic regression model\n\nX_train_sm = sm.add_constant(X_train)\nlogm1 = sm.GLM(y_train,X_train_sm,family=sm.families.Binomial())","64f16695":"res = logm1.fit()\nres.summary()","b792b008":"# Checking the vif\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","8cc826d3":"vif = pd.DataFrame()\nvif['features'] = X_train.columns\nvif['VIF'] =[variance_inflation_factor(X_train.values,i)for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif[\"VIF\"],2)\nvif = vif.sort_values(by = 'VIF',ascending = False)\nvif","a16be645":"# # Droping the hige p values columns\n\nX_train.drop(\"Last Notable Activity_Had a Phone Conversation\",1,inplace=True)","c7d5f342":"# refit the model \n\nX_train_sm = sm.add_constant(X_train)\nlogm2 = sm.GLM(y_train,X_train_sm,family=sm.families.Binomial())\nres = logm2.fit()\nres.summary()","c553f65a":"vif = pd.DataFrame()\nvif['features'] = X_train.columns\nvif['VIF'] =[variance_inflation_factor(X_train.values,i)for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif[\"VIF\"],2)\nvif = vif.sort_values(by = 'VIF',ascending = False)\nvif","5f100d8b":"# predicting the values using model\n\ny_train_pred = res.predict(sm.add_constant(X_train))\ny_train_pred","3c9c475b":"y_train_pred = y_train_pred.values.reshape(-1)","96ce9e0e":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values,'Converted_Prob':y_train_pred})\ny_train_pred_final.head()","a9d6c571":"# Creating the new column of prediction using cutt off as 0.5\n\ny_train_pred_final[\"Predicted\"] = y_train_pred_final[\"Converted_Prob\"].map(lambda x:1 if x>0.5 else 0)\ny_train_pred_final.head()","bbccd093":"# Creating the confusion matrix\n\nfrom sklearn import metrics\nconfusion  = metrics.confusion_matrix(y_train_pred_final.Converted,y_train_pred_final.Predicted)\nprint(confusion)","3114c650":"# Checking the accuracy\n\nmetrics.accuracy_score(y_train_pred_final.Converted,y_train_pred_final.Predicted)","736164a4":"tp = confusion[1,1]\ntn = confusion[0,0]\nfp = confusion[0,1]\nfn = confusion[1,0]","b8cce0ce":"# Calculationg the sensitivity and specificity\nprint(\"sensitivity-->\",tp\/(tp+fn))\nprint(\"specificity-->\",tn\/(tn+fp))","968e2cf4":"# Using the ROC function\n\ndef draw_roc(actual,probs):\n    fpr,tpr,threshold = metrics.roc_curve(actual,probs,drop_intermediate = False)\n    auc_score = metrics.roc_auc_score(actual,probs)\n    plt.figure(figsize=[10,10])\n    plt.plot(fpr,tpr,label = \"ROC curve (area = %0.2f)\"%auc_score)\n    plt.plot([0,1],[0,1],\"k--\")\n    plt.xlim([0.0,1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    return None","f20baee4":"fpr,tpr,thresholds = metrics.roc_curve(y_train_pred_final.Converted,y_train_pred_final.Converted_Prob)","67095a73":"draw_roc(y_train_pred_final.Converted,y_train_pred_final.Converted_Prob)","446c35ab":"# Making columns for different probability cutoff\n\nnumber = [float(x)\/10 for x in range(10)]\nfor i in number:\n    y_train_pred_final[i] = y_train_pred_final.Converted_Prob.map(lambda x:1 if x>i else 0)\ny_train_pred_final.head()","3e5d2d4e":"# finding the optimal cutoff using precision recall curve\n\nfrom sklearn.metrics import precision_recall_curve","6128db1d":"p,r,threshold = precision_recall_curve(y_train_pred_final.Converted,y_train_pred_final.Converted_Prob)","93362140":"# Plottin the precision recall curve\n\nplt.plot(threshold, p[:-1], \"g-\")\nplt.plot(threshold, r[:-1], \"r-\")\nplt.show()","edfdef85":"# Creating the dataframe of accuracy,sensitivity,specificity\n\ncuttoff_df = pd.DataFrame(columns=[\"prob\",'accuracy','sensitivity','specificity'])\nfor i in number:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted,y_train_pred_final[i])\n    total1 = sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cuttoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cuttoff_df)","4e72ae16":"# Finding the optimal cutoff using thes sensi-speci curve\n\ncuttoff_df.plot.line(x = 'prob',y = ['accuracy','sensitivity','specificity'])\nplt.show()","2c6205d9":"# Making final prediction using 0.43 cutoff\n\ny_train_pred_final['final_pred'] = y_train_pred_final.Converted_Prob.map(lambda x:1 if x>0.43 else 0)\ny_train_pred_final.head()","a6ff2ed9":"# checking the accuracy \nmetrics.accuracy_score(y_train_pred_final.Converted,y_train_pred_final.final_pred)","59f8c3e0":"# creating the confusion matrix \nconf2 = metrics.confusion_matrix(y_train_pred_final.Converted,y_train_pred_final.final_pred)\nconf2","e252a81e":"tp = conf2[1,1]\ntn = conf2[0,0]\nfp = conf2[0,1]\nfn = conf2[1,0]","27d8b071":"# Calculationg the sensitivity and specificity\nprint(\"sensitivity-->\",tp\/(tp+fn))\nprint(\"specificity-->\",tn\/(tn+fp))","70b8b12d":"# scaling the test data set \nX_test.iloc[:,:3] = scaler.transform(X_test.iloc[:,:3])\nX_test.head()","cf39bb10":"X_test = X_test[col]\nX_test.drop([\"Last Notable Activity_Had a Phone Conversation\"],1,inplace = True)\n","e5faeb8e":"# Making the prediction\n\ny_test_pred =res.predict(sm.add_constant(X_test)) ","0476e319":"y_test_pred","09f6a82c":"# Converting to dataframe\ny_pred_1 = pd.DataFrame(y_test_pred)\ny_pred_1.head()","6ff3714a":"y_test_df = pd.DataFrame(y_test)","529f3014":"# Reseting the index\n\ny_pred_1.reset_index(drop = True,inplace=True)\ny_test_df.reset_index(drop = True,inplace=True)","8c7939d8":"# Creating the final data set\ny_pred_final = pd.concat([y_test_df,y_pred_1],axis = 1)\ny_pred_final.head()","9cf94372":"y_pred_final = y_pred_final.rename(columns = {0:'Converted_prob'})","b8409cbd":"y_pred_final.head()","cadc2db6":"# Making the predcition using the 0.42 cutoff\n\ny_pred_final[\"final_prediction\"] = y_pred_final.Converted_prob.map(lambda x:1 if x>0.43 else 0)\ny_pred_final.head()","4976a844":"# Checking accuracy score\nmetrics.accuracy_score(y_pred_final.Converted,y_pred_final.final_prediction)","366a3a40":"# creating the confusion matrix\nconfusion2 = metrics.confusion_matrix(y_pred_final.Converted,y_pred_final.final_prediction)\nconfusion2","e0fd26b1":"tp = confusion2[1,1]\ntn = confusion2[0,0]\nfp = confusion2[0,1]\nfn = confusion2[1,0]","a802dc9a":"# Calculationg the sensitivity and specificity\nprint(\"sensitivity-->\",tp\/(tp+fn))\nprint(\"specificity-->\",tn\/(tn+fp))","7165ee9d":"## Making prediction on Test data set","43c88105":"## Data reading","11b14ea0":"### The area under curve is 0.87","3a5765af":"## Model building","29c8f5a8":"## Data Cleaning","8fd36f9b":"## Model Evaluation","84cef9fe":"## The Optimal cutoff came out ","e717273c":"## Finding the opimal cutt off","89afb1a5":"## Creating the dummies of the catgoerical variable","88e3f7e9":"## Data visulization"}}