{"cell_type":{"09c70ca6":"code","61695d85":"code","816041d7":"code","052e7a3e":"code","b42515af":"code","dde70c9f":"code","e91390c5":"code","9a342654":"code","2c1ffee3":"code","f4d6380e":"code","889bef3f":"code","f2e95dba":"code","44332808":"code","4f91b502":"code","d3bf419a":"code","3786e8cc":"code","e54b5fc1":"code","8b1a5abd":"code","fd7bffaf":"code","c271c50d":"code","1730488c":"code","22d2832a":"code","69964a81":"code","d44f7cf2":"code","2604934b":"code","2b9b2be7":"code","d6616b10":"code","3112b790":"code","42bbfa9f":"code","7d208634":"code","452fa168":"code","45e45717":"code","067d6ed5":"code","9da16e14":"code","100043cc":"code","207a9df5":"code","06a353f5":"code","b006c9be":"code","ee5d3cac":"code","68b5ed3d":"code","7786733d":"code","679f5f11":"code","779724d9":"code","534e8e2c":"code","dd2e3ff6":"code","52613956":"code","d4c72faf":"code","173334d4":"code","78d433b6":"code","cb2fac29":"code","b1c315c6":"code","d9ce1544":"code","fb12ddfb":"code","9d874216":"code","3f31c66e":"code","891c10b7":"code","80ba44c0":"code","549fe94f":"code","39474345":"code","07fdb844":"code","a29e4d8e":"code","8de850fe":"code","a9e6edb2":"markdown","e7790b99":"markdown","248bf530":"markdown","9e5d55f4":"markdown","d9a0927b":"markdown","4a0081bb":"markdown","057fdfbe":"markdown","cd6e7ac3":"markdown","71fd04d8":"markdown","fb01a8b1":"markdown","f4a5cb82":"markdown","c7b8b833":"markdown","50a1837e":"markdown","00601558":"markdown","ac793474":"markdown","279b463f":"markdown","28d12cef":"markdown","b0bb4923":"markdown","3bda43e2":"markdown","eb14621d":"markdown","7443b6ff":"markdown","769d6482":"markdown","26d3710e":"markdown","20b085ce":"markdown","1b5a2367":"markdown","5d5a3528":"markdown","010db08b":"markdown","b9207fdc":"markdown","ab116416":"markdown","d5069c92":"markdown","071f79ce":"markdown","a8f9b685":"markdown","61dcb771":"markdown","9512948b":"markdown","c7ca2a3e":"markdown","86283c7f":"markdown","8e1a85d5":"markdown","99a6b56a":"markdown","f57b5161":"markdown","fe87d515":"markdown","fba76f3d":"markdown","42aafb11":"markdown","8d048f53":"markdown","d6033cc0":"markdown","77440ee9":"markdown","b3fb030c":"markdown"},"source":{"09c70ca6":"# ignoring warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# importing modules\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nimport statsmodels.api as sm\nfrom sklearn.metrics import r2_score\n\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',81)","61695d85":"housing_df = pd.read_csv('https:\/\/cdn.upgrad.com\/UpGrad\/temp\/9cefd607-bbb7-443d-bb3e-eac9fef04453\/train.csv')\nhousing_df.head()","816041d7":"# 1460 rows and 81 columns\nhousing_df.shape","052e7a3e":"# column names\nhousing_df.columns","b42515af":"# column dtypes\nhousing_df.dtypes.sort_values()","dde70c9f":"housing_df.info()","e91390c5":"housing_df.describe()","9a342654":"housing_df.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0,800000))\nplt.show()","2c1ffee3":"housing_df.plot.scatter(x='TotalBsmtSF', y='SalePrice', ylim=(0,800000))\nplt.show()","f4d6380e":"plt.subplots(figsize=(7,7))\nsns.boxplot(x='OverallQual', y=\"SalePrice\", data=housing_df)","889bef3f":"plt.subplots(figsize=(26, 8))\nsns.boxplot('Neighborhood', y=\"SalePrice\", data=housing_df)\nplt.show()","f2e95dba":"plt.subplots(figsize=(25,8))\nfig = sns.boxplot(x='YearBuilt', y=\"SalePrice\", data=housing_df)\nplt.xticks(rotation=90);","44332808":"plt.subplots(figsize=(45, 24))\nsns.heatmap(housing_df.corr(), annot = True, vmax=.8, square=True);","4f91b502":"housing_df.drop(['Id','MoSold'],axis=1,inplace=True)\nhousing_df.columns","d3bf419a":"null_col = housing_df.columns[housing_df.isnull().sum()>0]\nnull_col","3786e8cc":"housing_df[null_col].isnull().sum().sort_values(ascending=False)","e54b5fc1":"housing_df.MasVnrType.value_counts()","8b1a5abd":"housing_df[housing_df.MasVnrType.isnull() & ~housing_df.MasVnrArea.isnull()]","fd7bffaf":"housing_df.LotFrontage.describe(percentiles=[0.2,0.4,0.6,0.8,0.9,1])","c271c50d":"housing_df.LotFrontage.plot.box()","1730488c":"housing_df.LotFrontage[housing_df.LotFrontage.isnull()] = housing_df.LotFrontage.median()","22d2832a":"housing_df.Electrical.value_counts()","69964a81":"col_dict = {'PoolQC':\"No Pool\",'MiscFeature':\"None\",'Alley':\"No alley access\",'Fence':\"No Fence\",'FireplaceQu':\"No Fireplace\",\n           'GarageYrBlt':0,'GarageType':\"No Garage\",'GarageFinish':\"No Garage\",'GarageQual':\"No Garage\",\n            'GarageCond':\"No Garage\",'BsmtExposure':\"No Basement\",'BsmtFinType2':\"No Basement\",'BsmtFinType1':\"No Basement\",\n           'BsmtCond':\"No Basement\",'BsmtQual':\"No Basement\",'MasVnrType':\"None\",'MasVnrArea':0,'Electrical':\"SBrkr\"}\nfor i in col_dict:\n    housing_df[i][housing_df[i].isnull()] = col_dict[i]\n    print('Count of null values after imputing = ',housing_df[i].isnull().sum())\n    print(housing_df[i].value_counts())   ","d44f7cf2":"# checking null values in columns\nhousing_df.isnull().sum().sort_values(ascending=False)","2604934b":"# checking null values in rows\nhousing_df[housing_df.isnull().sum(axis = 1) > 0]","2b9b2be7":"housing_df[housing_df.duplicated(keep=False)]","d6616b10":"# Check the outliers in all the numeric columns\nnum_col = ['ScreenPorch','LotFrontage','EnclosedPorch','BsmtFinSF2','OpenPorchSF','WoodDeckSF',\n           'MasVnrArea','2ndFlrSF','GarageArea','BsmtFinSF1','TotalBsmtSF', '1stFlrSF',\n           'BsmtUnfSF' ,'GrLivArea','LotArea']\nplt.figure(figsize=(17, 20))\nn = 1\nfor i in num_col:\n    plt.subplot(5,3,n)\n    sns.boxplot(y = i, data = housing_df)\n    n+=1\nplt.show()","3112b790":"housing_df[num_col].describe(percentiles=[.25,.5,.75,.90,.95,.99,1])","42bbfa9f":"# Removed Outliers\nfor col in num_col:\n    percentiles = housing_df[col].quantile([0.01,0.99]).values\n    housing_df[col][housing_df[col] <= percentiles[0]] = percentiles[0]\n    housing_df[col][housing_df[col] >= percentiles[1]] = percentiles[1]","7d208634":"def getHighNumericalValueCounts():\n    column = []\n    numerical_columns = housing_df.select_dtypes(include=['int64', 'float'])\n    for col in (numerical_columns):\n        if(housing_df[col].value_counts().max() >= 1241):\n            column.append(col)\n    return column\n\ncolumnsToBeRemoved = getHighNumericalValueCounts()\ncolumnsToBeRemoved\nhousing_df.drop(columnsToBeRemoved,axis=1,inplace=True)","452fa168":"from scipy import stats\nfrom scipy.stats import norm\n\n# checking the target variable\nsns.distplot(housing_df.SalePrice,fit=norm)","45e45717":"housing_df[\"SalePrice\"] = np.log1p(housing_df[\"SalePrice\"])\nsns.distplot(housing_df.SalePrice,fit = norm)","067d6ed5":"# features where manual level encoding is required \ncat_col_manual_encoding = ['ExterQual', 'ExterCond', 'BsmtQual','BsmtExposure', 'BsmtCond', 'HeatingQC', 'KitchenQual', \n                        'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']","9da16e14":"# Manual encoding for some categorical variable\ndef manual_encoding(x):\n    if x=='Ex':\n        x = 5\n    elif x=='Gd':\n        x = 4\n    elif x=='TA' or 'Av':\n        x = 3\n    elif x=='Fa' or 'Mn':\n        x = 2\n    elif x=='Po':\n        x = 1\n    elif x=='No':\n        x= 1\n    else:\n        x= 0\n    return x\n\nfor i in cat_col_manual_encoding:\n    housing_df[i] = housing_df[i].apply(manual_encoding)\n    print(housing_df[i].value_counts())","100043cc":"rem_cat_col = housing_df.select_dtypes(\"object\").columns\n\n# Creating dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(housing_df[rem_cat_col], drop_first=True)\n\n# Adding the results to the master dataframe\nhousing_df = pd.concat([housing_df, dummy1], axis=1)\n\n# dropping the original variables\nhousing_df.drop(rem_cat_col, axis=1, inplace=True)\nhousing_df.head()","207a9df5":"housing_df.shape","06a353f5":"housing_df['property_Age'] = housing_df['YrSold'] - housing_df['YearBuilt'] \nhousing_df['property_Age'].head()\n\n# dropping 'YearBuilt' and `YrSold` because of newly created variable `Property_Age`\nhousing_df.drop(['YearBuilt','YrSold'],axis=1,inplace=True)","b006c9be":"train_df = housing_df\ntrain_df.shape, housing_df.shape","ee5d3cac":"from sklearn.model_selection import train_test_split\ntrain_df, test_df = train_test_split(train_df, train_size=0.7, test_size=0.3, random_state=100)","68b5ed3d":"train_df.shape, test_df.shape","7786733d":"from sklearn.preprocessing import PowerTransformer\n\npt = PowerTransformer()\np_transformer = pt.fit_transform(train_df)\nPowerTransformer_df= pd.DataFrame(p_transformer, index=train_df.index, columns=train_df.columns)\nPowerTransformer_df.describe()","679f5f11":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nS_scaler = scaler.fit_transform(PowerTransformer_df)\nStandard_Scaler_df= pd.DataFrame(S_scaler,index=PowerTransformer_df.index, columns=PowerTransformer_df.columns)\n \n# Independent variables( features ) to X_train df\nX_train = Standard_Scaler_df.drop(['SalePrice'], axis=1)\n\n# Target variable to y_train df\ny_train = Standard_Scaler_df['SalePrice']\nX_train.shape , y_train.shape","779724d9":"lm = LinearRegression()\n\nfrom sklearn.feature_selection import RFE\nrfe = RFE(lm, 15)             # using RFE to get 15 features\nrfe = rfe.fit(X_train, y_train)\n\ncolumn = X_train.columns[rfe.support_]\ncolumn","534e8e2c":"X_train_rfe = X_train[column]\nX_train_rfe.shape","dd2e3ff6":"# Converting Test Data into Normal Distribution\n\np_transformer = pt.transform(test_df)\nPowerTransformer_df= pd.DataFrame(p_transformer, index=test_df.index, columns=test_df.columns)\nPowerTransformer_df\n\n# Performing Scaling\nS_scaler = scaler.transform(PowerTransformer_df)\nStandard_Scaler_df= pd.DataFrame(S_scaler,index=PowerTransformer_df.index, columns=PowerTransformer_df.columns)\n\n# Putting feature variable to X\nX_test = Standard_Scaler_df.drop(['SalePrice'], axis=1)\n\n# Target variable to y_test\ny_test = Standard_Scaler_df['SalePrice']\n\nX_test.shape, y_test.shape","52613956":"rr = Ridge(alpha=0.01)\nrr.fit(X_train_rfe, y_train) \n\npred_train_rr= rr.predict(X_train_rfe)\nprint(\"Train R-Squared in Ridge: \",r2_score(y_train, pred_train_rr))\n\nX_test_sm = X_test[column]\npred_test_rr= rr.predict(X_test_sm)\nprint(\"Test R-Squared in Ridge: \",r2_score(y_test, pred_test_rr))","d4c72faf":"# GridSearchCV\nfrom sklearn.model_selection import KFold\n\n# set up cross validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# specify range of hyperparameters\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500]}\n\n# GridSearchCV\n\n# Ridge model\nmodel = Ridge()\nmodel_cv = GridSearchCV(estimator = model, param_grid = params, \n                        scoring= 'r2', \n                        cv = folds, \n                        return_train_score=True, verbose = 1)            \nmodel_cv.fit(X_train_rfe, y_train)","173334d4":"print(model_cv.best_params_)","78d433b6":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","cb2fac29":"# plotting mean test and train scores with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper right')\n\nplt.show()","b1c315c6":"rr = Ridge(alpha=4)\nrr.fit(X_train_rfe, y_train) \n\npred_train_rr= rr.predict(X_train_rfe)\nprint(\"Alpha = 4: Train Score\", r2_score(y_train, pred_train_rr))\n\npred_test_rr= rr.predict(X_test_sm)\nprint(\"Alpha = 4: Test Score\",r2_score(y_test, pred_test_rr))","d9ce1544":"# Ridge model parameters\nrr_model_parameters = list(rr.coef_)\nrr_model_parameters.insert(0, rr.intercept_)\nrr_model_parameters = [round(x, 3) for x in rr_model_parameters]\n\ncols = X_train_rfe.columns\ncols = cols.insert(0, \"constant\")\n\nridge_df = list(zip(cols, rr_model_parameters))\nridge_df = pd.DataFrame(ridge_df)\nridge_df = ridge_df.rename(columns = {0:'Features',1:'Coefficients'})\nridge_df.sort_values(by=['Coefficients'], ascending=False)","fb12ddfb":"# lasso regression\nls = Lasso(alpha=0.001)\nls.fit(X_train_rfe, y_train)\n\n# predict\ny_train_pred = ls.predict(X_train_rfe)\nprint(\"Train R-Squared in Lasso :\",r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = ls.predict(X_test_sm)\nprint(\"Test R-Squared in Lasso : \" , r2_score(y_true=y_test, y_pred=y_test_pred))","9d874216":"# GridSearchCV\nfrom sklearn.model_selection import KFold\n\n# set up cross validation scheme\nfolds = KFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# specify range of hyperparameters\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0 ]}\n\n# GridSearchCV\n# lasso model\nmodel = Lasso()\nmodel_cv = GridSearchCV(estimator = model, param_grid = params, \n                        scoring= 'r2', \n                        cv = folds, \n                        return_train_score=True, verbose = 1)            \nmodel_cv.fit(X_train_rfe, y_train) ","3f31c66e":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","891c10b7":"print(model_cv.best_params_)","80ba44c0":"# plotting mean test and train scores with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n\n# plotting\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\nplt.xlabel('alpha')\nplt.ylabel('Negative Mean Absolute Error')\n\nplt.title(\"Negative Mean Absolute Error and alpha\")\nplt.legend(['train score', 'test score'], loc='upper right')\n\nplt.show()","549fe94f":"# lasso regression\nlasso = Lasso(alpha=0.0001)\nlasso.fit(X_train_rfe, y_train)\n\n# predict\ny_train_pred = lasso.predict(X_train_rfe)\nprint(\"Alpha = 0.0001: Train Score\",r2_score(y_true=y_train, y_pred=y_train_pred))\ny_test_pred = lasso.predict(X_test_sm)\nprint(\"Alpha = 0.0001: Test Score\",r2_score(y_true=y_test, y_pred=y_test_pred))","39474345":"lasso.coef_","07fdb844":"# lasso model parameters\nl_model_parameters = list(lasso.coef_)\nl_model_parameters.insert(0, lasso.intercept_)\nl_model_parameters = [round(x, 3) for x in l_model_parameters]\ncols = X_train_rfe.columns\ncols = cols.insert(0, \"constant\")\nlasso_df = list(zip(cols, l_model_parameters))\nlasso_df = pd.DataFrame(lasso_df)\nlasso_df = lasso_df.rename(columns = {0:'Parameters',1:'Coefficients'})\nlasso_df = lasso_df[lasso_df['Coefficients'] != 0]\nlasso_df.sort_values(by=['Coefficients'], ascending=False)","a29e4d8e":"ridge_df.sort_values(by=['Coefficients'], ascending=False).head(10)","8de850fe":"lasso_df.sort_values(by=['Coefficients'], ascending=False).head(10)","a9e6edb2":"- <b>`MasVnrArea`<\/b>","e7790b99":"## Splitting the data into train and test","248bf530":"- `GrLivArea`: Above grade (ground) living area square feet   \n- `GrLivArea` is linearly related to `SalePrice` to some extent with some outliers.    ","9e5d55f4":"### Checking whether `Target` variable is normally distributed or not","d9a0927b":"- Imputing `NaN` values with `median` value because of outlier presence.","4a0081bb":"- <b> model with optimal alpha","057fdfbe":"- Used `log1p` transformation to convert target variable to `normally distributed`","cd6e7ac3":"## 1. Reading and Understanding Data","71fd04d8":"- <b>Imputing `NaN` values in `MasVnrArea` with `0` as `NaN` values in `MasVnrArea` are `present only if MasVnrType` value is `NaN` as well<\/b>","fb01a8b1":"- <b>`MasVnrType`<\/b>","f4a5cb82":"- <b>Dropping Columns which are not useful for analysis much\n    - `Id` because we dont use id in analysis\n    - `MoSold` because there is no column which tells us month of house construction. So, `MoSold` doesn't bring much to analysis","c7b8b833":"|Column Name| Description| Column Name | Description |Column Name| Description| Column Name | Description |\n|--|--|--|--|--|--|--|--|\n| PoolQC | Pool Quality | MiscFeature | Miscellaneous Features | Alley | Type of Alley Access | Fence | Fence Quality | \n|FireplaceQu|Fireplace quality | LotFrontage| Linear Feet of street connected to house | GarageYrBlt | Year Garage built | GarageType | Garage type|\nGarageFinish|Interior finish of Garage|GarageQual|Garage Quality|GarageCond|Garage Condition | BsmtExposure |Basement Exposure|\n|BsmtFinType2|Rating of Basement finished area|BsmtFinType1|Rating of Basement finished area|BsmtCond|General Condition of Basement|BsmtQual|Height of Basement|\nMasVnrArea|Masonry Veener size|MasVnrType|Masonry Veener Type|Electrical|Electrical System|","50a1837e":"- <b> Neighborhood \/ SalePrice","00601558":"- <b> used log1p = log(n+1) to make target variable's distribution as normal distribution because target variable is skewed towards right. For Positive skewness, log tranformation usually works well.","ac793474":"- <b>No `NaN` values in dataframe<\/b>","279b463f":"- <b> model with optimal alpha","28d12cef":"- There is an increase in `SalePrice` generally with respect to time\n- We cant compare the `SalePrice` over the years because of inflation.","b0bb4923":"- <b> `TotalBsmtSF` \/ `SalePrice`","3bda43e2":"- Some `neighborhood` values are having positive impact on the `SalePrice`.","eb14621d":"- <b> Using `PowerTransformer` to make data more Gaussian-like ","7443b6ff":"## Checking Outliers\/ Outlier treatment","769d6482":"# House Price Advanced Regression\n## Problem Statement\n- A US-based housing company named Surprise Housing has decided to enter the Australian market. The company uses data analytics to purchase houses at a price below their actual values and flip them on at a higher price. For the same purpose, the company has collected a data set from the sale of houses in Australia. The data is provided in the CSV file below.\n\nThe company is looking at prospective properties to buy to enter the market. You are required to build a regression model using regularisation in order to predict the actual value of the prospective properties and decide whether to invest in them or not.\n\nThe company wants to know \n- Which variables are significant in predicting the price of a house\n- How well those variables describe the price of a house. Also, determine the optimal value of lambda for ridge and lasso regression.","26d3710e":"# Ridge model\n- Optimal alpha value for ridge is 4\n- Train data accuracy score for alpha = 4 is `89.7%`\n- Test data accuracy score for alpha = 4 is `88.6%`\n- Following are the top 10 predictor variables of ridge regression for `SalePrice`","20b085ce":"- Imputing `NaN` values with `None`","1b5a2367":"- Imputing `NaN` values with `SBrKr`\n\n|`Column Name`|Replacing NaN with|`Column Name`|Replacing NaN with|`Column Name`|Replacing NaN with|`Column Name`|Replacing NaN with|\n|--|--|--|--|--|--|--|--|\n|<b>PoolQC<\/b>|No Pool|<b>MiscFeature<\/b>|None|<b>Alley<\/b>|No alley access|<b>Fence<\/b>|No Fence|\n|<b>FireplaceQu<\/b>|No Fireplace|<b>GarageYrBlt<\/b>|0(No Garage)|<b>GarageType<\/b>|No Garage|<b>GarageFinish<\/b>|No Garage|\n|<b>GarageQual<\/b>|No Garage|<b>GarageCond<\/b>|No Garage|<b>BsmtExposure<\/b>|No Basement|<b>BsmtFinType2<\/b>|No Basement|\n|<b>BsmtFinType1<\/b>|No Basement|<b>BsmtCond<\/b>|No Basement|<b>BsmtQual<\/b>|No Basement|<b>MasVnrType<\/b>|None(Mode)|\n|<b>MasVnrArea<\/b>|0|<b>LotFrontage<\/b>|Median value|<b>Electrical<\/b>|SBrkr(Mode)|","5d5a3528":"- <b> `OverallQual` \/ `SalePrice`","010db08b":"# Ridge","b9207fdc":"# Exploratory Data Analysis [EDA]\n","ab116416":"## Creating Dummies\n- <b>`Manual Encoding`<\/b>","d5069c92":"# Lasso","071f79ce":"## RFE\n- <b> using RFE to select 15 features for model building","a8f9b685":"- <b>`No duplicate entries`<\/b>","61dcb771":"- `Electrical`","9512948b":"- <b> GrLivArea \/ SalePrice","c7ca2a3e":"- <b> Using `StandardScaler` to Standardize features by removing the mean and scaling to unit variance.","86283c7f":"- <b>Both Ridge and Lasso Regression models are good. Lasso is more preferrable because of it's low alpha value.","8e1a85d5":"- `TotalBsmtSF`: Total square feet of basement area\n- `TotalBsmtSF` is linearly related to SalePrice with a steep slope. It also have some outliers in the dataset    ","99a6b56a":"- `OverallQual`, `GrLivArea` and `TotalBsmtSF` are strongly correlated with `SalePrice`.\n\n- `TotalBsmtSF` and `1stFloor` are corelated. We can keep `TotalBsmtSF` just to say that our first guess was right \n\n- `TotRmsAbvGrd` and `GrLivArea` are also corelated.\n\n- `YearBuilt` is slightly correlated with `SalePrice`. ","f57b5161":"## Handling Skewed Data\n- Dropping the following columns that have more than 85% values associated to a specific value","fe87d515":"## Checking Multiple\/Duplicate Entries","fba76f3d":"- `Saleprice` is increasing with increase in `OverallQual`","42aafb11":"## Correlation Matrix","8d048f53":"- <b>YearBuilt \/ SalePrice","d6033cc0":"- <b>`LotFrontage`<\/b>","77440ee9":"# Lasso model\n- Optimal alpha value for lasso is 0.0001\n- Train data accuracy score for alpha = 0.0001 is `89.7%`\n- Test data accuracy score for alpha = 0.0001 is `88.6%`\n- Following are the top 10 predictor variables of lasso regression for `SalePrice`","b3fb030c":"## Checking for null values"}}