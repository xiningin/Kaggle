{"cell_type":{"b251fd2f":"code","e55e9361":"code","caccaa16":"code","6bf2d5b7":"code","a89081a6":"code","c2fa431c":"code","f80240c7":"code","47b6bdb3":"code","3a3e19a4":"code","90a42aa7":"code","9225f61c":"code","3b0e956a":"code","cbcf2cff":"code","10c3c178":"code","cef42e67":"code","0f3b27c2":"code","8c3d4de3":"code","0297bf07":"code","909dbb07":"code","768dcde7":"code","5b7dbe3f":"code","f72c9959":"code","7357a48e":"code","cfce4a21":"code","7ef3be33":"code","234c3c99":"code","76231b46":"code","42f56ba8":"code","30762e4a":"code","526e869d":"code","3ce87ec8":"code","3c9ee928":"code","e08240df":"markdown","499a6d24":"markdown","4b579e60":"markdown","bf423751":"markdown","b2b733e2":"markdown","cb187d2c":"markdown","a9fbf2b6":"markdown","68992af5":"markdown","f0887bf2":"markdown","5913cb73":"markdown","c82f2034":"markdown","7c09a1e5":"markdown","59b238d3":"markdown","2f1e543e":"markdown","137d0f37":"markdown","5f68a318":"markdown","7d63f63a":"markdown","57372da9":"markdown","bacf4970":"markdown","a4ab0e61":"markdown","0b8cbcf9":"markdown","b89711ff":"markdown","ff26b772":"markdown","a1731ac5":"markdown","499a59b2":"markdown","680750b5":"markdown"},"source":{"b251fd2f":"import pandas as pd # data processing, CSV file I\/O\nimport numpy as np # linear algebra\n%matplotlib inline\nimport matplotlib.pyplot as plt # charts\nimport seaborn as sns # charts\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n","e55e9361":"data_train = pd.read_csv(\"..\/input\/train.csv\")\ndata_test = pd.read_csv(\"..\/input\/test.csv\")\n\n# let's see first 15 entries\ndata_train.head(15)","caccaa16":"data_train.info()","6bf2d5b7":"data_train.describe()","a89081a6":"# save the 'Id' column and drop the from dataset since it's unnecessary for  the prediction process.\ndata_train_ID = data_train['Id']\ndata_test_ID = data_test['Id']\n\ndata_train.drop(\"Id\", axis = 1, inplace = True)\ndata_test.drop(\"Id\", axis = 1, inplace = True)\nprint(\"Done\")","c2fa431c":"correlation_matrix = data_train.corr()\nplt.subplots(figsize=(16,16))\nsns.heatmap(correlation_matrix, vmax=0.9)","f80240c7":"# create an pandas-type-Index with 10 variables with highest correlation with SalePrice \ncols = correlation_matrix.nlargest(10,'SalePrice')['SalePrice'].index\ncm = np.corrcoef(data_train[cols].values.T)\nht = sns.heatmap(cm,annot=True, annot_kws={'size': 10},fmt='.2f',\n                 yticklabels=cols.values, xticklabels=cols.values\n                ) #annot=show correlation value, annot_kws=size of annotation, fmt=round values to .2\nplt.show()","47b6bdb3":"#    Another way to plot:\n#data = pd.concat([data_train.SalePrice,data_train.GrLivArea],axis = 1)\n#data.plot.scatter(x='GrLivArea',y='SalePrice',ylim=(0,800000))\n\nfig, ax = plt.subplots()\nax.scatter(x = data_train['GrLivArea'], y = data_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","3a3e19a4":"data_train = data_train.drop(data_train[(data_train['GrLivArea'] > 4000) & \n                                        (data_train['SalePrice'] < 300000)].index)","90a42aa7":"data = pd.concat([data_train.SalePrice,data_train.GrLivArea],axis = 1)\ndata.plot.scatter(x='GrLivArea',y='SalePrice',ylim=(0,800000))\n","9225f61c":"fig, ax = plt.subplots()\nax.scatter(x = data_train['TotalBsmtSF'], y = data_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\nplt.show()","3b0e956a":"sns.distplot(data_train['SalePrice'] , fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(data_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(data_train['SalePrice'], plot=plt)\nplt.show()","cbcf2cff":"# log transformation usually works well with skewness\ndata_train['SalePrice'] = np.log1p(data_train['SalePrice'])","10c3c178":"sns.distplot(data_train['SalePrice'] , fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(data_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(data_train['SalePrice'], plot=plt)\nplt.show()","cef42e67":"print(data_train.shape)\nprint(data_test.shape)","0f3b27c2":"ntrain = data_train.shape[0]\nntest = data_test.shape[0]\nytrain = data_train.SalePrice.values\n\nall_data = pd.concat((data_train,data_test)).reset_index(drop = True)\nall_data.drop(['SalePrice'], axis = 1, inplace = True)\nprint(all_data.shape)","8c3d4de3":"total = all_data.isnull().sum().sort_values(ascending=False)\npercent = (all_data.isnull().sum() \/ all_data.isnull().count()).sort_values(ascending = False)\nmissing_data = pd.DataFrame(\n                            {'Total' : total[total > 0],\n                             'Missing Ratio': percent[percent > 0]\n                            }\n                            )\nmissing_data\n\n# other versions:\n#total = all_data.isnull().sum() \n#total = total.drop(total[total == 0].index).sort_values(ascending=False)\n#percent = (total \/ len(all_data)) * 100\n#missing_data = pd.concat([total,percent], axis = 1, keys=['Total','Missing ratio'])","0297bf07":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","909dbb07":"all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n# all_data.loc[all_data.LotFrontage.isna(),'LotFrontage'] = all_data[~all_data.LotFrontage.isna()].LotFrontage.mean()\n","768dcde7":"# Replacing missing data with None\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n\n# Replacing missing data with 0 (Since No garage = no cars in such garage.)\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n\n# For all these categorical basement-related features, NaN means that there is no basement.\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n\n# missing values are likely zero for having no basement\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n    \n# NA most likely means no masonry veneer for these houses. \n# We can fill 0 for the area and None for the type.\nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\n# 'RL' is by far the most common value. So we can fill in missing values with 'RL'\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\n# data description says NA means typical\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n\n# It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n\n# Only one NA value, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n\n# Both Exterior 1 & 2 have only one missing value -> substitute in the most common string\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n\n#Fill in again with most frequent which is \"WD\"\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\n# Na most likely means No building class. We can replace missing values with None\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\n","5b7dbe3f":"# all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . \n# Since the house with 'NoSewa' is in the training set, \n# this feature won't help in predictive modelling. We can then safely remove it.\nall_data = all_data.drop(['Utilities'], axis=1)\n\n","f72c9959":"#Check remaining missing values if any \ntotal = all_data.isnull().sum()\ntotal[total > 0]","7357a48e":"#The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n#Overall condition\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","cfce4a21":"# check which columns are categorical\nall_data.columns[all_data.dtypes == \"object\"]","7ef3be33":"# identify and create distinct values in categorical columns\nfrom sklearn.preprocessing import LabelEncoder\nfor col in all_data.columns[all_data.dtypes == \"object\"]:\n    all_data[col] = all_data[col].factorize()[0]\n","234c3c99":"# after features engineering split back the dataset into train and test parts.\ndata_train = all_data.iloc[:ntrain]\ndata_test = all_data.iloc[ntrain:]","76231b46":"# split train data into two groups: training and validation set\n# that we will use for tuning the model\nfrom sklearn.model_selection import train_test_split\ntrain_x, val_x, train_y, val_y = train_test_split(data_train, ytrain, test_size=0.3, random_state=42)","42f56ba8":"import xgboost as xgb\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_log_error\nfrom bayes_opt import BayesianOptimization\n\n# we will use hyperoptimastion to measure best values of parameters of XGBRegression model\n# however, we train the model with n_jobs = 4, learning rate lowest possible 0.01, and seed=42\ndtrain = xgb.DMatrix(data_train, label=ytrain)\n\ndef xgb_evaluate(max_depth, n_estimators,min_child_weight,subsample,\n                 colsample_bytree,colsample_bylevel,reg_lambda):\n    params = {\n              'max_depth': int(max_depth),\n              'n_estimators': n_estimators,\n              'min_child_weight': min_child_weight,\n              'n_jobs': 4,\n              'subsample': subsample,\n              'colsample_bytree': colsample_bytree,\n              'learning_rate': 0.01,\n              'reg_lambda': reg_lambda,\n              'seed': 42,\n              'silent':True\n    }\n\n    cv_result = xgb.cv(params, dtrain, num_boost_round = 500, nfold = 5, early_stopping_rounds=10)    \n    \n    # Bayesian optimization only knows how to maximize, not minimize, so return the negative RMSE\n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\n\nxgb_bo = BayesianOptimization(xgb_evaluate, {'max_depth': (3, 5), \n                                             'n_estimators': (500,3500),\n                                             'min_child_weight': (1.5,7.5),\n                                             'subsample':(0.3,1.0),\n                                             'colsample_bytree': (0.01,1.0),\n                                             'colsample_bylevel': (0.01,0.2),\n                                             'reg_lambda': (0.1,10.0)\n                                             })\n\nxgb_bo.maximize(init_points = 3, n_iter = 50, acq='ei')","30762e4a":"# Bayesian Optimalisation gives us a hint about best paramteres\n# after 53 loops best parameters we get are:\nmodel = xgb.XGBRegressor(colsample_bylevel=0.25,colsample_bytree=0.6,max_depth=4,min_child_weight=4.8,n_estimators=3000,reg_lambda=0.1,subsample=0.8, learning_rate=0.01, n_jobs=4,seed=42)\n\n# check the accuracy of cross valuation method\nmodel.fit(train_x, train_y, early_stopping_rounds=5, eval_set=[(val_x,val_y)], verbose=False)\nprint(\"Valuated accuracy:\", np.sqrt(mean_squared_log_error(model.predict(val_x), val_y)))\nprint(\"Trained accuracy:\", np.sqrt(mean_squared_log_error(model.predict(train_x), train_y)))\n\n# check the accuracy of the full train dataset\naccuracy = cross_val_score(model, data_train, ytrain, cv=5, scoring=\"neg_mean_squared_log_error\")\nprint(\"Accuracy of train data:\", np.sqrt(-accuracy.mean()))\n","526e869d":"# training the model of the full dataset\nmodel.fit(data_train, ytrain)","3ce87ec8":"y_pred = model.predict(data_test)\nY_pred = np.expm1(y_pred)\nY_pred","3c9ee928":"submission = pd.DataFrame({\n    \"Id\": data_test_ID, \n    \"SalePrice\": Y_pred \n})\nsubmission.to_csv('submission.csv', index=False)","e08240df":"When analysing the dataset and learning about the variables (do not forget about reading the attached description of the variables as well) it is helpful to take into an account such factors as: type of the variables (i.e. numerical, categorical,...), segments of variables (e.g. TotalBsmtSF and BsmtFinSF1 reports similiar properties of the house) and our expectation about the influence of the variable in SalePrice. A heatmap of correlation is extremely usefull to distinguish variables that have most impact on target as well on each other.","499a6d24":"# 4. Prediction and submition","4b579e60":"More or less all the observations of TotalBsmSF are \"in-liners\" (not outliners).","bf423751":"# 5. Refrences\n* [Comprehensive Data Explorartion with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) by **by Pedro Marcellino**: Great basic data analysis\n* [Stacked Regression to predict House Prices](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) by **by Sergine**: Best structured kernel on modelling I have found so far (for basic learning of machine learning)\n* [House Price Prediction EDA\/XGBoost - kernel prepared by tooezy](https:\/\/www.kaggle.com\/tooezy\/house-price-prediction-eda-xgboost)","b2b733e2":"## 2.3 \u0130mpute null and missing values","cb187d2c":"## 2.4 Label Encoding","a9fbf2b6":"The map shows that the strong relationships with SalePrice have among others: 'OverallQual', 'GrLivArea', 'GarageCars', 'GarageArea', 'TotalBsmtSF', '1stFlrSF'.... There are a couple of them, so let's see 10 first variables with highest correlation with the target","68992af5":"Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.","f0887bf2":"**Join train and test datasets in the same dataframe**","5913cb73":"* **1. Loading the data**\n    * 1.1 Load data\n    * 1.2 Study the dataset\n* **2. Data Processing**\n    * 2.1 Outliers\n    * 2.2 Target variable\n    * 2.3 \u0130mpute null and missing values\n    * 2.4 Label Encoding\n    * 2.5 Split training and valdiation set\n* **3. XGBoost**\n    * 3.1 Define the model\n    * 3.2 Parameter tuning\n* **4 Prediction and submition**\n* **5. References**","c82f2034":"Now, it is more readable.\n\nAre there any variables which information could be already described by another one? 'GarageCars' and 'GarageArea' are the example of a pair of variables that give the same information. We would need only one of them and we would take the one with higher value. The same case is with the pair 'TotalBsmtSF' and '1stFlrSF' as well as with 'TotRmsAbvGrd' and 'GrLivArea'.\n\n'FullBath' and 'YearBuilt' in this dataset are slightly correlated with 'SalePrice'. The latter should also undergo time-series analysis in order to decide whether it is significant to 'SalePrice'. 'FullBath', on the other hand, compared to living area, does not seem to have a true impact in real life on the price of the house.","7c09a1e5":"SalePrice variable is not normal but right skewed -> does not follow the diagonal line and has positive skewness.\nIf we want to use linear models efficiently we have to transform the data so it will be normally distributed.","59b238d3":"# DESCRIPTION\nIn this notebook we try to predict the sale price of residential homes in Ames, Iowa. The dataset comes from a Housing Prices Competition held at Kaggle (https:\/\/www.kaggle.com\/c\/home-data-for-ml-course ). We focus on implementing the most popular gradient boosted decision trees algorithm - XGBoost. The final score of predicition on *test data* usually falls into TOP 4% on public leaderboard (https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/leaderboard)","2f1e543e":"## 2.1 Outliers","137d0f37":"**SalePrice** is the variable we need to predict. So let's do some analysis on this variable first.","5f68a318":"#  2. Data Processing","7d63f63a":"Check the outliers on chosen variables:\n* GrLivArea\n* TotalBsmtSF","57372da9":"**Transforming some numerical variables that are really categorical**","bacf4970":"## 2.5 Split training and valdiation set","a4ab0e61":"## 1.2 Study the dataset","0b8cbcf9":"Data description says NaN means: \"No Pool\" in case of 'PoolQC', no misc feature for \"MiscFeature\", no alley, no fence and no fireplace etc..","b89711ff":"We can see that two houses have large GrLivArea values (right side of the plot) while the prices are relatively low. We can define them as outliers and delete them. Outliers removal is note always safe.  We decided to delete these two as they are very huge and really bad.","ff26b772":"## 2.2 Target variable","a1731ac5":"# 1. Loading the data\n## 1.1 Load data","499a59b2":"## 3.1 Training data","680750b5":"# 3. XGBoost\n## 3.1 Parameter Tuning"}}