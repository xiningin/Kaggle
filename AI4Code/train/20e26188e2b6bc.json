{"cell_type":{"474a5782":"code","c29520d9":"code","508f2b59":"code","960e4838":"code","5df72945":"code","a4a3f677":"code","b55c0375":"markdown","00bb43ad":"markdown","fb3e03fc":"markdown","afbbc585":"markdown","bb0c7413":"markdown","c2954246":"markdown","8fed2030":"markdown","15ac926b":"markdown","9cfa74f1":"markdown","451baed4":"markdown"},"source":{"474a5782":"# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.6 cannot be found (uncomment if needed). \n# !curl -X PURGE https:\/\/pypi.org\/simple\/kaggle-environments\n\n# ConnectX environment was defined in v0.1.6\n!pip install 'kaggle-environments>=0.1.6'","c29520d9":"from kaggle_environments import evaluate, make, utils\n\nenv = make(\"connectx\", debug=True)","508f2b59":"import random\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nenv.configuration.rows = 3\nenv.configuration.columns = 4\nenv.configuration.inarow = 3\n#env.specification","960e4838":"# rows is state\n# columns are actions\nq_table = np.zeros([3**(env.configuration.rows * env.configuration.columns),env.configuration.columns])","5df72945":"# Hyperparameters\nalpha = 0.9    # Learning rate\ngamma = 0.1    # Discount factor (0 = only this step matters)\n# epsilon is determined each step\n\nall_epochs = []\nall_penalties = []\n\nNB_STEPS = 10000\nNB_STEPS_RANDOM = 1000\nEPSILON_0 = 1\nEPSILON_END = 0.1\nNB_STEPS_EPSILON = 1000\ntrainer = env.train([None, \"random\"])\nepisodes = 0\nprogression = []\nstep = 0\nrandom.seed(10)\nfor _ in range(NB_STEPS):\n    observation = trainer.reset()\n  \n    # Init Vars\n    total_reward = 0\n    done = False\n    episode_step = 0  \n    while not done:\n        # This part computes the epsilon that will be used later on\n        if step < NB_STEPS_RANDOM:\n            epsilon = 1\n        elif step < NB_STEPS_RANDOM + NB_STEPS_EPSILON:\n            epsilon = EPSILON_0 - (EPSILON_0-EPSILON_END) * ((step-NB_STEPS_RANDOM) \/ NB_STEPS_EPSILON)\n        else:\n            epsilon = EPSILON_END\n        \n        # Sometimes, you cannot play a move because the column is full, so you take actions among the possible columns\n        possible_action = observation.board[0:env.configuration.columns]\n        possible_action = [i for i in range(len(possible_action)) if possible_action[i] == 0]\n        # This next line converts the state to an integer using ternary converter.\n        # For example, if the state is 0,1,2; the row will be 1*3 + 2 = 5. I will look the fifth row and take the action that yields the max value\n        obs_value = int(\"\".join(str(i) for i in observation.board),3)\n        if random.uniform(0, 1) < epsilon:\n            # Check the action space and choose a random action\n            action = random.choice(possible_action)\n        else:\n            # Check the learned values\n            action = possible_action[int(np.argmax(q_table[obs_value][possible_action],axis=0))]\n        \n        next_observation, reward, done, _ = trainer.step(action)\n        # 1 for a win, 0 for a draw and -1 for a lose.\n        if done :\n            if reward == None:\n                reward = -1\n        if reward == 0.5:\n            reward = 0\n        elif reward == 0:\n            reward = -1        \n          \n        # Old Q-table value\n        old_value = q_table[obs_value, action]\n        next_max = np.max(q_table[obs_value])\n\n        # Update the new value\n        # Bellman equation !\n        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n        q_table[obs_value, action] = new_value\n\n        observation = next_observation\n        \n        total_reward += reward\n        step += 1               \n        episode_step += 1\n\n        if done:\n            progression.append(total_reward)\n            if episodes % 100 == 0:\n                print(episodes, epsilon, sum(progression[-101:-1]),sum(sum(q_table)))\n            episodes += 1   \n\n# Lets look the performance of the agent for 100 games interval\nchunks = 100\nlist_progression = [progression[i:i + chunks] for i in range(0, len(progression), chunks)]\nlist_progression = [sum(i)\/len(i) for i in list_progression]\nplt.plot(list_progression)\nplt.show()","a4a3f677":"for _ in range(1000):\n    observation = trainer.reset()\n  \n    # Init Vars\n    total_reward = 0\n    done = False\n    episode_step = 0  \n    while not done:\n        \n        possible_action = observation.board[0:env.configuration.columns]\n        possible_action = [i for i in range(len(possible_action)) if possible_action[i] == 0]\n\n        # Check the learned values\n        obs_value = int(\"\".join(str(i) for i in observation.board),3)\n        action = possible_action[int(np.argmax(q_table[obs_value][possible_action],axis=0))]\n        \n        next_observation, reward, done, _ = trainer.step(action)\n        if done :\n            if reward == None:\n                reward = -1\n        \n        if reward == 0.5:\n            reward = 0\n        elif reward == 0:\n            reward = -1        \n\n        observation = next_observation\n        \n        total_reward += reward\n        step += 1               \n        episode_step += 1\n\n        if done:\n            progression.append(total_reward)\n            if episodes % 10 == 0:\n                print(episodes, epsilon, sum(progression[-11:-1]),sum(sum(q_table)))\n            episodes += 1","b55c0375":"# Create ConnectX Environment","00bb43ad":"Since AlphaStar made the news for beating an elite player in StarCraft II, reinforcement learning has piqued my interest. I have read alot of articles and tutorials about the subject and it seems to me that they go quickly to deep q-learning without explaining really what q-learning is. In my experience, the articles I read will talk about the cartpole environnement without code and then jump straight up to deep learning with keras-rl.\n\nWhen trying to implement simple q-learning myself, I was pretty lost. In a real example, what really is a Q-table?\n\nIn this notebook, I will try my best to make q-learning tangible.\n\nDisclaimer 1 : Part of this code comes from an example of Siraj Raval from some time ago.\n\nDisclaimer 2 : I am no expert in Python nor reinforcement learning so if you see ways to improve myself, do not hesitate to comment my notebook!","fb3e03fc":"# Evaluate your Agent\nThis next chunk of code looks at the performance without having to take a random action once in a while.","afbbc585":"In q-learning, there is 3 hyperparameters : alpha, gamma and epsilon. I find that a decaying epsilon works best, so I will start it at 1 and decay it to 0.1. This means that my agent will take completely random actions in the beginning and at some point only take a random action 1 time out of 10.","bb0c7413":"You can see with the second element in the list that the total reward for 10 games span is 10 most of the times, which means 10 win in 10 games. It is against a random agent, but still. It means the agent learned.","c2954246":"# Fill-up your Q-Table\n\nSo, what is q-learning anyway? It is a technique that, given a state of the environnement, either chooses the best actions amongst the possible actions or take a random action for exploration.\n\nFor this example, I make a q-learning model with a toy example of ConnectX. There will only by 3 rows, 4 columns and the winner only has to put 3 in a row to win.","8fed2030":"I hoped you have learned something with this notebook. It certainly helped me on my journey of understanding reinforcement learning.\n\nWith a simple toy example, we have a q-table with 2,000,000 elements. With the real game, we would have 3**(6*7) which gives a huuge number. This is why we need deep q-learning, because q-learning does not scale well.","15ac926b":"# Install kaggle-environments","9cfa74f1":"The performance quickly peeks at 85 reward in a 100 games span. When the agent lose, it's -1, so this is probably around 93% win.","451baed4":"For Q-learning, you need a Q-table. The Q-table is a matrix that maps every state possible to every actions possible.\nThere is 4 actions, so the matrix will have 4 columns. For the rows, it is a little bit more complicated. the board that I designed has 3 rows and 4 columns so 12 squares. Every square can either have 0, 1 or 2 in it. 0 for nothing, 1 for a token of player 1 and 3 for a token of player 2. So the number of rows is the number of possible combinations : 3**(3*4) = 531441.\n\nIf you really wanted to optimize this, you would see that the board is the same when you flip it, so it reduces by half the possibilities for an even number of columns. Also, most combinations do not exists. You can't have 1,0,1 for column. But that will not be covered here.\n\nFor a toy example, the matrix is 500,000 * 4 = 2,000,000 elements."}}