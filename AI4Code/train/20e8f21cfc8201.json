{"cell_type":{"b66ddf5b":"code","8904955b":"code","eab2f174":"code","4d0c163e":"code","212d63c4":"code","c6046a5f":"code","42441e48":"code","59587ac4":"code","b3c8f79d":"code","496f8312":"markdown","ec479c0b":"markdown","391eae84":"markdown","e2576c8a":"markdown","84ba26c5":"markdown","ed227e81":"markdown","9d4e1668":"markdown","84fcb793":"markdown"},"source":{"b66ddf5b":"import numpy as np\nimport pandas as pd\ndataset_CA=pd.read_csv(\"..\/input\/youtube-new\/CAvideos.csv\")\ndataset_DE=pd.read_csv(\"..\/input\/youtube-new\/DEvideos.csv\")\ndataset_FR=pd.read_csv(\"..\/input\/youtube-new\/FRvideos.csv\")\ndataset_GB=pd.read_csv(\"..\/input\/youtube-new\/GBvideos.csv\")\ndataset_IN=pd.read_csv(\"..\/input\/youtube-new\/INvideos.csv\")\ndataset_JP=pd.read_csv(\"..\/input\/youtube-new\/JPvideos.csv\",encoding='latin1')\ndataset_KR=pd.read_csv(\"..\/input\/youtube-new\/KRvideos.csv\",encoding='latin1')\ndataset_MX=pd.read_csv(\"..\/input\/youtube-new\/MXvideos.csv\",encoding='latin1')\ndataset_RU=pd.read_csv(\"..\/input\/youtube-new\/RUvideos.csv\",encoding='latin1')\ndataset_US=pd.read_csv(\"..\/input\/youtube-new\/USvideos.csv\")\ndataset_CA.isnull().sum()\ndataset_DE.isnull().sum()\ndataset_FR.isnull().sum()\ndataset_GB.isnull().sum()\ndataset_IN.isnull().sum()\ndataset_JP.isnull().sum()\ndataset_KR.isnull().sum()\ndataset_MX.isnull().sum()\ndataset_RU.isnull().sum()\ndataset_US.isnull().sum()","8904955b":"dataset_CA['region']='Canada'\ndataset_DE['region']='Germany'\ndataset_FR['region']='France'\ndataset_GB['region']='Great Britan'\ndataset_IN['region']='India'\ndataset_JP['region']='Japan'\ndataset_KR['region']='South Korea'\ndataset_MX['region']='Mexico'\ndataset_RU['region']='Russia'\ndataset_US['region']='United States'\ndf=pd.DataFrame(pd.concat([dataset_CA,dataset_DE,dataset_FR,dataset_GB,dataset_IN,dataset_JP,dataset_KR,dataset_MX,dataset_RU,dataset_US], ignore_index=True))\ndf = df.sample(frac=1).reset_index(drop=True)\ndf","eab2f174":"df.shape\n","4d0c163e":"df['region'].value_counts(normalize=True) * 100","212d63c4":"df=df.dropna()\ndf.shape","c6046a5f":"print(\"Data types and their frequency\\n{}\".format(df.dtypes.value_counts()))","42441e48":"from sklearn.preprocessing import OneHotEncoder,LabelEncoder\nlabelencoder = LabelEncoder()\n# Assigning numerical values and storing in another column\ndf['region_cat'] = labelencoder.fit_transform(df['region'])\nenc = OneHotEncoder(handle_unknown='ignore')\n# passing region-cat column (label encoded values of region)\n\no=pd.DataFrame(enc.fit_transform(df[['region_cat']]).toarray())\n#reducing the number of dummy values\ndf['category_id'].replace({18: 1, 30: 1,31: 1,32: 1,33: 1,34: 1,35: 1,36: 1,37: 1,38: 1,39: 1,40: 1,41: 1,42: 1,43: 1,44: 1,21: 22}, inplace=True)\n#adding a new column that is title length\ndf['title_length'] = df['title'].str.len()\n#here we are trying to delete the colummns(features) that we are not in need of in our model \n\"\"\"\ndel df['likes']\ndel df['dislikes']\ndel df['comment_count']\ndel df['ratings_disabled']\ndel df['video_error_or_removed']\ndel df['description']\ndel df['publish_time']\ndel df['trending_date']\n\"\"\"\n\ndf\ndef name_col(row):\n    if row['category_id'] == 2:\n        val = \"Autos & Vehicles\"\n    elif row['category_id'] == 1|row['category_id'] == 18|row['category_id'] == 30|row['category_id'] == 31|row['category_id'] == 32|row['category_id'] == 33|row['category_id'] == 34|row['category_id'] == 35|row['category_id'] == 36|row['category_id'] == 37|row['category_id'] == 38|row['category_id'] == 39|row['category_id'] == 40|row['category_id'] == 41|row['category_id'] == 42|row['category_id'] == 43|row['category_id'] == 44:\n        val = \"Film & Animation\"\n    elif row['category_id'] == 10:\n        val=\"Music\"\n    elif row['category_id']==15:\n        val=\"Pets & Animals\"\n    elif row['category_id']==17:\n        val=\"Sports\"\n    elif row['category_id']==19:\n        val=\"Travel & Events\"\n    elif row['category_id']==20:\n        val=\"Gaming\"\n    elif row['category_id']==22|row['category_id']==21:\n        val=\"People & Blogs\"\n    elif row['category_id']==23:\n        val=\"Comedy\"\n    elif row['category_id']==24:\n        val=\"Entertainment\"\n    elif row['category_id']==25:\n        val=\"News & Politics\"\n    elif row['category_id']==26:\n        val=\"Howto & Style\"\n    elif row['category_id']==27:\n        val=\"Education\"\n    elif row['category_id']==28:\n        val=\"Science & Technology\"\n    elif row['category_id']==29:\n        val=\"Nonprofits & Activism\"\n    return val\no=df\ndf['category_names'] = df.apply(name_col, axis=1)\ndf\n\n\n","59587ac4":"\n#splitting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\n#splitting the dataset into source variables (independent variables) and the target variable (dependent variable)\nX=df.loc[:, df.columns != 'views']\ny=df['views']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)","b3c8f79d":"# import the required library  \nimport numpy as np  \nimport pandas as pd  \nimport matplotlib.pyplot as plt  \n% matplotlib inline \ndf.boxplot(by ='day', column =['total_bill'], grid = False)\n","496f8312":"*Exploring the dataset*\n\n**We have 375942 samples with 17 features.Approximately 5% of the entries have missing descriptions**","ec479c0b":"*Handle Missing Values*\n\n**Here I dropped all observations with null values since they make up 5% of the entire dataset -> insignificant **","391eae84":"*Cleaning up data*\n\n**Here, i've created a column 'region' for each of the datasets to specify which region they come from. I then proceeded to combine all datasets into one dataframe.Furthermore, i shuffled DataFrame rows inorder to avoid any element of bias\/patterns before training ML model. **","e2576c8a":"*Investigate Categorical Columns*\n\n**We have 9 object columns that have text**","84ba26c5":"*Feature Selection*\n\n**We select those features which contribute most to our prediction variable in which we are interested in.\nUsing irrelevant features in our data can decrease the models prediction accuracy on unseen data, increase variance (model highly likely to overfit),and increase the algorithms complexity.**","ed227e81":"*Distribution of observations per region*\n","9d4e1668":"*Check for null values*\n\n**We can clearly see that there are missing values from description of videos coming from different regions** \n\n**(Canada, Germany, France, Great Britan, India, Japan, South Korea, Mexico, Russia, United States)**","84fcb793":"*Boxplots*"}}