{"cell_type":{"1ed590c1":"code","94ae4254":"code","20199352":"code","6d08c7d4":"code","057a02d5":"code","bc96509c":"code","d4ec4271":"code","9e15d949":"code","7f04d71b":"code","1aa3b080":"code","58679df7":"code","85e2422d":"code","cd514177":"code","1827e523":"code","71c33905":"code","ce296934":"code","b6194370":"code","5a72bb2f":"code","64ca8a05":"code","d7b0b766":"code","9ca0fa94":"code","7bf010a7":"code","06e80276":"code","34ebcafb":"code","141a9a17":"code","333be90c":"code","e896fb5b":"code","888d3243":"code","efdd922c":"code","0aadf7ef":"code","ff8ba1d0":"code","7cf5a0c0":"code","c74bd043":"code","d5cb70c6":"code","20cf532e":"code","8b279c4d":"code","15bc1307":"code","d35b7bc2":"code","37b92c81":"code","a85f9618":"code","b40b65ae":"code","a8369e42":"code","84d1456e":"code","025af1fd":"code","a2acaaf3":"code","7392dec4":"code","e9d1c1b5":"code","53237c0d":"code","4867f1d2":"code","92e6fad1":"code","105f3835":"code","d2b32344":"code","d4a6eebd":"code","536fd7d1":"markdown","fbab05db":"markdown","1c154a0d":"markdown","2c8bafb4":"markdown","3be0053d":"markdown","3f591124":"markdown","4139e2ee":"markdown","c5c1ff2b":"markdown"},"source":{"1ed590c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","94ae4254":"import pandas as pd\nimport numpy as np\nimport xgboost \n\n#Visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#For train & split\nfrom sklearn.model_selection import train_test_split,StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, LabelEncoder, OneHotEncoder\n\n#Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom catboost import CatBoostClassifier\n\n\n#Evaluation metrics \nfrom sklearn.metrics import accuracy_score,mean_absolute_error,confusion_matrix,classification_report\n\nfrom IPython.display import display\nimport warnings\nwarnings.filterwarnings('ignore')","20199352":"#Reading train and test data sets\n\ntrain = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntrain.head(3)","6d08c7d4":"#Making a copy of train and test datasets\n\ntrain_copy = train.copy()\ntest_copy = test.copy()","057a02d5":"# Printing dimentions of train and test datasets \n\nprint(\"Shape of train data:\", train.shape)\nprint(\"Shape of test data:\", test.shape)","bc96509c":"# Checking outliers in training set\n\nplt.figure(figsize = (15,10))\nplt.rcParams['font.size'] = 15\n\nplt.subplot(2,2,1)\nsns.scatterplot('PassengerId', 'Fare', hue = 'Survived', data = train)\n\nplt.subplot(2,2,2)\nsns.scatterplot('PassengerId', 'Age', hue = 'Survived', data = train)\n\nplt.subplot(2,2,3)\nsns.scatterplot('PassengerId', 'Parch', hue = 'Survived', data = train)\n\nplt.subplot(2,2,4)\nsns.scatterplot('PassengerId', 'SibSp', hue = 'Survived', data = train)","d4ec4271":"# Verifying records which are visually outliers\n\nprint(\"Records with Fare>500: \")\ndisplay(train[train['Fare']>500])\nprint()\nprint(\"Records with Age>75: \")\ndisplay(train[train['Age']>75])\nprint()\nprint(\"Records with Parch>6: \")\ndisplay(train[train['Parch'] == 6])","9e15d949":"# Dropping index no: 258, 679, 737 and 630\n\ntrain = train.drop([258, 679, 737, 630], axis = 0)","7f04d71b":"#Creating a function with name 'analysis' for extracting data type, unique and null count\n\ndef analysis(data):\n    return pd.DataFrame({\"Data Type\":data.dtypes, \"Unique Count\":data.apply(lambda x: x.nunique(),axis=0), \n                         \"Null Count\": data.isnull().sum() })","1aa3b080":"analysis(train)","58679df7":"# Fillings missing values \n\ntrain['Age'] = train['Age'].fillna(train['Age'].median())\ntrain['Embarked'] = train['Embarked'].fillna(train['Embarked'].mode()[0])","85e2422d":"# Filling missing values\n\ntest['Age'] = test['Age'].fillna(train['Age'].median())\ntest['Fare'] = test['Fare'].fillna(test['Fare'].median())","cd514177":"# Seperating target variable \n\nTarget = pd.DataFrame(train['Survived'])\ntrain = train.drop(['Survived'], axis = 1)\n\nprint(\"Shape of train data:\", train.shape)\nprint(\"Shape of test data:\", test.shape)","1827e523":"# Concat both train and test sets\n\ndata = pd.concat([train, test])\ndisplay(data.head(3))\nprint()\nprint(\"Shape of the data: \", data.shape)","71c33905":"# Creating new attribute 'Family_Size' using 'Parch' and 'SibSp'\n\n#data['Family_Size'] = data['Parch'] + data['SibSp'] \n#data.head(2)","ce296934":"# Extracting 'Family_Name' from Name\n\ndata['Family_Name'] = data['Name'].map(lambda x:x.split(\",\")[0])\ndata.head(3)","b6194370":"# Extracting 'Prefix' from 'Name'\n\ndata['Name'] = data['Name'].map(lambda x:x.split(\",\")[1].strip())\ndata = data.rename({'Name': 'Prefix'}, axis = 1)\ndata['Prefix'] = data['Prefix'].map(lambda x:x.split(\".\")[0].strip())\ndata.head(3)","5a72bb2f":"# Removing additional spaces if exists \n\ndata['Family_Name'] = data['Family_Name'].str.strip()\ndata['Prefix'] = data['Prefix'].str.strip()","64ca8a05":"# Creating new column 'Family', represents the if a person has same family member or not\n\ndata['Family'] = data.groupby('Family_Name')['Family_Name'].transform('count')\ndata['Family'] = np.where(data['Family']==1, 0, 1)\ndata.head()","d7b0b766":"# Filling missing 'Cabin' values with NA and extracting first letter from the Cabin\n\ndata['Cabin'] = data['Cabin'].fillna('NA')\ndata['Cabin'] = data['Cabin'].astype(str).str[0]\ndata.head()","9ca0fa94":"data.groupby(['Pclass', 'Cabin']).size()","7bf010a7":"# As per above results, filling Cabin values with sutable replacement \n\ndata['Cabin'] = np.where((data['Cabin']=='N') & (data['Pclass'] == 1), 'C', data['Cabin'])\ndata['Cabin'] = np.where((data['Cabin']=='N') & (data['Pclass'] == 2), 'F', data['Cabin'])\ndata['Cabin'] = np.where((data['Cabin']=='N') & (data['Pclass'] == 3), 'F', data['Cabin'])","06e80276":"data.head(3)","34ebcafb":"# Creating attributes 'T1' and 'T2' from 'Ticket'.\n# T1: Main classification of ticket\n# T1: Sub classificatoin, like numbers in cabin\n\nTkt = data[\"Ticket\"].str.split(\" \", n = 1, expand = True)\ndata['T1'] = Tkt[0]\ndata['T2'] = Tkt[1]\ndata.head(2)","141a9a17":"# Avioding possible errors in 'T1' and 'T2'\n\ndata['T1'] = np.where(data['Ticket'] == data['T1'], 'NA', data['T1'])\ndata['T2'] = data['T2'].fillna(data['Ticket'])\ndata['T1'] = data['T1'].str.replace('\/', '')\ndata['T1'] = data['T1'].str.replace('.', '')\ndata['T1'] = data['T1'].str.strip()\ndata.head(2)","333be90c":"# Drop unwanted columns\n\ndata = data.drop(['PassengerId', 'Ticket', 'Family_Name'], axis = 1)\ndata.head(3)","e896fb5b":"data_num_cols = data._get_numeric_data().columns \ndata_cat_cols = data.columns.difference(data_num_cols)\nprint(\"Numeric Columns: \", data_num_cols)\nprint()\nprint(\"Category Columns: \", data_cat_cols)","888d3243":"#Separating both numeric and categorical data from set\n\ndata_num_data = data.loc[:, data_num_cols]\ndata_cat_data = data.loc[:, data_cat_cols]\n\nprint(\"Shape of num data:\", data_num_data.shape)\nprint(\"Shape of cat data:\", data_cat_data.shape)","efdd922c":"# Scaling numeric features \n\ns_scaler = StandardScaler()\ndata_num_data_s = s_scaler.fit_transform(data_num_data)\n\ndata_num_data_s = pd.DataFrame(data_num_data_s, columns = data_num_cols )","0aadf7ef":"# Getting dummies for categorical features \n\ndata_cat_data = pd.get_dummies(data_cat_data )","ff8ba1d0":"data_num_data_s.reset_index(drop=True, inplace=True)\ndata_cat_data.reset_index(drop=True, inplace=True)\n\ndata_new = pd.concat([data_num_data_s, data_cat_data], axis = 1)","7cf5a0c0":"train_new = data_new.loc[:886,]\ntest_new = data_new.loc[887:,]\n\nprint(\"Shape of train data:\", train_new.shape)\nprint(\"Shape of test data:\", test_new.shape)","c74bd043":"# Splitting train and test data\n\ntrainx,valx,trainy,valy = train_test_split(train_new,Target,test_size=0.25,random_state=1234)\n\nprint(trainx.shape)\nprint(valx.shape)","d5cb70c6":"cat = CatBoostClassifier()\n\ncat.fit(trainx, trainy)","20cf532e":"#Predecting values on train and validation sets\n\npred_train_cat = cat.predict(trainx)\npred_val_cat = cat.predict(valx)","8b279c4d":"#Confusion Matrix\n\nresults = confusion_matrix(valy, pred_val_cat) \nprint('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(valy, pred_val_cat))\nprint ('Report : ')\nprint (classification_report(valy, pred_val_cat))","15bc1307":"xgb = xgboost.XGBClassifier(base_score=0.5, booster='gbtree', colasample_bytree=0.2,\n             colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n             gamma=0.2, learning_rate=0.05, max_delta_step=0, max_depth=15, \n             min_child_eight=2, missing=None, n_estimators=100, n_jobs=1, \n             nthread=None, objective='binary:logistic', random_state=0, \n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n             seed=None, silent=None, subsample=1, verbosity=1)\n\nxgb.fit(trainx, trainy)","d35b7bc2":"#Predecting values on train and validation sets\n\npred_train_xgb = xgb.predict(trainx)\npred_val_xgb = xgb.predict(valx)","37b92c81":"#Confusion Matrix\n\nresults = confusion_matrix(valy, pred_val_xgb) \nprint('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(valy, pred_val_xgb))\nprint ('Report : ')\nprint (classification_report(valy, pred_val_xgb))","a85f9618":"rfc = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='entropy', max_depth=25, max_features='sqrt',\n                       max_leaf_nodes=8, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=5,\n                       min_weight_fraction_leaf=0.0, n_estimators=110,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)\n\nrfc.fit(trainx, trainy)","b40b65ae":"#Predecting values on train and validation sets\n\npred_train_rfc = rfc.predict(trainx)\npred_val_rfc = rfc.predict(valx)","a8369e42":"#Confusion Matrix\n\nresults = confusion_matrix(valy, pred_val_rfc) \nprint('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(valy, pred_val_rfc))\nprint ('Report : ')\nprint (classification_report(valy, pred_val_rfc))","84d1456e":"gbc = GradientBoostingClassifier()\n\ngbc.fit(trainx, trainy)","025af1fd":"#Predecting values on train and validation sets\n\npred_train_gbc = gbc.predict(trainx)\npred_val_gbc = gbc.predict(valx)","a2acaaf3":"#Confusion Matrix\n\nresults = confusion_matrix(valy, pred_val_gbc) \nprint('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(valy, pred_val_gbc))\nprint ('Report : ')\nprint (classification_report(valy, pred_val_gbc))","7392dec4":"dtc = DecisionTreeClassifier(max_depth=15)\n\ndtc.fit(trainx, trainy)","e9d1c1b5":"#Predecting values on train and validation sets\n\npred_train_dt = dtc.predict(trainx)\npred_val_dt = dtc.predict(valx)","53237c0d":"#Confusion Matrix\n\nresults = confusion_matrix(valy, pred_val_dt) \nprint('Confusion Matrix :')\nprint(results) \nprint ('Accuracy Score :',accuracy_score(valy, pred_val_dt))\nprint ('Report : ')\nprint (classification_report(valy, pred_val_dt))","4867f1d2":"kf = StratifiedKFold(n_splits=5,shuffle=True,random_state=123)\npred_test_cat =0\ncat_score =[]\ni=1\nfor train_index,test_index in kf.split(train_new,Target):\n    print('{} of KFold {}'.format(i,kf.n_splits))\n    xtr,xvl = train_new.loc[train_index],train_new.loc[test_index]\n    ytr,yvl = Target.iloc[train_index],Target.iloc[test_index]\n    \n    cat = CatBoostClassifier(depth=5, learning_rate=0.03)\n    cat.fit(xtr,ytr)\n    score = accuracy_score(yvl,cat.predict(xvl))\n    print('Accuracy Score:',score)\n    cat_score.append(score)    \n    pred_test = cat.predict_proba(test_new)[:,1]\n    pred_test_cat +=pred_test\n    i+=1","92e6fad1":"pred_test_cat = pred_test_cat\/5","105f3835":"# Creating submission file\n\nsubmission = pd.DataFrame({'PassengerId': test_copy['PassengerId'], 'Survived': pred_test_cat})\nsubmission['Survived'] = submission['Survived'].apply(lambda x: 1 if x>0.5 else 0)\nsubmission.head()","d2b32344":"submission['Survived'].value_counts()","d4a6eebd":"submission.to_csv(\"Submission.csv\", index = False)","536fd7d1":"### 1. CatBoost","fbab05db":"### 4. GBClassifier","1c154a0d":"## Submission","2c8bafb4":"### Note:\nFrom above models, CATboost classifier works better so lets apply StratifiedKFlod  and check performance ","3be0053d":"### 5. DTClassifier","3f591124":"### 2. XGBoost","4139e2ee":"### 3. RFClassifier","c5c1ff2b":"## Models"}}