{"cell_type":{"3daceb45":"code","73d8b3be":"code","85acb5f6":"code","4cd91488":"code","c27ebe06":"code","44472956":"code","8cc789c5":"code","ecac7931":"code","a93d955e":"code","ecf1f29b":"code","8cbc1caf":"code","dd64cb00":"code","4e306ee7":"code","f790d90f":"code","752c8d7c":"code","6dafb499":"code","36607877":"code","f66145a2":"code","04acffbc":"code","3bdd3852":"code","5cc61600":"code","d4b788da":"code","57de5f4d":"code","e68cbdca":"code","21f006ea":"code","ef2a8e58":"code","5e417498":"code","6316523e":"code","75343d26":"code","d97feb79":"code","2a74e3a1":"code","4d001866":"code","853f110e":"code","8a3dfe6e":"code","8ff54243":"code","9aa4e5a6":"code","d264f69b":"code","07a2cfcb":"code","b0a835bc":"code","fcaf2d81":"code","40e84bf5":"code","78c2ecad":"code","3189dfcf":"code","45d00762":"code","3d3202b6":"code","30db22a8":"code","7c3d0ce9":"code","22c54b01":"code","3f2edee9":"code","1167bb6e":"code","e2d66481":"code","24c278e3":"code","3b871b38":"code","b0739f3a":"code","994d028e":"code","61d45c31":"code","077f3c2e":"code","6ef6b2eb":"code","039a8b15":"code","7604dce3":"code","99df394e":"code","68787bd4":"code","daf823f7":"code","25f4acd8":"code","c21e3aff":"code","9c7f39e3":"code","def397a8":"code","8a8f19d7":"code","c762991c":"code","6c20e351":"code","b20da0bb":"code","6af4bc30":"code","a0a09277":"code","4af172d8":"code","8699f53f":"code","6b4b6b78":"code","0528eff8":"code","74ad4695":"code","9a239d3c":"code","c47f904f":"code","5b5fd5b3":"code","81b12697":"code","0ed35d3b":"code","38b442bb":"code","3684a4a9":"code","8623e19b":"code","9bee4c5d":"code","bf536eed":"code","03aca967":"code","a986d7c1":"code","a8187065":"code","05badfda":"markdown","1cfc8e8d":"markdown","af0ccb5b":"markdown","f4fa24cd":"markdown","aec1ddaf":"markdown","3964bf72":"markdown","40e32cc3":"markdown","8db44dca":"markdown","b75c205f":"markdown","2263dad6":"markdown","946f3bec":"markdown","b59e5e7c":"markdown","62f8c70c":"markdown","313a0fbd":"markdown","7d21936c":"markdown","6654c349":"markdown","771ac87e":"markdown","e63e679f":"markdown","ff5c18e9":"markdown","78885ba2":"markdown","248dbc29":"markdown","1a0da903":"markdown","c4b9321d":"markdown","88918a27":"markdown","05365337":"markdown","4d2a3b16":"markdown","a7e62d99":"markdown","fa50c551":"markdown","39f7898d":"markdown","fb3b0126":"markdown","1570cc4b":"markdown","a4e37a9a":"markdown","9235bbe5":"markdown","02f64b0f":"markdown","334cd34f":"markdown","e78280fc":"markdown","f54d08ba":"markdown","a491243e":"markdown","931f70aa":"markdown","195d26ee":"markdown","997b4f6c":"markdown","795ab6cd":"markdown","82ab8048":"markdown","fcd1b14d":"markdown","da7483f1":"markdown","c5590611":"markdown","54b0c85a":"markdown","eb223e87":"markdown","eec0dd22":"markdown","42020bda":"markdown","002437fd":"markdown","9e7a613d":"markdown","3d3f3667":"markdown","55ea33cb":"markdown","9974d24c":"markdown","cd506570":"markdown","e7a76918":"markdown","393d5182":"markdown","2e5f7b78":"markdown","648204c3":"markdown","198e19b9":"markdown","8e278c1d":"markdown","d7ccdae2":"markdown","fcc5d2d2":"markdown","2ef1fcf2":"markdown"},"source":{"3daceb45":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","73d8b3be":"import matplotlib.pyplot as plt\nplt.rcParams['figure.figsize']=[20,8]\nimport seaborn as sns\n\n##\ndf=pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ndf.head(2)\ndf.dtypes","85acb5f6":"plt.figure(figsize=(10,6))\nsns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='coolwarm') # missing Value","4cd91488":"from sklearn.preprocessing import StandardScaler\n\n##\n#Standard deviation looks at how spread out a group of numbers is from the mean, by looking at the square root of the variance.(\u221avariance) \n#The variance measures the average degree to which each point differs from the mean\u2014the average of all data points.\nprint('Mean',df.age.mean(),'Variance',df.age.var(),'standard deviation',df.age.std())\n\n##\nage=np.array(df.age).reshape(-1, 1)\nstandard=StandardScaler().fit_transform(age) ## Age_changed = (Age\u2212\u03bc)\/\u03c3  WHERE \u03c3 is Standard Deviation and \u03bc is mean\n\n##\nst_mean=standard.mean(axis=0) #axis = 0 means along the column and axis = 1 means working along the row.\nst_var=standard.var(axis=0)\nst_std=standard.std(axis=0)\nprint('After-Mean',st_mean[0],'After-Variance',st_var[0],'After-standard deviation',st_std[0])\n\n# Compare the plots after Standardize\nplt.figure(figsize=(16,5))\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.set_title(\"Before-Standardize\",fontsize=24)\nsns.boxplot(x=df['age'],ax=ax1) \nax2.set_title(\"After-Standardize\",fontsize=24)\nsns.boxplot(x=standard,ax=ax2) \nplt.show()","c27ebe06":"data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n\n##\nscaler = StandardScaler()\n\n##\nscaler_fit=scaler.fit(data)\n##partial_fit(X, y=None, sample_weight=None)  All of X is processed as a single batch. \n#This is intended for cases when fit is not feasible due to very large number of n_samples or because X is read from a continuous stream.\nscaler_partial_fit=scaler.partial_fit(data)\n\n##\nscaler_transform=scaler_fit.transform(data)\nscaler_inverse_transform=scaler_fit.inverse_transform(scaler_transform) # data\n\n##\nprint(scaler_fit.mean_,scaler_partial_fit.mean_)\nprint(scaler_fit,scaler_partial_fit)\nprint(scaler_transform)\nprint(scaler_inverse_transform)\nprint(scaler_fit.get_params(deep=True))","44472956":"df1=pd.read_csv('..\/input\/building-permit-applications-data\/Building_Permits.csv',low_memory=False)\n\nnumerical_cols=df1.select_dtypes(['int64','float64'])[:100]\ncategorical_cols=df1.select_dtypes(['object'])[:100]\n\nplt.figure(figsize=(16,5))\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.set_title(\"numerical_cols\",fontsize=24)\nsns.heatmap(numerical_cols.isnull(),yticklabels=False,cbar=False,cmap='Spectral',ax=ax1) # missing Value\nax2.set_title(\"categorical_cols\",fontsize=24)\nsns.heatmap(categorical_cols.isnull(),yticklabels=False,cbar=False,cmap='Set1',ax=ax2) # missing Value\nplt.show()","8cc789c5":"from sklearn.impute import SimpleImputer\n\n##\nimp_mean = SimpleImputer(missing_values=np.nan, strategy='mean') #['mean', 'median', 'most_frequent', 'constant']\n##\nsimple_imputer=imp_mean.fit_transform(numerical_cols) #<class 'numpy.ndarray'>\nprint(type(simple_imputer))\n\n##\nnumerical_cols = pd.DataFrame(simple_imputer, columns = numerical_cols.columns)\nplt.figure(figsize=(16,5))\nfig, (ax1, ax2) = plt.subplots(1, 2)\nsns.heatmap(numerical_cols.isnull(),yticklabels=False,cbar=False,cmap='Spectral',ax=ax1) # missing Value\n\n##\nimp_mode = SimpleImputer(missing_values=np.nan, strategy='most_frequent') #['mean', 'median', 'most_frequent', 'constant']\n##\nsimple_imputer1=imp_mode.fit_transform(categorical_cols) #<class 'numpy.ndarray'>\n\n# ## As [Unit Suffix,Structural Notification,TIDF Compliance ] droped by SimpleImputer because full column is null\ncolumns=['Permit Number', 'Permit Type Definition', 'Permit Creation Date','Block', 'Lot', 'Street Number Suffix', 'Street Name', 'Street Suffix', 'Description', 'Current Status', 'Current Status Date','Filed Date', 'Issued Date', 'Completed Date','First Construction Document Date','Voluntary Soft-Story Retrofit', 'Fire Only Permit','Permit Expiration Date', 'Existing Use', 'Proposed Use', 'Existing Construction Type Description','Proposed Construction Type Description', 'Site Permit','Neighborhoods - Analysis Boundaries', 'Location']\ncategorical_cols = pd.DataFrame(simple_imputer1, columns =columns)\nsns.heatmap(numerical_cols.isnull(),yticklabels=False,cbar=False,cmap='Set1',ax=ax2) # missing Value","ecac7931":"from sklearn.preprocessing import LabelBinarizer\n\n##\ndf1['Street Suffix'].fillna(df1['Street Suffix'].mode()[0],inplace=True)\nstr_suf=df1['Street Suffix'].unique() #['St', 'Av', 'Tr', 'Ct', 'Bl', 'Wy', 'Dr', 'Rd', 'Cr', 'Pl', 'Ln','Hy', 'Pk', 'Al', 'Pz', 'Wk', 'Rw', 'So', 'Sw', 'No', 'Hl']\n\n##\nlb =LabelBinarizer()\nlb_fit_trans=lb.fit_transform(str_suf)\nprint(lb.classes_,lb.y_type_,lb.sparse_input_)\nprint('\\n',lb_fit_trans.shape,'\\n',lb_fit_trans)\n\n##\nlb1_fit_trans=lb.fit_transform(['yes', 'no', 'no', 'yes'])\nprint('\\n',lb.classes_,lb.y_type_,lb.sparse_input_)\ndisplay(lb1_fit_trans)","a93d955e":"from sklearn.preprocessing import OneHotEncoder\n\n##\ndf1['Existing Construction Type Description'].fillna(df1['Existing Construction Type Description'].mode()[0],inplace=True)\n\n##\nenc = OneHotEncoder()\nenc_fit_trans=enc.fit_transform(df1[['Existing Construction Type Description']])\n##\nobj_df=pd.DataFrame(enc_fit_trans.toarray(), columns=enc.categories_[0])\n##\nobj_df = obj_df.join(df1)\nobj_df.drop('Existing Construction Type Description',axis=1,inplace=True)\nobj_df.head(2)","ecf1f29b":"from sklearn.preprocessing import PolynomialFeatures\n\n# Load dataset\nurl = \"https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/sonar.csv\"\ndataset = pd.read_csv(url, header=None)\n# summarize the shape of the dataset\nprint(dataset.shape)\n# summarize each variable\ndisplay(dataset.head(2))\n\n# perform a polynomial features transform of the dataset\ntrans = PolynomialFeatures(degree=2)\ndata = trans.fit_transform(dataset.iloc[:,:-1])\n\n# convert the array back to a dataframe\ndataset = pd.DataFrame(data)\n\n# summarize\ndisplay(dataset.head(2))","8cbc1caf":"#simple Example\nX=[[1, 2],[3, 4],[100, 10]]\npoly = PolynomialFeatures(degree=2) #[1, a, b, a^2, ab, b^2].\nresult=poly.fit_transform(X)\ndisplay(result)\n##\npoly = PolynomialFeatures(degree=2,interaction_only=True) #[1, a, b ,ab].\nresult1=poly.fit_transform(X)\ndisplay(result1)","dd64cb00":"from sklearn.model_selection import cross_val_score,cross_val_predict,cross_validate\nfrom sklearn.tree import DecisionTreeRegressor\n\n# Load dataset\nurl = \"https:\/\/raw.githubusercontent.com\/jbrownlee\/Datasets\/master\/sonar.csv\"\nobj_df = pd.read_csv(url, header=None)\n\n#Label Encoding\nobj_df[60] = obj_df[60].astype('category')\n#Then you can assign the encoded variable to a new column using the cat.codes accessor:\nobj_df[60]=obj_df[60].cat.codes\ndisplay(obj_df.head(2))\n\n##\ny=obj_df.pop(60)\nX=obj_df\n\n##\nregressor = DecisionTreeRegressor(random_state=0) #  max_depth=None\nbase_estimator = DecisionTreeRegressor().fit(X, y)\nprint(\"The model training score is\" , base_estimator.score(X, y)) #A perfect score of 1.0 itself indicates the overfitting of the model.\nprint('base_estimator.predict',base_estimator.predict(obj_df.loc[[0]]))\n\nbase_estimator1 = DecisionTreeRegressor(max_depth=6,random_state=0).fit(X, y)\nprint(\"The model1 training score is\" , base_estimator1.score(X, y)) #A perfect score of 1.0 will indicates the overfitting of the model.\nprint('base_estimator1.predict',base_estimator1.predict(obj_df.loc[[1]]))\n\n##\nscore=cross_val_score(regressor, X, y, cv=10) #Evaluate a score by cross-validation #https:\/\/www.kaggle.com\/learn-forum\/59719\nprint('\\ncross_val_score',score)\n#cross_val_score returns score of test fold where cross_val_predict returns predicted y values for the test fold.\ny_pred = cross_val_predict(regressor, X, y, cv=20)\nprint('\\ncross_val_predict',y_pred)\n#\nscores = cross_validate(regressor, X, y, cv=3,scoring=('r2', 'neg_mean_squared_error'),return_train_score=True)#Evaluate metric(s) by cross-validation and also record fit\/score times.\nprint('\\ncross_validate',scores)","4e306ee7":"y_predict=base_estimator.predict(X)\ny_predict1=base_estimator1.predict(X)\nplt.figure(figsize=(16,5))\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.plot(y,y_predict)\nax1.text(0.2, 0.9,'Score = '+str(base_estimator.score(X,y)), fontsize=15)\nax1.set_title('DecisionTreeRegressor( max_depth=None,random_state=0)',fontsize=20)\nax1.set_ylabel(\"Predict-y\")\nax1.set_xlabel('Actual-y')\n##\nax2.plot(y,y_predict1)\nax2.text(0.2, 0.9,'Score = '+str(base_estimator1.score(X,y)), fontsize=15)\nax2.set_title(\"DecisionTreeRegressor(max_depth=6,random_state=0)\",fontsize=20)\nax2.set_ylabel(\"Predict-y\")\nax2.set_xlabel('Actual-y')\nplt.legend(loc='best')\nplt.show()","f790d90f":"from sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nregr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2)) #LinearSVR,SGDRegressor,Nystroem transformer.\nregr_fit=regr.fit(X, y)\nsvr_score=regr_fit.score(X, y, sample_weight=None)\ny_predict=regr_fit.predict(X)\n\n##\nprint(regr_fit)\nprint('\\n',regr_fit.get_params(deep=True))\nprint('\\nScore',svr_score)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_predict)\nplt.title('''('svr', SVR(epsilon=0.2)'''+str(),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.text(0.2, 0.9,'Score = '+str(svr_score), fontsize=15)\nplt.show()","752c8d7c":"reg = SVR(kernel = 'rbf')\nreg_fit=reg.fit(X, y)\nsvr_score=reg_fit.score(X, y, sample_weight=None)\ny_pred = reg_fit.predict(X)\nprint('\\nScore-rbf',svr_score)\n##\nreg_l = SVR(kernel = 'linear')\nreg_fit_l=reg_l.fit(X, y)\nsvr_score_l=reg_fit_l.score(X, y, sample_weight=None)\ny_pred1 = reg_fit_l.predict(X)\nprint('\\nScore-Linear',svr_score_l)\n\nplt.figure(figsize=(16,5))\nfig, (ax1, ax2) = plt.subplots(1, 2)\nax1.plot(y,y_pred)\nax1.errorbar(y, y_pred, fmt='o')\nax1.set_title('''SVR(kernel = 'rbf')''',fontsize=20)\nax1.set_ylabel(\"Predict-y\",fontsize=14)\nax1.set_xlabel('Actual-y',fontsize=14)\nax1.text(0.2, 0.9,'Score = '+str(svr_score), fontsize=15)\n##\nax2.plot(y,y_pred1)\nax2.errorbar(y, y_pred, fmt='o')\nax2.set_title('''SVR(kernel = 'Linear')''',fontsize=20)\nax2.set_ylabel(\"Predict-y\",fontsize=14)\nax2.set_xlabel('Actual-y',fontsize=14)\nax2.text(0.2, 0.9,'Score = '+str(svr_score_l), fontsize=15)\nplt.show()","6dafb499":"reg_p = SVR(kernel = 'poly')\nreg_fit_p=reg_p.fit(X, y)\nsvr_score_p=reg_fit_p.score(X, y, sample_weight=None)\ny_pred = reg_fit_p.predict(X)\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title('''SVR(kernel = 'poly')''',fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.text(0.2, 0.9,'Score = '+str(svr_score_p), fontsize=15)\nplt.show()","36607877":"#Working with Epislon\neps = 20\nreg_eps = SVR(kernel = 'rbf', epsilon=eps)\nreg_fit_eps=reg_eps.fit(X, y)\ny_pred = reg_fit_eps.predict(X)\nsvr_score_eps=reg_fit_eps.score(X, y, sample_weight=None)\nprint('''\\nSVR(kernel = 'rbf', epsilon=eps)''',svr_score_eps)","f66145a2":"# **How Hstack work**\nin_arr1 = np.array([ 1, 2, 3] )\nin_arr2 = np.array([ 4,5,6] )\nout_arr=np.hstack((in_arr1, in_arr2))\ndisplay(out_arr)\nin_arr1 = np.array([[ 1, 2, 3], [ -1, -2, -3]] )\nin_arr2 = np.array([[ 4, 5, 6], [ -4, -5, -6]] )\nout_arr=np.hstack((in_arr1, in_arr2))\ndisplay(out_arr)\nprint('\\n')\n# **How Vstack work**\nin_arr1 = np.array([ 1, 2, 3] )\nin_arr2 = np.array([ 4,5,6] )\nout_arr=np.vstack((in_arr1, in_arr2))\ndisplay(out_arr)\nin_arr1 = np.array([[ 1, 2, 3], [ -1, -2, -3]] )\nin_arr2 = np.array([[ 4, 5, 6], [ -4, -5, -6]] )\nout_arr=np.vstack((in_arr1, in_arr2))\ndisplay(out_arr)","04acffbc":"# # numpy.linspace() in Python\n# numpy.linspace(start,stop,num = 50,endpoint = True,retstep = False,dtype = None)\n# -> start  : [optional] start of interval range. By default start = 0\n# -> stop   : end of interval range\n# -> restep : If True, return (samples, step). By deflut restep = False\n# -> num    : [int, optional] No. of samples to generate\n# -> dtype  : type of output array\n##\nprint(np.linspace(start=2.0, stop=3.0,endpoint = True,num=5, retstep=True,dtype = None))\n##\nprint(np.linspace(start=2.0, stop=3.0,\n                  endpoint = False,num=5, retstep=True,dtype = None))\n##\nprint(np.linspace(start=2.0, stop=3.0,endpoint = True,num=5, \n                  retstep=False,dtype = 'int'))","3bdd3852":"from sklearn.svm import SVC\n##\nx=np.linspace(start=-5.0,stop=5.0,num=100)\nradius=10\ny=np.sqrt(radius**2 - x**2)\ny=np.hstack([y,-y])\nx=np.hstack([x,-x])\n##\nx1=np.linspace(start=-5.0,stop=5.0,num=100)\nradius1=5\ny1=np.sqrt(radius1**2 - x1**2)\ny1=np.hstack([y1,-y1])\nx1=np.hstack([x1,-x1])\n\n##\nplt.scatter(y,x)   # outer circle \nplt.scatter(y1,x1)  # inner circle ","5cc61600":"df7=pd.DataFrame(np.vstack([y,x]).T,columns=['X1','X2'])\ndf7['Y']=0\ndf8=pd.DataFrame(np.vstack([y1,x1]).T,columns=['X1','X2'])\ndf8['Y']=1\ndf9=df7.append(df8)\ndf9.head(5)\n\n##\nX=df9.iloc[:,:2]\ny=df9.Y\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nclassifier=SVC(kernel='linear').fit(X_train, y_train)\ny_pred=classifier.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nprint('linear - accuracy_score',accuracy_score(y_test, y_pred, normalize=True))  # less than 0.5 very bad because we trying to draw linear line for above diagram\n# accuracy_score(y_test, y_pred, normalize=False) # 65\n\n# kernel{\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019}, default=\u2019rbf\u2019\nclassifier1=SVC(kernel='sigmoid').fit(X_train, y_train)\ny_pred1=classifier1.predict(X_test)\nprint('sigmoid - accuracy_score',accuracy_score(y_test, y_pred1, normalize=True))\n\n# kernel{\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019}, default=\u2019rbf\u2019\nclassifier1=SVC(kernel='poly').fit(X_train, y_train)\ny_pred1=classifier1.predict(X_test)\nprint('poly - accuracy_score',accuracy_score(y_test, y_pred1, normalize=True))\n\n# kernel{\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019}, default=\u2019rbf\u2019\n# classifier1=SVC(kernel='precomputed').fit(X_train, y_train)   #ValueError: Precomputed matrix must be a square matrix. Input is a 268x2 matrix.\n\n# kernel{\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019}, default=\u2019rbf\u2019\nclassifier1=SVC(kernel='rbf').fit(X_train, y_train)\ny_pred1=classifier1.predict(X_test)\nprint('rbf - accuracy_score',accuracy_score(y_test, y_pred1, normalize=True))","d4b788da":"# Polynomail kernel working   K(x,y) = (xT.y +c)dimension  Increase the dimension and draw a linear line to get better result \n# X1_square,X2_square,X1*X2\ndisplay(df9.head(2))\ndf9['X1_square']=df9['X1']**2\ndf9['X2_square']=df9['X2']**2\ndf9['X1*X2']=(df9['X1']*df9['X2'])\ndisplay(df9.head(2))\n\n##\nX1=df9[['X1','X2','X1_square','X2_square','X1*X2']]\ny1=df9['Y']\n\nimport plotly.express as px # Before\nfig=px.scatter_3d(df9,x='X1',y='X2',z='X1*X2',color='Y') # single dimension(X1,X2) in diagram we not able to get good classification\nfig.show()","57de5f4d":"import plotly.express as px # After\nfig=px.scatter_3d(df9,x='X1_square',y='X2_square',z='X1*X2',color='Y') # Two dimension(X1_square,X2_square) in diagram \nfig.show()\n# Now we able to draw linear line and classify data\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\nclassifier=SVC(kernel='linear').fit(X_train, y_train)\ny_pred=classifier.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nprint('new linear - accuracy_score after increased dimesion',accuracy_score(y_test, y_pred, normalize=True)) \n# previous linear - accuracy_score 0.49242424242424243\n# new linear - accuracy_score 0.9574468085106383","e68cbdca":"from sklearn.linear_model import LinearRegression\n\nreg_fit = LinearRegression().fit(X, y)\nscore_linear=reg_fit.score(X, y)\ny_pred=reg_fit.predict(X)\n# print('LinearRegression-Scores',score_linear)\n# print('m',reg_fit.coef_,'b',reg_fit.intercept_)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title('''LinearRegression''',fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.text(0.2, 0.9,'Score = '+str(score_linear), fontsize=15)\nplt.show()","21f006ea":"from sklearn.preprocessing import StandardScaler\n\n##\nstd = StandardScaler()\nprint(std)\nstd_fit_trans_X=std.fit_transform(X)\n\nreg_fit = LinearRegression().fit(std_fit_trans_X, y)\nscore_linear1=reg_fit.score(std_fit_trans_X, y)\ny_pred=reg_fit.predict(std_fit_trans_X)\n# print('LinearRegression-Scores',score_linear1)\n# print('m',reg_fit.coef_,'b',reg_fit.intercept_)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title('''LinearRegression''',fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.text(0.2, 0.9,'Score = '+str(score_linear1), fontsize=15)\nplt.show()","ef2a8e58":"from sklearn.linear_model import Ridge\n\n#||y - Xw||^2_2 + alpha * ||w||^2_2\nclf = Ridge(alpha=8)# Regularization improves the conditioning of the problem and reduces the variance of the estimates\nprint(clf)\nridge_fit=clf.fit(X, y)\ny_pred=ridge_fit.predict(X)\nridge_score=ridge_fit.score(X, y) \n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title('''RidgeRegression l2 regularization''',fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.text(0.2, 0.9,'Score = '+str(ridge_score), fontsize=15)\nplt.show()","5e417498":"from sklearn import linear_model\n\nclf = linear_model.Lasso(alpha=0.1)\nprint(clf)\nclf_fit=clf.fit(X,y)\nclf_score=clf_fit.score(X, y)\n# print(clf_fit.coef_,clf_fit.intercept_)\ny_pred=clf_fit.predict(X)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title('''LassoRegression l1 regularization '''+'Score = '+str(clf_score),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","6316523e":"from sklearn.linear_model import SGDRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\n# Always scale the input. The most convenient way is to use a pipeline.\nreg = make_pipeline(StandardScaler(),SGDRegressor(max_iter=1000, tol=1e-3))\nreg_fit=reg.fit(X, y)\nreg_score=reg_fit.score(X, y)\ny_pred=reg_fit.predict(X)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title('''SGDRegressor  '''+'Score = '+str(reg_score),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","75343d26":"# Without scale the input. \nreg=SGDRegressor(max_iter=1000, tol=1e-3)\nreg_fit=reg.fit(X, y)\nreg_score=reg_fit.score(X, y)\ny_pred=reg_fit.predict(X)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title('''Without scale the input SGDRegressor  '''+'Score = '+str(reg_score),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","d97feb79":"from sklearn.linear_model import ElasticNet\nregr = ElasticNet(random_state=0)\nprint(regr)\nregr_fit=regr.fit(X, y)\nels_score=regr_fit.score(X, y)\n# print(regr_fit.coef_,regr_fit.intercept_)\ny_pred=regr_fit.predict(X)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(regr)+' Score = '+str(els_score),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","2a74e3a1":"from sklearn.ensemble import RandomForestRegressor\n\n\nregr = RandomForestRegressor(max_depth=2, random_state=0)\nregr_fit=regr.fit(X, y)\nran_score=reg_fit.score(X, y)\ny_pred=reg_fit.predict(X)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(regr)+' Score = '+str(ran_score),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","4d001866":"from sklearn.ensemble import GradientBoostingRegressor\nreg = GradientBoostingRegressor(random_state=0)\nreg_fit=reg.fit(X, y)\ngbr_score=reg_fit.score(X, y)\ny_pred=reg_fit.predict(X)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(reg)+' Score = '+str(gbr_score),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","853f110e":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\n\n# define the model\nmodel = GradientBoostingRegressor()\n\n# define the evaluation procedure\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n\n# evaluate the model >> Array of scores of the estimator for each run of the cross validation.\nn_scores = cross_val_score(model, X, y, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1)\n# n_scores = cross_val_score(model, X, y)\nprint(n_scores)\n\n# report performance\nprint('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))","8a3dfe6e":"from sklearn.neural_network import MLPRegressor\n\nregr_fit= MLPRegressor(random_state=1, max_iter=500).fit(X, y)\nmlp_score=reg_fit.score(X, y)\ny_pred=reg_fit.predict(X)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title('MLPRegressor '+' Score = '+str(mlp_score),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","8ff54243":"from sklearn.neural_network import MLPClassifier\n\n##\nclf=MLPClassifier(random_state=1, max_iter=300)\nclf_fit= clf.fit(X, y)\nmlp_scores=clf_fit.score(X,y)\ny_pred=clf_fit.predict(X)\ny_pred_prob=clf_fit.predict_proba(X)\n# print(y_pred_prob)\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(clf)+' Score = '+str(mlp_score),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","9aa4e5a6":"from sklearn.tree import DecisionTreeClassifier\n\n##\nclf = DecisionTreeClassifier(random_state=0) #DecisionTreeClassifier(criterion='entropy') default=\u201dgini\u201d\nclf_fit= clf.fit(X, y)\ndtc_scores=clf_fit.score(X,y)\ny_pred=clf_fit.predict(X)\ny_pred_prob=clf_fit.predict_proba(X)\n# print(y_pred_prob)\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(clf)+' Score = '+str(dtc_scores),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","d264f69b":"df5=pd.read_csv('..\/input\/drugs-a-b-c-x-y-for-decision-trees\/drug200.csv')\nfrom sklearn.preprocessing import LabelEncoder\nlb=LabelEncoder()\ndf5['Sex_n']=lb.fit_transform(df5['Sex'])\ndf5['BP_n']=lb.fit_transform(df5['BP'])\ndf5['Cholesterol_n']=lb.fit_transform(df5['Cholesterol'])\ndf5['Drug_n']=lb.fit_transform(df5['Drug'])\ndf5.head(2)","07a2cfcb":"!pip install pydotplus\n!pip install graphviz","b0a835bc":"\nfrom sklearn import tree\nimport pydotplus\nimport matplotlib.image as pltimg\n\n\nfeatures = ['Age','Sex_n','BP_n','Cholesterol_n','Na_to_K']\nX1=df5[features]\ny1=df5['Drug_n']\n\ndtree = DecisionTreeClassifier(criterion='entropy')\ndtree = dtree.fit(X1, y1)","fcaf2d81":"##\nplottree=tree.plot_tree(dtree)\nprint(plottree,'\\n')\n\n##\ndata = tree.export_graphviz(dtree, out_file=None, feature_names=features)\ngraph = pydotplus.graph_from_dot_data(data)\ngraph.write_png('mydecisiontree.png')\n##\nimg=pltimg.imread('mydecisiontree.png')\nimgplot = plt.imshow(img)\nplt.show()","40e84bf5":"from sklearn.tree import export_text\nimport graphviz \n##\ndata = tree.export_graphviz(dtree, out_file=None, feature_names=features)\ngraph = graphviz.Source(data) \ndisplay(graph)\n\n##\nr = export_text(dtree, feature_names=features)\nprint('\\n',r)","78c2ecad":"from sklearn.svm import SVC\n\n##\nclf = SVC(gamma='auto',probability=True)\nclf_fit= clf.fit(X, y)\nsvc_scores=clf_fit.score(X,y)\ny_pred=clf_fit.predict(X)\n##\ny_pred_prob=clf_fit.predict_proba(X)\ndec_func=clf_fit.decision_function(X)\n# print(dec_func)\n# print(y_pred_prob)\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(clf)+' Score = '+str(svc_scores),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","3189dfcf":"from sklearn.linear_model import LogisticRegression\n\n##\nclf = LogisticRegression(random_state=0)\nclf_fit=clf.fit(X, y)\nlogist_scores=clf_fit.score(X,y)\ny_pred=clf_fit.predict(X)\n##\ny_pred_prob=clf_fit.predict_proba(X)\ndec_func=clf_fit.decision_function(X)\n# print(dec_func)\n# print(y_pred_prob)\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(clf)+' Score = '+str(logist_scores),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","45d00762":"from sklearn.linear_model import SGDClassifier\n\nclf=SGDClassifier(max_iter=1000, tol=0.001,loss='log')\nclf_fit=clf.fit(X, y)\nsgd_scores=clf_fit.score(X,y)\ny_pred=clf_fit.predict(X)\n##\ny_pred_prob=clf_fit.predict_proba(X)\ndec_func=clf_fit.decision_function(X)\n# print(dec_func)\n# print(y_pred_prob)\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(clf)+' Score = '+str(sgd_scores),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","3d3202b6":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n\nclf = make_pipeline(StandardScaler(),SGDClassifier(max_iter=1000, tol=0.001,loss='log'))\nclf_fit=clf.fit(X, y)\nsgd_scores=clf_fit.score(X,y)\ny_pred=clf_fit.predict(X)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(clf)+' Score = '+str(sgd_scores),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","30db22a8":"from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf_fit=clf.fit(X, y)\ngauss_scores=clf_fit.score(X,y)\ny_pred=clf_fit.predict(X)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(clf)+' Score = '+str(gauss_scores),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","7c3d0ce9":"from sklearn.neighbors import KNeighborsClassifier\n\nclf = KNeighborsClassifier(n_neighbors=3) #algorithm{\u2018auto\u2019, \u2018ball_tree\u2019, \u2018kd_tree\u2019, \u2018brute\u2019}, default=\u2019auto\u2019\nclf_fit=clf.fit(X, y)\nkn_scores=clf_fit.score(X,y)\ny_pred=clf_fit.predict(X)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(clf)+' Score = '+str(kn_scores),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","22c54b01":"pipe_df=pd.read_csv('..\/input\/faulty-steel-plates\/faults.csv')\nX=pipe_df.iloc[:,:-1]\ny=pipe_df['Other_Faults']\nsns.countplot(x=\"Other_Faults\", data=pipe_df)\n\n# Splitting the dataset into training and test set.  \nfrom sklearn.model_selection import train_test_split  \nx_train, x_test, y_train, y_test= train_test_split(X, y, test_size= 0.25, random_state=0)\n\n#feature Scaling  \nfrom sklearn.preprocessing import StandardScaler    \nst_x= StandardScaler()   \nx_train= st_x.fit_transform(x_train)    \nx_test= st_x.transform(x_test)  \n\nclassifier = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2 )\nclf_fit=classifier.fit(x_train, y_train)\nkn_scores=clf_fit.score(x_test, y_test)\ny_pred=clf_fit.predict(x_test)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y_test,y_pred)\nplt.errorbar(y_test, y_pred, fmt='o')\nplt.title(str(clf)+' Score = '+str(kn_scores),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()\n\n#Creating the Confusion matrix  \nfrom sklearn.metrics import confusion_matrix  \ncm= confusion_matrix(y_test, y_pred)  \n# visualize confusion matrix with seaborn heatmap\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')\n\n#Import scikit-learn metrics module for accuracy calculation\nfrom sklearn import metrics\n\n# Model Accuracy: how often is the classifier correct?\nprint('Accuracy is the degree of closeness to true value.')\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n\n# Model Precision: what percentage of positive tuples are labeled as such?( how valid the results are)\nprint('\\nPrecision is the degree to which an instrument or process will repeat the same value.')\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\n\n# Model Recall: what percentage of complete the results are\nprint('\\nRecall is a metric that quantifies the number of correct positive predictions made out of all positive predictions that could have been made. ')\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\n\nprint('\\nThe F-score, also called the F1-score, is a measure of a model\\'s accuracy on a dataset.')\nprint('\\nSupport is the number of samples of the true response that lie in that class(Imbalanced dataset)')\nfrom sklearn.metrics import classification_report\nprint('\\nclassification_report\\n',classification_report(y_test, y_pred))","3f2edee9":"# First, we split the data into training and testing\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.25, random_state=0)\n    \n# We create two lists to keep training and test accuracies. We'll later use them to evaluate an \n# appropriate number of neighbors\ntraining_accuracy = []\ntest_accuracy = []\nerror_rate=[]\n# We define a range of 1 to 40(included) neighbors that will be tested\nneighbors_settings = range(1,40)\n\n# We loop the KNN model through the range of possible neighbors to evaluate which one would be \n# appropriate for this analysis\n\nfor n_neighbors in neighbors_settings:\n    \n    # creating the KNN classifier\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    # fitting the model\n    clf.fit(X_train, y_train)\n    #recording the accuracy of the training set\n    training_accuracy.append(clf.score(X_train, y_train))\n    #recording the accuracy of the test set\n    test_accuracy.append(clf.score(X_test, y_test))\n    ##\n    y_pred=clf_fit.predict(X_test)\n    error_rate.append(np.mean(y_pred != y_test))\n    \n# Data Visualization - Evaluating the accuracy of both the training and the testing sets against \n# n_neighbors\n    \nplt.plot(neighbors_settings, training_accuracy, label='Accuracy of the Training Set')\nplt.plot(neighbors_settings, test_accuracy, label='Accuracy of the Test Set')\nplt.title('Accuracy vs K-value',fontsize=24)\nplt.ylabel('Accuracy',fontsize=24)\nplt.xlabel('Number of Neighbors',fontsize=24)\nplt.legend()","1167bb6e":"plt.plot(neighbors_settings, error_rate,color='blue',linestyle='dashed',marker='o',markerfacecolor='red',markersize=10)\nplt.title('Error Rate vs K-value',fontsize=24)\nplt.ylabel('error_rate',fontsize=24)\nplt.xlabel('Number of Neighbors',fontsize=24)\nplt.legend()","e2d66481":"# Find the K-neighbors of a point.\nfrom sklearn.neighbors import NearestNeighbors\nneigh = NearestNeighbors(n_neighbors=1)\nneigh.fit(X)\n\n##\nprint(neigh.kneighbors(X))\nprint(neigh.kneighbors(X, return_distance=False))\n\n##\n# A = neigh.kneighbors_graph(X)  # Compute the (weighted) graph of k-Neighbors for points in X.\n# A = neigh.radius_neighbors_graph(X) #Compute the (weighted) graph of Neighbors for points in X.","24c278e3":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(max_depth=2, random_state=0)\nclf_fit=clf.fit(X, y)\nran_scores=clf_fit.score(X,y)\ny_pred=clf_fit.predict(X)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(clf)+' Score = '+str(ran_scores),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","3b871b38":"car_df=pd.read_csv('..\/input\/car-evaluation-data-set\/car_evaluation.csv',header=None)\ncar_df.columns=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\nX2 = car_df.drop(['class'], axis=1)\ny2 = car_df['class']\n\n# split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size = 0.33, random_state = 42)\n\n# import category encoders\nimport category_encoders as ce\nencoder = ce.OrdinalEncoder(cols=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\nX_train = encoder.fit_transform(X_train)\nX_test = encoder.transform(X_test)\n# import Random Forest classifier\n\nfrom sklearn.ensemble import RandomForestClassifier\n# instantiate the classifier \nrfc = RandomForestClassifier(random_state=0)\n# fit the model\nrfc.fit(X_train, y_train)\n# Predict the Test set results\ny_pred = rfc.predict(X_test)\n# Check accuracy score \nfrom sklearn.metrics import accuracy_score\nprint('Model accuracy score with 10 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))\n\n\n# instantiate the classifier with n_estimators = 100\nrfc_100 = RandomForestClassifier(n_estimators=100, random_state=0)\n# fit the model to the training set\nrfc_100.fit(X_train, y_train)\n# Predict on the test set results\ny_pred_100 = rfc_100.predict(X_test)\n# Check accuracy score \nprint('Model accuracy score with 100 decision-trees : {0:0.4f}'. format(accuracy_score(y_test, y_pred_100)))","b0739f3a":"#Find important features with Random Forest model \n# create the classifier with n_estimators = 100\n\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\n# fit the model to the training set\nclf.fit(X_train, y_train)\n# view the feature scores\nfeature_scores = pd.Series(clf.feature_importances_, index=X_train.columns).sort_values(ascending=False)\ndisplay(feature_scores)\n\n\n# Creating a seaborn bar plot\nsns.barplot(x=feature_scores, y=feature_scores.index)\n# Add labels to the graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\n# Add title to the graph\nplt.title(\"Visualizing Important Features\")\n# Visualize the graph\nplt.show()","994d028e":"#Build Random Forest model on selected features \n# declare feature vector and target variable\nX2 = car_df.drop(['class', 'doors'], axis=1)\ny2 = car_df['class']\n# split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X2, y2, test_size = 0.33, random_state = 42)\n# encode categorical variables with ordinal encoding\nencoder = ce.OrdinalEncoder(cols=['buying', 'maint', 'persons', 'lug_boot', 'safety'])\nX_train = encoder.fit_transform(X_train)\nX_test = encoder.transform(X_test)\n# instantiate the classifier with n_estimators = 100\nclf = RandomForestClassifier(random_state=0)\n# fit the model to the training set\nclf.fit(X_train, y_train)\n# Predict on the test set results\ny_pred = clf.predict(X_test)\n# Check accuracy score \nprint('Model accuracy score with doors variable removed : {0:0.4f}'. format(accuracy_score(y_test, y_pred))) # improved   \n","61d45c31":"# Confusion matrix\n# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nprint('Confusion matrix\\n\\n', cm)\n\n# Classification Report \nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","077f3c2e":"from sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)\nclf_fit=clf.fit(X, y)\ngra_scores=clf_fit.score(X,y)\ny_pred=clf_fit.predict(X)\n\n##\nplt.figure(figsize=(16,5))\nplt.plot(y,y_pred)\nplt.errorbar(y, y_pred, fmt='o')\nplt.title(str(clf)+' Score = '+str(gra_scores),fontsize=20)\nplt.ylabel(\"Predict-y\",fontsize=14)\nplt.xlabel('Actual-y',fontsize=14)\nplt.show()","6ef6b2eb":"df2=df[['age']]\ndf2[:2]","039a8b15":"from sklearn.cluster import KMeans\n\nclu = KMeans(n_clusters=3, random_state=0)\nclu_fit=clu.fit(age)\n# print(clu_fit.labels_)\n# print(clu_fit.cluster_centers_)\nage_pred=clu.predict(age)\n\n##\nfrom sklearn.metrics import silhouette_score\nscore=silhouette_score(age,age_pred)\nprint('silhouette_score ',score)\n\n# ##\nplt.figure(figsize=(16,5))\nplt.scatter(age,age_pred)\nplt.title(str(clu)+' silhouette_score= '+str(score),fontsize=20)\nplt.ylabel(\"Predict-age cluster\",fontsize=14)\nplt.xlabel('Actual-age',fontsize=14)\nplt.show()","7604dce3":"##\ndf=pd.read_csv('..\/input\/student-study-hour-v2\/Student Study Hour V2.csv')\nsns.scatterplot(x='Hours',y='Scores',data=df)\n\n##\nfrom sklearn.cluster import KMeans\nindiviual_cluster=[]\nnum_cluster =5\nfor i in range(1,num_cluster):\n    kmeans = KMeans(n_clusters=i, random_state=0)\n    kmeans.fit(df)\n    indiviual_cluster.append(kmeans.inertia_)\n##    \nplt.figure(figsize=(16,5))\nplt.plot(range(1,num_cluster),indiviual_cluster)\nplt.title('Elbow Method',fontsize=20)\nplt.ylabel(\"Cluster Score\",fontsize=14)\nplt.xlabel('Number of cluster',fontsize=14)\n##\nprint('indiviual_cluster ',indiviual_cluster)","99df394e":"n_clusters=2\nkmeans = KMeans(n_clusters=n_clusters,init='random',random_state=0).fit(df)\npred=kmeans.predict(df)\n\n##\nprint('predict\\n',pred)\ndf['Cluster']=pd.DataFrame(pred,columns=['cluster'])\n##\nsns.lmplot(x='Hours',y='Scores',data=df,hue='Cluster')\nsns.lmplot(x='Hours',y='Scores',data=df,fit_reg=False,hue='Cluster',legend=True)\n##\nprint('Labels\\n',kmeans.labels_,'\\ncluster_centers_\\n',kmeans.cluster_centers_,'\\ninertia_\\n',kmeans.inertia_,'in n_clusters ',n_clusters)","68787bd4":"from sklearn.cluster import DBSCAN\n\nclustering = DBSCAN(eps=1, min_samples=3)\nclu_fit_pred=clustering.fit_predict(age)\n# print(clustering.labels_)\n\n##\nfrom sklearn.metrics import silhouette_score\nscore=silhouette_score(age,clu_fit_pred)\nprint('silhouette_score ',score)\n\n# ##\nplt.figure(figsize=(16,5))\nplt.scatter(age,clu_fit_pred)\nplt.title(str(clustering)+' silhouette_score= '+str(score),fontsize=20)\nplt.ylabel(\"Predict-age cluster\",fontsize=14)\nplt.xlabel('Actual-age',fontsize=14)\nplt.show()","daf823f7":"from sklearn.cluster import AgglomerativeClustering\n\nclustering = AgglomerativeClustering()\nclu_fit_pred=clustering.fit_predict(age)\n# print(clustering.labels_)\n\n##\nfrom sklearn.metrics import silhouette_score\nscore=silhouette_score(age,clu_fit_pred)\nprint('silhouette_score ',score)\n# ##\nplt.figure(figsize=(16,5))\nplt.scatter(age,clu_fit_pred)\nplt.title(str(clustering)+' silhouette_score= '+str(score),fontsize=20)\nplt.ylabel(\"Predict-age cluster\",fontsize=14)\nplt.xlabel('Actual-age',fontsize=14)\nplt.show()","25f4acd8":"from sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram\nimport numpy as np\nX = np.array([[1, 2], [1, 4], [1, 0],[4, 2], [4, 4], [4, 0]])\nclustering =  AgglomerativeClustering(distance_threshold=0, n_clusters=None).fit(X)\nprint(clustering)\nprint(clustering.labels_)\n\n\ndef plot_dendrogram(model, **kwargs):\n    # Create linkage matrix and then plot the dendrogram\n\n    # create the counts of samples under each node\n    counts = np.zeros(model.children_.shape[0])\n    n_samples = len(model.labels_)\n    for i, merge in enumerate(model.children_):\n        current_count = 0\n        for child_idx in merge:\n            if child_idx < n_samples:\n                current_count += 1  # leaf node\n            else:\n                current_count += counts[child_idx - n_samples]\n        counts[i] = current_count\n\n    linkage_matrix = np.column_stack([model.children_, model.distances_,\n                                      counts]).astype(float)\n\n    # Plot the corresponding dendrogram\n    dendrogram(linkage_matrix, **kwargs)\n    \nplt.title('Hierarchical Clustering Dendrogram',fontsize=24)\n# plot the top three levels of the dendrogram\nplot_dendrogram(clustering, truncate_mode='level', p=3)\nplt.xlabel(\"Number of points in node (or index of point if no parenthesis).\",fontsize=24)\nplt.show()","c21e3aff":"pipe_df=pd.read_csv('..\/input\/faulty-steel-plates\/faults.csv')\nimport scipy.cluster.hierarchy as sch\n#Lets create a dendrogram variable linkage is actually the algorithm #itself of hierarchical clustering and then in linkage we have to #specify on which data we apply and engage. This is X dataset\ndendrogram = sch.dendrogram(sch.linkage(pipe_df, method  = \"ward\"))\nplt.title('Dendrogram',fontsize=24)\nplt.xlabel('Number of points in node',fontsize=24)\nplt.ylabel('Euclidean distances',fontsize=24)\nplt.show()\n##\nfrom sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage ='ward')\ny_pred=hc.fit_predict(pipe_df)\nprint('y_pred ',y_pred)\n##\nfrom sklearn.metrics import silhouette_score\nscore=silhouette_score(pipe_df,y_pred)\nprint('silhouette_score ',score)\n","9c7f39e3":"from sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram\nimport numpy as np\nX = np.array([[1, 2], [1, 4], [1, 0],[4, 2], [4, 4], [4, 0]])\nclustering =  AgglomerativeClustering().fit(X)\nprint(clustering)\nprint(clustering.labels_)\n\n\nimport scipy.cluster.hierarchy as sch\n#Lets create a dendrogram variable linkage is actually the algorithm #itself of hierarchical clustering and then in linkage we have to #specify on which data we apply and engage. This is X dataset\ndendrogram = sch.dendrogram(sch.linkage(X, method  = \"ward\"))\nplt.title('Dendrogram',fontsize=24)\nplt.xlabel('Number of points in node',fontsize=24)\nplt.ylabel('Euclidean distances',fontsize=24)\nplt.show()","def397a8":"from sklearn.cluster import AgglomerativeClustering \nhc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage ='ward')\nX = np.array([[1, 2], [1, 4], [1, 0],[4, 2], [4, 4], [4, 0]])\ny_hc=hc.fit_predict(X)\nprint(y_hc)\nprint('X[y_hc==1, 0]',X[y_hc==1, 0])\nplt.scatter(X[y_hc==0, 0], X[y_hc==0, 1], s=100, c='red', label ='Cluster 1')\nplt.scatter(X[y_hc==1, 0], X[y_hc==1, 1], s=100, c='blue', label ='Cluster 2')\nplt.scatter(X[y_hc==2, 0], X[y_hc==2, 1], s=100, c='green', label ='Cluster 3')\nplt.scatter(X[y_hc==3, 0], X[y_hc==3, 1], s=100, c='cyan', label ='Cluster 4')\nplt.scatter(X[y_hc==4, 0], X[y_hc==4, 1], s=100, c='magenta', label ='Cluster 5')\nplt.title('Clusters of Customers (Hierarchical Clustering Model)',fontsize=24)\nplt.xlabel('X',fontsize=24)\nplt.ylabel('Y',fontsize=24)\nplt.show()","8a8f19d7":"import numpy as np\nfrom sklearn.neighbors import NearestNeighbors #Unsupervised learner for implementing neighbor searches.\nsamples = [[0, 0, 2], [1, 0, 0], [0, 0, 1]]\n##\nneigh = NearestNeighbors(n_neighbors=2, radius=0.4)\nneigh.fit(samples)\n##\nindex_pos=neigh.kneighbors([[0, 0, 1.3]], 2, return_distance=False) #Find the K-neighbors of a point.\nprint('kneighbors ',index_pos)\n\nnbrs_index = neigh.radius_neighbors([[0, 0, 1.3]], 0.4, return_distance=False) #Find the neighbors within a given radius of a point or points.\nprint('radius_neighbors ',nbrs_index)\nprint(np.asarray(nbrs_index[0][0]))","c762991c":"pipe_df=pd.read_csv('..\/input\/faulty-steel-plates\/faults.csv')\n\nimport numpy as np\nfrom sklearn.neighbors import NearestNeighbors #Unsupervised learner for implementing neighbor searches.\n##\nneigh = NearestNeighbors(n_neighbors=2, radius=0.4)\nneigh.fit(pipe_df)\n##\ndata=pipe_df.loc[0,:].values.reshape(1,-1) #array([[ 4.20000e+01,  5.00000e+01,  2.70900e+05,.....,0.00000e+00,  0.00000e+00]])\nKneighbors=3 \nindex_pos=neigh.kneighbors(data, Kneighbors, return_distance=False) #Find the K-neighbors of a point.\nprint(str(Kneighbors)+' kneighbors ',index_pos[0])\n##\nnbrs_index = neigh.radius_neighbors(data, 0.4, return_distance=False) #Find the neighbors within a given radius of a point or points.\nprint('radius_neighbors ',nbrs_index)\nprint('radius_neighbors single element ',np.asarray(nbrs_index[0][0]))","6c20e351":"from sklearn.cluster import SpectralClustering\n\nclustering = SpectralClustering(n_clusters=2,assign_labels='discretize',random_state=0)\nclu_fit_pred=clustering.fit_predict(age)\n# print(clustering.labels_)\n# ##\nplt.figure(figsize=(16,5))\nplt.scatter(age,clu_fit_pred)\nplt.title(str(clustering),fontsize=20)\nplt.ylabel(\"Predict-age cluster\",fontsize=14)\nplt.xlabel('Actual-age',fontsize=14)\nplt.show()","b20da0bb":"from sklearn.decomposition import PCA\nimport numpy as np\nimport pandas as pd\n\nX = np.array([[-1, -1,1,2,3], [-2, -1,1,21,3], [-3, -2,1,2,3], [1, 1,11,2,3], [2, 1,1,2,13], [3, 12,1,2,3]])\nprint('Original Shape ',X.shape,'\\n')\npca=PCA(n_components=2)\npca_fit_trans=pca.fit_transform(X)\nprint(pca_fit_trans)\nprint('\\npca.explained_variance_ratio_ \\n',pca.explained_variance_ratio_,'\\n','pca.singular_values_ \\n',pca.singular_values_)\nprint('\\n pca.components_ \\n',pca.components_,'\\n\\nComponents Size \\n',pca.components_[0].size)\nprint('\\n PCA Shape ',pca_fit_trans.shape)","6af4bc30":"car_df.iloc[:,1:-1].columns","a0a09277":"# https:\/\/pandas.pydata.org\/docs\/reference\/api\/pandas.to_numeric.html\ncar_df=pd.read_csv('..\/input\/autompg-dataset\/auto-mpg.csv')\ncar_df.head()\ncar_df['horsepower']=pd.to_numeric(car_df['horsepower'], errors='coerce')\ncar_df.fillna(axis=0, method='backfill', inplace=True)\n# print(car_df.isnull().sum())\n\nfrom sklearn.preprocessing import StandardScaler\n\nX=car_df.iloc[:,1:-1]\ny=car_df.iloc[:,0]\n##\nscaler = StandardScaler()\nprint(scaler.fit(X))\nscaled_data=scaler.transform(X)\n##\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprint('\\n',pca.fit(scaled_data))\npca_transform=pca.transform(scaled_data)\nprint('\\nBefore ',scaled_data.shape,' After ',pca_transform.shape,'\\n')\n\n##\nplt.scatter(pca_transform[:,0],pca_transform[:,1],c=y,cmap='plasma') #c -> array-like or list of colors or color, optional\nplt.xlabel('First Principal Component',fontsize=24)\nplt.ylabel('Second Principal Component',fontsize=24)","4af172d8":"!pip install plotly","8699f53f":"##\nloading=pca.components_.T\ndf_load=pd.DataFrame(loading,columns=['PC1','PC2'],index=X.columns)\ndisplay(df_load)\n##\nexplained_variance=pca.explained_variance_ratio_\nprint('\\npca.explained_variance_ratio_ \\n',explained_variance)\nexplained_variance=np.insert(explained_variance,0,0) # https:\/\/www.geeksforgeeks.org\/numpy-insert-python\/\ncumulative_variance=np.cumsum(np.round(explained_variance)) # https:\/\/www.geeksforgeeks.org\/numpy-cumsum-in-python\/\npc_df=pd.DataFrame(['','PC1','PC2'],columns=['PC'])\nex_df=pd.DataFrame(explained_variance,columns=['explained_variance'])\ncum_df=pd.DataFrame(cumulative_variance,columns=['cumulative_variance'])\ndf_exp=pd.concat([pc_df,ex_df,cum_df],axis=1)\ndisplay(df_exp)\n##\nimport plotly.express as px #https:\/\/www.geeksforgeeks.org\/python-plotly-tutorial\/\nfig=px.bar(df_exp,x='PC',y='explained_variance',text='explained_variance',width=800) #https:\/\/plotly.com\/python\/bar-charts\/\nfig.update_traces(texttemplate='%{text:.3f}',textposition='outside')\nfig.show()\n##","6b4b6b78":"# from sklearn.linear_model import LinearRegression\n# reg = LinearRegression().fit(pca_transform, y)\n# score=reg.score(pca_transform, y)\n\n# y_pred = reg.predict(pca_transform)\n# plt.plot(y, y_pred, color ='k')\n# plt.title('LinearRegression score= '+str(score),fontsize=14)\n# plt.xlabel('Y',fontsize=14)\n# plt.ylabel('Y_Pred',fontsize=14)\n# plt.show()","0528eff8":"from sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.datasets import make_multilabel_classification\nX, _ = make_multilabel_classification(random_state=0)\nprint(X.shape)\nlda = LatentDirichletAllocation(n_components=5,random_state=0)\nlda_fit_trans=lda.fit_transform(X)\nprint('\\n',lda.components_,'\\n',lda.components_[0].size)\nprint('\\n',lda_fit_trans.shape)","74ad4695":"from sklearn.model_selection import KFold\nX = [\"a\", \"a\", \"a\", \"b\", \"b\", \"c\", \"c\", \"c\", \"c\", \"c\"]\nk_fold = KFold(n_splits=5)\nfor train_indices, test_indices in k_fold.split(X):\n    print('Train: %s | test: %s' % (train_indices, test_indices))","9a239d3c":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\npipe_df=pd.read_csv('..\/input\/faulty-steel-plates\/faults.csv')\nX=pipe_df.iloc[:,:-1]\ny=pipe_df['Other_Faults']\n\n# Splitting the dataset into training and test set.  \nfrom sklearn.model_selection import train_test_split  \nX_train, X_test, y_train, y_test= train_test_split(X, y, test_size= 0.25, random_state=0)\n\ndef get_score(model,X_train,X_test,y_train,y_test):\n    model.fit(X_train,y_train)\n    return model.score(X_test,y_test)\n\nprint('RandomForestClassifier ',get_score(RandomForestClassifier(max_depth=2, random_state=0),X_train,X_test,y_train,y_test))\nprint('KNeighborsClassifier ',get_score(KNeighborsClassifier(n_neighbors=3),X_train,X_test,y_train,y_test))\nprint('SVC ',get_score(SVC(gamma='auto'),X_train,X_test,y_train,y_test))\nprint('SGDClassifier ',get_score(SGDClassifier(max_iter=1000, tol=0.001,loss='log'),X_train,X_test,y_train,y_test))\nprint('MLPClassifier ',get_score(MLPClassifier(random_state=1, max_iter=300),X_train,X_test,y_train,y_test))\nprint('DecisionTreeClassifier ',get_score(DecisionTreeClassifier(random_state=0),X_train,X_test,y_train,y_test))\nprint('GaussianNB ',get_score(GaussianNB(),X_train,X_test,y_train,y_test))\nprint('LogisticRegression ',get_score(LogisticRegression(random_state=0),X_train,X_test,y_train,y_test))","c47f904f":"from sklearn.model_selection import KFold\n\nkf = KFold(n_splits=2)\n\nget_n=kf.get_n_splits(X)\nprint(get_n)\n\nkf1=KFold(n_splits=2, random_state=None, shuffle=False)\nfor train_index, test_index in kf1.split(age):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n","5b5fd5b3":"import numpy as np\nfrom sklearn.model_selection import RepeatedKFold\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([0, 0, 1, 1])\nrkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\nfor train_index, test_index in rkf.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]","81b12697":"import numpy as np\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([0, 0, 1, 1])\nrskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,random_state=36851234)\n\nget_n=rskf.get_n_splits(X, y)\nprint(get_n)\n\nfor train_index, test_index in rskf.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]","0ed35d3b":"import numpy as np\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\n\ndef get_score(model,X_train,X_test,y_train,y_test):\n    model.fit(X_train,y_train)\n    return model.score(X_test,y_test)\n\npipe_df=pd.read_csv('..\/input\/faulty-steel-plates\/faults.csv')\nX=pipe_df.iloc[:,:-1]\ny=pipe_df['Other_Faults']\n##\nrskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,random_state=36851234)\n\nrf=[];kn=[];dt=[];\nfor train_index, test_index in rskf.split(X, y):\n#     print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    rf.append(get_score(RandomForestClassifier(max_depth=2, random_state=0),X_train,X_test,y_train,y_test))\n    kn.append(get_score(KNeighborsClassifier(n_neighbors=3),X_train,X_test,y_train,y_test))\n    dt.append(get_score(DecisionTreeClassifier(random_state=0),X_train,X_test,y_train,y_test))\n##\nprint('RandomForestClassifier ',rf)\nprint('KNeighborsClassifier ',kn)\nprint('DecisionTreeClassifier ',dt)\n##\nget_n=rskf.get_n_splits(X, y)\nprint(get_n)","38b442bb":"from sklearn.model_selection import cross_val_score\nprint('RandomForestClassifier cross_val_score ',cross_val_score(RandomForestClassifier(max_depth=2, random_state=0),X,y))\nprint('KNeighborsClassifier cross_val_score ',cross_val_score(KNeighborsClassifier(n_neighbors=3),X,y))\nprint('DecisionTreeClassifier cross_val_score ',cross_val_score(DecisionTreeClassifier(random_state=0),X,y))","3684a4a9":"# Test data is selected in a way that male and female are in correct ratio  in Kfold\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nX = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\ny = np.array([0, 0, 1, 1])\nskf = StratifiedKFold(n_splits=2, random_state=None, shuffle=False)\n\nget_n=skf.get_n_splits(X, y)\nprint(get_n)\n\nfor train_indices, test_indices in skf.split(X,y):\n    print('Train: %s | test: %s' % (train_indices, test_indices))\n    X_train, X_test, y_train, y_test=X[train_indices],X[test_indices],y[train_indices],y[test_indices]","8623e19b":"from sklearn.model_selection import TimeSeriesSplit\n\ntscv = TimeSeriesSplit(max_train_size=None, n_splits=2)\nfor train_index, test_index in tscv.split(age):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)","9bee4c5d":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","bf536eed":"# from sklearn.model_selection import cross_val_score\n# print('RandomForestClassifier cross_val_score ',cross_val_score(RandomForestClassifier(max_depth=2, random_state=0),X,y))\n# print('KNeighborsClassifier cross_val_score ',cross_val_score(KNeighborsClassifier(n_neighbors=3),X,y))\n# print('DecisionTreeClassifier cross_val_score ',cross_val_score(DecisionTreeClassifier(random_state=0),X,y))","03aca967":"from sklearn.metrics import accuracy_score\ny_pred = [0, 2, 1, 3]\ny_true = [0, 1, 2, 3]\ndisplay(accuracy_score(y_true, y_pred))\ndisplay(accuracy_score(y_true, y_pred, normalize=False)) #The best performance is 1 with normalize == True\n#If False, return the number of correctly classified samples.\n\n##\n# In the multilabel case with binary label indicators\nimport numpy as np\ndisplay(accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2))))","a986d7c1":"from sklearn.metrics import log_loss\nlog_loss([\"spam\", \"ham\", \"ham\", \"spam\"],[[.1, .9], [.9, .1], [.8, .2], [.35, .65]])","a8187065":"from sklearn.datasets import load_breast_cancer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\nX, y = load_breast_cancer(return_X_y=True)\nclf = LogisticRegression(solver=\"liblinear\", random_state=0).fit(X, y)\n\npred_proba=clf.predict_proba(X)[:, 1]\n# print(pred_proba)\nprint(roc_auc_score(y, pred_proba))\n\ndec_function=clf.decision_function(X)\n# print(dec_function)\n##decision_function(X)  > Predict confidence scores for samples.\nprint(roc_auc_score(y,dec_function))","05badfda":"# Model Selection","1cfc8e8d":"# 2.DBSCAN\n\nPerform DBSCAN clustering from vector array or distance matrix.\n\nDBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.\n\neps: specifies how close points should be to each other to be considered a part of a cluster. It means that if the distance between two points is lower or equal to this value (eps), these points are considered neighbors.\n\n\nminPoints: the minimum number of points to form a dense region. For example, if we set the minPoints parameter as 5, then we need at least 5 points to form a dense region.\n\n\neps: if the eps value chosen is too small, a large part of the data will not be clustered. It will be considered outliers because don\u2019t satisfy the number of points to create a dense region. On the other hand, if the value that was chosen is too high, clusters will merge and the majority of objects will be in the same cluster. The eps should be chosen based on the distance of the dataset (we can use a k-distance graph to find it), but in general small eps values are preferable.\n\nminPoints: As a general rule, a minimum minPoints can be derived from a number of dimensions (D) in the data set, as minPoints \u2265 D + 1. Larger values are usually better for data sets with noise and will form more significant clusters. The minimum value for the minPoints must be 3, but the larger the data set, the larger the minPoints value that should be chosen.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.DBSCAN.html","af0ccb5b":"# Clustering\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/clustering.html\n\nhttps:\/\/youtu.be\/lQt92mh0N8I","f4fa24cd":"# 1.KFold\n\nK-Folds cross-validator\n\nProvides train\/test indices to split data in train\/test sets. Split dataset into k consecutive folds (without shuffling by default).\n\nEach fold is then used once as a validation while the k - 1 remaining folds form the training set.\n\n\nThe k-fold cross-validation procedure divides a limited dataset into k non-overlapping folds. Each of the k folds is given an opportunity to be used as a held-back test set, whilst all other folds collectively are used as a training dataset.\n\n![](https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-forum-message-attachments\/o\/inbox%2F4367831%2Fa7eaffa417f9905be8f0e22af7326ac0%2Fk-fold.jpg?generation=1609935772242624&alt=media)\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.KFold.html","aec1ddaf":"# 1.sklearn.metrics.accuracy_score\n\nClassification Metric: Accuracy classification score\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html","3964bf72":"#  Classification","40e32cc3":"In SVM, the dimension of the hyperplane depends upon which one? **the number of features**\n\nIn SVM, if the number of input features is 2, then the hyperplane is a _____.  **line,circle**\n\nIn SVM, if the number of input features is 3, then the hyperplane is a ---. **line**\n\nIn SVM, what is a hyperplane? **features**\n \nDeleting the support vectors won\u2019t change the position of the **hyperplane** \n \nSVM can solve the data points that are not linearly separable\n \n\n![](https:\/\/i0.wp.com\/www.adeveloperdiary.com\/wp-content\/uploads\/2020\/04\/Support-Vector-Machine-for-Beginners-Hard-Margin-Classifier-adeveloperdiary.com-2-.png?resize=768%2C281)","8db44dca":"# 3.Agglomerative Hierarchical Clustering\n\nAgglomerative Clustering.\n\nAgglomerative Clustering: Also known as bottom-up approach or hierarchical agglomerative clustering (HAC). \nRecursively merges pair of clusters of sample data; uses linkage distance.\n\n\nThe **divisive hierarchical clustering**, also known as DIANA (DIvisive ANAlysis) is the inverse of agglomerative clustering .\n\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*ICdqpcL62G1q_kIlVszrrg.png)\n\n\nWe assign each point to an individual cluster in this technique. Suppose there are 4 data points. We will assign each of these points to a cluster and hence will have 4 clusters in the beginning:\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/05\/Screenshot-from-2019-05-15-13-11-28.png)\n\nThen, at each iteration, we merge the closest pair of clusters and repeat this step until only a single cluster is left:\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/05\/Screenshot-from-2019-05-15-13-31-06.png)\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.AgglomerativeClustering.html\n\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_agglomerative_dendrogram.html\n\nhttps:\/\/medium.com\/@sametgirgin\/hierarchical-clustering-model-in-5-steps-with-python-6c45087d4318\n\nhttps:\/\/youtu.be\/UxM6x3annr0","b75c205f":"# 4. Lasso\n-Linear Model trained with L1 prior as regularizer (aka the Lasso).\n\nTechnically the Lasso model is optimizing the same objective function as the Elastic Net with l1_ratio=1.0 (no L2 penalty).\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso","2263dad6":"# 3.1 NearestNeighbors\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.NearestNeighbors.html","946f3bec":"Increase the dimension to seperate Classification \n\n![](https:\/\/d3i71xaburhd42.cloudfront.net\/31cd3633e1d79a5ddd6c25ce68753725dafed4f6\/3-TableI-1.png)","b59e5e7c":"# 8. GradientBoostingRegressor\n\nGradient Boosting for regression.\n\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage a regression tree is fit on the negative gradient of the given loss function.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html\n\nhttps:\/\/machinelearningmastery.com\/gradient-boosting-machine-ensemble-in-python\/","62f8c70c":"# 9.GradientBoostingClassifier\n\nGradient Boosting for classification.\n\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html","313a0fbd":"# 3.SparseCoder\n\nSparse coding.\n\nFinds a sparse representation of data against a fixed, precomputed dictionary.\n\nEach row of the result is the solution to a sparse coding problem. The goal is to find a sparse array code such that:\n\nX ~= code * dictionary\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.SparseCoder.html","7d21936c":"**How does K-NN work?**\n\nThe K-NN working can be explained on the basis of the below algorithm:\n\n* Step-1: Select the number K of the neighbors\n* Step-2: Calculate the Euclidean distance of K number of neighbors\n* Step-3: Take the K nearest neighbors as per the calculated Euclidean distance.\n* Step-4: Among these k neighbors, count the number of the data points in each category.\n* Step-5: Assign the new data points to that category for which the number of the neighbor is maximum.\n* Step-6: Our model is ready.\n\n**How to select the value of K in the K-NN Algorithm?**\n\nBelow are some points to remember while selecting the value of K in the K-NN algorithm:\n\n* There is no particular way to determine the best value for \"K\", so we need to try some values to find the best out of them. The most preferred value for K is 5.\n* A very low value for K such as K=1 or K=2, can be noisy and lead to the effects of outliers in the model.\n* Large values for K are good, but it may find some difficulties.\n\n![](https:\/\/www.chilimath.com\/wp-content\/uploads\/2018\/12\/the_distance_formula.png)\n![](https:\/\/codespeedy.com\/wp-content\/uploads\/2020\/03\/manhattan.jpg)\\\n\nhttps:\/\/www.youtube.com\/watch?v=p3HbBlcXDTE","6654c349":"# 3.TimeSeriesSplit\n\nTime Series cross-validator\n\nProvides train\/test indices to split time series data samples that are observed at fixed time intervals, in train\/test sets. In each split, test indices must be higher than before, and thus shuffling in cross validator is inappropriate.\n\nThis cross-validation object is a variation of KFold. In the kth split, it returns first k folds as train set and the (k+1)th fold as test set.\n\nNote that unlike standard cross-validation methods, successive training sets are supersets of those that come before them.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.TimeSeriesSplit.html\n\nhttps:\/\/youtu.be\/7062skdX05Y","771ac87e":"# 1. StandardScaler \n-Standardize features by removing the mean and scaling to unit variance\n\n**What is StandardScaler?**\n\nStandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. ... StandardScaler makes the mean of the distribution 0. About 68% of the values will lie be between -1 and 1\n\n**What is the use of StandardScaler?**\n\nStandardScaler removes the mean and scales each feature\/variable to unit variance. This operation is performed feature-wise in an independent way. StandardScaler can be influenced by outliers (if they exist in the dataset) since it involves the estimation of the empirical mean and standard deviation of each feature.\n\n\n**What is StandardScaler in machine learning?**\n\nIn Machine Learning, StandardScaler is used to resize the distribution of values so that the mean of the observed values is 0 and the standard deviation is 1\n\n\n\n**What is the difference between StandardScaler and normalizer?**\n\nThe main difference is that Standard Scalar is applied on Columns, while Normalizer is applied on rows, So make sure you reshape your data before normalizing it. StandardScaler standardizes features by removing the mean and scaling to unit variance, Normalizer rescales each sample.\n\n\n![](https:\/\/i.stack.imgur.com\/SPq4w.png)\n\n\n![](https:\/\/i.stack.imgur.com\/Z7ATR.png)\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.StandardScaler.html\n\nhttps:\/\/www.youtube.com\/watch?v=ZddUwo4R5ug","e63e679f":"# Ensemble \n\nuses two types of methods:\n\n1. **Bagging**\u2013 It creates a different training subset from sample training data with replacement & the final output is based on majority voting. For example,  Random Forest.\n\n2. **Boosting**\u2013 It combines weak learners into strong learners by creating sequential models such that the final model has the highest accuracy. For example,  ADA BOOST, XG BOOST\n\n![](https:\/\/editor.analyticsvidhya.com\/uploads\/4661536426211ba43ea612c8e1a6a1ed45507.png)","ff5c18e9":"# Machine Learning Quiz 03: Support Vector Machine\n\nhttps:\/\/kawsar34.medium.com\/machine-learning-quiz-03-support-vector-machine-c40cc80279a5","78885ba2":"# Reference\n\nhttps:\/\/www.educative.io\/blog\/scikit-learn-tutorial-linear-regression\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2015\/11\/easy-methods-deal-categorical-variables-predictive-modeling\/\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/12\/cheatsheet-scikit-learn-caret-package-for-python-r-respectively\/\n","248dbc29":"# 7. Cross_val_score\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html","1a0da903":"# 2.1 SVC(Support Vector Classification)\n\n\nkernel{\u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019}, default=\u2019rbf\u2019\n\nSupport vector machines (SVMs) are a set of supervised learning methods used for classification, regression and outliers detection.\n\n![](https:\/\/www.researchgate.net\/publication\/304611323\/figure\/fig8\/AS:668377215406089@1536364954428\/Classification-of-data-by-support-vector-machine-SVM.png)  \n\n**Maximize the distance for margin (2\/||W||)will give better result**\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\n\nhttps:\/\/www.youtube.com\/watch?v=H9yACitf-KM\n\nhttps:\/\/www.youtube.com\/watch?v=Js3GLb1xPhc\n","c4b9321d":"# 4. train_test_split\n\nSplit arrays or matrices into random train and test subsets\n\nQuick utility that wraps input validation and next(ShuffleSplit().split(X, y)) and application to input data into a single call for splitting (and optionally subsampling) data in a oneliner.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.train_test_split.html","88918a27":"# 7.KNeighborsClassifier\n\nClassifier implementing the k-nearest neighbors vote.\n\nK-NN is a **non-parametric algorithm**, which means it does not make any assumption on underlying data.\n\nIt is also called a **lazy learner algorithm** because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.\n\n Regarding the Nearest Neighbors algorithms, if it is found that two neighbors, neighbor k+1 and k, have identical distances but different labels, the results will depend on the ordering of the training data.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\n\nhttps:\/\/www.kaggle.com\/cristianvlad\/knn-with-scikit-learn-optimizing-for-n-neighbors\n\nhttps:\/\/www.youtube.com\/watch?v=otolSnbanQk","05365337":"# 5. SGDRegressor\nLinear model fitted by minimizing a regularized empirical loss with SGD\n\nSGD stands for Stochastic Gradient Descent: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing strength schedule (aka learning rate).\n\nThe regularizer is a penalty added to the loss function that shrinks model parameters towards the zero vector using either the squared euclidean norm L2 or the absolute norm L1 or a combination of both (Elastic Net). If the parameter update crosses the 0.0 value because of the regularizer, the update is truncated to 0.0 to allow for learning sparse models and achieve online feature selection.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDRegressor.html","4d2a3b16":"# 1.DecisionTreeRegressor \n- A decision tree regressor\n\nExtrapolation is an estimation of a value based on extending a known sequence of values or facts beyond the area that is certainly known. In a general sense, to extrapolate is to infer something that is not explicitly stated from existing information.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeRegressor.html\n\nhttps:\/\/www.programcreek.com\/python\/example\/83782\/sklearn.tree.DecisionTreeRegressor\n\nhttps:\/\/towardsdatascience.com\/decisiontreeregressor-stop-using-for-future-projections-e27104537f6a\n\nhttps:\/\/towardsdatascience.com\/how-to-identify-the-right-independent-variables-for-machine-learning-supervised-algorithms-439986562d32\n\nRefer : DecisionTreeClassifier","a7e62d99":"# 2.1 RepeatedStratifiedKFold\n\nRepeats Stratified K-Fold n times with different randomization in each repetition.\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RepeatedStratifiedKFold.html#sklearn.model_selection.RepeatedStratifiedKFold","fa50c551":"# 4.OneHotEncoder \n-Encode categorical integer features using a one-hot a.k.a one-of-K scheme\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html\n\nhttps:\/\/youtu.be\/NYtwyvyvDEk\n","39f7898d":"# 2. Imputer \n-Imputation transformer for completing missing values\n\nclass sklearn.impute.SimpleImputer(*, missing_values=nan, strategy='mean', fill_value=None, verbose=0, copy=True, add_indicator=False)\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html\n\nhttps:\/\/www.youtube.com\/watch?v=CGdINc8xpn0","fb3b0126":"# 7. sklearn.metrics.mutual_info_score\n\nMutual Information between two clusterings.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.mutual_info_score.html","1570cc4b":"# 6.RandomizedSearchCV\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html","a4e37a9a":"# Metric\n\nhttps:\/\/www.ritchieng.com\/machine-learning-evaluate-classification-model\/","9235bbe5":"# II. Regression","02f64b0f":"# 6. ElasticNet \n-Linear regression with combined L1 and L2 priors as regularizer.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet","334cd34f":"# 5.SGDClassifier\n\nLinear classifiers (SVM, logistic regression, etc.) with SGD training.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.SGDClassifier.html","e78280fc":"# 5.PolynomialFeatures \n- Generate polynomial and interaction features\n\nGenerate a new feature matrix consisting of all polynomial combinations of the features with degree less than or equal to the specified degree. For example, if an input sample is two dimensional and of the form [a, b], the degree-2 polynomial features are [1, a, b, a^2, ab, b^2].\n\n![](https:\/\/machinelearningmastery.com\/wp-content\/uploads\/2020\/02\/Line-Plot-of-the-Degree-vs-the-Number-of-Input-Features-for-the-Polynomial-Feature-Transform.png)\n\nDimension Increase Columns also Increase\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.PolynomialFeatures.html\n\nhttps:\/\/machinelearningmastery.com\/polynomial-features-transforms-for-machine-learning\/","f54d08ba":"# 3.1 Ridge\n-Linear least squares with l2 regularization.\n\n\nThe coefficient of determination  is defined as  **(ridge_fit.score(X, y))**\n \nwhere  is the residual sum of squares ((y_true - y_pred)** 2).sum() and  is the total sum of squares ((y_true - y_true.mean()) ** 2).sum(). The best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse). A constant model that always predicts the expected value of y, disregarding the input features, would get a  score of 0.0.\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge\n\nhttps:\/\/youtu.be\/9lRv01HDU0s\n\nhttps:\/\/youtu.be\/VqKq78PVO9g","a491243e":"**What does Kmeans inertia mean?**\n\nInertia measures how well a dataset was clustered by K-Means. It is calculated by measuring the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster. A good model is one with low inertia AND a low number of clusters ( K ).\n\n![](https:\/\/editor.analyticsvidhya.com\/uploads\/62725cluster0.PNG)\n\n\n\nhttps:\/\/youtu.be\/PONM8A7Gwl4\n\nhttps:\/\/youtu.be\/vsWrXfO3wWw\n\nhttps:\/\/youtu.be\/XtE7hqFsYc4\n\nhttps:\/\/youtu.be\/_jg1UFoef1c","931f70aa":"# 2. DecisionTreeClassifier\n\nEntropy 0 to 1 - check split of tree by feature's is pure or not\n\nwhere 1 (50% of yes \/50% of No)(4 yes\/ 4 no ) is impure  \n\nwhere 0 (5 yes\/0 no)(0 yes\/5 no) is pure\n\nEntropy tell's only about single node to get all node till leave calculate Information Gain\n\nInformation Gain -  simple telling like  (Entropy of any node (example:root)) - \u03a3 (Entropy of it's subset nodes)\n\n![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/20200620180439\/Gini-Impurity-vs-Entropy.png)\n\n\n( Entropy OR Gini Impurity )both are for calculating *Information Gain* but in python we use Gini for effective computation\n\n\ngini = refers to the quality of the split, and is always a number between 0.0 and 0.5, where 0.0 would mean all of the samples got the same result, and 0.5 would mean that the split is done exactly in the middle.\n\ngini = 0.219 means that about 22% of the samples would go in one direction\n\nsamples = 4 means that there are 4  left in this branch \n\nvalue = [1, 3] means that of these 4 , 1 will get a \"NO\" and 3 will get a \"YES\".\n\nGini = 1 - (x\/n)2 - (y\/n)2\n\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/05\/rfc_vs_dt11.png)\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/tree.html\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\n\nhttps:\/\/www.w3schools.com\/python\/python_ml_decision_tree.asp\n\nhttps:\/\/youtu.be\/PHxYNGo8NcI\n\nhttps:\/\/youtu.be\/FuTRucXB9rA\n\nhttps:\/\/youtu.be\/5aIFgrrTqOw\n\nhttps:\/\/youtu.be\/5O8HvA9pMew\n\nhttps:\/\/dhirajkumarblog.medium.com\/top-5-advantages-and-disadvantages-of-decision-tree-algorithm-428ebd199d9a\n\nrefer :DecisionTreeRegressor","195d26ee":"# DimensionalityReduction","997b4f6c":"# 3. LinearRegression\n-Ordinary least squares LinearRegression\n\nclass sklearn.linear_model.LinearRegression(*, fit_intercept=True, normalize='deprecated', copy_X=True, n_jobs=None, positive=False)\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html\n\nhttps:\/\/youtu.be\/8jazNUpO3lQ\n\nhttps:\/\/youtu.be\/1-OGRohmH2s\n\n\nHow can I increase the accuracy of my Linear Regression model?(machine learning with python)\n\nhttps:\/\/stackoverflow.com\/questions\/47577168\/how-can-i-increase-the-accuracy-of-my-linear-regression-modelmachine-learning","795ab6cd":"# 2.sklearn.metrics.log_loss\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.log_loss.html","82ab8048":"# 9. MLPRegressor\n\nMulti-layer Perceptron regressor.\n\nThis model optimizes the squared error using LBFGS or stochastic gradient descent.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPRegressor.html","fcd1b14d":"# 3.sklearn.metrics.roc_auc_score\n\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n\nNote: this implementation can be used with binary, multiclass and multilabel classification, but some restrictions apply (see Parameters).\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.roc_auc_score.html","da7483f1":"# 3. LabelBinarizer \n- Binarize labels in a one-vs-all fashion\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelBinarizer.html\n\nhttps:\/\/www.youtube.com\/watch?v=63IN-qnI2YI","c5590611":"# 4.DictionaryLearning\n\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.DictionaryLearning.html\n","54b0c85a":"# 1.1 RepeatedKFold\n\nRepeated K-Fold cross validator.\n\nRepeats K-Fold n times with different randomization in each repetition.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RepeatedKFold.html","eb223e87":"# 6. GaussianNB\n\nGaussian Naive Bayes (GaussianNB).\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html","eec0dd22":"# 2. SVR\n-Epsilon-Support Vector Regression\n\nFor large datasets consider using LinearSVR or SGDRegressor instead, possibly after a Nystroem transformer.\n\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVR.html\n\nhttps:\/\/www.kaggle.com\/dskagglemt\/svm-svr-basics\n","42020bda":"# 5. GridSearchCV\n\nExhaustive search over specified parameter values for an estimator.\n\nImportant members are fit, predict.\n\nGridSearchCV implements a \u201cfit\u201d and a \u201cscore\u201d method. It also implements \u201cscore_samples\u201d, \u201cpredict\u201d, \u201cpredict_proba\u201d, \u201cdecision_function\u201d, \u201ctransform\u201d and \u201cinverse_transform\u201d if they are implemented in the estimator used.\n\nThe parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html","002437fd":"# 1. PCA\n\nPrincipal component analysis (PCA).\n\nLinear dimensionality reduction using Singular Value Decomposition of the data to project it to a lower dimensional space. The input data is centered but not scaled for each feature before applying the SVD.\n\n![](https:\/\/i.stack.imgur.com\/lNP2g.png)\n\n![](https:\/\/miro.medium.com\/max\/2000\/1*ba0XpZtJrgh7UpzWcIgZ1Q.jpeg)\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html\n\nhttps:\/\/www.geeksforgeeks.org\/implementing-pca-in-python-with-scikit-learn\/\n\nhttps:\/\/youtu.be\/OFyyWcw2cyM\n\n\nhttps:\/\/youtu.be\/oiusrJ0btwA","9e7a613d":"# < Pre-Processing >","3d3f3667":"The distance of the vectors from the hyperplane is called the margin \n\n![](https:\/\/miro.medium.com\/max\/1626\/1*kzdqdDUTwNsAkVZNLQAPvQ.png)","55ea33cb":"# 7. RandomForestRegressor\n\n-A random forest regressor\n\nA random forest regressor.\n\nA random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n\nmax_depth int, default=None\n\nThe maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than min_samples_split samples.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html\n\nrefer : RandomForestClassifier","9974d24c":"* n_neighbors: To define the required neighbors of the algorithm. Usually, it takes 5.\n* metric='minkowski': This is the default parameter and it decides the distance between the points.\n* p=2: It is equivalent to the standard Euclidean metric.\n* p=1 (minkowski)> L1 Norms > Manhattan Distance\n\n**What is difference between precision and accuracy?**\n\n\nAccuracy is the degree of closeness to true value. Precision is the degree to which an instrument or process will repeat the same value. In other words, accuracy is the degree of veracity while precision is the degree of reproducibility.\n\n**What is micro and macro average?**\n\nA macro-average will compute the metric independently for each class and then take the average hence treating all classes equally, whereas a micro-average will aggregate the contributions of all classes to compute the average metric.\n\n\n","cd506570":"\n**Hard Margin**\n\nIf the training data is linearly separable, we can select two parallel hyperplanes that separate the two classes of data, so that the distance between them is as large as possible.\n\n**Soft Margin**\n\nAs most of the real-world data are not fully linearly separable, we will allow some margin violation to occur which is called soft margin classification. It is better to have a large margin, even though some constraints are violated. Margin violation means choosing a hyperplane, which can allow some data points to stay in either incorrect side of hyperplane and between margin and correct side of the hyperplane.\n\nIn order to find the **maximal margin****, we need to find maximize the margin between the data points and the hyperplane.\n\n![](https:\/\/miro.medium.com\/max\/1172\/1*PiGj6vEyBhxbXfK4bzwwTg.png)","e7a76918":"# 4.LogisticRegression\n\n\npenalty{\u2018l1\u2019, \u2018l2\u2019, \u2018elasticnet\u2019, \u2018none\u2019}, default=\u2019l2\u2019\nSpecify the norm of the penalty:\n\n'none': no penalty is added;\n\n'l2': add a L2 penalty term and it is the default choice;\n\n'l1': add a L1 penalty term;\n\n'elasticnet': both L1 and L2 penalty terms are added.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html","393d5182":"# 2. LatentDirichletAllocation\n\nLatent Dirichlet Allocation with online variational Bayes algorithm.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.LatentDirichletAllocation.html","2e5f7b78":"# 3.SVC\n\nC-Support Vector Classification.\n\nThe implementation is based on libsvm. The fit time scales at least quadratically with the number of samples and may be impractical beyond tens of thousands of samples. For large datasets consider using LinearSVC or SGDClassifier instead, possibly after a Nystroem transformer.\n\nThe multiclass support is handled according to a one-vs-one scheme.\n\nFor details on the precise mathematical formulation of the provided kernel functions and how gamma, coef0 and degree affect each other, see the corresponding section in the narrative documentation: Kernel functions.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html","648204c3":"![](https:\/\/miro.medium.com\/max\/650\/1*OyYyr9qY-w8RkaRh2TKo0w.png)","198e19b9":"# 1.KMeans\n1.Convergence\n\nWe should take care of Outlier and Intialization of centroid (which randomly selected) may centroid is introduced to be a \"far away\" point  in K mean \n\ninit='k-means++' > smart way to Intialization of centroid\n\ninit{\u2018k-means++\u2019, \u2018random\u2019}, callable or array-like of shape (n_clusters, n_features), default=\u2019k-means++\u2019\n\nK-means++ is the algorithm which is used to overcome the drawback posed by the k-means algorithm.\n(likelihood of picking a point as centroid is corresponding to the distance from the closest, recently picked centroid.)\n\nK  > select by elbow method\n\nTake care of all point in one cluster are close together(distance small) and distance between two cluster is large\n\nAs it is unsupervised there is no target and we can measure it by silhoutte score \n\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/cluster\/plot_kmeans_silhouette_analysis.html\n\n\n![](https:\/\/www.unioviedo.es\/compnum\/labs\/new\/d1.png)\n\n\nSilhouette Coefficient \n\nSilhouette Coefficient or silhouette score is a metric used to calculate the goodness of a clustering technique. Its value ranges from -1 to 1. \n\n![](https:\/\/miro.medium.com\/max\/712\/1*cUcY9jSBHFMqCmX-fp8BvQ.jpeg)\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.KMeans.html","8e278c1d":"# 2.StratifiedKFold\n\nStratified K-Folds cross-validator.  for Dataset target (0\/1) Inbalance\n\nProvides train\/test indices to split data in train\/test sets.\n\nThis cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.\n\n**Male and Female Inbalance So we use StratifiedKFold than KFold**\n\n![](https:\/\/i0.wp.com\/dataaspirant.com\/wp-content\/uploads\/2020\/12\/8-Stratified-K-Fold-Cross-Validation.png?ssl=1)\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.StratifiedKFold.html\n\nhttps:\/\/youtu.be\/7062skdX05Y","d7ccdae2":"# 8.RandomForestClassifier\n\nA random forest classifier.\n\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The sub-sample size is controlled with the max_samples parameter if bootstrap=True (default), otherwise the whole dataset is used to build each tree.\n\n![](https:\/\/static.javatpoint.com\/tutorial\/machine-learning\/images\/random-forest-algorithm2.png)\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2021\/06\/understanding-random-forest\/\n\nhttps:\/\/www.kaggle.com\/prashant111\/random-forest-classifier-tutorial\/notebook#9.-Exploratory-data-analysis-\n\nhttps:\/\/youtu.be\/lZ8dUozUli4\n\nhttps:\/\/youtu.be\/nxFG5xdpDto\n\nhttps:\/\/www.youtube.com\/watch?v=ok2s1vV9XW0&t=4s\n\nhttps:\/\/www.kaggle.com\/faressayah\/decision-trees-random-forest-for-beginners\n\nrefer :RandomForestRegressor","fcc5d2d2":"# 1. MLPClassifier\n\nMulti-layer Perceptron classifier.\n\nThis model optimizes the log-loss function using LBFGS or stochastic gradient descent.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html","2ef1fcf2":"# 4.SpectralBiclustering\n\nmore generally when a measure of the center and spread of the cluster is not a suitable description of the complete cluster, such as when clusters are nested circles on the 2D plane.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.cluster.SpectralClustering.html"}}