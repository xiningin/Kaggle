{"cell_type":{"51d4ad1c":"code","01b6bb84":"code","df50ded6":"code","ede76bcc":"code","96646894":"code","47e76cd3":"code","5b7b3896":"code","588cd54e":"code","8036b7b3":"code","96f2bd7e":"code","5577bd20":"code","5a526b61":"code","ad1b880c":"code","4920408a":"code","86082ff8":"code","0ec4661e":"code","d5230bec":"code","81ab12a6":"code","f4e9218c":"code","adf2bcb3":"code","5bcaece2":"code","fd094ab8":"code","8d136104":"code","885d0f14":"code","b013b701":"code","5b34e89f":"code","69effc40":"code","97396228":"code","11e2a4cb":"code","fc82210d":"code","910b83bf":"code","2bbc85a0":"code","9d0fac9c":"code","7e69ce52":"code","f62400ea":"code","96e53d44":"code","7db9cbcd":"code","46873472":"markdown","56b772b1":"markdown","073aa20f":"markdown","1593b612":"markdown","485c785f":"markdown","caf1f7ae":"markdown","6698cdb5":"markdown","a04deed5":"markdown","9017686c":"markdown","3f3d08e6":"markdown","7bb1a1b1":"markdown","140ff225":"markdown","060ee7b7":"markdown","4426b7f7":"markdown","a0b58bd2":"markdown","cd63d572":"markdown","de2d489e":"markdown","d34643c4":"markdown","cba01994":"markdown","a7c1162e":"markdown","9f3f5391":"markdown","51bb1e0e":"markdown","978eba18":"markdown","401dbba5":"markdown","4022a1a4":"markdown","f42c6abf":"markdown","19e06d05":"markdown","cbc20c95":"markdown","8c84c1f4":"markdown","9d364bb3":"markdown","0cc72f18":"markdown"},"source":{"51d4ad1c":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom collections import Counter","01b6bb84":"# set a custom colormap\nfrom matplotlib.colors import LinearSegmentedColormap\nfrom matplotlib import pyplot as plt \ncolors = ['#fefbff', '#fbcbff', '#707bfb', 'darkblue']\ncolors2 = [\"#FBCBFF\",\"#EEC4FF\",\"#E2BCFE\",\"#D5B5FE\",\"#C8AEFE\",\"#BCA7FD\",\"#AF9FFD\",\n            \"#A398FC\",\"#9691FC\",\"#898AFC\",\"#7D82FB\",\"#707BFB\"]\ncmap = LinearSegmentedColormap.from_list('mycmap', colors)\ncmap2 = LinearSegmentedColormap.from_list('mycmap', \n                                          ['#fefbff', '#fbcbff', '#707bfb'])\nsns.set_palette(colors)","df50ded6":"data = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","ede76bcc":"data.head()","96646894":"# check data types and missing values\ndata.info()","47e76cd3":"# set max number of categories\n# to differentiate continuous and discrete values\n# max num of categories may differ for each dataset\nNUM_CATEGORIES = 10","5b7b3896":"# the func is used to describe columns\n# (num unique values and type - continuous or discrete)\n\ndef cont_discr_stats(df, num_categories):\n  '''\n  Accepts a pandas DataFrame and max num of categories (integer)\n  Prints num of unique values and  type of the column\n  Returns nothing\n\n  '''\n  for column in df.columns:\n    unique_val = df[column].nunique()\n    print(column, '\\n' + str(unique_val), 'unique values')\n    if unique_val <= num_categories:\n      print('Categories:', df[column].unique())\n    else:\n      print('Continuous data')\n    print('========='*3)","588cd54e":"# describe columns\ncont_discr_stats(data, NUM_CATEGORIES)","8036b7b3":"# the func is used to get names of columns\n# with continuous and discrete values\n\ndef categorical_continuos(df, num_categories):\n  '''\n  Accepts a pandas DataFrame and max num of categories (integer)\n  Returns columns (names) with continuous and discrete values respectively\n\n  '''\n  continuos_cols = []\n  discrete_cols = []\n  for column in df.columns:\n    unique_val = df[column].nunique()\n    if unique_val <= num_categories:\n      discrete_cols.append(column)\n    else:\n      continuos_cols.append(column)\n  return continuos_cols, discrete_cols","96f2bd7e":"# names of columns with continuous and\n# discrete (categorical) values\ncont_cols, cat_cols = categorical_continuos(data, NUM_CATEGORIES)","5577bd20":"# get stats on continuous values\ndata[cont_cols].describe() ","5a526b61":"from sklearn.decomposition import PCA\npca = PCA(n_components=3)\n\n# select columns with only continuous data\nx = data.loc[:, cont_cols]\n# fit and transform data\nres = pca.fit_transform(x)\ntotal_var = pca.explained_variance_ratio_.sum() * 100\n\nimport plotly.express as px\nfig = px.scatter_3d(res, x=0, y=1, z=2, color=data['DEATH_EVENT'], \n                    title=f'Total Explained Variance: {total_var:.2f}%',\n                    color_continuous_scale=['#707bfb', 'darkblue'], \n                    template='plotly_white')\n\nfig.show()","ad1b880c":"plt.figure(figsize=(8, 8))\n\n# loop through columns with continuous data\nfor i, column in enumerate(data[cont_cols]):\n  # add violin plots in two columns\n  ax = plt.subplot(4, 2, i+1)\n  sns.violinplot(x='DEATH_EVENT', y=column, data=data)\n  \nplt.tight_layout()\nplt.show()","4920408a":"sns.heatmap(data.corr(), cmap=cmap)\nplt.show()","86082ff8":"data.corr()","0ec4661e":"# count num of female and male\n# smokers and non-smokers respectively\n\nfemale = data[data['sex']==1]\nf_smoke = len(female[female['smoking']==1])\nf_nosmoke = len(female[female['smoking']==0])\n\nmale = data[data['sex']==0]\nm_smoke = len(male[male['smoking']==1])\nm_nosmoke = len(male[male['smoking']==0])","d5230bec":"from scipy.stats import chi2_contingency as ch \n\n# perform chi-square test, only pvalue is needed\npivot = np.array([[f_smoke, f_nosmoke], [m_smoke, m_nosmoke]])\n_, pvalue, _, _ = ch(pivot)\n\nprint('p-value: {:.4f}'.format(pvalue))\nif pvalue > 0.05:\n  print('Categorical values are independent.')\nelse:\n  print('Categorical values are dependent.')","81ab12a6":"data = data.loc[:, data.columns != 'sex']\ncont_cols, cat_cols = categorical_continuos(data, NUM_CATEGORIES)","f4e9218c":"data.head()","adf2bcb3":"# make correlation table, remove 'DEATH_EVENT' column and row\ncorr = data.corr()\ncorr = corr.loc[corr.index != 'DEATH_EVENT', corr.columns != 'DEATH_EVENT']\nindexes = corr.index.tolist()\n\n# loop through correlation table\n# find corr coeficients > 0.2\nfor column in corr:\n  for index in indexes:\n    if index != column:\n      if abs(corr.loc[index, column]) > 0.2:\n        print(column + ' - ' +  index + ' - {:.3f}'.format(corr.loc[index, column]))","5bcaece2":"corr = data.corr()\ndeath_event_corr = corr.loc[corr.index != 'DEATH_EVENT', corr.columns == 'DEATH_EVENT']\ndeath_event_corr['DEATH_EVENT'] = death_event_corr['DEATH_EVENT'].apply(lambda x: abs(x))\ndeath_event_corr.sort_values('DEATH_EVENT', ascending=False, inplace=True)\nindexes = death_event_corr.index.tolist()\n\nplt.figure()\nsns.barplot(x=death_event_corr['DEATH_EVENT'], \n            y=indexes, \n            palette=colors2).set_title('Most correlated to \\'DEATH_EVENT\\' features')\n\nplt.show()","fd094ab8":"plt.figure(figsize=(8, 8))\nsns.set_palette(['darkblue', '#707bfb'])\n\n# line plots for continuous columns\n# where x - 'age'\nfor i, column in enumerate(data[cont_cols]):\n  if column != 'age':\n    ax = plt.subplot(int((len(cont_cols)+1)\/2), 2, i)\n    sns.lineplot(x='age', y=column, hue='DEATH_EVENT', data=data, estimator=None)\n    \nplt.tight_layout()\nplt.show()","8d136104":"plt.figure(figsize=(8, 8))\nsns.set_palette(['#707bfb', 'blue'])\n\n# histograms for continuous columns\nfor i, column in enumerate(data[cont_cols]):\n  ax = plt.subplot(int((len(cont_cols)+1)\/2), 2, i+1)\n  sns.histplot(x=column, data=data, kde=True, hue='DEATH_EVENT')\n\nplt.tight_layout()\nplt.show()","885d0f14":"plt.figure(figsize=(8, 8))\nsns.set_palette(['#707bfb', 'blue'])\n\n# histograms for categorical data\nfor i, column in enumerate(data[cat_cols]):\n  ax = plt.subplot(int((len(cat_cols)+1)\/2), 2, i+1)\n  sns.histplot(x=column, data=data, hue='DEATH_EVENT', discrete=True, shrink=.8)\n  plt.grid(True)\n\nplt.tight_layout()\nplt.show()","b013b701":"# get stats on categorical data that correspond to \n# lethal or not lethal cases\n\n# divide dataframe respectively to 'DEATH_EVENT' values\nlethal = data[data['DEATH_EVENT']==1]\nlethal = lethal.loc[:, lethal.columns != 'DEATH_EVENT']\nnonlethal = data[data['DEATH_EVENT']==0]\nnonlethal = nonlethal.loc[:, nonlethal.columns != 'DEATH_EVENT']\n\n# helper func to print info on number of records \ndef print_info(df, string):\n  print(f'for {len(df)} {string} cases:')\n\n  # loop through each categorical column \n  # count num of records of each category and the percent\n  for col in cat_cols:\n    if col != 'DEATH_EVENT':\n      print(col)\n      count = Counter(df[col])\n      percent1 = round(count[0] \/ len(df) * 100, 1)\n      percent2 = round(count[1] \/ len(df) * 100, 1)\n      print('0 - {} ({}%), '.format(count[0], percent1) + '1 - {} ({}%), '.format(count[1], percent2))\n\n\nprint_info(lethal, 'lethal')\nprint('\\n')\nprint_info(nonlethal, 'not lethal')","5b34e89f":"plt.figure(figsize=(8, 8))\nsns.set_palette(['#fbcbff'])\n\n# boxplot for columns with continuous data\nfor i, column in enumerate(data[cont_cols]):\n  ax = plt.subplot(int((len(cont_cols)+1)\/2), 2, i+1)\n  sns.boxplot(x=column, data=data)\n\nplt.tight_layout()\nplt.show()","69effc40":"data_new = data.copy()","97396228":"# delete outliers based on boxplot, borders assessed visually\noutliers1 = data.copy()\noutliers1 = outliers1[outliers1['creatinine_phosphokinase'] <1000]\noutliers1 = outliers1[outliers1['serum_creatinine'] <2]\noutliers1 = outliers1[outliers1['platelets'] <450000]","11e2a4cb":"# delete outliers on the boxplot\ndef whiskers(column):\n  q1 = np.quantile(column, 0.25)\n  q3 = np.quantile(column, 0.75)\n  x1 = q1 - 1.5*(q3-q1)\n  x2 = q3 + 1.5*(q3-q1)\n  return x1, x2\n\noutliers2 = data.copy()\ncolumns = ['creatinine_phosphokinase', 'serum_creatinine', 'platelets', 'serum_sodium']\nfor col in columns:\n  x1, x2 = whiskers(outliers2[col])\n  outliers2 = outliers2[outliers2[col] <= x2]\n  outliers2 = outliers2[x1 <= outliers2[col]]","fc82210d":"# delete data that is more than 3 standart deviation away\noutliers3 = data.copy()\ncolumns = ['creatinine_phosphokinase', 'serum_creatinine', 'platelets', 'serum_sodium']\nfor col in columns:\n  x1 = np.quantile(outliers3[col], 0.02)\n  x2 = np.quantile(outliers3[col], 0.997)\n  outliers3 = outliers3[outliers3[col] <= x2]\n  outliers3 = outliers3[x1 <= outliers3[col]]","910b83bf":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectFromModel","2bbc85a0":"# names of classifiers\nclassifiers = ['Logistic R', \n               'Random Forest', \n               'Decision Tree', \n               'Multilayer Perceprton', \n               'Ada Boost',  \n               'SVC', \n               'K Nearest Neighbours',\n               ]\n\n# initialize classifiers\nmodels = [LogisticRegression(max_iter=1000),\n          RandomForestClassifier(),\n          DecisionTreeClassifier(),\n          MLPClassifier(max_iter=10000),\n          AdaBoostClassifier(),                      \n          LinearSVC(max_iter=10000),\n          KNeighborsClassifier(),\n          ]\n\n# names that indicate how initial data was changed\nchanged = ['original data', \n                'feature selection (3)', \n                'feature selection (5)',\n                'outliers1',\n                'outliers2',\n                'outliers3',\n                'outliers3 + feature selection (3)',\n                'outliers3 + feature selection (5)'\n                ]\n\n# each dataset corresponds to a string in _changed_ var\ndatas = [data_new, data_new, data_new, \n         outliers1, outliers2, outliers3, outliers3, outliers3]\n","9d0fac9c":"classifiers_table = []\nfeatures_table = []\ncross_accuracy = []\n\nfor i in range(len(models)):\n  for j, df in enumerate(datas):\n\n    x = df.loc[:, df.columns != 'DEATH_EVENT'].copy()\n    y = df['DEATH_EVENT'].copy()\n\n    columns_to_normalize, _ = categorical_continuos(x, 3) \n\n    features = x[columns_to_normalize]\n    scaler = StandardScaler().fit(features.values)\n    features = scaler.transform(features.values)\n    x[columns_to_normalize] = features\n\n    if 'feature selection' in changed[j]:\n\n      clf = ExtraTreesClassifier(n_estimators=100)\n      clf = clf.fit(x, y)\n\n      if '(3)' in changed[j]:\n        max_features = 3\n      elif '(5)' in changed[j]:\n        max_features = 5\n      else:\n        max_features = None\n\n      selecter = SelectFromModel(clf, prefit=True, max_features=max_features)\n      x = selecter.transform(x)\n\n    scores_acc = cross_val_score(models[i], x, y, cv=10, scoring='accuracy')\n\n    cross_accuracy.append(scores_acc.mean())\n    \n    classifiers_table.append(classifiers[i])\n    features_table.append(changed[j])\n","7e69ce52":"# create table with mean accuracy per folds\n# for each combination\ndf = pd.DataFrame ({\n    'Classifier': classifiers_table,\n    'Feature selection': features_table,\n    'Accuracy on 10-folds CV': cross_accuracy\n})\n\ndf.style.background_gradient(cmap=cmap2)","f62400ea":"from sklearn.model_selection import KFold\n\n# import and initialize early stopping\nfrom tensorflow.keras.callbacks import EarlyStopping\nstop = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=5)\n\n# import sequential model and layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, InputLayer","96e53d44":"x = outliers3.loc[:, outliers3.columns != 'DEATH_EVENT'].values\ny = outliers3['DEATH_EVENT'].values","7db9cbcd":"# define the k-fold cross validator\nkfold = KFold(n_splits=10, shuffle=True)\nacc_per_fold = []\nn_features = x.shape[1]\n\n# k-fold cross validation model evaluation\nfor train, test in kfold.split(x, y):\n\n  # model architecture\n  seq = Sequential()\n  seq.add(InputLayer(input_shape=(n_features,)))\n  seq.add(Dense(64, activation='relu'))\n  seq.add(Dense(6, activation='relu'))\n  seq.add(Dense(2, activation='softmax'))\n\n  # compile the model\n  seq.compile(optimizer='adam', \n              loss='sparse_categorical_crossentropy', \n              metrics=['accuracy'])\n\n  # fit data to model\n  history = seq.fit(x[train], y[train],\n              batch_size=32,\n              epochs=20,\n              verbose=0)\n\n  # add accuracy per each fold\n  scores = seq.evaluate(x[test], y[test], verbose=0)\n  acc_per_fold.append(scores[1] * 100)\n\nprint(f'Average accuracy: {np.mean(acc_per_fold):.2f} \\\n(+- {np.std(acc_per_fold):.2f})')","46873472":"So, there is no need to fill in the missing values or perform label encoding for categorical data.","56b772b1":"### Violin plot","073aa20f":"Distributions differ with skew and modality depending on 'DEATH_EVENT' values.","1593b612":"According to the violin plot, the distribution of values that correspond to letal and nonletal cases, mostly differ for the columns 'time', 'ejection fraction' and 'serum creatinine'. This may be a hint for futher exploration.\nIndeed, if we take a look at the correlation matrix, 'DEATH_EVENT' has biggest correlation scores with the aforementioned columns.","485c785f":"# Exproratory data analysis","caf1f7ae":"Lineplot shows that there are no crucial patterns for the features that may change with age, even thougth 'time' is slightly smaller for lethal cases.","6698cdb5":"So it is suggested to delete 'sex' column as it has lower correlation with 'DEATH_EVENT' to avoid autocorrelation.","a04deed5":"Features with biggest num of outliers are 'creatinine_phosphokinase', 'serum_creatinine', 'platelets'. \nOutliers may decrease the accuracy of the classifier.","9017686c":"As the dataset (number of records) is not quite big, it is better to use k-fold cross-validation to evaluate the model.","3f3d08e6":"Reduce dimensionality to 3D usind PCA and plot results.","7bb1a1b1":"Let H0: categorical values 'sex' and 'smoking' are independent,\n\nH1: categorical values 'sex' and 'smoking' are dependent.","140ff225":"### Hypothesis testing","060ee7b7":"### Lineplot","4426b7f7":"On this step different datasets are generated by deleting outliers and selecting features.","a0b58bd2":"Sorted correlation coefficients between features and labels are:","cd63d572":"The other correlated features are:","de2d489e":"### Histograms","d34643c4":"Both time and age are important features so it is suggested not to delete them yet.","cba01994":"As we can see, continuous values have different standart deviations (from 1.03 to 97804.24), so the data should be normalized. ","a7c1162e":"There is unequal number of records of different categories and that may decrease classifier's perfomance. ","9f3f5391":"Also, it is quite easy to notice that 'sex' is highly correlated with 'smoking'.\n\nTo check if discrete values are independent chi-square test is used.\n\n ","51bb1e0e":"### DL model","978eba18":"Num or records of each sub-category is proportional to the values of column with labels ('DEATH_EVENT').","401dbba5":"### Correlation matrix","4022a1a4":"Clusters are not easy to distinguish, that may be caused by irrelevant features in the dataset.","f42c6abf":"# Data Engineering","19e06d05":"Model`s perfomance can be influenced by such factors as:\n- features present in the train data\n- outliers\n- autocorrelation\n\nSo data exploration and visualization steps help to better explore the peculiarities of the dataset.","cbc20c95":"## Data visualization","8c84c1f4":"### Boxplot","9d364bb3":"# Classical ML classifiers","0cc72f18":"Best accuracy for 10-fold cross-validation was achieved with logistic regression and deleting outliers that are more than 3 standart deviations away."}}