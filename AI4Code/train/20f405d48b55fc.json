{"cell_type":{"d8eea74b":"code","e00743fb":"code","67dae3ff":"code","fa735d6a":"code","a5efabd3":"code","f8ce2e81":"code","7977bc05":"code","b93299c6":"code","c6f2acdf":"code","2f1e4743":"code","6cf12e96":"code","12319eaa":"code","f05128e5":"code","f71cf219":"code","1b6411c8":"code","1cf6cc7f":"code","6229d380":"code","13c40980":"code","5de86102":"code","297b341d":"code","b654ce93":"code","a766d45a":"code","162a376a":"code","cd7f93bf":"code","93976305":"code","7e19ed3c":"code","962939b1":"code","93272ebe":"code","8f35cca5":"code","027b0d9f":"code","ca51c661":"code","9ca47d92":"code","ae5f810f":"code","7f56f969":"code","c8cb5a1c":"markdown","0d19020d":"markdown","fd5f71ec":"markdown","082e7638":"markdown","85d5513d":"markdown","220283f5":"markdown","ddb68008":"markdown","cd65b4c6":"markdown","73164290":"markdown","d3d15b03":"markdown","68ea71c9":"markdown","07d30ca8":"markdown","d0987853":"markdown","a16c87c8":"markdown","be95153f":"markdown","11686326":"markdown","12bd8829":"markdown","abf31278":"markdown","c2a34a36":"markdown","82089f0f":"markdown","6ed6f34b":"markdown","8db30f72":"markdown","3324126d":"markdown","b8c58d72":"markdown","906a7019":"markdown","1552246d":"markdown","7342a7c3":"markdown","f7208cb9":"markdown","7ab2c38c":"markdown"},"source":{"d8eea74b":"## Import Libraries\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom xgboost import XGBRegressor\nimport sklearn.metrics as metrics\nimport math\nfrom scipy.stats import norm, skew\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","e00743fb":"house_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhouse_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n","67dae3ff":"house_train.shape , house_test.shape ","fa735d6a":"house_train.head()","a5efabd3":"house_test.head()","f8ce2e81":"house_train.info()","7977bc05":"missing_values = house_train.isnull().sum()\n\ntotal_cells = np.product(house_train.shape)\ntotal_missing = missing_values.sum()\npercen_of_missing_value = (total_missing \/ total_cells) * 100\nprint('The percentage of missing values is: ',percen_of_missing_value)","b93299c6":"print(house_train['SalePrice'].describe())\n","c6f2acdf":" sns.distplot(house_train['SalePrice'])\n","2f1e4743":"print(\"Skewness: %f\" % house_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % house_train['SalePrice'].kurt())\n","6cf12e96":"house_train['SalePrice'] = np.log1p(house_train['SalePrice'])\nsns.distplot(house_train['SalePrice'], fit=norm);\n\n","12319eaa":"corr_cols = house_train.corr() # Here we find correlation between features.\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corr_cols, vmax=1, square=True);\n","f05128e5":"corr = house_train.corr()\nhighest_corr_features = corr.index[abs(corr[\"SalePrice\"])>0.6] # Why\nplt.figure(figsize=(10,10))\ng = sns.heatmap(house_train[highest_corr_features].corr(),annot=True,cmap=\"RdYlGn\")\n","f71cf219":"corr[\"SalePrice\"].sort_values(ascending=False)\n","1b6411c8":"features = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(house_train[features])\n","1cf6cc7f":"y_train = house_train['SalePrice']\ntest_id = house_test['Id']\nall_data = pd.concat([house_train, house_test], axis=0, sort=False)\nall_data = all_data.drop(['Id', 'SalePrice'], axis=1) # Why\nall_data.shape","6229d380":"Total = all_data.isnull().sum().sort_values(ascending=False)\npercent = (all_data.isnull().sum() \/ all_data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([Total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","13c40980":"all_data.drop((missing_data[missing_data['Total'] > 5]).index, axis=1, inplace=True)\nprint(all_data.isnull().sum().max())\n","5de86102":"total = all_data.isnull().sum().sort_values(ascending=False)\ntotal.head(19)","297b341d":"# filling the numeric data\nnumeric_missed = ['BsmtFinSF1',\n                  'BsmtFinSF2',\n                  'BsmtUnfSF',\n                  'TotalBsmtSF',\n                  'BsmtFullBath',\n                  'BsmtHalfBath',\n                  'GarageArea',\n                  'GarageCars']\n\nfor feature in numeric_missed:\n    all_data[feature] = all_data[feature].fillna(0)\n","b654ce93":"#filling categorical data\ncategorical_missed = ['Exterior1st',\n                  'Exterior2nd',\n                  'SaleType',\n                  'MSZoning',\n                   'Electrical',\n                     'KitchenQual']\n\nfor feature in categorical_missed:\n    all_data[feature] = all_data[feature].fillna(all_data[feature].mode()[0])\n","a766d45a":"#Fill in the remaining missing values with the values that are most common for this feature.\n\nall_data['Functional'] = all_data['Functional'].fillna('Typ')\n","162a376a":"all_data.drop(['Utilities'], axis=1, inplace=True)\n","cd7f93bf":"all_data.isnull().sum().max() #just checking that there's no missing data missing...\n","93976305":"\nnumeric_feats = all_data.dtypes[all_data.dtypes != 'object'].index\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nhigh_skew = skewed_feats[abs(skewed_feats) > 0.5]\nhigh_skew\n","7e19ed3c":"for feature in high_skew.index:\n    all_data[feature] = np.log1p(all_data[feature])\n","962939b1":"all_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']\n\n","93272ebe":"all_data = pd.get_dummies(all_data)\nall_data.head()","8f35cca5":"x_train =all_data[:len(y_train)]\nx_test = all_data[len(y_train):]\n","027b0d9f":"x_test.shape , x_train.shape","ca51c661":"from sklearn.metrics import make_scorer\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\nscorer = make_scorer(mean_squared_error,greater_is_better = False)\ndef rmse_CV_train(model):\n    kf = KFold(5,shuffle=True,random_state=42).get_n_splits(x_train.values)\n    rmse = np.sqrt(-cross_val_score(model, x_train, y_train,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)\ndef rmse_CV_test(model):\n    kf = KFold(5,shuffle=True,random_state=42).get_n_splits(train.values)\n    rmse = np.sqrt(-cross_val_score(model, x_test, y_test,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)\n","9ca47d92":"import xgboost as XGB\n\nthe_model = XGB.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, random_state =7, nthread = -1)\nthe_model.fit(x_train, y_train)\n","ae5f810f":"y_predict = np.floor(np.expm1(the_model.predict(x_test)))\ny_predict\n","7f56f969":"sub = pd.DataFrame()\nsub['Id'] = test_id\nsub['SalePrice'] = y_predict\nsub.to_csv('housesub.csv',index=False)","c8cb5a1c":"References:\nhttps:\/\/www.kaggle.com\/adamml\/how-to-be-in-top-10-for-beginner\n- I I sought help some code from this notebook.","0d19020d":"# 2-Loading the data","fd5f71ec":"#### Fix The Skewness in the other features\n","082e7638":"let's check if we have another missing values.","85d5513d":"# 1-Importing the libraries","220283f5":"Let's show the features and the number of Missing values","ddb68008":"Now we fixed it.","cd65b4c6":"Now, We explored the data and know the important features.","73164290":"#### ok let's focus on the features have highest correlation.","d3d15b03":"Ok, now as you see the correlation between features.. The colours show to us the strong and weak correlation.\nBut what we really need? we need the highest correlation between features and SalesPrice, so let's do it.","68ea71c9":"#### Before looking for Missing data: \nWe can concatenate train and test datasets, preprocess, and then divide them again. I think it will be easy for us.","07d30ca8":"## preface\nHello mates, it's my first kernel in Kaggle, i'm going to play with House Pricing with Advanced Regression Analysis and try using the correct model - after understanding the data- to achieve the right predict.\n- There may be errors in English so i told you it's my fisrt kernel \ud83d\ude01.","d0987853":"As we see, we have a positive sekew, we must fix it.","a16c87c8":"#### We cleaned the data very well, and now let's separate the data to its origin (train, test)","be95153f":"## 4. Looking for Missing Data ","11686326":"### 2. Let's continue exploring the target\n- Here we'll focus on the target columns, it's 'SalePrice'","12bd8829":"#### Let's add a new features","abf31278":"## 4. Feature Engineering","c2a34a36":" # 3-Exploring the data and different featrues to understand the data","82089f0f":"Well, if we look at these features that have many missing values, we will note that they are not important features, none of them has (correlation > 0.5), so if we delete them we will not miss the data.","6ed6f34b":"# Notes From a heatmap.\n- 'OverallQual' is the highest related feature with 'SalesPrice'.\n- 'GrLivArea' has a rate 0.7.\n- 'GarageCars','GarageArea' have a rate between 0.65-0.68, and they are also close and related.\n","8db30f72":"## 5. Converting the categorical to numerical.","3324126d":"### Here, we gonna know which features are important and have a statistical effect with 'SalesPrice'.\n- in normal you should ask an expert in this field, and he'll tell you the important features, but here we gonna know that by ourselves, let's do it.","b8c58d72":"As we see before, the percentage of the missing values is 5.889565364451209.\nOk, let's see the important features to get rid of the rest.","906a7019":"## Our Methdology\n1. Importing the libraries.\n2. Loading data\n3. Exploring the data and different featrues to understand the data\n4. Looking for missing values\n5. Converting categorical to numerical\n6. Using different Regression models to see the best algorithm.","1552246d":"OK, we'll deal with the missing data later on.\u270c","7342a7c3":"There are a lot of missing data as we see, let's see the percentage of them.","f7208cb9":"### Now, Filling the missing Data","7ab2c38c":"## 5. Apply ML Model"}}