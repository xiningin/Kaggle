{"cell_type":{"e8489746":"code","66604d11":"code","f7dc3450":"code","e2941b93":"code","960877bd":"code","a0a8719a":"code","e5eeff0e":"code","1beb5f0f":"code","8e8f9bd6":"code","dad29c5d":"code","56278301":"code","afeaccb5":"code","1fe77edd":"code","25bcfd0c":"code","a148bd8b":"code","dc72d500":"code","679e640a":"code","5d91411b":"code","c85759be":"code","edee0d6f":"code","7d7694b8":"code","1733d6d2":"code","d92364ac":"code","08148edb":"code","ec22ecb3":"code","48ca5fbf":"code","5254afe7":"code","896564ae":"markdown","bab5cd21":"markdown","bb548b82":"markdown","cb91ba5d":"markdown","fe465eb8":"markdown","fdd05da4":"markdown","cae88ae0":"markdown","e55bc639":"markdown","e3411720":"markdown","618a513b":"markdown","349d7c4f":"markdown","6894cee6":"markdown","5bc6ca46":"markdown","e44a9d30":"markdown","7b3b1888":"markdown","fa85048b":"markdown","1bfe9a03":"markdown","94a1e5d0":"markdown","d38a95b4":"markdown","16d740dc":"markdown","a6e10bea":"markdown","0b9bb3e7":"markdown","672251c0":"markdown","c137e208":"markdown","c684db0f":"markdown","eb50dbfc":"markdown","72c95d7a":"markdown","0c645bfd":"markdown","850cb675":"markdown","e86ed20e":"markdown","ce31e93a":"markdown","ae3cd149":"markdown","77a57638":"markdown","7e54be46":"markdown","487a4e1e":"markdown","d7bdbf2d":"markdown","922fb36d":"markdown","c0784745":"markdown","4c825c8b":"markdown","6213fb9b":"markdown","52be03a0":"markdown","bbd542dd":"markdown","e121dd9c":"markdown","1cd68ea6":"markdown","6a0187ca":"markdown","fcf34050":"markdown","70108edd":"markdown"},"source":{"e8489746":"%%writefile random_forest_random.py\nimport random\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\n\nactions =  np.empty((0,0), dtype = int)\nobservations =  np.empty((0,0), dtype = int)\ntotal_reward = 0\n\ndef random_forest_random(observation, configuration):\n    global actions, observations, total_reward\n    \n    if observation.step == 0:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        return action\n    \n    if observation.step == 1:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        observations = np.append(observations , [observation.lastOpponentAction])\n        # Keep track of score\n        winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n        if winner == 1:\n            total_reward = total_reward + 1\n        elif winner == 2:\n            total_reward = total_reward - 1        \n        return action\n\n    # Get Observation to make the tables (actions & obervations) even.\n    observations = np.append(observations , [observation.lastOpponentAction])\n    \n    # Prepare Data for training\n    # :-1 as we dont have feedback yet.\n    X_train = np.vstack((actions[:-1], observations[:-1])).T\n    \n    # Create Y by rolling observations to bring future a step earlier \n    shifted_observations = np.roll(observations, -1)\n    \n    # trim rolled & last element from rolled observations\n    y_train = shifted_observations[:-1].T\n    \n    # Set the history period. Long chains here will need a lot of time\n    if len(X_train) > 25:\n        random_window_size = 10 + random.randint(0,10)\n        X_train = X_train[-random_window_size:]\n        y_train = y_train[-random_window_size:]\n   \n    # Train a classifier model\n    model = xgb1 = XGBClassifier(\n learning_rate =0.01,\n n_estimators=25,\n nthread=4)\n    model.fit(X_train, y_train)\n\n    # Predict\n    X_test = np.empty((0,0), dtype = int)\n    X_test = np.append(X_test, [int(actions[-1]), observation.lastOpponentAction])\n    prediction = model.predict(X_test.reshape(1, -1))\n\n    # Keep track of score\n    winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n    if winner == 1:\n        total_reward = total_reward + 1\n    elif winner == 2:\n        total_reward = total_reward - 1\n   \n    # Prepare action\n    action = int((prediction + 1) % 3)\n    \n    # If losing a bit then change strategy and break the patterns by playing a bit random\n    if total_reward < -2:\n        win_tie = random.randint(0,1)\n        action = int((prediction + win_tie) % 3)\n\n    # Update actions\n    actions = np.append(actions , [action])\n\n    # Action \n    return action ","66604d11":"%%writefile hit_the_last_own_action.py\n\nmy_last_action = 0\n\ndef hit_the_last_own_action(observation, configuration):\n    global my_last_action\n    my_last_action = (my_last_action + 1) % 3\n    \n    return my_last_action","f7dc3450":"%%writefile rock.py\n\ndef rock(observation, configuration):\n    return 0","e2941b93":"%%writefile paper.py\n\ndef paper(observation, configuration):\n    return 1\n","960877bd":"%%writefile scissors.py\n\ndef scissors(observation, configuration):\n    return 2","a0a8719a":"%%writefile copy_opponent.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\ndef copy_opponent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return random.randrange(0, configuration.signs)","e5eeff0e":"%%writefile reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action","1beb5f0f":"%%writefile counter_reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_counter_action = None\n\n\ndef counter_reactionary(observation, configuration):\n    global last_counter_action\n    if observation.step == 0:\n        last_counter_action = random.randrange(0, configuration.signs)\n    elif get_score(last_counter_action, observation.lastOpponentAction) == 1:\n        last_counter_action = (last_counter_action + 2) % configuration.signs\n    else:\n        last_counter_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_counter_action","8e8f9bd6":"%%writefile statistical.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\naction_histogram = {}\n\ndef statistical(observation, configuration):\n    global action_histogram\n    if observation.step == 0:\n        action_histogram = {}\n        return\n    action = observation.lastOpponentAction\n    if action not in action_histogram:\n        action_histogram[action] = 0\n    action_histogram[action] += 1\n    mode_action = None\n    mode_action_count = None\n    for k, v in action_histogram.items():\n        if mode_action_count is None or v > mode_action_count:\n            mode_action = k\n            mode_action_count = v\n            continue\n\n    return (mode_action + 1) % configuration.signs","dad29c5d":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium(observation, configuration):\n    return random.randint(0, 2)","56278301":"%%writefile markov_agent.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores\/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","afeaccb5":"%%writefile memory_patterns.py\n\nimport random\n\n# how many steps in a row are in the pattern (multiplied by two)\nmemory_length = 6\n# current memory of the agent\ncurrent_memory = []\n# list of memory patterns\nmemory_patterns = []\n\ndef find_pattern(memory):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # if it's not first step, add opponent's last action to agent's current memory\n    if obs[\"step\"] > 0:\n        current_memory.append(obs[\"lastOpponentAction\"])\n    # if length of current memory is bigger than necessary for a new memory pattern\n    if len(current_memory) > memory_length:\n        # get momory of the previous step\n        previous_step_memory = current_memory[:memory_length]\n        previous_pattern = find_pattern(previous_step_memory)\n        if previous_pattern == None:\n            previous_pattern = {\n                \"actions\": previous_step_memory.copy(),\n                \"opp_next_actions\": [\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            memory_patterns.append(previous_pattern)\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1\n        # delete first two elements in current memory (actions of the oldest step in current memory)\n        del current_memory[:2]\n    my_action = random.randint(0, 2)\n    pattern = find_pattern(current_memory)\n    if pattern != None:\n        my_action_amount = 0\n        for action in pattern[\"opp_next_actions\"]:\n            # if this opponent's action occurred more times than currently chosen action\n            # or, if it occured the same amount of times, choose action randomly among them\n            if (action[\"amount\"] > my_action_amount or\n                    (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                my_action_amount = action[\"amount\"]\n                my_action = action[\"response\"]\n    current_memory.append(my_action)\n    return my_action","1fe77edd":"%%writefile multi_armed_bandit.py\n\n\nimport pandas as pd\nimport numpy as np\nimport json\n\n\n# base class for all agents, random agent\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n    \n# agent that returns (previousCompetitorStep + shift) % 3\nclass mirror_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['competitorStep'] + self.shift) % 3\n    \n    \n# agent that returns (previousPlayerStep + shift) % 3\nclass self_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['step'] + self.shift) % 3    \n\n\n# agent that beats the most popular step of competitor\nclass popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['competitorStep'] for x in history])\n        return (int(np.argmax(counts)) + 1) % 3\n\n    \n# agent that beats the agent that beats the most popular step of competitor\nclass anti_popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['step'] for x in history])\n        return (int(np.argmax(counts)) + 2) % 3\n    \n    \n# simple transition matrix: previous step -> next step\nclass transition_matrix(agent):\n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3)) + self.init_value\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type]), int(history[i+1][self.step_type])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type])]\/matrix[int(history[-1][self.step_type])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n    \n\n# similar to the transition matrix but rely on both previous steps\nclass transition_tensor(agent):\n    \n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type1 = 'step' \n            self.step_type2 = 'competitorStep'\n        else:\n            self.step_type2 = 'step' \n            self.step_type1 = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3, 3)) + 0.1\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type1]), int(history[i][self.step_type2]), int(history[i+1][self.step_type1])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])]\/matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n\n    \nagents = {\n    'mirror_0': mirror_shift(0),\n    'mirror_1': mirror_shift(1),  \n    'mirror_2': mirror_shift(2),\n    'self_0': self_shift(0),\n    'self_1': self_shift(1),  \n    'self_2': self_shift(2),\n    'popular_beater': popular_beater(),\n    'anti_popular_beater': anti_popular_beater(),\n    'random_transitison_matrix': transition_matrix(False, False),\n    'determenistic_transitison_matrix': transition_matrix(True, False),\n    'random_self_trans_matrix': transition_matrix(False, True),\n    'determenistic_self_trans_matrix': transition_matrix(True, True),\n    'random_transitison_tensor': transition_tensor(False, False),\n    'determenistic_transitison_tensor': transition_tensor(True, False),\n    'random_self_trans_tensor': transition_tensor(False, True),\n    'determenistic_self_trans_tensor': transition_tensor(True, True),\n    \n    'random_transitison_matrix_decay': transition_matrix(False, False, decay = 1.05),\n    'random_self_trans_matrix_decay': transition_matrix(False, True, decay = 1.05),\n    'random_transitison_tensor_decay': transition_tensor(False, False, decay = 1.05),\n    'random_self_trans_tensor_decay': transition_tensor(False, True, decay = 1.05),\n    \n    'determenistic_transitison_matrix_decay': transition_matrix(True, False, decay = 1.05),\n    'determenistic_self_trans_matrix_decay': transition_matrix(True, True, decay = 1.05),\n    'determenistic_transitison_tensor_decay': transition_tensor(True, False, decay = 1.05),\n    'determenistic_self_trans_tensor_decay': transition_tensor(True, True, decay = 1.05),\n}\n\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    \n    # bandits' params\n    step_size = 2 # how much we increase a and b \n    decay_rate = 1.05 # how much do we decay old historical data\n    \n    # I don't see how to use any global variables, so will save everything to a CSV file\n    # Using pandas for this is too much, but it can be useful later and it is convinient to analyze\n    def save_history(history, file = 'history.csv'):\n        pd.DataFrame(history).to_csv(file, index = False)\n\n    def load_history(file = 'history.csv'):\n        return pd.read_csv(file).to_dict('records')\n    \n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        save_history(history)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n        \n    \n    # load history\n    if observation.step == 0:\n        history = []\n        bandit_state = {k:[1,1] for k in agents.keys()}\n    else:\n        history = update_competitor_step(load_history(), observation.lastOpponentAction)\n        \n        # load the state of the bandit\n        with open('bandit.json') as json_file:\n            bandit_state = json.load(json_file)\n        \n        # updating bandit_state using the result of the previous step\n        # we can update all states even those that were not used\n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) \/ decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) \/ decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size\/2\n                bandit_state[name][1] += step_size\/2\n            \n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)","25bcfd0c":"%%writefile opponent_transition_matrix.py\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))","a148bd8b":"%%writefile decision_tree_classifier.py\n\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    features = np.append(features, rollouts['steps'])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    \n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action","dc72d500":"%%writefile statistical_prediction.py\n\nimport random\nimport pydash\nfrom collections import Counter\n\n# Create a small amount of starting history\nhistory = {\n    \"guess\":      [0,1,2],\n    \"prediction\": [0,1,2],\n    \"expected\":   [0,1,2],\n    \"action\":     [0,1,2],\n    \"opponent\":   [0,1],\n}\ndef statistical_prediction_agent(observation, configuration):    \n    global history\n    actions         = list(range(configuration.signs))  # [0,1,2]\n    last_action     = history['action'][-1]\n    opponent_action = observation.lastOpponentAction if observation.step > 0 else 2\n    \n    history['opponent'].append(opponent_action)\n\n    # Make weighted random guess based on the complete move history, weighted towards relative moves based on our last action \n    move_frequency       = Counter(history['opponent'])\n    response_frequency   = Counter(zip(history['action'], history['opponent'])) \n    move_weights         = [ move_frequency.get(n,1) + response_frequency.get((last_action,n),1) for n in range(configuration.signs) ] \n    guess                = random.choices( population=actions, weights=move_weights, k=1 )[0]\n    \n    # Compare our guess to how our opponent actually played\n    guess_frequency      = Counter(zip(history['guess'], history['opponent']))\n    guess_weights        = [ guess_frequency.get((guess,n),1) for n in range(configuration.signs) ]\n    prediction           = random.choices( population=actions, weights=guess_weights, k=1 )[0]\n\n    # Repeat, but based on how many times our prediction was correct\n    prediction_frequency = Counter(zip(history['prediction'], history['opponent']))\n    prediction_weights   = [ prediction_frequency.get((prediction,n),1) for n in range(configuration.signs) ]\n    expected             = random.choices( population=actions, weights=prediction_weights, k=1 )[0]\n\n    # Play the +1 counter move\n    action = (expected + 1) % configuration.signs\n    \n    # Persist state\n    history['guess'].append(guess)\n    history['prediction'].append(prediction)\n    history['expected'].append(expected)\n    history['action'].append(action)\n\n    # Print debug information\n    print('opponent_action                = ', opponent_action)\n    print('move_weights,       guess      = ', move_weights, guess)\n    print('guess_weights,      prediction = ', guess_weights, prediction)\n    print('prediction_weights, expected   = ', prediction_weights, expected)\n    print('action                         = ', action)\n    print()\n    \n    return action","679e640a":"# Upgrade kaggle_environments using pip before import\n!pip install -q -U kaggle_environments","5d91411b":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import make, evaluate","c85759be":"env = make(\"rps\", configuration={\"episodeSteps\": 1000}, debug=True)","edee0d6f":"env.run([\"random_forest_random.py\", \"nash_equilibrium.py\"])\n\nenv.render(mode=\"ipython\", width=500, height=400)","7d7694b8":"# raise SystemExit(\"Stop right there!\")","1733d6d2":"list_names = [\n    \"rock\", \n    \"paper\", \n    \"scissors\",\n    \"hit_the_last_own_action\",  \n    \"copy_opponent\", \n    \"reactionary\", \n    \"counter_reactionary\", \n    \"statistical\", \n    \"nash_equilibrium\",\n    \"markov_agent\", \n    \"memory_patterns\", \n    \"multi_armed_bandit\",\n    \"opponent_transition_matrix\",\n    \"decision_tree_classifier\",\n    \"statistical_prediction\",\n    \"random_forest_random\",\n]\nlist_agents = [agent_name + \".py\" for agent_name in list_names]\n\nscores = np.zeros((len(list_names), 10), dtype=int)","d92364ac":"env = make(\"rps\", configuration={\"episodeSteps\": 1000})","08148edb":"print(\"Simulation of battles. It can take some time...\")\n\nfor ind_agent_1 in range(10):\n    for ind_agent_2 in range(len(list_names)):\n        print(f\"LOG: Random Forest Random vs {list_names[ind_agent_2]}\", end=\"\\r\")\n        \n        current_score = evaluate(\n            \"rps\", \n            [\"random_forest_random.py\", list_agents[ind_agent_2]], \n            configuration={\"episodeSteps\": 1000}\n        )\n        \n        scores[ind_agent_2, ind_agent_1, ] = current_score[0][0]\n    \n    print(\"Round: \", ind_agent_1, \" of 10 \")","ec22ecb3":"df_scores = pd.DataFrame(\n    scores, \n    index=list_names, \n    columns=range(10),\n)\n\n\nplt.figure(figsize=(15, 10))\nsns.heatmap(\n    df_scores, annot=True, cbar=False, cmap='coolwarm', linewidths=1, linecolor='black', fmt=\"d\"\n)\nplt.suptitle('Random Forest Random vs all agents', fontsize=20)\nplt.title('Final Reward Score', fontsize=15)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(fontsize=15);","48ca5fbf":"df_review=pd.DataFrame()\ndf_review['Won'] = df_scores.select_dtypes(include='int').gt(0).sum(axis=1)\ndf_review['Tie'] = df_scores.select_dtypes(include='int').eq(0).sum(axis=1)\ndf_review['Lost'] = df_scores.select_dtypes(include='int').lt(0).sum(axis=1)","5254afe7":"plt.figure(figsize=(5, 10))\nsns.heatmap(\n    df_review, annot=True, cbar=False, cmap='coolwarm', linewidths=1, linecolor='black', fmt=\"d\"\n)\nplt.suptitle('Random Forest Random vs all agents', fontsize=20)\nplt.title('Total games Won-Tie-Lost', fontsize=15)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(fontsize=15);","896564ae":"Copy from kernel [(Not so) Markov \u26d3\ufe0f](https:\/\/www.kaggle.com\/alexandersamarin\/not-so-markov)","bab5cd21":"The idea of the agent:\n\n- A lot of agents use a simple baseline - copy the last action of the opponent.   \n- That's why we can simply hit our last actions (new action of the opponent)","bb548b82":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#FBE338; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h3>\n\n## Opponents\n* [Agent: Hit The Last Own Action](#1)\n* [Agent: Rock](#2)\n* [Agent: Paper](#3)\n* [Agent: Scissors](#4)\n* [Agent: Copy Opponent](#5)\n* [Agent: Reactionary](#6)\n* [Agent: Counter Reactionary](#7)\n* [Agent: Statistical](#8)\n* [Agent: Nash Equilibrium](#9)\n* [Agent: Markov Agent](#10)\n* [Agent: Memory Patterns](#11)\n* [Agent: Multi Armed Bandit](#12)\n* [Agent: Opponent Transition Matrix](#13)\n* [Agent: Decision Tree Classifier](#14)\n* [Agent: Statistical Prediction](#15)\n\n### Battle    \n* [Setup and validation](#100)\n* [Marathon: Random Forest Random against all agents](#102)\n* [Results](#103)\n* [Review](#104)\n","cb91ba5d":"Copy from kernel [RPS: Opponent Transition Matrix](https:\/\/www.kaggle.com\/group16\/rps-opponent-transition-matrix)","fe465eb8":"<a id=\"13\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Opponent Transition Matrix<center><h2>\n","fdd05da4":"<a id=\"9\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Nash Equilibrium<center><h2>\n\n\n","cae88ae0":"Copy from kernel [Rock Paper Scissors - Statistical Prediction](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-statistical-prediction)","e55bc639":"Copy from kernel [Decision Tree Classifier](https:\/\/www.kaggle.com\/alexandersamarin\/decision-tree-classifier?scriptVersionId=46415861)","e3411720":"We need to import the library for creating environments and simulating agent battles","618a513b":"<a id=\"1\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Random Forest Random<center><h2>","349d7c4f":"## The only change here is that XGboost is used instead of Random Forests","6894cee6":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nHit the last action of the opponent","5bc6ca46":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nAlways uses Scissors action","e44a9d30":"Copy from kernel [Rock, Paper, Scissors with Memory Patterns](https:\/\/www.kaggle.com\/yegorbiryukov\/rock-paper-scissors-with-memory-patterns)","7b3b1888":"Copy from kernel [Rock Paper Scissors - Nash Equilibrium Strategy](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-nash-equilibrium-strategy)\n\nNash Equilibrium Strategy (always random)","fa85048b":"<a id=\"2\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Rock<center><h2>","1bfe9a03":"<a id=\"5\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Copy Opponent<center><h2>","94a1e5d0":"<a id=\"11\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Memory Patterns<center><h2>\n\n\n","d38a95b4":"Create a rock-paper-scissors environment (RPS), and set 1000 episodes for each simulation","16d740dc":"<a id=\"14\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Decision Tree Classifier<center><h2>\n\n","a6e10bea":"Create environnment without debug","0b9bb3e7":"# Random Forest Random - Rock Paper Scissors\n\n`Random Forest Random` is a Rock Paper Scissors Agent that makes predictions using the `Random Forest Classification` with a bit of `random`. The `random` on this agent has been limited only when losing (and partially on window length) so that the true possibilities of the algorithm are revealed.\n\nScope of this notebook is to extensively test the performance of `Random Forest Random` vs public agents over 10 rounds of 1000 episodes against each agent.\n\nThe notebook is based on [Rock Paper Scissors - Agents Comparison](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-agents-comparison) with adaptations and extras so that can focus on a single agent.","672251c0":"<a id=\"1\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Hit The Last Own Action<center><h2>","c137e208":"<a id=\"3\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Paper<center><h2>","c684db0f":"<a id=\"4\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Scissors<center><h2>","eb50dbfc":"* `Random Forest Random` can identify the patterns of all simple agents in 5 actions or less.\n* `Statistical` is an easy opponent for `Random Forest Random` and performs better than `Markov Agent` almost every time.\n* Chances to win over `Memory Patterns` or `Multi Armed Bandit` are near zero an only by luck can beat them sometimes.\n* Luck is crucial for the outcome over `Opponent Transition Matrix`, `Decision Tree Classifier` and `Statistical Prediction` as the results can vary a lot over matches.\n* Final conclusion is that `Random Forest Classifiers` can be used to predict opponents actions on `Rock-Paper-Scissors` but advanced `defensive` mechanisms are required when the pattern is identified by the opponent.\n\n**Disclaimer: The above review is done on multiple runs of this notebook and the published results might not represent them exactly.**","72c95d7a":"<a id=\"6\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Reactionary<center><h2>\n","0c645bfd":"<a id=\"10\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Markov Agent<center><h2>\n\n\n","850cb675":"<a id=\"7\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Counter Reactionary<center><h2>\n\n","e86ed20e":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py","ce31e93a":"<a id=\"8\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Statistical<center><h2>\n\n\n","ae3cd149":"<a id=\"15\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Statistical Prediction<center><h2>\n\n","77a57638":"Run the simulation","7e54be46":"<a id=\"12\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Multi Armed Bandit<center><h2>","487a4e1e":"Copy from kernel [Multi-armed bandit vs deterministic agents](https:\/\/www.kaggle.com\/ilialar\/multi-armed-bandit-vs-deterministic-agents)","d7bdbf2d":"Setup battlefield","922fb36d":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nCopy the last action of the opponent","c0784745":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nAlways uses Paper action","4c825c8b":"<a id=\"103\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Results<center><h2>","6213fb9b":"<a id=\"102\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Marathon: Random Forest Random against all agents<center><h2>\n","52be03a0":"Validation","bbd542dd":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py    \n\nAlways uses Rock action","e121dd9c":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py","1cd68ea6":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/22838\/logos\/header.png?t=2020-11-02-21-55-44)","6a0187ca":"<a id=\"104\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Review<center><h2>","fcf34050":"## this kernel was coped from [Random Forest Random - Rock Paper Scissors](http:\/\/www.kaggle.com\/jumaru\/random-forest-random-rock-paper-scissors) \n## I hope you see his kernel and give it upvote\n\n","70108edd":"<a id=\"100\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Setup and validation<center><h2>"}}