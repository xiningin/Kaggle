{"cell_type":{"9a7f64ae":"code","5810207f":"code","3cf471eb":"code","d48432de":"code","ff408643":"code","8675591d":"code","d01742c6":"code","d708a069":"code","57101f81":"code","bbf4b9c9":"code","6383ab52":"code","473f32b0":"code","258f0a7f":"code","115d17d5":"code","d41a27ab":"code","0e2246ab":"code","4f9f05a4":"code","8b15fe2c":"code","d7f2cd63":"code","66e00d71":"code","15d1f4a8":"code","24ac56df":"code","6290d6ec":"code","e7e42558":"code","34ddbbb5":"code","6a066246":"code","46cd4ac0":"code","a46444af":"code","e3e16bc9":"code","7e771a08":"code","b7f37da2":"code","3a7715eb":"code","b3ee4cc2":"code","ce061a02":"code","284447ea":"code","0babaced":"code","12fb6134":"code","7732a3c9":"code","8b5a38ed":"code","3f57d6a8":"code","88907656":"code","2289c358":"code","8986b6fd":"code","0a659a66":"code","489bc9e7":"code","8a06b877":"code","f7258c60":"code","0b0e9b58":"code","fa9dbcee":"code","1797069c":"code","8082c55e":"code","01e17f42":"code","57b3d7b9":"code","92ee566e":"code","fd5c291f":"code","d2739fcb":"code","663584d5":"code","96c0164c":"code","2ddef3bf":"code","aeb912fa":"code","a2b1deea":"code","dc02c73f":"code","5b032c29":"code","2d38c05b":"code","d85549f6":"code","8df988ee":"code","f6f3182b":"code","121c2958":"code","cbffb88a":"code","abcf6d56":"code","9f9619d6":"code","62fc3e70":"markdown","4cebb522":"markdown","24e5c1e1":"markdown","5b95f5c1":"markdown","6424cc20":"markdown","13f34c62":"markdown","3ecb09a1":"markdown","7de767e9":"markdown","c75d68ab":"markdown","4aef92d5":"markdown","5a90e6b8":"markdown","07e7ba26":"markdown","8e68ea02":"markdown","be159e90":"markdown","7fdc806d":"markdown","ceed1f04":"markdown","ec9a1528":"markdown","ff06879f":"markdown","dec1fed7":"markdown","1c260c44":"markdown","4bd7de4e":"markdown","9225d9a9":"markdown","a8c0d5bc":"markdown","22f6e0c9":"markdown","0efdc1a3":"markdown","075672d5":"markdown","0c878404":"markdown","62484d34":"markdown","9cdc3a42":"markdown","5c49c148":"markdown","4214a048":"markdown","ff5765fe":"markdown","f2d91c50":"markdown","cac99cc6":"markdown","8157941c":"markdown","84c9a7f8":"markdown","d8d23b02":"markdown","4e02c41b":"markdown","0ec5c183":"markdown","3bf2e2c8":"markdown","c4504cd6":"markdown","99d1a5e6":"markdown","e42ffb53":"markdown"},"source":{"9a7f64ae":"from google.colab import drive\ndrive.mount('\/content\/gdrive')","5810207f":"cd '\/content\/gdrive\/My Drive\/Colab Notebooks\/DS projects'","3cf471eb":"# IMPORT packages, libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data split\nfrom sklearn.model_selection import train_test_split\n\n# ML models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\n\n# cross validation\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score\n\n# Evaluation metrics\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score,\\\n  precision_score, recall_score, f1_score, plot_roc_curve","d48432de":"import warnings\nwarnings.filterwarnings('ignore')","ff408643":"# Load dataset\ndf=pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ndf.head()","8675591d":"# Info\ndf.info()","d01742c6":"# Stats\ndf.describe().T","d708a069":"# missing values\ndf.isnull().sum()","57101f81":"# TARGET\ndf.target.value_counts()","bbf4b9c9":"df.sex.value_counts()","6383ab52":"pd.crosstab(df.sex, df.target)","473f32b0":"pd.crosstab(df.sex, df.target).plot(kind='bar');\nplt.ylabel('Count of people');\nplt.legend(['Female', 'Male']);","258f0a7f":"df.cp.value_counts()","115d17d5":"pd.crosstab(df.cp, df.target)","d41a27ab":"pd.crosstab(df.cp, df.target).plot(kind='bar')\nplt.ylabel('Count of people');\nplt.legend(['No disease', 'Heart disease'])\nplt.xticks(ticks=[0,1,2,3],rotation=30,labels=['Typical angina', 'Atypical angina','Non-anginal pain','Asymptomatic']);","0e2246ab":"df.age.describe()","4f9f05a4":"# AGE distribution among patients with heart disease\ndf[df.target==1].age.hist(bins=7);","8b15fe2c":"# Comparing the AGE distribution for different chest pains.\nfig, ax= plt.subplots(2,2)\nax[0,0].hist(df[(df.cp==0) & (df.target==1)].age, bins=5);\nax[0,0].set_title('cp=0');\nax[0,1].hist(df[(df.cp==1) & (df.target==1)].age, bins=5, color='red');\nax[0,1].set_title('cp=1');\nax[1,0].hist(df[(df.cp==2) & (df.target==1)].age, bins=5, color='orange');\nax[1,0].set_title('cp=2');\nax[1,1].hist(df[(df.cp==3) & (df.target==1)].age, bins=5, color='yellow');\nax[1,1].set_title('cp=3');\nfig.tight_layout()","d7f2cd63":"fig, ax= plt.subplots(figsize=(12,8))\nax=sns.heatmap(df.corr());","66e00d71":"for x in ['age','trestbps','chol','thalach']:\n  print(x.upper())\n  sns.boxplot(data=df, y=x);\n  plt.show()","15d1f4a8":"df.columns","24ac56df":"for x in ['age','trestbps','chol','thalach']:\n  print(x.upper())\n  sns.distplot(df[x], kde=True);\n  plt.show()\n  print('skew:', df[x].skew())\n  print('--------------------------------------')","6290d6ec":"sns.distplot(np.log(df.trestbps), kde=True)","e7e42558":"np.log(df.trestbps).skew()","34ddbbb5":"np.log(df.chol).skew()","6a066246":"outlier_dict={}\nscale_dict={}\ndef train_engg(X, feat=['age','trestbps','chol','thalach']):\n  for f in feat:\n    # print(f)\n    # outlier\n    first_quartile= np.percentile(X[f],25)\n    third_quartile= np.percentile(X[f],75)\n    iqr= third_quartile-first_quartile\n    lowest= first_quartile-(1.5*iqr)\n    highest= third_quartile+(1.5*iqr)\n    # print(lowest, highest)\n    outlier_dict[f]=[lowest,highest]\n    X[f]= np.where(X[f]>highest, highest, np.where(X[f]<lowest, lowest, X[f]))\n    \n    # transformation\/normalization\n    X[f]=np.log(X[f])\n\n    # scaling\n    mean=X[f].mean()\n    std=X[f].std()\n    scale_dict[f]=[mean,std]\n    X[f]=(X[f]-mean)\/std\n  return X\n","46cd4ac0":"# Test set method for feature engg\ndef test_engg(X, feat=['age','trestbps','chol','thalach']):\n  for f in feat:\n\n    # outlier\n    X[f]= np.where(X[f]>outlier_dict[f][1], outlier_dict[f][1],np.where(X[f]<outlier_dict[f][0], outlier_dict[f][0], X[f]))\n    \n    # transformation\/normalization\n    X[f]=np.log(X[f])\n\n    # scaling\n    X[f]=(X[f]-scale_dict[f][0])\/scale_dict[f][1]\n  return X\n\n# X_test=test_engg(X_test)","a46444af":"X= df.drop(['target'], axis=1)\n\ny= df.target","e3e16bc9":"# Splitting the data\n# Random seed for reproducibility\nnp.random.seed(42)\n\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.2)","7e771a08":"len(X_test)","b7f37da2":"len(X_train)","3a7715eb":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(), \n          \"Random Forest\": RandomForestClassifier(),\n          \"Linear SVC\": LinearSVC()}","b3ee4cc2":"# Function to train the models and record the scores\ndef fit_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    X_test : testing data\n    y_train : labels assosciated with training data\n    y_test : labels assosciated with test data\n    \"\"\"\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # dict to store model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit\/train the model\n        \n        X_train_new=train_engg(X_train.copy())\n\n        model.fit(X_train_new, y_train)\n        #print(name, model.get_params())\n        \n        # Evaluate the model and append its score to model_scores\n\n        X_test_new=test_engg(X_test.copy())\n\n        model_scores[name] = model.score(X_test_new, y_test) # accuracy\n    return model_scores","ce061a02":"model_scores= fit_score(models, X_train, X_test, y_train, y_test)\nmodel_scores","284447ea":"plt.bar(x=model_scores.keys(), height=model_scores.values());\nplt.ylabel('Accuracy'); ","0babaced":"np.logspace(-4,4,20)","12fb6134":"# Logistic Regression\nlr_grid={\"C\": [1,2,3,4,5,6,7,8,9], \"solver\": ['lbfgs','liblinear'], 'penalty':['l2']}\n\nnp.random.seed(42)\nlr_gs= GridSearchCV(LogisticRegression(), param_grid= lr_grid, cv=5, verbose=True, scoring='recall')\n\nX_train_new=train_engg(X_train.copy())\nlr_gs.fit(X_train_new, y_train);","7732a3c9":"lr_gs.best_params_","8b5a38ed":"# model score\nX_test_new= test_engg(X_test.copy())\nlr_gs.score(X_test_new, y_test)","3f57d6a8":"# TRAINING SCORE\nlr_gs.score(X_train_new, y_train)","88907656":"import sklearn\nsklearn.metrics.SCORERS.keys()","2289c358":"# Logistic Regression\nknn_grid={'n_neighbors':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]}\n\nnp.random.seed(42)\nknn_gs= GridSearchCV(KNeighborsClassifier(), param_grid= knn_grid, cv=5, verbose=True, scoring='recall')\n\nX_train_new=train_engg(X_train.copy())\nknn_gs.fit(X_train_new, y_train);","8986b6fd":"knn_gs.best_params_","0a659a66":"# model score\nX_test_new= test_engg(X_test.copy())\nknn_gs.score(X_test_new, y_test)","489bc9e7":"# TRAINING SCORE\nknn_gs.score(X_train_new, y_train)","8a06b877":"# Logistic Regression\nsvc_grid={'C':[0.01,0.05,0.1,0.5,1,2,3,4,5,6,7,8,9]}\n\nnp.random.seed(42)\nsvc_gs= GridSearchCV(LinearSVC(), param_grid= svc_grid, cv=5, verbose=True, scoring='recall')\n\nX_train_new=train_engg(X_train.copy())\nsvc_gs.fit(X_train_new, y_train);","f7258c60":"svc_gs.best_params_","0b0e9b58":"# model score\nX_test_new= test_engg(X_test.copy())\nsvc_gs.score(X_test_new, y_test)","fa9dbcee":"# TRAINING SCORE\nsvc_gs.score(X_train_new, y_train)","1797069c":"rf_grid = {\"n_estimators\": np.arange(10, 400, 50),\n           \"max_depth\": [None, 3, 5],\n           \"min_samples_split\": np.arange(2, 7, 2),\n           \"min_samples_leaf\": np.arange(15, 22, 2)}","8082c55e":"np.random.seed(42)\nrf_gs= GridSearchCV(RandomForestClassifier(), param_grid= rf_grid, cv=5, verbose=True)\n\nX_train_new=train_engg(X_train.copy())\nrf_gs.fit(X_train_new, y_train);","01e17f42":"rf_gs.best_params_","57b3d7b9":"rf_gs.score(X_test, y_test)","92ee566e":"# TRAINING SCORE\nrf_gs.score(X_train_new, y_train)","fd5c291f":"X_test_new= test_engg(X_test.copy())\ny_pred= knn_gs.predict(X_test_new)","d2739fcb":"y_test.values","663584d5":"pd.DataFrame({'Actual':y_test.values, 'Predicted':y_pred})","96c0164c":"sns.heatmap(confusion_matrix(y_test, y_pred), annot=True)\nplt.xlabel('Actual')\nplt.ylabel('Pred');","2ddef3bf":"# Import ROC curve function from metrics module\nfrom sklearn.metrics import plot_roc_curve\n\n# Plot ROC curve and calculate AUC metric\nplot_roc_curve(knn_gs, X_test_new, y_test);","aeb912fa":"print(classification_report(y_test, y_pred))","a2b1deea":"knn_gs.best_params_","dc02c73f":"clf= KNeighborsClassifier(n_neighbors=13)","5b032c29":"X_new= test_engg(X.copy())\ncv_acc = np.mean(cross_val_score(clf,\n                         X_new,\n                         y,\n                         cv=5, # 5-fold cross-validation\n                         scoring=\"accuracy\")) # accuracy as scoring\ncv_acc","2d38c05b":"# Cross-validated recall score\ncv_recall = np.mean(cross_val_score(clf,\n                                    X_new,\n                                    y,\n                                    cv=5, # 5-fold cross-validation\n                                    scoring=\"recall\")) # recall as scoring\ncv_recall","d85549f6":"# Cross-validated precision score\ncv_precision = np.mean(cross_val_score(clf,\n                                       X_new,\n                                       y,\n                                       cv=5, # 5-fold cross-validation\n                                       scoring=\"precision\")) # precision as scoring\ncv_precision","8df988ee":"# Cross-validated F1 score\ncv_f1 = np.mean(cross_val_score(clf,\n                                X_new,\n                                y,\n                                cv=5, # 5-fold cross-validation\n                                scoring=\"f1\")) # f1 as scoring\ncv_f1","f6f3182b":"X_train_new= train_engg(X_train.copy())","121c2958":"def show_feat_imp(clf2):\n  clf2.fit(X_train_new, y_train)\n  features_dict = dict(zip(df.columns, list(clf2.coef_[0])))\n  # Visualize feature importance\n  features_df = pd.DataFrame(features_dict, index=[0])\n  features_df.T.plot.bar(title=\"Feature Importance\", legend=False); ","cbffb88a":"# Logistic Regression\nshow_feat_imp(LogisticRegression(C=1, penalty='l2', solver='liblinear'))","abcf6d56":"# Linear SVC\nshow_feat_imp(LinearSVC(C=0.1))","9f9619d6":"sns.heatmap(df.corr())","62fc3e70":"### Linear SVC","4cebb522":"### Comparing \"cp\" feature with TARGET\ncp - chest pain type\n- 0: Typical angina: chest pain related decrease blood supply to the heart\n- 1: Atypical angina: chest pain not related to heart\n- 2: Non-anginal pain: typically esophageal spasms (non heart related)\n- 3: Asymptomatic: chest pain not showing signs of disease","24e5c1e1":"Here people with age 40-65 are more likely to have chest pain with label 1,2,3 which has higher risk of heart disease.\n\n**Age 40 to 65 risky**.","5b95f5c1":"## Data prep for modelling","6424cc20":"### Checking distribution","13f34c62":"We can see that using log transform we are able to reduce the **skewness** (left or right shift of distribution) and normalize the distribution.","3ecb09a1":"As 'trestbps' and 'chol' feature have skewness more than 0.5 (+ or -), we need to transform these.\n\nAnd then we will scale down the features.","7de767e9":"**Positive corr (target tends to 1->disease with inc. in these values)**\n- cp\n- thalach (max heart rate)\n- slope (more downslope of peak exercise ST segment increases risk)\n\n**Negative corr (risk reduces with inc. in values)**\n- exang (if angina\/ chest pain is due to exercise then low risk)\n- oldpeak\n- ca (more colored vessels seen during fluoroscopy, low risk)","c75d68ab":"All the features are of numerical type, so we need not go for any encoding.","4aef92d5":"## Evaluating model\n### KNN","5a90e6b8":"This is a nearly balanced dataset.","07e7ba26":"## Creating models","8e68ea02":"400 iterations means: 20 C x 2 penalties x 2 solvers x 5 cv= 400\n\nLet's see the best parameters and score.","be159e90":"## Classification report","7fdc806d":"#### Random Forest","ceed1f04":"We can see these features have outliers.","ec9a1528":"Here we can see Features like SEX, CP, EXANG, SLOPE, CA, THAL are given more importance by both the models. And their coefficients (+ve or -ve) match their corelation on the below heatmap.","ff06879f":"## Cross validation scores for different metrics","dec1fed7":"So, around 75% of females and 47% of males tend to have heart disease.\n\nLets plot it.","1c260c44":"## Feature transformation \/ Normalization","4bd7de4e":"### Co-relation matrix","9225d9a9":"### KNN","a8c0d5bc":"### Confusion matrix","22f6e0c9":"Model Scores without feature engineering:\n\n{'KNN': 0.6885245901639344,\n \n 'Linear SVC': 0.47540983606557374,\n\n 'Logistic Regression': 0.8852459016393442,\n\n 'Random Forest': 0.8360655737704918}","0efdc1a3":"## Methods for feature engineering\nHere we will make a function to apply outlier treatment, normalization and scaling on both train set and test set.\n\nWe will learn the features from the train set first into outlier_dict{} and scale_dict{} and use these over test set, rather than learning different features\/characters of test set.\n\nWe are going for **Outlier handling** before **Normalization** as Outlier handling helps in normalizing the data upto certain extent.","075672d5":"### Outlier detection\nWe will now try to find outliers in features with continuous values.","0c878404":"We can see here the stats for different features.\n\nFew of them have values ranging in hundreds and others have really low values, so we might need to **scale down our features**.\n\nWe can also see that all features have 330 records, i.e. no missing values. But still lets have a look.","62484d34":"### Comparing \"Sex\" feature with TARGET","9cdc3a42":"## Feature importance\nLets have a look at the model coeffients to mark the importance the models have given to different features.\n\nAs we do not get model coefficients for KNN, we will go for Logistic Regression and Linear SVC.","5c49c148":"## EDA","4214a048":"So, people with age 40-65 are more prone to heart disease.","ff5765fe":"So we have more males (1) than females (0) in our data set.","f2d91c50":"## Features\n1. age - age in years<br>\n2. sex - (1 = male; 0 = female)<br>\n\n3. cp - chest pain type\n- 0: Typical angina: chest pain related decrease blood supply to the heart\n- 1: Atypical angina: chest pain not related to heart\n- 2: Non-anginal pain: typically esophageal spasms (non heart related)\n- 3: Asymptomatic: chest pain not showing signs of disease\n\n\n4. trestbps - resting blood pressure (in mm Hg on admission to the hospital)\nanything above 130-140 is typically cause for concern\n\n5. chol - serum cholestoral in mg\/dl\nserum = LDL + HDL + .2 * triglycerides\nabove 200 is cause for concern\n\n6. fbs - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n'>126' mg\/dL signals diabetes\n\n7. restecg - resting electrocardiographic results\n- 0: Nothing to note\n- 1: ST-T Wave abnormality\ncan range from mild symptoms to severe problems\nsignals non-normal heart beat\n- 2: Possible or definite left ventricular hypertrophy\nEnlarged heart's main pumping chamber\n\n8. thalach - maximum heart rate achieved\n\n9. exang - exercise induced angina (1 = yes; 0 = no)\n\n10. oldpeak - ST depression induced by exercise relative to rest\nlooks at stress of heart during excercise\nunhealthy heart will stress more\n\n11. slope - the slope of the peak exercise ST segment\n- 0: Upsloping: better heart rate with excercise (uncommon)\n- 1: Flatsloping: minimal change (typical healthy heart)\n- 2: Downslopins: signs of unhealthy heart\n\n12. ca - number of major vessels (0-3) colored by flourosopy\ncolored vessel means the doctor can see the blood passing through\nthe more blood movement the better (no clots)\n\n13. thal - thalium stress result\n1,3: normal\n6: fixed defect: used to be defect but ok now\n7: reversable defect: no proper blood movement when excercising\n\n14. target - have disease or not (1=yes, 0=no) (= the predicted attribute)\nNote: No personal identifiable information (PPI) can be found in the dataset.","cac99cc6":"So, as cp goes from 0 to 3, the chances of heart diseases increase. And the highest chance is for people with **Atypical angina** and **Non-anginal pain**.","8157941c":"## Feature Engineering ","84c9a7f8":"So, none of our models are over-fitting as they have both training and test score same.","d8d23b02":"### Comparing age and cp","4e02c41b":"Here we can see 4 people each are **False positive** (Predicted as heart patients but actually not) and **False negative** (Predicted as not heart patients but actually are).","0ec5c183":"## ROC curve and AUC score\nThe large the area under curve (AUC), the better the model.","3bf2e2c8":"### GridSearchCV tuning\nIt will try every single hyperparameter combination to reach the best score possible.\n\nWe will be using RECALL score for tuning our models as it is a health related problem statement where we generally want to reduce FALSE NEGATIVES.\n\n#### Logistic Regression","c4504cd6":"# Prediction of heart disease","99d1a5e6":"## Hyper-parameter tuning with cross-validation","e42ffb53":"## Problem statement\nPredict if a aptient have heart disease based on his\/her clinical parameters.\n\n## DATA\nThe dataset can be found here: https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci\n\n"}}