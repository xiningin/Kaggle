{"cell_type":{"01431a46":"code","1331fb58":"markdown","cf0d7938":"markdown","f3843339":"markdown","b5f940a7":"markdown","4dea871b":"markdown","e44c923a":"markdown"},"source":{"01431a46":"#The Following is the code base for verification, it is not all ironed out yet.\n#Use with caution, and note the R and Python Code.\n\n# #Big Data Bowl\n# setwd(\"C:\/Users\/dcraw\/OneDrive\/Desktop\/Data Analytics Bowl\/nfl-big-data-bowl-2022\")\n# library(data.table)\n# plays = fread(\"plays.csv\")\n# plays = cbind(id = paste0(plays$gameId, plays$playId), plays)\n# \n# kickoff_plays = unique(plays$id[intersect(which(plays$specialTeamsPlayType == 'Kickoff'), which(plays$specialTeamsResult == \"Return\"))])\n# \n# tracking_file_names = c(\"tracking2018.csv\", \"tracking2019.csv\", \"tracking2020.csv\")\n# tracking_data = data.table()\n# \n# for(f in tracking_file_names){\n#   tracking_data = rbind(tracking_data, fread(f))\n# }\n# \n# tracking_data = cbind(id = paste0(tracking_data$gameId, tracking_data$playId),tracking_data)\n# \n# \n# kickoff_plays_tracking_data = tracking_data[which(tracking_data$id %in% kickoff_plays),]\n# \n# \n# #Only kick received plays\n# #get rid of punt\n# get_rid_of = function(df, s){\n#   plays = unique(df$id[which(df$event == s)])\n#   return(df[!which(df$id %in% plays),])\n# }\n# \n# #carve\n# df = get_rid_of(kickoff_plays_tracking_data, \"punt\")\n# df = get_rid_of(df, \"penalty_flag\")\n# df = get_rid_of(df, \"fumble\")\n# df = get_rid_of(df, \"handoff\")\n# df = get_rid_of(df, \"onside_kick\")\n# df = get_rid_of(df, \"drop_kick\")\n# df = get_rid_of(df, \"onside_kick\")\n# df = get_rid_of(df, \"out_of_bounds\")\n# df = get_rid_of(df, \"fair_catch\")\n# df = get_rid_of(df, \"kick_recovered\")\n# df = get_rid_of(df, \"lateral\")\n# df = get_rid_of(df, \"fumble_defense_recovered\")\n# df = get_rid_of(df, \"fumble_offense_recovered\")\n# df = get_rid_of(df, \"punt_downed\")\n# df = get_rid_of(df, \"punt_received\")\n# df = get_rid_of(df, \"autoevent_kickoff\")\n# df = get_rid_of(df, \"free_kick\")\n# \n# df_ordered = df[with(df, order(df$id, df$frame, df$displayName)),]\n# \n# fwrite(df_ordered, file = \"simple_kickoff_plays_ordered.csv\")\n\n#STEP 1: Read In and Format Data\n#Read in and manipulate the data\nimport torch\nimport csv\nimport random\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport matplotlib\nfrom itertools import groupby\nimport time\nimport torch.optim as optim\nfrom collections import deque, namedtuple\n\ndef index_of_closest(f, x, y):\n    cl_i = 0\n    football_coords = np.array([float(x),float(y)])\n    b = np.array([float(f[cl_i][2]), float(f[cl_i][3])])\n    cl_d = np.linalg.norm(football_coords - b)\n    for i in range(len(f)):\n        player_coords = np.array([ float(f[i][2]) , float(f[i][3])])\n        dist = np.linalg.norm(football_coords - player_coords)\n        if dist < cl_d:\n            cl_d = dist\n            cl_i = i\n    return cl_i              \n        \n\n\n\nfile = open('C:\/Users\/dcraw\/OneDrive\/Desktop\/Data Analytics Bowl\/nfl-big-data-bowl-2022\/simple_kickoff_plays_ordered.csv')\n\nreader = csv.reader(file)\nd = []\nheader = next(reader)\nfor r in reader:\n    d.append(r)\n\na = [list(v) for l,v in groupby(sorted(d, key = lambda x:x[0]), lambda x: x[0])]\n\nfor p in a:\n    for f in p:\n            f[15] = int(f[15])\n\n\nplays_db = []\nfor e in a:\n    plays_db.append([list(v) for l,v in groupby(sorted(e, key = lambda x: x[15]), lambda x: x[15])])\n\n\n#Flip plays to go one direction, to the right, meaning play direction is 'left'\nfor play in plays_db:\n    if play[0][0][-1] == 'right':\n        for frame in play:\n            for d in frame:\n                #print(d[11])\n                if d[11] != 'football':\n                    d[2] = float(d[2])-120.0\n                    d[7] = 360.0 - float(d[7])\n                    d[8] = 360.0 - float(d[8])\n                    d[-1] = 'left'\n    \n#Id Returners\nreturners = {}\nfor play in plays_db:\n    #index (frame) of 'kick_received'\n    f = [x for x in play if x[0][9] == 'kick_received'][0]\n    xf, yf = f[-1][2], f[-1][3]\n    i = index_of_closest(f[0:22], xf, yf)\n    #print(i)\n    returners.update({play[0][i][0]: f[i][10]})\n\n\n#Frame when ball caught\nframe_when_received = {}\nfor play in plays_db:\n    for frame in play:\n        if frame[0][9] == \"kick_received\":\n            frame_when_received.update({frame[0][0]: int(frame[0][15])})\n            break\n\n\n#Order: [0]: returner, [1:10]: teammates, [11:21]: opponents, [22]: ball\nordered_plays_db = []\nfor play in plays_db:\n    ordered_play = []\n    returner_id = returners[play[0][0][0]]\n    for frame in play:\n        ordered_frame = []\n        if int(frame[0][15]) >= frame_when_received[frame[0][0]]:\n            is_of_returner = [i for i,x in enumerate(frame) if x[10] == returner_id]\n            i_r = is_of_returner[0]\n            ordered_frame.append(frame[i_r])\n\n            returner_team = frame[i_r][14]\n            returner_teammates = [x for i,x in enumerate(frame) if x[14] == returner_team]\n            returner_teammates.remove(frame[i_r])\n            for tm in returner_teammates:\n                ordered_frame.append(tm)\n\n            opposition_players = [x for i,x in enumerate(frame) if x[14] != returner_team]\n            for opp in opposition_players:\n                ordered_frame.append(opp)\n                    \n            ordered_play.append(ordered_frame)\n\n    ordered_plays_db.append(ordered_play)\n\n\n#STEP 2: Initiate environment\n#Set Up Environment\nimport numpy as np\nimport gym\n\ndef move(d,s,x,y):\n    new_x = float(x) + float((np.sin(np.radians(d))*s))\n    new_y = float(y) + float((np.cos(np.radians(d))*s))\n    return new_x, new_y\n\ndef distance(a,b,c,d,m):\n    return np.linalg.norm(np.array([a,b]) - np.array([c,c])) < m\n\ndef close(x1, y1, x2, y2, d):\n    a = np.array([float(x1),float(y1)])\n    b = np.array([float(x2),float(y2)])\n    return np.linalg.norm(a-b) <= d\n\nclass KickoffReturn_ENV(gym.Env):\n    \n    def __init__(self):\n        #self.action_space = gym.spaces.Box(\n        #    low = 0.0,\n        #    high = 359.99,\n        #    shape = (1,1))\n\n        #360 degrees\n        self.action_space = gym.spaces.Discrete(360)\n        \n        #22 players: 1 self, 10 teammates, 11 opponents\n        #each has (x, y, s, a, dis, o)\n        self.observation_space = gym.spaces.Box(\n            low = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0]),\n            high = np.array([120.0, 53.3, 100.0, 100.0, 10.0, 360.0]))\n        \n\n    def reset(self):\n        self.state = np.array([[0.0]*6]*22)\n        self.reward = 0\n        self.done = False\n        self.info = {}\n\n        self.steps = 0\n\n        return flatten_state(self.state)\n\n    def step(self, action, players):\n\n        players = players[:-1]\n        \n        if self.steps == 0:\n            state_copy = []\n            for p in players:\n                #print(p)\n                state_copy.append([float(d) for d in p[2:8]])\n            \n            self.state = state_copy\n            \n        spd = 0.8\n        \n        if self.done:\n            print('Episode Over')\n        else:\n            assert self.action_space.contains(action)\n            self.steps += 1\n\n\n            if len(self.state) != 22:\n                self.state = stack_state(self.state)\n            #move agent\n            agent_x = self.state[0][0]\n            agent_y = self.state[0][1]\n\n            #print(agent_x)\n            self.state[0][0] = float(move(float(action), spd, agent_x, agent_y)[0])\n            self.state[0][1] = float(move(float(action), spd, agent_x, agent_y)[1])\n\n            #move others\n            for i in range(21):\n                self.state[i+1] = players[i+1][2:8]\n            \n            tackle_d = 1.0\n            block_d = 0.5\n            #check if tackle\n            agent_state = self.state[0]\n            for opp_state in self.state[11:21]:\n                if close(agent_state[0],agent_state[1], opp_state[0], opp_state[1], tackle_d):\n                #check if block\n                    self.done = True\n                    #self.reward = self.state[0][0]\n                    for tm_state in self.state[1:10]:\n                        if close(tm_state[0],tm_state[1], opp_state[0], opp_state[1], block_d):\n                            self.done = False\n\n            self.reward = self.state[0][0]\n\n            state_copy = []\n            for p in self.state:\n                state_copy.append([float(d) for d in p])\n\n            #Check In bounds\n            if self.state[0][0] < 0.0 or self.state[0][0] > 100.0:\n                self.done = True\n\n            if self.state[0][1] < 0.0 or self.state[0][1] > 53.3:\n                self.done = True\n                \n            self.state = flatten_state(state_copy)\n\n            \n\n            \n            return self.state, self.reward, self.done, self.info\n\n    def render(self):\n        w = 100\n        h = 53.3\n        fig = plt.figure()\n        \n        ax = plt.axes(xlim = (-10, w + 30),\n                      ylim = (-15, h + 5))\n        \n        plt.axis('off')\n\n        #agent marker\n        \n\n        rect = matplotlib.patches.Rectangle(\n            (-10, -5),\n            w + 40,\n            h + 10,\n            linewidth = 1,\n            facecolor = '#037f51',\n            capstyle = 'round')\n        ax.add_patch(rect)\n\n        rect = plt.Rectangle(\n            (0,0),\n            w + 20,\n            h,\n            ec = 'w',\n            fc = 'none',\n            lw = 2)\n\n        ax.add_patch(rect)\n\n        ax.add_patch(plt.Circle((self.state[0][0], self.state[0][1]), 0.2, color='yellow'))\n\n        for tm in self.state[1:11]:\n            ax.add_patch(plt.Circle((float(tm[0]), float(tm[1])), 0.1, color='b'))\n\n        for op in self.state[11:23]:\n            ax.add_patch(plt.Circle((float(op[0]), float(op[1])), 0.1, color='r'))\n\n        for i in range(21):\n            plt.plot([10+5*i,10+5*i],\n                     [0,h],\n                     c = 'w',\n                     lw = 2)\n        \n        \n        fig.show()\n        #fig, ax\n        \n\n\n\ndef run_one_episode(env, play_data):\n    env.reset()\n    sum_r = 0\n\n    for i in range(len(play_data)):\n        action = env.action_space.sample()\n\n        action = agent.get_action(env.state)\n\n        players = play_data[i][0:22]\n        \n        s, r, d, i = env.step(action, players)\n\n        #sum_r += r\n        #env.render()\n        if d:\n            break\n\n    return r\n\ndef flatten_state(s):\n    return [float(x) for p in s for x in p]\n\ndef stack_state(s):\n    a = []\n    for i in range(22):\n        b = []\n        for j in range(6):\n            b.append(float(s[i*0+j]))\n        a.append(b)\n            \n    return a\n\n\n\nclass qnet(torch.nn.Module):\n    def __init__(self, state_size, action_size, hidden_n):\n        super(qnet, self).__init__()\n        self.layer1 = torch.nn.Linear(state_size,hidden_n)\n        self.layer2 = torch.nn.Linear(hidden_n,hidden_n)\n        self.layer3 = torch.nn.Linear(hidden_n, action_size)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        x = torch.nn.functional.relu(x)\n        x = self.layer2(x)\n        x = torch.nn.functional.relu(x)\n        x = self.layer3(x)\n        return x\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\nclass Agent():\n    def __init__(self, state_size, action_size, learning_rate, batch_size, bank_size, hidden_n):\n        self.state_size = state_size\n        self.action_size = action_size\n\n        self.net = qnet(state_size, action_size, hidden_n).to(device)\n        self.target_net = qnet(state_size, action_size, hidden_n).to(device)\n        self.optimizer = torch.optim.Adam(self.net.parameters(), lr = learning_rate)\n\n        self.memories = ReplayBuffer(action_size, bank_size, batch_size)\n        self.t = 0\n\n    def step(self, state, action, r, next_state, terminated, tau, gamma, update_every, batch_size):\n        self.memories.add(state, action, r, next_state, terminated)\n        self.t += 1\n\n        if self.t % update_every == 0:\n            if len(self.memories) > batch_size:\n                mems_to_learn_from = self.memories.sample()\n                self.learn(mems_to_learn_from, gamma, tau)\n\n    def act(self, state, epsilon = 0):\n        state = torch.from_numpy(np.array(state)).float().unsqueeze(0).to(device)\n        self.net.eval()\n        with torch.no_grad():\n            action_values = self.net(state)\n        self.net.train()\n\n        if np.random.random() > epsilon:\n            return np.argmax(action_values.cpu().data.numpy())\n        else:\n            return np.random.choice(np.arange(self.action_size))\n\n    def learn(self, memories, gamma, tau):\n        states, actions, rs, next_states, terminateds = memories\n\n        #next_qtargets = self.target_net(next_states).detech().max(1)[0].unsqueeze(1)\n        next_qtargets = self.target_net(next_states).max(1)[0].unsqueeze(1)\n        target_val = rs + gamma * next_qtargets*(1-terminateds)\n        exp_val = self.net(states).gather(1,actions.type(torch.int64))\n\n        loss = torch.nn.functional.mse_loss(exp_val, target_val)\n        self.optimizer.zero_grad()\n        loss.backward()\n        self.optimizer.step()\n\n        self.update(self.net, self.target_net, tau)\n\n    def update(self, n, tn, tau):\n        for target_param, param in zip(tn.parameters(), n.parameters()):\n              target_param.data.copy_(tau * param.data + (1.0 - tau) * target_param.data)         \n\nclass ReplayBuffer:\n    def __init__(self, action_size, bank_size, batch_size):\n        self.batch_size = batch_size\n        self.action_size = action_size\n        self.memories = deque(maxlen=bank_size)\n        self.memory = namedtuple(\"Memory\", field_names=[\"state\",\"action\",\"reward\",\"next_state\",\"terminated\"])\n\n\n    def add(self, state, action, r, next_state, terminated):\n        self.memories.append(self.memory(state, action, r, next_state, terminated))\n\n\n    def sample(self):\n        sampled_mems = random.sample(self.memories, self.batch_size)\n\n        #print(torch.from_numpy(np.vstack([m.state for m in sampled_mems if m is not None])).float().to(device))\n\n        states = torch.from_numpy(np.vstack([m.state for m in sampled_mems if m is not None])).float().to(device)\n        actions = torch.from_numpy(np.vstack([m.action for m in sampled_mems if m is not None])).float().to(device)\n        rs = torch.from_numpy(np.vstack([m.reward for m in sampled_mems if m is not None])).float().to(device)\n        next_states = torch.from_numpy(np.vstack([m.next_state for m in sampled_mems if m is not None])).float().to(device)\n        terminateds = torch.from_numpy(np.vstack([m.terminated for m in sampled_mems if m is not None])).float().to(device)\n        \n        return (states, actions, rs, next_states, terminateds)\n\n    def __len__(self):\n        return len(self.memories)\n\n\n\n\ndef train_dqn(plays, num_eps, max_steps,\n              epsilon, epsilon_min, epsilon_decay,\n              tau, gamma, update_every, batch_size):\n\n    terminal_rewards = []\n\n    for ep_n in range(1, num_eps+1):\n\n        #ENV\n        env = KickoffReturn_ENV()\n        \n        play = random.choice(ordered_plays_db)\n        state = env.reset()\n        current_r = 0\n\n        for step_n in range(max_steps):\n            action_num = agent.act(state, epsilon)\n\n            if step_n < len(play):\n                players = play[step_n]\n            else:\n                players = play[-1]\n                \n            next_state, r, terminated, info = env.step(action_num, players)\n\n        \n            agent.step(state, action_num, r, next_state, terminated, tau, gamma, update_every, batch_size)\n            state = next_state\n            current_r = r\n            if terminated:\n                break\n        terminal_rewards.append(current_r)\n        epsilon = max(epsilon*epsilon_decay, epsilon_min)\n\n        if ep_n % 1000 == 0:\n            print(\"EPISODE:\", ep_n, \"   RETURNED TO:\", current_r)\n\n    return terminal_rewards\n\n#PARAMS\nNUMBER_OF_EPISODES = 100000\nMAX_NUMBER_STEPS = 5000 \nEPSILON = 1.0 \nEPSILON_MIN = 0.01 \nEPSILON_DECAY = 0.995 \nTAU = 0.01 \nGAMMA = 0.99 \nLEARNING_RATE = 0.0005 \nUPDATE_EVERY = 4 \nBATCH_SIZE = 64 \nBANK_SIZE = 100000\nNUM_HIDDEN_NODES = 360\n\n\n\nagent = Agent(132,360, LEARNING_RATE,\n              BATCH_SIZE, BANK_SIZE,NUM_HIDDEN_NODES)\n\n#def __init__(self, state_size, action_size,\n#             learning_rate, batch_size, bank_size, hidden_n):\n    \nscores = train_dqn(ordered_plays_db, NUMBER_OF_EPISODES, MAX_NUMBER_STEPS,\n                   EPSILON, EPSILON_MIN, EPSILON_DECAY, TAU,\n                   GAMMA, UPDATE_EVERY, BATCH_SIZE)\n\n\n\nplt.plot(range(len(scores)), scores)\nplt.title(\"FLAIR Returning Final Spot\")\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Return Result (Yardline)\")\nplt.ylim(-20,120)\nplt.show()\n\navs = []\nfor i in range(len(scores)):\n    avs.append(np.array(scores[0:i]).mean())\n\nplt.plot(range(len(avs)), avs)\nplt.title(\"FLAIR Average Final Spot\")\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Average Return Result (Yardline)\")\nplt.ylim(-20,120)\nplt.show()\n\navs100 = []\nfor j in range(len(scores)):\n    if j > 100:\n        avs100.append(np.array(scores[-100:-1]).mean())\n    \nplt.plot(range(len(avs100)), avs100)\nplt.title(\"FLAIR Average Final Spot of Previous 100 Returns\")\nplt.xlabel(\"Episode\")\nplt.ylabel(\"Average Return Result (Yardline)\")\nplt.ylim(-20,120)\nplt.show()\n","1331fb58":"# 4. Impacts\nVisualization: A development I am currently working on for FLAIR, is the visualization. Currently, it is not animated, but this is coming soon, and we will be able to watch the AI return. I wanted to include this, as it would be great to see the agent move around (I got stuck in the time crunch), but since we can see evidence of improvment of results, we know that there is some learning occurring.\n\nBroadcast: It would great to be able to put a viz in the broadcast (similar to RomoVision) and have the audience follow along and compare the AI to real returners.\n\nA New Statistic: If we say that FLAIR returns optimally, then we could compare a real returners return roucte, and decipher a Optimality Percentage of the player.\n\nCoaching: While not fully accurate to in-game scenarios, FLAIR, with more enhancements, could be a useful coaching tool, as players and coaches could work out patterns and trends that FLAIR finds.\n\n","cf0d7938":"# 2. Simulate!\n\nSo, now that I put my data in an easily accessible structure, it is time to set up the environment. This simplistic simulation allowed me to have the agent interact with the data. I let the agent control the direction it ran. (Later modifications allow us to control things like speed and acceleration, but to start I will just focus on direction, and use the returner's real speed). If it was too close to an opponent, without a teammate to block, then it was tackled and did not score as well, naturally. So, it was incentivised to avoid others and run forward. We also need to confine the AI to the field of play!\n\nSimilarly to the way the data was collected, I used frames, or steps, similar to OpenAI's gym, to create the environment, complete with __init__, reset(), step(), and render() functions. These allowed me to access the simulation and have the agent train. ","f3843339":"# 3. Train!\n\nOnce the environment was set up, I used a Deep Q Network to train the agent. There are extensive technical descriptions online; in short, I allowed the agent to explore, store up memories, and then learn from those memories to dread a Deep Neural Net. This would be allagous to having someone return a kickoff a million times, and as they are running, remember back to previous similar situations, and take an action that was useful before. (I let the agent see the whole field, which is not always accurate, but is a great future modification.)\n\nThe training results are here: We can see a bit of learning, so our agent is getitng better at returning, but this may take a lot of data. (I have more runs going now that I will post as soon as they are done!)\n\n![p2.png](attachment:a8ad56af-ade6-4655-92d0-a32291368bd1.png)","b5f940a7":"# 1. Wrangle\n\nThe key here is to get the data we need, and in the usable format. This means that we have to extract just the \"routine\" kickoffs. These are the ones where the ball is kicked, caught, and returned, with no other events occluding this simple order (muffs, onsides, etc.) Naturally we will only be looking at kickoffs. Due to the tools, I used R to acheive this. \n\nFirst, I created a new ID field by concatenating the Game ID and Play ID (note the new ID is still unique). Then I ordered all of the data by Game and Frame, so the plays were tidy. I sent this CSV over to python for further editting.\n\nOnce I read in the data, I structured this database of plays by:\n* Play\n* * Frame\n* * * (1) Returner\n* * * (2-11) Returner's Teammates\n* * * (12-22) Opponent Players\n\nI only included frames from the point the kick was received and on, as this is the only data my agent needs to learn from.","4dea871b":"Here I present the work for FLAIR: Focused Learning AI Returner. This is AI\/ML agent that takes in tracking data from the NFL of other players on the field and learns optimal return strategies! To accomplish this, I breakdown the process into three parts: Data Wrangling, Environment Development, and Agent Training. I also include mentions of its uses for a New Metric, Broadcast Potential, and Coaching.","e44c923a":"# 5. Discussion\nIt is always good to include some conceptual discussion!\n\nI chose this model not only because of my familiarity with it, but also becuase the idea of practicing and storing memories translates nicely to the problem at hand. Having working with DQN's I was able to leverage some previous knowledge and tinker with the paramters a but to get them to work. Of course, this DQN can be trained more, and refined, which I am currently working on, what I show here is like a proof of concept.\n\nI should note that for something like this, we have a small data set. (I only used 1877 plays, selecting a random one for the agent to train on.) In the scale of Reinforcement learning this can be vary small.\n\nThere is a natural extension to have this become a multi-agent model: the opponents just stop moving after the scripted data ends! Having an AI play for them, as well as the teammates is a geat extension.\n\nI think what woudl be most effective is some reward shaping. Currently, I am just using the yardline of return to inform the agent, but I htink adding in things like broken tackles, avoiding and such would be usefull to."}}