{"cell_type":{"34865d74":"code","73f2101e":"code","53e2987c":"code","b5de1f27":"code","b949a96b":"code","d5ce46a3":"code","72f21f00":"code","1eef4891":"code","e09c06cb":"code","c56e2432":"code","55a87cbd":"code","0dd73fd8":"code","7cf06eac":"code","cd1cbc9f":"code","ebaf3589":"code","ec0a87ec":"code","285e9ba7":"code","2edc3044":"code","dc651034":"code","741d9409":"code","93bd59e3":"code","7cdd4466":"code","bb57ff4b":"code","042d416a":"code","05bc50c8":"code","08215029":"code","2a9951c2":"code","ea1801d0":"code","878632e3":"code","7a78f997":"code","9d609e1d":"code","cd6530e1":"code","df968344":"code","684475cb":"code","f27c2226":"code","c4a7399b":"code","d25df49d":"code","e2a625f7":"code","0b876752":"code","b6470f74":"code","b3c80fb6":"code","4c83af47":"markdown","e7baadf2":"markdown","2fcd7f4f":"markdown","ceba7f8b":"markdown","348d8a79":"markdown","00a134a3":"markdown","9fdcb5c5":"markdown","d5bb8dd5":"markdown","88d9802a":"markdown","bc6719c2":"markdown","5c527c17":"markdown","0ef71268":"markdown","e446d2ab":"markdown","8273483b":"markdown","aaa43eca":"markdown","cc17b2ca":"markdown","1d92b8d2":"markdown","6e6bf093":"markdown","6f3c7347":"markdown","74285c99":"markdown","91338cff":"markdown","c62a58b5":"markdown","b0505d8c":"markdown","46f8d1bb":"markdown","dc223750":"markdown","e5e3399b":"markdown","824e13c9":"markdown","e0e546d0":"markdown","255ce548":"markdown","0378218f":"markdown"},"source":{"34865d74":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport os\nimport os.path\nimport matplotlib.pyplot as plt\n!pip install klib\nimport klib\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve, auc, precision_recall_curve, classification_report, average_precision_score\n\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import precision_score,f1_score\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_error\n\n# import packages for hyperparameters tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","73f2101e":"#input data\nroot_dir = '\/kaggle\/input\/widsdatathon2021\/'\n\nsample_submission = pd.read_csv(os.path.join(root_dir, \n                    'SampleSubmissionWiDS2021.csv'))\nsolution_template = pd.read_csv(os.path.join(root_dir, \n                    'SolutionTemplateWiDS2021.csv'))\nData_dictionary = pd.read_csv(os.path.join(root_dir,   \n                   'DataDictionaryWiDS2021.csv'))\nUnlabled_data = pd.read_csv(os.path.join(root_dir,     \n                'UnlabeledWiDS2021.csv'))\nTraining_data = pd.read_csv(os.path.join(root_dir,     \n                'TrainingWiDS2021.csv'))","53e2987c":"display(Data_dictionary.shape)\ndisplay(Unlabled_data.shape)\ndisplay(Training_data.shape)\ndisplay(solution_template.shape)","b5de1f27":"Training_data.isna().any().any()","b949a96b":"Training_data.drop('Unnamed: 0', axis = 1, inplace = True)","d5ce46a3":"display(Training_data.shape)","72f21f00":"display(Unlabled_data.shape)\nUnlabled_data.head()","1eef4891":"Unlabled_data.isna().any().any()","e09c06cb":"Unlabled_data.drop('Unnamed: 0', axis = 1, inplace = True)","c56e2432":"display(Unlabled_data.shape)\nUnlabled_data.head()","55a87cbd":"Training_data.info(verbose=True, null_counts=True)","0dd73fd8":"Training_data.dtypes.value_counts()","7cf06eac":"Training_data.isnull().sum()","cd1cbc9f":"def calc_missing_values(df_name):\n    \n    '''\n    Returns total number and percentage of missing value in each column of a\n    given dataframe.    \n    '''\n    # sum of missing values in each column\n    missing_values = df_name.isnull().sum() \n    \n    # percentage of missing values in each column\n    per_missing = df_name.isnull().sum() * 100 \/ len(df_name)\n    \n    # Table with sum and percentage of missing values\n    missing_table = pd.concat([missing_values, per_missing],axis = 1)\n        \n    # Assign column names\n    missing_table_rename = missing_table.rename(columns ={0: 'Missing Values', 1:'% of missing values'})\n    \n    # Sort it by percentage of missing values\n    \n    sorted_table = missing_table_rename[missing_table_rename.iloc[:,1] !=0].\\\n    sort_values('% of missing values', ascending = False).round(1)\n    \n    print('Out of ' + str(df_name.shape[1])+ ' columns in this dataframe '+ str(sorted_table.shape[0])+ \\\n                         ' columns have missing values')\n    \n    return sorted_table\n        ","ebaf3589":"# Training data\nmissing_train = calc_missing_values(Training_data)\nmissing_train[:20].style.background_gradient(cmap='viridis')","ec0a87ec":"# Test data\nmissing_test = calc_missing_values(Unlabled_data)\nmissing_test[:20].style.background_gradient(cmap='cividis')","285e9ba7":"train_df = klib.data_cleaning(Training_data) # removes duplicate and empty row\/col","2edc3044":"test_df = klib.data_cleaning(Unlabled_data) # removes duplicate and empty row\/cols#","dc651034":"#train_df = Training_data\n#test_df = Unlabled_data","741d9409":"train_df['diabetes_mellitus'].value_counts(normalize = True)","93bd59e3":"train_df['diabetes_mellitus'].astype(int).plot.hist();","7cdd4466":" train_df.dtypes","bb57ff4b":"cat_col_train = Training_data.select_dtypes('object').columns\ndisplay(len(cat_col_train))\ndisplay(cat_col_train)","042d416a":"cat_col_test = Unlabled_data.select_dtypes('object').columns\ndisplay(len(cat_col_test))\ndisplay(cat_col_test)","05bc50c8":"#klib.cat_plot(test_df)","08215029":"cat_list = Training_data.select_dtypes('object').columns\ndisplay(cat_list)","2a9951c2":"# Creating Label Encoder object\nle = LabelEncoder()\nfor ob in cat_list:\n    train_df[ob] = le.fit_transform(train_df[ob].astype(str))\n    test_df[ob] = le.fit_transform(test_df[ob].astype(str))\nprint(train_df.info())    \nprint(test_df.info()) ","ea1801d0":"train_df.fillna(-9999,inplace = True)\ntrain_df.isnull().sum()","878632e3":"test_df.fillna(-9999,inplace = True)\ntest_df.isnull().sum()","7a78f997":"Target = 'diabetes_mellitus'\ntrain_labels = train_df[Target]\ntrain_df_NT = train_df.drop(columns = [Target])\nfeatures = list(train_df_NT.columns)\nprint('Training data shape:', train_df_NT.shape)\nprint('Test data shape:', test_df.shape)","9d609e1d":"X, y = train_df_NT, train_labels","cd6530e1":"#create the train and validation set for cross-validation\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=123)","df968344":"import tensorflow as tf\nfrom tensorflow import keras","684475cb":"import kerastuner as kt","f27c2226":"def model_builder(hp):\n  model = keras.Sequential()\n  model.add(keras.layers.Flatten(input_shape=[172]))\n  \n  # Tune the number of units in the first Dense layer\n  # Choose an optimal value between 32-512\n  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n  model.add(keras.layers.Dense(units=hp_units, activation='relu'))\n  model.add(keras.layers.Dense(10))\n\n  # Tune the learning rate for the optimizer\n  # Choose an optimal value from 0.01, 0.001, or 0.0001\n  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n\n  model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n                loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                metrics=['accuracy'])\n\n  return model","c4a7399b":"tuner = kt.Hyperband(model_builder,\n                     objective='val_accuracy',\n                     max_epochs=10,\n                     factor=3,\n                     directory='my_dir',\n                     project_name='intro_to_kt')","d25df49d":" stop_early = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)","e2a625f7":"tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[stop_early])\n\n# Get the optimal hyperparameters\nbest_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n\nprint(f\"\"\"\nThe hyperparameter search is complete. The optimal number of units in the first densely-connected\nlayer is {best_hps.get('units')} and the optimal learning rate for the optimizer\nis {best_hps.get('learning_rate')}.\n\"\"\")","0b876752":"# Build the model with the optimal hyperparameters and train it on the data for 50 epochs\nmodel = tuner.hypermodel.build(best_hps)\nhistory = model.fit(X_train, y_train, epochs=50, validation_split=0.2)\n\nval_acc_per_epoch = history.history['val_accuracy']\nbest_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\nprint('Best epoch: %d' % (best_epoch,))","b6470f74":"hypermodel = tuner.hypermodel.build(best_hps)\nRetrain the model\nhypermodel.fit(X_val, y_val, epochs=best_epoch)","b3c80fb6":"eval_result = hypermodel.evaluate(X_val, y_val)\nprint(\"[test loss, test accuracy]:\", eval_result)","4c83af47":"To finish this tutorial, evaluate the hypermodel on the test data.","e7baadf2":"## Test data","2fcd7f4f":"Run the hyperparameter search. The arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above.","ceba7f8b":"## Missing values","348d8a79":"# Train the model\nFind the optimal number of epochs to train the model with the hyperparameters obtained from the search.","00a134a3":"## <span style='color:purple'> Input data <\/span>","9fdcb5c5":"### Column types in training and test data","d5bb8dd5":"## <span style='color:purple'>  Import libraries <\/span>","88d9802a":"There is class imbalance in this dataset.","bc6719c2":"Test data has 10234 entries and 179 variables which is 1 less than the training data due to the presence of TARGET column.","5c527c17":"# References\n\nhttps:\/\/www.tensorflow.org\/tutorials\/keras\/keras_tuner\n","0ef71268":"This column do not contain useful information so let's drop it.","e446d2ab":"Re-instantiate the hypermodel and train it with the optimal number of epochs from above.","8273483b":"There are some missing data in training data.","aaa43eca":"# Missing Values","cc17b2ca":"### Number of unique column datatypes ","1d92b8d2":"The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket. The algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. Hyperband determines the number of models to train in a bracket by computing 1 + logfactor(max_epochs) and rounding it up to the nearest integer.\n\nCreate a callback to stop training early after reaching a certain value for the validation loss.","6e6bf093":"Training data has 130157 entries and 180 variables. ","6f3c7347":"## Training data","74285c99":"# Categorical features","91338cff":"# <span style='color:purple'>  Women in Data Science Datathon 2021       <\/span>\n\n<div style=\"text-align: justify;\n             font-size:18px\">\nObjective of <span style='color:purple'> WiDS Datathon 2021  <\/span> is to develop models and make predictions to determine whether a patient admitted to ICU has been diagnosed with a particular type of diabetes, Diabetes Mellitus, using labeled training data from the first 24 hours of intensive care.\n    <\/div>","c62a58b5":"There are three unique datatypes.","b0505d8c":"Training data has 130157 entries and 181 variables. ","46f8d1bb":"Define our model just like we did for the regression tasks, with one exception. In the final layer include a 'sigmoid' activation so that the model will produce class probabilities.","dc223750":"## <span style='color:purple'> Data Exploration <\/span>\n","e5e3399b":"# Missing  data handling","824e13c9":"## Target Column in Training data","e0e546d0":"### Columns","255ce548":" The model builder function returns a compiled model and uses hyperparameters you define inline to hypertune the model.","0378218f":"# Instantiate the tuner and perform hypertuning\nInstantiate the tuner to perform the hypertuning. The Keras Tuner has four tuners available - RandomSearch, Hyperband, BayesianOptimization, and Sklearn. In this tutorial, you use the Hyperband tuner.\n\nTo instantiate the Hyperband tuner, you must specify the hypermodel, the objective to optimize and the maximum number of epochs to train (max_epochs)"}}