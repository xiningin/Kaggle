{"cell_type":{"74c1917e":"code","97a314b0":"code","1a5d6421":"code","7c5b93fd":"code","ba1c0d6e":"code","4fec8745":"code","a3529994":"code","42991368":"code","e5d737c2":"code","3cd1018d":"code","ddb0cbd5":"code","4fd914d5":"code","e6a02e8e":"code","56a60542":"code","17a508bd":"code","1624fd28":"code","e6a2a4be":"code","74c56e2d":"code","13d363c8":"code","2aa8cf4f":"code","7089b8b9":"code","c5349198":"code","772c0f8c":"code","a167a406":"code","3a0c75c9":"code","bc67f2cd":"code","5d8d4cb6":"code","1978ab84":"code","1b592489":"code","6e54e91f":"markdown","bfedd93b":"markdown","9919256a":"markdown","54b36a0f":"markdown","e6ba561a":"markdown","9c92f9d6":"markdown","0001e5af":"markdown","c3c8c31d":"markdown","8289f1f0":"markdown","09cde345":"markdown","aab47c19":"markdown","c97d428e":"markdown","eec84512":"markdown","4ec22283":"markdown","185731e7":"markdown","fa1cc2f0":"markdown","8ca9d0d4":"markdown"},"source":{"74c1917e":"#importing all the required librarys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","97a314b0":"#loading the dataset and reading csv\ndf= pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","1a5d6421":"#taking a peek into dataframe\ndf.head(5)","7c5b93fd":"# the number of rows and columns in a dataset\ndf.shape","ba1c0d6e":"# statistics for all columns of the dataset\ndf.describe()","4fec8745":"#columns names in the dataset\ndf.columns","a3529994":"df.nunique( )","42991368":"# information of dataset\ndf.info()","e5d737c2":"# all columns are now lowercase letters\ndf.columns=df.columns.str.strip().str.lower()\ndf.columns","3cd1018d":"df.isnull().sum( )","ddb0cbd5":"for i in df:\n   \n    fig, axs = plt.subplots(1,2,figsize=(15, 3))\n\n    sns.histplot(df[i],bins=20, kde=True,ax=axs[0]);\n    sns.boxplot(df[i], ax = axs[1], color='#99befd', fliersize=1);","4fd914d5":"# lets see the correlation between eachother by using heatmap\nfig, ax = plt.subplots(figsize=(12,8))\nmask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\nsns.heatmap(df.corr(), annot=True, cmap=\"Reds\", mask=mask, linewidth=0.5)\n","e6a02e8e":"# spliting training and testing data\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop(['quality'], axis = 1)\ny = df['quality']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=27)","56a60542":"# Scaling using MinMax Scaler\nfrom sklearn.preprocessing import MinMaxScaler\n\ncols = list(X_train.columns)\n\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_train = pd.DataFrame(X_train, columns=[cols])\n\nX_test = scaler.transform(X_test)\nX_test = pd.DataFrame(X_test, columns=[cols])\n\nX_train.describe()","17a508bd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score","1624fd28":"# Initiatlize the model\nlogreg = LogisticRegression(solver='liblinear', random_state = 0)\n\n# Fit the model\nlogreg.fit(X_train, y_train)\n\n# Predict data points \ny_pred_test = logreg.predict(X_test)\n\n# Print accuracy scores\nprint(f'Model accuracy score: {round(accuracy_score(y_test, y_pred_test) * 100, 2)}%')","e6a2a4be":"# Additional Metrics\n\nprint(f'Training set score: {round(logreg.score(X_train, y_train) * 100, 2)}%')\nprint(f'Test set score: {round(logreg.score(X_test, y_test) * 100, 2)}%')\n\n# Calculating null accuracy\n#null_accuracy = (y_test.value_counts()[0]\/(y_test.value_counts()[0]+y_test.value_counts()[1]))\n#print(f'Null Accuracy score: {round(null_accuracy * 100, 2)}%')","74c56e2d":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_jobs=-1, random_state=42)","13d363c8":"model.fit(X_train, y_train)","2aa8cf4f":"print((model.score(X_train, y_train))*100)\nprint((model.score(X_test, y_test))*100)","7089b8b9":"# Selecting Classifiers and inistaniate models\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score \nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score\n","c5349198":"from sklearn.preprocessing import StandardScaler\nscalar = StandardScaler()\ndf_scaled = pd.DataFrame(scalar.fit_transform(df), columns=df.columns)\ndf_scaled.head()","772c0f8c":"from sklearn.decomposition import PCA","a167a406":"pca = PCA(n_components=.95)","3a0c75c9":"df_pca = pd.DataFrame(pca.fit_transform(df_scaled))\nx = df_pca \nx","bc67f2cd":"import matplotlib.pyplot as plt\npd.DataFrame(pca.explained_variance_ratio_).plot.bar()\nplt.legend('')\nplt.xlabel('Principal Components')\nplt.ylabel('Explained Varience');","5d8d4cb6":"#splitting dataset into a training set and test set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.2, random_state = 0)","1978ab84":"\nfrom sklearn.linear_model import LogisticRegression\n\n   # classifier = LogisticRegression(random_state = 0)\nlogmodel = LogisticRegression(solver='liblinear')\n    \nlogmodel.fit(X_train, Y_train)","1b592489":"\ny_pred = logmodel.predict(X_test)\n\nprint(\"accuracy score:\", accuracy_score(Y_test,y_pred)*100)","6e54e91f":"## The data is cleaned and ready for modeling","bfedd93b":"# MODEL : Logstic Regression","9919256a":"## 4.\n### cheking for multicolinarity","54b36a0f":"## Accuracy score: 93.125","e6ba561a":"# MODEL : LOGISTIC REGRESSION","9c92f9d6":"# Data cleaning\n\n### 1.\n### we start with making all the column names into Lowercase","0001e5af":"#### we have found enough information from the dataset and it time to clean it for the best fit","c3c8c31d":"# MODEL : RANDOM FOREST","8289f1f0":"## Lets try PCA and see how it goes better on this data","09cde345":"* we can see that 5 pcs are explaining 80% of target variable","aab47c19":"### Beware of below code take a lot of time to run ","c97d428e":"## lets standardize the data before applying the PCA","eec84512":"###  Lets first split the data first ","4ec22283":"#### ","185731e7":"### 2.\n### we shall deal with missing and null values","fa1cc2f0":"## Data Wrangling\n### General Properties","8ca9d0d4":"\n## 6.\n## Datascaling"}}