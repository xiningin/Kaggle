{"cell_type":{"ac50659e":"code","f5b87863":"code","cb6737b9":"code","d8efee1a":"code","4f86dcc3":"code","575c9606":"code","80617030":"code","f6a660cc":"code","1b81175f":"code","00e193a1":"code","ef4702b0":"code","11aac482":"code","f8e7e32e":"code","58c3f840":"code","10951737":"markdown","7a399219":"markdown","67cc0b2d":"markdown","35bd82ff":"markdown","a71d7c3f":"markdown","a57a48f6":"markdown","3bc5073e":"markdown","6882415b":"markdown","6424a3e5":"markdown","6fdcc886":"markdown","e772abc9":"markdown","b3fc51fe":"markdown","1ec8b956":"markdown","1d7703f5":"markdown","855071c2":"markdown","064f27ab":"markdown","c4f12987":"markdown","a6294ef5":"markdown"},"source":{"ac50659e":"# Import some libraries\n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\nimport matplotlib.pyplot as plt # graphing\nimport os\nfrom datetime import datetime, timedelta # Used to subtract days from a date\n\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\n\n# Import environment\nfrom kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","f5b87863":"# Import training dataset\n(market_train_df, news_train_df) = env.get_training_data()","cb6737b9":"# Process news data function\ndef process_news_date(news_train_df):\n    # Define which columns I don't want - I just used my intuition to select these columns\n    news_columns_to_drop = ['firstCreated','sourceId','headline','takeSequence','provider','subjects','audiences','bodySize','companyCount','headlineTag','sentenceCount','assetCodes','firstMentionSentence','noveltyCount12H','noveltyCount24H','noveltyCount3D','noveltyCount5D','noveltyCount7D','volumeCounts12H','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\n    # Drop the columns chosen from above\n    news_train_df.drop(columns=news_columns_to_drop,inplace=True)\n    # Create sentiment word ratio from sentimentWordCount and wordCount <- i think this feature is helpful.\n    news_train_df['sentimentWordRatio'] = news_train_df['sentimentWordCount']\/news_train_df['wordCount']\n    # Drop sentimentWordCount and wordCount since they are incorporated into the new column sentimentWordRatio now\n    news_columns_to_drop = ['wordCount','sentimentWordCount']\n    news_train_df=news_train_df.drop(columns=news_columns_to_drop)\n    #return the news dataframe\n    return news_train_df","d8efee1a":"# Separate 'date' into year,month, and day. Then, add year,month, and day to the 'assetName'.\n# Performing this will allow me to merge news & market data with this new 'combined_index' column\ndef combined_index(df):\n    df['combined_index'] = (df['time'].dt.year).astype(str)+(df['time'].dt.month).astype(str)+(df['time'].dt.day).astype(str)+(df['assetName']).astype(str)\n    return df\n\n# mergy market & news data by 'combined_index'\ndef merge_market_news(market_df,news_df):\n    # By having .mean(), it will take average of numeric values if there are duplicate news for the same 'combined_index'\n    news_df = news_df.groupby('combined_index').mean()\n    # merge news data to market data using the 'combined_index' we created\n    market_df=market_df.merge(news_df,how='left',on='combined_index')\n    # since there are more items in market data, ther are lots of rows with NaNs, and we fill them with 0 for training purposes.\n    fill_na_columns = ['urgency','marketCommentary','relevance','sentimentClass','sentimentNegative','sentimentNeutral','sentimentPositive','sentimentWordRatio']\n    market_df[fill_na_columns]=market_df[fill_na_columns].fillna(0)\n    return market_df","4f86dcc3":"# Process news data\nnews_train_df=process_news_date(news_train_df)\n# Create 'combined_index' for news dataframe\nnews_train_df=combined_index(news_train_df).copy()\n# Create 'combined_index' for market dataframe\nmarket_train_df=combined_index(market_train_df).copy()\n# Merge market & news data\nmarket_train_df=merge_market_news(market_train_df,news_train_df)","575c9606":"# Pre-processes market data for training\ndef pre_process_market_data(market_train_df):\n    # Let's remove outliers based on our EDA. Remove anything outside [-1,1] for 'returnsOpenNextMktres10'\n    market_train_df = (market_train_df[(market_train_df['returnsOpenNextMktres10']<1) & (market_train_df['returnsOpenNextMktres10']>-1)]).copy()\n    # Let's choose our features\n    features = ['time','universe','volume','returnsClosePrevRaw1','returnsOpenPrevRaw1','returnsClosePrevRaw10','returnsOpenPrevRaw10','urgency','marketCommentary','relevance','sentimentClass','sentimentNegative','sentimentNeutral','sentimentPositive','sentimentWordRatio']\n    x = market_train_df[features].copy()\n    y = market_train_df[['returnsOpenNextMktres10','universe','time']].copy()\n    return x,y\n\n# Pre-processes market data for prediction for actual scoring. We are not provided with 'returnsOpenNextMktres10', hence no outlier removal is needed and we don't need to output target data.\ndef pre_process_market_data_actual_competition(market_train_df):\n    # Let's choose our features\n    features = ['volume','returnsClosePrevRaw1','returnsOpenPrevRaw1','returnsClosePrevRaw10','returnsOpenPrevRaw10','urgency','marketCommentary','relevance','sentimentClass','sentimentNegative','sentimentNeutral','sentimentPositive','sentimentWordRatio']\n    x = market_train_df[features]\n    return x","80617030":"x,y=pre_process_market_data(market_train_df)","f6a660cc":"# Splits data for training. Takes out 30 days worth of data between training and validation set to prevent data leakage\ndef split_train_test_and_time(x,y,test_size):    \n    # Splits data as specified test_size and creates a gap of 30 days between train and test. This helps data leakage so that the model doesn't know the future when training\n    X_train = x[x['time']<(x['time'][int(len(x)*(1-test_size))]-timedelta(days=30))]\n    y_train = y[y['time']<(y['time'][int(len(x)*(1-test_size))]-timedelta(days=30))]\n    X_test = x[x['time']>x['time'][int(len(x)*(1-test_size))]]\n    y_test = y[y['time']>y['time'][int(len(y)*(1-test_size))]]   \n    # Features to be used\n    features_no_universe = ['volume','returnsClosePrevRaw1','returnsOpenPrevRaw1','returnsClosePrevRaw10','returnsOpenPrevRaw10','urgency','marketCommentary','relevance','sentimentClass','sentimentNegative','sentimentNeutral','sentimentPositive','sentimentWordRatio']\n    # Filters out data with universe==0\n    # X_train = X_train[X_train.universe==1]\n    # y_train = y_train[y_train.universe==1]  \n    # Save time for calculating score later. It is used to group and sum x_t values each day\n    train_time = X_train['time']\n    X_train = X_train[features_no_universe]\n    y_train = y_train['returnsOpenNextMktres10']    \n    # Filters out data with universe==0 for accurate scoring\n    X_test = X_test[X_test.universe==1]\n    y_test = y_test[y_test.universe==1]\n    # Save time for calculating score later. It is used to group and sum x_t values each day\n    test_time = X_test['time']\n    X_test = X_test[features_no_universe]\n    y_test = y_test['returnsOpenNextMktres10']   \n    return X_train,X_test,y_train,y_test,train_time,test_time\n\n# Draw graph of train vs eval scores. Visualize training process once it's done\ndef draw_train_eval_graph(evals_result,params):\n    x_axix = range(1,len(evals_result['train']['sigma_score'])+1)\n    train_sigma_score = evals_result['train']['sigma_score']\n    eval_sigma_score = evals_result['eval']['sigma_score']\n\n    plt.plot(x_axix,train_sigma_score,label='Train')\n    plt.plot(x_axix,eval_sigma_score,label='Eval')\n    plt.legend()\n    print(\"eta: \",params['eta'],\", max_depth: \",params['max_depth'])\n\n# This will display real target vs predictions. Kind of a sanity check..\ndef compare_real_target_with_pred(x,y):\n    # Compare predict and actual nextMKTres side by side\n    input_for_pred = xgb.DMatrix(x.values)\n    y_pred = bst.predict(input_for_pred,ntree_limit = bst.best_ntree_limit)\n    data  = {'y_real':y.values, 'y_pred': y_pred}\n    print(pd.DataFrame(data))","1b81175f":"# sigma_score function is considered as a custom evaluation metric for xgboost\n# example of how custom evaluation function is incorporated into xgboost's training can be found here : https:\/\/github.com\/dmlc\/xgboost\/blob\/master\/demo\/guide-python\/custom_objective.py\ndef sigma_score(preds,dval):\n    # get y_target values\n    labels = dval.get_label()\n    # call time parameter to be used for grouping, so that we can add x_t values for each day\n    df_time = dval.params['extra_time']\n    # instead of making any prediction above 0 as 1, I chose anything above the mean of predictions (I call it market average) to be 1\n    preds[preds>preds.mean()]=1\n    # anything between market average(prediction mean) and 0 were given 0. \n    preds[(preds<=preds.mean())&(preds>=0)]=0\n    # any asset giving negative return...... -1 \n    preds[preds<0]=-1\n    # I assume you can take below approach too\n    #preds[preds>0]=1\n    #preds[preds<=0]=-1\n    \n    #calculate x_t and score as specified by the competition\n    x_t = pd.Series(preds*labels)\n    x_t_sum = x_t.groupby(df_time).sum()    \n    score = (x_t_sum.mean())\/(x_t_sum.std())\n    return 'sigma_score', round(score,5)","00e193a1":"import xgboost as xgb\n\n# remember, this is not train_test_split from sklearn. This is my own function. It devides data with 30 days gap and doesn't allow the model to look into the future.\nX_train,X_val,y_train,y_val,train_time,val_time=split_train_test_and_time(x,y,test_size=0.2)\n\n# Define datasets that xgboost accepts\nxgtrain = xgb.DMatrix(X_train.values,y_train.values)\nxgval = xgb.DMatrix(X_val.values,y_val.values)\n\n# We will 'inject' an extra parameter in order to have access to df_valid['time'] inside sigma_score without globals\nxgtrain.params = {'extra_time': train_time.factorize()[0]}\nxgval.params = {'extra_time': val_time.factorize()[0]}\n\n# define parameters. I found learning rate of 0.3 and max_depth of 6 to be suitable.\nparams ={'eta':0.5, 'max_depth':5,'objective':'reg:linear','silent':1,'eval_metric':'rmse'}\n# this allows cross validation. Make sure eval data is the latter one, so that the model will do an early stopping if eval data's sigma score doesn't increase.\n# We want the training to stop when eval data's sigma score doesn't increase so that we don't overfit our model to the training data\nevallist = [(xgtrain,'train'),(xgval,'eval')]\n# Save evaluation metric scores for displaying later\nevals_result = {}\n# perform 400 rounds at maximum if early stopping doesn't happen\nnum_round = 400\n# here, one thing to note is that our custom evaluation function 'sigma_score' is passed into 'feval'.\nbst = xgb.train(params,xgtrain,num_round,evallist,evals_result=evals_result,feval=sigma_score,maximize=True,early_stopping_rounds=50,verbose_eval=10)\n# # If early stopping is enabled during training, you can get predictions from the best iteration by using this-> y_test_pred = bst.predict(xgtest,ntree_limit = bst.best_ntree_limit)\n","ef4702b0":"# Compare predict and actual nextMKTres side by side\ncompare_real_target_with_pred(X_val,y_val)","11aac482":"# Compare predict and actual nextMKTres side by side\ncompare_real_target_with_pred(X_train,y_train)","f8e7e32e":"draw_train_eval_graph(evals_result,params)","58c3f840":"# Use functions defined previously to process data for the final submission\ndef make_my_confidence_predictions(market_obs_df,predictions_template_df):#, news_obs_df, predictions_template_df):\n    x=pre_process_market_data_actual_competition(market_obs_df).copy()\n    x = xgb.DMatrix(x.values)\n    predictions_template_df.confidenceValue = bst.predict(x,ntree_limit = bst.best_ntree_limit)\n    predictions_template_df.confidenceValue[predictions_template_df.confidenceValue>predictions_template_df.confidenceValue.mean()]=1\n    predictions_template_df.confidenceValue[(predictions_template_df.confidenceValue<=predictions_template_df.confidenceValue.mean())&(predictions_template_df.confidenceValue>=0)]=0\n    predictions_template_df.confidenceValue[predictions_template_df.confidenceValue<0]=-1\n    return predictions_template_df\n\nfor (market_obs_df, news_obs_df, predictions_template_df) in env.get_prediction_days():    \n    # Process news data\n    news_obs_df=process_news_date(news_obs_df)\n    # Truncate date so that both news and market can be combined\n    news_obs_df=combined_index(news_obs_df).copy()\n    # Truncate date so that both news and market can be combined\n    market_obs_df=combined_index(market_obs_df).copy()\n    market_obs_df=merge_market_news(market_obs_df,news_obs_df)\n    market_obs_df=pre_process_market_data_actual_competition(market_obs_df)\n    \n    predictions_df = make_my_confidence_predictions(market_obs_df, predictions_template_df)\n    env.predict(predictions_df)\nprint('Done!')\n# Write submission file    \nenv.write_submission_file()\n","10951737":"# 1.) Import Environment","7a399219":"### Function","67cc0b2d":"# 7.) Sigma Score Function","35bd82ff":"### Train Data","a71d7c3f":"### Functions Used","a57a48f6":"### Perform Data Processing on News Data & Merge it with Market Data","3bc5073e":"# 4.) Merge News Data & Market Data","6882415b":"# 2.) Load Initial Market & News Train Data","6424a3e5":"### Graph History of Learning for Val Data and Train Data","6fdcc886":"# 9.) Some Sanity Checks. Real vs Pred","e772abc9":"# 8.) Train Model - Evaluation of model","b3fc51fe":"### Function","1ec8b956":"# 6.) Other Functions","1d7703f5":"# 10.) For Final Submission","855071c2":"# 5.) Market Data Processing","064f27ab":"\n### Val Data","c4f12987":"# 3.) News Data Processing","a6294ef5":"### Perform Data Processing on Market Data"}}