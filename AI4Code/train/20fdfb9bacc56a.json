{"cell_type":{"ef80fe5f":"code","0cad3054":"code","f6878a76":"code","72dc3937":"code","baddf207":"code","614c6cce":"code","6660c075":"code","fef32a86":"code","9aaf1698":"code","5538974c":"code","6fd5defa":"code","6f1f351b":"code","d365966b":"code","5ccca90f":"code","4159d6d2":"code","3a76bd23":"markdown","878d7f13":"markdown","f668cff0":"markdown","6292813d":"markdown","0200e675":"markdown","eaf32380":"markdown","776c193a":"markdown","d95e6892":"markdown","e7643722":"markdown","721a83f8":"markdown","935fd950":"markdown","e612d8c2":"markdown"},"source":{"ef80fe5f":"#Generic Packages\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport datetime  # For TensorBoard\n\n#SK Learn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.utils import shuffle           \n\n#Plotting Libraries\nimport seaborn as sn; sn.set(font_scale=1.4)\nimport matplotlib.pyplot as plt             \n\n#openCV\nimport cv2                                 \n\n#Tensor Flow\nimport tensorflow as tf   \nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow import keras\n\n#Display Progress\nfrom tqdm import tqdm\n\n#Garbage Collector\nimport gc","0cad3054":"class_names = ['airplane', 'car', 'cat', 'dog', 'flower', 'fruit', 'motorbike', 'person']\nclass_names_label = {class_name:i for i, class_name in enumerate(class_names)}\n\nnb_classes = len(class_names)\n\nIMAGE_SIZE = (150, 150)","f6878a76":"#Function to Load Images & Labels\ndef load_data():\n    \n    datasets = ['..\/input\/image-dataset\/_train', '..\/input\/image-dataset\/_test']\n    output = []\n    \n    # Iterate through training and test sets\n    for dataset in datasets:\n        \n        images = []\n        labels = []\n        \n        print(\"Loading {}\".format(dataset))\n        \n        # Iterate through each folder corresponding to a category\n        for folder in os.listdir(dataset):\n            label = class_names_label[folder]\n            \n            # Iterate through each image in our folder\n            for file in tqdm(os.listdir(os.path.join(dataset, folder))):\n                \n                # Get the path name of the image\n                img_path = os.path.join(os.path.join(dataset, folder), file)\n                \n                # Open and resize the img\n                image = cv2.imread(img_path)\n                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n                image = cv2.resize(image, IMAGE_SIZE) \n                \n                # Append the image and its corresponding label to the output\n                images.append(image)\n                labels.append(label)\n                \n        images = np.array(images, dtype = 'float32')\n        labels = np.array(labels, dtype = 'int32')   \n        \n        output.append((images, labels))\n\n    return output","72dc3937":"#Loading Data (Training & Test Dataset)\n(train_images, train_labels), (test_images, test_labels) = load_data()","baddf207":"# Shuffle Training Dataset\ntrain_images, train_labels = shuffle(train_images, train_labels, random_state=25)","614c6cce":"#Label Dataset Shape\nn_train = train_labels.shape[0]\nn_test = test_labels.shape[0]\n\nprint (\"Number of training examples: {}\".format(n_train))\nprint (\"Number of testing examples: {}\".format(n_test))\nprint (\"Each image is of size: {}\".format(IMAGE_SIZE))","6660c075":"#Scale the data\ntrain_images = train_images \/ 255.0\ntest_images = test_images \/ 255.0","fef32a86":"#Build Model\n\ndef create_model():\n    return tf.keras.Sequential([\n    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (150, 150, 3)), \n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Conv2D(32, (3, 3), activation = 'relu'),\n    tf.keras.layers.MaxPooling2D(2,2),\n    tf.keras.layers.Flatten(),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(8, activation=tf.nn.softmax)\n])\n\n#Creating Model\nmodel = create_model()","9aaf1698":"# Custom Class for LR Finder\n\nclass LRFinder(Callback):\n    \"\"\"Callback that exponentially adjusts the learning rate after each training batch between start_lr and\n    end_lr for a maximum number of batches: max_step. The loss and learning rate are recorded at each step allowing\n    visually finding a good learning rate as per https:\/\/sgugger.github.io\/how-do-you-find-a-good-learning-rate.html via\n    the plot method.\n    \"\"\"\n\n    def __init__(self, start_lr: float = 1e-7, end_lr: float = 10, max_steps: int = 100, smoothing=0.9):\n        super(LRFinder, self).__init__()\n        self.start_lr, self.end_lr = start_lr, end_lr\n        self.max_steps = max_steps\n        self.smoothing = smoothing\n        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n        self.lrs, self.losses = [], []\n\n    def on_train_begin(self, logs=None):\n        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n        self.lrs, self.losses = [], []\n\n    def on_train_batch_begin(self, batch, logs=None):\n        self.lr = self.exp_annealing(self.step)\n        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)\n\n    def on_train_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        loss = logs.get('loss')\n        step = self.step\n        if loss:\n            self.avg_loss = self.smoothing * self.avg_loss + (1 - self.smoothing) * loss\n            smooth_loss = self.avg_loss \/ (1 - self.smoothing ** (self.step + 1))\n            self.losses.append(smooth_loss)\n            self.lrs.append(self.lr)\n\n            if step == 0 or loss < self.best_loss:\n                self.best_loss = loss\n\n            if smooth_loss > 4 * self.best_loss or tf.math.is_nan(smooth_loss):\n                self.model.stop_training = True\n\n        if step == self.max_steps:\n            self.model.stop_training = True\n\n        self.step += 1\n\n    def exp_annealing(self, step):\n        return self.start_lr * (self.end_lr \/ self.start_lr) ** (step * 1. \/ self.max_steps)\n\n    def plot(self):\n        fig, ax = plt.subplots(1, 1)\n        ax.set_ylabel('Loss')\n        ax.set_xlabel('Learning Rate')\n        ax.set_xscale('log')\n        ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n        ax.plot(self.lrs, self.losses)","5538974c":"# -- Model Parameters --\n\nepochs = 10\nbatch_size = 64\nval_size = 0.1","6fd5defa":"# -- Implementing Technique#1 --\nlr_finder = LRFinder()\n\n# Optimizer & Learning Rate\noptimizer = keras.optimizers.Adam()\n\n#Compile Model\nmodel.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Fit Model\nhistory = model.fit(train_images, train_labels, \n                    batch_size=batch_size, \n                    epochs=epochs, \n                    validation_split = val_size, \n                    verbose=-1,\n                    callbacks=[lr_finder]             # LR Finder\n                   )\n\n\n# Plot Loss vs Learning Rate\nlr_finder.plot()","6f1f351b":"# UPDATED Learning Rate\nlr = 1e-02\noptimizer = keras.optimizers.Adam(learning_rate = lr)\n\n\n#Re-Compile Model\nmodel.compile(optimizer = optimizer, loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n\n\n# Re-Fit the Model\nhistory = model.fit(train_images, train_labels, \n                    batch_size=batch_size, \n                    epochs=epochs, \n                    validation_split = val_size, \n                    verbose=-1)\n\n\n# Evaluate Model\ntest_loss = model.evaluate(test_images, test_labels, verbose=0)\nprint(\"\\n Model Accuracy (after): \",'{:.2%}'.format(test_loss[1]))","d365966b":"#garbage collection to save memory\ngc.collect()","5ccca90f":"# -- Define Logging Directory & TimeStamp --\n#log_dir = \"logs\/fit\/\" + datetime.datetime.now().strftime(\"%d%m%Y-%H%M\")\n\n# --TensorBoard CallBack --\n#tensorBoard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n\n# -- Fitting the Model with TensorBoard CallBack --\n#history = model2.fit(train_images, train_labels, \n#                    batch_size=batch_size, \n#                    epochs=epochs, \n#                    validation_split = val_size, \n#                    callbacks=[tensorBoard_callback],\n#                    verbose=-1)","4159d6d2":"# -- Load the TensorBoard notebook extension --\n#%load_ext tensorboard\n\n# -- Starting TensorBoard --\n#%tensorboard --logdir logs\/fit","3a76bd23":"**Learning Rate:** Let's just admit that, choosing a good learning rate (LR) is very crucial for training a neural-network (NN) model in deep learning. Assuming that we are training our model on a large dataset, choosing a low LR may lead to a high training time, whereas a high LR may lead to training divergence. Either of which is not good when dealing with a crucial deep-learning problem.\n\nHaving said that, there are ways to estimate a good learning rate. Following are the details.\n\n1. Technique#1\n2. Technique#2\n\n\n### Technique#1:\n\nThis technique is popularized by [FastAI](https:\/\/docs.fast.ai\/callbacks.lr_finder.html) & has a good implementation of learning rate finder. For detailed explanation about this technique please check the article [here](https:\/\/sgugger.github.io\/how-do-you-find-a-good-learning-rate.html)\n\nIn this technique:\n\n* We start with a low LR typically 1-e7\n* Then after each batch run, we increase the LR and record the loss against the LR\n* We stop when we reach a high LR (typically 10+) or the loss value is exponentially high\n* We plot a LR vs Loss chart and choose a LR where the loss decreases at rapid rate (steep curve)\n\n\n### Technique#2:\n\nUse [TensorBoard](https:\/\/www.tensorflow.org\/tensorboard) to do the visualization for you :-). To give a short intro, TensorBoard is a visualization software that comes with any standard TensorFlow installation. TensorBoard helps visualize the model parameters by providing  with a suite of web-application that help us to inspect and understand the TensorFlow runs and graphs. Currently, it provides five types of visualizations: scalars, images, audio, histograms, and graphs.\n\n\n\n#### We will implement one of these 2 techniques on a standard Image Classification problem.\n","878d7f13":"# Finding a Learning Rate & TensorBoard\n\nIn this notebook we'll learn about finding the good learning rate with & without TensorBoard. \n\n**Note:** This notebook assumes that the reader is well versed with basics of TensorFlow libraries\/framework & Learning Rate.\n\n\nAs always, I will attempt to keep this notebook fairely organized & well commented for easy learning. \n\n\n### Please do UPVOTE if you find it helpful, it means alot to me :-).","f668cff0":"## Technique#2\n\nTo use the TensorBoard, we simply have to add the callback functionality to the Kera's Model.fit()\n\n\nWe shall see the implementation of TensorBoard callback, however, we won't be executing the Model.fit() due to fact that the TensorBoard does not load in Kaggle Notebook. You can try implementing it in Google Colab or in jupyter notebook on local machine. The implementation stays the same.","6292813d":"# Data Scaling\n\nThis is done to improve the performance of the model","0200e675":"## Class Definition\n\nIn order to classify the images we need to pre-define the classes. Following are the pre-defined classes.\n\n#### Airplane, Car, Cat, Dog, Flower, Fruit, Motorbike, Person\n\nEach image in our dataset will belong to one of the above classes & not both.","eaf32380":"![](https:\/\/cdn-images-1.medium.com\/freeze\/max\/1000\/1*5qqTJN1vvwEgekXXbHzU5g.png?q=20)","776c193a":"# Data Load","d95e6892":"# Libraries","e7643722":"The results of the LRFinder. The losses are plotted against the log scaled learning rates. A good learning rate would be in the range where the loss is strictly decreasing at a rapid rate. In this case the LR = 1e-02\n\n**Note:** A value should be chosen in a region where the loss is rapidly, but strictly decreasing. It is important to rebuild and recompile the model after the LRFinder is used in order to reset the weights that were updated during the mock training run.","721a83f8":"# Build & Train Model\n\nBuilding & Training a simple CNN Model\n\n**Model Configuration**\n\n* Conv2D: (32 filters of size 3 by 3) The features will be \"extracted\" from the image.\n* MaxPooling2D: The images get half sized.\n* Flatten: Transforms the format of the images from a 2d-array to a 1d-array of 150 150 3 pixel values.\n* Relu : given a value x, returns max(x, 0).\n* Softmax: 8 neurons, probability that the image belongs to one of the classes.\n\n**Compiling Model**\n\n* Optimizer: adam = RMSProp + Momentum\n    * Momentum = takes into account past gradient to have a better update.\n    * RMSProp = exponentially weighted average of the squares of past gradients.\n* Loss function: we use sparse categorical crossentropy for classification, each images belongs to one class only","935fd950":"### As we can observe, using Technique#1 the Model accuracy has improved slightly. To improve the accuracy further we simply have to repeat the process again & re-fit the model with updated learning-rate.","e612d8c2":"## Technique#1\n\nThe implementation of this technique in TensorFlow is pretty simple. We simply utilize the Keras Callback function."}}