{"cell_type":{"1b504c80":"code","e50c3db7":"code","a5b2ff20":"code","30e55cc8":"code","4df26495":"code","ab3c6cb3":"code","f0107d31":"code","9d8bf55d":"code","fbf77304":"code","7511e3e2":"code","11047294":"code","9203ccc9":"code","ccdb3961":"code","54ab22f7":"code","adf5f489":"code","c57b359f":"code","5d5aaa92":"code","decc2dc0":"code","4741fcab":"code","d1fa30fa":"code","f93c909e":"code","65f963ce":"code","6b8cf446":"code","4bda2439":"code","c4948398":"markdown","3ef21bc9":"markdown","971b85b4":"markdown","cb1ca747":"markdown","f24b4341":"markdown","5b06fb2b":"markdown","b65f4a78":"markdown","7415b2c8":"markdown","a2006782":"markdown","0fb7f62e":"markdown","0624b624":"markdown","44219610":"markdown","b2d500bc":"markdown","83b5f890":"markdown","4f899c12":"markdown","52d50d88":"markdown","5d77cc9a":"markdown","165a60a9":"markdown","9e442931":"markdown","5623af1b":"markdown","b4c5d3a5":"markdown","05eaf819":"markdown","6f6b96f7":"markdown","0a858256":"markdown","101b4707":"markdown","175054e3":"markdown","15c0b363":"markdown","3f840f51":"markdown","4858df71":"markdown","3d55da42":"markdown","7dc175f8":"markdown"},"source":{"1b504c80":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(17)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras import optimizers\nfrom keras import Input\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import ModelCheckpoint\n\n# seaborn \uc124\uc815\nsns.set(style='white', context='notebook', palette='deep')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/digit-recognizer'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e50c3db7":"# Load the data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","a5b2ff20":"Y_train = train[\"label\"]\n\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"],axis = 1) \n\n# \ub370\uc774\ud130 \ubd84\ud3ec \ub9c9\ub300\uadf8\ub798\ud504 \ud45c\uc2dc\nsns.countplot(Y_train)\nplt.show()","30e55cc8":"# Check the data\nX_train.isnull().any().describe()","4df26495":"test.isnull().any().describe()","ab3c6cb3":"# Normalize the data\nX_train = X_train \/ 255.0\ntest = test \/ 255.0","f0107d31":"# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1) 3D \ud150\uc11c\ub85c \ubcc0\ud658\nX_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","9d8bf55d":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0]) \uc6d0-\ud56b \uc778\ucf54\ub529\nY_train = to_categorical(Y_train, num_classes = 10)","fbf77304":"# Set the random seed\nrandom_seed = 2","7511e3e2":"# Split the train and the validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)","11047294":"# Some examples\ng = plt.imshow(X_train[0][:,:,0])","9203ccc9":"# Set the CNN model \nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\n\nmodel.summary()","ccdb3961":"from keras.utils import plot_model\nplot_model(model, to_file='model.png', show_shapes=True, show_layer_names=True)\nfrom IPython.display import Image\nImage(\"model.png\")","54ab22f7":"# Compile the model\nmodel.compile(optimizer = 'nadam' , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])","adf5f489":"# Set a learning rate annealer\ncallbacks_list = [\n    ReduceLROnPlateau(\n        monitor='val_accuracy', \n        patience=3, \n        verbose=1, \n        factor=0.5, \n        min_lr=1e-05),\n    ModelCheckpoint(\n        filepath='MNIST_CNN_model.h5',\n        monitor='val_accuracy',\n        save_best_only=True\n    )]","c57b359f":"epochs = 50\nbatch_size = 512","5d5aaa92":"# With data augmentation to prevent overfitting\n\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","decc2dc0":"# Fit the model\nhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_val,Y_val),\n                              verbose = 2, steps_per_epoch=X_train.shape[0] \/\/ batch_size,\n                              callbacks=callbacks_list)","4741fcab":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'][5:], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'][5:], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'][5:], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_accuracy'][5:], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","d1fa30fa":"# Look at confusion matrix \n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","f93c909e":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","65f963ce":"model = load_model('MNIST_CNN_model.h5')","6b8cf446":"# predict results\nresults = model.predict(test)\n\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")","4bda2439":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"cnn_mnist_datagen.csv\",index=False)","c4948398":"## 2.2 Set the optimizer and annealer \uc635\ud2f0\ub9c8\uc774\uc800 \uc124\uc815\n\n\uc190\uc2e4 \ud568\uc218\ub85c \"categorical_crossentropy\" \uc0ac\uc6a9.\n\uc635\ud2f0\ub9c8\uc774\uc800\ub85c Nadam \uc0ac\uc6a9.","3ef21bc9":"Confusion matrix\ub294 \ubaa8\ub378\uc758 \ub2e8\uc810\uc744 \ud655\uc778\ud558\uae30 \uc704\ud574 \uc720\uc6a9\ud55c \ub3c4\uad6c\uc774\ub2e4.\n\n\uac80\uc99d \uacb0\uacfc\ub97c \uae30\ubc18\uc73c\ub85c confusion matrix\ub97c \uadf8\ub824 \ubcf8\ub2e4.","971b85b4":"\ubc18\ubcf5\uc801\uc73c\ub85c \ubaa8\ub378\uc744 \uc218\uc815\ud558\uace0 \ud6c8\ub828\ud558\uace0 \uac80\uc99d \ub370\uc774\ud130\uc5d0\uc11c \ud3c9\uac00\ud55c\ub2e4. \ub2e4\uc74c\uacfc \uac19\uc740 \uac83\ub4e4\uc744 \uc801\uc6a9\ud574 \ubcfc \uc218 \uc788\ub2e4.\n* \ub4dc\ub86d\uc544\uc6c3 \ucd94\uac00\n* \uce35\uc744 \ucd94\uac00\ud558\uac70\ub098 \uc81c\uac70\ud574\uc11c \ub2e4\ub978 \uad6c\uc870 \uc2dc\ub3c4\n* L1\uc774\ub098 L2 \ub610\ub294 \ub450 \uac00\uc9c0 \ubaa8\ub450 \ucd94\uac00\n* \ucd5c\uc801\uc758 \uc124\uc815\uc744 \ucc3e\uae30 \uc704\ud574 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \ubc14\uafb8\uc5b4 \uc2dc\ub3c4\n* \uc120\ud0dd\uc801\uc73c\ub85c \ud2b9\uc131 \uacf5\ud559 \uc2dc\ub3c4(\uc0c8\ub85c\uc6b4 \ud2b9\uc131\uc744 \ucd94\uac00\ud558\uac70\ub098, \uc720\uc6a9\ud558\uc9c0 \uc54a\uc744 \ub4ef\ud55c \ud2b9\uc131\uc744 \uc81c\uac70)","cb1ca747":"We can get a better sense for one of these examples by visualising the image and looking at the label.","f24b4341":"784\uac1c\uc758 \uac12\uc744 \uac00\uc9c4 \ubca1\ud130 \ub370\uc774\ud130\ub97c 28x28x1 \ud06c\uae30\uc758 3D \ud150\uc11c\ub85c \ubcc0\ud658 \n\nKeras requires an extra dimension in the end which correspond to channels. MNIST images are gray scaled so it use only one channel. For RGB images, there is 3 channels, we would have reshaped 784px vectors to 28x28x3 3D matrices. ","5b06fb2b":"# 4. Prediction and submition \uc608\uce21 \uc218\ud589 \ubc0f \uacb0\uacfc\ubb3c \uc81c\ucd9c\n## 4.1 Predict and Submit results \uc608\uce21 \uc218\ud589 \ubc0f \uacb0\uacfc\ubb3c \uc81c\ucd9c","b65f4a78":"\uc624\ub958\ub4e4\uc744 \ubd84\uc11d\ud574 \ubcf4\uc790.","7415b2c8":"\ub808\uc774\ube14\uc740 0\ubd80\ud130 9\uae4c\uc9c0\uc758 \uc815\uc218 \uac12\uc744 \uac00\uc9c0\uba70, \uc774\ub97c one-hot \ubca1\ud130\ub85c \uc778\ucf54\ub529 (ex : 2 -> [0,0,1,0,0,0,0,0,0,0]).","a2006782":"## 3.2 Confusion matrix","0fb7f62e":"\ud655\uc778 \uacb0\uacfc \ub204\ub77d\ub41c \uac12\uc774 \uc5c6\uc73c\ubbc0\ub85c, \ubcc4\ub2e4\ub978 \ucc98\ub9ac \uc5c6\uc774 \uacc4\uc18d \uc9c4\ud589","0624b624":"\uc5ec\uae30\uc11c \uc774 CNN \ubaa8\ub378\uc774 \ubaa8\ub4e0 \uc22b\uc790\ub4e4\uc5d0 \ub300\ud574, \uac80\uc99d \ub370\uc774\ud130\uc14b\uc758 \ud06c\uae30(\uc774\ubbf8\uc9c0 4200\uc7a5)\ub97c \uac10\uc548\ud560 \ub54c \ub9e4\uc6b0 \uc801\uc740 \uc624\ub958\ub85c \ud6cc\ub96d\ud558\uac8c \uc791\ub3d9\ud568\uc744 \ud655\uc778\ud560 \uc218 \uc788\ub2e4.\n\n\ud558\uc9c0\ub9cc, \ub54c\ub54c\ub85c 3\uacfc 8, 4\uc640 9\ub97c \uad6c\ubcc4\ud558\ub294 \ub370\ub294 \uc5b4\ub824\uc6c0\uc744 \uacaa\ub294 \uac83 \uac19\ub2e4.","44219610":"## 1.2 Check for null and missing values \ub204\ub77d\ub41c \uac12 \ud655\uc778","b2d500bc":"## 2.3 Data augmentation \ub370\uc774\ud130 \ubcf4\uac15","83b5f890":"# MNIST 12 Layer CNN Classification\n### **Yunseo Kim**\n#### 2019\/12\/22\n\n* **1. Data preparation \ub370\uc774\ud130 \uc804\ucc98\ub9ac**\n    * 1.1 Load data \ub370\uc774\ud130 \ub85c\ub4dc\n    * 1.2 Check for null and missing values \ub204\ub77d\ub41c \uac12 \ud655\uc778\n    * 1.3 Normalization \uc815\uaddc\ud654\n    * 1.4 Reshape \ud615\ud0dc \ubcc0\ud658\n    * 1.5 Label encoding \uc6d0-\ud56b \uc778\ucf54\ub529\n    * 1.6 Split training and valdiation set \ud6c8\ub828\uc6a9 \ub370\uc774\ud130\uc640 \uac80\uc99d\uc6a9 \ub370\uc774\ud130 \ubd84\ub9ac\n* **2. CNN**\n    * 2.1 Define the model \ubaa8\ub378 \uc815\uc758\n    * 2.2 Set the optimizer and annealer \uc635\ud2f0\ub9c8\uc774\uc800 \uc124\uc815\n    * 2.3 Data augmentation \ub370\uc774\ud130 \ubcf4\uac15\n* **3. Evaluate the model \ubaa8\ub378 \ud3c9\uac00**\n    * 3.1 Training and validation curves \ud6c8\ub828 \ubc0f \uac80\uc99d \uace1\uc120\n    * 3.2 Confusion matrix\n* **4. Prediction and submition \uc608\uce21 \uc218\ud589 \ubc0f \uacb0\uacfc\ubb3c \uc81c\ucd9c**\n    * 4.1 Predict and Submit results \uc608\uce21 \uc218\ud589 \ubc0f \uacb0\uacfc\ubb3c \uc81c\ucd9c\n    * 4.2 Save Model \ubaa8\ub378 \uc800\uc7a5","4f899c12":"Keras Sequential API \uc0ac\uc6a9\n\nConv2D \uacc4\uce35\uc73c\ub85c \ucc98\uc74c \ub450 \ucc28\ub840\ub294 32\uac1c\uc758 \ud544\ud130\ub97c \uc0ac\uc6a9\ud558\uace0 \ub2e4\uc74c \ub450 \ucc28\ub840\ub294 64\uac1c\uc758 \ud544\ud130 \uc0ac\uc6a9. \uac01\uac01\uc758 \ud544\ud130\ub294 \uc774\ubbf8\uc9c0\uc758 \uc77c\ubd80\ub97c \ubcc0\ud658\uc2dc\ud0b4. \n\uc774 \uacfc\uc815\uc744 \ud1b5\ud574 \ud2b9\uc9d5 \ub9f5 \uad6c\uc131.\n\nMaxPool2D \uacc4\uce35\uc73c\ub85c\ub294 \ub2e4\uc6b4\uc0d8\ud50c\ub9c1 \uc218\ud589. \ucef4\ud4e8\ud305 \ube44\uc6a9\uc744 \uc904\uc774\uace0, \uacfc\uc801\ud569\uc744 \ubc29\uc9c0\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9. \ud480\ub9c1 \ucc28\uc6d0\uc774 \ub192\uc744\uc218\ub85d \ub2e4\uc6b4\uc0d8\ud50c\ub9c1\uc774 \uc911\uc694\ud574\uc9d0.\n\nconvolutional \uacc4\uce35\uacfc pooling \uacc4\uce35\uc744 \ud569\uce68\uc73c\ub85c\uc368 CNN\uc774 \uad6d\uc9c0\uc801\uc778 \ud2b9\uc9d5\ub4e4\uc744 \uacb0\ud569\uc2dc\ud0a4\uace0 \uc774\ubbf8\uc9c0\uc758 \ub354 \uc77c\ubc18\uc801\uc778 \ud2b9\uc9d5\ub4e4\uc744 \ud559\uc2b5\ud560 \uc218 \uc788\uc74c.\n\n\ub4dc\ub86d\uc544\uc6c3\uc740 \uacfc\uc801\ud569 \ubc29\uc9c0\ub97c \uc704\ud55c regularization \uae30\ubc95\uc73c\ub85c, \uac01\uac01\uc758 \ud6c8\ub828 \uc0d8\ud50c\uc5d0 \ub300\ud558\uc5ec \uacc4\uce35\uc5d0\uc11c \uc77c\uc815 \ube44\uc728\uc758 \ub178\ub4dc\uac00 \ubb34\uc791\uc704\uc801\uc73c\ub85c \ubb34\uc2dc\ub428.\n\nrelu \ud65c\uc131\ud654 \ud568\uc218 \uc0ac\uc6a9.\n\n\ud569\uc131\uacf1 \uacc4\uce35\uacfc \ud480\ub9c1 \uacc4\uce35\uc744 \uac70\uce5c \ub4a4\uc5d0\ub294 \ucd5c\uc885 \ud2b9\uc9d5 \ub9f5\uc744 \ub2e8\uc77c \ubca1\ud130\ub85c \ubcc0\ud658\ud558\uace0, \uc804\uacb0\ud569\uce35\uc744 \uac70\uce58\uba70 \ubaa8\ub4e0 \ubc1c\uacac\ub41c \uad6d\uc9c0\uc801 \ud2b9\uc9d5\ub4e4\uc774 \uacb0\ud569\ub428.\n\n\ub9c8\uc9c0\ub9c9\uc73c\ub85c \uc18c\ud504\ud2b8\ub9e5\uc2a4 \ud568\uc218\ub97c \uc774\uc6a9\ud574 \uc8fc\uc5b4\uc9c4 \ub370\uc774\ud130\uac00 \uc5b4\ub5a4 \uc22b\uc790\uc5d0 \ud574\ub2f9\ud558\ub294\uc9c0 \ud655\ub960\uc744 \uacc4\uc0b0\ud558\uc5ec \ucd9c\ub825.","52d50d88":"## 1.3 Normalization \uc815\uaddc\ud654","5d77cc9a":"## 1.6 Split training and validation set  \ud6c8\ub828\uc6a9 \ub370\uc774\ud130\uc640 \uac80\uc99d\uc6a9 \ub370\uc774\ud130 \ubd84\ub9ac","165a60a9":"# 1. Data preparation \ub370\uc774\ud130 \uc804\ucc98\ub9ac\n## 1.1 Load data \ub370\uc774\ud130 \ub85c\ub4dc","9e442931":"\uacfc\uc801\ud569\uc744 \ubc29\uc9c0\ud558\uae30 \uc704\ud574, \uc190\uae00\uc528 \uc22b\uc790 \ub370\uc774\ud130\uc14b\uc744 \uc778\uc704\uc801\uc73c\ub85c \ud655\uc7a5\ud560 \ud544\uc694\uac00 \uc788\ub2e4. \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \uc791\uc740 \ubcc0\ud654\ub4e4\uc744 \uc90c\uc73c\ub85c\uc368 \ub370\uc774\ud130\uc758 \ub2e4\uc591\uc131\uacfc \uc77c\ubc18\uc131\uc744 \ub192\uc77c \uc218 \uc788\ub2e4. \n\n\uc608\ub97c \ub4e4\uba74 \ub2e4\uc74c\uacfc \uac19\ub2e4.\n\uc22b\uc790\uac00 \uc911\uc559\uc5d0 \uc788\uc9c0 \uc54a\uc74c\n\uc22b\uc790\uc758 \ud06c\uae30\uac00 \uac19\uc9c0 \uc54a\uc74c\n\uc774\ubbf8\uc9c0\uac00 \ud68c\uc804\ub428\n...\n\n\uc774\ub7ec\ud55c \uae30\ubc95\ub4e4\uc740 \ub370\uc774\ud130 \ubcf4\uac15(data augmentation) \uae30\ubc95\uc73c\ub85c \uc54c\ub824\uc838 \uc788\ub2e4. \ud754\ud788 \uc0ac\uc6a9\ud558\ub294 \uae30\ubc95\ub4e4\uc740 \ud68c\uc0c9\uc870 \ubcc0\ud658(grayscales), \uac00\ub85c \ubc29\ud5a5 \ub4a4\uc9d1\uae30(horizontal flips), \uc138\ub85c \ubc29\ud5a5 \ub4a4\uc9d1\uae30(vertical flips), \ubb34\uc791\uc704\uc801 \uc790\ub974\uae30(random crops), \uc0c9\uc0c1 \ub178\uc774\uc988 \ucd94\uac00(color jitters), translations, \ud68c\uc804(rotations) \ub4f1\ub4f1\uc774 \uc788\ub2e4. \n\n\uc774\ub7ec\ud55c \ubcc0\ud658\ub4e4 \uc911 \uba87 \uac00\uc9c0\ub97c \ud6c8\ub828 \ub370\uc774\ud130\uc5d0 \uc801\uc6a9\ud568\uc73c\ub85c\uc368, \ud6c8\ub828 \ub370\uc774\ud130\uc758 \uc591\uc744 \uc190\uc27d\uac8c 2~3\ubc30\ub85c \ub298\ub9ac\uace0 \ub9e4\uc6b0 \uac15\ub825\ud55c \ubaa8\ub378\uc744 \ub9cc\ub4e4 \uc218 \uc788\ub2e4.\n\n   - \ub370\uc774\ud130 \ubcf4\uac15 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc744 \uacbd\uc6b0 \uc57d 98%\uc758 \uc815\ud655\ub3c4\ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4. \n   - \ub370\uc774\ud130 \ubcf4\uac15 \uae30\ubc95\uc744 \uc0ac\uc6a9\ud560 \uacbd\uc6b0 \uc57d 99.7%\uc758 \uc815\ud655\ub3c4\ub97c \uc5bb\uc744 \uc218 \uc788\ub2e4.","5623af1b":"## 1.4 Reshape","b4c5d3a5":"# 3. Evaluate the model \ubaa8\ub378 \ud3c9\uac00\n## 3.1 Training and validation curves \ud6c8\ub828 \ubc0f \uac80\uc99d \uace1\uc120","05eaf819":"\uc704\uc5d0\uc11c \ud655\uc778\ud560 \uc218 \uc788\ub294 6\uac00\uc9c0 \uc624\ub958\ub294 \ucda9\ubd84\ud788 \ub0a9\ub4dd\ud560 \uc218 \uc788\uc744 \ub9cc\ud55c \uac83\ub4e4\uc774\ub2e4. \uc774\ub4e4 \uc911 \uba87\uba87\uc740 \uc0ac\ub78c \uc5ed\uc2dc \ubc94\ud560 \uc218 \uc788\ub2e4\uace0 \ubcf4\uc5ec\uc9c4\ub2e4.","6f6b96f7":"\uacfc\ub300\uc801\ud569\uc744 \uc644\ud654\uc2dc\ud0a4\uae30 \uc704\ud55c \uc77c\ubc18\uc801\uc778 \ubc29\ubc95\uc740 \ub124\ud2b8\uc6cc\ud06c\uc758 \ubcf5\uc7a1\ub3c4\uc5d0 \uc81c\ud55c\uc744 \ub450\uc5b4 \uac00\uc911\uce58\uac00 \uc791\uc740 \uac12\uc744 \uac00\uc9c0\ub3c4\ub85d \uac15\uc81c\ud558\ub294 \uac83\uc774\ub2e4. \uac00\uc911\uce58 \uac12\uc758 \ubd84\ud3ec\uac00 \ub354 \uade0\uc77c\ud558\uac8c \ub41c\ub2e4. \uc774\ub97c \uac00\uc911\uce58 \uaddc\uc81c\ub77c\uace0 \ud558\uace0 \ub124\ud2b8\uc6cc\ud06c\uc758 \uc190\uc2e4 \ud568\uc218\uc5d0 \ud070 \uac00\uc911\uce58\uc5d0 \uc5f0\uad00\ub41c \ube44\uc6a9\uc744 \ucd94\uac00\ud55c\ub2e4. \ub450 \uac00\uc9c0 \ud615\ud0dc\uc758 \ube44\uc6a9\uc774 \uc788\ub2e4.\n\n* L1 \uaddc\uc81c : \uac00\uc911\uce58\uc758 \uc808\ub300\uac12\uc5d0 \ube44\ub840\ud558\ub294 \ube44\uc6a9\uc774 \ucd94\uac00\ub41c\ub2e4(\uac00\uc911\uce58\uc758 L1 \ub178\ub984).\n* L2 \uaddc\uc81c : \uac00\uc911\uce58\uc758 \uc81c\uacf1\uc5d0 \ube44\ub840\ud558\ub294 \ube44\uc6a9\uc774 \ucd94\uac00\ub41c\ub2e4(\uac00\uc911\uce58\uc758 L2 \ub178\ub984). L2 \uaddc\uc81c\ub294 \uc2e0\uacbd\ub9dd\uc5d0\uc11c \uac00\uc911\uce58 \uac10\uc1e0\ub77c\uace0\ub3c4 \ubd80\ub978\ub2e4.\n\n\ucf00\ub77c\uc2a4\uc5d0\uc11c \uac00\uc911\uce58 \uaddc\uc81c \uc778\uc2a4\ud134\uc2a4\ub97c \uce35\uc758 \ud0a4\uc6cc\ub4dc \ub9e4\uac1c\ubcc0\uc218\ub85c \uc804\ub2ec\ud558\uc5ec \uac00\uc911\uce58 \uaddc\uc81c\ub97c \ucd94\uac00\ud560 \uc218 \uc788\ub2e4.","0a858256":" The key to the model evaluation is to divide the data into three datasets: training, validation, and test. Train models in training set and evaluate models in validation set. Immediately before releasing the model, the model is finally tested only once in the test set.\n \n \ubaa8\ub378 \ud3c9\uac00\uc758 \ud575\uc2ec\uc740 \ub370\uc774\ud130\ub97c \ud6c8\ub828, \uac80\uc99d, \ud14c\uc2a4\ud2b8 3\uac1c\uc758 \ub370\uc774\ud130\uc14b\uc73c\ub85c \ub098\ub204\ub294 \uac83\uc774\ub2e4. \ud6c8\ub828 \uc138\ud2b8\uc5d0\uc11c \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uace0 \uac80\uc99d \uc138\ud2b8\uc5d0\uc11c \ubaa8\ub378\uc744 \ud3c9\uac00\ud55c\ub2e4. \uc774\ud6c4 \ubaa8\ub378\uc744 \ucd9c\uc2dc\ud558\uae30 \uc9c1\uc804, \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc5d0\uc11c \ucd5c\uc885\uc801\uc73c\ub85c \ub531 \ud55c \ubc88 \ubaa8\ub378\uc744 \ud14c\uc2a4\ud2b8\ud55c\ub2e4.\n\n The reason only a training set and a test set are not used is that when developing a model, the setting of the model, or hyperparameter, is tuned. In essence, hyperparameter tuning is also learning to find good settings in any parameter space, so if hyperparameter tuning is conducted based on the performance of the validation set, it can be overfitted in the validation set even if the model is not trained directly with the validation set. This is called information leak. This is why three sets of data are used: training, validation, and test.\n \n \ud6c8\ub828 \uc138\ud2b8\uc640 \ud14c\uc2a4\ud2b8 \uc138\ud2b8 2\uac1c\ub9cc\uc744 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\ub294 \uc774\uc720\ub294 \ubaa8\ub378\uc744 \uac1c\ubc1c\ud560 \ub54c \ubaa8\ub378\uc758 \uc124\uc815, \uc989 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130(hyperparameter)\ub97c \ud29c\ub2dd\ud558\uae30 \ub54c\ubb38\uc774\ub2e4. \ubcf8\uc9c8\uc801\uc73c\ub85c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\ub3c4 \uc5b4\ub5a4 \ud30c\ub77c\ubbf8\ud130 \uacf5\uac04\uc5d0\uc11c \uc88b\uc740 \uc124\uc815\uc744 \ucc3e\ub294 \ud559\uc2b5\uc774\uae30 \ub54c\ubb38\uc5d0, \uac80\uc99d \uc138\ud2b8\uc758 \uc131\ub2a5\uc744 \uae30\ubc18\uc73c\ub85c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ud29c\ub2dd\uc744 \uc9c4\ud589\ud558\uba74 \uac80\uc99d \uc138\ud2b8\ub85c \ubaa8\ub378\uc744 \uc9c1\uc811 \ud6c8\ub828\ud558\uc9c0 \uc54a\ub354\ub77c\ub3c4 \uac80\uc99d \uc138\ud2b8\uc5d0 \uacfc\ub300\uc801\ud569\ub420 \uc218 \uc788\ub2e4. \uc774\ub97c \uc815\ubcf4 \ub204\uc124(information leak)\uc774\ub77c\uace0 \ud55c\ub2e4. \uc774\ub7ec\ud55c \uc774\uc720\ub85c \ud6c8\ub828, \uac80\uc99d, \ud14c\uc2a4\ud2b8 3\uac1c\uc758 \ub370\uc774\ud130\uc14b\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774\ub2e4.\n \n Methods such as hold-out validation, K-fold cross-validation, and iterated K-fold cross-validation using shuffling can be used for model evaluation.\n \n \ubaa8\ub378 \ud3c9\uac00\ub97c \uc704\ud574 \ub2e8\uc21c \ud640\ub4dc\uc544\uc6c3 \uac80\uc99d(hold-out validation), K-\uacb9 \uad50\ucc28 \uac80\uc99d(K-fold cross-validation), \uc154\ud50c\ub9c1(shuffling)\uc744 \uc0ac\uc6a9\ud55c \ubc18\ubcf5 K-\uacb9 \uad50\ucc28 \uac80\uc99d(iterated K-fold cross-validation) \ub4f1\uc758 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\ub2e4.\n \n* **hold-out validation \ub2e8\uc21c \ud640\ub4dc\uc544\uc6c3 \uac80\uc99d**\n \n Divide a certain amount of data into the validation set and the test set, which is the simplest way. There is a disadvantage that when there is less data, there may be too few samples from both the validation set and the test set to represent the entire given data statistically. Use the train_test_split() function of the Scikit-learn. It is important to continue to create and retrain new models, and to use both training and validation data when training the last model after obtaining the optimal hyperparameter.\n \n \ub370\uc774\ud130\uc758 \uc77c\uc815\ub7c9\uc744 \uac80\uc99d \uc138\ud2b8\uc640 \ud14c\uc2a4\ud2b8 \uc138\ud2b8\ub85c \ub5bc\uc5b4 \ub193\ub294\ub2e4. \uac00\uc7a5 \ub2e8\uc21c\ud55c \ubc29\ubc95\uc73c\ub85c, \ub370\uc774\ud130\uac00 \uc801\uc744 \ub54c\ub294 \uac80\uc99d \uc138\ud2b8\uc640 \ud14c\uc2a4\ud2b8 \uc138\ud2b8\uc758 \uc0d8\ud50c\uc774 \ub108\ubb34 \uc801\uc5b4 \uc8fc\uc5b4\uc9c4 \uc804\uccb4 \ub370\uc774\ud130\ub97c \ud1b5\uacc4\uc801\uc73c\ub85c \ub300\ud45c\ud558\uc9c0 \ubabb\ud560 \uc218 \uc788\ub2e4\ub294 \ub2e8\uc810\uc774 \uc788\ub2e4. \uc0ac\uc774\ud0b7\ub7f0\uc758 train_test_split() \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uba74 \ud3b8\ub9ac\ud558\ub2e4. \ud6c8\ub828-\ud3c9\uac00-\ud29c\ub2dd\uc744 \ubc18\ubcf5\ud560 \ub54c \uacc4\uc18d \uc0c8\ub85c\uc6b4 \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4 \ub2e4\uc2dc \ud6c8\ub828\uc2dc\ud0a4\uba70, \ucd5c\uc801\uc758 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub97c \uad6c\ud55c \ud6c4 \ub9c8\uc9c0\ub9c9 \ubaa8\ub378\uc744 \ud6c8\ub828\uc2dc\ud0ac \ub54c\ub294 \ud6c8\ub828 \ub370\uc774\ud130\uc640 \uac80\uc99d \ub370\uc774\ud130\ub97c \ubaa8\ub450 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \uc911\uc694\ud558\ub2e4.\n\n\n* **K-fold cross-validation K-\uacb9 \uad50\ucc28 \uac80\uc99d** \n \n Divide the data into K segments of the same size. For each split i, train the model with the remaining K-1 divisions and validate the model in split i. The final score will be averaged out of these K scores. This method is helpful when the performance of a model is varied depending on the data partition. It can be easily implemented using the cross_validate() function of the Scikit-learn. To use this function, the Keras model must be wrapped in a KerasClassifier or KerasRegressor class to be compatible with the Scikit-learn.\n \n \ub370\uc774\ud130\ub97c \ub3d9\uc77c\ud55c \ud06c\uae30\ub97c \uac00\uc9c4 K\uac1c \ubd84\ud560\ub85c \ub098\ub208\ub2e4. \uac01 \ubd84\ud560 i\uc5d0 \ub300\ud574 \ub098\uba38\uc9c0 K-1\uac1c\uc758 \ubd84\ud560\ub85c \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uace0 \ubd84\ud560 i\uc5d0\uc11c \ubaa8\ub378\uc744 \uac80\uc99d\ud55c\ub2e4. \ucd5c\uc885 \uc810\uc218\ub294 \uc774\ub807\uac8c \uc5bb\uc740 K\uac1c\uc758 \uc810\uc218\ub97c \ud3c9\uade0\ud558\uc5ec \uc5bb\ub294\ub2e4. \uc774 \ubc29\ubc95\uc740 \ubaa8\ub378\uc758 \uc131\ub2a5\uc774 \ub370\uc774\ud130 \ubd84\ud560\uc5d0 \ub530\ub77c \ud3b8\ucc28\uac00 \ud074 \ub54c \ub3c4\uc6c0\uc774 \ub41c\ub2e4. \uc0ac\uc774\ud0b7\ub7f0\uc758 cross_validate() \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uc5ec \uc27d\uac8c \uad6c\ud604\ud560 \uc218 \uc788\ub2e4. \uc774 \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\ub824\uba74 \ucf00\ub77c\uc2a4 \ubaa8\ub378\uc744 \uc0ac\uc774\ud0b7\ub7f0\uacfc \ud638\ud658\ub418\ub3c4\ub85d KerasClassifier\ub098 KerasRegressor \ud074\ub798\uc2a4\ub85c \uac10\uc2f8\uc57c \ud55c\ub2e4.\n \n \n* **iterated K-fold cross-validation using shuffling \uc154\ud50c\ub9c1\uc744 \uc0ac\uc6a9\ud55c \ubc18\ubcf5 K-\uacb9 \uad50\ucc28 \uac80\uc99d**\n\n It is used when relatively few data are available and the model is to be evaluated as accurately as possible. Apply K-fold cross-validation several times, but randomly mix the data each time before dividing into K segments. The final score is the average of the scores obtained by performing all K-fold cross-validation. It is very expensive to train and evaluate models. Apply RepeatedKFold and RepeatedStratifiedKFold classes in the Scikit-learn on the cross_validate() function.\n \n \ube44\uad50\uc801 \uac00\uc6a9 \ub370\uc774\ud130\uac00 \uc801\uace0 \uac00\ub2a5\ud55c \uc815\ud655\ud558\uac8c \ubaa8\ub378\uc744 \ud3c9\uac00\ud558\uace0\uc790 \ud560 \ub54c \uc0ac\uc6a9\ud55c\ub2e4. K-\uacb9 \uad50\ucc28 \uac80\uc99d\uc744 \uc5ec\ub7ec \ubc88 \uc801\uc6a9\ud558\ub418 K\uac1c\uc758 \ubd84\ud560\ub85c \ub098\ub204\uae30 \uc804\uc5d0 \ub9e4\ubc88 \ub370\uc774\ud130\ub97c \ubb34\uc791\uc704\ub85c \uc11e\ub294\ub2e4. \ucd5c\uc885 \uc810\uc218\ub294 \ubaa8\ub4e0 K-\uacb9 \uad50\ucc28 \uac80\uc99d\uc744 \uc2e4\ud589\ud574\uc11c \uc5bb\uc740 \uc810\uc218\uc758 \ud3c9\uade0\uc774 \ub41c\ub2e4. \uacb0\uad6d P(\ubc18\ubcf5 \ud69f\uc218) * K\uac1c\uc758 \ubaa8\ub378\uc744 \ud6c8\ub828\ud558\uace0 \ud3c9\uac00\ud558\ubbc0\ub85c \ube44\uc6a9\uc774 \ub9e4\uc6b0 \ub9ce\uc774 \ub4e0\ub2e4. \uc0ac\uc774\ud0b7\ub7f0\uc758 RepeatedKFold\uc640 RepeatedStratifiedKFold \ud074\ub798\uc2a4\ub97c cross_validate() \ud568\uc218\uc5d0 \uc801\uc6a9\ud558\uc5ec \uad6c\ud604\ud560 \uc218 \uc788\ub2e4.\n \n\uc5ec\uae30\uc11c\ub294 \ub370\uc774\ud130\ub97c 90%\uc758 \ud6c8\ub828\uc6a9 \ub370\uc774\ud130\uc640 10%\uc758 \uac80\uc99d\uc6a9 \ub370\uc774\ud130\ub85c \ubd84\ub9ac\n\nSince we have 42 000 training images of balanced labels (see 2.1 Load data), a random split of the train set doesn't cause some labels to be over represented in the validation set. Be carefull with some unbalanced dataset a simple random split could cause inaccurate evaluation during the validation. \n\nTo avoid that, you could use stratify = True option in train_test_split function (**Only for >=0.17 sklearn versions**).","101b4707":"\ubaa8\ub378\uc744 \ub9cc\ub4e4\uae30 \uc804\uc5d0 \uc138 \uac00\uc9c0 \uc911\uc694\ud55c \uc120\ud0dd\uc744 \ud574\uc57c \ud55c\ub2e4.\n\n* \ub9c8\uc9c0\ub9c9 \uce35\uc758 \ud65c\uc131\ud654 \ud568\uc218\n * \uc774\uc9c4 \ubd84\ub958 : \uc2dc\uadf8\ubaa8\uc774\ub4dc\n * \ub2e8\uc77c \ub808\uc774\ube14 \ub2e4\uc911 \ubd84\ub958 : \uc18c\ud504\ud2b8\ub9e5\uc2a4\n * \ub2e4\uc911 \ub808\uc774\ube14 \ub2e4\uc911 \ubd84\ub958 : \uc2dc\uadf8\ubaa8\uc774\ub4dc\n * \uc784\uc758 \uac12\uc5d0 \ub300\ud55c \ud68c\uadc0 : \uc5c6\uc74c\n * 0\uacfc 1 \uc0ac\uc774 \uac12\uc5d0 \ub300\ud55c \ud68c\uadc0 : \uc2dc\uadf8\ubaa8\uc774\ub4dc\n \n* \uc190\uc2e4 \ud568\uc218\n * \uc774\uc9c4 \ubd84\ub958 : binary_crossentropy\n * \ub2e8\uc77c \ub808\uc774\ube14 \ub2e4\uc911 \ubd84\ub958 : categorical_crossentropy\n * \ub2e4\uc911 \ub808\uc774\ube14 \ub2e4\uc911 \ubd84\ub958 : binary_crossentropy\n * \uc784\uc758 \uac12\uc5d0 \ub300\ud55c \ud68c\uadc0 : mse\n * 0\uacfc 1 \uc0ac\uc774 \uac12\uc5d0 \ub300\ud55c \ud68c\uadc0 : mse \ub610\ub294 binary_crossentropy\n \n* \ucd5c\uc801\ud654 \uc124\uc815 : \ub300\ubd80\ubd84\uc758 \uacbd\uc6b0 rmsprop\uacfc \uae30\ubcf8 \ud559\uc2b5\ub960\uc744 \uc0ac\uc6a9\ud558\ub294 \uac83\uc774 \ubb34\ub09c\ud568","175054e3":"# 2. CNN\n## 2.1 Define the model \ubaa8\ub378 \uc815\uc758","15c0b363":"10\uac1c \uc22b\uc790 \uac01\uac01\uc5d0 \ud574\ub2f9\ud558\ub294 \ub370\uc774\ud130 \uac1c\uc218\uac00 \ube44\uc2b7\ud55c \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc74c","3f840f51":"\uc77c\ubc18\uc801\uc73c\ub85c \ube44\uad50\uc801 \ud070 \uac12(\uc608\ub97c \ub4e4\uc5b4 \ub124\ud2b8\uc6cc\ud06c\uc758 \uac00\uc911\uce58 \ucd08\uae43\uac12\ubcf4\ub2e4 \ud6e8\uc52c \ud070 \uc5ec\ub7ec \uc790\ub9bf\uc218\ub97c \uac00\uc9c4 \uc815\uc218)\uc774\ub098 \uc2a4\ucf00\uc77c\uc774 \uade0\uc77c\ud558\uc9c0 \uc54a\uc740 \ub370\uc774\ud130\ub97c \uc2e0\uacbd\ub9dd\uc5d0 \uc8fc\uc785\ud558\ub294 \uac83\uc740 \uc704\ud5d8\ud558\ub2e4. \uc774\ub807\uac8c \ud558\uba74 \uc5c5\ub370\uc774\ud2b8\ud560 \uadf8\ub798\ub514\uc5b8\ud2b8\uac00 \ucee4\uc838 \ub124\ud2b8\uc6cc\ud06c\uc758 \uc218\ub834\uc744 \ubc29\ud574\ud55c\ub2e4. \ub124\ud2b8\uc6cc\ud06c\ub97c \uc27d\uac8c \ud559\uc2b5\uc2dc\ud0a4\ub824\uba74 \ub370\uc774\ud130\uac00 \ub2e4\uc74c \ud2b9\uc9d5\uc744 \ub530\ub77c\uc57c \ud55c\ub2e4.\n\n* \uc791\uc740 \uac12\uc744 \ucde8\ud55c\ub2e4. \uc77c\ubc18\uc801\uc73c\ub85c \ub300\ubd80\ubd84\uc758 \uac12\uc774 0~1 \uc0ac\uc774\uc5ec\uc57c \ud55c\ub2e4.\n* \uc2a4\ucf00\uc77c\uc774 \uade0\uc77c\ud574\uc57c \ud55c\ub2e4.\n\n\uac01 \ud2b9\uc131\ubcc4\ub85c \ud3c9\uade0\uc774 0, \ud45c\uc900 \ud3b8\ucc28\uac00 1\uc774 \ub418\ub3c4\ub85d \uc815\uaddc\ud654\ud558\ub294 \ubc29\ubc95\uc774 \ub9ce\uc774 \uc4f0\uc778\ub2e4.\n\n\uc5ec\uae30\uc11c\ub294, \uc774\ud6c4 \ub370\uc774\ud130 \uc99d\uc2dd \uacfc\uc815\uc5d0\uc11c \uc815\uaddc\ud654\ub97c \ud568\uaed8 \ucc98\ub9ac\ud560 \uac83\uc774\ub2e4.","4858df71":"## 1.5 Label encoding \uc6d0-\ud56b \uc778\ucf54\ub529","3d55da42":"\uc635\ud2f0\ub9c8\uc774\uc800\uac00 \uc190\uc2e4\ud568\uc218\uc758 \ucd5c\uc19f\uac12\uc5d0 \uac00\uc7a5 \uac00\uae5d\uac8c, \uadf8\ub9ac\uace0 \ub354 \ube60\ub974\uac8c \uc218\ub834\ud558\ub3c4\ub85d \ud558\uae30 \uc704\ud574\uc11c \ud559\uc2b5\ub960(learning rate, LR)\uc744 \uc870\uc815(annealing).\n\n\ud559\uc2b5\ub960\uc774\ub780 \ud55c \ubc88\uc758 \uac31\uc2e0\uc73c\ub85c \ud30c\ub77c\ubbf8\ud130\ub97c \uc5bc\ub9c8\ub098 \ub9ce\uc774 \uc218\uc815\ud560\uc9c0\ub97c \uacb0\uc815\ud558\ub294 \uac12\uc774\ub2e4.\n\ud559\uc2b5\ub960\uc774 \ub192\uc73c\uba74 \ud6c8\ub828\uc774 \ube60\ub974\uac8c \uc9c4\ud589\ub418\uc9c0\ub9cc, \uc190\uc2e4\ud568\uc218\uc758 \ucd5c\uc19f\uac12\uc73c\ub85c \uc81c\ub300\ub85c \uc218\ub834\ud558\uc9c0 \ubabb\ud560 \uc218 \uc788\ub2e4.\n\ud559\uc2b5\ub960\uc774 \ub0ae\uc73c\uba74 \ud6c8\ub828\uc774 \ub354\ub514\uac8c \uc9c4\ud589\ub41c\ub2e4.\n\n\ud6a8\uc728\uc801\uc73c\ub85c \uc190\uc2e4\ud568\uc218\uc758 \ucd5c\uc19f\uac12\uc5d0 \uc811\uadfc\ud558\uae30 \uc704\ud574\uc11c\ub294 \ud559\uc2b5\ub960\uc744 \ud6c8\ub828\uc744 \uc9c4\ud589\ud558\uba74\uc11c \uc810\uc9c4\uc801\uc73c\ub85c \uc904\uc5ec \ub098\uac00\ub294 \uac83\uc774 \uc88b\ub2e4.\n\ub192\uc740 \ud559\uc2b5\ub960\uc774 \uac16\ub294 \uc9e7\uc740 \ud6c8\ub828 \uc2dc\uac04\uc758 \uc774\uc810\uc744 \uc720\uc9c0\ud558\uae30 \uc704\ud574, \uc815\ud655\ub3c4\ub97c \ubaa8\ub2c8\ud130\ub9c1\ud558\uba74\uc11c 3 \uc5d0\ud3ec\ud06c\ub3d9\uc548 \uac1c\uc120\uc774 \uc774\ub904\uc9c0\uc9c0 \uc54a\uc73c\uba74 \ud559\uc2b5\ub960\uc744 0.5\uc73c\ub85c \uc904\uc774\ub294 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\uc600\ub2e4.\n\nKeras.callbacks\uc758 ReduceLROnPlateau \ud568\uc218 \uc0ac\uc6a9","7dc175f8":"\ub370\uc774\ud130 \ubcf4\uac15\uc744 \uc704\ud574 \ub2e4\uc74c\uacfc \uac19\uc740 \ubc29\ubc95 \uc0ac\uc6a9 :\n   - \ubb34\uc791\uc704\uc801\uc73c\ub85c \uc774\ubbf8\uc9c0\ub97c 10\ub3c4 \ud68c\uc804\n   - \ubb34\uc791\uc704\uc801\uc73c\ub85c \uc774\ubbf8\uc9c0\ub97c 10% \ud655\ub300\n   - \ubb34\uc791\uc704\uc801\uc73c\ub85c \uc774\ubbf8\uc9c0 \uac00\ub85c \uae38\uc774\ub97c 10% \ubcc0\ud654\uc2dc\ud0b4\n   - \ubb34\uc791\uc704\uc801\uc73c\ub85c \uc774\ubbf8\uc9c0 \uc138\ub85c \uae38\uc774\ub97c 10% \ubcc0\ud654\uc2dc\ud0b4\n   \n6\uacfc 9\uc640 \uac19\uc740 \uc22b\uc790\ub4e4\uc774 \uc798\ubabb \ubd84\ub958\ub420 \uc218 \uc788\uc73c\ubbc0\ub85c, \ub4a4\uc9d1\uae30(flip) \uae30\ubc95\uc740 \uc0ac\uc6a9\ud558\uc9c0 \uc54a\uc74c.\n\n\ub370\uc774\ud130 \ubcf4\uac15\uc774 \ub05d\ub098\uba74 \ud559\uc2b5 \uc9c4\ud589."}}