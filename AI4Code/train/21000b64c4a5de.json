{"cell_type":{"cbdbac17":"code","bd988e77":"code","c05fcec3":"code","39e7e074":"code","8a74d160":"code","e3948973":"code","520ba786":"code","ef6e5373":"code","6eedf250":"code","839ef2b7":"code","2b0e20f7":"code","6eb25bab":"code","78e2cd17":"code","7253731c":"code","78b2115a":"code","c1ff1508":"code","c8c5e529":"code","26d54ad6":"code","543ec7b4":"code","8fa382c1":"code","2aa27636":"code","3ab5eaae":"code","dc3596b4":"code","2a301a6d":"code","e6c7e580":"code","601e1ada":"code","a22170da":"code","82d48b74":"code","040d4089":"code","6b670c96":"markdown","e96ec2a8":"markdown","77164253":"markdown","81f1f2d2":"markdown","fb289e6e":"markdown","afff518e":"markdown","48842554":"markdown","dc892c29":"markdown","1e84a273":"markdown","cc438312":"markdown","a0247e99":"markdown","6c2e3b80":"markdown","f0c787e2":"markdown","bfebdc00":"markdown","7a6dd814":"markdown","11caea04":"markdown","2a2d868c":"markdown","5122afba":"markdown","46b22095":"markdown","fb783f6c":"markdown","188d4f43":"markdown"},"source":{"cbdbac17":"import pandas as pd\nimport numpy as np\nfrom itertools import product, combinations\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nimport gc\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn.ensemble import (RandomForestClassifier\n                              , RandomForestRegressor\n                              , AdaBoostClassifier\n                              , ExtraTreesClassifier\n                             )\nfrom lightgbm import LGBMClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split","bd988e77":"rand_state = 719\n\nfrom time import time\ndef timer(t): # t = beginning of timing\n    timing = time() - t\n    if timing < 60:\n        return str(round(timing,1)) + ' second(s)'\n    elif timing < 3600:\n        return str(round(timing \/ 60,1)) + ' minute(s)'\n    else:\n        return str(round(timing \/ 3600,1)) + ' hour(s)'","c05fcec3":"data_path = '\/kaggle\/input\/learn-together\/'\ndef reload(x):\n    return pd.read_csv(data_path + x, index_col = 'Id')\n\ntrain = reload('train.csv')\nn_train = len(train)\ntest = reload('test.csv')\nn_test = len(test)\n\nindex_test = test.index.copy()\ny_train = train.Cover_Type.copy()\n\nall_data = train.iloc[:,train.columns != 'Cover_Type'].append(test)\nall_data['train'] = [1]*n_train + [0]*n_test\n\ndel train\ndel test","39e7e074":"questionable_0 = ['Hillshade_9am', 'Hillshade_3pm'] # Hillshade_3pm visualization looks weird\ncorr_cols = {'Hillshade_9am': ['Hillshade_3pm', 'Aspect', 'Slope', 'Soil_Type10', 'Wilderness_Area1',\n                               'Wilderness_Area4', 'Vertical_Distance_To_Hydrology'],\n             'Hillshade_3pm': ['Hillshade_9am', 'Hillshade_Noon', 'Slope', 'Aspect']\n            }","8a74d160":"rfr = RandomForestRegressor(n_estimators = 100, random_state = rand_state, verbose = 1, n_jobs = -1)\nfor col in questionable_0:\n    print('='*20)\n    print(col)\n    all_data_0 = all_data[all_data[col] == 0].copy()\n    all_data_non0 = all_data[all_data[col] != 0].copy()\n    rfr.fit(all_data_non0[corr_cols[col]], all_data_non0[col])\n    pred = rfr.predict(all_data_0[corr_cols[col]])\n    pred_col = 'predicted_{}'.format(col)\n    \n    all_data[pred_col] = all_data[col].copy()\n    all_data.loc[all_data_0.index, pred_col] = pred\n\nfor col in questionable_0:\n    all_data['predicted_{}'.format(col)] = all_data['predicted_{}'.format(col)].apply(int)\ndel rfr\ndel corr_cols","e3948973":"def aspect_slope(df):\n    df['AspectSin'] = np.sin(np.radians(df.Aspect))\n    df['AspectCos'] = np.cos(np.radians(df.Aspect))\n    df['AspectSin_Slope'] = df.AspectSin * df.Slope\n    df['AspectCos_Slope'] = df.AspectCos * df.Slope\n    df['AspectSin_Slope_Abs'] = np.abs(df.AspectSin_Slope)\n    df['AspectCos_Slope_Abs'] = np.abs(df.AspectCos_Slope)\n    df['Hillshade_Mean'] = df[['Hillshade_9am',\n                              'Hillshade_Noon',\n                              'Hillshade_3pm']].apply(np.mean, axis = 1)\n    return df","520ba786":"def distances(df):\n    horizontal = ['Horizontal_Distance_To_Fire_Points', \n                  'Horizontal_Distance_To_Roadways',\n                  'Horizontal_Distance_To_Hydrology']\n    \n    df['Euclidean_to_Hydrology'] = np.sqrt(df['Horizontal_Distance_To_Hydrology']**2 + df['Vertical_Distance_To_Hydrology']**2)\n    df['EuclidHydro_Slope'] = df.Euclidean_to_Hydrology * df.Slope\n    df['Elevation_VDH_sum'] = df.Elevation + df.Vertical_Distance_To_Hydrology\n    df['Elevation_VDH_diff'] = df.Elevation - df.Vertical_Distance_To_Hydrology\n    df['Elevation_2'] = df.Elevation**2\n    df['Elevation_3'] = df.Elevation**3\n    df['Elevation_log1p'] = np.log1p(df.Elevation) # credit: https:\/\/www.kaggle.com\/evimarp\/top-6-roosevelt-national-forest-competition\/notebook\n    \n    for col1, col2 in combinations(zip(horizontal, ['HDFP', 'HDR', 'HDH']), 2):\n        df['{0}_{1}_diff'.format(col1[1], col2[1])] = df[col1[0]] - df[col2[0]]\n        df['{0}_{1}_sum'.format(col1[1], col2[1])] = df[col1[0]] + df[col2[0]]\n    \n    df['Horizontal_sum'] = df[horizontal].sum(axis = 1)\n    return df","ef6e5373":"def OHE_to_cat(df, colname, data_range): # data_range = [min_index, max_index+1]\n    df[colname] = sum([i * df[colname + '{}'.format(i)] for i in range(data_range[0], data_range[1])])\n    return df","6eedf250":"soils = [\n    [7, 15, 8, 14, 16, 17,\n     19, 20, 21, 23], #unknow and complex \n    [3, 4, 5, 10, 11, 13],   # rubbly\n    [6, 12],    # stony\n    [2, 9, 18, 26],      # very stony\n    [1, 24, 25, 27, 28, 29, 30,\n     31, 32, 33, 34, 36, 37, 38, \n     39, 40, 22, 35], # extremely stony and bouldery\n]\nsoil_dict = {}\nfor index, soil_group in enumerate(soils):\n    for soil in soil_group:\n        soil_dict[soil] = index\n\ndef rocky(df):\n    df['Rocky'] = sum(i * df['Soil_Type' + str(i)] for i in range(1,41))\n    df['Rocky'] = df['Rocky'].map(soil_dict)\n    return df","839ef2b7":"t = time()\nall_data = aspect_slope(all_data)\nall_data = distances(all_data)\nall_data = OHE_to_cat(all_data, 'Wilderness_Area', [1,5])\nall_data = OHE_to_cat(all_data, 'Soil_Type', [1,41])\nall_data = rocky(all_data)\nall_data.drop(['Soil_Type7', 'Soil_Type15', 'train'] + questionable_0, axis = 1, inplace = True)\n\n# Important columns: https:\/\/www.kaggle.com\/hoangnguyen719\/beginner-eda-and-feature-engineering\nimportant_cols = ['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology'\n                  , 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways'\n                  , 'Hillshade_Noon', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1'\n                  , 'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type3', 'Soil_Type4', 'Soil_Type10'\n                  , 'predicted_Hillshade_9am', 'predicted_Hillshade_3pm', 'AspectSin', 'AspectCos'\n                  , 'AspectSin_Slope', 'AspectCos_Slope', 'AspectSin_Slope_Abs', 'AspectCos_Slope_Abs'\n                  , 'Hillshade_Mean', 'Euclidean_to_Hydrology', 'EuclidHydro_Slope'\n                  , 'Elevation_VDH_sum', 'Elevation_VDH_diff', 'Elevation_2', 'Elevation_3'\n                  , 'Elevation_log1p', 'HDFP_HDR_diff', 'HDFP_HDR_sum', 'HDFP_HDH_diff'\n                  , 'HDFP_HDH_sum', 'HDR_HDH_diff', 'HDR_HDH_sum', 'Horizontal_sum'\n                  , 'Wilderness_Area', 'Soil_Type', 'Rocky'\n                 ]\n\nall_data = all_data[important_cols]\nprint('Total data transforming time: {}'.format(timer(t)))","2b0e20f7":"X_train = all_data.iloc[:n_train,:].copy()\nX_test = all_data.iloc[n_train:, :].copy()\ndel all_data\n\ndef mem_reduce(df):\n    # credit: https:\/\/www.kaggle.com\/arateris\/2-layer-k-fold-learning-forest-cover\n    t = time()\n    start_mem = df.memory_usage().sum() \/ 1024.0**2\n    for col in df.columns:\n        if df[col].dtype=='float64': \n            df[col] = df[col].astype('float32')\n        if df[col].dtype=='int64': \n            if df[col].max()<1: df[col] = df[col].astype(bool)\n            elif df[col].max()<128: df[col] = df[col].astype('int8')\n            elif df[col].max()<32768: df[col] = df[col].astype('int16')\n            else: df[col] = df[col].astype('int32')\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Reduce from {0:.3f} MB to {1:.3f} MB (decrease by {2:.2f}%)'.format(start_mem, end_mem, \n                                                                (start_mem - end_mem)\/start_mem*100))\n    print('Total memory reduction time: {}'.format(timer(t)))\n    return df\n\nX_train = mem_reduce(X_train)\nprint('='*10)\nX_test=mem_reduce(X_test)\ngc.collect()","6eb25bab":"X_train_train, X_train_test, y_train_train, y_train_test = train_test_split(X_train, y_train\n                                                                           , test_size = 0.3\n                                                                           , random_state = rand_state\n                                                                           )\ndel X_test","78e2cd17":"# RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators = 719\n                             , max_depth = 464\n                             , max_features = 0.3\n                             , min_samples_split = 2\n                             , min_samples_leaf = 1\n                             , bootstrap = False\n                             , verbose = 0\n                             , random_state = rand_state\n                            )\n# ExtraTreesClassifier\netc = ExtraTreesClassifier(n_estimators = 177\n                          , max_depth = 794\n                          , max_features = 0.9\n                          , min_samples_leaf = 1\n                          , min_samples_split = 2\n                          , bootstrap = False\n                          )\n\n# AdaBoostClassifier\nadac = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 16)\n                         , n_estimators = 794\n                         , learning_rate = 1\n                         )\n\n# LightGBMClassifier\nlgbc = LGBMClassifier(num_leaves = 50\n                      , max_depth = 15\n                      , learning_rate = 0.1\n                      , n_estimators = 1000\n                      , reg_lambda = 0.1\n                      , objective = 'multiclass'\n                      , num_class = 7\n                     )\n\n# KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 1\n                           , n_jobs =-1)","7253731c":"meta_clf = RandomForestClassifier(n_estimators = 700\n                                  , max_depth = 300\n                                  , min_samples_split = 2\n                                  , min_samples_leaf = 1\n                                  , max_features = 1\n                                  , bootstrap = False\n                                 )\n\nscc = StackingCVClassifier(classifiers = [rfc, etc, adac, lgbc, knn]\n                           , meta_classifier = meta_clf\n                           , cv = 3\n                           , random_state = rand_state\n                           , use_probas = True\n                           , verbose = 0\n                          )\n\nt = time()\nscc.fit(X_train_train, y_train_train)\nprint('Total training time: {}'.format(timer(t)))\ny_train_predict = scc.predict(X_train_test)\ny_train_predict = pd.Series(y_train_predict, index = X_train_test.index)","78b2115a":"# OUTPUT prediction\ntrain = reload('train.csv')\ntest = reload('test.csv')\ntrain_train = train.iloc[X_train_train.index - 1,].copy()\ntrain_test = train.iloc[X_train_test.index - 1,].copy() # because index=5864 equal position=5863\ntrain_test['Actual'] = y_train_test\ntrain_test['Predict'] = y_train_predict","c1ff1508":"errors = train_test.groupby('Actual')['Predict'].value_counts().sort_index().unstack(level=1).fillna(0)\nfor col in errors.columns:\n    errors[str(col)+'_pct'] = round(errors[col] \/ errors.sum(axis=1),3)\nplt.figure(figsize=(10,10))\nsns.heatmap(errors[[str(col) + '_pct' for col in range(1,8)]], annot=True)\nplt.show()","c8c5e529":"# stacking the errors table above\nerrors = errors[[str(i)+'_pct' for i in range(1,8)]].stack().reset_index()\nerrors.columns = list(errors.columns[:2]) + ['Actual_Predict_rate']\nerrors['Actual_Predict'] = [str(actual)+'_'+pred[0] for actual,pred in zip(errors.Actual\n                                                                           , errors.Predict\n                                                                          )]\n\nprint('Number of noticeable errors (errors with Actual_Predict_rate >= 0.05): {}'.format(len(errors[errors.Actual_Predict_rate>= 0.05]) - 7))\nprint(*[x for x in errors[errors.Actual_Predict_rate >= 0.05].Actual_Predict if int(x[0]) != int(x[2])], sep=', ')\n\nnoticeable_errors = [x for x in errors[errors.Actual_Predict_rate >= 0.05].Actual_Predict if int(x[0]) != int(x[2])]","26d54ad6":"# add error rate in the dataset\ntrain_test['Mis_Classified'] = [1 if x==False else 0 for x in train_test.Actual == train_test.Predict ]\ntrain_test['Actual_Predict'] = train_test.Actual.astype(str) + '_' + train_test.Predict.astype(str)\ntrain_test = train_test.merge(errors[['Actual_Predict_rate', 'Actual_Predict']], on='Actual_Predict', how='left')\ndel errors\ntrain_test.head()","543ec7b4":"numerical = ['Elevation', 'Horizontal_Distance_To_Hydrology',\n             'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways',\n             'Horizontal_Distance_To_Fire_Points',\n             'Aspect', 'Slope', \n             'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm']\n\ncategorical = ['Soil_Type{}'.format(i) for i in range(1,41) if i!=7 and i!=15] + ['Wilderness_Area{}'.format(i) for i in range(1,5)]","8fa382c1":"desc = train_test[numerical].describe().T.join(train_train[numerical].describe().T\n                                                     , how='left'\n                                                     , lsuffix = '_predict'\n                                                     , rsuffix = '_train'\n                                                    )\n\ndesc = desc.reindex([measure+df for measure in ['min_', '50%_', 'mean_', 'std_', 'max_']\n                    for df in ['predict', 'train']], axis = 1)\nprint('Test and Train dataset:')\ndesc","2aa27636":"desc = train_test[train_test.Mis_Classified == 0][numerical].describe().T\ndesc = desc.join(train_test[train_test.Mis_Classified == 1][numerical].describe().T\n                 , how='left'\n                 , lsuffix = '_cor'\n                 , rsuffix = '_mis'\n                )\n\ndesc = desc.reindex([measure+df for measure in ['min_', '50%_', 'mean_', 'std_', 'max_']\n                    for df in ['cor', 'mis']], axis = 1)\nprint('Correctly-classified and mis-classified:')\ndesc","3ab5eaae":"def distplot(df, columns, colors=['red', 'green', 'blue', 'c', 'purple'], bins_num = None, hist = True, kde = False): \n    # df is either dataframe or list of ('name_df',df)\n    # col is either string or list\n    sns.set_style('whitegrid')\n#### CONVERT INPUT DATA'S TYPE\n    if type(df) != list: \n        df = [('df',df)] \n    if type(columns) == str: \n        columns = [columns]\n    l_col = len(columns)\n    l_df = len(df)\n###### CALCULATE ROWS AND COLS OF GRAPHS\n    c = min([l_col, 3]) # cols\n    r = l_col\/\/3 + sum([l_col%3!=0]) # rows\n    fig = plt.figure(figsize=(c*7, r*6))\n    \n    for index in range(l_col):\n        column = columns[index]\n####### CALCULATE BINS OF HIST\n        if bins_num == None: \n            combined_data = np.hstack(tuple([df[x][1][column] for x in range(l_df)])) \n            n_bins = min(50,len(np.unique(combined_data))) # number of bins: <= 50\n            bins = np.histogram(combined_data, bins=n_bins)[1] # get \"edge\" of each bin\n        bins = next(b for b in [bins_num, bins] if b is not None)\n####### ADD SUBPLOT AND PLOT\n        ax = fig.add_subplot(r,c,index+1) \n        for i in range(l_df):\n            sns.distplot(df[i][1][column], bins=bins, hist = hist, kde=kde, color=colors[i], \n                         label=df[i][0], norm_hist=True, hist_kws={'alpha':0.4})\n        plt.xlabel(column)\n        if (l_df>1) & ((index+1) % c == 0): # legend at the graph on the right\n            ax.legend()\n    plt.tight_layout()\n    plt.show() ","dc3596b4":"distplot([('correct',train_test[train_test.Mis_Classified == 0])\n         , ('incorrect', train_test[train_test.Mis_Classified == 1])\n         , ('full_train', train)\n         , ('full_test', test)]\n        , columns = numerical\n         , hist = False\n         , kde = True\n        )","2a301a6d":"train_test = OHE_to_cat(train_test, 'Wilderness_Area', [1,5])\ntrain_test = OHE_to_cat(train_test, 'Soil_Type', [1,41])\ntrain_test","e6c7e580":"f = plt.figure(figsize=(8,6))\nsns.boxplot(x='Cover_Type', y='Elevation', data=train)\nplt.show()","601e1ada":"train_test['types'] = np.where(train_test.Actual.isin([1,2]), 'Cover Type 1 or 2'\n                                , np.where(train_test.Actual.isin([3,6]), 'Cover Type 3 or 6', 'Other Types'))\ndistplot([(x,train_test[(train_test['types'] == x) & (train_test.Mis_Classified == 1)]) for x in ['Cover Type 1 or 2'\n                                                                                                     , 'Cover Type 3 or 6'\n                                                                                                     , 'Other Types']]\n        , columns = 'Elevation'\n         , hist = False\n         , kde = True\n        )\ntrain_test.drop(columns = 'types', inplace = True)","a22170da":"train_test['errors'] = np.where(train_test.Actual_Predict.isin(['1_2','2_1'])\n                                , '1_2 or 2_1'\n                                , np.where(train_test.Actual_Predict.isin(['3_6', '6_3']), '3_6 or 6_3', 'other errors')\n                               )\ndistplot([(x,train_test[(train_test['errors'] == x) & (train_test.Mis_Classified == 1)]) for x in ['1_2 or 2_1', '3_6 or 6_3', 'other errors']]\n        , columns = 'Elevation'\n         , hist = False\n         , kde = True\n        )\ntrain_test.drop(columns = 'errors', inplace = True)","82d48b74":"distplot([(a_p,train_test[train_test.Actual_Predict == a_p]) for a_p in ['1_1', '1_2', '2_1', '2_2']]\n        , columns = 'Elevation'\n         , hist = False\n         , kde = True\n        )","040d4089":"train_test['errors'] = np.where(train_test.Actual_Predict.isin(['1_2','2_1'])\n                                , 'error 1-2'\n                                , np.where(train_test.Actual_Predict.isin(['1_1', '2_2']), 'correct 1-2', 'others')\n                               )\n\ndistplot([(e,train_test[train_test.errors == e]) for e in ['error 1-2', 'correct 1-2']]\n        , columns = numerical\n         , hist = False\n         , kde = True\n        )\n\ntrain_test.drop(columns = 'errors', inplace=True)","6b670c96":"**...This kernel is still under further work...**<br><br><br>\nThis kernel is part 7 of my work, listed below, in this competition. Any comments or suggestions you may have are greatly appreciated!\n1. [EAD and Feature Engineering](https:\/\/www.kaggle.com\/hoangnguyen719\/1-eda-and-feature-engineering\/notebook)\n2. [ExtraTreesClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/extratree-tuning)\n3. [AdaboostClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/adaboost-tuning)\n4. [LGBMClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/lightgbm-tuning)\n5. [KNearestClassifier tuning](https:\/\/www.kaggle.com\/hoangnguyen719\/knn-tuning)\n6. [StackingCVClassifier (use_probas tuning)](https:\/\/www.kaggle.com\/hoangnguyen719\/stacking-use-probas-tuning)\n7. [Mis-Classified Inspection](https:\/\/www.kaggle.com\/hoangnguyen719\/mis-classified-inspection)\n<br><br>\n*Note*: Many of my EDA parts have been greatly inspired by previous kernels in the competition and I have been trying to give credits to the owners as much as I can. However, because (1) many kernels appear to have the same ideas (even codes), which makes it hard to trace back where the ideas originated from, and (2) I carelessly forgot to note down all the sources (this is totally my bad), sometimes the credit may not be given where it's due. I apologize beforehand, and please let me know in the comment section if you have any question or suggestions. Thank you!\n<br><br>\n**Outline of this notebook**<br>\nI. [Package and Data Loading](#I.-Package-and-Data-Loading)<br>\nII. [Feature Engineering](#II.-Feature-Engineering) <br>\nIII. [Model Fitting](#III.-Model-Fitting) <br>\nIV. [Mis-classified Inspection](#IV.-Mis-classified-Inspection)","e96ec2a8":"### 2.1. Elevation\nFirst, let's look at the distribution of `Elevation` among the 7 `Cover_Type`.","77164253":"It seems that the spike seen above was most likely due to the 1_2 and 2_1 error types. To confirm this, let's see the `Elevation` histogram by `Actual_Predict`.","81f1f2d2":"### 2.3. Soil_Type & Wilderness","fb289e6e":"From this, we can see that though `Elevation` is a powerful predictor of `Cover_Type`, it can still confuse between `Cover_Type1` and `Cover_Type2`.","afff518e":"### 2.2. Distances","48842554":"## 2. Other Features\n### 2.1. Aspect & Slope","dc892c29":"### 2.4. Rockiness","1e84a273":"Let's dig deeper to in other features of `Actual_Predict` type1_2 and 2_1.","cc438312":"Mis-classified often have\n- higher `Elevation` (~ 3,000)\n- `Horizontal_Distance_To_Roadways` and `Horizontal_Distance_To_Fire_Points` slightly less skew to the right\n- `Slope` skewer to the right\n- `Hillshade_3pm` skewer to the left\n<br><br>\nthan the whole training set.","a0247e99":"# IV. Mis-classified Inspection\n## 1. Actual - Predicted distribution","6c2e3b80":"## 2. Model fitting","f0c787e2":"Let's look at `Elevation` distribution of `Cover_Type` with significant error rates (1, 2, 3 and 6)","bfebdc00":"## 1. Data Transform","7a6dd814":"Alright, so `Cover_Type` 1 and 2 are easily mistaken with one another, and so are type 3 and 6.","11caea04":"# II. Feature Engineering\n## 1. 0s imputation","2a2d868c":"`Cover_Type` 1 and 2 seem to be the most mistaken. Let's see what other types of mistake are also frequent (frequency threshold set at 0.05, meaning any noteworthy error types are those that occur 5% of more times).","5122afba":"### Memory reduction","46b22095":"## 2. Distribution of Significant Error Types","fb783f6c":"# I. Package and Data Loading","188d4f43":"# III. Model fitting"}}