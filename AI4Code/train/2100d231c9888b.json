{"cell_type":{"77b03853":"code","256f11f8":"code","305d8a03":"code","ae7e3a57":"code","1c96e594":"code","6202e993":"code","7189394f":"code","44979751":"code","64e8df6a":"code","f8f5a1a1":"code","5c59be8a":"code","13e41263":"code","6cb0fd21":"code","dc1ca42f":"code","4886fa0c":"code","a3448503":"code","c8b1c78a":"code","334c5d11":"code","4011f870":"code","f7318c21":"code","dbf2d37b":"code","cd629ac6":"code","4514c152":"code","bb5d0fa4":"code","48109c87":"code","00cae10b":"code","d26700ba":"markdown","74fcd4a7":"markdown"},"source":{"77b03853":"# Imported Libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nimport time\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport collections\nimport imblearn\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.metrics import make_scorer, f1_score\nfrom sklearn import model_selection\nfrom sklearn.datasets import make_classification\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom pylab import rcParams\nrcParams['figure.figsize'] = 14, 8\nRANDOM_SEED = 42\nLABELS = [\"Normal\", \"Fraud\"]\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom collections import Counter\nfrom sklearn.model_selection import KFold, StratifiedKFold\nimport warnings\nwarnings.filterwarnings(\"ignore\")","256f11f8":"import pandas as pd\ndata = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndata.head()\n","305d8a03":"data.isnull().sum().max()\n","ae7e3a57":"print('Legitimate transactions are', round(data['Class'].value_counts()[0]\/len(data) * 100,2), '% of the dataset')\nprint('Fraud transactions are', round(data['Class'].value_counts()[1]\/len(data) * 100,2), '% of the dataset')","1c96e594":"#colors = [\"#DF0101\", \"#0101DF\"]\ncolors = ['red', 'gold']\nexplode = (0.1, 0)  # explode 1st slice\n\n#sns.countplot('Class', data=data, palette=colors)\nlabels = ['Fraud (0.17)', 'Legit (99.83)']\nsizes = [492,284315]\nplt.pie(sizes,explode = explode, colors=colors, labels = labels,shadow=True, startangle=90)\n#plt.title('Distribution of 2 classes \\n (0: Legit || 1: Fraud)', fontsize=10)\n\n\n","6202e993":"# Scaling the Time and Amount features\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\n\nstd_scaler = StandardScaler()\nrob_scaler = RobustScaler()\n\ndata['scaled_amount'] = std_scaler.fit_transform(data['Amount'].values.reshape(-1,1))\ndata['scaled_time'] = std_scaler.fit_transform(data['Time'].values.reshape(-1,1))\n\ndata.drop(['Time','Amount'], axis=1, inplace=True)\nscaled_amount = data['scaled_amount']\nscaled_time = data['scaled_time']\n\ndata.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)\ndata.insert(0, 'scaled_amount', scaled_amount)\ndata.insert(1, 'scaled_time', scaled_time)\n\n# Amount and Time are Scaled!\n\ndata.head()","7189394f":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nprint('No Frauds', round(data['Class'].value_counts()[0]\/len(data) * 100,2), '% of the dataset')\nprint('Frauds', round(data['Class'].value_counts()[1]\/len(data) * 100,2), '% of the dataset')\n\nX = data.drop('Class', axis=1)\ny = data['Class']\n\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    original_Xtrain, original_Xtest = X.iloc[train_index], X.iloc[test_index]\n    original_ytrain, original_ytest = y.iloc[train_index], y.iloc[test_index]\n\n# We already have X_train and y_train for undersample data thats why I am using original to distinguish and to not overwrite these variables.\n# original_Xtrain, original_Xtest, original_ytrain, original_ytest = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Check the Distribution of the labels\n\n\n# Turn into an array\noriginal_Xtrain = original_Xtrain.values\noriginal_Xtest = original_Xtest.values\noriginal_ytrain = original_ytrain.values\noriginal_ytest = original_ytest.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprint('-' * 100)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label\/ len(original_ytrain))\nprint(test_counts_label\/ len(original_ytest))","44979751":"# To improve the accuracy of the model, we can remove those features that are highly\n# correlated with the class and are extreme outliers. We can change the threshold \n# to detect the outliers\nfrom scipy.stats import norm\n\nf, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(20, 6))\n\nv14_fraud_dist = data['V14'].loc[data['Class'] == 1].values\nsns.distplot(v14_fraud_dist,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('V14 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nv12_fraud_dist = data['V12'].loc[data['Class'] == 1].values\nsns.distplot(v12_fraud_dist,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('V12 Distribution \\n (Fraud Transactions)', fontsize=14)\n\n\nv17_fraud_dist = data['V17'].loc[data['Class'] == 1].values\nsns.distplot(v17_fraud_dist,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('V17 Distribution \\n (Fraud Transactions)', fontsize=14)\n\nplt.show()","64e8df6a":"# # -----> V14 Removing Outliers (Highest Negative Correlated with Labels)\nv14_fraud = data['V14'].loc[data['Class'] == 1].values\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\nv14_iqr = q75 - q25\nprint('iqr: {}'.format(v14_iqr))\n\nv14_cut_off = v14_iqr * 1.5\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('Cut Off: {}'.format(v14_cut_off))\nprint('V14 Lower: {}'.format(v14_lower))\nprint('V14 Upper: {}'.format(v14_upper))\n\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('Feature V14 Outliers for Fraud Cases: {}'.format(len(outliers)))\nprint('V10 outliers:{}'.format(outliers))\n\ndata = data.drop(data[(data['V14'] > v14_upper) | (data['V14'] < v14_lower)].index)\nprint('----' * 44)\n\n# -----> V12 removing outliers from fraud transactions\nv12_fraud = data['V12'].loc[data['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 Lower: {}'.format(v12_lower))\nprint('V12 Upper: {}'.format(v12_upper))\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\nprint('V12 outliers: {}'.format(outliers))\nprint('Feature V12 Outliers for Fraud Cases: {}'.format(len(outliers)))\ndata = data.drop(data[(data['V12'] > v12_upper) | (data['V12'] < v12_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(data)))\nprint('----' * 44)\n\n\n# Removing outliers V17 Feature\nv17_fraud = data['V17'].loc[data['Class'] == 1].values\nq25, q75 = np.percentile(v17_fraud, 25), np.percentile(v17_fraud, 75)\nv17_iqr = q75 - q25\n\nv17_cut_off = v17_iqr * 1.5\nv17_lower, v17_upper = q25 - v17_cut_off, q75 + v17_cut_off\nprint('V17 Lower: {}'.format(v17_lower))\nprint('V17 Upper: {}'.format(v17_upper))\noutliers = [x for x in v17_fraud if x < v17_lower or x > v17_upper]\nprint('V17 outliers: {}'.format(outliers))\nprint('Feature V17 Outliers for Fraud Cases: {}'.format(len(outliers)))\ndata = data.drop(data[(data['V17'] > v17_upper) | (data['V17'] < v17_lower)].index)\nprint('Number of Instances after outliers removal: {}'.format(len(data)))\n\n# Boxplots with outliers removed\n\nf,(ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20,6))\n\ncolors = ['#B3F9C5', '#f9c5b3']\n# Feature V14\nsns.boxplot(x=\"Class\", y=\"V14\", data=data,ax=ax1, palette=colors)\nax1.set_title(\"V14 Feature \\n Reduction of outliers\", fontsize=14)\nax1.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature 12\nsns.boxplot(x=\"Class\", y=\"V12\", data=data, ax=ax2, palette=colors)\nax2.set_title(\"V12 Feature \\n Reduction of outliers\", fontsize=14)\nax2.annotate('Fewer extreme \\n outliers', xy=(0.98, -17.3), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n# Feature V17\nsns.boxplot(x=\"Class\", y=\"V17\", data=data, ax=ax3, palette=colors)\nax3.set_title(\"V17 Feature \\n Reduction of outliers\", fontsize=14)\nax3.annotate('Fewer extreme \\n outliers', xy=(0.95, -16.5), xytext=(0, -12),\n            arrowprops=dict(facecolor='black'),\n            fontsize=14)\n\n\nplt.show()","f8f5a1a1":"# Handling imbalanced structure of the data with undersampling the overpresented class \n# Just for building the model not for testing!(avoiding overfitting)\n# I shuffle the data before creating the sub sample\ndata = data.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_data = data.loc[data['Class'] == 1]\nlegit_data = data.loc[data['Class'] == 0][:492]\n\nnormal_distributed_data = pd.concat([fraud_data, legit_data])\n\n# Shuffle dataframe rows\nnew_data = normal_distributed_data.sample(frac=1, random_state=42)\n\nnew_data.head()\n\nsns.countplot('Class', data=new_data, palette=colors)\nplt.title('Equally Distributed Classes', fontsize=10)\nplt.show()","5c59be8a":"# Visualizing the original dataset using PCA  after removing 2 features outliers\nX = data.drop('Class', axis=1)\ny = data['Class']\n\n\n# PCA Implementation\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(24,6))\n# labels = ['No Fraud', 'Fraud']\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\n\nblue_patch = mpatches.Patch(color='#0101DF', label='No Fraud')\nred_patch = mpatches.Patch(color='#DF0101', label='Fraud')\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\n\nax2.grid(True)\n\nax2.legend(handles=[blue_patch, red_patch])\n\nplt.show()","13e41263":"# Classifier Selection\nX = new_data.drop('Class', axis=1)\ny = new_data['Class']\n\nfrom sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values\n\n# Simple classifiers \n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"Random Forest Classifier\": RandomForestClassifier(),\n    \"XGBoost Classifier\": XGBClassifier()    \n}\n\n\n# Cross validation score for different classifiers\nfrom sklearn.model_selection import cross_val_score\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")","6cb0fd21":"# The use of anomaly detection algorithms with original and undersampled dataset\nfrom sklearn.metrics import confusion_matrix\n\n# Isolation Forest \n\noutliers = new_data.loc[data['Class']==1]\nnormal = new_data.loc[data['Class']==0]\noutliers = outliers.drop(['Class'] , axis=1)\nnormal = normal.drop(['Class'] , axis=1)\nnormal_=np.array(normal)\noutliers_=np.array(outliers)\nX_outliers = np.array(outliers)\nX_lof=np.r_[normal_+2,normal_-2,outliers_]\n\nclf = IsolationForest(max_samples=100)\nclf.fit(X_train)\n\ny_pred_train = clf.predict(X_train)\ny_pred_test = clf.predict(X_test)\ny_pred_outliers = clf.predict(X_outliers)\n\n# plot the line, the samples, and the nearest vectors to the plane\nx_train=X_train[:10000,[1,28]]\nx_test=X_test[:1000,[1,28]]\nx_outliers=X_outliers[:100,[1,28]]\n\nx_train=np.array(x_train)\nx_test=np.array(x_test)\nx_outliers=np.array(x_outliers)\n\nclf_test = IsolationForest(max_samples=100)\nclf_test.fit(x_train)\n\nxx, yy = np.meshgrid(np.linspace(-5, 5, 50), np.linspace(-5, 5, 50))\nZ = clf_test.decision_function(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\nplt.title(\"IsolationForest\")\nplt.contourf(xx, yy, Z, cmap=plt.cm.Blues_r)\n\nb1 = plt.scatter(x_train[:, 0], x_train[:, 1], c='red',\n                 s=20, edgecolor='k')\nb2 = plt.scatter(x_test[:, 0], x_test[:, 1], c='green',\n                 s=20, edgecolor='k')\nc = plt.scatter(x_outliers[:, 0], x_outliers[:, 1], c='white',\n                s=20, edgecolor='k')\nplt.axis('tight')\nplt.xlim((-5, 5))\nplt.ylim((-5, 5))\nplt.legend([b1, b2, c],\n           [\"training observations\",\n            \"new regular observations\", \"new abnormal observations\"],\n           loc=\"upper left\")\nplt.show()\n\n# Isolation Forest Evaluation\n\nprint(\"Test Accuracy score for IF   :\", list(y_pred_test).count(1)\/y_pred_test.shape[0])\nprint(\"Outliers Accuracy for IF :\", list(y_pred_outliers).count(-1)\/y_pred_outliers.shape[0])\n\n#cnf_IF = confusion_matrix(y_test,y_pred_test)\n#fig, ax = plt.subplots(2, 3,figsize=(22,12))\n\n#cnf_IF\n#a=sns.heatmap(cnf_IF, annot=True, cmap=plt.cm.copper)\n#plt.title(\"Isolation Forest with undersampling \\n Confusion Matrix\", fontsize=14)\n","dc1ca42f":"# Finding the best optimizer using GridSearchCV cross validation\n# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Logistic Regression \nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\n\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\n# We automatically get the logistic regression with the best parameters.\nlog_reg = grid_log_reg.best_estimator_\n\nknears_params = {\"n_neighbors\": list(range(2,5,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\n# KNears best estimator\nknears_neighbors = grid_knears.best_estimator_\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\n\n# SVC best estimator\nsvc = grid_svc.best_estimator_\n\n# Random Forest Classifier\nparam_grid = { \n    'n_estimators': [200, 500],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8],\n    'criterion' :['gini', 'entropy']\n}\n\ngrid_forest = GridSearchCV(RandomForestClassifier(), param_grid)\ngrid_forest.fit(X_train, y_train)\n\n# RF best estimator\nRF = grid_forest.best_estimator_\n# XGBoost Classifier\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['binary:logistic'],\n              'learning_rate': [0.05], #so called `eta` value\n              'max_depth': [6],\n              'min_child_weight': [11],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [10], #number of trees, change it to 1000 for better results\n              'missing':[-999],\n              'seed': [1337]}\ngrid_xgb = GridSearchCV(XGBClassifier(), parameters)\ngrid_xgb.fit(X_train, y_train)\n# XGB best estimator\nxgb = grid_xgb.best_estimator_\n\n# Isolation Forest \n#f1sc = make_scorer(f1_score(y_test,y_pred,average='micro'))\n\n#param_IF = {'n_estimators': list(range(100, 800, 5)), \n              #'max_samples': list(range(100, 500, 5)), \n              #'contamination': [0.1, 0.2, 0.3, 0.4, 0.5], \n              #'max_features': [5,10,15], \n              #'bootstrap': [True, False], \n              #'n_jobs': [5, 10, 20, 30]}\n#grid_IF = GridSearchCV(IsolationForest(),param_IF,cv=5,scoring=f1sc)\n#grid_IF.fit(X_train,y_train)\n# Isolation Forest best estimator\n#IF = grid_IF.best_estimator_\n","4886fa0c":"# Overfitting Case\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Overfitting : Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Overfitting: Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Overfitting: Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\nRF_score = cross_val_score(RF, X_train, y_train, cv=5)\nprint('Overfitting: RandomForest Classifier Cross Validation Score', round(RF_score.mean() * 100, 2).astype(str) + '%')\nxgb_score = cross_val_score(xgb, X_train, y_train, cv=5)\nprint('Overfitting: XGBoost Classifier Cross Validation Score', round(xgb_score.mean() * 100, 2).astype(str) + '%')\n#IF_score = cross_val_score(IF, X_train, y_train, cv=5)\n#print('Overfitting: Isolation Forest Cross Validation Score', round(IF_score.mean() * 100, 2).astype(str) + '%')","a3448503":"# We will undersample during cross validating\nundersample_X = new_data.drop('Class', axis=1)\nundersample_y = new_data['Class']\n\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    print(\"Train:\", train_index, \"Test:\", test_index)\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values \n\nundersample_accuracy = []\nundersample_precision = []\nundersample_recall = []\nundersample_f1 = []\nundersample_auc = []\n\n# Implementing NearMiss Technique \n# Distribution of NearMiss (Just to see how it distributes the labels we won't use these variables)\nX_nearmiss, y_nearmiss = NearMiss().fit_sample(undersample_X.values, undersample_y.values)\nprint('NearMiss Label Distribution: {}'.format(Counter(y_nearmiss)))\n\n# Cross Validating the right way\n\nfor train, test in sss.split(undersample_Xtrain, undersample_ytrain):\n    undersample_pipeline = imbalanced_make_pipeline(NearMiss(sampling_strategy='majority'), log_reg) # SMOTE happens during Cross Validation not before..\n    undersample_model = undersample_pipeline.fit(undersample_Xtrain[train], undersample_ytrain[train])\n    undersample_prediction = undersample_model.predict(undersample_Xtrain[test])\n    \n    undersample_accuracy.append(undersample_pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    undersample_precision.append(precision_score(original_ytrain[test], undersample_prediction))\n    undersample_recall.append(recall_score(original_ytrain[test], undersample_prediction))\n    undersample_f1.append(f1_score(original_ytrain[test], undersample_prediction))\n    undersample_auc.append(roc_auc_score(original_ytrain[test], undersample_prediction))","c8b1c78a":"# Let's Plot the Learning Curves\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import learning_curve\n\ndef plot_learning_curve(estimator1, estimator2, estimator3, estimator4, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(20,14), sharey=True)\n    if ylim is not None:\n        plt.ylim(*ylim)\n        \n    # First Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator1, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax1.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax1.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax1.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax1.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax1.set_title(\"Logistic Regression Learning Curve\", fontsize=14)\n    ax1.set_xlabel('Training size (m)')\n    ax1.set_ylabel('Score')\n    ax1.grid(True)\n    ax1.legend(loc=\"best\")\n    \n    # Second Estimator \n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator2, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax2.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax2.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax2.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax2.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax2.set_title(\"Knears Neighbors Learning Curve\", fontsize=14)\n    ax2.set_xlabel('Training size (m)')\n    ax2.set_ylabel('Score')\n    ax2.grid(True)\n    ax2.legend(loc=\"best\")\n    \n    # Third Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator3, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax3.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax3.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax3.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax3.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax3.set_title(\"Support Vector Classifier \\n Learning Curve\", fontsize=14)\n    ax3.set_xlabel('Training size (m)')\n    ax3.set_ylabel('Score')\n    ax3.grid(True)\n    ax3.legend(loc=\"best\")\n    \n    # Fourth Estimator\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator4, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    ax4.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"#ff9124\")\n    ax4.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"#2492ff\")\n    ax4.plot(train_sizes, train_scores_mean, 'o-', color=\"#ff9124\",\n             label=\"Training score\")\n    ax4.plot(train_sizes, test_scores_mean, 'o-', color=\"#2492ff\",\n             label=\"Cross-validation score\")\n    ax4.set_title(\"XGBoost Classifier \\n Learning Curve\", fontsize=14)\n    ax4.set_xlabel('Training size (m)')\n    ax4.set_ylabel('Score')\n    ax4.grid(True)\n    ax4.legend(loc=\"best\")\n    \n    \n    \n    \n    return plt  \n\n","334c5d11":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\nplot_learning_curve(log_reg, knears_neighbors, svc, xgb, X_train, y_train, (0.87, 1.01), cv=cv, n_jobs=4)\n","4011f870":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n                             method=\"decision_function\")\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nRF_pred = cross_val_predict(RF, X_train, y_train, cv=5)\n\nxgb_pred = cross_val_predict(xgb, X_train, y_train, cv=5)\n\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('Knears Neighbors Classifier: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Random Forest Classifier: ', roc_auc_score(y_train, RF_pred))\nprint('XGBoost Classifier: ', roc_auc_score(y_train, xgb_pred))\n","f7318c21":"# Plotting the roc curve\nlog_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, RF_pred)\nxgb_fpr, xgb_tpr, xgb_threshold = roc_curve(y_train, xgb_pred)\n\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr,knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr,xgb_fpr,xgb_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n 5 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_train, RF_pred)))\n    plt.plot(xgb_fpr, xgb_tpr, label='XGBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_train, xgb_pred)))\n\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr,knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr,xgb_fpr,xgb_tpr)\nplt.show()","dbf2d37b":"#%% Logistic Regression seems to be the winner!\nfrom sklearn.metrics import precision_recall_curve\n\nprecision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)\nfrom sklearn.metrics import recall_score, precision_score, f1_score, accuracy_score\ny_pred = log_reg.predict(X_train)\n\n# Overfitting Case\nprint('---' * 45)\nprint('Overfitting: \\n')\nprint('Recall Score: {:.2f}'.format(recall_score(y_train, y_pred)))\nprint('Precision Score: {:.2f}'.format(precision_score(y_train, y_pred)))\nprint('F1 Score: {:.2f}'.format(f1_score(y_train, y_pred)))\nprint('Accuracy Score: {:.2f}'.format(accuracy_score(y_train, y_pred)))\nprint('---' * 45)\n\n# How it should look like\nprint('---' * 45)\nprint('How it should be:\\n')\nprint(\"Accuracy Score: {:.2f}\".format(np.mean(undersample_accuracy)))\nprint(\"Precision Score: {:.2f}\".format(np.mean(undersample_precision)))\nprint(\"Recall Score: {:.2f}\".format(np.mean(undersample_recall)))\nprint(\"F1 Score: {:.2f}\".format(np.mean(undersample_f1)))\nprint('---' * 45)\nundersample_y_score = log_reg.decision_function(original_Xtest)\nfrom sklearn.metrics import average_precision_score\n\nundersample_average_precision = average_precision_score(original_ytest, undersample_y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      undersample_average_precision))\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, undersample_y_score)\n\nplt.step(recall, precision, color='#004a93', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#48a6ff')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('UnderSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          undersample_average_precision), fontsize=16)","cd629ac6":"#%% Logistic Regression with SMOTE\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\n\n\nprint('Length of X (train): {} | Length of y (train): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Length of X (test): {} | Length of y (test): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# List to append the score and then find the average\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Classifier with optimal parameters\n# log_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm = LogisticRegression()\n\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\n# Implementing SMOTE Technique \n# Cross Validating the right way\n# Parameters\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    best_est = rand_log_reg.best_estimator_\n    prediction = best_est.predict(original_Xtrain[test])\n    \n    accuracy_lst.append(pipeline.score(original_Xtrain[test], original_ytrain[test]))\n    precision_lst.append(precision_score(original_ytrain[test], prediction))\n    recall_lst.append(recall_score(original_ytrain[test], prediction))\n    f1_lst.append(f1_score(original_ytrain[test], prediction))\n    auc_lst.append(roc_auc_score(original_ytrain[test], prediction))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst)))\nprint(\"precision: {}\".format(np.mean(precision_lst)))\nprint(\"recall: {}\".format(np.mean(recall_lst)))\nprint(\"f1: {}\".format(np.mean(f1_lst)))\nprint('---' * 45)\n# Prediction\nlabels = ['No Fraud', 'Fraud']\nsmote_prediction = best_est.predict(original_Xtest)\nprint(classification_report(original_ytest, smote_prediction, target_names=labels))\ny_score = best_est.decision_function(original_Xtest)\naverage_precision = average_precision_score(original_ytest, y_score)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))\nfig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(original_ytest, y_score)\n\nplt.step(recall, precision, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision), fontsize=16)","4514c152":"#%% # SMOTE Technique (OverSampling) After splitting and Cross Validating\nsm = SMOTE(ratio='minority', random_state=42)\n# Xsm_train, ysm_train = sm.fit_sample(X_train, y_train)\n\n\nXsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)\n# We Improve the score by 2% points approximately \n# Implement GridSearchCV and the other models.\n\n# Logistic Regression\nt0 = time.time()\nlog_reg_sm = grid_log_reg.best_estimator_\nlog_reg_sm.fit(Xsm_train, ysm_train)\nt1 = time.time()\nprint(\"Fitting oversample data took :{} sec\".format(t1 - t0))","bb5d0fa4":"#%% TESTING CLASSIFIERS WITH UNDERSAMPLING\nfrom sklearn.metrics import confusion_matrix\n\n# Logistic Regression fitted using SMOTE technique\ny_pred_log_reg = log_reg_sm.predict(original_Xtest)\n\n# Other models fitted with UnderSampling\ny_pred_knear = knears_neighbors.predict(original_Xtest)\ny_pred_svc = svc.predict(original_Xtest)\ny_pred_RF = RF.predict(original_Xtest)\ny_pred_xgb = xgb.predict(original_Xtest)\ny_pred_lr = log_reg.predict(original_Xtest)\n\n\n\n#log_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\nlog_reg_cf = confusion_matrix(original_ytest, y_pred_lr)\nkneighbors_cf = confusion_matrix(original_ytest, y_pred_knear)\nsvc_cf = confusion_matrix(original_ytest, y_pred_svc)\ntree_cf = confusion_matrix(original_ytest, y_pred_RF)\nxgb_cf = confusion_matrix(original_ytest, y_pred_xgb)\n\n\n\n\nfig, ax = plt.subplots(2, 3,figsize=(22,12))\n\n\nsns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.copper)\nax[0, 0].set_title(\"Logistic Regression with undersampling \\n Confusion Matrix\", fontsize=14)\nax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(kneighbors_cf, ax=ax[0][1], annot=True, cmap=plt.cm.copper)\nax[0][1].set_title(\"KNearsNeighbors with undersampling \\n Confusion Matrix\", fontsize=14)\nax[0][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(svc_cf, ax=ax[1][0], annot=True, cmap=plt.cm.copper)\nax[1][0].set_title(\"Suppor Vector Classifier with undersampling \\n Confusion Matrix\", fontsize=14)\nax[1][0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\nsns.heatmap(tree_cf, ax=ax[1][1], annot=True, cmap=plt.cm.copper)\nax[1][1].set_title(\"Random Forest Classifier with undersampling \\n Confusion Matrix\", fontsize=14)\nax[1][1].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[1][1].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\n\nsns.heatmap(xgb_cf, ax=ax[1][2], annot=True, cmap=plt.cm.copper)\nax[1][2].set_title(\"XGBoost Classifier with undersampling \\n Confusion Matrix\", fontsize=14)\nax[1][2].set_xticklabels(['', ''], fontsize=20, rotation=90)\nax[1][2].set_yticklabels(['', ''], fontsize=20, rotation=360)\n\nactual_cmu = confusion_matrix(original_ytest,original_ytest)\n\nsns.heatmap(actual_cmu, ax=ax[0][2], annot=True, cmap=plt.cm.copper)\nax[0][2].set_title(\"100% Accuracy Confusion Matrix for undersampling\", fontsize=14)\nax[0][2].set_xticklabels(['', ''], fontsize=20, rotation=90)\nax[0][2].set_yticklabels(['', ''], fontsize=20, rotation=360)\n\nplt.show()\n\nplot_confusion_matrix(log_reg_cf, labels, title=\"Logistic Regression \\n Confusion Matrix\")\nplot_confusion_matrix(kneighbors_cf, labels, title=\"KNN \\n Confusion Matrix\")\nplot_confusion_matrix(svc_cf, labels, title=\"SVM \\n Confusion Matrix\")\nplot_confusion_matrix(tree_cf, labels, title=\"RF \\n Confusion Matrix\")\nplot_confusion_matrix(xgb_cf, labels, title=\"XGBoost \\n Confusion Matrix\")\nplot_confusion_matrix(actual_cmu, labels, title=\"100% Accuracy \\n Confusion Matrix\")\n\n\n\nfrom sklearn.metrics import classification_report\n\n\nprint('Logistic Regression:')\nprint(classification_report(original_ytest, y_pred_lr))\n\nprint('KNears Neighbors:')\nprint(classification_report(original_ytest, y_pred_knear))\n\nprint('Support Vector Classifier:')\nprint(classification_report(original_ytest, y_pred_svc))\n\nprint('Random Forest Classifier:')\nprint(classification_report(original_ytest, y_pred_RF))\n\nprint('XGBoost Classifier:')\nprint(classification_report(original_ytest, y_pred_xgb))","48109c87":"#%% # Final Score in the test set of logistic regression\nfrom sklearn.metrics import accuracy_score\n\n# Logistic Regression with Under-Sampling\ny_pred = log_reg.predict(original_Xtest)\nundersample_score = accuracy_score(original_ytest, y_pred)\n\n\n\n# Logistic Regression with SMOTE Technique (Better accuracy with SMOTE t)\ny_pred_sm = best_est.predict(original_Xtest)\noversample_score = accuracy_score(original_ytest, y_pred_sm)\n\n###NEW\n# confusion matrix for oversampling\nlog_reg_cf_up = confusion_matrix(original_ytest, y_pred_sm)\n\nfig, ax = plt.subplots(2, 3,figsize=(22,12))\n\n\nsns.heatmap(log_reg_cf_up, ax=ax[0][0], annot=True, cmap=plt.cm.copper)\nax[0, 0].set_title(\"Logistic Regression with oversampling \\n Confusion Matrix\", fontsize=14)\nax[0, 0].set_xticklabels(['', ''], fontsize=14, rotation=90)\nax[0, 0].set_yticklabels(['', ''], fontsize=14, rotation=360)\n\n\nactual_cm_up = confusion_matrix(original_ytest,original_ytest)\n\nsns.heatmap(actual_cm_up, ax=ax[0][2], annot=True, cmap=plt.cm.copper)\nax[0][2].set_title(\"100% Accuracy Confusion Matrix for oversampling\", fontsize=14)\nax[0][2].set_xticklabels(['', ''], fontsize=20, rotation=90)\nax[0][2].set_yticklabels(['', ''], fontsize=20, rotation=360)\n\nplt.show()\n###\nd = {'Technique': ['Random UnderSampling', 'Oversampling (SMOTE)'], 'Score': [undersample_score, oversample_score]}\nfinal_df = pd.DataFrame(data=d)\n\n# Move column\nscore = final_df['Score']\nfinal_df.drop('Score', axis=1, inplace=True)\nfinal_df.insert(1, 'Score', score)\n\n# Note how high is accuracy score it can be misleading! \nfinal_df","00cae10b":"#%% Neural Networks with undersampling and oversampling\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.layers import Dropout\n\nn_inputs = X_train.shape[1]\n\n# undersampling\nundersample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])\n\nundersample_model.summary()\nundersample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nundersample_model.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=2)\nundersample_predictions = undersample_model.predict(original_Xtest, batch_size=200, verbose=0)\nundersample_fraud_predictions = undersample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)\n\nimport itertools\n\n# Create a confusion matrix\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=14)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\nundersample_cm = confusion_matrix(original_ytest, undersample_fraud_predictions)\nactual_cm = confusion_matrix(original_ytest, original_ytest)\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(undersample_cm, labels, title=\"Random UnderSampling \\n Confusion Matrix\", cmap=plt.cm.Reds)\n\nfig.add_subplot(222)\nplot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)\n\n\nprint('Neural Networks with undersampling:')\nprint(classification_report(original_ytest, undersample_fraud_predictions))\n\n# Oversampling\nn_inputs = Xsm_train.shape[1]\n\noversample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])\noversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\noversample_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=2)\noversample_predictions = oversample_model.predict(original_Xtest, batch_size=200, verbose=0)\noversample_fraud_predictions = oversample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)\n\noversample_smote = confusion_matrix(original_ytest, oversample_fraud_predictions)\nactual_cm = confusion_matrix(original_ytest, original_ytest)\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(oversample_smote, labels, title=\"OverSampling (SMOTE) \\n Confusion Matrix\", cmap=plt.cm.Oranges)\n\nfig.add_subplot(222)\nplot_confusion_matrix(actual_cm, labels, title=\"Confusion Matrix \\n (with 100% accuracy)\", cmap=plt.cm.Greens)    \n\nprint('Neural Networks with SMOTE:')\nprint(classification_report(original_ytest, oversample_fraud_predictions))","d26700ba":"Inspired by this notebook:\n[https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets]","74fcd4a7":"I have examined the following classifiers to detect the fraud transactions and instead of Accuracy, I have used other metrics such as confusion matrix and roc auc score.\nLogisiticRegression\nKNN\nSupport Vector Machine\nRandom Forest Classifier\nXGBoost Classifier\nNeural Networks\n\nWorking on Anomaly Detection Algorithms for the next versions!"}}