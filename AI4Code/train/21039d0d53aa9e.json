{"cell_type":{"1de3d401":"code","5660aa3e":"code","44a1076e":"code","067623d6":"code","78958efe":"code","edc676c6":"code","2a8aa456":"code","17d8ab62":"code","227bd2e2":"code","5e5c8ac6":"code","90986b04":"code","bbb9d6c2":"code","ed6c8539":"code","19c4742b":"code","f6c9987f":"code","74a01785":"code","00c8978d":"code","c219708b":"code","fc880405":"code","95551f4b":"code","bbe9ed1a":"code","9cbdad10":"code","267997db":"code","df5a7688":"code","11dea227":"code","e98d5920":"code","ee2c102e":"code","1fab589d":"code","61aaadd8":"code","78e45f33":"code","80c74415":"code","27c42363":"code","b8d059e1":"code","72d6e910":"code","e2490320":"code","e72949e2":"code","b5ed2047":"code","0ba5e321":"markdown","d11a60d9":"markdown","25b4f522":"markdown","ecd559ce":"markdown","24099868":"markdown","87b0bf14":"markdown","e3087dfd":"markdown","1f1ea5fc":"markdown","304a33d1":"markdown","21151673":"markdown","1199b6e9":"markdown","d53be0e2":"markdown"},"source":{"1de3d401":"import matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom io import BytesIO\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport requests\nimport random\nimport glob\nimport cv2\nimport os\n\nimport math\nimport scipy\nimport tensorflow as tf \nfrom sklearn import metrics\n\nfrom tensorflow.keras import applications\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import models, layers, optimizers\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\n\n# for Transfer Learning\nfrom tensorflow.keras.applications import Xception\nfrom tensorflow.keras.applications import InceptionV3\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.applications import VGG19\nfrom tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.applications import MobileNet\nfrom tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.applications import EfficientNetB7\n\n%matplotlib inline","5660aa3e":"train_data_path = '..\/input\/10-monkey-species\/training\/training'\nvalidation_data_path = '..\/input\/10-monkey-species\/validation\/validation'\nmonkey_species = os.listdir(train_data_path)\nimg_width, img_height = 224, 224\nbatch_size = 4\n\nprint(\"Number of Categories:\", len(monkey_species))\nprint(\"Categories: \", monkey_species)","44a1076e":"train_datagen = ImageDataGenerator(\n    rotation_range = 30,\n    rescale=1. \/ 255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_path,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')\n\nvalidation_generator = test_datagen.flow_from_directory(\n    validation_data_path,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical')","067623d6":"def history_plot(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(acc) + 1)\n\n    plt.title('Training and validation accuracy')\n    plt.plot(epochs, acc, 'red', label='Training acc')\n    plt.plot(epochs, val_acc, 'blue', label='Validation acc')\n    plt.legend()\n\n    plt.figure()\n    plt.title('Training and validation loss')\n    plt.plot(epochs, loss, 'red', label='Training loss')\n    plt.plot(epochs, val_loss, 'blue', label='Validation loss')\n\n    plt.legend()\n    plt.show()","78958efe":"test_images = [\n    \"https:\/\/projectzerofootprint.com\/wp-content\/uploads\/2016\/08\/monkey-2-1080x768.jpg\",\n    \"https:\/\/i.ytimg.com\/vi\/Ptisy32iRRA\/hqdefault.jpg\",\n    \"https:\/\/images.pond5.com\/red-uakari-monkey-footage-064800523_iconl.jpeg\",\n    \"https:\/\/thejapanalps.com\/wp-content\/uploads\/2020\/03\/nihonsaru01.jpg\",\n    \"https:\/\/www.zoo-leipzig.de\/fileadmin\/_processed_\/e\/c\/csm_Weissbauch-Zwergseidenaeffchen_3_c46c37b6a1.jpg\",\n    \"https:\/\/cdn.britannica.com\/05\/181805-050-C9682415\/capuchin-monkey.jpg\",\n    \"https:\/\/www.neprimateconservancy.org\/uploads\/1\/5\/3\/8\/15380094\/silvery-marmoset-istock-153473655-resize_45.jpg\",\n    \"https:\/\/study.com\/cimages\/multimages\/16\/squirrel_monkeys.png\",\n    \"https:\/\/ars.els-cdn.com\/content\/image\/3-s2.0-B9780124095274000171-f17-04-9780124095274.jpg\",\n    \"https:\/\/media-cdn.tripadvisor.com\/media\/photo-s\/0a\/67\/93\/f5\/nilgiri-langur-karunkorangu.jpg\"\n]\n\ntest_labels = [\"n0\", \"n1\", \"n2\", \"n3\", \"n4\", \"n5\", \"n6\", \"n7\", \"n8\", \"n9\"]","edc676c6":"monkey_speciets_type = [\"Mantled Howler\",\"Patas Monkey\",\"Bald Uakari\",\n                        \"Japanese Macaque\",\"Pygmy Marmoset\",\"White Headed Capuchin\",\n                        \"Silvery Marmoset\",\"Ommon Squirrel Monkey\",\n                        \"Black Headed Night Monkey\",\"Nilgiri Langur\"]","2a8aa456":"Xception_base = Xception(weights='imagenet', include_top=False)\n\nx = Xception_base.output\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(512, activation='relu')(x)\npredictions = layers.Dense(int(len(train_generator.class_indices.keys())), activation='softmax')(x)\nXception_transfer = models.Model(inputs=Xception_base.input, outputs=predictions)","17d8ab62":"Xception_transfer.compile(loss='categorical_crossentropy',optimizer=optimizers.SGD(lr=1e-4, momentum=0.9), metrics=['accuracy'])\nhistory = Xception_transfer.fit_generator(train_generator,epochs=10,shuffle = True, verbose = 1, validation_data = validation_generator)","227bd2e2":"history_plot(history)","5e5c8ac6":"for (i,label) in enumerate(test_labels):\n    # img = load_img(img_name,target_size=(224,224))\n    response = requests.get(test_images[i])\n    img = Image.open(BytesIO(response.content))\n    img = np.asarray(img)\/255.\n    img = cv2.resize(img, (224,224))\n    prediction=Xception_transfer.predict(img.reshape(1, 224,224,3))\n    output = np.argmax(prediction)\n        \n    plt.title(\"Real: {} \\n Predict: {}\".format(monkey_speciets_type[i], monkey_speciets_type[output]))\n    plt.imshow(img)\n    plt.show()","90986b04":"model_base = InceptionV3(weights='imagenet',include_top=False)\n\nx = model_base.output\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(512,activation='relu')(x)\npredictions = layers.Dense(int(len(train_generator.class_indices.keys())) ,activation='softmax')(x)\nInceptionV3_model = models.Model(inputs= model_base.input, outputs=predictions)","bbb9d6c2":"InceptionV3_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9) ,metrics=['accuracy'])\nhistory = InceptionV3_model.fit_generator(train_generator, epochs=10, shuffle=True, verbose=1, validation_data=validation_generator)","ed6c8539":"history_plot(history)","19c4742b":"for (i,label) in enumerate(test_labels):\n    # img = load_img(img_name,target_size=(224,224))\n    response = requests.get(test_images[i])\n    img = Image.open(BytesIO(response.content))\n    img = np.asarray(img)\/255.\n    img = cv2.resize(img, (224,224))\n    prediction=InceptionV3_model.predict(img.reshape(1, 224,224,3))\n    output = np.argmax(prediction)\n        \n    plt.title(\"Real: {} \\n Predict: {}\".format(monkey_speciets_type[i], monkey_speciets_type[output]))\n    plt.imshow(img)\n    plt.show()","f6c9987f":"model_base = VGG16(weights='imagenet',include_top=False)\n\nx = model_base.output\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(512,activation='relu')(x)\npredictions = layers.Dense(int(len(train_generator.class_indices.keys())) ,activation='softmax')(x)\nVgg16_model = models.Model(inputs= model_base.input, outputs=predictions)","74a01785":"Vgg16_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9) ,metrics=['accuracy'])\nhistory = Vgg16_model.fit_generator(train_generator, epochs=10, shuffle=True, verbose=1, validation_data=validation_generator)","00c8978d":"history_plot(history)","c219708b":"for (i,label) in enumerate(test_labels):\n    # img = load_img(img_name,target_size=(224,224))\n    response = requests.get(test_images[i])\n    img = Image.open(BytesIO(response.content))\n    img = np.asarray(img)\/255.\n    img = cv2.resize(img, (224,224))\n    prediction=Vgg16_model.predict(img.reshape(1, 224,224,3))\n    output = np.argmax(prediction)\n        \n    plt.title(\"Real: {} \\n Predict: {}\".format(monkey_speciets_type[i], monkey_speciets_type[output]))\n    plt.imshow(img)\n    plt.show()","fc880405":"model_base = VGG19(weights='imagenet',include_top=False)\n\nx = model_base.output\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(512,activation='relu')(x)\npredictions = layers.Dense(int(len(train_generator.class_indices.keys())) ,activation='softmax')(x)\nVgg19_model = models.Model(inputs= model_base.input, outputs=predictions)","95551f4b":"Vgg19_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9) ,metrics=['accuracy'])\nhistory = Vgg19_model.fit_generator(train_generator, epochs=10, shuffle=True, verbose=1, validation_data=validation_generator)","bbe9ed1a":"history_plot(history)","9cbdad10":"for (i,label) in enumerate(test_labels):\n    # img = load_img(img_name,target_size=(224,224))\n    response = requests.get(test_images[i])\n    img = Image.open(BytesIO(response.content))\n    img = np.asarray(img)\/255.\n    img = cv2.resize(img, (224,224))\n    prediction=Vgg19_model.predict(img.reshape(1, 224,224,3))\n    output = np.argmax(prediction)\n        \n    plt.title(\"Real: {} \\n Predict: {}\".format(monkey_speciets_type[i], monkey_speciets_type[output]))\n    plt.imshow(img)\n    plt.show()","267997db":"model_base = ResNet50(weights='imagenet',include_top=False)\n\nx = model_base.output\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(512,activation='relu')(x)\npredictions = layers.Dense(int(len(train_generator.class_indices.keys())) ,activation='softmax')(x)\nResnet50_model = models.Model(inputs= model_base.input, outputs=predictions)","df5a7688":"Resnet50_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9) ,metrics=['accuracy'])\nhistory = Resnet50_model.fit_generator(train_generator, epochs=10, shuffle=True, verbose=1, validation_data=validation_generator)","11dea227":"history_plot(history)","e98d5920":"for (i,label) in enumerate(test_labels):\n    # img = load_img(img_name,target_size=(224,224))\n    response = requests.get(test_images[i])\n    img = Image.open(BytesIO(response.content))\n    img = np.asarray(img)\/255.\n    img = cv2.resize(img, (224,224))\n    prediction=Resnet50_model.predict(img.reshape(1, 224,224,3))\n    output = np.argmax(prediction)\n        \n    plt.title(\"Real: {} \\n Predict: {}\".format(monkey_speciets_type[i], monkey_speciets_type[output]))\n    plt.imshow(img)\n    plt.show()","ee2c102e":"model_base = MobileNet(weights='imagenet',include_top=False)\n\nx = model_base.output\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(512,activation='relu')(x)\npredictions = layers.Dense(int(len(train_generator.class_indices.keys())) ,activation='softmax')(x)\nMobileNet_model = models.Model(inputs= model_base.input, outputs=predictions)","1fab589d":"MobileNet_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9) ,metrics=['accuracy'])\nhistory = MobileNet_model.fit_generator(train_generator, epochs=10, shuffle=True, verbose=1, validation_data=validation_generator)","61aaadd8":"for (i,label) in enumerate(test_labels):\n    # img = load_img(img_name,target_size=(224,224))\n    response = requests.get(test_images[i])\n    img = Image.open(BytesIO(response.content))\n    img = np.asarray(img)\/255.\n    img = cv2.resize(img, (224,224))\n    prediction=MobileNet_model.predict(img.reshape(1, 224,224,3))\n    output = np.argmax(prediction)\n        \n    plt.title(\"Real: {} \\n Predict: {}\".format(monkey_speciets_type[i], monkey_speciets_type[output]))\n    plt.imshow(img)\n    plt.show()","78e45f33":"model_base = DenseNet121(weights='imagenet',include_top=False)\n\nx = model_base.output\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(512,activation='relu')(x)\npredictions = layers.Dense(int(len(train_generator.class_indices.keys())) ,activation='softmax')(x)\nDenseNet121_model = models.Model(inputs= model_base.input, outputs=predictions)","80c74415":"DenseNet121_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9) ,metrics=['accuracy'])\nhistory = DenseNet121_model.fit_generator(train_generator, epochs=10, shuffle=True, verbose=1, validation_data=validation_generator)","27c42363":"history_plot(history)","b8d059e1":"for (i,label) in enumerate(test_labels):\n    # img = load_img(img_name,target_size=(224,224))\n    response = requests.get(test_images[i])\n    img = Image.open(BytesIO(response.content))\n    img = np.asarray(img)\/255.\n    img = cv2.resize(img, (224,224))\n    prediction=DenseNet121_model.predict(img.reshape(1, 224,224,3))\n    output = np.argmax(prediction)\n        \n    plt.title(\"Real: {} \\n Predict: {}\".format(monkey_speciets_type[i], monkey_speciets_type[output]))\n    plt.imshow(img)\n    plt.show()","72d6e910":"model_base = EfficientNetB7(weights='imagenet',include_top=False)\n\nx = model_base.output\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dense(512,activation='relu')(x)\npredictions = layers.Dense(int(len(train_generator.class_indices.keys())) ,activation='softmax')(x)\nEfficientNetB7_model = models.Model(inputs= model_base.input, outputs=predictions)","e2490320":"EfficientNetB7_model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=1e-4, momentum=0.9) ,metrics=['accuracy'])\nhistory = EfficientNetB7_model.fit_generator(train_generator, epochs=10, shuffle=True, verbose=1, validation_data=validation_generator)","e72949e2":"history_plot(history)","b5ed2047":"for (i,label) in enumerate(test_labels):\n    # img = load_img(img_name,target_size=(224,224))\n    response = requests.get(test_images[i])\n    img = Image.open(BytesIO(response.content))\n    img = np.asarray(img)\/255.\n    img = cv2.resize(img, (224,224))\n    prediction=EfficientNetB7_model.predict(img.reshape(1, 224,224,3))\n    output = np.argmax(prediction)\n        \n    plt.title(\"Real: {} \\n Predict: {}\".format(monkey_speciets_type[i], monkey_speciets_type[output]))\n    plt.imshow(img)\n    plt.show()","0ba5e321":"<a id=\"MobileNet\"><\/a>\n<h1 style=\"background:#F38C8F; text-align:center; color: #2D6073;height:75px;line-height: 75px\">MobileNet<h1>","d11a60d9":"<h1 style=\"background:#F38C8F; text-align:center; color: #2D6073;height:75px;line-height: 75px\">Data Prepare<h1>","25b4f522":"<a id=\"EfficientNet\"><\/a>\n<h1 style=\"background:#F38C8F; text-align:center; color: #2D6073;height:75px;line-height: 75px\">EfficientNet<h1>","ecd559ce":"<h1 style=\"background:#F38C8F; text-align:center; color: #2D6073;height:75px;line-height: 75px\">Import the Necessary Packages<h1>","24099868":"<a id=\"InceptionV3\"><\/a>\n<h1 style=\"background:#F38C8F; text-align:center; color: #2D6073;height:75px;line-height: 75px\">InceptionV3<h1>","87b0bf14":"<a id=\"xception\"><\/a>\n<h1 style=\"background:#F38C8F; text-align:center; color: #2D6073;height:75px;line-height: 75px\">Xception<h1>","e3087dfd":"<img src=\"https:\/\/greenlogic.com.au\/wp-content\/uploads\/2018\/01\/0003_transfer-learning.jpg\" width=\"100%\"><\/img>\n\n<p style=\"color:#61193F;font-size:15px\">\nTransfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem.\nTransfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\nThe most common incarnation of transfer learning in the context of deep learning is the following worfklow:\n<br><br>\n<b>1-<\/b> Take layers from a previously trained model.<br>\n<b>2-<\/b> Freeze them, so as to avoid destroying any of the information they contain during future training rounds.<br>\n<b>3-<\/b> Add some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.<br>\n<b>4-<\/b> Train the new layers on your dataset.<br><br>\nA last, optional step, is fine-tuning, which consists of unfreezing the entire model you obtained above (or part of it), and re-training it on the new data with a very low learning rate. This can potentially achieve meaningful improvements, by incrementally adapting the pretrained features to the new data.\n<\/p>\n\n- [Xception](#xception)\n- [InceptionV3](#InceptionV3)\n- [VGG-16](#VGG_16)\n- [VGG-19](#VGG_19)\n- [ResNet50](#ResNet50)\n- [MobileNet](#MobileNet)\n- [DenseNet](#DenseNet)\n- [EfficientNet](#EfficientNet)","1f1ea5fc":"<a id=\"VGG_19\"><\/a>\n<h1 style=\"background:#F38C8F; text-align:center; color: #2D6073;height:75px;line-height: 75px\">VGG-19<h1>","304a33d1":"<div style=\"border: 2px solid #262626\">\n    <ul>\n        <li><b>n0: mantled_howler<\/b><\/li>\n        <li><b>n1: patas_monkey<\/b><\/li>\n        <li><b>n2: bald_uakari<\/b><\/li>\n        <li><b>n3: japanese_macaque<\/b><\/li>\n        <li><b>n4: pygmy_marmoset<\/b><\/li>\n        <li><b>n5: white_headed_capuchin<\/b><\/li>\n        <li><b>n6: silvery_marmoset<\/b><\/li>\n        <li><b>n7: common_squirrel_monkey<\/b><\/li>\n        <li><b>n8: black_headed_night_monkey<\/b><\/li>\n        <li><b>n9: nilgiri_langur<\/b><\/li>\n    <\/ul>\n<\/div>","21151673":"<a id=\"DenseNet\"><\/a>\n<h1 style=\"background:#F38C8F; text-align:center; color: #2D6073;height:75px;line-height: 75px\">DenseNet<h1>","1199b6e9":"<a id=\"ResNet50\"><\/a>\n<h1 style=\"background:#F38C8F; text-align:center; color: #2D6073;height:75px;line-height: 75px\">ResNet50<h1>","d53be0e2":"<a id=\"VGG_16\"><\/a>\n<h1 style=\"background:#F38C8F; text-align:center; color: #2D6073;height:75px;line-height: 75px\">VGG-16<h1>"}}