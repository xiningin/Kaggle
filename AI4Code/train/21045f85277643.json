{"cell_type":{"5b45b9bd":"code","2afaa3dd":"code","a2d7972f":"code","9ba00e2d":"code","a0d3fc33":"code","59d03d3c":"code","f17c98cd":"code","fa2df3c9":"code","32be7022":"code","c7c569c7":"code","9ff7b028":"code","6b80cfff":"code","f9e44729":"code","4d3b6bf8":"code","9d2cf873":"code","32816f3e":"code","b6fc0357":"code","59317eaa":"code","17df5dff":"markdown","afe0435e":"markdown","224dbe74":"markdown","8693a7bf":"markdown","c1140c58":"markdown","ca8f924e":"markdown","7ef428d9":"markdown","f5bd8ba7":"markdown","dae0221d":"markdown"},"source":{"5b45b9bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2afaa3dd":"datasetpath = '\/kaggle\/input\/dont-overfit-ii\/'\n\ndf = pd.read_csv(os.path.join(datasetpath, 'train.csv'))\n\nprint(\"The shape of the dataset is {}.\".format(df.shape))","a2d7972f":"df.head()","9ba00e2d":"df.describe()","a0d3fc33":"print(df.isnull().sum())\nprint(\"sum of sum null columns:\", df.isnull().sum().sum())","59d03d3c":"df.nunique()","f17c98cd":"df.var().sort_values()","fa2df3c9":"from sklearn.model_selection import train_test_split\n\nX = df.drop(columns=['id', 'target'])\ny = df['target']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","32be7022":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nLogReg = LogisticRegression(solver='liblinear')\n\nLogReg.fit(X_train, y_train)\n\npred = LogReg.predict(X_val)\n\nprint(\"Validation AUCROC score: {:.5f}\".format(roc_auc_score(y_val, pred)))\nprint(\"\\nTrain AUCROC score: {:.5f}\".format(roc_auc_score(y_train, LogReg.predict(X_train))))","c7c569c7":"from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\nscore = cross_val_score(LogisticRegression(solver='liblinear'), X, y,\n                        scoring='roc_auc', cv=skf)\n\nprint('Logistic regression model using cv = 10 folds')\n#print('AUCROC scores: ', score)\nprint('AUCROC mean(std): {:.5f}({:.5f}) '.format(score.mean(), score.std()))","9ff7b028":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\n\nmodel = LogisticRegression(solver='liblinear')\nC=np.linspace(0.001,0.3, 1000)\npenalty = ['l1','l2'] \n\nparam_grid = dict(penalty = penalty, C = C)\n#print(param_grid)\n\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n\ngrid_search = GridSearchCV(model, param_grid, scoring=\"roc_auc\", cv=skf, n_jobs=-1, verbose=1, return_train_score=True)\n\ngrid_result = grid_search.fit(X, y)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n    \nresults_log = pd.DataFrame(grid_search.cv_results_)\nresults_log.head()","6b80cfff":"print(\"reviewing test and train score.\")\nresults_log[results_log[\"params\"]==grid_result.best_params_].loc[:,[\"mean_test_score\",\"std_test_score\",\"mean_train_score\" ,\"std_train_score\"]].head()","f9e44729":"datasetpath = '\/kaggle\/input\/dont-overfit-ii\/'\n\ndf_test = pd.read_csv(os.path.join(datasetpath, 'test.csv'))\n\nprint(\"The shape of the dataset is {}.\".format(df_test.shape))","4d3b6bf8":"df_test.head()","9d2cf873":"df_test.describe()","32816f3e":"print(df_test.isnull().sum())\nprint(\"sum of sum null columns:\", df_test.isnull().sum().sum())","b6fc0357":"X_test = df_test.drop(columns='id')\n\ny_test_predicted = grid_search.predict(X_test)\n\ndf_test['target'] = y_test_predicted\n\ndf_test.head()","59317eaa":"df_test[['id', 'target']].to_csv('submission.csv', index=False)","17df5dff":"Reviewing test and train score of GridSearchCV for best parameters.","afe0435e":"### Show some statistics about the data","224dbe74":"### Training of Logistic Model\nusing logistic regression for less complexity as other models tends to be more complex. As to overcome overfitting we need to use less complex models. also, I have checked along side other classifier models and best result was logistic regression.<br>\n**Note**: solver = \u2018liblinear\u2019 as: For small datasets, \u2018liblinear\u2019 is a good choice, whereas \u2018sag\u2019 and \u2018saga\u2019 are faster for large ones.<br>\n[logisticRegressionDocumentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)\n\n1. splitting data using only 80% for training and 20% for validation.","8693a7bf":"3. Tuning logistic regression model using **GridSearchCV** across different values of penalty and C where ``penalty = ['l1', 'l2'] ,and C = np.linspace(0.001,0.3, 1000)``","c1140c58":"### Predicting 'target' in test data","ca8f924e":"2. spliting dataset using **StratifiedKFold** of ``10 splits`` for more accurate score. ","7ef428d9":"Check for **Nans** in test dataset","f5bd8ba7":"### Read test dataset","dae0221d":"### Read train dataset"}}