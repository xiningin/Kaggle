{"cell_type":{"196fc4f3":"code","4ccc26de":"code","937955d5":"code","ffe7bd3f":"code","fefb3e64":"code","1a216891":"code","0f1d69e5":"code","58c3e31e":"code","3cf2adcb":"code","d21600dc":"code","1649ea1d":"code","52c1d146":"code","a767631e":"code","ed4936bb":"code","c90a403a":"code","1c51ba55":"code","76b4c15a":"code","c2538350":"code","7ddf6683":"code","b6317166":"code","8a5c5168":"code","96ecccc6":"code","ea3c2765":"code","3d8c30a1":"code","f74146a6":"code","a73b3bc1":"code","eef3c51c":"code","32465bb4":"code","99ec003b":"code","2fb94892":"code","1fb7295f":"code","c15ea142":"code","f0487e5f":"code","bd468dd9":"code","4118415d":"code","c4dfca8b":"code","122136bf":"code","2db146eb":"code","1f30a0ac":"code","3af2082a":"code","e08b9903":"code","0b434675":"code","462be191":"code","bbb115e1":"code","8ef1d26b":"code","376f62c2":"code","9c608252":"code","3fc69e22":"markdown","17c2a963":"markdown","169bf0bf":"markdown","38ecd36c":"markdown","b67d6c03":"markdown","832e7c93":"markdown","c09bb364":"markdown","6f30940c":"markdown","6f5c9a84":"markdown","e9b6b6d9":"markdown","619b8a2f":"markdown","dd9b0f54":"markdown","45c410a5":"markdown","22709734":"markdown","d38e27c1":"markdown","5dc813a3":"markdown","4f6d0751":"markdown","fbc7c38e":"markdown","4b170909":"markdown","ff3e1c64":"markdown","af8a0786":"markdown","5776a5b3":"markdown","0a0c86e4":"markdown","493cd03c":"markdown","00bf0ee2":"markdown","0efc3770":"markdown","f7bfcbeb":"markdown","ba3497e6":"markdown","74a83cea":"markdown","d1af98eb":"markdown","ae9c82ea":"markdown","b224d7c2":"markdown","93c02640":"markdown","1aa44d9e":"markdown","f07a14ca":"markdown","5f6807c8":"markdown"},"source":{"196fc4f3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set()\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \npd.set_option('display.max_colwidth', -1) #to max the column width","4ccc26de":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.head(4)","937955d5":"train.columns","ffe7bd3f":"train['SalePrice'].describe()","fefb3e64":"plt.figure(figsize=(16, 10))\nplt.title('Sales Price Distribution')\nsns.distplot(train['SalePrice']);","1a216891":"target = np.log1p(train.SalePrice)\n\nplt.figure(figsize=(16, 10))\nplt.title('Transformed Sales Price Distribution')\nsns.distplot(target);","0f1d69e5":"num_feats = train.select_dtypes(include=[np.number])\nprint(num_feats.shape)\nnum_feats.columns","58c3e31e":"cate_feats = train.select_dtypes(include=[np.object])\nprint(cate_feats.shape)\ncate_feats.columns","3cf2adcb":"num_corr = num_feats.corr()\nprint(num_corr['SalePrice'].sort_values(ascending = False),'\\n')","d21600dc":"k= 11 #all correlations that are 0.5 and above\ncols = num_corr.nlargest(k,'SalePrice')['SalePrice'].index\nprint(cols)\ndata = np.corrcoef(train[cols].values.T)\nplt.figure(figsize=(14, 7))\nplt.title('Correlation of Numerical Features')\nsns.heatmap(data, vmax=.8, linewidths=0.01,square=True,annot=True,cmap='viridis',\n            xticklabels=cols.values ,annot_kws = {'size':12},yticklabels = cols.values)","1649ea1d":"fig, ((ax1, ax2), (ax3, ax4), (ax5, ax6))= plt.subplots(nrows=3,ncols=2,figsize=(15, 10))\n\nsns.regplot(x='OverallQual', y='SalePrice', data=train, ax=ax1)\n\nsns.regplot(x='GrLivArea', y='SalePrice', data=train, ax=ax2)\n\nsns.regplot(x='GarageCars', y='SalePrice', data=train, ax=ax3)\n\nsns.regplot(x='TotalBsmtSF', y='SalePrice', data=train, ax=ax4)\n\nsns.regplot(x='FullBath', y='SalePrice', data=train, ax=ax5)\n\nsns.regplot(x='YearRemodAdd', y='SalePrice', data=train, ax=ax6)\n\nplt.show()\n\nsns.regplot(x='YearBuilt', y='SalePrice', data=train)","52c1d146":"cate_null = cate_feats.isnull().sum().sort_values(ascending=False)\nnull_percent = (cate_feats.isnull().sum()\/cate_feats.isnull().count() * 100).sort_values(ascending=False)\nmissing_data = pd.concat([cate_null, null_percent], axis=1, keys=['Total Null Values', 'Percentage'])\n\nmissing_data.head(20)","a767631e":"dropped = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']\ncate_feats.drop(labels=dropped, axis=1, inplace=True)\ncate_feats.shape","ed4936bb":"# Since Electrical has just one null value...\ncate_feats.Electrical.value_counts()","c90a403a":"train.GarageCond = cate_feats['GarageCond'].fillna(value='None', inplace=True)\ntrain.GarageQual = cate_feats['GarageQual'].fillna(value='None', inplace=True)\ntrain.GarageFinish = cate_feats['GarageFinish'].fillna(value='None', inplace=True)\ntrain.GarageType = cate_feats['GarageType'].fillna(value='None', inplace=True)\ntrain.BsmtFinType2 = cate_feats['BsmtFinType2'].fillna(value='None', inplace=True)\ntrain.BsmtExposure = cate_feats['BsmtExposure'].fillna(value='None', inplace=True)\ntrain.BsmtQual = cate_feats['BsmtQual'].fillna(value='None', inplace=True)\ntrain.BsmtFinType1 = cate_feats['BsmtFinType1'].fillna(value='None', inplace=True)\ntrain.BsmtCond = cate_feats['BsmtCond'].fillna(value='None', inplace=True)\ntrain.MasVnrType = cate_feats['MasVnrType'].fillna(value='None', inplace=True)\ntrain.Electrical = cate_feats['Electrical'].fillna(value='SBrkr', inplace=True)","1c51ba55":"unique_list=[]\nfor column in cate_feats.columns:\n    no_of_unique = len(cate_feats[column].unique())\n    unique_list.append(no_of_unique)\n    print(\"The '{a}' column has {b} unique values\".format(a=column, b=no_of_unique))\nprint(\"Total number of categories with unique values is {c}\".format(c=len(unique_list)))","76b4c15a":"plt.figure(figsize=(16, 10))\nplt.xticks(rotation=90)\nsns.boxplot(x='Neighborhood', y='SalePrice', data=train)","c2538350":"bbb = ['Id','OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearRemodAdd', 'YearBuilt', 'SalePrice']\nnum_feats = num_feats[bbb]\nnum_feats.head()","7ddf6683":"train_data = pd.concat([num_feats, cate_feats], axis=1)\ntrain_data.shape","b6317166":"fig, ((ax1, ax2)) = plt.subplots(nrows=2,ncols=1,figsize=(11, 7))\n\nsns.regplot(x='TotalBsmtSF', y='SalePrice', data=train_data, ax=ax1)\n\nsns.regplot(x='GrLivArea', y='SalePrice', data=train_data, ax=ax2)","8a5c5168":"train_data = train_data.drop(train_data[train_data['GrLivArea'] > 4500].index)\ntrain_data = train_data.drop(train_data[train_data['TotalBsmtSF'] > 6000].index)\ntrain_data.head()","96ecccc6":"cols = list(train_data.columns)\nprint(cols[8])\ncols.pop(8)\ncols[:10]","ea3c2765":"test_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', usecols=cols)[cols]\nprint(test_data.shape)\ntest_data.head()","3d8c30a1":"null_test = test_data.isnull().sum().sort_values(ascending=False)\npercentage = (test_data.isnull().sum()\/test_data.isnull().count() * 100).sort_values(ascending=False)\nmissing_data = pd.concat([null_test, percentage], axis=1, keys=['Total Null Values', 'Percentage'])\n\nmissing_data.head(20)","f74146a6":"#Replacing the higher missing values with \"None\" as specified by the documentation.\ntest_data.GarageCond.fillna(value='None', inplace=True)\ntest_data.GarageQual.fillna(value='None', inplace=True)\ntest_data.GarageFinish.fillna(value='None', inplace=True)\ntest_data.GarageType.fillna(value='None', inplace=True)\ntest_data.BsmtFinType2.fillna(value='None', inplace=True)\ntest_data.BsmtExposure.fillna(value='None', inplace=True)\ntest_data.BsmtQual.fillna(value='None', inplace=True)\ntest_data.BsmtFinType1.fillna(value='None', inplace=True)\ntest_data.BsmtCond.fillna(value='None', inplace=True)\ntest_data.MasVnrType.fillna(value='None', inplace=True)\n\n#Replacing the low missing values with the most common.\ntest_data.Exterior1st.fillna(value=test_data.Exterior1st.value_counts().idxmax(), inplace=True)\ntest_data.TotalBsmtSF.fillna(value=test_data.TotalBsmtSF.median(), inplace=True)\ntest_data.Exterior2nd.fillna(value=test_data.Exterior2nd.value_counts().idxmax(), inplace=True)\ntest_data.SaleType.fillna(value=test_data.SaleType.value_counts().idxmax(), inplace=True)\ntest_data.KitchenQual.fillna(value=test_data.KitchenQual.value_counts().idxmax(), inplace=True)\ntest_data.GarageCars.fillna(value=test_data.GarageCars.median(), inplace=True)\ntest_data.Utilities.fillna(value=test_data.Utilities.value_counts().idxmax(), inplace=True)\ntest_data.Functional.fillna(value=test_data.Functional.value_counts().idxmax(), inplace=True)\ntest_data.MSZoning.fillna(value=test_data.MSZoning.value_counts().idxmax(), inplace=True)","a73b3bc1":"a = train_data.select_dtypes(include=[np.object])\ncols_list = list(a.columns)\ntrain = train_data\ntest = test_data","eef3c51c":"from sklearn.preprocessing import LabelEncoder\ntrain[cols_list] = train[cols_list].apply(LabelEncoder().fit_transform)\ntest[cols_list] = test[cols_list].apply(LabelEncoder().fit_transform)\n\ntrain.head()","32465bb4":"test.head()","99ec003b":"from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin","2fb94892":"X = train.drop(['Id', 'SalePrice'], axis=1)\ny = np.log1p(train.SalePrice)\nz = test.drop(['Id'], axis=1)","1fb7295f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","c15ea142":"scaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n#scale the prediction data\npred_test = scaler.transform(z)","f0487e5f":"Linear_reg = LinearRegression()\nLinear_reg.fit(X_train, y_train)\nLinear_pred = Linear_reg.predict(X_test)\nLinear_mse = np.sqrt(mean_squared_error(y_test, Linear_pred))\nLinear_r2 = r2_score(y_test, Linear_pred)\n\nprint('The Linear model has a root mean squared error of: {}'.format(Linear_mse))\nprint('The Linear model has an r2 score of: {}'.format(Linear_r2))","bd468dd9":"kfolds = KFold(n_splits=10, shuffle=True, random_state=42)","4118415d":"ridge = Ridge()\nparam_ridge = {'alpha': [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]}\nridge_model = GridSearchCV(ridge, param_ridge, cv=kfolds, n_jobs=2)\nridge_model.fit(X_train, y_train)\nprint(ridge_model.best_params_)","c4dfca8b":"ridge_model = ridge_model.best_estimator_\nridge_model.fit(X_train, y_train)\nridge_pred = ridge_model.predict(X_test)\nridge_mse = np.sqrt(mean_squared_error(y_test, ridge_pred))\nridge_r2 = r2_score(y_test, ridge_pred)\n\nprint('The Ridge model has a root mean squared error of: {}'.format(ridge_mse))\nprint('The Ridge model has an r2 score of: {}'.format(ridge_r2))","122136bf":"lasso = Lasso(max_iter=1e7, random_state=42)\nparam_lasso = {'alpha': [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]}\nlasso_model = GridSearchCV(lasso, param_lasso, cv=kfolds, n_jobs=2)\nlasso_model.fit(X_train, y_train)\nprint(lasso_model.best_params_)","2db146eb":"lasso_model = lasso_model.best_estimator_\nlasso_model.fit(X_train, y_train)\nlasso_pred = lasso_model.predict(X_test)\nlasso_mse = np.sqrt(mean_squared_error(y_test, lasso_pred))\nlasso_r2 = r2_score(y_test, lasso_pred)\n\nprint('The Lasso model has a root mean squared error of: {}'.format(lasso_mse))\nprint('The Lasso model has an r2 score of: {}'.format(lasso_r2))","1f30a0ac":"svr_model = SVR(C= 20, epsilon= 0.001, gamma=0.0009)\nsvr_model.fit(X_train, y_train)\nsvr_pred = svr_model.predict(X_test)\nsvr_mse = np.sqrt(mean_squared_error(y_test, svr_pred))\nsvr_r2 = r2_score(y_test, svr_pred)\n\nprint('The SVR model has a root mean squared error of: {}'.format(svr_mse))\nprint('The SVR model has an r2 score of: {}'.format(svr_r2))","3af2082a":"elastic_net = ElasticNet(max_iter=1e7, random_state=42)\nparam_enet = {'alpha': [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007],\n              'l1_ratio': [0.8, 0.85, 0.9, 0.95, 0.99, 1]}\nenet_model = GridSearchCV(elastic_net, param_enet, cv=5, n_jobs=2)\nenet_model.fit(X_train, y_train)\nprint(enet_model.best_params_)","e08b9903":"enet_model = enet_model.best_estimator_\nenet_model.fit(X_train, y_train)\nenet_pred = enet_model.predict(X_test)\nenet_mse = np.sqrt(mean_squared_error(y_test, enet_pred))\nenet_r2 = r2_score(y_test, enet_pred)\n\nprint('The ElasticNet model has a root mean squared error of: {}'.format(enet_mse))\nprint('The ElasticNet model has an r2 score of: {}'.format(enet_r2))","0b434675":"gboost_model = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, \n                                         max_features='sqrt', min_samples_leaf=15, min_samples_split=10, \n                                         loss='huber', random_state =42)\ngboost_model.fit(X_train, y_train)\ngboost_pred = gboost_model.predict(X_test)\ngboost_mse = np.sqrt(mean_squared_error(y_test, gboost_pred))\ngboost_r2 = r2_score(y_test, gboost_pred)\n\nprint('The Gradient Boosting Regressor has a root mean squared error of: {}'.format(gboost_mse))\nprint('The Gradient Boosting Regressor has an r2 score of: {}'.format(gboost_r2))","462be191":"class StackModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    def fit(self, X, y):\n        self.models_ = [x for x in self.models]\n        \n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","bbb115e1":"stack_model = StackModels(models= (gboost_model, svr_model, enet_model, lasso_model))\n\nstack_model.fit(X_train, y_train)\nstack_pred = stack_model.predict(X_test)\nstack_mse = np.sqrt(mean_squared_error(y_test, stack_pred))\nstack_r2 = r2_score(y_test, stack_pred)\n\nprint('The Stacked Model has a root mean squared error of: {}'.format(stack_mse))\nprint('The Stacked Model has an r2 score of: {}'.format(stack_r2))","8ef1d26b":"target = stack_model.predict(pred_test)\npredictions = np.exp(target)\n\nsubmission = pd.DataFrame({'Id': test['Id'],\n                          'SalePrice': predictions})\n","376f62c2":"submission.to_csv('Submission_stack.csv', index=False)\nsubmission.head()","9c608252":"SVR?","3fc69e22":"**Categorical features**","17c2a963":"This is a regression problem, what we're trying to predict is the SalesPrice, so lets take a look at it!","169bf0bf":"## References \nThese were some of the notebooks that really helped me with mine, definitely check them out for a more detailed experience.\n\nhttps:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n\nhttps:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard","38ecd36c":"**ElasticNet**","b67d6c03":"From the decription, we know that if some features have \"nan\" values, it means that the said feature doesn't exist for the corresponding observation. Therfore, we can replace said \"nan\" values with \"None\".\nBut for the features with more than 10% missing values, we'll probably just drop them, they most likely wont impact our model in anyway.","832e7c93":"Lets see if all our categorical features have unique values","c09bb364":"First, lets take a look at how many null values are in our categorical features","6f30940c":"**Test Data**","6f5c9a84":"# Model Selection and Training","e9b6b6d9":"**Gradient Boosting Regressor**","619b8a2f":"# Feature Selection","dd9b0f54":"From the above heatmap, we can see that some other features are highly correlated with one another. \n\n1. **TotalBsmtSF** and **1stFlrSF**, \n2. **GarageArea** and **GarageCars**, \n3. **TotRmsAbvGrd** and **GrLivArea**","45c410a5":"**SCATTERPLOT**\n\nA scatterplot can help us better visualise the correlation between SalePrice and its most correlated features and help us identify outliers if any!","22709734":"**Train-test split**","d38e27c1":"**Correlation with Numerical features and SalesPrice**","5dc813a3":"**Import necessary libraries**","4f6d0751":"Looking at the categories, the most likely to have an effect on price would be the neighborhood in which they are situated\nNow, we'll visualize the correlation between **Neighborhood** and our **SalePrice**","fbc7c38e":"# Predictions and Submission","4b170909":"It would seem as though our features exhibit a linear relationship with the **SalePrice** and we can observe outliers in **GrLivArea** and **TotalBsmtSF** scatter plots, we'll deal with those later.","ff3e1c64":"Lets better visualize the correlation between the more highly correlated features and **SalePrice**","af8a0786":"Due to the high correlation observed between some features, we'll be using those that have a higher correlation with **SalePrice**. \nFor example, we'll use **GrLivArea** and not **TotRmsAbvGrd** since they're both highly correlated with each but the former is better correlated with **SalePrice**","5776a5b3":"Import our Data","0a0c86e4":"**SVR**","493cd03c":"**Encoding Categorical Variables**","00bf0ee2":"Lets examine our numerical features!","0efc3770":"# Stacking Models","f7bfcbeb":"After exploring our training data extensively, we can finally select the features deemed most important for our final training feature matrix.","ba3497e6":"From our distribution plot above, we can see that we have a peaked distribution that is not normal with positive skewness!\n\nTherefore, before performing regression, our data has to be transformed, a logrithimic transformation should suffice.","74a83cea":"**Ridge**","d1af98eb":"Now for the categorical features","ae9c82ea":"**Lasso**","b224d7c2":"Lets import our tools for training and predictions","93c02640":"We'll scale the data using the **RobustScaler** module or **StandardScaler** to improve predictions basically!","1aa44d9e":"**Removing Outliers**\n\nFrom our data exploration, we remember that the features  **GrLivArea** and **TotalBsmtSF** had outliers, lets sort those out!","f07a14ca":"**Linear Regression**","5f6807c8":"# Data Exploration"}}