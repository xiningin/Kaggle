{"cell_type":{"47cbe884":"code","09b35399":"code","4d60ef69":"code","32e8c176":"code","7cde0553":"code","b85b19cd":"code","43cb886f":"code","3341e5e7":"code","011deac8":"code","fe0ea91f":"code","f142095c":"code","c996c765":"code","1719c89c":"code","eeaed5af":"markdown","e36ce211":"markdown","ed65e8bd":"markdown","f021c37d":"markdown","1b01f40c":"markdown","97b2a7b9":"markdown","4f0ffa63":"markdown","1aa99c9a":"markdown","88ca11e4":"markdown","4afb9c3f":"markdown","39f094ae":"markdown","1d42027d":"markdown","7987b493":"markdown","2fc4a363":"markdown","d849c45f":"markdown","85752a76":"markdown","2c062ab9":"markdown","465bd312":"markdown"},"source":{"47cbe884":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom lwoku import RANDOM_STATE, N_JOBS, VERBOSE, get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search\n\nimport pickle","09b35399":"VERBOSE=1","4d60ef69":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","32e8c176":"knn_clf = KNeighborsClassifier(n_jobs=N_JOBS)","7cde0553":"parameters = {\n    'n_neighbors': range(1, 11, 1)\n}\nclf = GridSearchCV(knn_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","b85b19cd":"parameters = {\n    'weights': ['uniform', 'distance']\n}\nclf = GridSearchCV(knn_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","43cb886f":"parameters = {\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']\n}\nclf = GridSearchCV(knn_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","3341e5e7":"parameters = {\n    'leaf_size': range(20, 50, 5)\n}\nclf = GridSearchCV(knn_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","011deac8":"parameters = {\n    'p': range(1, 4)\n}\nclf = GridSearchCV(knn_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","fe0ea91f":"parameters = {\n    'n_neighbors': range(1, 11, 1),\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'leaf_size': range(20, 50, 5),\n    'p': range(1, 4)\n}\nclf = GridSearchCV(knn_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","f142095c":"parameters = {\n    'n_neighbors': range(1, 11, 1),\n    'weights': ['uniform', 'distance'],\n    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n    'p': range(1, 4)\n}\nclf = GridSearchCV(knn_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","c996c765":"with open('clf.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","1719c89c":"clf.best_estimator_","eeaed5af":"# n_neighbors\n##### : int, optional (default = 5)\n\nNumber of neighbors to use by default for :meth:`kneighbors` queries.","e36ce211":"All algorithms score the same.\nThe brute is the one that needs the least fit time, but biggest score time.","ed65e8bd":"Distance weight score more than uniform one.","f021c37d":"The leaf size doesn't affect the score.","1b01f40c":"# weights\n##### : str or callable, optional (default = 'uniform')\n\nweight function used in prediction.  Possible values:\n- 'uniform' : uniform weights.  All points in each neighborhood\nare weighted equally.\n- 'distance' : weight points by the inverse of their distance.\nin this case, closer neighbors of a query point will have a\ngreater influence than neighbors which are further away.\n- [callable] : a user-defined function which accepts an\narray of distances, and returns an array of the same shape\ncontaining the weights.","97b2a7b9":"# Exhaustive search","4f0ffa63":"# algorithm\n##### : {'auto', 'ball_tree', 'kd_tree', 'brute'}, optional\n\nAlgorithm used to compute the nearest neighbors:\n- 'ball_tree' will use :class:`BallTree`\n- 'kd_tree' will use :class:`KDTree`\n- 'brute' will use a brute-force search.\n- 'auto' will attempt to decide the most appropriate algorithm\n  based on the values passed to :meth:`fit` method.\nNote: fitting on sparse input will override the setting of\nthis parameter, using brute force.","1aa99c9a":"# leaf_size\n##### : int, optional (default = 30)\n\nLeaf size passed to BallTree or KDTree. This can affect the\nspeed of the construction and query, as well as the memory\nrequired to store the tree.  The optimal value depends on the\nnature of the problem.","88ca11e4":"## Export grid search results","4afb9c3f":"it seems that `leaf_size` has no effect on the score.\n\nA new grid search is performed without it.","39f094ae":"Manhattan distance is the value that scores more.","1d42027d":"# metric\n##### : string or callable, default 'minkowski'\n\nthe distance metric to use for the tree.  The default metric is\nminkowski, and with p=2 is equivalent to the standard Euclidean\nmetric. See the documentation of the DistanceMetric class for a\nlist of available metrics.\n\n**Note**: Not evaluated","7987b493":"# metric_params\n##### : dict, optional (default = None)\n\nAdditional keyword arguments for the metric function.\n\n**Note**: Not evaluated","2fc4a363":"# Search over parameters","d849c45f":"# p\n##### : integer, optional (default = 2)\n\nPower parameter for the Minkowski metric. When p = 1, this is\nequivalent to using manhattan_distance (l1), and euclidean_distance\n(l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.","85752a76":"# Prepare data","2c062ab9":"The best score is with 1 neighbor, and the score decreases with the number of neighbors.","465bd312":"# Introduction\n\nThe aim of this notebook is to optimize the Logistic Regression model.\n\nFirst, all [k-nearest neighbors classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) parameters are analysed separately.\n\nThen, a grid search is carried out.\nThis is a search through all the combinations of parameters,\nwhich optimize the internal score in the train set.\n\nThe results are collected at [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization)."}}