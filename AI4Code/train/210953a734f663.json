{"cell_type":{"831abee5":"code","7b464651":"code","ea7e3b71":"code","d4342673":"code","1f287235":"code","e6fe923c":"code","e1736c0d":"code","2e69ee5e":"code","e2d1cf95":"code","dedd7425":"code","cc8175c1":"code","99b12c8b":"code","968a8a40":"code","35208ce6":"code","71180aff":"code","8a4e2756":"code","fbe1e337":"code","cc9ec092":"code","53196491":"code","191882dc":"code","61ff5d19":"code","5da0f5b7":"code","8e0e8da6":"code","f5fa9f0d":"code","80f762ef":"code","7dadb3af":"code","04ec2f52":"code","70b89a43":"code","6ad87482":"code","d2ec70c7":"code","47907b92":"code","0f470aa3":"code","735fad7f":"code","89e69aa3":"code","ea4b6367":"code","d142998b":"code","a0011336":"code","cfe8e5a9":"code","c55a80f9":"code","2c27f516":"code","ea64582b":"code","b55377b0":"code","f84535b9":"code","fc70c088":"code","830215af":"markdown","895b76f7":"markdown","945d9a74":"markdown","cefb3ca1":"markdown","fc3c6955":"markdown","a693e750":"markdown","aa9b5e4e":"markdown","3747abbb":"markdown","e6af9cae":"markdown","b0fae06a":"markdown","39e49eac":"markdown","513b9f3d":"markdown","abec5f32":"markdown","c18406f8":"markdown","778a59e7":"markdown","520810c0":"markdown","370e92b6":"markdown","749983b9":"markdown","62e8675b":"markdown","f38e3bbf":"markdown"},"source":{"831abee5":"!pip install holidays","7b464651":"%%time\n#This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ea7e3b71":"%%time\n# Import LGBM Regressor Model...\n\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold, TimeSeriesSplit\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nimport holidays","d4342673":"%%time\n# I like to disable my Notebook Warnings.\nimport warnings\nwarnings.filterwarnings('ignore')","1f287235":"%%time\n# Configure notebook display settings to only use 2 decimal places, tables look nicer.\npd.options.display.float_format = '{:,.2f}'.format\npd.set_option('display.max_columns', 15) \npd.set_option('display.max_rows', 50)","e6fe923c":"%%time\n# Define some of the notebook parameters for future experiment replication.\nSEED   = 42","e1736c0d":"%%time\n# Define the datasets locations...\n\nTRN_PATH = '\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv'\nTST_PATH = '\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv'\nSUB_PATH = '\/kaggle\/input\/tabular-playground-series-jan-2022\/sample_submission.csv'","2e69ee5e":"%%time\n# Read the datasets and create dataframes...\n\ntrain_df = pd.read_csv(TRN_PATH)\ntest_df = pd.read_csv(TST_PATH)\nsubmission_df = pd.read_csv(SUB_PATH)","e2d1cf95":"%%time\n# Explore the size of the dataset loaded...\n\ntrain_df.info()","dedd7425":"%%time\n# Explore the first 5 rows to have an idea what we are dealing with...\n\ntrain_df.head()","cc8175c1":"%%time\n# Explore the size of the dataset loaded...\n\ntest_df.info()","99b12c8b":"%%time\n# Explore the first 5 rows to have an idea what we are dealing with, in this case the Test Set...\n\ntest_df.head()","968a8a40":"%%time\n# Review some statistical information for the numeric variables...\n\ntrain_df.describe()","35208ce6":"%%time\n# Review some information for the categorical variables...\n\ncountry_list = train_df['country'].unique()\nstore_list = train_df['store'].unique()\nproduct_list = train_df['product'].unique()\n\nprint(f'Country List:{country_list}')\nprint(f'Store List:{store_list}')\nprint(f'Product List:{product_list}')","71180aff":"%%time\n# Review if there is missing information in the dataset...\n\ntrain_df.isnull().sum()","8a4e2756":"# Create a simple function to evaluate the time-ranges of the information provided.\n# It will help with the train \/ validation separations\n\ndef evaluate_time(df):\n    min_date = df['date'].min()\n    max_date = df['date'].max()\n    print(f'Min Date: {min_date} \/  Max Date: {max_date}')\n    return None\n\nevaluate_time(train_df)\nevaluate_time(test_df)","fbe1e337":"TARGET = 'num_sold'","cc9ec092":"# Country List:['Finland' 'Norway' 'Sweden']\nholiday_FI = holidays.CountryHoliday('FI', years=[2015, 2016, 2017, 2018, 2019])\nholiday_NO = holidays.CountryHoliday('NO', years=[2015, 2016, 2017, 2018, 2019])\nholiday_SE = holidays.CountryHoliday('SE', years=[2015, 2016, 2017, 2018, 2019])\n\nholiday_dict = holiday_FI.copy()\nholiday_dict.update(holiday_NO)\nholiday_dict.update(holiday_SE)\n\ntrain_df['date'] = pd.to_datetime(train_df['date']) # Convert the date to datetime.\ntrain_df['holiday_name'] = train_df['date'].map(holiday_dict)\ntrain_df['is_holiday'] = np.where(train_df['holiday_name'].notnull(), 1, 0)\ntrain_df['holiday_name'] = train_df['holiday_name'].fillna('Not Holiday')\n\ntest_df['date'] = pd.to_datetime(test_df['date']) # Convert the date to datetime.\ntest_df['holiday_name'] = test_df['date'].map(holiday_dict)\ntest_df['is_holiday'] = np.where(test_df['holiday_name'].notnull(), 1, 0)\ntest_df['holiday_name'] = test_df['holiday_name'].fillna('Not Holiday')","53196491":"train_df.sample(10)","191882dc":"def add_holydays(df):\n    \"\"\"\n    Flag the dataframe with a is_holyday field = 1 if the date is on the\n    dictionary of holydays loaded.\n    Args\n        df\n    Returs\n        df\n    \"\"\"\n    new_years_eve = ['12\/31\/2015','12\/31\/2016','12\/31\/2017','12\/31\/2018','12\/31\/2019']\n    chrismas_day = ['12\/24\/2015','12\/24\/2016','12\/24\/2017','12\/24\/2018','12\/24\/2019']","61ff5d19":"# Create some simple features base on the Date field...\n\ndef create_time_features(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Create features base on the date variable, the idea is to extract as much \n    information from the date componets.\n    Args\n        df: Input data to create the features.\n    Returns\n        df: A DataFrame with the new time base features.\n    \"\"\"\n    \n    df['date'] = pd.to_datetime(df['date']) # Convert the date to datetime.\n    \n    # Start the creating future process.\n    df['year'] = df['date'].dt.year\n    df['quarter'] = df['date'].dt.quarter\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['dayofweek'] = df['date'].dt.dayofweek\n    df['dayofmonth'] = df['date'].dt.days_in_month\n    df['dayofyear'] = df['date'].dt.dayofyear\n    df['weekofyear'] = df['date'].dt.weekofyear\n    df['weekday'] = df['date'].dt.weekday\n    df['is_weekend'] = np.where((df['weekday'] == 5) | (df['weekday'] == 6), 1, 0)\n    \n    return df","5da0f5b7":"# Apply the function 'create_time_features' to the dataset...\ntrain_df = create_time_features(train_df)\ntest_df = create_time_features(test_df)","8e0e8da6":"# Convert the Categorical variables to one-hoe encoded features...\n# It will help in the training process\n\nCATEGORICAL = ['country', 'store', 'product', 'holiday_name']\ndef create_one_hot(df, categ_colums = CATEGORICAL):\n    \"\"\"\n    Creates one_hot encoded fields for the specified categorical columns...\n    Args\n        df\n        categ_colums\n    Returns\n        df\n    \"\"\"\n    df = pd.get_dummies(df, columns=CATEGORICAL)\n    return df\n\n\ndef encode_categ_features(df, categ_colums = CATEGORICAL):\n    \"\"\"\n    Use the label encoder to encode categorical features...\n    Args\n        df\n        categ_colums\n    Returns\n        df\n    \"\"\"\n    le = LabelEncoder()\n    for col in categ_colums:\n        df['enc_'+col] = le.fit_transform(df[col])\n    return df\n\ntrain_df = encode_categ_features(train_df)\ntest_df = encode_categ_features(test_df)","f5fa9f0d":"def transform_target(df, taget = TARGET):\n    \"\"\"\n    Apply a log transformation to the target for better optimization \n    during training.\n    \"\"\"\n    df[TARGET] = np.log(df[TARGET])\n    return df\n\ntrain_df = transform_target(train_df, TARGET)","80f762ef":"train_df.head()","7dadb3af":"# Extract features and avoid certain columns from the dataframe for training purposes...\navoid = ['row_id', 'date', 'num_sold']\nFEATURES = [feat for feat in train_df.columns if feat not in avoid]\n\n# Print a list of all the features created...\nprint(FEATURES)","04ec2f52":"# Selecting Features....\nprint(FEATURES)","70b89a43":"FEATURES = [\n            #'country',\n            #'store',\n            #'product',\n            #'holiday_name',\n            #'is_holiday',\n            'year',\n            #'quarter',\n            'month',\n            'day',\n            'dayofweek',\n            #'dayofmonth',\n            #'dayofyear',\n            #'weekofyear',\n            #'weekday',\n            'is_weekend',\n            'enc_country',\n            'enc_store',\n            'enc_product',\n            #'enc_holiday_name'\n            ]","6ad87482":"# Creates the Train and Validation sets to train the model...\n# Define a cutoff date to split the datasets\nCUTOFF_DATE = '2018-01-01'\n\n# Split the data into train and validation datasets using timestamp best suited for timeseries...\nX_train = train_df[train_df['date'] < CUTOFF_DATE][FEATURES]\ny_train = train_df[train_df['date'] < CUTOFF_DATE][TARGET]\n\nX_val = train_df[train_df['date'] >= CUTOFF_DATE][FEATURES]\ny_val = train_df[train_df['date'] >= CUTOFF_DATE][TARGET]","d2ec70c7":"def SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","47907b92":"# Defines a really simple XGBoost Regressor...\n\nxgboost_params = {'eta'              : 0.1,\n                  'n_estimators'     : 16384,\n                  'max_depth'        : 8,\n                  'max_leaves'       : 256,\n                  'colsample_bylevel': 0.75,\n                  'colsample_bytree' : 0.75,\n                  'subsample'        : 0.75, # XGBoost would randomly sample 'subsample_value' of the training data prior to growing trees\n                  'min_child_weight' : 512,\n                  'min_split_loss'   : 0.002,\n                  'alpha'            : 0.08,\n                  'lambda'           : 128,\n                  'objective'        : 'reg:squarederror',\n                  'eval_metric'      : 'rmse', # Originally using RMSE, trying new functions...\n                  'tree_method'      : 'gpu_hist',\n                  'seed'             : SEED\n                  }\n\n# Create an instance of the XGBRegressor and set the model parameters...\nregressor = XGBRegressor(**xgboost_params)\n\n# Train the XGBRegressor using the train and validation datasets, \n# Utilizes early_stopping_rounds to control overfitting...\nregressor.fit(X_train,\n              y_train,\n              eval_set=[(X_val, y_val)],\n              early_stopping_rounds = 250,\n              verbose = 500)","0f470aa3":"val_pred = regressor.predict(X_val[FEATURES])\n# Convert the target back to non-logaritmic.\nval_pred = np.exp(val_pred)\ny_val = np.exp(y_val)\n\nscore = np.sqrt(mean_squared_error(y_val, val_pred))\nprint(f'RMSE: {score} \/ SMAPE: {SMAPE(y_val, val_pred)}')","735fad7f":"feats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(FEATURES, regressor.feature_importances_):\n    feats[feature] = importance #add the name\/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\nimportances.sort_values(by='Gini-importance', ascending=False).plot(kind='bar', rot=45, figsize=(10,5))","89e69aa3":"%%time\nN_SPLITS = 3\nEARLY_STOPPING_ROUNDS = 150 # Will stop training if one metric of one validation data doesn\u2019t improve in last round\nVERBOSE = 0 # Controls the level of information, verbosity","ea4b6367":"%%time\n# Define a Pipeline to process the data for the Model.\ntransformer = Pipeline(steps=[('scaler',StandardScaler()), ('min_max', MinMaxScaler(feature_range=(0, 1)))])\npreprocessor = ColumnTransformer(transformers=[('first', transformer, FEATURES)])       ","d142998b":"%%time\n# Cross Validation Loop for the Classifier.\ndef cross_validation_train(train, labels, test, model, model_params, n_folds = 5):\n    \"\"\"\n    The following function is responsable of training a model in a\n    cross validation loop and generate predictions on the specified test set.\n    The function provides the model feature importance list as other variables.\n\n    Args:\n    train  (Dataframe): ...\n    labels (Series): ...\n    test   (Dataframe): ...\n    model  (Model): ...\n    model_params (dict of str: int): ...\n\n    Return:\n    classifier  (Model): ...\n    feat_import (Dataframe): ...\n    test_pred   (Dataframe): ...\n    ...\n\n    \"\"\"\n    # Creates empty place holders for out of fold and test predictions.\n    oof_pred  = np.zeros(len(train)) # We are predicting prob. we need more dimensions.\n    oof_label = np.zeros(len(train))\n    test_pred = np.zeros(len(test)) # We are predicting prob. we need more dimensions\n    val_indexes_used = []\n    \n    # Creates empty place holder for the feature importance.\n    feat_import = np.zeros(len(FEATURES))\n    \n    # Creates Stratified Kfold object to be used in the train \/ validation\n    # phase of the model.\n    Kf = TimeSeriesSplit(n_splits = n_folds)\n    \n    # Start the training and validation loops.\n    for fold, (train_idx, val_idx) in enumerate(Kf.split(train)):\n        # Creates the index for each fold\n        print(f'Fold: {fold}')        \n        train_min_date = train_df.iloc[train_idx]['date'].min()\n        train_max_date = train_df.iloc[train_idx]['date'].max()\n        \n        valid_min_date = train_df.iloc[val_idx]['date'].min()\n        valid_max_date = train_df.iloc[val_idx]['date'].max()\n        \n        print(f'Train Min \/ Max Dates: {train_min_date} \/ {train_max_date}')\n        print(f'Valid Min \/ Max Dates: {valid_min_date} \/ {valid_max_date}')\n\n        print(f'Training on {train_df.iloc[train_idx].shape[0]} Records')\n        print(f'Validating on {train_df.iloc[val_idx].shape[0]} Records')\n        \n        # Generates the Fold. Train and Validation datasets\n        X_trn, y_trn = train.iloc[train_idx], labels.iloc[train_idx]\n        X_val, y_val = train.iloc[val_idx], labels.iloc[val_idx]\n        \n        val_indexes_used = np.concatenate((val_indexes_used, val_idx), axis=None)\n        \n        # Instanciate a classifier based on the model parameters\n        regressor = model(**model_params)\n \n        regressor.fit(X_trn, \n                      y_trn, \n                      eval_set = [(X_val, y_val)], \n                      early_stopping_rounds = EARLY_STOPPING_ROUNDS, \n                      verbose = VERBOSE)\n        \n        # Generate predictions using the trained model\n        val_pred = regressor.predict(X_val)\n        oof_pred[val_idx]  = val_pred # store the predictions for that fold.\n        oof_label[val_idx] = y_val # store the true labels for that fold.\n\n        # Calculate the model error based on the selected metric\n        error =  np.sqrt(mean_squared_error(y_val, val_pred))\n\n        # Print some of the model performance metrics\n        print(f'RMSE: {error}')\n        print(f'SMAPE: {SMAPE(y_val, val_pred)}')\n        print(\".\"*50)\n\n        # Populate the feature importance matrix\n        feat_import += regressor.feature_importances_\n\n        # Generate predictions for the test set\n        test_pred += (regressor.predict(test)) \/ n_folds\n                        \n    # Calculate the error across all the folds and print the reuslts\n    val_indexes_used = val_indexes_used.astype(int)\n    global_error = np.sqrt(mean_squared_error(labels.iloc[val_indexes_used], oof_pred[val_indexes_used]))\n    \n    print('')\n    print(f'RMSE: {global_error}...')\n    print(f'SMAPE: {SMAPE(labels.iloc[val_indexes_used], oof_pred[val_indexes_used])}...')\n    \n    return regressor, feat_import, test_pred, oof_label, oof_pred","a0011336":"%%time\n# Uses the cross_validation_train to build and train the model with XGBoost\nxgbr, feat_imp, predictions, oof_label, oof_pred = cross_validation_train(train  = train_df[FEATURES], \n                                                                          labels = train_df[TARGET], \n                                                                          test   = test_df[FEATURES], \n                                                                          model  = XGBRegressor, \n                                                                          model_params = xgboost_params,\n                                                                          n_folds = N_SPLITS\n                                                                          )","cfe8e5a9":"train_df.shape","c55a80f9":"oof_label.shape","2c27f516":"oof_pred.shape","ea64582b":"feats = {} # a dict to hold feature_name: feature_importance\nfor feature, importance in zip(FEATURES, xgbr.feature_importances_):\n    feats[feature] = importance #add the name\/value pair \n\nimportances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Gini-importance'})\nimportances.sort_values(by='Gini-importance', ascending=False).plot(kind='bar', rot=45, figsize=(12,5))","b55377b0":"# Use the created model to predict the sales for 2019...\npred = regressor.predict(test_df[FEATURES])\npred = np.exp(pred)\nsubmission_df['num_sold'] = pred\nsubmission_df.head(10)","f84535b9":"# Creates a submission file for Kaggle...\nsubmission_df.to_csv('submission.csv',index=False)","fc70c088":"# Use the created model to predict the sales for 2019...\npred = regressor.predict(test_df[FEATURES])\nsubmission_df['num_sold'] = predictions\nsubmission_df.head(10)","830215af":"---","895b76f7":"# Exploring the Dataframes, (Size, Stats, Nulls and Others) <a name=\"3\"><\/a>","945d9a74":"___","cefb3ca1":"# Model Inference (Submission to Kaggle) <a name=\"1\"><\/a>","fc3c6955":"___","a693e750":"# Train a Simple Model (XGBoost Regressor) <a name=\"7\"><\/a>","aa9b5e4e":"<a name=\"1\"><\/a>\n# Loading Python Libraries. ","3747abbb":"# Processing the Datasets for Training <a name=\"5\"><\/a>","e6af9cae":"---","b0fae06a":"# Results and Ideas...","39e49eac":"# \ud83c\udf81 TPS-JAN22, Quick EDA + XGBoost\nThe following model is a simple implementation using XGBoost. The objective is to provide a simple framework and foundation as a baseline for more sophisticated implementations.\nThe objective of this competition is the following.\n\n1. [Loading Python Libraries.](#1)\n2. [Loading CSV and Creating Dataframes.](#2)\n3. [Exploring the Dataframes, (Size, Stats, Nulls and Others).](#3)\n4. [Feature Engineering.](#4)\n5. [Processing the Datasets for Training.](#5)\n6. [Creates a Simple Train \/ Validation Strategy](#6)\n7. [Train a Simple Model (XGBoost Regressor)](#7)\n8. [Train a Simple Model (XGBoost Regressor) using a CV Loop](#8)\n9. [Model Inference (Submission to Kaggle)](#9)\n\n\n**Data Description** <\/br>\nFor this challenge, you will be predicting a full year worth of sales for three items at two stores located in three different countries. This dataset is completely fictional, but contains many effects you see in real-world data, e.g., weekend and holiday effect, seasonality, etc. The dataset is small enough to allow you to try numerous different modeling approaches.\n\nGood luck!\n\n\n\n**Objective** <\/br>\nUsing 2015 - 2018, predict the sales by date, country, store, and product for 2019.\n\n**Strategy** <\/br>\nBecause we are dealing with a time series type of estimation, we need to hide future information from the model; in this simple approach we will use as validation all the data from 2018, so we will train the model with data from 2015-2017\n\n**Update 12\/31\/2021**\n* Developed a simple Notebook, Quick EDA + Simple Feature Engineering.\n* Cross-Validation strategy based on a fixed date.\n\n**Update 01\/01\/2021**\n* Added Cross-Validation loop to the model.\n* Added new features, to identify weekends.\n* Added a proper table of contents.\n* Added features based on Holidays for each of the countries.\n\n**Update 01\/02\/2021**\n* Improved the CV training function to calculate the SMOTE properly.\n\n**Ideas that I want to implement**\n* New features based on trends.\n\n","513b9f3d":"# Creates a Simple Train \/ Validation Strategy <a name=\"6\"><\/a>","abec5f32":"---","c18406f8":"# Loading CSV and Creating Dataframes. <a name=\"2\"><\/a>","778a59e7":"___","520810c0":"### Model Results vs. Features Used in the Traininf and Validation...\n1. Plain features, nothing added to the model. Removed Id, Datetime and Target <\/br>\nRMSE: 141.17269369190075 \/ SMAPE: 17.040551866223385\n\n2. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear', 'weekofyear', 'weekday' <\/br>\nRMSE: 66.89475324109723 \/ SMAPE: 9.30006322183181\n\n3. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear', 'weekofyear', 'weekday', 'quarter' <\/br>\nRMSE: 67.4018691784641 \/ SMAPE: 9.343389593022566\n\n4. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear', 'weekofyear', 'weekday', 'quarter' <\/br>\nAdded new Features,'is_holiday'<\/br>\nRMSE: 66.59882566819414 \/ SMAPE: 9.477461518875648\n\n5. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear', 'weekofyear', 'weekday', 'quarter' <\/br>\nAdded new Features,'is_holiday', 'is_weekend'<\/br>\nRMSE: 66.27489712300181 \/ SMAPE: 9.370856195608114\n\n6. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear', 'weekofyear', 'weekday', 'quarter' <\/br>\nAdded new Features,'is_holiday', 'is_weekend','enc_holiday_name' <\/br>\nRMSE: 65.93668135230337 \/ SMAPE: 9.428644170683123\n\n7. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofmonth', 'dayofyear', 'weekofyear', 'weekday'<\/br>\nAdded new Features,'is_weekend' <\/br>\nRMSE: 66.73112188359103 \/ SMAPE: 9.29087254951728\n\n8. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'dayofyear', 'weekofyear', 'weekday'<\/br>\nAdded new Features,'is_weekend' <\/br>\nRMSE: 66.1329325693428 \/ SMAPE: 9.290678813131464\n\n9. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'weekofyear', 'weekday'<\/br>\nAdded new Features,'is_weekend' <\/br>\nRMSE: 66.13737123847237 \/ SMAPE: 9.256808780901792\n\n10. Added Datetime features,'year', 'month', 'day', 'dayofweek', 'weekday'<\/br>\nAdded new Features,'is_weekend' <\/br>\nRMSE: 65.40444050929132 \/ SMAPE: 9.045220024208168\n\n11. Added Datetime features,'year', 'month', 'day', 'dayofweek'<\/br>\nAdded new Features,'is_weekend' <\/br>\nRMSE: 65.20180075198031 \/ SMAPE: 9.049180607434174\n\n12. Added Datetime features,'year', 'month', 'day', 'dayofweek'<\/br>\nUsing RMSE and Log of the Target... <\/br>\nAdded new Features,'is_weekend' <\/br>\nRMSE: 63.544532908755954 \/ SMAPE: 8.460984766381136","370e92b6":"---","749983b9":"# Train a Simple Model (XGBoost Regressor) using a CV Loop. <a name=\"8\"><\/a>","62e8675b":"# Feature Engineering <a name=\"4\"><\/a>","f38e3bbf":"---"}}