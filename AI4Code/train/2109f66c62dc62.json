{"cell_type":{"4538f990":"code","8fddb37b":"code","ac4f6fa8":"code","c4007ad1":"code","eee77eeb":"code","2b464de2":"code","06c547f1":"code","bc43c508":"code","81f3c60f":"code","7170ef96":"code","a3466269":"code","46ea1414":"code","d80df095":"code","4c8d223e":"code","9d9a62f7":"code","10f7169e":"code","6cf0a50d":"code","a7e1075c":"code","2942ebb4":"code","ed65bd7a":"code","70924771":"code","f0331de5":"code","12932885":"code","bdfb9b47":"code","ad416f7c":"code","9ad19057":"code","00e818c2":"code","849101f0":"code","6c96eee9":"code","4d8ddf28":"markdown","3d244661":"markdown","800c73a6":"markdown","7c81fe14":"markdown"},"source":{"4538f990":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport itertools\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8fddb37b":"!pip install openpyxl","ac4f6fa8":"df = pd.read_excel('\/kaggle\/input\/progressive-rock-pop-songs-lyrics\/selected_songs.xlsx')\ndf.head(5)","c4007ad1":"df.isnull().sum()","eee77eeb":"#Codes by Pooja Jain https:\/\/www.kaggle.com\/jainpooja\/av-guided-hackathon-predict-youtube-likes\/notebook\n\ntext_cols = ['lyrics', 'Artist', 'Song', 'genre']\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nwc = WordCloud(stopwords = set(list(STOPWORDS) + ['|']), random_state = 42)\nfig, axes = plt.subplots(2, 2, figsize=(20, 12))\naxes = [ax for axes_row in axes for ax in axes_row]\n\nfor i, c in enumerate(text_cols):\n  op = wc.generate(str(df[c]))\n  _ = axes[i].imshow(op)\n  _ = axes[i].set_title(c.upper(), fontsize=24)\n  _ = axes[i].axis('off')\n\n#_ = fig.delaxes(axes[4])","2b464de2":"#Define a function to plot a bar plot easily\n\ndef bar_plot(df,x,x_title,y,title,colors=None,text=None):\n    fig = px.bar(x=x,\n                 y=y,\n                 text=text,\n                 labels={x: x_title.title()},          # replaces default labels by column name\n                 data_frame=df,\n                 color=colors,\n                 barmode='group',\n                 template=\"simple_white\",\n                 color_discrete_sequence=px.colors.qualitative.Prism)\n    \n    texts = [df[col].values for col in y]\n    for i, t in enumerate(texts):\n        fig.data[i].text = t\n        fig.data[i].textposition = 'inside'\n        \n    fig['layout'].title=title\n\n    for trace in fig.data:\n        trace.name = trace.name.replace('_',' ').title()\n\n    fig.update_yaxes(tickprefix=\"\", showgrid=True)\n\n    fig.show()","06c547f1":"# lets define a function to plot a histogram plot easily\n\ndef hist_plot(df,x,title):\n    fig = px.histogram(x=df[x],\n                       color_discrete_sequence=colors,\n                       opacity=0.8)\n\n    fig['layout'].title=title\n    fig.update_yaxes(tickprefix=\"\", showgrid=True)\n    fig.show()","bc43c508":"#Code by Alaa Sedeeq https:\/\/www.kaggle.com\/alaasedeeq\/commonlit-readability-eda\/comments#1292516\n\n#Find words spreading (each word frequency)\nfreq_d = pd.Series(' '.join(df['Song']).split()).value_counts()\n#Plot the words distribution\nfig = px.line(freq_d,\n              title='The word frequency visualization')\nfig.update_layout(showlegend=False) ","81f3c60f":"prepared_as_text = [line for line in df['Song']]\ntext_prepared_results = '\/n'.join(prepared_as_text)\n\ntext= ' '.join(t for t in df['Song'])\nwords_list= text.split()","7170ef96":"word_freq= {}\n\nfor word in set(words_list):\n    word_freq[word]= words_list.count(word)\n    \n#sorting the dictionary \nword_freq = dict(sorted(word_freq.items(), reverse=True, key=lambda item: item[1]))","a3466269":"#sort the data and put it in a data frame for the visualization\nword_freq_temp = dict(itertools.islice(word_freq.items(), 25))\nword_freq_df = pd.DataFrame(word_freq_temp.items(),columns=['word','count']).sort_values('count',ascending=False)","46ea1414":"#Code by Alaa Sedeeq https:\/\/www.kaggle.com\/alaasedeeq\/commonlit-readability-eda\/comments#1292516\n\nbar_plot(word_freq_df.reset_index(),\n         'word',\n         'Words',\n         ['count'],\n         title='Top 20 frequent words')","d80df095":"import nltk \nimport string\nfrom wordcloud import WordCloud\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob,Word\nfrom collections import Counter","4c8d223e":"#Code by Alaa Sedeeq https:\/\/www.kaggle.com\/alaasedeeq\/commonlit-readability-eda\/comments#1292516\n#Bigrams\nfrom nltk.util import ngrams    \n\ndef get_n_grans_count(text, n_grams, min_freq):\n    output = {}\n    tokens = nltk.word_tokenize(text)\n\n    #Create the n_gram\n    if n_grams == 2:\n        gs = nltk.bigrams(tokens)\n        \n    elif n_grams == 3:\n        gs = nltk.trigrams(tokens)\n\n    else:\n        return 'Only 2_grams and 3_grams are supported'\n    #compute frequency distribution for all the bigrams in the text\n    fdist = nltk.FreqDist(gs)\n    for k,v in fdist.items():\n        if v > min_freq:\n            index = ' '.join(k)\n            output[index] = v\n    \n    return output","9d9a62f7":"two_grams = get_n_grans_count(text, n_grams=2, min_freq=10)\ntwo_grams_df = pd.DataFrame(data=two_grams.items())\ntwo_grams_df = two_grams_df.sort_values(by=1,ascending=False).rename(columns={0:'Two grams',1:'Count'})\ntwo_grams_df","10f7169e":"bar_plot(two_grams_df.iloc[:20],\n         'Two grams',\n         'Two grams',\n         ['Count'],\n         title='Top 20 frequent bigram')","6cf0a50d":"def wordcloud(text,stopwords,ngram=1):\n    # text: if ngram>1, text should be a dictionary\n    wordcloud = WordCloud(width=1400, \n                          height=800,\n                          random_state=2021,\n                          background_color='black',\n                          colormap='Set2',\n                          stopwords=stop)\n    if ngram ==1:\n        wordc = wordcloud.generate(' '.join(text))\n    else:\n        wordc = wordcloud.generate_from_frequencies(text)\n    plt.figure(figsize=(20,10), facecolor='k')\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    \nwordcloud(df['Song'],stop)","a7e1075c":"two_grams_temp = {j.replace(' ','_'):k for j,k in two_grams.items()}\n\nwordcloud(two_grams_temp,stop,ngram=2)","2942ebb4":"three_grams = get_n_grans_count(text, n_grams=3, min_freq=0)\nthree_grams_df = pd.DataFrame(data=three_grams.items())\nthree_grams_df = three_grams_df.sort_values(by=1,ascending=False).rename(columns={0:'Three grams',1:'Count'})\nthree_grams_df","ed65bd7a":"bar_plot(three_grams_df.iloc[:20],\n         'Three grams',\n         'Three grams',\n         ['Count'],\n         title='Top 20 frequent trigram')","70924771":"three_grams_temp = {j.replace(' ','_') : k for j, k in three_grams.items()}\n\nwordcloud(three_grams_temp,stop,ngram=3)","f0331de5":"words_length = {}\n\nfor word in set(words_list):\n    words_length[word] = len(word)\n    \nwords_length = dict(sorted(words_length.items(), reverse=True, key=lambda item: item[1]))\n#sort the data and put it in a data frame for the visualization\nword_length_temp = dict(itertools.islice(words_length.items(), 25))\nwords_length_df = pd.DataFrame(words_length.items(),columns=['word','count']).sort_values('count',ascending=False)","12932885":"df['sentence_len']= df['Song'].str.len()\nprint('Max length     : {} \\nMin length     : {} \\nAverage Length : {}'.\\\n      format(max(df['sentence_len']),min(df['sentence_len']),df['sentence_len'].mean()))","bdfb9b47":"#the longest sentence we have\ndf[df['sentence_len']==max(df['sentence_len'])]['Song'].values[0]","ad416f7c":"#the shortest sentence we have\ndf[df['sentence_len']==min(df['sentence_len'])]['Song'].values[0]","9ad19057":"colors = px.colors.qualitative.Prism\n\nhist_plot(df,\n          'sentence_len',\n          title='Sentences lenght distribution with spaces')","00e818c2":"colors = px.colors.qualitative.Prism\n\n# lets define a function to plot a histogram plot easily\n\ndef hist_plot(df,x,title):\n    fig = px.histogram(x=df[x],\n                       color_discrete_sequence=colors,\n                       opacity=0.8)\n\n    fig['layout'].title=title\n    fig.update_yaxes(tickprefix=\"\", showgrid=True)\n    fig.show()","849101f0":"colors = px.colors.qualitative.Prism\n\ndf['sentence_len_no_sp']= df['Song'].str.split().map(lambda x: len(x))\n\nhist_plot(df,\n          'sentence_len_no_sp',\n          title='Sentences lengh distribution without spaces')","6c96eee9":"#Code by Olga Belitskaya https:\/\/www.kaggle.com\/olgabelitskaya\/sequential-data\/comments\nfrom IPython.display import display,HTML\nc1,c2,f1,f2,fs1,fs2=\\\n'#eb3434','#eb3446','Akronim','Smokum',30,15\ndef dhtml(string,fontcolor=c1,font=f1,fontsize=fs1):\n    display(HTML(\"\"\"<style>\n    @import 'https:\/\/fonts.googleapis.com\/css?family=\"\"\"\\\n    +font+\"\"\"&effect=3d-float';<\/style>\n    <h1 class='font-effect-3d-float' style='font-family:\"\"\"+\\\n    font+\"\"\"; color:\"\"\"+fontcolor+\"\"\"; font-size:\"\"\"+\\\n    str(fontsize)+\"\"\"px;'>%s<\/h1>\"\"\"%string))\n    \n    \ndhtml('Thank you Alaa Sedeeq @alaasedeeq for the script.' )","4d8ddf28":"#Trigrams","3d244661":"#Words length","800c73a6":"#Sentence level analysis\n\nSentence level analysis Text statistics include sentence length distribution, minimum, maximum, and average length. To check the sentence length distribution. Code and output are as follows:","7c81fe14":"#Code by Alaa Sedeeq https:\/\/www.kaggle.com\/alaasedeeq\/commonlit-readability-eda\/execution"}}