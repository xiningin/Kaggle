{"cell_type":{"10818602":"code","f7373e68":"code","50671b4d":"code","7d6c5340":"code","c530f59a":"code","566a5166":"code","74dbcc47":"code","786f77b1":"code","f001acba":"code","c188adac":"code","e5fc6e64":"code","35698b0f":"code","35563a52":"code","f9679c4d":"code","05c77aef":"code","d343e945":"code","415501ad":"code","3aa4dce5":"code","403624b7":"code","4d78a880":"code","5d8f8f1d":"code","9b51558f":"code","4df5c7af":"code","daa9753b":"code","109f085f":"code","707e56f4":"code","6113b389":"code","9b3c3346":"code","f9d47165":"code","9495e6a9":"code","183ac97b":"code","bf9c36d6":"code","32969bee":"code","21ba83de":"code","41a1ba32":"code","d8eaf8e9":"code","0c2c5569":"code","4e079fd2":"code","127da25e":"code","7a5ef45d":"code","55bbe5d8":"code","ebaafef0":"code","b30ba8a8":"markdown","70a48120":"markdown","9f1b3981":"markdown","101a9cac":"markdown","4b4823cf":"markdown","4a0559be":"markdown","424533c2":"markdown","7c4aeb80":"markdown","2a92bb2c":"markdown","1d9645e6":"markdown","dc3c157c":"markdown","0f9481fc":"markdown","ebca5844":"markdown","1c853673":"markdown","244990a1":"markdown","77bd6e4f":"markdown","9ebd2bc5":"markdown","74eb7de8":"markdown","5b75a24c":"markdown","440351c9":"markdown","14214cb5":"markdown","d0423438":"markdown","b03239b1":"markdown","d1f4b767":"markdown","7e6144a3":"markdown","f783e9dc":"markdown","08bb948a":"markdown","c3086efa":"markdown","f6caf024":"markdown","0d0f710c":"markdown","9e6441ad":"markdown","df713477":"markdown","ad553812":"markdown","dea55627":"markdown","8f501ed3":"markdown","ceeff7b1":"markdown","99547570":"markdown","470e737f":"markdown","bbe9c38d":"markdown","d97d772f":"markdown","62e993a5":"markdown"},"source":{"10818602":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f7373e68":"import pandas as pd\nimport numpy as np\nfrom pandas_profiling import ProfileReport\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nfrom plotly.subplots import make_subplots\nimport matplotlib\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.warn(\"this will not show\")\n\nprint('Libraries imported successfully..!!')","50671b4d":"#Reading the csv file heart.csv in variable \ndf = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","7d6c5340":"# looking at the first 5 rows of our data\ndf.head()","c530f59a":"print('Number of rows are',df.shape[0], 'and number of columns are ',df.shape[1])","566a5166":"# !pip install pandas-profiling==2.7.1 ","74dbcc47":"profile = ProfileReport(df, title = \"Pandas Profiling Report\",html = {'style' : {'full_width' : True}})","786f77b1":"profile.to_notebook_iframe()","f001acba":"dict = {}\nfor i in list(df.columns):\n    dict[i] = df[i].value_counts().shape[0]\n\npd.DataFrame(dict,index=[\"unique count\"]).T","c188adac":"df.info()","e5fc6e64":"df[df.duplicated()]","35698b0f":"df.drop_duplicates(inplace=True)","35563a52":"print('Number of rows are',df.shape[0], 'and number of columns are ',df.shape[1])","f9679c4d":"cont_features = [i for i in df.columns if df[i].nunique()>5]\ncat_features = [i for i in df.columns if df[i].nunique()<=5]\ntarget_feature = [\"output\"]\nprint(\"The categorial cols are : \", cat_features)\nprint(\"The continuous cols are : \", cont_features)\nprint(\"The target variable is :  \", target_feature)","05c77aef":"df.describe().T","d343e945":"df.isnull().sum()","415501ad":"df['output'].value_counts()","3aa4dce5":"df.corr().T","403624b7":"df1 = df.copy()\not = {0: \"Less chance of HA\",1:'More chance of HA'}\ndf1.output = [ot[item] for item in df1.output]","4d78a880":"ax = sns.countplot(data=df, x='output',palette=['#85bfdc','#f64c72'])\nax.set(xticklabels=['less chance of heart attack', 'more chance of heart attack'],title=\"Target Distribution\")\nax.tick_params(bottom=False)","5d8f8f1d":"fig = px.histogram(df1, x=\"age\",color=\"output\",\n                   marginal=\"box\",\n                   hover_data=df.columns,\n                  color_discrete_sequence=['#f64c72','#85bfdc'])\nfig.update_layout(\n    title=\"Heart attack chance corresponding to age\"\n)\nfig.show()","9b51558f":"more = df[df['output']==1]['trtbps']\nless = df[df['output']==0]['trtbps']\nfig = ff.create_distplot([less, more],['less chance of heart attack', 'more chance of heart attack']\n                         , show_hist=False, \n                        colors=['#85bfdc','#f64c72'])\nfig.update_layout(\n    title=\"Heart Attack chance corresponding to resting heart rate\",\n    xaxis_title=\"Resting heart rate\",\n)\nfig.show()","4df5c7af":"more = df[df['output']==1]['thalachh']\nless = df[df['output']==0]['thalachh']\nfig = ff.create_distplot([less, more],['less chance of heart attack', 'more chance of heart attack']\n                         , bin_size=5,\n                        colors=['#85bfdc','#f64c72'])\nfig.update_layout(\n    title=\"Heart Attack chance corresponding to maximum heart rate achieved\",\n    xaxis_title=\"Maximum heart rate achieved\",\n)\nfig.show()","daa9753b":"fig = px.box(df1, x=\"cp\", y=\"chol\",color='output',color_discrete_map={'Less chance of HA':'#85bfdc','More chance of HA':'#f64c72'})\nfig.update_layout(title=\"Effects of cholestrol corresponding to chest pain type on chances of heart attack\")\nfig.show()","109f085f":"temp = df.drop(['sex','cp','fbs','exng','restecg','exng','thall','caa','slp'], axis=1)\nfig, ax = plt.subplots(1, 1, figsize=(6,6))\ndf_cor = temp.corr()\nhalf = np.triu(np.ones_like(df_cor, dtype=np.bool))\n\nmy_colors = ['#85bfdc','#f64c72']\ncmap = matplotlib.colors.LinearSegmentedColormap.from_list('Custom', my_colors)\n\nheatmap = sns.heatmap(df_cor, \n            square=True, \n            mask=half,\n            linewidth=2.5, \n            vmax=0.4, vmin=0, \n            cmap=cmap, \n            cbar=False, \n            ax=ax,annot=True)\n\nheatmap.set(title=\"Heatmap of continous variables\")\nheatmap.set_yticklabels(heatmap.get_xticklabels(), rotation = 0)\nheatmap.spines['top'].set_visible(True)\nfig.text(1.2, 0.85, '''* thalachh(Maximum heart rate achieved) is positively correlated while,\n* oldpeak is negatively correlated with the output ''', \n         fontweight='light', fontfamily='serif', fontsize=11, va='top', ha='right') \n\nplt.tight_layout()","707e56f4":"# Create dimensions\nexng = go.parcats.Dimension(\n    values=df.exng,label=\"exng\"\n)\n\ncp = go.parcats.Dimension(\n    values=df.cp,label=\"cp\"\n)\n\nfbs = go.parcats.Dimension(\n    values=df.fbs,label=\"fbs\"\n)\n\ngender_dim = go.parcats.Dimension(values=df.sex, label=\"sex\")\n\nrestecg = go.parcats.Dimension(values=df.sex, label=\"restecg\")\nthall = go.parcats.Dimension(values=df.sex, label=\"thall\")\ncaa = go.parcats.Dimension(values=df.sex, label=\"caa\")\nslp = go.parcats.Dimension(values=df.sex, label=\"slp\")\n\nsurvival_dim = go.parcats.Dimension(\n    values=df.output, label=\"Outcome\", categoryarray=[0, 1],\n    ticktext=['Less chance', 'More chance']\n)\n\n# Create parcats trace\ncolor = df.output;\ncolorscale = [[0, '#85bfdc'], [1, '#f64c72']];\n\nfig = go.Figure(data = [go.Parcats(dimensions=[exng,slp,restecg,fbs,thall,caa,cp,\n                                              gender_dim,survival_dim],\n        line={'color': color, 'colorscale': colorscale},\n        hoveron='color', hoverinfo='count+probability',\n        labelfont={'size': 18, 'family': 'Times'},\n        tickfont={'size': 16, 'family': 'Times'},\n        arrangement='freeform')])\nfig.update_layout(title=\"Plotly parallel categorical plot for all the categorical labels\", )\nfig.show()","6113b389":"lbs = ['sex','cp','fbs','exng','restecg','thall','caa','slp']\n\nrows = 3\ncols = 3\n\nsubplot_titles = [l for l in lbs]\n\nspecs=[[{\"type\": \"bar\"},{\"type\": \"bar\"},{\"type\": \"bar\"}],\n       [{\"type\": \"bar\"},{\"type\": \"bar\"},{\"type\": \"bar\"}],\n       [{\"type\": \"bar\"},{\"type\": \"bar\"},None]]\n\n\nfig = make_subplots(\n        rows=rows,\n        cols=cols,\n        subplot_titles=subplot_titles,\n        specs=specs,  \n        print_grid=False\n)\n\nfor i, b in enumerate(lbs):\n    row = i \/\/ cols + 1\n    col = (i % rows) + 1\n    name = lbs[i]\n    l = [(100)*df[df[name]==x]['output'].sum()\/len(df[df[name]==x]['output']) \n         for x in range(len(df[name].value_counts().tolist()))]\n    fig.add_trace(go.Bar(\n    x = [x for x in range(len(df[name].value_counts().tolist()))],\n    y = l,\n    marker_color=['#85bfdc','#9999c9','#aa77aa','#cc6397','#f64c72'],\n    ),row=row,col=col)\n\nfig.update_layout(autosize = True,\n                  title=\"Percertage of people having 'more chance of heart attack' for each type\", \n                  title_x=0.5,\n                 showlegend=False)\nfig.show()","9b3c3346":"fig = px.scatter_3d(df1, x='oldpeak', y='thalachh', z='age',\n              color='output',size='trtbps',color_discrete_sequence=['#f64c72','#85bfdc'])\nfig.show()","f9d47165":"fig = px.scatter(df1,\n                x='thalachh',\n                y= 'chol',\n                color='output',\n                facet_col='cp', \n                facet_row='sex',\n                color_discrete_sequence=['#f64c72','#85bfdc'], \n                )\n\nfig.show()","9495e6a9":"# Scaling\nfrom sklearn.preprocessing import RobustScaler\n\n# Train Test Split\nfrom sklearn.model_selection import train_test_split\n\n# Models\nimport torch\nimport torch.nn as nn\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.naive_bayes import GaussianNB\nimport xgboost as xgb\n\n# Metrics\nfrom sklearn.metrics import accuracy_score, classification_report, roc_curve, confusion_matrix\n\n# Cross Validation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nprint('Packages imported...')","183ac97b":"# creating a copy of df\ndf2 = df\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df2, columns = cat_cols, drop_first = True)\n\n# defining the features and target\nX = df2.drop(['output'],axis=1)\ny = df2[['output']]\n\n# instantiating the scaler\nscaler = MaxAbsScaler()\n\n# scaling the continuous featuree\nX[con_cols] = scaler.fit_transform(X[con_cols])\nprint(\"The first 5 rows of X are\")\nX.head()","bf9c36d6":"# creating a copy of df\ndf2 = df\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df2, columns = cat_cols, drop_first = True)\n\n# defining the features and target\nX = df2.drop(['output'],axis=1)\ny = df2[['output']]\n\n# instantiating the scaler\nscaler = MinMaxScaler()\n\n# scaling the continuous featuree\nX[con_cols] = scaler.fit_transform(X[con_cols])\nprint(\"The first 5 rows of X are\")\nX.head()","32969bee":"# creating a copy of df\ndf2 = df\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df2, columns = cat_cols, drop_first = True)\n\n# defining the features and target\nX = df2.drop(['output'],axis=1)\ny = df2[['output']]\n\n# instantiating the scaler\nscaler = RobustScaler()\n\n# scaling the continuous featuree\nX[con_cols] = scaler.fit_transform(X[con_cols])\nprint(\"The first 5 rows of X are\")\nX.head()","21ba83de":"# creating a copy of df\ndf2 = df\n\n# define the columns to be encoded and scaled\ncat_cols = ['sex','exng','caa','cp','fbs','restecg','slp','thall']\ncon_cols = [\"age\",\"trtbps\",\"chol\",\"thalachh\",\"oldpeak\"]\n\n# encoding the categorical columns\ndf1 = pd.get_dummies(df2, columns = cat_cols, drop_first = True)\n\n# defining the features and target\nX = df2.drop(['output'],axis=1)\ny = df2[['output']]\n\n# instantiating the scaler\nscaler = StandardScaler()\n\n# scaling the continuous featuree\nX[con_cols] = scaler.fit_transform(X[con_cols])\nprint(\"The first 5 rows of X are\")\nX.head()","41a1ba32":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 42)\nprint(\"The shape of X_train is      \", X_train.shape)\nprint(\"The shape of X_test is       \",X_test.shape)\nprint(\"The shape of y_train is      \",y_train.shape)\nprint(\"The shape of y_test is       \",y_test.shape)","d8eaf8e9":"# instantiating the object and fitting\nclf = SVC(kernel='linear', C=1, random_state=42).fit(X_train,y_train)\n\n# predicting the values\ny_pred = clf.predict(X_test)\n\n# printing the test accuracy\n# print(\"The test accuracy score of SVM is \", accuracy_score(y_test, y_pred))\n\nlr_conf_matrix = confusion_matrix(y_test, y_pred)\nlr_acc_score = accuracy_score(y_test, y_pred)\nprint(\"\\nconfussion matrix :\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Support Vector Machines:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,y_pred))","0c2c5569":"# instantiating the object\nsvm = SVC()\n\n# setting a grid - not so extensive\nparameters = {\"C\":np.arange(1,10,1),'gamma':[0.00001,0.00005, 0.0001,0.0005,0.001,0.005,0.01,0.05,0.1,0.5,1,5]}\n\n# instantiating the GridSearchCV object\nsearcher = GridSearchCV(svm, parameters)\n\n# fitting the object\nsearcher.fit(X_train, y_train)\n\n# the scores\nprint(\"The best params are :\", searcher.best_params_)\nprint(\"The best score is   :\", searcher.best_score_)\n\n# predicting the values\ny_pred = searcher.predict(X_test)\n\n# printing the test accuracy\n# print(\"The test accuracy score of SVM after hyper-parameter tuning is \", accuracy_score(y_test, y_pred))\n\nlr_conf_matrix = confusion_matrix(y_test, y_pred)\nlr_acc_score = accuracy_score(y_test, y_pred)\nprint(\"\\nconfussion matrix :\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of SVM after hyper-parameter tuning:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,y_pred))","4e079fd2":"# instantiating the object\nlogreg = LogisticRegression()\n\n# fitting the object\nlogreg.fit(X_train, y_train)\n\n# calculating the probabilities\ny_pred_proba = logreg.predict_proba(X_test)\n\n# finding the predicted valued\ny_pred = np.argmax(y_pred_proba,axis=1)\n\n# printing the test accuracy\n# print(\"The test accuracy score of Logistric Regression is \", accuracy_score(y_test, y_pred))\n\nlr_conf_matrix = confusion_matrix(y_test, y_pred)\nlr_acc_score = accuracy_score(y_test, y_pred)\nprint(\"\\nconfussion matrix :\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,y_pred))","127da25e":"# instantiating the object\ndt = DecisionTreeClassifier(random_state = 42)\n\n# fitting the model\ndt.fit(X_train, y_train)\n\n# calculating the predictions\ny_pred = dt.predict(X_test)\n\n# printing the test accuracy\n# print(\"The test accuracy score of Decision Tree is \", accuracy_score(y_test, y_pred))\n\nlr_conf_matrix = confusion_matrix(y_test, y_pred)\nlr_acc_score = accuracy_score(y_test, y_pred)\nprint(\"\\nconfussion matrix :\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Decision Tree:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,y_pred))","7a5ef45d":"# instantiating the object\nrf = RandomForestClassifier()\n\n# fitting the model\nrf.fit(X_train, y_train)\n\n# calculating the predictions\ny_pred = dt.predict(X_test)\n\n# printing the test accuracy\n# print(\"The test accuracy score of Random Forest is \", accuracy_score(y_test, y_pred))\n\nlr_conf_matrix = confusion_matrix(y_test, y_pred)\nlr_acc_score = accuracy_score(y_test, y_pred)\nprint(\"\\nconfussion matrix :\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,y_pred))","55bbe5d8":"# instantiate the classifier\ngbt = GradientBoostingClassifier(n_estimators = 300,max_depth=1,subsample=0.8,max_features=0.2,random_state=42)\n\n# fitting the model\ngbt.fit(X_train,y_train)\n\n# predicting values\ny_pred = gbt.predict(X_test)\n# print(\"The test accuracy score of Gradient Boosting Classifier is \", accuracy_score(y_test, y_pred))\n\nlr_conf_matrix = confusion_matrix(y_test, y_pred)\nlr_acc_score = accuracy_score(y_test, y_pred)\nprint(\"\\nconfussion matrix :\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Gradient Boosting Classifie:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,y_pred))","ebaafef0":"scores=[]\nbest_estimators = {}\n\n\nmodel_params = {  \n    \n    \n    'KNeighborsClassifier': {\n        'model': KNeighborsClassifier(),\n        'params': {\n            'n_neighbors': [2,3,4,5,6,7,18,19,20],\n            'algorithm' : ['auto','ball_tree'],\n            'weights' : ['uniform','distance'],\n            'leaf_size' : [27,28,29,30,31]\n        }\n    },\n    \n    'DecisionTreeClassifier': {\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'criterion': ['gini','entropy'],\n            'max_depth' : [None,1,2,6,5]\n        }\n    },\n    \n    \n    'AdaBoostClassifier': {\n        'model': AdaBoostClassifier(),\n        'params': {\n            'n_estimators': [30,35,40,45,50,55],\n            'learning_rate' : [1,1.1,1.2,1.3,1.4,1.5],\n            'algorithm' : ['SAMME', 'SAMME.R']\n        }\n    },\n    \n    'GaussianNB': {\n        'model': GaussianNB(),\n        'params': {\n            \n        }\n    },\n    \n     'LOGISTIC_REGRESSION': {\n        'model': LogisticRegression(),\n        'params': {\n            'C': [1,2,3,4,5,6,7],\n            'solver' : [ 'liblinear', 'lbfgs'],\n            'multi_class' : ['auto', 'ovr' ]\n        }\n    },\n    \n        \n    'SVM': {\n        'model': SVC(),\n        'params': {\n             'C': [1,2,3,5,6,7],\n             'kernel': ['rbf','linear'],\n             'gamma': ['auto', 'scale']\n        }\n    },\n       \n    'RANDOM_FOREST':{\n        'model' : RandomForestClassifier(),\n        'params': {\n            'n_estimators':[1,2,3,4,5,10,15],\n            'criterion': ['entropy'],\n            'random_state' : [12,13],\n            'max_depth' : [5,6]\n\n        }\n    }\n}\n\nimport time\n\n\nfor model_name, mp in model_params.items():\n    clf = GridSearchCV(mp['model'], mp['params'], cv=5, return_train_score=False)\n    # print(mp['model'], mp['params'])\n    start_time = time.time()\n    \n    clf.fit(X_train, y_train)    \n    \n\n    scores.append({\n        'model': model_name,\n        'best_score': clf.best_score_,\n        'best_params': str(clf.best_params_)\n    })\n    best_estimators[model_name] = clf.best_estimator_\n    # print(f'{(time.time() - start_time)\/60} minutes')\n\nimport pandas as pd    \ndf3 = pd.DataFrame(scores,columns=['model','best_score','best_params'])\ndf3 = df3.sort_values(by='best_score',ascending=False)\n# print(df3)\n\n# for i in df3['model'].values.tolist():\n#     print(i ,':', cross_val_score(best_estimators[i],X_test,y_test,cv=5).mean())\n#     print(i)\n\nplt.figure(figsize = (10,5))\nsns.barplot(x = df3['best_score'], y = df3['model'], palette='pastel')","b30ba8a8":"## Tree Models","70a48120":"## Train and test split","9f1b3981":"## Logistic Regression","101a9cac":"# About dataset","4b4823cf":"### Using MinMaxScaler","4a0559be":"## Computing the correlation matrix","424533c2":"## Checking for duplicate rows","7c4aeb80":"## Create dimensions","2a92bb2c":">Here, we can clearly see that maximum heart rate is directly proportional to the chances of heart attack\n","1d9645e6":"### Hyperparameter tuning of SVC","dc3c157c":"> This comes as a surprise that in this data the mean age is lesser for higher chance of heart attack\n> ","0f9481fc":"# Modeling","ebca5844":"**A heart attack occurs when an artery supplying your heart with blood and oxygen becomes blocked. Fatty deposits build up over time, forming plaques in your heart's arteries. If a plaque ruptures, a blood clot can form and block your arteries, causing a heart attack.**","1c853673":"## Reading the Dataset","244990a1":"## Checking the shape of DataFrame","77bd6e4f":"### Decision Tree","9ebd2bc5":"### ","74eb7de8":"# Importing Libraries","5b75a24c":"## Scaling and Encoding features","440351c9":"## Removing the duplicates","14214cb5":"## Checking null values","d0423438":"### Using StandardScaler","b03239b1":"## Describing the Dataset","d1f4b767":"# Exploratory Data Analysis","7e6144a3":"## List of continuous and categorical features and output feature","f783e9dc":"## Checking the number of unique values in each column","08bb948a":"## Pandas Profiling Report","c3086efa":"### Random Forest","f6caf024":"## Checking new shape","0d0f710c":"### Using RobustScaler","9e6441ad":"## Support Vector Machines","df713477":"### Gradient Boosting Classifier - without tuning","ad553812":"## Checking how many classes in target variable ","dea55627":"### Using MaxAbsScaler","8f501ed3":"## Info of Dataset","ceeff7b1":">For certain categories the chances of heart attack was found high:-\n> - Age = 0\n> - cp = 2,3\n> - thall = 2\n> - caa = 0,4\n> - slp = 2","99547570":"   - **Age** : Age of the patient\n\n  - **Sex** : Sex of the patient\n\n  - **exang**: exercise induced angina (1 = yes; 0 = no)\n\n  - **ca**: number of major vessels (0-3)\n\n  - **cp** : Chest Pain type chest pain type\n\n    - **Value 1**: typical angina\n\n    - **Value 2**: atypical angina\n\n    - **Value 3**: non-anginal pain\n\n    - **Value 4**: asymptomatic\n\n  - **trtbps** : resting blood pressure (in mm Hg)\n\n  - **chol** : cholestoral in mg\/dl fetched via BMI sensor\n\n  - **fbs** : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\n  - **restecg** : resting electrocardiographic results\n\n      - **Value 0**: normal\n\n      - **Value 1**: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n\n      - **Value 2**: showing probable or definite left ventricular hypertrophy by Estes' criteria\n\n  - **thalachh** : maximum heart rate achieved\n\n  - **output** : 0 = less chance of heart attack 1 = more chance of heart attack","470e737f":">Some, features like resting heart rate are indifferent to chances of heart attack\n","bbe9c38d":"# Packages ","d97d772f":"# Making features model ready","62e993a5":"## Linear Classifiers"}}