{"cell_type":{"d1f823d4":"code","6b788fda":"code","fa562025":"code","6ccdac17":"code","919b8309":"code","83ce6635":"code","99742096":"code","eb5aa0b4":"code","f652bf0f":"code","9d2f2f79":"code","927df9f9":"code","fa4416b6":"code","f50ddb44":"code","47de0a7e":"code","706834e0":"code","78cafeb0":"code","07acd904":"code","0537e83b":"code","50f915eb":"code","02aa6c94":"code","9463c284":"code","8e7a53fc":"code","fa90c0c5":"code","69472e0f":"code","cc160349":"code","b2c6033d":"code","18ced03f":"code","492c805e":"code","8f95cc47":"code","86986121":"code","3d8f115b":"markdown","a4127795":"markdown","fe99fb6b":"markdown","87a48819":"markdown","04c0de18":"markdown","6e9f20b3":"markdown","91f86f2c":"markdown","933cc554":"markdown","d9df2672":"markdown","5fbeaf61":"markdown","89c40224":"markdown","e98d2680":"markdown","d9ca23c1":"markdown","d9d253dd":"markdown","da4e8ddb":"markdown","97a855f4":"markdown","148f28f2":"markdown","76ca9296":"markdown","da5dbefb":"markdown","2167e259":"markdown","6567f4b1":"markdown","792e2805":"markdown","210d7d74":"markdown","29aa4d1b":"markdown","c94a067f":"markdown","3e271f6e":"markdown","31748544":"markdown"},"source":{"d1f823d4":"import re\nimport os\nimport gc\nimport glob\nimport json\nimport time\nimport copy\nimport torch\nimport random\nimport datetime\nimport tokenizers\nimport numpy as np\nimport transformers\nimport pandas as pd\nimport torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tokenizers import *\nfrom transformers import *\nfrom functools import partial\nfrom tqdm.notebook import tqdm\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import GroupKFold","6b788fda":"SEED = 2020\n\nDATA_PATH = \"..\/input\/coleridgeinitiative-show-us-the-data\/\"\nDATA_PATH_TRAIN = DATA_PATH + 'train\/'\nDATA_PATH_TEST = DATA_PATH + 'test\/'\n\nNUM_WORKERS = 4\n\nVOCABS = {\n    \"bert-base-uncased\": \"..\/input\/vocabs\/bert-base-uncased-vocab.txt\",\n}\n\nMODEL_PATHS = {\n    'bert-base-uncased': '..\/input\/bertconfigs\/uncased_L-12_H-768_A-12\/uncased_L-12_H-768_A-12\/',\n    'bert-large-uncased-whole-word-masking-finetuned-squad': '..\/input\/bertconfigs\/wwm_uncased_L-24_H-1024_A-16\/wwm_uncased_L-24_H-1024_A-16\/',\n    'albert-large-v2': '..\/input\/albert-configs\/albert-large-v2\/albert-large-v2\/',\n    'albert-base-v2': '..\/input\/albert-configs\/albert-base-v2\/albert-base-v2\/',\n    'distilbert': '..\/input\/albert-configs\/distilbert\/distilbert\/',\n}\n\nFL_TH = 0.75\nDATA_PREPARING = False\nREMOVE_LONG = False","fa562025":"def load_text(id_, root=\"\"):\n    with open(os.path.join(root, id_ + \".json\")) as f:\n        text = json.load(f)\n    return text\n\n\ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\n\ndef create_data(df):\n    new_df = []\n\n    for idx in tqdm(range(len(df))):\n        article = load_text(df['Id'][idx], DATA_PATH_TRAIN)\n        id_, pub_title, dataset_title, dataset_label, cleaned_label = df.iloc[idx]\n        \n        \n        for i, section in enumerate(article):\n            text = section['text']\n            title = section['section_title']\n            \n            cleaned_text = clean_text(section['text'])\n            \n            found = cleaned_label in cleaned_text\n    \n            dic = {\n                \"id\": [id_], \n                \"section_id\": [i],\n                \"pub_title\": [pub_title], \n                \"dataset_title\": [dataset_title], \n                \"dataset_label\": [dataset_label], \n                \"cleaned_label\": [cleaned_label],\n                \"text\": [text],\n                \"cleaned_text\": [cleaned_text],\n                \"label_found\": [found],\n            }\n            new_df.append(pd.DataFrame.from_dict(dic))\n            \n    return pd.concat(new_df).reset_index(drop=True)\n\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\ndef jaccard_similarity(s1, s2):\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")    \n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) \/ union","6ccdac17":"# train\ntrain_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train.csv'\npaper_train_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\n\ntrain = pd.read_csv(train_path)\nprint('train size before agg.:', len(train))\n\n# Group by publication, training labels should have the same form as expected output.\ntrain = train.groupby('Id').agg({\n    'pub_title': 'first',\n    'dataset_title': '|'.join,\n    'dataset_label': '|'.join,\n    'cleaned_label': '|'.join\n}).reset_index()    \nprint('train size after agg.:', len(train))\n\ntrain = train.sort_values(by=['Id'])\ntrain.head()","919b8309":"# train\npseudo_train_path = '..\/input\/coleridge-pseudolabels\/submission.csv'\npaper_train_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\n\npseudo_train = pd.read_csv(pseudo_train_path)\nprint('pseudo_train size before drop_duplicates.:', len(pseudo_train))\n\npseudo_train = pseudo_train.drop_duplicates(subset='Id')\nprint('pseudo_train size after drop_duplicates:', len(pseudo_train), '\\n')\n\npseudo_train = pseudo_train.sort_values(by=['Id']).reset_index(drop=True)\npseudo_train.head()","83ce6635":"if DATA_PREPARING:\n    \n    final_predictions = []\n    for pred_match, perd_mlm in tqdm(zip(train['cleaned_label'], pseudo_train['PredictionString'])):\n        if pred_match:\n            labels = pred_match.split('|')\n\n            # literal_preds + pred_mlm_labels\n            if perd_mlm:\n                filtered_labels = labels\n                labels_mlm = perd_mlm.split('|')\n                for label_mlm in labels_mlm:\n                    if all(jaccard_similarity(label_mlm, got_label) < FL_TH for got_label in labels):\n                        filtered_labels.append(label_mlm)\n\n            # literal_preds\n            else: filtered_labels = labels\n\n        # pred_mlm_labels\n        elif perd_mlm:\n            filtered_labels = perd_mlm.split('|')\n\n        # ''\n        else:\n            filtered_labels = []\n\n        final_predictions.append('|'.join(filtered_labels))\n\n    train['cleaned_label'] = final_predictions\n\n    print(f'FL_TH = {FL_TH}: \\n{final_predictions[:4]}')\n    del pseudo_train, final_predictions\n\nelse:\n    del train, pseudo_train","99742096":"if DATA_PREPARING:\n    \n    train['cleaned_label_list'] = train['cleaned_label'].apply(lambda label_str: label_str.split('|'))\n\n    train_temp = copy.deepcopy(train)\n\n    for index, row in tqdm(train.iterrows()):\n        label_list = row['cleaned_label_list']\n        if len(label_list) == 1:\n            train_temp['cleaned_label'][index] = label_list[0]\n        elif len(label_list) > 1:\n            train_temp['cleaned_label'][index] = label_list[0]\n            for label in label_list[1:]:\n                row['cleaned_label'] = label\n                train_temp = train_temp.append(row, ignore_index=True)\n\n    train = train_temp.drop('cleaned_label_list', axis=1)\n    del train_temp","eb5aa0b4":"if DATA_PREPARING:\n    \n    new_df = create_data(train)  # Quite slow, could be sped-up with multi-processing.\n\n    del train","f652bf0f":"if DATA_PREPARING:\n    sns.countplot(x=new_df['label_found'])\n    plt.show()","9d2f2f79":"if DATA_PREPARING:\n    print(f'len(new_df) b\/ removing label_not_found = {len(new_df)}')\n\n    df = new_df[new_df['label_found']].reset_index(drop=True)\n    print(f'len(df) b\/ removing long texts = {len(df)}')\n    del new_df\n\n    if REMOVE_LONG:\n        df['length'] = df['cleaned_text'].apply(lambda x: len(x.split()))\n        df = df[df['length'] < 3000]  # remove too long texts\n        print(f'len(df) a\/ removing long texts = {len(df)}')\n\n    df.to_csv(\"df_train.csv\", index=False)  # saving, just in case\n    \nelse: df = None\n\ndf","927df9f9":"def find_data_sets_id(publication_id: int) -> list:\n    data_set_ids = []\n    for class_ in data_set_citations:\n        if class_['publication_id'] == publication_id:\n            data_set_ids.append(class_['data_set_id'])\n    return data_set_ids\n\n\ndef find_data_set_citations_mention_list(publication_id: int) -> str:\n    mention_list = []\n    for class_ in data_set_citations:\n        if class_['publication_id'] == publication_id:\n            cleaned_labels = list( map(clean_text, class_['mention_list']) )\n            mention_list.append( '|'.join(cleaned_labels) )\n    return '|'.join( [label for label in mention_list if label != ''] )\n\n\ndef find_data_sets_title(data_set_id: int) -> str:\n    for class_ in data_sets:\n        if class_['data_set_id'] == data_set_id:\n            return class_['title']\n\n\ndef RichContextDF(publications) -> pd.DataFrame:\n    '''\n    Use publications.json do generate a training set DF\n    '''\n    data_sets_title = []\n    cleande_labels = []\n    cleaned_text = []\n    for class_ in tqdm(publications):\n        publication_id = class_['publication_id']\n        \n        # to get data_sets_title\n        data_sets_title_temp = []\n        for data_sets_id in find_data_sets_id(publication_id):\n            data_sets_title_temp.append( find_data_sets_title(data_sets_id) )\n        data_sets_title.append('||'.join(data_sets_title_temp))\n        \n        # to get cleande_labels\n        cleande_labels.append( find_data_set_citations_mention_list(publication_id) )\n        \n        # to get text\n        with open(f'..\/input\/rich-context-competition-train-testtargz\/train_test\/files\/text\/{publication_id}.txt', 'r') as f:\n            text = f.readlines()\n        text = [line[:-1] for line in text]\n        text = list(map(clean_text, text))\n        text = ' '.join(text)        \n        cleaned_text.append(text)\n\n    return pd.DataFrame({\n        'dataset_title': data_sets_title,\n        'cleaned_label': cleande_labels,\n        'cleaned_text': cleaned_text,\n    })","fa4416b6":"if DATA_PREPARING:\n    with open(f'..\/input\/rich-context-competition-train-testtargz\/train_test\/publications.json', 'r') as f:\n        publications = json.load(f)\n    with open(f'..\/input\/rich-context-competition-train-testtargz\/train_test\/data_set_citations.json', 'r') as f:\n        data_set_citations = json.load(f)\n    with open(f'..\/input\/rich-context-competition-train-testtargz\/train_test\/data_sets.json', 'r') as f:\n        data_sets = json.load(f)","f50ddb44":"if DATA_PREPARING:\n    RichContext_train = RichContextDF(publications)\n    RichContext_train = RichContext_train[ RichContext_train['cleaned_label'] != '' ]\n    \nelse: RichContext_train = None\n\nRichContext_train","47de0a7e":"# prepare for required format\n\nif DATA_PREPARING:\n    RichContext_train['cleaned_label_list'] = RichContext_train['cleaned_label'].apply(lambda label_str: label_str.split('|'))\n\n    RichContext_train_temp = copy.deepcopy(RichContext_train)\n\n    for index, row in tqdm( RichContext_train.iterrows() ):\n        label_list = row['cleaned_label_list']\n        if len(label_list) == 1:\n            RichContext_train_temp['cleaned_label'][index] = label_list[0]\n        elif len(label_list) > 1:\n            RichContext_train_temp['cleaned_label'][index] = label_list[0]\n            for label in label_list[1:]:\n                row['cleaned_label'] = label\n                RichContext_train_temp = RichContext_train_temp.append(row, ignore_index=True)\n\n    RichContext_train = RichContext_train_temp.drop('cleaned_label_list', axis=1)\n    del RichContext_train_temp\n\nRichContext_train","706834e0":"# label_found and length\n\nif DATA_PREPARING:\n    label_found = []\n    for index, row in RichContext_train[['cleaned_label', 'cleaned_text']].iterrows():\n        cleaned_label = row['cleaned_label']\n        cleaned_text = row['cleaned_text']\n        found = cleaned_label in cleaned_text\n        label_found.append(found)\n    RichContext_train['label_found'] = label_found\n    del label_found\n    print(f'len(RichContext_train) b\/ removing label_not_found = {len(RichContext_train)}')\n    \n    RichContext_train = RichContext_train[RichContext_train['label_found']].reset_index(drop=True)\n    print(f'len(RichContext_train) b\/ removing long texts = {len(RichContext_train)}')\n\n    RichContext_train['length'] = RichContext_train['cleaned_text'].apply(lambda x: len(x.split()))\n\n    RichContext_train.to_csv(\"RichContext_train.csv\", index=False)\n    \nRichContext_train","78cafeb0":"if DATA_PREPARING:\n    df = df.append(RichContext_train, ignore_index=True)\n    del RichContext_train\n    df.to_csv(\"df_train_final.csv\", index=False)\n    \nelse: df = pd.read_csv('..\/input\/bert-for-question-answering-baseline-training\/df_train_final.csv')\n\ndf","07acd904":"def seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results.\n\n    Args:\n        seed (int): Number of the seed.\n    \"\"\"\n\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n\ndef save_model_weights(model, filename, verbose=1, cp_folder=\"\"):\n    \"\"\"\n    Saves the weights of a PyTorch model.\n\n    Args:\n        model (torch model): Model to save the weights of.\n        filename (str): Name of the checkpoint.\n        verbose (int, optional): Whether to display infos. Defaults to 1.\n        cp_folder (str, optional): Folder to save to. Defaults to \"\".\n    \"\"\"\n\n    if verbose:\n        print(f\"\\n -> Saving weights to {os.path.join(cp_folder, filename)}\\n\")\n    torch.save(model.state_dict(), os.path.join(cp_folder, filename))\n\n\ndef count_parameters(model, all=False):\n    \"\"\"\n    Count the parameters of a model.\n\n    Args:\n        model (torch model): Model to count the parameters of.\n        all (bool, optional):  Whether to count not trainable parameters. Defaults to False.\n\n    Returns:\n        int: Number of parameters.\n    \"\"\"\n\n    if all:\n        return sum(p.numel() for p in model.parameters())\n    else:\n        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","0537e83b":"class EncodedText:\n    def __init__(self, ids, offsets):\n        self.ids = ids\n        self.offsets = offsets\n\n\ndef create_tokenizer_and_tokens(config):\n    if \"roberta\" in config.selected_model:\n        raise NotImplementedError\n        \n    elif \"albert\" in config.selected_model:\n        raise NotImplementedError\n        \n    else:\n        tokenizer = BertWordPieceTokenizer(\n            MODEL_PATHS[config.selected_model] + 'vocab.txt',\n            lowercase=config.lowercase,\n        )\n\n        tokens = {\n            'cls': tokenizer.token_to_id('[CLS]'),\n            'sep': tokenizer.token_to_id('[SEP]'),\n            'pad': tokenizer.token_to_id('[PAD]'),\n        }\n    \n    return tokenizer, tokens","50f915eb":"def locate_label_string(text, label):\n    \"\"\"\n    Finds the label in the text\n    \"\"\"\n    len_label = len(label) - 1\n\n    candidates_idx = [i for i, e in enumerate(text) if e == label[1]]\n    for idx in candidates_idx:\n        if \" \" + text[idx: idx + len_label] == label:\n            idx_start = idx\n            idx_end = idx + len_label\n            break\n\n    assert (\n        text[idx_start:idx_end] == label[1:]\n    ), f'\"{text[idx_start: idx_end]}\" instead of \"{label}\" in \"{text}\"'\n\n    char_targets = np.zeros(len(text))\n    char_targets[idx_start:idx_end] = 1\n\n    return idx_start, idx_end, char_targets\n\n\ndef locate_label_tokens(offsets, char_targets):\n    \"\"\"\n    Finds the tokens corresponding to the found labels\n    \"\"\"\n    target_idx = []\n    for idx, (offset1, offset2) in enumerate(offsets):\n        if sum(char_targets[offset1:offset2]) > 0:\n            target_idx.append(idx)\n\n    if not len(target_idx):\n        for idx, (offset1, offset2) in enumerate(offsets):\n            if sum(char_targets[offset1:offset2]) > 0:\n                target_idx.append(idx)\n\n    return target_idx[0], target_idx[-1]","02aa6c94":"def process_data(\n    text,\n    label,\n    tokenizer,\n    tokens,\n    max_len=100,\n    model_name=\"bert\",\n):\n    \"\"\"\n    Prepares the data for the question answering task.\n    Adapted from Abishek's work on the Tweet Sentiment extraction competition, \n    check his work for more details !\n    \"\"\"\n    target_start, target_end = 0, 0\n    text = \" \" + \" \".join(str(text).split())\n    label = \" \" + \" \".join(str(label).split())\n\n    if label != \" \":\n        idx_start, idx_end, char_targets = locate_label_string(\n            text, label\n        )\n\n    tokenized = tokenizer.encode(text)\n    input_ids_text = tokenized.ids[1:-1]\n\n    # print(input_ids_text, len(input_ids_text))\n\n    offsets = tokenized.offsets[1:-1]\n\n    if label != \" \":\n        target_start, target_end = locate_label_tokens(offsets, char_targets)\n\n    if target_end >= max_len - 2:  # target is too far in the sentence, we crop its beginning.\n        n_tok_to_crop = target_start - max_len \/\/ 2\n        new_str_start = offsets[n_tok_to_crop][0]\n\n        input_ids_text = input_ids_text[n_tok_to_crop:]\n\n        offsets = [tuple(t) for t in np.array(offsets[n_tok_to_crop:]) - new_str_start]\n        text = text[new_str_start:]\n\n        target_start -= n_tok_to_crop\n        target_end -= n_tok_to_crop\n\n    input_ids = (\n        [tokens[\"cls\"]]\n        + input_ids_text[:max_len - 2]\n        + [tokens[\"sep\"]]\n    )\n\n    if \"roberta\" in model_name:\n        token_type_ids = [0] * len(input_ids)\n    else:\n        token_type_ids = [1] * len(input_ids)\n\n    text_offsets = [(0, 0)] + offsets[:max_len - 2] + [(0, 0)]\n\n    target_start += 1\n    target_end += 1\n\n    # target_end = min(target_end, max_len - 1)\n\n    assert len(input_ids) == len(token_type_ids) and len(input_ids) == len(text_offsets), (len(input_ids), len(text_offsets))  # noqa\n\n    padding_length = max_len - len(input_ids)\n    if padding_length > 0:\n        input_ids = input_ids + ([tokens[\"pad\"]] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        text_offsets = text_offsets + ([(0, 0)] * padding_length)\n\n    return {\n        \"ids\": input_ids,\n        \"token_type_ids\": token_type_ids,\n        \"targets_start\": target_start,\n        \"targets_end\": target_end,\n        \"text\": text,\n        \"label\": label,\n        \"offsets\": text_offsets,\n    }","9463c284":"class SectionDataset(Dataset):\n    def __init__(\n        self,\n        df,\n        tokenizer,\n        tokens,\n        max_len=512,\n        model_name=\"bert\",\n    ):\n        self.tokens = tokens\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        self.model_name = model_name\n\n        self.texts = df[\"cleaned_text\"].values\n        self.labels = df[\"cleaned_label\"].values\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        data = process_data(\n            self.texts[idx],\n            self.labels[idx],\n            self.tokenizer,\n            self.tokens,\n            max_len=self.max_len,\n            model_name=self.model_name,\n        )\n\n        return {\n            \"ids\": torch.tensor(data[\"ids\"], dtype=torch.long),\n            \"token_type_ids\": torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n            \"target_start\": torch.tensor(data[\"targets_start\"], dtype=torch.long),\n            \"target_end\": torch.tensor(data[\"targets_end\"], dtype=torch.long),\n            \"text\": data[\"text\"],\n            \"label\": data[\"label\"],\n            \"offsets\": torch.tensor(data[\"offsets\"], dtype=torch.long),\n        }","8e7a53fc":"TRANSFORMERS = {\n    \"roberta-base\": (RobertaModel, \"roberta-base\"),\n    'albert-base-v2': (AlbertModel, 'albert-base-v2'),\n    'albert-large-v2': (AlbertModel, 'albert-large-v2'),\n    'albert-xlarge-v2': (AlbertModel, 'albert-xlarge-v2'),\n    'albert-xxlarge-v2': (AlbertModel, 'albert-xxlarge-v2'),\n    \"bert-base-uncased\": (BertModel, \"bert-base-uncased\"),\n    \"bert-base-cased\": (BertModel, \"bert-base-cased\"),\n    \"bert-large-uncased-whole-word-masking\": (BertModel, \"bert-large-uncased-whole-word-masking\"),\n    \"distilbert-base-uncased-distilled-squad\": (\n        DistilBertModel,\n        \"distilbert-base-uncased-distilled-squad\"\n    )\n}\n\n\nclass QATransformer(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.name = model\n\n        self.pad_idx = 1 if \"roberta\" in self.name else 0\n\n        model_class, pretrained_weights = TRANSFORMERS[model]\n\n        self.transformer = model_class.from_pretrained(\n            pretrained_weights, output_hidden_states=True\n        )\n\n        self.nb_features = self.transformer.pooler.dense.out_features\n\n        self.logits = nn.Sequential(\n            nn.Linear(self.nb_features, self.nb_features),\n            nn.Tanh(),\n            nn.Linear(self.nb_features, 2),\n        )\n\n    def forward(self, tokens, token_type_ids):\n        \"\"\"\n        Usual torch forward function\n\n        Arguments:\n            tokens {torch tensor} -- Sentence tokens\n            token_type_ids {torch tensor} -- Sentence tokens ids\n        \"\"\"\n\n        hidden_states = self.transformer(\n            tokens,\n            attention_mask=(tokens != self.pad_idx).long(),\n            token_type_ids=token_type_ids,\n        )[-1]\n\n        features = hidden_states[-1]\n        logits = self.logits(features)\n\n        start_logits, end_logits = logits[:, :, 0], logits[:, :, 1]\n\n        return start_logits, end_logits","fa90c0c5":"def jaccard_similarity(s1, s2): # could be wrong, see CPMP's thread\n    l1 = s1.split(\" \")\n    l2 = s2.split(\" \")\n    intersection = len(list(set(l1).intersection(l2)))\n    union = (len(l1) + len(l2)) - intersection\n    return float(intersection) \/ union\n\n\ndef jaccard_similarity(str1, str2): # the right one\n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\ndef compute_score(y_true, y_pred, beta=0.5):\n    \"\"\"\n    From https:\/\/www.kaggle.com\/tungmphung\/coleridge-initiative-local-score-computation\/\n    \"\"\"\n    TP, FP, FN = 0, 0, 0\n\n    for truth, pred in zip(y_true, y_pred):\n        true_datasets = truth.split('|')\n        # Predicted strings for each publication are sorted alphabetically\n        # and processed in that order.\n        pred_datasets = sorted(pred.split('|'))\n\n        for true_dataset in true_datasets:\n            if len(pred_datasets):\n                match_scores = [jaccard_similarity(true_dataset, pred_dataset)\n                                for pred_dataset in pred_datasets]\n                # The prediction with the highest score for a given ground truth\n                # is matched with that ground truth.\n                match_index = np.argmax(match_scores)\n\n                if match_scores[match_index] >= 0.5:\n                    # Any matched predictions where the Jaccard score meets or\n                    # exceeds the threshold of 0.5 are counted as true positives (TP),\n                    TP += 1\n                else:\n                    # the remainder as false positives (FP).\n                    FP += 1\n\n                del(pred_datasets[match_index])\n            else:\n                # Any ground truths with no nearest predictions are counted as\n                # false negatives (FN).\n                FN += 1\n        # Any unmatched predictions are counted as false positives (FP).\n        FP += len(pred_datasets)\n\n    precision = TP \/ (TP + FP)\n    recall = TP \/ (TP + FN)\n    f_score = (1 + beta**2)*(precision*recall)\/((beta**2)*precision + recall)\n\n    return f_score","69472e0f":"def get_string_from_idx(text, idx_start, idx_end, offsets):\n    \"\"\"\n    Uses the offsets to retrieve the predicted string based on the start and end indices\n    \"\"\"\n    if idx_end < idx_start:\n        idx_end = idx_start\n\n    predicted_string = \"\"\n    for i in range(idx_start, idx_end + 1):\n        predicted_string += text[offsets[i][0]: offsets[i][1]]\n        if i + 1 < len(offsets) and offsets[i][1] < offsets[i + 1][0]:\n            predicted_string += \" \"\n\n    return predicted_string\n\ndef get_pred_from_logits(data, start_logits, end_logits, from_proba=False):\n    if not from_proba:\n        start_logits = torch.softmax(start_logits, dim=1).cpu().detach().numpy()\n        end_logits = torch.softmax(end_logits, dim=1).cpu().detach().numpy()\n\n    offsets = data[\"offsets\"].cpu().numpy()\n\n    preds = []\n    for i in range(len(start_logits)):\n        start_idx = np.argmax(start_logits[i])\n        end_idx = np.argmax(end_logits[i])\n        preds.append(get_string_from_idx(data[\"text\"][i], start_idx, end_idx, offsets[i]))\n\n    return preds","cc160349":"def ce_loss(\n    pred, truth, smoothing=False, trg_pad_idx=-1, eps=0.1\n):\n    \"\"\"\n    Computes the cross entropy loss with label smoothing\n\n    Args:\n        pred (torch tensor): Prediction\n        truth (torch tensor): Target\n        smoothing (bool, optional): Whether to use smoothing. Defaults to False.\n        trg_pad_idx (int, optional): Indices to ignore in the loss. Defaults to -1.\n        eps (float, optional): Smoothing coefficient. Defaults to 0.1.\n\n    Returns:\n        [type]: [description]\n    \"\"\"\n    truth = truth.contiguous().view(-1)\n\n    one_hot = torch.zeros_like(pred).scatter(1, truth.view(-1, 1), 1)\n\n    if smoothing:\n        n_class = pred.size(1)\n        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps \/ (n_class - 1)\n\n    loss = -one_hot * F.log_softmax(pred, dim=1)\n\n    if trg_pad_idx >= 0:\n        loss = loss.sum(dim=1)\n        non_pad_mask = truth.ne(trg_pad_idx)\n        loss = loss.masked_select(non_pad_mask)\n\n    return loss.sum()\n\n\ndef qa_loss_fn(start_logits, end_logits, start_positions, end_positions, config):\n    \"\"\"\n    Loss function for the question answering task.\n    It is the sum of the cross entropy for the start and end logits\n\n    Args:\n        start_logits (torch tensor): Start logits\n        end_logits (torch tensor): End logits\n        start_positions (torch tensor): Start ground truth\n        end_positions (torch tensor): End ground truth\n        config (dict): Dictionary of parameters for the CE Loss.\n\n    Returns:\n        torch tensor: Loss value\n    \"\"\"\n    bs = start_logits.size(0)\n\n    start_loss = ce_loss(\n        start_logits,\n        start_positions,\n        smoothing=config[\"smoothing\"],\n        eps=config[\"eps\"],\n    )\n\n    end_loss = ce_loss(\n        end_logits,\n        end_positions,\n        smoothing=config[\"smoothing\"],\n        eps=config[\"eps\"],\n    )\n\n    total_loss = start_loss + end_loss\n\n    return total_loss \/ bs\n","b2c6033d":"def trim_tensors(tokens, input_ids, model_name='bert', min_len=10):\n    \"\"\"\n    Trim tensors so that within a batch, padding is shortened.\n    This speeds up training for RNNs and Transformers\n\n    Arguments:\n        tokens {torch tensor} -- Text tokens\n\n    Keyword Arguments:\n        min_len {int} -- Minimum length to trim to (default: {10})\n\n    Returns:\n        torch tensor -- trimmed tokens\n    \"\"\"\n    pad_token = 1 if \"roberta\" in model_name else 0\n    max_len = max(torch.max(torch.sum((tokens != pad_token), 1)), min_len)\n    return tokens[:, :max_len], input_ids[:, :max_len]","18ced03f":"def fit(\n    model,\n    train_dataset,\n    val_dataset,\n    loss_config,\n    epochs=5,\n    batch_size=8,\n    weight_decay=0,\n    warmup_prop=0.0,\n    lr=5e-4,\n    cfg=dict()\n):\n\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, num_workers=NUM_WORKERS, shuffle=True,\n    )\n    val_loader = DataLoader(\n        val_dataset, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKERS\n    )\n\n    opt_params = []\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    for n, p in model.named_parameters():\n        wd = 0 if any(nd in n for nd in no_decay) else weight_decay\n        opt_params.append(\n            {\"params\": [p], \"weight_decay\": wd, \"lr\": lr}\n        )\n\n    optimizer = AdamW(opt_params, lr=lr, betas=(0.5, 0.999))\n\n    n_steps = epochs * len(train_loader)\n    num_warmup_steps = int(warmup_prop * n_steps)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps, n_steps\n    )\n\n    total_steps = 0\n    for epoch in range(epochs):\n        model.train()\n        start_time = time.time()\n\n        optimizer.zero_grad()\n        avg_loss = 0\n\n        for step, data in enumerate(train_loader):\n            total_steps += 1\n\n            ids, token_type_ids = trim_tensors(\n                data[\"ids\"], data[\"token_type_ids\"], model.name\n            )\n\n            start_logits, end_logits = model(ids.cuda(), token_type_ids.cuda())\n\n            loss = qa_loss_fn(\n                start_logits,\n                end_logits,\n                data[\"target_start\"].cuda(),\n                data[\"target_end\"].cuda(),\n                config=loss_config,\n            )\n\n            avg_loss += loss.item() \/ len(train_loader)\n            loss.backward()\n\n            nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n\n            optimizer.step()\n            scheduler.step()\n            model.zero_grad()\n\n        model.eval()\n        avg_val_loss = 0.\n        preds, truths = [], []\n\n        with torch.no_grad():\n\n            for data in val_loader:\n                ids, token_type_ids = trim_tensors(\n                    data[\"ids\"], data[\"token_type_ids\"], model.name\n                )\n\n                start_logits, end_logits = model(ids.cuda(), token_type_ids.cuda())\n\n                loss = qa_loss_fn(\n                    start_logits.detach(),\n                    end_logits.detach(),\n                    data[\"target_start\"].cuda(),\n                    data[\"target_end\"].cuda(),\n                    config=loss_config,\n                )\n\n                avg_val_loss += loss.item() \/ len(val_loader)\n\n                preds += get_pred_from_logits(data, start_logits, end_logits)\n                truths += data['label']\n\n        score = compute_score(truths, preds)\n\n        dt = time.time() - start_time\n        lr = scheduler.get_last_lr()[0]\n        print(f\"Epoch {epoch + 1}\/{epochs} \\t lr={lr:.1e} \\t t={dt:.0f}s \\t\", end=\"\")\n        print(\n            f\"loss={avg_loss:.3f} \\t val_loss={avg_val_loss:.3f} \\t val_score={score:.4f}\"\n        )\n        \n        save_model_weights(model, f\"{cfg['selected_model']}_{cfg['fold']}_{epoch}.pt\", cp_folder=\"\")\n\n    del loss, data, avg_val_loss, avg_loss, train_loader, val_loader\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return preds\n","492c805e":"def k_fold(config, df, save=True):\n    tokenizer, tokens = create_tokenizer_and_tokens(config)\n    \n    time = re.sub(' ', '_', str(datetime.datetime.now())[:16])\n    score = 0\n    \n    gkf = GroupKFold(n_splits=config.k)\n    folds = list(gkf.split(X=df, groups=df['dataset_title']))\n    \n    pred_oof = [''] * len(df)\n    \n    for fold, (train_idx, val_idx) in enumerate(folds):\n        if fold in config.selected_folds:\n            print(f\"\\n-------------   Fold {fold + 1} \/ {len(folds)}  -------------\\n\")\n            seed_everything(config.seed + fold)\n\n            model = QATransformer(config.selected_model).cuda()\n            model.zero_grad()\n\n            train_dataset = SectionDataset(\n                df.iloc[train_idx], \n                tokenizer, \n                tokens, \n                max_len=config.max_len, \n                model_name=config.selected_model\n            )\n\n            val_dataset = SectionDataset(\n                df.iloc[val_idx], \n                tokenizer, \n                tokens,\n                max_len=config.max_len, \n                model_name=config.selected_model\n            )\n\n            n_parameters = count_parameters(model)\n\n            print(f\"    -> {len(train_dataset)} training texts\")\n            print(f\"    -> {len(val_dataset)} validation texts\")\n            print(f\"    -> {n_parameters} trainable parameters\\n\")  \n\n            preds = fit(\n                model, \n                train_dataset, \n                val_dataset, \n                config.loss_config,\n                epochs=config.epochs, \n                batch_size=config.batch_size, \n                weight_decay=config.weight_decay, \n                lr=config.lr, \n                warmup_prop=config.warmup_prop,\n                cfg={'selected_model': config.selected_model,\n                    'fold': fold}\n            )\n\n            for i, idx in enumerate(val_idx):\n                pred_oof[idx] = preds[i]\n\n            if save:\n                save_model_weights(model, f'{config.selected_model}_{fold + 1}.pt', cp_folder=\"\")\n\n            del model, train_dataset, val_dataset\n            torch.cuda.empty_cache()\n            gc.collect()","8f95cc47":"class Config:\n    # General\n    k = 5\n    seed = 2021\n    selected_folds = [0] # 0\n\n    # Texts\n    max_len = 256\n    \n    # Architecture\n    selected_model = \"bert-base-uncased\"\n    lowercase = True\n    \n    # Loss function\n    loss_config = {\n        \"smoothing\": False,\n        \"eps\": 0.1,\n    }\n    \n    # Training\n    batch_size = 16\n    batch_size_val = batch_size * 2\n    weight_decay = 1.\n    \n    epochs = 5\n    lr = 5e-5\n    warmup_prop = 0.1","86986121":"# df = pd.read_csv(\"df_train.csv\")\n\nk_fold(\n    Config,\n    df,\n    save=True,\n)","3d8f115b":"## Prepare For create_data","a4127795":"## Pseudo","fe99fb6b":"## Preds from probas","87a48819":"# Utils","04c0de18":"## Locating labels","6e9f20b3":"### Dataset","91f86f2c":"## Process sample","933cc554":"# Training a Model","d9df2672":"A lot of sections have no label found, I won't use them for training.","5fbeaf61":"## Tokenizer","89c40224":"## Train","e98d2680":"## Train + Pseudo","d9ca23c1":"## Imports","d9d253dd":"## Loss","da4e8ddb":"## Fit function","97a855f4":"## Trim tensors\n\nHelps for speedup","148f28f2":"## Train + Pseudo + External","76ca9296":"## Params","da5dbefb":"## Metric","2167e259":"## External Data","6567f4b1":"## Main","792e2805":"## $k$-fold","210d7d74":"## Model","29aa4d1b":"# Initialization","c94a067f":"# \ud83e\udd17 Bert for Question Answering Baseline: Training\n\nThis code is adapted from my work in the Tweet Sentiment Extraction Competition.\n\nIt tackles the task as a Question Answering one, where the question is implicit and can be understood as : \"Which datasets are mentionned ?\"\n\n\nThe approach is quite na\u00efve and has a lot of flaws. Feel free to ask any question in the comments.\n\nInference Kernel : https:\/\/www.kaggle.com\/theoviel\/bert-for-question-answering-baseline-inference","3e271f6e":"# Data","31748544":"# Data Preparation\nInstead of having labels at article level, I refine the definition to section level. This allows for less sparsity. "}}