{"cell_type":{"f675dd35":"code","7111eaa9":"code","89f37ad6":"code","f5073769":"code","50da1548":"code","68ce14b7":"code","68f866c8":"code","f5f41426":"code","3e14f665":"code","c9cc74df":"code","8a6d0ee8":"code","e12929f1":"code","7169ca35":"code","6f1a7e21":"code","310700e7":"code","c84e8438":"code","35d0c70a":"code","b35d8404":"code","9642c863":"markdown","53e0d872":"markdown","4d40c1f4":"markdown","86977e2d":"markdown","b598c3cb":"markdown"},"source":{"f675dd35":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\nimport os.path\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image, display\nimport matplotlib.cm as cm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nimport tensorflow as tf\nimport inspect","7111eaa9":"# Reading the data (this part of the could is retrieved from DATALIRA's notebook (https:\/\/www.kaggle.com\/databeru\/fish-classifier-grad-cam-viz-acc-99-89)\nimage_dir = Path('..\/input\/a-large-scale-fish-dataset\/Fish_Dataset\/Fish_Dataset')\n\n# Get filepaths and labels\nfilepaths = list(image_dir.glob(r'**\/*.png'))\nlabels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1], filepaths))\n\nfilepaths = pd.Series(filepaths, name='Filepath').astype(str)\nlabels = pd.Series(labels, name='Label')\n\n# Concatenate filepaths and labels\nimage_df = pd.concat([filepaths, labels], axis=1)\n\n# Drop GT images\nimage_df = image_df[image_df['Label'].apply(lambda x: x[-2:] != 'GT')]\n\n#Resampling it\nimage_df = image_df.sample(frac = 1).reset_index(drop=True)\nimage_df.head()","89f37ad6":"# Plotting some of the images in the data for illustration purposes\nNUM_OF_ILLUSTRATION = 16\n\n#\nplt.figure(figsize=(20, 20))\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16,\n        }\n\nfor i in range(NUM_OF_ILLUSTRATION):\n    plt.subplot(4, 4, i + 1)\n    plt.title(image_df.Label[i], fontdict=font)\n    plt.imshow(plt.imread(image_df.Filepath[i]))","f5073769":"# Splitting data into train, validation, and test sets\nTRAIN_RATIO = 0.8\nVAL_RATIO = 0.10\nTEST_RATIO = 1-TRAIN_RATIO-VAL_RATIO\nIMG_SIZE = (200, 200)\n\n\ntrain_data, test_data = train_test_split(image_df, test_size = 1 - TRAIN_RATIO, random_state = 42)\nval_data, test_data = train_test_split(test_data, test_size=TEST_RATIO\/(TEST_RATIO + VAL_RATIO), random_state = 42)\n\nprint(train_data.shape)\nprint(test_data.shape)","50da1548":"# Load the images using ImageDataGenerator in Keras\ntrain_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.\/255,\n                                                                       rotation_range=40,\n                                                                       width_shift_range=0.2,\n                                                                       height_shift_range=0.2,\n                                                                       shear_range=0.2,\n                                                                       zoom_range=0.2,\n                                                                       horizontal_flip=True,\n                                                                       fill_mode='nearest',\n                                                                       validation_split=0.2)\n\n'''''\ntest_data_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1.\/255,\n                                                                      rotation_range=40,\n                                                                      width_shift_range=0.2,\n                                                                      height_shift_range=0.2,\n                                                                      shear_range=0.2,\n                                                                      zoom_range=0.2,\n                                                                      horizontal_flip=True,\n                                                                      fill_mode='nearest')\n'''''\n\n#\nx_train = train_data_generator.flow_from_dataframe(dataframe = train_data, \n                                                   x_col='Filepath', \n                                                   y_col='Label', \n                                                   target_size=IMG_SIZE, \n                                                   color_mode='rgb',\n                                                   class_mode='categorical',\n                                                   shuffle=False)\n\nx_val = train_data_generator.flow_from_dataframe(dataframe = val_data,\n                                                 x_col='Filepath',\n                                                 y_col='Label',\n                                                 target_size=IMG_SIZE,\n                                                 color_mode='rgb',\n                                                 class_mode='categorical',\n                                                 shuffle=False)\n\nx_test = train_data_generator.flow_from_dataframe(dataframe = test_data,\n                                                  x_col='Filepath',\n                                                  y_col='Label',\n                                                  target_size=IMG_SIZE,\n                                                  color_mode='rgb',\n                                                  class_mode='categorical',\n                                                  shuffle=False)","68ce14b7":"# Defining some of the hyperparameters in the CNN model\nINPUT_SHAPE = (200, 200, 3)\n\nNUM_CONV_LAYERS = 4\nNUM_FILTERS = [128, 64, 32, 16]\nCONV_AF = 'relu'\n\nNUM_DENSE_LAYERS = 4\nNUM_NEURONS = [512, 256, 128, 64]\nDENSE_AF = 'relu'","68f866c8":"# CNN configuration\ninput_layer = tf.keras.layers.Input(shape=(INPUT_SHAPE), name='input')\n\nx = tf.keras.layers.Conv2D(filters=NUM_FILTERS[0], kernel_size=(3, 3), activation=CONV_AF, name='conv_1')(input_layer)\nx = tf.keras.layers.MaxPool2D(pool_size=(2, 2), name= 'max_pooling2d_1')(x)\n\nfor i in range(1, NUM_CONV_LAYERS):\n    x = tf.keras.layers.Conv2D(filters=NUM_FILTERS[i], kernel_size=(3, 3), activation=CONV_AF,\n                               name='conv_'+str(i+1))(x)\n    x = tf.keras.layers.MaxPool2D(pool_size=(2, 2), name='max_pooling2d_'+str(i+1))(x)\n    \nx = tf.keras.layers.Flatten()(x)\n\nfor i in range(NUM_DENSE_LAYERS):\n    x = tf.keras.layers.Dense(units = NUM_NEURONS[i], activation=DENSE_AF, name='dense_'+str(i+1))(x)\n    \noutput_layer = tf.keras.layers.Dense(units = 9, activation='softmax', name='output')(x)\n\nmodel = tf.keras.Model(inputs=input_layer, outputs=output_layer)\nprint(model.summary())\n\n#\noptimizer = tf.keras.optimizers.Adam()\n\n#\nmodel.compile(optimizer=optimizer,\n              loss='categorical_crossentropy',\n              metrics=[\"accuracy\"])\n\n#\ntf.keras.utils.plot_model(model)\n\n# train the model\nearlyStopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                          min_delta=0,\n                                                          patience=3,\n                                                          verbose=1)\n\nhistory = model.fit(x=x_train, validation_data= x_val, epochs = 10, callbacks=earlyStopping_callback)","f5f41426":"# Summarize the training process\nfig = plt.figure(figsize=(20,7))\nfig.add_subplot(121)\n\n# Accuracy\nplt.plot(history.epoch, history.history['accuracy'], label = \"accuracy\")\nplt.plot(history.epoch, history.history['val_accuracy'], label = \"val_accuracy\")\n\nplt.title(\"Accuracy\", fontsize=18)\nplt.xlabel(\"Epochs\", fontsize=15)\nplt.ylabel(\"Accuracy\", fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\n\n#Adding Subplot 1 (For Loss)\nfig.add_subplot(122)\n\nplt.plot(history.epoch, history.history['loss'], label=\"loss\")\nplt.plot(history.epoch, history.history['val_loss'], label=\"val_loss\")\n\nplt.title(\"Loss\", fontsize=18)\nplt.xlabel(\"Epochs\", fontsize=15)\nplt.ylabel(\"Loss\", fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\nplt.show()","3e14f665":"# (this part of the could is retrieved from DATALIRA's notebook (https:\/\/www.kaggle.com\/databeru\/fish-classifier-grad-cam-viz-acc-99-89)\npred = model.predict(x_test)\npred = np.argmax(pred, axis=1)\n\n# Map the label\nlabels = (x_test.class_indices)\nlabels = dict((v,k) for k,v in labels.items())\npred = [labels[k] for k in pred]\n\n#\ncnn_model_test_acc = model.evaluate(x_test)[1]\ncnn_model_val_acc = model.evaluate(x_val)[1]\n\n#\ny_test = list(test_data.Label)\nprint(classification_report(y_test, pred))","c9cc74df":"# VGG16\npretrained_model_VGG16 = tf.keras.applications.VGG16(\n    input_shape=INPUT_SHAPE,\n    include_top=False)\n\nfor layer in pretrained_model_VGG16.layers:\n    layer.trainable = False\npretrained_model_VGG16.summary()\nprint('\\n\\n ---------------------------------------------------------------------------------------- \\n\\n')\n\n# ResNet50\npretrained_model_ResNet50 = tf.keras.applications.ResNet50V2(\n    input_shape=INPUT_SHAPE,\n    include_top=False)\n\nfor layer in pretrained_model_ResNet50.layers:\n    layer.trainable = False\npretrained_model_ResNet50.summary()\nprint('\\n\\n ---------------------------------------------------------------------------------------- \\n\\n')\n\n\n# Inceptionv3\npretrained_model_Inceptionv3 = tf.keras.applications.InceptionV3(\n    input_shape=INPUT_SHAPE,\n    include_top=False)\n\nfor layer in pretrained_model_Inceptionv3.layers:\n    layer.trainable = False\npretrained_model_Inceptionv3.summary()\nprint('\\n\\n ---------------------------------------------------------------------------------------- \\n\\n')\n\n\n# Xception\npretrained_model_Xception = tf.keras.applications.Xception(\n    input_shape=INPUT_SHAPE,\n    include_top=False)\n\nfor layer in pretrained_model_Xception.layers:\n    layer.trainable = False\npretrained_model_Xception.summary()\nprint('\\n\\n ---------------------------------------------------------------------------------------- \\n\\n')\n\n\n# MobileNetV2\npretrained_model_MobileNetV2 = tf.keras.applications.MobileNetV2(\n    input_shape=INPUT_SHAPE,\n    include_top=False,\n    pooling='avg')\n\nfor layer in pretrained_model_MobileNetV2.layers:\n    layer.trainable = False\npretrained_model_MobileNetV2.summary()\nprint('\\n\\n ---------------------------------------------------------------------------------------- \\n\\n')","8a6d0ee8":"model_df = pd.DataFrame({'model_name': ['VGG16', 'ResNet50', 'Inceptionv3', 'Xception', 'MobileNetV2'],\n                         'model': [pretrained_model_VGG16, pretrained_model_ResNet50, \n                                    pretrained_model_Inceptionv3, pretrained_model_Xception, \n                                    pretrained_model_MobileNetV2],\n                         'val_acc': [0.0, 0.0, 0.0, 0.0, 0.0], \n                         'test_acc': [0.0, 0.0, 0.0, 0.0, 0.0]})\nmodel_df","e12929f1":"NUM_DENSE_LAYERS = 4\nNUM_NEURONS = [512, 256, 128, 64]\nDENSE_AF = 'relu'","7169ca35":"# to keep track of the history of different models\nhistory_dict = []","6f1a7e21":"for i in range(model_df.shape[0]):\n    print('------------------------------------------------------------')\n    print(model_df['model_name'][i])\n    print('------------------------------------------------------------')\n    \n    pretrained_model = model_df['model'][i]\n    last_output_of_the_pretrained_model = pretrained_model.layers[-1].output\n    x = tf.keras.layers.Flatten()(last_output_of_the_pretrained_model)\n\n    for j in range(NUM_DENSE_LAYERS):\n        x = tf.keras.layers.Dense(units = NUM_NEURONS[j], activation=DENSE_AF, name='dense_'+str(j+1))(x)\n\n    output_layer = tf.keras.layers.Dense(units = 9, activation='softmax', name='output')(x)\n\n    model_temp = tf.keras.Model(inputs=pretrained_model.input, outputs=output_layer)\n    \n    #\n    optimizer = tf.keras.optimizers.Adam()\n\n    #\n    model_temp.compile(optimizer=optimizer,\n                  loss='categorical_crossentropy',\n                  metrics=[\"categorical_accuracy\", \"accuracy\"])\n\n    # train the model\n    earlyStopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n                                                              min_delta=0,\n                                                              patience=3,\n                                                              verbose=1) \n\n    history = model_temp.fit(x=x_train, validation_data= x_val, epochs = 10, callbacks=earlyStopping_callback)\n    \n    test_acc = model_temp.evaluate(x_test)[1]\n    val_acc = model_temp.evaluate(x_val)[1]\n    \n    model_df['test_acc'][i] = test_acc\n    model_df['val_acc'][i] = val_acc\n    \n    #\n    history_dict.append(history)","310700e7":"print('Best Model:\\n')\nmodel_df.sort_values(by=[\"test_acc\"], ascending=False).iloc[0, :]","c84e8438":"model_df.sort_values(by=[\"test_acc\"], ascending=False)","35d0c70a":"#\nprint('---------------------------------------')\nprint('Pre-trained Model Histories')\nprint('---------------------------------------\\n')\n\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16,\n        }\n\nfor i,j,k in [(0, 1, 2), (1, 3, 4), (2, 5, 6), (3, 7, 8), (4, 9, 10)]:\n    history_temp = history_dict[i]\n    \n    plt.figure(figsize=(10, 20))\n    plt.subplot(5, 2, j)\n\n    # Accuracy\n    plt.plot(history_temp.epoch, history_temp.history['accuracy'], label = \"accuracy\")\n    plt.plot(history_temp.epoch, history_temp.history['val_accuracy'], label = \"val_accuracy\")\n\n    plt.title(\"Accuracy_\"+model_df['model_name'][i], fontsize=18)\n    plt.xlabel(\"Epochs\", fontsize=15)\n    plt.ylabel(\"Accuracy\", fontsize=15)\n    plt.grid(alpha=0.3)\n    plt.legend()\n\n\n    #Adding Subplot 1 (For Loss)\n    plt.subplot(5, 2, k)\n\n    plt.plot(history_temp.epoch, history_temp.history['loss'], label=\"loss\")\n    plt.plot(history_temp.epoch, history_temp.history['val_loss'], label=\"val_loss\")\n\n    plt.title(\"Loss_\"+model_df['model_name'][i], fontsize=18)\n    plt.xlabel(\"Epochs\", fontsize=15)\n    plt.ylabel(\"Loss\", fontsize=15)\n    plt.grid(alpha=0.3)\n    plt.legend()\n\n    plt.show()","b35d8404":"print(f'{model_df.sort_values(by=[\"test_acc\"], ascending=False).iloc[0, 0]} performed better among the pre-trained models with test accuracy equal to {round(model_df.sort_values(by=[\"test_acc\"], ascending=False).iloc[0, 2], 4)}. Our CNN had test accuracy equal to {round(cnn_model_test_acc, 4)}.')\n\nif cnn_model_test_acc>model_df.sort_values(by=[\"test_acc\"], ascending=False).iloc[0, 2]:\n    print('Therefore CNN has performed better.')\nelse:\n    print(f'Therefore {model_df.sort_values(by=[\"test_acc\"], ascending=False).iloc[0, 0]} has performed better.')","9642c863":"**Convolutional Neural Network (no pretrained model)**\n\nBy changing the following hyperparameters, you can have different configurations.\n","53e0d872":"ImageDataGenerator parameters (retrieved from https:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/image-augmentation-on-the-fly-using-keras-imagedatagenerator\/ and https:\/\/stackoverflow.com\/questions\/57301330\/what-exactly-the-shear-do-in-imagedatagenerator-of-keras):\n\n1. Random Rotations --> \"**rotation_range**\" and \"**fill_mode**\" to randomly rotate images through any degree between 0 and 360\n\nWhen the image is rotated, some pixels will move outside the image and leave an empty area that needs to be filled in. You can fill this in different ways like a constant value or nearest pixel values, etc. This is specified in the \"fill_mode\" argument and the default value is \u201cnearest\u201d which simply replaces the empty area with the nearest pixel values.\n\n\n2. Random Shifts --> \"**width_shift_range**\" and \"**height_shift_range**\" to shift the pixels of the image either horizontally or vertically\n \n\n3. Random Flips --> \"**horizontal_flip**\" and \"**vertical_flip**\"  for flipping along the vertical or the horizontal axis.\n\n\n4. Random Brightness --> \"**brightness_range**\" to randomly changes the brightness of the image\n\nIt accepts a list of two float values and picks a brightness shift value from that range. Values less than 1.0 darkens the image, whereas values above 1.0 brighten the image.\n\n\n5. Random Zoom --> \"**zoom_range**\" to zooms in on the image or zooms out of the image\n\nYou could provide a list with two values specifying the lower and the upper limit. Else, if you specify a float value, then zoom will be done in the range [1-zoom_range,1+zoom_range].\n\n\n\n6. Shear transformation --> \"**shear_range**\" to shear the image\n\n'Shear' means that the image will be distorted along an axis, mostly to create or rectify the perception angles. It's usually used to augment images so that computers can see how humans see things from different angles. ","4d40c1f4":"By changing the following hyperparameters, you can have different configurations in the classification part of the CNN structure (the fully connected layers part). I've used the same structure as I did in the previous CNN part without pre-trained models.","86977e2d":"**Best pre-trained model**","b598c3cb":"**Comparing different pretrained models available in TensorFlow**"}}