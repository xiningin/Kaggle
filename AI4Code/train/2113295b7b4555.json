{"cell_type":{"68a27f9a":"code","54aa62c4":"code","0a543b7d":"code","4e69e367":"code","1c33adb4":"code","4b33b405":"code","b980d5cc":"code","4f31676f":"code","5af06f5e":"code","7deff605":"code","da07ea37":"code","27ee28a7":"code","df3a3422":"code","36b69fac":"code","812c62e6":"code","39de6724":"code","5557d4f9":"code","be4eae56":"code","9935ee89":"code","2c2c3be1":"code","f491cf37":"code","dbcbc7c1":"code","5711e9f3":"code","210aff95":"code","bf5f9292":"code","fde7861a":"code","88d757c0":"markdown","a854866e":"markdown","a97b2250":"markdown","ddafa377":"markdown","f09f8352":"markdown","cf60ebe1":"markdown","78d0f9ae":"markdown","bd9731c7":"markdown","8de3759c":"markdown","060d5921":"markdown","309f33f4":"markdown","314555a7":"markdown","f369b103":"markdown","cf8d2f1f":"markdown","03f9b7d0":"markdown","7d9a638f":"markdown","343b6ace":"markdown"},"source":{"68a27f9a":"### Portion of imports (1)\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2, os\nimport skimage.io as io","54aa62c4":"DATA_labels='..\/input\/dataset-bms-mol-v3\/train_labels.csv'\n## only this number of rows for testing purposes\nnrows1=1000\n#### loading the data ---\ndata = pd.read_csv(DATA_labels) #,nrows=nrows1\n\n# Let's add the path of the images to de DF...\ndata['path'] = data['image_id'].progress_apply(\n    lambda x: \"..\/input\/dataset-bms-mol-v3\/train\/{}\/{}\/{}\/{}.png\".format(\n        x[0], x[1], x[2], x))\n\n### Prepare to create only 2 Parts InChI --- Later adding more --\nlabel_splited2 = data['InChI'].progress_apply(lambda x: x.split('\/'))\n# Data Frame with label parts\nlabel_parts2 = pd.DataFrame.from_records(label_splited2.values)\nlabel_parts2.columns = np.array(label_parts2.columns + 1)\nlabel_parts2 = label_parts2.add_prefix('Part_')\n## important to add\nlabel_parts2['path'] = data['path']\n\n######\n###### Study only part2 for now ...\nlabel_parts2b = label_parts2.sample(nrows1)\nlabel_parts2b['InChI2p']= label_parts2b['Part_2']","0a543b7d":"Parts2dropMax=max([len(i) for i in label_splited2])+1 # for the InChI=1S\/\n#Parts2dropMax","4e69e367":"Parts2drop=['Part_'+str(i) for i in range(1,Parts2dropMax)]\nlabel_parts2b.drop(columns=Parts2drop,inplace=True)\nlabel_parts2b = label_parts2b.reset_index()","1c33adb4":"# Building the columns of the target from the constituent atoms\ndef split_atom(formula):\n    atom_dict = {atom: 0 for atom in atom_list}\n    now_atom = \"\"\n    now_num = \"\"\n    total_atom = 0\n    for char in formula+\"E\":\n        if char.isupper():\n            if now_atom != \"\":\n                if now_num == \"\":\n                    atom_dict[now_atom] = int(1)\n                    total_atom += 1\n                    now_atom = char\n                else:\n                    atom_dict[now_atom] = int(now_num)\n                    total_atom += int(now_num)\n                    now_atom = char\n                    now_num = \"\"\n            else:\n                now_atom = char\n        elif char.islower():\n            now_atom += char\n        else:\n            if now_atom != \"\":\n                now_num += char\n    atom_dict[\"total\"] = total_atom\n    return atom_dict","4b33b405":"import re\nimport itertools\n            \natom_list = [\"C\", \"H\", \"B\", \"Br\", \"Cl\", \"F\", \"I\", \"N\", \"O\", \"P\", \"S\", \"Si\", \"total\"]                \nprint(\"Lets include the following atoms in the training%s\" % set(atom_list))","b980d5cc":"atom_num_list = [split_atom(inchi) for inchi in tqdm(label_parts2b['InChI2p'])]\n#atom_num_list","4f31676f":"### Ensembling ..\ntrain_sampleb = pd.concat([label_parts2b, pd.DataFrame(atom_num_list)], axis=1)\ntrain_sampleb.head()","5af06f5e":"atom_list = [\"C\", \"H\", \"B\", \"Br\", \"Cl\", \"F\", \"I\", \"N\", \"O\", \"P\", \"S\", \"Si\"]\ntrain_sample = train_sampleb[['path', *atom_list]]\ntrain_sample.head()","7deff605":"import tensorflow as tf\nimport keras\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.experimental import CosineDecay\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.models import Sequential, load_model\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Conv2D, BatchNormalization, MaxPool2D\nimport Levenshtein","da07ea37":"def train_trans_func(image):\n    return image \/ 255.\n\ndef val_trans_func(image):\n    return image \/ 255.\n\ndatagen_train = ImageDataGenerator(preprocessing_function = train_trans_func)\ndatagen_val = ImageDataGenerator(preprocessing_function = val_trans_func)","27ee28a7":"exploreSize = train_sample.copy()\nexploreSize['img_tensor'] = train_sample['path'].progress_apply(lambda x: io.imread(x))\n# Let's exlore the dimensions of images to take an \"average number...\"\nexploreSize['img_height'] = exploreSize['img_tensor'].progress_apply(lambda x: np.shape(x)[0])\nexploreSize['img_width'] = exploreSize['img_tensor'].progress_apply(lambda x: np.shape(x)[1])","df3a3422":"sns.jointplot(x = exploreSize['img_width'].astype('float32'), \n              y = exploreSize['img_height'].astype('float32'),\n              height = 4, color = '#930077')\nplt.show()","36b69fac":"img_size=380\nclass_mode = \"raw\"\ninterpolation = \"nearest\"\nshuffle = False ## I want the map to be \"untouchable\"for now...\ncolor_mode = \"grayscale\"\nbatch_size = 32\nler_rat=0.001\n    \ndef create_train_set(train):\n    train_set = datagen_train.flow_from_dataframe(train,\n                                                  directory = None,\n                                                  seed = 12345,\n                                                  x_col = \"path\",\n                                                  y_col = atom_list,\n                                                  target_size = (img_size, img_size),\n                                                  class_mode = class_mode,\n                                                  interpolation = interpolation,\n                                                  shuffle = shuffle,\n                                                  color_mode = color_mode,\n                                                  batch_size = batch_size)\n    return train_set\n    \ndef create_val_set(val):\n    val_set = datagen_val.flow_from_dataframe(val,\n                                              directory = None,\n                                              seed=12345,\n                                              x_col = \"path\",\n                                              y_col = atom_list,\n                                              target_size = (img_size, img_size),\n                                              class_mode = class_mode,\n                                              interpolation = interpolation,\n                                              shuffle = shuffle,\n                                              color_mode = color_mode,\n                                              batch_size = batch_size)\n    return val_set","812c62e6":"from sklearn.model_selection import train_test_split\n\ntrain, val = train_test_split(train_sample, test_size=0.2, random_state=12345)\nvalid_set = create_val_set(val)\ntrain_set = create_train_set(train)","39de6724":"n_CLASS=len(atom_list)\nn_CLASS","5557d4f9":"def create_model():\n    model = Sequential()\n    model.add(Conv2D(16, 3, activation=\"relu\", padding=\"same\", input_shape=(img_size, img_size, 1)))\n    model.add(Conv2D(16, 3, activation=\"relu\", padding=\"same\"))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"valid\"))\n    model.add(Conv2D(32, 3, activation=\"relu\", padding=\"same\"))\n    model.add(Conv2D(32, 3, activation=\"relu\", padding=\"same\"))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"valid\"))\n    model.add(Conv2D(64, 3, activation=\"relu\", padding=\"same\"))\n    model.add(Conv2D(64, 3, activation=\"relu\", padding=\"same\"))\n    model.add(BatchNormalization())\n    model.add(MaxPool2D(pool_size=(2, 2), strides=None, padding=\"valid\"))\n    model.add(Conv2D(64, 3, activation=\"relu\", padding=\"same\"))\n    model.add(Conv2D(64, 3, activation=\"relu\", padding=\"same\"))\n    model.add(BatchNormalization())\n    model.add(GlobalAveragePooling2D())\n    model.add(Dense(n_CLASS, activation=\"relu\"))\n    return model\n\nmodel = create_model()\nmodel.summary()","be4eae56":"step_size_train = train_set.n \/\/ train_set.batch_size\nstep_size_valid = valid_set.n \/\/ valid_set.batch_size\nprint(step_size_train,step_size_valid)","9935ee89":"from tensorflow.keras.callbacks import EarlyStopping\n\nearly_stopping = EarlyStopping(\n    patience=10,\n    min_delta=0.01,\n    restore_best_weights=True,\n)","2c2c3be1":"epochs=20\nmodel = create_model()\n\n\nmodel.compile(optimizer = Adam(learning_rate= ler_rat),\n              loss='MAE',\n              metrics=['MAE'])\n\n\n## Training --\nhistory = model.fit(train_set,\n                    validation_data = valid_set,\n                    epochs = epochs,\n                    batch_size = batch_size,\n                    steps_per_epoch = step_size_train,\n                    validation_steps = step_size_valid,\n                    callbacks=[early_stopping])","f491cf37":"history_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss','val_loss']].plot(title=\"MAE\") #","dbcbc7c1":"def image_viz(image, title, figsize=(12,6)):\n    \"\"\"\n    Function for image visualization.\n    Takes image tensor, plot title (label) and figsize.\n    \"\"\"\n    plt.figure(figsize = figsize)\n    #image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    plt.imshow(image)\n    plt.title(title, size = 16)\n    plt.axis('off')\n    plt.show()\n    \n# ref: https:\/\/www.kaggle.com\/ihelon\/molecular-translation-exploratory-data-analysis \ndef convert_image_id_2_path(image_id: str, convert) -> str:\n    return \"..\/input\/dataset-bms-mol-v3\/\"+convert+\"\/{}\/{}\/{}\/{}.png\".format(\n        image_id[0], image_id[1], image_id[2], image_id \n    )\n\n#ref: https:\/\/www.kaggle.com\/ihelon\/molecular-translation-exploratory-data-analysis\ndef visualize_image(image_id, label, convert, ipath):\n    plt.figure(figsize=(8, 8))\n    print(image_id)\n    if ipath==1:\n        image = cv2.imread(convert_image_id_2_path(image_id, convert))\n    else:\n        image=cv2.imread(image_id)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    plt.imshow(image)\n    plt.title(f\"{label}\", fontsize=14)\n    plt.axis(\"off\")\n    \n    plt.show()","5711e9f3":"sample_row = data.iloc[0]\nvisualize_image(\n        sample_row[\"image_id\"], sample_row[\"InChI\"], 'train',1\n    )","210aff95":"def arr2formula(atom_arr):\n    formula = \"\"\n    for atom, num in zip(atom_list, atom_arr):\n        if num > 1:\n            formula += \"%s%s\" % (atom, int(num))\n        elif num == 1:\n            formula += atom\n    return formula\n\nlsval = 0.\n\nfor i, (test, formula_list) in enumerate(tqdm(train_set)):\n    if i >= len(train_set):\n        break\n    predicts = np.round(model.predict(test))\n    for predict, formula in zip(predicts, formula_list):\n        predict_formula = arr2formula(predict)\n        true_formula = arr2formula(formula)\n        if i < 10:\n            print(\"Predicted, = %s, true = %s\" % (predict_formula, true_formula))\n        lsval += Levenshtein.distance(predict_formula, true_formula) \/ (nrows1 * (1 - 0.1))\n\nprint(\"The Levenshtein distance of train data is %s\" % lsval)","bf5f9292":"## taking only one batch --\nfor i, (test, formula_list) in enumerate(tqdm(train_set)):\n    if i >= 1:\n        break\n    Mypredict1=test\n    Myformula_list1=formula_list","fde7861a":"idx=10\nimage_viz(Mypredict1[idx], arr2formula(Myformula_list1[idx]))\n#####\nsample_row = train.iloc[idx]\nvisualize_image(\n        sample_row[\"path\"], arr2formula(np.array([i for i in sample_row[atom_list]])), 'train',0\n    )","88d757c0":"<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n\n# Data exploration:\n    \n<\/div>","a854866e":"<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n\n## Visualizing\n    \n<\/div>","a97b2250":"## The generator","ddafa377":"# Very new preprocessing (v1.0)","f09f8352":"## References:\n\n1) https:\/\/www.kaggle.com\/wineplanetary\/understanding-inchi-format-and-arrange-train-label","cf60ebe1":"***","78d0f9ae":"# BMS competition\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/e7\/L-Ascorbic_acid.svg)\n\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#42c497;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 10px;\n              color:white;\">\n    \nIn a technology-forward world, sometimes the best and easiest tools are still pen and paper. Organic chemists frequently draw out molecular work with the Skeletal formula, a structural notation used for centuries. Recent publications are also annotated with machine-readable chemical descriptions (InChI), but there are decades of scanned documents that can't be automatically searched for specific chemical depictions. Automated recognition of optical chemical structures, with the help of machine learning, could speed up research and development efforts.\n\nUnfortunately, most public data sets are too small to support modern machine learning models. Existing tools produce 90% accuracy but only under optimal conditions. Historical sources often have some level of image corruption, which reduces performance to near zero. In these cases, time-consuming, manual work is required to reliably convert scanned chemical structure images into a machine-readable format.\n\nBristol-Myers Squibb is a global biopharmaceutical company working to transform patients' lives through science. Their mission is to discover, develop, and deliver innovative medicines that help patients prevail over serious diseases.\n    \n<\/p>\n<\/div>","bd9731c7":"<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n\n# Using CNN techniques\n    \n<\/div>","8de3759c":"# Main code -- Generation of matrices:","060d5921":"<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n\n## The model\n    \n<\/div>","309f33f4":"<h5 style=\"text-align: center; font-family: Verdana; font-size: 12px; font-style: normal; font-weight: bold; text-decoration: None; text-transform: none; letter-spacing: 1px; color: black; background-color: #ffffff;\">CREATED BY: Jos\u00e9 Manuel Ram\u00edrez <\/h5>","314555a7":"### Remove the \"random_state\" in the test splitting, make sure they are the same, and then return and turn the shuffle on again...","f369b103":"<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n\n### Expand the formula \n\n<\/div>","cf8d2f1f":"<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \nThis is a modest beginning for the exploration of the BMs data. Use of GPUs is desirable.\n\n<\/div>","03f9b7d0":"<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n\n# This is the prediction --- work in progress..\n    \n<\/div>","7d9a638f":"<div class=\"alert alert-block alert-info\" style=\"font-size:24px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc &nbsp; \n\n### making sure\n    \n<\/div>","343b6ace":"# Preprocessing data:"}}