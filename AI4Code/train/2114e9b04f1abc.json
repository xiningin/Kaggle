{"cell_type":{"ad6e40f8":"code","9171a44f":"code","0bbd7304":"code","86928cc8":"code","8b09dc55":"code","2aa56e97":"code","32f993b0":"code","a084f120":"code","123e3784":"code","2cb474bd":"code","b43228ba":"code","31c3707a":"code","a15d7550":"code","06742ad3":"code","66d2426f":"code","dcc6eeec":"code","b4942689":"code","631e458a":"code","f5f0b4d5":"code","e2d756de":"code","a201c2cc":"code","9fdbba11":"code","9b160e66":"code","32da5038":"code","dbf51380":"code","48c8fe87":"markdown","ff1a753a":"markdown","58b37b96":"markdown","aa9c2719":"markdown","1eacbaee":"markdown","ec3b8515":"markdown","aa327902":"markdown","9544a901":"markdown","e52ac980":"markdown","9d5bbf45":"markdown","ecca0c78":"markdown","cfa30759":"markdown","3c3e36a8":"markdown","f3d116cc":"markdown","715400ab":"markdown","797f5652":"markdown","392edba8":"markdown","a097261b":"markdown","b85974cf":"markdown","91fe7529":"markdown","1f22b11b":"markdown","bfa3fbb5":"markdown","c471dc1e":"markdown","be9e4c88":"markdown"},"source":{"ad6e40f8":"#below code are used directly on the dataset we haven't applied smote for imbalanced data\n#becoz the dataset is hugely imbalanced\n\nimport pandas as pd\ntrain_df = pd.read_csv(\"\/kaggle\/input\/creditcard.csv\")\nX = train_df.drop(columns={'Class'})\ny = train_df['Class']\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\ny_test = y_test.ravel()\ny_train = y_train.ravel()","9171a44f":"X.info()","0bbd7304":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(20,14))\ncorr = X.corr()\nsns.heatmap(corr)","86928cc8":"# fitting logistic regression to the training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train, y_train)\n# predictiing the test result\ny_pred = classifier.predict(X_test)\n# making the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nimport seaborn as sns\nsns.heatmap(cm, annot=True)\n#find accuracy\nfrom sklearn.metrics import accuracy_score\nprint('logistic regression:',accuracy_score(y_test,y_pred))\n# find classification report\nfrom sklearn.metrics import f1_score , precision_score , recall_score\nprint('f1_score:',f1_score(y_test,y_pred))\nprint('precision_score:',precision_score(y_test,y_pred))\nprint('recall_score:',recall_score(y_test,y_pred))","8b09dc55":"\n# Fitting naive byes classifier to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train,y_train)\n# Predicting the Test set results\ny_pred2 = classifier.predict(X_test)\n# making the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm2 = confusion_matrix(y_test, y_pred2)\nimport seaborn as sns\nsns.heatmap(cm2, annot=True)\n#find accuracy\nfrom sklearn.metrics import accuracy_score\nprint('naive byes:',accuracy_score(y_test,y_pred2))\n# find classification report\nfrom sklearn.metrics import f1_score , precision_score , recall_score\nprint('f1_score:',f1_score(y_test,y_pred2))\nprint('precision_score:',precision_score(y_test,y_pred2))\nprint('recall_score:',recall_score(y_test,y_pred2))","2aa56e97":"\n# Fitting Decision Tree Classification to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n# Predicting the Test set results\ny_pred3 = classifier.predict(X_test)\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm3 = confusion_matrix(y_test, y_pred3)\nimport seaborn as sns\nsns.heatmap(cm3, annot=True)\n#find accuracy\nfrom sklearn.metrics import accuracy_score\nprint('decision tree:',accuracy_score(y_test,y_pred3))\n# find classification report\nfrom sklearn.metrics import f1_score , precision_score , recall_score\nprint('f1_score:',f1_score(y_test,y_pred3))\nprint('precision_score:',precision_score(y_test,y_pred3))\nprint('recall_score:',recall_score(y_test,y_pred3))\n\n","32f993b0":"\n# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n# Predicting the Test set results\ny_pred4 = classifier.predict(X_test)\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm4 = confusion_matrix(y_test, y_pred4)\nimport seaborn as sns\nsns.heatmap(cm4, annot=True)\n#find accuracy\nfrom sklearn.metrics import accuracy_score\nprint('random forest:',accuracy_score(y_test,y_pred4))\n# find classification report\nfrom sklearn.metrics import f1_score , precision_score , recall_score\nprint('f1_score:',f1_score(y_test,y_pred4))\nprint('precision_score:',precision_score(y_test,y_pred4))\nprint('recall_score:',recall_score(y_test,y_pred4))","a084f120":"#ANN\n# Importing the Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Initialising the ANN\nclassifier = Sequential()\nclassifier.add(Dense(10, activation = 'relu', input_dim = 30))\nclassifier.add(Dense(10, activation = 'relu'))\nclassifier.add(Dense(1,  activation = 'sigmoid'))\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nclassifier.summary()\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 1000, epochs = 20)\n# Predicting the Test set results\ny_pred5 = classifier.predict(X_test).round()\ny_pred5 = (y_pred5 > 0.5)\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\ncm5 = confusion_matrix(y_test, y_pred5)\nsns.heatmap(cm5, annot=True)\n#find accuracy\nfrom sklearn.metrics import accuracy_score\nprint('ANN:',accuracy_score(y_test,y_pred5))\n# find classification report\nfrom sklearn.metrics import f1_score , precision_score , recall_score\nprint('f1_score:',f1_score(y_test,y_pred5))\nprint('precision_score:',precision_score(y_test,y_pred5))\nprint('recall_score:',recall_score(y_test,y_pred5))","123e3784":"y.value_counts()","2cb474bd":"y.describe()","b43228ba":"fraud = train_df[train_df['Class'] == 1]\nvalid = train_df[train_df['Class'] == 0]\n\nprint(\"Fraud transaction statistics\")\nprint(fraud[\"Amount\"].describe())\nprint(\"\\nNormal transaction statistics\")\nprint(valid[\"Amount\"].describe())","31c3707a":"# describes info about train and test set\nprint(\"X_train dataset: \", X_train.shape)\nprint(\"y_train dataset: \", y_train.shape)\nprint(\"X_test dataset: \", X_test.shape)\nprint(\"y_test dataset: \", y_test.shape)","a15d7550":"print(\"before applying smote:\",format(sum(y_train == 1)))\nprint(\"before applying smote:\",format(sum(y_train == 0)))","06742ad3":"\n# import SMOTE module from imblearn library\n# pip install imblearn if you don't have\nfrom imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state=2)\nX_train, y_train = sm.fit_sample(X_train, y_train)\n\nprint('After applying smote X_train: {}\\n'.format(X_train.shape))\nprint('After applying smote y_train: {}\\n'.format(y_train.shape))\n\nprint(\"After applying smote label '1': {}\\n\".format(sum(y_train == 1)))\nprint(\"After applying smote label '0': {}\\n\".format(sum(y_train == 0)))\n","66d2426f":"# fitting logistic regression to the training set\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=0)\nclassifier.fit(X_train, y_train)\n# predictiing the test result\ny_pred = classifier.predict(X_test)\n# making the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nimport seaborn as sns\nsns.heatmap(cm, annot=True)\n#find accuracy\nfrom sklearn.metrics import accuracy_score\nprint('SMOTE+LR:',accuracy_score(y_test,y_pred))\n# find classification report\nfrom sklearn.metrics import f1_score , precision_score , recall_score , classification_report\nprint('classification_report:',classification_report(y_test,y_pred))\nprint('f1_score:',f1_score(y_test,y_pred))\nprint('precision_score:',precision_score(y_test,y_pred))\nprint('recall_score:',recall_score(y_test,y_pred))\n","dcc6eeec":"# Fitting naive byes classifier to the Training set\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train,y_train)\n# Predicting the Test set results\ny_pred2 = classifier.predict(X_test)\n# making the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm2 = confusion_matrix(y_test, y_pred2)\nimport seaborn as sns\nsns.heatmap(cm2, annot=True)\n#find accuracy\nfrom sklearn.metrics import accuracy_score\nprint('SMOTE+naive byes:',accuracy_score(y_test,y_pred2))\n# find classification report\nfrom sklearn.metrics import f1_score , precision_score , recall_score, classification_report\nprint('classification_report:',classification_report(y_test,y_pred))\nprint('f1_score:',f1_score(y_test,y_pred2))\nprint('precision_score:',precision_score(y_test,y_pred2))\nprint('recall_score:',recall_score(y_test,y_pred2))","b4942689":"\n# Fitting Decision Tree Classification to the Training set\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n# Predicting the Test set results\ny_pred3 = classifier.predict(X_test)\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm3 = confusion_matrix(y_test, y_pred3)\nimport seaborn as sns\nsns.heatmap(cm3, annot=True)\n#find accuracy\nfrom sklearn.metrics import accuracy_score\nprint('SMOTE+decision tree:',accuracy_score(y_test,y_pred3))\n# find classification report\nfrom sklearn.metrics import f1_score , precision_score , recall_score, classification_report\nprint('classification_report:',classification_report(y_test,y_pred))\nprint('f1_score:',f1_score(y_test,y_pred3))\nprint('precision_score:',precision_score(y_test,y_pred3))\nprint('recall_score:',recall_score(y_test,y_pred3))\n\n","631e458a":"\n# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)\n# Predicting the Test set results\ny_pred4 = classifier.predict(X_test)\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm4 = confusion_matrix(y_test, y_pred4)\nimport seaborn as sns\nsns.heatmap(cm4, annot=True)\n#find accuracy\nfrom sklearn.metrics import accuracy_score\nprint('SMOTE+random forest:',accuracy_score(y_test,y_pred4))\n# find classification report\nfrom sklearn.metrics import f1_score , precision_score , recall_score, classification_report\nprint('classification_report:',classification_report(y_test,y_pred))\nprint('f1_score:',f1_score(y_test,y_pred4))\nprint('precision_score:',precision_score(y_test,y_pred4))\nprint('recall_score:',recall_score(y_test,y_pred4))","f5f0b4d5":"#ANN\n# Importing the Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\n# Initialising the ANN\nclassifier = Sequential()\nclassifier.add(Dense(10, activation = 'relu', input_dim = 30))\nclassifier.add(Dense(10, activation = 'relu'))\nclassifier.add(Dense(1,  activation = 'sigmoid'))\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\nclassifier.summary()\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 1000, epochs = 20)\n# Predicting the Test set results\ny_pred5 = classifier.predict(X_test).round()\ny_pred5 = (y_pred5 > 0.5)\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns\ncm5 = confusion_matrix(y_test, y_pred5)\nsns.heatmap(cm5, annot=True)\n#find accuracy\nfrom sklearn.metrics import accuracy_score\nprint('SMOTE+ANN:',accuracy_score(y_test,y_pred5))\n# find classification report\nfrom sklearn.metrics import f1_score , precision_score , recall_score, classification_report\nprint('classification_report:',classification_report(y_test,y_pred))\nprint('f1_score:',f1_score(y_test,y_pred5))\nprint('precision_score:',precision_score(y_test,y_pred5))\nprint('recall_score:',recall_score(y_test,y_pred5))","e2d756de":"X_train = X_train.reshape(X_train.shape[0] , X_train.shape[1],1)\nX_test = X_test.reshape(X_test.shape[0] , X_test.shape[1],1)","a201c2cc":"X_train.shape , X_test.shape","9fdbba11":"import tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\n# Initialising the CNN\nclassifier = tf.keras.models.Sequential()\nclassifier.add(tf.keras.layers.Convolution1D(32 , 2 , activation='relu',input_shape=X_train[0].shape))\nclassifier.add(tf.keras.layers.BatchNormalization())\nclassifier.add(tf.keras.layers.Dropout(0.2))\n\nclassifier.add(tf.keras.layers.Convolution1D(64 , 2 , activation='relu'))\nclassifier.add(tf.keras.layers.BatchNormalization())\nclassifier.add(tf.keras.layers.Dropout(0.2))\n\nclassifier.add(tf.keras.layers.Convolution1D(128 , 2 , activation='relu'))\nclassifier.add(tf.keras.layers.BatchNormalization())\nclassifier.add(tf.keras.layers.Dropout(0.2))\n\nclassifier.add(tf.keras.layers.Flatten())\nclassifier.add(tf.keras.layers.Dense(units=256, activation='relu'))\nclassifier.add(tf.keras.layers.Dense(units=1, activation='sigmoid'))\nclassifier.compile(optimizer=Adam(lr = 0.0001), loss='binary_crossentropy', metrics=['accuracy'])\nclassifier.summary()","9b160e66":"history = classifier.fit(X_train, y_train, batch_size = 100, epochs = 10 , validation_data=(X_test,y_test),verbose=1)","32da5038":"# Predicting the Test set results\ny_pred = classifier.predict(X_test).flatten().round()\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nimport seaborn as sns\nsns.heatmap(cm, annot=True)\n#find accuracy\nfrom sklearn.metrics import accuracy_score\nprint('CNN:',accuracy_score(y_test,y_pred))\n# find classification report\nfrom sklearn.metrics import f1_score , precision_score , recall_score , classification_report\nprint('classification_report:',classification_report(y_test,y_pred))\nprint('f1_score:',f1_score(y_test,y_pred))\nprint('precision_score:',precision_score(y_test,y_pred))\nprint('recall_score:',recall_score(y_test,y_pred))","dbf51380":"import matplotlib.pyplot as plt\ndef plot_curve(history , epoch):\n    epoch_range = range(1 , epoch+1)\n    plt.plot(epoch_range , history.history['accuracy'])\n    plt.plot(epoch_range, history.history['val_accuracy'])\n    plt.title('model accuracy')\n    plt.xlabel('Accuracy')\n    plt.ylabel('Epoch')\n    plt.legend(['Train','Val'])\n    plt.show()\n\n    epoch_range = range(1, epoch + 1)\n    plt.plot(epoch_range, history.history['loss'])\n    plt.plot(epoch_range, history.history['val_loss'])\n    plt.title('model loss')\n    plt.xlabel('Loss')\n    plt.ylabel('Epoch')\n    plt.legend(['Train', 'Val'])\n    plt.show()\n\nplot_curve(history , 10)","48c8fe87":"# **Welcome to my exploration of Credit Card fraud detection!**\n\nI will implement technique an technique called SMOTE, supervised models, semi supervised learning algorithms and a deep learning model.","ff1a753a":"# 4)random forest after applying smote","58b37b96":"# **BEFORE APPLYING SMOTE**","aa9c2719":"# Convolutional neural network","1eacbaee":"# 1)logistic regression","ec3b8515":"Please, if you think that I can do anything in a better way,feedback will be appreciated.","aa327902":"# **AFTER APPLYING SMOTE**","9544a901":"# 5)ANN","e52ac980":"# 3)Decision tree after applying smote","9d5bbf45":"# 4)random forest","ecca0c78":"# 5)ANN after applying smote","cfa30759":"# **Please if you have any suggestion to improve my models, let me know!**","3c3e36a8":"In Machine Learning and Data Science we often come across a term called Imbalanced Data Distribution, it happens when observations in one of the class are much higher or lower than the other classes.examples such as Fraud Detection, Anomaly Detection etc.","f3d116cc":"there are mainly two methods which are useful in the imbalance dataset to convert it in balance dataset\nso here we are using smote which stands for **SMOTE (synthetic minority oversampling technique)**.It aims to balance class distribution by randomly replicating the minority class and make dataset shape same for example if i have two classes 1 and 2 if 2 is minority class then smote will randomly replicate the data and make them same like  1 = 10000 and 2 = 200 then after applying smote the both classes will have same values 1 = 10000 and 2 = 10000.","715400ab":"# 2)naive bayes","797f5652":"# **CNN**","392edba8":"# 3)Decision tree","a097261b":"# 2)naive bayes after applying smote","b85974cf":"if you want to learn more then below link is great. it includes how to use smote on regression and also on classification\n\n**https:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/**","91fe7529":"now i am applying smote for balance the dataset you can see the difference in classification report.","1f22b11b":"**now i am applying convolutional neural network for which we have to apply a function called reshape**","bfa3fbb5":"# 1)logistic regressoin after applying smote","c471dc1e":"We have a clearly imbalanced data.\nIt's very common when treating of frauds.","be9e4c88":"without applying smote which is use for balance the dataset"}}