{"cell_type":{"8ad1250f":"code","04b466ec":"code","230e2060":"code","b1640bb4":"code","43b932df":"code","6855855e":"code","d5ca38aa":"code","d9c847b0":"code","70f1e225":"code","183f9714":"code","8bd40b26":"code","2ba1faad":"code","2a9688aa":"code","73d755d6":"code","a2fda03b":"code","80f2349b":"code","30705d6b":"code","0c89d420":"code","d9f73dcb":"code","83add8ee":"code","c2e225c7":"code","f4f0261b":"code","c312843f":"code","d1cf2c95":"code","05ba5da8":"code","c7969c1c":"code","21ec7b75":"code","ac7963db":"code","d98b4db4":"code","a326893c":"markdown","43a70886":"markdown","621856f7":"markdown","f0382562":"markdown","a5aec26f":"markdown","d7d4dc2c":"markdown","ade1eeee":"markdown","44740c32":"markdown","91f5eab2":"markdown","d7db4a64":"markdown","70576019":"markdown","26527f77":"markdown","e9915060":"markdown","d94d4818":"markdown","c9ff298d":"markdown","5c4d5583":"markdown","5c89157f":"markdown","bc01eed1":"markdown","11e74da0":"markdown","651ea09e":"markdown","d690b898":"markdown","9d158626":"markdown","ef258e3f":"markdown","9acb3def":"markdown","96573288":"markdown","5460b98b":"markdown","097a7fb0":"markdown"},"source":{"8ad1250f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport sklearn\nimport os\nimport plotly\nimport plotly.graph_objs as go\nimport time\nimport itertools\nimport cv2\nimport seaborn as sns\nimport warnings\nimport tqdm\nimport math\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n%matplotlib inline\nplotly.offline.init_notebook_mode(True)\n\ndataset_dir = '..\/input\/'\nmeta_info = os.path.join(dataset_dir, 'Meta.csv')\ntrain_csv_path = os.path.join(dataset_dir, 'Train.csv')\ntest_csv_path = os.path.join(dataset_dir, 'Test.csv')\nlabels = ['20 km\/h', '30 km\/h', '50 km\/h', '60 km\/h', '70 km\/h', '80 km\/h', '80 km\/h end', '100 km\/h', '120 km\/h', 'No overtaking',\n               'No overtaking for tracks', 'Crossroad with secondary way', 'Main road', 'Give way', 'Stop', 'Road up', 'Road up for track', 'Brock',\n               'Other dangerous', 'Turn left', 'Turn right', 'Winding road', 'Hollow road', 'Slippery road', 'Narrowing road', 'Roadwork', 'Traffic light',\n               'Pedestrian', 'Children', 'Bike', 'Snow', 'Deer', 'End of the limits', 'Only right', 'Only left', 'Only straight', 'Only straight and right', \n               'Only straight and left', 'Take right', 'Take left', 'Circle crossroad', 'End of overtaking limit', 'End of overtaking limit for track']","04b466ec":"train_data_color = '#0f7b8e'\ntest_data_color = '#630f8e'\n\ntrainDf = pd.read_csv(train_csv_path)\ntestDf = pd.read_csv(test_csv_path)\nmetaDf = pd.read_csv(meta_info)\n\ntrainDf['Path'] = list(map(lambda x: os.path.join(dataset_dir,x.lower()), trainDf['Path']))\ntestDf['Path'] = list(map(lambda x: os.path.join(dataset_dir,x.lower()), testDf['Path']))\nmetaDf['Path'] = list(map(lambda x: os.path.join(dataset_dir,x.lower()), metaDf['Path']))\n\ntrainDf.sample(3)","230e2060":"fig, axs = plt.subplots(1, 2, sharex=True, sharey=True, figsize=(25, 6))\naxs[0].set_title('Train classes distribution')\naxs[0].set_xlabel('Class')\naxs[0].set_ylabel('Count')\naxs[1].set_title('Test classes distribution')\naxs[1].set_xlabel('Class')\naxs[1].set_ylabel('Count')\n\nsns.countplot(trainDf.ClassId, ax=axs[0])\nsns.countplot(testDf.ClassId, ax=axs[1])\naxs[0].set_xlabel('Class ID');\naxs[1].set_xlabel('Class ID');","b1640bb4":"trainDfDpiSubset = trainDf[(trainDf.Width < 80) & (trainDf.Height < 80)];\ntestDfDpiSubset = testDf[(testDf.Width < 80) & (testDf.Height < 80)];\n\ng = sns.JointGrid(x=\"Width\", y=\"Height\", data=trainDfDpiSubset)\nsns.kdeplot(trainDfDpiSubset.Width, trainDfDpiSubset.Height, cmap=\"Reds\",\n        shade=False, shade_lowest=False, ax=g.ax_joint)\nsns.kdeplot(testDfDpiSubset.Width, testDfDpiSubset.Height, cmap=\"Blues\",\n        shade=False, shade_lowest=False, ax=g.ax_joint)\nsns.distplot(trainDfDpiSubset.Width, kde=True, hist=False, color=\"r\", ax=g.ax_marg_x, label='Train distribution')\nsns.distplot(testDfDpiSubset.Width, kde=True, hist=False, color=\"b\", ax=g.ax_marg_x, label='Test distribution')\nsns.distplot(trainDfDpiSubset.Width, kde=True, hist=False, color=\"r\", ax=g.ax_marg_y, vertical=True)\nsns.distplot(testDfDpiSubset.Height, kde=True, hist=False, color=\"b\", ax=g.ax_marg_y, vertical=True)\ng.fig.set_figwidth(25)\ng.fig.set_figheight(8)\nplt.show();","43b932df":"sns.set_style()\nrows = 6\ncols = 8\nfig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(25, 12))\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.9, wspace=None, hspace=None)\nmetaDf = metaDf.sort_values(by=['ClassId'])\n\nidx = 0\nfor i in range(rows):\n    for j in range(cols):\n        if idx > 42:\n            break\n            \n        img = cv2.imread(metaDf[\"Path\"].tolist()[idx], cv2.IMREAD_UNCHANGED)\n        img[np.where(img[:,:,3]==0)] = [255,255,255,255]\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (60,60))\n        \n        axs[i,j].imshow(img)\n        axs[i,j].set_facecolor('xkcd:salmon')\n        axs[i,j].set_facecolor((1.0, 0.47, 0.42))\n        axs[i,j].set_title(labels[int(metaDf[\"ClassId\"].tolist()[idx])])\n        axs[i,j].get_xaxis().set_visible(False)\n        axs[i,j].get_yaxis().set_visible(False)\n        idx += 1","6855855e":"rows = 6\ncols = 8+4\nfig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(25, 12))\nplt.subplots_adjust(left=None, bottom=None, right=None, top=0.9, wspace=None, hspace=None)\nvisualize = trainDf.sample(rows*cols)\n\nidx = 0\nfor i in range(rows):\n    for j in range(cols):\n        img = cv2.imread(visualize[\"Path\"].tolist()[idx])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (60,60))\n        axs[i,j].imshow(img)\n        axs[i,j].set_title(labels[int(visualize[\"ClassId\"].tolist()[idx])])\n        axs[i,j].get_xaxis().set_visible(False)\n        axs[i,j].get_yaxis().set_visible(False)\n        idx += 1","d5ca38aa":"img_load_size = (60,60)\nzero_img = np.zeros([12,img_load_size[0], img_load_size[1], 3])\nzero_label = np.zeros([12,1])\n\ndef parse_function(filename, label):\n        image_string = tf.read_file(filename)\n        image = tf.image.decode_jpeg(image_string, channels=3)\n#         image = tf.py_func(eq, [image], image.dtype)\n        image.set_shape([None, None, 3])\n        \n        return filename, image, label\n    \ndef train_preprocess(filename, image, label):\n    image = tf.image.convert_image_dtype(image, tf.float32)\n    image = tf.image.resize_images(image, img_load_size)\n    return filename, image, label\n\ndef augmentate(filename, image, label):\n    grad = tf.random.uniform(shape=[], minval=-0.3, maxval=0.3)\n    dx = tf.random.uniform(shape=[], minval=-15, maxval=15, dtype=tf.int32)\n    dy = tf.random.uniform(shape=[], minval=-15, maxval=15, dtype=tf.int32)\n    image = tf.contrib.image.rotate(image, grad)\n    image = tf.contrib.image.translate(image, translations=[dx, dy])\n    \n    return filename, image, label\n\ndef eq(img: np.ndarray):\n    res = img.copy()\n    res[:, :, 0] = cv2.equalizeHist(img[:, :, 0])\n    res[:, :, 1] = cv2.equalizeHist(img[:, :, 1])\n    res[:, :, 2] = cv2.equalizeHist(img[:, :, 2])\n    \n    return res\n\ndef tf_equalize_histogram(image):\n    values_range = tf.constant([0., 255.], dtype = tf.float32)\n    histogram = tf.histogram_fixed_width(tf.to_float(image), values_range, 256)\n    cdf = tf.cumsum(histogram)\n    cdf_min = cdf[tf.reduce_min(tf.where(tf.greater(cdf, 0)))]\n\n    img_shape = tf.shape(image)\n    pix_cnt = img_shape[-3] * img_shape[-2]\n    px_map = tf.round(tf.to_float(cdf - cdf_min) * 255. \/ tf.to_float(pix_cnt - 1))\n    px_map = tf.cast(px_map, tf.uint8)\n\n    gth = tf.gather_nd(px_map, tf.cast(image, tf.int32))\n    eq_hist = tf.expand_dims(gth, 2)\n    return image","d9c847b0":"tf.reset_default_graph()\n\nepochs = 100\nbatch_size = 12\nprefetch_count = 1\nsamples_train = len(trainDf)\nsamples_test = len(testDf)\n\ndataset_train = tf.data.Dataset.from_tensor_slices((trainDf['Path'], trainDf['ClassId']))\ndataset_train = dataset_train.shuffle(len(trainDf['Path']))\ndataset_train = dataset_train.repeat(epochs)\ndataset_train = dataset_train.map(parse_function, num_parallel_calls=4)\ndataset_train = dataset_train.map(train_preprocess, num_parallel_calls=4)\ndataset_train = dataset_train.map(augmentate, num_parallel_calls=4)\ndataset_train = dataset_train.batch(batch_size)\ndataset_train = dataset_train.prefetch(prefetch_count)\n\ndataset_iterator = tf.data.Iterator.from_structure(dataset_train.output_types,\n                                                          dataset_train.output_shapes)\n\n\ndataset_test = tf.data.Dataset.from_tensor_slices((testDf['Path'], testDf['ClassId']))\ndataset_test = dataset_test.shuffle(len(testDf['Path']))\ndataset_test = dataset_test.repeat(epochs+1)\ndataset_test = dataset_test.map(parse_function, num_parallel_calls=4)\ndataset_test = dataset_test.map(train_preprocess, num_parallel_calls=4)\ndataset_test = dataset_test.batch(batch_size)\ndataset_test = dataset_test.prefetch(prefetch_count)\n\n\ntrain_init_op = dataset_iterator.make_initializer(dataset_train)\ntest_init_op = dataset_iterator.make_initializer(dataset_test)\n\nload_filename, load_img, load_label = dataset_iterator.get_next()","70f1e225":"fig, ax = plt.subplots(ncols=8, nrows=1, figsize=(15, 6))\nwith tf.Session() as sess:\n    sess.run(train_init_op)\n    for j in range(8):\n        i, l = sess.run([load_img, load_label])\n        i = (i[0]*255).astype(np.uint8)\n        ax[j].imshow(i)\n        ax[j].set_title(labels[l[0]])","183f9714":"dp_rate = tf.placeholder(dtype=tf.float32, shape=[], name='dp_rate')\n\nimg_placeholder = tf.placeholder(shape=[None, 60,60,3], dtype=tf.float32, name='img_placeholder')\nlabel_placeholder = tf.placeholder(shape=[None, 1], dtype=tf.int64, name='label_placeholder')\nmanual_load = tf.placeholder(dtype=tf.bool, shape=[], name='manual_load_placeholder')\n\n# inp = net = tf.cond(pred=manual_load, true_fn=lambda : img_placeholder, false_fn=lambda : load_img, name='network_start')\n# label = tf.cond(pred=manual_load, true_fn=lambda : label_placeholder, false_fn=lambda : load_label, name='label')\n\ninp = net = tf.cond(manual_load, lambda: img_placeholder, lambda: load_img)\nlabel = load_label\n\nconv1 = net = tf.layers.conv2d(inputs=net, filters=16, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\nnet = tf.layers.batch_normalization(inputs=net)\nconv2 = net = tf.layers.conv2d(inputs=net, filters=32, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\nnet = tf.layers.batch_normalization(inputs=net)\n\nconv3 = net = tf.layers.conv2d(inputs=net, filters=32, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\nnet = tf.layers.batch_normalization(inputs=net)\nconv4 = net = tf.layers.conv2d(inputs=net, filters=64, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\nnet = tf.layers.batch_normalization(inputs=net)\n\nnet = tf.layers.max_pooling2d(inputs=net, pool_size=(2,2), strides=(2,2))\n\nconv5 = net = tf.layers.conv2d(inputs=net, filters=64, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\nnet = tf.layers.batch_normalization(inputs=net)\nconv6 = net = tf.layers.conv2d(inputs=net, filters=128, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\nnet = tf.layers.batch_normalization(inputs=net)\n\nconv5 = net = tf.layers.conv2d(inputs=net, filters=256, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\nnet = tf.layers.batch_normalization(inputs=net)\nconv6 = net = tf.layers.conv2d(inputs=net, filters=400, kernel_size=(3,3), strides=(1,1), activation=tf.nn.leaky_relu)\nnet = tf.layers.batch_normalization(inputs=net)\n\nflatten1 = net = tf.layers.flatten(inputs=net)\n\ndp1 = net = tf.layers.dropout(inputs=net, rate=dp_rate)\ndense1 = net = tf.layers.dense(inputs=net, units=256)\nlogits = tf.layers.dense(inputs=net, units=43)\n\npred_classes = tf.argmax(logits, axis=1)\npred_probas = tf.nn.softmax(logits)\n\nacc, acc_op = tf.metrics.accuracy(labels=label, predictions=pred_classes)\nend_loss = tf.losses.sparse_softmax_cross_entropy(logits=logits, labels=label)\n\nloss = end_loss\n\nlabel_transpose = tf.transpose(label)\ncorrect_prediction = tf.equal(pred_classes, label_transpose)\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nconfusion_matrix_op = tf.confusion_matrix(labels=label, predictions=pred_classes, num_classes=43)\n\nopt = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)","8bd40b26":"config = tf.ConfigProto()\nconfig.gpu_options.per_process_gpu_memory_fraction = 0.4\nsess = tf.Session(config=config)\n\nsaver = tf.train.Saver()\nsess.run(tf.global_variables_initializer())\n# irn.load_weights('inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5')\ntrain_history = {'loss':[], 'acc':[], 'val_loss':[], 'val_acc':[]}\nbest_acc = 0\n\nfor e in range(epochs):\n    epoch_history = {'loss':[], 'acc':[], 'val_loss':[], 'val_acc':[]}\n    \n    sess.run(train_init_op)\n    for i in tqdm.tqdm_notebook(range(samples_train\/\/batch_size), ascii=True, desc='Train epoch {}'.format(e)):\n        _, _loss, _acc, mn = sess.run([opt, loss, accuracy, inp], feed_dict={dp_rate: 0.3, manual_load: False, img_placeholder: zero_img, label_placeholder: zero_label})\n#         print(np.mean(mn))\n        epoch_history['loss'].append(_loss)\n        epoch_history['acc'].append(_acc)\n        \n    sess.run(test_init_op)\n    for i in tqdm.tqdm_notebook(range(samples_test\/\/batch_size), ascii=True, desc='Test epoch {}'.format(e)):\n        _loss, _acc = sess.run([loss, accuracy], feed_dict={dp_rate: 0, manual_load: False, img_placeholder: zero_img, label_placeholder: zero_label})\n        epoch_history['val_loss'].append(_loss)\n        epoch_history['val_acc'].append(_acc)\n        \n    train_history['loss'].append(np.mean(epoch_history['loss']))\n    train_history['acc'].append(np.mean(epoch_history['acc']))\n    train_history['val_loss'].append(np.mean(epoch_history['val_loss']))\n    train_history['val_acc'].append(np.mean(epoch_history['val_acc']))\n    \n    print(\"***EPOCH SUMMARY*** Loss: {} Acc: {} | Test Loss: {} Test Acc {}\".format(train_history['loss'][-1], train_history['acc'][-1],\n                                                                                    train_history['val_loss'][-1], train_history['val_acc'][-1]))\n\n    if train_history['val_acc'][-1] > best_acc:\n        best_acc = train_history['val_acc'][-1]\n        save_path = saver.save(sess, \".\/model.ckpt\")\n        print(\"Model saved in path: %s\" % save_path)","2ba1faad":"titlefont = dict(family='Courier New, monospace', size=18, color='#7f7f7f')\nlayout = go.Layout(title='Traing & Test loss', xaxis=dict(title='Epoch', titlefont=titlefont),\n                                    yaxis=dict(title='Loss', titlefont=titlefont))\nfig = go.Figure(data=[go.Scatter(y=train_history['loss'], name='Train loss'), go.Scatter(y=train_history['val_loss'], name='Test loss')], layout=layout)\nplotly.offline.iplot(fig)\n\nlayout = go.Layout(title='Traing & Test accuracy', xaxis=dict(title='Epoch', titlefont=titlefont),\n                                    yaxis=dict(title='Accuracy', titlefont=titlefont))\nfig = go.Figure(data=[go.Scatter(y=train_history['acc'], name='Train accuracy'), go.Scatter(y=train_history['val_acc'], name='Test accuracy')], layout=layout)\nplotly.offline.iplot(fig)","2a9688aa":"saver.restore(sess, \".\/model.ckpt\")\nsess.run(test_init_op)\nconfusion_matrix = np.zeros([43,43])\ntest_analys = trainDf.copy()\npredictions = []\nprobabilities = []\nanalys = []\n\nfor i in tqdm.tqdm_notebook(range(samples_test\/\/batch_size), ascii=True, desc='Test best model'):\n    _files, _predictions, _probas, _gts, _cm = sess.run([load_filename, pred_classes, pred_probas, load_label, confusion_matrix_op], feed_dict={dp_rate: 0, manual_load: False, img_placeholder: zero_img, label_placeholder: zero_label})\n    confusion_matrix += _cm\n    for i in range(batch_size):\n        sample_info = {'image': _files[i].decode(), 'prediction': int(_predictions[i]), 'gt': int(_gts[i]), 'gt_probas': _probas[i][_gts[i]],\n                       'prediction_probas': _probas[i][_predictions[i]], 'prediction_type': 'Correct' if _gts[i] == _predictions[i] else 'Wrong'}\n        for cls_id, j in enumerate(_probas[i]):\n            sample_info['prob_{}'.format(cls_id)] = j\n        analys.append(sample_info)\n\nanalys_df = pd.DataFrame(analys)","73d755d6":"analys_df.sample(4)","a2fda03b":"rows = 3\ncols = 4\nfig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(25, 8))\nvisualize = trainDf.sample(rows*cols)\n\nanalys_df_copy = analys_df.copy()\nanalys_df_copy = analys_df_copy.sample(frac=1)\n\nidx = 0\nfor i in range(rows):\n    for j in range(cols):\n        img = cv2.imread(analys_df_copy.iloc[idx]['image'])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (100, 100))\n        \n        gt = analys_df_copy.iloc[idx]['gt']\n        pred = analys_df_copy.iloc[idx]['prediction']\n        \n        axs[i,j].imshow(img)\n        axs[i,j].set_title('Predicted: {}\\nGround truth {}'.format(labels[pred], labels[gt]), fontsize=14)\n        axs[i,j].get_xaxis().set_visible(False)\n        axs[i,j].get_yaxis().set_visible(False)\n        idx += 1\n        \nfig.suptitle(\"Random prediction\", fontsize=30, y=2.1, x=0.515);\nplt.subplots_adjust(left=None, bottom=None, right=0.9, top=1.9, wspace=None, hspace=None)","80f2349b":"rows = 3\ncols = 4\nfig, axs = plt.subplots(rows, cols, sharex=True, sharey=True, figsize=(25, 8))\nvisualize = trainDf.sample(rows*cols)\n\nanalys_df_copy = analys_df[analys_df['prediction_type'] == 'Wrong'].copy()\nanalys_df_copy = analys_df_copy.sample(frac=1)\n\nidx = 0\nfor i in range(rows):\n    for j in range(cols):\n        img = cv2.imread(analys_df_copy.iloc[idx]['image'])\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, (100, 100))\n        \n        gt = analys_df_copy.iloc[idx]['gt']\n        pred = analys_df_copy.iloc[idx]['prediction']\n        \n        axs[i,j].imshow(img)\n        axs[i,j].set_title('Predicted: {}\\nGround truth {}'.format(labels[pred], labels[gt]), fontsize=14)\n        axs[i,j].get_xaxis().set_visible(False)\n        axs[i,j].get_yaxis().set_visible(False)\n        idx += 1\n        \nfig.suptitle(\"Wrong prediction\", fontsize=30, y=2.1, x=0.515);\nplt.subplots_adjust(left=None, bottom=None, right=0.9, top=1.9, wspace=None, hspace=None)","30705d6b":"output_dir = '.\/output'\nerror_dir = '.\/output\/errors'\nif not os.path.isdir(output_dir):\n    os.mkdir(output_dir)\nif not os.path.isdir(error_dir):\n    os.mkdir(error_dir)\n\nfor idx, row in tqdm.tqdm_notebook(analys_df[analys_df['prediction_type'] == 'Wrong'].iterrows()):\n    name = os.path.splitext(os.path.basename(row['image']))[0]\n    name = '{}__{}__as__{}.png'.format(name, labels[row['gt']].replace(' ', '_'), labels[row['prediction']].replace(' ', '_'))\n    img = cv2.imread(row['image'])\n    cv2.imwrite(os.path.join(error_dir, name), img)","0c89d420":"!tar -cvf .\/errors.tar .\/output\/errors 1>\/dev\/null","d9f73dcb":"cm = confusion_matrix\nf = np.sum(cm, axis=1)\nnormalized_cm = cm\nfor i in range(43):\n    normalized_cm[i, :] \/= sum(normalized_cm[i, :])\n\nnormalized_cm = np.round(normalized_cm, 2)\n    \nfig, ax = plt.subplots(1,1, figsize=((20, 20)))\n\nax.imshow(normalized_cm)\n\nax.set_xticks(np.arange(len(labels)))\nax.set_yticks(np.arange(len(labels)))\n\nax.set_xticklabels(labels)\nax.set_yticklabels(labels)\n\nplt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n\nfor i in range(len(labels)):\n   for j in range(len(labels)):\n       ax.text(j, i, normalized_cm[i, j], ha=\"center\", va=\"center\", color=\"w\")\n\nax.set_title('Confusion matrix');","83add8ee":"fig, axs = plt.subplots(1, 1, sharex=False, sharey=True, figsize=(25, 7))\n\npx = sns.countplot(x='prediction_type', data=analys_df, ax=axs)\naxs.set_title('Prediction type distribution', fontsize=18)\naxs.set_xlabel('Prediction type', fontsize=16)\naxs.set_ylabel('Fraction', fontsize=16);\n\n\ntotal = analys_df.shape[0]\nfor idx, p in enumerate(px.patches):\n        px.annotate('{:.1f}%'.format(p.get_height()\/total*100), (p.get_x()+0.365, p.get_height()+100), fontsize=18)\n\n\npx.yaxis.set_ticks(np.linspace(0, total, 11))\npx.set_yticklabels(map('{:.1f}%'.format, 100*px.yaxis.get_majorticklocs()\/total));","c2e225c7":"analys_df.prediction_type.value_counts()","f4f0261b":"correct_prediction_by_class = analys_df[analys_df['prediction_type'] == 'Correct']['gt'].value_counts() \/ testDf['ClassId'].value_counts().sort_index()\ncorrect_prediction_by_class_df = pd.DataFrame({'accuracy': correct_prediction_by_class, 'class': labels})\n\nwrong_prediction_by_class = analys_df[analys_df['prediction_type'] == 'Wrong']['gt'].value_counts() \/ testDf['ClassId'].value_counts().sort_index()\nwrong_prediction_by_class_df = pd.DataFrame({'error': wrong_prediction_by_class, 'class': labels})\n\nfig, axs = plt.subplots(2, 1, sharex=False, sharey=True, figsize=(25, 27))\n\nsns.barplot(x='class', y='accuracy', data=correct_prediction_by_class_df, ax=axs[1])\nsns.barplot(x='class', y='error', data=wrong_prediction_by_class_df, ax=axs[0])\n\naxs[0].set_title('Wrong prediction grouped by class', fontsize=18)\naxs[0].set_xlabel('Class', fontsize=16)\naxs[0].set_ylabel('Percent of wrong prediction', fontsize=16)\naxs[0].set_xticklabels(rotation=90, labels=labels)\n\naxs[1].set_title('Correct prediction grouped by class', fontsize=18)\naxs[1].set_xlabel('Class', fontsize=16)\naxs[1].set_ylabel('Percent of correct prediction', fontsize=16)\naxs[1].set_xticklabels(rotation=90, labels=labels);","c312843f":"fig, axs = plt.subplots(1, 1, sharex=False, sharey=True, figsize=(25,12))\n\nsns.boxplot(x='prediction_type', y='prediction_probas', data=analys_df, ax=axs);\naxs.set_title('Prediction probabilities distribution', fontsize=18);\naxs.set_xlabel('Predict type', fontsize=16)\naxs.set_ylabel('Probability distribution', fontsize=16);","d1cf2c95":"fig, axes = plt.subplots(2, 2, sharex=False, sharey=True, figsize=(25,10))\n\nconfidence_thresholds = iter([0.6, 0.7, 0.8, 0.9])\n\nfor i in range(2):\n    for j in range(2):\n        confidence_threshold = next(confidence_thresholds)\n        \n        analys_df_confidence = analys_df.copy()\n        new_predict = []\n        for idx, row in analys_df_confidence.iterrows():\n            new_predict.append('Not sure' if row['prediction_probas'] < confidence_threshold else 'Probably correct')\n\n        analys_df_confidence['confidence_analys'] = new_predict\n        \n        axs = axes[i,j]\n        \n        px = sns.countplot(x='prediction_type', hue='confidence_analys', data=analys_df_confidence, ax=axs);\n        axs.set_title('Confidence threshold {}'.format(confidence_threshold), fontsize=18)\n        axs.set_xlabel('Prediction type', fontsize=16)\n        axs.set_ylabel('Fraction', fontsize=16);\n        axs.legend(title='Confidence prediction')\n\n        total = analys_df_confidence.shape[0]\n        for idx, p in enumerate(px.patches):\n            px.annotate('{:.1f}%'.format(p.get_height()\/total*100), (p.get_x()+0.14, p.get_height()+100), fontsize=18)\n\n\n        px.yaxis.set_ticks(np.linspace(0, total, 11))\n        px.set_yticklabels(map('{:.1f}%'.format, 100*px.yaxis.get_majorticklocs()\/total));\n\nplt.subplots_adjust(left=None, bottom=None, right=None, top=1.2, wspace=None, hspace=None)\nfig.suptitle('Confidence analys', fontsize=20, y=1.3, x=0.51);","05ba5da8":"confidences = []\n\nfor confidence_threshold in tqdm.tqdm_notebook(np.arange(start=0, stop=1.1, step=0.01)):\n\n    analys_df_confidence = analys_df.copy()\n    new_predict = []\n    for idx, row in analys_df_confidence.iterrows():\n        new_predict.append('Not sure' if row['prediction_probas'] < confidence_threshold else 'Probably correct')\n\n    analys_df_confidence['confidence_analys'] = new_predict\n\n    false_positive = analys_df_confidence[(analys_df_confidence['prediction_type'] == 'Wrong') &\n                                          (analys_df_confidence['confidence_analys'] == 'Probably correct')].shape[0]\/analys_df_confidence.shape[0]\n    \n    true_positive = analys_df_confidence[(analys_df_confidence['prediction_type'] == 'Correct') &\n                                          (analys_df_confidence['confidence_analys'] == 'Probably correct')].shape[0]\/analys_df_confidence.shape[0]\n    \n    not_sure_at_wrong = analys_df_confidence[(analys_df_confidence['prediction_type'] == 'Wrong') &\n                                          (analys_df_confidence['confidence_analys'] == 'Not sure')].shape[0]\/analys_df_confidence.shape[0]\n    \n    not_sure_at_correct = analys_df_confidence[(analys_df_confidence['prediction_type'] == 'Correct') &\n                                          (analys_df_confidence['confidence_analys'] == 'Not sure')].shape[0]\/analys_df_confidence.shape[0]\n    \n    cf_level_result = {'fp': false_positive, 'tp': true_positive, 'ns_w': not_sure_at_wrong, 'ns_c': not_sure_at_correct, 'cf': confidence_threshold}\n    confidences.append(cf_level_result)\n    \nconfidences = pd.DataFrame(confidences)\n\nfig, axes = plt.subplots(1, 1, sharex=False, sharey=True, figsize=(25,10))\n\n\nsns.lineplot(x='cf', y='fp', data=confidences, ax=axes, label='False positive');\nsns.lineplot(x='cf', y='tp', data=confidences, ax=axes, label='True positive');\nsns.lineplot(x='cf', y='ns_c', data=confidences, ax=axes, label='Not shure at correct prediction');\nsns.lineplot(x='cf', y='ns_w', data=confidences, ax=axes, label='Not shure at wrong prediction');\n\naxes.set_yscale('log')\naxes.set_xlabel('Confidence threshold', fontsize=16)\naxes.set_ylabel('Fraction of prediction', fontsize=16);\naxes.legend(loc='center left', prop={'size': 16})\nfig.suptitle('Confidence threshold analys', fontsize=20);\nplt.grid()","c7969c1c":"import matplotlib.gridspec as gridspec\nres = sess.run([load_img, load_label, conv1, conv2, conv3, conv4, conv5, conv6],\n               feed_dict={dp_rate: 0,\n                          manual_load: False,\n                          img_placeholder: np.zeros([12,60,60,3])})\nimg, label = res[0], res[1]\nimg_visible = (img*255).astype(np.uint8)[0, :, :, :]\nfig, ax = plt.subplots(1,1, figsize=(6,6))\nax.imshow(img_visible)\nax.set_title(labels[label[0]])\n\nfilters = res[2:]\nlayer_names = ['Convolutional layer {}'.format(x+1) for x in range(len(filters))]","21ec7b75":"for filter_index in range(len(filters)):\n    layers = filters[filter_index]\n    filter_count = layers.shape[3]\n    n_columns = 6\n    n_rows = math.ceil(filter_count \/ n_columns) + 1\n    fig = plt.figure(figsize=(24,n_rows*4));\n    fig.suptitle(layer_names[filter_index], fontsize=16)\n    for i in range(filter_count):\n        plt.subplot(n_rows, n_columns, i+1)\n        plt.axis('off')\n        plt.title('Filter: {0} '.format(str(i)))\n        plt.imshow(layers[0,:,:,i], interpolation=\"nearest\", cmap='bwr')\n    plt.show()","ac7963db":"def grad_cam(sess, layer, predicted_class, nb_classes, img):\n    conv_layer = layer\n    one_hot = tf.sparse_to_dense(predicted_class, [nb_classes], 1.0)\n    signal = tf.multiply(logits, one_hot)\n    loss = tf.reduce_mean(signal)\n    \n    grads = tf.gradients(loss, conv_layer)[0]\n    # Normalizing the gradients\n    norm_grads = tf.div(grads, tf.sqrt(tf.reduce_mean(tf.square(grads))) + tf.constant(1e-5))\n\n    output, grads_val = sess.run([conv_layer, norm_grads], feed_dict={dp_rate: 0, manual_load: True,\n                                                                     img_placeholder: img})\n    output = output[0]           \n    grads_val = grads_val[0]     \n\n    weights = np.mean(grads_val, axis = (0, 1))\n    cam = np.ones(output.shape[0 : 2], dtype = np.float32)\n\n    # Taking a weighted average\n    for i, w in enumerate(weights):\n        cam += w * output[:, :, i]\n\n    # Passing through ReLU\n    cam = np.maximum(cam, 0)\n    cam = cam \/ np.max(cam)\n    cam = cv2.resize(cam, (224,224))\n\n    # Converting grayscale to 3-D\n    cam3 = np.expand_dims(cam, axis=2)\n    cam3 = np.tile(cam3,[1,1,3])\n\n    return cam3","d98b4db4":"_img, _lb = sess.run([load_img, load_label])\ngrad_ipt = _img.copy()\n\nsamples = _img.shape[0]\/\/2\nfig, ax = plt.subplots(ncols=samples, nrows=2, figsize=(20, 8))\nfor i in range(samples):\n    ax[0, i].imshow(_img[i, :, :, :])\n    ax[0, i].get_xaxis().set_visible(False)\n    ax[0, i].get_yaxis().set_visible(False)\n    ax[0, i].set_title(labels[_lb[i]])\n    \n    image = _img[i, :, :, :]\n    \n    height = image.shape[0]\n    width = image.shape[1]\n\n    grad_ipt[0, :, :] = _img[i, :, :]\n    heat_map = grad_cam(sess, conv1, _lb[i], 43, grad_ipt)\n    # resize heat map\n    heat_map_resized = cv2.resize(heat_map, (height, width))\n\n    # normalize heat map\n    max_value = np.max(heat_map_resized)\n    min_value = np.min(heat_map_resized)\n    normalized_heat_map = (heat_map_resized - min_value) \/ (max_value - min_value)\n    normalized_heat_map = cv2.applyColorMap((normalized_heat_map*255).astype(np.uint8), cv2.COLORMAP_JET)\n    \n    ax[1, i].imshow(normalized_heat_map)\n    ax[1, i].get_xaxis().set_visible(False)\n    ax[1, i].get_yaxis().set_visible(False)","a326893c":"### Numerically accuracy by class\nConfusion matrix produce perfect visualization for understanding why some class is so bad in predictions, but it's no so convenient way to sum up numerrically how this class is good predicted. Below are histgrams of corretly and wrong predicted classes without correlation with other classes","43a70886":"### Tensorflow data pipeline\n\nAbove we have prepared pandas DataFrame with all data. Now, let's create operaition for loading data using [tf.data.Dataset](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/data\/Dataset)","621856f7":"### Confusion matrix\n\nConfusion matrix gives us additional information about accuracy distribution. It's naturally that network may be confused in prediction 'Pedestrian' and 'Other dangerous' signs. Also, confusion matrix can give us idea what to improove in network","f0382562":"### Discover dataset balance\nThe easiest way to discover dataset balance - build histogram. We consider to use [seaborn](https:\/\/seaborn.pydata.org\/) library based on matplotlib for pretty data visualization.\n\nTrain and test subset of dataset have similar balance distribution. Train and test split provided by GTSRB.","a5aec26f":"### Convolution neural network visualization\n\nThere are many method for consolutional neural network visualization in many papers. The simplest one - visualize image after consolutional layer. We stores all results after each convolutional layers","d7d4dc2c":"For more clearly answer, let's approximate function of confidence level threshold","ade1eeee":"### Samples visualization\n\nIt is good idea to visualize samples in order to brief data exploration. Image visualization can help to understand data problem. Some solutions (such as histogram equalization) can be discovered by visual data exploration","44740c32":"### Model definition\n\nModel graph defined using native tensorflow API. Also, we should defined some placeholders in order to have opportunity to load custom controled data into netwrok (for further model analysis)","91f5eab2":"Convolutional layers have many filters. Visualizing all images is very epxencive and hard, but we will try. Results after convulition is not normalized, so we will normalized it manualy. Visualization looks like heatmap but it is not heatmap. It's just pretty visualization","d7db4a64":"And create tar archive for convenient way to download all at once","70576019":"Here are histogram of prediction types","26527f77":"### Grad-CAM\n\nAnother good way to understand what our model have learned - using Grad-CAM method. [Grad-CAM](https:\/\/arxiv.org\/abs\/1610.02391) - is Gradient-weighted Class Activation Map. We can see which parts of image are important for prediction (i.e. mostly cause prediction).","e9915060":"Eveluate some samples and visualize Grad-CAM heatmaps","d94d4818":"### Wrong prediction visualization\n\nOur model can't achieve perfect accuracy (i.e. 100%). Let's visualize wrong predicted samples. Some of them have realy bad quality, resolution. Others have unexpected artifacts (such as extreame rotation, half hidden signs or shaddow). This situations wasn't present in train part, so network have no idea how to deal with it","c9ff298d":"### Image size distribution\nDataset contains thouthands of images. Images don't have the same resolution. Some of them are big, other are small. We should somehow choose appropriate resolution of samples. The best way to visualize width and height corellation - using multivariate plotting.\n\nAs we can see bellow, most of images is rectangular (it can be prooved by applying liniar regression on the samples resolution). Most of samples are about 35x35 pixels. And only few samples have big resolution like a 100x100 pixels.","5c4d5583":"### Tensorflow utils\n\nWe decide to use native tensorflow as deep learning framework without high level API. The best way to implement data pipleline - implement load data operation as node of comoutition graph. This will be the first node of computitional graph (i.e. all other node is depended from it). This approach has several adantages:\n\n- Parallelism out of the box (provided by dataflow graph principies)\n- Opportunity to implement data augmentation on GPU (not implemented here)\n- Convenient way to wrok with it provided by tf.data infrastructure\n- Fast way to use native and 3rd party python libraries wrapped by tf.py_func (deprecated since 1.13.? version)\n\nHere we defined utils for loading tensors, augmentation and other data manipulation","5c89157f":"### Random prediction visualization\n\nUsing information computed above, we can visualize some random samples with their predictions. As we can see - it is impossible to recognize some pictures definitely by human, but network still generates correct predictions. Awesome!","bc01eed1":"## Necessary imports\n\nMain dependencies:\n\n* tensorflow 1.x for neural networks\n* matplotlib for visualization\n* pandas for data manipulation","11e74da0":"### Training result\n\nIt's good practice to visualize accuracy and loss evolution. [plotly](https:\/\/plot.ly\/) library gives as way to build interactive figures inplace in jupyter","651ea09e":"### Confidence level threshold\n\nFind threshold for confidence level by experiment. All predictions under this threshold will be threated as 'No shure'. It is clearly, that we can't achive more accuracy that we had before, but we can reduce wrong predictions","d690b898":"### Model performance data preparation\n\nFor futher model analysis we need some data. Very good solution - store all statistic data in pandas DataFrame data structure. Let's evaluate all test samples and store all information about prediction, probabilities and other information","9d158626":"### Load data\nWe should remap path because of kaggle converts folder to lowercase mode","ef258e3f":"### Target class visualization\nIt is not a sample in the dataset, it is just a picture of sign. Some of them may be different from the dataset samples because of dataset contains images of German traffic signs and pictures bellow are Ukrainian traffic signs ([source of pictures](http:\/\/pdd.ua\/33\/))","9acb3def":"### Confidence analysis\n\nClassification neural networks produce not only discrete prediction. Model is also generated probability for each class (provided by softmax layer). Can we break up with wrong prediction by analys confidence levels?\n\nLet's build confidence level distribution","96573288":"### Model evaluation analys overview\n\nWe have built pandas DataFrame. Let's observe it structure","5460b98b":"### Model training\n\nTrain model several epoch and store results of each epoch for further analysis","097a7fb0":"Let's dump all wrong prediction for further analys by someone else"}}