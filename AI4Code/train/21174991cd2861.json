{"cell_type":{"e9cfae8e":"code","c5e23064":"code","5d288d9d":"code","59e23452":"code","6e438794":"code","2de771dd":"code","6623fdf1":"code","e73c004c":"code","3587072f":"code","543d8f92":"code","6ee97306":"code","8a62145d":"code","5b881eb1":"code","618ec1d6":"code","1e4f8b4c":"code","12ab50c7":"code","02db1a78":"code","d2c26321":"code","26905495":"code","eebecbb2":"markdown","4d1e45a7":"markdown","ef8b1e61":"markdown","29be48a9":"markdown","da60c627":"markdown","1e3ae70e":"markdown","54270f22":"markdown","4e7ba0c8":"markdown","241fcae0":"markdown","bf4b81dc":"markdown","abd1d1b3":"markdown","1456e22f":"markdown","5dcdd01c":"markdown","2eef346f":"markdown"},"source":{"e9cfae8e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBClassifier\nfrom imblearn.under_sampling import RandomUnderSampler # Used for under sampling. explained further in notebook.\nimport collections\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nimport optuna","c5e23064":"df_train_og = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ndf_test_og  = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')\nsubmission  = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')","5d288d9d":"df_train_og.shape","59e23452":"df_train_og.head()","6e438794":"df_train_og.nunique()","2de771dd":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2\n\n    for col in df.columns:\n        col_type = df[col].dtypes\n\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n\n    if verbose:\n        print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n \n    return df","6623fdf1":"df_train = reduce_mem_usage(df_train_og)\ndf_test = reduce_mem_usage(df_test_og)\ndel df_train_og\ndel df_test_og","e73c004c":"cat_count = collections.Counter(df_train['Cover_Type'])\ncat_freq = cat_count.values()\ncat = cat_count.keys()\nplt.bar(cat , cat_freq)\n\nprint(cat_count)","3587072f":"df_train = df_train[(df_train['Cover_Type'] != 4) & (df_train['Cover_Type'] != 5)]","543d8f92":"rus = RandomUnderSampler(sampling_strategy = \"not minority\")\nX  = df_train.drop(columns = ['Id' , 'Cover_Type','Soil_Type7' , 'Soil_Type15'])\ny = df_train['Cover_Type']\nX_res,y_res = rus.fit_resample(X,y)","6ee97306":"cat_count = collections.Counter(y_res)\ncat_freq = cat_count.values()\ncat = cat_count.keys()\nplt.bar(cat , cat_freq)\n\nprint(cat_count)","8a62145d":"x_train,x_test,y_train,y_test = train_test_split(X_res,y_res,test_size = 0.2)","5b881eb1":"def objective_xgb(trial):\n    xgb_params = {\n        'learning_rate': 0.03,\n        'tree_method': 'gpu_hist',\n        'booster': 'gbtree',\n        'eval_metric' : 'mlogloss',\n        'objective' : 'multi:softmax',\n        'n_estimators': trial.suggest_int('n_estimators', 500, 1000, 100),\n#         'reg_lambda': trial.suggest_int('reg_lambda', 1, 100),\n#         'reg_alpha': trial.suggest_int('reg_alpha', 1, 10),\n        'subsample': trial.suggest_float('subsample', 0.2, 0.8, step=0.1),\n#         'colsample_bytree': trial.suggest_float('colsample_bytree', 0.2, 1.0, step=0.1),\n        'max_depth': trial.suggest_int('max_depth', 3, 10), \n#         'min_child_weight': trial.suggest_int('min_child_weight', 2, 10),\n        'gamma': trial.suggest_float('gamma', 0, 1.0),\n        'predictor' : 'gpu_predictor'\n    }\n    \n    pipe = Pipeline(steps = [\n    \n    ('step1' , StandardScaler()),\n    ('step2' , XGBClassifier(**xgb_params))\n     ])\n    \n    pipe.fit(x_train,y_train)\n    y_pred = pipe.predict(x_test)\n    return accuracy_score(y_test,y_pred)","618ec1d6":"study_xgb= optuna.create_study(direction = 'maximize')\nstudy_xgb.optimize(objective_xgb, n_trials=50)","1e4f8b4c":"best_params_xgb = study_xgb.best_params","12ab50c7":"pipe = Pipeline(steps = [\n    \n    ('step1' , StandardScaler()),\n    ('step2' , XGBClassifier(**best_params_xgb))\n     ])","02db1a78":"pipe.fit(x_train,y_train)\ny_pred = pipe.predict(x_test)\nprint(accuracy_score(y_test,y_pred))","d2c26321":"df_test = df_test.drop(columns = ['Id' , 'Soil_Type7' , 'Soil_Type15'])\nFinal_pred = pipe.predict(df_test)","26905495":"submission['Cover_Type'] = Final_pred\nsubmission.to_csv('Submission.csv' , index=False)","eebecbb2":"* We can see that, after under sampling all categories has equal number of rows. (Categories 4 and 5 are displayed as blank because we have removed it from dataset)","4d1e45a7":"* Lets drop all rows associated with categories 4 and 5.","ef8b1e61":"* Train test split","29be48a9":"We have 56 columns in dataset out of which \"Cover_Type\" is the target variable that we have to predict.","da60c627":"<!-- # One way Anova test for finding important features.\n\n* Now we will try to find important features using ANOVA test.\n* Anova test is generally carried out between numerical features and categorical features.\n* Here categorical feature is target variable i.e \"Cover_Type\" and numerical features are - \n\n        Elevation                                \n        Aspect                                    \n        Slope                                     \n        Horizontal_Distance_To_Hydrology         \n        Vertical_Distance_To_Hydrology           \n        Horizontal_Distance_To_Roadways        \n        Hillshade_9am                             \n        Hillshade_Noon                            \n        Hillshade_3pm                             \n        Horizontal_Distance_To_Fire_Points\n        \n * For scope of this notebook we will be considering only numerical features and not other categorical features.\n         -->","1e3ae70e":"# Pipeline and model building ","54270f22":"* Below pipeline first performs Standardization on input features and then uses XGBClassifier to predict target variable.","4e7ba0c8":"Thanks for reading till here. If you found it helpul or interesting please consider dropping a comment.\n\nIn case of any correction or suggestion, you can let me know in comments as well.\n\nThanks.","241fcae0":"* In order to transform our dataset to balanced dataset , lets reduce number of rows for each target category to number of rows associated with least frequent category.\n*  Here category 6 has 11426 rows associated with it. For each category , we will be selecting random 11426 rows so that each category will have equal number of rows.\n* ID column in data is unique for each row and does not contribute for predicting target variale.  \n* seperating target variable from all the features below.","bf4b81dc":"* Below snippet describes number of unique values each column has.\n* Except for first few columns , all other columns are actually categorical in nature with value 0 or 1.\n* Columns named \"Soil_Type15\" and \"Soil_Type7\" have only 1 unique value which is 0. We can drop this 2 columns.","abd1d1b3":"# Data Analysis\n\n* Imported required libraries and loaded csv files.","1456e22f":"# Dealing with Imbalanced dataset\n\n* From below graph we can observe that target variable is highly imbalanced. Infact, category 4 has only 377 records associated with it and category 5 has only one.\n* Note - here value on y-axis is represented in 10 to the power 6. That means 0.1 on y-axis represents value of 100000.","5dcdd01c":"# Goal of this notebook\n\n* to do analysis of data.\n* Perform Under sampling ,to mitigate any biasing due to imbalanced dataset.\n* Used XGB classifier (tuned using Optuna) and acheived 94% accuracy.","2eef346f":"* As this data consists of almost 4 million records and to make our computation faster lets reduce memory size of it using following code snippet."}}