{"cell_type":{"0524c2fa":"code","93d6de9e":"code","2b4a2143":"code","39f2a099":"code","419ae928":"code","dff20238":"code","e53ebfd9":"code","1d049629":"markdown","ffb2ecee":"markdown","52f27c06":"markdown","3a4ac972":"markdown","85877408":"markdown","9cdaeadf":"markdown","98f7cae4":"markdown","55c61cef":"markdown"},"source":{"0524c2fa":"import numpy as np\nimport gym\nimport random\n\nfrom tqdm import tqdm","93d6de9e":"env = gym.make('Taxi-v3')\nenv.render()","2b4a2143":"action_size = env.action_space.n\nprint(f\"Total possible actions: {action_size}\")\n\nstate_size = env.observation_space.n\nprint(f\"Total states: {state_size}\")","39f2a099":"qtable = np.zeros((state_size,action_size))\nprint(qtable)","419ae928":"total_episodes = 50000       # Total episodes to train the agent for\ntotal_test_episodes = 10     # Number of test episodes\nmax_steps = 99               # Terminate if the agent takes more than 99 steps\n\nalpha = 0.7                  # Learning rate\ngamma = 0.618                # Discounting rate for rewards\n\n# parameters for maintaining trade-off between exploration-exploitation\nepsilon = 1.0                # Exploration rate\nmax_epsilon = 1.0            # Exploration probability at the start\nmin_epsilon = 0.01           # Minimum exploration probability\ndecay_rate = 0.01            # rate at which epsilon shrinks ","dff20238":"# iterate over every episode\nfor episode in tqdm(range(total_episodes)):\n    \n    # Reset the environment at every episode\n    state = env.reset()\n    # flag to check if episode is terminated or not\n    done = False\n    \n    # iterate over all steps that the agent can take in an episode\n    for step in range(max_steps): \n        \n        # Select an action based on the epsilon-greedy policy\n        \n        # probability to select exploitation\n        one_minus_epsilon = random.uniform(0,1)\n        \n        # if one_minus_epsilon is greater than epsilon then exploit \n        if one_minus_epsilon > epsilon:\n            action = np.argmax(qtable[state,:])\n        # else explore by selecting an action randomly\n        else:\n            action = env.action_space.sample()\n            \n        # Take this action to reach the next state and get a reward \n        new_state, reward, done, info = env.step(action)\n        \n        # update the Q-Table based on the formula given in the algorithm\n        qtable[state,action] = qtable[state,action] + alpha*(reward + gamma*np.max(qtable[new_state,:]) - qtable[state,action])\n        \n        # update the current state\n        state = new_state\n        \n        # if the agent has reached termination state then break\n        if done:\n            break\n        \n    # epsilon decay to maintain trade-off between exploration-exploitation\n    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)","e53ebfd9":"# keep track of all rewards\nrewards = []\n\nfor episode in range(total_test_episodes):\n    \n    state = env.reset()\n    done = False\n    total_rewards = 0\n    print(f\"{'*'*80}\")\n    print(f\"Episode {episode + 1}:\")\n    \n    for step in range(max_steps):\n        \n        # render every frame of the agent\n        env.render()\n        \n        # take an action that has max expected future reward given in that state\n        action = np.argmax(qtable[state,:])\n        \n        new_state, reward, done, info = env.step(action)\n        \n        total_rewards += reward\n        \n        if done:\n            # keep track of rewards received at every episode\n            rewards.append(total_rewards)\n            print(f\"Score: {total_rewards}\")\n            break\n        \n        state = new_state\n\nenv.close()\nprint(f\"Average Rewards: {sum(rewards)\/total_test_episodes}\")","1d049629":"### 0. Necessary Dependancies","ffb2ecee":"# Taxi-v3 using Q-Learning\n\n<img src=\"https:\/\/machinelearningjourney.com\/wp-content\/uploads\/2020\/07\/Taxi-demo.gif\" style=\"height:200px;\"><\/img>\n\n\n\nThere are 4 locations (labeled by different letters) and your job is to pick up the passenger at one location and drop him off in another. You receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.","52f27c06":"### 3. Test the agent","3a4ac972":"### 2. Q-Learning Algorithm\n\n![](https:\/\/leimao.github.io\/images\/blog\/2019-03-14-RL-On-Policy-VS-Off-Policy\/q-learning.png)","85877408":"### 1. Pre-Setup","9cdaeadf":"**Create the Environment**","98f7cae4":"**Hyperparameters**","55c61cef":"**Create the Q-Table**\n\nThe Q-table is NxM table where N corresponds to the number of states and M corresponds to the number of actions that the agent can take."}}