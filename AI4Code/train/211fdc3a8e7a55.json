{"cell_type":{"53838cdb":"code","8a7c3c84":"code","00362100":"code","3d427b4c":"code","c347eb8a":"code","5fc2aa1b":"code","67e647f3":"code","3ef52c36":"code","6cd866d0":"code","0282171c":"code","4081d4f6":"code","c2d80f6e":"code","61bec534":"code","d4fc8076":"code","1d8f1cf8":"code","e7f72a2a":"code","d4e2ca36":"code","6b6a7d4a":"code","fea832ba":"code","35d9e393":"code","b5bc50b1":"code","c871c73c":"code","2df5e9d8":"code","10246c23":"code","dba23ea0":"code","b4adcd21":"code","77e600d2":"code","81dcdb8d":"code","6a9dce53":"code","db4e2b02":"code","231bce97":"code","49f04d5c":"code","2a30e7c7":"code","efad188e":"code","bb68a775":"code","bee247a7":"code","8a5c654a":"code","6e29ca12":"markdown","0ad7eb84":"markdown","afcff294":"markdown","325d0fcc":"markdown","a8d0283d":"markdown","95b13190":"markdown","e5685407":"markdown","55e95e6d":"markdown","07efc079":"markdown","c21f8e47":"markdown","cde7309b":"markdown","536a6517":"markdown","98fc358c":"markdown","7222cdbe":"markdown","d56ee461":"markdown","5394d60d":"markdown","72d33a0e":"markdown","66c46840":"markdown","2592b745":"markdown","6b986901":"markdown","4160b84c":"markdown","87b92b8a":"markdown","2499bbfc":"markdown","4e844220":"markdown","6bac8645":"markdown","711cf5e3":"markdown","4bff8275":"markdown","7fd712b4":"markdown","0e5283b3":"markdown","2a43dbc3":"markdown","8968a7d6":"markdown","3e4fa9a4":"markdown"},"source":{"53838cdb":"import pandas as pd\nimport numpy as np","8a7c3c84":"add = \"..\/input\/1429_1.csv\"\n\nreviews = pd.read_csv(add,low_memory=False)\nreviews.columns = ['id', 'name', 'asins', 'brand', 'categories', 'keys', 'manufacturer','date', 'dateAdded', 'dateSeen',\n       'didPurchase', 'doRecommend', 'id','numHelpful', 'rating', 'sourceURLs','text', 'title', 'userCity',\n       'userProvince', 'username']","00362100":"reviews.nunique()","3d427b4c":"reviews.isnull().sum()\n#lets drop usernames, userProvince,id,didPurchase","c347eb8a":"reviews.drop(labels=['didPurchase','id','userCity','userProvince'],axis=1,inplace=True)","5fc2aa1b":"reviews.isnull().sum()","67e647f3":"rating_perperson=reviews.username.value_counts()\n#ratings \nprint (\"Total ratings : \" + str(sum(rating_perperson)))\nprint (\"Total users : \" + str(len(rating_perperson)))\nprint(\"Users giving bulk ratings (more than 10) : \" + str(sum(rating_perperson >10)))\nbulk = rating_perperson[rating_perperson >10]\nbulk_rating = sum(bulk)\nprint (\"Bulk ratings : \" + str(bulk_rating))\nprint (\"Populations of bulk ratings : \" + str(bulk_rating*100\/sum(rating_perperson)))\nprint (\"Populations of bulk users : \" + str(sum(rating_perperson >10)*100\/len(rating_perperson)))\nrating_perperson.value_counts().plot(kind='pie',figsize=(10,10), title='Ratings Per User')","3ef52c36":"reviews['bulk']= reviews['username'].apply(lambda x : 1 if x in bulk.index else 0)\n#gives us the category whether a rating is bulk or not\nfrom matplotlib import pyplot\n#series.hist(by=series)\nprint(reviews.rating.hist(by=reviews.bulk))\nprint(reviews[reviews.bulk==1].rating.describe())\nprint(reviews[reviews.bulk==0].rating.describe())\n","6cd866d0":"from matplotlib import pyplot\n%matplotlib inline\n\nstar = reviews.rating.value_counts()\nprint(\"*** Rating distribution ***\")\nprint(star)\nstar.sort_index(inplace=True)\nstar.plot(kind='bar',title='Amazon customer ratings',figsize=(6,6),style='Solarize_Light2')","0282171c":"NPS_score = round (100*((star.loc[5])-sum(star.loc[1:3]))\/sum(star.loc[:]),2)\nprint (\" NPS score of Amazon is : \"  + str(NPS_score))\n","4081d4f6":"kindle = reviews[reviews.name=='Amazon Kindle Paperwhite - eBook reader - 4 GB - 6 monochrome Paperwhite - touchscreen - Wi-Fi - black,,,']","c2d80f6e":"kindle.isnull().sum()\n# The dataset looks good to go","61bec534":"kindle_s = kindle.rating.value_counts()\nkindle_s.sort_index(inplace=True)\n\nKindle_NPS_score = round (100*(kindle_s[5]-sum(kindle_s[1:3]))\/sum(kindle_s),2)\nprint (\" NPS score of Kindle is : \"  + str(Kindle_NPS_score))\n#better NPS than overall amazon\nkindle_s.plot(kind='bar',title='Amazon customer ratings',figsize=(6,6),style='Solarize_Light2')","d4fc8076":"kindle.doRecommend.value_counts()","1d8f1cf8":"kindle.rating.hist(by=kindle.doRecommend,figsize=(12,6))","e7f72a2a":"plus_kindle = kindle[kindle.doRecommend==True].rating.value_counts()\nplus_kindle.sort_index(inplace=True)\nrecomm_NPS = round(100*(sum(plus_kindle[4:5])-sum(plus_kindle[1:2]))\/sum(plus_kindle),2)\nminus_kindle = kindle[kindle.doRecommend==False].rating.value_counts()\nminus_kindle.sort_index(inplace=True)\nnotrecomm_NPS = round(100*(sum(minus_kindle[4:5])-sum(minus_kindle[1:2]))\/sum(minus_kindle),2)\nprint(\"Those who recommend amazon kindle generate high NPS score of \" + str(recomm_NPS))\nprint(\"Those who DO NOT recommend kindle produce a NPS score of \" + str(notrecomm_NPS))\nprint(\" ~ pretty much correct definition of NPS score\")\n","d4e2ca36":"kindle['temp'] = kindle.date.apply(lambda x : pd.to_datetime(x))\nkindle_review_dates = kindle.date.value_counts()\nkindle_review_dates.sort_index(inplace=True)\nkindle_review_dates.plot(kind='area',figsize=(12,6))","6b6a7d4a":"rating_perdate = kindle_review_dates.sort_values(ascending=False)\npeakrating = rating_perdate[:20]\npeak_month=[]\nfor x in peakrating.index:\n    peak_month.append(pd.to_datetime(x).month)\npd.Series(peak_month).value_counts()","fea832ba":"rating_series = pd.DataFrame(kindle.date)\ndforms=[]\nfor x in rating_series.date:\n    dforms.append((pd.to_datetime(x)).value)\n# now we have dforms which has dates transformed to numeric values\nrating2 = rating_series.assign(date_min = dforms)\nrating2.reset_index(inplace=True)\n#rating2.set_index('date_min')\n#rating2.columns=['timestamp_string','review_count','date_min']\nbins = np.linspace(min(rating2.date_min),max(rating2.date_min),num=50)\nrating2.hist(column='date_min', bins=20,figsize=(10,6),)\nrating2.hist(column='date_min', bins=30,figsize=(10,6))\nrating2.hist(column='date_min', bins=50,figsize=(10,6))\n","35d9e393":"def NPS_eval (A):\n    score =0\n    for x in A[:]:\n        if (x>4) :\n            score+=1\n        elif (x<4) :\n            score-=1\n    return 100*score\/len(A)    ","b5bc50b1":"NPS_overtime = kindle[['temp','rating']]\nNPS_overtime.groupby(by='temp').agg(NPS_eval).plot(figsize=(15,10))\n","c871c73c":"NPS_overtime['timeline']= NPS_overtime['temp'].apply(lambda x : (x.month+(12*(x.year-2015))))\nNPS_by_month= NPS_overtime.groupby(by='timeline').agg(NPS_eval)\nprint(NPS_by_month.plot())\nNPS_by_month.sort_values(by='rating')","2df5e9d8":"comments = pd.concat([kindle['text']+\". \"+ kindle['title'],kindle['rating'],kindle['doRecommend']],axis=1)\ncomments.columns=['text','rating','recommend']","10246c23":"import string\nimport nltk\nfrom nltk import PorterStemmer\nimport re \n\nstopwords = nltk.corpus.stopwords.words('english')\nps = PorterStemmer()\nwn = nltk.WordNetLemmatizer()\n\n\ndef clean_stem (sent): \n    temp1 =\"\".join(x for x in sent if x not in string.punctuation)\n    temp2 = re.split('\\W+',temp1.lower())\n    temp3 = [ps.stem(x) for x in temp2 if x not in stopwords]\n    return temp3\n\ndef clean_lemma (sent): \n    temp1 =\"\".join(x for x in sent if x not in string.punctuation)\n    temp2 = re.split('\\W+',temp1.lower())\n    temp3 = [wn.lemmatize(x) for x in temp2 if x not in stopwords]\n    return temp3\n\ntext=\"Hello this is, my happiest place. organize, organizes, and organizing in Happy world ! with happiness ..\\\nso much of happy!! \"\n\nprint(\"Stemmed \" + \"-\".join(clean_stem(text)))\nprint(\"Lemmatized \" + \"-\".join(clean_lemma(text)))","dba23ea0":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectstem = TfidfVectorizer(analyzer=clean_stem)\nvectlemm = TfidfVectorizer(analyzer=clean_lemma)\n\ntextfeatures=vectstem.fit_transform(comments['text'])\nprint(\"Stemmed - \" + str(len(vectstem.get_feature_names())))\n\nvectlemm.fit_transform(comments['text'])\nprint(\"Lemmatized - \" + str(len(vectlemm.get_feature_names())))\n","b4adcd21":"pd.DataFrame(textfeatures.toarray()).head(15)","77e600d2":"textmatrix = pd.DataFrame(textfeatures.toarray(),columns=vectstem.vocabulary_)\ntextmatrix.head(5)","81dcdb8d":"sum_scores = pd.DataFrame(textmatrix.sum(),columns=['sum_scores_TFIDF'])\nsum_scores.head(10)","6a9dce53":"# Need to see most important words in the reviews\n# words used by many people or less frequent in sentences\nsum_scores.sort_values(by='sum_scores_TFIDF',ascending=True)[:5] ","db4e2b02":"#high usage of words in reviews\nsum_scores.sort_values(by='sum_scores_TFIDF',ascending=False)[:5]","231bce97":"pd.set_option('display.max_colwidth', 0)\ncomments.head()","49f04d5c":"from nltk.sentiment.vader import SentimentIntensityAnalyzer\nsid = SentimentIntensityAnalyzer()\n\nhappy = \"I am Happy. this is so awesome. I love life. I will be in heaven\"\n#when you find free food in university\nprint(\"happy \" + str(sid.polarity_scores(text)))\n\n\nsad = \"i hate this. I am mad this is stupid. I will kill you\"\n#when your professor gives you a ZERO in assignment\nprint(\"sad \" + str(sid.polarity_scores(sad)))\n\nneut = \"I will come. You should go. This is blue color\"\n#when you state facts and nothing else\nprint(\"dont care - \" + str(sid.polarity_scores(neut)))\n\nsrishti = \"money\"\nprint(\"dss - \" + str(sid.polarity_scores(srishti)))\n","2a30e7c7":"# Feature 1 : Sentiment compound value\ndef sentiment(x):\n    score = sid.polarity_scores(x)\n    return score['compound']\n    \n#sentiment(happy)\ncomments['sentiment']= comments['text'].apply(lambda x : sentiment(x))","efad188e":"# Feature 2 : Length of string\n\ncomments['length'] = comments['text'].apply(lambda x : len(re.split('\\W+',x)))\ncomments[comments['rating']==5].head(10)\n\n# before we proceed - we need to convert all true >> 1 and false as 0\ndef convert(x):\n    \n    if x==True:\n        return 1\n    else :\n        return 0\n    \nprint(convert(\"False\"))\n\ncomments['target_rec'] = comments['recommend'].apply(lambda x : convert(x))\ncomments.head(5)","bb68a775":"comments[comments['rating']==1].head(5)","bee247a7":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.model_selection import train_test_split\n\n# need to reset index of the comments column to match with textfeatures\nnew_sentiment = comments.sentiment.reset_index()['sentiment']\nnew_length = comments.length.reset_index()['length']\n\nx_features = pd.concat([new_sentiment,new_length,\n                        pd.DataFrame(textfeatures.toarray(),\n                        columns=vectstem.vocabulary_)],axis=1)\nx_train, x_test, y_train, y_test = train_test_split(x_features,comments.target_rec,test_size=0.2)\n\nrf = RandomForestClassifier(n_jobs=-1,n_estimators=50,max_depth=90)\nrfmodel=rf.fit(x_train,y_train)\n\ny_pred = rfmodel.predict(x_test)\nsorted(zip(rfmodel.feature_importances_,x_train.columns),reverse=True)[0:10]\n","8a5c654a":"precision, recall, fscore , support = score(y_test,y_pred,average='binary')\nprint('Precision: {} \/ Recall :{} \/ Accuracy {} '.format(round(precision,3),\n                                                         round(recall,3),\n                                                         round((y_pred==y_test).sum()\/len(y_test),3)))","6e29ca12":"Lets deep dive and pick product to analyse","0ad7eb84":"**What is sentiment analysis ? No idea ? \n**Read the next code block","afcff294":"# 3. Find the NPS net promoter score of amazon\n- What's NPS score ?\n- How do we calculated for amazon  ?","325d0fcc":"![alt text](https:\/\/i.guim.co.uk\/img\/media\/f5da07b449b6dfe3891a8462b44f4050d272880b\/0_0_3200_2360\/master\/3200.jpg?w=620&q=20&auto=format&usm=12&fit=max&dpr=2&s=211109be6ff35dad62c27fae8ff20797)\n\n","a8d0283d":"There are lot of null values and irrelevant columns ","95b13190":"The outline for the project will be as follows- \n1. Understand and clean the data\n    - Check for null values\n    - Drop columns which arent useful\n2. Speculate whether ratings are genuine ?\n    - what if the one user is trying to give all rating ?\n    - How will the distribution look for bulk users ?\n    - How many users are bulk ?\n3. Find the NPS net promoter score of amazon\n    - What's NPS score ?\n    - How do we calculated for amazon  ?\n4. Pick a product and deep dive\n    - We will pick one variation of kindle product drill & analyse its characteristics\n5. [Paper white kindle] - NPS score  ? \n6. [Paper white kindle] - Plot time series for review\n    - How to handle date time text ?\n    - How to plot time series on a graph ? \n    - How does the graph look like in small intervals of 5 days or 10 days or 30 days ?\n    - Did the performance (NPS) go up or down with time ?\n7. [Paper white kindle] Predict Recommendations based on reviews content\n    - Make a clean function\n        - Remove punctuations\n        - Remove stopwords\n        - Stem vs Lemmatize\n    - Create a TFIDF vectorizer\n    - Create Features\n    - Understand and explore sentiment analysis\n        - Use compound feature\n    - Use RandomForestClassifier\n    - Check the score \n        ","e5685407":"### Are the all the reviews given by same group of users ? ","55e95e6d":"# 6. [Paper white kindle] - Plot time series for reviews \n- How to handle date time text ?\n- How to plot time series on a graph ? \n- How does the graph look like in small intervals of 5 days or 10 days or 30 days ?\n- Did the performance (NPS) go up or down with time ?\n","07efc079":"## Can we predict Recommendations with given comments on product ?","c21f8e47":"# NPS Score ( Net promoter score ) ","cde7309b":"# 5. [Paper white kindle] - NPS score  ? ","536a6517":"#### Looks like amazon is really good ","98fc358c":"#### Understand the output\n    - sid.polarity is a dictionary\n    - pos and neg indicates - positive and negative emotions in sentence\n    - we should be interested in compund score which calculates the final effect\n   ","7222cdbe":"# 7. [Paper white kindle] Predict Recommendations based on reviews content\n- Make a clean function\n  - Remove punctuations\n  - Remove stopwords\n  - Stem vs Lemmatize\n- Create a TFIDF vectorizer\n- Create Features\n- Understand and explore sentiment analysis\n    - Use compound feature\n- Use RandomForestClassifier\n- Check the score \n        ","d56ee461":"### Distribution of User rating","5394d60d":"### The scope of the project is to explore the reviews submitted by users and understand in depth about the recommendations","72d33a0e":"# 4. Pick a product and deep dive\n- We will pick one variation of kindle product drill & analyse its characteristics","66c46840":"## CHEERS !","2592b745":"### Lets create vectors from the text columns","6b986901":"### Lets predict recommendation !","4160b84c":"#### Although the pie chart reveals that most of the users have given single rating but its interesting to note following fact\n #### 1 : Only 0.55 % of the users are bulk users\n #### 2 : Around 9 % of the ratings have been submitted by just 0.55% users - Does it seem odd to you ?","87b92b8a":"### Well ! that picture says it all. Now we dont think that bulk users are spam since the have the same rating distribution as others","2499bbfc":"# Amazon Reviews on kindle products","4e844220":"# 2. Speculate whether ratings are genuine ?\n- what if the one user is trying to give all rating ?\n- How will the distribution look for bulk users ?\n- How many users are bulk ?","6bac8645":"### What about recommendations ? How is rating related to recommendation ?","711cf5e3":"#### Insight \n1. January month has the highest number of peaks >> Activity is high >> More Sales during Jan ( We all know)\n2. There is high degree of variance in reviews added over time\n\n","4bff8275":"The column names dont make sense - Need to update them with real words\n    - for this we use vectstem.vocabulary_ to modify the columns","7fd712b4":"Stemmed has features 18 % lower than that of Lemmatized. \n    - In the above example in happy line, you can see how ineffective lemmatization can be \n    - Thus we will be applying cleanstem algo here\n    - Lower features means more information density in the compressed columns","0e5283b3":"#### Net Promoters Score helps us evaluate customer satisfaction and loyalty\n\nRating 1,2,3 - Detractors <br>\nRating 4   - Passive <br>\nRating 5 - Promoters <br>\n\nNPS = (Promoters - Detractors)\/Total ratings * 100","2a43dbc3":"### Lets have a look on our stemmed data","8968a7d6":"# 1. Understand and clean the data\n\n- Check for null values\n- Drop columns which arent useful\n","3e4fa9a4":"### Lets build features on our data\n"}}