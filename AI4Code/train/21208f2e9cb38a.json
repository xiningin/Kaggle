{"cell_type":{"fb1eb0b9":"code","b505076d":"code","d509b35e":"code","cd1885de":"code","da1d6298":"code","5bcf1a68":"code","4174c990":"code","1ec03dd5":"code","6a7fb1a1":"code","13af5721":"code","c7c9b9de":"code","c1a866a8":"code","2bab065d":"code","38358f6a":"code","e8b22482":"code","660680d9":"code","2a84d74d":"code","6f2cdfe0":"code","2d6fd006":"code","563b2cf9":"code","80d59695":"code","5e101af8":"code","8e497ff9":"code","5312fd47":"code","3bf4c263":"code","f665be95":"code","7082c488":"code","bdb7f6fd":"code","3d9bdec5":"code","40c42457":"code","f79d5a1d":"code","2bee8e34":"code","224b4866":"code","cf900e50":"code","2aa18d84":"code","7335ec53":"code","46d2f2b0":"code","92853d90":"code","283e68ab":"code","b5ae9f43":"code","c7ece77f":"code","12e52b0d":"code","92e36560":"code","1055dc5d":"code","da085362":"code","802eea2f":"code","a0c69b2e":"code","30b066ce":"markdown","22fd0368":"markdown","940baf2e":"markdown","618a4bbc":"markdown","2ac40b3a":"markdown","348eb07a":"markdown","598f2f82":"markdown","e7426a6b":"markdown","7191a8f5":"markdown","d75e7bb8":"markdown","39fe0f24":"markdown","25fdc25a":"markdown","0e15aa8d":"markdown","be5c8ab7":"markdown","613cb3f5":"markdown","89efc302":"markdown","2a007c26":"markdown","b1932557":"markdown","6b0fa6c8":"markdown","9dd8fc14":"markdown","250fd01b":"markdown","d4005eea":"markdown","2640fec3":"markdown","147d34e4":"markdown","fc7a34a9":"markdown","e8c9a302":"markdown","49b1c720":"markdown","3ec7c7c3":"markdown","1ce73643":"markdown","edc2410f":"markdown","5b26756c":"markdown","a749ad65":"markdown","8ad0608a":"markdown","d76943ab":"markdown","9028f47b":"markdown","13fffe4b":"markdown","4c2f9856":"markdown"},"source":{"fb1eb0b9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom pytz import timezone\nimport pytz\n\n%matplotlib inline\n\nplt.style.use('fivethirtyeight')\ntrain = pd.read_csv('..\/input\/SolarPrediction.csv')","b505076d":"train.head()","d509b35e":"train.shape","cd1885de":"train.columns","da1d6298":"train.dtypes","5bcf1a68":"#hawaii = timezone('Pacific\/Honolulu')\n\n# Creamos una copia del original\ntrain_origial = train.copy()\ndf = train.copy()\n\ntrain_origial.index = pd.to_datetime(df['UNIXTime'], unit='s')\n#df.index= df.index.tz_localize(pytz.utc).tz_convert(hawaii)\n\ntrain['DateTime'] = train_origial.index\ntrain_origial['DateTime'] = train_origial.index \ntrain.head()","4174c990":"train_radiation = train.drop(['UNIXTime', 'Data', 'Time', 'Temperature','TimeSunRise', 'TimeSunSet',\n                         'Pressure', 'Humidity', 'WindDirection(Degrees)', 'Speed' ], axis=1)","1ec03dd5":"for i in (train_radiation, train_origial):\n    i['year'] = i.DateTime.dt.year\n    i['month'] = i.DateTime.dt.month\n    i['day'] = i.DateTime.dt.day\n    i['Hour'] = i.DateTime.dt.hour","6a7fb1a1":"train_radiation['Day of week'] = train_radiation['DateTime'].dt.dayofweek\ntemp_rad = train_radiation['DateTime']","13af5721":"# Funcion para saber si es fin de semana o no, poco relevante ...\ndef applyer(row):\n    if row.dayofweek == 5 or row.dayofweek == 6:\n        return 1\n    else:\n        return 0\ntemp2 = train_radiation['DateTime'].apply(applyer)\ntrain_radiation['weekend'] = temp2","c7c9b9de":"train_radiation.index = train_radiation['DateTime']","c1a866a8":"train_radiation.head()","2bab065d":"df_rad = train_radiation.drop('DateTime', 1)\nts = df_rad['Radiation']\nplt.figure(figsize= (20,5))\nplt.title('Radiation vs Time')\nplt.xlabel('Time (Year-Month-Day))')\nplt.ylabel('Radiation level')\nplt.plot(ts)","38358f6a":"train_radiation['Date']=pd.to_datetime(train_radiation.DateTime).dt.strftime('%Y-%m-%d')\ntrain_radiation.index = train_radiation.Date\ntrain_radiation.head()","e8b22482":"train_radiation.groupby('month')['Radiation'].mean().plot.bar(figsize = (20,5),\n                                                              title = 'Monthly Average Radiation',\n                                                              fontsize = 14)","660680d9":"temp = train_radiation.groupby(['day'])['Radiation'].mean()\ntemp.plot(figsize = (20,5), title = \"Average per day radiation Month\", fontsize = 14)","2a84d74d":"temp = train_radiation.groupby(['day', 'Hour'])['Radiation'].mean()\ntemp.plot(figsize = (20,5), title = \"Average Radiation per Daily, Hour\", fontsize = 14)","6f2cdfe0":"temp = train_radiation.groupby(['Hour'])['Radiation'].mean()\ntemp.plot(figsize = (20,5), title = \"Average Radiation per Hour\", fontsize = 14)","2d6fd006":"train_radiation.groupby('Day of week')['Radiation'].mean().plot.bar(figsize = (20,6),\n                                                                   title = 'Average radiation per day per week')","563b2cf9":"train_radiation['Timestamp'] = pd.to_datetime(train_radiation.DateTime, format = '%d-%m-%y %H:%M')\ntrain_radiation.index = train_radiation.Timestamp\n\n#Hourly\nhourly = train_radiation.resample('H').mean()\n\n#Daily\ndaily = train_radiation.resample('D').mean()\n\n#Weekly\nweekly = train_radiation.resample('W').mean()\n    \n#Monthly\nmonthly = train_radiation.resample('M').mean()","80d59695":"ig,axs = plt.subplots(4,1)\n\nhourly.Radiation.plot(figsize = (15,8), title = \"Hourly\", fontsize = 14, ax = axs[0])\ndaily.Radiation.plot(figsize = (15,8), title = \"Daily\", fontsize = 14, ax = axs[1])\nweekly.Radiation.plot(figsize = (15,8), title = \"Weekly\", fontsize = 14, ax = axs[2])\nmonthly.Radiation.plot(figsize = (15,8), title = \"Monthly\", fontsize = 14, ax = axs[3])\nplt.tight_layout()","5e101af8":"From = '2016-10-01'\nTo   = '2016-12-01'\n\nhourly = hourly.loc[From:To,:]\ndaily = daily.loc[From:To,:]\nweekly = weekly.loc[From:To,:] \nmonthly = monthly.loc[From:To,:] \n\nig,axs = plt.subplots(4,1)\nhourly.Radiation.plot(figsize = (15,8), title = \"Hourly\", fontsize = 14, ax = axs[0])\ndaily.Radiation.plot(figsize = (15,8), title = \"Daily\", fontsize = 14, ax = axs[1])\nweekly.Radiation.plot(figsize = (15,8), title = \"Weekly\", fontsize = 14, ax = axs[2])\nmonthly.Radiation.plot(figsize = (15,8), title = \"Monthly\", fontsize = 14, ax = axs[3])\nplt.tight_layout()","8e497ff9":"from statsmodels.tsa.stattools import adfuller\n\ndef test_stationarity(df, ts):\n    # Determining rolling statics\n    rolmean = df[ts].rolling(window = 12, center = False).mean()\n    rolstd = df[ts].rolling(window = 12, center = False).std()\n    \n    # Plot rolling statistics\n    orig = plt.plot(df[ts], color = 'blue', label = 'Original')\n    mean = plt.plot(rolmean, color = 'red' , label = 'Promedio')\n    std = plt.plot(rolstd, color = 'black', label = 'Desviacion Estandar')\n    \n    plt.legend(loc = 'best')\n    plt.title('Promedio y Desviacion Estandar para %s' %(ts))\n    plt.xticks(rotation = 45)\n    plt.show(block = False)\n    plt.close()\n    \n    # Perform Dickey-Fuller test:\n    # Null Hypothesis (H_0): time series is not stationary\n    # Alternate Hypothesis (H_1): time series is stationary\n    \n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(df[ts], autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4],\n                         index = ['Test Statistic',\n                                  'p-value',\n                                  '# Lags Used',\n                                  'Number of Observations Used'])\n    for key, value in dftest[4].items():\n        dfoutput['Critical Value (%s)' %key] = value\n    print(dfoutput)","5312fd47":"test_stationarity(df = train_radiation, ts = 'Radiation')","3bf4c263":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nimport statsmodels.api as sm\nfrom statsmodels.tsa.api import Holt, ExponentialSmoothing, SimpleExpSmoothing\nfrom statsmodels.tsa.stattools import acf, pacf\nfrom statsmodels.tsa.arima_model import ARIMA","f665be95":"_train = hourly.loc['2016-10-02':'2016-11-13',:]\nvalid = hourly.loc['2016-11-14': '2016-11-28',:]","7082c488":"_train.head()","bdb7f6fd":"valid.head()","3d9bdec5":"_train.Radiation.plot(figsize=(25,5), title = 'Radiacion Diaria', fontsize=14, label='Train')\nvalid.Radiation.plot(figsize=(25,5), title = 'Radiacion Diaria', fontsize=14, label='Valid')\nplt.xlabel('DateTime')\nplt.ylabel('Radiation')\nplt.legend(loc = 'best')","40c42457":"plt.style.use('default')\nplt.figure(figsize = (16,8))\nsm.tsa.seasonal_decompose(_train.Radiation).plot()\nresult = sm.tsa.stattools.adfuller(_train.Radiation)\nplt.show()","f79d5a1d":"dd = np.asarray(_train.Radiation)\ny_hat =valid.copy()\ny_hat['naive'] = dd[len(dd)- 1]\nplt.figure(figsize = (25,5))\nplt.plot(_train.index, _train['Radiation'],label = 'Train')\nplt.plot(valid.index, valid['Radiation'], label = 'Validation')\nplt.plot(y_hat.index, y_hat['naive'],  label = 'Naive')\nplt.legend(loc = 'best')\nplt.tick_params(axis = 'x', rotation = 45)","2bee8e34":"rmse = sqrt(mean_squared_error(valid['Radiation'], y_hat['naive']))\nrmse","224b4866":"y_hat_holt = valid.copy()\nfit1 = Holt(np.asarray(_train['Radiation'])).fit(smoothing_level = 0.01, smoothing_slope = 0.1)\ny_hat_holt['Holt_linear'] = fit1.forecast(len(valid))\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(25,5))\nplt.plot(_train.index, _train['Radiation'],label = 'Train')\nplt.plot(valid.index, valid['Radiation'], label = 'Validation')\nplt.plot(y_hat.index, y_hat_holt['Holt_linear'], label = 'Holt Linear')\nplt.legend(loc='best')","cf900e50":"rmse = sqrt(mean_squared_error(valid['Radiation'],  y_hat_holt.Holt_linear))\nrmse","2aa18d84":"y_hat_avg2 = valid.copy()\nfit2 = SimpleExpSmoothing(np.asarray(_train['Radiation'])).fit(smoothing_level=0.02,optimized=False)\ny_hat_avg2['SES'] = fit2.forecast(len(valid))\nplt.figure(figsize=(25,5))\nplt.plot(_train['Radiation'], label='Train')\nplt.plot(valid['Radiation'], label='Test')\nplt.plot(y_hat_avg2['SES'], label='SES')\nplt.legend(loc='best')\nplt.show()","7335ec53":"rms = sqrt(mean_squared_error(valid.Radiation, y_hat_avg2.SES))\nprint(\"Error: \", rms)","46d2f2b0":"y_hat_avg = valid.copy()\nfit1 = ExponentialSmoothing(np.asarray(_train['Radiation']), seasonal_periods=4, trend = 'add', seasonal= 'add').fit()\ny_hat_avg['Holt_Winter'] = fit1.forecast(len(valid))\nplt.figure(figsize = (25,5))\nplt.plot(_train.index, _train['Radiation'],label = 'Train')\nplt.plot(valid.index, valid['Radiation'], label = 'Validation')\nplt.plot(y_hat_avg.index, y_hat_avg['Holt_Winter'], label = 'Holt_Winter')\nplt.legend(loc = 'best')","92853d90":"rms = sqrt(mean_squared_error(valid.Radiation, y_hat_avg.Holt_Winter))\nprint(\"error: \", rms)","283e68ab":"def plot_acf_pacf(df, ts):\n  \"\"\"\n  Plot auto-correlation function (ACF) and partial auto-correlation (PACF) plots\n  \"\"\"\n  f, (ax1, ax2) = plt.subplots(2,1, figsize = (10, 5)) \n\n  #Plot ACF: \n\n  ax1.plot(lag_acf)\n  ax1.axhline(y=0,linestyle='--',color='gray')\n  ax1.axhline(y=-1.96\/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n  ax1.axhline(y=1.96\/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n  ax1.set_title('Autocorrelation Function for %s' %(ts))\n\n  #Plot PACF:\n  ax2.plot(lag_pacf)\n  ax2.axhline(y=0,linestyle='--',color='gray')\n  ax2.axhline(y=-1.96\/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n  ax2.axhline(y=1.96\/np.sqrt(len(df[ts])),linestyle='--',color='gray')\n  ax2.set_title('Partial Autocorrelation Function for %s' %(ts))\n  \n  plt.tight_layout()\n  plt.show()\n  plt.close()\n  \n  return","b5ae9f43":"lag_acf = acf(np.array(_train['Radiation']), nlags = 20)\nlag_pacf = pacf(np.array(_train['Radiation']), nlags = 20, method='ols')\n\nplot_acf_pacf(df = _train, ts = 'Radiation')","c7ece77f":"fit2 = sm.tsa.statespace.SARIMAX(_train.Radiation, order=(1,0,1),seasonal_order=(1,1,0,12), trend='ct')\nres = fit2.fit()\ny_hat_avg['SARIMA'] = res.predict(start=\"2016-11-14\", end=\"2016-11-29\", dynamic=True)\nplt.figure(figsize=(20,5))\nplt.plot( _train['Radiation'], label='Train')\nplt.plot(valid['Radiation'], label='Test')\nplt.plot(y_hat_avg['SARIMA'], label='SARIMA')\nplt.legend(loc='best')","12e52b0d":"res.summary()","92e36560":"rms = sqrt(mean_squared_error(valid.Radiation, y_hat_avg['SARIMA']))\nprint('Error:', rms)","1055dc5d":"model = ARIMA(_train.Radiation, order=(1, 0, 1))  \nresults_MA = model.fit()  \nplt.plot(_train.Radiation)\nplt.plot(results_MA.fittedvalues, color='red')","da085362":"results_MA.summary()","802eea2f":"def do_lstm_model(df, \n                  ts, \n                  look_back, \n                  epochs, \n                  type_ = None, \n                  train_fraction = 0.67):\n  \"\"\"\n   Create LSTM model\n   Source: https:\/\/machinelearningmastery.com\/time-series-prediction-lstm-recurrent-neural-networks-python-keras\/\n  \"\"\"\n  # Import packages\n  import numpy\n  import matplotlib.pyplot as plt\n  from pandas import read_csv\n  import math\n  from keras.models import Sequential\n  from keras.layers import Dense\n  from keras.layers import LSTM\n  from sklearn.preprocessing import MinMaxScaler\n  from sklearn.metrics import mean_squared_error\n\n  # Convert an array of values into a dataset matrix\n  def create_dataset(dataset, look_back=1):\n    \"\"\"\n    Create the dataset\n    \"\"\"\n    dataX, dataY = [], []\n    for i in range(len(dataset)-look_back-1):\n      a = dataset[i:(i+look_back), 0]\n      dataX.append(a)\n      dataY.append(dataset[i + look_back, 0])\n    return numpy.array(dataX), numpy.array(dataY)\n\n  # Fix random seed for reproducibility\n  numpy.random.seed(7)\n\n  # Get dataset\n  dataset = df[ts].values\n  dataset = dataset.astype('float32')\n\n  # Normalize the dataset\n  scaler = MinMaxScaler(feature_range=(0, 1))\n  dataset = scaler.fit_transform(dataset.reshape(-1, 1))\n  \n  # Split into train and test sets\n  train_size = int(len(dataset) * train_fraction)\n  test_size = len(dataset) - train_size\n  train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n  \n  # Reshape into X=t and Y=t+1\n  look_back = look_back\n  trainX, trainY = create_dataset(train, look_back)\n  testX, testY = create_dataset(test, look_back)\n  \n  # Reshape input to be [samples, time steps, features]\n  if type_ == 'regression with time steps':\n    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n  elif type_ == 'stacked with memory between batches':\n    trainX = numpy.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n    testX = numpy.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n  else:\n    trainX = numpy.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n    testX = numpy.reshape(testX, (testX.shape[0], 1, testX.shape[1]))\n  \n  # Create and fit the LSTM network\n  batch_size = 1\n  model = Sequential()\n  \n  if type_ == 'regression with time steps':\n    model.add(LSTM(4, input_shape=(look_back, 1)))\n  elif type_ == 'memory between batches':\n    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n  elif type_ == 'stacked with memory between batches':\n    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n    model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n  else:\n    model.add(LSTM(4, input_shape=(1, look_back)))\n  \n  model.add(Dense(1))\n  model.compile(loss='mean_squared_error', optimizer='adam')\n\n  if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n    for i in range(100):\n      model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n      model.reset_states()\n  else:\n    model.fit(trainX, \n              trainY, \n              epochs = epochs, \n              batch_size = 1, \n              verbose = 2)\n  \n  # Make predictions\n  if type_ == 'memory between batches' or type_ == 'stacked with memory between batches':\n    trainPredict = model.predict(trainX, batch_size=batch_size)\n    testPredict = model.predict(testX, batch_size=batch_size)\n  else:\n    trainPredict = model.predict(trainX)\n    testPredict = model.predict(testX)\n  \n  # Invert predictions\n  trainPredict = scaler.inverse_transform(trainPredict)\n  trainY = scaler.inverse_transform([trainY])\n  testPredict = scaler.inverse_transform(testPredict)\n  testY = scaler.inverse_transform([testY])\n  \n  # Calculate root mean squared error\n  trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n  print('Train Score: %.2f RMSE' % (trainScore))\n  testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n  print('Test Score: %.2f RMSE' % (testScore))\n  \n  # Shift train predictions for plotting\n  trainPredictPlot = numpy.empty_like(dataset)\n  trainPredictPlot[:, :] = numpy.nan\n  trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n  \n  # Shift test predictions for plotting\n  testPredictPlot = numpy.empty_like(dataset)\n  testPredictPlot[:, :] = numpy.nan\n  testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n  \n  # Plot baseline and predictions\n  plt.plot(scaler.inverse_transform(dataset))\n  plt.plot(trainPredictPlot)\n  plt.plot(testPredictPlot)\n  plt.show()\n  plt.close()\n  \n  return","a0c69b2e":"# LSTM Network for Regression\ndo_lstm_model(df = train_radiation, \n              ts = 'Radiation', \n              look_back = 1, \n              epochs = 5)","30b066ce":"## **Transformation and visualization of regular data**","22fd0368":"## **Predictions**","940baf2e":"### **Change Index for datetime data type**","618a4bbc":"## **Visualizacion descompuesta por peridos**","2ac40b3a":"### **Generation DataFrame of Radiation**","348eb07a":"### **SARIMAX & ARIMA**","598f2f82":"# EXTRA LSTM\n## Recurrent Neural Networks\n\n  - https:\/\/machinelearningmastery.com\/time-series-prediction-lstm-recurrent-neural-networks-python-keras\/\n  - https:\/\/blog.statsbot.co\/time-series-prediction-using-recurrent-neural-networks-lstms-807fa6ca7f\n  - https:\/\/adventuresinmachinelearning.com\/recurrent-neural-networks-lstm-tutorial-tensorflow\/","e7426a6b":"####  ** Error RMS for Naive Approach**","7191a8f5":"### **Clean index**","d75e7bb8":"### What does the Dickey-Fuller Test tell us?\n\nThis is one of the statistical tests to verify the stationarity. Here the null hypothesis is that the time series is not stationary. The results of the tests include a test statistic (test statistic) and some critical values for the difference confidence levels. If the \"Test Statistic\" is smaller than the \"Critical Value 1%\", the null hypothesis is rejected, therefore the series is stationary.\n\nIn summary,\n\n Critical-Value = cv = -3.43 |\n Test Statistic = ts = -23.77\n\n* H0: It is not stationary; ts> cv\n* H1: It is stationary; ts <cv\n\nTherefore, H0 is rejected, which indicates that the series ** is stationary **.\n\u00a0\nSource: https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/time-series-forecasting-codes-python\/","39fe0f24":"## **Look at stationarity**\n\nIt is assumed that the data of the underlying time series are stationary. This assumption gives us some ** nice ** statistical properties that allow us to use several models for forecasting.\n\nStationary is a statistical assumption that a time series has:\n\n\u00a0\u00a0- __Media constant__\n\u00a0\u00a0- __Constant balance__\n\u00a0\u00a0- __The autocovariedad does not depend on the time__\n\nIn short, if we use past data to predict future data, we must assume that the data will follow the same general trends and patterns as in the past. This general statement is valid for most training data and modeling tasks.\n\nSometimes we need to transform the data to make it stationary. However, this transformation then questions whether these data are really stationary and can be modeled using these techniques.\n\nSource: https:\/\/www.analyticsvidhya.com\/blog\/2015\/12\/complete-tutorial-time-series-modeling\/","25fdc25a":"### **Imports**","0e15aa8d":"The ARIMA forecast for a stationary time series is no more than a linear equation (like a linear regression). The predictors depend on the parameters (p, d, q) of the ARIMA model:\n\n\u00a0\u00a0 - ** Number of AR terms (autoregressive) (p): ** AR terms are only delays of the dependent variable. For example, if p is 5, the predictors for x (t) will be x (t-1) ... .x (t-5).\n\u00a0\u00a0 - ** Number of MA terms (moving average) (q): ** MA terms are delayed forecast errors in the prediction equation. For example, if q is 5, the predictors for x (t) will be e (t-1) ... .e (t-5) where e (i) is the difference between the moving average at the instantaneous moment and the real value.\n\u00a0\u00a0 - ** Number of differences (d): ** are the number of non-seasonal differences, that is, in this case we take the difference of first order.\n   \n\u00a0\u00a0 Source: https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/time-series-forecasting-codes-python\/","be5c8ab7":"## **Initial observations**","613cb3f5":"It can be seen that the most regular data is between October and the end of November beginning of December. Since if we see the graphs above, between December and January there are missing dataframe data, and the same happens in September, so these ranges of analysis are discarded.","89efc302":"### **Time features**","2a007c26":"#### ACF and PACF Plots\n** How do we determine p, d and q? ** For p and q, we can use ACF and PACF graphs (below).\n\n** Autocorrelation function (ACF) **. Correlation between the time series with a delayed version of itself.\n\n** Partial autocorrelation function (PACF) **. Additional correlation explained by each successive lagged term.\n\n** How do we interpret the ACF and PACF graphs? **\n\n- p - Delay value where the PACF graph crosses the upper confidence interval for the first time.\n- q - Delay value where the ACF graph crosses the upper confidence interval for the first time.","b1932557":"### **Simple Exponential Smoothing**","6b0fa6c8":"### **UNIX Time to Datetime Transformation**","9dd8fc14":"## **Hypothesis**","250fd01b":"### **Conclusions**\n\nAs seen in the PACF chart, the largest amount of partial correlation is between 1 and 0, so ** p has a value of 1 **\n\nThe same happens in the ACF chart, so ** has a value of 1 **.","d4005eea":"####  ** Error RMS for Holt Winter**","2640fec3":"## **Radiation Analysis and Data Visualization**","147d34e4":"### **Divide data for training and validation**","fc7a34a9":"## **Data preprocessing**","e8c9a302":"####  ** Error RMS for Holt Linear**","49b1c720":"### **Decomposition by season**","3ec7c7c3":"### **Holt Winter**\n","1ce73643":"### **Naive Approach**","edc2410f":"### **ARIMA(1,0,1)**","5b26756c":"### **Stationary series test function**","a749ad65":"### **Holt Linear**","8ad0608a":"### Finally\nWe include a seasonal effect in an additive way, which means that we add a term that allows the process to depend on the fourth MA delay. It may be that, on the contrary, we want to model a seasonal effect in a multiplicative way. We often write the model then as ARIMA (p, d, q) \u00d7 (P, D, Q) s, where the letters of low intensity indicate the specification for the non-seasonal component, and uppercase letters indicate the specification of the Season component s is the periodicity of the stations (for example, it is often 4 for quarterly data or 12 for monthly data).\n\nAs it says above, it was used as parameter 12, since it is a monthly data","d76943ab":"####  ** Error RMS for Simple Exponentian Smoothing**","9028f47b":"### **SARIMAX(1,0,1)**","13fffe4b":"> ## ** Imports and data loading**","4c2f9856":"### **Preprocessing result**"}}