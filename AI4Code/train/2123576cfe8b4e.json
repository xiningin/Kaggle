{"cell_type":{"12317cc4":"code","c31ee864":"code","9c09bd2d":"code","aa65d1b5":"code","65b08a0d":"code","82861dff":"code","2a8369cd":"code","734adcd3":"code","ed33b8a3":"code","6956839a":"code","accdbb56":"code","9af5a6f1":"code","93e72afb":"code","95ba08f5":"code","9e7ed34a":"code","48955bdb":"code","76fdaa27":"code","ab379bf7":"code","9774d7d9":"code","2dfee454":"code","9e0c0481":"code","76c0888c":"code","6f9884c0":"code","3f3ed9e3":"code","1979e238":"code","8f9efaf4":"code","9b95de10":"code","5c8eb714":"code","dcac820e":"code","1020e1d1":"code","1b660336":"code","fd3e1d11":"code","22abd36d":"code","cd3ae50b":"code","f769a835":"code","2a00a61b":"code","a9a638ef":"code","4883f74c":"code","d8ad4178":"markdown","9e8fb4f5":"markdown","a2e481fd":"markdown","5b280a03":"markdown","39420e7e":"markdown","0c98aab6":"markdown","3d6ed428":"markdown","eae7222c":"markdown","dedf78d0":"markdown","ac209a50":"markdown","4cedc68a":"markdown","ae60f359":"markdown","9bd5451d":"markdown","41463e7c":"markdown","c5ed6204":"markdown","c61635f0":"markdown","b5143d9d":"markdown","382d13b0":"markdown","cc56b6a9":"markdown","27378f2f":"markdown","6d0e2def":"markdown","8a97911a":"markdown","413181e5":"markdown","b90d8ccc":"markdown","39dcca0c":"markdown","2fc82c1d":"markdown","4d3ed82c":"markdown","2c1ed3bf":"markdown","55f60ba6":"markdown","7a48984c":"markdown","a8b64c5a":"markdown","47c4c8a4":"markdown","64c93684":"markdown","22152620":"markdown","a91d8756":"markdown","03de7a62":"markdown","cf0571fc":"markdown","d54a5093":"markdown","f039cb04":"markdown","32df6409":"markdown","45ae7569":"markdown","4bb41a14":"markdown","0d069431":"markdown","2f7472dc":"markdown","a8294290":"markdown","f8772cfe":"markdown","d0e67202":"markdown","3c86f4eb":"markdown","7cab3684":"markdown","efd105b0":"markdown","7f23bf5a":"markdown","e7f7b0f6":"markdown","d76b80b5":"markdown","d71b4d55":"markdown"},"source":{"12317cc4":"# install scispacy\n!pip install scispacy\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\n\n# install langdetect\n!pip install langdetect","c31ee864":"# enable widgets\n!jupyter nbextension enable --py --sys-prefix widgetsnbextension","9c09bd2d":"# Helper packages.\nfrom IPython.core.display import display, HTML\nimport os\nimport pandas as pd\npd.set_option('max_colwidth', 1000)\npd.set_option('max_rows', 100)\nimport numpy as np\nnp.set_printoptions(threshold=10000)\nimport pickle\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport re\nimport json\nfrom tqdm.auto import tqdm\nimport textwrap\nimport importlib as imp\nfrom scipy.spatial.distance import cdist\nimport gc\n\n# Packages with tools for text processing.\n# if you have not downloaded stopwords, run the following line\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nimport nltk\nnltk.download('stopwords')\nimport scispacy\nimport spacy\n\n# Packages for working with text data.\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Packages for getting data ready for and building a LDA model\nimport gensim\nfrom gensim import corpora, models\nfrom gensim.models.coherencemodel import CoherenceModel\nfrom langdetect import detect\n\n# Package for FastText\nimport fasttext\n\n# Other plotting tools.\nimport pyLDAvis\nimport pyLDAvis.gensim\nfrom wordcloud import WordCloud\nfrom IPython.display import display, Markdown, Latex\nimport ipywidgets as widgets\n\n# Print current directory\nprint('Current directory is {}'.format(os.getcwd()))\n\n# Extend notebook to full width\ndisplay(HTML(\"<style>.container {width:100% !important; }<\/style>\"))\n\n# Check python version\nfrom platform import python_version\nprint('Current python version is {}'.format(python_version()))","aa65d1b5":"input_data_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nworking_data_path = '\/kaggle\/input\/cov19-pickles\/'","65b08a0d":"source_column = 'text' # if abstract, change to 'abstract'\nid_colname = 'cord_uid' # id in metadata for each article\nsplit_sentence_by = '(?<=\\\\.) ?(?![0-9a-z])' # sentence splitter","82861dff":"# load from pickle (generated below)\nmeta_full_text = pickle.load(open(working_data_path + 'all_papers.pkl', 'rb'))","2a8369cd":"print(source_column)","734adcd3":"corpus = meta_full_text[source_column]","ed33b8a3":"valid_tokens = pickle.load(open(working_data_path + 'TM_valid_tokens.pkl', 'rb')) # valida tokens after parsing\nX = pickle.load(open(working_data_path + 'TM_X.pkl', 'rb')) # valid tokens with their count","6956839a":"np.random.seed(1)\ntexts = pickle.load(open(working_data_path + 'TM_texts.pkl', 'rb'))\n\ndictionary = gensim.corpora.Dictionary(texts)\nbow_corpus = pickle.load(open(working_data_path + 'TM_bow_corpus.pkl', 'rb'))","accdbb56":"bow_doc_1 = bow_corpus[0]\nprint(corpus[corpus.index[0]])\nfor i in tqdm(range(len(bow_doc_1))):\n    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_1[i][0], dictionary[bow_doc_1[i][0]],bow_doc_1[i][1]))","9af5a6f1":"del X, valid_tokens\ngc.collect()","93e72afb":"limit=20; start=10; step=1;","95ba08f5":"model_list = pickle.load(open(working_data_path + 'TM_model_list.pkl', 'rb'))\ncoherence_values = pickle.load(open(working_data_path + 'TM_coherence_values.pkl', 'rb'))","9e7ed34a":"x = range(start, limit, step)\ntopic_num = x[np.argmax(coherence_values)]\n\nplt.plot(x, coherence_values)\nplt.title(\"Optimal Number of Topics is \" + str(topic_num))\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","48955bdb":"del model_list, coherence_values\ngc.collect()","76fdaa27":"lda_model = pickle.load(open(working_data_path+'TM_lda_model.pkl','rb'))","ab379bf7":"for idx, topic in lda_model.print_topics(-1):\n    print('Topic: {} Word: {}'.format(idx, topic))","9774d7d9":"from IPython.display import HTML\nHTML(filename=working_data_path + 'TM_lda_vis.html')","2dfee454":"cols = ['#029386','#f97306','#ff796c','#cb416b','#fe01b1',\n        '#fd411e','#be03fd','#1fa774','#04d9ff','#c9643b',\n        '#7ebd01','#155084','#fd4659','#06b1c4','#8b88f8',\n        '#029386','#f97306']","9e0c0481":"topics = lda_model.show_topics(num_words=20,num_topics=topic_num,formatted=False)\ncloud = WordCloud(background_color='black',color_func=lambda *args, **kwargs: cols[i],prefer_horizontal=1.0, font_step=1, width=350,height=200)","76c0888c":"# Make word clouds for all topics\nfig, axes = plt.subplots(3, 6, figsize=(25,10), sharex=True, sharey=True)\n\nfor i, ax in tqdm(enumerate(axes.flatten())):\n    if i < len(topics):\n        fig.add_subplot(ax)\n        topic_words = dict(topics[i][1])\n        cloud.generate_from_frequencies(topic_words, max_font_size=50)\n        plt.gca().imshow(cloud)\n        plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n        plt.gca().axis('off')\n    else:\n        ax.axis('off')\n\nplt.subplots_adjust(wspace=0, hspace=0)\nplt.axis('off')\nplt.margins(x=0, y=0)\nplt.tight_layout()\nplt.show()","6f9884c0":"del texts, dictionary, bow_corpus, lda_model\ngc.collect()","3f3ed9e3":"# covid earlist date\ncov_earliest_date = datetime.strptime('2019-12-01', \"%Y-%m-%d\")\n# covid key terms\ncov_key_terms = ['covid\\\\W19','covid19', 'covid', '2019\\\\Wncov', '2019ncov', 'ncov\\\\W2019','sars\\\\Wcov\\\\W2', 'sars\\\\Wcov2', '\u65b0\u578b\u51a0\u72b6\u75c5\u6bd2']\n# covid related terms\ncov_related_terms = '(novel|new)( beta| )coronavirus'","1979e238":"selected_m = 'cbow'\nselected_epoch = 3","8f9efaf4":"print(source_column)\n\nselected_text = 'raw_' + source_column\nmodel_name_suffix = selected_m + '_' + selected_text + '_epoch' + str(selected_epoch)","9b95de10":"search_column = 'abstract' \n\nsearch_text = 'raw_' + search_column\nsearch_name_suffix = selected_m + '_' + search_text + '_epoch' + str(selected_epoch)","5c8eb714":"def get_covid19(data):\n    cov_key_terms_mask = data[source_column].str.lower().str.contains('|'.join(cov_key_terms))\n    cov_related_terms_mask = data[source_column].str.lower().str.contains(cov_related_terms)\n\n    data['WHO_covidence'] = False\n    data.loc[~data['who_covidence_id'].isnull(), 'WHO_covidence'] = True\n\n    data['contain_key_terms'] = False\n    data.loc[cov_key_terms_mask,'contain_key_terms'] = True\n\n    data['contain_related_terms'] = False\n    data.loc[cov_related_terms_mask,'contain_related_terms'] = True\n\n    data['after_earliest_date'] = False\n    data.loc[data.publish_time>= cov_earliest_date,'after_earliest_date'] = True\n\n    covid19 = data[data.contain_key_terms | (data.contain_related_terms & data.after_earliest_date) | (data.WHO_covidence & (data.contain_related_terms | data.after_earliest_date))]\n    covid19.reset_index(drop=True, inplace=True)\n    print(\"There are a total number of {} papers satisfying the above definition\".format(len(covid19)))\n    return covid19","dcac820e":"covid19 = get_covid19(meta_full_text)\nprint(covid19.shape)\ncovid19[:1]","1020e1d1":"del meta_full_text\ngc.collect()","1b660336":"sents_in_paper = pickle.load(open(working_data_path + 'fasttext_model_' + search_column + '_sents_in_paper.pkl', 'rb'))\npaper_lookup = pickle.load(open(working_data_path + 'fasttext_model_' + search_column + '_paper_lookup.pkl', 'rb'))","fd3e1d11":"del covid19\ngc.collect()","22abd36d":"model = fasttext.load_model(working_data_path + 'fasttext_model_' + model_name_suffix)","cd3ae50b":"emb_len = len(model.get_output_matrix()[0])","f769a835":"X = pickle.load(open(working_data_path + 'fasttext_model_' + search_name_suffix + '_X.pkl', 'rb'))","2a00a61b":"# create search\nclass Quicksearch:\n    def __init__(self, modl, emb_len, sentences, sentence_embeddings, paper_lookup):\n        self.modl = modl\n        self.emb_len = emb_len\n        self.sentences = sentences\n        self.sentence_embeddings = sentence_embeddings\n        self.paper_lookup = paper_lookup\n    def get_candidate_ranking(self, sent):\n        y = self.modl.get_sentence_vector(sent)\n        scores = cdist(self.sentence_embeddings,[y],'cosine').ravel()\n        return list(zip(list(self.sentences.keys()), scores))\n    def term(self, init, placeholder, description):\n        return widgets.Textarea(value=init, \n                                placeholder=placeholder, \n                                description=description, \n                                layout=widgets.Layout(width='90%', display='flex'))\n    def sort(self, init, options, description):\n        return widgets.Dropdown(options=options,\n                                  value=init,\n                                  description=description, \n                                  layout=widgets.Layout(width='90%', display='flex'))\n    def top(self, init, maxx, description):\n        return widgets.IntSlider(min=1, \n                                 max=maxx, \n                                 value=init, \n                                 description=description, \n                                 layout=widgets.Layout(width='90%', display='flex'))\n    def search(self, term, sort_by, show_top):\n        if term == '':\n            print('')\n        else:\n            term = term.lower()\n            sent_rank, paper_rank, final_result = [], dict(), []\n            \n            # get ranking for search results\n            ranked_sentences = sorted(self.get_candidate_ranking(term), key=lambda x:x[1])\n\n            # for each sentence, record content, rank, order in paper\n            # for each paper, record highest ranked sentence\n            for i, ranked_sentence in enumerate(ranked_sentences):\n                if i < show_top:\n                    sentence = ranked_sentence[0]\n                    score = ranked_sentence[1]\n\n                    r = dict()\n                    r['rank'] = i + 1\n                    r['sentence'] = sentence\n                    r['paper id'] = self.sentences[sentence][0]\n                    r['sentence_order'] = self.sentences[sentence][1]\n                    sent_rank.append(r)\n                    \n                    #record highest ranking sentence\n                    if self.sentences[sentence][0] not in paper_rank: \n                        paper_rank[self.sentences[sentence][0]] = i + 1\n    \n            # for each paper, lookup information on that paper\n            for key, group in pd.DataFrame(sent_rank).groupby('paper id'):\n                r = dict()\n                r['rank'] = paper_rank[key]\n                r['publish_time'] = self.paper_lookup[key]['publish_time']\n                r['title'] = self.paper_lookup[key]['title']\n                r['journal'] = self.paper_lookup[key]['journal']\n                r['url'] = self.paper_lookup[key]['url']\n                r['topic'] = self.paper_lookup[key]['topic']\n                r['sentences'] = [sent for sent, order in sorted(zip(group['sentence'].values, group['sentence_order'].values), key=lambda r: r[1])]\n                final_result.append(r)\n            final_result = pd.DataFrame(final_result)\n            \n            # print search results\n            if_ascend = False if sort_by == 'publish_time' else True\n            \n            print('Search Results for ' + '\\033[1m' + '\"' + term.upper() + '\\033[0m' + '\"\\n')\n            \n            for k,r in final_result.sort_values(by=[sort_by], ascending=if_ascend).iterrows():\n                r['url'] = \"\" if r['url'] is None else r['url']\n                r['journal'] = 'NA' if r['journal'] is None else r['journal']\n                r['publish_time'] = '' if pd.isnull(r['publish_time']) else datetime.strftime(r['publish_time'], '%Y-%m-%d')\n                r['sentences'] = '...'.join(r['sentences'])\n                \n                print('\\033[1m' + r['title'] + '\\033[0m')\n                print('\\033[1m' + 'Results: ' + '\\033[0m' + r['sentences'])\n                print('\\033[1m' + 'Publish Time: ' + '\\033[0m' + r['publish_time'])\n                print('\\033[1m' + 'Journal: ' + '\\033[0m' + r['journal'])\n                print('\\033[1m' + 'Link: ' + '\\033[0m' + r['url'])\n                print('\\n')","a9a638ef":"# define quicksearch\nquicksearch = Quicksearch(model, emb_len, sents_in_paper, X, paper_lookup)\n\n# set up init options\ninit_show = 10\ninit_max = 100\ninit_sort = 'publish_time'\ninit_search = 'incubation period'\ninit_options = {'Most Recent': 'publish_time', 'Most Similar': 'rank'}\n\n# set up widget\nterm = quicksearch.term(init=init_search, placeholder='', description='Search: ')\nsort_by = quicksearch.sort(init=init_sort, options=init_options, description='Sort By: ')\nshow_top = quicksearch.top(init=init_show, maxx=init_max, description='Filter # of Sentences to Show: ')\nshow_top.style.handle_color='darkred'\nterm.style.description_width = '100px'\nsort_by.style.description_width = '100px'\nshow_top.style.description_width = '180px'\n\nsearch = widgets.interactive(quicksearch.search, \n                             term = term, \n                             sort_by = sort_by, \n                             show_top = show_top)","4883f74c":"display(search)","d8ad4178":"Our goal in this section is to build a search tool that uses sentence embeddings to rank sentences by their cosine similarity to that of a search term. We proceed as follows:\n1. Get covid-related literature and split them into sentences to apply word embeddings upon\n2. Train fastText model on the entire literature to get word embeddings\n3. Pre-calculate sentence embeddings using those word embeddings \n4. Build search tool to rank sentences by their cosine similarity to the query term","9e8fb4f5":"    # 2. parse publish time to datetime object\n    def str2time(s):\n        try:\n            return datetime.strptime(s, '%Y-%m-%d')\n        except:\n            try:\n                return datetime.strptime(s, '%Y %B')\n            except:\n                try:\n                    return datetime.strptime(s, '%Y %b')\n                except:\n                    try:\n                        return datetime.strptime(s, '%Y %B %b')\n                    except:\n                        try:\n                            return datetime.strptime(s, '%Y %b %d') \n                        except:\n                            try:\n                                return datetime.strptime(s, '%Y')\n                            except:\n                                return pd.NaT\n                            return pd.NaT\n                        return pd.NaT\n                    return pd.NaT\n                return pd.NaT\n            return pd.NaT\n        return pd.NaT\n\n\n    metadata['full_text_file_path'] = None\n    for i in tqdm(metadata.index):\n        row = metadata.iloc[i,:]\n        full_text_file_path = []\n\n        # 3. get full path to .pdf or .pmc: prioritize pdf as source, if none, search for pmc\n        if row.pdf_json_files: \n            full_text_file_path.extend([path.strip() for path in row.pdf_json_files.split(';')])\n        else:\n            if row.pmc_json_files:\n                full_text_file_path.extend([path.strip() for path in row.pmc_json_files.split(';')])\n\n        if row.publish_time is None: row.publish_time = ''\n        publish_time = re.sub(' ([a-zA-Z]{3}-[a-zA-Z]{3})|(Spring)|(Summer)|(Autumn)|(Fall)|(Winter)','', row.publish_time).strip()\n        publish_time = str2time(publish_time)\n\n        metadata.loc[i,'publish_time'] = publish_time\n        metadata.loc[i,'full_text_file_path'] = full_text_file_path","a2e481fd":"    arr = X.toarray()\n    texts = []\n    for i in tqdm(range(arr.shape[0])):\n        text = []\n        for j in range(arr.shape[1]):\n            occurrence = arr[i,j]\n            if occurrence > 0:\n                text.extend([valid_tokens[j]] * occurrence)\n        texts.append(text)\n\n    pickle.dump(texts, open(working_data_path + 'TM_texts.pkl', 'wb'))","5b280a03":"### 3. Pre-calculate sentence embeddings using those word embeddings","39420e7e":"    # 9. drop redundant columns and save to pickle\n    meta_full_text.drop(['sha', 'pmcid', 'pubmed_id', 's2_id', 'license', 'mag_id', 'arxiv_id', 'pdf_json_files', 'pmc_json_files', 'full_text_file_path', 'full_text'], inplace=True, axis=1)\n\n    print(meta_full_text.shape)\n    print(meta_full_text.columns)\n    print('number of unique cord_uid is {}'.format(len(meta_full_text.cord_uid.unique())))\n\n    pickle.dump(meta_full_text, open(working_data_path + 'all_papers.pkl', 'wb'))","0c98aab6":"### 2.Train fastText model on the entire literature to get word embeddings","3d6ed428":"The first thing that we want to do is just to explore the themes of this corpus - figuring out what the main topics are there in this literature. We can use LDA to do that. Note that the `source_column` that we are using here is column `text` - so mostly full text and in some cases abstract when full text is not available. This should give us a comprehensive coverage of all tokens provided in the literature. \n\nHere are the steps for this data exploration:\n1.     build tokenizer to parse out valid tokens and get their count with `CountVectorizer`\n1.     find the optimal number of topics such that overall, tokens are assigned as few topics as possible and documents are assigned as few topics as possible\n1.     visualize topic representations for the optimal number of topics","eae7222c":"    # Test tokenizer\n    sentence_test = '($2196.8)\/case (in)fidelity \u03bcg \u03bcg\/ml a=b2 www.website.org \u03b1-gal 2-len a.'\n    spacy_tokenizer(sentence_test)","dedf78d0":"## Install and Load Packages","ac209a50":"We calculate sentence embeddings for sentences in abstract, so all of our search results are sentences from abstract only. We can switch to `'text'` if we want to search all text. ","4cedc68a":"    # create file with individual sentence on each line\n    file = open(working_data_path + 'fasttext_model_' + source_column + '_by_sentence.txt', 'w', encoding='utf-8')\n    for txt in filter(None, corpus.values):\n        file.write('\\n'.join(re.split(split_sentence_by, txt)))\n    file.close()\n\n    # run model\n    model = fasttext.train_unsupervised(working_data_path + 'fasttext_model_' + source_column + '_by_sentence.txt', \n                                        model = selected_m, \n                                        epoch = selected_epoch)\n    model.save_model(working_data_path + 'fasttext_model_' + model_name_suffix)","ae60f359":"*code to generate visualization*","9bd5451d":"    # 1. import metadata\n    metadata = pd.read_csv(input_data_path + 'metadata.csv', encoding='utf-8').replace({np.nan: None})\n    print(metadata.shape)\n    print('\\nNumber of NA for each column: ')\n    metadata.isnull().sum(axis=0)","41463e7c":"## Data Preprocessing","c5ed6204":"    def compute_coherence_values(dictionary, corpus, texts, limit, start = 2, step = 3):\n        coherence_values = []\n        model_list = []\n        for num_topics in tqdm(range(start, limit, step)):\n            model = gensim.models.LdaMulticore(corpus = corpus, id2word = dictionary, num_topics = num_topics, random_state = 1)\n            model_list.append(model)\n            coherencemodel = CoherenceModel(model = model, texts = texts, dictionary = dictionary, coherence = 'c_v')\n            coherence_values.append(coherencemodel.get_coherence())\n            print('Number of topics: {}, Coherence value: {}'.format(num_topics, coherencemodel.get_coherence()))\n\n        return model_list, coherence_values\n\n    model_list, coherence_values = (compute_coherence_values(dictionary = dictionary, \n                                                             corpus = bow_corpus,\n                                                             texts = texts, \n                                                             start = start, limit = limit, step = step))\n\n    pickle.dump(model_list, open(working_data_path + 'TM_model_list.pkl', 'wb'))\n    pickle.dump(coherence_values, open(working_data_path + 'TM_coherence_values.pkl', 'wb'))","c61635f0":"## Search Covid Literature using `fastText` embeddings","b5143d9d":"### 2. find the optimal number of topics","382d13b0":"### 1. Get covid-related literature and split them into sentences to apply word embeddings upon","cc56b6a9":"    # get sentence embeddings\n    X = pd.DataFrame(pd.np.empty((len(list(sents_in_paper.keys())), emb_len)))\n\n    i = 0\n    for sent in tqdm(list(sents_in_paper.keys())):\n        X.iloc[i] = model.get_sentence_vector(sent)\n        i+=1\n\n    pickle.dump(X, open(working_data_path + 'fasttext_model_' + search_name_suffix + '_X' + '.pkl', 'wb'))","27378f2f":"    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics = topic_num, id2word = dictionary, workers = 4, passes = 2)\n    print(lda_model)\n    pickle.dump(lda_model, open(working_data_path+'TM_lda_model.pkl','wb'))","6d0e2def":"We need `texts` (tokenized texts with repetition), `dictionary` (map from word IDs to words) and `bow_corpus` (count by word ID) before training LDA models, which is generated by the next 4 cells.","8a97911a":"remove redundant variables:","413181e5":"## Set Directories","b90d8ccc":"## Set Global Variables","39dcca0c":"    def sentence_to_paper(df, id_colname, text_colname, topic_colname_prefix, split_sentence_by):\n        # link sentences to a paper: sents_in_paper\n        sents_in_paper = dict()\n        papers = [(paper[id_colname], paper[text_colname]) if paper[text_colname] is not None else (paper[id_colname], \"\") for paper in df.to_dict(orient='row')]\n        sents = [(paper[0], re.split(split_sentence_by, paper[1])) for paper in papers]\n        sent_order = 1\n        for pair in np.concatenate([list(zip(id, sent)) for id, sent in [([sent[0]]*len(sent[1]),sent[1]) for sent in sents]]):\n            sent = pair[1]\n            if sent not in sents_in_paper:\n                sents_in_paper[sent] = (pair[0], sent_order)\n                sent_order += 1\n\n        # lookup paper information: paper_lookup        \n        paper_lookup = dict()\n        for paper in df.to_dict(orient='records'):\n            id = str(paper[id_colname])\n            if id not in paper_lookup:\n                paper[topic_colname_prefix] = dict((k, paper[k]) for k in paper.keys() if k.startswith(topic_colname_prefix))\n                paper_lookup[id] = paper    \n\n        return sents_in_paper, paper_lookup\n\n\n    sents_in_paper, paper_lookup = sentence_to_paper(covid19, \n                                                     id_colname=id_colname, \n                                                     text_colname=search_column, \n                                                     topic_colname_prefix='topic', \n                                                     split_sentence_by=split_sentence_by)\n\n    pickle.dump(sents_in_paper, open(working_data_path + 'fasttext_model_' + search_column + '_sents_in_paper.pkl', 'wb'))\n    pickle.dump(paper_lookup, open(working_data_path + 'fasttext_model_' + search_column + '_paper_lookup.pkl', 'wb'))","2fc82c1d":"remove redundant variables before proceeding:","4d3ed82c":"First, we get covid-related literature:","2c1ed3bf":"Next we split them into sentences, and for each sentence we generate a dictionary to lookup paper-level info. ","55f60ba6":"Now we try to find the optimal model by trying various numbers of topics, from 10 to 20, and comparing their coherence scores `coherence_values`.","7a48984c":"Visualize wordclouds:","a8b64c5a":"### 1. Parse out valid tokens and get their count","47c4c8a4":"I train the fastText model with `cbow` (instead of `skipgram`) and number of epochs equaling 3:","64c93684":"*code to generate `text`*","22152620":"* the paper contains *covid key terms* anywhere in the paper\n* the paper contains *covid related terms* anywhere in the paper AND is published after *2019-12-01*\n* the paper is marked as *WHO #Covidence* AND, EITHER contains *related terms* OR is published after *2019-12-01*","a91d8756":"    dictionary = gensim.corpora.Dictionary(texts)\n    bow_corpus = [dictionary.doc2bow(doc) for doc in texts]\n    pickle.dump(bow_corpus, open(working_data_path + 'TM_bow_corpus.pkl', 'wb'))","03de7a62":"    # Load SpaCy for lemmatization\n    nlp_lg = spacy.load('en_core_sci_lg',disable=['tagger', 'parser', 'ner'])\n    nlp_lg.max_length = np.max([len(t) for t in corpus.values])\n\n\n    # Establish stop words\n\n    # default stop words\n    stop_words=stopwords.words('english')\n\n    # custom CORD19 stop words, mostly from Daniel Wolffram's submission \"Topic Modeling: Finding Related articles\"\n    cord_stopwords = ['doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', 'rights', 'reserved', \n                      'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI','-PRON-',\n                      'abstract']\n    # all stop words\n    for word in tqdm(cord_stopwords):\n        if (word not in stop_words):\n            stop_words.append(word)\n        else:\n            continue\n\n    # update SpaCy stop words list\n    for w in tqdm(stop_words):\n        nlp_lg.vocab[w].is_stop = True\n\n    # Build tokenizer\n    def spacy_tokenizer(sentence):\n\n        # removes substrings before it's tokenized and stemmed\n        def removeParenthesesNumbers(v):\n            char_list_rm = ['[(]','[)]','[\u2032\u00b7]']\n            char_list_rm_spc = [' no[nt]-',' non', ' low-', ' high-']\n            v = re.sub('|'.join(char_list_rm), '', v)\n            v = re.sub('|'.join(char_list_rm_spc), ' ', v)\n            return(v)\n\n        sentence = removeParenthesesNumbers(sentence)\n        tokenized_list = []\n        sentence_letters_only = re.sub('[^a-zA-Z]', '', sentence).strip()\n\n        if sentence_letters_only!=\"\":\n            lang = detect(sentence)\n\n            if lang=='en': # only focus on english literature\n                # define types of tokens that should be removed using regex\n                token_rm = ['(www.\\S+)','(-[1-9.])','([\u223c\u2248\u2265\u2264\u2266\u2a7e\u2a7d\u2192\u03bc]\\S+)','(\\S+=\\S+)','(http\\S+)']\n                tokenized_list = [word.lemma_ for word in nlp_lg(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space)]\n                tokenized_list = [word for word in tokenized_list if not re.search('|'.join(token_rm),word)]\n                tokenized_list = [word for word in tokenized_list if len(re.findall('[a-zA-Z]',word))>1]\n                tokenized_list = [word for word in tokenized_list if re.search('^[a-zA-Z0-9]',word)]\n        return tokenized_list","cf0571fc":"This `meta_full_text` is generated by the next 5 cells:\n1.     import `metadata`\n1.     parse publish time to datetime object\n1.     get full path to .pdf or .pmc\n1.     get full text\n1.     drop records with empty abstract and empty body text\n1.     check duplicated text: most likely due to publications on different journals in which case we keep the latest\n1.     check duplicated cord_uid: most likely due to publications on different journals in which case we keep the latest\n1.     drop redundant columns and save to pickle","d54a5093":"Print Topics:","f039cb04":"    # 4. extract full text from JSON files\n    def get_paper_info(json_data):\n        return ' '.join([t['text'] for t in json_data['body_text']])\n\n    full_text = []\n    for r in tqdm(metadata.to_dict(orient='records')):\n        record = []\n        for p in r['full_text_file_path']:\n            with open(input_data_path + p, 'r', encoding='utf-8') as f:\n                data = json.load(f)\n                record.append(get_paper_info(data))\n        full_text_ = '\\n'.join(np.unique(record)) if len(record) > 0 else None\n        full_text.append(full_text_)\n    metadata['full_text'] = full_text\n\n    # 5. drop records with empty abstract AND empty full text\n    meta_full_text = metadata\n    meta_full_text[source_column]= np.where(meta_full_text['full_text'].isnull(), meta_full_text['abstract'], meta_full_text['full_text'])\n    meta_full_text = meta_full_text.dropna(subset = [source_column]).reset_index(drop=True)\n\n    # 6. check duplicated text: most likely due to publications on different journals - in which case we keep the latest one\n    print('In total, {} of the rows have a duplicated {} column, and there are a total of {} duplicated {} entries.'.format(sum([len(g) for k, g in meta_full_text.groupby(source_column) if len(g) > 1]), source_column, len([1 for k, g in meta_full_text.groupby(source_column) if len(g) > 1]), source_column))\n    meta_full_text = meta_full_text.sort_values('publish_time', ascending=False).drop_duplicates(source_column)\n\n    # 7. check duplicated cord_uid: most likely due to publications on different journals - in which case we keep the latest one\n    print('In total, {} of the rows have a duplicated {} column, and there are a total of {} duplicated {} entries.'.format(sum([len(g) for k, g in meta_full_text.groupby(id_colname) if len(g) > 1]), id_colname, len([1 for k, g in meta_full_text.groupby(id_colname) if len(g) > 1]), id_colname))\n    meta_full_text = meta_full_text.sort_values('publish_time', ascending=False).drop_duplicates(id_colname)\n\n    # 8. remove metadata from memory to clear space\n    del metadata\n    gc.collect()","32df6409":"We generate word embeddings from sentences in **all text** - full text, or abstract if full text is not available.","45ae7569":"## Topic Modeling","4bb41a14":"In this last section, we uses `fastText` to generate word embeddings for tokens in the existing literature and calculate sentence embeddings for sentences in covid related literature. We define covid-related literature to be those that satisfy at least one of the conditions below:","0d069431":"    vis = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n    pyLDAvis.display(vis)\n    pyLDAvis.save_html(vis, working_data_path + 'TM_lda_vis.html')","2f7472dc":"#### Submission for COVID-19 Open Research Dataset Challenge (CORD-19)","a8294290":"remove redundant variables:","f8772cfe":"`valid_tokens` and `X` are generated by the next 3 cells","d0e67202":"We get the model with the largest coherence score:","3c86f4eb":"    # Initialize `CountVectorizer`. Remove common and sparse terms\n    vec = CountVectorizer(max_df = .8, min_df = .001, tokenizer = spacy_tokenizer)\n\n    # Transform the list of snippets into DTM.\n    X = vec.fit_transform(tqdm(corpus))\n\n    valid_tokens = vec.get_feature_names()\n\n    # pickle\n    pickle.dump(X, open(working_data_path + 'TM_X.pkl', 'wb'))\n    pickle.dump(valid_tokens, open(working_data_path + 'TM_valid_tokens.pkl', 'wb'))","7cab3684":"Before optimizing the model, we delete redundant variables.","efd105b0":"### 4. Build search tool to rank sentences by their cosine similarity to the query term","7f23bf5a":"Visualize topic modeling results:","e7f7b0f6":"### 3. visualize topic representations for the optimal number of topics","d76b80b5":"*code to generate `dictionary` and `bow_corpus`*","d71b4d55":"# Understanding Covid 19 with Topic Modeling and Sentence Embeddings"}}