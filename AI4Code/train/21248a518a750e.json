{"cell_type":{"3ced43ec":"code","ab1143b6":"code","1e64e060":"code","8fe1c1bc":"code","24c6eb4b":"code","e6755dc6":"code","865d763c":"code","e58e78fd":"code","27f48146":"code","9b1e4834":"code","1cebb7ab":"code","cf36a328":"code","dc3eeffe":"code","594431a0":"code","ca7c2d63":"code","92596e32":"code","1bd95fba":"code","26a4734d":"code","0f9f77b6":"code","9f91dded":"code","2591a99e":"code","a7d725be":"code","45af78c3":"code","19ce7209":"markdown","8b5d1447":"markdown","07efebfb":"markdown","a9a4ff5c":"markdown","af0d90e9":"markdown","fa1aa873":"markdown","0e70a000":"markdown","79de56df":"markdown"},"source":{"3ced43ec":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab1143b6":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\", index_col='id')\nprint(train.shape)\ntrain.head()","1e64e060":"train.info()","8fe1c1bc":"train.describe()","24c6eb4b":"test = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\", index_col='id')\nprint(test.shape)\ntest.head()","e6755dc6":"test.info()","865d763c":"test.describe()","e58e78fd":"import matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(20, 2.5))\nfig.subplots_adjust(hspace=0.6, wspace=0.3)\n\nj = 1\n\nfor i in range(0, 15):\n    ax = fig.add_subplot(1, 3, j)\n    ax.hist(train.iloc[:, 10+i], bins=30)\n    ax.set_title(train.columns[10+i])\n    #ax.set_xlabel(train.columns[10+i])\n    ax.set_ylabel(\"freq\")\n    #ax.set_ylim(0,100)\n    j +=1\n    \n    if j<=3:\n        continue\n    \n    else:\n        plt.show()\n        j = 1\n        fig = plt.figure(figsize=(20, 2.5))\n        fig.subplots_adjust(hspace=0.6, wspace=0.3)\n        \nplt.show()","27f48146":"fig = plt.figure(figsize=(20, 2.5))\nfig.subplots_adjust(hspace=0.6, wspace=0.3)\n\nj = 1\n\nfor i in range(0, 14):\n    ax = fig.add_subplot(1, 3, j)\n    ax.hist(test.iloc[:, 10+i], bins=30)\n    ax.set_title(test.columns[10+i])\n    #ax.set_xlabel(train.columns[10+i])\n    ax.set_ylabel(\"freq\")\n    #ax.set_ylim(0,100)\n    j +=1\n    \n    if j<=3:\n        continue\n    \n    else:\n        plt.show()\n        j = 1\n        fig = plt.figure(figsize=(20, 2.5))\n        fig.subplots_adjust(hspace=0.6, wspace=0.3)\n        \nfor s in range(j, 4):\n    ax = fig.add_subplot(1, 3, s)\n        \nplt.show()","9b1e4834":"from sklearn.model_selection import train_test_split\n\nX_full = train.drop(['target'], axis=1)\ny = train['target']\nX_full_dummy = pd.get_dummies(X_full)\nX_test_dummy = pd.get_dummies(test)\n\nX_train_full, X_valid_full, y_train, y_valid = train_test_split(X_full_dummy, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)","1cebb7ab":"!pip install pycaret","cf36a328":"from pycaret.regression import *\n\ntrain_ml = pd.concat([X_train_full, y_train], axis=1)\nexp1 = setup(train_ml, target = \"target\", session_id=1, normalize=True,\n            silent=True, fold=5, n_jobs=-1)","dc3eeffe":"compare_models()","594431a0":"selected_model = create_model('catboost')","ca7c2d63":"tuned_model = tune_model(selected_model, fold=10, early_stopping=True)","92596e32":"tuned_model.get_all_params()","1bd95fba":"plot_model(tuned_model)","26a4734d":"evaluate_model(tuned_model)","0f9f77b6":"predict_model(tuned_model)","9f91dded":"import matplotlib.pyplot as plt\n\npre = predict_model(tuned_model)\npre\nplt.scatter(x=pre[\"target\"], y=pre[\"Label\"])\n\nplt.title(\"Prediction of validation data\")\nplt.xlabel('Obserbed')\nplt.ylabel('Predicted')","2591a99e":"final_model = finalize_model(tuned_model)\nprint(final_model)","a7d725be":"unseen_predictions = predict_model(final_model, data=X_test_dummy)\nprint(unseen_predictions .shape)\nunseen_predictions.head()","45af78c3":"output = pd.DataFrame({'id': unseen_predictions.index,\n                       'target': unseen_predictions[\"Label\"]})\noutput.to_csv('submission1.csv', index=False)","19ce7209":"# 4. Searching best model","8b5d1447":"# 6. Predicting unseen data","07efebfb":"# 3. Preparing data","a9a4ff5c":"# 1. Reading data","af0d90e9":"## 2-1. Histogram of train data","fa1aa873":"# 2. Observing features","0e70a000":"# 5. Predicting validation data","79de56df":"## 2-2. Histogram of test data"}}