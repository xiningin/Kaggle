{"cell_type":{"8d197a8c":"code","5e88fe6e":"code","ef50738a":"code","1c6e65ec":"code","04288aa3":"code","13b3416c":"code","11950a71":"code","29b8bb5e":"code","1ad79e27":"code","9bc9c472":"code","f4ff9e9e":"code","74e53f5a":"code","543cffe8":"code","4a77848a":"code","874c3b1a":"code","ae39f156":"code","6cc2414a":"code","319b1381":"code","924441e3":"code","d42937cc":"code","671fa033":"code","e32c446d":"code","2e4f8c8e":"code","fef4aa8a":"code","83eef7aa":"code","60c7543b":"code","91f8016e":"code","f9d9eb82":"code","6d30bef6":"code","0953b480":"code","160cda88":"code","96144d8a":"code","22275ddf":"code","3d323f38":"code","c85a9e46":"code","8f0e6263":"code","bd215caa":"code","58812a44":"code","300ed098":"code","c76fe30d":"code","7a6d5f14":"code","feed0821":"code","7c16cb94":"code","c991a3eb":"code","95f2612b":"code","a357e996":"code","ca84d061":"code","e1b5f683":"code","3c062ee7":"code","52bb6a40":"code","2e510aa8":"code","3333d471":"code","f7929604":"code","eabd8c6b":"code","c3a50c98":"code","a1b9c435":"code","15d39792":"code","58222c49":"code","6bc4d2d8":"code","bd5dc0c3":"code","3f27398e":"code","81bca9de":"code","91c96389":"code","11a34847":"code","e8d8d25c":"code","950bfbb1":"code","af467fc8":"code","81267dec":"code","10ba070f":"code","94d97cfb":"code","bc19501d":"code","6024c673":"code","f3c21b28":"code","5be54aaa":"code","ffabedbe":"code","c8712d0a":"code","2854d45e":"code","550cb7a7":"code","7d75bd6d":"code","c9141f85":"code","d9299cef":"code","00d49a80":"code","946fb149":"code","ce1c63ea":"code","c2e02a88":"code","1f632c9d":"code","99303a14":"code","6b1ce10a":"code","b9f8c17e":"code","1a0480c7":"code","2b4befdf":"code","61fe97d6":"code","c3e54c4f":"code","70cb0b9f":"code","eed39231":"code","ffed7abd":"code","8b24da76":"code","36188198":"code","c5286150":"code","7ce42078":"code","3539db35":"code","ede71f48":"code","a2b65283":"code","adf828ac":"code","6c38c3b6":"code","99697d43":"code","8f59bff0":"code","d734dd11":"code","5d3ef766":"code","8576a987":"code","98c8b07f":"code","af24ad28":"code","1606bc4e":"code","3166f609":"code","afb1056a":"code","2e40f751":"code","46c70a9a":"code","70e9ae4b":"code","79491427":"code","27245aa4":"code","7a579646":"code","3ef969ba":"code","4be9b14f":"code","38da1ade":"code","5f8e1642":"code","e5a648ab":"code","25330616":"code","9377ef34":"code","6c508361":"code","08eda68f":"code","badca73f":"code","a0a4ab9e":"code","d0c3af8d":"code","4597544b":"code","25f2a0f5":"code","25bdf03e":"code","7e00d824":"code","e78dfc3d":"code","d98f1e73":"code","9ccfeea9":"code","f126740d":"code","f780c211":"code","739502fc":"code","b080bcd7":"code","919feef2":"code","1fe68138":"code","9638bcef":"code","1f774c1e":"code","aabd4823":"code","02ac9701":"code","ff8611b9":"code","7061341a":"code","4ccbcd4b":"code","380cfe19":"code","2a91f6f2":"code","4a958a49":"code","e41e99d7":"code","2a4ca5d4":"code","5dec1829":"code","3180b48b":"code","e262b5af":"code","1f7eb04b":"code","fa9940bf":"code","6deefef3":"code","e968a505":"code","7088f0ee":"code","108d5d3a":"code","2303c0ae":"code","a69b700b":"code","b9482ed7":"code","5d0c2802":"code","cb52a1b4":"code","3dadde00":"code","6a282424":"code","7f730846":"code","e1800aa4":"code","76e8f6f4":"code","9b533a65":"code","27d9d360":"code","547ee4ef":"code","2514514e":"code","fd67f1ea":"code","d61a1dac":"code","7c8f43cc":"code","f72cd26d":"code","0fcaff25":"code","3a56ddaa":"code","5287210e":"code","444ebb64":"code","8ff15763":"code","1f9252b5":"code","1701d533":"code","f3e97156":"code","e5d39d36":"markdown","5dd871bd":"markdown","52acd011":"markdown","21d65ab7":"markdown","966c438f":"markdown","cfa9e618":"markdown","b22ead50":"markdown","1e7bc062":"markdown","b6540423":"markdown","7e99f835":"markdown","b4adad05":"markdown","27d25161":"markdown","547cba94":"markdown","53f846be":"markdown","0501e11e":"markdown","3ae750fe":"markdown","61fb3f8c":"markdown","42cded7e":"markdown","4856d41c":"markdown","12379b79":"markdown","aa9ce919":"markdown","6dab0f03":"markdown","3e396c2f":"markdown","9083293e":"markdown","35a09511":"markdown","b8b3a8fc":"markdown","7ea09a9c":"markdown","e52ec4d0":"markdown","cc185268":"markdown","efba6ede":"markdown","225a672e":"markdown","0414164e":"markdown","c637b423":"markdown","55ca70ae":"markdown","eea8ff0c":"markdown","a105c877":"markdown","cf53e3d2":"markdown","d262d0e2":"markdown","1b79e6e5":"markdown","9e1b2f99":"markdown","c3d48edc":"markdown","9362536c":"markdown","5d95da49":"markdown","5851666d":"markdown","668ce871":"markdown","fff5eb78":"markdown","b947b5a4":"markdown","2bb2a137":"markdown","c517065b":"markdown","5aed1e7e":"markdown","0a1bb0aa":"markdown","29646d0b":"markdown","647380fc":"markdown","5d2c5ab3":"markdown","618cf64f":"markdown","67d37fa5":"markdown","8d17007a":"markdown","c987e1a4":"markdown","dedd0dc0":"markdown","5b7c5eba":"markdown","050fb7a0":"markdown","072c83da":"markdown","c0629697":"markdown","366cd54f":"markdown","19c0ff64":"markdown","0e4d03a9":"markdown","21ded451":"markdown","084415b9":"markdown","02073f2b":"markdown","32f8c0ac":"markdown","69ac3b73":"markdown","a6accd5b":"markdown","e136a77f":"markdown","1c8feac5":"markdown","b3fdae8e":"markdown","0de3e508":"markdown","1e5cad5d":"markdown","6ff9eedb":"markdown","9c78f280":"markdown","63bcc648":"markdown","ffd543c6":"markdown","2141490d":"markdown","1522e028":"markdown","2e75960e":"markdown","cba4894b":"markdown","d9693096":"markdown","30ed8f9c":"markdown","9043e5f0":"markdown","cfaf8fbc":"markdown","27009c8b":"markdown"},"source":{"8d197a8c":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom matplotlib.pyplot import xticks\n%matplotlib inline\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)","5e88fe6e":"leads_df = pd.read_csv(r\"Leads.csv\")\nleads_df.head()","ef50738a":"leads_df.drop(['Asymmetrique Activity Index','Asymmetrique Profile Index',\n           'Asymmetrique Activity Score','Asymmetrique Profile Score',\n           'Tags','Lead Quality','Lead Profile'],axis= 1, inplace = True)","1c6e65ec":"lead_dup = leads_df.copy()\n\n# Checking for duplicates and dropping the entire duplicate row if any\nlead_dup.drop_duplicates(subset=None, inplace=True)\nlead_dup.shape, leads_df.shape","04288aa3":"leads_df.shape","13b3416c":"leads_df.info()","11950a71":"leads_df.describe()","29b8bb5e":"# Converting 'Select' values to NaN.\nleads_df = leads_df.replace('Select', np.nan)\nleads_df.head()","1ad79e27":"leads_df.drop(['Prospect ID', 'Lead Number'], 1, inplace = True)\nleads_df.shape","9bc9c472":"100*(leads_df.isnull().sum()\/leads_df.shape[0]).sort_values(ascending=False)","f4ff9e9e":"# we will drop the columns having more than 60% NA values.\nleads_df.drop(leads_df.columns[100*(leads_df.isnull().sum()\/leads_df.shape[0])>60],axis=1,inplace=True)\nleads_df.shape","74e53f5a":"null_col = ['City','Specialization','What matters most to you in choosing a course','What is your current occupation','Country']\nfor i in null_col:\n    print(leads_df[i].value_counts(normalize=True))","543cffe8":"leads_df['City'].replace(np.nan,'Mumbai',inplace=True)\n\nleads_df['Specialization'].replace(np.nan, 'Others',inplace=True)\n\nleads_df['What matters most to you in choosing a course'].replace(np.nan, 'Better Career Prospects',inplace=True)\n\nleads_df['What is your current occupation'].replace(np.nan, 'Unemployed',inplace=True)\n\nleads_df['Country'].replace(np.nan, 'India',inplace=True)","4a77848a":"# Rest missing values are under 1.5% so we can drop these rows.\nleads_df.dropna(inplace = True)","874c3b1a":"# checking null values again\n100*(leads_df.isnull().sum()\/leads_df.shape[0]).sort_values(ascending=False)","ae39f156":"print(100*(leads_df.shape[0]\/ lead_dup.shape[0]),\"% of original rows is available for EDA\")","6cc2414a":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"Lead Origin\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","319b1381":"100*(leads_df.Converted.groupby(leads_df['Lead Origin']).mean()).sort_values(ascending=False)","924441e3":"plt.figure(figsize = (25,5))\nax= sns.countplot(x = \"Lead Source\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","d42937cc":"leads_df['Lead Source'].replace(['google'], 'Google',inplace=True)\nleads_df['Lead Source'].replace(['Click2call', 'Live Chat', 'NC_EDM', 'Pay per Click Ads', 'Press_Release',\n  'Social Media', 'WeLearn', 'bing', 'blog', 'testone', 'welearnblog_Home', 'youtubechannel'], 'Others',inplace=True)\nleads_df['Lead Source'].value_counts()","671fa033":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"Lead Source\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","e32c446d":"100*(leads_df.Converted.groupby(leads_df['Lead Source']).mean()).sort_values(ascending=False)","2e4f8c8e":"plt.figure(figsize = (20,5))\nplt.subplot(1,2,1)\nax= sns.countplot(x = \"Do Not Email\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.subplot(1,2,2)\nax= sns.countplot(x = \"Do Not Call\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","fef4aa8a":"print(leads_df.Converted.groupby(leads_df['Do Not Email']).mean())\nprint(leads_df.Converted.groupby(leads_df['Do Not Call']).mean())","83eef7aa":"leads_df.TotalVisits.describe(percentiles=[.05,.25, .5, .75, .90, .95, .99])","60c7543b":"plt.figure(figsize = (10,4))\nsns.violinplot(leads_df.TotalVisits)\nplt.show()","91f8016e":"percentiles = leads_df['TotalVisits'].quantile([0.05,0.95]).values\nleads_df['TotalVisits'][leads_df['TotalVisits'] <= percentiles[0]] = percentiles[0]\nleads_df['TotalVisits'][leads_df['TotalVisits'] >= percentiles[1]] = percentiles[1]","f9d9eb82":"plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\nsns.violinplot(leads_df.TotalVisits)\nplt.subplot(1,2,2)\nsns.violinplot(y = 'TotalVisits', x = 'Converted', data = leads_df)\nplt.show()","6d30bef6":"leads_df['Total Time Spent on Website'].describe()","0953b480":"plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\nsns.violinplot(leads_df['Total Time Spent on Website'])\nplt.subplot(1,2,2)\nsns.violinplot(y = 'Total Time Spent on Website', x = 'Converted', data = leads_df)\nplt.show()","160cda88":"leads_df['Page Views Per Visit'].describe()","96144d8a":"plt.figure(figsize = (10,5))\nsns.violinplot(leads_df['Page Views Per Visit'])\nplt.show()","22275ddf":"percentiles = leads_df['Page Views Per Visit'].quantile([0.05,0.95]).values\nleads_df['Page Views Per Visit'][leads_df['Page Views Per Visit'] <= percentiles[0]] = percentiles[0]\nleads_df['Page Views Per Visit'][leads_df['Page Views Per Visit'] >= percentiles[1]] = percentiles[1]","3d323f38":"plt.figure(figsize=(20,8))\nplt.subplot(1,2,1)\nsns.violinplot(leads_df['Page Views Per Visit'])\n\nplt.subplot(1,2,2)\nsns.violinplot(y = 'Page Views Per Visit', x = 'Converted', data = leads_df)\nplt.show()","c85a9e46":"leads_df['Last Activity'].value_counts(normalize=True)","8f0e6263":"plt.figure(figsize = (25,5))\nax= sns.countplot(x = \"Last Activity\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","bd215caa":"leads_df['Last Activity'].replace(['Had a Phone Conversation', 'View in browser link Clicked', \n                                                       'Visited Booth in Tradeshow', 'Approached upfront',\n                                                       'Resubscribed to emails','Email Received', 'Email Marked Spam'],\n                                                      'Other_Activity',inplace=True)","58812a44":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"Last Activity\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","300ed098":"100*(leads_df.Converted.groupby(leads_df['Last Activity']).mean()).sort_values(ascending=False)","c76fe30d":"leads_df.Country.value_counts()","7a6d5f14":"plt.figure(figsize = (25,5))\nax= sns.countplot(x = \"Country\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","feed0821":"100*(leads_df.Converted.groupby(leads_df.Country).mean()).sort_values(ascending=False)","7c16cb94":"leads_df.Specialization.value_counts()","c991a3eb":"leads_df['Specialization'].replace(['Others'], 'Other_Specialization',inplace=True)","95f2612b":"plt.figure(figsize = (25,5))\nax= sns.countplot(x = \"Specialization\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","a357e996":"100*(leads_df.Converted.groupby(leads_df.Specialization).mean()).sort_values(ascending=False)","ca84d061":"leads_df['What is your current occupation'].value_counts()","e1b5f683":"leads_df['What is your current occupation'].replace(['Other'], 'Other_Occupation',inplace=True)","3c062ee7":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"What is your current occupation\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","52bb6a40":"leads_df.Converted.groupby(leads_df['What is your current occupation']).mean().sort_values(ascending=False)","2e510aa8":"leads_df['What matters most to you in choosing a course'].value_counts()","3333d471":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"What matters most to you in choosing a course\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","f7929604":"100*(leads_df.Converted.groupby(leads_df['What matters most to you in choosing a course']).mean()).sort_values(ascending=False)","eabd8c6b":"leads_df.Search.value_counts()","c3a50c98":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"Search\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","a1b9c435":"100*(leads_df.Converted.groupby(leads_df.Search).mean()).sort_values(ascending=False)","15d39792":"leads_df.Magazine.value_counts()","58222c49":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"Magazine\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","6bc4d2d8":"leads_df['Newspaper Article'].value_counts()","bd5dc0c3":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"Newspaper Article\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","3f27398e":"leads_df['X Education Forums'].value_counts()","81bca9de":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"X Education Forums\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","91c96389":"leads_df['Newspaper'].value_counts()","11a34847":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"Newspaper\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","e8d8d25c":"leads_df['Digital Advertisement'].value_counts()","950bfbb1":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"Digital Advertisement\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","af467fc8":"leads_df['Through Recommendations'].value_counts()","81267dec":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"Through Recommendations\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","10ba070f":"leads_df['Receive More Updates About Our Courses'].value_counts()","94d97cfb":"plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"Receive More Updates About Our Courses\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","bc19501d":"leads_df['Update me on Supply Chain Content'].value_counts()","6024c673":"plt.figure(figsize = (10,6))\nax= sns.countplot(x = \"Update me on Supply Chain Content\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","f3c21b28":"leads_df['Get updates on DM Content'].value_counts()","5be54aaa":"plt.figure(figsize = (10,6))\nax= sns.countplot(x = \"Get updates on DM Content\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","ffabedbe":"leads_df['I agree to pay the amount through cheque'].value_counts()","c8712d0a":"plt.figure(figsize = (10,6))\nax= sns.countplot(x = \"I agree to pay the amount through cheque\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","2854d45e":"leads_df['A free copy of Mastering The Interview'].value_counts()","550cb7a7":"plt.figure(figsize = (10,6))\nax= sns.countplot(x = \"A free copy of Mastering The Interview\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","7d75bd6d":"leads_df.City.value_counts()","c9141f85":"plt.figure(figsize = (10,6))\nax= sns.countplot(x = \"City\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","d9299cef":"100*(leads_df.Converted.groupby(leads_df.City).mean()).sort_values(ascending=False)","00d49a80":"leads_df['Last Notable Activity'].value_counts()","946fb149":"plt.figure(figsize = (20,6))\nax= sns.countplot(x = \"Last Notable Activity\", hue = \"Converted\", data = leads_df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nax.set_yscale('log')\nplt.show()","ce1c63ea":"100*(leads_df.Converted.groupby(leads_df['Last Notable Activity']).mean()).sort_values(ascending=False)","c2e02a88":"leads_df.drop(['What matters most to you in choosing a course','Search','Magazine','Newspaper Article','X Education Forums','Newspaper',\n           'Digital Advertisement','Through Recommendations','Receive More Updates About Our Courses','Update me on Supply Chain Content',\n           'Get updates on DM Content','I agree to pay the amount through cheque','A free copy of Mastering The Interview','Country'],axis=1,inplace=True)\nleads_df.columns","1f632c9d":"plt.figure(figsize = (10,5))\nsns.heatmap(leads_df.corr(), annot = True, cmap=\"RdYlGn_r\")\nplt.show()","99303a14":"print(leads_df['Last Activity'].value_counts())\nprint(leads_df['Last Notable Activity'].value_counts())","6b1ce10a":"leads_df.drop(['Last Notable Activity'],axis= 1, inplace = True)\nleads_df.columns","b9f8c17e":"print('%age of Columns retained = ',(leads_df.shape[1]\/lead_dup.shape[1])*100,'%')","1a0480c7":"print(\"%age of rows retained = \",(leads_df.shape[0]\/lead_dup.shape[0])*100,'%')","2b4befdf":"leads_df.shape","61fe97d6":"leads_df.head()","c3e54c4f":"# List of variables to map\n\nbin_col =  ['Do Not Email', 'Do Not Call']\n\n# Defining the map function\ndef binary(x):\n    return x.map({'Yes': 1, \"No\": 0})\n\n# Applying the function to the housing list\nleads_df[bin_col] = leads_df[bin_col].apply(binary)\nleads_df.head()","70cb0b9f":"dummy = pd.get_dummies(leads_df[['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation',\n                              'City']], drop_first=True)\n\ndummy.head()","eed39231":"# Adding the results to the master dataframe\nleads_df = pd.concat([leads_df, dummy], axis=1)\nleads_df.head()","ffed7abd":"leads_df.drop(['Lead Origin', 'Lead Source', 'Last Activity', 'Specialization','What is your current occupation','City'], axis = 1,inplace=True)\nleads_df.head()","8b24da76":"leads_df.shape","36188198":"from sklearn.model_selection import train_test_split\n\n# Feature variables dataframe as X\nX = leads_df.drop(['Converted'], axis=1)\nX.head()","c5286150":"X.shape","7ce42078":"# Dependent variable as y\ny = leads_df['Converted']\ny.head()","3539db35":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=333)","ede71f48":"X_train.head()","a2b65283":"X_train.shape, X_test.shape,y_train.shape,y_test.shape","adf828ac":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']] = scaler.fit_transform(X_train[['TotalVisits','Total Time Spent on Website','Page Views Per Visit']])\n\nX_train.head()","6c38c3b6":"# Checking the Conversion Rate\nprint(round((sum(leads_df['Converted'])\/leads_df.Converted.shape[0])*100,2),'% is the conversion rate')","99697d43":"import statsmodels.api as sm\n\n# Logistic regression model\nlogm = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm.fit().summary()","8f59bff0":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg,18)             # running RFE with 18 variables as output\nrfe = rfe.fit(X_train, y_train)\n\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","d734dd11":"column = X_train.columns[rfe.support_]\ncolumn","5d3ef766":"X_train.columns[~rfe.support_]","8576a987":"X_train_sm = sm.add_constant(X_train[column])\nlogm1 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm1.fit()\nres.summary()","98c8b07f":"# Check VIF values of feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[column].columns\nvif['VIF'] = [variance_inflation_factor(X_train[column].values, i) for i in range(X_train[column].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","af24ad28":"column1 = column.drop('What is your current occupation_Housewife',1)\n\ncolumn1","1606bc4e":"X_train_sm = sm.add_constant(X_train[column1])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","3166f609":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[column1].columns\nvif['VIF'] = [variance_inflation_factor(X_train[column1].values, i) for i in range(X_train[column1].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","afb1056a":"column2 = column1.drop('Lead Source_Reference',1)\ncolumn2","2e40f751":"X_train_sm = sm.add_constant(X_train[column2])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","46c70a9a":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[column2].columns\nvif['VIF'] = [variance_inflation_factor(X_train[column2].values, i) for i in range(X_train[column2].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","70e9ae4b":"column3 = column2.drop('What is your current occupation_Unemployed',1)\ncolumn3","79491427":"X_train_sm = sm.add_constant(X_train[column3])\nlogm4 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm4.fit()\nres.summary()","27245aa4":"vif = pd.DataFrame()\nvif['Features'] = X_train[column3].columns\nvif['VIF'] = [variance_inflation_factor(X_train[column3].values, i) for i in range(X_train[column3].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","7a579646":"column4 = column3.drop('What is your current occupation_Student',1)\ncolumn4","3ef969ba":"X_train_sm = sm.add_constant(X_train[column4])\nlogm5 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm5.fit()\nres.summary()","4be9b14f":"vif = pd.DataFrame()\nvif['Features'] = X_train[column4].columns\nvif['VIF'] = [variance_inflation_factor(X_train[column4].values, i) for i in range(X_train[column4].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","38da1ade":"column5 = column4.drop('Last Activity_Unreachable',1)\ncolumn5","5f8e1642":"X_train_sm = sm.add_constant(X_train[column5])\nlogm6 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm6.fit()\nres.summary()","e5a648ab":"vif = pd.DataFrame()\nvif['Features'] = X_train[column5].columns\nvif['VIF'] = [variance_inflation_factor(X_train[column5].values, i) for i in range(X_train[column5].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","25330616":"column6 = column5.drop('Last Activity_Unsubscribed',1)","9377ef34":"X_train_sm = sm.add_constant(X_train[column6])\nlogm7 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm7.fit()\nres.summary()","6c508361":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","08eda68f":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","badca73f":"y_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Converted_prob':y_train_pred})\ny_train_pred_final.head()","a0a4ab9e":"y_train_pred_final['predicted'] = y_train_pred_final.Converted_prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","d0c3af8d":"from sklearn import metrics\n\n# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.predicted )\nprint(confusion)","4597544b":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.predicted))","25f2a0f5":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[column6].columns\nvif['VIF'] = [variance_inflation_factor(X_train[column6].values, i) for i in range(X_train[column6].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","25bdf03e":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","7e00d824":"# Let's see the sensitivity of our model\nTP \/ float(TP+FN)","e78dfc3d":"# Let us calculate specificity\nTN \/ float(TN+FP)","d98f1e73":"# Calculate false postive rate - predicting Converted when customer does not have Converted\nprint(FP\/ float(TN+FP))","9ccfeea9":"# positive predictive value \nprint (TP \/ float(TP+FP))","f126740d":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","f780c211":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None\n","739502fc":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Converted, \n                                         y_train_pred_final.Converted_prob, drop_intermediate = False )","b080bcd7":"draw_roc(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)","919feef2":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Converted_prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","1fe68138":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","9638bcef":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'],figsize = (10,5))\nplt.grid(True)\nplt.show()","1f774c1e":"y_train_pred_final['final_predicted'] = y_train_pred_final.Converted_prob.map( lambda x: 1 if x > 0.34 else 0)\n\ny_train_pred_final.head()","aabd4823":"y_train_pred_final['Lead_Score'] = y_train_pred_final.Converted_prob.map( lambda x: round(x*100))\n\ny_train_pred_final.head()","02ac9701":"# Let's check the overall accuracy.\ntrainaccuracy= metrics.accuracy_score(y_train_pred_final.Converted, y_train_pred_final.final_predicted)\ntrainaccuracy","ff8611b9":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final.final_predicted )\nconfusion2","7061341a":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","4ccbcd4b":"# Let's see the sensitivity of our model\ntrainsensitivity= TP \/ float(TP+FN)\ntrainsensitivity","380cfe19":"# Let us calculate specificity\ntrainspecificity= TN \/ float(TN+FP)\ntrainspecificity","2a91f6f2":"# Calculate false postive rate - predicting Converted when customer does not have Converted\nprint(FP\/ float(TN+FP))","4a958a49":"# Positive predictive value \nprint (TP \/ float(TP+FP))","e41e99d7":"# Negative predictive value\nprint(TN \/ float(TN+ FN))","2a4ca5d4":"from sklearn.metrics import precision_score, recall_score\n\nprecision= precision_score(y_train_pred_final.Converted , y_train_pred_final.predicted)\nprecision","5dec1829":"recall=recall_score(y_train_pred_final.Converted, y_train_pred_final.predicted)\nrecall","3180b48b":"trainF1_score= 2 * (precision * recall) \/ (precision + recall)\ntrainF1_score","e262b5af":"from sklearn.metrics import precision_recall_curve\n\np, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Converted_prob)","1f7eb04b":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","fa9940bf":"X_test[['TotalVisits','Total Time Spent on Website',\n        'Page Views Per Visit']] = scaler.transform(X_test[['TotalVisits',\n                                                                'Total Time Spent on Website','Page Views Per Visit']])\n\nX_test.describe()","6deefef3":"X_test = X_test[column6]\nX_test.head()","e968a505":"X_test_sm = sm.add_constant(X_test)","7088f0ee":"y_test_pred = res.predict(X_test_sm)","108d5d3a":"y_test_pred[:10]","2303c0ae":"# Converting y_pred to a dataframe which is an array\ny_test_pred_1 = pd.DataFrame(y_test_pred)\ny_test_pred_1.head()","a69b700b":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","b9482ed7":"# Appending y_test_df and y_test_pred_1\ny_pred_final = pd.concat([y_test_df, y_test_pred_1],axis=1)\ny_pred_final.head()","5d0c2802":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Converted_prob'})\ny_pred_final.head()","cb52a1b4":"y_pred_final['final_predicted'] = y_pred_final.Converted_prob.map(lambda x: 1 if x > 0.34 else 0)\n\ny_pred_final['Lead_Score'] = y_pred_final.Converted_prob.map( lambda x: round(x*100))\n","3dadde00":"Lead_Score=y_pred_final.copy()\ny_pred_final.head()","6a282424":"Lead_Score.reset_index(level=0, inplace=True)\nLead_Score.drop(['Converted', 'Converted_prob', 'final_predicted'], 1, inplace = True)\nLead_Score.head()","7f730846":"Lead=lead_dup.copy()\nLead.reset_index(level=0, inplace=True)\nLead.drop(['Lead Origin', 'Lead Source','Do Not Email', 'Do Not Call', 'Converted', 'TotalVisits',\n       'Total Time Spent on Website', 'Page Views Per Visit', 'Last Activity',\n       'Country', 'Specialization', 'How did you hear about X Education',\n       'What is your current occupation',\n       'What matters most to you in choosing a course', 'Search', 'Magazine',\n       'Newspaper Article', 'X Education Forums', 'Newspaper',\n       'Digital Advertisement', 'Through Recommendations',\n       'Receive More Updates About Our Courses',\n       'Update me on Supply Chain Content', 'Get updates on DM Content',\n       'City', 'I agree to pay the amount through cheque',\n       'A free copy of Mastering The Interview', 'Last Notable Activity'], 1, inplace = True)\nLead.head()","e1800aa4":"Lead_Score=pd.merge(Lead,Lead_Score,on='index')\nLead_Score.drop(['index'], 1, inplace = True)\nLead_Score.head()","76e8f6f4":"Lead_Score.sort_values([\"Lead_Score\"], ascending = False,inplace=True)\nLead_Score.head()","9b533a65":"# Let's check the overall accuracy.\ntestaccuracy= metrics.accuracy_score(y_pred_final.Converted, y_pred_final.final_predicted)\ntestaccuracy","27d9d360":"confusion2 = metrics.confusion_matrix(y_pred_final.Converted, y_pred_final.final_predicted )\nconfusion2","547ee4ef":"# Let's see the sensitivity of our lmodel\ntestsensitivity=TP \/ float(TP+FN)\ntestsensitivity","2514514e":"# Let us calculate specificity\ntestspecificity= TN \/ float(TN+FP)\ntestspecificity","fd67f1ea":"precision= precision_score(y_pred_final.Converted , y_pred_final.final_predicted)\nprecision","d61a1dac":"recall=recall_score(y_pred_final.Converted , y_pred_final.final_predicted)\nrecall","7c8f43cc":"testF1_score= 2 * (precision * recall) \/ (precision + recall)\ntestF1_score","f72cd26d":"Lead_Score","0fcaff25":"# Let us compare the values obtained for Train & Test:\nprint(\"Train Data Accuracy    :{} %\".format(round((trainaccuracy*100),2)))\nprint(\"Train Data Sensitivity :{} %\".format(round((trainsensitivity*100),2)))\nprint(\"Train Data Specificity :{} %\".format(round((trainspecificity*100),2)))\nprint(\"Train Data F1 Score    :{}  \".format(round((trainF1_score),2)))\nprint(\"Test Data Accuracy     :{} %\".format(round((testaccuracy*100),2)))\nprint(\"Test Data Sensitivity  :{} %\".format(round((testsensitivity*100),2)))\nprint(\"Test Data Specificity  :{} %\".format(round((testspecificity*100),2)))\nprint(\"Test Data F1 Score     :{}  \".format(round((testF1_score),2)))","3a56ddaa":"from sklearn.metrics import classification_report\n\nprint (classification_report(y_train_pred_final['Converted'], y_train_pred_final['final_predicted']))","5287210e":"print (classification_report(y_pred_final.Converted, y_pred_final.final_predicted))","444ebb64":"pd.options.display.float_format = '{:.2f}'.format\nnew_params = res.params[1:]\nnew_params.sort_values()","8ff15763":"feature_importance = new_params\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nfeature_importance","1f9252b5":"sorted_idx = np.argsort(feature_importance,kind='quicksort',order='list of str')\nsorted_idx","1701d533":"pos = np.arange(sorted_idx.shape[0]) + .5\n\nfeatfig = plt.figure(figsize=(10,6))\nfeatax = featfig.add_subplot(1, 1, 1)\nfeatax.barh(pos, feature_importance[sorted_idx], align='center', color = 'tab:red',alpha=0.8)\nfeatax.set_yticks(pos)\nfeatax.set_yticklabels(np.array(X_train[column6].columns)[sorted_idx], fontsize=12)\nfeatax.set_xlabel('Relative Feature Importance', fontsize=14)\n\nplt.tight_layout()   \nplt.show()","f3e97156":"pd.DataFrame(feature_importance).reset_index().sort_values(by=0,ascending=False).head(3)","e5d39d36":"### Checking VIF","5dd871bd":"**6th Iteration**","52acd011":"## Conclusion\n- After tuning the model, these are the following characteristics:\n    - All variables have p-value < 0.05. \n    - All the features have very low VIF values, meaning, there is hardly any muliticollinearity among the features.\n    - The overall accuracy of 0.8 at a probability threshold of 0.34 on the test dataset is also very acceptable\n\n**Using this model, the dependent variable value was predicted as per the following threshold values of Conversion probability:**\n\n|Dataset|Threshold Value|Accuracy|Sensitivity|Specificity|False Positive Rate|Positive Predictive Value|Negative Predicted Value|Precision|Recall|F1 Score|\n|--|--|--|--|--|--|--|--|--|--|--|\n|Train|0.5|0.8049318087890894|0.6607402031930334|0.8931823228958472|0.10681767710415278|0.7910512597741095|0.8113778495057494|\n|Train|0.34|0.8065849290535887|0.8156748911465893|0.8010215411947591|0.19897845880524095|0.19897845880524095|0.8765492102065614|0.7910512597741095|0.6607402031930334|0.7200474495848161|\n|Test|0.34|0.8|0.8156748911465893|0.8010215411947591||||0.7158469945355191|0.7717231222385862|0.7427356484762581|\n\n## Recommendation\n- These are the strong predictors for dependent variable\n\n|Features with Positive Coefficient Value|Coeffient Value|Features with Negative Coefficient Value|Coefficient Value|\n|--|--|--|--|\nLast Activity_Email Opened|0.55|Do Not Email|-1.28|\nTotal Time Spent on Website|1.11|Specialization_Other_Specialization   |                 -1.19|\nLead Source_Olark Chat      | 1.11|Last Activity_Olark Chat Conversation           |    -1.14|\nLast Activity_SMS Sent       |  1.73|Lead Origin_Landing Page Submission    |    -1.12|\nLast Activity_Other_Activity  |                          2.31|||\nWhat is your current occupation_Working Professional|    2.69|||\nLead Source_Welingak Website                         |   3.32|||\nLead Origin_Lead Add Form                             |  3.33|||","21d65ab7":"- Almost all leads chose 'Better Career Prospects'.\n\n### `INSIGHT`\n- Nothing much can be inferred from this column\n\n### Search","966c438f":"- Renaming `Others` to `Other_Specialization`.","cfa9e618":"- All entries are `No`.\n\n### `INSIGHT`\n- Not much can be inferred from this column.\n\n### Newspaper Article","b22ead50":"### 4.5.1 Feature Selection using RFE","1e7bc062":"## 4.4 Feature Scaling\n- Using Standardization as the way to scale the features","b6540423":"- **As per the business requirements, there is a model to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted**","7e99f835":"- There are a number of outliers in the data. We will cap the outliers to 95% value for analysis.","b4adad05":"### 4.10.1 Precision and recall tradeoff","27d25161":"- `TotalVisits` & `Page Views per Visit` are higly correlated with correlation coefficient `0.77`","547cba94":"# Lead Scoring \n\n## Problem Statement\n\nAn education company named X Education sells online courses to industry professionals. On any given day, many professionals who are interested in the courses land on their website and browse for courses. \n    \n   The company markets its courses on several websites and search engines like Google. Once these people land on the website, they might browse the courses or fill up a form for the course or watch some videos. When these people fill up a form providing their email address or phone number, they are classified to be a lead. Moreover, the company also gets leads through past referrals. Once these leads are acquired, employees from the sales team start making calls, writing emails, etc. Through this process, some of the leads get converted while most do not. The typical lead conversion rate at X education is around 30%. \n    \n   Now, although X Education gets a lot of leads, its lead conversion rate is very poor. For example, if, say, they acquire 100 leads in a day, only about 30 of them are converted. To make this process more efficient, the company wishes to identify the most potential leads, also known as \u2018Hot Leads\u2019. If they successfully identify this set of leads, the lead conversion rate should go up as the sales team will now be focusing more on communicating with the potential leads rather than making calls to everyone. \n    \n\nThe CEO, in particular, has given a ballpark of the target lead conversion rate to be around 80%.\n\n\n## Goals\nThere are quite a few goals for this case study.\n\n\nBuild a model to assign a lead score between 0 and 100 to each of the leads which can be used by the company to target potential leads. A higher score would mean that the lead is hot, i.e. is most likely to convert whereas a lower score would mean that the lead is cold and will mostly not get converted.\nThere are some more problems presented by the company which your model should be able to adjust to if the company's requirement changes in the future so you will need to handle these as well. \n\n# 0. Importing Modules","53f846be":"## 4.2 Creating a dummy variable for some of the categorical variables and dropping the first one.","0501e11e":"- Most leads are from `India`. So, `India` is the core market for the business.\n- There is high lead conversion from `Europe`, `USA`, `Middle East`.\n\n### `INSIGHT`\n\n**There is a huge potential from `Europe`, `USA`, `Middle East`**\n\n### Specialization","3ae750fe":"- `SMS Sent` having both high lead count and significant lead conversion rate of `69.19%`\n- `Had a Phone Conversation` having good conversion rate of `92.8%` but had less lead count.\n\n### `INSIGHT`\n- We should focus more on `calling the leads` as the conversion rate is high though the lead count is less.\n- `SMS Sent`is a strong positive predictor\n\n## **Results**\n\n\n**Based on the univariate analysis, There are some columns which cannot bring much to analysis and model building. Hence, Dropping these columns from further analysis**\n\n|Columns which dont bring much to analysis||||\n|--|--|--|--|\n|What matters most to you in choosing a course|Search|Magazine|Newspaper Article\nX Education Forums|Newspaper|Digital Advertisement|Through Recommendations|\nReceive More Updates About Our Courses|Update me on Supply Chain Content|Get updates on DM Content|I agree to pay the amount through cheque|\n|A free copy of Mastering The Interview|Country|","61fb3f8c":"### 4.5.3 VIF Check","42cded7e":"- Almost all entries are `No`.\n\n### `INSIGHT`\n- Not much can be inferred from this column.\n\n### Through Recommendations","4856d41c":"- Median for converted and not converted leads are the same.\n\n### `INSIGHT`\n**Nothng conclusive can be said on the basis of Total Visits.**\n### Total time spent on website","12379b79":"- Most leads are from `Mumbai` with 36.37% conversion rate.\n- `Thane & Outskirts` and `Other Cities of Maharashtra` have considerable lead conversion rate with considerable lead count.\n\n### `INSIGHT`\n- Focus more on `Thane & Outskirts` and `Other Cities of Maharashtra` as the yhave considerable lead conversion rates\n\n### Last Notable Activity","aa9ce919":"**Plot showing the feature variables based on their relative coefficient values**","6dab0f03":"- Renaming `Other` to `Other_Occupation`","3e396c2f":"### Top 3 features which contribute most towards the probability of a lead getting converted","9083293e":"### `INSIGHT`\n- **98.2% of data is retained**\n- **Data is free of null values**\n\n# 3.Exploratory Data Analytics\n\n## 3.1 Univariate Analysis\n\n### Lead Origin","35a09511":"## 4.9 Assigning Lead Score","b8b3a8fc":"- Dropping `What is your current occupation_Student` because of high P-Value","7ea09a9c":"## 2.4 Dealing with Null values","e52ec4d0":"**5th Iteration**","cc185268":"## 4.5 Model Building","efba6ede":"#### `INSIGHT`\n- There are 9240 rows and 30 columns in dataframe after dropping some irrelevant columns\n- There are 25 columns of `object` datatype and `3` of `int64` datatype and `2` of `float64` datatype","225a672e":"\n# Final Observation:\n\n- **With the insights from analyzing the data, We are able to build a model to produce lead Score , which helps sales team to easily target hot leads & convert them into Customers.**","0414164e":"### Metrics beyond simply accuracy","c637b423":"- For `Do Not Call`, the conversion rate of `Yes` is very high but the number of leads are just 2. So, we can't take that seriously\n- Most of the leads are ok with `Email` and `Call`\n### `INSIGHT`\n\n- Not much can be inferred from both `Do Not Call` and `Do Not Email`\n\n### Total Visits","55ca70ae":"- There is no clear value with high conversion rate.\n- These are the top 5 `Specializations` with high conversion rate\n\n|Specialization|Conversion rate|\n|--|--|\nBanking, Investment And Insurance|    48.955224|\nHealthcare Management             |   48.717949|\nMarketing Management               |  48.238153|\nOperations Management               | 46.893788|\nHuman Resource Management            |45.400239|\n\n### `INSIGHT`\n- There are no specializations that stand out when it comes to lead conversion rate.\n- It's better to concentrate more on the above 5 specializations.\n\n### Occupation","eea8ff0c":"## 1. Loading the CSV and Data Inspection\n\n### 1.1 Data Loading","a105c877":"## 4.8 Finding Optimal Cutoff Point\n\n- Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","cf53e3d2":"## Determining Feature Importance\n- Selecting the coefficients of the selected features from our final model excluding the intercept","d262d0e2":"- Almost all entries are `No`.\n\n### `INSIGHT`\n- Nothing much can be inferred from this column.\n\n### Magazine","1b79e6e5":"### `INSIGHT`\n- `A free copy of Mastering The Interview` doesn't help much in analysis\n\n### City","9e1b2f99":"- Dropping `Last Activity_Unsubscribed` because of high p value, below or equal to 0.01 is the accepted range here.","c3d48edc":"## 3.2 Checking `correlation coefficients` to see which variables are highly correlated","9362536c":"**3rd Iteration**","5d95da49":"- Almost all entries are `No`.\n\n### `INSIGHT`\n- Not much can be inferred from this column.\n\n### Receive More Updates About Our Courses","5851666d":"**4th Iteration**","668ce871":"## 2.2 Dropping `Lead Number` and `Prospect ID`\n- These two columns are irrelevant to analysis as they are just ID columns","fff5eb78":"- Data is now free of `null` values\n\n## 2.5 Checking the Data retained after clearing out the null values ","b947b5a4":"- Clubbing these columns to `Other_Activity` because the leads from these columns individually are very less and can complicate the analysis.\n\n||||\n|--|--|--|\n|Had a Phone Conversation|View in browser link Clicked|Visited Booth in Tradeshow|\nApproached upfront|Resubscribed to emails|Email Received|\nEmail Marked Spam|||","2bb2a137":"- Replacing the values having less occurance because it makes analysis too complicated.\n- Replacing `google` with `Google`.\n- Replacing these values with `Others`\n\n|Values with less occurance||||\n|--|--|--|--|\n|Click2call|Live Chat|NC_EDM|Pay per Click Ads|\nPress_Release|Social Media|WeLearn|bing|\nblog|testone|welearnblog_Home|youtubechannel|","c517065b":"# 4. Data Preparation\n## 4.1 Converting some binary variables (Yes\/No) to 1\/0","5aed1e7e":"### 4.6 Creating a dataframe containing actual Converted values and the predicted probabilities","0a1bb0aa":"- Most of the lead have their `Email Opened` and `SMS Sent` as their last activity.\n- Conversion rate for leads with last activity as `SMS Sent` is almost 62%.\n\n### `INSIGHT`\n\n- `SMS Sent` has high leads conversion\n\n### Country","29646d0b":"- **From the curve above, 0.34 is the optimum point to take it as a cutoff probability**","647380fc":"## 4.3 TRAIN - TEST SPLIT","5d2c5ab3":"- All 5 columns are categorical. So, the imputing value for null are\n\n|Column|Value imputing|%age of how often that value occurs|\n|--|--|--|\n|City|Mumbai|57.8%|\n|What matters most to you in choosing a course|Better Career Prospects     | 0.999541|\n|What is your current occupation|Unemployed            |  0.854962|\n|Country|India                  | 0.957663|\n|Specialization|Other||\n\n- **For `Specialization`, there is no clear value that can be imputed. Non Availability of the option they want to choose maybe the reason behind the null values. So, replacing null values in `Specialization` with `Other`.**","618cf64f":"- `Google` and `Direct Traffic` generates maximum number of leads.\n- Conversion Rate of `Reference`, `welingak website` leads are high.\n\n### `INSIGHT`\n- **Generate more leads from `Reference` and `Welingak Website`.**\n- **To improve overall lead conversion rate, focus should be on improving lead conversion of `Olark Chat`, `Organic Search`, `Direct Traffic`, and `Google` leads**\n\n### Do Not Email & Do Not Call","67d37fa5":"## 4.11 Making Predictions on Test Set","8d17007a":"- All entries are 'No'. \n\n### `INSIGHT`\n- Not much can be inferred from this column.\n\n### I agree to pay the amount through cheque","c987e1a4":"### 1.3 Duplicate Check","dedd0dc0":"## 2.3 checking the null percentage in all columns","5b7c5eba":"### 1.4 Data General Inspection","050fb7a0":"- leads from `Working Professional`,`Businessman` have high chances of converting to paying customer.\n- Most leads are from `Unemployed`but has around 30-35% conversion rate.\n\n### `INSIGHT`\n- We should concentrate more on `Working Professional` and `Businessman`\n\n### What matters most to you in choosing a course","072c83da":"- Dropping `Last Activity_Unreachable` because of high P-Value","c0629697":"- Almost all entries are `No`.\n\n### `INSIGHT`\n- Not much can be inferred from this column.\n\n### X Education Forums","366cd54f":"- There are a number of outliers in this column `TotalVisits`.So, we will cap the outliers to 95% value for analysis.","19c0ff64":"- **Creating new column 'predicted' with 1 if Converted_Prob > 0.5 else 0**","0e4d03a9":"- All entries are 'No'. \n\n### `INSIGHT`\n- Not much can be inferred from this column.\n\n### Get updates on DM Content","21ded451":"- **The shape is same for both dataframes. Hence, No Duplicate values.**","084415b9":"- Median for converted and unconverted leads is the same.\n\n### `INSIGHT`\n- **Nothing much can be inferred from `Page Views Per Visit` about lead conversion**\n\n### Last Activity","02073f2b":"### 4.5.2 Assessing the model with StatsModels","32f8c0ac":"- All entries are 'No'.\n\n### `INSIGHT`\n- Not much can be inferred from this column.\n\n### A free copy of Mastering The Interview","69ac3b73":"## 4.7 Plotting the ROC Curve\n\nAn ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","a6accd5b":"### Model Evaluation Metrics","e136a77f":"- Dropping `What is your current occupation_Unemployed` because of High VIF","1c8feac5":"### 1.2 Remove Columns which are now generated after we call a lead","b3fdae8e":"- Leads spending more time on the website are more likely to be converted.\n\n### `INSIGHT`\n\n- **Website should be made more engaging and intuitive with more information so that leads spend more them. Thus, increasing the chances of lead conversion.**\n\n### Page views per visit","0de3e508":"# 2. Data Cleaning\n## 2.1 Replacing `Select` value in some columns with null values\n- This is because customer did not select any option from the list, hence it shows select. Select values are as good as NULL.","1e5cad5d":"**2nd Iteration**","6ff9eedb":"- Website Should be made more intuitive and engaging with more information added because it's a strong predictor with a strong positive coefficient\n- `Lead Add Form ` in `Lead Origin` should be given high importance\n- `Welingak Website` in `Lead Source` is a reliable one in attracting new leads.","9c78f280":"- So, concentrating on columns having considerable missing values. They are\n\n|Column Names|Null percentage|\n|--|--|\nCity|                                             39.707792|\nSpecialization|                                   36.580087|\nWhat matters most to you in choosing a course|    29.318182|\nWhat is your current occupation|                  29.112554|\nCountry|                                          26.634199|","63bcc648":"## 4.10 Precision and Recall\n\n- **Using sklearn utilities for the same done above**","ffd543c6":"- Some values of `Last Activity` are coverved as values under `Last Notable Activity`\n- So, dropping `Last Notable Activity`","2141490d":"### Getting a relative coeffient value for all the features wrt the feature with the highest coefficient\n- **feature_importance = abs(new_params)**","1522e028":"- Almost all entries are `No`.\n\n### `INSIGHT`\n- Not much can be inferred from this column.\n\n### Newspaper","2e75960e":"- All entries are `No`. \n\n### `INSIGHT`\n- Not much can be inferred from this column.\n\n### Update me on Supply Chain Content","cba4894b":"- Dropping `Lead Source_Reference` because of high VIF and P-Value","d9693096":"**7th Iteration**","30ed8f9c":"**Sorting the feature variables based on their relative coefficient values**","9043e5f0":"- Dropping `What is your current occupation_Housewife` column because of high P-Value","cfaf8fbc":"- Almost all entries are `No`.\n\n### `INSIGHT`\n- Not much can be inferred from this column.\n\n### Digital Advertisement","27009c8b":"- `API` and `Landing Page Submission` have `31.16%` and `36.17%` conversion rate respectively and count of lead originated from them are considerable.\n- `Lead Add Form` has `93.63%` conversion rate but count of lead are not very high.\n- `Lead Import` are very less in count.\n\n### `INSIGHT`\n- **Generate more leads from `Lead Add Form`.**\n- **To improve overall lead conversion rate, we need to focus more on improving lead conversion of `API` and `Landing Page Submission`**\n\n### Lead Source"}}