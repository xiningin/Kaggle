{"cell_type":{"3c306495":"code","5c09a5df":"code","1b2e4d94":"code","7e73f240":"code","5bc985eb":"code","3f01cd60":"code","16b385ed":"code","effb83fe":"code","d7e8d6fa":"code","872ee6a0":"code","b8a4d75b":"code","81c83649":"code","410ae113":"code","1c842640":"code","9c39b584":"code","af11f3d0":"code","6ec91515":"code","73c0a159":"code","9360a752":"code","7089d872":"code","76c14df9":"code","36a1dae6":"code","31442fbf":"code","942b88ae":"code","121484de":"code","757189c4":"code","b41a123a":"code","a2c098dd":"markdown","80e4fc53":"markdown","309ee6b7":"markdown","48726da9":"markdown","711de7c1":"markdown","ff27c399":"markdown","c0ec825c":"markdown","3c9d09bf":"markdown","f8e43e8d":"markdown","3a6b3b59":"markdown"},"source":{"3c306495":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfile = '..\/input\/CryptocoinsHistoricalPrices.csv'\n\ndf = pd.read_csv(file,\n                      names = ['No','Date','Open','High','Low','Close','Volume','Market_Cap','Coin','Open_Cero','Close_Cero','Delta'],\n                      index_col = ['Date'], \n                      parse_dates = ['Date'],\n                      usecols = [0, 1, 2, 3, 4, 5, 8, 11],\n                      na_filter = True)\ndf.head()","5c09a5df":"# Looking at first row, it has the names of columns but we already have them, so we'll remove it.\ndf = df[1:]","1b2e4d94":"# Some values are still non numeric, we can see it by looking at their type\nprint('Open column type', type(df.Open[0]))\nprint('Close column type', type(df.Close[0]))","7e73f240":"# But, why they are still str and not numeric? Let's try to convert it into numeric as it\ntry:\n    df.Open = pd.to_numeric(df.Open)\nexcept Exception as e:\n    print(e)\n# Check for Nan    \ndf = df.reset_index()\nprint(df.isnull().sum())","5bc985eb":"# First, we remove the index to manage date column as well\ndf = df.reset_index()\ndf.dropna(inplace=True)\nprint(df.isnull().sum())\n# Once we clean de data set, let's try again to convert Open and Close columns to numeric\nprint(\"Open parsing errors\",pd.to_numeric(df.Open, errors='occurrs').isnull().sum())\nprint(\"Close parsing errors\",pd.to_numeric(df.Close, errors='occurrs').isnull().sum())\n# Converting the data, we found there are no more error while parsing. So, move on!\ndf.Open = pd.to_numeric(df.Open)\ndf.Close = pd.to_numeric(df.Close)","3f01cd60":"from pandas import Timestamp\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Get all rows of BTC\ndf_btc = df[df.Coin == 'BTC']\n# Sort by date (just in case)\ndf_btc.sort_values(by=['Date'])\n# Create dataframe only with dates, this will be used to create the transformation.\ndate_range = pd.date_range(start = '2013-01-01', end = '2018-12-31')\ndf_date = pd.DataFrame({'date': date_range, 'random': np.random.randint(1, high=100, size=len(date_range))})\n# Change date to timestamp to be able to perform transformation\ndate2scal = df_date.date.apply(pd.to_datetime).apply(Timestamp.timestamp)\n\nsc = MinMaxScaler(feature_range = (0,1))\nsc.fit(date2scal.values.reshape(-1,1))","16b385ed":"# Create two copies, both are goingo to be used to train, but one will be for the input and, the other one, for the output.\ndf_x = df_btc.copy()\ndf_y = df_btc.copy()\ndf_x.drop(df_x.head(1).index,inplace=True)\ndf_y.drop(df_y.tail(1).index,inplace=True)","effb83fe":"# The data frame used for input needs to be transformed.\ndf_date = df_x.Date.apply(pd.to_datetime).apply(Timestamp.timestamp)\ndate_scaled = sc.transform(df_date.values.reshape(-1,1))","d7e8d6fa":"# Plot transformed information\nimport matplotlib.pyplot as plt\nopen_log = np.log(df_x.Open.values) \ny_log = np.log(df_y.Open.values)\nplt.plot(open_log)","872ee6a0":"# Create an array of zeros to store all data transformed.\nstruct = np.zeros((open_log.shape[0], 2))\nstruct[:,0] = date_scaled[:,0]\nstruct[:,1] = open_log\ndf_struct = pd.DataFrame({'data': struct[:,1], 'date': df_x.Date})","b8a4d75b":"# Create the array for the training with samples of 60 (this is taken as it from Derrick's writting)\nX_train = np.array([struct[i-60:i] for i in range(60, open_log.shape[0])])\nY_train = np.array([y_log[i] for i in range(60, open_log.shape[0])])","81c83649":"# Import keras modules to build model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM, GRU\nfrom keras.layers import Dropout\nfrom keras import initializers, optimizers\nimport tensorflow as tf\n\n\nsgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nnormal = initializers.glorot_normal()\nbias_init = initializers.Constant(value=1)\n    \nregressor = Sequential()\n\nregressor.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], 2),\n                   bias_initializer=bias_init, unit_forget_bias=True))\nregressor.add(Dropout(0.2))\n    \nregressor.add(LSTM(units = 50, return_sequences = True, recurrent_initializer = normal,\n                   bias_initializer=bias_init, unit_forget_bias=True))\nregressor.add(Dropout(0.2))\n    \nregressor.add(LSTM(units = 50, return_sequences = True, recurrent_initializer = normal,\n                   bias_initializer=bias_init, unit_forget_bias=True))\nregressor.add(Dropout(0.2))\n    \nregressor.add(LSTM(units = 50, recurrent_initializer = normal,\n                   bias_initializer=bias_init, unit_forget_bias=True))\nregressor.add(Dropout(0.2))\n\nregressor.add(Dense(units = 1))\n\nregressor.compile(optimizer = sgd, loss = 'mean_squared_error')\n\nregressor.fit(X_train, Y_train, epochs = 100, batch_size = 32)","410ae113":"# Let's test the model using the training data set\nval_sp = regressor.predict(X_train)","1c842640":"# The resuts looks much the same. Now it's time predict some particular period\nplt.plot(np.exp(Y_train),label='original')\nplt.plot(np.exp(val_sp),label='prediction')\nplt.legend(loc='best')","9c39b584":"# We take the last 60 samples of original data\ndf_test = df[0:60]\n# Again, we just want BTC (this part isn't necessary because looking at original dataframe, the first 60 values are from BTC)\ndf_test = df_test[df_test.Coin == 'BTC']\n# Convert the open values to numeric\ndf_test.Open = pd.to_numeric(df_test.Open)\n# Convert Date to timestamp for convertion using the scalar function from above\nconv_date = df_test.Date.apply(pd.to_datetime).apply(Timestamp.timestamp)\ndate_test_scaled = sc.transform(conv_date.values.reshape(-1,1))\ntest_log = np.log(df_test.Open.values)\n# Combine all convertions into one array\nX_test = np.zeros((test_log.shape[0],2), dtype=\"float64\")\nX_test[:,0] = date_test_scaled[:,0]\nX_test[:,1] = test_log","af11f3d0":"from keras.preprocessing.sequence import pad_sequences\n# Keras has a functionality to create a pad sequence based on a list, so we convert all into list. Then pass it to function.\nl = X_test.shape[0]\nx_test = [X_test[v:v+l].tolist() for v in range(0, l)]\nx_test = pad_sequences(x_test, dtype='float64')","6ec91515":"# Predict the corresponding output\nstock_pred = regressor.predict(x_test)\nplt.plot(np.exp(stock_pred))\nplt.plot(df_test.Open.values)","73c0a159":"from statsmodels.tsa.stattools import adfuller\ndef test_stationarity(tieseries):\n\n    rollmean = tieseries.rolling(window=12).mean()\n    rollstd = tieseries.rolling(window=12).std()\n\n    plt.plot(tieseries, color=\"blue\", label=\"Original\")\n    plt.plot(rollmean, color=\"red\", label=\"Rolling Mean\")\n    plt.plot(rollstd, color=\"black\", label=\"Rolling Std\")    \n    plt.legend(loc='best')\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show(block=False)\n\n    #Perform Dickey-Fuller test:\n    print('Results of Dickey-Fuller Test:')\n    dftest = adfuller(tieseries, autolag='AIC')\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    print(dfoutput,'%f' % (1\/10**8))","9360a752":"#df_x = df_btc.copy()\n#df_y = df_btc.copy()\ndf_struct = df_x\ndf_struct.Open = np.log(df_x.Open)\ndf_struct.Date = df_x.Date.apply(pd.to_datetime)\nts = df_struct.set_index(['Date'])\nts = ts['Open']\ntest_stationarity(ts)","7089d872":"# Moving average\nmov_average = ts.rolling(12).mean()\n# Plot it with the serie\nplt.plot(mov_average)\nplt.plot(ts)","76c14df9":"# Substract the trend\nts_log_mov_av_diff = ts - mov_average\n# Remove Nan values\nts_log_mov_av_diff.dropna(inplace=True)\n# Test again stationarity\ntest_stationarity(ts_log_mov_av_diff)","36a1dae6":"new_df = ts.reset_index()\nnew_date = new_df.Date.apply(pd.to_datetime).apply(Timestamp.timestamp)\nnew_date_scaled = sc.transform(new_date.values.reshape(-1,1))\nnew_open_log = np.log(new_df.Open.values) \nX_test2 = np.zeros((new_open_log.shape[0],2), dtype=\"float64\")\nX_test2[:,0] = new_date_scaled[:,0]\nX_test2[:,1] = new_open_log\ny_log2 = np.log(new_df.Open.values)","31442fbf":"X_train2 = np.array([X_test2[i-60:i] for i in range(60, open_log.shape[0])])\nY_train2 = np.array([y_log2[i] for i in range(60, open_log.shape[0])])","942b88ae":"\nbias_init2 = initializers.Constant(value=-1)\n    \nregressor2 = Sequential()\n\nregressor2.add(GRU(units = 50, return_sequences = True, input_shape = (X_train2.shape[1], 2),\n                   bias_initializer=bias_init2))\nregressor2.add(Dropout(0.2))\n    \nregressor2.add(GRU(units = 50, return_sequences = True, recurrent_initializer = normal,\n                   bias_initializer=bias_init2))\nregressor2.add(Dropout(0.2))\n    \nregressor2.add(GRU(units = 50, return_sequences = True, recurrent_initializer = normal,\n                   bias_initializer=bias_init2))\nregressor2.add(Dropout(0.2))\n    \nregressor2.add(GRU(units = 50, recurrent_initializer = normal,\n                   bias_initializer=bias_init2))\nregressor2.add(Dropout(0.2))\n\nregressor2.add(Dense(units = 1))\n\nregressor2.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\nregressor2.fit(X_train2, Y_train2, epochs = 100, batch_size = 32)","121484de":"# Test the result with the training\nnew_pred = regressor2.predict(X_train2)","757189c4":"plt.plot(new_pred, label='Prediction')\nplt.plot(Y_train2, label='Real value')\nplt.legend(loc='best')","b41a123a":"l = X_test2[:60].shape[0]\nx_test2 = [X_test2[v:v+l].tolist() for v in range(0, l)]\nx_test2 = pad_sequences(x_test2, dtype='float64')\nstock_pred = regressor2.predict(x_test2)\nplt.plot(stock_pred, label='pred')\nplt.plot(X_test2[:60,1], label='real')\nplt.legend(loc='best')","a2c098dd":"Once all information is cleaned, we'll get only the cryptocurrency of interest, that is Bitcon. Also, we sort all values by date and start a transformation.\n\nAccording with Derrick, a neural network works better if data scaled. He use *MinMaxScalar* to transform data but we're going to make some changes and that would be:\n\n* Instead of using only one column to make predictions, we're going to use two column, date and open\n* Date will be transformed with *MinMaxScaler* from 0 to 1\n* Open will be transformed using logarithmic transformation.","80e4fc53":"The results are well approximated, it follows the trend and reach some peaks. But we're not done, let's try to predict just 60 values of original dataset.","309ee6b7":"Wow, it tries to approach as much as it can but they don't look like. These results can be improved by extending the data or the epochs.\n\nBut we'll try to improved these results with another approach, ARIMA and SARIMAX models are used to predict values for time series, one of the main characteristics that needs the data is to be stationary (I'd made a [kernel](https:\/\/www.kaggle.com\/laloromero\/first-approach-to-stock-market-prediction) talking in deep about these model, please take a look for further information), so we're going to use the following function to see if our data is or not stationary.\n\nIt's common to use Dickey-Fuller test to proof stationary, propose a null hypothesis that serie is stationary and an alternative hypothesis otherwise. How we reject the alternative hypothes? if and only if Test Statistic parameter is lower than critical values. If it's lower than the 10% critical value, then we're 90% confidence that the serie is stationary; if it's lower than the 5% critical value, then we're 95% confidence that the serie is stationary and so on.","48726da9":"During some readings I found [this post](https:\/\/www.kdnuggets.com\/2018\/11\/keras-long-short-term-memory-lstm-model-predict-stock-prices.html) in KDnuggets about Long Short-Term Memory and how to use it for stock market prediction. So, I decided to write down a kernel with an example.\n\n### What is (in general) Long Sort-Term Memory (LSTM) ###\n\nLSTM is a type of recurrent neural network (RNN) that store past information to use it when predicting future values. Storing data is made by using hidden layers self connected, that ensure the consistant flow of information simulating a memory.\n\nOne of the most interesting parts about RNN is how the manage the input and output information. As vanilla neural networks (and convolutional neural network) only manage fixed input and output vectors, the RNN can handle sequence of vectors for input, output or both.\n\nFor more in deep explaniation about it, you can refer to the following pages.\n* [A Beginner\u2019s Guide to Implementing Long Short-Term Memory Networks (LSTM).](https:\/\/heartbeat.fritz.ai\/a-beginners-guide-to-implementing-long-short-term-memory-networks-lstm-eb7a2ff09a27)\n* [Recurrent Neural Networks Tutorial, Part 1 \u2013 Introduction to RNNs](http:\/\/www.wildml.com\/2015\/09\/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns\/)\n* [Recurrent Neural Networks for Beginners](https:\/\/medium.com\/@camrongodbout\/recurrent-neural-networks-for-beginners-7aca4e933b82)\n* [An Introduction to Recurrent Neural Networks](https:\/\/medium.com\/explore-artificial-intelligence\/an-introduction-to-recurrent-neural-networks-72c97bf0912)\n\nNow, knowing the basics of what is a recurrent neural network it's time to procced with the example. We're going to use a data set of historical prices of most popular cryptocoins. \n\nFirst, we'll load numpy and pandas library to read the data. ","711de7c1":"Aaaand the result is better than previous. I mean, it's not perfect, but well approached (better than the other)\n\n## Conclusions ##\nRNN are popular for image recognition because the ability to manage with no fixed inputs, being able to manage other type of data such as time series. Under my perspective, ARIMA and SARIMAX deal better with timeseries, of course, they are model created to deal with it, but I thought that RNN can deal much better because its ability to generalize. Also, the implementation is much harder and requieres much time than using othe models. Of course, it has lots to improve like the architecture, the optimizers and so on. So, fell free to write down your comments on how to improve it and if it's useful.","ff27c399":"Finally!! After lots of preparations we can create the RNN. Here I'll take the freedome to modify the original one posted on Derrick's writting, according with Danijar Hafner in this [post](https:\/\/danijar.com\/tips-for-training-recurrent-neural-networks\/) we add the following optimizations:\n* Layer normalization (after different testing, I only add normalization for the hidden layers)\n* Forget gate bias, all bias initialized at 1\n\nFrom Derrick original model, we have:\n> 1. 50 units which is the dimensionality of the output space\n> 2. return_sequences=True which determines whether to return the last output in the output sequence, or the full sequence\n> 3. input_shape as the shape of our training set.\n\nDerrick and Danijar suggest to add a dropout to improve results. But I was wondering\n\n### What is dropout and why is used? ###\n According to Wikipedia\n> The term \u201cdropout\u201d refers to dropping out units (both hidden and visible) in a neural network.\n\nThat is, when a neural network is learning we specifiy the percentage of units that will be dropped at random and is used because it abvois the overfitting avoiding the co-dependency.\n\nTo review more technical details, refere to this text [Dropout in (Deep) Machine learning](https:\/\/medium.com\/@amarbudhiraja\/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5).\n\nAnd last but not least I had read out there (sorry, but I lost where I found this) that SGD improves LSTM and GRU networks. I made some experiments and find out that SGD improves results, for this particular case, than Adam. The SGD parameters was taken from keras [documentation](https:\/\/keras.io\/optimizers\/). Other thing added (but that I lost where I found it) is bias initializer with variable bias using normal distribution.","c0ec825c":"The result accept out alternative hypothesis, meaning that our serie is not stationary. But we can convert it by removing it's trend by substracting a moving average, again, further information is available in my [kernel](https:\/\/www.kaggle.com\/laloromero\/first-approach-to-stock-market-prediction)","3c9d09bf":"Taking a look at the error message, it says that some values aren't numbers, the message said there are a value that has a comma and, for some reason, pandas can't interpretate it. Also, there are columns having Nan values and one of them is Date column. Let's clean this up by removing all Nan values for Date.","f8e43e8d":"When we test the stationarity after removing trend, we observe this time the serie is stationary with a 99% of confidence. Now the serie is stationary, let's make the same convertions as previous.","3a6b3b59":"And here we are again, this time we're going to change the RNN from a LSTM to a GRU. Removing unit forget gate (because it hasn't), and changing bias initializer for a constant value of -1. Also, the optimizer is Adam because, after various experiments, it improves better the RNN."}}