{"cell_type":{"23943a2f":"code","e4f4a302":"code","f245009f":"code","5137d98f":"code","f0c9d5c6":"code","a2e3c174":"code","04785799":"code","a67a5d2a":"code","c08ceaef":"code","4653b6ad":"code","7146379f":"code","5bcf8fed":"code","d22ed417":"code","22587012":"code","48bb6108":"code","8646ee77":"code","3c2748b2":"code","66ac0c4f":"code","bd35cf30":"code","52e9513f":"code","72c72199":"code","1795bef9":"code","62285107":"code","fc9049dd":"code","5bb46505":"code","e53f6fe2":"code","e935bb21":"code","45e0e061":"code","5f024b57":"code","bdc18639":"code","54a650b0":"code","d7ee5880":"code","73962840":"markdown","6b5f0436":"markdown","3ee7b8ef":"markdown","60f79791":"markdown","a573c192":"markdown","3a360401":"markdown","12d61e1e":"markdown","305c41d8":"markdown","3f062f48":"markdown","2800d20e":"markdown","b3738cf0":"markdown","49e7640d":"markdown","9399d46f":"markdown","4c74c65d":"markdown","2cc0690e":"markdown","73edd2f5":"markdown","d447426a":"markdown","037fb022":"markdown","c92683dc":"markdown","9cfec7f6":"markdown","665e4369":"markdown","02b837d4":"markdown","da1c951b":"markdown","999c81fa":"markdown"},"source":{"23943a2f":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n","e4f4a302":"# Load the training set\ntrain_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n\n# Load the test set\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\n# df List to impute or perform any common operation on both datasets. \ndf_all_data = [train_data, test_data]\n\nprint (train_data.shape)\nprint (test_data.shape)","f245009f":"print('Train columns with null values:\\n', train_data.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values:\\n', test_data.isnull().sum())\nprint(\"-\"*10)","5137d98f":"# fetch empty 'Embarked' values from train set\n\ntrain_data[train_data['Embarked'].isnull()]","f0c9d5c6":"# get average Fare values for Pclass=1 across the Embarkation ports\n\ntrain_data[train_data['Pclass'] ==1].groupby('Embarked')['Fare'].median()","a2e3c174":"# apply 'Embarked' with 'C'\n\ntrain_data['Embarked'].fillna('C', inplace=True)","04785799":"# fetch empty 'Fare' values from test set\n\ntest_data[test_data['Fare'].isnull()]","a67a5d2a":"# get avergae fare where Pclass=3, Embarked='S' and Sex='male'\n\navg_fare = test_data[(test_data.Pclass == 3) & \n                     (test_data.Embarked == \"S\") & \n                     (test_data.Sex == \"male\")].Fare.mean()\n\n# apply the average fare to the empty record\n\ntest_data.Fare.fillna(avg_fare, inplace=True)","c08ceaef":"train_data[train_data['Age'].isnull()]\ntest_data[test_data['Age'].isnull()]","4653b6ad":"# function to predict age based on other independent features using machine learning model\n\ndef predictAge(df):\n    \n    df_withAge = df[pd.isnull(df['Age']) == False]\n    df_WithoutAge = df[pd.isnull(df['Age'])]\n\n    colums = ['Pclass','SibSp','Parch','Fare','Age']\n\n    df_withAge = df_withAge[colums]\n    df_WithoutAge = df_WithoutAge[colums]\n\n    features = ['Pclass','SibSp','Parch','Fare']\n\n    model = RandomForestRegressor()\n    model.fit(df_withAge[features], df_withAge['Age'])\n\n    predicted_age = model.predict(X = df_WithoutAge[features])\n    \n    df.loc[df.Age.isnull(), \"Age\"] = predicted_age.astype(int)\n    \n\ndef getAgeGroup(age):\n    a=''\n    if age<=15:\n        a='Child'\n    elif age<=30:\n        a='Young'\n    elif age<=50:\n        a='Adult'\n    else:\n        a='Old'\n    return a","7146379f":"#call the function to apply missing age on train set\n\npredictAge(train_data)\n\n#call the function to apply missing age on test set\n\npredictAge(test_data)   ","5bcf8fed":"train_data\ntest_data","d22ed417":"# loop through both train and test datasets\n\nfor dataset in df_all_data:   \n\n    #If SibSp and\/or Parch is > 0, add it to determine the FamilySize\n    dataset['TotalFamilyMembers'] = dataset.SibSp + dataset.Parch + 1\n    \n    family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\n    dataset['FamilySize'] = dataset['TotalFamilyMembers'].map(family_map)\n\n   \n    #this is now not required as we have mapped the FamilySize and IsAlone feature using the count\n    dataset.drop(['TotalFamilyMembers'], axis=1, inplace=True)\n    \n\n    #get the title of the passenger\n    dataset['Title'] = dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n  \n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr',\\\n                                                    'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n  \n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\n    \n    dataset['AgeGroup']=dataset['Age'].map(getAgeGroup)\n    \n  ","22587012":"train_data\ntest_data","48bb6108":"cat_features = ['Embarked', 'Parch', 'Pclass', 'Sex', 'SibSp', 'FamilySize', 'AgeGroup','Title']\n\nfig, axs = plt.subplots(ncols=2, nrows=3, figsize=(20, 10))\nplt.subplots_adjust(right=1.5, top=1.25)\n\nfor i, feature in enumerate(cat_features, 1):    \n    plt.subplot(2, 4, i)\n    sns.countplot(x=feature, hue='Survived', data=train_data)\n    \n    plt.xlabel('{}'.format(feature), size=15, labelpad=10)\n    plt.ylabel('Passenger Count', size=15, labelpad=15)    \n    plt.tick_params(axis='x', labelsize=15)\n    plt.tick_params(axis='y', labelsize=15)\n    \n    plt.legend(['Not Survived', 'Survived'], loc='upper center', prop={'size': 15})\n    plt.title('Count of Survival vs {} '.format(feature), size=15, y=1)\n\nplt.show()","8646ee77":"#drop the columns from test and train dataset\n\nPassengerId = test_data['PassengerId']\n\ntrain_data.drop(['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age'], axis=1, inplace=True)\ntest_data.drop(['PassengerId','Name','Cabin','Ticket','SibSp','Parch','Age'], axis=1, inplace=True)","3c2748b2":"train_data = pd.get_dummies(train_data, \n                    columns=['Pclass','Sex','Embarked','FamilySize','Title','AgeGroup'], drop_first=False)\ntest_data = pd.get_dummies(test_data, \n                    columns=['Pclass','Sex','Embarked','FamilySize','Title','AgeGroup'], drop_first=False)\n\n##","66ac0c4f":"plt.figure(figsize=(14,14))\nplt.title('Corelation Matrix', size=8)\nsns.heatmap(train_data.astype(float).corr(method='pearson').abs(),linewidths=0.1,vmax=1.0, \n            square=True, cmap='coolwarm', linecolor='white', annot=True)\nplt.show()","bd35cf30":"# separating our independent and dependent variable\nX = train_data.drop(['Survived'], axis = 1)\ny = train_data[\"Survived\"]\ncolumn_names = X.columns\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size = .20, random_state=0)","52e9513f":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\npd.DataFrame(X_train, columns=column_names).head()","72c72199":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\nfrom sklearn.metrics import mean_absolute_error, accuracy_score\nfrom sklearn.metrics import classification_report, balanced_accuracy_score, confusion_matrix\n","1795bef9":"## Logistic Regression\n\n## Choosing penalties(Lasso(l1) or Ridge(l2))\npenalties = ['l1','l2']\n\nmax_iters = [1000,2500,3000,5000]\n\n## setting param for param_grid in GridSearchCV. \nparam = {'penalty': penalties, 'C': np.logspace(-4, 4, 20), 'max_iter': max_iters, 'solver' : ['liblinear']}\n\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\n\n\n## Calling on GridSearchCV object. \ngrid = GridSearchCV(estimator=LogisticRegression(), \n                            param_grid = param,\n                            scoring = 'accuracy',\n                            n_jobs =-1,\n                            cv = 5)\n\n\n## Fitting the model\ngrid.fit(X_train, y_train)\n\nmodel_logreg = grid.best_estimator_\n\nmodel_logreg.fit(X_train, y_train)\n\ny_pred = model_logreg.predict(X_test)\n\nlog_reg_accuracy = accuracy_score(y_test,y_pred)\n\nprint (\"Model Accuracy:\\t{}\".format(log_reg_accuracy))\n\nprint(\"-\"*10)\n\nprint(classification_report(y_test, y_pred))\n\npd.DataFrame(confusion_matrix(y_test,y_pred),\\\n            columns=[\"Predicted Not-Survived\", \"Predicted Survived\"],\\\n            index=[\"Not-Survived\",\"Survived\"] )","62285107":"from sklearn.tree import DecisionTreeClassifier\n\nmax_depth = range(1,30)\nmax_feature = [15,20,25,30,35,'auto']\n\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\n\nmodel_dtc = DecisionTreeClassifier()\n\ngrid = GridSearchCV(estimator=DecisionTreeClassifier(), \n                                  param_grid = param, \n                                  verbose=False, \n                                  cv=5,\n                                  n_jobs = -1)\n## Fitting the model\n\ngrid.fit(X_train, y_train)\n\nmodel_dtc = grid.best_estimator_\n\nmodel_dtc.fit(X_train, y_train)\n\ny_pred = model_dtc.predict(X_test)\n\nmodel_dtc_accuracy = accuracy_score(y_test,y_pred)\n\nprint (\"Model Accuracy:\\t{}\".format(model_dtc_accuracy))\n\nprint(\"-\"*10)\n\nprint(classification_report(y_test, y_pred))\n\npd.DataFrame(confusion_matrix(y_test,y_pred),\\\n            columns=[\"Predicted Not-Survived\", \"Predicted Survived\"],\\\n            index=[\"Not-Survived\",\"Survived\"] )","fc9049dd":"from sklearn.svm import SVC\n\nCs = [0.001, 0.01, 0.1, 1,1.5,2,2.5,3,4,5, 10] ## penalty parameter C for the error term. \ngammas = [0.0001,0.001, 0.01, 0.1, 1]\nparam_grid = {'C': Cs, 'gamma' : gammas}\n\n\ngrid = GridSearchCV(SVC(kernel = 'rbf', probability=True), param_grid, cv=5) ## 'rbf' stands for gaussian kernel\n\ngrid.fit(X_train, y_train)\n\nmodel_svc = grid.best_estimator_\n\nmodel_svc.fit(X_train, y_train)\n\ny_pred = model_svc.predict(X_test)\n\nmodel_svc_accuracy = accuracy_score(y_test,y_pred)\n\nprint (\"Model Accuracy:\\t{}\".format(model_svc_accuracy))\n\nprint(\"-\"*10)\n\nprint(classification_report(y_test, y_pred))\n\npd.DataFrame(confusion_matrix(y_test,y_pred),\\\n            columns=[\"Predicted Not-Survived\", \"Predicted Survived\"],\\\n            index=[\"Not-Survived\",\"Survived\"] )","5bb46505":"from sklearn.ensemble import RandomForestClassifier\n\nn_estimators = [120,125,130,135,140,150,160,180,200];\nmax_depth = range(1,10);\ncriterions = ['gini', 'entropy'];\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions}\n\ngrid = GridSearchCV(estimator=RandomForestClassifier(max_features = 'auto'),\n                                 param_grid=parameters,\n                                 cv=5,\n                                 n_jobs = -1)\ngrid.fit(X_train,y_train)\n\nmodel_rfc = grid.best_estimator_\n\nmodel_rfc.fit(X_train,y_train)\n\n\ny_pred = model_rfc.predict(X_test)\n\nmodel_rfc_accuracy = accuracy_score(y_test,y_pred)\n\nprint (\"Model Accuracy:\\t{}\".format(model_rfc_accuracy))\n\nprint(\"-\"*10)\n\nprint(classification_report(y_test, y_pred))\n\npd.DataFrame(confusion_matrix(y_test,y_pred),\\\n            columns=[\"Predicted Not-Survived\", \"Predicted Survived\"],\\\n            index=[\"Not-Survived\",\"Survived\"] )\n","e53f6fe2":"#from xgboost import XGBClassifier\n\nfrom xgboost.sklearn import XGBClassifier\n\nparameters = {'objective':['binary:logistic'],\n              'learning_rate': [0.05], #so called `eta` value\n              'max_depth': [5,6,7],\n              'subsample': [0.6,0.7,0.8],\n              'n_estimators': [200,400,600,800,1000]}\n\ngrid = GridSearchCV(XGBClassifier(), parameters, scoring='accuracy', n_jobs=-1, cv=5)\n\ngrid.fit(X_train,y_train)\n\nmodel_xgb = grid.best_estimator_\n\nmodel_xgb.fit(X_train,y_train)\n\ny_pred = model_xgb.predict(X_test)\n\nmodel_xgb_accuracy = accuracy_score(y_test,y_pred)\n\nprint (\"Model Accuracy:\\t{}\".format(model_xgb_accuracy))\n\nprint(\"-\"*10)\n\nprint(classification_report(y_test, y_pred))\n\npd.DataFrame(confusion_matrix(y_test,y_pred),\\\n            columns=[\"Predicted Not-Survived\", \"Predicted Survived\"],\\\n            index=[\"Not-Survived\",\"Survived\"] )\n","e935bb21":"# predict probabilities\npred_prob1 = model_logreg.predict_proba(X_test)\npred_prob2 = model_dtc.predict_proba(X_test)\npred_prob3 = model_svc.predict_proba(X_test)\npred_prob4 = model_rfc.predict_proba(X_test)\npred_prob5 = model_xgb.predict_proba(X_test)\n\n\nfrom sklearn.metrics import roc_curve\n\n# roc curve for models\nfpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)\nfpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)\nfpr3, tpr3, thresh3 = roc_curve(y_test, pred_prob3[:,1], pos_label=1)\nfpr4, tpr4, thresh4 = roc_curve(y_test, pred_prob4[:,1], pos_label=1)\nfpr5, tpr5, thresh5 = roc_curve(y_test, pred_prob5[:,1], pos_label=1)\n\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)\n","45e0e061":"from sklearn.metrics import roc_auc_score\n\n# auc scores\nlogreg_auc_score = roc_auc_score(y_test, pred_prob1[:,1])\ndtc_auc_score = roc_auc_score(y_test, pred_prob2[:,1])\nsvc_auc_score = roc_auc_score(y_test, pred_prob3[:,1])\nrfc_auc_score = roc_auc_score(y_test, pred_prob4[:,1])\nxgb_auc_score = roc_auc_score(y_test, pred_prob5[:,1])\n\n\n\nprint(logreg_auc_score,dtc_auc_score,svc_auc_score,rfc_auc_score,xgb_auc_score)","5f024b57":"results = pd.DataFrame({\n    'Model Key': ['Logistic Regression', 'Decision Tree', 'SVC', 'Random Forest', 'XGBClassifier'],\n    'Model Name': [model_logreg, model_dtc, model_svc, model_rfc, model_xgb],\n    'Model Accuracy': [ log_reg_accuracy, model_dtc_accuracy, model_svc_accuracy, model_rfc_accuracy, model_xgb_accuracy],\n    'Model Auc_Score': [ logreg_auc_score,dtc_auc_score,svc_auc_score,rfc_auc_score,xgb_auc_score]})\n\nresult_df = results.sort_values(by=['Model Accuracy'], ascending=False)\nresult_df.reset_index(drop=True)\n","bdc18639":"import matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\n# plot roc curves\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')\nplt.plot(fpr2, tpr2, linestyle='-.',color='green', label='Decision Tree')\nplt.plot(fpr3, tpr3, linestyle=':',color='brown', label='SVC')\nplt.plot(fpr4, tpr4, linestyle='-',color='purple', label='Random Forest Classifier')\nplt.plot(fpr5, tpr5, linestyle='-.',color='black', label='XGB')\n\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","54a650b0":"model_xgb.fit(X,y)\n\ntest_prediction = model_xgb.predict(test_data)\n\nsurvivors = pd.DataFrame(test_prediction, columns = ['Survived'])\n\nlen(survivors)\n\nsurvivors.insert(0, 'PassengerId', PassengerId, True)\n\nsurvivors\n","d7ee5880":"survivors.to_csv('first_submission_xgb.csv', index = False)","73962840":"Feature scaling is an important concept of machine learning models. Often times a dataset contain features highly varying in magnitude and unit.\nHere Age and Fare is much higher in magnitude compared to others machine learning features. This can create problems as many machine learning models will get confused thinking Age and Fare have higher weight than other features. Therefore, we need to do feature scaling to get a better result. ","6b5f0436":"<h2>Dealing with Missing Values<\/h2>\nIt is good practice to identify and replace missing values for each column in your input data prior to modeling your prediction task. This is called missing data imputation, or imputing for short.\n\nA popular approach for data imputation is to calculate a statistical value for each column (such as a mean) and replace all missing values for that column with the statistic. It is a popular approach because the statistic is easy to calculate using the training dataset and because it often results in good performance.\n\nLet us see which records have null values in 'Embarked' column\n\nNote: **Embarked** column represents Port of Embarkation in this dataset. \n\nThe possible values are : C (Cherbourg), Q (Queenstown), S (Southampton)","3ee7b8ef":"From the above results, we may be able to solve these two missing values by looking at other independent variables of the two raws. \nBoth passengers paid a fare of $80, are of Pclass 1 and female Sex.\n\nLet us make use of this information and find out the median 'Fare' values for each 'Embarked' port across the training dataset for Pclass=1","60f79791":"Now its time to look at the empty 'Age' records ","a573c192":"There is only 1 record with empty Fare column. \nLet us apply same 'Fare' as per the other independent features ","3a360401":"Given below are some helper functions that will assist in predicting age using machine learning models.\nThe other functions will help in apply missing Cabin values based on average Fare values from similar records - which we will see in coming sections","12d61e1e":"<h2>Loading Datasets<\/h2>\n<p>We will now proceed to load the train and test dataset provided with this competition <\/p>","305c41d8":"The above result shows the average 'Fare' across various Embarkation ports.\n\nFor our missing records, the Fare = $80 is close to Embarked port = 'C'. Let us impute 'C' for these null values","3f062f48":"<h2>Final Submission<\/h2>","2800d20e":"<p>As you see above, the shape returns data across the train and test set.\nIn the train set we have 12 columns whereas in test we have 11 columns. The \"Survived\" column is intentionally removed from test set in order to apply the model preditions<\/p>\n\nNow let us dig deep into the dataset to see if there are any null values.\nMissing values are representative of the messiness of real world data. There can be a multitude of reasons why they occur \u2014 ranging from human errors during data entry, incorrect sensor readings, to software bugs in the data processing pipeline.","b3738cf0":"Now, let us see which records have null values in 'Fare' column in test dataset","49e7640d":"Submit the results","9399d46f":"<h2> Helper Functions <\/h2>","4c74c65d":"Please upvote if you like this notebook","2cc0690e":"The first thing you should do is count how many you have and try to visualize their distributions. For this step to work properly you should manually inspect the data (or at least a subset of it) to try to determine how they are designated. Possible variations are: \u2018NaN\u2019, \u2018NA\u2019, \u2018None\u2019, \u2018 \u2019, \u2018?\u2019 and others.\n\nIn the above result, we see features - Age, Cabin, Embarked are having null (Possible variations are: \u2018NaN\u2019, \u2018NA\u2019, \u2018None\u2019, \u2018 \u2019, \u2018?\u2019 and others) values in large numbers. ","73edd2f5":"<h2>Loading Libraries<\/h2>\n<p>Python is a fantastic language with a vibrant community that produces many amazing libraries. I am not a big fan of importing everything at once for the newcomers. So, I am going to introduce a few necessary libraries for now, and as we go on, we will keep unboxing new libraries when it seems appropriate.<\/p>","d447426a":"Now let us encode the categorical columns with unique numeric values using the LabelEncoder \nThis will help us to plot the correlation and heatmap","037fb022":"<h2>Correlation Matrix and Heatmap<\/h2>\nLet us now drop the columns which are of less important for performing the machine learning ","c92683dc":"<h2>Visualizations<\/h2>\n\nData visualization is a technique that uses an array of static and interactive visuals within a specific context to help people understand and make sense of large amounts of data. The data is often displayed in a story format that visualizes patterns, trends and correlations that may otherwise go unnoticed. \n<br><br>\nThe below visualizations show the relationship between 'Survived' Vs other independent features ","9cfec7f6":"Before we apply any machine learning models, It is important to separate dependent and independent variables. Our dependent variable or target variable is something that we are trying to find, and our independent variable is the features we use to find the dependent variable. The way we use machine learning algorithm in a dataset is that we train our machine learning model by specifying independent variables and dependent variable. To specify them, we need to separate them from each other, and the code below does just that.","665e4369":"<h2>Modeling the data<\/h2>","02b837d4":"From the above correations we see there is strong relation between Survived and Sex, followed by Titles","da1c951b":"Now let us work on feature engineering\n\n<h2> Feature Engineering <\/h2>","999c81fa":"From the above results, XGBClassifier appears to be efficient \nLets use this model on the test data and submit the results"}}