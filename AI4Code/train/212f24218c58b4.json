{"cell_type":{"3568dd3c":"code","1f9360c8":"code","43aaca84":"code","d870da20":"code","31b21b4d":"code","c7446d7d":"code","9aa59130":"code","fb08eafe":"code","128da4c5":"code","f63eee94":"code","093e1f4b":"code","8184354d":"code","22e5e13d":"code","0ee9f7c8":"code","77d7c216":"code","77c7e95f":"code","db6f37f4":"code","9571ad26":"code","14c88abd":"code","85ead4b4":"code","9625f799":"code","b0e46b11":"code","9848c576":"code","38ede5fd":"code","a7c72375":"code","7d6fd3ad":"code","cbbaa533":"markdown","867d8662":"markdown","e6523acf":"markdown","4384a544":"markdown"},"source":{"3568dd3c":"!pip install git+https:\/\/github.com\/qubvel\/segmentation_models.pytorch","1f9360c8":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport random\nfrom torch.utils.data import Dataset\nimport os\nimport cv2\nimport albumentations as albu\nfrom sklearn.model_selection import train_test_split\nimport segmentation_models_pytorch as smp\nfrom torch.utils.data import DataLoader\nimport torch\nfrom catalyst.dl.runner import SupervisedRunner\nfrom catalyst.dl.callbacks import DiceCallback, InferCallback, OptimizerCallback,CriterionCallback,CheckpointCallback,JaccardCallback,IouCallback\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nimport tqdm\nfrom catalyst.contrib.criterion import DiceLoss, IoULoss, FocalLossBinary\nimport torch.nn as nn\nimport pandas as pd\nfrom catalyst.dl import utils\nimport gc\nfrom torch.optim.optimizer import Optimizer, required\nimport math","43aaca84":"#plot descriptive stats\ndef DescriptiveStats(trainData):\n    #check how many fish,flower,gravel and sugar valid data are in the training set\n    occuranceDict={\"Fish\":0,\"Flower\":0,\"Gravel\":0,\"Sugar\":0}\n    for i in range(len(trainData['EncodedPixels'])):\n        if (pd.isnull(trainData['EncodedPixels'][i]))==False:\n            cloudType=trainData[\"Image_Label\"][i].split('_')[1]\n            occuranceDict[cloudType]+=1   \n    print(\"how many fish,flower,gravel and sugar valid data are in the training set\")\n    labels = occuranceDict.keys()\n    sizes = occuranceDict.values()\n    explode = (0.1, 0.1, 0.1, 0.1) \n    \n    fig1, ax1 = plt.subplots()\n    ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n            shadow=True, startangle=90)\n    ax1.axis('equal')  \n    plt.show()\n    print(\"how many clouds are per picture\")\n    #check how many clouds are per picture in the training set\n    occurancePerPic=trainData.loc[trainData['EncodedPixels'].isnull() == False, 'Image_Label'].apply(lambda x: x.split('_')[0]).value_counts().value_counts()\n    labels2 = occurancePerPic.keys() \n    fig2, ax2 = plt.subplots()\n    ax2.pie(occurancePerPic, explode=explode, labels=labels2, autopct='%1.1f%%',\n            shadow=True, startangle=90)\n    ax2.axis('equal')  \n    plt.show()","d870da20":"#show a specific image\ndef ShowImg(trainData,imgName):\n    image = Image.open(path+\"\/train_images\/\"+imgName)\n    print(\"Original Img\")\n    plt.imshow(image)\n    plt.show()\n    ss=trainData.loc[trainData['im_id']==imgName, 'EncodedPixels']\n    for row in ss:\n        print(trainData.loc[trainData['EncodedPixels']==row, \"label\"])\n        try: # label might not be there!\n            mask = rle_decode(row)\n        except Exception as exception:\n            mask = np.zeros((1400, 2100))\n            continue\n            \n        plt.imshow(image)\n        plt.imshow(mask, alpha=0.3, cmap='gray')\n        plt.show()","31b21b4d":"# create a custom loss (total loss=alfa*IoULoss + beta*DiceLoss + gamma*FocalLossBinary)\nclass CustomLoss(nn.Module):\n    def __init__(self, alpha=.7, beta=1.5, gamma=.4):\n        super().__init__()\n        self.alpha=alpha\n        self.beta=beta\n        self.gamma=gamma\n        self.lossIOU=IoULoss()\n        self.lossDice=DiceLoss()\n        self.lossFocal=FocalLossBinary()\n    def forward(self, input, target):\n        loss=self.alpha*self.lossIOU(input.cpu(), target.cpu()) + self.beta*self.lossDice(input.cpu(), target.cpu()) + self.beta*self.lossFocal(input.cpu(), target.cpu())\n        return loss.mean()\/(alpha+beta+gamma)","c7446d7d":"class RAdam(Optimizer):\n\n    def __init__(self, params, lr=2*1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        \n        self.degenerated_to_sgd = degenerated_to_sgd\n        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n            for param in params:\n                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n                    param['buffer'] = [[None, None, None] for _ in range(10)]\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = group['buffer'][int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 \/ (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t \/ (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) \/ (N_sma_max - 4) * (N_sma - 2) \/ N_sma * N_sma_max \/ (N_sma_max - 2)) \/ (1 - beta1 ** state['step'])\n                    elif self.degenerated_to_sgd:\n                        step_size = 1.0 \/ (1 - beta1 ** state['step'])\n                    else:\n                        step_size = -1\n                    buffered[2] = step_size\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:\n                    if group['weight_decay'] != 0:\n                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n                    p.data.copy_(p_data_fp32)\n                elif step_size > 0:\n                    if group['weight_decay'] != 0:\n                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n                    p.data.copy_(p_data_fp32)\n\n        return loss","9aa59130":"#Create mask based on df, image name and shape\ndef make_mask(df, image_name= 'img.jpg',\n              shape= (1400, 2100)):\n    encoded_masks = df.loc[df['im_id'] == image_name, 'EncodedPixels']\n    masks = np.zeros((shape[0], shape[1], 4), dtype=np.float32)\n\n    for idx, label in enumerate(encoded_masks.values):\n        if label is not np.nan:\n            mask = rle_decode(label)\n            masks[:, :, idx] = mask\n\n    return masks","fb08eafe":"#Decode rle encoded mask    \ndef rle_decode(mask_rle='', shape=(1400, 2100)):\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int)\n                       for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape, order='F')","128da4c5":"def mask2rle(img):\n    pixels = img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","f63eee94":"#reshape after using albumentation\ndef ConvertToTensorFormat(x, **kwargs):\n    return x.transpose(2, 0, 1).astype('float32')","093e1f4b":"def post_process(probability, threshold, min_size):\n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((350, 525), np.float32)\n    num = 0\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predictions[p] = 1\n            num += 1\n    return predictions, num","8184354d":"def dice(img1, img2):\n    img1 = np.asarray(img1).astype(np.bool)\n    img2 = np.asarray(img2).astype(np.bool)\n\n    intersection = np.logical_and(img1, img2)\n\n    return 2. * intersection.sum() \/ (img1.sum() + img2.sum())\n\ndef sigmoid(x): return 1\/(1+np.exp(-x))","22e5e13d":"class CloudDataset(Dataset):\n    def __init__(self,data,dataSetType,transforms,img_ids,preprocessing):\n        self.data=data\n        self.preprocessing=preprocessing\n        self.transforms=transforms\n        self.img_ids=img_ids\n        if (dataSetType==\"train\"):\n            self.imgFolder=path+\"\/train_images\"\n        if (dataSetType==\"test\"):\n            self.imgFolder=path+\"\/test_images\"\n\n    def __getitem__(self, idx):\n        image_name = self.img_ids[idx]\n\n        mask = make_mask(self.data, image_name)\n        image_path = os.path.join(self.imgFolder, image_name)\n        \n        img = cv2.imread(image_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        augmented = self.transforms(image=img, mask=mask)\n        img = augmented['image']\n        mask = augmented['mask']\n        \n        if self.preprocessing:\n            preprocessed = self.preprocessing(image=img, mask=mask)\n            img = preprocessed['image']\n            mask = preprocessed['mask']\n            \n            \n        return img, mask\n\n    def __len__(self):\n        return len(self.img_ids)      ","0ee9f7c8":"#preprocess for specific network used\ndef get_preprocessing(preprocessing_fn=None):\n    if preprocessing_fn is not None:\n        _transform = [\n            albu.Lambda(image=preprocessing_fn),\n            albu.Lambda(image=ConvertToTensorFormat, mask=ConvertToTensorFormat),\n        ]\n    else:\n        _transform = [\n            albu.Normalize(),\n            albu.Lambda(image=ConvertToTensorFormat, mask=ConvertToTensorFormat),\n        ]\n    return albu.Compose(_transform)\ndef get_training_augmentation(p=0.5):\n    train_transform = [\n        albu.Resize(320, 640),\n        albu.HorizontalFlip(p=0.25),\n        albu.VerticalFlip(p=0.25),\n        albu.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=0.5, border_mode=0),\n        albu.GridDistortion(p=0.25)\n    ]\n    return albu.Compose(train_transform)\n\n#for validation dataset it is just resize\ndef get_validation_augmentation():\n    train_transform = [\n        albu.Resize(320, 640)\n    ]\n    return albu.Compose(train_transform)","77d7c216":"path = '..\/input\/understanding_cloud_organization'\n\n#read files\ntrainData = pd.read_csv(path+'\/train.csv')\nsubSample = pd.read_csv(path+'\/sample_submission.csv')\n\n#We can see that are not major imbalance in the dataset\nDescriptiveStats(trainData)\n\n#rearrange dataframe\ntrainData['label'] = trainData['Image_Label'].apply(lambda x: x.split('_')[1])\ntrainData['im_id'] = trainData['Image_Label'].apply(lambda x: x.split('_')[0])\n\nsubSample['label'] = subSample['Image_Label'].apply(lambda x: x.split('_')[1])\nsubSample['im_id'] = subSample['Image_Label'].apply(lambda x: x.split('_')[0])","77c7e95f":"#plot a random image\nimageToPlot=random.choice(trainData['im_id'])\nShowImg(trainData,imageToPlot)\n","db6f37f4":"uniqueImgId=trainData.im_id.unique()\n#split in train-test\ntrain_ids, valid_ids = train_test_split(\n        uniqueImgId,\n        random_state=142,\n        test_size=0.08)\n#using efficientnet-b3 with imagenet weights\nENCODER = 'se_resnext101_32x4d'\nENCODER_WEIGHTS = 'imagenet'\nDEVICE = 'cuda'\npreprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)\nnum_workers = 0\nbs = 6\n\nACTIVATION = None\nmodel = smp.FPN(\n    encoder_name=ENCODER, \n    encoder_weights=ENCODER_WEIGHTS, \n    classes=4, \n    activation=ACTIVATION,\n)","9571ad26":"train_dataset = CloudDataset(data=trainData, dataSetType='train', img_ids=train_ids, transforms = get_training_augmentation(), preprocessing=get_preprocessing(preprocessing_fn))\nvalid_dataset = CloudDataset(data=trainData, dataSetType='train', img_ids=valid_ids, transforms = get_validation_augmentation(), preprocessing=get_preprocessing(preprocessing_fn))\n\ntrain_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=num_workers)\nvalid_loader = DataLoader(valid_dataset, batch_size=bs, shuffle=False, num_workers=num_workers)\nloaders = {\n    \"train\": train_loader,\n    \"valid\": valid_loader\n}","14c88abd":"num_epochs = 50\nlogdir = \".\/logs\/CloudsSegmentation\/\"\n\noptimizer = RAdam(model.parameters())\n            \n#criterion=CustomLoss()\ncriterion=smp.utils.losses.BCEDiceLoss()\nscheduler = ReduceLROnPlateau(optimizer, factor=0.3, patience=5)\n\nrunner = SupervisedRunner()\ntorch.cuda.empty_cache()\ngc.collect()    ","85ead4b4":"runner.train(\n    model=model,\n    criterion=criterion,\n    optimizer=optimizer,\n    scheduler=scheduler,\n    loaders=loaders,\n    callbacks=[CriterionCallback(),OptimizerCallback(accumulation_steps =3)],\n    logdir=logdir,\n    num_epochs=num_epochs,\n    verbose=True\n)","9625f799":"#load best checkpoint\n\nencoded_pixels = []\nloaders = {\"infer\": valid_loader}\nrunner.infer(\n    model=model,\n    loaders=loaders,\n    callbacks=[\n        CheckpointCallback(\n            resume=f\"{logdir}\/checkpoints\/best.pth\"),\n        InferCallback()\n    ],\n)","b0e46b11":"resizedMasks=[]\nprobabilities = np.zeros((len(valid_dataset)*4, 350, 525))\n# for each valid set and prediction on valid set:\n# the predictions should be scaled down to a 350 x 525 pixel image\n# make a resizedMasks of the valid elements\n#make a probabilities mask with 4*len(valid_dataset) (labels)\nfor i in range(len(valid_dataset)):\n    batch=valid_dataset[i]\n    output=runner.callbacks[0].predictions[\"logits\"][i]\n    image, mask = batch\n    for m in mask:\n        m = cv2.resize(m, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n        resizedMasks.append(m)\n        \n    for j in range(len(output)):\n        probability = cv2.resize(output[j], dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n        probabilities[i * 4 + j, :, :] = probability","9848c576":"#find the optimum threshold for each class\n\nclass_params = {}\nfor class_id in range(4):\n    print(\"Calculating optimum threshold for class:\",class_id)\n    attempts = []\n    for t in range(300, 1000, 5):\n        t \/= 100\n        for ms in [9000,10000,11000,12000,13000,14000,15000,16000,17000,18000,19000,20000,21000,22000,23000,24000,25000]:\n            masks = []\n            for i in range(class_id, len(probabilities), 4):\n                probability = probabilities[i]\n                predict, num_predict = post_process(sigmoid(probability), t, ms)\n                masks.append(predict)\n\n            d = []\n            for i, j in zip(masks, resizedMasks[class_id::4]):\n                if (i.sum() == 0) & (j.sum() == 0):\n                    d.append(1)\n                else:\n                    d.append(dice(i, j))\n\n            attempts.append((t, ms, np.mean(d)))\n\n    attempts_df = pd.DataFrame(attempts, columns=['threshold', 'size', 'dice'])\n\n\n    attempts_df = attempts_df.sort_values('dice', ascending=False)\n    print(attempts_df.head())\n    best_threshold = attempts_df['threshold'].values[0]\n    best_size = attempts_df['size'].values[0]\n    \n    class_params[class_id] = (best_threshold, best_size)\n        \n        \nprint(class_params)     ","38ede5fd":"del probabilities\ndel resizedMasks\ndel attempts_df\ntorch.cuda.empty_cache()\ngc.collect()   ","a7c72375":"torch.cuda.empty_cache()\ngc.collect()    \n\ntest_ids = subSample['Image_Label'].apply(lambda x: x.split('_')[0]).drop_duplicates().values\n\ntest_dataset = CloudDataset(data=subSample, dataSetType='test', img_ids=test_ids, transforms = get_validation_augmentation(), preprocessing=get_preprocessing(preprocessing_fn))\ntest_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)\n\nloaders = {\"test\": test_loader}  \n\n\nencoded_pixels = []\nimage_id = 0\nfor i, test_batch in enumerate(tqdm.tqdm(loaders['test'])):\n    runner_out = runner.predict_batch({\"features\": test_batch[0].cuda()})['logits']\n    for i, batch in enumerate(runner_out):\n        for probability in batch:\n            \n            probability = probability.cpu().detach().numpy()\n            if probability.shape != (350, 525):\n                probability = cv2.resize(probability, dsize=(525, 350), interpolation=cv2.INTER_LINEAR)\n            predict, num_predict = post_process(sigmoid(probability), class_params[image_id % 4][0], class_params[image_id % 4][1])\n            if num_predict == 0:\n                encoded_pixels.append('')\n            else:\n                r = mask2rle(predict)\n                encoded_pixels.append(r)\n            image_id += 1","7d6fd3ad":"subSample['EncodedPixels'] = encoded_pixels\nsubSample.to_csv('sub23k.csv', columns=['Image_Label', 'EncodedPixels'], index=False)","cbbaa533":"**Libraries: **  \n\nTorch(with Catalyst and segmentation_models_pytorch), OpenCV, Albumentation and other standard Python libraries(numpy, pandas, matplotlib)  \n\n**Data visualization: ** \n\ncheck how many fish,flower,gravel and sugar valid data are in the training set  \ncheck how many clouds are per picture in the training set  \nshow a specific or random image with the segmentation data plotted over the original image  \n\n**Image augmentation**  \n\nFinal version: Resize(320, 640), HorizontalFlip,VerticalFlip,ShiftScaleRotate  \nOther attempts: Blur, MedianBlur, GridDistortion  \n  \n**Loss function**  \n\nAlthough the project is evaluated by Dice loss I have tested multiple loss function:  \nDice loss  \nIoU loss  \nFocal loss  \nA custom metric which contains a linear combination of Dice loss, IoU loss and Focal loss each with a specific weight  \n\n**Network arhitecture**  \n\nI have build the segmentation arhitecture based on different types of networks:  \nResnet50  \nResnet101\nEfficientnet-b2\nEfficientnet-b7  \nDensenet121  \n\n**Other aspects**  \n  \n  I have tried various learning rate options starting from 1e-3 for Encoder and 1e-2 for Decoder to smaller ones(5e-4 and 5e-3) and even equal learning rates for Encoder and Decoder  \n  Also, I have used and really helped ReduceLROnPlateau with factor=0.3 and patience=5  \n  \n**Problems and Issues**  \n\nI faced a problem with the Catalyst callbacks (DiceCallback(),InferCallback()). The RAM memory keep increasing along with the training iterations until it reaches the kernel limits. It seems that it is a memory leak somewhere or the I am not using them right\n\n**Acknowledgements and Inspirations**  \n\nA lot of thanks to Andrew Lukyanenko. His great kernel was a source of inspiration for general aproach and a lot of usefull functions(https:\/\/www.kaggle.com\/artgor\/segmentation-in-pytorch-using-convenient-tools#Exploring-augmentations-with-albumentations) \n  \n**Next steps**  \n\nDo more data visualization and analytics(cloud types corellations per picture), Aleksandra Deis has a great kernel on data exploration and visualization (https:\/\/www.kaggle.com\/aleksandradeis\/understanding-clouds-eda)  \nTry other data augmentation (Affine, Fliplr, ElasticTransformation, etc.)  \nTry new network arhitectures  \nImplement the same design using PyTorch from scratch(Dhananjay Raut has a great kernel on this: https:\/\/www.kaggle.com\/dhananjay3\/image-segmentation-from-scratch-in-pytorch)\n\n**Best score so far**  \n\n0.6331","867d8662":"\nIn this challenge, we have to build a model to classify cloud organization patterns from satellite images.\nThere are many ways in which clouds can organize, but the boundaries between different forms of organization are murky. This makes it challenging to build traditional rule-based algorithms to separate cloud features. The human eye, however, is really good at detecting features\u2014such as clouds that resemble flowers.\nThe input data are images with 4 different types of clouds (fish, flower, gravel and sugar) and a csv file which describes the clouds position in the image.\nThe predicted encodings should be against images that are scaled by 0.25 per side. In other words, while the images in Train and Test are 1400 x 2100 pixels, the predictions should be scaled down to a 350 x 525 pixel image. The reduction is required to achieve reasonable submission evaluation times","e6523acf":"**About the project**","4384a544":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/MaxPlanck\/Teaser_AnimationwLabels.gif\" width=\"800px\">\n\n"}}