{"cell_type":{"55edcca4":"code","7fbde622":"code","29cc7e73":"code","33df3359":"code","9a712c54":"code","5029592f":"code","642dba2d":"code","63b822dd":"code","58792c23":"code","db31f97d":"code","c92a405d":"code","d5291042":"code","da219233":"code","b5c8325d":"code","dd87ea0b":"code","72cdd314":"code","3c81eab3":"code","cd736706":"code","2928e77d":"markdown","f8563949":"markdown","cd47fe0d":"markdown","958df8c1":"markdown","aa2f0ed0":"markdown","a2a29b9a":"markdown"},"source":{"55edcca4":"import numpy as np\nimport pandas as pd\n\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import roc_auc_score\n\nSEED = 47","7fbde622":"train = pd.read_csv('..\/input\/techuklon-int20h\/train.csv')\ntest = pd.read_csv('..\/input\/techuklon-int20h\/test.csv')\nss = pd.read_csv('..\/input\/techuklon-int20h\/Samle_Submission.csv')","29cc7e73":"agg = ['mean', 'std', 'min', 'max', 'sum', 'median']\n\ntrain_X = train.drop(columns=['target', 'Week']).groupby(['Id']).agg(agg).reset_index()\ntrain_X.columns = ['Id'] + [f'{col}_{stat}' for col in train.columns[2:-1] for stat in agg]\ntrain_X['target'] = train.groupby(['Id'])['target'].max().reset_index()['target']","33df3359":"X, y = train_X.drop(columns=['target', 'Id']), train_X['target']","9a712c54":"def calc(X, y, X_test, model, cv, oof):\n    \n    res = []\n    local_probs = pd.DataFrame()\n    for i, (tdx, vdx) in enumerate(cv.split(X, y)):\n        X_train, X_valid, y_train, y_valid = X.iloc[tdx], X.iloc[vdx], y.iloc[tdx], y.iloc[vdx]\n        model.fit(X_train, y_train)\n        preds = model.predict_proba(X_valid)[:,1]\n        \n        if oof:\n            oof_predict = model.predict_proba(X_test)[:,1]\n            local_probs['fold_%i'%i] = oof_predict\n        \n        score = roc_auc_score(y_valid, preds)\n        print(f'{i} Fold: {score:.4f}')\n        res.append(score)\n    \n    print(f'AVG score: {np.mean(res):.4f}')\n    return np.mean(res), local_probs.mean(axis=1)","5029592f":"test_X = test.drop(columns=['Week']).groupby(['Id']).agg(agg).reset_index()\ntest_X.columns = ['Id'] + [f'{col}_{stat}' for col in train.columns[2:-1] for stat in agg]","642dba2d":"best_lgbm = {'n_estimators': 991,\n 'max_depth': 3,\n 'learning_rate': 0.036386125120857434,\n 'subsample': 0.014257208320520425,\n 'colsample_bytree': 0.4113657182157471,\n 'min_child_weight': 9,\n 'reg_alpha': 0.4433813076766884,\n 'reg_lambda': 2.871918517181243,\n             'random_state': SEED}","63b822dd":"cb = CatBoostClassifier(iterations=250, random_state=SEED, verbose=0)\nlgbm = LGBMClassifier(**best_lgbm)\nkfold = KFold(n_splits=5, random_state=SEED, shuffle=True)","58792c23":"score, predict_lgbm = calc(X, y, test_X.drop(columns=['Id']), lgbm, kfold, True)","db31f97d":"score, predict_cb = calc(X, y, test_X.drop(columns=['Id']), cb, kfold, True)","c92a405d":"# import h2o\n# from h2o.automl import H2OAutoML\n\n# h2o.init()","d5291042":"# y = \"target\"\n# x = list(train_X.columns) \n# x.remove(y)","da219233":"# train_X = h2o.H2OFrame(train_X)\n# train_X[y] = train_X[y].asfactor()","b5c8325d":"# # Increase max_runtime_secs the value for a better prediction.\n\n# aml = H2OAutoML(max_runtime_secs = 60*2, sort_metric='AUC', stopping_metric='AUC' , stopping_rounds=100)\n# aml.train(x = x, y = y, training_frame = train_X)","dd87ea0b":"# aml.leaderboard","72cdd314":"# test_X = test.drop(columns=['Week']).groupby(['Id']).agg(agg).reset_index()\n# test_X.columns = ['Id'] + [f'{col}_{stat}' for col in train.columns[2:-1] for stat in agg]","3c81eab3":"# predict_automl = aml.leader.predict(h2o.H2OFrame(test_X.drop(columns=['Id']))).as_data_frame()['p1']","cd736706":"test_X['Predicted'] = 0.6 * predict_lgbm + 0.4 * predict_cb # + 0.3 * predict_automl\ntest_X[['Id', 'Predicted']].to_csv('submit.csv', index=0)","2928e77d":"**Automated machine learning (AutoML)** is the process of automating the end-to-end process of applying machine learning to real-world problems. In a typical machine learning application, the typical stages (and sub-stages) of work are the following:\n\n* Data preparation\n    * data pre-processing\n    * feature engineering\n    * feature extraction\n    * feature selection\n* Model selection\n* Hyperparameter optimization (to maximize the performance of the final model)\n\nAutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning.","f8563949":"# Preprocessing\n\nSince we need to predict churn without depending on the period, we decided to try 2 preprocessing options:\n* Use all training records and predict for each Id and Week in the test. After that, for the final forecast, we aggregate the weekly forecasts for each Id.\n* (It seemed better to us) First, aggregate the features to the Id level (without dependence on the period) and get a set of statistics for each feature, for example, the mean and variance. Using the resulting set of features, we solve the usual regression problem.","cd47fe0d":"# Create a submission","958df8c1":"# Modeling\n\nFor training, checking and forecasting, we use the following function. It is good because it generates oof-predicts.\n\n**Out-of-Fold Predictions** was used as a prediction method. An out-of-fold prediction is a prediction by the model during the k-fold cross-validation procedure. That is, out-of-fold predictions are those predictions made on the holdout datasets during the resampling procedure. There will be one prediction for each example in the training dataset.","aa2f0ed0":"# (Bonus) Magic","a2a29b9a":"The set of hyperparameters for LGBM was selected using **Optuna**. You can see how to use it in our other [notebook](https:\/\/www.kaggle.com\/imgremlin\/4th-place-in-fraud-detection-from-zindi)."}}