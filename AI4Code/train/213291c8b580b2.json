{"cell_type":{"42cc0038":"code","e251eb99":"code","d25d0e8c":"code","7ba31092":"code","48e73705":"code","574fa997":"code","32142ca2":"code","8b094c93":"code","36020d81":"code","c541d246":"code","536c0455":"code","98afc9e2":"code","df0bfa90":"code","989a4d1f":"code","b6c57a65":"code","97f48027":"code","acd3be9c":"code","f763a164":"code","c71cda24":"code","1efdcf1d":"code","9fe2b563":"code","052eca9f":"code","62908370":"code","6b2ff064":"code","9e385264":"code","c4fb55d8":"code","2f4186d3":"code","b1610884":"code","227b7ab4":"code","1c69c068":"code","0faf06cb":"code","ba6a28bb":"code","18304e97":"code","1b8a3e27":"code","9f3ba933":"code","07d7996e":"code","f1ed9528":"code","50cfce25":"code","e571105f":"code","f709be8f":"code","29628c4b":"code","efdbfb82":"code","cbc93e94":"code","410c90e1":"code","32439f3b":"code","cbf0796a":"markdown","35089aa6":"markdown","cf6f0e9e":"markdown","40ed07b1":"markdown","956193ca":"markdown","248b46f4":"markdown","474888dc":"markdown"},"source":{"42cc0038":"from __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport matplotlib.pyplot as plt # plotting\nimport matplotlib.image as mpimg # images\nimport numpy as np #numpy\nimport seaborn as sns\nimport tensorflow.compat.v2 as tf #use tensorflow v2 as a main \nimport tensorflow.keras as keras # required for high level applications\nfrom sklearn.model_selection import train_test_split # split for validation sets\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import normalize # normalization of the matrix\nimport scipy\nimport pandas as pd\nimport unicodedata, re, string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom textblob import TextBlob\nimport itertools","e251eb99":"train_df = pd.read_csv(r'\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='latin_1')\ntest_df = pd.read_csv(r'\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding='latin_1')","d25d0e8c":"def show_history(history):\n    plt.figure()\n    for key in history.history.keys():\n        plt.plot(history.epoch, history.history[key], label=key)\n    plt.legend()\n    plt.tight_layout()","7ba31092":"class Mish(keras.layers.Activation):\n    def __init__(self, activation, **kwargs):\n        super(Mish, self).__init__(activation, **kwargs)\n        self.__name__ = 'Mish'\ndef mish(inputs):\n    return inputs * tf.math.tanh(tf.math.softplus(inputs))\nkeras.utils.get_custom_objects().update({'mish': Mish(mish)})","48e73705":"train_df.head()","574fa997":"test_df.head()","32142ca2":"train_df = train_df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\ntest_df = test_df.drop(['UserName', 'ScreenName', 'Location', 'TweetAt'], axis=1)\ntrain_df[\"Sentiment\"].replace({\"Neutral\": 1, \"Positive\": 1, \"Extremely Negative\": 0, \"Negative\": 0, \"Extremely Positive\": 1}, inplace=True)\ntest_df[\"Sentiment\"].replace({\"Neutral\": 1, \"Positive\": 1, \"Extremely Negative\": 0, \"Negative\": 0, \"Extremely Positive\": 1}, inplace=True)","8b094c93":"train_df.shape","36020d81":"sns.countplot(x='Sentiment', data=train_df)","c541d246":"train_df.Sentiment.value_counts()","536c0455":"train_df['length'] = train_df.OriginalTweet.apply(len)","98afc9e2":"sns.barplot(x='Sentiment', y='length', data = train_df)","df0bfa90":"for x in train_df.loc[:10, 'OriginalTweet']:\n    print(x)\n    print('---------')","989a4d1f":"def remove_users_http(word):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    word = re.sub(r'http\\S+', '', word)\n    word = re.sub(r'@\\w+', '', word)\n    word = re.sub(r'#\\w+', '', word)\n    return word\n\ndef remove_non_ascii(words):\n    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n    return new_words\n\ndef to_lowercase(words):\n    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = word.lower()\n        new_words.append(new_word)\n    return new_words\n\ndef remove_punctuation(words):\n    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(r'[^\\w\\s]', '', word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_numbers(words):\n    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n    new_words = []\n    for word in words:\n        new_word = re.sub(\"\\d+\", \"\", word)\n        if new_word != '':\n            new_words.append(new_word)\n    return new_words\n\ndef remove_stopwords(words):\n    \"\"\"Remove stop words from list of tokenized words\"\"\"\n    new_words = []\n    for word in words:\n        if word not in stopwords.words('english'):\n            new_words.append(word)\n    return new_words\n\ndef stem_words(words):\n    \"\"\"Stem words in list of tokenized words\"\"\"\n    stemmer = LancasterStemmer()\n    stems = []\n    for word in words:\n        stem = stemmer.stem(word)\n        stems.append(stem)\n    return stems\n\ndef lemmatize_verbs(words):\n    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n    lemmatizer = WordNetLemmatizer()\n    lemmas = []\n    for word in words:\n        lemma = lemmatizer.lemmatize(word, pos='v')\n        lemmas.append(lemma)\n    return lemmas\n\ndef normalize(words):\n    words = remove_non_ascii(words)\n    words = to_lowercase(words)\n    words = remove_punctuation(words)\n    words = remove_numbers(words)\n    words = remove_stopwords(words)\n    return words\n\ndef form_sentence(tweet):\n    tweet = remove_users_http(tweet)\n    tweet_blob = TextBlob(tweet)\n    return tweet_blob.words","b6c57a65":"train_df['Clean_text'] = train_df['OriginalTweet'].apply(form_sentence)\ntest_df['Clean_text'] = test_df['OriginalTweet'].apply(form_sentence)","97f48027":"train_df.head()","acd3be9c":"test_df.head()","f763a164":"train_df['Clean_text'] = train_df['Clean_text'].apply(normalize)\ntest_df['Clean_text'] = test_df['Clean_text'].apply(normalize)","c71cda24":"train_df.head()","1efdcf1d":"test_df.head()","9fe2b563":"def fix_nt(words):\n    st_res = []\n    for i in range(0, len(words) - 1):\n        if words[i+1] == \"n't\" or words[i+1] == \"nt\":\n            st_res.append(words[i]+(\"n't\"))\n        else:\n            if words[i] != \"n't\" and words[i] != \"nt\":\n                st_res.append(words[i])\n    return st_res","052eca9f":"train_df['Clean_text'] = train_df['Clean_text'].apply(fix_nt)\ntest_df['Clean_text'] = test_df['Clean_text'].apply(fix_nt)","62908370":"train_df['Clean_text'] = train_df['Clean_text'].apply(lambda x: \" \".join(x))\ntest_df['Clean_text'] = test_df['Clean_text'].apply(lambda x: \" \".join(x))","6b2ff064":"train_df.head()","9e385264":"test_df.head()","c4fb55d8":"all_words = list(itertools.chain(*train_df.Clean_text))","2f4186d3":"dist = nltk.FreqDist(all_words)\ndist","b1610884":"len(dist)","227b7ab4":"max(train_df.Clean_text.apply(len))","1c69c068":"from tensorflow import string as tf_string\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization","0faf06cb":"embedding_dim = 128 # Dimension of embedded representation - this is already part of latent space, there is captured some dependecy among words, we are learning this vectors in ANN\nvocab_size = 10000 # Number of unique tokens in vocabulary\nsequence_length = 30 # Output dimension after vectorizing - words in vectorited representation are independent\n\nvect_layer = TextVectorization(max_tokens=vocab_size, output_mode='int', output_sequence_length=sequence_length)\nvect_layer.adapt(train_df.Clean_text.values)","ba6a28bb":"X_train, X_valid, y_train, y_valid = train_test_split(train_df.Clean_text, train_df.Sentiment, test_size=0.1, random_state=13)","18304e97":"X_test = test_df.Clean_text\ny_test = test_df.Sentiment","1b8a3e27":"print('Vocabulary example: ', vect_layer.get_vocabulary()[:10])\nprint('Vocabulary shape: ', len(vect_layer.get_vocabulary()))","9f3ba933":"from tensorflow.compat.v1.keras.layers import CuDNNGRU, CuDNNLSTM\nfrom tensorflow.keras.layers import LSTM, GRU, Bidirectional","07d7996e":"input_layer = keras.layers.Input(shape=(1,), dtype=tf_string)\nx_v = vect_layer(input_layer)\nemb = keras.layers.Embedding(vocab_size, embedding_dim)(x_v)\nx = LSTM(64, activation='mish', return_sequences=True)(emb)\nx = GRU(64, activation='mish', return_sequences=True)(x)\nx = keras.layers.Flatten()(x)\nx = keras.layers.Dense(128, 'mish')(x)\nx = keras.layers.Dense(64, 'mish')(x)\nx = keras.layers.Dense(32, 'mish')(x)\nx = keras.layers.Dropout(0.2)(x)\noutput_layer = keras.layers.Dense(1, 'sigmoid')(x)\n\nmodel = keras.Model(input_layer, output_layer)\nmodel.summary()\n\nmodel.compile(optimizer='rmsprop', loss=keras.losses.BinaryCrossentropy(), metrics=['accuracy'])\n","f1ed9528":"es = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=70, restore_best_weights=True)\n\nbatch_size = 128\nepochs = 7\nhistory = model.fit(X_train.values, y_train.values, validation_data=(X_valid.values, y_valid.values), callbacks=[es], epochs=epochs, batch_size=batch_size)","50cfce25":"show_history(history)","e571105f":"y_test_loss, accuracy = model.evaluate(X_test, y_test)","f709be8f":"y_pred = model.predict(X_test).ravel()","29628c4b":"y_pred = [1 if x >= 0.5 else 0 for x in y_pred]","efdbfb82":"accuracy_score(y_true=y_test, y_pred=y_pred)","cbc93e94":"f1_score(y_true=y_test, y_pred=y_pred)","410c90e1":"print(classification_report(y_true=y_test, y_pred=y_pred))","32439f3b":"print(confusion_matrix(y_true=y_test, y_pred=y_pred))","cbf0796a":"Po\u010det pozitivn\u00edch a po\u010det a negativn\u00edch zpr\u00e1v","35089aa6":"Ukazka origin\u00e1ln\u00edch zpr\u00e1v","cf6f0e9e":"\nMetody a funkce pro upraven\u00ed textu v tweetu","40ed07b1":"Rozd\u011blen\u00ed trenovac\u00ed sady sady na trenovaci a valida\u010dn\u00ed","956193ca":"Pro \u0159e\u0161en\u00ed problemu jsem se rozhodl vyu\u017e\u00edt pouze text a proto D\u00e1le ji\u017e nepracuji s datem lokac\u00ed nebo se jm\u00e9ny.\nD\u00e1le jsem si upravil t\u0159idy kde jsem p\u016fvodn\u00edch 5 t\u0159id rozd\u011blil na 2 pozitivn\u00ed a negativn\u00ed s t\u00edm \u017ee neutr\u00e1lni jsem za\u0159adil do skupiny s pozitivn\u00edmi","248b46f4":"p\u0159ipraven\u00ed testovac\u00ed sady","474888dc":"\nhttps:\/\/www.kaggle.com\/datatattle\/covid-19-nlp-text-classification\n\nColumns:\n\n1) Location\n\n2) Tweet At\n\n3) Original Tweet\n\n4) Label"}}