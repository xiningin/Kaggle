{"cell_type":{"d6463a59":"code","61db1efd":"code","34d42d5d":"code","2db777fe":"code","8b3ee3b2":"code","72084aa5":"code","cb86e497":"code","334cc6f6":"code","bbfc3e9a":"code","66723513":"code","1212be97":"code","a85eeaf1":"code","bb89006f":"code","a8f03ff7":"code","1846a256":"code","4ce19228":"code","72b8c12e":"code","4eff4976":"code","395bd188":"code","d39820c9":"code","beed1d23":"code","18b56af5":"code","d5e8709a":"code","09f08f35":"code","4590b850":"code","a2ddf8e0":"code","7baef933":"code","ec4e00f8":"code","9281a31b":"code","f039f990":"code","999f7db2":"code","bf3c6e29":"code","3836db8c":"code","4cebdb79":"code","0238a2d6":"code","65e12149":"code","1694e8f2":"code","12420a72":"code","58b7f519":"code","1e42a6d0":"code","a851d851":"code","a1cfc4bf":"code","39f4049a":"code","7956ffb2":"code","4dd5d212":"code","e7d83b00":"code","b86889cc":"code","b5a9f35b":"code","885f15bf":"code","76492d03":"code","97614b2d":"code","cc690811":"code","1126af65":"code","cdc2c267":"code","d4fcc378":"code","35bd1400":"code","5a5c9291":"code","9fcc06bf":"code","b0662779":"code","ed1a3526":"code","7efd77dc":"code","fbd2aa8f":"code","2baa5f56":"code","9b57a959":"code","6030e445":"code","6b76dfef":"code","d02b1605":"code","00ce1311":"code","e2c9934b":"code","87337538":"markdown","373c360e":"markdown","dd3a6cb0":"markdown","19d75841":"markdown","992aa8ee":"markdown","23e4d9e8":"markdown","033eab10":"markdown","eccece2c":"markdown","8343103c":"markdown","fb2c2e2e":"markdown","b8a1523a":"markdown","c2fd0725":"markdown","007a3136":"markdown","4dfc8f94":"markdown","9e4f58ed":"markdown","fa07b3a6":"markdown","2ae5f755":"markdown","65befde6":"markdown","80a7b545":"markdown","45ffd117":"markdown","32f0e75d":"markdown","93040786":"markdown","91e3b599":"markdown","2cbea54d":"markdown","4cc3be55":"markdown","6dd3c0fb":"markdown","f7b6a023":"markdown","5b021429":"markdown","fb49adc7":"markdown","369bc5fb":"markdown","7d3aa872":"markdown","402d9c89":"markdown","1aa4a639":"markdown","7f1839b8":"markdown","8b95cfd5":"markdown","6d45f3d0":"markdown","d1bdae09":"markdown","655a554f":"markdown","b5f8eced":"markdown","236c8520":"markdown","492ad8f0":"markdown","b9778656":"markdown","52c8dd03":"markdown","6fdee4c2":"markdown","d9e1f17f":"markdown","75e9f969":"markdown"},"source":{"d6463a59":"#The installation of seaborn package will take few time\nfrom  datetime import datetime, timedelta\nimport pip\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n# Pretty display for notebooks\n%matplotlib inline","61db1efd":"#Load the data\ndata = pd.read_csv('..\/input\/pricesofhousesinthecityofwindsorcanada\/Housing.csv',sep=\";\")","34d42d5d":"data.info()","2db777fe":"data.head()","8b3ee3b2":"\"\"\"categorical_data = ['bedrooms', 'bathrms', 'stories', 'driveway', 'recroom', 'fullbase', 'gashw', 'airco', 'garagepl',\n                   'prefarea']\ndataset = pd.get_dummies(data, columns = categorical_data)\"\"\"","72084aa5":"prices = data['price']","cb86e497":"prices.head(5).append(prices.tail(5))","334cc6f6":"# Success\nprint('Housing dataset has {0} data points with {1} variables each.'.format(*data.shape))","bbfc3e9a":"data.head(4)","66723513":"features = data.drop('price', axis = 1)","1212be97":"features.head()","a85eeaf1":"# TODO: Minimum price of the data\nminimum_price = np.min(prices)\n# Alternative using pandas\n# minimum_price = prices.min()\n\n# TODO: Maximum price of the data\nmaximum_price = np.max(prices)\n# Alternative using pandas\n# maximum_price = prices.max()\n\n# TODO: Mean price of the data\nmean_price = np.mean(prices)\n# Alternative using pandas\n# mean_price = prices.mean()\n\n# TODO: Median price of the data\nmedian_price = np.median(prices)\n# Alternative using pandas\n# median_price = prices.median()\n\n# TODO: Standard deviation of prices of the data\nstd_price = np.std(prices)\n# Alternative using pandas \n# std_price = prices.std(ddof=0)\n\n# There are other statistics you can calculate too like quartiles\nfirst_quartile = np.percentile(prices, 25)\nthird_quartile = np.percentile(prices, 75)\ninter_quartile = third_quartile - first_quartile\n\n# Show the calculated statistics\nprint(\"Statistics for housing dataset:\\n\")\nprint(\"Minimum price: ${:,.2f}\".format(minimum_price))\nprint(\"Maximum price: ${:,.2f}\".format(maximum_price))\nprint(\"Mean price: ${:,.2f}\".format(mean_price))\nprint(\"Median price ${:,.2f}\".format(median_price))\nprint(\"Standard deviation of prices: ${:,.2f}\".format(std_price))\nprint(\"First quartile of prices: ${:,.2f}\".format(first_quartile))\nprint(\"Second quartile of prices: ${:,.2f}\".format(third_quartile))\nprint(\"Interquartile (IQR) of prices: ${:,.2f}\".format(inter_quartile))","bb89006f":"# here we set the figure size to 15x8\nplt.figure(figsize=(15, 8))\n# plot two values price per lot size\nplt.scatter(data.price, data.lotsize)\nplt.xlabel(\"price \", fontsize=14)\nplt.ylabel(\"lot size\", fontsize=14)\nplt.title(\"Scatter plot of price and lot size\",fontsize=18)\nplt.show()","a8f03ff7":"# here we set the figure size to 15x8\nplt.figure(figsize=(15, 8))\n# plot two values price per lot size\nplt.scatter(data.price, data.bedrooms)\nplt.xlabel(\"price \", fontsize=14)\nplt.ylabel(\"bedrooms\", fontsize=14)\nplt.title(\"Scatter plot of price and bedrooms\",fontsize=18)\nplt.show()","1846a256":"# here we set the figure size to 15x8\nplt.figure(figsize=(15, 8))\n# plot two values price per lot size\nplt.scatter(data.price, data.bathrms)\nplt.xlabel(\"price \", fontsize=14)\nplt.ylabel(\"bathrooms \", fontsize=14)\nplt.title(\"Scatter plot of price and bathrooms\",fontsize=18)\nplt.show()","4ce19228":"# here we set the figure size to 15x8\nplt.figure(figsize=(15, 8))\n# plot two values price per lot size\nplt.scatter(data.price, data.stories)\nplt.xlabel(\"price \", fontsize=14)\nplt.ylabel(\"stories\", fontsize=14)\nplt.title(\"Scatter plot of price and stories\",fontsize=18)\nplt.show()","72b8c12e":"# here we set the figure size to 15x8\nplt.figure(figsize=(15, 8))\n# plot two values price per lot size\nplt.scatter(data.price, data.driveway)\nplt.xlabel(\"price \", fontsize=14)\nplt.ylabel(\"driveway\", fontsize=14)\nplt.title(\"Scatter plot of price and driveway\",fontsize=18)\nplt.show()","4eff4976":"import matplotlib.pyplot as plt\nimport seaborn\nfrom seaborn import *\nseaborn.set_style(\"ticks\")\nf, ax = plt.subplots(figsize=(15, 12))\nseaborn.stripplot(data = data, x='price', y='garagepl', jitter=.5)\nplt.show()","395bd188":"import matplotlib.pyplot as plt\nimport seaborn\nfrom seaborn import *\nseaborn.set_style(\"ticks\")\nf, ax = plt.subplots(figsize=(15, 12))\nseaborn.stripplot(data = data, x='price', y='prefarea', jitter=.5)\nplt.show()","d39820c9":"import matplotlib.pyplot as plt\nimport seaborn\nfrom seaborn import *\nseaborn.set_style(\"white\")\nf, ax = plt.subplots(figsize=(15, 12))\nseaborn.stripplot(data = data, x='price', y='airco', jitter=.5)\nplt.show()","beed1d23":"import matplotlib.pyplot as plt\nimport seaborn\nfrom seaborn import *\nseaborn.set_style(\"white\")\nf, ax = plt.subplots(figsize=(15, 12))\nseaborn.stripplot(data = data, x='price', y='gashw', jitter=.5)\nplt.show()","18b56af5":"f, ax = plt.subplots(figsize=(15, 10))\nseaborn.stripplot(data = data, x='stories', y='price', jitter=.1)\nplt.show()","d5e8709a":"f, ax = plt.subplots(figsize=(15, 10))\nseaborn.stripplot(data = data, x='airco', y='price', jitter=.1)\nplt.show()","09f08f35":"f, ax = plt.subplots(figsize=(15, 10))\nseaborn.stripplot(data = data, x='garagepl', y='price', jitter=.1)\nplt.show()","4590b850":"f, ax = plt.subplots(figsize=(15, 10))\nseaborn.stripplot(data = data, x='fullbase', y='price', jitter=.1)\nplt.show()","a2ddf8e0":"f, ax = plt.subplots(figsize=(25, 25))\nseaborn.violinplot(data = data, x='bedrooms', y='price')\nplt.show()","7baef933":"f, ax = plt.subplots(figsize=(25, 25))\nseaborn.violinplot(data = data, x='garagepl', y='price')\nplt.show()","ec4e00f8":"f, ax = plt.subplots(figsize=(25, 25))\nseaborn.violinplot(data = data, x='fullbase', y='price')\nplt.show()","9281a31b":"# Generate a custom diverging colormap\ncmap = seaborn.diverging_palette(220, 10, as_cmap=True)\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 10))\n# Compute the correlation matrix\ncorr = features.corr()\n#print(corr)\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n# Draw the heatmap with the mask and correct aspect ratio\nseaborn.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.title('Correlation matrix', fontsize = 20)\nplt.show()","f039f990":"plt.figure(figsize=(17,8))\ndata.bedrooms.value_counts().nlargest(10).plot(kind='bar')\nplt.xlabel('Bedrooms Frequency')\nplt.title(\"Frequency of TOP 10 bedrooms distribution\",fontsize=18)\nplt.show()","999f7db2":"plt.figure(figsize=(17,8))\ndata.lotsize.value_counts().nlargest(10).plot(kind='bar')\nplt.xlabel('Lot size Frequency')\nplt.title(\"Frequency of TOP 20 lot size distribution\",fontsize=18)\nplt.show()","bf3c6e29":"# now we use the train_test_split function already available in sklearn library to split our data set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(features, prices, test_size = .20, random_state = 42)","3836db8c":"import xgboost\nn_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster = ['gbtree', 'gblinear']\nbase_score = [0.25, 0.5, 0.75, 1]\nlearning_rate = [0.05, 0.1, 0.15, 0.20]\nmin_child_weight = [1, 2, 3, 4]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth': max_depth,\n    'learning_rate' : learning_rate,\n    'min_child_weight' : min_child_weight,\n    'booster' : booster,\n    'base_score' : base_score\n    }","4cebdb79":"regressor = xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0,\n             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1, verbosity=1)","0238a2d6":"regressor.fit(X_train, Y_train)","65e12149":"regressor = xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0,\n             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n             max_depth=2, min_child_weight=1, missing=None, n_estimators=900,\n             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1, verbosity=1)\nregressor.fit(X_train, Y_train)\n\npredicted = regressor.predict(X_test)\nresidual = Y_test - predicted\n\nfig = plt.figure(figsize=(30,30))\nax1 = plt.subplot(211)\nseaborn.distplot(residual, color ='teal')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.title('Residual counts',fontsize=35)\nplt.xlabel('Residual',fontsize=25)\nplt.ylabel('Count',fontsize=25)\n\nax2 = plt.subplot(212)\nplt.scatter(predicted, residual, color ='teal')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.xlabel('Predicted',fontsize=25)\nplt.ylabel('Residual',fontsize=25)\nplt.axhline(y=0)\nplt.title('Residual vs. Predicted',fontsize=35)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE of XGBoost:')\nprint(rmse)","1694e8f2":"from sklearn.metrics import r2_score\nprint('Variance score of XGBoost: %.2f' % r2_score(Y_test, predicted))","12420a72":"from catboost import CatBoostRegressor\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import cross_val_score\nmodelcatBoost = CatBoostRegressor(verbose=0, n_estimators=100)\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(modelcatBoost, X_train, Y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')\nprint('Mean Absolute Error: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n# fit the model on the whole dataset\nmodelCat = CatBoostRegressor(verbose=0, n_estimators=100)\nmodelCat.fit(X_train, Y_train)","58b7f519":"# predict the results\ny_pred=modelCat.predict(X_test)","1e42a6d0":"y_pred","a851d851":"predicted = modelCat.predict(X_test)\nresidual = Y_test - predicted\n\nfig = plt.figure(figsize=(30,30))\nax1 = plt.subplot(211)\nseaborn.distplot(residual, color ='teal')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.title('Residual counts',fontsize=35)\nplt.xlabel('Residual',fontsize=25)\nplt.ylabel('Count',fontsize=25)\n\nax2 = plt.subplot(212)\nplt.scatter(predicted, residual, color ='teal')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.xlabel('Predicted',fontsize=25)\nplt.ylabel('Residual',fontsize=25)\nplt.axhline(y=0)\nplt.title('Residual vs. Predicted',fontsize=35)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE of CatBoost:')\nprint(rmse)","a1cfc4bf":"from sklearn.metrics import r2_score\nprint('Variance score of CatBoost: %.2f' % r2_score(Y_test, predicted))","39f4049a":"# evaluate the model\nfrom lightgbm import LGBMRegressor as lgb\nfrom sklearn.model_selection import RepeatedKFold\n\nmodelLightGBM = lgb()\ncv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=1)\nn_scores = cross_val_score(model, X_train, Y_train, \n                           scoring='neg_mean_absolute_error', \n                           cv=cv, n_jobs=-1, \n                           error_score='raise')\n\nprint('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))\n# fit the model on the whole dataset\nmodelLightGBM = lgb(objective='regression', num_leaves=100, learning_rate=0.085, n_estimators=60)\nmodelLightGBM.fit(X_train, Y_train)","7956ffb2":"modelLightGBM.fit(X_train, Y_train, eval_set=[(X_test, Y_test)], eval_metric='rmse', early_stopping_rounds=100)","4dd5d212":"predicted = modelLightGBM.predict(X_test)\nresidual = Y_test - predicted\n\nfig = plt.figure(figsize=(30,30))\nax1 = plt.subplot(211)\nseaborn.distplot(residual, color ='teal')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.title('Residual counts',fontsize=35)\nplt.xlabel('Residual',fontsize=25)\nplt.ylabel('Count',fontsize=25)\n\nax2 = plt.subplot(212)\nplt.scatter(predicted, residual, color ='teal')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.xlabel('Predicted',fontsize=25)\nplt.ylabel('Residual',fontsize=25)\nplt.axhline(y=0)\nplt.title('Residual vs. Predicted',fontsize=35)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE of LightGBM:')\nprint(rmse)","e7d83b00":"from sklearn.metrics import r2_score\nprint('Variance score of LightGBM: %.2f' % r2_score(Y_test, predicted))","b86889cc":"\"\"\"from lightgbm import plot_importance\nfig, ax = plt.subplots(figsize=(12,10))\nLGBMRegressor.plot_importance(modelLightGBM, max_num_features=50, height=0.8, ax=ax)\nax.grid(False)\nplt.title(\"LightGBM - Feature Importance\", fontsize=15)\nplt.show()\"\"\"","b5a9f35b":"from sklearn import neighbors\n# the value of n_neighbors will be changed when we plot the histogram showing the lowest RMSE value\nknn = neighbors.KNeighborsRegressor(n_neighbors=5)\nknn.fit(X_train, Y_train)\n\npredicted = knn.predict(X_test)\nresidual = Y_test - predicted\n\nfig = plt.figure(figsize=(30,30))\nax1 = plt.subplot(211)\nseaborn.distplot(residual, color ='teal')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.title('Residual counts',fontsize=35)\nplt.xlabel('Residual',fontsize=25)\nplt.ylabel('Count',fontsize=25)\n\nax2 = plt.subplot(212)\nplt.scatter(predicted, residual, color ='teal')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.xlabel('Predicted',fontsize=25)\nplt.ylabel('Residual',fontsize=25)\nplt.axhline(y=0)\nplt.title('Residual vs. Predicted',fontsize=35)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)","885f15bf":"from sklearn.metrics import r2_score\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))","76492d03":"rmse_l = []\nnum = []\nfor n in range(2, 16):\n    knn = neighbors.KNeighborsRegressor(n_neighbors=n)\n    knn.fit(X_train, Y_train)\n    predicted = knn.predict(X_test)\n    rmse_l.append(np.sqrt(mean_squared_error(Y_test, predicted)))\n    num.append(n)","97614b2d":"df_plt = pd.DataFrame()\ndf_plt['rmse'] = rmse_l\ndf_plt['n_neighbors'] = num\nax = plt.figure(figsize=(15,7))\nseaborn.barplot(data = df_plt, x = 'n_neighbors', y = 'rmse')\nplt.show()","cc690811":"from sklearn.tree import DecisionTreeRegressor\n\ndtr = DecisionTreeRegressor(max_features='auto')\ndtr.fit(X_train, Y_train)\npredicted = dtr.predict(X_test)\nresidual = Y_test - predicted\n\nfig = plt.figure(figsize=(30,30))\nax1 = plt.subplot(211)\nseaborn.distplot(residual, color ='orange')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.title('Residual counts',fontsize=35)\nplt.xlabel('Residual',fontsize=25)\nplt.ylabel('Count',fontsize=25)\n\nax2 = plt.subplot(212)\nplt.scatter(predicted, residual, color ='orange')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.xlabel('Predicted',fontsize=25)\nplt.ylabel('Residual',fontsize=25)\nplt.axhline(y=0)\nplt.title('Residual vs. Predicted',fontsize=35)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)","1126af65":"print('Variance score: %.2f' % r2_score(Y_test, predicted))","cdc2c267":"from sklearn import linear_model\n\nregr = linear_model.LinearRegression()\nregr.fit(X_train, Y_train)\n\npredicted = regr.predict(X_test)\nresidual = Y_test - predicted\n\nfig = plt.figure(figsize=(30,30))\nax1 = plt.subplot(211)\nseaborn.distplot(residual, color ='teal')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.title('Residual counts',fontsize=35)\nplt.xlabel('Residual',fontsize=25)\nplt.ylabel('Count',fontsize=25)\n\nax2 = plt.subplot(212)\nplt.scatter(predicted, residual, color ='teal')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.xlabel('Predicted',fontsize=25)\nplt.ylabel('Residual',fontsize=25)\nplt.axhline(y=0)\nplt.title('Residual vs. Predicted',fontsize=35)\n\nplt.show()\n\nfrom sklearn.metrics import mean_squared_error\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nprint('RMSE:')\nprint(rmse)","d4fcc378":"print('Variance score: %.2f' % r2_score(Y_test, predicted))","35bd1400":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\n\nr_sq = []\ndeep = []\nmean_scores = []\n\n#loss : {\u2018ls\u2019, \u2018lad\u2019, \u2018huber\u2019, \u2018quantile\u2019}\nfor n in range(3, 11):\n    gbr = GradientBoostingRegressor(loss ='ls', max_depth=n)\n    gbr.fit (features, prices)\n    deep.append(n)\n    r_sq.append(gbr.score(features, prices))\n    mean_scores.append(cross_val_score(gbr, features, prices, cv=12).mean())","5a5c9291":"plt_gbr = pd.DataFrame()\n\nplt_gbr['mean_scores'] = mean_scores\nplt_gbr['depth'] = deep\nplt_gbr['R square'] = r_sq\n\nf, ax = plt.subplots(figsize=(15, 5))\nseaborn.barplot(data = plt_gbr, x='depth', y='R square')\nplt.show()\n\nf, ax = plt.subplots(figsize=(15, 5))\nseaborn.barplot(data = plt_gbr, x='depth', y='mean_scores')\nplt.show()","9fcc06bf":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import cross_val_score\n\ngbr = GradientBoostingRegressor(loss ='ls', max_depth=6)\ngbr.fit (X_train, Y_train)\npredicted = gbr.predict(X_test)\nresidual = Y_test - predicted\n\nfig = plt.figure(figsize=(30,30))\nax1 = plt.subplot(211)\nseaborn.distplot(residual, color ='teal')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.title('Residual counts',fontsize=35)\nplt.xlabel('Residual',fontsize=25)\nplt.ylabel('Count',fontsize=25)\n\nax2 = plt.subplot(212)\nplt.scatter(predicted, residual, color ='teal')\nplt.tick_params(axis='both', which='major', labelsize=20)\nplt.xlabel('Predicted',fontsize=25)\nplt.ylabel('Residual',fontsize=25)\nplt.axhline(y=0)\nplt.title('Residual vs. Predicted',fontsize=35)\n\nplt.show()\n\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nscores = cross_val_score(gbr, features, prices, cv=12)\n\nprint('\\nCross Validation Scores:')\nprint(scores)\nprint('\\nMean Score:')\nprint(scores.mean())\nprint('\\nRMSE:')\nprint(rmse)","b0662779":"print('Variance score: %.2f' % r2_score(Y_test, predicted))","ed1a3526":"model = linear_model.LinearRegression()\nscore = np.mean(np.sqrt(-cross_val_score(model, X_train, Y_train,scoring=\"neg_mean_squared_error\", cv = 5)))\nprint(\"linear regression score: \", score)","7efd77dc":"from pylab import *\nfrom sklearn.linear_model import Ridge\nrcParams['figure.figsize'] = 8,5\ncv = 5 #number of folds in cross-validation\n\nalphas = np.logspace(-5,2,20)\nscores = np.zeros((len(alphas),cv))\nscores_mu = np.zeros(len(alphas))\nscores_sigma = np.zeros(len(alphas))\n\nfor i in range(0,len(alphas)):\n    modelRidge = Ridge(alpha=alphas[i])\n    scores[i,:] = np.sqrt(-cross_val_score(model, X_train, Y_train,scoring=\"neg_mean_squared_error\", cv = cv))\n    scores_mu[i] = np.mean(scores[i,:])\n    scores_sigma[i] = np.std(scores[i,:])\n \n#for i in range(0,cv):\n#    plot(alphas,scores[:,i], 'b--', alpha=0.5)\nplot(alphas,scores_mu,'c-',lw=3, alpha=0.5, label = \"Ridge\")\nfill_between(alphas,np.array(scores_mu)-np.array(scores_sigma),\n             np.array(scores_mu)+np.array(scores_sigma),color=\"c\",alpha=0.5)\n\n#print(\"best score in Ridge: \",min(scores_mu))\n\nfor i in range(0,len(alphas)):\n    modelLasso = linear_model.Lasso(alpha=alphas[i])\n    scores[i,:] = np.sqrt(-cross_val_score(model, X_train, Y_train,scoring=\"neg_mean_squared_error\", cv = cv))\n    scores_mu[i] = np.mean(scores[i,:])\n    scores_sigma[i] = np.std(scores[i,:])\n\nplot(alphas,scores_mu,'g-',lw=3, alpha=0.5, label=\"Lasso\")\nfill_between(alphas,np.array(scores_mu)-np.array(scores_sigma),\n             np.array(scores_mu)+np.array(scores_sigma),color=\"g\",alpha=0.5)\n\nxscale(\"log\")\nplt.xlabel(\"alpha\", size=20)\nplt.ylabel(\"rmse\", size=20)\nlegend(loc=2)\nrmse = np.sqrt(mean_squared_error(Y_test, predicted))\nscores = cross_val_score(modelRidge, features, prices, cv=12)\nprint(\"---->Ridge Regression<----\")\nprint('\\nCross Validation Scores:')\nprint(scores)\nprint('\\nMean Score:')\nprint(scores.mean())\nprint('\\nRMSE:')\nprint(rmse)\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))\n\nrmse2 = np.sqrt(mean_squared_error(Y_test, predicted))\nscores2 = cross_val_score(modelLasso, features, prices, cv=12)\n\nprint(\"---->Lasso Regression<----\")\nprint('\\nCross Validation Scores:')\nprint(scores2)\nprint('\\nMean Score:')\nprint(scores2.mean())\nprint('\\nRMSE:')\nprint(rmse2)\nprint('Variance score: %.2f' % r2_score(Y_test, predicted))","fbd2aa8f":"# user_input = [5700, 2,2,1,0,1,0,1,0,1,1]\nuser_input = {'lotsize':5700, 'bedrooms':2, 'bathrms':2, 'stories':1, 'driveway':0,'recroom':1,'fullbase':0,\n              'gashw':1,'airco':0,'garagepl':1,'prefarea':1}\ndef input_to_one_hot(features):\n    # initialize the target vector with zero values\n    enc_input = np.zeros(11)\n    # set the numerical input as they are\n    enc_input[0] = features['lotsize']\n    enc_input[1] = features['bedrooms']\n    enc_input[2] = features['bathrms']\n    enc_input[3] = features['stories']\n    enc_input[4] = features['driveway']\n    enc_input[5] = features['recroom']\n    enc_input[6] = features['fullbase']\n    enc_input[7] = features['gashw']\n    enc_input[8] = features['airco']\n    enc_input[9] = features['garagepl']\n    enc_input[10] = features['prefarea']\n    return enc_input","2baa5f56":"print(input_to_one_hot(user_input))","9b57a959":"a = input_to_one_hot(user_input)","6030e445":"price_pred = gbr.predict([a])","6b76dfef":"print(\"The predicted price of your house's characteristics is :\", price_pred[0])","d02b1605":"from sklearn.externals import joblib\n\njoblib.dump(gbr, 'model.pkl')","00ce1311":"gbr = joblib.load('model.pkl')","e2c9934b":"print(\"the best price for this house is\",gbr.predict([a])[0])","87337538":"<FONT size=\"4p\">As we can see from the plot above, the house price increase respectivly by the lot size, and more generally we can say that the more the lot size augment, the price augment too, while in the some cases the bigger lot size still have a medium price, and this is totally logical since whenever a large lot size doesn't mean that it will have the highest price due to other features.<\/FONT>","373c360e":"<FONT size=\"4p\">The above plot indicate that there's a repartition of prices depends on bedroom's number: for 1 or 6 bedrooms house's price is between 25000 and 80000 but for 3 or 4 bedrooms the price is so various which indicates that houses with 2 or 3 or 4 have more percentage to be selled than those with 1 or 5 or 6 bedrooms.<\/FONT>","dd3a6cb0":"To do that we first build a fucntion that takes a simple user input and transform it to a one hot encoding.","19d75841":"# Interpretation","992aa8ee":"## KNN Regression","23e4d9e8":"<FONT size=\"5p\" color=\"blue\">Let's predict an observation never seen before<\/FONT>","033eab10":"# XGBoost Regressor","eccece2c":"# Model Evaluation","8343103c":"<FONT size=\"4p\">Some Insights with Violin plot. This chart is a combination of a Box Plot and a Density Plot that is rotated and placed on each side, to show the distribution shape of the data.<\/FONT>","fb2c2e2e":"# <FONT color=\"purple\">What about Simple Linear Regression<\/FONT>","b8a1523a":"<FONT size=\"4p\">For the moment we will use the K nearset neighbors regressor model with its numerical features, to get a basic view on our model how it behaves.<\/FONT>","c2fd0725":"<FONT size=\"4p\">By looking at the last RMSE score we've last improvements, as you can see from the \"Residual vs. Predicted\" that the predicted score is closer to zero and is tighter around the lines which means that we are guessing a lot closer to the price.<\/FONT>","007a3136":"<FONT size=\"4p\">From the plot above, we can clearly visualise a lot of information such as the minimum, maximum price for houses and also get perception on the Median values, but more particularly what we got in violin plot other than teh box plot, is the density plot width known as Kernel Density Plots.<\/FONT>","4dfc8f94":"<FONT size=\"4p\">As we can see from the plot above,there is a variety in house's price depends on the existence of a driveway or not :we can categorise them into two groups ; house's between 25000 and nearly 80000 and the other group is higher than 80000.<\/FONT>","9e4f58ed":"## Linear Regression","fa07b3a6":"<FONT size=\"4p\">In this exercice I will pursuit a Data Science process to build and deploy a Machine Learning Model that can predict a house price, by following the steps below:<\/FONT>","2ae5f755":"Let's visualize the distribution of houses price by their number of stories, and look how it behaves.","65befde6":"Let's visualize the distribution of houses price by their number of bathrooms, and look how it behaves.","80a7b545":"<FONT size=\"4p\">The root-mean-square deviation (RMSD) or root-mean-square error (RMSE) (or sometimes root-mean-squared error) is a frequently used measure of the differences between values (sample and population values) predicted by a model or an estimator and the values actually observed. The RMSD represents the sample standard deviation of the differences between predicted values and observed values. These individual differences are called residuals when the calculations are performed over the data sample that was used for estimation, and are called prediction errors when computed out-of-sample. The RMSD serves to aggregate the magnitudes of the errors in predictions for various times into a single measure of predictive power. RMSD is a measure of accuracy, to compare forecasting errors of different models for a particular data and not between datasets, as it is scale-dependent. ~ WikiPedia\nBy comparing the Tree Regression with the KNN Regression we can see that the RMSE was augmented from 21493 to 25591 which let us say that this model is less accurate than the last one, we still have to test other regression algorithm to check if there is anyone better than those ones.<\/FONT>","45ffd117":"# CatBoost for Regression","32f0e75d":"## Lasso and Ridge Regression","93040786":"## Save the best model","91e3b599":"Hyperparameters are different from parameters, which are the internal coefficients or weights for a model found by the learning algorithm. Unlike parameters, hyperparameters are specified by the practitioner when configuring the model.\n\nTypically, it is challenging to know what values to use for the hyperparameters of a given algorithm on a given dataset, therefore it is common to use random or grid search strategies for different hyperparameter values.\n\nThe more hyperparameters of an algorithm that you need to tune, the slower the tuning process. Therefore, it is desirable to select a minimum subset of model hyperparameters to search or tune.\n\nNot all model hyperparameters are equally important. Some hyperparameters have an outsized effect on the behavior, and in turn, the performance of a machine learning algorithm.\n\nAs a machine learning practitioner, you must know which hyperparameters to focus on to get a good result quickly.","2cbea54d":"# Data Splitting","4cc3be55":"<FONT size=\"4p\">The above plot indicate that there's a repartition of prices depends on bathroom's number: for 1 or 2 bathrooms house's price is between 20000 and 190000 but for 3 or 4 bedrooms the price is so various which indicates that houses with 1 or 2 bathrooms have more percentage to be selled than those with 3 or 4 bathrooms.<\/FONT>","6dd3c0fb":"<FONT size=\"4p\">Our housing dataset in composed of 546 observations and 12 features of type integer:\n   lotsize,bedrooms,bathrms,stories,garagepl,driveway,recroom,fullbase,gashw,airco,prefarea,price.\n<\/FONT>","f7b6a023":"<FONT size=\"4p\">Usually we split our data into three parts : Training , validation and Testing set, but for simplicity we will use only train and test with 20% in test size.<\/FONT>","5b021429":"<FONT size=\"4p\">I remember it was my first machine learning model that I have created and I spent a lot of time in order to get a good result.This was two years ago : before I heard about the Kaggle community. So I uploaded it as it was. This precious kernel helped me to have an opportunity of Machine Learning internship ,it was an amazing opportunity.<br\/>\nPlease if you appreciate my work, and you find it helpful <FONT color=\"red\">Do an Upvote<\/FONT> <br\/>\nI am looking for your Feedbacks to enhance my kernel.<\/FONT>","fb49adc7":"Let's visualize the distribution of house price by their driveway, and look how it behaves.","369bc5fb":"Let's visualize the distribution of houses price by their number of bedrooms, and look how it behaves.","7d3aa872":"## Descision Tree Regression","402d9c89":"<ol>\n  <li>Loading Data<\/li>\n  <li>Data preprocessing and cleaning<\/li>\n  <li>Data exploration and visualisation<\/li>\n  <li>Data modeling<\/li>\n    <li>Evaluating the model<\/li>\n<\/ol>","1aa4a639":"<FONT size=\"4p\">It appears that 3 nearest neighbors is the optimal number of neighbors.<\/FONT>","7f1839b8":"# Exploratory Data Analysis","8b95cfd5":"# Conclusion","6d45f3d0":"## Boosting","d1bdae09":"<G><I><FONT size=\"20\">Data Modeling<\/FONT><\/I><\/G>","655a554f":"<table class='table table-striped'> <thead> <tr> <th>Model<\/th> <th>Variance Score<\/th> <th>RMSE<\/th><\/tr> <\/thead> \n<tbody> <tr> \n    <th scope='row'>XGBoost<\/th> <td>0.57<\/td> <td>17087<\/td><\/tr><tr> \n    <th scope='row'>CatBoost<\/th> <td>0.60<\/td> <td>16301<\/td><\/tr><tr> \n    <th scope='row'>LightGBM<\/th> <td>0.61<\/td> <td>16103<\/td><\/tr><tr> \n    <th scope='row'>LightGBM after Hyperparameter Tuning<\/th> <td>0.63<\/td> <td>15488<\/td><\/tr><tr> \n    <th scope='row'>KNN<\/th> <td>0.31<\/td> <td>21684<\/td><\/tr> <tr> \n    <th scope='row'>Multiple Linear Regression <\/th> <td>0.16<\/td> <td>24756<\/td><\/tr><tr> \n    <th scope='row'>Gradient Boosting\t<\/th> <td>0.62<\/td> <td>16016<\/td><\/tr> <tr> \n    <th scope='row'>Decision Tree<\/th> <td>0.50<\/td> <td>18260<\/td><\/tr><tr> \n    <th scope='row'>Lasso Regression<\/th> <td>0.51<\/td> <td>18217<\/td><\/tr><tr> \n    <th scope='row'>Ridge Regression<\/th> <td>0.51<\/td> <td>18217<\/td><\/tr><\/tbody> \n    <\/table>","b5f8eced":"Let's visualize the distribution of house price by their lot size, and look how it behaves","236c8520":"<FONT size=\"4p\">As we can see we got 30% in the r^2 score by using n_neighbors = 5, we still don't know if it's the optimal number of neighors or not, so for that we will plot a histogram of different Root Mean Squared Error by n_neighbors and see who's have the lowest RMSE value, and another thing is that the mean of cross validation values is very low which may indicate that our model had overfitted.<\/FONT>","492ad8f0":"<FONT size=\"4p\">Boosting is a machine learning ensemble meta-algorithm for primarily reducing bias, and also variance in supervised learning, and a family of machine learning algorithms which convert weak learners to strong ones. Boosting is based on the question posed by Kearns and Valiant (1988, 1989): Can a set of weak learners create a single strong learner? A weak learner is defined to be a classifier which is only slightly correlated with the true classification (it can label examples better than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification. ~ WikiPedia) \nLet's see if boosting can give us perfect scores.<\/FONT>","b9778656":"<FONT size=\"4p\">Finally, our model is ready to be deployed and used.However, as we see even the gradient boosting is the best model for our dataset but it still have a low accuracy 62% which indicates that we can enhance our model in order to get higher score.<br\/>\n    To do that, first of all, to train the machine perfectly and have a very close prediction values to real ones we need to collect more data and have a rich dataset.<br\/>\n    Moreover, we can involve other factors (features) that can affect strongly house's price such as economic indicators : the housing market and the economy often reflect each other, so if things are not good economically, the housing market can also be slow.<br\/>\n    Also, we can add the local market : for example if your home is in excellent condition, in the best location, with quality finishes, the number of other properties for sale in your area can affect the value of your home. If there are a lot of buyers competing for a small number of homes, it's a sellers market. Conversely, a market with few buyers but many homes in the market is considered a buyer's market.<\/FONT>","52c8dd03":"# LightGBM Regressor and Tuning Hyperparameter","6fdee4c2":"<FONT size=\"4p\">It appears that the Gradient Boosting model regressor win the battle with the lowest RMSE value and the highest R^2 score. In the following table we will do a benchmarking resuming all the models tested above.<\/FONT>","d9e1f17f":"# Introduction","75e9f969":"# <FONT color=\"fuchsia\">Correlation Matrix<\/FONT>"}}