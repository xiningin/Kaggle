{"cell_type":{"9c538bfe":"code","b2f9ebd4":"code","cea1cf4f":"code","57f4fe74":"code","ec5b1227":"code","df98d7a2":"code","3a8c4659":"code","daa40052":"code","85e1d077":"code","cd68380d":"code","47f42fe2":"code","94f1d953":"code","9a3118f6":"code","c3769455":"code","30cf4bb1":"code","4d433c70":"code","ba057d6c":"markdown","40cf393d":"markdown"},"source":{"9c538bfe":"from keras.models import Sequential\nfrom keras.layers import Convolution2D\nfrom keras.layers import MaxPooling2D\nfrom tensorflow.keras.applications.resnet50 import ResNet50 \n#,NASNetLarge\nfrom keras.layers import Flatten\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dense,Dropout\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.optimizers import Adam ,RMSprop\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.applications.vgg16 import VGG16","b2f9ebd4":"image_width, image_height = 224,224\n\ntrain_datagen = ImageDataGenerator(\n      rescale=1.\/255,\n      rotation_range=20,\n      shear_range=0.2,\n      zoom_range=0.2,\n      width_shift_range=0.3,\n      height_shift_range=0.3,\n      horizontal_flip=True,\n      fill_mode='nearest')\n\ntest_datagen = ImageDataGenerator(rescale = 1.\/255.)\n\ntrain_generator = train_datagen.flow_from_directory(\n                    \"..\/input\/datasetcan\/train\",\n                    batch_size=128,\n                    class_mode='categorical',\n                    shuffle=True,\n                    target_size=(image_width, image_height)\n)     \n\ntest_generator =  test_datagen.flow_from_directory(\n                    \"..\/input\/datasetcan\/test\",\n                    batch_size=128,\n                    class_mode='categorical',\n                    shuffle=True,\n                    target_size=(image_width, image_height)\n)","cea1cf4f":"train_steps = train_generator.n \/\/ train_generator.batch_size\n\nprint(train_steps)","57f4fe74":"test_steps = test_generator.n \/\/ test_generator.batch_size\n\nprint(test_steps)","ec5b1227":"vvg = VGG16(input_shape=(224,224, 3), include_top=False,weights = 'imagenet')","df98d7a2":"for layers in (vvg.layers):\n    layers.trainable = False","3a8c4659":"vvg.layers","daa40052":"model = Sequential()\n\n# Add the ResNet50 convolutional base model\nmodel.add(vvg)\n\n# Add new layers\nmodel.add(Flatten())\nmodel.add(Dense(4096 , activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(4096 , activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(4096, activation='relu'))\nmodel.add(Dropout(0.4))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.summary()","85e1d077":"from tensorflow.keras.utils import plot_model\nfrom IPython.display import Image\nplot_model(model, to_file='convnet.png', show_shapes=True,show_layer_names=True)\nImage(filename='convnet.png') ","cd68380d":"lr = 1e-6\nmodel.compile(loss='categorical_crossentropy', optimizer=Adam(lr), metrics=['accuracy'])","47f42fe2":"mcp = ModelCheckpoint('modelVVG.h5', verbose=1)","94f1d953":"es = EarlyStopping(patience=2,verbose=1)","9a3118f6":"history = model.fit(train_generator,steps_per_epoch=train_steps,epochs=30,validation_data=test_generator,validation_steps=test_steps,verbose=1,callbacks=[mcp,es])","c3769455":"model.evaluate(test_generator, verbose=1, steps=test_steps)","30cf4bb1":"import matplotlib.pyplot as plt\n\naccuracy      = history.history['accuracy']\nval_accuracy  = history.history['val_accuracy']\nloss     = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs   = range(len(accuracy))\n\nplt.plot(epochs, accuracy)\nplt.plot(epochs, val_accuracy)\nplt.title('Training and validation accuracy')\nplt.figure()\n\n# Plot training and validation loss per epoch\nplt.plot(epochs, loss)\nplt.plot(epochs, val_loss)\nplt.title('Training and validation loss')","4d433c70":"import numpy as np\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.models import load_model\n\nimage_width, image_height = 224,224\n\nimg = image.load_img('..\/input\/siim-isic-melanoma-classification\/jpeg\/test\/ISIC_0052060.jpg', target_size=(image_width, image_height))\nimg = image.img_to_array(img)\nimg = np.expand_dims(img, axis = 0)\nimg\/= 255.\nmodel = load_model('modelVVG.h5')\nresult = model.predict(img)\ntrain_generator.class_indices\nif result[0][0] == 0:\n    prediction = 'Malignant'\nelse:\n    prediction = 'Benign'\nprint(prediction)","ba057d6c":"## What is VGG16 model?\nVGG16 (also called OxfordNet) is a convolutional neural network architecture named after the Visual Geometry Group from Oxford, who developed it. ... By only keeping the convolutional modules, our model can be adapted to arbitrary input sizes. The model loads a set of weights pre-trained on ImageNet.\n\n![](https:\/\/neurohive.io\/wp-content\/uploads\/2018\/11\/vgg16.png)","40cf393d":"# Skin cancer\n\nSkin cancers are cancers that arise from the skin. They are due to the development of abnormal cells that have the ability to invade or spread to other parts of the body.\n\n\n\n![](https:\/\/media.springernature.com\/lw685\/springer-static\/image\/art%3A10.1007%2Fs11042-020-09388-2\/MediaObjects\/11042_2020_9388_Fig3_HTML.png)\n\n\n## Dataset Link \n[Here](https:\/\/www.kaggle.com\/c\/siim-isic-melanoma-classification\/data)"}}