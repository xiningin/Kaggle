{"cell_type":{"c7008ee3":"code","7fd14235":"code","4cf709cc":"code","1c07b325":"code","968016c2":"code","c67be1e6":"code","28fa07c9":"code","c0bd39dc":"code","d3c7b95c":"code","d1b5f155":"code","387d3554":"code","6a8a5ec7":"code","e127c080":"code","46dbc228":"code","e3201239":"code","cf71a32f":"code","1e90f75a":"code","d58a1ea6":"code","92f7d312":"code","8ab17a22":"code","ec967c8f":"code","681132cb":"code","84134c97":"code","d238ff6a":"code","4c04e8ea":"markdown","8084d1a9":"markdown","52ea3153":"markdown","027a1874":"markdown","c815eabd":"markdown","6f37ea8f":"markdown","33f63ea7":"markdown","cfa6ffa4":"markdown","b399bdaa":"markdown","281cf73d":"markdown","13cddaea":"markdown","2d87f3fb":"markdown","4f2616fe":"markdown","d4e907c4":"markdown"},"source":{"c7008ee3":"!pip install sweetviz","7fd14235":"# Standart Inputs\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# EDA tool\nimport sweetviz\n\n# Used to display Html file for a later step\nimport IPython\n\n# Model Selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\n\n# Models\nfrom sklearn.linear_model import LogisticRegression","4cf709cc":"data = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-benign-or-malignant\/tumor.csv\")","1c07b325":"# Check data size and view first 10 rows\nprint(f\"The data has {data.shape[1]} features and {data.shape[0]} rows\\n\")\ndata.head(10)","968016c2":"# Now we drop the id column since it's doesn't contain valuable information\ndata.drop([data.columns[0]], axis= 1, inplace= True)","c67be1e6":"# Check for null values\npd.isnull(data).sum()\n# The data has no null values","28fa07c9":"# Check types of columns\ndata.dtypes","c0bd39dc":"# Our classes are encoded as '2' and '4' for malignant and benign\n# these two lines instead codes them at 0 and 1 for same classes\ndata['Class'].loc[data['Class'] == 2] = 0\ndata['Class'].loc[data['Class'] == 4] = 1","d3c7b95c":"# View Distributions of each feature\ndata.hist( grid = False,yrot= 30, figsize=(16,12))\nplt.show()","d1b5f155":"# Let's analyze the full data set with the amazing SweetViz tool\nreport = sweetviz.analyze(data, target_feat='Class')","387d3554":"# Since the sweetviz return an html file that's saved we need to display it here using the IPython command\nIPython.display.IFrame(src='SWEETVIZ_REPORT.html', width=1080, height=600)","6a8a5ec7":"# Let's have a closer look at the two columns we noticed to have high correlation\n# we'll check the percentage of malignant tumor to a cell size higher than 5\n# you too can change the cell shape to view different insights\ncellShape = 5\nmask = data['Uniformity of Cell Shape'] > cellShape\nfilteredData = data[mask]\nmalignantTumors = filteredData.loc[data['Class'] == 1]\nprint(f\"{int(malignantTumors.shape[0] \/ filteredData.shape[0] * 100)}% of {filteredData.shape[0]} tumors with uniformity of cell shape higher than {cellShape} are malignant\")\n\n# Let's check cell size too\ncellSize = 4\nmask = data['Uniformity of Cell Size'] > cellSize\nfilteredData = data[mask]\nprint(f\"{int(malignantTumors.shape[0] \/ filteredData.shape[0] * 100)}% of {filteredData.shape[0]} tumors with uniformity of cell size higher than {cellSize} are malignant\")\n","e127c080":"# Encode labels to strings\nlabels = data['Class']\nlabels[labels==1] = 'Malignant'\nlabels[labels==0] = 'Benign'\n\n# Plot relation between the two columns labeld by type of tumor\nplt.figure(figsize=(7,7))\nsns.set_style('dark')\nsns.scatterplot(data=data, x='Uniformity of Cell Shape', y= 'Uniformity of Cell Size',\n                       hue= labels,\n                       s=100)\nplt.show()","46dbc228":"# Adding name of columns to variables for easier use\nuCellSize = data.columns[1]\nuCellShape = data.columns[2]\nprint(f\"{uCellSize}\\n{uCellShape}\")","e3201239":"# Split data to train and target\nX , Y = data[[uCellSize,uCellShape]] , data['Class']\nX.head(2)","cf71a32f":"# Split data into train and validation data\n# this can give us intuition on how the model would do in data it hasn't been exposed to\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1)\nprint(f\"Train size {X_train.shape[0]} rows\\nTest Size {X_test.shape[0]} rows\")","1e90f75a":"n_iterations = 100","d58a1ea6":"# Prepare model\nlogisticModel = LogisticRegression(max_iter = n_iterations, multi_class= 'ovr', class_weight='balanced')","92f7d312":"# Cross validation score\nscores = cross_val_score(logisticModel,X_train,Y_train, cv= 3)\nprint(f\"Average Cross Validation Score: {sum(scores) \/ 3}\")","8ab17a22":"# fit on train data\nlogisticModel.fit(X_train,Y_train)\n# socre on validation data\nlogisticModel.score(X_test, Y_test)","ec967c8f":"X_train, X_test, Y_train, Y_test = train_test_split(data.drop(['Class'], axis = 1), data['Class'], test_size=0.1)\nprint(f\"Train size {X_train.shape[0]} rows\\nTest Size {X_test.shape[0]} rows\")","681132cb":"allFeaturesModel = LogisticRegression(max_iter= 100, multi_class='ovr',class_weight='balanced')","84134c97":"scores = cross_val_score(allFeaturesModel,X_train, Y_train, cv= 3)\nprint(f\"Average Cross Validation Score: {sum(scores) \/ 3}\")","d238ff6a":"# fit on train data\nallFeaturesModel.fit(X_train,Y_train)\n# socre on validation data\nallFeaturesModel.score(X_test, Y_test)","4c04e8ea":"# Models\nlet's try to fit a model on the above data then we can go back to do more digging again\n\nThe problem at hand is a simple binary classification so a Logistic Regression model should do the job","8084d1a9":"### Choosen Features","52ea3153":"The score is close which is a good sign, but.. what if we tried using all features","027a1874":"# Load Data","c815eabd":"# EDA\n<p> For now we have discoverd that our data luckily has no null values and all columns are of integer types and are in range 1-10 except for the \"class\" column ( last part is stated in the data description) <br\/> now let's dive more into it","6f37ea8f":"# Imports","33f63ea7":"### All Features\nsame steps as above but using all features of data","cfa6ffa4":"There are some outliers as seen above but that won't be a problem, it's clear that higher values are almost all malignant","b399bdaa":"# Conclusion\n<h3>The data set was pretty clean and prepared, from our EDA we found that almost all tumors beyond certain size and shape is malignant, however other features contributed to the diagnosis since our model with all features did better than the one with the picked features.<h3>\n\nYou can still try different models with different parametes to find the highest accuracy.\n\n<h5> <em>Have extra insights or noticed things not mentioned here? Please feel free to share and discuss them <\/em><\/h5>","281cf73d":"## First let's split the data to a train and test set\ni will be using only the uniformity columns","13cddaea":"<h3>First by viewing the Associations between columns (by clicking associations button at top), we can see that:<\/h3>\n\u2022 Uniformity of cell size and Uniformity of cell shape  has a noticable high correlation with the target column\n\n\u2022 Another thing we can notice by data distribution is that most of data is skewed towards low values , which is excpected since these small values indicate there's no anomaly\n","2d87f3fb":"# Cleaning","4f2616fe":"accuracy is pretty good, now let's test it on our validation set","d4e907c4":"## Logistic Regression\nMore info on the topic : [Logistic Regression](https:\/\/towardsdatascience.com\/logistic-regression-detailed-overview-46c4da4303bc)"}}