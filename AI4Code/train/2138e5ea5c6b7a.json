{"cell_type":{"7f402dbf":"code","42d58fbc":"code","6fbf0773":"code","49998184":"code","b47fbe46":"code","04a69038":"code","1db73457":"code","4ec39319":"code","5df48bc4":"code","5d4f7f9e":"code","998ab2a6":"code","b0b2a6df":"code","3396f722":"code","ebb062a7":"markdown","5909a261":"markdown","56243a70":"markdown","313da496":"markdown"},"source":{"7f402dbf":"%matplotlib inline","42d58fbc":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.utils.data.sampler","6fbf0773":"print(os.listdir(\"..\/input\"))","49998184":"class MNIST:\n    \"\"\"Dataset of 42000 28x28 grayscale images of handwritten characters\"\"\"\n\n    image_shape = (1, 28, 28)\n    num_classes = 10\n\n    @classmethod\n    def _get_images(cls, df):\n        x = df.values\n        x = x.reshape((-1, *cls.image_shape))\n        x = x.astype(np.float32)\n        x \/= 255\n        return x\n\n    def __init__(self, input_file, train=True):\n\n        df = pd.read_csv(input_file)\n\n        if train:\n            self.labels = df['label'].values\n            df = df.drop(columns=['label'])\n\n        self.images = self._get_images(df)\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        if hasattr(self, 'labels'):\n            return self.images[idx, :], self.labels[idx]\n        else:\n            return self.images[idx, :]","b47fbe46":"class DigitRecognizerCNN(nn.Module):\n    \"\"\"Simple convolutional network consisting of 2 convolution layers with\n    max-pooling followed by two fully-connected layers and a softmax output\n    layer\n    \"\"\"\n\n    def __init__(self, num_classes):\n\n        super().__init__()\n\n        self.features = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1),\n            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(p=0.2),\n            nn.Linear(32 * 7 * 7, 784),\n            nn.ReLU(),\n            nn.Dropout(p=0.2),\n            nn.Linear(784, 784),\n            nn.ReLU(),\n            nn.Linear(784, num_classes))\n\n    def forward(self, X):\n        x = self.features(X)\n        x = x.view(x.size(0), 32 * 7 * 7)\n        x = self.classifier(x)\n        return x","04a69038":"def train(model,\n          loss_fn,\n          optimizer,\n          train_batches,\n          cv_batches,\n          device,\n          num_epochs=30,\n          status_every=5):\n\n    \"\"\"Train a model with a given loss function and optimizer by iterating over\n    mini-batches. Return training loss and cross-validation score for each\n    mini-batch\n    \"\"\"\n\n    losses_scores = []\n\n    for epoch in range(num_epochs):\n\n        epoch_losses = []\n\n        for images, labels in train_batches:\n\n            images = images.to(device)\n            labels = labels.to(device)\n\n            logits = model(images)\n            loss = loss_fn(logits, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_losses.append(loss.item())\n\n        epoch_scores = []\n\n        for images, labels in cv_batches:\n\n            images = images.to(device)\n            labels = labels.to(device)\n\n            logits = model(images)\n            _, labels_pred = logits.max(dim=1)\n            score = (labels_pred == labels).float().mean()\n            epoch_scores.append(score.item())\n\n        losses_scores.append({'epoch': epoch,\n                              'loss': epoch_losses,\n                              'score': epoch_scores})\n\n        if epoch % status_every == 0 or epoch == num_epochs - 1:\n            print(f'epoch={epoch:g}, '\n                  f'loss={np.mean(epoch_losses):g}, '\n                  f'cv_score={np.mean(epoch_scores):g}, '\n                  f'cv_score_std={np.std(epoch_scores):g}')\n\n    return losses_scores","1db73457":"def train_model(train_batches,\n                cv_batches,\n                device,\n                num_epochs=30,\n                learning_rate=1e-4,\n                weight_decay=1e-3):\n    \n    \"\"\"Create and train a model with the specified hyperparameters\"\"\"\n\n    model = DigitRecognizerCNN(num_classes=MNIST.num_classes)\n    model = model.to(device)\n\n    loss_fn = nn.CrossEntropyLoss()\n\n    optimizer = torch.optim.Adam(model.parameters(),\n                                 lr=learning_rate,\n                                 weight_decay=weight_decay)\n\n    losses_scores = train(model,\n                          loss_fn,\n                          optimizer,\n                          train_batches,\n                          cv_batches,\n                          device,\n                          num_epochs=num_epochs)\n\n    return losses_scores, model","4ec39319":"def get_train_cv_indices(num_examples, train_fraction, random_seed=42):\n    \"\"\"Return indices of training and cross-validation data, determined by\n    taking random subsets of the data with a specified fraction of examples to\n    use in constructing the training set\"\"\"\n    np.random.seed(random_seed)\n    indices = np.random.permutation(num_examples)\n    train_examples = int(train_fraction * num_examples)\n    train_indices = indices[:train_examples]\n    cv_indices = indices[train_examples:]\n    return train_indices, cv_indices\n\n\n\ndef get_data_loader(data, indices, batch_size):\n    \"\"\"Return an iterator over mini-batches given a dataset, indices of a\n    subset of data to include in the batches, and batch size\"\"\"\n    sampler = torch.utils.data.sampler.SubsetRandomSampler(indices)\n    return torch.utils.data.DataLoader(\n        data, sampler=sampler, batch_size=batch_size)","5df48bc4":"if torch.cuda.is_available():\n    print('Using CUDA')\n    device = torch.device('cuda')\nelse:\n    print('CUDA not available. Using CPU')\n    device = torch.device('cpu')\n\ndata = MNIST('..\/input\/train.csv')\n\ntrain_indices, cv_indices = get_train_cv_indices(\n    len(data), train_fraction=0.95, random_seed=42)\n\ntrain_batches = get_data_loader(data, train_indices, batch_size=256)\ncv_batches = get_data_loader(data, cv_indices, batch_size=256)\n\nprint(f'Training on {len(train_indices)} examples. '\n      f'Cross-validating with {len(cv_indices)} examples')\n\nlosses_scores, model = train_model(train_batches,\n                                   cv_batches,\n                                   device,\n                                   num_epochs=30,\n                                   learning_rate=1e-4,\n                                   weight_decay=0.003)","5d4f7f9e":"plt.figure(figsize=(11, 7))\n\nfor metric in ['loss', 'score']:\n    (pd.concat({d['epoch']: pd.Series(d[metric], name=metric)\n                for d in losses_scores},\n               names=['epoch'])\n     .groupby('epoch').mean()\n     .plot(label=metric))\n\nplt.axhline(0, ls='--')\nplt.axhline(1, ls='--')\nplt.legend();","998ab2a6":"test_data = MNIST('..\/input\/test.csv', train=False)\nimages = torch.Tensor(test_data.images)\nlogits = model.to('cpu')(images)\n_, y_pred = torch.max(logits, dim=1)","b0b2a6df":"submission = pd.DataFrame({'ImageId': np.arange(len(y_pred)) + 1, 'Label': y_pred})\nsubmission.head()","3396f722":"submission.to_csv('submission.csv', index=False)","ebb062a7":"# Training and Cross-validation","5909a261":"# Model","56243a70":"# Data","313da496":"# Create submission file"}}