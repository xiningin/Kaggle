{"cell_type":{"4e7c4dc0":"code","ad256fc0":"code","e0e76301":"code","cdb43253":"code","cabc88d3":"code","b2dde444":"code","07fd71a7":"code","ac89e889":"code","eef86a28":"code","21ac1fa6":"code","e6d65234":"code","779998b6":"code","b8b81ed7":"code","a287fbe2":"code","8d1a42d3":"code","a8646910":"code","36276162":"code","31fd6370":"code","5bc0aa59":"code","8eff7199":"code","5bf50c56":"code","42719d8e":"code","f656158f":"code","6056ecec":"code","d1ae176d":"code","3e981118":"code","504a41c4":"code","bc74d1fd":"code","1271f8ed":"code","6d15099f":"code","47a78cd8":"markdown","c2f254a4":"markdown","b658585c":"markdown","d734eff1":"markdown","b16a4c07":"markdown","68665afd":"markdown","ae52a76e":"markdown","5a62240a":"markdown","b7bce8d8":"markdown","25556d56":"markdown","f9af3a65":"markdown","4a97d8d4":"markdown","876cb242":"markdown","89a4d49c":"markdown","f7e244d1":"markdown","fa5d6a5d":"markdown","da3702ff":"markdown","5e76ef37":"markdown","51e5e2be":"markdown","0dbfeb91":"markdown","a7e211d1":"markdown","6175026c":"markdown","00dd16b6":"markdown"},"source":{"4e7c4dc0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/tweets\"))\n\n# Any results you write to the current directory are saved as output.","ad256fc0":"import re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport string\nimport nltk","e0e76301":"train  = pd.read_csv('..\/input\/tweets\/train_E6oV3lV.csv')\ntest = pd.read_csv('..\/input\/tweets\/test_tweets_anuFYb8.csv')","cdb43253":"train.head()","cabc88d3":"combi = train.append(test, ignore_index=True)","b2dde444":"def remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt  ","07fd71a7":"# remove twitter handles (@user)\ncombi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")","ac89e889":"combi.head()","eef86a28":"# remove special characters, numbers, punctuations\ncombi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")","21ac1fa6":"combi.head(10)","e6d65234":"combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","779998b6":"tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())\ntokenized_tweet.head()","b8b81ed7":"from nltk.stem.porter import *\nstemmer = PorterStemmer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x]) # stemming\ntokenized_tweet.head()","a287fbe2":"for i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n\ncombi['tidy_tweet'] = tokenized_tweet\ncombi.head()","8d1a42d3":"all_words = ' '.join([text for text in combi['tidy_tweet']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","a8646910":"normal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]])\n\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","36276162":"negative_words = ' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\nwordcloud = WordCloud(width=800, height=500,\nrandom_state=21, max_font_size=110).generate(negative_words)\nplt.figure(figsize=(10, 7))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","31fd6370":"# function to collect hashtags\ndef hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags","5bc0aa59":"# extracting hashtags from non racist\/sexist tweets\n\nHT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0])\n\n# extracting hashtags from racist\/sexist tweets\nHT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1])\n\n# unnesting list\nHT_regular = sum(HT_regular,[])\nHT_negative = sum(HT_negative,[])\n","8eff7199":"a = nltk.FreqDist(HT_regular)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","5bf50c56":"b = nltk.FreqDist(HT_negative)\ne = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n# selecting top 10 most frequent hashtags\ne = e.nlargest(columns=\"Count\", n = 10)   \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","42719d8e":"from sklearn.feature_extraction.text import CountVectorizer\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# bag-of-words feature matrix\nbow = bow_vectorizer.fit_transform(combi['tidy_tweet'])","f656158f":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# TF-IDF feature matrix\ntfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])","6056ecec":"import gensim\ntokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split()) # tokenizing\n\nmodel_w2v = gensim.models.Word2Vec(\n            tokenized_tweet,\n            size=200, # desired no. of features\/independent variables \n            window=5, # context window size\n            min_count=2,\n            sg = 1, # 1 for skip-gram model\n            hs = 0,\n            negative = 10, # for negative sampling\n            workers= 2, # no.of cores\n            seed = 34)\n\nmodel_w2v.train(tokenized_tweet, total_examples= len(combi['tidy_tweet']), epochs=20)","d1ae176d":"def word_vector(tokens, size):\n    vec = np.zeros(size).reshape((1, size))\n    count = 0.\n    for word in tokens:\n        try:\n            vec += model_w2v[word].reshape((1, size))\n            count += 1.\n        except KeyError: # handling the case where the token is not in vocabulary\n                         \n            continue\n    if count != 0:\n        vec \/= count\n    return vec","3e981118":"wordvec_arrays = np.zeros((len(tokenized_tweet), 200))\n\nfor i in range(len(tokenized_tweet)):\n    wordvec_arrays[i,:] = word_vector(tokenized_tweet[i], 200)\n    \nwordvec_df = pd.DataFrame(wordvec_arrays)\nwordvec_df.shape\n\n","504a41c4":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\ntrain_bow = bow[:31962,:]\ntest_bow = bow[31962:,:]\n\n# splitting data into training and validation set\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n\nlreg = LogisticRegression()\nlreg.fit(xtrain_bow, ytrain) # training the model\n\nprediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\nprediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int) # calculating f1 score","bc74d1fd":"train_tfidf = tfidf[:31962,:]\ntest_tfidf = tfidf[31962:,:]\n\nxtrain_tfidf = train_tfidf[ytrain.index]\nxvalid_tfidf = train_tfidf[yvalid.index]\n\nlreg.fit(xtrain_tfidf, ytrain)\n\nprediction = lreg.predict_proba(xvalid_tfidf)\nprediction_int = prediction[:,1] >= 0.3\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)","1271f8ed":"train_w2v = wordvec_df.iloc[:31962,:]\ntest_w2v = wordvec_df.iloc[31962:,:]\n\nxtrain_w2v = train_w2v.iloc[ytrain.index,:]\nxvalid_w2v = train_w2v.iloc[yvalid.index,:]","6d15099f":"lreg.fit(xtrain_w2v, ytrain)\n\nprediction = lreg.predict_proba(xvalid_w2v)\nprediction_int = prediction[:,1] >= 0.3\nprediction_int = prediction_int.astype(np.int)\nf1_score(yvalid, prediction_int)","47a78cd8":"Word cloud cho c\u00e1c nh\u00e3n kh\u00f4ng ph\u00e2n bi\u1ec7t ch\u1ee7ng t\u1ed9c v\u00e0 gi\u1edbi t\u00ednh","c2f254a4":"Word2vec Feature","b658585c":"B\u1ecf d\u1ea5u, s\u1ed1 v\u00e0 c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t","d734eff1":"TF-IDF features","b16a4c07":"* TF = (S\u1ed1 l\u1ea7n t\u1eeb t xu\u1ea5t hi\u1ec7n trong document)\/(s\u1ed1 l\u01b0\u1ee3ng c\u00e1c t\u1eeb trong v\u0103n b\u1ea3n)\n* IDF = log(N\/n), where, N t\u1ed5ng s\u1ed1 document v\u00e0 n s\u1ed1 l\u01b0\u1ee3ng document m\u00e0 t\u1eeb t xu\u1ea5t hi\u1ec7n \u1edf trong \u0111\u00f3.\n* TF-IDF = TF*IDF\n","68665afd":"Building model using Bag-of-Words features","ae52a76e":"**Non-Racist\/Sexist Tweets**","5a62240a":"Bag-of-words features : t\u1eebng gi\u00e1 tr\u1ecb trong vecter n\u00f3i l\u00ean t\u1ea7n su\u1ea5t xh c\u1ee7a t\u1eeb \u0111\u00f3 trong tweet ","b7bce8d8":"**Extracting Features **","25556d56":"Building model using TF-IDF features","f9af3a65":"**Generation and Visualization from Tweets**\n* Understand common word: wordcloud","4a97d8d4":"**Preprocessing and cleaning**","876cb242":"**Tokenization**","89a4d49c":"**Label** \"1\" n\u1ebfu tweet ph\u00e2n bi\u1ec7t ch\u1ee7ng t\u1ed9c ho\u1eb7c gi\u1edbi t\u00ednh v\u00e0 \"0\" n\u1ebfu ng\u01b0\u1ee3c l\u1ea1i","f7e244d1":"Word Emdeding","fa5d6a5d":"Ph\u00e2n bi\u1ec7t ch\u1ee7ng t\u1ed9c v\u00e0 gi\u1edbi t\u00ednh","da3702ff":"N\u1ed1i c\u00e1c token l\u1ea1i v\u1edbi nhau","5e76ef37":"Racist\/Sexist Tweets","51e5e2be":"**Model Building: Sentiment Analysis**\n* Logistic Regression","0dbfeb91":"Lo\u1ea1i b\u1ecf c\u00e1c t\u1eeb ng\u1eafn nh\u01b0 \"hmm\" ho\u1eb7c \"oh\" n\u00f3 kh\u00f4ng h\u1eefu \u00edch trong ph\u00e2n t\u00edch","a7e211d1":"Hastag cho ta bi\u1ebft xu h\u01b0\u1edbng \u0111ang di\u1ec5n ra tr\u00ean twitter","6175026c":"**Stemming**","00dd16b6":"B\u00e0i to\u00e1n ph\u00e2n l\u1edbp xem li\u1ec7u tweet c\u00f3 ph\u00e2n bi\u1ec7t ch\u1ee7ng t\u1ed9c v\u00e0 gi\u1edbi t\u00ednh kh\u00f4ng?\n\n[https:\/\/www.analyticsvidhya.com\/blog\/2018\/07\/hands-on-sentiment-analysis-dataset-python\/?utm_source=facebook.com&utm_medium=social&fbclid=IwAR0c7T39dgRnmayeKNygl9L69wyvvflugS3FvlgeqGNuFDpym3flF9bIVHo](http:\/\/)"}}