{"cell_type":{"30d4a10b":"code","ed99dfb1":"code","63955474":"code","77ce9da9":"code","0257593d":"code","5290d086":"code","b8a22352":"code","5a1f6e5f":"code","5157ba98":"code","7650efea":"code","c145985a":"code","8a300e0b":"code","bb4cdf2f":"code","1d56e73d":"code","c511e7ff":"markdown"},"source":{"30d4a10b":"! pip install pyspark","ed99dfb1":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas_profiling\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","63955474":"from pyspark.sql import SparkSession, DataFrame, functions as F\nfrom pyspark.ml.feature import Imputer, StringIndexer, VectorIndexer, VectorAssembler, OneHotEncoderEstimator, PCA, Bucketizer\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml import Pipeline\nspark = SparkSession.builder.getOrCreate()\nspark","77ce9da9":"sdf_train = spark.read.csv('\/kaggle\/input\/titanic\/train.csv', inferSchema = True, header = True)\nsdf_test = spark.read.csv('\/kaggle\/input\/titanic\/test.csv', inferSchema = True, header = True)","0257593d":"# ref: https:\/\/www.kaggle.com\/garbamoussa\/titatanic-overfitting-underfitting\ndef _evaluate_initials(sdf: DataFrame) -> DataFrame:\n    dizip_initials = {k:v for k,v in (zip(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don'],\n                                         ['Miss','Miss','Miss','Mr','Mr','Mrs','Mrs','Other','Other','Other','Mr','Mr','Mr']))}\n    _sdf = sdf.withColumn('Initial',  F.regexp_extract( sdf['Name'], ('([A-Za-z]+)\\.'),1 ) )\n    _sdf = _sdf.replace(dizip_initials,1,'Initial')\n    return _sdf\n\ndef _handle_missing_age(sdf: DataFrame) -> DataFrame:\n    _sdf = sdf\n    _sdf = _sdf.withColumn('Age', F.when((F.isnull(_sdf['Age'])) & (_sdf['Initial'] == 'Mr') , 33 )\\\n                            .otherwise(F.when((F.isnull(_sdf['Age'])) & (_sdf['Initial'] == 'Mrs') , 36)\\\n                            .otherwise(F.when((F.isnull(_sdf['Age'])) & (_sdf['Initial'] == 'Master') , 5)\\\n                            .otherwise(F.when((F.isnull(_sdf['Age'])) & (_sdf['Initial'] == 'Miss') , 22)\\\n                            .otherwise(F.when((F.isnull(_sdf['Age'])) & (_sdf['Initial'] == 'Other') , 46)\\\n                            .otherwise(_sdf['Age']) )))))\n    return _sdf\n    \n    \n# _sdf.select('Age').distinct().toPandas().T                        \n# _sdf.select('Age').toPandas().profile_report()","5290d086":"# ref : https:\/\/www.kaggle.com\/kabure\/titanic-baseline-eda-pipes-easy-to-starters\ndef _create_family_size(sdf: DataFrame) -> DataFrame :\n#     family_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', \n#               5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large',\n#               11: 'Large'}\n\n    _sdf = sdf.withColumn('FamilySize', sdf['Parch'] + sdf['SibSp'] + 1 )\n#     # bucketting\n#     family_map = {1: 0, 2: 1, 3: 1, 4: 1, 5: 2, 6: 2, 7: 3, 8: 3, 11: 3}\n#     _sdf = _sdf.replace(family_map,1,'FamilySize')\n    \n    return _sdf\n\n# _create_family_size(sdf_train).toPandas().T","b8a22352":"def _clean_dataset(sdf: DataFrame, col_to_convert: list, col_to_impute: list) -> DataFrame:\n    for col in col_to_convert:\n        sdf = sdf.withColumn(col,sdf[col].cast('double'))\n    col_to_impute += col_to_convert\n\n    imputer = Imputer(inputCols = col_to_impute, outputCols = col_to_impute)\n    sdf = imputer.fit(sdf).transform(sdf)\n    return sdf","5a1f6e5f":"sdf_train_cleaned = _clean_dataset ( \n    _handle_missing_age(\n    _evaluate_initials(\n    _create_family_size(sdf_train)\n    )) \n    ,['Ticket','SibSp','Parch'],['Fare'] \n)\n\nsdf_test_cleaned = _clean_dataset ( \n    _handle_missing_age(\n    _evaluate_initials(\n    _create_family_size(sdf_test)\n    )) \n    ,['Ticket','SibSp','Parch'],['Fare'] \n)\n","5157ba98":"numeric_cols = ['PassengerId','Survived', 'Pclass','Age', 'SibSp','Parch','Ticket','Fare'] \nnumeric_features = ['PassengerId','Pclass','Age', 'SibSp','Parch','Fare'] \n# numeric_cols_test = ['PassengerId', 'Pclass','Age', 'SibSp','Parch','Ticket','Fare', 'Sex'] \nstring_features = [ 'Embarked', 'Sex'] #, 'Cabin'\n# string_features += ['Initial']","7650efea":"# sdf_test_cleaned.toPandas().profile_report()\nnumeric_features","c145985a":"_stages = []\nstring_indexer =  [StringIndexer(inputCol = column , \\\n                                 outputCol = column + '_S_indx', handleInvalid = \"skip\") for column in string_features]\n_stages += string_indexer\n\none_hot_encoder = [OneHotEncoderEstimator(inputCols = [column + '_S_indx' for column in string_features ], \\\n                                          outputCols =  [column + '_encoded' for column in string_features ])]\n_stages += one_hot_encoder\n\nvect_indexer = [VectorIndexer(inputCol = column + '_encoded',\n                             outputCol = column + '_V_indx', maxCategories=10) for column in string_features]\n_stages += vect_indexer\n\nfamilt_size_splits = [1, 2, 5, 7, 100] #[-float(\"inf\"), 1, 2, 5, float(\"inf\")]\nbucketizer = Bucketizer(splits = familt_size_splits, inputCol = 'FamilySize',outputCol = 'bucketized_FamilySize')\n_stages += [bucketizer]\n\nnumeric_features += ['bucketized_FamilySize']\n\nassemblerInput =  [f  for f in numeric_features]  \nassemblerInput += [f + \"_V_indx\" for f in string_features]\nvector_assembler = VectorAssembler(inputCols = assemblerInput, \\\n                                   outputCol = 'vect_features')\n_stages += [vector_assembler]\n\n# pca = PCA(inputCol = 'vect_features', outputCol = 'pca_features', k = 5)\n# _stages += [pca]\n\nrf = RandomForestClassifier(labelCol = 'Survived', featuresCol = 'vect_features', numTrees = 100, maxDepth = 4, maxBins = 1000)\n_stages += [rf]\n\npipeline = Pipeline(stages = _stages)\nmodel = pipeline.fit(sdf_train_cleaned)\nsdf_predict = model.transform(sdf_test_cleaned)\nsdf_predict.toPandas().profile_report()","8a300e0b":"sdf_submission = sdf_predict.select('PassengerId','prediction')\\\n                            .withColumn('Survived',sdf_predict['prediction'].cast('integer'))\\\n                            .select('PassengerId','Survived')\n# sdf_submission.toPandas().T","bb4cdf2f":"sdf_submission.coalesce(1).write.csv(\"submission\",mode=\"overwrite\",header=True)\n","1d56e73d":"print(os.listdir('submission'))\n","c511e7ff":"<a href='submission\/part-00000-7ed8afef-ec95-4247-b97a-12b11efcd035-c000.csv'>Download<\/a>"}}