{"cell_type":{"34b4b56f":"code","f8caa081":"code","5282d3d9":"code","7eee8432":"code","819b3898":"code","0a11213d":"code","5e281d3c":"code","2a0ae551":"code","d7e719d2":"code","6d8647b9":"code","cf082f3b":"code","1050f9e3":"code","69f496db":"code","e8a95487":"code","e1155694":"code","13e4a3bf":"code","28d7a6be":"code","932e4df9":"code","72f13221":"code","9bd83608":"code","0381751d":"code","5198d5b6":"code","3129387e":"code","573cdac1":"code","0fcf2bd2":"code","d401d0a7":"code","afd87bc4":"code","e9050781":"code","09dd99cc":"code","7c5c4875":"code","301e7b5e":"code","fe2e6beb":"code","5cbdcd11":"code","ccfe6ded":"code","e5b2e850":"code","b4664e36":"code","e50b17df":"code","375a7163":"code","111d2f5f":"code","c043274e":"code","0eecd42d":"code","09fced4d":"code","5ed154d7":"code","85d23e5c":"code","b820727f":"code","e7594b30":"code","bc474b36":"code","dbd48bcc":"code","c08e4d94":"code","7e75b19b":"code","ed3e48cf":"code","3fdbfc7e":"code","caa70605":"code","e1d63ce7":"code","9bb5675a":"code","14d2e8ae":"code","26338528":"code","071b47b1":"code","c69d6a04":"code","b4004d8e":"code","6bd26623":"code","8a242ff1":"code","bde592f5":"code","1cb26211":"code","966611d5":"code","709ed839":"code","52250c04":"code","52e5700a":"markdown","cdbbb50c":"markdown","25ae484e":"markdown","c1f40f0b":"markdown","2f673fe7":"markdown","61a3c4ad":"markdown","d9a3a254":"markdown","a6904c90":"markdown","f90333cc":"markdown","8ab60425":"markdown","6d650f00":"markdown","1439777c":"markdown","8ed8d809":"markdown","f86686fd":"markdown","8f210d94":"markdown","f48ca754":"markdown","c8f9ccf5":"markdown","10681d36":"markdown","813fee8c":"markdown","bfab9c2f":"markdown","9441c77f":"markdown","1358c36e":"markdown","8b2dd63a":"markdown","5bea62c6":"markdown","98115873":"markdown","e5afca2c":"markdown","69b2fb78":"markdown","bab14724":"markdown","44b51296":"markdown","fb857231":"markdown","66c97b1b":"markdown","a4d3bd53":"markdown","6a004208":"markdown","6ca79a92":"markdown"},"source":{"34b4b56f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","f8caa081":"# Importing the imporant libraries that will be used thoughout the notebook\nimport pandas as pd \nimport numpy as np \nfrom IPython.display import display\n\nimport matplotlib.pyplot as plt \nimport re\nimport string\n\nimport nltk\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.corpus import words\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.sentiment.util import *\nnltk.download('stopwords')\nnltk.download('vader_lexicon')\n\n\nfrom collections import Counter\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\nimport plotly.express as px\n\nsns.set(style=\"darkgrid\")","5282d3d9":"# reading data from csv file\ndf = pd.read_csv(\"..\/input\/nigeria-endsars-tweets\/NigeriaEndSars data.csv\")\ndf.head(5)","7eee8432":"len(df)","819b3898":"# printing the data shape (how many rows and column)\ndf.shape","0a11213d":"# filtering needed columns\nneeded_columns=['username','date','content']\ndf=df[needed_columns]\ndf.head()","5e281d3c":"# convert username type from object to category for assigning the numbers after it\ndf.username=df.username.astype('category')\ndf.username=df.username.cat.codes # assign a unique numerical code to each category\ndf.date=pd.to_datetime(df.date).dt.date # it will give only the date","2a0ae551":"# printing first 5 rows\ndf.head(5)","d7e719d2":"# assigning content column to 'texts' variable\ntexts=df.content\ntexts","6d8647b9":"remove_url=lambda x:re.sub(r'http\\S+','',str(x))\ntexts_lr=texts.apply(remove_url)\ntexts_lr","cf082f3b":"to_lower=lambda x: x.lower()\ntexts_lr_lc=texts_lr.apply(to_lower)\ntexts_lr_lc","1050f9e3":"remove_puncs= lambda x:x.translate(str.maketrans('','',string.punctuation))\ntexts_lr_lc_np=texts_lr_lc.apply(remove_puncs)\ntexts_lr_lc_np","69f496db":"more_words=[\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\nstop_words=set(stopwords.words('english')) #nltk package\nstop_words.update(more_words)\n\nremove_words=lambda x: ' '.join([word for word in x.split() if word not in stop_words]) #.join is from package string\ntexts_lr_lc_np_ns=r=texts_lr_lc_np.apply(remove_words)\ntexts_lr_lc_np_ns","e8a95487":"words_list=[word for line in texts_lr_lc_np_ns for word in line.split()]\nwords_list[:5]","e1155694":"# creating dataframe and bar graph of most common 50 words with their frequency\nword_counts=Counter(words_list).most_common(50)\nword_df=pd.DataFrame(word_counts)\nword_df.columns=['word','frq']\ndisplay(word_df.head(5))\n# px=import plotly.express\n#px.bar(word_df,x='word',y='frq',title='Most common words')\n\nfig = plt.figure(figsize = (15, 7))\n \n# creating the bar plot\nplt.bar(word_df['word'],word_df['frq'])\nplt.xticks(rotation=90)\nplt.xlabel('word')\nplt.ylabel('frq')\nplt.title('Most common words')\nplt.show()","13e4a3bf":"display(df.head(5))\ndf.text=texts_lr_lc_np_ns\ndisplay(df.head(5))","28d7a6be":"def clean_text(text):\n    '''remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\ndf['content'] = df['content'].apply(lambda x: clean_text(x))\ndisplay(df)","932e4df9":"# function to remove emoticons, symbols or flags by their codes\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","72f13221":"# applying remove_emoji function on tweets\ndf['content']=df['content'].apply(lambda x: remove_emoji(x))\ndisplay(df)","9bd83608":"# getting polarity scores of tweets and storing them in variable 'sentiment_scores'\nsid=SentimentIntensityAnalyzer()\nps=lambda x:sid.polarity_scores(x)\nsentiment_scores=df.text.apply(ps)\nsentiment_scores","0381751d":"# create the data frame of negative, neutral, positive and compound polarity scroes\nsentiment_df=pd.DataFrame(data=list(sentiment_scores))\ndisplay(sentiment_df)","5198d5b6":"# it will label the tweets as neutral if its compound polarity is 0 and positive if its greater than 0 and negative if its less than 0\nlabelize=lambda x:'neutral' if x==0 else('positive' if x>0 else 'negative')\nsentiment_df['label']=sentiment_df.compound.apply(labelize)\ndisplay(sentiment_df.head(10))","3129387e":"display(df.head(5))\ndata=df.join(sentiment_df.label)\nsentiment_df = df.join(sentiment_df)\ndisplay(data.head(5))","573cdac1":"counts_df=data.label.value_counts().reset_index()\ndisplay(counts_df)","0fcf2bd2":"plt.figure(figsize=(8,5)) \nsns.barplot(x='index',y='label',data=counts_df)","d401d0a7":"data_agg=data[['username','date','label']]\ndisplay(data_agg.head(5))","afd87bc4":"data_agg=data_agg.groupby(['date','label'])\ndisplay(data_agg.head(5))","e9050781":"data_agg=data_agg.count()\ndisplay(data_agg.head(5))","09dd99cc":"data_agg=data_agg.reset_index()\ndisplay(data_agg.head(5))","7c5c4875":"data_agg.columns=['date','label','counts']\ndisplay(data_agg.head())","301e7b5e":"neg = data_agg[data_agg['label']=='negative']\npos = data_agg[data_agg['label']=='positive']\nneu = data_agg[data_agg['label']=='neutral']","fe2e6beb":"# px.line(data_agg,x='date',y='counts',color='label',title='Daily Tweet Sentimental Analysis')\nfig = plt.figure(figsize = (15, 7))\nplt.plot(pos['date'],pos['counts'], label='postivie')\nplt.plot(neg['date'],neg['counts'], label='negative')\nplt.plot(neu['date'],neu['counts'], label='neutral')\n \n# Add labels and title\nplt.title(\"Daily Tweet Sentimental Analysis\")\nplt.xlabel(\"date\")\nplt.ylabel(\"count\")\nplt.legend()\nplt.show()","5cbdcd11":"df['content']=df['content'].apply(lambda x: remove_emoji(x))\ndisplay(df)","ccfe6ded":"from wordcloud import WordCloud","e5b2e850":"cut_text = \" \".join(df.text)\nmax_words=100\nword_cloud = WordCloud(\n                    background_color='white',\n                    stopwords=set(stop_words),\n                    max_words=max_words,\n                    max_font_size=30,\n                    scale=5,\n    colormap='magma',\n                    random_state=1).generate(cut_text)\nfig = plt.figure(1, figsize=(50,50))\nplt.axis('off')\nplt.title('Word Cloud for Top '+str(max_words)+' words with # EndSARS on Twitter\\n', fontsize=100,color='blue')\nfig.subplots_adjust(top=2.3)\nplt.imshow(word_cloud)\nplt.show()","b4664e36":"df['date'] = pd.to_datetime(df['date'])\ndf['cleaned'] = texts_lr_lc_np_ns","e50b17df":"df_oct = df[(df['date']>='2020-10-01') & (df['date']<='2020-10-31')].reset_index(drop=True)\ndf_oct_20 = df[df['date']>='2020-10-20'].reset_index(drop=True)\n\ntexts_lr_lc_np_ns_oct = df_oct['cleaned']\ntexts_lr_lc_np_ns_oct_20 = df_oct_20['cleaned']","375a7163":"words_list_oct=[word for line in texts_lr_lc_np_ns_oct for word in line.split()]\nprint('Oct month',words_list[:5])\n\nwords_list_oct_20=[word for line in texts_lr_lc_np_ns_oct_20 for word in line.split()]\nprint('20th Oct',words_list[:5])","111d2f5f":"# creating dataframe and bar graph of most common 50 words with their frequency\nword_counts=Counter(words_list_oct).most_common(50)\nword_df=pd.DataFrame(word_counts)\nword_df.columns=['word','frq']\ndisplay(word_df.head(5))\n# px=import plotly.express\n#display(px.bar(word_df,x='word',y='frq',title='Most common words for Oct month'))\nfig = plt.figure(figsize = (15, 7))\n \n# creating the bar plot\nplt.bar(word_df['word'],word_df['frq'])\nplt.xticks(rotation=90)\nplt.xlabel('word')\nplt.ylabel('frq')\nplt.title('Most common words for Oct month')\nplt.show()\n\nword_counts=Counter(words_list_oct_20).most_common(50)\nword_df=pd.DataFrame(word_counts)\nword_df.columns=['word','frq']\ndisplay(word_df.head(5))\n# px=import plotly.express\n# display(px.bar(word_df,x='word',y='frq',title='Most common words for 20 Oct'))\nfig = plt.figure(figsize = (15, 7))\n \n# creating the bar plot\nplt.bar(word_df['word'],word_df['frq'])\nplt.xticks(rotation=90)\nplt.xlabel('word')\nplt.ylabel('frq')\nplt.title('Most common words for 20 Oct')\nplt.show()","c043274e":"df_oct['content'] = df_oct['cleaned']\ndf_oct_20['content'] = df_oct_20['cleaned']","0eecd42d":"df_oct.drop('cleaned',axis=1,inplace=True)\ndf_oct_20.drop('cleaned',axis=1,inplace=True)","09fced4d":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\ndf_oct['content'] = df_oct['content'].apply(lambda x: clean_text(x))\ndf_oct_20['content'] = df_oct_20['content'].apply(lambda x: clean_text(x))\ndisplay(df_oct)\ndisplay(df_oct_20)","5ed154d7":"# function to remove emoticons, symbols or flags by their codes\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","85d23e5c":"# applying remove_emoji function on tweets\ndf_oct['content']=df_oct['content'].apply(lambda x: remove_emoji(x))\ndf_oct_20['content']=df_oct_20['content'].apply(lambda x: remove_emoji(x))\ndisplay(df_oct)\ndisplay(df_oct_20)","b820727f":"# getting polarity scores of tweets and storing them in variable 'sentiment_scores'\nsid=SentimentIntensityAnalyzer()\nps=lambda x:sid.polarity_scores(x)\nsentiment_scores_oct=df_oct.content.apply(ps)\nsentiment_scores_oct_20=df_oct_20.content.apply(ps)\ndisplay(sentiment_scores_oct)\ndisplay(sentiment_scores_oct_20)","e7594b30":"# create the data frame of negative, neutral, positive and compound polarity scroes\nsentiment_df_oct=pd.DataFrame(data=list(sentiment_scores_oct))\nsentiment_df_oct_20=pd.DataFrame(data=list(sentiment_scores_oct_20))\ndisplay(sentiment_df_oct)\ndisplay(sentiment_df_oct_20)","bc474b36":"# it will label the tweets as neutral if its compound polarity is 0 and positive if its greater than 0 and negative if its less than 0\nlabelize=lambda x:'neutral' if x==0 else('positive' if x>0 else 'negative')\nsentiment_df_oct['label']=sentiment_df_oct.compound.apply(labelize)\nsentiment_df_oct_20['label']=sentiment_df_oct_20.compound.apply(labelize)\ndisplay(sentiment_df_oct.head(10))\ndisplay(sentiment_df_oct_20.head(10))","dbd48bcc":"display(df_oct.head(5))\ndata_oct=df.join(sentiment_df_oct.label)\ndisplay(data_oct.head(5))\ndisplay(df_oct_20.head(5))\ndata_oct_20=df.join(sentiment_df_oct_20.label)\ndisplay(data_oct_20.head(5))","c08e4d94":"counts_df_oct=data_oct.label.value_counts().reset_index()\ncounts_df_oct_20=data_oct_20.label.value_counts().reset_index()\ndisplay(counts_df_oct)\ndisplay(counts_df_oct_20)","7e75b19b":"plt.figure(figsize=(8,5)) \nsns.barplot(x='index',y='label',data=counts_df_oct)\n\nplt.figure(figsize=(8,5)) \nsns.barplot(x='index',y='label',data=counts_df_oct_20)","ed3e48cf":"data_agg_oct=data_oct[['username','date','label']]\ndata_agg_oct_20=data_oct_20[['username','date','label']]\ndisplay(data_agg_oct.head(5))\ndisplay(data_agg_oct_20.head(5))","3fdbfc7e":"data_agg_oct=data_agg_oct.groupby(['date','label'])\ndata_agg_oct_20=data_agg_oct_20.groupby(['date','label'])\ndisplay(data_agg_oct.head(5))\ndisplay(data_agg_oct_20.head(5))","caa70605":"data_agg_oct=data_agg_oct.count()\ndata_agg_oct_20=data_agg_oct_20.count()\ndisplay(data_agg_oct.head(5))\ndisplay(data_agg_oct_20.head(5))","e1d63ce7":"data_agg_oct=data_agg_oct.reset_index()\ndata_agg_oct_20=data_agg_oct_20.reset_index()\ndisplay(data_agg_oct.head(5))\ndisplay(data_agg_oct_20.head(5))","9bb5675a":"data_agg_oct.columns=['date','label','counts']\ndata_agg_oct_20.columns=['date','label','counts']\ndisplay(data_agg_oct.head())\ndisplay(data_agg_oct_20.head())","14d2e8ae":"neg = data_agg_oct[data_agg_oct['label']=='negative']\npos = data_agg_oct[data_agg_oct['label']=='positive']\nneu = data_agg_oct[data_agg_oct['label']=='neutral']\n\n# display(px.line(data_agg_oct,x='date',y='counts',color='label',title='Tweet Sentimental Analysis Oct'))\nfig = plt.figure(figsize = (15, 7))\nplt.plot(pos['date'],pos['counts'], label='postivie')\nplt.plot(neg['date'],neg['counts'], label='negative')\nplt.plot(neu['date'],neu['counts'], label='neutral')\n \n# Add labels and title\nplt.title(\"Tweet Sentimental Analysis Oct\")\nplt.xlabel(\"date\")\nplt.ylabel(\"counts\")\nplt.legend()\nplt.show()\n\nneg = data_agg_oct_20[data_agg_oct_20['label']=='negative']\npos = data_agg_oct_20[data_agg_oct_20['label']=='positive']\nneu = data_agg_oct_20[data_agg_oct_20['label']=='neutral']\n\n# display(px.line(data_agg_oct_20,x='date',y='counts',color='label',title='Daily Tweet Sentimental Analysis 20th Oct'))\nfig = plt.figure(figsize = (15, 7))\nplt.plot(pos['date'],pos['counts'], label='postivie')\nplt.plot(neg['date'],neg['counts'], label='negative')\nplt.plot(neu['date'],neu['counts'], label='neutral')\n \n# Add labels and title\nplt.title(\"Daily Tweet Sentimental Analysis 20th Oct\")\nplt.xlabel(\"date\")\nplt.ylabel(\"counts\")\nplt.legend()\nplt.show()","26338528":"df['content']=df['content'].apply(lambda x: remove_emoji(x))\ndisplay(df)","071b47b1":"from wordcloud import WordCloud","c69d6a04":"cut_text = \" \".join(df.text)\nmax_words=100\nword_cloud = WordCloud(\n                    background_color='white',\n                    stopwords=set(stop_words),\n                    max_words=max_words,\n                    max_font_size=30,\n                    scale=5,\n    colormap='magma',\n                    random_state=1).generate(cut_text)\nfig = plt.figure(1, figsize=(50,50))\nplt.axis('off')\nplt.title('Word Cloud for Top '+str(max_words)+' words with # EndSARS on Twitter\\n', fontsize=100,color='blue')\nfig.subplots_adjust(top=2.3)\nplt.imshow(word_cloud)\nplt.show()","b4004d8e":"date_df = df[['date']]\ndate_df['count'] = 1","6bd26623":"df[(df['date']>='2020-10-01') & (df['date']<='2020-10-31')]","8a242ff1":"df1 = df.groupby(df['date'].dt.to_period('M'))['content'].count()\ndf1 = df1.resample('M').asfreq().fillna(0)\ndf1.plot(kind='bar',figsize=(20,10))","bde592f5":"daily_tweets = df.groupby(['date'])['content'].count()\n\nfig = plt.figure(figsize = (15,5))\nplt.plot(daily_tweets.index,daily_tweets.values)\nplt.title('Daily Tweets\\' Trend', fontsize=16)\nplt.xlabel('Dates')\nplt.ylabel('# of Tweets')\nplt.show()","1cb26211":"sentiment_dist = data.label.value_counts()\n\nplt.pie(sentiment_dist, labels=sentiment_dist.index, explode= (0.1,0,0),\n        colors=['yellowgreen', 'gold', 'lightcoral'],\n        autopct='%1.1f%%', shadow=True, startangle=140)\nplt.title(\"Tweets\\' Sentiment Distribution \\n\", fontsize=16, color='Black')\nplt.axis('equal')\nplt.tight_layout()\nplt.show()","966611d5":"sentiment_df['username'] = sentiment_df['username'].astype(str)\n# Function to filter top 10 tweets by sentiment\ndef top10AccountsBySentiment(sentiment):\n    df = sentiment_df.query(\"label==@sentiment\")\n    top10 = df.groupby(by=[\"username\"])['label'].count().sort_values(ascending=False)[:10]\n    return(top10)","709ed839":"# Top 10 tweets by each sentiment\ntop10_pos = top10AccountsBySentiment(\"positive\")\ntop10_neg = top10AccountsBySentiment(\"negative\")\ntop10_neu = top10AccountsBySentiment(\"neutral\")\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, squeeze=True, figsize=(16,8))\nfig.suptitle('Top 10 Twitter Accounts \\n', fontsize=20)\n\nax1.barh(top10_pos.index, top10_pos.values, color='yellowgreen')\nax1.set_title(\"\\n\\n Positive Tweets\", fontsize=16)\n\nax2.barh(top10_neg.index, top10_neg.values, color='lightcoral')\nax2.set_title(\"\\n\\n Negative Tweets\", fontsize=16)\n\nax3.barh(top10_neu.index, top10_neu.values, color='gold')\nax3.set_title(\"\\n\\n Neutral Tweets\", fontsize=16);\n\nfig.tight_layout()\nfig.show()","52250c04":"pd.set_option('display.max_colwidth', None)\nprint('Top 10 positive tweets')\ndisplay(sentiment_df[sentiment_df['label']=='positive'].sort_values('compound',ascending=False)[0:10])\nprint('Top 10 negative tweets')\ndisplay(sentiment_df[sentiment_df['label']=='negative'].sort_values('compound')[0:10])\nprint('Top 10 neutral tweets')\ndisplay(sentiment_df[sentiment_df['label']=='neutral'].sort_values('compound')[0:10])","52e5700a":"### Let's do some additional data cleaning\n","cdbbb50c":"## Sentiment Analysis\n","25ae484e":"### Let's convert the username type to category so we can assign a unique numerical code to each username","c1f40f0b":"### Converting all tweets to lowercase","2f673fe7":"**EDA and sentiment analysis on EndSARS tweets.**<br>\n\n**Context**<br>\n\nEnd SARS is a decentralised social movement and series of mass protests against police brutality in Nigeria. The slogan calls for the disbanding of the Special Anti-Robbery Squad (SARS), a notorious unit of the Nigerian Police with a long record of abuses. The protests which takes its name from the slogan started in 2017 as a Twitter campaign using the hashtag #EndSARS to demand the disbanding of the unit by the Nigerian government. After experiencing a revitalisation in October 2020 following more revelations of the abuses of the unit, mass demonstrations occurred throughout the major cities of Nigeria, accompanied by vociferous outrage on social media platforms. About 28 million tweets bearing the hashtag have been accumulated on Twitter alone.<br>\n\nSource: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/End_SARS)<br>\nInspiration: The EndSARS Movement<br>\n \nTo begin with, I have on-boarded my Kaggle dataset, following tweets scraping exploration using this [snscrape notebook](https:\/\/www.kaggle.com\/ulrich07\/snscrape-exploration), with guidance from my mentor @ulrich G.<br>\n\nThe result is a dataset I uploaded on Kaggle named \u201cNigeria EndSARS tweets\u201d which contains  9737 random tweets with the hashtag #EndSARS. The tweets fall  between dates 03rd December 2017 and 22nd April 2021. <br>\n\nI would also like to acknowledge the following notebooks which were quite useful in this work. [COVID 19 Sentiment Analysis](https:\/\/www.kaggle.com\/kartikmohan1999\/covid19-sentiment-analysis) , [Covid19 Tweets EDA and Sentiment Analysis](https:\/\/www.kaggle.com\/purvasingh\/covid19-tweets-eda-and-sentiment-analysis) and my good friend Jawad Ahmad who assisted me greatly as well. \n\nThis notebook is organized as follows:<br>\n\n**1. Preprocessing on Nigeria EndSARS tweets**<br> \n\n    * Filtering needed columns.\n    * Assigning content columns to text variables.\n    * Remove URL from tweets\n    * Convert tweets to lowercase\n    * Removing punctuations, stopwords, brackets, emojis, links, words containing numbers etc.\n\n**2. EDA and Sentiment Analysis**<br>\n\n    * Top 5 words and frequency\n    * 50 most common words and frequency\n    * Getting polarity scores of tweets\n    * Labelling scores based on compound polarity value\n    * Joining the Labels column to the tweets.\n    * Plotting the Sentiment scores\n    * Group tweets by date and Labels\n    * Plotting the daily tweets sentiment analysis\n    * Generating word clouds\n    * Sentiment analysis and word cloud for the month of October which was the month of the EndSARS protest.\n    * Sentiment Analysis and word cloud for 20th October 2020 which was the day EndSARS protesters were shot at the Lekki Toll gate in Lagos, Nigeria.\n    * Plotting the general tweets sentiment distribution\n    * Top 10 twitter accounts with positive, negative and neutral tweets\n    * Top 10 tweets by each sentiment based on polarity scores in descending order to manually check the validity of the sentiment analysis.\n    \n    \n     \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","61a3c4ad":"**Conclusion**<br> \n\nThe EndSARS movement peaked in the month of October but it is clear from the analysis done that the tweets frequency have declined shortly after however, the frequency of tweets is higher now compared to the time before the protest and killing happened.<br> \n\nThe analysis of most common words for October and 20th October 2020 includes the word protesters and killing. The sentiment analysis for the 20th of october had the highest negative labels.<br> \n\nIn 2018 and 2019 there are times with sudden increase of the #EndSARS hashtags possibly triggered by viral cases of police brutality around these time frames. The movement was gaining momentum during these times as well that eventually led to the October nationwide protests.<br> \n\nIn conclusion, as this was a beginers attempt, It will be interesting to explore further and see what more can be done with this dataset and especially  with the performance of the sentiment analysis and accuracy which seems to still be a challenge here even though looking at the top ten tweets for each sentiments seem quite accurate, the work can be improved especially with respect to the neutral sentiments labelling <br> \n\nThe performance of a text classification model is heavily dependent upon the type of words used and type of features created for classification.  I may be useful to work towards re-training pre-existing sentiment algorithm and customise to my specific case which may still be used to improve the results of my analysis in this notebook.\n","d9a3a254":"#### let's create a big list of words out of all the tweets","a6904c90":"### group number of counts by\n#### date\n#### positive,neutral,negative","f90333cc":"### Generate wordcloud for this period","8ab60425":"### actually the 'username' is the count of users, so need to change the column name","6d650f00":"### let's join two dataframes","1439777c":"## Sentiment Analysis for October 2020 and 20th October 2020","8ed8d809":"### Some addtional cleaning","f86686fd":"### Labeling the scores based on the compound polarity value","8f210d94":"### Let's look at the Top 10 accounts by each sentiments\n","f48ca754":"### Let's visualise the 50 most common words","c8f9ccf5":"### Let's remove emoticons, symbols or flags by their codes","10681d36":"### Plotting the sentiment score counts","813fee8c":"### Let's print and go through the tweets - Top 10 tweets by each sentiment based on the polarity scores ","bfab9c2f":"### Removing URLs from tweets","9441c77f":"### Removing punctuations","1358c36e":"## Let's zoom into October 2020 and 20th October 2020 and re-run the same analysis for this specific month and day. This is when the protests and shootings happened.","8b2dd63a":"### Labeling the scores based on the compound polarity value","5bea62c6":"### Removing stopwords","98115873":"### Generating wordcloud for the specific period","e5afca2c":"#### put the Cleaned text in main dataframe","69b2fb78":"### let's join two dataframes","bab14724":"### Plotting the sentiment score counts","44b51296":"#### Putting the Cleaned text in main dataframe","fb857231":"### Let's filter the coloumns needed","66c97b1b":"### actually the 'username' is the count of users, so need to change the column name","a4d3bd53":"### Let's assign the content column to 'texts' variable","6a004208":"### group number of counts by\n#### date\n#### positive,neutral,negative","6ca79a92":"#### let's create a big list of words out of all the tweets"}}