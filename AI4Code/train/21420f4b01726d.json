{"cell_type":{"1a80289f":"code","0701cdde":"code","d9ba7e2f":"code","6206a633":"code","4e50fe62":"code","811f89f1":"code","b5630660":"code","bccf22cf":"code","67d9a7b4":"code","c1621e32":"code","fff89877":"code","cfab4839":"code","1f17116e":"code","cbf580ea":"code","1ac09f33":"code","5033ae56":"code","57434173":"code","b322c398":"markdown","18804e51":"markdown","dce59946":"markdown"},"source":{"1a80289f":"import torch.utils.data\nimport csv\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nfrom torch.utils.data import Dataset\nimport cv2\nimport os\nimport json\nfrom torchvision import models\nfrom torch.nn import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear\n\nimport math\nfrom torchvision import transforms\nimport torch\nimport torchvision\nimport tqdm\nimport numpy as np\nfrom torch.nn.functional import ctc_loss, log_softmax","0701cdde":"class DetectionDataset(torch.utils.data.Dataset):\n    def __init__(self, root, transformations=None, split=\"train\", splitSize=0.9):\n        super(DetectionDataset, self).__init__()\n        self.transformations = transformations\n        self.root = root\n\n        with open(os.path.join(root, \"train.json\"), 'r') as f:\n            data = json.load(f)\n\n        data = [x for x in data if x['file'] != \"train\/25632.bmp\"]\n\n        num_lines = len(data)\n\n        sz = round(splitSize * num_lines)\n        if split == \"train\":\n            self.data_dict = data[:sz]\n        elif split == \"val\":\n            self.data_dict = data[sz:]\n\n    def __getitem__(self, idx):\n        data = self.data_dict[idx]\n        img_path = os.path.join(self.root, data['file'])\n\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        original_boxes = data['nums']\n        num_objs = len(original_boxes)\n        boxes = []\n        for bbox in original_boxes:\n            bbox = bbox['box']\n            xmin = min(bbox[0][0], bbox[3][0])\n            xmax = max(bbox[1][0], bbox[2][0])\n            ymin = min(bbox[0][1], bbox[1][1])\n            ymax = max(bbox[2][1], bbox[3][1])\n            boxes.append([xmin, ymin, xmax, ymax])\n\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        labels = torch.ones((num_objs,), dtype=torch.int64)\n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n\n        image_id = torch.tensor([idx])\n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n\n        target = {}\n        target[\"boxes\"] = boxes\n        target[\"labels\"] = labels\n        target[\"image_id\"] = image_id\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n\n        if self.transformations is not None:\n            image = self.transformations(image)\n\n        return image, target\n\n    def __len__(self):\n        return len(self.data_dict)","d9ba7e2f":"class DetectionDatasetTest(torch.utils.data.Dataset):\n    def __init__(self, root, transformations=None):\n        super(DetectionDatasetTest, self).__init__()\n        self.transformations = transformations\n        self.root = root\n\n        self.images = []\n        line_count = 0\n        with open(os.path.join(root, \"submission.csv\"), 'r') as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=',')\n            for row in csv_reader:\n                if line_count == 0:\n                    line_count += 1\n                else:\n                    self.images.append(row[0])\n\n    def __getitem__(self, idx):\n        im_p = self.images[idx]\n        img_path = os.path.join(self.root, im_p)\n\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        target = {\"file\": im_p}\n        if self.transformations is not None:\n            image = self.transformations(image)\n\n        return image, target\n\n    def __len__(self):\n        return len(self.images)","6206a633":"def collate_fn(batch):\n    return tuple(zip(*batch))","4e50fe62":"train_transforms = transforms.Compose([\n        transforms.ToPILImage(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n    ])","811f89f1":"batch_size = 4\n\ntrain_dataset = DetectionDataset('data', train_transforms, split=\"train\")\nval_dataset = DetectionDataset('data', train_transforms, split=\"val\")\ndata_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=batch_size, shuffle=True, num_workers=0,\n    pin_memory=True,\n    collate_fn=collate_fn, drop_last=True)\ndata_loader_val = torch.utils.data.DataLoader(\n    val_dataset, batch_size=batch_size, shuffle=False, num_workers=0,\n    pin_memory=True,\n    collate_fn=collate_fn, drop_last=False)","b5630660":"print(\"Creating model...\")\ndevice = torch.device(\"cuda: 0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True, progress=True,\n                                                             num_classes=91, pretrained_backbone=True)\nnum_classes = 2\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\nfor param in model.parameters():\n    param.requires_grad = False;\nfor param in model.rpn.parameters():\n    param.requires_grad = True;\nfor param in model.roi_heads.parameters():\n    param.requires_grad = True;\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.01,momentum=0.9, weight_decay=0.0005)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)","bccf22cf":"def train_one_epoch(model, optimizer, data_loader, device):\n    model.train()\n\n    total_losses = []\n    for images, targets in tqdm.tqdm(data_loader, position=0, leave=True):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        if math.isfinite(loss_value):\n            total_losses.append(loss_value)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n\n    print(f\"Train: {np.mean(np.array(total_losses))}\")","67d9a7b4":"def evaluate(model, optimizer, data_loader, device):\n    total_losses = []\n    for images, targets in tqdm.tqdm(data_loader, position=0, leave=True):\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        optimizer.zero_grad()\n        if math.isfinite(loss_value):\n            total_losses.append(loss_value)\n\n    print(f\"Validation: {np.mean(np.array(total_losses))}\")","c1621e32":"model.to(device)\nfor epoch in range(3):\n    train_one_epoch(model, optimizer, data_loader, device)\n    evaluate(model, optimizer, data_loader_val, device=device)\n\ntorch.save(model.state_dict(), f'first_model.pth')","fff89877":"test_dataset = DetectionDatasetTest('data', train_transforms)\ndata_loader_test = torch.utils.data.DataLoader(\n    test_dataset, batch_size=batch_size, shuffle=False, num_workers=0,\n    pin_memory=True,\n    collate_fn=collate_fn, drop_last=False)\n\nmodel.eval()\nthresh = 0.8\n\nresult = {}\n\nfor images, targets in tqdm.tqdm(data_loader_test, position=0, leave=True):\n    images = list(image.to(device) for image in images)\n    preds = model(images)\n\n    for j in range(len(preds)):\n        file = targets[j]['file']\n        boxes = []\n\n        prediction = preds[j];\n        for i in range(len(prediction['boxes'])):\n            x_min, y_min, x_max, y_max = map(int, prediction['boxes'][i].tolist())\n            label = int(prediction['labels'][i].cpu())\n            score = float(prediction['scores'][i].cpu())\n            if score > thresh:\n                boxes.append([x_min, y_min, x_max, y_max])\n        result[file] = boxes\n\njs = json.dumps(result)\nwith open(f\"first_answer.json\", \"w\") as f:\n    f.write(js)","cfab4839":"def collate_fn_difsize(batch):\n    images, seqs, seq_lens, texts = [], [], [], []\n    for sample in batch:\n        images.append(torch.from_numpy(sample[\"image\"]).permute(2, 0, 1).float())\n        seqs.extend(sample[\"seq\"])\n        seq_lens.append(sample[\"seq_len\"])\n        texts.append(sample[\"text\"])\n    images = torch.stack(images)\n    seqs = torch.Tensor(seqs).int()\n    seq_lens = torch.Tensor(seq_lens).int()\n    batch = {\"image\": images, \"seq\": seqs, \"seq_len\": seq_lens, \"text\": texts}\n    return batch\n\nclass Resize(object):\n    def __init__(self, size=(320, 64)):\n        self.size = size\n\n    def __call__(self, item):\n        item['image'] = cv2.resize(item['image'], self.size, interpolation=cv2.INTER_AREA)\n        return item\n\nclass RecognitionDataset(Dataset):\n    def __init__(self, root, alphabet=\"0123456789ABEKMHOPCTYX\", transforms=None, split=\"train\", splitSize=0.9):\n        super(RecognitionDataset, self).__init__()\n\n        self.alphabet = alphabet\n        self.root = root\n        with open(os.path.join(root, \"train.json\"), 'r') as f:\n            data = json.load(f)\n\n        data = [x for x in data if x['file'] != \"train\/25632.bmp\"]\n\n        self.data_dict = []\n        for item in data:\n            for number in item['nums']:\n                r = {\n                    \"box\": number[\"box\"],\n                    \"text\": number[\"text\"],\n                    \"file\": item[\"file\"]\n                }\n                self.data_dict.append(r)\n\n        sz = round(splitSize * len(self.data_dict))\n        if split == \"train\":\n            self.data_dict = self.data_dict[:sz]\n        elif split == \"val\":\n            self.data_dict = self.data_dict[sz:]\n\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.data_dict)\n\n    def __getitem__(self, item):\n        val = self.data_dict[item]\n\n        box = np.array(val[\"box\"])\n        xmin = box[:,0].min()\n        xmax = box[:,0].max()\n        ymin = box[:,1].min()\n        ymax = box[:,1].max()\n\n        xmin = max(xmin, 0)\n        ymin = max(ymin, 0)\n\n        img_path = os.path.join(self.root, val[\"file\"])\n        # print(img_path, xmin, xmax, ymin, ymax)\n        image = cv2.imread(img_path).astype(np.float32) \/ 255.\n        image = image[ymin:ymax+1, xmin:xmax+1]\n\n        text = val[\"text\"]\n        seq = self.text_to_seq(text)\n        seq_len = len(seq)\n        output = dict(image=image, seq=seq, seq_len=seq_len, text=text)\n        if self.transforms is not None:\n            output = self.transforms(output)\n        return output\n\n    def text_to_seq(self, text):\n        seq = [self.alphabet.find(c) + 1 for c in text]\n        return seq\n\nclass FeatureExtractor(Module):\n\n    def __init__(self, input_size=(64, 320), output_len=20):\n        super(FeatureExtractor, self).__init__()\n\n        h, w = input_size\n        resnet = getattr(models, 'resnet18')(pretrained=True)\n        self.cnn = Sequential(*list(resnet.children())[:-2])\n\n        self.pool = AvgPool2d(kernel_size=(h \/\/ 32, 1))\n        self.proj = Conv2d(w \/\/ 32, output_len, kernel_size=1)\n\n        self.num_output_features = self.cnn[-1][-1].bn2.num_features\n\n    def apply_projection(self, x):\n        x = x.permute(0, 3, 2, 1).contiguous()\n        x = self.proj(x)\n        x = x.permute(0, 2, 3, 1).contiguous()\n        return x\n\n    def forward(self, x):\n        features = self.cnn(x)\n        features = self.pool(features)\n        features = self.apply_projection(features)\n\n        return features\n\nclass SequencePredictor(Module):\n\n    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.1, bidirectional=False):\n        super(SequencePredictor, self).__init__()\n\n        self.num_classes = num_classes\n        self.rnn = GRU(input_size=input_size,\n                       hidden_size=hidden_size,\n                       num_layers=num_layers,\n                       dropout=dropout,\n                       bidirectional=bidirectional)\n\n        fc_in = hidden_size if not bidirectional else 2 * hidden_size\n        self.fc = Linear(in_features=fc_in,\n                         out_features=num_classes)\n\n    def _init_hidden_(self, batch_size):\n        num_directions = 2 if self.rnn.bidirectional else 1\n        return torch.zeros(self.rnn.num_layers * num_directions, batch_size, self.rnn.hidden_size)\n\n    def _prepare_features_(self, x):\n        x = x.squeeze(1)\n        x = x.permute(2, 0, 1)\n        return x\n\n    def forward(self, x):\n        x = self._prepare_features_(x)\n\n        batch_size = x.size(1)\n        h_0 = self._init_hidden_(batch_size)\n        h_0 = h_0.to(x.device)\n        x, h = self.rnn(x, h_0)\n\n        x = self.fc(x)\n        return x\n\n\nclass CRNN(Module):\n\n    def __init__(self, alphabet=\"0123456789ABEKMHOPCTYX\",\n                 cnn_input_size=(64, 320), cnn_output_len=20,\n                 rnn_hidden_size=128, rnn_num_layers=2, rnn_dropout=0.1, rnn_bidirectional=False):\n        super(CRNN, self).__init__()\n        self.alphabet = alphabet\n        self.features_extractor = FeatureExtractor(input_size=cnn_input_size, output_len=cnn_output_len)\n        print(self.features_extractor.num_output_features)\n        self.sequence_predictor = SequencePredictor(input_size=self.features_extractor.num_output_features,\n                                                    hidden_size=rnn_hidden_size, num_layers=rnn_num_layers,\n                                                    num_classes=len(alphabet) + 1, dropout=rnn_dropout,\n                                                    bidirectional=rnn_bidirectional)\n\n    def forward(self, x):\n        features = self.features_extractor(x)\n        sequence = self.sequence_predictor(features)\n        return sequence\n\ndef pred_to_string(pred, abc = \"0123456789ABEKMHOPCTYX\"):\n    seq = []\n    for i in range(len(pred)):\n        label = np.argmax(pred[i])\n        seq.append(label - 1)\n    out = []\n    for i in range(len(seq)):\n        if len(out) == 0:\n            if seq[i] != -1:\n                out.append(seq[i])\n        else:\n            if seq[i] != -1 and seq[i] != seq[i - 1]:\n                out.append(seq[i])\n    out = ''.join([abc[c] for c in out])\n    return out\n\ndef decode(pred, abc = \"0123456789ABEKMHOPCTYX\"):\n    pred = pred.permute(1, 0, 2).cpu().data.numpy()\n    outputs = []\n    for i in range(len(pred)):\n        outputs.append(pred_to_string(pred[i], abc))\n    return outputs","1f17116e":"class RecognitionDatasetTest(Dataset):\n    def __init__(self, root, alphabet=\"0123456789ABEKMHOPCTYX\", transforms=None):\n        super(RecognitionDatasetTest, self).__init__()\n\n        self.alphabet = alphabet\n        self.root = root\n        with open(\"first_answer.json\", 'r') as f:\n            data = json.load(f)\n\n        self.data_dict = []\n        for file, boxes in data.items():\n            for box in boxes:\n                r = {\n                    \"box\": box,\n                    \"file\": file\n                }\n                self.data_dict.append(r)\n\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.data_dict)\n\n    def __getitem__(self, item):\n        val = self.data_dict[item]\n\n        box = np.array(val[\"box\"])\n        xmin = box[0]\n        xmax = box[2]\n        ymin = box[1]\n        ymax = box[3]\n\n        xmin = max(xmin, 0)\n        ymin = max(ymin, 0)\n\n        img_path = os.path.join(self.root, val[\"file\"])\n        image = cv2.imread(img_path).astype(np.float32) \/ 255.\n        image = image[ymin:ymax+1, xmin:xmax+1]\n\n        seq = []\n        seq_len = 0\n        output = dict(image=image, seq=seq, seq_len=seq_len, text=val[\"file\"])\n        if self.transforms is not None:\n            output = self.transforms(output)\n        return output","cbf580ea":"crnn = CRNN()\nnum_epochs = 10\nbatch_size = 512\nnum_workers = 4\n\noptimizer = torch.optim.Adam(crnn.parameters(), lr=3e-4, amsgrad=True, weight_decay=1e-4)\n\ntrain_dataset = RecognitionDataset(\"data\", transforms=Resize())\nval_dataset = RecognitionDataset(\"data\", split=\"val\", transforms=Resize())\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset,\n                                               batch_size=batch_size, shuffle=True, num_workers=num_workers,\n                                               pin_memory=True,\n                                               drop_last=True, collate_fn=collate_fn_difsize)\nval_dataloader = torch.utils.data.DataLoader(val_dataset,\n                                             batch_size=batch_size, shuffle=False, num_workers=num_workers,\n                                             pin_memory=True,\n                                             drop_last=False, collate_fn=collate_fn_difsize)\ncrnn.to(device);\n","1ac09f33":"for i, epoch in enumerate(range(num_epochs)):\n    epoch_losses = []\n    crnn.train()\n    for j, b in enumerate(tqdm.tqdm(train_dataloader, total=len(train_dataloader))):\n        images = b[\"image\"].to(device)\n        seqs_gt = b[\"seq\"]\n        seq_lens_gt = b[\"seq_len\"]\n\n        seqs_pred = crnn(images).cpu()\n        log_probs = log_softmax(seqs_pred, dim=2)\n        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n\n        loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n                        targets=seqs_gt,  # N, S or sum(target_lengths)\n                        input_lengths=seq_lens_pred,  # N\n                        target_lengths=seq_lens_gt)  # N\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        epoch_losses.append(loss.item())\n    print(\"Train \", np.mean(epoch_losses))\n    crnn.eval()\n    val_losses = []\n    for i, b in enumerate(tqdm.tqdm(val_dataloader, total=len(val_dataloader))):\n        images = b[\"image\"].to(device)\n        seqs_gt = b[\"seq\"]\n        seq_lens_gt = b[\"seq_len\"]\n\n        with torch.no_grad():\n            seqs_pred = crnn(images).cpu()\n        log_probs = log_softmax(seqs_pred, dim=2)\n        seq_lens_pred = torch.Tensor([seqs_pred.size(0)] * seqs_pred.size(1)).int()\n\n        loss = ctc_loss(log_probs=log_probs,  # (T, N, C)\n                        targets=seqs_gt,  # N, S or sum(target_lengths)\n                        input_lengths=seq_lens_pred,  # N\n                        target_lengths=seq_lens_gt)  # N\n\n        val_losses.append(loss.item())\n\n    print(\"Eval\", np.mean(val_losses))\n\ntorch.save(crnn.state_dict(), f'second_model.pth')","5033ae56":"crnn.eval()\ntest_dataset = RecognitionDatasetTest(\"data\", transforms=Resize())\n\ntest_dataloader = torch.utils.data.DataLoader(test_dataset,\n                                              batch_size=batch_size, shuffle=False, num_workers=num_workers,\n                                              pin_memory=True,\n                                              drop_last=False, collate_fn=collate_fn_difsize)\n\nresult = {}\nfor i, b in enumerate(tqdm.tqdm(test_dataloader, total=len(test_dataloader))):\n    images = b[\"image\"].to(device)\n    preds = crnn(images.to(device)).cpu().detach()\n    texts_pred = decode(preds, crnn.alphabet)\n\n    for i in range(len(texts_pred)):\n        file = b[\"text\"][i]\n        pred = texts_pred[i]\n\n        if file not in result:\n            result[file] = []\n        result[file].append(pred)","57434173":"line_count = 0;\nsubmit = {file : ' '.join(text) for file, text in result.items()}\n\nwith open(os.path.join(\"data\", \"submission.csv\"), 'r') as csv_file:\n    csv_reader = csv.reader(csv_file, delimiter=',')\n    for row in csv_reader:\n        if line_count == 0:\n            line_count += 1\n        else:\n            if row[0] not in result:\n                submit[row[0]] = ''\n\nwith open('my_submit.csv', 'w') as f:\n    f.write(f\"file_name,plates_string\\n\")\n    for key in submit.keys():\n        f.write(f\"{key},{submit[key]}\\n\")\n","b322c398":"**CRNN**","18804e51":"**Car plates number recognition**\n\nThe code consists of two neural networks.\n\n1. FasterRCNN with Resnet50+FPN backbone to find plates bounding boxes on image\n2. CRNN for number recognition\n\nPublic leaderbord score: 1.68164\n","dce59946":"**FasterRCNN**"}}