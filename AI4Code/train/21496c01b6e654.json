{"cell_type":{"b0f25ed4":"code","e29f4993":"code","8410cbad":"code","40d2e22c":"code","38a55496":"code","bdcba25b":"code","1e21667e":"code","c0fcbb22":"code","ad08323c":"code","f510d20b":"code","4afce631":"code","2c42a332":"code","e3065a2f":"code","0f52823a":"code","48944619":"code","05b751b2":"code","28306b62":"code","23ed7031":"code","d248883b":"code","2f54319a":"code","27a4fa88":"code","5a813d53":"code","bcd0e662":"code","a7796928":"code","ed93fa1e":"code","d11eded6":"code","ac8fce11":"code","3cb2765f":"code","88872137":"code","eaf2ec90":"code","89389735":"code","47d6bd69":"code","9bbbb3a5":"code","808a2104":"code","95eca6e3":"code","9ac93a3e":"code","f4afc050":"code","f423539f":"code","9153ceae":"code","3f58302c":"code","44190e0d":"code","c0bd2172":"code","0585fde3":"code","97510801":"code","9cc77681":"code","aaf1143d":"code","3eb9a879":"code","15b7470c":"code","0887ff1a":"code","9bea7be7":"code","53502d73":"code","b5afb4e3":"code","b42a55c0":"code","b7aea0d6":"code","6c12e784":"code","217c44ea":"code","f96a7e1d":"code","5cb6f006":"code","99339903":"code","0230be04":"code","a73815f5":"code","1d13b335":"code","53800c44":"markdown","56f44388":"markdown","ba6c1560":"markdown","8260dc20":"markdown","904fe794":"markdown","7cc75a31":"markdown","15b8f9d9":"markdown","a4a5000f":"markdown","a23f44c3":"markdown","0f4e0e55":"markdown","2e9b37e2":"markdown","9408e94d":"markdown","48112325":"markdown","8935861e":"markdown","6a4cf41f":"markdown","9b3575df":"markdown","126fc989":"markdown","3352bb19":"markdown","c537b0d4":"markdown","881f7ec0":"markdown","4d8d7ee1":"markdown","c9b108cc":"markdown"},"source":{"b0f25ed4":"#supress warning \nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\n","e29f4993":"data = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")","8410cbad":"data.head()","40d2e22c":"data.info()","38a55496":"#remove last columns, also we don't need id\ndata.drop(data.columns[len(data.columns)-1], axis=1, inplace=True)\ndata.drop('id', axis=1, inplace=True)","bdcba25b":"data.diagnosis.value_counts()","1e21667e":"sns.set_style('whitegrid')\ndata.diagnosis.value_counts().plot(kind='bar',color=[\"lightblue\", \"salmon\"])","c0fcbb22":"categorical_val=[]\ncontinuous_val=[]\nfor c in data.columns:\n    #print('==================')\n    #print(f\"{c}:{data[c].unique()}\")\n    if len(data[c].unique()) <= 10:\n        categorical_val.append(c)\n    else:\n        continuous_val.append(c)","ad08323c":"print(categorical_val)\nprint(continuous_val)","f510d20b":"plt.figure(figsize=(20,50))\nfor i, column in enumerate(continuous_val,1):\n    plt.subplot(10,3,i)\n    sns.distplot(data[data['diagnosis']=='M'][column],rug=False,label=\"M\")\n    sns.distplot(data[data['diagnosis']=='B'][column],rug=False,label='B')\n    plt.xlabel(column)\n    plt.legend()","4afce631":"df = data.replace({'diagnosis':{\"M\":1,\"B\":0}})","2c42a332":"df.head()","e3065a2f":"corr_matrix=df.corr()\nfig, ax = plt.subplots(figsize=(15,15))\nax = sns.heatmap(corr_matrix, annot=True, linewidths=0.5, fmt='.2f',cmap='YlGnBu')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom+0.5, top-0.5)","0f52823a":"col_drop = ['perimeter_mean','radius_mean','compactness_mean',\n            'concave points_mean','radius_se','perimeter_se',\n            'radius_worst','perimeter_worst','compactness_worst',\n            'concave points_worst','compactness_se','concave points_se',\n            'texture_worst','area_worst','concavity_worst']\ndf2 = df.drop(col_drop,axis=1)","48944619":"df2.head()","05b751b2":"fig, ax = plt.subplots(figsize=(15,15))\nax = sns.heatmap(df2.corr(), annot=True, linewidths=0.5, fmt='.2f',cmap='YlGnBu')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom+0.5, top-0.5)","28306b62":"x = df2.drop('diagnosis',axis=1)","23ed7031":"x.shape","d248883b":"#Calculate VIF \n#from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n#vif = pd.DataFrame()\n#vif[\"features\"] = x.columns\n#vif[\"VIF Factor\"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n","2f54319a":"#vif","27a4fa88":"#while vif[vif['VIF Factor'] > 10]['VIF Factor'].any():    \n#    remove = vif.sort_values('VIF Factor',ascending=0)['features'][1] \n    #print(remove)\n    #print(continuous_val)\n#    x.drop(remove,axis=1,inplace=True)\n#    vif = pd.DataFrame()\n#    vif[\"features\"] = x.columns\n#    vif[\"VIF Factor\"] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n#    print(vif)\n#    print('======================')\n    ","5a813d53":"df2.drop('diagnosis',axis=1).corrwith(df.diagnosis).plot(kind='bar',grid=True,figsize=(12,8),\n                                                       title='Correlation with diagnosis')","bcd0e662":"#there is no categorical variable other than our dependent variables, so we don't have to creat dummy variables for our models.\ncategorical_val","a7796928":"#store variable names \ncol_sc = list(df2.columns)\ncol_sc.remove('diagnosis')","ed93fa1e":"col_sc","d11eded6":"#scale our data\n#from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\ndf2[col_sc] = sc.fit_transform(df2[col_sc])","ac8fce11":"df2.head()","3cb2765f":"#from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n\ndef score(m, x_train, y_train, x_test, y_test, train=True):\n    if train:\n        pred=m.predict(x_train)\n        print('Train Result:\\n')\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred)*100:.2f}%\")\n        print(f\"Precision Score: {precision_score(y_train, pred)*100:.2f}%\")\n        print(f\"Recall Score: {recall_score(y_train, pred)*100:.2f}%\")\n        print(f\"F1 score: {f1_score(y_train, pred)*100:.2f}%\")\n        print(f\"Confusion Matrix:\\n {confusion_matrix(y_train, pred)}\")\n    elif train == False:\n        pred=m.predict(x_test)\n        print('Test Result:\\n')\n        print(f\"Accuracy Score: {accuracy_score(y_test, pred)*100:.2f}%\")\n        print(f\"Precision Score: {precision_score(y_test, pred)*100:.2f}%\")\n        print(f\"Recall Score: {recall_score(y_test, pred)*100:.2f}%\")\n        print(f\"F1 score: {f1_score(y_test, pred)*100:.2f}%\")\n        print(f\"Confusion Matrix:\\n {confusion_matrix(y_test, pred)}\")\n            \n    ","88872137":"#from sklearn.model_selection import train_test_split\n\nx = df2.drop('diagnosis',axis=1)\ny = df2['diagnosis']\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.3,random_state=42)","eaf2ec90":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg = logreg.fit(x_train, y_train)","89389735":"score(logreg, x_train, y_train, x_test, y_test, train=True)","47d6bd69":"score(logreg, x_train, y_train, x_test, y_test, train=False)","9bbbb3a5":"#C represents the strength of the regularization. higher values of C correspond to less regularization\nC = [1, .5, .25, .1, .05, .025, .01, .005, .0025] \nl1_metrics = np.zeros((len(C), 5)) \nl1_metrics[:,0] = C\n\nfor index in range(0, len(C)):\n    logreg = LogisticRegression(penalty='l1', C=C[index], solver='liblinear') \n    logreg = logreg.fit(x_train, y_train)\n    pred_test_Y = logreg.predict(x_test)\n    l1_metrics[index,1] = np.count_nonzero(logreg.coef_) \n    l1_metrics[index,2] = accuracy_score(y_test, pred_test_Y) \n    l1_metrics[index,3] = precision_score(y_test, pred_test_Y) \n    l1_metrics[index,4] = recall_score(y_test, pred_test_Y)\n    \ncol_names = ['C','Non-Zero Coeffs','Accuracy','Precision','Recall'] \nprint(pd.DataFrame(l1_metrics, columns=col_names))","808a2104":"logreg_t = LogisticRegression(penalty='l1', C=0.25, solver='liblinear')\nlogreg_t = logreg_t.fit(x_train,y_train)","95eca6e3":"score(logreg_t, x_train, y_train, x_test, y_test, train=True)","9ac93a3e":"score(logreg_t, x_train, y_train, x_test, y_test, train=False)","f4afc050":"from sklearn import tree\n\ntree1 = tree.DecisionTreeClassifier()\ntree1 = tree1.fit(x_train, y_train)","f423539f":"score(tree1, x_train, y_train, x_test, y_test, train=True)","9153ceae":"score(tree1, x_train, y_train, x_test, y_test, train=False)","3f58302c":"#decide the tree depth!\ndepth_list = list(range(2,15))\ndepth_tuning = np.zeros((len(depth_list), 4)) \ndepth_tuning[:,0] = depth_list\n\nfor index in range(len(depth_list)):\n    mytree = tree.DecisionTreeClassifier(max_depth=depth_list[index]) \n    mytree = mytree.fit(x_train, y_train)\n    pred_test_Y = mytree.predict(x_test)\n    depth_tuning[index,1] = accuracy_score(y_test, pred_test_Y) \n    depth_tuning[index,2] = precision_score(y_test, pred_test_Y) \n    depth_tuning[index,3] = recall_score(y_test, pred_test_Y)\n    \ncol_names = ['Max_Depth','Accuracy','Precision','Recall'] \nprint(pd.DataFrame(depth_tuning, columns=col_names))","44190e0d":"tree2 = tree.DecisionTreeClassifier(max_depth=3)\ntree2 = tree2.fit(x_train,y_train)","c0bd2172":"score(tree2, x_train, y_train, x_test, y_test, train=True)","0585fde3":"score(tree2, x_train, y_train, x_test, y_test, train=False)","97510801":"import graphviz\nexported = tree.export_graphviz( decision_tree=tree2,\n                                out_file=None,\n                                feature_names=x.columns,\n                                precision=1,\n                                class_names=['B','M'], \n                                filled = True)\ngraph = graphviz.Source(exported) \ndisplay(graph)","9cc77681":"from sklearn.ensemble import RandomForestClassifier\n\nforest = RandomForestClassifier(n_estimators=1000, random_state= 42)\nforest = forest.fit(x_train,y_train)","aaf1143d":"score(forest, x_train, y_train, x_test, y_test, train=True)","3eb9a879":"score(forest, x_train, y_train, x_test, y_test, train=False)","15b7470c":"from sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators, 'max_features': max_features,\n               'max_depth': max_depth, 'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}","0887ff1a":"random_grid","9bea7be7":"forest2 = RandomForestClassifier(random_state=42)\n\n#Random search of parameters, using 3 fold cross validation, search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = forest2, param_distributions=random_grid,\n                              n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n\nrf_random.fit(x_train,y_train)\n","53502d73":"rf_random.best_params_","b5afb4e3":"forest3 = RandomForestClassifier(bootstrap=True,\n                                 max_depth=20, \n                                 max_features='sqrt', \n                                 min_samples_leaf=2, \n                                 min_samples_split=2,\n                                 n_estimators=1200)\nforest3 = forest3.fit(x_train, y_train)","b42a55c0":"score(forest3, x_train, y_train, x_test, y_test, train=True)","b7aea0d6":"score(forest3, x_train, y_train, x_test, y_test, train=False)","6c12e784":"from sklearn.svm import SVC\n\nsvm = SVC()\nsvm = svm.fit(x_train,y_train)","217c44ea":"score(svm, x_train, y_train, x_test, y_test, train=True)","f96a7e1d":"score(svm, x_train, y_train, x_test, y_test, train=False)","5cb6f006":"from sklearn.model_selection import GridSearchCV\n\nsvm_model = SVC()\n\nparams = {\"C\":(0.1, 0.5, 1, 2, 5, 10, 20), \n          \"gamma\":(0.001, 0.01, 0.1, 0.25, 0.5, 0.75, 1), \n          \"kernel\":('poly', 'rbf')}\n\nsvm_grid = GridSearchCV(svm_model, params, n_jobs=-1, cv=5, verbose=1, scoring=\"accuracy\")\nsvm_grid.fit(x_train, y_train)","99339903":"svm_grid.best_params_","0230be04":"svm2 = SVC(C=2, gamma=0.01, kernel='rbf')\nsvm2 = svm2.fit(x_train, y_train)","a73815f5":"score(svm2, x_train, y_train, x_test, y_test, train=True)","1d13b335":"score(svm2, x_train, y_train, x_test, y_test, train=False)","53800c44":"The result looks pretty great. How about we tuning our model to prevent over-fitting issue and make the model become more general to unseen samples?","56f44388":"# Exploratory Data Analysis (EDA)","ba6c1560":"Some columns are correlated with each other. For example, area_mean, perimeter_mean and area_mean basically are same things. Thus, we can just keep 1 column to avoid collinearity. For other simulate columns, we'll do the same thing.","8260dc20":"Next, we can plot the tree!","904fe794":"We can found malignant and benign tumors show different distribution in some columns:  \nradius_mean: malignant tumors has lager radius mean.  \nperimeter_mean: malignant tumors has lager perimeter mean.  \narea_mean: malignant tumors has lager area mean.\ncompactness_mean: malignant tumors has lager compactness mean.  \nconcavity_mean: malignant tumors has lager concavity_mean.  \nconcavity_points_mean: malignant tumors has lager concavity_points_mean.  \n","7cc75a31":"Seems like an over-fitting issue. Again, let's try pruning the tree.","15b8f9d9":"Max depth = 3 seems a good choice!","a4a5000f":"In this kernel, we try to select the useful parameters by conducting visualization analysis. We also check the correlation matrix to avoid collinearity. After that, we use logistic, decision tree, random forest and SVM models for prediction, we even tune all this model to prevent over-fitting issue. Comparing the outcome, the logistic model gives the most precise prediction for our test data. \n\n\nThanks for your time!  \nIf this kernel is helpful, please upvote and write comments below to let me know. It would be such a great motivation for me:)","a23f44c3":"# Conclusion","0f4e0e55":"Great! This model got slightly better in test sample than the original one. ","2e9b37e2":"The important parameters details in SVM model can be founded in here: https:\/\/medium.com\/all-things-ai\/in-depth-parameter-tuning-for-svc-758215394769 ","9408e94d":"## M4: SVM ","48112325":"## M3: Ramdom Forest ","8935861e":"We finally choose C=0.25 because it got best performance with fewer parameters.","6a4cf41f":"Next, go with tuning! The article of random forest here provides details in hyperparameter tuning: https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74","9b3575df":"# Applying Machine Learning Algorithms","126fc989":"We see lots of features are highly correlated with diagnosis. ","3352bb19":"Attribute Information:\n\n1) ID number  \n2) Diagnosis (M = malignant, B = benign)\n\n\nTen real-valued features are computed for each cell nucleus:  \n\na) radius (mean of distances from center to points on the perimeter)  \nb) texture (standard deviation of gray-scale values)  \nc) perimeter  \nd) area  \ne) smoothness (local variation in radius lengths)  \nf) compactness (perimeter^2 \/ area - 1.0)  \ng) concavity (severity of concave portions of the contour)  \nh) concave points (number of concave portions of the contour)  \ni) symmetry  \nj) fractal dimension (\"coastline approximation\" - 1)  \n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.","c537b0d4":"## M1: Logistic Regression","881f7ec0":"## M2: Decision Tree","4d8d7ee1":"# Model Preparation","c9b108cc":"In this kernel, we will conduct EDA to select important variables for breast cancer prediction and apply several models(logistic\/ decision tree\/ random forest\/ SVM) to find out the best models. Enjoy! "}}