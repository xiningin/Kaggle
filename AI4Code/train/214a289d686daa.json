{"cell_type":{"d3fd61c2":"code","d39ceb6d":"code","ac6c95d0":"code","2b7d5716":"code","ad597fe9":"code","fb5203cc":"code","1da0d9bd":"code","d43978b8":"code","6ec98f8d":"code","3457ae50":"code","3a28d814":"code","0c8045e8":"code","64158c24":"code","b963f44b":"code","71525fe4":"code","d57efec3":"code","e95a1c7d":"code","aef04a22":"code","6c912d8c":"code","f4886a80":"code","a0bdbdae":"code","f30ca9c3":"code","5e364cec":"code","228f3366":"code","1dec7250":"code","373d134c":"code","daeb55c8":"code","10281fda":"code","4c4b2f44":"code","8437e4e2":"code","8aceda1b":"code","25fc338b":"code","b06b811c":"code","e1d8fb80":"code","c04fdcc8":"code","ccc122eb":"code","0f4ecd36":"code","274e4587":"code","29f63c77":"code","e5fb7976":"code","6d1dc657":"code","40a377d3":"code","de4a9000":"code","cb531887":"code","32c71f77":"code","2d837521":"code","f96cc5ce":"code","a294ab62":"code","e5e77bc4":"code","16b4d110":"code","f40ccf0d":"code","ae56d404":"code","83663a11":"code","9d62d5b1":"code","4c8574ce":"markdown","11f07289":"markdown","b1aba693":"markdown","a58f177d":"markdown","5c5fbec8":"markdown","3e69566a":"markdown","27eafa8f":"markdown","7c72175f":"markdown","165d1f2a":"markdown","caab08c9":"markdown","d276e022":"markdown","777da439":"markdown","dfbce88e":"markdown","00405c73":"markdown","8f3f3bd3":"markdown","1d8bed30":"markdown","0269190c":"markdown","e78423d9":"markdown","ee3aef7a":"markdown","751ebd06":"markdown","3a1d4b0f":"markdown","65691aa6":"markdown","c6fbce19":"markdown","2027a29e":"markdown","4026185f":"markdown","93661aa8":"markdown","9e87fa29":"markdown","8c4327ec":"markdown","bbd6d9d7":"markdown","e297810c":"markdown","14379be3":"markdown","a578884b":"markdown","4d7b4679":"markdown","231f4246":"markdown","9434b76e":"markdown","3f1ced5d":"markdown","7a5d3e56":"markdown","70583269":"markdown","100791cc":"markdown","82f3e325":"markdown","4686794b":"markdown"},"source":{"d3fd61c2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom imblearn.over_sampling import SMOTE # used to over-sample imbalanced dataset\nfrom sklearn.feature_extraction.text import TfidfVectorizer # converts sentences into vectors\nfrom sklearn.naive_bayes import MultinomialNB # Naive Bayes model for discrete scores\nfrom sklearn.linear_model import Ridge # Ridge regression model\nimport re # regular expression libary\nfrom bs4 import BeautifulSoup # used to interpret websites\nfrom tqdm.auto import tqdm # progress bar\nimport optuna # finds best parameters\nfrom sklearn.model_selection import train_test_split # used to split data into training and validation sets\nfrom sklearn.model_selection import StratifiedKFold # used to cross-validate the data\n\n\n# ignore SettingWithCopyWarning\nimport warnings\nfrom pandas.core.common import SettingWithCopyWarning\nwarnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d39ceb6d":"# set seed for randomness\nrseed=201","ac6c95d0":"import imblearn","2b7d5716":"pre_train_df1 = pd.read_csv(\n    \"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\n\n# naive bayes target\npre_train_df1['y_nb'] = (\n    pre_train_df1[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n    .sum(axis=1) > 0 ).astype(int)","ad597fe9":"# Create a score that measure how much toxic is a comment\ntoxicity_weights = {'obscene': 0.02, 'toxic': 0.05, 'threat': 0.27, \n            'insult': 0.11, 'severe_toxic': 0.28, 'identity_hate': 0.27}\n\nfor cat in toxicity_weights:\n    pre_train_df1[cat] = pre_train_df1[cat] * toxicity_weights[cat]\n    \npre_train_df1['y_rr'] = pre_train_df1[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1)","fb5203cc":"# organize the dataframe to only include the text and targets\npre_train_df1 = pre_train_df1[['comment_text', 'y_nb','y_rr']].rename(\n    columns={'comment_text': 'text'})\npre_train_df1.sample(5)","1da0d9bd":"pre_train_df2 = pd.read_csv(\n    \"\/kaggle\/input\/ruddit-jigsaw-dataset\/Dataset\/ruddit_with_text.csv\")\n\n# rename the columns containing the text data and score\npre_train_df2 = pre_train_df2[['txt', 'offensiveness_score']].rename(columns={'txt': 'text',\n                                                          'offensiveness_score':'score'})\n\n\n# label all comments with an offensiveness score greater than 0 as 1's, otherwise 0's\npre_train_df2['y_nb'] = (pre_train_df2['score'] > 0 ).astype(int)\n\n\n","d43978b8":"pre_train_df2['y_rr'] = pre_train_df2['score'].copy()\npre_train_df2.loc[pre_train_df2['y_rr']<0,'y_rr']=0","6ec98f8d":"# drop the score column containing the offensiveness values\npre_train_df2.drop(['score'], axis=1, inplace=True)\npre_train_df2.sample(5)","3457ae50":"if 1==0:\n    pre_train_df3 = pd.read_csv(\n        \"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/jigsaw-unintended-bias-train.csv\")\n    print(len(pre_train_df3))\n    pre_train_df3['y_nb'] = (\n        pre_train_df3[['toxic', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack']] >= .5 ).any(axis=1).astype(int)","3a28d814":"if 1==0:\n    # Create a score that measure how much toxic is a comment\n    toxicity_weights = {'obscene': 0.02, 'toxic': 0.05, 'threat': 0.27, \n                'insult': 0.11, 'severe_toxicity': 0.28, 'identity_attack': 0.27}\n\n    for cat in toxicity_weights:\n        pre_train_df3[cat] = pre_train_df3[cat] * toxicity_weights[cat]\n\n    pre_train_df3['y_rr'] = pre_train_df3[['toxic', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack']].max(axis=1)\n\n\n    pre_train_df3.loc[pre_train_df3['y_nb']>0,['comment_text','toxic', 'severe_toxicity', 'obscene', 'threat', 'insult', 'identity_attack','y_nb']].sample(5)","0c8045e8":"if 1==0:\n    pre_train_df3 = pre_train_df3[['comment_text', 'y_nb','y_rr']].rename(\n        columns={'comment_text': 'text'})","64158c24":"print (\"The first training dataset has %i rows.\" % len(pre_train_df1))\nprint (\"The first training dataset has %i toxic comments.\" % (pre_train_df1['y_nb'] == 1).sum())\nprint (\"The first training dataset has %i non-toxic comments.\" % (pre_train_df1['y_nb'] == 0).sum())\n\nprint (\"The second training dataset has %i rows.\" % len(pre_train_df2))\nprint (\"The second training dataset has %i toxic comments.\" % (pre_train_df2['y_nb'] == 1).sum())\nprint (\"The second training dataset has %i non-toxic comments.\" % (pre_train_df2['y_nb'] == 0).sum())\n\nif 1==0:\n    print (\"The third training dataset has %i rows.\" % len(pre_train_df3))\n    print (\"The third training dataset has %i toxic comments.\" % (pre_train_df3['y_nb'] == 1).sum())\n    print (\"The third training dataset has %i non-toxic comments.\" % (pre_train_df3['y_nb'] == 0).sum())","b963f44b":"train_df_list=[]\nfor cur_train_df in [pre_train_df1,pre_train_df2]:\n    # undersample to the number of toxic comments (undersample_n)\n    undersample_n = (cur_train_df['y_nb'] == 1).sum()\n\n    # perform undersample\n    cur_train_df_y0_undersample = cur_train_df.loc[cur_train_df['y_nb'] == 0,:].sample(\n        n=undersample_n, random_state=rseed)\n\n    # generate new training dataframe given undersampled commets\n    cur_train_df = pd.concat([cur_train_df.loc[cur_train_df['y_nb'] == 1,:], cur_train_df_y0_undersample])\n\n    train_df_list.append(cur_train_df)\n    print(cur_train_df['y_nb'].value_counts())\ntrain_df = pd.concat(train_df_list)","71525fe4":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text","d57efec3":"import nltk\nfrom nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ndef clean(data, col):\n    \n    data[col] = data[col].str.replace('https?:\/\/\\S+|www\\.\\S+', ' social medium ', regex=True)      \n        \n    data[col] = data[col].str.lower()\n    data[col] = data[col].str.replace(\"4\", \"a\") \n    data[col] = data[col].str.replace(\"2\", \"l\")\n    data[col] = data[col].str.replace(\"5\", \"s\") \n    data[col] = data[col].str.replace(\"1\", \"i\") \n    data[col] = data[col].str.replace(\"!\", \"i\") \n    data[col] = data[col].str.replace(\"|\", \"i\", regex=False) \n    data[col] = data[col].str.replace(\"0\", \"o\") \n    data[col] = data[col].str.replace(\"l3\", \"b\") \n    data[col] = data[col].str.replace(\"7\", \"t\") \n    data[col] = data[col].str.replace(\"7\", \"+\") \n    data[col] = data[col].str.replace(\"8\", \"ate\") \n    data[col] = data[col].str.replace(\"3\", \"e\") \n    data[col] = data[col].str.replace(\"9\", \"g\")\n    data[col] = data[col].str.replace(\"6\", \"g\")\n    data[col] = data[col].str.replace(\"@\", \"a\")\n    data[col] = data[col].str.replace(\"$\", \"s\", regex=False)\n    data[col] = data[col].str.replace(\"#ofc\", \" of fuckin course \")\n    data[col] = data[col].str.replace(\"fggt\", \" faggot \")\n    data[col] = data[col].str.replace(\"your\", \" your \")\n    data[col] = data[col].str.replace(\"self\", \" self \")\n    data[col] = data[col].str.replace(\"cuntbag\", \" cunt bag \")\n    data[col] = data[col].str.replace(\"fartchina\", \" fart china \")    \n    data[col] = data[col].str.replace(\"youi\", \" you i \")\n    data[col] = data[col].str.replace(\"cunti\", \" cunt i \")\n    data[col] = data[col].str.replace(\"sucki\", \" suck i \")\n    data[col] = data[col].str.replace(\"pagedelete\", \" page delete \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"i'm\", \" i am \")\n    data[col] = data[col].str.replace(\"offuck\", \" of fuck \")\n    data[col] = data[col].str.replace(\"centraliststupid\", \" central ist stupid \")\n    data[col] = data[col].str.replace(\"hitleri\", \" hitler i \")\n    data[col] = data[col].str.replace(\"i've\", \" i have \")\n    data[col] = data[col].str.replace(\"i'll\", \" sick \")\n    data[col] = data[col].str.replace(\"fuck\", \" fuck \")\n    data[col] = data[col].str.replace(\"f u c k\", \" fuck \")\n    data[col] = data[col].str.replace(\"shit\", \" shit \")\n    data[col] = data[col].str.replace(\"bunksteve\", \" bunk steve \")\n    data[col] = data[col].str.replace('wikipedia', ' social medium ')\n    data[col] = data[col].str.replace(\"faggot\", \" faggot \")\n    data[col] = data[col].str.replace(\"delanoy\", \" delanoy \")\n    data[col] = data[col].str.replace(\"jewish\", \" jewish \")\n    data[col] = data[col].str.replace(\"sexsex\", \" sex \")\n    data[col] = data[col].str.replace(\"allii\", \" all ii \")\n    data[col] = data[col].str.replace(\"i'd\", \" i had \")\n    data[col] = data[col].str.replace(\"'s\", \" is \")\n    data[col] = data[col].str.replace(\"youbollocks\", \" you bollocks \")\n    data[col] = data[col].str.replace(\"dick\", \" dick \")\n    data[col] = data[col].str.replace(\"cuntsi\", \" cuntsi \")\n    data[col] = data[col].str.replace(\"mothjer\", \" mother \")\n    data[col] = data[col].str.replace(\"cuntfranks\", \" cunt \")\n    data[col] = data[col].str.replace(\"ullmann\", \" jewish \")\n    data[col] = data[col].str.replace(\"mr.\", \" mister \", regex=False)\n    data[col] = data[col].str.replace(\"aidsaids\", \" aids \")\n    data[col] = data[col].str.replace(\"njgw\", \" nigger \")\n    data[col] = data[col].str.replace(\"wiki\", \" social medium \")\n    data[col] = data[col].str.replace(\"administrator\", \" admin \")\n    data[col] = data[col].str.replace(\"gamaliel\", \" jewish \")\n    data[col] = data[col].str.replace(\"rvv\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"admins\", \" admin \")\n    data[col] = data[col].str.replace(\"pensnsnniensnsn\", \" penis \")\n    data[col] = data[col].str.replace(\"pneis\", \" penis \")\n    data[col] = data[col].str.replace(\"pennnis\", \" penis \")\n    data[col] = data[col].str.replace(\"pov.\", \" point of view \", regex=False)\n    data[col] = data[col].str.replace(\"vandalising\", \" vandalism \")\n    data[col] = data[col].str.replace(\"cock\", \" dick \")\n    data[col] = data[col].str.replace(\"asshole\", \" asshole \")\n    data[col] = data[col].str.replace(\"youi\", \" you \")\n    data[col] = data[col].str.replace(\"afd\", \" all fucking day \")\n    data[col] = data[col].str.replace(\"sockpuppets\", \" sockpuppetry \")\n    data[col] = data[col].str.replace(\"iiprick\", \" iprick \")\n    data[col] = data[col].str.replace(\"penisi\", \" penis \")\n    data[col] = data[col].str.replace(\"warrior\", \" warrior \")\n    data[col] = data[col].str.replace(\"loil\", \" laughing out insanely loud \")\n    data[col] = data[col].str.replace(\"vandalise\", \" vanadalism \")\n    data[col] = data[col].str.replace(\"helli\", \" helli \")\n    data[col] = data[col].str.replace(\"lunchablesi\", \" lunchablesi \")\n    data[col] = data[col].str.replace(\"special\", \" special \")\n    data[col] = data[col].str.replace(\"ilol\", \" i lol \")\n    data[col] = data[col].str.replace(r'\\b[uU]\\b', 'you', regex=True)\n    data[col] = data[col].str.replace(r\"what's\", \"what is \")\n    data[col] = data[col].str.replace(r\"\\'s\", \" is \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=False)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \")\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \")\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=False)\n    data[col] = data[col].str.replace('\\s+', ' ', regex=True)  # will remove more than one whitespace character\n#     text = re.sub(r'\\b([^\\W\\d_]+)(\\s+\\1)+\\b', r'\\1', re.sub(r'\\W+', ' ', text).strip(), flags=re.I)  # remove repeating words coming immediately one after another\n    data[col] = data[col].str.replace(r'(.)\\1+', r'\\1\\1', regex=True) # 2 or more characters are replaced by 2 characters\n#     text = re.sub(r'((\\b\\w+\\b.{1,2}\\w+\\b)+).+\\1', r'\\1', text, flags = re.I)\n    data[col] = data[col].str.replace(\"[:|\u2663|'|\u00a7|\u2660|*|\/|?|=|%|&|-|#|\u2022|~|^|>|<|\u25ba|_]\", '', regex=True)\n    \n    \n    data[col] = data[col].str.replace(r\"what's\", \"what is \")    \n    data[col] = data[col].str.replace(r\"\\'ve\", \" have \", regex=False)\n    data[col] = data[col].str.replace(r\"can't\", \"cannot \")\n    data[col] = data[col].str.replace(r\"n't\", \" not \", regex=False)\n    data[col] = data[col].str.replace(r\"i'm\", \"i am \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'re\", \" are \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'d\", \" would \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'ll\", \" will \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'scuse\", \" excuse \", regex=False)\n    data[col] = data[col].str.replace(r\"\\'s\", \" \", regex=False)\n\n    # Clean some punctutations\n    data[col] = data[col].str.replace('\\n', ' \\n ')\n    data[col] = data[col].str.replace(r'([a-zA-Z]+)([\/!?.])([a-zA-Z]+)',r'\\1 \\2 \\3', regex=True)\n    # Replace repeating characters more than 3 times to length of 3\n    data[col] = data[col].str.replace(r'([*!?\\'])\\1\\1{2,}',r'\\1\\1\\1', regex=True)    \n    # Add space around repeating characters\n    data[col] = data[col].str.replace(r'([*!?\\']+)',r' \\1 ', regex=True)    \n    # patterns with repeating characters \n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1{2,}\\b',r'\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'([a-zA-Z])\\1\\1{2,}\\B',r'\\1\\1\\1', regex=True)\n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    data[col] = data[col].str.replace(r'[ ]{2,}',' ', regex=True).str.strip()   \n    data[col] = data[col].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n    tqdm.pandas()\n    data[col] = data[col].progress_apply(text_cleaning)\n    return data","e95a1c7d":"# Test clean function\ntest_clean_df = pd.DataFrame({\"text\":\n                              [\"heyy\\n\\nkkdsfj\",\n                               \"hi   how\/are\/U ???\",\n                               \"hey?????\",\n                               \"noooo!!!!!!!!!   comeone !! \",\n                              \"cooooooooool     brooooooooooo  coool brooo\",\n                              \"naaaahhhhhhh\",\"'re been cool\"]})\ndisplay(test_clean_df)\nclean(test_clean_df,'text')","aef04a22":"# create TF-IDF object\nvec = TfidfVectorizer()\n\n# fit the TF-IDF object to the CLEAN training comments\ntrain_df = clean(train_df,'text')\nX = vec.fit_transform(train_df['text'])\nX","6c912d8c":"X","f4886a80":"nb_model = MultinomialNB() \nnb_model.fit(X, train_df['y_nb'])\n\nrr_model = Ridge(alpha=0.5)\nrr_model.fit(X, train_df['y_rr'])","a0bdbdae":"val_df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\nval_df.head()","f30ca9c3":"val_df = clean(val_df,'less_toxic')\nval_df = clean(val_df,'more_toxic')\nX_less_toxic = vec.transform(val_df['less_toxic'])\nX_more_toxic = vec.transform(val_df['more_toxic'])","5e364cec":"nb_p1 = nb_model.predict_proba(X_less_toxic)\nnb_p2 = nb_model.predict_proba(X_more_toxic)\nrr_p1 = rr_model.predict(X_less_toxic)\nrr_p2 = rr_model.predict(X_more_toxic)\n","228f3366":"# Validation Accuracy\n#naive bayes\nnb_acc = (nb_p1[:, 1] < nb_p2[:, 1]).mean()\n# ridge regression\nrr_acc = (rr_p1 < rr_p2).mean()\n# both\nmean_acc = (((nb_p1[:, 1]+rr_p1)\/2) < ((nb_p2[:, 1]+rr_p2)\/2)).mean()\n\nprint (\"Naive Bayes Validation: %f\" % nb_acc)\nprint (\"Ridge Regression Validation: %f\" % rr_acc)\nprint (\"Mean Validation: %f\" % mean_acc)","1dec7250":"sub_df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nsub_df = clean(sub_df,'text')\nX_test = vec.transform(sub_df['text'])\nnb_p3 = nb_model.predict_proba(X_test)\nrr_p3 = rr_model.predict(X_test)","373d134c":"sub_df['score'] =  ((nb_p3[:, 1]+rr_p3)\/2)","daeb55c8":"# uncomment below to generate original submission file\n#sub_df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","10281fda":"less_toxic_score_df=pd.DataFrame()\nless_toxic_score_df[\"text\"] = val_df[\"less_toxic\"].copy()\nless_toxic_score_df[\"y_rr\"] = 0\nmore_toxic_score_df=pd.DataFrame()\nmore_toxic_score_df[\"text\"] = val_df[\"more_toxic\"].copy()\nmore_toxic_score_df[\"y_rr\"] = 1\ntoxic_score_df = pd.concat([less_toxic_score_df, more_toxic_score_df], ignore_index=True)","4c4b2f44":"toxic_score_df.head()","8437e4e2":"# sort the comments (not necessary)\ntoxic_score_df = toxic_score_df.sort_values(by = 'text').copy()\n\n# use groupby function to group the comments\nval_score_df = toxic_score_df.groupby('text')['y_rr'].mean().reset_index()\n\n# set `y` to \"0\" if the average is less than or equal to 0.5 and set `y` to \"1\" otherwise\nval_score_df['y_nb'] = (val_score_df['y_rr'] > .5).astype(int)\n\n# drop the `score` column\n#val_score_df.drop(['score'], axis=1, inplace=True)","8aceda1b":"print (\"The training data has %i rows.\" % len(val_score_df))\nprint (\"The training data has %i toxic comments.\" % (val_score_df['y_nb'] == 1).sum())\nprint (\"The training data has %i non-toxic comments.\" % (val_score_df['y_nb'] == 0).sum())","25fc338b":"# we chose a random test size, this number may be optimized for improved results\ntrain_df2, val_df2 = train_test_split(val_score_df, test_size=0.70, random_state = rseed)","b06b811c":"print (\"The training data has %i rows.\" % len(train_df2))\nprint (\"The training data has %i toxic comments.\" % (train_df2['y_nb'] == 1).sum())\nprint (\"The training data has %i non-toxic comments.\" % (train_df2['y_nb'] == 0).sum())","e1d8fb80":"vec2 = TfidfVectorizer()\n\n# fit the TF-IDF object to the training comments\ntrain_df2 = clean(train_df2,'text')\nX2 = vec2.fit_transform(train_df2['text'])\nX2","c04fdcc8":"nb_model2 = MultinomialNB()\nnb_model2.fit(X2, train_df2['y_nb'])\n\nrr_model2 = Ridge(alpha=0.7)\nrr_model2.fit(X2, train_df2['y_rr'])","ccc122eb":"val_df2 = clean(val_df2,'text')\nX_predict = vec2.transform(val_df2['text'])\nnb_predictions = nb_model2.predict_proba(X_predict)\nrr_predictions = rr_model2.predict(X_predict)","0f4ecd36":"val_df2.loc[:,'nb_pred']=nb_predictions[:, 1]\nval_df2.loc[:,'rr_pred']=rr_predictions\n# limit values to be between 0 to 1\nval_df2.loc[val_df2[\"rr_pred\"]<0,\"rr_pred\"] = 0\nval_df2.loc[val_df2[\"rr_pred\"]>1,\"rr_pred\"] = 1\nval_df2.loc[:,'pred_score']=(val_df2[\"nb_pred\"] + val_df2[\"rr_pred\"])\/2\n\n","274e4587":"# get a dataframe where each comment has a predicted\n# and true score, we split the comments in half\n# evenly and then predict which score is more toxic\ndef marginRankLossF(df, predScoreCol, trueScoreCol):\n    df = df.copy().reset_index()\n    total_rows=len(df)\n    if total_rows % 2 == 1:\n        total_rows = total_rows - 1\n    half_rows = total_rows \/\/ 2\n    #print(total_rows)\n    input1_df = df.loc[range(0,half_rows),:].copy().reset_index()\n    #print(input1_df.head())\n    input2_df = df.loc[range(half_rows,total_rows),:].copy().reset_index()\n    #print(input2_df.head())\n    input1 = input1_df[predScoreCol]\n    #print(input1.head())\n    input2 = input2_df[predScoreCol]\n    #print(input2.head())\n    target = (input1_df[trueScoreCol] > input2_df[trueScoreCol]).astype(int) \n    target = target.replace(0, -1)\n    #print((input1_df[predScoreCol] > input2_df[predScoreCol]).astype(int).replace(0,-1).head())\n    a= np.multiply((-1*target),(input1-input2))\n    output = np.maximum(np.zeros(len(a)),a)\n    return(output)","29f63c77":"print(marginRankLossF(val_df2, 'pred_score', 'y_rr').describe())","e5fb7976":"sub_df = sub_df.rename(\n    columns={'score': 'score1'})\nsub_df = clean(sub_df,'text')\nX_test2 = vec2.transform(sub_df['text'])\n\nnb_sub_pred2 = nb_model2.predict_proba(X_test2)\nrr_sub_pred2 = rr_model2.predict(X_test2)\n\n# limit prediction values to be between 0 to 1\nrr_sub_pred2[rr_sub_pred2 < 0] = 0\nrr_sub_pred2[rr_sub_pred2 > 1] = 1","6d1dc657":"sub_df['score2'] = (nb_sub_pred2[:, 1] + rr_sub_pred2)\/2","40a377d3":"sub_df.loc[:,'score'] = sub_df.loc[:,['score1','score2']].astype(float).mean(axis=1)","de4a9000":"#sub_df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","cb531887":"train_df3 = pd.concat([train_df,train_df2])","32c71f77":"# create TF-IDF object\nvec3 = TfidfVectorizer()\n\n# fit the TF-IDF object to the CLEAN training comments\ntrain_df3 = clean(train_df3,'text')\nX3 = vec3.fit_transform(train_df3['text'])\nX3","2d837521":"nb_model3 = MultinomialNB()\nnb_model3.fit(X3, train_df3['y_nb'])\n\nrr_model3 = Ridge(alpha=0.7)\nrr_model3.fit(X3, train_df3['y_rr'])","f96cc5ce":"val_df2 = clean(val_df2,'text')\nX_predict = vec3.transform(val_df2['text'])\nnb_predictions = nb_model3.predict_proba(X_predict)\nrr_predictions = rr_model3.predict(X_predict)","a294ab62":"val_df2.loc[:,'nb_pred']=nb_predictions[:, 1]\nval_df2.loc[:,'rr_pred']=rr_predictions\n# limit values to be between 0 to 1\nval_df2.loc[val_df2[\"rr_pred\"]<0,\"rr_pred\"] = 0\nval_df2.loc[val_df2[\"rr_pred\"]>1,\"rr_pred\"] = 1\nval_df2.loc[:,'pred_score']=(val_df2[\"nb_pred\"] + val_df2[\"rr_pred\"])\/2\n\nval_df2.loc[:,'error'] = 1-abs(val_df2['y_rr']-val_df2['pred_score'])","e5e77bc4":"val_df2['error'].describe()","16b4d110":"val_df2['error'].mean()","f40ccf0d":"sub_df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nsub_df = clean(sub_df,'text')\nX_test3 = vec3.transform(sub_df['text'])\n\nnb_sub_pred3 = nb_model3.predict_proba(X_test3)\nrr_sub_pred3 = rr_model3.predict(X_test3)\n\n# limit prediction values to be between 0 to 1\nrr_sub_pred3[rr_sub_pred3 < 0] = 0\nrr_sub_pred3[rr_sub_pred3 > 1] = 1\n\nsub_df['score'] = (nb_sub_pred3[:, 1] + rr_sub_pred3)\/2","ae56d404":"#sub_df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","83663a11":"\ndef objective(trial):\n    \n    # get dataset1\n    pre_train_df1 = pd.read_csv(\n        \"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\n\n    # naive bayes target for dataset1\n    pre_train_df1['y_nb'] = (\n        pre_train_df1[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n        .sum(axis=1) > 0 ).astype(int)\n\n    \n    obscene_w1 = trial.suggest_float('obscene_w1', 0, 1)\n    toxic_w1 = trial.suggest_float('toxic_w1', 0, 1)\n    threat_w1 = trial.suggest_float('threat_w1', 1, 2)\n    insult_w1 = trial.suggest_float('insult_w1', 0, 1)\n    severe_toxic_w1 = trial.suggest_float('severe_toxic_w1', 1, 2)\n    identity_hate_w1 = trial.suggest_float('identity_hate_w1', 1, 2)\n    \n    \n    # Create a score that measure how much toxic is a comment\n    toxicity_weights = {'obscene': obscene_w1, 'toxic': toxic_w1, 'threat': threat_w1, \n                'insult': insult_w1, 'severe_toxic': severe_toxic_w1, 'identity_hate': identity_hate_w1}\n\n    for cat in toxicity_weights:\n        pre_train_df1[cat] = pre_train_df1[cat] * toxicity_weights[cat]\n\n    # ridge regression target for dataset1\n    pre_train_df1['y_rr'] = pre_train_df1[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1)\n\n    # organize the dataframe to only include the text and targets\n    pre_train_df1 = pre_train_df1[['comment_text', 'y_nb','y_rr']].rename(\n        columns={'comment_text': 'text'})\n    \n    \n    # get training data from validation set\n    error_results=[]\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=rseed)\n    for train_df2_index, val_df2_index in skf.split(val_score_df['text'], val_score_df['y_nb']):\n        \n        train_df2 = val_score_df.loc[train_df2_index,:]\n        val_df2 = val_score_df.loc[val_df2_index,:]\n    \n        train_df_list=[train_df2]\n        for cur_train_df in [pre_train_df1,pre_train_df2]:\n            # undersample to the number of toxic comments (undersample_n)\n            undersample_n = (cur_train_df['y_nb'] == 1).sum()\n\n            # perform undersample\n            cur_train_df_y0_undersample = cur_train_df.loc[cur_train_df['y_nb'] == 0,:].sample(\n                n=undersample_n, random_state=rseed)\n\n            # generate new training dataframe given undersampled commets\n            cur_train_df = pd.concat([cur_train_df.loc[cur_train_df['y_nb'] == 1,:], cur_train_df_y0_undersample])\n\n            train_df_list.append(cur_train_df)\n        train_df3 = pd.concat(train_df_list)\n\n        # create TF-IDF object\n        vec3 = TfidfVectorizer()\n\n        # fit the TF-IDF object to the CLEAN training comments\n        train_df3 = clean(train_df3,'text')\n        X3 = vec3.fit_transform(train_df3['text'])\n        X3\n\n        nb_model3 = MultinomialNB()\n        nb_model3.fit(X3, train_df3['y_nb'])\n\n        alpha_v = trial.suggest_float('alpha_v', 0, 2)\n\n        rr_model3 = Ridge(alpha=alpha_v)\n        rr_model3.fit(X3, train_df3['y_rr'])\n\n        val_df2 = clean(val_df2,'text')\n        X_predict = vec3.transform(val_df2['text'])\n        nb_predictions = nb_model3.predict_proba(X_predict)\n        rr_predictions = rr_model3.predict(X_predict)\n\n        val_df2.loc[:,'nb_pred']=nb_predictions[:, 1]\n        val_df2.loc[:,'rr_pred']=rr_predictions\n        # limit values to be between 0 to 1\n        val_df2.loc[val_df2[\"rr_pred\"]<0,\"rr_pred\"] = 0\n        val_df2.loc[val_df2[\"rr_pred\"]>1,\"rr_pred\"] = 1\n        val_df2.loc[:,'pred_score']=(val_df2[\"nb_pred\"] + val_df2[\"rr_pred\"])\/2\n\n        #val_df2.loc[:,'error'] = 1-abs(val_df2['y_rr']-val_df2['pred_score'])\n        #error_results.append(val_df2['error'].mean())\n        error_results.append(marginRankLossF(val_df2, 'pred_score', 'y_rr').mean())\n    print(error_results)\n    return (sum(error_results) \/ len(error_results))\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=20)\nbest_param_dict = study.best_params\nbest_param_dict","9d62d5b1":"# get dataset1\npre_train_df1 = pd.read_csv(\n    \"\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\n\n# naive bayes target\npre_train_df1['y_nb'] = (\n    pre_train_df1[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]\n    .sum(axis=1) > 0 ).astype(int)\n\n\n# Create a score that measure how much toxic is a comment\ntoxicity_weights = {'obscene': study.best_params['obscene_w1'],\n                    'toxic': study.best_params['toxic_w1'],\n                    'threat': study.best_params['threat_w1'], \n                    'insult': study.best_params['insult_w1'],\n                    'severe_toxic': study.best_params['severe_toxic_w1'],\n                    'identity_hate': study.best_params['identity_hate_w1']}\n\nfor cat in toxicity_weights:\n    pre_train_df1[cat] = pre_train_df1[cat] * toxicity_weights[cat]\n\npre_train_df1['y_rr'] = pre_train_df1[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1)\n\n# organize the dataframe to only include the text and targets\npre_train_df1 = pre_train_df1[['comment_text', 'y_nb','y_rr']].rename(\n    columns={'comment_text': 'text'})\n\n# this time use entire validation set for training the model\ntrain_df_list=[]\nfor cur_train_df in [pre_train_df1,pre_train_df2,val_score_df]:\n    # undersample to the number of toxic comments (undersample_n)\n    undersample_n = (cur_train_df['y_nb'] == 1).sum()\n\n    # perform undersample\n    cur_train_df_y0_undersample = cur_train_df.loc[cur_train_df['y_nb'] == 0,:].sample(\n        n=undersample_n, random_state=rseed)\n\n    # generate new training dataframe given undersampled commets\n    cur_train_df = pd.concat([cur_train_df.loc[cur_train_df['y_nb'] == 1,:], cur_train_df_y0_undersample])\n\n    train_df_list.append(cur_train_df)\ntrain_df3 = pd.concat(train_df_list)\n\n# create TF-IDF object\nvec3 = TfidfVectorizer()\n\n# fit the TF-IDF object to the CLEAN training comments\ntrain_df3 = clean(train_df3,'text')\nX3 = vec3.fit_transform(train_df3['text'])\nX3\n\nnb_model3 = MultinomialNB()\nnb_model3.fit(X3, train_df3['y_nb'])\n\nrr_model3 = Ridge(alpha=study.best_params['alpha_v'])\nrr_model3.fit(X3, train_df3['y_rr'])\n\n# predict submission comments\n\nsub_df = pd.read_csv(\"\/kaggle\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")\nsub_df = clean(sub_df,'text')\nX_test3 = vec3.transform(sub_df['text'])\n\nnb_sub_pred3 = nb_model3.predict_proba(X_test3)\nrr_sub_pred3 = rr_model3.predict(X_test3)\n\n# limit prediction values to be between 0 to 1\nrr_sub_pred3[rr_sub_pred3 < 0] = 0\nrr_sub_pred3[rr_sub_pred3 > 1] = 1\n\nsub_df['score'] = (nb_sub_pred3[:, 1] + rr_sub_pred3)\/2\n\nsub_df[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","4c8574ce":"### Submission with new model","11f07289":"For training ridge regression we set the target to zero if the offensiveness score is less than 0.","b1aba693":"add the predictions to the submission data frame","a58f177d":"## Use validation data from current competition to add to data from the original competition","5c5fbec8":"## Import Ruddit Jigsaw dataset\nFor training Naive Bayes we label each comment (1) toxic or 0 for not toxic. ","3e69566a":"## Import Jigsaw Unintended Bias challenge data (deprecated)\n\nI tried using this third dataset, but it didn't help the model\n\nFor training Naive Bayes, comments were labeled as toxic (y_nb=1) if any of the 'toxic', 'severe_toxicity', 'obscene', 'threat', 'insult', and 'identity_attack' values were equal to or greater than 0.5. ","27eafa8f":"Below I created a function to measure the expected margin rank loss given the validation comment scores.","7c72175f":"This dataset does not look seriously imbalanced so we can continue by splitting the full validation set into a smaller training and validation set.","165d1f2a":"## Submission","caab08c9":"### Fit Models\nWe fit the naive bayes and ridge regression models to the comments from this new training data","d276e022":"### Fit Models\nWe fit the naive bayes and ridge regression models to the comments from this new training data","777da439":"To validate the models we use the training data for this competition.","dfbce88e":"### Comments to vectors","00405c73":"get the average of the two models","8f3f3bd3":"# Optimize weights to train Jigsaw Toxic Comment Classification challenge data for Ridge Regression model","1d8bed30":"# Create training data from other jigsaw competitions\nTo train the data we need comments (X) as features and then a \"ground truth\" of the comment's toxicity (y). We use the Jigsaw Toxic Comment Classification challenge training data which labels each comment as either 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', or 'identity_hate' using 1's and 0's. Since in this competition these categories are all considered to be toxic, we transform this training data to list whether each comment is toxic or not.","0269190c":"The naive bayes model is run on the the comments labeled as \"less_toxic\" and \"more_toxic\" seperately. If a comment is more toxic it should have a higher value.","e78423d9":"For training ridge regression we must put weights on each of the categories to measure the toxicity of a comment.","ee3aef7a":"generate submission file","751ebd06":"## Fit Models\nWe fit the gaussian process regression model using the sklearn Ridge function to the comments from the training data based on whether they are labeled as toxic or not.","3a1d4b0f":"First we generate a table with two columns: 'text' and 'y_rr'. The 'y_rr' will equal *0* if the comment was considered less toxic and *1* if the comment was considered more toxic in the pairwise comparisons.","65691aa6":"Now I will reset the weights with the optimized values and generate a new submission.","c6fbce19":"For training Naive Bayes we label each comment (1) toxic or 0 for not toxic. ","2027a29e":"generate submission file","4026185f":"### Use validation data from current competition to create seperate model\nThen validate our results on the new validation data","93661aa8":"Group the y_rr values given to each comment_text and get the average of each distinct comment_text. Then set `y_nb` to \"0\" if the average is less than or equal to 0.5 and set `y` to \"1\" otherwise","9e87fa29":"## Comments to vectors","8c4327ec":"To validate the model we measure whether the \"more_toxic\" comments got higher values than the \"less_toxic\" comments","bbd6d9d7":"### Use validation data from current competition to create seperate model\nThen validate our results on the new validation data","e297810c":"below we use a clean function from ANDREJ MARINCHENKO","14379be3":"### Comments to vectors","a578884b":"# Background\nHere we take the average of two different models. Both models use outside data and a fraction of validation data for training. \n\nThe following code was inspired by the following\n* notebook by JULI\u00c1N PELLER: https:\/\/www.kaggle.com\/julian3833\/jigsaw-incredibly-simple-naive-bayes-0-768\n* notebook by ANDREJ MARINCHENKO: https:\/\/www.kaggle.com\/andrej0marinchenko\/jigsaw-ensemble-0-86\n* notebook by MANAV: https:\/\/www.kaggle.com\/manabendrarout\/pytorch-roberta-ranking-baseline-jrstc-infer\n\nSo far this notebook is split into three submissions:\n1. Using the current competition training datasets as validation data for running a Naive Bayes model\n2. Using the current competition training datasets as a seperate training data to generate two Naive Bayes models\n3. Using the current competition training datasets as additional training data to generate a single Naive Bayes models","4d7b4679":"### Submission with new model","231f4246":"## Import Jigsaw Toxic Comment Classification challenge data","9434b76e":"For training ridge regression we must put weights on each of the categories to measure the toxicity of a comment. After weighing these score we take the maximum value as the toxicity score.\n","3f1ced5d":"# Add more training data using current competition\nWe can split the 30108 rows of training data from the current competition for both training and validation. Unlike the comments from the first competition which are binary (1=toxic, 0=not) we can label the comments from the current competition fractions based on whether they were more toxic than the comparing comment more often than not.","7a5d3e56":"# Imports\nImport the following libraries and data from \n* Jigsaw Toxic Comment Classification challenge - predicts whether a comment was 'toxic' 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate' (1,0)\n* Ruddit Jigsaw dataset - scores between -1 (maximally supportive) and 1 (maximally offensive)\n* Jigsaw Unintended Bias in Toxicity Classification - predicts whether a comment was 'toxic' 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate' (1,0)\n* Jigsaw Toxic Severity rating - predicts how toxic a comment is compared to other comments","70583269":"The dataset is very unbalanced. Below is code to undersample the majority class. I tried over-sampling and it decreased the accuracy of the model.","100791cc":"below we use a clean function from MANAV","82f3e325":"## Imbalanced dataset\nBelow we run code to see if the comments have a relatively equal amount of toxic and non-toxic comments.","4686794b":"The \"predict_proba\" function generated a 2D array where the first dimension lists the probability the comment is not toxic and the second contains the probability that the comment is toxic."}}