{"cell_type":{"a96873b4":"code","67bb83f9":"code","16e08ad7":"code","d96969ee":"code","abe5d020":"code","5fef8b9a":"code","a570ed00":"code","4060df17":"code","1be9d89f":"code","7cdd6e6b":"code","321b0292":"code","8974036e":"code","443cdd8f":"code","e12cfeea":"code","db9c846d":"code","642a9406":"code","98acbd70":"code","29999238":"code","1b579944":"code","56d0584a":"code","ca7e1ee6":"code","7cf5d182":"code","87f5eb86":"code","73a4d3fd":"code","991b3631":"code","b134faad":"code","9df3b1fc":"code","1dd73549":"code","7ac2eebe":"code","615fe6af":"code","afb1a6e7":"code","70e18a5b":"code","0e623b67":"code","0b437b76":"code","abbdec3c":"code","c161cd64":"markdown","8c68c972":"markdown","6add4a3c":"markdown","ef2c9f69":"markdown","77463ded":"markdown","23c71053":"markdown","8becfa59":"markdown","c7911328":"markdown","fdecb11d":"markdown","9a3ea6a3":"markdown","bea51481":"markdown","d4c4c2ad":"markdown","140da4eb":"markdown","92c85b81":"markdown","0ba368af":"markdown","20a93f59":"markdown","ddceaa7b":"markdown"},"source":{"a96873b4":"# Import all the required libs\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Dropout, MaxPool2D, Flatten, BatchNormalization, Reshape \nfrom keras.utils.np_utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nreduce_lr = ReduceLROnPlateau(monitor= 'val_loss',\n                             factor= 0.2,\n                             patience= 2,\n                             min_lr = 0.00001)","67bb83f9":"# load data\ntrain = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","16e08ad7":"train.head()","d96969ee":"# prepare data\nX_train = train.drop(['label'], axis= 1).values.reshape(-1, 28, 28, 1)\ny_train = to_categorical(train['label'].values)\n\nX_test = test.values.reshape(-1, 28, 28, 1)","abe5d020":"print('shape of train set: ',X_train.shape)\nprint('shape of test set: ',X_test.shape)","5fef8b9a":"model = Sequential()\nmodel.add(Conv2D(32, kernel_size= 3, padding= 'same',\\\n                 activation= 'relu', input_shape= (28, 28, 1)))\nmodel.add(MaxPool2D(2))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation= 'relu'))\n#output\nmodel.add(Dense(10, activation= 'softmax'))\n\n# compile the model\nmodel.compile(loss= 'categorical_crossentropy', metrics= ['accuracy'],\\\n             optimizer= 'adam')\n\n# fit the model\nepochs = 10\nhistory = model.fit(X_train, y_train, callbacks= [reduce_lr],\\\n          validation_split= 0.2, epochs= epochs, batch_size= 80)\n\n# plot learning curves\nplt.plot(np.arange(1, epochs+ 1), history.history['val_accuracy'])\nplt.plot(np.arange(1, epochs+ 1), history.history['accuracy'])\nplt.ylim(0.97, 1)","a570ed00":"# Build CNN\nnum = 3\nmodel = [0]* 3\n\nfor i in range(num):\n    model[i] = Sequential()\n    model[i].add(Conv2D(32, kernel_size= 3, activation= 'relu',\\\n                       input_shape= (28, 28, 1), padding= 'same'))\n    model[i].add(MaxPool2D(2))\n    if i > 0:\n        model[i].add(Conv2D(32, kernel_size= 3, activation= 'relu', padding= 'same'))\n        model[i].add(MaxPool2D(2))\n    if i > 1:\n        model[i].add(Conv2D(32, kernel_size= 3, activation= 'relu', padding= 'same'))\n        model[i].add(MaxPool2D(2))\n    \n    model[i].add(Flatten())\n    model[i].add(Dense(256, activation= 'relu'))\n    model[i].add(Dense(10, activation= 'softmax'))\n    \n    # compile model\n    model[i].compile(loss= 'categorical_crossentropy', \n                    optimizer= 'adam',\n                    metrics= ['accuracy'])","4060df17":"# fit the models\n\nepochs= 15\nhistory = [0]*num\n\nfor j in range(num):\n    history[j] = model[j].fit(X_train, y_train, validation_split= 0.3,\\\n            epochs= epochs, callbacks= [reduce_lr],verbose=2, batch_size= 80)\n    print('Model {} trained'.format(j+1))","1be9d89f":"# plot learning curves\n\nfor i in range(num):\n    plt.plot(np.arange(1, epochs + 1), history[i].history['val_accuracy'], label= '{}_conv_Layer'.format(i+1))\nplt.xlabel('Epochs')\nplt.ylabel('Validation Accuracy')\nplt.title('Model comparison on validation set accuracy')\nplt.legend(loc= 'upper left')\nplt.ylim(0.96, 1)\nplt.show()","7cdd6e6b":"# Build CNN\nnum = 4\nmodel = [0]*num\n\nfor i in range(num):\n    model[i] = Sequential()\n    \n    model[i].add(Conv2D(2**(i+3), kernel_size= 3, activation= 'relu', input_shape= (28, 28, 1),\\\n                       padding= 'same'))\n    model[i].add(MaxPool2D(2))\n    \n    model[i].add(Conv2D(2**(i+4), kernel_size= 3, activation= 'relu', padding= 'same'))\n    model[i].add(MaxPool2D(2))\n    \n    model[i].add(Flatten())\n    model[i].add(Dense(256, activation= 'relu'))\n    model[i].add(Dense(10, activation= 'softmax'))\n    \n    # compile model\n    model[i].compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])","321b0292":"# fit the models\nhistory = [0]*num\nepochs= 15\n\nfor j in range(num):\n    history[j] = model[j].fit(X_train, y_train, validation_split= 0.3,\\\n            epochs= epochs, callbacks= [reduce_lr],verbose=0, batch_size= 80)\n    print('Model {} trained'.format(j+1))","8974036e":"# plot learning curves\nfor i in range(num):\n    plt.plot(np.arange(1, epochs + 1), history[i].history['val_accuracy'],\\\n             label= '{}_{}_CNN'.format(2**(i+3), 2**(i+4)))\nplt.xlabel('Epochs')\nplt.ylabel('Validation Accuracy')\nplt.title('Model comparison on validation set accuracy')\nplt.legend(loc= 'upper left')\nplt.ylim(0.96, 1)\nplt.show()","443cdd8f":"# Build CNN\nnum= 4\nmodel = [0]*num\n\nfor i in range(num):\n    model[i] = Sequential()\n    \n    model[i].add(Conv2D(32, kernel_size= 3 + i*2, activation= 'relu', input_shape= (28, 28, 1),\\\n                       padding= 'same'))\n    model[i].add(MaxPool2D(2))\n    \n    model[i].add(Conv2D(64, kernel_size= 3 + i*2, activation= 'relu', padding= 'same'))\n    model[i].add(MaxPool2D(2))\n    \n    model[i].add(Flatten())\n    model[i].add(Dense(256, activation= 'relu'))\n    model[i].add(Dense(10, activation= 'softmax'))\n                 \n    # Compile\n    model[i].compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n    ","e12cfeea":"# fit the data\nhistory = [0]*num\nepochs= 15\n\nfor j in range(num):\n    history[j] = model[j].fit(X_train, y_train, validation_split= 0.3,\\\n            epochs= epochs, callbacks= [reduce_lr],verbose=0, batch_size= 80)\n    print('Model {} trained'.format(j+1))","db9c846d":"# plot learing curves\nfor i in range(num):\n    plt.plot(np.arange(1, epochs + 1), history[i].history['val_accuracy'],\\\n             label= '{}x{}_kernel'.format(3 + i*2, 3 + i*2))\nplt.xlabel('Epochs')\nplt.ylabel('Validation Accuracy')\nplt.title('Model comparison on validation set accuracy')\nplt.legend(loc= 'upper left')\nplt.ylim(0.96, 1)\nplt.show()","642a9406":"# Build CNN\nnum = 3\nmodel = [0]*num\n\nfor i in range(num):\n    model[i] = Sequential()\n    \n    model[i].add(Conv2D(32, kernel_size= 3, activation= 'relu', input_shape= (28, 28, 1), padding= 'same'))\n    model[i].add(MaxPool2D(2))\n    \n    model[i].add(Conv2D(64, kernel_size= 3, activation= 'relu', padding= 'same'))\n    model[i].add(MaxPool2D(2))\n    \n    model[i].add(Flatten())\n    model[i].add(Dense(128, activation= 'relu'))\n    \n    if i > 0:\n        model[i].add(Dense(128, activation= 'relu'))\n    if i > 1:\n        model[i].add(Dense(128, activation= 'relu'))\n        \n    model[i].add(Dense(10, activation= 'softmax'))\n    \n    # compile\n    model[i].compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n    ","98acbd70":"# fit the data\nhistory = [0]*num\nepochs= 15\n\nfor j in range(num):\n    history[j] = model[j].fit(X_train, y_train, validation_split= 0.3, callbacks= [reduce_lr],\\\n                             epochs= epochs, batch_size= 80, verbose= 0)\n    print('Model {} is trained'.format(j+1))","29999238":"# plot learing curves\nfor i in range(num):\n    plt.plot(np.arange(1, epochs + 1), history[i].history['val_accuracy'],\\\n             label= '{}_Dense_layer'.format(i+1))\nplt.xlabel('Epochs')\nplt.ylabel('Validation Accuracy')\nplt.title('Model comparison on validation set accuracy')\nplt.legend(loc= 'upper left')\nplt.ylim(0.96, 1)\nplt.show()","1b579944":"# Build CNN\nnum = 4\nmodel = [0]*4\n\nfor i in range(num):\n    model[i] = Sequential()\n    \n    model[i].add(Conv2D(32, kernel_size= 3, activation= 'relu', input_shape= (28, 28, 1), padding= 'same'))\n    model[i].add(MaxPool2D(2))\n    \n    model[i].add(Conv2D(64, kernel_size= 3, activation= 'relu', padding= 'same'))\n    model[i].add(MaxPool2D(2))\n    \n    model[i].add(Flatten())\n    model[i].add(Dense(2**(i+5), activation= 'relu'))\n    model[i].add(Dense(10, activation= 'softmax'))\n    \n    # compile\n    model[i].compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n    ","56d0584a":"# fit the data\nhistory = [0]*num\nepochs= 15\n\nfor j in range(num):\n    history[j] = model[j].fit(X_train, y_train, validation_split= 0.3, callbacks= [reduce_lr],\\\n                             epochs= epochs, batch_size= 80, verbose= 0)\n    print('Model {} is trained'.format(j+1))","ca7e1ee6":"# plot learing curves\nfor i in range(num):\n    plt.plot(np.arange(1, epochs + 1), history[i].history['val_accuracy'],\\\n             label= '{}_Dense_layer'.format(2**(i+5)))\nplt.xlabel('Epochs')\nplt.ylabel('Validation Accuracy')\nplt.title('Model comparison on validation set accuracy')\nplt.legend(loc= 'upper left')\nplt.ylim(0.96, 1)\nplt.show()","7cf5d182":"# Build CNN\nnum = 7\nmodel = [0]*num\n\nfor i in range(num):\n    model[i] = Sequential()\n    \n    model[i].add(Conv2D(32, kernel_size= 3, activation= 'relu', input_shape= (28, 28, 1), padding= 'same'))\n    model[i].add(MaxPool2D(2))\n    model[i].add(Dropout(i*0.1))\n    \n    model[i].add(Conv2D(64, kernel_size= 3, activation= 'relu', padding= 'same'))\n    model[i].add(MaxPool2D(2))\n    model[i].add(Dropout(i*0.1))\n    \n    model[i].add(Flatten())\n    model[i].add(Dense(256, activation= 'relu'))\n    model[i].add(Dropout(i*0.1))\n    model[i].add(Dense(10, activation= 'softmax'))\n    \n    # compile\n    model[i].compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])","87f5eb86":"# fit the data\nhistory = [0]*num\nepochs= 20\n\nfor j in range(num):\n    history[j] = model[j].fit(X_train, y_train, validation_split= 0.3, callbacks= [reduce_lr],\\\n                             epochs= epochs, batch_size= 80, verbose= 0)\n    print('Model {} is trained'.format(j+1))","73a4d3fd":"# plot learning curve\nfor i in range(num):\n    plt.plot(np.arange(1, epochs + 1), history[i].history['val_accuracy'],\\\n             label= '{}%_Dropout'.format(i*10))\nplt.xlabel('Epochs')\nplt.ylabel('Validation Accuracy')\nplt.title('Model comparison on validation set accuracy')\nplt.legend(loc= 'upper left')\nplt.ylim(0.96, 1)\nplt.show()","991b3631":"# Build CNN\nnum = 5\nmodel= [0]*num\nhistory = [0]*num\nepochs= 20\n\n# Basic model\n\nmodel[0] = Sequential()\n    \nmodel[0].add(Conv2D(32, kernel_size= 3, activation= 'relu', input_shape= (28, 28, 1), padding= 'same'))\nmodel[0].add(MaxPool2D(2))\nmodel[0].add(Dropout(0.1))\n\nmodel[0].add(Conv2D(64, kernel_size= 3, activation= 'relu', padding= 'same'))\nmodel[0].add(MaxPool2D(2))\nmodel[0].add(Dropout(0.1))\n\nmodel[0].add(Flatten())\nmodel[0].add(Dense(256, activation= 'relu'))\nmodel[0].add(Dropout(0.1))\nmodel[0].add(Dense(10, activation= 'softmax'))\n\n# compile\nmodel[0].compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n\n\n# fit data\nhistory[0] = model[0].fit(X_train, y_train, validation_split= 0.3, callbacks= [reduce_lr],\\\n                             epochs= epochs, batch_size= 80, verbose= 2)","b134faad":"# Model with Data augmentation\nX_train2, X_val2, y_train2, y_val2 = train_test_split(X_train, y_train, test_size= 0.3)\n\ndatagen = ImageDataGenerator(featurewise_center= False,\n                             samplewise_center= False,\n                             featurewise_std_normalization= False,\n                             samplewise_std_normalization= False,\n                             zca_whitening= False,\n                             rotation_range= 10,\n                             zoom_range = 0.1,\n                             width_shift_range= 0.1,\n                             height_shift_range= 0.1,\n                             horizontal_flip= False,\n                             vertical_flip= False)\n\ndatagen.fit(X_train)","9df3b1fc":"model[1] = Sequential()\n    \nmodel[1].add(Conv2D(32, kernel_size= 3, activation= 'relu', input_shape= (28, 28, 1), padding= 'same'))\nmodel[1].add(MaxPool2D(2))\nmodel[1].add(Dropout(0.1))\n\nmodel[1].add(Conv2D(64, kernel_size= 3, activation= 'relu', padding= 'same'))\nmodel[1].add(MaxPool2D(2))\nmodel[1].add(Dropout(0.1))\n\nmodel[1].add(Flatten())\nmodel[1].add(Dense(256, activation= 'relu'))\nmodel[1].add(Dropout(0.1))\nmodel[1].add(Dense(10, activation= 'softmax'))\n\n# compile\nmodel[1].compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])\n\n# fit data\nhistory[1] = model[1].fit(datagen.flow(X_train2, y_train2, batch_size= 80),validation_data= (X_val2, y_val2),\\\n                         steps_per_epoch= X_train2.shape[0]\/\/80, epochs= epochs, callbacks= [reduce_lr],\\\n                         verbose= 2)","1dd73549":"# model with batch normalization \nmodel[2] = Sequential()\n    \nmodel[2].add(Conv2D(32, kernel_size= 3, activation= 'relu', input_shape= (28, 28, 1), padding= 'same'))\nmodel[2].add(MaxPool2D(2))\nmodel[2].add(BatchNormalization())\nmodel[2].add(Dropout(0.1))\n\nmodel[2].add(Conv2D(64, kernel_size= 3, activation= 'relu', padding= 'same'))\nmodel[2].add(MaxPool2D(2))\nmodel[2].add(BatchNormalization())\nmodel[2].add(Dropout(0.1))\n\nmodel[2].add(Flatten())\nmodel[2].add(Dense(256, activation= 'relu'))\nmodel[2].add(BatchNormalization())\nmodel[2].add(Dropout(0.1))\nmodel[2].add(Dense(10, activation= 'softmax'))\n\n# compile\nmodel[2].compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])","7ac2eebe":"# fit the data\nhistory[2] = model[2].fit(X_train, y_train, validation_split= 0.3,\\\n                         batch_size= 80, epochs= epochs, callbacks= [reduce_lr],\\\n                         verbose= 2)","615fe6af":"# model with no pooling layers\nmodel[3] = Sequential()\n    \nmodel[3].add(Conv2D(32, kernel_size= 3, activation= 'relu', input_shape= (28, 28, 1), padding= 'same'))\nmodel[3].add(Conv2D(32, kernel_size= 3, activation= 'relu', strides= 2, padding= 'same'))\nmodel[3].add(Dropout(0.1))\n\nmodel[3].add(Conv2D(64, kernel_size= 3, activation= 'relu', padding= 'same'))\nmodel[3].add(Conv2D(64, kernel_size= 3, activation= 'relu', strides= 2, padding= 'same'))\nmodel[3].add(Dropout(0.1))\n\nmodel[3].add(Flatten())\nmodel[3].add(Dense(256, activation= 'relu'))\nmodel[3].add(Dropout(0.1))\nmodel[3].add(Dense(10, activation= 'softmax'))\n\n# compile\nmodel[3].compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])","afb1a6e7":"# fit data\nhistory[3] = model[3].fit(X_train, y_train, validation_split= 0.3,\\\n                         batch_size= 80, epochs= epochs, callbacks= [reduce_lr],\\\n                         verbose= 2)","70e18a5b":"# model with all advance concepts\nmodel[4] = Sequential()\n    \nmodel[4].add(Conv2D(32, kernel_size= 3, activation= 'relu', input_shape= (28, 28, 1), padding= 'same'))\nmodel[4].add(BatchNormalization())\nmodel[4].add(Conv2D(32, kernel_size= 3, activation= 'relu', strides= 2, padding= 'same'))\nmodel[4].add(BatchNormalization())\nmodel[4].add(Dropout(0.1))\n\nmodel[4].add(Conv2D(64, kernel_size= 3, activation= 'relu', padding= 'same'))\nmodel[4].add(BatchNormalization())\nmodel[4].add(Conv2D(64, kernel_size= 3, activation= 'relu', strides= 2, padding= 'same'))\nmodel[4].add(BatchNormalization())\nmodel[4].add(Dropout(0.1))\n\nmodel[4].add(Flatten())\nmodel[4].add(Dense(256, activation= 'relu'))\nmodel[4].add(BatchNormalization())\nmodel[4].add(Dropout(0.1))\nmodel[4].add(Dense(10, activation= 'softmax'))\n\n# compile\nmodel[4].compile(loss= 'categorical_crossentropy', optimizer= 'adam', metrics= ['accuracy'])","0e623b67":"# fit augmented data\nhistory[4] = model[4].fit(datagen.flow(X_train2, y_train2, batch_size= 80),validation_data= (X_val2, y_val2),\\\n                         steps_per_epoch= X_train2.shape[0]\/\/80, epochs= epochs, callbacks= [reduce_lr],\\\n                         verbose= 2)","0b437b76":"# plot learning curves\nlabels = ['Basic model',\n         'DA',\n         'BN',\n         'Pool~Conv',\n         'Combined']\nfor i in range(num):\n    plt.plot(np.arange(1, epochs + 1), history[i].history['val_accuracy'],\\\n             label= labels[i])\nplt.xlabel('Epochs')\nplt.ylabel('Validation Accuracy')\nplt.title('Model comparison on validation set accuracy')\nplt.legend(loc= 'upper left')\nplt.ylim(0.96, 1)\nplt.show()","abbdec3c":"# predict with combined model\nepochs= 25\n\nmodel[4].fit_generator(datagen.flow(X_train,y_train, batch_size=64), epochs = epochs, \n    steps_per_epoch = X_train.shape[0]\/\/64, callbacks=[reduce_lr], verbose=0)\n\n# SUBMIT\nresults = model[4].predict(X_test)\nresults = np.argmax(results,axis = 1)\nresults = pd.Series(results,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\nsubmission.to_csv(\"MNIST_SUB.csv\",index=False)","c161cd64":"10% dropout is enough for our model","8c68c972":"## First Trial\n\nNow we'll start tweaking our base model to improve efficiency while maintaining minimal computational cost.\n\nFirst experiment is to add more convolutional layers to the network.\nFirst few conv layers good at identifying general features like edges. Later conv layers start differentiating more complex features in the image like object identification. \n\nI'll try models with two and three conv layers. Adding a forth layer will not be feasible as after 3rd conv layer the size of image is already very small(because of pooling).","6add4a3c":"## The foundation model\n\nI'll start with the most basic model with one Conv2d layer followed by a pooling layer and finally a dense layer.","ef2c9f69":"### Combined model performed significantly better so I'll go with it.","77463ded":"# CNN with MNIST Data\n\nIn this notebook I'll build a Convolutional neural network using keras framework. The optimal model is achieved after many trial and error which I'll depict here in step by step procedure. \n\nA very basic question is why one should go for CNN when images are concered?\n\nWhen I was starting with CNN this was the first question that came in mind. After I had enough experience with CNNs, I was finally able to answer this(ofcourse you can find thousands of explainations online too). CNNs are good in expressing the data rather than just start processing it. While using CNNs, we preserve the spatial information of pixels(we dont have to reshape input images) which let us focus on the correlation of adjacent pixels. This is motivated by the way human visualization work. \nThe processing part is done by the Dense layers we add after the convolutional layers.\n\nEnough talk, let's get to work.","23c71053":"256 neurons in the Dense layer.","8becfa59":"## Fifth Trial\n\nSize of Dense layers.\n\n**Cases**: 32, 64, 128, 256","c7911328":"CNN with 2 Convolutional layers performed better. \nCNN with 3 Conv layers performed bad because the number of parameters decreased significantly.","fdecb11d":"1 Dense layer is enough for our model. we'll not go for 3 layers as it does not result in any significant change in accuracy.","9a3ea6a3":"## Third Trial\n\nLet us see the kernel size effect on the network\n\n**Cases**:\n1. 3x3\n2. 5x5\n3. 7x7\n4. 9x9\n\nThese are the dimensions of kernel matrix","bea51481":"## Seventh Trial\n\nNow we'll try some advance concepts \n\n1. Data Augmentation\n2. Batch normalization\n3. Using Conv layers with stride=2 in place of Pooling layers","d4c4c2ad":"## Forth Trial\n\nNumber of Dense layers \n**Cases**: 1, 2, 3 Dense layers","140da4eb":"## Second Trial\n\nDifferent number of feature maps in each Conv layer greatly affects the model in terms of number of paramters. Feature maps usually increase in deeper Conv layers.\n\n**Cases**:\n1. 10, 20\n2. 16, 32\n3. 32, 64\n4. 64, 128\n\nThese are the number of convolutions in the two layers","92c85b81":"## Predict","0ba368af":"## Sixth Trial\n\nDropout in the network\n**Cases**:\n0%, 10%, 20%, 30%, 40%, 50%, 60%","20a93f59":"Model having 32 kernels in first Conv layer and 64 kernels in second Conv layer performed equally good as with 64, 128. Keeping in mind the computational cost, we'll go with 32, 64","ddceaa7b":"3x3 kernel performed better than any other size.  "}}