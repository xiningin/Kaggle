{"cell_type":{"1d157664":"code","997d2986":"code","bc14c8f1":"code","e57e7a4d":"code","d5033e16":"code","844ca6ba":"code","9e9c40c3":"code","1544531f":"code","2245e513":"code","1aedf33d":"code","76b12b4d":"code","4478839c":"code","6af8df12":"code","4a9ce6f9":"code","8a11cecd":"code","ce55701d":"code","000e27f5":"code","878eb7c2":"code","ba674599":"code","7be1fd3d":"code","67df9a41":"code","88dba682":"code","43c1c0d9":"code","4f7549ce":"code","88ac9cc7":"code","04fd13e8":"code","e3c1b7bb":"code","f72cc574":"code","2a6f172b":"code","1a430aa5":"markdown","bf3855ea":"markdown","7861ff54":"markdown","976fb20c":"markdown","5d32d1da":"markdown","932cce57":"markdown","e428650f":"markdown","ae571b6c":"markdown","c2859631":"markdown","c1f61ecc":"markdown","7793e5f4":"markdown"},"source":{"1d157664":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","997d2986":"data = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndata","bc14c8f1":"data.describe()","e57e7a4d":"data.info()","d5033e16":"data = data.drop([\"Unnamed: 32\"], axis=1)","844ca6ba":"data","9e9c40c3":"data_id = data[\"id\"]\ndata_id = pd.DataFrame(data_id, columns = [\"id\"])  #Saving IDs in a separate Dataframe\ndata.drop([\"id\"],axis=1, inplace=True)   #Dropping id from data","1544531f":"data[\"diagnosis\"].value_counts()","2245e513":"_, axes = plt.subplots(figsize=(6, 4))\nsns.boxplot(x=\"diagnosis\", y=\"radius_mean\", data=data, ax=axes)","1aedf33d":"_, axes = plt.subplots(figsize=(6, 4))\nsns.boxplot(x=\"diagnosis\", y=\"perimeter_mean\", data=data, ax=axes)","76b12b4d":"_, axes = plt.subplots(figsize=(6, 4))\nsns.boxplot(x=\"diagnosis\", y=\"area_mean\", data=data, ax=axes)","4478839c":"_, axes = plt.subplots(figsize=(6, 4))\nsns.boxplot(x=\"diagnosis\", y=\"smoothness_mean\", data=data, ax=axes)","6af8df12":"_, axes = plt.subplots(figsize=(6, 4))\nsns.boxplot(x=\"diagnosis\", y=\"concavity_mean\", data=data, ax=axes)","4a9ce6f9":"_, axes = plt.subplots(figsize=(6, 4))\nsns.boxplot(x=\"diagnosis\", y=\"symmetry_mean\", data=data, ax=axes)","8a11cecd":"_, axes = plt.subplots(figsize=(6, 4))\nsns.boxplot(x=\"diagnosis\", y=\"fractal_dimension_mean\", data=data, ax=axes)","ce55701d":"d = {\"B\":0, \"M\":1}\ndata[\"diagnosis\"] = data[\"diagnosis\"].map(d)\ndata","000e27f5":"corr_matrix = data.corr()\nplt.figure(figsize=(30,20))\nsns.heatmap(corr_matrix, annot=True);","878eb7c2":"correlated_columns = list({\n        \"perimeter_mean\",\n        \"radius_mean\",\n        \"perimeter_worst\",\n        \"radius_worst\",\n        \"perimeter_se\",\n        \"radius_se\"})\ndata = data.drop(columns=correlated_columns, axis=1)","ba674599":"data","7be1fd3d":"X = data.drop([\"diagnosis\"], axis=1)\ny = data[\"diagnosis\"]","67df9a41":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.18,random_state=32)\nprint(\"Length of training set = \", len(X_train))\nprint(\"Length of test set = \", len(X_test))","88dba682":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","43c1c0d9":"from sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel_knn = KNeighborsClassifier(n_neighbors=5)\nmodel_knn.fit(X_train, y_train)\ny_pred_knn = model_knn.predict(X_test)\naccuracy_knn = metrics.accuracy_score(y_pred_knn, y_test)\nscores = cross_val_score(model_knn, X_train, y_train, cv=5)\ncv_score_knn = scores.mean()\nprint(\"Cross-val-score: \", cv_score_knn)\nprint(\"Accuracy score: \", accuracy_knn)","4f7549ce":"from sklearn.tree import DecisionTreeClassifier\nmodel_dt = DecisionTreeClassifier(max_depth = 5, min_samples_split=3)\nmodel_dt.fit(X_train, y_train)\ny_pred_dt = model_dt.predict(X_test)\naccuracy_dt = metrics.accuracy_score(y_pred_dt, y_test)\nscores = cross_val_score(model_dt, X_train, y_train, cv=5)\ncv_score_dt = scores.mean()\nprint(\"Cross-val-score: \", cv_score_dt)\nprint(\"Accuracy score: \", accuracy_dt)","88ac9cc7":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nmodel_rf = RandomForestClassifier(max_depth = 7, min_samples_split=5)\nmodel_rf.fit(X_train, y_train)\ny_pred_rf = model_rf.predict(X_test)\naccuracy_rf = metrics.accuracy_score(y_pred_rf, y_test)\nscores = cross_val_score(model_rf, X_train, y_train, cv=5)\ncv_score_rf = scores.mean()\nprint(\"Cross-val-score: \", cv_score_rf)\nprint(\"Accuracy score: \", accuracy_rf)","04fd13e8":"from sklearn.linear_model import SGDClassifier, LogisticRegression\nmodel_gd = SGDClassifier()\nmodel_gd.fit(X_train, y_train)\ny_pred_gd = model_dt.predict(X_test)\naccuracy_gd = metrics.accuracy_score(y_pred_gd, y_test)\nscores = cross_val_score(model_gd, X_train, y_train, cv=5)\ncv_score_gd = scores.mean()\nprint(\"Cross-val-score: \", cv_score_gd)\nprint(\"Accuracy score: \", accuracy_gd)","e3c1b7bb":"model_logR = LogisticRegression()\nmodel_logR.fit(X_train, y_train)\ny_pred_logR = model_logR.predict(X_test)\naccuracy_logR = metrics.accuracy_score(y_pred_logR, y_test)\nscores = cross_val_score(model_logR, X_train, y_train, cv=5)\ncv_score_logR = scores.mean()\nprint(\"Cross-val-score: \", cv_score_logR)\nprint(\"Accuracy score: \", accuracy_logR)","f72cc574":"model_ada = AdaBoostClassifier()\nmodel_ada.fit(X_train, y_train)\ny_pred_ada = model_ada.predict(X_test)\naccuracy_ada = metrics.accuracy_score(y_pred_ada, y_test)\nscores = cross_val_score(model_ada, X_train, y_train, cv=5)\ncv_score_ada = scores.mean()\nprint(\"Cross-val-score: \", cv_score_ada)\nprint(\"Accuracy score: \", accuracy_ada)","2a6f172b":"print(\"Final accuracy of model = \", accuracy_logR * 100)","1a430aa5":"### **Logistic Regression**","bf3855ea":"*Standard Scaling*","7861ff54":"### **Random Forest Classifier** ","976fb20c":"***As the last column(Unnamed: 32) is filled with NaN values, we will drop it.***","5d32d1da":"### **Stochastic Gradient Descent Classifier**","932cce57":"### **AdaBoost Classifier**","e428650f":"*Splitting the data into training and test sets*","ae571b6c":"### **Decision Tree Classifier**","c2859631":"### **KNN Classifier**","c1f61ecc":"***Logistic Regression performs the best on our Dataset with 98.05% accuracy.***","7793e5f4":"Dropping correlated columns: **perimeter_mean, radius_mean, perimeter_worst, radius_worst, perimeter_se, radius_se** "}}