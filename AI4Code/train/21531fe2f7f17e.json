{"cell_type":{"31c8bb6c":"code","26284263":"code","40b46701":"code","41d79b8c":"code","9ed58b32":"code","8034c788":"code","f6dd17bf":"code","f28a0229":"code","87c21dd9":"code","a3137a95":"code","9695093d":"code","ec7272fd":"code","700f2056":"code","d4a9cda6":"code","66fd95d1":"code","3b2696e8":"code","7addd3be":"code","2f6ead07":"code","fbbd7175":"code","1949ef3f":"code","2f089900":"code","dc67f4ba":"code","1759ca06":"code","dd363304":"code","25801340":"code","f6287886":"code","c0f9960c":"code","1fc8f2cb":"code","a93c4089":"code","a0cb90a3":"code","8811a27b":"code","7cfc5f8d":"code","446aa238":"code","d4b82fb7":"code","0789dddf":"code","88628ad6":"code","7f15edc7":"code","bec2da8c":"code","fba498fa":"code","7d45732b":"code","3e202ca8":"code","c93cc15e":"code","06dc122b":"code","f9134334":"code","2ddf13e0":"code","9ba4a2a3":"code","e5931360":"code","18b87333":"code","04815b31":"code","5d3f8ba7":"code","1300ddf2":"code","82dd236b":"code","5f2c8730":"code","a84794a1":"code","299eb99a":"code","e7dd526b":"code","c0af53ac":"code","07b15b01":"code","cddf9d96":"code","dbcd9e0c":"code","68f7f22d":"code","fd1f2e9a":"code","74377680":"code","526e9e9c":"code","5e95d8ba":"code","2d4ea896":"code","0791cc8b":"code","9a43b4f7":"code","17035434":"code","f73b6bfe":"code","20ce846c":"code","c31dde3f":"code","a4835f9c":"code","f1ee8d4e":"code","cbed509f":"code","ed238053":"code","2067fbda":"code","1b813766":"code","af202168":"code","ca7595de":"markdown","4908a1a3":"markdown","adc09eee":"markdown","66450fb5":"markdown"},"source":{"31c8bb6c":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport re\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.set(style='white', context='notebook', palette='deep')\n\nfrom collections import Counter\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Decision_Tree\nfrom sklearn import tree","26284263":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ncombine = [train_df, test_df]\ndataset =  pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)","40b46701":"train_df.shape, test_df.shape, dataset.shape","41d79b8c":"# Acquire filenames\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","9ed58b32":"train_df.head()","8034c788":"train_df.describe(include=['object'])","f6dd17bf":"train_df.describe()","f28a0229":"train_df.info()\nprint('_'*40)\ntest_df.info()","87c21dd9":"# Summarie and statistics\ntrain_df.describe()\n\n# Review survived rate using percentiles\n#train_df['Survived'].quantile([.61,.62])\n#train_df['Pclass'].quantile([.4, .45])\n#train_df['SibSp'].quantile([.65, .7])\n#train_df[['Age', 'Fare']].quantile([.05,.1,.2,.4,.6,.8,.9,.99])","a3137a95":"# Outlier detection \n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   ","9695093d":"Outliers_to_drop = detect_outliers(train_df,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","ec7272fd":"print(train_df.isnull().sum().sort_values(ascending=False).head())\ntest_df.isnull().sum().sort_values(ascending=False).head()","700f2056":"train_df.loc[Outliers_to_drop]","d4a9cda6":"train_df.describe()","66fd95d1":"train_df.describe(include=['object'])","3b2696e8":"test_df.describe()","7addd3be":"g = sns.heatmap(train_df[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True,fmt = \".2f\",cmap = \"coolwarm\",alpha=1,vmin=-1, vmax=1)","2f6ead07":"# Explore SibSp feature vs Survived\ng = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train_df,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","fbbd7175":"# Explore Parch feature vs Survived\ng  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train_df,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","1949ef3f":"# Explore Parch feature vs Survived\ng  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train_df,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","2f089900":"# Explore Age vs Survived\ng = sns.FacetGrid(train_df, col='Survived')\ng = g.map(sns.distplot, \"Age\")","dc67f4ba":"# Explore Age distibution \ng = sns.kdeplot(train_df[\"Age\"][(train_df[\"Survived\"] == 0) & (train_df[\"Age\"].notnull())], color=\"Red\", shade = True)\ng = sns.kdeplot(train_df[\"Age\"][(train_df[\"Survived\"] == 1) & (train_df[\"Age\"].notnull())], ax =g, color=\"Blue\", shade= True)\ng.set_xticks(range(0,100,10))\ng.set_xlabel(\"Age\")\ng.set_ylabel(\"Frequency\")\ng = g.legend([\"Not Survived\",\"Survived\"])","1759ca06":"dataset[\"Fare\"].isnull().sum()","dd363304":"#Fill Fare missing values with the median value\ndataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].median())","25801340":"# Explore Fare distribution \ng = sns.distplot(dataset[\"Fare\"], color=\"r\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","f6287886":"# Apply log to Fare to reduce skewness distribution\ndataset[\"Fare\"] = dataset[\"Fare\"].map(lambda i: np.log(i+1))","c0f9960c":"g = sns.distplot(dataset[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(dataset[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","1fc8f2cb":"g = sns.barplot(x=\"Sex\",y=\"Survived\",data=train_df)\ng = g.set_ylabel(\"Survival Probability\")","a93c4089":"train_df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","a0cb90a3":"# Explore Pclass vs Survived\ng = sns.factorplot(x=\"Pclass\",y=\"Survived\",data=train_df,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","8811a27b":"# Explore Pclass vs Survived by Sex\ng = sns.factorplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=train_df,\n                   size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","7cfc5f8d":"dataset[\"Embarked\"].isnull().sum()\n#Fill Embarked nan values of dataset set with 'S' most frequent value\ndataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")","446aa238":"# Explore Embarked vs Survived \ng = sns.factorplot(x=\"Embarked\", y=\"Survived\",  data=train_df,\n                   size=6, kind=\"bar\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","d4b82fb7":"# Explore Pclass vs Embarked \ng = sns.factorplot(\"Pclass\", col=\"Embarked\",  data=train_df,\n                   size=6, kind=\"count\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")","0789dddf":"# convert Sex into categorical value 0 for male and 1 for female\ndataset[\"Sex\"] = dataset[\"Sex\"].map({\"male\": 0, \"female\":1})","88628ad6":"g = sns.heatmap(dataset[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),fmt = \".2f\",cmap=\"BrBG\",annot=True,alpha=.8,vmin=-1, vmax=1)","7f15edc7":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ng = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\ng.map(plt.hist, 'Age', bins=20, alpha=.7)\ng.add_legend()","bec2da8c":"## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(dataset[\"Age\"][dataset[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = dataset[\"Age\"].median()\n    age_pred = dataset[\"Age\"][((dataset['SibSp'] == dataset.iloc[i][\"SibSp\"]) & (dataset['Parch'] == dataset.iloc[i][\"Parch\"]) & (dataset['Pclass'] == dataset.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        dataset['Age'].iloc[i] = age_pred\n    else :\n        dataset['Age'].iloc[i] = age_med","fba498fa":"dataset['Age'].isnull().sum()","7d45732b":"g = sns.factorplot(x=\"Survived\", y = \"Age\",data = train_df, kind=\"box\")\ng = sns.factorplot(x=\"Survived\", y = \"Age\",data = train_df, kind=\"violin\")","3e202ca8":"print(\"Before\", dataset.shape)\n\ndataset.drop(['Cabin', 'Ticket', 'PassengerId'], axis=True, inplace=True)\n\nprint('After', dataset.shape)","c93cc15e":"dataset['Title'] = dataset['Name'].str.extract('([A-Za-z]+)\\.', expand=False)","06dc122b":"pd.crosstab(dataset['Title'], dataset['Sex'])","f9134334":"g = sns.countplot(x=\"Title\",data=dataset)\ng = plt.setp(g.get_xticklabels(), rotation=45)","2ddf13e0":"# Convert to categorical values Title \ndataset[\"Title\"] = dataset[\"Title\"].replace(['Lady','Countess','Capt','Col','Don','Dr','Major','Rev','Sir','Jonkheer','Dona',\"Ms\" ,\"Mme\",\"Mlle\"],'Rare')","9ba4a2a3":"g = sns.countplot(dataset[\"Title\"])","e5931360":"g = sns.factorplot(x=\"Title\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"survival probability\")","18b87333":"# Drop Name variable\ndataset.drop(labels = [\"Name\"], axis = 1, inplace = True)","04815b31":"# Create a family size descriptor from SibSp and Parch\ndataset['Familysize'] = dataset['Parch'] + dataset['SibSp'] + 1","5d3f8ba7":"g = sns.factorplot(x=\"Familysize\",y=\"Survived\",data = dataset)\ng = g.set_ylabels(\"Survival Probability\")","1300ddf2":"# Create new feature of family size\nbins = [0,1,2,4,8]\nlabels = ['Single','SmallF','MedF','LargeF']\ndataset[\"Familysize\"] = pd.cut(dataset[\"Familysize\"], bins, labels=labels)","82dd236b":"g = sns.factorplot(x=\"Familysize\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_ylabels(\"survival probability\")","5f2c8730":"# Drop Fsize\/SibSp\/Parch variable\ndataset.drop(labels = ['SibSp','Parch'], axis = 1, inplace = True)","a84794a1":"# Create categorical values for Pclass\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset[\"Sex\"] = dataset[\"Sex\"].astype(\"category\")","299eb99a":"dataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"],prefix=\"Em\")\ndataset = pd.get_dummies(dataset, columns = [\"Familysize\"],prefix=\"Fs\")","e7dd526b":"dataset = pd.get_dummies(dataset)","c0af53ac":"dataset.head()","07b15b01":"train_df.shape","cddf9d96":"## Separate train dataset and test dataset\n\ntrain = dataset[:train_df.shape[0]]\nX_test = dataset[train_df.shape[0]:]\nX_test.drop(labels=[\"Survived\"],axis = 1,inplace=True)","dbcd9e0c":"## Separate train features and label \nX_train = train.drop(labels = [\"Survived\"],axis = 1)\nY_train = train[\"Survived\"].astype(int)","68f7f22d":"logreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nY_pred = logreg.predict(X_test)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","fd1f2e9a":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","74377680":"gaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nY_pred = gaussian.predict(X_test)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","526e9e9c":"perceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nY_pred = perceptron.predict(X_test)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","5e95d8ba":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nY_pred = linear_svc.predict(X_test)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","2d4ea896":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","0791cc8b":"sgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","9a43b4f7":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","17035434":"random_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","f73b6bfe":"models = pd.DataFrame({\n    'Model': [ 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent', 'Linear SVC', \n              'Decision Tree'],\n    'Score': [acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by='Score', ascending=False)","20ce846c":"x=np.asarray(X_train)\ny=np.asarray(Y_train)","c31dde3f":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nX = torch.from_numpy(x).type(torch.FloatTensor)\ny = torch.from_numpy(y).type(torch.LongTensor)","a4835f9c":"X_train.shape","f1ee8d4e":"#our class must extend nn.Module\nclass MyClassifier(nn.Module):\n    def __init__(self):\n        super(MyClassifier,self).__init__()\n        #Our network consists of 3 layers. 1 input, 1 hidden and 1 output layer\n        #This applies Linear transformation to input data. \n        self.fc1 = nn.Linear(19,40)\n        \n        #This applies linear transformation to produce output data\n        self.fc2 = nn.Linear(40,20)\n        self.fc3 = nn.Linear(20,2)\n        \n    #This must be implemented\n    def forward(self,x):\n        #Output of the first layer\n        x = self.fc1(x)\n        #Activation function is Relu. Feel free to experiment with this\n        x = F.tanh(x)\n        #This produces output\n        x = self.fc2(x)\n        x = F.tanh(x)\n        x = self.fc3(x)\n        return x\n        \n    #This function takes an input and predicts the class, (0 or 1)        \n    def predict(self,x):\n        #Apply softmax to output. \n        pred = F.softmax(self.forward(x))\n        ans = []\n        #Pick the class with maximum weight\n        for t in pred:\n            if t[0]>t[1]:\n                ans.append(0)\n            else:\n                ans.append(1)\n        return torch.tensor(ans)\n","cbed509f":"#Initialize the model        \nmodel = MyClassifier()\n#Define loss criterion\ncriterion = nn.CrossEntropyLoss()\n#Define the optimizer\noptimizer = torch.optim.Adamax(model.parameters(), lr=0.001)","ed238053":"#Number of epochs\nepochs = 50000\n#List to store losses\nlosses = []\nfor i in range(epochs):\n    #Precit the output for Given input\n    y_pred = model.forward(X)\n    #Compute Cross entropy loss\n    loss = criterion(y_pred,y)\n    #Add loss to the list\n    losses.append(loss.item())\n    #Clear the previous gradients\n    optimizer.zero_grad()\n    #Compute gradients\n    loss.backward()\n    #Adjust weights\n    optimizer.step()","2067fbda":"from sklearn.metrics import accuracy_score\nprint(accuracy_score(model.predict(X),y))","1b813766":"z=np.asarray(X_test)\nT = torch.from_numpy(z).type(torch.FloatTensor)\nY_pred=model.predict(T)\n","af202168":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)\nsubmission","ca7595de":"3.6.2 Creating new feature extracting from existing\u00b6\nWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.","4908a1a3":"As we can see, Fare distribution is very skewed. This can lead to overweigth very high values in the model, even if it is scaled.\n\nIn this case, it is better to transform it with the log function to reduce this skew.","adc09eee":"fare has max correlation","66450fb5":"outlier detection"}}