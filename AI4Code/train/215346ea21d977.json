{"cell_type":{"0476f931":"code","a97bc67a":"code","22775e2e":"code","804c4820":"code","e7822a62":"code","5a733ad6":"code","16bf0cd4":"code","ab21a9ab":"code","239891fd":"code","49756ead":"code","a9f0d257":"code","3078d34d":"code","72383c79":"code","9743ec88":"code","b4a8e599":"code","df8e0617":"code","64fc14b9":"code","bc9c59cd":"code","37cfeae6":"code","de3467d4":"code","fd1e7e1b":"code","53e3a572":"code","6a2691bf":"code","77d74678":"code","f8c15ec8":"code","e38a1166":"code","898c1400":"code","15c856a6":"code","fc13ffe9":"code","fdd8ddc1":"code","ad4a379e":"code","21d1ed53":"code","27c6e30c":"code","fc111965":"code","d6afa8ea":"code","b8264f67":"code","5357d4af":"code","ce85b733":"code","37f72974":"code","0b7fb39b":"code","c629f541":"code","6282691e":"code","ba9453c1":"code","cc0df4d3":"code","c15e214e":"code","f18efe19":"code","46992f00":"code","05a38591":"code","9ab7344a":"code","0b024bd4":"code","f8c236dc":"code","dddce593":"markdown","449d17ed":"markdown","e3e0a2f7":"markdown","31900f01":"markdown","9234edb6":"markdown","6ae66f23":"markdown","c7c900c7":"markdown","a1834db3":"markdown","991e18b6":"markdown","85d61100":"markdown","49a2ea23":"markdown","715305fb":"markdown","20fe81a6":"markdown","34361435":"markdown","cf631d4c":"markdown","4150b50b":"markdown","c13b3cec":"markdown","38fc20a0":"markdown","32aa843b":"markdown","7b6dbc78":"markdown","31f26113":"markdown","21408ea6":"markdown","2d29ec75":"markdown","6396b0ec":"markdown","4d91d911":"markdown","14e42bf3":"markdown","8a5aa11f":"markdown","f09ce1a7":"markdown","037db63f":"markdown"},"source":{"0476f931":"# Importing the libraries \nimport pandas as pd\nimport numpy as np\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","a97bc67a":"# Importing the Boston Housing dataset\nfrom sklearn.datasets import load_boston\nboston = load_boston()","22775e2e":"# Initializing the dataframe\ndata = pd.DataFrame(boston.data)","804c4820":"# See head of the dataset\ndata.head()","e7822a62":"#Adding the feature names to the dataframe\ndata.columns = boston.feature_names\ndata.head()","5a733ad6":"#Adding target variable to dataframe\ndata['PRICE'] = boston.target \n# Median value of owner-occupied homes in $1000s","16bf0cd4":"#Check the shape of dataframe\ndata.shape","ab21a9ab":"data.columns","239891fd":"data.dtypes","49756ead":"# Identifying the unique number of values in the dataset\ndata.nunique()","a9f0d257":"# Check for missing values\ndata.isnull().sum()","3078d34d":"# See rows with missing values\ndata[data.isnull().any(axis=1)]","72383c79":"# Viewing the data statistics\ndata.describe()","9743ec88":"# Finding out the correlation between the features\ncorr = data.corr()\ncorr.shape","b4a8e599":"# Plotting the heatmap of correlation between features\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, cbar=True, square= True, fmt='.1f', annot=True, annot_kws={'size':15}, cmap='Greens')","df8e0617":"# Spliting target variable and independent variables\nX = data.drop(['PRICE'], axis = 1)\ny = data['PRICE']","64fc14b9":"# Splitting to training and testing data\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 4)","bc9c59cd":"# Import library for Linear Regression\nfrom sklearn.linear_model import LinearRegression\n\n# Create a Linear regressor\nlm = LinearRegression()\n\n# Train the model using the training sets \nlm.fit(X_train, y_train)","37cfeae6":"# Value of y intercept\nlm.intercept_","de3467d4":"#Converting the coefficient values to a dataframe\ncoeffcients = pd.DataFrame([X_train.columns,lm.coef_]).T\ncoeffcients = coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'})\ncoeffcients","fd1e7e1b":"# Model prediction on train data\ny_pred = lm.predict(X_train)","53e3a572":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","6a2691bf":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","77d74678":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","f8c15ec8":"# Checking Normality of errors\nsns.distplot(y_train-y_pred)\nplt.title(\"Histogram of Residuals\")\nplt.xlabel(\"Residuals\")\nplt.ylabel(\"Frequency\")\nplt.show()","e38a1166":"# Predicting Test data with the model\ny_test_pred = lm.predict(X_test)","898c1400":"# Model Evaluation\nacc_linreg = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_linreg)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","15c856a6":"# Import Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Create a Random Forest Regressor\nreg = RandomForestRegressor()\n\n# Train the model using the training sets \nreg.fit(X_train, y_train)","fc13ffe9":"# Model prediction on train data\ny_pred = reg.predict(X_train)","fdd8ddc1":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","ad4a379e":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","21d1ed53":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","27c6e30c":"# Predicting Test data with the model\ny_test_pred = reg.predict(X_test)","fc111965":"# Model Evaluation\nacc_rf = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_rf)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","d6afa8ea":"# Import XGBoost Regressor\nfrom xgboost import XGBRegressor\n\n#Create a XGBoost Regressor\nreg = XGBRegressor()\n\n# Train the model using the training sets \nreg.fit(X_train, y_train)","b8264f67":"# Model prediction on train data\ny_pred = reg.predict(X_train)","5357d4af":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","ce85b733":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","37f72974":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","0b7fb39b":"#Predicting Test data with the model\ny_test_pred = reg.predict(X_test)","c629f541":"# Model Evaluation\nacc_xgb = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_xgb)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","6282691e":"# Creating scaled set to be used in model to improve our results\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","ba9453c1":"# Import SVM Regressor\nfrom sklearn import svm\n\n# Create a SVM Regressor\nreg = svm.SVR()","cc0df4d3":"# Train the model using the training sets \nreg.fit(X_train, y_train)","c15e214e":"# Model prediction on train data\ny_pred = reg.predict(X_train)","f18efe19":"# Model Evaluation\nprint('R^2:',metrics.r2_score(y_train, y_pred))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_train, y_pred))*(len(y_train)-1)\/(len(y_train)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_train, y_pred))\nprint('MSE:',metrics.mean_squared_error(y_train, y_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_train, y_pred)))","46992f00":"# Visualizing the differences between actual prices and predicted values\nplt.scatter(y_train, y_pred)\nplt.xlabel(\"Prices\")\nplt.ylabel(\"Predicted prices\")\nplt.title(\"Prices vs Predicted prices\")\nplt.show()","05a38591":"# Checking residuals\nplt.scatter(y_pred,y_train-y_pred)\nplt.title(\"Predicted vs residuals\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Residuals\")\nplt.show()","9ab7344a":"# Predicting Test data with the model\ny_test_pred = reg.predict(X_test)","0b024bd4":"# Model Evaluation\nacc_svm = metrics.r2_score(y_test, y_test_pred)\nprint('R^2:', acc_svm)\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_test_pred))*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_test_pred))\nprint('MSE:',metrics.mean_squared_error(y_test, y_test_pred))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_test_pred)))","f8c236dc":"models = pd.DataFrame({\n    'Model': ['Linear Regression', 'Random Forest', 'XGBoost', 'Support Vector Machines'],\n    'R-squared Score': [acc_linreg*100, acc_rf*100, acc_xgb*100, acc_svm*100]})\nmodels.sort_values(by='R-squared Score', ascending=False)","dddce593":"### Model Evaluation","449d17ed":"CRIM per capita crime rate by town <br>\nZN proportion of residential land zoned for lots over 25,000 sq.ft. <br>\nINDUS proportion of non-retail business acres per town <br>\nCHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) <br>\nNOX nitric oxides concentration (parts per 10 million) <br>\nRM average number of rooms per dwelling <br>\nAGE proportion of owner-occupied units built prior to 1940 <br>\nDIS weighted distances to five Boston employment centres <br>\nRAD index of accessibility to radial highways <br>\nTAX full-value property-tax rate per 10,000usd <br>\nPTRATIO pupil-teacher ratio by town <br>\nB 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town <br>\nLSTAT % lower status of the population <br>","e3e0a2f7":"### Model Evaluation","31900f01":"#### For test data","9234edb6":"Each record in the database describes a Boston suburb or town.","6ae66f23":"Here the model evaluations scores are almost matching with that of train data. So the model is not overfitting.","c7c900c7":"The problem that we are going to solve here is that given a set of features that describe a house in Boston, our machine learning model must predict the house price. To train our machine learning model with boston housing data, we will be using scikit-learn\u2019s boston dataset.\n\nIn this dataset, each row describes a boston town or suburb. There are 506 rows and 13 attributes (features) with a target column (price).\nhttps:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/housing\/housing.names","a1834db3":"# XGBoost Regressor","991e18b6":"# SVM Regressor","85d61100":"#### Model Evaluation","49a2ea23":"#### Train the model","715305fb":"# Random Forest Regressor ","20fe81a6":"\ud835\udc45^2 : It is a measure of the linear relationship between X and Y. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.\n\nAdjusted \ud835\udc45^2 :The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.\n\nMAE : It is the mean of the absolute value of the errors. It measures the difference between two continuous variables, here actual and predicted values of y.\u00a0\n\nMSE: The\u00a0mean square error\u00a0(MSE) is just like the MAE, but\u00a0squares\u00a0the difference before summing them all instead of using the absolute value.\u00a0\n\nRMSE: The\u00a0mean square error\u00a0(MSE) is just like the MAE, but\u00a0squares\u00a0the difference before summing them all instead of using the absolute value.\u00a0\n\n\n\n\n","34361435":"## Hence XGBoost Regression works the best for this dataset.****","cf631d4c":"Here the residuals are normally distributed. So normality assumption is satisfied","4150b50b":"#### Train the model","c13b3cec":"### Please upvote if you found this kernel useful! :) <br>\n### Feedback is greatly appreciated!","38fc20a0":"# Boston house price prediction","32aa843b":"#### Training the model","7b6dbc78":"# Linear regression","31f26113":"#### For test data","21408ea6":"max_depth (int) \u2013 Maximum tree depth for base learners.\n\nlearning_rate (float) \u2013 Boosting learning rate (xgb\u2019s \u201ceta\u201d)\n\nn_estimators (int) \u2013 Number of boosted trees to fit.\n\ngamma (float) \u2013 Minimum loss reduction required to make a further partition on a leaf node of the tree.\n\nmin_child_weight (int) \u2013 Minimum sum of instance weight(hessian) needed in a child.\n\nsubsample (float) \u2013 Subsample ratio of the training instance.\n\ncolsample_bytree (float) \u2013 Subsample ratio of columns when constructing each tree.\n\nobjective (string or callable) \u2013 Specify the learning task and the corresponding learning objective or a custom objective function to be used (see note below).\n\nnthread (int) \u2013 Number of parallel threads used to run xgboost. (Deprecated, please use n_jobs)\n\nscale_pos_weight (float) \u2013 Balancing of positive and negative weights.\n","2d29ec75":"#### Model Evaluation","6396b0ec":"#### Training the model","4d91d911":"#### For test data","14e42bf3":"There is no pattern visible in this plot and values are distributed equally around zero. So Linearity assumption is satisfied","8a5aa11f":"C : float, optional (default=1.0): The penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\n\nkernel : string, optional (default='rbf\u2019): kernel parameters selects the type of hyperplane used to separate the data. It must be one of 'linear', 'poly', 'rbf', 'sigmoid', 'precomputed\u2019 or a callable.\n\ndegree : int, optional (default=3): Degree of the polynomial kernel function (\u2018poly\u2019). Ignored by all other kernels.\n\ngamma : float, optional (default='auto\u2019): It is for non linear hyperplanes. The higher the gamma value it tries to exactly fit the training data set. Current default is 'auto' which uses 1 \/ n_features.\n\ncoef0 : float, optional (default=0.0): Independent term in kernel function. It is only significant in 'poly' and 'sigmoid'.\n\nshrinking : boolean, optional (default=True): Whether to use the shrinking heuristic.","f09ce1a7":"#### For test data","037db63f":"# Evaluation and comparision of all the models"}}