{"cell_type":{"858d79b7":"code","231a1d9b":"code","602233fd":"code","f564c755":"code","153ca1e6":"code","ff481289":"code","6102ad08":"code","4fcd86d1":"code","bdb86724":"code","14e3c9dc":"code","42199d12":"code","8de8fc0d":"code","aa519968":"code","a72acc14":"code","4fe2fbcb":"code","4b65d39a":"code","1da027b6":"code","6874de00":"code","2a6676ff":"code","4d9b715d":"code","9fd8ebb7":"code","40044e39":"code","a679f9b6":"code","49b4685e":"code","1dedea51":"code","1bd95c29":"code","90406674":"code","033bc4a3":"code","a1cfd29c":"code","574b5c3e":"code","3b30c540":"code","99bf1915":"code","ac25a157":"code","bfba070c":"code","687f2319":"code","ec1af1d3":"code","f8721e9c":"code","81e3a0fc":"code","dae4f13d":"code","aee6cb4b":"code","f554274f":"code","2987e5cc":"code","2e94b6ac":"code","e701b8bb":"code","4bf50145":"code","e1d11b54":"code","e816aacd":"code","06ac8fd2":"code","93441a86":"code","d629b13d":"code","6eb4efcd":"code","1b16bdb1":"code","c405a713":"code","a3994c8a":"code","8700c13b":"code","2b94a253":"code","57d866dc":"code","fd99674a":"code","92937db1":"code","12ff39fa":"code","55df29f2":"code","9f9c2b03":"code","a60909d5":"code","1e795b15":"code","091564c2":"code","2af8172d":"code","636f6e79":"code","02e2fffe":"code","d1a11d4b":"code","63c77691":"code","0d674bcc":"code","bf70bd5d":"code","68544212":"code","be686eed":"code","accc0fa9":"code","db8bcf3c":"code","bac13cd3":"code","0a6f07a6":"code","e6036793":"code","7200f129":"code","3a32b876":"code","7bb2a73b":"code","9472ea2f":"code","ed20ef4e":"code","d4a7f10b":"code","86c2f71a":"code","2ac39cc5":"code","cc9547fc":"code","ef5a0eb0":"code","9352303d":"code","6f3dc45a":"code","b6867c55":"code","7da160d1":"code","f2ec9ed9":"code","67565a05":"code","49827bc3":"code","19ddae56":"code","f7268eb7":"code","f31130da":"code","8d49fcfe":"code","b9f07f88":"code","1d475718":"code","34273622":"code","1d6f7806":"code","8ebfd570":"code","f0fd91e5":"code","06a2a53c":"code","950bcc69":"code","474aa84f":"code","46ea055d":"code","3df31685":"code","c826ece0":"code","be940592":"code","17b9601a":"code","d3e40a89":"code","1334a9d3":"code","fa0a782e":"code","b3f0f81c":"code","96d66080":"code","a20e93a3":"code","58d55507":"code","c7c8ecdd":"code","f7e0d3cb":"code","a7e28f32":"code","a9e48fde":"code","c363ceeb":"code","d249f519":"code","b3945c6c":"code","7e259bd4":"code","c0a3e03a":"code","2f18b72d":"code","cfe968b0":"code","64a83182":"code","d03e32bd":"code","e72da277":"code","e7bf8851":"code","9d5b9eca":"code","fbff4c6e":"code","6024a97e":"code","d058ffd4":"code","40a5d64f":"code","10dde851":"code","7bd96677":"code","6435eff0":"code","7524b8c6":"code","f1eaee20":"code","57321705":"code","b0037ffa":"code","65725af1":"code","b3bd6a44":"code","9563ec2d":"code","12df396e":"code","4b064410":"code","9d14d5c1":"code","bea7efcf":"code","899dbe32":"code","a7505a4b":"code","7707deb4":"code","85df0960":"code","8f51a7ea":"code","15a5855f":"code","59c1937e":"code","f17ac808":"code","588b44ea":"code","24e5430f":"code","88dbbd2c":"code","cdf46cc1":"code","d539fe5c":"code","d134602f":"code","caee5a7f":"code","20712afb":"code","61cec081":"code","c1427bbb":"code","1b306286":"code","5f7ba76f":"code","ca5f09a1":"code","3117a5bc":"code","0dace7e9":"code","86dd3993":"code","7c99a05e":"code","b855e565":"code","cd0b26fc":"code","222dd7ce":"code","ebdc1ffa":"code","2bf6f140":"code","0ae0f77d":"code","b63fd6d1":"code","d7254051":"code","5e190e87":"code","4813475a":"code","80c72fdf":"code","bcbce00e":"code","2337f752":"code","b5b8cb95":"code","7b6a8b44":"code","14ff8e26":"code","7abfea0d":"code","7aacd927":"code","57e103d9":"code","228bd826":"code","40be1d7e":"code","02c392da":"code","3ff11b64":"code","fcca9e8a":"code","31807e4b":"code","220ae193":"code","5876a8fb":"code","d821e58b":"code","36b40838":"code","96fa4aa7":"code","738ef154":"code","cf28436a":"code","52f3f06f":"code","606df13f":"code","47128115":"code","f718a78c":"code","a2128b26":"code","29496a18":"code","aeef4124":"code","8d3e1f9a":"code","cd9419f5":"code","f7ce9d2f":"code","5c7bf777":"code","7eef9075":"code","b00aa828":"code","570c8843":"code","87d624d5":"code","d1160462":"code","97a191dc":"code","29525b31":"code","e45154b3":"code","bc0a7c4a":"code","ac979390":"code","f351fe95":"code","eb06e413":"code","95717050":"code","0dbe5c0d":"code","6b23d8be":"code","f5a4353c":"code","e8586da7":"code","6d2f6fe3":"code","eafc5230":"code","eb62c5c1":"code","955340d5":"code","bc3eb3ec":"code","e600300e":"code","89ce6a01":"code","13f939bc":"code","f9d2e199":"code","28d27299":"code","0b529841":"code","7abfa82e":"code","173b3126":"code","9a2b5ca7":"markdown","5dac5d8b":"markdown","02a59ca6":"markdown","6b8c80d3":"markdown","42e285dc":"markdown","38d7296e":"markdown","e403ad35":"markdown","f842ba12":"markdown","32aa5e05":"markdown","e0bbfdb1":"markdown","8fb89e81":"markdown","0b57fbfa":"markdown","01e5d40a":"markdown","3b2840e6":"markdown","3fb03a58":"markdown","eedcd5ca":"markdown","ff51ef43":"markdown","20c6cd6e":"markdown","ca1a10d0":"markdown","3b71cf62":"markdown","489adf89":"markdown","99d2157f":"markdown","cfa03f38":"markdown","7be26dba":"markdown","d9c04c3c":"markdown","0c03caf8":"markdown","b5ba1bfa":"markdown","e0a40280":"markdown","65ed2312":"markdown","fa090952":"markdown","b6e05ee9":"markdown","851f33bd":"markdown","d5487032":"markdown","87aef8ba":"markdown","70d0af29":"markdown","eca86ed8":"markdown","1068d335":"markdown","8b8a0c84":"markdown","4e815d40":"markdown","5a5ed830":"markdown","20134a3f":"markdown","ff2e33a9":"markdown","e05d40ef":"markdown","f7942af2":"markdown","8c1009a3":"markdown","10cbddef":"markdown","4225ecdc":"markdown","1b298cde":"markdown"},"source":{"858d79b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","231a1d9b":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nplt.figure(figsize = (20, 18))\nimport seaborn as sns\nfrom yellowbrick.target import FeatureCorrelation\nfrom sklearn.feature_selection import chi2, f_classif, mutual_info_classif\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix,classification_report,plot_confusion_matrix,accuracy_score,roc_auc_score,roc_curve,auc\nfrom sklearn.model_selection import train_test_split,StratifiedKFold,cross_val_score\nfrom bayes_opt import BayesianOptimization\nimport xgboost as xgb\n# hyperopt is hyperparameter optimization by defining an objective function and declaring a search space\nfrom hyperopt import hp, fmin, tpe, Trials, STATUS_OK\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom cuml import RandomForestClassifier as cuRF\nfrom cuml.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost","602233fd":"import warnings\nwarnings.filterwarnings('ignore')","f564c755":"train_data = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntrain_data.shape","153ca1e6":"train_data.head()","ff481289":"train_data.dtypes","6102ad08":"train_data.info() # all features are continuous","4fcd86d1":"sum(train_data.isnull().sum()) # no null values","bdb86724":"# dropping the id column\ntrain_data = train_data.drop(\"id\", axis=1)","14e3c9dc":"train_data.head()","42199d12":"train_data.describe().T # the features are on varying scale","8de8fc0d":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","aa519968":"# Compresses the training data \ntrain_data = reduce_mem_usage(train_data)","a72acc14":"train_data.f0.describe()","4fe2fbcb":"sns.distplot(train_data['f0'])\n","4b65d39a":"from seaborn import boxplot\nfig, ax = plt.subplots(figsize=(8, 6))\nboxplot(x=\"target\", y=\"f0\", data=train_data)\n","1da027b6":"from seaborn import violinplot\nfig, ax = plt.subplots(figsize=(8, 6))\nviolinplot(x=\"target\", y=\"f0\", data=train_data)\n","6874de00":"violinplot(x=\"target\", y=\"f1\", data=train_data)","2a6676ff":"fig = sns.FacetGrid(train_data, hue=\"target\", aspect=.75, height=4)\nfig.map_dataframe(sns.scatterplot, x=\"f0\", y=\"f1\")  # bill_depth_mm vs body_mass_g in the 3 islands and the 3 species\nfig.set_axis_labels(\"f0\", \"f1\")\nfig.add_legend()","4d9b715d":"violinplot(x=\"target\", y=\"f2\", data=train_data)","9fd8ebb7":"violinplot(x=\"target\", y=\"f3\", data=train_data)","40044e39":"violinplot(x=\"target\", y=\"f4\", data=train_data)","a679f9b6":"violinplot(x=\"target\", y=\"f5\", data=train_data)","49b4685e":"violinplot(x=\"target\", y=\"f6\", data=train_data)","1dedea51":"violinplot(x=\"target\", y=\"f7\", data=train_data)","1bd95c29":"violinplot(x=\"target\", y=\"f8\", data=train_data)","90406674":"violinplot(x=\"target\", y=\"f9\", data=train_data)","033bc4a3":"violinplot(x=\"target\", y=\"f10\", data=train_data)","a1cfd29c":"fig = sns.FacetGrid(train_data, hue=\"target\", aspect=.75, height=4)\nfig.map_dataframe(sns.scatterplot, x=\"f1\", y=\"f2\")  \nfig.set_axis_labels(\"f1\", \"f2\")\nfig.add_legend()","574b5c3e":"fig = sns.FacetGrid(train_data, hue=\"target\", aspect=.75, height=4)\nfig.map_dataframe(sns.scatterplot, x=\"f0\", y=\"f2\")  \nfig.set_axis_labels(\"f0\", \"f2\")\nfig.add_legend()","3b30c540":"fig = sns.FacetGrid(train_data, hue=\"target\", aspect=.75, height=4)\nfig.map_dataframe(sns.scatterplot, x=\"f2\", y=\"f3\")  \nfig.set_axis_labels(\"f2\", \"f3\")\nfig.add_legend()","99bf1915":"fig = sns.FacetGrid(train_data, hue=\"target\", aspect=.75, height=4)\nfig.map_dataframe(sns.scatterplot, x=\"f3\", y=\"f4\")  \nfig.set_axis_labels(\"f3\", \"f4\")\nfig.add_legend()","ac25a157":"fig = sns.FacetGrid(train_data, hue=\"target\", aspect=.75, height=4)\nfig.map_dataframe(sns.scatterplot, x=\"f4\", y=\"f5\")  \nfig.set_axis_labels(\"f4\", \"f5\")\nfig.add_legend()","bfba070c":"fig = sns.FacetGrid(train_data, hue=\"target\", aspect=.75, height=4)\nfig.map_dataframe(sns.scatterplot, x=\"f5\", y=\"f6\")  \nfig.set_axis_labels(\"f5\", \"f6\")\nfig.add_legend()","687f2319":"fig = sns.FacetGrid(train_data, hue=\"target\", aspect=.75, height=4)\nfig.map_dataframe(sns.scatterplot, x=\"f7\", y=\"f8\")  \nfig.set_axis_labels(\"f7\", \"f8\")\nfig.add_legend()","ec1af1d3":"fig = sns.FacetGrid(train_data, hue=\"target\", aspect=.75, height=4)\nfig.map_dataframe(sns.scatterplot, x=\"f9\", y=\"f10\")  \nfig.set_axis_labels(\"f9\", \"f10\")\nfig.add_legend()","f8721e9c":"plt.figure(figsize=(16, 6))\nheatmap = sns.heatmap(train_data[['f0','f1','f2','f3','f4','f5','f6','f7','f8','f9','f10']].corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","81e3a0fc":"sns.countplot(data=train_data,x='target')","dae4f13d":"df = train_data[['f0','f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9','f10','target']]","aee6cb4b":"df.plot.scatter(x='f1', y='f10',c='target',colormap='viridis',figsize=(20, 18));","f554274f":"target = df['target']\n\nfeatures = df.drop('target', axis=1)","2987e5cc":"feature_names = list(features.columns)\nfigure(figsize=(20,18), dpi=80)\nvisualizer = FeatureCorrelation(labels = feature_names, method='pearson')\n\nvisualizer.fit(features, target)\n\nvisualizer.poof()","2e94b6ac":"select_univariate = SelectKBest(f_classif, k=4).fit(features, target)\n\nfeatures_mask = select_univariate.get_support()\n\nfeatures_mask\n\nselected_columns = features.columns[features_mask]\n\nselected_columns\n\nselected_features = features[selected_columns]\n\nselected_features.head()","e701b8bb":"from sklearn import feature_selection\nmic = feature_selection.mutual_info_classif( features, target)\nfig, ax = plt.subplots(figsize=(10, 8))\n(pd.DataFrame({\"feature\": features.columns, \"vimp\": mic}).set_index(\"feature\").plot.barh(ax=ax))\n","4bf50145":"from sklearn.feature_selection import RFE\nmodel = RandomForestClassifier( n_estimators=10)\nrfe = RFE(model, 4)\nrfe.fit(features, target)\nprint(rfe.support_)\nprint(features.loc[:, rfe.support_].head())","e1d11b54":"fig = sns.FacetGrid(train_data, hue=\"target\", aspect=.75, height=4)\nfig.map_dataframe(sns.scatterplot, x=\"f19\", y=\"f20\")  # bill_depth_mm vs body_mass_g in the 3 islands and the 3 species\nfig.set_axis_labels(\"f19\", \"f20\")\nfig.add_legend()","e816aacd":"df = train_data[['f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19','f20','target']]","06ac8fd2":"target = df['target']\n\nfeatures = df.drop('target', axis=1)","93441a86":"feature_names = list(features.columns)\nfigure(figsize=(20,18), dpi=80)\nvisualizer = FeatureCorrelation(labels = feature_names, method='pearson')\n\nvisualizer.fit(features, target)\n\nvisualizer.poof()","d629b13d":"select_univariate = SelectKBest(f_classif, k=4).fit(features, target)\n\nfeatures_mask = select_univariate.get_support()\n\nfeatures_mask\n\nselected_columns = features.columns[features_mask]\n\nselected_columns\n\nselected_features = features[selected_columns]\n\nselected_features.head()","6eb4efcd":"mic = feature_selection.mutual_info_classif( features, target)\nfig, ax = plt.subplots(figsize=(10, 8))\n(pd.DataFrame({\"feature\": features.columns, \"vimp\": mic}).set_index(\"feature\").plot.barh(ax=ax))\n","1b16bdb1":"from sklearn.feature_selection import RFE\nmodel = RandomForestClassifier( n_estimators=10)\nrfe = RFE(model, 4)\nrfe.fit(features, target)\nprint(rfe.support_)\nprint(features.loc[:, rfe.support_].head())","c405a713":"df = train_data[['f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29','f30','target']]","a3994c8a":"target = df['target']\n\nfeatures = df.drop('target', axis=1)","8700c13b":"feature_names = list(features.columns)\nfigure(figsize=(20,18), dpi=80)\nvisualizer = FeatureCorrelation(labels = feature_names, method='pearson')\n\nvisualizer.fit(features, target)\n\nvisualizer.poof()","2b94a253":"select_univariate = SelectKBest(f_classif, k=4).fit(features, target)\n\nfeatures_mask = select_univariate.get_support()\n\nfeatures_mask\n\nselected_columns = features.columns[features_mask]\n\nselected_columns\n\nselected_features = features[selected_columns]\n\nselected_features.head()","57d866dc":"mic = feature_selection.mutual_info_classif( features, target)\nfig, ax = plt.subplots(figsize=(10, 8))\n(pd.DataFrame({\"feature\": features.columns, \"vimp\": mic}).set_index(\"feature\").plot.barh(ax=ax))","fd99674a":"from sklearn.feature_selection import RFE\nmodel = RandomForestClassifier( n_estimators=10)\nrfe = RFE(model, 4)\nrfe.fit(features, target)\nprint(rfe.support_)\nprint(features.loc[:, rfe.support_].head())","92937db1":"df = train_data[['f31', 'f32', 'f33', 'f34', 'f35', 'f36', 'f37', 'f38', 'f39','f40','target']]","12ff39fa":"target = df['target']\nfeatures = df.drop('target', axis=1)","55df29f2":"feature_names = list(features.columns)\nfigure(figsize=(20,18), dpi=80)\nvisualizer = FeatureCorrelation(labels = feature_names, method='pearson')\n\nvisualizer.fit(features, target)\n\nvisualizer.poof()","9f9c2b03":"select_univariate = SelectKBest(f_classif, k=4).fit(features, target)\n\nfeatures_mask = select_univariate.get_support()\n\nfeatures_mask\n\nselected_columns = features.columns[features_mask]\n\nselected_columns\n\nselected_features = features[selected_columns]\n\nselected_features.head()","a60909d5":"mic = feature_selection.mutual_info_classif( features, target)\nfig, ax = plt.subplots(figsize=(10, 8))\n(pd.DataFrame({\"feature\": features.columns, \"vimp\": mic}).set_index(\"feature\").plot.barh(ax=ax))","1e795b15":"from sklearn.feature_selection import RFE\nmodel = RandomForestClassifier( n_estimators=10)\nrfe = RFE(model, 4)\nrfe.fit(features, target)\nprint(rfe.support_)\nprint(features.loc[:, rfe.support_].head())","091564c2":"df = train_data[['f41', 'f42', 'f43', 'f44', 'f45', 'f46', 'f47', 'f48', 'f49','f50','target']]","2af8172d":"target = df['target']\nfeatures = df.drop('target', axis=1)","636f6e79":"feature_names = list(features.columns)\nfigure(figsize=(20,18), dpi=80)\nvisualizer = FeatureCorrelation(labels = feature_names, method='pearson')\n\nvisualizer.fit(features, target)\n\nvisualizer.poof()","02e2fffe":"select_univariate = SelectKBest(f_classif, k=4).fit(features, target)\n\nfeatures_mask = select_univariate.get_support()\n\nfeatures_mask\n\nselected_columns = features.columns[features_mask]\n\nselected_columns\n\nselected_features = features[selected_columns]\n\nselected_features.head()","d1a11d4b":"mic = feature_selection.mutual_info_classif( features, target)\nfig, ax = plt.subplots(figsize=(10, 8))\n(pd.DataFrame({\"feature\": features.columns, \"vimp\": mic}).set_index(\"feature\").plot.barh(ax=ax))","63c77691":"from sklearn.feature_selection import RFE\nmodel = RandomForestClassifier( n_estimators=10)\nrfe = RFE(model, 4)\nrfe.fit(features, target)\nprint(rfe.support_)\nprint(features.loc[:, rfe.support_].head())","0d674bcc":"df = train_data[['f51', 'f52', 'f53', 'f54', 'f55', 'f56', 'f57', 'f58', 'f59','f60','target']]","bf70bd5d":"target = df['target']\nfeatures = df.drop('target', axis=1)","68544212":"feature_names = list(features.columns)\nfigure(figsize=(20,18), dpi=80)\nvisualizer = FeatureCorrelation(labels = feature_names, method='pearson')\n\nvisualizer.fit(features, target)\n\nvisualizer.poof()","be686eed":"select_univariate = SelectKBest(f_classif, k=4).fit(features, target)\n\nfeatures_mask = select_univariate.get_support()\n\nfeatures_mask\n\nselected_columns = features.columns[features_mask]\n\nselected_columns\n\nselected_features = features[selected_columns]\n\nselected_features.head()","accc0fa9":"mic = feature_selection.mutual_info_classif( features, target)\nfig, ax = plt.subplots(figsize=(10, 8))\n(pd.DataFrame({\"feature\": features.columns, \"vimp\": mic}).set_index(\"feature\").plot.barh(ax=ax))","db8bcf3c":"from sklearn.feature_selection import RFE\nmodel = RandomForestClassifier( n_estimators=10)\nrfe = RFE(model, 4)\nrfe.fit(features, target)\nprint(rfe.support_)\nprint(features.loc[:, rfe.support_].head())","bac13cd3":"df = train_data[['f61', 'f62', 'f63', 'f64', 'f65', 'f66', 'f67', 'f68', 'f69','f70','target']]","0a6f07a6":"target = df['target']\nfeatures = df.drop('target', axis=1)","e6036793":"feature_names = list(features.columns)\nfigure(figsize=(20,18), dpi=80)\nvisualizer = FeatureCorrelation(labels = feature_names, method='pearson')\n\nvisualizer.fit(features, target)\n\nvisualizer.poof()","7200f129":"select_univariate = SelectKBest(f_classif, k=4).fit(features, target)\n\nfeatures_mask = select_univariate.get_support()\n\nfeatures_mask\n\nselected_columns = features.columns[features_mask]\n\nselected_columns\n\nselected_features = features[selected_columns]\n\nselected_features.head()","3a32b876":"mic = feature_selection.mutual_info_classif( features, target)\nfig, ax = plt.subplots(figsize=(10, 8))\n(pd.DataFrame({\"feature\": features.columns, \"vimp\": mic}).set_index(\"feature\").plot.barh(ax=ax))","7bb2a73b":"from sklearn.feature_selection import RFE\nmodel = RandomForestClassifier( n_estimators=10)\nrfe = RFE(model, 4)\nrfe.fit(features, target)\nprint(rfe.support_)\nprint(features.loc[:, rfe.support_].head())","9472ea2f":"df = train_data[['f71', 'f72', 'f73', 'f74', 'f75', 'f76', 'f77', 'f78', 'f79','f80','target']]","ed20ef4e":"target = df['target']\nfeatures = df.drop('target', axis=1)","d4a7f10b":"feature_names = list(features.columns)\nfigure(figsize=(20,18), dpi=80)\nvisualizer = FeatureCorrelation(labels = feature_names, method='pearson')\n\nvisualizer.fit(features, target)\n\nvisualizer.poof()","86c2f71a":"select_univariate = SelectKBest(f_classif, k=4).fit(features, target)\n\nfeatures_mask = select_univariate.get_support()\n\nfeatures_mask\n\nselected_columns = features.columns[features_mask]\n\nselected_columns\n\nselected_features = features[selected_columns]\n\nselected_features.head()","2ac39cc5":"mic = feature_selection.mutual_info_classif( features, target)\nfig, ax = plt.subplots(figsize=(10, 8))\n(pd.DataFrame({\"feature\": features.columns, \"vimp\": mic}).set_index(\"feature\").plot.barh(ax=ax))","cc9547fc":"from sklearn.feature_selection import RFE\nmodel = RandomForestClassifier( n_estimators=10)\nrfe = RFE(model, 4)\nrfe.fit(features, target)\nprint(rfe.support_)\nprint(features.loc[:, rfe.support_].head())","ef5a0eb0":"df = train_data[['f81', 'f82', 'f83', 'f84', 'f85', 'f86', 'f87', 'f88', 'f89','f90','target']]","9352303d":"target = df['target']\nfeatures = df.drop('target', axis=1)","6f3dc45a":"feature_names = list(features.columns)\nfigure(figsize=(20,18), dpi=80)\nvisualizer = FeatureCorrelation(labels = feature_names, method='pearson')\n\nvisualizer.fit(features, target)\n\nvisualizer.poof()","b6867c55":"select_univariate = SelectKBest(f_classif, k=4).fit(features, target)\n\nfeatures_mask = select_univariate.get_support()\n\nfeatures_mask\n\nselected_columns = features.columns[features_mask]\n\nselected_columns\n\nselected_features = features[selected_columns]\n\nselected_features.head()","7da160d1":"mic = feature_selection.mutual_info_classif( features, target)\nfig, ax = plt.subplots(figsize=(10, 8))\n(pd.DataFrame({\"feature\": features.columns, \"vimp\": mic}).set_index(\"feature\").plot.barh(ax=ax))","f2ec9ed9":"from sklearn.feature_selection import RFE\nmodel = RandomForestClassifier( n_estimators=10)\nrfe = RFE(model, 4)\nrfe.fit(features, target)\nprint(rfe.support_)\nprint(features.loc[:, rfe.support_].head())","67565a05":"df = train_data[['f91', 'f92', 'f93', 'f94', 'f95', 'f96', 'f97', 'f98', 'f99','target']]","49827bc3":"target = df['target']\nfeatures = df.drop('target', axis=1)","19ddae56":"feature_names = list(features.columns)\nfigure(figsize=(20,18), dpi=80)\nvisualizer = FeatureCorrelation(labels = feature_names, method='pearson')\n\nvisualizer.fit(features, target)\n\nvisualizer.poof()","f7268eb7":"select_univariate = SelectKBest(f_classif, k=4).fit(features, target)\n\nfeatures_mask = select_univariate.get_support()\n\nfeatures_mask\n\nselected_columns = features.columns[features_mask]\n\nselected_columns\n\nselected_features = features[selected_columns]\n\nselected_features.head()","f31130da":"mic = feature_selection.mutual_info_classif( features, target)\nfig, ax = plt.subplots(figsize=(10, 8))\n(pd.DataFrame({\"feature\": features.columns, \"vimp\": mic}).set_index(\"feature\").plot.barh(ax=ax))","8d49fcfe":"from sklearn.feature_selection import RFE\nmodel = RandomForestClassifier( n_estimators=10)\nrfe = RFE(model, 4)\nrfe.fit(features, target)\nprint(rfe.support_)\nprint(features.loc[:, rfe.support_].head())","b9f07f88":"del features,target","1d475718":"std = preprocessing.StandardScaler()","34273622":"std = preprocessing.StandardScaler()\ntrain_data[['f3','f4','f8','f10','f14','f16','f17','f20','f21','f22','f25','f27','f31','f32','f34','f40','f41','f43','f44','f50','f54','f55','f57','f60','f62', 'f64', 'f66', 'f67','f71', 'f75', 'f77', 'f80','f81', 'f82', 'f83', 'f90','f91', 'f96', 'f97', 'f98']] = std.fit_transform(train_data[['f3','f4','f8','f10','f14','f16','f17','f20','f21','f22','f25','f27','f31','f32','f34','f40','f41','f43','f44','f50','f54','f55','f57','f60','f62','f64', 'f66', 'f67','f71', 'f75', 'f77', 'f80','f81', 'f82', 'f83', 'f90','f91', 'f96', 'f97', 'f98']])","1d6f7806":"train_data[['f3','f4','f8','f10','f14','f16','f17','f20','f21','f22','f25','f27','f31','f32','f34','f40','f41','f43','f44','f50','f54','f55','f57','f60','f62','f64', 'f66', 'f67','f71', 'f75', 'f77', 'f80','f81', 'f82', 'f83', 'f90','f91', 'f96', 'f97', 'f98']].head()","8ebfd570":"y = train_data[['target']]\nX = train_data[['f3','f4','f8','f10','f14','f16','f17','f20','f21','f22','f25','f27','f31','f32','f34','f40','f41','f43','f44','f50','f54','f55','f57','f60','f62','f64', 'f66', 'f67','f71', 'f75', 'f77', 'f80','f81', 'f82', 'f83', 'f90','f91', 'f96', 'f97', 'f98']]\nX_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.3, random_state=42)\n","f0fd91e5":"lr = LogisticRegression(random_state=42)\n","06a2a53c":"lr.fit(X_train, y_train)\n","950bcc69":"lr.score(X_test, y_test)\n","474aa84f":"y_pred = lr.predict(X_test)","46ea055d":"y_pred","3df31685":"accuracy_score(y_test,y_pred)","c826ece0":"print(classification_report(y_test,y_pred))","be940592":"from yellowbrick.features import FeatureImportances\nfig, ax = plt.subplots(figsize=(16, 14))\nfi_viz = FeatureImportances(lr)\nfi_viz.fit(X, y)\nfi_viz.poof()\n","17b9601a":"X = pd.concat([X_train, X_test])\ny = pd.concat([y_train, y_test])\n\n","d3e40a89":"for model in [LogisticRegression]:\n     skflr = model()\n     skf = StratifiedKFold(n_splits=10, random_state=42)\n     s = cross_val_score(skflr, X, y, scoring=\"roc_auc\", cv=skf)\n     print(\"Accuracy = \", s.mean())\n","1334a9d3":"# Loads test data set\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/test.csv\")\n\n# Removes ID column as it is not required for prediction\ntest.drop([\"id\"], axis=1, inplace=True)","fa0a782e":"# Loads submission data set that acts just as a template for submission\nsubmission = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")","b3f0f81c":"test=test[['f3','f4','f8','f10','f14','f16','f17','f20','f21','f22','f25','f27','f31','f32','f34','f40','f41',\n           'f43','f44','f50','f54','f55','f57','f60','f62','f64', 'f66', 'f67','f71', 'f75', 'f77', 'f80','f81', 'f82', 'f83', 'f90','f91', 'f96', 'f97', 'f98']]","96d66080":"test[['f3','f4','f8','f10','f14','f16','f17','f20','f21','f22','f25','f27','f31','f32','f34','f40','f41',\n      'f43','f44','f50','f54','f55','f57','f60','f62','f64', 'f66', 'f67','f71', 'f75', 'f77', 'f80','f81', 'f82', \n      'f83', 'f90','f91', 'f96', 'f97', 'f98']]= pd.DataFrame(preprocessing.scale(test[['f3','f4','f8','f10','f14','f16','f17','f20','f21','f22','f25','f27','f31','f32','f34','f40','f41',\n           'f43','f44','f50','f54','f55','f57','f60','f62','f64', 'f66', 'f67','f71', 'f75', 'f77', 'f80','f81', 'f82', 'f83', 'f90','f91', 'f96', 'f97', 'f98']]))","a20e93a3":"test[['f3','f4','f8','f10','f14','f16','f17','f20','f21','f22','f25','f27','f31','f32','f34','f40','f41',\n           'f43','f44','f50','f54','f55','f57','f60','f62','f64', 'f66', 'f67','f71', 'f75', 'f77', 'f80','f81', 'f82', 'f83', 'f90','f91', 'f96', 'f97', 'f98']].head()","58d55507":"predictions = lr.predict(test)","c7c8ecdd":"submission[\"target\"] = predictions","f7e0d3cb":"submission","a7e28f32":"submission.to_csv(\".\/submission.csv\", index=False) ## 0.69399","a9e48fde":"nb = GaussianNB()\nnb.fit(X_train,y_train)","c363ceeb":"nb.score(X_test,y_test)","d249f519":"predictions = nb.predict(test)","b3945c6c":"submission[\"target\"] = predictions","7e259bd4":"submission","c0a3e03a":"submission.to_csv(\".\/submission.csv\", index=False) ## 0.63862","2f18b72d":"svc = SVC(kernel='rbf', C=10, gamma=1, cache_size=2000)","cfe968b0":"svc.fit(X_train, y_train)","64a83182":"y_pred = svc.predict(X_test)","d03e32bd":"y_pred=y_pred.reshape(180000,1)","e72da277":"y_test.shape","e7bf8851":"y_pred.shape","9d5b9eca":"np.sum(y_pred==y_test) \/ y_test.shape[0] * 100","fbff4c6e":"# cuml Random Forest params     \ncu_rf_params = { 'n_estimators': 500, 'max_depth':8 } ","6024a97e":"X_train = X_train.astype(np.float32)\ny_train = y_train.astype(np.float32)\nX_test = X_test.astype(np.float32)\ny_test = y_test.astype(np.float32)","d058ffd4":"cu_rf = cuRF(**cu_rf_params)\ncu_rf.fit(X_train, y_train)  ","40a5d64f":"y_pred = cu_rf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","10dde851":"# Create StratifiedKFold object.\nskf = StratifiedKFold(n_splits=10, shuffle=True, random_state= 40)\nval_acc = []\ntest_predictions = []\nsubmission_predictions = []","7bd96677":"for fold, (train_index, test_index) in enumerate(skf.split(X_train, y_train)):\n    x_train_fold, x_test_fold = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train_fold, y_test_fold = y_train.iloc[train_index], y_train.iloc[test_index]\n    print('Fold', fold )\n    \n    cu_rf.fit(x_train_fold, y_train_fold)\n    print(\"score : \",cu_rf.score(x_train_fold, y_train_fold))\n    \n    y_pred = cu_rf.predict(x_test_fold)\n    print(accuracy_score(y_test_fold, y_pred))\n    \n    preds = cu_rf.predict(X_test)\n    test_predictions.append(preds)\n    \n    submission = cu_rf.predict(test)\n    submission_predictions.append(submission)","6435eff0":"y_pred = np.mean(np.column_stack(test_predictions), axis=1)\ny_pred = y_pred.astype('int32')","7524b8c6":"y_test = y_test.astype('int32')","f1eaee20":"print(accuracy_score(y_test, y_pred))","57321705":"submission_preds = np.mean(np.column_stack(submission_predictions), axis=1)\nsubmission_preds = submission_preds.astype('int32')","b0037ffa":"#submission[\"target\"] = submission_preds","65725af1":"#submission","b3bd6a44":"#submission['target'].value_counts()","9563ec2d":"#submission.to_csv(\".\/submission.csv\", index=False) ## 0.64382","12df396e":"# Create StratifiedKFold object.\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state= 40)","4b064410":"# Performs cross validation on XGB Classifier\n\nmodel = XGBClassifier(n_estimators=500,objective='binary:logistic', eval_metric='auc',tree_method='gpu_hist')\nmodel_score = cross_val_score(model, X, y, scoring='roc_auc', cv=skf.split(X, y), n_jobs=-1, verbose=10)","9d14d5c1":"print(model_score.mean())","bea7efcf":"del model_score, model","899dbe32":"parameter_space = {\n    'learning_rate': (0.01, 1.0),\n    'n_estimators': (100, 1000),\n    'max_depth': (2,10),\n    'subsample': (0.4, 1.0),\n    'colsample_bytree' :(0.4, 1.0),\n    'gamma': (0, 5)}\n\ndef xgboost_hyper_param(learning_rate,\n                        n_estimators,\n                        max_depth,\n                        subsample,\n                        colsample_bytree,\n                        gamma):\n\n    max_depth = int(max_depth)\n    n_estimators = int(n_estimators)\n\n    clf = XGBClassifier(\n        tree_method='gpu_hist',\n        max_depth=max_depth,\n        learning_rate=learning_rate,\n        n_estimators=n_estimators,\n        objective = 'binary:logistic',\n        eval_metric='auc',\n        gamma=gamma)\n    return np.mean(cross_val_score(clf, X, y, cv=5, scoring='roc_auc'))\n\noptimizer = BayesianOptimization(\n    f=xgboost_hyper_param,\n    pbounds=parameter_space,\n    random_state=100,\n)","a7505a4b":"import warnings\nwarnings.filterwarnings('ignore')","7707deb4":"optimizer.maximize(init_points=2, n_iter=5, acq='ei', xi=0.0)","85df0960":"optimizer.res","8f51a7ea":"params_gbm = optimizer.max['params']\nparams_gbm['max_depth'] = round(params_gbm['max_depth'])\nparams_gbm['n_estimators'] = round(params_gbm['n_estimators'])\nparams_gbm","15a5855f":"X_train, X_test, y_train, y_test = train_test_split(X, y,stratify=y, test_size=0.3, random_state=42)","59c1937e":"dtrain = xgb.DMatrix(data=X_train, label=y_train)\ndval = xgb.DMatrix(data=X_test, label=y_test)\ndel X_train,y_train,X_test,y_test\nparams = {'colsample_bytree': 0.7668000197236247,\n 'gamma': 4.791427871088578,\n 'learning_rate': 0.015842405367374232,\n 'max_depth': 8,\n 'n_estimators': 900,\n 'subsample': 0.6733049507073745}\n\nparams[\"max_depth\"] = int(params[\"max_depth\"])\nparams[\"objective\"] = \"binary:logistic\"\nparams[\"eval_metric\"] = \"auc\"\nparams[\"tree_method\"] = \"gpu_hist\"\n    \nmodel = xgb.train(\n        params, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=200)","f17ac808":"dtest = xgb.DMatrix(data=test)\npredictions = model.predict(dtest)","588b44ea":"#submission[\"target\"] = predictions","24e5430f":"#submission","88dbbd2c":"# Saves test predictions\n#submission.to_csv(\".\/submission.csv\", index=False) # 0.72384","cdf46cc1":"del model, predictions","d539fe5c":"fold_no = 1\nfor train_index, test_index in skf.split(X, y):\n    print('Fold = ',fold_no)\n    y_val = y.iloc[test_index]\n    dtrain = xgb.DMatrix(data=X.iloc[train_index], label=y.iloc[train_index])\n    dval = xgb.DMatrix(data=X.iloc[test_index], label=y.iloc[test_index])\n    fold_no +=1","d134602f":"hyperparameter_space = { \n                        'learning_rate': hp.uniform('learning_rate', 0.01, 0.3),\n                        'max_depth': hp.quniform(\"max_depth\", 2, 6, 1),\n                        'min_child_weight' : hp.quniform('min_child_weight', 1, 8, 1),\n                        'reg_alpha' : hp.uniform('reg_alpha', 1e-8, 100),\n                        'reg_lambda' : hp.uniform('reg_lambda', 1e-8, 100),\n                        'gamma': hp.uniform ('gamma', 0.0, 1.0),\n                        'subsample': hp.uniform(\"subsample\", 0.1, 1.0),\n                        'colsample_bytree': hp.uniform('colsample_bytree', 0.1, 1.0)\n                       }","caee5a7f":"def optimize_hyppara(hyperparameter_space):\n    # Converts parameter value to int as required by XGBoost\n    hyperparameter_space[\"max_depth\"] = int(hyperparameter_space[\"max_depth\"])\n    hyperparameter_space[\"objective\"] = \"binary:logistic\"\n    hyperparameter_space[\"eval_metric\"] = \"auc\"\n    hyperparameter_space[\"tree_method\"] = \"gpu_hist\"\n    \n    model = xgb.train(\n        hyperparameter_space, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=False)\n    \n    predictions = model.predict(dval)\n    \n    roc_auc = roc_auc_score(y_val, predictions)\n    \n    del predictions, model, hyperparameter_space\n    \n    return {\"loss\": -roc_auc, \"status\": STATUS_OK}","20712afb":"# Starts hyperparameters tuning\ntrials = Trials()\nbest_model_params = fmin(fn=optimize_hyppara,space=hyperparameter_space, max_evals=50,algo=tpe.suggest,trials=trials)","61cec081":"best_model_params","c1427bbb":"del dtrain, dval,y_val","1b306286":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\ndtrain = xgb.DMatrix(data=X_train, label=y_train)\ndval = xgb.DMatrix(data=X_test, label=y_test)\ndel X_train,y_train,X_test,y_test\nparams = {'colsample_bytree': 0.10515757933101197,\n 'gamma': 0.6542994638529418,\n 'learning_rate': 0.06786577881330061,\n 'max_depth': 2.0,\n 'min_child_weight': 6.0,\n 'reg_alpha': 27.332999614653477,\n 'reg_lambda': 30.889493181700296,\n 'subsample': 0.9965789712247324}\n\nparams[\"max_depth\"] = int(params[\"max_depth\"])\nparams[\"objective\"] = \"binary:logistic\"\nparams[\"eval_metric\"] = \"auc\"\nparams[\"tree_method\"] = \"gpu_hist\"\n    \nmodel = xgb.train(\n        params, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=200)","5f7ba76f":"# Adds other important parameters\nbest_model_params[\"max_depth\"] = int(best_model_params[\"max_depth\"])\nbest_model_params[\"objective\"] = \"binary:logistic\"\nbest_model_params[\"eval_metric\"] = \"auc\"\nbest_model_params[\"tree_method\"] = \"gpu_hist\"","ca5f09a1":"dtest = xgb.DMatrix(data=test)\npredictions = model.predict(dtest)","3117a5bc":"#submission[\"target\"] = predictions\n\n# Checks for sumbission file before saving\n#submission","0dace7e9":"# Saves test predictions\n#submission.to_csv(\".\/submission.csv\", index=False) # 0.72613","86dd3993":"del model, dtest, predictions","7c99a05e":"# Gets the model trained over cross validation and predictions \n# against each iteration is stored\n\ntest_predictions = []\n\ndtest = xgb.DMatrix(data=test)\n\nfor fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n    print(\"fold\", fold)\n\n    dtrain = xgb.DMatrix(data=X.iloc[train_index], label=y.iloc[train_index])\n    dval = xgb.DMatrix(data=X.iloc[val_index], label=y.iloc[val_index])\n    \n    model = xgb.train(\n        best_model_params, \n        dtrain, \n        num_boost_round=2000, \n        evals=[(dtrain, 'train'), (dval, 'eval')],\n        early_stopping_rounds=50, verbose_eval=200)\n    \n    predictions = model.predict(dtest)\n    \n    test_predictions.append(predictions)\n    \n    del predictions, model, dval, dtrain","b855e565":"test_predictions","cd0b26fc":"del dtest","222dd7ce":"# Predictions stored against each cross validation iteration finally gets aeveraged\n# and target column is set with that averaged predictions\n#submission[\"target\"] = np.mean(np.column_stack(test_predictions), axis=1)\n\n# Checks for sumbission file before saving\n#submission","ebdc1ffa":"# Saves test predictions\n#submission.to_csv(\".\/submission.csv\", index=False # 0.72667","2bf6f140":"model = LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=- 1, learning_rate=0.1, n_estimators=300, device = \"gpu\")","0ae0f77d":"# Performs cross validation on LGBM Classifier\nmodel_score = cross_val_score(model, X, y, scoring='roc_auc', cv=skf.split(X, y), n_jobs=-1, verbose=10)","b63fd6d1":"print(model_score.mean())","d7254051":"del model, model_score","5e190e87":"parameter_space = {\n    'learning_rate': (0.01, 1.0),\n    'n_estimators': (100, 1000),\n    'num_leaves': (80, 100),\n    'feature_fraction': (0.1, 0.9),\n    'bagging_fraction': (0.8, 1),\n     'max_depth': (17, 25),\n     'min_split_gain': (0.001, 0.1),\n     'min_child_weight': (10, 25),\n     'colsample_bytree' :(0.4, 1.0)\n   }\n\n","4813475a":"def lgb_hyper_param(learning_rate,\n                        n_estimators,\n                        num_leaves,\n                        feature_fraction,\n                        bagging_fraction,\n                        max_depth,\n                        min_split_gain,\n                        min_child_weight,\n                        colsample_bytree):\n\n    max_depth = int(max_depth)\n    n_estimators = int(n_estimators)\n\n    clf = LGBMClassifier(\n        device='gpu',\n        boosting_type='gbdt',\n        max_depth=max_depth,\n        learning_rate=learning_rate,\n        n_estimators=n_estimators,\n        objective = 'binary',\n        metric='auc')\n    return np.mean(cross_val_score(clf, X, y, cv=5, scoring='roc_auc'))","80c72fdf":"optimizer = BayesianOptimization(\n    f=lgb_hyper_param,\n    pbounds=parameter_space,\n    random_state=100,\n)","bcbce00e":"optimizer.maximize(init_points=2, n_iter=5, acq='ei', xi=0.0)","2337f752":"optimizer.res","b5b8cb95":"params_gbm = optimizer.max['params']\nparams_gbm['max_depth'] = round(params_gbm['max_depth'])\nparams_gbm['n_estimators'] = round(params_gbm['n_estimators'])\nparams_gbm","7b6a8b44":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\ndtrain = lgb.Dataset(X_train, label=y_train)\ndval = lgb.Dataset(X_test, label=y_test,reference=dtrain)","14ff8e26":"del X_train,y_train,X_test,y_test\nparams ={'bagging_fraction': 0.9428901602120185,\n 'colsample_bytree': 0.4155177024177823,\n 'feature_fraction': 0.8821389082620034,\n 'learning_rate': 0.23807965335569445,\n 'max_depth': 19,\n 'min_child_weight': 13.197890748572707,\n 'min_split_gain': 0.0887958747434185,\n 'n_estimators': 415,\n 'num_leaves': 82.36988658853868}\n\nparams[\"max_depth\"] = int(params[\"max_depth\"])\nparams[\"num_leaves\"] = int(params[\"num_leaves\"])\nparams[\"objective\"] = \"binary\"\nparams[\"metric\"] = \"auc\"\nparams[\"device\"] = \"gpu\"\n    \nmodel = lgb.train(\n        params, \n        dtrain, \n        num_boost_round=2000, \n        valid_sets =(dval,),\n        early_stopping_rounds=50, verbose_eval=200)","7abfea0d":"\npredictions = model.predict(test)","7aacd927":"predictions","57e103d9":"#submission[\"target\"] = predictions","228bd826":"#submission","40be1d7e":"# Saves test predictions\n#submission.to_csv(\".\/submission.csv\", index=False) # 0.71","02c392da":"fold_no = 1\nfor train_index, test_index in skf.split(X, y):\n    print('Fold = ',fold_no)\n    y_val = y.iloc[test_index]\n    dtrain =  lgb.Dataset(data=X.iloc[train_index], label=y.iloc[train_index])\n    dval =  lgb.Dataset(data=X.iloc[test_index], label=y.iloc[test_index])\n    fold_no +=1                    ","3ff11b64":"hyperparameter_space = { \n                        'learning_rate': hp.uniform('learning_rate', 0.01, 1.0),\n                        'max_depth': hp.quniform(\"max_depth\", 2, 20,1),\n                        'n_estimators': hp.uniform(\"n_estimators\",100,1000),\n                        'num_leaves': hp.uniform(\"num_leaves\", 80,100),\n                        'feature_fraction': hp.uniform(\"feature_fraction\",0.1, 0.9),\n                        'bagging_fraction': hp.uniform(\"bagging_fraction\",0.8, 1),\n                        'min_split_gain' : hp.uniform('min_split_gain', 0.001, 0.1),\n                        'min_child_weight': hp.quniform('min_child_weight',10, 25,1),\n                        'reg_alpha' : hp.uniform('reg_alpha', 1e-8, 100),\n                        'reg_lambda' : hp.uniform('reg_lambda', 1e-8, 100)\n                       }","fcca9e8a":"def optimize_hyppara(hyperparameter_space):\n    # Converts parameter value to int as required by XGBoost\n    hyperparameter_space[\"max_depth\"] = int(hyperparameter_space[\"max_depth\"])\n    hyperparameter_space[\"n_estimators\"] = int(hyperparameter_space[\"n_estimators\"])\n    hyperparameter_space[\"num_leaves\"] = int(hyperparameter_space[\"num_leaves\"])\n    hyperparameter_space[\"objective\"] = \"binary\"\n    hyperparameter_space[\"metric\"] = \"auc\"\n    hyperparameter_space[\"device\"] = \"gpu\"\n    \n    model = lgb.train(\n        hyperparameter_space, \n        dtrain, \n        num_boost_round=2000, \n        valid_sets=[dval],\n        early_stopping_rounds=50, verbose_eval=False,valid_names=['valid'])\n   \n    score = model.best_score['valid']['auc']\n   \n    return score\n    \n    #del predictions, model, hyperparameter_space\n    \n    #return {\"loss\": -roc_auc, \"status\": STATUS_OK}","31807e4b":"# Starts hyperparameters tuning\ntrials = Trials()\nbest_model_params = fmin(fn=optimize_hyppara,space=hyperparameter_space, max_evals=50,algo=tpe.suggest,trials=trials)","220ae193":"best_model_params","5876a8fb":"params = {'bagging_fraction': 0.9115199584760522,\n 'feature_fraction': 0.8903279132717933,\n 'learning_rate': 0.9991958623009387,\n 'max_depth': 20.0,\n 'min_child_weight': 25.0,\n 'min_split_gain': 0.08401863889755357,\n 'n_estimators': 747.8900198385907,\n 'num_leaves': 99.60204048837596,\n 'reg_alpha': 35.87885686359565,\n 'reg_lambda': 98.47582584848364}\n\nparams[\"max_depth\"] = int(params[\"max_depth\"])\nparams[\"n_estimators\"] = int(params[\"n_estimators\"])\nparams[\"num_leaves\"] = int(params[\"num_leaves\"])\nparams[\"objective\"] = \"binary\"\nparams[\"metric\"] = \"auc\"\nparams[\"device\"] = \"gpu\"\n    \nmodel = lgb.train(\n        params, \n        dtrain, \n        num_boost_round=2000, \n        valid_sets=[dval],\n        early_stopping_rounds=100, verbose_eval=200)","d821e58b":"predictions = model.predict(test)","36b40838":"#submission[\"target\"] = predictions\n\n# Checks for sumbission file before saving\n#submission","96fa4aa7":"# Saves test predictions\n#submission.to_csv(\".\/submission.csv\", index=False) # 0.69651","738ef154":"# Gets the model trained over cross validation and predictions \n# against each iteration is stored\n\ntest_predictions = []","cf28436a":"best_model_params[\"max_depth\"] = int(best_model_params[\"max_depth\"])\nbest_model_params[\"n_estimators\"] = int(best_model_params[\"n_estimators\"])\nbest_model_params[\"num_leaves\"] = int(best_model_params[\"num_leaves\"])\nbest_model_params[\"objective\"] = \"binary\"\nbest_model_params[\"metric\"] = \"auc\"\nbest_model_params[\"device\"] = \"gpu\"","52f3f06f":"for fold, (train_index, val_index) in enumerate(skf.split(X, y)):\n    print(\"fold\", fold)\n\n    dtrain = lgb.Dataset(data=X.iloc[train_index], label=y.iloc[train_index])\n    dval = lgb.Dataset(data=X.iloc[val_index], label=y.iloc[val_index])\n    \n    model = lgb.train(\n        best_model_params, \n        dtrain, \n        num_boost_round=2000, \n        valid_sets=[dval],\n        early_stopping_rounds=50, verbose_eval=200)\n    \n    predictions = model.predict(test)\n    \n    test_predictions.append(predictions)\n    \n    del predictions, model, dval, dtrain","606df13f":"test_predictions","47128115":"# Predictions stored against each cross validation iteration finally gets aeveraged\n# and target column is set with that averaged predictions\n#@submission[\"target\"] = np.mean(np.column_stack(test_predictions), axis=1)","f718a78c":"# Checks for sumbission file before saving\n#submission","a2128b26":"# Saves test predictions\n#submission.to_csv(\".\/submission.csv\", index=False )","29496a18":"# split dataset into train and test sets\nX_train_full, X_test, y_train_full, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n# split training set into train and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.33, random_state=1)\n# summarize data split\nprint('Train: %s, Val: %s, Test: %s' % (X_train.shape, X_val.shape, X_test.shape))","aeef4124":"# get a list of base models\ndef get_models():\n    models = list()\n    models.append(('lr', LogisticRegression()))\n    models.append(('bayes', GaussianNB()))\n    return models","8d3e1f9a":"# fit the blending ensemble\ndef fit_ensemble(models, X_train, X_val, y_train, y_val):\n    # fit all models on the training set and predict on hold out set\n    meta_X = list()\n    for name, model in models:\n        # fit in training set\n        model.fit(X_train, y_train)\n        # predict on hold out set\n        yhat = model.predict(X_val)\n        # reshape predictions into a matrix with one column\n        yhat = yhat.reshape(len(yhat), 1)\n        # store predictions as input for blending\n        meta_X.append(yhat)\n        # create 2d array from predictions, each set is an input feature\n        meta_X = np.hstack(meta_X)\n        # define blending model\n        blender = LogisticRegression()\n        # fit on predictions from base models\n        blender.fit(meta_X, y_val)\n        return blender","cd9419f5":"# make a prediction with the blending ensemble\ndef predict_ensemble(models, blender, X_test):\n    # make predictions with base models\n    meta_X = list()\n    for name, model in models:\n        # predict with base model\n        yhat = model.predict(X_test)\n        # reshape predictions into a matrix with one column\n        yhat = yhat.reshape(len(yhat), 1)\n        # store prediction\n        meta_X.append(yhat)\n        # create 2d array from predictions, each set is an input feature\n        meta_X = np.hstack(meta_X)\n        # predict\n        return blender.predict(meta_X)","f7ce9d2f":"...\n# create the base models\nmodels = get_models()\n# train the blending ensemble\nblender = fit_ensemble(models, X_train, X_val, y_train, y_val)\n# make predictions on test set\nyhat = predict_ensemble(models, blender, X_test)","5c7bf777":"...\n# evaluate predictions\nscore = accuracy_score(y_test, yhat)\nprint('Blending Accuracy: %.3f' % score)","7eef9075":"predictions = predict_ensemble(models, blender, test)","b00aa828":"predictions","570c8843":"#submission[\"target\"] = predictions\n#submission","87d624d5":"# Saves test predictions\n#submission.to_csv(\".\/submission.csv\", index=False ) #0.69408","d1160462":"model1 = LogisticRegression(random_state=123)\nmodel1.fit(X_train, y_train)\n","97a191dc":"model2 = GaussianNB()\nmodel2.fit(X_train,y_train)","29525b31":"model3 = xgboost.XGBClassifier(\n        tree_method='gpu_hist',\n        objective = 'binary:logistic',\n        eval_metric='auc')\nmodel3.fit(X_train,y_train)","e45154b3":"preds1=model1.predict(X_val)\npreds2=model2.predict(X_val)\npreds3=model3.predict(X_val)","bc0a7c4a":"\ntest_preds1=model1.predict(X_test)\ntest_preds2=model2.predict(X_test)\ntest_preds3=model3.predict(X_test)","ac979390":"final_preds1=model1.predict(test)\nfinal_preds2=model2.predict(test)\nfinal_preds3=model3.predict(test)","f351fe95":"# form a new dataset for valid and test via stackng the predictions\n\nstacked_predictions = np.column_stack((preds1,preds2,preds3))\nstacked_predictions","eb06e413":"stacked_test_preds = np.column_stack((test_preds1,test_preds2,test_preds3))\nstacked_test_preds","95717050":"final_stacked_test_preds = np.column_stack((final_preds1,final_preds2,final_preds3))\nfinal_stacked_test_preds","0dbe5c0d":"model = LogisticRegression(random_state=123)\nmodel.fit(stacked_predictions,y_val)\nmodel.score(stacked_test_preds,y_test)","6b23d8be":"#submission[\"target\"] = final_stacked_test_preds\n#submission","f5a4353c":"# Saves test predictions\n#submission.to_csv(\".\/submission.csv\", index=False ) # 0.69408","e8586da7":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping","6d2f6fe3":"\nearly_stopping = EarlyStopping(\n min_delta=0.001, # Minimium amount of change to count as an improvement\n patience=10, # How many epochs to wait before stopping\n restore_best_weights=True)","eafc5230":"\nmodel = Sequential([\n Dense(80, activation='relu', input_shape=[X_train.shape[1]]),\n Dropout(0.2),\n BatchNormalization(),\n Dense(40, activation='relu'),\n Dropout(0.2),\n BatchNormalization(),\n Dense(20, activation='relu'),\n Dropout(0.2),\n BatchNormalization(),\n Dense(1, activation='sigmoid')])\n","eb62c5c1":"model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['AUC'])","955340d5":"BATCH_SIZE=128","bc3eb3ec":"model.fit(\n X_train, y_train,\n validation_data=(X_val, y_val),\n batch_size=BATCH_SIZE,\n epochs=20,\n callbacks=[early_stopping], \n verbose=1) ","e600300e":"model.evaluate(X_test, y_test, verbose=0)","89ce6a01":"predictions = (model.predict(X_test) > 0.5).astype(\"int32\")","13f939bc":"print(classification_report(y_test,predictions))","f9d2e199":"preds = (model.predict(test) > 0.5).astype(\"int32\")","28d27299":"preds","0b529841":"#submission[\"target\"] = preds","7abfa82e":"#submission","173b3126":"#submission.to_csv(\".\/submission.csv\", index=False ) # 0.693","9a2b5ca7":"#### f21 to f30 features","5dac5d8b":"# RF","02a59ca6":"# LightGBM and GPU and Tuning","6b8c80d3":"#### StratifiedKFold Cross Validation","42e285dc":"#### f41,f43,f44,f50 features","38d7296e":"#### f61 to f70","e403ad35":"# Normalization","f842ba12":"# Stacking","32aa5e05":"#### f14,f16,f17,f20","e0bbfdb1":"#### f11 to f20 features","8fb89e81":"#### f41 to f50 features","0b57fbfa":"#### f81 to f90","01e5d40a":"####  f21, f22, f24, f25, f26, f27, f30 are the features to consider","3b2840e6":"# Sequential Dense Model (ANN) with Keras and Tensorflow","3fb03a58":"#### f62, f64, f66, f67","eedcd5ca":"# EDA","ff51ef43":"#### f41, f43, f44,f47,f49,f50","20c6cd6e":"# SVM","ca1a10d0":"#### f91 to f100","3b71cf62":"# Objective is binary classification given the set of features","489adf89":"#### we will work with features f21, f22, f25, f27","99d2157f":"#### f31,f34,f40","cfa03f38":"#### Stratified cross validation","7be26dba":"#### f3,f4,f8,f10","d9c04c3c":"# XGBoost","0c03caf8":"#### f3, f4, f8, f9, f10","b5ba1bfa":"#### https:\/\/machinelearningmastery.com\/blending-ensemble-machine-learning-with-python\/","e0a40280":"#### Convert the datatypes to np.float32 because some cuML models require the input to be np.float32.","65ed2312":"# Data ","fa090952":"# Hyperparameter Tuning with Bayesian Optimization","b6e05ee9":"# Feature Selection with YellowBricks, SelectKBest and Sklearn Mutual Information, Recursive Feature Elimination","851f33bd":"#### f91, f96, f97, f98","d5487032":"#### f71, f75, f77, f80","87aef8ba":"#### With SelectKBest","70d0af29":"#### f31 to f40 features","eca86ed8":"# Libraries","1068d335":"#### f54, f55, f57, f60","8b8a0c84":"#### f81, f82, f83, f90","4e815d40":"#### f71 to f80","5a5ed830":"#### will consider f31, f32, f34, f40","20134a3f":"# b) Hyperopt","ff2e33a9":"#### Ten columns at a time subsetting and applying yellowbricks package to get the important features","e05d40ef":"#### f51 to f60","f7942af2":"#### a) bayes_opt","8c1009a3":"# Logistic Regression","10cbddef":"# Naive Bayes","4225ecdc":"# Blending","1b298cde":"#### f14,f16,f17, f20"}}