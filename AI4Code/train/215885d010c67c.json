{"cell_type":{"59388fd5":"code","2eb90cde":"code","95d7314f":"code","409474bd":"code","29fde196":"code","18a3fd19":"code","3e2426cb":"code","3e62365c":"code","b79b70b7":"code","51f86cf0":"code","ae1d2f95":"code","992386b6":"code","0dcb8a25":"code","2cd19498":"code","c4d0edf6":"code","b5b3ad75":"code","31c68165":"code","0a33e476":"code","29bc51ad":"code","6499bb67":"code","ca208318":"code","3e5705bd":"code","474b3f63":"code","38834c12":"code","66bf6e35":"code","d13a1a62":"code","cca88e5e":"code","2029f169":"code","30544bf8":"code","02826847":"code","cd3ac31f":"code","7ba355a7":"code","08a3983f":"code","b2e3645e":"code","76ff357f":"code","4eff68a9":"code","ba3c89c8":"code","8c06dee3":"code","294d7393":"code","4fca86d5":"code","1e41117c":"code","4fce7a3c":"code","b6777846":"code","ccaeec0c":"code","2db733d0":"code","41804d4d":"code","0fb23775":"code","cb800798":"code","f60c30ac":"code","1503d3ab":"code","ab228d67":"code","f694779e":"code","833525aa":"code","852c7efc":"code","32893768":"markdown","908ca0c6":"markdown","bc1a091b":"markdown","9fc67675":"markdown","a5d44581":"markdown","e7d2f997":"markdown","ff493a3d":"markdown","aac7e9f4":"markdown","90a61e78":"markdown","7d9b228d":"markdown","cce9cfcf":"markdown","376300ac":"markdown","3b859bdd":"markdown","628c8527":"markdown","975c3d62":"markdown","ae9565f7":"markdown","1ae508f0":"markdown","671c986b":"markdown","0bd09aa3":"markdown","812cdc94":"markdown","1043500b":"markdown","953222d9":"markdown","39dfd6f5":"markdown","ccd0b0e6":"markdown","7d9feb32":"markdown","350919ef":"markdown","86490c49":"markdown","ca91efd2":"markdown","b706ed74":"markdown","ca0c6452":"markdown","d0067a06":"markdown","847fc4bc":"markdown","44a0c456":"markdown","87439cd1":"markdown","3cf7e9e9":"markdown","83cc8f05":"markdown","5561118c":"markdown"},"source":{"59388fd5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2eb90cde":"#Setting the seed value to get consistent results \n\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n\nseed_value= 0\n\nos.environ['PYTHONHASHSEED']=str(seed_value)\n\nimport random\nrandom.seed(seed_value)\n\nnp.random.seed(seed_value)\n\nimport tensorflow as tf\n\ntf.random.set_seed(seed_value)\n\n","95d7314f":"from keras import backend as K\nsession_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1,\n                                        allow_soft_placement=True) \nsess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\ntf.compat.v1.keras.backend.set_session(sess)","409474bd":"#imports\n\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.compose import ColumnTransformer\nfrom keras import layers\nfrom keras.layers import Input, Dense, Activation\nfrom keras.models import Model\nfrom keras.preprocessing import image\nfrom keras.utils import layer_utils\nfrom keras.utils import plot_model\nfrom keras.layers import Dropout\nfrom keras.regularizers import l2\nfrom keras.regularizers import l1\n#np.random.seed(5)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","29fde196":"df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","18a3fd19":"#Lets take a look at the data\ndf.head()","3e2426cb":"df.info()","3e62365c":"#Good news! we have no NAN values in our dataset\ndf.isnull().sum()","b79b70b7":"#Exploring age distribution\nplt.figure(figsize=(15,8))\nsns.distplot(df.age,color='#86bf91')","51f86cf0":"labels=['Male','Female']\nplt.figure(figsize=(6,6))\nplt.pie(df.sex.value_counts(), labels=labels, autopct='%1.1f%%', shadow=True);\n","ae1d2f95":"#Binning the age data to see how aging affects heart disease\nbins = np.linspace(df.age.min(), df.age.max(),8)\nbins = bins.astype(int)\ndf_age = df.copy()\ndf_age['binned'] = pd.cut(df_age['age'],bins=bins)\ndf_age.head()","992386b6":"#Lets take a look at the target distribution with respect to ages\nimport seaborn as sns\nplt.figure(figsize=(15,8))\nsns.countplot(x='binned',hue = 'target',data = df_age,edgecolor=None)","0dcb8a25":"df_age.binned.value_counts()","2cd19498":"bins = [df.age.min(), 35, 55, np.inf]\nlabels = ['young','middle','older']\ndf_cat = df.copy()\ndf_cat['binned'] = pd.cut(df_cat['age'],bins=bins,labels=labels)\ndf_cat.head()","c4d0edf6":"\nax = df_cat.groupby('binned')['cp'].value_counts(normalize=True).unstack('cp').plot(kind='bar',figsize=(15,9),rot=0)\nax.set_xlabel(\"Binned Age Groups\");\nax.set_ylabel(\"CP Percentages\");","b5b3ad75":"ax = df_cat.groupby('cp')['target'].value_counts(normalize=True).unstack('target').plot(kind='bar',figsize=(15,9),rot=0)\nax.legend(['Disease Free','Has Disease'])\nax.set_xlabel('Chest Pain Type');\nax.set_ylabel('Percentages');","31c68165":"df_cat.groupby('target')['cp'].value_counts().unstack('cp').plot(kind='bar',figsize=(15,9),rot=0)","0a33e476":"bins = np.linspace(df_cat.chol.min(),df_cat.chol.max(),30)\nplt.figure(figsize=(15,8))\nsns.distplot(df_cat.chol,bins=bins)","29bc51ad":"\n    \nbins = [df_cat.chol.min(), 200, 239,np.inf]\nlabels = ['Normal','Borderline','High']\ndf_chol = df_cat.copy(deep=True)\ndf_chol['chol_bin'] = pd.cut(df_cat['chol'],bins = bins,labels = labels)\nplt.figure(figsize=(15,8))\nsns.countplot(x = 'chol_bin',data=df_chol,hue='target')","6499bb67":"ax = df.groupby('ca')['target'].value_counts(normalize=True).unstack('target').plot(kind='bar',figsize=(15,9),rot=0)\nax.legend(['Disease Free','Has Disease'])\nax.set_xlabel('# of major vessels (0-4) colored by flourosopy',fontdict={'fontsize':14});\nax.set_ylabel('Percentages',fontdict={'fontsize':14});","ca208318":"df.ca.value_counts()","3e5705bd":"ax = df.groupby('thal')['target'].value_counts(normalize=True).unstack('target').plot(kind='bar',figsize=(15,9),rot=0)\nax.legend(['Disease Free','Has Disease'])\nax.set_xlabel('Thalemesia')\nax.set_ylabel = ('Percentages')","474b3f63":"df.thal.value_counts()","38834c12":"#First define the limit for normal heart rate limit for each patient according to their age category\n\ndf_t = df_cat.copy(deep=True)\ndf_t.loc[df_t.binned=='young','hr_bin'] = 200\ndf_t.loc[df_t.binned=='middle','hr_bin'] = 185\ndf_t.loc[df_t.binned=='older','hr_bin'] = 160\ndf_t.head()","66bf6e35":"#Then categorizing the heart rate category as Normal or High in thalach_bin\n\ndf_t['thalach_bin'] = np.where(df_t.eval(\"thalach <= hr_bin \"), \"Normal\", \"High\")\ndf_t","d13a1a62":"#grouping df_t to get the counts of the patients in each group\n\ndf_thalach = df_t.groupby(['thalach_bin','target','binned']).count()\ndf_thalach","cca88e5e":"\n\n#Dividing inital values for each age_binned group by summed up  entries per each age_binned group to get percentages\n\ndf_thalach.iloc[[0,3,6,9]]\/= df_thalach.iloc[[0,3,6,9]].age.sum()\ndf_thalach.iloc[[1,4,7,10]]\/= df_thalach.iloc[[1,4,7,10]].age.sum()\ndf_thalach.iloc[[2,5,8,11]]\/= df_thalach.iloc[[2,5,8,11]].age.sum()\n\ndf_thalach = df_thalach.reset_index(level='target')\ndf_thalach = df_thalach.reset_index(level='binned')\ndf_thalach = df_thalach[['age','target','binned']]\ndf_thalach.rename(columns={'age':'density'},inplace=True)\ndf_thalach\n","2029f169":"#df_thalach.reset_index(level='thalach_bin',inplace=True)\n\ndf_thalach = df_thalach.reset_index()\nplt.figure(figsize=(15,8));\ng = sns.FacetGrid(df_thalach, col = 'binned', height=5, aspect=1.2,hue='target');\ng.map(sns.barplot,  \"thalach_bin\", \"density\",alpha=0.6);\nplt.legend();","30544bf8":"ax = df.groupby('restecg')['target'].value_counts(normalize=True).unstack('target').plot(kind='bar',figsize=(15,9),rot=0)\nax.legend(['Disease Free','Has Disease']);\nax.set_xlabel('Resting Electrocardiographic Results');\nax.set_ylabel('Percentages');","02826847":"df.restecg.value_counts()","cd3ac31f":"def trestbps_bin(row):\n    \n    if row['trestbps'] <= 80:\n        value = 'Low'\n    \n    elif row['trestbps'] > 120:\n        value = 'High'\n    else:\n        value = 'Normal'\n        \n    return value","7ba355a7":"df_trestbps = df.copy()\ndf_trestbps['trestbps_bin'] = df.apply(trestbps_bin,axis=1)\n\n\ndf_trestbps","08a3983f":"ax = df_trestbps.groupby('trestbps_bin')['target'].value_counts(normalize=True).unstack('target').plot(kind='bar',figsize=(15,9),rot=0)\nax.legend(['Disease Free','Has Disease'])\nax.set_xlabel('Blood Pressure Bin');\nax.set_ylabel('Percentages');\n","b2e3645e":"df_trestbps.trestbps_bin.value_counts()","76ff357f":"ax = df.groupby('exang')['target'].value_counts(normalize=True).unstack('target').plot(kind='bar',figsize=(15,9),rot=0)\nax.legend(['Disease Free','Has Disease'])\nax.set_xlabel('Exercise Induced Angina');\nax.set_ylabel('Percentages');\n","4eff68a9":"#first seperate categorical and numerical data to apply different transformations\n\ny = df.target\ndf_transformed = df.copy(deep=True)\nnumeric_features = ['age','trestbps','chol','thalach','ca','oldpeak']\ncategorical_features = ['sex','cp','fbs','restecg','exang','slope','thal']\n\nenc = OneHotEncoder(sparse=False,drop='first')\nenc.fit(df_transformed[categorical_features])\n\ncol_names = enc.get_feature_names(categorical_features)\ndf_transformed = pd.concat([df_transformed.drop(categorical_features, 1),\n          pd.DataFrame(enc.transform(df_transformed[categorical_features]),columns = col_names)], axis=1).reindex()\ndf_transformed.head()\n","ba3c89c8":"scaler = StandardScaler()\n\n\ndf_transformed[numeric_features]  = scaler.fit_transform(df_transformed[numeric_features])\ndf_transformed.head()","8c06dee3":"#Split the data \nX_train, X_test, y_train, y_test = train_test_split(df_transformed.drop('target',axis=1), df_transformed['target'], test_size = .2, random_state=10)\nrf_model = RandomForestClassifier(max_depth=5, random_state=137)\nrf_model.fit(X_train, y_train)","294d7393":"from sklearn.metrics import confusion_matrix\ndef conf_matrix(X_test,y_test,model):\n    \n    y_pred = model.predict(X_test)\n    return confusion_matrix(y_test, y_pred)\n    \n    ","4fca86d5":"conf_matrix(X_test,y_test,rf_model)","1e41117c":"from sklearn.metrics import accuracy_score\ny_test_pred = rf_model.predict(X_test)\naccuracy_score(y_test, y_test_pred)","4fce7a3c":"from sklearn.model_selection import GridSearchCV\nn_estimators = [10, 30, 50, 100]\nmax_depth = [5, 8, 15]\nmin_samples_split = [2, 5, 10, 15, 40]\nmin_samples_leaf = [1, 2, 5, 10] \n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\ngridF = GridSearchCV(rf_model, hyperF, cv = 3, verbose = 1, \n                      n_jobs = -1)\nbestF = gridF.fit(X_train, y_train)","b6777846":"gridF.best_params_","ccaeec0c":"hp_list = gridF.best_params_\n\nimproved_model = RandomForestClassifier(max_depth=hp_list['max_depth'], min_samples_leaf = hp_list['min_samples_leaf'],\n                                        min_samples_split = hp_list['min_samples_split'], n_estimators = hp_list['n_estimators'],random_state = 137)","2db733d0":"improved_model.fit(X_train, y_train)","41804d4d":"conf_matrix(X_test,y_test,improved_model)","0fb23775":"y_test_pred = improved_model.predict(X_test)\naccuracy_score(y_test, y_test_pred)","cb800798":"\ndef model_nn(input_shape):\n    \n\n    # Define the input placeholder as a tensor with shape input_shape\n    X_input = Input(input_shape)\n\n  \n    X = Dense(512,kernel_regularizer=l2(0.01),kernel_initializer = 'random_uniform')(X_input)\n    X = Activation('relu')(X)\n    X = Dropout(0.5,seed=10)(X)\n    X = Dense(256,kernel_regularizer=l2(0.01),kernel_initializer = 'random_uniform')(X)\n    X = Activation('relu')(X)\n    X = Dropout(0.25,seed=10)(X)\n    X = Dense(16,kernel_regularizer=l2(0.01),kernel_initializer = 'random_uniform')(X)\n    X = Activation('relu')(X)\n    X = Dropout(0.25,seed=10)(X)\n    X = Dense(1, activation='sigmoid')(X)\n\n    # Create model. \n    model = Model(inputs = X_input, outputs = X, name='nnModel')\n\n    return model","f60c30ac":"nnModel = model_nn(X_train.shape)\n","1503d3ab":"nnModel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\nnnModel.fit(x = X_train, y = y_train, epochs = 100, batch_size = 16)","ab228d67":"preds = nnModel.evaluate(x= X_test, y=y_test)\nprint()\nprint (\"Loss = \" + str(preds[0]))\nprint (\"Test Accuracy = \" + str(preds[1]))","f694779e":"df_transformed.head()","833525aa":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(rf_model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm,feature_names = X_test.columns.tolist())","852c7efc":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(improved_model, random_state=1).fit(X_test, y_test)\neli5.show_weights(perm,feature_names = X_test.columns.tolist())","32893768":"Enough for chest pain, lets take a look at the other variables. We are always told that high cholestorol is a indicator for the heart diseases. Lets see if our data supports this claim.","908ca0c6":"\n\nChest pain type 0 is the most common one among the patients with no heart disease whereas chest pain type 2 is the most common for sick patients","bc1a091b":"For young aged group, we have only patients with high max heart rate and percentage of patients with disease is higher for this category. Also, heart disease is less common for older patients with a Normal maximum heart rate compared to High maximum heart rate in the same age group.","9fc67675":"National Cholesterol Education Program (NCEP) guidelines provide specific numbers for cholesterol ranges:\nNormal: less than 200 mg\/dL\nBorderline high: 200 to 239 mg\/dL\nHigh: 240 mg\/dL or above\n\nLets bin our data according to this.\n","a5d44581":"Our dataframe is ready to feed into ML model","e7d2f997":" We have %80 test set accuracy with a Random Forest Classifier. Not bad. Let's try to improve this by tuning hyperparameters of the model.","ff493a3d":"This actually matches the analysis done in the beginning. We predicted by looking at the distributions that thal, exang,ca,cp and thalach will be good indicators for heart disease and also predicted that chol, trestbps are not very good indicators. Output of permutation importance lies perfectly with our analysis!","aac7e9f4":"Next feature we will investigate is the Resting electrocardiographic measurement (0 = normal, 1 = having ST-T wave abnormality, 2 = showing probable or definite left ventricular hypertrophy by Estes' criteria) called 'restecg' in our dataset","90a61e78":"First, we will take a look at the distrubition of serum choloestrol with regard to age.","7d9b228d":"Starting our analysis with the feature chest pain type - 'cp'","cce9cfcf":"TIME FOR PREDICTION!","376300ac":"As shown on the plot, \"0\" type chest pain is the most common one among patients in every age group. Another interesting take is that although we do not have any young patients with \"2\" type chest pain, \"2\" type chest pain is the second common type of chest pain among middle and older patients","3b859bdd":"This part is where generating the transformed output as a dataframe comes handy. Weights of Permutation Importance are linked to the columns of the transformed dataframe. Permutation Importance simply shuffles each column in itself and calculates how much loss function suffered from reordering of the column.","628c8527":"Now lets look at the effect of different chest pain types to heart disease","975c3d62":"With the help of neural networks we managed the increase test set accuracy by %2","ae9565f7":"Now on to the variable maximum heart rate called 'thalach' in our dataset.","1ae508f0":"Lets split patients into 3 groups by age. We will be categorizing patients into young adults (ages 18-35 years), middle-aged adults (ages 36-55 years), and older adults (aged older than 55 years).","671c986b":"It seems that chest pain type is a good indicator for the heart disease since the percentages of patients with heart disease is relatively high for type1,type2 and type3 ","0bd09aa3":"Another issue I feel a need to mention is that as seen below, not all types of ca is sampled equally in our dataset. We have 5 patients with ca level 4 whereas we have 175 people with ca level 0.","812cdc94":"Let's continue with the analysis of thalemesia named 'thal' in our dataset.","1043500b":"Interesting, according to our data there is no relationship between cholestoral  levels and a heart disease. Keep in mind that this a small sample size and we cannot make concrete deductions about the whole population. At least we can conclude that cholestoral probably will not be a very important variable for our prediction model. We will come back to this later.","953222d9":"Lets try the pin down which features are the most important for our model.","39dfd6f5":"Lets continue our analysis with the another variables. Well I cheated here a little bit and ask my sister who is a doctor about which variables do she think that most related to heart diseases. Her answer was ca(number of major vessels (0-3) colored by flourosopy),thalassemia(a type of blood disorder) and maximum heart rate . Domain knowledge always comes in handy. So, we will continue our analysis with the variable 'ca'.","ccd0b0e6":"As you can see by the count values of the binned age groups, analyzing the data with counts don't make much sense because some bin counts are lower than the others. For example, we have only 6 people in the bin 70-77 , 29-35 and 89 people in the bin 56-63. Thus, we will be plotting the distrubitions with the percentages from now on.","7d9feb32":"Last but not least, we will be investigating the feature Exercise Induced Angina called 'exang' in out dataset.","350919ef":"For resting blood pressure, we need to bin the blood pressure values. Although, normal rate for blood pressure differs slighltly with age, I will give constant values for blood pressure bin edges as follows:\n\nLow < 80 < Normal < 120 < High","86490c49":"Heart disease is much more common among patients with restecg measured 1 ","ca91efd2":"We managed to improve accuracy on the test set by %3 with fine tuning the hyperparameters","b706ed74":"As shown on the pie chart above,males are oversampled in our dataset.","ca0c6452":"Categorical features of the original dataframe are one hot encoded.Let's move on to the continuous features","d0067a06":"Except the bin with range 56-63, there are more patients with heart disease than patients with no heart disease. This makes sense considering our dataset consists of people that applied to hospitals because of related symptoms.","847fc4bc":"Ca level 0 and 4 has the most patients with a heart disease. In addition, level of differences for each ca level is relatively big compared to other distributions. Maybe my sister was right and number of major vessels could be a good indicator for heart disease.","44a0c456":"Exang looks like a good indicator for a heart disease since for both type 0 and type 1 difference between sick people and healthy people is high.","87439cd1":"Especially thal level 2 seems to be a good indicator since this value is not undersampled in our dataset with a count of 166!","3cf7e9e9":"\nSick patients and healthy patients are nearly equally distributed for high blood pressure levels. Interestingly, for normal blood pressure levels percentage of patients without a heart disease is larger. I don't think this feature will be that important to our models predictions","83cc8f05":"Normal maximum heart rate differs with the age of the patient. Thus we will be categorizing the heart rates of patients according to their ages. \n\nYoung patients -> normal < 200 < High\n\nMiddle aged patients -> normal < 180 < High\n\nOlder patients -> normal < 170 < high\n","5561118c":"We will start by applying one hot encoding to categorical features and standard scaling to numerical features.Normally, these transformations produce numpy arrays instead of pandas dataframe. I will be changing to code a little bit to make the outputs of these transformations pandas dataframe in order to increase the readilibity. After evulation process is complete, generated data frames will be input to the Permuation Importance to figure out which features are most important for our model. Let's begin."}}