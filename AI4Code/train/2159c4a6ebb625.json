{"cell_type":{"08766807":"code","26c563a1":"code","92480b79":"code","79c92173":"code","0a24cceb":"code","206a53a2":"code","027ff37c":"code","f2cda5d0":"code","4f0ac995":"code","fc18db3d":"code","a33d9729":"code","54e16285":"code","222ccf33":"code","e62f6c13":"code","7ffab27a":"code","25f8ac8e":"code","aac724dd":"code","59d7dfa1":"code","8e54d26e":"code","44183419":"code","d7724c48":"code","afc1f530":"code","297e5729":"code","f752b8a3":"code","f94e2d80":"code","c6e64d2e":"code","2b7a4c73":"code","c3fb928f":"code","22da8c84":"markdown","62ad280f":"markdown","9559dcff":"markdown","7dcceb1e":"markdown","7855a770":"markdown","c7e95b23":"markdown","9736ce7c":"markdown","5e0853c6":"markdown","5543ca91":"markdown","9febac36":"markdown","ca5f247d":"markdown","ea776ab3":"markdown","598e80cb":"markdown","6b84b80a":"markdown","c65f9790":"markdown","dce74b1d":"markdown","7978e45f":"markdown"},"source":{"08766807":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n        import re # for regular expressions\nimport pandas as pd \npd.set_option(\"display.max_colwidth\", 200)\nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport string\nimport nltk # for text manipulation\nimport warnings\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\n# Any results you write to the current directory are saved as output.","26c563a1":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsub = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","92480b79":"train.head()","79c92173":"train.target.value_counts()","0a24cceb":"train.shape","206a53a2":"train = train.drop(['keyword'], axis=1)\ntrain = train.drop(['location'], axis=1)","027ff37c":"##Remove Pattern with username @\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt","f2cda5d0":"train['text'] = np.vectorize(remove_pattern)(train['text'], \"@[\\w]*\") \ntrain","4f0ac995":"##Remove all the hyperlinks from the texts\ntrain['text'] = train['text'].str.replace('http\\S+|www.\\S+', '', case=False)\n\ntrain['text'] = train['text'].str.replace(\"[^a-zA-Z#]\", \" \")","fc18db3d":"##Removing all the words with less than 3 characters\ntrain['text'] = train['text'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>2]))","a33d9729":"##Removing stopwords\n# extracting the stopwords from nltk library\nsw = stopwords.words('english')\n# displaying the stopwords\nnp.array(sw);","54e16285":"def stopwords(text):\n    '''a function for removing the stopword'''\n    # removing the stop words and lowercasing the selected words\n    text = [word.lower() for word in text.split() if word.lower() not in sw]\n    # joining the list of words with space separator\n    return \" \".join(text)","222ccf33":"train['text'] = train['text'].apply(stopwords)\ntrain.head(10)","e62f6c13":"# function to collect hashtags\ndef hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags","7ffab27a":"HT_disaster = hashtag_extract(train['text'][train['target'] == 1])\n\n# extracting hashtags from racist\/sexist tweets\nHT_no_disaster = hashtag_extract(train['text'][train['target'] == 0])\n\n# unnesting list\nHT_disaster = sum(HT_disaster,[])\nHT_no_disaster = sum(HT_no_disaster,[])","25f8ac8e":"a = nltk.FreqDist(HT_disaster)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n\n# selecting top 20 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 20) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","aac724dd":"b = nltk.FreqDist(HT_no_disaster)\ne = pd.DataFrame({'Hashtag': list(b.keys()), 'Count': list(b.values())})\n\n# selecting top 20 most frequent hashtags\ne = e.nlargest(columns=\"Count\", n = 20)   \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")","59d7dfa1":"##Stemming Operation\n# create an object of stemming function\nstemmer = SnowballStemmer(\"english\")\n\ndef stemming(text):    \n    '''a function which stems each word in the given text'''\n    text = [stemmer.stem(word) for word in text.split()]\n    return \" \".join(text) ","8e54d26e":"train['text'] = train['text'].apply(stemming)","44183419":"##Vectorizing the Text\nvectorizer = CountVectorizer(analyzer='word', binary=True)\nvectorizer.fit(train['text'])","d7724c48":"X = vectorizer.transform(train['text']).todense()\ny = train['target'].values\nX.shape, y.shape","afc1f530":"##Machine learning the model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score","297e5729":"##Splitting the data into test and train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2020)","f752b8a3":"model = LogisticRegression()\nmodel.fit(X_train, y_train)","f94e2d80":"##Evaluate the Model\nprediction_log = model.predict(X_test) # predicting on the validation set\n\nf1score = f1_score(y_test, prediction_log)\nprint(f\"Model Score: {f1score * 100} %\")","c6e64d2e":"sub_test = test['text']\ntest_X = vectorizer.transform(sub_test).todense()\ntest_X.shape","2b7a4c73":"pred_test = model.predict(test_X)","c3fb928f":"sub['target'] = pred_test\nsub.to_csv(\"submission.csv\", index=False)\nsub.head()","22da8c84":"Check the shape and the number of targets used in the dataset","62ad280f":"Vectorizing the Text","9559dcff":"**LOGISTOIC REGRESSION**","7dcceb1e":"**2**. **Text Processing**\n\nIn this step we will remove the noise from the tweets. For this we will perform the below steps.\n* Removing the pattern used in the tweets text(Eg-: @XXX)\n* Removing the hyperlink provided in the tweets text\n* Removing the special characters from the text\n* Removing all the words which have length less than 3\n* Removing the stopwords\n* Analyzing the #hashtags in the tweets\n* Stemming\n* Vectorizing the text","7855a770":"Removing all the words with less than 3 characters","c7e95b23":"Analyzing the #hashtags in the tweets","9736ce7c":"> Drop the location and keyword column from the dataset.","5e0853c6":"The Logistic Regression Model gave the accuracy of around 76%.\nThe Support Vector and Ramdom Forest Classifier gives around 73.4%.\nSo we will use the log model for submission.","5543ca91":"Removing all the Hyperlinks","9febac36":"**A very Basic Kernel on NLP covering all the basic steps that are used in NLP.\nIn this kernel we will be performing the following steps.\n1. Data gathering\n2. Text Processing\n3. Model Generation\n4. Evaluating the Model\n5. Submission**","ca5f247d":"As expected, most of the terms are negative with a few neutral terms as well. So, it\u2019s not a bad idea to keep these hashtags in our data as they contain useful information. ","ea776ab3":"**1. Data Gathering**","598e80cb":"Stemming the Text","6b84b80a":"**Submission**","c65f9790":"Removing stopwords","dce74b1d":"Removing Pattern","7978e45f":"**Creating the Model.**\n\nWe will try different algorithm and see which model gives the better output.\n1. Logistic Regression\n2. Support Vector \n3. Random Forest Classifier"}}