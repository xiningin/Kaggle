{"cell_type":{"5f749faa":"code","7cc8035b":"code","7cd6b5e4":"code","b9f27252":"code","33992899":"code","78779266":"code","8c75133e":"code","2b00f0d4":"code","911177d2":"code","7ba30843":"code","5fd5e44f":"code","1cf15dc9":"code","35f04ca1":"code","6f463e02":"code","a185cc3b":"code","040a8a1d":"code","270b8622":"code","a313380e":"code","e4e62721":"code","551295c8":"code","e8f80693":"code","ad714d91":"code","f591c110":"code","31d7c80f":"code","2df159a7":"code","042b5393":"code","95bd2571":"code","f1701f3a":"code","75501981":"code","b335f4cb":"code","d09679d7":"code","233bda6d":"code","9b42eeb0":"code","ae38d5b4":"code","61112c12":"code","ac46c9b6":"code","85b71d87":"code","3912ba0e":"markdown","520b497d":"markdown","e5acf219":"markdown","abb3a0cb":"markdown","9e7353bc":"markdown","634ba8b1":"markdown","fa140641":"markdown","6eb5469f":"markdown","d3ce82e1":"markdown","09c07f44":"markdown","ebb1b825":"markdown","917b7038":"markdown","a2b930eb":"markdown","dce6d9c8":"markdown"},"source":{"5f749faa":"import warnings\nwarnings.filterwarnings(action='ignore',category = DeprecationWarning)\nwarnings.simplefilter(action='ignore',category = DeprecationWarning)\n\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport pickle\nimport re\n\nimport os\nimport time\nimport datetime\nimport gc\nimport shutil\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, TimeSeriesSplit, KFold\nfrom sklearn import metrics\nfrom sklearn.utils import class_weight\n\nimport catboost as cb\n\nimport tensorflow as tf\n\nimport skopt.plots\nimport scikitplot as skplt\n\nfrom tqdm import tqdm_notebook\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","7cc8035b":"# Installing the most recent version of skopt directly from Github (from fork which have some bugs fixed)\n!pip install git+https:\/\/github.com\/darenr\/scikit-optimize\n\nimport skopt.plots\n\n# Custom codes to make code clear\n%run ..\/input\/imports\/help_functions.py\n%run ..\/input\/imports\/fixed_bayes_search.py\n\nLOCAL = False\nfraud_data_dir = '..\/input\/ieee-fraud-detection\/'","7cd6b5e4":"data_dir","b9f27252":"data_version = 'v5'\nif LOCAL:\n    data_dir = fraud_data_dir + 'data\/data_' + data_version + '\/'\nelse:\n    data_dir = 'data\/data_' + data_version + '\/'\n\ndata_available = os.path.exists(data_dir + 'data.pkl') and os.path.exists(data_dir + 'submission.pkl')\nload_data = False\n\nuse_catboost = True","33992899":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 250)","78779266":"def load_only_data(data_dir, load_data = False):\n    \n    if load_data or not os.path.exists(data_dir + 'data.pkl'):\n        print(\"Loading data from CSV files\")\n        data_id = pd.read_csv(fraud_data_dir + 'train_identity.csv')\n        data_trans = pd.read_csv(fraud_data_dir + 'train_transaction.csv')\n\n        print(\"Merging data\")\n        data = pd.merge(data_trans, data_id, on='TransactionID', how='left')\n\n        # Ensure that all columns use the proper data type\n        data = reduce_mem_usage(data)\n        \n        print(f\"Shape of data_id is: {data_id.shape}\")\n        print(f\"Shape of data_trans is: {data_trans.shape}\")\n        print(f\"Shape of data is: {data.shape}\")\n\n    else:\n        print(\"Loading data from \",data_dir)\n\n        with open(data_dir + 'data.pkl', 'rb') as file:\n            data = pickle.load(file)\n\n        # Submission deleted and not loaded in order to keep memory lower, and load it when needed\n        #with open(data_dir + 'submission.pkl', 'rb') as file:\n        #    submission = pickle.load(file)\n\n        #if 'submission' in locals():\n        #    del submission\n\n        print(\"Data loaded\")\n        \n    return data\n\ndef load_submission(data_dir, load_data = False):\n    if load_data or not os.path.exists(data_dir + 'submission.pkl'):\n        print(\"Loading submission from CSV files\")\n        submission_id = pd.read_csv(fraud_data_dir + 'test_identity.csv')\n        submission_trans = pd.read_csv(fraud_data_dir + 'test_transaction.csv')\n        \n        print(\"Merging submission\")\n        submission = pd.merge(submission_trans, submission_id, on='TransactionID', how='left')\n        \n        # Ensure that all columns use the proper data type\n        submission = reduce_mem_usage(submission)\n        \n        print(f\"Shape of submission_id is: {submission_id.shape}\")\n        print(f\"Shape of submission_trans is: {submission_trans.shape}\")\n        print(f\"Shape of submission is: {submission.shape}\")\n    else:\n        print(\"Loading submission from \",data_dir)\n        \n        with open(data_dir + 'submission.pkl', 'rb') as file:\n            submission = pickle.load(file)\n\n        print(\"Data loaded\")\n    \n    return submission\n\ndata = load_only_data(data_dir, load_data)\n#submission = load_submission(data_dir, load_data)","8c75133e":"data.head()","2b00f0d4":"print(f\"There are {data.isnull().any().sum()} columns with missing values\")","911177d2":"def remove_unique_and_NaN_columns(df):\n    too_much_NaN_cols = [col for col in df.columns if df[col].isnull().mean() > 0.9]\n    one_value_per_column = [col for col in df.columns if df[col].nunique() <= 1]\n\n    remove_columns = too_much_NaN_cols + one_value_per_column\n\n    print(\"The following columns are removed: \", remove_columns)\n    return remove_columns","7ba30843":"data['isFraud'].value_counts(normalize = True)","5fd5e44f":"print(f\"{data['id_01'].isnull().mean().round(4) * 100}% of transactions does not have associated an identity\")","1cf15dc9":"if load_data or not os.path.exists(data_dir + 'data.pkl'):\n    # Negative downsampling\n    data_pos = data[data['isFraud']==1]\n    data_neg = data[data['isFraud']==0]\n\n    data_neg = data_neg.sample(3*int(data_pos.shape[0] ), random_state=42)\n    data = pd.concat([data_pos,data_neg]).sort_index()\n    print(f\"Shape of data is: {data.shape}\")","35f04ca1":"def unique_identifier(df):\n    df['uid'] =  df['card1'].astype(str)+'_'+df['card2'].astype(str)\n    df['uid_1'] =  df['uid'].astype(str)+'_'+df['card3'].astype(str)\n    df['uid_2'] =  df['uid_1'].astype(str)+'_'+df['card5'].astype(str)\n    df['uid_3'] =  df['uid_2'].astype(str)+'_'+df['addr1'].astype(str)+'_'+df['addr2'].astype(str)\n    df['uid_4'] = df['card4'].astype(str)+'_'+df['card6'].astype(str)\n    df['uid_5'] = df['uid_3'].astype(str)+'_'+df['uid_4'].astype(str)","6f463e02":"def transactionDT(df):\n    # As commented in the forum discussion, this column probably is shown in seconds\n    # If we count as initial moment the first entry which is 86400, we can create some kind of clock\n    df['seconds'] = (df['TransactionDT'] % 60).astype('int8')\n    df['minutes'] = ((df['TransactionDT'] \/ 60) % 60).astype('int8')\n    df['hours'] = ((df['TransactionDT'] \/ 3600) % 24).astype('int8')\n    df['days_week'] = ((df['TransactionDT'] \/ 86400) % 7).astype('int8')\n    df['days_year'] = ((df['TransactionDT'] \/ 86400) % 365).astype('int16')\n    df['harmonic_seconds'] =  make_harmonic_features(df['seconds'], 60)[1].astype('float32')\n    df['harmonic_minutes'] =  make_harmonic_features(df['minutes'], 60)[1].astype('float32')\n    df['harmonic_hours'] =  make_harmonic_features(df['hours'], 24)[1].astype('float32')\n    df['harmonic_days_week'] =  make_harmonic_features(df['days_week'], 7)[1].astype('float32')\n    df['harmonic_days_year'] =  make_harmonic_features(df['days_year'], 365)[1].astype('float32')\n    \n# As a clock pass from 23 to 0 and they are near as 13 to 14, we use an harmonic clock using cos() and sin()\ndef make_harmonic_features(value, period=24):\n    return np.cos(value * 2 * np.pi \/ period), np.sin(value * 2 * np.pi \/ period)    ","a185cc3b":"def time_from_fraud(df, isSubmission = False, time_from_fraud_dict = {}):\n    group_by_columns = ['uid']\n    columns_to_treat = ['minutes_from_last_fraud', 'hours_from_last_fraud']\n    \n    if not isSubmission:\n        df['last_fraud'] = df['TransactionDT'][df['isFraud'] == 1]\n        df['last_fraud'] = df['last_fraud'].shift(1)\n        df['last_fraud'] = df['last_fraud'].fillna(method='ffill')\n\n        df['minutes_from_last_fraud'] = ((df['TransactionDT'] - df['last_fraud']) \/ 60).round(0).astype('Int32')\n        df['hours_from_last_fraud'] = ((df['TransactionDT'] - df['last_fraud']) \/ 3600).round(0).astype('Int32')\n\n        # Create median column of time from last fraud by UID        \n        for column_to_treat in tqdm_notebook(columns_to_treat):\n            df['median_' + column_to_treat] = df.groupby(group_by_columns)[column_to_treat].transform('median')\n\n            # For the few cases where all the times the UID is NaN we take the median of the whole feature\n            df.loc[df['median_' + column_to_treat].isnull(), 'median_' + column_to_treat + '_check'] = 1\n            df['median_' + column_to_treat + '_check'] = df['median_' + column_to_treat + '_check'].fillna(0)\n            df.loc[df['median_' + column_to_treat].isnull(), 'median_' + column_to_treat] = df[column_to_treat].median()\n\n            #time_from_fraud_dict = df.groupby(group_by_columns)['median_' + column_to_treat]\n        df.drop(columns_to_treat + ['last_fraud'], axis = 1, inplace = True)\n    else:\n        \n        df_medians = pd.DataFrame(df[group_by_columns],index = df.index)\n\n        values_group_by = data.groupby(group_by_columns)[group_by_columns].transform('min').values\n        values_group_by = values_group_by.reshape(len(values_group_by))\n\n        # Create median column of time from last fraud by UID        \n        for column_to_treat in tqdm_notebook(columns_to_treat):\n            data_medians = pd.DataFrame(index = values_group_by)\n            data_medians['median_' + column_to_treat] = data.groupby(group_by_columns)['median_' + column_to_treat].transform('median').values\n\n            # For the few cases where all the times the UID is NaN we take the median used in data, which are the ones that have a check (all are the same, so we can use min() to get the value)\n            data_medians.loc[data_medians['median_' + column_to_treat].isnull(), 'median_' + column_to_treat] = min(data.loc[data['median_' + column_to_treat + '_check'] == 1 ,'median_' + column_to_treat])\n            data_medians = data_medians.drop_duplicates()\n\n            # Now merge and set the new column to the output\n            df_medians = df_medians.merge(data_medians, how = 'left', left_on = group_by_columns, right_index = True)\n            df['median_' + column_to_treat] = df_medians['median_' + column_to_treat].copy()            ","040a8a1d":"def add_check_null_columns(df):\n    nan_pd = df.isnull().copy()\n    nan_pd = nan_pd.T[nan_pd.any()].T\n    nan_pd.columns = [column + '_isnull' if not '_isnull' in column else column for column in nan_pd.columns]\n    df = df.merge(nan_pd, left_index = True, right_index = True)\n    \ndef add_check_null_rows(df):\n    bins = [0, 1, 5, 10, 25, 50, 75, 100]\n    df['all_NaN_bins'] = pd.cut(abs(df.isna().sum(axis=1).astype(np.int8)), bins=bins)\n    if data_version < 'v5':\n        df['all_NaN'] = abs(df.isna().sum(axis=1).astype(np.int8))","270b8622":"def select_only_top_values(df, col, n_top_values, others_value = 'Others', nan_value = 'NaN'):\n    value_counts = df[col].value_counts(dropna = True)\n    if len(value_counts) > n_top_values:\n        top_value_counts = value_counts.iloc[:n_top_values].index.values\n        others_value_counts = value_counts.iloc[n_top_values:].index.values\n        col_dict = dict(zip(list(top_value_counts) + list(others_value_counts), list(top_value_counts) + [others_value,] * len(value_counts)))\n    else:\n        col_dict = dict(zip(value_counts.index.values, value_counts.index.values))\n\n    col_dict[np.nan] = nan_value\n    return col_dict","a313380e":"def device_info_featuring(df, device_dict = {}, isSubmission = False, n_top_values = 100):\n    # DeviceInfo_1 the main part of the DeviceInfo and DeviceInfo_2 gets the build version \n    df['DeviceInfo_1'] = df.DeviceInfo.str.split(\" Build\/\", n=1, expand=True)[0]\n    df['DeviceInfo_2'] = df.DeviceInfo.str.split(\" Build\/\", n=1, expand=True)[1]\n    \n    # Identify IE browser\n    ie_cond1 = df['DeviceInfo'].str.startswith('rv:11', na=False)\n    ie_cond2 = df['DeviceInfo'].str.startswith('Trident\/', na=False)\n    df.loc[ie_cond2 & ie_cond2, 'DeviceInfo_1'] = \"IE\" \n    \n    # Identify Mozilla browser\n    mozilla_cond1 = df['DeviceInfo'].str.startswith('rv:', na=False)\n    df.loc[mozilla_cond1 & ~ie_cond1,'DeviceInfo_1'] = \"MOZILLA\" \n    \n    # DeviceInfo_3 get the first part of the DeviceInfo_1, till it finds one of the symbols\n    df['DeviceInfo_3'] = df['DeviceInfo_1'].copy()\n    for symbol in [\" \", \"-\", \"_\", \"(\", \"\/\"]:\n        df['DeviceInfo_3'] = df['DeviceInfo_3'].str.extract(r'([^{}]+)'.format(symbol))\n     \n    # DeviceInfo_3 refining, shorting some brands\n    brands = ['HTC', 'IdeaTab', 'KF', 'LG', 'Lenovo', 'ME', 'RCT', 'SGP', 'XT', 'verykool']\n    df['DeviceInfo_3'] = df['DeviceInfo_3'].str.extract(r'({})'.format('^' + '|^'.join(brands)))\n        \n    # DeviceInfo_4 returns the main part of build version\n    device_info_build_featuring(df)\n\n    for col in ['DeviceInfo_1','DeviceInfo_2','DeviceInfo_3','DeviceInfo_4']:\n        device_dict[col] = select_only_top_values(df, col, n_top_values)\n        df[col] = df[col].map(device_dict[col]).fillna('Others')\n    \n    return device_dict\n    \ndef device_info_build_featuring(df):\n    cond1 = r'\\bHUAWEI[a-z|A-Z]+[^-]+' # Starts with HUAWEI\n    cond2 = r'^[a-z|A-Z]{3}' # Starts with 3 letters\n    cond3 = r'^[0-9]+[.][0-9]+[.][a-z|A-Z]' # Starts with format like 33.4.A or 6.5.A\n\n    condition = r'({})'.format('|'.join([cond1,cond2,cond3]))\n\n    df['DeviceInfo_4'] = df['DeviceInfo_2'].str.upper()\n    df['DeviceInfo_4'] = df['DeviceInfo_2'].str.extract(condition, flags = re.IGNORECASE)\n\n    # Rest copy the same\n    cond_rest = (df['DeviceInfo_2'].notnull()) & (df['DeviceInfo_4'].isnull())\n    df.loc[cond_rest, 'DeviceInfo_4'] = df['DeviceInfo_2'].copy()","e4e62721":"def emails_domains_featuring(df, n_top_domains = 10, emails_dict = {}, isSubmission = False):\n    P_cols = ['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']\n    R_cols = ['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']\n    df[P_cols] = df['P_emaildomain'].str.split('.', expand=True)\n    df[R_cols] = df['R_emaildomain'].str.split('.', expand=True)\n\n    for col in P_cols + R_cols:\n        if not isSubmission:\n            emails_dict[col] = select_only_top_values(df, col, n_top_domains)\n\n        df[col] = df[col].map(emails_dict[col]).fillna('Others')\n    \n    return emails_dict","551295c8":"def divide_mean_by_grouping(df, column_to_treat, group_by_column, isSubmission = False, group_by_means_dict = {}):\n    if not isSubmission:\n        group_by_means_dict[column_to_treat + \"_to_mean_by_\" + group_by_column] = df.groupby(group_by_column)[column_to_treat].mean()\n    \n    df[column_to_treat + \"_to_mean_by_\" + group_by_column] = df[column_to_treat].astype('float64') \/ df.merge(group_by_means_dict[column_to_treat + \"_to_mean_by_\" + group_by_column], how = 'left', left_on = group_by_column, suffixes = ('','_new'), right_index = True)[column_to_treat + '_new']\n    \n    return group_by_means_dict","e8f80693":"def clean_inf_nan(df):\n    float_cols = df.dtypes[df.dtypes.astype(str).str.startswith('float')].index.values\n    for col in tqdm_notebook(float_cols):\n        df[col] = df[col].replace([np.inf, -np.inf], np.nan)","ad714d91":"def treat_NaN(df, group_by_columns):\n    cond1 = df.isnull().any()\n    cond2 = df.dtypes.astype(str).str.startswith('float')\n    cond3 = df.dtypes == 'object'\n    \n    float_cols_with_nulls = df.dtypes.loc[cond1 & cond2].index.values\n    \n    for col in tqdm_notebook(float_cols_with_nulls):\n        treat_NaN_by_groups(df, col, group_by_columns)\n        \n    # For categorical columns which have NaN, we replace the np.nan for 'NaN' which is accepted as a new category\n    object_cols_with_nulls = df.dtypes.loc[cond1 & cond3].index.values\n    \n    for col in tqdm_notebook(object_cols_with_nulls):\n        df.loc[df[col].isnull(), col] = 'NaN'\n        \n\ndef treat_NaN_by_groups(df, column_to_treat, group_by_columns):\n    # If percentage of outliers is less than the threshold, then use mean, otherwise median\n    threshold_mean_median = 0.01\n    threshold_outliers = 3\n    mask_outliers = (np.abs(stats.zscore(df[column_to_treat][df[column_to_treat].notnull()])) > threshold_outliers)\n    if mask_outliers.sum() \/ len(mask_outliers) > threshold_mean_median:\n        # Firstly trying to set the NaN value from group mean\n        df.loc[df[column_to_treat].isnull(), column_to_treat] = df.groupby(group_by_columns)[column_to_treat].transform('mean')\n        # The remaining NaN will have mean value\n        df.loc[df[column_to_treat].isnull(), column_to_treat] = df[column_to_treat].mean()\n    else:\n        # Firstly trying to set the NaN value from group median\n        df.loc[df[column_to_treat].isnull(), column_to_treat] = df.groupby(group_by_columns)[column_to_treat].transform('median')\n        # The remaining NaN will have median value\n        df.loc[df[column_to_treat].isnull(), column_to_treat] = df[column_to_treat].median()","f591c110":"def skewed_data_transformation(df, columns_to_log):\n    \n    # Only do transformation on columns where all values are positive\n    df_positive = df[(df[columns_to_log] >= 0).all().index.values].copy()\n    \n    # Set type to be sure there is no problem of limitation by data type (later they will be reset depending on the values)\n    int_cols = df_positive.dtypes[df_positive.dtypes.astype(str).str.startswith('int')].index.values\n    float_cols = df_positive.dtypes[df_positive.dtypes.astype(str).str.startswith('float')].index.values\n    df_positive.loc[:, int_cols] = df_positive[int_cols].astype('int64')\n    df_positive.loc[:, float_cols] = df_positive[float_cols].astype('float64')\n    \n    # Log transformation\n    df_log = df_positive.apply(np.log).replace([np.inf, -np.inf], 0)\n    df_log.columns = df_log.columns.map(lambda x : 'log_' + str(x))\n    df = df.merge(df_log, how = 'left', left_index = True, right_index = True)\n    \n    # Square transformation\n    df_square = df_positive.pow(2)\n    df_square.columns = df_square.columns.map(lambda x : 'square_' + str(x))\n    df = df.merge(df_square, how = 'left', left_index = True, right_index = True)\n    \n    # Transformation for left skewed data\n    df_left_skew = -df_positive.pow(-1\/2).replace([np.inf, -np.inf], 0)\n    df_left_skew.columns = df_left_skew.columns.map(lambda x : 'left_skew_' + str(x))\n    df = df.merge(df_left_skew, how = 'left', left_index = True, right_index = True)\n    \n    return df","31d7c80f":"def feature_engineering(df, isSubmission = False, emails_dict = {}, n_top_domains = 10, group_by_means_dict = {}, device_dict = {}, n_top_values = 100):\n    print('Starting feature engineering')\n    \n    #Sort data by TransactionDT\n    print('Sorting data by TransactionDT')\n    df = df.sort_values('TransactionDT')\n    \n    # Featuring Unique identifier\n    print('Creating Unique identifiers')\n    unique_identifier(df)\n    \n    # Identify TransactionID which does match with identity table\n    print('Featuring \"Transaction_match_identity\"...')\n    df['Transaction_match_identity'] = ~ df['id_01'].isnull()\n    \n    # Extraction of information from TransactionDT\n    print('Featuring \"TransactionDT\"...')\n    transactionDT(df)\n    time_from_fraud(df, isSubmission)\n    \n    # Matematical transformations of following columns:\n    columns_to_log = ['TransactionAmt', 'dist1', 'id_17', 'id_19', 'id_20', 'C13', 'C1', 'V91', 'addr1', 'C14', 'V317', 'V258', 'D1', 'C6', 'D2', 'C4', 'V310', 'C5', 'C9', 'C11']\n    df = skewed_data_transformation(df, columns_to_log)\n    \n    # Extract info from DeviceInfo\n    print('Featuring \"DeviceInfo\"...')\n    device_dict = device_info_featuring(df, device_dict, isSubmission, n_top_values)\n    \n    # Extract info from emails domains\n    print('Featuring \"Emails domain...')\n    emails_dict = emails_domains_featuring(df, n_top_domains = n_top_domains, emails_dict = emails_dict, isSubmission = isSubmission)\n    \n    #Divide by meaning grouping by different columns\n    print('Featuring \"Divide by mean\"...')\n    columns_to_mean = ['TransactionAmt', 'dist1', 'id_17', 'id_19', 'id_20', 'C13', 'C1', 'V91', 'addr1', 'C14', 'V317', 'V258', 'D1', 'C6', 'D2', 'C4', 'V310', 'C5', 'C9', 'C11']\n    group_by_columns = ['ProductCD','uid', 'uid_1', 'uid_2', 'uid_3', 'uid_4', 'uid_5', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6','P_emaildomain', 'DeviceInfo', 'M3', 'M4', 'M5', 'M6', 'P_emaildomain', 'R_emaildomain', 'id_19', 'id_20', 'id_31', 'id_33', 'hours', 'days_week', 'days_year']\n    for group_by in tqdm_notebook(group_by_columns, desc = '1st Loop'):\n        for col in tqdm_notebook(columns_to_mean, desc = '2nd Loop', leave = False):\n            group_by_means_dict = divide_mean_by_grouping(df, col, group_by, isSubmission = isSubmission, group_by_means_dict = group_by_means_dict)\n    \n    # Replace -np.inf and np.inf for np.nan\n    clean_inf_nan(df)\n    \n    # Add a column which check if the original had null values (only for columns which have null values)\n    print('Featuring \"add_check_null_columns\"')\n    add_check_null_columns(df)\n    print('Featuring \"add_check_null_rows\"')\n    add_check_null_rows(df)\n    \n    # Replace NaN values with mean of grouped data from columns which have nulls and are float64 type\n    print('Featuring \"Replace NaN on float64\"...')\n    group_by_columns = ['uid']\n    #group_by_columns = ['ProductCD', 'card1']\n    treat_NaN(df, group_by_columns)\n    \n    return df, emails_dict, group_by_means_dict, device_dict","2df159a7":"def get_categorical_columns():\n    \n    categorical_columns = data.dtypes.iloc[np.where(~data.dtypes.astype(str).str.startswith('float'))[0]].index.values\n    #categorical_columns = ['ProductCD',\n    #     'card1','card2','card3','card4','card5','card6',\n    #     'P_emaildomain','R_emaildomain',\n    #     'M1','M2','M3','M4','M5','M6','M7','M8','M9',\n    #     'id_12','id_13','id_14','id_15','id_16','id_17','id_18','id_19','id_20','id_21','id_22','id_23','id_24','id_25','id_26','id_27','id_28','id_29','id_30','id_31','id_32','id_33','id_34','id_35','id_36','id_37','id_38',\n    #     'DeviceType',\n    #     'DeviceInfo','DeviceInfo_1','DeviceInfo_2','DeviceInfo_3','DeviceInfo_4',\n    #     'seconds', 'minutes', 'hours', 'days_week', 'days_year', \n    #     'minutes_from_last_fraud', 'hours_from_last_fraud', 'median_minutes_from_last_fraud', 'median_hours_from_last_fraud'\n    #     'uid', 'uid_1', 'uid_2', 'uid_3', 'uid_4', 'uid_5']\n\n    return categorical_columns","042b5393":"def encode_categorical_data(data_df, submission_df):\n    #categorical_columns = [col for col,col_type in train.dtypes.items() if col_type == 'object']\n\n    print(\"Treating categorical columns\")\n    for col in tqdm_notebook(categorical_columns):\n        if col in data_df.columns.values:\n            #print(f\"Treating column {col}, {categorical_columns.index(col) + 1} out of {len(categorical_columns)}\")\n            le = LabelEncoder()\n            le.fit(list(data_df[col].astype(str).str.upper().values) + list(submission_df[col].astype(str).str.upper().values))\n            data_df[col] = le.transform(list(data_df[col].astype(str).str.upper().values))\n            submission_df[col] = le.transform(list(submission_df[col].astype(str).str.upper().values))","95bd2571":"%%time\nif load_data or not data_available:\n    print(\"Starting data treatments\")\n    try:\n        \n        remove_columns = remove_unique_and_NaN_columns(data)\n        data = data.drop(remove_columns, axis = 1)\n\n        n_top_domains = 10\n        n_top_values = 100\n        data, emails_dict, group_by_means_dict, device_dict = feature_engineering(data, n_top_domains  = n_top_domains, n_top_values = n_top_values)\n\n        # Ensure that all columns use the proper data type\n        data = reduce_mem_usage(data)\n        \n        submission = load_submission(data_dir, load_data)\n        submission = submission.drop(remove_columns, axis = 1)\n        submission, _, _, _ = feature_engineering(submission, isSubmission = True, \n                                         emails_dict = emails_dict, n_top_domains = n_top_domains, \n                                         group_by_means_dict = group_by_means_dict,\n                                        device_dict = device_dict, n_top_values = n_top_values)\n\n        categorical_columns = get_categorical_columns()\n        \n        if not use_catboost:\n            %time encode_categorical_data(data, submission)\n\n        # Ensure that all columns use the proper data type\n        submission = reduce_mem_usage(submission)\n        data = reduce_mem_usage(data)\n    finally:\n        if not os.path.exists(data_dir):\n            os.makedirs(data_dir)\n\n        with open(data_dir + 'data.pkl', 'wb') as file:\n            print(\"Saving data in \", data_dir + 'data.pkl')\n            %time pickle.dump(data, file)\n\n        # Submission saved and then deleted in order to keep memory lower\n        with open(data_dir + 'submission.pkl', 'wb') as file:\n            print(\"Saving data in \", data_dir + 'submission.pkl')\n            %time pickle.dump(submission, file)\n        del submission\n        gc.collect()","f1701f3a":"train, test = train_test_split(data.sort_values('TransactionDT'), test_size = 0.1, shuffle = False)\n\n#del data\n\ntrain, valid = train_test_split(train, test_size = 0.1, shuffle = False)\n\ny = train.copy()['isFraud']\nX = train.copy().drop('isFraud', axis = 1)\n\ndel train\n\ny_valid = valid.copy()['isFraud']\nX_valid = valid.copy().drop('isFraud', axis = 1)\n\ndel valid\n\ny_test = test.copy()['isFraud']\nX_test = test.copy().drop('isFraud', axis = 1)\n\ndel test\n\n#del submission\n\ngc.collect()\n\nx_to_remove = ['TransactionID', 'isFraud', 'TransactionDT']\nx_to_remove += ['uid', 'uid_1','uid_2','uid_3','uid_4','uid_5','card1','card2','card3','card4','card5','card6']\nx_to_remove += ['P_emaildomain', 'R_emaildomain', 'DeviceInfo']\nx_to_remove += ['all_NaN_bins']\nx_columns = [col for col in list(X.columns) if col not in x_to_remove]","75501981":"do_training = True\n\nuse_predifined_params = True\n\nn_ensemble = 5\n\nif LOCAL:\n    task_type = \"GPU\"\nelse:\n    task_type = \"CPU\"\n\ncatBoost_models_dirs = []\nBayesSearchCV_dirs = []\nfor dir_file in os.listdir(data_dir):\n    if dir_file.startswith('CatBoostClassifier'):\n        catBoost_models_dirs.append(data_dir + dir_file)\n    elif dir_file.startswith('BayesSearchCV'):\n        BayesSearchCV_dirs.append(data_dir + dir_file)\n        \ncatBoost_models_dirs.reverse()","b335f4cb":"%%time\nif do_training or len(catBoost_models_dirs) == 0:\n    \n    # Create 3 random features which will serve as baseline to reject features\n    baseline_features = ['random_binary', 'random_uniform', 'random_integers']\n    X = X.drop(baseline_features, axis = 1, errors = 'ignore')\n    X['random_binary'] = np.random.choice([0, 1], X.shape[0])\n    X['random_uniform'] = np.random.uniform(0, 1, X.shape[0])\n    X['random_integers'] = np.random.randint(0, X.shape[0] \/ 2, X.shape[0])\n    x_columns = [col for col in list(X.columns) if col not in x_to_remove]\n    \n    # Get the indexes for the categorical columns which CatBoost requires to out-perform other algorithms\n    cat_features_index = [x_columns.index(col) for col in categorical_columns if col in x_columns]\n\n    estimator = cb.CatBoostClassifier(iterations = 100,\n                              eval_metric = \"AUC\",\n                              cat_features = cat_features_index,\n                              #rsm = 0.3,\n                              scale_pos_weight = y.value_counts()[0] \/ y.value_counts()[1],\n                              task_type = task_type,\n                              metric_period = 50,\n                              verbose = False\n                           )\n    \n    n_top_features = None\n    \n    catboost_feature_selection, df_catboost_feature_selection = shadow_feature_selection(\n        estimator, y, X[x_columns], \n        baseline_features = baseline_features, n_top_features = n_top_features,\n        collinear_threshold = 0.98, cum_importance_threshold = 0.99,\n        max_loops = 100, n_iterations_mean = 3, times_no_change_features = 3,\n        need_cat_features_index = True, categorical_columns = categorical_columns,\n        plot_correlation = True)\n\n    print(\"Features selected:\")\n    df_catboost_feature_selection","d09679d7":"def save_catboost_model(catboost_model, catboost_feature_selection):\n    param_dict = {\n                        'learning_rate' : 'lr',\n                        'depth' : 'depth',\n                        'l2_leaf_reg' : 'l2',\n                        'random_strength' : 'rs',\n                        'one_hot_max_size' : '1H',\n                        'bagging_temperature' : 'bag_temp',\n                        'min_data_in_leaf' : 'min_data',\n                        'iterations' : 'iter',\n                        'od_wait' : 'od_wait'\n        }\n    \n    save_folder = data_dir\n    save_folder += 'CatBoostClassifier'\n    save_folder += '-' + str(np.round(catboost_model.get_best_score()['validation']['AUC'],6)) + '_score'\n\n    for param, value in predefined_params.items():\n        if param in param_dict.keys():\n            if \".\" in str(value):\n                save_folder += '-' + str(np.round(value, 4)) + '_' + param_dict[param]\n            else:\n                save_folder += '-' + str(value) + '_' + param_dict[param]\n\n    # Create a folder if it does not exist\n    if not os.path.exists(save_folder):\n        os.makedirs(save_folder)\n    \n    try:\n        # Save model, feature_selection and results\n        with open(save_folder + '\/' + 'model.pkl', 'wb') as file:\n            pickle.dump(catboost_model, file)\n\n        with open(save_folder + '\/' + 'feature_selection.pkl', 'wb') as file:\n            pickle.dump(catboost_feature_selection, file)\n\n        with open(save_folder + '\/' + 'best_score.pkl', 'wb') as file:\n            pickle.dump(catboost_model.get_best_score(), file)\n    except CatBoostError:\n        print(\"Issue saving model on: \", save_folder)\n        shutil.rmtree(save_folder, ignore_errors=True)","233bda6d":"def train_catboost(params, X, y, X_valid, y_valid, catboost_feature_selection, cat_features_index = None, save_models = True, task_type = \"GPU\", verbose = True, plot = True):\n    \n    catboost_model = cb.CatBoostClassifier(iterations = 50000,\n                                              eval_metric = \"AUC\",\n                                              cat_features = cat_features_index,\n                                              scale_pos_weight = y.value_counts()[0] \/ y.value_counts()[1],\n                                              task_type=task_type,\n                                              metric_period = 100,\n                                              od_pval = 0.00001,\n                                              od_wait = 50)\n    \n    catboost_model.set_params(**params)\n    catboost_model.fit(X[catboost_feature_selection], y, \n                       eval_set = (X_valid[catboost_feature_selection], y_valid),\n                      use_best_model = True,\n                      #early_stopping_rounds = True,\n                      plot = plot,\n                      verbose = verbose)\n\n    print(f\"Best score {catboost_model.get_best_score()} with params {params}\")\n    if save_models:\n        save_catboost_model(catboost_model, catboost_feature_selection)\n        \n    return (catboost_model, catboost_feature_selection, catboost_model.get_best_score()['validation']['AUC'])","9b42eeb0":"%%time\n\nlist_catboost_models = []\n\n# Kaggle have some restrictions on HDD space and we could have space issues if we save the models\nif LOCAL:\n    save_models = True\nelse:\n    save_models = False\n\nif not do_training and len(catBoost_models_dirs) > 0:\n    \n    for i in tqdm_notebook(range(n_ensemble)):\n        best_model_dir = catBoost_models_dirs[i]\n        if BayesSearchCV_dirs != []:\n            latest_BayesSearchCV_dir = max(BayesSearchCV_dirs, key=os.path.getctime)\n\n        print(\"Loading model and result_dict from folder: \" + best_model_dir)\n\n        with open(best_model_dir + '\/' + 'model.pkl', 'rb') as file:\n            catboost_model = pickle.load(file)\n\n        with open(best_model_dir + '\/' + 'feature_selection.pkl', 'rb') as file:\n            catboost_feature_selection = pickle.load(file)    \n        \n        if os.path.exists(best_model_dir + '\/' + 'best_score.pkl'):\n            with open(best_model_dir + '\/' + 'best_score.pkl', 'rb') as file:\n                catboost_best_score = pickle.load(file) \n            list_catboost_models.append((catboost_model, catboost_feature_selection, catboost_best_score['validation']['AUC']))\n        else:\n            start_index = best_model_dir.find('CatBoostClassifier') + len('CatBoostClassifier') + 1\n            score = float(best_model_dir[start_index : start_index + 7])\n            list_catboost_models.append((catboost_model, catboost_feature_selection, score))\n\n        # Stop looking for more models if there is not more\n        if len(catBoost_models_dirs) - 1 == i:\n            break\n    \n    if BayesSearchCV_dirs != []:\n        with open(latest_BayesSearchCV_dir + '\/' + 'result_dict.pkl', 'rb') as file:\n            catboost_result_dict = pickle.load(file)\n            \n    print(\"Done\")\n\nelif use_predifined_params:\n    cat_features_index = [catboost_feature_selection.index(col) for col in categorical_columns if col in catboost_feature_selection]\n\n    list_predefined_params = []\n\n    predefined_params = {\n                        'learning_rate' : 0.05,\n                        'depth' : 4,\n                        'l2_leaf_reg' : 5,\n                        'random_strength' : 1,\n                        'one_hot_max_size' : 2,\n                        #'min_data_in_leaf' : 5,\n                        'bagging_temperature' : 0.01\n        }\n    list_predefined_params.append(predefined_params.copy())\n    \n    predefined_params = {\n                        'learning_rate' : 0.05,\n                        'depth' : 5,\n                        'l2_leaf_reg' : 20,\n                        'random_strength' : 15,\n                        'one_hot_max_size' : 2,\n                        #'min_data_in_leaf' : 10,\n                        'bagging_temperature' : 0.01\n        }\n    list_predefined_params.append(predefined_params.copy())\n    \n    predefined_params = {\n                        'learning_rate' : 0.05,\n                        'depth' : 6,\n                        'l2_leaf_reg' : 40,\n                        'random_strength' : 15,\n                        'one_hot_max_size' : 2,\n                        #'min_data_in_leaf' : 20,\n                        'bagging_temperature' : 0.01\n        }\n    list_predefined_params.append(predefined_params.copy())\n    \n    predefined_params = {\n                        'learning_rate' : 0.05,\n                        'depth' : 7,\n                        'l2_leaf_reg' : 120,\n                        'random_strength' : 1,\n                        'one_hot_max_size' : 2,\n                        #'min_data_in_leaf' : 25,\n                        'bagging_temperature' : 0.01\n        }\n    list_predefined_params.append(predefined_params.copy())\n    \n    predefined_params = {\n                        'learning_rate' : 0.05,\n                        'depth' : 8,\n                        'l2_leaf_reg' : 200,\n                        'random_strength' : 1,\n                        'one_hot_max_size' : 25,\n                        #'min_data_in_leaf' : 50,\n                        'bagging_temperature' : 0.01\n                        \n        }\n    list_predefined_params.append(predefined_params.copy())\n\n\n    for params in tqdm_notebook(list_predefined_params):\n\n        list_catboost_models.append(\n            train_catboost(params, \n                       X, y, \n                       X_valid, y_valid, \n                       catboost_feature_selection,\n                       cat_features_index,\n                       save_models = save_models,\n                       task_type = task_type)\n        )\nelse:\n    \n    cat_features_index = [catboost_feature_selection.index(col) for col in categorical_columns if col in catboost_feature_selection]\n\n    search_spaces = {\n                    'learning_rate' : (0.01, 0.5, 'log-uniform'),\n                    'depth' : (3,16),\n                    'l2_leaf_reg' : (20,150),\n                    'random_strength' : (1,20),\n                    'one_hot_max_size' : (2,25),\n                    'bagging_temperature' : (0.0, 1.0)\n    }\n    \n    bayes_search = FixedBayesSearchCV(\n                                estimator = cb.CatBoostClassifier(iterations = 300,\n                                                                  eval_metric = \"AUC\",\n                                                                  cat_features = cat_features_index,\n                                                                  scale_pos_weight = y.value_counts()[0] \/ y.value_counts()[1],\n                                                                  task_type=\"GPU\",\n                                                                  metric_period = 40),\n                                search_spaces = search_spaces,\n                                scoring = 'roc_auc',\n                                cv = KFold(n_splits=3),\n                                return_train_score = True,\n                                n_jobs = 1,\n                                n_iter = 50,   \n                                verbose = 1,\n                                refit = False)\n\n    %time bayes_search.fit(X[catboost_feature_selection], y)\n    \n    catboost_result_dict = bayes_search.cv_results_\n    print(f\"Best score {bayes_search.best_score_} with params {bayes_search.best_params_}\")\n    \n    search_spaces_folder = data_dir\n    search_spaces_folder += 'BayesSearchCV'\n    for key in search_spaces.keys():\n        search_spaces_str += '_' + key\n        search_spaces_str += '(' + str(search_spaces[key][0]) + '-' + str(search_spaces[key][1]) + ')'\n    \n    # Create a folder if it does not exist\n    if not os.path.exists(search_spaces_folder):\n        os.makedirs(search_spaces_folder)\n    \n    with open(search_spaces_folder + '\/' + 'result_dict.pkl', 'wb') as file:\n        pickle.dump(catboost_result_dict, file)\n    \n    list_best_params = np.array(pd.DataFrame(catboost_result_dict).nlargest(n_ensemble, 'mean_test_score')['params'])\n\n    for i in tqdm_notebook(range(n_ensemble)):\n\n        list_catboost_models.append(\n            train_catboost(list_best_params[i], \n               X, y, \n               X_valid, y_valid, \n               catboost_feature_selection,\n               cat_features_index,\n               save_models = save_models,\n               task_type = task_type)\n        )","ae38d5b4":"if not use_predifined_params:\n    result_pd = pd.DataFrame(catboost_result_dict)\n    plot_x_columns = ['bagging_temperature', 'depth', 'l2_leaf_reg','learning_rate','one_hot_max_size','random_strength']\n    list_dict_scatters = []\n    for param in plot_x_columns:\n        dict_param = {}\n        dict_param['x_column'] = 'param_' + param\n        dict_param['y_column'] = 'mean_test_score'\n        dict_param['title'] = param\n\n        list_dict_scatters.append(dict_param)\n\n    plot_list_scatters(result_pd, list_dict_scatters, subplot_cols = 3, subplot_rows = 2)","61112c12":"def ensemble_catboosts(X, list_models):\n    sum_scores = 0\n    \n    y_ensemble = np.zeros(shape=len(X))\n    \n    for model,feature_selection,score in tqdm_notebook(list_models):\n        y_pred_cat = model.predict_proba(X[feature_selection])[:,1] * score \/ len(list_models)\n        sum_scores += score\n        \n        y_ensemble = np.add(y_ensemble, y_pred_cat)\n        \n    return y_ensemble \/ sum_scores","ac46c9b6":"print(\"Plot Train Ensemble ROC AUC\")\nplot_roc_auc(y, ensemble_catboosts(X[catboost_feature_selection], list_catboost_models))\n\nprint(\"Plot Valid Ensemble ROC AUC\")\nplot_roc_auc(y_valid, ensemble_catboosts(X_valid[catboost_feature_selection], list_catboost_models))\n\nprint(\"Plot Test Ensemble ROC AUC\")\nplot_roc_auc(y_test, ensemble_catboosts(X_test[catboost_feature_selection], list_catboost_models))","85b71d87":"submission = load_submission(data_dir, False)\ny_submission = ensemble_catboosts(submission, list_catboost_models)\n\nsubmission_pd = pd.DataFrame()\nsubmission_pd['TransactionID'] = submission['TransactionID']\nsubmission_pd['isFraud'] = y_submission\nsubmission_pd.set_index('TransactionID', inplace = True)\n\nsubmission_pd.to_csv('submission' + str(datetime.date.today()) + '.csv')","3912ba0e":"### Feature engineering\n\nWe are going to create several feature engineering treatments as:\n\n1) Identify TransactionID which does match with identity table\n\n2) Create new features from TransactionDT data\n\n3) Transform some float data with log, square or root square in case they are skewed\n\n4) Create new features from DeviceInfo data\n\n5) Divide by meaning grouping by different columns\n\n6) Replace -np.inf and np.inf for np.nan\n\n7) New columns which check where there are original NaN values\n\n8) Replace NaN values with mean of grouped data from columns of float64 type ","520b497d":"## Predict Submission","e5acf219":"### Plots of parameters effects on mean_test_score (AUC)","abb3a0cb":"### Encode categorical data\n\nTransform all the categorical data into LabelEncoder (this will only be used if we are not using CatBoost algorithm)","9e7353bc":"### Ploting ROC AUC","634ba8b1":"### Execute Data Treatment","fa140641":"### Ensemble of CatBoost Classifier and Deep Neuronal Network","6eb5469f":"### Column Feature Selection","d3ce82e1":"## Training using CatBoost Classifier in a Bayesian Search\n\nWe are going to create a model usuing CatBoost Classifier and looking for the best hyperparameters using a Bayesian Search","09c07f44":"### Target variable analysis\n\nTarget data 'isFraud' is highly imbalanced, so we are not interested on accuracy, but ROC AUC.","ebb1b825":"### Rebalance data\nAs the data is such unbalanced, we need to do some treatment here. There are two types of sampling, (1) downsampling (removing data from the dominant target class), (2) upsampling (duplicating data from the target class in minority). In this case we prefered downsampling the data.","917b7038":"### Train and validation split\n\nSplit data into train, validation and test in order to build and evaluate correctly the models. We are not shuffling because the data is ordered by TransactionDT and we prefer to evaluate in such order to be more realistic.","a2b930eb":"### Load data","dce6d9c8":"### Training CatBoost Classifier"}}