{"cell_type":{"11ce830a":"code","558a893a":"code","d51e40c2":"code","e0e83772":"code","4e8ba808":"code","886f889b":"code","5c925fbb":"code","67d596c4":"code","cc4f0a23":"code","193efa16":"code","344d8d87":"code","67f971a1":"code","cf57089f":"code","62e3f6cb":"code","41e38326":"code","455b9ef6":"code","38a249f0":"code","2f63267c":"code","d388ee82":"code","1fefe8fd":"code","420e78f0":"code","641c2cc0":"code","3a22fc84":"code","e03e3233":"code","b9950afe":"code","91094cd9":"code","1c8f0a2a":"code","9dc14d6e":"code","06e29020":"code","d27c75e1":"code","123f73f1":"code","929f0615":"code","fc2b6cfd":"code","92ef5487":"code","7f8199da":"code","e8e0ec6d":"code","e5d815cd":"code","52362b94":"code","c2022f33":"code","76f46448":"code","564c2dcd":"code","1233e8f8":"code","7ce717e7":"code","feca45a7":"code","af5bda55":"code","a4642fd0":"code","1b4bad9e":"code","5665a827":"code","31fc165c":"code","dd41e6d6":"code","ec379f6a":"code","49b02d7f":"code","33b060f5":"code","f86a19b3":"code","faa68faa":"code","862ac015":"code","50cceec9":"code","0d25b809":"code","ba228b52":"code","30e60912":"code","8dac45d3":"code","fcb0bc6f":"code","7e8b997f":"code","0072ac6c":"code","e64fc2b6":"code","214c118c":"code","d012d000":"code","11697722":"code","13bd33c5":"code","30665376":"code","dc6f5195":"code","2162e4ce":"code","cafd434a":"code","1f855367":"code","0464265a":"code","505e51c2":"code","10b6c518":"code","f7975f63":"code","fd045e23":"code","7f2677ec":"code","cb77cadb":"code","096f9ddf":"code","68fa0546":"code","caa24555":"code","6ac99921":"code","4c2091ea":"code","21884299":"code","3cb00f1d":"code","10007ed3":"code","aba97f04":"code","9abca38b":"code","c41a5ecc":"code","59d2510c":"code","e0048f97":"code","276f791f":"code","68e35650":"code","f2cbd0b1":"code","2d3b885b":"code","ddaa1729":"code","a8c2e869":"code","23b70105":"code","f03d7353":"code","fa66928b":"code","05aa7115":"code","d3e6c20c":"code","faa2ab02":"code","997ed58d":"code","244353e1":"code","a538e0bb":"code","ad46acc6":"code","bb82f7bd":"code","20fbabdf":"code","0f1f12d7":"code","94b0cce6":"code","291130c4":"code","d5366c5b":"code","abbaa872":"code","d879a166":"code","77770f22":"code","7b857d4b":"code","74c6fe3c":"code","f3c9b547":"code","535390a9":"code","6f78a0c3":"code","07799577":"code","9dd5f6d0":"markdown","89894b97":"markdown","5b7c12fd":"markdown","2072ce93":"markdown","582ac24b":"markdown","f1e96bee":"markdown","8c7ab15c":"markdown","cd6d225e":"markdown","fffbd0f6":"markdown","2a4e3793":"markdown","16f63bb3":"markdown","dbf6e7b5":"markdown","26ed5922":"markdown","d77d4021":"markdown","6fbf237f":"markdown","87d0bcc3":"markdown","751f7a0a":"markdown","c36def98":"markdown","40c74a89":"markdown","403bcde8":"markdown","15a41741":"markdown","a91d1275":"markdown","45e49faa":"markdown","457d419a":"markdown","175696da":"markdown","2d08d8ee":"markdown","674c36e1":"markdown","efefe109":"markdown","170727dc":"markdown","f8290eb9":"markdown","1da18f1a":"markdown","05001deb":"markdown","ea692db3":"markdown","d1d6bd88":"markdown","a69a2921":"markdown","7cd2effe":"markdown","970a95bd":"markdown","9c401abf":"markdown","8495aea0":"markdown","26b4bfc5":"markdown","ce8e5b14":"markdown","ce275906":"markdown","fcdccafe":"markdown","781bdf01":"markdown","2427fe7c":"markdown","be0dc2ef":"markdown","d710bcee":"markdown","a9e3cb43":"markdown","aba167ba":"markdown","a584464e":"markdown","2201b1b5":"markdown","727062f7":"markdown","e6c9fc59":"markdown","3a060e61":"markdown","db708135":"markdown","dbdd97ea":"markdown","e46a0935":"markdown","5dd89b9c":"markdown","4b17c007":"markdown","e102786b":"markdown","799f17cd":"markdown","79812411":"markdown","52455f8a":"markdown","8e7b6453":"markdown","ba446ae4":"markdown","3102423a":"markdown","06c784ac":"markdown","454472f6":"markdown","f20e4591":"markdown","259da1d9":"markdown","a8158da5":"markdown","844b28c7":"markdown","b7b3169b":"markdown","a803afc9":"markdown","fa8b8774":"markdown","eb8d28dc":"markdown","7acedd56":"markdown","ad68563f":"markdown"},"source":{"11ce830a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","558a893a":"import matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\nimport gc\nimport lightgbm as lgb\nimport time\n# import datetime\n# import xgboost as xgb\n# import time\n# import itertools\n# from sklearn.linear_model import LinearRegression\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import OneHotEncoder\n# from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import r2_score\n\n%matplotlib inline\nsns.set()","d51e40c2":"INPUT_DIR = '\/kaggle\/input\/m5-forecasting-accuracy'\n\ncalendar_df = pd.read_csv(f\"{INPUT_DIR}\/calendar.csv\")\nsell_prices_df = pd.read_csv(f\"{INPUT_DIR}\/sell_prices.csv\")\nsales_train_validation_df = pd.read_csv(f\"{INPUT_DIR}\/sales_train_validation.csv\")\nsample_submission_df = pd.read_csv(f\"{INPUT_DIR}\/sample_submission.csv\")","e0e83772":"# Calendar data type cast -> Memory Usage Reduction\ncalendar_df[[\"month\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"wday\"]] = calendar_df[[\"month\", \"snap_CA\", \"snap_TX\", \"snap_WI\", \"wday\"]].astype(\"int8\")\ncalendar_df[[\"wm_yr_wk\", \"year\"]] = calendar_df[[\"wm_yr_wk\", \"year\"]].astype(\"int16\") \ncalendar_df[\"date\"] = calendar_df[\"date\"].astype(\"datetime64\")\n\nnan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\nfor feature in nan_features:\n    calendar_df[feature].fillna('unknown', inplace = True)\n\ncalendar_df[[\"weekday\", \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]] = calendar_df[[\"weekday\", \"event_name_1\", \"event_type_1\", \"event_name_2\", \"event_type_2\"]] .astype(\"category\")","4e8ba808":"# Sales Training dataset cast -> Memory Usage Reduction\nsales_train_validation_df.loc[:, \"d_1\":] = sales_train_validation_df.loc[:, \"d_1\":].astype(\"int16\")","886f889b":"# Make ID column to sell_price dataframe\nsell_prices_df.loc[:, \"id\"] = sell_prices_df.loc[:, \"item_id\"] + \"_\" + sell_prices_df.loc[:, \"store_id\"] + \"_validation\"","5c925fbb":"sell_prices_df = pd.concat([sell_prices_df, sell_prices_df[\"item_id\"].str.split(\"_\", expand=True)], axis=1)\nsell_prices_df = sell_prices_df.rename(columns={0:\"cat_id\", 1:\"dept_id\"})\nsell_prices_df[[\"store_id\", \"item_id\", \"cat_id\", \"dept_id\"]] = sell_prices_df[[\"store_id\",\"item_id\", \"cat_id\", \"dept_id\"]].astype(\"category\")\nsell_prices_df = sell_prices_df.drop(columns=2)","67d596c4":"def make_dataframe():\n    # Wide format dataset \n    df_wide_train = sales_train_validation_df.drop(columns=[\"item_id\", \"dept_id\", \"cat_id\", \"state_id\",\"store_id\", \"id\"]).T\n    df_wide_train.index = calendar_df[\"date\"][:1913]\n    df_wide_train.columns = sales_train_validation_df[\"id\"]\n    \n    # Making test label dataset\n    df_wide_test = pd.DataFrame(np.zeros(shape=(56, len(df_wide_train.columns))), index=calendar_df.date[1913:], columns=df_wide_train.columns)\n    df_wide = pd.concat([df_wide_train, df_wide_test])\n\n    # Convert wide format to long format\n    df_long = df_wide.stack().reset_index(1)\n    df_long.columns = [\"id\", \"value\"]\n\n    del df_wide_train, df_wide_test, df_wide\n    gc.collect()\n    \n    df = pd.merge(pd.merge(df_long.reset_index(), calendar_df, on=\"date\"), sell_prices_df, on=[\"id\", \"wm_yr_wk\"])\n    df = df.drop(columns=[\"d\"])\n#     df[[\"cat_id\", \"store_id\", \"item_id\", \"id\", \"dept_id\"]] = df[[\"cat_id\"\", store_id\", \"item_id\", \"id\", \"dept_id\"]].astype(\"category\")\n    df[\"sell_price\"] = df[\"sell_price\"].astype(\"float16\")   \n    df[\"value\"] = df[\"value\"].astype(\"int32\")\n    df[\"state_id\"] = df[\"store_id\"].str[:2].astype(\"category\")\n\n\n    del df_long\n    gc.collect()\n\n    return df\n\ndf = make_dataframe()","cc4f0a23":"def add_date_feature(df):\n    df[\"year\"] = df[\"date\"].dt.year.astype(\"int16\")\n    df[\"month\"] = df[\"date\"].dt.month.astype(\"int8\")\n    df[\"week\"] = df[\"date\"].dt.week.astype(\"int8\")\n    df[\"day\"] = df[\"date\"].dt.day.astype(\"int8\")\n    df[\"quarter\"]  = df[\"date\"].dt.quarter.astype(\"int8\")\n    return df","193efa16":"df = add_date_feature(df)\ndf","344d8d87":"temp_series = df.groupby([\"cat_id\", \"date\"])[\"value\"].sum()\ntemp_series","67f971a1":"plt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each Category\")\nplt.legend()\n","cf57089f":"temp_series = temp_series.loc[temp_series.index.get_level_values(\"date\") >= \"2015-01-01\"]\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.xlabel(\"Year-Month\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each Category from 2015\")\nplt.legend()","62e3f6cb":"# Plot only December, 2015\ntemp_series = temp_series.loc[(temp_series.index.get_level_values(\"date\") >= \"2015-12-01\") & (temp_series.index.get_level_values(\"date\") <= \"2015-12-31\")]\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total sold item per day in December, 2015\")\nplt.legend()","41e38326":"temp_series.loc[(temp_series.index.get_level_values(\"date\") >= \"2015-12-24\") & (temp_series.index.get_level_values(\"date\") <= \"2015-12-26\")]","455b9ef6":"temp_series = df.groupby([\"cat_id\", \"wday\"])[\"value\"].sum()\ntemp_series","38a249f0":"plt.figure(figsize=(6, 4))\nleft = np.arange(1,8) \nwidth = 0.3\nweeklabel = [\"Saturday\", \"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\"]    # Please Confirm df\n\n\nplt.bar(left, temp_series[temp_series.index.get_level_values(\"cat_id\") == \"FOODS\"].values, width=width, label=\"FOODS\")\nplt.bar(left + width, temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, width=width, label=\"HOUSEHOLD\")\nplt.bar(left + width + width, temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, width=width, label=\"HOBBIES\")\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.xticks(left, weeklabel, rotation=60)\nplt.xlabel(\"day of week\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total sold item in each daytype\")","2f63267c":"temp_series = df.groupby([\"state_id\", \"date\"])[\"value\"].sum()\ntemp_series","d388ee82":"plt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].values, label=\"CA\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].values, label=\"TX\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].values, label=\"WI\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each State\")\nplt.legend()","1fefe8fd":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].values, label=\"CA_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].values, label=\"CA_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].values, label=\"CA_3\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].values, label=\"CA_4\")\n\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total Item Sold Transition of each Store in CA\")\nplt.legend()","420e78f0":"temp_series = df.groupby([\"store_id\", \"date\"])[\"item_id\"].count()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_1\"].values, label=\"CA_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_2\"].values, label=\"CA_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_3\"].values, label=\"CA_3\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"CA_4\"].values, label=\"CA_4\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of item entries\")\nplt.title(\"Total item entries in each CA stores\")\nplt.legend()","641c2cc0":"temp_series = df.groupby([\"state_id\", \"store_id\", \"year\", \"month\"])[\"value\"].std()\ntemp_series","3a22fc84":"fig, axs = plt.subplots(1, 3, figsize=(12, 6), sharey=True)\n\n# We can use for loop, of course! And that'll be better, sorry, this is for my easy trial. \nsns.lineplot(x=temp_series[(temp_series.index.get_level_values(\"state_id\") == \"CA\")].index.get_level_values(\"month\"), \n             y=temp_series[(temp_series.index.get_level_values(\"state_id\") == \"CA\")].values, \n             hue=temp_series[(temp_series.index.get_level_values(\"state_id\") == \"CA\")].index.get_level_values(\"store_id\"), \n             legend=False,\n             ax=axs[0])\nsns.lineplot(x=temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].index.get_level_values(\"month\"),\n             y=temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].values, \n             hue=temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].index.get_level_values(\"store_id\"), \n             legend=False,\n             ax=axs[1])\nsns.lineplot(x=temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].index.get_level_values(\"month\"),\n             y=temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].values, \n             hue=temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].index.get_level_values(\"store_id\"),\n             ax=axs[2])\n\n\n\nplt.legend(bbox_to_anchor=(1.01, 1.01))\naxs[0].set_title(\"CA\")\naxs[0].set_xticks(range(1, 13))\naxs[0].set_ylabel(\"Standard deviation of sold items in one month\")\naxs[1].set_title(\"TX\")\naxs[1].set_xticks(range(1, 13))\naxs[2].set_title(\"WI\")\naxs[2].set_xticks(range(1, 13))\n\nfig.suptitle(\"Standard deviation of sold items in one month in each store\")","e03e3233":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].values, label=\"WI_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].values, label=\"WI_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].values, label=\"WI_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of sold items\")\nplt.title(\"Total item sold in each WI stores\")\nplt.legend()","b9950afe":"temp_series = df.groupby([\"store_id\", \"date\"])[\"item_id\"].count()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_1\"].values, label=\"WI_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_2\"].values, label=\"WI_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"WI_3\"].values, label=\"WI_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"# of item entries\")\nplt.title(\"Total item entries in each WI stores\")\nplt.legend()","91094cd9":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].values, label=\"TX_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].values, label=\"TX_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].values, label=\"TX_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Total sold item per day\")\nplt.title(\"Total item sold in each TX stores\")\nplt.legend()","1c8f0a2a":"temp_series = df.groupby([\"store_id\", \"date\"])[\"item_id\"].count()\n\nplt.figure(figsize=(12, 4))\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_1\"].values, label=\"TX_1\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_2\"].values, label=\"TX_2\")\nplt.plot(temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].index.get_level_values(\"date\"), temp_series[temp_series.index.get_level_values(\"store_id\") == \"TX_3\"].values, label=\"TX_3\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Total item entries\")\nplt.title(\"Total item entries in each TX stores\")\nplt.legend()","9dc14d6e":"temp_series = df.groupby([\"store_id\", \"date\"])[\"value\"].sum()\ntemp_series","06e29020":"# Find the day when items are sold less than 1000 of each store\n# Let's take a look at TX_2 for example\ntemp_series.loc[(temp_series.values < 1000) & (temp_series.index.get_level_values(\"date\") <= \"2016-04-22\")].loc[\"TX_2\"]","d27c75e1":"# Find the day when items are sold most of each store\ntemp_series.groupby([\"store_id\"]).idxmax()","123f73f1":"temp_series = temp_series.reset_index()\ntemp_series","929f0615":"plt.plot(temp_series[(temp_series[\"store_id\"] == \"CA_1\") & ((temp_series[\"date\"] >= \"2013-07-15\") & (temp_series[\"date\"] <= \"2013-10-15\"))][\"date\"],\n         temp_series[(temp_series[\"store_id\"] == \"CA_1\") & ((temp_series[\"date\"] >= \"2013-07-15\") & (temp_series[\"date\"] <= \"2013-10-15\"))][\"value\"])\nplt.xticks(rotation=60)\nplt.ylabel(\"# of sold items\")\nplt.xlabel(\"date\")\nplt.title(\"Item sold transition around its most sold day in CA_1 store\")","fc2b6cfd":"import statsmodels.api as sm\nimport scipy","92ef5487":"# Analysis target item is the most sold one.\n# In this Dynamic Factor Analysis Session, we'll try to find the hidden factor of this item sales transition among states.\nitem_id = \"FOODS_3_090\"\nstate_list = [\"CA\", \"TX\", \"WI\"]","7f8199da":"# First, we extract the target item sold sum in each state\ntemp_series = df[(df.date >= \"2015-01-01\") & (df.date <= \"2016-01-01\") & (df.item_id == item_id)].groupby([\"date\", \"state_id\"])[\"value\"].sum()\n\n# Then convert it to dataframe type.\ntemp_df = pd.concat([pd.DataFrame(temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].values),\n                     pd.DataFrame(temp_series[temp_series.index.get_level_values(\"state_id\") == \"TX\"].values), \n                     pd.DataFrame(temp_series[temp_series.index.get_level_values(\"state_id\") == \"WI\"].values)], axis=1) \ntemp_df.columns = state_list\ntemp_df.index = temp_series[temp_series.index.get_level_values(\"state_id\") == \"CA\"].index.get_level_values(\"date\")\ntemp_df.head()","e8e0ec6d":"# First, we plot this item transition in each state\ntemp_df.plot(figsize=(12, 4))\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.ylabel(\"# of sold item\")\nplt.title(f\"{item_id} sales transition in each state\")","e5d815cd":"# Next Step, we create diff column, which shows the difference of today and tommorow's sold count.  \n# For flattening, we apply log function and standardization\n\ndiff_cols = [\"diff_\" + state for state in state_list]      # diff columns (row data)\nstd_cols = [\"std_diff_\" + state for state in state_list]   # diff columns (after standardization)\n\nfor state in state_list:\n    col = \"diff_\" + state\n    temp_df[col] = np.log(temp_df[state] + 0.1).diff() * 100\n    temp_df[col] = temp_df[col].fillna(method=\"bfill\")\n    \n    # Standardization\n    std_col = \"std_\" + col\n    temp_df[std_col] = (temp_df[col] - temp_df[col].mean()) \/ temp_df[col].std()","52362b94":"# Conveert it to Z-value\ntemp_df[std_cols] = temp_df[std_cols].apply(scipy.stats.zscore, axis=0)","c2022f33":"temp_df","76f46448":"# Create the model\n# This time, for simplicity, unobserved factor (k_factors) is 1, and factor_order is 1 (i.e. it follows an AR(1) process) , \n# error_order is 1 (i.e. error has order 1 autocorelated), \n\nendog = temp_df.loc[:, std_cols]\nmod = sm.tsa.DynamicFactor(endog, k_factors=1, factor_order=1, error_order=1)\ninitial_res = mod.fit(method='powell', disp=False)\nres = mod.fit(initial_res.params, disp=False)","564c2dcd":"print(res.summary(separate_params=False))","1233e8f8":"from pandas_datareader.data import DataReader\n\nfig, ax = plt.subplots(figsize=(13,3))\n\n# Plot the factor\ndates = endog.index._mpl_repr()\nax.plot(dates, res.factors.filtered[0], label='Factor')\nax.legend()\n\n# Retrieve and also plot the NBER recession indicators\nrec = DataReader('USREC', 'fred', start=temp_df.index.min(), end=temp_df.index.max())\nylim = ax.get_ylim()\nplt.title(\"Fluctuations extracted by Factor 1\")\nplt.ylabel(\"Fluctuations\")\nplt.xlabel(\"Year-Month\")","7ce717e7":"res.coefficients_of_determination","feca45a7":"res.plot_coefficients_of_determination();","af5bda55":"temp_series = df.groupby([\"store_id\", \"cat_id\"])[\"value\"].sum()","a4642fd0":"store_id_list_by_state = [[\"CA_1\", \"CA_2\", \"CA_3\", \"CA_4\"], [\"TX_1\", \"TX_2\", \"TX_3\"], [\"WI_1\", \"WI_2\", \"WI_3\"]] ","1b4bad9e":"fig, axs = plt.subplots(3, 4, figsize=(16, 12), sharey=True) \n\nfor row in range(len(store_id_list_by_state)):\n    for col in range(len(store_id_list_by_state[row])):\n        axs[row, col].bar(x=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].index.get_level_values(\"cat_id\"),\n                          height=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].values,\n                         color=[\"orange\", \"green\", \"blue\"], label=[\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"])\n        axs[row, col].set_title(store_id_list_by_state[row][col])\n        axs[row, col].set_ylabel(\"# of items\")\n\nfig.suptitle(\"Each category item sold in each store\")","5665a827":"fig, axs = plt.subplots(3, 4, figsize=(16, 12), sharey=True) \n\nfor row in range(len(store_id_list_by_state)):\n    for col in range(len(store_id_list_by_state[row])):\n        axs[row, col].bar(x=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].index.get_level_values(\"cat_id\"),\n                          height=temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].values \/ temp_series[temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col]].sum(),\n                         color=[\"orange\", \"green\", \"blue\"], label=[\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"])\n        axs[row, col].set_title(store_id_list_by_state[row][col])\n        axs[row, col].set_ylabel(\"% of each category\")\n\nfig.suptitle(\"Each category item sold percentage in each store\")","31fc165c":"cat_id = \"FOODS\"\n\ntemp_series = df.groupby([\"store_id\", \"cat_id\", \"wday\"])[\"value\"].sum()\ntemp_series = temp_series[temp_series.index.get_level_values(\"cat_id\") == cat_id]\ntemp_series","dd41e6d6":"weekday = [\"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]","ec379f6a":"# Combine all these three figures.\ncat_list = [\"FOODS\", \"HOBBIES\", \"HOUSEHOLD\"]\ncolor_list = [\"orange\", \"green\", \"blue\"]\ntemp_series = df.groupby([\"store_id\", \"cat_id\", \"wday\"])[\"value\"].sum()\nwidth = 0.25\n\nfig, axs = plt.subplots(3, 4, figsize=(20, 12), sharey=True) \n\nfor row in range(len(store_id_list_by_state)):\n    for col in range(len(store_id_list_by_state[row])):\n        for i, cat in enumerate(cat_list):\n            height_numerator = temp_series[(temp_series.index.get_level_values(\"cat_id\") == cat) & (temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col])].values\n            height_denominater = height_numerator.sum()\n\n            axs[row, col].bar(x=temp_series[(temp_series.index.get_level_values(\"cat_id\") == cat) & (temp_series.index.get_level_values(\"store_id\") == store_id_list_by_state[row][col])].index.get_level_values(\"wday\") + width * (i-1),\n                              height=height_numerator \/ height_denominater,\n                             tick_label=weekday, color=color_list[i], width=width, label=cat)\n            axs[row, col].set_title(store_id_list_by_state[row][col])\n            axs[row, col].legend()\n            \nfig.suptitle(\"HOBBIES item sold in each store in each day\")","49b02d7f":"fig, axs = plt.subplots(1, 3, sharey=True)\nfig.suptitle(\"Snap Purchase Enable Day Count of each store\")\n\nsns.countplot(x=\"snap_CA\", data =calendar_df, ax=axs[0])\nsns.countplot(x=\"snap_TX\", data =calendar_df, ax=axs[1])\nsns.countplot(x=\"snap_WI\", data =calendar_df, ax=axs[2])","33b060f5":"temp_df = calendar_df.groupby([\"year\"])[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].sum()\ntemp_df","f86a19b3":"# This cell is just visuallizing the above dataframe.\nplt.bar(temp_df.index, temp_df.snap_CA)\nplt.ylabel(\"# of snap purchase allowed day\")\nplt.xlabel(\"Year\")\nplt.title(\"Snap Purchase allowed day yearly transition\")","faa68faa":"temp_df = calendar_df[calendar_df[\"year\"] == 2015].groupby([\"month\"])[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].sum()\ntemp_df","862ac015":"# Just visualizing the above dataframe\nplt.bar(temp_df.index, temp_df.snap_CA)\nplt.ylabel(\"# of snap purchase allowed day\")\nplt.xlabel(\"Month\")\nplt.title(\"Snap Purchase allowed day monthly trend\")","50cceec9":"temp_df = calendar_df[calendar_df[\"year\"] == 2015].groupby([\"weekday\"])[[\"snap_CA\", \"snap_TX\", \"snap_WI\"]].sum()\ntemp_df","0d25b809":"plt.bar(temp_df.index, temp_df.snap_CA)\nplt.xticks(rotation=60)\nplt.ylabel(\"# of snap purchase allowed day\")\nplt.xlabel(\"Day type\")\nplt.title(\"Snap Purchase allowed day weekly trend\")","ba228b52":"# Make temp dataframe with necessary information\ntemp_df = df.groupby([\"date\", \"state_id\"])[[\"value\"]].sum()\ntemp_df = temp_df.reset_index()\ntemp_df = temp_df.merge(calendar_df[[\"date\", \"snap_CA\", \"snap_TX\", \"snap_WI\"]], on=\"date\")\ntemp_df","30e60912":"np.argmax(temp_df.groupby([\"date\", \"state_id\"])[\"value\"].sum())","8dac45d3":"temp_df = temp_df[(temp_df.date >= \"2016-02-15\") & (temp_df.date <= \"2016-03-25\") & (temp_df.state_id == \"CA\")]\ntemp_df","fcb0bc6f":"fig, ax1 = plt.subplots()\nplt.xticks(rotation=60)\nax1.plot(\"date\", \"value\", data=temp_df[temp_df.state_id == \"CA\"])\nax2 = ax1.twinx()  \nax2.scatter(\"date\", \"snap_CA\", data=temp_df[temp_df.state_id == \"CA\"])","7e8b997f":"plt.figure(figsize=(8, 6))\nsns.countplot(x=\"event_type_1\", data=calendar_df[calendar_df[\"event_name_1\"] != \"unknown\"])\nplt.xticks(rotation=90)\nplt.title(\"Event Type Count in event name 1 column\")","0072ac6c":"# Let's check the distribution of snap purchase day and event day\n# Accirding to the graph, Snap CA is allowed especially when sport event occurs.\n\nplt.figure(figsize=(8, 6))\nsns.countplot(x=\"event_type_1\", data=calendar_df[calendar_df[\"event_name_1\"] != \"unknown\"], hue=\"snap_CA\")\nplt.xticks(rotation=90)\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.title(\"Snap Purchse allowed day Count in each event category\")","e64fc2b6":"temp_series = df.groupby([\"cat_id\", \"event_type_1\"])[\"value\"].mean()\ntemp_series","214c118c":"plt.bar(x=temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"event_type_1\"), \n        height=temp_series[temp_series.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values)\nplt.title(\"HOBBIES Item Sold mean in each event type\")\nplt.ylabel(\"Item sold mean\")\nplt.xlabel(\"Event Type\")","d012d000":"# find out most sold item for example\ndf[df[\"value\"] == df[\"value\"].max()]","11697722":"target_id = \"FOODS_3_090_CA_3_validation\"\ntemp_df = df[df[\"id\"] == target_id]\ntemp_df","13bd33c5":"weekday = [\"Sat\", \"Sun\", \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\"]\n\n# Create one hot weekday column from wday column to calculate correlation later. \nfor idx, val in enumerate(weekday):\n    temp_df.loc[:, val] = (temp_df[\"wday\"] == idx + 1).astype(\"int8\")\n\ntemp_df\n# sns.heatmap(temp_df[[\"value\", \"snap_CA\", ]].corr(), annot=True)","30665376":"# Create Event Flag (Any events occur: 1, otherwise: 0)\n# Create Each Event Type Flag\ntemp_df.loc[:, \"is_event_day\"] = (temp_df[\"event_name_1\"] != \"unknown\").astype(\"int8\")\ntemp_df.loc[:, \"is_sport_event\"] = (temp_df[\"event_type_1\"] == \"Sporting\").astype(\"int8\")\ntemp_df.loc[:, \"is_cultural_event\"] = (temp_df[\"event_type_1\"] == \"Cultural\").astype(\"int8\")\ntemp_df.loc[:, \"is_national_event\"] = (temp_df[\"event_type_1\"] == \"National\").astype(\"int8\")\ntemp_df.loc[:, \"is_religious_event\"] = (temp_df[\"event_type_1\"] == \"Religious\").astype(\"int8\")\n\ntemp_df.head()","dc6f5195":"# Plot Heatmap with these columns made in previous cells\nplt.figure(figsize=(14, 10))\nsns.heatmap(temp_df[[\"value\", \"sell_price\", \"snap_CA\", \"is_event_day\", \"is_sport_event\", \"is_cultural_event\", \"is_national_event\", \"is_religious_event\"] + weekday].corr(), annot=True)\nplt.title(\"Heatmap with values, snap_CA,  event_flag and weekday columns\")","2162e4ce":"df.groupby(\"cat_id\")[\"sell_price\"].mean()","cafd434a":"df.groupby(\"cat_id\")[\"sell_price\"].describe()","1f855367":"sns.boxplot(data=df, x=\"cat_id\", y='sell_price')\nplt.title(\"Boxplot of sell prices in each category\")","0464265a":"plt.figure(figsize=(12, 6))\nsns.boxplot(data=df, x=\"cat_id\", y='sell_price', hue=\"store_id\")\nplt.title(\"Boxplot of sell prices in each store\")","505e51c2":"# One Item Sell Price Transition\nsns.lineplot(data=df[df[\"item_id\"] == \"FOODS_3_090\"], x='date', y='sell_price', hue=\"store_id\")\nplt.legend(bbox_to_anchor=(1.01, 1.01))\nplt.title(\"Sell price change of 'FOODS_3_090' in each store\")","10b6c518":"df[\"is_event_day\"] = (df[\"event_name_1\"] != \"unknown\").astype(\"int8\")\ndf.head()","f7975f63":"sns.heatmap(df[df[\"item_id\"] == \"FOODS_3_090\"][[\"value\", \"sell_price\", \"is_event_day\"]].corr(), annot=True)\nplt.title(\"Heatmap of value, sell_price and event flag\")","fd045e23":"temp_df = df.groupby([\"date\", \"cat_id\"])[\"sell_price\"].mean()\ntemp_df","7f2677ec":"plt.figure(figsize=(8,4))\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.legend()\nplt.xlabel(\"Year\")\nplt.ylabel(\"Mean price\")\nplt.title(\"Mean price transition of each category\")","cb77cadb":"temp_df = df.groupby([\"date\", \"cat_id\"])[\"item_id\"].count()","096f9ddf":"plt.figure(figsize=(8,4))\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"FOODS\"].values, label=\"FOODS\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOUSEHOLD\"].values, label=\"HOUSEHOLD\")\nsns.lineplot(x=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].index.get_level_values(\"date\"), y=temp_df[temp_df.index.get_level_values(\"cat_id\") == \"HOBBIES\"].values, label=\"HOBBIES\")\nplt.legend()\nplt.xlabel(\"Year\")\nplt.ylabel(\"Registered Item Counts\")\nplt.title(\"Registered Item Counts Transition in each category\")","68fa0546":"sns.jointplot(df[\"value\"], df[\"sell_price\"])","caa24555":"df[\"sell_price_diff\"] = df.groupby(\"id\")[\"sell_price\"].transform(lambda x: x - x.mean()).astype(\"float32\")","6ac99921":"sns.lineplot(df[df[\"item_id\"] == \"FOODS_3_090\"][\"date\"],df[df[\"item_id\"] == \"FOODS_3_090\"][\"sell_price_diff\"], hue=df[\"store_id\"]) \nplt.legend(bbox_to_anchor=(1.01, 1.01))","4c2091ea":"temp_df = df[df[\"item_id\"] == \"FOODS_3_090\"].groupby(\"date\")[\"value\"].sum()","21884299":"fig, ax1 = plt.subplots(figsize=(12, 4))\nax1.plot(temp_df)\nax1.set_ylabel(\"# of Sold items\")\nax2 = ax1.twinx()  \nax2.plot(df[(df[\"item_id\"] == \"FOODS_3_090\") & (df[\"store_id\"] == \"CA_3\")][\"date\"],\n         df[df[\"item_id\"] == \"FOODS_3_090\"].groupby(\"date\")[\"sell_price_diff\"].mean(), color=\"red\")\nax2.set_ylabel(\"price_diff [\\$]\") \nplt.title(\"FOODS_3_090 sold number and price difference from mean\")","3cb00f1d":"# Find the most sold items in  HOBBIES section\nnp.argmax(df[df[\"cat_id\"] == \"HOBBIES\"].groupby(\"item_id\")[\"value\"].sum())","10007ed3":"temp_df = df[df[\"item_id\"] == \"HOBBIES_1_371\"].groupby(\"date\")[\"value\"].sum()","aba97f04":"fig, ax1 = plt.subplots(figsize=(12, 4))\nax1.plot(temp_df)\nax1.set_ylabel(\"# of Sold items\")\nax2 = ax1.twinx()  \nax2.plot(df[(df[\"item_id\"] == \"HOBBIES_1_371\") & (df[\"store_id\"] == \"CA_3\")][\"date\"],\n         df[df[\"item_id\"] == \"HOBBIES_1_371\"].groupby(\"date\")[\"sell_price_diff\"].mean(), color=\"red\")\nax2.set_ylabel(\"price_diff [\\$]\") \nplt.title(\"HOBBIES_1_371 sold number and price difference from mean\")","9abca38b":"temp_series = df.groupby([\"date\", \"item_id\"])[\"value\"].sum()\ntemp_series","c41a5ecc":"# Find Top 12 items that the mean of sold counts in all stores is high\nhigh_sold_item_top12 = temp_series.groupby(\"item_id\").mean().sort_values(ascending=False)[:12].index\nhigh_sold_item_top12","59d2510c":"fig, axs = plt.subplots(3, 4, figsize=(20, 12), sharey=True, sharex=True) \n\nfor row in range(3):\n    for col in range(4):\n        target_item = high_sold_item_top12[row*4+col]\n        \n        axs[row, col].plot(temp_series[temp_series.index.get_level_values(\"item_id\") == target_item].values)\n        axs[row, col].set_title(target_item)\n        axs[row, col].set_ylabel(\"# of sold items in all stores\")\n#         axs[idx, col].set_xticks(temp_series[temp_series.index.get_level_values(\"item_id\") == target_item].index.get_level_values(\"date\"))\n        axs[row, col].legend()\n            \nfig.suptitle(\"Top 12 item sold in all stores in each day\")","e0048f97":"df.head()","276f791f":"# Take a closer look,\nsns.violinplot(x = df.loc[df[\"year\"] == 2015, \"quarter\"], y = df.loc[df[\"year\"] == 2015, \"value\"])\nplt.ylim(0, 10)","68e35650":"temp_df = df.groupby([\"date\", \"cat_id\", \"dept_id\"])[\"value\", \"sell_price\"].mean()","f2cbd0b1":"fig, axs = plt.subplots(2, 3, figsize=(14, 10)) \n\n\nfor col in range(3):\n    target_cat = cat_list[col]\n    sns.scatterplot(\"value\", \"sell_price\", hue=temp_df[temp_df.index.get_level_values(\"cat_id\") == target_cat].index.get_level_values(\"dept_id\") ,\n                data=temp_df[temp_df.index.get_level_values(\"cat_id\") == target_cat], ax=axs[0, col])\n#     axs[0, col].plot(temp_series[temp_series.index.get_level_values(\"item_id\") == target_item].values)\n    axs[0, col].set_title(f\"{target_cat} \")\n    axs[0, col].set_ylabel(\"sell_price\")\n    axs[0, col].set_xlabel(\"value\")\n#         axs[idx, col].set_xticks(temp_series[temp_series.index.get_level_values(\"item_id\") == target_item].index.get_level_values(\"date\"))\n    axs[0, col].legend()\n    \n    temp_series = df[df[\"cat_id\"] == target_cat].groupby([\"date\", \"dept_id\"])[\"item_id\"].count()\n    sns.lineplot(x=temp_series.index.get_level_values(\"date\"), y=temp_series.values, hue=temp_series.index.get_level_values(\"dept_id\"), ax=axs[1, col])\n    axs[1, col].set_title(f\"{target_cat}\")\n    axs[1, col].set_ylabel(\"count\")\n    axs[1, col].legend()\n            \nfig.suptitle(\"Daily Average value - price plot and item count transition in each dept.\")","2d3b885b":"df[\"num_of_next_week_event\"] = df.groupby(\"id\")[\"is_event_day\"].transform(lambda x: x.shift(-7).rolling(7).sum().fillna(0)).astype(\"int8\")","ddaa1729":"df[df.id == \"HOBBIES_1_008_CA_1_validation\"][25:50]","a8c2e869":"fig, axs = plt.subplots(1, 3, figsize=(8, 6), sharey=True)\nsns.scatterplot(\"num_of_next_week_event\", \"value\", data=df[df[\"cat_id\"] == \"HOBBIES\"], color=\"green\",ax=axs[0])\nsns.scatterplot(\"num_of_next_week_event\", \"value\", data=df[df[\"cat_id\"] == \"HOUSEHOLD\"], color=\"orange\", ax=axs[1])\nsns.scatterplot(\"num_of_next_week_event\", \"value\", data=df[df[\"cat_id\"] == \"FOODS\"], ax=axs[2])\naxs[0].set_title(\"HOBBIES\")\naxs[1].set_title(\"HOUSEHOLD\")\naxs[2].set_title(\"FOODS\")\n\nfig.suptitle(\"Relationship between next week events count and sold item counts\")","23b70105":"df[\"lag_1\"] = df.groupby(\"id\")[\"value\"].transform(lambda x: x.shift(1)).astype(\"float16\")\ndf[\"lag_7\"] = df.groupby(\"id\")[\"value\"].transform(lambda x: x.shift(7)).astype(\"float16\")","f03d7353":"# plt.figure(figsize=(8, 8))\n# sns.pairplot(df[[\"cat_id\", \"value\", \"lag_1\"]], hue=\"cat_id\")","fa66928b":"sns.pairplot(df[[\"cat_id\", \"value\", \"lag_1\", \"lag_7\"]], hue=\"cat_id\")","05aa7115":"!pip install calmap","d3e6c20c":"import calmap\n\ntemp_df = df.groupby([\"state_id\", \"date\"])[\"value\"].sum()\ntemp_df = temp_df.reset_index()\ntemp_df = temp_df.set_index(\"date\")\ntemp_df","faa2ab02":"fig, axs = plt.subplots(3, 1, figsize=(10, 10))\ncalmap.yearplot(temp_df.loc[temp_df[\"state_id\"] == \"CA\", \"value\"], year=2015, ax=axs[0])\naxs[0].set_title(\"CA\")\ncalmap.yearplot(temp_df.loc[temp_df[\"state_id\"] == \"TX\", \"value\"], year=2015, ax=axs[1])\naxs[1].set_title(\"TX\")\ncalmap.yearplot(temp_df.loc[temp_df[\"state_id\"] == \"WI\", \"value\"], year=2015, ax=axs[2])\naxs[2].set_title(\"WI\")","997ed58d":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder","244353e1":"# First, we try store clustering by using weekly total sales \ntemp_df = df.groupby([\"store_id\", \"wm_yr_wk\"])[\"value\"].sum()\ntemp_df = temp_df.reset_index()\ntemp_df","a538e0bb":"# Convert it to wide format\ntemp_df_wide = temp_df.pivot(index=\"store_id\", columns=\"wm_yr_wk\", values=\"value\")\ntemp_df_wide","ad46acc6":"# By using PCA, we could decrease 10 rows * 282 dimentions to 10 rows * 2 dimentions\npca = PCA(n_components=2)","bb82f7bd":"pca.fit(temp_df_wide)","20fbabdf":" pca.explained_variance_ratio_","0f1f12d7":"result = pca.transform(temp_df_wide)\nresult","94b0cce6":"result_df = pd.DataFrame(result)\nresult_df.index = temp_df_wide.index\nresult_df.columns = [\"PC1\", \"PC2\"]\nresult_df","291130c4":"ax = result_df.plot(kind='scatter', x='PC2', y='PC1', figsize=(12, 6))\n\nfor idx, store_id in enumerate(result_df.index):\n    ax.annotate(  \n        store_id,\n       (result_df.iloc[idx].PC2, result_df.iloc[idx].PC1)\n    )\n\nax.set_title(\"PCA result of all shops\")","d5366c5b":"temp_df = df.groupby([\"item_id\", \"wm_yr_wk\"])[\"value\"].sum()\ntemp_df = temp_df.reset_index()\ntemp_df","abbaa872":"temp_df = temp_df.fillna(method=\"bfill\")","d879a166":"# Convert it to wide format\ntemp_df_wide = temp_df.pivot(index=\"item_id\", columns=\"wm_yr_wk\", values=\"value\")\ntemp_df_wide","77770f22":"# By using PCA, we could decrease 10 rows * 282 dimentions to 10 rows * 2 dimentions\npca = PCA(n_components=2)","7b857d4b":"pca.fit(temp_df_wide)","74c6fe3c":"pca.explained_variance_ratio_","f3c9b547":"result = pca.transform(temp_df_wide)\nresult","535390a9":"result_df = pd.DataFrame(result)\nresult_df.index = temp_df_wide.index\nresult_df.columns = [\"PC1\", \"PC2\"]\nresult_df","6f78a0c3":"result_df.index.str[:5]","07799577":"plt.figure(figsize=(8, 6))\nsns.scatterplot(x=\"PC2\", y=\"PC1\", data=result_df, hue=result_df.index.str[:5])\nplt.title(\"PCA result for item_id column\")","9dd5f6d0":"## Dynamic Factor Analysis Trial\nDynamic Factor analysis is one of the dimention reduction technoloies like PCA (Principle Component Analysis)  \nSuppose we have some time-series data, we can apply this technique to find the hidden factor of these time-series data.  \n(You can learn more here: https:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/statespace_dfm_coincident.html)  \nAnd what I tried below is similar to the tutorial of this statsmodel's website.\n\nWe apply this to the target item sold of each store and find some hidden factors among stores.","89894b97":"# References\nFollowing notebooks are the great notebooks in this competition. \nFor whom hasn't check these notebooks, I strongly recommend you to take a look at these notebooks.\n(I'm sorry if I missed some other great kernels, I'll take a lookt at other notebooks if I have enough time.)\n\nData Visualization:\n\n- **M5 Forecasting - Starter Data Exploration**  \n  https:\/\/www.kaggle.com\/robikscube\/m5-forecasting-starter-data-exploration  \n\n-  **Back to (predict) the future - Interactive M5 EDA**  \n   https:\/\/www.kaggle.com\/headsortails\/back-to-predict-the-future-interactive-m5-eda\n\nMaking Prediction:\n\n- **M5 - Three shades of Dark: Darker magic**  \n  https:\/\/www.kaggle.com\/kyakovlev\/m5-three-shades-of-dark-darker-magic\n\n-  **Very fst Model**  \n   https:\/\/www.kaggle.com\/ragnar123\/very-fst-model\n   \n   \n   \nFor PCA, I also refered some Japanese site here:\nhttps:\/\/qiita.com\/mshinoda88\/items\/0e54e7e03e1aa52edb5b\n","5b7c12fd":"I got top 12 sold items in all stores per one day.  \nThe result shows \"FOODS_3_090\" has very higher sold than others.  \nAnd all items here are FOODS category and department no is 3!\n\nSo, next question, are there any features in each dept?  \nFor example, in FOODS_3 department, do they treat cheap and frequently-used item?","2072ce93":"1. In all categories, the periodical trends is seemed weekly.  \n   In previous graph, we can't easily recognize that HOUSEHOLD and HOBBIES have weekly features, but in this graph we can.\n   \n2. The day when all item sold is 0 is seemed to be Christmas Day, not new year's day, confirm it below.","582ac24b":"I don't understand why mean = NaN when using *.describe* method, however, *.mean()* method accurately calculate category's sell price mean.  \nLet's plot it with some ways!","f1e96bee":"From the above plot, we could also find FOODS category has more variance than other categories.  \nFOODS category has some extra high point in PC1 compared to others.","8c7ab15c":"## PCA Trial","cd6d225e":"Let's check whether the trainsaction is right.  \n\nLooking at \"num_of_next_week_event\" column, the first point of non-zero in the above dataframe is on \"2011-03-02\" (Wednesday) .  \nSince we want to know the next week event sum, this means on \"2011-03-09\" (next Wednesday) we have some events and this is true.  \nBetween \"2011-03-10\" and \"2011-03-15\", we dont' have any more events, so the num_of_next_week_event column between \"2011-03-03\" and \"2011-03-08\" should be 1 \n(This is because we count only \"2011-03-09\" day)  \nAnd corresponding to the event on \"2011-03-16\", next_week_event_sum column gets 1 on \"2011-03-09'.  \nNext day, we also have events, so corresponding to that, next_week_event_sum column gets 2 from \"2011-03-10\".  \n\nDoes this explanation make sense to you?\n\nLike this way, we can add feature of next_week event, since we ofen go shopping when we have big events in the near furture.  \nNow let's see whether this column has really positive relation to the value.","fffbd0f6":"1. CA is the most sold state of these three states.  \n   TX and WI are not so different except for the year 2011 and 2012.\n  \n2. All three states have some periodical features as we've already seen in category-based item sold graph. \n\nFirst, let's focus on stores in CA.","2a4e3793":"OK, total count in one year is almost the same in all years and all states.\nHow about monthly distribution?","16f63bb3":"Compared to the previous FOOD item result, this item has lower volatility of sell price.   \nAnd the number of sold items has also lower volatility.  \n(In the FOOD item, we have around 2.5 times as much as usual sell on the top season)  \n\nAgain, we see some non sold period in the latter half of 2012.  \nDoes all item have these kinds of non-sold period?","dbf6e7b5":"## One Item Features Analysis","26ed5922":"On Christmas Day, the items sold are seemed to be 0, let's check it with *.loc* method","d77d4021":"1. Stores in WI have similar item sold count.  \n   Before 2013, WI_3 is the most sold store in WI, but WI_2 gradually increases its proportion. (Especially around summer in 2012)\n   \n2. In some point, WI_1 rapidly increase its sold item count. (Around on November 2012)\n\n-> Total Sold out count depends on the number of entries at that day.  So Now we check the total item entries in each store as we did in CA stores.","6fbf237f":"Though the item is same, sell price is slightly different in each store and each season. -> Some promotion season?","87d0bcc3":"We can see in quarter 3 (from July to September), the item sold seems less than other quarters.\n\nviolinplot method can plot values with its distribution.  \nYou can search seaborn violinplot (https:\/\/seaborn.pydata.org\/generated\/seaborn.violinplot.html)\n> A violin plot plays a similar role as a box and whisker plot. It shows the distribution of quantitative data across several levels of one (or more) categorical variables such that those distributions can be compared. Unlike a box plot, in which all of the plot components correspond to actual datapoints, the violin plot features a kernel density estimation of the underlying distribution.\n\nThus, the difference between boxplot and violinplot is, as the official document says, violinplot plots with its kernel density estimation.","751f7a0a":"This is the summary result of model fitting. \nIt describes model information and some statistical values.  ","c36def98":"All store has lower price than mean value in the latter half of 2013.  ","40c74a89":"# Store Analysis","403bcde8":"1. As we can probraly guess, Saturday or Sunday is the day which the items are most sold.  \n   Tuesday or Wednesday is the least sold days.  \n   -> Later, we visualize these correlation factors with heatmap. Looking forward to it!\n\n2. HOBBIES are not so day dependent compared to FOODS or HOUSEHOLD.  ","15a41741":"Compared to other states, TX stores have similar tendency regarding registered entries.","a91d1275":"1. With lineplot method in seaborn, we can plot single line with error bands showing a confidence interval.  \n   You can serach seaborn lineplot and find this sentence. (https:\/\/seaborn.pydata.org\/generated\/seaborn.lineplot.html)\n>  By default, the plot aggregates over multiple y values at each value of x and shows an estimate of the central tendency and a confidence interval for that estimate.  \n\n   In this case, this command plots each month item sold standard deviation with confidence interval in 4 years. (From 2011 to 2015)\n   \n\n2. Since CA_3 is the most sold store in CA, standard deviation of this store is also higher than others.  \n   By using seaborn, we can create these good plots.\n   \n \nLet's check other state, WI next!","45e49faa":"## Sell price and value relationship","457d419a":"Event Day flag and Sell Price don't have so strong relationship.  \nHowever, when we buy items for some events, we perhaps buy items 1 week ~ 1 day before the event, not on the same day.  \nSo we have to take this into consideration.  (I'll tackle with this analysis later.)","175696da":"## Calendar Visualization","2d08d8ee":"OK, total count in one month is the same through the whole year.\nHow about weekly distribution?","674c36e1":"Regarding weekly trend, we can find no biased distribution.  \nThis is also almost uniformly distributed like year total and month total.","efefe109":"# Import Libraries and Data Input","170727dc":"1. Oops, in 2015, it seems some extreme points exist.  \n   For exmaple, around Febrary, TX_2 has almost 0 item sold. (I assume this store is closed exceptionally.)\n   In contrast, in one summer day of that year, TX_3 increased its total item sold exprosively.\n   \n2. TX_2 has most item sold especially before 2014.","f8290eb9":"The mean of item price in each category seems to have some change in these dataset periods.  \nIs this simply because the expensive item increased in later periods?","1da18f1a":"Let's check the sales of event day!","05001deb":"1. Three stores in CA have similar amount of item sold record.  \n   CA_3 has more item sold a little bit compared to others.  \n\n2. The standard deviation of each store seems different, confirm it later.\n\n3. From around 2015 Spring or Summer, CA_2 increased its sold record rapidly. We have to investigate the reasons.\n\n","ea692db3":"As we've already seen above, the registered item count trends are different in each store.  \nWI_2 increased its item register around summer in 2012, then WI_3 increased around November of that year.  \n\nFrom 2013, all stores have similar trend.  Next, stores in TX states!","d1d6bd88":"# Future Work\nIf I have time, I'd like to tackle with the following things.\n\n1. Apply Dynamic Analysis and find out the relationship among state and stores.  ->  Done (6th public version)\n2. Make One item or one store prediction model for beginners like me to learn how to use lightgbm as a regressor.\n3. Check out the pre-processing effect. Is that effective considering the noise samples like Christmas or other irregularly days.\n4. More detailed analysis and find out some useful information for making prediction.","a69a2921":"In short, from the dataset of the target item sold, we extract one unobserved factor among states.  \nAnd the above plot shows how this factor effects to the item sold in each state.  \n\nThe code below calculates $R^2$ values of how this factor explains the total sold transition.","7cd2effe":"1. OK. Total snap purchase allowed day of each state is the same in all years.\n\n2. From 2011 to 2015, there are about 120 days when snap purchase is allowed.  \n   (As for 2016, we only have the first half of whole year.)","970a95bd":"1. Oops, in 2015, it seems some extreme points exist.  \n   For exmaple, around Febrary, TX_2 has almost 0 item sold. (I assume this store is closed exceptionally.)\n   \n   -> On 2015-03-24, TX_2 has very little item sold. ","9c401abf":"### Standard deviation analysis in each store\nNow let's check each store's standard deviation.  \nIn the previous section, we found that CA_3 has a little bit more sales than other stores in CA.  \nHow about deviation of these stores?","8495aea0":"## Relationship of Lag Variables","26b4bfc5":"## Item Sold in each day type","ce8e5b14":"From these analysis, we can find the following things regarding department.\n\n**Common**\n- Departments in each category are seemed to be classified by average sell price of its department.  \n\n**FOODS**\n- dept_3 has lower price and high volume plot compared to others.  \n- In addition, dept_3 has more items than other departments.\n\n**HOBBIES**\n- dept_1 has more items than dept_2\n\n**HOUSEHOLD**\n- Unlike other categories, this category seemed that it doesn't have clear difference in each department.","ce275906":"# Summary\n   In this notebook, through some easy data visualization, we found some points regarding this dataset like below.\n   1. The transition of all items sold in each category \n      - Some periodical effect. (Weekly and monthly)\n      - On christmas day, there are almost no sales\n   2. Which category is the most sold one?  \n      - FOODS is the most sold item category of these three categories.\n      - HOUSEHOLD category is the 2nd one, and the HOBBIES are the least sold one.\n   3. Which day type is the most sold day? \n      - Saturday and Sunday is the most item sold day types.\n      - In contrast, on weekdays like Tuesday, there are less item sold.\n   4. The transition in all stores by each state\n      - In CA, CA_3 store is the most sold store.  \n      - In other states, not so much difference appeared.  \n      - These sales trasition often corresponds to the registered item entries.\n   5. Snap purchase allowed day visuaizaiton\n      - The total count of Snap purchase allowed day in whole year is almost the same from 2011.\n      - The total count of Snap purchase allowed day in one month is 10 in every month.\n      - The total count of Snap purchase allowed day in each day type is almost uniformly distributed.\n      - However, there are some biased patterns regarding snap purchase allowed flag.\n        (i.e. it is not like one day in three consective days regularly, but all days in one week and none in next week)\n   \n   6. Dynamic Factor Analysis Trial\n      - We found the unobserveed factor by using dynamic factor analysis in one item (FOODS_3_090) \n      - This factor explains the sales transition in each state well.\n    \n   And finally we visualize some points by using heatmap.","fcdccafe":"## Sell Price Analysis","781bdf01":"I thought when some cultual or sporting event occurs, HOBBIES item are more likely to be sold.  \nHowever, this plot doesn't mean this hypothesis clearly.","2427fe7c":"# Snap Purchase Analysis\nLet's see how snap purchase allowed day is distributed.","be0dc2ef":"The most sold out item in this dataseet is FOODS_3_090_CA_3_validation","d710bcee":"# Acknowledgment\n\nI apologize that my english are somewhat wrong and my codes are not so beautiful one like others' codes.  \nHowever, I tried hard to make simle codes as much as I can especially for beginners like me to learn how to use matplotlib and seaborn to do data visualization.  \nAny comments or upvotes can be my very strong motivation towards much harder work! Thank you!","a9e3cb43":"## Next Week Events and This Week Sales Relationship Analysis","aba167ba":"All categories have similar trends though the volume of these values are different.  \nFrom 2015, the number of registered items is seemed to be relatively constant.","a584464e":"OK, event tyoe distributes like the graph above.   \n(Most of the values are actually \"unknown\", but for visualization, I omitted unknown value)","2201b1b5":"# Event Pattern Analysis\nLet's check event pattern in event_name_1 column.  \n(As for event_name_2 column, there are much less non-null values compeared to event_name_1 column.","727062f7":"The price of some Household category is super expensive like over 100 \\$.  \nOn the other hand, foods are mostly around 5 to 10 \\$ and don't have a large deviation.","e6c9fc59":"# Data Visualization Notebook\n\n1st public version: Created on May 6th, 2020.  \n2nd public version: Created on May 7th, 2020.  \n3rd public version: Created on May 10th, 2020.  \n4th public version: Created on May 15th, 2020.  (Add Department Analysis)  \n5th public version: Created on May 16th, 2020.  (Add Next week event and value analysis)  \n6th public version: Created on May 16th, 2020.  (Add [Dynamic Factor Analysis](#Dynamic-Factor-Analysis-Trial))  \n7th public version: Created on May 23th, 2020.  (Fix some codes in [Standard deviation analysis in each store](#Standard-deviation-analysis-in-each-store), Thank you for your comment, Kevin.)  \n8th public version: Created on May 26th, 2020. (Fix Bugs and Add Calendar Plot)\n9th public version: Created on May 26th, 2020. (Add more precise PCA Plot, See [PCA Section](#PCA-Trial))\n\nHello, everyone.  \nI'm one of the challenger of this competition **M5 -accuracy- **.  \nI'd like to find some useful insights for predictiong the test dataset.  \n\nHere I created a notebook for data visualization especially for beginners like me.  \nI appreciate all of your views and comments.\n\nhttps:\/\/www.kaggle.com\/robikscube\/m5-forecasting-starter-data-exploration  \nHis notebook is super great.  \nIf you have time, I strongly recommend you to take a look.\nAnd I'll introduce other good kernels in Reference Section\n\nHere is table of contents in this notebook:\n- [Import Libraries and Data Input](#Import-Libraries-and-Data-Input)\n- [Data Cleaning](#Data-Cleaning)\n- [Data Visualization](#Data-Visualization)\n   -  [Total Item Sold Transition](#Total-Item-Sold-Transition)\n   -  [Item Sold in each day type](#Item-Sold-in-each-day-type)\n   -  [Item sold in each State and Store](#Item-sold-in-each-State-and-Store)\n      - [Standard deviation analysis in each store](#Standard-deviation-analysis-in-each-store)\n   -  [Item Sold relation Analysis](#Item-Sold-relation-Analysis)\n   -  [Dynamic Factor Analysis Trial](#Dynamic-Factor-Analysis-Trial)\n   -  [Store Analysis](#Store-Analysis)\n   -  [Snap Purchase Analysis](#Snap-Purchase-Analysis)\n   -  [Event Pattern Analysis](#Event-Pattern-Analysis)\n   -  [One Item Features Analysis](#One-Item-Features-Analysis)\n   -  [Sell Price Analysis](#Sell-Price-Analysis)\n   -  [Sell price and value relationship](#Sell-price-and-value-relationship)\n   -  [Department Analysis](#Department-Analysis)\n   -  [Next Week Events and This Week Sales Relationship Analysis](#Next-Week-Events-and-This-Week-Sales-Relationship-Analysis)\n   -  [Relationship of Lag Variables](#Relationship-of-Lag-Variables)\n   -  [Calendar Visualization](#Calendar-Visualization)\n   -  [PCA Trial](#PCA-Trial)\n  \n- [Summary](#Summary)\n- [Future Work](#Future-Work)\n- [References](#References)","3a060e61":"We can find the following things.\n1. Regarding value and other columns correlation:\n   - snap purchase and other events flag has little correltion.\n   - Saturday has the most positive effect on values, and Tuesday has the most negative effect.  \n     (We've previously seen Saturday is the most item sold day in one week [here](#Item-Sold-in-each-day-type).)\n     \n2. Regarding snap_CA and weekdays columns correlation:\n   - As we've previously seen, snap_CA is uniformly distributed in each day type.  \n     Thus, the correlation between snap_CA and weekdays columns (ex. Monday, Tuesday, ...) are almost 0.\n\n3. Others:\n   - Looking at event and sunday correlation,it is just 0.089.  \n     I thought most part of events oocur on Sunday, but it wasn't so much as I had exoected.\n   - Regarding sell price, we look below.","db708135":"**Point of the graph**\n1. FOODS is the most sold item category of these three categories.  \n   HOUSEHOLD is the 2nd one, and HOBBIES are the least sold one.  \n\n\n2. FOODS category appearently has some periodical feature.   \n   During one year, it seems more items are sold in summer than in winter, however, we have to verify this.  \n   As for more short time interval, it seems the trend has monthly or weekly features. (Let's take a look below)\n\n3. HOUSEHOLD category items sold is gradually increasing from 2011.  \n   However, it may be because some items are not in the store in 2011.  \n   So we have to take the total item in the store into account.\n   Periodical Features are not so clear in this category compared to FOODS.\n  \n4. In HOBBIES category, periodical features are less appearent like HOUSEHOLD category.\n\n5. In some point (around the end of year), all categories don't have any sold.  So I think we have to consider whether we take these days into account when training models.\n\nSo let's take a look at the latest year, 2015!","dbdd97ea":"Some items are sold even on Christmas Day, but I think these are completely noisy values.   \n\nUntil now, we can find the items sold have something weekly fetures. So let's think this:   \n**Next Question: Which day of the week is the items sold most?**","e46a0935":"As for FOODS_3_090, when the price gets lower than its mean, the number of sold gets higher. \nEspecially, the latter half of 2013, # of sold items gets higher about 2.5 times than usual, and this is the period when this item had lower price than usual. \nIt seems that this item has no sell for some consectuive days in some points.  \nIt can be noise when making models.\n\n\nLet's see some other items.","5dd89b9c":"## Item sold in each State and Store","4b17c007":"# Data Cleaning\nFirst, let's combine all three dataframe.  \nThe important thing is changing data format from wide to long to make prediction model easier  \n(Though this notebook doesn't dive into predicition model itself.)\n\n","e102786b":"Now we can do the same thing in item category.  \nAnd let's try item clustering.","799f17cd":"We've already seen HOUSEHOLD has some exceptionally expensive sell price.  \nNow we found these items aren't sold anywhere, but only in some stores.","79812411":"From above things, we may think \"oh, snap_enable day is distributed uniformly, like 1 day in 3 consecutive days.\" because all of these barplot show it's not so different in any month, any day.  \nHowever, it is **completely different** as I'll show you below.  \n(i.e. snap purchase enable day is distributed biasedly.)","52455f8a":"In above figure, each plot means whether the day allows snap purchase or not in CA.  \nAs you can see, snap purchase enable day is not regularly distributed like one day in three consective days.  \n(ex. 2016-03-01 > 2016-03-04 > 2016-03-07 > ...)  \nIt is actually biasedly distributed like the figure above.  \n(i.e. Snap purchase Enable Day continues from 2016-03-01 to 2016-03-10)  \nAnd on these days, sales are also increased.  ","8e7b6453":"1. OK, the total count of snap purchase enable day looks similar in these three stores.\n\n2. The total count of snap purchase enable day is about one half of that of non-enable day.\n\nNext Let's see whether snap purchase is how-distributed in one year.","ba446ae4":"## Department Analysis","3102423a":"Through the year, we have 10 snap purchase allowed days in one month.  \nThis tendency is the same from 2012 to 2015.  \n(In 2011, no snap days in January)","06c784ac":"This value shows how much percentage do these components explain the total dataset.  \nIn this case, principal component 1 explains 85 % of the data and component 2 does 8 %.   \nSo the total 93 % of the data could be explained with these components","454472f6":"Find the most item sold day for example and take a look at the relationship between snap purchase allowed flag and values.","f20e4591":"Dark Red days mean the sales on that day is large.  \nOn the contrary, white days mean the sales on that day is low.  \n(You can see on Christmas day the plot has white cells.)","259da1d9":"Maybe 1 day or 1 week lag variables are important in this case.  \nI'll find the strength of correlation between the sold of the day and past few days in my next version.","a8158da5":"## Discount Season Presumption\nIs there a period when all items have lower price than usual in Walmart?","844b28c7":"Actually, we couldn't see the clear relationship between next week event and this week's sold count.  \nI had expected that the sales would increase if there are many events in the next week. \nWe need more analysis regarding this point.\n\nSo far, we often use subplots and drop each category as a different plot.  \nMany methods of Seaborn have hue attribute and this can drop all categories in one plot with different color.  \nIt is very useful, but this time, it often causes Memory Error and makes kernel die, so this time I didn't use it.","b7b3169b":"This result is also interesting.  \nOf course, when the price of item is expensive, less items are sold. (left top field)  \nAnd when the price gets lower, more items are sold.  (Right bottom field)  \nHowever, the relationship is not likely linear but likely to be inverse propotion.","a803afc9":"We can find this factor explains each store column well, especially for TX state,  \nThus, we can conclude these three states are explained by one unobserved factor.  \n(Not so different from state to state)","fa8b8774":"From now on, let's find out the similarities among stores by using PCA!","eb8d28dc":"## Quarter Analysis","7acedd56":"# Data Visualization\n## Total Item Sold Transition","ad68563f":"3. From around 2015 Spring or Summer, CA_2 increased its sold record rapidly. We have to investigate the reasons.\n\n-> It is because item registered in CA_2 increased rapidly.  \nAfter summer in 2015, all stores in CA have similar registered item count"}}