{"cell_type":{"217686df":"code","35d7b30e":"code","0b987381":"code","e6a7a720":"code","4fad2f04":"code","99fdcfb2":"code","bee2a405":"code","12495753":"code","b449b067":"code","a9aebd03":"code","92d5f216":"code","575de1f9":"markdown","ab121c36":"markdown","7f875838":"markdown"},"source":{"217686df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35d7b30e":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\n\n# Python \u22653.5 is required\nimport sys\nassert sys.version_info >= (3, 5)\n\n# Scikit-Learn \u22650.20 is required\nimport sklearn\nassert sklearn.__version__ >= \"0.20\"\n\n# Common imports\nimport numpy as np\nimport os\n\n# to make this notebook's output stable across runs\nnp.random.seed(42)\n\n# To plot pretty figures\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)\nplt.rc('font', size=12) \nplt.rc('figure', figsize = (12, 5))\n\n# Settings for the visualizations\nimport seaborn as sns\nprint(sns.__version__)\nassert sns.__version__ >= \"0.10\"\nsns.set_style(\"whitegrid\")\nsns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2,'font.family': [u'times']})\n\nimport pandas as pd\npd.set_option('display.max_rows', 25)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', 50)\n\n# Ignore useless warnings (see SciPy issue #5998)\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")","0b987381":"train_set = pd.read_csv('\/kaggle\/input\/housing-house-prediction\/train_set.csv',index_col=0) \ntest_set = pd.read_csv('\/kaggle\/input\/housing-house-prediction\/test_set.csv',index_col=0)","e6a7a720":"# print the dataset size\nprint(\"There is\", train_set.shape[0], \"samples\")\nprint(\"Each sample has\", train_set.shape[1], \"features\")","4fad2f04":"# print the top elements from the dataset\ntrain_set.head()\n","99fdcfb2":"# As it can be seen the database contains several features, some of them numerical and some of them are categorical.\n# It is important to check each of the to understand it.\n# Check each features. If the features are numerical it is imporant to check the distributions a.\n#                      If the features are categorical it is important to check the number of the categories and the disitribution","bee2a405":"train_set.dtypes\n","12495753":"# print those categorical features\ntrain_set.select_dtypes(include=['object']).head()","b449b067":"# We can check how many different type there is in the dataset using the folliwing line\ntrain_set[\"Type\"].value_counts()","a9aebd03":"sns.countplot(y=\"Type\", data=train_set, color=\"c\")\n","92d5f216":"## the features\n\nfeatures = ['Rooms','Landsize', 'BuildingArea', 'YearBuilt']\n## DEFINE YOUR FEATURES\nX = train_set[features].fillna(0)\ny = train_set[['Price']]\n\n## the model\n# KNeighborsRegressor\nfrom sklearn import neighbors\nn_neighbors = 3 # you can modify this paramenter (ONLY THIS ONE!!!)\nmodel = neighbors.KNeighborsRegressor(n_neighbors)\n\n## fit the model\nmodel.fit(X, y)\n\n## predict training set\ny_pred = model.predict(X)\n\n## Evaluate the model and plot it\nfrom sklearn.metrics import mean_squared_error, r2_score\nprint(\"----- EVALUATION ON TRAIN SET ------\")\nprint(\"RMSE\",np.sqrt(mean_squared_error(y, y_pred)))\nprint(\"R^2: \",r2_score(y, y_pred))\n\n\nplt.scatter(y, y_pred)\nplt.xlabel('Price')\nplt.ylabel('Predicted price');\nplt.show()\n\n## predict the test set and generate the submission file\nX_test = test_set[features].fillna(0)\ny_pred = model.predict(X_test)\n\ndf_output = pd.DataFrame(y_pred)\ndf_output = df_output.reset_index()\ndf_output.columns = ['index','Price']\n\ndf_output.to_csv('baseline.csv',index=False)","575de1f9":"## The problem\n\nThe machine learning is to predict the house price, but before that it is imporntat to study the dataset and its features\n\n","ab121c36":"## BASELINE MODEL\n\nhttps:\/\/www.kaggle.com\/c\/mlub-housing-house-prediction\/notebooks\n\nThis is a simple model that uses the K-nearest Neighbors Regressor\n\nThis model only uses 4 feaures: 'Rooms','Landsize', 'BuildingArea', 'YearBuilt'","7f875838":"It would be interesting to visualize all features (numerical and catergorical) in order to undertand them.\n\nCheck out this blog for plotting distribution: https:\/\/seaborn.pydata.org\/tutorial\/distributions.html\n\nSeaborn version of this blog can be different from the one intalled in your machine (version 0.11 has been just realeased)\nCheck out this blog for plotting categorical data: https:\/\/seaborn.pydata.org\/tutorial\/categorical.html"}}