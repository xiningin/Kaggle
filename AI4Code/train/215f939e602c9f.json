{"cell_type":{"d19a5f14":"code","84c7486d":"code","8fe69ac3":"code","2c929dbe":"code","1a339f8a":"code","68322188":"code","c802c963":"code","a578d694":"code","e7de4616":"code","5371122f":"code","a6aee93d":"code","d6918571":"code","eb458810":"code","89ffdea3":"code","bc6e09bf":"code","f5f8056c":"code","56d86acf":"code","6ea17471":"code","2c86d38c":"code","14a89616":"code","3216fd2b":"code","2bee52d0":"markdown","6e381788":"markdown","53c40dda":"markdown","d1610ef3":"markdown","77c54262":"markdown","139d0bc8":"markdown","6ff48b5f":"markdown","041aff7a":"markdown","552b99d4":"markdown","403c55d5":"markdown","48b96568":"markdown"},"source":{"d19a5f14":"!pip install tensorflow-text","84c7486d":"!pip freeze > kaggle_image_requirements.txt","8fe69ac3":"# Import neural network libraries\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text","2c929dbe":"# Some other key imports\nimport os\nimport re\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm","1a339f8a":"# Params for bert model and tokenization\nNsamp = 1000 # number of samples to generate in each class - 'spam', 'not spam'\nmaxtokens = 200 # the maximum number of tokens per document\nmaxtokenlen = 100 # the maximum length of each token","68322188":"def tokenize(row):\n    if row is None or row is '':\n        tokens = \"\"\n    else:\n        try:\n            tokens = row.split(\" \")[:maxtokens]\n        except:\n            tokens=\"\"\n    return tokens","c802c963":"def reg_expressions(row):\n    tokens = []\n    try:\n        for token in row:\n            token = token.lower()\n            token = re.sub(r'[\\W\\d]', \"\", token)\n            token = token[:maxtokenlen] # truncate token\n            tokens.append(token)\n    except:\n        token = \"\"\n        tokens.append(token)\n    return tokens","a578d694":"import nltk\n\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstopwords = stopwords.words('english')    \nprint(stopwords) # see default stopwords\n\ndef stop_word_removal(row):\n    token = [token for token in row if token not in stopwords]\n    token = filter(None, token)\n    return token","e7de4616":"# Input data files are available in the \"..\/input\/\" directory.\nfilepath = \"..\/input\/enron-email-dataset\/emails.csv\"\n\n# Read the data into a pandas dataframe called emails\nemails = pd.read_csv(filepath)\n\nprint(\"Successfully loaded {} rows and {} columns!\".format(emails.shape[0], emails.shape[1]))\nprint(emails.head())","5371122f":"# take a closer look at the first email\nprint(emails.loc[0][\"message\"])","a6aee93d":"# Separate headers from the message bodies\nimport email\n\ndef extract_messages(df):\n    messages = []\n    for item in df[\"message\"]:\n        # Return a message object structure from a string\n        e = email.message_from_string(item)    \n        # get message body  \n        message_body = e.get_payload()\n        messages.append(message_body)\n    print(\"Successfully retrieved message body from e-mails!\")\n    return messages\n\nbodies = extract_messages(emails)","d6918571":"# extract random 10000 enron email bodies for building dataset\nimport random\nbodies_df = pd.DataFrame(random.sample(bodies, 10000))\n\n# expand default pandas display options to make emails more clearly visible when printed\npd.set_option('display.max_colwidth', 300)\n\nbodies_df.head() # you could do print(bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames","eb458810":"filepath = \"..\/input\/fraudulent-email-corpus\/fradulent_emails.txt\"\nwith open(filepath, 'r',encoding=\"latin1\") as file:\n    data = file.read()\n    \n# split on a code word appearing close to the beginning of each email\nfraud_emails = data.split(\"From r\")\n\nprint(\"Successfully loaded {} spam emails!\".format(len(fraud_emails)))","89ffdea3":"fraud_bodies = extract_messages(pd.DataFrame(fraud_emails,columns=[\"message\"],dtype=str))\nfraud_bodies_df = pd.DataFrame(fraud_bodies[1:])\n\nfraud_bodies_df.head() # you could do print(fraud_bodies_df.head()), but Jupyter displays this nicer for pandas DataFrames","bc6e09bf":"import random\n\n# Convert everything to lower-case, truncate to maxtokens and truncate each token to maxtokenlen\nEnronEmails = bodies_df.iloc[:,0].apply(tokenize)\nEnronEmails = EnronEmails.apply(stop_word_removal)\nEnronEmails = EnronEmails.apply(reg_expressions)\nEnronEmails = EnronEmails.sample(Nsamp)\n\nSpamEmails = fraud_bodies_df.iloc[:,0].apply(tokenize)\nSpamEmails = SpamEmails.apply(stop_word_removal)\nSpamEmails = SpamEmails.apply(reg_expressions)\nSpamEmails = SpamEmails.sample(Nsamp)\n\nraw_data = pd.concat([SpamEmails,EnronEmails], axis=0).values","f5f8056c":"print(\"Shape of combined data represented as numpy array is:\")\nprint(raw_data.shape)\nprint(\"Data represented as numpy array is:\")\nprint(raw_data)\n\n# corresponding labels\nCategories = ['spam','notspam']\nheader = ([1]*Nsamp)\nheader.extend(([0]*Nsamp))","56d86acf":"# function for shuffling data in unison with labels\/header\ndef unison_shuffle(a, b):\n    p = np.random.permutation(len(b))\n    data = a[p]\n    header = np.asarray(b)[p]\n    return data, header\n\n# function for converting data into the right format, due to the difference in required format from sklearn models\n# we expect a single string per email here, versus a list of tokens for the sklearn models previously explored\ndef convert_data(raw_data,header):\n    converted_data, labels = [], []\n    for i in range(raw_data.shape[0]):\n        out = ' '.join(raw_data[i])\n        converted_data.append(out)\n        labels.append(header[i])\n        #print(i)\n    converted_data = np.array(converted_data, dtype=object)[:, np.newaxis]\n    \n    return converted_data, np.array(labels)\n\nraw_data, header = unison_shuffle(raw_data, header)\n\n# split into independent 70% training and 30% testing sets\nidx = int(0.7*raw_data.shape[0])\n# 70% of data for training\ntrain_x, train_y = convert_data(raw_data[:idx],header[:idx])\n# remaining 30% for testing\ntest_x, test_y = convert_data(raw_data[idx:],header[idx:])\n\nprint(\"train_x\/train_y list details, to make sure it is of the right form:\")\nprint(len(train_x))\nprint(train_x)\nprint(train_y[:5])\nprint(train_y.shape)","6ea17471":"# Function to build overall model\ndef build_model(max_seq_length):\n    # tf hub bert model path\n    bert_path = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/4\" \n\n    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string)\n    \n    \n    preprocessor = hub.KerasLayer(\n    \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_preprocess\/3\")\n    encoder_inputs = preprocessor(text_input)\n    encoder = hub.KerasLayer(bert_path,trainable=False)\n    outputs = encoder(encoder_inputs)\n    pooled_output = outputs[\"pooled_output\"]      # [batch_size, 768].\n    sequence_output = outputs[\"sequence_output\"]  # [batch_size, seq_length, 768].# just extract BERT features\n    \n    # train dense classification layer on top of extracted pooled output features\n    dense = tf.keras.layers.Dense(256, activation=\"relu\")(pooled_output)\n    pred = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense)\n\n    model = tf.keras.Model(inputs=text_input, outputs=pred)\n    model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n    model.summary()\n\n    return model","2c86d38c":"# Build model\nmodel = build_model(maxtokens)\n\n# Instantiate variables\n#initialize_vars(sess)\n\n# Train model\nhistory = model.fit(train_x,train_y,validation_data=(test_x,test_y),epochs=5,batch_size=32)","14a89616":"import matplotlib.pyplot as plt\n\ndf_history = pd.DataFrame(history.history)\nfig,ax = plt.subplots()\nplt.plot(range(df_history.shape[0]),df_history['val_accuracy'],'bs--',label='validation')\nplt.plot(range(df_history.shape[0]),df_history['accuracy'],'r^--',label='training')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.title('BERT Email Classification Training')\nplt.legend(loc='best')\nplt.grid()\nplt.show()\n\nfig.savefig('BERTConvergence.eps', format='eps')\nfig.savefig('BERTConvergence.pdf', format='pdf')\nfig.savefig('BERTConvergence.png', format='png')\nfig.savefig('BERTConvergence.svg', format='svg')","3216fd2b":"from IPython.display import HTML\ndef create_download_link(title = \"Download file\", filename = \"data.csv\"):  \n    html = '<a href={filename}>{title}<\/a>'\n    html = html.format(title=title,filename=filename)\n    return HTML(html)\n\ncreate_download_link(filename='BERTConvergence.svg')","2bee52d0":"Write requirements to file, anytime you run it, in case you have to go back and recover Kaggle dependencies. **MOST OF THESE REQUIREMENTS WOULD NOT BE NECESSARY FOR LOCAL INSTALLATION**\n\nLatest known such requirements are hosted for each notebook in the companion github repo, and can be pulled down and installed here if needed. Companion github repo is located at https:\/\/github.com\/azunre\/transfer-learning-for-nlp","6e381788":"# Read and Preprocess Fraudulent \"419\" Email Corpus","53c40dda":"# Define Tokenization, Stop-word and Punctuation Removal Functions\nBefore proceeding, we must decide how many samples to draw from each class. We must also decide the maximum number of tokens per email, and the maximum length of each token. This is done by setting the following overarching hyperparameters","d1610ef3":"# Visualize and Save Convergence Info","77c54262":"# Read and Preprocess Enron dataset\nRead Enron dataset and get a sense for the data by printing sample messages to screen\n\nadd Codeadd Markdown","139d0bc8":"## Stop-word removal\n\nLet\u2019s define a function to remove stopwords - words that occur so frequently in language that they offer no useful information for classification. This includes words such as \u201cthe\u201d and \u201care\u201d, and the popular library NLTK provides a heavily-used list that will employ.","6ff48b5f":"## Use regular expressions to remove unnecessary characters\n\nNext, we define a function to remove punctuation marks and other nonword characters (using regular expressions) from the emails with the help of the ubiquitous python regex library. In the same step, we truncate all tokens to hyperparameter maxtokenlen defined above.","041aff7a":"# WARNING\n**Please make sure to \"COPY AND EDIT NOTEBOOK\" to use compatible library dependencies! DO NOT CREATE A NEW NOTEBOOK AND COPY+PASTE THE CODE - this will use latest Kaggle dependencies at the time you do that, and the code will need to be modified to make it work. Also make sure internet connectivity is enabled on your notebook**","552b99d4":"# Build, Train and Evaluate BERT Model\nFirst define critical functions that define various components of the BERT model","403c55d5":"# Putting It All Together To Assemble Dataset\nNow, putting all the preprocessing steps together we assemble our dataset...","48b96568":"# Preliminaries\nFirst install critical dependencies not already on the Kaggle docker image"}}