{"cell_type":{"ef079cf9":"code","17e85a12":"code","b370caf1":"code","b5f97c4f":"code","5dcf1da9":"code","8843a112":"code","bbc2c350":"code","40a09710":"code","3700ad99":"code","128a1abc":"code","5cc8e2ac":"code","515e314f":"code","6d66fc1a":"code","bb71c562":"code","070ef2d3":"code","ac198012":"code","b0b39cb3":"code","87ab1d0e":"code","00cec558":"code","258aee4b":"code","c22265f6":"code","a80707e9":"code","abe4a96f":"code","3ec7602e":"code","0fc53a41":"code","54203d88":"code","b177c9f8":"code","e605cdec":"code","5a5039bd":"code","24d9b277":"code","6b2ca3b7":"code","18e75d77":"code","77125e51":"code","d3931f3c":"code","240cde4a":"code","2cf0473e":"code","87a6a92c":"code","ed1898b7":"code","1e54b7b0":"code","40072afd":"code","fe33722a":"code","4e8650ab":"code","ee2ca901":"code","b365825e":"code","150e3e4e":"code","657a5874":"code","17d67989":"code","a087ca79":"code","4554c784":"code","23fbd098":"code","58df2989":"code","91965d48":"code","78e5efe4":"code","f8f3bc4e":"code","6796d65b":"code","6bc0eca8":"code","5e3eeb74":"code","dbade750":"code","16502b9d":"code","42be56d4":"code","0fa3b21b":"code","19ec348b":"code","c2de1f13":"code","c9ce55c1":"code","5cff734f":"code","98d87b43":"code","92fde44d":"code","8e4d09e0":"code","04e7dd05":"code","b597bebe":"code","93c9cdaa":"code","d39120a4":"code","1cd129e4":"code","dafc690c":"code","e60e1384":"code","122aa89e":"code","b20b37ab":"code","4e7b2416":"markdown","91ce4597":"markdown","b8cc1890":"markdown","ef6f31e3":"markdown","7a7aebfb":"markdown","f3e601c8":"markdown","92e1fdac":"markdown","0185b96d":"markdown","b7f5ca40":"markdown","12b9e58d":"markdown","79965390":"markdown","8ef56b4b":"markdown","6baf315a":"markdown","88f8e615":"markdown","64270e3d":"markdown","d6847d0e":"markdown","2a6c7954":"markdown","63d1db2f":"markdown","e34f7cc6":"markdown","c880ab3a":"markdown","56713984":"markdown","32d4cebb":"markdown","efb57717":"markdown","63fd04a2":"markdown","0ed823c9":"markdown","bfd33eb3":"markdown","f069dd2d":"markdown","65c83987":"markdown","92c0907d":"markdown","a459d256":"markdown"},"source":{"ef079cf9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","17e85a12":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Classifiers\n#import decisiontreeclassifier\nfrom sklearn import tree\nfrom sklearn.tree import export_text\nfrom sklearn.tree import DecisionTreeClassifier\n#import logisticregression classifier\nfrom sklearn.linear_model import LogisticRegression\n\n#import knn classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#for validating your classification model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\n\n# feature selection\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# grid search\nfrom sklearn.model_selection import GridSearchCV\n\n# advanced algorthms\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.svm import SVC\n\n#ignore warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b370caf1":"# load data\ncl = pd.read_csv('\/kaggle\/input\/bank-marketing-campaign\/bank-full - Copy.csv', header=None)\ncl.head()","b5f97c4f":"# Renaming column 0\ncl = cl.rename(columns={0 : 'word'})\ncl.head()","5dcf1da9":"# Splitting dataset from one column to it various columns\ncl=cl.word.str.split(';', expand=True)\ncl.head()","8843a112":"# Renamimg columns\ncl = cl.rename(columns={0 : 'age', 1 : 'job', 2 : 'marital', 3 : 'education', 4 : 'default', 5 : 'balance', 6 : 'housing',\n                        7 : 'loan', 8 : 'contact', 9 : 'day', 10 : 'month', 11 : 'duration', 12 : 'campaign', 13 : 'pdays',\n                       14 : 'previous', 15 : 'poutcome', 16 : 'target'})\ncl.head()","bbc2c350":"# Dropping row 0\ncl=cl.drop([0])\ncl.head()","40a09710":"# Pivot talbe for output thus those less chance of heart attack and more chance of heart attack.\ncl.groupby(['target']).size()","3700ad99":"# plot for output\ncl.groupby(['target']).size().plot(kind='bar')","128a1abc":"# target vs age. identifying the number of client who have subscribe or not\ncl.groupby(['target', 'age']).size()","5cc8e2ac":"# target vs job. identifying the number of client who have subscribe or not\ncl.groupby(['target', 'job']).size()","515e314f":"# target vs month. identifying the number of client who have subscribe or not\ncl.groupby(['target', 'month']).size().plot(kind='bar')","6d66fc1a":"# target vs month. identifying the number of client who have subscribe or not\ncl.groupby(['target', 'day']).size().plot(kind='bar')","bb71c562":"# Number of people with housing\ncl.groupby('housing').size()","070ef2d3":"# Number of people with housing\ncl.groupby('housing').size().plot(kind='bar')","ac198012":"# Number of people with loan\ncl.groupby('loan').size()","b0b39cb3":"# Number of people with housing\ncl.groupby('loan').size().plot(kind='bar')","87ab1d0e":"# Replacing all strings with numbers\ncl = cl.replace({'target' : '\"no\"'}, {'target' : 0})\ncl = cl.replace({'target' : '\"yes\"'}, {'target' : 1})\ncl = cl.replace({'poutcome' : '\"unknown\"'}, {'poutcome' : 1})\ncl = cl.replace({'poutcome' : '\"other\"'}, {'poutcome' : 2})\ncl = cl.replace({'poutcome' : '\"failure\"'}, {'poutcome' : 3})\ncl = cl.replace({'poutcome' : '\"success\"'}, {'poutcome' : 4})\ncl = cl.replace({'contact' : '\"unknown\"'}, {'contact' : 1})\ncl = cl.replace({'contact' : '\"telephone\"'}, {'contact' : 2})\ncl = cl.replace({'contact' : '\"cellular\"'}, {'contact' : 3})\ncl = cl.replace({'loan' : '\"no\"'}, {'loan' : 0})\ncl = cl.replace({'loan' : '\"yes\"'}, {'loan' : 1})\ncl = cl.replace({'housing' : '\"no\"'}, {'housing' : 0})\ncl = cl.replace({'housing' : '\"yes\"'}, {'housing' : 1})\ncl = cl.replace({'default' : '\"no\"'}, {'default' : 0})\ncl = cl.replace({'default' : '\"yes\"'}, {'default' : 1})\ncl = cl.replace({'education' : '\"unknown\"'}, {'education' : 1})\ncl = cl.replace({'education' : '\"primary\"'}, {'education' : 2})\ncl = cl.replace({'education' : '\"secondary\"'}, {'education' : 3})\ncl = cl.replace({'education' : '\"tertiary\"'}, {'education' : 4})\ncl = cl.replace({'marital' : '\"married\"'}, {'marital' : 1})\ncl = cl.replace({'marital' : '\"divorced\"'}, {'marital' : 2})\ncl = cl.replace({'marital' : '\"single\"'}, {'marital' : 3})\ncl = cl.replace({'job' : '\"unknown\"'}, {'job' : 1})\ncl = cl.replace({'job' : '\"unemployed\"'}, {'job' : 2})\ncl = cl.replace({'job' : '\"housemaid\"'}, {'job' : 3})\ncl = cl.replace({'job' : '\"student\"'}, {'job' : 4})\ncl = cl.replace({'job' : '\"self-employed\"'}, {'job' : 5})\ncl = cl.replace({'job' : '\"blue-collar\"'}, {'job' : 6})\ncl = cl.replace({'job' : '\"management\"'}, {'job' : 7})\ncl = cl.replace({'job' : '\"retired\"'}, {'job' : 8})\ncl = cl.replace({'job' : '\"entrepreneur\"'}, {'job' : 9})\ncl = cl.replace({'job' : '\"technician\"'}, {'job' : 10})\ncl = cl.replace({'job' : '\"admin.\"'}, {'job' : 11})\ncl = cl.replace({'job' : '\"services\"'}, {'job' : 12})\ncl = cl.replace({'pdays' : '-1'}, {'pdays' : 0})\ncl.head()","00cec558":"# Replacing all strings with numbers\ncl = cl.replace({'month' : '\"jan\"'}, {'month' : 1})\ncl = cl.replace({'month' : '\"feb\"'}, {'month' : 2})\ncl = cl.replace({'month' : '\"mar\"'}, {'month' : 3})\ncl = cl.replace({'month' : '\"apr\"'}, {'month' : 4})\ncl = cl.replace({'month' : '\"may\"'}, {'month' : 5})\ncl = cl.replace({'month' : '\"jun\"'}, {'month' : 6})\ncl = cl.replace({'month' : '\"jul\"'}, {'month' : 7})\ncl = cl.replace({'month' : '\"aug\"'}, {'month' : 8})\ncl = cl.replace({'month' : '\"sep\"'}, {'month' : 9})\ncl = cl.replace({'month' : '\"oct\"'}, {'month' : 10})\ncl = cl.replace({'month' : '\"nov\"'}, {'month' : 11})\ncl = cl.replace({'month' : '\"dec\"'}, {'month' : 12})\ncl.head()","258aee4b":"# Looking for data type\ncl.info()","c22265f6":"# converting data type from object to numbers\ncl['balance'] = cl['balance'].astype(int)\ncl['day'] = cl['day'].astype(int)\ncl['duration'] = cl['duration'].astype(int)\ncl['campaign'] = cl['campaign'].astype(int)\ncl['pdays'] = cl['pdays'].astype(int)\ncl['previous'] = cl['previous'].astype(int)\ncl['age'] = cl['age'].astype(int)\ncl.info()","a80707e9":"# Looking for data type\ncl.info()","abe4a96f":"# looking for null values in the dataset\ncl.isnull().sum()","3ec7602e":"# to check outliers\ncl.describe()","0fc53a41":"# Pivot talbe for output thus those less chance of heart attack and more chance of heart attack.\ncl.groupby(['target']).size()","54203d88":"# plot for output\ncl.groupby(['target']).size().plot(kind='bar')","b177c9f8":"cl.groupby(['target', 'age']).size()","e605cdec":"cl.groupby(['target', 'education']).size()","5a5039bd":"# data visualisation for output and age\nsns.violinplot(x=\"housing\", y=\"target\", data=cl)","24d9b277":"# data visualisation for output and chol\nsns.catplot(\"target\", \"marital\", data=cl, kind='bar')","6b2ca3b7":"# Correlation Analysis without Dummies\ncl.corr()","18e75d77":"cl.nunique()","77125e51":"cl_1 = pd.get_dummies(cl, columns=['target', 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact',\n                                  'poutcome'])\ncl_1.head()","d3931f3c":"cl_1=cl_1.corr()\ncl_1","240cde4a":"# Heat Map\nfig = plt.figure(figsize = (30,15))\nsns.heatmap(cl_1, annot = True)","2cf0473e":"import scipy.stats as stats\n# T-test for clients who is less likely or more likely in relation to age if their mean values are the same\nno = cl[cl['target'] == 0]['age']\nyes = cl[cl['target'] == 1]['age']\n\nstats.ttest_ind(no, yes)","87a6a92c":"# T-test for clients who is less likely or more likely in relation to cp if their mean values are the same\nno = cl[cl['target'] == 0]['housing']\nyes = cl[cl['target'] == 1]['housing']\n\nstats.ttest_ind(no, yes)","ed1898b7":"# T-test for clients who is less likely or more likely in relation to thalachh if their mean values are the same\nno = cl[cl['target'] == 0]['job']\nyes = cl[cl['target'] == 1]['job']\n\nstats.ttest_ind(no, yes)","1e54b7b0":"# T-test for clients who is less likely or more likely in relation to exng if their mean values are the same\nno = cl[cl['target'] == 0]['pdays']\nyes = cl[cl['target'] == 1]['pdays']\n\nstats.ttest_ind(no, yes)","40072afd":"# looking for unique variables\ncl.nunique()","fe33722a":"## Categorical to Dummy Variables\ncl =  pd.get_dummies(cl, columns=[\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"],\n                         prefix=[\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"],\n                         drop_first=True)\ncl.head(2)","4e8650ab":"### c. checking for p-value (TESTING FOR STATISTICAL SIGNIFICANCE OF INDEPENDENT VARIABLES)\nimport scipy.stats as stats\ncl_corr = pd.DataFrame() # Correlation matrix\ncl_p = pd.DataFrame() # Matrix of p-values\nfor x in cl.columns:   # assuming cl as your dataframe name\n   for y in cl.columns:\n      corr = stats.pearsonr(cl[x], cl[y])\n      cl_corr.loc[x,y] = corr[0]\n      cl_p.loc[x,y] = corr[1]\n\ncl_p['target']","ee2ca901":"# Assigning dataset into dependent (y) and independent (x)\ny = cl['target']\nX = cl.drop(['target'], axis=1)","b365825e":"# Normalizing all features\nfrom sklearn import preprocessing\nX_2 = preprocessing.StandardScaler().fit(X).transform(X)\nX_2[0:5]","150e3e4e":"# Splitting data into 70\/30. 70% trained dataset and 30% test dataset\nX_train, X_test, y_train, y_test = train_test_split(X_2, y, train_size = 0.7, random_state =100)","657a5874":"# Model Selection\nplt.figure()\n\n# Add the models to the list that you want to view on the ROC plot\nmodels = [\n{\n    'label': 'Decision Tree',\n    'model': DecisionTreeClassifier(),\n},\n{\n    'label': 'K-nearest neighbors',\n    'model': KNeighborsClassifier(),\n},\n{\n    'label': 'Logistic Regression',\n    'model': LogisticRegression(solver='lbfgs', max_iter=20000),\n},\n{\n    'label': 'Random Forest',\n    'model': RandomForestClassifier(n_estimators=100),\n}\n]\n\n# Below for loop iterates through your models list\nfor m in models:\n    model = m['model'] # select the model\n    model.fit(X_train, y_train) # train the model\n    #y_pred=model.predict(X_test) # predict the test data\n    \n    # Compute False postive rate, and True positive rate\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:,1])\n\n    # Calculate Area under the curve to display on the plot\n    auc = metrics.roc_auc_score(y_test,model.predict(X_test))\n\n    # Now, plot the computed values\n    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], auc))\n\n# Custom settings for the plot \nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('1-Specificity(False Positive Rate)')\nplt.ylabel('Sensitivity(True Positive Rate)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()   # Display","17d67989":"# Model Selection for advance classification algorithms\nplt.figure()\n\n# Add the models to the list that you want to view on the ROC plot\nmodels = [\n{\n    'label': 'Support Vector Machine',\n    'model': SVC(gamma='auto', probability=True),\n},\n{\n    'label': 'Gradient Boosting',\n    'model': GradientBoostingClassifier(),\n},\n{\n    'label': 'Neural Network',\n    'model': MLPClassifier(solver='lbfgs', max_iter=15000),\n}\n]\n\n# Below for loop iterates through your models list\nfor m in models:\n    model = m['model'] # select the model\n    model.fit(X_train, y_train) # train the model\n    #y_pred=model.predict(X_test) # predict the test data\n    \n    # Compute False postive rate, and True positive rate\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, model.predict_proba(X_test)[:,1])\n\n    # Calculate Area under the curve to display on the plot\n    auc = metrics.roc_auc_score(y_test,model.predict(X_test))\n\n    # Now, plot the computed values\n    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (m['label'], auc))\n\n# Custom settings for the plot \nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('1-Specificity(False Positive Rate)')\nplt.ylabel('Sensitivity(True Positive Rate)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()   # Display","a087ca79":"# using Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(criterion=\"entropy\", max_depth = 30, min_samples_leaf=5)\ndt.fit(X_train,y_train)  # train\npredTree = dt.predict(X_test)   # prediction","4554c784":"from sklearn import metrics\nimport matplotlib.pyplot as plt\nprint(\"DecisionTrees's Accuracy: \", metrics.accuracy_score(y_test, predTree))","23fbd098":"# Evaluation Metrics\nprint(metrics.accuracy_score(y_test, dt.predict(X_test)))\nprint(\"--------------------------------------------------------\")\nprint(metrics.confusion_matrix(y_test, dt.predict(X_test))) \nprint(\"--------------------------------------------------------\")\nprint(metrics.classification_report(y_test, dt.predict(X_test)))\nprint(\"--------------------------------------------------------\")\nprint(metrics.roc_auc_score(y_test, dt.predict(X_test)))","58df2989":"# 10 fold cross validation evaluation\nscores = cross_val_score(DecisionTreeClassifier(), X_test, y_test, scoring='accuracy', cv=10)\nprint(scores)\nprint(scores.mean())","91965d48":"# RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(n_estimators=100)    #building 100 decision trees\nclf=clf.fit(X_train, y_train)\nclf.score(X_test, y_test)","78e5efe4":"# Evaluation metrics\nprint(metrics.accuracy_score(y_test, clf.predict(X_test))) #overall accuracy\nprint(metrics.confusion_matrix(y_test, clf.predict(X_test)))\nprint(metrics.classification_report(y_test, clf.predict(X_test)))","f8f3bc4e":"# 10 fold cross validation evaluation\nscores = cross_val_score(RandomForestClassifier(), X_test, y_test, scoring='accuracy', cv=10)\nprint(scores)\nprint(scores.mean())","6796d65b":"# another method\npd.DataFrame(clf.feature_importances_, index = X.columns,\n                                    columns=['importance']).sort_values('importance', ascending=False)","6bc0eca8":"# Visualization for the most important features.\npd.DataFrame(clf.feature_importances_, index = X.columns,\n                                    columns=['importance']).sort_values('importance', ascending=True).plot(kind='barh', \n                                                                                                            legend=None);","5e3eeb74":"# GradientBoostingClassifier\n# initialize \ngb = GradientBoostingClassifier(n_estimators=100, random_state=0)\n\n# fit the model\ngb.fit(X_train, y_train)","dbade750":"#Model evaluation\n# http:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html\nprint(metrics.accuracy_score(y_test, gb.predict(X_test)))\nprint(\"--------------------------------------------------------\")\nprint(metrics.confusion_matrix(y_test, gb.predict(X_test))) \nprint(\"--------------------------------------------------------\")\nprint(metrics.classification_report(y_test, gb.predict(X_test)))\nprint(\"--------------------------------------------------------\")\nprint(metrics.roc_auc_score(y_test, gb.predict(X_test)))","16502b9d":"# 10-fold cross-validation\nscores = cross_val_score(gb, X_test, y_test, scoring='accuracy', cv=10)\nprint(scores)\nprint(scores.mean())","42be56d4":"# Feature importance\npd.DataFrame(gb.feature_importances_, index = X.columns,\n                                    columns=['importance']).sort_values('importance', ascending=False)","0fa3b21b":"# load data\ntt = pd.read_csv('\/kaggle\/input\/bank-marketing-campaign\/bank.csv', header=None)\ntt.head()","19ec348b":"# Renaming column 0\ntt = tt.rename(columns={0 : 'word'})\ntt.head()","c2de1f13":"# Splitting dataset from one column to it various columns\ntt=tt.word.str.split(';', expand=True)\ntt.head()","c9ce55c1":"# Renamimg columns\ntt = tt.rename(columns={0 : 'age', 1 : 'job', 2 : 'marital', 3 : 'education', 4 : 'default', 5 : 'balance', 6 : 'housing',\n                        7 : 'loan', 8 : 'contact', 9 : 'day', 10 : 'month', 11 : 'duration', 12 : 'campaign', 13 : 'pdays',\n                       14 : 'previous', 15 : 'poutcome', 16 : 'target'})\ntt.head()","5cff734f":"# Dropping row 0\ntt=tt.drop([0])\ntt.head()","98d87b43":"# Replacing all strings with numbers\ntt = tt.replace({'target' : '\"no\"'}, {'target' : 0})\ntt = tt.replace({'target' : '\"yes\"'}, {'target' : 1})\ntt = tt.replace({'poutcome' : '\"unknown\"'}, {'poutcome' : 1})\ntt = tt.replace({'poutcome' : '\"other\"'}, {'poutcome' : 2})\ntt = tt.replace({'poutcome' : '\"failure\"'}, {'poutcome' : 3})\ntt = tt.replace({'poutcome' : '\"success\"'}, {'poutcome' : 4})\ntt = tt.replace({'contact' : '\"unknown\"'}, {'contact' : 1})\ntt = tt.replace({'contact' : '\"telephone\"'}, {'contact' : 2})\ntt = tt.replace({'contact' : '\"cellular\"'}, {'contact' : 3})\ntt = tt.replace({'loan' : '\"no\"'}, {'loan' : 0})\ntt = tt.replace({'loan' : '\"yes\"'}, {'loan' : 1})\ntt = tt.replace({'housing' : '\"no\"'}, {'housing' : 0})\ntt = tt.replace({'housing' : '\"yes\"'}, {'housing' : 1})\ntt = tt.replace({'default' : '\"no\"'}, {'default' : 0})\ntt = tt.replace({'default' : '\"yes\"'}, {'default' : 1})\ntt = tt.replace({'education' : '\"unknown\"'}, {'education' : 1})\ntt = tt.replace({'education' : '\"primary\"'}, {'education' : 2})\ntt = tt.replace({'education' : '\"secondary\"'}, {'education' : 3})\ntt = tt.replace({'education' : '\"tertiary\"'}, {'education' : 4})\ntt = tt.replace({'marital' : '\"married\"'}, {'marital' : 1})\ntt = tt.replace({'marital' : '\"divorced\"'}, {'marital' : 2})\ntt = tt.replace({'marital' : '\"single\"'}, {'marital' : 3})\ntt = tt.replace({'job' : '\"unknown\"'}, {'job' : 1})\ntt = tt.replace({'job' : '\"unemployed\"'}, {'job' : 2})\ntt = tt.replace({'job' : '\"housemaid\"'}, {'job' : 3})\ntt = tt.replace({'job' : '\"student\"'}, {'job' : 4})\ntt = tt.replace({'job' : '\"self-employed\"'}, {'job' : 5})\ntt = tt.replace({'job' : '\"blue-collar\"'}, {'job' : 6})\ntt = tt.replace({'job' : '\"management\"'}, {'job' : 7})\ntt = tt.replace({'job' : '\"retired\"'}, {'job' : 8})\ntt = tt.replace({'job' : '\"entrepreneur\"'}, {'job' : 9})\ntt = tt.replace({'job' : '\"technician\"'}, {'job' : 10})\ntt = tt.replace({'job' : '\"admin.\"'}, {'job' : 11})\ntt = tt.replace({'job' : '\"services\"'}, {'job' : 12})\ntt = tt.replace({'pdays' : '-1'}, {'pdays' : 0})\ntt = tt.replace({'month' : '\"jan\"'}, {'month' : 1})\ntt = tt.replace({'month' : '\"feb\"'}, {'month' : 2})\ntt = tt.replace({'month' : '\"mar\"'}, {'month' : 3})\ntt = tt.replace({'month' : '\"apr\"'}, {'month' : 4})\ntt = tt.replace({'month' : '\"may\"'}, {'month' : 5})\ntt = tt.replace({'month' : '\"jun\"'}, {'month' : 6})\ntt = tt.replace({'month' : '\"jul\"'}, {'month' : 7})\ntt = tt.replace({'month' : '\"aug\"'}, {'month' : 8})\ntt = tt.replace({'month' : '\"sep\"'}, {'month' : 9})\ntt = tt.replace({'month' : '\"oct\"'}, {'month' : 10})\ntt = tt.replace({'month' : '\"nov\"'}, {'month' : 11})\ntt = tt.replace({'month' : '\"dec\"'}, {'month' : 12})\ntt.head()","92fde44d":"# converting data type from object to numbers\ntt['balance'] = tt['balance'].astype(int)\ntt['day'] = tt['day'].astype(int)\ntt['duration'] = tt['duration'].astype(int)\ntt['campaign'] = tt['campaign'].astype(int)\ntt['pdays'] = tt['pdays'].astype(int)\ntt['previous'] = tt['previous'].astype(int)\ntt['age'] = tt['age'].astype(int)\ntt.info()","8e4d09e0":"# looking for null values in the dataset\ntt.isnull().sum()","04e7dd05":"## Categorical to Dummy Variables\ntt =  pd.get_dummies(tt, columns=[\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"],\n                         prefix=[\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"],\n                         drop_first=True)\ntt.head(2)","b597bebe":"# Assigning dataset into dependent (y) and independent (x)\ny_1 = tt['target']\nX_1 = tt.drop(['target'], axis=1)","93c9cdaa":"# Normalizing all features\nfrom sklearn import preprocessing\nX_3 = preprocessing.StandardScaler().fit(X_1).transform(X_1)\nX_3[0:2]","d39120a4":"# Logistic Regression\ny_predict = gb.predict(X_3)\ny_predict[0:20]\n","1cd129e4":"# Evaluation Metrics\nprint(metrics.accuracy_score(y_1, gb.predict(X_3)))\nprint(metrics.confusion_matrix(y_1, gb.predict(X_3)))\nprint(metrics.classification_report(y_1, gb.predict(X_3)))\nprint(metrics.roc_auc_score(y_1, gb.predict(X_3)))","dafc690c":"# 10 fold cross validation evaluation\nscores = cross_val_score(gb, X_3, y_1, scoring='accuracy', cv=10)\nprint(scores)\nprint(scores.mean())","e60e1384":"# Feature importance\npd.DataFrame(gb.feature_importances_, index = X_1.columns,\n                                    columns=['importance']).sort_values('importance', ascending=False)","122aa89e":"from sklearn.metrics import f1_score\nf1_score(y_1, y_predict, average='weighted') \n\n# The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. \n# It is a good way to show that a classifer has a good value for both recall and precision.\n# And finally, we can tell the average accuracy for this classifier is the average of the F1-score for both labels, which is 0.89 in our case.","b20b37ab":"# ROC, Area Under Curve\npreds = gb.predict_proba(X_3)[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_1, y_predict)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1.05])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","4e7b2416":"# Evaluation of prediction to original output","91ce4597":"# Model Deployment\n- We going to employ the Gradient Boosting Classifier model since it gave as the highest prediction.\n- Also, we going to use out test dataset for our model deployment.","b8cc1890":"# From our analysis, Gradient Boosting came out on top with 70% using ROC.","ef6f31e3":"- The 10 fold cross validation gave us an accuracy of 0.87.","7a7aebfb":"# Statistical inference\n","f3e601c8":"# Exploratory Analysis","92e1fdac":"# From our analysis, Decision Tree came out on top with 71% using ROC.","0185b96d":"- Null hypothesis says two means are almost same.\n- p-value is a probability if the null hypothesis is true. A high p-value (> 0.05) means we can't reject the null hypothesis\n- Since we have a low p-value, we reject the null hypothesis that output and thalachh are the same in terms of painthreshold (No difference)","b7f5ca40":"- The 5 most important features are duration, age, balance, day and poutcome_4.","12b9e58d":"- From our Random Forest Classifier model, we have 11614 cliets who have not subscribed and 371 misclassified as clients who have subscribed.\n- We also have 908 misclassiffied clients who have not subscribed and 671 client who have subscribed.\n- our model accuracy is 0.91","79965390":"### At 0.05 level of significance to test the statistical significance of the X variables, marital_2, job_11, month_8,  and and job_10 are not statistically significant whiles the rest of X variables are statistically significant using the p-value.\n### Our null hypthesis is X variables (independent) are not statistically significant in predicting target (dependent variable). We will fail to reject the null hypothesis if the p-value for our x variables are greater than significant level 0.05.\n### The p-values of some X variables such as age, poutcome_1, loan_1 and others are all less than the 0.05 significant level therefore we reject the null hypothesis and that they are all statistically significant in predicting the dependent variable output.","8ef56b4b":"- Null hypothesis says two means are almost same.\n- p-value is a probability if the null hypothesis is true. A high p-value (> 0.05) means we can't reject the null hypothesis\n- Since we have a low p-value, we reject the null hypothesis that output and age are the same in terms of painthreshold (No difference)","6baf315a":"- The 5 most important features are duration, poutcome_4, housing_1, age, and pdays.","88f8e615":"- The 5 most important features are duration, poutcome_4, housing_1, age, and pdays.","64270e3d":"# Correlation Analysis with dummies","d6847d0e":"- From our Random Forest Classifier model, we have 3885 cliets who have not subscribed and 115 misclassified as clients who have subscribed.\n- We also have 303 misclassiffied clients who have not subscribed and 218 client who have subscribed.\n- our model accuracy is 0.91","2a6c7954":"# Business Intelligence with Visualization","63d1db2f":"## First of all we need to explain the positives and negatives associated with the numbers obtained.\n- Positive Correlation; A positive correlation is a relationship between 2 variables which the increase of one variable causes an increase for another variable.\n- Negative Correlation; The Negative correlation is the opposite, it\u2019s a relationship between 2 variables which the increase of one variable causes a decrease for another variable. This applies otherwise.\n- From our correlation analysis, we can see age has a positive very weak correlation with output_0 (less chance of heart attack). This means that as age increase the more likely there is a chance of heart attack and vice versa.\n- We can also see that AGE has a negative weak correlation with output_1 (more chance of heart attack). This means that the lower the age the patient will not have more chance of a heat attack and vice versa.","e34f7cc6":"# Discriptive Analysis and Data Visualization","c880ab3a":"- From our Random Forest Classifier model, we have 11663 cliets who have not subscribed and 322 misclassified as clients who have subscribed.\n- We also have 939 misclassiffied clients who have not subscribed and 640 client who have subscribed.\n- our model accuracy is 0.91","56713984":"- Null hypothesis says two means are almost same.\n- p-value is a probability if the null hypothesis is true. A high p-value (> 0.05) means we can't reject the null hypothesis\n- Since we have a low p-value, we reject the null hypothesis that output and exng are the same in terms of painthreshold (No difference)","32d4cebb":"- Null hypothesis says two means are almost same.\n- p-value is a probability if the null hypothesis is true. A high p-value (> 0.05) means we can't reject the null hypothesis\n- Since we have a low p-value, we reject the null hypothesis that output and cp are the same in terms of painthreshold (No difference)","efb57717":"- Our 10 fold cross validation gave us 0.90 accuracy","63fd04a2":"# Data Cleaning nad Transformation","0ed823c9":"- From our decision tree model, we have 11289 cliets who have not subscribed and 696 misclassified as clients who have subscribed.\n- We also have 859 misclassiffied clients who have not subscribed and 720 client who have subscribed.\n- our model accuracy is 0.89","bfd33eb3":"- The 10 fold cross validation gave us an accuracy of 0.90","f069dd2d":"- Our 10 fold cross validation gave us 0.90","65c83987":"# Model Building","92c0907d":"# Further data cleaning and Transformation","a459d256":"# Data Cleaning and Transformation"}}