{"cell_type":{"e2af983e":"code","1e9bff24":"code","1db1f377":"code","785bd824":"code","5c1f32fc":"code","85399af1":"code","cae5ab3d":"code","184184c9":"code","42b35ded":"code","1bab8d94":"code","cbcf6ac3":"code","9c8faf92":"code","abed049e":"code","818e2028":"code","fb0fdd6d":"code","59e54e7e":"code","d9316677":"code","2deb847f":"code","1f34eaa7":"code","616e2f01":"code","35258455":"code","e9423417":"code","004e5caa":"markdown","ea56e78e":"markdown","3d241b63":"markdown","7e970233":"markdown","bd5851e2":"markdown","9b85c65a":"markdown","4deb1f92":"markdown"},"source":{"e2af983e":"# Essentials:\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# t-SNE visualization\nfrom sklearn.manifold import TSNE\n\n# imputation\nfrom sklearn.impute import KNNImputer\n\n# Scaling\nfrom sklearn.preprocessing import StandardScaler\n\n# PCA\nfrom sklearn.decomposition import PCA\n\n# K-means for Clustering\nfrom sklearn.cluster import KMeans\n\n# elbow method\nfrom yellowbrick.cluster import KElbowVisualizer\n\n# cluster metrics\nfrom sklearn.metrics import davies_bouldin_score\nfrom sklearn.metrics import silhouette_score\n\n# Silhouette Visualizer\nfrom yellowbrick.cluster import SilhouetteVisualizer\n","1e9bff24":"df = pd.read_csv('\/kaggle\/input\/ccdata\/CC GENERAL.csv')","1db1f377":"df.head()","785bd824":"df.describe()","5c1f32fc":"df.info()","85399af1":"features = ['BALANCE', 'BALANCE_FREQUENCY', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']","cae5ab3d":"# Check for columns with missing values and impute them with the median.\ndef impute_nan(df):\n    \"\"\"Check for columns with missing values and impute them with the median.\"\"\"\n    nan_cols = df.columns[df.isnull().any()].tolist()\n    nan_length = len(nan_cols)\n    if nan_length == 0:\n        return df\n    else:\n        print('Imputed features:', nan_cols)\n        for x in nan_cols:\n            df[x].fillna(df[x].median(), inplace=True)\n        return df\n\ndf = impute_nan(df)","184184c9":"# Check for outliers.\ndef outliers(df, features):\n    \"\"\"Count the number of outliers for each feature using the IQR\"\"\"\n    num_outliers = []\n    pct_outliers = []\n    total_rows = []\n    interquartile_range = []\n    for i in features:\n        Q1 = df[i].quantile(.25)\n        Q3 = df[i].quantile(.65)\n        IQR = Q3 - Q1\n        outliers = len(df[(df[i] < (Q1-1.5*IQR)) | (df[i] > (Q3+1.5*IQR))])\n        rows = len(df[i])\n        pct = outliers\/rows\n        interquartile_range.append(IQR)\n        num_outliers.append(outliers)\n        pct_outliers.append(pct)\n        total_rows.append(rows)\n        \n    count_outliers = pd.DataFrame({'Feature': features\n                               , 'Num_Outliers': num_outliers\n                                , 'Percent_Outliers': pct_outliers\n                                , 'IQR': interquartile_range\n                               , 'Total_Rows': total_rows}).sort_values('Percent_Outliers', ascending=False)\n    return count_outliers\n\ncount_outliers = outliers(df=df, features=features)\ncount_outliers","42b35ded":"# Scale the features\nfrom sklearn.preprocessing import RobustScaler\nX = df[features]\nX_scaled = pd.DataFrame(RobustScaler().fit_transform(X), columns=X.columns, index=X.index)\ndf_scaled = pd.concat([df['CUST_ID'], X_scaled], axis=1)","1bab8d94":"# Use the elbow method to choose the optimal number of clusters \nX = df_scaled[features]\nkmeans = KMeans(random_state=1)\nkmeans_vis = KElbowVisualizer(kmeans, k=(1,15), metric='distortion', timings=False).fit(X)\nprint('Optimal number of clusters:', kmeans_vis.elbow_value_)","cbcf6ac3":"kmean= KMeans(n_clusters=6)\nkmean.fit(X)\nlabels=kmean.labels_","9c8faf92":"plt.figure(figsize=(10,6))\nsns.scatterplot(data=X, x='ONEOFF_PURCHASES', y='PURCHASES', hue=labels)\nplt.title('Distribution of clusters based on One off purchases and total purchases')\nplt.show()","abed049e":"plt.figure(figsize=(10,6))\nsns.scatterplot(data=X, x='CREDIT_LIMIT', y='PURCHASES', hue=labels)\nplt.title('Distribution of clusters based on Credit limit and total purchases')\nplt.show()","818e2028":"kmeanplus= KMeans(init= 'k-means++',n_clusters=6)\nkmeanplus.fit(X)\nlabelplus=kmeanplus.labels_","fb0fdd6d":"plt.figure(figsize=(10,6))\nsns.scatterplot(data=X, x='ONEOFF_PURCHASES', y='PURCHASES', hue=labelplus)\nplt.title('Distribution of clusters based on One off purchases and total purchases')\nplt.show()","59e54e7e":"plt.figure(figsize=(10,6))\nsns.scatterplot(data=X, x='CREDIT_LIMIT', y='PURCHASES', hue=labelplus)\nplt.title('Distribution of clusters based on Credit limit and total purchases')\nplt.show()","d9316677":"from sklearn.cluster import AgglomerativeClustering\nfrom scipy.cluster.hierarchy import dendrogram,linkage\nZ=linkage(X,method=\"ward\")\n","2deb847f":"plt.figure(figsize=(15,10))\ndendrogram(Z,leaf_rotation=90,p=5,color_threshold=20,leaf_font_size=10,truncate_mode='level')\nplt.axhline(y=125, color='r', linestyle='--')\nplt.show()","1f34eaa7":"model=AgglomerativeClustering(n_clusters=6,affinity='euclidean',linkage='complete')\nmodel.fit(X)\nlabelagg=model.labels_","616e2f01":"plt.figure(figsize=(10,6))\nsns.scatterplot(data=X, x='CREDIT_LIMIT', y='PURCHASES', hue=labelagg)\nplt.title('Distribution of clusters based on Credit limit and total purchases')\nplt.show()","35258455":"from sklearn.cluster import DBSCAN\nimport plotly.express as px\nmodel_db = DBSCAN(eps=11, min_samples=6, metric='euclidean')\nmodel_db.fit(X)\nX['cluster_DB'] = model_db.labels_.astype(object)+1","e9423417":"plt.figure(figsize=(12, 8))\nsns.scatterplot(X['CREDIT_LIMIT'], X['PURCHASES'], hue=X['cluster_DB'], \n                palette=sns.color_palette('hls', np.unique(model_db.labels_).shape[0]))\nplt.title('DBSCAN with epsilon 11, min samples 6')\nplt.show()\n","004e5caa":"## Import Libraries","ea56e78e":"## K-Mean\nThe clusters will be the customer segments.\nFind optimal number of clusters \nUse the elbow method to choose the optimal number of clusters. The KElbowVisualizer libary loops through a range of number of clusters and records the distortion (the sum of the squared distances between each observation vector and its dominating centroid). Then it plots the distortion against number of clusters, and identifies the optimal number of clusters using the point on the chart that looks like an elbow.","3d241b63":"## Data Columns\n**CUSTID** : Identification of Credit Card holder (Categorical)\n\n**BALANCE** : Balance amount left in their account to make purchases\n\n**BALANCEFREQUENCY** : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated)\n\n**PURCHASES** : Amount of purchases made from account\n\n**ONEOFFPURCHASES**: Maximum purchase amount done in one-go\n\n**INSTALLMENTSPURCHASES** : Amount of purchase done in installment\n\n**CASHADVANCE** : Cash in advance given by the user\n\n**PURCHASESFREQUENCY** : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased)\n\n**ONEOFFPURCHASESFREQUENCY** : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased) PURCHASESINSTALLMENTSFREQUENCY : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done)\n\n**CASHADVANCEFREQUENCY** : How frequently the cash in advance being paid\n\n**CASHADVANCETRX** : Number of Transactions made with \"Cash in Advanced\"\n\n**PURCHASESTRX** : Numbe of purchase transactions made\n\n**CREDITLIMIT** : Limit of Credit Card for user\n\n**PAYMENTS** : Amount of Payment done by user\n\n**MINIMUM_PAYMENTS** : Minimum amount of payments made by user\n\n**PRCFULLPAYMENT** : Percent of full payment paid by user\n\n**TENURE** : Tenure of credit card service for user","7e970233":"## Hierarchical Clustering\n","bd5851e2":"# Credit Card Dataset for Clustering\nWe are going to experiment using these clustering methods:\n1. K-Means\n2. K-Means++\n3. Hirarchical\n4. DBSCAN","9b85c65a":"## Density Based Clustering (DBSCAN)\n","4deb1f92":"## K-Mean++"}}