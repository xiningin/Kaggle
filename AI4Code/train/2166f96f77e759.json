{"cell_type":{"f38bb01f":"code","322fd74b":"code","48a9e111":"code","75ac2838":"code","5d1708f7":"code","7ac563a9":"markdown","c9f9741f":"markdown","daab2d11":"markdown","f638b1f3":"markdown","9ad2bef6":"markdown","98273921":"markdown","304ee2e2":"markdown","3c4cc5dc":"markdown","fdfddec2":"markdown","b49cb2fa":"markdown","191ad2eb":"markdown","13a288fd":"markdown","105a9c75":"markdown","39028c54":"markdown","665fb415":"markdown","3dc3239d":"markdown","2fc55447":"markdown","d84fda0a":"markdown","a1dd5a22":"markdown","6089350e":"markdown","ed399333":"markdown","2f94a9cc":"markdown","31a076e1":"markdown","d5751a91":"markdown","bc0b2de5":"markdown","caf44ab7":"markdown","f6c6f808":"markdown","ba3be289":"markdown","7b84781c":"markdown","6e1090d2":"markdown","8e2d0ca5":"markdown","35875d3d":"markdown","ccc0ade1":"markdown","cada33f0":"markdown","a057305e":"markdown","c6c5c377":"markdown","7063add9":"markdown","ab2b0b04":"markdown","86f7a847":"markdown","e60243cf":"markdown","04012d41":"markdown","dd2f57a0":"markdown","3b68021d":"markdown","f0aa3e25":"markdown","db60f10c":"markdown","ac1d8188":"markdown","d7e34bfa":"markdown","f6b12e2c":"markdown","256fbc85":"markdown"},"source":{"f38bb01f":"## Importing Libraries\nfrom sklearn.datasets import load_iris\nimport matplotlib.pyplot as plt\n\n## Loading the Dataset\niris = load_iris(return_X_y = True, as_frame= True)\ndataset = iris[0]\ntarget = iris[1]\n\n## Plotting Histogram\ndataset.hist(edgecolor='black', linewidth=1.2)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()","322fd74b":"max_depth = 5\nmin_samples_split = 2 \ncriterion = \"gini\"  \nsplitter = \"best\" \n\nfrom sklearn.tree import DecisionTreeClassifier\n\nX, y = load_iris(return_X_y=True)\nclf = DecisionTreeClassifier(\n    max_depth=max_depth,\n    min_samples_split=min_samples_split,\n    criterion=criterion,\n    splitter=splitter\n)\nclf = clf.fit(X,y)","48a9e111":"from sklearn import tree\n%matplotlib inline\nfn=['sepal length (cm)','sepal width (cm)','petal length (cm)','petal width (cm)']\ncn=['setosa', 'versicolor', 'virginica']\nfig = plt.figure(figsize=(20,15))\n_ = tree.plot_tree(clf, \n                   feature_names=fn,  \n                   class_names=cn,\n                   filled=True)","75ac2838":"max_depth = 5\nmin_samples_split = 2  \ncriterion = \"mse\"  \nsplitter = \"best\" \n\nfrom sklearn.tree import DecisionTreeRegressor\n\nX, y = load_iris(return_X_y=True)\nreg = DecisionTreeRegressor(\n    max_depth=max_depth,\n    min_samples_split=min_samples_split,\n    criterion=criterion,\n    splitter=splitter\n)\nreg = reg.fit(X,y)","5d1708f7":"%matplotlib inline\ntree.plot_tree(reg, filled = True)\nplt.show()","7ac563a9":"![MD-4,%20Gini-min.png](attachment:MD-4,%20Gini-min.png)","c9f9741f":"### ID3\n---\nThe core algorithm for building decision trees is called ID3. Developed by J. R. Quinlan, this algorithm employs a top-down, greedy search through the space of possible branches with no backtracking. ID3 uses Entropy and Information Gain to construct a decision tree.\n\n### C4.5 \n---\nC4.5 builds decision trees from a set of training data in the same way as ID3, using the concept of information entropy.\n\nC4.5 made a number of improvements to ID3. Some of these are:\nHandling both continuous and discrete attributes \u2014 In order to handle continuous attributes, C4.5 creates a threshold and then splits the list into those whose attribute value is above the threshold and those that are less than or equal to it.\n\nHandling training data with missing attribute values \u2014 C4.5 allows attribute values to be marked as ? for missing. Missing attribute values are simply not used in gain and entropy calculations. Handling attributes with differing costs.\nPruning trees after creation \u2014 C4.5 goes back through the tree once it\u2019s been created and attempts to remove branches that do not help by replacing them with leaf nodes.\n\n### CART\n---\nClassification and Regression Trees or CART for short is a term introduced by Leo Breiman to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems.\n\nClassically, this algorithm is referred to as \u201cdecision trees\u201d, but on some platforms like R they are referred to by the more modern term CART.\n\nThe CART algorithm provides a foundation for important algorithms like bagged decision trees, random forest and boosted decision trees.","daab2d11":"## Complexity\n\n","f638b1f3":"## Weaknesses of Decision Tree","9ad2bef6":"![MD-3,Entropy-min.png](attachment:MD-3,Entropy-min.png)","98273921":"We can also export the tree into GraphViz format using [`export_graphviz()`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.export_graphviz.html#sklearn-tree-export-graphviz).","304ee2e2":"We use the [`sklearn.tree.DecisionTreeRegressor`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeRegressor.html?highlight=decisiontreeregressor#sklearn.tree.DecisionTreeRegressor) method from the sklearn library.\n\n### Parameters\n---\n\n* **criterion:** \"`gini`\" or \"`entropy`\"\n* **splitter:** \"`best`\" or \"`random`\"\n* **max_depth (*int*):** The maximum depth of the tree. Defaults to `None`, then nodes are expanded until all leaves are pure\n* **min_samples_split (*int*):** The minimum number of samples required to split an internal node.\n\n\nYou can find the other parameters [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeRegressor.html?highlight=decisiontreeregressor#sklearn.tree.DecisionTreeRegressor).","3c4cc5dc":"In the case of a decision tree for classification, namely, a **classification tree**, the goodness of a split is quantified by an impurity measure. A split is \"pure\" if after the split, for all branches, all the instances choosing a branch belong to the same class.\n\nIf node $m$ is not pure, then the instances should be split to decrease impurity, and there are multiple possible attributes on which we can split. We look for the split that minimzes impurity after the split because we want to generate the smallest tree. If the subsets, after the split are closer to pure, fewer splits(*if any*) will be needed afterward. Of course this is locally optimal, and we have no guarantee of finding the smallest decision tree.\n\nIt can also be said that at each step during tree construction, we choose the split that causes the largest decrease in impurity, which is the difference between the impurity of data reaching node $m$ and the total entropy of data reaching its branches after the split.\n\n---\n\nOne problem is that such splitting favours attributes with many values. When there are many values, there are many branches, and the impurity can be much less. Nodes with many branches are complex and go against our idea of splitting class discriminants into simple decisions. Methods have been proposed to penalize such attributes and to balance the impurity drop and the branching factor.\n\nTo alleviate overfitting, tree construction ends when nodes become pure enough, namely, a subset of data is not split further if $I < {\\theta}_I$. Here, ${\\theta}_I$ is a complexity parameter. When it's small, the variance is high and the tree grows large to reflect the training set accurately, and when they are large, variance is lower and a smaller tree roughly represents the training set.","fdfddec2":"We use perhaps the best known dataset in pattern recognition literature. The data set contains 3 classes of 50 instances each, where each class refers to a *type of iris plant*. One class is linearly separable from the other 2; the latter are NOT linearly separable from each other.\n\n**Attributes**\n\n1. Sepal length (*in cm*)\n2. Sepal width (*in cm*)\n3. Petal length (*in cm*)\n4. Petal width (*in cm*)\n5. Class:\n  - Iris Setosa\n  - Iris Versicolour\n  - Iris Virginica\n","b49cb2fa":"## Visualization","191ad2eb":"### Max Depth = 3\n------","13a288fd":"## Articles\n---\n* [Decision Tree. It begins here](https:\/\/medium.com\/@rishabhjain_22692\/decision-trees-it-begins-here-93ff54ef134)\n\n* [Decision tree learning](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning)\n\n### Python Notebooks\n---\n* [Introduction to Decision Trees (Titanic dataset)](https:\/\/www.kaggle.com\/dmilla\/introduction-to-decision-trees-titanic-dataset)\n* [Decision Trees and Random Forest Tutorial](https:\/\/www.kaggle.com\/faressayah\/decision-trees-and-random-forest-tutorial)\n* [A Guide to Decision Trees for Beginners](https:\/\/www.kaggle.com\/vipulgandhi\/a-guide-to-decision-trees-for-beginners)\n\n### Research Papers\n---\n* L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984.\n\n## Dataset Links\n---\n* [Iris Data Set](https:\/\/archive.ics.uci.edu\/ml\/datasets\/iris)\n","105a9c75":"### Gini Index\n---\n* Gini Index, also known as *Gini impurity*, calculates the amount of probability of a specific feature that is classified incorrectly when selected randomly. If all the elements are linked with a single class then it can be called pure.\n\n* It varies between 0 and 1\n\n* It's calculated by deducting the sum of square of probabilites of each class from one\n\n> $Gini \\, Index = 1 - \\sum_{i-1}^{n} (P_i)^2$\n\n### Information Gain\n---\n* Entropy is the measurement of the impurity or randomness in data points\n\n* Entropy is calculated between 0 and 1\n\n* Entropy in information theory specifies the minimum number of bits needed to encode the class code of an instance.\n\n* Information Gain is applied to quantify which feature provides maximal information about the classification based on the notion of entropy, i.e. by quantifying the size of uncertainty, disorder or impurity, in general, with the intention of decreasing the amount of entropy initiating from the top (root node) to bottom(leaves nodes)\n\n> $Entropy = -\\sum_{i=1}^{n}p_i * log_2(p_i)$\n\n### Reduction in Variance\n---\n\n* Reduction in variance is an algorithm used for continuous target variables.\n\n* This algorithm uses the standard formula of variance to choose the best split. The split with lower variance is selected as the criteria to split the population.\n\n> $ Variance = \\frac{\\sum (X - \\bar{X})^2}{n} $\n\n### Chi-Square\n\n---\n\n* It is an algorithm to find out the statistical significance between the differences between sub nodes and parent node.\n\n* We measure it by sum of squares of standardised differences between observed and expected frequencies of target variable.","39028c54":"### Max Depth = 4\n------------","665fb415":"## Terminology\n\n\n","3dc3239d":"## Different Depths","2fc55447":"### Max Depth = 3\n----","d84fda0a":"### Max Depth = 2\n----","a1dd5a22":"![MD-2-min.png](attachment:MD-2-min.png)","6089350e":"![MD-5-min.png](attachment:MD-5-min.png)","ed399333":"## Where to Split ?","2f94a9cc":"![MD-3-min.png](attachment:MD-3-min.png)","31a076e1":"# Resources","d5751a91":"# Introduction\n\n\n\n\n","bc0b2de5":"![MD-4-min.png](attachment:MD-4-min.png)","caf44ab7":"![MD-3,%20Gini-min.png](attachment:MD-3,%20Gini-min.png)","f6c6f808":"## Pruning","ba3be289":"### Distribution of Length and Width","7b84781c":"### Max Depth = 5\n----","6e1090d2":"We use the [`sklearn.tree.DecisionTreeClassifier`](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn-tree-decisiontreeclassifier) method from the sklearn library to create our Classification Tree.\n\n### Parameters\n---\n\n* **criterion:** \"`gini`\" or \"`entropy`\"\n* **splitter:** \"`best`\" or \"`random`\"\n* **max_depth (*int*):** The maximum depth of the tree. Defaults to `None`, then nodes are expanded until all leaves are pure\n* **min_samples_split (*int*):** The minimum number of samples required to split an internal node.\n\nYou can find the other parameters [here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html#sklearn-tree-decisiontreeclassifier).","8e2d0ca5":"\n1. **Root Node:** Represents the entire population or sample and this further gets divided into two or more homogeneous sets.\n2. **Leaf Node:** That node which can't get segregated into further nodes\n3. **Splitting:** Dividing the root node\/subtree into different parts on the basis of some condition\n4. **Pruning:** Opposite of splitting. Removing unwanted branches from the tree\n\n5. **Branch\/Sub-Tree:** A sub-section of decision tree\n\n6. **Parent and Child Node:** A node, which is divided into sub-nodes is called parent node of sub-nodes where sub-nodes are the child of parent node","35875d3d":"# The Iris Dataset\n\n","ccc0ade1":"# Building the Model","cada33f0":"## Visualising the Regression Model","a057305e":"## Building the Regression Model","c6c5c377":"### Max Depth = 4\n----","7063add9":"* It produces very simple understandable rules. For smaller trees, not much mathematical and computational knowledge is required to understand this model.\n\n* Works well for most of the problems.\n\n* It can handle both numerical and categorical varaibles.\n\n* Can work well both with small and large training data sets.\n\n* Decision trees provide a definite clue of which features are more useful for classification.","ab2b0b04":"* Decision tree models are often biased towards features having more number of possible values.\n\n* This model gets overfitted or underfitted quite easily.\n\n* Decision trees are prone to errors in classification problems with many classes and relatively small number of training examples.\n\n* A decision tree can be computationally expensive to train.\n\n* Large trees are complex to understand.","86f7a847":"# Analysing Parameter Effects","e60243cf":"## Strengths of Decision Tree","04012d41":"## **Gini** vs **Entropy** for Splitting","dd2f57a0":"Frequently, a node is not split further if the number of training instances reaching a node is smaller than a certain percentage of the training set. The idea is that any decision based on too few instances causes variance and thus generalization error. Stopping tree constructs early on before it is full is called **prepruning** the tree.\n\nAnother possibility to get simpler trees is **postpruning**, which in practice works better than prepruning. Tree growing is greedy and at each step, we make a decision, and continue further on, never backtracking and trying out an alternative. The only exception is postpruning where we try to find and prune unnecessary subtrees.\n\nIn postpruning, we grow the tree full untill all the leaves are pure and we have no training error. We then find subtrees that cause overfitting and we prune them. From the initial labeled set, we set aside a **pruning set**, unused during training. For each subtree, we replace it with a leaf node labeled with the training instances covered by the subtree. If the leaf node does not perform worse than the subtree on the pruning set, we prune the subtree and keep the leaf node because the additional complexity of the subtree is not justifies; otherwise, we keep the subtree.\n\nComparing prepruning and postpruning, we can say that **prepruning is faster but postpruning generally leads to more accurate trees**.","3b68021d":"![MD-4,%20Entropy-min.png](attachment:MD-4,%20Entropy-min.png)","f0aa3e25":"## Building the Classification model","db60f10c":"The run time cost to construct a balanced binary tree is :\n\n* $O(n_{samples}n_{features}log(n_{samples}))$\n\n","ac1d8188":"As, we can see if we keep the `max_depth` as constant the structure of the trees remains same. Irrespective of the splitting criterion","d7e34bfa":"## Algorithms used in Decision Trees","f6b12e2c":"\nA *decision tree* is a hierarchical data structure implementing the divide-and-conquer stratergy. It is an efficient nonparametric method, which can be used for both classification and regression. \n\nIn **parametric estimation**, we define a model over the whole input space and learn its parameters from all of the training data. Then we use the same model and the same parameter set for any test input. In **nonparametric estimation**, we divide the input space into local regions, defined be distance measures.\n\nA decision tree is a non-parametric model in the sense that we do not assume any parametric form for the class densities and the *tree structure is not fixed* a priori but the tree grows, branches and leaves are added, during learning depending on the complexity of the problem inherent in the data.\n\n\n\n> A **tree** is a set of one or more nodes. A node $x_i$, subscript $i$ being either empty or a sequence of one or more non-negative integers, is joined to another node $x_{ij}$ by an arc directed from $x_i$ to $x_{ij}$.\n\n","256fbc85":"A *regression tree* is constructed in almost the same manner as a classification tree, except that the impurity measure that is appropriate for classification is replaced by a measure appropriate for regression. "}}