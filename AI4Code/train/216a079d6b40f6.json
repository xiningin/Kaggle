{"cell_type":{"e7e0d55c":"code","d74844db":"code","f9f258a6":"code","4c595bcf":"code","88b9b2db":"code","19eb1c46":"code","1038aef0":"code","0912c23e":"code","e86d51d7":"code","ba54a9a6":"code","fecb0ad7":"code","3357e365":"code","1ce03a59":"code","b67c7208":"code","cde8c74b":"code","c39e54f9":"code","f50746f8":"code","7aadf8ac":"code","faae392c":"code","0384589e":"code","b7e9c89b":"code","78382c59":"code","74864708":"code","b8d715fe":"code","fe1b5fc4":"markdown","e038b9f9":"markdown","9fd656bd":"markdown","70a98e25":"markdown","323b4826":"markdown","7c32ba6d":"markdown","5355203b":"markdown","9d93c201":"markdown","c0d67a76":"markdown","81139eee":"markdown","a766b480":"markdown","4b1964c9":"markdown","df472d51":"markdown","7c03d5c6":"markdown","d87dba60":"markdown","ecc9908d":"markdown","29c2a9c1":"markdown","df48aa16":"markdown"},"source":{"e7e0d55c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","d74844db":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","f9f258a6":"df = pd.read_csv('\/kaggle\/input\/ionosphere\/ionosphere_data.csv')\ndf","4c595bcf":"sns.countplot(x='column_ai', data=df)","88b9b2db":"df.drop(columns=['column_b'], inplace=True)","19eb1c46":"df.rename(columns={'column_ai': 'label'}, inplace=True)\ndf['label'] = df.label.astype('category')\nencoding = {'g': 1, 'b': 0}\ndf.label.replace(encoding, inplace=True)\ndf","1038aef0":"df['column_a'] = df.column_a.astype('float64')","0912c23e":"X = df.values[:, :-1]\ny = df.values[:, -1]","e86d51d7":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=7)","ba54a9a6":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n# The boolean feature does not need to be normalized.\nx_train[:, 1:] = scaler.fit_transform(x_train[:, 1:])\nx_test[:, 1:] = scaler.transform(x_test[:, 1:])","fecb0ad7":"from sklearn.manifold import TSNE\n\nx_embedded = TSNE(n_components=2).fit_transform(x_train)\nplt.scatter(x_embedded[:, 0], x_embedded[:, 1], color=['green' if label else 'red' for label in y_train])\nplt.show()","3357e365":"from sklearn.decomposition import PCA\n\nx_embedded = PCA(n_components=2).fit_transform(x_train)\nplt.scatter(x_embedded[:, 0], x_embedded[:, 1], color=['green' if label else 'red' for label in y_train])\nplt.show()","1ce03a59":"iterations = 100\nbatch_size = 32","b67c7208":"from torch.utils.data import Dataset\n\n\nclass TrainData(Dataset):\n    \n    def __init__(self, x_train, y_train):\n        self.x_train = x_train\n        self.y_train = y_train\n    \n    def __getitem__(self, index):\n        return self.x_train[index], self.y_train[index]\n    \n    def __len__ (self):\n        return len(self.x_train)\n\n    \nclass TestData(Dataset):\n    \n    def __init__(self, x_test):\n        self.x_test = x_test\n        \n    def __getitem__(self, index):\n        return self.x_test[index]\n        \n    def __len__ (self):\n        return len(self.x_test)","cde8c74b":"train_data = TrainData(torch.from_numpy(x_train).to(torch.float32), torch.from_numpy(y_train).to(torch.float32))\ntest_data = TestData(torch.from_numpy(x_test).to(torch.float32))","c39e54f9":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_data, batch_size=1)","f50746f8":"class Network(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        # Number of input features is 33.\n        self.linear_1 = nn.Linear(33, 64)\n        self.linear_2 = nn.Linear(64, 64)\n        self.linear_3 = nn.Linear(64, 1)\n        self.dropout = nn.Dropout(p=0.1)\n    \n    def forward(self, inputs):\n        out = self.linear_1(inputs)\n        out = F.relu(out)\n        out = self.linear_2(out)\n        out = F.relu(out)\n        out = self.dropout(out)\n        out = self.linear_3(out)\n        # Sigmoid activation is later applied by the loss function for numerical stability.\n        return out","7aadf8ac":"network = Network()\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.Adam(network.parameters(), lr=1e-4)","faae392c":"from sklearn.metrics import roc_auc_score\n\nn_batches = len(train_loader)\n\nnetwork.train()\n\nloss_li = []\nscore_li = []\n\nfor it in range(iterations):\n    it_loss = 0\n    it_score = 0\n    for x_batch, y_batch in train_loader:\n        optimizer.zero_grad()\n        y_real = y_batch.unsqueeze(1)\n        y_pred = network(x_batch)\n        loss = criterion(y_pred, y_real)\n        y_pred = torch.sigmoid(y_pred.detach())\n        score = roc_auc_score(y_real, y_pred)\n        loss.backward()\n        optimizer.step()\n        it_loss += loss.item()\n        it_score += score\n    loss_li.append(it_loss \/ n_batches)\n    score_li.append(it_score \/ n_batches)\n    print('[Iteration {}] Loss: {:.4f}, Area-Under-Curve: {:.4f}'.format(it, it_loss \/ n_batches, it_score \/ n_batches))\n        ","0384589e":"plt.plot(loss_li)\nplt.xlabel('Iteration')\nplt.ylabel('Binary Cross-Entropy Loss')\nplt.show()","b7e9c89b":"plt.plot(score_li)\nplt.xlabel('Iteration')\nplt.ylabel('Area Under Curve')\nplt.show()","78382c59":"network.eval()\n\npredictions = []\n\nwith torch.no_grad():\n    for x_batch in test_loader:\n        y_pred = network(x_batch)\n        y_pred = torch.sigmoid(y_pred)\n        predictions.append(y_pred.squeeze().tolist())\n\ny_pred = np.round(predictions)","74864708":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred)","b8d715fe":"from sklearn.metrics import classification_report\n\nclassification_report(y_test, y_pred, output_dict=True)","fe1b5fc4":"# Toying with the Dataset\n\nLet's visualize the data and see if we can find anything interesting! We'll first use **t-SNE** to reduce the dimensions of the data while preserving the relative distance of the vectors.","e038b9f9":"## Model Definition\n\nWe'll be using a **DropOut** layer in the model to ensure that the network does not overfit on the training set.","9fd656bd":"# Dataset\n\nAccording to the Kaggle Dataset Page:\n\n> This radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. See the paper for more details. The targets were free electrons in the ionosphere. \"Good\" radar returns are those showing evidence of some type of structure in the ionosphere. \"Bad\" returns are those that do not; their signals pass through the ionosphere.\n> \n> Received signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number. There were 17 pulse numbers for the Goose Bay system. Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal.","70a98e25":"## Class Distribution\n\nThe dataset has a class imbalance proble, but it's not that severe.","323b4826":"## Data Pipeline","7c32ba6d":"The dataset doesn't seem to be that complicated. Let's now see what it would look like when processed by the PCA algorithm:","5355203b":"# Notebook Setup\n\nWe'll install and import the needed libraries here.","9d93c201":"## Training\n\nWe'll use the **Area Under Curve** metric for evaluating the model during the training. Using the Accuracy metric is not a good idea, considering the label imbalance present in the dataset.","c0d67a76":"## Configurations","81139eee":"## Preparing the Dataset for Training and Validation","a766b480":"Quite the performance! Looks like we didn't overfit after all :)","4b1964c9":"## Evaluation\n\nWe'll compute and analyze the **Confusion Matrix** of the model using `test_data`. Then, we'll compute the **Precision**, **Recall**, and **F1 Score** of each class.","df472d51":"With the right transformation, the dataset can even be linearly separated! We can expect a high accuracy of even a simple model.","7c03d5c6":"# Modeling\n\nWe need to first define a few other modules before we can get to the network.","d87dba60":"# Preprocessing","ecc9908d":"## Removing the Useless Features\n\nThe second feature, `column_b`, has no variance and therefore isn't useful to the model.","29c2a9c1":"## Converting the Remaining Boolean Feature Into Numeric","df48aa16":"## Encoding the Output Classes\n\nPyTorch takes in integer class indices in its loss functions."}}