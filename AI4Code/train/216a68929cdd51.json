{"cell_type":{"c4424c5e":"code","8d33b1bd":"code","a1f1864a":"code","18f11374":"code","86867502":"code","6137a076":"code","af7cd8ea":"code","62ad3f92":"code","8252a349":"code","20b28647":"code","789b318c":"code","c98a1a38":"code","ffc61d7b":"code","94360707":"code","b9a9ec26":"code","3335bc7d":"code","4278def5":"code","b391e458":"code","1e807676":"code","be45d976":"code","8b81c04f":"code","adfd2f01":"code","f73cc701":"code","36c7ddc7":"code","609cf926":"code","e00feea7":"code","5c04a42a":"code","15106340":"code","c8ac2cbe":"code","f267cb9a":"code","1d1b50c0":"code","d85ce120":"code","4bb823e1":"code","ba0ebd4b":"code","ef6fa0b7":"code","53171deb":"code","419ea9de":"code","4b36893c":"code","4585c8ee":"code","d74f0788":"code","8316c12d":"code","1d3cdabe":"code","2d42a7dc":"code","3bab5e34":"code","58017b85":"code","b648daca":"code","18edc30a":"code","34bd32bf":"code","96ea3935":"code","57e73fc4":"code","bca134f7":"code","abaac03a":"code","bde61e0f":"code","a0d6dd8a":"code","7d36e1b3":"code","b2871d6b":"code","9538b14d":"markdown","85dfb400":"markdown","be688379":"markdown","146eb695":"markdown","5f733288":"markdown","16bc0360":"markdown","d53ca42d":"markdown","2378c620":"markdown","7e5bfa3b":"markdown","7869fecf":"markdown","b4fd169f":"markdown","af830724":"markdown","edc6a460":"markdown","4ae39637":"markdown","15cad150":"markdown","a4b0f235":"markdown","474d317e":"markdown","86303a3e":"markdown","77d011b5":"markdown","29a5518b":"markdown","babb01c9":"markdown","5b62c268":"markdown","872719c7":"markdown","8f69ffca":"markdown","52db451a":"markdown","d131afe3":"markdown","2d2c3f30":"markdown","b2377470":"markdown","1d9a93be":"markdown","e56dbca5":"markdown","45dc3f5a":"markdown","b5633b5e":"markdown","7a6a55fd":"markdown","ad8651ee":"markdown","daad86c1":"markdown","3f8c89a7":"markdown","ee50f187":"markdown"},"source":{"c4424c5e":"# This Python 3 environment comes with many helpful analytics libraries installed\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization\nsns.set_style('whitegrid')\nimport matplotlib.pyplot as plt # data visualization\n%matplotlib inline","8d33b1bd":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier","a1f1864a":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","18f11374":"titanic_train = pd.read_csv(\"..\/input\/train.csv\")\ntitanic_test = pd.read_csv(\"..\/input\/test.csv\")","86867502":"print(\"Train: rows:{} columns:{}\".format(titanic_train.shape[0], titanic_train.shape[1]))","6137a076":"print(\"Test Data Shape\",titanic_test.shape)","af7cd8ea":"titanic_train.head()","62ad3f92":"titanic_train.tail()","8252a349":"titanic_test.head()","20b28647":"titanic_test.tail()","789b318c":"print(\"Total Number of passagner on Titanic (from training data):\", str(len(titanic_train)))","c98a1a38":"sns.countplot(x=\"Survived\", data=titanic_train)","ffc61d7b":"sns.countplot(x=\"Survived\", hue = 'Sex', data=titanic_train)","94360707":"sns.countplot(x = \"Survived\", hue = \"Pclass\", data = titanic_train)","b9a9ec26":"titanic_train['Age'].plot.hist()","3335bc7d":"titanic_train['Fare'].plot.hist(bins = 20, figsize = (10,5))","4278def5":"titanic_train.info()\nprint(\"----------------------------\")\ntitanic_test.info()","b391e458":"sns.countplot(x= \"SibSp\", data = titanic_train)","1e807676":"sns.countplot(x = \"Parch\", data = titanic_train)","be45d976":"titanic_train.isnull().sum()","8b81c04f":"sns.heatmap(titanic_train.isnull(), yticklabels=False, cmap = 'viridis')","adfd2f01":"sns.boxplot(x = 'Pclass', y = 'Age', data = titanic_train)","f73cc701":"print(\"Training dataset columns:\",titanic_train.columns)\nprint(\"-------------------------------\")\nprint(\"Training dataset columns:\",titanic_test.columns)","36c7ddc7":"# Drop unnecessary columns, these columns won't be useful in analysis and prediction\ntitanic_train = titanic_train.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\ntitanic_test = titanic_test.drop(['Name','Ticket','Cabin'], axis=1)","609cf926":"print(\"Training dataset columns:\",titanic_train.columns)\nprint(\"-------------------------------\")\nprint(\"Training dataset columns:\",titanic_test.columns)","e00feea7":"# Only in titanic_train dataset, fill the two missing values with the most occurred value, which is \"S\".\ntitanic_train[\"Embarked\"] = titanic_train[\"Embarked\"].fillna(\"S\")\n\n# plot\nsns.factorplot('Embarked','Survived', data=titanic_train, size=5, aspect=3)\n\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,5))\n\n# sns.factorplot('Embarked', data=titanic_df, kind='count', order=['S','C','Q'], ax=axis1)\n# sns.factorplot('Survived',hue=\"Embarked\",data=titanic_df,kind='count',order=[1,0],ax=axis2)\nsns.countplot(x='Embarked', data=titanic_train, ax=axis1)\nsns.countplot(x='Survived', hue=\"Embarked\", data=titanic_train, order=[1,0], ax=axis2)\n\n# group by embarked, and get the mean for survived passengers for each value in Embarked\nembark_perc = titanic_train[[\"Embarked\", \"Survived\"]].groupby(['Embarked'],as_index=False).mean()\nsns.barplot(x='Embarked', y='Survived', data=embark_perc,order=['S','C','Q'],ax=axis3)","5c04a42a":"'''\nEither to consider Embarked column in predictions, and remove \"S\" dummy variable, \nand leave \"C\" & \"Q\", since they seem to have a good rate for Survival.\n\nOR, don't create dummy variables for Embarked column, just drop it, \nbecause logically, Embarked doesn't seem to be useful in prediction.\n'''\n\nembark_dummies_titanic_train  = pd.get_dummies(titanic_train['Embarked'])\nembark_dummies_titanic_train.drop(['S'], axis=1, inplace=True)\n\nembark_dummies_titanic_test  = pd.get_dummies(titanic_test['Embarked'])\nembark_dummies_titanic_test.drop(['S'], axis=1, inplace=True)","15106340":"titanic_train = titanic_train.join(embark_dummies_titanic_train)\ntitanic_test  = titanic_test.join(embark_dummies_titanic_test)","c8ac2cbe":"titanic_train.drop(['Embarked'], axis=1,inplace=True)\ntitanic_test.drop(['Embarked'], axis=1,inplace=True)","f267cb9a":"titanic_train.head()","1d1b50c0":"titanic_test.head()","d85ce120":"# Only for titanic_test, since there is a missing \"Fare\" values\ntitanic_test[\"Fare\"].fillna(titanic_test[\"Fare\"].median(), inplace=True)","4bb823e1":"# Convert from float to int\ntitanic_train['Fare'] = titanic_train['Fare'].astype(int)\ntitanic_test['Fare'] = titanic_test['Fare'].astype(int)","ba0ebd4b":"# Get fare for survived & didn't survive passengers \nfare_not_survived = titanic_train[\"Fare\"][titanic_train[\"Survived\"] == 0]\nfare_survived     = titanic_train[\"Fare\"][titanic_train[\"Survived\"] == 1]\n\n# Get average and std for fare of survived\/not survived passengers\navg_fare = pd.DataFrame([fare_not_survived.mean(), fare_survived.mean()])\nstd_fare = pd.DataFrame([fare_not_survived.std(), fare_survived.std()])","ef6fa0b7":"# plot\ntitanic_train['Fare'].plot(kind='hist', figsize=(15,3),bins=100, xlim=(0,50))\n\navg_fare.index.names = std_fare.index.names = [\"Survived\"]\navg_fare.plot(yerr=std_fare,kind='bar',legend=False)","53171deb":"fig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Original Age Values - Titanic')\naxis2.set_title('New Age Values - Titanic')\n\n# axis3.set_title('Original Age values - Test')\n# axis4.set_title('New Age values - Test')\n\n# Get Average, STD, and Number of NaN values in titanic_train\navg_age_titanic_train = titanic_train[\"Age\"].mean()\nstd_age_titanic_train = titanic_train[\"Age\"].std()\ncount_nan_age_titanic_train = titanic_train[\"Age\"].isnull().sum()\n\n# Get Average, STD, and Number of NaN values in titanic_test\navg_age_titanic_test = titanic_test[\"Age\"].mean()\nstd_age_titanic_test = titanic_test[\"Age\"].std()\ncount_nan_age_titanic_test = titanic_test[\"Age\"].isnull().sum()\n\n# Generate random numbers between (mean - std) & (mean + std)\nrand_1 = np.random.randint(avg_age_titanic_train - std_age_titanic_train, avg_age_titanic_train + std_age_titanic_train, size = count_nan_age_titanic_train)\nrand_2 = np.random.randint(avg_age_titanic_test - std_age_titanic_test, avg_age_titanic_test + std_age_titanic_test, size = count_nan_age_titanic_test)\n\n# plot original Age values\n# NOTE: drop all null values, and convert to int\ntitanic_train['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n# test_df['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n\n# fill NaN values in Age column with random values generated\ntitanic_train[\"Age\"][np.isnan(titanic_train[\"Age\"])] = rand_1\ntitanic_test[\"Age\"][np.isnan(titanic_test[\"Age\"])] = rand_2\n\n# Convert from float to int\ntitanic_train['Age'] = titanic_train['Age'].astype(int)\ntitanic_test['Age'] = titanic_test['Age'].astype(int)\n        \n# plot new Age Values\ntitanic_train['Age'].hist(bins=70, ax=axis2)\n#titanic_test['Age'].hist(bins=70, ax=axis4)","419ea9de":"# peaks for survived\/not survived passengers by their age\nfacet = sns.FacetGrid(titanic_train, hue=\"Survived\",aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, titanic_train['Age'].max()))\nfacet.add_legend()\n\n# average survived passengers by age\nfig, axis1 = plt.subplots(1,1,figsize=(18,4))\navg_age = titanic_train[[\"Age\", \"Survived\"]].groupby(['Age'],as_index=False).mean()\nsns.barplot(x='Age', y='Survived', data=avg_age)","4b36893c":"\"\"\" \nInstead of having two columns Parch & SibSp, We can have only one column represent \nif the passenger had any family member aboard or not,\nMeaning, if having any family member(whether parent, brother, ...etc) will increase chances of Survival or not.\n\"\"\"\n# make changes with training dataset\ntitanic_train['Family'] =  titanic_train[\"Parch\"] + titanic_train[\"SibSp\"]\ntitanic_train['Family'].loc[titanic_train['Family'] > 0] = 1\ntitanic_train['Family'].loc[titanic_train['Family'] == 0] = 0\n\n# make changes with test dataset\ntitanic_test['Family'] =  titanic_test[\"Parch\"] + titanic_test[\"SibSp\"]\ntitanic_test['Family'].loc[titanic_test['Family'] > 0] = 1\ntitanic_test['Family'].loc[titanic_test['Family'] == 0] = 0","4585c8ee":"# Now we will drop Parch & SibSp\ntitanic_train = titanic_train.drop(['SibSp','Parch'], axis=1)\ntitanic_test = titanic_test.drop(['SibSp','Parch'], axis=1)","d74f0788":"# Plot\nfig, (axis1,axis2) = plt.subplots(1,2,sharex=True,figsize=(10,5))\n\n# sns.factorplot('Family',data=titanic_train,kind='count',ax=axis1)\nsns.countplot(x='Family', data=titanic_train, order=[1,0], ax=axis1)\n\n# average of survived for those who had\/didn't have any family member\nfamily_perc = titanic_train[[\"Family\", \"Survived\"]].groupby(['Family'],as_index=False).mean()\nsns.barplot(x='Family', y='Survived', data=family_perc, order=[1,0], ax=axis2)\n\naxis1.set_xticklabels([\"With Family\",\"Alone\"], rotation=0)","8316c12d":"# As we see, children(age < 16) on aboard seem to have a high chances for Survival.\n# So, we can classify passengers as males, females, and child\n\ndef get_person(passenger):\n    age,sex = passenger\n    return 'child' if age < 16 else sex\n    \ntitanic_train['Person'] = titanic_train[['Age','Sex']].apply(get_person,axis=1)\ntitanic_test['Person'] = titanic_test[['Age','Sex']].apply(get_person,axis=1)\n\n# No need to use Sex column since we created Person column\ntitanic_train.drop(['Sex'],axis=1,inplace=True)\ntitanic_test.drop(['Sex'],axis=1,inplace=True)","1d3cdabe":"# Create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers\nperson_dummies_titanic_train  = pd.get_dummies(titanic_train['Person'])\nperson_dummies_titanic_train.columns = ['Child','Female','Male']\nperson_dummies_titanic_train.drop(['Male'], axis=1, inplace=True)\n\nperson_dummies_titanic_test  = pd.get_dummies(titanic_test['Person'])\nperson_dummies_titanic_test.columns = ['Child','Female','Male']\nperson_dummies_titanic_test.drop(['Male'], axis=1, inplace=True)\n\ntitanic_train = titanic_train.join(person_dummies_titanic_train)\ntitanic_test = titanic_test.join(person_dummies_titanic_test)\n\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\n\n# sns.factorplot('Person',data=titanic_train,kind='count',ax=axis1)\nsns.countplot(x='Person', data=titanic_train, ax=axis1)\n\n# average of survived for each Person(male, female, or child)\nperson_perc = titanic_train[[\"Person\", \"Survived\"]].groupby(['Person'],as_index=False).mean()\nsns.barplot(x='Person', y='Survived', data=person_perc, ax=axis2, order=['male','female','child'])","2d42a7dc":"titanic_train.drop(['Person'],axis=1,inplace=True)\ntitanic_test.drop(['Person'],axis=1,inplace=True)","3bab5e34":"# sns.factorplot('Pclass',data=titanic_train,kind='count',order=[1,2,3])\nsns.factorplot('Pclass','Survived',order=[1,2,3], data=titanic_train, size=5)\n\n# create dummy variables for Pclass column, & drop 3rd class as it has the lowest average of survived passengers\npclass_dummies_titanic_train  = pd.get_dummies(titanic_train['Pclass'])\npclass_dummies_titanic_train.columns = ['Class_1','Class_2','Class_3']\npclass_dummies_titanic_train.drop(['Class_3'], axis=1, inplace=True)\n\npclass_dummies_titanic_test  = pd.get_dummies(titanic_test['Pclass'])\npclass_dummies_titanic_test.columns = ['Class_1','Class_2','Class_3']\npclass_dummies_titanic_test.drop(['Class_3'], axis=1, inplace=True)\n\ntitanic_train = titanic_train.join(pclass_dummies_titanic_train)\ntitanic_test = titanic_test.join(pclass_dummies_titanic_test)","58017b85":"titanic_train.drop(['Pclass'],axis=1,inplace=True)\ntitanic_test.drop(['Pclass'],axis=1,inplace=True)","b648daca":"titanic_train.head()","18edc30a":"titanic_test.head()","34bd32bf":"# Descriptive statistics for each column\ntitanic_train.describe()","96ea3935":"titanic_test.describe()","57e73fc4":"X_train = titanic_train.drop(\"Survived\",axis=1)\nY_train = titanic_train[\"Survived\"]\nX_test  = titanic_test.drop(\"PassengerId\",axis=1).copy()","bca134f7":"logreg = LogisticRegression()\n\nlogreg.fit(X_train, Y_train)\n\nY_pred = logreg.predict(X_test)\n\nacc_log = logreg.score(X_train, Y_train)\n\nacc_log","abaac03a":"# Support Vector Machines\n\nsvc = SVC()\n\nsvc.fit(X_train, Y_train)\n\nY_pred = svc.predict(X_test)\n\nacc_svc = svc.score(X_train, Y_train)\n\nacc_svc","bde61e0f":"# Random Forests\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\n\nrandom_forest.fit(X_train, Y_train)\n\nY_pred = random_forest.predict(X_test)\n\nacc_random_forest = random_forest.score(X_train, Y_train)\n\nacc_random_forest","a0d6dd8a":"# Get Correlation Coefficient for each feature using Logistic Regression\ncoeff_df = pd.DataFrame(titanic_train.columns.delete(0))\ncoeff_df.columns = ['Features']\ncoeff_df[\"Coefficient Estimate\"] = pd.Series(logreg.coef_[0])\n\n# preview\ncoeff_df","7d36e1b3":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines',\n              'Random Forest'],\n    'Score': [acc_log, acc_svc,\n              acc_random_forest]})\nmodels.sort_values(by='Score', ascending=False)","b2871d6b":"submission = pd.DataFrame({\n        \"PassengerId\": titanic_test[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('titanic_submission1.csv', index=False)","9538b14d":"#### Let's preview top 5 and bottom 5 records from training dataset","85dfb400":"## Project WorkFlow","be688379":"## Define Training and Testing datasets","146eb695":"### Data Analysis\nData pre-processing is one of the most important steps in machine learning. It is the most important step that helps in building machine learning models more accurately. In machine learning, there is an 80\/20 rule. Every data scientist should spend 80% time for data pre-processing and 20% time to actually perform the analysis.\n","5f733288":"## Introduction of ML:\nWhat is Machine Learning (ML)?\nMachine Learning (ML) is the science of getting machine to act without being explicitly programmed.\n\nMachine learning is a type of artificial intelligence (AI) that allows software applications to learn from the data and become more accurate in predicting outcomes without human intervention.\n\nMachine Learning is a subset of artificial intelligence (AI) which focuses mainly on machine learning from their experience and making predictions based on its experience.\n\nMachine Learning is an application of artificial intelligence (AI) which provides systems the ability to automatically learn and improve from experience without being explicitly programmed.","16bc0360":"### Problem Feature\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. That's why the name DieTanic. This is a very unforgetable disaster that no one in the world can forget.\n\nIt took about $7.5 million to build the Titanic and it sunk under the ocean due to collision. The Titanic Dataset is a very good dataset for begineers to start a journey in data science and participate in competitions in Kaggle.","d53ca42d":"## Age","2378c620":"## Import all required or necessary libraries","7e5bfa3b":"### Import or Load all of your data from the data folder","7869fecf":"## Family","b4fd169f":"### Know how to import your data?\n\nFind what you have in your data folder?","af830724":"### Data Pre-Processing & Data Cleaning","edc6a460":"## Training and Testing the Models\n1. Logistic Regression\n2. SVM (Support Vetor Machine)\n3. Random Forest","4ae39637":"I hope this kernal is useful to you to learn exploratory data analysis and classification problem.\n\nIf find this notebook help you to learn, Please Upvote.\n\nThank You!!","15cad150":"Let's see, how to check missed data?","a4b0f235":"## About Titanic Problem\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.\n\nIn this challenge, we ask you to complete the analysis of what sorts of people were likely to survive. In particular, we ask you to apply the tools of machine learning to predict which passengers survived the tragedy.","474d317e":"### Variables:\n1. Age : Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n2. Sibsp : The dataset defines family relations in this way...\n\n    a. Sibling = brother, sister, stepbrother, stepsister\n\n    b. Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n3. Parch: The dataset defines family relations in this way...\n\n    a. Parent = mother, father\n\n    b. Child = daughter, son, stepdaughter, stepson\n\n    c. Some children travelled only with a nanny, therefore parch=0 for them.\n\n4. Pclass : A proxy for socio-economic status (SES).\n    1st = Upper\n    2nd = Middle\n    3rd = Lower\n\n5. Embarked : Nominal datatype\n\n6. Name: Nominal datatype . It could be used in feature engineering to derive the gender from title.\n\n7. Sex: Nominal datatype\n\n8. Ticket: That have no impact on the outcome variable. Thus, they will be excluded from analysis\n\n9. Cabin: It'a a nominal datatype that can be used in feature engineering\n\n10. Fare: Indicating the fare\n\n11. PassengerID: have no impact on the outcome variable. Thus, it will be excluded from analysis\n\n12. Survival: dependent variable , 0 or 1\n","86303a3e":"### 1. Logistic Regression\n","77d011b5":"### Problem Definition:\nI think one of the important things when you start a new machine learning project is Defining your problem. that means you should understand business problem.( Problem Formalization).","29a5518b":"# Learn_ML_from_Titanic_Disaster \nExploratory Data Analysis (EDA) and Machine Learning to predict the survival of Titanic Passengers\n\n![](https:\/\/miro.medium.com\/max\/1024\/0*KfHijq1bO1nDV5Dl.jpg)\n\n\n\n\n\n\n\nYou are going to embark on your first Exploratory Data Analysis (EDA) and Machine Learning to predict the survival of Titanic Passengers. This is the genesis challenge for most onboarding data scientists and will set you up for success. I hope this article inspires you. All aboard!!!","babb01c9":"#### Let's see the size or shpae of your data","5b62c268":"#### Librarires using for Machine Learning Algorithm","872719c7":"![](https:\/\/miro.medium.com\/max\/1000\/0*w5x4Af4EEQPvD7La)","8f69ffca":"## Fare","52db451a":"## Sex","d131afe3":"## 2. Support Vector Machine (SVM)","2d2c3f30":"## 3. Random Forest","b2377470":"## Exploratory Data Analysis","1d9a93be":"## Evaluation","e56dbca5":"# PClass","45dc3f5a":"#### Let's preview top 5 and bottom 5 records form test dataset","b5633b5e":"## Submission","7a6a55fd":"### Embarked","ad8651ee":"### Objective:\nAs a data scientist, it's your job to predict if a passenger survived the sinking of the Titanic or not. For each PassengerId in the test set, you must predict a 0 or 1 value for the Survived variable.(binary classification)","daad86c1":"Above mentioned information shows some missing values present in both training and test datasets","3f8c89a7":"1. Introduction of ML\n2. About Titanic Problem\n3. Project Work Flow\n    - Problem Defintion\n        - Problme Feature\n        - Variables\n        - Objective\n        - Input & Output\n    - Loading Packages or Import Libraries\n    - Gathering Data or Data Collection\n    - Exploratory Data Analysis(EDA)\n        - Data Analysis\n        - Data Pre-processing\n        - Data Wraggling   \n    - Training and Testing the model\n    - Evaluation\n    - Submission","ee50f187":"## Gathering Data or Data Collection"}}