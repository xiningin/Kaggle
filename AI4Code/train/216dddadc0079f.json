{"cell_type":{"b9dd5b6a":"code","edf40a42":"code","3ca182c4":"code","8b74fe07":"code","e9f98278":"code","e8ae2c31":"code","adcc6dc2":"code","aa929ec7":"code","603b5822":"code","7cb1be34":"code","2df61bb1":"code","7256884c":"code","ade0a7d8":"code","bc686a03":"code","6551e1fa":"code","1a455546":"code","d9580de6":"code","f98d9a5a":"code","d355fa21":"code","fa9e0b59":"code","adeac236":"code","c72e9302":"code","5dd8c431":"code","b00d0e2f":"code","8eb6b8ef":"code","e3c49fa6":"code","b5accc21":"code","7d5aa036":"code","c0ad05ce":"code","eaa7b835":"code","2d25e22b":"code","7e5e8798":"code","ba33ec31":"code","9a5ce515":"code","f6b5b887":"code","2a07e4a9":"code","79d783d2":"code","b555af8e":"code","8c01f1ba":"code","0b5c7e69":"code","9af63b53":"code","62c5fc45":"markdown","548c7804":"markdown","49abdb59":"markdown","0eafb78e":"markdown","afd6a72f":"markdown","27c510fd":"markdown","f7cd7bc5":"markdown","8270f5fc":"markdown","a249bca3":"markdown","a32e79f7":"markdown","553f0e5d":"markdown","90cbe679":"markdown","a1f6c492":"markdown","6a67a900":"markdown","63561a95":"markdown","4e97cffb":"markdown","b3e73db0":"markdown","44cfcccc":"markdown","cb217782":"markdown","701c4812":"markdown","85dd9690":"markdown"},"source":{"b9dd5b6a":"pip install openpyxl","edf40a42":"pip install git+https:\/\/github.com\/Nordant\/plantstat.git#egg=plantstat","3ca182c4":"# Basic packages\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import *\n\n# ignoring warnings\nimport warnings\nwarnings.simplefilter(\"ignore\")","8b74fe07":"import plantstat\n\nprint('Description:', plantstat.__description__)\nprint('Version:', plantstat.__version__)\nprint('Created by', plantstat.__author__)","e9f98278":"from plantstat import Variable_Analyzer","e8ae2c31":"# Data example\ndata = np.array([[54, 27, 43, 30, 29, 23, 71, 68, 64, 66, \n                  64, 70, 49, 49, 55, 45, 48, 49, 38, 42, 35],\n                [53, 43, 55, 63, 82, 79, 70, 57, 60, 43, \n                 49, 65, 54],\n                [53, 43, 46, 74, 57, 66, 72, 38, 45, 63, \n                 56, 58, 57, 39, 35, 64, 45, 52, 50, 36, 41],\n                [36, 45, 23, 83, 70, 82, 77, 41, 37, 48, \n                 55, 52, 22],\n                [35, 42, 49, 74, 83, 73, 68, 60, 45, 70, \n                 52, 72, 59, 48, 62, 72, 62, 38, 59, 64, 71]])\n\nlabels = ['Control', 'Var1', 'Var2', 'Var3', 'Var4']","adcc6dc2":"# Define Analyzer\na = Variable_Analyzer(data, labels)","aa929ec7":"# All functions\nprint(a.stat(np.mean))\nprint(a.stat(iqr))","603b5822":"# Variables' lengths\nprint('Length:', a.var_len())\n\n# Return data labels (variables' names)\nprint('Labels:', a.labels)\n\n# Return outliers and data without outliers (default parameters: central_measure = np.median, iqr_n = 1.5)\na.outliers()","7cb1be34":"print(a.cleaned_data)","2df61bb1":"# Return boxplots for all variables (with saving in local directory)\na.boxplot(save = True)","7256884c":"# Data Frame with basic statistics (default parameters: p_value = True, save = False, f_format = 'excel')\na.basic_stats(save = True)","ade0a7d8":"# Data Frame with statistics tests (default parameters: p_value = True, save = False, f_format = 'excel')\na.var_compare(save = True)","bc686a03":"# Pairs of variables\na.get_pairs(indices = True)","6551e1fa":"a.get_pairs()","1a455546":"# Correlation matrix (default parameters: method = 'pearson', heatmap = False, save = False, f_format = 'excel')\na.corrs(method = 'pearson', heatmap = True, save = True)","d9580de6":"# QQplots for all variables\na.QQplot(save = True)","f98d9a5a":"# Pairplot\na.pair_plot(save = True)","d355fa21":"from plantstat import AutoML_Classifier\n\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split","fa9e0b59":"# Data example\niris = datasets.load_iris()\nX = pd.DataFrame(iris.data)\ny = iris.target\nclass_names = iris.target_names\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, \n                                                    random_state = 0)","adeac236":"from random import sample\nfor idx, _ in enumerate(X_train.columns):\n    X_train.iloc[sample(range(0, len(X_train)), 5), idx] = np.NaN\n\nX_train.isnull().sum()","c72e9302":"# Model\nmodel = AutoML_Classifier(n_iter = 40)\nmodel.fit(X_train, y_train)\n\n### Model detailed information\n# model.cv_results_\n# model.best_estimator_\n# model.best_pipeline","5dd8c431":"# Prediction (default parameters: save = False, f_format = 'excel') and classification report (default parameters: labels = None, cmap = 'inferno', save = False)\nprint(model.predict(X_test, save = True))\nprint(model.predict_proba(X_test, save = True, f_format = 'csv')[:5])\nmodel.classification_report(X_test, y_test, labels = class_names, cmap = 'cividis', save = True)","b00d0e2f":"# AutoML model without some algorithms\nparams = {'scoring_func': 'balanced_accuracy', \n          'n_iter': 40, \n          'random_state': 0, \n          'cv': 5,\n          'LogisticRegression': True, \n          'KNN': True,\n          'DecisionTree': True, \n          'RandomForest': True,\n          'LinearSVC': True, \n          'GradientBoosting': False,\n          'XGB': False}\n\nmodel = AutoML_Classifier(**params)\nmodel.fit(X_train, y_train)\nmodel.classification_report(X_test, y_test, labels = class_names)","8eb6b8ef":"from plantstat import AutoML_Regressor","e3c49fa6":"# Data example\nX, y = datasets.load_boston(return_X_y = True)\nX = pd.DataFrame(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, \n                                                    random_state = 0)","b5accc21":"from random import sample\nfor idx, _ in enumerate(X_train.columns):\n    X_train.iloc[sample(range(0, len(X_train)), 5), idx] = np.NaN\n\nX_train.isnull().sum()","7d5aa036":"# Model\nmodel = AutoML_Regressor(n_iter = 40)\nmodel.fit(X_train, y_train)\n\n# Model detailed information\n\n# model.cv_results_\n# model.best_estimator_\n# model.best_pipeline","c0ad05ce":"# Predcition and report (default parameters: metric = mean_absolute_error, save = False)\nprint(model.predict(X_test, save = True)[:10])\nmodel.prediction_report(X_test, y_test, save = True)","eaa7b835":"# AutoML model without some algorithms\nparams = {'scoring_func': 'neg_mean_squared_error', \n          'n_iter': 40, \n          'random_state': 0, \n          'cv': 5,\n          'LinearRegression': True, \n          'Lasso': True,\n          'Ridge': True,\n          'ElasticNet': True,\n          'RandomForest': True,\n          'SVR': True, \n          'GradientBoosting': False,\n          'XGB': False}\n\nmodel = AutoML_Regressor(**params)\nmodel.fit(X_train, y_train)\nmodel.prediction_report(X_test, y_test)","2d25e22b":"from plantstat import Clusterer\nfrom plantstat.data_generators import ClusterData","7e5e8798":"# Create syntetic data with 5 features, 1000 samples and 4 clusters\ndata_gen = ClusterData(n_features = 5, n_samples = 1000, cluster_std = 1.2,\n                       centers_range = (4, 5), random_state = 0)\nX = pd.DataFrame(data_gen.generate())\nX","ba33ec31":"# Create K-means model for clustering (the model includes PCA with 2 components)\nkmeans = Clusterer(is_pca = True, clusterer = 'kmeans')\nkmeans.fit(X, save = True)","9a5ce515":"# Score values for various number of clusters\nkmeans.scores","f6b5b887":"# Preprocessed data (after scaling and PCA)\nkmeans.X","2a07e4a9":"# Prediction (basis on fit results)\npreds = kmeans.predict(k = 4, save = True)\npreds[:10]","79d783d2":"# The real number of labels\nprint('Unique labels: %i' %len(np.unique(data_gen.labels)))","b555af8e":"# Create DBSCAN model for clustering (the model includes PCA with 2 components)\ndbscan = Clusterer(is_pca = True, clusterer = 'DBSCAN')\ndbscan.fit(X, save = True)","8c01f1ba":"# Score values for various eps values\ndbscan.scores","0b5c7e69":"# Preprocessed data (after scaling and PCA)\ndbscan.X","9af63b53":"# Prediction (basis on fit results)\npreds = dbscan.predict(eps = 0.36, save = True)\npreds[:10]","62c5fc45":"## AutoML_Classifier","548c7804":"Also, you analyzer can return some usefull information, such as length of variables, labels, data outliers...","49abdb59":"## Clusterer (K-means)","0eafb78e":"<h1 style='color:white; background:#089e38; border:0'><center>PlantStat package: quick overview of possibilities<\/center><\/h1>\n\n![](https:\/\/storage.googleapis.com\/kaggle-datasets-images\/1278174\/2130058\/537706462c05e8b7637a037546c0bbc3\/dataset-cover.jpeg?t=2021-04-17-18-16-14)\n\nA package with a set of functions for fast and convenient statistical processing of experimental data. Also includes simple AutoML algorithms for classification and regression.\n\nThe package is based on such basic packages as:\n\n- [numpy](https:\/\/numpy.org\/)\n- [pandas](https:\/\/pandas.pydata.org\/)\n- [matplotlib](https:\/\/matplotlib.org\/)\n- [scikit-learn](https:\/\/scikit-learn.org\/stable\/)\n- [scipy](https:\/\/docs.scipy.org\/doc\/scipy\/reference\/index.html)\n- [statsmodels](https:\/\/www.statsmodels.org\/stable\/index.html)\n\n## Installing from the source:\n`pip install git+https:\/\/github.com\/Nordant\/plantstat.git#egg=plantstat`\n\n**The package is under development**, and therefore its functionality is still very limited. Bugs and errors are also possible since the package functions were tested on a small amount of real data. **The number of functions will be expanded, and possible problems will be fixed during the next updates**. \n\nThis package was written by a non-professional developer and was originally intended for the needs of a narrow circle of specialists in the field of plant sciences (mainly plant physiology and biochemistry). \n\nMany approaches and methods used in it are still far from ideal. The author publishes the package on GitHub and Kaggle to share with the community and improve with feedbacks.\n\nThe author is also grateful to **Gianluca Malato** for an excellent post on [AutoML Algorithms](https:\/\/towardsdatascience.com\/how-to-build-your-own-automl-library-in-python-from-scratch-995940f3fa71).\n\n# Package on [GitHub](https:\/\/github.com\/Nordant\/plantstat) and [Kaggle](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/plantstat-package-statistics-and-automl)! Take a look if you are interested!","afd6a72f":"After training, you can predict classes or probabilities, as well as look at the classification report.","27c510fd":"<h1 style='color:white; background:#089e38; border:0'><center>Variable_Analyzer Features<\/center><\/h1>\n\nThe object takes two arguments: np.array with lists, each containing one variable, and a list with variable names.","f7cd7bc5":"The analyzer can also visualize a number of dependencies between variables in the format of a correlation matrix or pairplot. You can also look at the Q-Q plot of each variable.","8270f5fc":"You can use any statistical function from numpy or scipy packages for calculation basic statistics. The 'stat' function greatly expands the statistical capabilities of a package.","a249bca3":"You can visualize all variables in boxplot format. Also, **almost all functions in the package have 'save' parameter (default = True) with which the function outputs are saved in a local directory.** You need the openpyxl package.","a32e79f7":"## AutoML_Regressor","553f0e5d":"If you separately need all possible pairs of variables to be compared, you can get them using a special function in the format of indices or names.","90cbe679":"<h1 style='color:white; background:#089e38; border:0'><center>WORK IN PROGRESS...<\/center><\/h1>\n\n# Package on [GitHub](https:\/\/github.com\/Nordant\/plantstat) and [Kaggle](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/plantstat-package-statistics-and-automl)! Take a look if you are interested!","a1f6c492":"The algorithm accepts a number of parameters as input, such as the scoring function, the number of iterations to find the algorithm, the random state, and the number of cross-validation repetitions.","6a67a900":"Let's define some random values in train data as NaN.","63561a95":"<h1 style='color:white; background:#089e38; border:0'><center>AutoML<\/center><\/h1>\n\nThe package presents simple AutoML algorithms for classification and regression, which receive raw data as input and automatically replace missing values, normalize data (if necessary), select the optimal algorithm, and train this algorithm for prediction. So far, the algorithm has not been optimized to perfection and maximum flexibility, so inaccuracies and problems with very dirty data can occur.\n\nTo demonstrate the capabilities of the algorithm, we use the well-known datasets Iris (for classification) and Boston house-prices (for regression).\n\n### **To speed up computations, it is recommended to use a GPU!**","4e97cffb":"...or cleanned data (without outliers)","b3e73db0":"Let's define some random values in train data as NaN.","44cfcccc":"Also, **you can disable the use of certain machine learning algorithms by the classifier by setting the corresponding parameters to False**.","cb217782":"### Import plantstat package and check some information","701c4812":"The next two functions return dataframes with basic statistics and statistical tests (**Shapiro test,normaltest, Bartlet test, T-test, Wilcoxon test, Kruskal-Wallis H-test, Kolmogorov-Smirnov test, Mann-Whitney rank test**).","85dd9690":"## Clusterer (DBSCAN)"}}