{"cell_type":{"38a02b3c":"code","40de37ef":"code","c7c93e23":"code","77223ebf":"code","e1c02491":"code","6a335659":"code","a3a7ccfb":"code","37707114":"code","1c125bac":"code","e161f17d":"code","6019e5cb":"code","e7a6d4f6":"code","cc7657ac":"code","7844449f":"code","a579b29d":"code","feaf2a74":"code","fa6f576f":"code","cac874f0":"code","468a66d7":"code","2c38ebe7":"code","e7d184f5":"code","957cd655":"code","ce460eec":"code","c69f7629":"code","cf4e6a7a":"code","18277a51":"code","4d6fe2ab":"code","e3f043db":"code","9e2c9cc1":"code","6c2a8013":"code","03151f24":"code","240042dc":"code","0a7773fd":"code","5021671c":"code","0ccc8192":"code","e1c22392":"code","0dd93e11":"code","b6a5b9fb":"code","796da229":"code","cf05746f":"code","d9ee9dc3":"code","88e507e2":"code","8213adcf":"code","ab2eea0b":"code","733adc02":"code","6aee7621":"code","6ddac3b3":"code","8b6fa1cd":"code","35c19204":"code","e9355195":"code","d0c94be7":"code","9e658a78":"code","3b88ee32":"code","77bb0f96":"code","106b10ea":"code","33f2d1c3":"code","4cd8b0c1":"code","4ca415b4":"code","c5b5e2da":"code","f17747fb":"code","4c5bbd1c":"code","ce5c64d7":"code","8a1c17d4":"code","61cd032c":"code","40003263":"code","13e541f9":"code","093661cc":"code","b89dc67b":"code","b77d369f":"code","d05b0f49":"code","e9595585":"code","933d3ad5":"code","e6f6fa6e":"code","5e9d1889":"code","56a8c877":"code","ed9bc583":"code","1bd55518":"code","31c5fc56":"markdown","6710d528":"markdown","53318fd2":"markdown","1f6e4090":"markdown","26051e9e":"markdown","7f8942d4":"markdown","aba7ea87":"markdown","982d760f":"markdown","acbb57f8":"markdown","574e2b80":"markdown","63b528fc":"markdown","268a0701":"markdown","8a10ff70":"markdown","0957f154":"markdown","43b494dc":"markdown","761c44be":"markdown","c6206148":"markdown","a10603f4":"markdown","1f01c1c1":"markdown","f90e85c2":"markdown","90fc0c09":"markdown","3b8b3d5b":"markdown","9e44a6a0":"markdown","f6f40daf":"markdown","1ab5a154":"markdown","8257c10f":"markdown","25d5caa6":"markdown","b46c6b93":"markdown","5efb432f":"markdown","8b997305":"markdown","3380ab5c":"markdown","ded4a5c3":"markdown","7e2fe5e1":"markdown","03fd0c1b":"markdown","b3c070be":"markdown","cb64423b":"markdown","39ac759f":"markdown","22b23fc7":"markdown","eef6e61c":"markdown","a784263c":"markdown","4fb38e82":"markdown","eb7c7a3b":"markdown","368a6619":"markdown","2edf71df":"markdown","a17a3f31":"markdown","c28427e9":"markdown"},"source":{"38a02b3c":"# Basic Library\nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nimport scipy.stats as stats\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# sklearn utility\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics   \nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\n## XGBoost\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\n\n### LightGBM\nfrom lightgbm import LGBMClassifier\nimport lightgbm as lgb\n\n### CatBoost\nfrom catboost import CatBoostClassifier\nimport catboost as catboost\n\n## sklearn ensembles \nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier","40de37ef":"# Titanic Dataset\ntitanic_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntitanic_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ndataset = \"titanic\"\nIdCol = 'PassengerId'\ntargetCol = 'Survived'\ntitanic_train.head()","c7c93e23":"titanic_train.columns","77223ebf":"titanic_train.dtypes","e1c02491":"titanic_train.nunique()","6a335659":"int_or_float = titanic_train.dtypes[titanic_train.dtypes.isin(['int64', 'float64'])].index\nprint(\"Int or Flaot Columns : \", list(int_or_float))\nnum_cols = ['Age', 'SibSp', 'Parch', \"Fare\"]\nprint(\"Num Cols : \", num_cols)\ncat_cols = ['Pclass', 'Sex', 'Embarked']\nprint(\"Cat Cols : \", cat_cols)","a3a7ccfb":"train_len = len(titanic_train)\ncombined =  pd.concat(objs=[titanic_train, titanic_test], axis=0).reset_index(drop=True)\n#combined.tail()","37707114":"def missing_values_details(df):\n    total = df.isnull().sum()\n    \n    missing_df = pd.DataFrame({'count_missing': total}).reset_index().rename(columns={'index':'column_name'})\n    missing_df['percent_missing'] = missing_df['count_missing']\/len(df)\n    missing_df = missing_df.sort_values(by='count_missing', ascending=False)\n    missing_df = missing_df[missing_df['count_missing']!=0]\n    print('Info : {} out of {} columns have mising values'.format(len(missing_df), len(df.columns)))\n    missing_90 = missing_df[missing_df['percent_missing']>0.9]\n    missing_80 = missing_df[missing_df['percent_missing']>0.8]\n    missing_70 = missing_df[missing_df['percent_missing']>0.7]\n    print(\"Info : {} columns have more that 90% missing values\".format(len(missing_90)))\n    print(\"Info : {} columns have more that 80% missing values\".format(len(missing_80)))\n    print(\"Info : {} columns have more that 70% missing values\".format(len(missing_70)))\n    \n    return missing_df","1c125bac":"missing_values_details(titanic_train)","e161f17d":"def check_class_balance(df, target_col):\n    counts = df[target_col].value_counts()\n    class_df = pd.DataFrame(counts).reset_index().rename(columns={target_col:'counts', 'index':'class'})\n    class_df.plot.bar(x='class', y='counts')\n    print('Info : There are {} classes in the target column'.format(len(class_df)))\n    max_class = class_df['counts'].max() \n    min_class = class_df['counts'].min()\n    max_diff = max_class - min_class\n    print(\"Info : Maximum difference between 2 classes is {} observations that is {} times w.r.t. minimum class\".format(max_diff, (max_diff\/min_class)))\n    return class_df","6019e5cb":"check_class_balance(titanic_train, 'Survived')","e7a6d4f6":"def detect_outliers(df,n,features):\n    outlier_indices = []\n    # iterate over features(columns)\n    for col in features:\n        Q1 = np.percentile(df[col], 25)\n        Q3 = np.percentile(df[col],75)\n        IQR = Q3 - Q1\n        \n        outlier_step = 1.5 * IQR\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        outlier_indices.extend(outlier_list_col)\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from num_cols\noutliers_rows = detect_outliers(titanic_train,2,num_cols)\nprint(len(outliers_rows))\n# Drop outliers\ntitanic_train = titanic_train.drop(outliers_rows, axis = 0).reset_index(drop=True)","cc7657ac":"def describe_num_col(train, col):\n    #### This function provides detailed comparison of a numerical varible\n    ### missing value\n    count_train = train[col].isnull().sum()\n    #print(\"######    Variable Name : {}    ######\".format(col))\n    \n    #### Skewness and Kurtosis\n    train_k = stats.kurtosis(train[col].dropna(), bias=False)\n    \n    train_s = stats.skew(train[col].dropna(), bias=False)\n    \n    #### Outliers\n    \n    def count_outliers(df, col):\n        mean_d = np.mean(df[col])\n        std_d = np.std(df[col])\n        \n        scaled = (df[col]-mean_d)\/std_d\n        outliers = abs(scaled) > 3\n        if len(outliers.value_counts()) > 1:\n            return outliers.value_counts()[1]\n        else:\n            return 0   \n    \n    train_o = count_outliers(train, col)\n        \n    summ_df = pd.DataFrame({'info':['missing_count', 'missing_percent', 'skewness', 'kurtosis', 'outlier_count', 'outlier_percent'],\n                           'train_set':[count_train, (count_train\/len(train))*100, train_s, train_k, train_o, (train_o\/len(train))*100]})\n    \n#     print(\"######    Summary Data\")\n#     display(summ_df)\n    \n    #print(\"######    Distribution and Outliers comparision plots\")\n    \n    fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(10, 6))\n    \n    plot10 = sns.distplot(train[train['Survived']==0][col],ax=ax1, label='Not Survived')\n    sns.distplot(train[train['Survived']==1][col],ax=ax1,color='red', label='Survived')\n    plot10.axes.legend()\n    ax1.set_title('Distribution of {name}'.format(name=col))\n    \n    sns.boxplot(x='Survived',y=col,data=train,ax=ax2)\n    #plt.xticks(ticks=[0,1],labels=['Non-Diabetes','Diabetes'])\n    ax2.set_xlabel('Category') \n    ax2.set_title('Boxplot of {name}'.format(name=col))\n    \n    \n    fig.show()    \n    \n    return","7844449f":"for col in num_cols:\n    describe_num_col(titanic_train, col)","a579b29d":"### The column Fare is continuous and it is right skewed\n# Apply log transformation to Fare to reduce skewness distribution\ncombined[\"Fare\"] = combined[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)","feaf2a74":"def describe_cat_col(df, col):\n    ### unique values\n    count_u = df[col].nunique()\n    #print(\"Info : There are {} unique values\".format(count_u))\n    nulls = df[col].isnull().sum()\n    #print(\"Info : There are {} missing values that is {} percent\".format(nulls, nulls\/len(df)))\n    \n    ### Percent share df\n    share_df = pd.DataFrame(df[col].value_counts()).reset_index().rename(columns={'index':'class_name',col:'counts'})\n    share_df['percent_share'] = share_df['counts']\/sum(share_df['counts'])\n    share_df = share_df.sort_values(by='percent_share', ascending=False)\n    #display(share_df)\n        \n        \n    if (count_u > 3 and count_u < 10):\n        fig, ax  = plt.subplots()\n        fig.suptitle(col + ' Distribution', color = 'red')\n        explode = list((np.array(list(df[col].dropna().value_counts()))\/sum(list(df[col].dropna().value_counts())))[::-1])\n        labels = list(df[col].dropna().unique())\n        sizes = df[col].value_counts()\n        #ax.pie(sizes, explode=explode, colors=bo, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.9)\n        ax.pie(sizes,  explode=explode, startangle=60, labels=labels,autopct='%1.0f%%', pctdistance=0.9)\n        ax.add_artist(plt.Circle((0,0),0.2,fc='white'))\n        plt.show()\n    \n    else:\n        plt.figure()\n        plt.title(col + ' Distribution', color = 'red')\n        sns.barplot(x=col,y='Survived', data = df)\n        plt.show()\n        \n    return","fa6f576f":"for col in cat_cols:\n    #print(\"Column Name : {}\".format(col))\n    describe_cat_col(titanic_train, col)","cac874f0":"#### Filling the Embarked column with mode\ncombined['Embarked'] = combined['Embarked'].fillna(combined['Embarked'].value_counts().index[0])\ncombined['Embarked'].isnull().sum()","468a66d7":"combined = pd.get_dummies(combined, columns = [\"Embarked\"], prefix=\"Em\")","2c38ebe7":"# Explore Age vs Sex, Parch , Pclass and SibSP\ng = sns.factorplot(y=\"Age\",x=\"Sex\",data=combined,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Sex\",hue=\"Pclass\", data=combined,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"Parch\", data=combined,kind=\"box\")\ng = sns.factorplot(y=\"Age\",x=\"SibSp\", data=combined,kind=\"box\")","e7d184f5":"# convert Sex into categorical value 0 for male and 1 for female\ncombined[\"Sex\"] = combined[\"Sex\"].map({\"male\": 0, \"female\":1})\n\n# Filling missing value of Age \n\n## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(combined[\"Age\"][combined[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = combined[\"Age\"].median()\n    age_pred = combined[\"Age\"][((combined['SibSp'] == combined.iloc[i][\"SibSp\"]) & (combined['Parch'] == combined.iloc[i][\"Parch\"]) & (combined['Pclass'] == combined.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        combined['Age'].iloc[i] = age_pred\n    else :\n        combined['Age'].iloc[i] = age_med","957cd655":"combined['Cabin'].describe()","ce460eec":"# Replace the Cabin number by the type of cabin 'X' if not\ncombined[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in combined['Cabin'] ])","c69f7629":"combined = pd.get_dummies(combined, columns = [\"Cabin\"],prefix=\"Cabin\")","cf4e6a7a":"# Get Title from Name\ncombined_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in combined[\"Name\"]]\ncombined[\"Title\"] = pd.Series(combined_title)\ncombined[\"Title\"].head()","18277a51":"# Convert to categorical values Title \ncombined[\"Title\"] = combined[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ncombined[\"Title\"] = combined[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ncombined[\"Title\"] = combined[\"Title\"].astype(int)","4d6fe2ab":"g = sns.factorplot(x=\"Title\",y=\"Survived\",data=combined,kind=\"bar\")\ng = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\ng = g.set_ylabels(\"survival probability\")","e3f043db":"# Drop Name variable\ncombined.drop(labels = [\"Name\"], axis = 1, inplace = True)","9e2c9cc1":"## Treat Ticket by extracting the ticket prefix. When there is no prefix it returns X. \n\nTicket = []\nfor i in list(combined.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ncombined[\"Ticket\"] = Ticket\ncombined[\"Ticket\"].head()","6c2a8013":"combined = pd.get_dummies(combined, columns = [\"Ticket\"], prefix=\"T\")","03151f24":"# Create categorical values for Pclass\ncombined[\"Pclass\"] = combined[\"Pclass\"].astype(\"category\")\ncombined = pd.get_dummies(combined, columns = [\"Pclass\"],prefix=\"Pc\")","240042dc":"# Drop useless variables \ncombined.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","0a7773fd":"combined.head()","5021671c":"train = combined[:train_len]\ntest = combined[train_len:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)\ntrain[\"Survived\"] = train[\"Survived\"].astype(int)\ny = train[\"Survived\"]\ntrain = train.drop(labels = [\"Survived\"],axis = 1)\ntrain_x, val_x, train_y, val_y = train_test_split(train, y, test_size=0.2)\ntest_id = titanic_test['PassengerId']\ntest_x = test","0ccc8192":"assert len(train_x.columns) == len(test_x.columns)","e1c22392":"### Function to evaluate model performance\n\ndef evaluate_model_performnce(y_test, y_pred):\n    cm = confusion_matrix(y_test, y_pred)\n    \n    # Visualizing model performance\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, fmt='g'); #annot=True to annotate cells\n\n    # labels, title and ticks\n    ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n    ax.set_title('Confusion Matrix'); \n\n    tn, fp, fn, tp = cm.ravel()\n    #print(tn, fp, fn, tp)\n    precision = tp\/(tp+fp)\n    recall = tp\/(tp+fn)\n    f1 = 2 * (precision * recall) \/ (precision + recall)\n    accuracy = ((tp+tn)\/(tp+tn+fp+fn))*100\n    print(\"Precision : \",precision)\n    print(\"Recall : \",recall)\n    print(\"F1 Score : \",f1)\n    print(\"Validation Accuracy : \",accuracy)\n    accuracy = metrics.accuracy_score(y_test, y_pred)\n    print(\"Accuracy Score : \", accuracy)\n\n    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n    auc = metrics.auc(fpr, tpr)\n    print(\"AUC Value : \", auc)\n    \n    return accuracy, auc, f1","0dd93e11":"## Function to create the submission file\n\ndef make_submission_file(filename, probab, test_id, IdCol, targetCol, threshold=None):\n    submit = pd.DataFrame()\n    submit[IdCol] = test_id\n    submit[targetCol] = probab\n    if threshold!=None:\n        pred = [1 if x>=threshold else 0 for x in probab]\n        submit[targetCol] = pred\n    submit.to_csv(filename, index=False)\n    return submit","b6a5b9fb":"xgb_clf = XGBClassifier()\nxgb_clf.fit(train_x, train_y,eval_metric=[\"auc\", \"logloss\"],verbose=True)","796da229":"threshold = 0.4\nxgb_val_prob = xgb_clf.predict_proba(val_x)\nxgb_val_prob = pd.DataFrame(xgb_val_prob)[1]\nxgb_val_pred = [1 if x >= threshold else 0 for x in xgb_val_prob]\nxgb_acc, xgb_auc, xgb_f1 = evaluate_model_performnce(val_y, xgb_val_pred) ","cf05746f":"xgb_prob = xgb_clf.predict_proba(test_x)\nxgb_prob = pd.DataFrame(xgb_prob)[1]","d9ee9dc3":"#xgb_prob","88e507e2":"xgb_sub = make_submission_file(dataset+\"_xgb_default.csv\", xgb_prob, test_id, IdCol, targetCol, threshold=0.5)\nxgb_sub.head()","8213adcf":"lgb_clf = LGBMClassifier()\nlgb_clf.fit(train_x, train_y)","ab2eea0b":"threshold = 0.4\nlgb_val_prob = lgb_clf.predict_proba(val_x)\nlgb_val_prob = pd.DataFrame(lgb_val_prob)[1]\nlgb_val_pred = [1 if x >= threshold else 0 for x in lgb_val_prob]\nlgb_acc, lgb_auc, lgb_f1 = evaluate_model_performnce(val_y, lgb_val_pred) ","733adc02":"lgb_prob = lgb_clf.predict_proba(test_x)\nlgb_prob = pd.DataFrame(lgb_prob)[1]","6aee7621":"#lgb_prob","6ddac3b3":"lgb_sub = make_submission_file(dataset+\"_lgb_default.csv\", lgb_prob, test_id, IdCol, targetCol, threshold=0.5)\nlgb_sub.head()","8b6fa1cd":"cat_clf = CatBoostClassifier(verbose=0)\ncat_clf.fit(train_x, train_y)","35c19204":"threshold = 0.4\ncat_val_prob = cat_clf.predict_proba(val_x)\ncat_val_prob = pd.DataFrame(cat_val_prob)[1]\ncat_val_pred = [1 if x >= threshold else 0 for x in cat_val_prob]\ncat_acc, cat_auc, cat_f1 = evaluate_model_performnce(val_y, cat_val_pred) ","e9355195":"cat_prob = cat_clf.predict_proba(test_x)\ncat_prob = pd.DataFrame(cat_prob)[1]","d0c94be7":"#cat_prob","9e658a78":"cat_sub = make_submission_file(dataset+\"_cat_default.csv\", cat_prob, test_id, IdCol, targetCol, threshold=0.5)\ncat_sub.head()","3b88ee32":"ens_val_prob = 0.4*cat_val_prob + 0.3*lgb_val_prob + 0.3*xgb_val_prob","77bb0f96":"threshold = 0.4\nens_val_pred = [1 if x >= threshold else 0 for x in ens_val_prob]\nens_acc, ens_auc, ens_f1 = evaluate_model_performnce(val_y, ens_val_pred) ","106b10ea":"ens_prob = 0.4*cat_prob + 0.3*lgb_prob + 0.3*xgb_prob","33f2d1c3":"#y_prob","4cd8b0c1":"ens_sub = make_submission_file(dataset+\"_weighted_ens.csv\", ens_prob, test_id, IdCol, targetCol, threshold=0.5)\nens_sub.head()","4ca415b4":"trained_clfs = [xgb_clf, lgb_clf, cat_clf]\n\ndef ensembling_engine(trained_clfs, train_x, train_y, test_x, ensembler):\n    train_matrix = np.empty((train_x.shape[0], len(trained_clfs)))\n    for (n, clf) in enumerate(trained_clfs):\n        train_matrix[:,n] = pd.DataFrame(clf.predict_proba(train_x))[1]      #pd.DataFrame(cat_prob)[1]\n        \n    ensembler.fit(train_matrix, train_y)\n    \n    test_matrix = np.empty((test_x.shape[0], len(trained_clfs)))\n    for (n, clf) in enumerate(trained_clfs):\n        test_matrix[:,n] = pd.DataFrame(clf.predict_proba(test_x))[1]   \n        \n    ens_prob = ensembler.predict_proba(test_matrix)\n    return ens_prob","c5b5e2da":"ensembler = LogisticRegression()","f17747fb":"lr_ens_val_prob = ensembling_engine(trained_clfs, train_x, train_y, val_x, ensembler)\nlr_ens_val_prob = pd.DataFrame(lr_ens_val_prob)[1]\nthreshold = 0.4\nlr_ens_val_pred = [1 if x >= threshold else 0 for x in lr_ens_val_prob]\nle_ens_acc, lr_ens_auc, lr_ens_f1 = evaluate_model_performnce(val_y, lr_ens_val_pred) ","4c5bbd1c":"lr_ens_prob = ensembling_engine(trained_clfs, train_x, train_y, test_x, ensembler)\nlr_ens_prob = pd.DataFrame(lr_ens_prob)[1]\n#lr_ens_prob\nlr_ens_sub = make_submission_file(dataset+\"lr_ens.csv\", lr_ens_prob, test_id, IdCol, targetCol, threshold=0.5)\nlr_ens_sub.head()","ce5c64d7":"ensembler = RandomForestClassifier()","8a1c17d4":"rfc_ens_val_prob = ensembling_engine(trained_clfs, train_x, train_y, val_x, ensembler)\nrfc_ens_val_prob = pd.DataFrame(rfc_ens_val_prob)[1]\nthreshold = 0.4\nrfc_ens_val_pred = [1 if x >= threshold else 0 for x in rfc_ens_val_prob]\nrfc_ens_acc, rfc_ens_auc, rfc_ens_f1 = evaluate_model_performnce(val_y, rfc_ens_val_pred) ","61cd032c":"rfc_ens_prob = ensembling_engine(trained_clfs, train_x, train_y, test_x, ensembler)\nrfc_ens_prob = pd.DataFrame(rfc_ens_prob)[1]\n#rfc_ens_prob\nrfc_ens_sub = make_submission_file(dataset+\"rfc_ens.csv\", rfc_ens_prob, test_id, IdCol, targetCol, threshold=0.5)\nrfc_ens_sub.head()","40003263":"ensembler = GradientBoostingClassifier()","13e541f9":"gbc_ens_val_prob = ensembling_engine(trained_clfs, train_x, train_y, val_x, ensembler)\ngbc_ens_val_prob = pd.DataFrame(gbc_ens_val_prob)[1]\nthreshold = 0.4\ngbc_ens_val_pred = [1 if x >= threshold else 0 for x in gbc_ens_val_prob]\ngbc_ens_acc, gbc_ens_auc, gbc_ens_f1 = evaluate_model_performnce(val_y, gbc_ens_val_pred) ","093661cc":"gbc_ens_prob = ensembling_engine(trained_clfs, train_x, train_y, test_x, ensembler)\ngbc_ens_prob = pd.DataFrame(gbc_ens_prob)[1]\n#gbc_ens_prob\ngbc_ens_sub = make_submission_file(\"gbc_ens.csv\", gbc_ens_prob, test_id, IdCol, targetCol, threshold=0.5)\ngbc_ens_sub.head()","b89dc67b":"from sklearn.ensemble import VotingClassifier","b77d369f":"v_xgb = XGBClassifier()\nv_lgb = LGBMClassifier()\nv_cat = CatBoostClassifier(verbose=0)\n\nv_clf = VotingClassifier(estimators=[('xgb', v_xgb), ('lgb', v_lgb), ('cat', v_cat)], voting='soft')\nv_clf.fit(train_x, train_y)","d05b0f49":"threshold = 0.4\nv_val_prob = v_clf.predict_proba(val_x)\nv_val_prob = pd.DataFrame(v_val_prob)[1]\nv_val_pred = [1 if x >= threshold else 0 for x in v_val_prob]\nv_acc, v_auc, v_f1 = evaluate_model_performnce(val_y, v_val_pred) ","e9595585":"v_prob = v_clf.predict_proba(test_x)\nv_prob = pd.DataFrame(v_prob)[1]","933d3ad5":"v_sub = make_submission_file(dataset+\"_voting_ens.csv\", v_prob, test_id, IdCol, targetCol, threshold=0.5)\nv_sub.head()","e6f6fa6e":"from sklearn.ensemble import StackingClassifier","5e9d1889":"st_xgb = XGBClassifier()\nst_lgb = LGBMClassifier()\nst_cat = CatBoostClassifier(verbose=0)\n\nst_clf = StackingClassifier(estimators=[('xgb', st_xgb), ('lgb', st_lgb), ('cat', st_cat)], \n                            final_estimator=GradientBoostingClassifier())\nst_clf.fit(train_x, train_y)","56a8c877":"threshold = 0.4\nst_val_prob = st_clf.predict_proba(val_x)\nst_val_prob = pd.DataFrame(st_val_prob)[1]\nst_val_pred = [1 if x >= threshold else 0 for x in st_val_prob]\nst_acc, st_auc, st_f1 = evaluate_model_performnce(val_y, st_val_pred) ","ed9bc583":"st_prob = st_clf.predict_proba(test_x)\nst_prob = pd.DataFrame(st_prob)[1]","1bd55518":"st_sub = make_submission_file(dataset+\"_stacking_ens.csv\", st_prob, test_id, IdCol, targetCol, threshold=0.5)\nst_sub.head()","31c5fc56":"The next two methods are used on untrained-classifier.\n\n### 3) VotingClassifier\n\nIt's a hard\/soft Voting\/Majority Rule classifier for unfitted estimators. In \u2018hard\u2019 mode, it uses predicted class labels for majority rule voting. Else if \u2018soft\u2019, predicts the class label based on the argmax of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers. It's available in sklearn.","6710d528":"3.3.2 Age\n\n[Reference](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)","53318fd2":"## Reading the Data!","1f6e4090":"Ensembling on Validation Set and evaluating it!","26051e9e":"### Some utility functions","7f8942d4":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">3. Feature Engineering<\/p>","aba7ea87":"## Finally Ensembling!\n\n> #### If you are absolute beginner to ensembling and find this notebook a bit difficult to follow. I encourage you to lokk at this thread [Gateway to Ensembling!!](https:\/\/www.kaggle.com\/getting-started\/219696)\n\nNow we'll se how we can combine the learning from individual models and make an ensemble.\n\n### 1) Weighted Ensembling Approach\n\nThis is the most simple approach for ensembling. We take weighted average of the scores of individual model that gives us the ensembled score. You can decide the weight of an individual model based on the validation score of that model or your experience. \n\nNote - We can treat these weights as hyperparameter and optimize them like we do in Logistic Regression. (w1x1 + w2x2 + ....) Right? Yes, This is the next technique.","982d760f":"Making Prediction on leaderboard dataset","acbb57f8":"### 3) CatBoost","574e2b80":"Making prediction on leaderboard dataset","63b528fc":"### 4) StackingClassifier\n\nStacked generalization consists in stacking the output of individual estimator and use a classifier to compute the final prediction. Stacking allows to use the strength of each individual estimator by using their output as input of a final estimator. So basically what we have implemented in technique 2 is similar to stacking classifier. It's readily available in sklearn.","268a0701":"## 2) Ensemblinng using a ML model\n\nLike discussed in the previous section here we train a separate model on the outcome of individual models and then using that model an ensembled model.","8a10ff70":"On Leaderboard set","0957f154":"### 3.1) Numerical columns","43b494dc":"## 2) LightGBM","761c44be":"#### These are some of the methods used for ensembling. If you found this kernel interesting consider UPVOTING it. Thanks! #Open_to_discussion.\n\n#### I have prepared another notebook on Hyper-parameter optimization that you may find insightful. Here - [Getting Started with Hyper-parameter Optimization](https:\/\/www.kaggle.com\/pashupatigupta\/getting-started-with-hyper-parameter-optimization). One highlight is that it talks about Bayesian Optimization in detail.\n\n### Happy Learning!!","c6206148":"Writitng the submission file","a10603f4":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">1. Introduction<\/p>","1f01c1c1":"### 2.3) Gradient Booster\n\nLet's try with gradient booster. We'll do ensembling on Validation Set and then evaluate it!","f90e85c2":"Ensembling on leaderboard dataset","90fc0c09":"### This sums-up the feature engineering part. We have the development data ready now.","3b8b3d5b":"Writitng the submission file","9e44a6a0":"Making prediction of leaderboard dataset","f6f40daf":"Writing submission file","1ab5a154":"Wikipedia Says - In statistics and machine learning, ensemble methods use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. \n\nIn simple terms - In ensembling we try to use the learnings from multiple algorithms to make a more efficient algorithm. For example Random Forest algorithms - It's a ensembling algorithm that uses multiple decision trees. Look at the below picture - \n\n<img src=\"https:\/\/www.kdnuggets.com\/wp-content\/uploads\/ensemble-framework-packt.jpg\">\n\n\n\n<br\/>So we try to use multiple algorithms to enhance the performance. The concept of ensembling is pretty simple. What's required is a robust technique to do this ensembling. In this Notebook we'll se what are techniques used for ensembling. For ex - \n\n- Weighted Ensembling Approach\n- VotingClassifier\n- StackingClassifier\n- Ensembling using a custom model\n- Bayesian optimal classifier.\n\nDetails in each forthcoming sections. We'll cover each step of model development. \n\n- Data Processing\n- Feature Engineering\n- Model Development\n- Ensembling\n- Evaluation\n\nLet's Begin!","8257c10f":"Age distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age.\n\nHowever, 1st class passengers are older than 2nd class passengers who are also older than 3rd class passengers.\n\nMoreover, the more a passenger has parents\/children the older he is and the more a passenger has siblings\/spouses the younger he is.","25d5caa6":"### 2.1) Missing Values","b46c6b93":"### 2.2) Check for class balance","5efb432f":"Writitng the submission file","8b997305":"### 3.3) Filling the missing values\n3.3.1 - Embarked","3380ab5c":"### 2.3) Outliers","ded4a5c3":"### 3.4) Feature Transformation\n\n3.4.1 Name -> Title","7e2fe5e1":"3.3.3 Cabin","03fd0c1b":"### 2.1) Logistic Regression\n\nLet's try with simple logistic regression. We'll do ensembling on Validation Set and then evaluate it!","b3c070be":"3.4.2 Ticket","cb64423b":"Making prediction of leaderboard dataset","39ac759f":"Writing submission file","22b23fc7":"## 1) XGBoost","eef6e61c":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">2. Data Preparation<\/p>","a784263c":"### 3.2) Catrgorical columns","4fb38e82":"### 2.2) RandomForest\n\nLet's try with random forest. We'll do ensembling on Validation Set and then evaluate it!","eb7c7a3b":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">Ensembling<\/p>","368a6619":"### Before directly jumping into ensembling. Let's first train some models individually and see how they are performing. \nWe'll train the best models -\n- XGBoost\n- LightBoost\n- CatBoost \n<br\/>\n\nAnd the ensemble them. Later we can check with other models too.\n\nNote - We'll be using default parameter's as of now.","2edf71df":"3.4.2 Pclass","a17a3f31":"## <p style=\"background-color:skyblue; font-family:newtimeroman; font-size:180%; text-align:center\">Model Development<\/p>\n\n> ### Training the base learners","c28427e9":"Making prediction of leaderboard dataset"}}