{"cell_type":{"85c16fc2":"code","0d0bbe44":"code","80b73b7a":"code","79b28aaa":"code","047965b4":"code","66bde1a5":"code","efdca363":"code","eef351ae":"markdown","5d48c061":"markdown"},"source":{"85c16fc2":"#Import the BM25 index\n!pip install rank_bm25\nimport pickle\nimport os\n!git clone https:\/\/github.com\/CoronaWhy\/CORD-19-QA \nos.chdir('CORD-19-QA')\n!wget -O bert_bioasq_final-bm25.pkl https:\/\/publicweightsdata.s3.us-east-2.amazonaws.com\/bert_bioasq_final-bm25.pkl\nfrom bm25_index import BM25Index\nos.chdir('\/kaggle\/working')\n\n#Import Transformers \n!pip install transformers\n!wget -O scibert_uncased.tar https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-research\/scibert\/huggingface_pytorch\/scibert_scivocab_uncased.tar\n!tar -xvf scibert_uncased.tar\nimport torch\nfrom transformers import BertTokenizer, BertModel\n\n#Import key libraries \nimport numpy as np \nimport pandas as pd\nimport plotly.graph_objects as go\nimport networkx as nx\nimport itertools\nfrom random import randrange\nimport math\nimport re\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import sent_tokenize,word_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import TfidfVectorizer","0d0bbe44":"#Load metadata.csv (from https:\/\/www.kaggle.com\/ajrwhite\/covid-19-thematic-tagging-with-regular-expressions\/notebook)\ndef load_metadata(metadata_file):\n    df = pd.read_csv(metadata_file,\n                 dtype={'Microsoft Academic Paper ID': str,\n                        'pubmed_id': str},\n                 low_memory=False)\n    df.doi = df.doi.fillna('').apply(doi_url)\n    #df['authors_short'] = df.authors.apply(shorten_authors)\n    #df['sorting_date'] = pd.to_datetime(df.publish_time)\n    print(f'loaded DataFrame with {len(df)} records')\n    #return df.sort_values('sorting_date', ascending=False)\n    return df\n\n# Fix DOI links\ndef doi_url(d):\n    if d.startswith('http'):\n        return d\n    elif d.startswith('doi.org'):\n        return f'http:\/\/{d}'\n    else:\n        return f'http:\/\/doi.org\/{d}'\n\nmetadata = load_metadata('..\/input\/CORD-19-research-challenge\/metadata.csv')","80b73b7a":"#Search results for a query\nthe_index = pickle.load(open('CORD-19-QA\/bert_bioasq_final-bm25.pkl', 'rb'))\nthe_search_terms = 'coronavirus bat to human transmission'\nabstracts = the_index.search(the_search_terms, 10)\ndummy_search_results = pd.merge(abstracts, metadata, left_on = ['paper_id'], right_on = ['sha'], how = 'inner')","79b28aaa":"##Preprocess the search result abstracts\ncovid_text = dummy_search_results['abstract_y']\n\n## remove special characters from string and set to lowercase\ndef cleanString(text):\n    regex = r'[^a-zA-Z0-9\\s]'\n    text = re.sub(regex,'',text)\n    text = text.rstrip(\"\\n\")\n    return text.lower()\n\n## remove special characters from 'text' string\nfor i in range(0, len(dummy_search_results)):\n    covid_text.at[i] = cleanString(covid_text[i])\n        \n## Remove stopwords and words of length 1\nstop_words = set(stopwords.words('english'))\nfor i in range(0, len(covid_text)):\n    rowbody = ''\n    for word in word_tokenize(covid_text[i]):\n        if word not in stop_words and len(word)>1:\n            rowbody = rowbody + word + ' '\n    covid_text.at[i] = rowbody\n    \n## Tokenize words\nfor i in range(0, len(covid_text)):\n    rowbody = ''\n    for word in word_tokenize(covid_text[i]):\n        rowbody = rowbody + word + ' '\n    covid_text.at[i] = rowbody    \n\n## Calculate TF-IDF\ntfidf_vectorizer=TfidfVectorizer(smooth_idf=True, use_idf=True)\ntfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(covid_text.to_list())\n\n## Calculate TF-IDF score for every word in a given document\ndef getTFIDF_byindex(dataset, docnum):\n    one_vector=tfidf_vectorizer_vectors[int(docnum)]\n    # Create df, where rows are each word and column is tfidf score\n    df = pd.DataFrame(one_vector.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n    # Add cord_uid document unique identifier column to df\n    df['cord_uid'] = dataset.loc[int(docnum),'cord_uid']\n    # Return dataframe sorted by tfidf score\n    return (df.sort_values(by='tfidf', ascending=False))\n\n## Calculate TF-IDF for each document in the search results\nallres = []\nfor i in range(0, len(dummy_search_results)):\n    corduid = dummy_search_results.loc[i,'cord_uid']\n    allres.append(getTFIDF_byindex(dummy_search_results, i))\n    \n## display dictionary of resulting tf-idf dataframes (one dataframe per document)\nallres = pd.concat(allres, sort=False)","047965b4":"G = nx.Graph()\nfor combo in itertools.combinations(allres['cord_uid'].unique(), 2):\n    A = set(allres[(allres['cord_uid']==combo[0]) & (allres['tfidf']>0.1)].index).difference(the_search_terms.split())\n    B = set(allres[(allres['cord_uid']==combo[1]) & (allres['tfidf']>0.1)].index).difference(the_search_terms.split())        \n    C = A.intersection(B)\n    if len(C)>0:\n        G.add_edge(combo[0], combo[1], weight = len(C), papers = list(C))   ","66bde1a5":"#Using plotly, draw a chord diagram. Nodes are papers; edges and edge thickness indicates the presence and frequency of shared keywords.   \n\n#Machinery to define an edge as a B\u00e9zier curve\n\n#Computes the distance between two 2D points: A, B\ndef dist (A,B):\n    return np.linalg.norm(np.array(A)-np.array(B))\n\n#Threshold distances between nodes on the unit circle and the list of parameters for interior control points of the B\u00e9zier curve\nDist=[0, dist([1,0], 2*[np.sqrt(2)\/2]), np.sqrt(2), dist([1,0],  [-np.sqrt(2)\/2, np.sqrt(2)\/2]), 2.0]\nparams=[1.2, 1.5, 1.8, 2.1]\n\n#Returns the index of the interval the distance d belongs to\ndef get_idx_interv(d, D):\n    k=0\n    while(round(d,2)>D[k]):\n        k+=1\n    return  k-1\n\n#The default number of points evaluated on a B\u00e9zier edge is 5. Then setting the Plotly shape of the edge line as spline, the five points are interpolated.\nclass InvalidInputError(Exception):\n    pass\n#Returns the point corresponding to the parameter t, on a B\u00e9zier curve of control points given in the list b.\ndef deCasteljau(b,t):\n    N=len(b)\n    if(N<2):\n        raise InvalidInputError(\"The  control polygon must have at least two points\")\n    a=np.copy(b) #shallow copy of the list of control points \n    for r in range(1,N):\n        a[:N-r,:]=(1-t)*a[:N-r,:]+t*a[1:N-r+1,:]\n    return a[0,:]\n\n#Returns an array of shape (nr, 2) containing the coordinates of nr points evaluated on the B\u00e9zier curve, at equally spaced parameters in [0,1]\ndef BezierCv(b, nr=5):\n    t=np.linspace(0, 1, nr)\n    return np.array([deCasteljau(b, t[k]) for k in range(nr)])\n\n\n#Create Edges\n#Add edges as disconnected lines in individual traces and nodes as a scatter trace, along with hidden edge traces in the middle of each edge for interactivity.\npos=nx.circular_layout(G)  #node locations along the unit circle\n    \nedge_traces=[]\nhidden_edge_traces=[]\nedge_colors=['#d4daff','#84a9dd', '#5588c8', '#6d8acf']\n\nfor edge in G.edges():\n    edge_x = []\n    edge_y = []\n    hidden_x = []\n    hidden_y = []\n    x0, y0 = pos[edge[0]] #xy-coordinates of the first node on the edge \n    x1, y1 = pos[edge[1]] #xy-coordinates of the second node on the edge\n    d = dist([x0,y0], [x1,y1])\n    K=get_idx_interv(d, Dist)\n    b=[pos[edge[0]], pos[edge[0]]\/params[K], pos[edge[1]]\/params[K], pos[edge[1]]]\n    color=edge_colors[K]\n    pts=BezierCv(b, nr=5)\n    mark=deCasteljau(b,0.9)\n    edge_x.append(x0)\n    edge_x.append(x1)\n    edge_y.append(y0)\n    edge_y.append(y1)\n    hidden_x.append(mark[0])\n    hidden_y.append(mark[1])\n    \n    edge_traces.append(go.Scatter(\n        x=pts[:,0], \n        y=pts[:,1],\n        mode = 'lines',\n        line=dict(color=color, shape='spline', width=G.edges[edge]['weight']), \n        hoverinfo='none'))\n    \n    hidden_edge_traces.append(go.Scatter(\n        x=hidden_x, \n        y=hidden_y, \n        text=' '.join(G.edges[edge]['papers']), \n        mode='markers', \n        hoverinfo='text', \n        marker=go.scatter.Marker(opacity=0)))\n\nnode_x = []\nnode_y = []\nfor node in G.nodes():\n    x, y = pos[node]\n    node_x.append(x)\n    node_y.append(y)\n\nnode_trace = go.Scatter(\n    x=node_x, \n    y=node_y,\n    hoverinfo = 'text',\n    mode='markers',\n    text = [metadata.at[metadata[metadata['cord_uid']==node].index[0], 'title'] for node in G.nodes()])","efdca363":"#Create Network Graph\n\nfig = go.Figure(data = [*edge_traces, *hidden_edge_traces, node_trace],\n             layout=go.Layout(\n                title='<br>Animal host evidence of spill-over to humans',\n                titlefont_size=16,\n                title_x = 0.5,\n                showlegend=False,\n                hovermode='closest',\n                margin=dict(b=20,l=5,r=5,t=40),\n                annotations=[ dict(\n                    text='Search Terms: ' + the_search_terms,\n                    showarrow=False,\n                    xref=\"paper\", yref=\"paper\",\n                    x=0.005, y=-0.002 ) ], \n                xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n                yaxis=dict(showgrid=False, zeroline=False, showticklabels=False))\n                )\nfig.show()","eef351ae":"## This code uses the BM-25 index to return relevant search results for an arbitrary set of keywords entered by a domain expert (from https:\/\/www.kaggle.com\/isaacmg\/scibert-embeddings).  Keywords are identified in each search result via tf-idf and common keywords are visualized across search results via chord diagram.","5d48c061":"## This code creates a chord diagram with papers as nodes, inserting an edge between papers if they share at least one keyword and modeling the number of shared keywords by edge thickness.  "}}