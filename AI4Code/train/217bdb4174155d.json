{"cell_type":{"51b89abb":"code","074774b9":"code","e4d9f297":"code","0e412782":"code","89c5d8e1":"code","a6b2441d":"code","8b6daad9":"code","910f4518":"code","8085f035":"code","19e9da77":"code","c6720f8f":"code","550ff428":"code","d5b48ceb":"code","56ff1644":"code","57d292c2":"code","9f09a5af":"code","4e72ee4f":"code","850c93d3":"code","c11bdfdc":"code","c825365e":"code","d3ed926a":"code","ca43a511":"code","71164758":"code","8e850b28":"code","983e2036":"code","b83f895b":"code","11d732e2":"code","c046fce5":"code","9a362079":"code","4e3ba10c":"code","f30dd2e8":"code","1f12c806":"code","89015177":"code","dc4836c8":"code","9fbe3203":"code","ad32be8b":"code","ab9eee36":"code","f9f4ba91":"code","a524afe0":"code","d7c45713":"code","cea1723f":"markdown","079b8017":"markdown","0e52ae00":"markdown","19806b36":"markdown","f2584a5f":"markdown","1ff2ad4e":"markdown","62664a95":"markdown","40b5dc5d":"markdown","de4919a2":"markdown","1b9e1a37":"markdown","f949714c":"markdown","04009837":"markdown","ceeaacc8":"markdown","12b75754":"markdown"},"source":{"51b89abb":"import pandas as pd\n\ndf = pd.read_csv(\"..\/input\/train.csv\")\nprint(df.info())","074774b9":"from sklearn.metrics.scorer import make_scorer\n\ndef rmlse(y, y0):\n    assert len(y) == len(y0)\n    return np.sqrt(np.mean(np.power(np.log1p(y)-np.log1p(np.clip(y0, 0, None)), 2)))\n\nrmsle_scorer = make_scorer(rmlse, greater_is_better=False)","e4d9f297":"import pandas as pd\n\ndf = pd.read_csv(\"..\/input\/train.csv\")\nprint(df.info())","0e412782":"test = pd.read_csv(\"..\/input\/test.csv\")\nprint(test.info())","89c5d8e1":"print(df.genres)","a6b2441d":"import numpy as np\n\nallgenres = set([i[\"name\"] for j in df.genres[df.genres.notnull()] for i in eval(j)])\nd = {i: np.zeros(df.shape[0], dtype=int) for i in allgenres}\nd[\"Missing\"] = np.zeros(df.shape[0], dtype=int)\ngenres_matrix = pd.DataFrame(d)\n\ngenres_matrix[\"Missing\"][pd.isnull(df.genres)] = 1.0\nfor j,i in enumerate(pd.notnull(df.genres)):\n    if i:\n        for k in eval(df.genres[j]):\n            genres_matrix.loc[j, k[\"name\"]] += 1\ngenres_matrix[\"genres_number\"] = genres_matrix.apply(sum, axis=1)","8b6daad9":"print(set(genres_matrix.genres_number))","910f4518":"print(df.production_companies[0:10])","8085f035":"allcompanies = set([i[\"name\"] for j in df.production_companies[df.production_companies.notnull()] for i in eval(j)])\nd = {i: np.zeros(df.shape[0], dtype=int) for i in allcompanies}\nd[\"Missing\"] = np.zeros(df.shape[0], dtype=int)\ncompanies_matrix = pd.DataFrame(d)\n\ncompanies_matrix[\"Missing\"][pd.isnull(df.production_companies)] = 1.0\nfor j,i in enumerate(pd.notnull(df.production_companies)):\n    if i:\n        for k in eval(df.production_companies[j]):\n            companies_matrix.loc[j, k[\"name\"]] += 1\ncompanies_matrix[\"companies_number\"] = companies_matrix.sum(axis=1)-companies_matrix.Missing","19e9da77":"print(df.production_countries[0:10])","c6720f8f":"import numpy as np\n\nallcountries = set([i[\"name\"] for j in df.production_countries[df.production_countries.notnull()] for i in eval(j)])\nd = {i: np.zeros(df.shape[0], dtype=int) for i in allcountries}\nd[\"Missing\"] = np.zeros(df.shape[0], dtype=int)\ncountries_matrix = pd.DataFrame(d)\n\ncountries_matrix[\"Missing\"][pd.isnull(df.production_countries)] = 1.0\nfor j,i in enumerate(pd.notnull(df.production_countries)):\n    if i:\n        for k in eval(df.production_countries[j]):\n            countries_matrix.loc[j, k[\"name\"]] += 1\ncountries_matrix[\"countries_number\"] = countries_matrix.sum(axis=1)-countries_matrix.Missing\nprint(countries_matrix)","550ff428":"print(df.spoken_languages[0:10])","d5b48ceb":"alllanguages = set([i[\"iso_639_1\"] for j in df.spoken_languages[df.spoken_languages.notnull()] for i in eval(j)])\nd = {i: np.zeros(df.shape[0], dtype=int) for i in alllanguages}\nd[\"Missing\"] = np.zeros(df.shape[0], dtype=int)\nlanguages_matrix = pd.DataFrame(d)\n\nlanguages_matrix[\"Missing\"][pd.isnull(df.spoken_languages)] = 1.0\nfor j,i in enumerate(pd.notnull(df.spoken_languages)):\n    if i:\n        for k in eval(df.spoken_languages[j]):\n            languages_matrix.loc[j, k[\"iso_639_1\"]] += 1\nlanguages_matrix[\"languages_number\"] = languages_matrix.sum(axis=1)-languages_matrix.Missing\nprint(languages_matrix)","56ff1644":"print(df.title[0:10])","57d292c2":"print(df.original_title[0:10])","9f09a5af":"df[\"title_as_original\"] = [int(i) for i in (df.title == df.original_title)]\nprint(df.title_as_original[0:10])","4e72ee4f":"df[\"title_length\"] = [len(i) for i in df.original_title]\ndf[\"title_words_count\"] = [len(i.split(\" \")) for i in df.original_title]\nprint(df.title_length[0:10])\nprint(df.title_words_count[0:10])","850c93d3":"df[\"first_world\"] = [i.split(\" \")[0] for i in df.original_title]\nprint(len(set(df.first_world)))\nprint(set(df.first_world))","c11bdfdc":"import pandas as pd\nimport numpy as np\n\ndf = pd.read_csv(\"..\/input\/train.csv\")\ntrain = pd.DataFrame(df[[\"budget\", \"popularity\", \"runtime\", \"status\", \"original_language\"]])\ntrain = pd.get_dummies(train)\ntest = pd.read_csv(\"..\/input\/test.csv\")\ndfte = pd.DataFrame(test[[\"budget\", \"popularity\", \"runtime\", \"status\", \"original_language\"]])\ndfte = pd.get_dummies(dfte)\nmissing_columns = set(dfte.columns) - set(train.columns)\nfor _ in missing_columns:\n    train[_] = 0\nmissing_columns = set(train.columns) - set(dfte.columns)\nfor _ in missing_columns:\n    dfte[_] = 0","c825365e":"train.loc[1335, \"runtime\"] = 130.0\ntrain.loc[2302, \"runtime\"] = 90.0\ntrain[\"homepage_missing\"] = np.array(df.homepage.isna(), dtype=int)\ntrain[\"belongs_to_collection_missing\"] = np.array(df.belongs_to_collection.isna(), dtype=int)\ntrain[\"release_day\"] = [int(i.split(\"\/\")[1]) for i in df.release_date]\ntrain[\"release_month\"] = [int(i.split(\"\/\")[0]) for i in df.release_date]\ntrain[\"release_year\"] = [int(i.split(\"\/\")[2]) for i in df.release_date]\ntrain[\"release_year\"] = [2000+i if i < 18 else 1900+i for i in train.release_year]\n\ntrain[\"poster_length\"] = 0\ntrain.loc[df.poster_path.notnull(), \"poster_length\"] = [len(i) for i in df.poster_path[df.poster_path.notnull()]]\n\nlabel = df[\"revenue\"]","d3ed926a":"train[\"contains_com\"] = 0\ntrain[\"contains_uk\"] = 0\ntrain[\"contains_fr\"] = 0\ntrain[\"contains_de\"] = 0\ntrain[\"contains_net\"] = 0\ntrain[\"contains_kr\"] = 0\ntrain[\"contains_disney\"] = 0\ntrain[\"contains_sony\"] = 0\ntrain[\"contains_warnerbros\"] = 0\ntrain[\"contains_indexhtml\"] = 0\ntrain[\"contains_movie\"] = 0\ntrain[\"contains_wikipedia\"] = 0\ntrain[\"count_slash\"] = 0\n\ntrain.loc[df.homepage.notnull(), \"contains_com\"] = [1 if ((i != \"\") & (\".com\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_uk\"] = [1 if ((i != \"\") & (\".uk\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_fr\"] = [1 if ((i != \"\") & (\".fr\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_de\"] = [1 if ((i != \"\") & (\".de\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_net\"] = [1 if ((i != \"\") & (\".net\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_kr\"] = [1 if ((i != \"\") & (\".kr\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_disney\"] = [1 if ((i != \"\") & (\"disney\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_sony\"] = [1 if ((i != \"\") & (\"sony\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_warnerbros\"] = [1 if ((i != \"\") & (\"warnerbros\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_indexhtml\"] = [1 if ((i != \"\") & (\"index.html\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_movie\"] = [1 if ((i != \"\") & (\"movie\" in i.lower())) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"contains_wikipedia\"] = [1 if ((i != \"\") & (\"wikipedia\" in i)) else 0 for i in df.homepage[df.homepage.notnull()]]\ntrain.loc[df.homepage.notnull(), \"count_slash\"] = [len(i.split(\"\/\")) for i in df.homepage[df.homepage.notnull()]]","ca43a511":"import numpy as np\n\nd = {i: np.zeros(df.shape[0], dtype=int) for i in allgenres}\nd[\"Missing\"] = np.zeros(df.shape[0], dtype=int)\ngenres_matrix = pd.DataFrame(d)\n\ngenres_matrix[\"Missing\"][pd.isnull(df.genres)] = 1.0\nfor j,i in enumerate(pd.notnull(df.genres)):\n    if i:\n        for k in eval(df.genres[j]):\n            genres_matrix.loc[j, k[\"name\"]] += 1\ngenres_matrix[\"genres_number\"] = genres_matrix.sum(axis=1)-genres_matrix.Missing\nprint(genres_matrix.shape)","71164758":"allcompanies = set([i[\"name\"] for j in df.production_companies[df.production_companies.notnull()] for i in eval(j)])\nd = {i: np.zeros(train.shape[0], dtype=int) for i in allcompanies}\nd[\"Missing\"] = np.zeros(train.shape[0], dtype=int)\ncompanies_matrix = pd.DataFrame(d)\n\ncompanies_matrix[\"Missing\"][pd.isnull(df.production_companies)] = 1.0\nfor j,i in enumerate(pd.notnull(df.production_companies)):\n    if i:\n        for k in eval(df.production_companies[j]):\n            companies_matrix.loc[j, k[\"name\"]] += 1\ncompanies_matrix[\"companies_number\"] = companies_matrix.sum(axis=1)-companies_matrix.Missing\nprint(companies_matrix.shape)","8e850b28":"import numpy as np\n\nallcountries = set([i[\"name\"] for j in df.production_countries[df.production_countries.notnull()] for i in eval(j)])\nd = {i: np.zeros(df.shape[0], dtype=int) for i in allcountries}\nd[\"Missing\"] = np.zeros(df.shape[0], dtype=int)\ncountries_matrix = pd.DataFrame(d)\n\ncountries_matrix[\"Missing\"][pd.isnull(df.production_countries)] = 1.0\nfor j,i in enumerate(pd.notnull(df.production_countries)):\n    if i:\n        for k in eval(df.production_countries[j]):\n            countries_matrix.loc[j, k[\"name\"]] += 1\ncountries_matrix[\"countries_number\"] = countries_matrix.sum(axis=1)-countries_matrix.Missing\nprint(countries_matrix.shape)","983e2036":"d = {i: np.zeros(df.shape[0], dtype=int) for i in alllanguages}\nd[\"Missing\"] = np.zeros(df.shape[0], dtype=int)\nlanguages_matrix = pd.DataFrame(d)\n\nlanguages_matrix[\"Missing\"][pd.isnull(df.spoken_languages)] = 1.0\nfor j,i in enumerate(pd.notnull(df.spoken_languages)):\n    if i:\n        for k in eval(df.spoken_languages[j]):\n            languages_matrix.loc[j, k[\"iso_639_1\"]] += 1\nlanguages_matrix[\"languages_number\"] = languages_matrix.sum(axis=1)-languages_matrix.Missing\nprint(languages_matrix.shape)","b83f895b":"train[\"title_as_original\"] = [int(i) for i in (df.title == df.original_title)]\ntrain[\"title_length\"] = [len(i) for i in df.original_title]\ntrain[\"title_words_count\"] = [len(i.split(\" \")) for i in df.original_title]\nfirst_word = [i.split(\" \")[0] for i in df.original_title]\nfirst_words = set(first_word)","11d732e2":"d = {i: np.zeros(train.shape[0], dtype=int) for i in first_words}\ntitles_matrix = pd.DataFrame(d)\nfor j,i in enumerate(first_words):\n    if i in first_words:\n        titles_matrix.loc[j, i] += 1\nprint(titles_matrix.shape)","c046fce5":"train[\"row\"] = np.linspace(0, train.shape[0], train.shape[0], dtype=int)\ngenres_matrix[\"row\"]= np.linspace(0, train.shape[0], train.shape[0], dtype=int)\ncompanies_matrix[\"row\"]= np.linspace(0, train.shape[0], train.shape[0], dtype=int)\ncountries_matrix[\"row\"]= np.linspace(0, train.shape[0], train.shape[0], dtype=int)\nlanguages_matrix[\"row\"]= np.linspace(0, train.shape[0], train.shape[0], dtype=int)\ntitles_matrix[\"row\"]= np.linspace(0, train.shape[0], train.shape[0], dtype=int)\ntrain = pd.concat([train, genres_matrix, companies_matrix, countries_matrix, languages_matrix, titles_matrix], axis=1, join=\"inner\")\ntrain.drop([\"row\"], axis = 1, inplace=False)\nprint(train.shape)","9a362079":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\n\nmodel  = RandomForestRegressor(n_estimators=100, random_state=2019)\nscores_randomforest = cross_val_score(model, train, label, cv=10, scoring=rmsle_scorer)\nprint(-np.mean(scores_randomforest), \"+\/-\" ,np.std(scores_randomforest))","4e3ba10c":"model  = RandomForestRegressor(n_estimators=100)\nmodel.fit(train, label)","f30dd2e8":"dfte[\"homepage_missing\"] = np.array(test.homepage.isna(), dtype=int)\ndfte[\"belongs_to_collection_missing\"] = np.array(test.belongs_to_collection.isna(), dtype=int)\ndfte.loc[243, \"runtime\"] = 93.0\ndfte.loc[1489, \"runtime\"] = 91.0\ndfte.loc[1632, \"runtime\"] = 100.0\ndfte.loc[3817, \"runtime\"] = 90.0\n\ntest.loc[828, \"release_date\"] = \"03\/30\/2001\"\ndfte[\"release_day\"] = [int(i.split(\"\/\")[1]) for i in test.release_date]\ndfte[\"release_month\"] = [int(i.split(\"\/\")[0]) for i in test.release_date]\ndfte[\"release_year\"] = [int(i.split(\"\/\")[2]) for i in test.release_date]\ndfte[\"release_year\"] = [2000+i if i < 18 else 1900+i for i in dfte.release_year]\n\ndfte[\"poster_length\"] = 0\ndfte.loc[test.poster_path.notnull(), \"poster_length\"] = [len(i) for i in test.poster_path[test.poster_path.notnull()]]","1f12c806":"dfte[\"contains_com\"] = 0\ndfte[\"contains_uk\"] = 0\ndfte[\"contains_fr\"] = 0\ndfte[\"contains_de\"] = 0\ndfte[\"contains_net\"] = 0\ndfte[\"contains_kr\"] = 0\ndfte[\"contains_disney\"] = 0\ndfte[\"contains_sony\"] = 0\ndfte[\"contains_warnerbros\"] = 0\ndfte[\"contains_indexhtml\"] = 0\ndfte[\"contains_movie\"] = 0\ndfte[\"contains_wikipedia\"] = 0\ndfte[\"count_slash\"] = 0\n\ndfte.loc[test.homepage.notnull(), \"contains_com\"] = [1 if ((i != \"\") & (\".com\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_uk\"] = [1 if ((i != \"\") & (\".uk\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_fr\"] = [1 if ((i != \"\") & (\".fr\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_de\"] = [1 if ((i != \"\") & (\".de\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_net\"] = [1 if ((i != \"\") & (\".net\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_kr\"] = [1 if ((i != \"\") & (\".kr\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_disney\"] = [1 if ((i != \"\") & (\"disney\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_sony\"] = [1 if ((i != \"\") & (\"sony\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_warnerbros\"] = [1 if ((i != \"\") & (\"warnerbros\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_indexhtml\"] = [1 if ((i != \"\") & (\"index.html\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_movie\"] = [1 if ((i != \"\") & (\"movie\" in i.lower())) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"contains_wikipedia\"] = [1 if ((i != \"\") & (\"wikipedia\" in i)) else 0 for i in test.homepage[test.homepage.notnull()]]\ndfte.loc[test.homepage.notnull(), \"count_slash\"] = [len(i.split(\"\/\")) for i in test.homepage[test.homepage.notnull()]]","89015177":"import numpy as np\n\nd = {i: np.zeros(test.shape[0], dtype=int) for i in allgenres}\nd[\"Missing\"] = np.zeros(test.shape[0], dtype=int)\ngenres_matrix = pd.DataFrame(d)\n\ngenres_matrix[\"Missing\"][pd.isnull(test.genres)] = 1.0\nfor j,i in enumerate(pd.notnull(test.genres)):\n    if i:\n        for k in eval(test.genres[j]):\n            genres_matrix.loc[j, k[\"name\"]] += 1\ngenres_matrix[\"genres_number\"] = genres_matrix.sum(axis=1)-genres_matrix.Missing\nprint(genres_matrix.shape)","dc4836c8":"d1 = {i: np.zeros(test.shape[0], dtype=int) for i in allcompanies}\nd1[\"Missing\"] = np.zeros(test.shape[0], dtype=int)\ncompanies_matrix = pd.DataFrame(d1)\n\ncompanies_matrix[\"Missing\"][pd.isnull(test.production_companies)] = 1.0\nfor j,i in enumerate(pd.notnull(test.production_companies)):\n    if i:\n        for k in eval(test.production_companies[j]):\n            if (k[\"name\"] in d1.keys()):\n                companies_matrix.loc[j, k[\"name\"]] += 1\ncompanies_matrix[\"companies_number\"] = companies_matrix.sum(axis=1)-companies_matrix.Missing\nprint(companies_matrix.shape)","9fbe3203":"d = {i: np.zeros(test.shape[0], dtype=int) for i in allcountries}\nd[\"Missing\"] = np.zeros(test.shape[0], dtype=int)\ncountries_matrix = pd.DataFrame(d)\n\ncountries_matrix[\"Missing\"][pd.isnull(test.production_countries)] = 1.0\nfor j,i in enumerate(pd.notnull(test.production_countries)):\n    if i:\n        for k in eval(test.production_countries[j]):\n            if (k[\"name\"] in d.keys()):\n                countries_matrix.loc[j, k[\"name\"]] += 1\n\ncountries_matrix[\"countries_number\"] = countries_matrix.sum(axis=1)-countries_matrix.Missing\nprint(countries_matrix.shape)","ad32be8b":"d = {i: np.zeros(test.shape[0], dtype=int) for i in alllanguages}\nd[\"Missing\"] = np.zeros(test.shape[0], dtype=int)\nlanguages_matrix = pd.DataFrame(d)\n\nlanguages_matrix[\"Missing\"][pd.isnull(test.spoken_languages)] = 1.0\nfor j,i in enumerate(pd.notnull(test.spoken_languages)):\n    if i:\n        for k in eval(test.spoken_languages[j]):\n            if (k[\"iso_639_1\"] in d.keys()):\n                languages_matrix.loc[j, k[\"iso_639_1\"]] += 1\nlanguages_matrix[\"languages_number\"] = languages_matrix.sum(axis=1)-languages_matrix.Missing\nprint(languages_matrix.shape)","ab9eee36":"dfte[\"title_as_original\"] = [int(i) for i in (test.title == test.original_title)]\ndfte[\"title_length\"] = [len(i) for i in test.original_title]\ndfte[\"title_words_count\"] = [len(i.split(\" \")) for i in test.original_title]\n\nfirst_word_array = [i.split(\" \")[0] for i in test.original_title]\nd = {i: np.zeros(test.shape[0], dtype=int) for i in first_words}\ntitles_matrix = pd.DataFrame(d)\nfor j,i in enumerate(titles_matrix):\n    if i in first_words:\n        titles_matrix.loc[j, i] += 1\nprint(titles_matrix.shape)","f9f4ba91":"dfte[\"row\"] = np.linspace(0, dfte.shape[0], dfte.shape[0], dtype=int)\ngenres_matrix[\"row\"]= np.linspace(0, dfte.shape[0], dfte.shape[0], dtype=int)\ncompanies_matrix[\"row\"]= np.linspace(0, dfte.shape[0], dfte.shape[0], dtype=int)\ncountries_matrix[\"row\"]= np.linspace(0, dfte.shape[0], dfte.shape[0], dtype=int)\nlanguages_matrix[\"row\"]= np.linspace(0, dfte.shape[0], dfte.shape[0], dtype=int)\ntitles_matrix[\"row\"]= np.linspace(0, dfte.shape[0], dfte.shape[0], dtype=int)\ndfte = pd.concat([dfte, genres_matrix, companies_matrix, countries_matrix, \n                  languages_matrix, titles_matrix], axis=1, join=\"inner\")\ndfte.drop([\"row\"], axis = 1, inplace=False)\nprint(dfte.shape)","a524afe0":"print(set(dfte.columns).difference(set(train.columns)))","d7c45713":"predictions = model.predict(dfte)\npredictions = np.clip(predictions, 0, None)\nsubmission = pd.DataFrame({\n    \"id\" : test.id,\n    \"revenue\": predictions\n})\nsubmission.to_csv(\"submission.csv\", index=False)","cea1723f":"# Model\n\n## Select train and test","079b8017":"## Features","0e52ae00":"## Prepare submission","19806b36":"Train on the full train set:","f2584a5f":"# Adding variables","1ff2ad4e":"The input variables currenly unused:\n\n* **id**                      \n* ~~**belongs_to_collection**~~\n* ~~**budget**~~                  \n* ~~**genres**~~                  \n* ~~**homepage**~~                 \n* **imdb_id**                  \n* ~~**original_language**~~        \n* **original_title**           \n* **overview**                 \n* ~~**popularity**~~             \n* ~~**poster_path**~~             \n* ~~**production_companies**~~  \n* **production_countries**     \n* ~~**release_date**~~            \n* ~~**runtime**~~                \n* **spoken_languages**        \n* ~~**status**~~                   \n* **tagline**                  \n* **title**                   \n* **Keywords**                \n* **cast**                     \n* **crew**                    \n* ~~**revenue**~~                 \n\nThe metric to be used is **RMLSE**:","62664a95":"## Production companies","40b5dc5d":"## Train the model","de4919a2":"## Production countries","1b9e1a37":"## Genres","f949714c":"Prepare the test set:","04009837":"# Importing data","ceeaacc8":"## Titles","12b75754":"## Spoken languages"}}