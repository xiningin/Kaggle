{"cell_type":{"6d197001":"code","9b710223":"code","819038c9":"code","2ddf5bbc":"code","26b4c0a8":"code","644f287a":"code","179b3f47":"code","c36225cf":"code","54936bb3":"code","315d40a7":"code","38185db7":"code","c1f986a6":"code","3239fcc9":"code","fe18cbe7":"code","53ab4075":"code","e265921b":"code","fc2e9186":"code","00f90a18":"code","d1d3f367":"code","c04cec14":"code","5e602861":"code","508fdd11":"code","44027527":"code","73642fa4":"code","96f6bde8":"code","8d86745d":"code","56311d43":"code","dac79b6b":"code","fa0cf0fe":"code","73e46167":"code","4887f5a1":"code","81e3b934":"code","a942b702":"code","c2c94028":"code","48b8b238":"code","84f17a67":"code","3828ea21":"code","9b556b42":"code","53df6870":"code","15007b18":"code","29294afe":"markdown","cff1ca04":"markdown","6079b301":"markdown","f70166ad":"markdown"},"source":{"6d197001":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9b710223":"pip install scrapy","819038c9":"pip install selenium","2ddf5bbc":"import urllib.request\nwith urllib.request.urlopen('https:\/\/pdki-indonesia.dgip.go.id\/index.php\/merek\/bGRmMUJubEtBalc4eGFpclRBWDdQUT09?q=PIERRE+CARDIN&type=1') as response:\n   html = response.read()\n   print(html)","26b4c0a8":"pip install webdriver-manager","644f287a":"from selenium import webdriver\nfrom webdriver_manager.firefox import GeckoDriverManager\n\ndriver = webdriver.Firefox(executable_path=GeckoDriverManager().install())","179b3f47":"from selenium import webdriver\nfrom webdriver_manager.firefox import GeckoDriverManager\n\ndriver = webdriver.Firefox(executable_path=GeckoDriverManager().install())","c36225cf":"from selenium import webdriver\nfrom selenium.webdriver.common.keys import Keys\n\ndriver = webdriver.Firefox()\ndriver.get(\"https:\/\/putusan3.mahkamahagung.go.id\/direktori\/kategori\/jenis\/merek.html\")\n\n\ndriver.close()","54936bb3":"from requests import get\nfrom requests.exceptions import RequestException\nfrom contextlib import closing\nfrom bs4 import BeautifulSoup","315d40a7":"# Python 3.\nimport urllib    \nfrom bs4 import BeautifulSoup\n\nurl = \"https:\/\/pdki-indonesia.dgip.go.id\/index.php\/paten?q=sabun&type=1&filter_by=diberi\"\nresp = urllib.request.urlopen(url)\n# Get server encoding per recommendation of Martijn Pieters.\nsoup = BeautifulSoup(resp, from_encoding=resp.info().get_param('charset'))  \nexternal_links = set()\ninternal_links = set()\nfor line in soup.find_all('a'):\n    link = line.get('href')\n    if not link:\n        continue\n    if link.startswith('http'):\n        external_links.add(link)\n    else:\n        internal_links.add(link)\n\n# Depending on usage, full internal links may be preferred.\nfull_internal_links = {\n    urllib.parse.urljoin(url, internal_link) \n    for internal_link in internal_links\n}\n\n# Print all unique external and full internal links.\nfor link in external_links.union(full_internal_links):\n    print(link)","38185db7":"import urllib2\nfrom bs4 import BeautifulSoup\na=urllib2.urlopen('https:\/\/pdki-indonesia.dgip.go.id\/index.php\/paten?q=sabun&type=1&filter_by=diberi')\ncode=a.read()\nsoup=BeautifulSoup(code)\nlinks=soup.findAll(\"a\")\n#To get href part alone\nprint (links[0].attrs['href'])","c1f986a6":"from bs4 import BeautifulSoup\nimport requests\nURL = 'https:\/\/pdki-indonesia.dgip.go.id\/index.php\/merek\/bGRmMUJubEtBalc4eGFpclRBWDdQUT09?q=PIERRE+CARDIN&type=1'\npage = requests.get(URL)\n\nsoup = BeautifulSoup(page.content, 'html.parser')\nprint(soup)","3239fcc9":"import requests\nimport urllib.request\nimport time\nfrom bs4 import BeautifulSoup","fe18cbe7":"pip install mathematicians","53ab4075":"from mathematicians import simple_get\nraw_html = simple_get('https:\/\/putusan3.mahkamahagung.go.id\/pengadilan\/direktori\/pengadilan\/mahkamah-agung\/jenis\/merek.html')\n","e265921b":"from bs4 import BeautifulSoup\nraw_html = open('https:\/\/putusan3.mahkamahagung.go.id\/pengadilan\/direktori\/pengadilan\/mahkamah-agung\/jenis\/merek.html').read()","fc2e9186":"import requests.sessions\nrequests= requests.sessions.Session()\nurl = 'https:\/\/putusan3.mahkamahagung.go.id\/pengadilan\/direktori\/pengadilan\/mahkamah-agung\/jenis\/merek\/page\/2.html'\nresponse = requests.get(url,verify=False)","00f90a18":"from bs4 import BeautifulSoup\nimport requests\nURL = 'https:\/\/bphn.jdihn.go.id\/pencarian?tipe_dokumen=1'\npage = requests.get(URL)\n\nsoup = BeautifulSoup(page.content, 'html.parser')\nhrefs = set()\n\nprint(soup.prettify())","d1d3f367":"soup.find_all(\"a\")","c04cec14":"articles = soup.find_all(\"a\")\nprint(articles)\nfor a in articles:\n    single = a.get('href')\n    print(single.text)","5e602861":"soup.find('a').get('href')","508fdd11":"soup.find('p').get_text()","44027527":"articles = div.find_all('article')\nfor a in articles:\n    single = a.find('h2')\n    print(single.find('a').text)","73642fa4":"from bs4 import BeautifulSoup\nraw_html = open('http:\/\/www.detik.com').read()\nhtml = BeautifulSoup(raw_html, 'html.parser')","96f6bde8":"pip install pyopenssl","8d86745d":"import requests\n\nURL = 'https:\/\/putusan3.mahkamahagung.go.id\/direktori\/putusan\/b4d3cee7fd75a32c47fc1f520f2ca92a.html'\npage = requests.get(URL, verify=False)","56311d43":"import urllib \nfrom urllib.request import urlopen\nhtml = urlopen(\"https:\/\/putusan3.mahkamahagung.go.id\/pengadilan\/direktori\/pengadilan\/mahkamah-agung\/jenis\/merek.html\")\ncontents = html.read()\nprint(contents)","dac79b6b":"import urllib \nfrom urllib.request import urlopen\nhtml = urlopen(\"https:\/\/bphn.jdihn.go.id\/pencarian?tipe_dokumen=1&page=2\")\ncontents = html.read()\nprint(contents)","fa0cf0fe":"import requests\nfrom bs4 import BeautifulSoup\n\nURL = 'https:\/\/putusan3.mahkamahagung.go.id\/direktori\/putusan\/b4d3cee7fd75a32c47fc1f520f2ca92a.html'\npage = requests.get(URL)\n\nsoup = BeautifulSoup(page.content, 'html.parser')","73e46167":"import requests\nfrom bs4 import BeautifulSoup\n\nURL = 'https:\/\/bphn.jdihn.go.id\/pencarian?tipe_dokumen=1&page=2'\npage = requests.get(URL)\n\nsoup = BeautifulSoup(page.content, 'html.parser')","4887f5a1":"pip install selenium","81e3b934":"from selenium import webdriver\nfrom selenium.webdriver.support.ui import WebDriverWait\nbrowser = webdriver.Firefox()\n# Step 2) Navigate to Facebook\nbrowser.get(\"https:\/\/putusan3.mahkamahagung.go.id\/pengadilan\/direktori\/pengadilan\/mahkamah-agung\/jenis\/merek.html\")","a942b702":"def unduh(alamat):\n    f= open(\"\/kaggle\/working\/sabun2.txt\",\"a+\")\n    resp = urllib.request.urlopen(url)\n    # Get server encoding per recommendation of Martijn Pieters.\n    soup = BeautifulSoup(resp, from_encoding=resp.info().get_param('charset'))  \n    external_links = set()\n    internal_links = set()\n    for line in soup.find_all('a'):\n        link = line.get('href')\n        if not link:\n            continue\n        if link.startswith('http'):\n            external_links.add(link)\n        else:\n            internal_links.add(link)\n\n    # Depending on usage, full internal links may be preferred.\n    full_internal_links = {\n        urllib.parse.urljoin(url, internal_link) \n        for internal_link in internal_links\n    }\n\n    # Print all unique external and full internal links.\n    for link in external_links.union(full_internal_links):\n    \n        result =link.find('https:\/\/pdki-indonesia.dgip.go.id\/index.php\/paten\/')\n        if (result>=0):\n            print(link)\n            f.write(link+\"\\n\")\n    f.close()       ","c2c94028":"# Python 3.\nimport urllib    \nfrom bs4 import BeautifulSoup\nurl = \"https:\/\/pdki-indonesia.dgip.go.id\/index.php\/paten?q=udara&type=1&filter_by=diberi\"\nunduh(url)\n\nfor i in range(1,159):\n    hasil=i*10\n    url = \"https:\/\/pdki-indonesia.dgip.go.id\/index.php\/paten?q=udara&type=1&filter_by=diberi&skip=\"+str(hasil)\n    print(url)\n    unduh(url)","48b8b238":"# Python 3.\nimport urllib    \nfrom bs4 import BeautifulSoup\n\n  \nurl = \"https:\/\/pdki-indonesia.dgip.go.id\/index.php\/paten?q=susu&type=1&filter_by=diberi\"\nunduh(url)\nf.close()","84f17a67":"# Python 3.\nimport urllib    \nfrom bs4 import BeautifulSoup\n\nurl = \"https:\/\/pdki-indonesia.dgip.go.id\/index.php\/paten?q=sabun&type=1&filter_by=diberi\"\nresp = urllib.request.urlopen(url)\n# Get server encoding per recommendation of Martijn Pieters.\nsoup = BeautifulSoup(resp, from_encoding=resp.info().get_param('charset'))  \nexternal_links = set()\ninternal_links = set()\nfor line in soup.find_all('a'):\n    link = line.get('href')\n    if not link:\n        continue\n    if link.startswith('http'):\n        external_links.add(link)\n    else:\n        internal_links.add(link)\n\n# Depending on usage, full internal links may be preferred.\nfull_internal_links = {\n    urllib.parse.urljoin(url, internal_link) \n    for internal_link in internal_links\n}\n\n# Print all unique external and full internal links.\nfor link in external_links.union(full_internal_links):\n    \n    result =link.find('https:\/\/pdki-indonesia.dgip.go.id\/index.php\/paten\/')\n    if (result>=0):\n        print(link)","3828ea21":"# Using readlines() \nfile1 = open(\"..\/input\/alamatpaten\/sabun2 (7).txt\",\"r\") \nLines = file1.readlines() \ni=0\n# Strips the newline character \nfor line in Lines: \n    if (i>300):\n        break\n    else: \n        if (i>200):\n            print(i)\n            print(line)\n            unduhisi(line)\n    i=i+1    \nfile1.close()    ","9b556b42":"import requests\nfrom bs4 import BeautifulSoup\ndef unduhisi(alamat):\n    f= open(\"\/kaggle\/working\/judul300.txt\",\"a+\")\n    fisi= open(\"\/kaggle\/working\/isi300.txt\",\"a+\")\n\n    URL =alamat\n    page = requests.get(URL)\n\n    soup = BeautifulSoup(page.content, 'html.parser')\n    quotes = soup.find_all('div',class_='span-3')\n\n    #print(quotes)\n\n\n    for q in quotes:\n        head = q.find('h1')\n        print(head)\n        f.write(str(head))\n        \n        quote = str(q.find('p', class_='value2'))\n        if len(quote) >4:\n            print(quote)\n            fisi.write(quote)\n\n    f.close()\n    fisi.close()\n","53df6870":"pip install python-certifi-win32","15007b18":"import requests\nimport requests.sessions\n\nfrom bs4 import BeautifulSoup\nURL ='https:\/\/bphn.jdihn.go.id\/pencarian?tipe_dokumen=1&jenis_peraturan=7'\n\n\npage = requests.get(URL, verify=False)\n\nsoup = BeautifulSoup(page.content, 'html.parser')\n\nquotes = soup.find_all('div')\nprint(quotes)","29294afe":"https:\/\/jdih.kemdikbud.go.id\/arsip\/","cff1ca04":"undang-undang","6079b301":"https:\/\/hackersandslackers.com\/scraping-urls-with-beautifulsoup\/","f70166ad":"benar"}}