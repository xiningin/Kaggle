{"cell_type":{"e7b7b51b":"code","fc8fc11d":"code","cf4125b7":"code","a60d78aa":"code","80687035":"code","f2d84a89":"code","8a7ef05f":"code","38f65c01":"code","022fd39e":"code","dd418742":"code","23716e17":"code","2e7caceb":"code","6665642f":"code","1fb0e8dd":"code","b2483677":"code","71426f0b":"code","c6debe53":"code","c47d35b8":"code","22cfe010":"code","be67e4ae":"code","c94f5af2":"code","fa0f2053":"code","d9db4494":"code","bef5433a":"markdown"},"source":{"e7b7b51b":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nfrom skimage import io, transform\nimport matplotlib.pyplot as plt\nimport random\nimport os","fc8fc11d":"IMAGE_WIDTH=200\nIMAGE_HEIGHT=200\nIMAGE_CHANNELS=3\nEPOCHS=30\nIMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\nPATH='..\/input\/dataset-for-mask-detection\/dataset\/'\nPATH2= '..\/input\/face-mask-detection\/dataset\/'","cf4125b7":"# \ub370\uc774\ud130 \ubd88\ub7ec\uc624\uae30\nwith_mask = os.listdir(PATH+\"with_mask\")\nwithout_mask = os.listdir(PATH+\"without_mask\")\nwith_mask2 = os.listdir(PATH2+\"with_mask\")\nwithout_mask2 = os.listdir(PATH2+\"without_mask\")\n\n\ndef add_path1(filename):\n    return PATH +'with_mask\/' + filename\ndef add_path2(filename):\n    return PATH + 'without_mask\/' + filename\ndef add_path3(filename):\n    return PATH2 +'with_mask\/' + filename\ndef add_path4(filename):\n    return PATH2 + 'without_mask\/' + filename\n\nw_mask = list(map(add_path1, with_mask))\nwo_mask = list(map(add_path2, without_mask))\nw_mask2 = list(map(add_path3, with_mask2))\nwo_mask2 = list(map(add_path4, without_mask2))\n\n","a60d78aa":"# \ub370\uc774\ud130 preprocessing & label\n\ndef dataset(file_list_with, file_list_without,file_list_with2, file_list_without2,size=IMAGE_SIZE,flattened=False):\n    data = []\n    labels = []\n    sum_1 = 0\n    sum_2 = 0\n    for i, file in enumerate(file_list_with):\n        if(file == PATH + \"with_mask\/.ipynb_checkpoints\"):\n            continue\n        image = io.imread(file)\n        image = transform.resize(image, size, mode='constant')\n        data.append(image)\n        labels.append(1)\n    for i, file in enumerate(file_list_without):\n        if(file == PATH + \"without_mask\/.ipynb_checkpoints\"):\n            continue\n        image = io.imread(file)\n        image = transform.resize(image, size, mode='constant')\n        data.append(image)\n        labels.append(0)\n    for i, file in enumerate(file_list_with2):\n        if(file == PATH2 + \"with_mask\/.ipynb_checkpoints\"):\n            continue\n        image = io.imread(file)\n        image = transform.resize(image, size, mode='constant')\n        if(image.shape == (200,200,4)):\n            sum_1 += 1\n            continue\n        data.append(image)\n        labels.append(1)\n    for i, file in enumerate(file_list_without2):\n        if(file == PATH2 + \"without_mask\/.ipynb_checkpoints\"):\n            continue\n        image = io.imread(file)\n        image = transform.resize(image, size, mode='constant')\n        if(image.shape == (200,200,4)):\n            sum_2 += 1\n            continue\n        data.append(image)\n        labels.append(0)\n    \n    print(sum_1, sum_2)\n    return np.array(data), np.array(labels)","80687035":"# skimage \uc758 transform.resize \uac00 auto scale \ub418\uc11c \ub098\uc624\ub294\ub4ef\ud569\ub2c8\ub2e4.\n# 0-1 \uc758 \ubc94\uc704\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\nX, y = dataset(w_mask, wo_mask,w_mask2, wo_mask2)\nprint(X.shape,y.shape)\n","f2d84a89":"# \ub370\uc774\ud130 \ud655\uc778\ud558\uae30\nsample_1 = random.choice(X)\n\nf = plt.figure()\nplt.imshow(sample_1)\nplt.show(block=True)\n","8a7ef05f":"\n# create model\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv2D, Flatten, Dense, Activation, BatchNormalization, MaxPooling2D, Dropout","38f65c01":"def create_model():\n    model = Sequential()\n    model.add(Conv2D(64, (3,3), activation='relu', strides=(2,2), input_shape=(IMAGE_WIDTH,IMAGE_HEIGHT,IMAGE_CHANNELS)))\n    model.add(Conv2D(64, (3,3), activation='relu'))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(128, (3,3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(128, (3,3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(256, (3,3), activation='relu'))\n    model.add(MaxPooling2D((2, 2)))\n    model.add(Dropout(0.3))\n    model.add(Conv2D(256, (3,3), activation='relu'))\n    model.add(Dropout(0.4))\n    model.add(Flatten())\n    model.add(Dense(512, activation='relu'))\n    model.add(Dropout(0.4))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n    return model","022fd39e":"model1 = create_model()\nmodel1.summary()","dd418742":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.10,stratify=y)","23716e17":"print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)","2e7caceb":"partial_x_train, validation_x_train, partial_y_train, validation_y_train = train_test_split(x_train, y_train, test_size=0.20)","6665642f":"print(partial_x_train.shape,validation_x_train.shape,partial_y_train.shape,validation_y_train.shape)","1fb0e8dd":"print('The size of the training set: ',len(x_train))\nprint('The size of the partial training set: ',len(partial_x_train))\nprint('The size of the validation training set: ',len(validation_x_train))\nprint('The size of the testing set: ',len(x_test))","b2483677":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=2, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)\ncallbacks = [learning_rate_reduction]","71426f0b":"history = model1.fit(\n    partial_x_train, \n    partial_y_train,\n    validation_data=(validation_x_train, validation_y_train),\n    epochs=EPOCHS, \n    batch_size=32,\n    verbose =1,\n    callbacks=callbacks\n)\n","c6debe53":"def smooth_curve(points, factor=0.8): #this function will make our plots more smooth\n    smoothed_points = []\n    for point in points:\n        if smoothed_points:\n            previous = smoothed_points[-1]\n            smoothed_points.append(previous*factor+point*(1-factor))\n        else:\n            smoothed_points.append(point)\n    return smoothed_points\n","c47d35b8":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n","22cfe010":"epochs = range(1, len(acc)+1)\nplt.plot(epochs, smooth_curve(acc), 'bo', label='Training acc')\nplt.plot(epochs, smooth_curve(val_acc), 'r-', label='Validation acc')\nplt.legend()\nplt.title('Training and Validation Acc')\nplt.figure()\n\nplt.plot(epochs, smooth_curve(loss), 'bo', label='Training loss')\nplt.plot(epochs, smooth_curve(val_loss), 'r-', label='Validation loss')\nplt.legend()\nplt.title('Training and Validation loss')\nplt.show()","be67e4ae":"test_loss, test_acc = model1.evaluate(x_test, y_test, steps=32)\nprint('The final test accuracy: ',test_acc)","c94f5af2":"predictions = model1.predict(x_test)     # Vector of probabilities\npred_labels = np.argmax(predictions, axis = 1) # We take the highest probability","fa0f2053":"def print_mislabeled_images(test_images, test_labels, pred_labels):\n    \"\"\"\n        Print 25 examples of mislabeled images by the classifier, e.g when test_labels != pred_labels\n    \"\"\"\n    BOO = (test_labels == pred_labels)\n    mislabeled_indices = random.choice(np.where(BOO == 0)[0])\n    mislabeled_images = test_images[mislabeled_indices]\n    mislabeled_labels = pred_labels[mislabeled_indices]\n    print(mislabeled_labels)\n    title = \"Some examples of mislabeled images by the classifier:\"\n    f = plt.figure()\n    plt.imshow(mislabeled_images)\n    plt.show(block=True)\n","d9db4494":"print_mislabeled_images(x_test, y_test, pred_labels)","bef5433a":"# Mask Face Classification"}}