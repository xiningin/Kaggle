{"cell_type":{"4a2657e8":"code","a2f15de6":"code","00cb94ae":"code","60bb698a":"code","ddc1b1b8":"code","037fb9e9":"code","e7f6c6be":"code","bde10dae":"code","4857ba07":"code","271f8bbe":"code","03572864":"code","a2bc2631":"code","6dd1e809":"code","54a6356b":"code","0ff7406a":"code","448c8265":"code","576f698c":"markdown","4a87ee18":"markdown","8e118259":"markdown","bc734ee9":"markdown","c0ecd4a5":"markdown"},"source":{"4a2657e8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy as sp\nfrom scipy import linalg\n# from scipy.sparse.linalg  as linalg\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(filename)\n        if(filename == \"diamonds.csv\"):\n            filepath_diamond = os.path.join(dirname, filename)\n            print(os.path.join(dirname, filename))\n        elif(filename == \"breastcancer.csv\"):\n            filepath_breast = os.path.join(dirname, filename)\n            print(os.path.join(dirname, filename))\n\n            \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2f15de6":"df = pd.read_csv(filepath_diamond).iloc[:,[1,5,7]]  #.iloc[:, [0,4,6]]\ndf.head()","00cb94ae":"def mahalanobis(x=None, data=None, cov=None):\n    \"\"\"Compute the Mahalanobis Distance between each row of x and the data  \n    x    : vector or matrix of data with, say, p columns.\n    data : ndarray of the distribution from which Mahalanobis distance of each observation of x is to be computed.\n    cov  : covariance matrix (p x p) of the distribution. If None, will be computed from data.\n    \"\"\"\n    print(np.mean(data))\n    x_minus_mu = x - np.mean(data)\n    if not cov:\n        cov = np.cov(data.values.T)\n    inv_covmat = linalg.inv(cov) #\u77e9\u9635\u7684\u9006\n    \n    left_term = np.dot(x_minus_mu, inv_covmat)\n    mahal = np.dot(left_term, x_minus_mu.T)\n    return mahal.diagonal() #\u5bf9\u89d2\u77e9\u9635\n\ndf_x = df[['carat', 'depth', 'price']].head(500)\ndf_x['mahala'] = mahalanobis(x=df_x, data=df[['carat', 'depth', 'price']])\ndf_x.head()","60bb698a":"# Critical values for two degrees of freedom\nfrom scipy.stats import chi2 #\u5361\u65b9\u5206\u5e03\nchi2.ppf((1-0.01), df=2)\n#> 9.21","ddc1b1b8":"# Compute the P-Values\ndf_x['p_value'] = 1 - chi2.cdf(df_x['mahala'], 2)\n\n# Extreme values with a significance level of 0.01\ndf_x.loc[df_x.p_value < 0.01].head(10)","037fb9e9":"def getLabelClass(line):\n    if(line ==\"benign\"):\n        return 0\n    elif(line == \"malignant\"):\n        return 1\n    \ndf = pd.read_csv(filepath_breast, \n                 usecols=['Cl.thickness', 'Cell.size', 'Marg.adhesion', \n                          'Epith.c.size', 'Bare.nuclei', 'Bl.cromatin', 'Normal.nucleoli', \n                          'Mitoses', 'Class'])\ndf.dropna(inplace=True)  # drop missing values.\ndf.head()\ndf['Class'] = df['Class'].map(lambda x: getLabelClass(x))\n\n\n# \"benign\"\n# \"malignant\"","e7f6c6be":"df","bde10dae":"from sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(df.drop('Class', axis=1), df['Class'], test_size=0.3, random_state=100)\n\n# Split the training data as pos and neg\nxtrain_pos = xtrain.loc[ytrain == 1, :]\nxtrain_neg = xtrain.loc[ytrain == 0, :]\n","4857ba07":"xtrain_pos","271f8bbe":"class MahalanobisBinaryClassifier():\n    def __init__(self, xtrain, ytrain):\n        self.xtrain_pos = xtrain.loc[ytrain == 1, :] \n        self.xtrain_neg = xtrain.loc[ytrain == 0, :] \n\n    def predict_proba(self, xtest):\n        pos_neg_dists = [(p,n) for p, n in zip(mahalanobis(xtest, self.xtrain_pos), mahalanobis(xtest, self.xtrain_neg))] #mahalanobis this func is mah dist\n        return np.array([(1-n\/(p+n), 1-p\/(p+n)) for p,n in pos_neg_dists]) # here 1-n means\uff1apos pro \uff0c 1-p means\uff1a neg pro\n\n    def predict(self, xtest):\n        return np.array([np.argmax(row) for row in self.predict_proba(xtest)]) # get  max value  index in array\n\n","03572864":"\nclf = MahalanobisBinaryClassifier(xtrain, ytrain)        \npred_probs = clf.predict_proba(xtest)\npred_class = clf.predict(xtest)\n# print(pred_probs)\n# print(pred_class)\n# Pred and Truth\npred_actuals = pd.DataFrame([(pred, act) for pred, act in zip(pred_class, ytest)], columns=['pred', 'true']) # show pred result\nprint(pred_actuals[:5])  ","a2bc2631":"from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\ntruth = pred_actuals.loc[:, 'true']\npred = pred_actuals.loc[:, 'pred']\nscores = np.array(pred_probs)[:, 1]\nprint('AUROC: ', roc_auc_score(truth, scores)) # roc\nprint('\\nConfusion Matrix: \\n', confusion_matrix(truth, pred)) # confusion matrix\nprint('\\nAccuracy Score: ', accuracy_score(truth, pred))\nprint('\\nClassification Report: \\n', classification_report(truth, pred))","6dd1e809":"df = pd.read_csv(filepath_breast, \n                 usecols=['Cl.thickness', 'Cell.size', 'Marg.adhesion', \n                          'Epith.c.size', 'Bare.nuclei', 'Bl.cromatin', 'Normal.nucleoli', \n                          'Mitoses', 'Class'])\n\ndf.dropna(inplace=True)  # drop missing values.\n\ndf['Class'] = df['Class'].map(lambda x: getLabelClass(x))\n\nfrom sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(df.drop('Class', axis=1), df['Class'], test_size=.5, random_state=100)\n\n# Split the training data as pos and neg\nxtrain_pos = xtrain.loc[ytrain == 1, :] # now here\uff0c only use pos data","54a6356b":"class MahalanobisOneclassClassifier():\n    def __init__(self, xtrain, significance_level=0.01):\n        self.xtrain = xtrain\n        self.critical_value = chi2.ppf((1-significance_level), df=xtrain.shape[1]-1)\n        print('Critical value is: ', self.critical_value)\n\n    def predict_proba(self, xtest):\n        mahalanobis_dist = mahalanobis(xtest, self.xtrain)\n        self.pvalues = 1 - chi2.cdf(mahalanobis_dist, 2) # use chi2 pvalue\n        return mahalanobis_dist\n\n    def predict(self, xtest):\n        return np.array([int(i) for i in self.predict_proba(xtest) > self.critical_value])\n\nclf = MahalanobisOneclassClassifier(xtrain_pos, significance_level=0.05)\nmahalanobis_dist = clf.predict_proba(xtest)\n\n# Pred and Truth\nmdist_actuals = pd.DataFrame([(m, act) for m, act in zip(mahalanobis_dist, ytest)], columns=['mahal', 'true_class'])\nprint(mdist_actuals[:5])     ","0ff7406a":"# quantile cut in 10 pieces\nmdist_actuals['quantile'] = pd.qcut(mdist_actuals['mahal'], \n                                    q=[0, .10, .20, .3, .4, .5, .6, .7, .8, .9, 1], \n                                    labels=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n# sort by mahalanobis distance\nmdist_actuals.sort_values('mahal', inplace=True)\nperc_truths = mdist_actuals.groupby('quantile').agg({'mahal': np.mean, 'true_class': np.sum}).rename(columns={'mahal':'avg_mahaldist', 'true_class':'sum_of_trueclass'})\nprint(perc_truths)","448c8265":"from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n\n# Positive if mahalanobis \npred_actuals = pd.DataFrame([(int(p), y) for y, p in zip(ytest, clf.predict_proba(xtest) < clf.critical_value)], columns=['pred', 'true'])\n\n# Accuracy Metrics\ntruth = pred_actuals.loc[:, 'true']\npred = pred_actuals.loc[:, 'pred']\nprint('\\nConfusion Matrix: \\n', confusion_matrix(truth, pred))\nprint('\\nAccuracy Score: ', accuracy_score(truth, pred))\nprint('\\nClassification Report: \\n', classification_report(truth, pred))","576f698c":"# 8. Usecase 3: One-Class Classification\nOne Class classification is a type of algorithm where the training dataset contains observations belonging to only one class.\n\nWith only that information known, the objective is to figure out if a given observation in a new (or test) dataset belongs to that class.\n\nYou might wonder when would such a situation occur. Well, it\u2019s a quite common problem in Data Science.\n\nFor example consider the following situation: You have a large dataset containing millions of records that are NOT yet categorized as 1\u2019s and 0\u2019s. But you also have with you a small sample dataset containing only positive (1\u2019s) records. By learning the information in this sample dataset, you want to classify all the records in the large dataset as 1\u2019s and 0\u2019s.\n\nBased on the information from the sample dataset, it is possible to tell if any given sample is a 1 or 0 by viewing only the 1\u2019s (and having no knowledge of the 0\u2019s at all).\n\nThis can be done using Mahalanobis Distance.\n\nLet\u2019s try this on the BreastCancer dataset, only this time we will consider only the malignant observations (class column=1) in the training data.","4a87ee18":"Mahalanobis distance is an effective multivariate distance metric that measures the distance between a point and a distribution. It is an extremely useful metric having, excellent applications in multivariate anomaly detection, classification on highly imbalanced datasets and one-class classification. This post explains the intuition and the math with practical examples on three machine learning use cases.\n[inference](https:\/\/www.machinelearningplus.com\/mahalanobis-distance\/)","8e118259":"# 7. Usecase 2: Mahalanobis Distance for Classification Problems","bc734ee9":"# Malanobis Distance \u2013 Understanding the math with examples (python)","c0ecd4a5":"# Conclusion\n\nIn this post, we covered nearly everything about Mahalanobis distance: the intuition behind the formula, the actual calculation in python and how it can be used for multivariate anomaly detection, binary classification, and one-class classification. It is known to perform really well when you have a highly imbalanced dataset.\n\nHope it was useful? Please leave your comments below and I will see you in the next one.\n\n"}}