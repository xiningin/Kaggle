{"cell_type":{"a93ab62d":"code","ba6e4245":"code","d702a164":"code","24d1ad38":"code","35245679":"code","7df752c9":"code","8a686cab":"code","4350d9e2":"code","5436090f":"code","89918349":"code","015d8c0e":"code","e82b72ed":"code","736b8da0":"code","dd01c000":"code","297f3d7c":"code","419c1c26":"code","aeee33c3":"code","397ec871":"code","21aba828":"code","519ed4b9":"code","5b3e6cc7":"code","67196ddd":"code","777d7752":"code","eefc6d1b":"code","9021b197":"code","5709709c":"code","3bf0b2ce":"code","5249525c":"code","e60bc036":"code","46601e24":"code","fb2b39cf":"code","ec6ff158":"code","54a9427c":"code","3f3db834":"code","4de87ada":"code","d8f5b3e2":"code","bdbb162c":"code","9b00ac7a":"code","97f9eac0":"code","ac873230":"code","e7f1c160":"code","62ec0890":"code","8c28eb3e":"code","0a99f2b3":"code","877ad4af":"code","cfd2ca1e":"code","a0d6957b":"code","68cce9ca":"code","dedce6d7":"code","bd66e846":"code","25c1d653":"code","92019392":"code","0cae5689":"code","a4e15460":"code","601c86a1":"code","4e01985d":"code","62ad565d":"code","762bd3ff":"code","6f9dc87e":"code","e0a69042":"code","eaaf57a5":"code","a0e6bd9e":"code","49acd843":"code","10e11943":"code","66165ded":"code","47d77232":"code","b9fbd880":"code","0cecd088":"code","64aa4fa2":"code","cb0e51cd":"code","0cb7da75":"code","473acb32":"code","9ce1f4f5":"code","810d7b74":"code","93ba6f95":"code","46846170":"code","497d1283":"code","547bbc4d":"code","a97e18c0":"code","810ae7ef":"code","dbc379d4":"code","054d0b90":"code","327e12dc":"code","e36723e1":"code","9396475f":"markdown","f5bed734":"markdown","0e227b19":"markdown","35541ff1":"markdown","a3befa5d":"markdown","3289b7b7":"markdown","8a047f7f":"markdown","5d5b31a3":"markdown","c0fb3cf9":"markdown","2699b62f":"markdown","d8ec1312":"markdown","0c3523ba":"markdown","bfb57e10":"markdown","a9c3e25f":"markdown","a4e4c3ba":"markdown","2af58719":"markdown","21643703":"markdown","86dbc0c2":"markdown","1d2213ae":"markdown","a8a2f63b":"markdown","beeb194b":"markdown","c7d23e8c":"markdown","585b40f1":"markdown","d3effd7c":"markdown","11329c32":"markdown","1f00d3b0":"markdown","e751ff6e":"markdown","5a5d58f6":"markdown","f1b12cb1":"markdown","b5e122b8":"markdown","5801245f":"markdown","5d64ff92":"markdown","e3da1493":"markdown","5e1fd9bc":"markdown","ded3f6d9":"markdown","37b93438":"markdown","5a5a5b58":"markdown","d4a60343":"markdown","e876e5ce":"markdown","e2d7b3fd":"markdown","9f911ddb":"markdown","e367a1b3":"markdown","9b0edc9b":"markdown","af07fcfd":"markdown","5da5169c":"markdown","760e2580":"markdown","91540073":"markdown","5ff328fe":"markdown","daeee7fc":"markdown","52390faf":"markdown"},"source":{"a93ab62d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nimport collections\nfrom collections import Counter as count","ba6e4245":"data= pd.read_csv(\"..\/input\/uci-turkiye-student-evaluation-data-set\/turkiye-student-evaluation_generic.csv\")\ndata.head()","d702a164":"data.columns","24d1ad38":"data.info()","35245679":"data.shape","7df752c9":"data.isnull().sum()","8a686cab":"data.describe()","4350d9e2":"data.shape","5436090f":"sns.countplot(x='class',data=data)\nplt.show()","89918349":"plt.figure(figsize=(15,6))\nsns.boxplot(data=data.loc[:,'Q1':'Q25'])\nplt.show()","015d8c0e":"from sklearn.preprocessing import StandardScaler","e82b72ed":"sc = StandardScaler()\ndata = pd.DataFrame(sc.fit_transform(data),columns=data.columns)","736b8da0":"from sklearn.cluster import KMeans","dd01c000":"cluster_range = range(1,10)\ncluster_errors = []\n\nfor num_clusters in cluster_range:\n    clusters = KMeans(num_clusters, n_init=10, max_iter=100)\n    clusters.fit(data)\n    \n    cluster_errors.append(clusters.inertia_)\n    \npd.DataFrame({'num_clusters':cluster_range, 'Error': cluster_errors})","297f3d7c":"plt.figure(figsize=(10,5))\nplt.plot(cluster_range, cluster_errors, marker = \"o\" )\nplt.title('Elbow Plot')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Error')\nplt.xticks(cluster_range)\nplt.show()","419c1c26":"kmeans = KMeans(n_clusters = 3, init = 'k-means++')\ny_kmeans = kmeans.fit_predict(data)","aeee33c3":"y_kmeans","397ec871":"count(y_kmeans)","21aba828":"from scipy.cluster.hierarchy import dendrogram, linkage","519ed4b9":"plt.figure(figsize=(20,10))\nZ = linkage(data, method='ward')\ndendrogram(Z, leaf_rotation=90, p=10, truncate_mode='level', leaf_font_size=6, color_threshold=8)\nplt.title('Dendogram')\nplt.show()","5b3e6cc7":"from sklearn.cluster import AgglomerativeClustering","67196ddd":"ac = AgglomerativeClustering(n_clusters=3, affinity='euclidean',  linkage='ward')\nac.fit(data)","777d7752":"ac.labels_","eefc6d1b":"y_ac=ac.fit_predict(data)","9021b197":"count(y_ac)","5709709c":"first0=[2225,1554]\nsecond1=[1230,2173]\nthird2=[2365,2093]\nclusters=['Kmeans','Agglm Cluster']\nd=pd.DataFrame({'Clusters':clusters,'FirstC':first0,'SecondC':second1,'ThirdC':third2})\nd","3bf0b2ce":"df=data.copy()","5249525c":"kmeans = KMeans(3, n_init=5, max_iter=100)\nkmeans.fit(df)\ndf['label'] = kmeans.labels_\ndf.head()","e60bc036":"df['label'].value_counts()","46601e24":"sns.pairplot(df,hue='label')\nplt.show()","fb2b39cf":"from sklearn.decomposition import PCA","ec6ff158":"pca=PCA()\npca.fit(data)","54a9427c":"data_pca= pca.transform(data)\ndata_pca.shape","3f3db834":"pca.components_","4de87ada":"cumsum=np.cumsum(pca.explained_variance_ratio_)\ncumsum","d8f5b3e2":"plt.figure(figsize=(10,6))\n\nplt.plot(range(1,34), cumsum, color='k', lw=2)\n\nplt.xlabel('Number of components')\nplt.ylabel('Total explained variance')\n\nplt.axvline(8, c='b')\nplt.axhline(0.9, c='r')\n\nplt.show()","bdbb162c":"pca = PCA(n_components=8)\npca.fit(data)\ndata_pca = pd.DataFrame(pca.transform(data))\ndata_pca.shape","9b00ac7a":"sns.pairplot(data_pca, diag_kind='kde')\nplt.show()","97f9eac0":"cluster_range = range(1,16)\ncluster_errors = []\n\nfor num_clusters in cluster_range:\n    clusters = KMeans(num_clusters, n_init=10, max_iter=100)\n    clusters.fit(data_pca)\n    \n    cluster_errors.append(clusters.inertia_)\n    \npd.DataFrame({'num_clusters':cluster_range, 'Error': cluster_errors})","ac873230":"plt.figure(figsize=(10,5))\nplt.plot(cluster_range, cluster_errors, marker = \"o\" )\nplt.title('Elbow Plot')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Error')\nplt.xticks(cluster_range)\nplt.show()","e7f1c160":"pca_df = data_pca.copy()\nkmeans = KMeans(3, n_init=10, max_iter=100)\nkmeans.fit(pca_df)\npca_df['label'] = kmeans.labels_\npca_df['label'].value_counts()","62ec0890":"plt.figure(figsize=(20,10))\nlink = linkage(data_pca, method='ward')\ndendrogram(link, leaf_rotation=90, p=10, truncate_mode='level', leaf_font_size=6, color_threshold=8)\nplt.title('Dendogram')\nplt.show()","8c28eb3e":"ac = AgglomerativeClustering(n_clusters=3, affinity='euclidean',  linkage='ward')\nac.fit(data_pca)","0a99f2b3":"y_ac=ac.fit_predict(data_pca)","877ad4af":"count(y_ac)","cfd2ca1e":"first0=[2226,2756]\nsecond1=[1231,2379]\nthird2=[2363,685]\nclusters=['Kmeans','Agglm Cluster']\nd=pd.DataFrame({'Clusters':clusters,'FirstC':first0,'SecondC':second1,'ThirdC':third2})\nd","a0d6957b":"df.head()","68cce9ca":"X=df.drop(columns='label')\ny=df['label']","dedce6d7":"from sklearn.model_selection import train_test_split","bd66e846":"Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.3, random_state=1)\n\nprint(Xtrain.shape)\nprint(Xtest.shape)\nprint(ytrain.shape)\nprint(ytest.shape)","25c1d653":"from sklearn import metrics","92019392":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(Xtrain, ytrain)","0cae5689":"print('Training score =', lr.score(Xtrain, ytrain))\nprint('Test score =', lr.score(Xtest, ytest))","a4e15460":"ypred1=lr.predict(Xtest)","601c86a1":"acc1=(metrics.accuracy_score(ytest,ypred1))\nacc1","4e01985d":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(ytest, ypred1)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.show()","62ad565d":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(Xtrain, ytrain)\n\nprint('Training score =', dt.score(Xtrain, ytrain))\nprint('Test score =', dt.score(Xtest, ytest))","762bd3ff":"ypred2=dt.predict(Xtest)","6f9dc87e":"acc2=(metrics.accuracy_score(ytest,ypred2))\nacc2","e0a69042":"cm = confusion_matrix(ytest, ypred2)\nsns.heatmap(cm, annot=True, fmt='d')\nplt.show()","eaaf57a5":"from sklearn.neighbors import KNeighborsClassifier\n\nscore=[]\nfor k in range(1,100):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(Xtrain, ytrain)\n    ypred3=knn.predict(Xtest)\n    accuracy=metrics.accuracy_score(ypred3,ytest)\n    score.append(accuracy*100)\n    print (k,': ',accuracy)","a0e6bd9e":"score.index(max(score))+1","49acd843":"round(max(score))","10e11943":"knn = KNeighborsClassifier(n_neighbors=9)\nknn.fit(Xtrain, ytrain)\n\nprint('Training score =', knn.score(Xtrain, ytrain))\nprint('Test score =', knn.score(Xtest, ytest))","66165ded":"from sklearn.naive_bayes import GaussianNB","47d77232":"gnb = GaussianNB()\ngnb.fit(Xtrain, ytrain)\n\nprint('Training score =', gnb.score(Xtrain, ytrain))\nprint('Test score =', gnb.score(Xtest, ytest))","b9fbd880":"Algorithm=['LogisticRegression','Decision Tree','KNN','Naive Bayes']\nTrain_Accuracy=[0.985,1.00,0.977,0.988]\nTest_Accuracy=[0.975,0.939,0.963,0.988]","0cecd088":"Before_PCA = pd.DataFrame({'Algorithm': Algorithm,'Train_Accuracy': Train_Accuracy,'Test_Accuracy':Test_Accuracy})\nBefore_PCA","64aa4fa2":"df1=data_pca.copy()","cb0e51cd":"kmeans = KMeans(3, n_init=5, max_iter=100)\nkmeans.fit(df1)\ndf1['label'] = kmeans.labels_\ndf1.head()","0cb7da75":"X1=df1.drop(columns='label')\ny1=df1['label']","473acb32":"X_train, X_test, y_train, y_test = train_test_split(X1, y1, test_size=0.3, random_state=1)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","9ce1f4f5":"lr_pca = LogisticRegression()\nlr_pca.fit(X_train, y_train)\nprint('Training score =', lr_pca.score(X_train, y_train))\nprint('Test score =', lr_pca.score(X_test, y_test))","810d7b74":"dt_pca = DecisionTreeClassifier()\ndt_pca.fit(X_train, y_train)\nprint('Training score =', dt_pca.score(X_train, y_train))\nprint('Test score =', dt_pca.score(X_test, y_test))","93ba6f95":"score=[]\nfor k in range(1,100):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    ypred=knn.predict(X_test)\n    accuracy=metrics.accuracy_score(ypred,y_test)\n    score.append(accuracy*100)\n    print (k,': ',accuracy)","46846170":"score.index(max(score))+1","497d1283":"(max(score))","547bbc4d":"knn_pca = KNeighborsClassifier(n_neighbors=7)\nknn_pca.fit(X_train, y_train)\n\nprint('Training score =', knn_pca.score(X_train, y_train))\nprint('Test score =', knn_pca.score(X_test, y_test))","a97e18c0":"gnb_pca = GaussianNB()\ngnb_pca.fit(X_train, y_train)\nprint('Training score =', gnb_pca.score(X_train, y_train))\nprint('Test score =', gnb_pca.score(X_test, y_test))","810ae7ef":"Algorithm=['LogisticRegression','Decision Tree','KNN','Naive Bayes']\nTrain_Accuracy=[0.987,1.00,0.987,0.975]\nTest_Accuracy=[0.979,0.995,0.980,0.967]","dbc379d4":"After_PCA = pd.DataFrame({'Algorithm': Algorithm,'Train_Accuracy': Train_Accuracy,'Test_Accuracy':Test_Accuracy})\nAfter_PCA","054d0b90":"Algorithm=['LR BPCA','DT BPCA','KNN BPCA','NB BPCA','LR APCA','DT APCA','KNN APCA','NB APCA']\nTrain_Accuracy=[0.985,1.00,0.977,0.988,0.987,1.00,0.987,0.975]\nTest_Accuracy=[0.975,0.939,0.963,0.988,0.979,0.995,0.980,0.967]","327e12dc":"Final = pd.DataFrame({'Algorithm': Algorithm,'Train_Accuracy': Train_Accuracy,'Test_Accuracy':Test_Accuracy})\nFinal","e36723e1":"plt.subplots(figsize=(15,6))\nsns.lineplot(x=\"Algorithm\", y=\"Train_Accuracy\",data=Final,palette='hot',label='Train Accuracy')\nsns.lineplot(x=\"Algorithm\", y=\"Test_Accuracy\",data=Final,palette='hot',label='Test Accuracy')\n\nplt.xticks(rotation=90)\nplt.title('MLA Accuracy Comparison')\nplt.legend()\nplt.show()","9396475f":"# *Splitting the data before PCA :*","f5bed734":"**Model is good fit.**","0e227b19":"**Model is good fit.**","35541ff1":"# *Data Ingestion :*","a3befa5d":"**From the above dendogram we can see 3 clusters.**","3289b7b7":"### *KNN :*","8a047f7f":"### *Naive Bayes :*","5d5b31a3":"**Model is somewhat underfit.**","c0fb3cf9":"**Model is good fit.**","2699b62f":"**Inference :**\n* Naive Bayes algorithm has performed well with an accuracy 0f 98.8 percent.\n* Decision Tree has not performed well and it is under fit.","d8ec1312":"### *Elbow Plot :* ","0c3523ba":"# *Exploratory Data Analysis :*","bfb57e10":"### *Decision Tree Classifier :*","a9c3e25f":"**By the Dendogram we can see that there are 3 optimal number of clusters.**","a4e4c3ba":"# *Final Model :*","2af58719":"**Now fit Hierarchical clustering to the data**","21643703":"# *Splitting the data after PCA :*","86dbc0c2":"### *Descriptive Statistics :*","1d2213ae":"**Based on the elbow graph we can go for 3 clusters.**","a8a2f63b":"**Model is good fit:**","beeb194b":"### *KNN :*","c7d23e8c":"# *Scaling :*","585b40f1":"data_pca is the dataset got after PCA. ","d3effd7c":"# *K Means Clustering :*","11329c32":"### *Kmeans Clustering :*","1f00d3b0":"**Inference :**\n* All the models performed well.\n* Decision Tree has 100% on training and 99.5% on testing.","e751ff6e":"### *Logistic Regression :*","5a5d58f6":"**Above count was the count of 3 clusters.**","f1b12cb1":"**Elbow Plot :**","b5e122b8":"# *Hierarchical Clustering :*","5801245f":"### *Agglomerative Clustering :*","5d64ff92":"BPCA indicates BEFORE PCA\nAPCA indicates AFTER PCA","e3da1493":"### *Decision Tree Classifier :*","5e1fd9bc":"**There are no null values in a given dataset.**","ded3f6d9":"# *Convert Unsupervised data into Supervised data :*","37b93438":"**Since the data is already scaled , now apllying PCA fro dimensionality reduction :** ","5a5a5b58":"**It was observed that Q14,Q15,Q17,Q19:Q22 and Q25 questions with good rating.**","d4a60343":"### *Logistic Regression :*","e876e5ce":"**Model is good fit.**","e2d7b3fd":"**Model is under fit.**","9f911ddb":"**Inference :**\n* Naive Bayes before PCA  performed well.\n* Logistic Regression after PCA performed well.\n* Naive Bayes(Before PCA) is the best model from all the model where training and testing sores are equal.","e367a1b3":"**Model is Best Fit.**","9b0edc9b":"## *Agglomerative Clustering :*","af07fcfd":"**Inference :**\n* Descrpitive statistics of columns difficulty, Q1, Q2....Q28 are almost same.\n* Mean and Median of class column approximately same.\n","5da5169c":"**90 percent of data consists 8 components.**","760e2580":"**Inference :**\n* First cluster is some what nearer in both the methods.","91540073":"### *Naive Bayes :*","5ff328fe":"### *Null Values :*","daeee7fc":"# *PCA :*","52390faf":"**Inference :**\n* From the above dataframe we can compare the clusters of both the algorithmns.\n* Third cluster number from both methods was almost close."}}