{"cell_type":{"2edf80f5":"code","c1d40c94":"code","6b4d4a69":"code","3b7d95ce":"code","50245515":"code","d0edabfc":"code","05778849":"code","73ce341e":"code","b707c97a":"code","eca16029":"code","a42234f1":"code","c43c0648":"code","dc17fa38":"code","f2cdd5d5":"code","1c8623f8":"code","3bd9f912":"code","20d53608":"code","6319bb1c":"code","ceb3681e":"code","f39ac1f6":"code","c1587f91":"code","1d4de48a":"code","3ea4d113":"code","6584857b":"markdown","189b77a8":"markdown","5856484f":"markdown","f01db6ff":"markdown","0b627fbb":"markdown","c37e9b01":"markdown","296db150":"markdown","b511fc8e":"markdown","30e3e80e":"markdown","7d1c6f34":"markdown","cea36f7b":"markdown","25a851e2":"markdown","a00c9fe8":"markdown","cccdb4d1":"markdown","0d94092b":"markdown","25e4dd75":"markdown","58892b65":"markdown","8450e7ba":"markdown"},"source":{"2edf80f5":"#Needed libraries and modules\nimport pandas as pd                                  # data processing\nimport numpy as np                                   # linear algebra functionalities \nimport seaborn as sns                                # visualization library\nimport matplotlib.pyplot as plt                      # visualization library\n\n#Modeling\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LinearRegression, Ridge, ElasticNet\nfrom sklearn.svm import SVR\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nimport multiprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c1d40c94":"path = \"..\/input\/weatherww2\/Summary of Weather.csv\"\ndf = pd.read_csv(path)\ndf","6b4d4a69":"#Reshape to covert from numpy 1 array to numpy 2D array which is what we need to work with.\n\nX = df['MinTemp'].values.reshape(-1,1)         #X contains the observations of the independent variable\nY = df['MaxTemp'].values.reshape(-1,1)         #Y contains the observations of the dependent variable\n \nprint(\"Type and size of the  vector X:\", type(X), X.shape)\nprint(\"Type and size of the  vector Y:\", type(Y), Y.shape)","3b7d95ce":"#Scatter plot of the data\n\nwidth = 30\nheight = 15\nplt.figure(figsize=(width, height))\n\nplt.scatter(X,Y)\nplt.title('MinTemp vs MaxTemp')  \nplt.xlabel('MinTemp')  \nplt.ylabel('MaxTemp')  ","50245515":"#Distribution plot of MaxTemp\n\nwidth = 20\nheight = 10\nplt.figure(figsize=(width, height))\nsns.distplot(Y, hist=False, label=\"MaxTemp\")\n\nplt.title('Distribution of MaxTemp')  \nplt.xlabel('MaxTemp')\nplt.ylabel('Density of observations')","d0edabfc":"#Spliting the data into train (80%) and test (20%) sets.\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=1)\n\nprint('Amount of elements in the train set:', X_train.size)\nprint('Amount of elements in the test set:', Y_test.size)\nprint(\"Shape of train and test vectors\", Y_train.shape, X_test.shape)","05778849":"#Training linear model\n\nlm = LinearRegression()\n\nlm.fit(X_train,Y_train)\n\nprint('The learned intercept and coefficients are', lm.intercept_, lm.coef_) ","73ce341e":"#Making predictions using the entire data set and plotting \n\nYhat = lm.predict(X)\n\nwidth = 30\nheight = 15\nplt.figure(figsize=(width, height))\n\nplt.scatter(X,Y)\nplt.plot(X, Yhat, color='red', linewidth=4)\nplt.title('MinTemp vs MaxTemp')  \nplt.xlabel('MinTemp')  \nplt.ylabel('MaxTemp')\nplt.show()","b707c97a":"print('Acuracy of simple linear regression on the test set:', lm.score(X_test, Y_test))\n\nYhat_test = lm.predict(X_test)\nprint('Mean squared error of simple linear regression on the test set:', mean_squared_error(Y_test, Yhat_test))","eca16029":"width = 30\nheight = 15\nplt.figure(figsize=(width, height))\n\n#Ploting the actual and predicted distributions\nax1 = sns.distplot(Y_test, hist=False, color=\"b\", label=\"Actual Value\")\nsns.distplot(Yhat_test, hist=False, color=\"r\", label=\"Predicted Values\" , ax=ax1)\n\nplt.title('Distribution')  \nplt.xlabel('MaxTemp')\nplt.ylabel('Density of observations')\n\nplt.show()\nplt.close()","a42234f1":"CV_param_grid = [{'alpha': [0.1, 0.2, 0.4, 0.8, 1.6, 3.2, 6.4] }]\n\nCV_best_estimators = []\nCV_best_r2_scores = []\nCV_best_mse_scores = []\nCV_best_params = []\n\nrandom_state = 47\n\ndegrees = 20\nfor d in range(degrees):\n    pf = PolynomialFeatures(degree=d+1)\n    Xd = pf.fit_transform(X_train)\n    \n    #The grid serch computes both, the r^2 and the negative mse, but it selects the best model based on the r^2 score.\n    Grid = GridSearchCV(estimator= Ridge(random_state = random_state), param_grid=CV_param_grid, scoring=['r2','neg_mean_squared_error'], \n                        cv=20, refit='r2', n_jobs=multiprocessing.cpu_count())\n    Grid.fit(Xd, Y_train)\n    CV_best_estimators.append(Grid.best_estimator_)\n    CV_best_r2_scores.append(Grid.best_score_)\n    CV_best_mse_scores.append((-1)*Grid.cv_results_['mean_test_neg_mean_squared_error'][Grid.best_index_])\n    CV_best_params.append(Grid.best_params_)\n","c43c0648":"print('The list of best parameters for each degree is:\\n', CV_best_params)\nprint('The list of best CV r^2 scores for each degree is:\\n', CV_best_r2_scores)\nprint('The list of best CV mse scores for each degree is:\\n', CV_best_mse_scores)","dc17fa38":"test_r2_scores = []\ntest_mse_scores = []\n\n\nfor d in range(degrees):\n    pf = PolynomialFeatures(degree=d+1)\n    Xd_test = pf.fit_transform(X_test)\n    Ydhat = CV_best_estimators[d].predict(Xd_test)\n    test_r2_scores.append(r2_score(Ydhat, Y_test))\n    test_mse_scores.append(mean_squared_error(Ydhat, Y_test))","f2cdd5d5":"print('The list of r^2 scores in the test set for each degree is:\\n', test_r2_scores)\nprint('The list of mse scores in the test set for each degree is:\\n', test_mse_scores)","1c8623f8":"width = 24\nheight = 8\nplt.figure(figsize=(width, height))\n\nplt.subplot(1, 2, 1)\nplt.plot(range(1,degrees+1), CV_best_r2_scores, color='blue', label=\"CV r^2 score\")\nplt.plot(range(1,degrees+1), test_r2_scores, color='red', label=\"Test r^2 score\")\nplt.legend(loc=\"lower right\")\nplt.title('CV R2 score vs Degree') \n\nplt.subplot(1, 2, 2)\nplt.plot(range(1,degrees+1), CV_best_mse_scores, color='blue', label=\"CV mse score\")\nplt.plot(range(1,degrees+1), test_mse_scores, color='red', label=\"Test mse score\")\nplt.legend(loc=\"top right\")\nplt.title('CV MSE score vs Degree') \n\nplt.show()\n","3bd9f912":"#Conclussion\nbest_d_r2_cv = CV_best_r2_scores.index(max(CV_best_r2_scores))+1\nbest_d_mse_cv = CV_best_mse_scores.index(min(CV_best_mse_scores))+1\n\nprint(\"Degree with best CV R^2 score:\", best_d_r2_cv)\nprint(\"Degree with best CV MSE score:\", best_d_mse_cv)","20d53608":"print('The best hyperparameters found after CV are:\\n', 'degree:', best_d_r2_cv, '\\n',  \n      'alpha:', CV_best_params[best_d_r2_cv-1]['alpha'],\n     )","6319bb1c":"BestModel = CV_best_estimators[best_d_r2_cv-1]\nBestModel","ceb3681e":"X_new = np.linspace(min(X), max(X), 2000).reshape(2000,1)\npf = PolynomialFeatures(degree= best_d_r2_cv)\nX_trans = pf.fit_transform(X_new)\nYhat_trans = BestModel.predict(X_trans)\n\n#Plotting predictions\nwidth = 30\nheight = 15\nplt.figure(figsize=(width, height))\n\nplt.scatter(X,Y)\nplt.plot(X_new, Yhat_trans, color='red', linewidth=4)\nplt.title('MinTemp vs MaxTemp')  \nplt.xlabel('MinTemp')  \nplt.ylabel('MaxTemp')\nplt.show()","f39ac1f6":"pf = PolynomialFeatures(degree= best_d_r2_cv)\nXd_test = pf.fit_transform(X_test)\nYdhat_test = BestModel.predict(Xd_test)\n\nprint('Acuracy of the above polynomial regression on the test set:', BestModel.score(Xd_test, Y_test))\n\nprint('Mean squared error of the above polynomial regression on the test set:', mean_squared_error(Y_test, Ydhat_test))","c1587f91":"def PerformaceLimitCases (Model, X, Y, lim_inf, lim_sup, degree=1):\n    \"\"\"\n    This function returs the r^2 scores of a model in the interval  lim_inf < X < lim_sup\n    \"\"\"\n    if degree == 1:\n        return Model.score(X[(X > lim_inf) & (X < lim_sup)].reshape(-1,1), Y[(X > lim_inf) & (X < lim_sup)].reshape(-1,1))\n    else:\n        pf = PolynomialFeatures(degree= degree)\n        Xt = pf.fit_transform(X[(X > lim_inf) & (X < lim_sup)].reshape(-1,1))\n        return Model.score(Xt, Y[(X > lim_inf) & (X < lim_sup)].reshape(-1,1))","1d4de48a":"print('Perfomance of the BestModel in the range (-34, -30):', \n      PerformaceLimitCases(BestModel, X_test, Y_test, -34, -30, best_d_r2_cv))\nprint('Perfomance of the linear model outside of the range (-34, -30):', \n      PerformaceLimitCases(lm, X_test, Y_test, -34, -30, 1))\n","3ea4d113":"width = 30\nheight = 15\nplt.figure(figsize=(width, height))\n\n#Ploting the actual and predicted distributions\nax1 = sns.distplot(Y_test, hist=False, color=\"b\", label=\"Actual Value\")\nsns.distplot(Ydhat_test, hist=False, color=\"r\", label=\"Predicted Values\" , ax=ax1)\n\nplt.title('Distribution')  \nplt.xlabel('MaxTemp')\nplt.ylabel('Density of observations')\n\nplt.show()\nplt.close()","6584857b":" # 2. Linear regression <a class=\"anchor\" id=\"Section2\"><\/a>","189b77a8":"We see that in the interval (-34, -30) the linear model performs better. \n\nLet's also check the predicted  distribution of the polnomial model. ","5856484f":"The polynomial model yields beter r^2 and mse errors than the linear regression model (around 0.76 and 17.6 respectively). \n\nHowever, in certain regions the variance seems to be very high, espacially at the lower extreme values. Let's compare de performance of the polynomial and the linear model in this region.","f01db6ff":"Let's see how good these models perform in the test set","0b627fbb":"Let's see how the above polynomial regression model performs in the test set.","c37e9b01":"Weather Conditions in World War Two: Is there a relationship between the daily minimum and maximum temperature? Can you predict the maximum temperature given the minimum temperature?","296db150":"<h1 align=center><font size=5>Predicting Max Temp in a day given Min Temp. <\/font><\/h1>","b511fc8e":"# 4. Conclusion. Model comparison <a class=\"anchor\" id=\"Section4\"><\/a>","30e3e80e":"Lets plot the curve of our trained model together with data set ","7d1c6f34":"This looks pretty accurate. Let's check if there is another model that fits better the data.","cea36f7b":"\n### Table of Contents\n\n* [Loading and visualizing the data](#Section1)\n* [Linear regression](#Section2)\n* [Polynomial regression](#Section3)\n* [Conclusion. Model comparison](#Section4)\n","25a851e2":"Let's plot the r^2 and mse scores per degree","a00c9fe8":"We see that the linear model seems to provide a more accurate distribution, especially in values around 30 degrees. \nMoreover, the improvements in the r^2 and mse scores are not vere significative (from 0.76 to 0.79 and from 17 to 15). Therefore, it seems reasonable to keep  the linear regression as a predictive model for this data set.","cccdb4d1":"# 1. Loading and visualizing the data <a class=\"anchor\" id=\"Section1\"><\/a>\n\n","0d94092b":"In this section, we look for the best polynomial model that fits our data. To that end we will use a Ridge model which considers a regularization parameter proportional to the L2 norm. See https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.Ridge.html\n\nFor each degree d=1, ..., n, we do a grid search along the  regularization parameters alpha. This yields:\n\n- A best estimator for each degree, i.e. the best parameters alpha.\n- associated r\u02c62 and mean squared error (mse) scores for each of these best estimators \n\n","25e4dd75":"# 3. Polynomial regression <a class=\"anchor\" id=\"Section3\"><\/a>","58892b65":"Let's compare the distribution of the predicted max temperatures  with the actual one.","8450e7ba":"Let us see how our model performs on new data."}}