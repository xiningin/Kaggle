{"cell_type":{"a866a1cc":"code","9a26be90":"code","b95738a8":"code","7b3f78ce":"code","49385961":"code","b5094559":"code","8841f41b":"code","e586f557":"code","72fb1329":"code","5ef6a26c":"code","277f09ae":"code","ee43155b":"code","5bb2fd20":"code","cca2b41d":"code","a87cae99":"code","996b9224":"code","32f4b44b":"code","bddbd455":"code","0faf0db9":"code","cf880c0b":"code","adcf9211":"code","f25408b0":"code","f9630e67":"code","ceec3afb":"markdown","5e1a1f72":"markdown","177a8167":"markdown","08ad7387":"markdown","23d485aa":"markdown","27976946":"markdown","6fbb396c":"markdown"},"source":{"a866a1cc":"%matplotlib inline\n#\u9019\u662fjuoyter notebook\u7684magic word\u02d9\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom IPython import display","9a26be90":"import os\n#\u5224\u65b7\u662f\u5426\u5728jupyter notebook\u4e0a\ndef is_in_ipython():\n    \"Is the code running in the ipython environment (jupyter including)\"\n    program_name = os.path.basename(os.getenv('_', ''))\n\n    if ('jupyter-notebook' in program_name or # jupyter-notebook\n        'ipython'          in program_name or # ipython\n        'jupyter' in program_name or  # jupyter\n        'JPY_PARENT_PID'   in os.environ):    # ipython-notebook\n        return True\n    else:\n        return False\n\n\n#\u5224\u65b7\u662f\u5426\u5728colab\u4e0a\ndef is_in_colab():\n    if not is_in_ipython(): return False\n    try:\n        from google import colab\n        return True\n    except: return False\n\n#\u5224\u65b7\u662f\u5426\u5728kaggke_kernal\u4e0a\ndef is_in_kaggle_kernal():\n    if 'kaggle' in os.environ['PYTHONPATH']:\n        return True\n    else:\n        return False\n\nif is_in_colab():\n    from google.colab import drive\n    drive.mount('\/content\/gdrive')\n\nos.environ['TRIDENT_BACKEND'] = 'pytorch'\nkaggle_kernal=None\nif is_in_kaggle_kernal():\n    os.environ['TRIDENT_HOME'] = '.\/trident'\n    \nelif is_in_colab():\n    os.environ['TRIDENT_HOME'] = '\/content\/gdrive\/My Drive\/trident'\n  \n","b95738a8":"#\u70ba\u78ba\u4fdd\u5b89\u88dd\u6700\u65b0\u7248 \n!pip install cupy\n!pip uninstall tridentx -y\n!pip install ..\/input\/trident\/tridentx-0.7.3.19-py3-none-any.whl --upgrade\n\nimport re\nimport pandas\nimport json\nimport copy\n\nimport cupy as np\n#\u8abf\u7528trident api\n\nimport random\nfrom tqdm import tqdm\nimport scipy\nimport time\nimport glob\n","7b3f78ce":"import trident as T\nfrom trident import *","49385961":"human_faces=glob.glob('..\/input\/face-recognition-dataset\/Extracted Faces\/Extracted Faces\/*\/*.*g')\nanime_faces=glob.glob('..\/input\/animefacedataset\/images\/*.*g')\nrandom.shuffle(anime_faces)\nprint(len(human_faces))\nprint(len(anime_faces))","b5094559":"!pip install face_recognition\n!pip install imutils\n#\u4e0b\u8f09\u9810\u8a13\u7df4\u52d5\u756b\u4eba\u81c9\u6aa2\u6e2c\ndownload_file_from_google_drive('1NckKw7elDjQTllRxttO87WY7cnQwdMqz',  filename='checkpoint_landmark_191116.pth.tar')\n\nimport torch\nimport torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\nfrom collections import OrderedDict\n\n\n# Cacaded Face Alignment\nclass CFA(nn.Module):\n    def __init__(self, output_channel_num, checkpoint_name=None):\n        super(CFA, self).__init__()\n\n        self.output_channel_num = output_channel_num\n        self.stage_channel_num = 128\n        self.stage_num = 2\n\n        self.features = nn.Sequential(\n            nn.Conv2d(  3,  64, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d( 64,  64, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d( 64, 128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(128, 128, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(128, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n\n            # nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            # nn.MaxPool2d(kernel_size=2, stride=2),\n            # nn.Conv2d(256, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            # nn.Conv2d(512, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True),\n            # nn.Conv2d(512, 512, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True))\n\n            nn.Conv2d(256, 256, kernel_size=3, dilation=1, padding=1), nn.ReLU(inplace=True))\n        \n        self.CFM_features = nn.Sequential(\n            #nn.Conv2d(512, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1), nn.ReLU(inplace=True),\n            nn.Conv2d(256, self.stage_channel_num, kernel_size=3, padding=1), nn.ReLU(inplace=True))\n\n        # cascaded regression\n        stages = [self.make_stage(self.stage_channel_num)]\n        for _ in range(1, self.stage_num):\n            stages.append(self.make_stage(self.stage_channel_num + self.output_channel_num))\n        self.stages = nn.ModuleList(stages)\n        \n        # initialize weights\n        if checkpoint_name:\n            snapshot = torch.load(checkpoint_name,map_location=torch.device('cuda'))\n            self.load_state_dict(snapshot['state_dict'])\n        else:\n            self.load_weight_from_dict()\n    \n\n    def forward(self, x):\n        feature = self.features(x)\n        feature = self.CFM_features(feature)\n        heatmaps = [self.stages[0](feature)]\n        for i in range(1, self.stage_num):\n            heatmaps.append(self.stages[i](torch.cat([feature, heatmaps[i - 1]], 1)))\n        return heatmaps\n    \n\n    def make_stage(self, nChannels_in):\n        layers = []\n        layers.append(nn.Conv2d(nChannels_in, self.stage_channel_num, kernel_size=3, padding=1))\n        layers.append(nn.ReLU(inplace=True))\n        for _ in range(4):\n            layers.append(nn.Conv2d(self.stage_channel_num, self.stage_channel_num, kernel_size=3, padding=1))\n            layers.append(nn.ReLU(inplace=True))\n        layers.append(nn.Conv2d(self.stage_channel_num, self.output_channel_num, kernel_size=3, padding=1))\n        return nn.Sequential(*layers)\n\n\n    def load_weight_from_dict(self):\n        model_urls = 'https:\/\/download.pytorch.org\/models\/vgg16-397923af.pth'\n        weight_state_dict = model_zoo.load_url(model_urls)\n        all_parameter = self.state_dict()\n        all_weights   = []\n        for key, value in all_parameter.items():\n            if key in weight_state_dict:\n                all_weights.append((key, weight_state_dict[key]))\n            else:\n                all_weights.append((key, value))\n        all_weights = OrderedDict(all_weights)\n        self.load_state_dict(all_weights)","8841f41b":"import numpy as np\nimport torch\nfrom torchvision import transforms\nimport cv2\nfrom PIL import Image, ImageDraw,ImageFont\n\n \n\n# param\nnum_landmark = 24\n\ncheckpoint_name =os.path.join(get_trident_dir(),'downloads','checkpoint_landmark_191116.pth.tar')\n\n\n# detector\nanime_landmark_detector = CFA(output_channel_num=num_landmark + 1, checkpoint_name=checkpoint_name).cuda()\nanime_landmark_detector.trainable=False\n\ndef detect_anime_faces(img_path):\n    img_width = 128\n    # transform\n    normalize   = transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                       std=[0.5, 0.5, 0.5])\n    train_transform = [transforms.ToTensor(), normalize]\n    train_transform = transforms.Compose(train_transform)\n\n    # input image & detect face\n    img = cv2.imread(img_path)\n    img = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n    draw = ImageDraw.Draw(img)\n   \n    x_, y_, w_, h_ =0,0,img.width,img.height\n\n    # adjust face size\n    x = max(x_ - w_ \/ 8, 0)\n    rx = min(x_ + w_ * 9 \/ 8, img.width)\n    y = max(y_ - h_ \/ 4, 0)\n    by = y_ + h_\n    w = rx - x\n    h = by - y\n\n    # draw result of face detection\n    draw.rectangle((x, y, x + w, y + h), outline=(0, 0, 255), width=3)\n\n    # transform image\n    img_tmp = img.crop((x, y, x+w, y+h))\n    img_tmp = img_tmp.resize((img_width, img_width), Image.BICUBIC)\n    img_tmp = train_transform(img_tmp)\n    img_tmp = img_tmp.unsqueeze(0).cuda()\n\n    # estimate heatmap\n    heatmaps = anime_landmark_detector(img_tmp)\n    print(len(heatmaps),heatmaps)\n    heatmaps = heatmaps[-1].cpu().detach().numpy()[0]\n    print(heatmaps.shape)\n    font = ImageFont.truetype(r'DejaVuSans.ttf', 8)  \n    # calculate landmark position\n    for i in range(num_landmark):\n        heatmaps_tmp = cv2.resize(heatmaps[i], (img_width, img_width), interpolation=cv2.INTER_CUBIC)\n        landmark = np.unravel_index(np.argmax(heatmaps_tmp), heatmaps_tmp.shape)\n        landmark_y = landmark[0] * h \/ img_width\n        landmark_x = landmark[1] * w \/ img_width\n        print(i,'({0},{1})'.format(landmark_x,landmark_y))\n\n        # draw landmarks\n        draw.ellipse((x + landmark_x - 1, y + landmark_y - 1, x + landmark_x + 1, y + landmark_y + 1), fill=(255, 0, 0))\n        if 5<=i<10:\n            draw.text((x + landmark_x - 1, y + landmark_y - 1),str(i), font=font,fill=(0, 64, 128))\n    return img.resize((img.width*5,img.height*5))\n\n\n","e586f557":"display.display(detect_anime_faces(random.choice(anime_faces)))","72fb1329":"import dlib\nimport imutils\nfrom imutils.face_utils import *\ndownload_file_from_google_drive('1Nw45nQtnrykB-P6QyIHm70XriJUlSsII',  filename='shape_predictor_68_face_landmarks.dat')\n#\u5ba3\u544a\u81c9\u90e8\u5075\u6e2c\u5668\uff0c\u4ee5\u53ca\u8f09\u5165\u9810\u8a13\u7df4\u7684\u81c9\u90e8\u7279\u5fb5\u9ede\u6a21\u578b\ndetector = dlib.get_frontal_face_detector()\nhuman_landmark_detector = dlib.shape_predictor(os.path.join(get_trident_dir(),'downloads','shape_predictor_68_face_landmarks.dat'))\n\ndef detect_human_faces(imag_path):\n    img=cv2.imread(imag_path)\n    face_rects = detector(img,2)\n    for i, d in enumerate(face_rects):\n        #\u8b80\u53d6\u6846\u5de6\u4e0a\u53f3\u4e0b\u5ea7\u6a19\n        x1 = d.left()\n        y1 = d.top()\n        x2 = d.right()\n        y2 = d.bottom()\n        #\u6839\u64da\u6b64\u5ea7\u6a19\u7bc4\u570d\u8b80\u53d6\u81c9\u90e8\u7279\u5fb5\u9ede\n        shape = human_landmark_detector(img, d)\n        #\u5c07\u7279\u5fb5\u9ede\u8f49\u70banumpy\n        shape = shape_to_np(shape)# (68,2)\n        #\u6309\u7167\u7279\u5fb5\u9ede\u756b\u5713\n        for (x, y) in shape:\n            cv2.circle(img, (x, y), 2, (215, 120, 0), -1)\n        #\u5728\u81c9\u90e8\u4f4d\u7f6e\u6253\u6846\n        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 4, cv2.LINE_AA)\n        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    return array2image(img)","5ef6a26c":"detect_human_faces(random.choice(human_faces))","277f09ae":"#\u5169\u8005\u7684\u5b9a\u7fa9\u5728\u773c\u775b\u90e8\u5206\u5dee\u7570\u8f03\u5927\uff0c\u56e0\u6b64\u50c5\u63a1\u53d6\u5b9a\u7fa9\u76f8\u5bb9\u7684\u9ede\nlandmark_mapping=OrderedDict()\nlandmark_mapping[0]=0\nlandmark_mapping[1]=8\nlandmark_mapping[2]=16\nlandmark_mapping[3]=17\nlandmark_mapping[4]=19\nlandmark_mapping[5]=21\nlandmark_mapping[6]=22\nlandmark_mapping[7]=24\nlandmark_mapping[8]=26\nlandmark_mapping[9]=33\nlandmark_mapping[10]=36\nlandmark_mapping[12]=39\nlandmark_mapping[15]=42\nlandmark_mapping[17]=45\nlandmark_mapping[20]=48\nlandmark_mapping[21]=51\nlandmark_mapping[22]=54\nlandmark_mapping[23]=57\n","ee43155b":"   \n    \ndef img2landmarks_human(img_data):\n    results=[]\n    for n in range(len(img_data)):\n        landmarks=[]\n        img=reverse_image_backend_adaption(img_data[n]*127.5+127.5).astype(np.uint8)\n        \n        img=cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n        \n        face_rects = detector(img,2)\n        \n        for i, d in enumerate(face_rects):\n            if i==0:\n                #\u8b80\u53d6\u6846\u5de6\u4e0a\u53f3\u4e0b\u5ea7\u6a19\n                x1 = d.left()\n                y1 = d.top()\n                x2 = d.right()\n                y2 = d.bottom()\n                #\u6839\u64da\u6b64\u5ea7\u6a19\u7bc4\u570d\u8b80\u53d6\u81c9\u90e8\u7279\u5fb5\u9ede\n                shape = human_landmark_detector(img, d)\n\n                #\u5c07\u7279\u5fb5\u9ede\u8f49\u70banumpy\n                shape = shape_to_np(shape)# (68,2)\n\n                for k in landmark_mapping.keys():\n                    v=landmark_mapping[k]\n                    landmarks.append([shape[v,0]\/128.0,shape[v,1]\/128.0])\n        if len(landmarks)!=18:\n            results.append(np.zeros(36,dtype=np.float32))\n        else:\n            results.append(np.array(landmarks,dtype=np.float32).reshape(-1))\n    \n    \n    results= np.stack(results,axis=0)\n    return results\n\ndef img2landmarks_anime(img_data):\n    img_width=128\n    results=[]\n    #\u4fee\u6539\u70ba\u6279\u6b21\u63a8\u8ad6\uff0c\u4ee5\u52a0\u901f\u7522\u751f\u5efa\u6a21\u6a23\u672c\u652f\u52d5\u756b\u4eba\u95dc\u9375\u9ede\u901f\u5ea6\n    batch_heatmaps = anime_landmark_detector(to_tensor(img_data).cuda())\n \n    batch_heatmaps = to_numpy(batch_heatmaps[-1])\n    \n    for n in range(len(img_data)):\n        landmarks=[]\n        \n        heatmaps = batch_heatmaps[n]\n        \n\n        # calculate landmark position\n        for i in range(24):\n            heatmaps_tmp = cv2.resize(heatmaps[i], (img_width, img_width), interpolation=cv2.INTER_CUBIC)\n            landmark = np.unravel_index(np.argmax(heatmaps_tmp), heatmaps_tmp.shape)\n            landmark_y = landmark[0] \/ img_width\n            landmark_x = landmark[1]  \/ img_width \n            if i in landmark_mapping.keys():\n                landmarks.append([landmark_x,landmark_y])\n        results.append(np.array(landmarks,dtype=np.float32).reshape(-1))\n  \n    results= np.stack(results,axis=0)\n    return results\n            ","5bb2fd20":"ds1=ImageDataset(human_faces,symbol='human_faces')\nds2=ImageDataset(anime_faces,symbol='anime_faces')\n\ndata_provider=DataProvider(traindata=Iterator(data=ds1,unpair=ds2))\ndata_provider.image_transform_funcs=[\n                     Resize((128,128)),\n                     RandomAdjustGamma(gamma_range=(0.6,1.5)),\n                     RandomAdjustContrast(value_range=(0.6, 1.5)),\n                     RandomAdjustHue(value_range=(-0.2, 0.2)),\n                     Normalize(127.5,127.5)]","cca2b41d":"human_imgs,anime_imgs=data_provider.next()\n\nprint(human_imgs.shape)\nprint(anime_imgs.shape)\npred_human_landmarks=img2landmarks_human(human_imgs)\nprint(pred_human_landmarks.shape,pred_human_landmarks.dtype)\nprint(pred_human_landmarks)\npred_anime_landmarks=img2landmarks_anime(anime_imgs)\n\nprint(pred_anime_landmarks.shape,pred_anime_landmarks.dtype)","a87cae99":"data_provider.preview_images()","996b9224":"encoder=Sequential(\n    Conv2d_Block((5,5),32,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(32,128,128)\n    Conv2d_Block((3,3),64,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(64,64,64)\n    Conv2d_Block((3,3),64,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(64,32,32)\n    Conv2d_Block((3,3),128,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False,dropout_rate=0.2),#(128,16,16)\n    Conv2d_Block((3,3),128,strides=2,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,8,8)\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization=None,use_bias=False),#(128,4,4)\n    Reshape((-1)), #(2048)\n    Dense(256,activation=None,use_bias=False),\n    L2Norm(),\n)\nencoder.share_memory()\nencoder[-1].keep_output=True\n\ndecoder_human=ModuleDict({\n'human_output':\n    Sequential(\n        Dense(128*4*4,activation=None,use_bias=False), #(2048))\n        BatchNorm(),\n        Reshape((128,4,4)), #(128,4,4)\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False) ,#((128,4,4))\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,8,8)\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False,dropout_rate=0.2),#(128,16,16)\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,32,32)\n        Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,64,64)\n        Upsampling2d(scale_factor=2,mode='bicubic'),\n        Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,128,128)\n        Conv2d((1,1),3,strides=1,auto_pad=True,activation='tanh',use_bias=False)\n    ),\n'human_landmarks_output':\n    Dense((18*2),activation='sigmoid')\n},is_multicasting=True)\n\ndecoder_anime=ModuleDict({\n'anime_output':Sequential(\n    Dense(128*4*4,activation=None,use_bias=False), #(2048))\n    BatchNorm(),\n    Reshape((128,4,4)), #(128,4,4)\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False) ,#((128,4,4))\n    Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,8,8)\n    Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False,dropout_rate=0.2),#(128,16,16)\n    Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,32,32)\n    Upsampling2d(scale_factor=2,mode='pixel_shuffle'),\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,64,64)\n    Upsampling2d(scale_factor=2,mode='bicubic'),\n    Conv2d_Block((3,3),128,strides=1,auto_pad=True,activation='leaky_relu',normalization='batch',use_bias=False),#(128,128,128)\n    Conv2d((1,1),3,strides=1,auto_pad=True,activation='tanh',use_bias=False)\n    ),\n'anime_landmarks_output':\n    Dense((18*2),activation='sigmoid')\n},is_multicasting=True)","32f4b44b":"autoencoder_human=Model(input_shape=(3,128,128), output=Sequential(encoder,decoder_human))\nautoencoder_anime=Model(input_shape=(3,128,128), output=Sequential(encoder,decoder_anime))\nprint(autoencoder_human.signature)\nprint(autoencoder_anime.signature)\n\n#autoencoder_combine.summary()","bddbd455":"import cv2\ndef update_datafeed_human(training_context):\n    training_context['data_feed']=OrderedDict()\n    training_context['data_feed']['x']='human_faces'\n    training_context['data_feed']['output']='human_output'\n    training_context['data_feed']['target']='human_faces'\n    training_context['train_data']['human_landmarks']=to_tensor(img2landmarks_human(to_numpy(training_context['train_data']['human_faces']).copy()))\n    training_context['train_data']['embedded1']=training_context['current_model'][0][-1].output.copy()\n    _=training_context['current_model'](training_context['train_data']['human_output'].float())\n    training_context['train_data']['embedded2']=training_context['current_model'][0][-1].output\n    \n    \ndef update_datafeed_anime(training_context):\n    training_context['data_feed']=OrderedDict()\n    training_context['data_feed']['x']='anime_faces'\n    training_context['data_feed']['output']='anime_output'\n    training_context['data_feed']['target']='anime_faces'\n    training_context['train_data']['anime_landmarks']=to_tensor(img2landmarks_anime(to_numpy(training_context['train_data']['anime_faces']).copy()))\n    training_context['train_data']['embedded1']=training_context['current_model'][0][-1].output.copy()\n    _=training_context['current_model'](training_context['train_data']['anime_output'].float())\n    training_context['train_data']['embedded2']=training_context['current_model'][0][-1].output\n    \n    if training_context['steps']>0 and training_context['steps']%len(data_provider.traindata.data)==0:\n        random.shuffle(data_provider.traindata.unpair.items)\n    \n    \ndef draw_human(training_context):\n    model=training_context['current_model']\n    model.eval()\n    train_data=training_context['train_data']\n    input_data=to_numpy(train_data['human_faces'])\n    result_data=model(input_data)\n    output_data=to_numpy(result_data['human_output'])\n    landmark_data=np.round(to_numpy(result_data['human_landmarks_output']).reshape((len(input_data),18,2))*128).astype(np.uint8)\n    output_data2=to_numpy(autoencoder_anime(train_data['human_faces'])['anime_output'])\n    \n    input_list=[]\n    output_list=[]\n    output2_list=[]\n    landmarks_list=[]\n    for i in range(4):\n        input_list.append(reverse_image_backend_adaption(np.clip(input_data[i]*127.5+127.5,0,255)))\n        img=reverse_image_backend_adaption(np.clip(output_data[i]*127.5+127.5,0,255)).astype(np.float32).copy()\n        output_list.append(img.copy())\n        #print(img.shape)\n        #output=cv2.cvtColor(output,cv2.COLOR_RGB2GBR)\n        for (x, y) in landmark_data[i]:\n            \n            cv2.circle(img, (x, y), 4, (255, 255, 0), -1)\n        #output=cv2.cvtColor(output,cv2.COLOR_BGR2RGB)\n        landmarks_list.append(img)\n        output2_list.append(reverse_image_backend_adaption(np.clip(output_data2[i]*127.5+127.5,0,255)))\n        \n        \n    input_list=np.concatenate(input_list,axis=1)\n    output_list=np.concatenate(output_list,axis=1)\n    output2_list=np.concatenate(output2_list,axis=1)\n    landmarks_list=np.concatenate(landmarks_list,axis=1)\n    model.train()\n    final=np.concatenate([input_list,output_list,landmarks_list,output2_list],axis=0)\n\n    display.display(array2image(final))\n        \n    \ndef draw_anime(training_context):\n    model=training_context['current_model']\n    model.eval()\n    train_data=training_context['train_data']\n    input_data=to_numpy(train_data['anime_faces'])\n    result_data=model(input_data)\n    output_data=to_numpy(result_data['anime_output'])\n    landmark_data=np.round(to_numpy(result_data['anime_landmarks_output']).reshape((len(input_data),18,2))*128).astype(np.uint8)\n    output_data2=to_numpy(autoencoder_human(train_data['anime_faces'])['human_output'])\n    input_list=[]\n    output_list=[]\n    output2_list=[]\n    landmarks_list=[]\n    for i in range(4):\n        input_list.append(reverse_image_backend_adaption(np.clip(input_data[i]*127.5+127.5,0,255)))\n        img=reverse_image_backend_adaption(np.clip(output_data[i]*127.5+127.5,0,255)).astype(np.float32).copy()\n        output_list.append(img.copy())\n        #print(img.shape)\n        #output=cv2.cvtColor(output,cv2.COLOR_RGB2GBR)\n        for (x, y) in landmark_data[i]:\n            cv2.circle(img, (x, y), 4, (255, 255, 0), -1)\n        #output=cv2.cvtColor(output,cv2.COLOR_BGR2RGB)\n        landmarks_list.append(img)\n        output2_list.append(reverse_image_backend_adaption(np.clip(output_data2[i]*127.5+127.5,0,255)))\n    input_list=np.concatenate(input_list,axis=1)\n    output_list=np.concatenate(output_list,axis=1)\n    output2_list=np.concatenate(output2_list,axis=1)\n    landmarks_list=np.concatenate(landmarks_list,axis=1)\n    \n    model.train()\n    final=np.concatenate([input_list,output_list,landmarks_list,output2_list],axis=0)\n\n    display.display(array2image(final))\n    \n    \n\n    \n# autoencoder_human.training_context['data_feed']=OrderedDict()\n# autoencoder_human.training_context['data_feed']['x']='human_faces'\n# autoencoder_human.training_context['data_feed']['output']='human_output'\n# autoencoder_human.training_context['data_feed']['target']='human_faces'\n\n\n# autoencoder_anime.training_context['data_feed']=OrderedDict()\n# autoencoder_anime.training_context['data_feed']['x']='anime_faces'\n# autoencoder_anime.training_context['data_feed']['output']='anime_output'\n# autoencoder_anime.training_context['data_feed']['target']='anime_faces'","0faf0db9":"torch.cuda.synchronize()\ntorch.cuda.empty_cache()\ngc.collect()","cf880c0b":"def anime_landmark_loss(anime_landmarks_output,anime_landmarks):\n    return ((anime_landmarks_output-anime_landmarks)**2).sum()\/len(anime_landmarks_output)\n\ndef human_landmark_loss(human_landmarks_output,human_landmarks):\n    agg_target=human_landmarks.sum(-1)\n    mask=agg_target!=0\n    return ((human_landmarks_output[mask,:]-human_landmarks[mask,:])**2).sum()\/mask.float().sum()\n\n#\u5c07\u539f\u59cb\u8f38\u5165\u5716\u7247\u8207autoencoder\u751f\u6210\u5716\u7247\u653e\u5165\u7de8\u78bc\u5668\u6240\u7522\u751f\u7684\u7279\u5fb5\u5411\u91cf\u7684L1 loss\ndef feature_align_loss(embedded1,embedded2):\n    embedded1=embedded1.detach()\n    return abs(embedded2-embedded1).sum()\/32\n\n#\u6839\u64da\u81c9\u90e8\u7279\u5fb5\u9ede\u627e\u51fa\u81c9\u90e8\u7684\u5de6\u4e0a\u53f3\u4e0b\u5ea7\u6a19\uff0c\u91dd\u5c0d\u9019\u500b\u5340\u57df\u7684L1 LOSS\u505a\u9032\u4e00\u6b65\u5f37\u5316\ndef anime_landmark_attention_loss(anime_landmarks,anime_output,anime_faces):\n    agg_target=anime_landmarks.sum(-1)\n    mask=agg_target!=0\n    \n    anime_output=anime_output[mask,:,:,:]\n    anime_faces=anime_faces[mask,:,:,:]\n    anime_landmarks=anime_landmarks[mask,:].reshape((len(anime_output),18,2))*128\n    total_loss=to_tensor(0.0).float().requires_grad=True\n    for n in range(len(anime_output)):\n        x1=anime_landmarks[n,:,0].min().long()\n        y1=anime_landmarks[n,:,1].min().long()\n        x2=anime_landmarks[n,:,0].max().long()\n        y2=anime_landmarks[n,:,1].max().long()\n        #print(x1,y1,x2,y2)\n        attention_anime_output=anime_output[n,:,y1:y2,x1:x2]\n        attention_anime_faces=anime_faces[n,:,y1:y2,x1:x2]\n\n        #print('attention_anime_output',attention_anime_output.shape,'attention_anime_faces',attention_anime_faces.shape)\n        #print('attention_anime_output',attention_anime_output.mean(),'attention_anime_faces',attention_anime_faces.mean())\n        total_loss=total_loss+ square(attention_anime_output-attention_anime_faces).sum()\n    return total_loss\/(128*len(anime_output))\n\ndef human_landmark_attention_loss(human_landmarks,human_output,human_faces):\n    agg_target=human_landmarks.sum(-1)\n    mask=agg_target!=0\n    \n    human_output=human_output[mask,:,:,:]\n    human_faces=human_faces[mask,:,:,:]\n    human_landmarks=human_landmarks[mask,:].reshape((len(human_output),18,2))*128\n    #print(human_landmarks.shape,human_landmarks)\n    total_loss=to_tensor(0.0).float().requires_grad=True\n    \n    for n in range(len(human_output)):\n        \n        x1=human_landmarks[n,:,0].min().long()\n        y1=human_landmarks[n,:,1].min().long()\n        x2=human_landmarks[n,:,0].max().long()\n        y2=human_landmarks[n,:,1].max().long()\n        #print(x1,y1,x2,y2)\n        attention_human_output=human_output[n,:,y1:y2,x1:x2]\n        attention_human_faces=human_faces[n,:,y1:y2,x1:x2]\n        #print('attention_human_output',attention_human_output.shape,'attention_human_faces',attention_human_faces.shape)\n        #print('attention_human_output',attention_human_output.mean(),'attention_human_faces',attention_human_faces.mean())\n        total_loss=total_loss+ square(attention_human_output-attention_human_faces).sum()\n    return total_loss\/(128*len(human_output))\n    \n\ndef anime_rmse_landmarks(anime_landmarks_output,anime_landmarks):\n    return rmse(anime_landmarks_output,anime_landmarks)\n\ndef human_rmse_landmarks(human_landmarks_output,human_landmarks):\n    agg_target=human_landmarks.sum(-1)\n    mask=agg_target!=0\n    return rmse(human_landmarks_output[mask,:],human_landmarks[mask,:])\n\nfrom trident.models import vgg \nvgg_used=vgg.VGG16(pretrianed=True,include_top=False).model\n\n#.with_loss(MS_SSIMLoss(window_size=11, window_sigma=1.5, data_range=2),loss_weight=4)\\\nautoencoder_human.with_optimizer(optimizer=AdaBelief,lr=5e-4,betas=(0.9, 0.999),weight_decay=4e-5)\\\n    .with_loss(L1Loss,loss_weight=8)\\\n    .with_loss(L2Loss,loss_weight=32)\\\n    .with_loss(human_landmark_loss,loss_weight=32)\\\n    .with_loss(human_landmark_attention_loss,loss_weight=0.1)\\\n    .with_loss(PerceptionLoss(net=vgg_used),loss_weight=0.01)\\\n    .with_loss(feature_align_loss)\\\n    .with_metric(rmse,name='rmse')\\\n    .with_metric(psnr,name='psnr')\\\n    .with_metric(human_rmse_landmarks,name='landmark_rmse',print_only=True)\\\n    .with_regularizer('l2',reg_weight=1e-5)\\\n    .with_initializer(kaiming_normal)\\\n    .with_model_save_path('.\/Models\/autoencoder_human.pth')\\\n    .with_grad_clipping(3)\\\n    .with_learning_rate_scheduler(CosineLR(frequency=3000,unit='batch',max_lr=5e-4, min_lr=1e-8))\\\n    .trigger_when(when='on_batch_end',frequency=50,unit='batch',action=draw_human)\\\n    .trigger_when(when='on_loss_calculation_start',frequency=1,unit='batch',action=update_datafeed_human)\\\n    .with_automatic_mixed_precision_training()\n\nautoencoder_anime.with_optimizer(optimizer=AdaBelief,lr=5e-4,betas=(0.9, 0.999),weight_decay=4e-5)\\\n    .with_loss(L1Loss,loss_weight=8)\\\n    .with_loss(L2Loss,loss_weight=32)\\\n    .with_loss(anime_landmark_loss,loss_weight=32)\\\n    .with_loss(anime_landmark_attention_loss,loss_weight=0.1)\\\n    .with_loss(PerceptionLoss(net=vgg_used),loss_weight=0.01)\\\n    .with_loss(feature_align_loss)\\\n    .with_loss(EdgeLoss,loss_weight=16)\\\n    .with_metric(rmse,name='rmse')\\\n    .with_metric(psnr,name='psnr')\\\n    .with_metric(anime_rmse_landmarks,name='landmark_rmse',print_only=True)\\\n    .with_regularizer('l2',reg_weight=1e-5)\\\n    .with_regularizer('total_variation_norm_reg',reg_weight=32)\\\n    .with_initializer(kaiming_normal)\\\n    .with_grad_clipping(3)\\\n    .with_model_save_path('.\/Models\/autoencoder_anime.pth')\\\n    .with_learning_rate_scheduler(CosineLR(frequency=3000,unit='batch',max_lr=5e-4, min_lr=1e-8))\\\n    .trigger_when(when='on_batch_end',frequency=50,unit='batch',action=draw_anime)\\\n    .trigger_when(when='on_loss_calculation_start',frequency=1,unit='batch',action=update_datafeed_anime)\\\n    .with_automatic_mixed_precision_training()\n\n#\u5982\u679c\u8981\u9084\u539f\u5b58\u6a94\u7684\u6a21\u578b\uff0c\u8a18\u5f97\u8981\u5728with_initializer\u4e4b\u5f8c\u518d\u9084\u539f\uff0c\u5426\u5247\u88ab\u521d\u59cb\u5316\u6389\u7b49\u65bc\u767d\u8a13\u7df4\u4e86\u3002\n\n#\u9019\u6bb5\u8f09\u5165\u6a21\u578b\u70ba\u4f55\u5beb\u7684\u6bd4\u8f03\u8907\u96dc\uff0c\u56e0\u70ba\u9019\u6b21\u6d89\u53ca\u5230\u6a21\u578b\u7d50\u69cb\u4e5f\u767c\u751f\u4e86\u8b8a\u5316\uff0c\u6240\u4ee5\u7576\u4ed6\u6aa2\u67e5\u5230\u9019\u500b\u6a21\u578b\u7d50\u69cb\u4e0d\u4e00\u81f4(\u6211\u5011\u4e4b\u524d\u7248\u672c\u7684autoencoder)\n#\u9019\u908a\u6703\u76f4\u63a5\u5354\u52a9\u4fee\u6b63\u7d50\u69cb\u8b8a\u66f4\uff0c\u9019\u6a23\u5927\u5bb6\u5c31\u53ef\u4ee5\u6cbf\u7528\u4e4b\u524d\u8a13\u7df4\u7684\u6a21\u578b\u4e86\nis_resume=True\nif is_resume and os.path.exists('.\/Models\/autoencoder_human.pth'):\n    try:\n        autoencoder_human.load_model('.\/Models\/autoencoder_human.pth')\n    except:\n        old_model = torch.load('.\/Models\/autoencoder_human.pth',map_location=torch.device('cuda'))\n        is_version2=len([k for k,v in old_model[1].named_parameters() if 'human_output.' in k])>0\n        if is_version2:\n            statedict=old_model.state_dict()\n            if '1.human_output.13.conv.weight' in statedict:\n                print(statedict['1.human_output.13.conv.weight'].shape)\n                statedict['1.human_output.13.conv.weight']=concate([statedict['1.human_output.13.conv.weight']]*4,axis=1)\n                print(statedict['1.human_output.13.conv.weight'].shape)\n            autoencoder_human.model.load_state_dict(statedict)\n        else:\n            with torch.no_grad():\n            \n                for (k, v), (k1, v1) in zip(old_model[1].named_parameters(), autoencoder_human.model[1]['human_output'].named_parameters()):\n                    if int_shape(v) == int_shape(v1) and k1.replace('1.human_output.', '1.') == k:\n                        v1 = v\n\n                        print(k1, ' load weight success')\n                    elif '13.conv.weight' in k and list(int_shape(v))==[128, 32, 3, 3]  and k1.replace('1.human_output.', '1.') == k:\n\n                        print(k1, ' pass success')\n                        pass\n                    else:\n                        print(k1, ' load weight fail')\n                        print(k,'v', v.shape,k1,'v1',v1.shape)\n                if isinstance(autoencoder_human.model[1]['human_output'][12],Upsampling2d):\n                    if autoencoder_human.model[1]['human_output'][12].mode=='pixel_shuffle':\n                        autoencoder_human.model[1]['human_output'][12].mode='bicubic'\n\n\nelif os.path.exists('..\/input\/face-avatar\/Models\/autoencoder_human.pth'):\n\n    try:\n        autoencoder_human.load_model('..\/input\/face-avatar\/Models\/autoencoder_human.pth')\n    except:\n        \n        old_model = torch.load('..\/input\/face-avatar\/Models\/autoencoder_human.pth',map_location=torch.device('cuda'))\n        is_version2=len([k for k,v in old_model[1].named_parameters() if 'human_output.' in k])>0\n        print('is_version2',is_version2)\n        statedict=old_model.state_dict()\n        if is_version2:\n            \n            if '1.human_output.13.conv.weight' in statedict:\n                print(statedict['1.human_output.13.conv.weight'].shape)\n                statedict['1.human_output.13.conv.weight']=concate([statedict['1.human_output.13.conv.weight']]*4,axis=1)\n                print(statedict['1.human_output.13.conv.weight'].shape)\n            autoencoder_human.model.load_state_dict(statedict)\n        else:\n            \n            with torch.no_grad():\n\n                for k1, v1 in  autoencoder_human.model[1]['human_output'].named_parameters():\n                    if int_shape(v) == int_shape(v1) and k1.replace('human_output.', '1.') in statedict:\n                        v1=statedict[k1.replace('human_output.', '1.')]\n                        print(k1, ' load weight success')\n                    elif '13.conv.weight' in k and list(int_shape(v))==[128, 32, 3, 3]  and k1.replace('1.human_output.', '1.') == k:\n                        print(k1, ' pass success')\n                        pass\n                    else:\n                        print(k1, ' load weight fail')\n                        print(k,'v', v.shape,k1,'v1',v1.shape)\n\n                if isinstance(autoencoder_human.model[1]['human_output'][12],Upsampling2d):\n                    if autoencoder_human.model[1]['human_output'][12].mode=='pixel_shuffle':\n                        autoencoder_human.model[1]['human_output'][12].mode='bicubic'\n            \n        \n\nif is_resume and os.path.exists('.\/Models\/autoencoder_anime.pth'):\n    try:\n        autoencoder_anime.load_model('.\/Models\/autoencoder_anime.pth')\n    except:\n        \n        old_model = torch.load('.\/Models\/autoencoder_anime.pth')\n        is_version2=len([k for k,v in old_model[1].named_parameters() if 'anime_output.' in k])>0\n        print('is_version2',is_version2)\n        if is_version2:\n            statedict=old_model.state_dict()\n            if '1.anime_output.13.conv.weight' in statedict:\n                print(statedict['1.anime_output.13.conv.weight'].shape)\n                statedict['1.anime_output.13.conv.weight']=concate([statedict['1.anime_output.13.conv.weight']]*4,axis=1)\n                print(statedict['1.anime_output.13.conv.weight'].shape)\n            autoencoder_anime.model.load_state_dict(statedict)\n        else:\n            with torch.no_grad():\n                for (k, v), (k1, v1) in zip(old_model[1].named_parameters(), autoencoder_anime.model[1]['anime_output'].named_parameters()):\n                    if int_shape(v) == int_shape(v1) and k1.replace('1.anime_output.', '1.') == k:\n                        v1 = v\n                        print(k1, ' load weight success')\n                    elif '13.conv.weight' in k and list(int_shape(v))==[128, 32, 3, 3]  and k1.replace('1.anime_output.', '1.') == k:\n                        print(k1, ' pass success')\n                        pass\n                    else:\n                        print(k1, ' load weight fail')\n                        print(k,'v', v.shape,k1,'v1',v1.shape)\n                if isinstance(autoencoder_anime.model[1]['anime_output'][12],Upsampling2d):\n                    if autoencoder_anime.model[1]['anime_output'][12].mode=='pixel_shuffle':\n                        autoencoder_anime.model[1]['anime_output'][12].mode='bicubic'\n\nelif os.path.exists('..\/input\/face-avatar\/Models\/autoencoder_anime.pth'):\n\n    try:\n        autoencoder_anime.load_model('..\/input\/face-avatar\/Models\/autoencoder_anime.pth')\n    except:\n        old_model = torch.load('..\/input\/face-avatar\/Models\/autoencoder_anime.pth',map_location=torch.device('cuda'))\n        is_version2=len([k for k,v in old_model[1].named_parameters() if 'anime_output.' in k])>0\n        print('is_version2',is_version2)\n        if is_version2:\n            statedict=old_model.state_dict()\n            if '1.anime_output.13.conv.weight' in statedict:\n                print(statedict['1.anime_output.13.conv.weight'].shape)\n                statedict['1.anime_output.13.conv.weight']=concate([statedict['1.anime_output.13.conv.weight']]*4,axis=1)\n                print(statedict['1.anime_output.13.conv.weight'].shape)\n            autoencoder_anime.model.load_state_dict(statedict)\n        else:\n            with torch.no_grad():\n                statedict=old_model.state_dict()\n                for k1, v1 in  autoencoder_anime.model[1]['anime_output'].named_parameters():\n                    \n                    if int_shape(v) == int_shape(v1) and k1.replace('1.anime_output.', '1.') == k:\n                        v1.copy_(v)\n                        print(k1, ' load weight success')\n                    elif '13.conv.weight' in k and list(int_shape(v))==[128, 32, 3, 3]  and k1.replace('1.anime_output.', '1.') == k:\n                        print(k1, ' pass success')\n                        pass\n                    else:\n                        print(k1, ' load weight fail')\n                        print(k,'v', v.shape,k1,'v1',v1.shape)\n                if isinstance(autoencoder_anime.model[1]['anime_output'][12],Upsampling2d):\n                    if autoencoder_anime.model[1]['anime_output'][12].mode=='pixel_shuffle':\n                        autoencoder_anime.model[1]['anime_output'][12].mode='bicubic'\n    \n#\u78ba\u8a8d\u4e00\u4e0b\u5169\u500b\u81ea\u7de8\u78bc\u5668\u7684\u7de8\u78bc\u5668\u6b0a\u503c\u662f\u5426\u4e00\u81f4\nfor p1,p2 in zip(autoencoder_human.model[0].parameters(),autoencoder_anime.model[0].parameters()):\n    print(np.array_equal(to_numpy(p1),to_numpy(p2)))\n\nautoencoder_human.summary()\nautoencoder_anime.summary()","adcf9211":"torch.cuda.synchronize()\ntorch.cuda.empty_cache()\ngc.collect()","f25408b0":"plan=TrainingPlan()\\\n    .add_training_item(autoencoder_anime,name='anime')\\\n    .add_training_item(autoencoder_human,name='human')\\\n    .with_data_loader(data_provider)\\\n    .repeat_epochs(500)\\\n    .with_batch_size(32)\\\n    .print_progress_scheduling(5,unit='batch')\\\n    .save_model_scheduling(50,unit='batch')\\\n    .display_loss_metric_curve_scheduling(frequency=100,unit='batch',imshow=True)","f9630e67":"plan.start_now()","ceec3afb":"\u5be6\u4f5c\u591a\u4efb\u52d9\u8a13\u7df4\u6700\u597d\u7684\u65b9\u6cd5\u5c31\u662f\u4f7f\u7528ModuleDict\uff0c\u9019\u662fdtrident\u4e2d\u7684\u7279\u6709\u529f\u80fd\uff0c\u4f60\u53ef\u4ee5\u5c07ModuleDict\u7686\u5728\u5404\u500b\u5b50\u4efb\u52d9\u5171\u7528\u9aa8\u67b6\u7684\u672b\u7aef\uff0c\u7136\u5f8c\u5728ModuleDict\u4e2d\u7684Key\u5c31\u6703\u662f\u8f38\u51fa\u540d\u7a31\uff0c\u800cValue\u5247\u662f\u653e\u7f6e\u5404\u500b\u5b50\u4efb\u52d9\u7368\u7acb\u7684\u5206\u985e\u5668\u6216\u662f\u6aa2\u6e2c\u982d\uff0c\u8acb\u8a18\u5f97\u5c07ModuleDict\u7684is_multicasting\u8a2d\u5b9a\u70baTrue\uff0c\u9019\u8868\u793a\u9810\u8a2d\u63a8\u8ad6\u884c\u70ba\u6703\u662f\u63a5\u6536\u9aa8\u67b6\u7684\u8f38\u51fa\uff0c\u540c\u6b65\u7684\u5c07\u6b64\u8f38\u51fa\u9935\u5165\u5404\u500bValue\u3002","5e1a1f72":"\u7e3d\u5171\u4e00\u500bencoder\uff0c\u7531\u65bc\u771f\u4eba\u8207\u52d5\u756b\u4eba\u7269\u5171\u7528\u540c\u4e00\u500bencoder\uff0c\u6240\u4ee5\u7de8\u78bc\u5668\u6703\u50be\u5411\u4fdd\u7559\u771f\u4eba\u8207\u52d5\u756b\u5171\u540c\u7684\u7279\u6027(\u81c9\u7684\u4fe1\u606f)\uff0c\u4f46\u662f\u6709\u5169\u500b\u7368\u7acb\u7684\u89e3\u78bc\u5668\uff0c\u4e00\u500b\u662fdecoder_human\u8ca0\u8cac\u89e3\u78bc\u771f\u4eba\u5716\u7247\uff0c\u4e00\u500b\u662fdecoder_anime\u8ca0\u8cac\u89e3\u78bc\u52d5\u756b\u4eba\u7269\u3002\u6211\u5011\u7e3d\u5171\u53ef\u4ee5\u8a2d\u8a08\u51fa\u5169\u500b\u81ea\u7de8\u78bc\u5668\uff0c\u5169\u8005\u5171\u7528\u76f8\u540c\u7684\u7de8\u78bc\u5668\u3002\u70ba\u4e86\u907f\u514d\u8de8\u57f7\u884c\u7e8c\u5b58\u53d6\u7570\u5e38\uff0c\u7de8\u78bc\u5668\u8981\u57f7\u884cshare_memory()\u64cd\u4f5c\u3002","177a8167":"\u63a5\u4e0b\u4f86\u5247\u662f\u8981\u5ba3\u544aencoder\u4ee5\u53ca\u5169\u500bdecoder\u7684\u5c0d\u61c9\u7d50\u69cb\u3002\u5728\u6b64\uff0cdecoder\u4e0d\u5efa\u8b70\u4f7f\u7528TransConv_2d\uff0c\u56e0\u70ba\u5bb9\u6613\u7522\u751f\u683c\u5b50\u72c0\u7684\u507d\u5f71\u3002\u5efa\u8b70\u6539\u4f7f\u7528Pixel_shuffle\uff0c\u4f46\u662f\u5176\u7f3a\u9ede\u662f\u5b83\u5f97\u9760\u901a\u9053\u6578\u8b8a\u6210\u539f\u4f86\u76841\/4\u4f86\u63db\u53d6\u5716\u7247\u9577\u5bec\u500d\u589e\uff0c\u6240\u4ee5\u4f7f\u7528\u5b8cPixel_shuffle\uff0c\u8981\u9032\u884c\u5347\u7dad\u7684\u52d5\u4f5c\uff0c\u9019\u6a23\u901a\u9053\u6578\u624d\u4e0d\u6703\u5feb\u901f\u6d88\u5931\u3002","08ad7387":"\n\u4f7f\u7528\u7684\u512a\u5316\u5668\u70baDiffGrad\nhttps:\/\/arxiv.org\/abs\/1909.11015\nhttps:\/\/kknews.cc\/zh-tw\/code\/ljl4bke.html","23d485aa":"\u9084\u8a18\u5f97\u8ab2\u7a0b\u4e2d\u63d0\u5230\u5982\u4f55\u63d0\u5347\u7279\u5fb5\u5411\u91cf\u7684\u8868\u9054\u80fd\u529b\uff0c\u90a3\u5c31\u662f\u7d66\u5b83\u76f8\u95dc\u7684\u5b50\u4efb\u52d9\uff0c\u9019\u6a23\u6a21\u578b\u5728\u8a13\u7df4\u904e\u7a0b\u4e2d\uff0c\u5c31\u6703\u5c07\u6b64\u4efb\u52d9\u6d89\u53ca\u7684\u77e5\u8b58\u8207\u4fe1\u606f\u69ae\u7279\u5fb5\u5411\u91cf\u4e4b\u4e2d\u3002\u90a3\u95dc\u65bc\u4eba\u81c9(\u5c24\u5176\u662f\u9700\u8981\u878d\u5408\u7684\u662f\u771f\u4eba\u8207\u52d5\u756b\u7684\u81c9)\uff0c\u90a3\u4f86\u6709\u751a\u9ebc\u6bd4\u4eba\u81c9\u7279\u5fb5\u9ede\u6aa2\u6e2c\u66f4\u597d\u7684\u5b50\u4efb\u52d9\u5462\u3002\n\n\u53ea\u4e0d\u904e\u4eba\u81c9\u7279\u5fb5\u9ede\u6aa2\u7cd9\u4e0d\u96e3\u627e\uff0c\u4f46\u662f\u52d5\u756b\u4eba\u81c9\u7279\u5fb5\u9ede\u6aa2\u6e2c\u5c31\u6bd4\u8f03\u7a00\u5c11\u4e86\u3002\u4f46\u9084\u662f\u88ab\u6211\u627e\u5230\u4e86\u4e00\u500b\u7bc4\u4f8b\u3002","27976946":"\u6309\u7167\u5716\u793a\uff0c\u53ef\u4ee5\u5224\u65b7\u52d5\u756b\u4eba\u81c924\u95dc\u9375\u9ede\u7684\u5b9a\u7fa9\u5206\u5225\u662f(\u4ee5\u4e0b\u5de6\u53f3\u6307\u6211\u5011\u8996\u89d2\u770b\u5230\u4e4b\u5de6\u53f3\uff0c\u975e\u4eba\u7269\u81ea\u8eab\u8996\u89d2\u4e4b\u5de6\u53f3):  \n0: \u5de6\u8033(\u5716\u7247\u5de6\u65b9)  \n1: \u4e0b\u5df4  \n2: \u53f3\u8033  \n3: \u5de6\u7709\u5c3e  \n4: \u5de6\u7709\u4e2d\u5fc3  \n5: \u5de6\u7709\u982d  \n6: \u53f3\u7709\u982d  \n7: \u53f3\u7709\u4e2d\u5fc3  \n8: \u53f3\u7709\u5c3e  \n9: \u9f3b\u982d  \n10: \u5de6\u773c\u5916\u5074  \n11: \u5de6\u773c\u4e0a\u65b9  \n12: \u5de6\u773c\u4e0a\u65b9\u5167\u5074  \n13: \u5de6\u773c\u4e0b\u65b9  \n14: \u5de6\u773c\u77b3\u5b54   \n15: \u53f3\u773c\u4e0a\u65b9\u5167\u5074  \n16: \u53f3\u773c\u4e0a\u65b9   \n17: \u53f3\u773c\u5916\u5074   \n18: \u53f3\u773c\u4e0b\u65b9  \n19: \u53f3\u773c\u77b3\u5b54  \n20: \u5634\u5507\u5de6\u5074    \n21: \u5634\u5507\u4e0a\u65b9  \n22: \u5634\u5507\u53f3\u5074  \n23: \u5634\u5507\u4e0b\u65b9  \n","6fbb396c":"\u5728\u6b64\u6211\u5011\u52a0\u5165\u5169\u500b\u6578\u64da\u96c6\u505a\u70ba\u8cc7\u6599\u4f86\u6e90\uff0c\u771f\u4eba\u81c9\u7167\u7247\u4f86\u81ea\u65bc\u300cface-recognition-dataset\u300d\uff0c\u5c0d\u8a71\u4eba\u81c9\u4f86\u81ea\u65bc\u300canimefacedataset\u300d\uff0c\u900f\u904eglob\u8a9e\u6cd5\u6293\u53d6\u6240\u6709\u5716\u6a94\u8def\u5f91\uff0c\u4e26\u8a08\u7b97\u5716\u7247\u6578\u91cf\u3002"}}