{"cell_type":{"7aee23c7":"code","a2daf901":"code","d70098c5":"code","161fd2f7":"code","7856d173":"code","f9f9c1fd":"code","ae939352":"code","06c2442c":"code","d6996e3d":"code","54664fd8":"code","5512bdfb":"code","371d6de4":"code","f7ee23a1":"code","2d50ff82":"code","896c0d58":"code","589012c2":"code","c21ad9ed":"code","65364dd4":"code","250d04ed":"code","f40efe9b":"code","1ca3d55c":"code","4d7e9e03":"code","468d2f96":"code","5a2111cc":"code","b2fe8525":"code","664d1499":"code","d39286fc":"code","321c7241":"code","bee49898":"code","814bd4e4":"code","634fdca6":"code","55350380":"code","89b1f949":"code","d5d4c808":"code","abd3be3b":"code","4d5bd6aa":"code","f6254c5c":"code","8bc2d4d3":"code","92af8fea":"code","78fa8347":"code","d558eef7":"code","165d41b7":"code","a50f47c6":"code","9770a89d":"code","4adcd564":"code","e48200a2":"code","75af4324":"code","be11b502":"code","6725bf2d":"code","b37fe217":"code","240373f6":"code","52803a63":"code","22ca73a0":"code","cd775ef4":"code","f0a62e2e":"code","ca24c37f":"code","e71862a0":"code","f9d02e11":"code","4bd56ed5":"code","c9e9ba7d":"code","ab1cb0ed":"code","e0603744":"code","75fe36a4":"code","31535973":"code","78b8ae92":"code","e760b721":"code","65025eef":"code","bf654161":"code","4e972579":"code","78165bb5":"code","e365d41e":"code","38a67fbf":"code","1f0e0497":"code","3ec15c72":"code","feab10d7":"code","66c7b696":"code","25b74244":"code","d6cbbd78":"code","36032c76":"code","4ca09685":"code","9ff7c38a":"code","610e714d":"code","f5a0c54d":"code","4f359494":"code","0bf862a9":"code","1b858576":"code","f1e9da91":"code","1e0bd7ce":"code","adfb65be":"code","94b6b62f":"code","553d6901":"code","a34ae877":"code","3523de5a":"code","4615d7b6":"code","0b49f5b8":"code","faa1740c":"markdown","a71a2826":"markdown","9f7485b0":"markdown","b106c77f":"markdown","1fc7163f":"markdown","a0dd5dc6":"markdown","4cb73489":"markdown","55612664":"markdown","5e25e07f":"markdown","2e1efef6":"markdown","2bff371b":"markdown","11d39b30":"markdown","205d4666":"markdown","dcc38773":"markdown","b395fb06":"markdown","e5c238e0":"markdown","ec642d2d":"markdown","890461d8":"markdown","a7c952db":"markdown","c98ce3fc":"markdown","b52de91f":"markdown","953cd45d":"markdown","fbe4d1e7":"markdown","d03eb3f6":"markdown","788e12f4":"markdown","acbc8da2":"markdown","c5794573":"markdown","44a8c443":"markdown","37f2f94c":"markdown","a7bd65aa":"markdown","2e243d18":"markdown","00de1c9e":"markdown","7af7818e":"markdown","ccdd8e8c":"markdown","733d1a31":"markdown","4dcede16":"markdown","795b0b34":"markdown","28ad7100":"markdown","40a19b57":"markdown","769a66f2":"markdown"},"source":{"7aee23c7":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a2daf901":"import pandas_profiling\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\nsns.set(style='ticks', context='talk')\nplt.style.use('dark_background')","d70098c5":"DATADIR = '..\/input\/titanic\/'\n\ntrain  = pd.read_csv('{0}train.csv'.format(DATADIR))\ntest   = pd.read_csv('{0}test.csv'.format(DATADIR))\n\ntrain_len = len(train)\nIDtest = test['PassengerId'] ","161fd2f7":"train.head()","7856d173":"test.head()","f9f9c1fd":"dataset = pd.concat( objs=[train, test], axis=0 ).reset_index(drop=True)","ae939352":"fig, ax = plt.subplots(1,1,figsize=(10,6))\n\nsns.distplot(dataset['Age'][(dataset['Age'].notnull())&(dataset['Survived']==0)] ,color='r', ax=ax )\nsns.distplot(dataset['Age'][(dataset['Age'].notnull())&(dataset['Survived']==1)] ,color='b', ax=ax )\n\nplt.legend(['Not Survived', 'Survived'])","06c2442c":"def age_classify(x):\n    return int(x\/\/10)","d6996e3d":"dataset['Age'].isnull().sum()","54664fd8":"dataset['n_Age'] = dataset['Age'][dataset['Age'].notnull()].apply(age_classify)","5512bdfb":"dataset['n_Age'].value_counts()","371d6de4":"m = len(dataset['n_Age'].value_counts())\ntotal = dataset['n_Age'].value_counts().sum()\ntotal_null = dataset['n_Age'].isnull().sum()\n\nage_percent = [0]*m\nage_dist    = [0]*m\n\nfor i in range(m):\n    age_percent[i]=round((dataset['n_Age'].value_counts()[i]\/total),3)\n    age_dist[i] = int(round(age_percent[i]*total_null))\n\nage_dist[3]-=1\nprint('percentage[%] : ',np.multiply(age_percent,100))\nprint('counts for n_Age :',age_dist)\n","f7ee23a1":"\ntotal_null = dataset['n_Age'].isnull().sum()\n\nfor i in dataset['n_Age'].index[dataset['n_Age'].isnull()]:\n    if age_dist[0] != 0: \n        dataset['n_Age'][i]=0.0\n        age_dist[0]-=1\n    elif age_dist[1] != 0: \n        dataset['n_Age'][i]=1.0\n        age_dist[1]-=1\n    elif age_dist[2] != 0:\n        dataset['n_Age'][i]=2.0\n        age_dist[2]-=1\n    elif age_dist[3] != 0: \n        dataset['n_Age'][i]=3.0\n        age_dist[3]-=1\n    elif age_dist[4] != 0: \n        dataset['n_Age'][i]=4.0\n        age_dist[4]-=1\n    elif age_dist[5] != 0:\n        dataset['n_Age'][i]=5.0\n        age_dist[5]-=1\n    elif age_dist[6] != 0: \n        dataset['n_Age'][i]=6.0\n        age_dist[6]-=1            \n    elif age_dist[7] != 0: \n        dataset['n_Age'][i]=7.0\n        age_dist[7]-=1\n    elif age_dist[8] != 0: \n        dataset['n_Age'][i]=8.0\n        age_dist[8]-=1","2d50ff82":"dataset['n_Age'].isnull().sum()","896c0d58":"fig, ax = plt.subplots(1,2, figsize=(14, 6))\n\ng1 = sns.countplot( x='n_Age', hue='Survived',data=dataset, palette='YlGnBu_r', ax=ax[0])\ng2 = sns.factorplot(x='n_Age', y='Survived',  data=dataset, palette='YlGnBu_r', ax=ax[1], kind = 'bar')\n\nplt.close(g2.fig)\nplt.subplots_adjust(wspace = 0.3)","589012c2":"fig , ax= plt.subplots(1,2,figsize=(18,6))\n\ng1 = sns.factorplot(x='n_Age',hue='Survived', data=dataset, kind='count', palette='YlGnBu_r', ax=ax[0])\ng2 = sns.factorplot(x='n_Age',  y='Survived', data=dataset, kind='bar',   palette='YlGnBu_r', ax=ax[1])\n\nax[0].legend(['Not Survived', 'Survived'],loc='best')\nax[1].set_ylabel('Survived Probability')\n\nplt.subplots_adjust(wspace=1)\nplt.close(g1.fig)\nplt.close(g2.fig)","c21ad9ed":"dataset.drop(labels=['Age'], axis=1, inplace=True)","65364dd4":"dataset['Sex'].value_counts()","250d04ed":"fig, ax = plt.subplots(1,2, figsize=(12,6))\n\ng = sns.factorplot( x='Sex', y='Survived', data=dataset, palette='Blues_r', kind='bar' ,ax=ax[0] )\nplt.close(g.fig)\n\ng2 = sns.countplot( x='Sex', data=dataset, palette='Blues_r', ax=ax[1] )\nplt.subplots_adjust(wspace=0.5)\n\nax[0].set_xticklabels(['Female', 'Male']);\nax[1].set_xticklabels(['Female', 'Male']);","f40efe9b":"dataset['Sex'].head()","1ca3d55c":"fig, ax = plt.subplots(2,2, figsize=(15,15))\n\ng1 = sns.countplot(x='n_Age',                  hue='Sex', data=dataset, palette='Blues_r',     ax=ax[0,0])\ng2 = sns.barplot(  x='n_Age',    y='Survived', hue='Sex', data=dataset, palette='Greens_r',    ax=ax[0,1])\ng3 = sns.barplot(  x='Pclass',   y='Survived', hue='Sex', data=dataset, palette='Oranges_r',   ax=ax[1,0])\ng4 = sns.barplot(  x='Embarked', y='Survived', hue='Sex', data=dataset, palette='Reds_r',      ax=ax[1,1])\n\ng1.set_ylabel('Counts')\ng2.set_ylabel('survived Probability')\ng3.set_ylabel('survived Probability')\ng4.set_ylabel('survived Probability')\n\nax[0,0].legend(['Female','Male'])\n\nplt.subplots_adjust(wspace=0.3)","4d7e9e03":"dataset['SibSp'].value_counts()","468d2f96":"dataset['Parch'].value_counts()","5a2111cc":"dataset['FamSize']=dataset['SibSp']+dataset['Parch']+1","b2fe8525":"dataset['FamSize'].isnull().sum()","664d1499":"dataset['FamSize'].value_counts()","d39286fc":"fig, ax = plt.subplots(2,1, figsize=(12,10))\n\ng1 = sns.countplot(x='FamSize', data=dataset, palette='RdBu', ax=ax[0])\ng2 = sns.factorplot(x='FamSize', y='Survived',kind='bar', data=dataset, palette='RdBu', ax=ax[1])\nax[1].set(ylabel='survived probability')\n\nplt.close(g2.fig)\nplt.subplots_adjust(hspace=0.4)","321c7241":"dataset.drop(labels=['SibSp','Parch'], axis=1, inplace=True)","bee49898":"dataset['Name'].unique()","814bd4e4":"dataset['Name'].isnull().sum()","634fdca6":"dataset['Title'] = [0]*dataset['Name'].shape[0]","55350380":"num=0\nfor x in dataset['Name']:\n    title = x.split(',')[1].split(',')[0].split('.')[0]\n    title = title.split(' ')[1]\n    dataset['Title'][num]=title\n    num+=1","89b1f949":"dataset['Title'].value_counts()","d5d4c808":"dataset['Title'].replace(['Rev','Dr','Col','Major','Jonkheer','Ms','Mlle','Mme','Lady','Dona','the Countess','Sir','Capt','Don'],'Rare', inplace=True)\ndataset['Title'] = dataset['Title'].replace(['Ms','Mlle','Mme','Lady','Dona','the'],'Miss')\ndataset['Title'] = dataset['Title'].replace(['Sir','Capt','Don'],'Mr')\n\ndataset['Title'].unique()","abd3be3b":"fig, ax = plt.subplots(1,2, figsize=(15, 6))\n\ng1 = sns.countplot(x='Title', data=dataset, palette='YlGnBu_r', ax=ax[0] )\ng2 = sns.factorplot(x='Title', y='Survived', data=dataset, palette='YlGnBu_r',kind='bar', ax=ax[1])\nplt.close(g2.fig)","4d5bd6aa":"dataset.drop(labels=['Name'], axis=1, inplace=True)\ndataset.head()","f6254c5c":"dataset['Embarked'].value_counts()","8bc2d4d3":"dataset['Embarked'].fillna(method='ffill', inplace = True)","92af8fea":"dataset['Embarked'].isnull().sum()","78fa8347":"fig, ax = plt.subplots(1,2, figsize=(18,6))\n\ng = sns.factorplot(x='Embarked', y='Survived',data=dataset, kind='bar', palette='binary', ax=ax[0])\nax[0].set(ylabel='survived probability')\n\ng2 = sns.countplot(x='Embarked', hue='Survived',data=dataset, palette='binary',ax=ax[1])\ng2.set(ylabel='survived counts')\ng2.legend(['Not Survived', 'Survived'])\n\nplt.close(g.fig)\nplt.subplots_adjust(wspace=0.3)","d558eef7":"dataset['Fare'].fillna(method='ffill',inplace=True )","165d41b7":"dataset['Fare'].isnull().sum()","a50f47c6":"dataset['n_Fare'] = (dataset['Fare']\/\/1.0)","9770a89d":"dataset['n_Fare'].isnull().sum()","4adcd564":"dataset['n_Fare'] = pd.qcut(dataset['n_Fare'], 7)","e48200a2":"def mapping(x):\n    x= str(x)\n    left= (x.split(', ')[0].split('(')[1])\n    if   left =='-0.001': return 1\n    elif left =='7.0'   : return 2\n    elif left =='8.0'   : return 3\n    elif left =='12.0'  : return 4\n    elif left =='19.0'  : return 5\n    elif left =='27.0'  : return 6\n    elif left =='59.0'  : return 7    ","75af4324":"dataset['n_Fare'] = dataset['n_Fare'].map(mapping)\ndataset['n_Fare'].astype(int)","be11b502":"dataset['n_Fare'].value_counts()","6725bf2d":"fig, ax = plt.subplots(1,1, figsize=(15, 6))\ng=sns.factorplot(x='n_Fare', y='Survived', data=dataset, kind='bar', palette='YlGnBu_r', ax=ax)\nplt.close(g.fig)","b37fe217":"fig, ax = plt.subplots(1,2,figsize=(17, 6))\n\ng  = sns.countplot( x='n_Fare', hue='Survived', data=dataset, palette='Greens_r', ax=ax[0])\ng2 = sns.factorplot(x='n_Fare', y='Survived',   data=dataset, palette='Blues_r',  ax=ax[1], kind='bar')\nax[0].set(ylabel='Survived counts')\nax[1].set(ylabel='Survived probability')\n\nplt.close(g2.fig)\nplt.subplots_adjust(wspace=0.3)","240373f6":"dataset.drop(labels=['Fare', 'PassengerId'],axis=1, inplace=True)","52803a63":"dataset['Pclass'].value_counts()","22ca73a0":"dataset['Pclass'].isnull().sum()","cd775ef4":"fig, ax = plt.subplots(1,2, figsize=(18, 6))\n\ng = sns.factorplot( x='Pclass', y='Survived', data=dataset, palette='Greens_r', kind='bar', ax=ax[0] )\nplt.close(g.fig)\n\ng = sns.countplot( x='Pclass', hue='Survived',data=dataset, palette='bone_r', ax=ax[1] )\ng.legend(['Not Survived', 'Survived'])","f0a62e2e":"dataset['Cabin'].value_counts()","ca24c37f":"def refine(x):\n    if type(x)==float: return 'X'\n    else:\n        if len(x)>4: return x.split(' ')[0][0]\n        else: return x[0]","e71862a0":"dataset['n_Cabin'] = dataset['Cabin'].apply(refine)","f9d02e11":"dataset['n_Cabin'].unique()","4bd56ed5":"fig, ax = plt.subplots(1,2, figsize=(15, 6))\ng1 = sns.countplot( x='n_Cabin', data=dataset, palette='rainbow' ,ax=ax[0] )\ng2 = sns.factorplot( x='n_Cabin', y='Survived', data=dataset, palette='rainbow', kind='bar', ax=ax[1] )\n\nplt.subplots_adjust(wspace=1.0)\nplt.close(g2.fig)","c9e9ba7d":"dataset['n_Cabin']=dataset['n_Cabin'].map({'A':2,'B':1,'C':3,'D':1,'E':1,'F':3,'G':2,'T':1,'X':4})","ab1cb0ed":"sns.factorplot(x='n_Cabin',y='Survived', data=dataset, kind='bar', palette='YlGnBu_r')","e0603744":"dataset.drop(labels=['Cabin'], axis=1, inplace=True)","75fe36a4":"dataset['Ticket'].value_counts()","31535973":"dataset['Ticket_Frequency'] = dataset.groupby('Ticket')['Ticket'].transform('count')","78b8ae92":"fig, ax = plt.subplots(1,2, figsize=(15, 6))\n\ng1 = sns.countplot(x='Ticket_Frequency',              hue='Sex', data=dataset, palette='Oranges_r', ax=ax[0]  )\ng2 = sns.barplot(  x='Ticket_Frequency', y='Survived',hue='Sex', data=dataset, palette='Oranges_r', ax=ax[1]  )\ng1.legend(['Female', 'Male'])\nplt.subplots_adjust(wspace=0.3)","e760b721":"dataset = dataset.drop(labels=['Ticket'],axis=1)","65025eef":"dataset.head()","bf654161":"#dataset.profile_report()","4e972579":"print('Emabarked unique() :' , dataset['Embarked'].unique())\nprint('Title.    unique() :' , dataset['Title'   ].unique())\nprint('Sex       unique() :' , dataset['Sex'     ].unique())\nprint('Pclass    unique() :' , dataset['Pclass'  ].unique())\nprint('n_Age     unique() :' , dataset['n_Age'   ].unique())\nprint('FamSize   unique() :' , dataset['FamSize' ].unique())\nprint('n_Cabin   unique() :' , dataset['n_Cabin' ].unique())\nprint('n_Fare    unique() :' , dataset['n_Fare'  ].unique())","78165bb5":"# One-hot encoding for non integer values\ndataset = pd.get_dummies(dataset, columns=[ 'Sex'     ], prefix='Sx')\ndataset = pd.get_dummies(dataset, columns=[ 'Embarked'], prefix='Em')\ndataset = pd.get_dummies(dataset, columns=[ 'Title'   ], prefix='Tt')\ndataset = pd.get_dummies(dataset, columns=[ 'n_Fare'  ], prefix='nF')\n\n# dataset = pd.get_dummies(dataset, columns=[ 'n_Age'   ], prefix='nA')\n# dataset = pd.get_dummies(dataset, columns=[ 'Pclass'  ], prefix='Pc')\n# dataset = pd.get_dummies(dataset, columns=[ 'FamSize' ], prefix='FS')\n# dataset = pd.get_dummies(dataset, columns=[ 'Ticket_Frequency'], prefix='nT')","e365d41e":"# This part is for removing low feature_importance_values.\n# However that doesn't mean that we don't need these values.\n\n# So to check this, I erased values which has low importance values, But the result doesn't changed.\n\n# dataset.drop(['nA_1','nA_4','nA_5','nA_6','nA_7','nA_8'],axis=1, inplace=True)\n# dataset.drop(['Tt_1','Tt_2','Tt_3','Tt_4'],axis=1,inplace=True)\n# dataset.drop(['Em_1','Em_2'],              axis=1,inplace=True)\n# dataset.drop(['Pc_1','Pc_2'],              axis=1,inplace=True)\n# dataset.drop(['nT_5','nT_8','nT_7','nT_11','nF_2','nF_3','nF_7'],       axis=1,inplace=True)\n# dataset.drop(['FS_1','FS_2','FS_4','FS_5','FS_6','FS_7','FS_8','FS_11'],axis=1,inplace=True)","38a67fbf":"dataset.head()","1f0e0497":"train = dataset[:train_len]\ntest  = dataset[train_len:]\ntest.drop(labels=['Survived'], axis=1, inplace=True)","3ec15c72":"train['Survived'] = train['Survived'].astype(int)\nY_train = train['Survived']\nX_train = train.drop(labels=['Survived'], axis=1)","feab10d7":"kfold = StratifiedKFold(n_splits=10)","66c7b696":"from xgboost import XGBClassifier\n\nrandom_state=3\nclassifiers = []\n\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state), random_state=random_state, learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state=random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(XGBClassifier(random_state=random_state))","25b74244":"cv_results = []\n\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, y=Y_train, scoring='accuracy', cv=kfold, n_jobs=4))\n\ncv_means = []\ncv_std   = []\nfor cv_result in cv_results:\n    cv_means.append( cv_result.mean() )\n    cv_std.append(   cv_result.std()  )\n    \ncv_res = pd.DataFrame({'CrossValMeans':cv_means, 'CorssValerrors':cv_std,\n                       'Algorithm':['SVC','DecisionTree','AdaBoost','RandomForest','ExtraTrees',\n                                    'GradientBoosting','MultipleLayerPerceptron','KNeighbors','LogisticRegression','LinearDiscriminantAnalysis','XGBoost']})\n\ng = sns.barplot('CrossValMeans','Algorithm', data=cv_res, palette='YlGnBu', orient='h', **{'xerr':cv_std})\ng = g.set(title='Cross Validation scores',xlabel='Mean Accuracy')","d6cbbd78":"XGB = XGBClassifier(random_state=0)\n\nxgb_param_grid={'colsample_bylevel':[0.9],\n                'colsample_bytree' :[0.8],\n                'gamma'            :[0.99],\n                'max_depth'        :[4],\n                'min_child_weight' :[1],\n                'n_estimators'     :[10],\n                'nthread'          :[4],\n                'silent'           :[True]}\n\ngsXGB = GridSearchCV(XGB, param_grid=xgb_param_grid, cv=kfold, scoring='accuracy', n_jobs=4, verbose=1)\ngsXGB.fit(X_train, Y_train)\nXGBC_best = gsXGB.best_estimator_\n\ngsXGB.best_score_","36032c76":"DTC = DecisionTreeClassifier(max_depth=1)\n\nadaDTC = AdaBoostClassifier(DTC, random_state=0)\n\nada_param_grid = {'base_estimator__criterion':['gini', 'entropy'],\n                  'base_estimator__splitter' :['best', 'random'],\n                  'algorithm'                :['SAMME', 'SAMME.R'],\n                  'n_estimators'             :[100],\n                  'learning_rate'            :[0.0001, 0.001, 0.01, 0.1, 1, 5, 10]}\n\ngsadaDTC = GridSearchCV( adaDTC, param_grid=ada_param_grid, cv=kfold, scoring='accuracy', n_jobs=4, verbose=1 )\ngsadaDTC.fit(X_train,Y_train)\nada_best = gsadaDTC.best_estimator_\n\ngsadaDTC.best_score_","4ca09685":"ExtC = ExtraTreesClassifier(random_state=0)\n\nex_param_grid = {'max_depth'        :[4],\n                 'max_features'     :[7],\n                 'min_samples_split':[8],\n                 'min_samples_leaf' :[4],\n                 'bootstrap'        :[False],\n                 'n_estimators'     :[100],\n                 'criterion'        :['gini']}\n\ngsExtC = GridSearchCV(ExtC, param_grid=ex_param_grid, cv=kfold, scoring='accuracy', n_jobs=4, verbose=1)\ngsExtC.fit(X_train, Y_train)\n\nExtC_best = gsExtC.best_estimator_\n\ngsExtC.best_score_","9ff7c38a":"RFC = RandomForestClassifier(random_state=0)\n\nrf_param_grid = {'max_depth'        :[6],\n                 'max_features'     :[10],\n                 'min_samples_split':[10],\n                 'bootstrap'        :[False],\n                 'n_estimators'     :[100],\n                 'criterion'        :['gini']}\n\ngsRFC = GridSearchCV(RFC, param_grid=rf_param_grid, cv=kfold, scoring='accuracy', n_jobs=4, verbose=1)\ngsRFC.fit(X_train, Y_train)\nRFC_best = gsRFC.best_estimator_\n\ngsRFC.best_score_","610e714d":"GBC = GradientBoostingClassifier(random_state=0)\n\ngb_param_grid = {'loss'            :['deviance'],\n                 'n_estimators'    :[10],\n                 'learning_rate'   :[0.001, 0.01, 0.1, 1, 10, 100],\n                 'max_depth'       :[4],\n                 'min_samples_leaf':[8],\n                 'max_features'    :[0.3, 0.1]\n                }\n\ngsGBC = GridSearchCV( GBC, param_grid=gb_param_grid, cv=kfold, scoring='accuracy',n_jobs=4, verbose=1 )\n\ngsGBC.fit(X_train, Y_train)\nGBC_best = gsGBC.best_estimator_\n\ngsGBC.best_score_","f5a0c54d":"SVMC = SVC(probability=True)\nsvc_param_grid = {'kernel':['rbf'],\n                  'gamma' :[0.01,0.1,1,10,100],\n                  'C'     :[0.01,0.1,1,10,100]}\n\ngsSVMC = GridSearchCV(SVMC, param_grid=svc_param_grid, cv=kfold, scoring='accuracy', n_jobs=4, verbose=1)\n\ngsSVMC.fit(X_train, Y_train)\nSVMC_best = gsSVMC.best_estimator_\n\ngsSVMC.best_score_","4f359494":"parameters = {'solver':['adam'], 'max_iter':[800], 'alpha':10.0**-np.arange(1,10), 'hidden_layer_sizes':[40,20,10], 'random_state':[0,1] }\ngsMLPC=GridSearchCV(MLPClassifier(), parameters, n_jobs=-1)\n\ngsMLPC.fit(X_train, Y_train)\nMLPC_best = gsMLPC.best_estimator_\n\ngsMLPC.best_score_","0bf862a9":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('Training examples')\n    plt.ylabel('Score')\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean( train_scores, axis=1 )\n    train_scores_std  = np.std(  train_scores, axis=1 )\n    test_scores_mean  = np.mean( test_scores, axis=1  )\n    test_scores_std   = np.std(  test_scores, axis=1  )\n    \n    plt.fill_between( train_sizes, train_scores_mean-train_scores_std, train_scores_mean+train_scores_std, alpha=0.1, color='r' )\n    plt.fill_between( train_sizes, test_scores_mean-train_scores_std,  test_scores_mean+train_scores_std,  alpha=0.1, color='g' )\n    \n    plt.plot(train_sizes, train_scores_mean, 'o-', color='r', label='Training score')\n    plt.plot(train_sizes, test_scores_mean,  'o-', color='g', label='Cross-validation score')\n    plt.legend(loc='best')\n    return plt","1b858576":"g1 = plot_learning_curve( gsGBC.best_estimator_,    'Gradient Boosting Learning Curve', X_train, Y_train, cv=kfold )\ng2 = plot_learning_curve( gsExtC.best_estimator_,   'ExtraTrees learning curves',       X_train, Y_train, cv=kfold )\ng3 = plot_learning_curve( gsSVMC.best_estimator_,   'SVC learning curves',              X_train, Y_train, cv=kfold )\ng4 = plot_learning_curve( gsadaDTC.best_estimator_, 'AdaBoost learning curves',         X_train, Y_train, cv=kfold )\ng5 = plot_learning_curve( gsRFC.best_estimator_,    'RF learning curves',               X_train, Y_train, cv=kfold )\ng6 = plot_learning_curve( gsMLPC.best_estimator_,   'MLP learning curves',              X_train, Y_train, cv=kfold )\ng6 = plot_learning_curve( gsXGB.best_estimator_,    'XGB learning curves',              X_train, Y_train, cv=kfold )","f1e9da91":"nrows = 2\nncols = 2\nfig, axes = plt.subplots(nrows=nrows, ncols=ncols, sharex='all', figsize=(15,15))\n\nnames_classifiers = [('AdaBoosting', ada_best),('ExtraTrees',ExtC_best),('RandomForest',RFC_best),\n                     ('GradientBoosting',GBC_best)]\n\nnclassifier = 0\nfor row in range(nrows):\n    for col in range(ncols):\n        name = names_classifiers[nclassifier][0]\n        classifier = names_classifiers[nclassifier][1]\n        indices = np.argsort(classifier.feature_importances_)[::-1][:40]\n        g = sns.barplot(y=X_train.columns[indices][:40],x=classifier.feature_importances_[indices][:40], orient='h', ax=axes[row][col])\n        g.set_xlabel('Relative importance', fontsize=12)\n        g.set_ylabel('Features', fontsize=12)\n        g.tick_params(labelsize=9)\n        g.set_title(name+' feature importance')\n        nclassifier +=1\n        \nplt.subplots_adjust(wspace=0.4, hspace=0.2)","1e0bd7ce":"test_Survived_RFC  = pd.Series( RFC_best.predict(test), name='RFC' )\ntest_Survived_ExtC = pd.Series(ExtC_best.predict(test), name='ExtC')\ntest_Survived_SVMC = pd.Series(SVMC_best.predict(test), name='SVC' )\ntest_Survived_AdaC = pd.Series( ada_best.predict(test), name='Ada' )\ntest_Survived_GBC  = pd.Series( GBC_best.predict(test), name='GBC' )\ntest_Survived_MLP  = pd.Series(MLPC_best.predict(test), name='MLP' )\ntest_Survived_XGB  = pd.Series(XGBC_best.predict(test), name='XGB' )\n\nensemble_results = pd.concat( [test_Survived_RFC, test_Survived_ExtC, test_Survived_SVMC, \n                               test_Survived_AdaC, test_Survived_GBC, test_Survived_MLP, test_Survived_XGB], axis=1 )\ng = sns.heatmap(ensemble_results.corr(), annot=True, annot_kws={'size':12})","adfb65be":"# votingC = VotingClassifier(estimators = [('rfc',RFC_best),('extc',ExtC_best),('svc',SVMC_best),('adac',ada_best),('gbc',GBC_best),('mlp',MLPC_best)],voting='soft',n_jobs=4)\n# print(votingC)\n# votingC = votingC.fit(X_train, Y_train)","94b6b62f":"# test_Survived = pd.Series(votingC.predict(test), name='Survived')\n\n# results = pd.concat([IDtest, test_Survived],axis=1)\n\n# results.to_csv('51th_submission.csv', index=False)","553d6901":"test_Survived = gsExtC.predict(test)\n\nsubmission = pd.DataFrame({\n    'PassengerId' : IDtest,\n    'Survived' : test_Survived\n})\n\nsubmission.to_csv('ExtC_55th.csv', index=False)","a34ae877":"test_Survived = gsXGB.predict(test)\n\nsubmission = pd.DataFrame({\n    'PassengerId' : IDtest,\n    'Survived' : test_Survived\n})\n\nsubmission.to_csv('XGB_55th.csv', index=False)","3523de5a":"test_Survived = gsGBC.predict(test)\n\nsubmission = pd.DataFrame({\n    'PassengerId' : IDtest,\n    'Survived' : test_Survived\n})\n\nsubmission.to_csv('GBC_55th.csv', index=False)","4615d7b6":"test_Survived = gsSVMC.predict(test)\n\nsubmission = pd.DataFrame({\n    'PassengerId' : IDtest,\n    'Survived' : test_Survived\n})\n\nsubmission.to_csv('SVMC_55th.csv', index=False)","0b49f5b8":"# test_Survived = mlp.predict(test)\n\n# submission = pd.DataFrame({\n#     'PassengerId' : IDtest,\n#     'Survived' : test_Survived\n# })\n\n# submission.to_csv('my_NN_submission.csv', index=False)","faa1740c":"> Before handling 'Age' null data, I'm gonna separate few class with 10 years.","a71a2826":"> ### 1.2 Cleaning","9f7485b0":"> As saw you above, there's different number of people <br>\n> So I thought that I need to fill NaN data with staying this distribution","b106c77f":"## 2.3 Name\n> ### 2.3.1 Cleaning\n> ### 2.3.2 Engineering\n> ### 2.3.3 Check Survival Rate","1fc7163f":" ### 2.1.2 Engineering\n> Make string value ('female', 'male) to integer( 0 \/ 1 )","a0dd5dc6":"> 1st Class has highest survival rate.","4cb73489":"> ### 5.1 Cleaning","55612664":"# 6. Ticket\n> ## 6.1 Cleaning","5e25e07f":"> We can check the correlation between Ticket number length and Sex\n\n> If Ticket length is 3 and female, She would be survived.\n> And also legnth 5 and female.\n\n> So, I'm gonna use this data.","2e1efef6":"First, check the features.","2bff371b":"> This finds percentage of 'n_Age' class.\n\n> 0~9   years: 7.8%<br>\n> 10~19 years: 13.7%<br>\n> ...<br>\n> 80~   years: 0.1%<br>","11d39b30":"### 2. Cross validate models. ","205d4666":"### One-Hot Encoding","dcc38773":"> ### 4.1 Cleaning","b395fb06":"> As you can see above, there's no NaN value so that I don't have to replace them.","e5c238e0":"> ### 2.2.3 Check Survival Rate","ec642d2d":"### 2.1.3 Correlation with Age, Pclass, Embarked\n> Make the plot","890461d8":"## 2.1 Sex\n> ### 2.1.1 Cleaning Data","a7c952db":"## 5. Pclass, Cabin","c98ce3fc":"### 1. Seperate dataset to train\/test set.","b52de91f":"## 2. Sex \/ SibSp+Parch \/ Name\n___\n\n\n ### 2.1 Sex<br>\n>  2.1.1 Cleaning<br>\n>  2.1.2 Correlation with Age, Pclass, Embarked \n\n ### 2.2 FamSize = SibSp + Parch + 1\n> 2.2.1 Cleaning<br>\n> 2.2.2 Engineering<br>\n> 2.2.3 Check Survival Rate.\n\n ### 2.3 Name\n> 2.3.1 Cleaning <br>\n> 2.3.2 Engineering<br>\n> 2.3.3 Check Survival Rate.\n___","953cd45d":"> There's no NaN value.<br>\nHowever, 'Name' data is not usefull by itself.<br>\nI thought that I need to refine this data.<br><br>\n\n>Then, how can I refine it?<br>\nLet's think about that emergency situation that so many people need to escape.<br>\n\n>As there're so many people, we can't call their total name.<br>\nMaybe we should not do that for rescue much more people rapidly.<br>\n\n>\nSo, I thought maybe I only need the title of their Name's<br>\n\n\n","fbe4d1e7":"> ### 3.2 Engineering\n> Nothing special but make string value to integer.","d03eb3f6":"> ### 4.2 Engineering","788e12f4":"### 5.3 Check the Survival Rate","acbc8da2":"> You can see \"Lady, Kids, Elderly First\"","c5794573":"# Features\n> ##  1. Age\n> ##  2. Sex, SibSp+Parch, Name\n> ##  3. Embarked\n> ##  4. Fare\n> ##  5. Pclass, Cabin\n","44a8c443":"> There's nothing to clean up.\n\n> ### 5.2 Engineering\n> Actually, no need to refine. ","37f2f94c":"## 3. Embarked\n\n> ### 3.1 Cleaning<br>\n: Replaceing NaN data","a7bd65aa":"> 2) Replacing Rare value with one of clear titles('Miss', 'Mr', 'Mrs', 'Master') <br>","2e243d18":"> ### 4.3 Checking Survival rate","00de1c9e":" ## 1. Age\n \n > ### 1.1 Check Null ","7af7818e":"# Modeling","ccdd8e8c":"## 2.2 FamilySize = SibSp + Parch\n> ### 2.2.1. Cleaning","733d1a31":"## 4. Fare","4dcede16":"> There's no NaN value to clean","795b0b34":"There's nothing to clean up for NaN value.","28ad7100":"> ### 2.2.2 Engineering Feature","40a19b57":" ### 2.3.2 Engineering Feature\n\n> 1) Sperate Title from Name <br>\n","769a66f2":"> As you can see above, even though there's so many men, Most of them could not survived.<br>\n> Again, They behave **<u>'Lady, elderly First'<\/u>** even if they were in desperate situation."}}