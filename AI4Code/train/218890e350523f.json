{"cell_type":{"18861746":"code","d97762da":"code","bea12428":"code","d4b60fca":"code","5f6b1054":"code","7786a11a":"code","4a58a9b4":"code","e1f5e674":"code","ce60045a":"code","2c6afd56":"code","d7bb38a3":"code","eb76530f":"code","016a0911":"code","7ea77a7f":"code","e4484acf":"code","030b0b63":"code","8c627b9f":"code","9678733a":"code","f1497fa9":"code","10a93870":"code","ff991ec9":"code","1ffbd3ce":"code","3347301c":"code","a9bf046e":"code","10993248":"code","6d79d026":"code","5ea9cf3e":"code","39237f1d":"code","f1339443":"code","a68e4364":"code","ca257696":"code","86c3c399":"code","102f387f":"code","1887f271":"code","922b4eca":"code","a5114ec6":"code","aceb3436":"code","22a9caac":"code","375fd0d2":"code","5175f079":"code","9fc6827c":"code","253011e6":"code","82a04288":"code","7af50422":"code","eab6b081":"code","91346c1f":"code","85d46a2b":"code","bdaf7953":"code","78c8cb46":"code","cb21afae":"code","63a66651":"code","e577f2f1":"code","aad44506":"code","37aa4d90":"code","15516299":"code","76727c56":"code","f9b14e86":"code","eceb109f":"code","0f3a27ee":"code","085b653f":"code","4791c4b5":"code","d3358679":"code","fee76e57":"code","5c812bc5":"code","30ac7f30":"code","3345402f":"code","04cace85":"code","c9fa528d":"code","85c05ae9":"code","07a99e35":"code","dc36ff92":"code","4ea2146e":"code","02914ccc":"code","bc4d2d9c":"code","f44b934f":"code","9f3df9df":"code","080c2c4c":"code","cb7306cc":"code","63b1464d":"code","78a8ea34":"code","569568c5":"code","6a5d49f8":"code","104663f4":"code","11702706":"code","82a78302":"code","76a8059b":"code","d5368779":"code","e0373d26":"code","70215586":"code","9995cd0a":"code","cfd28027":"code","e8ee15b8":"code","6cd79113":"code","3d1aafc0":"code","349e5c17":"code","6afbaf9a":"code","74747016":"code","36894e56":"code","ac0841af":"markdown","465ae0c9":"markdown"},"source":{"18861746":"# Doing charges prediction ANN(artificial neural network)\n# comparing results with my previous model on this link  https:\/\/www.kaggle.com\/lukakujundilujan\/my-first-model-accuracy-86\n#Importing tools for our work\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n%matplotlib inline\n\ndf = pd.read_csv(\"..\/input\/insurance\/insurance.csv\")\ndf.head()","d97762da":"df.info()","bea12428":"sns.countplot(x=\"sex\", data=df)","d4b60fca":"#countplot that shows how much each person has children .. most of the people that are paying insurance have 0 or fewer than 3 kids. \nsns.countplot(x=\"children\", data=df);","5f6b1054":"#distribution of \"charges\"(how much is paid for insurance). red line is mean or an average.\nplt.figure(figsize=(12,4))\nsns.histplot(df['charges'],kde=False,bins=40)\nplt.axvline(df[\"charges\"].mean(), c=\"r\")\nplt.text(10000,120,'Mean',rotation=90, size =\"large\", c=\"r\");","7786a11a":"#correlation between some variables. notice that smokers, sex and region is not here.\n#We must convert those variables into the numerical categories..\ndf.corr()","4a58a9b4":"#distribution of BMI - body mass index. Red line is mean\nplt.figure(figsize=(12,8));\nsns.displot(df['bmi'])\nplt.axvline(df[\"bmi\"].mean(), c=\"r\")\nplt.text(23,100,'BMI Mean',rotation=90, size =\"large\", c=\"r\");\n","e1f5e674":"#smokers vs non smokers\nsns.countplot(x=\"smoker\", data=df);","ce60045a":"#average price of health insurance\ndf[\"charges\"].mean()","2c6afd56":"#average charges for health insurance per smoker - clearly if you smoke you pay much higher\ndf[(df['smoker'] == \"yes\")][\"charges\"].mean()","d7bb38a3":"df[(df['smoker'] == \"no\")][\"charges\"].mean()","eb76530f":"#regions are equally distributed\nsns.countplot(x=\"region\", data=df);","016a0911":"plt.figure(figsize=(12,7))\nsns.heatmap(df.corr(),annot=True,cmap='viridis');","7ea77a7f":"#in first correlation map , smokers, sex and region were not included. we will convert them (not regions) into num categories\ndf_for_heatmat =df.copy()\ndummies2 = pd.get_dummies(df_for_heatmat[['sex','smoker']],drop_first=True)\ndf_for_heatmat = pd.concat([df_for_heatmat,dummies2],axis=1)\ndf_for_heatmat = df_for_heatmat.drop(['sex','smoker'],axis=1)","e4484acf":"#we have people who smokes and males here\nplt.figure(figsize=(16,10))\nsns.heatmap(df_for_heatmat.corr(),annot=True,cmap='viridis');","030b0b63":"#how much skinny people pays for health insurance - obviously the lowest amount of the money\ndf[df[\"bmi\"]<18.5][\"charges\"].mean()\n\n","8c627b9f":"#our data shows , as BMI increases there is also an increase of charges. how correlation is not the strongest, we will see\ndf[(df['bmi'] > 18.5) & (df['bmi'] < 24.9)][\"charges\"].mean()","9678733a":"df[(df['bmi'] > 24.9) & (df['bmi'] < 29.9)][\"charges\"].mean()","f1497fa9":"df[(df['bmi'] > 29.9) & (df['bmi'] < 39.9)][\"charges\"].mean()","10a93870":"#BMI over 40 and charges. most expensive category, which makes sense\ndf[df[\"bmi\"]>39.9][\"charges\"].mean()","ff991ec9":"#there is some positive correlation between increased BMI and amount of money that you will pay for health insruance\nsns.regplot(x='charges',y='bmi',data=df,scatter_kws={\"alpha\":0.8,\"s\": 24}, line_kws={\"color\":\"purple\"})\n","1ffbd3ce":"#we make new copy , it will be called df2 , so we can manipulate with data, while we have original clean\ndf2 =df.copy()","3347301c":"#checking is there any significant difference between regions - as you can see there is not. \n#we can exclude this variable from our model\nfor x in df[\"region\"].unique():\n    print(\"Average cost per of the charges in region \",x ,\" is \", round(df.loc[df[\"region\"]==x][\"charges\"].mean(),2), \"$\")","a9bf046e":"#we are collectiing people that have 3,4,5 kids, into new category - 3 or more\ndf2[\"children\"]=df2[\"children\"].replace([3, 4,5], '3 or more')","10993248":"df2['children'].unique()","6d79d026":"child_order = [0, 1, 2,  '3 or more']","5ea9cf3e":"#new countplot now with \"3 or more kids\"\nsns.countplot(x=\"children\", data=df2 , order=child_order);","39237f1d":"df2[\"children\"].value_counts()","f1339443":"#regions are also equally distributed \nsns.countplot(x=\"region\", data=df) ","a68e4364":"#is there any difference in number of kids and charges for health insurance?\n\n#how much people with 3 or more kids pay\"\ndf2[df2[\"children\"]==\"3 or more\"][\"charges\"].mean()","ca257696":"#how much people with 0 kids pay\"\ndf2[df2[\"children\"]==0][\"charges\"].mean()","86c3c399":"#how much people with 1 kid pay\"\ndf2[df2[\"children\"]==1][\"charges\"].mean()","102f387f":"#how much people with 2 or more kids pay\"\ndf2[df2[\"children\"]==2][\"charges\"].mean()","1887f271":"#average price for health insurance for all regions(and all people)\ndf[\"charges\"].mean()","922b4eca":"#code will explain, i \"invented\" this code after struggling to find best way to explain this :)\nfor x in df[\"region\"].unique():\n    print(\"Average cost per of the charges in region \",x ,\" is \", round(df.loc[df[\"region\"]==x][\"charges\"].mean(),2), \"$\")","a5114ec6":"#code will explain\nfor x in df2[\"children\"].unique():\n    print(\"Average cost per of the charges if a number of kids is \",x , round(df.loc[df2[\"children\"]==x][\"charges\"].mean(),2), \"$\")","aceb3436":"len(df[\"age\"].unique())","22a9caac":"#distribution of age categories. most of people are young. that explains why here most of the people have 0 kids.\nsns.histplot(df['age'],kde=False,bins=47);","375fd0d2":"#some weak positive correlation - older you get, more you pay, but correlation is weak.\nsns.regplot(x='age',y='charges',data=df,scatter_kws={\"alpha\":0.8,\"s\": 24}, line_kws={\"color\":\"purple\"})","5175f079":"#interesting -age and people who are paying over 30 000$. some positive weak correlation\nsns.regplot(x =\"age\", y=\"charges\", data=df[df[\"charges\"]>30000], line_kws={\"color\":\"purple\"});","9fc6827c":"#age and people who are paying under 30 000$. some positive correlation\nsns.regplot(x =\"age\", y=\"charges\", data=df[(df['charges'] < 30000)], line_kws={\"color\":\"purple\"});","253011e6":"#correlation between age and people who are paying under 30 000$. some positive correlation\ndf[(df['charges'] < 30000)][\"charges\"].corr(df[\"age\"])","82a04288":"#correlation between age and charges overall -some positive weak correlation\ndf['charges'].corr(df[\"age\"])","7af50422":"#checking is there any null(empty)data\ndf.isna().sum()","eab6b081":"#as we concluded , we are droping region data, because i think it is totaly irrelevant for our model, \n\ndf2=df2.drop(\"region\", axis=1)","91346c1f":"#if we want to make our model we need to convert everything into numbers. we are makeing new categories from sex, childern and smoker variable\ndummies = pd.get_dummies(df2[['sex', 'children','smoker']],drop_first=True)\ndf2 = df2.drop(['sex', 'children','smoker'],axis=1)\ndf2 = pd.concat([df2,dummies],axis=1)\ndf2.T","85d46a2b":"#time to import our model\nfrom sklearn.model_selection import train_test_split","bdaf7953":"#prepare our model. \nX =df2.drop(\"charges\", axis=1).values\ny =df2[\"charges\"].values","78c8cb46":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","cb21afae":"#to use tensorflow keras models we need to scale variables\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","63a66651":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nX_train.shape","e577f2f1":"#i am not an expert in this. I watched one turtorial. Here is Sequental model. \n#We have 4 \"hidden layers\" with 7 neurons , as our X_train shape have 7 input variables\nmodel =Sequential()\nmodel.add(Dense(7, activation=\"relu\"))\nmodel.add(Dense(7, activation=\"relu\"))\nmodel.add(Dense(7, activation=\"relu\"))\nmodel.add(Dense(7, activation=\"relu\"))\n#aparently optimizer adam is best one.there is more on wikipedia for \"relu\" and \"adam\". i dont know very much into details to be honest\nmodel.add(Dense(1))\nmodel.compile(optimizer=\"adam\", loss=\"mse\",metrics=[])","aad44506":"model.fit(x=X_train, y=y_train, validation_data=(X_test, y_test), batch_size=128, epochs=400)","37aa4d90":"#how or model was performing\nlosses =pd.DataFrame(model.history.history)\nlosses.plot()","15516299":"#lets check how far away we are from predicted values\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, explained_variance_score, r2_score","76727c56":"predictions =model.predict(X_test)","f9b14e86":"#on average our model is missing about 4000$. one month ago i used RandomForestRegressor from sklearn and i was off about 2500\n#that proves that RandomForestRegressor could be better for simple lineral models. \nmean_absolute_error(y_test, predictions)","eceb109f":"df[\"charges\"].describe()","0f3a27ee":"explained_variance_score(y_test, predictions)","085b653f":"r2_score(y_test, predictions)","4791c4b5":"#how far we are off from \"red line\"\nplt.scatter(x =y_test, y=predictions )\nplt.plot(y_test,y_test,\"r\");","d3358679":"#Here i am experimenting. I ll change model params, like -less layers, include dropout, early stoping, exclude most expensive charges","fee76e57":"model2 =Sequential()\nmodel2.add(Dense(7, activation=\"relu\"))\nmodel2.add(Dense(4, activation=\"relu\"))\nmodel2.add(Dense(1))\nmodel2.compile(optimizer=\"adam\", loss=\"mse\")","5c812bc5":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop= EarlyStopping(monitor=\"val_loss\",mode=\"min\",verbose=1, patience=25)\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.20, random_state=42)\nX_train2 = scaler.fit_transform(X_train2)\nX_test2 = scaler.transform(X_test2)","30ac7f30":"model2.fit(x=X_train2, y=y_train2, validation_data=(X_test2, y_test2), batch_size=64, epochs=400, callbacks=[early_stop])","3345402f":"losses2 =pd.DataFrame(model2.history.history)\nlosses2.plot()","04cace85":"predictions2 =model2.predict(X_test2)","c9fa528d":"mean_absolute_error(y_test2, predictions2)","85c05ae9":"explained_variance_score(y_test2, predictions2)","07a99e35":"r2_score(y_test2, predictions2)","dc36ff92":"#Lets plot second model\nplt.scatter(x =y_test2, y=predictions2 )\nplt.plot(y_test2,y_test2,\"r\")","4ea2146e":"#lets go on 3rd model\ndf3 =df2.copy()","02914ccc":"#last try. lets exclude those few off values - people who are paying over 50 000$ for health insurance. \n# also i will introduce Dropout. Shall we get better results?\ndf3 =df3.drop(df3.loc[df[\"charges\"]>50000].index)","bc4d2d9c":"#7 most expensive charges excluded from our model\nlen(df2)-len(df3)","f44b934f":"#lets make new model without those \"over 50 000$ charges\"\nX3 =df3.drop(\"charges\", axis=1).values\ny3 =df3[\"charges\"].values","9f3df9df":"X_train3, X_test3, y_train3, y_test3 = train_test_split(X3, y3, test_size=0.3, random_state=42)","080c2c4c":"X_train3 = scaler.fit_transform(X_train3)\nX_test3 = scaler.transform(X_test3)","cb7306cc":"from tensorflow.keras.layers import Dropout","63b1464d":"model3 =Sequential()\nmodel3.add(Dense(7, activation=\"relu\"))\nmodel3.add(Dropout(0.5))\nmodel3.add(Dense(4, activation=\"relu\"))\nmodel3.add(Dropout(0.5))\nmodel3.add(Dense(1))\nmodel3.compile(optimizer=\"adam\", loss=\"mse\")","78a8ea34":"early_stop= EarlyStopping (monitor=\"val_loss\",mode=\"min\",verbose=1, patience=25)","569568c5":"model3.fit(x=X_train3, y=y_train3, validation_data=(X_test3, y_test3), batch_size=32, epochs=150,\n          callbacks=[early_stop]\n         )","6a5d49f8":"losses3 =pd.DataFrame(model3.history.history)\nlosses3.plot()","104663f4":"predictions3 =model3.predict(X_test3)","11702706":"mean_absolute_error(y_test3, predictions3)","82a78302":"r2_score(y_test3, predictions3)","76a8059b":"explained_variance_score(y_test3, predictions3)","d5368779":"#our model with r2_score, and mean_absolute_error now is even less useful. We cant use it.\nplt.scatter(x =y_test3, y=predictions3 )\nplt.plot(y_test3,y_test3,\"r\")","e0373d26":"#lets pick some info from our dataset!(we will mask it so model will think its new data ofc)\ndf2.iloc[0]","70215586":"guess=df2.iloc[0].drop(\"charges\").values","9995cd0a":"guess","cfd28027":"round(df2.iloc[0][\"charges\"],2)","e8ee15b8":"print(\"while test price of health insurance was \",df2.iloc[0][\"charges\"],\" model 1 predicted \", model.predict(scaler.transform(guess.reshape(1, -1))) ,\" ,model 2 predicted \", model2.predict(scaler.transform(guess.reshape(1, -1))), \" and model 3 predicted \", model3.predict(scaler.transform(guess.reshape(1, -1))),\"$\")","6cd79113":"#lets try again!\ndf2.iloc[617]","3d1aafc0":"guess2=df2.iloc[617].drop(\"charges\").values","349e5c17":"print(\"while test price of health insurance was \",round(df2.iloc[617][\"charges\"],2),\" model 1 predicted \", model.predict(scaler.transform(guess2.reshape(1, -1))) ,\" ,model 2 predicted \", model2.predict(scaler.transform(guess2.reshape(1, -1))), \" and model 3 predicted \", model3.predict(scaler.transform(guess2.reshape(1, -1))),\"$\")","6afbaf9a":"#and fun thing - lets see how much I would pay\n#i am 34 old male, i do not smoke, i dont have kids, and my BMI is 23\nguess_me =pd.DataFrame(data={\"age\":[34], \"bmi\":[23], \"sex\":[1], \"child1\":[0],\"child2\":[0],\"child3\":[0], \"smoker\":[0]})","74747016":"guess_me=guess_me.values","36894e56":"print(\" model 1 predict that i will pay \", model.predict(scaler.transform(guess_me.reshape(1, -1))) ,\" ,model 2 predict \", model2.predict(scaler.transform(guess_me.reshape(1, -1))), \" and model 3 predict \", model3.predict(scaler.transform(guess_me.reshape(1, -1))),\"$\")","ac0841af":"Lets make some predictions!","465ae0c9":"Conlcusion. Scikit Learn RandomForestRegressor seems to better at this. Totally i think we need more samples, and maybe one or more important categories.. like, i am not a expert -but maybe - body fat index, heart rate index .. i would pick that if i was insurance agency."}}