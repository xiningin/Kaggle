{"cell_type":{"8ba8b457":"code","f9626d63":"code","058ceb34":"code","aa2c54b1":"code","9a622d81":"code","8799c2c0":"code","9dc5fbca":"code","703b2cf5":"code","a28f681e":"code","7c02fb9b":"code","fa574ea1":"code","ca62ff3c":"code","fd68e4d8":"code","fe0f0fbc":"code","25861883":"code","f7718a4e":"code","9e9cf282":"code","bb3e0e1c":"code","6460f5d1":"code","d54da552":"code","901d43a1":"code","d82a4c42":"code","6ee5eab4":"code","c2a311f6":"code","ec9350fa":"code","c92b07ed":"markdown","8fbcf993":"markdown","d3d11fcb":"markdown"},"source":{"8ba8b457":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\/\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9626d63":"sample_submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntrain_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')","058ceb34":"train_targets_scored.head()","aa2c54b1":"train_features.head()","9a622d81":"test_features.head()","8799c2c0":"train_targets_scored.sum()[1 : ].sort_values()","9dc5fbca":"#targets = ['atp-sensitive_potassium_channel_antagonist', 'erbb2_inhibitor']\n#train_targets_scored.loc[:, targets] = 0","703b2cf5":"targets = ['atp-sensitive_potassium_channel_antagonist', 'erbb2_inhibitor']\ntrain_targets_scored.loc[:, targets] = 0.000012","a28f681e":"X = train_features.iloc[: ,2:].values\ny = train_targets_scored.iloc[:,1:].values\nX_final = test_features.iloc[: ,2:].values\n","7c02fb9b":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nX[:,1] = le.fit_transform(X[:,1])\nX_final[:,1] = le.fit_transform(X_final[:,1])","fa574ea1":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)\nX_final = sc.transform(X_final)","ca62ff3c":"from sklearn.preprocessing import MinMaxScaler\nmmsc = MinMaxScaler() \nX = sc.fit_transform(X) \nX_final = sc.transform(X_final) ","fd68e4d8":"from sklearn.preprocessing import QuantileTransformer\nqt = QuantileTransformer()\nX[:,2:] = qt.fit_transform(X[:,2:])\nX_final[:,2:] = qt.transform(X_final[:,2:])","fe0f0fbc":"from sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\nX[:,2:] = rs.fit_transform(X[:,2:])\nX_final[:,2:] = rs.transform(X_final[:,2:])\n","25861883":"print(X)\nprint(X.shape)","f7718a4e":"print(X_final)\nprint(X_final.shape)","9e9cf282":"from sklearn.model_selection import train_test_split\n#X_1, X_test, y_1, y_test = train_test_split(X, y, test_size = 0.2)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2)","bb3e0e1c":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Activation,Dense,Dropout,BatchNormalization,Input\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras import regularizers\nimport tensorflow_addons as tfa","6460f5d1":"model = Sequential([\n    Input(X_train.shape[1]),\n    BatchNormalization(),\n    Dropout(0.9),\n    tfa.layers.WeightNormalization(Dense(2048, activation=\"relu\")),\n    BatchNormalization(),\n    Dropout(0.4),\n    tfa.layers.WeightNormalization(Dense(1048, activation=\"relu\")),\n    BatchNormalization(),\n    Dropout(0.2),\n    tfa.layers.WeightNormalization(Dense(1048, activation=\"relu\")),\n    BatchNormalization(),\n    Dropout(0.2),\n    tfa.layers.WeightNormalization(Dense(1048, activation=\"relu\")),\n    BatchNormalization(),\n    Dropout(0.2),\n    tfa.layers.WeightNormalization(Dense(y_train.shape[1], activation=\"sigmoid\"))\n])\n\nmodel.compile(\n    optimizer='adam',\n    loss='binary_crossentropy',\n    metrics=['accuracy']\n)\n\nmodel.summary()","d54da552":"def my_func(arg):\n  arg = tf.convert_to_tensor(arg, dtype=tf.float32)\n  return arg","901d43a1":"import matplotlib.pyplot as plt\n","d82a4c42":"history = model.fit(\n    my_func(X_train), my_func(y_train), verbose=2, epochs=50,\n    validation_data=(my_func(X_val), my_func(y_val)),batch_size=32,\n    callbacks=[\n        tf.keras.callbacks.ReduceLROnPlateau(\n            monitor='val_loss', \n            factor=0.3, \n            patience=3,\n            epsilon = 1e-4, \n            mode = 'min',\n            verbose=1\n        ),\n        tf.keras.callbacks.EarlyStopping(\n            monitor='val_loss',\n            min_delta=0,\n            patience=10,\n            mode='auto',\n            verbose=1,\n            baseline=None,\n            restore_best_weights=True\n        )\n    ]\n)","6ee5eab4":"loss_train = history.history['loss']\nloss_val = history.history['val_loss']\nepochs = range(0,len(loss_train))\nplt.plot(epochs, loss_train, 'g', label='Training loss')\nplt.plot(epochs, loss_val, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","c2a311f6":"loss_train = history.history['accuracy']\nloss_val = history.history['val_accuracy']\nepochs = range(0, len(loss_train)) \nplt.plot(epochs, loss_train, 'g', label='Training accuracy')\nplt.plot(epochs, loss_val, 'b', label='validation accuracy')\nplt.title('Training and Validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","ec9350fa":"y_pred = model.predict(my_func(X_final))\ny_pred = np.clip(y_pred,0.001,0.999) \ncolumns = list(sample_submission.columns)\ncolumns.remove('sig_id')\n\nfor i in range(len(columns)):\n    sample_submission[columns[i]] = y_pred[:, i]\n\nsample_submission.to_csv('submission.csv', index=False)","c92b07ed":"Feature Scalling (0,1)","8fbcf993":"version 1 is for public all other version are experiment","d3d11fcb":"MODEL TENSOR FLOW NEURAL NETWORK"}}