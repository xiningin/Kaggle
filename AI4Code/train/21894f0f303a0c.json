{"cell_type":{"d012f2ac":"code","6f102cd6":"code","10b73868":"code","f3992654":"code","bd091f51":"code","13106111":"code","2fe5b6c0":"code","b50044bc":"code","e2420f81":"code","a0191933":"code","7fe8ee03":"code","c41ad659":"code","8f84cfee":"code","0446fc65":"code","73ce9aba":"code","afa850ff":"code","bb7e8b22":"code","65d6f6c6":"code","d5c298cd":"code","825efb0c":"code","b78c5c8f":"code","d22b21e2":"code","1b2baee0":"code","17212a8a":"markdown","445088f0":"markdown","90957ffb":"markdown","e95abbce":"markdown","a4b9e4e4":"markdown","8796de4c":"markdown","d9900104":"markdown","dbf38807":"markdown","3de97ceb":"markdown","5eade5fe":"markdown","64b5c312":"markdown","0a2f4133":"markdown","1b4f105d":"markdown","12214337":"markdown","9f8e8105":"markdown","03d5b0ae":"markdown","a46bddd3":"markdown","6408299a":"markdown","763d5f50":"markdown"},"source":{"d012f2ac":"#loading data\n\nimport numpy as np \nimport pandas as pd \nimport riiideducation \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\nimport os\nimport warnings \nwarnings.filterwarnings('ignore')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/riiid-test-answer-prediction'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","6f102cd6":"train_df = pd.read_csv('\/kaggle\/input\/riiid-test-answer-prediction\/train.csv', low_memory=False, nrows=10**6, \n                       dtype={'row_id': 'int64', 'timestamp': 'int64', 'user_id': 'int32', 'content_id': 'int16', 'content_type_id': 'int8',\n                              'task_container_id': 'int16', 'user_answer': 'int8', 'answered_correctly': 'int8', 'prior_question_elapsed_time': 'float32', \n                             'prior_question_had_explanation': 'boolean',\n                             }\n                      )\ntrain_df.head()","10b73868":"question = pd.read_csv('..\/input\/riiid-test-answer-prediction\/questions.csv')\nquestion.head()","f3992654":"lecture = pd.read_csv('..\/input\/riiid-test-answer-prediction\/lectures.csv')\nlecture.head()","bd091f51":"\nquestions_interactions = train_df.merge(question, left_on = 'content_id', right_on = 'question_id', how = 'left')\nquestions_interactions = questions_interactions[questions_interactions.content_type_id == 0]\nquestions_interactions.rename(columns = {'part': 'test_part'}, inplace = True)\n\nlectures_interactions = train_df.merge(lecture, left_on = 'content_id', right_on = 'lecture_id', how = 'left') \nlectures_interactions.rename(columns = {'part': 'category'}, inplace = True)\nlectures_interactions = lectures_interactions[lectures_interactions.content_type_id == 1]\n\nquestions_interactions.shape, lectures_interactions.shape","13106111":"print(questions_interactions.isnull().sum())\nlectures_interactions.isnull().sum()","2fe5b6c0":"# let's give the prior_question_elapsed_time nulls -1 as the user didn't didn't have previous bundle yet\n# and -1 for the prior_question_had_explanation after converting booleans into integers\n#as they didn't have any questions before\nindeces = questions_interactions[questions_interactions.prior_question_had_explanation.isnull()].index\nprint(indeces)\nvalues = {'prior_question_elapsed_time': -1, 'prior_question_had_explanation': False}\nquestions_interactions.fillna(value=values,inplace=True)\nquestions_interactions.prior_question_had_explanation = questions_interactions.prior_question_had_explanation.astype('int8')\nquestions_interactions.loc[indeces,'prior_question_had_explanation'] = -1\nquestions_interactions.head()","b50044bc":"questions_interactions.prior_question_had_explanation.value_counts()","e2420f81":"lectures_interactions.head()","a0191933":"lectures_interactions.drop(columns=['content_type_id','user_answer','answered_correctly','prior_question_elapsed_time','prior_question_had_explanation'],inplace =True)\nlectures_interactions.head()","7fe8ee03":"start_mem_usg1 = questions_interactions.memory_usage().sum() \/ 1024**2 \nstart_mem_usg2 = lectures_interactions.memory_usage().sum() \/ 1024**2 \n\nprint(\"Memory usage of questions_interactions dataframe is :\",start_mem_usg1,\" MB\")\nprint(\"Memory usage of lectures_interactions dataframe is :\",start_mem_usg2,\" MB\")","c41ad659":"questions_interactions.dtypes","8f84cfee":"# as the content id is the same as the question id and the content_type_id has only the zero values\n# we will drop them\nquestions_interactions.drop(columns=['content_id','content_type_id'],inplace=True)","0446fc65":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n           # print(\"dtype after: \",props[col].dtype)\n           # print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props, NAlist\n\n","73ce9aba":"questions_interactions,_ = reduce_mem_usage(questions_interactions)","afa850ff":"questions_interactions.dtypes","bb7e8b22":"continous_columns = ['timestamp','user_id','task_container_id','prior_question_elapsed_time','question_id','bundle_id']\nquestions_interactions.hist(column=continous_columns, grid=False,figsize=(20,15));","65d6f6c6":"color = sns.color_palette()[0]\ndiscrete_columns = ['answered_correctly','user_answer','correct_answer','prior_question_had_explanation','test_part']\nfor col in discrete_columns:\n    plt.figure(figsize=(5,5))\n    sns.countplot(data=questions_interactions,x=col,color=color)\n    plt.show()","d5c298cd":"df = questions_interactions.copy()\ndf = df.assign(tags2=df['tags'].str.split(' ')).explode('tags2')\ndf['tags2'] = df['tags2'].astype('int32') \ndf['tags2'].nunique()","825efb0c":"df['tags2'].hist(grid=False,figsize=(10,5));","b78c5c8f":"lectures_interactions.head()","d22b21e2":"continous_cols = ['timestamp','user_id','content_id','task_container_id','lecture_id','tag']\nlectures_interactions.hist(column=continous_cols, grid=False,figsize=(20,15));","1b2baee0":"color = sns.color_palette()[0]\ndiscrete_columns = ['type_of','category']\nfor col in discrete_columns:\n    plt.figure(figsize=(5,5))\n    sns.countplot(data=lectures_interactions,x=col,color=color)\n    plt.show()","17212a8a":"## Wait for part 2 with more insights and modular data wrangling :)","445088f0":"## Let's take a look at the lectures_interactions data frame","90957ffb":"### Let's deal with nulls","e95abbce":"The distribution is more like normal distribution which may give the tags ids meaning.","a4b9e4e4":"* The category has the same shape like the test_part in the question_interactions which suggests close relationship between them\n* Most of the lectures types are concept based.","8796de4c":"* The number of answered correctly questions is double the wrong answered questions\n* Most of the questions had explanation\n* Users answers and correct answers are very similar (however there are a lot of wrong answers) which may indicates an interesting relation between them\n* The 5th test part has a lot of records followed by the second part","d9900104":"> The columns that has nulls in the question interactions are the columns that puts nulls in the place where ni previous records are present.","dbf38807":"## Let's try improving the memory usage for faster analysis","3de97ceb":"## Let's firstly import the libraries","5eade5fe":"Now let's deal with the lectures interactions nulls","64b5c312":"## Let's explore the questions interaction distributions","0a2f4133":"* content_type_id\n* user_answer\n* answered_correctly \n* prior_question_elapsed_time \n* prior_question_had_explanation\n\ndoesn't have any meaning in this data frame anymore so we will drop them.","1b4f105d":"* The time stamp has a left skewed distributions which may indicate the relative small times the users use the application in before stopping using it and \n* user_id, content_id, lecture_id looks to have uniform distributions which means they are just random ids\n* task_container_id is left skewed which is strange and may indicate a meaning in these ids which makes thier distribution affected by the student actions\n* The tags here have uniform distribution.","12214337":"  * The interactions type is represented in the content_type_id column (0 for question interaction and 1 for question interaction)","9f8e8105":"## Refrences \n1- memory used reduction: https:\/\/www.kaggle.com\/cdeotte\/dae-book3c from the cool grand master: Chris Deotte\n\n2- https:\/\/stackoverflow.com\/questions\/12680754\/split-explode-pandas-dataframe-string-entry-to-separate-rows","03d5b0ae":"> Most of the interactions are with questions.","a46bddd3":"## Let's preprocess the tags feature and explore it too","6408299a":"* The bundle_id and the question_id have the same exact distribution which may indicate a duplicate column.\n* user_id has uniform distributioin which indicates random user ids are used in the application\n* The time stamp has a left skewed distributions which may indicate the relative small times the users use the application in before stopping using it and \n* prior_question_elapsed_time is left skewed too which may indicate that most of the students don't take a lot of time before answering a question\n* task_container_id is left skewed which is strange and may indicate a meaning in these ids which makes thier distribution affected by the student actions","763d5f50":"* The question csv and lecture csv files hold information about the questions and the lectures present in the training dataset as the types of the interactions in the training data sets are either an interaction with lectures or questions."}}