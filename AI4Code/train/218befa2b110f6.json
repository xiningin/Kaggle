{"cell_type":{"fccde3ca":"code","15121f32":"code","f40570db":"code","0aff7064":"code","b5f7d163":"code","a905a2b6":"code","a63c72cc":"code","eb3f519a":"code","d1ec2edf":"code","763e7e56":"code","54f18fb6":"code","4acb0f9a":"code","f791cd4d":"code","68b27524":"code","5bb75a6e":"code","83363242":"code","32cc4504":"code","6111cf5d":"code","7234aa38":"code","3cf0baa6":"code","a5ad38c2":"code","f3490fa4":"code","b305e698":"code","28cfe720":"code","58f249c8":"code","32bde12a":"code","bd5c7103":"code","c75caa72":"code","8f590c3f":"markdown","e77bafee":"markdown","1797c6ee":"markdown","ef82130c":"markdown","f17ae9f6":"markdown","76c12b42":"markdown","b682058a":"markdown","82d62f54":"markdown","384cb520":"markdown","15f4d5ea":"markdown","7053cfdc":"markdown","da7fe3af":"markdown","7fb089fe":"markdown","718ff7bf":"markdown","fd2705e0":"markdown","6885af5e":"markdown","a1759e3a":"markdown","bf9725b4":"markdown","da050dc8":"markdown","c4d0e6c7":"markdown","31ca623b":"markdown","49199b86":"markdown","9aff2391":"markdown","0f9c31fa":"markdown","bf00e1c6":"markdown","cc675177":"markdown","bb0edf85":"markdown","798f72a3":"markdown","163ef248":"markdown","e894ea9b":"markdown","7878db0e":"markdown","0a974b51":"markdown","a4cfd4de":"markdown","9807b2d3":"markdown","75841729":"markdown","b031f3d4":"markdown","6fbf3edd":"markdown","7576a33d":"markdown","55b23d0b":"markdown","ae9f63c3":"markdown"},"source":{"fccde3ca":"from statsmodels.graphics.tsaplots import plot_pacf                 #Used to Plot PACF plot.\nfrom statsmodels.graphics.tsaplots import plot_acf                  #Used to Plot ACF plot.\nfrom statsmodels.tsa.arima_process import ArmaProcess               #Used to generate data point\nfrom statsmodels.tsa.stattools import pacf                          #Used to find PACF characteristics.\nfrom statsmodels.regression.linear_model import yule_walker         #Used to estimate parameters \nfrom statsmodels.tsa.stattools import adfuller                      #Used to check stationarity\nimport matplotlib.pyplot as plt               \nimport numpy as np\n%matplotlib inline","15121f32":"ar3 = np.array([1, 0.8, 0.6,0.1])\nma = np.array([1])\nsimulated_AR3_data = ArmaProcess(ar3, ma).generate_sample(nsample=1000)\nplt.figure(figsize=[10, 7.5]); # Set dimensions for figure\nplt.plot(simulated_AR3_data)\nplt.title(\"Simulated AR(3) Process\")\nplt.show()","f40570db":"plot_acf(simulated_AR3_data,lags=20);","0aff7064":"plot_pacf(simulated_AR3_data,lags=20);","b5f7d163":"rho, sigma = yule_walker(simulated_AR3_data, 3, method='mle')\nprint(f'rho: {-rho}')\nprint(f'sigma: {sigma}')","a905a2b6":"ar3 = np.array([1])\nma3 = np.array([1, 0.9, 0.3,0.8])\nMA3_process = ArmaProcess(ar3, ma3).generate_sample(nsample=1000)","a63c72cc":"plt.figure(figsize=[10, 7.5]); # Set dimensions for figure\nplt.plot(MA3_process)\nplt.title('Moving Average Process of Order 3')\nplt.show()","eb3f519a":"plot_acf(MA3_process, lags=20)\nplt.show()","d1ec2edf":"plot_pacf(MA3_process,lags=20)\nplt.show()","763e7e56":"arparams = [.75, -.25]\nmaparams = [.65, .35]\narma_process = ArmaProcess.from_coeffs(arparams, maparams).generate_sample(nsample=1000)","54f18fb6":"plt.figure(figsize=[10, 7.5]); # Set dimensions for figure\nplt.plot(arma_process)\nplt.title('ARMA Process of Order (2,2)')\nplt.show()","4acb0f9a":"plot_acf(arma_process, lags=20)\nplt.show()","f791cd4d":"plot_pacf(arma_process,lags=20)\nplt.show()","68b27524":"import pandas as pd\nurl = 'https:\/\/github.com\/marcopeix\/time-series-analysis\/blob\/master\/data\/jj.csv?raw=True'\ndf = pd.read_csv(url)\nprint(df.head(5))","5bb75a6e":"import matplotlib.pyplot as plt\nplt.figure(figsize=[15, 7.5]); # Set dimensions for figure\nplt.scatter(df['date'], df['data'])\nplt.title('Quarterly EPS for Johnson & Johnson')\nplt.ylabel('EPS per share ($)')\nplt.xlabel('Date')\nplt.xticks(rotation=90)\nplt.grid(True)\nplt.show()","83363242":"from statsmodels.graphics.tsaplots import plot_pacf                 #Used to Plot PACF plot.\nfrom statsmodels.graphics.tsaplots import plot_acf                  #Used to Plot ACF plot.\nplot_acf(df['data'],lags=20)\nplot_pacf(df['data'],lags=20)\nplt.show()","32cc4504":"from statsmodels.tsa.stattools import adfuller\nad_fuller_result = adfuller(df['data'])\nprint(f'ADF Statistic: {ad_fuller_result[0]}')\nprint(f'p-value: {ad_fuller_result[1]}')","6111cf5d":"import numpy as np\ndf['data'] = np.log(df['data'])\ndf['data'] = df['data'].diff()\ndf = df.drop(df.index[0])\ndf.head()","7234aa38":"plt.figure(figsize=[15, 7.5]); # Set dimensions for figure\nplt.plot(df['data'])\nplt.title(\"Log Difference of Quarterly EPS for Johnson & Johnson\")\nplt.show()","3cf0baa6":"ad_fuller_result = adfuller(df['data'])\nprint(f'ADF Statistic: {ad_fuller_result[0]}')\nprint(f'p-value: {ad_fuller_result[1]}')","a5ad38c2":"plot_pacf(df['data'], lags=20);\nplot_acf(df['data'], lags=20);\nplt.show()","f3490fa4":"from itertools import product \nfrom tqdm import tqdm_notebook\nps = range(0, 8, 1)\nd = 1\nqs = range(0, 8, 1)\n# Create a list with all possible combination of parameters\nparameters = product(ps, qs)\nparameters_list = list(parameters)\norder_list = []\nfor each in parameters_list:\n    each = list(each)\n    each.insert(1, 1)\n    each = tuple(each)\n    order_list.append(each)","b305e698":"%%capture\nimport statsmodels.api as sm\n\nresults=[]\nfor order in order_list:\n    try: \n        model = sm.tsa.statespace.SARIMAX(df['data'], order=order).fit(disp=-1);\n    except:\n        continue  \n    aic = model.aic;\n    results.append([order, model.aic]);","28cfe720":"result_df = pd.DataFrame(results)\nresult_df.columns = ['(p, d, q)', 'AIC']\n#Sort in ascending order, lower AIC is better\nresult_df = result_df.sort_values(by='AIC', ascending=True).reset_index(drop=True)","58f249c8":"result_df.head(5)","32bde12a":"best_model = sm.tsa.statespace.SARIMAX(df['data'], order=(3,1,3)).fit()\nprint(best_model.summary())","bd5c7103":"from statsmodels.stats.diagnostic import acorr_ljungbox\nljung_box, p_value = acorr_ljungbox(best_model.resid)\nprint(f'Ljung-Box test: {ljung_box[:10]}')\nprint(f'p-value: {p_value[:10]}')","c75caa72":"plot_pacf(best_model.resid,lags=20);\nplot_acf(best_model.resid,lags=20);\nplt.show()","8f590c3f":"The ACF is not informative and we see some sinusoidal shape. The PACF suggests an $AR(3) \\textrm{ or } AR(4)$ process. We use the AIC score to decide.","e77bafee":"The p-value is clearly $\\geq 0.05, \\textrm{our } \\alpha$, meaning the we cannot reject the null hypothesis stating that the time series is non-stationary. Therefore, we must apply some transformation and some differencing to remove the trend and remove the change in variance.","1797c6ee":"## Auto Correlation Function (ACF)\nACF is an (complete) auto-correlation function which gives us values of auto-correlation of any series with its lagged values.\n\nThe Auto Correlation Function (ACF) is defined as:\n\n$$ \\rho (s,t) = \\frac{\\gamma(s,t)}{\\sqrt{\\gamma(s,s) \\gamma(t,t)}} $$","ef82130c":"# Simulation of MA(3) Process:\nWe simulate the MA(3) Process : $$y_t = 3+ 0.3 \\epsilon_{t-1} + 0.1 \\epsilon_{t-2} + 0.05 \\epsilon_{t-3}$$","f17ae9f6":"## Auto-covariance Function\nAssuming the variance of $X_t$ is finite, the autocovariance function is given by\n$$ \\gamma (s,t) = cov(X_s, X_t) = \\mathbb{E} [(X_s - \\mu_s)(X_t - \\mu_t)] $$\n\nIt measures the linear dependence between two points on the same series observed at different times.\n","76c12b42":"## Schwartz Bayesian Information Criterion (SBC)\nThe SBC is defined as \n$$ SBC = k log(n) -2 log (L(\\theta)) $$\nn is the sample size; the number of observations or number of data points. k is the number of parameters which the model estimates, and $\\theta$ is the set of all parameters.\n\n$L(\\theta)$ represents the likelihood of the model tested, given the data, when evaluated at maximum likelihood values of $\\theta$.\n\nComparing models with the Bayesian information criterion simply involves calculating the BIC for each model. The model with the lowest BIC is considered the best.","b682058a":"## Stationary Time Series\nA stationary time series is one whose statistical properties such as mean, variance, autocorrelation, etc. are all constant over time. Tests like the Dickey-Fuller test can be performed to check if a time series is staionary or not. Stationarity of a time series is an important assumption for many time series models. \nIf the time series is not stationary, \"transformations\" can be performed to make it stationary, like differencing or log-differencing.","82d62f54":"## ARMA\nARMA(p,q) process is the combination of AR(p) process and MA(q) process. It can be mathematically expressed as:\n$$ y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\textrm{...} + \\phi_p y_{t-p} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\textrm{...} + \\theta_q \\epsilon_{t-q} + \\epsilon_t $$","384cb520":"These plots resemble that of white noise. So we can proceed to forecasting.","15f4d5ea":"## Box-Jenkins Methodology\n\nThese techniques are used in the Box-Jenkins Methodology to find the optimal process to model a time-series.\nThe steps in the Box-Jenkins Methodology are:\n","7053cfdc":"The null hypothesis for the Ljung-Box test is that there is no autocorrelation. Looking at the p-values above, we can see that they are above 0.05. Therefore, we cannot reject the null hypothesis, and the residuals are indeed not correlated. We can further support that by plotting the ACF and PACF of the residuals.","da7fe3af":"ACF plot is a bar chart of coefficients of correlation between a time series and it lagged values","7fb089fe":"PACF plot has a sharp fall after lag 3, suggesting a AR(3) process.","718ff7bf":"The order associated with the lowest AIC is (3,1,3). Now, we can print a summary of the best model, which an ARIMA (3,1,3).","fd2705e0":"## Akaike Information Criterion (AIC)\nThe Akaike information criterion (AIC) is an estimator of out-of-sample prediction error and thereby relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection. It estimates models relatively, meaning that AIC scores are only useful in comparison with other AIC scores for the same dataset. A lower AIC score is better.\nAIC works by evaluating the model\u2019s fit on the training data, and adding a penalty term for the complexity of the model (similar fundamentals to regularization). The desired result is to find the lowest possible AIC, which indicates the best balance of model fit with generalizability. This serves the eventual goal of maximizing fit on out-of-sample data.\n$$ AIC = -2 ln(K) + 2k $$\nwhere L is likelihood and k is the number of parameters. \nAIC uses a model\u2019s maximum likelihood estimation (log-likelihood) as a measure of fit. Log-likelihood is a measure of how likely one is to see their observed data, given a model. The model with the maximum likelihood is the one that \u201cfits\u201d the data the best. The natural log of the likelihood is used as a computational convenience.\n\nNote: AIC makes an assumption that all models are trained on the same data, so using AIC to decide between different orders of differencing is technically invalid, since one data point is lost through each order of differencing.\n\nAssumpions: \n1) Using the same data between models.\n2) Measuring the same outcome variable between models.\n3) Have a sample of infinite size (Often a large sample is good enough to approximate, but since using AIC often means that you have a small sample size, there is a sample-size adjusted formula called AICc, which adds a correction term that converges to the AIC answer for large samples, but gives a more accurate answer for smaller samples.).\n\n\nAlthough the easiest way to is to simply choose the model with the lowest AIC score. Understanding the AIC scores further and choosing a model where residuals are uncorrelated and are white noise and all the parameters are significant whilst maintaining the lowest possible AIC score is a better idea.\n\nIf we have $n$ models with scores $AIC_1, AIC_2, \\textrm{...}, AIC_n$, then the probability that the $i^{th}$ model minimizes information loss is given by $P_i = e^{\\frac{AIC_{min}-AIC_i}{2}}$. \n\nThe best approach to choose the appropriate model is:\n1) Set an alpha level (say = 0.05) and reject any model with $P_i \\leq \\alpha$.\n2) For the rest of the models, we can created a weighted sum of the models according to their probabiliy or choose the model which satisfies all important assumptions while maintaining the lowest AIC score.\n\n\nSome downfalls: AIC only measures the relative quality of models. This means that all models tested could still fit poorly.\n\n\n","6885af5e":"# Identification of the type of Process using ACF-PACF plots","a1759e3a":"The equation above says that the position X at time $t$ depends on the noise at time $t$, plus the noise at time $t-1$ (with a certain weight theta), plus some noise at time $t-2$ (with a certain weight and so on till $t-q$.","bf9725b4":"The plots above are also a clear indication of non-stationarity. We conduct the Dickey-Fuller Test to confirm our hypothesis:","da050dc8":"1) Identification of the model:\nUsing the correlogram (ACF Plot) and Partial Correlogram (PACF Plot) we decide the p,d,q for ARIMA(p,d,q).\n\n2) Parameter estimation of the chosen model:\nThe coefficients are estimated by methods like Maximum Likelihood Estimators like the Yule-Walker Algorithm.\n\n3) Diagnostic Checking:\nWe check if the residuals are normally distributed as white noise. We make sure there is no correlation in the residuals.\n\n4) Forecasting","c4d0e6c7":"The p-value is clearly $\\leq 0.05 \\textrm{ ,our } \\alpha$. So this series is stationary.","31ca623b":"# Modelling the quarterly EPS for Johnson:\nWe finally wrap up by working with a real world example.\n","49199b86":"## Moving Average Process (MA)\nA model that uses the dependency between an observation and a residual error from a moving average model applied to lagged observations.\nMathematically a MA(q) process is expressed as\n$$ y_t = \\epsilon_t + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\textrm{...} + \\theta_q \\epsilon_{t-q} $$\nwhere $q$ is the order and $\\epsilon_t$ are the noise terms.","9aff2391":"## Auto Regressive Process (AR)\n\nAR uses a linear combination of past values of the target to make forecasts.\nMathematically a AR(p) process is expressed as: \n$$ y_t = c + \\phi_1 y_{t-1} + \\phi_2 y_{t-2} + \\textrm{...} + \\phi_p y_{t-p} + \\epsilon_t $$\nwhere $p$ is the order, $c$ is the constant and $\\epsilon$ is the noise.\n\n\nA time series is said to be AR when present value of the time series can be obtained using previous values of the same time series i.e the present value is weighted average of its past values.","0f9c31fa":"The ACF plot cuts off after 3 lags suggesting a MA(3) process.","bf00e1c6":"# ARIMA Models","cc675177":"# Time Series Analysis\nA time series is a sequential set of data points, measured typically over successive times.\n\nTime series analysis comprises of methods for analyzing time series data in order to extract meaningful statistics and other characteristics of data.\n\n### Categorisation of Time Series Analysis:\n\n| Time Domain | Frequency Domain   |\n|------|------|\n|   Investigates algged relationships (Autocorrelation Analysis)  | Investigation of cycles|\n\n\n\n| Univariate | Multivariate  |\n|------|------|\n|  Records of a single variable  |  Records of more than one variable|\n\n\n\n| Linear | Non Linear  |\n|------|------|\n|  Assumes current value is a linear function of past values  |  Assumes current value is a non-linear function of past values|\n\n\n\n| Discrete  | Continuous  |\n|------|------|\n|  Observations measured at discrete points in time.  |  Observations are measured at every instance of time. |\n\n\n### Components of Time Series Analysis:\n\n#### Trend\nTendency of the time series to increase\/ decrease over time.\n#### Seasonality\nExplains periodical fluctuations within a year.\n#### Cyclical Variation\nDescribes the medium-term changes caused by circumstances, which repeat in cycles. The duration of a cycle extends over longer period of time.\n#### Irregular variation\nCaused by unpredictable influences, which are not regular and also do not repeat in a particular pattern.","bb0edf85":"$\u22121 \u2264 \\rho(s,t) \u2264 1$","798f72a3":"# Simulation of ARMA(2,2) Process\n","163ef248":"These four components can be used in two models:\n\n##### Additive Model:\n$$Y(t)=T(t)+S(t)+C(t)+I(t)$$\nThis assumes all four components are independent of each other.\n##### Multiplicative Model:\n$$Y(t)=T(t)*S(t)*C(t)*I(t)$$\nThis assumes all four components are not necessarily independent of each other.","e894ea9b":"Time series can be defined as a collection of random variables indexed according to the order they are obtained in time, $X_1, X_2, X_3, . . . X_t$ will typically be discrete.\nThe collection of random variables {Xt} is referred to as a stochastic process, while the observed values are referred to as a realization of the stochastic process.\n","7878db0e":"### For an Auto Regressive Process (AR(p)) :\n1) The ACF declines exponentially.\n2) The PACF spikes at lag p followed by a sharp cut-off.\n\n### For an Moving Average Process (MA(q)) :\n1) The ACF spikes at lag q followed by a sharp cut-off.\n2) The PACF declines exponentially.\n\nMixed processes decline on both ACF and PACF. If ACF and PACF decline slowly, it suggests non-stationarity.\n","0a974b51":"We will now simulate the various processes and try to identify them using ACF-PACF plots, and Method of Moments estimators like the Yule-Walker Estimator.","a4cfd4de":"ACF measures the linear predictability of $X_t$ using only $X_s$.","9807b2d3":"Information Criteria represent another solution to obtain the optimal number of lags $(\ud835\udc5d \\textrm{ and } \ud835\udc5e)$ for ARMA models.","75841729":"## Information Criterion - Akaike Information Criterion (AIC) and Schwartz Bayesian Criterion:","b031f3d4":"The parameter for the MA process at lag 2 does not seem to be statistically significant according to the p-value.\nThe final part of modelling a time series is to study the residuals.\nIdeally, the residuals will be white noise, with no autocorrelation.\nA good way to test this is to use the Ljung-Box test.","6fbf3edd":"where $\\hat{X}_{t+h}$ and $\\hat{X}_t$ is defined as: \n\n$$ \\hat{X}_{t+h} = \\beta_1 X_{t+h-1} + \\beta_2 X_{t+h-2} + \\textrm{...} + \\beta_{h-1} X_{t+1} $$\n\nand\n\n$$ \\hat{X}_t = \\beta_1 X_{t+1} + \\beta_2 X_{t+2} + \\textrm{...} +\\beta_{h-1} X_{t+h-1} $$ ","7576a33d":"## ARIMA\nARIMA stands for AutoRegressive Integrated Moving Average.\nThis model is the combination of autoregression, a moving average model and differencing.\nDifferencing is useful to remove the trend in a time series and make it stationary.\nIt simply involves subtracting a point a $t-1$ from time $t$.\nA standard notation is used of ARIMA(p, d, q) where the parameters are substituted with integer values to quickly\nindicate the specific ARIMA model being used.\n1) p The number of lag observations included in the model, also called the lag order.\n2) d The number of times that the raw observations are differenced, also called the degree of differencing.\n3) q The size of the moving average window, also called the order of moving average.\n\nA value of 0 can be used for a parameter, which indicates to not use that element of the model.\nIt can be mathematically expressed as:\n$$ y'_t = c + \\phi_1 y'_{t-1} + \\phi_2 y'_{t-2} + \\textrm{...} + \\phi_p y'_{t-p} + \\theta_1 \\epsilon_{t-1} + \\theta_2 \\epsilon_{t-2} + \\textrm{...} + \\theta_q \\epsilon_{t-q} + \\epsilon_t $$\n","55b23d0b":"## Simulation of AR(3) Process:\n\nWe simulate the AR(3) Process : $$y_t = 3+ 0.3 y_{t-1} + 0.1 y_{t-2} + 0.05 y_{t-3}$$","ae9f63c3":"## Partial Auto Correlation Functions (PACF)\nInstead of finding correlations of present with lags like ACF, it finds correlation of the residuals (which remains after removing the effects which are already explained by the earlier lag(s)) with the next lag value.\nFor a stationary process $X_t$ , the PACF (denoted as $\\phi_{hh}$), for $h = 1, 2, \\textrm{...}$ is defined as\n\n$$ \\phi_{11} = corr(X_{t+1}, X_t ) = \\rho_1$$\n\nand \n\n$$ \\phi_{hh} = corr(X_{t+h} - \\hat{X}_{t+h}, X_t - \\hat{X}_t), h \\geq 2 $$ "}}