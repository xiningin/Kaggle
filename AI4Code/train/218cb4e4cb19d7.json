{"cell_type":{"f0caa4ba":"code","34c36c3f":"code","9cff1474":"code","02521c37":"code","d5c68861":"code","c896c12a":"code","a9f09550":"code","bca7c632":"code","78af4e2f":"code","5f4382d0":"code","41b266ac":"code","84657001":"code","93194350":"code","553874d9":"markdown"},"source":{"f0caa4ba":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.linear_model import TweedieRegressor\nfrom sklearn.metrics import mean_squared_error","34c36c3f":"# Loading data \nX_train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")","9cff1474":"# Preparing data as a tabular matrix\ny_train = X_train.target\nX_train = X_train.set_index('id').drop('target', axis='columns')\nX_test = X_test.set_index('id')","02521c37":"# Pointing out categorical features\ncategoricals = [item for item in X_train.columns if 'cat' in item]","d5c68861":"# Dealing with categorical data using get_dummies\ndummies = pd.get_dummies(X_train.append(X_test)[categoricals])\nX_train[dummies.columns] = dummies.iloc[:len(X_train), :]\nX_test[dummies.columns] = dummies.iloc[len(X_train): , :]\ndel(dummies)","c896c12a":"# Dealing with categorical data using OrdinalEncoder (only when there are 3 or more levels)\nordinal_encoder = OrdinalEncoder()\nX_train[categoricals[3:]] = ordinal_encoder.fit_transform(X_train[categoricals[3:]]).astype(int)\nX_test[categoricals[3:]] = ordinal_encoder.transform(X_test[categoricals[3:]]).astype(int)\nX_train = X_train.drop(categoricals[:3], axis=\"columns\")\nX_test = X_test.drop(categoricals[:3], axis=\"columns\")","a9f09550":"# Feature selection (https:\/\/www.kaggle.com\/lucamassaron\/tutorial-feature-selection-with-boruta-shap)\nimportant_features = ['cat1_A', 'cat1_B', 'cat5', 'cat8', 'cat8_C', 'cat8_E', 'cont0', \n                      'cont1', 'cont10', 'cont11', 'cont12', 'cont13', 'cont2', 'cont3', \n                      'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9']\n\ncategoricals = ['cat5', 'cat8']\n\nX_train = X_train[important_features]\nX_test = X_test[important_features]","bca7c632":"# Stratifying the data\nkm = KMeans(n_clusters=32, random_state=0)\npca = PCA(n_components=16, random_state=0)\n\npca.fit(X_train)\nkm.fit(pca.transform(X_train))\n\nprint(np.unique(km.labels_, return_counts=True))\n\ny_stratified = km.labels_","78af4e2f":"# Clustering continuos values (acting as non linearity correctors)\nn_clusters = 16\nkm = KMeans(n_clusters=n_clusters, random_state=0)\nfor i in range(14):\n    feats = [f'cont{i}_{j}'for j in range(n_clusters)]\n    km.fit(X_train[[f'cont{i}']])\n    X_train[feats] = pd.get_dummies(km.predict(X_train[[f'cont{i}']])).values\n    X_test[feats] = pd.get_dummies(km.predict(X_test[[f'cont{i}']])).values","5f4382d0":"interactions =(\n[('cont2_11', 'cont7_9'), ('cont2_11', 'cont7_13'), ('cont2_6', 'cont4_5'), ('cat8', 'cont8_1'),\n ('cont5_6', 'cont8_0'), ('cont5_6', 'cont7_0'), ('cont7_0', 'cont8_0'), ('cat8_C', 'cont4_5'), \n ('cont4_5', 'cont6_5'), ('cont4_5', 'cont7'), ('cont0_13', 'cont8_15'), ('cont5', 'cont5_6'),\n ('cont7_13', 'cont7_9'), ('cont4_3', 'cont7_0'), ('cont5_14', 'cont5_6'), ('cont11', 'cont5_14'),\n ('cont2_11', 'cont2_12'), ('cont2_11', 'cont5_3'), ('cont1_1', 'cont7_6'), ('cont7_9', 'cont8_6'),\n ('cont2_12', 'cont7_13'), ('cont4_3', 'cont5_6'), ('cont2_6', 'cont6_5'), ('cont2_6', 'cont5_5'),\n ('cat8_C', 'cont2_6'), ('cat8', 'cont7_9'), ('cont4_9', 'cont8_15'), ('cont4_14', 'cont7_9'),\n ('cont1_15', 'cont5_11'), ('cont4_3', 'cont7_4'), ('cont7_9', 'cont8_1'), ('cont4_3', 'cont8_0'),\n ('cont4_3', 'cont5'), ('cont2_12', 'cont7_9'), ('cont5_6', 'cont6_4'), ('cont1_1', 'cont1_13'), \n ('cont4_5', 'cont7_9'), ('cont4_12', 'cont8_15'), ('cont5_3', 'cont7_9'), ('cont4_5', 'cont5_5'),\n ('cont0_3', 'cont4_5'), ('cont5_6', 'cont6_0'), ('cat8', 'cont8_2'), ('cont5_5', 'cont6_5'),\n ('cont4_3', 'cont6_0'), ('cat8', 'cont2_4'), ('cont1_1', 'cont4_5'), ('cont5', 'cont8_0'),\n ('cont0_1', 'cont4_0'), ('cont13', 'cont8_0')]\n)\n\nfor a, b in interactions:\n    X_train[f\"{a}*{b}\"] = np.prod(X_train[[a, b]].values, axis=1)\n    X_test[f\"{a}*{b}\"] = np.prod(X_test[[a, b]].values, axis=1)","41b266ac":"# Cross-validation prediction\nfolds = 10\nskf = StratifiedKFold(n_splits=folds,\n                      shuffle=True, \n                      random_state=0)\n\nfold_idxs = list(skf.split(X_train, y_stratified))\n\npredictions = np.zeros(len(X_test))\nscore = list()\n\nfor k, (train_idx, val_idx) in enumerate(fold_idxs):\n    \n    ss = StandardScaler()\n    \n    y = y_train[train_idx]\n    X = ss.fit_transform(X_train.iloc[train_idx, :]).astype(np.float32)\n    Xv = ss.transform(X_train.iloc[val_idx, :]).astype(np.float32)\n    Xt = ss.transform(X_test).astype(np.float32)\n    \n    glm = TweedieRegressor(power=1.75, alpha=0.0001, max_iter=10000)\n    glm.fit(X, y)\n    \n    val_preds = glm.predict(Xv)\n    val_rmse = mean_squared_error(y_true=y_train[val_idx], y_pred=val_preds, squared=False)\n    print(f\"Fold {k} RMSE: {val_rmse:0.5f}\")\n    predictions += glm.predict(Xt).ravel()\n    score.append(val_rmse)\n    \npredictions \/= folds\nprint(f\"CV RMSE {np.mean(score):0.5f} ({np.std(score):0.5f})\")","84657001":"# Preparing the submission\nsubmission = pd.DataFrame({'id':X_test.index, \n                           'target': predictions})\n\nsubmission.to_csv(\"submission.csv\", index = False)","93194350":"submission","553874d9":"When ensembling using blending it is important to blend models of similar performance but the less possible correlated between them.\n\nInstead, when stacking, it is just important to stack models that are different from each other. It will be the stacking algorithm to decide if even an under-perforimng model can help in any way your ensemble.\n\nIn this kernel I propose to you the tweedie regression, a regression based on tweedie distribution which is suitable for modelling claims in insurance (our artificial data has been taken from an insurance dataset).\n\nThe model uses some \"tricks\" to perform better:\n\n* binarization of continuous variables based on k-means cluster analysis\n* some selected interactions\n\nactually, if you are using this model in a stacking enseble, you don't need to add any of these engeneered features.\n\nMake your stacking richer and more varied!\n\nHappy Kaggling!\n"}}