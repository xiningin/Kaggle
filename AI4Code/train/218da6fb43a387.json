{"cell_type":{"dbbe26f8":"code","3f700428":"code","a3a496a0":"code","f50d0ad0":"code","8cc69083":"code","baf03819":"code","70ebe69b":"code","30069bbf":"markdown"},"source":{"dbbe26f8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\npd.set_option('display.max_columns', 500)\nimport os\nimport warnings\nfrom functools import partial\nwarnings.filterwarnings(\"ignore\")\n\nimport janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set\n\nimport pandas as pd\nfrom sklearn import preprocessing\nimport gc\nimport catboost as cat\nimport numpy as np","3f700428":"%time\ntrain = pd.read_pickle('\/kaggle\/input\/janestreet\/jane_street_train.pkl.gzip')\nprint (\"Data is loaded!\")\n\nprint('train shape is {}'.format(train.shape))\n\ntrain = train[train['weight'] != 0]\ntrain['action'] = (train['resp'] > 0).astype('int')\n\nX_train = train.loc[:, train.columns.str.contains('feature')]\ny_train = train.loc[:, 'action']\ngroups = train['date'].values\nweightes = train['weight'].values\nresps = train['resp'].values\n\ndel train\ngc.collect()","a3a496a0":"def group_time_split(groups: np.ndarray, splits, X):\n    group_list = np.unique(groups)\n    n_samples = len(X)\n    n_groups = len(group_list)\n    indices = np.arange(n_samples)\n    test_size = n_groups \/\/ splits\n    test_starts = range(test_size + n_groups % splits, n_groups, test_size)\n    test_starts = list(test_starts)\n    for test_start in test_starts:\n        yield (\n            indices[np.isin(groups, group_list[:test_start])],\n            indices[np.isin(groups, group_list[test_start:])],\n        )\n\n\ndef utility_score_last(date, weight, resp, action):\n    count_i = date[-1] + 1\n    Pi = np.bincount(date, weight * resp * action)\n    t = np.sum(Pi) \/ np.sqrt(np.sum(Pi ** 2)) * np.sqrt(250 \/ count_i)\n    u = np.clip(t, 0, 6) * np.sum(Pi)\n    return u\n\n\ndef get_opt_threshold(pred, y_true, groups, weights, bins=50):\n    p_min, p_max = pred.min(), pred.max()\n    opt_u = 0\n    opt_th = p_min\n    for th in np.linspace(p_min, p_max, endpoint=False, num=bins):\n        act = np.where(pred > th, 1, 0)\n        u = utility_score_last(groups, weights, y_true, act)\n        if u > opt_u:\n            opt_u = u\n            print(u)\n            opt_th = th\n    return opt_th\n\n\ndef optimize(params, X_train: pd.DataFrame, y_train:pd.DataFrame, groups: np.ndarray, splits=5):\n\n    print(params)\n    p = {'learning_rate': params['learning_rate'],\n         'max_depth': params['max_depth'],\n         'reg_alpha': params['reg_alpha'],\n         'subsample': params['subsample'],\n         'colsample_bytree': params['colsample_bytree'],\n         'boosting_type': 'gbdt',\n         'random_state': 2020,\n         'subsample_freq': 1,\n         'num_leaves': 2 ** params['max_depth'] - 1,\n         'metric': 'None'\n         }\n\n    weighted_auc = 0\n    div = splits * (splits - 1) \/ 2\n\n    for i, (tr_idx, val_idx) in enumerate(group_time_split(groups, splits, X_train)):\n        tr_X = X_train.iloc[tr_idx]\n        tr_y = y_train.iloc[tr_idx]\n        val_X = X_train.iloc[val_idx]\n        val_y = y_train.iloc[val_idx]\n\n        clf = lgb.LGBMClassifier(**p)\n        clf.fit(tr_X, tr_y, eval_set=[(val_X, val_y)], eval_metric='auc',\n                early_stopping_rounds=20, verbose=False)\n\n        val_pred = clf.predict_proba(val_X)[:, 1]\n\n        weighted_auc += (i + 1) * roc_auc_score(val_y, val_pred) \/ div\n\n        gc.collect()\n\n    return - weighted_auc","f50d0ad0":"splits = 4\n\nbase_params = {'colsample_bylevel': 0.601104234749781, 'l2_leaf_reg': 6.201131333667228, \n               'learning_rate': 0.033455321575393125, 'max_depth': 10.0, 'subsample': 0.8051610581570489,\n              'random_state': 2020, 'eval_metric': 'AUC'}\n\n\n# avg_best_iterations = 0\nmodels = []\nopt_thresholds = []\n\n\nfor i, (tr_idx, val_idx) in enumerate(group_time_split(groups, splits, X_train)):\n    print(\"#\"*50)\n    print(\"fold:\", i)\n    if i < 2:\n        continue\n    tr_X = X_train.iloc[tr_idx]\n    tr_y = y_train.iloc[tr_idx]\n#     tr_group = groups[tr_idx]\n#     tr_weight = weightes[tr_idx]\n#     tr_resp = resps[tr_idx]\n    \n    val_X = X_train.iloc[val_idx]\n    val_y = y_train.iloc[val_idx]\n    val_group = groups[val_idx]\n    val_weight = weightes[val_idx]\n    val_resp = resps[val_idx]\n    \n    Tr = cat.Pool(tr_X, tr_y)\n    val = cat.Pool(val_X, val_y)\n    \n    clf = cat.CatBoostClassifier(**base_params)\n    clf.fit(Tr, eval_set=val,\n            early_stopping_rounds=20, verbose=False)\n\n    val_pred = clf.predict_proba(val_X)[:, 1]\n    \n    models.append(clf)\n    val_pred = clf.predict_proba(val_X)[:, 1]\n    \n    opt_th = get_opt_threshold(val_pred, val_resp, val_group, val_weight)\n    opt_thresholds.append(opt_th)\n    \n    gc.collect()\n\ndel tr_X, tr_y, val_X, val_y, val_pred, val_group, val_weight, val_resp\ngc.collect()","8cc69083":"base_params['n_estimators'] = models[-1].best_iteration_ + 5\nclf = cat.CatBoostClassifier(**base_params)\n\nclf.fit(X_train, y_train)\n    \ndel X_train, y_train\ngc.collect()","baf03819":"# weighted_opt = (np.arange(1,5) * opt_thresholds).sum() \/ 10\n# weighted_opt = np.min(opt_thresholds)\nweighted_opt = 0.5\nmodel = models[-1]  # only consider the last two because of the limited time","70ebe69b":"for (test_df, sample_prediction_df) in iter_test:\n    if test_df['weight'].item() > 0:\n        X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n\n        y_preds = (2 * clf.predict_proba(X_test)[:, 1] + model.predict_proba(X_test)[:, 1]) \/ 3\n        y_preds = np.where(y_preds>weighted_opt, 1, 0)\n        sample_prediction_df.action = y_preds\n    else:\n        sample_prediction_df.action = 0\n    env.predict(sample_prediction_df)","30069bbf":"several seconds to load by saving original dataset to pickle file"}}