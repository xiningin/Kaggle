{"cell_type":{"b4b8e413":"code","812a3378":"code","155303c9":"code","8dad6cab":"code","00d8d327":"code","1f9d4d44":"code","8d2f1b0d":"code","9b60cb02":"code","db94beea":"code","561a5dc7":"code","b54b4341":"code","dee8922a":"code","44aaab63":"code","d9f16829":"code","33dc2e43":"code","4665db94":"code","8821c923":"code","9d51de3a":"code","ad80189f":"code","cff01d9a":"code","02f77a53":"code","2d833714":"code","577e005b":"code","b575a510":"code","9d84b500":"code","a512eafe":"code","72156f21":"code","1acc460a":"code","7dc4e1a3":"code","f43faf0b":"code","5e2eeb97":"code","d471a99d":"code","27ca6304":"code","578aad19":"code","64dd3e37":"code","4e8ac692":"code","c9b9fe1f":"code","45875e70":"code","569c82c4":"code","4cb9f0f9":"code","e50cc8c9":"code","7257d219":"code","25863852":"code","e88dea46":"code","1a518725":"code","0883eacb":"markdown","18049f09":"markdown","e93d77f0":"markdown","bcd05ed0":"markdown","cb4a9218":"markdown","51620568":"markdown","92163630":"markdown","fbf6e438":"markdown","1dd2f6ff":"markdown","3dfa65ae":"markdown","1c313b9d":"markdown","cb9d8d73":"markdown","420fe03e":"markdown","ce569bfc":"markdown","423b1f0c":"markdown","7d008d2c":"markdown","87d6e240":"markdown","851cbe28":"markdown","5f352cb9":"markdown","44fac5a6":"markdown","b7cda9d7":"markdown","b83602f7":"markdown","5b1eddbd":"markdown","7b1a9b62":"markdown","f88323b5":"markdown","122dfd4e":"markdown","f6306903":"markdown","141c3375":"markdown","066a878a":"markdown","05c32293":"markdown","54869def":"markdown","446f1b90":"markdown","c0bfeff3":"markdown","2503b20a":"markdown","21982a28":"markdown","76095104":"markdown"},"source":{"b4b8e413":"import numpy as np\n\n# Set the random seed for reproducability\nnp.random.seed(42)","812a3378":"import pandas as pd","155303c9":"# Reads in the csv-files and creates a dataframe using pandas\n\n# base_set = pd.read_csv('data\/housing_data.csv')\n# benchmark = pd.read_csv('data\/housing_test_data.csv')\n# sampleSubmission = pd.read_csv('data\/sample_submission.csv')","8dad6cab":"base_set = pd.read_csv('..\/input\/dat158-2019\/housing_data.csv')\nbenchmark = pd.read_csv('..\/input\/dat158-2019\/housing_test_data.csv')\nsample_submission = pd.read_csv('..\/input\/dat158-2019\/sample_submission.csv')","00d8d327":"%matplotlib inline\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","1f9d4d44":"base_set.head()","8d2f1b0d":"benchmark.head()","9b60cb02":"base_set.info()","db94beea":"benchmark.info()","561a5dc7":"base_set.describe()","b54b4341":"correlations = base_set.corr()\ncorrelations[\"median_house_value\"]","dee8922a":"base_set.hist(bins=50, figsize=(15,15))\nplt.show()","44aaab63":"base_set.plot(kind=\"scatter\", \n           x=\"longitude\", \n           y=\"latitude\", \n           alpha=0.4,\n           s=base_set[\"population\"]\/100, \n           label=\"population\",\n           c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n           figsize=(15,7))\nplt.legend()","d9f16829":"from pandas.plotting import scatter_matrix\nattributes = ['median_house_value', 'median_income',\n             'total_rooms', 'housing_median_age']\nscatter_matrix(base_set[attributes], figsize=(12,8))","33dc2e43":"# There are null values in total_bedrooms, we fill those with the median\ndef fill_null(dataset, column):\n    values = {column: dataset[column].median()}\n    \n    return dataset.fillna(values)\n\n# For these particular sets, there are only null values in the 'total_bedrooms' column.\nbase_set = fill_null(base_set, 'total_bedrooms')\nbenchmark = fill_null(benchmark, 'total_bedrooms')","4665db94":"benchmark = benchmark.drop(columns=['Id'])","8821c923":"base_set.isnull().any()","9d51de3a":"benchmark.isnull().any()","ad80189f":"labels_column = 'median_house_value'\n\nX = base_set.drop(columns=[labels_column])\nY = pd.DataFrame(base_set[labels_column], columns=[labels_column])","cff01d9a":"X.head()","02f77a53":"Y.head()","2d833714":"def derive_datapoints(dataset):\n    dataset['bedrooms_per_room'] = dataset['total_bedrooms'] \/ dataset['total_rooms']\n\n    dataset['rooms_per_household'] = dataset['total_rooms'] \/ dataset['households']\n    dataset['bedrooms_per_household'] = dataset['total_bedrooms'] \/ dataset['households']\n    dataset['population_per_household'] = dataset['population'] \/ dataset['households']\n    \n    return dataset\n\nX = derive_datapoints(X)\nbenchmark = derive_datapoints(benchmark)\n\n# One-hot encoding\nX = pd.get_dummies(X)\nbenchmark = pd.get_dummies(benchmark)","577e005b":"# Some housekeeping, we need to ensure the test set has the same columns as the training set\n# The missing columns will be the onehot-encoded values\nmissing_columns = set( X.columns ) - set( benchmark.columns )\n\n# We fill the values in the missing columns with 0, as they are one-hot encoded values that don't exist in the set\nfor column in missing_columns:\n    benchmark[column] = 0\n\n# Ensure the order of column in the test set is in the same order than in train set\nbenchmark = benchmark[X.columns]","b575a510":"X.head()","9d84b500":"benchmark.head()","a512eafe":"from sklearn.model_selection import train_test_split\n\ntrain_to_valtest_ratio = .2\nvalidate_to_test_ratio = .5\n\n# First split our main set\n(X_train,\n X_validation_and_test,\n Y_train,\n Y_validation_and_test) = train_test_split(X, Y, test_size=train_to_valtest_ratio)\n\n# Then split our second set into validation and test\n(X_validation,\n X_test,\n Y_validation,\n Y_test) = train_test_split(X_validation_and_test, Y_validation_and_test, test_size=validate_to_test_ratio)","72156f21":"from sklearn.preprocessing import MinMaxScaler, StandardScaler\n\nX_scaler = StandardScaler().fit(X_train)\ndef scale_dataset_X(dataset):\n    return X_scaler.transform(dataset)\n\nX_train_scaled = scale_dataset_X(X_train)\nX_validation_scaled = scale_dataset_X(X_validation)\nX_test_scaled = scale_dataset_X(X_test)","1acc460a":"from keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential([\n    Dense(15, activation='relu', input_dim=X_train.shape[1]),\n    Dense(15, activation='relu'),\n    Dense(60, activation='relu'),\n    Dense(120, activation='relu'),\n    Dense(60, activation='relu'),\n    Dense(15, activation='relu'),\n    Dense(1),\n])\n\nmodel.summary()","7dc4e1a3":"import keras.backend as K\n\ndef rmse(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true)))\n\nmodel.compile(optimizer='adadelta', # adam, sgd, adadelta\n              loss=rmse,\n              metrics=[rmse, 'mae'])","f43faf0b":"from keras.callbacks import EarlyStopping\n\nearly_stopper = EarlyStopping(patience=3)\n\ntraining_result = model.fit(X_train_scaled, Y_train,\n                            batch_size=32,\n                            epochs=250,\n                            validation_data=(X_validation_scaled, Y_validation),\n                            callbacks=[early_stopper])","5e2eeb97":"# Plot model accuracy over epoch\nplt.plot(training_result.history['mean_absolute_error'])\nplt.plot(training_result.history['val_mean_absolute_error'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n# Plot model loss over epoch\nplt.plot(training_result.history['loss'])\nplt.plot(training_result.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","d471a99d":"validate_result = model.test_on_batch(X_validation_scaled, Y_validation)\nvalidate_result","27ca6304":"test_result = model.test_on_batch(X_test_scaled, Y_test)\ntest_result","578aad19":"from sklearn.ensemble import RandomForestRegressor\n\nrfr_model = RandomForestRegressor()\nrfr_model.fit(X_train, Y_train)\n\n# Get the mean absolute error on the validation data\nrfr_predictions = rfr_model.predict(X_test)\n\nrfr_error =  np.sqrt(np.mean((rfr_predictions - Y_test['median_house_value']) ** 2))\nrfr_error","64dd3e37":"import re\n\nregex = re.compile(r\"[|]|<\", re.IGNORECASE)\n\n# XGBoost does not support some of the column names\n\nX_train.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train.columns.values]\nX_test.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_test.columns.values]\n\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport scipy.stats as st\n\none_to_left = st.beta(10, 1)  \nfrom_zero_positive = st.expon(0, 50)\n\nxgb_reg = XGBRegressor(nthreads=-1)\n\nxgb_gs_params = {  \n    \"n_estimators\": st.randint(3, 40),\n    \"max_depth\": st.randint(3, 40),\n    \"learning_rate\": st.uniform(0.05, 0.4),\n    \"colsample_bytree\": one_to_left,\n    \"subsample\": one_to_left,\n    \"gamma\": st.uniform(0, 10),\n    'reg_alpha': from_zero_positive,\n    \"min_child_weight\": from_zero_positive,\n}\n\nxgb_gs = RandomizedSearchCV(xgb_reg, xgb_gs_params, n_jobs=1)  \nxgb_gs.fit(X_train.as_matrix(), Y_train)  \n\nxgb_model = xgb_gs.best_estimator_ \n\nxgb_predictions = xgb_model.predict(X_test.as_matrix())\n\nxgb_error =  np.sqrt(np.mean((xgb_predictions - Y_test['median_house_value']) ** 2))\nxgb_error","4e8ac692":"print(f'NN RMSE:                            {test_result[0]}')\nprint(f'RandomForestRegressor RMSE:         {rfr_error}')\nprint(f'XGBRegressor RMSE:                  {xgb_error}')","c9b9fe1f":"# Scale test data\nbenchmark_scaled = scale_dataset_X(benchmark)","45875e70":"benchmark.head()","569c82c4":"X.head()","4cb9f0f9":"median_house_value = model.predict(benchmark_scaled)","e50cc8c9":"len(median_house_value)","7257d219":"median_house_value","25863852":"submission = pd.DataFrame({\n    'Id': [i for i in range(len(median_house_value))],\n    'median_house_value': median_house_value.flatten()\n})","e88dea46":"submission.head()","1a518725":"# Stores a csv file to submit to the kaggle competition\nsubmission.to_csv('submission.csv', index=False)","0883eacb":"## Data preparation\n\nIn this section we will preprocess the data and construct a model, which we will then train so we are able to make predictions with it. Lastly we will test in on a test set we create.","18049f09":"### Feature engineering\n\nHere we derive useful datapoints from existing ones. We also use one-hot encoding to transform string columns into separate columns containing numbers.","e93d77f0":"For this particular dataset, we also need to do a couple extra things to the benchmark set.","bcd05ed0":"A comparison of all the models","cb4a9218":"To make the scales of the numbers appropriate for the neural network, we should to do some scaling.","51620568":"## Understanding the data\n\nNow that we have our data, we need to investigate it so that we are able to leverage it to the fullest extent.","92163630":"Testing with RandomForestRegressor","fbf6e438":"## Table of Contents\n\n1. [Loading the Data](#loading-the-data)\n    * [Setup](#setup)\n    * [Data](#data)\n2. [Understanding the Data](#understanding-the-data)\n    * [Conclusions](#conclusions)\n3. [Data Preparation](#data-preparation)\n    * [Cleaning up bad values](#cleaning-up-bad-values)\n    * [Separating values and labels](#separating-values-and-labels)\n    * [Feature engineering](#feature-engineering)\n    * [Splitting up the dataset](#splitting-up-the-dataset)\n    * [Normalization](#normalization)\n4. [Machine Learning](#data-preparation-and-machine-learning)\n    * [Setting up the model](#setting-up-the-model)\n    * [Training the model](#training-the-model)\n    * [Testing the model](#testing-the-model)\n4. [Making a Benchmark Submission](#making-a-benchmark-submission)","1dd2f6ff":"Importing Numpy now so that it is ready for later.","3dfa65ae":"### Splitting up the dataset\n\nWe split our base set into separate datasets for training, testing and validation.","1c313b9d":"# Predicting Housing Prices in California","cb9d8d73":"### Testing the model\n\nFinally, we churn the test set through the model we created.","420fe03e":"#### Kaggle-specific way of accessing the data\n\nOn Kaggle the data is stored in the folder `..\/input\/dat158-2019\/`.","ce569bfc":"Go to Kaggle competition website and download the data. Make a new folder in your DAT158ML repository called `data`. Store the Kaggle competition data in this folder.","423b1f0c":"**Note from competition**\n\nHere's a simple getting started notebook that shows you how to load the data, and how to create a Kaggle submission file. Remember that you should structure your notebook after the 8 step guide, as detailed in the [Assignment 1 instructions](https:\/\/hvl.instructure.com\/courses\/9086\/assignments\/17277). ","7d008d2c":"We will use Matplotlib to plot various things throughout the notebook.","87d6e240":"### Normalization","851cbe28":"Testing with XGBoost","5f352cb9":"We will use Pandas throughout the notebook to hold and manage our datasets.","44fac5a6":"Scatter plot showing the distribution of housing value across California, from low (blue) to high (red).","b7cda9d7":"### Set up the model\n\nNow, it is time to set up the architecture.","b83602f7":"**Note from competition**\n\nAfter you have trained your model and have found predictions on your test data, you must create a csv-file that contains 'Id' and your predictions in two coloums.\n\nWe have assumed that you have called your predictions 'median_house_value' after you have trained your model.\n\nThis is just for demonstrational purposes, that is why all our predictions is zero. Yours will be filled with numbers.","5b1eddbd":"Finally, we check to see that neither of the datasets contain `NaN` values.","7b1a9b62":"### Trying other models","f88323b5":"## Loading the Data <a class=\"anchor\" id=\"loading-the-data\"><\/a>\n\nBefore we do anything, we need to make sure we have all our data ready.","122dfd4e":"### Machine Learning","f6306903":"## Making a Benchmark Submission\n\nFor the benchmark data, it is important that we put it through the same preparation steps as the training set.","141c3375":"### Conclusions\n\n...","066a878a":"Then you should uncomment the code and run the following two cells. **Warning:** This doesn't work in this Kaggle hosted notebook! See below","05c32293":"**Note from competition**\n\nThis part you should code and figure out yourself. Play around with different ways to prepare the data, different machine learning models and settings of hyperparameters\n\nRemember to create your own validation set to evaluate your models. Your test set will not contain labels and are therefore not suited for evaluating and tuning your different models.","54869def":"### Training the model\n\nLet's fit the model on the data.","446f1b90":"### Separating values and labels\n\nIt is time to split the dataset into values and labels. To do that, we drop the label column and call that `X`, and take the label column alone and call that `Y`. Afterwards we are ready to start shaping our dataset.","c0bfeff3":"Now, let's look into how the fitting went.","2503b20a":"Looking at the correlations between the values, we can see that the median income has the strongest correlation to the median house value.","21982a28":"### Cleaning up bad values\n\nIn this section we handle empty values in the dataset. There are `null` values in both the main dataset and the benchmark set, which we have to clean up. Two of the most common possible strategies for this are as follows:\n\n1. Remove rows with `NaN` values\n2. Fill `NaN` values with median or mean of all the other values\n\nHere, we go for the latter, because we want to use all the data as best as we can. Median is good for values that vary a lot, so we go for that here.","76095104":"The `Id` column in the benchmark set is not needed, so we remove that."}}