{"cell_type":{"d40c430f":"code","496e1434":"code","f62846f9":"code","eaab6c49":"code","d09a4ce3":"code","fa28106f":"code","6db19dc0":"code","fa37ee27":"code","f4db11ff":"code","a9ffc7cd":"code","7b43d5d0":"code","be02950e":"code","ae23521b":"code","42f6b78a":"code","f9630d13":"code","f391bdf8":"code","58e34f3d":"code","3b2df300":"code","0f2846e2":"code","7c1e19bd":"code","ee7adb1e":"code","83803a71":"code","93397d89":"code","c974ea39":"code","d1441e97":"code","9561afb3":"code","9e881b45":"code","759b2e9c":"code","c22d1d4d":"code","28162667":"code","c81224eb":"code","2db0293d":"code","513c90fb":"code","c73eded9":"code","b399759a":"code","3a703eff":"code","efb77575":"code","04837fb7":"markdown","9c172596":"markdown","b73f0676":"markdown","d73cdf1a":"markdown","164c985b":"markdown","bdcba6e7":"markdown","578ed064":"markdown","25b60ccd":"markdown","7495875a":"markdown","b4e6e1ed":"markdown","c5f56bc2":"markdown","cc4ed9ad":"markdown","43c9018b":"markdown","7e261ece":"markdown","e7d39f80":"markdown","5dfaffef":"markdown","3313d5b4":"markdown","6d14120f":"markdown","45e2033a":"markdown","cfde015d":"markdown","8d44e112":"markdown","5d249931":"markdown","138ef744":"markdown"},"source":{"d40c430f":"# Import Data Libraries\nimport pandas as pd\nimport numpy as np\n\n# Import Visualization Libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","496e1434":"# Read in the datasets\ntrain = pd.read_csv(\"..\/input\/iowa-house-prices\/train.csv\")\ntest = pd.read_csv(\"..\/input\/iowa-house-prices\/test.csv\")\n\n# Drop ID columns\ntrain.drop(columns=\"Id\", axis=1, inplace=True)\ntest.drop(columns=\"Id\", axis=1, inplace=True)","f62846f9":"# Explore the format\nprint(\"Train shape: {}\".format(train.shape))\nprint(\"Test shape: {}\".format(test.shape))\n\n# Explore the head of the dataframe\ntrain.head()","eaab6c49":"test.head()","d09a4ce3":"# # Other helpful commands\n# train.dtypes          # check the datatypes of the columns\n# train.columns         # check name of columns\n# train.shape           # check the shape of the columns\n# train.isna().sum()    # check how much missing data in each column","fa28106f":"train.columns","6db19dc0":"# Leaked columns\nleaked_columns = ['MoSold', 'YrSold', 'SaleType', 'SaleCondition']\n\n# Remove in BOTH datasets\ntrain.drop(labels = leaked_columns, axis = 1, inplace = True)\ntest.drop(labels = leaked_columns, axis = 1, inplace= True)","fa37ee27":"# # Check if the columns have dissapeared\n# train.columns","f4db11ff":"# Check the data types of the columns\ntrain.dtypes","a9ffc7cd":"# Check for missing values in the target column\ntrain[\"SalePrice\"].isna().sum()","7b43d5d0":"# Plot of the target column\nplt.figure(figsize = (16, 4))\nsns.distplot(a = train[\"SalePrice\"], color = \"#FF7F50\")\nplt.title(\"Distribution of Sales Price\", fontsize=16);","be02950e":"# Storing the target variable separately\ny = train[\"SalePrice\"]\ntrain.drop(columns=[\"SalePrice\"], axis=1, inplace=True)","ae23521b":"# Select ONLY numerical columns\nnumerical_cols = [col for col in train.columns if \n                  train[col].dtype in [\"int64\", \"float64\"]]","42f6b78a":"# Find the columns that have null values\nna_count_n = train[numerical_cols].isna().sum()   # this becomes a Pandas Series\nna_count_n[na_count_n > 0]                          # filter","f9630d13":"# Select columns with NAs\nna_numeric_columns = na_count_n[na_count_n > 0].index\nna_numeric_data = train[na_numeric_columns].dropna(axis=0)\n\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n# Plot the distribution of these variables\n# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nplt.figure(figsize = (16, 4))\nsns.distplot(a = na_numeric_data[na_numeric_columns[0]], color = \"blue\")\nplt.title(f\"{na_numeric_columns[0]}\", fontsize=16);","f391bdf8":"plt.figure(figsize = (16, 4))\nsns.distplot(a = na_numeric_data[na_numeric_columns[1]], color = \"purple\")\nplt.title(f\"{na_numeric_columns[1]}\", fontsize=16);","58e34f3d":"plt.figure(figsize = (16, 4))\nsns.distplot(a = na_numeric_data[na_numeric_columns[2]], color = \"orange\")\nplt.title(f\"{na_numeric_columns[2]}\", fontsize=16);","3b2df300":"# Import Simple Imputer and from SkLearn\nfrom sklearn.impute import SimpleImputer","0f2846e2":"# Prepare the Imputation Objects\nmedian_impute = SimpleImputer(strategy = 'median')\nmode_impute = SimpleImputer(strategy = 'most_frequent')\nmean_impute = SimpleImputer(strategy = 'mean')","7c1e19bd":"def apply_imputation(impute_object, column):\n    '''Function that applies the imputation to the desired column.\n    Returns the values for train and test.'''\n\n    ### attention at the difference between fit_transform and transform!\n    imputed_train = impute_object.fit_transform(X = train[[column]])\n    imputed_test = impute_object.transform(X = test[[column]])\n    \n    return imputed_train, imputed_test","ee7adb1e":"# Make the Imputation\ntrain['LotFrontage'] = apply_imputation(median_impute, 'LotFrontage')[0]\ntest['LotFrontage'] = apply_imputation(median_impute, 'LotFrontage')[1]\n\ntrain['MasVnrArea'] = apply_imputation(mode_impute, 'MasVnrArea')[0]\ntest['MasVnrArea'] = apply_imputation(mode_impute, 'MasVnrArea')[1]\n\ntrain['GarageYrBlt'] = apply_imputation(mean_impute, 'GarageYrBlt')[0]\ntest['GarageYrBlt'] = apply_imputation(mean_impute, 'GarageYrBlt')[1]","83803a71":"# # Import the Standard Scaler\n# from sklearn.preprocessing import StandardScaler\n\n# # Scale the data\n# scaler = StandardScaler()\n# scaled_matrix_train = pd.DataFrame(scaler.fit_transform(train[numerical_cols]),\n#                                    columns=numerical_cols)\n# scaled_matrix_test = pd.DataFrame(scaler.transform(test[numerical_cols]),\n#                                    columns=numerical_cols)\n\n# # Erase old data and append scaled one\n# train.drop(columns=numerical_cols, axis=1, inplace=True)\n# test.drop(columns=numerical_cols, axis=1, inplace=True)\n\n# train = pd.concat([train, scaled_matrix_train], axis=1)\n# test = pd.concat([test, scaled_matrix_test], axis=1)","93397d89":"# Select ONLY categorical columns\ncateg_cols = [col for col in train.columns if \n              train[col].dtype in [\"object\"]]","c974ea39":"# Find the columns that have null values\nna_count_s = train[categ_cols].isna().sum()     # this becomes a Pandas Series\nna_count_s[na_count_s > 0]                        # filter","d1441e97":"# Drop all columns with more than 50% missing data\nto_drop = na_count_s[na_count_s > train.shape[0]*0.5].index\n\ntrain.drop(labels=to_drop, axis=1, inplace=True)\ntest.drop(labels=to_drop, axis=1, inplace=True)\n\n# Update the categ_cols\ncateg_cols = list(set(categ_cols) - set(to_drop))","9561afb3":"# The computer doesn't know how to read letters :)\ntrain[categ_cols]","9e881b45":"# Import OneHotEncoder from sklearn\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder","759b2e9c":"# Create the imputation object\nc_mode_impute = SimpleImputer(strategy = 'most_frequent')\n# Create the encoder object\nencoder = OneHotEncoder(handle_unknown='ignore')","c22d1d4d":"# Make the Imputation\nfor column in categ_cols:\n    train[column] = apply_imputation(c_mode_impute, column)[0]\n    test[column] = apply_imputation(c_mode_impute, column)[1]\n    \n\n# Perform One Hot Encoding\nencoded_train = pd.DataFrame(encoder.fit_transform(train[categ_cols]).toarray())\nencoded_test = pd.DataFrame(encoder.transform(test[categ_cols]).toarray())\n\n# Drop old columns and replace with encoded ones\ntrain.drop(columns=categ_cols, axis=1, inplace=True)\ntest.drop(columns=categ_cols, axis=1, inplace=True)\n\ntrain = pd.concat([train, encoded_train], axis=1)\ntest = pd.concat([test, encoded_test], axis=1)","28162667":"# Data Splitting\nfrom sklearn.model_selection import train_test_split\n\n# Models (or Algorithms)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# Model Evaluation\nfrom sklearn.metrics import mean_absolute_error\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","c81224eb":"# train -> training data\n# test -> testing data (unlabeled)\n# y -> target variable (we stored it from the train data)\n\n# Target Variable: y\n# Features -> X\nX = train\n\n# Split data further\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.3, \n                                                      random_state = 0)","2db0293d":"# ~~~~~~~~~~~~~~~~\n# LinearRegression\n# ~~~~~~~~~~~~~~~~\n\nlinear_model = LinearRegression()\n\n# Train the model on training data\nlinear_model.fit(X=X_train, y=y_train)\n\n# Predict on validation data\npredictions = linear_model.predict(X_valid)\n\n# Get how well it performed\nmae_linear = mean_absolute_error(y_valid, predictions)\n\nprint(\"Linear: {:,}\".format(mae_linear))","513c90fb":"# ~~~~~~~~~~~~~~~~~~~~~\n# DecisionTreeRegressor\n# ~~~~~~~~~~~~~~~~~~~~~\n\ntree_model = DecisionTreeRegressor()\n\n# Train the model on training data\ntree_model.fit(X=X_train, y=y_train)\n\n# Predict on validation data\npredictions = tree_model.predict(X_valid)\n\n# Get how well it performed\nmae_tree = mean_absolute_error(y_valid, predictions)\n\nprint(\"Tree: {:,}\".format(mae_tree))","c73eded9":"# ~~~~~~~~~~~~~~~~~~~~~\n# RandomForestRegressor\n# ~~~~~~~~~~~~~~~~~~~~~\n\nrf_model = RandomForestRegressor()\n\n# Train the model on training data\nrf_model.fit(X=X_train, y=y_train)\n\n# Predict on validation data\npredictions = rf_model.predict(X_valid)\n\n# Get how well it performed\nmae_rf = mean_absolute_error(y_valid, predictions)\n\nprint(\"Random Forest: {:,}\".format(mae_rf))","b399759a":"# ~~~~~~~~~~~~\n# XGBRegressor\n# ~~~~~~~~~~~~\n\nxgb_model = XGBRegressor(n_estimators=600)\n\n# Train the model on training data\nxgb_model.fit(X=X_train, y=y_train)\n\n# Predict on validation data\npredictions = xgb_model.predict(X_valid)\n\n# Get how well it performed\nmae_xgb = mean_absolute_error(y_valid, predictions)\n\nprint(\"XGBoost: {:,}\".format(mae_xgb))","3a703eff":"# This model is created after Grid Search\n### check out version 4 of this notebook to see how I did it :)\nmy_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n                        colsample_bynode=1, colsample_bytree=0.8, gamma=0.0,\n                        importance_type='gain', learning_rate=0.005, \n                        max_delta_step=0, max_depth=4, min_child_weight=1, \n                        missing=None, n_estimators=5000, n_jobs=1, \n                        nthread=4, objective='reg:squarederror', random_state=0,\n                        reg_alpha=1, reg_lambda=1, scale_pos_weight=1, seed=27,\n                        silent=None, subsample=0.8, verbosity=1)\n\n# Fit on the entire training data\nmy_model.fit(X, y)","efb77575":"# Predict and Submit\nfinal_predictions = my_model.predict(test)\n\n# Import test to get ID\nX_test = pd.read_csv(\"..\/input\/iowa-house-prices\/test.csv\", index_col ='Id')\n\noutput = pd.DataFrame({'Id': X_test.index, 'SalePrice' : final_predictions})\noutput.to_csv('submission_final.csv', index = False)","04837fb7":"### #I. Looking at the Target column","9c172596":"## 2.1 Data Validation\n\nThis step is extremely important. It ensures that during the training session you have some sort of **indicator on how your model is performing**.\n\nIt also takes care of model **overfitting**: when a model overfits it means that the model isn't learning patterns and generalities from the data. Fitting the points too well might lead to a very high accuracy of the model during training, but a very low score when you actually deploy it into production:\n\n<img src=\"https:\/\/i.imgur.com\/7632QAP.png\" width=600>\n\nOne solution to this is to *split* the training data into a **training** part and **validation** part.\n\nHence, you can **train** the model on the Training Data and then **predict** on the Validation Data. This way, you can use you labeled data not only for training, but also for validating how your model performs.\n\n<img src=\"https:\/\/miro.medium.com\/max\/1552\/1*Nv2NNALuokZEcV6hYEHdGA.png\" width=600>\n\nThere are many more other options to this technique, such as K Fold or Stratified K Fold, but we won't get into them in this notebook.","b73f0676":"The Decission Tree is MUCH better, with an error of only 24,823.\n\n### #III. Random Forest Regressor\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/7\/76\/Random_forest_diagram_complete.png\" width=300>","d73cdf1a":"#### Scale the Data\n\nYou'll also need to be scaling the data, so that all the variables are normalised. It can also speed up the model computations.\n\n#### Introducing \"StandardScaler\"\ud83e\udd41","164c985b":"### #III. Categorical Data\n\n> Categorical data is the one stored in object datatypes.","bdcba6e7":"We have 2 datasets that we need to import:\n* `train` : it's the TRAINING data. The model *learns* by looking at it and understanding patterns through different algorithms. It is **labeled** (meaning that we have the houses *price* column in there).\n* `test` : it's the data for TESTING. Once we create our model and we know are working, we can test it on a NEW completely unseen dataset, to see if it can properly generalise the information learned during testing.\n\n> Splitting the data into TRAIN and TEST ensures that the model is **robust**, meaning that it is able to **generalise**. \n\n<img src=\"https:\/\/i.imgur.com\/WrMWQT3.png\" width=600>","578ed064":"## 1.2 Check for Leakages \ud83d\udebf\n\nA **leakage** is when you are using *features* that you wouldn't have available if a **new case** comes up to train your model.\n\nImagine trying to predict the price you will sell your house with, but your model all of the sudden expects you to input the *day you sold the house*, or if the transaction was *cash or through a bank order*. You can't possibly know these aspects, as you **haven't sold the house yet**.\n\nHence, we need to look for leakages in our data before proceeding with the analysis.","25b60ccd":"Now all we have to do is **impute** the rest of the data and **encode** it (transform it from data type `object` to data type `int`)","7495875a":"## 1.3 Data Preprocessing \ud83e\uddf9\n\nThere are many techniques that can be performed during this phase. Some of them are:\n* checking for **missing** data\n* analysis of the distributions and **patterns** in the data\n* creating **new features** from the existing data (feature engineering)\n* **encoding** the categorical features\n* etc.","b4e6e1ed":"The linear model looks like it's **underfitting** big time.\n\n### #II. Decission Tree Regressor\n\n<img src=\"https:\/\/miro.medium.com\/max\/2000\/1*WerHJ14JQAd3j8ASaVjAhw.jpeg\" width=300>","c5f56bc2":"# 3. There's more\n\nThere's soooooo much more to this. I'll leave here some **names** of techniques or agorithms that you might want to check out in order to further improve this score:\n\n* Feature Engineering ([check out this notebook](https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial))\n* Fancy Impute (or any other Imputation technique)\n* PCA before model training\n* K Fold or Stratified K Fold\n* Model Selection (try other models)\n* Hyperparameter Tunning\n* many many many more, you just need to be curious \ud83d\ude43\n\n# 4. Submit to Competition\n\nOnce you're ready, you can use your model to predict on the `test` dataset and submit to the [Iowa Housing Prices Competition](https:\/\/www.kaggle.com\/c\/home-data-for-ml-course).\n\nIf you want to learn mode on Machine Learning or just take a deep dive into this tutorial, check out the [Kaggle Courses](https:\/\/www.kaggle.com\/learn\/overview) in the Machine Learning Series:\n* Intro to ML\n* Intermediate ML\n* ML Explainability","cc4ed9ad":"# 2. Model Training \ud83d\udcbb\u23f0\n\nIn this Chapter we'll learn how to do the following:\n\n1. **Prepare** the data to properly feed to the model\n2. Create multiple **models** and assess which one is the best\n\n### Libraries \ud83d\udcda","43c9018b":"#### Introducing \"SimpleImputer\"\ud83e\udd41","7e261ece":"## 2.2 Model Selection: trial and error\n\nNow we start to try 1 by 1 the 4 algorithms we imported earlier. Remember, there are many more other algorithms that you can try for this problem. Head to the [sklearn documentation](https:\/\/scikit-learn.org\/stable\/) to find more.\n\n### #I. Linear Model\n\n<img src=\"https:\/\/backlog.com\/wp-blog-app\/uploads\/2019\/12\/Nulab-Gradient-descent-for-linear-regression-using-Golang-Blog.png\" width=300>","e7d39f80":"Let's see if there are any missing values between these columns.","5dfaffef":"Again, a very nice improvement. The Random Forest is performing better than the Decission Tree.\n\n### #IV. XGBoost\n\n> Still an ensemble, but more complicated. You can find [documentation here](https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/model.html).","3313d5b4":"**How should we make the imputation?**\n\n> **Imputation** is a technique that replaces missing values in the data with another value (like the mean, median, mode, or other more complex operations). *Be weary of the bias*!","6d14120f":"Let's see if there are any missing values between these columns.","45e2033a":"> **Happy Data Sciencin'!**\n\n<img src=\"https:\/\/i.imgur.com\/cUQXtS7.png\">\n\n# Specs on how I prepped & trained \u2328\ufe0f\ud83c\udfa8\n### (*locally*)\n* Z8 G4 Workstation \ud83d\udda5\n* 2 CPUs & 96GB Memory \ud83d\udcbe\n* NVIDIA Quadro RTX 8000 \ud83c\udfae\n* RAPIDS version 0.17 \ud83c\udfc3\ud83c\udffe\u200d\u2640\ufe0f","cfde015d":"### #II. Numerical Data\n\n> Numerical columns are the ones that are of type `int` or `float`.","8d44e112":"#### Introducing \"One Hot Encoder\" \ud83e\udd41\n\nThere are different types of encoding methodologies:\n* **Label Encoding**: when you convert the categories (e.g.: day, night, noon) into numbers\/labels (e.g.: 1, 2, 3).\n* **One Hot Encoding**: when you create a *flag* for each category, with 1 where that category appears and 0 otherwise.\n\n<img src=\"https:\/\/i.imgur.com\/3725Gwc.png\" width=600>","5d249931":"## 1.1 Importing the data","138ef744":"<img src=\"https:\/\/i.imgur.com\/GNiaWNs.png\">\n<center><h1>Predict House Prices in Iowa<\/h1><\/center>\n<center><h2>CORGI Workshop<\/h2><\/center>\n\n# Introduction\n\n### What is a Regression Problem?\n\nA Regression problem is a **supervised machine learning technique**, which has the aim of predicting a *real* or *continuous* variable (e.g.: salary, height, house prices \ud83d\ude09, etc.) by looking at other *helper* variables (which can be also called *features*).\n\nHence, we're trying to:\n* predict a TARGET\n* by looking at FEATURES\n\n<img src=\"https:\/\/i.imgur.com\/eXHXYdZ.png\" width = 600>\n\n\n### Predicting House Prices in Iowa (Kaggle Competition) \ud83c\udfe1\n\nThis competition aims to predict the Sales Price of Houses in Iowa, by looking at various aspects of the house or area, such as *number of rooms, number of floors, garage, the street, pool, utilities etc.*\n\nThere are 2 Chapters that we'll cover to achieve our goal:\n1. **Data Preprocessing**\n2. **Data Modeling**\n\n\n# 1. Data Preprocessing \ud83d\udee0\n\nIn this Chpater we'll learn how to do the following:\n\n1. Import and analyse the data\n2. Clean the data from missing values\n3. Encode categorical variables\n\n### Libraries \ud83d\udcda"}}