{"cell_type":{"f8e2c180":"code","cda07d08":"code","4e30fa27":"code","2642c5c3":"code","b1bf2c6b":"code","dcb8392d":"code","64511ab0":"code","f696c173":"code","c431c035":"code","c1d24d46":"code","406a81b0":"code","a0db4472":"code","5d9a88ad":"code","fb5b227c":"code","342cc365":"code","ec8dfc4c":"code","2f722bb7":"code","67a12bc6":"code","89e36d28":"code","2ad9074c":"code","15fc4dac":"markdown","46d8da81":"markdown"},"source":{"f8e2c180":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom IPython.display import display\nimport random\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt","cda07d08":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e30fa27":"def load_data():\n    training_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\n    training_data = training_data.sample(frac=1, random_state=42).reset_index(drop=True)\n    test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n    return training_data, test_data","2642c5c3":"training_data, test_data = load_data()","b1bf2c6b":"def plot_correlation_map(data):\n    corr = data.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n        corr, \n        cmap = cmap,\n        square=True, \n        cbar_kws={ 'shrink' : .9 }, \n        ax=ax, \n        annot = True, \n        annot_kws = { 'fontsize' : 12 }\n    )\n\ndef plot_col_x_col_hist(data, col1_name, col2_name, ax, bins=None):\n    palette = {0: 'tab:red', 1: 'tab:green'} if  col2_name == 'Survived' else sns.color_palette('coolwarm', as_cmap = True)\n    sns.histplot(\n        data=data,\n        x=col1_name, hue=col2_name,\n        multiple=\"fill\",\n        stat='density',\n        palette=palette,\n        edgecolor=\".3\",\n        bins=bins,\n        linewidth=.5,\n        ax=ax\n    )\n    ax.set_title(col1_name + ' <--> ' + col2_name)\n\ndef plot_column_insights_hist(data, col_name, ax, bins=None):\n    sns.histplot(\n        data=data,\n        x=col_name,\n        stat='count',\n        palette=sns.color_palette('coolwarm', as_cmap = True),\n        edgecolor=\".3\",\n        bins=30,\n        linewidth=.5,\n        ax=ax\n    )\n    ax.set_title(col_name)\n    \ndef plot_columns_x_survival_insights(data):\n    f, ax = plt.subplots(6,2,figsize=(30, 30))\n    sns.countplot(data.Survived, ax=ax[0][0])\n    ax[0][0].set_title('Survived')\n\n    plot_col_x_col_hist(data, \"Sex\", \"Survived\", ax[0][1])\n    plot_col_x_col_hist(data, \"Pclass\", \"Survived\", ax[1][0], bins=3)\n    plot_col_x_col_hist(data, \"Age\",\"Survived\", ax[1][1], bins=30)\n    plot_col_x_col_hist(data, \"Age\", \"Pclass\", ax[2][0], bins=30)\n    plot_col_x_col_hist(data, \"Embarked\", \"Survived\", ax[2][1])\n    plot_col_x_col_hist(data, \"Fare\", \"Survived\", ax[3][0], bins=[0, 10, 50, 75, 150, 400, 1000])\n    plot_col_x_col_hist(data, \"Title\", \"Survived\", ax[3][1])\n    plot_col_x_col_hist(data, \"GenderFilled\", \"Survived\", ax[4][0])\n    plot_col_x_col_hist(data, \"Deck\", \"Survived\", ax[4][1])\n    plot_col_x_col_hist(data, \"Familysz\", \"Survived\", ax[5][0], bins=10)\n    plot_col_x_col_hist(data, \"Single\", \"Survived\", ax[5][1], bins=2)\n    \n\ndef plot_detailed_column_insights(data):\n    f, ax = plt.subplots(4,2,figsize=(30, 30))\n    plot_column_insights_hist(data, 'Sex', ax[0][0])\n    plot_column_insights_hist(data, 'Pclass', ax[0][1])\n    plot_column_insights_hist(data, 'Age', ax[1][0], bins=30)\n    plot_column_insights_hist(data, 'Embarked', ax[1][1])\n    plot_column_insights_hist(data, 'Fare', ax[2][0], bins=[0, 10, 50, 75, 150, 400, 1000])\n    plot_column_insights_hist(data, 'Title', ax[2][1])\n    plot_column_insights_hist(data, 'Fare', ax[3][0])","dcb8392d":"display(training_data.head(10))\nplot_correlation_map(training_data)","64511ab0":"TITLES = ['Miss','Mrs','Rev','Mr', 'Master', 'Other', 'Mr']\n\ndef extract_title(data):\n    data['Title'] = data.Name.str.extract('([A-Za-z]+)\\.')\n    #data['Title'].replace(['Mlle','Ms',  'Mme', 'Major','Lady','Countess','Jonkheer','Col',  'Rev',  'Capt','Don','Dr', 'Dona'],\n    #                      ['Miss','Miss','Mrs', 'Sir',  'Mrs', 'Mr',      'Mr',      'Mr',   'Rev',  'Mr',  'Mr', 'Mr', 'Mrs'],inplace=True)\n    \n    data['Title'].replace(['Mlle','Ms',  'Mme', 'Major','Lady','Countess','Jonkheer','Col',  'Rev',  'Capt','Don','Dr', 'Sir','Dona'],\n                          ['Miss','Miss','Miss','Mr',   'Mrs', 'Mrs',     'Other',   'Other','Other','Mr',  'Mr', 'Mr', 'Mr', 'Mrs'],inplace=True)\n    \n    data.loc[~data['Title'].isin(TITLES), 'Title'] = \"Other\"\n    return data\n\ndef extract_family_features(data):\n    data['Familysz'] = data['SibSp'] + data['Parch'] + 1\n    data['Single'] = data['Familysz'] == 0\n    return data\n\ndef extract_gender_feature(data):\n    data['Gender'] = 'child'\n    data.loc[(data.Age <= 12 ),'Gender'] = 'child'\n    data.loc[(data.Age > 12)&(data.Sex=='female'),'Gender'] = 'female'\n    data.loc[(data.Age > 12)&(data.Sex=='male'),'Gender'] = 'male'\n    \n    data['GenderFilled'] = 'child'\n    data.loc[(data.AgeFilled <= 12 ),'GenderFilled'] = 'child'\n    data.loc[(data.AgeFilled > 12)&(data.Sex=='female'),'GenderFilled'] = 'female'\n    data.loc[(data.AgeFilled > 12)&(data.Sex=='male'),'GenderFilled'] = 'male'\n    return data\n\ndef fill_missing_age(data):\n    titles = TITLES\n    \n    missing_age_value = {\n        'Miss': 21.0,\n        'Mrs': 35.0,\n        'Rev': 45.0,\n        'Mr':32.0,\n        'Master': 4,\n        'Other': 48.0,\n        'Sir': 47,\n    }\n    \n    data['AgeFilled'] = data['Age']\n    #for t in titles:\n    #    age_value_median = data[data[\"Title\"] == t][\"Age\"].median()\n    #    age_value_mean = data[data[\"Title\"] == t][\"Age\"].mean()\n    #    data.loc[(data.AgeFilled.isnull())&(data.Title==t), 'AgeFilled'] = age_value_median\n    #    print(t + \" Title : Med\" + str(age_value_median) + \" Mea :\" + str(age_value_mean))\n    #print()\n    for key in missing_age_value:\n        data.loc[(data.AgeFilled.isnull())&(data.Title==key), 'AgeFilled'] = missing_age_value[key]\n    return data\n\ndef fill_missing_fare(data):\n    pclass = [1,2,3]\n    data['FareFilled'] = data['Fare']\n    missing_fare_value = {\n        1: 84.0,\n        2: 20.0,\n        3: 13.0,\n    }\n    #for pc in pclass:\n    #    fare_value_median = data[data[\"Pclass\"] == pc][\"Fare\"].median() \n    #    fare_value_mean = data[data[\"Pclass\"] == pc][\"Fare\"].mean() \n    #    print(str(pc) + \" Class : Med\" + str(fare_value_median) + \" Mea :\" + str(fare_value_mean))\n    #    data.loc[(data.FareFilled.isnull())&(data.Pclass==pc), 'FareFilled'] = fare_value_median\n    for key in missing_fare_value:\n        data.loc[(data.FareFilled.isnull())&(data.Pclass==key), 'FareFilled'] = missing_fare_value[key]\n\n    return data\n\ndef extract_deck(data):\n    data['Deck'] = data['Cabin'].str.get(0)\n    return data\n    \ndef extract_features(data):\n    data = extract_title(data)\n    data = fill_missing_age(data)\n    data = extract_gender_feature(data)\n    data = extract_family_features(data)\n    data = fill_missing_fare(data)\n    data = extract_deck(data)\n    return data","f696c173":"training_data = extract_features(training_data)\ntest_data = extract_features(test_data)","c431c035":"z = training_data.copy()\nz['HC'] =  training_data['Cabin'].notnull()\nsns.histplot(\n    data=z,\n    x = 'HC',\n    hue=\"Survived\",\n    multiple=\"fill\",\n    stat='density',\n    palette={0: 'tab:red', 1: 'tab:green'},\n    edgecolor=\".3\",\n    bins=2,\n    linewidth=.5,\n)","c1d24d46":"plot_columns_x_survival_insights(training_data)","406a81b0":"plot_detailed_column_insights(training_data)","a0db4472":"plot_detailed_column_insights(test_data)","5d9a88ad":"from sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder, MinMaxScaler, FunctionTransformer, OneHotEncoder, Binarizer,KBinsDiscretizer\nfrom sklearn.pipeline import make_pipeline\n\nclass FeatureTransformer:\n    def __init__(self):\n        self.pipeline = []\n\n    def mean_imputer(self):\n        self.pipeline.append(SimpleImputer(missing_values=np.nan, strategy=\"mean\", add_indicator=False))\n        return self\n    \n    def const_imputer(self, value):\n        self.pipeline.append(SimpleImputer(missing_values=np.nan, strategy=\"constant\", fill_value=value, add_indicator=False))\n        return self\n    \n    def frequent_imputer(self):\n        self.pipeline.append(SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\", add_indicator=False))\n        return self\n    \n    def ordinal_encoder(self, cat='auto'):\n        self.pipeline.append(OrdinalEncoder(categories=[cat]))\n        return self\n    \n    def binary_encoder(self):\n        self.pipeline.append(Binarizer())\n        return self\n    \n    def onehot_encoder(self):\n        self.pipeline.append(OneHotEncoder(handle_unknown='ignore', dtype='int', sparse=False))\n        return self\n    \n    def normalize(self):\n        self.pipeline.append(StandardScaler())\n        return self\n    \n    def normalize_to_range(self, range=(0,1)):\n        self.pipeline.append(MinMaxScaler(feature_range=range))\n        return self\n    \n    def custom_bin(self, bins, labels):\n        ft = FunctionTransformer(lambda X: pd.cut(np.squeeze(X), bins, labels=labels, retbins=False).to_numpy().reshape(-1,1))\n        self.pipeline.append(ft)\n        return self\n    \n    def has_value_binary_encoder(self):\n        ft = FunctionTransformer(lambda X: pd.notnull(np.squeeze(X)).to_numpy().reshape(-1,1))\n        self.pipeline.append(ft)\n        return self\n    \n    def uniform_bin(self, n_bins=5):\n        # {\u2018onehot\u2019, \u2018onehot-dense\u2019, \u2018ordinal\u2019}, default=\u2019onehot\u2019\n        # strategy{\u2018uniform\u2019, \u2018quantile\u2019, \u2018kmeans\u2019}, default=\u2019quantile\u2019\n        # uniform: All bins in each feature have identical widths.\n        # quantile: All bins in each feature have the same number of points.\n        # kmeans: Values in each bin have the same nearest center of a 1D k-means cluster.\n        self.pipeline.append(KBinsDiscretizer(n_bins=n_bins, encode='onehot', strategy='uniform'))\n        return self\n    \n    def quantile_bin(self, n_bins=5, enc='onehot'):\n        # {\u2018onehot\u2019, \u2018onehot-dense\u2019, \u2018ordinal\u2019}, default=\u2019onehot\u2019\n        # strategy{\u2018uniform\u2019, \u2018quantile\u2019, \u2018kmeans\u2019}, default=\u2019quantile\u2019\n        # uniform: All bins in each feature have identical widths.\n        # quantile: All bins in each feature have the same number of points.\n        # kmeans: Values in each bin have the same nearest center of a 1D k-means cluster.\n        self.pipeline.append(KBinsDiscretizer(n_bins=n_bins, encode='onehot', strategy='quantile'))\n        return self\n\n    def transform(self):\n        return make_pipeline(*self.pipeline)\n        ","fb5b227c":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\n\ndef get_features_transforms():\n    age_bins = [0, 1, 13, 20, 60, np.inf]\n    age_labels = ['infant', 'kid', 'teen', 'adult', 'senior']\n    \n    #fare_bins = [0, 10, 50, 75, 150, 400, np.inf]\n    fare_bins = [0, 11, 51, 76, 151, 301, np.inf]\n    fare_labels = ['tier1', 'tier2', 'tier3', 'tier4', 'tier5', 'tier6']\n    \n    deck_labels  = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', 'X']\n    \n    family_bins = [0, 2, 5, 8, np.inf]\n    family_labels = ['alone', 'small', 'medium', 'large']\n\n    features_transfomer = make_column_transformer(\n        ### SEX & Gender###\n        (FeatureTransformer().onehot_encoder().transform(), ['GenderFilled']),\n        \n        ### Embarked ###\n        (FeatureTransformer().const_imputer('S').onehot_encoder().transform(), ['Embarked']),\n        \n        ### Pclass ###\n        ('passthrough', ['Pclass']),\n        \n        ### Age ###\n        #(FeatureTransformer().custom_bin(age_bins, age_labels).onehot_encoder().transform(), ['AgeFilled']), #NT\n        (FeatureTransformer().custom_bin(age_bins, age_labels).ordinal_encoder(cat=age_labels).transform(), ['AgeFilled']), #T\n        \n        \n        ### Fare ###\n        (FeatureTransformer().custom_bin(fare_bins, fare_labels).const_imputer('tier1').ordinal_encoder(cat=fare_labels).transform(), ['FareFilled']), # T\n        \n        ### Title ###\n        #(FeatureTransformer().const_imputer('Other').onehot_encoder().transform(), ['Title']), #T\n\n        \n        ### Cabin ### \n        (FeatureTransformer().const_imputer('X').ordinal_encoder(cat=deck_labels).transform(), ['Deck']), # T (try ordinal)\n        \n        ### Single ###\n        #- family seg\n        (FeatureTransformer().custom_bin(family_bins, family_labels).const_imputer('alone').ordinal_encoder(cat=family_labels).transform(), ['Familysz']),\n        #('passthrough', ['Single']), #T\n    )\n    return features_transfomer\n\ndef get_GBDT_model(): \n    classifier = make_pipeline(get_features_transforms(), \n                               GradientBoostingClassifier(\n                                   # The loss function to be optimized. \u2018deviance\u2019 refers to deviance (= logistic regression) for classification with probabilistic outputs. \n                                   # For loss \u2018exponential\u2019 gradient boosting recovers the AdaBoost algorithm.\n                                   loss='deviance',\n                                   learning_rate=0.01,\n                                   # The number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance.\n                                   n_estimators=150, #200, \n                                   max_depth=20, #6, #16,\n                                   min_samples_split=20, #10, #2,\n                                   max_leaf_nodes=30, #8, #,16,\n                                   #subsample=0.5,\n                               ))\n    return classifier\n\ndef train_eval_predict(classifier, training_data, test_data):\n    scores = cross_val_score(classifier, training_data, training_data['Survived'].ravel(), cv=4)\n    print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))\n    Y_data = training_data['Survived']\n    X_data = training_data.drop('Survived', axis=1)\n    classifier = classifier.fit(X_data, Y_data.ravel())\n    return classifier.predict(test_data)\n\ndef GBDT_fit_predict(training_data, test_data):\n    model = get_GBDT_model()\n    return train_eval_predict(model, training_data, test_data)\n    \ndef get_MLP_model(training_data, test_data):\n    classifier = make_pipeline(get_features_transforms(), \n                               MLPClassifier(\n                                   hidden_layer_sizes=(64, 64, 64),\n                                   #early_stopping=True,\n                                   #solver='sgd',\n                                   solver='adam',\n                                   #learning_rate =\"constant\",\n                                   max_iter=2000,\n                                   #batch_size='auto',\n                                   learning_rate_init=0.00005,\n                                   alpha=0.001,\n                               )\n                              )\n    return classifier\n\ndef MLP_fit_predict(training_data, test_data):\n    model = get_MLP_model(training_data, test_data)\n    return train_eval_predict(model, training_data, test_data)\n\ndef perform_hyperparam_search(model, hyper_parameters, training_data):\n    classifier = GridSearchCV(model, hyper_parameters, scoring=\"accuracy\", cv=3)\n    Y_data = training_data['Survived']\n    X_data = training_data.drop('Survived', axis=1)\n    classifier.fit(X_data, Y_data)\n    \n    print(\"Best parameters set found on development set:\")\n    print()\n    print(classifier.best_params_)\n    print()\n    print(\"Grid scores on development set:\")\n    print()\n    means = classifier.cv_results_[\"mean_test_score\"]\n    stds = classifier.cv_results_[\"std_test_score\"]\n    for mean, std, params in zip(means, stds, classifier.cv_results_[\"params\"]):\n        print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))\n    print()\n    \ndef GBDT_hyperparam_search(training_data, test_data):\n    GBDT = get_GBDT_model()\n    hyper_parameters = {\n        'gradientboostingclassifier__n_estimators':[150, 200, 400],\n        'gradientboostingclassifier__learning_rate':[0.01],\n        'gradientboostingclassifier__max_leaf_nodes':[None, 4, 8, 16, 30, 32],\n        'gradientboostingclassifier__max_depth':[None, 3, 4, 6, 8, 16, 20, 32],\n        'gradientboostingclassifier__subsample':[0.5, 1.0],\n        'gradientboostingclassifier__min_samples_split':[2, 5, 10, 20, 30],\n    }\n    perform_hyperparam_search(GBDT, hyper_parameters, training_data)\n    \ndef MLP_hyperparam_search(training_data, test_data):\n    MLP = get_MLP_model(training_data, test_data)\n     \n    hyper_parameters = [\n        {\n            'mlpclassifier__solver':[\"adam\"],\n            'mlpclassifier__hidden_layer_sizes': [(32,), (64,), (128,), (265,), (32, 32,), (64, 64,), (128, 128,), (256, 256,),],\n            #'mlpclassifier__learning_rate_init':[0.00001, 0.00005, 0.0001],\n            #'mlpclassifier__max_iter':[200, 1000, 2000],\n            #'mlpclassifier__early_stopping': [True, False],\n            #'mlpclassifier__alpha': [0.001, 0.01],\n        },\n        #{\n        #    'mlpclassifier__solver':[\"sgd\"],\n        #    'mlpclassifier__learning_rate_init':[0.00001, 0.00005, 0.0001],\n        #    'mlpclassifier__max_iter':[200, 1000, 2000],\n        #    'mlpclassifier__learning_rate': [\"constant\", \"adaptive\"],\n        #    'mlpclassifier__nesterovs_momentum': [False, True],\n        #    'mlpclassifier__momentum':[0, 0.5, 0.9],\n        #    'mlpclassifier__early_stopping': [True, False],\n        #    'mlpclassifier__alpha': [0.00001, 0.0001, 0.001, 0.05],\n        #},\n    ]\n    perform_hyperparam_search(MLP, hyper_parameters, training_data)","342cc365":"def submit_predictions(test_data, predictions):\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    output.to_csv('submission.csv', index=False)\n    print(\"Your submission was successfully saved!\")","ec8dfc4c":"training_data = extract_features(training_data)\ntest_data = extract_features(test_data)\nt = training_data.drop('Survived', axis=1)\nx = get_features_transforms()\ny = x.fit_transform(t)\ndisplay(t.head(5))\ndisplay(y[0:5])\nz = x.transform(test_data)\ndisplay(test_data.head(5))\ndisplay(z[0:5])","2f722bb7":"#GBDT_predictions = GBDT_fit_predict(training_data, test_data)\n#submit_predictions(test_data, GBDT_predictions)","67a12bc6":"MLP_predictions = MLP_fit_predict(training_data, test_data)\nsubmit_predictions(test_data, MLP_predictions)","89e36d28":"#MLP_hyperparam_search(training_data, test_data)\n# 0.806 (+\/-0.008) for {'learning_rate_init': 0.001, max_iter': 2000} # Adam\n# 0.810 (+\/-0.008) for {'learning_rate_init': 0.00005, 'max_iter': 1000} # Adam\n# 0.822 (+\/-0.011) for {'learning_rate_init': 0.0001, 'max_iter': 200} # Adam\n# 0.823 (+\/-0.034) for {'learning_rate_init': 0.00001, 'max_iter': 2000} # Adam\n# 0.827 (+\/-0.021) for {'learning_rate_init': 0.00005, 'max_iter': 1000} # Adam\n# 0.832 (+\/-0.015) for {'learning_rate_init': 0.00005, 'max_iter': 200} # Adam\n# 0.831 (+\/-0.025) for {'alpha': 0.001, 'mlpclassifier__early_stopping': False, 'mlpclassifier__learning_rate_init': 1e-05, 'mlpclassifier__max_iter': 2000, 'mlpclassifier__solver': 'adam'}\n# 0.831 (+\/-0.022) for {'alpha': 0.001, 'mlpclassifier__early_stopping': False, 'mlpclassifier__learning_rate_init': 5e-05, 'mlpclassifier__max_iter': 200, 'mlpclassifier__solver': 'adam'}\n\n# 0.814 (+\/-0.014) for {'hidden_layer_sizes': (32,), 'mlpclassifier__solver': 'adam'}\n# 0.831 (+\/-0.022) for {'hidden_layer_sizes': (512,), 'mlpclassifier__solver': 'adam'}\n# 0.809 (+\/-0.008) for {'hidden_layer_sizes': (256, 256), 'mlpclassifier__solver': 'adam'}\n# 0.811 (+\/-0.005) for {'hidden_layer_sizes': (64, 64, 64), 'mlpclassifier__solver': 'adam'}\n","2ad9074c":"#GBDT_hyperparam_search(training_data, test_data)\n#0.817 -  0.849\n#0.833 (+\/-0.016) for {' learning_rate': 0.01, ' max_depth': None, ' max_leaf_nodes': 4, ' min_samples_split': 2, ' n_estimators': 200, ' subsample': 0.5}\n#0.816 -  0.852\n#0.834 (+\/-0.018) for {' learning_rate': 0.01, ' max_depth': 8, ' max_leaf_nodes': 4, ' min_samples_split': 20, ' n_estimators': 200, ' subsample': 0.5}\n#0.815 - 0.855\n#0.835 (+\/-0.020) for {' learning_rate': 0.01, ' max_depth': None, ' max_leaf_nodes': 4, ' min_samples_split': 5, ' n_estimators': 200, ' subsample': 0.5}\n\n#0.807\n#0.829 (+\/-0.022) for {'gradientboostingclassifier__learning_rate': 0.01, 'gradientboostingclassifier__max_depth': 32, 'gradientboostingclassifier__max_leaf_nodes': 8, 'gradientboostingclassifier__min_samples_split': 30, 'gradientboostingclassifier__n_estimators': 150, 'gradientboostingclassifier__subsample': 0.5}\n\n#0.813\n#0.828 (+\/-0.015) for {'gradientboostingclassifier__learning_rate': 0.01, 'gradientboostingclassifier__max_depth': 6, 'gradientboostingclassifier__max_leaf_nodes': 8, 'gradientboostingclassifier__min_samples_split': 10, 'gradientboostingclassifier__n_estimators': 200, 'gradientboostingclassifier__subsample': 0.5}\n\n#0.823 (+\/-0.003) for {'gradientboostingclassifier__learning_rate': 0.01, 'gradientboostingclassifier__max_depth': 16, 'gradientboostingclassifier__max_leaf_nodes': 16, 'gradientboostingclassifier__min_samples_split': 2, 'gradientboostingclassifier__n_estimators': 200, 'gradientboostingclassifier__subsample': 0.5}\n\n#0.818 (+\/-0.000) for {'gradientboostingclassifier__learning_rate': 0.01, 'gradientboostingclassifier__max_depth': None, 'gradientboostingclassifier__max_leaf_nodes': 32, 'gradientboostingclassifier__min_samples_split': 20, 'gradientboostingclassifier__n_estimators': 150, 'gradientboostingclassifier__subsample': 0.5}\n","15fc4dac":"# **Feature Extraction**","46d8da81":"# Data Exploration"}}