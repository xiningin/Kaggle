{"cell_type":{"cb54ba71":"code","0d9f1890":"code","464afa5c":"code","87378ce5":"code","e40d92bd":"code","828af8bc":"code","e463846f":"code","a1c98eeb":"code","35520bd4":"code","d8a0c5de":"code","ba425966":"code","13a40d92":"code","fb809756":"code","c48dde48":"code","70bd4538":"code","f80169ca":"code","eebc9bef":"code","990a5eac":"code","4d87b52b":"code","f6d104b7":"code","d2f7e310":"code","50b6b3ad":"code","4bfcd713":"code","20ab21d2":"code","7f1979cf":"code","9a4d2d8a":"code","f17d1a0b":"code","04a7c134":"code","fd03e125":"code","a40e7562":"code","c962f492":"code","727850cc":"code","c11ae09c":"code","669a0cbf":"code","5fa2a89d":"markdown","a90e3bd1":"markdown","a4d96839":"markdown","af137abe":"markdown","2c8e6794":"markdown","6772ff65":"markdown","6b6b13a5":"markdown","61b521dd":"markdown","5f2aa791":"markdown","c498c1c8":"markdown","cd807d72":"markdown","8a764126":"markdown","0301abd8":"markdown","047aed81":"markdown","31257584":"markdown","31715714":"markdown","a183f406":"markdown","0b63eadf":"markdown","98a7773e":"markdown","fccad810":"markdown","d700ef4a":"markdown","ccc9f3e0":"markdown"},"source":{"cb54ba71":"# importing libraries.\nimport pandas as pd\nimport numpy as np\n\n# for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.style.use(\"tableau-colorblind10\")\n# for imbalance data\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.metrics import confusion_matrix\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n\n\nimport warnings\nwarnings.filterwarnings('ignore')","0d9f1890":"train_data = pd.read_csv(\"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\")\ntrain_data.head()","464afa5c":"print(\"Summary for train data \\n\")\n\nprint(\"Shape:\\n\", train_data.shape)\nprint(\"----\"*10)\n\nprint(\"Discription: \\n\", train_data.describe())\nprint(\"----\"*10)\n\nprint(train_data.info())\nprint(\"----\"*10)\n\n# Let's take a look at unique values for each column\/ features with object datatype.\nprint(\"Value counts\")\nfor col in train_data.select_dtypes(object):\n    print(\"----\"*10)\n    print(train_data[col].value_counts())","87378ce5":"# For last_new_job column.\ntrain_data.last_new_job.replace(\"never\", 0, inplace=True)\n\n# Let's consider experience less than 1 means 0 and greater than 20 means 21.\ntrain_data.experience.replace({'<1': 0, '>20': 21}, inplace=True)","e40d92bd":"# Training hours.\n# To check outliers in Training hours.\nplt.figure(figsize=(14, 8))\nsns.histplot(train_data.training_hours, kde=True, fill=False)\n\nplt.title(\"Distribution of training hours\")\nplt.show()","828af8bc":"print(\"Skewness: \", train_data.training_hours.skew())\nprint(\"Median: \", train_data.training_hours.quantile(0.50))\nQ3 = train_data.training_hours.quantile(0.95)\nprint(\"Q3: \", Q3)\n\n# removing outliers\ntrain_data['training_hours'] = np.where(train_data.training_hours > Q3, Q3, train_data.training_hours)\nprint(\"Skewness: \", train_data.training_hours.skew())","e463846f":"# Missing Data.\npercent = (train_data.isnull().sum() \/ train_data.shape[0] ) *100\nmissingData = pd.DataFrame(round(percent, 2), columns=['PercentMissingData'])\nmissingData","a1c98eeb":"# Handling missing data.\n# Fill all object type of data with most occurance values in the columns.\n\ndef filling_data(col, data):\n    \"\"\"\n    Filling Missig data of object datatypes columns with its mode value.\n    \"\"\"\n    value = data[col].mode()[0]\n    data[col].fillna(value, inplace=True)\n\nfor col in train_data.select_dtypes(object):\n    if train_data[col].isnull().sum() > 0:\n        filling_data(col, train_data)","35520bd4":"print(train_data.dtypes)\n\ntrain_data['experience'] = train_data.experience.astype(int)\n# train_data['last_new_job'] = train_data.last_new_job.astype(int)\ntrain_data['target'] = train_data.target.astype(int)\ntrain_data['training_hours'] = train_data.training_hours.astype(int)","d8a0c5de":"# Average Total experience by education level.\ngrp_data = pd.DataFrame(train_data.groupby('education_level').mean()['experience'])\ngrp_data.sort_values(by='experience', ascending=False, inplace=True)\n\n# Plot\ngrp_data.plot(kind='bar', figsize=(14, 8), title=\"Average Experience of Employee by Education level\", fontsize=14)\n\nplt.show()","ba425966":"# Average experience in Candidates by gender and education_level\ngrp_data = pd.DataFrame(train_data.groupby(['education_level', 'gender']).mean()['experience'])\n\n# Plot\ngrp_data.unstack().plot(kind='bar', figsize=(14, 8), title=\"Average of Total Experience by Gender and Education level\", fontsize=13)\n\nplt.xticks(rotation=40)\nplt.show()","13a40d92":"# Observation for most common education major streams in candidates.\ngrp_data = pd.DataFrame(train_data.groupby(['major_discipline']).count()['enrollee_id'])\ngrp_data.sort_values(by='enrollee_id', ascending=False, inplace=True)\n\ngrp_data.plot(kind='bar', figsize=(14, 8), fontsize=14)\n\nplt.ylabel(\"Count\")\nplt.title(\"Most Common Education major in Candidates \")\n\nplt.xticks(rotation=45)\nplt.show()","fb809756":"# Observation for most common education level in candidates.\ngrp_data = pd.DataFrame(train_data.groupby(['education_level']).count()['enrollee_id'])\ngrp_data.rename(columns={'enrollee_id':'EducationLevelCounts'}, inplace=True)\ngrp_data.sort_values(by='EducationLevelCounts', ascending=False, inplace=True)\n\ngrp_data.plot(kind='bar', figsize=(14, 8), fontsize=14)\n\nplt.ylabel(\"Count\")\nplt.title(\"Most Common Degree in Candidates \")\n\nplt.xticks(rotation=45)\nplt.show()","c48dde48":"# Examine the last_new_job variable.\n\nfig, axs = plt.subplots(nrows = 2, ncols = 2, figsize=(20, 15))\n\n# Number of observation for last_new_job.\n\nax1 = sns.countplot(train_data.last_new_job, color=\"#0779E4\", ax = axs[0, 0], label='counts')\nax1.set_title(\"Number of Obeservation for job change in years\")\nax1.legend()\n\n\n# Relation between year of difference between job change and relevent experience?\nb = pd.DataFrame(train_data.groupby(['last_new_job', 'relevent_experience']).count()['enrollee_id'])\nb.rename(columns={'enrollee_id':'Counts'}, inplace=True)\nax2 = b.unstack().plot(ax=axs[0,1])\nax2.set_title(\"Most freqently job changes by education level\")\n\n\n# Relation between year of difference between job change and education level?\nc = pd.DataFrame(train_data.groupby(['last_new_job', 'education_level']).count()['enrollee_id'])\nc.rename(columns={'enrollee_id':'Counts'}, inplace=True)\nax3 = c.unstack().plot(kind='line', ax=axs[1,0])\nax3.set_title(\"Most freqently job changes by Education Level\")\n\n\n# Who changes the job Most frequently?\nd = pd.DataFrame(train_data.groupby(['last_new_job', 'gender']).count()['enrollee_id'])\nd.rename(columns={'enrollee_id':'Counts'}, inplace=True)\nax4 = d.unstack().plot(kind='line', ax=axs[1,1])\nax4.set_title(\"Most freqently job changes by Gender\")\n\n\nplt.show()","70bd4538":"# Observation for Relevent experience.\na = train_data.groupby(by=['education_level','relevent_experience']).count()['enrollee_id']\n\n# plot\na.unstack().plot(kind='bar', figsize=(14, 8), fontsize=16)\n\nplt.ylabel(\"Counts\")\nplt.title(\"Relevant Experience in Data Science by Education level\")\nplt.xticks(rotation=45)\n\nplt.show()","f80169ca":"# Candidates with more number of experience are looking for job change?\n\nPer_exp_df = (train_data.groupby(['experience', 'target']).count()['enrollee_id']) \/ (train_data.groupby('experience').count()['enrollee_id'])*100\nPer_exp_df = round(Per_exp_df, 2)\nPer_exp_df.unstack().plot(kind='line', figsize=(14, 8), fontsize=14)\n\nplt.ylabel(\"Percentage\")\nplt.title(\"Percentage of candidates with overall experience.\")\n\nplt.show()","eebc9bef":"# Observation for Graduates.\n\ngrads_df = train_data.loc[train_data.education_level == 'Graduate']\n\n# Who is ready to change the Job?\na = pd.DataFrame(grads_df.groupby(['last_new_job', 'target']).count()['enrollee_id'])\n\na.unstack().plot(kind='line', figsize=(14, 8))\n\nplt.title(\"Number of Graduate looking for job changes.\")\nplt.show()","990a5eac":"# Graduates who enrolled in Training are willing to change the Job?\nenrollGrads_df = pd.DataFrame((grads_df.groupby(by=['enrolled_university', 'target']).count()['enrollee_id']))\ntotal_enrolledDf = pd.DataFrame((grads_df.groupby(by=['enrolled_university']).count()['enrollee_id']))\n\na = round((enrollGrads_df \/ total_enrolledDf) * 100)\na.rename(columns={'enrollee_id':'GradsCounts'}, inplace=True)\n\n# Visualization for the same.\na.unstack().plot(kind='bar',figsize=(14, 8), fontsize=16)\n\nplt.ylabel(\"Percentage\")\nplt.title(\"Percentage of Graduates Enrolled in Course\")\nplt.xticks(rotation=0)\nplt.show()","4d87b52b":"# Does training_hours has influence to change a job?\n\na = pd.DataFrame(train_data.groupby(['training_hours','target']).count()['enrollee_id'])\na.rename(columns= {'enrollee_id': \"Counts\"}, inplace=True)\na.reset_index(inplace=True)\n\nplt.figure(figsize=(14, 8))\nsns.scatterplot(a.Counts,a.training_hours, hue='target', data=a)\nplt.title(\"Training Hours by target\")\nplt.show()","f6d104b7":"# correlation.\ncorr = train_data.corr()\nmask = np.triu(corr)\n\n# correlation plot\n\nplt.figure(figsize=(14, 8))\nsns.heatmap(corr, annot=True, vmin=-1, vmax=1, center=0, fmt=\"0.2g\", square=True, mask=mask, cmap='coolwarm')\n\nplt.xlabel(\"Features\")\nplt.title(\"Correlation between variables.\")\n\nplt.show()","d2f7e310":"# Examine city_development_index.\n\nprint(\"Median: \", train_data.city_development_index.median())\nprint(\"Mean: \", train_data.city_development_index.mean())\nprint(\"Skewness: \", train_data.city_development_index.skew()) # should be between -1 and 1\n\n# plot\nplt.figure(figsize=(14, 8))\nsns.histplot(train_data.city_development_index, fill=False, kde=True)\nplt.title(\"Distribution plot\")\nplt.show()","50b6b3ad":"labels = ['40%', '50%', '60%', '70%', '80%', '90%']\nbins = [0.400, 0.500, 0.600, 0.700, 0.800, 0.900, 1.000]\n\ntrain_data[\"city_development_percent\"] = pd.cut(train_data.city_development_index, labels=labels, bins=bins)\n\nb = train_data.groupby(by=['city_development_percent', 'target']).count()['enrollee_id']\n\n# Visualization for the same.\nb.unstack().plot(kind='bar', stacked=True, figsize=(14, 8), fontsize=16)\n\nplt.ylabel(\"Count\")\nplt.title(\"Observation of number of candidates according to City Development index.\")\n\nplt.xticks(rotation=0)\n\nplt.show()","4bfcd713":"\ndef encoder(data):\n    \n    # Label encoder.\n    cols = ['relevent_experience', 'enrolled_university', 'gender']\n    \n    for col in cols:\n        a = pd.get_dummies(data[col])\n        data = data.join(a)\n    \n    return data\n        \n\ndef fequency_encoder_calculator(data):\n    \n    # Frequency encoder for more than one categorical value features.\n    cols = ['city','major_discipline', 'company_size','company_type', 'last_new_job']\n    \n    for col in cols:\n        fe = data.groupby(col).size() \/ data.shape[0]\n        col_name = col + \"_fe\"\n        data[col_name] = data[col].map(fe)\n        \n        \n\ndef StandardScalar(data):\n    \n    # Normalizing data using Z-score scaling.\n    cols = data.columns\n        \n    for col in cols:\n        mean = data[col].mean()\n        std = data[col].std()\n        data.loc[:, col] = (data.loc[:, col] - mean) \/ std\n\n","20ab21d2":"# train_data \ntrain_data = encoder(train_data)\nfequency_encoder_calculator(train_data)\ntrain_data['education_level'].replace({'Primary School':0, 'High School':1, 'Graduate':2, 'Masters':3, 'Phd':4 }, inplace=True)","7f1979cf":"train_data.head()","9a4d2d8a":"train_data.set_index('enrollee_id', inplace=True)\ntrain_data.drop('city_development_percent', axis=1, inplace=True)\ndata = train_data.select_dtypes(exclude=object)\ndata.drop('target', axis=1, inplace=True)","f17d1a0b":"data.head()","04a7c134":"# standardization\ncols = ['experience', 'training_hours']\na = data[cols]\nStandardScalar(a)\n\nfor col in cols:\n    data[col] = a[col]\n    \ndata.head()","fd03e125":"y = train_data['target']","a40e7562":"# For imbalance dataset.\n\nsm = SMOTE(random_state=1)\nsm_data, sm_y = sm.fit_resample(data, y)","c962f492":"\ndef model_eval(model):\n    \n    print(model)\n    print(\"--\"*10)\n    \n    # fitting model\n    model.fit(X_train, y_train)\n    \n    # prediction\n    y_train_preds = model.predict(X_train)\n    y_test_preds = model.predict(X_test)\n    \n    # ROC accuracy score.\n    train_accuracy = roc_auc_score(y_train, y_train_preds)\n    test_accuracy = roc_auc_score(y_test, y_test_preds)\n    \n    print(\"Train Accuracy: \", train_accuracy)\n    print(\"Test Accuracy: \", test_accuracy)\n    \n    \n    # Overall Accuracy of Model.\n    score = accuracy_score(y_test, y_test_preds)\n    print(\"Accuracy score: \", score)\n    print(\"--\"*10)\n    \n    return y_test_preds\n    \n    \n# spliting data into to train and test sets.\nX_train, X_test, y_train, y_test = train_test_split(sm_data, sm_y, test_size=0.30, random_state=0)\n\n# LogisticRegression\nLR_clf = LogisticRegression(random_state=0)\n\n# GradientBoostingClassifier\nGrd_clf = GradientBoostingClassifier(learning_rate=0.05, n_estimators=500, random_state=0)\n\n# RandomForestClassifier\nRandom_clf = RandomForestClassifier(random_state=45)\n\n# prediction from each classifiers\nY_predictions = {}\n\nfor model in [LR_clf, Grd_clf, Random_clf]:\n    predicts = model_eval(model)\n    Y_predictions[str(model).split(\"(\")[0]] = predicts","727850cc":"# confusion matrix for test data with Random Forest Classifier.\n\nTP, FN, TN, FP = confusion_matrix(y_test, Y_predictions['RandomForestClassifier']).ravel()\n\nprecision = round((TP \/ (TP + TN)*100), 2)\nrecall = round((TP \/ (TP + FP)*100), 2)\n\nprint(\"Precision: \",precision)\nprint(\"Recall: \", recall)","c11ae09c":"from sklearn.metrics import plot_confusion_matrix\n\ndisp = plot_confusion_matrix(Random_clf, X_test, y_test, cmap=plt.cm.PuBu, normalize='true')\ndisp.ax_.set_title(\"Confusion matrix for \"+ str(Random_clf).split('(')[0])\nplt.show()","669a0cbf":"from sklearn.metrics import plot_roc_curve\n\ndisp = plot_roc_curve(Random_clf, X_test, y_test)\ndisp.ax_.set_title(\"ROC curve for \"+ str(Random_clf).split('(')[0])\nsns.lineplot([0, 1],[0, 1], color=\"r\", linestyle=\"--\", alpha=0.7)\nplt.show()","5fa2a89d":"# **HR Analytics: Job Change of Data Scientists**\n\n## **1. Context**\n\nA company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which conduct by the company. Many people signup for their training. Company wants to know which of these candidates are really wants to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates. Information related to demographics, education, experience are in hands from candidates signup and enrollment.\n\n\n## **2. Problem Statement**\n   * To understand the factors that lead a person to leave current job.\n   * Find a probability of candidate looking for a job.\n","a90e3bd1":"## **3. Train Data**","a4d96839":"Candidate with **less hours** of training are willing to change a job as compare to candidates with **more trainig hours**.","af137abe":"Graduate with **1** and **5** **years of difference** between last job and new jobs are **Looking for job change**.","2c8e6794":"**Higher experience is observed for higher education level(PhD).**","6772ff65":"## Model Evaluation","6b6b13a5":"## Data Normalization","61b521dd":"Candidates with **City_development_index** **between 0.60 and 0.70** are mostly ready to change the job.","5f2aa791":"Graduates who take a **full time course** are mostly ready to change the job.","c498c1c8":"**More than 20 years of experience is observed in PhD holders who does not specified their gender. Overall average experience of Female irrespective of their education is lower than Male.**","cd807d72":"**Most of the candidates are *Graduates*.**","8a764126":"Discription for **last_new_job** is Difference in years between previous job and current job. The **last_new_job** has value *never* So, let's change *`never`* into *`0`*.","0301abd8":"Most of the enrolled candidates have **1 year of difference** between their new job and previous job and these are mostly **Graduates** and **Masters** candidates. Candidates with 1 year of difference in new job and previous job **have relevent experience** in Data science.","047aed81":"## **7. Casting Datatypes**","31257584":"## **8. Data Visualization**","31715714":"## 5. Outliers","a183f406":"   \n## **4. Data Description**\n\n**Features**\n\n* **enrollee_id** : Unique ID for candidate\n* **city: City code**\n* **city_ development _index** : Developement index of the city (scaled)\n* **gender**: Gender of candidate\n* **relevent_experience**: Relevant experience of candidate\n* **enrolled_university**: Type of University course enrolled if any\n* **education_level**: Education level of candidate\n* **major_discipline**:Education major discipline of candidate\n* **experience**: Candidate total experience in years\n* **company_size**: No of employees in current employer's company\n* **company_type** : Type of current employer\n* **lastnewjob**: Difference in years between previous job and current job\n* **training_hours**: training hours completed\n* **target**: 0 \u2013 Not looking for job change, 1 \u2013 Looking for a job change\n","0b63eadf":"Most of the candidates with  **Graduates** and **Masters** degree have a **relevent experience** in Data Science as compare to other Education levels. ","98a7773e":"## **6. Missing Data**","fccad810":"Of course, Candidates with **0** experience are Looking for Job and Candidates with **More experience** are less willing to change a job or **not looking for job**.","d700ef4a":"There is correlation between **city_development_index** and **Experience**, **last_new_job**, **target** features and **last_new_job** is correlated to the **experience** as well.","ccc9f3e0":"**As expected, more candidates are from *Science, Technology Engieering and Mathamatics* (STEM) background Since this training is for Data Science role.**"}}