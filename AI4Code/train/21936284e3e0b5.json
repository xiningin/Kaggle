{"cell_type":{"5ab9c92f":"code","a1bdf835":"code","b993e7b5":"code","8d90af59":"code","783beefc":"code","2bcd4b90":"code","622095f7":"code","2c3d1026":"code","a81f806f":"code","3718c306":"code","b763723a":"code","078f0236":"code","2353fdca":"code","564703dd":"code","43645637":"code","85a4dcca":"code","f6c9ac9d":"code","c9949f5b":"code","48235ac4":"code","49b247b8":"code","ad1291e7":"code","b886cc1c":"code","db20a489":"code","67abc413":"code","2704e462":"code","2d0d35d0":"code","c1d8e859":"code","a6a3f0e9":"code","cab6c00c":"code","7934bd6d":"code","d21818f9":"code","423c9a25":"code","671d0738":"code","552ceecb":"code","b2bdb31b":"code","1e710099":"code","88e72697":"code","bac0b584":"code","2431464b":"code","be99207f":"code","5315a5e5":"code","fe9f19ab":"code","59cca861":"code","9549210c":"code","195d3ed8":"code","9d346a0e":"code","59f1e5ee":"code","79d4baa7":"code","9e03659e":"code","1d4a55f5":"code","28d2e215":"markdown","3765c357":"markdown","9e25907e":"markdown","52310bdd":"markdown","e3c0f425":"markdown","7dabee5d":"markdown","649a9855":"markdown","d127cf58":"markdown","b163f61f":"markdown","fc4fda8f":"markdown","77fcfdab":"markdown","1aa01b88":"markdown","61934f47":"markdown","72fc72d5":"markdown","2d1a2d59":"markdown","63e4eb95":"markdown","7cfb1891":"markdown","a5037768":"markdown","ae044cc8":"markdown","d2175056":"markdown","1b2cd6b6":"markdown","f157b449":"markdown","d6c0aeb7":"markdown"},"source":{"5ab9c92f":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np \nimport pandas as pd \n\n# Visualization\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import parallel_coordinates\nimport eli5\n\n# Modeling\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport lightgbm as lgbm\nimport xgboost as xgb\n\n# Metrics\nfrom sklearn.metrics import r2_score\n\npd.set_option('max_columns',100)","a1bdf835":"# Download data\ndf = pd.read_csv('..\/input\/ammonium-prediction-in-river-water\/PB_1996_2019_NH4.csv', sep=';')\ndf.head(3)","b993e7b5":"# Date group by months\ndf['date'] = pd.to_datetime(df['Date'], format='%d.%m.%Y', errors='coerce').dt.to_period('m')\ndf","8d90af59":"# Selection the main data\ndf = df[['ID_Station','date','NH4']]\ndf.head(3)","783beefc":"# Dataset transformation\ndf_id_list = df['ID_Station'].unique().tolist()\ndf_id_str_list = [str(x) for x in df_id_list]\ndf = pd.pivot_table(df, values='NH4', index=['date'], columns='ID_Station')\ndf.columns = df_id_str_list\ndf = df.reset_index(drop=False)\ndf.head(3)","2bcd4b90":"df.info()","622095f7":"# Selection stations with the biggest length of data series (more 80% from all dates)\ncol2 = []\nfor col in df_id_str_list:\n    if len(df[col]) - df[col].isna().sum() > 0.6*len(df):\n        col2.append(col)\ndf2 = df[['date'] + col2]\ndf2.info()","2c3d1026":"df2 = df2.dropna().reset_index(drop=True)\ndf2.info()","a81f806f":"df2","3718c306":"df2[col2].plot(figsize=(16,8))","b763723a":"df2[['16','35']].plot(figsize=(16,3))","078f0236":"df2[col2].mean()","2353fdca":"col3 = col2.copy()\ncol3.remove('16')\ncol3.remove('35')\ndf3 = df2[['date'] + col3]\ndf3[col3].plot(figsize=(16,8))","564703dd":"target_name = '29'","43645637":"# We have numerical data only\n\n# # Encoding categorical features\n# numerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n# categorical_columns = []\n# features = train.columns.values.tolist()\n# for col in features:\n#     if train[col].dtype in numerics: continue\n#     categorical_columns.append(col)\n# for col in categorical_columns:\n#     if col in train.columns:\n#         le = LabelEncoder()\n#         le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n#         train[col] = le.transform(list(train[col].astype(str).values))\n#         test[col] = le.transform(list(test[col].astype(str).values)) ","85a4dcca":"# Get data\ndata = df3[col3]\ntarget = data.pop(target_name)","f6c9ac9d":"# Get train abd test data\ntrain, test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=0)","c9949f5b":"train","48235ac4":"test.info()","49b247b8":"#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target_train, test_size=0.2, random_state=0)\ntrain_set = lgbm.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgbm.Dataset(Xval, Zval, silent=False)","ad1291e7":"# Tuning LGB model\n# See parameters in the documentation https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression', # for classification task - \"binary\" or other\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 50 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',     # eval_metric, for classification task - \"binary\" or other\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 2,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=2000,\n                   early_stopping_rounds=10, verbose_eval=10, valid_sets=valid_set)","b886cc1c":"# FI diagram drawing\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","db20a489":"# FI diagram saving\nfeature_score = pd.DataFrame(train.columns, columns = ['feature']) \nfeature_score['LGB'] = modelL.feature_importance()","67abc413":"# Prediction\ny_train_lgb = modelL.predict(train, num_iteration=modelL.best_iteration)\ny_preds_lgb = modelL.predict(test, num_iteration=modelL.best_iteration)","2704e462":"#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\ndata_train = xgb.DMatrix(train)\ndata_test  = xgb.DMatrix(test)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]","2d0d35d0":"# Tuning XGB model\n# See parameters in the documentation https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html\nparms = {'max_depth':5, # maximum depth of a tree\n         'objective':'reg:squarederror', # for classification task - \"reg:logistic\" or other\n         'eval_metric':'rmse',      # for classification task - \"error\" or other\n         'learning_rate':0.01,\n         'subsample':0.8, # SGD will use this percentage of data\n         'colsample_bylevel':0.9,\n         'min_child_weight': 2,\n         'seed': 0}\nmodelx = xgb.train(parms, data_tr, num_boost_round=2000, evals = evallist,\n                  early_stopping_rounds=300, maximize=False, \n                  verbose_eval=100)\n\nprint('score = %1.5f, n_boost_round =%d.'%(modelx.best_score,modelx.best_iteration))","c1d8e859":"# FI diagram drawing\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","a6a3f0e9":"# FI diagram saving\nfeature_score['XGB'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))","cab6c00c":"# Prediction\ny_train_xgb = modelx.predict(data_train)\ny_preds_xgb = modelx.predict(data_test)","7934bd6d":"# Standardization for regression models\nScaler_train = preprocessing.MinMaxScaler().fit(train)\ntrain = pd.DataFrame(Scaler_train.transform(train), columns=train.columns, index=train.index)\ntest = pd.DataFrame(Scaler_train.transform(test), columns=test.columns, index=test.index)","d21818f9":"# Linear Regression Tuning\nlinreg = LinearRegression()\nlinreg.fit(train, target_train)","423c9a25":"# FI diagram drawing\ncoeff_linreg = pd.DataFrame(train.columns)\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"LinRegress\"] = pd.Series(linreg.coef_)\ncoeff_linreg.sort_values(by='LinRegress', ascending=False)","671d0738":"# Eli5 visualization\neli5.show_weights(linreg)","552ceecb":"# FI diagram saving\ncoeff_linreg[\"LinRegress\"] = coeff_linreg[\"LinRegress\"].abs()\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\nfeature_score","b2bdb31b":"# Prediction\ny_train_linreg = linreg.predict(train)\ny_preds_linreg = linreg.predict(test)","1e710099":"# MinMax scaling all feature importances\nfeature_score = pd.DataFrame(\n    preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns,\n    index=feature_score.index\n)\n\n# Create mean column\nfeature_score['Mean'] = feature_score.mean(axis=1)","88e72697":"# Merging FI diagram\n\n# Set weight of models\nw_lgb = 0.4\nw_xgb = 0.5\nw_linreg = 1 - w_lgb - w_xgb\nw_linreg\n\n# Create merging column with different weights\nfeature_score['Merging'] = w_lgb*feature_score['LGB'] + w_xgb*feature_score['XGB'] + w_linreg*feature_score['LinRegress']\nfeature_score.sort_values('Merging', ascending=False)","bac0b584":"# Plot the feature importances\nplot_title = \"Feature Importance - Advanced Visualization with Matplotlib\"\nfeature_score.sort_values('Merging', ascending=False).plot(kind='bar', figsize=(20, 10), title = plot_title)","2431464b":"def plot_feature_parallel(df, title):\n    # Draw Pandas.parallel_coordinates for features of the given df\n    \n    plt.figure(figsize=(15,12))\n    parallel_coordinates(df, 'feature', colormap=plt.get_cmap(\"tab20c\"), lw=3)\n    plt.title(title)\n    plt.xlabel(\"Models\")\n    plt.ylabel(\"Feature importance\")\n    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n    plt.savefig('graph.png')\n    plt.show()","be99207f":"# List of models\nfeature_score_columns = feature_score.columns\nfeature_score_columns","5315a5e5":"feature_score = feature_score.reset_index(drop=False)\nplot_feature_parallel(feature_score, f\"All Feature Importance - Advanced Visualization\")","fe9f19ab":"feature_score","59cca861":"def features_selection_by_weights(df, threshold):\n    # Selection features with weights more threshold at least in a one column (model)\n\n    features_list = df.feature.tolist()\n    features_best = []\n    for i in range(len(df)):\n        feature_name = features_list[i]\n        feature_is_best = False\n        for col in feature_score_columns:\n            if df.loc[i, col] > threshold:\n                feature_is_best = True\n        if feature_is_best:\n            features_best.append(feature_name)\n    \n    return df[df['feature'].isin(features_best)].reset_index(drop=True)","9549210c":"# Selection the best features\nthreshold_fi = 0.25\nfeature_score_best = features_selection_by_weights(feature_score, threshold_fi)\nfeature_score_best","195d3ed8":"plot_feature_parallel(feature_score_best, f\"All Feature Importance of the best of features - Advanced Visualization\")","9d346a0e":"# Target for test data\ntarget_test[:10].values","59f1e5ee":"# Mean solution\ny_train_mean = (y_train_lgb + y_train_xgb + y_train_linreg)\/3  # for training data\ny_preds_mean = (y_preds_lgb + y_preds_xgb + y_preds_linreg)\/3  # for test data\ny_preds_mean[:10]","79d4baa7":"# Merging solutions\ny_train = w_lgb*y_train_lgb + w_xgb*y_train_xgb + w_linreg*y_train_linreg  # for training data\ny_preds = w_lgb*y_preds_lgb + w_xgb*y_preds_xgb + w_linreg*y_preds_linreg  # for test data\ny_preds[:10]","9e03659e":"def plot_data(target, y, y_mean, y_lgb, y_xgb, y_lr, title):\n    # Drawing plot with title and with with given target and predicted y\n    \n    def acc(y_pred):\n        return str(round(r2_score(target,y_pred),2))\n    \n    x = np.arange(len(target))\n    plt.figure(figsize=(16,10))\n    plt.scatter(x, target, label = \"Target data\", color = 'k', s=100)\n    plt.plot(x, y_lgb, label = f\"Model LGB forecast ({acc(y_lgb)})\", color = 'b')\n    plt.plot(x, y_xgb, label = f\"Model XGB forecast ({acc(y_xgb)})\", color = 'orange')\n    plt.plot(x, y_lr, label = f\"Model Linear Regression forecast ({acc(y_lr)})\", color = 'g')\n    plt.plot(x, y_mean, label = f\"Mean forecast ({acc(y_mean)})\", color = 'r')\n    plt.plot(x, y, label = f\"Merging forecasts ({acc(y)})\", color = 'purple')\n    plt.plot(x, np.full(len(target), 0.5), label = \"Maximum allowable value\", color = 'brown')\n    plt.title(title)\n    plt.legend(loc='best')\n    plt.grid(True)","1d4a55f5":"# Building plots\nplot_data(target_train, y_train, y_train_mean, y_train_lgb, y_train_xgb, y_train_linreg, 'Prediction for the training data (r2_score metrics)')\nplot_data(target_test, y_preds, y_preds_mean, y_preds_lgb, y_preds_xgb, y_preds_linreg, 'Prediction for the test data (r2_score metrics)')","28d2e215":"### 7.2 Pandas.parallel_coordinates <a class=\"anchor\" id=\"7.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","3765c357":"### 7.1 Matplotlib <a class=\"anchor\" id=\"7.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","9e25907e":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download datasets](#2)\n1. [FE & EDA](#3)\n1. [Preparing to modeling](#4)\n1. [Tuning models, building the feature importance diagrams and prediction](#5)\n    -  [LGBM](#5.1)\n    -  [XGB](#5.2)\n    -  [Linear Regression](#5.3)\n1. [Comparison and merging of all feature importance diagrams](#6)\n1. [Feature Importance - Advanced Visualization](#7)\n    -  [Matplotlib](#7.1)\n    -  [Pandas.parallel_coordinates](#7.2)\n1. [Analysis of data forecasting results](#8)","52310bdd":"## 4. Preparing to modeling <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","e3c0f425":"Your comments and feedback are most welcome.","7dabee5d":"## 6. Comparison and merging of all feature importance diagrams <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","649a9855":"<a class=\"anchor\" id=\"0\"><\/a>\n\n# The importance of all features in different models - Advanced Visualization with Matplotlib and Pandas.parallel_coordinates\n## Feature Importance diagrams of 3 models (XGB, LGB, LinReg) and the solution as weighted average of its\n## The code is universal for both the Classification and the Regression tasks\n### For the example for the my dataset [\"Ammonium prediction in river water\"](https:\/\/www.kaggle.com\/vbmokin\/ammonium-prediction-in-river-water)","d127cf58":"## 7. Feature Importance - Advanced Visualization <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","b163f61f":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","fc4fda8f":"## 3. FE & EDA <a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","77fcfdab":"### 5.1 LGBM <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","1aa01b88":"[Go to Top](#0)","61934f47":"### 5.2 XGB<a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","72fc72d5":"## 8. Analysis of data forecasting results<a class=\"anchor\" id=\"8\"><\/a>\n\n[Back to Table of Contents](#0.1)","2d1a2d59":"Data for stations '16' and '35' are very small and very differ from others. Remove it.","63e4eb95":"## 5. Tuning models, building the feature importance diagrams and prediction<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","7cfb1891":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","a5037768":"Then you can remove insignificant features or decide to change the weights of the models' solutions, or you can first find out what accuracy the previously selected weights will give, and then experiment with their options.","ae044cc8":"I hope you find this kernel useful and enjoyable.","d2175056":"Let's try to predict the data of the last station 29 based on data from other stations (located upstream of this river).","1b2cd6b6":"This based on my notebooks:\n* [Merging FE & Prediction - xgb, lgb, logr, linr](https:\/\/www.kaggle.com\/vbmokin\/merging-fe-prediction-xgb-lgb-logr-linr)\n* [FE - Feature Importance - Advanced Visualization](https:\/\/www.kaggle.com\/vbmokin\/fe-feature-importance-advanced-visualization)","f157b449":"The analysis showed that:\n1. The average solution (\"Mean\") is better than the combined with certain weights (\"Merging\"), which means that those weights were chosen unsuccessfully and need to be changed.\n2. Accuracy (r2_score) of models on the test dataset is very bad, which means that the models still need to be improved or make them more complex ensembles.","d6c0aeb7":"### 5.3 Linear Regression <a class=\"anchor\" id=\"5.3\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}