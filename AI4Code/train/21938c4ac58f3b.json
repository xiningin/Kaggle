{"cell_type":{"0291236a":"code","de3984ce":"code","51ee3457":"code","056424cf":"code","2dc34ba7":"code","4e7b48eb":"code","05711876":"code","27203bb6":"code","502ef8e8":"code","e88f9ad5":"code","755c8d23":"code","bb40a58e":"code","b9fe94c4":"code","7f531c04":"code","249e7530":"code","fc4826b6":"code","0c982308":"code","a563bf69":"code","d5f74544":"code","f7afc9a2":"code","43f11700":"code","2986e83e":"code","db340415":"code","47609ac4":"code","0866ae0e":"markdown","da3e4a6c":"markdown","80f29bd1":"markdown","d420670d":"markdown","c5dfaffc":"markdown","3d307f95":"markdown","a978097c":"markdown","f76c91c7":"markdown","fc1007fa":"markdown","ef43430a":"markdown","667704ee":"markdown","3a90681e":"markdown","9c8f6de9":"markdown","c6e46d5f":"markdown","0c1758ac":"markdown","d9cc96c7":"markdown","052553ca":"markdown"},"source":{"0291236a":"# for using regular expressiion\nimport re\n\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \n\n#for plotting seaborn plots\nimport seaborn as sns\n\n#The string module contains a number of useful constants and classes\nimport string\n\n#NLTK is a leading platform for building Python programs to work \n#with human language data. It provides easy-to-use interfaces to \n#over 50 corpora and lexical resources such as WordNet, \n#along with a suite of text processing libraries for \n#classification, tokenization, stemming, tagging, parsing, \n#and semantic reasoning, wrappers for industrial-strength NLP \n#libraries, and an active discussion forum.\n\nimport nltk\n\nimport warnings \nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\n#When using the 'inline' backend, so that matplotlib graphs will be included in notebook,\n%matplotlib inline","de3984ce":"train  = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","51ee3457":"train.head()","056424cf":"combi = train.append(test, ignore_index=True)","2dc34ba7":"#a user-defined function to remove unwanted text patterns from the tweets.\n#inputs -- two arguments, one is the original string of text and the other is the pattern of text that we want to remove from the string.\n#outputs - -returns the same input string but without the given pattern. We will use this function to remove the pattern \u2018@user\u2019 from all the tweets in our data.\n\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i,'', input_txt)\n        \n    return input_txt ","4e7b48eb":"# remove twitter handles (@user)\ncombi['tidy_tweet'] = np.vectorize(remove_pattern)(combi['tweet'], \"@[\\w]*\")","05711876":"# checking the changes in the data\ncombi.head()","27203bb6":"# remove special characters, numbers, punctuations\ncombi['tidy_tweet'] = combi['tidy_tweet'].str.replace(\"[^a-zA-Z#]\", \" \")","502ef8e8":"combi['tidy_tweet'] = combi['tidy_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))","e88f9ad5":"# lets see the changes in the dataset\ncombi.head()","755c8d23":"tokenized_tweet = combi['tidy_tweet'].apply(lambda x: x.split())\ntokenized_tweet.head()","bb40a58e":"from nltk.stem.porter import *\nstemmer = PorterStemmer()\n\ntokenized_tweet = tokenized_tweet.apply(lambda x: [stemmer.stem(i) for i in x])    # stemming\n\n\ntokenized_tweet.head()","b9fe94c4":"#snitch these tokens back togather\nfor i in range(len(tokenized_tweet)):\n    tokenized_tweet[i] = ' '.join(tokenized_tweet[i])\n\n\ncombi['tidy_tweet'] = tokenized_tweet","7f531c04":"# common words\n\nall_words = ' '.join([text for text in combi['tidy_tweet']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\n\nplt.figure(figsize=(14, 9))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","249e7530":"## non resisit or normal words\n\nnormal_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 0]])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n\nplt.figure(figsize=(14,9))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","fc4826b6":"# rasisit or negative words\n\nnegative_words =' '.join([text for text in combi['tidy_tweet'][combi['label'] == 1]])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\n\nplt.figure(figsize=(14,9))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","0c982308":"# function to collect hashtags\ndef hashtag_extract(x):\n    hashtags = []\n    # Loop over the words in the tweet\n    for i in x:\n        ht = re.findall(r\"#(\\w+)\", i)\n        hashtags.append(ht)\n\n    return hashtags","a563bf69":"\n# extracting hashtags from non racist\/sexist tweets\n\nHT_regular = hashtag_extract(combi['tidy_tweet'][combi['label'] == 0])\n\n# extracting hashtags from racist\/sexist tweets\nHT_negative = hashtag_extract(combi['tidy_tweet'][combi['label'] == 1])\n\n# unnesting list\nHT_regular = sum(HT_regular,[])\nHT_negative = sum(HT_negative,[])","d5f74544":"a = nltk.FreqDist(HT_regular)\nd = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n\n# selecting top 20 most frequent hashtags     \nd = d.nlargest(columns=\"Count\", n = 20) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=d, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","f7afc9a2":"b = nltk.FreqDist(HT_negative)\ne = pd.DataFrame({'Hashtag': list(a.keys()),\n                  'Count': list(a.values())})\n# selecting top 10 most frequent hashtags   \n\n\ne = e.nlargest(columns=\"Count\", n = 10) \nplt.figure(figsize=(16,5))\nax = sns.barplot(data=e, x= \"Hashtag\", y = \"Count\")\nax.set(ylabel = 'Count')\nplt.show()","43f11700":"# Applying Bag-of-words\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# bag-of-words feature matrix\nbow = bow_vectorizer.fit_transform(combi['tidy_tweet'])","2986e83e":"# Applying TF-IDF \n\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n# TF-IDF feature matrix\ntfidf = tfidf_vectorizer.fit_transform(combi['tidy_tweet'])","db340415":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\n\ntrain_bow = bow[:31962,:]\ntest_bow = bow[31962:,:]\n\n# splitting data into training and validation set\nxtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train['label'], random_state=42, test_size=0.3)\n\nlreg = LogisticRegression()\nlreg.fit(xtrain_bow, ytrain) # training the model\n\nprediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\nprediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int) # calculating f1 score\n\n","47609ac4":"# Building model using TF-IDF features\ntrain_tfidf = tfidf[:31962,:]\ntest_tfidf = tfidf[31962:,:]\n\nxtrain_tfidf = train_tfidf[ytrain.index]\nxvalid_tfidf = train_tfidf[yvalid.index]\n\nlreg.fit(xtrain_tfidf, ytrain)\n\nprediction = lreg.predict_proba(xvalid_tfidf)\nprediction_int = prediction[:,1] >= 0.3\nprediction_int = prediction_int.astype(np.int)\n\nf1_score(yvalid, prediction_int)","0866ae0e":"### removing short words (0f <3 characters)\n","da3e4a6c":"### plotting negative hashtags","80f29bd1":"### For convenience only, let\u2019s first combine train and test set. \n\n### This saves the trouble of performing the same steps twice on test and train.\n","d420670d":"### Removing Punctuations, Numbers, and Special Characters","c5dfaffc":"## Model Building: Sentiment Analysis\n\n\n### using Bag-of-words","3d307f95":"### A wordcloud is a visualization wherein the most frequent words appear in large size and the less frequent words appear in smaller sizes.","a978097c":"## Story generation and visualization from tweets\ntry to answer \n\nthese questions\n\n\nWhat are the most common words in the entire dataset?\n\n\nWhat are the most common words in the dataset for negative and positive tweets, respectively?\n\n\nHow many hashtags are there in a tweet?\n\n\nWhich trends are associated with my dataset?\n\n\nWhich trends are associated with either of the sentiments? Are they compatible with the sentiments?\n ","f76c91c7":"### Tokenization\n\ntokenize all the cleaned tweets in our dataset.\n\nTokens are individual terms or words, and tokenization is the process of splitting a string of text into tokens.","fc1007fa":"### Stemming\n\n\nStemming is a rule-based process of stripping the suffixes (\u201cing\u201d, \u201cly\u201d, \u201ces\u201d, \u201cs\u201d etc) from a word.","ef43430a":"# twiter sentiment analysis ","667704ee":"### impact of hashtag on tweets sentiment","3a90681e":"### loading data sets","9c8f6de9":"### using IF-IDF\n    ","c6e46d5f":"\nso the data has 3 columns id, label, and tweet. \n\nlabel is the binary target variable and tweet contains the tweets that is to be  cleaned and preprocessed\n\nso we will work on \n\n1.Removing Twitter Handles (@user)","0c1758ac":"### plotting positive hashtags","d9cc96c7":"## Removing Twitter Handles (@user)\n### we are going to use regular expression here","052553ca":"## Extracting features fromm cleaned words\nTo analyze a preprocessed data, it needs to be converted into features.\n\n\nDepending upon the usage, text features can be constructed using assorted \n\n\ntechniques \u2013 Bag-of-Words, TF-IDF, and Word Embeddings."}}