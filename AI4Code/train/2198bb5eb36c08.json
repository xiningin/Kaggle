{"cell_type":{"c87986dc":"code","b1f687c3":"code","25f63aa5":"code","9789b197":"code","61a942b7":"code","5e3e7a9a":"code","014d9bed":"code","00d1c5a4":"code","6bf81279":"code","e37bec0f":"code","c491b7aa":"code","aaad5125":"code","c45d5dfa":"code","70dd6ca4":"code","7d8f3d44":"code","d59040c4":"code","8e7c189e":"code","3dddf818":"code","afdb22c0":"code","2487b6d1":"code","92e8e6db":"code","e465b605":"code","6394c744":"code","dd5c166a":"code","ee5bc533":"code","a4a8a0dc":"code","3a101799":"code","2d44accb":"code","c8109c97":"code","f2ff57a0":"code","62d27210":"code","fcb20f91":"code","b3594834":"code","068b0c65":"code","cc7b32a3":"code","1b19b5af":"code","45308737":"code","0d227c73":"code","0409a4ab":"code","935e6862":"code","152fc875":"code","08861088":"code","a73336ce":"code","d196d52d":"code","de44aebe":"code","1646c98e":"code","356a50f7":"code","1a288748":"code","b7015257":"code","9bc8707c":"code","5e73df27":"code","155da923":"code","3e903a9e":"code","d5f51bfe":"code","2b0771a4":"code","7e4013e5":"code","02c2a298":"code","2365b1f3":"code","5caf3142":"code","0824bca1":"code","532da3e9":"code","1ccfe447":"code","02b3ffdd":"code","7c2124c3":"code","76bac45d":"code","bccf17bc":"code","d9a1c34c":"code","3d0ee52a":"code","58dd7c6d":"code","250f1087":"code","133b5520":"code","a8c2c0f7":"code","5aa7e5fb":"code","541fbbd2":"code","e2426637":"code","2bd6aefc":"code","40a49c25":"code","7f9b610f":"code","29727f01":"code","b79f0c3f":"code","b5d709bb":"code","785d0337":"code","8f02271b":"code","1e93e670":"code","bed1dc4a":"code","ce9c34de":"code","71e60257":"code","467ce00f":"code","890a1bba":"code","2728b8b6":"code","b18c6809":"code","8967e411":"code","444faed1":"code","a8775e0e":"code","13546d85":"code","43eed3cb":"code","2a3a1989":"code","6a08726e":"code","974dc6e1":"code","af693e45":"code","eb95f1c8":"code","ad947eb8":"code","61eae7f8":"code","35bad051":"code","ee19ce49":"code","ebe95712":"code","38d91a40":"code","d681b8c5":"code","8fdd2149":"code","c30d6f37":"code","8e74e545":"code","e8dad5f6":"code","577745ec":"code","0fa438e4":"code","dc3b94b8":"code","14ff8d5f":"code","8d0eb3f4":"code","73ef0ed3":"code","1bc8f422":"code","e2ac1222":"code","141994ff":"code","55c65fb2":"code","048e6c82":"code","61e58080":"code","e08ded5d":"code","e7bd04f6":"code","a6851cd8":"code","8dc8af3d":"code","b6617975":"code","7b8820be":"code","0e5552f9":"code","0997713f":"code","514c0fbd":"code","bf384b8d":"code","6eefcaff":"code","1511a2c9":"code","5471fbed":"markdown","079d6cb6":"markdown","0024cf5a":"markdown","ba491f4e":"markdown","52b9c480":"markdown","dd779157":"markdown","7b3a06e6":"markdown","f5673bfc":"markdown","76a8a196":"markdown","8bc73b03":"markdown","50c162d2":"markdown","e0e60ac9":"markdown","f7fd6c82":"markdown","82c1f67d":"markdown","45c1dbbb":"markdown","375f575d":"markdown","c53610d9":"markdown","87ba2680":"markdown","ba278032":"markdown","8b55c315":"markdown","473f5e0f":"markdown","d232113e":"markdown","eb8fe6c6":"markdown","a89ff7e3":"markdown","6634b36f":"markdown","614464aa":"markdown","f0759a28":"markdown","c1583bd4":"markdown","b435bbc2":"markdown","d61a79d4":"markdown","816183a3":"markdown","735f662e":"markdown","142cf6eb":"markdown","08108434":"markdown","c5939154":"markdown","00a3407d":"markdown","8f85d56f":"markdown"},"source":{"c87986dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b1f687c3":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')","25f63aa5":"df = pd.read_csv('\/kaggle\/input\/bank-marketing\/bank-additional-full.csv', delimiter=';')","9789b197":"df.head()","61a942b7":"df.info()","5e3e7a9a":"var_categorical = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"day_of_week\", \n                  \"poutcome\"]\nvar_numerical = [\"age\", \"duration\", \"campaign\", \"pdays\", \"previous\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\",\n                \"euribor3m\", \"nr.employed\"]\nprint(len(var_categorical), len(var_numerical))","014d9bed":"df.replace('unknown', np.NaN, inplace=True)","00d1c5a4":"df.isnull().sum()\/len(df)*100","6bf81279":"# Number of distinct values in variables\nfor i, column in enumerate(df.columns):\n    print(\"{}. \".format(i) + str(column.title()) + \": {}\". format(df[column].nunique()))","e37bec0f":"# Function to label the count on top of each bar in graph\ndef label_values(ax, spacing=5):\n    total = 0\n    for rect in ax.patches:\n        total += rect.get_height()\n\n    for rect in ax.patches:\n        y_value = rect.get_height()\n        x_value = rect.get_x() + rect.get_width() \/ 2\n\n        space = spacing\n        \n        va = 'bottom'\n        \n        if y_value < 0:\n            space *= -1\n            va = 'top'\n        label = \"{:.2f}, {:.2f}\".format(y_value, y_value\/total*100)\n        ax.annotate(\n            label,                      \n            (x_value, y_value),         \n            xytext=(0, space),          \n            textcoords=\"offset points\", \n            ha='center',                \n            va=va)                      ","c491b7aa":"ax = sns.countplot(x = df[\"y\"])  #Imbalanced dataset\nlabel_values(ax, spacing=-15)\nplt.show()","aaad5125":"for column in var_categorical:\n    plt.figure(figsize=(15, 6))\n    print(column.title())\n    ax = sns.countplot(x = df[column])\n    label_values(ax)\n    plt.show()","c45d5dfa":"for column in var_categorical:\n    plt.figure(figsize=(15, 6))\n    print(column.title())\n    ax = sns.countplot(x = df[column], hue=df[\"y\"])\n    label_values(ax)\n    plt.show()","70dd6ca4":"plt.figure(figsize=(15, 6))\nprint(column.title())\nax = sns.countplot(x = df[\"poutcome\"], hue=df[\"y\"])\nlabel_values(ax)\nplt.show()","7d8f3d44":"# Default variable has no impact on the client subscribing for term deposit.\ndf.drop([\"default\"], axis = 1, inplace=True)","d59040c4":"var_categorical = list(set(var_categorical) - set(['default']))","8e7c189e":"df.dropna(inplace=True)","3dddf818":"df.info()","afdb22c0":"#### Drop the duration (as Important note:  this attribute highly affects the output target (e.g., if duration=0 then y=\"no\"). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.)\nduration = df[\"duration\"]\ndf.drop([\"duration\"], axis = 1, inplace=True)","2487b6d1":"var_numerical = list(set(var_numerical) - set(['duration']))","92e8e6db":"plt.figure(figsize=(15, 14))\nsns.heatmap(df.corr(), annot=True, cmap='Spectral')\nplt.show()","e465b605":"sns.heatmap(df[[\"emp.var.rate\", \"nr.employed\", \"euribor3m\"]].corr(), annot=True, cmap='Spectral')\nplt.show()","6394c744":"df.drop(['emp.var.rate', 'nr.employed'], axis = 1, inplace=True)","dd5c166a":"var_numerical = list(set(var_numerical) - set(['emp.var.rate', 'nr.employed']))","ee5bc533":"i = 1\nfor column in var_numerical:\n    print(column.title())\n    plt.subplots(figsize=(16, 35))\n    plt.subplot(len(var_numerical) + 1, 3, i)\n    sns.boxplot(y = df[column])\n    i += 1\n    plt.subplot(len(var_numerical) + 1, 3, i)\n    sns.distplot(x = df[column])\n    i += 1\n    plt.subplot(len(var_numerical) + 1, 3, i)\n    sns.boxplot(x = df[\"y\"], y = df[column])\n    i += 1\n    plt.show()","a4a8a0dc":"df[var_numerical].describe()","3a101799":"print(df[\"campaign\"].quantile(0.05), len(df[df[\"campaign\"]<df[\"campaign\"].quantile(0.05)] ))\nprint(df[\"campaign\"].quantile(0.95), len(df[df[\"campaign\"]>df[\"campaign\"].quantile(0.95)] ))","2d44accb":"df1 = df[(df[\"campaign\"] > 23) & (df[\"y\"] == 'yes')]\nlen(df1[['campaign', 'y']])","c8109c97":"df1 = df[(df[\"campaign\"] > 8) & (df[\"y\"] == 'yes')]\nprint(len(df1[['campaign', 'y']]), len(df[df[\"campaign\"]>8]))","f2ff57a0":"# We will convert all the values above 10 in campaign to 10\ndf[\"campaign\"] = df[\"campaign\"].apply(lambda x: 8 if x > 8 else x)","62d27210":"print(df[\"age\"].quantile(0.05), len(df[df[\"age\"]<df[\"age\"].quantile(0.05)] ))\nprint(df[\"age\"].quantile(0.95), len(df[df[\"age\"]>df[\"age\"].quantile(0.95)] ))","fcb20f91":"print(df[\"previous\"].quantile(0.05), len(df[df[\"previous\"]<df[\"previous\"].quantile(0.05)] ))\nprint(df[\"previous\"].quantile(0.95), len(df[df[\"previous\"]>2] ))","b3594834":"for i in range(1, max(df[\"previous\"])+1):\n    print(i, end = \" \")\n    df1 = df[(df[\"previous\"] >= i) & (df[\"y\"] == 'yes')]\n    print(len(df1[['previous', 'y']]), len(df[df[\"previous\"]>= i]))","068b0c65":"# We will convert all the values above 2 in previous to 2\ndf[\"previous\"] = df[\"previous\"].apply(lambda x: 2 if x >= 2 else x)","cc7b32a3":"print(df[\"pdays\"].quantile(0.05), len(df[df[\"pdays\"]<df[\"pdays\"].quantile(0.05)] ))\nprint(df[\"pdays\"].quantile(0.95), len(df[df[\"pdays\"]>df[\"pdays\"].quantile(0.95)] ))","1b19b5af":"df1 = df[(df[\"pdays\"] >= 999) & (df[\"y\"] == 'yes')]\nprint(len(df1[['pdays', 'y']]), len(df[df[\"pdays\"]>=999]))\ndf1 = df[(df[\"pdays\"] < 999) & (df[\"y\"] == 'yes')]\nprint(len(df1[['pdays', 'y']]), len(df[df[\"pdays\"]<999]))\ndf1 = df[(df[\"pdays\"] < 28) & (df[\"y\"] == 'yes')]\nprint(len(df1[['pdays', 'y']]), len(df[df[\"pdays\"]<28]))\ndf1 = df[(df[\"pdays\"] < 20) & (df[\"y\"] == 'yes')]\nprint(len(df1[['pdays', 'y']]), len(df[df[\"pdays\"]<20]))\ndf1 = df[(df[\"pdays\"] < 10) & (df[\"y\"] == 'yes')]\nprint(len(df1[['pdays', 'y']]), len(df[df[\"pdays\"]<10]))","45308737":"# Look into the pdays values which are less than 999\ndf1 = df[~(df[\"pdays\"] >= 999)]\nsns.boxplot(x = df1[\"y\"], y = df1['pdays'])\nplt.show()","0d227c73":"# visualizing numerical columns\nsns.pairplot(hue='y', data = df)\nplt.show()","0409a4ab":"df.drop(['campaign'], axis = 1, inplace=True)\nvar_numerical = list(set(var_numerical) - set(['campaign']))","935e6862":"df.isnull().sum()\/len(df)*100","152fc875":"print(len(var_categorical), var_categorical)\nprint(len(var_numerical), var_numerical)","08861088":"print(df[\"loan\"].value_counts())\nprint(df[\"housing\"].value_counts())\nprint(df[\"contact\"].value_counts())\nprint(df[\"y\"].value_counts())","a73336ce":"varlist = ['loan', 'housing', 'y']\n\ndf[varlist] = df[varlist].apply(lambda x: x.map({'yes':1, 'no': 0}))","d196d52d":"# visualizing numerical columns\nsns.pairplot(hue='y', data = df)\nplt.show()","de44aebe":"sns.displot(x = 'age', hue='y', data=df)\nplt.show()","1646c98e":"plt.figure(figsize=(15, 14))\nsns.heatmap(df.corr(), annot=True, cmap='Spectral')\nplt.show()","356a50f7":"# Get the dummy variables for contact, poutcome , job, month, marital, day_of_week, education\n# Let's drop the first column from  using 'drop_first = True' as first column can be derived using other columns \n# (dropping redundant column)\n\ncontact = pd.get_dummies(df.contact, drop_first=True)\npoutcome = pd.get_dummies(df.poutcome, drop_first=True)\njob = pd.get_dummies(df.job, drop_first=True)\nmonth = pd.get_dummies(df.month, drop_first=True)\nmarital = pd.get_dummies(df.marital, drop_first=True)\nday_of_week = pd.get_dummies(df.day_of_week, drop_first=True)\neducation = pd.get_dummies(df.education, drop_first=True)","1a288748":"# Concat the dummy columns to the dataframe\ndf = pd.concat([df, contact, poutcome , job, month, marital, day_of_week, education], axis=1)\ndf.head()","b7015257":"# Dropping contact, poutcome , job, month, marital, day_of_week, education as we have concatenated the dummies in our original dataframe\ndf.drop(['contact', 'poutcome' , 'job', 'month', 'marital', 'day_of_week', 'education'], axis = 1, inplace = True)\ndf.head()","9bc8707c":"df.columns, len(df.columns)","5e73df27":"plt.figure(figsize=(16, 16))\nsns.heatmap(df.corr(), annot=True, cmap='Spectral')\nplt.show()","155da923":"sns.heatmap(df[['success', 'nonexistent', 'previous', 'pdays']].corr(), annot=True, cmap='Spectral')\nplt.show()","3e903a9e":"df.drop(['previous', 'pdays'], axis = 1, inplace = True)","d5f51bfe":"var_numerical = list(set(var_numerical) - set(['previous', 'pdays']))","2b0771a4":"plt.figure(figsize=(16, 16))\nsns.heatmap(df.corr(), annot=True, cmap='Spectral')\nplt.show()","7e4013e5":"df.corr()['y']","02c2a298":"df.info()","2365b1f3":"from sklearn.model_selection import train_test_split","5caf3142":"# We will divide the training and testing set in 80% and 20% respectively\n# We used random_state = 100 so that everytime we run it we will have same set of training and testing set\ndf_train, df_test = train_test_split(df, train_size = 0.8, random_state = 100)","0824bca1":"print(\"Train Dataset: \"+ str(len(df_train)) + \" Test Dataset: \" + str(len(df_test)))\nprint(\"Train Target Label counts: \")\nprint(df_train[\"y\"].value_counts())\nprint(\"Test Target Label counts: \")\nprint(df_test[\"y\"].value_counts())","532da3e9":"from sklearn.preprocessing import MinMaxScaler, StandardScaler","1ccfe447":"# MinMaxScaler Object\nscaler = MinMaxScaler()","02b3ffdd":"# Fit and Transform the data\n# Fit will calculate our Min and Max values\n# Transform will operate on standardisation function and scales our values\ndf_train[var_numerical] = scaler.fit_transform(df_train[var_numerical])","7c2124c3":"# Transform will operate on standardisation function and scales our values\ndf_test[var_numerical] = scaler.transform(df_test[var_numerical])","76bac45d":"df_train.head()","bccf17bc":"df_test.head()","d9a1c34c":"df.info()","3d0ee52a":"# Divide the train data into X and y\n\ny_train = df_train.pop('y')\nX_train = df_train","58dd7c6d":"# Divide the test data into X and y\n\ny_test = df_test.pop('y')\nX_test = df_test","250f1087":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","133b5520":"from sklearn.metrics import accuracy_score, confusion_matrix","a8c2c0f7":"from sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV","5aa7e5fb":"from sklearn import metrics","541fbbd2":"# Draw ROC curve from training and test data probability\ndef draw_roc( train_actual, train_probs, test_actual, test_probs ):\n    train_fpr, train_tpr, train_thresholds = metrics.roc_curve( train_actual, train_probs,\n                                              drop_intermediate = False )\n    test_fpr, test_tpr, test_thresholds = metrics.roc_curve( test_actual, test_probs,\n                                              drop_intermediate = False )\n    train_auc_score = metrics.roc_auc_score( train_actual, train_probs )\n    test_auc_score = metrics.roc_auc_score( test_actual, test_probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( train_fpr, train_tpr, label='ROC curve (area = %0.2f)' % train_auc_score )\n    plt.plot( test_fpr, test_tpr, label='ROC curve (area = %0.2f)' % test_auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","e2426637":"# Train model using hyperparmeter tuning\ndef training_model_hyperparameter(model, scoring, params_grid, X_train, y_train):\n    folds = StratifiedKFold(n_splits = 10, shuffle = True, random_state=100)\n\n    grid = GridSearchCV(estimator = model, scoring=scoring, param_grid = params_grid, cv = folds, \n                           verbose=0, return_train_score=True, n_jobs=3)\n    grid.fit(X_train, y_train)\n    return grid","2bd6aefc":"# Predict values and propability of training and testing data\ndef prediction_model(model, X_train, y_train, X_test, y_test):\n    y_train_pred = model.predict(X_train)\n    y_train_pred_prob = model.predict_proba(X_train)[:, 1]\n    y_test_pred = model.predict(X_test)\n    y_test_pred_prob = model.predict_proba(X_test)[:, 1]\n    return y_train_pred, y_train_pred_prob, y_test_pred, y_test_pred_prob","40a49c25":"from sklearn.linear_model import LogisticRegression","7f9b610f":"logreg = LogisticRegression(solver='lbfgs', random_state = 100)\nlogreg = logreg.fit(X_train, y_train)","29727f01":"y_train_pred, y_train_pred_prob, y_test_pred,y_test_pred_prob= prediction_model(logreg, X_train, y_train, X_test, y_test)","b79f0c3f":"draw_roc(y_train, y_train_pred_prob, y_test, y_test_pred_prob)","b5d709bb":"print(\"Accuracy train: \", accuracy_score(y_train, y_train_pred))\nprint(\"Accuracy test: \", accuracy_score(y_test, y_test_pred))","785d0337":"from sklearn.feature_selection import RFE","8f02271b":"logreg_rfe20 = LogisticRegression(random_state = 100)","1e93e670":"rfe20 = RFE(logreg_rfe20, n_features_to_select = 20)\nrfe20 = rfe20.fit(X_train, y_train)","bed1dc4a":"# Column name, RFE is True\/False, Ranking of column\nprint(list(zip(X_train, rfe20.support_,rfe20.ranking_)))\n# Columns with RFE Support as True\ncol = X_train.columns[rfe20.support_]\nprint(len(col), col)\n# Columns with RFE Support as False\nX_train.columns[~rfe20.support_]","ce9c34de":"# Creating X_train_rfe dataframe with RFE selected variables\n\nX_train_rfe20 = X_train[col]\nX_test_rfe20 = X_test[col]","71e60257":"logreg_rfe20 = LogisticRegression(random_state = 100)\nlogreg_rfe20 = logreg_rfe20.fit(X_train_rfe20, y_train)","467ce00f":"y_train_pred_rfe20, y_train_pred_rfe20_prob, y_test_pred_rfe20, y_test_pred_rfe20_prob = prediction_model(logreg_rfe20, X_train_rfe20, y_train, X_test_rfe20, y_test)","890a1bba":"draw_roc(y_train, y_train_pred_rfe20_prob, y_test, y_test_pred_rfe20_prob)","2728b8b6":"print(\"Accuracy train: \", accuracy_score(y_train, y_train_pred_rfe20))\nprint(\"Accuracy test: \", accuracy_score(y_test, y_test_pred_rfe20))","b18c6809":"lr_model = LogisticRegression()","8967e411":"# Create the param grid for logistic regression\nweights = np.linspace(0.2,0.80,4)\nlr_param_grid = {\n    'max_iter': [75, 100, 125],\n    'penalty': ['l1', 'l2'],\n    'C': [0.5 , 1, 1.5],\n    'solver': ['liblinear', 'lbfgs'],\n    'class_weight': [{0:x, 1:1.0-x} for x in weights]\n}\nprint(lr_param_grid)","444faed1":"lr_grid = training_model_hyperparameter(lr_model, 'roc_auc', lr_param_grid, X_train, y_train)","a8775e0e":"y_train_pred_lr, y_train_pred_lr_prob, y_test_pred_lr, y_test_pred_lr_prob = \\\n                                    prediction_model(lr_grid, X_train, y_train, X_test, y_test)","13546d85":"draw_roc(y_train, y_train_pred_lr_prob, y_test, y_test_pred_lr_prob)","43eed3cb":"lr_grid.best_params_","2a3a1989":"cv_results = pd.DataFrame(lr_grid.cv_results_)\ncv_results","6a08726e":"cv_results[[\"param_max_iter\", \"param_penalty\", \"param_solver\", \"mean_train_score\",\"mean_test_score\"]]","974dc6e1":"print(\"Accuracy train: \", accuracy_score(y_train, y_train_pred_lr))\nprint(\"Accuracy test: \", accuracy_score(y_test, y_test_pred_lr))","af693e45":"# Create the param grid for random forest\nparam_grid_rf = [{'n_estimators': [30, 50, 75, 100],\n               'max_depth': [5, 6, 7, 8, 9, 10],\n               'max_features': [10, 15, 25, 30],\n    'class_weight': [{0:x, 1:1.0-x} for x in weights]}]\nprint(param_grid_rf)","eb95f1c8":"rf_model = RandomForestClassifier()","ad947eb8":"rf_grid = training_model_hyperparameter(rf_model, 'roc_auc', param_grid_rf, X_train, y_train)","61eae7f8":"rf_grid.best_params_","35bad051":"cv_results = pd.DataFrame(rf_grid.cv_results_)\ncv_results","ee19ce49":"cv_results[[\"param_max_depth\",\"param_max_features\",\"param_n_estimators\",\"mean_train_score\",\"mean_test_score\"]]","ebe95712":"y_train_pred_rf, y_train_pred_rf_prob, y_test_pred_rf, y_test_pred_rf_prob = \\\n                                    prediction_model(rf_grid, X_train, y_train, X_test, y_test)","38d91a40":"draw_roc(y_train, y_train_pred_rf_prob, y_test, y_test_pred_rf_prob)","d681b8c5":"print(\"Accuracy train: \", accuracy_score(y_train, y_train_pred_rf))\nprint(\"Accuracy test: \", accuracy_score(y_test, y_test_pred_rf))","8fdd2149":"from sklearn.ensemble import GradientBoostingClassifier","c30d6f37":"gb_model = GradientBoostingClassifier(verbose = 1)","8e74e545":"params_grid_gb = {\n    \"n_iter_no_change\": ['None', 5, 10],\n    \"n_estimators\": [30, 50, 75, 100],\n    \"learning_rate\": [0.05, 0.1, 0.15],\n    'class_weight': [{0:x, 1:1.0-x} for x in weights]\n}","e8dad5f6":"gb_grid = training_model_hyperparameter(gb_model, 'roc_auc', params_grid_gb, X_train, y_train)","577745ec":"gb_grid.best_params_","0fa438e4":"cv_results = pd.DataFrame(gb_grid.cv_results_)\ncv_results","dc3b94b8":"cv_results[[\"param_learning_rate\",\"param_n_estimators\",\"param_n_iter_no_change\",\"mean_train_score\",\"mean_test_score\"]]","14ff8d5f":"y_train_pred_gb, y_train_pred_gb_prob, y_test_pred_gb, y_test_pred_gb_prob = \\\n                                    prediction_model(gb_grid, X_train, y_train, X_test, y_test)","8d0eb3f4":"draw_roc(y_train, y_train_pred_gb_prob, y_test, y_test_pred_gb_prob)","73ef0ed3":"print(\"Accuracy train: \", accuracy_score(y_train, y_train_pred_gb))\nprint(\"Accuracy test: \", accuracy_score(y_test, y_test_pred_gb))","1bc8f422":"from sklearn.svm import SVC","e2ac1222":"svc_model = SVC()","141994ff":"params_grid_svc = {\n    \"kernel\": [\"linear\", \"rbf\"],\n    \"degree\": [1, 2],\n    'class_weight': [{0:x, 1:1.0-x} for x in weights]\n}","55c65fb2":"svc_grid = training_model_hyperparameter(svc_model, 'roc_auc', params_grid_svc, X_train, y_train)","048e6c82":"svc_grid.best_params_","61e58080":"cv_results = pd.DataFrame(svc_grid.cv_results_)\ncv_results","e08ded5d":"cv_results[[\"param_kernel\",\"param_degree\",\"mean_train_score\",\"mean_test_score\"]]","e7bd04f6":"print(\"Accuracy train: \", accuracy_score(y_train, svc_grid.predict(X_train)))\nprint(\"Accuracy test: \", accuracy_score(y_test, svc_grid.predict(X_test)))","a6851cd8":"from xgboost import XGBClassifier","8dc8af3d":"xgb_model = XGBClassifier()","b6617975":"# Create the param grid for random forest\nparam_grid_xgb = {\n               'max_depth': [5, 7],\n               'max_features': [10, 20, 30],\n    'class_weight': [{0:x, 1:1.0-x} for x in weights]\n}\nprint(param_grid_xgb)","7b8820be":"xgb_grid = training_model_hyperparameter(xgb_model, 'roc_auc', param_grid_xgb, X_train, y_train)","0e5552f9":"cv_results = pd.DataFrame(xgb_grid.cv_results_)\ncv_results","0997713f":"cv_results[[\"param_max_depth\",\"param_max_features\",\"mean_train_score\",\"mean_test_score\"]]","514c0fbd":"y_train_pred_xgb, y_train_pred_xgb_prob, y_test_pred_xgb, y_test_pred_xgb_prob = \\\n                                    prediction_model(xgb_grid, X_train, y_train, X_test, y_test)","bf384b8d":"draw_roc(y_train, y_train_pred_xgb_prob, y_test, y_test_pred_xgb_prob)","6eefcaff":"print(\"Accuracy train: \", accuracy_score(y_train, y_train_pred_xgb))\nprint(\"Accuracy test: \", accuracy_score(y_test, y_test_pred_xgb))","1511a2c9":"lr_fpr, lr_tpr, lr_thresholds = metrics.roc_curve( y_test, y_test_pred_lr_prob,\n                                              drop_intermediate = False )\nrf_fpr, rf_tpr, rf_thresholds = metrics.roc_curve( y_test, y_test_pred_rf_prob,\n                                              drop_intermediate = False )\ngb_fpr, gb_tpr, gb_thresholds = metrics.roc_curve( y_test, y_test_pred_gb_prob,\n                                              drop_intermediate = False )\nxgb_fpr, xgb_tpr, xgb_thresholds = metrics.roc_curve( y_test, y_test_pred_xgb_prob,\n                                              drop_intermediate = False )\nlr_auc_score = metrics.roc_auc_score( y_test, y_test_pred_lr_prob )\nrf_auc_score = metrics.roc_auc_score( y_test, y_test_pred_rf_prob )\ngb_auc_score = metrics.roc_auc_score( y_test, y_test_pred_gb_prob )\nxgb_auc_score = metrics.roc_auc_score( y_test, y_test_pred_xgb_prob )\nplt.figure(figsize=(5, 5))\nplt.plot( lr_fpr, lr_tpr, label='LR ROC curve (area = %0.2f)' % lr_auc_score )\nplt.plot( rf_fpr, rf_tpr, label='RF ROC curve (area = %0.2f)' % rf_auc_score )\nplt.plot( gb_fpr, gb_tpr, label='GB ROC curve (area = %0.2f)' % gb_auc_score )\nplt.plot( gb_fpr, gb_tpr, label='XGB ROC curve (area = %0.2f)' % xgb_auc_score )\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate or [1 - True Negative Rate]')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()","5471fbed":"\n# iv. Random Forest Classifier","079d6cb6":"We will user AUC for performance metrics as the target class is imbalanced.","0024cf5a":"#### Train Gradient Boosting Classifier model with hyperparameter tuning","ba491f4e":"# Model Building ","52b9c480":"# v. Gradient Boosting Classifier","dd779157":"<br> **Insights from continuous variables**\n<br>\n<br>\n    1. Campaign: If the number of contacts performed during this campaign and for this client become more than 23 then there is very high possibility that client will not subscribe for term deposit. Even if we contact a person more than 8 times probability is still low.\n    **We will drop this column as it will not be known before hand. But it is important to see that we should not contact any client more than 8 times during any campaign.**<br>\n    2. Consumer price index: If this value is high then probability of client not subscribing is slightly higher. <br>\n    3. Previous: If we contact client before a campaign then there is high chances that client will subscribe. We converted all the values above 2 to 2 based on the given data\n    <br>\n    4. Euribor 3 month rate: If this rate is high there is high chances of clients not subscribing to term deposit.<br>\n    5. Pdays: If we start contacting client 1 month before campaign there is high probability of that client subscribing.","7b3a06e6":"With this we can say that 'emp.var.rate' (employment variation rate ) and 'nr.employed' (number of employees) are positively correlated with euribor.\nSo we will **drop 'emp.var.rate' and 'nr.employed'** as **'euribor'** also give us the price of money in current market.","f5673bfc":"# Compare Models","76a8a196":"# Dividing into X and Y sets for the model building","8bc73b03":"#### Train Random Forest Classifier model with hyperparameter tuning","50c162d2":"# One Hot Encoding for categorical variables","e0e60ac9":"**Euribor** is the acronym for the Euro Interbank Offered Rate. This is the interest rate at which credit institutions lend money to each other, which is often referred to as **\u201cthe price of money\u201d**.","f7fd6c82":"Positive high correlation between:\n<br>    1. 'emp.var.rate' and 'nr.employed'\n<br>    2. 'emp.var.rate' and 'euribor3m'\n<br>    3. 'euribor3m' and 'nr.employed'","82c1f67d":"# iii. Training model with hyperparameter Tuning","45c1dbbb":"## Handle highly correlated variables","375f575d":"The binary classification goal is to predict if the client will subscribe a bank term deposit (variable y).","c53610d9":"# Bank Marketing (with social\/economic context)","87ba2680":"#### a. Train Model","ba278032":"**Negative Correlation:** \n<br>    1. Previous v\/s Nonexistent\n<br>    2. Pdays v\/s Success","8b55c315":"We will drop the missing values as imputing this missing values in an assumption which can effect out dataset","473f5e0f":"## b. Numerical Variables","d232113e":"**Missing Attribute Values:** There are several missing values in some categorical attributes, all coded with the \"unknown\" label. These missing values can be treated as a possible class label or using deletion or imputation techniques. ","eb8fe6c6":"# Loading Dataset","a89ff7e3":"#### Train Gradient Boosting Classifier model with hyperparameter tuning","6634b36f":"Look into 'campaign', 'age', 'previous', 'cons.conf.idx', 'pdays'","614464aa":"# vii. XGBoost Classifier","f0759a28":"Our data is not normally distributed so we will use MinMaxScaler. It can also handle some of the outliers which we have in our data.","c1583bd4":"# Missing values","b435bbc2":"## i. Univariate Analysis and Segmented Univariate Analysis","d61a79d4":"## a. Categorical Variables","816183a3":"# i. Logistic Regression","735f662e":"# Split Data Into Train and Test","142cf6eb":"**Insights from categorical variables (based on univariate analysis)**\n\n    1. Job: Highest Number (around 25%) of application are from admin type of job.\n    2. Default: Default variable has no impact on the client subscribing for term deposit. As we can see with no as input client took the term deposit and client having credit are not taking term deposit. So we will drop this feature.\n    3. Marital: Around 60% of client were approached were married. \n    4. Education: Client with university degree and high school were approached more as compare to other and they have higher success rate as well. (in terms of term deposit number)\n    5. Housing: Housing loan does not have much effect on the number of term deposit purchased.\n    6. Loan: We approach around 84% of client with not having personal loan. \n    7. Contact: Around 64% calls are from cellular.\n    8. Month: Around 33% were approached in may and in January, Febuary we don't have data or no one was approached. Success rate was almost same in june, july and August. \n    9. day_of_week: We have 5 days collected values. There is no significant different in the number of client approached and number of people subscribed.\n    10. poutcome: If a client took the term deposit last time than there is higher chances of that client subscribing to it again.","08108434":"# Rescaling the continuous variables","c5939154":"# Exploratory Data Analysis","00a3407d":"# vi. SVM (Support Vector Machine)","8f85d56f":"# ii. Feature Selection Using RFE with 20 Features"}}