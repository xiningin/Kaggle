{"cell_type":{"168bd462":"code","dcbba9d8":"code","272dc68c":"code","08569019":"code","10600639":"code","5438b1dc":"code","be2854ed":"code","9f8bf8a1":"code","e2ab3df5":"code","587b4f11":"markdown","59df0fad":"markdown","effac16f":"markdown"},"source":{"168bd462":"# Imports\n\nimport os\nimport random\nimport numpy as np\nimport pandas as pd\nimport glob\nimport re\nimport gc; gc.enable()\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, SequentialSampler, DataLoader\n\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_linear_schedule_with_warmup, logging\n\nimport transformers\n\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.metrics import mean_squared_error\n","dcbba9d8":"# Constants\n\nSEED = 42\n\nHIDDEN_SIZE = 1024\nMAX_LEN = 300\n\nINPUT_DIR = '..\/input\/commonlitreadabilityprize'\nBASELINE_DIR = '..\/input\/baseline-mp-ft2'\nMODEL_DIR = '..\/input\/roberta-transformers-pytorch\/roberta-large'\n\nTOKENIZER = AutoTokenizer.from_pretrained(MODEL_DIR)\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBATCH_SIZE = 8","272dc68c":"# Utility functions\n\ndef seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \n    \nseed_everything(SEED)","08569019":"# Data\n\nsubmission = pd.read_csv(os.path.join(INPUT_DIR, 'sample_submission.csv'))\ntest = pd.read_csv(os.path.join(INPUT_DIR, 'test.csv'))\ntest.head()","10600639":"# Dataset\n\nclass CLRPDataset(Dataset):\n    def __init__(self, texts, tokenizer):\n        self.texts = texts\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        encode = self.tokenizer(\n            self.texts[idx],\n            padding='max_length',\n            max_length=MAX_LEN,\n            truncation=True,\n            add_special_tokens=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        ) \n        return encode","5438b1dc":"# Model\n\nclass MeanPoolingModel(nn.Module):\n    \n    def __init__(self, model_name):\n        super().__init__()\n        \n        config = AutoConfig.from_pretrained(model_name)\n        self.model = AutoModel.from_pretrained(model_name, config=config)\n        self.layer_norm = nn.LayerNorm(HIDDEN_SIZE)\n        self.linear = nn.Linear(HIDDEN_SIZE, 1)\n        self.loss = nn.MSELoss()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        \n        outputs = self.model(input_ids, attention_mask)\n        last_hidden_state = outputs[0]\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings \/ sum_mask\n        norm_mean_embeddings = self.layer_norm(mean_embeddings)\n        logits = self.linear(norm_mean_embeddings)\n        \n        preds = logits.squeeze(-1).squeeze(-1)\n        \n        if labels is not None:\n            loss = self.loss(preds.view(-1).float(), labels.view(-1).float())\n            return loss\n        else:\n            return preds\n        \n","be2854ed":"def predict(df, model):\n    \n    ds = CLRPDataset(df.excerpt.tolist(), TOKENIZER)\n    dl = DataLoader(\n        ds,\n        batch_size=BATCH_SIZE,\n        shuffle=False,\n        pin_memory=False\n    )\n    \n    model.to(DEVICE)\n    model.eval()\n    model.zero_grad()\n    \n    predictions = []\n    for batch in tqdm(dl):\n        inputs = {key:val.reshape(val.shape[0], -1).to(DEVICE) for key,val in batch.items()}\n        outputs = model(**inputs)\n        predictions.extend(outputs.detach().cpu().numpy().ravel())\n        \n    return predictions\n    ","9f8bf8a1":"# Calculate predictions of each fold and average them\n\nfold_predictions = []\nfor path in glob.glob(BASELINE_DIR + '\/*.ckpt'):\n    model = MeanPoolingModel(MODEL_DIR)\n    model.load_state_dict(torch.load(path))\n    fold = int(re.match(r'.*_f_?(\\d)_.*', path).group(1))\n    print(f'*** fold {fold}: {path} ***')\n    y_pred = predict(test, model)\n    fold_predictions.append(y_pred)\n    \n    # Free memory\n    del model\n    gc.collect()\n    \npredictions = np.mean(fold_predictions, axis=0)","e2ab3df5":"# Submission\n\nsubmission['target'] = predictions\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","587b4f11":"This is an inference notebook. \n\nIt uses 5 folds of a model using the <b>MeanPoolingModel<\/b> representation (see my notebook on [the best transformer representations](https:\/\/www.kaggle.com\/jcesquiveld\/best-transformer-representations)), which is the one with which I've obtained the best results.\n\nFor training this model, I've used used the following <b>strategy<\/b>:\n<ul>\n    <li>I've used Roberta-large as my base model.<\/li>\n    <li>Because the dataset is small, and in my experiments I've seen that 5 epochs achieve almost the same results as training for a longer time, I've done all the runs just for 5 epochs.<\/li>\n    <li>I've pretrained Roberta-large using the competition data and an extended dataset with extra excerpts from the same entries in Wikipedia and SimpleWikipedia present in the competition dataset. Also, more than 1200 excerpts in the training set correspond to books freely available in <a href='https:\/\/gutenberg.org\/'>the gutenberg project web site<\/a>, so I've used extra excerpts from the <b>same books<\/b>.I've combined then in different ways. For example, only the competition data pretraining for 2 epochs, boths datasets pretraining for 2 epochs, only the extended dataset, pretraining for 5 epochs, etc. <\/li>\n    <li>I've also used layer-wise learning rate decay (see my post <a href='https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/251761'>Layer-wise learning rate decay. What values to use?<\/a><\/li>. As you can see there, there's no clear winner, though the mean RMSE best value is for an initial learning rate of 3e-5 and a multiplier factor of 0.975.<\/li>\n    <li>As the competition dataset is small, I've made different combinations of: pretrained strategy, initial learning rate, multiplier, and I've also used 5 different seeds. From these experiments, I've chosen the best loss values (I've evaluated every 20 iterations) for each fold. These are the results (LB 0.462):<\/li>\n<\/ul>\n        <table align='left'>\n            <tr><th>Fold<\/th><th>Loss<\/th><\/tr>\n            <tr><td>0<\/td><td>0.2227<\/td><\/tr>\n            <tr><td>1<\/td><td>0.2424<\/td><\/tr>\n            <tr><td>2<\/td><td>0.2143<\/td><\/tr>\n            <tr><td>3<\/td><td>0.1960<\/td><\/tr>\n            <tr><td>4<\/td><td>0.2354<\/td><\/tr>\n        <\/table>","59df0fad":"<h2>Prediction<\/h2>","effac16f":"<h1>RoBERTa-large 5-fold single model (MeanPooling)<\/h1>"}}