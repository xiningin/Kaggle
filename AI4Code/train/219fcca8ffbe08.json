{"cell_type":{"10b65cbe":"code","31170ee2":"code","08ed9e0a":"code","c498858b":"code","6c2a6f40":"code","3108efcb":"code","7f713ef7":"code","4bb3dbca":"code","1d109025":"code","ac44c09d":"markdown","25061920":"markdown","2fe9e843":"markdown","85e68fc0":"markdown","dc419827":"markdown","7497d403":"markdown"},"source":{"10b65cbe":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datatable as dt\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras import Sequential, Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import Dense, Flatten, InputLayer, Dropout, Input\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","31170ee2":"### set seeds\nmy_seed = 42\n\nnp.random.seed(my_seed)\ntf.random.set_seed(my_seed)","08ed9e0a":"### load dataframes\ndf_train = dt.fread('..\/input\/tabular-playground-series-nov-2021\/train.csv').to_pandas()\ndf_test = dt.fread('..\/input\/tabular-playground-series-nov-2021\/test.csv').to_pandas()\n\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\n\n### split into X, y\nX = df_train.drop(columns=['id','target']).copy()\ny = df_train['target'].copy()\n\nX_test = df_test.drop(columns='id').copy()\n\n### standardize data\nscaler = StandardScaler()\n\nX = pd.DataFrame(columns=X.columns, data=scaler.fit_transform(X))\nX_test = pd.DataFrame(columns=X_test.columns, data=scaler.transform(X_test))","c498858b":"### define callbacks\nearly_stopping = EarlyStopping(\n    monitor='val_loss', \n    min_delta=0, \n    patience=16, \n    verbose=0,\n    mode='min', \n    baseline=None, \n    restore_best_weights=True\n)\n\nreduce_lr = ReduceLROnPlateau(\n    monitor='val_loss', \n    factor=0.2,\n    patience=5,\n    mode='min'\n)","6c2a6f40":"### create baseline-model\ndef get_model():\n    inp = Input(shape=X.shape[1], name='input')\n    h = Dense(128, activation='swish')(inp)\n    h = Dropout(0.25)(h)\n    h = Dense(64, activation='swish')(h)\n    h = Dropout(0.25)(h)\n    h = Dense(32, activation='swish')(h)\n    h = Dropout(0.25)(h)\n    h = Dense(1, activation='sigmoid')(h)\n    \n    model = Model(inputs=inp, outputs=h)\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n        loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=1e-3),\n        metrics=['AUC']\n    )\n    return model","3108efcb":"EPOCHS = 100\nBATCH_SIZE = 1024\nVERBOSE = 0\nN_SPLITS = 10\n\n### cross-validation \ncv = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=my_seed)\n\nscores = {fold:None for fold in range(cv.n_splits)}\npredictions = []\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n\n    model = get_model()\n\n    print('**'*20)\n    print(f\"Fold {fold+1} || Training\")\n    print('**'*20)\n    \n    history = model.fit(\n        X_train, y_train,\n        validation_data=(X_valid, y_valid),\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        verbose=VERBOSE,\n        callbacks=[\n            early_stopping,\n            reduce_lr\n        ]\n    )\n    \n    scores[fold] = (history.history)\n    \n    print(f\"Fold {fold+1} || Max Validation AUC: {np.max(scores[fold]['val_auc'])}\")\n    \n    prediction = model.predict(X_test, batch_size=BATCH_SIZE).reshape(1,-1)[0]\n    predictions.append(prediction)\n\nprint('**'*20)\nprint('Finished Training')\nprint('**'*20)\n\noverall_auc = [np.max(scores[fold]['val_auc']) for fold in range(cv.n_splits)]\nprint('Overall Mean AUC: ', np.mean(overall_auc))","7f713ef7":"### plot train and valid loss over number of epochs\nfig, ax = plt.subplots(2, 5, tight_layout=True, figsize=(20,5))\nax = ax.flatten()\n\nfor fold in range(cv.n_splits):\n    df_eval = pd.DataFrame({'train_loss': scores[fold]['loss'], 'valid_loss': scores[fold]['val_loss']})\n\n    min_train = np.round(np.min(df_eval['train_loss']),5)\n    min_valid = np.round(np.min(df_eval['valid_loss']),5)\n    delta = np.round(min_valid - min_train,5)\n    \n    sns.lineplot(\n        x=df_eval.index,\n        y=df_eval['train_loss'],\n        label='train_loss',\n        ax = ax[fold]\n    )\n\n    sns.lineplot(\n        x=df_eval.index,\n        y=df_eval['valid_loss'],\n        label='valid_loss',\n        ax = ax[fold]\n    )\n    \n    ax[fold].set_ylabel('')\n    ax[fold].set_xlabel(f\"Fold {fold+1}\\nmin_train: {min_train}\\nmin_valid: {min_valid}\\ndelta: {delta}\", fontstyle='italic')\n\nsns.despine()","4bb3dbca":"def postprocess_separate(submission_df, test_df=None, pure_df=None):\n    \"\"\"Update submission_df so that the predictions for the two sides of the hyperplane don't overlap.\n    \n    Parameters\n    ----------\n    submission_df : pandas DataFrame with columns 'id' and 'target'\n    test_df : the competition's test data\n    pure_df : the competition's original training data\n    \n    From https:\/\/www.kaggle.com\/ambrosm\/tpsnov21-007-postprocessing\n    \"\"\"\n    \n    sub = submission_df\n    \n    if pure_df is None: pure_df = pd.read_csv('..\/input\/november21\/train.csv')\n    if pure_df.shape != (600000, 102): raise ValueError(\"pure_df has the wrong shape\")\n    if test_df is None: test_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\n    if test_df.shape[0] != submission_df.shape[0] or test_df.shape[1] != 101: raise ValueError(\"test_df has the wrong shape\")\n\n    # Find the separating hyperplane for pure_df, step 1\n    # Use an SVM with almost no regularization\n    model1 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model1.fit(pure_df.drop(columns=['id', 'target']), pure_df.target)\n    pure_pred = model1.predict(pure_df.drop(columns=['id', 'target']))\n    print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 1 599999\n    # model1 is not perfect: it predicts the wrong class for 1 of 600000 samples\n\n    # Find the separating hyperplane for pure_df, step 2\n    # Fit a second SVM to a subset of the points which contains the support vectors\n    pure_pred = model1.decision_function(pure_df.drop(columns=['id', 'target']))\n    subset_df = pure_df[(pure_pred > -5) & (pure_pred < 0.9)]\n    model2 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model2.fit(subset_df.drop(columns=['id', 'target']), subset_df.target)\n    pure_pred = model2.predict(pure_df.drop(columns=['id', 'target']))\n    print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 0 600000\n    # model2 is perfect: it predicts the correct class for all 600000 training samples\n    \n    pure_test_pred = model2.predict(test_df.drop(columns=['id', 'target'], errors='ignore'))\n    lmax, rmin = sub[pure_test_pred == 0].target.max(), sub[pure_test_pred == 1].target.min()\n    if lmax < rmin:\n        print(\"There is no overlap. No postprocessing needed.\")\n        return\n    # There is overlap. Remove this overlap\n    sub.loc[pure_test_pred == 0, 'target'] -= lmax + 1\n    sub.loc[pure_test_pred == 1, 'target'] -= rmin - 1\n    print(sub[pure_test_pred == 0].target.min(), sub[pure_test_pred == 0].target.max(),\n          sub[pure_test_pred == 1].target.min(), sub[pure_test_pred == 1].target.max())","1d109025":"### average predictions over each fold and create submission file\nsample_submission['target'] = np.mean(np.column_stack(predictions), axis=1)\nsample_submission.to_csv('.\/nn_baseline.csv', index=False)\n\npostprocess_separate(sample_submission)\nsample_submission.to_csv('.\/nn_baseline-with-post-processing.csv', index=False)","ac44c09d":"## <span style=\"background:#818181;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">Evaluation<\/span>","25061920":"## <span style=\"background:#818181;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">Introduction<\/span>","2fe9e843":"## <span style=\"background:#818181;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">Submission<\/span>","85e68fc0":"## <span style=\"background:#818181;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">Import Data & Pre-Processing<\/span>","dc419827":"<div style=\"font-size: 1em; font-family: Verdana\">\n    <b>Hi,<\/b><br><br>\n    I just wanted to share my baseline-model with you guys.<br>\n    I've just recently started getting into 'deep learning' and read a lot of basics.<br>\n    This is the reason why I decided to use this month competition to get some practice with Neural-Networks.<br><br>\n    Also make sure to check out my EDA for TPS November 2021 <a href=\"https:\/\/www.kaggle.com\/mlanhenke\/tps-11-simple-basic-eda\">here<\/a>. <br><br>\n    <em>If you like this notebook or copy any parts of it please make sure to leave an upvote...<\/em><br><br>\n    <em><b>Thanks for taking some time to stop by and read my notebook!<\/b><\/em>\n<\/div>\n\n","7497d403":"## <span style=\"background:#818181;padding:0.3em;width:100%;display:block;border-radius:0.1em;color:white;font-family:Monospace\">Modeling<\/span>"}}