{"cell_type":{"a80b4bbb":"code","f59f7da5":"code","f4f48e07":"code","5d9d8ca5":"code","c6fbefb5":"code","ea307ed4":"code","b556ae36":"code","c2bf380e":"code","f93bae9a":"code","b1ba78d1":"code","0b5d1c04":"code","32d0b894":"code","34006df0":"code","d81834d5":"code","884c60de":"code","d0e7c8c2":"code","caaf9388":"code","d0c62f83":"code","a3542dd4":"code","c07677a5":"code","0eecda3c":"code","a09c84e7":"code","0aeab463":"code","3505fc34":"code","56835f74":"code","b342d677":"code","76f586eb":"code","2c10151c":"code","672498ee":"code","03da9bdc":"code","27f5ebfe":"code","fc07a7f5":"code","e9a15ebd":"code","3744d546":"code","9515a3e2":"code","e6c2acb3":"code","6e164a8b":"code","7cd96d0b":"code","84d51856":"code","a213686c":"markdown","e0d7cf9f":"markdown","e7e15ca1":"markdown","550c18d6":"markdown","a0b845ca":"markdown","17e86a76":"markdown","70a49ccf":"markdown","292564ab":"markdown","383821f3":"markdown","c9db7dd5":"markdown","64208630":"markdown","20839ff8":"markdown","84046796":"markdown"},"source":{"a80b4bbb":"#importing libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f59f7da5":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f4f48e07":"df = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf.shape","5d9d8ca5":"df.head()","c6fbefb5":"df.dtypes","ea307ed4":"df['quality'].unique()","b556ae36":"new=[]\nfor row in df['quality']:\n    if (row<=5):\n        val=1\n    else:\n        val=2\n    new.append(val)","c2bf380e":"df['newquality']=new","f93bae9a":"from collections import Counter\nprint(sorted(Counter(df['newquality']).items()))","b1ba78d1":"sns.countplot(df['newquality'],palette='RdPu')","0b5d1c04":"#correaltion map\nsns.heatmap(df.corr())","32d0b894":"X=df.drop(['quality','newquality'],axis=1)\ny=df['newquality']","34006df0":"#dividing the dataset into train and test sets\nfrom sklearn.model_selection import train_test_split\nx_train1,x_test,y_train1,y_test=train_test_split(X,y,test_size=0.20,random_state=1234)","d81834d5":"from imblearn.over_sampling import SMOTE\n\noversample = SMOTE()\nx_train, y_train = oversample.fit_resample(x_train1, y_train1)","884c60de":"sns.countplot(y_train,palette='RdPu')","d0e7c8c2":"print(sorted(Counter(y_train).items()))","caaf9388":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#libraries for model evaluation\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import classification_report","d0c62f83":"#logistic regression\nlr = LogisticRegression(max_iter=20000,penalty='l2')\nmodel1=lr.fit(x_train, y_train)\nprint(\"train accuracy:\",model1.score(x_train, y_train),\"\\n\",\"test accuracy:\",model1.score(x_test,y_test))\nlrpred = lr.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for logistic regression\")\nprint(classification_report(lrpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for logistic regression\")\ndisplr = plot_confusion_matrix(lr, x_test, y_test,cmap=plt.cm.RdPu , values_format='d')\n","a3542dd4":"#linear discriminant analysis\nlda = LinearDiscriminantAnalysis()\nmodel2=lda.fit(x_train, y_train)\nprint(\"train accuracy:\",model2.score(x_train, y_train),\"\\n\",\"test accuracy:\",model2.score(x_test,y_test))\n\nldapred = lda.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for linear discriminant analysis\")\nprint(classification_report(ldapred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for linear discriminant analysis\")\ndisplr = plot_confusion_matrix(lda, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","c07677a5":"#decision tree classifier\ndt=DecisionTreeClassifier()\nmodel3=dt.fit(x_train, y_train)\nprint(\"train accuracy:\",model3.score(x_train, y_train),\"\\n\",\"test accuracy:\",model3.score(x_test,y_test))\n\ndtpred = dt.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for decision tree classifier\")\nprint(classification_report(dtpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for decision tree classifier\")\ndisplr = plot_confusion_matrix(dt, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","0eecda3c":"#random forest classifier\nrf=RandomForestClassifier()\nmodel4=rf.fit(x_train, y_train)\nprint(\"train accuracy:\",model4.score(x_train, y_train),\"\\n\",\"test accuracy:\",model4.score(x_test,y_test))\n\nrfpred = rf.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for random forest classifier\")\nprint(classification_report(rfpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for random forest classifier\")\ndisplr = plot_confusion_matrix(rf, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","a09c84e7":"#bagging classifier\nbg=BaggingClassifier()\nmodel5=bg.fit(x_train, y_train)\nprint(\"train accuracy:\",model5.score(x_train, y_train),\"\\n\",\"test accuracy:\",model5.score(x_test,y_test))\n\nbgpred = bg.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for bagging classifier\")\nprint(classification_report(bgpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for bagging classifier\")\ndisplr = plot_confusion_matrix(bg, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","0aeab463":"# gradient boost classifier \ngbm=GradientBoostingClassifier()\nmodel6=gbm.fit(x_train, y_train)\nprint(\"train accuracy:\",model6.score(x_train, y_train),\"\\n\",\"test accuracy:\",model6.score(x_test,y_test))\n\ngbmpred = gbm.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for gradient boosting classifier\")\nprint(classification_report(gbmpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for gradient boosting classifier\")\ndisplr = plot_confusion_matrix(gbm, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","3505fc34":"# adaboost classifier \nada=AdaBoostClassifier()\nmodel7=ada.fit(x_train, y_train)\nprint(\"train accuracy:\",model7.score(x_train, y_train),\"\\n\",\"test accuracy:\",model7.score(x_test,y_test))\n\nadapred = ada.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for adaboost classifier\")\nprint(classification_report(adapred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for adaboost classifier\")\ndisplr = plot_confusion_matrix(ada, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","56835f74":"# extreme gradient boost classifier\nxgb = XGBClassifier()\nmodel8=xgb.fit(x_train.values, y_train)\nprint(\"train accuracy:\",model8.score(x_train, y_train),\"\\n\",\"test accuracy:\",model8.score(x_test,y_test))\n\nxgbpred = xgb.predict(x_test.values)\nprint(\"\\n\")\nprint(\"classification report for extreme gradient boosting classifier\")\nprint(classification_report(xgbpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for extreme gradient boosting classifier\")\ndisplr = plot_confusion_matrix(xgb, x_test.values, y_test ,cmap=plt.cm.RdPu , values_format='d')","b342d677":"# extra tree classifier\nextree = ExtraTreesClassifier()\nmodel9=extree.fit(x_train, y_train)\nprint(\"train accuracy:\",model9.score(x_train, y_train),\"\\n\",\"test accuracy:\",model9.score(x_test,y_test))\n\nextpred = extree.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for extra tree classifier\")\nprint(classification_report(extpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for extra tree classifier\")\ndisplr = plot_confusion_matrix(extree, x_test, y_test ,cmap=plt.cm.RdPu, values_format='d')","76f586eb":"# voting classifer\nfrom sklearn.ensemble import VotingClassifier\nclf1 = ExtraTreesClassifier()\nclf2 = RandomForestClassifier()\n\nvc = VotingClassifier(estimators=[('ext', clf1),('rf', clf2)], voting='soft')\nmodel10=vc.fit(x_train, y_train)\nprint(\"train accuracy:\",model10.score(x_train, y_train),\"\\n\",\"test accuracy:\",model10.score(x_test,y_test))\n\nvcpred = vc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for voting classifier\")\nprint(classification_report(vcpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for voting classifier\")\ndisplr = plot_confusion_matrix(vc, x_test, y_test ,cmap=plt.cm.RdPu, values_format='d')","2c10151c":"# stacking classifier \nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nestimators = [('rf', RandomForestClassifier(n_estimators=10, random_state=42)),('ext', make_pipeline(StandardScaler(),  ExtraTreesClassifier(random_state=42)))]\nsc= StackingClassifier( estimators=estimators)\n\nmodel11=sc.fit(x_train, y_train)\nprint(\"train accuracy:\",model11.score(x_train, y_train),\"\\n\",\"test accuracy:\",model11.score(x_test,y_test))\n\nscpred = sc.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for voting classifier\")\nprint(classification_report(scpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for voting classifier\")\ndisplr = plot_confusion_matrix(sc, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')\n\n","672498ee":"#logistic regression without resampling\nlrw = LogisticRegression(max_iter=20000,penalty='l2')\nmodel12=lrw.fit(x_train1, y_train1)\nprint(\"train accuracy:\",model12.score(x_train1, y_train1),\"\\n\",\"test accuracy:\",model12.score(x_test,y_test))\nlrwpred = lrw.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for logistic regression\")\nprint(classification_report(lrwpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for logistic regression\")\ndisplr = plot_confusion_matrix(lrw, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')\n","03da9bdc":"#linear discriminant analysis\nldaw = LinearDiscriminantAnalysis()\nmodel13=ldaw.fit(x_train1, y_train1)\nprint(\"train accuracy:\",model13.score(x_train1, y_train1),\"\\n\",\"test accuracy:\",model13.score(x_test,y_test))\n\nldawpred = ldaw.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for linear discriminant analysis\")\nprint(classification_report(ldawpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for linear discriminant analysis\")\ndisplr = plot_confusion_matrix(ldaw, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","27f5ebfe":"#decision tree classifier\ndtw=DecisionTreeClassifier()\nmodel14=dtw.fit(x_train1, y_train1)\nprint(\"train accuracy:\",model14.score(x_train1, y_train1),\"\\n\",\"test accuracy:\",model14.score(x_test,y_test))\n\ndtwpred = dtw.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for decision tree classifier\")\nprint(classification_report(dtwpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for decision tree classifier\")\ndisplr = plot_confusion_matrix(dtw, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","fc07a7f5":"rfw=RandomForestClassifier()\nmodel15=rfw.fit(x_train1, y_train1)\nprint(\"train accuracy:\",model15.score(x_train1, y_train1),\"\\n\",\"test accuracy:\",model15.score(x_test,y_test))\n\nrfwpred = rfw.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for random forest classifier\")\nprint(classification_report(rfwpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for random forest classifier\")\ndisplr = plot_confusion_matrix(rfw, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","e9a15ebd":"bgw=BaggingClassifier()\nmodel16=bgw.fit(x_train1, y_train1)\nprint(\"train accuracy:\",model16.score(x_train1, y_train1),\"\\n\",\"test accuracy:\",model16.score(x_test,y_test))\n\nbgwpred = bgw.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for bagging classifier\")\nprint(classification_report(bgwpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for bagging classifier\")\ndisplr = plot_confusion_matrix(bgw, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","3744d546":"gbmw=GradientBoostingClassifier()\nmodel17=gbmw.fit(x_train1, y_train1)\nprint(\"train accuracy:\",model17.score(x_train1, y_train1),\"\\n\",\"test accuracy:\",model17.score(x_test,y_test))\n\ngbmwpred = gbmw.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for gradient boosting classifier\")\nprint(classification_report(gbmwpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for gradient boosting classifier\")\ndisplr = plot_confusion_matrix(gbmw, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","9515a3e2":"adaw=AdaBoostClassifier()\nmodel18=adaw.fit(x_train1, y_train1)\nprint(\"train accuracy:\",model18.score(x_train1, y_train1),\"\\n\",\"test accuracy:\",model18.score(x_test,y_test))\n\nadawpred = adaw.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for adaboost classifier\")\nprint(classification_report(adawpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for adaboost classifier\")\ndisplr = plot_confusion_matrix(adaw, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","e6c2acb3":"xgbw = XGBClassifier()\nmodel19=xgbw.fit(x_train1.values, y_train1)\nprint(\"train accuracy:\",model19.score(x_train1, y_train1),\"\\n\",\"test accuracy:\",model19.score(x_test,y_test))\n\nxgbwpred = xgbw.predict(x_test.values)\nprint(\"\\n\")\nprint(\"classification report for extreme gradient boosting classifier\")\nprint(classification_report(xgbwpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for extreme gradient boosting classifier\")\ndisplr = plot_confusion_matrix(xgbw, x_test.values, y_test ,cmap=plt.cm.RdPu, values_format='d')","6e164a8b":"extreew = ExtraTreesClassifier()\nmodel20=extreew.fit(x_train1, y_train1)\nprint(\"train accuracy:\",model20.score(x_train1, y_train1),\"\\n\",\"test accuracy:\",model20.score(x_test,y_test))\n\nextwpred = extreew.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for extra tree classifier\")\nprint(classification_report(extwpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for extra tree classifier\")\ndisplr = plot_confusion_matrix(extreew, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","7cd96d0b":"clf1 = ExtraTreesClassifier()\nclf2 = RandomForestClassifier()\n\nvcw = VotingClassifier(estimators=[('ext', clf1),('rf', clf2)], voting='soft')\nmodel21=vcw.fit(x_train1, y_train1)\nprint(\"train accuracy:\",model21.score(x_train1, y_train1),\"\\n\",\"test accuracy:\",model21.score(x_test,y_test))\n\nvcwpred = vcw.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for voting classifier\")\nprint(classification_report(vcwpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for voting classifier\")\ndisplr = plot_confusion_matrix(vcw, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')","84d51856":"estimators = [('rf', RandomForestClassifier(n_estimators=10, random_state=2)),('ext', make_pipeline(StandardScaler(),  ExtraTreesClassifier(random_state=12)))]\nscw= StackingClassifier( estimators=estimators, final_estimator=LogisticRegression())\n\nmodel22=scw.fit(x_train1, y_train1)\nprint(\"train accuracy:\",model22.score(x_train1, y_train1),\"\\n\",\"test accuracy:\",model22.score(x_test,y_test))\n\nscwpred = scw.predict(x_test)\nprint(\"\\n\")\nprint(\"classification report for voting classifier\")\nprint(classification_report(scwpred,y_test))\nprint(\"\\n\")\nprint(\"confusion matrix for voting classifier\")\ndisplr = plot_confusion_matrix(scw, x_test, y_test ,cmap=plt.cm.RdPu , values_format='d')\n","a213686c":"##  <p style=\"background-color:#E2ADC3;color:black;font-size:20px;text-align:center;border-radius:10px 10px;border:2px solid purple;\">\ud83d\udc8e Results <\/p>","e0d7cf9f":"<font size=\"4\"><b>Highly appreciate your questions or feedback related to this notebook. THANK YOU <span style='font-size:22px;'>&#128522;<\/span><\/b> <\/font>\n\n\n\n\n<center><img src=\"https:\/\/media.giphy.com\/media\/JPgYys8I2ql8sPAFxz\/giphy.gif\" style=\"width:500px;height:300px;\"><\/center>","e7e15ca1":"<font size=\"4\">here we can see that dataet is not balanced. to overcome this problem we can use oversampling or undersampling. since low class have small number of observations here I am using oversampling techinque.<\/font>","550c18d6":"##  <p style=\"background-color:#E2ADC3;color:black;font-size:20px;text-align:center;border-radius:10px 10px;border:2px solid purple;\"> \ud83d\udcca resampling using synthetic minority oversampling technique <\/p>","a0b845ca":"<font size=\"4\">now let's see accuracy values without using smote resampling technique<\/font>","17e86a76":"\n##  <p style=\"background-color:#E2ADC3;color:black;font-size:20px;text-align:center;border-radius:10px 10px;border:2px solid purple;\">\ud83d\udc8e Introduction <\/p>\n\n<font size=\"4\"> We can use machine learning in production process to ensure that every wine bottle has the highest quality. This will in turn reduce the cost with regard to each wine bottle as less human labor is required for monitoring of quality. Therefore, this analysis is\ndone to <span style=\"color:#FF7F7F;\">predict wine quality given some important attributes in wine.<\/span>\n    The detailed descriptive analysis for this dataset can be found <a href=\"https:\/\/www.kaggle.com\/sisharaneranjana\/what-makes-great-wine-great\">here. <\/a> You can get a clear idea about what features make great wine.. great? from that.<\/font>","70a49ccf":"<p style=\"background-color:#E2ADC3;color:black;font-size:22px;text-align:center;border-radius:10px 9px;font-weight:bold;border:2px solid purple;\">Wine Quality Prediction <span style='font-size:29px; background-color:#9D476B ;'>&#127863;<\/span><\/p>\n\n\n<center><img src=\"https:\/\/github.com\/Isharaneranjana\/kaggle_gif\/blob\/main\/ACIDS%20(2).gif?raw=true\"><\/center>","292564ab":"<font size=\"4\">recoding the response variable to two categories as high quality and low quality.<\/font>","383821f3":"<font size=\"4\">from the correaltion plot we can see there are some highly correlated variables. in this analysis we are looking for prediction accuracy hence those correlations are neglected and also from the background study of red wines you can see that all the variables in this dataset are really important for final quality prediction.<\/font>\n\n<center><img src=\"https:\/\/media.giphy.com\/media\/l2Je34w7WkZ84f3os\/giphy.gif\" style=\"width:500px;height:300px;\"><\/center>","c9db7dd5":"<font size=\"4\">from the above models stacking classifier and random forest classifier have higher perfromance than other models. <span style=\"color:red;\">stacking classifier<\/span> is the one with highest accuracy <span style=\"color:red;\">85.94%<\/span>. We can see that without resampling there is a slight increase in accuracy. hence the model without SMOTE resampling is the best one. The accuracy can be increased by tuning hyper parameters of these models using scikit learn randomized grid search.  <\/font>","64208630":"<font size=\"4\">stacking classifier gives the highest accuracy which is 85.31%. precision and recall values are also high for this model. hence with SMOTE resampling the best model is stacking classifier with random forest classifier and extra tree classifier.<\/font>","20839ff8":"##  <p style=\"background-color:#E2ADC3;color:black;font-size:20px;text-align:center;border-radius:10px 10px;border:2px solid purple;\"> \ud83c\udfaf Objectives <\/p>\n\n<font size=\"4\"> The objectives of this project are as follows\n                  <p style=\"margin-left:20px;\">\ud83d\udccc To experiment with different classification methods to see which yields the highest accuracy<\/p><\/font>\n       <font size=\"4\">           <p style=\"margin-left:20px;\">\ud83d\udccc To determine the effect of oversampling for classification accuracy<\/p> \n<\/font>","84046796":"##  <p style=\"background-color:#E2ADC3;color:black;font-size:20px;text-align:center;border-radius:10px 10px;border:2px solid purple;\"> \u2728 models without resampling <\/p>"}}