{"cell_type":{"59a2e120":"code","d22ca0d3":"code","dcebe17f":"code","09076e72":"code","a283c282":"code","f098bf6e":"code","9a01ec45":"code","a2da5c4c":"code","35af58dd":"code","08cb1471":"code","377120c8":"code","8419e8a4":"code","aa2ebe19":"code","0b8ab4cf":"code","ebca898e":"code","42553399":"code","c5df064c":"code","a255abd5":"code","0a9b4375":"code","4d83b7b2":"code","08e20590":"code","78ca5dea":"code","95c5d18d":"code","e0be468c":"code","8b9858d8":"code","d6d6272e":"markdown","43364faa":"markdown","98bffa60":"markdown","1ed710c8":"markdown","3c5556e0":"markdown","d15811e8":"markdown","a698091b":"markdown","335bd843":"markdown","fe117015":"markdown","dde2c854":"markdown","320e0461":"markdown","812ef40a":"markdown","9c78593e":"markdown"},"source":{"59a2e120":"import os\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import hstack\nimport eli5\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display_html","d22ca0d3":"def prepare_sparse_features(path_to_train, path_to_test, path_to_site_dict,\n                           vectorizer_params, after_load_fn=None):\n    times = ['time%s' % i for i in range(1, 11)]\n    train_df = pd.read_csv(path_to_train, index_col='session_id', parse_dates=times)\n    test_df = pd.read_csv(path_to_test, index_col='session_id', parse_dates=times)\n    train_df = train_df.sort_values(by='time1')\n    \n    if after_load_fn is not None:\n        train_df = after_load_fn(train_df)\n        test_df = after_load_fn(test_df)\n    \n    with open(path_to_site_dict, 'rb') as f:\n        site2id = pickle.load(f)\n    id2site = {v:k for (k, v) in site2id.items()}\n    id2site[0] = 'unknown'\n    \n    sites = ['site%s' % i for i in range(1, 11)]\n    train_sessions = train_df[sites].fillna(0).astype('int').apply(lambda row: ' '.join([id2site[i] for i in row]), axis=1).tolist()\n    test_sessions = test_df[sites].fillna(0).astype('int').apply(lambda row: ' '.join([id2site[i] for i in row]), axis=1).tolist()\n    vectorizer = TfidfVectorizer(**vectorizer_params)\n    X_train = vectorizer.fit_transform(train_sessions)\n    X_test = vectorizer.transform(test_sessions)\n    y_train = train_df['target'].astype('int').values\n    \n    train_times, test_times = train_df[times], test_df[times]\n    \n    return X_train, X_test, y_train, vectorizer, train_times, test_times\n\ndef add_time_features(times, X_sparse, add_hour=True):\n    hour = times['time1'].apply(lambda ts: ts.hour)\n    morning = ((hour >= 7) & (hour <= 11)).astype('int').values.reshape(-1, 1)\n    day = ((hour >= 12) & (hour <= 18)).astype('int').values.reshape(-1, 1)\n    evening = ((hour >= 19) & (hour <= 23)).astype('int').values.reshape(-1, 1)\n    night = ((hour >= 0) & (hour <=6)).astype('int').values.reshape(-1, 1)\n    \n    objects_to_hstack = [X_sparse, morning, day, evening, night]\n    feature_names = ['morning', 'day', 'evening', 'night']\n    \n    if add_hour:\n        # we'll do it right and scale hour dividing by 24\n        objects_to_hstack.append(hour.values.reshape(-1, 1) \/ 24)\n        feature_names.append('hour')\n        \n    X = hstack(objects_to_hstack)\n    return X, feature_names\n\ndef add_day_month(times, X_sparse):\n    day_of_week = times['time1'].apply(lambda t: t.weekday()).values.reshape(-1, 1)\n    month = times['time1'].apply(lambda t: t.month).values.reshape(-1, 1) \n    # linear trend: time in a form YYYYMM, we'll divide by 1e5 to scale this feature \n    year_month = times['time1'].apply(lambda t: 100 * t.year + t.month).values.reshape(-1, 1) \/ 1e5\n    \n    objects_to_hstack = [X_sparse, day_of_week, month, year_month]\n    feature_names = ['day_of_week', 'month', 'year_month']\n        \n    X = hstack(objects_to_hstack)\n    return X, feature_names\n\ndef pre_process():\n    X_train_with_times1, new_feat_names = add_time_features(train_times, X_train_sites)\n    X_test_with_times1, _ = add_time_features(test_times, X_test_sites)\n    X_train_with_times1.shape, X_test_with_times1.shape\n\n    X_train_with_times2, new_feat_names = add_time_features(train_times, X_train_sites, add_hour=False)\n    X_test_with_times2, _ = add_time_features(test_times, X_test_sites, add_hour=False)\n    \n    train_durations = (train_times.max(axis=1) - train_times.min(axis=1)).astype('timedelta64[ms]').astype(int)\n    test_durations = (test_times.max(axis=1) - test_times.min(axis=1)).astype('timedelta64[ms]').astype(int)\n\n    scaler = StandardScaler()\n    train_dur_scaled = scaler.fit_transform(train_durations.values.reshape(-1, 1))\n    test_dur_scaled = scaler.transform(test_durations.values.reshape(-1, 1))\n    \n    X_train_with_time_correct = hstack([X_train_with_times2, train_dur_scaled])\n    X_test_with_time_correct = hstack([X_test_with_times2, test_dur_scaled])\n    \n    X_train_final, more_feat_names = add_day_month(train_times, X_train_with_time_correct)\n    X_test_final, _ = add_day_month(test_times, X_test_with_time_correct)    \n    \n    feat_names = new_feat_names + ['sess_duration'] + more_feat_names\n    \n    return X_train_final, X_test_final, feat_names\n\n# A helper function for writing predictions to a file\ndef write_to_submission_file(predicted_labels, out_file,\n                             target='target', index_label=\"session_id\"):\n    predicted_df = pd.DataFrame(predicted_labels,\n                                index = np.arange(1, predicted_labels.shape[0] + 1),\n                                columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)\n\n    \ndef train_and_predict(model, X_train, y_train, X_test, cv, site_feature_names, \n                      new_feature_names=None, scoring='roc_auc',\n                      top_n_features_to_show=30, submission_file_name='submission.csv'):\n    \n    \n    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, \n                            scoring=scoring, n_jobs=4)\n    print('CV scores', cv_scores)\n    print('CV mean: {}, CV std: {}'.format(cv_scores.mean(), cv_scores.std()))\n    model.fit(X_train, y_train)\n    \n    if new_feature_names:\n        all_feature_names = site_feature_names + new_feature_names \n    else: \n        all_feature_names = site_feature_names\n    \n    display_html(eli5.show_weights(estimator=model, \n                  feature_names=all_feature_names, top=top_n_features_to_show))\n    \n    if new_feature_names:\n        print('New feature weights:')\n    \n        print(pd.DataFrame({'feature': new_feature_names, \n                        'coef': model.coef_.flatten()[-len(new_feature_names):]}))\n    \n    test_pred = model.predict_proba(X_test)[:, 1]\n    write_to_submission_file(test_pred, submission_file_name) \n    \n    return cv_scores\n\n# A helper function for writing predictions to a file\ndef write_to_submission_file(predicted_labels, out_file,\n                             target='target', index_label=\"session_id\"):\n    predicted_df = pd.DataFrame(predicted_labels,\n                                index = np.arange(1, predicted_labels.shape[0] + 1),\n                                columns=[target])\n    predicted_df.to_csv(out_file, index_label=index_label)\n\n    \ndef train_and_predict(model, X_train, y_train, X_test, cv, site_feature_names, \n                      new_feature_names=None, scoring='roc_auc', show_eli=False,\n                      top_n_features_to_show=30, submission_file_name='submission.csv'):\n    \n    \n    cv_scores = cross_val_score(model, X_train, y_train, cv=cv, \n                            scoring=scoring, n_jobs=4)\n    print('CV scores', cv_scores)\n    print('CV mean: {}, CV std: {}'.format(cv_scores.mean(), cv_scores.std()))\n    model.fit(X_train, y_train)\n    \n    if new_feature_names:\n        all_feature_names = site_feature_names + new_feature_names \n    else: \n        all_feature_names = site_feature_names\n\n    if show_eli:\n        display_html(eli5.show_weights(estimator=model, \n                      feature_names=all_feature_names, top=top_n_features_to_show))\n    \n    if new_feature_names:\n        print('New feature weights:')\n    \n        print(pd.DataFrame({'feature': new_feature_names, \n                        'coef': model.coef_.flatten()[-len(new_feature_names):]}))\n    \n    test_pred = model.predict_proba(X_test)[:, 1]\n    write_to_submission_file(test_pred, submission_file_name) \n    \n    return cv_scores    ","dcebe17f":"PATH_TO_DATA = '..\/input\/'\nSEED = 17","09076e72":"time_split = TimeSeriesSplit(n_splits=10)\nlogit = LogisticRegression(C=1, random_state=SEED, solver='liblinear')","a283c282":"%%time\nX_train_sites, X_test_sites, y_train, vectorizer, train_times, test_times = prepare_sparse_features(\n    path_to_train=os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n    path_to_test=os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n    path_to_site_dict=os.path.join(PATH_TO_DATA, 'site_dic.pkl'),\n    vectorizer_params={'ngram_range': (1, 5), \n                       'max_features': 50000,\n                       'tokenizer': lambda s: s.split()}\n)\n\nX_train_final, X_test_final, new_feat_names = pre_process()","f098bf6e":"cv_scores6 = train_and_predict(model=logit, X_train=X_train_final, y_train=y_train,\n                               X_test=X_test_final, cv=time_split,\n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names,\n                               submission_file_name='subm6.csv')","9a01ec45":"c_values = np.logspace(-2, 2, 20)\nlogit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values}, scoring='roc_auc', n_jobs=4, cv=time_split, verbose=1)","a2da5c4c":"%%time\nlogit_grid_searcher.fit(X_train_final, y_train);\nlogit_grid_searcher.best_score_, logit_grid_searcher.best_params_","35af58dd":"final_model = logit_grid_searcher.best_estimator_","08cb1471":"cv_scores7 = train_and_predict(model=final_model, X_train=X_train_final, y_train=y_train, \n                               X_test=X_test_final, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names,\n                               cv=time_split, submission_file_name='subm7.csv')","377120c8":"cv_scores7 > cv_scores6","8419e8a4":"import re\nimport pickle\n\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\nsns.set()\n\nPATH = Path('..\/input\/')\n\ntimes = ['time%s' % i for i in range(1, 11)]\n\ndef plot_series(df, field, label, **kwargs):\n    df = df.copy()\n    df['date'] = pd.DatetimeIndex(df[field]).normalize()\n    df = (df['date'].value_counts()\/len(df)).sort_index().resample('1D').interpolate()\n    df.plot(label=label, **kwargs)\n    \ndf_train = pd.read_csv(PATH\/'train_sessions.csv', index_col='session_id', parse_dates=times)\ndf_test = pd.read_csv(PATH\/'test_sessions.csv', index_col='session_id', parse_dates=times)","aa2ebe19":"fig , (ax1,ax2) = plt.subplots(2,1,figsize = (16, 12 )) \nfig.suptitle('Year-Month Distributions', fontsize=16)\nsns.countplot((df_train.time1.dt.year * 100 + df_train.time1.dt.month).apply(str), ax=ax1)\nax1.set_title(\"Train distribution\") \nsns.countplot((df_test.time1.dt.year * 100 + df_test.time1.dt.month).apply(str), ax=ax2)\nax2.set_title(\"Test distribution\");","0b8ab4cf":"plot_series(df_train, 'time1', 'train-all', figsize=(24, 8))\nplot_series(df_train[df_train['target']==1], 'time1', 'train-alice')\nplot_series(df_test, 'time1', 'test')\nplt.legend();","ebca898e":"def fix_incorrect_date_formats(df, columns_to_fix):\n    for time in columns_to_fix:\n        d = df[time]\n        d_fix = d[d.dt.day <= 12]\n        d_fix = pd.to_datetime(d_fix.apply(str), format='%Y-%d-%m %H:%M:%S')\n        df.loc[d_fix.index.values, time] = d_fix\n    return df","42553399":"df_train_fixed = fix_incorrect_date_formats(df_train, times)\ndf_test_fixed = fix_incorrect_date_formats(df_test, times)\nplot_series(df_train_fixed, 'time1', 'train-all', figsize=(24, 8))\nplot_series(df_train_fixed[df_train_fixed['target']==1], 'time1', 'train-alice')\nplot_series(df_test_fixed, 'time1', 'test')\nplt.legend();","c5df064c":"fig , (ax1,ax2) = plt.subplots(1,2,figsize = ( 15 , 6 )) \nfig.suptitle('Year-Month Distributions', fontsize=16)\nsns.countplot((df_train_fixed.time1.dt.year * 100 + df_train_fixed.time1.dt.month).apply(str), ax=ax1)\nax1.set_title(\"Train distribution\") \nsns.countplot((df_test_fixed.time1.dt.year * 100 + df_test_fixed.time1.dt.month).apply(str), ax=ax2)\nax2.set_title(\"Test distribution\");","a255abd5":"%%time\nX_train_sites, X_test_sites, y_train, vectorizer, train_times, test_times = prepare_sparse_features(\n    after_load_fn=(lambda df: fix_incorrect_date_formats(df, times)), # Applying fix\n    path_to_train=os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n    path_to_test=os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n    path_to_site_dict=os.path.join(PATH_TO_DATA, 'site_dic.pkl'),\n    vectorizer_params={'ngram_range': (1, 5), \n                       'max_features': 50000,\n                       'tokenizer': lambda s: s.split()}\n)\n\nX_train_final, X_test_final, new_feat_names = pre_process()","0a9b4375":"time_split = TimeSeriesSplit(n_splits=10)\nlogit = LogisticRegression(C=1, random_state=SEED, solver='liblinear')","4d83b7b2":"cv_scores8 = train_and_predict(model=logit, X_train=X_train_final, y_train=y_train,\n                               X_test=X_test_final, cv=time_split,\n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names,\n                               submission_file_name='subm8.csv')","08e20590":"c_values = np.logspace(-2, 2, 20)\nlogit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values}, scoring='roc_auc', n_jobs=4, cv=time_split, verbose=1)","78ca5dea":"%%time\nlogit_grid_searcher.fit(X_train_final, y_train);\nlogit_grid_searcher.best_score_, logit_grid_searcher.best_params_","95c5d18d":"final_model = logit_grid_searcher.best_estimator_","e0be468c":"cv_scores9 = train_and_predict(model=final_model, X_train=X_train_final, y_train=y_train, \n                               X_test=X_test_final, \n                               site_feature_names=vectorizer.get_feature_names(),\n                               new_feature_names=new_feat_names,\n                               cv=time_split, submission_file_name='subm9.csv')","8b9858d8":"cv_scores9 > cv_scores8","d6d6272e":"Now, after the fix is applied, the data has nice distributions across the different months and all the spikes and gaps are gone. Bonus: there is a nice overlap between train and test datasets.\n\nAfter this transformation, you can use ```TimeSeriesSplit``` or ```StratifiedKFold```: both CV schemas should give a good correlation between local CV scores and the public leaderbords scores. And you can use CV to perform hyper-parameters tuning.","43364faa":"### Submission 6: local 0.913373+-0.0650 | 0.95062 pub","98bffa60":"### Submission 8: local 0.9052172+-0.102551 | 0.94843 pub","1ed710c8":"![image.png](attachment:image.png)\n\nThere are few patterns here:\n\n* Jan-Nov 2013 has data in the 12th day of each month and nothing else in the days 1-12.\n* Jan-Dec 2014 has data in the days 1-5 of each month and nothing else in the days 1-12.\n* Nov-Dec 2013 has some data in days 12+\n* Jan-May 2014 has some data in days 12+\n\nWhat is so special about number 12? Looks like the dataset has parsing error: we have dates in two formats: YYYY-MM-DD and YYYY-DD-MM.","3c5556e0":"### Let's try to fix it","d15811e8":"### Submission 7: local 0.9164614+-0.0641 | 0.95055 pub","a698091b":"We got a good boost here. The hyper-parameters tuning works now and local CV correlates with public leaderbord. Now, because we can trust our validation schema, it is possible to assess quality of submissions without submits to public leaderbord.","335bd843":"Unfortunately, the change dropped the score. But before we discard it as bad, let's check if hyper-parameters tuning will work.","fe117015":"# Fixing Cross-Validation","dde2c854":"### Submission 9: local 0.9099734+-0.09774 | 0.94922 pub","320e0461":"# Prior work\n\nThis section is the code from a [strong kernel made by yorko@](https:\/\/www.kaggle.com\/kashnitsky\/model-validation-in-a-competition).\nWe took it derectly to v6\/v7 outputs.","812ef40a":"# Testing the Same Models After the Dates Fix","9c78593e":"As we seen in prior work, tuning hyper-parameters helped only in 6 folds out of 10 and our public score dropped from 0.95062 to 0.95055 after hyper-parameters tuning. Before we'll start fixing things, let's look at the dates in the dataset."}}