{"cell_type":{"a05b2088":"code","30125fa2":"code","832b12b6":"code","9fbf5b6c":"code","f2084f2e":"code","eeb57112":"code","fe52d2e2":"code","9e189cff":"code","e928922e":"code","8e552859":"code","6c11158a":"code","edf71159":"code","b891d23b":"code","7075fda4":"code","b5c7baec":"code","8ef8efcf":"code","a9743395":"code","2be72452":"code","89e8cc45":"code","361714a9":"code","fff1a253":"code","11bdddf5":"code","40580b99":"code","ae1003bf":"code","d3e8ec6a":"code","897b2319":"code","514d720d":"code","de7143bd":"code","63e00bf9":"code","bf418ab2":"code","c0849e6c":"code","94a67b76":"code","1b9aa227":"code","31e6bc85":"code","d84749bc":"code","8eba7ce7":"markdown","68dd36fb":"markdown","4b554456":"markdown","9838ea9c":"markdown","602c5f79":"markdown","f5df56bf":"markdown","95b314fa":"markdown","0d8192ee":"markdown","d8ab0a32":"markdown","3ebe560b":"markdown","5e53b557":"markdown","50f24659":"markdown","e24fa739":"markdown","4f669cb7":"markdown","2f3cd2c9":"markdown","322b27fe":"markdown","11864bf1":"markdown","9d82b3ad":"markdown","75b102ba":"markdown","fcc4acd0":"markdown","c6f33981":"markdown","4072f96e":"markdown","e4d9c812":"markdown","10716251":"markdown","4ec2da45":"markdown"},"source":{"a05b2088":"Image(\"..\/input\/giffyy\/giphy.gif\")","30125fa2":"from IPython.display import Image\nimport os\n!ls ..\/input\/","832b12b6":"import pandas as pd\nimport numpy as np\nfrom pandas import DataFrame","9fbf5b6c":"train = pd.read_csv('..\/input\/friends-transcript\/friends_quotes.csv')","f2084f2e":"train","eeb57112":"train['word_count'] = train['quote'].apply(lambda x: len(str(x).split(\" \")))\ntrain[['quote','word_count']].head()","fe52d2e2":"train['char_count'] = train['quote'].str.len() ## this also includes spaces\ntrain[['quote','char_count']].head()","9e189cff":"def avg_word(sentence):\n  words = sentence.split()\n  return (sum(len(word) for word in words)\/len(words))\n\ntrain['avg_word'] = train['episode_title'].apply(lambda x: avg_word(x))\ntrain[['quote','avg_word']].head()","e928922e":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\n\ntrain['stopwords'] = train['quote'].apply(lambda x: len([x for x in x.split() if x in stop]))\ntrain[['quote','stopwords']].head()","8e552859":"train['hastags'] = train['quote'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\ntrain[['quote','hastags']].head()","6c11158a":"train['numerics'] = train['quote'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ntrain[['quote','numerics']].head()","edf71159":"train['upper'] = train['quote'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\ntrain[['quote','upper']].head()","b891d23b":"train['quote'] = train['quote'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ntrain['quote'].head()","7075fda4":"train['quote'] = train['quote'].str.replace('[^\\w\\s]','')\ntrain['quote'].head()","b5c7baec":"from nltk.corpus import stopwords\nstop = stopwords.words('english')\ntrain['quote'] = train['quote'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\ntrain['quote'].head()","8ef8efcf":"freq = pd.Series(' '.join(train['quote']).split()).value_counts()[:10]\nfreq","a9743395":"freq = list(freq.index)\ntrain['quote'] = train['quote'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrain['quote'].head()","2be72452":"freq = pd.Series(' '.join(train['quote']).split()).value_counts()[-10:]\nfreq","89e8cc45":"freq = list(freq.index)\ntrain['quote'] = train['quote'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\ntrain['quote'].head()","361714a9":"from textblob import TextBlob\ntrain['quote'][:5].apply(lambda x: str(TextBlob(x).correct()))","fff1a253":"TextBlob(train['quote'][1]).words","11bdddf5":"from nltk.stem import PorterStemmer\nst = PorterStemmer()\ntrain['quote'][:5].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))","40580b99":"from textblob import Word\ntrain['quote'] = train['quote'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\ntrain['quote'].head()","ae1003bf":"TextBlob(train['quote'][0]).ngrams(3)","d3e8ec6a":"tf1 = (train['quote'][1:2]).apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis = 0).reset_index()\ntf1.columns = ['words','tf']\ntf1","897b2319":"for i,word in enumerate(tf1['words']):\n  tf1.loc[i, 'idf'] = np.log(train.shape[0]\/(len(train[train['quote'].str.contains(word)])))\n\ntf1","514d720d":"tf1['tfidf'] = tf1['tf'] * tf1['idf']\ntf1","de7143bd":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=1000, lowercase=True, analyzer='word',\n stop_words= 'english',ngram_range=(1,1))\ntrain_vect = tfidf.fit_transform(train['quote'])\n\ntrain_vect","63e00bf9":"from sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(max_features=1000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\ntrain_bow = bow.fit_transform(train['quote'])\ntrain_bow","bf418ab2":"train['quote'][:5].apply(lambda x: TextBlob(x).sentiment)","c0849e6c":"train['sentiment'] = train['quote'].apply(lambda x: TextBlob(x).sentiment[0] )\ntrain[['quote','sentiment']].head()","94a67b76":"import matplotlib.pyplot as plt\nSentiment_count=train.groupby('sentiment').count()\nplt.bar(Sentiment_count.index.values, Sentiment_count['quote'])\nplt.xlabel('Review Sentiments')\nplt.ylabel('Number of Review')\nplt.show()","1b9aa227":"#WordCloud\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud, STOPWORDS\nplt.rcParams['font.size']= 15              \nplt.rcParams['savefig.dpi']= 100         \nplt.rcParams['figure.subplot.bottom']= .1","31e6bc85":"plt.figure(figsize=(15,15))\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(background_color='black', stopwords=stopwords, max_words=2000, max_font_size=80,\n                      random_state=420).generate(str(train['quote']))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.title(\"Friends [WordCloud]\")\nplt.axis('off')\nplt.show()","d84749bc":"plt.figure(figsize=(15,15))\nstopwords = set(STOPWORDS)\n\nwordcloud = WordCloud(background_color='black', stopwords=stopwords, max_words=2000, max_font_size=100,\n                      random_state=420).generate(str(train['quote']))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.imshow(wordcloud)\nplt.title(\"Friends <3 [WordCloud]\")\nplt.axis('off')\nplt.show()","8eba7ce7":"Removal of Stop Words\nAs we discussed earlier, stop words (or commonly occurring words) should be removed from the text data. For this purpose, we can either create a list of stopwords ourselves or we can use predefined libraries.","68dd36fb":"Inverse Document Frequency\nThe intuition behind inverse document frequency (IDF) is that a word is not of much use to us if it\u2019s appearing in all the documents.","4b554456":"Common word removal\nPreviously, we just removed commonly occurring words in a general sense. We can also remove commonly occurring words from our text data First, let\u2019s check the 10 most frequently occurring words in our text data then take call to remove or retain.","9838ea9c":"One of the most basic features we can extract is the number of words in each quote. The basic intuition behind this is that generally, the negative sentiments contain a lesser amount of words than the positive ones.","602c5f79":"term frequency\nTerm frequency is simply the ratio of the count of a word present in a sentence, to the length of the sentence.\n\nTherefore, we can generalize term frequency as:\n\nTF = (Number of times term T appears in the particular row) \/ (number of terms in that row)","f5df56bf":" Rare words removal\nSimilarly, just as we removed the most common words, this time let\u2019s remove rarely occurring words from the text. Because they\u2019re so rare, the association between them and other words is dominated by noise. You can replace rare words with a more general form and then this will have higher counts","95b314fa":"**When you watch friends, you just don't watch it, you live it. Every character has its own way to be funny. \n\nThis note book is my first experiement in NLP.**","0d8192ee":"Steaming","d8ab0a32":"Number of special characters\nOne more interesting feature which we can extract from a tweet is calculating the number of hashtags or mentions present in it. This also helps in extracting extra information from our text data.\n\nHere, we make use of the \u2018starts with\u2019 function because hashtags (or mentions) always appear at the beginning of a word.","3ebe560b":"Bag of Words\nBag of Words (BoW) refers to the representation of text which describes the presence of words within the text data. The intuition behind this is that two similar text fields will contain similar kind of words, and will therefore have a similar bag of words. Further, that from the text alone we can learn something about the meaning of the document.","5e53b557":"Number of Uppercase words\nAnger or rage is quite often expressed by writing in UPPERCASE words which makes this a necessary operation to identify those words.","50f24659":"-ngrams\nN-grams are the combination of multiple words used together. Ngrams with N=1 are called unigrams. Similarly, bigrams (N=2), trigrams (N=3) and so on can also be used.","e24fa739":"Sentiment Analysis\nIf you recall, our problem was to detect the sentiment of the tweet. So, before applying any ML\/DL models (which can have a separate feature detecting the sentiment using the textblob library), let\u2019s check the sentiment of the first few tweets.","4f669cb7":"Term Frequency \u2013 Inverse Document Frequency (TF-IDF)\nTF-IDF is the multiplication of the TF and IDF which we calculated above.","2f3cd2c9":"Number of characters\nThis feature is also based on the previous feature intuition. Here, we calculate the number of characters in each tweet. This is done by calculating the length of the quote.","322b27fe":"Now, let\u2019s remove these words as their presence will not of any use in classification of our text data.","11864bf1":"Lower case\nThe first pre-processing step which we will do is transform our tweets into lower case. This avoids having multiple copies of the same words. For example, while calculating the word count, \u2018Analytics\u2019 and \u2018analytics\u2019 will be taken as different words.","9d82b3ad":"Removing Punctuation\nThe next step is to remove punctuation, as it doesn\u2019t add any extra information while treating text data. Therefore removing all instances of it will help us reduce the size of the training data.","75b102ba":"Number of numerics\nJust like we calculated the number of words, we can also calculate the number of numerics which are present in the tweets. It does not have a lot of use in our example, but this is still a useful feature that should be run while doing similar exercises. For example, ","fcc4acd0":"We can see that the TF-IDF has penalized words like \u2018don\u2019t\u2019, \u2018can\u2019t\u2019, and \u2018use\u2019 because they are commonly occurring words. However, it has given a high weight to \u201cdisappointed\u201d since that will be very useful in determining the sentiment of the tweet.\n\nWe don\u2019t have to calculate TF and IDF every time beforehand and then multiply it to obtain TF-IDF. Instead, sklearn has a separate function to directly obtain it:","c6f33981":"Lemmatization","4072f96e":"Above, you can see that it returns a tuple representing polarity and subjectivity of each tweet. Here, we only extract polarity as it indicates the sentiment as value nearer to 1 means a positive sentiment and values nearer to -1 means a negative sentiment. This can also work as a feature for building a machine learning model.","e4d9c812":"Average Word Length\nWe will also extract another feature which will calculate the average word length of each quote. This can also potentially help us in improving our model.\n\nHere, we simply take the sum of the length of all the words and divide it by the total length of the quote:","10716251":"Number of stopwords\nGenerally, while solving an NLP problem, the first thing we do is to remove the stopwords. But sometimes calculating the number of stopwords can also give us some extra information which we might have been losing before.\n\nHere, we have imported stopwords from NLTK, which is a basic NLP library in python.","4ec2da45":"Spelling check"}}