{"cell_type":{"3fa8e7a8":"code","b4606731":"code","9a20af16":"code","e148010e":"code","21492cc0":"code","beedf6f3":"code","f0afb793":"code","c807e41e":"code","cba134de":"code","361fd0b2":"code","b7d234b3":"code","df7fdd50":"markdown","221f439a":"markdown","96a9c7c2":"markdown","ff11e283":"markdown","8d0fc884":"markdown","317f0602":"markdown","70e483d0":"markdown","ba7dc584":"markdown","aa1cdf89":"markdown","bf3cfe33":"markdown"},"source":{"3fa8e7a8":"!wget http:\/\/www.manythings.org\/anki\/fra-eng.zip","b4606731":"!unzip .\/fra-eng.zip","9a20af16":"import numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom tensorflow.keras.models import Model,Sequential\nfrom tensorflow.keras.layers import Dense,LSTM,Input,Embedding,TimeDistributed,RepeatVector\nfrom nltk.translate.bleu_score import SmoothingFunction,corpus_bleu\nsmoothie = SmoothingFunction().method4","e148010e":"data_path = '.\/fra.txt' # path of the file\nnum_sentences = 20000 # no of sentences from the dataset that we are going to use\n\n# opening the text file and getting the data \nwith open(data_path,'r') as f:\n    lines = f.read().split('\\n')\n\n    \nc=0 # to count the number of sentences\n\n# data cleaning\nsource_texts,target_texts = [],[]\nfor line in lines: # going through each lines\n    if c == num_sentences: # if we have 20000 sentences than we will get out of this loop\n        break \n    elif '\\t' in line:\n        op_data,ip_data,_ = line.lower().rstrip().split('\\t') # lowering the data and then spliting the data\n        \n        # to remove the punctuation we did not include last character\n        source_text = ip_data[:-1].strip()\n        target_text = op_data[:-1].strip()\n        # removing the unprintable character\n        # for english and french we will take anly alphabets of brespective languages and numbers\n        target_text = re.sub(\"[^a-z 1-9\\'-]\",\"\",target_text) \n        source_text = re.sub(\"[^a-z\u00e0\u00e2\u00e3\u00e7\u00e9\u00e8\u00ea\u00eb\u00ee\u00ef\u00f4\u0153\u00f9\u00fb\u00fc\u00ff 1-9\\'-]\",\"\",source_text) \n        \n        source_texts.append(source_text)\n        target_texts.append(target_text)\n        c+=1\n\n# train_test_split of the source and target data\nsource_train,source_test,target_train,target_test = train_test_split(source_texts,target_texts,test_size = 0.2, random_state= 0)","21492cc0":"# tokenizer for data\ndef create_tokenizer(texts):\n    tokenizer = Tokenizer(oov_token='<UNK>')\n    tokenizer.fit_on_texts(texts)\n    return tokenizer\n\n# one_hot encoding of the target data\ndef one_hot(pad_seq,max_sent_length,num_vocab):\n    target_data_one_hot = np.zeros((len(pad_seq),max_sent_length,num_vocab))\n    for i,w in enumerate(pad_seq):\n        for j,d in enumerate(w):\n            target_data_one_hot[i,j,d] = 1\n    return target_data_one_hot\n\n# for padding the data\ndef encoding_text(tokenizer,text,max_length):\n    text_seq = tokenizer.texts_to_sequences(text)\n    pad_seq = pad_sequences(text_seq,maxlen= max_length)\n    return pad_seq\n\n# to find the maximum length of the sentence from data\ndef max_length(text):\n    return max(len(l.split()) for l in text)\n    ","beedf6f3":"# preparing source tokenizer and getting relevant information\nsource_tokenizer = create_tokenizer(source_train)\nsource_vocab = source_tokenizer.word_index\nnum_source_vocab = len(source_vocab)+1\nmax_source_length = max_length(source_train)\n\n# preparing target tokenizer and getting relevant information\ntarget_tokenizer = create_tokenizer(target_train)\ntarget_vocab = target_tokenizer.word_index\nnum_target_vocab = len(target_vocab)+1\nmax_target_length = max_length(target_train)\n\n# preparing the training data\nsource_train_seq_pad = encoding_text(source_tokenizer,source_train,max_source_length) # padding of the source sentences\ntarget_train_seq_pad = encoding_text(target_tokenizer,target_train,max_target_length) # padding of the target sentences\ntarget_train_seq_pad = one_hot(target_train_seq_pad,max_target_length,num_target_vocab) # one hot encoding of the padded target senteces\n\n# preparing the test data\nsource_test_seq_pad = encoding_text(source_tokenizer,source_test,max_source_length) # padding of the source sentences\ntarget_test_seq_pad = encoding_text(target_tokenizer,target_test,max_target_length) # padding of the target sentences\ntarget_test_seq_pad = one_hot(target_test_seq_pad,max_target_length,num_target_vocab) # one hot encoding of the padded target senteces\n \nprint(num_source_vocab,num_target_vocab,max_source_length,max_target_length)","f0afb793":"model = Sequential()\nmodel.add(Input(shape=(max_source_length,)))\nmodel.add(Embedding(num_source_vocab,512,mask_zero=True))\nmodel.add(LSTM(512,return_sequences = False))\nmodel.add(RepeatVector(max_target_length))\nmodel.add(LSTM(512,return_sequences = True))\nmodel.add(TimeDistributed(Dense(num_target_vocab,activation = 'softmax')))\n\nmodel.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics=['acc'])\n\nmodel.summary()\n\nes = EarlyStopping(monitor='val_acc',patience= 5,min_delta=0.01) # EarlyStoping callback to stop the fitting before all epochs\nfilepath = '.\/fre2eng.h5' # filepath required for checkpoint\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max') # ModelCheckPoint to save the best model\n\nhistory = model.fit(source_train_seq_pad, target_train_seq_pad, \n                    epochs= 50,\n                    batch_size=64, \n                    validation_data = (source_test_seq_pad,target_test_seq_pad), \n                    verbose=1,\n                    callbacks=[checkpoint,es])","c807e41e":"# loading the weights from the best saved model\nmodel.load_weights(filepath)","cba134de":"# a dictionary having key is a token number for a particular word and value is a word\n# this will required to decode the predicted sequence\ntarget_vocab_idx = {v:k for k,v in target_tokenizer.word_index.items()}\n\n# function to predict the decoded sequence\ndef predict_sequence(model,sent,vocab_idx):\n    prediction = model.predict(sent.reshape(1,max_source_length))[0]\n    integers = [np.argmax(vector) for vector in prediction]\n    target = []\n    for i in integers:\n        if i != 0:\n            word = vocab_idx[i]\n            if word is None:\n                break\n            target.append(word)\n            \n    return ' '.join(target)\n\n# for evaluation of the model through BLEU_score\ndef bleu_score(model,ip,ip_raw,op_raw,vocab_idx):\n    \n    prediction,actual = [],[]\n    for i,sent in enumerate(ip):\n        \n        if i%10 == 0: # to print the progress\n            print('\\rprogress ',(i+1)*100\/\/len(ip),'%',sep='',end='',flush = True)\n        \n        translation = predict_sequence(model,sent,vocab_idx)\n        \n        prediction.append(translation)\n        actual.append(op_raw[i])\n    \n    print()\n    # printing the first ten sentences\n    for i in range(10):\n        print('French_sentence -',ip_raw[i],' | ',\n            'English_actual_sentence -',op_raw[i],' | ',\n            'English_predicted_sentence -',prediction[i])\n    \n    print()\n    # printing the BLEU_score\n    print('BLEU_SCORE')\n    print('BLEU score-1: %f' % corpus_bleu(actual, prediction, weights=(1.0, 0, 0, 0),smoothing_function=smoothie,auto_reweigh=False))\n    print('BLEU score-2: %f' % corpus_bleu(actual, prediction, weights=(0.5, 0.5, 0, 0),smoothing_function=smoothie,auto_reweigh=False))\n    print('BLEU score-3: %f' % corpus_bleu(actual, prediction, weights=(0.3, 0.3, 0.3, 0),smoothing_function=smoothie,auto_reweigh=False))\n    print('BLEU score-4: %f' % corpus_bleu(actual, prediction, weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=smoothie,auto_reweigh=False))","361fd0b2":"bleu_score(model,source_train_seq_pad,source_train,target_train,target_vocab_idx)","b7d234b3":"bleu_score(model,source_test_seq_pad,source_test,target_test,target_vocab_idx)","df7fdd50":"### Opening the dataset","221f439a":"### Importing required libraries","96a9c7c2":"### Making the required functions for the data preprocessing","ff11e283":"### Data cleaning and train_test_split","8d0fc884":"### Preparing training and testing data","317f0602":"### Evaluating the model on testing dataset","70e483d0":"### Preparing and running the Autoencoder model ","ba7dc584":"### Evaluating the model on training dataset","aa1cdf89":"### Making the functions to predict the sequence and BLEU_sccore","bf3cfe33":"### Downloading the dataset"}}