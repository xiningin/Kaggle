{"cell_type":{"2c2bc0dc":"code","6572a756":"code","e7d0ba68":"code","a560147e":"code","e501fc22":"code","5ca278b4":"code","bd210470":"code","4a7a9c98":"code","a6f97036":"code","03a6b7d9":"code","a73817a5":"code","6d5d37f5":"code","ee7d8eaa":"code","dedcd4c4":"code","06b86208":"code","1b89969b":"markdown","14a8817b":"markdown","adeaaff2":"markdown","12e3936b":"markdown","fb341a5b":"markdown","98ca7180":"markdown","f71ca6ad":"markdown","4ff09d39":"markdown","3af676d1":"markdown","fb4c6f2a":"markdown","1f034969":"markdown","c8a5064c":"markdown","378364e1":"markdown"},"source":{"2c2bc0dc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6572a756":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import RobustScaler\ndf = pd.read_csv('..\/input\/creditcardfraud\/version\/3.csv')\ndf.head()","e7d0ba68":"print(df['Class'].sum())\nprint(df['Class'].count() - df['Class'].sum())","a560147e":"scaler = RobustScaler()\ndf['Amount'] = scaler.fit_transform(df['Amount'].values.reshape(-1, 1))\ndf['Time'] = scaler.fit_transform(df['Time'].values.reshape(-1, 1))\ndf.head()","e501fc22":"from sklearn.model_selection import StratifiedShuffleSplit\ndef stratified_shuffle_split(X, y):\n    stratifiedShuffleSplit = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n    for train_index, test_index in stratifiedShuffleSplit.split(X, y):\n        print(\"N\u00famero de Transa\u00e7\u00f5es:\\nTreino: \", len(train_index), \"\\nTeste: \", len(test_index))\n        X_train, X_test = X[train_index], X[test_index]\n        y_train, y_test = y[train_index], y[test_index]\n        print(\"\\nN\u00famero de Fraudes:\\nTreino: \", np.sum(y_train), \"\\nTeste: \", np.sum(y_test))\n    return X_train, X_test, y_train, y_test","5ca278b4":"from sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\ndef get_measures(X_train, X_test, y_train, y_test, y_pred):\n    print(\"\\nAcur\u00e1cia (Treino)\", clf.score(X_train, y_train))\n    print(\"Acur\u00e1cia (Teste)\", clf.score(X_test, y_test))","bd210470":"from sklearn.metrics import confusion_matrix\ndef show_confusion_matrix(y_test, y_pred):\n    print(\"\\n\", confusion_matrix(y_test, y_pred))","4a7a9c98":"from sklearn.metrics import classification_report\nfrom sklearn import tree\nfrom sklearn.tree import export_text\ndef run_classifier(clf, X, y):\n    #divide os dados entre treino e teste\n    X_train, X_test, y_train, y_test = stratified_shuffle_split(X, y)\n    #executa o classificador\n    clf = clf.fit(X_train, y_train)\n    #predi\u00e7\u00e3o das classes para os dados de teste\n    y_pred = clf.predict(X_test)\n    #exibe algumas m\u00e9tricas\n    #get_measures(X_train, X_test, y_train, y_test, y_pred)\n    print(classification_report(y_test, y_pred))\n    print(clf.get_params())\n    #exibe confusion matriz\n    show_confusion_matrix(y_test, y_pred)\n    #print(export_text(clf))","a6f97036":"#cria as vari\u00e1veis\nX = df.drop('Class', axis=1).values\ny = df['Class'].values","03a6b7d9":"from sklearn import tree\nfrom sklearn.linear_model import LogisticRegression\n\nclf = LogisticRegression(class_weight=\"balanced\")\nrun_classifier(clf, X, y)\n\n#instancia a \u00e1rvore de decis\u00e3o\nclf = tree.DecisionTreeClassifier()\nrun_classifier(clf, X, y)","a73817a5":"from sklearn import tree\n\n#instancia a \u00e1rvore de decis\u00e3o\nclf = tree.DecisionTreeClassifier()\nrun_classifier(clf, X, y)","6d5d37f5":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier(class_weight=\"balanced\")\nrun_classifier(clf, X, y)","ee7d8eaa":"from sklearn.ensemble import AdaBoostClassifier\n\nclf = AdaBoostClassifier(n_estimators=100, random_state=0, class_weight=\"balanced\")\nrun_classifier(clf, X, y)","dedcd4c4":"from sklearn.ensemble import GradientBoostingClassifier\n\nclf = GradientBoostingClassifier(min_samples_leaf=10, class_weight=\"balanced\")\nrun_classifier(clf, X, y)","06b86208":"import xgboost as xgb\n\nclf = xgb.XGBClassifier(class_weight=\"balanced\")\nrun_classifier(clf, X, y)","1b89969b":"1.5 - Combinando \u00e1rvores de decis\u00e3o","14a8817b":"> **Exibi\u00e7\u00e3o dos dados e explica\u00e7\u00e3o sobre a base de dados**","adeaaff2":"# Aula 4 -  Gradient Boosting\n\n4.1 - Entendendo o Gradient Boosting\n\n4.2 - Aplicando o Gradient Boosting\n","12e3936b":"5.5 - Outros aspectos do XGBoost\n\n5.6 - Conclus\u00e3o","fb341a5b":"*Principais par\u00e2metros para explorar: learning rate, n\u00famero de \u00e1rvores, profundidade m\u00e1xima da \u00e1rvore, n\u00famero m\u00ednimo de samples em uma folha.*","98ca7180":"# Aula 3 - Boosting\n3.1 - O que \u00e9 Boosting?\n\n3.2 - Entendendo o AdaBoost\n\n3.3 - Aplicando o AdaBoost\n","f71ca6ad":"> **Normaliza\u00e7\u00e3o de algumas features**","4ff09d39":"# Aula 2 - Bagging\n2.1 - O que \u00e9 Bagging?\n\n2.2 - Entendendo o Random Forest\n\n2.3 - Aplicando o Random Forest","3af676d1":"# Aula 1 - \u00c1rvores de Decis\u00e3o\n1.1 - Introdu\u00e7\u00e3o\n\n1.2 - O que \u00e9 uma \u00e1rvore de decis\u00e3o?\n\n1.3 - Analisando os nossos dados\n","fb4c6f2a":"1.4 - Aplicando um algoritmo de \u00e1rvore de decis\u00e3o","1f034969":"# Aula 5 - Extreme Gradient Boosting\n5.1 - Entendendo o XGBoost\n\n5.2 - Lidando com o overfitting\n\n5.3 - Dividindo a \u00e1rvore\n\n5.4 - Aplicando o XGBoost","c8a5064c":"N\u00famero de transa\u00e7\u00f5es com fraude\n\nN\u00famero de transa\u00e7\u00f5es sem fraude","378364e1":"> **M\u00e9todo para chamar o *stratifiedShuffleSplit() para dividir as inst\u00e2ncias de treino e teste***\n"}}