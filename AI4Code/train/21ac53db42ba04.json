{"cell_type":{"6a4526c7":"code","07c1df8a":"code","1480ffe6":"code","d6e97867":"code","8fd10176":"code","6ddb6c4d":"code","e3cce9df":"code","7ae75124":"code","586637a7":"code","507a34e8":"code","7300418a":"code","350d2d0c":"code","b57ad94f":"code","74cdb7c9":"code","59878b31":"code","c543c14b":"code","615c8108":"code","a423f337":"code","3a1ff248":"code","74567fe6":"code","e71eeaf5":"code","97c00eb3":"code","866ea611":"code","8ae91d63":"code","3a201f9b":"markdown","1c642d5c":"markdown"},"source":{"6a4526c7":"%matplotlib inline\nimport os\nimport pandas as pd\nfrom glob import glob\nimport numpy as np\nfrom fastai import *\nfrom fastai.vision import *\nimport torch \nimport matplotlib.pyplot as plt","07c1df8a":"path = Path('..\/input\/aptos2019-blindness-detection')\npath_train = path\/'train_images'\npath_test = path\/'test_images'\npath, path_train, path_test","1480ffe6":"labels = pd.read_csv(path\/'train.csv')\nlabels.head()","d6e97867":"img = open_image(path_train\/'000c1434d8d7.png')\nimg.show(figsize = (7,7))\nprint(img.shape)","8fd10176":"labels['diagnosis'].value_counts().plot(kind = 'bar', title='Distribution of diagnosis categories')\nplt.show()","6ddb6c4d":"tfms = get_transforms(\n    do_flip=True,\n    flip_vert=True,\n    max_warp=0.1,\n    max_rotate=66.,\n    max_zoom=1.1,\n    max_lighting=0.1,\n    p_lighting=0.5\n)\naptos19_stats = ([0.42, 0.22, 0.075], [0.27, 0.15, 0.081])","e3cce9df":"test_labels = pd.read_csv(path\/'sample_submission.csv')\ntest = ImageList.from_df(test_labels, path = path_test, suffix = '.png')","7ae75124":"src = (ImageList.from_df(labels, path = path_train, suffix = '.png')\n       .split_by_rand_pct(seed = 42)\n       .label_from_df(cols = 'diagnosis')\n       .add_test(test))","586637a7":"data = (\n    src.transform(\n        tfms,\n        size = 128, \n        resize_method=ResizeMethod.SQUISH,\n        padding_mode='zeros'\n    )\n    .databunch(bs=32)\n    .normalize(aptos19_stats))","507a34e8":"data.show_batch(3, figsize = (7,7))","7300418a":"print(data.classes)\nprint(len(data.train_ds))\nprint(len(data.valid_ds))\nprint(len(data.test_ds))","350d2d0c":"!mkdir models\n!mkdir -p \/tmp\/.cache\/torch\/checkpoints\n!cp ..\/input\/resnet34\/resnet34.pth \/tmp\/.cache\/torch\/checkpoints\/resnet34-333f7ec4.pth","b57ad94f":"kappa = KappaScore()\nkappa.weights = \"quadratic\"","74cdb7c9":"learn = cnn_learner(\n    data, \n    models.resnet34, \n    metrics = [accuracy, kappa], \n    model_dir = Path('..\/kaggle\/working'),\n    path = Path(\".\")\n)","59878b31":"learn.fit_one_cycle(15)\nlearn.save('resnet34')","c543c14b":"learn.load('resnet34')\nlearn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()\nlearn.fit_one_cycle(20, slice(1e-6,5e-4))\nlearn.save('resnet34-1')","615c8108":"# learn.load('resnet152-2')\n# data = (\n#     src.transform(\n#         tfms,\n#         size = 1024, \n#         resize_method=ResizeMethod.SQUISH,\n#         padding_mode='zeros'\n#     )\n#     .databunch(bs=4)\n#     .normalize(aptos19_stats))\n# learn.data = data\n# learn.freeze()\n# learn.lr_find()\n# learn.recorder.plot()\n# learn.fit_one_cycle(4, 2e-4)\n# learn.save('resnet152-3')","a423f337":"# learn.load('resnet152-3')\n# learn.unfreeze()\n# learn.lr_find()\n# learn.recorder.plot()\n# learn.fit_one_cycle(6, 2e-5)\n# learn.save('resnet152-4')\n","3a1ff248":"learn.load('resnet34-1')\npreds, _ = learn.get_preds(ds_type=DatasetType.Test)","74567fe6":"submission = pd.read_csv(path\/'sample_submission.csv')\nsubmission.head()","e71eeaf5":"preds = np.array(preds.argmax(1)).astype(int).tolist()\npreds[:5]","97c00eb3":"submission['diagnosis'] = preds\nsubmission.head()","866ea611":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","8ae91d63":"submission.to_csv('submission.csv', index = False)","3a201f9b":"## Preparing Submission","1c642d5c":"## Double the size of images"}}