{"cell_type":{"28bb85e9":"code","7a2f88fe":"code","bd295ff6":"code","36f025c1":"code","52d5e466":"code","55625476":"code","89a3b50a":"code","93bdb2cd":"code","cb927960":"code","517a2491":"code","dbfe6852":"code","73b51b49":"code","4e67182a":"code","eb6635a9":"code","1745f45b":"code","624d9fe1":"code","ca5b74d7":"code","f9fd34ea":"code","d88fa00a":"code","0cfd6880":"code","8abcad2b":"code","73bd4f88":"code","13e6e376":"code","a9d1c473":"code","3227ecf7":"code","3cb68f8f":"code","15ac4e66":"code","8a2281a0":"code","30172b44":"code","49686eeb":"code","88783c83":"code","ca220446":"code","f03de9c7":"code","36e04c71":"code","6153a257":"code","21d4d8d0":"code","51a721de":"code","a8c03da4":"code","4000239a":"code","3d4660c3":"code","b3d13be8":"code","158c9fa8":"code","e05190aa":"code","7b2ebf12":"code","c2a96d2e":"code","c5560f5b":"code","1b2125ba":"code","93ab5ca3":"code","d650e383":"code","8f168bfd":"code","c23665da":"code","0dd768c1":"code","953b19c8":"code","b77e14cb":"code","42a90866":"code","32c028ae":"code","a134b5c5":"code","daa433be":"code","9e38cdb0":"code","2e0a9207":"code","68c93a72":"code","f458bfc4":"code","029917ca":"code","c7b8976e":"code","4ef19452":"code","7cadda63":"code","c702d70d":"code","a12dd2a7":"code","526ee14e":"code","f9a7ee21":"code","e6696de2":"code","52075418":"code","fbd5461d":"code","d32db7ba":"code","aa78598d":"code","08e2b946":"code","d2a2dfeb":"code","b398e96c":"code","3b528ffd":"code","8bb0e064":"code","c041b901":"code","e6109b94":"code","0ed86f44":"code","4f44c8a7":"code","60433243":"code","3f6c1825":"code","eb46e8e8":"code","9838a7a5":"code","3f85bdac":"code","5ba35ca5":"code","49399fb5":"code","d8c46495":"code","6910f3cc":"code","ad3212b0":"code","7dcaaabb":"code","d96ab4c3":"code","c5513fe5":"code","60237cac":"code","e2a86d83":"code","a46e6696":"code","9bae4ecc":"markdown","4d67c89f":"markdown","efd02af0":"markdown","343f2451":"markdown","f7a8a70d":"markdown","2dccf0c0":"markdown","154a45e2":"markdown"},"source":{"28bb85e9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport warnings\nwarnings.simplefilter(action = 'ignore')\n\n\n# Standardization\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n# Train-Test split\nfrom sklearn.model_selection import train_test_split \n\n# Importing classification report and confusion matrix from sklearn metrics\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\n# Importing the PCA module\nfrom sklearn.decomposition import PCA\n\n# Importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Importing the below library and configuring to display all columns in a dataframe\nfrom IPython.display import display\npd.options.display.max_columns = None\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a2f88fe":"##Loading the data\ntraindata = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\ntestdata =pd.read_csv('\/kaggle\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')","bd295ff6":"print(traindata.shape)\nprint(testdata.shape)","36f025c1":"\n\ntraindata.isnull().sum()","52d5e466":"traindata.dtypes","55625476":"null_cols_90 = traindata.columns[round(traindata.isnull().sum()\/len(traindata.index)*100,2) > 90].tolist()\nprint(len(null_cols_90))","89a3b50a":"null_cols_80 = traindata.columns[round(traindata.isnull().sum()\/len(traindata.index)*100,2) > 80].tolist()\nprint(len(null_cols_80))","93bdb2cd":"#  columns that has missing values\nnum_null_cols = traindata.columns[traindata.isnull().any()].tolist()\nprint(num_null_cols)\nprint(\"number of null colums\",len(num_null_cols))","cb927960":"##outlier detection\n##multi collineraity check\n\nnum_null_cols_1 = num_null_cols[:19]\nnum_null_cols_2 = num_null_cols[19:38]\nnum_null_cols_3 = num_null_cols[38:57]\nnum_null_cols_4 = num_null_cols[57:76]\nnum_null_cols_5 = num_null_cols[76:95]\nnum_null_cols_6 = num_null_cols[95:114]\nnum_null_cols_7 = num_null_cols[114:133]\nnum_null_cols_8= num_null_cols[133:160]\n\n","517a2491":"num_cols_mean=[]\nnum_cols_median=[]\n","dbfe6852":"# Visualizing first part of num_null_cols which is num_null_cols_1 using box_plot\nplt.figure(figsize=(40,15))\nsns.boxplot(data=traindata[num_null_cols_1])\nplt.show()\n\n# This group can be imputed by median as ll have outliers","73b51b49":"traindata[num_null_cols_1].describe().T","4e67182a":"plt.figure(figsize=(40,15))\nsns.boxplot(data=traindata[num_null_cols_2])\nplt.show()","eb6635a9":"##'hematocrit_apache', 'map_apache'\n\ntraindata[num_null_cols_2].describe().T","1745f45b":"plt.figure(figsize=(40,15))\nsns.boxplot(data=traindata[num_null_cols_3])\nplt.show()\n","624d9fe1":"traindata[num_null_cols_3].describe().T","ca5b74d7":"plt.figure(figsize=(40,15))\nsns.boxplot(data=traindata[num_null_cols_4])\nplt.show()","f9fd34ea":"traindata[num_null_cols_4].describe().T","d88fa00a":"plt.figure(figsize=(40,15))\nsns.boxplot(data=traindata[num_null_cols_5])\nplt.show()","0cfd6880":"traindata[num_null_cols_5].describe().T","8abcad2b":"plt.figure(figsize=(40,15))\nsns.boxplot(data=traindata[num_null_cols_6])\nplt.show()","73bd4f88":"traindata[num_null_cols_6].describe().T","13e6e376":"plt.figure(figsize=(40,15))\nsns.boxplot(data=traindata[num_null_cols_7])\nplt.show()","a9d1c473":"traindata[num_null_cols_7].describe().T","3227ecf7":"plt.figure(figsize=(40,15))\nsns.boxplot(data=traindata[num_null_cols_8])\nplt.show()","3cb68f8f":"traindata[num_null_cols_8].describe().T","15ac4e66":"traindata.head()","8a2281a0":"pd.DataFrame(traindata.isnull().sum().sort_values(ascending=False), columns = [\"Values Missing\"])\n##sort_values()","30172b44":"cat_col = traindata.select_dtypes('object').columns\nprint(len(cat_col))\nprint(cat_col)","49686eeb":"traindata[cat_col].describe().T","88783c83":"traindata[cat_col].isnull().sum()","ca220446":"\ntraindata=traindata.drop_duplicates()","f03de9c7":"traindata.shape","36e04c71":"## drop data which has more than 90% blanks\nprint(traindata.shape) \nprint(testdata.shape) \ntraindata = traindata.drop(null_cols_90,axis = 1) \ntestdata = testdata.drop(null_cols_90,axis = 1) \nprint(traindata.shape) \nprint(testdata.shape)","6153a257":"## few other columns that  dont seem to affect the outcome\nprint(traindata.shape)\ntraindata = traindata.drop(['hospital_admit_source','icu_admit_source','icu_stay_type','icu_type'],axis=1)\ntestdata = testdata.drop(['hospital_admit_source','icu_admit_source','icu_stay_type','icu_type'],axis=1)\nprint(traindata.shape)\nprint(testdata.shape)","21d4d8d0":"cat_col = cat_col.drop(['hospital_admit_source','icu_admit_source','icu_stay_type','icu_type'])","51a721de":"print(cat_col)","a8c03da4":"## Feature engineering\n## Step1: Categorical variable: gender and ethnicity","4000239a":"print(traindata.shape)\nprint(testdata.shape)","3d4660c3":"traindata['gender'].fillna(value =traindata['gender'].mode()[0], inplace =True)\ntraindata['gender'].value_counts()\n\n","b3d13be8":"traindata['ethnicity'].value_counts()","158c9fa8":"traindata['ethnicity'].fillna(value =traindata['ethnicity'].mode()[0], inplace =True)\ntraindata['ethnicity'].value_counts()\n","e05190aa":"testdata['gender'].fillna(value =testdata['gender'].mode()[0], inplace =True)\ntestdata['ethnicity'].fillna(value =testdata['ethnicity'].mode()[0], inplace =True)\n","7b2ebf12":"print(traindata.shape)\nprint(testdata.shape)","c2a96d2e":"# Using  Label Encoder for gender and ethncity\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nfor ob in cat_col:\n    traindata[ob] = le.fit_transform(traindata[ob].astype(str))\n    testdata[ob] = le.fit_transform(testdata[ob].astype(str))\nprint(traindata.info())    \nprint(testdata.info())","c5560f5b":"## few other that can be dropped as they dont seem related to the target varaiable\n               \n\nprint(traindata.shape)\ntraindata = traindata.drop(['Unnamed: 0','hospital_id','icu_id','pre_icu_los_days','readmission_status'],axis=1)\nprint(traindata.shape)\n\n\nprint(testdata.shape)\ntestdata = testdata.drop(['Unnamed: 0','hospital_id','icu_id','pre_icu_los_days','readmission_status'],axis=1)\nprint(testdata.shape)","1b2125ba":"pd.DataFrame(traindata.isnull().sum().sort_values(ascending=False), columns = [\"Values Missing\"])\n##sort_values()","93ab5ca3":"## age, height and weight, BMI, fillup with mean value\n## rest we can do median imputation","d650e383":"\ntraindata['bmi'].fillna(value = traindata['bmi'].mean(), inplace=True)\ntraindata['weight'].fillna(value = traindata['weight'].mean(), inplace=True)\ntraindata['height'].fillna(value = traindata['height'].mean(), inplace=True)\ntraindata['age'].fillna(value = traindata['age'].mean(), inplace=True)\n\ntestdata['bmi'].fillna(value = testdata['bmi'].mean(), inplace=True)\ntestdata['weight'].fillna(value = testdata['weight'].mean(), inplace=True)\ntestdata['height'].fillna(value = testdata['height'].mean(), inplace=True)\ntestdata['age'].fillna(value = testdata['age'].mean(), inplace=True)\n","8f168bfd":"traindata.isnull().sum().head(10)","c23665da":"##Load X & Y\n\nX = traindata.drop(\"diabetes_mellitus\" , axis=1)\n\nY = traindata.pop(\"diabetes_mellitus\")","0dd768c1":"## fill it up with median\n\n# imputing missing values\nX= X.fillna(traindata.median())\nprint(X.isnull().sum())\n","953b19c8":"testdata = testdata.fillna(testdata.median())\nprint(testdata.isnull().sum())","b77e14cb":"# Check the data for missing values, outliers \nmetric= X.describe().transpose()\nmetric\n","42a90866":" X['age'].value_counts()\n    ","32c028ae":"##drop rows that has all nan colums\ntraindata= traindata.dropna(how='all')\ntraindata.shape\n\ntestdata= testdata.dropna(how='all')\ntestdata.shape","a134b5c5":"traindata = traindata.reset_index()","daa433be":"traindata.replace([np.inf, -np.inf], np.nan, inplace=True)","9e38cdb0":"X.head()","2e0a9207":"X.isnull().sum()","68c93a72":"Y.head()","f458bfc4":"Y.isnull().sum()","029917ca":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.30, random_state=1)","c7b8976e":"from sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndt_model = tree.DecisionTreeClassifier(criterion = 'gini' ,random_state = 1)\n##from sklearn.tree import DecisionTreeClassifier\n##tree = DecisionTreeClassifier(random_state=0)","4ef19452":"dt_model.fit(X_train,Y_train)\n","7cadda63":"dt_model.feature_importances_ \n","c702d70d":"pd.Series(dt_model.feature_importances_, index = X_train.columns).sort_values(ascending = False)\n","a12dd2a7":"\ndt_model.score(X_test, Y_test)   #overfitting model train score > test sco","526ee14e":"dt_model.score(X_test, Y_test)   #overfitting model train score > test sco","f9a7ee21":"Y_pred_train = dt_model.predict(X_train)\nY_pred_test = dt_model.predict(X_test)","e6696de2":"from sklearn.metrics import confusion_matrix, classification_report","52075418":"confusion_matrix(Y_train,Y_pred_train).ravel()        # ravel is to get values as array","fbd5461d":"tn, fp , fn, tp = confusion_matrix(Y_train,Y_pred_train).ravel()  ","d32db7ba":"confusion_matrix(Y_train,Y_pred_train)","aa78598d":"print(classification_report(Y_train,Y_pred_train))","08e2b946":"confusion_matrix(Y_test,Y_pred_test)","d2a2dfeb":"print(classification_report(Y_test,Y_pred_test))","b398e96c":"X_train.info()","3b528ffd":"X_test.info()","8bb0e064":"reg_dt_model  = tree.DecisionTreeClassifier(criterion='gini',max_depth = 1000, min_samples_leaf = 2500,min_samples_split = 5000)","c041b901":"reg_dt_model.fit(X_train, Y_train)","e6109b94":"reg_dt_model.score(X_train,Y_train)","0ed86f44":"reg_dt_model.score(X_test,Y_test)","4f44c8a7":"from sklearn.model_selection import GridSearchCV","60433243":"dt_grid_model = tree.DecisionTreeClassifier()","3f6c1825":"param_grid_values = {\n    'max_depth' : [500, 800, 1000,1200],\n    'min_samples_leaf' : [2200, 2500, 2700],\n    'min_samples_split' : [4400, 5000, 5400, 5800]\n}","eb46e8e8":"grid_search = GridSearchCV(estimator = dt_grid_model, param_grid = param_grid_values , cv = 10, scoring = 'accuracy')","9838a7a5":"grid_search.fit(X_train, Y_train)","3f85bdac":"grid_search.best_params_","5ba35ca5":"best_dt_model = grid_search.best_estimator_","49399fb5":"best_dt_model.score(X_train,Y_train)","d8c46495":"best_dt_model.score(X_test,Y_test)","6910f3cc":"print(testdata.shape)\nprint(X.shape)","ad3212b0":"# calculate the fpr and tpr for all thresholds of the classification\nimport sklearn.metrics as metrics\n\n##from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n##from sklearn.datasets import make_regression\n\nprobs = best_dt_model.predict_proba(testdata)\n\npreds = probs[:,1]\n\nprint(preds)\n##fpr, tpr, threshold = metrics.roc_curve(Y_test, preds)\n##roc_auc = metrics.auc(fpr, tpr)","7dcaaabb":"diabetes_prediction = pd.DataFrame({'diabetes_mellitus': probs[:, 1]})\ndiabetes_prediction.head()","d96ab4c3":"\ndiabetes_encounter = testdata[[\"encounter_id\"]]","c5513fe5":"diabetes_prediction_final = pd.concat([testdata['encounter_id'].reset_index(drop=True), diabetes_prediction], axis=1)\ndiabetes_prediction_final","60237cac":"import xgboost\nfrom sklearn import svm\nfrom numpy import loadtxt\nfrom xgboost import XGBClassifier","e2a86d83":"XGBmodel = xgboost.XGBClassifier()\nXGBmodel.fit(X_train, Y_train)\nY_pred= XGBmodel.predict(X_test)\nacc = accuracy_score(Y_test, Y_pred)\nprint(\"Accuracy of XGB is %s\"%(acc))\ncm = confusion_matrix(Y_test, Y_pred)\nprint(\"Confusion Matrix of XGB is %s\"%(cm))","a46e6696":"probs = XGBmodel.predict_proba(testdata)\n\npreds = probs[:,1]\n\nprint(preds)","9bae4ecc":"np.any(np.isnan(traindata))","4d67c89f":"Check how many columns have more than 90 %","efd02af0":"## Data Preprocessing ##","343f2451":"## Imputation ##","f7a8a70d":"It is large dataset. to understand this better. I am splitting them based on the columns that has null value to to check on outlier. so that i can mean, median impute.","2dccf0c0":"For categorical variable we can drop icu related details as it adds no value\nfor ethincity and gender we can do mode imputation","154a45e2":"traindata.dtypes.value_counts()"}}