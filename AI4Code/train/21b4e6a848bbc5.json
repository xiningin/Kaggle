{"cell_type":{"87c983c1":"code","7e2a0b9d":"code","734493bf":"code","62734c4d":"code","02132b6c":"code","69dde0e6":"code","2e3c62d4":"code","ffb1efdc":"code","7f9220de":"code","aad3a75e":"code","68246ab4":"code","2501679d":"code","70b62d7e":"code","bbe38a40":"code","a963a684":"code","24321c4f":"code","9edc8d2b":"code","a850fe8a":"code","d1119a8f":"code","779975e5":"code","0493bf4a":"code","f8e87b4c":"code","6a07aeae":"code","701c32f0":"code","731f7eec":"code","3f045a74":"code","c1b8f3fd":"code","928b5f10":"markdown","a31f7aec":"markdown","e60fface":"markdown","cd202606":"markdown","885c4148":"markdown","dfc50d51":"markdown","ada30ab0":"markdown","791a26df":"markdown"},"source":{"87c983c1":"#invite people for the Kaggle party\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Put this when it's called\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\n\nimport os\nprint(os.listdir(\"..\/input\"))","7e2a0b9d":"school = pd.read_csv('..\/input\/2016 School Explorer.csv')","734493bf":"school.head()","62734c4d":"school.shape","02132b6c":"pd.Series(school.columns)","69dde0e6":"#correlation matrix\ncorrmat = school.corr()\nf, ax = plt.subplots(figsize=(24, 18))\nsns.heatmap(corrmat, vmax=.8, square=True);","2e3c62d4":"school[['Grades','SED Code']].groupby(by='Grades').count().sort_values(by='SED Code',ascending=False)","ffb1efdc":"total = school.isnull().sum().sort_values(ascending=False)\npercent = (school.isnull().sum()\/school.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(25)","7f9220de":"#Analysing Top 3 columns with most missing values\nmissing_col = ['Other Location Code in LCGMS', 'Adjusted Grade', 'New?']\nt1 = school[[missing_col[0],'SED Code']].groupby(by=missing_col[0]).count()\nt2 = school[[missing_col[1],'SED Code']].groupby(by=missing_col[1]).count()\nt3 = school[[missing_col[2],'SED Code']].groupby(by=missing_col[2]).count()\npd.concat([t1,t2,t3])","aad3a75e":"#Since the values (other than Null) are not useful, it is better to drop these 3 columns\nschool = school.drop(missing_col, axis=1)","68246ab4":"#Let's find and drop some more columns which are not useful (at least for now)\nschool.head(2)\ndrop_list = ['School Name', 'Address (Full)', 'Grades'] #dropping Grades becasue 'Grades Low' and 'Grades High' columns provide sufficient information\nschool = school.drop(drop_list, axis=1)","2501679d":"#Now we should do some basic categorical univariate exploration\ncountplot_list = ['District', 'City', 'Zip', 'Community School?']\nschool[['District', 'City', 'Zip']].groupby(by=['District','City']).count()","70b62d7e":"sns.countplot(school['Community School?'])","bbe38a40":"#simplifying further by dropping these analyzed features, at least for now\nschool = school.drop(['District', 'City', 'Zip', 'Location Code'], axis=1) #Not dropping 'Community School?' and dropping one other 'Location Code' feature","a963a684":"# missing value treatment\ntotal = school.isnull().sum().sort_values(ascending=False)\npercent = (school.isnull().sum()\/school.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","24321c4f":"school.describe()","9edc8d2b":"# sns.lmplot( x=\"Latitude\", y=\"Longitude\", data=school, fit_reg=False, hue='City', legend=True, size=10)","a850fe8a":"# Pre-process : Remove % sign and convert object-type into float-type\nrace_cols = ['Percent Asian','Percent Black','Percent Hispanic','Percent White']\nschool[race_cols] = school[race_cols].replace({'\\%': ''}, regex=True)","d1119a8f":"school[race_cols].sum(axis=1).sort_values(ascending=False).plot(kind='hist')","779975e5":"for col in race_cols:\n    school[col] = school[col].astype('float64')","0493bf4a":"plt.figure(figsize=(20,5))\nsns.distplot(school[race_cols[0]] , color=\"red\", label=\"Asian\", hist=False)\nsns.distplot(school[race_cols[1]] , color=\"green\", label=\"Black\", hist=False)\nsns.distplot(school[race_cols[2]] , color=\"yellow\", label=\"Hispanic\", hist=False)\nsns.distplot(school[race_cols[3]] , color=\"magenta\", label=\"White\", hist=False)","f8e87b4c":"# Skewness\nskew_values = stats.skew(school[race_cols], nan_policy = 'omit')\npd.concat([pd.DataFrame(list(race_cols), columns=['Features']), \n           pd.DataFrame(list(skew_values), columns=['Skewness degree'])], axis = 1)","6a07aeae":"### Schools Rating Analysis - Univariate Categorical\nrating_cols = ['Rigorous Instruction Rating', 'Collaborative Teachers Rating', 'Supportive Environment Rating', 'Effective School Leadership Rating',\n               'Strong Family-Community Ties Rating', 'Trust Rating', 'Student Achievement Rating']\nrating_df = school[rating_cols]\nrating_group = pd.Series()\nfor i in rating_cols:\n    rating_group = pd.concat([rating_group, rating_df[i].value_counts(dropna = False)], axis=1, join='outer')\nrating_group = rating_group.drop(0, axis=1)\nrating_tags = ['Exceeding Target', 'Meeting Target', 'Approaching Target', 'Not Meeting Target']\nrating_group = rating_group.reindex(rating_tags)","701c32f0":"rating_grp = rating_group.reset_index()\nax = rating_grp.plot(x='index', kind='bar')\nax.legend(loc='center left', bbox_to_anchor=(1, 0.5))","731f7eec":"col_list = rating_cols + race_cols\ndf = school[col_list]\ndf.head(2)","3f045a74":" # We have to normalize the percent aggregates for each race. E.g.::\ndf.iloc[:,[0,7,8,9,10]].groupby(by='Rigorous Instruction Rating', axis=0).sum().apply(lambda x:100*x \/ float(x.sum())).reset_index()","c1b8f3fd":"def barplot_race(i, title):\n    df.iloc[:,[i,7,8,9,10]].groupby(by=title, axis=0).sum().apply(lambda x:100*x \/ float(x.sum())).reset_index().plot(x=title, kind='bar')\n    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n    plt.xticks(rotation=20)\n    plt.show()\nfor i,col in enumerate(rating_cols):\n    barplot_race(i,col)","928b5f10":"#### Total % should be 100 in ideal case, let's see the distribution of 'Sum% of all races for each school'","a31f7aec":"That's really a 'hot'map, divided into 2 bright sections.\nSection 1 has Grade 3 to Grade 5 variables whereas Section 2 has Grade 6 to Grade 8 variables.\n\nBut why?\nHypothesis: There are 2 school categories, one having Grade 3 to Grade 5 facilities, other having Grade 6 to Grade 8 facilities.","e60fface":"### Children Races Analysis - Univariate Distribution","cd202606":"# Missing Data","885c4148":"## Correlation Matrix (heatmap style)","dfc50d51":"## Lattitude Longitude Analysis","ada30ab0":"It would be interesting to compare the individual as well as total rating points against 'races'\n\nFor that, first compare 'Target Achievement Effectiveness' vs. Race","791a26df":"### There is more weight on the right tail in case of 'Asian', 'ELL' and White. \nHypothesis: There is a racial discrimination against Black and Hispanic race children"}}