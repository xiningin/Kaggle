{"cell_type":{"a3c69b31":"code","8e05af75":"code","7445226f":"code","f4593a67":"code","0bb5312c":"code","eba258fe":"code","62119981":"code","d01d54bf":"code","49bb702e":"code","eff9ba91":"code","b427f34a":"code","8a09627b":"code","3a6c6f3e":"code","39c7b153":"code","0238b07e":"code","6d546c33":"code","8a37be6e":"markdown","ac751a08":"markdown","aefb2d6e":"markdown","d1469fc5":"markdown","9be01e3b":"markdown","d3a26c54":"markdown","ca929029":"markdown","ff4ca056":"markdown","4f0d7d32":"markdown","6afed593":"markdown","3a31976b":"markdown","c014a4a9":"markdown","1f54596d":"markdown","0ca14fb9":"markdown","d9d64900":"markdown","7606fe8f":"markdown","f83a94a5":"markdown","1a202b85":"markdown","347753a4":"markdown","1be7b985":"markdown","32cb35da":"markdown","73b9acc5":"markdown","67d5baea":"markdown","acdcfcbb":"markdown"},"source":{"a3c69b31":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, RandomizedSearchCV\n\nfrom sklearn.metrics import recall_score\n","8e05af75":"att = pd.read_csv('..\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv')\natt.head()","7445226f":"att.info()","f4593a67":"attFeatures = []\nfor i in att.columns:\n    attFeatures.append([i, att[i].nunique(), att[i].drop_duplicates().values])\npd.DataFrame(attFeatures, columns = ['Features', 'Unique Number', 'Values'])","0bb5312c":"att['Attrition'] = np.where(att['Attrition'] == 'Yes', 1, 0)","eba258fe":"att.drop(columns=['EmployeeCount', 'Over18', 'StandardHours'], inplace=True)","62119981":"transformer = ColumnTransformer([\n    ('one hot', OneHotEncoder(drop = 'first'), ['BusinessTravel', 'Department', 'EducationField', 'Gender',\n                                                'JobRole', 'MaritalStatus', 'OverTime']),\n], remainder = 'passthrough')","d01d54bf":"att['Attrition'].value_counts()\/att.shape[0]*100","49bb702e":"X = att.drop('Attrition', axis = 1)\ny = att['Attrition']\n\nX.shape","eff9ba91":"X_train, X_test, y_train, y_test = train_test_split(X,y,\n                                                   stratify = y,\n                                                   test_size = 0.3,\n                                                   random_state = 3131)","b427f34a":"logreg = LogisticRegression()\ntree = DecisionTreeClassifier(random_state = 3131)\nknn = KNeighborsClassifier()\nrf = RandomForestClassifier(random_state = 3131)","8a09627b":"logreg_pipe = Pipeline([('transformer', transformer), ('logreg', logreg)])\ntree_pipe = Pipeline([('transformer', transformer), ('tree', tree)])\nknn_pipe = Pipeline([('transformer', transformer), ('scale', MinMaxScaler()), ('knn', knn)])\nrf_pipe = Pipeline([('transformer', transformer), ('rf', rf)])\n\ndef model_evaluation(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_cv = cross_val_score(model, X_train, y_train, cv = skfold, scoring = metric)\n    return model_cv\n\nlogreg_pipe_cv = model_evaluation(logreg_pipe, 'recall')\ntree_pipe_cv = model_evaluation(tree_pipe, 'recall')\nknn_pipe_cv = model_evaluation(knn_pipe, 'recall')\nrf_pipe_cv = model_evaluation(rf_pipe, 'recall')\n\nfor model in [logreg_pipe, tree_pipe, knn_pipe, rf_pipe]:\n    model.fit(X_train, y_train)\n    \nscore_mean = [logreg_pipe_cv.mean(), tree_pipe_cv.mean(), knn_pipe_cv.mean(), rf_pipe_cv.mean()]\nscore_std = [logreg_pipe_cv.std(), tree_pipe_cv.std(), knn_pipe_cv.std(), rf_pipe_cv.std()]\nscore_recall_score = [recall_score(y_test, logreg_pipe.predict(X_test)),\n            recall_score(y_test, tree_pipe.predict(X_test)), \n            recall_score(y_test, knn_pipe.predict(X_test)), \n            recall_score(y_test, rf_pipe.predict(X_test))]\nmethod_name = ['Logistic Regression', 'Decision Tree Classifier', 'KNN Classifier', 'Random Forest Classifier']\ncv_summary = pd.DataFrame({\n    'method': method_name,\n    'mean score': score_mean,\n    'std score': score_std,\n    'recall score': score_recall_score\n})\ncv_summary","3a6c6f3e":"rus = RandomUnderSampler(random_state = 3131)\nX_under, y_under = rus.fit_resample(X_train, y_train) \n\nlogreg_pipe_under = Pipeline([('transformer', transformer), ('rus', rus), ('logreg', logreg)])\ntree_pipe_under = Pipeline([('transformer', transformer), ('rus', rus), ('tree', tree)])\nknn_pipe_under = Pipeline([('transformer', transformer), ('scale', MinMaxScaler()), ('rus', rus), ('knn', knn)])\nrf_pipe_under = Pipeline([('transformer', transformer), ('rus', rus), ('rf', rf)])\n\ndef model_evaluation(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_cv = cross_val_score(model, X_train, y_train, cv = skfold, scoring = metric)\n    return model_cv\n\nlogreg_under_cv = model_evaluation(logreg_pipe_under, 'recall') \ntree_under_cv = model_evaluation(tree_pipe_under, 'recall')\nknn_under_cv = model_evaluation(knn_pipe_under, 'recall')\nrf_under_cv = model_evaluation(rf_pipe_under, 'recall')\n\nfor model in [logreg_pipe_under, tree_pipe_under, knn_pipe_under, rf_pipe_under]:\n    model.fit(X_train, y_train)\n    \nscore_mean = [logreg_under_cv.mean(), tree_under_cv.mean(), knn_under_cv.mean(),\n              rf_under_cv.mean()]\nscore_std = [logreg_under_cv.std(), tree_under_cv.std(), knn_under_cv.std(),\n             rf_under_cv.std()]\nscore_recall_score = [recall_score(y_test, logreg_pipe_under.predict(X_test)),\n            recall_score(y_test, tree_pipe_under.predict(X_test)), \n            recall_score(y_test, knn_pipe_under.predict(X_test)), \n            recall_score(y_test, rf_pipe_under.predict(X_test))]\nmethod_name = ['Logistic Regression UnderSampling', 'Decision Tree Classifier UnderSampling',\n              'KNN Classifier UnderSampling', 'Random Forest Classifier UnderSampling']\nunder_summary = pd.DataFrame({\n    'method': method_name,\n    'mean score': score_mean,\n    'std score': score_std,\n    'recall score': score_recall_score\n})\nunder_summary","39c7b153":"ros = RandomOverSampler(random_state = 3131)\nX_over, y_over = ros.fit_resample(X_train, y_train) \n\nlogreg_pipe_over = Pipeline([('transformer', transformer), ('ros', ros), ('logreg', logreg)])\ntree_pipe_over = Pipeline([('transformer', transformer), ('ros', ros), ('tree', tree)])\nknn_pipe_over = Pipeline([('transformer', transformer), ('scale', MinMaxScaler()), ('ros', ros), ('knn', knn)])\nrf_pipe_over = Pipeline([('transformer', transformer), ('ros', ros), ('rf', rf)])\n\ndef model_evaluation(model, metric):\n    skfold = StratifiedKFold(n_splits = 5)\n    model_cv = cross_val_score(model, X_train, y_train, cv = skfold, scoring = metric)\n    return model_cv\n\nlogreg_over_cv = model_evaluation(logreg_pipe_over, 'recall') \ntree_over_cv = model_evaluation(tree_pipe_over, 'recall')\nknn_over_cv = model_evaluation(knn_pipe_over, 'recall')\nrf_over_cv = model_evaluation(rf_pipe_over, 'recall')\n\nfor model in [logreg_pipe_over, tree_pipe_over, knn_pipe_over, rf_pipe_over]:\n    model.fit(X_train, y_train)\n    \nscore_mean = [logreg_over_cv.mean(), tree_over_cv.mean(), knn_over_cv.mean(),\n              rf_over_cv.mean()]\nscore_std = [logreg_over_cv.std(), tree_over_cv.std(), knn_over_cv.std(),\n             rf_over_cv.std()]\nscore_recall_score = [recall_score(y_test, logreg_pipe_over.predict(X_test)),\n            recall_score(y_test, tree_pipe_over.predict(X_test)), \n            recall_score(y_test, knn_pipe_over.predict(X_test)), \n            recall_score(y_test, rf_pipe_over.predict(X_test))]\nmethod_name = ['Logistic Regression OverSampling', 'Decision Tree Classifier OverSampling',\n              'KNN Classifier OverSampling', 'Random Forest Classifier OverSampling']\nover_summary = pd.DataFrame({\n    'method': method_name,\n    'mean score': score_mean,\n    'std score': score_std,\n    'recall score': score_recall_score\n})\nover_summary","0238b07e":"estimator = Pipeline([\n    ('transformer', transformer),\n    ('ros', ros),\n    ('model', logreg)\n])\n\nhyperparam_space = {\n    'model__C': [100, 10, 1, 0.1, 0.01, 0.001],\n    'model__solver': ['liblinear', 'newton-cg', 'lbfgs'],\n    'model__max_iter': [50, 100, 150, 200],\n    'model__random_state': [3131]\n}\n\nrandom = RandomizedSearchCV(\n                estimator,\n                param_distributions = hyperparam_space,\n                cv = StratifiedKFold(n_splits = 5),\n                scoring = 'recall',\n                n_iter = 10,\n                n_jobs = -1)\n\nrandom.fit(X_train, y_train)\n\nprint('best score', random.best_score_)\nprint('best param', random.best_params_)","6d546c33":"estimator.fit(X_train, y_train)\ny_pred_estimator = estimator.predict(X_test)\nrecall_estimator = recall_score(y_test, y_pred_estimator)\n\nrandom.best_estimator_.fit(X_train, y_train)\ny_pred_random = random.best_estimator_.predict(X_test)\nrecall_best_estimator = recall_score(y_test, y_pred_random)\n\nscore_list = [recall_estimator, recall_best_estimator]\nmethod_name = ['Logistic Regression OverSampling Before Tuning', 'Logistic Regression OverSampling After Tuning']\nbest_summary = pd.DataFrame({\n    'method': method_name,\n    'score': score_list\n})\nbest_summary","8a37be6e":"* *0 = Stay*\n* *1 = Resign*\n\n        - TN: Predicted: Stay and Actual: Stay\n        - TP: Predicted: Resign and Actual: Resign\n        - FP: Predicted: Resign and Actual: Stay\n        - FN: Predicted: Stay and Actual: Resign\n\n*From this matrix, I choose to push the FN or recall score to anticipate the employees not to resign because of the prediction is wrong.*","ac751a08":"# Preprocessing","aefb2d6e":"*The score looks good rather than before. From this Under Sampling, I will choose Decision Tree Classifier because it has the highest recall score.*","d1469fc5":"**RandomUnderSampler Model**","9be01e3b":"**Define Model**\n- I test with 4 models to find the best model:\n\n    * Logistic Regression\n    * Decision Tree Classifier\n    * K-Nearest Neighbor\n    * Random Forest Classifier","d3a26c54":"*From this method, I will choose Decision Tree Classifier because it has the highest recall score. But overall, the score still not good enough to do a prediction. While I process the data, it already indicates that the data is imbalanced. I decide to handle it using Under Sampling and Over Sampling.*","ca929029":"*I drop these columns because it's only has one value for all rows.*","ff4ca056":"*Indicated imbalance data*","4f0d7d32":"# Compairing Result","6afed593":"So far, this is the best model that I got for predicting attrition in this case.","3a31976b":"### OverSampling","c014a4a9":"After HyperParameter Tuning process, the score is getting higher, it means that tuning process can improve the model.","1f54596d":"# Handling Imbalance","0ca14fb9":"### UnderSampling","d9d64900":"*I use 0.3 as default score for test_size and X.shape for random_state so the data will be devided equally.*","7606fe8f":"# Cross Validation","f83a94a5":"*Now, the score getting worse again. The only one who stands out from the others. It's Logistic Regression using Over Sampling method. The recall score is the highest of other models. The rest models indicates underfitting.*","1a202b85":"*I define number **1 is Yes, means resign** and number **0 is No, means stay**.*","347753a4":"*Based on 3 methods, Cross Validation, Under Sampling, and Over Sampling, I pick Logistic Regression as the most stable model and using Over Sampling method, and continue to HyperParameter Tuning process.*","1be7b985":"# Modeling","32cb35da":"**Preprocessing Scheme**\n* OneHotEncoding: BusinessTravel, Department, EducationField, Gender, JobRole, MaritalStatus, OverTime\n* The rest will be pass through.","73b9acc5":"**Splitting Data**","67d5baea":"**RandomOverSampler Model**","acdcfcbb":"# HyperParam Tuning"}}