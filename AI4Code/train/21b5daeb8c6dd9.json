{"cell_type":{"e1ff6490":"code","b431a18f":"code","4f26c462":"code","b5f8e4f9":"code","b08ea897":"code","09460849":"code","d30302d0":"code","ec6702cb":"code","4fe081b2":"code","e6e1eb98":"code","9ebac738":"code","af3d35b4":"code","9ad41d8f":"code","46680935":"code","7b6b04c1":"code","7f513041":"code","707535fd":"code","80e623d4":"code","9915c0b4":"code","c17aa165":"code","c9b83e0c":"code","41441617":"code","08398255":"code","eccdfda1":"code","e303200b":"code","bc625d87":"code","76e32138":"code","3f40b143":"code","735644fb":"code","14b9630d":"code","2b318689":"code","9c2602ee":"code","bf963edb":"code","aee24f5c":"code","df9550b0":"code","54e4c568":"code","bdc2dba5":"code","3a489b2a":"code","7bb8ce50":"code","cb924008":"code","7f612d32":"code","565df522":"code","be779f24":"code","7da505c2":"code","4b8b46a0":"code","9d3f4bde":"markdown","a95d681f":"markdown","30c456e1":"markdown","401b9424":"markdown","e2ac4ce3":"markdown","e30d8c97":"markdown","5e8c2514":"markdown","4fc513c1":"markdown","75f58009":"markdown","ec1712cf":"markdown","328a49b3":"markdown","80fc3b4a":"markdown","56829890":"markdown","9a58c848":"markdown","b398b23b":"markdown","63b2fe35":"markdown","407e9ac3":"markdown","320610cb":"markdown","5484fd33":"markdown","197de989":"markdown","7dbb3dad":"markdown","1df95c7a":"markdown","927f5c3a":"markdown","e5e07c44":"markdown","f20b0d20":"markdown","73d5f235":"markdown","ba8419d3":"markdown","197204fd":"markdown","924a2220":"markdown","b768c6ca":"markdown","f389215e":"markdown","4fc70f86":"markdown","3087bc17":"markdown","2c3a27cb":"markdown","afa435bd":"markdown","5e2c9566":"markdown"},"source":{"e1ff6490":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b431a18f":"!pip install autoviml --no-cache-dir --ignore-installed","4f26c462":"import numpy as np\nimport pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport textblob\nfrom textblob import TextBlob, Word\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","b5f8e4f9":"from autoviml.Auto_ViML import Auto_ViML","b08ea897":"def avg_word_len (sentence):\n    words = sentence.split()\n    avg_len = sum(len(word) for word in words)\/len(words)\n    return avg_len\n\ndef extract_ngrams(data, num):\n    '''\n    Function to generate n-grams from sentences\n    '''\n    n_grams = TextBlob(data).ngrams(num)\n    return [ ' '.join(grams) for grams in n_grams]","09460849":"train = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/test.csv')","d30302d0":"train.head()","ec6702cb":"# Creating a copy of train dataset for text analysis\n\ndf_train = train.copy()\ndf_train['char_count'] = df_train['tweet'].str.len()\ndf_train_sort_charcount = df_train.sort_values(by='char_count', ascending=False)\ndf_train_sort_charcount[['tweet', 'char_count']].head()","4fe081b2":"df_train.head()","e6e1eb98":"df_train['word_count'] = df_train['tweet'].apply(lambda x: len(str(x).split(\" \")))\ndf_train_sort_wordcount = df_train.sort_values(by='word_count', ascending=False)\ndf_train_sort_wordcount[['tweet','word_count']].head()","9ebac738":"df_train.info()","af3d35b4":"# Number of hashtags in a tweet\n\ndf_train['hashtags'] = df_train['tweet'].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))\ndf_train_sort_hashtags = df_train.sort_values(by='hashtags', ascending=False)\ndf_train_sort_hashtags[['tweet', 'hashtags']].head()","9ad41d8f":"stop_words = stopwords.words('english')\n\ndf_train['stopwords'] = df_train['tweet'].apply(lambda x: len([i for i in x.split() if i in stop_words]))\ndf_train_sort_stopwords = df_train.sort_values(by='stopwords', ascending=False)\ndf_train_sort_stopwords[['tweet', 'stopwords']].head()","46680935":"df_train.iloc[371,2]","7b6b04c1":"df_train['upper_word'] = df_train['tweet'].apply(lambda x: len([x for x in x.split() if x.isupper()]))\ndf_train_sort_uppercase = df_train.sort_values(by='upper_word', ascending=False)\ndf_train_sort_uppercase[['tweet', 'upper_word']].head(15)","7f513041":"df_train['avg_word_len'] = df_train['tweet'].apply(lambda x: round(avg_word_len(x),1))\ndf_train_sort_avg_word_len = df_train.sort_values(by='avg_word_len', ascending=False)\ndf_train_sort_avg_word_len[['tweet', 'avg_word_len']].head()","707535fd":"data = df_train['tweet'][0]\n \nprint(\"1-gram: \", extract_ngrams(data, 1))\nprint(\"2-gram: \", extract_ngrams(data, 2))\nprint(\"3-gram: \", extract_ngrams(data, 3))\nprint(\"4-gram: \", extract_ngrams(data, 4))","80e623d4":"tf = df_train['tweet'][1:2].apply(lambda x: pd.value_counts(x.split())\/len(x.split())).sum(axis=0).reset_index()\ntf.columns = ['words', 'tf']\ntf","9915c0b4":"for i,word in enumerate(tf['words']):\n    tf.loc[i, 'idf'] = np.log(df_train.shape[0]\/(len(df_train[df_train['tweet'].str.contains(word)])))    \ntf","c17aa165":"tfidf = TfidfVectorizer(max_features=10000, lowercase=True, analyzer='word', stop_words= 'english',ngram_range=(1,1))\ndf_train_tfidf = tfidf.fit_transform(df_train['tweet'])\ndf_train_tfidf","c9b83e0c":"bag_of_words = CountVectorizer(max_features=10000, lowercase=True, ngram_range=(1,1),analyzer = \"word\")\ndf_train_bag_of_words = bag_of_words.fit_transform(df_train['tweet'])\ndf_train_bag_of_words","41441617":"df_train['sentiment'] = df_train['tweet'][:20].apply(lambda x: TextBlob(x).sentiment[0]) #performing only for the first 20 records.\ndf_train['polarity'] = df_train['tweet'][:20].apply(lambda x: TextBlob(x).sentiment[1])\ndf_train[['tweet','sentiment','polarity']].head()","08398255":"TextBlob(str(df_train['tweet'][1])).sentiment","eccdfda1":"df_train.head()","e303200b":"# Creating a copy of dataset to preprocess the data\n\ndf_train_dpp = train.copy()","bc625d87":"df_train_dpp['tweet_lower'] = df_train_dpp['tweet'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\ndf_train_dpp[['tweet', 'tweet_lower']].head()","76e32138":"stop_words = stopwords.words('english')\n\ndf_train_dpp['tweet_stopwords'] = df_train_dpp['tweet_lower'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\ndf_train_dpp[['tweet', 'tweet_stopwords']].head()","3f40b143":"df_train_dpp['tweet_punc'] = df_train_dpp['tweet_stopwords'].str.replace('[^\\w\\s]', '')\ndf_train_dpp[['tweet', 'tweet_punc']].head()","735644fb":"# Frequency of common words in all the tweets\n\ncommon_top20 = pd.Series(' '.join(df_train_dpp['tweet_punc']).split()).value_counts()[:20]\nprint(common_top20)\n\n\n# Remove these top 20 freq words\ncommon = list(common_top20.index)\n\ndf_train_dpp['tweet_comm_remv'] = df_train_dpp['tweet_punc'].apply(lambda x: \" \".join(x for x in x.split() if x not in common))\ndf_train_dpp[['tweet','tweet_comm_remv']].head()","14b9630d":"# Frequency of common words in all the tweets\nrare_top20 = pd.Series(\" \".join(df_train_dpp['tweet_comm_remv']).split()).value_counts()[-20:]\nprint(rare_top20)\n\n# Remove these top 20 common words\nrare = list(rare_top20.index)\n\ndf_train_dpp['tweet_rare_remv'] = df_train_dpp['tweet_comm_remv'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare))\ndf_train_dpp[['tweet','tweet_rare_remv']].head()","2b318689":"# Using textblob\n\ndf_train_dpp['tweet_rare_remv'][:10].apply(lambda x: str(TextBlob(x).correct()))","9c2602ee":"df_train_dpp['tweet_rare_remv'][:10].apply(lambda x: TextBlob(x).words)","bf963edb":"st = PorterStemmer()\ndf_train_dpp['tweet_rare_remv'][:10].apply(lambda x: \" \".join([st.stem(word) for word in x.split()]))","aee24f5c":"df_train_dpp['tweet_rare_remv'][:10].apply(lambda x: \" \".join(Word(word) for word in x.split()))","df9550b0":"from sklearn.feature_extraction.text import CountVectorizer\ncorpus = [\n          'This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?',\n         ]\n\nvectorizer = CountVectorizer()\nX = vectorizer.fit_transform(corpus)\nprint(vectorizer.get_feature_names())","54e4c568":"print(X.toarray())","bdc2dba5":"vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\nX2 = vectorizer2.fit_transform(corpus)\nprint(vectorizer2.get_feature_names())","3a489b2a":"print(X2.toarray())","7bb8ce50":"from sklearn.feature_extraction.text import HashingVectorizer\ncorpus = [\n          'This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?',\n         ]\nvectorizer = HashingVectorizer(n_features=2**4)\nX = vectorizer.fit_transform(corpus)\nprint(X.shape)","cb924008":"print(X.toarray())","7f612d32":"train.head()","565df522":"input_feature, target = \"tweet\", \"label\"","be779f24":"from autoviml.Auto_ViML import Auto_NLP\n\ntrain_x, test_x, final, predicted= Auto_NLP(input_feature, train, test,target,\n                                            score_type=\"balanced_accuracy\",\n                                            top_num_features=500,\n                                            modeltype=\"Classification\",\n                                            verbose=2,\n                                            build_model=True)","7da505c2":"test['label']=final.predict(test[input_feature])","4b8b46a0":"test_predicted = test.copy()\ntest_predicted.to_csv('prediction.csv',index = False)","9d3f4bde":"# 4.0 Using Auto NLP","a95d681f":"**1.5 UpperCase Word Count**","30c456e1":"# User Defined Functions","401b9424":"**2.1 Stopwords Removal**","e2ac4ce3":"* Hashing Vectorizer converts text to a matrix of occurrences using the \u201chashing trick\u201d.\n\n* It converts a collection of text documents to a matrix of token occurrences.\n\n* It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm=\u2019l1\u2019 or projected on the euclidean unit sphere if norm=\u2019l2\u2019.\n\n* This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping.\n\n* Each word is mapped to a feature and using the hash function converts it to a hash.\n\n* If the word occurs again in the body of the text it is converted to that same feature which allows us to count it in the same feature without retaining a dictionary in memory.","e30d8c97":"**2.3 Common Word Removal**","5e8c2514":"# 5.0 Prediction","4fc513c1":"IDF = log(N\/n), where, N is the total number of rows and n is the number of rows in which the word was present","75f58009":"**1.9 Inverse Document Frequency**","ec1712cf":"# Libraries","328a49b3":"# 3.1 Hashing Vectorizer","80fc3b4a":"**1.12 Sentiment Analysis**","56829890":"# 1. Feature Extraction \n* Creation of brand new features","9a58c848":"**2.7 Stemming**","b398b23b":"**2.5 Spelling Correction**","63b2fe35":"**Extract features using NLP techniques below**","407e9ac3":"* Sentiment(polarity=0.2, subjectivity=0.2)\n\n\nSentiment analysis is basically the process of determining the attitude or the emotion of the writer, i.e., whether it is positive or negative or neutral.\nThe sentiment function of textblob returns two properties, polarity, and subjectivity.\nPolarity is float which lies in the range of [-1,1] where 1 means positive statement and -1 means a negative statement. Subjective sentences generally refer to personal opinion, emotion or judgment whereas objective refers to factual information. Subjectivity is also a float which lies in the range of [0,1].","320610cb":"**2.0 Convert the sentence to Lower Case**","5484fd33":"# 3.0 Count Vectorization","197de989":"* CounterVectorization is a SciKitLearn library takes any text document and returns each unique word as a feature with the count of number of times that word occurs.\n\n* While this can generate lot of features with some extremely useful parameters that help avoid that including stop_words, n_grams, and max_features.","7dbb3dad":"# 2 Pre-Processing","1df95c7a":"**1.2 Count of Hash Tags**","927f5c3a":"**1.3 Stopword Count**","e5e07c44":"**1.8 Term Frequency**","f20b0d20":"**1.10 TF-IDF**","73d5f235":"**2.6 Tokenization**","ba8419d3":"**2.4 Rare Words Removal**","197204fd":"**1.7 N-grams**","924a2220":"# Data Import","b768c6ca":"**2.2 Punctutation Remova**l","f389215e":"**1.6 Average Word Length**","4fc70f86":"**1.0 Character Count**","3087bc17":"**1.1 Word Count**","2c3a27cb":"**2.8 Lemmatization**","afa435bd":"TF = (Number of times term T appears in the particular row) \/ (number of terms in that row)","5e2c9566":"**1.11 Bag of Words**"}}