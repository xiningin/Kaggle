{"cell_type":{"c64bdf37":"code","81cba24f":"code","dbbc4e48":"code","b895c6de":"code","8923122e":"code","d811e43f":"code","693b4fca":"code","620cf008":"code","2f12ce06":"code","88778a71":"code","e394d1f7":"code","759d3498":"code","db81997f":"code","bc335061":"code","507327e2":"code","71a404cc":"code","ddec3821":"code","2eeb3b5f":"code","77e3e065":"code","cdea3634":"code","3de26e98":"markdown","5652aa9c":"markdown","8c2b7bad":"markdown","202663ef":"markdown","48ac3680":"markdown"},"source":{"c64bdf37":"# Importing Libraries\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport random\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nseed = 42\nnp.random.seed = seed\nfrom sklearn.model_selection import train_test_split\nfrom skimage.io import imread, imshow\nfrom skimage.transform import resize","81cba24f":"# Defining Shape and Size of Image\nIMG_WIDTH = 128\nIMG_HEIGHT = 128\nIMG_CHANNELS = 3\n\nTRAIN_PATH = '\/kaggle\/input\/clothing-coparsing-dataset'","dbbc4e48":"# Listing all image and mask name from file\n\nX_imageid = os.listdir(os.path.join(TRAIN_PATH,'images'))\ny_imageid = os.listdir(os.path.join(TRAIN_PATH,'labels','pixel_level_labels_colored'))\nX_imageid.sort()\ny_imageid.sort()","b895c6de":"# Creating zeros tensor of Input and output sizes. Later it will be filled with data.\nX = np.zeros((len(X_imageid),IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS),dtype = np.uint8)\ny = np.zeros((len(X_imageid),IMG_HEIGHT,IMG_WIDTH,IMG_CHANNELS),dtype = np.uint8)\nprint(X.shape,y.shape)","8923122e":"# No. of Mask available as samplesize\n\nsamplesize = 1003\nX_imageid = X_imageid[:samplesize]\ny_imageid = y_imageid[:samplesize]\nX = X[:samplesize]\ny = y[:samplesize]","d811e43f":"# Loading data into zeros tensor variable as created above\nfor i,id_ in tqdm(enumerate(X_imageid),total = len(X_imageid)):\n    path = os.path.join(TRAIN_PATH,'images',id_)\n    img = imread(path)\n    img = resize(img,(IMG_HEIGHT,IMG_WIDTH),mode='constant',preserve_range = True)\n    X[i] =img\n        \n    mask_path = os.path.join(TRAIN_PATH,'labels','pixel_level_labels_colored',id_.split('.')[0]+'.png')\n\n    img = imread(mask_path)\n  \n    img = resize(img,(IMG_HEIGHT,IMG_WIDTH),mode='constant',preserve_range = True)\n    y[i] = img\n\n","693b4fca":"# Scaling by Normalizing data between 0 to 1\nX = X\/255.0\ny = y\/255.0","620cf008":"# Spliting into train and valid data\nX_train,X_valid,y_train,y_valid = train_test_split(X,y,random_state = seed,test_size = 0.2)\nprint(X_train.shape,X_valid.shape)","2f12ce06":"# Random Sample View of image and image mask\nimage_x = random.randint(0, len(X_train)-1)\nimshow(X_train[image_x])\nplt.show()\nimshow(np.squeeze(y_train[image_x]))\nplt.show()","88778a71":"# Building U-Net Architecture Model\ndef build_model():\n    tf.keras.backend.clear_session()\n    inputs = tf.keras.layers.Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n\n    #Contraction path\n    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(inputs)\n    c1 = tf.keras.layers.Dropout(0.1)(c1)\n    c1 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n    p1 = tf.keras.layers.MaxPooling2D((2, 2))(c1)\n\n    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n    c2 = tf.keras.layers.Dropout(0.1)(c2)\n    c2 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n    p2 = tf.keras.layers.MaxPooling2D((2, 2))(c2)\n\n    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n    c3 = tf.keras.layers.Dropout(0.2)(c3)\n    c3 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n    p3 = tf.keras.layers.MaxPooling2D((2, 2))(c3)\n\n    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n    c4 = tf.keras.layers.Dropout(0.2)(c4)\n    c4 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n    p4 = tf.keras.layers.MaxPooling2D(pool_size=(2, 2))(c4)\n\n    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n    c5 = tf.keras.layers.Dropout(0.3)(c5)\n    c5 = tf.keras.layers.Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n\n    #Expansive path \n    u6 = tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n    u6 = tf.keras.layers.concatenate([u6, c4])\n    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n    c6 = tf.keras.layers.Dropout(0.2)(c6)\n    c6 = tf.keras.layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n\n    u7 = tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n    u7 = tf.keras.layers.concatenate([u7, c3])\n    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n    c7 = tf.keras.layers.Dropout(0.2)(c7)\n    c7 = tf.keras.layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n\n    u8 = tf.keras.layers.Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n    u8 = tf.keras.layers.concatenate([u8, c2])\n    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n    c8 = tf.keras.layers.Dropout(0.1)(c8)\n    c8 = tf.keras.layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n\n    u9 = tf.keras.layers.Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n    u9 = tf.keras.layers.concatenate([u9, c1], axis=3)\n    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n    c9 = tf.keras.layers.Dropout(0.1)(c9)\n    c9 = tf.keras.layers.Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n\n    outputs = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid')(c9)\n\n    model = tf.keras.Model(inputs=[inputs], outputs=[outputs])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","e394d1f7":"\nmodel = build_model()\nprint(\"Input Shape :  \",model.input_shape)\nprint(\"Output Shape:  \",model.output_shape)\nmodel.summary()","759d3498":"checkpointer = tf.keras.callbacks.ModelCheckpoint('cloth_model.h5', verbose=1, save_best_only=True)\ncallbacks = [\n        tf.keras.callbacks.TensorBoard(log_dir='logs')]","db81997f":"# Training\nresults = model.fit(X_train, y_train, validation_split=0.1, batch_size=16, epochs=50, callbacks=callbacks)","bc335061":"# Testing Data on Training Set\nimage_x = random.randint(0, len(X_train)-1)\nimshow(X_train[image_x])\nplt.show()\nX_testing = np.expand_dims(X_train[image_x],axis = 0)\n\nimshow(np.squeeze(y_train[image_x]))\nplt.show()\n\nprediction = model.predict(X_testing)\n\nimshow(np.squeeze(prediction))\nplt.show()","507327e2":"images_x = set()\nwhile(len(images_x)!=10):\n    images_x.add(random.randint(0, len(X_valid)-1))\n\nfor num,image_x in enumerate(images_x):\n    plt.figure(figsize=(9,9))\n    plt.subplot(1,3,1)\n    plt.title(\"Real Image\")\n    imshow(X_valid[image_x])\n    \n    plt.subplot(1,3,2)\n    plt.title(\"Labeled Output\")\n    imshow(np.squeeze(y_valid[image_x]))\n    \n    X_testing = np.expand_dims(X_valid[image_x],axis = 0)\n    prediction = model.predict(X_testing)\n    plt.subplot(1,3,3)\n    plt.title('Predcted Output')\n    imshow(np.squeeze(prediction))\n    plt.show()","71a404cc":"model.save('model_unet.h5')","ddec3821":"def try_out_own_image(image_path):\n    img = imread(image_path)\n    img = resize(img,(IMG_HEIGHT,IMG_WIDTH),mode='constant',preserve_range = True)\n    img = img\/255.0\n    print(img.shape)\n    \n    X_testing = np.expand_dims(img,axis = 0)\n    prediction = model.predict(X_testing)\n    plt.figure(figsize=(6,9))\n    plt.subplot(1,2,1)\n    plt.title('Real Image')\n    imshow(img)\n    plt.subplot(1,2,2)\n    imshow(np.squeeze(prediction))\n    plt.title('Predicted Output')\n    plt.show()\n    \n    \n    \n    ","2eeb3b5f":"image_path = '..\/input\/clothing-coparsing-dataset\/images\/1009.jpg'\ntry_out_own_image(image_path)","77e3e065":"image_path = '..\/input\/clothing-coparsing-dataset\/images\/2067.jpg'\ntry_out_own_image(image_path)","cdea3634":"image_path = '..\/input\/clothing-coparsing-dataset\/images\/1888.jpg'\ntry_out_own_image(image_path)","3de26e98":"**Blog on Image Segmentation and U-net. [check here](https:\/\/analyticsindiamag.com\/my-experiment-with-unet-building-an-image-segmentation-model\/)**","5652aa9c":"![Masking Demo_image](https:\/\/analyticsindiamag.com\/wp-content\/uploads\/2020\/07\/u-net-segmentation-e1542978983391.png)","8c2b7bad":"### Testing Data on Validation Set","202663ef":"**U-net Research Paper available at [unetpaper](https:\/\/arxiv.org\/abs\/1505.04597)**","48ac3680":"# Image Masking by U-NET Architecture From Scratch"}}