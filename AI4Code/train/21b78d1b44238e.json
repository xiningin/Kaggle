{"cell_type":{"fbda596f":"code","b3c5c745":"code","ccf5a000":"code","66c5c83b":"code","7c6c991b":"code","507d5690":"code","2ba5fc55":"code","36f4b1f3":"code","186f0750":"code","89c74ddf":"code","a9671b6f":"code","54c1c52a":"code","ece7779f":"code","aef0898e":"code","cbb3ee5b":"code","51ee9584":"code","7578bb32":"code","99f8d888":"code","a98b0932":"code","471cff68":"code","7a06d9cb":"code","79adfcdd":"code","314f11d5":"code","84ec3c79":"code","cc8a05d6":"code","0447f003":"code","1d2af6bb":"code","8a9d8dc3":"code","2086403a":"code","6228f2c4":"code","9fc44b1f":"code","ebb3f6c5":"code","a1696254":"code","6628f63e":"code","67d64ef2":"code","59a1205a":"code","9797ad89":"code","48b42d13":"code","7d82ff56":"code","3d3d5a32":"code","f60736f9":"code","c6fbb561":"code","baa4ce9a":"code","35904aec":"code","1df4d865":"code","5eac1476":"code","2708d208":"code","7cac3888":"code","d1335f0d":"code","fb421ec3":"code","e371ea3b":"code","945396d2":"code","a4963804":"code","23c0b20c":"code","629ec17f":"code","3651c046":"code","94bbaae8":"code","475b51a4":"code","1f61c907":"code","4390bb89":"code","e5ab60dc":"code","9d928276":"markdown","e7ae1fae":"markdown","f265e96d":"markdown","ac7d3bde":"markdown","2c226572":"markdown","f715e9fc":"markdown","e1c9b2c7":"markdown","025856bf":"markdown","daba9127":"markdown","d35c28b6":"markdown","d82c4252":"markdown","5cc9bd48":"markdown","60961da4":"markdown","53dea696":"markdown","571f3485":"markdown","9e70cf5d":"markdown","ed13bbc5":"markdown","b76fff2d":"markdown","ab6c56ca":"markdown","a83273b9":"markdown","9e4c57e4":"markdown","c8ed10dc":"markdown","3f7f1dc6":"markdown","922a6015":"markdown","ece8474a":"markdown","11530b3d":"markdown","17ae9d0e":"markdown","df0518e6":"markdown","65131143":"markdown","c2f69a13":"markdown","3c429588":"markdown","74e4c789":"markdown"},"source":{"fbda596f":"# To print multiple output in a cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'\n\n# from google.colab.patches import cv2_imshow","b3c5c745":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#from albumentations import *\nimport cv2\nimport copy\nimport os\nprint(os.listdir(\"..\/input\"))\n\n#!pip install pretrainedmodels\nfrom tqdm import tqdm_notebook as tqdm\nfrom torchvision.models import *\n#import pretrainedmodels\n\nfrom fastai.vision import *\nfrom fastai.vision.models import *\nfrom fastai.vision.learner import model_meta\nfrom fastai.callbacks import * \n\n#from utils import *\nimport sys\n\n# Any results you write to the current directory are saved as output.","ccf5a000":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n# from sklearn.metrics import f1_score\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nimport cv2\nimport glob\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","66c5c83b":"from pathlib import Path\nfrom fastai import *\nfrom fastai.vision import *\nimport torch\nfrom fastai.callbacks.hooks import *","7c6c991b":"## set the data folder\ndata_folder = Path(\"..\/input\/hp-2020\/jh_2020\")","507d5690":"print(os.listdir(data_folder))","2ba5fc55":"# ..\/input\/hp-2020\/jh_2020\/images","36f4b1f3":"print(os.listdir('..\/input\/hp-2020\/jh_2020\/images'))","186f0750":"data_path = \"..\/input\/hp-2020\/jh_2020\/images\/\"\npath = os.path.join(data_path , \"*jpg\")\n","89c74ddf":"print(os.listdir(data_path))","a9671b6f":"files = glob.glob(path)\ndata=[]\nfor file in files:\n    image = cv2.imread(file)\n    data.append(image)","54c1c52a":"# data[1]","ece7779f":"## read the csv data files\ntrain_df = pd.read_csv('..\/input\/hp-2020\/jh_2020\/train.csv')\ntest_df = pd.read_csv('..\/input\/hp-2020\/jh_2020\/test.csv')\nsubmit = pd.read_csv('..\/input\/hp-2020\/jh_2020\/sample_submission.csv')","aef0898e":"print(test_df.shape)\nprint(train_df.shape)","cbb3ee5b":"train_images = data[:1646]\ntest_images= data[1646:]","51ee9584":"train_df.head(2)","7578bb32":"train_df['emergency_or_not'].value_counts()","99f8d888":"dupli=train_df[train_df['emergency_or_not']==1].sample(150)","a98b0932":"dupli.shape\ndupli.head(4)","471cff68":"# path=\"\/content\/images\"\nprint(train_df.shape)\n","7a06d9cb":"# for i in dupli['image_names']:    \n#     new_image='copy'+i\n#     original_img = cv2.imread(\"\/content\/images\/\"+i)\n# #     clone_img = copy.copy(original_img)\n# #     data.append(clone_img)\n#     cv2.imwrite(data_path + new_image,original_img)\n\n#     d=[{'image_names':new_image,'emergency_or_not':1}]\n\n#     train_df=train_df.append(d,ignore_index=True,sort=False)","79adfcdd":"# print(train_df.shape)\n# len(listdir(path))","314f11d5":"train_df['emergency_or_not'].value_counts()","84ec3c79":"\n##transformations to be done to images\ntfms = get_transforms(do_flip=False,flip_vert=False ,max_rotate=10.0, max_zoom=1.22, max_lighting=0.22, max_warp=0.4, p_affine=0.75,\n                      p_lighting=0.75)\n#, xtra_tfms=zoom_crop(scale=(0.9,1.8), do_rand=True, p=0.8))\n\n\n#Apply new transformations\n\n# tfms = get_transforms(do_flip=False,flip_vert=False ,max_rotate=10.0, max_zoom=1.22, max_lighting=0.22, max_warp=0.4, p_affine=0.75,\n#                       p_lighting=0.75, xtra_tfms=zoom_crop(scale=(0.9,1.8), do_rand=True, p=0.8))\n\n## create databunch of test set to be passed\ntest_img = ImageList.from_df(test_df, path=data_folder, folder='images')","cc8a05d6":"np.random.seed(45)\n## create source of train image databunch\nsrc = (ImageList.from_df(train_df, path=data_folder, folder='images')\n       .split_by_rand_pct(0.2)\n       #.split_none()\n       .label_from_df()\n       .add_test(test_img))","0447f003":"data = (src.transform(tfms, size=300,padding_mode='reflection',resize_method=ResizeMethod.SQUISH)\n        .databunch(path='.', bs=16, device= torch.device('cuda:0')).normalize(imagenet_stats))\n\n# data = (src.transform(tfms, size=350,padding_mode='reflection',resize_method=ResizeMethod.SQUISH)\n#         .databunch(path='.', bs=16).normalize(imagenet_stats))","1d2af6bb":"print(data.classes)","8a9d8dc3":"data.show_batch(rows=3, figsize=(7,7))","2086403a":"#lets create learner. tried with resnet152, densenet201, resnet101\n# learn = cnn_learner(data=data, base_arch=models.densenet201, metrics=[FBeta(beta=1, average='macro'),error_rate],\n#                     callback_fns=ShowGraph)\n\n\n\nlearn = cnn_learner(data=data, base_arch=models.densenet201, metrics=[FBeta(beta=1, average='macro'), accuracy],\n                    callback_fns=ShowGraph).mixup()","6228f2c4":"# learn.summary()","9fc44b1f":"\n#lets find the correct learning rate to be used from lr finder\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","ebb3f6c5":"#lets start with steepset slope point. adding wd (weight decay) not to overfit as we are running 10 epochs \nlr = 1e-04\n#learn.fit_one_cycle(10, slice(lr))\n# learn.fit_one_cycle(10, max_lr=slice(lr), wd=0.2)\n\n# learn.fit_one_cycle(5, max_lr=slice(3e-5,3e-2))         # previous\n\n\n# learn.fit_one_cycle(5, max_lr=slice(3e-5,1e-3),wd=0.2)   # with this we got better accuracy \n\n# for first layer don't use slice as it used to train other layers.\n\nlearn.fit_one_cycle(10, slice(lr),wd=0.1)   ","a1696254":"learn.fit_one_cycle(10, slice(lr),wd=0.1) ","6628f63e":"# save the model\n\nlearn.save('stage1_model')","67d64ef2":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(dpi=120)","59a1205a":"#lets plot the lr finder record\nlearn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","9797ad89":"# train for  more cycles after unfreezing\n# learn.fit_one_cycle(5,slice(1e-05),wd=0.15)\nlearn.fit_one_cycle(10, slice(1e-03, lr\/10),wd=0.15)","48b42d13":"# save the model\n\nlearn.save('stage2_model')","7d82ff56":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(dpi=120)","3d3d5a32":"learn.freeze_to(-3)","f60736f9":"## finding the LR\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","c6fbb561":"# 96.52% accuracy achieved\nlearn.fit_one_cycle(6, slice(1e-02, lr\/10),wd=0.15)","baa4ce9a":"# save the model\n\nlearn.save('stage3_model')","35904aec":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(dpi=120)","1df4d865":"## freezing initial all layers except last 2 layers\n\n# 96.18%\nlearn.freeze_to(-2)","5eac1476":"## finding the LR\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","2708d208":"## training for few cylcles more\nlearn.fit_one_cycle(5, slice(5e-04, lr\/40),wd=0.15)","7cac3888":"## training for few cylcles more\nlearn.fit_one_cycle(5, slice(5e-04, lr\/100),wd=0.15)","d1335f0d":"# save the model\n\nlearn.save('stage4_model_new')","fb421ec3":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(dpi=120)","e371ea3b":"learn.freeze_to(-1)","945396d2":"## finding the LR\nlearn.lr_find()\nlearn.recorder.plot(suggestion=True)","a4963804":"## training even more\nlearn.fit_one_cycle(5, slice(1e-05, lr\/30),wd=0.05)","23c0b20c":"learn.fit_one_cycle(6, slice(5e-06, lr\/100))","629ec17f":"\n##learn.TTA improves score further. lets see for the validation set\npred_val,y = learn.TTA(ds_type=DatasetType.Valid)\nfrom sklearn.metrics import f1_score, accuracy_score\nvalid_preds = [np.argmax(pred_val[i]) for i in range(len(pred_val))]\nvalid_preds = np.array(valid_preds)\ny = np.array(y)\naccuracy_score(valid_preds,y),f1_score(valid_preds,y, average='micro')\n","3651c046":"# y","94bbaae8":"# preds,y = learn.TTA(ds_type=DatasetType.Test)\npreds,_ = learn.get_preds(ds_type = DatasetType.Test)\nlabelled_preds = [np.argmax(preds[i]) for i in range(len(preds))]\n\nlabelled_preds = np.array(labelled_preds)","475b51a4":"# test_df.shape","1f61c907":"len(labelled_preds)","4390bb89":"#create submission file\ndf = pd.DataFrame({'image_names':test_df['image_names'], 'emergency_or_not':labelled_preds}, columns=['image_names', 'emergency_or_not'])\ndf.to_csv('submission_model_new1.csv', index=False)","e5ab60dc":"d=pd.read_csv('submission1.csv')\nd.head()","9d928276":"- Train for few more cycles (we will be setting two LRs in below trainings: first one to train the initial layers and second to train last layers ).\n- As initial layers' stats are imagenet stats which are helpful in finding patterns (discussed above) not the exact ships, so we will be training those layers with very low learning rates (to not to greatly change those initial layer parameters)","e7ae1fae":"## Evaluation Metric\n- The evaluation metric for this competition is ```Accuracy```.","f265e96d":"1. ### - Do upvote\n1. ### - Share\n1. ### - For queries use ```comment section```","ac7d3bde":"### In this Section of code we gonna duplicate a set of 150 image of class='1' ","2c226572":"## Data Transformation","f715e9fc":"- At first glance fastai seems to be bit complicated but as you learn it became easy to work with it.","e1c9b2c7":"Now Unfreeze entire model.This Sets every layer group to trainable (i.e. requires_grad=True)","025856bf":"<img src=\"https:\/\/raw.githubusercontent.com\/AIVenture0\/JanataHack-Computer-Vision-Hackathon-21st-Rank-\/master\/logsheet.jpg\"\/>","daba9127":"### Creating duplicate images","d35c28b6":"### Suggestions: \n\n- Maintain a proper logsheet for you models,parameters and pretrained models,submission(It will help you to track you progress).\n- Try as many methods as you can.\n- While training a model you need fast processing so its better to use kaggle GPU ( Can also use Google collab GPU but over there processing is very slow, for an estimate \n\n    - Kaggle Training time`~10mins\n    - Google Training time ~1:30hr","d82c4252":"## Logsheet Sample","5cc9bd48":"## Sample Images\n\n<center><img src=\"https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2018\/08\/Emgen.jpg\"\/><\/center>","60961da4":"- Didn't see much improvement.","53dea696":"> ## New methodology\n> ","571f3485":"\nlets freeze the all layers except last 3 as these are initial layers for finding recurring pattern\/ shapes\/corners etc. (not exactly helpful in finding ships). so its better not to change stats of those layers","9e70cf5d":"## Data Description\n- ```train.zip```: contains 2 csvs and 1 folder containing image data\n- ```train.csv``` \u2013 [\u2018image_names\u2019, \u2018emergency_or_not\u2019] contains the image name and correct class for 1646 (70%) train images.\n\n- ```images``` \u2013 contains __2352__ images for both train and test sets\n\n- ```test.csv```: [\u2018image_names\u2019] contains just the image names for the 706 (30%) test images\n\n- ```sample_submission.csv```: [\u2018image_names\u2019,\u2019emergency_or_not\u00ad\u2019] contains the exact format for a valid submission (1 - For Emergency Vehicle, 0 - For Non Emergency Vehicle)","ed13bbc5":"__Fatalities__ due to traffic delays of emergency vehicles such as __ambulance & fire brigade__ is a huge problem. In daily life, we often see that emergency vehicles face difficulty in passing through traffic. So differentiating a vehicle into an emergency and non emergency category can be an important component in __traffic monitoring__ as well as self drive car systems as reaching on time to their destination is critical for these services.\n\n______\n\n## Task \n\nIn this problem, you will be working on __classifying vehicle images__ as either belonging to the ```emergency vehicle or non-emergency vehicle category```. \n\n- For the same, you are provided with the __train__ and the __test dataset__. Emergency vehicles usually includes ```police cars, ambulance and fire brigades```.","b76fff2d":"- So here you can check the images in training data increased to 1796 and number of images increased to 2502","ab6c56ca":"# <center><font color='red'><b>JanataHack: Computer Vision Hackathon<\/b><\/font><\/center>\n\n___\n\n<center>15 Day Hackathon From Analytics Vidhya<\/center>","a83273b9":"- Choose a random sample of 150 Images","9e4c57e4":"## Prepare the cnn_learner","c8ed10dc":"- Previously we did the whole training with image size=400 and all the time validation_accuracy revolve around 97% and the test accuracy is around 96%\n- This time try with 300 image size(because in my previous training i achieve the best score of 97% with size=300)\n\n- Beware that we have to adjust batchsize accordingly to run out of memory.","3f7f1dc6":"- Can clearly see there is an imbalance in the both clases","922a6015":"### Number of images before and shape of training data after.","ece8474a":"## About the Hackathon","11530b3d":"### Number of images before and shape of training data before.","17ae9d0e":"#### For this competition i also used keras for with MobileNet pretrained model. \n\n[Check this out](https:\/\/github.com\/AIVenture0\/JanataHack-Computer-Vision-Hackathon-21st-Rank-)","df0518e6":"### Accuracy check","65131143":"### Importing the necessary libraries","c2f69a13":"<center><img src=\"https:\/\/datahack-prod.s3.ap-south-1.amazonaws.com\/__sized__\/contest_cover\/cover_FJJYomD-thumbnail-1200x1200-90.jpg\"\/>","3c429588":"## FINAL PREDICTION FOR SUBMISSION","74e4c789":"- You can design something like this for your model to track the training progress. I know it is a bit tedious do track all progress side by side, but believe me, if you want to feature among top best then track down all your progress."}}