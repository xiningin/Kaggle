{"cell_type":{"b8687422":"code","454c5434":"code","1ad1c80b":"code","501317a6":"code","57a488dd":"code","902b2fe3":"code","dd902d9f":"code","2d976873":"code","be5bca40":"code","81f138bc":"code","19c34db6":"code","3c67eda2":"code","b8e4049e":"code","ee1316a4":"code","e4fdbf9b":"markdown","e9c6f066":"markdown","27ac713a":"markdown","90c3cd86":"markdown","4777f7cd":"markdown","7f50b072":"markdown","dfcb0ccd":"markdown","00f6dffc":"markdown","a3e6cb65":"markdown","07ef6354":"markdown","a863924e":"markdown","3c38e1b0":"markdown","46a028a8":"markdown","71d9ebf6":"markdown","fb3f2de2":"markdown","fa57f0af":"markdown","96093838":"markdown","d5e05a16":"markdown","b814c508":"markdown","c16800cf":"markdown","32e67651":"markdown","1124ba83":"markdown","e9fe257e":"markdown","a17379e9":"markdown"},"source":{"b8687422":"import glob\nimport random as rn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import RMSprop, Adam\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","454c5434":"path = '\/kaggle\/input\/chest-xray-pneumonia\/chest_xray\/'\n\n\n# define paths\ntrain_normal_dir = path + 'train\/NORMAL\/'\ntrain_pneu_dir = path + 'train\/PNEUMONIA\/'\n\ntest_normal_dir = path + 'test\/NORMAL\/'\ntest_pneu_dir = path + 'test\/PNEUMONIA\/'\n\nval_normal_dir = path + 'val\/NORMAL\/'\nval_pneu_dir = path + 'val\/PNEUMONIA\/'\n\n\n# find all files, our files has extension jpeg\ntrain_normal_cases = glob.glob(train_normal_dir + '*jpeg')\ntrain_pneu_cases = glob.glob(train_pneu_dir + '*jpeg')\n\ntest_normal_cases = glob.glob(test_normal_dir + '*jpeg')\ntest_pneu_cases = glob.glob(test_pneu_dir + '*jpeg')\n\nval_normal_cases = glob.glob(val_normal_dir + '*jpeg')\nval_pneu_cases = glob.glob(val_pneu_dir + '*jpeg')\n\n\n# make path using \/ instead of \\\\ ... this may be redudant step\ntrain_normal_cases = [x.replace('\\\\', '\/') for x in train_normal_cases]\ntrain_pneu_cases = [x.replace('\\\\', '\/') for x in train_pneu_cases]\ntest_normal_cases = [x.replace('\\\\', '\/') for x in test_normal_cases]\ntest_pneu_cases = [x.replace('\\\\', '\/') for x in test_pneu_cases]\nval_normal_cases = [x.replace('\\\\', '\/') for x in val_normal_cases]\nval_pneu_cases = [x.replace('\\\\', '\/') for x in val_pneu_cases]\n\n\n# create lists for train, test & validation cases, create labels as well\ntrain_list = []\ntest_list = []\nval_list = []\n\nfor x in train_normal_cases:\n    train_list.append([x, 0])\n    \nfor x in train_pneu_cases:\n    train_list.append([x, 1])\n    \nfor x in test_normal_cases:\n    test_list.append([x, 0])\n    \nfor x in test_pneu_cases:\n    test_list.append([x, 1])\n    \nfor x in val_normal_cases:\n    val_list.append([x, 0])\n    \nfor x in val_pneu_cases:\n    val_list.append([x, 1])\n\n\n# shuffle\/randomize data as they were loaded in order: normal cases, then pneumonia cases\nrn.shuffle(train_list)\nrn.shuffle(test_list)\nrn.shuffle(val_list)\n\n\n# create dataframes\ntrain_df = pd.DataFrame(train_list, columns=['image', 'label'])\ntest_df = pd.DataFrame(test_list, columns=['image', 'label'])\nval_df = pd.DataFrame(val_list, columns=['image', 'label'])","1ad1c80b":"plt.figure(figsize=(20,5))\n\nplt.subplot(1,3,1)\nsns.countplot(train_df['label'])\nplt.title('Train data')\n\nplt.subplot(1,3,2)\nsns.countplot(test_df['label'])\nplt.title('Test data')\n\nplt.subplot(1,3,3)\nsns.countplot(val_df['label'])\nplt.title('Validation data')\n\nplt.show()","501317a6":"plt.figure(figsize=(20,8))\nfor i,img_path in enumerate(train_df[train_df['label'] == 1][0:4]['image']):\n    plt.subplot(2,4,i+1)\n    plt.axis('off')\n    img = plt.imread(img_path)\n    plt.imshow(img, cmap='gray')\n    plt.title('Pneumonia')\n    \nfor i,img_path in enumerate(train_df[train_df['label'] == 0][0:4]['image']):\n    plt.subplot(2,4,4+i+1)\n    plt.axis('off')\n    img = plt.imread(img_path)\n    plt.imshow(img, cmap='gray')\n    plt.title('Healthy \/ Normal')","57a488dd":"def process_data(img_path):\n    img = cv2.imread(img_path)\n    img = cv2.resize(img, (196, 196))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    img = img\/255.0\n    img = np.reshape(img, (196,196,1))\n    \n    return img\n\ndef compose_dataset(df):\n    data = []\n    labels = []\n\n    for img_path, label in df.values:\n        data.append(process_data(img_path))\n        labels.append(label)\n        \n    return np.array(data), np.array(labels)","902b2fe3":"X_train, y_train = compose_dataset(train_df)\nX_test, y_test = compose_dataset(test_df)\nX_val, y_val = compose_dataset(val_df)\n\nprint('Train data shape: {}, Labels shape: {}'.format(X_train.shape, y_train.shape))\nprint('Test data shape: {}, Labels shape: {}'.format(X_test.shape, y_test.shape))\nprint('Validation data shape: {}, Labels shape: {}'.format(X_val.shape, y_val.shape))","dd902d9f":"# define generator\ndatagen = ImageDataGenerator(\n    rotation_range=10,\n    zoom_range = 0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    horizontal_flip=False,\n    vertical_flip=False\n)\n\n# fit generator on our train features\ndatagen.fit(X_train)","2d976873":"y_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\ny_val = to_categorical(y_val)","be5bca40":"model = Sequential()\n\nmodel.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu', input_shape=(196, 196, 1)))\nmodel.add(Conv2D(filters=8, kernel_size=(7,7), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3,3)))\n\nmodel.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))\nmodel.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3,3)))\n\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(Conv2D(filters=64, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(2, activation='softmax'))\n\noptimizer = Adam(lr=0.0001, decay=1e-5)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\ncallback = EarlyStopping(monitor='loss', patience=6)\nhistory = model.fit(datagen.flow(X_train,y_train, batch_size=4), validation_data=(X_test, y_test), epochs = 100, verbose = 1, callbacks=[callback], class_weight={0:6.0, 1:0.5})","81f138bc":"plt.figure(figsize=(20,5))\n\n# plot loss & val loss\nplt.subplot(1,2,1)\nsns.lineplot(x=history.epoch, y=history.history['loss'], color='red', label='Loss')\nsns.lineplot(x=history.epoch, y=history.history['val_loss'], color='orange', label='Val Loss')\nplt.title('Loss on train vs test')\nplt.legend(loc='best')\n\n# plot accuracy and val accuracy\nplt.subplot(1,2,2)\nsns.lineplot(x=history.epoch, y=history.history['accuracy'], color='blue', label='Accuracy')\nsns.lineplot(x=history.epoch, y=history.history['val_accuracy'], color='green', label='Val Accuracy')\nplt.title('Accuracy on train vs test')\nplt.legend(loc='best')\n\nplt.show()","19c34db6":"y_test_hat = model.predict(X_test, batch_size=4)\ny_test_hat = np.argmax(y_test_hat, axis=1)\ny_test = np.argmax(y_test, axis=1)","3c67eda2":"# calculate confusion matrix & classification report\nconf_m = confusion_matrix(y_test, y_test_hat)\nclas_r = classification_report(y_test, y_test_hat)\n\n# plot confusion matrix as heatmap\nplt.figure(figsize=(5,3))\nsns.set(font_scale=1.2)\nax = sns.heatmap(conf_m, annot=True,xticklabels=['H', 'P'], yticklabels=['H', 'P'], cbar=False, cmap='Blues',linewidths=1, linecolor='black', fmt='.0f')\nplt.yticks(rotation=0)\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nax.xaxis.set_ticks_position('top') \nplt.title('Confusion matrix - test data\\n(H - healthy\/normal, P - pneumonia)')\nplt.show()\n\n# print classification report\nprint('Classification report on test data')\nprint(clas_r)","b8e4049e":"y_val_hat = model.predict(X_val, batch_size=4)\ny_val_hat = np.argmax(y_val_hat, axis=1)\ny_val = np.argmax(y_val, axis=1)","ee1316a4":"plt.figure(figsize=(20,15))\nfor i,x in enumerate(X_val):\n    plt.subplot(4,4,i+1)\n    plt.imshow(x.reshape(196, 196), cmap='gray')\n    plt.axis('off')\n    plt.title('Predicted: {}, Real: {}'.format(y_val_hat[i], y_val[i]))  ","e4fdbf9b":"# Pneumonia detection based on Chest X-Ray images on imbalanced data using just few steps and getting accuracy at least 94%","e9c6f066":"Model bit slower, but getting above 92% accuracy that's great result!","27ac713a":"We will also create dataframes so we can visualize distribution of cases in each set","90c3cd86":"# Intro\nToday we are going to apply CNN on X-Ray images of chest. Dataset is imbalanced (approx. 1:3), images may have different site and can have one or 3 color channels.","4777f7cd":"**Wow**, not bad! Actually **great**! There is just one misclassification. Even doctor would have problems to classify it as correct, don't you think?. What about compiling model, creating web app and allow doctors to upload their X-Rays and get information what is likelihood patient has pneumonia? Good business plan ;)","7f50b072":"Now let's show images, real & predicted labels","dfcb0ccd":"It may be redudant step, but we are going to convert our 1D array of target labels into 2D array, changing classification from binary to categorical. During my tests, binary classification was slower and less accurate then categorical.","00f6dffc":"# Visualize distribution of cases\nNow let's draw how are our datasets (im)balanced. You will find out, that training dataset is highly imbalanced, testing dataset is slightly imbalanced and validation dataset is balanced","a3e6cb65":"## Draw loss on train vs test data evolution","07ef6354":"## Validation data\nFirst predict labels and get them to 1D array","a863924e":"Using our functions let's prepare our train, test and validation arrays from dataframes","3c38e1b0":"At start, we will preprocess our data in very simple and intuitive way (load, resize, convert to grayscale, create labels). Model is using grayscale images as for me it did not make too much sense to use X-Ray images as colorful images.","46a028a8":"# Load data\nOur data consists of 3 folders - train, test, val. Train & test are used for modeling, validation will be used to check performance of model. Size of validation set is very small (16 cases).","71d9ebf6":"### Thanks for checking my notebook, if you liked it, make sure to **vote** for for this notebook!","fb3f2de2":"This kind of load may not work well for large dataset as you will run out of memory. Data generator with flow_from_directory could be used, but it's really slow","fa57f0af":"# Draw few samples of each case\nIn this step we simply want to see few cases of pneumonia and few cases of healthy people. For uninterested person it may not be easy to identify pneumonia on image. You will also notice that images does not have same dimension and must be rescaled to same width & height.","96093838":"## Confusion matrix on test data","d5e05a16":"To make it simple, define 2 functions.\n* **process_data** - load image, resize it, convert to grayscale, normalize and reshape to dimension required for tensorflow\n* **compose_dataset** - loop through images, generating 2 numpy arrays. First contains image itself as matrix, second contains label","b814c508":"# Load libraries\nWe will use standard \"data science\" libraries including numpy, pandas, tensorflow, matplotlib and seaborn. Some additional libraries like glob and cv2 are used.","c16800cf":"# Data preprocessing\nIn following section we are going to prepare our data for modeling. Down-size of this technique is that you may run out of memory very quickly... but it's really fast!","32e67651":"It seems our algorithm is more likely predicting pneumonia, that is probably caused by imbalanced dataset and even setting class weights in tensorflow fit did not helped so much. We could increase weight for normal\/healthy cases to get even better results.","1124ba83":"# Evaluation\nFirst, we will quickly check evolution of loss and accuracy over epochs and then draw confusion matrix on test dataa. Then how our validation set (16 cases) will work with trained model and compare real vs predicted label","e9fe257e":"# Modelling\nSimple sequential model is used, starting with 2 convolutional networks of kernel size (7,7) and max pooling with pool size (3,3), followed by 2 convolutional networks of kernel size (7,7) and same pool size and finalized by several repeating sets of 2 convolutional networks of kernel size (3,3) with max pooling and pool size (2,2)","a17379e9":"**Image augmentation** is very important to make our model robust to unseen data. It takes each image and modify it slightly so simply said in each epoch, different image (generated from same) is sent to model for training."}}