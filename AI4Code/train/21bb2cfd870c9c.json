{"cell_type":{"189045e6":"code","073011cf":"code","15c00315":"code","8e237310":"code","cb54a1e2":"code","a7e50520":"code","50752602":"code","2641dd10":"code","4e06e54e":"code","9765cf78":"code","993d3862":"code","5c8b6c77":"code","c3797b3c":"code","5dac5c19":"code","818762e1":"code","e1a90fd3":"code","89dfb9c4":"code","3c1168ce":"code","a79abd91":"code","3f96a6a5":"code","058a61c4":"code","e538382a":"code","24e0e710":"code","ead51b12":"code","e989026d":"code","40e89d3c":"markdown","b6c2bb4b":"markdown","1db57853":"markdown","3b3f1e1a":"markdown","9611a285":"markdown","8fb6e2a7":"markdown","a8d14b84":"markdown","134a19c2":"markdown","9328efcd":"markdown","3a206ec4":"markdown","3902e51d":"markdown","49652bb4":"markdown","97046496":"markdown","60d1abd9":"markdown","b4c03266":"markdown","d26a49ee":"markdown","21ccf330":"markdown","f7c686ab":"markdown","f2fdc6da":"markdown","5d617eec":"markdown"},"source":{"189045e6":"\"\"\"# LSTM with Dropout for sequence classification in the IMDB dataset\nimport numpy\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\n# fix random seed for reproducibility\nnumpy.random.seed(7)\n# load the dataset but only keep the top n words, zero the rest\ntop_words = 5000\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n# truncate and pad input sequences\nmax_review_length = 500\nX_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\nX_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n# create the model\nembedding_vecor_length = 32\nmodel = Sequential()\nmodel.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\nmodel.fit(X_train, y_train, epochs=3, batch_size=64)\n# Final evaluation of the model\nscores = model.evaluate(X_test, y_test, verbose=0)\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))\"\"\"","073011cf":"# basic library imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, Dropout\nfrom keras.layers import LSTM\nfrom keras.layers import GRU\nfrom keras.layers import Conv1D, MaxPooling1D, GlobalMaxPooling1D\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.utils import compute_class_weight\nfrom keras import optimizers\nfrom sklearn.utils import shuffle\nfrom keras.utils import to_categorical\nfrom time import time, localtime, strftime\n\nimport os\nprint(os.listdir(\"..\/input\"))","15c00315":"\"\"\"common settings for model training, remember to choose!!!\"\"\"\n\nMOSTCOMMONWORDS = 2000 #WORDS = 1000\nREVIEWLENGTH = 400 #LENGTH = 100\nSAMPLES = 10000 #N = 10000\nWORDEMBEDDIM = 12\nopt0 = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\nopt1 = optimizers.RMSprop()\nEPOCHS = 20\nenableStopwords = True\n\nMODELNAME = \"brownleeConv1D_LSTM\"\nVERBOSE = 2\nBATCHSIZE = 32\nVALIDATIONPERCENT = 0.25\n#CW = cw #classweights","8e237310":"# Read the training data\ndata = pd.read_csv('..\/input\/drugsComTrain_raw.csv')\ndata = shuffle(data)\ndata.head(1000)","cb54a1e2":"## here we must add the proper character single quote ' into the dataset so that nltk will work nicely with it\n\nif enableStopwords:\n    data['review'] = data['review'].str.replace(\"&#039;\", \"'\", regex=False)\n    data.head(1000)","a7e50520":"# Create multiclass classifier labels from numeric ratings\n# NOTE! you can use the \\ character for multiline python statements!\n\nr = data['rating']\nlabels = \\\n1* ((0 <= r) & ( r <= 2))  \\\n+ 2*((3 <= r) & (r <= 4)) \\\n+ 3*((5 <= r) & (r <= 6)) \\\n+ 4*((7 <= r) & (r <= 8)) \\\n+ 5*((9 <= r) & (r <= 10))\n\nlabels = labels -1 #this prevents class errors from later on, so now the classes should be from 0 to including 4\n\ndata['label'] = labels\ndata.head(15)","50752602":"# double check that the ratings and labels distribution worked\ndata.plot(x = 'rating', y = 'label', kind = 'scatter')\nplt.show()\n","2641dd10":"# show labels histogram and distribution\ndata.hist(column = 'label', bins = np.arange(-1, 8), align = 'left');\n","4e06e54e":"\n# get some information about the length of the review texts, to see how big they tend to be\ndata['review_length'] = data['review'].apply(len)\ndata['review_length'].describe()","9765cf78":"data['label'].describe(), data['label'].unique()\n","993d3862":"# histogram of review_lengths of the reviews\ndata.hist('review_length', bins = np.arange(0, 1500, 100));\nplt.title('Distribution of the reviews lengths')\nplt.xlabel('Review length')\nplt.ylabel('Count')\nplt.show()\n","5c8b6c77":"\"\"\"w = compute_class_weight('balanced', np.unique(data['label']), data['label'])\ncw = {0:w[0], 1:w[1], 2:w[2], 3:w[3], 4:w[4]}\ncw\"\"\"","c3797b3c":"\n# Read a part of the reviews and create training sequences (x_train)\nsamples = data['review'].iloc[:SAMPLES]\n\ntestsamples = data['review'].iloc[SAMPLES : 2*SAMPLES] ##get test_data, and later on get test_targets, so we can compare results\n\n","5dac5c19":"\"\"\"for the preprocessing with stopwords and lemmatization, I recommend the episode of Karpon Parhaat episode 12, from 11:56 onwards\nhttps:\/\/youtu.be\/rovKTI7_35o?t=717\n\nIt's absolutely glorious video about patient diagnosis audio dictation, where doctors themselves typically omit\nuseless words for diagnosis, but this results in hilarious results for the actual patient history (based on the audio\ndictation) (finnish language only of course!)\n\"\"\"\n\n\"\"\"attempt to use nltk tokenizing and lemmatization with stopwords so we dont get 'garbage in, garbage out' phenomenon.\nnltk stopwording and lemmatizations should help firstly with stopwords so that the useless English words like\n'you', 'me' 'I' dont end up spamming our later keras tokenizer because those are very common words but they are \nalso useless really for any kind of analysis in a traditional sense of text analysis. These kinds of useless\nEnglish words are also harmful probably for our keras model, because it tries to make sense of garbage words, that \nare very common, but of low informatinal value.\n\nAlso, lemmatization should make only the basic version of the word available into the keras tokenizing, such as\ndrugs's turns into drug which is the base word\n\"\"\"\n\n\nimport nltk\nimport urllib.request\nimport math\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\n\n\"\"\"helper function from Vesa Ollikainen's example code for text analytics\"\"\"\ndef stopword_and_lemmatize(review):\n    \n    tokens = review.split()\n    tokens=[token.lower() for token in tokens if token.isalpha()]\n    sw = stopwords.words('english')\n    ctokens = tokens\n    for token in tokens:\n        if token in sw:\n            ctokens.remove(token)\n    lemmatizer = WordNetLemmatizer()\n    for i in range(len(ctokens)):\n        ctokens[i] = lemmatizer.lemmatize(ctokens[i])\n        \n\n    return ' '.join(ctokens)\n\n\"\"\"checking the results of stopword_and_lemmatize on samples\nfrom my point of view it looked like at least the stopwords functioned\nproperly, I'm not sure if the lemmatization really worked???!!!\"\"\"\n\nprint(\"original was:\\n\", samples.head(5))\nif enableStopwords:\n    str0 = samples.apply(stopword_and_lemmatize)\n    print(\"transformed was:\\n\",str0.head(5))\n    samples = str0 ##assigns the lemmatized and stopworded series into the samples, GOOD\n    str1 = testsamples.apply(stopword_and_lemmatize)\n    testsamples =str1\n\n","818762e1":"\ntokenizer = Tokenizer(num_words = MOSTCOMMONWORDS)\ntokenizer.fit_on_texts(samples)\nsequences = tokenizer.texts_to_sequences(samples)\nx_train = pad_sequences(sequences, maxlen = REVIEWLENGTH)\n\ntokenizer1 = Tokenizer(num_words = MOSTCOMMONWORDS)\ntokenizer1.fit_on_texts(testsamples)\ntestsequences = tokenizer1.texts_to_sequences(testsamples)\nx_test = pad_sequences(testsequences, maxlen = REVIEWLENGTH) ##test_data, and later we will get test_targets in one hot coded way\n","e1a90fd3":"\"\"\"\n# Convert the labels to one_hot_category values\n\"\"\"\n#train_labels = labels[:SAMPLES]\none_hot_labels = to_categorical(labels[:SAMPLES], num_classes = 5)\n\ntestlabels = labels[SAMPLES : 2*SAMPLES]\none_hot_testtargets = to_categorical(testlabels, num_classes = 5)\n\n# Check the training and label sets\nx_train.shape, one_hot_labels.shape","89dfb9c4":"# We use the same plotting commands several times, so create a function for that purpose\ndef plot_history(history):\n    f, ax = plt.subplots(1, 2, figsize = (16, 7))\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    epochs = range(1, len(acc) + 1)\n    plt.sca(ax[0])\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.sca(ax[1])\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n    plt.show()\n    \n# Similarly create a function for model training, for demonstration purposes we use constant values\ndef train_model(model, x, y, e = EPOCHS, bs = BATCHSIZE, v = VERBOSE, vs = VALIDATIONPERCENT):\n    h = model.fit(x, y, batch_size = bs, epochs = e,  verbose = v, validation_split = vs)\n    return h","3c1168ce":"# Seventh model: Embedding -> 2 x Conv1D + MaxPooling -> GRU -> Dense\nm7 = Sequential()\nm7.add(Embedding(MOSTCOMMONWORDS, WORDEMBEDDIM, input_length = REVIEWLENGTH))\nm7.add(Conv1D(filters=WORDEMBEDDIM, kernel_size=3, padding='same', activation='relu'))\nm7.add(MaxPooling1D(pool_size=2))\nm7.add(LSTM(units = WORDEMBEDDIM, dropout = 0.4, recurrent_dropout = 0.4))\nm7.add(Dropout(0.4))\nm7.add(Dense(5, activation = 'softmax'))\nm7.compile(optimizer = opt1, loss = 'categorical_crossentropy', metrics = ['acc'])\nm7.summary()\n","a79abd91":"# Train and plot the history\nh7 = train_model(m7, x_train, one_hot_labels)\nplot_history(h7)","3f96a6a5":"##save trained model\nmodel_name = strftime('Case3-%Y-%m-%d-%H%M%S', localtime()) + MODELNAME\nprint('modelname was=',model_name,'\\n')\n\n\n# Save the model\nm7.save(model_name)\nprint('')\nprint('Model saved to file:', model_name)\nprint('')","058a61c4":"from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report\nfrom sklearn.metrics import accuracy_score\nimport warnings\n\n# Function to display confusion matrix, classification report and final accuracy\ndef display_results(model, test_data, test_targets):\n    # Get the true and predicted values\n    y_true = test_targets\n    predict = model.predict(test_data)\n    y_pred = np.argmax(predict, axis = 1)\n\n    \n\n    # Calculate and print the metrics results\n\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n    \n        cm = confusion_matrix(y_true, y_pred)\n        print('Confusion matrix:')\n        print(cm)\n        print('')\n\n        a = accuracy_score(y_true, y_pred)\n        print('Accuracy: {:.4f}'.format(a))\n        print('')\n\n        cr = classification_report(y_true, y_pred)\n        print('Classification report:')\n        print(cr)\n        \n        print('')\n        k = cohen_kappa_score(y_true, y_pred, weights = 'quadratic')\n        print(\"Quadratic weighted Cohen's kappa score = {:.4f}\".format(k))","e538382a":"# Get the true labels for testdata and check them\ny_true = testlabels\nprint(y_true)\n\n","24e0e710":"## get the predicts based on testdata and check them also\npredicts = m7.predict(x_test)\nprint(predicts )\nprint('')","ead51b12":"## transform the predicts into regular format, from the one hot coding\ny_pred = np.argmax(predicts, axis = 1)\nprint(y_pred)\nprint('')\n\n## check what it predicted\nprint(np.unique(y_pred))","e989026d":"\n##print confusionMatrix, classificationReport, weightedCohenKappa,\ndisplay_results(m7, x_test, testlabels)","40e89d3c":"##  brownlee's model 1st run\n\n1. it heavily overfits quite fast, but the good thing is that in hybrid models conv1D and RNN the training time is short and train acc is very good\n2. I will attempt add more dropout layers to the brownlee model after maxpool and lstm most likely","b6c2bb4b":"## replace the \"&#039\"; with the proper character '","1db57853":"## make the 5 class labels from the numeric ratings [0,1,2,3,4]","3b3f1e1a":"## make classweights with sklearn to use for models\n* EDIT:: I wasn't sure if one-hot coding will actually work well with class weights from sklearn, I read about some conflicting information that \nmaybe class_weights doesn't work so well with one hot labels","9611a285":"## What to do...\n\n* make multiclass classifier with RNN\/LSTM\/GRU based model, and use it to classify drug review texts and predict the category (1-5)\n* predict the multiclass type of drug rating based on the drug reviews\n* give metrics about accuracy, cohen's kappa, classification report\n* maybe 4-5 categories from the numeric drug review ratings and try to classify converted ratings, based on the drug reviews\n* I will try to use embedding layer and word embedding, and it appears one-hot coding of labels may be necessary also (multiclass)\n\n* use nltk libraries preprocessing for the raw string reviews so you discard stopwords and hopefully lemmatize also\n\n* hope for the best! and print the results near the end","8fb6e2a7":"## Convert labels to one-hot-categories (shouldlnt be necessary with sparse_crossentropy)\n* EDIT:: actually it was necessary because I got poor results accuracy== 0 when using sparse_categoricalcrossentropy,\nso that I just switched back to the regular categorical_crossentropy loss function. And to use the plain old categorical_crossentropy there was the one-hot-coding of labels","a8d14b84":"# case3 drug review, predict rating multiclass classifier with RNNnetworks\nCognitive Systems for Health Technology Applications<br>\n15.3.2019, Lauri Solin<br>\n[Helsinki Metropolia University of Applied Sciences](https:\/\/www.metropolia.fi\/en)\n\nuseful links for sources:\n\nEspecially Great thanks to our teacher Sakari Lukkanen for giving example code for keras!\n\nAlso thanks to our cognitive systems math teacher Vesa Ollikainen for giving stopwords lemmatization code example\n\nalso professor Jason Brownlee had great materials online so I definitely read his materials also\n\nhttps:\/\/machinelearningmastery.com\/sequence-classification-lstm-recurrent-neural-networks-python-keras\/\n\nhttps:\/\/www.kaggle.com\/sakarilukkarinen\/demo-02-checking-the-metrics\n\nhttps:\/\/machinelearningmastery.com\/make-predictions-long-short-term-memory-models-keras\/\n\nhttps:\/\/github.com\/sakluk\/cognitive-systems-for-health-technology\/blob\/master\/Week_2_Case_1_(drafty_notes).ipynb\n\n\n\nhttps:\/\/shrikar.com\/deep-learning-with-keras-and-python-for-multiclass-classification\/\n\nhttps:\/\/www.kaggle.com\/moghazy\/predecting-rating-based-on-reviews\n\nhttps:\/\/www.kaggle.com\/sakarilukkarinen\/embedding-lstm-gru-and-conv1d?scriptVersionId=11256850\n\nhttps:\/\/stackoverflow.com\/questions\/45047266\/keras-sparse-categorical-crossentropy-loss-function-output-shape-didnt-match\n","134a19c2":"## brownlee's model 3rd run\n1. the results were such that it still overfits quite heavily\n2. I noticed that the embedding layer has the largest percentage of parameters (160000\/ 171589= 0.93 = 93% of all trainables was inside embedded layer), so that if we want to reduce overfitting, we must use smaller and smaller embedding layer worddimension\n3. currently the wordembeddim = 32, which in my opinion results in the overfit because vast majority of trainable parameters are there in the wordembeddings, but learning those parameters doesn't seem to help in getting good validation accuracy at all, and validation loss was bad also.\n4. changes for the next run must include diminishing the wordembeddim = 12 or something small like that, also reduce the units in LSTM, and reduce units in the Convfilters\n5. also reduce mostcommonwords=2000, it has effect on embedded layer trainable parameters","9328efcd":"## Settled down version brownleeLSTM+ conv1D\n\n* I decided that ultimately the \"brownlee model\" was still little bit overfitting, because I tried keeping same layers, but increasing LSTM units,  and conv filter amounts, and the result\nwas that the overfitting became worse, and therefore started at earlier stage in epochs\n\n* commit 6 from this notebook gave a relatively good fit, and I will base this final commit version on that commit 6 model's architecture. where the \n\n> reviewlength=400, \n\n> mostcommonwords = 2000, \n\n> wordembeddim=12, \n\n> samples=10000, \n\n> epochs= 20, \n\n> optimizer= RMSprop with default learnrate, \n\n> nltk preprocessing was used\n\n> trainable parameters can be found in this notebook's model summary\n\n\n* the model was not perfect and it was more difficult to optimize the convnet + LSTM style architectures because there wasn't a clear sense how much the conv1D filters affect the situation.\n\n\n## other \"worthwhile\" and good architectures that I tried to optimize were: \n\n1. teacher's GRU+dropout,  commit 5  https:\/\/www.kaggle.com\/late347\/case3-rnn-predictdrugreview-gru-drop\n2. and teacher's dual stack GRU with dropout, commit 4  https:\/\/www.kaggle.com\/late347\/case3-rnn-predictdrugreview-stackofgrus\n3. teacher's conv1D + GRU, commit 9  https:\/\/www.kaggle.com\/late347\/case3-rnn-predictdrugreview-conv1d-gru\n\n\n\n","3a206ec4":"## prof Jason Brownlee's adapted model with LSTM and conv1D","3902e51d":"## brownlee's model 2nd run\n1.  the results were such that it still overfits quite heavily, so that trainacc is very high at 0.7 at the end,  and trainloss dropped nicely, but validationacc stalled, and validation loss increased\n\n2. changes for next run: reduce LSTM units = 32, increase all dropouts =0.4. NOTE:: according to keras documentation lstm dropout only affects inputs to that lstm, so that I decided to put originally a plain old dropout layer immediately after the LSTM layer, and recurrent_dropout only affects recurrent state weights according to keras docs \n\n3. I will drop plain old dropout from between maxpool and LSTM because it seems not necessary\n\n\n> dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the inputs.\n\n\n> recurrent_dropout: Float between 0 and 1. Fraction of the units to drop for the linear transformation of the recurrent state.\n\n4. hope for the best, and train for 20 epochs","49652bb4":"## \u00b4Brownlee's original model served as inspiration (but original model overfitted I think, so I reduced it)","97046496":"## Helper functions","60d1abd9":"## more optimizations\n* it appears that the embedding layer parameters= wordembedDimension x dictionarySize, so that we should maybe try to keep wordEmbed dimension smaller, and maybe then increase the dictionarySize a bit\n* then, we can attempt to increase model complexity to see if the rest of the model (not embedded parameters only) is able to learn anything\n* keep hyperparameters same as gru+dropout version (dictSize= 1000, reviewlength=500, embeddim = 8), train for 15 epochs","b4c03266":"## Text pre-processing\n\n1.  sample total amount of reviews\n1.  tokenize\n1.  form dictionary of most common words\n1. make to sequences\n1. padding\/cutoff into equal length sequences","d26a49ee":"## attempt to use proper preprocessing nltk with stopwords and lemmatization","21ccf330":"## metrics helper function for confusionMatrix, cohenKappa and classificationReport","f7c686ab":"## make sanity check what we get from our LSTM+conv1D model, with testdata and testlabels and predicts","f2fdc6da":"## Data preprocessing","5d617eec":"## Overall results from \"Brownlee conv1D+LSTM\"\n\n1.  it appears that the model predictions were quite bad based on the cohen kappa score being so low\n2. also test accuracy was a bit low compared to validation accuracy \n3. also it seems that the sanity check revealed that the model essentially trained on the wrong things, because it only predicted so heavily either 0 or 4. \n4.  same can be seen in the confusion matrix also.\n5.  This was very depressing because the validation accuracy and trainign accuracy had a good trendline, at least, but the underlying model apparently was based on false pretences in a sense\n6. possible remedies could be to implement properly class_weights into the model training because the ratingslabels from [0, 1, 2, 3, 4] were a bit unbalanced in the beginning\n7. alternatively there is a way to re-sample to data such that you get certain amount of random ratingslabels for each category, but it was a bit difficult to code\n8. if there is a way to introduce the class_weights properly into softmax classifier, then the other option could be to test the model with raw unprocessed reviewtexts, vs the nltk text review preprocessing and see if there was any difference. (certainly from validationphase it didn't seem to make huge diference either way)\n9. other avenue of research could have been to increase the SAMPLES from 10000 to something of a bigger slice from the traindataset and see how much longer it would take to train the model, and to see if there was any differences in test set results\n\n\n10. currently the trainingtime per epoch = 115s in the kaggle, which was a bit on the good side of the \"patience-barrier\" for my own patience. But the other stacked GRUs and LSTMs usually took a huge amount of time to train."}}