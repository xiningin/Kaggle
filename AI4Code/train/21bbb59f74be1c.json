{"cell_type":{"3e12e237":"code","f4e3237d":"code","21fc6572":"code","a7a8f9f9":"code","ed9af6eb":"code","21a06afd":"code","4c24d9d6":"code","da4db925":"code","1474618b":"code","a0da7a2c":"code","8973404e":"code","c95d8552":"code","d33cb383":"code","5d5b1c02":"code","4b7c91fd":"code","e42169c8":"code","5e88b630":"code","6aa5224c":"code","fe64ce74":"markdown","b1366296":"markdown","11d0a4ba":"markdown","079032ca":"markdown","c53e3f77":"markdown","5f3cba03":"markdown"},"source":{"3e12e237":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n!pip install catboost\n!pip install xgboost\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f4e3237d":"data = pd.read_csv(\"\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv\")\ndata","21fc6572":"data.describe().T","a7a8f9f9":"data[\"age\"].plot(kind = \"hist\")","ed9af6eb":"# corr_matrix = data.corr().T\n\n# from pandas.plotting import scatter_matrix\n\n# scatter_matrix(data)\ndata.corr().T","21a06afd":"# target and feature data\ny = data[\"output\"].copy()\nX = data.copy()\ndel X[\"output\"]\n# del X[\"fbs\"]\n# del X[\"chol\"]\nX","4c24d9d6":"# Train test and split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40, random_state = 32)","da4db925":"# clf = DecisionTreeClassifier(max_leaf_nodes = 5, random_state = 80) = 85%\nclf = CatBoostClassifier(verbose=0, n_estimators=40)\nclf.fit(X_train, y_train)","1474618b":"predictions = clf.predict(X_test)","a0da7a2c":"actuals = np.ceil(list(y_test))\n\nfor prediction,actual in list(zip(predictions, actuals))[:10]:\n    print(\"Prediction:\", int(prediction), \"Actual:\", int(actual))","8973404e":"import seaborn as sn\n\nconfusion_matrix = pd.crosstab(actuals, predictions, rownames=['Actual'], colnames=['Predicted'])\n\nsn.heatmap(confusion_matrix, annot=True)\n\nconfusion_matrix","c95d8552":"from sklearn.metrics import confusion_matrix\nconfusion_matrix = confusion_matrix(actuals, predictions)\nconfusion_matrix","d33cb383":"# Get confusion matrix metrics\ntrue_positives = int(confusion_matrix[:1,:1])\ntrue_negatives = int(confusion_matrix[-1:, -1:])\nfalse_positives = int(confusion_matrix[:1,-1:])\nfalse_negatives = int(confusion_matrix[-1:, :-1])","5d5b1c02":"# Sensitivity\nsensitivity = round(true_positives \/ (true_positives + false_negatives), 2)\nsensitivity","4b7c91fd":"# Specificity\nspecificity = round(true_negatives \/ (true_negatives + false_positives), 2)\nspecificity","e42169c8":"# Precision\nprecision = round(true_positives \/ (true_positives + false_positives),2)\nprecision","5e88b630":"# Accuracy\nround(accuracy_score(predictions, y_test), 2)","6aa5224c":"# F1 score\nround(2*(specificity * precision) \/ (specificity + precision), 2)","fe64ce74":"# **Train, test, split and assign classifier**","b1366296":"# **Predictions**","11d0a4ba":"# **Set features and lables**","079032ca":"# **Explore the data**","c53e3f77":"# **Accuracy : 88%**\n# **F1 Score : 87%**","5f3cba03":"# **Confusion matrix**"}}