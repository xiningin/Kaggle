{"cell_type":{"162f356b":"code","f8839bc5":"code","ac41ceeb":"code","7d1da83f":"code","02862c3f":"code","b9b923d4":"code","52b51625":"code","af6f62cc":"code","48a2a1a7":"code","76b38bf2":"code","ee9c774e":"code","c797935f":"code","eff5c3f8":"code","67e00177":"code","b40d8a33":"code","eef11805":"markdown","fcaaebe1":"markdown","82fa30ec":"markdown","16ad2f73":"markdown","f7fbdff1":"markdown","1643a53f":"markdown","902201f8":"markdown","ae7fb3d5":"markdown","29c3bd18":"markdown","e7c2a06a":"markdown","cafc5555":"markdown","f7e776a1":"markdown","b3d8baf7":"markdown","abc781cb":"markdown"},"source":{"162f356b":"import torch\nimport numpy as np\nfrom datetime import datetime\nfrom torch.utils.data import DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport pandas as pd\nimport gc\nfrom sklearn.preprocessing import OneHotEncoder\nfrom torch.utils.data import Dataset\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\n\ndef seed_everything(seed=42):\n    print('Setting Random Seed')\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","f8839bc5":"fts_categorical = ['cat0', 'cat1', 'cat2', 'cat3', 'cat4', 'cat5', 'cat6', 'cat7', 'cat8', 'cat9', 'cat10', \n                   'cat11', 'cat12', 'cat13', 'cat14', 'cat15', 'cat16', 'cat17', 'cat18']\n\nfts_continuous = ['cont0', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7', 'cont8', 'cont9', 'cont10']\n\n#unique counts should be the count of train PLUS test\nunique_counts=[  2,  15,  19,  13,  20,  84,  16,  51,  61,  19, 307,   2,   2,\n         2,   2,   4,   4,   4,   4]\n\nprint('Categorical Features', fts_categorical)\nprint('Continuous Features', fts_continuous)\n\nprint('Categorical Feature Count', len(fts_categorical))\nprint('Continuous Feature Count', len(fts_continuous))","ac41ceeb":"PATH = '\/kaggle\/input\/tabular-playground-series-mar-2021\/'\n\nrun_key = 'DAE_TABMAR21_ST3' #due to long run time prefer to split into sections of 600 epochs\n\nDAE_CFG = {'load_path': '\/kaggle\/input\/tabmar21-final-dae-030421\/DAE_TABMAR21_ST2_model_checkpoint_final.pth',\n           'batch_size': 384, \n           'init_lr': np.round(0.0003 * (0.998**1200),8), \n           'lr_decay': 0.999, #rate of decrease of learning rate - i set this closer to 1.0 for stage 3\n           'noise_decay': 0.9995, #rate of decrease of noise level - i set this closer to 1.0 for stage 3\n           'max_epochs': 600, \n           'save_freq': 50, \n           'hidden_size': 1024, \n           'num_subspaces': 8, \n           'embed_dim': 128, \n           'num_heads': 8, \n           'dropout': 0, \n           'feedforward_dim': 512, \n           'emphasis': 0.75, #weighing of loss to 'corrupted' data points - i tried varying over time, did not show immediate improvement\n           'task_weights': [19, 11], #weighting for continuous vs categorical\n           'mask_loss_weight': 2, #weighting of mask prediction vs prediction of reconstructed original data values\n           'prob_categorical': np.round(0.5* (0.999**1200),4), #probability of noise in categoricals\n           'prob_continuous': np.round(0.5* (0.999**1200),4), #probability of noise in continuous\n           'run_key': run_key}\n\nwith open(f\"DAE_CFG_{run_key}\", 'wb') as f:\n    pickle.dump(DAE_CFG, f)","7d1da83f":"DAE_CFG","02862c3f":"tracking_df = pd.DataFrame(index=range(DAE_CFG['max_epochs']),\n                           columns=['loss', 'lr', 'run_code', 'time', 'noise_categorical', 'noise_continuous'],\n                           data=0.0)\n\ntracking_df['lr'] = DAE_CFG['init_lr'] * (DAE_CFG['lr_decay']**tracking_df.index)\ntracking_df['noise_categorical'] = DAE_CFG['prob_categorical'] * (DAE_CFG['noise_decay']**tracking_df.index)\ntracking_df['noise_continuous'] = DAE_CFG['prob_continuous'] * (DAE_CFG['noise_decay']**tracking_df.index)\ntracking_df['run_code'] = run_key\n\nsns.set(font_scale=1.4)\nfig,axes=plt.subplots(nrows=1,ncols=3,figsize=(18,6))\n\naxes[0].plot(tracking_df.index, tracking_df['lr'], color='Blue')\naxes[0].set_ylim(0,)\naxes[0].set_title('Learning Rate')\naxes[0].set_xlabel('Epochs')\n\naxes[1].plot(tracking_df.index, tracking_df['noise_categorical'], color='Red')\naxes[1].set_ylim(0,1)\naxes[1].set_title('Categorical Noise Prob')\n\naxes[2].plot(tracking_df.index, tracking_df['noise_continuous'], color='Red')\naxes[2].set_ylim(0,1)\naxes[2].set_title('Continuous Noise Prob')\n\nplt.tight_layout()","b9b923d4":"def get_data():\n    train_data = pd.read_csv(PATH+'train.csv')\n    test_data = pd.read_csv(PATH+'test.csv')\n    \n    #combine train and test data vertically\n    X_nums = np.vstack([\n        train_data.iloc[:, 20:-1].to_numpy(),\n        test_data.iloc[:, 20:].to_numpy()\n    ])\n    X_nums = (X_nums - X_nums.mean(0)) \/ X_nums.std(0) #normalize\n    \n    #stack the categorical data\n    X_cat = np.vstack([\n        train_data.iloc[:, 1:20].to_numpy(),\n        test_data.iloc[:, 1:20].to_numpy()\n    ])\n    #encode the categoricals\n    encoder = OneHotEncoder(sparse=False)\n    X_cat = encoder.fit_transform(X_cat)\n    \n    #join the categorical and one hot encoded continuous data horizontally\n    X = np.hstack([X_cat, X_nums])\n    y = train_data['target'].to_numpy().reshape(-1, 1)\n    return X, y, X_cat.shape[1], X_nums.shape[1] #this lets us know how many categorical and continuous features there are\n\nclass SingleDataset(Dataset):\n    def __init__(self, x, is_sparse=False):\n        self.x = x.astype('float32')\n        self.is_sparse = is_sparse\n\n    def __len__(self):\n        return self.x.shape[0]\n\n    def __getitem__(self, index):\n        x = self.x[index]\n        if self.is_sparse: x = x.toarray().squeeze()\n        return x    ","52b51625":"bce_logits = torch.nn.functional.binary_cross_entropy_with_logits\nmse = torch.nn.functional.mse_loss","af6f62cc":"#torch docs\n\n#embed_dim \u2013 total dimension of the model.\n#num_heads \u2013 parallel attention heads.\n#dropout \u2013 a Dropout layer on attn_output_weights. Default: 0.0.\n#bias \u2013 add bias as module parameter. Default: True.\n#add_bias_kv \u2013 add bias to the key and value sequences at dim=0.\n#add_zero_attn \u2013 add a new batch of zeros to the key and value sequences at dim=1.\n#kdim \u2013 total number of features in key. Default: None.\n#vdim \u2013 total number of features in value. Default: None.\n\nclass TransformerEncoder(torch.nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout, feedforward_dim):\n        super().__init__()\n        self.attn = torch.nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)\n        self.linear_1 = torch.nn.Linear(embed_dim, feedforward_dim)\n        self.linear_2 = torch.nn.Linear(feedforward_dim, embed_dim)\n        self.layernorm_1 = torch.nn.LayerNorm(embed_dim)\n        self.layernorm_2 = torch.nn.LayerNorm(embed_dim)\n    \n    def forward(self, x_in):        \n        attn_out, _ = self.attn(x_in, x_in, x_in)        \n        x = self.layernorm_1(x_in + attn_out)\n        ff_out = self.linear_2(torch.nn.functional.relu(self.linear_1(x)))\n        x = self.layernorm_2(x + ff_out)\n        return x","48a2a1a7":"class TransformerAutoEncoder(torch.nn.Module):\n    def __init__(\n            self, \n            num_inputs, \n            n_cats, \n            n_nums, \n            hidden_size=1024, \n            num_subspaces=8,\n            embed_dim=128, \n            num_heads=8, \n            dropout=0, \n            feedforward_dim=512, \n            emphasis=.75, \n            task_weights=[len(fts_categorical), len(fts_continuous)],\n            mask_loss_weight=2,\n        ):\n        super().__init__()\n        assert hidden_size == embed_dim * num_subspaces\n        self.n_cats = n_cats\n        self.n_nums = n_nums\n        self.num_subspaces = num_subspaces\n        self.num_heads = num_heads\n        self.embed_dim = embed_dim\n        self.emphasis = emphasis\n        self.task_weights = np.array(task_weights) \/ sum(task_weights)\n        self.mask_loss_weight = mask_loss_weight\n\n        self.excite = torch.nn.Linear(in_features=num_inputs, out_features=hidden_size)\n        self.encoder_1 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        self.encoder_2 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)\n        self.encoder_3 = TransformerEncoder(embed_dim, num_heads, dropout, feedforward_dim)        \n        \n        self.mask_predictor = torch.nn.Linear(in_features=hidden_size, out_features=num_inputs)\n        self.reconstructor = torch.nn.Linear(in_features=hidden_size + num_inputs, out_features=num_inputs)\n\n    def divide(self, x):\n        batch_size = x.shape[0]\n        x = x.reshape((batch_size, self.num_subspaces, self.embed_dim)).permute((1, 0, 2))\n        return x\n\n    def combine(self, x):\n        batch_size = x.shape[1]\n        x = x.permute((1, 0, 2)).reshape((batch_size, -1))\n        return x\n\n    def forward(self, x):\n        x = torch.nn.functional.relu(self.excite(x))\n        \n        x = self.divide(x)\n        x1 = self.encoder_1(x)\n        x2 = self.encoder_2(x1)\n        x3 = self.encoder_3(x2)\n        x = self.combine(x3)\n        \n        predicted_mask = self.mask_predictor(x)\n        reconstruction = self.reconstructor(torch.cat([x, predicted_mask], dim=1))\n        return (x1, x2, x3), (reconstruction, predicted_mask)\n\n    def split(self, t):\n        return torch.split(t, [self.n_cats, self.n_nums], dim=1)\n\n    def feature(self, x):\n        attn_outs, _ = self.forward(x)\n        return torch.cat([self.combine(x) for x in attn_outs], dim=1) #the feature is the data extracted for use in inference\n\n    def loss(self, x, y, mask, reduction='mean'):   \n        _, (reconstruction, predicted_mask) = self.forward(x)\n        \n        x_cats, x_nums = self.split(reconstruction)\n        y_cats, y_nums = self.split(y)\n        \n        #weights are detemined by the emphasis - which is currently heavier weights for corrupted data (mask = 1)\n        w_cats, w_nums = self.split(mask * self.emphasis + (1 - mask) * (1 - self.emphasis))\n        \n        #BCE loss for reconstructed vs actual categoricals\n        cat_loss = self.task_weights[0] * torch.mul(w_cats, bce_logits(x_cats, y_cats, reduction='none')) \n        \n        #mse loss for reconstructed vs actual continuous\n        num_loss = self.task_weights[1] * torch.mul(w_nums, mse(x_nums, y_nums, reduction='none'))\n        \n        #BCE+MSE = reconstruction loss\n        reconstruction_loss = torch.cat([cat_loss, num_loss], dim=1) if reduction == 'none' else cat_loss.mean() + num_loss.mean()\n        \n        #mask loss = how well the model predicts which values are corrupted - can use BCE as this is 0\/1\n        mask_loss = self.mask_loss_weight * bce_logits(predicted_mask, mask, reduction=reduction)\n        \n        return reconstruction_loss + mask_loss if reduction == 'mean' else [reconstruction_loss, mask_loss]","76b38bf2":"class SwapNoiseMasker(object):\n    def __init__(self, probas, decay_rate):\n        self.probas = torch.from_numpy(np.array(probas))\n        self.decay_rate = decay_rate\n    def apply(self, X, epoch_number):\n        epoch_probas = self.probas * (self.decay_rate ** epoch_number)        \n        \n        #generates Y\/N for swap \/ dont swap\n        should_swap = torch.bernoulli(epoch_probas.to(X.device) * torch.ones((X.shape)).to(X.device))\n        \n        #switches data where swap = Y\n        corrupted_X = torch.where(should_swap == 1, X[torch.randperm(X.shape[0])], X)\n        \n        #mask is whether data has been changed or not\n        #nb for one hot categorical data, presumably quite often mask != shouldswap, as 1 is swapped for 1 or 0 swapped for 0\n        mask = (corrupted_X != X).float()\n        return corrupted_X, mask","ee9c774e":"#repeats of probabilities for one hot encoding which creates new columns for categoricals\nrepeats = [x for x in unique_counts] + [1 for x in range(len(fts_continuous))]\n\n#probabilities for original columns\nprobas = [DAE_CFG['prob_categorical'] for x in range(len(fts_categorical))] + [DAE_CFG['prob_continuous'] for x in range(len(fts_continuous))]\n\n#expand these to the one hot columns\nswap_probas = sum([[p] * r for p, r in zip(probas, repeats)], [])\n\nprint('length',len(swap_probas))\nprint('examples',swap_probas[0:10])","c797935f":"#  get data\nX, Y, n_cats, n_nums = get_data()\n\nseed_everything()\n\ntrain_dl = DataLoader(\n    dataset=SingleDataset(X),\n    batch_size=DAE_CFG['batch_size'],\n    shuffle=True,\n    pin_memory=True,\n    drop_last=True\n)","eff5c3f8":"# setup model\n\nmodel_params = dict(\n    hidden_size=DAE_CFG['hidden_size'],\n    num_subspaces=DAE_CFG['num_subspaces'],\n    embed_dim=DAE_CFG['embed_dim'],\n    num_heads=DAE_CFG['num_heads'],\n    dropout=DAE_CFG['dropout'],\n    feedforward_dim=DAE_CFG['feedforward_dim'],\n    emphasis=DAE_CFG['emphasis'],\n    mask_loss_weight=DAE_CFG['mask_loss_weight']\n)\n\ndae = TransformerAutoEncoder(\n    num_inputs=X.shape[1],\n    n_cats=n_cats,\n    n_nums=n_nums,\n    **model_params\n).cuda()\nmodel_checkpoint = 'model_checkpoint.pth'\n\nmodel_state = torch.load(DAE_CFG['load_path'])\ndae.load_state_dict(model_state['model'])\n\noptimizer = torch.optim.Adam(dae.parameters(), lr=DAE_CFG['init_lr'])\nscheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=DAE_CFG['lr_decay'])","67e00177":"class AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val, self.avg, self.sum, self.count = 0, 0, 0, 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","b40d8a33":"noise_maker = SwapNoiseMasker(swap_probas, DAE_CFG['noise_decay'])\n\nfor epoch in range(DAE_CFG['max_epochs']):    \n       \n    t0 = datetime.now()\n    dae.train()\n    meter = AverageMeter()\n    \n    for i, x in enumerate(train_dl):\n        x = x.cuda()\n        x_corrputed, mask = noise_maker.apply(x, epoch) #added epoch to allow noise level to decrease over time\n        optimizer.zero_grad()\n        \n        loss = dae.loss(x_corrputed, x, mask)\n        \n        loss.backward()\n        optimizer.step()\n\n        meter.update(loss.detach().cpu().numpy())\n\n    delta = (datetime.now() - t0).seconds\n    scheduler.step()\n    \n    print('epoch {:5d} - loss {:.6f} - {:4.0f} sec per epoch'.format(epoch, meter.avg, delta))  \n    \n    model_checkpoint = f'model_checkpoint_{epoch}.pth'\n\n    tracking_df.loc[epoch, 'loss'] = meter.avg\n    tracking_df.loc[epoch, 'time'] = delta\n    \n    if epoch%DAE_CFG['save_freq']==0:\n        \n        print('Saving to checkpoint')\n        #as i have flat noise level across all columns, i just print the noise average\n        print('average noise level', np.array(swap_probas).mean()*(DAE_CFG['noise_decay']**epoch))\n        model_checkpoint = f'{run_key}_model_checkpoint_{epoch}.pth'\n        torch.save({\n                \"optimizer\": optimizer.state_dict(),\n                \"scheduler\": scheduler.state_dict(),\n                \"model\": dae.state_dict()\n            }, model_checkpoint\n        )\n        \n\nmodel_checkpoint = f'{run_key}_model_checkpoint_final.pth'\ntorch.save({\n        \"optimizer\": optimizer.state_dict(),\n        \"scheduler\": scheduler.state_dict(),\n        \"model\": dae.state_dict()\n    }, model_checkpoint\n)\n\ntracking_df.to_csv(f'{run_key}_tracking_loss.csv')","eef11805":"## Configuration","fcaaebe1":"# Prepare Data","82fa30ec":"## Check Noise and Learning Rate","16ad2f73":"# Training DAE Model","f7fbdff1":"# Define Column Noise Probabilities","1643a53f":"- v1 - 10 epochs to check code runs\n- v2 - 600 epochs, stage 1\n- v3 - reload v2 weights, 600 epochs, stage 2\n- v4 - reload v3 weights, 600 epochs, stage 3","902201f8":"# Noise Masker","ae7fb3d5":"# Functions to Get Data","29c3bd18":"# Losses","e7c2a06a":"# Prepare DAE Model","cafc5555":"# Versions","f7e776a1":"# AutoEncoder","b3d8baf7":"If you find this notebook useful, please visit original discussion post (#1 Solution in Feb21 Comp, which is where I have taken this code from) and upvote.\n\nhttps:\/\/www.kaggle.com\/c\/tabular-playground-series-feb-2021\/discussion\/222745\n\nAnd winning solution from Jan\n\nhttps:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2021\/discussion\/216037","abc781cb":"I did not find major improvements over a starting flat 0.5 noise level across all columns\n\nHowever as per this solution here i think the noise for one hot encoded data is not ideal and overall approach to noise for categoricals can likely be improved\n\nhttps:\/\/www.kaggle.com\/c\/tabular-playground-series-mar-2021\/discussion\/229868"}}