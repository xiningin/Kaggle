{"cell_type":{"cd105769":"code","91aa460b":"code","2d9186e2":"code","9f9a36f6":"code","24f52b8b":"code","4615f1d9":"code","35a7f069":"code","5fc01377":"code","dc122a04":"code","184a7653":"code","cd1716e0":"code","d68c388c":"markdown","968fdcdc":"markdown","3b31a5cc":"markdown","4ef04d8f":"markdown","673bcf80":"markdown","a092cbc3":"markdown","35fe4382":"markdown","cddb4155":"markdown","17762b9f":"markdown","1d198bda":"markdown","60cca2cb":"markdown","fc5be8f7":"markdown","7de5c214":"markdown","899f28f2":"markdown","add473de":"markdown","e9bd1afc":"markdown","7cae93c6":"markdown"},"source":{"cd105769":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","91aa460b":"# I am using the same dataset as my other notebook Bigram_Keras_explain, I will also use the same seed for benchmarking\n\nimport keras \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras import layers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing import sequence\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Input, Dense, Dropout, Embedding, Flatten, LSTM, Bidirectional\nfrom keras.models import Model\nfrom keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\n\nprint(pd.__version__)\n\n","2d9186e2":"df = pd.read_csv(\"\/kaggle\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\", names=['comment', 'label'], header=0, encoding='utf-8')","9f9a36f6":"# df=df1.sample(n=50000, random_state=23)\n# Take a sample if necessary; for faster training\n\n#check data format\nprint(df.iloc[0])\n\n#check the labels in the dataset\ndf.label.value_counts()\n\n#converts label into integer values\ndf['label'] = df.label.astype('category').cat.codes","24f52b8b":"# counts the number of classes\n# label the target feature\n\nnum_class = len(np.unique(df.label.values))\ny = df['label'].values\nprint(\"\\nThere are a total of \" + str(num_class) + \" classes.\")\n\n# evenly distributed dataset\ndf.groupby('label').count()","4615f1d9":"max_features = 22000 # number of words to be used in the word corpse\nmaxlen = 1600 #caught off point of the lenght of a sentence\n\ntokenizer = Tokenizer()\n#fits the comments on the comments\ntokenizer.fit_on_texts(df.comment.values)\n#changes the tokens into sequence data structures\npost_seq= tokenizer.texts_to_sequences(df.comment.values)\npost_seq_padded= pad_sequences(post_seq, maxlen=maxlen)","35a7f069":"X_train, X_test, y_train, y_test = train_test_split(post_seq_padded, y, test_size=0.3, random_state=23)\n\ny_train = np.array(y_train)\ny_test = np.array(y_test)","5fc01377":"from keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.layers import LSTM\nfrom keras import optimizers\n\nmodel = Sequential()\nmodel.add(Embedding(max_features, 100, input_length=maxlen))\nmodel.add(Dropout(0.5))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(2, activation='softmax'))\n\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['acc'])\n\n\n# prints out summary of model\nmodel.summary()\n\n# saves the model weights\n# the train\/validation\/test split is 26500\/8750\/15000, it's essential to hold out a decent chunck of unseen data\nfilepath=\"weights-simple.hdf5\"\ncheckpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\nhistory = model.fit([X_train], batch_size=32, y=to_categorical(y_train), verbose=1, validation_split=0.25, \n          shuffle=True, epochs=3, callbacks=[checkpointer])","dc122a04":"#plot model\ndf_result = pd.DataFrame({'epochs':history.epoch, 'accuracy': history.history['acc'], 'validation_accuracy': history.history['val_acc']})\ng = sns.pointplot(x=\"epochs\", y=\"accuracy\", data=df_result, fit_reg=False)\ng = sns.pointplot(x=\"epochs\", y=\"validation_accuracy\", data=df_result, fit_reg=False, color='green')\n\n","184a7653":"#get prediction accuarcy for testing dataset 15000 samples\npredicted = model.predict(X_test)\npredicted_best = np.argmax(predicted, axis=1)\nprint (accuracy_score(predicted_best, y_test))\npredicted=pd.DataFrame(data=predicted)\n\n","cd1716e0":"print (classification_report(predicted_best, y_test))","d68c388c":"![image.png](attachment:image.png)\n\nhttps:\/\/towardsdatascience.com\/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66","968fdcdc":"### input gate\n\n- same as forget gate\n- one additional tanh function added to the cell state at time = t\n- what information will be output into long term memory is funtion of the previous output multiply by the forget gate value + the value of the current cell state x the sigmoid output\n- the sigmoid can still be 0 in the input gate, which means that tanh cell is being passed through to long term memory\n\n- my personal interpretation is the model is deciding the blend of the previous information + the current information\n","3b31a5cc":"### We have to keep the matrix in the same sequential order as it was represented when us LSTM; order and sequencing matters. We cannot use feature selection for example in TfidfVectorizer","4ef04d8f":"At even higher macro level\n\n![image.png](attachment:image.png)","673bcf80":"### output gate\n\nfinally, the output gate is deciding on how much informaiton to pass up ( long term memory) and how much to carry over to the short term memeory channel to the next H t\n\n\n![image.png](attachment:image.png)","a092cbc3":"now that the mechanism of LSTM is understood. More complexed architechture can be comprehended more easyily, such as Bi directional LSTM, where the network backpropagates not only on forward looking, but also can backpropagate given future outcome","35fe4382":"At a macro level \n\n![image.png](attachment:image.png)","cddb4155":"# Bi-Directional LSTM Keras","17762b9f":"## Conclusion \n\nFor this sentiment analysis task on IMDB dataset. The IDF unigram + bigram (https:\/\/www.kaggle.com\/eriche523\/bigram-keras-explained) still performs better and trains faster than Bidirectional LSTM. As expect Bidirectional LSTM specializes in NER task which is a much hard problem to tackle then sentiment analysis. ","1d198bda":"![image.png](attachment:image.png)","60cca2cb":"### forget gate\n\n- weight of the forget gate\n- sum multiple by the previous cell (H t-1) and the input X t, get a scalar\n- plus the bias\n- Then after all that, input the scalar into the sigmoid function\n\nif the output is 0 that means that no information is fed into the long term memory, which is represented by the first horizontal line going through the cell\n\nif the output is 1 that means that all of the information is fed into the long term memory, probably something that capture important context and it is worth keeping and passing it on to t+1 for example.\n\nthe output can be between 0 and 1\n\n\n\n\n![image.png](attachment:image.png)","fc5be8f7":"Micro Level of a LSTM Cell\n\n3 components of LSTM cell\n\n1. forget gate\n2. input gate\n3. output gate\n","7de5c214":"http:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/","899f28f2":"![image.png](attachment:image.png)","add473de":"LSTM ( long short term memory) is a RNN model.\n\nAt a macro level\n\nIt encapsulate the idea of 'how much weight' should be assigned to past (X t-n) information. In the same capacity, 'how much weight' should we forget on past information.\nLSTM represent a more natural way of thinking. As humans we don't read a sentence in a single setting and assign weights to each word. We read and comprehend from left to right (in most cultures).\n\nA word or phrase occuring at the beginning of a sentence may provide significant context to the meaning of a sentence. And LSTM captures that notion and allows it to carry through the entire sentence (memorize and assign significant weight to what was said at the begining of a sentence).\n\nHowever in other cases words or phrases that also occur at the begining of a sentence may not provide any significant context, and thus no significant information will be retain via the LSTM cell.","e9bd1afc":"### let A= [come, hell, or, high, water]\n\n### once tokenized let M= [1,2,3,4,5]\n\n### it will feed M matrix into the LSTM cells, once finished the first forward feed layer it will reverse the matrix\n\n### M_reversed= [5,4,3,2,1]\n\n### [water, high, or, hell, come]\n\n### Esssentially the bidirectional LSTM is updating it's graident based on the future outcome.\n\n### then before proceeding into the output layer, the model will aggregate the two matrix. by default it's concatentation in Keras Bidreictional Wrapper. But you can specify\n\n### - merge_mode: Mode by which outputs of the forward and backward RNNs will be combined. One of {'sum', 'mul', 'concat', 'ave', None}. If None, the outputs will not be combined, they will be returned as a list.\n\n### concatentation\n\n### M + M_reversed\n\n### 1 X 10\n\n### [1,2,3,4,5,5,4,3,2,1]","7cae93c6":"![image.png](attachment:image.png)"}}