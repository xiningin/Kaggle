{"cell_type":{"867393c1":"code","01a0c93f":"code","f3ffddbe":"code","be7f6235":"code","f2cb9c12":"code","75fc7143":"code","732e32df":"code","e2839bd2":"code","6a03890d":"code","a8467d17":"code","0ebb9a1f":"code","f743ef04":"code","b44d363f":"code","e5544fbc":"code","000fe51f":"code","14793f60":"code","c53c63ee":"code","6284e88f":"code","6fa52826":"code","ef3c6f02":"code","88aba944":"code","c5c484c9":"code","b12633a1":"code","a84be368":"code","2ef7dea9":"code","0e421b8a":"code","e7b2c68c":"code","932b6b69":"code","f4ea4854":"code","236dffd9":"code","d28610d2":"code","f72bb9b8":"code","57b2b967":"code","c7e4cc0f":"code","fc626ef0":"code","7df55d16":"code","3edda14d":"code","f8e84065":"code","b8fc1dfe":"code","07c1b258":"code","9f48de3f":"code","ee07a600":"code","46196eb0":"code","0ab705a1":"markdown","44836c7c":"markdown","b52e1f5b":"markdown","fa5244e3":"markdown","fea23c1c":"markdown","71e5d27f":"markdown","4454d201":"markdown","a0016650":"markdown","73e39d35":"markdown","3d954210":"markdown","29afbe40":"markdown","94e1ed3b":"markdown","f468fa2c":"markdown","a7859b2d":"markdown","727da863":"markdown","d4861206":"markdown","779fbc63":"markdown","8254cfe2":"markdown","67c1cebf":"markdown","9a317089":"markdown","ba6dc2de":"markdown","cef6e39c":"markdown","0c062223":"markdown","5e229e91":"markdown","ec67551a":"markdown","cd181068":"markdown"},"source":{"867393c1":"# Importing necessary libraries \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\ncolor = sns.color_palette()\nimport plotly.graph_objs as go\npy.init_notebook_mode(connected=True)\nimport plotly.tools as tls\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport glob\nimport re\nimport string","01a0c93f":"os.listdir(r'..\/input\/nlp-estimate\/nlp estimate\/NLP Estimate\/')","f3ffddbe":"filenames =glob.glob(\"..\/input\/nlp-estimate\/nlp estimate\/NLP Estimate\/arq*.txt\")\n\ndata = []\nfor filename in filenames:\n    df = pd.read_csv(filename,delimiter='\\n',names=['Text'])\n    df['Filename'] = os.path.basename(filename)\n    data.append(df)\n\ndf_Req = pd.concat(data, ignore_index=True)","be7f6235":"df_Req['File_ID']=df_Req['Filename'].map(lambda x : re.findall('\\d+',x))\ndf_Req['File_ID']=df_Req['File_ID'].map(lambda x :x[0])\ndf_Req['File_ID']=pd.to_numeric(df_Req['File_ID'])","f2cb9c12":"df_Req.info() # Lets check information about our current dataframe ","75fc7143":"df_Req.sort_values(by='File_ID',inplace=True)\ndf_Req.set_index('File_ID')","732e32df":"df_Req.shape","e2839bd2":"df_Est=pd.read_csv(\"..\/input\/nlp-estimate\/nlp estimate\/NLP Estimate\/estimate.csv\",names=['Estimate']) ","6a03890d":"df_Est.insert(0, 'Est_ID', range(0, len(df_Est)))","a8467d17":"df_Est.head(10)#Lets verify the data","0ebb9a1f":"df_Merge=pd.merge(df_Req, df_Est, how='inner', on=None, left_on='File_ID', right_on='Est_ID',\n         left_index=False, right_index=False, sort=False,\n         suffixes=('_x', '_y'), copy=True, indicator=False,\n         validate=None)","f743ef04":"df_Merge.drop(columns =['Filename','File_ID','Est_ID'],inplace=True)","b44d363f":"df_Merge.head(38)","e5544fbc":"df=df_Merge","000fe51f":"df.shape","14793f60":"df.dtypes","c53c63ee":"df.isnull().sum()","6284e88f":"def CleanText(Text):\n    Text = re.sub(r'html',' ',Text)\n    Text = re.sub(r'<div>',' ',Text)\n    Text = re.sub(r'<p>',' ',Text)\n    Text = re.sub(r'<pre>',' ',Text)\n    Text = re.sub(r'<code>',' ',Text)\n    Text = re.sub(r'html',' ',Text)\n    Text = re.sub(r'< div>',' ',Text)\n    Text = re.sub(r'< p>',' ',Text)\n    Text = re.sub(r'< pre>',' ',Text)\n    Text = re.sub(r'< code>',' ',Text)\n    Text = re.sub(r'< code>',' ',Text)\n    ## Use string method to do further cleanup from punctuation and digits which will may not give any additional insight\n    trans_punct = str.maketrans('', '', string.punctuation)\n    trans_digit = str.maketrans('', '', string.digits)\n    Text = Text.translate(trans_punct)\n    Text = Text.translate(trans_digit)\n    Text = Text.lower()\n    return Text\n    ","6fa52826":"df['Cleaned']= df['Text'].apply(CleanText)","ef3c6f02":"df.head(10) ### Verifiying the data ","88aba944":"#!pip install wordcloud (This is not required if wordcloud was installed)","c5c484c9":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\n\ndef show_wordcloud(data, title = None):\n    wordcloud = WordCloud(\n        background_color='white',\n        stopwords=stopwords,\n        max_words=100,\n        max_font_size=50, \n        scale=3,\n        random_state=1 \n).generate(str(data))\n\n    fig = plt.figure(1, figsize=(15, 15))\n    plt.axis('off')\n    if title: \n        fig.suptitle(title, fontsize=20)\n        fig.subplots_adjust(top=2.3)\n\n    plt.imshow(wordcloud)\n    plt.show()\n\nshow_wordcloud(df['Cleaned'])","b12633a1":"df['Estimate'].value_counts()","a84be368":"labellist=[]\ncnt=df['Estimate'].value_counts().head(10)\nlabellist=cnt.index[::-1]","2ef7dea9":"labellist","0e421b8a":"cnt = df['Estimate'].value_counts().head(5)\nprint(cnt)\n\ntrace = go.Bar(\n    y=cnt.index[::-1],\n    x=cnt.values[::-1],\n    orientation = 'h',\n    marker=dict(\n        color=cnt.values[::-1],\n        colorscale = 'Blues',\n        reversescale = False\n    ),\n)\n\nlayout = dict(\n    title='Estimate Distribution',\n    )\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"Estimates\")","e7b2c68c":"df = df.loc[df['Estimate'].isin([1,2,3,5,8])]","932b6b69":"df['Req_Length']=df['Text'].apply(len)","f4ea4854":"sns.set(font_scale=2.0)\n\ng = sns.FacetGrid(df,col='Estimate',size=5)\ng.map(plt.hist,'Req_Length')","236dffd9":"df1 = df.loc[df['Estimate'].isin([1])]\ndf8 = df.loc[df['Estimate'].isin([8])]\ndf2 = df.loc[df['Estimate'].isin([2])]\ndf5 = df.loc[df['Estimate'].isin([5])]\ndf3 = df.loc[df['Estimate'].isin([3])]","d28610d2":"show_wordcloud(df1['Cleaned'])","f72bb9b8":"show_wordcloud(df2['Cleaned'])","57b2b967":"show_wordcloud(df8['Cleaned'])","c7e4cc0f":"show_wordcloud(df5['Cleaned'])","fc626ef0":"sns.set(font_scale=1.2)\nplt.figure(figsize = (8,4))\nsns.heatmap(df.corr(),cmap='coolwarm',annot=True,linewidths=.5)","7df55d16":"df['StoryType'] = df['Estimate']<6","3edda14d":"from sklearn.model_selection import train_test_split\ntrain_text, test_text, train_y, test_y = train_test_split(df['Cleaned'],df['StoryType'],test_size = 0.2,shuffle =True)","f8e84065":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM,Bidirectional,Dropout\nfrom keras.utils.np_utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D","b8fc1dfe":"MAX_NB_WORDS = 20000\n\n# get the raw text data\ntexts_train = train_text.astype(str)\ntexts_test = test_text.astype(str)\n\n#  vectorize the text samples \ntokenizer = Tokenizer(nb_words=MAX_NB_WORDS, char_level=False)\ntokenizer.fit_on_texts(texts_train)\nsequences = tokenizer.texts_to_sequences(texts_train)\nsequences_test = tokenizer.texts_to_sequences(texts_test)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","07c1b258":"MAX_SEQUENCE_LENGTH = 200\n#pad sequences are used to bring all sentences to same size.\n# pad sequences with 0s\nx_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', x_train.shape)\nprint('Shape of data test tensor:', x_test.shape)","9f48de3f":"model = Sequential()\nmodel.add(Embedding(MAX_NB_WORDS, 128))\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2,input_shape=(1,))))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(1, activation='sigmoid'))","ee07a600":"model.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","46196eb0":"model.fit(x_train, train_y,\n          batch_size=128,\n          epochs=3,\n          validation_data=(x_test, test_y))","0ab705a1":"#### Here we are using  Keras tokenizer to tokenize requirement texts and create word embeddings . ","44836c7c":"#### Lets have a look at our final data before text analysis","b52e1f5b":"#### The file names are  a string , therefore the sorting order gets changed and arq1 , arq10 , arq100 etc comes before arq2 . This will become a problem while matching the requirmenent file with the estimate . Thefore I have extracted the file id from filename using regex and lamda function. Then added the id as a column in dataframe","fa5244e3":"## Try deep learning to see whether we can predict a large story from a small one . Here I have considered 1 to 5 story points are small story and 8 as large story . ","fea23c1c":"#### Let us see the distribution of requirements  for each type of estimates ","71e5d27f":"## We have seen that there is no significant relation between requirement length and the estimate , lets confirm this assumption using correlation plot","4454d201":"### This notebook is to do a data processing and consolidation of all the requirement text file and corresponding estimate . I have tried to do basic EDA on the texts and the corresponding estimate to find out if there is any relationship between the requirement texts and estimate .","a0016650":"#### There are no null values (There were not supposed to be as the data processing was to take the content from each file , and there were no 0 byte file ) so no null handling required ","73e39d35":"#### Based on File_ID and Est_ID both dataframes were joined to have a combined dataframe for further analysis .","3d954210":"### Create sequential model . Here I have used Bidirectional LSTM \n#### Dropout layer as 20% neuron to be dropped \n#### sigmoid activation function at the last dense layer \n#### Since there are two output class (large and small) , used the binary crossentropy as the loss function","29afbe40":"#### Loop through the filenames and the load the text content into dataframe . Concatenate all the dataframe at the end to get a single dataframe with all the requirement texts","94e1ed3b":"#### Lets see how the estimates are distributed in the dataset ","f468fa2c":"#### Lets see if there is a relationship with length of requirement text and the estimate ? (Does longer requirement text means more elaboration and lesser story point ? )","a7859b2d":"### By reviewing the worldcloud we can see mainly \n##### Estimate 1 - Talks about desktop , android, allow , titanium , new , mobile , sample , test, confirm\n##### Estimate 2 - Talks about files ,  Installer ,need , test , editor ,create, enable , studio and titanium \n##### Estimate 5- Talks about  Usergrid fix , tool , deploy, project , shard , test , android ,run , create\n##### Estimate 8- Talks about  Window , Project , Studio and Titanium \n\n## Need to investigate what is Titanium ? Probably an application which occurs in the text for most classes ? Do we need this ? ","727da863":"#### **By seeing the plots , we can conclude that there is no significant relationship between requirement text length and the corresponding estimate**","d4861206":"## Data Pre-Processing","779fbc63":"#### Dropping the columns which does not give any insight towards NLP or estimation","8254cfe2":"### Lets now see what kind of words appear most in writing requirements for software application development ","67c1cebf":"#### After analyzing the data we can see that there are special tag as  < div> ,< p> , < code>, < pre> etc present in the requirement text .We dont want those tags to appear in list of words and throw model off track ","9a317089":"## Check for more insights ","ba6dc2de":"#### Here we are sorting the values based on the derived file ids , to match with the estimation text . ","cef6e39c":"#### Lets now analyze and see if there is any difference in commonly used words for different estimations. Slicing dataframes by estimations and checking the worldcloud","0c062223":"#### Applying this function to all rows in dataframe and storing the cleaned data in another column","5e229e91":"### We can see that validation accuracy is decreasing layer by layer . Need to check if other architecture and data preprocessing can help in increasing the test accuracy . \n\n### I have already tried bagging and boosting method and the accuracy was only 35\/36 % . Will update  in next version. ","ec67551a":"#### We can see 82% of total records fall within estimate 1 to 8. Therefore we are filtering out other classes to avoid class imbalance problem \n\n#### \"The class imbalance problem typically occurs when, in a classification problem, there are many more instances of some classes than others. In such cases, standard classifiers tend to be overwhelmed by the large classes and ignore the small ones.\" \n","cd181068":"#### We are now going to load the estimate data and match with Requirement data to combine in a single dataframe . "}}