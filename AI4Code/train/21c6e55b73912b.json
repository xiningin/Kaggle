{"cell_type":{"4ab18da7":"code","e157ac6d":"code","efe51e97":"code","d489d1da":"code","6da5d827":"code","ff35f906":"code","cbb14c10":"code","3500ac42":"code","af675cac":"code","537c4356":"code","8a86415d":"code","e961f22d":"code","e25f2586":"code","2bb443b1":"code","cf231ed9":"code","a2d98621":"code","b0e3ce9a":"code","c35e51cd":"code","8c965d85":"code","1aad2ef4":"code","ac16e99e":"code","27c5fe50":"code","b577543f":"code","044f57a6":"code","9c2a553d":"code","bfab5e25":"code","dd7c8edd":"code","640f7210":"code","e415dc3b":"code","bc47cf8e":"code","aef0e2c6":"code","4fe18bd1":"code","0b2d1b0b":"code","dea0901d":"code","3827b36c":"code","ae975368":"code","48d22b06":"code","22f7ff3b":"code","3f3b674c":"code","1f30a9d7":"code","5b0dcbdb":"code","3419b826":"code","49dd92b8":"code","234e7dee":"code","edb507bf":"markdown","a587c929":"markdown","b6317e26":"markdown","d12752a1":"markdown","36551bad":"markdown","f58d1864":"markdown","5aa9dbb3":"markdown","27e31d54":"markdown","62763433":"markdown","b668153d":"markdown","beb71984":"markdown","78d16359":"markdown","80078a0e":"markdown","20343856":"markdown","87d19b1d":"markdown","f63f2892":"markdown","731225c6":"markdown","1b8f586b":"markdown","da05e5de":"markdown","cf0d34fc":"markdown","dd9446ce":"markdown","bc78fa28":"markdown","90b79930":"markdown"},"source":{"4ab18da7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))","e157ac6d":"train = pd.read_csv('..\/input\/train.csv', encoding = \"ISO-8859-1\")\ntrain.head(10)","efe51e97":"train.info()","d489d1da":"obj_cols = train.select_dtypes('object')\nobj_cols.head()","6da5d827":"obj_cols.drop('Born',axis=1,inplace=True)\ntrain['Born'] = pd.to_datetime(train.Born)","ff35f906":"train.Born.head()","cbb14c10":"for c in obj_cols.columns:\n    print('Obj Col: ', c, '   Number of Unqiue Values ->', len(obj_cols[c].value_counts()))","3500ac42":"373+37+18+16+2+573+308+18+68","af675cac":"fig, ax=plt.subplots(1,2,figsize=(18,10))\nobj_cols['Cntry'].value_counts().sort_values().plot(kind='barh',ax=ax[0]) \nax[0].set_title(\"Counts of Hockey Players by Country\");\nobj_cols['Cntry'].value_counts().plot(kind='pie', autopct='%.2f', shadow=True,ax=ax[1]);\nax[1].set_title(\"Distribution of Hockey Players by Country\");\n","537c4356":"fig, ax=plt.subplots(1,1,figsize=(12,8))\nobj_cols['Team'].value_counts().plot(kind='bar',ax=ax);\nplt.title('Counts of Team Values');","8a86415d":"train.Salary.head(10)","e961f22d":"fig, ax=plt.subplots(1,1,figsize=(12,8))\ntrain.Salary.plot(kind='hist',ax=ax, bins=20);\nplt.title(\"Distribution of Salaries\");\nplt.xlabel('Dollars');","e25f2586":"sal_gtmil = train[train.Salary >= 1e7]","2bb443b1":"sal_gtmil.head(10)","cf231ed9":"import lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","a2d98621":"def data_clean(x):\n    ## Were going to change Born to date time\n    x['Born'] = pd.to_datetime(x.Born, yearfirst=True)\n    x['dowBorn'] = x.Born.dt.dayofweek\n    x[\"doyBorn\"] = x.Born.dt.dayofyear\n    x[\"monBorn\"] = x.Born.dt.month\n    x['yrBorn'] = x.Born.dt.year\n    ## Drop Pr\/St due to NaNs from other countries and First Name\n    x.drop(['Pr\/St','First Name'], axis=1, inplace=True)\n    ocols = ['City', 'Cntry', 'Nat', 'Last Name', 'Position', 'Team']\n    for oc in ocols:\n        temp = pd.get_dummies(x[oc])\n        x = x.join(temp, rsuffix=str('_'+oc))\n    x['Hand'] = pd.factorize(x.Hand)[0]\n    x.drop(ocols, axis=1, inplace=True)\n    x.drop(['Born'],axis=1,inplace=True)\n    return x\n    \n    ","b0e3ce9a":"try:\n    del train, x0, xc, test\nexcept:\n    pass","c35e51cd":"train = pd.read_csv('..\/input\/train.csv', encoding=\"ISO-8859-1\")\ntrain.head()","8c965d85":"test = pd.read_csv('..\/input\/test.csv', encoding=\"ISO-8859-1\")\ntest.head()","1aad2ef4":"full = train.merge(test, how='outer')\nprint(train.shape, test.shape, full.shape)","ac16e99e":"y = np.log(full.Salary.dropna())\nfull0 = full.drop(['Salary'],axis=1)","27c5fe50":"fig, ax=plt.subplots(1,1,figsize=(10,6))\ny.plot(ax=ax);\nplt.title(\"Ln Salary\");","b577543f":"obj_cols.columns","044f57a6":"full_c = data_clean(full0)","9c2a553d":"print(full0.shape, full_c.shape)","bfab5e25":"full_c.head()","dd7c8edd":"ss = StandardScaler()","640f7210":"full_cs = ss.fit_transform(full_c)","e415dc3b":"train_c = full_cs[:612]\ntest_c = full_cs[612:]","bc47cf8e":"print(train_c.shape, y.shape, test_c.shape)","aef0e2c6":"type(y)","4fe18bd1":"folds = 3\nlgbm_params = {\n    \"max_depth\": -1,\n    \"num_leaves\": 1000,\n    \"learning_rate\": 0.01,\n    \"n_estimators\": 1000,\n    \"objective\":'regression',\n    'min_data_in_leaf':64,\n    'feature_fraction': 0.8,\n    'colsample_bytree':0.8,\n    \"metric\":['mae','mse'],\n    \"boosting_type\": \"gbdt\",\n    \"n_jobs\": -1,\n    \"reg_lambda\": 0.9,\n    \"random_state\": 123\n}\npreds = 0\nfor f in range(folds):\n    xt, xv, yt, yv = train_test_split(train_c, y.values, test_size=0.2, random_state=((f+1)*123))\n    \n    xtd = lgb.Dataset(xt, label=yt)\n    xvd = lgb.Dataset(xv, label=yv)\n    mod = lgb.train(params=lgbm_params, train_set=xtd, \n                    num_boost_round=1000, valid_sets=xvd, valid_names=['valset'],\n                    early_stopping_rounds=20, verbose_eval=20)\n    \n    preds += mod.predict(test_c)\n    \npreds = preds\/folds\n    \n    ","0b2d1b0b":"acts = pd.read_csv('..\/input\/test_salaries.csv', encoding=\"ISO-8859-1\")\nacts['preds'] = np.exp(preds)\nacts.head()","dea0901d":"import matplotlib\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error","3827b36c":"fig, ax=plt.subplots(1,1,figsize=(12,8))\nacts.plot(ax=ax, style=['b-','r-']);\nplt.title(\"Comparison of Preds and Actuals\");\nplt.ylabel('$');\nax.get_yaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\nplt.tight_layout()","ae975368":"mse = mean_squared_error(np.log(acts.Salary), np.log(acts.preds))\nmae = mean_absolute_error(np.log(acts.Salary), np.log(acts.preds))","48d22b06":"print(\"Ln Level Mean Squared Error :\", mse)\nprint(\"Ln Level Mean Absolute Error :\", mae)","22f7ff3b":"fi_df = pd.DataFrame( 100*mod.feature_importance()\/mod.feature_importance().max(), \n                      index=full_c.columns, #mod.feature_name(),\n                      columns =['importance'])","3f3b674c":"fig, ax=plt.subplots(1,1,figsize=(12,8))\nfi_df.sort_values(by='importance',ascending=True).iloc[-20:].plot(kind='barh', color='C0', ax=ax)\nplt.title(\"Normalized Feature Importances\");","1f30a9d7":"import statsmodels.api as sma","5b0dcbdb":"top10 = fi_df.sort_values(by='importance',ascending=True).iloc[-10:].index\ntop10\n\nexog = pd.DataFrame(test_c, columns=full_c.columns)[list(top10)].fillna(0)","3419b826":"ols = sma.OLS(exog=exog, endog=acts.Salary)\nols_fit = ols.fit()\nprint(ols_fit.summary())","49dd92b8":"ols_preds = ols_fit.predict()","234e7dee":"fig, ax=plt.subplots(1,1,figsize=(12,8))\nacts.Salary.plot(ax=ax, color='C1');\nax.plot(ols_preds, color='C0');\nplt.title(\"Comparison of StatsModels Preds and Actuals\");\nplt.ylabel('$');\nplt.legend(['salary actual','ols preds']);\nax.get_yaxis().set_major_formatter(\n    matplotlib.ticker.FuncFormatter(lambda x, p: format(int(x), ',')))\nplt.tight_layout()","edb507bf":"There are a lot of salalries less than a million.\n\nThis will very likely skew our data when we try to model it.\n\nLets take a look at salaries above a million.","a587c929":"## Salary\n\nLets take a gander at whats going on with our target.","b6317e26":"## Actual Test Data","d12752a1":"# NHL Analytics Analysis\n\nThis is a brief analysis of NHL player salaries and some regression modeling using Light GBM as well as statistical analysis using statsmodels.\n\nSince the data set is not a time based series we dont have to worry too much about serial correlation or heteroskedasticity.\n\nThe features of the data set are explained on the Kaggle page.\n","36551bad":"## Model\n\nMy thoughts with this are to dummy out most of the objects variables and using light gbm.","f58d1864":"Top Features are:\n- Draft Year\n- Where the player was drafted over-all.\n- Year Born\n- Time On Ice \/ Games Played\n- Time On Ice (TOI\/GP.l)\n- Draft Round\n- PLayers Avg Gane Score\n- Percentage of all opposing shot attempts blocked by this player\n- Expected goals (weighted shots) for this individual, which is shot attempts weighted by shot location\n- Day Of Year Born\n\n\nTime On Ice per GP doesnt seem so unusual as you would pay the ones who were better more and if theyre better, then they get paid more.\n\nThe Percentage of opposing shot attempts blocked is kinda of wild! Selke Trophy much?\n","5aa9dbb3":"This cell below splits on the shape indices we have a few cells up.","27e31d54":"### LGB Model","62763433":"In this instance, we were able to predict negative values which doesnt make sense and would have to be cut off at 0.","b668153d":"If we dummied this out, we would get approximately an addition 1413 columns, minus the original 9.\n\nWe probably wouldnt need the first names though. It would be more interesting to keep the last names given some of hockey's family tradition.\n\nThere is something strange about the team category though since there is apparently 68 unique values...and currently only 31 hockey teams.","beb71984":"So there is 73 float features, 71 int, and 10 objects.","78d16359":"Based on the items above, it appears we missed a lot of the outliers which is something we would need to grab with perhaps a quantile regression.","80078a0e":"According to the OLS, Draft Round and Block Shot % and Overall draft Position are not that statistically significant for this model.\n\nThe model has a good R squared.\n","20343856":"Now its into date time format and out of the object cols.","87d19b1d":"## Clean Up","f63f2892":"It looks like these are mostly straight forward and we will dig into them in a second but lets take care of the **Born** column.","731225c6":"## A Little Exploration","1b8f586b":"Lets take a look at the distribution of hockey players Countries relative to this data set.","da05e5de":"So now we see what going on with the team values, there are actually some players who split teams so these are accounted for in this data set as well.","cf0d34fc":"Right here we see some big names in this set. This set also only appears to be 7 items long.","dd9446ce":"For regressions I like to take the top couple effects from the Decision Tree and model it using statsmodels to get a better idea of the stastitical soundness of the model.","bc78fa28":"We had done some cleaning above but were going to create a function that handles it to work out the test data as well.\n\nAdditionally, we found out that the test and train splits arent evenly split so we need to go back and merge the data first.","90b79930":"Canada, USA, Sweden, Russia, Czekoslovakia, Finlad, etc."}}