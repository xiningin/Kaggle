{"cell_type":{"0f39d8b8":"code","4f61c8ce":"code","3c3431cf":"code","52aafee6":"code","f7ba4dee":"code","62961250":"code","444291da":"code","b7cb35cc":"code","0410365c":"code","9af23d5c":"code","59f1a63d":"code","53557c81":"code","0a6f84fc":"code","e8be16c2":"code","b2d5ed30":"code","5a7307b5":"code","269da7b3":"code","605b4fdc":"code","67b60dac":"code","31600899":"code","927c1f61":"code","f2ab8ee4":"markdown"},"source":{"0f39d8b8":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport warnings\nfrom collections import Counter\nfrom sklearn.feature_selection import mutual_info_classif\nwarnings.filterwarnings('ignore')","4f61c8ce":"train = pd.read_csv('..\/input\/porto-seguro-safe-driver-prediction\/train.csv')\ntrain.head()","3c3431cf":"rows = train.shape[0]\ncolumns = train.shape[1]\nprint('The train dataset contains {} rows and {} columns'.format(rows, columns))","52aafee6":"train.isnull().any().any()","f7ba4dee":"train_copy = train\ntrain_copy = train_copy.replace(-1, np.NaN)","62961250":"import missingno as msno\nmsno.matrix(df=train_copy.iloc[:,2:39], figsize=(20,14), color=(0.42,0.1,0.05))","444291da":"data = [go.Bar(x=train['target'].value_counts().index.values,\n              y=train['target'].value_counts().values,\n              text=\"Distribution of target variable\")]\nlayout = go.Layout(title='Target variable distribution')\n\nfig = go.Figure(data=data, layout = layout)\n\npy.iplot(fig, filename='baisc-bar')","b7cb35cc":"Counter(train.dtypes.values)","0410365c":"train_float = train.select_dtypes(include=['float64'])\ntrain_int = train.select_dtypes(include=['int64'])","9af23d5c":"colormap = plt.cm.magma\nplt.figure(figsize=(16,12))\nplt.title('Pearson correlation of continuous features', y=1.05, size = 15)\nsns.heatmap(train_float.corr(), linewidths=0.1, vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","59f1a63d":"data = [\n    go.Heatmap(\n        z= train_int.corr().values,\n        x=train_int.columns.values,\n        y=train_int.columns.values,\n        colorscale='Viridis',\n        reversescale = False,\n        opacity = 1.0)\n]\n\nlayout = go.Layout(title = 'Pearson Correlation of Integer-type features',\n                  xaxis=dict(ticks='', nticks=36),\n                  yaxis=dict(ticks=''),\n                  width=900, height=700)\n\nfig = go.Figure(data = data, layout = layout)\npy.iplot(fig, filename = 'labelled-heatmap')","53557c81":"mf = mutual_info_classif(train_float.values, train.target.values, n_neighbors=3, random_state=17)\nprint(mf)","0a6f84fc":"bin_col = [col for col in train.columns if '_bin' in col]\nzero_list = []\none_list = []\nfor col in bin_col:\n    zero_list.append((train[col]==0).sum())\n    one_list.append((train[col]==1).sum())","e8be16c2":"trace1 = go.Bar(\nx = bin_col, y=zero_list, name = 'Zero count')\ntrace2 = go.Bar(\nx = bin_col, y = one_list, name='One count')\n\ndata = [trace1, trace2]\nlayout = go.Layout(\nbarmode = 'stack', title='Count of 1 and 0 in binary variables')\n\nfig = go.Figure(data=data, layout = layout)\npy.iplot(fig, filename='stacked-bar')","b2d5ed30":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=150, max_depth=8, min_samples_leaf = 4,\n                           max_features = 0.2, n_jobs=-1, random_state=0)\nrf.fit(train.drop(['id','target'],axis=1), train.target)\nfeatures = train.drop(['id', 'target'], axis=1).columns.values\nprint('----- Training Done -----')","5a7307b5":"trace = go.Scatter(y = rf.feature_importances_, x=features,\n                  mode = 'markers', marker=dict(\n                  sizemode = 'diameter', sizeref=1, size = 13,\n                  color = rf.feature_importances_,\n                  colorscale = 'Portland', showscale=True),\n                  text = features)\ndata= [trace]\n\nlayout = go.Layout(autosize=True, title='Random Forest Feature Importance',\n                  hovermode = 'closest', xaxis=dict(ticklen=5, showgrid=False,\n                                                   zeroline=False, showline=False),\n                  yaxis=dict(title='Feature Importance', showgrid=False,\n                            zeroline=False, ticklen=5, gridwidth=2), showlegend=False)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='scatter2010')","269da7b3":"x, y = (list(x) for x in zip(*sorted(zip(rf.feature_importances_, features), reverse=False)))\ntrace2 = go.Bar(x=x, y=y, marker=dict(color=x, colorscale='Viridis', reversescale=True),\n               name = 'Random Forest Feature importance', orientation='h')\n\nlayout = dict(title='Barplot of Feature importances', width=900, height=2000,\n             yaxis=dict(showgrid=False, showline=False, showticklabels=True))\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')","605b4fdc":"from sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\n\ndecision_tree = tree.DecisionTreeClassifier(max_depth=3)\ndecision_tree.fit(train.drop(['id', 'target'], axis=1), train.target)\nwith open('tree1.dot', 'w') as f:\n     f = tree.export_graphviz(decision_tree, out_file=f, max_depth=4,\n                             impurity=False, feature_names=train.drop(['id','target'], axis=1).columns.values, \n                              class_names = ['No', 'Yes'], rounded = True, filled=True)\n        \ncheck_call(['dot','-Tpng', 'tree1.dot', '-o', 'tree1.png'])\n        \nimg = Image.open('tree1.png')\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage('sample-out.png')","67b60dac":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(n_estimators=100, max_depth=3, min_samples_leaf=4, max_features=0.2, random_state=0)\ngb.fit(train.drop(['id','target'], axis=1), train.target)\nfeatures = train.drop(['id', 'target'], axis=1).columns.values\nprint('----- Training Done -----')","31600899":"trace = go.Scatter(\n    y = gb.feature_importances_,\n    x = features,\n    mode='markers',\n    marker=dict(\n        sizemode = 'diameter',\n        sizeref = 1,\n        size = 13,\n        color = gb.feature_importances_,\n        colorscale='Portland',\n        showscale=True\n    ),\n    text = features\n)\ndata = [trace]\n\nlayout= go.Layout(\n    autosize= True,\n    title= 'Gradient Boosting Machine Feature Importance',\n    hovermode= 'closest',\n     xaxis= dict(\n         ticklen= 5,\n         showgrid=False,\n        zeroline=False,\n        showline=False\n     ),\n    yaxis=dict(\n        title= 'Feature Importance',\n        showgrid=False,\n        zeroline=False,\n        ticklen= 5,\n        gridwidth= 2\n    ),\n    showlegend= False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='scatter2010')","927c1f61":"x, y= (list(x) for x in zip(*sorted(zip(gb.feature_importances_, features),\n                            reverse=False)))\ntrace2 = go.Bar(x=x, y=y, marker=dict(color=x, colorscale='Viridis', reversescale=True),\n               name = 'Gradient Boosting Classifier Feature importance', orientation='h')\n\nlayout = dict(title = 'Barplot of Feature Importances', width=900, height=2000,\n             yaxis = dict(showgrid=False, showline=False, showticklabels=True))\nfig1 = go.Figure(data=[trace2])\nfig1['layout'].update(layout)\npy.iplot(fig1, filename='plots')","f2ab8ee4":"Q. What is mutual information?\n\nA. It is another useful tool as it allows one to inspect the mutual information between the target variable and the corresponding featurese it its calculated against.\n\nThe sklearn implementation of the mutual_info_classif function tells us that is 'relies on nonparametric method based on entropy estimation from k-nearest neighbors distances'."}}