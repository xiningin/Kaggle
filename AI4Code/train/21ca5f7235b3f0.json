{"cell_type":{"2e4ff4a3":"code","abdb6a84":"code","bd076741":"code","66950556":"code","8727a57a":"code","41401606":"code","4b5ccf41":"code","27b610d9":"code","ae16f380":"markdown","9adef07e":"markdown","79610fa3":"markdown"},"source":{"2e4ff4a3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d, Axes3D\nimport matplotlib.animation as animation\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","abdb6a84":"df = pd.read_csv('\/kaggle\/input\/goodreadsbooks\/books.csv',error_bad_lines=False)\ndf = df.reset_index(drop=True)\ndf.authors = [i.split('\/')[0] for i in df.authors]\ndf.head()","bd076741":"from wordcloud import WordCloud\n\nd = {}\nfor x, a in zip(df.authors.value_counts(), df.authors.value_counts().index):\n    d[a] = x\n\nwordcloud = WordCloud()\nwordcloud.generate_from_frequencies(frequencies=d)\nplt.figure(num=None, figsize=(12, 10), dpi=80, facecolor='w', edgecolor='k')\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.title(\"Word Cloud\", fontsize=20)\nplt.savefig(\"cloud.png\", dpi=200)\nplt.show()","66950556":"df = df.drop_duplicates('authors')\ndf = df.reset_index(drop=True)\ndf = df[:30]\nsize = len(df.authors)\nencoder, scaler = LabelEncoder(), MinMaxScaler()\naut = encoder.fit_transform(df.authors) \nrat = scaler.fit_transform(df[['average_rating']])","8727a57a":"class Latent_Embed(nn.Module):\n    def __init__(self, vocab_size, embedding_dim):\n        super(Latent_Embed, self).__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.linear1 = nn.Linear(3, 2)\n        self.linear2 = nn.Linear(2, 1)\n\n    def forward(self, inputs):\n        embeds = self.embeddings(inputs)\n        out = F.relu(self.linear1(embeds))\n        out = F.relu(self.linear2(out))\n        return out\n\n\naut_t = torch.tensor(aut)\nrat_t = torch.tensor(rat)\nloss_function = nn.MSELoss()\nmodel = Latent_Embed(size, 3)\noptimizer = optim.SGD(model.parameters(), lr=0.0001)\n\nfor epoch in range(10):\n    total_loss = 0\n    for context, target in zip(aut_t, rat_t):\n        model.zero_grad()\n        log_probs = model(context)\n        loss = loss_function(log_probs.double(), target.view(1).double())\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    print(\"Loss: \", total_loss\/len(aut_t))","41401606":"embedding_weights = pd.DataFrame(model.embeddings.weight.detach().numpy())\nembedding_weights.columns = ['X1','X2','X3']\nembedding_weights","4b5ccf41":"fig = plt.figure(num=None, figsize=(14, 12), dpi=80, facecolor='w', edgecolor='k')\nax = plt.axes(projection='3d')\nfor index, (x, y, z) in enumerate(zip(embedding_weights['X1'], \n                                      embedding_weights['X2'], \n                                      embedding_weights['X3'])):\n    ax.scatter(x, y, z, color='b', s=12)\n    ax.text(x, y, z, str(df.authors[index]), size=12, zorder=2.5, color='k')\n\nax.set_title(\"Word Embedding\", fontsize=20)\nax.set_xlabel(\"X1\", fontsize=20)\nax.set_ylabel(\"X2\", fontsize=20)\nax.set_zlabel(\"X3\", fontsize=20)\nplt.show()","27b610d9":"def rotate(angle):\n    ax.view_init(azim=angle)\n\nprint(\"Making animation\")\nres_animation = animation.FuncAnimation(fig, rotate, frames=np.arange(0, 365, 2), interval=100)\nres_animation.save('embedding.gif', dpi=100, writer='imagemagick')","ae16f380":"# Machine Learning Visualization 4","9adef07e":"## Wordclouds","79610fa3":"## Embeddings"}}