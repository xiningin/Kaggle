{"cell_type":{"8f1d03af":"code","f5b120ed":"code","1c8cb21c":"code","20b4ae80":"code","ba32d6fc":"code","e03a7faf":"code","0c462217":"code","413be09b":"code","97f2709f":"code","f75dbbf9":"code","b24193ea":"code","11e013fe":"code","a3536277":"code","1c153a58":"code","fc123218":"code","b6d82b08":"code","ac3bc3e2":"code","53ab340d":"code","fb8c34f1":"code","e243404a":"code","2b582b37":"code","dd45702f":"code","267f9823":"code","0ea89bc4":"code","d78771fc":"code","6b380853":"markdown","93294365":"markdown","18f976a7":"markdown","535745db":"markdown","cdb340c9":"markdown","1a305187":"markdown","bbfd0c86":"markdown","02b297bf":"markdown"},"source":{"8f1d03af":"VERSION = \"nightly\"  #@param [\"1.5\" , \"20200325\", \"nightly\"]\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version $VERSION","f5b120ed":"!pip install transformers==3.0.0","1c8cb21c":"from sklearn.metrics import  accuracy_score\nfrom sklearn.model_selection import train_test_split\n\ntrain = pd.read_csv('data\/train.csv')\ntrain = train.fillna('')\ntrain['text'] = train['keyword'] + ' ' + train['location'] + ' '+ train['text']\ntrain = train[['text','target']]\ndf_train, df_valid = train_test_split(train, test_size=0.2, random_state=42)\ndf_train.to_csv('train.csv')\ndf_valid.to_csv('valid.csv')","20b4ae80":"from transformers import BertConfig, RobertaConfig, DistilBertConfig, BertModel, RobertaModel, DistilBertModel\nfrom torch import nn\nimport torch\nclass BertStyleModel(torch.nn.Module):\n    \n    def __init__(self, model_type):\n        super().__init__()\n        \n        self.model_type = model_type\n        if (model_type == 'roberta'):\n            config_path = 'roberta-base'\n            model_path = 'roberta-base'\n            config = RobertaConfig.from_pretrained(config_path)\n            config.output_hidden_states = True\n            self.bert = RobertaModel.from_pretrained(model_path, config=config)\n        elif (model_type == 'distilbert'):\n            config_path = 'distilbert-base-uncased'\n            config = DistilBertConfig.from_json_file(config_path)\n            model_path = 'distilbert-base-uncased'\n            config.output_hidden_states = True\n            self.bert = DistilBertModel.from_pretrained(model_path, config=config)\n        elif (model_type == 'bert'):\n            config_path = 'bert-base-uncased'\n            config = BertConfig.from_pretrained(config_path)\n            config.output_hidden_states = True\n            model_path = 'bert-base-uncased'\n            self.bert = BertModel.from_pretrained(model_path, config=config)\n            \n        self.dropout = nn.Dropout(p=0.2)\n        self.high_dropout = nn.Dropout(p=0.5)   \n        self.cls_token_head = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(768 * 4, 768),\n            nn.ReLU(inplace=True),\n        )\n        self.classifier = nn.Linear(768, 1)\n        \n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n        \n        if (self.model_type == 'roberta'):\n            outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            hidden_layers = outputs[2]\n        elif (self.model_type == 'distilbert'):\n            outputs = self.bert(input_ids, attention_mask=attention_mask)\n            hidden_layers = outputs[1]\n        elif (self.model_type == 'bert'):     \n            outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            hidden_layers = outputs[2]\n        \n        hidden_states_cls_embeddings = [x[:, 0] for x in hidden_layers[-4:]]\n        x = torch.cat(hidden_states_cls_embeddings, dim=-1)\n        cls_output = self.cls_token_head(x)\n        logits = torch.mean(torch.stack([\n            #Multi Sample Dropout takes place here\n            self.classifier(self.high_dropout(cls_output))\n            for _ in range(5)\n        ], dim=0), dim=0)\n        outputs = logits\n        return outputs","ba32d6fc":"from torch import nn\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nfrom transformers import T5Tokenizer, T5Model\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef loss_fn(ypred, label):\n    return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\ndef seed_all(seed):\n    random.seed(seed)\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)","e03a7faf":"\nimport os\nimport torch\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom collections import OrderedDict, namedtuple\nimport torch.nn as nn\nfrom torch.optim import lr_scheduler\nimport joblib\n\nimport logging\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\nimport sys\nfrom sklearn import metrics, model_selection\n\nimport warnings\nimport torch_xla\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.data_parallel as dp\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.utils.utils as xu\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.test.test_utils as test_utils\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nclass BERTDatasetTraining:\n    def __init__(self, comment_text, targets, tokenizer, max_length):\n        self.comment_text = comment_text\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.comment_text)\n\n    def __getitem__(self, item):\n        comment_text = str(self.comment_text[item])\n        comment_text = \" \".join(comment_text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            comment_text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        padding_length = self.max_length - len(ids)\n        \n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[item], dtype=torch.float)\n        }","0c462217":"import torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\nfrom sklearn import metrics\nimport torch\nfrom  transformers import AdamW\nfrom transformers import  get_linear_schedule_with_warmup\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport transformers\nmx = BertStyleModel('bert')\ndef _run():\n    def loss_fn(outputs, targets):\n        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n\n    def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n        model.train()\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            token_type_ids = d[\"token_type_ids\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            optimizer.zero_grad()\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n            loss = loss_fn(outputs, targets)\n            if bi % 10 == 0:\n                xm.master_print(f'bi={bi}, loss={loss}')\n\n            loss.backward()\n            xm.optimizer_step(optimizer)\n            if scheduler is not None:\n                scheduler.step()\n\n    def eval_loop_fn(data_loader, model, device):\n        model.eval()\n        fin_targets = []\n        fin_outputs = []\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            token_type_ids = d[\"token_type_ids\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n            targets_np = targets.cpu().detach().numpy().tolist()\n            outputs_np = outputs.cpu().detach().numpy().tolist()\n            fin_targets.extend(targets_np)\n            fin_outputs.extend(outputs_np)    \n\n        return fin_outputs, fin_targets\n\n    df_train = pd.read_csv('train.csv')\n    df_valid = pd.read_csv('valid.csv')\n    MAX_LEN = 192\n    TRAIN_BATCH_SIZE = 4\n    EPOCHS = 3\n\n    tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n\n    train_targets = df_train.target.values\n    valid_targets = df_valid.target.values\n\n    train_dataset = BERTDatasetTraining(\n        comment_text=df_train.text.values,\n        targets=train_targets,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=True)\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=1\n    )\n\n    valid_dataset = BERTDatasetTraining(\n        comment_text=df_valid.text.values,\n        targets=valid_targets,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=16,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n    \n    device = xm.xla_device()\n    model = mx.to(device)\n    \n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    lr = 0.4 * 1e-5 * xm.xrt_world_size()\n    num_train_steps = int(len(train_dataset) \/ TRAIN_BATCH_SIZE \/ xm.xrt_world_size() * EPOCHS)\n    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n\n    for epoch in range(EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n        xm.save(model.state_dict(), \"bert.bin\")\n        np.save('bert', np.array(o))\n        auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n        xm.master_print(f'AUC = {auc}')","413be09b":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = _run()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","97f2709f":"class BERTDatasetTest:\n    def __init__(self, comment_text, tokenizer, max_length):\n        self.comment_text = comment_text\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.comment_text)\n\n    def __getitem__(self, item):\n        comment_text = str(self.comment_text[item])\n        comment_text = \" \".join(comment_text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            comment_text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        padding_length = self.max_length - len(ids)\n        \n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n           \n        }\ndef test_loop_fn(data_loader, model, device):\n        model.eval()\n        fin_targets = []\n        fin_outputs = []\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            token_type_ids = d[\"token_type_ids\"]\n            \n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n                token_type_ids=token_type_ids\n            )\n\n          \n            outputs_np = outputs.cpu().detach().numpy().tolist()\n            fin_outputs.extend(outputs_np)    \n\n        return fin_outputs\ndef test_model():\n  MAX_LEN = 192\n  device = xm.xla_device()\n  model = BertStyleModel('bert')\n  model.load_state_dict(torch.load(\"bert.bin\"))\n  model.to(device)\n  df_test = pd.read_csv('data\/test.csv')\n  df_test = df_test.fillna('')\n  df_test['text'] = df_test['keyword'] + ' ' + df_test['location'] + ' '+ df_test['text']\n  df_test = df_test[['text']]\n  tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n\n  test_dataset = BERTDatasetTest(\n        comment_text=df_test.text.values,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n  test_sampler = torch.utils.data.distributed.DistributedSampler(\n          test_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n  test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=16,\n        sampler=test_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n  para_loader = pl.ParallelLoader(test_loader, [device])\n  ypred = test_loop_fn(para_loader.per_device_loader(device), model, device)\n  ypred= np.array(ypred)\n  np.save('result.npy', ypred)\n  return ypred\n\n","f75dbbf9":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = test_model()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')","b24193ea":"submit = pd.read_csv('data\/sample_submission.csv')\nypred = np.load('result.npy')\nsubmit['target'] = ypred\nsubmit['target'].hist()\nplt.show()","11e013fe":"pred = [int(i> 0.5) for i in ypred] \nplt.hist(pred)\nplt.show()\nsubmit['target'] = pred\nsubmit.to_csv('submit.csv', index=False)\n### get 81.918 acccuracy","a3536277":"class BERTDatasetTraining:\n    def __init__(self, comment_text, targets, tokenizer, max_length):\n        self.comment_text = comment_text\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n        self.targets = targets\n\n    def __len__(self):\n        return len(self.comment_text)\n\n    def __getitem__(self, item):\n        comment_text = str(self.comment_text[item])\n        comment_text = \" \".join(comment_text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            comment_text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        padding_length = self.max_length - len(ids)\n        \n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'targets': torch.tensor(self.targets[item], dtype=torch.float)\n        }","1c153a58":"import torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.distributed.parallel_loader as pl\nfrom sklearn import metrics\nimport torch\nfrom  transformers import AdamW\nfrom transformers import  get_linear_schedule_with_warmup\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport transformers\n\nmx = BertStyleModel('roberta')\ndef _run():\n    def loss_fn(outputs, targets):\n        return nn.BCEWithLogitsLoss()(outputs, targets.view(-1, 1))\n\n    def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n        model.train()\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            optimizer.zero_grad()\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n            )\n\n            loss = loss_fn(outputs, targets)\n            if bi % 10 == 0:\n                xm.master_print(f'bi={bi}, loss={loss}')\n\n            loss.backward()\n            xm.optimizer_step(optimizer)\n            if scheduler is not None:\n                scheduler.step()\n\n    def eval_loop_fn(data_loader, model, device):\n        model.eval()\n        fin_targets = []\n        fin_outputs = []\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            targets = d[\"targets\"]\n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n            )\n\n            targets_np = targets.cpu().detach().numpy().tolist()\n            outputs_np = outputs.cpu().detach().numpy().tolist()\n            fin_targets.extend(targets_np)\n            fin_outputs.extend(outputs_np)    \n\n        return fin_outputs, fin_targets\n\n    df_train = pd.read_csv('train.csv')\n    df_valid = pd.read_csv('valid.csv')\n    MAX_LEN = 192\n    TRAIN_BATCH_SIZE = 4\n    EPOCHS = 3\n\n    tokenizer = transformers.RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n\n    train_targets = df_train.target.values\n    valid_targets = df_valid.target.values\n\n    train_dataset = BERTDatasetTraining(\n        comment_text=df_train.text.values,\n        targets=train_targets,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n    train_sampler = torch.utils.data.distributed.DistributedSampler(\n          train_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=True)\n\n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=1\n    )\n\n    valid_dataset = BERTDatasetTraining(\n        comment_text=df_valid.text.values,\n        targets=valid_targets,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n    valid_sampler = torch.utils.data.distributed.DistributedSampler(\n          valid_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=16,\n        sampler=valid_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n    \n    device = xm.xla_device()\n    model = mx.to(device)\n\n    print(\"%s: traning\"%device)\n    param_optimizer = list(model.named_parameters())\n    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n    optimizer_grouped_parameters = [\n        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n\n    lr = 0.4 * 1e-5 * xm.xrt_world_size()\n    num_train_steps = int(len(train_dataset) \/ TRAIN_BATCH_SIZE \/ xm.xrt_world_size() * EPOCHS)\n    xm.master_print(f'num_train_steps = {num_train_steps}, world_size={xm.xrt_world_size()}')\n\n    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n\n    for epoch in range(EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        train_loop_fn(para_loader.per_device_loader(device), model, optimizer, device, scheduler=scheduler)\n\n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        o, t = eval_loop_fn(para_loader.per_device_loader(device), model, device)\n        xm.save(model.state_dict(), \"roberta.bin\")\n        np.save('roberta', np.array(t))\n        auc = metrics.roc_auc_score(np.array(t) >= 0.5, o)\n        xm.master_print(f'AUC = {auc}')","fc123218":"# Start training processes\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = _run()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","b6d82b08":"class BERTDatasetTest:\n    def __init__(self, comment_text, tokenizer, max_length):\n        self.comment_text = comment_text\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.comment_text)\n\n    def __getitem__(self, item):\n        comment_text = str(self.comment_text[item])\n        comment_text = \" \".join(comment_text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            comment_text,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        padding_length = self.max_length - len(ids)\n        \n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),           \n        }\ndef test_loop_fn(data_loader, model, device):\n        model.eval()\n        fin_targets = []\n        fin_outputs = []\n        for bi, d in enumerate(data_loader):\n            ids = d[\"ids\"]\n            mask = d[\"mask\"]\n            \n\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n\n            outputs = model(\n                input_ids=ids,\n                attention_mask=mask,\n            )\n\n          \n            outputs_np = outputs.cpu().detach().numpy().tolist()\n            fin_outputs.extend(outputs_np)    \n\n        return fin_outputs\ndef test_model():\n  MAX_LEN = 192\n  device = xm.xla_device()\n  model = BertStyleModel('roberta')\n  model.load_state_dict(torch.load(\"roberta.bin\"))\n  model.to(device)\n  df_test = pd.read_csv('data\/test.csv')\n  df_test = df_test.fillna('')\n  df_test['text'] = df_test['keyword'] + ' ' + df_test['location'] + ' '+ df_test['text']\n  df_test = df_test[['text']]\n  tokenizer = transformers.RobertaTokenizer.from_pretrained(\"roberta-base\", do_lower_case=True)\n\n  test_dataset = BERTDatasetTest(\n        comment_text=df_test.text.values,\n        tokenizer=tokenizer,\n        max_length=MAX_LEN\n    )\n\n  test_sampler = torch.utils.data.distributed.DistributedSampler(\n          test_dataset,\n          num_replicas=xm.xrt_world_size(),\n          rank=xm.get_ordinal(),\n          shuffle=False)\n\n  test_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=16,\n        sampler=test_sampler,\n        drop_last=False,\n        num_workers=1\n    )\n  para_loader = pl.ParallelLoader(test_loader, [device])\n  ypred = test_loop_fn(para_loader.per_device_loader(device), model, device)\n  ypred= np.array(ypred)\n  np.save('result2.npy', ypred)\n  return ypred\n","ac3bc3e2":"# Start test processes XLA\ndef _mp_fn(rank, flags):\n    torch.set_default_tensor_type('torch.FloatTensor')\n    a = test_model()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=1, start_method='fork')","53ab340d":"submit = pd.read_csv('data\/sample_submission.csv')\nypred = np.load('result2.npy')\nsubmit['target'] = ypred\nplt.hist(ypred)\nplt.show()\npred = [int(i> 0.5) for i in ypred] \nplt.hist(pred)\nplt.show()\nsubmit['target'] = pred\nsubmit.to_csv('submit2.csv', index=False)\n### get 0.83389 acccuracy","fb8c34f1":"Try to concat 2 model.\nI really think: ROBERTA is better for this task. So try to multiply Roberta result with greatter 1 number. Get result better. \n    Summary: roberta*1.3 and get top 15%","e243404a":"bert = np.load('result.npy')\nroberta= np.load('result2.npy')","2b582b37":"bert_ = np.array([int(b>0.5) for b in bert])\nroberta_ = np.array([int(b>0.5) for b in roberta])\n","dd45702f":"bert_.sum() , roberta_.sum()","267f9823":"roberta2 = roberta*1.3 #  Best today is 0.83512\nroberta_ = np.array([int(b>0.5) for b in roberta2])\nprint(roberta_.sum())","0ea89bc4":"submit = pd.read_csv('sample_submission.csv')\nypred = roberta2\nsubmit['target'] = ypred\nplt.hist(ypred)\nplt.show()\npred = [int(i> 0.5) for i in ypred] \nplt.hist(pred)\nplt.show()\nsubmit['target'] = pred\nsubmit.to_csv('submit2.csv', index=False)","d78771fc":"##Submit the file\nimport pandas as pd\nsubmit = pd.read_csv('\/kaggle\/input\/nlpstarted\/submit2 (1).csv')\nsubmit.to_csv('submission.csv', index=False)","6b380853":"## ROBERTA","93294365":"## TPU use in this notebook to speed up training time.\n![XLA](https:\/\/xla.rocks\/_xlawpx\/wp-content\/uploads\/2019\/02\/logo-xla-2019.svg)\nTransformers to finetune\n\nPyTorch\/XLA is a Python package that uses the XLA deep learning compiler to connect the PyTorch deep learning framework and Cloud TPUs. You can try it right now, for free, on a single Cloud TPU with Google Colab, and use it in production and on Cloud TPU Pods with Google Cloud.\nNice to use it in this notebook!\n\n\n* Note:\n     Install XLA must the first shell you run to success install it.\nI cannot train model on Kaggle because memory and RAM requirement. So i trainning it on colab. You can visit this [link](https:\/\/colab.research.google.com\/drive\/16zHlALeStz-2vWBgE5Zol4wguPCz2Avz?usp=sharing) for details: \n\nPlease give me a upvote if it helpfully!!!!!!!!!!!!\n","18f976a7":"## BERT","535745db":"## Data Processing\nPreprocessing: Not in 2020\n* Just split in 2 dataset. \n","cdb340c9":"## Training","1a305187":"## Model config ","bbfd0c86":"## Postprocessing","02b297bf":"I got top 1% accuracy with ensemble of BERT+ROBERTA+ELECTRA. "}}