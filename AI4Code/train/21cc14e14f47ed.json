{"cell_type":{"c84f89f2":"code","67c280a9":"code","2284b442":"code","15d3c262":"code","852a5aa7":"code","b24ddcb2":"code","9838bb4a":"code","8cd084f9":"code","499111c4":"code","81a2ef30":"code","c1c74f1d":"code","57904e2a":"code","a67d6a5f":"code","6c0b89f0":"code","96b3fd6c":"code","007e3893":"markdown","ce125202":"markdown","d968aee3":"markdown","227b048f":"markdown","f104d46a":"markdown","c8f2ba59":"markdown","614ab973":"markdown","847a81ee":"markdown"},"source":{"c84f89f2":"import gc\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_error\n\nimport tensorflow as tf\n#tf-KERAS\u6559\u7a0bhttps:\/\/blog.csdn.net\/weixin_45250844\/article\/details\/92842390?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163752194416780274170199%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163752194416780274170199&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-92842390.pc_search_es_clickV2&utm_term=tensorflow.keras&spm=1018.2226.3001.4187\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, LSTM, GRU\nfrom tensorflow.keras.layers import Bidirectional, Multiply\nimport os\nos.environ['CUDA_VISIBLE_DEVICES']='1'\nnp.random.seed(42)\ntf.random.set_seed(42)\n","67c280a9":"train_df = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\nprint(f\"train_df: {train_df.shape}\")\ntrain_df.head()","2284b442":"test_df = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nprint(f\"train_df: {train_df.shape}\")\ntest_df.head()","15d3c262":"#\u7279\u5f81\u6784\u5efa\n#\u6784\u5efa\u7406\u8bba\u4e0a\u5b58\u5728\u76f8\u5173\u6027\uff0c\u4ee5\u53ca\u4f1a\u5f71\u54cd\u7ed3\u679c\u7684\u7279\u5f81\ndef add_features(df):\n    #in_out_out\u4ea4\u53c9\u8fc7\u7a0b\u4e2d\u7684\u7279\u5f81## \u4e0a\u9762\u56fe\u4e00\n    df['cross']= df['u_in'] * df['u_out']\n    #\u2013\u2013\n    df['cross2']= df['time_step'] * df['u_out']\n    #area\u7edf\u8ba1\u91cf\n    df['area'] = df['time_step'] * df['u_in']\n    #https:\/\/blog.csdn.net\/weixin_48135624\/article\/details\/113829632?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163791438716780271959636%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163791438716780271959636&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-113829632.pc_search_es_clickV2&utm_term=cumsum&spm=1018.2226.3001.4187\n    df['area'] = df.groupby('breath_id')['area'].cumsum()\n    #cumsum\u8fdb\u884c\u5bb9\u91cf\u7edf\u8ba1\n    df['time_step_cumsum'] = df.groupby(['breath_id'])['time_step'].cumsum()\n#     1 2 3 4? 5 \n#     sumsum?- 1,3,6,10,15\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    print(\"Step-1...Completed\")\n    #shift\u7279\u5f81\uff0c\u8be6\u7ec6\u53c2\u7167coggle\n#     1 2 3 4 5\n#     nan 1 2 3 4\n#     2 3 4 5 nan\n    #https:\/\/mp.weixin.qq.com\/s\/s7iJ0rNnGDoBMvlnt0NWKQ\n    #\u628a\u6bcf\u4e2abreath_id\u8fdb\u884c\u5206\u5272\uff0c\u518d\u83b7\u5f97shift\u7279\u5f81\n    df['u_in_lag1'] = df.groupby('breath_id')['u_in'].shift(1)\n    df['u_out_lag1'] = df.groupby('breath_id')['u_out'].shift(1)\n    df['u_in_lag_back1'] = df.groupby('breath_id')['u_in'].shift(-1)\n    df['u_out_lag_back1'] = df.groupby('breath_id')['u_out'].shift(-1)\n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2)\n    df['u_out_lag2'] = df.groupby('breath_id')['u_out'].shift(2)\n    df['u_in_lag_back2'] = df.groupby('breath_id')['u_in'].shift(-2)\n    df['u_out_lag_back2'] = df.groupby('breath_id')['u_out'].shift(-2)\n    df['u_in_lag3'] = df.groupby('breath_id')['u_in'].shift(3)\n    df['u_out_lag3'] = df.groupby('breath_id')['u_out'].shift(3)\n    df['u_in_lag_back3'] = df.groupby('breath_id')['u_in'].shift(-3)\n    df['u_out_lag_back3'] = df.groupby('breath_id')['u_out'].shift(-3)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4)\n    df['u_out_lag4'] = df.groupby('breath_id')['u_out'].shift(4)\n    df['u_in_lag_back4'] = df.groupby('breath_id')['u_in'].shift(-4)\n    df['u_out_lag_back4'] = df.groupby('breath_id')['u_out'].shift(-4)\n    df = df.fillna(0)\n    print(\"Step-2...Completed\")\n    #\u4e0d\u53ef\u6784\u5efa\u8fc7\u591a\u65e0\u5b9e\u9645\u610f\u4e49\u7684\u65f6\u5e8f\u7279\u5f81\uff0c\u8fc7\u591a\u7684\u65f6\u5e8f\u7279\u5f81\u4f1a\u5bfc\u81f4\u6a21\u578b\u7684\u6cdb\u5316\u6027\u4e0b\u964d\n    \n    df['breath_id__u_in__max'] = df.groupby(['breath_id'])['u_in'].transform('max')\n    df['breath_id__u_in__mean'] = df.groupby(['breath_id'])['u_in'].transform('mean')\n    #\u8be5u_in\u7684\u6570\u636e\u4e0e\u6700\u5927\u503c\u7684\u5dee\u8ddd\uff0cu_in\u4e0e\u6700\u5c0f\u503c\u7684\u5dee\u8ddd\n    df['breath_id__u_in__diffmax'] = df.groupby(['breath_id'])['u_in'].transform('max') - df['u_in']\n    df['breath_id__u_in__diffmean'] = df.groupby(['breath_id'])['u_in'].transform('mean') - df['u_in']\n    print(\"Step-3...Completed\")\n    #u_in\u4e0eu_lagin\u7684\u6570\u503c\u5dee\u8ddd\uff08\uff09\n    df['u_in_diff1'] = df['u_in'] - df['u_in_lag1']\n    df['u_out_diff1'] = df['u_out'] - df['u_out_lag1']\n    df['u_in_diff2'] = df['u_in'] - df['u_in_lag2']\n    df['u_out_diff2'] = df['u_out'] - df['u_out_lag2']\n    df['u_in_diff3'] = df['u_in'] - df['u_in_lag3']\n    df['u_out_diff3'] = df['u_out'] - df['u_out_lag3']\n    df['u_in_diff4'] = df['u_in'] - df['u_in_lag4']\n    df['u_out_diff4'] = df['u_out'] - df['u_out_lag4']\n    print(\"Step-4...Completed\")\n    ################\n    #\u5b9a\u4e49one\u5168\u662f1\n    df['one'] = 1\n    #\u8fdb\u884c\u7d2f\u52a0,groupby\u540e\u8fdb\u884c\u7d2f\u52a0\n    df['count'] = (df['one']).groupby(df['breath_id']).cumsum()\n    #\u5bf9\u4e4b\u524d\u7684u_in_cumsum\u8fdb\u884c\u5e73\u5747\uff0c\u589e\u52a0\u6cdb\u5316\u6027\n    df['u_in_cummean'] =df['u_in_cumsum'] \/df['count']\n    #\u5bf9breath_id\u505ashift\u7279\u5f81\n    df['breath_id_lag']=df['breath_id'].shift(1).fillna(0)\n    df['breath_id_lag2']=df['breath_id'].shift(2).fillna(0)\n    df['breath_id_lagsame']=np.select([df['breath_id_lag']==df['breath_id']],[1],0)\n    df['breath_id_lag2same']=np.select([df['breath_id_lag2']==df['breath_id']],[1],0)\n    df['breath_id__u_in_lag'] = df['u_in'].shift(1).fillna(0)\n    df['breath_id__u_in_lag'] = df['breath_id__u_in_lag'] * df['breath_id_lagsame']\n    df['breath_id__u_in_lag2'] = df['u_in'].shift(2).fillna(0)\n    df['breath_id__u_in_lag2'] = df['breath_id__u_in_lag2'] * df['breath_id_lag2same']\n    print(\"Step-5...Completed\")\n    \n#     diff\uff1a\u6c42\u5dee\u548c\u6c42\u5bfc\n# diff(X)\uff1a\u5bf9\u4e8e\u4e00\u4e2a\u5411\u91cf\u6765\u8bf4\uff0cdiff(X)\u5c31\u662f [X(2)-X(1) X(3)-X(2) \u2026 X(n)-X(n-1)] >>>>\u6b64\u65f6\u8fd9\u4e2a\u5411\u91cf\u7684\u7ef4\u6570\u662fn-1\u7ef4\u3002\u5bf9\u4e8e\u4e00\u4e2a\u77e9\u9635\u6765\u8bf4\uff0c\u7ed3\u679c\u662f\uff1a[X(2:n,:) - X(1:n-1,:)]\uff1b\u5bf9\u4e8e\u4e00\u4e2aN*D\u7684\u77e9\u9635\uff0c\u7ed3\u679c\u662f\u540e\u4e00\u884c\u51cf\u524d\u4e00\u884c\u7684\u5dee\u503c\u3002\n# diff(X,N) \uff1a\u76f8\u5f53\u4e8eN\u9636\u884c\u5dee\u5206\uff0c\u4e5f\u5c31\u662f\u76f8\u5f53\u4e8e\u505aN\u6b21diff\uff08x\uff09\uff0c\u5982\u679cN\u5927\u4e8ex\u7684\u884c\u6570\uff0c\u5219\u6700\u7ec8\u4f1a\u6210\u4e3a\u4e00\u4e2a[]\u3002>>\u7a7a\u5355\u4e2a\u5143\u7d20\u7684\u77e9\u9635\n# diff(X,N,DIM) \uff1a\u505aN\u6b21\u5dee\u5206\u3002DIM\u662f\u65b9\u5411\u3002>>DIM=1\u8868\u793a\u884c\u5dee\u5206\uff1bDIM=2\u8868\u793a\u5217\u5dee\u5206\u3002\n# \u603b\u7ed3\uff1a\n# diff\uff08x\uff09= diff\uff08x\uff0c1\uff09=diff\uff08x\uff0c1\uff0c1\uff09\uff1b\n\n    \n    df['time_step_diff'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n    df['ewm_u_in_mean'] = (df\\\n                           .groupby('breath_id')['u_in']\\\n                           .ewm(halflife=9)\\\n                           .mean()\\\n                           .reset_index(level=0,drop=True))\n    #agg\uff1f   https:\/\/blog.csdn.net\/qq_24753293\/article\/details\/80323487?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163791532916780274146481%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163791532916780274146481&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-80323487.pc_search_es_clickV2&utm_term=%E3%80%82agg&spm=1018.2226.3001.4187\n    df[[\"15_in_sum\",\"15_in_min\",\"15_in_max\",\"15_in_mean\"]] = (df\\\n                                                              .groupby('breath_id')['u_in']\\\n                                                              .rolling(window=15,min_periods=1)\\\n                                                              .agg({\"15_in_sum\":\"sum\",\n                                                                    \"15_in_min\":\"min\",\n                                                                    \"15_in_max\":\"max\",\n                                                                    \"15_in_mean\":\"mean\"})\\\n                                                               .reset_index(level=0,drop=True))\n    print(\"Step-6...Completed\")\n    \n    df['u_in_lagback_diff1'] = df['u_in'] - df['u_in_lag_back1']\n    df['u_out_lagback_diff1'] = df['u_out'] - df['u_out_lag_back1']\n    df['u_in_lagback_diff2'] = df['u_in'] - df['u_in_lag_back2']\n    df['u_out_lagback_diff2'] = df['u_out'] - df['u_out_lag_back2']\n    print(\"Step-7...Completed\")\n    \n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df['R__C'] = df[\"R\"].astype(str) + '__' + df[\"C\"].astype(str)\n    df = pd.get_dummies(df)\n    print(\"Step-8...Completed\")\n    \n    return df\n\n\nprint(\"Train data...\\n\")\ntrain = add_features(train_df)\n\nprint(\"\\nTest data...\\n\")\ntest = add_features(test_df)\n\ndel train_df\ndel test_df\ngc.collect()","852a5aa7":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\n#\u5220\u9664\u65e0\u7528\u7279\u5f81\ntrain.drop(['pressure','id', 'breath_id','one','count',\n            'breath_id_lag','breath_id_lag2','breath_id_lagsame',\n            'breath_id_lag2same'], axis=1, inplace=True)\n\ntest = test.drop(['id', 'breath_id','one','count','breath_id_lag',\n                  'breath_id_lag2','breath_id_lagsame',\n                  'breath_id_lag2same'], axis=1)\n\nprint(f\"train: {train.shape} \\ntest: {test.shape}\")","b24ddcb2":"1","9838bb4a":"#\u6570\u636e\u8fdb\u884cRobustScaler\u6e05\u6670\u6e05\u6d17\nscaler = RobustScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])\n\nprint(f\"train: {train.shape} \\ntest: {test.shape} \\ntargets: {targets.shape}\")\n# train: (75450, 80, 64) \n# test: (50300, 80, 64) \n# targets: (75450, 80)","8cd084f9":"pressure = targets.squeeze().reshape(-1,1).astype('float32')\n\nP_MIN = np.min(pressure)\nP_MAX = np.max(pressure)\nP_STEP = (pressure[1] - pressure[0])[0]\nprint('Min pressure: {}'.format(P_MIN))\nprint('Max pressure: {}'.format(P_MAX))\nprint('Pressure step: {}'.format(P_STEP))\nprint('Unique values:  {}'.format(np.unique(pressure).shape[0]))\n\ndel pressure\ngc.collect()\n# Min pressure: -1.8957443237304688\n# Max pressure: 64.82099151611328\n# Pressure step: 0.07030248641967773\n# Unique values:  950","499111c4":"#kaggle\u786c\u4ef6\u8bbe\u7f6ehttps:\/\/blog.csdn.net\/weixin_42813521\/article\/details\/121450371?ops_request_misc=&request_id=&biz_id=&utm_medium=distribute.pc_search_result.none-task-blog-2~all~es_rank~default-1-121450371.pc_search_es_clickV2&utm_term=tpu+%3D+tf.distribute.cluster_resolver.TPUClusterResolver%28%29&spm=1018.2226.3001.4187\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.exbperimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = tpu_strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = 512\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {BATCH_SIZE}\")","81a2ef30":"# https:\/\/blog.csdn.net\/weixin_45250844\/article\/details\/92842390?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163752194416780274170199%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163752194416780274170199&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-92842390.pc_search_es_clickV2&utm_term=tensorflow.keras&spm=1018.2226.3001.4187\n# https:\/\/blog.csdn.net\/qq_38147421\/article\/details\/107694477?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522163799062816780255211247%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=163799062816780255211247&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-107694477.pc_search_es_clickV2&utm_term=gru&spm=1018.2226.3001.4187\ndef dnn_model():\n    \n    x_input = Input(shape=(train.shape[-2:]))#x_input   (80,64)  \u5176\u4e2d\u768475450\u4e0a\u7684\u7ef4\u5ea6\u7531batch_size\u8fdb\u884c\u51b3\u5b9a\n    x1 = Bidirectional(LSTM(units=768, return_sequences=True))(x_input)\n    x2 = Bidirectional(LSTM(units=512, return_sequences=True))(x1)\n    x3 = Bidirectional(LSTM(units=384, return_sequences=True))(x2)\n    x4 = Bidirectional(LSTM(units=256, return_sequences=True))(x3)\n    x5 = Bidirectional(LSTM(units=128, return_sequences=True))(x4)\n    \n    z2 = Bidirectional(GRU(units=384, return_sequences=True))(x2)\n    \n    z31 = Multiply()([x3, z2])\n    z31 = BatchNormalization()(z31)\n    z3 = Bidirectional(GRU(units=256, return_sequences=True))(z31)\n    \n    z41 = Multiply()([x4, z3])\n    z41 = BatchNormalization()(z41)\n    z4 = Bidirectional(GRU(units=128, return_sequences=True))(z41)\n    \n    z51 = Multiply()([x5, z4])\n    z51 = BatchNormalization()(z51)\n    z5 = Bidirectional(GRU(units=64, return_sequences=True))(z51)\n    \n    x = Concatenate(axis=2)([x5, z2, z3, z4, z5])\n    \n    x = Dense(units=128, activation='selu')(x)\n    \n    x_output = Dense(units=1)(x)\n\n    model = Model(inputs=x_input, outputs=x_output, \n                  name='DNN_Model')\n    return model","c1c74f1d":"model = dnn_model()#\u5b9e\u4f8b\u5316\nmodel.summary()","57904e2a":"plot_model(\n    model, \n    to_file='Google_Brain_Keras_Model.png', \n    show_shapes=True,\n    show_layer_names=True\n)\n###lstm\u63d0\u53d6\u7279\u7279\u5f81\uff0c\u4e4b\u540e\u5377\u79ef\u5206\u7c7b\n#\u4e3a\u4e86\u9632\u6b62\u7279\u5f81\u4e22\u5931\uff0c\u7528\u4e86\u8df3\u8dc3\u94fe\u63a5\uff0c\u6b8b\u5dee\u8fde\u63a5\n#\u7528\u53cc\u5934gru\u548clstm\u4e3a\u4e86\u66f4\u597d\u5730\u5229\u7528\u957f\u77ed\u65f6\u95f4\u7279\u5f81\n","a67d6a5f":"with strategy.scope():\n    \n    VERBOSE = 0\n#     fit \u4e2d\u7684 verbose\n# verbose\uff1a\u65e5\u5fd7\u663e\u793a\n# verbose = 0 \u4e3a\u4e0d\u5728\u6807\u51c6\u8f93\u51fa\u6d41\u8f93\u51fa\u65e5\u5fd7\u4fe1\u606f\n# verbose = 1 \u4e3a\u8f93\u51fa\u8fdb\u5ea6\u6761\u8bb0\u5f55\n# verbose = 2 \u4e3a\u6bcf\u4e2aepoch\u8f93\u51fa\u4e00\u884c\u8bb0\u5f55\n# \u6ce8\u610f\uff1a \u9ed8\u8ba4\u4e3a 1\n    test_preds = []\n    kf = KFold(n_splits=24, shuffle=True, random_state=2021)###24\u6298\uff0c\u8bad\u7ec324\u6b21#\u4ea4\u53c9\u9a8c\u8bc1\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        \n        model = dnn_model()\n        model.compile(optimizer=\"adam\", loss=\"mae\")#\u4f18\u5316\u5668 ADAM  loss\uff1aMAE\n#ReduceLROnPlateau\n# \u5f53\u68c0\u6d4b\u6307\u6807\u672a\u5f97\u5230\u6539\u5584\uff0c\u8fdb\u884cn\u500d\u7684\u5b66\u4e60\u7387\u8c03\u6574\u5e38\u5e38\u80fd\u83b7\u5f97\u8f83\u597d\u7684\u6548\u679c\u3002\n# \u5b9a\u4e49\u4e00\u4e2acallback\u53c2\u6570reduce_lr\uff1a\n# reduce_lr = ReduceLROnPlateau(monitor=\u2018val_loss\u2019, factor=0.5, patience=2, verbose=1)\n\n# monitor\uff1a\u88ab\u76d1\u6d4b\u7684\u91cf\n# factor\uff1a\u6bcf\u6b21\u51cf\u5c11\u5b66\u4e60\u7387\u7684\u56e0\u5b50\uff0c\u5b66\u4e60\u7387\u5c06\u4ee5lr = lr*factor\u7684\u5f62\u5f0f\u88ab\u51cf\u5c11\n# patience\uff1a\u5f53patience\u4e2aepoch\u8fc7\u53bb\u800c\u6a21\u578b\u6027\u80fd\u4e0d\u63d0\u5347\u65f6\uff0c\u5b66\u4e60\u7387\u51cf\u5c11\u7684\u52a8\u4f5c\u4f1a\u88ab\u89e6\u53d1\n# mode\uff1a\u2018auto\u2019\uff0c\u2018min\u2019\uff0c\u2018max\u2019\u4e4b\u4e00\uff0c\u5728min\u6a21\u5f0f\u4e0b\uff0c\u5982\u679c\u68c0\u6d4b\u503c\u89e6\u53d1\u5b66\u4e60\u7387\u51cf\u5c11\u3002\u5728max\u6a21\u5f0f\u4e0b\uff0c\u5f53\u68c0\u6d4b\u503c\u4e0d\u518d\u4e0a\u5347\u5219\u89e6\u53d1\u5b66\u4e60\u7387\u51cf\u5c11\u3002\n# epsilon\uff1a\u9608\u503c\uff0c\u7528\u6765\u786e\u5b9a\u662f\u5426\u8fdb\u5165\u68c0\u6d4b\u503c\u7684\u201c\u5e73\u539f\u533a\u201d\n# cooldown\uff1a\u5b66\u4e60\u7387\u51cf\u5c11\u540e\uff0c\u4f1a\u7ecf\u8fc7cooldown\u4e2aepoch\u624d\u91cd\u65b0\u8fdb\u884c\u6b63\u5e38\u64cd\u4f5c\n# min_lr\uff1a\u5b66\u4e60\u7387\u7684\u4e0b\u9650\n\n        lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.85, \n                               patience=7, verbose=VERBOSE)\n        #\u6a21\u578b\u4fdd\u5b58\n        save_locally = tf.saved_model.SaveOptions(experimental_io_device='\/job:localhost')\n        chk_point = ModelCheckpoint(f'.\/Bidirect_LSTM_model_2021_{fold+1}C.h5', options=save_locally, \n                                    monitor='val_loss', verbose=VERBOSE, \n                                    save_best_only=True, mode='min')\n        #early_stopping\u8bbe\u7f6e\uff0cmonitor\uff1aval\u2014\u2014loss\uff1a\u4f7f\u7528\u9a8c\u8bc1\u96c6\u7684loss\u76d1\u6d4b\uff0cpatience= 30:\u5f5330\u4e2abatchloss\u6ca1\u6709\u53d1\u751f\u4e0b\u964d\u5c31\u505c\u6b62\u8bad\u7ec3\n        es = EarlyStopping(monitor=\"val_loss\", patience=30, \n                           verbose=VERBOSE, mode=\"min\", #VERBOSE\uff0c\u65e5\u5fd7\u662f\u5426\u6253\u5370\n                           restore_best_weights=True)#\u662f\u5426\u4fdd\u7559\u6700\u597d\u7684\u6a21\u578b\u6743\u91cd\n        \n        model.fit(X_train, y_train, \n                  validation_data=(X_valid, y_valid), \n                  epochs=300,\n                  verbose=VERBOSE,\n                  batch_size=BATCH_SIZE, \n                  callbacks=[lr, chk_point, es])#callbacks =lr:\u52a8\u6001\u5b66\u4e60\u7387\uff0cchk_point\uff1a\u6a21\u578b\u4fdd\u5b58\uff0ces\uff1aearly\u2014\u2014stopping\n        \n        load_locally = tf.saved_model.LoadOptions(experimental_io_device='\/job:localhost')\n        model = load_model(f'.\/Bidirect_LSTM_model_2021_{fold+1}C.h5', options=load_locally)\n        \n        y_true = y_valid.squeeze().reshape(-1, 1)\n        y_pred = model.predict(X_valid, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1)\n        score = mean_absolute_error(y_true, y_pred)\n        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n        \n        test_preds.append(model.predict(test, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1).squeeze())","6c0b89f0":"submission = pd.read_csv('\/home\/extend\/yanhao\/ventilator-pressure-prediction\/sample_submission.csv')\nsubmission[\"pressure\"] = sum(test_preds)\/24\nsubmission.to_csv('mean_submission.csv', index=False)","96b3fd6c":"submission[\"pressure\"] = np.median(np.vstack(test_preds),axis=0)\nsubmission[\"pressure\"] = np.round((submission.pressure - P_MIN)\/P_STEP) * P_STEP + P_MIN\nsubmission[\"pressure\"] = np.clip(submission.pressure, P_MIN, P_MAX)\nsubmission.to_csv('median_submission.csv', index=False)","007e3893":"# \u6a21\u578b\u5b9a\u4e49\n","ce125202":"# \u7279\u5f81\u5de5\u7a0b","d968aee3":"![\u622a\u5c4f2021-11-26 \u4e0b\u53484.24.50.png](attachment:ff1987a9-b96f-47d4-b4ad-b36fdfa8e9c7.png)","227b048f":"# \u5e93\u5bfc\u5165\n","f104d46a":"# \u6570\u636e\u5207\u5272\n\n","c8f2ba59":"- \u6a21\u578b\u6750\u6599 \u5c42\nBidirectional RNN(BRNN)\u540c\u65f6\u4f7f\u7528\u6b63\u5411\u548c\u53cd\u5411\u7684\u5e8f\u5217\u6765\u8fdb\u884c\u9884\u6d4b\uff0c\u662f\u53cc\u5411\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\n","614ab973":"# \u786c\u4ef6\u8bbe\u7f6e\n","847a81ee":"\u8fd9\u662f\u4e00\u4e2a\u6709\u4e24\u4e2a\u7279\u5f81(x\/y)\u7684\u4e8c\u5206\u7c7b\u6570\u636e\u96c6\uff0c\u56db\u79cd\u9884\u5904\u7406\u65b9\u6cd5\uff1a\n![\u622a\u5c4f2021-11-23 \u4e0b\u53489.02.06.png](attachment:d8a674fc-9149-4a70-baa7-851b4ac56466.png)\n- StandardScaler\uff1a\u786e\u4fdd\u6bcf\u4e2a\u7279\u5f81\u7684\u5e73\u5747\u503c\u4e3a0\uff0c\u65b9\u5dee\u4e3a1\u3002\n\n- RobustScaler\uff1a\u4f7f\u7528\u4e2d\u4f4d\u6570\u548c\u56db\u5206\u4f4d\u6570\uff08\u56db\u5206\u4e4b\u4e00\uff09\uff0c\u786e\u4fdd\u6bcf\u4e2a\u7279\u5f81\u7684\u7edf\u8ba1\u5c5e\u6027\u90fd\u4f4d\u4e8e\u540c\u4e00\u8303\u56f4\u3002\n- MinMaxScalar\uff1a\u79fb\u52a8\u6570\u636e\uff0c\u4f7f\u6240\u6709\u7279\u5f81\u90fd\u521a\u597d\u4f4d\u4e8e0-1\u4e4b\u95f4\u3002\n- Normalizer\uff1a\u5bf9\u6bcf\u4e2a\u6570\u636e\u70b9\u8fdb\u884c\u7f29\u653e\uff0c\u4f7f\u5f97\u7279\u5f81\u5411\u91cf\u7684\u6b27\u5f0f\u957f\u5ea6\u7b49\u4e8e1\u3002"}}