{"cell_type":{"3fb42f38":"code","227f8aab":"code","72d702c8":"code","a5a1fbe2":"code","041235e0":"code","d0018892":"code","f52e9e28":"code","1ed696ce":"code","6b2a1718":"code","18d4b7d5":"code","252775ea":"code","65bb7992":"code","f956bfba":"code","933cb0e9":"code","ccc4ea21":"code","dbe64dd4":"code","876a855e":"code","6ac9e361":"code","27d63764":"code","6e7c32cd":"code","6c16f54d":"code","e60e6a62":"code","2f41628b":"code","e5943f9c":"code","7bc613e2":"code","51cfcb97":"code","a8ba313d":"code","eed1e226":"code","8cacf3a1":"code","6723dd8a":"code","960d2bda":"code","895c2c0e":"code","80629d7f":"code","aaf44fb8":"code","836a3625":"markdown","5c6fc27d":"markdown","b90160fa":"markdown","7aed3da6":"markdown","41e80bf3":"markdown","9f90a15b":"markdown","55b1542e":"markdown","d938d3b3":"markdown","5871abfa":"markdown","9e33bc34":"markdown","a060e4d2":"markdown","7d7ab887":"markdown","e2802e67":"markdown","dc738009":"markdown","d40e3901":"markdown","6a4f3121":"markdown","de268117":"markdown","45602cfc":"markdown","ae48592c":"markdown","c6deb65e":"markdown","738b06e1":"markdown","a7254ffa":"markdown","2a5ddbec":"markdown","032d2162":"markdown","610731e9":"markdown","cc97383a":"markdown"},"source":{"3fb42f38":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n!pip install texthero\n!pip install -U spacy\nimport texthero as hero\nfrom texthero import preprocessing\nfrom texthero import stopwords\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tqdm.notebook import trange, tqdm\nfrom nltk.tokenize import sent_tokenize\nimport nltk, re, string, collections\nfrom nltk.util import ngrams \nfrom sklearn.feature_extraction.text import CountVectorizer","227f8aab":"import matplotlib.pyplot as plt","72d702c8":"\ndf = pd.read_csv(\"\/kaggle\/input\/spam-or-not-spam-dataset\/spam_or_not_spam.csv\")\n#remove nans\ndf.email[165]\n\n","a5a1fbe2":"default_stopwords = stopwords.DEFAULT\ncustom_stopwords = default_stopwords.union(set([\"number\",\"url\",\"hyperlink\",\"list\",\"spamassassin\",\"rpm\",\"date\",\"freshrpms \"]))\ndf['clean_email'] = tqdm(hero.clean(df['email']))\ndf['clean_email'] = tqdm(hero.remove_stopwords(df['clean_email'], custom_stopwords))\ndf['clean_email'] = tqdm(hero.clean(df['clean_email']))\n\ndf.replace(\"\", float(\"NaN\"), inplace=True)\ndf = df.dropna()\n\ndf.count()\ndf['clean_email']","041235e0":"df['clean_email'].iloc[2]","d0018892":"df['tfidf'],features = tqdm(\n    (hero.tfidf(df['clean_email'],return_feature_names=True))\n)\ndf[[\"tfidf\", \"clean_email\"]]\n\ndf['pca'] = tqdm(\n   (df['tfidf']\n   .pipe(hero.pca)\n))\n\ndf['tokens'] = tqdm(hero.tokenize(df[\"clean_email\"]))\ndf = df.dropna()","f52e9e28":"print(\"Word Tokens:\",len(features),features[400:410])","1ed696ce":"df['tokens'].head()","6b2a1718":"df[\"email\"].count()","18d4b7d5":"\ncount_df = df.groupby('label')['clean_email'].count()\n\nprint(count_df,count_df.plot(kind = 'bar',ylabel=\"Number of Observations\"))","252775ea":"print(\"Word cloud for emails that are not spam\")\nhero.wordcloud(df[\"clean_email\"].loc[df['label'] == 0])","65bb7992":"print(\"Word cloud for emails that are spam\")\nhero.wordcloud(df[\"clean_email\"].loc[df['label'] == 1])","f956bfba":"NUM_TOP_WORDS = 20\ntop_20 = hero.visualization.top_words(df[\"clean_email\"].loc[df['label'] == 0]).head(NUM_TOP_WORDS)\ntop_20.plot.bar(rot=90, title=\"Top 20 words in non-spam emails\",ylabel=\"Frequency\")\nplt.show(block=True)","933cb0e9":"NUM_TOP_WORDS = 20\ntop_20 = hero.visualization.top_words(df[\"clean_email\"].loc[df['label'] == 1]).head(NUM_TOP_WORDS)\ntop_20.plot.bar(rot=90, title=\"Top 20 words in spam emails\",ylabel=\"Frequency\")\nplt.show(block=True)","ccc4ea21":"df['totalwords'] = df['clean_email'].str.split().str.len()","dbe64dd4":"n, bins, patches = plt.hist(df['totalwords'],range=(0,400))\nplt.xticks(bins)\nplt.grid(color='white', axis='x')\nplt.xlabel(\"# of Words\")\nplt.ylabel(\"# of Emails\")\nplt.show()","876a855e":"print(\"Statistics on number of words in observation 0 is not spam, 1 is spam\")\nprint(df.groupby(\"label\")[\"totalwords\"].describe())\ndf[\"totalwords\"].mean()","6ac9e361":"df['tokenized_sents'] = df.apply(lambda row: sent_tokenize(row['email']), axis = 1)\ndf[\"sentence_len\"]= df['tokenized_sents'].str.len()\ndf[\"sentence_len\"].describe()","27d63764":"word_vectorizer = CountVectorizer(ngram_range=(2,2), analyzer='word')\nsparse_matrix = tqdm(word_vectorizer.fit_transform(df['clean_email']),total=len(df['clean_email']))\nfrequencies = sum(sparse_matrix).toarray()[0]\nprint(\"Done\")","6e7c32cd":"n_grams = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=\"frequency\", ascending=False)\nn_grams.describe()\nn_grams.head(20)","6c16f54d":"word_vectorizer.set_params(ngram_range=(3,3))\nsparse_matrix = tqdm(word_vectorizer.fit_transform(df['clean_email']),total=len(df['clean_email']))\nfrequencies = sum(sparse_matrix).toarray()[0]\nprint(\"Done\")","e60e6a62":"pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency']).sort_values(by=\"frequency\", ascending=False).head(20)","2f41628b":"from sklearn.feature_extraction.text import CountVectorizer\n# df= df.drop(df.query('label == 0').sample(frac=.8).index)\n\ncv = CountVectorizer()\nX = cv.fit_transform(df[\"clean_email\"].values).toarray()\ny = df[\"label\"]","e5943f9c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=21)","7bc613e2":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\n\nclassifier = MultinomialNB()\n# classifier = GaussianNB()\nclassifier.fit(X_train , y_train)","51cfcb97":"y_pred = classifier.predict(X_test)","a8ba313d":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.winter):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=30)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize=20)\n    plt.yticks(tick_marks, classes, fontsize=20)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", \n                 color=\"white\" if cm[i, j] < thresh else \"black\", fontsize=40)\n    \n    plt.tight_layout()\n    plt.ylabel('True label', fontsize=30)\n    plt.xlabel('Predicted label', fontsize=30)\n\n    return plt\n\ncm = confusion_matrix(y_test, y_pred)\n\nfig = plt.figure(figsize=(10, 10))\nplot = plot_confusion_matrix(cm, classes=['Not Spam','Spam'], normalize=True, title='Confusion matrix')\nplt.show()","eed1e226":"from sklearn.metrics import accuracy_score, precision_score\naccuracy_score(y_test, y_pred)","8cacf3a1":"testString = pd.DataFrame([\"\"\"(1) FINAL MESSAGE: Payout Verification - $2500 PAYOUT is\nready to be addressed in your Name and we want to be sure it gets to the right place. \nClick below to start the confirmation process. The sooner you act, the sooner it can be in your hands!\"\"\"])\nvecs = cv.transform(testString[0])\nif classifier.predict(vecs.toarray())[0] == 0:\n    print(\"Not Spam\")\nelse:\n    print(\"Spam\")","6723dd8a":"from keras.preprocessing.text import Tokenizer\ntokenizer = Tokenizer(num_words=10000)\ntokenizer.fit_on_texts(df[\"clean_email\"].values)\nsequences = tokenizer.texts_to_sequences(df[\"clean_email\"].values)\n\nX_train, X_test, y_train, y_test = train_test_split(sequences, y, test_size = 0.20, random_state=21)","960d2bda":"from keras import preprocessing\nx_train = preprocessing.sequence.pad_sequences(X_train, 1000)\nx_test = preprocessing.sequence.pad_sequences(X_test, 1000)","895c2c0e":"from keras.models import Sequential\nfrom keras.layers import Dense,Embedding,SimpleRNN,Dropout\n\nmodel = Sequential()\nmodel.add(Embedding(len(x_train), 128, input_length=1000))\nmodel.add(Dropout(0.2))\nmodel.add(SimpleRNN(16,activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1))\nmodel.compile(optimizer='RMSprop', loss='binary_crossentropy', metrics=['acc'])\nmodel.summary()\n\nhistory = model.fit(x_train, y_train,\n                    epochs=20,\n                    batch_size=128,\n                    validation_split=0.25)","80629d7f":"result=model.evaluate(x_test,y_test)\nprint(\"test loss:{}\\ntest accuracy:{}\".format(result[0],result[1]))","aaf44fb8":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","836a3625":"Cleaned up dataset and removed all empty rows. Also removed all stop words.","5c6fc27d":"## Top 20 Tri-grams:","b90160fa":"Total number of observations:","7aed3da6":"## Word Tokenizer:","41e80bf3":"## Average # of words in spam vs non spam email:","9f90a15b":"# Preamble\n\nTitle: *Spam Classifier*\n\nAuthor: Christian Mechem\n\nEmail: cmechem@msudenver.edu\n\nLast Update: 11\/17\/2021","55b1542e":"## Sample of what observation looks like:","d938d3b3":"Test string to see if it is spam or not","5871abfa":"# Classification:","9e33bc34":"# Data Analysis:","a060e4d2":"## Top 20 Bi-grams:","7d7ab887":"Pad clean_email sequences so that every input is the same length","e2802e67":"## Word Frequency:","dc738009":"## Sentence Analysis:","d40e3901":"Function to create confusion matrix","6a4f3121":"Predict Values","de268117":"## N-gram analysis:","45602cfc":"## Average # of words in observation:","ae48592c":"Get Accuracy","c6deb65e":"# Imports:","738b06e1":"## Using Naive Bayes","a7254ffa":"## Using a RNN","2a5ddbec":"## Label summary statistics:\nNumber of non-spam emails (label=0) vs spam emails (label=1)","032d2162":"No punction in original dataset so there are no sentences","610731e9":"# Process Spam or Not Dataset:","cc97383a":"Split dataset by 20% for train and test"}}