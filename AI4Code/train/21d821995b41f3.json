{"cell_type":{"6f726cd1":"code","86d82b35":"code","ac92fc4f":"code","352d91a6":"code","a2acc55a":"code","84ad93b2":"code","3e28a4ac":"code","0bd1409c":"code","1654aec6":"code","c5d88334":"code","7c9c4e09":"code","eff592c9":"code","88c21939":"code","700f1e09":"code","1ce0eb8e":"code","8e479991":"code","11309a53":"code","6fddd2f1":"code","56fbfd6c":"code","a4ab3ca4":"code","5eb2e475":"code","0fdf42f6":"code","06dd78d5":"code","fbb6d9de":"code","a38f29df":"code","038c53a2":"code","463e209a":"code","35ba61d5":"code","ac43bcd9":"code","31892962":"code","7f7b043a":"code","d050b3a0":"code","5030edd3":"code","84e2934e":"code","7ef21f27":"code","bc0a4caa":"code","0beedeb4":"code","0a2fbadc":"code","c890d3e1":"code","ed5350ec":"code","c49d1bd8":"code","e4e1a5c5":"code","67302a2d":"code","658beedb":"code","86b1d3be":"code","55ac8436":"code","3cfdcc25":"code","757e054f":"code","b011779d":"code","37839493":"code","a7336284":"code","a88fca2a":"code","ddefacb8":"code","100382b1":"code","7245602a":"markdown","9fbe328a":"markdown","db7d41f7":"markdown","d679a95c":"markdown","a3e93551":"markdown","5d9d2ff1":"markdown","a73a1c9f":"markdown","530eb177":"markdown","3885c559":"markdown","60460228":"markdown","4cb78fd4":"markdown","0ea15930":"markdown","cc328d87":"markdown","662baf4c":"markdown","0045c8b3":"markdown","5145257f":"markdown","b2172a27":"markdown","5a1063d8":"markdown"},"source":{"6f726cd1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86d82b35":"from gensim.models import Word2Vec\nfrom gensim.models import KeyedVectors\nfrom tqdm import tqdm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport sqlite3\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gensim\nimport plotly.offline as pyoff\nimport plotly.graph_objs as go\nfrom nltk.util import ngrams\nimport re\nimport unicodedata\nimport nltk\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report\nfrom prettytable import PrettyTable\nfrom bs4 import BeautifulSoup\nfrom tqdm import tqdm\nimport re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords","ac92fc4f":"df = pd.read_csv('\/kaggle\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')\ndf.head()","352d91a6":"sns.countplot(df['Rating'])","a2acc55a":"df.isna().sum()","84ad93b2":"stop = set(stopwords.words('english'))","3e28a4ac":"def cleaner(phrase):\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can't\", 'can not', phrase)\n  \n  # general\n    phrase = re.sub(r\"n\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'re'\",\" are\", phrase)\n    phrase = re.sub(r\"\\'s\",\" is\", phrase)\n    phrase = re.sub(r\"\\'ll\",\" will\", phrase)\n    phrase = re.sub(r\"\\'d\",\" would\", phrase)\n    phrase = re.sub(r\"\\'t\",\" not\", phrase)\n    phrase = re.sub(r\"\\'ve\",\" have\", phrase)\n    phrase = re.sub(r\"\\'m\",\" am\", phrase)\n    \n    return phrase","0bd1409c":"cleaned_title = []\n\nfor sentance in tqdm(df['Review'].values):\n    sentance = str(sentance)\n    sentance = re.sub(r\"http\\S+\", \"\", sentance)\n    sentance = BeautifulSoup(sentance, 'lxml').get_text()\n    sentance = cleaner(sentance)\n    sentance = re.sub(r'[?|!|\\'|\"|#|+]', r'', sentance)\n    sentance = re.sub(\"\\S*\\d\\S*\", \"\", sentance).strip()\n    sentance = re.sub('[^A-Za-z]+', ' ', sentance)\n    sentance = ' '.join(e.lower() for e in sentance.split() if e.lower() not in stop)\n    cleaned_title.append(sentance.strip())","1654aec6":"df['Review'] = cleaned_title\ndf.head()","c5d88334":"from wordcloud import WordCloud,STOPWORDS","7c9c4e09":"ratings = [1,2,3,4,5]\nplt.ion()\n\nfor rating in ratings:\n    plt.figure(figsize = (20,20))\n    userdf = df[df['Rating'] == rating]\n    wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(userdf.Review))\n    plt.imshow(wc , interpolation = 'bilinear')\n    plt.title(rating)\n    plt.show()\n    plt.draw()\n    plt.pause(0.001)\n    plt.clf()","eff592c9":"def basic_clean(text):\n  \"\"\"\n  A simple function to clean up the data. All the words that\n  are not designated as a stop word is then lemmatized after\n  encoding and basic regex parsing are performed.\n  \"\"\"\n  wnl = nltk.stem.WordNetLemmatizer()\n  stopwords = nltk.corpus.stopwords.words('english')\n  text = (unicodedata.normalize('NFKD', text)\n    .encode('ascii', 'ignore')\n    .decode('utf-8', 'ignore')\n    .lower())\n  words = re.sub(r'[^\\w\\s]', '', text).split()\n  return [wnl.lemmatize(word) for word in words if word not in stopwords]","88c21939":"def plot_bigram(words):    \n    bigram_words=(pd.Series(nltk.ngrams(words, 2)).value_counts())[:30]\n    bigram_words=pd.DataFrame(bigram_words)\n    bigram_words['idx']=bigram_words.index\n    bigram_words['idx']=bigram_words.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+')',axis=1)\n    plot_data = [\n        go.Bar(\n            x=bigram_words['idx'],\n            y=bigram_words[0],\n            #name='True',\n            #x_axis=\"OTI\",\n            #y_axis=\"time\",\n            marker = dict(\n                color = 'Red'\n            )\n        )\n    ]\n    plot_layout = go.Layout(\n            title='Top 30 bi-grams',\n            yaxis_title='Count',\n            xaxis_title='bi-gram',\n            plot_bgcolor='rgba(0,0,0,0)'\n        )\n    fig = go.Figure(data=plot_data, layout=plot_layout)\n    pyoff.iplot(fig)","700f1e09":"reviews = basic_clean(''.join(str(df[df['Rating']==1]['Review'].tolist())))\nplot_bigram(reviews)","1ce0eb8e":"reviews = basic_clean(''.join(str(df[df['Rating']==5]['Review'].tolist())))\nplot_bigram(reviews)","8e479991":"def plot_trigram(words):    \n    trigram_words=(pd.Series(nltk.ngrams(words, 3)).value_counts())[:30]\n    trigram_words=pd.DataFrame(trigram_words)\n    trigram_words['idx']=trigram_words.index\n    trigram_words['idx']=trigram_words.apply(lambda x: '('+x['idx'][0]+', '+x['idx'][1]+', ' + x['idx'][2]+')',axis=1)\n    plot_data = [\n        go.Bar(\n            x=trigram_words['idx'],\n            y=trigram_words[0],\n            #name='True',\n            #x_axis=\"OTI\",\n            #y_axis=\"time\",\n            marker = dict(\n                color = 'Green'\n            )\n        )\n    ]\n    plot_layout = go.Layout(\n            title='Top 30 tri-grams',\n            yaxis_title='Count',\n            xaxis_title='Tri-gram',\n            plot_bgcolor='rgba(0,0,0,0)'\n        )\n    fig = go.Figure(data=plot_data, layout=plot_layout)\n    pyoff.iplot(fig)","11309a53":"reviews = basic_clean(''.join(str(df[df['Rating']==1]['Review'].tolist())))\nplot_trigram(reviews)","6fddd2f1":"reviews = basic_clean(''.join(str(df[df['Rating']==5]['Review'].tolist())))\nplot_trigram(reviews)","56fbfd6c":"reviews = basic_clean(''.join(str(df[df['Rating']==3]['Review'].tolist())))\nplot_bigram(reviews)","a4ab3ca4":"reviews = basic_clean(''.join(str(df[df['Rating']==2]['Review'].tolist())))\nplot_trigram(reviews)","5eb2e475":"X = df['Review']\ny = df['Rating']\n\nfrom sklearn.model_selection import train_test_split\n\nX_Train, X_test, y_Train, y_test = train_test_split(X, y, random_state=0, stratify=y, test_size=0.15)\nprint('Test set created')\nX_train, X_cross, y_train, y_cross = train_test_split(X_Train, y_Train, random_state=0, stratify=y_Train, test_size=0.15)\nprint('Cross val set Created')","0fdf42f6":"from sklearn.naive_bayes import MultinomialNB","06dd78d5":"tf_idf = TfidfVectorizer(ngram_range=(1,3))\ntf_idf.fit(X_train)\nTrain_TFIDF = tf_idf.transform(X_train)\nCrossVal_TFIDF = tf_idf.transform(X_cross)\nTest_TFIDF= tf_idf.transform(X_test)","fbb6d9de":"alpha_set=[0.00001,0.0001,0.001,0.01,0.1,1.0,10.0,100.0,1000.0,10000.0]\nTrain_AUC_TFIDF = []\nCrossVal_AUC_TFIDF = []\nfor i in alpha_set:\n  naive_b=MultinomialNB(alpha=i)\n  naive_b.fit(Train_TFIDF, y_train)\n  Train_y_pred =  naive_b.predict_proba(Train_TFIDF)[0:,]\n  Train_AUC_TFIDF.append(roc_auc_score(y_train,Train_y_pred, multi_class='ovr'))\n  CrossVal_y_pred =  naive_b.predict_proba(CrossVal_TFIDF)[0:,]\n  CrossVal_AUC_TFIDF.append(roc_auc_score(y_cross,CrossVal_y_pred, multi_class='ovr'))","a38f29df":"Alpha_set=[]\nfor i in range(len(alpha_set)):\n  Alpha_set.append(np.math.log(alpha_set[i]))","038c53a2":"plt.plot(Alpha_set, Train_AUC_TFIDF, label='Train AUC')\nplt.scatter(Alpha_set, Train_AUC_TFIDF)\nplt.plot(Alpha_set, CrossVal_AUC_TFIDF, label='CrossVal AUC')\nplt.scatter(Alpha_set, CrossVal_AUC_TFIDF)\nplt.legend()\nplt.xlabel(\"alpha : hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()","463e209a":"optimal_alpha=alpha_set[CrossVal_AUC_TFIDF.index(max(CrossVal_AUC_TFIDF))]\nprint(optimal_alpha)\n\nClassifier=MultinomialNB(alpha=optimal_alpha)\nClassifier.fit(Train_TFIDF, y_train)","35ba61d5":"auc_train_tfidf = roc_auc_score(y_train,Classifier.predict_proba(Train_TFIDF)[0:,], multi_class='ovr')\nprint (\"AUC for Train set\", auc_train_tfidf)\n\nauc_test_tfidf = roc_auc_score(y_test,Classifier.predict_proba(Test_TFIDF)[0:,], multi_class='ovr')\nprint (\"AUC for Test set\",auc_test_tfidf)","ac43bcd9":"print('Confusion Matrix of Train Data')\nTrain_mat=confusion_matrix(y_train,Classifier.predict(Train_TFIDF))\nprint (Train_mat)","31892962":"print('Confusion Matrix of Test Data')\nTest_mat=confusion_matrix(y_test,Classifier.predict(Test_TFIDF))\nprint (Test_mat)","7f7b043a":"from sklearn.metrics import accuracy_score, f1_score\n\nacc = accuracy_score(y_test,Classifier.predict(Test_TFIDF))\n\nf1 = f1_score(y_test,Classifier.predict(Test_TFIDF), average='macro')\n\nprint ('Accuracy is : ', acc)\nprint ('F1 Score is :', f1)","d050b3a0":"count_vect = CountVectorizer()\ncount_vect.fit(X_train)\nX_train_counts = count_vect.transform(X_train)\nX_cross_counts = count_vect.transform(X_cross)\nX_test_counts = count_vect.transform(X_test)","5030edd3":"alpha_set=[0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\nTrain_AUC_BOW = []\nCrossVal_AUC_BOW = []\nfor i in alpha_set:\n  naive_b=MultinomialNB(alpha=i)\n  naive_b.fit(X_train_counts, y_train)\n  Train_y_pred =  naive_b.predict_proba(X_train_counts)[0:,]\n  Train_AUC_BOW.append(roc_auc_score(y_train,Train_y_pred, multi_class='ovr'))\n  CrossVal_y_pred =  naive_b.predict_proba(X_cross_counts)[0:,]\n  CrossVal_AUC_BOW.append(roc_auc_score(y_cross,CrossVal_y_pred, multi_class='ovr'))","84e2934e":"plt.plot(Alpha_set, Train_AUC_BOW, label='Train AUC')\nplt.scatter(Alpha_set, Train_AUC_BOW)\nplt.plot(Alpha_set, CrossVal_AUC_BOW, label='CrossVal AUC')\nplt.scatter(Alpha_set, CrossVal_AUC_BOW)\nplt.legend()\nplt.xlabel(\"alpha : hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()","7ef21f27":"optimal_alpha=alpha_set[CrossVal_AUC_BOW.index(max(CrossVal_AUC_BOW))]\nprint(optimal_alpha)\n\nClassifier1=MultinomialNB(alpha=optimal_alpha)\nClassifier1.fit(X_train_counts, y_train)","bc0a4caa":"auc_train_bow = roc_auc_score(y_train,Classifier1.predict_proba(X_train_counts)[0:,], multi_class='ovr')\nprint (\"AUC for Train set\", auc_train_bow)\n\nauc_test_bow = roc_auc_score(y_test,Classifier1.predict_proba(X_test_counts)[0:,], multi_class='ovr')\nprint (\"AUC for Test set\",auc_test_bow)","0beedeb4":"print('Confusion Matrix of Test Data')\nTest_mat=confusion_matrix(y_test,Classifier1.predict(X_test_counts))\nprint (Test_mat)","0a2fbadc":"recall = np.diag(Test_mat) \/ np.sum(Test_mat, axis = 1)\nprecision = np.diag(Test_mat) \/ np.sum(Test_mat, axis = 0)","c890d3e1":"c = np.unique(df['Rating'])\n\nplt.plot(c,precision, label='precision')\nplt.scatter(c,precision)\nplt.plot(c,recall, label='recall')\nplt.scatter(c,recall)\nplt.plot(c,2*(precision * recall) \/ (precision + recall) , label='F1 score')\nplt.scatter(c,2*(precision * recall) \/ (precision + recall))\nplt.legend()\nplt.xlabel(\"Categories \")\nplt.ylabel(\"Scores\")\nplt.title(\"PLOTS\")\n\nplt.show()","ed5350ec":"y_train.value_counts()","c49d1bd8":"from imblearn.over_sampling import SMOTE\nstrategy = {1:5000, 2:5000, 3:5000, 4:5000, 5:6541}\nsmote = SMOTE(sampling_strategy=strategy)\n\nX_sm, y_sm = smote.fit_resample(Train_TFIDF,y_train)\nX_test_sm, y_test_sm = smote.fit_resample(Test_TFIDF,y_test)\nX_cross_sm, y_cross_sm = smote.fit_resample(CrossVal_TFIDF,y_cross)","e4e1a5c5":"y_sm.value_counts()","67302a2d":"alpha_set=[0.00001,0.0001,0.001,0.01,0.1,1,10,100,1000,10000]\nTrain_AUC_BOW = []\nCrossVal_AUC_BOW = []\nfor i in alpha_set:\n  naive_b=MultinomialNB(alpha=i)\n  naive_b.fit(X_sm, y_sm)\n  Train_y_pred =  naive_b.predict_proba(X_sm)[0:,]\n  Train_AUC_BOW.append(roc_auc_score(y_sm,Train_y_pred, multi_class='ovr'))\n  CrossVal_y_pred =  naive_b.predict_proba(X_cross_sm)[0:,]\n  CrossVal_AUC_BOW.append(roc_auc_score(y_cross_sm,CrossVal_y_pred, multi_class='ovr'))","658beedb":"plt.plot(Alpha_set, Train_AUC_BOW, label='Train AUC')\nplt.scatter(Alpha_set, Train_AUC_BOW)\nplt.plot(Alpha_set, CrossVal_AUC_BOW, label='CrossVal AUC')\nplt.scatter(Alpha_set, CrossVal_AUC_BOW)\nplt.legend()\nplt.xlabel(\"alpha : hyperparameter\")\nplt.ylabel(\"AUC\")\nplt.title(\"ERROR PLOTS\")\nplt.show()","86b1d3be":"optimal_alpha=alpha_set[CrossVal_AUC_TFIDF.index(max(CrossVal_AUC_TFIDF))]\nprint(optimal_alpha)\n\nClassifier=MultinomialNB(alpha=optimal_alpha)\nClassifier.fit(X_sm, y_sm)","55ac8436":"print('Confusion Matrix of Test Data')\nTest_mat=confusion_matrix(y_test_sm,Classifier.predict(X_test_sm))\nprint (Test_mat)","3cfdcc25":"acc = accuracy_score(y_test_sm,Classifier.predict(X_test_sm))\nf1 = f1_score(y_test_sm,Classifier.predict(X_test_sm), average='macro')\n\nprint ('Accuracy is : ', acc)\nprint ('F1 Score is :', f1)","757e054f":"auc_train_bow = roc_auc_score(y_sm,Classifier.predict_proba(X_sm)[0:,], multi_class='ovr')\nprint (\"AUC for Train set\", auc_train_bow)\n\nauc_test_bow = roc_auc_score(y_test_sm,Classifier.predict_proba(X_test_sm)[0:,], multi_class='ovr')\nprint (\"AUC for Test set\",auc_test_bow)","b011779d":"print('Confusion Matrix of Test Data')\nTest_mat=confusion_matrix(y_test_sm,Classifier.predict(X_test_sm))\nprint (Test_mat)","37839493":"recall = np.diag(Test_mat) \/ np.sum(Test_mat, axis = 1)\nprecision = np.diag(Test_mat) \/ np.sum(Test_mat, axis = 0)\n\nc = np.unique(df['Rating'])\n\nplt.plot(c,precision, label='precision')\nplt.scatter(c,precision)\nplt.plot(c,recall, label='recall')\nplt.scatter(c,recall)\nplt.plot(c,2*(precision * recall) \/ (precision + recall) , label='F1 score')\nplt.scatter(c,2*(precision * recall) \/ (precision + recall))\nplt.legend()\nplt.xlabel(\"Categories \")\nplt.ylabel(\"Scores\")\nplt.title(\"PLOTS\")\n\nplt.show()","a7336284":"from sklearn.ensemble import RandomForestClassifier\n\n# define the model\nmodel = RandomForestClassifier(n_estimators=1000, class_weight='balanced', max_depth=50, bootstrap=True,max_features='sqrt')\nmodel.fit(Train_TFIDF, y_train)\ny_pred = model.predict(Test_TFIDF)","a88fca2a":"# Training accuracy\nprint(\"The training accuracy is: \")\nprint(accuracy_score(y_train, model.predict(Train_TFIDF)))","ddefacb8":"# Test accuracy\nprint(\"The test accuracy is: \")\nprint(accuracy_score(y_test, y_pred))","100382b1":"print(\"Confusion Matrix\")\nprint(confusion_matrix(y_test, y_pred))\n\n\n# Classification report\nprint(\"Classification report\")\nprint(classification_report(y_test, y_pred))","7245602a":"### Let's try using the SMOTE algorithm for oversampling the low population classes","9fbe328a":"Tha dataset seems to be **highly imbalanced**, this might turn out to be a problem in model building, we will have to take steps which can make the model balanced.\n\nOr we can try to form 3 classes:\n#### 1. Negative - Class 1 or 2\n#### 2. Neutral - Class 3\n#### 3. Positive - Class 4 or 5","db7d41f7":"### Data Cleaning","d679a95c":"1. The tri-grams are able to capture the sentiment better, we can try having the ngram range (3,3)","a3e93551":"### Please leave a comment and upvote the notebook.\n### Thanks.\n### Happy Learning","5d9d2ff1":"#### 1. Naive bayes algorithm","a73a1c9f":"### About SMOTE\n\nSMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line.\n\nSpecifically, a random example from the minority class is first chosen. Then k of the nearest neighbors for that example are found (typically k=5). A randomly selected neighbor is chosen and a synthetic example is created at a randomly selected point between the two examples in feature space.\n\nA general downside of the approach is that synthetic examples are created without considering the majority class, possibly resulting in ambiguous examples if there is a strong overlap for the classes.","530eb177":"### Model Building","3885c559":"#### Let us find the bigrams and trigrams for the reviews","60460228":"### EDA","4cb78fd4":"### 1. Creating WordClouds","0ea15930":"We see that the model is able to predict classes for 'Class 5,4' and 'Class 1,2' without much difficulty.\nIt seems that the predictions for class 3 were not easily captured by the model.\n\nIf we try the Random Forests let's see what is the performance of our model.","cc328d87":"1. We see that the bigrams for Postive class are highly representative","662baf4c":"1. The bigrams like (worst, hotel), (room, service) do provide us a good analysis of the dataset\n2. Punta Cana is a town in Dominican Republic ","0045c8b3":"Using BOW vectorizer","5145257f":"As predicted before, the model fails for the neutral class (3) since it is not able to capture the true relationship. This is where naive bayes fails since it provides prior probabilities to all words in the corpus for each class.\n\nIt fails to capture the semantic relationship.","b2172a27":"As we can see, the random forests too is not able to capture the sentiment for the neutral class, we now have to use more advanced models like 'Boosting' or \"Neural Networks\".","5a1063d8":"Using TF-IDF Vectorizer"}}