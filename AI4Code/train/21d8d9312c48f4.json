{"cell_type":{"c54a7ac4":"code","295fa42e":"code","f885b810":"code","aa0a0f2a":"code","32dc57d9":"code","3dab06da":"code","3b76959a":"code","34b6bfe5":"code","17e58ed6":"code","0e0e67b8":"code","49c5f553":"code","64c88347":"code","6cc6c18b":"code","46cd6e45":"code","84b6ddc0":"code","90832eb0":"code","a1315dd5":"code","c7c1fb62":"code","357938df":"code","0ce61938":"code","ae599af8":"code","8ded2131":"code","368679a6":"code","0c24166d":"code","cf283005":"code","7d03ad1b":"code","026dd20c":"code","90b3b226":"code","faa93e78":"code","98d17265":"code","ea88e2db":"code","78a4a417":"code","06c30b37":"code","cf09401b":"code","5b656c5a":"code","3d2a2d1a":"code","3403212d":"code","0d5e2edb":"code","c49c8876":"code","0a827429":"code","ba05bf76":"code","747508cc":"code","900f756a":"code","bb9459cc":"markdown","1ec26dea":"markdown","e24447d9":"markdown","761c2fea":"markdown","ed013d6e":"markdown","b3ce9dde":"markdown","c2e598ac":"markdown","8137c678":"markdown","f2f1876d":"markdown","0986812f":"markdown","18662749":"markdown","ffa79157":"markdown","f675d9df":"markdown","db54708c":"markdown","1f84395b":"markdown","1aec85b6":"markdown","50c544ec":"markdown","6f1155ba":"markdown","5b458a24":"markdown","3d3dd1bf":"markdown","f5204ab7":"markdown"},"source":{"c54a7ac4":"import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport os\nimport random\nimport re\nfrom PIL import Image\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pylab import *\nimport os\nimport sys\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.layers import *\n\n\n#from tensorflow.keras.engine import Layer\nfrom tensorflow.keras.applications.vgg16 import *\nfrom tensorflow.keras.models import *\n#from tensorflow.keras.applications.imagenet_utils import _obtain_input_shape\nimport tensorflow.keras.backend as K\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Convolution2D, ZeroPadding2D, MaxPooling2D, Cropping2D, Conv2D\nfrom tensorflow.keras.layers import Input, Add, Dropout, Permute, add\nfrom tensorflow.compat.v1.layers import conv2d_transpose\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.python.keras.callbacks import TensorBoard, ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","295fa42e":"print(tf.__version__)","f885b810":"x = tf.random.uniform([3, 3])\n\nprint(\"Is there a GPU available: \"),\nprint(tf.test.is_gpu_available())\n\nprint(\"Is the Tensor on GPU #0:  \"),\nprint(x.device.endswith('GPU:0'))\n\nprint(\"Device name: {}\".format((x.device)))","aa0a0f2a":"print(tf.executing_eagerly())","32dc57d9":"!wget http:\/\/mi.eng.cam.ac.uk\/research\/projects\/VideoRec\/CamSeq01\/CamSeq01.zip","3dab06da":"cwd = os.getcwd()\ncwd","3b76959a":"!mkdir data\n!mkdir data\/CamSeq01","34b6bfe5":"!unzip CamSeq01.zip -d data\/CamSeq01","17e58ed6":"def _read_to_tensor(fname, output_height=224, output_width=224, normalize_data=False):\n    '''Function to read images from given image file path, and provide resized images as tensors\n        Inputs: \n            fname - image file path\n            output_height - required output image height\n            output_width - required output image width\n            normalize_data - if True, normalize data to be centered around 0 (mean 0, range 0 to 1)\n        Output: Processed image tensors\n    '''\n    \n    # Read the image as a tensor\n    img_strings = tf.io.read_file(fname)\n    imgs_decoded = tf.image.decode_jpeg(img_strings)\n    \n    # Resize the image\n    output = tf.image.resize(imgs_decoded, [output_height, output_width])\n    \n    # Normalize if required\n    if normalize_data:\n        output = (output - 128) \/ 128\n    return output","0e0e67b8":"img_dir = 'data\/CamSeq01\/'\n\n# Required image dimensions\noutput_height = 224\noutput_width = 224","49c5f553":"def read_images(img_dir):\n    '''Function to get all image directories, read images and masks in separate tensors\n        Inputs: \n            img_dir - file directory\n        Outputs \n            frame_tensors, masks_tensors, frame files list, mask files list\n    '''\n    \n    # Get the file names list from provided directory\n    file_list = [f for f in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, f))]\n    \n    # Separate frame and mask files lists, exclude unnecessary files\n    frames_list = [file for file in file_list if ('_L' not in file) and ('txt' not in file)]\n    masks_list = [file for file in file_list if ('_L' in file) and ('txt' not in file)]\n    \n    frames_list.sort()\n    masks_list.sort()\n    \n    print('{} frame files found in the provided directory.'.format(len(frames_list)))\n    print('{} mask files found in the provided directory.'.format(len(masks_list)))\n    \n    # Create file paths from file names\n    frames_paths = [os.path.join(img_dir, fname) for fname in frames_list]\n    masks_paths = [os.path.join(img_dir, fname) for fname in masks_list]\n    \n    # Create dataset of tensors\n    frame_data = tf.data.Dataset.from_tensor_slices(frames_paths)\n    masks_data = tf.data.Dataset.from_tensor_slices(masks_paths)\n    \n    # Read images into the tensor dataset\n    frame_tensors = frame_data.map(_read_to_tensor)\n    masks_tensors = masks_data.map(_read_to_tensor)\n    \n    print('Completed importing {} frame images from the provided directory.'.format(len(frames_list)))\n    print('Completed importing {} mask images from the provided directory.'.format(len(masks_list)))\n    \n    return frame_tensors, masks_tensors, frames_list, masks_list\n\nframe_tensors, masks_tensors, frames_list, masks_list = read_images(img_dir)","64c88347":"# Make an iterator to extract images from the tensor dataset\n\nframe_batches = tf.compat.v1.data.make_one_shot_iterator(frame_tensors)  # outside of TF Eager, we would use make_one_shot_iterator\nmask_batches = tf.compat.v1.data.make_one_shot_iterator(masks_tensors)\n\n","6cc6c18b":"n_images_to_show = 5\n\nfor i in range(n_images_to_show):\n    \n    # Get the next image from iterator\n    frame = frame_batches.next().numpy().astype(np.uint8)\n    mask = mask_batches.next().numpy().astype(np.uint8)\n    \n    #Plot the corresponding frames and masks\n    fig = plt.figure(figsize=(10,8))\n    fig.add_subplot(1,2,1)\n    plt.grid(b = None)\n    plt.imshow(frame)\n    fig.add_subplot(1,2,2)\n    plt.imshow(mask)\n    plt.grid(b = None)\n    plt.show()","46cd6e45":"DATA_PATH = 'data\/CamSeq01\/'\n\n# Create folders to hold images and masks\n\nfolders = ['train_frames\/train', 'train_masks\/train', 'val_frames\/val', 'val_masks\/val']\n\n\nfor folder in folders:\n    try:\n        os.makedirs(DATA_PATH + folder)\n    except Exception as e: print(e)","84b6ddc0":"def generate_image_folder_structure(frames, masks, frames_list, masks_list):\n    '''Function to save images in the appropriate folder directories \n        Inputs: \n            frames - frame tensor dataset\n            masks - mask tensor dataset\n            frames_list - frame file paths\n            masks_list - mask file paths\n    '''\n    #Create iterators for frames and masks\n    frame_batches = tf.compat.v1.data.make_one_shot_iterator(frames)  # outside of TF Eager, we would use make_one_shot_iterator\n    mask_batches = tf.compat.v1.data.make_one_shot_iterator(masks)\n    \n    #Iterate over the train images while saving the frames and masks in appropriate folders\n    dir_name='train'\n    for file in zip(frames_list[:-round(0.2*len(frames_list))],masks_list[:-round(0.2*len(masks_list))]):\n        \n        \n        #Convert tensors to numpy arrays\n        frame = frame_batches.next().numpy().astype(np.uint8)\n        mask = mask_batches.next().numpy().astype(np.uint8)\n        \n        #Convert numpy arrays to images\n        frame = Image.fromarray(frame)\n        mask = Image.fromarray(mask)\n        \n        #Save frames and masks to correct directories\n        frame.save(DATA_PATH+'{}_frames\/{}'.format(dir_name,dir_name)+'\/'+file[0])\n        mask.save(DATA_PATH+'{}_masks\/{}'.format(dir_name,dir_name)+'\/'+file[1])\n    \n    #Iterate over the val images while saving the frames and masks in appropriate folders\n    dir_name='val'\n    for file in zip(frames_list[-round(0.2*len(frames_list)):],masks_list[-round(0.2*len(masks_list)):]):\n        \n        \n        #Convert tensors to numpy arrays\n        frame = frame_batches.next().numpy().astype(np.uint8)\n        mask = mask_batches.next().numpy().astype(np.uint8)\n        \n        #Convert numpy arrays to images\n        frame = Image.fromarray(frame)\n        mask = Image.fromarray(mask)\n        \n        #Save frames and masks to correct directories\n        frame.save(DATA_PATH+'{}_frames\/{}'.format(dir_name,dir_name)+'\/'+file[0])\n        mask.save(DATA_PATH+'{}_masks\/{}'.format(dir_name,dir_name)+'\/'+file[1])\n    \n    print(\"Saved {} frames to directory {}\".format(len(frames_list),DATA_PATH))\n    print(\"Saved {} masks to directory {}\".format(len(masks_list),DATA_PATH))\n    \ngenerate_image_folder_structure(frame_tensors, masks_tensors, frames_list, masks_list)\n\n#generate_image_folder_structure(train_frames, train_masks, val_files, 'val')","90832eb0":"def parse_code(l):\n    '''Function to parse lines in a text file, returns separated elements (label codes and names in this case)\n    '''\n    if len(l.strip().split(\"\\t\")) == 2:\n        a, b = l.strip().split(\"\\t\")\n        return tuple(int(i) for i in a.split(' ')), b\n    else:\n        a, b, c = l.strip().split(\"\\t\")\n        return tuple(int(i) for i in a.split(' ')), c","a1315dd5":"label_codes, label_names = zip(*[parse_code(l) for l in open(img_dir+\"label_colors.txt\")])\nlabel_codes, label_names = list(label_codes), list(label_names)\nlabel_codes[:5], label_names[:5]","c7c1fb62":"code2id = {v:k for k,v in enumerate(label_codes)}\nid2code = {k:v for k,v in enumerate(label_codes)}","357938df":"name2id = {v:k for k,v in enumerate(label_names)}\nid2name = {k:v for k,v in enumerate(label_names)}","0ce61938":"def rgb_to_onehot(rgb_image, colormap = id2code):\n    '''Function to one hot encode RGB mask labels\n        Inputs: \n            rgb_image - image matrix (eg. 256 x 256 x 3 dimension numpy ndarray)\n            colormap - dictionary of color to label id\n        Output: One hot encoded image of dimensions (height x width x num_classes) where num_classes = len(colormap)\n    '''\n    num_classes = len(colormap)\n    shape = rgb_image.shape[:2]+(num_classes,)\n    encoded_image = np.zeros( shape, dtype=np.int8 )\n    for i, cls in enumerate(colormap):\n        encoded_image[:,:,i] = np.all(rgb_image.reshape( (-1,3) ) == colormap[i], axis=1).reshape(shape[:2])\n    return encoded_image\n\n\ndef onehot_to_rgb(onehot, colormap = id2code):\n    '''Function to decode encoded mask labels\n        Inputs: \n            onehot - one hot encoded image matrix (height x width x num_classes)\n            colormap - dictionary of color to label id\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    single_layer = np.argmax(onehot, axis=-1)\n    output = np.zeros( onehot.shape[:2]+(3,) )\n    for k in colormap.keys():\n        output[single_layer==k] = colormap[k]\n    return np.uint8(output)","ae599af8":"# Normalizing only frame images, since masks contain label info\ndata_gen_args = dict(rescale=1.\/255)\nmask_gen_args = dict()\n\ntrain_frames_datagen = ImageDataGenerator(**data_gen_args)\ntrain_masks_datagen = ImageDataGenerator(**mask_gen_args)\nval_frames_datagen = ImageDataGenerator(**data_gen_args)\nval_masks_datagen = ImageDataGenerator(**mask_gen_args)\n\n# Seed defined for aligning images and their masks\nseed = 1","8ded2131":"def TrainAugmentGenerator(seed = 1, batch_size = 5):\n    '''Train Image data generator\n        Inputs: \n            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n            batch_size - number of images to import at a time\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    train_image_generator = train_frames_datagen.flow_from_directory(\n    DATA_PATH + 'train_frames\/',\n    batch_size = batch_size, seed = seed, target_size = (224, 224))\n\n    train_mask_generator = train_masks_datagen.flow_from_directory(\n    DATA_PATH + 'train_masks\/',\n    batch_size = batch_size, seed = seed, target_size = (224, 224))\n\n    while True:\n        X1i = train_image_generator.next()\n        X2i = train_mask_generator.next()\n        \n        #One hot encoding RGB images\n        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n        \n        yield X1i[0], np.asarray(mask_encoded)\n\ndef ValAugmentGenerator(seed = 1, batch_size = 5):\n    '''Validation Image data generator\n        Inputs: \n            seed - seed provided to the flow_from_directory function to ensure aligned data flow\n            batch_size - number of images to import at a time\n        Output: Decoded RGB image (height x width x 3) \n    '''\n    val_image_generator = val_frames_datagen.flow_from_directory(\n    DATA_PATH + 'val_frames\/',\n    batch_size = batch_size, seed = seed, target_size = (224, 224))\n\n\n    val_mask_generator = val_masks_datagen.flow_from_directory(\n    DATA_PATH + 'val_masks\/',\n    batch_size = batch_size, seed = seed, target_size = (224, 224))\n\n\n    while True:\n        X1i = val_image_generator.next()\n        X2i = val_mask_generator.next()\n        \n        #One hot encoding RGB images\n        mask_encoded = [rgb_to_onehot(X2i[0][x,:,:,:], id2code) for x in range(X2i[0].shape[0])]\n        \n        yield X1i[0], np.asarray(mask_encoded)\n        \n","368679a6":"from tqdm import tqdm","0c24166d":"from tqdm import tqdm_gui","cf283005":"from tensorflow.keras.models import *\nfrom tensorflow.keras.layers import *\n\n\nimport os\nVGG_Weights_path = \"pretrained_weights\/vgg16_weights_tf_dim_ordering_tf_kernels.h5\"\n\n\ndef VGGSegnet( n_classes ,  input_height=224, input_width=224 , vgg_level=3):\n\n    img_input = Input(shape=(input_height,input_width,3))\n\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1', data_format='channels_last' )(img_input)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2', data_format='channels_last' )(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool', data_format='channels_last' )(x)\n    f1 = x\n    # Block 2\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1', data_format='channels_last' )(x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2', data_format='channels_last' )(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool', data_format='channels_last' )(x)\n    f2 = x\n\n    # Block 3\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1', data_format='channels_last' )(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2', data_format='channels_last' )(x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3', data_format='channels_last' )(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool', data_format='channels_last' )(x)\n    f3 = x\n\n    # Block 4\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1', data_format='channels_last' )(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2', data_format='channels_last' )(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3', data_format='channels_last' )(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool', data_format='channels_last' )(x)\n    f4 = x\n\n    # Block 5\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1', data_format='channels_last' )(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2', data_format='channels_last' )(x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3', data_format='channels_last' )(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool', data_format='channels_last' )(x)\n    f5 = x\n\n    x = Flatten(name='flatten')(x)\n    x = Dense(4096, activation='relu', name='fc1')(x)\n    x = Dense(4096, activation='relu', name='fc2')(x)\n    x = Dense( 1000 , activation='softmax', name='predictions')(x)\n\n    vgg  = Model(  img_input , x  )\n    vgg.load_weights(VGG_Weights_path)\n\n    levels = [f1 , f2 , f3 , f4 , f5]\n\n    o = levels[ vgg_level ]\n\n    o = ( ZeroPadding2D( (1,1) , data_format='channels_last' ))(o)\n    o = ( Conv2D(512, (3, 3), padding='valid', data_format='channels_last'))(o)\n    o = ( BatchNormalization())(o)\n    \n    o = ( UpSampling2D( (2,2), data_format='channels_last'))(o)\n    o = ( ZeroPadding2D( (1,1), data_format='channels_last'))(o)\n    o = ( Conv2D( 512, (3, 3), padding='valid', data_format='channels_last'))(o)\n    o = ( BatchNormalization())(o)\n    \n    o = ( UpSampling2D( (2,2), data_format='channels_last'))(o)\n    o = ( ZeroPadding2D( (1,1), data_format='channels_last'))(o)\n    o = ( Conv2D( 256, (3, 3), padding='valid', data_format='channels_last'))(o)\n    o = ( BatchNormalization())(o)\n\n    o = ( UpSampling2D((2,2)  , data_format='channels_last' ) )(o)\n    o = ( ZeroPadding2D((1,1) , data_format='channels_last' ))(o)\n    o = ( Conv2D( 128 , (3, 3), padding='valid' , data_format='channels_last' ))(o)\n    o = ( BatchNormalization())(o)\n\n    o = ( UpSampling2D((2,2)  , data_format='channels_last' ))(o)\n    o = ( ZeroPadding2D((1,1)  , data_format='channels_last' ))(o)\n    o = ( Conv2D( 64 , (3, 3), padding='valid'  , data_format='channels_last' ))(o)\n    o = ( BatchNormalization())(o)\n    \n    \n\n    o =  Conv2D( n_classes , (3, 3) , padding='same', data_format='channels_last' )( o )\n    o = (Activation('softmax'))(o)\n    model = Model( img_input , o )\n\n\n    return model\n","7d03ad1b":"os.getcwd()\n!mkdir pretrained_weights","026dd20c":"!wget -O pretrained_weights\/vgg16_weights_tf_dim_ordering_tf_kernels.h5  https:\/\/github.com\/fchollet\/deep-learning-models\/releases\/download\/v0.1\/vgg16_weights_tf_dim_ordering_tf_kernels.h5","90b3b226":"model = VGGSegnet( 32  , vgg_level=3)\n","faa93e78":"model.summary()","98d17265":"\ndef tversky_loss(y_true, y_pred):\n    alpha = 0.5\n    beta  = 0.5\n    \n    ones = K.ones(K.shape(y_true))\n    p0 = y_pred      # proba that voxels are class i\n    p1 = ones-y_pred # proba that voxels are not class i\n    g0 = y_true\n    g1 = ones-y_true\n    \n    num = K.sum(p0*g0, (0,1,2,3))\n    den = num + alpha*K.sum(p0*g1,(0,1,2,3)) + beta*K.sum(p1*g0,(0,1,2,3))\n    \n    T = K.sum(num\/den) # when summing over classes, T has dynamic range [0 Ncl]\n    \n    Ncl = K.cast(K.shape(y_true)[-1], 'float32')\n    return Ncl-T\n\n\ndef tversky(y_true, y_pred, smooth=1):\n    y_true_pos = K.flatten(y_true)\n    y_pred_pos = K.flatten(y_pred)\n    true_pos = K.sum(y_true_pos * y_pred_pos)\n    false_neg = K.sum(y_true_pos * (1-y_pred_pos))\n    false_pos = K.sum((1-y_true_pos)*y_pred_pos)\n    alpha = 0.7\n    return (true_pos + smooth)\/(true_pos + alpha*false_neg + (1-alpha)*false_pos + smooth)\n\n\n\ndef focal_tversky_loss_r(y_true,y_pred):\n    pt_1 = tversky(y_true, y_pred)\n    gamma = 0.75\n    return K.pow((1-pt_1), gamma)","ea88e2db":"@tf.function()\ndef dice_coef(y_true, y_pred):\n    mask =  tf.equal(y_true, 255)\n    mask = tf.logical_not(mask)\n    y_true = tf.boolean_mask(y_true, mask)\n    y_pred = tf.boolean_mask(y_pred, mask)\n    \n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) \/ (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1.-dice_coef(y_true, y_pred)","78a4a417":"smooth = 1.","06c30b37":"model.compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[dice_coef,'accuracy',tversky])\n","cf09401b":"# tb = TensorBoard(log_dir='logs', write_graph=True)\nmc = ModelCheckpoint(mode='max', filepath='camvid_model_vgg16_segnet_checkpoint.h5', monitor='accuracy', save_best_only='True', save_weights_only='True', verbose=1)\n# es = EarlyStopping(mode='min', monitor='val_loss', patience=4, verbose=1)\ncallbacks = [mc]","5b656c5a":"batch_size = 5\nsteps_per_epoch = np.ceil(float(len(frames_list) - round(0.2*len(frames_list))) \/ float(batch_size))\nsteps_per_epoch","3d2a2d1a":"validation_steps = (float((round(0.2*len(frames_list)))) \/ float(batch_size))\nvalidation_steps","3403212d":"num_epochs = 65","0d5e2edb":"# Train model\n\nbatch_size = 5\nresult = model.fit_generator(TrainAugmentGenerator(), steps_per_epoch=steps_per_epoch ,\n                validation_data = ValAugmentGenerator(), \n                validation_steps = validation_steps, epochs=num_epochs, verbose=1,callbacks = callbacks )\n# model.save_weights(\"camvid_model_vgg16_segnet.h5\", overwrite=True)","c49c8876":"# Get actual number of epochs model was trained for\nN = len(result.history['loss'])\n\n#Plot the model evaluation history\nplt.style.use(\"ggplot\")\nfig = plt.figure(figsize=(20,8))\n\nfig.add_subplot(1,2,1)\nplt.title(\"Training Loss\")\nplt.plot(np.arange(0, N), result.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, N), result.history[\"val_loss\"], label=\"val_loss\")\nplt.ylim(0, 1)\n\nfig.add_subplot(1,2,2)\nplt.title(\"Training Accuracy\")\nplt.plot(np.arange(0, N), result.history[\"accuracy\"], label=\"train_accuracy\")\nplt.plot(np.arange(0, N), result.history[\"val_accuracy\"], label=\"val_accuracy\")\nplt.ylim(0, 1)\n\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend(loc=\"lower left\")\nplt.show()","0a827429":"training_gen = TrainAugmentGenerator()\ntesting_gen = ValAugmentGenerator()","ba05bf76":"\nbatch_img,batch_mask = next(testing_gen)\npred_all= model.predict(batch_img)\nnp.shape(pred_all)\n","747508cc":"for i in range(0,np.shape(pred_all)[0]):\n    \n    fig,ax = plt.subplots(1,2,figsize=(10,8))\n    \n    ax[0].imshow(batch_img[i])\n    ax[0].title.set_text('Actual frame')\n    ax[0].grid(b=None)\n    ax[0].imshow(onehot_to_rgb(batch_mask[i],id2code),alpha = 0.6)\n    \n    ax[1].set_title('Predicted frames')\n    ax[1].imshow(batch_img[i])\n    ax[1].imshow(onehot_to_rgb(pred_all[i],id2code),alpha = 0.6)\n    ax[1].grid(b=None)\n    plt.show()","900f756a":"!rm -rf .\/*","bb9459cc":"### Function to create SegNet model using VGG-16 pre-trained weights\n","1ec26dea":"### Parse and extract label names and codes","e24447d9":"### Function to import and process frames and masks as tensors","761c2fea":"### Saving frames and masks to correct directories","ed013d6e":"### Function to parse the file \"label_colors.txt\" which contains the class definitions","b3ce9dde":"## Train and save the U-Net model","c2e598ac":"## Data preparation - Importing, Cleaning and Creating structured directory ","8137c678":"### Creating folder structure common for Computer Vision problems","f2f1876d":"### Extract and display model frame, prediction and mask batch","0986812f":"### Displaying Images in the train dataset","18662749":"# Creating custom Image data generators","ffa79157":"# Model Evaluation","f675d9df":"### Model evaluation historical plots","db54708c":"### Defining data generators","1f84395b":"### Define functions for one hot encoding rgb labels, and decoding encoded predictions","1aec85b6":"### Image directory and size parameters","50c544ec":"## Defining dice co-efficients for model performance","6f1155ba":"### Reading frames and masks\n- Mask file names end in \"\\_L.png\"\n","5b458a24":"with albummentation image mask augumentation  on training up to 150 epochs model can be improved very much","3d3dd1bf":"## Extract Target Class definitions","f5204ab7":"### Custom image data generators for creating batches of frames and masks"}}