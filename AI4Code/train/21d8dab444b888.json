{"cell_type":{"3f8b06e3":"code","8b1605ee":"code","027898ef":"code","f77ce676":"code","ef8c4c9b":"code","c683a304":"code","9ae87520":"code","c8b5f5f9":"code","a9e76f9a":"code","5fab5dc3":"code","9d77c2b0":"code","8466df5d":"code","36ae5742":"code","6378964b":"code","6158294d":"code","edec8d7d":"code","72d41725":"code","7cb95d0f":"code","fe808221":"code","177716b4":"code","3050f60d":"code","9f9e46ca":"code","76c64638":"code","3ae589b2":"code","d541351d":"code","106c43da":"code","46eaad1b":"code","03f62b91":"code","d14c4ee0":"code","a69bd0dc":"code","e3d61569":"code","4c637d14":"code","706b36d0":"code","4a4008aa":"code","51bbc8ef":"code","eab60308":"code","8a2dca2e":"code","ef91b789":"code","86ea78a0":"markdown","0d9af096":"markdown","de7b50c5":"markdown","c062b3f1":"markdown","43e41a83":"markdown","59c6f93d":"markdown","c79d962b":"markdown","7641c9f8":"markdown"},"source":{"3f8b06e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b1605ee":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport bz2 # To open zipped files\nimport re # regular expressions\nimport os\nimport gc\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nfrom nltk.classify import SklearnClassifier\n","027898ef":"train_file = bz2.BZ2File('..\/input\/amazonreviews\/train.ft.txt.bz2')\ntest_file = bz2.BZ2File('..\/input\/amazonreviews\/test.ft.txt.bz2')","f77ce676":"train_file_lines = train_file.readlines()\ntest_file_lines = test_file.readlines()","ef8c4c9b":"# clean the older variables\ndel train_file, test_file\ngc.collect()","c683a304":"#Convert from raw binary strings to strings that can be parsed\n\ntrain_file_lines = [x.decode('utf-8') for x in train_file_lines]\ntest_file_lines = [x.decode('utf-8') for x in test_file_lines]","9ae87520":"print(type(train_file_lines), type(test_file_lines), \"\\n\")\n\nprint(\"Train Data Volume:\", len(train_file_lines), \"\\n\")\nprint(\"Test Data Volume:\", len(test_file_lines), \"\\n\\n\")\n\nprint(\"Demo: \", \"\\n\")\nfor x in train_file_lines[:2]:\n    print(x, \"\\n\")","c8b5f5f9":"#Extracting Labels from the data\n\ntrain_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in train_file_lines]\ntest_labels = [0 if x.split(' ')[0] == '__label__1' else 1 for x in test_file_lines]","a9e76f9a":"train_labels[0]","5fab5dc3":"sns.countplot(train_labels)\nplt.title('Train Labels distribution')","9d77c2b0":"sns.countplot(test_labels)\nplt.title('Test Labels distribution')","8466df5d":"#Extracting Reviews from the data\n\ntrain_sentences = [x.split(' ', 1)[1][:-1] for x in train_file_lines]\ntest_sentences = [x.split(' ', 1)[1][:-1] for x in test_file_lines]","36ae5742":"train_sentences[0]","6378964b":"#Let\u2019s count number of words in reviews and see it distribution\ntrain_sentences_size = list(map(lambda x: len(x.split()), train_sentences))\n\nsns.distplot(train_sentences_size)\nplt.xlabel(\"#words in reviews\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Word Frequency Distribution in Reviews\")","6158294d":"train_label_len = pd.DataFrame({\"labels\": train_labels, \"len\": train_sentences_size})\ntrain_label_len.head()","edec8d7d":"# Now we\u2019ll divide it by sentiment and calculate average values\n\nneg_mean_len = train_label_len.groupby('labels')['len'].mean().values[0]\npos_mean_len = train_label_len.groupby('labels')['len'].mean().values[1]\n\nprint(f\"Negative mean length: {neg_mean_len:.2f}\")\nprint(f\"Positive mean length: {pos_mean_len:.2f}\")\nprint(f\"Mean Difference: {neg_mean_len-pos_mean_len:.2f}\")\nsns.catplot(x='labels', y='len', data=train_label_len, kind='box')\nplt.xlabel(\"labels (0->negative, 1->positive)\")\nplt.ylabel(\"#words in reviews\")\nplt.title(\"Review Size Categorization\")","72d41725":"del neg_mean_len,pos_mean_len\ngc.collect()","7cb95d0f":"# Clean URLs\n\nfor i in range(len(train_sentences)):\n    if 'www.' in train_sentences[i] or 'http:' in train_sentences[i] or 'https:' in train_sentences[i] or '.com' in train_sentences[i]:\n        train_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", train_sentences[i])\n\nfor i in range(len(test_sentences)):\n    if 'www.' in test_sentences[i] or 'http:' in test_sentences[i] or 'https:' in test_sentences[i] or '.com' in test_sentences[i]:\n        test_sentences[i] = re.sub(r\"([^ ]+(?<=\\.[a-z]{3}))\", \"<url>\", test_sentences[i])","fe808221":"list(filter(lambda x: '<url>' in x, train_sentences))[0]","177716b4":"del train_file_lines, test_file_lines\ngc.collect()","3050f60d":"import nltk\nfrom nltk import pos_tag\nfrom nltk import sent_tokenize, word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom string import punctuation\n\nnltk.download('averaged_perceptron_tagger')\n\nwnl = WordNetLemmatizer()\n\ndef penn2morphy(penntag):\n    \"\"\" Converts Penn Treebank tags to WordNet. \"\"\"\n    morphy_tag = {'NN':'n', 'JJ':'a',\n                  'VB':'v', 'RB':'r'}\n    try:\n        return morphy_tag[penntag[:2]]\n    except:\n        return 'n' \n    \ndef lemmatize_sent(text): \n    # Text input is string, returns lowercased strings.\n    return [wnl.lemmatize(word.lower(), pos=penn2morphy(tag)) \n            for word, tag in pos_tag(word_tokenize(text))]\n\nlemmatize_sent('He is WALKING walking to school')","9f9e46ca":"# Stopwords from stopwords-json\nstopwords_json = {\"en\":[\"a\",\"a's\",\"able\",\"about\",\"above\",\"according\",\"accordingly\",\"across\",\"actually\",\"after\",\"afterwards\",\"again\",\"against\",\"ain't\",\"all\",\"allow\",\"allows\",\"almost\",\"alone\",\"along\",\"already\",\"also\",\"although\",\"always\",\"am\",\"among\",\"amongst\",\"an\",\"and\",\"another\",\"any\",\"anybody\",\"anyhow\",\"anyone\",\"anything\",\"anyway\",\"anyways\",\"anywhere\",\"apart\",\"appear\",\"appreciate\",\"appropriate\",\"are\",\"aren't\",\"around\",\"as\",\"aside\",\"ask\",\"asking\",\"associated\",\"at\",\"available\",\"away\",\"awfully\",\"b\",\"be\",\"became\",\"because\",\"become\",\"becomes\",\"becoming\",\"been\",\"before\",\"beforehand\",\"behind\",\"being\",\"believe\",\"below\",\"beside\",\"besides\",\"best\",\"better\",\"between\",\"beyond\",\"both\",\"brief\",\"but\",\"by\",\"c\",\"c'mon\",\"c's\",\"came\",\"can\",\"can't\",\"cannot\",\"cant\",\"cause\",\"causes\",\"certain\",\"certainly\",\"changes\",\"clearly\",\"co\",\"com\",\"come\",\"comes\",\"concerning\",\"consequently\",\"consider\",\"considering\",\"contain\",\"containing\",\"contains\",\"corresponding\",\"could\",\"couldn't\",\"course\",\"currently\",\"d\",\"definitely\",\"described\",\"despite\",\"did\",\"didn't\",\"different\",\"do\",\"does\",\"doesn't\",\"doing\",\"don't\",\"done\",\"down\",\"downwards\",\"during\",\"e\",\"each\",\"edu\",\"eg\",\"eight\",\"either\",\"else\",\"elsewhere\",\"enough\",\"entirely\",\"especially\",\"et\",\"etc\",\"even\",\"ever\",\"every\",\"everybody\",\"everyone\",\"everything\",\"everywhere\",\"ex\",\"exactly\",\"example\",\"except\",\"f\",\"far\",\"few\",\"fifth\",\"first\",\"five\",\"followed\",\"following\",\"follows\",\"for\",\"former\",\"formerly\",\"forth\",\"four\",\"from\",\"further\",\"furthermore\",\"g\",\"get\",\"gets\",\"getting\",\"given\",\"gives\",\"go\",\"goes\",\"going\",\"gone\",\"got\",\"gotten\",\"greetings\",\"h\",\"had\",\"hadn't\",\"happens\",\"hardly\",\"has\",\"hasn't\",\"have\",\"haven't\",\"having\",\"he\",\"he's\",\"hello\",\"help\",\"hence\",\"her\",\"here\",\"here's\",\"hereafter\",\"hereby\",\"herein\",\"hereupon\",\"hers\",\"herself\",\"hi\",\"him\",\"himself\",\"his\",\"hither\",\"hopefully\",\"how\",\"howbeit\",\"however\",\"i\",\"i'd\",\"i'll\",\"i'm\",\"i've\",\"ie\",\"if\",\"ignored\",\"immediate\",\"in\",\"inasmuch\",\"inc\",\"indeed\",\"indicate\",\"indicated\",\"indicates\",\"inner\",\"insofar\",\"instead\",\"into\",\"inward\",\"is\",\"isn't\",\"it\",\"it'd\",\"it'll\",\"it's\",\"its\",\"itself\",\"j\",\"just\",\"k\",\"keep\",\"keeps\",\"kept\",\"know\",\"known\",\"knows\",\"l\",\"last\",\"lately\",\"later\",\"latter\",\"latterly\",\"least\",\"less\",\"lest\",\"let\",\"let's\",\"like\",\"liked\",\"likely\",\"little\",\"look\",\"looking\",\"looks\",\"ltd\",\"m\",\"mainly\",\"many\",\"may\",\"maybe\",\"me\",\"mean\",\"meanwhile\",\"merely\",\"might\",\"more\",\"moreover\",\"most\",\"mostly\",\"much\",\"must\",\"my\",\"myself\",\"n\",\"name\",\"namely\",\"nd\",\"near\",\"nearly\",\"necessary\",\"need\",\"needs\",\"neither\",\"never\",\"nevertheless\",\"new\",\"next\",\"nine\",\"no\",\"nobody\",\"non\",\"none\",\"noone\",\"nor\",\"normally\",\"not\",\"nothing\",\"novel\",\"now\",\"nowhere\",\"o\",\"obviously\",\"of\",\"off\",\"often\",\"oh\",\"ok\",\"okay\",\"old\",\"on\",\"once\",\"one\",\"ones\",\"only\",\"onto\",\"or\",\"other\",\"others\",\"otherwise\",\"ought\",\"our\",\"ours\",\"ourselves\",\"out\",\"outside\",\"over\",\"overall\",\"own\",\"p\",\"particular\",\"particularly\",\"per\",\"perhaps\",\"placed\",\"please\",\"plus\",\"possible\",\"presumably\",\"probably\",\"provides\",\"q\",\"que\",\"quite\",\"qv\",\"r\",\"rather\",\"rd\",\"re\",\"really\",\"reasonably\",\"regarding\",\"regardless\",\"regards\",\"relatively\",\"respectively\",\"right\",\"s\",\"said\",\"same\",\"saw\",\"say\",\"saying\",\"says\",\"second\",\"secondly\",\"see\",\"seeing\",\"seem\",\"seemed\",\"seeming\",\"seems\",\"seen\",\"self\",\"selves\",\"sensible\",\"sent\",\"serious\",\"seriously\",\"seven\",\"several\",\"shall\",\"she\",\"should\",\"shouldn't\",\"since\",\"six\",\"so\",\"some\",\"somebody\",\"somehow\",\"someone\",\"something\",\"sometime\",\"sometimes\",\"somewhat\",\"somewhere\",\"soon\",\"sorry\",\"specified\",\"specify\",\"specifying\",\"still\",\"sub\",\"such\",\"sup\",\"sure\",\"t\",\"t's\",\"take\",\"taken\",\"tell\",\"tends\",\"th\",\"than\",\"thank\",\"thanks\",\"thanx\",\"that\",\"that's\",\"thats\",\"the\",\"their\",\"theirs\",\"them\",\"themselves\",\"then\",\"thence\",\"there\",\"there's\",\"thereafter\",\"thereby\",\"therefore\",\"therein\",\"theres\",\"thereupon\",\"these\",\"they\",\"they'd\",\"they'll\",\"they're\",\"they've\",\"think\",\"third\",\"this\",\"thorough\",\"thoroughly\",\"those\",\"though\",\"three\",\"through\",\"throughout\",\"thru\",\"thus\",\"to\",\"together\",\"too\",\"took\",\"toward\",\"towards\",\"tried\",\"tries\",\"truly\",\"try\",\"trying\",\"twice\",\"two\",\"u\",\"un\",\"under\",\"unfortunately\",\"unless\",\"unlikely\",\"until\",\"unto\",\"up\",\"upon\",\"us\",\"use\",\"used\",\"useful\",\"uses\",\"using\",\"usually\",\"uucp\",\"v\",\"value\",\"various\",\"very\",\"via\",\"viz\",\"vs\",\"w\",\"want\",\"wants\",\"was\",\"wasn't\",\"way\",\"we\",\"we'd\",\"we'll\",\"we're\",\"we've\",\"welcome\",\"well\",\"went\",\"were\",\"weren't\",\"what\",\"what's\",\"whatever\",\"when\",\"whence\",\"whenever\",\"where\",\"where's\",\"whereafter\",\"whereas\",\"whereby\",\"wherein\",\"whereupon\",\"wherever\",\"whether\",\"which\",\"while\",\"whither\",\"who\",\"who's\",\"whoever\",\"whole\",\"whom\",\"whose\",\"why\",\"will\",\"willing\",\"wish\",\"with\",\"within\",\"without\",\"won't\",\"wonder\",\"would\",\"wouldn't\",\"x\",\"y\",\"yes\",\"yet\",\"you\",\"you'd\",\"you'll\",\"you're\",\"you've\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"z\",\"zero\"]}\nstopwords_json_en = set(stopwords_json['en'])\nstopwords_nltk_en = set(stopwords.words('english'))\nstopwords_punct = set(punctuation)\n# Combine the stopwords. Its a lot longer so I'm not printing it out...\nstoplist_combined = set.union(stopwords_json_en, stopwords_nltk_en, stopwords_punct)","76c64638":"# Clean Digits\n\ndef preprocess_text(text):\n    # Input: str, i.e. document\/sentence\n    # Output: list(str) , i.e. list of lemmas\n    return [word for word in lemmatize_sent(text) \n            if word not in stoplist_combined\n            and not word.isdigit()]","3ae589b2":"train_sentences[10]","d541351d":"preprocess_text(train_sentences[10])","106c43da":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vect = CountVectorizer(analyzer=preprocess_text)","46eaad1b":"train_set = count_vect.fit_transform(train_sentences[:10000])","03f62b91":"train_set.toarray().shape","d14c4ee0":"test_set = count_vect.transform(test_sentences[:1000])","a69bd0dc":"most_freq_words = pd.DataFrame(count_vect.vocabulary_.items(), columns=['word', 'frequency'])[:100].sort_values(ascending=False, by = \"frequency\")[:20]\nmost_freq_words.plot.bar(x=\"word\", y=\"frequency\", rot=70, title=\"Most Frequent Words\")","e3d61569":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()","4c637d14":"clf.fit(train_set, train_labels[:10000])","706b36d0":"from sklearn.metrics import accuracy_score\n\n# To predict our tags (i.e. whether requesters get their pizza), \n# we feed the vectorized `test_set` to .predict()\npredictions_valid = clf.predict(test_set)\n\nprint('Amazon Sentiment Analysis Accuracy = {}'.format(\n        accuracy_score(predictions_valid, test_labels[:1000]) * 100)\n     )","4a4008aa":"def important_features(vectorizer,classifier,n=40):\n    class_labels = classifier.classes_\n    feature_names =vectorizer.get_feature_names()\n\n    topn_class1 = sorted(zip(classifier.feature_count_[0], feature_names),reverse=True)[:n]\n    topn_class2 = sorted(zip(classifier.feature_count_[1], feature_names),reverse=True)[:n]\n\n    class1_frequency_dict = {}\n    class2_frequency_dict = {}\n    \n    for coef, feat in topn_class1:\n        class1_frequency_dict.update( {feat : coef} )\n\n    for coef, feat in topn_class2:\n        class2_frequency_dict.update( {feat : coef} )\n\n    return (class1_frequency_dict, class2_frequency_dict)","51bbc8ef":"neg_frequency_dict, pos_frequency_dict = important_features(count_vect, clf)","eab60308":"neg_feature_freq = pd.DataFrame(neg_frequency_dict.items(), columns = [\"feature_word\", \"frequency\"])  \npos_feature_freq = pd.DataFrame(pos_frequency_dict.items(), columns = [\"feature_word\", \"frequency\"])  ","8a2dca2e":"neg_feature_freq.plot.bar(x=\"feature_word\", y=\"frequency\", rot=70, figsize=(15, 5), title=\"Important Negative Features(words)\")","ef91b789":"pos_feature_freq.plot.bar(x=\"feature_word\", y=\"frequency\", rot=70, figsize=(15, 5), title=\"Important Positive Features(words)\")","86ea78a0":"Stopwords\nStopwords are non-content words that primarily has only grammatical function. Often we want to remove stopwords when we want to keep the \"gist\" of the document\/sentence.\n\nUsing a stronger\/longer list of stopwords\nAfter applying NLTK Stopwords we have still dangly model verbs (i.e. 'could', 'wont', etc.).\n\nWe can combine the stopwords we have in NLTK with other stopwords list we find online.\n\nPersonally, I like to use stopword-json because it has stopwrds in 50 languages","0d9af096":"Methods\n\n\nWhat methods did you use to analyze the data and why are they appropriate? Be sure to adequately, but briefly, describe your methods.\nMultinomial Naive Bayes classifier in sklearn\n\nClassification\n\nClassification simply means putting our data points into bins\/box. You can also think of it as assigning label to our data points, e.g. given box of fruits, sort them in apples, oranges and others.\n\nNaive Bayes Classification\n\nIn statistics, Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong independence assumptions between the features. They are among the simplest Bayesian network models\n\nThere are different variants of Naive Bayes (NB) classifier in sklearn.\n\nMultinomial is a big word but it just means many classes\/categories\/bins\/boxes that needs to be classified.","de7b50c5":"Lowercasing\n\nThe CAPS in the texts are RATHER irritating although we KNOW the guy is trying to EMPHASIZE on something ;P\n\nWe can simply lowercase them after we do sent_tokenize() and word_tokenize(). The tokenizers uses the capitalization as cues to know when to split so removing them before the calling the functions would be sub-optimal.\n\nTokenization\n\nSentence tokenization is the process of splitting up strings into \u201csentences\u201d\nWord tokenization is the process of splitting up \u201csentences\u201d into \u201cwords\u201d\nStemming and Lemmatization\u00b6\nOften we want to map the different forms of the same word to the same root word, e.g. \"walks\", \"walking\", \"walked\" should all be the same as \"walk\".\n\nThe stemming and lemmatization process are hand-written regex rules written find the root word.\n\nStemming: Trying to shorten a word with simple regex rules\nLemmatization: Trying to find the root word with linguistics rules (with the use of regexes)","c062b3f1":"Word Embeddings\n\n\nFrom Strings to Vectors\nVector is an array of numbers\n\nVector Space Model: conceptualizing language as a whole lot of numbers\n\nBag-of-Words (BoW): Counting each document\/sentence as a vector of numbers, with each number representing the count of a word in the corpus\n\nVector space model or term vector model is an algebraic model for representing text documents as vectors of identifiers, for example, index terms. It is used in information filtering, information retrieval, indexing and relevancy rankings.\n\nWe are using sklearn CountVectorizer to create Vector Space Model\u00b6\n","43e41a83":"#Data Preparation and Cleaning#\n\nAt a high-level, what did you need to do to prepare the data for analysis? Describe what problems, if any, did you encounter with the dataset?\nEach review with more than 100 words was imported and tokenized. Afterward, all of the tokens that were punctuations, label, stopword, or not an English word (emoji, special character, foreign languages) were removed.\n\nThe remaining tokens would undergo the process of lemmatizing using WordNetLemmatizer to reduce word to its root form, and Lemmatization using Wordnet Lemmatizer to acquire primal terms before appending into a clean list.\n\nFollowing are the process followed:\n\nLowercasing\n\nTokenization\n\nStemming and Lemmatization\n\nRemoving Stopwords\n\nRemoving Punctuations\n\nRemoving Digits\n\nRemoving Url\u2019s","59c6f93d":"Limitations\n\nIf applicable, describe limitations to your findings. For example, you might note that these results were true for British Premier league players but may not be applicable to other leagues because of differences in league structures.\nOr you may note that your data has inherent limitations. For example, you may not have access to the number of Twitter followers per users so you assumed all users are equally influential. If you had the number of followers, you could weight the impact of their tweet\u2019s sentiment by their influence (# of followers).\nModel is trained on the Amazon Product Review data, and does not assure guarantee of predicting correct label (positive or negative)for other e-commerce websites.\n\nThe knowledge of categories of products are unknown, so we can to predict accuracy of prediction in product category segments.\n\nThe predictions are always limitied to efficiency of:\n\nData Cleaning Algorithms\n\nText Embedding Algorithms\n\nPrediction Algorithm","c79d962b":"****Feature Importance****","7641c9f8":"Vectorization with sklearn\n\n\nIn scikit-learn, there're pre-built functions to do the preprocessing and vectorization.\n\nIt will be the object that contains the vocabulary (i.e. the first row of our table above) and has the function to convert any sentence into the counts vectors"}}