{"cell_type":{"4da0afbc":"code","763f44a3":"code","8fc9d1ad":"code","6a87c582":"code","e3d70a9c":"code","cc107736":"code","72dfec2d":"code","7f41a707":"code","49424f50":"code","61110d57":"code","ec94371b":"code","bf40f045":"code","a7d121e6":"code","b43c73f6":"code","3bbfb3c8":"code","76339471":"code","20ff14c5":"code","59601d60":"code","0b47dd0a":"markdown","143ed3dc":"markdown","4ee199a5":"markdown","0464df8e":"markdown","3234fa1e":"markdown","12bd9ab4":"markdown","b0904b7e":"markdown","74281a9c":"markdown","90d291e2":"markdown","a898aa30":"markdown","3022685f":"markdown"},"source":{"4da0afbc":"import torch\n\n# If there's a GPU available...\nif torch.cuda.is_available():    \n\n    # Tell PyTorch to use the GPU.    \n    device = torch.device(\"cuda\")\n\n    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n\n# If not...\nelse:\n    print('No GPU available, using CPU instead.')\n    device = torch.device(\"cpu\")","763f44a3":"from torch.utils.data.dataset import Dataset\nimport csv\nimport pandas as pd\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nimport numpy as np\n\nclass MyDataset(Dataset):\n\n    def __init__(self, data_path, dict_path, max_length_sentences=15, max_length_word=100, test=False):\n        super(MyDataset, self).__init__()\n\n        texts, targets = [], []\n        with open(data_path) as csv_file:\n            reader = csv.DictReader(csv_file, quotechar='\"')\n            for idx, line in enumerate(reader):\n                text = \"\"\n                for tx in line['excerpt']:\n                    text += tx.lower()\n                    text += \" \"\n                if not test:\n                    target = float(line['target'])\n                    targets.append(target)\n                texts.append(text)\n\n        self.texts = texts\n        self.targets = targets\n        self.dict = pd.read_csv(filepath_or_buffer=dict_path, header=1, sep=\" \", quoting=csv.QUOTE_NONE,\n                                usecols=[0]).values\n        self.dict = [word[0] for word in self.dict]\n        self.max_length_sentences = max_length_sentences\n        self.max_length_word = max_length_word\n        self.num_classes = 1 # len(set(self.targets))\n\n    def __len__(self):\n        return len(self.targets)\n\n    def __getitem__(self, index, test=False):\n        if not test:\n            target = self.targets[index]\n        text = self.texts[index]\n        document_encode = [\n            [self.dict.index(word) if word in self.dict else -1 for word in word_tokenize(text=sentences)] for\n                                                                    sentences in sent_tokenize(text=text)]\n\n        for sentences in document_encode:\n            if len(sentences) < self.max_length_word:\n                extended_words = [-1 for _ in range(self.max_length_word - len(sentences))]\n                sentences.extend(extended_words)\n\n        if len(document_encode) < self.max_length_sentences:\n            extended_sentences = [[-1 for _ in range(self.max_length_word)] for _ in\n                                  range(self.max_length_sentences - len(document_encode))]\n            document_encode.extend(extended_sentences)\n\n        document_encode = [sentences[:self.max_length_word] for sentences in document_encode][\n                          :self.max_length_sentences]\n\n        document_encode = np.stack(arrays=document_encode, axis=0)\n        document_encode += 1\n        if test:\n            # float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint8, and bool\n            return document_encode.astype(np.float32)\n        return document_encode.astype(np.float32), target ","8fc9d1ad":"#train = MyDataset(data_path=\"..\/input\/commonlitreadabilityprize\/train.csv\",\n#                  dict_path=\"..\/input\/fasttext\/wiki-news-300d-1M-subword.txt\")\n# print (train.__getitem__(index=1)[0].shape)\n#for index in range(train.__len__()):\n#    print(train.__getitem__(index=index))\n# test = MyDataset(data_path=\"..\/input\/commonlitreadabilityprize\/test.csv\",\n#                  dict_path=\"..\/input\/fasttext\/wiki-news-300d-1M-subword.txt\", test=True)\n# print(test.__getitem__(index=1, test=True).shape)","6a87c582":"import torch.nn as nn\nimport random\n\ndef get_max_lengths(data_path):\n    word_length_list = []\n    sent_length_list = []\n    with open(data_path) as csv_file:\n        reader = csv.reader(csv_file, quotechar='\"')\n        for idx, line in enumerate(reader):\n            text = \"\"\n            for tx in line[1:]:\n                text += tx.lower()\n                text += \" \"\n            sent_list = sent_tokenize(text)\n            sent_length_list.append(len(sent_list))\n\n            for sent in sent_list:\n                word_list = word_tokenize(sent)\n                word_length_list.append(len(word_list))\n\n        sorted_word_length = sorted(word_length_list)\n        sorted_sent_length = sorted(sent_length_list)\n\n    return sorted_word_length[int(0.8*len(sorted_word_length))], sorted_sent_length[int(0.8*len(sorted_sent_length))]\n\ndef matrix_mul(input, weight, bias=False):\n    feature_list = []\n    for feature in input:\n        feature = torch.mm(feature, weight)\n        if isinstance(bias, torch.nn.parameter.Parameter):\n            feature = feature + bias.expand(feature.size()[0], bias.size()[1])\n        feature = torch.tanh(feature).unsqueeze(0)\n        feature_list.append(feature)\n\n    return torch.cat(feature_list, 0).squeeze()\n\ndef element_wise_mul(input1, input2):\n\n    feature_list = []\n    for feature_1, feature_2 in zip(input1, input2):\n        feature_2 = feature_2.unsqueeze(1).expand_as(feature_1)\n        feature = feature_1 * feature_2\n        feature_list.append(feature.unsqueeze(0))\n    output = torch.cat(feature_list, 0)\n\n    return torch.sum(output, 0).unsqueeze(0)\n\ndef set_seed(seed_value=42):\n    # Set seed for reproducibility.\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)","e3d70a9c":"\"\"\"\n@author: Viet Nguyen <nhviet1009@gmail.com>\n\"\"\"\n\nclass WordAttNet(nn.Module):\n    def __init__(self, word2vec_path, hidden_size=50, num_layers=4):\n        super(WordAttNet, self).__init__()\n        #dict = pd.read_csv(filepath_or_buffer=word2vec_path, header=None, sep=\" \", quoting=csv.QUOTE_NONE).values[:, 1:]\n        dict = pd.read_csv(filepath_or_buffer=word2vec_path, header=1, sep=\" \",\n                           quoting=csv.QUOTE_NONE, dtype=str, low_memory=True).values[:500000, 1:]\n        for idx in range(len(dict)):\n            dict[idx] = np.float16(dict[idx])\n        dict_len, embed_size = dict.shape\n        dict_len += 1\n        unknown_word = np.zeros((1, embed_size))\n        dict = torch.from_numpy(np.concatenate([unknown_word, dict], axis=0).astype(np.float))\n\n        self.word_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 2 * hidden_size))\n        self.word_bias = nn.Parameter(torch.Tensor(1, 2 * hidden_size))\n        self.context_weight = nn.Parameter(torch.Tensor(2 * hidden_size, 1))\n\n        self.lookup = nn.Embedding(num_embeddings=dict_len, embedding_dim=embed_size).from_pretrained(dict)\n        self.gru = nn.GRU(embed_size, hidden_size, num_layers, bidirectional=True)\n        self._create_weights(mean=0.0, std=0.8)\n\n    def _create_weights(self, mean=0.0, std=0.8):\n        self.word_weight.data.normal_(mean, std)\n        self.context_weight.data.normal_(mean, std)\n\n    def forward(self, input, hidden_state):\n        #print(f'input: {torch.isnan(input).any()}')\n        output = self.lookup(input.long())\n        #print(f'output1: {torch.isnan(output).any()}')\n        f_output, h_output = self.gru(output.float(), hidden_state)  # feature output and hidden state output\n        #print(f'f_output: {torch.isnan(f_output).any()}')\n        output = matrix_mul(f_output, self.word_weight, self.word_bias)\n        #print(f'output2: {torch.isnan(output).any()}')\n        output = matrix_mul(output, self.context_weight).permute(1,0)\n        #print(f'output3: {torch.isnan(output).any()}')\n        output = F.softmax(output)\n        #print(f'output4: {torch.isnan(output).any()}')\n        output = element_wise_mul(f_output,output.permute(1,0))\n        #print(f'output5: {torch.isnan(output).any()}')\n        return output, h_output","cc107736":"\"\"\"\n@author: Viet Nguyen <nhviet1009@gmail.com>\n\"\"\"\n\nclass SentAttNet(nn.Module):\n    def __init__(self, sent_hidden_size=50, word_hidden_size=50, num_classes=100, num_layers=4):\n        super(SentAttNet, self).__init__()\n\n        self.sent_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 2 * sent_hidden_size))\n        self.sent_bias = nn.Parameter(torch.Tensor(1, 2 * sent_hidden_size))\n        self.context_weight = nn.Parameter(torch.Tensor(2 * sent_hidden_size, 1))\n\n        self.gru = nn.GRU(2 * word_hidden_size, sent_hidden_size, num_layers, bidirectional=True)\n        #self.fc = nn.Linear(2 * sent_hidden_size, num_classes)\n        \n        # self.sent_softmax = nn.Softmax()\n        # self.fc_softmax = nn.Softmax()\n        self._create_weights(mean=0.0, std=0.5)\n\n    def _create_weights(self, mean=0.0, std=0.5):\n        self.sent_weight.data.normal_(mean, std)\n        self.context_weight.data.normal_(mean, std)\n\n    def forward(self, input, hidden_state):\n        f_output, h_output = self.gru(input, hidden_state)\n        output = matrix_mul(f_output, self.sent_weight, self.sent_bias)\n        output = matrix_mul(output, self.context_weight).permute(1, 0)\n        output = F.softmax(output)\n        output = element_wise_mul(f_output, output.permute(1, 0)).squeeze(0)\n        #output = self.fc1(output)\n        return output, h_output","72dfec2d":"\"\"\"\n@author: Viet Nguyen <nhviet1009@gmail.com>\n\"\"\"\n\nclass HierAttNet(nn.Module):\n    def __init__(self, word_hidden_size, sent_hidden_size, batch_size, num_classes, pretrained_word2vec_path,\n                 max_sent_length, max_word_length, num_layers=4):\n        super(HierAttNet, self).__init__()\n        self.num_layers = num_layers\n        self.batch_size = batch_size\n        self.word_hidden_size = word_hidden_size\n        self.sent_hidden_size = sent_hidden_size\n        self.max_sent_length = max_sent_length\n        self.max_word_length = max_word_length\n\n        self.word_att_net = WordAttNet(pretrained_word2vec_path, word_hidden_size, num_layers=self.num_layers*2)\n        self.sent_att_net = SentAttNet(sent_hidden_size, word_hidden_size, num_classes, num_layers=self.num_layers)\n        self._init_hidden_state()\n        \n        self.regressor = nn.Sequential(\n                            #nn.Dropout(0.5),\n                            nn.Linear(2 * sent_hidden_size, sent_hidden_size),\n                            nn.ReLU(),\n                            nn.Linear(sent_hidden_size, num_classes)\n                         )\n\n    def _init_hidden_state(self, last_batch_size=None):\n        if last_batch_size:\n            batch_size = last_batch_size\n        else:\n            batch_size = self.batch_size\n        self.word_hidden_state = torch.zeros(self.num_layers*2*2, batch_size, self.word_hidden_size)\n        self.sent_hidden_state = torch.zeros(self.num_layers*2, batch_size, self.sent_hidden_size)\n        if torch.cuda.is_available():\n            self.word_hidden_state = self.word_hidden_state.cuda()\n            self.sent_hidden_state = self.sent_hidden_state.cuda()\n\n    def forward(self, input):\n        output_list = []\n        for i in input:\n            output, self.word_hidden_state = self.word_att_net(i.permute(1, 0), self.word_hidden_state)\n            output_list.append(output)\n        output = torch.cat(output_list, 0)\n        output, self.sent_hidden_state = self.sent_att_net(output, self.sent_hidden_state)\n        output = self.regressor(output)\n        return output","7f41a707":"from tensorboardX import SummaryWriter\nfrom torch.utils.data import DataLoader\nimport os\nimport shutil\nimport torch.nn.functional as F\n\ndef train(training_generator, val_generator, opt, model):\n    if os.path.isdir(opt.log_path):\n        shutil.rmtree(opt.log_path)\n    os.makedirs(opt.log_path)\n    writer = SummaryWriter(opt.log_path)\n    # writer.add_graph(model, torch.zeros(opt.batch_size, max_sent_length, max_word_length))\n\n    #if torch.cuda.is_available():\n    #    model.cuda()\n    criterion = nn.MSELoss(reduction='mean')\n    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n                                 lr=opt.lr, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, threshold=5e-2)\n#     optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n#     scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=0.0001, max_lr=1,\n#                                                   step_size_up=500,mode=\"triangular2\")\n    \n    best_loss = 1e5\n    best_epoch = 0\n    model.train()\n    num_iter_per_epoch = len(training_generator)\n    for epoch in range(opt.num_epoches):\n        for i, batch in enumerate(training_generator):\n            if len(batch) < 2:\n                continue\n            feature, label = tuple(t.to(device) for t in batch)\n            model.zero_grad()\n            if torch.cuda.is_available():\n                feature = feature.cuda()\n                label = label.cuda()\n            optimizer.zero_grad()\n            model._init_hidden_state()  # fer aix\u00f2 per cada lot?\u00bf ###############\n            predictions = model(feature.permute(1, 0, 2))\n            #print(predictions)\n            loss = criterion(torch.reshape(predictions.float(), (-1,)), label.float())\n            #print(loss)\n            loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)  #############3\n            optimizer.step()\n            if i % 4 == 0:\n                print(\"Epoch: {}\/{}, Iteration: {}\/{}, Lr: {}, Loss\/batch: {}\".format(\n                    epoch + 1,\n                    opt.num_epoches,\n                    i + 1,\n                    num_iter_per_epoch,\n                    optimizer.param_groups[0]['lr'],\n                    loss))\n            writer.add_scalar('Train\/Loss', loss, epoch * num_iter_per_epoch + i)\n            writer.add_scalar('Train\/Accuracy', epoch * num_iter_per_epoch + i)\n        if epoch % opt.test_interval == 0:\n            model.eval()\n            loss_ls = []\n            te_label_ls = []\n            te_pred_ls = []\n            itr = 0\n            for te_feature, te_label in val_generator:\n                if len(te_label) < 2:\n                    continue\n                itr += 1\n                num_sample = len(te_label)\n                if torch.cuda.is_available():\n                    te_feature = te_feature.cuda()\n                    te_label = te_label.cuda()\n                with torch.no_grad():\n                    model._init_hidden_state(num_sample)\n                    te_predictions = model(te_feature.permute(1, 0, 2))\n                te_loss = criterion( torch.reshape(te_predictions, (-1,)), te_label )\n                loss_ls.append(te_loss)# * num_sample)\n                te_label_ls.extend(te_label.clone().cpu())\n                te_pred_ls.append(te_predictions.clone().cpu())\n            te_loss = torch.sqrt(sum(loss_ls) \/ val_generator.__len__())\n            te_pred = torch.cat(te_pred_ls, 0)\n            te_label = np.array(te_label_ls)\n            scheduler.step(torch.unsqueeze(sum(loss_ls), 0))\n            output_file.write(\n                \"Epoch: {}\/{} \\nTest loss: {}\".format(\n                    epoch + 1, opt.num_epoches,\n                    te_loss))\n            print(\"Epoch: {}\/{}, Lr: {}, Loss: {}\".format(\n                epoch + 1,\n                opt.num_epoches,\n                optimizer.param_groups[0]['lr'],\n                te_loss))\n            writer.add_scalar('Test\/Loss', te_loss, epoch)\n            writer.add_scalar('Test\/Accuracy', epoch)\n            model.train()\n            if te_loss + opt.es_min_delta < best_loss:\n                best_loss = te_loss\n                best_epoch = epoch\n                torch.save(model, opt.saved_path + os.sep + \"whole_model_han\")\n\n            # Early stopping\n            if epoch - best_epoch > opt.es_patience > 0:\n                print(\"Stop training at epoch {}. The lowest loss achieved is {}\".format(epoch, te_loss))\n                break","49424f50":"from types import SimpleNamespace\nopt = SimpleNamespace(\n        batch_size = 64,\n        num_epoches = 10,\n        lr = 1e-4,\n        word_hidden_size = 80,\n        sent_hidden_size = 40,\n        num_layers = 5,\n        es_min_delta = 0.0,\n        es_patience = 5,\n        train_set_path = \"..\/input\/commonlitreadabilityprize\/train.csv\",\n        test_set = \"..\/input\/commonlitreadabilityprize\/test.csv\",\n        test_interval = 1,\n        word2vec_path = \"..\/input\/glove50d200d\/glove.6B.50d.txt\",\n        log_path = \".\/tensorboard\/han_voc\",\n        saved_path = \"trained_models\",\n        p = 0.1  # size of val set\n)","61110d57":"if torch.cuda.is_available():\n        torch.cuda.manual_seed(123)\nelse:\n    torch.manual_seed(123)\n\nfilename = opt.saved_path + os.sep + \"logs.txt\"\nos.makedirs(os.path.dirname(filename), exist_ok=True)\noutput_file = open(filename, \"w\")\noutput_file.write(\"Model's parameters: {}\".format(vars(opt)))\n\ntraining_params = {\"batch_size\": opt.batch_size,\n                   \"pin_memory\": True,\n                   \"shuffle\": True,\n                   \"drop_last\": True}\ntest_params = {\"batch_size\": opt.batch_size,\n               \"pin_memory\": True,\n               \"shuffle\": False,\n               \"drop_last\": False}\n\n#word_length, sent_length = get_max_lengths(opt.train_set_path)\nword_length, sent_length = 80, 15\ntrain_dataset = MyDataset(opt.train_set_path,\n                  opt.word2vec_path,\n                  sent_length, word_length)\nprint(train_dataset.__len__())","ec94371b":"#from sklearn.model_selection import train_test_split\n#excerpts_train, excerpts_val, targets_train, targets_val = train_test_split(excerpts_training, targets_training, test_size=0.1, random_state=2021)\n\ntraining_set, val_set = torch.utils.data.random_split(train_dataset,\n                        [train_dataset.__len__() - int(train_dataset.__len__()*opt.p), int(opt.p*train_dataset.__len__())])\ntraining_generator = DataLoader(training_set, **training_params)\nval_generator = DataLoader(val_set, **test_params) ","bf40f045":"torch.autograd.set_detect_anomaly(True)\n\nimport time\n\nbegin = time.time()\nset_seed(37)\nmodel = HierAttNet(opt.word_hidden_size, opt.sent_hidden_size, opt.batch_size, 1,\n                   opt.word2vec_path, sent_length, word_length, num_layers=opt.num_layers)\nmodel.to(device)\n#if __name__ == '__main__':\ntrain(training_generator, val_generator, opt, model)\nprint(f'elapsed: {time.time() - begin}')","a7d121e6":"# # Concatenate the train set and the validation set\n# full_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\n\n# #full_train_data = train_data\n# full_train_sampler = RandomSampler(full_train_data)\n# full_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=TRAIN_BATCH_SIZE)","b43c73f6":"# # # Train the Bert Classifier on the entire training data\n# set_seed(42)\n# XLN_regressor, optimizer, scheduler = initialize_model(epochs=init_epochs, freeze=False)\n# train(XLN_regressor, full_train_dataloader, epochs=2)\n\n# # set_seed(42)\n# # XLN_regressor, optimizer, scheduler = initialize_model(epochs=init_epochs, freeze=False)\n# # train(XLN_regressor, train_dataloader, val_dataloader, epochs=2, evaluation=True)","3bbfb3c8":"# import torch.nn.functional as F\n\n# def XLN_predict(model, test_dataloader):\n#     #Perform a forward pass on the trained BERT model to predict.\n\n#     # Put the model into the evaluation mode. The dropout layers are disabled during\n#     # the test time.\n#     model.eval()\n#     all_preds = []\n\n#     # For each batch in our test set...\n#     for batch in test_dataloader:\n#         # Load batch to GPU\n#         b_input_ids, b_attn_mask, b_token_type_ids = tuple(t.to(device) for t in batch)[:3]\n\n#         # Compute preds\n#         with torch.no_grad():\n#             preds = model(b_input_ids, b_attn_mask, b_token_type_ids)\n#         all_preds.append(preds)\n    \n#     # Concatenate preds from each batch\n#     all_preds = torch.cat(all_preds, dim=0)\n\n#     # Apply softmax to calculate probabilities\n#     #probs = F.softmax(all_preds, dim=1).cpu().numpy()\n\n#     return all_preds","76339471":"# # Run `preprocessing_for_bert` on the test set\n# print('Tokenizing data...')\n# test_inputs, test_masks, test_type_ids = preprocessing_for_XLN(excerpts_test)\n\n# # Create the DataLoader for our test set\n# test_dataset = TensorDataset(test_inputs, test_masks, test_type_ids)\n# test_sampler = SequentialSampler(test_dataset)\n# test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=VALID_BATCH_SIZE)","20ff14c5":"# # Compute predictions on the test set\n# preds_test = XLN_predict(XLN_regressor, test_dataloader)","59601d60":"# submission = df_test[['id']]\n# submission['target'] = np.array(preds_test.cpu()) #np.mean(test_pred, axis=0)\n# submission.to_csv('submission.csv', index=False)\n# display(submission.head(10))","0b47dd0a":"# Model","143ed3dc":"## Training","4ee199a5":"### Format","0464df8e":"fastText: wiki-news-300d-1M-subword: https:\/\/fasttext.cc\/docs\/en\/english-vectors.html\n\nenwiki_20180420_500d.txt: https:\/\/wikipedia2vec.github.io\/wikipedia2vec\/pretrained\/\n\nGloVe: https:\/\/nlp.stanford.edu\/projects\/glove\/","3234fa1e":"## Prediction on Test Set","12bd9ab4":"## Read Data and Prepare Dataset","b0904b7e":"## Train on full training dataset","74281a9c":"## Utils","90d291e2":"### Preproess Test Data","a898aa30":"## Predictions and Submission","3022685f":"### Predict"}}