{"cell_type":{"6e0dd3d0":"code","66fc23ae":"code","aaf06c53":"code","4cf4b7e8":"code","fd378c4b":"code","ab56ff94":"code","236899ae":"code","8f9615dd":"code","02197001":"code","a38aa749":"code","80fb3c70":"code","8e984b8b":"code","12610543":"code","43ffb53b":"code","4d3924d4":"code","de0a9379":"code","1346a1ee":"code","a2e5688f":"code","90b36e55":"code","912dde0d":"code","4d374c89":"code","897898f8":"code","6d707e5f":"code","08bb64eb":"code","30225ce5":"code","f6fc2177":"code","1bb7e3f6":"code","3e441a72":"code","2700c0da":"code","f5213a51":"code","7eb6397f":"code","b89c2412":"code","cf46d4b1":"code","d44c885f":"code","14950fa3":"markdown","30182e5e":"markdown","c91764b3":"markdown","90ab4610":"markdown","919b4207":"markdown","fe0ecfd8":"markdown","90344a7a":"markdown","79217c6a":"markdown"},"source":{"6e0dd3d0":"import numpy as np\nimport pandas as pd\nimport pandas\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold,GroupKFold","66fc23ae":"train=pd.read_csv('..\/input\/ltfs-2\/train_fwYjLYX.csv')\ntest=pd.read_csv('..\/input\/ltfs-2\/test_1eLl9Yf.csv')","aaf06c53":"def mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","4cf4b7e8":"train.head()","fd378c4b":"train['application_date']=pd.to_datetime(train['application_date'])\ntest['application_date']=pd.to_datetime(test['application_date'])","ab56ff94":"import holidays\nhol_list = holidays.IND(years = [2017,2018,2019])\nhol_list = [date for date,name in hol_list.items()]\ntrain['hol'] = train['application_date'].isin(hol_list) * 1\ntest['hol'] = test['application_date'].isin(hol_list) * 1","236899ae":"def dateFeatures(df, label=None,seg=None):\n    features = ['day','week','dayofweek','month','quarter','year','dayofyear','weekofyear','is_month_start','is_month_end','is_quarter_start','is_quarter_end','is_year_start','is_year_end']\n    date = df['application_date']\n    for col in features:\n        df[col] = getattr(date.dt,col) * 1","8f9615dd":"train = train[['application_date','segment','case_count']]\ntrain_s1=train[train['segment']==1].groupby(['application_date']).sum().reset_index().sort_values('application_date')\ntrain_s2=train[train['segment']==2].groupby(['application_date']).sum().reset_index().sort_values('application_date')\ntest_s1=test[test['segment']==1][['application_date']].sort_values('application_date')\ntest_s2=test[test['segment']==2][['application_date']].sort_values('application_date')","02197001":"dateFeatures(train_s1)\ndateFeatures(train_s2)\ndateFeatures(test_s1)\ndateFeatures(test_s2)","a38aa749":"test_s2.head()","80fb3c70":"sns.boxplot(train_s1['case_count'])","8e984b8b":"sns.distplot(train_s1['case_count'])","12610543":"train_s1['case_count'].describe()","43ffb53b":"case_max = train_s1['case_count'].max()\ntrain_s1[train_s1['case_count']==case_max]","4d3924d4":"train_s1[(train_s1['application_date'] >= '2017-03-01') & (train_s1['application_date'] <= '2017-03-31')]","de0a9379":"train_s1[(train_s1['application_date'] >= '2018-03-01') & (train_s1['application_date'] <= '2018-03-31')]","1346a1ee":"train_s1[(train_s1['application_date'] >= '2019-03-01') & (train_s1['application_date'] <= '2019-03-31')]","a2e5688f":"train_s1[train_s1['case_count'] > 7000]","90b36e55":"train_s1[train_s1['case_count']<20]","912dde0d":"train_s1 = train_s1[(train_s1['case_count'] > 20) & (train_s1['case_count'] < 7000)]","4d374c89":"train_s1 = train_s1[train_s1['case_count']<=10000]\ntrain_s1=train_s1.reset_index().drop('index',axis=1)","897898f8":"train_s2.describe()","6d707e5f":"sns.distplot(train_s2['case_count'])","08bb64eb":"sns.boxplot(train_s2['case_count'])","30225ce5":"train_s2[train_s2['case_count']>35000]","f6fc2177":"train_s2 = train_s2[train_s2['case_count']<36000]\ntrain_s2=train_s2.reset_index().drop('index',axis=1)","1bb7e3f6":"y1 = train_s1['case_count']\ny2 = train_s2['case_count']\ntrain_s1.drop(['case_count','segment','application_date'],axis=1,inplace=True)\ntrain_s2.drop(['case_count','segment','application_date'],axis=1,inplace=True)","3e441a72":"test_s1.drop(['application_date'],axis=1,inplace=True)\ntest_s2.drop(['application_date'],axis=1,inplace=True)","2700c0da":"kf=GroupKFold(n_splits=20)\ns1models = []\ns2models = []\n\nX = train_s1\ny = y1\nloss = []\n\nprint(\"loss:\")\ngrp = train_s1['day'].values\nfor train_index, test_index in kf.split(X,y,grp):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model=RandomForestRegressor(n_estimators = 150 ,random_state=42,max_features =8)\n    model.fit(X_train,y_train)\n    preds=model.predict(X_test)\n    print(mean_absolute_percentage_error(y_test,preds))\n    loss.append(mean_absolute_percentage_error(y_test,preds))\n    s1models.append(model.predict(test_s1))","f5213a51":"s1models = s1models[1:19]","7eb6397f":"X = train_s2\ny = y2\nloss = []\nprint(\"loss : \")\ngrp = train_s2['dayofyear'].values\nfor train_index, test_index in kf.split(X,y,grp):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    model=RandomForestRegressor(n_estimators=150,random_state=42,max_features=8)\n    model.fit(X_train,y_train)\n    preds=model.predict(X_test)\n    print(mean_absolute_percentage_error(y_test,preds))\n    loss.append(mean_absolute_percentage_error(y_test,preds))\n    s2models.append(model.predict(test_s2))","b89c2412":"del s2models[2]","cf46d4b1":"test.loc[test.segment==1, 'case_count']=np.mean(s1models,0)\ntest.loc[test.segment==2, 'case_count']=np.mean(s2models,0)","d44c885f":"test.to_csv('submission.csv',index=False) ","14950fa3":"Based on the segment, We will split dataset into two.","30182e5e":"# LTFS Data Science FinHack 2\n\nLTFS receives a lot of requests for its various finance offerings that include housing loan, two-wheeler loan, real estate financing and micro loans. The number of applications received is something that varies a lot with season. Going through these applications is a manual process and is tedious. Accurately forecasting the number of cases received can help with resource and manpower management resulting into quick response on applications and more efficient processing.\n\n## Problem Statement\nYou have been appointed with the task of forecasting daily cases for next 3 months for 2 different business segments aggregated at the country level keeping in consideration the following major Indian festivals (inclusive but not exhaustive list): Diwali, Dussehra, Ganesh Chaturthi, Navratri, Holi etc. (You are free to use any publicly available open source external datasets). Some other examples could be:\n\nWeather Macroeconomic variables Note that the external dataset must belong to a reliable source.\n\nData Dictionary The train data has been provided in the following way:\n\n* For business segment 1, historical data has been made available at branch ID level For business segment 2, historical data has been made available at State level.\n\nTrain File Variable Definition application_date Date of application segment Business Segment (1\/2) branch_id Anonymised id for branch at which application was received state State in which application was received (Karnataka, MP etc.) zone Zone of state in which application was received (Central, East etc.) case_count (Target) Number of cases\/applications received\n\nTest File Forecasting needs to be done at country level for the dates provided in test set for each segment.\n\nVariable Definition id Unique id for each sample in test set application_date Date of application segment Business Segment (1\/2)\n\n### Evaluation\nEvaluation Metric The evaluation metric for scoring the forecasts is **MAPE (Mean Absolute Percentage Error)* M with the formula:\n\n\nWhere At is the actual value and Ft is the forecast value.\n\nThe Final score is calculated using MAPE for both the segments using the formula:","c91764b3":"It seems that value was simulated one. So, lets review some of higher case count values","90ab4610":"Case counts are rightly skewed let's analyse the maximum value data","919b4207":"Observation : \nMaximum case count recorded date was 1 year earlier than the given data\nIt was a Saturday\nThe count was higher might be either employee has to finish their target or simulated value. So, Let's analyse the same day on previous and next year","fe0ecfd8":"Outlier detection","90344a7a":"**Reference taken from this github link**\n\nhttps:\/\/github.com\/rajat5ranjan\/AV-LTFS-Data-Science-FinHack-2","79217c6a":"Outlier detection"}}