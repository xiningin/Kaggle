{"cell_type":{"2f5a96ca":"code","cb71e918":"code","642a8501":"code","b834ef15":"code","3ee92ba5":"code","036d8988":"code","48e3da9e":"code","5d68ecfd":"code","a622a862":"code","cdc8203b":"code","c9c52138":"code","a8fb27dc":"code","7c048ee2":"code","0b443ffc":"code","1cf4b10c":"markdown","9c582f9f":"markdown","fc907371":"markdown","c8d83b2d":"markdown","dafd2415":"markdown","8322bf45":"markdown","beb1153c":"markdown","7fef83a7":"markdown","6eaf0492":"markdown","833b1a7e":"markdown","168f15cb":"markdown","881122ec":"markdown","ec2d0d05":"markdown","4f922ec0":"markdown","280b88a4":"markdown","b29c9c9b":"markdown","bb21e9f9":"markdown","33daf8e6":"markdown","44a393eb":"markdown","2cc4ceb8":"markdown","6224e5cc":"markdown","014211bb":"markdown","540ab8f1":"markdown","99442e32":"markdown","a82b6edb":"markdown","a076dae9":"markdown"},"source":{"2f5a96ca":"# Importando pacotes\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport sklearn\nimport sklearn.datasets\nimport sklearn.linear_model\nimport matplotlib\n\n# Configurando vari\u00e1vel de tamanho de gr\u00e1ficos\n%matplotlib inline\nmatplotlib.rcParams['figure.figsize'] = (6.0, 4.0)","cb71e918":"# Gerar o conjunto de dados\nnp.random.seed(0) #Garante que todos n\u00f3s tenhamos o mesmo resultado\nX, y = sklearn.datasets.make_moons(200, noise=0.20)\nplt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral)","642a8501":"# Treina um classificador com os par\u00e2metros padr\u00e3o\nclf = sklearn.linear_model.LogisticRegressionCV()\nclf.fit(X, y)","b834ef15":"# Fun\u00e7\u00e3o que vai nos ajudar a separar as regi\u00f5es de cada classe no gr\u00e1fico, conforme os par\u00e2metros do classificador\n# N\u00e3o se preocupe em entender os c\u00f3digos do matplotlib \ndef plot_decision_boundary(pred_func):\n    # Set min and max values and give it some padding\n    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n    h = 0.01\n    # Generate a grid of points with distance h between them\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n    # Predict the function value for the whole gid\n    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    # Plot the contour and training examples\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)","3ee92ba5":"# Imprime a fronteira de decis\u00e3o do classificador\nplot_decision_boundary(lambda x: clf.predict(x))\nplt.title(\"Logistic Regression\")","036d8988":"num_examples = len(X) # tamanho do conjunto de treino - n\u00famero de exemplos\nnn_input_dim = 2 # dimens\u00e3o da camada de entrada\nnn_output_dim = 2 # dimens\u00e3o da camada de sa\u00edda\n\n# Par\u00e2metros do Gradiente Descendente.\nepsilon = 0.01 # Taxa de aprendizado\nreg_lambda = 0.01 # Regulariza\u00e7\u00e3o","48e3da9e":"# Avalia o custo m\u00e9dio total da nossa RN em rela\u00e7\u00e3o aos dados.\ndef calculate_loss(model):\n    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n    # Forward propagation to calculate our predictions\n    z1 = X.dot(W1) + b1\n    a1 = np.tanh(z1)\n    z2 = a1.dot(W2) + b2\n    exp_scores = np.exp(z2)\n    probs = exp_scores \/ np.sum(exp_scores, axis=1, keepdims=True)\n    # Calculating the loss\n    corect_logprobs = -np.log(probs[range(num_examples), y])\n    data_loss = np.sum(corect_logprobs)\n    # Add regulatization term to loss (optional)\n    data_loss += reg_lambda\/2 * (np.sum(np.square(W1)) + np.sum(np.square(W2)))\n    return 1.\/num_examples * data_loss","5d68ecfd":"# Fun\u00e7\u00e3o que usa os par\u00e2metros da nossa RN para prever 0 ou 1.\n# Note que a \u00faltima linha apenas troca as probabilidades por um valor inteiro 0 ou 1.\ndef predict(model, x):\n    W1, b1, W2, b2 = model['W1'], model['b1'], model['W2'], model['b2']\n    # Forward propagation\n    z1 = x.dot(W1) + b1\n    a1 = np.tanh(z1)\n    z2 = a1.dot(W2) + b2\n    exp_scores = np.exp(z2)\n    probs = exp_scores \/ np.sum(exp_scores, axis=1, keepdims=True)\n    return np.argmax(probs, axis=1)","a622a862":"# Essa fun\u00e7\u00e3o aprende os par\u00e2metros w1,w2,b1 e b2 da nossa rede. \n# A cada passada os par\u00e2metros v\u00e3o se ajustando para diminuir o erro\n# - nn_hdim: n\u00famero de n\u00f3s da camada escondida\n# - num_passes: N\u00famero de itera\u00e7\u00f5es do Gradiente Descendente\n# - print_loss: Se verdadeiro imprime o custo a cada 1000 itera\u00e7\u00f5es\n\ndef build_model(nn_hdim, num_passes=20000, print_loss=False):\n    \n    # Initialize the parameters to random values. We need to learn these.\n    np.random.seed(0)\n    W1 = np.random.randn(nn_input_dim, nn_hdim) \/ np.sqrt(nn_input_dim)\n    b1 = np.zeros((1, nn_hdim))\n    W2 = np.random.randn(nn_hdim, nn_output_dim) \/ np.sqrt(nn_hdim)\n    b2 = np.zeros((1, nn_output_dim))\n\n    # This is what we return at the end\n    model = {}\n    \n    # Gradient descent. For each batch...\n    for i in range(0, num_passes):\n\n        # Forward propagation\n        z1 = X.dot(W1) + b1\n        a1 = np.tanh(z1)\n        z2 = a1.dot(W2) + b2\n        exp_scores = np.exp(z2)\n        probs = exp_scores \/ np.sum(exp_scores, axis=1, keepdims=True)\n\n        # Backpropagation\n        delta3 = probs\n        delta3[range(num_examples), y] -= 1\n        dW2 = (a1.T).dot(delta3)\n        db2 = np.sum(delta3, axis=0, keepdims=True)\n        delta2 = delta3.dot(W2.T) * (1 - np.power(a1, 2))\n        dW1 = np.dot(X.T, delta2)\n        db1 = np.sum(delta2, axis=0)\n\n        # Add regularization terms (b1 and b2 don't have regularization terms)\n        dW2 += reg_lambda * W2\n        dW1 += reg_lambda * W1\n\n        # Gradient descent parameter update\n        W1 += -epsilon * dW1\n        b1 += -epsilon * db1\n        W2 += -epsilon * dW2\n        b2 += -epsilon * db2\n        \n        # Assign new parameters to the model\n        model = { 'W1': W1, 'b1': b1, 'W2': W2, 'b2': b2}\n        \n        # Optionally print the loss.\n        # This is expensive because it uses the whole dataset, so we don't want to do it too often.\n        if print_loss and i % 1000 == 0:\n              print(\"Custo ap\u00f3s a itera\u00e7\u00e3o %i: %f\" %(i, calculate_loss(model)))\n    \n    return model","cdc8203b":"# Constroi o modelo\nmodel = build_model(3, print_loss=True)\n\n# Imprime a fronteira de classifica\u00e7\u00e3o\nplot_decision_boundary(lambda x: predict(model, x))\nplt.title(\"Fronteira de classifica\u00e7\u00e3o para 3 neur\u00f4nios\")","c9c52138":"plt.figure(figsize=(16, 32))\nhidden_layer_dimensions = [1, 2, 3, 4, 5, 20, 50]\nfor i, nn_hdim in enumerate(hidden_layer_dimensions):\n    plt.subplot(5, 2, i+1)\n    plt.title('Tamanho da camada escondida %d' % nn_hdim)\n    model = build_model(nn_hdim)\n    plot_decision_boundary(lambda x: predict(model, x))\nplt.show()","a8fb27dc":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD\nfrom keras.utils import np_utils","7c048ee2":"#Converte o y para o formato do keras (1 coluna para cada classe)\ny2 = np_utils.to_categorical(y)","0b443ffc":"model = Sequential()\nmodel.add(Dense(3, input_dim=X.shape[1], activation='tanh', name=\"hidden\"))\nmodel.add(Dense(2, activation=\"softmax\", name=\"output\"))\n\nsgd = SGD(lr=1)\nmodel.compile(loss='categorical_crossentropy', optimizer=sgd)\nmodel.summary()\nmodel.fit(X, y2, epochs=500, shuffle=True, verbose=0)\nplot_decision_boundary(lambda x: model.predict_classes(x))","1cf4b10c":"### Primeira tentativa: Uma RN com camada escondida de 3 neur\u00f4nios","9c582f9f":"N\u00f3s podemos escolher o n\u00famero de neur\u00f4nios da camada escondida. Quanto mais neur\u00f4nios, maior a capacidade da RN aprender fun\u00e7\u00f5es mais complexas, mas a alta dimensionalidade vem com um custo: maior tempo de computa\u00e7\u00e3o e maior chance de *overfiting*\n\nEmbora existam algumas recomenda\u00e7\u00f5es para a escolha do tamanho, n\u00e3o \u00e9 uma defini\u00e7\u00e3o que funciona sempre. Aqui n\u00f3s iremos apenas experimentar com algumas quantidades.","fc907371":"O gr\u00e1fico mostra que a fronteira da decis\u00e3o \u00e9 linear.","c8d83b2d":"## Gerando o conjunto de dados\n\n\n\nVamos come\u00e7ar gerando os dados que vamos usar. Felizmente, o [scikit-learn](http:\/\/scikit-learn.org\/) tem v\u00e1rios geradores de conjuntos de dados embutido e vamos usar o [`make_moons`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_moons.html).","dafd2415":"$$\n\\begin{aligned}\nz_1 & = xW_1 + b_1 \\\\\na_1 & = \\tanh(z_1) \\\\\nz_2 & = a_1W_2 + b_2 \\\\\na_2 & = \\hat{y} = \\mathrm{softmax}(z_2)\n\\end{aligned}\n$$","8322bf45":"Outra decis\u00e3o \u00e9 a escolha da fun\u00e7\u00e3o de ativa\u00e7\u00e3o. Uma fun\u00e7\u00e3o de ativa\u00e7\u00e3o n\u00e3o linear \u00e9 o que nos permite aprender fun\u00e7\u00f5es n\u00e3o lineares. Escolhas comuns s\u00e3o [tanh](https:\/\/reference.wolfram.com\/language\/ref\/Tanh.html), [sigmoid function](https:\/\/en.wikipedia.org\/wiki\/Sigmoid_function), ou [ReLUs](https:\/\/en.wikipedia.org\/wiki\/Rectifier_(neural_networks). \n\nN\u00f3s iremos usar `tanh` que tem bom desempenho em muitos casos. Uma boa propriedade dessa fun\u00e7\u00e3o \u00e9 que a derivada de  $\\tanh x$ \u00e9 $1-\\tanh^2 x$. Isso nos economiza tempo de computa\u00e7\u00e3o, pois precisamos calcular apenas uma vez a fun\u00e7\u00e3o. ","beb1153c":"# Implementando uma Rede Neuronal do zero\n\nNesse laborat\u00f3rio iremos demonstrar como implementar uma rede neuronal do zero, sem uso de bibliotecas de aprendizado de m\u00e1quina. O exerc\u00edcio serve para entender como funciona internamente uma rede neuronal, mas n\u00e3o recomendamos implementar do zero. H\u00e1 diversos frameworks que j\u00e1 implementam redes neuronais com desempenho excepcional e praticamente livre de defeitos. No fim desse laborat\u00f3rio mostraremos o [Keras](https:\/\/keras.io\/), que \u00e9 a biblioteca que usaremos para os demais laborat\u00f3rios de <i>Deep Learning<\/i>.\n\nEsse laborat\u00f3rio foi retirado e traduzido desse [link](http:\/\/www.wildml.com\/2015\/09\/implementing-a-neural-network-from-scratch\/)\n","7fef83a7":"Primeiro implementamos o c\u00e1lculo da fun\u00e7\u00e3o de custo.","6eaf0492":"$$\n\\begin{aligned}\n& \\delta_3 = y - \\hat{y} \\\\\n& \\delta_2 = (1 - \\tanh^2z_1) \\circ \\delta_3W_2^T \\\\\n& \\frac{\\partial{L}}{\\partial{W_2}} = a_1^T \\delta_3  \\\\\n& \\frac{\\partial{L}}{\\partial{b_2}} = \\delta_3\\\\\n& \\frac{\\partial{L}}{\\partial{W_1}} = x^T \\delta_2\\\\\n& \\frac{\\partial{L}}{\\partial{b_1}} = \\delta_2 \\\\\n\\end{aligned}\n$$","833b1a7e":"Embora a f\u00f3rmula pare\u00e7a complicada, ela simplesmente calcula o logaritmo da previs\u00e3o (probabilidade) caso a nossa estimativa esteja errada. Depois tira a m\u00e9dia desses logaritmos.","168f15cb":"Como n\u00f3s queremos que nossa sa\u00edda seja uma probabilidade entre 0 e 1 (quanto maior, mais chance de ser da classe 1), usamos a fun\u00e7\u00e3o [softmax](https:\/\/en.wikipedia.org\/wiki\/Softmax_function) de ativa\u00e7\u00e3o no neur\u00f4nio de sa\u00edda. Ela \u00e9 uma fun\u00e7\u00e3o que converte as sa\u00eddas da RN em probabilidades.","881122ec":"# Usando o Keras\n\nNessa se\u00e7\u00e3o demonstramos como construir a mesma rede com 3 neur\u00f4nios escondidos usando o Keras.","ec2d0d05":"$z_i$ \u00e9 a entrada da camada $i$ e $a_i$ \u00e9 a sa\u00edda da camada $i$ depois de aplicar a fun\u00e7\u00e3o de ativa\u00e7\u00e3o. $W_1, b_1, W_2, b_2$ s\u00e3o os par\u00e2metros da nossa RN que queremos aprender. Voc\u00ea pode pensar nesses par\u00e2metros como matrizes transformando os dados entre as camadas da sua RN. Olhando as multiplica\u00e7\u00f5es matriciais abaixo n\u00f3s podemos deduzir as dimens\u00f5es de cada matriz.Se n\u00f3s usarmos 500 neur\u00f4nios na camada escondida, ent\u00e3o $W_1 \\in \\mathbb{R}^{2\\times500}$, $b_1 \\in \\mathbb{R}^{500}$, $W_2 \\in \\mathbb{R}^{500\\times2}$, $b_2 \\in \\mathbb{R}^{2}$. Agora ficou f\u00e1cil de ver o impacto do aumento do n\u00famero de neur\u00f4nios no tamanho das matrizes e consequentemente no n\u00famero de par\u00e2metros que temos de aprender.","4f922ec0":"Vamos agora construir uma Rede Neuronal (RN) com uma camada de entrada, uma escondida e uma de sa\u00edda. O n\u00famero de neur\u00f4nios na camada de entrada \u00e9 determinado pela dimensionalidade dos dados (n\u00famero de *features*). No nosso caso ser\u00e3o 2 (x, y). De forma similar, o n\u00famero de neur\u00f4nios da camada de sa\u00edda \u00e9 definido pelo n\u00famero de classes do nosso problema. No caso especial de termos 2 classes apenas, como \u00e9 o caso, podemos usar apenas um neur\u00f4nio que codificar\u00e1 0 para uma classe e 1 para a outra.\n\nA nossa RN se parecer\u00e1 com a figura abaixo.\n<img src='https:\/\/api.ning.com\/files\/GNAEZE7oZyk4*ybdpkm3CKvkjsvJgX97URSW1ISY2og9Mw6SxctGEpBwNbRWdQMXZS7DW*fm8bc-nZfqc2fKQkNfsnr-v4di\/nn3fromscratch3layernetwork1024x693.png' style='width: 50%'\/>","280b88a4":"Podemos ver que ao aumentar o n\u00famero de neur\u00f4nios da camada escondida, conseguimos separar melhor os dados, mas observamos igualmente que os modelos gerados s\u00e3o mais sucet\u00edveis a *overfiting*, ou seja, v\u00e3o generalizar menos o problema que temos em m\u00e3os.","b29c9c9b":"## Regress\u00e3o Log\u00edstica\n\nApenas para demonstrar o que afirmamos no par\u00e1grafo anterior, vamos usar a Regress\u00e3o Log\u00edstica do Scikit-learn","bb21e9f9":"### Implementa\u00e7\u00e3o\n\nAgora vamos a um pouco de c\u00f3digo. Come\u00e7aremos definindo os par\u00e2metros para nossa RN e para o gradiente descendente.","33daf8e6":"O conjunto de dados gerado tem duas classes (pontos vermelhos e azuis) e duas vari\u00e1veis (eixo X e Y). \n\nNosso objetivo \u00e9 criar uma fun\u00e7\u00e3o capaz de classificar os azuis e vermelhos corretamente, dado um par de valores (x,y). \u00c9 importante salientar que os dados n\u00e3o s\u00e3o *linearmente separ\u00e1veis*, ou seja, n\u00e3o \u00e9 poss\u00edvel com uma reta separar as duas classes no plano. Nesse caso, separadores lineares como Regress\u00e3o Log\u00edstica n\u00e3o \u00e9 capaz de separar adequadamente as duas classes, a menos que seja feita alguma engenharia com os atributos x e y gerando novos atributos.\n\nNa verdade, uma das grandes vantagens de Redes Neuronais \u00e9 n\u00e3o ter de se preocupar com [Engenharia de Features](http:\/\/machinelearningmastery.com\/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it\/), pois a camada escondida ir\u00e1 aprender novas features automaticamente.","44a393eb":"# Testando outros tamanhos\n","2cc4ceb8":"Agora implementamos o *forward propagation* que faz as previs\u00f5es","6224e5cc":"### Aprendendo os par\u00e2metros\n\nAprender os par\u00e2metros da nossa rede significa encontrar os valores para nossas matrizes ($W_1, b_1, W_2, b_2$) que minimizam o erro nos nossos dados de treino. Mas como definimos o que \u00e9 o erro? N\u00f3s chamamos a fun\u00e7\u00e3o que define o erro de *fun\u00e7\u00e3o de custo (loss)*. Quando falamos de problemas de classifica\u00e7\u00e3o, a escolha natural \u00e9 [cross-entropy loss](https:\/\/en.wikipedia.org\/wiki\/Cross_entropy#Cross-entropy_error_function_and_logistic_regression). Se temos $N$ exemplos de treinamento (pares de x e y) e $C$ classes ent\u00e3o o nosso custo para nossa previs\u00e3o  $\\hat{y}$ em rela\u00e7\u00e3o \u00e0s classes reais $y$ dos nossos dados \u00e9 dado por:\n\n$$\n\\begin{aligned}\nL(y,\\hat{y}) = - \\frac{1}{N} \\sum_{n \\in N} \\sum_{i \\in C} y_{n,i} \\log\\hat{y}_{n,i}\n\\end{aligned}\n$$\n\n","014211bb":"Agora a nossa fun\u00e7\u00e3o que treina a RN","540ab8f1":"O resultado parece muito bom. Parece que com apenas 3 neur\u00f4nios nossa RN foi capaz de separar bem as duas classes.","99442e32":"## Treinando uma rede neuronal","a82b6edb":"### Como a Rede Neuronal faz as previs\u00f5es (ou estimativas)\n\nNossa RN faz previs\u00f5es usando *forward propagation*, que nada mais \u00e9 que algumas multiplica\u00e7\u00f5es matriciais e aplica\u00e7\u00e3o de nossas fun\u00e7\u00f5es de ativa\u00e7\u00e3o. Sendo $x$ nossa matriz de dimens\u00e3o $(2,n)$, n\u00f3s calculamos nossa previs\u00e3o $\\hat{y}$ tamb\u00e9m de dimens\u00e3o $(2,n)$* da seguinte forma:","a076dae9":"Nosso objetivo \u00e9 aprender os par\u00e2metros que minimizam a fun\u00e7\u00e3o de custo. Para isso vamos usar o m\u00e9todo do [gradient descent](http:\/\/cs231n.github.io\/optimization-1\/). \n\nComo entrada para o m\u00e9todo do gradiente descendente, precisamos calcular os gradientes (que \u00e9 um vetor que indica o sentido\/dire\u00e7\u00e3o de crescimento de uma fun\u00e7\u00e3o) da nossa fun\u00e7\u00e3o de custo em rela\u00e7\u00e3o aos nossos par\u00e2metros. Para isso calculamos as derivadas parciais da fun\u00e7\u00e3o de custo em rela\u00e7\u00e3o a cada um dos par\u00e2metros: $\\frac{\\partial{L}}{\\partial{W_1}}$, $\\frac{\\partial{L}}{\\partial{b_1}}$, $\\frac{\\partial{L}}{\\partial{W_2}}$, $\\frac{\\partial{L}}{\\partial{b_2}}$. Para calcular os gradientes usamos o m\u00e9todo de *backpropagation*, que \u00e9 uma forma eficiente de calcular os gradientes come\u00e7ando da sa\u00edda da nossa RN. N\u00e3o entraremos em detalhes de como funciona e n\u00e3o se preocupe em entender as f\u00f3rumas abaixo. ([Aqui](http:\/\/colah.github.io\/posts\/2015-08-Backprop\/) ou [here](http:\/\/cs231n.github.io\/optimization-2\/)) tem boas explica\u00e7\u00f5es sobre como funciona.\n\nAplicando backpropagation na nossa fun\u00e7\u00e3o de custo temos:"}}