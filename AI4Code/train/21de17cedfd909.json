{"cell_type":{"69bb7e63":"code","fcc0b4a1":"code","825f806d":"code","3f01799b":"code","b96ab685":"code","48d9d234":"code","0ee03732":"code","578ad92a":"code","610da5cd":"code","61c2766a":"code","bea76605":"code","12383b99":"code","388161b4":"code","27104dac":"code","08089be1":"code","93ee0a05":"code","147d0485":"code","fdb26ea9":"code","fff26337":"code","124e0f46":"code","11318cee":"markdown","964982ff":"markdown","632cc9b5":"markdown","8f793d57":"markdown"},"source":{"69bb7e63":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\nimport tensorflow as tf\nimport tensorflow.keras\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization, Input\n\nfrom tqdm import tqdm # \ubc18\ubcf5\ubb38 \uc9c4\ud589 \uc0c1\ud669\uc744 \ud504\ub85c\uadf8\ub798\uc2a4 \ubc14\ub85c \ubcf4\uc5ec\uc8fc\ub294 \ub77c\uc774\ube0c\ub7ec\ub9ac\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau # \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\uac00 \ud5a5\uc0c1\ub418\uc9c0 \uc54a\ub294 \uacbd\uc6b0 lr\uc744 \uc904\uc5ec\uc8fc\ub294 \ud074\ub798\uc2a4\nfrom sklearn.model_selection import train_test_split # \ub370\uc774\ud130\uc14b test-train \ubd84\ub9ac\nfrom sklearn.metrics import confusion_matrix # actual class & predict class\uc758 \uc815\uc624 \ubd84\ub958\ud45c \nimport PIL.Image as Image, PIL.ImageDraw as Draw, PIL.ImageFont as Font # PIL -> image\ub97c \ucc98\ub9ac\ud558\uae30 \uc704\ud55c \ud074\ub798\uc2a4\n\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder # \uc6d0-\ud56b \uc778\ucf54\ub529 -> categorical data\uc77c \ub54c \uc0ac\uc6a9, \ub808\uc774\ube14 \uc778\ucf54\ub354 -> data type\uc774 'object'\uc778 \uceec\ub7fc\uc744 \ucc3e\uc544 \uce74\ud14c\uace0\ub9ac\ud654\ub41c \uc22b\uc790 \uac12\uc73c\ub85c \ubcc0\ud658  \nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler # \uac1c\uc120\uc758 \uc5ec\uc9c0\uac00 \uc5c6\uc744 \ub54c, \ud559\uc2b5\uc744 \uc870\uae30 \uc885\ub8cc\ud558\ub294 function, lr \uc904\uc5ec\uc8fc\ub294 function\n\nfrom tensorflow.keras.models import load_model \nimport seaborn as sb\n\nimport scipy  # \ub300\uc6a9\ub7c9 \ub370\uc774\ud130 \ucc98\ub9ac\uc6a9 \ub77c\uc774\ube0c\ub7ec\ub9ac \nfrom scipy.ndimage import rotate # ndimage -> \uc774\ubbf8\uc9c0\ub97c \ubc30\uc5f4\ub85c \ubcc0\ud658 -> \ucc98\ub9ac\uac00 \uc26c\uc6c0 \n\nimport cv2 # \uc774\ubbf8\uc9c0 & \ube44\ub514\uc624 \uc77d\uae30 (OpenCV)\nimport random\nimport os # \ucef4\ud4e8\ud130 \ub0b4 \ud30c\uc77c \ud655\uc778\/\uc5f4\uae30 \ubaa8\ub4c8 \nfrom glob import glob \nimport time, gc","fcc0b4a1":"path = '..\/input\/natural-images\/natural_images\/'    # \uae30\ubcf8 \uacbd\ub85c \uc9c0\uc815","825f806d":"train_list = glob(os.path.join(path + '**\/', '*.jpg'))   # \ub9ac\uc2a4\ud2b8\ub85c \uc774\ubbf8\uc9c0 \ubc1b\uc544\uc624\uae30  ex) [.....\/0380.jpg, ...\/2951.jpg ...]","3f01799b":"names = os.listdir(path)          # \ub77c\ubca8 \uac2f\uc218, \uc885\ub958 \uc54c\uae30\nnames","b96ab685":"def proc_img(filepath):\n    \"\"\"\uc0ac\uc9c4\uc758 \ub370\uc774\ud130\ud504\ub808\uc784 \ub9cc\ub4e0 \ud6c4, \ub77c\ubca8 \uc815\ud574\uc8fc\uae30\n    \"\"\"\n\n    labels = [str(filepath[i]).split(\"\/\")[-2] for i in range(len(filepath))]\n    \n\n    filepath = pd.Series(filepath, name='Filepath').astype(str)       # \uc0ac\uc9c4\uc8fc\uc18c \uc2dc\ub9ac\uc988\ub85c \ub9cc\ub4e4\uc5b4\uc8fc\uae30\n    labels = pd.Series(labels, name='Label')                          # \ub77c\ubca8 \uc2dc\ub9ac\uc988\ub85c \ub9cc\ub4e4\uc5b4\uc8fc\uae30\n\n    # Concatenate filepaths and labels\n    df = pd.concat([filepath, labels], axis=1)                        # \ud569\uccd0\uc8fc\uae30 axis =1\uc778 \uc774\uc720? -> '\uc5f4'\ub85c \ud569\uce60 \uac83\uc774\uae30 \ub54c\ubb38\n\n    # Shuffle the DataFrame and reset index\n    df = df.sample(frac=1,random_state=0).reset_index(drop = True)    # \uc0ac\uc9c4 \uc11e\uc5b4\uc8fc\uae30, \uae30\uc874 \uc778\ub371\uc2a4 \uc0ad\uc81c\n    \n    return df\n\ndf = proc_img(train_list)\n\nprint(f'Number of pictures: {df.shape[0]}\\n')\nprint(f'Number of different labels: {len(df.Label.unique())}\\n')\nprint(f'Labels: {df.Label.unique()}')\n\n# \ub370\uc774\ud130\ud504\ub808\uc784\uc740 2\uc5f4 ->\ud30c\uc77c\uc8fc\uc18c, \ub77c\ubca8 \ndf.head(5)","48d9d234":"natural = df['Label'].value_counts()\n\ncolors = [\"purple\", \"green\", \"orange\",\"blue\", \"red\", \"pink\", \"skyblue\", \"black\"]\n\nsb.set_style(\"whitegrid\")\nplt.figure(figsize=(16,8))\nplt.title(\"Number of pictures\", fontsize = 25)\nplt.xlabel(\"Type\")\nsb.barplot(x= natural.index, y= natural, palette=colors )\nplt.show()","0ee03732":"fig, axe = plt.subplots(nrows=5, ncols=8, figsize=(20, 10),\n                        subplot_kw={'xticks': [], 'yticks': []})\n\nfor i, ax in enumerate(axe.flat):\n    ax.imshow(plt.imread(df.Filepath[i]))\n    ax.set_title(df.Label[i], fontsize = 12)\nplt.tight_layout(pad=1)\nplt.show()","578ad92a":"# train, test split\n# \uc18d\ub3c4\ub97c \uc704\ud574 60%\ub9cc \ud6c8\ub828\uc6a9\uc73c\uc624 \uc0ac\uc6a9\ud55c\ub2e4.\ntrain_df,test_df = train_test_split(df.sample(frac = 0.6), test_size=0.2,random_state=0)\n# train_df,test_df = train_test_split(df, test_size=0.1,random_state=0)","610da5cd":"def generate():\n    # Load the Images with a generator and Data Augmentation\n    train_generator = tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.1)\n\n    test_generator = tf.keras.preprocessing.image.ImageDataGenerator()\n\n    train_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='training',\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        rotation_range = 30,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    val_images = train_generator.flow_from_dataframe(\n        dataframe=train_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=True,\n        seed=0,\n        subset='validation',\n        zoom_range=0.15,\n        width_shift_range=0.2,\n        height_shift_range=0.2,\n        shear_range=0.15,\n        rotation_range = 30,\n        horizontal_flip=True,\n        fill_mode=\"nearest\"\n    )\n\n    test_images = test_generator.flow_from_dataframe(\n        dataframe=test_df,\n        x_col='Filepath',\n        y_col='Label',\n        target_size=(224, 224),\n        color_mode='rgb',\n        class_mode='categorical',\n        batch_size=32,\n        shuffle=False\n    )\n    \n    return train_generator,test_generator,train_images,val_images,test_images","61c2766a":"imagefil = Sequential()\nimagefil.add(Conv2D(64, kernel_size = (3,3), padding = 'same', activation = 'relu', input_shape = (224, 224, 3)))\nimagefil.add(BatchNormalization())\nimagefil.add(MaxPool2D((2,2)))\n\nimagefil.add(Conv2D(128, kernel_size = (3,3), padding = 'same', activation = 'relu'))\nimagefil.add(BatchNormalization())\nimagefil.add(MaxPool2D((2,2)))\n\nimagefil.add(Conv2D(256, kernel_size = (3,3), padding = 'same', activation = 'relu'))\nimagefil.add(BatchNormalization())\nimagefil.add(Conv2D(256, kernel_size = (3,3), padding = 'same', activation = 'relu'))\nimagefil.add(BatchNormalization())\nimagefil.add(MaxPool2D(pool_size =2, strides =2, padding ='same'))\n\nimagefil.add(Conv2D(512, kernel_size = (3,3), padding = 'same', activation = 'relu'))\nimagefil.add(BatchNormalization())\nimagefil.add(Conv2D(512, kernel_size = (3,3), padding = 'same', activation = 'relu'))\nimagefil.add(BatchNormalization())\nimagefil.add(MaxPool2D(pool_size =2, strides =2, padding ='same'))\n\nimagefil.add(Conv2D(512, kernel_size = (3,3), padding = 'same', activation = 'relu'))\nimagefil.add(BatchNormalization())\nimagefil.add(Conv2D(512, kernel_size = (3,3), padding = 'same', activation = 'relu'))\nimagefil.add(BatchNormalization())\nimagefil.add(MaxPool2D(pool_size =2, strides =2, padding ='same'))\n\nimagefil.add(Flatten())\nimagefil.add(Dense(4096, activation = 'relu'))\nimagefil.add(Dropout(0.5))\nimagefil.add(Dense(4096, activation = 'relu'))\nimagefil.add(Dropout(0.5))\nimagefil.add(Dense(8, activation = 'softmax'))","bea76605":"sgd = tf.keras.optimizers.SGD(learning_rate = 0.015, decay = 1e-8, momentum = 0.9, nesterov = True)\n\nimagefil.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])","12383b99":"imagefil.summary()","388161b4":"train_generator,test_generator,train_images,val_images,test_images = generate()   # generator \uc0dd\uc131\nprint('\\n')\n\ncallbacks_list = [tf.keras.callbacks.EarlyStopping(         # \ubd88\ud544\uc694\ud55c \ud559\uc2b5\uc2dc\uac04 \uc904\uc774\uae30 \uc704\ud574 earlystopping        \n        monitor = 'val_accuracy', patience = 10),\n                  tf.keras.callbacks.ReduceLROnPlateau(     # local minimum\uc5d0 \ube60\uc9c0\uc9c0 \uc54a\uae30 \uc704\ud574 \ud559\uc2b5\ub960 \uc870\uc808\n        monitor='val_loss', factor=0.4, patience=5 ,)] \n\nhistory = imagefil.fit(train_images, validation_data=val_images, epochs=100, validation_steps = len(val_images), callbacks = callbacks_list)    ","27104dac":"plt.figure(figsize = (15,8))\nloss_ax = plt.subplot(1,2,1)\nloss_ax.plot(history.history['loss'],'red', label = 'Train Loss')\nloss_ax.plot(history.history['val_loss'],'blue', label = 'Val Loss')\n\nloss_ax.set_xlabel('epochs')\nloss_ax.set_ylabel('Loss')\n\nloss_ax.legend(loc = 'upper right')\n\nacc_ax = plt.subplot(1,2,2)\nacc_ax.plot(history.history['accuracy'],'orange', label = 'Train Accuracy')\nacc_ax.plot(history.history['val_accuracy'],'purple', label = 'Val Accuracy')\n\nacc_ax.set_xlabel('epochs')\nacc_ax.set_ylabel('Accuracy')\n\nacc_ax.legend(loc = 'upper right')\nplt.show()","08089be1":"pred = imagefil.predict(test_images)\npred = np.argmax(pred, axis = 1)","93ee0a05":"pred[:20]","147d0485":"from sklearn.metrics import classification_report,accuracy_score\nfrom IPython.display import Markdown, display\n\n\n# Map the label\nlabels = (train_images.class_indices)\nlabels = dict((v,k) for k,v in labels.items())","fdb26ea9":"labels","fff26337":"pred = [labels[k] for k in pred]\n\ny_test = list(test_df.Label)\nacc = accuracy_score(y_test,pred)\ntotal_acc = round(acc,4)\n#     printmd(f'**{name} has a {acc * 100:.2f}% accuracy on the test set**')","124e0f46":"print(total_acc)","11318cee":"## 4. \ud6c8\ub828 \ubc0f \uacb0\uacfc","964982ff":"## 2. \ub370\uc774\ud130 Augmentation\n\n- train data\uc218\uac00 \uc801\uc74c -> \ub370\uc774\ud130 \ubd88\ub824\uc8fc\uae30","632cc9b5":"## 1. \ub370\uc774\ud130 \uc804\ucc98\ub9ac \ubc0f \uc2dc\uac01\ud654","8f793d57":"## 3. \ubaa8\ub378 \uc124\uc815\n\n- input \ud06c\uae30\ub294 \uc704\uc5d0\uc11c \uc124\uc815\ud55c target size\uc640 \uac19\uac8c \ud55c\ub2e4. (224, 224)"}}