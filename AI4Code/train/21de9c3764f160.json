{"cell_type":{"7a81e3ea":"code","3972b5e1":"code","5c4f511d":"code","2db08e30":"code","8065cee2":"code","9abfe14e":"code","352ded5a":"code","707a0935":"code","f139d16f":"code","4f416a02":"code","9c4357c5":"code","07c01a1d":"code","cbeb19a2":"code","58e08596":"code","ae42e659":"code","4e50a06b":"code","53bbe74b":"code","b0d0b132":"code","dcce1ee7":"code","462e42a5":"code","572e64e7":"code","2644f75a":"markdown","b28645c2":"markdown","a3bb3cf1":"markdown","785e1d19":"markdown","6ca61e21":"markdown","20a4203e":"markdown","13e8c3e4":"markdown","609bf3d6":"markdown","f15918f3":"markdown","bad24b60":"markdown","f5c343fc":"markdown","e27ea3d6":"markdown","d179e110":"markdown","484118b3":"markdown","9909921c":"markdown","f28c4f59":"markdown","36d37d3f":"markdown","a39aa2f0":"markdown","358b78f1":"markdown","eb61e117":"markdown","47fbe7f4":"markdown","676056f3":"markdown","b2357356":"markdown","74ac06ba":"markdown","3e32c74c":"markdown","b9c0f4a3":"markdown"},"source":{"7a81e3ea":"# import data science libraries\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\n###so that graphs are loaded in the notebook only\n%matplotlib inline \n\n# Import tensorflow\nimport tensorflow as tf","3972b5e1":"###loading MNIST dataset which is inside tensorflow libray only\nfrom tensorflow.keras.datasets import mnist\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()","5c4f511d":"images = X_train[:3]\nlabels = y_train[:3]","2db08e30":"# Let's see the first 3 images and corresponding labels in our training data\nimages = X_train[:3]\nlabels = y_train[:3]\n\nfor index, image in enumerate(images):\n    print ('Label:', labels[index])\n    print ('Digit in the image', np.argmax(labels[index]))  #argmax picks out the label with highest probability\n    plt.imshow(image.reshape(28,28),cmap='gray')\n    plt.show()","8065cee2":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.optimizers import SGD\n##Tensorboard as we discussed is for visualisation\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.utils import to_categorical\nfrom datetime import datetime\n\n","9abfe14e":"### Reshaping the inputs\n\nX_train = X_train.reshape(60000, 784)\nX_test = X_test.reshape(10000, 784)","352ded5a":"### Converting the outputs to_categorical \n\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)","707a0935":"##Defining sequential API\nmodel1 = Sequential()\n##input layer\nmodel1.add(Dense(50, activation='relu', input_shape= (784,) ))\n\n##we will use softmax as activation function because this is a multiclass classification problem.\n### if you dont know what softmax , you need to understand it mathematically.\n#Intuitively what it does is it converts numbers into probabilities.\n###find out more here --> https:\/\/www.youtube.com\/watch?v=p-XCC0y8eeY\nmodel1.add(Dense(10, activation='softmax'))\nmodel1.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])","f139d16f":"## for tensorboard visualisation you can check out the following link: https:\/\/youtu.be\/Uzkhn5ENJzQ\nlogdir = \"logs\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = TensorBoard(log_dir=logdir)","4f416a02":"training_history = model1.fit(\n    X_train, # input\n    y_train, # output\n    batch_size=32,\n    verbose=1, # Suppress chatty output; use Tensorboard instead\n    epochs=10,\n    validation_data=(X_test, y_test),\n    callbacks=[tensorboard_callback],\n)","9c4357c5":"model2 = Sequential()\n##10 Neurons layer\nmodel2.add(Dense(10, activation='relu', input_shape= (784,) ))\n\n##Many 512 Neuron Layer...try to play with the number of layers (increase or decrease)\nmodel2.add(Dense(512, activation='relu'))\nmodel2.add(Dense(512, activation='relu'))\nmodel2.add(Dense(512, activation='relu'))\nmodel2.add(Dense(512, activation='relu'))\nmodel2.add(Dense(512, activation='relu'))\nmodel2.add(Dense(512, activation='relu'))\nmodel2.add(Dense(10, activation='softmax'))\nmodel2.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])","07c01a1d":"logdir = \"logs\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = TensorBoard(log_dir=logdir)","cbeb19a2":"training_history = model2.fit(\n    X_train, # input\n    y_train, # output\n    batch_size=32,\n    verbose=1, # Suppress chatty output; use Tensorboard instead\n    epochs=10,\n    validation_data=(X_test, y_test),\n    callbacks=[tensorboard_callback],\n)","58e08596":"from tensorflow.keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)","ae42e659":"model3 = Sequential()\nmodel3.add(Dense(128, activation='relu', input_shape= (784,) ))\nmodel3.add(Dense(256, activation='relu'))\nmodel3.add(Dense(128, activation='relu'))\nmodel3.add(Dense(10, activation='softmax'))\nmodel3.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01), metrics=['accuracy'])","4e50a06b":"logdir = \"logs\/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\ntensorboard_callback = TensorBoard(log_dir=logdir)","53bbe74b":"training_history = model3.fit(\n    X_train, # input\n    y_train, # output\n    batch_size = 32,\n    verbose = 1, # Suppress chatty output; use Tensorboard instead\n    epochs = 10,\n    validation_data = (X_test, y_test),\n    callbacks = [tensorboard_callback, es],\n)","b0d0b132":"from IPython.display import Image\nImage(url=\"https:\/\/miro.medium.com\/max\/1200\/1*iWQzxhVlvadk6VAJjsgXgg.png\", width=800, height=500)","dcce1ee7":"from tensorflow.keras.layers import Dropout\nfrom tensorflow.keras import regularizers","462e42a5":"model4 = Sequential()\nmodel4.add(Dense(10, activation='relu', input_shape= (784,) ))\nmodel4.add(Dropout(0.2)) ###using dropout\nmodel4.add(Dense(256,activation='relu', kernel_regularizer = 'l2'))###using regularizer\nmodel4.add(Dropout(0.2))###using dropout\nmodel4.add(Dense(256,activation='relu', kernel_regularizer = 'l2')) ###using regularizer\nmodel4.add(Dropout(0.2))###using dropout\nmodel4.add(Dense(10, activation='softmax'))\nmodel4.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.001), metrics=['accuracy'])","572e64e7":"training_history = model4.fit(\n    X_train, # input\n    y_train, # output\n    batch_size=32,\n    verbose=1, # Suppress chatty output; use Tensorboard instead\n    epochs=10,\n    validation_data=(X_test, y_test),\n    callbacks=[tensorboard_callback, es],\n)","2644f75a":"### Understanding Overfitting and Underfitting","b28645c2":"`Overfitting` is a scenario where the model performs well on training data but performs poorly on data not seen during training. This basically means that the model has memorized the training data instead of learning the relationships between features and labels. Overfitting is a situation where the model has high variance, memorizing the random noise in the training set.","a3bb3cf1":"#### Why not just use early stopping rather than regularisation ?\n\nThe main downside of early stopping is that this couples two tasks:\n\n1. Algorithm to optimize the cost function j (eg gradient descent, adam etc)\n2. Prevent overfitting (ie get more data, regularization)\n\nbecause by stopping gradient decent early, we are sort of breaking whatever we are doing to optimize cost function J and simultaneously trying to not over fit. Rather than using early stopping, one alternative is just use L2 regularization then we can just train the neural network as long as possible(the downside here is we have to try a lot of values of the regularization parameter lambda and hence it becomes computationally expensive)","785e1d19":"### Conclusion\n\n- Large weights in a neural network are a sign of a more complex network that has overfit the training data.\n- Probabilistically dropping out nodes in the network is a simple and effective regularization method.\n- A large network with more training and the use of a weight constraint are suggested when using dropout.","6ca61e21":"\n\nAdding more layers increases the accuracy here, but it might lead to **Overfitting** or **generalisation error**.","20a4203e":"Overfitting is easy to diagnose with the accuracy(or loss) visualizations you have available. If \"Accuracy\" (measured against the training set) is very good and \"Validation Accuracy\" (measured against a validation set) is not as good, then your model is overfitting. (as seen in image)","13e8c3e4":"*By dropping a unit out, we mean temporarily removing it from the network, along with all its incoming and outgoing connections*\n\nDropout has the effect of making the training process noisy, forcing nodes within a layer to probabilistically take on more or less responsibility for the inputs.\nThis conceptualization suggests that perhaps dropout breaks-up situations where network layers co-adapt to correct mistakes from prior layers, in turn making the model more robust.","609bf3d6":"#### Topics in Optimize a neural network:\n1. Understanding Overfitting and Underfitting\n2. Strategies to avoid overfitting\/underfitting\n3. Effect of number of layers and number of neurons using MNSIT Dataset\n4. Early Stopping\n5. Regularization\n6. Dropouts\n7. Conclusion","f15918f3":"How to get through this :\n- Either limit the number of Epoch (not preferred)\n- Use EarlyStopping criteria (preferred)","bad24b60":"Usually we have overfitting rather than underfitting in Neural Networks.","f5c343fc":"`Neural network with only one input and output layer`","e27ea3d6":"### Regularization\n\nRegularization is a technique which makes slight modifications to the learning algorithm such that the model generalizes better. This in turn improves the model\u2019s performance on the unseen data as well. \n\nAdding more than required layers might also lead to overfitting. \n\n*By looking at the graph below can you guess, what should be an ideal model complexity.*","d179e110":"The training stopped because the *val_loss* was not reducing and it checked this till 5 epochs since we mentioned *patience* = 5","484118b3":"![ooverfitting](https:\/\/github.com\/dphi-official\/Deep_Learning_Bootcamp\/blob\/master\/1.png?raw=true)","9909921c":"### Dropouts\n\nDropout is a regularization method that approximates training a large number of neural networks with different architectures in parallel. As mentioned, it is preferrably used when training a large neural network. ","f28c4f59":"EarlyStopping basically stops training at the point when performance on a validation dataset starts to degrade. (It would by default need a validation set to be able to work).\n\n```python\nfrom tensorflow.keras.callbacks import EarlyStopping\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=50)\n```\n`monitor` :  you can choose any metric to monitor like val_loss, val_acc. <br>\n`mode` :  by default it is auto and knows that you want to minimize loss or maximize accuracy. like *min* in case of val_loss. <br>\n`patience`: patience is the number to epochs to wait before stopping it. ex -  If we are chasing *val_loss* and if it is not reducing from the last 50 epoch we stop the training ","36d37d3f":"### Early Stopping\n\nWhen training a large network, there will be a point during training when the model will stop generalizing and start learning the statistical noise in the training dataset.\n\nThis overfitting of the training dataset will result in an increase in generalization error, making the model less useful at making predictions on new data.\n\nThe challenge is to train the network long enough that it is capable of learning the mapping from inputs to outputs, but not training the model so long that it overfits the training data.","a39aa2f0":"### Strategies to avoid overfitting\/underfitting\n\n\nYou can combat overfitting by reducing the complexity of your model (i.e. reducing the number of trainable parameters).This is done by :\n\n- Use fewer layers (shallower networks), fewer neurons per layer (narrow networks)\n- Using **Dropouts**\n- Using **Regularisation**\n- **Early Stopping** in some cases\n\nYou can combat underfitting by:\n\n- increasing the complexity of your model i.e. increasing layers and number of neurons.With more layers, the network can learn more sophisticated relationships and perhaps perform well on difficult real-world tasks.","358b78f1":"### Effect of number of layers and number of neurons using MNSIT Dataset\nHere is a simple example of how number of layers and number of neurons affect the model","eb61e117":"Dorpouts is majorly used in deep\/wide neural networks. This is a very basic one, not very deep one. As you can see, the accuracy is increasing very slowly and steadily and the model is yet to converge. It will take around 100 more epochs. ","47fbe7f4":"#### Adding many layers to the model","676056f3":"* \n\nOptimising a Neural Network demands to tune several hyperparameters simulataneously.","b2357356":"It can be used with most types of layers, such as dense fully connected layers, convolutional layers, and recurrent layers such as the long short-term memory network layer. Dropout may be implemented on any or all hidden layers in the network as well as the visible or input layer. \n\n`NOTE` : It is not used on the output layer.","74ac06ba":"`Underfitting` is the opposite counterpart of overfitting where the model exhibits high bias. This situation can occur when the model is not sufficiently complex to capture the relationship between features and labels (or if your model is too strictly regularized).\n\nUnderfitting is a bit harder to diagnose. If Accuracy and Validation Accuracy are similar but are both poor, then you may be underfitting. \n","3e32c74c":"![perfect_point.png](https:\/\/drive.google.com\/uc?export=view&id=1isDRyvRtMFPhC8RQHL9Puh-OoOm0z9FP)","b9c0f4a3":"\n\nHere is how to use regularization in DNN.\n```python\nmodel.add(Dense(256,activation='relu', kernel_regularizer = 'l2'))\n```"}}