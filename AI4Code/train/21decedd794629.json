{"cell_type":{"a09177a2":"code","3b07fe3c":"code","ed948536":"code","fc8608bd":"code","b87bc901":"code","23ce7ee2":"code","6b2d8371":"code","2b0f6c32":"code","2bb4377b":"code","61044093":"code","d081feec":"code","86e09d11":"code","6cf80f6e":"code","c598c51f":"code","e2cf889f":"code","7c804dad":"code","84cdf3fd":"code","80096b08":"code","a4979943":"markdown","a478e490":"markdown","78b4ef7e":"markdown","c99b6526":"markdown","108c1d62":"markdown","c6014783":"markdown","248c638e":"markdown","960011c6":"markdown"},"source":{"a09177a2":"from sklearn.datasets import fetch_20newsgroups\nnews_data = fetch_20newsgroups(subset = 'all', random_state=1)","3b07fe3c":"print(news_data.keys())","ed948536":"import pandas as pd\nprint('value of target and distribution \\n',pd.Series(news_data.target).value_counts().sort_index())\nprint('name of target \\n', news_data.target_names)","fc8608bd":"print(news_data.data[0])","b87bc901":"\n\n#train data\ntrain_news = fetch_20newsgroups(subset = 'train',\n                   remove = ('headers','footers','quotes'),\n                  random_state = 1)\n\nX_train = train_news.data\ny_train = train_news.target\n\n#test data\ntest_news = fetch_20newsgroups(subset = 'test',\n                   remove = ('headers','footers','quotes'),\n                  random_state = 1)\n\nX_test = test_news.data\ny_test = test_news.target\n\nprint('train data : {0}, test data : {1}'.format(len(train_news.data),len(test_news.data)))","23ce7ee2":"from sklearn.feature_extraction.text import CountVectorizer\n\n#feature vector based on count\ncnt_vect = CountVectorizer()\ncnt_vect.fit(X_train)\nX_train_cnt_vect = cnt_vect.transform(X_train)\n\n#apply to test data\nX_test_cnt_vect = cnt_vect.transform(X_test)\n\nprint('CountVectorizer of train data: ',X_train_cnt_vect.shape )\nprint('it means there are 11314 of context and 101631 of different word in here!')","6b2d8371":"# apply to logistic regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nlr_clf = LogisticRegression()\nlr_clf.fit(X_train_cnt_vect,y_train)\npred_1 = lr_clf.predict(X_test_cnt_vect)\naccuracy_score(pred_1,y_test)\nprint('accuracy of logistic : ', accuracy_score(pred_1,y_test))","2b0f6c32":"# apply linear SVM\nfrom sklearn.svm import LinearSVC\n\nls_clf = LinearSVC()\nls_clf.fit(X_train_cnt_vect,y_train)\npred_2 = ls_clf.predict(X_test_cnt_vect)\nprint('accuracy of linear SVM : ', accuracy_score(pred_2,y_test))","2bb4377b":"# apply naive bayes\nfrom sklearn.naive_bayes import BernoulliNB\n\nnb_clf = BernoulliNB()\nnb_clf.fit(X_train_cnt_vect,y_train)\npred_3 = nb_clf.predict(X_test_cnt_vect)\nprint('accuracy of naive bayes : ', accuracy_score(pred_3,y_test))","61044093":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#feature vector based on count\ntfidf_vect = TfidfVectorizer()\ntfidf_vect.fit(X_train)\nX_train_tfidf_vect = tfidf_vect.transform(X_train)\n\n#apply to test data\nX_test_tfidf_vect = tfidf_vect.transform(X_test)","d081feec":"# apply to logistic regression\n\nlr_clf = LogisticRegression()\nlr_clf.fit(X_train_tfidf_vect,y_train)\npred_4 = lr_clf.predict(X_test_tfidf_vect)\nprint('accuracy of logistic : ', accuracy_score(pred_4,y_test))","86e09d11":"# apply linear SVM\n\nls_clf = LinearSVC()\nls_clf.fit(X_train_tfidf_vect,y_train)\npred_5 = ls_clf.predict(X_test_tfidf_vect)\nprint('accuracy of linear SVM : ', accuracy_score(pred_5,y_test))","6cf80f6e":"# apply naive bayes\n\nnb_clf = BernoulliNB()\nnb_clf.fit(X_train_tfidf_vect,y_train)\npred_6 = nb_clf.predict(X_test_tfidf_vect)\nprint('accuracy of naive bayes : ', accuracy_score(pred_6,y_test))","c598c51f":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#feature vector based on TF-IDF with parameter\ntfidf_vect_p = TfidfVectorizer(stop_words='english', ngram_range=(1,2),max_df=300)\ntfidf_vect_p.fit(X_train)\nX_train_tfidf_vect = tfidf_vect_p.transform(X_train)\n\n#apply to test data\nX_test_tfidf_vect = tfidf_vect_p.transform(X_test)","e2cf889f":"# apply to logistic regression\n\nlr_clf = LogisticRegression()\nlr_clf.fit(X_train_tfidf_vect,y_train)\npred_7 = lr_clf.predict(X_test_tfidf_vect)\nprint('accuracy of logistic : ', accuracy_score(pred_7,y_test))","7c804dad":"# apply linear SVM\n\nls_clf = LinearSVC()\nls_clf.fit(X_train_tfidf_vect,y_train)\npred_8 = ls_clf.predict(X_test_tfidf_vect)\nprint('accuracy of linear SVM : ', accuracy_score(pred_8,y_test))","84cdf3fd":"# apply naive bayes\n\nnb_clf = BernoulliNB()\nnb_clf.fit(X_train_tfidf_vect,y_train)\npred_9 = nb_clf.predict(X_test_tfidf_vect)\nprint('accuracy of naive bayes : ', accuracy_score(pred_9,y_test))","80096b08":"print('\\t\\t','logistic\\t','SVM\\t\\t','naive bayes')\nprint('-'*80)\nprint('count \\t\\t','0.6068\\t\\t','0.5720\\t\\t','0.4579')\nprint('TF-IDF \\t\\t','0.6736\\t\\t','0.6919\\t\\t','0.4579')\nprint('TF-IDF\\t\\t','0.6922\\t\\t','0.7060\\t\\t','0.2006')\n","a4979943":"# conclusion\n\nAmong the 9 model, the model that apply TF-IDF with parameter and linear SVM model is most accurate !","a478e490":"# TF-IDF\n\nAt second, we will use TF-IDF feature vector and apply to each of logistic regression, linear SVM and naive bayes model. Let's compare the accuracy.","78b4ef7e":"# preprocessing\n\nTo check the each of data value, print first of the data.\n\nYou can see that, in the context, it contain variable kinds of data like e-mail adress, title name, writer info, etc. We only want the context of the article, so we have to remove all the other thing. ","c99b6526":"# TF-IDF with hyperparameter\n\nAt last, we will use TF-IDF feature vector and assign some hyperparameter to this stuff. Let's compare the accuracy.","108c1d62":"# feature vector\n\nIn this data, we will use Bag of Words model. It is a model that ignore the sequence and put the frequency of word into the feature vector. there are two types of Bag of Words model.\n\n* based on count \n\nIt put just frequency to each word.\n\n* based on TF-IDF(Term Frequency - Inverse Document Frequency)\n\nIt put frequency to each word in each document and give penalty to overall frequently appeared word.","c6014783":"As you can see, there are 20 types of target value, and each target names can be checked in target_names","248c638e":"# Data explanation\n\nIn scikit-learn package, there are some example data and today I will use 20 news groups data and analysis it. The purpose of this analysis is that making the model based on train data and predict the classification of other test data. If you check the dataset, you will find it return the type of dictionary bunch.\n\nLet's load the data and check the key values","960011c6":"# count \n\nAt first, we will use count feature vector and apply to each of logistic regression, linear SVM and naive bayes model. Let's compare the accuracy."}}