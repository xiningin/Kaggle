{"cell_type":{"b3cef754":"code","4536c3f4":"code","f6f6ea49":"code","7a91fdcd":"code","d34bc69b":"code","822814bd":"code","c4908b9d":"code","7827d796":"code","a3d28edf":"code","a72c8a4f":"code","b1eb2b1f":"code","cd39a8cc":"code","e91a551b":"code","9d91d066":"code","6d7780ed":"code","181eb82f":"code","a260165f":"code","d550a913":"code","2510fd77":"code","ac380327":"code","a942c8ca":"code","e2c5f6d8":"code","5c6c8b6c":"code","1323e779":"code","e23d4d75":"code","c6fbc84a":"code","db7f3580":"code","5e9da496":"code","8167d981":"code","8c1a74be":"code","595a4c7b":"code","77ef7d7d":"code","123557fb":"code","c11cf038":"code","8d574de2":"code","0291cf84":"code","6da94d76":"code","b19f04ab":"code","d99bb567":"code","769211a4":"code","482c950f":"code","592cc59e":"code","83cfc158":"code","8ab82211":"code","e4e1d0cc":"code","4711f36e":"markdown","baecb445":"markdown","9d812f9e":"markdown","592a88ea":"markdown","653dc4e6":"markdown","b7f75316":"markdown","7509b2fe":"markdown","f146b60e":"markdown","531cb13a":"markdown","0ae633e7":"markdown","cea11b5e":"markdown","5f0da09f":"markdown","5bd64868":"markdown","795ef02f":"markdown","cbde7f72":"markdown","f8269241":"markdown","43e79aef":"markdown","5522c4d2":"markdown","2a2610d4":"markdown","631c8881":"markdown","badb957e":"markdown","59bdd906":"markdown","a6322629":"markdown","b01eb8aa":"markdown","93ed4f95":"markdown","340712e9":"markdown","1c3fdc65":"markdown","20b7cf4b":"markdown","a3112159":"markdown","7f4be1e4":"markdown","3bea6a28":"markdown","9308e6c2":"markdown","7691bdfa":"markdown","38591ecc":"markdown","674b8134":"markdown","abda3942":"markdown","d0c7b880":"markdown","f7eef8e2":"markdown","8ab00eb9":"markdown","94c0cd47":"markdown","d7259836":"markdown","3b51a41f":"markdown","59fc965c":"markdown","23bab275":"markdown","fd8757d8":"markdown"},"source":{"b3cef754":"# Import Dependencies\n%matplotlib inline\n\n# Start Python Imports\nimport math, time, random, datetime\n\n# Data Manipulation\nimport numpy as np\nimport pandas as pd\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport missingno\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\nsns.set_palette(\"Set2\")\n\nprint(\"Setup Complete\")","4536c3f4":"# Import train and test data\n# Path of the file to read\ntrain_filepath = \"\/kaggle\/input\/titanic\/train.csv\"\ntest_filepath = \"\/kaggle\/input\/titanic\/test.csv\"\n\ntrain = pd.read_csv(train_filepath)\ntest = pd.read_csv(test_filepath)\n\n# Print the first rows of the training data\ntrain.head()","f6f6ea49":"print('The number of samples into the train data is {}.'.format(train.shape[0]))","7a91fdcd":"# View snipped of the training data\ntest.head() # We can see it contains the same columns as the test df","d34bc69b":"print('The number of samples into the test data is {}.'.format(test.shape[0]))","822814bd":"# We need to see the amount of missing values present in each column.\ntrain.isnull().sum()","c4908b9d":"# Plot graphic of missing values\nmissingno.matrix(train, figsize = (30,10))","7827d796":"# Different data types in the dataset\ntrain.dtypes","a3d28edf":"# percent of missing \"Age\" \nprint('Percent of missing \"Age\" records is %.2f%%' %((train['Age'].isnull().sum()\/train.shape[0])*100))","a72c8a4f":"ax = train[\"Age\"].hist(bins=15, density=True, stacked=True, alpha=0.6)\ntrain[\"Age\"].plot(kind='density', color='teal')\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","b1eb2b1f":"# mean age\nprint('The mean of \"Age\" is %.2f' %(train[\"Age\"].mean(skipna=True)))\n# median age\nprint('The median of \"Age\" is %.2f' %(train[\"Age\"].median(skipna=True)))","cd39a8cc":"# percent of missing \"Cabin\" \nprint('Percent of missing \"Cabin\" records is %.2f%%' %((train['Cabin'].isnull().sum()\/train.shape[0])*100))","e91a551b":"# percent of missing \"Embarked\" \nprint('Percent of missing \"Embarked\" records is %.2f%%' %((train['Embarked'].isnull().sum()\/train.shape[0])*100))","9d91d066":"print('Boarded passengers grouped by port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton):')\nprint(train['Embarked'].value_counts())\nsns.countplot(x='Embarked', data=train)\nplt.show()","6d7780ed":"train_data = train.copy()\ntrain_data[\"Age\"].fillna(train[\"Age\"].median(skipna=True), inplace=True)\ntrain_data[\"Embarked\"].fillna(train['Embarked'].value_counts().idxmax(), inplace=True)\ntrain_data.drop('Cabin', axis=1, inplace=True)","181eb82f":"# check missing values in adjusted train data\ntrain_data.isnull().sum()","a260165f":"# preview adjusted train data\ntrain_data.head()","d550a913":"plt.figure(figsize=(15,8))\nax = train[\"Age\"].hist(bins=15, density=True, stacked=True, alpha=0.6)\ntrain[\"Age\"].plot(kind='density')\nax = train_data[\"Age\"].hist(bins=15, density=True, stacked=True, alpha=0.5)\ntrain_data[\"Age\"].plot(kind='density')\nax.legend(['Raw Age', 'Adjusted Age'])\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","2510fd77":"## Create categorical variable for traveling alone\ntrain_data['TravelAlone']=np.where((train_data[\"SibSp\"]+train_data[\"Parch\"])>0, 0, 1)\ntrain_data.drop('SibSp', axis=1, inplace=True)\ntrain_data.drop('Parch', axis=1, inplace=True)","ac380327":"#create categorical variables and drop some variables\ntraining=pd.get_dummies(train_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\ntraining.drop('Sex_female', axis=1, inplace=True)\ntraining.drop('PassengerId', axis=1, inplace=True)\ntraining.drop('Name', axis=1, inplace=True)\ntraining.drop('Ticket', axis=1, inplace=True)\n\nfinal_train = training\nfinal_train.head()","a942c8ca":"test.isnull().sum()","e2c5f6d8":"test_data = test.copy()\ntest_data[\"Age\"].fillna(train[\"Age\"].median(skipna=True), inplace=True)\ntest_data[\"Fare\"].fillna(train[\"Fare\"].median(skipna=True), inplace=True)\ntest_data.drop('Cabin', axis=1, inplace=True)\n\ntest_data['TravelAlone']=np.where((test_data[\"SibSp\"]+test_data[\"Parch\"])>0, 0, 1)\n\ntest_data.drop('SibSp', axis=1, inplace=True)\ntest_data.drop('Parch', axis=1, inplace=True)\n\ntesting = pd.get_dummies(test_data, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\ntesting.drop('Sex_female', axis=1, inplace=True)\ntesting.drop('PassengerId', axis=1, inplace=True)\ntesting.drop('Name', axis=1, inplace=True)\ntesting.drop('Ticket', axis=1, inplace=True)\n\nfinal_test = testing\nfinal_test.head()","5c6c8b6c":"# Visualize the count of survivors\nsns.countplot('Survived', data=train)","1323e779":"sns.barplot('Pclass', 'Survived', data=train)\nplt.show()","e23d4d75":"sns.barplot('Sex', 'Survived', data=train)\nplt.show()","c6fbc84a":"plt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train[\"Age\"][final_train.Survived == 1], shade=True)\nsns.kdeplot(final_train[\"Age\"][final_train.Survived == 0], shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Age for Surviving Population and Deceased Population')\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","db7f3580":"sns.barplot('TravelAlone', 'Survived', data=final_train)\nplt.show()","5e9da496":"plt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train[\"Fare\"][final_train.Survived == 1], shade=True)\nsns.kdeplot(final_train[\"Fare\"][final_train.Survived == 0], shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\nax.set(xlabel='Fare')\nplt.xlim(-20,200)\nplt.show()","8167d981":"sns.barplot('Embarked', 'Survived', data=train)\nplt.show()","8c1a74be":"# Preprocessing\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, label_binarize\n\n# Machine learning\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection, tree, preprocessing, metrics, linear_model\nfrom sklearn.linear_model import LinearRegression, LogisticRegression, SGDClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nprint(\"Setup Complete\")","595a4c7b":"# Let's look at our data\nfinal_train.head()","77ef7d7d":"# Split the dataframe into data and labels\nX = final_train.drop('Survived', axis=1) # data\ny = final_train.Survived # labels","123557fb":"# Shape of the data (without labels)\nX.shape","c11cf038":"# split the data intro training and validation data, split based on a random number generator.\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)","8d574de2":"# Define model\ntree_model = DecisionTreeClassifier(random_state=1)\n\n# Fit model\ntree_model.fit(train_X, train_y);\n\n# Get MAE on Model\nval_tree_predictions = tree_model.predict(val_X)\nprint(mean_absolute_error(val_y,val_tree_predictions))","0291cf84":"# Define model\nrf_model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=1)\n\n# Fit Model\nrf_model.fit(train_X, train_y);\n\n# Get MAE on model\nrf_preds = rf_model.predict(val_X)\nprint(mean_absolute_error(val_y, rf_preds))\n","6da94d76":"# Create a list of columns to be used for the predictions\nwanted_test_columns = X.columns\nwanted_test_columns","b19f04ab":"# Make Predictions\npredictions = tree_model.predict(final_test[wanted_test_columns])","d99bb567":"# Our predictions array is comprised of 0's and 1's (Survived or Did Not Survive)\npredictions[:20]","769211a4":"# Create a submisison dataframe and append the relevant columns\nsubmission = pd.DataFrame()\nsubmission['PassengerId'] = test['PassengerId']\nsubmission['Survived'] = predictions # our model predictions on the test dataset\nsubmission.head()\n","482c950f":"# Let's convert our submission dataframe 'Survived' column to ints\nsubmission['Survived'] = submission['Survived'].astype(int)\nprint('Converted Survived column to integers.')","592cc59e":"# How does our submission dataframe look?\nsubmission.head()","83cfc158":"# Are our test and submission dataframes the same length?\nif len(submission) == len(test):\n    print(\"Submission dataframe is the same length as test ({} rows).\".format(len(submission)))\nelse:\n    print(\"Dataframes mismatched, won't be able to submit to Kaggle.\")","8ab82211":"# Convert submisison dataframe to csv for submission to csv \n# for Kaggle submisison\nsubmission.to_csv('..\/decisiontreeclassifier_submission.csv', index=False)\nprint('Submission CSV is ready!')","e4e1d0cc":"# Check the submission csv to make sure it's in the right format\nsubmissions_check = pd.read_csv(\"..\/decisiontreeclassifier_submission.csv\")\nsubmissions_check.head()","4711f36e":"Passengers who boarded in Cherbourg, France, appear to have the highest survival rate.  Passengers who boarded in Southhampton were marginally less likely to survive than those who boarded in Queenstown.  This is probably related to passenger class, or maybe even the order of room assignments.","baecb445":"By far the most passengers boarded in Southhampton, so we'll impute those 2 NaN's w\/ \"S\".","9d812f9e":"According to the Kaggle data dictionary, both SibSp and Parch relate to traveling with family. For simplicity's sake (and to account for possible multicollinearity), I'll combine the effect of these variables into one categorical predictor: whether or not that individual was traveling alone.","592a88ea":"# Titanic Project\n**Description**: The goal of this project is to predict if a passenger will survive on the titanic. This work also represents as a guide on how to use different machine learning classification algorithms.\n\n\n1. [Import data and python packages](#t1.)\n2. [Assess Data Quality & Missing Values](#t2.)\n3. [Exploratory Data Analysis](#t3.)\n4. [Building Machine Learning Models and Results](#t4.)","653dc4e6":"<a id=\"t2.4.1.\"><\/a>\n## 2.4.1. Additional Variables","b7f75316":"We need to figure out where we are missing data to account it once we are doing a prediction.","7509b2fe":"<a id=\"t3.\"><\/a>\n# 3. Exploratory Data Analysis","f146b60e":"Given that we want to evaluate every model that we use, for this example, we will use a metric called **Mean Absoulute Error (MAE)**. With this goal, we will measure the performance of the model on data that was not used to build the model (validation data). The scikit-learn library has a function train_test_split to break up the data in two pieces. We'' use some of the data as a training data to fit the model, and we'll use the other data as validation data to calculate mean_absolute error.","531cb13a":"~20% of entries for passenger age are missing. Let's see what the 'Age' variable looks like in general.","0ae633e7":"The age distribution for survivors and deceased is actually very similar.  One notable difference is that, of the survivors, a larger proportion were children.  The passengers evidently made an attempt to save children by giving them a place on the life rafts. \n\n\n1. [Import data and python packages](#t1.)\n2. [Assess Data Quality & Missing Values](#t2.)\n3. [Exploratory Data Analysis](#t3.)\n4. [Logistic Regression and Results](#t4.)","cea11b5e":"### Step 1. Load the libraries","5f0da09f":"77% of records are missing, which means that imputing information and using this variable for prediction is probably not wise.  We'll ignore this variable in our model.","5bd64868":"### Data Types\nWe can see our dataset consist on different types of data. However, as we continue to analyze it, we might find features which are numerical and should actually be categorial.","795ef02f":"## 3.2. Exploration of Passenger Class\n\nDescription: The ticket class of the passenger.\n\nKey: 1 = 1st, 2 = 2nd, 3 = 3rd\n\nWe will look at the distribution for each feature to determine if there are values which are completely outside of distribution, we may not want to include them in our model","cbde7f72":"<a id=\"t2.1.\"><\/a>\n## 2.1.    Age - Missing Values","f8269241":"### Step 5. Submission","43e79aef":"<a id=\"t2.\"><\/a>\n# 2. Data Quality & Missing Value Assessment","5522c4d2":"There are only 2 (0.22%) missing values for \"Embarked\", so we can just impute with the port where most people boarded.","2a2610d4":"This is a very obvious difference.  Clearly being female greatly increased your chances of survival.","631c8881":"In this case, we can see that the MAE is slightly larger than when using the Decision Tree Classifier Model. For this reason, we will use the earlier model to predict the outcome.","badb957e":"<a id=\"t2.3.\"><\/a>\n## 2.3. Embarked - Missing Values","59bdd906":"Checking which columns contain empty values, looks like columns Age, Cabin and embarked are the only columns missing some values. ","a6322629":"I'll also create categorical variables for Passenger Class (\"Pclass\"), Gender (\"Sex\"), and Port Embarked (\"Embarked\"). ","b01eb8aa":"## 3.4. Exploration of Age\nDescription: The age of the passenger.","93ed4f95":"### Now, apply the same changes to the test data. <br>\nI will apply to same imputation for \"Age\" in the Test data as I did for my Training data (if missing, Age = 28).  <br> I'll also remove the \"Cabin\" variable from the test data, as I've decided not to include it in my analysis. <br> There were no missing values in the \"Embarked\" port variable. <br> I'll add the dummy variables to finalize the test set.  <br> Finally, I'll impute the 1 missing value for \"Fare\" with the median, 14.45.","340712e9":"### Step 2. Separate the data","1c3fdc65":"<a id=\"t4.\"><\/a>\n# 4. Building Machine Learning Models and Results","20b7cf4b":"### Step 4. Predict\n","a3112159":"We need our submission dataframe to look like the gender_submisison dataframe, so we'll turn the Survived column into integers.","7f4be1e4":"<a id=\"t2.2.\"><\/a>\n## 2.2. Cabin - Missing Values","3bea6a28":"As the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model.  Passengers who paid lower fare appear to have been less likely to survive.  This is probably strongly correlated with Passenger Class.","9308e6c2":"Based on my assessment of the missing values in the dataset, I'll make the following changes to the data:\n* If \"Age\" is missing for a given row, I'll impute with 28 (median age).\n* If \"Embarked\" is missing for a riven row, I'll impute with \"S\" (the most common boarding port).\n* I'll ignore \"Cabin\" as a variable. There are too many missing values for imputation. ","7691bdfa":"## 3.5. Exploration of Traveling Alone vs. With Family\n\nDescription: This feature describes if the passenger had any siblings, spouses, children or parents aboard the Titanic.","38591ecc":"<a id=\"t2.4.\"><\/a>\n## 2.4. Final Adjustments to Data (Train & Test)","674b8134":"Since \"Age\" is (right) skewed, using the mean might give us biased results by filling in ages that are older than desired. To deal with this, we'll use the median to impute the missing values. ","abda3942":"Our mean absolute error for the out-of-sample data when using the Decision Tree Classifier Model is 0.2063.\n\n#### Random Forests\nDecision trees leave you with a difficult decision. A deep tree with lots of leaves will overfit because each prediction is coming from historical data from only the few houses at its leaf. But a shallow tree with few leaves will perform poorly because it fails to capture as many distinctions in the raw data.\n\nEven today's most sophisticated modeling techniques face this tension between underfitting and overfitting. But, many models have clever ideas that can lead to better performance. We'll look at the random forest as an example.\n\nThe random forest uses many trees, and it makes a prediction by averaging the predictions of each component tree. It generally has much better predictive accuracy than a single decision tree and it works well with default parameters. If you keep modeling, you can learn more models with even better performance, but many of those are sensitive to getting the right parameters.","d0c7b880":"<a id=\"t1.\"><\/a>\n# 1. Import Data & Python Packages","f7eef8e2":"## 3.6. Exploration of Fare\n\nDescription: How much the ticket cost.","8ab00eb9":"## 3.1. Exploration of our Target Feature: Survived\nDescription: Whether the passenger survived or not.\n    \nKey: 0 = did not survive, 1 = survived\n    \nThis is the variable we want our machine learning model to predict based off all the others.","94c0cd47":"## 3.7. Exploration of Embarked\n\nDescription: The port where the passenger boarded the Titanic.\n\nKey: C = Cherbourg, Q = Queenstown, S = Southampton","d7259836":"Individuals traveling without family were more likely to die in the disaster than those with family aboard. Given the era, it's likely that individuals traveling alone were likely male.","3b51a41f":"## 3.3. Exploration of Gender Variable\nDescription: The sex of the passenger (male or female).\n\n0 = male\n1 = female","59fc965c":"We can see that there is no target variable in the test data, since as we said earlier, the goal is to predict this target using different machine learning algorithms.","23bab275":"### Step 3. Define and Validate the model\n#### Decision Tree Classifier","fd8757d8":"The datasets used in this competition consists on two similar datasets that include passenger information like name, age, gender, socio-economic class, etc. One dataset is titled *train.csv* and the other is titled *test.csv*.\n\nTrain.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d.\n\nThe *test.csv* dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger. It\u2019s your job to predict these outcomes.\n\nUsing the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived. \n\n\nThe columns consist on:\n\n* **Survival:** 0 = No, 1 = Yes\n\n* **pclass (Ticket class):** 1 = 1st, 2 = 2nd, 3 = 3rd\n\n* **sex:** Sex\n\n* **Age:** Age in years\n\n* **sibsp:** number of siblings\/spouses aboard the Titanic\n\n* **parch:** number of parents\/children aboard the Titanic\n\n* **ticket:** Ticket number\n\n* **fare:** Passenger fare\n\n* **cabin:** Cabin number\n\n* **embarked:** Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton"}}