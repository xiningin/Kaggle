{"cell_type":{"d281178b":"code","fdcc071e":"code","b6b11261":"code","e9197346":"code","2510a0a6":"code","66e06ef3":"code","aa793b41":"code","007e7049":"code","54dd74af":"code","529658b8":"code","31815ffa":"code","84f25144":"code","a567e173":"code","cb2194de":"code","b65cb3d2":"code","5b60a048":"code","226d1f0c":"code","d4230538":"code","ae80ca0b":"code","c5c5cf1c":"code","32ccfafc":"code","bd5d1337":"code","fcc854b0":"code","fd69e694":"code","0032146b":"markdown","ef09e913":"markdown","500eedab":"markdown","5204c8bd":"markdown","8b72cc9f":"markdown","1cd691c2":"markdown","583e90f0":"markdown","9f8ddf8d":"markdown","d92c212a":"markdown","ee0bd0a2":"markdown","6b40d671":"markdown","cddf24bc":"markdown","ed0bd16a":"markdown","61a45e55":"markdown","f7323c88":"markdown","2b6b6357":"markdown","7e0ed746":"markdown","c8779a9d":"markdown","9004bd62":"markdown","749275cf":"markdown"},"source":{"d281178b":"import imageio\nfrom PIL import Image\nimport cv2\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport subprocess\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\n%matplotlib inline\nplt.rcParams['figure.dpi'] = 150\n\nimport seaborn as sns\n\nfrom IPython.display import Video, display\n\n#block those warnings from pandas about setting values on a slice\nimport warnings\nwarnings.filterwarnings('ignore')\n","fdcc071e":"# Read in the image labels file\nimg_labels = pd.read_csv('\/kaggle\/input\/nfl-impact-detection\/image_labels.csv')\nimg_labels.head()","b6b11261":"# Get a summary on the data type\n\nimg_labels.info()","e9197346":"# Set the name of our working image\nimg_name = img_labels['image'][0]\nimg_name","2510a0a6":"# Define the path to our selected image\nimg_path = f\"\/kaggle\/input\/nfl-impact-detection\/images\/{img_name}\"","66e06ef3":"# Read in and plot the image\nimg = imageio.imread(img_path) \nplt.imshow(img)\nplt.show()","aa793b41":"### Function to add labels to an image\n\ndef add_img_boxes(image_name, image_labels):\n    # Set label colors for bounding boxes\n    HELMET_COLOR = (0, 0, 0)    # Black\n\n    boxes = img_labels.loc[img_labels['image'] == img_name]\n    for j, box in boxes.iterrows():\n        color = HELMET_COLOR \n\n        # Add a box around the helmet\n        # Note that cv2.rectangle requires us to specify the top left pixel and the bottom right pixel\n        cv2.rectangle(img, (box.left, box.top), (box.left + box.width, box.top + box.height), color, thickness=1)\n        \n    # Display the image with bounding boxes added\n    plt.imshow(img)\n    plt.show()","007e7049":"add_img_boxes(img_name, img_labels)","54dd74af":"# Read in the video labels file\nvideo_labels = pd.read_csv('\/kaggle\/input\/nfl-impact-detection\/train_labels.csv')\nvideo_labels.head()","529658b8":"# Define the video we'll process\nvideo_name = video_labels['video'][0]\nvideo_name","31815ffa":"# Define the path and then display the video using \nvideo_path = f\"\/kaggle\/input\/nfl-impact-detection\/train\/{video_name}\"\ndisplay(Video(data=video_path, embed=True))","84f25144":"# Create a function to annotate the video at the provided path using labels from the provided dataframe, return the path of the video\ndef annotate_video(video_path: str, video_labels: pd.DataFrame) -> str:\n    VIDEO_CODEC = \"MP4V\"\n    HELMET_COLOR = (0, 0, 0)    # Black\n    IMPACT_COLOR = (0, 0, 255)  # Red\n    video_name = os.path.basename(video_path)\n    \n    vidcap = cv2.VideoCapture(video_path)\n    fps = vidcap.get(cv2.CAP_PROP_FPS)\n    width = int(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    output_path = \"labeled_\" + video_name\n    tmp_output_path = \"tmp_\" + output_path\n    output_video = cv2.VideoWriter(tmp_output_path, cv2.VideoWriter_fourcc(*VIDEO_CODEC), fps, (width, height))\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        \n        # We need to add 1 to the frame count to match the label frame index that starts at 1\n        frame += 1\n        \n        # Let's add a frame index to the video so we can track where we are\n        img_name = f\"{video_name}_frame{frame}\"\n        cv2.putText(img, img_name, (0, 50), cv2.FONT_HERSHEY_SIMPLEX, 1.0, HELMET_COLOR, thickness=2)\n    \n        # Now, add the boxes\n        boxes = video_labels.query(\"video == @video_name and frame == @frame\")\n        for box in boxes.itertuples(index=False):\n            if box.impact == 1 and box.confidence > 1 and box.visibility > 0:    # Filter for definitive head impacts and turn labels red\n                color, thickness = IMPACT_COLOR, 2\n            else:\n                color, thickness = HELMET_COLOR, 1\n            # Add a box around the helmet\n            cv2.rectangle(img, (box.left, box.top), (box.left + box.width, box.top + box.height), color, thickness=thickness)\n            cv2.putText(img, box.label, (box.left, max(0, box.top - 5)), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, thickness=1)\n        output_video.write(img)\n    output_video.release()\n    \n    # Not all browsers support the codec, we will re-load the file at tmp_output_path and convert to a codec that is more broadly readable using ffmpeg\n    if os.path.exists(output_path):\n        os.remove(output_path)\n    subprocess.run([\"ffmpeg\", \"-i\", tmp_output_path, \"-crf\", \"18\", \"-preset\", \"veryfast\", \"-vcodec\", \"libx264\", output_path])\n    os.remove(tmp_output_path)\n    \n    return output_path","a567e173":"# Label the video and display it - this will take a bit\nlabeled_video = annotate_video(f\"\/kaggle\/input\/nfl-impact-detection\/train\/{video_name}\", video_labels)\ndisplay(Video(data=labeled_video, embed=True))","cb2194de":"# Filter for definitive impacts labeled for this video\nvideo_impacts = video_labels.loc[(video_labels.video == video_name) & (video_labels.impact == 1) & (video_labels.confidence > 1) & (video_labels.visibility > 0)]\n\nlen(video_impacts) # how many definitive impacts in this play","b65cb3d2":"# Get this list of definitive impacts\nvideo_impacts","5b60a048":"# Get the name of the sideline view associated with this play and display it\n\nsideline_video_name = video_name.replace(\"Endzone\", \"Sideline\")\n# Define the path and then display the video using \nsideline_video_path = f\"\/kaggle\/input\/nfl-impact-detection\/train\/{sideline_video_name}\"\ndisplay(Video(data=sideline_video_path, embed=True))","226d1f0c":"track_data = pd.read_csv('\/kaggle\/input\/nfl-impact-detection\/train_player_tracking.csv')\ntrack_data.head()","d4230538":"# Filter the track data to the play of interest\ngame_key = track_data['gameKey'][0]\nplay_id = track_data['playID'][0]\nplay_track = track_data.loc[(track_data.gameKey == game_key) & (track_data.playID == play_id)]\nlen(play_track)","ae80ca0b":"# See what events are stored in the data\nplay_track['event'].unique()","c5c5cf1c":"# Build a dataframe for the player positions at the snap\n\nat_snap = play_track.loc[play_track.event == 'ball_snap']\nat_snap","32ccfafc":"def create_football_field(linenumbers=True,\n                          endzones=True,\n                          highlight_line=False,\n                          highlight_line_number=50,\n                          highlighted_name='Line of Scrimmage',\n                          fifty_is_los=False,\n                          figsize=(12, 6.33)):\n    \"\"\"\n    Function that plots the football field for viewing plays.\n    Allows for showing or hiding endzones.\n    \"\"\"\n    rect = patches.Rectangle((0, 0), 120, 53.3, linewidth=0.1,\n                             edgecolor='r', facecolor='forestgreen', zorder=0)  # changed the field color to forestgreen\n\n    fig, ax = plt.subplots(1, figsize=figsize)\n    ax.add_patch(rect)\n\n    plt.plot([10, 10, 10, 20, 20, 30, 30, 40, 40, 50, 50, 60, 60, 70, 70, 80,\n              80, 90, 90, 100, 100, 110, 110, 120, 0, 0, 120, 120],\n             [0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3,\n              53.3, 0, 0, 53.3, 53.3, 0, 0, 53.3, 53.3, 53.3, 0, 0, 53.3],\n             color='white')\n    if fifty_is_los:\n        plt.plot([60, 60], [0, 53.3], color='gold')\n        plt.text(62, 50, '<- Player Yardline at Snap', color='gold')\n    # Endzones\n    if endzones:\n        ez1 = patches.Rectangle((0, 0), 10, 53.3,\n                                linewidth=0.1,\n                                edgecolor='r',\n                                facecolor='blue',\n                                alpha=0.2,\n                                zorder=0)\n        ez2 = patches.Rectangle((110, 0), 120, 53.3,\n                                linewidth=0.1,\n                                edgecolor='r',\n                                facecolor='blue',\n                                alpha=0.2,\n                                zorder=0)\n        ax.add_patch(ez1)\n        ax.add_patch(ez2)\n    plt.xlim(0, 120)\n    plt.ylim(-5, 58.3)\n    plt.axis('off')\n    if linenumbers:\n        for x in range(20, 110, 10):\n            numb = x\n            if x > 50:\n                numb = 120 - x\n            plt.text(x, 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white')\n            plt.text(x - 0.95, 53.3 - 5, str(numb - 10),\n                     horizontalalignment='center',\n                     fontsize=20,  # fontname='Arial',\n                     color='white', rotation=180)\n    if endzones:\n        hash_range = range(11, 110)\n    else:\n        hash_range = range(1, 120)\n\n    for x in hash_range:\n        ax.plot([x, x], [0.4, 0.7], color='white')\n        ax.plot([x, x], [53.0, 52.5], color='white')\n        ax.plot([x, x], [22.91, 23.57], color='white')\n        ax.plot([x, x], [29.73, 30.39], color='white')\n\n    if highlight_line:\n        hl = highlight_line_number + 10\n        plt.plot([hl, hl], [0, 53.3], color='yellow')\n        plt.text(hl + 2, 50, '<- {}'.format(highlighted_name),\n                 color='yellow')\n    return fig, ax\n\ncreate_football_field()\nplt.show()","bd5d1337":"# This is just a small helper function to set the color mapping for the Track Plot\n# The visiting team *usually* wears white \ndef set_color(row):\n    if 'H' in row['player']:\n        return \"black\"\n    else:\n        return \"white\"\n\nat_snap['color'] = at_snap.apply(lambda row: set_color(row), axis=1)\nat_snap","fcc854b0":"# Plot the positions of players at the snap\n\nfig, ax = create_football_field()\nat_snap.plot(x=\"x\", y=\"y\",  kind='scatter', ax=ax, color = at_snap['color'], s=300)\nat_snap_home = at_snap.loc[at_snap['player'].str.contains('H')]\nat_snap_away = at_snap.loc[at_snap['player'].str.contains('V')]\n\nfor index, row in at_snap_away.iterrows():\n    ax.annotate(row['player'], (row['x'], row['y']), verticalalignment='center', horizontalalignment='center')\nfor index, row in at_snap_home.iterrows():\n    ax.annotate(row['player'], (row['x'], row['y']), verticalalignment='center', horizontalalignment='center', color = 'white')\nx_min = min(at_snap['x']) - 5\nx_max = max(at_snap['x']) + 5\ny_min = min(at_snap['y']) - 5\ny_max = max(at_snap['y']) + 5\nax.set_xlim(x_min, x_max)\nax.set_ylim(y_min, y_max)\nplt.show()","fd69e694":"# Plot the positions of players through the play\n\nplay_track['color'] = play_track.apply(lambda row: set_color(row), axis=1)\n\n# Filter to only include time after the snap\nsnap_time = at_snap['time'].iloc[0]\nplay_track = play_track.loc[play_track['time'] > snap_time]\n\nfig, ax = create_football_field()\nplay_track.plot(x=\"x\", y=\"y\",  kind='scatter', ax=ax, color = play_track['color'], s= 1)\n\nplt.show()","0032146b":"If you watch the video carefully (or pause at the appropriate time), you will note that the bounding box flashes red at the moment of impact.  \n\nYou can get a list of the helmet impacts for this view in the following manner (this will provide the frames and labels for the players with impacts).","ef09e913":"Due to the custom metric, this competition relies on an evaluation pipeline which is slightly different than a typical code competition. Your notebook must import and submit via the custom nflimpact python module available in Kaggle notebooks. To submit, simply add these three lines at the end of your code:","500eedab":"### Video Data\n\nThe labeled video dataset provides video for 60 plays observed from both the sideline and endzone perspective (120 videos total).  The video_labels.csv file contains labeled bounding boxes for every helmet that is visible in every frame of every video.  ","5204c8bd":"### Import Needed Packages","8b72cc9f":"### Submission Instructions","1cd691c2":"```\n# Code for generating a dataframe of your solution goes here\n# solution_df = {a dataframe containing all of your predicted impacts}\n\nimport nflimpact\nenv = nflimpact.make_env()\n\nenv.predict(solution_df) # solution_df is a pandas dataframe of your entire submission file\n\n```","583e90f0":"To start, we are going plot the player positions at the snap.  Let's use a helper function to set the color for the home and visiting team.","9f8ddf8d":"```\ngameKey,playID,view,video,frame,left,width,top,height\n57590,3607,Endzone,57590_003607_Endzone.mp4,1,1,1,1,1\n57590,3607,Sideline,57590_003607_Sideline.mp4,1,1,1,1,1\n57595,1252,Endzone,57595_001252_Endzone.mp4,1,1,1,1,1\n57595,1252,Sideline,57595_001252_Sideline.mp4,1,1,1,1,1\netc.\n```","d92c212a":"### Image Data Overview\n\nThe labeled image dataset consists of 9947 labeled images and a .csv file named image_labels.csv that contains the labeled bounding boxes for all images.  This dataset is provided to support the development of helmet detection algorithms. ","ee0bd0a2":"Let's write a function for adding the bounding boxes from the label to the image.  Note that the pixel geometry starts with (0,0) in the top left of the image.  To draw the bounding box, we need to specify the top left pixel location and the bottom right pixel location of the image.","6b40d671":"The following code for generating an image of a football field is borrowed (with permission) from Kaggle Grandmaster Rob Mulla.  You can see his original notebook here:  \n\nhttps:\/\/www.kaggle.com\/robikscube\/nfl-big-data-bowl-plotting-player-position\/notebook","cddf24bc":"Let's develop a function that will add bounding boxes to every frame in the video.","ed0bd16a":"The dataframe should be in the following format. Each row in your submission represents a single predicted bounding box for a helmet impact for the given frame. Note that it is not required to include labels of which players had an impact, only a bounding box where it occurred.","61a45e55":"Note that every play consists of two views - a sideline view and an endzone view.  So, to find the other view of this play:","f7323c88":"Let's filter the track data to analyze the same play we displayed above (happens to be the first play in the file).","2b6b6357":"Let's bring in an image and go ahead and add the labels.  ","7e0ed746":"### Data Overview\n\nThis notebook provides an overview of the data and some examples of how to access and conduct some initial plotting of the data that has been provided.  There are three different types of data provided for this problem:\n\n* **Image Data:**  Almost 10,000 images and associated helmet labels for the purpose of building a helmet detection computer vision system.\n\n* **Video Data:**  120 videos (60 plays) from both a sideline and endzone point of view (one each per play) with associated helmet and helmet impact labels for the purpose of building a helmet impact detection computer vision system.\n\n* **Tracking Data:**  Tracking data for all players that participate in the provided 60 plays.\n\nThis overview provides an example for how to parse and plot each of these data types.  It also briefly summarizes the needed steps to submit a solution for scoring.","c8779a9d":"The gameKey, playID, video, and frame fields facilitate matching the bounding box to the appropriate video file and video frame.  The label field corresponds to the player field in the tracking data, providing a unique identifier for the helmets of players that are participating in the play.  However, there are also helmets (players) that appear in the videos that are not participating in the play.  These players are identified with the labels V00 (non-participant on the visiting team) or H00 (non-participant on the home team).  In rare cases that a player cannot be uniquely identified that is participating in the play (for example when only the helmet is visible in a pile-up), the appropriate generic V00 or H00 label is applied to that helmet bounding box. \n\nThe Sideline and Endzone views have been time-synced such that the snap occurs 10 frames into the video.  This time alignment should be considered to be accurate to within +\/- 3 frames or 0.05 seconds (video data is recorded at approximately 59.94 frames per second). \n\nFor the purposes of evaluation, **definitive helmet impacts are defined as meeting three criteria:**\n\n\u2022\timpact = 1\n\n\u2022\tconfidence > 1\n\n\u2022\tvisibility > 0\n\nThose labels with confidence = 1 document cases in which human labelers asserted it was possible that a helmet impact occurred, but it was not clear that the helmet impact altered the trajectory of the helmet.  Those labels with visibility = 0 indicate that although there is reason to believe that an impact occurred to that helmet at that time, the impact itself was not visible from the view.\n\nLet's bring in the very first video and display it.","9004bd62":"### Tracking Data\n\nThe player track file in .csv format includes player position, direction, and orientation data for each player during the entire course of the play collected using the Next Gen Stats (NGS) system. This data is indexed by gameKey, playID, and player, with the time variable providing a temporal index within an individual play.","749275cf":"We can now see in the image above that bounding boxes have been added to every helmet.  "}}