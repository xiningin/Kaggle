{"cell_type":{"de344066":"code","9a56ee6d":"code","51bf2aeb":"code","1f726d23":"code","eaf3e3e5":"code","5b63724e":"code","a97d39ed":"code","c5cbb528":"code","b73a82f9":"code","ec1feaac":"code","55c7b1a6":"code","8a22f097":"code","18c1d6ba":"code","03bb71f8":"code","2743c384":"code","05c2e41a":"code","8b3225d7":"code","2216c4d9":"code","3368e99f":"code","b83bccf1":"code","0d1e7620":"code","1dc9c3c4":"code","1c72805c":"code","adc336d2":"code","416b9f10":"code","c0f9ff3d":"code","2a25d410":"code","f4cd5024":"code","211238cb":"code","c70a6e75":"code","c84e0793":"code","6bf20a83":"code","70dab4c7":"code","82698392":"code","6267b7eb":"code","087c0dd6":"code","c4d6a5a6":"markdown","b4899c0a":"markdown","5ed54529":"markdown","396eb3bf":"markdown","54d96659":"markdown","db4a3da3":"markdown","dd9287fd":"markdown","3a531ec7":"markdown","a8201ebc":"markdown","289cdb96":"markdown","fe3c6d8e":"markdown"},"source":{"de344066":"import pandas as pd\nimport numpy as np\n\nimport lightgbm as lgb\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn import model_selection, preprocessing, metrics\nimport datetime\\\n\nimport os\nimport json\nimport numpy as np\nimport pandas as pd\nfrom pandas.io.json import json_normalize","9a56ee6d":"def load_df(csv_path='..\/input\/train.csv', nrows=None):\n    JSON_COLUMNS = ['device', 'geoNetwork', 'totals', 'trafficSource']\n    \n    df = pd.read_csv(csv_path, \n                     converters={column: json.loads for column in JSON_COLUMNS}, \n                     dtype={'fullVisitorId': str}, # Important!!\n                     nrows=nrows)\n    \n    for column in JSON_COLUMNS:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}_{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    print(f\"Loaded {os.path.basename(csv_path)}. Shape: {df.shape}\")\n    return df","51bf2aeb":"%%time\ndf_train = load_df()\ndf_test = load_df(\"..\/input\/test.csv\")","1f726d23":"df_train[\"totals_transactionRevenue\"] = df_train[\"totals_transactionRevenue\"].astype('float')","eaf3e3e5":"const_cols = [c for c in df_train.columns if df_train[c].nunique(dropna=False)==1 ]\nconst_cols","5b63724e":"cols_to_drop = const_cols + ['sessionId']\n\ndf_train = df_train.drop(cols_to_drop + [\"trafficSource_campaignCode\"], axis=1)\ndf_test = df_test.drop(cols_to_drop, axis=1)","a97d39ed":"print(df_train.shape, df_test.shape)","c5cbb528":"# Impute 0 for missing target values\ndf_train[\"totals_transactionRevenue\"].fillna(0, inplace=True)\ntrain_y = df_train[\"totals_transactionRevenue\"].values\ntrain_id = df_train[\"fullVisitorId\"].values\ntest_id = df_test[\"fullVisitorId\"].values\n\n\n# label encode the categorical variables and convert the numerical variables to float\ncat_cols = [\"channelGrouping\", \"device_browser\", \n            \"device_deviceCategory\", \"device_operatingSystem\", \n            \"geoNetwork_city\", \"geoNetwork_continent\", \n            \"geoNetwork_country\", \"geoNetwork_metro\",\n            \"geoNetwork_networkDomain\", \"geoNetwork_region\", \n            \"geoNetwork_subContinent\", \"trafficSource_adContent\", \n            \"trafficSource_adwordsClickInfo.adNetworkType\", \n            \"trafficSource_adwordsClickInfo.gclId\", \n            \"trafficSource_adwordsClickInfo.page\", \n            \"trafficSource_adwordsClickInfo.slot\", \"trafficSource_campaign\",\n            \"trafficSource_keyword\", \"trafficSource_medium\", \n            \"trafficSource_referralPath\", \"trafficSource_source\",\n            'trafficSource_adwordsClickInfo.isVideoAd',\n            'trafficSource_isTrueDirect', 'device_isMobile']","b73a82f9":"df_train['date'] = df_train['date'].apply(lambda x: datetime.date(int(str(x)[:4]), int(str(x)[4:6]), int(str(x)[6:])))\ndf_train['date'] = pd.to_datetime(df_train['date'])","ec1feaac":"num_cols = [\"totals_hits\", \"totals_pageviews\", \n            \"visitNumber\", \"visitStartTime\", \n            'totals_bounces',  'totals_newVisits']    \n\nfor col in num_cols:\n    df_train[col] = df_train[col].astype(float)\n    df_test[col] = df_test[col].astype(float)","55c7b1a6":"df_train_copy = df_train.copy()\ndf_test_copy = df_test.copy()\n\ndf_train = df_train_copy.copy()\ndf_test = df_test_copy.copy()","8a22f097":"train_dates = df_train['date'].copy()","18c1d6ba":"for col in cat_cols:\n    print(col)\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(df_train[col].values.astype('str')) + list(df_test[col].values.astype('str')))\n    df_train[col] = lbl.transform(list(df_train[col].values.astype('str')))\n    df_test[col] = lbl.transform(list(df_test[col].values.astype('str')))","03bb71f8":"df_train.shape","2743c384":"for col in df_train.columns:\n    if col not in num_cols and col not in cat_cols:\n        print(col)","05c2e41a":"not_use_cols = ['date', 'fullVisitorId', 'visitId', 'totals_transactionRevenue']","8b3225d7":"len(cat_cols) + len(num_cols)","2216c4d9":"len(not_use_cols)","3368e99f":"# Split the train dataset into development and valid based on time \ndev_df = df_train[df_train['date']<=datetime.date(2017,5,31)]\nval_df = df_train[df_train['date']>datetime.date(2017,5,31)]\ndev_y = np.log1p(dev_df[\"totals_transactionRevenue\"].values)\nval_y = np.log1p(val_df[\"totals_transactionRevenue\"].values)\n\nuse_cols = [col for col in df_train.columns if col not in not_use_cols]\n\ndev_X = dev_df[use_cols] \nval_X = val_df[use_cols] \ntest_X = df_test[use_cols] \n\n# custom function to run light gbm model\ndef run_lgb(train_X, train_y, val_X, val_y, test_X):\n    params = {\n        \"objective\" : \"regression\",\n        \"metric\" : \"rmse\", \n        \"num_leaves\" : 30,\n        \"min_child_samples\" : 100,\n        \"learning_rate\" : 0.1,\n        \"bagging_fraction\" : 0.7,\n        \"feature_fraction\" : 0.5,\n        \"bagging_frequency\" : 5,\n        \"bagging_seed\" : 1989,\n        \"verbosity\" : -1,\n        'seed': 1989\n    }\n    \n    lgtrain = lgb.Dataset(train_X, label=train_y)\n    lgval = lgb.Dataset(val_X, label=val_y)\n    model = lgb.train(params, lgtrain, 10000, valid_sets=[lgtrain, lgval], early_stopping_rounds=500, verbose_eval=100)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model\n\n# Training the model #\npred_test, model = run_lgb(dev_X, dev_y, val_X, val_y, test_X)","b83bccf1":"sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\npred_test[pred_test<0] = 0\nsub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"Label_encoding.csv\", index=False)","0d1e7620":"def frequency_encoding(frame, col):\n    freq_encoding = frame.groupby([col]).size()\/frame.shape[0] \n    freq_encoding = freq_encoding.reset_index().rename(columns={0:'{}_Frequency'.format(col)})\n    return frame.merge(freq_encoding, on=col, how='left')","1dc9c3c4":"df_train = df_train_copy.copy()\ndf_test = df_test_copy.copy()","1c72805c":"len_train = df_train.shape[0]\ndf_all = pd.concat([df_train, df_test])\n\nfor col in tqdm(cat_cols):\n    df_all = frequency_encoding(df_all, col)\n\ndf_train = df_all[:len_train]\ndf_test = df_all[len_train:]\n\nprint(df_train.shape, df_test.shape)","adc336d2":"df_train.drop(cat_cols, axis=1, inplace=True)\ndf_test.drop(cat_cols, axis=1, inplace=True)\n\nfreq_cat_cols = ['{}_Frequency'.format(col) for col in cat_cols]\n\n# Split the train dataset into development and valid based on time \ndev_df = df_train[train_dates<=datetime.date(2017,5,31)]\nval_df = df_train[train_dates>datetime.date(2017,5,31)]\n\ndev_y = np.log1p(dev_df[\"totals_transactionRevenue\"].values)\nval_y = np.log1p(val_df[\"totals_transactionRevenue\"].values)\n\nuse_cols = [col for col in df_train.columns if col not in not_use_cols]\n\ndev_X = dev_df[use_cols] \nval_X = val_df[use_cols] \ntest_X = df_test[use_cols]  \n\n# Training the model #\npred_test, model = run_lgb(dev_X, dev_y, val_X, val_y, test_X)","416b9f10":"sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\npred_test[pred_test<0] = 0\nsub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"Freq_encoding.csv\", index=False)","c0f9ff3d":"from sklearn.model_selection import KFold","2a25d410":"def mean_k_fold_encoding(col, alpha):\n    target_name = 'totals_transactionRevenue'\n    target_mean_global = df_train[target_name].mean()\n    \n    nrows_cat = df_train.groupby(col)[target_name].count()\n    target_means_cats = df_train.groupby(col)[target_name].mean()\n    target_means_cats_adj = (target_means_cats*nrows_cat + \n                             target_mean_global*alpha)\/(nrows_cat+alpha)\n    # Mapping means to test data\n    encoded_col_test = df_test[col].map(target_means_cats_adj)\n    \n    kfold = KFold(n_splits=5, shuffle=True, random_state=1989)\n    parts = []\n    for trn_inx, val_idx in kfold.split(df_train):\n        df_for_estimation, df_estimated = df_train.iloc[trn_inx], df_train.iloc[val_idx]\n        nrows_cat = df_for_estimation.groupby(col)[target_name].count()\n        target_means_cats = df_for_estimation.groupby(col)[target_name].mean()\n\n        target_means_cats_adj = (target_means_cats * nrows_cat + \n                                target_mean_global * alpha) \/ (nrows_cat + alpha)\n\n        encoded_col_train_part = df_estimated[col].map(target_means_cats_adj)\n        parts.append(encoded_col_train_part)\n        \n    encoded_col_train = pd.concat(parts, axis=0)\n    encoded_col_train.fillna(target_mean_global, inplace=True)\n    encoded_col_train.sort_index(inplace=True)\n    \n    return encoded_col_train, encoded_col_test","f4cd5024":"df_train = df_train_copy.copy()\ndf_test = df_test_copy.copy()","211238cb":"for col in tqdm(cat_cols):\n    temp_encoded_tr, temp_encoded_te = mean_k_fold_encoding(col, 5)\n    new_feat_name = 'mean_k_fold_{}'.format(col)\n    df_train[new_feat_name] = temp_encoded_tr.values\n    df_test[new_feat_name] = temp_encoded_te.values","c70a6e75":"df_train.drop(cat_cols, axis=1, inplace=True)\ndf_test.drop(cat_cols, axis=1, inplace=True)\n\nmean_cat_cols = ['mean_k_fold_{}'.format(col) for col in cat_cols]\n\n# Split the train dataset into development and valid based on time \ndev_df = df_train[train_dates<=datetime.date(2017,5,31)]\nval_df = df_train[train_dates>datetime.date(2017,5,31)]\n\ndev_y = np.log1p(dev_df[\"totals_transactionRevenue\"].values)\nval_y = np.log1p(val_df[\"totals_transactionRevenue\"].values)\n\nuse_cols = [col for col in df_train.columns if col not in not_use_cols]\n\ndev_X = dev_df[use_cols] \nval_X = val_df[use_cols] \ntest_X = df_test[use_cols] \n\n# Training the model #\npred_test, model = run_lgb(dev_X, dev_y, val_X, val_y, test_X)","c84e0793":"sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\npred_test[pred_test<0] = 0\nsub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"mean_encoding.csv\", index=False)","6bf20a83":"df_train = df_train_copy.copy()\ndf_test = df_test_copy.copy()\n\nlen_train = df_train.shape[0]\ndf_all = pd.concat([df_train, df_test])\n\nfor col in tqdm(cat_cols):\n    df_all = frequency_encoding(df_all, col)\n\ndf_train = df_all[:len_train]\ndf_test = df_all[len_train:]\n\nprint(df_train.shape, df_test.shape)\n\nfor col in tqdm(cat_cols):\n    temp_encoded_tr, temp_encoded_te = mean_k_fold_encoding(col, 5)\n    new_feat_name = 'mean_k_fold_{}'.format(col)\n    df_train[new_feat_name] = temp_encoded_tr.values\n    df_test[new_feat_name] = temp_encoded_te.values","70dab4c7":"for col in cat_cols:\n    print(col)\n    lbl = preprocessing.LabelEncoder()\n    lbl.fit(list(df_train[col].values.astype('str')) + list(df_test[col].values.astype('str')))\n    df_train[col] = lbl.transform(list(df_train[col].values.astype('str')))\n    df_test[col] = lbl.transform(list(df_test[col].values.astype('str')))","82698392":"freq_cat_cols = ['{}_Frequency'.format(col) for col in cat_cols]\nmean_cat_cols = ['mean_k_fold_{}'.format(col) for col in cat_cols]","6267b7eb":"print(df_train.shape, df_test.shape)\n\n# Split the train dataset into development and valid based on time \ndev_df = df_train[train_dates<=datetime.date(2017,5,31)]\nval_df = df_train[train_dates>datetime.date(2017,5,31)]\ndev_y = np.log1p(dev_df[\"totals_transactionRevenue\"].values)\nval_y = np.log1p(val_df[\"totals_transactionRevenue\"].values)\n\nuse_cols = [col for col in df_train.columns if col not in not_use_cols]\n\ndev_X = dev_df[use_cols] \nval_X = val_df[use_cols] \ntest_X = df_test[use_cols]  \n\n# Training the model #\npred_test, model = run_lgb(dev_X, dev_y, val_X, val_y, test_X)","087c0dd6":"sub_df = pd.DataFrame({\"fullVisitorId\":test_id})\npred_test[pred_test<0] = 0\nsub_df[\"PredictedLogRevenue\"] = np.expm1(pred_test)\nsub_df = sub_df.groupby(\"fullVisitorId\")[\"PredictedLogRevenue\"].sum().reset_index()\nsub_df.columns = [\"fullVisitorId\", \"PredictedLogRevenue\"]\nsub_df[\"PredictedLogRevenue\"] = np.log1p(sub_df[\"PredictedLogRevenue\"])\nsub_df.to_csv(\"all.csv\", index=False)","c4d6a5a6":"# Background\n- As you know, there are features which have high cardinality in this competition.\n- I've studied and read some discussions, blogs and articles about high cardinality.\n- In this kernel, I'll experiment to see which encoding works better.\n- Label encoding, Frequency encoding and Mean encoding will be tested.\n- Because I'm student, I welcome your feedback on anything of this contents.!\n- Ok, Let's see!.\n- To compare, I forked great kernel https:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-baseline-ga-customer-revenue.\n- I set the same rrandom number and same parameters for each cases.\n- I want to recommend you, this kernel. https:\/\/www.kaggle.com\/vprokopev\/mean-likelihood-encodings-a-comprehensive-study. \n- This kernel experiments and explains these encodings. Very useful!","b4899c0a":"- Of course, it's not easy to say the all(Label + Fre + Mean) is the best choice.\n- But, adding other encoding performs better than only using label encoding.","5ed54529":"# Result\n| Encoding  | Training RMSE  | VALID RMSE  |  RMSE(Tr) \/ RMSE(vld)  | LB \n|---|---|---|---|---|\n| Label encoding |  1.52503  | 1.69546  |  1.111755 |  1.4470\n|  Frequency encoding | 1.52039  | 1.69291 | 1.113471 | 1.4545\n|  Mean encoding |  1.52247 | 1.6955  | 1.113651  |  1.4448\n|Label + Fre + Mean (All) | 1.51965 | 1.69179 | 1.1132761 |  1.4417","396eb3bf":"- Traning RMSE: All  < Freq < Mean < Label\n- Valid RMSE: All < Freq < Label < Mean\n- Tr\/vld ratio: Label < All < Freq < Mean","54d96659":"# Mean encoding","db4a3da3":"- Freq, Mean encoding tend to overfit to training set.\n- But LB of Mean encoding is lower than LB of Freq.","dd9287fd":"# Label + Frequency + Mean encoding","3a531ec7":"# Label-encoding","a8201ebc":"# Frequency encoding","289cdb96":"# More\n- I know, this experiment is so simple. The result could be changed depending on hyper-parameters and algoritm.\n- But, I think adding various encoding give us better performance than using only one encoding strategy because applying various approachs (like ensemble)  commoly shows good result.","fe3c6d8e":"# Conclusion"}}