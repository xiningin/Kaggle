{"cell_type":{"32d34bf1":"code","86057d9b":"code","0b9ebddd":"code","227f6174":"code","f60d09cc":"code","09b56006":"code","c223b057":"code","76aa7a26":"code","a15efdc0":"code","e6f6a437":"code","c6fa9c56":"code","e566070c":"code","77bfeae2":"code","da97b0e4":"code","9c08db47":"code","40c5bab6":"code","487a5ff0":"code","54041609":"code","5a5dcf5a":"code","8965524b":"code","726069d0":"code","725b078f":"code","868891fe":"code","5b5fe49d":"code","0756c8e3":"code","5afe93da":"code","8003d888":"code","9ad12587":"code","978e0caf":"code","a55598d0":"code","b10b2145":"code","33ad5688":"code","af2168aa":"code","dd7ca7c2":"code","c7e6335d":"code","6005b8ac":"code","c2d6fdf3":"code","3c3fff24":"code","db84828e":"code","14587e52":"code","f078569d":"code","36e1070f":"code","78194ca3":"code","191bbd05":"code","ab2b191e":"code","3f90518d":"code","eeae3f23":"code","60b8ad66":"code","6beb0d7d":"code","362ea71a":"code","eb5dc64b":"code","a21e2985":"code","277aed45":"code","1a24c886":"code","dbdb3ea5":"code","6f46c990":"code","6e74d63c":"code","6096e339":"code","0b742de6":"code","15071add":"code","45363541":"code","2458c1dd":"code","b44cde95":"code","9b8d539b":"code","08782f12":"code","8e1187aa":"code","9e4908c1":"code","360859b3":"code","eee82133":"code","0f39fb30":"code","1e1e4c92":"code","99c1c22b":"code","4931fef5":"code","af4df8fa":"code","61de81e2":"code","8fd90e9f":"code","8b4803e8":"code","3a0f778d":"markdown","8b1b07f6":"markdown","ee96c067":"markdown","c0c2b645":"markdown","4a7a5d3c":"markdown","7aca0290":"markdown","975be616":"markdown","38fcb784":"markdown","893387a1":"markdown","ac66336f":"markdown","d7e57494":"markdown","79291a3d":"markdown","e6f39d17":"markdown","2cdf70b3":"markdown","8870d470":"markdown","0014658b":"markdown","9caf4abd":"markdown","1ec4adb5":"markdown","b7d64fdb":"markdown","77704dd7":"markdown","2e59413a":"markdown","6eeed31c":"markdown","fdf53af0":"markdown","e1b00db4":"markdown","984e398f":"markdown","f98bfee8":"markdown","05a3a2c1":"markdown","a0a1ba44":"markdown","e1c09c7a":"markdown","f66987ec":"markdown","75c15cbe":"markdown","9dcba860":"markdown","46e6928b":"markdown","ac6be712":"markdown","ebb5ab48":"markdown","a3c92316":"markdown","46c9c1de":"markdown","c7739ffc":"markdown","623689ea":"markdown","f74d17ff":"markdown","d39dfcd0":"markdown","c19c62ce":"markdown","e63c7c68":"markdown","7b9af1e9":"markdown","b11cc24a":"markdown","58293b68":"markdown","6afa1594":"markdown","0702bd61":"markdown","853f50c1":"markdown","20946eed":"markdown","c2787702":"markdown","534c6ef3":"markdown","2792601d":"markdown","da979d2e":"markdown","aeb1295b":"markdown","7b68e9c1":"markdown","4c00de2e":"markdown","c256dc7f":"markdown","42cb7f51":"markdown","43b6964b":"markdown","088f1014":"markdown","68aeac2b":"markdown","e0a87266":"markdown","3c3f97d4":"markdown","11540797":"markdown","4ca868a5":"markdown","cd3ece96":"markdown","fd59612f":"markdown","0039a580":"markdown","51d19d04":"markdown","0629d823":"markdown","bdffeb27":"markdown","35e2eb6e":"markdown","eb0d166b":"markdown","3a07a760":"markdown","a1f1b94a":"markdown"},"source":{"32d34bf1":"# Data Analysis\nimport pandas as pd\nimport numpy as np\nfrom numpy import asarray\nfrom numpy import savetxt\nfrom numpy import loadtxt\nimport pickle as pkl\nfrom scipy import sparse\n\n# Data Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport wordcloud\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Text Processing\nimport re\nimport itertools\nimport string\nimport collections\nfrom collections import Counter\nfrom sklearn.preprocessing import LabelEncoder\nimport nltk\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import stopwords\nfrom nltk import word_tokenize\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n\n# Machine Learning packages\nimport sklearn\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nimport sklearn.cluster as cluster\nfrom sklearn.manifold import TSNE\n\n# Model training and evaluation\nfrom sklearn.model_selection import train_test_split\n\n#Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n#Metrics\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, accuracy_score, balanced_accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score, multilabel_confusion_matrix, confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# Ignore noise warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")","86057d9b":"#loading dataset\ndata_set = pd.read_csv(\"..\/input\/mbti-type\/mbti_1.csv\")\ndata_set.tail()","0b9ebddd":"data_set.isnull().any()","227f6174":"nRow, nCol = data_set.shape\nprint(f'There are {nRow} rows and {nCol} columns')","f60d09cc":"data_set.dtypes","09b56006":"data_set.info()","c223b057":"data_set.describe(include=['object'])","76aa7a26":"types = np.unique(np.array(data_set['type']))\ntypes","a15efdc0":"total = data_set.groupby(['type']).count()*50\ntotal","e6f6a437":"plt.figure(figsize = (12,4))\nplt.bar(np.array(total.index), height = total['posts'],)\nplt.xlabel('Personality types', size = 14)\nplt.ylabel('No. of posts available', size = 14)\nplt.title('Total posts for each personality type')","c6fa9c56":"#Plotting this in descending order for better understanding of this visualization\ncnt_srs = data_set['type'].value_counts()\nplt.figure(figsize=(12,4))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8)\nplt.xlabel('Personality types', fontsize=12)\nplt.ylabel('No. of posts availables', fontsize=12)\nplt.show()","e566070c":"df = data_set.copy()\n#this function counts the no of words in each post of a user\ndef var_row(row):\n    l = []\n    for i in row.split('|||'):\n        l.append(len(i.split()))\n    return np.var(l)\n\n#this function counts the no of words per post out of the total 50 posts in the whole row\ndf['words_per_comment'] = df['posts'].apply(lambda x: len(x.split())\/50)\ndf['variance_of_word_counts'] = df['posts'].apply(lambda x: var_row(x))\n\nplt.figure(figsize=(15,10))\nsns.swarmplot(\"type\", \"words_per_comment\", data=df)","77bfeae2":"plt.figure(figsize=(15,10))\nsns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df, kind=\"hex\")","da97b0e4":"def plot_jointplot(mbti_type, axs, titles):\n    df_1 = df[df['type'] == mbti_type]\n    sns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df_1, kind=\"hex\", ax = axs, title = titles)\n\nplt.figure(figsize=(24, 5))    \ni = df['type'].unique()\nk = 0\n\nfor m in range(1,3):\n  for n in range(1,7):\n    df_1 = df[df['type'] == i[k]]\n    sns.jointplot(\"variance_of_word_counts\", \"words_per_comment\", data=df_1, kind=\"hex\" )\n    plt.title(i[k])\n    k+=1\nplt.show()","9c08db47":"df[\"length_posts\"] = df[\"posts\"].apply(len)\nsns.distplot(df[\"length_posts\"]).set_title(\"Distribution of Lengths of all 50 Posts\")","40c5bab6":"#Finding the most common words in all posts.\nwords = list(df[\"posts\"].apply(lambda x: x.split()))\nwords = [x for y in words for x in y]\nCounter(words).most_common(40)","487a5ff0":"#Plotting the most common words with WordCloud.\nwc = wordcloud.WordCloud(width=1200, height=500, \n                         collocations=False, background_color=\"white\", \n                         colormap=\"tab20b\").generate(\" \".join(words))\n\n# collocations to False  is set to ensure that the word cloud doesn't appear as if it contains any duplicate words\nplt.figure(figsize=(25,10))\n# generate word cloud, interpolation \nplt.imshow(wc, interpolation='bilinear')\n_ = plt.axis(\"off\")","54041609":"fig, ax = plt.subplots(len(df['type'].unique()), sharex=True, figsize=(15,len(df['type'].unique())))\nk = 0\nfor i in df['type'].unique():\n    df_4 = df[df['type'] == i]\n    wordcloud = WordCloud(max_words=1628,relative_scaling=1,normalize_plurals=False).generate(df_4['posts'].to_string())\n    plt.subplot(4,4,k+1)\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.title(i)\n    ax[k].axis(\"off\")\n    k+=1","5a5dcf5a":"def extract(posts, new_posts):\n    for post in posts[1].split(\"|||\"):\n        new_posts.append((posts[0], post))\n\nposts = []\ndf.apply(lambda x: extract(x, posts), axis=1)\nprint(\"Number of users\", len(df))\nprint(\"Number of posts\", len(posts))\nprint(\"5 posts from start are:\")\nposts[0:5]","8965524b":"def preprocess_text(df, remove_special=True):\n    texts = df['posts'].copy()\n    labels = df['type'].copy()\n\n    #Remove links \n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'https?:\\\/\\\/.*?[\\s+]', '', x.replace(\"|\",\" \") + \" \"))\n    \n    #Keep the End Of Sentence characters\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\.', ' EOSTokenDot ', x + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'\\?', ' EOSTokenQuest ', x + \" \"))\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'!', ' EOSTokenExs ', x + \" \"))\n    \n    #Strip Punctation\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[\\.+]', \".\",x))\n\n    #Remove multiple fullstops\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^\\w\\s]','',x))\n\n    #Remove Non-words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'[^a-zA-Z\\s]','',x))\n\n    #Convert posts to lowercase\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: x.lower())\n\n    #Remove multiple letter repeating words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'([a-z])\\1{2,}[\\s|\\w]*','',x)) \n\n    #Remove very short or long words\n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{0,3})?\\b','',x)) \n    df[\"posts\"] = df[\"posts\"].apply(lambda x: re.sub(r'(\\b\\w{30,1000})?\\b','',x))\n\n    #Remove MBTI Personality Words - crutial in order to get valid model accuracy estimation for unseen data. \n    if remove_special:\n        pers_types = ['INFP' ,'INFJ', 'INTP', 'INTJ', 'ENTP', 'ENFP', 'ISTP' ,'ISFP' ,'ENTJ', 'ISTJ','ENFJ', 'ISFJ' ,'ESTP', 'ESFP' ,'ESFJ' ,'ESTJ']\n        pers_types = [p.lower() for p in pers_types]\n        p = re.compile(\"(\" + \"|\".join(pers_types) + \")\")\n    \n    return df\n\n#Preprocessing of entered Text\nnew_df = preprocess_text(data_set)","726069d0":"#Remove posts with less than X words\nmin_words = 15\nprint(\"Before : Number of posts\", len(new_df)) \nnew_df[\"no. of. words\"] = new_df[\"posts\"].apply(lambda x: len(re.findall(r'\\w+', x)))\nnew_df = new_df[new_df[\"no. of. words\"] >= min_words]\n\nprint(\"After : Number of posts\", len(new_df))","725b078f":"new_df.head()","868891fe":"# Converting MBTI personality (or target or Y feature) into numerical form using Label Encoding\n# encoding personality type\nenc = LabelEncoder()\nnew_df['type of encoding'] = enc.fit_transform(new_df['type'])\n\ntarget = new_df['type of encoding'] ","5b5fe49d":"new_df.head(15)","0756c8e3":"# The python natural language toolkit library provides a list of english stop words.\nprint(stopwords.words('english'))","5afe93da":"# Vectorizing the posts for the model and filtering Stop-words\nvect = CountVectorizer(stop_words='english') \n\n# Converting posts (or training or X feature) into numerical form by count vectorization\ntrain =  vect.fit_transform(new_df[\"posts\"])","8003d888":"train.shape","9ad12587":"X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.4, stratify=target, random_state=42)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","978e0caf":"accuracies = {}\n\n#Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100, random_state = 1)\nrandom_forest.fit(X_train, y_train)\n\n# make predictions for test data\nY_pred = random_forest.predict(X_test)\npredictions = [round(value) for value in Y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\naccuracies['Random Forest'] = accuracy* 100.0 \nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","a55598d0":"#XG boost Classifier\nxgb = XGBClassifier()\nxgb.fit(X_train,y_train)\n\nY_pred = xgb.predict(X_test)\npredictions = [round(value) for value in Y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\naccuracies['XG Boost'] = accuracy* 100.0\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n","b10b2145":"#Gradient Descent\nsgd = SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, y_train)\n\nY_pred = sgd.predict(X_test)\npredictions = [round(value) for value in Y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\naccuracies['Gradient Descent'] = accuracy* 100.0\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","33ad5688":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\nY_pred = logreg.predict(X_test)\npredictions = [round(value) for value in Y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\naccuracies['Logistic Regression'] = accuracy* 100.0\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","af2168aa":"#KNN Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(X_train, y_train)\n\nY_pred = knn.predict(X_test)\npredictions = [round(value) for value in Y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\naccuracies['KNN'] = accuracy* 100.0\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\n#try to find best k value\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(X_train, y_train)\n    scoreList.append(knn2.score(X_test, y_test))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(scoreList)*100\n\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))","dd7ca7c2":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(X_train, y_train)\n\nY_pred = svm.predict(X_test)\n\npredictions = [round(value) for value in Y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\naccuracies['SVM'] = accuracy* 100.0\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","c7e6335d":"pd.DataFrame.from_dict(accuracies, orient='index', columns=['Accuracies(%)'])","6005b8ac":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","c2d6fdf3":"X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=0.33, stratify=target, random_state=42)\nprint ((X_train.shape),(y_train.shape),(X_test.shape),(y_test.shape))","3c3fff24":"accuracies = {}\n\n#Random Forest\nrandom_forest = RandomForestClassifier(n_estimators=100, random_state = 1)\nrandom_forest.fit(X_train, y_train)\n\n# make predictions for test data\nY_pred = random_forest.predict(X_test)\npredictions = [round(value) for value in Y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\naccuracies['Random Forest'] = accuracy* 100.0 \nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","db84828e":"#XG boost Classifier\nxgb = XGBClassifier()\nxgb.fit(X_train,y_train)\n\nY_pred = xgb.predict(X_test)\npredictions = [round(value) for value in Y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\naccuracies['XG Boost'] = accuracy* 100.0\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","14587e52":"#Gradient Descent\nsgd = SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, y_train)\n\nY_pred = sgd.predict(X_test)\npredictions = [round(value) for value in Y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\naccuracies['Gradient Descent'] = accuracy* 100.0\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","f078569d":"# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\nY_pred = logreg.predict(X_test)\npredictions = [round(value) for value in Y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\naccuracies['Logistic Regression'] = accuracy* 100.0\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","36e1070f":"#KNN Classifier\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\nknn.fit(X_train, y_train)\n\nY_pred = knn.predict(X_test)\npredictions = [round(value) for value in Y_pred]\n\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\naccuracies['KNN'] = accuracy* 100.0\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n\n#try to find best k value\nscoreList = []\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors = i)  # n_neighbors means k\n    knn2.fit(X_train, y_train)\n    scoreList.append(knn2.score(X_test, y_test))\n    \nplt.plot(range(1,20), scoreList)\nplt.xticks(np.arange(1,20,1))\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score\")\nplt.show()\n\nacc = max(scoreList)*100\n\nprint(\"Maximum KNN Score is {:.2f}%\".format(acc))","78194ca3":"from sklearn.svm import SVC\nsvm = SVC(random_state = 1)\nsvm.fit(X_train, y_train)\n\nY_pred = svm.predict(X_test)\n\npredictions = [round(value) for value in Y_pred]\n# evaluate predictions\naccuracy = accuracy_score(y_test, predictions)\naccuracies['SVM'] = accuracy* 100.0\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","191bbd05":"pd.DataFrame.from_dict(accuracies, orient='index', columns=['Accuracies(%)'])","ab2b191e":"colors = [\"purple\", \"green\", \"orange\", \"magenta\",\"#CFC60E\",\"#0FBBAE\"]\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,10))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","3f90518d":"data = pd.read_csv(\"..\/input\/mbti-type\/mbti_1.csv\")\ndata","eeae3f23":"def get_types(row):\n    t=row['type']\n\n    I = 0; N = 0\n    T = 0; J = 0\n    \n    if t[0] == 'I': I = 1\n    elif t[0] == 'E': I = 0\n    else: print('I-E not found') \n        \n    if t[1] == 'N': N = 1\n    elif t[1] == 'S': N = 0\n    else: print('N-S not found')\n        \n    if t[2] == 'T': T = 1\n    elif t[2] == 'F': T = 0\n    else: print('T-F not found')\n        \n    if t[3] == 'J': J = 1\n    elif t[3] == 'P': J = 0\n    else: print('J-P not found')\n    return pd.Series( {'IE':I, 'NS':N , 'TF': T, 'JP': J }) \n\ndata = data.join(data.apply (lambda row: get_types (row),axis=1))\ndata.head(5)","60b8ad66":"print (\"Introversion (I) \/  Extroversion (E):\\t\", data['IE'].value_counts()[0], \" \/ \", data['IE'].value_counts()[1])\nprint (\"Intuition (N) \/ Sensing (S):\\t\\t\", data['NS'].value_counts()[0], \" \/ \", data['NS'].value_counts()[1])\nprint (\"Thinking (T) \/ Feeling (F):\\t\\t\", data['TF'].value_counts()[0], \" \/ \", data['TF'].value_counts()[1])\nprint (\"Judging (J) \/ Perceiving (P):\\t\\t\", data['JP'].value_counts()[0], \" \/ \", data['JP'].value_counts()[1])","6beb0d7d":"#Plotting the distribution of each personality type indicator\nN = 4\nbottom = (data['IE'].value_counts()[0], data['NS'].value_counts()[0], data['TF'].value_counts()[0], data['JP'].value_counts()[0])\ntop = (data['IE'].value_counts()[1], data['NS'].value_counts()[1], data['TF'].value_counts()[1], data['JP'].value_counts()[1])\n\nind = np.arange(N)    # the x locations for the groups\n# the width of the bars\nwidth = 0.7           # or len(x) can also be used here\n\np1 = plt.bar(ind, bottom, width, label=\"I, N, T, F\")\np2 = plt.bar(ind, top, width, bottom=bottom, label=\"E, S, F, P\") \n\nplt.title('Distribution accoss types indicators')\nplt.ylabel('Count')\nplt.xticks(ind, ('I \/ E',  'N \/ S', 'T \/ F', 'J \/ P',))\nplt.legend()\n\nplt.show()","362ea71a":"data[['IE','NS','TF','JP']].corr()","eb5dc64b":"cmap = plt.cm.RdBu\ncorr = data[['IE','NS','TF','JP']].corr()\nplt.figure(figsize=(12,10))\nplt.title('Features Correlation Heatmap', size=15)\nsns.heatmap(corr, cmap=cmap,  annot=True, linewidths=1)","a21e2985":"lemmatiser = WordNetLemmatizer()\n\n# Remove the stop words for speed \nuseless_words = stopwords.words(\"english\")\n\n# Remove these from the posts\nunique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n       'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\nunique_type_list = [x.lower() for x in unique_type_list]\n\n# Or we can use Label Encoding (as above) of this unique personality type indicator list\n# from sklearn.preprocessing import LabelEncoder\n# unique_type_list = ['INFJ', 'ENTP', 'INTP', 'INTJ', 'ENTJ', 'ENFJ', 'INFP', 'ENFP',\n#        'ISFP', 'ISTP', 'ISFJ', 'ISTJ', 'ESTP', 'ESFP', 'ESTJ', 'ESFJ']\n# lab_encoder = LabelEncoder().fit(unique_type_list)","277aed45":"# Splitting the MBTI personality into 4 letters and binarizing it\n\nb_Pers = {'I':0, 'E':1, 'N':0, 'S':1, 'F':0, 'T':1, 'J':0, 'P':1}\nb_Pers_list = [{0:'I', 1:'E'}, {0:'N', 1:'S'}, {0:'F', 1:'T'}, {0:'J', 1:'P'}]\n\ndef translate_personality(personality):\n    # transform mbti to binary vector\n    return [b_Pers[l] for l in personality]\n\n#To show result output for personality prediction\ndef translate_back(personality):\n    # transform binary vector to mbti personality\n    s = \"\"\n    for i, l in enumerate(personality):\n        s += b_Pers_list[i][l]\n    return s\n\nlist_personality_bin = np.array([translate_personality(p) for p in data.type])\nprint(\"Binarize MBTI list: \\n%s\" % list_personality_bin)","1a24c886":"def pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True):\n  list_personality = []\n  list_posts = []\n  len_data = len(data)\n  i=0\n  \n  for row in data.iterrows():\n      # check code working \n      # i+=1\n      # if (i % 500 == 0 or i == 1 or i == len_data):\n      #     print(\"%s of %s rows\" % (i, len_data))\n\n      #Remove and clean comments\n      posts = row[1].posts\n\n      #Remove url links \n      temp = re.sub('http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', posts)\n\n      #Remove Non-words - keep only words\n      temp = re.sub(\"[^a-zA-Z]\", \" \", temp)\n\n      # Remove spaces > 1\n      temp = re.sub(' +', ' ', temp).lower()\n\n      #Remove multiple letter repeating words\n      temp = re.sub(r'([a-z])\\1{2,}[\\s|\\w]*', '', temp)\n\n      #Remove stop words\n      if remove_stop_words:\n          temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ') if w not in useless_words])\n      else:\n          temp = \" \".join([lemmatiser.lemmatize(w) for w in temp.split(' ')])\n          \n      #Remove MBTI personality words from posts\n      if remove_mbti_profiles:\n          for t in unique_type_list:\n              temp = temp.replace(t,\"\")\n\n      # transform mbti to binary vector\n      type_labelized = translate_personality(row[1].type) #or use lab_encoder.transform([row[1].type])[0]\n      list_personality.append(type_labelized)\n      # the cleaned data temp is passed here\n      list_posts.append(temp)\n\n  # returns the result\n  list_posts = np.array(list_posts)\n  list_personality = np.array(list_personality)\n  return list_posts, list_personality\n\nlist_posts, list_personality  = pre_process_text(data, remove_stop_words=True, remove_mbti_profiles=True)\n\nprint(\"Example :\")\nprint(\"\\nPost before preprocessing:\\n\\n\", data.posts[0])\nprint(\"\\nPost after preprocessing:\\n\\n\", list_posts[0])\nprint(\"\\nMBTI before preprocessing:\\n\\n\", data.type[0])\nprint(\"\\nMBTI after preprocessing:\\n\\n\", list_personality[0])","dbdb3ea5":"nRow, nCol = list_personality.shape\nprint(f'No. of posts = {nRow}  and No. of Personalities = {nCol} ')","6f46c990":"# Vectorizing the database posts to a matrix of token counts for the model\ncntizer = CountVectorizer(analyzer=\"word\", \n                             max_features=1000,  \n                             max_df=0.7,\n                             min_df=0.1) \n# the feature should be made of word n-gram \n# Learn the vocabulary dictionary and return term-document matrix\nprint(\"Using CountVectorizer :\")\nX_cnt = cntizer.fit_transform(list_posts)\n\n#The enumerate object yields pairs containing a count and a value (useful for obtaining an indexed list)\nfeature_names = list(enumerate(cntizer.get_feature_names()))\nprint(\"10 feature names can be seen below\")\nprint(feature_names[0:10])\n\n# For the Standardization or Feature Scaling Stage :-\n# Transform the count matrix to a normalized tf or tf-idf representation\ntfizer = TfidfTransformer()\n\n# Learn the idf vector (fit) and transform a count matrix to a tf-idf representation\nprint(\"\\nUsing Tf-idf :\")\n\nprint(\"Now the dataset size is as below\")\nX_tfidf =  tfizer.fit_transform(X_cnt).toarray()\nprint(X_tfidf.shape)","6e74d63c":"#counting top 10 words\nreverse_dic = {}\nfor key in cntizer.vocabulary_:\n    reverse_dic[cntizer.vocabulary_[key]] = key\ntop_10 = np.asarray(np.argsort(np.sum(X_cnt, axis=0))[0,-10:][0, ::-1]).flatten()\n[reverse_dic[v] for v in top_50]","6096e339":"personality_type = [ \"IE: Introversion (I) \/ Extroversion (E)\", \"NS: Intuition (N) \/ Sensing (S)\", \n                   \"FT: Feeling (F) \/ Thinking (T)\", \"JP: Judging (J) \/ Perceiving (P)\"  ]\n\nfor l in range(len(personality_type)):\n    print(personality_type[l])","0b742de6":"print(\"X: 1st posts in tf-idf representation\\n%s\" % X_tfidf[0])","15071add":"print(\"For MBTI personality type : %s\" % translate_back(list_personality[0,:]))\nprint(\"Y : Binarized MBTI 1st row: %s\" % list_personality[0,:])","45363541":"# Posts in tf-idf representation\nX = X_tfidf","2458c1dd":"#Random Forest model for MBTI dataset\n# Individually training each mbti personlity type\nfor l in range(len(personality_type)):\n    \n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\n    # fit model on training data\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    \n    predictions = [round(value) for value in y_pred]\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    \n    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))","b44cde95":"#XGBoost model for MBTI dataset \n# Individually training each mbti personlity type\nfor l in range(len(personality_type)):\n    \n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\n    # fit model on training data\n    model = XGBClassifier()\n    model.fit(X_train, y_train)\n\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    \n    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))","9b8d539b":"# Stocastic Gradient Descent for MBTI dataset\n# Individually training each mbti personlity type\nfor l in range(len(personality_type)):\n\n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\n    # fit model on training data\n    model = SGDClassifier() \n    model.fit(X_train, y_train)\n\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    \n    predictions = [round(value) for value in y_pred]\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    \n    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))","08782f12":"# Logistic Regression for MBTI dataset\n# Individually training each mbti personlity type\nfor l in range(len(personality_type)):\n\n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\n    # fit model on training data\n    model = LogisticRegression() \n    model.fit(X_train, y_train)\n\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    \n    predictions = [round(value) for value in y_pred]\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    \n    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))","8e1187aa":"#2 KNN model for MBTI dataset\n# Individually training each mbti personlity type\nfor l in range(len(personality_type)):\n\n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\n    # fit model on training data\n    model = KNeighborsClassifier(n_neighbors = 2)  # n_neighbors means k\n    model.fit(X_train, y_train)\n\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    \n    predictions = [round(value) for value in y_pred]\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n   \n    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))","9e4908c1":"# SVM model for MBTI dataset\n# Individually training each mbti personlity type\nfor l in range(len(personality_type)):\n    \n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\n    # fit model on training data\n    model = SVC(random_state = 1)\n    model.fit(X_train, y_train)\n\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    \n    predictions = [round(value) for value in y_pred]\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    \n    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))","360859b3":"# setup parameters for xgboost\nparam = {}\n\nparam['n_estimators'] = 200 #100\nparam['max_depth'] = 2 #3\nparam['nthread'] = 8 #1\nparam['learning_rate'] = 0.2 #0.1\n\n# Individually training each mbti personlity type\nfor l in range(len(personality_type)):\n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    seed = 7\n    test_size = 0.33\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)\n\n    # fit model on training data\n    model = XGBClassifier(**param)\n    model.fit(X_train, y_train)\n    # make predictions for test data\n    y_pred = model.predict(X_test)\n    predictions = [round(value) for value in y_pred]\n    # evaluate predictions\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"%s Accuracy: %.2f%%\" % (personality_type[l], accuracy * 100.0))","eee82133":"my_posts  = \"\"\" Hi I am 21 years, currently, I am pursuing my graduate degree in computer science and management (Mba Tech CS ), It is a 5-year dual degree.... My CGPA to date is 3.8\/4.0 . I have a passion for teaching since childhood. Math has always been the subject of my interest in school. Also, my mother has been one of my biggest inspirations for me. She started her career as a teacher and now has her own education trust with preschools schools in Rural and Urban areas. During the period of lockdown, I dwelled in the field of blogging and content creation on Instagram.  to spread love positivity kindness . I hope I am able deliver my best to the platform and my optimistic attitude helps in the growth that is expected. Thank you for the opportunity. \"\"\"\n\n# The type is just a dummy so that the data prep function can be reused\nmydata = pd.DataFrame(data={'type': ['INFJ'], 'posts': [my_posts]})\n\nmy_posts, dummy  = pre_process_text(mydata, remove_stop_words=True, remove_mbti_profiles=True)\n\nmy_X_cnt = cntizer.transform(my_posts)\nmy_X_tfidf =  tfizer.transform(my_X_cnt).toarray()","0f39fb30":"# setup parameters for xgboost\nparam = {}\nparam['n_estimators'] = 200\nparam['max_depth'] = 2\nparam['nthread'] = 8\nparam['learning_rate'] = 0.2\n\n#XGBoost model for MBTI dataset\nresult = []\n# Individually training each mbti personlity type\nfor l in range(len(personality_type)):\n    print(\"%s classifier trained\" % (personality_type[l]))\n    \n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\n    # fit model on training data\n    model = XGBClassifier(**param)\n    model.fit(X_train, y_train)\n    \n    # make predictions for my  data\n    y_pred = model.predict(my_X_tfidf)\n    result.append(y_pred[0])","1e1e4c92":"print(\"The result is: \", translate_back(result))","99c1c22b":"my_posts = \"\"\" They act like they care They tell me to share But when I carve the stories on my arm The doctor just calls it self harm I\u2019m not asking for attention There\u2019s a reason I have apprehensions I just need you to see What has become of me||| I know I\u2019m going crazy But they think my thoughts are just hazy When in that chaos, in that confusion I\u2019m crying out for help, to escape my delusions||| Mental health is a state of mind How does one keep that up when assistance is denied All my failed attempts to fight the blaze You treat it like its a passing phase||| Well stop, its not, because mental illness is real Understand that we\u2019re all not made of steel Because when you brush these issues under the carpet You make it seem like its our mistake we\u2019re not guarded||| Don\u2019t you realise that its a problem that needs to be addressed Starting at home, in our nest Why do you keep your mouths shut about such things Instead of caring for those with broken wings||| What use is this social stigma When mental illness is not even such an enigma Look around and you\u2019ll see the numbers of the affected hiding under the covers ||| This is an issue that needs to be discussed Not looked down upon with disgust Mental illness needs to be accepted So that people can be protected ||| Let me give you some direction People need affection The darkness must be escaped Only then the lost can be saved||| Bring in a change Something not very strange The new year is here Its time to eradicate fear||| Recognise the wrists under the knives To stop mental illness from taking more lives Let\u2019s break the convention Start \u2018suicide prevention\u2019.||| Hoping the festival of lights drives the darkness of mental illness away\"\"\"\nmydata = pd.DataFrame(data={'type': ['INFP'], 'posts': [my_posts]})\nmy_posts, dummy  = pre_process_text(mydata, remove_stop_words=True, remove_mbti_profiles=True)\nmy_X_cnt = cntizer.transform(my_posts)\nmy_X_tfidf =  tfizer.transform(my_X_cnt).toarray()","4931fef5":"# setup parameters for xgboost\nparam = {}\nparam['n_estimators'] = 200\nparam['max_depth'] = 2\nparam['nthread'] = 8\nparam['learning_rate'] = 0.2\n\n#XGBoost model for MBTI dataset\nresult = []\n# Individually training each mbti personlity type\nfor l in range(len(personality_type)):\n    print(\"%s classifier trained\" % (personality_type[l]))\n    \n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\n    # fit model on training data\n    model = XGBClassifier(**param)\n    model.fit(X_train, y_train)\n    \n    # make predictions for my  data\n    y_pred = model.predict(my_X_tfidf)\n    result.append(y_pred[0])","af4df8fa":"print(\"The result is: \", translate_back(result))","61de81e2":"my_posts  = \"\"\" I dont think anyone would be able to live 300 years i am not talking about the physical ability to do so but the mental fortitude unless you decide to live away from civilization it simply is not possible.|||Believe me you would not want to live for that long alone , unless there are others who can live for 300 years as well.|||You cannot enjoy something if you say something to yourself like \u2018I wanna enjoy this , i think this thing is gonna be fun\u2019 believe me it doesn\u2019t work.|||I think this problem might be face by a lot of people.|||Firstly you should only study stuff that interests you . (obvious)|||Now there are subjects that you school forces you to take and you have no option but to some how score in those subjects. (What i used to think is subjects like history , geography and most of all Hindi are utterly useless , i wanna be a programmer why do i study these)|||But because i had no choice i thought lets give these a try. I started questioning things and when i dug deep into the events of history and and why each event took place or how it was of benefit to the benefactor. This questioning and finding out the reasons made me like history.|||My point is unless ya\u2019ll start questioning and researching further than whats necessary for exams you wont like that subject. All the subjects are beautiful , its what you choose to see. Basically give everything a real shot in life , everything works out. (my advice seems ironical as if you dont like the subject and i am telling you to research further but try it once )|||And also never study because there is exam or because you have to do an assignment or because someone is told you to or because \u2026.|||But please ONLY STUDY BECAUSE YOU FEEL LIKE AND BECAUSE YOU WANT TO. Until you develop this sense of want to study it will be hard for you to like it. You must like it so much , that you know when people say after studying maths for 1 hour they took a break of 20 mins watching their favorite tv show(lets say friends)|||For you it should be like after 1 hour of x(that you dont like but HAVE to do) activity you take a break of 20 mins and you study , (like i like reading article on ai so i do that) you might like bio you will do that what i want to say is that is what it means to like something and only then you can truly enjoy it.|||If only something known as luck existed. (no offence to the readers or person who asked the question)|||Luck is a really interesting term , a really complex illusion. What i am saying is there is nothing known as luck that exists. Something simply doesnt just happen. It happens for a reason and with a reason.|||Some over here might claim that if it is not luck then what is it that cause (cause a child to be born in a rich family or a person to be saved by weirdest phenomenon and escape death.)|||What i want ya\u2019ll to know is firstly that being born rich cannot be called as \u201clucky\u201d like we cannot say to be born in a rich family is particularly a good thing there are many reasons to this (some people like to work their way up , Some want to experience the life troubles, well whatever the reasons might be) So firstly when we say something to be lucky we just CONSIDER that what happened was good. Same goes with the case of being saved from and awful accident. We still dont know the purpose of life or our existence and hence we dont know if living is a boon. this might be harsh for some but Reality is Harsh.|||What want ya\u2019ll to know is never feel bad if something good(in your perspective) happens to someone as It might as well not turn out to be good if you see the BIG picture.|||Besides its also a good thing to think this way as its boosts up your hopes , like you might consider that everything that has happened to you has made you what you are and even if you don\u2019t appreciate your conditions there is someone somewhere who would want to be in that.|||I think the all of us are 100% selfish. (no offence)|||The thing is even if we say we care about someone and then we help that person in reality we are just making ourselves happy by helping that someone.|||What i mean to say is even when people talk about sacrifices for others the reality is that sacrifice made that person or those persons happy which you cared for and thus those people being happy makes you happy.|||Everything comes down to you. You can try and deny it but you all know it.|||Now about those people who sacrifice their lives for others that is a peculiar case , and here too (this might be hard for some to believe) but they sacrifice life for someone they loved (they thought they loved) but the truth is in a situation where a person sacrifices his life for another the truth is that if he hadnt dont that he couldn\u2019t have survived without that person anyways and then there are always some who seek glory.|||I hope you get the point. Even when you say that people spend 30s and 40s the truth is making their kids life perfect gives them happiness. There are people without kids too cause for these making their kids life perfect doesnt give them as much happiness as focusing on their own goals might.|||Now i believe there might be many who thought that making kids life perfect might give them happiness but it turns out to be false and then they are stuck there fulfilling moral obligations. It all comes down to your resolves and how firm you are in you decisions.|||Isn\u2019t it fun to watch our disciples fight among themselves to prove that only one of us exists!|||I tend to believe that everything in this universe HAS TO HAVE A PURPOSE. Rather than thinking that the universe is a useless place and we have no purpose i would rather think we are just too stupid and dont know or cannot find the purpose. I have always wondered that what would be our reason to exist , once i thought of us (humans on earth) a crop created by aliens that takes this long to grow (i mean may be it would be not possible to create humans by a process other than evolution) so the aliens started the life on earth and are now just waiting for us to evolve , and so our purpose according to that is nothing but to serve as food or may be what ever they want , now then the question arises is what would their purpose be , And all such hypothetical situation lead to to scenario to go into infinity.|||After a lot of such crazy thought i came to 2 conclusions , 1 is pretty simple our purpose of life is find a purpose for our lives , and the weird thing is unlike other things once you find what your were looking for , the process ends there. The paradox is after you find your purpose of life did your succeed in finishing your purpose of life or did you just begun ?|||The 2nd one is what most people should agree is happiness. Now I think that this happiness should be confined to YOUR\u2019s and ONLY your Happiness. Now the thing is some people gain happiness by giving people happiness.|||Let happiness be a quantifiable entity. We shall say that we start 0 oh hp . |||The zone where we dont feel happy or sad is 0 . Anything above 0 means you are happy and less than 0 is you are sad .|||Now one might think that if we suppose attain 100 hp , and do nothing after that we shall remain happy , the problem is that after a while our bar above which we remain moves up (simple adaptation) so now anything below 100hp is sad . This is the main problem with happiness , and So we need to keep doing stuff and increasing our hp . So maybe this counts as a purpose of life.|||Like when we are told about those saints and stuff who abandon society and live their life alone in discrete places where no one can disturb them. I believe the simple reason for this they have found happiness in doing nothing . Like someone finds happiness in making more money (there is a possibility that you might not make and hence be sad ) So these saint type people do something that has no opposite , like i know when they do nothing (i know its opposite is everything) and thats the very reason they go away from the civilization where they literally will have to do nothing . I also tend to think of these people as cowards who fear that they might loose at point.|||But the very point arises is what did you gain by gaining happiness. So again there is no end so Keep chasing the infinity its pointless but Keeps you busy(till the point you are alive) then after you are dead i guess nothing matters.|||But anyways thats the most easy thing to obtain happiness without sadness.|||But the very point arises is what did you gain by gaining happiness. So again there is no end so Keep chasing the infinity its pointless but Keeps you busy(till the point you are alive) then after you are dead i guess nothing matters.|||But if the life after death theory is true , and if by chance are memories are also transferred every time we are reborn. We all are fucked We are for infinity stuck chasing the infinity . (I wonder what happens when infinity chases infinity It will be fun to watch !) |||I think rather than worrying about these things we should just enjoy life . Because there will always something that we dont know that we dont know and thus we shall never know that. this is just 2 loops of not know , you can reach it , but Do it infinitely . Infinity is a bitch.|||Well To all beings good luck finding a purpose and to all those who know that it doesnt exist ya\u2019ll are fucked.\"\"\"\nmydata = pd.DataFrame(data={'type': ['ENTP'], 'posts': [my_posts]})\nmy_posts, dummy  = pre_process_text(mydata, remove_stop_words=True, remove_mbti_profiles=True)\nmy_X_cnt = cntizer.transform(my_posts)\nmy_X_tfidf =  tfizer.transform(my_X_cnt).toarray()","8fd90e9f":"# setup parameters for xgboost\nparam = {}\nparam['n_estimators'] = 200\nparam['max_depth'] = 2\nparam['nthread'] = 8\nparam['learning_rate'] = 0.2\n\n#XGBoost model for MBTI dataset\nresult = []\n# Individually training each mbti personlity type\nfor l in range(len(personality_type)):\n    print(\"%s classifier trained\" % (personality_type[l]))\n    \n    Y = list_personality[:,l]\n\n    # split data into train and test sets\n    seed = 7\n    test_size = 0.33\n    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=7)\n\n    # fit model on training data\n    model = XGBClassifier(**param)\n    model.fit(X_train, y_train)\n    \n    # make predictions for my  data\n    y_pred = model.predict(my_X_tfidf)\n    result.append(y_pred[0])","8b4803e8":"print(\"The result is: \", translate_back(result)) ","3a0f778d":"## Comparing Algorithms","8b1b07f6":"\n\n*   It is inferenced that a lot of hyperlinks are presnt in these posts\n*   It is safe to assume that url links do not provide any real information about a user's personality, hence, we need to clean our dataset for these too.\n\nThis given sample dataset does not come from the entire Kaggle user population; rather, it comes from Kaggle users who leave comments; thus, our ML model's conclusion cannot be applied to all Kaggle users, only to those who leave comments. \n\nFurthermore, with more data, more accurate models could be obtained. As a result, the model may fail to classify a personality at the lower end.\n\n\n\n","ee96c067":"From this heatmap also, it is unclear if it shows anything valuable for interpretation","c0c2b645":"## Pesonality Prediction 3 - short essay","4a7a5d3c":"\n\n*   For all the plots you can see that most of the posts have words btw 100-150 and most of no. of words per comment by a user is nearly 25 to 30 range.\n*   Exception to this case is for the plots for ISPJ and ISTJ, but this maybe due to the fact that there are significantly less no. of posts available for these personality types (further shown by the bar plots below)\n\n*   We can see that there is no correlation observed between variance of word count and the words per comment.\n*   But there is a weak negative correlation observed between the 2 features for few personalities. Maybe this could be due to the low no. of posts available for that type in the given Kaggle dataset.\n*   No useful inferences can be made by analyzing the individual jointplots as the total no of posts for each personlaity type is different.\n\nHence, these features will not be useful in building our Personality prediction model.\n\n","7aca0290":"*  we can see there are a no. of irrelevant words present in the dataset (e.g. ha, ar, Ti etx.) which will need to be removed\n*  Interestingly, among the most common words in the word clouds of individual personality types, is the names of MBTI personlity types themselves.\n\nIt would hence be necessary to clean our posts by removing these MBTI words from each of them as part of our pre-processing stage, before training the model for better evaluation results.","975be616":"\n\n*   We choose label encoding over one-hot encoding to reduce the pre-processing time, and majority due to the fact that there are predefined 16 values under MBTI and assigning unique integer based on alphabetical ordering seems like a viable option\n*   It seems like a better option to the curse of dimensionality in the feature space.\n\n","38fcb784":"**The link to our dataset**:\nhttps:\/\/www.kaggle.com\/datasnaek\/mbti-type\n\nThis dataset from Kaggle comes with two columns: the Myers-Briggs type of a user and 50 user posts stored as strings. Here is an example of the last 5 rows.","893387a1":"- We have successfully removed all irrelevant words ","ac66336f":"Data visualization for no. of posts for each personality type","d7e57494":"# Exploratory data analysis","79291a3d":"You have fun and can check your own personality from 16 personalities website https:\/\/www.16personalities.com\/","e6f39d17":"# Four Classifiers across MBTI axis","2cdf70b3":"## Personality Prediction 2 - a poem","8870d470":"Group by allows you to split your data into separate groups to perform computations for better analysis.\n\n","0014658b":"**Counting** No. of posts in one class \/ Total no. of posts in the other class","9caf4abd":"\n**Tf\u2013idf** for feature engineering evaluates how relevant\/important a word is to a document in a collection of documnets or corpus. As we train individual classifiers here, it is very useful for scoring words in machine learning algorithms for Natural Language Processing. \n\nFor our model we vectorize using count vectorizer and tf-idf vectorizer keeping the words appearing btw 10% to 70% of the posts.","1ec4adb5":"An assumption made in our model is that each letter type is independent of other types i.e. A person\u2019s introversion\/extroversion is not related to their judgement\/perception. Nevertheless, we want to still test them below using a heat map","b7d64fdb":"## Feature Engineering","77704dd7":"**1.  LabelEncoder** : Provided by Sklearn library that converts the the levels of categorical features (labels) into numeric form so as to convert it into the machine-readable form. It encode labels with a value between 0 and n_classes-1 where n is the number of distinct labels. If a label repeats it assigns the same value to as assigned earlier. \n","2e59413a":"## Comparing Algorithms","6eeed31c":"## Training & Evaluating Models","fdf53af0":"Inference : test_size=0.3 gives marginally better results for all algorithms\n\n\nAs we can the above ML classifiers performs at efficieny of nearly 50% only - which is pretty bad. So, instead of selecting all 16 types of personalities as a unique feature, we hence train 4 classifiers individually to classify their personalities based on MBTI type.\n\n\n\n---\n\n\n\nThe Myers Briggs Type Indicator (or MBTI for short) is a personality type system that divides everyone into **16 distinct personality types across 4 axis**:\n\n- Introversion (I) \u2013 Extroversion (E)\n- Intuition (N) \u2013 Sensing (S)\n- Thinking (T) \u2013 Feeling (F)\n- Judging (J) \u2013 Perceiving (P)\n","e1b00db4":"**2.  CountVectorizer** is used to convert a collection of text documents to a vector of term\/token counts and build a vocabulary of known words, but also to encode new documents using that vocabulary. It also enables the pre-processing of text data prior to generating the vector representation. \n\nHere, we use **stop_words='english'** with CountVectorizer since this just counts the occurrences of each word in its vocabulary, extremely common words like \u2018the\u2019, \u2018and\u2019, etc. will become very important features while they add little meaning to the text. This is an important step in pre-processing as our model can often be improved if you don\u2019t take those words into account. \n","984e398f":"\n\n*   There are 16 unique personality type indicators in the dataset\n*   INFP is the most frequently occuring personality type in our dataset\n\n (no. of occurences is 1832)\n*   Lastly, there are no repeating posts in the dataset\n","f98bfee8":"## Pre-Processing Stage","05a3a2c1":"Let's see how the posts look in Binarized MBTI personality indicator representation: (we have taken 1st post for demonstration)\n\n","a0a1ba44":"**SWARM PLOT** :\n\nSwarm Plots, also called beeswarm plots, they plot all of the data points \n\n","e1c09c7a":"*Fun Fact : The above results match with real life findings by researchers across various personality and psycological studies like*\n\n\n\n\n\n\nWe can compare this with the fact that **Introverts** are a minority, making up roughly 16 percent of people [[1]](https:\/\/celesteheadlee.medium.com\/congratulations-youre-probably-not-an-introvert-8bcfcc918a18#:~:text=Even%20among%20introverts%2C%20though%2C%20there,innate%20needs%20for%20Homo%20sapiens.). Eventhough among introverts, there are varying degrees, and Carl Jung said, \u201cThere is no such thing as a pure **Extrovert** or a pure introvert\" Hence it is tricky to classify a person with 1 type. \n\nWhile the population is split roughly 50\/50 on the other dimensions, a full 70% of people show a preference for **Sensing** over **Intuition** when taking a personality test. Because Intuitives are the minority, the onus is on them to adjust to the Sensor way of thinking.\n\nThe differences between **Judging** and **Perceiving** are probably the most marked differences of all the four preferences. People with strong Judging preferences might have a hard time accepting people with strong Perceiving preferences, and vice-versa. On the other hand, a \"mixed\" couple (one Perceiving and one Judging) can complement each other very well, if they have developed themselves enough to be able to accept each other's differences.","f66987ec":"- Therefore we now have 595 features for each user post.","75c15cbe":"Checking if there are any missing or null values present in the dataset.","9dcba860":"**Features Correlation Analysis**","46e6928b":"*   The 2 histogram plots represent Gaussian distribution of a sample space, which in our case comprises of no. of words per comment and associated variance of word counts from our dataset.\n*   In the hexagonal plot, the hexagon with most number of points gets **darker color**. So if you look at the above plot, you can see that most of the posts have words between 100 and 150 and most of no. of words per comment by a user is between 25-30.\n*   We can see that there is no correlation observed between variance of word count and the words per comment.\n*   There is a strong relationship when there are 25-30 words per comment & the variance of word counts is 100-150\n*   This is also visible by analyzing the histogram plots on both the axis.\n\n","ac6be712":"---\nNow we train the model in multiple ML algorithms namely: Random Forest, XGBoost, Gradient Descent, Support Vector Machine,  in order to choose the classifier which shows the best accuracy results. \n\nAdditionaly, we split the dataset into testing and training in multiple ratios to find out which gives the best results. ","ebb5ab48":"\n\n*   There are only 2 columns in the dataset\n*   Total no. of rows are 8675\n*   There are no null values present in the dataset\n*   One Disadvantage is that all values are textual, hence they have to be converted to numerical form to train the ML model\n\n","a3c92316":"## Splitting into X and Y feature","46c9c1de":"- It is unclear if the matrix shows anything valuable for interpretation\n\n\n\n","c7739ffc":"Let's see how the posts look in TF-IDF representation: (we have taken 1st post for demonstration)","623689ea":"Cleaning of data in the posts","f74d17ff":"We can clearly see that this model underfits out dataset when we apply split ratio of 60:40 on our dataset i.e. the model has  not learned enough from the training data, resulting in low generalization and unreliable predictions. (almost all the results are near 50%, which is not good)","d39dfcd0":"# Pre-Processing Stage\n**NOTE**: this phase might take some time to execute due to computationaly expensive operations","c19c62ce":"Therefore we have successfully converted the textual data into numerical form","e63c7c68":"We find that these accuracies are improved than before. Hence we fine tune the hyperparameters XG boost and then train the Personalty detection model.","7b9af1e9":"Now we see the Wordclouds for each Personality Type. We produced 16 Word Clouds for 16 groups of personality. These word clouds are generated such that the size of each word is proportional to its appearance frequency in the top posts. We consider these word clouds to be illustrative of some of the unique ways that different MBTIs use language.","b11cc24a":"- We infer that there is **unequal distribution** even among each of the 4 axis in the entries of out dataset. i.e. out of IE:E is the majority, in NS:S is the majority. While TF and JP have realtively less differnce between them. \n\n\n\n","58293b68":"Since the original dataset only came with 2 features, the Type and 50 posts for each person, *we decided to create additional features* for exploring & analysing our dataset.\n\nAfter we added our features, we did some data exploration to see how the raw data looks and to see how important our features were for distinguishing types across the MBTI personalities. Below are plots *further showing the type imbalances in our data.*","6afa1594":"**WORDCLOUD**\n\nWordCloud is a technique to show which words are the most frequent among the given text. ","0702bd61":"**JOINT PLOT**\n\nPlotting the joint plot between the no. of words that occur in each comment v\/s \ntheir variance\n\nJointplot is seaborn library specific and can be used to quickly visualize and analyze the relationship between two variables and describe their individual distributions on the same plot. \n\nA distribution plot at the top for the column on the x-axis, a distribution plot on the right for the column on the y-axis and a scatter plot in between that shows the mutual distribution of data for both the columns. The area below the histogram helps in calculating the PDF, i.e. Probability Density function and the highest peak of the curve is the mean of distribution.","853f50c1":"## Personality Prediction 1 - cover letter","20946eed":"We preprocess the posts by using Lemmitization technique. **Lemmatization** is the process of grouping together the different inflected forms of a word so they can be analysed as a single item. Lemmatization is similar to stemming but it brings context to the words, hence we use this instead in our model. So it links words with similar meaning to one word. ","c2787702":"We can again see that this model underfits out dataset when we apply split ratio of 70:30 on our dataset i.e. the model has  not learned enough from the training data, resulting in low generalization and unreliable predictions. (almost all the results are near 50%, which is not good)","534c6ef3":"A number of configuration heuristics have been published in the original gradient boosting papers such as:\n\n* learning_rate in XGBoost should be set to 0.1 or lower, and smaller values will require the addition of more trees.\n* tree_depth in XGBoost should be configured in the range of 2-to-8, where not much benefit is seen with deeper trees.\nand so on...\n\nHere we have tried few parameters i order to improve our model's performance.","2792601d":"# **MBTI Personality Prediction Machine Learning Model**\n\n\n---\n\n","da979d2e":"# Feature Engineering ","aeb1295b":"Dataset Information","7b68e9c1":"Out of all the models, seen above we see that on an average XG Boost gives relatively good performance, hence we choose it to build our Personality prediction model. This will be beneficial as XGBoost model [[2]](https:\/\/machinelearningmastery.com\/xgboost-python-mini-course\/) can even be used to evaluate and report on the performance on a test set for the model during training.\n","4c00de2e":"\n\n*   So now there are 98555 features in our dataset for 8466 rows (users)\n\n","c256dc7f":"The size of the dataset","42cb7f51":"Now we add columns for personality type indicators\n\n","43b6964b":"Binarizing the each personality type feature","088f1014":"# Training & Evaluating : 70-30 split","68aeac2b":"Now we finding the unique values from the 'type' of personality column","e0a87266":"Hence we split the features as :\n\nX: User Posts in TF-IDF representation\n\nY: Personality type in Binarized MBTI form","3c3f97d4":"Counting the no. of users and posts in the given MBTI Kaggle dataset","11540797":"This plot further shows clearly that there are a no. of imbalances in our dataset, showing all the observations along with some representation of the underlying distribution using our added features.\n*   INFP has the most cluttered showing there are most no. of comments of this type of personality \n\n\n\n","4ca868a5":"**DISTANCE PLOT:**\n\nThis seaborn visualization method shows the histogram distribution of data for a single column.","cd3ece96":"The dataset is clearly unbalanced throughout the different classes. We observe that some of the personality types has a lot more data than others, the most common Kaggle users personality is INFP (Introvert Intuition Feeling Perceiving).\n\nHowever, we reach this conclusion based on user comments: we can consider for now that users who comment on social media more frequently are more intoverted, perceptive, and emotional. ","fd59612f":"# Training & Evaluating : 60-40 split","0039a580":"Now we see the Joint Plots for each Personality Type","51d19d04":"In natural language processing, useless words are referred to as **stop words**. ","0629d823":"### Splitting into X and Y variable","bdffeb27":"*   Using the above code, if a person has I, N, T and J, the value across the 4 axis of MBTI i.e. IE, NS, TF and JP respectively, will be 1. Else 0. \n\nThis will help us calculate for e.g. how many Introvert posts are present v\/s how many Extrovert posts are presnt, out of all the given entries in our labelled Kaggle dataset. This is done in order to extplore the dataset for all the individual Personality Indices of MBTI  \n","35e2eb6e":"\n\n*   We observe that almost all of these were were the most occuring words in our wordcloud above\n\n","eb0d166b":"\n\n*   We can see that most no of lengthly posts have between 7000-9000 words.\n*  The line that you see represents the kernel density estimation. It is a fundamental data smoothing problem where inferences about the population, based on a finite data sample. This kernel density estimate is a function defined as the sum of a kernel function on every data point.\n\n","3a07a760":"**Note:** Algorithms below can be quite time consuming","a1f1b94a":"*   The posts majorly contain general words like : I, I'm, so, me, or, if, and, can etc. It is safe to assume that these words won't really provide any useful information to train the ML model as most of them are stop-words, stem-words, or other useless words.\n*   Hence quite a lot pre-processing is required for individual user posts for each peronality type in the given MBTI dataset\n\n"}}