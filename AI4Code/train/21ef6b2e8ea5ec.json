{"cell_type":{"eff27856":"code","68855287":"code","b8d180b2":"code","abc798b2":"code","f8ba76e7":"code","01f29d08":"code","29909f73":"code","874e4388":"code","13587c80":"code","152ea38b":"code","2eb6d5a0":"code","f18f6cb5":"code","006aee8c":"code","5b2f9979":"code","e1696bfc":"markdown","5095063a":"markdown","3f3a45b3":"markdown","e8247700":"markdown","a62b22ce":"markdown","f1a0e882":"markdown","30e080cb":"markdown","c6dce4b3":"markdown","b038a903":"markdown","53c18870":"markdown"},"source":{"eff27856":"import joblib\nimport os\n\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow_addons as tfa\n\nfrom functools import partial\nfrom PIL import Image\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import KFold\nfrom sklearn.utils import shuffle\nfrom kaggle_datasets import KaggleDatasets\n\n\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import applications, layers, Model, Input\nfrom tensorflow.keras import (layers, Sequential, activations, initializers)\nfrom tensorflow.keras.applications import EfficientNetB5","68855287":"class MultiAtrous(tf.keras.Model):\n    def __init__(self, dilation_rates = [6, 12, 18], upsampling = 1, kernel_size = 3, padding = \"same\", **kwargs):\n        super(MultiAtrous, self).__init__(name = 'MultiAtrous', **kwargs)\n        self.dilation_rates = dilation_rates\n        self.kernel_size = kernel_size\n        self.upsampling = upsampling\n        self.padding = padding\n        self.dilated_convs = [layers.Conv2D(filters = int(1024 \/ 4), kernel_size = self.kernel_size, padding = self.padding, dilation_rate = rate) for rate in self.dilation_rates]\n        self.gap_branch = Sequential([layers.Lambda(lambda t4d: K.mean(t4d, axis = (1, 2), keepdims = True), name = 'GlobalAverage2D'), layers.Conv2D(int(1024 \/ 2), kernel_size = 1), layers.Activation('relu'), layers.UpSampling2D(size = self.upsampling, interpolation = \"bilinear\")], name = 'gap_branch')\n\n    def call(self, inputs, training = None, **kwargs):\n        local_feature = []\n        for dilated_conv in self.dilated_convs:\n            x = dilated_conv(inputs)\n            x = self.gap_branch(x)\n            local_feature.append(x)\n        return tf.concat(local_feature, axis = -1)\n\n    def get_config(self):\n        config = {'dilation_rates': self.dilation_rates, 'kernel_size': self.kernel_size, 'padding': self.padding, 'upsampling': self.upsampling}\n        base_config = super(MultiAtrous, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\nclass DOLGLocalBranch(tf.keras.Model):\n    def __init__(self, img_size, **kwargs):\n        super(DOLGLocalBranch, self).__init__(name = 'LocalBranch', **kwargs)\n        self.multi_atrous = MultiAtrous(padding = 'same', upsampling = int(img_size \/ 32))\n        self.conv1 = layers.Conv2D(1024, kernel_size = 1)\n        self.conv2 = layers.Conv2D(1024, kernel_size = 1, use_bias = False)\n        self.conv3 = layers.Conv2D(1024, kernel_size = 1)\n        self.bn = layers.BatchNormalization()\n\n    def call(self, inputs, training = None, **kwargs):\n        local_feat = self.multi_atrous(inputs)\n        local_feat = self.conv1(local_feat)\n        local_feat = tf.nn.relu(local_feat)\n        local_feat = self.conv2(local_feat)\n        local_feat = self.bn(local_feat)\n        norm_local_feat = tf.math.l2_normalize(local_feat)\n        attn_map = tf.nn.relu(local_feat)\n        attn_map = self.conv3(attn_map)\n        attn_map = activations.softplus(attn_map)\n        return norm_local_feat * attn_map\n\nclass OrthogonalFusion(layers.Layer):\n    def __init__(self, **kwargs):\n        super().__init__(name = 'OrthogonalFusion', **kwargs)\n\n    def call(self, inputs):\n        local_feat, global_feat = inputs\n        height = local_feat.shape[1]\n        width = local_feat.shape[2]\n        depth = local_feat.shape[3]\n\n        local_feat = tf.reshape(local_feat, [-1, height * width, depth])\n        local_feat = tf.transpose(local_feat, perm = [0, 2, 1])\n\n        projection = tf.matmul(tf.expand_dims(global_feat, axis = 1), local_feat)\n        projection = tf.matmul(tf.expand_dims(global_feat, axis = 2), projection)\n        projection = tf.reshape(projection, [-1, height, width, depth])\n\n        global_feat_norm = tf.norm(global_feat, ord = 2, axis = 1)\n        projection = projection \/ tf.reshape(global_feat_norm * global_feat_norm, shape = [-1, 1, 1, 1])\n        local_feat = tf.transpose(local_feat, perm = [0, 1, 2])\n        local_feat = tf.reshape(local_feat, [-1, height, width, depth])\n\n        orthogonal_comp = local_feat - projection\n        global_feat = tf.expand_dims(tf.expand_dims(global_feat, axis = 1), axis = 1)\n        global_feat = tf.broadcast_to(global_feat, tf.shape(local_feat))\n        output = tf.concat([global_feat, orthogonal_comp], axis = -1)\n        return output\n\nclass GeneralizedMeanPooling2D(layers.Layer):\n    def __init__(self, init_norm = 3.0, normalize = False, epsilon = 1e-6, **kwargs):\n        self.init_norm = init_norm\n        self.normalize = normalize\n        self.epsilon = epsilon\n        super(GeneralizedMeanPooling2D, self).__init__(name = 'GeM', **kwargs)\n\n    def build(self, input_shape):\n        self.p = self.add_weight(name = \"norms\", shape = (input_shape[-1],), initializer = initializers.constant(self.init_norm), trainable = True)\n        super(GeneralizedMeanPooling2D, self).build(input_shape)\n\n    def call(self, inputs):\n        x = tf.reduce_mean(tf.abs(inputs ** self.p), axis = [1, 2], keepdims = False) + self.epsilon\n        x = x ** (1.0 \/ self.p)\n        if self.normalize:\n            x = tf.nn.l2_normalize(x, 1)\n        return x\n\n    def get_config(self):\n        config = {'init_norm': self.init_norm, 'normalize': self.normalize, 'epsilon': self.epsilon}\n        base_config = super(GeneralizedMeanPooling2D, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n    \n\nclass DOLGNet(tf.keras.Model):\n    def __init__(self, img_size, **kwargs):\n        \n        self.img_size = img_size\n        super(DOLGNet, self).__init__(name = 'DOLGNet', **kwargs)\n        \n        self.orthogonal_fusion = OrthogonalFusion()\n        self.local_branch = DOLGLocalBranch(img_size)\n        self.glob_branch_pool = Sequential([layers.GlobalAveragePooling2D(), layers.Dense(1024, activation = None)], name = 'GlobalBranchPooling')\n        \n        base = applications.EfficientNetB5(\n            include_top = False,\n            weights = 'imagenet',\n            input_shape=(img_size,img_size,3),\n            input_tensor = Input((img_size, img_size, 3))\n        )\n        \n        # Batchlayers not to be trained\n        for layer in reversed(base.layers):\n            if isinstance(layer, tf.keras.layers.BatchNormalization):\n                layer.trainable = False\n            else:\n                layer.trainable = True\n        \n        self.new_base = Model([base.inputs], [base.get_layer('block5g_add').output,\n            base.get_layer('block7c_add').output\n        ], name = 'EfficientNet')\n        \n        \n        self.classifier = Sequential([layers.GlobalAveragePooling2D(name = 'HeadGAP')], name = 'Classifiers')\n        \n            \n    def call(self, inputs, training = None, **kwargs):\n        to_local, to_global = self.new_base(inputs)\n        local_feat = self.local_branch(to_local)\n        global_feat = self.glob_branch_pool(to_global)\n        \n        orthogonal_feat = self.orthogonal_fusion([local_feat, global_feat])\n        \n        return self.classifier(orthogonal_feat)\n\n\n    def build_graph(self):\n        x = tf.keras.layers.Input(shape = (self.img_size, self.img_size, 3), name=\"cnn_input\")\n        return Model(inputs = [x], outputs = self.call(x))","b8d180b2":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    DEVICE = \"TPU\"\n    batchsize_factor = 32\nexcept:\n    DEVICE = \"notTPU\"\n    strategy = tf.distribute.get_strategy()\n    batchsize_factor = 32\n\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\n\nREPLICAS =  strategy.num_replicas_in_sync\nBATCH_SIZE = batchsize_factor * strategy.num_replicas_in_sync\nGCS_PATH = KaggleDatasets().get_gcs_path(f'tfrecs-new')\nFILENAMES = tf.io.gfile.glob(GCS_PATH + '\/*.tfrecords')","abc798b2":"raw_dataset = tf.data.TFRecordDataset(FILENAMES[0])\n\ndef _get_keys(raw_dataset):\n    for raw_record in raw_dataset.take(1):\n        example = tf.train.Example()\n        example.ParseFromString(raw_record.numpy())\n        return dict(example.features.feature).keys()\n\n# Get feature columns from tfrecords and exclude features which are not used for training\nCONSIDERED_COLS = [k for k in _get_keys(raw_dataset) if k not in [\"image\", \"image_height\", \"image_width\", \"score\"]]\nprint(f\"{len(CONSIDERED_COLS)} features e.g. {CONSIDERED_COLS[0:5]}\")\n\nCONSIDERED_COLS = list(map(lambda x: x.lower(), CONSIDERED_COLS))","f8ba76e7":"DEFAULT_IMG_SIZE = (512,512)\n\ndef _parse_image(proto, train):\n    \n    image_feature_description = {\n        'image': tf.io.FixedLenFeature([], tf.string),\n        'image_width': tf.io.FixedLenFeature([], tf.int64),\n        'image_height': tf.io.FixedLenFeature([], tf.int64)\n        \n    }\n    \n    features = dict(\n        image_feature_description, **{k: tf.io.FixedLenFeature([], tf.float32) for k in CONSIDERED_COLS}\n    )\n        \n    if train:\n        features[\"score\"] = tf.io.FixedLenFeature([], tf.float32)\n        \n    return tf.io.parse_single_example(proto, features)\n\n\ndef decode_image(image, img_size, normalize=False):\n    image = tf.image.decode_jpeg(image, channels=3) \n    shapes = tf.shape(image)\n    h, w = shapes[-3], shapes[-2]\n    small = tf.minimum(h, w)\n    image = tf.image.resize_with_crop_or_pad(image, small, small)\n    image = tf.image.resize(image, img_size)\n    \n    # EfficientNet shouldn't be normalized as this is done in a custom model layer\n    if normalize:\n        image = tf.cast(image, tf.float16)\n        image = image \/ 255.0\n    \n    return tf.reshape(image, [*img_size, 3])\n\ndef get_image_and_label(proto, train, img_size):\n    sample = _parse_image(proto, train=train)\n    \n    img = decode_image(sample[\"image\"], img_size)\n    \n    features = {\"cnn_input\": img,\n               \"dense_input\": tf.stack([sample[c] for c in CONSIDERED_COLS])}\n    \n    if train:\n        return features, tf.cast(sample[\"score\"], tf.float32) \/ 100.0  \n    \n    return features, None\n\ndef augmentation(img):\n    \n    img = tf.image.random_flip_left_right(img)\n    \n    img = tf.image.random_flip_up_down(img)\n    \n    if tf.random.uniform([], 0, 1.0, dtype = tf.float32) > 0.75:\n        img = tf.image.transpose(img)\n    \n    probablity_rotation = tf.random.uniform([], 0, 1.0, dtype = tf.float32)\n    if probablity_rotation > 0.75:\n        img = tf.image.rot90(img, k = 3)\n    elif probablity_rotation > 0.5:\n        img = tf.image.rot90(img, k = 2)\n    elif probablity_rotation > 0.25:\n        img = tf.image.rot90(img, k = 1)   \n        \n    return img\n\ndef augmentation_wrapper(x, y):\n    x.update({\"cnn_input\": augmentation(x[\"cnn_input\"])})\n    return x, y\n\ndef scaling_wrapper(x, y):\n    return x[\"cnn_input\"] \/ 255.0\n\ndef get_tfrecord_size(tfrecord):\n    return sum(1 for _ in tfrecord)\n\ndef get_training_dataset(tfr, batchsize, img_size=DEFAULT_IMG_SIZE):\n    return tfr.map(partial(get_image_and_label, train=True, img_size=img_size)).repeat().map(\n        lambda x,y: augmentation_wrapper(x,y)).shuffle(1000).batch(batchsize, drop_remainder=True).prefetch(AUTOTUNE)\n\ndef get_validation_dataset(tfr, batchsize, img_size=DEFAULT_IMG_SIZE):\n    return tfr.map(partial(get_image_and_label, train=True, img_size=img_size)).batch(batchsize, drop_remainder=True).prefetch(AUTOTUNE)\n\ndef get_normalization_batch(tfr, batchsize, img_size=DEFAULT_IMG_SIZE):\n    return tfr.map(partial(get_image_and_label, train=True, img_size=img_size)).map(\n        lambda x,y: scaling_wrapper(x,y)).shuffle(1000).batch(batchsize, drop_remainder=True).prefetch(AUTOTUNE)","01f29d08":"def dolg():\n    tf.keras.backend.reset_uids()\n    model = DOLGNet(img_size = DEFAULT_IMG_SIZE[0]) \n    return model.build_graph()","29909f73":"def dense_net():\n    inputs = tf.keras.layers.Input(shape=len(CONSIDERED_COLS), name=\"dense_input\")\n    embedding_layer = tf.keras.layers.Embedding(input_dim=len(CONSIDERED_COLS), output_dim=10, name=\"emb_2\")(inputs)\n    flatten_layer = tf.keras.layers.Flatten()(embedding_layer)\n    dense_layer = tf.keras.layers.Dense(10, activation=\"relu\")(flatten_layer)\n    model = tf.keras.Model(inputs=inputs, outputs=dense_layer)\n    return model","874e4388":"def stack_model(models):\n    combined_inputs = [model.input for model in models]\n    combined_outputs = [model.output for model in models]\n    concat_layer = tf.keras.layers.concatenate(combined_outputs)\n    dense_layer_ = tf.keras.layers.Dense(1024, activation=\"relu\")(concat_layer)\n    dropout_layer_0 = tf.keras.layers.Dropout(0.3)(dense_layer_)\n    dense_layer_0 = tf.keras.layers.Dense(512, activation=\"relu\")(dropout_layer_0)\n    dropout_layer = tf.keras.layers.Dropout(0.3)(dense_layer_0)\n    dense_layer_1 = tf.keras.layers.Dense(128, activation=\"relu\")(dropout_layer)\n    dense_layer_2 = tf.keras.layers.Dense(32, activation=\"relu\")(dense_layer_1)\n    dense_layer_3 = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dense_layer_2)\n    return tf.keras.Model(inputs=combined_inputs, outputs=[dense_layer_3])","13587c80":"EPOCHS = 15\n\ndef lrfn(epoch, bs=BATCH_SIZE, epochs=EPOCHS):\n\n    LR_START = 1e-5\n    LR_MAX = 1e-4\n    LR_FINAL = 5e-5\n    LR_RAMPUP_EPOCHS = 3\n    LR_SUSTAIN_EPOCHS = 0\n    DECAY_EPOCHS = epochs  - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1\n    LR_EXP_DECAY = (LR_FINAL \/ LR_MAX) ** (1 \/ (EPOCHS - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS - 1))\n\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = LR_START + (LR_MAX + LR_START) * (epoch \/ LR_RAMPUP_EPOCHS) ** 2.5\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        epoch_diff = epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS\n        decay_factor = (epoch_diff \/ DECAY_EPOCHS) * 3.141592653589793\n        decay_factor= (tf.math.cos(decay_factor).numpy() + 1) \/ 2        \n        lr = LR_FINAL + (LR_MAX - LR_FINAL) * decay_factor\n\n    return lr","152ea38b":"import matplotlib.pyplot as plt  \nrng = [i for i in range(20)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y);","2eb6d5a0":"_dolg = dolg()\n_dense = dense_net()\nstacked_model = stack_model([_dolg,_dense])\nstacked_model.compile()\ntf.keras.utils.plot_model(stacked_model, show_shapes=True)","f18f6cb5":"kfold = KFold(n_splits=4, shuffle=True, random_state=0)\nfilenames = [f for f in [_ for _ in os.listdir(\"..\/input\/tfrecs-new\") if len(_.split(\".\")) > 1] if f.split(\".\")[1] ==\"tfrecords\"]\nfolds = {}\nfor fold, (train_idx, val_idx) in enumerate(kfold.split(filenames)):\n  folds[fold] = {\"split\": (np.take(filenames, train_idx), np.take(filenames, val_idx))}","006aee8c":"ignore_order = tf.data.Options()\nignore_order.experimental_deterministic = False\n\nif DEVICE ==\"TPU\":\n    tf.tpu.experimental.initialize_tpu_system(tpu)","5b2f9979":"cv_history = []\n\n\nimg_size = DEFAULT_IMG_SIZE\n\nfor fold in [3]:\n\n    validation_records = tf.data.TFRecordDataset(\n      [f for f in FILENAMES if f.split(\"\/\")[-1] in folds[fold][\"split\"][1]],\n      num_parallel_reads=AUTOTUNE)\n\n    train_records = tf.data.TFRecordDataset([f for f in FILENAMES if f.split(\"\/\")[-1] in folds[fold][\"split\"][0]],\n                                          num_parallel_reads=AUTOTUNE)\n\n    train_records = train_records.with_options(ignore_order)\n\n    validation_data = get_validation_dataset(validation_records, BATCH_SIZE, img_size) \n    \n    print(f\"Used batchsize: {BATCH_SIZE}\")\n    \n    train_data = get_training_dataset(train_records, BATCH_SIZE, img_size)\n\n    train_size = get_tfrecord_size(train_records)\n    validation_size = get_tfrecord_size(validation_records)\n\n    with strategy.scope():\n\n        model1 = dolg()\n        model2 = dense_net()\n        model = stack_model([model1, model2])\n\n        # get data to set mean and std vor the normalization layers\n        model.get_layer('EfficientNet').get_layer('normalization').adapt(\n            get_normalization_batch(train_records, BATCH_SIZE, img_size)\n        )   \n\n        opt = tf.keras.optimizers.Adam(lr=0.001)\n        \n        loss = tf.keras.losses.BinaryCrossentropy(label_smoothing=0.01)\n\n        model.compile(loss=loss, optimizer=opt, metrics=[\"mse\", tf.keras.metrics.RootMeanSquaredError()])\n\n        model.summary()\n\n        cb_lr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: lrfn(epoch), verbose=1)\n\n        cb_earlystop = tf.keras.callbacks.EarlyStopping( \n                           patience=3, restore_best_weights=True, verbose=1)\n\n        params = {\n            \"epochs\":15,\n            \"steps_per_epoch\":train_size\/\/BATCH_SIZE,\n            \"validation_data\": validation_data,\n            \"callbacks\": [cb_earlystop, cb_lr]\n        } \n\n        print(f\"Fold: {fold}, {train_size} train images {validation_size} validation images\")\n\n        history = model.fit(train_data, **params)\n\n        cv_history.append(history.history)\n","e1696bfc":"# KFold Splitting and Training","5095063a":"# Inspect TFRecords\n\nThe dataset contains several features:\n* All features provided by the competition hosts (accessory, info, collage, action, face, near, human...)\n* An image quality assessment feature called \"brisque\" and calculated with the piq pytorch package (https:\/\/github.com\/photosynthesis-team\/piq)\n* An colorfulness score (calculated according to https:\/\/www.pyimagesearch.com\/2017\/06\/05\/computing-image-colorfulness-with-opencv-and-python\/)\n* The image size and the size ration (both scaled between 0 and 1)\n* Image brightness and saturation\n* The type of the animal (0: dog, 1: cat). For the classification a pretrained ResNet50V2 was used\n* The breed of the dog (using an Inception architecture trained on the Kaggle dog breed dataset). The values were one-hot encoded afterwards.","3f3a45b3":"# Building the complete model consisting of a DLOG branch and another branch for including the categorical features","e8247700":"# PetFinder.my - Pawpularity Contest Solution using a DOLG model and additional categorical data","a62b22ce":"# Initalize TPU","f1a0e882":"# Prepare TFRecords and apply light augmentations","30e080cb":"# DOLG implementation with EfficientNetB5 base\n**DOLG implementation origins from https:\/\/github.com\/innat\/DOLG-TensorFlow (see also the authors Kaggle Notebooks https:\/\/www.kaggle.com\/ipythonx). Some adaptions were made to use an EfficientNet base**","c6dce4b3":"# Overall architecture\n\n**The model has two different inputs. An image for the cnn part and and feature vector containing the meta information**","b038a903":"**Training code which was submitted to the Petfinder.my Competition**\n\nThis notebook deals with:\n* training code of a Deep Orthogonal Fusion of Local and Global Features (DOLG) model\n* adding an additional branch which uses categorical information\n* categorical data includes (i) image meta features like colorfulness, saturation, size ratio ... and (ii) image content feature like animal type, breed\n* training can be executed on TPU\n\n**CV score was around 17.5 and Private Leaderbord score ~17.8**","53c18870":"# Learning rate sheduler"}}